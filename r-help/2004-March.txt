From gagongabe at earthlink.net  Mon Mar  1 00:43:25 2004
From: gagongabe at earthlink.net (Gabriel Lawson)
Date: Sun, 29 Feb 2004 15:43:25 -0800
Subject: [R] Rcmd SHLIB
References: <Pine.LNX.4.44.0402290748330.29606-100000@gannet.stats>
Message-ID: <006101c3ff1d$d3155170$6501a8c0@lankhmar>

Thanks tons.  If you get a moment and feel like answering another question:
Regarding the readme.packages file excerpt below:  How can I set the R_HOME
path?  Is it always the R install directory?  Also, is creating libR.a and
libRblas.a a neccessary step in creating ANY dll for use with R?
BEWARE: Don't expect this to work if the path to R_HOME contains spaces.

It may work, but we don't recommend it.



Thanks for your time,

gabe





----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Gabriel Lawson" <gagongabe at earthlink.net>
Cc: <r-help at stat.math.ethz.ch>
Sent: Saturday, February 28, 2004 11:54 PM
Subject: Re: [R] Rcmd SHLIB


> Most likely you don't have dlltool in your path.  Further, I suspect you
> don't have the correct make in your path since AFAIK that does not use
> CreateProcess.
>
> Please check and check again that you have followed exactly all the
> instructions in the file readme.packages.  See Q3.1 in the rw-FAQ.
>
> On Sat, 28 Feb 2004, Gabriel Lawson wrote:
>
> > Ok, I think I may have a path or permissions problem (below).  Anyone
> > know which settings I should check?
> >
> > When I use
> >
> > "Rcmd SHLIB <filename>"
> >
> > I get:
> >
> > C:\Program Files\R\rw1081\bin>Rcmd SHLIB info.diffusion.c
> > process_begin: CreateProcess((null), dlltool -k --as as --dllname
R.dll --def R.
> > exp --output-lib libR.a, ...) failed.
> > make (e=2): The system cannot find the file specified.
> > make[1]: *** [libR.a] Error 2
> > make: *** [libR] Error 2
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From ggrothendieck at myway.com  Mon Mar  1 01:27:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 29 Feb 2004 19:27:12 -0500 (EST)
Subject: [R] Proportions again
Message-ID: <20040301002712.E07373958@mprdmxin.myway.com>


That's true; however, 

CrossTable(x,x)

does give the desired counts and proportions in the margin
line at the bottom.  See the row labelled Column Total in
the following example based on Carlos' vector:

> sex<-c(1,2,2,1,1,2,2,2)
> CrossTable(sex,sex)

 
   Cell Contents
|-----------------|
|               N |
|   N / Row Total |
|   N / Col Total |
| N / Table Total |
|-----------------|

 
Total Observations in Table:  8 

 
             | sex 
         sex |         1 |         2 | Row Total | 
-------------|-----------|-----------|-----------|
           1 |         3 |         0 |         3 | 
             |     1.000 |     0.000 |     0.375 | 
             |     1.000 |     0.000 |           | 
             |     0.375 |     0.000 |           | 
-------------|-----------|-----------|-----------|
           2 |         0 |         5 |         5 | 
             |     0.000 |     1.000 |     0.625 | 
             |     0.000 |     1.000 |           | 
             |     0.000 |     0.625 |           | 
-------------|-----------|-----------|-----------|
Column Total |         3 |         5 |         8 | 
             |     0.375 |     0.625 |           | 
-------------|-----------|-----------|-----------|


---
Date:   Sun, 29 Feb 2004 13:48:27 -0600 
From:   Marc Schwartz <MSchwartz at MedAnalytics.com>
To:   <ggrothendieck at myway.com> 
Cc:   <mcardeal at atarde.com.br>,R-Help <r-help at stat.math.ethz.ch> 
Subject:   RE: [R] Proportions again 

 
On Sun, 2004-02-29 at 12:40, Gabor Grothendieck wrote:
> Several people have alrady answered you by this time and
> in addition to their answers you might also be interested
> in CrossTable in package gregmisc.

Gabor,

Thanks for pointing out CrossTable().

Just as a quick heads up/clarification for Carlos, CrossTable() is
designed to cross-tabulate two vectors and generate counts,
row/column/table proportions and other results from the 2 dimensional
cross-tab in a (hopefully) nicely formatted fashion.

It is not presently designed to handle generating proportions from the
tabulation of a single vector with repeating values (such as Carlos'
example) and will generate an error message if a single vector is
passed.

In that scenario, as many folks have already recommended, the
combination of table() and prop.table() would be preferred.

HTH,

Marc Schwartz



From dmurdoch at pair.com  Mon Mar  1 01:29:45 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 29 Feb 2004 19:29:45 -0500
Subject: [R] Rcmd SHLIB
In-Reply-To: <006101c3ff1d$d3155170$6501a8c0@lankhmar>
References: <Pine.LNX.4.44.0402290748330.29606-100000@gannet.stats>
	<006101c3ff1d$d3155170$6501a8c0@lankhmar>
Message-ID: <uqv440h0eqsab4nicvvmsghlodmcg3fm0o@4ax.com>

On Sun, 29 Feb 2004 15:43:25 -0800, you wrote:

>Thanks tons.  If you get a moment and feel like answering another question:
>Regarding the readme.packages file excerpt below:  How can I set the R_HOME
>path?  Is it always the R install directory?  Also, is creating libR.a and
>libRblas.a a neccessary step in creating ANY dll for use with R?

R_HOME should be the base directory where you install R.

The libR.a and libRblas.a files are used by the gcc linker to tell it
how to make calls into R.dll or Rblas.dll.  If you're using a
different compiler or you don't need to make calls into those dlls
(e.g. you've got a very simple DLL), you won't need them.

Duncan Murdoch



From MSchwartz at medanalytics.com  Mon Mar  1 02:19:29 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Sun, 29 Feb 2004 19:19:29 -0600
Subject: [R] Proportions again
In-Reply-To: <20040301002712.E07373958@mprdmxin.myway.com>
References: <20040301002712.E07373958@mprdmxin.myway.com>
Message-ID: <1078103969.7704.62.camel@localhost.localdomain>

On Sun, 2004-02-29 at 18:27, Gabor Grothendieck wrote:
> That's true; however, 
> 
> CrossTable(x,x)
> 
> does give the desired counts and proportions in the margin
> line at the bottom.  See the row labelled Column Total in
> the following example based on Carlos' vector:
> 
> > sex<-c(1,2,2,1,1,2,2,2)
> > CrossTable(sex,sex)

snip

OK....true. I had not considered that approach from a design
perspective, but it does provide the requisite information.

One could feasibly reduce some of the complexity of the table by setting
some of the prop.* arguments to false:

CrossTable(sex, sex, prop.c = FALSE, prop.t = FALSE)

which would yield:


   Cell Contents
|-----------------|
|               N |
|   N / Row Total |
|-----------------|

 
Total Observations in Table:  8 

 
             | sex 
         sex |         1 |         2 | Row Total | 
-------------|-----------|-----------|-----------|
           1 |         3 |         0 |         3 | 
             |     1.000 |     0.000 |     0.375 | 
-------------|-----------|-----------|-----------|
           2 |         0 |         5 |         5 | 
             |     0.000 |     1.000 |     0.625 | 
-------------|-----------|-----------|-----------|
Column Total |         3 |         5 |         8 | 
-------------|-----------|-----------|-----------|


This gives you the proportions in the row margins versus the columns.
You still end up with some extraneous data, but perhaps a little less
so.

Just seems like overkill to get the same information that:

> prop.table(table(sex))
sex
    1     2
0.375 0.625

gives you, without the lengthy function call.  :-)


Thanks Gabor.

Best regards,

Marc



From nielssteenkrogh at hotmail.com  Mon Mar  1 02:23:14 2004
From: nielssteenkrogh at hotmail.com (Niels Steen Krogh)
Date: Mon, 01 Mar 2004 02:23:14 +0100
Subject: [R] glm logistic model,
	prediction intervals on impact af age 60 compared to age 30
Message-ID: <BAY7-F61NXwQbaL0xiJ00029cff@hotmail.com>

Dear R-list.
I have done a logistic glm using Age as explanatory variable for some 
allergic event.

#the model
model2d<-glm(formula=AEorSAEInfecBac~Age,family=binomial("logit"),data=emrisk)
#predictions for age 30 and 60
preds<-predict(model2d,data.frame(Age=c(30,60)),se.fit=TRUE)
# prediction interval
predsxx<-cbind(fit=preds$fit,lower=preds$fit-1.96*preds$se,upper=preds$fit+1.96*preds$se)
#transformation
model2dres<-family(model2d)$linkinv(predsxx)


In my next step I want to know the confidence interval (CI) for the change 
in risk for the allergic event to occur for age 60 compared to age 30.
The estimates from the model suggest a 80 pct. higher risk for age 60 
compared to age 30.
(100*model2dres[2]/model2dres[1])

But how should I get the 95% CI of the 80pct. increase??

I've looked in the effects package but  did'nt find an answer.

Any hints?


R 1.8.1.
Windows
Cand. Polit.
Niels Steen Krogh
Solsortvej 44
2000 F.

Tlf: 3888 8613

ZiteLab / EmpoweR youR data with R, Zope and SOAP



From ggrothendieck at myway.com  Mon Mar  1 02:26:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 29 Feb 2004 20:26:43 -0500 (EST)
Subject: [R] Proportions again
Message-ID: <20040301012643.341D0396B@mprdmxin.myway.com>



I agree its overkill but prop.table generalizes to mulitple
dimensions, in which case it becomes comparable to CrossTable,
so I thought it was worth mentioning.

By the way, as the author of CrossTable, perhaps you 
might might consider generalizing CrossTable to the 1D 
case too?

---
Date:   Sun, 29 Feb 2004 19:19:29 -0600 
From:   Marc Schwartz <MSchwartz at MedAnalytics.com>
To:   <ggrothendieck at myway.com> 
Cc:   <mcardeal at atarde.com.br>,R-Help <r-help at stat.math.ethz.ch> 
Subject:   RE: [R] Proportions again 

 
On Sun, 2004-02-29 at 18:27, Gabor Grothendieck wrote:
> That's true; however, 
> 
> CrossTable(x,x)
> 
> does give the desired counts and proportions in the margin
> line at the bottom. See the row labelled Column Total in
> the following example based on Carlos' vector:
> 
> > sex<-c(1,2,2,1,1,2,2,2)
> > CrossTable(sex,sex)

snip

OK....true. I had not considered that approach from a design
perspective, but it does provide the requisite information.

One could feasibly reduce some of the complexity of the table by setting
some of the prop.* arguments to false:

CrossTable(sex, sex, prop.c = FALSE, prop.t = FALSE)

which would yield:


Cell Contents
|-----------------|
| N |
| N / Row Total |
|-----------------|


Total Observations in Table: 8 


| sex 
sex | 1 | 2 | Row Total | 
-------------|-----------|-----------|-----------|
1 | 3 | 0 | 3 | 
| 1.000 | 0.000 | 0.375 | 
-------------|-----------|-----------|-----------|
2 | 0 | 5 | 5 | 
| 0.000 | 1.000 | 0.625 | 
-------------|-----------|-----------|-----------|
Column Total | 3 | 5 | 8 | 
-------------|-----------|-----------|-----------|


This gives you the proportions in the row margins versus the columns.
You still end up with some extraneous data, but perhaps a little less
so.

Just seems like overkill to get the same information that:

> prop.table(table(sex))
sex
1 2
0.375 0.625

gives you, without the lengthy function call. :-)


Thanks Gabor.

Best regards,

Marc



From Duncan.Mackay at flinders.edu.au  Mon Mar  1 02:29:09 2004
From: Duncan.Mackay at flinders.edu.au (Duncan Mackay)
Date: Mon, 1 Mar 2004 11:59:09 +1030
Subject: [R] se.contrast ....too hard??? .... Too easy????? .....too
	trivial???? ...... Too boring.....too????????
Message-ID: <001401c3ff2c$98625190$b9e66081@duncanlt>

Hi all,
Regular and avid readers of this column will know that Don Driscoll and
I have recently posted two messages requesting assistance concerning an
apparent failure of "se.contrast" to produce an se for a contrast. So
far, an ominous silence rings in our ears, but read on Gentle Reader,
and see if even the machinations of "debug" doesn't stimulate you to
respond with a revelatory epistle.

Thanks!!!
Duncan



Here is my first message :-

Just to follow up Don Driscoll's earlier post, can anyone please explain
why "se.contrast" fails here??

> shp<-factor(rep(c("reserve","strip"),each=96))
> 
> 
>
site<-factor(rep(c("1g","1p","1t","2g","2p","2t","3g","3p","3t","4g","4p
","4t"),each=16))
> 
> pit<-factor(rep(1:16,12))
> 
>
reptsp<-c(4,5,6,4,6,6,6,7,3,5,2,2,4,8,5,4,2,4,2,2,4,5,2,4,4,4,3,2,3,2,5,
3,5,3,4,4,4,3,4,
+
3,4,4,4,3,4,3,6,3,3,5,4,6,4,4,2,4,2,6,5,5,5,7,4,4,5,1,4,5,6,5,5,2,6,3,5,
6,4,5,
+
4,8,2,4,2,4,2,4,3,3,4,4,3,2,1,3,4,4,2,2,3,2,4,1,2,2,3,4,5,5,3,5,5,4,1,1,
2,1,3,
+
1,4,1,6,1,2,3,2,2,2,1,1,2,2,6,5,3,2,3,5,3,2,3,2,1,3,2,4,4,3,3,3,1,2,4,3,
4,5,6,
+
5,2,3,2,2,5,5,5,2,2,5,2,4,4,3,2,2,3,2,2,2,2,5,4,3,3,5,2,5,4,3,2,2,2,1,2)
> 
> ddata<-data.frame(shp,pit,site,reptsp)
> 
> repmod2<-aov(reptsp~shp/site+ Error(shp/site))
> summary(repmod2)

Error: shp
    Df Sum Sq Mean Sq
shp  1  53.13   53.13

Error: shp:site
         Df Sum Sq Mean Sq
shp:site 10 61.885   6.189

Error: Within
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals 180 318.56    1.77               
> 
> table(ddata$shp)

reserve   strip 
     96      96 
> 
> se.contrast(repmod2, list(shp=="strip", shp=="reserve"),data=ddata)
Error in qr.qty(strata$qr, scontrast) : qr and y must have the same
number of rows
> 
?????????????????????


Thanks,
Duncan


...............and here is what debug says:-

> debug("qr.qty")
> se.contrast(repmod2, list(shp=="strip", shp=="reserve"),data=ddata)
debugging in: qr.qty(e.qr, contrast)
debug: {
    if (!is.qr(qr)) 
        stop("argument is not a QR decomposition")
    if (is.complex(qr$qr)) {
        y <- as.matrix(y)
        if (!is.complex(y)) 
            y[] <- as.complex(y)
        return(.Call("qr_qy_cmplx", qr, y, 1, PACKAGE = "base"))
    }
    a <- attr(qr, "useLAPACK")
    if (!is.null(a) && is.logical(a) && a) 
        return(.Call("qr_qy_real", qr, as.matrix(y), 1, PACKAGE =
"base"))
    n <- nrow(qr$qr)
    k <- as.integer(qr$rank)
    ny <- NCOL(y)
    if (NROW(y) != n) 
        stop("qr and y must have the same number of rows")
    storage.mode(y) <- "double"
    .Fortran("dqrqty", as.double(qr$qr), n, k, as.double(qr$qraux), 
        y, ny, qty = y, PACKAGE = "base")$qty
}
Browse[1]> 
debug: if (!is.qr(qr)) stop("argument is not a QR decomposition")
Browse[1]> 
debug: if (is.complex(qr$qr)) {
    y <- as.matrix(y)
    if (!is.complex(y)) 
        y[] <- as.complex(y)
    return(.Call("qr_qy_cmplx", qr, y, 1, PACKAGE = "base"))
}
Browse[1]> 
debug: a <- attr(qr, "useLAPACK")
Browse[1]> 
debug: if (!is.null(a) && is.logical(a) && a) return(.Call("qr_qy_real",

    qr, as.matrix(y), 1, PACKAGE = "base"))
Browse[1]> 
debug: n <- nrow(qr$qr)
Browse[1]> 
debug: k <- as.integer(qr$rank)
Browse[1]> 
debug: ny <- NCOL(y)
Browse[1]> 
debug: if (NROW(y) != n) stop("qr and y must have the same number of
rows") Browse[1]> ny [1] 1 Browse[1]> NCOL(y) [1] 1 Browse[1]>
nrow(qr$qr) [1] 192 Browse[1]> print(n) [1] 192 Browse[1]> dim(qr$qr)
[1] 192  24 Browse[1]> NROW(y) [1] 192 Browse[1]> NROW(y)==n [1] TRUE
Browse[1]> Q

So, if NROW(y)==n, why do I get the message "qr and y must have the same
number of rows"

Any help gratefully appreciated....
Duncan

**********************************************

R, me bucko

*****************************************
Dr. Duncan Mackay
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide
S.A.    5001
AUSTRALIA

Ph (08) 8201 2627    FAX (08) 8201 3015

http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at medanalytics.com  Mon Mar  1 02:54:35 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Sun, 29 Feb 2004 19:54:35 -0600
Subject: [R] Proportions again
In-Reply-To: <20040301012643.341D0396B@mprdmxin.myway.com>
References: <20040301012643.341D0396B@mprdmxin.myway.com>
Message-ID: <1078106075.7704.92.camel@localhost.localdomain>

On Sun, 2004-02-29 at 19:26, Gabor Grothendieck wrote:
> I agree its overkill but prop.table generalizes to mulitple
> dimensions, in which case it becomes comparable to CrossTable,
> so I thought it was worth mentioning.
> 
> By the way, as the author of CrossTable, perhaps you 
> might might consider generalizing CrossTable to the 1D 
> case too?



As a result of this exchange, it is now on my todo list... :-)

Thanks Gabor.

Marc



From ru68y7s at myrealbox.com  Mon Mar  1 06:43:32 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Sun, 29 Feb 2004 22:43:32 -0700
Subject: [R] Re: R-help Digest, Vol 12, Issue 27
Message-ID: <1078119812.c7fba21cru68y7s@myrealbox.com>

Dear R helpers,

I am interested in doing an overlapping moving block bootstrap (Davinson and Hinkley(1997)) on financial market data
using R.

I have daily stock returns and have tried
( unsuccessfully using sample and boot function) to create a sample containing consecutive "trading days"


For example i want to sample data from a return series x days forward when an event(binary) occurs.

Also, has anyone programmed the 'omega' function used by some funds to rank returns using cdf's and comparing areas under two return curves for a mutual fund?

Any help is greatly appreciated.

Thank you ,

S Viswanath



From ripley at stats.ox.ac.uk  Mon Mar  1 08:24:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Mar 2004 07:24:25 +0000 (GMT)
Subject: [R] Rcmd SHLIB
In-Reply-To: <006101c3ff1d$d3155170$6501a8c0@lankhmar>
Message-ID: <Pine.LNX.4.44.0403010721100.6156-100000@gannet.stats>

On Sun, 29 Feb 2004, Gabriel Lawson wrote:

> Thanks tons.  If you get a moment and feel like answering another question:
> Regarding the readme.packages file excerpt below:  How can I set the R_HOME
> path?  Is it always the R install directory?  Also, is creating libR.a and

It is where you installed R.

> libRblas.a a neccessary step in creating ANY dll for use with R?

No, but it is a necessary step to using the supplied tools and it is 
needed for many DLLs.

> BEWARE: Don't expect this to work if the path to R_HOME contains spaces.
> 
> It may work, but we don't recommend it.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From albedo at pisem.net  Mon Mar  1 08:28:13 2004
From: albedo at pisem.net (Albedo)
Date: Mon, 01 Mar 2004 09:28:13 +0200
Subject: [R] question about mixed effects model
Message-ID: <4042E60D.9090201@pisem.net>

Hello.

I have some trouble with mixed effects in R, similar to problems
that other people had with not nested models and lme, as I
understand from the mailing list archive. Unfortunately, I could
not understand the solutions that were proposed...

I have a data set with response variable (y) and two explanatory
variables x1 and x2 (x1 - fixed factor, x2 - random factor). Fixed
factor x1 is repeated twice for each x2 value.

I believe that the correct way to fit a mixed model  would be

ddd <- groupedData(y ~ x1 | x2, dd)
l1 <- lme(y ~ x1, random = pdBlocked(list(pdIdent(~x2-1))), data = ddd)

But I get an error
Error in pdConstruct.pdBlocked(object, form = form, nam = nam, data =
data,  :    None of the arguments specify more than one block


I would also have to test for significance of each variable (x1 and x2)
and the way to do this seem to be to fit a model without x1 and compare
it to the original l1 model (and then the same for x2), but I can't seem
to find a way to do this.

Thanks a lot.



From ripley at stats.ox.ac.uk  Mon Mar  1 08:35:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Mar 2004 07:35:35 +0000 (GMT)
Subject: [R] glm logistic model, prediction intervals on impact af age
	60 compared to age 30
In-Reply-To: <BAY7-F61NXwQbaL0xiJ00029cff@hotmail.com>
Message-ID: <Pine.LNX.4.44.0403010728380.6156-100000@gannet.stats>

As I understand what you have done, model2dres are probabilities, not
risks.  Since this is a logistic regression, you want to interpret the
results via log odds (or odds).  The log odds of an event at age 60 vs age
30 are just 30x the coefficient for Age.

On Mon, 1 Mar 2004, Niels Steen Krogh wrote:

> Dear R-list.
> I have done a logistic glm using Age as explanatory variable for some 
> allergic event.
> 
> #the model
> model2d<-glm(formula=AEorSAEInfecBac~Age,family=binomial("logit"),data=emrisk)
> #predictions for age 30 and 60
> preds<-predict(model2d,data.frame(Age=c(30,60)),se.fit=TRUE)
> # prediction interval
> predsxx<-cbind(fit=preds$fit,lower=preds$fit-1.96*preds$se,upper=preds$fit+1.96*preds$se)
> #transformation
> model2dres<-family(model2d)$linkinv(predsxx)
> 
> 
> In my next step I want to know the confidence interval (CI) for the change 
> in risk for the allergic event to occur for age 60 compared to age 30.
> The estimates from the model suggest a 80 pct. higher risk for age 60 
> compared to age 30.
> (100*model2dres[2]/model2dres[1])
> 
> But how should I get the 95% CI of the 80pct. increase??
> 
> I've looked in the effects package but  did'nt find an answer.

You would need the covariance of those estimates to do so in the
asymptotic setting (and you would need to get that from first principles),
but unless your effects have been estimated very precisely the
non-linearity is likely to make any confidence intervals you derived
pretty inaccurate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Mar  1 09:46:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Mar 2004 08:46:53 +0000 (GMT)
Subject: [R] Confused in simplest-possible function
In-Reply-To: <20040229171421.GA25919@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0403010823470.6337-100000@gannet.stats>

This really isn't to do with functions, but to do with the fine details 
of formatting inside methods for summary().  It happens if you can summary 
directly:

> a
[1]  854.2 1533.3 1011.1
> print(a, digits=4)
[1]  854.2 1533.3 1011.1
> summary(a)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  854.2   932.7  1011.0  1133.0  1272.0  1533.0

In the last two cases, the result is supposed to be printed to 4
significant figures, but for print() it is applied to the whole vector and
for summary() to each of the numbers individually.

> DF <- data.frame(a=a)
> print(DF, digits=4)
       a
1  854.2
2 1533.3
3 1011.1
> summary(DF)
       a
 Min.   : 854.2
 1st Qu.: 932.6
 Median :1011.1
 Mean   :1132.9
 3rd Qu.:1272.2
 Max.   :1533.3

again applies the digits to the whole set of results.

The underlying difference is that summary.default calls signif() to set
the number of significant digits and summary.data.frame calls format() and
they (deliberately) behave differently when applied to a vector.

It looks like it was decided about three years ago that this was less 
confusing than what went before -- if we were to change anything it would 
be how summary.default behaved I think.


On Sun, 29 Feb 2004, Ajay Shah wrote:

> I wrote the following code:
> 
>   ---------------------------------------------------------------------------
>   oneindex <- function(x) {
>         summary(x)
>   }
> 
>   A <- read.table("try.data",
>                   col.names=c("date", "lNifty"))
>   summary(A)
>   oneindex(A$lNifty)
>   ---------------------------------------------------------------------------
> 
> where I read in data, make a summary directly, and then call a
> function `oneindex' which merely makes a summary.
> 
> I'm puzzled because the two summaries disagree :
> 
> > oneindex <- function(x) {
> +       summary(x)
> + }
> > 
> > A <- read.table("try.data",
> +                 col.names=c("date", "lNifty"))
> > summary(A)
>          date         lNifty      
>  2000-06-12:  1   Min.   : 854.2  
>  2000-06-13:  1   1st Qu.:1032.8  
>  2000-06-15:  1   Median :1088.7  
>  2000-06-16:  1   Mean   :1123.6  
>  2000-06-19:  1   3rd Qu.:1178.7  
>  2000-06-20:  1   Max.   :1533.3  
>  (Other)   :780                   
> > oneindex(A$lNifty)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>   854.2  1033.0  1089.0  1124.0  1179.0  1533.0 
> 
> Here you see the median showing up as 1088.7 in the 1st case but
> 1089.0 in the 2nd case. How could that happen?
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From WeiQiang.Li at seagate.com  Mon Mar  1 10:18:12 2004
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Mon, 1 Mar 2004 17:18:12 +0800
Subject: [R] How to plot Histogram with frequence overlaid by distribution
	curve
Message-ID: <OF9B98249A.2806B48A-ON48256E4A.002C071B-48256E4A.00333ECF@notes.seagate.com>

Hi,
      I am facing the problem that I want to plot a histogram chart set
freq to true and overlay with normal or weibull or exponential distribution
curve.

      The sample code is shown as below:
      >samp<-c(-8.2262,-8.2262,-8.2262,-8.20209,-8.09294,-8.07321,-8.07321,
      -8.07321,-8.07175,-8.04948,-8.04948,-8.04948,-8.03848,-8.03848,
      -8.026,-7.92517,-7.92517,-7.77218,-7.62414,-7.62414,-7.62414,
      -7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.28924,
      -7.28924,-6.78729,-6.25307)

      >hist(samp,freq=TRUE,br=20)
      >curve(dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)

      In the chart created based on above command, curve scale is too small
compared to the freqeunce. My question here is how to adjust the scale of
distribution curve. Thanks !

Best Regards
WeiQiang Li



From demiurg at post.tau.ac.il  Mon Mar  1 11:11:41 2004
From: demiurg at post.tau.ac.il (demiurg@post.tau.ac.il)
Date: Mon,  1 Mar 2004 12:11:41 +0200
Subject: [R] boxcox in MASS library
Message-ID: <1078135901.40430c5def863@webmail.tau.ac.il>

Help page for boxcox function in MASS library says that 
the transformation is y^lambda, which is different from
the Y' = log(Y) if lambda = 0 , 
Y' = ((Y ^ lambda) - 1)/lambda otherwise I'm used to.

Is this just a help page typo ?

Thanks.


----------------------------------------------------------------
This message was sent using IMP, the Internet Messaging Program.



From ripley at stats.ox.ac.uk  Mon Mar  1 12:03:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Mar 2004 11:03:14 +0000 (GMT)
Subject: [R] boxcox in MASS library
In-Reply-To: <1078135901.40430c5def863@webmail.tau.ac.il>
Message-ID: <Pine.LNX.4.44.0403011056310.9891-100000@gannet.stats>

On Mon, 1 Mar 2004 demiurg at post.tau.ac.il wrote:

> Help page for boxcox function in MASS library says that 
> the transformation is y^lambda, which is different from
> the Y' = log(Y) if lambda = 0 , 
> Y' = ((Y ^ lambda) - 1)/lambda otherwise I'm used to.
> 
> Is this just a help page typo ?

No.  It makes no difference to the profile log-likelihood which is what
boxcox() computes.

It looks to me as if you failed to consult the reference.  MASS is support 
software for a book, and the book has the complete details.  (This is in 
the posting guide.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From etptupaf at bs.ehu.es  Mon Mar  1 12:08:46 2004
From: etptupaf at bs.ehu.es (F. Tusell)
Date: Mon, 01 Mar 2004 12:08:46 +0100
Subject: [R] Maps from PX-Map files
Message-ID: <404319BE.7080306@bs.ehu.es>

I have been handed a CD ROM with statistical data readable with a 
program named PC-Axis. There
is the possibility of creating custom maps to display the data. 
Aparently, this is handled by a program
named PX-Map. Looking in the CD ROM I see files with extension .shp 
which I guess contain
information about boundaries or shapes.

Does any one know if there is a way to use this information from within 
R? I have been looking at
the maps package and the GRASS interface to no avail. Any help appreciated.

-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr?a y Estad?stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From sam.kemp2 at ntlworld.com  Mon Mar  1 13:06:50 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Mon, 01 Mar 2004 12:06:50 +0000
Subject: [R] dynamic linking
Message-ID: <4043275A.1080503@ntlworld.com>

Hi,

I want to set up a dynamic link between a library e.g. myLibrary.a and a 
C++ file myProgram.cc to use in R. Is this possible? If so how does one 
go about doing it?

Any help will be greatly appreciated.

Cheers,

Sam.



From ggrothendieck at myway.com  Mon Mar  1 13:12:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  1 Mar 2004 07:12:09 -0500 (EST)
Subject: [R] How to plot Histogram with frequence overlaid by distribution
	curve
Message-ID: <20040301121209.ACD6639A0@mprdmxin.myway.com>



histdata <- hist(samp,freq=TRUE,br=20)
curve(max(histdata$count)*dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)

---
Date:   Mon, 1 Mar 2004 17:18:12 +0800 
From:   <WeiQiang.Li at seagate.com>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] How to plot Histogram with frequence overlaid by distribution curve 

 
Hi,
I am facing the problem that I want to plot a histogram chart set
freq to true and overlay with normal or weibull or exponential distribution
curve.

The sample code is shown as below:
>samp<-c(-8.2262,-8.2262,-8.2262,-8.20209,-8.09294,-8.07321,-8.07321,
-8.07321,-8.07175,-8.04948,-8.04948,-8.04948,-8.03848,-8.03848,
-8.026,-7.92517,-7.92517,-7.77218,-7.62414,-7.62414,-7.62414,
-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.28924,
-7.28924,-6.78729,-6.25307)

>hist(samp,freq=TRUE,br=20)
>curve(dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)

In the chart created based on above command, curve scale is too small
compared to the freqeunce. My question here is how to adjust the scale of
distribution curve. Thanks !

Best Regards
WeiQiang Li



From Roger.Bivand at nhh.no  Mon Mar  1 13:15:59 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 1 Mar 2004 13:15:59 +0100 (CET)
Subject: [R] Maps from PX-Map files
In-Reply-To: <404319BE.7080306@bs.ehu.es>
Message-ID: <Pine.LNX.4.44.0403011304290.26543-100000@reclus.nhh.no>

On Mon, 1 Mar 2004, F. Tusell wrote:

> I have been handed a CD ROM with statistical data readable with a 
> program named PC-Axis. There
> is the possibility of creating custom maps to display the data. 
> Aparently, this is handled by a program
> named PX-Map. Looking in the CD ROM I see files with extension .shp 
> which I guess contain
> information about boundaries or shapes.

As I read one googled PX-MAP source:

http://www.ginorden.org/konf/getpaper.php?ID=71

it looks as though: "Map data are held in the shape format used by ESRI 
ArcView", so either the shapefiles or the maptools packages should be able 
to read the polygon boundaries. Matching the polygons to the data may be 
more difficult. Fuller details at Statistics Norway:

http://www.ssb.no/english/municipalities/software/pxmap/

from which Windows binaries can be downloaded.

> 
> Does any one know if there is a way to use this information from within 
> R? I have been looking at
> the maps package and the GRASS interface to no avail. Any help appreciated.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From tblackw at umich.edu  Mon Mar  1 14:03:43 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Mon, 1 Mar 2004 08:03:43 -0500 (EST)
Subject: [R] dynamic linking
In-Reply-To: <4043275A.1080503@ntlworld.com>
References: <4043275A.1080503@ntlworld.com>
Message-ID: <Pine.SOL.4.58.0403010802001.1057@millipede.gpcc.itd.umich.edu>

Sam  -

On CRAN, under "Manuals" I believe you will find one with a title
similar to "Writing R extensions".  The correct title will be given
in the R FAQ.  I think that's the one you want.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 1 Mar 2004, Samuel Kemp wrote:

> Hi,
>
> I want to set up a dynamic link between a library e.g. myLibrary.a and a
> C++ file myProgram.cc to use in R. Is this possible? If so how does one
> go about doing it?
>
> Any help will be greatly appreciated.
>
> Cheers,
>
> Sam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wildscop at yahoo.com  Mon Mar  1 14:25:38 2004
From: wildscop at yahoo.com (WilD KID)
Date: Mon, 1 Mar 2004 05:25:38 -0800 (PST)
Subject: [R] Nonparametric test of randomness (Run Test)
Message-ID: <20040301132538.18811.qmail@web11408.mail.yahoo.com>

Dear all,

Does R or S-plus or any of their packages provide
Non-parametric "Run test" (which tests whether a
sequence of numbers might be random or not)?  If yes,
i'd like a numerical illustration of this test.

Any response / help / comment / suggestion will be
greatly appreciated. Thanks in advance.



-------------------------------
Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training (ISRT)
University of Dhaka, Dhaka -1000, Bangladesh
-------------------------------

=====
____________________________

WilDKiD <wildscop at yahoo.com>



From coursol at mathens.u-psud.fr  Mon Mar  1 15:36:17 2004
From: coursol at mathens.u-psud.fr (coursol)
Date: Mon, 1 Mar 2004 15:36:17 +0100 (CET)
Subject: [R] Maps from PX-Map files
In-Reply-To: <404319BE.7080306@bs.ehu.es>
Message-ID: <Pine.LNX.4.31.0403011533260.5355-100000@pc00.mathens.u-psud.fr>



On Mon, 1 Mar 2004, F. Tusell wrote:

> I have been handed a CD ROM with statistical data readable with a
> program named PC-Axis. There
> is the possibility of creating custom maps to display the data.
> Aparently, this is handled by a program
> named PX-Map. Looking in the CD ROM I see files with extension .shp
> which I guess contain
> information about boundaries or shapes.
>
> Does any one know if there is a way to use this information from within
> R? I have been looking at
> the maps package and the GRASS interface to no avail. Any help appreciated.
>
> --
> Fernando TUSELL                                e-mail:
> Departamento de Econometría y Estadística           etptupaf at bs.ehu.es
> Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
> Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
> E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

If .shp are ESRI shape files, you can try the Rmap library.

Ref: http://www.maths.lancs.ac.uk/Software/Rmap/
     and
       http://shapelib.maptools.org
         or
      http://www.remotesensing.org/gdal/

Jean Coursol



From rolf at math.unb.ca  Mon Mar  1 14:43:09 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 1 Mar 2004 09:43:09 -0400 (AST)
Subject: [R] How to plot Histogram with frequence overlaid by distribution
	curve
Message-ID: <200403011343.i21Dh938017030@erdos.math.unb.ca>


In response to a posting from WeiQiang Li:

> Hi,
> I am facing the problem that I want to plot a histogram chart set
> freq to true and overlay with normal or weibull or exponential distribution
> curve.
> 
> The sample code is shown as below:
> >samp<-c(-8.2262,-8.2262,-8.2262,-8.20209,-8.09294,-8.07321,-8.07321,
> -8.07321,-8.07175,-8.04948,-8.04948,-8.04948,-8.03848,-8.03848,
> -8.026,-7.92517,-7.92517,-7.77218,-7.62414,-7.62414,-7.62414,
> -7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.28924,
> -7.28924,-6.78729,-6.25307)
> 
> >hist(samp,freq=TRUE,br=20)
> >curve(dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)
> 
> In the chart created based on above command, curve scale is too small
> compared to the freqeunce. My question here is how to adjust the scale of
> distribution curve. Thanks !
> 

Gabor Grothendieck wrote:

> histdata <- hist(samp,freq=TRUE,br=20)
> curve(max(histdata$count)*dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)

This is a perfectly correct answer to a question that should not have
been asked in the first place.  What WeiQiang Li proposes to do makes
no sense at all, and will simply confuse and mislead the viewer/reader.
If a histogram is to be overlaid with a density curve that histogram
should represent a density and hence should be plotted on the density
scale --- NOT on the frequency scale.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From andy_liaw at merck.com  Mon Mar  1 15:13:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Mar 2004 09:13:23 -0500
Subject: [R] Nonparametric test of randomness (Run Test)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF78C9@usrymx25.merck.com>

I don't know the literature at all, but intuitively, you would need an
ordering relation to be able to define `runs'.  How would one do that in
more than one dimension?

Cheers,
Andy

> From: WilD KID
> 
> Dear all,
> 
> Does R or S-plus or any of their packages provide
> Non-parametric "Run test" (which tests whether a
> sequence of numbers might be random or not)?  If yes,
> i'd like a numerical illustration of this test.
> 
> Any response / help / comment / suggestion will be
> greatly appreciated. Thanks in advance.
> 
> 
> 
> -------------------------------
> Mohammad Ehsanul Karim <wildscop at yahoo.com>
> Institute of Statistical Research and Training (ISRT)
> University of Dhaka, Dhaka -1000, Bangladesh
> -------------------------------
> 
> =====
> ____________________________
> 
> WilDKiD <wildscop at yahoo.com>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From spencer.graves at pdf.com  Mon Mar  1 16:18:31 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Mar 2004 07:18:31 -0800
Subject: [R] How to plot Histogram with frequence overlaid by distribution
	curve
In-Reply-To: <200403011343.i21Dh938017030@erdos.math.unb.ca>
References: <200403011343.i21Dh938017030@erdos.math.unb.ca>
Message-ID: <40435447.9090603@pdf.com>

?truehist in library(MASS)? 

library(MASS)
truehist(samp)
x <- seq(-8.5, -6, length=51)
lines(x, dnorm(x, mean(samp), sqrt(var(samp))))

      This just worked for me in both S-Plus 6.2 and R 1.8.1.
hope this helps.  spencer graves
Rolf Turner wrote:

>In response to a posting from WeiQiang Li:
>
>  
>
>>Hi,
>>I am facing the problem that I want to plot a histogram chart set
>>freq to true and overlay with normal or weibull or exponential distribution
>>curve.
>>
>>The sample code is shown as below:
>>    
>>
>>>samp<-c(-8.2262,-8.2262,-8.2262,-8.20209,-8.09294,-8.07321,-8.07321,
>>>      
>>>
>>-8.07321,-8.07175,-8.04948,-8.04948,-8.04948,-8.03848,-8.03848,
>>-8.026,-7.92517,-7.92517,-7.77218,-7.62414,-7.62414,-7.62414,
>>-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.28924,
>>-7.28924,-6.78729,-6.25307)
>>
>>    
>>
>>>hist(samp,freq=TRUE,br=20)
>>>curve(dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)
>>>      
>>>
>>In the chart created based on above command, curve scale is too small
>>compared to the freqeunce. My question here is how to adjust the scale of
>>distribution curve. Thanks !
>>
>>    
>>
>
>Gabor Grothendieck wrote:
>
>  
>
>>histdata <- hist(samp,freq=TRUE,br=20)
>>curve(max(histdata$count)*dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)
>>    
>>
>
>This is a perfectly correct answer to a question that should not have
>been asked in the first place.  What WeiQiang Li proposes to do makes
>no sense at all, and will simply confuse and mislead the viewer/reader.
>If a histogram is to be overlaid with a density curve that histogram
>should represent a density and hence should be plotted on the density
>scale --- NOT on the frequency scale.
>
>				cheers,
>
>					Rolf Turner
>					rolf at math.unb.ca
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From christoph.lehmann at gmx.ch  Mon Mar  1 16:40:20 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 01 Mar 2004 16:40:20 +0100
Subject: [R] pca scores for newdata
Message-ID: <1078155620.1105.22.camel@christophl>

Hi
I used princomp on a dataset x[!sub,]. How can I get the scores for
another dataset, say x[sub,]? I didn't succeed using predict()

thanks for a hint

cheers

christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From christoph.lehmann at gmx.ch  Mon Mar  1 17:11:32 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 01 Mar 2004 17:11:32 +0100
Subject: [R] PLS scores for newdata
Message-ID: <1078157492.1105.30.camel@christophl>

Hi

I used pls.pcr on a dataset x[!sub,]. How can I get the scores for
another dataset, say x[sub,]? I couldn't reproduce the scores for the
training set by multiplying the data with the loadings, even after first
scaling the data (scale()). pls.pcr only gives the XScores for the
training data.

many thanks for a hint

cheers

christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From ripley at stats.ox.ac.uk  Mon Mar  1 17:15:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Mar 2004 16:15:23 +0000 (GMT)
Subject: [R] pca scores for newdata
In-Reply-To: <1078155620.1105.22.camel@christophl>
Message-ID: <Pine.LNX.4.44.0403011612560.10978-100000@gannet.stats>

On Mon, 1 Mar 2004, Christoph Lehmann wrote:

> I used princomp on a dataset x[!sub,]. How can I get the scores for
> another dataset, say x[sub,]? I didn't succeed using predict()

What did you try?  Whatever it was, predict() is the correct idea:

> data(USArrests)
> pc.cr <- princomp(USArrests, cor = TRUE)
> predict(pc.cr, USArrests[1:10,])
             
                   Comp.1      Comp.2      Comp.3       Comp.4
  Alabama      0.98556588  1.13339238 -0.44426879  0.156267145
  Alaska       1.95013775  1.07321326  2.04000333 -0.438583440
  Arizona      1.76316354 -0.74595678  0.05478082 -0.834652924
  Arkansas    -0.14142029  1.11979678  0.11457369 -0.182810896
  California   2.52398013 -1.54293399  0.59855680 -0.341996478
  Colorado     1.51456286 -0.98755509  1.09500699  0.001464887
  Connecticut -1.35864746 -1.08892789 -0.64325757 -0.118469414
  Delaware     0.04770931 -0.32535892 -0.71863294 -0.881977637
  Florida      3.01304227  0.03922851 -0.57682949 -0.096284752
  Georgia      1.63928304  1.27894240 -0.34246008  1.076796812

based on the example on the help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schnitzlerj at gmx.de  Mon Mar  1 17:25:48 2004
From: schnitzlerj at gmx.de (Johannes Schnitzler)
Date: Mon, 1 Mar 2004 17:25:48 +0100 (MET)
Subject: [R] dbf write
Message-ID: <22745.1078158348@www15.gmx.net>

Dear all,

i'm looking for a possibility to save a data frame in dbase format without
using RODBC.

Has anybody a r-script  / package for this purpose?

Thank's in advance for your help.

Johannes



From Christoph.Scherber at uni-jena.de  Mon Mar  1 17:45:22 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Mon, 01 Mar 2004 17:45:22 +0100
Subject: [R] boxplot notches
Message-ID: <404368A2.4000506@uni-jena.de>

Dear list members,

Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are 
calculated? What do these notches represent exactly? I?d suppose they 
are Conficence Intervals for the median, but I?ve also been told they 
might show Least Significant Difference (LSD) equivalents.

I would very much appreciate any help from you.

Best regards
Chris.



From wildscop at yahoo.com  Mon Mar  1 17:54:48 2004
From: wildscop at yahoo.com (WilD KID)
Date: Mon, 1 Mar 2004 08:54:48 -0800 (PST)
Subject: [R] Nonparametric test of randomness [Run Test]
Message-ID: <20040301165448.1863.qmail@web11402.mail.yahoo.com>

Dear Andy,

Usual literature of runs:

Run: a run is defined to be a maximum subsequence of
like elements.[Mendenhall, Scheaffer, and Wackerly
(1986), Mathematical Statistics with Applications, 3rd
Ed., Duxbury Press, CA, Page 597]

Run: a sequence of 1 or more identical symbols that is
preceded and followed by either a different symbol or
no symbol at all.[Matre, Gilbreath (1983) Statistics
for Business nad Economics, revides ed, Business pub.
inc, texas, Page 527]. Gibbons(1971) also says
something like it.

My problem is -
1. which table of run test do i use? Using
distribution of R, or using R as a test statistic? or
both? Both are due to Swed, Eisenhart (1943) "tables
for testing randomness of grouping in a sequence of
alternatives".
2. Does R or S-plus or any of their packages provide
Non-parametric ordinary "Run test"?


Any response / help / comment / suggestion will be
greatly appreciated. Thanks in advance.

-------------------------------
Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training (ISRT)
University of Dhaka, Dhaka -1000, Bangladesh
-------------------------------

=====
____________________________

WilDKiD <wildscop at yahoo.com>



From Virgilio.Gomez at uv.es  Mon Mar  1 18:27:08 2004
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Mon, 01 Mar 2004 18:27:08 +0100
Subject: [R] Parallelizing  spatial computations
Message-ID: <1078162028.31675.69.camel@chomsky.estadi.uv.es>

Hi,

I have to run lots of simulations based on several spatial scan
statistics. Since the main overhead is due to a call to 'apply' , I 
was wondering whether it could be possible to parallelize this "loop".

I know there are a few packages for parallel computing in R but I don't
know whether it's worth trying them. Have anyone used these packages
before?

I have a linux machine with two 700-MHz processors and 256 Mb RAM. I
know it's not too much, but I hope to save some time by using both
processors.

Thank you in advance.

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35



From f0z6305 at labs.tamu.edu  Mon Mar  1 18:36:30 2004
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Mon, 1 Mar 2004 11:36:30 -0600
Subject: [R] How to calculate the derivative of matrix w.r.t another matrix?
Message-ID: <00a301c3ffb3$bea6fef0$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040301/c264bd05/attachment.pl

From hosking at watson.ibm.com  Mon Mar  1 18:40:40 2004
From: hosking at watson.ibm.com (J. R. M. Hosking)
Date: Mon, 01 Mar 2004 12:40:40 -0500
Subject: [R] PWM help
Message-ID: <40437598.5020205@watson.ibm.com>

On Wed, 25 Feb 2004, Jonathan Wang wrote:

 > I saw a Help e-mail related to MLE.  Does R have a probability weighted
 > method (PWM) estimator function?  I can't seem to find anything on PWM,
 > unless my eyes are playing trick on me.

R has no built-in facilities for PWMs, nor for L-moments (which are
generally preferable to PWMs -- see Hosking and Wallis, "Regional
Frequency Analysis", Cambridge Univ Press, 1997, sec. 2.4).

Here is an R/Splus implementation of a function for estimating
sample L-moments (routine SAMLMU from my LMOMENTS
Fortran package at http://lib.stat.cmu.edu/general/lmoments).
It has been lightly tested, but carries no guarantees of accuracy.

samlmu <- function (x, nmom = 4, sort.data = T)
{
    xok <- x[!is.na(x)]
    n <- length(xok)
    if (nmom <= 0) return(numeric(0))
    if (nmom <= 2) rnames <- paste("l", 1:nmom, sep = "_")
    else rnames <- c("l_1", "l_2", paste("t", 3:nmom, sep = "_"))
    lmom <- rep(NA, nmom)
    names(lmom) <- rnames
    if (n == 0) return(lmom)
    if (sort.data == T) xok <- sort(xok)
    nmom.actual <- min(nmom, n)
    lmom[1] <- mean(xok)
    if (nmom.actual == 1) return(lmom)
    temp <- seq(1-n, n-1, by = 2)
    p1 <- rep(1, n)
    p <- temp/(n-1)
    lmom[2] <- mean(xok * p)
    if (nmom.actual == 2) return(lmom)
    if (xok[1] == xok[n]) {
        warning("all data values equal")
        return(lmom)
    }
    for (j in 3:nmom.actual) {
        p2 <- p1
        p1 <- p
        p <- ((2*j-3)*temp*p1 - (j-2)*(n+j-2)*p2) / ((j-1)*(n-j+1))
        lmom[j] <- mean(xok * p)/lmom[2]
    }
    return(lmom)
}



J. R. M. Hosking
hosking at watson.ibm.com



From tlumley at u.washington.edu  Mon Mar  1 18:54:48 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 1 Mar 2004 09:54:48 -0800 (PST)
Subject: [R] boxplot notches
In-Reply-To: <404368A2.4000506@uni-jena.de>
References: <404368A2.4000506@uni-jena.de>
Message-ID: <Pine.A41.4.58.0403010944180.64642@homer07.u.washington.edu>

On Mon, 1 Mar 2004, Christoph Scherber wrote:

> Dear list members,
>
> Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are
> calculated? What do these notches represent exactly? I?d suppose they
> are Conficence Intervals for the median, but I?ve also been told they
> might show Least Significant Difference (LSD) equivalents.

The help page says that " If the notches of two plots do not overlap then
the medians are significantly different at the 5 percent level."

The only thing wrong with this is that it isn't true.  The code says that
the notches are +/- 1.58 IQR/sqrt(n), so I think the claimed confidence
level holds only for normal distribuitons with small amounts of
contamination.


	-thomas



From maechler at stat.math.ethz.ch  Mon Mar  1 19:38:13 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 1 Mar 2004 19:38:13 +0100
Subject: [R] Exactness of ppois
In-Reply-To: <16391.59047.650874.65973@gargle.gargle.HOWL>
References: <40069BCA.60109@uni-bayreuth.de>
	<16391.59047.650874.65973@gargle.gargle.HOWL>
Message-ID: <16451.33557.320020.312589@gargle.gargle.HOWL>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Fri, 16 Jan 2004 14:27:03 +0100 writes:

>>>>> "Matthias" == Matthias Kohl <Matthias.Kohl at uni-bayreuth.de>
>>>>>     on Thu, 15 Jan 2004 13:55:22 +0000 writes:

    Matthias> Hello, by checking the precision of a convolution
    Matthias> algorithm, we found the following "inexactness":
    Matthias> We work with R Version 1.8.1 (2003-11-21) on
    Matthias> Windows systems (NT, 2000, XP).

    Matthias> Try the code:

    Matthias> So for lambda=977.8 and x=1001 we get a distance
    Matthias> of about 5.2e-06.  (This inexactness seems to hold
    Matthias> for all lambda values greater than about 900.)

    Matthias> BUT, summing about 1000 terms of exactness around 1e-16,
    Matthias> we would expect an error of order 1e-13.

    Matthias> We suspect algorithm AS 239 to cause that flaw.

    MM> correct.   Namely, because

    MM> ppois(x, lambda, lower_tail, log_p) :=  
    MM> pgamma(lambda, x + 1, 1., !lower_tail, log_p)

    MM> and pgamma(x, alph, scale) uses AS 239, currently. 
    MM> So this thread is really about the precision of R's current pgamma().

    MM> In your example, (x = 977.8, alph = 1002, scale=1) and 
    MM> in pgamma.c,
    MM>   alphlimit = 1000;
    MM> and later

    MM> /* use a normal approximation if alph > alphlimit */
    MM>  if (alph > alphlimit) {
    MM>    pn1 = sqrt(alph) * 3. * (pow(x/alph, 1./3.) + 1. / (9. * alph) - 1.);
    MM>    return pnorm(pn1, 0., 1., lower_tail, log_p);
    MM>  }

    MM> So, we could conceivably 
    MM> improve the situation by increasing `alphlimit'.
   
    MM> Though, I don't see a real need for this (and it will cost CPU
    MM> cycles in these cases).

I've now investigated this in some detail.
As a consequence, I will substantially increas 'alphlimit' for
R-devel aka 1.9.0-to-bet,
from 1000. to probably 100'000.

    Matthias> Do you think this could cause other problems apart
    Matthias> from that admittedly extreme example?

    MM> no, I don't think.  Look at

    >> lam <- 977.8
    >> (p1 <- ppois(1001, lam))
    MM> [1] 0.77643705
    >> (p2 <- sum(dpois(0:1001, lam)))
    MM> [1] 0.77643187

    MM> Can you imagine a situation where this difference matters?

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Mon Mar  1 19:43:45 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 1 Mar 2004 19:43:45 +0100
Subject: [R] boxplot notches
In-Reply-To: <Pine.A41.4.58.0403010944180.64642@homer07.u.washington.edu>
References: <404368A2.4000506@uni-jena.de>
	<Pine.A41.4.58.0403010944180.64642@homer07.u.washington.edu>
Message-ID: <16451.33889.108292.454206@gargle.gargle.HOWL>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:

    TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
    >> Dear list members,
    >> 
    >> Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are
    >> calculated? What do these notches represent exactly? I?d suppose they
    >> are Conficence Intervals for the median, but I?ve also been told they
    >> might show Least Significant Difference (LSD) equivalents.

    TL> The help page says that 
    TL> " If the notches of two plots do not overlap then
    TL>   the medians are significantly different at the 5 percent level."

    TL> The only thing wrong with this is that it isn't true.
    TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
    TL> so I think the claimed confidence level holds only for
    TL> normal distribuitons with small amounts of contamination.

I think John Tukey's idea was that this formula (or just the fact of
using median and quartiles) is still often approximately correct
for quite a few kinds of moderate contaminations...

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ripley at stats.ox.ac.uk  Mon Mar  1 21:06:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Mar 2004 20:06:50 +0000 (GMT)
Subject: [R] boxplot notches
In-Reply-To: <16451.33889.108292.454206@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>

On Mon, 1 Mar 2004, Martin Maechler wrote:

> >>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
> >>>>>     on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:
> 
>     TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
>     >> Dear list members,
>     >> 
>     >> Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are
>     >> calculated? What do these notches represent exactly? I?d suppose they
>     >> are Conficence Intervals for the median, but I?ve also been told they
>     >> might show Least Significant Difference (LSD) equivalents.
> 
>     TL> The help page says that 
>     TL> " If the notches of two plots do not overlap then
>     TL>   the medians are significantly different at the 5 percent level."
> 
>     TL> The only thing wrong with this is that it isn't true.
>     TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
>     TL> so I think the claimed confidence level holds only for
>     TL> normal distribuitons with small amounts of contamination.
> 
> I think John Tukey's idea was that this formula (or just the fact of
> using median and quartiles) is still often approximately correct
> for quite a few kinds of moderate contaminations...

It may be approximately correct for the width of a CI (and when I checked 
it was only appproximately correct for a normal), but I would seriously 
doubt if it were approximately correct for a significance level of 5%.
Remember how fast the tails of the asymptotic normal distribution decay: a 
20% error turns 5% into 2%.

BTW, if there is a precise reference for this it would be good to add it
to boxplot.stats.Rd, as the confidence limits are unexplained there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Mon Mar  1 21:48:30 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 1 Mar 2004 16:48:30 -0400 (AST)
Subject: [R] How to plot Histogram with frequence overlaid by distribution
	curve
Message-ID: <200403012048.i21KmUCn007661@erdos.math.unb.ca>

Spencer Graves wrote:

> ?truehist in library(MASS)? 
> 
> library(MASS)
> truehist(samp)
> x <- seq(-8.5, -6, length=51)
> lines(x, dnorm(x, mean(samp), sqrt(var(samp))))

> This just worked for me in both S-Plus 6.2 and R 1.8.1.

	What's wrong with ``hist(samp,prob=TRUE)''?

> hope this helps.  spencer graves

	I wasn't aware that I was in need of any help.  I merely made
	a comment on a wrong-headed question that was addressed to
	this list.


				cheers,

					Rolf Turner



From apv at capital.net  Mon Mar  1 22:10:12 2004
From: apv at capital.net (Arend P. van der Veen)
Date: Mon, 01 Mar 2004 16:10:12 -0500
Subject: [R] RODBC
Message-ID: <1078175412.1906.27.camel@freebsd>

Hi,

I have installed RODBC on FreeBSD 4.9 and I am using the PostgreSQL ODBC
Driver that is distributed with unixODBC 2.2.8.  I can access ODBC Data
Sources from Open Office so I think that everything is properly
installed.  When I installed RODBC I had to set

export LIBS=-L/usr/local/lib

so that R could locate my ODBC manager.

I do not have any problems opening database connections and qurey
tables.  However, when I try and close a connection I get the following
warning:

> odbcClose(channel)
R.bin in free(): warning: chunk is already free

Is this something that I need to worry about ?

Thanks in advance,
Arend van der Veen



From maj at stats.waikato.ac.nz  Mon Mar  1 22:23:23 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 02 Mar 2004 10:23:23 +1300
Subject: [R] Scanning tab-separated numbers
Message-ID: <4043A9CB.8040403@stats.waikato.ac.nz>

I want to paste in the following numbers into a scan:

0.023	0.032	0.054	0.069	0.081	0.094
0.105	0.127	0.148	0.169	0.188	0.216

they are separated by tabs alone, unless my mailer has done something to 
the tabs.

Now have a look at this:

 > scan()
1: 0.0230.0320.0540.0690.0810.094
1: 0.1050.1270.1480.1690.1880.216
Error in scan() : "scan" expected a real, got 
"0.0230.0320.0540.0690.0810.094"
 > scan(sep="\t")
1: 0.0230.0320.0540.0690.0810.094
1: 0.1050.1270.1480.1690.1880.216
Error in scan(sep = "   ") : "scan" expected a real, got 
"0.0230.0320.0540.0690.0810.094"
 >

Platform: Windows XP   Release 1.8.1

I can't seem to scan in tab-separated numbers even when I try to tell R 
to expect that. (this may be related to the Sweave problem I mentioned a 
few days ago.)

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From sundar.dorai-raj at pdf.com  Mon Mar  1 22:39:22 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 01 Mar 2004 15:39:22 -0600
Subject: [R] Scanning tab-separated numbers
In-Reply-To: <4043A9CB.8040403@stats.waikato.ac.nz>
References: <4043A9CB.8040403@stats.waikato.ac.nz>
Message-ID: <4043AD8A.9010801@pdf.com>



Murray Jorgensen wrote:
> I want to paste in the following numbers into a scan:
> 
> 0.023    0.032    0.054    0.069    0.081    0.094
> 0.105    0.127    0.148    0.169    0.188    0.216
> 
> they are separated by tabs alone, unless my mailer has done something to 
> the tabs.
> 
> Now have a look at this:
> 
>  > scan()
> 1: 0.0230.0320.0540.0690.0810.094
> 1: 0.1050.1270.1480.1690.1880.216
> Error in scan() : "scan" expected a real, got 
> "0.0230.0320.0540.0690.0810.094"
>  > scan(sep="\t")
> 1: 0.0230.0320.0540.0690.0810.094
> 1: 0.1050.1270.1480.1690.1880.216
> Error in scan(sep = "   ") : "scan" expected a real, got 
> "0.0230.0320.0540.0690.0810.094"
>  >
> 
> Platform: Windows XP   Release 1.8.1
> 
> I can't seem to scan in tab-separated numbers even when I try to tell R 
> to expect that. (this may be related to the Sweave problem I mentioned a 
> few days ago.)
> 
> Murray
> 

If you save the numbers to a file (say "x.txt") then

scan("x.txt", sep = "\t")

should do it. Or highlight your text, copy to the clipboard ("Ctrl-C"), 
and try:

scan("clipboard", sep = "\t")

Read the help for ?scan.

-sundar



From rbaer at atsu.edu  Mon Mar  1 22:41:53 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Mon, 1 Mar 2004 15:41:53 -0600
Subject: [R] Scanning tab-separated numbers
References: <4043A9CB.8040403@stats.waikato.ac.nz>
Message-ID: <007301c3ffd6$0326bb20$2e80010a@BigBaer>

The problem would seem to be that the Windows GUI does not accept the TAB
key as input.  There is no whitespace between your values to be recongnized.
Take your sequesnce and save it as a tab separated file in Excel or through
Notepad, and you will be able to scan it in just fine using
scan("savedfile.txt")

I am not sure whether this behavior of the TAB key in the interface is
intentional or not, but it is the same on my Windows 2000 machine, yet the
TAB separated file reads in fine with scan().

Rob


----- Original Message ----- 
From: "Murray Jorgensen" <maj at stats.waikato.ac.nz>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Monday, March 01, 2004 3:23 PM
Subject: [R] Scanning tab-separated numbers


> I want to paste in the following numbers into a scan:
>
> 0.023 0.032 0.054 0.069 0.081 0.094
> 0.105 0.127 0.148 0.169 0.188 0.216
>
> they are separated by tabs alone, unless my mailer has done something to
> the tabs.
>
> Now have a look at this:
>
>  > scan()
> 1: 0.0230.0320.0540.0690.0810.094
> 1: 0.1050.1270.1480.1690.1880.216
> Error in scan() : "scan" expected a real, got
> "0.0230.0320.0540.0690.0810.094"
>  > scan(sep="\t")
> 1: 0.0230.0320.0540.0690.0810.094
> 1: 0.1050.1270.1480.1690.1880.216
> Error in scan(sep = "   ") : "scan" expected a real, got
> "0.0230.0320.0540.0690.0810.094"
>  >
>
> Platform: Windows XP   Release 1.8.1
>
> I can't seem to scan in tab-separated numbers even when I try to tell R
> to expect that. (this may be related to the Sweave problem I mentioned a
> few days ago.)
>
> Murray
>
> -- 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From w.vervoort at acss.usyd.edu.au  Mon Mar  1 23:28:51 2004
From: w.vervoort at acss.usyd.edu.au (Willem Vervoort)
Date: Tue, 02 Mar 2004 09:28:51 +1100
Subject: [R] non-negative least-squares
Message-ID: <4043B923.6030304@acss.usyd.edu.au>

Hi all,
I am trying to do an inversion of electromagnetic data with non-negative 
least squares method (Tikhonov regularisation) and have got it 
programmed in S-Plus. However I am trying to move all my scripts from 
S-Plus to R.

Is there an equivalent to nnls.fit in R?
I think this can be done with pcls? Right?

S-Plus script: A, L and data are matrices, lambda is a vector of 
possible lambda (smoothing) values

> "nntik"<-function(A,L,data,lambda)
> {
> H<-rbind(A,lambda*L)
> i<-1:(nrow(L)+length(data))
> q<-ifelse(i<=length(data),data[i],0)
> nntik<-nnls.fit(H,q)
> return(nntik)
> }

I think this is the same as what pcls states:


Willem

-- 
Dr R.W. Vervoort
McCaughey Senior Lecturer Hydrology and Catchment Management			
Faculty of Agriculture, Food and Natural Resources
Rm 503, Watt Bldg.
http://www.agric.usyd.edu.au/mccaughey

Postal:
Bldg A03
The University of Sydney, NSW, 2006

phone: +61 (2) 9351 8744
fax:   +61 (2) 9351 5108
e-mail: w.vervoort at acss.usyd.edu.au



From dj at research.bell-labs.com  Mon Mar  1 23:29:41 2004
From: dj at research.bell-labs.com (David James)
Date: Mon, 1 Mar 2004 17:29:41 -0500
Subject: [R] boxplot notches
In-Reply-To: <Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>;
	from ripley@stats.ox.ac.uk on Mon, Mar 01, 2004 at 08:06:50PM
	+0000
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
Message-ID: <20040301172941.A115@jessie.research.bell-labs.com>

Prof Brian Ripley wrote:
> On Mon, 1 Mar 2004, Martin Maechler wrote:
> 
> > >>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
> > >>>>>     on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:
> > 
> >     TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
> >     >> Dear list members,
> >     >> 
> >     >> Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are
> >     >> calculated? What do these notches represent exactly? I?d suppose they
> >     >> are Conficence Intervals for the median, but I?ve also been told they
> >     >> might show Least Significant Difference (LSD) equivalents.
> > 
> >     TL> The help page says that 
> >     TL> " If the notches of two plots do not overlap then
> >     TL>   the medians are significantly different at the 5 percent level."
> > 
> >     TL> The only thing wrong with this is that it isn't true.
> >     TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
> >     TL> so I think the claimed confidence level holds only for
> >     TL> normal distribuitons with small amounts of contamination.
> > 
> > I think John Tukey's idea was that this formula (or just the fact of
> > using median and quartiles) is still often approximately correct
> > for quite a few kinds of moderate contaminations...
> 
> It may be approximately correct for the width of a CI (and when I checked 
> it was only appproximately correct for a normal), but I would seriously 
> doubt if it were approximately correct for a significance level of 5%.
> Remember how fast the tails of the asymptotic normal distribution decay: a 
> 20% error turns 5% into 2%.
> 
> BTW, if there is a precise reference for this it would be good to add it
> to boxplot.stats.Rd, as the confidence limits are unexplained there.

@article{McGi:Tuke:Lars:1978,
author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
title = {Variations of {B}ox plots},
year = {1978},
journal = {The American Statistician},
volume = {32},
pages = {12--16},
keywords = {Exploratory data analysis; Graphics}
}

@book{Cham:Clev:Klei:Tuke:1983,
author = {Chambers, John M. and Cleveland, William S. and Kleiner, Beat
and Tukey, Paul A.},
title = {Graphical methods for data analysis},
year = {1983},
pages = {395},
publisher = {Wadsworth Publishing Co Inc}
}

> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jhallman at frb.gov  Mon Mar  1 23:29:29 2004
From: jhallman at frb.gov (jhallman@frb.gov)
Date: Mon, 01 Mar 2004 17:29:29 -0500
Subject: [R] dev.print and X11(canvas = "black")
Message-ID: <20040301222929.E6B5EAEB17@mail.rsma.frb.gov>

In Splus, I often use graphics windows with a black background and white
foreground.  The S print.graph() function sends the current plot to my
printer but with a white background and black foreground.  I'd like to
be able to do something similar in R, but can't figure out how.  I've
tried various permutations of dev.copy() and dev.print(), but it seems
that the foreground color is fixed when the original plot is drawn, and
dev.copy always redraws it with that color.  So even though I can get my
postscript output with a white background, things that were drawn white on the
dark background of my X11 device are still drawn white when dev.copy'd
to a postscript device.  Since the postscript has a white background,
that means the white foreground stuff doesn't show up.

Is there a way to handle this?  Or do I just have to give up on ever
using a dark canvas in X11() if I ever want to print the result?

Please reply directly.

Jeff



From schoenle at fas.harvard.edu  Mon Mar  1 23:35:38 2004
From: schoenle at fas.harvard.edu (Raphael Schoenle)
Date: Mon, 1 Mar 2004 17:35:38 -0500
Subject: [R] superimposing two scatterplots
Message-ID: <000801c3ffdd$85922a70$268ff78c@acer4jbjp1qwlp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040301/1f9758a0/attachment.pl

From abunn at montana.edu  Tue Mar  2 00:06:18 2004
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 1 Mar 2004 16:06:18 -0700
Subject: [R] superimposing two scatterplots
In-Reply-To: <000801c3ffdd$85922a70$268ff78c@acer4jbjp1qwlp>
Message-ID: <000001c3ffe1$d0f7a180$78f05a99@msu.montana.edu>

?points

     plot(-4:4, -4:4, type = "n")# setting up coord. system
     points(rnorm(200), rnorm(200), col = "red")
     points(rnorm(100)/2, rnorm(100)/2, col = "blue", cex = 1.5)

> -----Original Message-----
> From: r-help-bounces+abunn=montana.edu at stat.math.ethz.ch 
> [mailto:r-help-bounces+abunn=montana.edu at stat.math.ethz.ch] 
> On Behalf Of Raphael Schoenle
> Sent: Monday, March 01, 2004 3:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] superimposing two scatterplots
> 
> 
> Hi,
>  
> How can I plot two scatterplots on the same, one panel?
> I have two times series (price data for two goods) for the 
> same period.
>  
> Many thanks,
>  
> -R
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rxg218 at psu.edu  Tue Mar  2 01:00:24 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 01 Mar 2004 19:00:24 -0500
Subject: [R] some question regarding random forest
Message-ID: <1078185624.28603.44.camel@ra.chem.psu.edu>

Hi,
  I had two questions regarding random forests for regression.

1) I have read the original paper by Breiman as well as a paper
dicussing an application of random forests and it appears that the one
of the nice features of this technique is good predictive ability.

However I have some data with which I have generated a linear model
using lm(). I can get an RMS error of 0.43 and an R^2 of 0.62. 
However when I make a plot of predicted versus observed using the
randomForest() function the plot is much more scattered (RMS error of
0.55 and R^2 of 0.33) than for a similar plot using the linear model.
(When a test set is supplied to the models the R^2 values are close).

My question is: should I expect the randomForest to give me similar or
better results than a simple linear model? In the above case I was
expecting that for the training data (ie the data with which the random
forest was built) I would get less scatter in the plot and a lower RMSE.
(I realize that too much stock should'nt be placed in R^2).

The papers note that overfitting is not a problem with random forests
and so I was wondering what I could do to improve the results  -I've
tried playing with the number of trees and the value of m_try but I dont
see much change.

Is there anything that I can do to improve the results for a random
forest model? (Are there any signifcant papers, apart from Breiman, that
I should be reading related to random forests?)

2) My second question is related to interpretation of the variable
importance plot using var.imp.plot(). I realise that the variables are
ordered in order of decreasing importance. However for example I see
that there is a large decrease in the value of Importance from the first
variable (ie most important) to the second one. Whereas for other pairs
the difference in the Importance value is not so large.

Is the difference between the Importance value a measure of 'how much
more important' a variable is? Or am I going in the wrong direction?

In additionn, is there any sort of rule or heuristic that can be used to
say for example that the first N variables account for the model? Or is
the interpretation of variable importance descriptive in nature?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Science kind of takes the fun out of the portent business.
-Hobbes



From andy_liaw at merck.com  Tue Mar  2 02:00:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Mar 2004 20:00:06 -0500
Subject: [R] some question regarding random forest
Message-ID: <3A822319EB35174CA3714066D590DCD504AF78D0@usrymx25.merck.com>

Rajarshi,

1. You need to be aware that the predicted values returned by randomForest
for the training data are _not_ really model prediction for the training
data.  They are the `out-of-bag' predictions; i.e., each predicted value is
based on only about 36% of the trees in the forest (i.e., when the data
point is left out of the bootstrap sample, thus can be predicted
`honestly').  The out-of-bag prediction provides a convenient `honest'
estimate of prediction error, without resorting to cross-validation or
another layer of bootstrap.  Thus the MSE and R^2 you get from the OOB
prediction will be similar to what you would get out of an independent test
set, or cross-validation.  

If you really want prediction on the training data using _all_ trees in
forest, you need to use predict(), and supply the training data.  You will
see the over-optimistic prediction that way.  (For classification, this
usually gives perfect prediction!)

Increasing number of trees beyond, say, a few hundred, is unlikely to
increase performance.  I usually only do that to get more stable estimate of
variable importance.  RF is relatively resistant to parameter tuning.  I do
not consider number of trees as a tuning parameter, as the theory says the
generalization error converges as the number of trees goes to infinity.
Some people had found that changing nodesize can lead to different
performance.  You may want to check out the tune() function in the package
e1071, which can be used tune randomForest.

Prof. Breiman is working on a paper for Statistical Science.  Stay tuned for
that.

For #2, I don't understand what you mean by `the other pair'.  There are
many ways to measure `variable importance', and they can have very different
interpretations.  The particular ones implemented in randomForest are
explained in the help page, as well as the `manual' Prof. Breiman provided
on his web site (cited in help page).

HTH,
Andy

> From: Rajarshi Guha
> 
> Hi,
>   I had two questions regarding random forests for regression.
> 
> 1) I have read the original paper by Breiman as well as a paper
> dicussing an application of random forests and it appears that the one
> of the nice features of this technique is good predictive ability.
> 
> However I have some data with which I have generated a linear model
> using lm(). I can get an RMS error of 0.43 and an R^2 of 0.62. 
> However when I make a plot of predicted versus observed using the
> randomForest() function the plot is much more scattered (RMS error of
> 0.55 and R^2 of 0.33) than for a similar plot using the linear model.
> (When a test set is supplied to the models the R^2 values are close).
> 
> My question is: should I expect the randomForest to give me similar or
> better results than a simple linear model? In the above case I was
> expecting that for the training data (ie the data with which 
> the random
> forest was built) I would get less scatter in the plot and a 
> lower RMSE.
> (I realize that too much stock should'nt be placed in R^2).
> 
> The papers note that overfitting is not a problem with random forests
> and so I was wondering what I could do to improve the results  -I've
> tried playing with the number of trees and the value of m_try 
> but I dont
> see much change.
> 
> Is there anything that I can do to improve the results for a random
> forest model? (Are there any signifcant papers, apart from 
> Breiman, that
> I should be reading related to random forests?)
> 
> 2) My second question is related to interpretation of the variable
> importance plot using var.imp.plot(). I realise that the variables are
> ordered in order of decreasing importance. However for example I see
> that there is a large decrease in the value of Importance 
> from the first
> variable (ie most important) to the second one. Whereas for 
> other pairs
> the difference in the Importance value is not so large.
> 
> Is the difference between the Importance value a measure of 'how much
> more important' a variable is? Or am I going in the wrong direction?
> 
> In additionn, is there any sort of rule or heuristic that can 
> be used to
> say for example that the first N variables account for the 
> model? Or is
> the interpretation of variable importance descriptive in nature?
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Science kind of takes the fun out of the portent business.
> -Hobbes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From dmurdoch at pair.com  Tue Mar  2 02:45:20 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 01 Mar 2004 20:45:20 -0500
Subject: [R] How to plot Histogram with frequence overlaid by distribution
	curve
In-Reply-To: <200403011343.i21Dh938017030@erdos.math.unb.ca>
References: <200403011343.i21Dh938017030@erdos.math.unb.ca>
Message-ID: <mfp740h46q6qv7undub5v2262knclgdbfp@4ax.com>

On Mon, 1 Mar 2004 09:43:09 -0400 (AST), you wrote:
>What WeiQiang Li proposes to do makes
>no sense at all, and will simply confuse and mislead the viewer/reader.
>If a histogram is to be overlaid with a density curve that histogram
>should represent a density and hence should be plotted on the density
>scale --- NOT on the frequency scale.

They should be plotted on the same scale, but why require the density
scale?  I've never seen anyone plot a density with the units shown
(e.g. m^{-1} if the variable measures a distance); this makes me think
that nobody really reads density plots in a quantitative way.  They're
read like likelihoods: the scale doesn't matter, only relative heights
matter.

Duncan Murdoch



From bitwrit at ozemail.com.au  Tue Mar  2 03:29:34 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 2 Mar 2004 13:29:34 +1100
Subject: [R] Nonparametric test of randomness (Run Test)
Message-ID: <20040302022032.RAIY17006.smta08.mail.ozemail.net@there>

Hi Mohammad,

There an implementation of Knuth's run test in the pLab programs. While these 
are apparently not being maintained anymore, there is a good description of 
the code here:

http://random.mat.sbg.ac.at/team/#software

Download this file - the Mathematica code is on page 65.

 Leeb, H. and Lendl, O.: pLab -- Library reference, Version 1.0. Report No. 
4, pLab -- reports, University of Salzburg, 1997. Abstract and compressed 
postscript file available . 

Jim



From vograno at evafunds.com  Tue Mar  2 07:39:38 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 1 Mar 2004 22:39:38 -0800
Subject: [R] passing a string from .C()
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3A80@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040301/95511076/attachment.pl

From ripley at stats.ox.ac.uk  Tue Mar  2 08:06:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 07:06:39 +0000 (GMT)
Subject: [R] RODBC
In-Reply-To: <1078175412.1906.27.camel@freebsd>
Message-ID: <Pine.LNX.4.44.0403020703470.12356-100000@gannet.stats>

On Mon, 1 Mar 2004, Arend P. van der Veen wrote:

> Hi,
> 
> I have installed RODBC on FreeBSD 4.9 and I am using the PostgreSQL ODBC
> Driver that is distributed with unixODBC 2.2.8.  I can access ODBC Data
> Sources from Open Office so I think that everything is properly
> installed.  When I installed RODBC I had to set
> 
> export LIBS=-L/usr/local/lib
> 
> so that R could locate my ODBC manager.
> 
> I do not have any problems opening database connections and qurey
> tables.  However, when I try and close a connection I get the following
> warning:
> 
> > odbcClose(channel)
> R.bin in free(): warning: chunk is already free
> 
> Is this something that I need to worry about ?

Yes.  We've seen problems with that PostgreSQL driver, and I think you
need to use the one that is distributed by the psqlodbc project (as
described in the RODBC README file!).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar  2 08:15:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 07:15:36 +0000 (GMT)
Subject: [R] dev.print and X11(canvas = "black")
In-Reply-To: <20040301222929.E6B5EAEB17@mail.rsma.frb.gov>
Message-ID: <Pine.LNX.4.44.0403020710471.12356-100000@gannet.stats>

On Mon, 1 Mar 2004 jhallman at frb.gov wrote:

> In Splus, I often use graphics windows with a black background and white
> foreground.  The S print.graph() function sends the current plot to my
> printer but with a white background and black foreground.  I'd like to

S (and S-PLUS) only has a few colour numbers, and you can change their 
interpretation on the screen graphics devices after plotting.  R does plot 
in colours rather than numbers.

> be able to do something similar in R, but can't figure out how.  I've
> tried various permutations of dev.copy() and dev.print(), but it seems
> that the foreground color is fixed when the original plot is drawn, and
> dev.copy always redraws it with that color.  So even though I can get my
> postscript output with a white background, things that were drawn white on the
> dark background of my X11 device are still drawn white when dev.copy'd
> to a postscript device.  Since the postscript has a white background,
> that means the white foreground stuff doesn't show up.
> 
> Is there a way to handle this?  Or do I just have to give up on ever
> using a dark canvas in X11() if I ever want to print the result?

There are many ways to print the result, and re-running the plot commands 
after switching to a postscript() device is one of the best.  This would 
work for you if you only ever use colour numbers and switch the palette
(and par("fg")).

> Please reply directly.
> 
> Jeff

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar  2 09:10:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 08:10:53 +0000 (GMT)
Subject: [R] Scanning tab-separated numbers
In-Reply-To: <007301c3ffd6$0326bb20$2e80010a@BigBaer>
Message-ID: <Pine.LNX.4.44.0403020802270.16572-100000@gannet.stats>

On Mon, 1 Mar 2004, Robert W. Baer, Ph.D. wrote:

> The problem would seem to be that the Windows GUI does not accept the TAB
> key as input.  There is no whitespace between your values to be recongnized.
> Take your sequesnce and save it as a tab separated file in Excel or through
> Notepad, and you will be able to scan it in just fine using
> scan("savedfile.txt")
> 
> I am not sure whether this behavior of the TAB key in the interface is
> intentional or not, but it is the same on my Windows 2000 machine, yet the
> TAB separated file reads in fine with scan().

It does seem to be intentional: all ctrl characters (those <= 0x1f) are
reserved for controlling the console.  There is no implementation of tabs
in RGui, so TAB = ^I is currently ignored (along with ^G ^Q ^R ^S).

Note that pasting tabs into the Unix R console does not work either (tab
is used for completion in readline), and generally you should not expect
to be able to paste any control characters into a console (even inside
quotes).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ali at hmg.inpg.fr  Tue Mar  2 09:11:56 2004
From: ali at hmg.inpg.fr (Abdou Ali)
Date: Tue, 2 Mar 2004 09:11:56 +0100
Subject: [R] filled contour
In-Reply-To: <Pine.A41.4.53.0403011351120.47962@lgit1.obs.ujf-grenoble.fr>
References: <Pine.A41.4.53.0403011351120.47962@lgit1.obs.ujf-grenoble.fr>
Message-ID: <20040302081154.CFA81334A5@ltheln11.hmg.inpg.fr>

Hello,
how can I fill map contour with R without setting the legend. I cannot see an 
option to skip the legend for filled.contour.
Thank for your response.



From ripley at stats.ox.ac.uk  Tue Mar  2 09:49:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 08:49:18 +0000 (GMT)
Subject: [R] passing a string from .C()
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3A80@phost015.intermedia.net>
Message-ID: <Pine.LNX.4.44.0403020827290.16652-100000@gannet.stats>

On Mon, 1 Mar 2004, Vadim Ogranovich wrote:

> Could someone please point to an example of passing strings from .C()
> calls back to R? 

There are not many, as such things are better done using .Call.

> I want to be able to do something like this:
>  
> str <- .C("return_foo_string", str=character(1))$str
>  
> void return_foo_string(char ** str) {
>     *str = "foo";
> }
>  
> The above code has at least two memory allocation "concerns": 
> 1) How to properly allocate "foo". I should probably use R_alloc, e.g.
>  
> char foo[] = "foo";
> *str = R_alloc(sizeof(foo), 1);

That would suffice.  Most uses I see in R addons either use a static
buffer or preallocate enough space in the character vector passed in to .C
(e.g. function tghyper in package SuppDists).


> 2) I don't know if the string pointed to by *str before the
> re-assignment, which now becomes dangling, will be properly reclaimed.

You need to consult the source code (here src/main/dotcode.c).  
The R character vector is copied to a **char construct on the way in, and
the **char construct is copied back to an R character vector on the way
back.  All the allocation is done by R_alloc, and its stack is reset as
do_dotcode is left.  So storage is reclaimed at the next garbage
collection after the .C call.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bob.ohara at helsinki.fi  Tue Mar  2 09:55:14 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Tue, 02 Mar 2004 10:55:14 +0200
Subject: [R] Problem with Integrate
Message-ID: <40444BF2.9050900@helsinki.fi>

The background: I'm trying to fit a Poisson-lognormal distrbutuion to 
some data.  This is a way of modelling species abundances:
N ~ Pois(lam)
log(lam) ~ N(mu, sigma2)
The number of individuals are Poisson distributed with an abundance 
drawn from a log-normal distrbution.

To fit this to data, I need to integrate out lam.  In principle, I can 
do it this way:

PLN1 <- function(lam, Count, mu, sigma2) {
   dpois(Count, exp(lam), log=F)*dnorm(LL, mu, sqrt(sigma2))
}

and integrate between -Inf and Inf.  For example, with mu=2, and 
sigma2=2.8 (which are roughly right for the data), and Count=73, I get this:

 > integrate(PLN1, -10, 10, Count=73, mu=2, sigma2=2.8)
0.001289726 with absolute error < 2.5e-11
 > integrate(PLN1, -20, 20, Count=73, mu=2, sigma2=2.8)
0.001289726 with absolute error < 2.5e-11
 > integrate(PLN1, -100, 100, Count=73, mu=2, sigma2=2.8)
2.724483e-10 with absolute error < 5.3e-10
 > integrate(PLN1, -500, 500, Count=73, mu=2, sigma2=2.8)
1.831093e-73 with absolute error < 3.6e-73
 > integrate(PLN1, -1000, 1000, Count=73, mu=2, sigma2=2.8)
Error in integrate(PLN1, -1000, 1000, Count = 73, mu = 2, sigma2 = 2.8):
         non-finite function value
In addition: Warning message:
NaNs produced in: dpois(x, lambda, log)

So, the integral gets smaller, and then gives an error.

I then tried entering the formula directly:
PLN2 <- function(LL, Count, mu, sigma2) {
exp(-LL-(log(LL)-mu)^2/(2*sigma2))*LL^(Count-1)/(gamma(Count+1)*sqrt(2*pi*sigma2))
}

 > integrate(PLN2, 0, 100, Count=73, mu=2, sigma2=2.8)
0.001287821 with absolute error < 2.6e-10
 > integrate(PLN2, 0, 1000, Count=73, mu=2, sigma2=2.8)
0.001289726 with absolute error < 2.9e-08
 > integrate(PLN2, 0, 10000, Count=73, mu=2, sigma2=2.8)
0.001289726 with absolute error < 9.7e-06
 > integrate(PLN2, 0, 19100, Count=73, mu=2, sigma2=2.8)
1.160307e-08 with absolute error < 2.3e-08
 > integrate(PLN2, 0, 19200, Count=73, mu=2, sigma2=2.8)
Error in integrate(PLN2, 0, 19200, Count = 73, mu = 2, sigma2 = 2.8) :
         non-finite function value

And the same thing happens.

I assume that this is because for much of the range, the integral is 
basically zero.

Can anyone suggest a fix?  Preferably one that will work with Count=320 
and Count=0 (both of which I have in the data).

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From jean-noel.candau at avignon.inra.fr  Tue Mar  2 10:10:42 2004
From: jean-noel.candau at avignon.inra.fr (Jean-Noel)
Date: Tue, 2 Mar 2004 10:10:42 +0100
Subject: [R] Import range of cells from Excel
Message-ID: <GOENJEALPPDFMBOMCKOJKEKGCBAA.jean-noel.candau@avignon.inra.fr>

Dear all,
I would like to import a range of cells (e.g. F10:K234) from an Excel
worksheet to R. I have looked for documentation on RODBC and RDCOMClient but
I was not able to find enough information to solve my problem and all the
examples I have seen were dealing with an entire worksheet, not a range of
cells.
Thanks,

Jean-Noel

Jean-Noel Candau
INRA - Unit? de Recherches Foresti?res M?diterran?ennes
Avenue A. Vivaldi
84000 AVIGNON
Tel: (33) 4 90 13 59 22
Fax: (33) 4 90 13 59 59



From simon at stats.gla.ac.uk  Tue Mar  2 10:20:49 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 2 Mar 2004 09:20:49 +0000 (GMT)
Subject: [R] non-negative least-squares
In-Reply-To: <4043B923.6030304@acss.usyd.edu.au>
References: <4043B923.6030304@acss.usyd.edu.au>
Message-ID: <Pine.SOL.4.58.0403020911570.14754@moon.stats.gla.ac.uk>

I think pcls will do what you want. You'll need to explicitly set up the
constraint matrix, since pcls is designed to deal with general linear
inequality constraints (i.e. it will do more than just non-negativity).
Also the penalty matrix/matrices for pcls would be t(L)%*%L, I think.

btw, there's a pre-release version of mgcv 1.0 at

http://www.stats.gla.ac.uk/~simon/simon/mgcv.html

which has a less obscure way of supplying the penalty matrices to pcls,
and might therefore be a bit easier to use. (Also features gamms, tensor
product smoothing, p-splines and stuff.)

best,
Simon

> Is there an equivalent to nnls.fit in R?
> I think this can be done with pcls? Right?
>
> S-Plus script: A, L and data are matrices, lambda is a vector of
> possible lambda (smoothing) values
>
> > "nntik"<-function(A,L,data,lambda)
> > {
> > H<-rbind(A,lambda*L)
> > i<-1:(nrow(L)+length(data))
> > q<-ifelse(i<=length(data),data[i],0)
> > nntik<-nnls.fit(H,q)
> > return(nntik)
> > }
>
> I think this is the same as what pcls states:
>
>
> Willem
>
> --
> Dr R.W. Vervoort
> McCaughey Senior Lecturer Hydrology and Catchment Management
> Faculty of Agriculture, Food and Natural Resources
> Rm 503, Watt Bldg.
> http://www.agric.usyd.edu.au/mccaughey
>
> Postal:
> Bldg A03
> The University of Sydney, NSW, 2006
>
> phone: +61 (2) 9351 8744
> fax:   +61 (2) 9351 5108
> e-mail: w.vervoort at acss.usyd.edu.au
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Christoph.Scherber at uni-jena.de  Tue Mar  2 11:18:19 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 02 Mar 2004 11:18:19 +0100
Subject: [R] boxplot notches
In-Reply-To: <20040301172941.A115@jessie.research.bell-labs.com>
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
	<20040301172941.A115@jessie.research.bell-labs.com>
Message-ID: <40445F6B.4020901@uni-jena.de>

Dear colleagues,

I think it would be a good idea to include a short note in the R 
boxplot() help file, stating exactly how the confidence levels are 
calculated
("the notches are +/- 1.58 IQR/sqrt(n)")  - at least as a guidance for 
users not advanced enough to directly interpret the code.

Would this be possible?

Regards,
Christoph.

David James wrote:

> Prof Brian Ripley wrote:
>
>> On Mon, 1 Mar 2004, Martin Maechler wrote:
>>
>>>>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>>>> on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:
>>>>>>>
>>> TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
>>> >> Dear list members,
>>> >>
>>> >> Can anyone tell me how the notches in boxplot(Y~X,notch=T) are
>>> >> calculated? What do these notches represent exactly? I?d suppose they
>>> >> are Conficence Intervals for the median, but I?ve also been told they
>>> >> might show Least Significant Difference (LSD) equivalents.
>>>
>>> TL> The help page says that
>>> TL> " If the notches of two plots do not overlap then
>>> TL> the medians are significantly different at the 5 percent level."
>>>
>>> TL> The only thing wrong with this is that it isn't true.
>>> TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
>>> TL> so I think the claimed confidence level holds only for
>>> TL> normal distribuitons with small amounts of contamination.
>>>
>>> I think John Tukey's idea was that this formula (or just the fact of
>>> using median and quartiles) is still often approximately correct
>>> for quite a few kinds of moderate contaminations...
>>
>> It may be approximately correct for the width of a CI (and when I 
>> checked
>> it was only appproximately correct for a normal), but I would seriously
>> doubt if it were approximately correct for a significance level of 5%.
>> Remember how fast the tails of the asymptotic normal distribution 
>> decay: a
>> 20% error turns 5% into 2%.
>>
>> BTW, if there is a precise reference for this it would be good to add it
>> to boxplot.stats.Rd, as the confidence limits are unexplained there.
>
>
> @article{McGi:Tuke:Lars:1978,
> author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
> title = {Variations of {B}ox plots},
> year = {1978},
> journal = {The American Statistician},
> volume = {32},
> pages = {12--16},
> keywords = {Exploratory data analysis; Graphics}
> }
>
> @book{Cham:Clev:Klei:Tuke:1983,
> author = {Chambers, John M. and Cleveland, William S. and Kleiner, Beat
> and Tukey, Paul A.},
> title = {Graphical methods for data analysis},
> year = {1983},
> pages = {395},
> publisher = {Wadsworth Publishing Co Inc}
> }
>
>> -- 
>> Brian D. Ripley, ripley at stats.ox.ac.uk
>> Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford, Tel: +44 1865 272861 (self)
>> 1 South Parks Road, +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK Fax: +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From spencer.graves at pdf.com  Tue Mar  2 11:53:41 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 02 Mar 2004 02:53:41 -0800
Subject: [R] Problem with Integrate
In-Reply-To: <40444BF2.9050900@helsinki.fi>
References: <40444BF2.9050900@helsinki.fi>
Message-ID: <404467B5.3060906@pdf.com>

      Have you done a search of "www.r-project.org" -> search -> "R site 
search" for "hermite quadrature"?  I just got 11 hits on this, the 
second of which referred to "gauss.quad {statmod}".  This said, "See 
also ... gauss.quad.prob {statmod}".  I haven't tried it, but it looks 
like what you want. 

      Hope this helps. 
      spencer graves

Anon. wrote:

> The background: I'm trying to fit a Poisson-lognormal distrbutuion to 
> some data.  This is a way of modelling species abundances:
> N ~ Pois(lam)
> log(lam) ~ N(mu, sigma2)
> The number of individuals are Poisson distributed with an abundance 
> drawn from a log-normal distrbution.
>
> To fit this to data, I need to integrate out lam.  In principle, I can 
> do it this way:
>
> PLN1 <- function(lam, Count, mu, sigma2) {
>   dpois(Count, exp(lam), log=F)*dnorm(LL, mu, sqrt(sigma2))
> }
>
> and integrate between -Inf and Inf.  For example, with mu=2, and 
> sigma2=2.8 (which are roughly right for the data), and Count=73, I get 
> this:
>
> > integrate(PLN1, -10, 10, Count=73, mu=2, sigma2=2.8)
> 0.001289726 with absolute error < 2.5e-11
> > integrate(PLN1, -20, 20, Count=73, mu=2, sigma2=2.8)
> 0.001289726 with absolute error < 2.5e-11
> > integrate(PLN1, -100, 100, Count=73, mu=2, sigma2=2.8)
> 2.724483e-10 with absolute error < 5.3e-10
> > integrate(PLN1, -500, 500, Count=73, mu=2, sigma2=2.8)
> 1.831093e-73 with absolute error < 3.6e-73
> > integrate(PLN1, -1000, 1000, Count=73, mu=2, sigma2=2.8)
> Error in integrate(PLN1, -1000, 1000, Count = 73, mu = 2, sigma2 = 2.8):
>         non-finite function value
> In addition: Warning message:
> NaNs produced in: dpois(x, lambda, log)
>
> So, the integral gets smaller, and then gives an error.
>
> I then tried entering the formula directly:
> PLN2 <- function(LL, Count, mu, sigma2) {
> exp(-LL-(log(LL)-mu)^2/(2*sigma2))*LL^(Count-1)/(gamma(Count+1)*sqrt(2*pi*sigma2)) 
>
> }
>
> > integrate(PLN2, 0, 100, Count=73, mu=2, sigma2=2.8)
> 0.001287821 with absolute error < 2.6e-10
> > integrate(PLN2, 0, 1000, Count=73, mu=2, sigma2=2.8)
> 0.001289726 with absolute error < 2.9e-08
> > integrate(PLN2, 0, 10000, Count=73, mu=2, sigma2=2.8)
> 0.001289726 with absolute error < 9.7e-06
> > integrate(PLN2, 0, 19100, Count=73, mu=2, sigma2=2.8)
> 1.160307e-08 with absolute error < 2.3e-08
> > integrate(PLN2, 0, 19200, Count=73, mu=2, sigma2=2.8)
> Error in integrate(PLN2, 0, 19200, Count = 73, mu = 2, sigma2 = 2.8) :
>         non-finite function value
>
> And the same thing happens.
>
> I assume that this is because for much of the range, the integral is 
> basically zero.
>
> Can anyone suggest a fix?  Preferably one that will work with 
> Count=320 and Count=0 (both of which I have in the data).
>
> Bob
>



From H.Andersson at nioo.knaw.nl  Tue Mar  2 12:15:21 2004
From: H.Andersson at nioo.knaw.nl (Andersson, Henrik)
Date: Tue, 2 Mar 2004 12:15:21 +0100
Subject: [R] R programming
Message-ID: <65F6E1EC64DCA6489800C09A2007FC6E18174A@cememail1.nioo.int>

I have been using R for a few months to plot my data, and fit
statistical models and so on.

It is stated that R is not only a package for statistcs and graphics but
also a programming language.

Currently I am working with Fortran 90 to do numerical simulations (1D
reactive -transport), and subsequently visualize it in some external
software (spreadsheet). As I now started using R I will plot my results
using R, but my question is:

Could I use R as a replacement for the Fortran program, or is
programming in R more restricted? 

E.g. is it possible to use subroutines in R?

If you think programming in R is great and you have some documentation,
please point me to it.

Thanks, Henrik Andersson
-------------------------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
mailto:h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From sean.oriordain at swiftcall.com  Tue Mar  2 12:22:28 2004
From: sean.oriordain at swiftcall.com (Sean O'Riordain)
Date: Tue, 02 Mar 2004 11:22:28 +0000
Subject: [R] R programming
In-Reply-To: <65F6E1EC64DCA6489800C09A2007FC6E18174A@cememail1.nioo.int>
References: <65F6E1EC64DCA6489800C09A2007FC6E18174A@cememail1.nioo.int>
Message-ID: <40446E74.3090207@swiftcall.com>

Hi Henrick,

if you checked the posting guide (refer the bottom of all emails) you'd 
have spotted a link

# R Language Definition (aka `R Language Manual') describes the R 
language, data objects, etc.
http://cran.r-project.org/doc/manuals/R-lang.pdf

cheers,
Sean

ps. R is indeed a full and complete language



Andersson, Henrik wrote:
> I have been using R for a few months to plot my data, and fit
> statistical models and so on.
> 
> It is stated that R is not only a package for statistcs and graphics but
> also a programming language.
> 
> Currently I am working with Fortran 90 to do numerical simulations (1D
> reactive -transport), and subsequently visualize it in some external
> software (spreadsheet). As I now started using R I will plot my results
> using R, but my question is:
> 
> Could I use R as a replacement for the Fortran program, or is
> programming in R more restricted? 
> 
> E.g. is it possible to use subroutines in R?
> 
> If you think programming in R is great and you have some documentation,
> please point me to it.
> 
> Thanks, Henrik Andersson
> -------------------------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> mailto:h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From sam.kemp2 at ntlworld.com  Tue Mar  2 13:01:13 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Tue, 02 Mar 2004 12:01:13 +0000
Subject: [R] dynamic linking
Message-ID: <40447789.6050902@ntlworld.com>

Hi,

I managed to fixed my problem with linking a ".a" file (an archive of .o 
files) with the following.....

g++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  
-g -O2 -c fileGT.cc -o fileGT.o
(i.e. the beginning bit of R CMD SHLIB)

then I linked (underlined) the files with....

g++ -shared -L/usr/local/lib -o fileGT.so fileGT.o 
_-I/home/sekemp/Documents/ann_0.2/include 
-L/home/sekemp/Documents/ann_0.2/lib -lANN_

This is just for information incase any one else gets stuck on this 
problem and is googling around. There may be better ways where one does 
not need to type so much information into the compiler but hey it works!

Cheers,

Sam

----
Samuel Edward Kemp BSc (Hons) /Cardiff/,
Neural & Evolutionary Computation Research Group,
University of Glamorgan,
Wales,
UK.



From ripley at stats.ox.ac.uk  Tue Mar  2 13:23:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 12:23:46 +0000 (GMT)
Subject: [R] boxplot notches
In-Reply-To: <20040301172941.A115@jessie.research.bell-labs.com>
Message-ID: <Pine.LNX.4.44.0403021218300.17161-100000@gannet.stats>

On Mon, 1 Mar 2004, David James wrote:

> Prof Brian Ripley wrote:
> > On Mon, 1 Mar 2004, Martin Maechler wrote:
> > 
> > > >>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
> > > >>>>>     on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:
> > > 
> > >     TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
> > >     >> Dear list members,
> > >     >> 
> > >     >> Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are
> > >     >> calculated? What do these notches represent exactly? I?d suppose they
> > >     >> are Conficence Intervals for the median, but I?ve also been told they
> > >     >> might show Least Significant Difference (LSD) equivalents.
> > > 
> > >     TL> The help page says that 
> > >     TL> " If the notches of two plots do not overlap then
> > >     TL>   the medians are significantly different at the 5 percent level."
> > > 
> > >     TL> The only thing wrong with this is that it isn't true.
> > >     TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
> > >     TL> so I think the claimed confidence level holds only for
> > >     TL> normal distribuitons with small amounts of contamination.
> > > 
> > > I think John Tukey's idea was that this formula (or just the fact of
> > > using median and quartiles) is still often approximately correct
> > > for quite a few kinds of moderate contaminations...
> > 
> > It may be approximately correct for the width of a CI (and when I checked 
> > it was only appproximately correct for a normal), but I would seriously 
> > doubt if it were approximately correct for a significance level of 5%.
> > Remember how fast the tails of the asymptotic normal distribution decay: a 
> > 20% error turns 5% into 2%.
> > 
> > BTW, if there is a precise reference for this it would be good to add it
> > to boxplot.stats.Rd, as the confidence limits are unexplained there.
> 
> @article{McGi:Tuke:Lars:1978,
> author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
> title = {Variations of {B}ox plots},
> year = {1978},
> journal = {The American Statistician},
> volume = {32},
> pages = {12--16},
> keywords = {Exploratory data analysis; Graphics}
> }

That has the rationale.

> @book{Cham:Clev:Klei:Tuke:1983,
> author = {Chambers, John M. and Cleveland, William S. and Kleiner, Beat
> and Tukey, Paul A.},
> title = {Graphical methods for data analysis},
> year = {1983},
> pages = {395},
> publisher = {Wadsworth Publishing Co Inc}
> }

That has (p.62) 1.57 not 1.58 and says non-overlap is `strong evidence' of 
a difference.

I have added appropriate references to the boxplot and boxplot.stats help 
pages.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetil at entelnet.bo  Tue Mar  2 13:47:33 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 02 Mar 2004 08:47:33 -0400
Subject: [R] boxplot notches
In-Reply-To: <Pine.A41.4.58.0403010944180.64642@homer07.u.washington.edu>
References: <404368A2.4000506@uni-jena.de>
Message-ID: <40444A25.25980.2000063@localhost>

On 1 Mar 2004 at 9:54, Thomas Lumley wrote:

> On Mon, 1 Mar 2004, Christoph Scherber wrote:
> 
> > Dear list members,
> >
> > Can anyone tell me how the notches in boxplot(Y~X,notch=T)  are
> > calculated? What do these notches represent exactly? I?d suppose
> > they are Conficence Intervals for the median, but I?ve also been
> > told they might show Least Significant Difference (LSD) equivalents.
> 
> The help page says that " If the notches of two plots do not overlap
> then the medians are significantly different at the 5 percent level."
> 
> The only thing wrong with this is that it isn't true.  The code says
> that the notches are +/- 1.58 IQR/sqrt(n), so I think the claimed
> confidence level holds only for normal distribuitons with small
> amounts of contamination.
> 

Couldn't this be replaced with confidence limits based on order 
statistics, which are nonparametrically correct, although they take 
some more to compute.

Kjetil Halvorsen

> 
>  -thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Tue Mar  2 13:53:20 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 02 Mar 2004 13:53:20 +0100
Subject: [R] filled contour
In-Reply-To: <20040302081154.CFA81334A5@ltheln11.hmg.inpg.fr>
References: <Pine.A41.4.53.0403011351120.47962@lgit1.obs.ujf-grenoble.fr>
Message-ID: <404491D0.8004.1554088@localhost>

Hi

On 2 Mar 2004 at 9:11, Abdou Ali wrote:

> Hello,
> how can I fill map contour with R without setting the legend. I cannot

does

image() with added contour() 

do what you want?


Cheers
Petr

> see an option to skip the legend for filled.contour. Thank for your
> response.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Tue Mar  2 14:02:30 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 02 Mar 2004 14:02:30 +0100
Subject: [R] Import range of cells from Excel
In-Reply-To: <GOENJEALPPDFMBOMCKOJKEKGCBAA.jean-noel.candau@avignon.inra.fr>
Message-ID: <404493F6.368.15DA1CC@localhost>

Hi

What about to copy a range, paste it to notepad, save it as a *.txt 
file and read with read.delim().

Cheers
Petr

On 2 Mar 2004 at 10:10, Jean-Noel wrote:

> Dear all,
> I would like to import a range of cells (e.g. F10:K234) from an Excel
> worksheet to R. I have looked for documentation on RODBC and
> RDCOMClient but I was not able to find enough information to solve my
> problem and all the examples I have seen were dealing with an entire
> worksheet, not a range of cells. Thanks,
> 
> Jean-Noel
> 
> Jean-Noel Candau
> INRA - Unit? de Recherches Foresti?res M?diterran?ennes
> Avenue A. Vivaldi
> 84000 AVIGNON
> Tel: (33) 4 90 13 59 22
> Fax: (33) 4 90 13 59 59
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From dj at research.bell-labs.com  Tue Mar  2 14:48:13 2004
From: dj at research.bell-labs.com (David James)
Date: Tue, 2 Mar 2004 08:48:13 -0500
Subject: [R] Import range of cells from Excel
In-Reply-To: <GOENJEALPPDFMBOMCKOJKEKGCBAA.jean-noel.candau@avignon.inra.fr>;
	from jean-noel.candau@avignon.inra.fr on Tue, Mar 02, 2004 at
	10:10:42AM +0100
References: <GOENJEALPPDFMBOMCKOJKEKGCBAA.jean-noel.candau@avignon.inra.fr>
Message-ID: <20040302084813.A7278@jessie.research.bell-labs.com>

Jean-Noel wrote:
> Dear all,
> I would like to import a range of cells (e.g. F10:K234) from an Excel
> worksheet to R. I have looked for documentation on RODBC and RDCOMClient but
> I was not able to find enough information to solve my problem and all the
> examples I have seen were dealing with an entire worksheet, not a range of
> cells.

In the case of RDCOMClient, you can do:

  xls <- COMCreate("Excel.Application")
  xls[["Workbooks"]]$Open("Book1.xls")     
  rng <- xls[["ActiveSheet"]]$Range("F10:K234")
  x <- rng[["Value"]]

Depending on what the range contains (e.g., the range consists
of only one data type or not), x may be a list with each element
corresponding to a column in the excel range.

The thing to remember here is how to build ranges -- e.g., use
the Range() method of the worksheet.  This, though, is not R specific,
and it's determined by the Excel COM specification.

Regards,

--
David


> Thanks,
> 
> Jean-Noel
> 
> Jean-Noel Candau
> INRA - Unit? de Recherches Foresti?res M?diterran?ennes
> Avenue A. Vivaldi
> 84000 AVIGNON
> Tel: (33) 4 90 13 59 22
> Fax: (33) 4 90 13 59 59
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From friendly at yorku.ca  Tue Mar  2 14:48:36 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 02 Mar 2004 08:48:36 -0500
Subject: [R] boxplot notches
Message-ID: <404490B4.30106@yorku.ca>

>
>
>>I think John Tukey's idea was that this formula (or just the fact of
>>> using median and quartiles) is still often approximately correct
>>> for quite a few kinds of moderate contaminations...
>>    
>>
>
>It may be approximately correct for the width of a CI (and when I checked 
>it was only appproximately correct for a normal), but I would seriously 
>doubt if it were approximately correct for a significance level of 5%.
>Remember how fast the tails of the asymptotic normal distribution decay: a 
>20% error turns 5% into 2%.
>
>BTW, if there is a precise reference for this it would be good to add it
>to boxplot.stats.Rd, as the confidence limits are unexplained there.
>
>  
>

The factor 1.58 for H-spr/\sqrt{n} comes from the product of three 
approximations going from a 95%
confidence interval for a difference in means, to one for a difference 
in medians, using the H-spr=IQR
instead of the standard deviation:

    H-spr/1.349  \approx \sigma in a N(0,1) dist/n
    \sqrt{ \pi / 2} \approx std error of a median
   1.7 / sqrt{n}  is the average of 1.96 and 1.39=1.96/\sqrt{2}, factors 
for the standard error of the difference
         between two means, in the cases where one variance is tiny, and 
where both are equal.

I believe this is explained in

@Article{McGill-etal:78,
  author =       "R. McGill and J. W. Tukey and W. Larsen",
  year =         "1978",
  title =        "Variations of Box Plots",
  journal =      TAS,
  volume =       "32",
  pages =        "12--16",
}

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From rbaer at atsu.edu  Tue Mar  2 15:41:01 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Tue, 2 Mar 2004 08:41:01 -0600
Subject: [R] superimposing two scatterplots
References: <000801c3ffdd$85922a70$268ff78c@acer4jbjp1qwlp>
Message-ID: <00f701c40064$61f56be0$2e80010a@BigBaer>

Look at:
?plot
?points
----- Original Message ----- 
From: "Raphael Schoenle" <schoenle at fas.harvard.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, March 01, 2004 4:35 PM
Subject: [R] superimposing two scatterplots


> Hi,
>
> How can I plot two scatterplots on the same, one panel?
> I have two times series (price data for two goods) for the same period.
>
> Many thanks,
>
> -R
>
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From Christoph.Scherber at uni-jena.de  Tue Mar  2 15:51:06 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 02 Mar 2004 15:51:06 +0100
Subject: [R] boxplot notches
In-Reply-To: <404490B4.30106@yorku.ca>
References: <404490B4.30106@yorku.ca>
Message-ID: <40449F5A.1090904@uni-jena.de>

In McGill et al. (1978) there?s a description of the calculation as 
follows (p. 16):

"The widths [are] computed from the midspread or interquartile range (R) 
of the data (...), and the number of observations (N) for each group. 
The Gaussian-based asymptotic approximation (Kendall and Stuart 1967) of 
the standard deviation s of the median (M) is given by

s=1.25 R/1.35 sqrt(N)

and can be shown to be reasonably broadly applicable to other 
distributions (...)

The notch around each median can then be calculated as

M +- Cs,

where C is a constant. Should one desire a notch indicating 95 percent 
confidence interval about each median, C = 1.96 would be used (...)

It can be shown that C=1.96 would only be appropriate if the standard 
deviations of the two groups were vastly different (...) Thus, the 
notches were computed as

M+-1.7(1.25R/1.35 sqrt(N))

Hope this helps. Best regards
Chris.

REF:
McGill, R; Tukey, JW &  Larsen, WA (1978) Variations of Box Plots. The 
American Statistician, Vol.32 No. 1, pp.12-16.
Kendall, MG & Stuart, A (1967): The Advanced Theory of Statistics, 
Vol.1, 2nd ed., Ch14., New York, Hafner Publishing Co.

*****************************************


Michael Friendly wrote:

>>
>>
>>> I think John Tukey's idea was that this formula (or just the fact of
>>>
>>>> using median and quartiles) is still often approximately correct
>>>> for quite a few kinds of moderate contaminations...
>>>
>>>   
>>
>>
>> It may be approximately correct for the width of a CI (and when I 
>> checked it was only appproximately correct for a normal), but I would 
>> seriously doubt if it were approximately correct for a significance 
>> level of 5%.
>> Remember how fast the tails of the asymptotic normal distribution 
>> decay: a 20% error turns 5% into 2%.
>>
>> BTW, if there is a precise reference for this it would be good to add it
>> to boxplot.stats.Rd, as the confidence limits are unexplained there.
>>
>>  
>>
>
> The factor 1.58 for H-spr/\sqrt{n} comes from the product of three 
> approximations going from a 95%
> confidence interval for a difference in means, to one for a difference 
> in medians, using the H-spr=IQR
> instead of the standard deviation:
>
>    H-spr/1.349  \approx \sigma in a N(0,1) dist/n
>    \sqrt{ \pi / 2} \approx std error of a median
>   1.7 / sqrt{n}  is the average of 1.96 and 1.39=1.96/\sqrt{2}, 
> factors for the standard error of the difference
>         between two means, in the cases where one variance is tiny, 
> and where both are equal.
>
> I believe this is explained in
>
> @Article{McGill-etal:78,
>  author =       "R. McGill and J. W. Tukey and W. Larsen",
>  year =         "1978",
>  title =        "Variations of Box Plots",
>  journal =      TAS,
>  volume =       "32",
>  pages =        "12--16",
> }
>



From p.b.pynsent at bham.ac.uk  Tue Mar  2 16:16:23 2004
From: p.b.pynsent at bham.ac.uk (P. B. Pynsent)
Date: Tue, 2 Mar 2004 15:16:23 +0000
Subject: [R] boxplot notches
In-Reply-To: <40445F6B.4020901@uni-jena.de>
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
	<20040301172941.A115@jessie.research.bell-labs.com>
	<40445F6B.4020901@uni-jena.de>
Message-ID: <910A7B49-6C5C-11D8-81B4-000A95B0CE8A@bham.ac.uk>

A Google search showed  that all this was discussed in April 1988 with 
an extensive reply to the question from M Maechler.
I, as a non-statistician, blindly believed what was written in the 
boxplot() help file, I am sure many would be grateful to this help 
being modified.

I still do not understand why , 6 years later with GHz processors, 
boxplot() could not have an option to produce exact intervals. After 
all,  a range option is offered for the whiskers.
At least then non-overlapping notches would have some meaning, wouldn't 
they?

On 2 Mar 2004, at 10:18, Christoph Scherber wrote:

> Dear colleagues,
>
> I think it would be a good idea to include a short note in the R 
> boxplot() help file, stating exactly how the confidence levels are 
> calculated
> ("the notches are +/- 1.58 IQR/sqrt(n)")  - at least as a guidance for 
> users not advanced enough to directly interpret the code.
>
> Would this be possible?
>
> Regards,
> Christoph.
>
> David James wrote:
>
>> Prof Brian Ripley wrote:
>>
>>> On Mon, 1 Mar 2004, Martin Maechler wrote:
>>>
>>>>>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>>>>> on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:
>>>>>>>>
>>>> TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
>>>> >> Dear list members,
>>>> >>
>>>> >> Can anyone tell me how the notches in boxplot(Y~X,notch=T) are
>>>> >> calculated? What do these notches represent exactly? I?d suppose 
>>>> they
>>>> >> are Conficence Intervals for the median, but I?ve also been told 
>>>> they
>>>> >> might show Least Significant Difference (LSD) equivalents.
>>>>
>>>> TL> The help page says that
>>>> TL> " If the notches of two plots do not overlap then
>>>> TL> the medians are significantly different at the 5 percent level."
>>>>
>>>> TL> The only thing wrong with this is that it isn't true.
>>>> TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
>>>> TL> so I think the claimed confidence level holds only for
>>>> TL> normal distribuitons with small amounts of contamination.
>>>>
>>>> I think John Tukey's idea was that this formula (or just the fact of
>>>> using median and quartiles) is still often approximately correct
>>>> for quite a few kinds of moderate contaminations...
>>>
>>> It may be approximately correct for the width of a CI (and when I 
>>> checked
>>> it was only appproximately correct for a normal), but I would 
>>> seriously
>>> doubt if it were approximately correct for a significance level of 
>>> 5%.
>>> Remember how fast the tails of the asymptotic normal distribution 
>>> decay: a
>>> 20% error turns 5% into 2%.
>>>
>>> BTW, if there is a precise reference for this it would be good to 
>>> add it
>>> to boxplot.stats.Rd, as the confidence limits are unexplained there.
>>
>>
>> @article{McGi:Tuke:Lars:1978,
>> author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
>> title = {Variations of {B}ox plots},
>> year = {1978},
>> journal = {The American Statistician},
>> volume = {32},
>> pages = {12--16},
>> keywords = {Exploratory data analysis; Graphics}
>> }
>>
>> @book{Cham:Clev:Klei:Tuke:1983,
>> author = {Chambers, John M. and Cleveland, William S. and Kleiner, 
>> Beat
>> and Tukey, Paul A.},
>> title = {Graphical methods for data analysis},
>> year = {1983},
>> pages = {395},
>> publisher = {Wadsworth Publishing Co Inc}
>> }
>>
>>> -- 
>>> Brian D. Ripley, ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford, Tel: +44 1865 272861 (self)
>>> 1 South Parks Road, +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK Fax: +44 1865 272595
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
P. B. Pynsent,
Research & Teaching Centre,
Royal Orthopaedic Hospital,
Northfield,
Birmingham, B31 2AP,
U. K.



From m1jjh00 at frb.gov  Tue Mar  2 16:17:39 2004
From: m1jjh00 at frb.gov (Jeffrey J. Hallman)
Date: Tue, 02 Mar 2004 10:17:39 -0500
Subject: [R] dev.print and X11(canvas = "black") 
In-Reply-To: Message from Prof Brian Ripley <ripley@stats.ox.ac.uk> of "Tue,
	02 Mar 2004 07:15:36 GMT."
	<Pine.LNX.4.44.0403020710471.12356-100000@gannet.stats> 
Message-ID: <20040302151739.27489AEA4D@mail.rsma.frb.gov>

Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

   > On Mon, 1 Mar 2004 jhallman at frb.gov wrote:

  >> In Splus, I often use graphics windows with a black background and white
  >> foreground.  The S print.graph() function sends the current plot to my
  >> printer but with a white background and black foreground.  I'd like to

   > S (and S-PLUS) only has a few colour numbers, and you can change their 
   > interpretation on the screen graphics devices after plotting.  R does plot 
   > in colours rather than numbers.

  >> be able to do something similar in R, but can't figure out how.  I've
  >> tried various permutations of dev.copy() and dev.print(), but it seems
  >> that the foreground color is fixed when the original plot is drawn, and
  >> dev.copy always redraws it with that color.  So even though I can get my
  >> postscript output with a white background, things that were drawn white on the
  >> dark background of my X11 device are still drawn white when dev.copy'd
  >> to a postscript device.  Since the postscript has a white background,
  >> that means the white foreground stuff doesn't show up.
  >> 
  >> Is there a way to handle this?  Or do I just have to give up on ever
  >> using a dark canvas in X11() if I ever want to print the result?

   > There are many ways to print the result, and re-running the plot commands 
   > after switching to a postscript() device is one of the best.  This would 
   > work for you if you only ever use colour numbers and switch the palette
   > (and par("fg")).

Jim Lemon made essentially the same suggestion: run the code that
created the plot twice, once with X11 as the active device, and again
with postscript as the active device.

The trouble with this is that is doesn't handle plots that the user has
edited.  I have some charts that users add things to interactively,
using combinations of locator(), text(), mtext(), and arrows().  There's
no nice way to know what a user has done to a plot before he attempts to
print it.

I know that R is internally maintaining a display list.  Is there a way
to access that without dropping into C?  And what's in it, anyway?

Jeff



From tlumley at u.washington.edu  Tue Mar  2 16:18:56 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 2 Mar 2004 07:18:56 -0800 (PST)
Subject: [R] Problem with Integrate
In-Reply-To: <40444BF2.9050900@helsinki.fi>
References: <40444BF2.9050900@helsinki.fi>
Message-ID: <Pine.A41.4.58.0403020717380.58162@homer40.u.washington.edu>

On Tue, 2 Mar 2004, Anon. wrote:

> The background: I'm trying to fit a Poisson-lognormal distrbutuion to
> some data.  This is a way of modelling species abundances:
> N ~ Pois(lam)
> log(lam) ~ N(mu, sigma2)
> The number of individuals are Poisson distributed with an abundance
> drawn from a log-normal distrbution.
>
> To fit this to data, I need to integrate out lam.  In principle, I can
> do it this way:
>
> PLN1 <- function(lam, Count, mu, sigma2) {
>    dpois(Count, exp(lam), log=F)*dnorm(LL, mu, sqrt(sigma2))
> }
>
> and integrate between -Inf and Inf.  For example, with mu=2, and
> sigma2=2.8 (which are roughly right for the data), and Count=73, I get this:
>
>  > integrate(PLN1, -10, 10, Count=73, mu=2, sigma2=2.8)
> 0.001289726 with absolute error < 2.5e-11
>  > integrate(PLN1, -20, 20, Count=73, mu=2, sigma2=2.8)
> 0.001289726 with absolute error < 2.5e-11
>  > integrate(PLN1, -100, 100, Count=73, mu=2, sigma2=2.8)
> 2.724483e-10 with absolute error < 5.3e-10
>  > integrate(PLN1, -500, 500, Count=73, mu=2, sigma2=2.8)
> 1.831093e-73 with absolute error < 3.6e-73
>  > integrate(PLN1, -1000, 1000, Count=73, mu=2, sigma2=2.8)
> Error in integrate(PLN1, -1000, 1000, Count = 73, mu = 2, sigma2 = 2.8):
>          non-finite function value
> In addition: Warning message:
> NaNs produced in: dpois(x, lambda, log)
>
> So, the integral gets smaller, and then gives an error.
<snip>
> I assume that this is because for much of the range, the integral is
> basically zero.

The help page for integrate() says

     When integrating over infinite intervals do so explicitly, rather
     than just using a large number as the endpoint.  This increases
     the chance of a correct answer - any function whose integral over
     an infinite interval is finite must be near zero for most of that
     interval.

That is, if you want an integral from 0 to Inf, do that.

	-thomas



From p.b.pynsent at bham.ac.uk  Tue Mar  2 16:19:53 2004
From: p.b.pynsent at bham.ac.uk (P. B. Pynsent)
Date: Tue, 2 Mar 2004 15:19:53 +0000
Subject: [R] boxplot notches
In-Reply-To: <40445F6B.4020901@uni-jena.de>
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
	<20040301172941.A115@jessie.research.bell-labs.com>
	<40445F6B.4020901@uni-jena.de>
Message-ID: <0E135340-6C5D-11D8-81B4-000A95B0CE8A@bham.ac.uk>

A Google search showed  that all this was discussed in April 1988 with 
an extensive reply to the question from M Maechler.
Sorry above should be 1998

On 2 Mar 2004, at 10:18, Christoph Scherber wrote:

> Dear colleagues,
>
> I think it would be a good idea to include a short note in the R 
> boxplot() help file, stating exactly how the confidence levels are 
> calculated
> ("the notches are +/- 1.58 IQR/sqrt(n)")  - at least as a guidance for 
> users not advanced enough to directly interpret the code.
>
> Would this be possible?
>
> Regards,
> Christoph.
>
> David James wrote:
>
>> Prof Brian Ripley wrote:
>>
>>> On Mon, 1 Mar 2004, Martin Maechler wrote:
>>>
>>>>>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>>>>> on Mon, 1 Mar 2004 09:54:48 -0800 (PST) writes:
>>>>>>>>
>>>> TL> On Mon, 1 Mar 2004, Christoph Scherber wrote:
>>>> >> Dear list members,
>>>> >>
>>>> >> Can anyone tell me how the notches in boxplot(Y~X,notch=T) are
>>>> >> calculated? What do these notches represent exactly? I?d suppose 
>>>> they
>>>> >> are Conficence Intervals for the median, but I?ve also been told 
>>>> they
>>>> >> might show Least Significant Difference (LSD) equivalents.
>>>>
>>>> TL> The help page says that
>>>> TL> " If the notches of two plots do not overlap then
>>>> TL> the medians are significantly different at the 5 percent level."
>>>>
>>>> TL> The only thing wrong with this is that it isn't true.
>>>> TL> The code says that the notches are +/- 1.58 IQR/sqrt(n),
>>>> TL> so I think the claimed confidence level holds only for
>>>> TL> normal distribuitons with small amounts of contamination.
>>>>
>>>> I think John Tukey's idea was that this formula (or just the fact of
>>>> using median and quartiles) is still often approximately correct
>>>> for quite a few kinds of moderate contaminations...
>>>
>>> It may be approximately correct for the width of a CI (and when I 
>>> checked
>>> it was only appproximately correct for a normal), but I would 
>>> seriously
>>> doubt if it were approximately correct for a significance level of 
>>> 5%.
>>> Remember how fast the tails of the asymptotic normal distribution 
>>> decay: a
>>> 20% error turns 5% into 2%.
>>>
>>> BTW, if there is a precise reference for this it would be good to 
>>> add it
>>> to boxplot.stats.Rd, as the confidence limits are unexplained there.
>>
>>
>> @article{McGi:Tuke:Lars:1978,
>> author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
>> title = {Variations of {B}ox plots},
>> year = {1978},
>> journal = {The American Statistician},
>> volume = {32},
>> pages = {12--16},
>> keywords = {Exploratory data analysis; Graphics}
>> }
>>
>> @book{Cham:Clev:Klei:Tuke:1983,
>> author = {Chambers, John M. and Cleveland, William S. and Kleiner, 
>> Beat
>> and Tukey, Paul A.},
>> title = {Graphical methods for data analysis},
>> year = {1983},
>> pages = {395},
>> publisher = {Wadsworth Publishing Co Inc}
>> }
>>
>>> -- 
>>> Brian D. Ripley, ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford, Tel: +44 1865 272861 (self)
>>> 1 South Parks Road, +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK Fax: +44 1865 272595
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
P. B. Pynsent,
Research & Teaching Centre,
Royal Orthopaedic Hospital,
Northfield,
Birmingham, B31 2AP,
U. K.



From bob.ohara at helsinki.fi  Tue Mar  2 16:23:23 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Tue, 02 Mar 2004 17:23:23 +0200
Subject: [R] Problem with Integrate
References: <40444BF2.9050900@helsinki.fi>
	<Pine.A41.4.58.0403020717380.58162@homer40.u.washington.edu>
Message-ID: <4044A6EB.4000506@helsinki.fi>

Thomas Lumley wrote:
> On Tue, 2 Mar 2004, Anon. wrote:
> 
> 
>>The background: I'm trying to fit a Poisson-lognormal distrbutuion to
>>some data.  This is a way of modelling species abundances:
>>N ~ Pois(lam)
>>log(lam) ~ N(mu, sigma2)
>>The number of individuals are Poisson distributed with an abundance
>>drawn from a log-normal distrbution.
>>
>>To fit this to data, I need to integrate out lam.  In principle, I can
>>do it this way:
>>
>>PLN1 <- function(lam, Count, mu, sigma2) {
>>   dpois(Count, exp(lam), log=F)*dnorm(LL, mu, sqrt(sigma2))
>>}
>>
>>and integrate between -Inf and Inf.  For example, with mu=2, and
>>sigma2=2.8 (which are roughly right for the data), and Count=73, I get this:
>>
>> > integrate(PLN1, -10, 10, Count=73, mu=2, sigma2=2.8)
>>0.001289726 with absolute error < 2.5e-11
>> > integrate(PLN1, -20, 20, Count=73, mu=2, sigma2=2.8)
>>0.001289726 with absolute error < 2.5e-11
>> > integrate(PLN1, -100, 100, Count=73, mu=2, sigma2=2.8)
>>2.724483e-10 with absolute error < 5.3e-10
>> > integrate(PLN1, -500, 500, Count=73, mu=2, sigma2=2.8)
>>1.831093e-73 with absolute error < 3.6e-73
>> > integrate(PLN1, -1000, 1000, Count=73, mu=2, sigma2=2.8)
>>Error in integrate(PLN1, -1000, 1000, Count = 73, mu = 2, sigma2 = 2.8):
>>         non-finite function value
>>In addition: Warning message:
>>NaNs produced in: dpois(x, lambda, log)
>>
>>So, the integral gets smaller, and then gives an error.
> 
> <snip>
> 
>>I assume that this is because for much of the range, the integral is
>>basically zero.
> 
> 
> The help page for integrate() says
> 
>      When integrating over infinite intervals do so explicitly, rather
>      than just using a large number as the endpoint.  This increases
>      the chance of a correct answer - any function whose integral over
>      an infinite interval is finite must be near zero for most of that
>      interval.
> 
> That is, if you want an integral from 0 to Inf, do that.
> 
Sorry, I forgot to put that in my message.  It gives the same error as a 
large value.  I assume it's all a result of the NaN's being returned.

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From tlumley at u.washington.edu  Tue Mar  2 16:24:41 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 2 Mar 2004 07:24:41 -0800 (PST)
Subject: [R] boxplot notches
In-Reply-To: <910A7B49-6C5C-11D8-81B4-000A95B0CE8A@bham.ac.uk>
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
	<20040301172941.A115@jessie.research.bell-labs.com>
	<40445F6B.4020901@uni-jena.de>
	<910A7B49-6C5C-11D8-81B4-000A95B0CE8A@bham.ac.uk>
Message-ID: <Pine.A41.4.58.0403020719130.58162@homer40.u.washington.edu>

On Tue, 2 Mar 2004, P. B. Pynsent wrote:

> A Google search showed  that all this was discussed in April 1988 with
> an extensive reply to the question from M Maechler.
> I, as a non-statistician, blindly believed what was written in the
> boxplot() help file, I am sure many would be grateful to this help
> being modified.
>
> I still do not understand why , 6 years later with GHz processors,
> boxplot() could not have an option to produce exact intervals. After
> all,  a range option is offered for the whiskers.
> At least then non-overlapping notches would have some meaning, wouldn't
> they?

Well, they would have *some* meaning, but it would be hard to say exactly
what. There isn't an exact confidence interval for  the difference in
medians, so you can't find a level for two confidence intervals that
corresponds to a specified level for the test of equality of medians.

	-thomas



From tlumley at u.washington.edu  Tue Mar  2 16:25:46 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 2 Mar 2004 07:25:46 -0800 (PST)
Subject: [R] Problem with Integrate
In-Reply-To: <4044A6EB.4000506@helsinki.fi>
References: <40444BF2.9050900@helsinki.fi>
	<Pine.A41.4.58.0403020717380.58162@homer40.u.washington.edu>
	<4044A6EB.4000506@helsinki.fi>
Message-ID: <Pine.A41.4.58.0403020724580.58162@homer40.u.washington.edu>

On Tue, 2 Mar 2004, Anon. wrote:

> Thomas Lumley wrote:
> > The help page for integrate() says
> >
> >      When integrating over infinite intervals do so explicitly, rather
> >      than just using a large number as the endpoint.  This increases
> >      the chance of a correct answer - any function whose integral over
> >      an infinite interval is finite must be near zero for most of that
> >      interval.
> >
> > That is, if you want an integral from 0 to Inf, do that.
> >
> Sorry, I forgot to put that in my message.  It gives the same error as a
> large value.  I assume it's all a result of the NaN's being returned.
>

In that case I think you need to decide what the NaNs should be and set
them to that.  If they should be 0 then make them 0.

	-thomas



From ripley at stats.ox.ac.uk  Tue Mar  2 16:34:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 15:34:55 +0000 (GMT)
Subject: [R] boxplot notches
In-Reply-To: <404490B4.30106@yorku.ca>
Message-ID: <Pine.LNX.4.44.0403021532560.30516-100000@gannet.stats>

On Tue, 2 Mar 2004, Michael Friendly wrote:

> >
> >
> >>I think John Tukey's idea was that this formula (or just the fact of
> >>> using median and quartiles) is still often approximately correct
> >>> for quite a few kinds of moderate contaminations...
> >>    
> >>
> >
> >It may be approximately correct for the width of a CI (and when I checked 
> >it was only appproximately correct for a normal), but I would seriously 
> >doubt if it were approximately correct for a significance level of 5%.
> >Remember how fast the tails of the asymptotic normal distribution decay: a 
> >20% error turns 5% into 2%.
> >
> >BTW, if there is a precise reference for this it would be good to add it
> >to boxplot.stats.Rd, as the confidence limits are unexplained there.
> >
> >  
> >
> 
> The factor 1.58 for H-spr/\sqrt{n} comes from the product of three 
> approximations going from a 95%
> confidence interval for a difference in means, to one for a difference 
> in medians, using the H-spr=IQR
> instead of the standard deviation:
> 
>     H-spr/1.349  \approx \sigma in a N(0,1) dist/n
>     \sqrt{ \pi / 2} \approx std error of a median
>    1.7 / sqrt{n}  is the average of 1.96 and 1.39=1.96/\sqrt{2}, factors 
> for the standard error of the difference
>          between two means, in the cases where one variance is tiny, and 
> where both are equal.
> 
> I believe this is explained in
> 
> @Article{McGill-etal:78,
>   author =       "R. McGill and J. W. Tukey and W. Larsen",
>   year =         "1978",
>   title =        "Variations of Box Plots",
>   journal =      TAS,
>   volume =       "32",
>   pages =        "12--16",
> }

Yes it is (see earlier messages in the thread), but note that 1.7 is 
pretty unprincipled and leads to quite large errors in the nominal 5% 
significance level.

The appropriate help pages have been updated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Mar  2 16:53:44 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  2 Mar 2004 10:53:44 -0500 (EST)
Subject: [R] dev.print and X11(canvas = 
Message-ID: <20040302155344.F36F43A03@mprdmxin.myway.com>



Try this:

dev.control(displaylist="enable")
plot(1:4)
identify(1:4) 
# perform some interactions which label points
myplot <- recordPlot()
plot(1:10) # overwrite first plot
myplot
# at this point the original plot is back with your annotations

---

Date:   Tue, 02 Mar 2004 10:17:39 -0500 
From:   Jeffrey J. Hallman <m1jjh00 at frb.gov>
To:   Prof Brian Ripley <ripley at stats.ox.ac.uk> 
Cc:   <jhallman at frb.gov>, <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] dev.print and X11(canvas = "black")  

[...]
I know that R is internally maintaining a display list. Is there a way
to access that without dropping into C? And what's in it, anyway?

Jeff



From maechler at stat.math.ethz.ch  Tue Mar  2 17:10:45 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 2 Mar 2004 17:10:45 +0100
Subject: [R] boxplot notches
In-Reply-To: <910A7B49-6C5C-11D8-81B4-000A95B0CE8A@bham.ac.uk>
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
	<20040301172941.A115@jessie.research.bell-labs.com>
	<40445F6B.4020901@uni-jena.de>
	<910A7B49-6C5C-11D8-81B4-000A95B0CE8A@bham.ac.uk>
Message-ID: <16452.45573.754546.114639@gargle.gargle.HOWL>

>>>>> "P" == P B Pynsent <p.b.pynsent at bham.ac.uk>
>>>>>     on Tue, 2 Mar 2004 15:16:23 +0000 writes:

    P> A Google search showed  that all this was discussed in April 1998 with 
    P> an extensive reply to the question from M Maechler.

Yes, indeed:
     http://finzi.psych.upenn.edu/R/Rhelpold/archive/0839.html , 
and I hadn't remembered,  there giving quite a bit more numeric details
than we have had in this thread. 

    P> I, as a non-statistician, blindly believed what was written in the 
    P> boxplot() help file, I am sure many would be grateful to this help 
    P> being modified.

there's nothing wrong in there, AFAIK, is there?

    P> I still do not understand why , 6 years later with GHz processors, 
    P> boxplot() could not have an option to produce exact intervals. After 
    P> all,  a range option is offered for the whiskers.
    P> At least then non-overlapping notches would have some meaning, wouldn't 
    P> they?

back in 1998, I had answered to Peter Dalgaard's

       PD> Search me... However, wouldn't it be better in any case to do an
       PD> exact 95% CI based on the binomial distribution? Of course, you
       PD> need at least 6 observations to do that.

  MM> No, please not yet another definition of the boxplot!
  MM> People looking at boxplots should be able to rely on their knowledge of
  MM> what a boxplot is.

and I still very much adhere to that.

If one really wants, there's not too much wrong with adding
something like "median.test()" with the corresponding confidence
interval {if it's agreed that you'd want the "close-to-boundary"
	  order statistics and [pq]binomial for that},
but I'd already vote tentatively against another
boxplot option which would change the way the notches are
computed.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ripley at stats.ox.ac.uk  Tue Mar  2 17:28:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 16:28:24 +0000 (GMT)
Subject: [R] Problem with Integrate
In-Reply-To: <4044A6EB.4000506@helsinki.fi>
Message-ID: <Pine.LNX.4.44.0403021623480.30600-100000@gannet.stats>

I think what you actually want is an expectation with respect to a normal 
random variable.  If so, you should be integrating wrt to a standard 
normal (after a linear change) and not wrt to Lebesgue measure.  That is, 
use something like Gauss-Hermite integration, although I expect 
integrate() might work well enough over the range (-4, 4).

On Tue, 2 Mar 2004, Anon. wrote:

> Thomas Lumley wrote:
> > On Tue, 2 Mar 2004, Anon. wrote:
> > 
> > 
> >>The background: I'm trying to fit a Poisson-lognormal distrbutuion to
> >>some data.  This is a way of modelling species abundances:
> >>N ~ Pois(lam)
> >>log(lam) ~ N(mu, sigma2)
> >>The number of individuals are Poisson distributed with an abundance
> >>drawn from a log-normal distrbution.
> >>
> >>To fit this to data, I need to integrate out lam.  In principle, I can
> >>do it this way:
> >>
> >>PLN1 <- function(lam, Count, mu, sigma2) {
> >>   dpois(Count, exp(lam), log=F)*dnorm(LL, mu, sqrt(sigma2))
> >>}
> >>
> >>and integrate between -Inf and Inf.  For example, with mu=2, and
> >>sigma2=2.8 (which are roughly right for the data), and Count=73, I get this:
> >>
> >> > integrate(PLN1, -10, 10, Count=73, mu=2, sigma2=2.8)
> >>0.001289726 with absolute error < 2.5e-11
> >> > integrate(PLN1, -20, 20, Count=73, mu=2, sigma2=2.8)
> >>0.001289726 with absolute error < 2.5e-11
> >> > integrate(PLN1, -100, 100, Count=73, mu=2, sigma2=2.8)
> >>2.724483e-10 with absolute error < 5.3e-10
> >> > integrate(PLN1, -500, 500, Count=73, mu=2, sigma2=2.8)
> >>1.831093e-73 with absolute error < 3.6e-73
> >> > integrate(PLN1, -1000, 1000, Count=73, mu=2, sigma2=2.8)
> >>Error in integrate(PLN1, -1000, 1000, Count = 73, mu = 2, sigma2 = 2.8):
> >>         non-finite function value
> >>In addition: Warning message:
> >>NaNs produced in: dpois(x, lambda, log)
> >>
> >>So, the integral gets smaller, and then gives an error.
> > 
> > <snip>
> > 
> >>I assume that this is because for much of the range, the integral is
> >>basically zero.
> > 
> > 
> > The help page for integrate() says
> > 
> >      When integrating over infinite intervals do so explicitly, rather
> >      than just using a large number as the endpoint.  This increases
> >      the chance of a correct answer - any function whose integral over
> >      an infinite interval is finite must be near zero for most of that
> >      interval.
> > 
> > That is, if you want an integral from 0 to Inf, do that.
> > 
> Sorry, I forgot to put that in my message.  It gives the same error as a 
> large value.  I assume it's all a result of the NaN's being returned.
> 
> Bob
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Mar  2 17:37:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Mar 2004 17:37:38 +0100
Subject: [R] Problem with Integrate
In-Reply-To: <4044A6EB.4000506@helsinki.fi>
References: <40444BF2.9050900@helsinki.fi>
	<Pine.A41.4.58.0403020717380.58162@homer40.u.washington.edu>
	<4044A6EB.4000506@helsinki.fi>
Message-ID: <x24qt7w6gd.fsf@biostat.ku.dk>

"Anon." <bob.ohara at helsinki.fi> writes:

> >>I assume that this is because for much of the range, the integral is
> >>basically zero.
> > The help page for integrate() says
> >      When integrating over infinite intervals do so explicitly,
> > rather
> >      than just using a large number as the endpoint.  This increases
> >      the chance of a correct answer - any function whose integral over
> >      an infinite interval is finite must be near zero for most of that
> >      interval.
> > That is, if you want an integral from 0 to Inf, do that.
> >
> Sorry, I forgot to put that in my message.  It gives the same error as
> a large value.  I assume it's all a result of the NaN's being returned.

It does seem to help a bit if you modify the integrand to

> PLN1
function(lam, Count, mu, sigma2) {
   t1 <- dpois(Count, exp(lam), log=F)
   t2 <- dnorm(lam, mu, sqrt(sigma2))
   ifelse(t1==0|t2==0,0,t1*t2)
}

> integrate(PLN1, -Inf, Inf, Count=73, mu=2, sigma2=2.8)
0.001289726 with absolute error < 6.5e-05
Warning messages:
1: NaNs produced in: dpois(x, lambda, log)
2: NaNs produced in: dpois(x, lambda, log)


For the generic problem, you might need to expand around the maximum,
e.g.

# -log(PLN1) -- better behaved for optimizing
pln2 <-function(lam, Count, mu, sigma2) {
   t1 <- dpois(Count, exp(lam), log=T)
   t2 <- dnorm(lam, mu, sqrt(sigma2), log=T)
   ifelse(t1==0|t2==0,0,-t1-t2)
}

xmax <- nlm(pln2,0, Count=73, mu=2, sigma2=2.8)$estimate # or optimize()

r1 <- uniroot(function(x,...)(PLN1(x,...)-1e-30),c(-100,xmax), 
              Count=73, mu=2, sigma2=2.8)$root
r2 <- uniroot(function(x,...)(PLN1(x,...)-1e-30),c(xmax,100), 
              Count=73, mu=2, sigma2=2.8)$root

integrate(PLN1, r1, r2, Count=73, mu=2, sigma2=2.8)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From FWS4 at CDRH.FDA.GOV  Tue Mar  2 17:48:22 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Tue, 2 Mar 2004 11:48:22 -0500
Subject: [R] Jackknife after bootstrap influence values in boot package?
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB78F@drm556>

Is there a routine in the boot package to get the jackknife-after- 
bootstrap influence values?  That is, the influence values of
a jackknife of the bootstrap estimates?  
I can see how one would go about it from the jack.after.boot code, but that
routine only makes pretty pictures.
It wouldn't be hard to write, but I find it hard to believe this 
isn't part of the package already.

Thanks for any help.

-Frank



From aiminy at iastate.edu  Tue Mar  2 17:59:29 2004
From: aiminy at iastate.edu (Aimin Yan)
Date: Tue, 02 Mar 2004 10:59:29 -0600
Subject: [R] how to delete a matrix column
Message-ID: <6.0.1.1.2.20040302105847.01cb9870@aiminy.mail.iastate.edu>

Hello,
I am new to R, How to delete a matrix column.
Thanks,



From ripley at stats.ox.ac.uk  Tue Mar  2 17:59:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 16:59:35 +0000 (GMT)
Subject: [R] dev.print and X11(canvas = "black") 
In-Reply-To: <20040302151739.27489AEA4D@mail.rsma.frb.gov>
Message-ID: <Pine.LNX.4.44.0403021653520.30708-100000@gannet.stats>

On Tue, 2 Mar 2004, Jeffrey J. Hallman wrote:

> Jim Lemon made essentially the same suggestion: run the code that
> created the plot twice, once with X11 as the active device, and again
> with postscript as the active device.
> 
> The trouble with this is that is doesn't handle plots that the user has
> edited.  I have some charts that users add things to interactively,
> using combinations of locator(), text(), mtext(), and arrows().  There's
> no nice way to know what a user has done to a plot before he attempts to
> print it.

Yes, see the sections on `Graphics hardcopy' in section A.1 of MASS4 for 
exactly this comment: it is hardly news.

You could record what the users do as they do it by using your own 
versions of these functions.

> I know that R is internally maintaining a display list.  Is there a way
> to access that without dropping into C?  And what's in it, anyway?

Not really (recordPlot etc does), and not documented AFAIK (but Paul
Murrell will be able to correct me if I am wrong here).  However, I am
pretty sure that it has the actual plot colours and not the colour numbers
in it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hdoran at nasdc.org  Tue Mar  2 18:44:33 2004
From: hdoran at nasdc.org (Harold Doran)
Date: Tue, 2 Mar 2004 12:44:33 -0500
Subject: [R] how to delete a matrix column
Message-ID: <66578BFC0BA55348B5907A0F798EE93066353E@ernesto.NASDC.ORG>

You can use

dataset$columnname=NULL

HCD
 
------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628

 
 


-----Original Message-----
From: Aimin Yan [mailto:aiminy at iastate.edu]
Sent: Tuesday, March 02, 2004 11:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] how to delete a matrix column


Hello,
I am new to R, How to delete a matrix column.
Thanks,

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Giles.Heywood at CommerzbankIB.com  Tue Mar  2 18:54:09 2004
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Tue, 2 Mar 2004 17:54:09 -0000 
Subject: [R] an extension to 'array'
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF7477@xmx8lonib.lonib.commerzbank.com>

1. I want to extend the 'array' class, and prefer to use S4 in the belief
that this is the best structure for new projects (as the documentation
says).  I actually wish to do something similar to the excellent Oarray by
Jonathan Rougier, but as this class is S3, I can't see how to extend it by
S4.  Am I being dumb here?

2. Seeking then to define a similar class in S4, my principal task is to
modify (extend?) a the extractor "[", modifying the range variables i,j,...
. I run into problems here, though.  The following is a minimal example:

setClass("xarray",representation("array"))

setMethod(f="[", 
       signature=signature("xarray","ANY"),  
       definition=function(x,i,j, ..., drop)
                    {
                    xcall <- match.call()
			  #sundry processes on xcall to adjust i,j,...
                    y <- x at .Data
                    xcall[[2]] <- as.name("y")
                    eval(xcall)
                    }
       )       

foo <- new("xarray",array(1:24,c(2,3,4)))
foo[1,1,1,drop=FALSE]	#gives expected result i.e. a 1 in a [1,1,1] array
foo[1,1,,drop=FALSE]	#again gives the expected result in a [1,1,4] array
foo[1,,,drop=FALSE]	#gives an error "Error in y[i = 1, , drop = FALSE] :
incorrect number of dimensions"

Examining match.call() e.g. in the browser shows that there is indeed one
fewer argument than is required 

Called from: foo[1, , , drop = FALSE]
Browse[1]> xcall
y[i = 1, , drop = FALSE]

3.  I welcome input on this, at different levels, including the following:

- Is there a relatively small correction to what I have attempted and all is
well? 

- Can I instead extend the (perfectly functional) S3 class Oarray using S4?


- What is the better way to do what I am attempting in the last 3 lines of
my method i.e. a small tweak to the arguments of a S4 class using variables
from its slots, then calling the method for the superclass?

- Is there a package on CRAN which, by my studying it closely, I will gain
enlightenment?

I might also get RTFM, but then I *have* read quite a lot of the
documentation.  If the advice is RTFM, I request some assistance in finding
the correct sections for my question.  I would happily read the whole of the
Green Book if I could gain enlightenment from it, but there have been a lot
of changes since it was written.

Thanks

- Giles


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From ryszard.czerminski at pharma.novartis.com  Tue Mar  2 18:59:47 2004
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Tue, 2 Mar 2004 12:59:47 -0500
Subject: [R] row.names are dropped when extracting one column ?
Message-ID: <OFAF7D5F89.5FC724F8-ON85256E4B.006292B6-85256E4B.00630128@EU.novartis.net>

Apparently row names are dropped when I extract
single column from a data frame. Why this behaviour ?

> y <- as.matrix(df[,1:2]); length(row.names(y))
[1] 324
> y <- as.matrix(df[,1:1]); length(row.names(y))
[1] 0

Best regards,

Ryszard



From angel_lul at hotmail.com  Tue Mar  2 19:34:48 2004
From: angel_lul at hotmail.com (Angel -)
Date: Tue, 02 Mar 2004 18:34:48 +0000
Subject: [R] how to delete a matrix column
Message-ID: <LAW11-F14S1NcY6WmX700039351@hotmail.com>

Shouldn't this be in the "An introduction to R", It is explained for a 
vector but I couldn't find it for matrix.

A<-matrix(1:25,5,5)
A<-A[,-2] # delete column 2
A<-A[,-c(1,4)] #delete columns 1 and 4


Angel


Aimin Yan wrote:

>Hello,
>I am new to R, How to delete a matrix column.
>Thanks,
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>.
>



From rolf at math.unb.ca  Tue Mar  2 19:38:58 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 2 Mar 2004 14:38:58 -0400 (AST)
Subject: [R] how to delete a matrix column
Message-ID: <200403021838.i22IcwtJ006709@erdos.math.unb.ca>

Aimin Yan wrote:

> I am new to R, How to delete a matrix column.

e.g. y <- x[,-5] deletes column 5

I was going to point out to the inquirer where this feature
is documented --- but I can't bleedin' find it!!!

Can anyone tell me?  I tried ?"[" but got no joy that
I could discern.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From MSchwartz at medanalytics.com  Tue Mar  2 20:06:39 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 02 Mar 2004 13:06:39 -0600
Subject: [R] row.names are dropped when extracting one column ?
In-Reply-To: <OFAF7D5F89.5FC724F8-ON85256E4B.006292B6-85256E4B.00630128@EU.novartis.net>
References: <OFAF7D5F89.5FC724F8-ON85256E4B.006292B6-85256E4B.00630128@EU.novartis.net>
Message-ID: <1078254399.29932.163.camel@localhost.localdomain>

On Tue, 2004-03-02 at 11:59, ryszard.czerminski at pharma.novartis.com
wrote:
> Apparently row names are dropped when I extract
> single column from a data frame. Why this behaviour ?
> 
> > y <- as.matrix(df[,1:2]); length(row.names(y))
> [1] 324
> > y <- as.matrix(df[,1:1]); length(row.names(y))
> [1] 0
> 
> Best regards,
> 
> Ryszard


Please see R-FAQ 7.7:

"Why do my matrices lose dimensions?"

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Tue Mar  2 20:18:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 19:18:04 +0000 (GMT)
Subject: [R] row.names are dropped when extracting one column ?
In-Reply-To: <OFAF7D5F89.5FC724F8-ON85256E4B.006292B6-85256E4B.00630128@EU.novartis.net>
Message-ID: <Pine.LNX.4.44.0403021913540.31039-100000@gannet.stats>

On Tue, 2 Mar 2004 ryszard.czerminski at pharma.novartis.com wrote:

> Apparently row names are dropped when I extract
> single column from a data frame. Why this behaviour ?
> 
> > y <- as.matrix(df[,1:2]); length(row.names(y))
> [1] 324
> > y <- as.matrix(df[,1:1]); length(row.names(y))
> [1] 0

Why are you converting a subsetted data frame to a matrix?  df[, 1:2] is a 
data frame, and df[, 1:1] is a single column (most likely a vector). The 
latter is not going to have row names, but it might have names.  It 
happens that in S the row names are not copied across as names.

row.names applies to a data frame, and not to a matrix, strictly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From swanson at yellowstoneresearch.org  Tue Mar  2 20:18:43 2004
From: swanson at yellowstoneresearch.org (Alan Swanson)
Date: Tue, 02 Mar 2004 12:18:43 -0700
Subject: [R] gls anova wald test calculations
Message-ID: <6.0.0.22.1.20040302121800.01b01570@www.yellowstoneresearch.org>

I have a question about the Wald test F-statistics that are calculated when 
the anova() command is used on a singular gls or lme object.  As I recall 
from my linear models class, the Wald test examines H0: C'B = d0 vs Ha: C'B 
!= d0.  Does anybody know how this C matrix is constructed in R?  Is there 
a way to see the C matrix that R is using?  In my situation, I'm looking at 
marginal ANOVA tables and would like to know how the other factors, and in 
particular the interaction terms, are being treated for a given row of the 
table.  I'm assuming that some sort of averaging takes place but can't find 
anything about it in the documentation I have.  Thanks, Alan



From ripley at stats.ox.ac.uk  Tue Mar  2 20:21:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 19:21:33 +0000 (GMT)
Subject: [R] how to delete a matrix column
In-Reply-To: <200403021838.i22IcwtJ006709@erdos.math.unb.ca>
Message-ID: <Pine.LNX.4.44.0403021918460.31039-100000@gannet.stats>

On Tue, 2 Mar 2004, Rolf Turner wrote:

> Aimin Yan wrote:
> 
> > I am new to R, How to delete a matrix column.
> 
> e.g. y <- x[,-5] deletes column 5
> 
> I was going to point out to the inquirer where this feature
> is documented --- but I can't bleedin' find it!!!
> 
> Can anyone tell me?  I tried ?"[" but got no joy that
> I could discern.

`An Introduction to R', for one.

The help page for "[" is long enough, but needs to be split/have some 
xrefs added for things like this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aarditi at lighthouse.org  Tue Mar  2 21:27:00 2004
From: aarditi at lighthouse.org (Arditi, Aries)
Date: Tue, 2 Mar 2004 15:27:00 -0500
Subject: [R] possible bug in aov?
Message-ID: <7300144209720B45B0773007B0E33F1E18C754@USJRAEXCH01.Lighthouse.PRI>

Hi, I'm interested in doing a repeated measures anova using aov.  The procedure is nicely described in section 6.7.1, pp. 24-27 of Baron and Li's "Notes on the use of R for psychology experiments and
questionnaires," and I've reproduced their example exactly.

My own problem is almost identical to theirs:

rawdat<-c(1.6530074e+001, 1.2124254e+001, 1.0040371e+001, 1.5317610e+001, 1.2332377e+001,
1.0102947e+001, 1.7087520e+001, 1.1692890e+001, 1.1809286e+001, 1.4748297e+001, 1.0717683e+001,
8.7908203e+000, 1.3140706e+001, 9.7947745e+000, 8.3588329e+000, 1.5623756e+001, 9.6382132e+000,
8.7209537e+000, 1.5321620e+001, 1.1347325e+001,  8.5177789e+000,  1.3271254e+001,  9.7376040e+000,
8.7803153e+000  , 1.3155236e+001, 1.0157526e+001, 8.4003486e+000, 1.3083896e+001, 9.6644615e+000,
8.1586026e+000, 1.2167609e+001 , 9.1319943e+000, 7.4251013e+000, 1.3284386e+001, 9.1596335e+000,
7.9175876e+000,  1.1876537e+002,  7.8831935e+001,  6.2203784e+001,  1.0729023e+002,  7.3790904e+001, 5.8588328e+001,  1.1890329e+002,  6.6345430e+001,  5.3117505e+001,  3.7261647e+002, 2.4538572e+002,  2.2372152e+002,  3.2602801e+002,  2.3267124e+002,  2.0944016e+002,  2.9755054e+002,  2.3970708e+002,  2.2379642e+002)

sample.df <- data.frame(dep.variable=rawdat,
subject=factor(rep(paste("subj",1:6, sep=""),each=9)),
factor1=factor(rep(rep(c("fac1level1","fac1level2","fac1level3"),each=6),3)),
factor2=factor(rep(c("fac2level1","fac2level2","fac2level3"),each=18))
)
sample.aov <- aov(dep.variable ~ factor1 * factor2 + Error(subject/(factor1+factor2)), data=df)

But the aov function returns an error:

Error in "names<-.default"(`*tmp*`, value = nmstrata) :
        names attribute must be the same length as the vector

I am using R1.8.1.

Any help or insight on a workaround would be appreciated.

Aries Arditi
Senior Fellow
Arlene R. Gordon Research Institute
Lighthouse International



From vograno at evafunds.com  Tue Mar  2 21:45:12 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 2 Mar 2004 12:45:12 -0800
Subject: [R] error() and C++ destructors
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3A84@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040302/67ee7bba/attachment.pl

From feh3k at spamcop.net  Tue Mar  2 18:31:33 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Tue, 2 Mar 2004 12:31:33 -0500
Subject: [R] boxplot notches
In-Reply-To: <Pine.A41.4.58.0403020719130.58162@homer40.u.washington.edu>
References: <16451.33889.108292.454206@gargle.gargle.HOWL>
	<Pine.LNX.4.44.0403011959360.11548-100000@gannet.stats>
	<20040301172941.A115@jessie.research.bell-labs.com>
	<40445F6B.4020901@uni-jena.de>
	<910A7B49-6C5C-11D8-81B4-000A95B0CE8A@bham.ac.uk>
	<Pine.A41.4.58.0403020719130.58162@homer40.u.washington.edu>
Message-ID: <20040302123133.7ca63170.feh3k@spamcop.net>

On Tue, 2 Mar 2004 07:24:41 -0800 (PST)
Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Tue, 2 Mar 2004, P. B. Pynsent wrote:
> 
> > A Google search showed  that all this was discussed in April 1988 with
> > an extensive reply to the question from M Maechler.
> > I, as a non-statistician, blindly believed what was written in the
> > boxplot() help file, I am sure many would be grateful to this help
> > being modified.
> >
> > I still do not understand why , 6 years later with GHz processors,
> > boxplot() could not have an option to produce exact intervals. After
> > all,  a range option is offered for the whiskers.
> > At least then non-overlapping notches would have some meaning,
> > wouldn't they?
> 
> Well, they would have *some* meaning, but it would be hard to say
> exactly what. There isn't an exact confidence interval for  the
> difference in medians, so you can't find a level for two confidence
> intervals that corresponds to a specified level for the test of equality
> of medians.
> 
> 	-thomas

I have been using the following approximation to get a confidence interval
for the difference in two medians.

1. Compute the nonparametric confidence interval for each median (which
selects 2 order statistics)

2. Solve for the standard error that, using the normal approximation,
would yield the same confidence interval width as the nonparametric
interval

3. For the confidence limits for the difference use a normal approximation
with a standard error equal to the square root of the sum of squares of
the standard errors computed in step 2

The S code for steps 1 and 2 is:

    y <- sort(y[!is.na(y)])
    n <- length(y)
    r <- pmin(qbinom(c(.025,.975), n, .5) + 1, n)  ## Exact 0.95 C.L.
    w <- y[r[2]] - y[r[1]]                         ## Width of C.L.
    var.med <- ((w/1.96)^2)/4      ## Approximate variance of median

-Frank Harrell



From p.murrell at auckland.ac.nz  Tue Mar  2 22:17:28 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 03 Mar 2004 10:17:28 +1300
Subject: [R] dev.print and X11(canvas = "black")
References: <Pine.LNX.4.44.0403021653520.30708-100000@gannet.stats>
Message-ID: <4044F9E8.4030001@stat.auckland.ac.nz>

Hi


Prof Brian Ripley wrote:
 > On Tue, 2 Mar 2004, Jeffrey J. Hallman wrote:
 >
 >
 >>Jim Lemon made essentially the same suggestion: run the code that
 >>created the plot twice, once with X11 as the active device, and again
 >>with postscript as the active device.
 >>
 >>The trouble with this is that is doesn't handle plots that the user has
 >>edited.  I have some charts that users add things to interactively,
 >>using combinations of locator(), text(), mtext(), and arrows().  There's
 >>no nice way to know what a user has done to a plot before he attempts to
 >>print it.
 >
 >
 > Yes, see the sections on `Graphics hardcopy' in section A.1 of MASS4 for
 > exactly this comment: it is hardly news.
 >
 > You could record what the users do as they do it by using your own
 > versions of these functions.
 >
 >
 >>I know that R is internally maintaining a display list.  Is there a way
 >>to access that without dropping into C?  And what's in it, anyway?
 >
 >
 > Not really (recordPlot etc does), and not documented AFAIK (but Paul
 > Murrell will be able to correct me if I am wrong here).  However, I am
 > pretty sure that it has the actual plot colours and not the colour 
numbers
 > in it.


Just to confirm, the current internal display list should be considered 
a black box;  the only user-level access to it is record/replayPlot() 
and dev.copy/print/control().

Paul

p.s. If you really want to know what's in the display list, you should 
start reading at GEcreateSnapshot in R/src/main/engine.c
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From solares at unsl.edu.ar  Tue Mar  2 22:21:02 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Tue, 2 Mar 2004 18:21:02 -0300 (ART)
Subject: [R] zoom graphics
Message-ID: <49035.170.210.173.216.1078262462.squirrel@inter17.unsl.edu.ar>

Hi, i don't understand how i cant zoom in and zoom out a graphics (plots)
exist a package for that? Thanks Ruben



From ripley at stats.ox.ac.uk  Tue Mar  2 22:26:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Mar 2004 21:26:32 +0000 (GMT)
Subject: [R] possible bug in aov?
In-Reply-To: <7300144209720B45B0773007B0E33F1E18C754@USJRAEXCH01.Lighthouse.PRI>
Message-ID: <Pine.LNX.4.44.0403022120040.31295-100000@gannet.stats>

Two errors of yours are apparent in your example:

1) df is a function, and sample.df is not used in your code.  I presume 
you actually used data=sample.df.

2) factor2 does not vary within subject so your model makes no sense.

It is the second that leads, correctly, to an error message (if an arcane 
one).

On Tue, 2 Mar 2004, Arditi, Aries wrote:

> Hi, I'm interested in doing a repeated measures anova using aov.  The procedure is nicely described in section 6.7.1, pp. 24-27 of Baron and Li's "Notes on the use of R for psychology experiments and
> questionnaires," and I've reproduced their example exactly.
> 
> My own problem is almost identical to theirs:
> 
> rawdat<-c(1.6530074e+001, 1.2124254e+001, 1.0040371e+001, 1.5317610e+001, 1.2332377e+001,
> 1.0102947e+001, 1.7087520e+001, 1.1692890e+001, 1.1809286e+001, 1.4748297e+001, 1.0717683e+001,
> 8.7908203e+000, 1.3140706e+001, 9.7947745e+000, 8.3588329e+000, 1.5623756e+001, 9.6382132e+000,
> 8.7209537e+000, 1.5321620e+001, 1.1347325e+001,  8.5177789e+000,  1.3271254e+001,  9.7376040e+000,
> 8.7803153e+000  , 1.3155236e+001, 1.0157526e+001, 8.4003486e+000, 1.3083896e+001, 9.6644615e+000,
> 8.1586026e+000, 1.2167609e+001 , 9.1319943e+000, 7.4251013e+000, 1.3284386e+001, 9.1596335e+000,
> 7.9175876e+000,  1.1876537e+002,  7.8831935e+001,  6.2203784e+001,  1.0729023e+002,  7.3790904e+001, 5.8588328e+001,  1.1890329e+002,  6.6345430e+001,  5.3117505e+001,  3.7261647e+002, 2.4538572e+002,  2.2372152e+002,  3.2602801e+002,  2.3267124e+002,  2.0944016e+002,  2.9755054e+002,  2.3970708e+002,  2.2379642e+002)

Where did those e+001 etc come from?  Not from R, AFAIK.

> sample.df <- data.frame(dep.variable=rawdat,
> subject=factor(rep(paste("subj",1:6, sep=""),each=9)),
> factor1=factor(rep(rep(c("fac1level1","fac1level2","fac1level3"),each=6),3)),
> factor2=factor(rep(c("fac2level1","fac2level2","fac2level3"),each=18))
> )
> sample.aov <- aov(dep.variable ~ factor1 * factor2 + Error(subject/(factor1+factor2)), data=df)
> 
> But the aov function returns an error:
> 
> Error in "names<-.default"(`*tmp*`, value = nmstrata) :
>         names attribute must be the same length as the vector
> 
> I am using R1.8.1.
> 
> Any help or insight on a workaround would be appreciated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hodgess at gator.uhd.edu  Tue Mar  2 22:56:07 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Tue, 2 Mar 2004 15:56:07 -0600
Subject: [R] time series with diff. y axes
Message-ID: <200403022156.i22Lu7p15570@gator.dt.uh.edu>

Dear R People:

Someone had asked a question about plotting 2 time series
with different y-axes.  There is a nice example in S+, but here is
a toy example (modified from the S+ example) which someone may find
useful:

> y1 <- rnorm(48)
> y2 <- rnorm(48)
> y1.ts <- ts(y1,start=c(1996,1),freq=12)
> y2.ts <- ts(y2,start=c(1996,1),freq=12)
> par(mar=rep(5,4))
> plot(y1.ts,ylab="Horses")
> par(new=T)
> plot(window(y2.ts,start(y1.ts),end(y1.ts)),lty=2,
+ axes=F,ylab=" ")
> axis(4)
> mtext("Hounds",side=4,line=3)
> title(main="Horses and Hounds")
> 

Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From rxg218 at psu.edu  Tue Mar  2 22:52:20 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 02 Mar 2004 16:52:20 -0500
Subject: [R] using object reference
Message-ID: <1078264340.1953.4.camel@ra.chem.psu.edu>

Hi,
  I have read the previous thread on using references to objects in a
function but the solution suggested does'nt seem to be working.

basically I have an object x which has an attribute a containing some
text. I would like to pass x to a function which will change the
attribute a with some new text and have the change visible when the
function exits.

something like

attr(x,'a') <- 'some text'
f <- function(z) {
  attr(z,'a') <- 'some new text'
}

So that when I call f(x)
attr(x,'a')

gives

'some new text'

I went by the example below 
g <- function(z) eval(eval(substitute(expression(z[1] <<- z[1]+1))))
a <- 1:5
g(a)  # increments first element of a by 1
a     # c(2,2,3,4,5)

replcing the innermost bracket with attr(z,'a') <- 'some new text' but
the the after returning from the function the attribute of x does not
get changed.

Could anybody point out how I could achieve this? Do I need to use the
R.oo package or can this be done without external packages?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
How I wish I were what I was when I wished I were what I am.



From abunn at montana.edu  Tue Mar  2 23:16:25 2004
From: abunn at montana.edu (Andy Bunn)
Date: Tue, 2 Mar 2004 15:16:25 -0700
Subject: [R] using object reference
In-Reply-To: <1078264340.1953.4.camel@ra.chem.psu.edu>
Message-ID: <000001c400a4$03aa1050$78f05a99@msu.montana.edu>

Don't you just need to use return?

x <- 1
attr(x,'a') <- 'some text'
f <- function(z) {
  attr(z,'a') <- 'some new text'
  return(z)
}
y <- f(x)
y



From rxg218 at psu.edu  Tue Mar  2 23:22:08 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 02 Mar 2004 17:22:08 -0500
Subject: [R] using object reference
In-Reply-To: <000001c400a4$03aa1050$78f05a99@msu.montana.edu>
References: <000001c400a4$03aa1050$78f05a99@msu.montana.edu>
Message-ID: <1078266128.2120.2.camel@ra.chem.psu.edu>

On Tue, 2004-03-02 at 17:16, Andy Bunn wrote:
> Don't you just need to use return?
> 
> x <- 1
> attr(x,'a') <- 'some text'
> f <- function(z) {
>   attr(z,'a') <- 'some new text'
>   return(z)
> }
> y <- f(x)
> y

Well what I'm trying to is to pop up a Tk window and allow the user to
add some text which gets placed into the attribute of the variable
passed to the function.

So basically i have a Tk toplevel window with a text area and a button.
When the button is clicked the callback gets the text area contents and
places it into the attribute of the supplied variable and destroys the
window

I tried returning the variable but it did'nt work (and it seemed less
elegant than allowing the function directly modify the passed in
variable)

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Chemistry professors never die, they just fail to react.



From tblackw at umich.edu  Wed Mar  3 00:19:19 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Tue, 2 Mar 2004 18:19:19 -0500 (EST)
Subject: [R] using object reference
In-Reply-To: <1078264340.1953.4.camel@ra.chem.psu.edu>
References: <1078264340.1953.4.camel@ra.chem.psu.edu>
Message-ID: <Pine.SOL.4.58.0403021811130.5582@millipede.gpcc.itd.umich.edu>

Rajarshi  -

Read  help("<-")  to see what's going on in the example
from previous emails.  In your new function, in addition
to setting  attr(z,'a')  you will need to assign the
result in the global environment.

However, please think hard about whether the final result
couldn't be achieved in some much more straightforward
manner than setting an attribute on an existing object.
That sounds like asking for trouble down the road.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 2 Mar 2004, Rajarshi Guha wrote:

> Hi,
>   I have read the previous thread on using references to objects in a
> function but the solution suggested does'nt seem to be working.
>
> basically I have an object x which has an attribute a containing some
> text. I would like to pass x to a function which will change the
> attribute a with some new text and have the change visible when the
> function exits.
>
> something like
>
> attr(x,'a') <- 'some text'
> f <- function(z) {
>   attr(z,'a') <- 'some new text'
> }
>
> So that when I call f(x)
> attr(x,'a')
>
> gives
>
> 'some new text'
>
> I went by the example below
> g <- function(z) eval(eval(substitute(expression(z[1] <<- z[1]+1))))
> a <- 1:5
> g(a)  # increments first element of a by 1
> a     # c(2,2,3,4,5)
>
> replcing the innermost bracket with attr(z,'a') <- 'some new text' but
> the the after returning from the function the attribute of x does not
> get changed.
>
> Could anybody point out how I could achieve this? Do I need to use the
> R.oo package or can this be done without external packages?
>
> Thanks,
>
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> How I wish I were what I was when I wished I were what I am.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From alistair.campbell at jcu.edu.au  Wed Mar  3 00:41:11 2004
From: alistair.campbell at jcu.edu.au (Alistair Campbell)
Date: Wed, 03 Mar 2004 09:41:11 +1000
Subject: [R] Problem untarring R-1.8.1.tgz
Message-ID: <40451B97.4070206@jcu.edu.au>

Hi,

I have been trying to untar the source file for R and having no success.

I have used decompression utilities that handle .tar and .tgz file but I 
keep getting an error that there is a "Header CRC Error".

So, I am wondering if this is just me and I am not using the right 
decompression utility or whether there is an error in the headers for 
the zip file.

Anyone have any advice?

Alistair Campbell
James Cook University



From alistair.campbell at jcu.edu.au  Wed Mar  3 00:47:41 2004
From: alistair.campbell at jcu.edu.au (Alistair Campbell)
Date: Wed, 03 Mar 2004 09:47:41 +1000
Subject: [R] Re: Untarring R-1.8.1.tgz
Message-ID: <40451D1D.9010009@jcu.edu.au>

Hi,

Disregard my post. I used another utility and it worked perfectly. The 
others are obviously not up to the task!

Cheers

Alistair Campbell



From p.dalgaard at biostat.ku.dk  Wed Mar  3 00:53:56 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 00:53:56 +0100
Subject: [R] Problem untarring R-1.8.1.tgz
In-Reply-To: <40451B97.4070206@jcu.edu.au>
References: <40451B97.4070206@jcu.edu.au>
Message-ID: <x2k722byaz.fsf@biostat.ku.dk>

Alistair Campbell <alistair.campbell at jcu.edu.au> writes:

> Hi,
> 
> I have been trying to untar the source file for R and having no success.
> 
> I have used decompression utilities that handle .tar and .tgz file but
> I keep getting an error that there is a "Header CRC Error".
> 
> So, I am wondering if this is just me and I am not using the right
> decompression utility or whether there is an error in the headers for
> the zip file.
> 
> Anyone have any advice?

I'd suspect a download error. Some browsers do strange things to
binary files. (Heck, they do strange things with text files all the
time.) Otherwise, you might need to give us a bit more information
about what you did and tried exactly.

        -p 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Matthew.Kelly at csiro.au  Wed Mar  3 01:54:17 2004
From: Matthew.Kelly at csiro.au (Matthew.Kelly@csiro.au)
Date: Wed, 3 Mar 2004 11:54:17 +1100
Subject: [R] R2HTML adding caption to data-frame
Message-ID: <39B7E7009F2DD840AA747934AB551E69184846@exnsw1-arm.nsw.csiro.au>

Hi,
I was wondering if there was an easy way to add a caption to a data frame when it is being sent to a html page using R2HTML. 

currently the following command will produce a reasonably formatted table in a html file
HTML(mydataframe)

It would be good if you could send caption with the table something like 
HTML(lCaption="my table caption",mydataframe) # this works but leaves a blank line and caption is outside the table definition in the html source produced



From mvdv at spamcop.net  Wed Mar  3 04:10:20 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Wed, 3 Mar 2004 14:10:20 +1100
Subject: [R] Adding text (coefts) to pairs panels
Message-ID: <000401c400cd$0fe41cc0$344610ac@FEB0480>

Hi,
First of all, thanks for the efforts of all the developers and contributors
- I'm very new to R and at the moment am just using it to create some
graphics, but it seems to be quite powerful.
I've googled the archives and wasn't able to find anyhting that dealt with
this problem, so would appreciate any suggestions/tips.

In a pairs plot I'd like to plot a linear regression line in each panel and,
also in each panel, I'd like to have some text in the panel showing the
coefts, etc.
I'm able to get the regressions line plotted, but not the text/coefficients.
Using one of the R examples, example(pairs), I have got this far:

panel.myfitline<-function(x, y, digits=2, prefix="", cex.cor, ...)
{
    res<-panel.smooth(x,y, col.smooth="blue", ...)
    reg <- coef(lm(y ~ x))
    abline(coef=reg,untf=F)
    const<-format(reg[1], trim = FALSE, digits = NULL, nsmall = 0, justify =
"left", ...)
    const<-paste(prefix, const, sep="")
    slope<-format(reg[2], trim = FALSE, digits = NULL, nsmall = 0, justify =
"left", ...)
    slope<-paste(prefix, slope, sep="")
    if(missing(cex.cor)) cex <- 0.8/strwidth(const)
    text(0.1, 0.5, const, cex=cex)
    if(missing(cex.cor)) cex <- 0.8/strwidth(slope)
    text(0.5, 0.1, slope, cex=cex)
}
pairs(USJudgeRatings[1:5], lower.panel=panel.smooth, diag.panel=panel.hist,
upper.panel=panel.myfitline)

Mark Van De Vyver
PhD
Lecturer
Finance Discipline
School of Business
Faculty of Economics and Business
Economics & Business Building H69
The University of Sydney  
Sydney  NSW  2006  Australia
Telephone: +61 2 9351-6452
Fax: +61 2 9351-6461
Mobile: 0428 281407

mailto:mvdv at spamcop.net
http:\\www.econ.usyd.edu.au



From gagongabe at earthlink.net  Wed Mar  3 04:19:16 2004
From: gagongabe at earthlink.net (Gabriel Lawson)
Date: Tue, 2 Mar 2004 19:19:16 -0800
Subject: [R] Rcmd SHLIB
References: <Pine.LNX.4.44.0403010721100.6156-100000@gannet.stats>
Message-ID: <003301c400ce$4f345830$6501a8c0@lankhmar>

Thanks again for the help.  I went through readme.package a couple of times,
but I still get the error below.  It seems to be a little closer as I am no
longer seeing CreateProcess in the output.

Beneath the error, I have pasted my modified MkRules.  I think I am messing
up/missing one or more of the paths in it, but I filled in all those that
seemed obvious to me.  Anyone spot anything wrong?

gabe

C:\Program Files\R\rw1081\src\gnuwin32>make libR.a
dlltool -k --as as   --dllname R.dll --def R.exp --output-lib libR.a
make: *** [libR.a] Error 255

#-*- Makefile -*-

## This files contains tabs: make sure your editor leaves them unchanged.

## ===== configuration macros for building packages or R ===============

# Alternatives MINGW (mingw), CROSS (Linux)

BUILD=MINGW

HELP = YES

ifeq ($(strip $(BUILD)),CROSS)

WINHELP = NO

else

WINHELP = CHM # NO, CHM, BOTH

endif

## ============== configuration macros for building R ===================

LEA_MALLOC=YES

# Set to YES and specify the path if you want to use the ATLAS BLAS.

USE_ATLAS=NO

ATLAS_PATH=/R/ATLAS/lib/WinNT_PIIISSE1

# Where does 'HTML Help Workshop' live? (unused if compiled HTML help is

# not requested. Spaces allowed.)

HHWDIR=C:/Program Files/HTML Help Workshop

# Where does Tcl/Tk live? Spaces allowed.

#TCL_HOME = $(RHOME)/Tcl

TCL_HOME = C:/PROGRA~1/R/rw1081/Tcl

TCL_VERSION = 84

## ====== configuration macros for building installers ===========

# location where Inno Setup 4.x was installed

ISDIR=C:/packages/Inno4

## =================== cross-compilation settings =================

ifeq ($(strip $(BUILD)),CROSS)

# Next two might be i386-mingw32msvc- or i586- depending on the
cross-compiler.

#BINPREF=mingw32-

BINPREF=C:/MinGW/bin

# Set this to where the mingw32 include files are. It must be accurate.

#HEADER=/users/ripley/R/cross-tools/mingw32/include

HEADER=C:/MinGW/include

endif

# path (possibly full path) to same version of R on the host system

#R_EXE=R

R_EXE=C:/PROGRA~1/R/rw1081/bin/R.exe

## =============== end of user-customizable parts ===================

ifeq ($(strip $(BUILD)),MINGW)

BINPREF=

MINGW32CFLAG=

MINGW32LDFLAG=

FLIBS=-lg2c

AWK=gawk

## only safe for gcc >= 3.1

DEPARG=-MM

endif

ifeq ($(strip $(BUILD)),CROSS)

MINGW32CFLAG=-isystem $(HEADER)

MINGW32LDFLAG=

FLIBS=-lg2c

AWK=awk

## BDR's build of cross-compiler is 2.95.3

DEPARG=-M

endif

PERL=perl

RM=rm -f

SED=sed

ECHO=echo

CP=cp

MKDIR=mkdir

CAT=cat

CC=$(BINPREF)gcc $(MINGW32CFLAG)

CXX=$(BINPREF)g++ $(MINGW32CFLAG)

F77=$(BINPREF)g77

AS=$(BINPREF)as

DLL_LDMAIN=gcc

DLL=$(BINPREF)$(DLL_LDMAIN) $(MINGW32LDFLAG)

DLLTOOL=$(BINPREF)dlltool -k --as $(AS)

LINKER=$(BINPREF)gcc $(MINGW32LDFLAG)

AR=$(BINPREF)ar

RANLIB=$(BINPREF)ranlib

NM=$(BINPREF)nm

CPP=$(CC) -E

RESCOMP=$(BINPREF)windres

.SUFFIXES: .c .cc .cpp .C .f .o .a .def .exp .dll .exe .d

.c.d:

@echo "making $@ from $<"

@$(CC) $(DEPARG) $(CFLAGS) $($*-CFLAGS) $< -o $@

.cc.d:

@echo "making $@ from $<"

@$(CXX) $(DEPARG) $(CXXFLAGS) $($*-CXXFLAGS) $< -o $@

.cpp.d:

@echo "making $@ from $<"

@$(CXX) $(DEPARG) $(CXXFLAGS) $($*-CXXFLAGS) $< -o $@

.C.d:

@echo "making $@ from $<"

@$(CXX) $(DEPARG) $(CXXFLAGS) $($*-CXXFLAGS) $< -o $@

.c.o:

$(CC) $(CFLAGS) $($*-CFLAGS) -c $< -o $@

.f.o:

$(F77) $(FFLAGS) $($*-FFLAGS) -c $< -o $@

.cc.o:

$(CXX) $(CXXFLAGS) $($*-CXXFLAGS) -c $< -o $@

.cpp.o:

$(CXX) $(CXXFLAGS) $($*-CXXFLAGS) -c $< -o $@

.C.o:

$(CXX) $(CXXFLAGS) $($*-CXXFLAGS) -c $< -o $@

%.exe:

$(LINKER) $(LINKFLAGS) $($*-LINKFLAGS) -o $@ $^ $($*-LIBS) $(LIBS)

%.dll:

# @$(ECHO) ------- Building $@ from $^ --------

@$(ECHO) LIBRARY $* > $*.def

@$(ECHO) EXPORTS >> $*.def

@$(NM) $^ > Defs

@$(SED) -n '/^........ [BCDRT] _/s/^........ [BCDRT] _/ /p' Defs >> $*.def

@$(RM) Defs

$(DLL) --shared $(DLLFLAGS) $($*-DLLFLAGS) -o $@ $*.def $^ $($*-DLLLIBS)
$(DLLLIBS)

@$(RM) $*.def

lib%.a: %.def

@$(ECHO) -------- Building $@ from $^ --------

$(DLLTOOL) $(DLLTOOLFLAGS) $($*-DLLTOOLFLAGS) --dllname $*.dll --def
$*.def --output-lib lib$*.a

%.a:

# @$(ECHO) -------- Building $@ from $^ --------

$(AR) cr $@ $^

$(RANLIB) $@

%.o: %.rc

$(RESCOMP) $(RESFLAGS) $($*-RESFLAGS) -i $< -o $@

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Gabriel Lawson" <gagongabe at earthlink.net>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, February 29, 2004 11:24 PM
Subject: Re: [R] Rcmd SHLIB


> On Sun, 29 Feb 2004, Gabriel Lawson wrote:
>
> > Thanks tons.  If you get a moment and feel like answering another
question:
> > Regarding the readme.packages file excerpt below:  How can I set the
R_HOME
> > path?  Is it always the R install directory?  Also, is creating libR.a
and
>
> It is where you installed R.
>
> > libRblas.a a neccessary step in creating ANY dll for use with R?
>
> No, but it is a necessary step to using the supplied tools and it is
> needed for many DLLs.
>
> > BEWARE: Don't expect this to work if the path to R_HOME contains spaces.
> >
> > It may work, but we don't recommend it.
> >
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From ggrothendieck at myway.com  Wed Mar  3 04:27:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  2 Mar 2004 22:27:51 -0500 (EST)
Subject: [R] using object reference
Message-ID: <20040303032751.18C9B395D@mprdmxin.myway.com>


The double arrow in the example you cite is essential;
you can't leave it out since that's what tells R to 
search through its parents' environments:

attr(z,'a') <<- 'some new text' 


---
Date:   Tue, 02 Mar 2004 16:52:20 -0500 
From:   Rajarshi Guha <rxg218 at psu.edu>
To:   R <r-help at stat.math.ethz.ch> 
Subject:   [R] using object reference 

 
Hi,
I have read the previous thread on using references to objects in a
function but the solution suggested does'nt seem to be working.

basically I have an object x which has an attribute a containing some
text. I would like to pass x to a function which will change the
attribute a with some new text and have the change visible when the
function exits.

something like

attr(x,'a') <- 'some text'
f <- function(z) {
attr(z,'a') <- 'some new text'
}

So that when I call f(x)
attr(x,'a')

gives

'some new text'

I went by the example below 
g <- function(z) eval(eval(substitute(expression(z[1] <<- z[1]+1))))
a <- 1:5
g(a) # increments first element of a by 1
a # c(2,2,3,4,5)

replcing the innermost bracket with attr(z,'a') <- 'some new text' but
the the after returning from the function the attribute of x does not
get changed.

Could anybody point out how I could achieve this? Do I need to use the
R.oo package or can this be done without external packages?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>;
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
How I wish I were what I was when I wished I were what I am.



From chunlei_ke at yahoo.com  Wed Mar  3 05:20:31 2004
From: chunlei_ke at yahoo.com (Chunlei Ke)
Date: Tue, 2 Mar 2004 20:20:31 -0800 (PST)
Subject: [R] Package ASSIST
Message-ID: <20040303042031.68883.qmail@web10310.mail.yahoo.com>

Dear R-list,

We have put together a package ASSIST, a collection of
R functions for fitting many spline-based
non-parametric/semi-parametric linear/non-linear
fixed/mixed models, including polynomial, periodic,
spherical, thin-plate splines and L splines as well as
smoothing spline ANOVA models. It also coveres some
cases of generalized additive models,  multiple index
models, varying coefficient models, functional linear
models, and self-modeling nonlinear regression models.


The package contains five main R functions:

1. ssr fits non-parametric regression models for 
independent and correlated Gaussian data, and for
independent binomial, Poisson and Gamma data; 

2. slm fits semi-parametric linear mixed-effects
models; 

3. nnr fits non-parametric nonlinear regression
models;

4. snr fits semi-parametric nonlinear regression
models; and
 
5. snm fits semi-parametric nonlinear mixed-effects
models. 

These extend such existing functions in R (S+) as
smooth.spline, gam, nls, lme and nlme.

ASSIST may be obtained at CRAN or

http://www.pstat.ucsb.edu/faculty/yuedong/software
 
Detail information about these functions and many
examples can be found in the manual of this package,
which again can be downloaded from the above site. 

Comments and suggestions are highly appreciated!

Great thanks!

Yuedong Wang 
and Chunlei Ke
 

__________________________________




From ajayshah at mayin.org  Tue Mar  2 19:00:10 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 2 Mar 2004 23:30:10 +0530
Subject: [R] Stuck in trying to convert repetitive code into a function
Message-ID: <20040302180010.GA15763@igidr.ac.in>

Folks,

I have the following repetitive code which works correctly:

   A = read.table(file="junior.csv", sep=",", col.names=c("date", "l"));
   A$date = chron(as.character(A$date), format="m/d/y");
   r.junior = levels2weeklyret(lastfriday, A$date, A$l);
   plot(A$date, A$l, type="l", col="red", main="Junior levels")
   z <- locator(1)

   A = read.table(file="kospi.csv", sep=",", col.names=c("date", "l"));
   A$date = chron(as.character(A$date), format="m/d/y");
   r.kospi = levels2weeklyret(lastfriday, A$date, A$l);
   plot(A$date, A$l, type="l", col="red", main="Kospi levels")
   z <- locator(1)

I tried to write it more nicely as follows:

   eat.a.file <- function(fn, label, lastfriday) {
     f = read.table(file=fn, sep=",", col.names=c("date", "l"));
     f$date = chron(as.character(f$date), format="m/d/y");
     plot(f$date, f$l, type="l", col="red", main=label);
     z <- locator(1);
     return(levels2weeklyret(lastfriday, f$date, f$l));
   }
   r.junior = eat.a.file(fn="junior.csv", label="Junior", lastfriday);
   r.kospi  = eat.a.file(fn="kospi.csv", label="Kospi", lastfriday);

When I do this, I get this error at the first of the two function calls:

Error in vector("double", length) : negative length vectors are not allowed
Execution halted

I have tried hard to experiment with various ways of writing it, but
am truly stumped. Any ideas on what might be going wrong? I am new to
R and have only written a few functions so far.

On a slightly related note, what is the situation in R on passing by
reference versus passing by value? I looked at the standard docs and
they seem to insist that a function can only send back one return
value. How does a function send back multiple things that it has
created?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Mon Mar  1 17:48:33 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 1 Mar 2004 22:18:33 +0530
Subject: [R] Find out the day of week for a chron object?
Message-ID: <20040301164833.GQ689@igidr.ac.in>

I know that this is correct:

    library(chron)
    x = dates("01-03-04", format="d-m-y", out.format="day mon year")
    print(x)

It gives me the string "01 Mar 2004" which is correct.


I also know that I can say:

    print(day.of.week(3,1,2004))

in which case he says 1, for today is monday.


My question is: How do I combine these two!? :-) I have a data file
which is being parsed nicely and read in using the chron() function. I
need to identify fridays and treat them differently. So I need to run
the day.of.week function. But day.of.week() doesn't eat a chron
object, he insists he wants m,d,y. This seems quite odd. Any idea what
I can do?

Thanks,

        -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Wed Mar  3 05:35:12 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 3 Mar 2004 10:05:12 +0530
Subject: [R] Nonparametric test of randomness (Run Test)
Message-ID: <20040303043512.GA16824@igidr.ac.in>

Suppose you have a returns vector r, which contains values like +3,
-2, -7, +3, etc. Then say:

  require(tseries);
  test = runs.test(factor(sign(r)))
  print(test)

If you want to access the prob value of the test from within a
program, this is found in test$p.value

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From edd at debian.org  Wed Mar  3 06:01:51 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 2 Mar 2004 23:01:51 -0600
Subject: [R] Find out the day of week for a chron object?
In-Reply-To: <20040301164833.GQ689@igidr.ac.in>
References: <20040301164833.GQ689@igidr.ac.in>
Message-ID: <20040303050151.GA3628@sonny.eddelbuettel.com>

On Mon, Mar 01, 2004 at 10:18:33PM +0530, Ajay Shah wrote:
> I know that this is correct:
> 
>     library(chron)
>     x = dates("01-03-04", format="d-m-y", out.format="day mon year")
>     print(x)
> 
> It gives me the string "01 Mar 2004" which is correct.
> 
> 
> I also know that I can say:
> 
>     print(day.of.week(3,1,2004))
> 
> in which case he says 1, for today is monday.
> 
> 
> My question is: How do I combine these two!? :-) I have a data file
> which is being parsed nicely and read in using the chron() function. I
> need to identify fridays and treat them differently. So I need to run
> the day.of.week function. But day.of.week() doesn't eat a chron
> object, he insists he wants m,d,y. This seems quite odd. Any idea what
> I can do?

Chron and date are older packages, you may want to use the more recent (and
very powerful) DateTimeClasses

> parsedDate <- strptime("01-03-04", "%d-%d-%y")
> format(parsedDate)
[1] "2004-01-03"
> class(parsedDate)
[1] "POSIXt"  "POSIXlt"
> weekdays(parsedDate)
[1] "Saturday"

Start with help(DateTimeClasses), if you ever used the C functions strptime
and strftime it shouldn't be too foreign.  And do look at the mailing list
archives (and/or Google), as questions get answered on this quite often. 

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From christianlederer at t-online.de  Wed Mar  3 07:17:44 2004
From: christianlederer at t-online.de (Christian Lederer)
Date: Wed, 03 Mar 2004 07:17:44 +0100
Subject: [R] Confusion about coxph and Helmert contrasts
Message-ID: <40457888.3050305@t-online.de>

Hi,

perhaps this is a stupid question, but i need some help about
Helmert contrasts in the Cox model.


I have a survival data frame with an unordered factor `group'
with levels 0 ... 5.
Calculating the Cox model with Helmert contrasts, i expected that
the first coefficient would be the same as if i had used treatment 
contrasts, but this is not true.
I this a error in reasoning, or is it possible, that both contrasts use
a different ordering of the levels?
I am confused about the fact, that coefficients for different levels are 
calculated, depending on weather i include level 0 or not:

If level 0 is included, with both contrasts coxph calculates 
coefficients for group 1 ... group 5.
But if i exclude level 0, coxph calculates coeffficients for
group 2 ... 5 with treatment and coefficients for group 1 ... 4 using
helmert:

 > options(contrasts = c("contr.treatment", "contr.poly"))
 > cox.t <- coxph(Surv(time,cens) ~ as.factor(group), data=data)
 > options(contrasts = c("contr.helmert", "contr.poly"))
 > cox.h <- coxph(Surv(time,cens) ~ as.factor(group), data=data)
 > cox.t$coefficients
as.factor(group)1 as.factor(group)2 as.factor(group)3 as.factor(group)4
        -0.4301351         0.2078431        -0.0694138         0.3442058
as.factor(group)5
         0.1368716
 > cox.h$coefficients
as.factor(group)1 as.factor(group)2 as.factor(group)3 as.factor(group)4
      -0.215067529       0.140970216       0.001170877       0.083426449
as.factor(group)5
       0.021061935

 > data <- data[data$group > 0,]
 > options(contrasts = c("contr.treatment", "contr.poly"))
 > cox.t <- coxph(Surv(time,cens) ~ as.factor(group), data=data)
 > options(contrasts = c("contr.helmert", "contr.poly"))
 > cox.h <- coxph(Surv(time,cens) ~ as.factor(group), data=data)
 > cox.t$coefficients
as.factor(group)2 as.factor(group)3 as.factor(group)4 as.factor(group)5
         0.6399999         0.3612753         0.7762236         0.5688559
 > cox.h$coefficients
as.factor(group)1 as.factor(group)2 as.factor(group)3 as.factor(group)4
        0.31999997        0.01375844        0.11061629        0.02489624

A am absolutely confused; any help is welcome.

Christian :-)



From jeff.hamann at forestinformatics.com  Wed Mar  3 08:48:43 2004
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Tue, 2 Mar 2004 23:48:43 -0800
Subject: [R] why is rcmd shlib including files not in the list?
Message-ID: <002401c400f3$f6e54510$0a00a8c0@rodan>

I've been trying to create a shlib file (using mingw instead of the
cygwin -- after reconfiguring my machine, yet again) and I'm getting the
following behaviour that I think is incorrect:

When using rcmd shlib and explicitly listing the files to include in the
build such as:

C:\Sequential\Code>rcmd shlib --output=optflikam flikam.c as47.f as154.f
as197.f
making corrAdaptiveSim.d from corrAdaptiveSim.c
corrAdaptiveSim.c:65:57: /rosetta/Statistics/R/R-1.8.1/src/include/R.h: No
such file or directory
corrAdaptiveSim.c:66:66:
/rosetta/Statistics/R/R-1.8.1/src/include/Rinternals.h: No such file or
directory
corrAdaptiveSim.c:73:57: /rosetta/Statistics/R/R-1.8.1/src/nmath/qt.c: No
such file or directory
corrAdaptiveSim.c:78:18: as47.c: No such file or directory
corrAdaptiveSim.c:79:19: as197.c: No such file or directory
make: *** [corrAdaptiveSim.d] Error 1

C:\Sequential\Code>


Other files are "automatically" included which should not be included and
the results are:

making corrAdaptiveSim.d from corrAdaptiveSim.c
corrAdaptiveSim.c:65:57: /rosetta/Statistics/R/R-1.8.1/src/include/R.h: No
such file or directory
corrAdaptiveSim.c:66:66:
/rosetta/Statistics/R/R-1.8.1/src/include/Rinternals.h: No such file or
directory
corrAdaptiveSim.c:73:57: /rosetta/Statistics/R/R-1.8.1/src/nmath/qt.c: No
such file or directory
corrAdaptiveSim.c:78:18: as47.c: No such file or directory
corrAdaptiveSim.c:79:19: as197.c: No such file or directory
make: *** [corrAdaptiveSim.d] Error 1

Is this a bug or is there something I'm not doing correctly?

Jeff.


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-754-1428
(cell) 541-740-5988
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From ligges at statistik.uni-dortmund.de  Wed Mar  3 09:14:44 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 03 Mar 2004 09:14:44 +0100
Subject: [R] R2HTML adding caption to data-frame
In-Reply-To: <39B7E7009F2DD840AA747934AB551E69184846@exnsw1-arm.nsw.csiro.au>
References: <39B7E7009F2DD840AA747934AB551E69184846@exnsw1-arm.nsw.csiro.au>
Message-ID: <404593F4.2030509@statistik.uni-dortmund.de>

Matthew.Kelly at csiro.au wrote:

> Hi,
> I was wondering if there was an easy way to add a caption to a data frame when it is being sent to a html page using R2HTML. 
> 
> currently the following command will produce a reasonably formatted table in a html file
> HTML(mydataframe)
> 
> It would be good if you could send caption with the table something like 
> HTML(lCaption="my table caption",mydataframe) # this works but leaves a blank line and caption is outside the table definition in the html source produced
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please send feature requests related to packages to the package maintainer.

   library(help=R2HTML)
...
Maintainer: Eric Lecoutre <lecoutre at stat.ucl.ac.be>
...

I'm sure Eric (in CC) will appreciate your comments.


Uwe Ligges



From p.dalgaard at biostat.ku.dk  Wed Mar  3 09:57:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 09:57:34 +0100
Subject: [R] Confusion about coxph and Helmert contrasts
In-Reply-To: <40457888.3050305@t-online.de>
References: <40457888.3050305@t-online.de>
Message-ID: <x2smgqz4sh.fsf@biostat.ku.dk>

christianlederer at t-online.de (Christian Lederer) writes:

> Hi,
> 
> perhaps this is a stupid question, but i need some help about
> Helmert contrasts in the Cox model.
> 
> 
> I have a survival data frame with an unordered factor `group'
> with levels 0 ... 5.
> Calculating the Cox model with Helmert contrasts, i expected that
> the first coefficient would be the same as if i had used treatment
> contrasts, but this is not true.
> I this a error in reasoning, or is it possible, that both contrasts use
> a different ordering of the levels?
> I am confused about the fact, that coefficients for different levels
> are calculated, depending on weather i include level 0 or not:
> 
> If level 0 is included, with both contrasts coxph calculates
> coefficients for group 1 ... group 5.
> But if i exclude level 0, coxph calculates coeffficients for
> group 2 ... 5 with treatment and coefficients for group 1 ... 4 using
> helmert:
.....
> A am absolutely confused; any help is welcome.

You are not the first... Probably the best way of becoming less
confused is to look at the contrast matrices:


> contr.treatment(5)
  2 3 4 5
1 0 0 0 0
2 1 0 0 0
3 0 1 0 0
4 0 0 1 0
5 0 0 0 1
> contr.helmert(5)
  [,1] [,2] [,3] [,4]
1   -1   -1   -1   -1
2    1   -1   -1   -1
3    0    2   -1   -1
4    0    0    3   -1
5    0    0    0    4

These give the coefficient matrices for mapping from regression
coefficients to individual group levels (determined up to an additive
constant of course). Staring long enough at these you'll realize
that the first treatment contrast is the difference between gr1 and
gr2 but the first Helmert contr. is *half* that difference, etc. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Mar  3 10:01:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Mar 2004 09:01:36 +0000 (GMT)
Subject: [R] why is rcmd shlib including files not in the list?
In-Reply-To: <002401c400f3$f6e54510$0a00a8c0@rodan>
Message-ID: <Pine.LNX.4.44.0403030858030.2177-100000@gannet.stats>

It is not including files, but it is trying to compute dependencies from 
them.  This is changed in R-devel.

Meanwhile, just use a directory containing only the files you are trying 
to include.

*Please* don't post to both R-help and R-devel: I have removed R-devel 
from the Cc: list.

On Tue, 2 Mar 2004, Jeff D. Hamann wrote:

> I've been trying to create a shlib file (using mingw instead of the
> cygwin -- after reconfiguring my machine, yet again) and I'm getting the
> following behaviour that I think is incorrect:
> 
> When using rcmd shlib and explicitly listing the files to include in the
> build such as:
> 
> C:\Sequential\Code>rcmd shlib --output=optflikam flikam.c as47.f as154.f
> as197.f
> making corrAdaptiveSim.d from corrAdaptiveSim.c
> corrAdaptiveSim.c:65:57: /rosetta/Statistics/R/R-1.8.1/src/include/R.h: No
> such file or directory
> corrAdaptiveSim.c:66:66:
> /rosetta/Statistics/R/R-1.8.1/src/include/Rinternals.h: No such file or
> directory
> corrAdaptiveSim.c:73:57: /rosetta/Statistics/R/R-1.8.1/src/nmath/qt.c: No
> such file or directory
> corrAdaptiveSim.c:78:18: as47.c: No such file or directory
> corrAdaptiveSim.c:79:19: as197.c: No such file or directory
> make: *** [corrAdaptiveSim.d] Error 1
> 
> C:\Sequential\Code>
> 
> 
> Other files are "automatically" included which should not be included and
> the results are:
> 
> making corrAdaptiveSim.d from corrAdaptiveSim.c
> corrAdaptiveSim.c:65:57: /rosetta/Statistics/R/R-1.8.1/src/include/R.h: No
> such file or directory
> corrAdaptiveSim.c:66:66:
> /rosetta/Statistics/R/R-1.8.1/src/include/Rinternals.h: No such file or
> directory
> corrAdaptiveSim.c:73:57: /rosetta/Statistics/R/R-1.8.1/src/nmath/qt.c: No
> such file or directory
> corrAdaptiveSim.c:78:18: as47.c: No such file or directory
> corrAdaptiveSim.c:79:19: as197.c: No such file or directory
> make: *** [corrAdaptiveSim.d] Error 1
> 
> Is this a bug or is there something I'm not doing correctly?
> 
> Jeff.
> 
> 
> ---
> Jeff D. Hamann
> Forest Informatics, Inc.
> PO Box 1421
> Corvallis, Oregon USA 97339-1421
> (office) 541-754-1428
> (cell) 541-740-5988
> jeff.hamann at forestinformatics.com
> www.forestinformatics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From scott.newey at szooek.slu.se  Wed Mar  3 10:05:46 2004
From: scott.newey at szooek.slu.se (Scott Newey)
Date: Wed, 3 Mar 2004 10:05:46 +0100
Subject: [R] partial autocorrelation for Rt vs. Nt-1, ......., Nt-h
Message-ID: <000d01c400fe$e90f6550$93640ac1@szooek.slu.se>

Dear list, following a previous querry we are still stuck!

As pointed out by Erin Hodges the "ts" library includes a PACF function
which reports the partial correlation of population density at time t
against lagged population density.

However, what we are trying to calculate is the partial correlation between
rate of population change, Rt=log Nt/Nt-1, against lagged population
densities. Which is the partial rate correlation function propsed by
Berryman & Turchin (1999).

The list archives give show methods for estimating partial correlation for 2
or 3 variables only. Could any list member please suggest how this could be
done for more variables, say Nt-1 to Nt-10.

Thank you,
Scott
=============================================

Hello Tomas!

There are functions for pacf and plot.acf.

They are in library(ts)

Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu

To: R-help at stat.math.ethz.ch
Date: Sat, 28 Feb 2004 19:52:33 +0100
Subject: [R] Stepwise regression and partial correlation for wildlife census
	time series

Dear List;

We are doing a time series analysis of wildlife census data. We use a
stepwise regression of the annual per capita rate of increase against
pervious years population size (log transformed) as suggested by
Berryman & Turchin (2001, Oikos 92:265-270).

How can we obtain the partial correlation coefficients in R to make a
plot of them against the lag as in a standard PACF?

Yours Sincerely

Tomas Willebrand



From Simon.Fear at synequanon.com  Wed Mar  3 10:22:52 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 3 Mar 2004 09:22:52 -0000
Subject: [R] Stuck in trying to convert repetitive code into a function
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02207@synequanon01>

I can't debug this because you haven't said what
levels2weeklyret() does. However, I can offer a tip:
use traceback() straight after the error to see exactly
where it goes wrong. If still baffled, use debug() [takes
a bit of learning].

To return multiple values, return a list (see ?return) or
other multi-component structure.

If I may offer another word of advice: don't use single
letter variable names, especially l which is identical to 1
in many fonts, and don't use f or fn except for 
a function name. Also we are all supposed to use names
like `eatFile` rather than the old-style `eat.file`, though R 
base itself is full of names.somethings and probably 
always will be.

HTH

> -----Original Message-----
> From: Ajay Shah [mailto:ajayshah at mayin.org]
> Sent: 02 March 2004 18:00
> To: r-help
> Subject: [R] Stuck in trying to convert repetitive code into 
> a function
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Folks,
> 
> I have the following repetitive code which works correctly:
> 
>    A = read.table(file="junior.csv", sep=",", 
> col.names=c("date", "l"));
>    A$date = chron(as.character(A$date), format="m/d/y");
>    r.junior = levels2weeklyret(lastfriday, A$date, A$l);
>    plot(A$date, A$l, type="l", col="red", main="Junior levels")
>    z <- locator(1)
> 
>    A = read.table(file="kospi.csv", sep=",", 
> col.names=c("date", "l"));
>    A$date = chron(as.character(A$date), format="m/d/y");
>    r.kospi = levels2weeklyret(lastfriday, A$date, A$l);
>    plot(A$date, A$l, type="l", col="red", main="Kospi levels")
>    z <- locator(1)
> 
> I tried to write it more nicely as follows:
> 
>    eat.a.file <- function(fn, label, lastfriday) {
>      f = read.table(file=fn, sep=",", col.names=c("date", "l"));
>      f$date = chron(as.character(f$date), format="m/d/y");
>      plot(f$date, f$l, type="l", col="red", main=label);
>      z <- locator(1);
>      return(levels2weeklyret(lastfriday, f$date, f$l));
>    }
>    r.junior = eat.a.file(fn="junior.csv", label="Junior", lastfriday);
>    r.kospi  = eat.a.file(fn="kospi.csv", label="Kospi", lastfriday);
> 
> When I do this, I get this error at the first of the two 
> function calls:
> 
> Error in vector("double", length) : negative length vectors 
> are not allowed
> Execution halted
> 
> I have tried hard to experiment with various ways of writing it, but
> am truly stumped. Any ideas on what might be going wrong? I am new to
> R and have only written a few functions so far.
> 
> On a slightly related note, what is the situation in R on passing by
> reference versus passing by value? I looked at the standard docs and
> they seem to insist that a function can only send back one return
> value. How does a function send back multiple things that it has
> created?
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From copellifulvio at yahoo.it  Wed Mar  3 10:43:20 2004
From: copellifulvio at yahoo.it (=?iso-8859-1?q?Fulvio=20Copex?=)
Date: Wed, 3 Mar 2004 10:43:20 +0100 (CET)
Subject: [R] plot(x,y) with errors
Message-ID: <20040303094320.38598.qmail@web25208.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040303/1eddad32/attachment.pl

From Simon.Fear at synequanon.com  Wed Mar  3 10:44:00 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 3 Mar 2004 09:44:00 -0000
Subject: [R] how to delete a matrix column
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02208@synequanon01>

I would recommend here

y <- subset(x, select= -someColName)

which is not xref'd from ?"[" (though the obscure .subset is),
and is not mentioned in the relevant section of the 
Introduction to R either.

I once foolishly offered to amend this section of the manual,
but I have subsequently realised that I haven't the faintest idea
how to even start doing that.

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 02 March 2004 19:22
> To: Rolf Turner
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] how to delete a matrix column
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> On Tue, 2 Mar 2004, Rolf Turner wrote:
> 
> > Aimin Yan wrote:
> > 
> > > I am new to R, How to delete a matrix column.
> > 
> > e.g. y <- x[,-5] deletes column 5
> > 
> > I was going to point out to the inquirer where this feature
> > is documented --- but I can't bleedin' find it!!!
> > 
> > Can anyone tell me?  I tried ?"[" but got no joy that
> > I could discern.
> 
> `An Introduction to R', for one.
> 
> The help page for "[" is long enough, but needs to be split/have some 
> xrefs added for things like this.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From k.wang at auckland.ac.nz  Wed Mar  3 10:50:30 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 3 Mar 2004 22:50:30 +1300
Subject: [R] [OT]Anyone Going to useR! Conference?
Message-ID: <20040303095054.CICA9867.web2-rme.xtra.co.nz@kevinlpt>

[Apologies for the slightly off-topic posting]

Is anyone going to the useR! Conference who's interested in sharing a
hotel room?

Email me privately if you're interested.  I'm thinking of getting to
Vienna on 17th/18th May (Conference starts on 20th) so I have about two
days to look around and enjoy the scenaries....but the exact dates has
not been determined yet.

Cheers,

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
SLC Stats Workshops Co-ordinator
The University of Auckland
New Zealand



From ajayshah at mayin.org  Wed Mar  3 10:55:14 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 3 Mar 2004 15:25:14 +0530
Subject: [R] get.hist.quote - is great, but am I missing something?
Message-ID: <20040303095514.GQ689@igidr.ac.in>

I find it's just great to be able to say:

  library(tseries)
  x <- get.hist.quote(instrument="ongc.ns")

and it gets a full time-series of the stock price of the symbol
ongc.ns from Yahoo quote.

However, once my hopes have been raised by such beauty :-) I get
disappointed when I do

> plot(x)

and the annotation is horrible! The x axis is not labelled as
dates. The default plot method for get.hist.quote should be better,
no?

I was not able to understand the object returned by get.hist.quote. If
I say:

> summary(x)
      Open             High             Low            Close      
 Min.   : 397.0   Min.   : 407.3   Min.   :395.1   Min.   :398.4  
 1st Qu.: 494.2   1st Qu.: 501.4   1st Qu.:482.7   1st Qu.:490.4  
 Median : 614.9   Median : 622.7   Median :600.7   Median :610.0  
 Mean   : 615.6   Mean   : 627.1   Mean   :599.7   Mean   :611.9  
 3rd Qu.: 690.5   3rd Qu.: 707.4   3rd Qu.:676.2   3rd Qu.:691.5  
 Max.   :1000.0   Max.   :1000.0   Max.   :930.0   Max.   :944.9  
 NA's   :  88.0   NA's   :  88.0   NA's   : 88.0   NA's   : 88.0  

there is no mention of a 'time' variable. And, I'm unable to extract
(say) a vector of closing prices - e.g. if I say:

> closingprices <- x$Close
> print(closingprices)
NULL

I guess I'm not understanding the object that get.hist.quote makes. In
general, what are R facilities for discovering what a given object is?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From spencer.graves at pdf.com  Wed Mar  3 10:55:45 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 03 Mar 2004 01:55:45 -0800
Subject: [R] using object reference
In-Reply-To: <20040303032751.18C9B395D@mprdmxin.myway.com>
References: <20040303032751.18C9B395D@mprdmxin.myway.com>
Message-ID: <4045ABA1.6080607@pdf.com>

      Yes, but use sparingly, because any use of "<<-", "assign", etc., 
from within a function to change something other than what appears in 
the standard return from a function call generates "spaghetti code" that 
is difficult to maintain.  A month or a year from now, someone (the 
developer or someone else) may have difficulty understanding how "z" got 
created or changed. 

      Consider the following: 

 > x <- 1
 >
 > f <- function(z) {
+ attr(z,'a') <<- 'some new text'
+ }
 >
 > f(x)
 >
 > z
[1] 1
attr(,"a")
[1] "some new text"
 > x
[1] 1

      If you intend to create an object "z" this way using the function 
"f", that's fine.  However, if you subsequently forget, it can be very 
confusing: 
 > z <- 3
 > f(2)
 > z
[1] 2
attr(,"a")
[1] "some new text"

      Why is "z" no longer "3"?  I didn't obviously "assign" to "z" the 
results of f(2). 

 > z <- "x"
 > g <- function(a){
+  w <- f(a)
+  w2 <- paste("2", w)
+  list(w=w, w2=w2)
+ }
 > g(2)
$w
[1] "some new text"

$w2
[1] "2 some new text"

 > z
[1] 2
attr(,"a")
[1] "some new text"
 >
      Moreover, the resulting code will not transfer to S-Plus 6.2. 

 > f(2)
Problem in f(2): "<<-" not allowed in replacements: see ?"<<-"
Use traceback() to see the call stack

      I suggest you use the standard function return mechanism, and not 
try to melt gracefully through the wall. 

      hope this helps.  spencer graves

Gabor Grothendieck wrote:

>The double arrow in the example you cite is essential;
>you can't leave it out since that's what tells R to 
>search through its parents' environments:
>
>attr(z,'a') <<- 'some new text' 
>
>
>---
>Date:   Tue, 02 Mar 2004 16:52:20 -0500 
>From:   Rajarshi Guha <rxg218 at psu.edu>
>To:   R <r-help at stat.math.ethz.ch> 
>Subject:   [R] using object reference 
>
> 
>Hi,
>I have read the previous thread on using references to objects in a
>function but the solution suggested does'nt seem to be working.
>
>basically I have an object x which has an attribute a containing some
>text. I would like to pass x to a function which will change the
>attribute a with some new text and have the change visible when the
>function exits.
>
>something like
>
>attr(x,'a') <- 'some text'
>f <- function(z) {
>attr(z,'a') <- 'some new text'
>}
>
>So that when I call f(x)
>attr(x,'a')
>
>gives
>
>'some new text'
>
>I went by the example below 
>g <- function(z) eval(eval(substitute(expression(z[1] <<- z[1]+1))))
>a <- 1:5
>g(a) # increments first element of a by 1
>a # c(2,2,3,4,5)
>
>replcing the innermost bracket with attr(z,'a') <- 'some new text' but
>the the after returning from the function the attribute of x does not
>get changed.
>
>Could anybody point out how I could achieve this? Do I need to use the
>R.oo package or can this be done without external packages?
>
>Thanks,
>
>-------------------------------------------------------------------
>Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>;
>GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
>-------------------------------------------------------------------
>How I wish I were what I was when I wished I were what I am.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Wed Mar  3 11:06:41 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 03 Mar 2004 02:06:41 -0800
Subject: [R] plot(x,y) with errors
In-Reply-To: <20040303094320.38598.qmail@web25208.mail.ukl.yahoo.com>
References: <20040303094320.38598.qmail@web25208.mail.ukl.yahoo.com>
Message-ID: <4045AE31.6050503@pdf.com>

      Does the following do what you want: 

 > x<-c(1,2,3,4,5,6,7,8,9,10)
 > y<-c(1.3,2.5,4.6,5.3,5.9,6.7,7.4,8.5,9.4,10.4)
 > erry<-c(0.2,0.3,0.2,0.1,0.4,0.2,0.3,0.4,0.3,0.2)
 >
 > plot(x, y)
 >
 > n <- length(x)
 > Y <- array(c(y-erry, y+erry), dim=c(n,2))
 > for(i in 1:n)
+  lines(rep(x[i],2), Y[i,])

      Others may have more elegant solutions, but this is simple enough 
for me to understand AND produces essentially the same result in R 1.8.1 
and S-Plus 6.2. 

      hope this helps.  spencer graves

Fulvio Copex wrote:

>Dear all,
>I have 2 variables,
>x<-c(1,2,3,4,5,6,7,8,9,10)
>y<-c(1.3,2.5,4.6,5.3,5.9,6.7,7.4,8.5,9.4,10.4)
>each point of the y variable has an error:
>erry<-(0.2,0.3,0.2,0.1,0.4,0.2,0.3,0.4,0.3,0.2)
>how to plot(x,y) with the errors segments?
>Thank you,
>Copex 
>
>
>
>
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ligges at statistik.uni-dortmund.de  Wed Mar  3 11:13:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 03 Mar 2004 11:13:37 +0100
Subject: [R] plot(x,y) with errors
In-Reply-To: <20040303094320.38598.qmail@web25208.mail.ukl.yahoo.com>
References: <20040303094320.38598.qmail@web25208.mail.ukl.yahoo.com>
Message-ID: <4045AFD1.80108@statistik.uni-dortmund.de>

Fulvio Copex wrote:

> Dear all,
> I have 2 variables,
> x<-c(1,2,3,4,5,6,7,8,9,10)
> y<-c(1.3,2.5,4.6,5.3,5.9,6.7,7.4,8.5,9.4,10.4)
> each point of the y variable has an error:
> erry<-(0.2,0.3,0.2,0.1,0.4,0.2,0.3,0.4,0.3,0.2)
> how to plot(x,y) with the errors segments?
> Thank you,
> Copex 


See ?segments.
Are the errors intended to be mean-centered ???

Uwe Ligges




> 
> 
> 
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Wed Mar  3 11:32:36 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 03 Mar 2004 10:32:36 +0000
Subject: [R] zoom graphics
In-Reply-To: <49035.170.210.173.216.1078262462.squirrel@inter17.unsl.edu.ar>
References: <49035.170.210.173.216.1078262462.squirrel@inter17.unsl.edu.ar>
Message-ID: <4045B444.3080202@lancaster.ac.uk>

solares at unsl.edu.ar wrote:
> Hi, i don't understand how i cant zoom in and zoom out a graphics (plots)
> exist a package for that? Thanks Ruben
> 

  I dont think you can do it quite like you can zoom in and out in a 
program like 'photoshop'. All you can really do is redraw the plot with 
a different set of X and Y limits.

Example:

# some data
  xy=list(x=rnorm(100),y=rnorm(100))

# default plot shows all the data:
  plot(xy)

# define an interactive 'zoom' function:
zoom=function(){reg=locator(2);plot(reg,type='n')}

# click two points at the corners of the zoom region:
zoom()

# add the points - dont use 'plot' since this resets the axes:
points(xy)

  If you've got a complex plot with lots of things on it, then yes, you 
have to redraw it all again, but then you probably should have put that 
complex plot into a single function!

Barry



From Giles.Heywood at CommerzbankIB.com  Wed Mar  3 11:37:16 2004
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Wed, 3 Mar 2004 10:37:16 -0000 
Subject: [R] get.hist.quote - is great, but am I missing something?
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF7478@xmx8lonib.lonib.commerzbank.com>

If you use priceIts() in package 'its' (Irregular Time Series), you get
similar functionality, and the labelling etc in plot() recognizes the
calendar.  You can also do further calendar-based extractions, etc.

- Giles

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ajay Shah
> Sent: 03 March 2004 09:55
> To: r-help
> Subject: [R] get.hist.quote - is great, but am I missing something?
> 
> 
> I find it's just great to be able to say:
> 
>   library(tseries)
>   x <- get.hist.quote(instrument="ongc.ns")
> 
> and it gets a full time-series of the stock price of the symbol
> ongc.ns from Yahoo quote.
> 
> However, once my hopes have been raised by such beauty :-) I get
> disappointed when I do
> 
> > plot(x)
> 
> and the annotation is horrible! The x axis is not labelled as
> dates. The default plot method for get.hist.quote should be better,
> no?
> 
> I was not able to understand the object returned by get.hist.quote. If
> I say:
> 
> > summary(x)
>       Open             High             Low            Close      
>  Min.   : 397.0   Min.   : 407.3   Min.   :395.1   Min.   :398.4  
>  1st Qu.: 494.2   1st Qu.: 501.4   1st Qu.:482.7   1st Qu.:490.4  
>  Median : 614.9   Median : 622.7   Median :600.7   Median :610.0  
>  Mean   : 615.6   Mean   : 627.1   Mean   :599.7   Mean   :611.9  
>  3rd Qu.: 690.5   3rd Qu.: 707.4   3rd Qu.:676.2   3rd Qu.:691.5  
>  Max.   :1000.0   Max.   :1000.0   Max.   :930.0   Max.   :944.9  
>  NA's   :  88.0   NA's   :  88.0   NA's   : 88.0   NA's   : 88.0  
> 
> there is no mention of a 'time' variable. And, I'm unable to extract
> (say) a vector of closing prices - e.g. if I say:
> 
> > closingprices <- x$Close
> > print(closingprices)
> NULL
> 
> I guess I'm not understanding the object that get.hist.quote makes. In
> general, what are R facilities for discovering what a given object is?
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From patrick.hausmann at uni-bremen.de  Wed Mar  3 11:53:30 2004
From: patrick.hausmann at uni-bremen.de (Patrick Hausmann)
Date: Wed, 3 Mar 2004 11:53:30 +0100
Subject: [R] Calculating percentiles from Grouped Data (gapply)
Message-ID: <4045C73A.32159.D27786@localhost>

Dear R-list,

I try to calculate the 10- and 90-Percentile from grouped data.
Using 'lappy' it works fine but I have no success with 'gapply':
 
df <- data.frame(test = runif(100))
df <- df[order(df[,1]),]
z <- rep(1:10, each = 10)
lapply(split(df, z), function(x) quantile(x, probs=c(10,90)/100))

library(nlme)
gapply(as.data.frame(df), which=1, 
          function(x) quantile(x, probs=c(10,90)/100), group=z)
#  Error in sort(x, partial = unique(c(lo, hi))) : 
#    `x' must be atomic

Thanks for any help
Patrick



From p.dalgaard at biostat.ku.dk  Wed Mar  3 12:00:12 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 12:00:12 +0100
Subject: [R] plot(x,y) with errors
In-Reply-To: <4045AE31.6050503@pdf.com>
References: <20040303094320.38598.qmail@web25208.mail.ukl.yahoo.com>
	<4045AE31.6050503@pdf.com>
Message-ID: <x2d67uyz43.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

>       Does the following do what you want: > x<-c(1,2,3,4,5,6,7,8,9,10)
>  > y<-c(1.3,2.5,4.6,5.3,5.9,6.7,7.4,8.5,9.4,10.4)
>  > erry<-c(0.2,0.3,0.2,0.1,0.4,0.2,0.3,0.4,0.3,0.2)
>  >
>  > plot(x, y)
>  >
>  > n <- length(x)
>  > Y <- array(c(y-erry, y+erry), dim=c(n,2))
>  > for(i in 1:n)
> +  lines(rep(x[i],2), Y[i,])

(Actually, segments(x,y-erry,x,y+erry) would be a more direct way.)

>       Others may have more elegant solutions, but this is simple
> enough for me to understand AND produces essentially the same result
> in R 1.8.1 and S-Plus 6.2.     hope this helps.  spencer graves

There are a number of fine points to get right, though, esp. if the
bars get longer than in this example.

I think there are special "plot with errorbars" in some of the "misc"
packages on CRAN (their authors are likely to chime in any moment
now), but for a relatively simple solution, I think I'd do

plot(x, y, type="n", ylim=range(y+erry,y-erry))
arrows(x,y-erry,x,y+erry,code=3,length=.005*diff(range(x)),angle=90)
points(x,y,pch=21,bg="white")


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From copellifulvio at yahoo.it  Wed Mar  3 12:06:09 2004
From: copellifulvio at yahoo.it (=?iso-8859-1?q?Fulvio=20Copex?=)
Date: Wed, 3 Mar 2004 12:06:09 +0100 (CET)
Subject: [R] plot(x,y) with errors
In-Reply-To: <x2d67uyz43.fsf@biostat.ku.dk>
Message-ID: <20040303110609.36238.qmail@web25206.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040303/9b116d40/attachment.pl

From v.demartino2 at virgilio.it  Wed Mar  3 12:09:51 2004
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Wed, 3 Mar 2004 12:09:51 +0100
Subject: [R] Changing background in splom et al.
Message-ID: <4043C77E00002C11@ims3e.cp.tin.it>

Context: Windows XP, R 1.8.1

I'm studying Venables-Ripley "MASS" book and having a go at the  many examples
in library MASS. The code I'm checking (from script ch04.R) now is 

......
data(swiss)
splom(~ swiss, aspect = "fill",
  panel = function(x, y, ...) {
     panel.xyplot(x, y, ...); panel.loess(x, y, ...)
  }
)

which produces an agreable plot with a gray background and cyan points,
but.....
Copying the plot as a metafile into Word and having a laser printer I'd
better stick to a b/w plot. 

How can I turn the background to white and the cyan points to black?

Ciao - Vittorio



From solares at unsl.edu.ar  Wed Mar  3 12:30:11 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Wed, 3 Mar 2004 08:30:11 -0300 (ART)
Subject: [R] interactive graphic s
Message-ID: <42611.170.210.173.216.1078313411.squirrel@inter17.unsl.edu.ar>

Hi, i dont understand ?Graphics in R are interactives or not?, I hear the
the package iplots can do it (zoom, scaling etc), is true that?, Thanks Ruben



From mkondrin at hppi.troitsk.ru  Wed Mar  3 23:57:35 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Wed, 03 Mar 2004 14:57:35 -0800
Subject: [R] zoom graphics
In-Reply-To: <4045B444.3080202@lancaster.ac.uk>
References: <49035.170.210.173.216.1078262462.squirrel@inter17.unsl.edu.ar>
	<4045B444.3080202@lancaster.ac.uk>
Message-ID: <404662DF.3080706@hppi.troitsk.ru>

Barry Rowlingson wrote:

> solares at unsl.edu.ar wrote:
>
>> Hi, i don't understand how i cant zoom in and zoom out a graphics 
>> (plots)
>> exist a package for that? Thanks Ruben
>>
>
>  I dont think you can do it quite like you can zoom in and out in a 
> program like 'photoshop'. All you can really do is redraw the plot 
> with a different set of X and Y limits.
>
Hello !
Zooming can be done in R in photoshop-like fashion, but this requires 
grid-package graphics, not the standard one (and some amount of hacking 
too). For example, look here -  
http://www.hppi.troitsk.ru/Kondrin/disgrace.htm.



From ggrothendieck at myway.com  Wed Mar  3 12:58:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 06:58:02 -0500 (EST)
Subject: [R] Find out the day of week for a chron object?
Message-ID: <20040303115802.EB07A39A4@mprdmxin.myway.com>


Here are three different ways (using x as defined in your 
post):

 with( month.day.year(x), day.of.week(month,day,year) )

 do.call( "day.of.week", month.day.year(x) )

 as.numeric(x-3)%%7   # uses fact that chron(3) is Sunday

---
Date:   Mon, 1 Mar 2004 22:18:33 +0530 
From:   Ajay Shah <ajayshah at mayin.org>
To:   r-help <r-help at stat.math.ethz.ch> 
Subject:   [R] Find out the day of week for a chron object? 

 
I know that this is correct:

library(chron)
x = dates("01-03-04", format="d-m-y", out.format="day mon year")
print(x)

It gives me the string "01 Mar 2004" which is correct.


I also know that I can say:

print(day.of.week(3,1,2004))

in which case he says 1, for today is monday.


My question is: How do I combine these two!? :-) I have a data file
which is being parsed nicely and read in using the chron() function. I
need to identify fridays and treat them differently. So I need to run
the day.of.week function. But day.of.week() doesn't eat a chron
object, he insists he wants m,d,y. This seems quite odd. Any idea what
I can do?

Thanks,

-ans.

-- 
Ajay Shah Consultant
ajayshah at mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi



From Giles.Heywood at CommerzbankIB.com  Wed Mar  3 13:06:46 2004
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Wed, 3 Mar 2004 12:06:46 -0000 
Subject: [R] match.call(), S4
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF7479@xmx8lonib.lonib.commerzbank.com>

I get different results from match.call(), according to whether a function
is dispatched via S3 or S4.  Specifically, when I use S4 dispatch in the
following example, the match.call() result is of length 1 less than I
expect.  I need to add an extra comma to get the same results as in the S3
method.

--------example

setClass("foo",representation("matrix"))
setMethod("[", c("foo","ANY"),  function(x, i, j, drop, ...)
    {
    mc <- match.call()
    print(mc)
    xmat <- x at .Data
    mc[[2]] <- as.name("xmat")
    return(eval(mc))
    }) 
"[.bar" <- function(x, ...)
    {
    mc <- match.call()
    print(mc)
    xmat <- unclass(x)
    mc[[1]] <- as.name("[")
    mc[[2]] <- as.name("xmat")
    return(eval(mc))
    }

ff <- new("foo",matrix(1:6,2,3))
bb <- structure(matrix(1:6,2,3),class="bar")

all.equal(bb[1],ff[1,])		#did not expect to need the extra comma in
ff[1,]
all.equal(bb[1,],ff[1,,])


------output


> all.equal(ff[1,],bb[1])
ff[i = 1]
[1] "length of mc is  3"
"[.bar"(x = bb, 1)
[1] "length of mc is  3"
[1] TRUE
> all.equal(bb[1,],ff[1,,])
"[.bar"(x = bb, 1, )
[1] "length of mc is  4"
ff[i = 1, ]
[1] "length of mc is  4"
[1] TRUE

------final comment

This is a minimal example.  I am really trying to invoke the method of the
superclass (matrix) after modifying the range arguments in the extract
method.  Perhaps there's a better way to do the whole kaboodle?

Thanks

- Giles
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    8.1            
year     2003           
month    11             
day      21             
language R              


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From ggrothendieck at myway.com  Wed Mar  3 13:10:26 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 07:10:26 -0500 (EST)
Subject: [R] Stuck in trying to convert repetitive code into a function
Message-ID: <20040303121026.E3CDE3984@mprdmxin.myway.com>


Issue the command:

debug(eat.a.file)

and then run your eat.a.file call.  This will step you through 
the function showing where it goes wrong.

By the way, 

- you don't need semicolons at the end of each line,
- the format you are specifying on chron is the default anyways
  so you can leave out format= 
- if you give use the argument as.is=1 on read.table then it 
  will return the date column as a character so you can convert it 
  to character like this:   A$date <- chron(A$date)

---
Folks,

I have the following repetitive code which works correctly:

A = read.table(file="junior.csv", sep=",", col.names=c("date", "l"));
A$date = chron(as.character(A$date), format="m/d/y");
r.junior = levels2weeklyret(lastfriday, A$date, A$l);
plot(A$date, A$l, type="l", col="red", main="Junior levels")
z <- locator(1)

A = read.table(file="kospi.csv", sep=",", col.names=c("date", "l"));
A$date = chron(as.character(A$date), format="m/d/y");
r.kospi = levels2weeklyret(lastfriday, A$date, A$l);
plot(A$date, A$l, type="l", col="red", main="Kospi levels")
z <- locator(1)

I tried to write it more nicely as follows:

eat.a.file <- function(fn, label, lastfriday) {
f = read.table(file=fn, sep=",", col.names=c("date", "l"));
f$date = chron(as.character(f$date), format="m/d/y");
plot(f$date, f$l, type="l", col="red", main=label);
z <- locator(1);
return(levels2weeklyret(lastfriday, f$date, f$l));
}
r.junior = eat.a.file(fn="junior.csv", label="Junior", lastfriday);
r.kospi = eat.a.file(fn="kospi.csv", label="Kospi", lastfriday);

When I do this, I get this error at the first of the two function calls:

Error in vector("double", length) : negative length vectors are not allowed
Execution halted

I have tried hard to experiment with various ways of writing it, but
am truly stumped. Any ideas on what might be going wrong? I am new to
R and have only written a few functions so far.

On a slightly related note, what is the situation in R on passing by
reference versus passing by value? I looked at the standard docs and
they seem to insist that a function can only send back one return
value. How does a function send back multiple things that it has
created?

-- 
Ajay Shah Consultant
ajayshah at mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi



From ripley at stats.ox.ac.uk  Wed Mar  3 13:12:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Mar 2004 12:12:58 +0000 (GMT)
Subject: [R] partial autocorrelation for Rt vs. Nt-1, ......., Nt-h
In-Reply-To: <000d01c400fe$e90f6550$93640ac1@szooek.slu.se>
Message-ID: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>

Your subject line is `partial autocorrelation', but that is not what you 
describe in the body of the email.

Partial correlations are by definition between two variables, just like 
correlations (but they can be partial-ed on other variables).

Please read the posting guide and the references there, and try to `ask a 
good question' that unambiguously tells your readers what you want to 
know, rather than leave them guessing.

On Wed, 3 Mar 2004, Scott Newey wrote:

> Dear list, following a previous querry we are still stuck!
> 
> As pointed out by Erin Hodges the "ts" library includes a PACF function
> which reports the partial correlation of population density at time t
> against lagged population density.

Rather, it computes the `partial autocorrelation' that your subject line 
mentions.

> However, what we are trying to calculate is the partial correlation between
> rate of population change, Rt=log Nt/Nt-1, against lagged population
> densities. Which is the partial rate correlation function propsed by
> Berryman & Turchin (1999).
> 
> The list archives give show methods for estimating partial correlation for 2
> or 3 variables only. Could any list member please suggest how this could be
> done for more variables, say Nt-1 to Nt-10.
> 
> Thank you,
> Scott
> =============================================
> 
> Hello Tomas!
> 
> There are functions for pacf and plot.acf.
> 
> They are in library(ts)
> 
> Hope this helps!
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> To: R-help at stat.math.ethz.ch
> Date: Sat, 28 Feb 2004 19:52:33 +0100
> Subject: [R] Stepwise regression and partial correlation for wildlife census
> 	time series
> 
> Dear List;
> 
> We are doing a time series analysis of wildlife census data. We use a
> stepwise regression of the annual per capita rate of increase against
> pervious years population size (log transformed) as suggested by
> Berryman & Turchin (2001, Oikos 92:265-270).
> 
> How can we obtain the partial correlation coefficients in R to make a
> plot of them against the lag as in a standard PACF?
> 
> Yours Sincerely
> 
> Tomas Willebrand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Wed Mar  3 13:25:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 07:25:27 -0500 (EST)
Subject: [R] Find out the day of week for a chron object?
Message-ID: <20040303122527.DD42D396A@mprdmxin.myway.com>



The specific code given below is correct but, in general, using
POSIXt is not a good idea since it gives rise to subtle problems
with time zones.  

You won't run into problems just converting character data
to POSIXlt, as the code below does, but most people who use 
POSIXlt also use POSIXct and that is where you run into 
problems.

The archives are full of such subtle time zone problems.  I 
have even found and reported subtle time zone problems in 
widely used R packages.

chron is more suitable for typical statistics problems that
don't require time zones.   What POSIXt is good for is file 
stamps and other operating system related time problems.

---
Date:   Tue, 2 Mar 2004 23:01:51 -0600 
From:   Dirk Eddelbuettel <edd at debian.org>
To:   Ajay Shah <ajayshah at mayin.org> 
Cc:   r-help <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Find out the day of week for a chron object? 

 
On Mon, Mar 01, 2004 at 10:18:33PM +0530, Ajay Shah wrote:
> I know that this is correct:
> 
> library(chron)
> x = dates("01-03-04", format="d-m-y", out.format="day mon year")
> print(x)
> 
> It gives me the string "01 Mar 2004" which is correct.
> 
> 
> I also know that I can say:
> 
> print(day.of.week(3,1,2004))
> 
> in which case he says 1, for today is monday.
> 
> 
> My question is: How do I combine these two!? :-) I have a data file
> which is being parsed nicely and read in using the chron() function. I
> need to identify fridays and treat them differently. So I need to run
> the day.of.week function. But day.of.week() doesn't eat a chron
> object, he insists he wants m,d,y. This seems quite odd. Any idea what
> I can do?

Chron and date are older packages, you may want to use the more recent (and
very powerful) DateTimeClasses

> parsedDate <- strptime("01-03-04", "%d-%d-%y")
> format(parsedDate)
[1] "2004-01-03"
> class(parsedDate)
[1] "POSIXt" "POSIXlt"
> weekdays(parsedDate)
[1] "Saturday"

Start with help(DateTimeClasses), if you ever used the C functions strptime
and strftime it shouldn't be too foreign. And do look at the mailing list
archives (and/or Google), as questions get answered on this quite often. 

Dirk

-- 
The relationship between the computed price and reality is as yet unknown. 
-- From the pac(8) manual page



From ggrothendieck at myway.com  Wed Mar  3 13:45:00 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 07:45:00 -0500 (EST)
Subject: [R] get.hist.quote - is great, but am I missing something?
Message-ID: <20040303124500.05DEE3984@mprdmxin.myway.com>


Here are a number of options:

plotOHLC(x)   # in tseries package

or 

require(chron)
x <- get.hist.quote( your.symbol, origin = chron(0) )
tt <- chron( time( x ) )
plot(tt, x[,"Close"])

or

# using tt and x just calculated
require(zoo)
plot(zoo(x,tt))

or 

see Giles' post on his its package


---
Date:   Wed, 3 Mar 2004 15:25:14 +0530 
From:   Ajay Shah <ajayshah at mayin.org>
To:   r-help <r-help at stat.math.ethz.ch> 
Subject:   [R] get.hist.quote - is great, but am I missing something? 

 
I find it's just great to be able to say:

library(tseries)
x <- get.hist.quote(instrument="ongc.ns")

and it gets a full time-series of the stock price of the symbol
ongc.ns from Yahoo quote.

However, once my hopes have been raised by such beauty :-) I get
disappointed when I do

> plot(x)

and the annotation is horrible! The x axis is not labelled as
dates. The default plot method for get.hist.quote should be better,
no?

I was not able to understand the object returned by get.hist.quote. If
I say:

> summary(x)
Open High Low Close 
Min. : 397.0 Min. : 407.3 Min. :395.1 Min. :398.4 
1st Qu.: 494.2 1st Qu.: 501.4 1st Qu.:482.7 1st Qu.:490.4 
Median : 614.9 Median : 622.7 Median :600.7 Median :610.0 
Mean : 615.6 Mean : 627.1 Mean :599.7 Mean :611.9 
3rd Qu.: 690.5 3rd Qu.: 707.4 3rd Qu.:676.2 3rd Qu.:691.5 
Max. :1000.0 Max. :1000.0 Max. :930.0 Max. :944.9 
NA's : 88.0 NA's : 88.0 NA's : 88.0 NA's : 88.0 

there is no mention of a 'time' variable. And, I'm unable to extract
(say) a vector of closing prices - e.g. if I say:

> closingprices <- x$Close
> print(closingprices)
NULL

I guess I'm not understanding the object that get.hist.quote makes. In
general, what are R facilities for discovering what a given object is?

-- 
Ajay Shah Consultant
ajayshah at mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Wed Mar  3 14:06:29 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 08:06:29 -0500 (EST)
Subject: [R] plot(x,y) with errors
Message-ID: <20040303130629.DA22E39A5@mprdmxin.myway.com>



see plotCI in package gregmisc

---
Date:   03 Mar 2004 12:00:12 +0100 
From:   Peter Dalgaard <p.dalgaard at biostat.ku.dk>
To:   Spencer Graves <spencer.graves at pdf.com> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] plot(x,y) with errors 

[...]
I think there are special "plot with errorbars" in some of the "misc"
packages on CRAN



From ggrothendieck at myway.com  Wed Mar  3 14:13:49 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 08:13:49 -0500 (EST)
Subject: [R] Changing background in splom et al.
Message-ID: <20040303131349.DC16239B7@mprdmxin.myway.com>


In Word, right click your picture, choose Format Picture,
select the Picture tab and under Color: choose Greyscale.

Depending on your printer, your print driver may also have 
some options to this effect that you can access after choosing 
File | Print.

There is undoubtedly a solution on the R side too but I usually
do touch up in Word so I can use the GUI.


Date:   Wed, 3 Mar 2004 12:09:51 +0100 
From:   <v.demartino2 at virgilio.it>
To:   r-help <r-help at stat.math.ethz.ch> 
Subject:   [R] Changing background in splom et al. 

 
Context: Windows XP, R 1.8.1

I'm studying Venables-Ripley "MASS" book and having a go at the many examples
in library MASS. The code I'm checking (from script ch04.R) now is 

......
data(swiss)
splom(~ swiss, aspect = "fill",
panel = function(x, y, ...) {
panel.xyplot(x, y, ...); panel.loess(x, y, ...)
}
)

which produces an agreable plot with a gray background and cyan points,
but.....
Copying the plot as a metafile into Word and having a laser printer I'd
better stick to a b/w plot. 

How can I turn the background to white and the cyan points to black?

Ciao - Vittorio



From Ted.Harding at nessie.mcc.ac.uk  Wed Mar  3 13:36:04 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 03 Mar 2004 12:36:04 -0000 (GMT)
Subject: [R] A basic question ...
Message-ID: <XFMail.040303123604.Ted.Harding@nessie.mcc.ac.uk>

Sorry to ask a question so basic that it's almost silly ...

I have data which can be expressed in contingency table
form as

Factor1  Factor2  Counts1  Counts2
==============================
   A        U      nAU1     nAU2
   A        V      nAV1     nAV2
   A        W      nAW1     nAW2

   B        U      nBU1     nBU2
   B        V      nBV1     nBV2
   B        W      nBW1     nBW3

and I want to submit these to 'loglin' to do the usual
sort of thing.

Question: what command do I give to R to generate the
requisite "table" object for 'loglin'?

With thanks!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 03-Mar-04                                       Time: 12:36:04
------------------------------ XFMail ------------------------------



From rxg218 at psu.edu  Wed Mar  3 14:48:59 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Wed, 03 Mar 2004 08:48:59 -0500
Subject: [R] using object reference
In-Reply-To: <4045ABA1.6080607@pdf.com>
References: <20040303032751.18C9B395D@mprdmxin.myway.com>
	<4045ABA1.6080607@pdf.com>
Message-ID: <1078321739.5504.5.camel@ra.chem.psu.edu>

On Wed, 2004-03-03 at 04:55, Spencer Graves wrote:
>       Yes, but use sparingly, because any use of "<<-", "assign", etc., 
> from within a function to change something other than what appears in 
> the standard return from a function call generates "spaghetti code" that 
> is difficult to maintain.  A month or a year from now, someone (the 
> developer or someone else) may have difficulty understanding how "z" got 
> created or changed. 

Thanks to everybody for their responses.

I do realize that changing a variable in the global namespace from a
function is not a good way to design code however for what I want I
can't see a way around it.

As I mentioned in an earlier mail the function creates a Tk window with
a text entry. When the user clicks on the button in the window the
buttons callback function gets the text of the entry box and places it
in the attribute 'a' of the variable passed to the function.

When I call my function it draws the Tk window and then returns to the R
prompt.

So doing something like

x <- add.text(x)

Does'nt give me what I want (it makes x a tclObj) and I can see why it
would not work this way.

Is there is a way that I can achieve this without having to use <<- ?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
COBOL is for morons.
-- E.W. Dijkstra



From slacey at umich.edu  Wed Mar  3 15:08:22 2004
From: slacey at umich.edu (Steven Lacey)
Date: Wed, 3 Mar 2004 09:08:22 -0500
Subject: [R] nesting vectors in a dataframe
Message-ID: <000d01c40129$00712b00$5081d38d@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040303/6ad736b0/attachment.pl

From Christoph.Scherber at uni-jena.de  Wed Mar  3 15:24:22 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 03 Mar 2004 15:24:22 +0100
Subject: [R] zoom graphics
In-Reply-To: <4045B444.3080202@lancaster.ac.uk>
References: <49035.170.210.173.216.1078262462.squirrel@inter17.unsl.edu.ar>
	<4045B444.3080202@lancaster.ac.uk>
Message-ID: <4045EA96.50800@uni-jena.de>

Interactive Plots with zoom options etc. can be performed using the 
"iplots" library. It?s really very useful and can be downloaded from

http://stats.math.uni-augsburg.de/iPlots/index.shtml

Best regards
Chris

Barry Rowlingson wrote:

> solares at unsl.edu.ar wrote:
>
>> Hi, i don't understand how i cant zoom in and zoom out a graphics 
>> (plots)
>> exist a package for that? Thanks Ruben
>>
>
>  I dont think you can do it quite like you can zoom in and out in a 
> program like 'photoshop'. All you can really do is redraw the plot 
> with a different set of X and Y limits.
>
> Example:
>
> # some data
>  xy=list(x=rnorm(100),y=rnorm(100))
>
> # default plot shows all the data:
>  plot(xy)
>
> # define an interactive 'zoom' function:
> zoom=function(){reg=locator(2);plot(reg,type='n')}
>
> # click two points at the corners of the zoom region:
> zoom()
>
> # add the points - dont use 'plot' since this resets the axes:
> points(xy)
>
>  If you've got a complex plot with lots of things on it, then yes, you 
> have to redraw it all again, but then you probably should have put 
> that complex plot into a single function!
>
> Barry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Christoph.Scherber at uni-jena.de  Wed Mar  3 15:25:16 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Wed, 03 Mar 2004 15:25:16 +0100
Subject: [R] interactive graphic s
In-Reply-To: <42611.170.210.173.216.1078313411.squirrel@inter17.unsl.edu.ar>
References: <42611.170.210.173.216.1078313411.squirrel@inter17.unsl.edu.ar>
Message-ID: <4045EACC.5020900@uni-jena.de>

Dear Ruben,

Yes, the iplots package works fine! You can download it from 
http://stats.math.uni-augsburg.de/iPlots/index.shtml

Good luck!
Chris.



solares at unsl.edu.ar wrote:

>Hi, i dont understand ?Graphics in R are interactives or not?, I hear the
>the package iplots can do it (zoom, scaling etc), is true that?, Thanks Ruben
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Mar  3 15:57:10 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 3 Mar 2004 15:57:10 +0100 (CET)
Subject: [R] read.spss and time/date information
In-Reply-To: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>
References: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>
Message-ID: <Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>


Hi,

I could not find any information on how `read.spss' deals with date
information. As an example, I created a file containing two variables,
one numeric (values = (1, 2)) and one of type "Datum" in SPSS (german
version with values "11.02.2003" and "03.04.1999" and I get in R:

SPSSfile = url("http://www.imbe.med.uni-erlangen.de/~hothorn/dates.sav", "rb")
SPSSdata = readBin(SPSSfile, "numeric", n = 10000)
writeBin(SPSSdata, con = "dummy.sav")
library(foreign)
read.spss("dummy.sav")

$DUMMY
[1] 1 2

$DATE
[1] 13264300800 13142476800

attr(,"label.table")
attr(,"label.table")$DUMMY
NULL

attr(,"label.table")$DATE
NULL


Could anyone give me a a hint how I can convert 13264300800 to 2003/02/11
again, please?

Best,

Torsten



From andy_liaw at merck.com  Wed Mar  3 16:06:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 3 Mar 2004 10:06:32 -0500
Subject: [R] Adding text (coefts) to pairs panels
Message-ID: <3A822319EB35174CA3714066D590DCD504AF78F7@usrymx25.merck.com>

You need to figure out the "user coordinates" for each panel, so you can put
the text within the range that's being plotted.  Here's an example:

panel.myfitline <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
    usr <- par("usr")
    res<-panel.smooth(x,y, col.smooth="blue", ...)
    reg <- coef(lm(y ~ x))
    abline(coef=reg,untf=F)
    const<-format(reg[1], trim = FALSE, digits = NULL, nsmall = 0, justify =
"left", ...)
    const<-paste(prefix, const, sep="")
    slope<-format(reg[2], trim = FALSE, digits = NULL, nsmall = 0, justify =
"left", ...)
    slope<-paste(prefix, slope, sep="")
    if(missing(cex.cor)) cex <- 0.8/strwidth(const)
    text(.95*usr[2], usr[3] +.05*(usr[4]-usr[3]), const, cex=cex)
    if(missing(cex.cor)) cex <- 0.8/strwidth(slope)
    text(usr[1] + .1*(usr[2]-usr[1]), .98*usr[4], slope, cex=cex)
}

HTH,
Andy

> From: Mark Van De Vyver
> 
> Hi,
> First of all, thanks for the efforts of all the developers 
> and contributors
> - I'm very new to R and at the moment am just using it to create some
> graphics, but it seems to be quite powerful.
> I've googled the archives and wasn't able to find anyhting 
> that dealt with
> this problem, so would appreciate any suggestions/tips.
> 
> In a pairs plot I'd like to plot a linear regression line in 
> each panel and,
> also in each panel, I'd like to have some text in the panel 
> showing the
> coefts, etc.
> I'm able to get the regressions line plotted, but not the 
> text/coefficients.
> Using one of the R examples, example(pairs), I have got this far:
> 
> panel.myfitline<-function(x, y, digits=2, prefix="", cex.cor, ...)
> {
>     res<-panel.smooth(x,y, col.smooth="blue", ...)
>     reg <- coef(lm(y ~ x))
>     abline(coef=reg,untf=F)
>     const<-format(reg[1], trim = FALSE, digits = NULL, nsmall 
> = 0, justify =
> "left", ...)
>     const<-paste(prefix, const, sep="")
>     slope<-format(reg[2], trim = FALSE, digits = NULL, nsmall 
> = 0, justify =
> "left", ...)
>     slope<-paste(prefix, slope, sep="")
>     if(missing(cex.cor)) cex <- 0.8/strwidth(const)
>     text(0.1, 0.5, const, cex=cex)
>     if(missing(cex.cor)) cex <- 0.8/strwidth(slope)
>     text(0.5, 0.1, slope, cex=cex)
> }
> pairs(USJudgeRatings[1:5], lower.panel=panel.smooth, 
> diag.panel=panel.hist,
> upper.panel=panel.myfitline)
> 
> Mark Van De Vyver
> PhD
> Lecturer
> Finance Discipline
> School of Business
> Faculty of Economics and Business
> Economics & Business Building H69
> The University of Sydney  
> Sydney  NSW  2006  Australia
> Telephone: +61 2 9351-6452
> Fax: +61 2 9351-6461
> Mobile: 0428 281407
> 
mailto:mvdv at spamcop.net
http:\\www.econ.usyd.edu.au

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From deepayan at stat.wisc.edu  Wed Mar  3 16:15:28 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 3 Mar 2004 09:15:28 -0600
Subject: [R] Changing background in splom et al.
In-Reply-To: <4043C77E00002C11@ims3e.cp.tin.it>
References: <4043C77E00002C11@ims3e.cp.tin.it>
Message-ID: <200403030915.28794.deepayan@stat.wisc.edu>

On Wednesday 03 March 2004 05:09, v.demartino2 at virgilio.it wrote:
> Context: Windows XP, R 1.8.1
>
> I'm studying Venables-Ripley "MASS" book and having a go at the  many
> examples in library MASS. The code I'm checking (from script ch04.R) now
> is
>
> ......
> data(swiss)
> splom(~ swiss, aspect = "fill",
>   panel = function(x, y, ...) {
>      panel.xyplot(x, y, ...); panel.loess(x, y, ...)
>   }
> )
>
> which produces an agreable plot with a gray background and cyan points,
> but.....
> Copying the plot as a metafile into Word and having a laser printer I'd
> better stick to a b/w plot.
>
> How can I turn the background to white and the cyan points to black?

You probably want to do 

trellis.device(color = FALSE)

or 

trellis.device(device = "win.metafile", ## or whatever the device name is
               color = FALSE)

before calling splom().

Deepayan



From ripley at stats.ox.ac.uk  Wed Mar  3 16:30:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Mar 2004 15:30:29 +0000 (GMT)
Subject: [R] match.call(), S4
In-Reply-To: <8CBAA121CEB4D5118CB200508BB2BBEF05BF7479@xmx8lonib.lonib.commerzbank.com>
Message-ID: <Pine.LNX.4.44.0403031516210.4107-100000@gannet.stats>

On Wed, 3 Mar 2004, Heywood, Giles wrote:

> I get different results from match.call(), according to whether a function
> is dispatched via S3 or S4.  Specifically, when I use S4 dispatch in the
> following example, the match.call() result is of length 1 less than I
> expect.  I need to add an extra comma to get the same results as in the S3
> method.

I am confused by your puzzlement, and don't believe this has anything to
do with S4 vs S3.  Consider

> tmp <- function(i, j, k) match.call()
> tmp(1)
tmp(i = 1)
> tmp(1,)
tmp(i = 1)
> tmp2 <- function(x, ...) match.call()
> tmp2(1)
tmp2(x = 1)
> tmp2(1, )
tmp2(x = 1,)
> tmp2(1,,2)
tmp2(x = 1, , 2)

so why are you surprised?  `...' is not handled the same way as other 
arguments, and your S3 and S4 methods have different formal argument 
lists.

Take a look at [.data.frame for the sort of code you need to write to 
handle this at R level.


> --------example
> 
> setClass("foo",representation("matrix"))
> setMethod("[", c("foo","ANY"),  function(x, i, j, drop, ...)
>     {
>     mc <- match.call()
>     print(mc)
>     xmat <- x at .Data
>     mc[[2]] <- as.name("xmat")
>     return(eval(mc))
>     }) 
> "[.bar" <- function(x, ...)
>     {
>     mc <- match.call()
>     print(mc)
>     xmat <- unclass(x)
>     mc[[1]] <- as.name("[")
>     mc[[2]] <- as.name("xmat")
>     return(eval(mc))
>     }
> 
> ff <- new("foo",matrix(1:6,2,3))
> bb <- structure(matrix(1:6,2,3),class="bar")
> 
> all.equal(bb[1],ff[1,])		#did not expect to need the extra comma in
> ff[1,]
> all.equal(bb[1,],ff[1,,])
> 
> 
> ------output
> 
> 
> > all.equal(ff[1,],bb[1])
> ff[i = 1]
> [1] "length of mc is  3"
> "[.bar"(x = bb, 1)
> [1] "length of mc is  3"
> [1] TRUE
> > all.equal(bb[1,],ff[1,,])
> "[.bar"(x = bb, 1, )
> [1] "length of mc is  4"
> ff[i = 1, ]
> [1] "length of mc is  4"
> [1] TRUE
> 
> ------final comment
> 
> This is a minimal example.  I am really trying to invoke the method of the
> superclass (matrix) after modifying the range arguments in the extract
> method.  Perhaps there's a better way to do the whole kaboodle?
> 
> Thanks
> 
> - Giles
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R              
> 
> 
> ********************************************************************** 
> This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Stefano.Guazzetti at ausl.re.it  Wed Mar  3 16:58:16 2004
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Wed, 3 Mar 2004 16:58:16 +0100
Subject: R: [R] Changing background in splom et al.
Message-ID: <F298786BF61DE64590054AF31EA1B4C8016D1F0A@PEPI.ausl.org>


trellis.device(bg="white", color=F)

before your call to splom could make what you want but 
take also a look at

?trellis.par.set


Stefano

> -----Messaggio originale-----
> Da: v.demartino2 at virgilio.it [mailto:v.demartino2 at virgilio.it]
> Inviato: mercoled? 3 marzo 2004 12.10
> A: r-help
> Oggetto: [R] Changing background in splom et al.
> 
> 
> Context: Windows XP, R 1.8.1
> 
> I'm studying Venables-Ripley "MASS" book and having a go at 
> the  many examples
> in library MASS. The code I'm checking (from script ch04.R) now is 
> 
> ......
> data(swiss)
> splom(~ swiss, aspect = "fill",
>   panel = function(x, y, ...) {
>      panel.xyplot(x, y, ...); panel.loess(x, y, ...)
>   }
> )
> 
> which produces an agreable plot with a gray background and 
> cyan points,
> but.....
> Copying the plot as a metafile into Word and having a laser 
> printer I'd
> better stick to a b/w plot. 
> 
> How can I turn the background to white and the cyan points to black?
> 
> Ciao - Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From edd at debian.org  Wed Mar  3 17:02:10 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 3 Mar 2004 10:02:10 -0600
Subject: [R] get.hist.quote - is great, but am I missing something?
In-Reply-To: <20040303095514.GQ689@igidr.ac.in>
References: <20040303095514.GQ689@igidr.ac.in>
Message-ID: <20040303160210.GA9291@sonny.eddelbuettel.com>

On Wed, Mar 03, 2004 at 03:25:14PM +0530, Ajay Shah wrote:
> I find it's just great to be able to say:
> 
>   library(tseries)
>   x <- get.hist.quote(instrument="ongc.ns")
> 
> and it gets a full time-series of the stock price of the symbol
> ongc.ns from Yahoo quote.
> 
> However, once my hopes have been raised by such beauty :-) I get
> disappointed when I do
> 
> > plot(x)

a) use  plotOHLC(x), not plot(), PlotOHLC is also in tseries.

b) install the its package which has a variant of get.hist.quote
   and plots that too (though it doesn;t skip weekends)
   
> and the annotation is horrible! The x axis is not labelled as
> dates. The default plot method for get.hist.quote should be better,
> no?

So are you intending to contribute one?  
 
> I was not able to understand the object returned by get.hist.quote. If
> I say:
> 
> > summary(x)
>       Open             High             Low            Close      
>  Min.   : 397.0   Min.   : 407.3   Min.   :395.1   Min.   :398.4  
>  1st Qu.: 494.2   1st Qu.: 501.4   1st Qu.:482.7   1st Qu.:490.4  
>  Median : 614.9   Median : 622.7   Median :600.7   Median :610.0  
>  Mean   : 615.6   Mean   : 627.1   Mean   :599.7   Mean   :611.9  
>  3rd Qu.: 690.5   3rd Qu.: 707.4   3rd Qu.:676.2   3rd Qu.:691.5  
>  Max.   :1000.0   Max.   :1000.0   Max.   :930.0   Max.   :944.9  
>  NA's   :  88.0   NA's   :  88.0   NA's   : 88.0   NA's   : 88.0  
> 
> there is no mention of a 'time' variable. And, I'm unable to extract
> (say) a vector of closing prices - e.g. if I say:
> 
> > closingprices <- x$Close
> > print(closingprices)
> NULL
> 
> I guess I'm not understanding the object that get.hist.quote makes. In
> general, what are R facilities for discovering what a given object is?

class(x)
str(x)
...

Hth, Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From ggrothendieck at myway.com  Wed Mar  3 17:21:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 11:21:12 -0500 (EST)
Subject: [R] read.spss and time/date information
Message-ID: <20040303162112.0052839E9@mprdmxin.myway.com>



I don't use SPSS but following through on your detective work 
can provide the likely answer.

First note that both date numbers are evenly divisible by the number 
of seconds in a day, i.e. 24*60*60.  This suggests that these numbers
are seconds since some origin.

Since we know "2003/02/11" corresponds to 13264300800 we deduce that
the origin must be 

   spss.orig <- as.POSIXct("2003/02/11") - 13264300800

so  spss.orig+x  gives the POSIXct date if x is the SPSS number.

For example,

> spss.orig <- as.POSIXct("2003/02/11") - 13264300800
> spss.orig + c(13264300800, 13142476800)
[1] "2003-02-11 Eastern Standard Time" "1999-04-03 Eastern Standard Time"


An alternative might be to do this all in the GMT time zone:

spss.orig <- as.POSIXct("2003/02/11", tz="GMT") - 13264300800
format(spss.orig + c(13264300800, 13142476800), tz="GMT")


---
Date:   Wed, 3 Mar 2004 15:57:10 +0100 (CET) 
From:   Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] read.spss and time/date information 

 

Hi,

I could not find any information on how `read.spss' deals with date
information. As an example, I created a file containing two variables,
one numeric (values = (1, 2)) and one of type "Datum" in SPSS (german
version with values "11.02.2003" and "03.04.1999" and I get in R:

SPSSfile = url("http://www.imbe.med.uni-erlangen.de/~hothorn/dates.sav";, "rb")
SPSSdata = readBin(SPSSfile, "numeric", n = 10000)
writeBin(SPSSdata, con = "dummy.sav")
library(foreign)
read.spss("dummy.sav")

$DUMMY
[1] 1 2

$DATE
[1] 13264300800 13142476800

attr(,"label.table")
attr(,"label.table")$DUMMY
NULL

attr(,"label.table")$DATE
NULL


Could anyone give me a a hint how I can convert 13264300800 to 2003/02/11
again, please?

Best,

Torsten



From bob.ohara at helsinki.fi  Wed Mar  3 17:26:46 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Wed, 03 Mar 2004 18:26:46 +0200
Subject: [R] Problem with Integrate
References: <40444BF2.9050900@helsinki.fi>	<Pine.A41.4.58.0403020717380.58162@homer40.u.washington.edu>	<4044A6EB.4000506@helsinki.fi>
	<x24qt7w6gd.fsf@biostat.ku.dk>
Message-ID: <40460746.90602@helsinki.fi>

Peter Dalgaard wrote:
> "Anon." <bob.ohara at helsinki.fi> writes:
> 
> 
>>>>I assume that this is because for much of the range, the integral is
>>>>basically zero.
>>>
>>>The help page for integrate() says
>>>     When integrating over infinite intervals do so explicitly,
>>>rather
>>>     than just using a large number as the endpoint.  This increases
>>>     the chance of a correct answer - any function whose integral over
>>>     an infinite interval is finite must be near zero for most of that
>>>     interval.
>>>That is, if you want an integral from 0 to Inf, do that.
>>>
>>
>>Sorry, I forgot to put that in my message.  It gives the same error as
>>a large value.  I assume it's all a result of the NaN's being returned.
> 
> 
> It does seem to help a bit if you modify the integrand to
> 
> 
>>PLN1
> 
> function(lam, Count, mu, sigma2) {
>    t1 <- dpois(Count, exp(lam), log=F)
>    t2 <- dnorm(lam, mu, sqrt(sigma2))
>    ifelse(t1==0|t2==0,0,t1*t2)
> }
> 
> 
<snip>

Mange tak!

And thank you to everyone else for their help too.  The generic solution 
will help me with the next stage of my work.  I can see that it might 
break down, but only under rather bizarre circumstances.

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From ccleland at optonline.net  Wed Mar  3 17:37:19 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 03 Mar 2004 11:37:19 -0500
Subject: [R] read.spss and time/date information
In-Reply-To: <Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
References: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>
	<Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
Message-ID: <404609BF.6010206@optonline.net>

Torsten Hothorn wrote:
> I could not find any information on how `read.spss' deals with date
> information. As an example, I created a file containing two variables,
> one numeric (values = (1, 2)) and one of type "Datum" in SPSS (german
> version with values "11.02.2003" and "03.04.1999" and I get in R:
> 
> SPSSfile = url("http://www.imbe.med.uni-erlangen.de/~hothorn/dates.sav", "rb")
> SPSSdata = readBin(SPSSfile, "numeric", n = 10000)
> writeBin(SPSSdata, con = "dummy.sav")
> library(foreign)
> read.spss("dummy.sav")
> 
> $DUMMY
> [1] 1 2
> 
> $DATE
> [1] 13264300800 13142476800
> 
> attr(,"label.table")
> attr(,"label.table")$DUMMY
> NULL
> 
> attr(,"label.table")$DATE
> NULL
> 
> 
> Could anyone give me a a hint how I can convert 13264300800 to 2003/02/11
> again, please?

   Date variables in SPSS contain the number of seconds since 
October 14, 1582.  I think you could do something like this to 
convert:

 > ISOdate(1582, 10, 14) + c(13264300800, 13142476800)
[1] "2003-02-11 07:00:00 Eastern Standard Time"
[2] "1999-04-03 07:00:00 Eastern Standard Time"

hope this helps,

Chuck Cleland

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Wed Mar  3 17:39:25 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 03 Mar 2004 17:39:25 +0100
Subject: [R] read.spss and time/date information
In-Reply-To: <Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
References: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>
	<Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
Message-ID: <40460A3D.2040008@statistik.uni-dortmund.de>

Torsten Hothorn wrote:

> Hi,
> 
> I could not find any information on how `read.spss' deals with date
> information. As an example, I created a file containing two variables,
> one numeric (values = (1, 2)) and one of type "Datum" in SPSS (german
> version with values "11.02.2003" and "03.04.1999" and I get in R:
> 
> SPSSfile = url("http://www.imbe.med.uni-erlangen.de/~hothorn/dates.sav", "rb")
> SPSSdata = readBin(SPSSfile, "numeric", n = 10000)
> writeBin(SPSSdata, con = "dummy.sav")
> library(foreign)
> read.spss("dummy.sav")
> 
> $DUMMY
> [1] 1 2
> 
> $DATE
> [1] 13264300800 13142476800
> 
> attr(,"label.table")
> attr(,"label.table")$DUMMY
> NULL
> 
> attr(,"label.table")$DATE
> NULL
> 
> 
> Could anyone give me a a hint how I can convert 13264300800 to 2003/02/11
> again, please?
> 

Hi Torsten,

Idee:

R> (13264300800-13142476800) / (60*60*24):
[1] 1410

Vom 03.04.1999 bis 11.02.2003: 1410 Tage? Kommt grob hin.
Also ist 13264300800 Anzahl Sekunden seit????
Ist das vielleicht bei SPSS irgendwo dokumentiert?

Bis naechste Woche!

Gruss,
Uwe



From p.dalgaard at biostat.ku.dk  Wed Mar  3 17:52:01 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 17:52:01 +0100
Subject: [R] read.spss and time/date information
In-Reply-To: <Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
References: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>
	<Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
Message-ID: <x2r7w9yitq.fsf@biostat.ku.dk>

Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:

> Could anyone give me a a hint how I can convert 13264300800 to 2003/02/11
> again, please?

> ISOdate(1582,10,14)  + 13264300800
[1] "2003-02-11 13:00:00 CET"
> ISOdate(1582,10,14)  +  13142476800
[1] "1999-04-03 14:00:00 CEST"

[October 14, 1582 is Day 1 of the Gregorian calendar.]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hodgess at gator.uhd.edu  Wed Mar  3 18:19:25 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 3 Mar 2004 11:19:25 -0600
Subject: [R] partial correlation?
Message-ID: <200403031719.i23HJPS01176@gator.dt.uh.edu>

Hi R People!

Are Tomas and Scott  looking for partial correlation, please?
Do they mean something like "r_1.23", please?

Thanks,
Erin



From torsten at hothorn.de  Wed Mar  3 18:26:36 2004
From: torsten at hothorn.de (torsten@hothorn.de)
Date: Wed, 3 Mar 2004 18:26:36 +0100 (CET)
Subject: [R] read.spss and time/date information
In-Reply-To: <x2r7w9yitq.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0403031202350.2724-100000@gannet.stats>
	<Pine.LNX.4.51.0403031551010.12807@artemis.imbe.med.uni-erlangen.de>
	<x2r7w9yitq.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.51.0403031822570.13505@artemis.imbe.med.uni-erlangen.de>



On Wed, 3 Mar 2004, Peter Dalgaard wrote:

> Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:
>
> > Could anyone give me a a hint how I can convert 13264300800 to 2003/02/11
> > again, please?
>
> > ISOdate(1582,10,14)  + 13264300800
> [1] "2003-02-11 13:00:00 CET"
> > ISOdate(1582,10,14)  +  13142476800
> [1] "1999-04-03 14:00:00 CEST"
>
> [October 14, 1582 is Day 1 of the Gregorian calendar.]
>

I tried January 1th 1970 as "baseline" but I never would have
dreamed of October 14, 1582.

Thanks to all responders!

Torsten

> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From Giles.Heywood at CommerzbankIB.com  Wed Mar  3 18:27:57 2004
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Wed, 3 Mar 2004 17:27:57 -0000 
Subject: [R] get.hist.quote - is great, but am I missing something?
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF747D@xmx8lonib.lonib.commerzbank.com>

If you wish to 'skip' (i.e. not interpolate) weekends in its, you could use
the following:

prices <- priceIts(instrument="ongc.ns")
plot(union(prices,newIts(start=start(prices),end=end(prices))),interp="none"
)

- Giles

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Dirk 
> Eddelbuettel
> Sent: 03 March 2004 16:02
> To: Ajay Shah
> Cc: r-help
> Subject: Re: [R] get.hist.quote - is great, but am I missing 
> something?
> 
> 
> On Wed, Mar 03, 2004 at 03:25:14PM +0530, Ajay Shah wrote:
> > I find it's just great to be able to say:
> > 
> >   library(tseries)
> >   x <- get.hist.quote(instrument="ongc.ns")
> > 
> > and it gets a full time-series of the stock price of the symbol
> > ongc.ns from Yahoo quote.
> > 
> > However, once my hopes have been raised by such beauty :-) I get
> > disappointed when I do
> > 
> > > plot(x)
> 
> a) use  plotOHLC(x), not plot(), PlotOHLC is also in tseries.
> 
> b) install the its package which has a variant of get.hist.quote
>    and plots that too (though it doesn;t skip weekends)
>    
> > and the annotation is horrible! The x axis is not labelled as
> > dates. The default plot method for get.hist.quote should be better,
> > no?
> 
> So are you intending to contribute one?  
>  
> > I was not able to understand the object returned by 
> get.hist.quote. If
> > I say:
> > 
> > > summary(x)
> >       Open             High             Low            Close      
> >  Min.   : 397.0   Min.   : 407.3   Min.   :395.1   Min.   :398.4  
> >  1st Qu.: 494.2   1st Qu.: 501.4   1st Qu.:482.7   1st Qu.:490.4  
> >  Median : 614.9   Median : 622.7   Median :600.7   Median :610.0  
> >  Mean   : 615.6   Mean   : 627.1   Mean   :599.7   Mean   :611.9  
> >  3rd Qu.: 690.5   3rd Qu.: 707.4   3rd Qu.:676.2   3rd Qu.:691.5  
> >  Max.   :1000.0   Max.   :1000.0   Max.   :930.0   Max.   :944.9  
> >  NA's   :  88.0   NA's   :  88.0   NA's   : 88.0   NA's   : 88.0  
> > 
> > there is no mention of a 'time' variable. And, I'm unable to extract
> > (say) a vector of closing prices - e.g. if I say:
> > 
> > > closingprices <- x$Close
> > > print(closingprices)
> > NULL
> > 
> > I guess I'm not understanding the object that 
> get.hist.quote makes. In
> > general, what are R facilities for discovering what a given 
> object is?
> 
> class(x)
> str(x)
> ...
> 
> Hth, Dirk
> 
> -- 
> The relationship between the computed price and reality is as 
> yet unknown.  
>                                              -- From the 
> pac(8) manual page
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From feh3k at spamcop.net  Wed Mar  3 18:31:02 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 3 Mar 2004 17:31:02 +0000
Subject: [R] read.spss and time/date information
In-Reply-To: <20040303162112.0052839E9@mprdmxin.myway.com>
References: <20040303162112.0052839E9@mprdmxin.myway.com>
Message-ID: <20040303173102.0ab0fa4a.feh3k@spamcop.net>

On Wed,  3 Mar 2004 11:21:12 -0500 (EST)
"Gabor Grothendieck" <ggrothendieck at myway.com> wrote:

> 
> 
> I don't use SPSS but following through on your detective work 
> can provide the likely answer.
> 
> First note that both date numbers are evenly divisible by the number 
> of seconds in a day, i.e. 24*60*60.  This suggests that these numbers
> are seconds since some origin.
> 
> Since we know "2003/02/11" corresponds to 13264300800 we deduce that
> the origin must be 
> 
>    spss.orig <- as.POSIXct("2003/02/11") - 13264300800
> 
> so  spss.orig+x  gives the POSIXct date if x is the SPSS number.
> 
> For example,
> 
> > spss.orig <- as.POSIXct("2003/02/11") - 13264300800
> > spss.orig + c(13264300800, 13142476800)
> [1] "2003-02-11 Eastern Standard Time" "1999-04-03 Eastern Standard
> Time"
> 
> 
> An alternative might be to do this all in the GMT time zone:
> 
> spss.orig <- as.POSIXct("2003/02/11", tz="GMT") - 13264300800
> format(spss.orig + c(13264300800, 13142476800), tz="GMT")
> 
> 
> ---
> Date:   Wed, 3 Mar 2004 15:57:10 +0100 (CET) 
> From:   Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de>
> To:   <r-help at stat.math.ethz.ch> 
> Subject:   [R] read.spss and time/date information 
> 
>  
> 
> Hi,
> 
> I could not find any information on how `read.spss' deals with date
> information. As an example, I created a file containing two variables,
> one numeric (values = (1, 2)) and one of type "Datum" in SPSS (german
> version with values "11.02.2003" and "03.04.1999" and I get in R:
> 
> SPSSfile =
> url("http://www.imbe.med.uni-erlangen.de/~hothorn/dates.sav";, "rb")
> SPSSdata = readBin(SPSSfile, "numeric", n = 10000) writeBin(SPSSdata,
> con = "dummy.sav") library(foreign)
> read.spss("dummy.sav")
> 
> $DUMMY
> [1] 1 2
> 
> $DATE
> [1] 13264300800 13142476800
> 
> attr(,"label.table")
> attr(,"label.table")$DUMMY
> NULL
> 
> attr(,"label.table")$DATE
> NULL
> 
> 
> Could anyone give me a a hint how I can convert 13264300800 to
> 2003/02/11 again, please?
> 
> Best,
> 
> Torsten
> 

The spss.get function in the Hmisc packages allows you to specify a vector
of variable names that are to be automatically converted to dates (POSIXct
form).  It calls read.spss to to the main work.

Speaking of read.spss, which I have found handles labels and value labels
very well, I am having a difficulty with character variables ending up as
factors even when the number of levels equals the number of observations. 
I would be nice to have an option to keep character variables as-is (the
sas.get function in Hmisc provides many options for this).

Frank Harrell



From hodgess at gator.uhd.edu  Wed Mar  3 18:52:57 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 3 Mar 2004 11:52:57 -0600
Subject: [R] get.hist.quote
Message-ID: <200403031752.i23Hqv505317@gator.dt.uh.edu>

Dear R People:

Here is a silly one:
Where is get.hist.quote, please?

Thanks,
Erin
mailto: hodgess at gator.uhd.edu



From djw1005 at cam.ac.uk  Wed Mar  3 19:41:57 2004
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Wed, 3 Mar 2004 18:41:57 +0000 (GMT)
Subject: [R] nesting vectors in a dataframe
In-Reply-To: <000d01c40129$00712b00$5081d38d@lsa.adsroot.itcs.umich.edu>
Message-ID: <Pine.SOL.3.96.1040303183902.22233A-100000@draco.cus.cam.ac.uk>


Steven Lacey wrote:
> I want to create a dataframe where one of the columns does not hold
> individual elements, but a vector instead. 
> ...
> However, if my dataframe does not already exist, then I cannot create a
> dataframe with vectors using the following syntax:
> data.frame(x1=list(c(5,6,7),c(8,9,10)))

data.frame(x1=I(list(c(5,6,7),c(8,9,10))))

will work. You need to tell R to keep the list AsIs, using the AsIs
function I(.)

Damon.



From p.dalgaard at biostat.ku.dk  Wed Mar  3 19:47:27 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 19:47:27 +0100
Subject: [R] read.spss and time/date information
In-Reply-To: <20040303173102.0ab0fa4a.feh3k@spamcop.net>
References: <20040303162112.0052839E9@mprdmxin.myway.com>
	<20040303173102.0ab0fa4a.feh3k@spamcop.net>
Message-ID: <x2fzcpbwe8.fsf@biostat.ku.dk>

Frank E Harrell Jr <feh3k at spamcop.net> writes:

> Speaking of read.spss, which I have found handles labels and value labels
> very well, I am having a difficulty with character variables ending up as
> factors even when the number of levels equals the number of observations. 
> I would be nice to have an option to keep character variables as-is (the
> sas.get function in Hmisc provides many options for this).

Will the max.value.labels not help? (otherwise, as.character should)

BTW, we seem to have a bug:

x <- read.spss(
  "/home/pd/R-1.8.1/tests/Packages/foreign/tests/electric.sav",
   max=1)
x$FAMHXCVR

gives

  [1] "Y" "N" "N" "Y" "N" "N" "N" "N" "N" "Y" "Y" "N" "Y" "Y" "Y" "N" "N" "N"
...
[235] "N" "N" "N" "N" "N" "N"
attr(,"value.labels")
       YES         NO
"Y       " "N       "

but a higher level of max gives all NA. Looks like trailing blanks in
value labels isn't handled right.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From a.trapletti at bluewin.ch  Wed Mar  3 19:49:15 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Wed, 03 Mar 2004 19:49:15 +0100
Subject: [R] get.hist.quote - is great, but am I missing something?
Message-ID: <404628AB.5030006@bluewin.ch>

>
> I find it's just great to be able to say:
>
> library(tseries)
> x <- get.hist.quote(instrument="ongc.ns")
>
> and it gets a full time-series of the stock price of the symbol
> ongc.ns from Yahoo quote.
>
> However, once my hopes have been raised by such beauty I get
> disappointed when I do
>
>>> plot(x)
>
>
> and the annotation is horrible! The x axis is not labelled as
> dates. The default plot method for get.hist.quote should be better,
> no?


It depends what you want to do. But it all behaves as it is documented. 
Since the object returned by get.hist.quote is a multivariate time 
series (class "mts") plot.mts is called. As documented, the times are 
given in Julian dates since 1899-12-30. Therefore the "horrible" 
annotation.

If you would like a more user-friendly plot, try plotOHLC(x).

>
> I was not able to understand the object returned by get.hist.quote. If
> I say:
>
>>> summary(x)
>
> Open High Low Close
> Min. : 397.0 Min. : 407.3 Min. :395.1 Min. :398.4
> 1st Qu.: 494.2 1st Qu.: 501.4 1st Qu.:482.7 1st Qu.:490.4
> Median : 614.9 Median : 622.7 Median :600.7 Median :610.0
> Mean : 615.6 Mean : 627.1 Mean :599.7 Mean :611.9
> 3rd Qu.: 690.5 3rd Qu.: 707.4 3rd Qu.:676.2 3rd Qu.:691.5
> Max. :1000.0 Max. :1000.0 Max. :930.0 Max. :944.9
> NA's : 88.0 NA's : 88.0 NA's : 88.0 NA's : 88.0
>
> there is no mention of a 'time' variable. And, I'm unable to extract
> (say) a vector of closing prices - e.g. if I say:
>
>>> closingprices <- x$Close
>>> print(closingprices)
>
> NULL
>
> I guess I'm not understanding the object that get.hist.quote makes. In
> general, what are R facilities for discovering what a given object is?

I suggest, you study first one of the beginners manuals of the R 
environment: http://cran.r-project.org/manuals.html
Then you would easily see that, e.g., x[,"Close"] gives you the series 
of closing prices, time(x) gives you the series of times and so on...

>
> -- 
> Ajay Shah Consultant
> ajayshah at mayin.org Department of Economic Affairs
> http://www.mayin.org/ajayshah Ministry of Finance, New Delhi
>
>

best
Adrian


--
Dr. Adrian Trapletti
Trapletti Statistical Computing
Wildsbergstrasse 31, 8610 Uster
Switzerland
Phone & Fax : +41 (0) 1 994 5631
Mobile : +41 (0) 76 370 5631
Email : mailto:a.trapletti at bluewin.ch
WWW : http://trapletti.homelinux.com



From Yongchao.Ge at mssm.edu  Wed Mar  3 19:56:45 2004
From: Yongchao.Ge at mssm.edu (Yongchao Ge)
Date: Wed, 03 Mar 2004 13:56:45 -0500 (EST)
Subject: [R] generating normal numbers: GetRNGstate, PutRNGstate
In-Reply-To: <200403021105.i22B3ggt010927@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0403031328430.30386-100000@ge.biomath.mssm.edu>

Hi 

I'd like to generate thousands of normal numbers from my C function using 
the C API functions provided R. I have two options:

1. double norm_rand(); (page 61 of R extension 1.8.1)
2. double rnorm(double mu, double sigma); (page 58 of R extension 1.8.1)

If my understanding of R-exts is correct, then I only need to call 
GetRNGstate once, and then call 1000 norm_rand, and then call 
PutRNGstate once for the 1st option.

For the 2nd option, I have to call 1000 times for each of GetRNGstate, 
rnorm, and PutRNGstate.

The pseudo-code for option 1 will be:

Method 1:

GetRNGstate();
for(i=1;i<1000;i++){
   x[i]=norm_rand();
}
PutRNGstate();

The pseudo code for option 2 will be:

Method 2:

for(i=1;i<1000;i++){
   GetRNGstate();
   x[i]=rnorm(0,1);
   PutRNGstate();
}

Of course, I can also write a slower version for option 1, i.e. call 
GetRNGstate and PutRNGstate each time for norm_rand.

method 3:

for(i=1;i<1000;i++){
   GetRNGstate();
   x[i]=norm_rand(); 
   PutRNGstate();
}


My questions are:

1. Are the three methods all correct for generating random numbers?

2. Are they generating the exactly the same random number if we have the 
same random seed?

I searched the R help and google and I didn't find answers. The reason for 
to ask is that if both of the above answers are right, then I'd better 
off use the method 1, which is the fastest as I have to generate 
hundreds thousands of normal numbers.

Thanks!

Yongchao


p.s. please cc to me as I am not on the online list, only on the 
daily digest list.



From elvis at xlsolutions-corp.com  Wed Mar  3 20:00:56 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed,  3 Mar 2004 12:00:56 -0700
Subject: [R] 
	Course in Toronto***R/Splus Programming Techniques, April 15-16,
	2004
Message-ID: <20040303190056.29424.qmail@webmail-2-1.secureserver.net>


   XLSolutions Corporation ([1]www.xlsolutions-corp.com) is proud
   to announce April 2004 2-day "R/S-plus Fundamentals and Programming
   Techniques" in CANADA.
   ****Toronto, Canada ------------------> April 15-16, 2004
   Reserve your seat now at the early bird rates! Payment due AFTER the
   class.

   Course Description:
   This two-day beginner to intermediate R/S-plus course focuses
    on a broad spectrum of topics,
   from reading raw data to a comparison of R and S. We will learn
   the essentials of data manipulation, graphical visualization
   and R/S-plus programming. We will explore statistical data analysis
   tools,including graphics with data sets. How to enhance your plots.
   We will perform basic statistics and fit linear regression models.
   Participants are encouraged to bring data for interactive sessions

   With the following outline:
   - An Overview of R
   - Data Manipulation and Graphics
   - Using Lattice Graphics
   - A Comparison of R and S-Plus
   - How can R Complement SAS?
   - Writing Functions
   - Avoiding Loops
   - Vectorization
   - Statistical Modeling
   - Project Management
   - Techniques for Effective use of R and S
   - Enhancing Plots
   - Using High-level Plotting Functions
   - Building and Distributing Packages (libraries)
   XLSolutions Corporation ([2]www.xlsolutions-corp.com) is proud
   to announce April 2004 2-day "R/S-plus Fundamentals and Programming
   Techniques" in CANADA.
   ****Toronto, Canada ------------------> April 15-16, 2004
   Reserve your seat now at the early bird rates! Payment due AFTER the
   class.

   Course Description:
   This two-day beginner to intermediate R/S-plus course focuses
    on a broad spectrum of topics,
   from reading raw data to a comparison of R and S. We will learn
   the essentials of data manipulation, graphical visualization
   and R/S-plus programming. We will explore statistical data analysis
   tools,including graphics with data sets. How to enhance your plots.
   We will perform basic statistics and fit linear regression models.
   Participants are encouraged to bring data for interactive sessions

   With the following outline:
   - An Overview of R
   - Data Manipulation and Graphics
   - Using Lattice Graphics
   - A Comparison of R and S-Plus
   - How can R Complement SAS?
   - Writing Functions
   - Avoiding Loops
   - Vectorization
   - Statistical Modeling
   - Project Management
   - Techniques for Effective use of R and S
   - Enhancing Plots
   - Using High-level Plotting Functions
   - Building and Distributing Packages (libraries)

   Early Bird Research: $995 (Includes course materials and snacks);
   Email us for group discounts.
   Email Sue Turner: [3]sue at xlsolutions-corp.com
   Phone: 206-686-1578
   Visit us: [4]www.xlsolutions-corp.com/training.htm
   Please let us know if you and your colleagues are interested in this
   classto take advantage of group discount. Register now to secure your
   seat!
   Interested in R/Splus Adva nced course? email us.

   Cheers,
   Elvis Miller, PhD
   Manager Training.
   XLSolutions Corporation
   206 686 1578
   [5]www.xlsolutions-corp.com
   [6]elvis at xlsolutions-corp.com
   Email us for group discounts.
   Email Sue Turner: [7]sue at xlsolutions-corp.com
   Phone: 206-686-1578
   Visit us: [8]www.xlsolutions-corp.com/training.htm
   Please let us know if you and your colleagues are interested in this
   classto take advantage of group discount. Register now to secure your
   seat!
   Interested in R/Splus Advanced course? email us.

   Cheers,
   Elvis Miller, PhD
   Manager Training.
   XLSolutions Corporation
   206 686 1578
   [9]www.xlsolutions-corp.com
   [10]elvis at xlsolutions-corp.com

References

   1. http://www.xlsolutions-corp.com/
   2. http://www.xlsolutions-corp.com/
   3. mailto:sue at xlsolutions-corp.com
   4. http://www.xlsolutions-corp.com/training.htm
   5. http://www.xlsolutions-corp.com/
   6. mailto:elvis at xlsolutions-corp.com
   7. mailto:sue at xlsolutions-corp.com
   8. http://www.xlsolutions-corp.com/training.htm
   9. http://www.xlsolutions-corp.com/
  10. mailto:elvis at xlsolutions-corp.com


From mvdv at spamcop.net  Wed Mar  3 20:17:24 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 4 Mar 2004 06:17:24 +1100
Subject: [R] Adding text (coefts) to pairs panels
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF78F7@usrymx25.merck.com>
Message-ID: <000701c40154$29f3a310$344610ac@FEB0480>

Andy,
Thank you very much - that works and shows me how to go about working out
the range of possible values for the position co-ords.
Much appreciated.
Mark



> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Sent: Thursday, March 04, 2004 2:07 AM
> To: 'Mark Van De Vyver'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Adding text (coefts) to pairs panels
> 
> 
> You need to figure out the "user coordinates" for each panel, 
> so you can put the text within the range that's being 
> plotted.  Here's an example:
> 
> panel.myfitline <- function(x, y, digits=2, prefix="", cex.cor, ...) {
>     usr <- par("usr")
>     res<-panel.smooth(x,y, col.smooth="blue", ...)
>     reg <- coef(lm(y ~ x))
>     abline(coef=reg,untf=F)
>     const<-format(reg[1], trim = FALSE, digits = NULL, nsmall 
> = 0, justify = "left", ...)
>     const<-paste(prefix, const, sep="")
>     slope<-format(reg[2], trim = FALSE, digits = NULL, nsmall 
> = 0, justify = "left", ...)
>     slope<-paste(prefix, slope, sep="")
>     if(missing(cex.cor)) cex <- 0.8/strwidth(const)
>     text(.95*usr[2], usr[3] +.05*(usr[4]-usr[3]), const, cex=cex)
>     if(missing(cex.cor)) cex <- 0.8/strwidth(slope)
>     text(usr[1] + .1*(usr[2]-usr[1]), .98*usr[4], slope, cex=cex) }
> 
> HTH,
> Andy
> 
> > From: Mark Van De Vyver
> > 
> > Hi,
> > First of all, thanks for the efforts of all the developers
> > and contributors
> > - I'm very new to R and at the moment am just using it to 
> create some
> > graphics, but it seems to be quite powerful.
> > I've googled the archives and wasn't able to find anyhting 
> > that dealt with
> > this problem, so would appreciate any suggestions/tips.
> > 
> > In a pairs plot I'd like to plot a linear regression line in
> > each panel and,
> > also in each panel, I'd like to have some text in the panel 
> > showing the
> > coefts, etc.
> > I'm able to get the regressions line plotted, but not the 
> > text/coefficients.
> > Using one of the R examples, example(pairs), I have got this far:
> > 
> > panel.myfitline<-function(x, y, digits=2, prefix="", cex.cor, ...) {
> >     res<-panel.smooth(x,y, col.smooth="blue", ...)
> >     reg <- coef(lm(y ~ x))
> >     abline(coef=reg,untf=F)
> >     const<-format(reg[1], trim = FALSE, digits = NULL, nsmall 
> > = 0, justify =
> > "left", ...)
> >     const<-paste(prefix, const, sep="")
> >     slope<-format(reg[2], trim = FALSE, digits = NULL, nsmall 
> > = 0, justify =
> > "left", ...)
> >     slope<-paste(prefix, slope, sep="")
> >     if(missing(cex.cor)) cex <- 0.8/strwidth(const)
> >     text(0.1, 0.5, const, cex=cex)
> >     if(missing(cex.cor)) cex <- 0.8/strwidth(slope)
> >     text(0.5, 0.1, slope, cex=cex)
> > }
> > pairs(USJudgeRatings[1:5], lower.panel=panel.smooth, 
> > diag.panel=panel.hist,
> > upper.panel=panel.myfitline)
> > 
> > Mark Van De Vyver
> > PhD
> > Lecturer
> > Finance Discipline
> > School of Business
> > Faculty of Economics and Business
> > Economics & Business Building H69
> > The University of Sydney
> > Sydney  NSW  2006  Australia
> > Telephone: +61 2 9351-6452
> > Fax: +61 2 9351-6461
> > Mobile: 0428 281407
> > 
> mailto:mvdv at spamcop.net
> http:\\www.econ.usyd.edu.au
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From gracestat at yahoo.com  Wed Mar  3 20:25:24 2004
From: gracestat at yahoo.com (Grace Conlon)
Date: Wed, 3 Mar 2004 11:25:24 -0800 (PST)
Subject: [R] How to read Excel file and access the data item?
Message-ID: <20040303192524.23303.qmail@web21405.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040303/8bce9e3b/attachment.pl

From Carlisle.Thacker at noaa.gov  Wed Mar  3 20:28:33 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Wed, 03 Mar 2004 14:28:33 -0500
Subject: [R] need help with smooth.spline
Message-ID: <404631E1.B9F5A143@noaa.gov>

Dear R listers,

When using smooth.spline to interpolate data, results are generally
good.  However, some cases produce totally unreasonable results.

The data are values of pressure, temperature, and salinity from a
probe that is lowered into the ocean, and the objective is to
interpolate temperature and salinity to specified pressures.  While
smooth.spline provides excellent values at the observed pressures,
there are cases when the values at the desired pressures are
unusable.  A dataframe with four such profiles, indicated by values of
id, is attached.  My target values for pressure are seq(25,1600,25),
but 1:500 is also interesting.  

Setting all.knots = TRUE helps, but it would be nice to be able to do
better.

Any suggestions?

Thanks,

Carlisle

> version
         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    8.0                 
year     2003                
month    10                  
day      08                  
language R                   


-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From rbaer at atsu.edu  Wed Mar  3 20:35:01 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 3 Mar 2004 13:35:01 -0600
Subject: [R] get.hist.quote - is great, but am I missing something?
References: <404628AB.5030006@bluewin.ch>
Message-ID: <00e601c40156$9eb14750$2e80010a@BigBaer>

> > I guess I'm not understanding the object that get.hist.quote makes. In
> > general, what are R facilities for discovering what a given object is?
>
> I suggest, you study first one of the beginners manuals of the R
> environment: http://cran.r-project.org/manuals.html
> Then you would easily see that, e.g., x[,"Close"] gives you the series
> of closing prices, time(x) gives you the series of times and so on...

Some things you could do to investigate the object (besides printing it
out):
str(x)
class(x)
mode(x)
structure(s)

Rob



From aiminy at iastate.edu  Wed Mar  3 20:42:04 2004
From: aiminy at iastate.edu (Aimin Yan)
Date: Wed, 03 Mar 2004 13:42:04 -0600
Subject: [R] (no subject)
Message-ID: <6.0.1.1.2.20040303134132.01cc1488@aiminy.mail.iastate.edu>

how to produce a  Row Reduced Echelon Form for a matrix in R?
Aimin Yan



From nali at umn.edu  Wed Mar  3 21:20:55 2004
From: nali at umn.edu (Na Li)
Date: Wed, 03 Mar 2004 14:20:55 -0600
Subject: [R] How to read Excel file and access the data item?
In-Reply-To: <20040303192524.23303.qmail@web21405.mail.yahoo.com> (Grace
	Conlon's message of "Wed, 3 Mar 2004 11:25:24 -0800 (PST)")
References: <20040303192524.23303.qmail@web21405.mail.yahoo.com>
Message-ID: <7o4qt5lm1k.fsf@bass.biostat.umn.edu>

On 3 Mar 2004, Grace Conlon verbalised:

> In R, How to read Excel file and access the data item?
> Thank you.

The catdoc package (http://www.45.free.net/~vitus/ice/catdoc/) includes
a program xls2csv that converts xls to csv (comma separated value) files.

In a Unix-like system, I often do

blah <- read.csv (pipe ("xls2csv somefile.xls"))

that saves the intermediate step of creating a csv file.  

An Excel file can have irregular cells (descriptions, graphs, etc) and
multiple sheets (xls2csv puts all sheets in one file, consecutively). You
need watch out for those and double check to make sure you are reading the
correct data.

I am not sure how good xls2csv is in handling xls files with formula
though.

Michael



From mhr at cens.ucla.edu  Wed Mar  3 21:30:26 2004
From: mhr at cens.ucla.edu (Mohammad Rahimi)
Date: Wed, 3 Mar 2004 12:30:26 -0800 (PST)
Subject: [R] sketchin a line
Message-ID: <Pine.GSO.4.58.0403031228500.12424@cens1>



Hi folks,

I guess it is a very premitive question.

If i have already created an image.

image(input)

and i want to add a line from (x0,y0) to (x1,y1) on that
image. how i can sketch a line superimposed on that.

Best regards
-m



From ripley at stats.ox.ac.uk  Wed Mar  3 21:33:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Mar 2004 20:33:32 +0000 (GMT)
Subject: [R] generating normal numbers: GetRNGstate, PutRNGstate
In-Reply-To: <Pine.LNX.4.44.0403031328430.30386-100000@ge.biomath.mssm.edu>
Message-ID: <Pine.LNX.4.44.0403032026001.4891-100000@gannet.stats>

On Wed, 3 Mar 2004, Yongchao Ge wrote:

> I'd like to generate thousands of normal numbers from my C function using 
> the C API functions provided R. I have two options:
> 
> 1. double norm_rand(); (page 61 of R extension 1.8.1)
> 2. double rnorm(double mu, double sigma); (page 58 of R extension 1.8.1)

Page numbers depend on the paper size you used (and I guess you didn't use
the world standard A4)  but what I guess is your page 61 says

  See Random numbers, for the protocol in using the random-variate 
  routines.

and that refers you back to page 58.

> If my understanding of R-exts is correct, then I only need to call 
> GetRNGstate once, and then call 1000 norm_rand, and then call 
> PutRNGstate once for the 1st option.
> 
> For the 2nd option, I have to call 1000 times for each of GetRNGstate, 
> rnorm, and PutRNGstate.

That is *not* what the manual says.  Just the same as 1).

> The pseudo-code for option 1 will be:
> 
> Method 1:
> 
> GetRNGstate();
> for(i=1;i<1000;i++){
>    x[i]=norm_rand();
> }
> PutRNGstate();
> 
> The pseudo code for option 2 will be:
> 
> Method 2:
> 
> for(i=1;i<1000;i++){
>    GetRNGstate();
>    x[i]=rnorm(0,1);
>    PutRNGstate();
> }
> 
> Of course, I can also write a slower version for option 1, i.e. call 
> GetRNGstate and PutRNGstate each time for norm_rand.
> 
> method 3:
> 
> for(i=1;i<1000;i++){
>    GetRNGstate();
>    x[i]=norm_rand(); 
>    PutRNGstate();
> }
> 
> 
> My questions are:
> 
> 1. Are the three methods all correct for generating random numbers?

Yes, given the answer to 2.

> 2. Are they generating the exactly the same random number if we have the 
> same random seed?

Yes.  Try it and see!

> I searched the R help and google and I didn't find answers. The reason for 
> to ask is that if both of the above answers are right, then I'd better 
> off use the method 1, which is the fastest as I have to generate 
> hundreds thousands of normal numbers.

See the above answer.  And please don't expect answers to questions like
this to be in the R-help: they are in the manuals and especially in the
source code.  It would be a good exercise for you to confirm this by

A) reading the R sources
B) doing some tests for yourself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From srivas at k-state.edu  Wed Mar  3 21:35:06 2004
From: srivas at k-state.edu (Sivakumar Mohandass)
Date: Wed, 3 Mar 2004 14:35:06 -0600
Subject: [R] Publication quality graphs
Message-ID: <000001c4015f$04101420$af378281@vijaya>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040303/6049b365/attachment.pl

From flom at ndri.org  Wed Mar  3 21:42:12 2004
From: flom at ndri.org (Peter Flom)
Date: Wed, 03 Mar 2004 15:42:12 -0500
Subject: [R] Location of polr function
Message-ID: <s045fcf8.071@MAIL.NDRI.ORG>

Hello

I am running R 1.8.1 on a  Windows platform

I am attempting to fit an ordinal logistic regression model, using the
polr function, as described in Venables and Ripley.  But when I try

model4 <- polr(ypsxcat~committed + as.factor(sex)
 + as.factor(drugusey) + anycsw + as.factor(sex)*committed
  + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
duhray)

I get a message that the polr function was not found.  

Any help appreciated

thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From andy_liaw at merck.com  Wed Mar  3 21:51:51 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 3 Mar 2004 15:51:51 -0500
Subject: [R] Publication quality graphs
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7902@usrymx25.merck.com>

Presumably you have the data and R scripts that generated the jpegs.  Just
substitute the calls to jpeg() (or bitmap()) to postscript(...,
onefile=FALSE) and you shall have the highest quality EPS.

HTH,
Andy

> From: Sivakumar Mohandass
> 
> Dear all,
> 
> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried 
> converting .jpeg
> files to these formats but it looses its quality both on the 
> screen and
> on paper. Could some one please help.
> 
> Thanks in advance,
> ____________________
> Sivakumar Mohandass,
> Department of Entomology,
> Kansas State University
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ligges at statistik.uni-dortmund.de  Wed Mar  3 21:58:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 03 Mar 2004 21:58:03 +0100
Subject: [R] Publication quality graphs
References: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <404646DB.328A0196@statistik.uni-dortmund.de>



Sivakumar Mohandass wrote:
> 
> Dear all,
> 
> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
> files to these formats but it looses its quality both on the screen and
> on paper. Could some one please help.

So you might want to use the postscript() device to produce eps.

See ?Devices and ?postscript for more details

Uwe Ligges



> Thanks in advance,
> ____________________
> Sivakumar Mohandass,
> Department of Entomology,
> Kansas State University
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From feh3k at spamcop.net  Wed Mar  3 21:57:14 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 3 Mar 2004 20:57:14 +0000
Subject: [R] Publication quality graphs
In-Reply-To: <000001c4015f$04101420$af378281@vijaya>
References: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <20040303205714.537764c1.feh3k@spamcop.net>

On Wed, 3 Mar 2004 14:35:06 -0600
"Sivakumar Mohandass" <srivas at k-state.edu> wrote:

> Dear all,
> 
> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
> files to these formats but it looses its quality both on the screen and
> on paper. Could some one please help.
> 
> Thanks in advance,
> ____________________
> Sivakumar Mohandass,
> Department of Entomology,
> Kansas State University

Why would you need to convert from jpeg?  Start out with eps.

Frank



From ligges at statistik.uni-dortmund.de  Wed Mar  3 22:00:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 03 Mar 2004 22:00:19 +0100
Subject: [R] How to read Excel file and access the data item?
References: <20040303192524.23303.qmail@web21405.mail.yahoo.com>
Message-ID: <40464763.25985B8B@statistik.uni-dortmund.de>



Grace Conlon wrote:
> 
> In R, How to read Excel file and access the data item?
> Thank you.
> 

See R's Data Import/Export manual. At once, two solutions spring to
mind:

a) Export to csv or any other ASCII format and import in R.
b) RODBC


Uwe Ligges



From MSchwartz at medanalytics.com  Wed Mar  3 22:01:34 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 03 Mar 2004 15:01:34 -0600
Subject: [R] Publication quality graphs
In-Reply-To: <000001c4015f$04101420$af378281@vijaya>
References: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <1078347463.6995.20.camel@localhost.localdomain>

On Wed, 2004-03-03 at 14:35, Sivakumar Mohandass wrote:
> Dear all,
> 
> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
> files to these formats but it looses its quality both on the screen and
> on paper. Could some one please help.


Your best option is to generate an EPS file using the postscript()
function.

See ?postscript for more information and take careful note of the
function arguments required to create an EPS file. Principally:

     The postscript produced by R is EPS (_Encapsulated PostScript_)
     compatible, and can be included into other documents, e.g., into
     LaTeX, using '\includegraphics{<filename>}'.  For use in this way
     you will probably want to set 'horizontal = FALSE, onefile =
     FALSE, paper = "special"'.

You will need to specify the 'height' and 'width' arguments to define
your page size.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Wed Mar  3 22:06:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 03 Mar 2004 22:06:42 +0100
Subject: [R] Calculating percentiles from Grouped Data (gapply)
References: <4045C73A.32159.D27786@localhost>
Message-ID: <404648E2.4A5A8638@statistik.uni-dortmund.de>



Patrick Hausmann wrote:
> 
> Dear R-list,
> 
> I try to calculate the 10- and 90-Percentile from grouped data.
> Using 'lappy' it works fine but I have no success with 'gapply':
> 
> df <- data.frame(test = runif(100))
> df <- df[order(df[,1]),]
> z <- rep(1:10, each = 10)
> lapply(split(df, z), function(x) quantile(x, probs=c(10,90)/100))
> 
> library(nlme)
> gapply(as.data.frame(df), which=1,
>           function(x) quantile(x, probs=c(10,90)/100), group=z)
> #  Error in sort(x, partial = unique(c(lo, hi))) :
> #    `x' must be atomic


For use with quantile(), you have to make x in your anonymous function a
vector as in:

  gapply(as.data.frame(df), which=1,
      function(x) quantile(x[[1]], probs=c(10,90)/100), group=z)


Uwe Ligges

> Thanks for any help
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Wed Mar  3 22:10:07 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 03 Mar 2004 16:10:07 -0500
Subject: [R] Publication quality graphs
In-Reply-To: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <BC6BB3DF.515E%sdavis2@mail.nih.gov>

See ?postscript.

Sean


On 3/3/04 3:35 PM, "Sivakumar Mohandass" <srivas at k-state.edu> wrote:

> Dear all,
> 
> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
> files to these formats but it looses its quality both on the screen and
> on paper. Could some one please help.
> 
> Thanks in advance,
> ____________________
> Sivakumar Mohandass,
> Department of Entomology,
> Kansas State University
> 
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Yongchao.Ge at mssm.edu  Wed Mar  3 22:09:00 2004
From: Yongchao.Ge at mssm.edu (Yongchao Ge)
Date: Wed, 03 Mar 2004 16:09:00 -0500 (EST)
Subject: [R] generating normal numbers: GetRNGstate, PutRNGstate
In-Reply-To: <Pine.LNX.4.44.0403032026001.4891-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0403031557290.30386-100000@ge.biomath.mssm.edu>

See my reply at the end of the email.

> > I'd like to generate thousands of normal numbers from my C function using 
> > the C API functions provided R. I have two options:
> > 
> > 1. double norm_rand(); (page 61 of R extension 1.8.1)
> > 2. double rnorm(double mu, double sigma); (page 58 of R extension 1.8.1)
> 
> Page numbers depend on the paper size you used (and I guess you didn't use
> the world standard A4)  but what I guess is your page 61 says
> 
>   See Random numbers, for the protocol in using the random-variate 
>   routines.
> 
> and that refers you back to page 58.
> 
> > If my understanding of R-exts is correct, then I only need to call 
> > GetRNGstate once, and then call 1000 norm_rand, and then call 
> > PutRNGstate once for the 1st option.
> > 
> > For the 2nd option, I have to call 1000 times for each of GetRNGstate, 
> > rnorm, and PutRNGstate.
> 
> That is *not* what the manual says.  Just the same as 1).
> 

Hi Prof Ripley

Thank you for the answers. Sorry for the misconfusion of the pages numbers 
as the "local standard" here is using "letter" size, not "A4".

The following is the last paragraph above section 6.4

The C code behind R's rxxx functions can be accessed by including the 
header file  Rmath.h ; See Section 5.7.1 [Distribution functions], page 
61. Those calls generate a single variate and should also be enclosed in 
calls to GetRNGstate and PutRNGstate.

Note the "*** generate a single variate and should also be enclosed in
calls to GetRNGstate***" in the last sentence. That's why I'm suspecting I 
have to use the method 2. Probably I have misunderstood this sentence. 

Yongchao



From andy_liaw at merck.com  Wed Mar  3 22:33:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 3 Mar 2004 16:33:24 -0500
Subject: [R] Location of polr function
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7904@usrymx25.merck.com>

help.search() is your friend:

polr(MASS)               Proportional Odds Logistic Regression

HTH,
Andy

> From: Peter Flom
> 
> Hello
> 
> I am running R 1.8.1 on a  Windows platform
> 
> I am attempting to fit an ordinal logistic regression model, using the
> polr function, as described in Venables and Ripley.  But when I try
> 
> model4 <- polr(ypsxcat~committed + as.factor(sex)
>  + as.factor(drugusey) + anycsw + as.factor(sex)*committed
>   + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
> duhray)
> 
> I get a message that the polr function was not found.  
> 
> Any help appreciated
> 
> thanks
> 
> Peter
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From macq at llnl.gov  Wed Mar  3 22:48:59 2004
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 3 Mar 2004 13:48:59 -0800
Subject: [R] Publication quality graphs
In-Reply-To: <000001c4015f$04101420$af378281@vijaya>
References: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <p06002004bc6c01cea085@[128.115.153.6]>

The R postscript() driver will produce eps files. See ?postscript.

If the journal requires that eps files include a preview image 
(something that will display the image on the computer screen) then 
that can be added after the fact with, for example, Adobe 
Illustrator. R's driver does not produce them.

-Don

At 2:35 PM -0600 3/3/04, Sivakumar Mohandass wrote:
>Dear all,
>
>A journal in which we wanted our manuscript published requires the
>figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
>files to these formats but it looses its quality both on the screen and
>on paper. Could some one please help.
>
>Thanks in advance,
>____________________
>Sivakumar Mohandass,
>Department of Entomology,
>Kansas State University
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From apjaworski at mmm.com  Wed Mar  3 22:52:43 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Wed, 3 Mar 2004 15:52:43 -0600
Subject: [R] Location of polr function
Message-ID: <OFCF40D295.83433979-ON86256E4C.00765CA5-86256E4C.00782F3D@mmm.com>


Peter,

These things can be a little mysterious at the beginning.  Lots of things
in R are arranged in groups called packages (or libraries).  Some basic
ones come preinstalled with R, some you have to install yourself.  If you
type

      library()

at your R prompt you should get a list of all packages installed on your
system.

In order to use the functionality of a package you have to load it by
specifying its name as an argument to the library call.  For example,

      library(nlme)

will load the package named "nlme"  (if it is installed on your system) and
give you the functionality of the nlme package, i.e. access to its
functions..  Typing

      search()

will show you what packages are already loaded on your system.

Search.help will allow you to find in which package your function resides
in.  Typing

      search.help(polr)

on my system returns

      polr(MASS)              Proportional Odds Logistic Regression

meaning that the polr function belongs to the package MASS which is
installed on your system.  The only thing remaining to do is typing

      library(MASS)

Of course, if MASS is not installed on your system you will get nothing in
response (actually you will get a message saying that nothing appropriate
was found).  Then, you would need to go to a CRAN site and use their search
engine to find things about polr function.

Hope this helps,

Andy


__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+--------------------------------------------------->
|         |           "Peter Flom" <flom at ndri.org>            |
|         |           Sent by:                                |
|         |           r-help-bounces+apjaworski=mmm.com at stat.m|
|         |           ath.ethz.ch                             |
|         |                                                   |
|         |                                                   |
|         |           03/03/2004 14:42                        |
|         |                                                   |
|---------+--------------------------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       <r-help at stat.math.ethz.ch>                                                                                   |
  |      cc:                                                                                                                    |
  |      Subject:  [R] Location of polr function                                                                                |
  >-----------------------------------------------------------------------------------------------------------------------------|




Hello

I am running R 1.8.1 on a  Windows platform

I am attempting to fit an ordinal logistic regression model, using the
polr function, as described in Venables and Ripley.  But when I try

model4 <- polr(ypsxcat~committed + as.factor(sex)
 + as.factor(drugusey) + anycsw + as.factor(sex)*committed
  + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
duhray)

I get a message that the polr function was not found.

Any help appreciated

thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Mar  3 23:03:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 23:03:16 +0100
Subject: [R] Publication quality graphs
In-Reply-To: <000001c4015f$04101420$af378281@vijaya>
References: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <x2brndbnbv.fsf@biostat.ku.dk>

"Sivakumar Mohandass" <srivas at k-state.edu> writes:

> Dear all,
> 
> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
> files to these formats but it looses its quality both on the screen and
> on paper. Could some one please help.

The postscript() device will do EPS, as will dev.copy2eps(). In fact,
almost all graphics in Introductory Statistics with R were done with

X11(height=3.5,width=4.4,pointsize=8)
par(mar=c(4,4,3,2)+.1)

and then (e.g.)

plot(height,weight)
dev.copy2eps(file="h-w.ps")

(plus a epstopdf step, but you wouldn't need that)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From flom at ndri.org  Wed Mar  3 23:15:21 2004
From: flom at ndri.org (Peter Flom)
Date: Wed, 03 Mar 2004 17:15:21 -0500
Subject: [R] location of polr - more details
Message-ID: <s04612c5.023@MAIL.NDRI.ORG>

Andrew Robinson and Andy Jaworski both pointed out that polr is in
library MASS.  MASS seems to be missing from my installation, and the
former Andrew suggested that I reinstall it.

When I tried help.search('polr'), nothing was found, and when I tried
library(MASS) or require(MASS) the package wasn't found.

Thanks for the help.  I'm kind of mystified as to how MASS could not be
installed with 1.8.1 without any errors or warnings, but I will try to
reinstall R.  

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From itayf at fhcrc.org  Wed Mar  3 23:11:40 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Wed, 3 Mar 2004 14:11:40 -0800 (PST)
Subject: [R] Publication quality graphs
In-Reply-To: <000001c4015f$04101420$af378281@vijaya>
Message-ID: <Pine.LNX.4.44.0403031410450.16416-100000@cezanne.fhcrc.org>


On Wed, 3 Mar 2004, Sivakumar Mohandass wrote:

> A journal in which we wanted our manuscript published requires the
> figures as a tiff, eps or PowerPoint formated. I tried converting .jpeg
> files to these formats but it looses its quality both on the screen and
> on paper. Could some one please help.
> 

help(postscript)



From ccleland at optonline.net  Wed Mar  3 23:16:09 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 03 Mar 2004 17:16:09 -0500
Subject: [R] Location of polr function
In-Reply-To: <s045fcf8.071@MAIL.NDRI.ORG>
References: <s045fcf8.071@MAIL.NDRI.ORG>
Message-ID: <40465929.8040903@optonline.net>

Peter Flom wrote:
> I am running R 1.8.1 on a  Windows platform
> 
> I am attempting to fit an ordinal logistic regression model, using the
> polr function, as described in Venables and Ripley.  But when I try
> 
> model4 <- polr(ypsxcat~committed + as.factor(sex)
>  + as.factor(drugusey) + anycsw + as.factor(sex)*committed
>   + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
> duhray)
> 
> I get a message that the polr function was not found.  

Load the MASS package prior to calling polr().

library(MASS)

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From h.wickham at auckland.ac.nz  Wed Mar  3 23:31:55 2004
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Thu, 04 Mar 2004 11:31:55 +1300
Subject: [R] Location of polr function
In-Reply-To: <s045fcf8.071@MAIL.NDRI.ORG>
References: <s045fcf8.071@MAIL.NDRI.ORG>
Message-ID: <40465CDB.2020101@auckland.ac.nz>

> I get a message that the polr function was not found.  

Did you remember to load the MASS library first?
library(MASS)

Hadley



From p.dalgaard at biostat.ku.dk  Wed Mar  3 23:32:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Mar 2004 23:32:24 +0100
Subject: [R] Location of polr function
In-Reply-To: <s045fcf8.071@MAIL.NDRI.ORG>
References: <s045fcf8.071@MAIL.NDRI.ORG>
Message-ID: <x27jy1blzb.fsf@biostat.ku.dk>

"Peter Flom" <flom at ndri.org> writes:

> Hello
> 
> I am running R 1.8.1 on a  Windows platform
> 
> I am attempting to fit an ordinal logistic regression model, using the
> polr function, as described in Venables and Ripley.  But when I try
> 
> model4 <- polr(ypsxcat~committed + as.factor(sex)
>  + as.factor(drugusey) + anycsw + as.factor(sex)*committed
>   + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
> duhray)
> 
> I get a message that the polr function was not found.  
> 
> Any help appreciated

library(MASS)

(and help.search("polr") might have given you the idea)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Mar  3 23:38:00 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 03 Mar 2004 14:38:00 -0800
Subject: [R] Location of polr function
In-Reply-To: <s045fcf8.071@MAIL.NDRI.ORG>
References: <s045fcf8.071@MAIL.NDRI.ORG>
Message-ID: <40465E48.10108@pdf.com>

 > help.search("polr")
Help files with alias or concept or title matching 'polr' using
regular expression matching:



polr(MASS)              Proportional Odds Logistic Regression



Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.
 >
      Did you ask "library(MASS)" first? 
      hope this helps.  spencer graves

Peter Flom wrote:

>Hello
>
>I am running R 1.8.1 on a  Windows platform
>
>I am attempting to fit an ordinal logistic regression model, using the
>polr function, as described in Venables and Ripley.  But when I try
>
>model4 <- polr(ypsxcat~committed + as.factor(sex)
> + as.factor(drugusey) + anycsw + as.factor(sex)*committed
>  + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
>duhray)
>
>I get a message that the polr function was not found.  
>
>Any help appreciated
>
>thanks
>
>Peter
>
>Peter L. Flom, PhD
>Assistant Director, Statistics and Data Analysis Core
>Center for Drug Use and HIV Research
>National Development and Research Institutes
>71 W. 23rd St
>www.peterflom.com
>New York, NY 10010
>(212) 845-4485 (voice)
>(917) 438-0894 (fax)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From kjetil at entelnet.bo  Wed Mar  3 23:53:01 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 03 Mar 2004 18:53:01 -0400
Subject: [R] Location of polr function
In-Reply-To: <s045fcf8.071@MAIL.NDRI.ORG>
Message-ID: <4046298D.28195.B729A6@localhost>

On 3 Mar 2004 at 15:42, Peter Flom wrote:

> Hello
> 
> I am running R 1.8.1 on a  Windows platform
> 
> I am attempting to fit an ordinal logistic regression model, using the
> polr function, as described in Venables and Ripley.  But when I try

It would help if you started by saying 

library(MASS)

Not knowing that, you could find out by trying

help.search("polr")

Kjetil Halvorsen

> 
> model4 <- polr(ypsxcat~committed + as.factor(sex)
>  + as.factor(drugusey) + anycsw + as.factor(sex)*committed
>   + as.factor(sex)*as.factor(drugusey)+as.factor(sex)*anycsw, data =
> duhray)
> 
> I get a message that the polr function was not found.  
> 
> Any help appreciated
> 
> thanks
> 
> Peter
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From v_bill_pikounis at merck.com  Thu Mar  4 00:11:11 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Wed, 3 Mar 2004 18:11:11 -0500
Subject: [R] Publication quality graphs
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F04156127@usrymx18.merck.com>

Hi Sivakumar,
Also, just of potential note: in case you are working with Windows and
generate postscript files, you will not see the EPS image on-screen when it
is inserted for instance in a Word document.  Rest assured that if you print
the document, it will show up.  All journal editors can see such EPS files
on screen since they will use true publishing software like Adobe's.

Hope that helps.
Bill


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> Sent: Wednesday, March 03, 2004 4:02 PM
> To: Sivakumar Mohandass
> Cc: R-Help
> Subject: Re: [R] Publication quality graphs
> 
> 
> On Wed, 2004-03-03 at 14:35, Sivakumar Mohandass wrote:
> > Dear all,
> > 
> > A journal in which we wanted our manuscript published requires the
> > figures as a tiff, eps or PowerPoint formated. I tried 
> converting .jpeg
> > files to these formats but it looses its quality both on 
> the screen and
> > on paper. Could some one please help.
> 
> 
> Your best option is to generate an EPS file using the postscript()
> function.
> 
> See ?postscript for more information and take careful note of the
> function arguments required to create an EPS file. Principally:
> 
>      The postscript produced by R is EPS (_Encapsulated PostScript_)
>      compatible, and can be included into other documents, e.g., into
>      LaTeX, using '\includegraphics{<filename>}'.  For use in this way
>      you will probably want to set 'horizontal = FALSE, onefile =
>      FALSE, paper = "special"'.
> 
> You will need to specify the 'height' and 'width' arguments to define
> your page size.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From bxc at steno.dk  Thu Mar  4 00:39:51 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 4 Mar 2004 00:39:51 +0100
Subject: [R] read.spss and time/date information
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179DEE3@exdkba022.novo.dk>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.] On Behalf Of 
> torsten at hothorn.deethz.ch
> Sent: Wednesday, March 03, 2004 6:27 PM
> To: Peter Dalgaard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] read.spss and time/date information
> 
> 
> 
> 
> On Wed, 3 Mar 2004, Peter Dalgaard wrote:
> 
> > Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:
> >
> > > Could anyone give me a a hint how I can convert 13264300800 to 
> > > 2003/02/11 again, please?
> >
> > > ISOdate(1582,10,14)  + 13264300800
> > [1] "2003-02-11 13:00:00 CET"
> > > ISOdate(1582,10,14)  +  13142476800
> > [1] "1999-04-03 14:00:00 CEST"
> >
> > [October 14, 1582 is Day 1 of the Gregorian calendar.]
> >
> 
> I tried January 1th 1970 as "baseline" but I never would have 
> dreamed of October 14, 1582.

This is the day the present Gregorian calendar was instituted, replacing
the
old Julian that did not have leap years. So it is actually quite
sensible
as it is the earliest possible date that is in unbroken sequence to
present
times. 

Actually I am amazed that this has not been chosen as the natural origin
for
any of the R-pakages...

Bendix Carstensen

> 
> Thanks to all responders!
> 
> Torsten
> 
> > --
> >    O__  ---- Peter Dalgaard             Blegdamsvej 3
> >   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> >  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Wanzare at HCJP.com  Thu Mar  4 00:47:34 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Thu, 4 Mar 2004 08:47:34 +0900
Subject: [R] get.hist.quote
Message-ID: <1CBA12F2D414914989C723D196B287DC13D258@jp-svr-ex1.HCJP.COM>

In tseries package. You might have to download & install the package If
library(tseries) doesn't work. 

HTH

Manoj

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
Sent: Thursday, March 04, 2004 2:53 AM
To: r-help at stat.math.ethz.ch
Subject: [R] get.hist.quote

Dear R People:

Here is a silly one:
Where is get.hist.quote, please?

Thanks,
Erin
mailto: hodgess at gator.uhd.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Mar  4 01:08:35 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  3 Mar 2004 19:08:35 -0500 (EST)
Subject: [R] How to read Excel file and access the data item?
Message-ID: <20040304000835.0D2403976@mprdmxin.myway.com>


There are a number of ways to do this:

1. select range including headers in Excel and
copy to clipboard (via ctrl-C) and in R:
mydata <- read.table("clipboard", header=T)

2. In Excel File | Save As and select csv as type.  In R:
mydata <- read.csv("/myfile.csv")

3. RODBC package
This package implements a Microsoft ODBC connection.

e.g.
      require(RODBC)
      z <- odbcConnectExcel("dd.xls")
      dd <- sqlFetch(z,"Sheet1")
      close(z)

4. RDCOMClient package

See http://www.omegahat.org/RDCOMClient/ 
This packages implemented a Microsoft COM connection.

5. Thomas Baier's R COM package

http://sunsite.univie.ac.at/rcom/
Similar to RDCOMClient.

6. Third party utilities

There are a number of free utilities for converting from
Excel to csv that you can find via googling.  

---
Date:   Wed, 3 Mar 2004 11:25:24 -0800 (PST) 
From:   Grace Conlon <gracestat at yahoo.com>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] How to read Excel file and access the data item? 

 
In R, How to read Excel file and access the data item?
Thank you.



From christof.bigler at colorado.edu  Thu Mar  4 02:14:25 2004
From: christof.bigler at colorado.edu (Christof Bigler)
Date: Wed, 3 Mar 2004 18:14:25 -0700
Subject: [R] Ordinal logistic regression using spatial data
Message-ID: <46DCBF13-6D79-11D8-AB59-000A27D7D440@colorado.edu>

I have a spatial data set with ordinal response variable containing 
four levels. I would like to know if and how spatial autocorrelation 
can be taken into account when ordinal logistic regression is used 
(e.g. the function lrm from the Design package).

Thanks for your help!
Christof



From jfox at mcmaster.ca  Thu Mar  4 02:37:43 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 3 Mar 2004 20:37:43 -0500
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <6.0.1.1.2.20040303134132.01cc1488@aiminy.mail.iastate.edu>
Message-ID: <20040304013741.MOCW21160.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Amin,

I have a function (created just for demonstration, and reproduced below) for
finding the row-echelon form of a matrix. I'm sure that many list members
could produce something that's better numerically, but this should be OK at
least for toy problems.

John

--------- snip -------------

rowEchelonForm <- function(X, tol=.Machine$double.eps){
    if ((!is.matrix(X)) || (!is.numeric(X))) stop("argument must be a
numeric matrix")
    Z <- X
    for (i in 1:min(dim(X))){
        if (i > 1) Z[i-1,] <- 0
        which <- which.max(abs(Z[,i]))  # find maximum pivot in current
column at or below current row
        pivot <- X[which, i]
        if (abs(pivot) <= tol) next     # check for 0 pivot
        if (which > i) X[c(i,which),] <- X[c(which,i),]  # exchange rows
        X[i,] <- X[i,]/pivot            # pivot
        row <- X[i,]                    
        X <- X - outer(X[,i], row)      # sweep
        X[i,] <- row                    # restore current row
        }
    n <- nrow(X)
    for (i in 1:n) if (max(abs(X[i,])) <= tol) X[c(i,n),] <- X[c(n,i),]   #
0 rows to bottom
    X
    }
        

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
> Sent: Wednesday, March 03, 2004 2:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
> 
> how to produce a  Row Reduced Echelon Form for a matrix in R?
> Aimin Yan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Mar  4 02:47:46 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 3 Mar 2004 20:47:46 -0500
Subject: [R] need help with smooth.spline
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7905@usrymx25.merck.com>

If you really want interpolation, should you be using spline() rather than
smooth.spline()?  The later is for smoothing data observed with noise, not
for interpolation.

Andy

> From: W. C. Thacker
> 
> Dear R listers,
> 
> When using smooth.spline to interpolate data, results are generally
> good.  However, some cases produce totally unreasonable results.
> 
> The data are values of pressure, temperature, and salinity from a
> probe that is lowered into the ocean, and the objective is to
> interpolate temperature and salinity to specified pressures.  While
> smooth.spline provides excellent values at the observed pressures,
> there are cases when the values at the desired pressures are
> unusable.  A dataframe with four such profiles, indicated by values of
> id, is attached.  My target values for pressure are seq(25,1600,25),
> but 1:500 is also interesting.  
> 
> Setting all.knots = TRUE helps, but it would be nice to be able to do
> better.
> 
> Any suggestions?
> 
> Thanks,
> 
> Carlisle
> 
> > version
>          _                   
> platform sparc-sun-solaris2.9
> arch     sparc               
> os       solaris2.9          
> system   sparc, solaris2.9   
> status                       
> major    1                   
> minor    8.0                 
> year     2003                
> month    10                  
> day      08                  
> language R                   
> 
> 
> -- 
> 
> William Carlisle Thacker                            
>                                                     
> Atlantic Oceanographic and Meteorological Laboratory
> 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
> Office: (305) 361-4323           Fax: (305) 361-4392
> 
> "Too many have dispensed with generosity 
>      in order to practice charity."     Albert Camus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From feh3k at spamcop.net  Thu Mar  4 03:29:16 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 4 Mar 2004 02:29:16 +0000
Subject: [R] Ordinal logistic regression using spatial data
In-Reply-To: <46DCBF13-6D79-11D8-AB59-000A27D7D440@colorado.edu>
References: <46DCBF13-6D79-11D8-AB59-000A27D7D440@colorado.edu>
Message-ID: <20040304022916.5e550cb6.feh3k@spamcop.net>

On Wed, 3 Mar 2004 18:14:25 -0700
Christof Bigler <christof.bigler at colorado.edu> wrote:

> I have a spatial data set with ordinal response variable containing 
> four levels. I would like to know if and how spatial autocorrelation 
> can be taken into account when ordinal logistic regression is used 
> (e.g. the function lrm from the Design package).
> 
> Thanks for your help!
> Christof

lrm (through the robcov and bootcov functions) can take into account
general intra-cluster correlations.  This works best when clusters do not
contain very many observations.  Different clusters are assumed to be
independent.

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From juliacline at sbcglobal.net  Thu Mar  4 03:44:26 2004
From: juliacline at sbcglobal.net (Cline Julia)
Date: Wed, 3 Mar 2004 18:44:26 -0800
Subject: [R] can RAqua open an SPSS file
Message-ID: <D9E545AF-6D85-11D8-9D96-000A95CD6FE0@sbcglobal.net>

I have an SPSS data file that I would like to analyze using RAqua.  Is 
there any way for me to open the file using RAqua.  I can probably get 
a hard copy of the file contents from my collaborator, but I'd rather 
not have to deal with all that input.

Julie Cline



From gblevins at mn.rr.com  Thu Mar  4 04:19:00 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Wed, 3 Mar 2004 21:19:00 -0600
Subject: [R] A file manipulation question
Message-ID: <011d01c40197$70a02750$1c361d41@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040303/c327ff66/attachment.pl

From spencer.graves at pdf.com  Thu Mar  4 04:45:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 03 Mar 2004 19:45:12 -0800
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <20040304013741.MOCW21160.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20040304013741.MOCW21160.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <4046A648.1010604@pdf.com>

How does this compare with R of the qr decomposition?  spencer graves

John Fox wrote:

>Dear Amin,
>
>I have a function (created just for demonstration, and reproduced below) for
>finding the row-echelon form of a matrix. I'm sure that many list members
>could produce something that's better numerically, but this should be OK at
>least for toy problems.
>
>John
>
>--------- snip -------------
>
>rowEchelonForm <- function(X, tol=.Machine$double.eps){
>    if ((!is.matrix(X)) || (!is.numeric(X))) stop("argument must be a
>numeric matrix")
>    Z <- X
>    for (i in 1:min(dim(X))){
>        if (i > 1) Z[i-1,] <- 0
>        which <- which.max(abs(Z[,i]))  # find maximum pivot in current
>column at or below current row
>        pivot <- X[which, i]
>        if (abs(pivot) <= tol) next     # check for 0 pivot
>        if (which > i) X[c(i,which),] <- X[c(which,i),]  # exchange rows
>        X[i,] <- X[i,]/pivot            # pivot
>        row <- X[i,]                    
>        X <- X - outer(X[,i], row)      # sweep
>        X[i,] <- row                    # restore current row
>        }
>    n <- nrow(X)
>    for (i in 1:n) if (max(abs(X[i,])) <= tol) X[c(i,n),] <- X[c(n,i),]   #
>0 rows to bottom
>    X
>    }
>        
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
>>Sent: Wednesday, March 03, 2004 2:42 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] (no subject)
>>
>>how to produce a  Row Reduced Echelon Form for a matrix in R?
>>Aimin Yan
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at medanalytics.com  Thu Mar  4 04:46:27 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 03 Mar 2004 21:46:27 -0600
Subject: [R] A file manipulation question
In-Reply-To: <011d01c40197$70a02750$1c361d41@glblpyirxqz5lp>
References: <011d01c40197$70a02750$1c361d41@glblpyirxqz5lp>
Message-ID: <1078371987.6995.113.camel@localhost.localdomain>

On Wed, 2004-03-03 at 21:19, Greg Blevins wrote:
> Hello R experts,
> 
> The following problem outstrips my current programming knowledge. 
> 
> I have a dataframe with two fields that looks like the following:
> 
> ID     Contract
> 01     1
> 01     1
> 02     2
> 02     3
> 02     1
> 03     2
> 03     2
> 03     2
> 03     1
> 03     1
> 03     1
> etc...
> 
> I would like to end up with a dataframe with one row per ID where the
> value in the contract field would be the highest value recorded for a
> single ID. As you can see above, the number of IDs varies irregularly.
> Given the above, the new file would look like the following:
> 
> ID     Contract
> 01     1
> 02     3
> 03     2
> 
> Thanks in advance for your suggestions.

# Create the data frame
df <- data.frame(ID = I(c(rep("01", 2), rep("02", 3), rep("03", 6))),
                 Contract = c(1, 1, 2, 3, 1, 2, 2, 2, 1, 1, 1, ))

> df
   ID Contract
1  01        1
2  01        1
3  02        2
4  02        3
5  02        1
6  03        2
7  03        2
8  03        2
9  03        1
10 03        1
11 03        1

# Now use aggregate() to condense df by ID, using the max
# value of Contract
> aggregate(df$Contract, list(ID = df$ID), max)
  ID x
1 01 1
2 02 3
3 03 2


See ?aggregate for more information.  By default, aggregate() names the
function derived column as 'x'. You can of course rename it as you need.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Thu Mar  4 04:46:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 3 Mar 2004 22:46:32 -0500
Subject: [R] A file manipulation question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7907@usrymx25.merck.com>

Say your data frame is `dat'.  Try:

    tapply(dat$Contract, dat$ID, max)

(The output is not a data frame, but that shouldn't be a problem...)

HTH,
Andy


> From: Greg Blevins
> 
> Hello R experts,
> 
> The following problem outstrips my current programming knowledge. 
> 
> I have a dataframe with two fields that looks like the following:
> 
> ID     Contract
> 
> 01     1
> 
> 01     1
> 
> 02     2
> 
> 02     3
> 
> 02     1
> 
> 03     2
> 
> 03     2
> 
> 03     2
> 
> 03     1
> 
> 03     1
> 
> 03     1
> 
> etc...
> 
> I would like to end up with a dataframe with one row per ID 
> where the value in the contract field would be the highest 
> value recorded for a single ID. As you can see above, the 
> number of IDs varies irregularly.  Given the above, the new 
> file would look like the following:
> 
> ID     Contract
> 
> 01     1
> 
> 02     3
> 
> 03     2
> 
> Thanks in advance for your suggestions.
> 
> Gregory L. Blevins The Market Solustions Group, Partner
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From NAVMSE-CORPEXCH at cpgcorp.com.sg  Thu Mar  4 05:14:06 2004
From: NAVMSE-CORPEXCH at cpgcorp.com.sg (NAVMSE-CORPEXCH@cpgcorp.com.sg)
Date: Thu, 4 Mar 2004 12:14:06 +0800
Subject: [R] Symantec AVF detected that you sent a message with a prohibited
	attachment name
Message-ID: <f2ea01c4019f$22998210$db59060a@ISNET.CORP.ORG>

Subject of the message: Re: Your music
Recipient of the message: Jenny Ong Pei Pei
Prohibited attachment: mp3music.pif




---------------------------------------------------------------- 
Privileged/Confidential Information may be contained in this message.

If you are not the addressee indicated in this message (or responsible for delivery of the message to such person), 
you may not copy or deliver this message to anyone.

In such case, you should destroy this message and kindly notify the sender by reply e-mail.

Opinions, conclusions and other information in this message that do not relate to the official business of my firm shall be understood as neither given nor endorsed by it.



From yantian2001 at yahoo.com  Thu Mar  4 05:10:51 2004
From: yantian2001 at yahoo.com (Yan Li)
Date: Wed, 3 Mar 2004 20:10:51 -0800 (PST)
Subject: [R] scaling data for microarray
Message-ID: <20040304041051.39761.qmail@web12009.mail.yahoo.com>

Hello All,

I am new to R and want to use R to scale my data
before analysis.  I need to do two fold scaling: a).
scale on a per object (chip) basis, scale to a mean of
0 and stddev to 1; b). scale on a per feature basis:
scale data linearly to the interval [0,1].

Could somebody help me with this?
Thanks a lot,

Jas

 

__________________________________




From andrewr at uidaho.edu  Thu Mar  4 04:53:08 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 3 Mar 2004 19:53:08 -0800
Subject: [R] A file manipulation question
In-Reply-To: <011d01c40197$70a02750$1c361d41@glblpyirxqz5lp>
References: <011d01c40197$70a02750$1c361d41@glblpyirxqz5lp>
Message-ID: <200403031953.08740.andrewr@uidaho.edu>

How about something like ... (if your data frame is called the.data)



summarized <- as.data.frame(levels(the.data$ID))
names(summarized) <- "ID"
summarized$Contract <- as.numeric(tapply(the.data$Contract, the.data$ID, max))




Andrew

On Wednesday 03 March 2004 19:19, Greg Blevins wrote:
> Hello R experts,
>
> The following problem outstrips my current programming knowledge.
>
> I have a dataframe with two fields that looks like the following:
>
> ID     Contract
>
> 01     1
>
> 01     1
>
> 02     2
>
> 02     3
>
> 02     1
>
> 03     2
>
> 03     2
>
> 03     2
>
> 03     1
>
> 03     1
>
> 03     1
>
> etc...
>
> I would like to end up with a dataframe with one row per ID where the value
> in the contract field would be the highest value recorded for a single ID.
> As you can see above, the number of IDs varies irregularly.  Given the
> above, the new file would look like the following:
>
> ID     Contract
>
> 01     1
>
> 02     3
>
> 03     2
>
> Thanks in advance for your suggestions.
>
> Gregory L. Blevins The Market Solustions Group, Partner
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From ajayshah at mayin.org  Wed Mar  3 19:33:59 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 4 Mar 2004 00:03:59 +0530
Subject: [R] Find out the day of week for a chron object?
In-Reply-To: <20040303115802.EB07A39A4@mprdmxin.myway.com>
References: <20040303115802.EB07A39A4@mprdmxin.myway.com>
Message-ID: <20040303183359.GM689@igidr.ac.in>

On Wed, Mar 03, 2004 at 06:58:02AM -0500, Gabor Grothendieck wrote:
> 
> Here are three different ways (using x as defined in your 
> post):
> 
>  with( month.day.year(x), day.of.week(month,day,year) )
> 
>  do.call( "day.of.week", month.day.year(x) )
> 
>  as.numeric(x-3)%%7   # uses fact that chron(3) is Sunday

Thanks! Using this, I wrote --

  library(chron);
  prevFriday <- function(x) {
    repeat {
      x <- x - 1;
      if (5 == with(month.day.year(x), day.of.week(month,day,year))) break;
    }
    return(x);
  }
  x = dates("12-02-04", format="d-m-y")
  print(prevFriday(x))

and it works. :-)

Could someone give me a glimmer into HOW and WHY that expression 
with(month.day.year(x), day.of.week(month,day,year))
works? :-)

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Thu Mar  4 06:08:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  4 Mar 2004 00:08:08 -0500 (EST)
Subject: [R] Find out the day of week for a chron object?
Message-ID: <20040304050808.1AFEC39B5@mprdmxin.myway.com>


month.day.year takes a chron object and produces a list
with three components named month, day and year.  with
executes its second argument in an environment where the
components of the list of the first argument are variables
so arg 2 can refer to the names month, day and year.


---
Date:   Thu, 4 Mar 2004 00:03:59 +0530 
From:   Ajay Shah <ajayshah at mayin.org>
To:   Gabor Grothendieck <ggrothendieck at myway.com> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Find out the day of week for a chron object? 

[...]
Could someone give me a glimmer into HOW and WHY that expression 
with(month.day.year(x), day.of.week(month,day,year))
works? :-)

-- 
Ajay Shah Consultant
ajayshah at mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Thu Mar  4 06:12:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  4 Mar 2004 00:12:09 -0500 (EST)
Subject: [R] A file manipulation question
Message-ID: <20040304051209.08D9E3A14@mprdmxin.myway.com>


You can ensure the name gets set appropriately like this:

> aggregate(list(Contract=df$Contract), list(ID=df$ID), max)
  ID Contract
1 01        1
2 02        3
3 03        2


---
Date:   Wed, 03 Mar 2004 21:46:27 -0600 
From:   Marc Schwartz <MSchwartz at medanalytics.com>
To:   Greg Blevins <gblevins at mn.rr.com> 
Cc:   R-Help <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] A file manipulation question 

[...]

> aggregate(df$Contract, list(ID = df$ID), max)
ID x
1 01 1
2 02 3
3 03 2


See ?aggregate for more information. By default, aggregate() names the
function derived column as 'x'. You can of course rename it as you need.

HTH,

Marc Schwartz



From mvdv at spamcop.net  Thu Mar  4 07:00:28 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 4 Mar 2004 17:00:28 +1100
Subject: [R] Alternative mail archives?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF78F7@usrymx25.merck.com>
Message-ID: <000101c401ad$ffb70150$344610ac@FEB0480>

Hi, 
The searchable mail archive at http://maths.newcastle.edu.au/~rking/R/ is
very useful.  Unfortunately it seems that many of the emails are not
available, many show up in the search results but then return a "file not
found" when following the link.
Is anyone else experiencing this, and is there an alternative mail archive
with a search facility?
TIA
Mark



From ggrothendieck at myway.com  Thu Mar  4 07:09:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  4 Mar 2004 01:09:08 -0500 (EST)
Subject: [R] Find out the day of week for a chron object?
Message-ID: <20040304060908.548F639B2@mprdmxin.myway.com>



You can avoid the loop in your calculation.  Since Friday is
day of the week 5, if the day of the week d of chron date x
is greater than 5 then Friday was d-5 days ago; otherwise, if d-5 is 
zero or negative then we have to add 7 to it to ensure that its 
in the past.  Thus:

prevFriday <- function(x) {
	d <- with(month.day.year(x), day.of.week(month,day,year)) 
	x - ifelse( d-5>0, d-5, d-5+7 )
}

Date:   Thu, 4 Mar 2004 00:03:59 +0530 
From:   Ajay Shah <ajayshah at mayin.org>
To:   Gabor Grothendieck <ggrothendieck at myway.com> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Find out the day of week for a chron object? 

 
On Wed, Mar 03, 2004 at 06:58:02AM -0500, Gabor Grothendieck wrote:
> 
> Here are three different ways (using x as defined in your 
> post):
> 
> with( month.day.year(x), day.of.week(month,day,year) )
> 
> do.call( "day.of.week", month.day.year(x) )
> 
> as.numeric(x-3)%%7 # uses fact that chron(3) is Sunday

Thanks! Using this, I wrote --

library(chron);
prevFriday <- function(x) {
repeat {
x <- x - 1;
if (5 == with(month.day.year(x), day.of.week(month,day,year))) break;
}
return(x);
}
x = dates("12-02-04", format="d-m-y")
print(prevFriday(x))

and it works. :-)

Could someone give me a glimmer into HOW and WHY that expression 
with(month.day.year(x), day.of.week(month,day,year))
works? :-)

-- 
Ajay Shah Consultant
ajayshah at mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi



From WeiQiang.Li at seagate.com  Thu Mar  4 07:26:39 2004
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Thu, 4 Mar 2004 14:26:39 +0800
Subject: [R] How to plot Histogram with frequence overlaid by
	distribution	curve
In-Reply-To: <200403011343.i21Dh938017030@erdos.math.unb.ca>
Message-ID: <OFB6448F89.C2B0457E-ON48256E4D.0022E18C-48256E4D.00238954@notes.seagate.com>


Hi,

      I am having a new problem that how I can set the probability scale
from 0 to 1 when I using on probability scale. Sometimes I get the maximum
probabilty more than 100%.

      Thanks!

Best Regards,
WeiQiang

In response to a posting from WeiQiang Li:

> Hi,
> I am facing the problem that I want to plot a histogram chart set
> freq to true and overlay with normal or weibull or exponential
distribution
> curve.
>
> The sample code is shown as below:
> >samp<-c(-8.2262,-8.2262,-8.2262,-8.20209,-8.09294,-8.07321,-8.07321,
> -8.07321,-8.07175,-8.04948,-8.04948,-8.04948,-8.03848,-8.03848,
> -8.026,-7.92517,-7.92517,-7.77218,-7.62414,-7.62414,-7.62414,
> -7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.59027,-7.28924,
> -7.28924,-6.78729,-6.25307)
>
> >hist(samp,freq=TRUE,br=20)
> >curve(dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)
>
> In the chart created based on above command, curve scale is too small
> compared to the freqeunce. My question here is how to adjust the scale of
> distribution curve. Thanks !
>

Gabor Grothendieck wrote:

> histdata <- hist(samp,freq=TRUE,br=20)
> curve(max(histdata$count)*dnorm(x,mean=mean(samp),sd=sd(samp)),add=TRUE)

This is a perfectly correct answer to a question that should not have
been asked in the first place.  What WeiQiang Li proposes to do makes
no sense at all, and will simply confuse and mislead the viewer/reader.
If a histogram is to be overlaid with a density curve that histogram
should represent a density and hence should be plotted on the density
scale --- NOT on the frequency scale.

                                                 cheers,

                                                             Rolf Turner

rolf at math.unb.ca



From itayf at fhcrc.org  Thu Mar  4 07:26:56 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Wed, 3 Mar 2004 22:26:56 -0800 (PST)
Subject: [R] Storing interpolation functions in R object
Message-ID: <Pine.LNX.4.44.0403032057080.16985-100000@cezanne.fhcrc.org>


Dear all,

I want to derive from a data set that I have a set of 9 
interpolation functions using approxfun() and store
them in an R object. The data has some structure that I would 
like to reflect in the storage, so ideally I would store them in 
a data.frame. So far I failed.

Here is what I tried:

# My actual data has similar structure:
> x <- 1:9
> y <- matrix(c(x*2, x*3, x*4), nr=3, nc=9)
> f.df <- list()
> cases <- c("case0", "case1", "case2")
> for (i in 1:3) {f.df[[cases[i]]] <- y*i}

# Prepare storage place
> funcs <- data.frame(NA, NA, NA)
> names(funcs) <- cases

# Try to store interpolation functions
> for (c in cases) {
+         for (i in 1:3) {
+           funcs[i,c] <- approxfun(x, f.df[[c]][i,])
+         }
+       }
Error in "[<-"(`*tmp*`, iseq, value = vjj) :
        incompatible types

# Failed to change the mode of a column:
> mode(f.df[["case0"]]) <- "function"
Error in as.function.default(x, envir) : list argument expected


My attempts to initialize a data.frame into "function" mode 
using, as.function(), led to more failures.

Is it possible to do?
and how?

Thank you for any suggestions or comments.
	Itay

--------------------------------------------------------------
itayf at fhcrc.org		Fred Hutchinson Cancer Research Center



From mvdv at spamcop.net  Thu Mar  4 07:26:47 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 4 Mar 2004 17:26:47 +1100
Subject: [R] Map projections
Message-ID: <000201c401b1$ac1960c0$344610ac@FEB0480>

Hi, 
This may already be in the R-help email archive but I can't seem to view
msgs older than 2002/2001....
In trying to work out which map projection I wanted to use I wrote a loop to
print them out, along with the info that is in the documentation of
'mapproj'.  If there is interest I'd like to contribute it to...? I think it
is too big to be an example (121 lines)
Even if there is no sensible place for it to go, I'd like post it and
possibly get some help in ironing out a few things.
Some room for improvement is:
-setting 'good' (i.e. pretty output) parameters for each of the projections,
this will also give a feel for the effect of the projection parameters.
-formatting the text strings so that they fit the plot/map width.
-formatting the text strings so that they scale (pointsize) when making a
multiple plot display.
-improving the appearance when fill is used; some projections look quite
bad, but this may be very tough.  Or maybe I've just got the wong parameter
values.

If it is appropriate to post the R file to the mail list, let me know and
I'll do so.  Alt. I can post the file contents in a message (121 long
lines).  Which is the correct protocol?
TIA
Mark



From mvdv at spamcop.net  Thu Mar  4 07:38:37 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 4 Mar 2004 17:38:37 +1100
Subject: [R] Accurate area map projections
Message-ID: <000301c401b3$533ee0e0$344610ac@FEB0480>

Hi,
Could any one point me to the projection, and parameters if necessary, that
would show each country/continent with it's area accurately refelcted on the
plot? E.g. aitoff vs. albers vs. bonne vs. cylequalearea vs. guyou - they
don't all look the same to mee but some of the documentations suggests they
are equal area?  Of course this isn't my field, so I am largely guessing and
am prorbably making some naive assumptions :)
I'm pretty sure this is on the R-help archive but I don't seem to be able to
access most of the posts that the search returns - I keep getting a 'file
not found' page...
TIA
Mark



From ligges at statistik.uni-dortmund.de  Thu Mar  4 08:29:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Mar 2004 08:29:27 +0100
Subject: [R] sketchin a line
In-Reply-To: <Pine.GSO.4.58.0403031228500.12424@cens1>
References: <Pine.GSO.4.58.0403031228500.12424@cens1>
Message-ID: <4046DAD7.5040207@statistik.uni-dortmund.de>

Mohammad Rahimi wrote:

> 
> Hi folks,
> 
> I guess it is a very premitive question.
> 
> If i have already created an image.
> 
> image(input)
> 
> and i want to add a line from (x0,y0) to (x1,y1) on that
> image. how i can sketch a line superimposed on that.


See ?lines

Uwe Ligges



> Best regards
> -m
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Mar  4 08:30:58 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Mar 2004 08:30:58 +0100
Subject: [R] can RAqua open an SPSS file
In-Reply-To: <D9E545AF-6D85-11D8-9D96-000A95CD6FE0@sbcglobal.net>
References: <D9E545AF-6D85-11D8-9D96-000A95CD6FE0@sbcglobal.net>
Message-ID: <4046DB32.4050207@statistik.uni-dortmund.de>

Cline Julia wrote:

> I have an SPSS data file that I would like to analyze using RAqua.  Is 
> there any way for me to open the file using RAqua.  I can probably get a 
> hard copy of the file contents from my collaborator, but I'd rather not 
> have to deal with all that input.
> 
> Julie Cline
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

See function read.spss() in package "foreign".

Uwe Ligges



From ripley at stats.ox.ac.uk  Thu Mar  4 08:33:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 07:33:08 +0000 (GMT)
Subject: [R] read.spss and time/date information
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179DEE3@exdkba022.novo.dk>
Message-ID: <Pine.LNX.4.44.0403040725200.5660-100000@gannet.stats>

On Thu, 4 Mar 2004, BXC (Bendix Carstensen) wrote:

> > On Wed, 3 Mar 2004, Peter Dalgaard wrote:
> > 
> > > Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de> writes:
> > >
> > > > Could anyone give me a a hint how I can convert 13264300800 to 
> > > > 2003/02/11 again, please?
> > >
> > > > ISOdate(1582,10,14)  + 13264300800
> > > [1] "2003-02-11 13:00:00 CET"
> > > > ISOdate(1582,10,14)  +  13142476800
> > > [1] "1999-04-03 14:00:00 CEST"
> > >
> > > [October 14, 1582 is Day 1 of the Gregorian calendar.]
> > >
> > 
> > I tried January 1th 1970 as "baseline" but I never would have 
> > dreamed of October 14, 1582.
> 
> This is the day the present Gregorian calendar was instituted, replacing
> the
> old Julian that did not have leap years. So it is actually quite
> sensible
> as it is the earliest possible date that is in unbroken sequence to
> present
> times. 
> 
> Actually I am amazed that this has not been chosen as the natural origin
> for
> any of the R-pakages...

Note the numbers are large, and you would have to ensure that you have at
least 34 bits of accuracy.  You won't have that for integer storage, and
you cannot actually guarantee you have that for doubles (although we do
come pretty close to assuming IEC60559 aka IEEE754 arithmetic).  Even if 
you do have such doubles, you need 8 bytes where other representations 
work happily with 4.  (This would be less cogent if people only wanted to 
represent dates and used days and not seconds.  But then why would users 
actually care about the internal representation?)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Thu Mar  4 08:35:38 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 04 Mar 2004 08:35:38 +0100
Subject: [R] How to read Excel file and access the data item?
In-Reply-To: <20040303192524.23303.qmail@web21405.mail.yahoo.com>
Message-ID: <4046EA5A.23848.83C035@localhost>

Hi

On 3 Mar 2004 at 11:25, Grace Conlon wrote:

> In R, How to read Excel file and access the data item?
> Thank you.

Quite strightforward way is to copy your data to notepad, save to 
a txt file and read this txt file to r by appropriate read.table() 
command.

Or you can use coppying through clipboard (with some 
restrictions) by this function.

readClip<- function(n=3, header=T)
{

vstup<-readClipboard()


if (header) {
jmena<-vstup[1]
vstup<-vstup[-1]
jmena<-as.character(unlist(strsplit(jmena,split="[[:space:]]")))

}

vystup<-
data.frame(matrix(as.numeric(unlist(strsplit(vstup,split="[[:space:]
]"))),length(vstup),n,byrow=T))

if (header) names(vystup)<-jmena

vystup

}

where n is number of columns you want to copy.






> 
> 
> ---------------------------------
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Thu Mar  4 08:41:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Mar 2004 08:41:20 +0100
Subject: [R] Storing interpolation functions in R object
In-Reply-To: <Pine.LNX.4.44.0403032057080.16985-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0403032057080.16985-100000@cezanne.fhcrc.org>
Message-ID: <4046DDA0.80909@statistik.uni-dortmund.de>

Itay Furman wrote:

> Dear all,
> 
> I want to derive from a data set that I have a set of 9 
> interpolation functions using approxfun() and store
> them in an R object. The data has some structure that I would 
> like to reflect in the storage, so ideally I would store them in 
> a data.frame. So far I failed.
> 
> Here is what I tried:
> 
> # My actual data has similar structure:
> 
>>x <- 1:9
>>y <- matrix(c(x*2, x*3, x*4), nr=3, nc=9)
>>f.df <- list()
>>cases <- c("case0", "case1", "case2")
>>for (i in 1:3) {f.df[[cases[i]]] <- y*i}
> 
> 
> # Prepare storage place
> 
>>funcs <- data.frame(NA, NA, NA)
>>names(funcs) <- cases
> 
> 
> # Try to store interpolation functions
> 
>>for (c in cases) {
> 
> +         for (i in 1:3) {
> +           funcs[i,c] <- approxfun(x, f.df[[c]][i,])
> +         }
> +       }
> Error in "[<-"(`*tmp*`, iseq, value = vjj) :
>         incompatible types
> 
> # Failed to change the mode of a column:
> 
>>mode(f.df[["case0"]]) <- "function"
> 
> Error in as.function.default(x, envir) : list argument expected
> 
> 
> My attempts to initialize a data.frame into "function" mode 
> using, as.function(), led to more failures.
> 
> Is it possible to do?
> and how?
> 
> Thank you for any suggestions or comments.
> 	Itay
> 
> --------------------------------------------------------------
> itayf at fhcrc.org		Fred Hutchinson Cancer Research Center
> 



You cannot store a function that way. You might want to make "func" a 
list of lists as in:


# Prepare storage place

  funcs <- vector(mode = "list", length = 3)
  names(funcs) <- cases


# Try to store interpolation functions

  for (c in cases) {
      funcs[[c]] <- vector(mode = "list", length = 3)
      for (i in 1:3) {
          funcs[[c]][[i]] <- approxfun(x, f.df[[c]][i,])
      }
  }



Uwe Ligges



From petr.pikal at precheza.cz  Thu Mar  4 08:53:52 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 04 Mar 2004 08:53:52 +0100
Subject: [R] sketchin a line
In-Reply-To: <4046DAD7.5040207@statistik.uni-dortmund.de>
References: <Pine.GSO.4.58.0403031228500.12424@cens1>
Message-ID: <4046EEA0.2903.947138@localhost>

Hi

On 4 Mar 2004 at 8:29, Uwe Ligges wrote:

> Mohammad Rahimi wrote:
> 
> > 
> > Hi folks,
> > 
> > I guess it is a very premitive question.
> > 
> > If i have already created an image.
> > 
> > image(input)
> > 
> > and i want to add a line from (x0,y0) to (x1,y1) on that
> > image. how i can sketch a line superimposed on that.
> 
> 
> See ?lines

or maybe ?segments
Petr

> 
> Uwe Ligges
> 
> 
> 
> > Best regards
> > -m
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Thu Mar  4 08:54:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 07:54:41 +0000 (GMT)
Subject: [R] Storing interpolation functions in R object
In-Reply-To: <Pine.LNX.4.44.0403032057080.16985-100000@cezanne.fhcrc.org>
Message-ID: <Pine.LNX.4.44.0403040742260.5660-100000@gannet.stats>

A column of a data frame is a vector, and all columns should have the same 
length.  You cannot meet those basic requirements if you make a column 
class to be "function", but you can have a list of functions.

However, in your case you seem to want a 3x3 array of functions, so the 
natural structure would be a matrix and not a data frame. As in

funcs <- matrix(vector("list", 9), 3, 3)
colnames(funcs) <- cases
for (c in cases)
    for (i in 1:3)
        funcs[[i,c]] <- approxfun(x, f.df[[c]][i,])

Since this is a list, you access elements as funcs[[i,j]].


On Wed, 3 Mar 2004, Itay Furman wrote:

> 
> Dear all,
> 
> I want to derive from a data set that I have a set of 9 
> interpolation functions using approxfun() and store
> them in an R object. The data has some structure that I would 
> like to reflect in the storage, so ideally I would store them in 
> a data.frame. So far I failed.
> 
> Here is what I tried:
> 
> # My actual data has similar structure:
> > x <- 1:9
> > y <- matrix(c(x*2, x*3, x*4), nr=3, nc=9)
> > f.df <- list()
> > cases <- c("case0", "case1", "case2")
> > for (i in 1:3) {f.df[[cases[i]]] <- y*i}
> 
> # Prepare storage place
> > funcs <- data.frame(NA, NA, NA)
> > names(funcs) <- cases
> 
> # Try to store interpolation functions
> > for (c in cases) {
> +         for (i in 1:3) {
> +           funcs[i,c] <- approxfun(x, f.df[[c]][i,])
> +         }
> +       }
> Error in "[<-"(`*tmp*`, iseq, value = vjj) :
>         incompatible types
> 
> # Failed to change the mode of a column:
> > mode(f.df[["case0"]]) <- "function"
> Error in as.function.default(x, envir) : list argument expected
> 
> 
> My attempts to initialize a data.frame into "function" mode 
> using, as.function(), led to more failures.
> 
> Is it possible to do?
> and how?
> 
> Thank you for any suggestions or comments.
> 	Itay
> 
> --------------------------------------------------------------
> itayf at fhcrc.org		Fred Hutchinson Cancer Research Center
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Lennart.Borgman at astrazeneca.com  Thu Mar  4 09:38:35 2004
From: Lennart.Borgman at astrazeneca.com (Lennart.Borgman@astrazeneca.com)
Date: Thu, 4 Mar 2004 09:38:35 +0100 
Subject: [R] Alternative mail archives?
Message-ID: <26D5AB9F6512D611A8610001FA7E136F0327844F@se-drc-mail4.selu.astrazeneca.net>

I have no solution. However I wonder why the mail archives at
https://www.stat.math.ethz.ch/pipermail/r-help/ are not searchable by
Google. That would perhaps be a solution otherwise, since you can restrict
Google to search within just one site. (This applies to some other search
engines too.) 

Since I am also interested in the possibility to search the archive I have
just sent a question to Google to see if they have a solution. I have also
sent that mail to this mailing list. (I hope that does not disturb.)

- Lennart


-----Original Message-----
From: Mark Van De Vyver [mailto:mvdv at spamcop.net]
Sent: 4 mars 2004 07:10
To: r-help at stat.math.ethz.ch
Subject: [R] Alternative mail archives?


Hi, 
The searchable mail archive at http://maths.newcastle.edu.au/~rking/R/ is
very useful.  Unfortunately it seems that many of the emails are not
available, many show up in the search results but then return a "file not
found" when following the link.
Is anyone else experiencing this, and is there an alternative mail archive
with a search facility?
TIA
Mark

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Lennart.Borgman at astrazeneca.com  Thu Mar  4 09:38:32 2004
From: Lennart.Borgman at astrazeneca.com (Lennart.Borgman@astrazeneca.com)
Date: Thu, 4 Mar 2004 09:38:32 +0100 
Subject: [R] Searching a mail archive
Message-ID: <26D5AB9F6512D611A8610001FA7E136F0327844E@se-drc-mail4.selu.astrazeneca.net>

Hi,

I am trying to search the mail archives for the R statistical software.
Unfortunately I can not use the site: - search in the Google Bar, since the
site search is grayed out for the mail archive site (starting from for
example
https://www.stat.math.ethz.ch/pipermail/r-help/2004-March/045312.html).

I have tried to get around this by using a phrase search like this:

+"Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]" +"More
information about the R-help mailing list"

This does not work either. I get a lot of hits that does not include the
second phrase (which seems to be a possible error in Google to me - or I am
just misunderstanding something).


I have two questions:

1) Why is site search grayed out on the site above?

2) Can I make the phrase search work so that I only get hits in the mail
archive?


Kind Regards,
Lennart



From Roger.Bivand at nhh.no  Thu Mar  4 10:18:14 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 4 Mar 2004 10:18:14 +0100 (CET)
Subject: [R] Accurate area map projections
In-Reply-To: <000301c401b3$533ee0e0$344610ac@FEB0480>
Message-ID: <Pine.LNX.4.44.0403041009140.29175-100000@reclus.nhh.no>

On Thu, 4 Mar 2004, Mark Van De Vyver wrote:

> Hi,
> Could any one point me to the projection, and parameters if necessary, that
> would show each country/continent with it's area accurately refelcted on the
> plot? E.g. aitoff vs. albers vs. bonne vs. cylequalearea vs. guyou - they
> don't all look the same to mee but some of the documentations suggests they
> are equal area?  Of course this isn't my field, so I am largely guessing and
> am prorbably making some naive assumptions :)
> I'm pretty sure this is on the R-help archive but I don't seem to be able to
> access most of the posts that the search returns - I keep getting a 'file
> not found' page...

One of the best online resources is the collection of documentation links 
at:

http://www.remotesensing.org/proj

The manual has figures with examples of projections.

GMT also has projection documentation:

http://gmt.soest.hawaii.edu/

The printed resources, which are well worth consulting, are: John P. 
Snyder (1987) Map projections - a working manual, USGS professional paper 
1395; Lev M. Bugayevskiy and John P. Snyder (1995) Map projections - a 
reference manual (London, Taylor & Francis). The field is quite extensive, 
and most often a choice that is "right" for one use will not be for 
another. Even measuring area accurately in the field is bad enough, but 
representing it on a flat 2D surface has challenged people for a long time 
and will certainly continue to do so (cartography is an interesting part 
of graphic visualisation!).

> TIA
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From M.VanDeVyver at econ.usyd.edu.au  Thu Mar  4 10:46:25 2004
From: M.VanDeVyver at econ.usyd.edu.au (Mark Van De Vyver)
Date: Thu, 4 Mar 2004 20:46:25 +1100
Subject: [R] Accurate area map projections
Message-ID: <D5E776D7B9EAA14AA68A5E004C7A9B698531D1@evs1.econ.usyd.edu.au>

Hi Roger, thanks very much for your help.  I'll follow those references up.
Regards
Mark

-----Original Message-----
From:	Roger Bivand [mailto:Roger.Bivand at nhh.no]
Sent:	Thu 04-Mar-04 8:18 PM
To:	Mark Van De Vyver
Cc:	r-help at stat.math.ethz.ch
Subject:	Re: [R] Accurate area map projections
On Thu, 4 Mar 2004, Mark Van De Vyver wrote:

> Hi,
> Could any one point me to the projection, and parameters if necessary, that
> would show each country/continent with it's area accurately refelcted on the
> plot? E.g. aitoff vs. albers vs. bonne vs. cylequalearea vs. guyou - they
> don't all look the same to mee but some of the documentations suggests they
> are equal area?  Of course this isn't my field, so I am largely guessing and
> am prorbably making some naive assumptions :)
> I'm pretty sure this is on the R-help archive but I don't seem to be able to
> access most of the posts that the search returns - I keep getting a 'file
> not found' page...

One of the best online resources is the collection of documentation links 
at:

http://www.remotesensing.org/proj

The manual has figures with examples of projections.

GMT also has projection documentation:

http://gmt.soest.hawaii.edu/

The printed resources, which are well worth consulting, are: John P. 
Snyder (1987) Map projections - a working manual, USGS professional paper 
1395; Lev M. Bugayevskiy and John P. Snyder (1995) Map projections - a 
reference manual (London, Taylor & Francis). The field is quite extensive, 
and most often a choice that is "right" for one use will not be for 
another. Even measuring area accurately in the field is bad enough, but 
representing it on a flat 2D surface has challenged people for a long time 
and will certainly continue to do so (cartography is an interesting part 
of graphic visualisation!).

> TIA
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From baron at psych.upenn.edu  Thu Mar  4 11:12:45 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 4 Mar 2004 05:12:45 -0500
Subject: [R] Alternative mail archives?
In-Reply-To: <26D5AB9F6512D611A8610001FA7E136F0327844F@se-drc-mail4.selu.astrazeneca.net>
References: <26D5AB9F6512D611A8610001FA7E136F0327844F@se-drc-mail4.selu.astrazeneca.net>
Message-ID: <20040304101245.GA16148@psych>

http://finzi.psych.upenn.edu/

This is listed under Search on the main R page.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From Jan.Verbesselt at agr.kuleuven.ac.be  Thu Mar  4 11:25:07 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Thu,  4 Mar 2004 11:25:07 +0100
Subject: [R] Lineair regression modelling between time series //correlation
	analysis
Message-ID: <1078395907.4047040350b39@webmail1.kuleuven.be>

Dear R specialists,

I'm working with time series and want to investigate the relationship
between two time series by correlation analysis or by fitting a gen.
lineair model to the plot of x(timeserie1) and y(timeserie2).

Lin1 <- data.frame(
        Nr = c(1:lengte),
        NDII = window(ts.mNDII,c(1998,10),c(2003,11)),
        InvERC = window(Inv.ERC,c(1998,10),c(2003,11))
        )

summary(glm(NDII ~ InvERC, data=Lin1, family=gaussian(link ="identity")))

Error in "storage.mode<-"(`*tmp*`, value = "double") : 
        invalid time series parameters specified

How can I solve this error?

Are there specific functions which I can use to investigate the
relationship between two time series? (object ts is ok) ....Does somebody
has some examples of how to solve this statistical problem? ARIMA, AR,
ACF of the package "ts" for time series analysis

Thanks a lot,
Best regards,
Jan Verbesselt



From casty at giub.unibe.ch  Thu Mar  4 11:48:18 2004
From: casty at giub.unibe.ch (Carlo Casty)
Date: Thu, 4 Mar 2004 11:48:18 +0100 (MET)
Subject: [R] prcomp: error code 1 from Lapack routine dgesdd
Message-ID: <Pine.SOL.3.95.1040304112535.18354A-100000@fog>

Dear all

I have a big matrix of standardized values (dimensions 285x5829) and R
fails to calculate 
the principal components using prcomp() with the following error message:

pc <- prcomp(my.matrix)
Error in La.svd(x, nu, nv, method) : error code 1 from Lapack routine
dgesdd

Is the matrix too big? I'm using R-1.8.1 under Unix (Solaris8) and
Linux(Suse 8.2). I tried to
perform a principal component analysis using a huge matrix with the
dimensions 1000x10000 and it worked without an error message.

test <- matrix(rnorm(10000000), ncol=1000, nrow=10000)
pc <- prcomp(test)


Any hints are welcome.
Many thanks in advance and best regards,
Carlo

******************************************************
Carlo Casty
Climatology and Meteorology Phone:  +41 (0)31 6318545
Institute of Geography      Fax:    +41 (0)31 6318511
University of Bern          e-mail:casty at giub.unibe.ch
Hallerstr. 12               www.giub.unibe.ch/~casty
3012 Bern/SWITZERLAND



From Lennart.Borgman at astrazeneca.com  Thu Mar  4 11:50:54 2004
From: Lennart.Borgman at astrazeneca.com (Lennart.Borgman@astrazeneca.com)
Date: Thu, 4 Mar 2004 11:50:54 +0100 
Subject: [R] Alternative mail archives?
Message-ID: <26D5AB9F6512D611A8610001FA7E136F03278454@se-drc-mail4.selu.astrazeneca.net>

Excuse me for my previous answers, they were not very well thought out. I
hope this one is better. Thanks to those that made me realize this!

Mark, did you try to look at the "cached" link in the search results? The
search is carried out by Google and the "cached" link is often a very useful
feature. The "cached" link goes to Google's stored copy of the page.

- Lennart


-----Original Message-----
From: Mark Van De Vyver [mailto:mvdv at spamcop.net]
Sent: 4 mars 2004 07:10
To: r-help at stat.math.ethz.ch
Subject: [R] Alternative mail archives?


Hi, 
The searchable mail archive at http://maths.newcastle.edu.au/~rking/R/ is
very useful.  Unfortunately it seems that many of the emails are not
available, many show up in the search results but then return a "file not
found" when following the link.
Is anyone else experiencing this, and is there an alternative mail archive
with a search facility?
TIA
Mark

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Mar  4 12:03:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 11:03:34 +0000 (GMT)
Subject: [R] prcomp: error code 1 from Lapack routine dgesdd
In-Reply-To: <Pine.SOL.3.95.1040304112535.18354A-100000@fog>
Message-ID: <Pine.LNX.4.44.0403041100410.6265-100000@gannet.stats>

On Thu, 4 Mar 2004, Carlo Casty wrote:

> Dear all
> 
> I have a big matrix of standardized values (dimensions 285x5829) and R
> fails to calculate 
> the principal components using prcomp() with the following error message:
> 
> pc <- prcomp(my.matrix)
> Error in La.svd(x, nu, nv, method) : error code 1 from Lapack routine
> dgesdd
> 
> Is the matrix too big? I'm using R-1.8.1 under Unix (Solaris8) and

No.  It indicates a failure of the algorithm used, hence something about 
the matrix in your example.

> Linux(Suse 8.2). I tried to
> perform a principal component analysis using a huge matrix with the
> dimensions 1000x10000 and it worked without an error message.
> 
> test <- matrix(rnorm(10000000), ncol=1000, nrow=10000)
> pc <- prcomp(test)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Thu Mar  4 12:09:22 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 04 Mar 2004 06:09:22 -0500
Subject: [R] scaling data for microarray
In-Reply-To: <20040304041051.39761.qmail@web12009.mail.yahoo.com>
Message-ID: <BC6C7892.553D%sdavis2@mail.nih.gov>


Jas,

You should look into using bioconductor (http://www.bioconductor.org), a
group of over 65 R packages for dealing with all aspects of microarray
analysis.  There are numerous functions in those packages dealing in
multiple ways with your problem.

Sean

On 3/3/04 11:10 PM, "Yan Li" <yantian2001 at yahoo.com> wrote:

> Hello All,
> 
> I am new to R and want to use R to scale my data
> before analysis.  I need to do two fold scaling: a).
> scale on a per object (chip) basis, scale to a mean of
> 0 and stddev to 1; b). scale on a per feature basis:
> scale data linearly to the interval [0,1].
> 
> Could somebody help me with this?
> Thanks a lot,
> 
> Jas
> 
> 
> 
> __________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Mar  4 12:21:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 11:21:08 +0000 (GMT)
Subject: [R] Lineair regression modelling between time series //correlation
	analysis
In-Reply-To: <1078395907.4047040350b39@webmail1.kuleuven.be>
Message-ID: <Pine.LNX.4.44.0403041111470.6287-100000@gannet.stats>

On Thu, 4 Mar 2004, Jan Verbesselt wrote:

> Dear R specialists,
> 
> I'm working with time series and want to investigate the relationship
> between two time series by correlation analysis or by fitting a gen.
> lineair model to the plot of x(timeserie1) and y(timeserie2).
> 
> Lin1 <- data.frame(
>         Nr = c(1:lengte),
>         NDII = window(ts.mNDII,c(1998,10),c(2003,11)),
>         InvERC = window(Inv.ERC,c(1998,10),c(2003,11))
>         )
> 
> summary(glm(NDII ~ InvERC, data=Lin1, family=gaussian(link ="identity")))

Why are you using glm to do linear regression?  lm() is the preferred 
function.

> Error in "storage.mode<-"(`*tmp*`, value = "double") : 
>         invalid time series parameters specified
> 
> How can I solve this error?

We would need to be able to reproduce it to be able to help you.  A quick 
attempt

library(MASS)
Lin1 <- data.frame(male=mdeaths, female=fdeaths)
summary(glm(female ~ male, data=Lin1, family=gaussian(link ="identity")))

works, as there is no technical problem with having time series in such a
fit (although the result may be statistically invalid).

> Are there specific functions which I can use to investigate the
> relationship between two time series? (object ts is ok) 

Many, including lm().

>....Does somebody
> has some examples of how to solve this statistical problem? ARIMA, AR,
> ACF of the package "ts" for time series analysis

What is your statistical problem, precisely?  Using the regression 
capabilities on arima() or function gls() in package nlme spring to mind, 
and ccf() and spectrum() can display cross-correlations and ar() can fit a 
joint AR model.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From boudennj at ensisun.imag.fr  Thu Mar  4 12:35:15 2004
From: boudennj at ensisun.imag.fr (boudennj@ensisun.imag.fr)
Date: Thu,  4 Mar 2004 12:35:15 +0100
Subject: [R] (no subject)
Message-ID: <1078400115.40471473655b1@webmail.imag.fr>


Hello,

     I went to interface R and C++ And I want to include R objects in C++. I 
have a probleme because Rf_allocVector and others function are not in the R 
library of mine version.
If you have a idear that may be very helpful.

Thinks.

-------------------------------------------------
envoy? via Webmail/IMAG !



From mase at is.titech.ac.jp  Thu Mar  4 12:36:22 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Thu, 04 Mar 2004 20:36:22 +0900
Subject: [R] "Statistiques avec R"
Message-ID: <404714B6.8040909@is.titech.ac.jp>

Dear R users,

I want to share my joy with you. Please see the following
excellent introduction to R "Statistiques avec R " by
Vincent Zoonekynd

http://zoonek2.free.fr/UNIX/48_R/all.html

In paticular, you can see a lot of fascinating graphics
examples of R from which you can get many hints.

Soryy if this is already well-known, but the CRAN search
did not show nothing with the keyword "Zoonekynd".

Cheers,

Shigeru Mase,
Dept. Comp. and Math. Sciences
Tokyo Institute of Technology,
Meguro, Tokyo, Japan 152-8550



From rrsilva at ib.usp.br  Thu Mar  4 09:57:03 2004
From: rrsilva at ib.usp.br (=?iso-8859-1?q?Rog=E9rio=20Rosa=20da=20Silva?=)
Date: Thu, 4 Mar 2004 08:57:03 +0000
Subject: [R] boot package 
Message-ID: <200403040857.03454.rrsilva@ib.usp.br>


Dear all 

As part of an ongoing study on the ecomorphology of ant communities, I have 
obtained a matrix with 156 row (species) and 20 columns (several measurements 
of body shape) for 4 localities.

For each community, I calculated a matrix of Euclidean distances between all
pairs of species. From this matrix, I extracted two measures of community
structure: i) I identified the distance from a individual to its nearest
neighbor (NND) in the morphological space and then calculated the averages of 
the NND (MNND); ii) the mean of the Euclidean distances (MED). NND and MED 
are of practical use in describing spatial relations between species. The 
results of Euclidean distance studies in each of the four localities were 
compared with each other for evidence repeating patterns.

To determine wheter ou not the morphological arrangement of species in 
communities reflected internal organization, MNND and MED should be compared 
with randomly generated communities.

It is possible to write a function in th boot package to evaluate the values 
of average nearest-neighbor distance and of the mean of Euclidean distances 
in randomly generated communities ? One set of 1000 random communities each 
with 78, 90, 100 and 102 species must be generaded from the pool of species 
(156 species). The only restrictions on species composition  is that no 
species could be placed in a community more than once, and only rows values 
can be permuted.

Sorry, I don't know how to define the statistic term in the boot() function 
from the boot library in R. I have studied the boot package, argument list of 
functions and the functions definition. Can someone point me the correct 
syntax?

I would very much appreciate any help

Best regards

Rog?rio



From rolf at math.unb.ca  Thu Mar  4 12:55:22 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 4 Mar 2004 07:55:22 -0400 (AST)
Subject: [R] How to plot Histogram with frequence overlaid by
	distribution	curve
Message-ID: <200403041155.i24BtMVZ008985@erdos.math.unb.ca>


You wrote:

> I am having a new problem that how I can set the probability scale
> from 0 to 1 when I using on probability scale. Sometimes I get the
> maximum probabilty more than 100%.

When you set prob=TRUE you get a ***density*** scale.  The resulting
histogram represents a probability density function (directly comparable
with the normal density function you sought to superimpose) which
***integrates*** to 1.  The values of a density funcion need not
be less than 1; these values are not probabilities.  They are probability
densities.  Roughly if f(x) is the p.d.f. for X then

		f(x) dx approx. = P(x <= X <= x + dx)

I.e. f(x) dx will be less than 1, but f(x) on its own need not be.

O.K.?

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From arv at ono.com  Thu Mar  4 13:19:42 2004
From: arv at ono.com (antonio rodriguez)
Date: Thu, 4 Mar 2004 13:19:42 +0100
Subject: [R] "Statistiques avec R"
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <IPEFKICOHOECENGJBAGLOECFDAAA.arv@ono.com>

Tr?s bien!!

Antonio

> -----Mensaje original-----
> De: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Shigeru Mase
> Enviado el: jueves, 04 de marzo de 2004 12:36
> Para: r-help
> Asunto: [R] "Statistiques avec R"
>
>
> Dear R users,
>
> I want to share my joy with you. Please see the following
> excellent introduction to R "Statistiques avec R " by
> Vincent Zoonekynd
>
> http://zoonek2.free.fr/UNIX/48_R/all.html
>
> In paticular, you can see a lot of fascinating graphics
> examples of R from which you can get many hints.
>
> Soryy if this is already well-known, but the CRAN search
> did not show nothing with the keyword "Zoonekynd".
>
> Cheers,
>
> Shigeru Mase,
> Dept. Comp. and Math. Sciences
> Tokyo Institute of Technology,
> Meguro, Tokyo, Japan 152-8550
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> ---
> Incoming mail is certified Virus Free.
> Checked by AVG anti-virus system (http://www.grisoft.com).
> Version: 6.0.596 / Virus Database: 379 - Release Date: 26/02/2004
>
---



From Christoph.Scherber at uni-jena.de  Thu Mar  4 13:53:32 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 04 Mar 2004 13:53:32 +0100
Subject: [R] binomial errors in split-plot design
Message-ID: <404726CC.8010102@uni-jena.de>

Dear all,

I have proportion data with binomial errors. The problem is that the 
whole experiment was laid out as a split-plot design.

Ideally, what I would like is having a glm with an Error term such as 
glm(y~x+Error(A/B)) but I fear this is not possible. Would using lme be 
an alternative? How could I state the variance structure, then?

I would very much appreciate any suggestions!
Best regards
Christoph



From monica.palaseanu-lovejoy at stud.man.ac.uk  Thu Mar  4 14:02:35 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Thu, 4 Mar 2004 13:02:35 -0000
Subject: [R] shared library load - failure
In-Reply-To: <200403041116.i24B8bhI024821@hypatia.math.ethz.ch>
Message-ID: <E1AysV2-000M2f-IO@probity.mcc.ac.uk>

Hi,

Somebody was kind enough to help me with R and have sent a 
shared library. When i try to load it with dyn.load i get the following 
error: LoadLibrary failure:  %1 is not a valid Win32 application.

Now the library was done on a Linux machine and i am working on 
a Windows machine. How can i translate the library from Linux to 
Windows?

Thank you for your help,

Monica



From christian.hoffmann at wsl.ch  Thu Mar  4 14:08:06 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Thu, 04 Mar 2004 14:08:06 +0100
Subject: [R] Wait for the user to type <Return>, depending on argument.
In-Reply-To: <1078391690.4046f38ad47cd@webmail.wsl.ch>
References: <4046EE82.7060706@wsl.ch> <1078391690.4046f38ad47cd@webmail.wsl.ch>
Message-ID: <40472A36.5070501@wsl.ch>

Hi,

I hope I will not bother you.

This is a useful tool in programming pauses:

function(ask) {
   if (ask & interactive() & sink.number()==0) readline("\nType 
<Return>\t to continue : ")
}

It guards against batch use and sink-ing:

for (ii in 1:5) {
     cat(ii,"\n")
     waitReturn(ii %% 2 == 1)
   }

sink("blafile")
for (ii in 1:5) {
     cat(ii,"\n")
     waitReturn(ii %% 2 == 1)
   }
sink()
#look at file "blafile"

Documentation: (R style)
\name{waitReturn}
\alias{waitReturn}
\title{Wait for <Return>}
\description{
   Wait for the user to type <Return>, depending on argument.
}
\usage{
   waitReturn(ask=TRUE)
}
\arguments{
   \item{ask}{ \code{TRUE} will generate the interruption, \code{FALSE} 
will not.}
}
\details{
   The interruption will only generated for the interactive use of R and
   if the call is not \code{sink}ed (where it would hang the process).
}
\value{
   None.
}

\examples{
   for (ii in 1:5) {
     cat(ii,"\n")
     waitReturn(ii %% 2 == 1)
   }
}
\author{Christian W. Hoffmann, \email{christian.hoffmann at wsl.ch},
	\url{http://www.wsl.ch/staff/christian.hoffmann}
}
\keyword{programming}


Christian


-- 
--  --> New telephone number, old one will work until 2006
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: hoffmacw at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922..  ..77  (self)
CH-8903 Birmensdorf, Switzerland            ..11(exchange), ..15  (Fax)



From ligges at statistik.uni-dortmund.de  Thu Mar  4 14:14:52 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Mar 2004 14:14:52 +0100
Subject: [R] shared library load - failure
In-Reply-To: <E1AysV2-000M2f-IO@probity.mcc.ac.uk>
References: <E1AysV2-000M2f-IO@probity.mcc.ac.uk>
Message-ID: <40472BCC.7080002@statistik.uni-dortmund.de>

Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> Somebody was kind enough to help me with R and have sent a 
> shared library. When i try to load it with dyn.load i get the following 
> error: LoadLibrary failure:  %1 is not a valid Win32 application.
> 
> Now the library was done on a Linux machine and i am working on 
> a Windows machine. How can i translate the library from Linux to 
> Windows?
> 
> Thank you for your help,
> 
> Monica


You cannot. You have to recompile from sources.

Uwe Ligges



From sam.kemp2 at ntlworld.com  Thu Mar  4 14:17:46 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Thu, 04 Mar 2004 13:17:46 +0000
Subject: [R] adding trend to an arima model
Message-ID: <40472C7A.2080208@ntlworld.com>

Hi,

Does anyone know a method for adding a linear/polynominal trend to a 
simulated arima model using the arima.sim function?

Any help will be greatly appreciated.

Cheers,

Sam.



From amurta at ipimar.pt  Thu Mar  4 14:21:46 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Thu, 4 Mar 2004 13:21:46 +0000
Subject: [R] "Statistiques avec R"
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
References: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <200403041321.46839.amurta@ipimar.pt>

Thank you for your suggestion. It is really a very good (and pretty) 
introduction to R.
Cheers

Alberto

On Thursday 04 March 2004 11:36, Shigeru Mase wrote:
> Dear R users,
>
> I want to share my joy with you. Please see the following
> excellent introduction to R "Statistiques avec R " by
> Vincent Zoonekynd
>
> http://zoonek2.free.fr/UNIX/48_R/all.html
>
> In paticular, you can see a lot of fascinating graphics
> examples of R from which you can get many hints.
>
> Soryy if this is already well-known, but the CRAN search
> did not show nothing with the keyword "Zoonekynd".
>
> Cheers,
>
> Shigeru Mase,
> Dept. Comp. and Math. Sciences
> Tokyo Institute of Technology,
> Meguro, Tokyo, Japan 152-8550
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From flom at ndri.org  Thu Mar  4 14:24:26 2004
From: flom at ndri.org (Peter Flom)
Date: Thu, 04 Mar 2004 08:24:26 -0500
Subject: [R] Resolution of problem re location polr and of MASS
Message-ID: <s046e7c8.051@MAIL.NDRI.ORG>

First, my apologies for not giving all the details in my first post.  

Second, the problem was somewhat more complicated than simply using
help.search('polr') or library(MASS).  When I tried these, I got
notices that polr could not be found anywhere, and that no package
called MASS existed.  When I went to the  CRAN site, there was no
package called MASS.  I am not sure how I installed R 1.8.1 on a Windows
machine without installing MASS and without getting any errors, but I
did.

Third, thanks to all who replied, and especially to Andy Jaworski who
showed me how to solve the problem, namely to get the package VR from
CRAN and then type library(MASS).  

Thanks again, 

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From MSchwartz at medanalytics.com  Thu Mar  4 14:42:54 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 04 Mar 2004 07:42:54 -0600
Subject: [R] A file manipulation question
In-Reply-To: <20040304051209.08D9E3A14@mprdmxin.myway.com>
References: <20040304051209.08D9E3A14@mprdmxin.myway.com>
Message-ID: <1078407774.6995.132.camel@localhost.localdomain>

<Banging head on desk>

Yep. Knew that. There was even an exchange on r-help last fall on this
and I was one of the participants, though with a slightly different
solution using as.data.frame, which gets us to the same end result:

> with(df, aggregate(as.data.frame(Contract), list(ID = ID), max))
  ID Contract
1 01        1
2 02        3
3 03        2

Substituting list() for as.data.frame() works of course in a similar
fashion, which Simon Fear pointed out in that same thread:

> with(df, aggregate(list(Contract = Contract), list(ID = ID), max))
  ID Contract
1 01        1
2 02        3
3 03        2

and of course you can rename the Contract column directly if you wish to
give the name further meaning:

> with(df, aggregate(list(Contract.Max = Contract), list(ID = ID), max))
  ID Contract.Max
1 01            1
2 02            3
3 03            2

The key is to not have the 'x' argument ('Contract' in this case) passed
as a vector.

Thanks Gabor!


On Wed, 2004-03-03 at 23:12, Gabor Grothendieck wrote:
> You can ensure the name gets set appropriately like this:
> 
> > aggregate(list(Contract=df$Contract), list(ID=df$ID), max)
>   ID Contract
> 1 01        1
> 2 02        3
> 3 03        2
> 
> 
> ---
> Date:   Wed, 03 Mar 2004 21:46:27 -0600 
> From:   Marc Schwartz <MSchwartz at medanalytics.com>
> To:   Greg Blevins <gblevins at mn.rr.com> 
> Cc:   R-Help <r-help at stat.math.ethz.ch> 
> Subject:   Re: [R] A file manipulation question 
> 
> [...]
> 
> > aggregate(df$Contract, list(ID = df$ID), max)
> ID x
> 1 01 1
> 2 02 3
> 3 03 2
> 
> 
> See ?aggregate for more information. By default, aggregate() names the
> function derived column as 'x'. You can of course rename it as you need.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com



From Carlisle.Thacker at noaa.gov  Thu Mar  4 14:47:47 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Thu, 04 Mar 2004 08:47:47 -0500
Subject: [R] need help with smooth.spline
References: <3A822319EB35174CA3714066D590DCD504AF7905@usrymx25.merck.com>
Message-ID: <40473383.47DF2F8B@noaa.gov>

Andy,

As the data are often noisy, smoothing splines should be appropriate.

The first example profile shows an isothermal (constant temperature)
layer in the upper ocean followed by a sharp thermocline (large
temperature gradient), but there are relatively few observations
defining this sharp transition.  In this case simple linear
interpolation works fine, but smooth.spline() with all defaults gives
an absolutely absurd value in the isothermal layer.  With all.knots =
TRUE, the values in the isothermal layer are much better but still
peculiar.

Given the sampling and the data, is it possible to get smooth.spline()
do better?  If so, would that adversely impact its performance for
other cases?  (There are thousands of profiles.)  If not, is there a
simp[le way to select cases that smooth.spline() should not be
expected to handle, so they can be treated separately?

Thanks,

Carlisle

"Liaw, Andy" wrote:
> 
> If you really want interpolation, should you be using spline() rather than
> smooth.spline()?  The later is for smoothing data observed with noise, not
> for interpolation.
> 
> Andy
> 
> > From: W. C. Thacker
> >
> > Dear R listers,
> >
> > When using smooth.spline to interpolate data, results are generally
> > good.  However, some cases produce totally unreasonable results.
> >
> > The data are values of pressure, temperature, and salinity from a
> > probe that is lowered into the ocean, and the objective is to
> > interpolate temperature and salinity to specified pressures.  While
> > smooth.spline provides excellent values at the observed pressures,
> > there are cases when the values at the desired pressures are
> > unusable.  A dataframe with four such profiles, indicated by values of
> > id, is attached.  My target values for pressure are seq(25,1600,25),
> > but 1:500 is also interesting.
> >
> > Setting all.knots = TRUE helps, but it would be nice to be able to do
> > better.
> >
> > Any suggestions?
> >
> > Thanks,
> >
> > Carlisle
> >
> > > version
> >          _
> > platform sparc-sun-solaris2.9
> > arch     sparc
> > os       solaris2.9
> > system   sparc, solaris2.9
> > status
> > major    1
> > minor    8.0
> > year     2003
> > month    10
> > day      08
> > language R
> >
> >
> > --
> >
> > William Carlisle Thacker
> >
> > Atlantic Oceanographic and Meteorological Laboratory
> > 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
> > Office: (305) 361-4323           Fax: (305) 361-4392
> >
> > "Too many have dispensed with generosity
> >      in order to practice charity."     Albert Camus
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From jfox at mcmaster.ca  Thu Mar  4 14:52:29 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 4 Mar 2004 08:52:29 -0500
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <4046A648.1010604@pdf.com>
Message-ID: <20040304135226.VSOB25409.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer,

I'd be surprised if the qr decomposition as computed in R weren't a lot more
stable numerically, but I'm no expert. As well, I don't know how to get the
row-echelon form from the qr decomposition -- though I suspect that you or
someone else on the list is about to enlighten me.

Regards,
 John 

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Wednesday, March 03, 2004 10:45 PM
> To: John Fox
> Cc: 'Aimin Yan'; r-help at stat.math.ethz.ch
> Subject: Re: [R] row-echelon form (was no subject)
> 
> How does this compare with R of the qr decomposition?  spencer graves
> 
> John Fox wrote:
> 
> >Dear Amin,
> >
> >I have a function (created just for demonstration, and reproduced 
> >below) for finding the row-echelon form of a matrix. I'm 
> sure that many 
> >list members could produce something that's better numerically, but 
> >this should be OK at least for toy problems.
> >
> >John
> >
> >--------- snip -------------
> >
> >rowEchelonForm <- function(X, tol=.Machine$double.eps){
> >    if ((!is.matrix(X)) || (!is.numeric(X))) stop("argument 
> must be a 
> >numeric matrix")
> >    Z <- X
> >    for (i in 1:min(dim(X))){
> >        if (i > 1) Z[i-1,] <- 0
> >        which <- which.max(abs(Z[,i]))  # find maximum pivot 
> in current 
> >column at or below current row
> >        pivot <- X[which, i]
> >        if (abs(pivot) <= tol) next     # check for 0 pivot
> >        if (which > i) X[c(i,which),] <- X[c(which,i),]  # 
> exchange rows
> >        X[i,] <- X[i,]/pivot            # pivot
> >        row <- X[i,]                    
> >        X <- X - outer(X[,i], row)      # sweep
> >        X[i,] <- row                    # restore current row
> >        }
> >    n <- nrow(X)
> >    for (i in 1:n) if (max(abs(X[i,])) <= tol) X[c(i,n),] <- 
> X[c(n,i),]   #
> >0 rows to bottom
> >    X
> >    }
> >        
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
> >>Sent: Wednesday, March 03, 2004 2:42 PM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] (no subject)
> >>
> >>how to produce a  Row Reduced Echelon Form for a matrix in R?
> >>Aimin Yan
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >  
> >
>



From ripley at stats.ox.ac.uk  Thu Mar  4 14:59:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 13:59:38 +0000 (GMT)
Subject: [R] binomial errors in split-plot design
In-Reply-To: <404726CC.8010102@uni-jena.de>
Message-ID: <Pine.LNX.4.44.0403041355560.6607-100000@gannet.stats>

There are several possibilities, including glmmPQL (MASS) and GLMM (lme4).
Be careful with the interpretation, as you don't have the advantages of 
balance that the classical AoV gives you.

On Thu, 4 Mar 2004, Christoph Scherber wrote:

> I have proportion data with binomial errors. The problem is that the 
> whole experiment was laid out as a split-plot design.
> 
> Ideally, what I would like is having a glm with an Error term such as 
> glm(y~x+Error(A/B)) but I fear this is not possible. Would using lme be 
> an alternative? How could I state the variance structure, then?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Mar  4 15:10:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 14:10:34 +0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <1078400115.40471473655b1@webmail.imag.fr>
Message-ID: <Pine.LNX.4.44.0403041408010.6687-100000@gannet.stats>

On Thu, 4 Mar 2004 boudennj at ensisun.imag.fr wrote:

>      I went to interface R and C++ And I want to include R objects in C++. I 
> have a probleme because Rf_allocVector and others function are not in the R 
> library of mine version.
> If you have a idear that may be very helpful.

Perhaps you could start by telling us what your version is and how you 
came to this conclusion?  Functions like Rf_allocVector *are* in the R 
executable (Unix) or DLL (Windows), and will be in the libR.so (Unix) 
shared library if you configured R to make it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mwh_acs at hotmail.com  Thu Mar  4 15:13:34 2004
From: mwh_acs at hotmail.com (Michael)
Date: Thu, 04 Mar 2004 14:13:34 +0000
Subject: [R] R newbie question
Message-ID: <BC6CEA0E.1A903%mwh_acs@hotmail.com>



Hi all

I have just started using R (1.8.1) on Mac OS X & am very excited about the
potential of such a flexible system with such good graphics potential & so
many useful packages available. Congratulations to all the developers & I
hope to be contributing some packages myself in the near future, once I have
got my head around what is required.

It will take me some time to convert seamlessly from the syntax of STATA
which I have used for many years, but which continues to ignore the requests
of its' users for better graphical output (e.g. 3D contour & surface
plotting).  I am still working my way through the R documentation.

I can see that scatterplot matrices are available (via pairs etc.) as well
as 2D kernel density estimates & contour plots - though I am still getting
used to all the options that can be specified.  My question is: is it
possible to use some options to produce the scatterplot format but with 2D
KDE contour plots in each pane rather than the points themselves?

A secondary question is: how can I define a different 'colour table' for a
contour plot e.g. one based on 'heat' so that low values are black, up
through blue, green, red, orange, yellow & white.

Many thanks & please feel free to reply directly to my email
(mwh_acs at hotmail.com)

Michael Hopkins

 
_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/

      _/    _/   _/_/_/               Hopkins Research Ltd
     _/    _/   _/    _/
    _/_/_/_/   _/_/_/                Virtual R&D Solutions
   _/    _/   _/   _/
  _/    _/   _/     _/               Tel: 0781 3467 381 (UK)
                   
_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/_/

        `All models are wrong, but some are useful' - George Box



From mwgrant2001 at yahoo.com  Thu Mar  4 15:19:19 2004
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Thu, 4 Mar 2004 06:19:19 -0800 (PST)
Subject: [R] "Statistiques avec R" 
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <20040304141919.47009.qmail@web20811.mail.yahoo.com>

Indeed, the site is a treasure. Thank you very much
for pointing it out.

Arigato gozaimashita

Michael Grant

--- Shigeru Mase <mase at is.titech.ac.jp> wrote:
> Dear R users,
> 
> I want to share my joy with you. Please see the
> following
> excellent introduction to R "Statistiques avec R "
> by
> Vincent Zoonekynd
> 
> http://zoonek2.free.fr/UNIX/48_R/all.html
> 
> In paticular, you can see a lot of fascinating
> graphics
> examples of R from which you can get many hints.
> 
> Soryy if this is already well-known, but the CRAN
> search
> did not show nothing with the keyword "Zoonekynd".
> 
> Cheers,
> 
> Shigeru Mase,
> Dept. Comp. and Math. Sciences
> Tokyo Institute of Technology,
> Meguro, Tokyo, Japan 152-8550
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


__________________________________


From spencer.graves at pdf.com  Thu Mar  4 15:31:09 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 04 Mar 2004 06:31:09 -0800
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <20040304135226.VSOB25409.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <20040304135226.VSOB25409.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <40473DAD.90805@pdf.com>

      How about the following: 

 > A <- array(1:6, dim=c(3, 2))
 > A.qr <- qr(A)
 > qr.R(A.qr)
          [,1]      [,2]
[1,] -3.741657 -8.552360
[2,]  0.000000  1.963961
 >
    I'm no expert, either, and I don't have time now to research this 
further.  Perhaps someone else can further enlighten us both. 
spencer graves

John Fox wrote:

>Dear Spencer,
>
>I'd be surprised if the qr decomposition as computed in R weren't a lot more
>stable numerically, but I'm no expert. As well, I don't know how to get the
>row-echelon form from the qr decomposition -- though I suspect that you or
>someone else on the list is about to enlighten me.
>
>Regards,
> John 
>
>  
>
>>-----Original Message-----
>>From: Spencer Graves [mailto:spencer.graves at pdf.com] 
>>Sent: Wednesday, March 03, 2004 10:45 PM
>>To: John Fox
>>Cc: 'Aimin Yan'; r-help at stat.math.ethz.ch
>>Subject: Re: [R] row-echelon form (was no subject)
>>
>>How does this compare with R of the qr decomposition?  spencer graves
>>
>>John Fox wrote:
>>
>>    
>>
>>>Dear Amin,
>>>
>>>I have a function (created just for demonstration, and reproduced 
>>>below) for finding the row-echelon form of a matrix. I'm 
>>>      
>>>
>>sure that many 
>>    
>>
>>>list members could produce something that's better numerically, but 
>>>this should be OK at least for toy problems.
>>>
>>>John
>>>
>>>--------- snip -------------
>>>
>>>rowEchelonForm <- function(X, tol=.Machine$double.eps){
>>>   if ((!is.matrix(X)) || (!is.numeric(X))) stop("argument 
>>>      
>>>
>>must be a 
>>    
>>
>>>numeric matrix")
>>>   Z <- X
>>>   for (i in 1:min(dim(X))){
>>>       if (i > 1) Z[i-1,] <- 0
>>>       which <- which.max(abs(Z[,i]))  # find maximum pivot 
>>>      
>>>
>>in current 
>>    
>>
>>>column at or below current row
>>>       pivot <- X[which, i]
>>>       if (abs(pivot) <= tol) next     # check for 0 pivot
>>>       if (which > i) X[c(i,which),] <- X[c(which,i),]  # 
>>>      
>>>
>>exchange rows
>>    
>>
>>>       X[i,] <- X[i,]/pivot            # pivot
>>>       row <- X[i,]                    
>>>       X <- X - outer(X[,i], row)      # sweep
>>>       X[i,] <- row                    # restore current row
>>>       }
>>>   n <- nrow(X)
>>>   for (i in 1:n) if (max(abs(X[i,])) <= tol) X[c(i,n),] <- 
>>>      
>>>
>>X[c(n,i),]   #
>>    
>>
>>>0 rows to bottom
>>>   X
>>>   }
>>>       
>>>
>>> 
>>>
>>>      
>>>
>>>>-----Original Message-----
>>>>From: r-help-bounces at stat.math.ethz.ch 
>>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
>>>>Sent: Wednesday, March 03, 2004 2:42 PM
>>>>To: r-help at stat.math.ethz.ch
>>>>Subject: [R] (no subject)
>>>>
>>>>how to produce a  Row Reduced Echelon Form for a matrix in R?
>>>>Aimin Yan
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>>   
>>>>
>>>>        
>>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>> 
>>>
>>>      
>>>
>
>  
>



From andy_liaw at merck.com  Thu Mar  4 15:30:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 4 Mar 2004 09:30:05 -0500
Subject: [R] need help with smooth.spline
Message-ID: <3A822319EB35174CA3714066D590DCD504AF790B@usrymx25.merck.com>

Hi Carlisle,

If I understand you correctly, the problem is smooth.spline() not handling
sharp jump(s), right?  If so, it's probably easier to try something that can
handle such features.  Wavelet `denoising' (as opposed to `smoothing', and
available in the wavethresh package) is well known for being able to handle
abrupt changes (very `spatially adaptive').  Other things you might consider
are mars() in the `mda' package (which fits splines in an adaptive fashion)
and locfit() in the `locfit' package.  For locfit, you will want to specify
local smoothing parameter selection, via a call like

  locfit(..., alpha=c(0, 0, 2), acri="cp")

You might need to play with the `2' a bit to get the right amount of
smoothing.  The details are in Loader's book `Local regression and
Likelihood'.

HTH,
Andy

> From: W. C. Thacker
> 
> Andy,
> 
> As the data are often noisy, smoothing splines should be appropriate.
> 
> The first example profile shows an isothermal (constant temperature)
> layer in the upper ocean followed by a sharp thermocline (large
> temperature gradient), but there are relatively few observations
> defining this sharp transition.  In this case simple linear
> interpolation works fine, but smooth.spline() with all defaults gives
> an absolutely absurd value in the isothermal layer.  With all.knots =
> TRUE, the values in the isothermal layer are much better but still
> peculiar.
> 
> Given the sampling and the data, is it possible to get smooth.spline()
> do better?  If so, would that adversely impact its performance for
> other cases?  (There are thousands of profiles.)  If not, is there a
> simp[le way to select cases that smooth.spline() should not be
> expected to handle, so they can be treated separately?
> 
> Thanks,
> 
> Carlisle
> 
> "Liaw, Andy" wrote:
> > 
> > If you really want interpolation, should you be using 
> spline() rather than
> > smooth.spline()?  The later is for smoothing data observed 
> with noise, not
> > for interpolation.
> > 
> > Andy
> > 
> > > From: W. C. Thacker
> > >
> > > Dear R listers,
> > >
> > > When using smooth.spline to interpolate data, results are 
> generally
> > > good.  However, some cases produce totally unreasonable results.
> > >
> > > The data are values of pressure, temperature, and salinity from a
> > > probe that is lowered into the ocean, and the objective is to
> > > interpolate temperature and salinity to specified 
> pressures.  While
> > > smooth.spline provides excellent values at the observed pressures,
> > > there are cases when the values at the desired pressures are
> > > unusable.  A dataframe with four such profiles, indicated 
> by values of
> > > id, is attached.  My target values for pressure are 
> seq(25,1600,25),
> > > but 1:500 is also interesting.
> > >
> > > Setting all.knots = TRUE helps, but it would be nice to 
> be able to do
> > > better.
> > >
> > > Any suggestions?
> > >
> > > Thanks,
> > >
> > > Carlisle
> > >
> > > > version
> > >          _
> > > platform sparc-sun-solaris2.9
> > > arch     sparc
> > > os       solaris2.9
> > > system   sparc, solaris2.9
> > > status
> > > major    1
> > > minor    8.0
> > > year     2003
> > > month    10
> > > day      08
> > > language R
> > >
> > >
> > > --
> > >
> > > William Carlisle Thacker
> > >
> > > Atlantic Oceanographic and Meteorological Laboratory
> > > 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
> > > Office: (305) 361-4323           Fax: (305) 361-4392
> > >
> > > "Too many have dispensed with generosity
> > >      in order to practice charity."     Albert Camus
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> > 
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> -- 
> 
> William 
> Carlisle Thacker                            
>                                                     
> Atlantic Oceanographic and Meteorological Laboratory
> 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
> Office: (305) 361-4323           Fax: (305) 361-4392
> 
> "Too many have dispensed with generosity 
>      in order to practice charity."     Albert Camus
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Christoph.Scherber at uni-jena.de  Thu Mar  4 15:37:12 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Thu, 04 Mar 2004 15:37:12 +0100
Subject: [R] binomial errors in split-plot design
In-Reply-To: <Pine.LNX.4.44.0403041355560.6607-100000@gannet.stats>
References: <Pine.LNX.4.44.0403041355560.6607-100000@gannet.stats>
Message-ID: <40473F18.9090703@uni-jena.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040304/d043d83d/attachment.pl

From ripley at stats.ox.ac.uk  Thu Mar  4 15:45:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 14:45:23 +0000 (GMT)
Subject: [R] adding trend to an arima model
In-Reply-To: <40472C7A.2080208@ntlworld.com>
Message-ID: <Pine.LNX.4.44.0403041442100.6740-100000@gannet.stats>

What does "+" give you?  It works for me -- simulate the model, simulate 
the trend and add them.

x <- arima.sim(list(ar=0.3), 100) + 1:100

is an AR(1) with a linear trend.

> arima(x, c(1,0,0), xreg=1:100)

Call:
arima(x = x, order = c(1, 0, 0), xreg = 1:100)

Coefficients:
         ar1  intercept   1:100
      0.4510    -0.1704  1.0031
s.e.  0.0915     0.3528  0.0060

...

On Thu, 4 Mar 2004, Samuel Kemp wrote:

> Does anyone know a method for adding a linear/polynominal trend to a 
> simulated arima model using the arima.sim function?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Sinnwell.Jason at mayo.edu  Thu Mar  4 15:58:29 2004
From: Sinnwell.Jason at mayo.edu (Jason Sinnwell)
Date: Thu, 4 Mar 2004 08:58:29 -0600 (CST)
Subject: [R] load data for mypkg-Ex.R
Message-ID: <200403041458.i24EwTW22059@dastardly.mayo.edu>

Thank you for your suggestion.  I think I have a good solution to my issues.  
And for reference I thought I would post what worked for me.  

I had to make a function called setupData() that gets the same argument in Splus 
and R.  I had to modify your suggestion a little because the sgml and Rd files 
were expecting different parameters.  This function runs exactly as data() in R 
and does nothing in Splus. 

setupData <- function(...) {
    result <- if(exists("is.R") && is.function(is.R) && is.R()) {
               data
             } else {
               function(...) invisible(NULL)
             }
    eval(result(...))
}

Then in all the help files which use a data set I have:
> setupData(my.data)
As wished for, and expected.

If I had more data sets to manage (maybe someday), I would probably mimic the 
clever use of delay() in the MASS library.  

> Date: Fri, 27 Feb 2004 10:20:50 -0800 (PST)
> From: Thomas Lumley <tlumley at u.washington.edu>
> To: Jason Sinnwell <Sinnwell.Jason at mayo.edu>
> cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] load data for mypkg-Ex.R
> MIME-Version: 1.0
> 
> On Fri, 27 Feb 2004, Jason Sinnwell wrote:
> 
> > Using R 1.7.1 in Solaris
> >
> > I'm developing a package for both Splus and R, and I'm trying to use all the
> > same files for R and Splus, both function files and help files.  I have two
> > questions.
> >
> > 1) The file made by R CMD check to run .Rd-examples posts examples from 
files in
> > alphabetical order.  Is it okay/recommended/common-practice to set up all 
the
> > example data in the first two (alphabetically-sorted) examples and assume 
that
> > data exists for the rest of the examples?
> 
> No.  In fact, it is specifically disallowed.
> 
> > 2)  Since data() is not understood by Splus, I don't want to put a
> > 	     > data(example.data)
> > in the sgml file because then the Splus example would not run as data() 
doesn't
> > exist there.  Is there a spot I can make sure this data is loaded when 
running
> > the examples, but not to load the data every time you load the library, as 
it
> > would take up unnecessary space.  It is a ~~220 x 25 data.frame, is that 
enough
> > size to worry about this?
> 
> I wouldn't think it was big enough to worry seriously about
> 
> > I'm considering using the NAMESPACE or .First.lib() within zzz.R but that 
would
> > load the data every time the library is loaded.  Also considering something
> > like:
> > >if (<check for R using is.R()>)
> > >  data(example.data)
> > > <run example>
> >
> > In the example but that would create confusion for users.
> 
> You could define a function
> 
> if (is.R())
>   setupData<-data
> else
>   setupData<-function(...) invisible(NULL)
> 
> and then use setupData() instead of data(), or you could look at what MASS
> does using delay() to autoload data as needed.
> 
> 
> 	-thomas

+--------------------------+
|Jason P. Sinnwell, M.S.   |
|Mayo Clinic, Rochester    |
|Health Sciences Research  |
|Division of Biostatistics |
|507.284.3270              |
+--------------------------+



From vito.muggeo at giustizia.it  Thu Mar  4 16:14:54 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 4 Mar 2004 16:14:54 +0100
Subject: R: [R] adding trend to an arima model
References: <40472C7A.2080208@ntlworld.com>
Message-ID: <002f01c401fb$76131e40$5c13070a@PROCGEN>

As arima.sim() simulates stationary ARMA "errors" if your underlying model
is additive I think you can type, for instance, just:

x<-1:100 #time variable
mu<-10+.5*x #linear trend
y<-arima.sim(length(x), model=list(ar=.5, ma=-.3),sd=25)+mu
arima(y, order=c(1,0,1),include.mean=TRUE,xreg=x)


best,
vito

----- Original Message -----
From: Samuel Kemp <sam.kemp2 at ntlworld.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 04, 2004 2:17 PM
Subject: [R] adding trend to an arima model


> Hi,
>
> Does anyone know a method for adding a linear/polynominal trend to a
> simulated arima model using the arima.sim function?
>
> Any help will be greatly appreciated.
>
> Cheers,
>
> Sam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Carlisle.Thacker at noaa.gov  Thu Mar  4 16:23:30 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Thu, 04 Mar 2004 10:23:30 -0500
Subject: [R] need help with smooth.spline
References: <3A822319EB35174CA3714066D590DCD504AF790B@usrymx25.merck.com>
Message-ID: <404749F2.4333F6FF@noaa.gov>

"Liaw, Andy" wrote:
Andy,

Is it known that smooth.spline() has a problem handling sharp jumps? 
That is part of the question. It seems to work fine for some sharp
jumps, but I have not yet been able to determine for which cases it
should work well and for which it should fail. 

Maybe at least part of the problem has to do with end-point behavior. 
Or with sampling intervals.

The data are from an archive, so their quality and their sampling
characteristics are far from uniform.  Moreover, the true variability
is non-normal, so recognizing bad data is difficult.  As the objective
is to identify models for estimating salinity from temperature and
pressure, what is important is to avoid outliers, hoping that the
contamination from bad data with believable values is small.

I'll try to get the packages installed and take a look at the function
you mentioned.

In the meantime, do you know of some criteria for recognizing when
smooth.spline might fail?  It seems to work quite well for the bulk of
the data.

Thanks,

Carlisle



> Hi Carlisle,
> 
> If I understand you correctly, the problem is smooth.spline() not handling
> sharp jump(s), right?  If so, it's probably easier to try something that can
> handle such features.  Wavelet `denoising' (as opposed to `smoothing', and
> available in the wavethresh package) is well known for being able to handle
> abrupt changes (very `spatially adaptive').  Other things you might consider
> are mars() in the `mda' package (which fits splines in an adaptive fashion)
> and locfit() in the `locfit' package.  For locfit, you will want to specify
> local smoothing parameter selection, via a call like
> 
>   locfit(..., alpha=c(0, 0, 2), acri="cp")
> 
> You might need to play with the `2' a bit to get the right amount of
> smoothing.  The details are in Loader's book `Local regression and
> Likelihood'.
> 
> HTH,
> Andy
> 
> > From: W. C. Thacker
> >
> > Andy,
> >
> > As the data are often noisy, smoothing splines should be appropriate.
> >
> > The first example profile shows an isothermal (constant temperature)
> > layer in the upper ocean followed by a sharp thermocline (large
> > temperature gradient), but there are relatively few observations
> > defining this sharp transition.  In this case simple linear
> > interpolation works fine, but smooth.spline() with all defaults gives
> > an absolutely absurd value in the isothermal layer.  With all.knots =
> > TRUE, the values in the isothermal layer are much better but still
> > peculiar.
> >
> > Given the sampling and the data, is it possible to get smooth.spline()
> > do better?  If so, would that adversely impact its performance for
> > other cases?  (There are thousands of profiles.)  If not, is there a
> > simp[le way to select cases that smooth.spline() should not be
> > expected to handle, so they can be treated separately?
> >
> > Thanks,
> >
> > Carlisle
> >
> > "Liaw, Andy" wrote:
> > >
> > > If you really want interpolation, should you be using
> > spline() rather than
> > > smooth.spline()?  The later is for smoothing data observed
> > with noise, not
> > > for interpolation.
> > >
> > > Andy
> > >
> > > > From: W. C. Thacker
> > > >
> > > > Dear R listers,
> > > >
> > > > When using smooth.spline to interpolate data, results are
> > generally
> > > > good.  However, some cases produce totally unreasonable results.
> > > >
> > > > The data are values of pressure, temperature, and salinity from a
> > > > probe that is lowered into the ocean, and the objective is to
> > > > interpolate temperature and salinity to specified
> > pressures.  While
> > > > smooth.spline provides excellent values at the observed pressures,
> > > > there are cases when the values at the desired pressures are
> > > > unusable.  A dataframe with four such profiles, indicated
> > by values of
> > > > id, is attached.  My target values for pressure are
> > seq(25,1600,25),
> > > > but 1:500 is also interesting.
> > > >
> > > > Setting all.knots = TRUE helps, but it would be nice to
> > be able to do
> > > > better.
> > > >
> > > > Any suggestions?
> > > >
> > > > Thanks,
> > > >
> > > > Carlisle
> > > >
> > > > > version
> > > >          _
> > > > platform sparc-sun-solaris2.9
> > > > arch     sparc
> > > > os       solaris2.9
> > > > system   sparc, solaris2.9
> > > > status
> > > > major    1
> > > > minor    8.0
> > > > year     2003
> > > > month    10
> > > > day      08
> > > > language R



From jfox at mcmaster.ca  Thu Mar  4 16:37:32 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 4 Mar 2004 10:37:32 -0500
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <40473DAD.90805@pdf.com>
Message-ID: <20040304153731.XFFL25409.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer,

The R matrix from the qr decomposition isn't quite in row-echelon form,
because the leading entry in each row is not 1, and the other entries in a
column with a leading entry aren't all 0. Some more examples:

> A  # nonsingular
     [,1] [,2] [,3]
[1,]    2   -2    0
[2,]    1   -1    1
[3,]    4    4   -4
> rowEchelonForm(A)
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
> qr.R(qr(A))
          [,1]      [,2]       [,3]
[1,] -4.582576 -2.400397  3.2732684
[2,]  0.000000  3.903600 -2.3421602
[3,]  0.000000  0.000000  0.8944272


> B  # rank 2
     [,1] [,2] [,3] [,4]
[1,]   -2    0   -1    2
[2,]    4    0    1    0
[3,]    6    0    1    2
> rowEchelonForm(B)
     [,1] [,2] [,3] [,4]
[1,]    1    0    0    1
[2,]    0    0    1   -4
[3,]    0    0    0    0
> qr.R(qr(B))
         [,1]      [,2] [,3]          [,4]
[1,] 7.483315 1.6035675    0  1.069045e+00
[2,] 0.000000 0.6546537    0 -2.618615e+00
[3,] 0.000000 0.0000000    0  6.336077e-16 


Regards,
 John

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Thursday, March 04, 2004 9:31 AM
> To: John Fox
> Cc: 'Aimin Yan'; r-help at stat.math.ethz.ch
> Subject: Re: [R] row-echelon form (was no subject)
> 
>       How about the following: 
> 
>  > A <- array(1:6, dim=c(3, 2))
>  > A.qr <- qr(A)
>  > qr.R(A.qr)
>           [,1]      [,2]
> [1,] -3.741657 -8.552360
> [2,]  0.000000  1.963961
>  >
>     I'm no expert, either, and I don't have time now to 
> research this further.  Perhaps someone else can further 
> enlighten us both. 
> spencer graves
> 
> John Fox wrote:
> 
> >Dear Spencer,
> >
> >I'd be surprised if the qr decomposition as computed in R 
> weren't a lot 
> >more stable numerically, but I'm no expert. As well, I don't 
> know how 
> >to get the row-echelon form from the qr decomposition -- though I 
> >suspect that you or someone else on the list is about to 
> enlighten me.
> >
> >Regards,
> > John
> >
> >  
> >
> >>-----Original Message-----
> >>From: Spencer Graves [mailto:spencer.graves at pdf.com]
> >>Sent: Wednesday, March 03, 2004 10:45 PM
> >>To: John Fox
> >>Cc: 'Aimin Yan'; r-help at stat.math.ethz.ch
> >>Subject: Re: [R] row-echelon form (was no subject)
> >>
> >>How does this compare with R of the qr decomposition?  
> spencer graves
> >>
> >>John Fox wrote:
> >>
> >>    
> >>
> >>>Dear Amin,
> >>>
> >>>I have a function (created just for demonstration, and reproduced
> >>>below) for finding the row-echelon form of a matrix. I'm
> >>>      
> >>>
> >>sure that many
> >>    
> >>
> >>>list members could produce something that's better 
> numerically, but 
> >>>this should be OK at least for toy problems.
> >>>
> >>>John
> >>>
> >>>--------- snip -------------
> >>>
> >>>rowEchelonForm <- function(X, tol=.Machine$double.eps){
> >>>   if ((!is.matrix(X)) || (!is.numeric(X))) stop("argument
> >>>      
> >>>
> >>must be a
> >>    
> >>
> >>>numeric matrix")
> >>>   Z <- X
> >>>   for (i in 1:min(dim(X))){
> >>>       if (i > 1) Z[i-1,] <- 0
> >>>       which <- which.max(abs(Z[,i]))  # find maximum pivot
> >>>      
> >>>
> >>in current
> >>    
> >>
> >>>column at or below current row
> >>>       pivot <- X[which, i]
> >>>       if (abs(pivot) <= tol) next     # check for 0 pivot
> >>>       if (which > i) X[c(i,which),] <- X[c(which,i),]  #
> >>>      
> >>>
> >>exchange rows
> >>    
> >>
> >>>       X[i,] <- X[i,]/pivot            # pivot
> >>>       row <- X[i,]                    
> >>>       X <- X - outer(X[,i], row)      # sweep
> >>>       X[i,] <- row                    # restore current row
> >>>       }
> >>>   n <- nrow(X)
> >>>   for (i in 1:n) if (max(abs(X[i,])) <= tol) X[c(i,n),] <-
> >>>      
> >>>
> >>X[c(n,i),]   #
> >>    
> >>
> >>>0 rows to bottom
> >>>   X
> >>>   }
> >>>       
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>>>-----Original Message-----
> >>>>From: r-help-bounces at stat.math.ethz.ch 
> >>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
> >>>>Sent: Wednesday, March 03, 2004 2:42 PM
> >>>>To: r-help at stat.math.ethz.ch
> >>>>Subject: [R] (no subject)
> >>>>
> >>>>how to produce a  Row Reduced Echelon Form for a matrix in R?
> >>>>Aimin Yan
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list 
> >>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! 
> >>>>http://www.R-project.org/posting-guide.html
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list 
> >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html
> >>> 
> >>>
> >>>      
> >>>
> >
> >  
> >
>



From maechler at stat.math.ethz.ch  Thu Mar  4 16:44:16 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 4 Mar 2004 16:44:16 +0100
Subject: [R] No attachments for R-help ("need help with smooth.spline")
In-Reply-To: <40473383.47DF2F8B@noaa.gov>
References: <3A822319EB35174CA3714066D590DCD504AF7905@usrymx25.merck.com>
	<40473383.47DF2F8B@noaa.gov>
Message-ID: <16455.20176.152717.358324@gargle.gargle.HOWL>

I hope you see that almost all binary attachments are not allowed  (and
silently deleted) on R-help, as it clearly says in the posting
guide, or directly on http://www.r-project.org/mail.html

You should typically use  dump(<dataframe>)
and paste ( not attach !) the result of that to (the end of)
your message {or do something even more readable -- plain text
in any case!}

With regards from your list maintainer,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ripley at stats.ox.ac.uk  Thu Mar  4 16:44:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 15:44:43 +0000 (GMT)
Subject: [R] binomial errors in split-plot design
In-Reply-To: <40473F18.9090703@uni-jena.de>
Message-ID: <Pine.LNX.4.44.0403041542530.7003-100000@gannet.stats>

First define what you mean by residuals.
Then extract them.
Then use qqnorm and qqline as usual.

Note that the first step is not clearcut even for a binomial glm, nor for 
a Gaussian mixed-effects model.

On Thu, 4 Mar 2004, Christoph Scherber wrote:

> Thanks!
> 
> And how can I then plot  a Q-Q line for model checking? qqnorm works 
> fine, but I couldn?t find how to use qqline for mixed effects models of 
> this type
> 
> so far, I have tried (e.g.)
> 
> qqnorm(glm1,~resid(.)|TREATMENT)
> 
> but I don?t know how to specify qqline for this
> 
> the full model is 
> glm1_glmmPQL(cbindarea~BLOCK+targetweight+TREATMENT+SOWNDIV+GRASS+LEG+SHERB+THERB,random=~1|PLOTCODE/TREATMENT,family=binomial)
> 
> where categorical variables are in capital letters
> 
> Best regards,
> Chris.
> 
> 
> Prof Brian Ripley wrote:
> 
> >There are several possibilities, including glmmPQL (MASS) and GLMM (lme4).
> >Be careful with the interpretation, as you don't have the advantages of 
> >balance that the classical AoV gives you.
> >
> >On Thu, 4 Mar 2004, Christoph Scherber wrote:
> >
> >  
> >
> >>I have proportion data with binomial errors. The problem is that the 
> >>whole experiment was laid out as a split-plot design.
> >>
> >>Ideally, what I would like is having a glm with an Error term such as 
> >>glm(y~x+Error(A/B)) but I fear this is not possible. Would using lme be 
> >>an alternative? How could I state the variance structure, then?
> >>    
> >>
> >
> >  
> >
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mase at is.titech.ac.jp  Thu Mar  4 16:49:04 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Fri, 05 Mar 2004 00:49:04 +0900
Subject: [R] "Statistiques avec R"
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
References: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <40474FF0.7030103@is.titech.ac.jp>

Isn't it marvellous?

If you also want to have an overview of R graphics (and do not hesitate
to visit a site in Japanese), please try also

http://www.okada.jp.org/RWiki/?%A5%B0%A5%E9%A5%D5%A5%A3%A5%C3%A5%AF%A5%B9%BB%B2%B9%CD%BC%C2%CE%E3%BD%B8

Sorry, this too long URL is a characteristics of softwares called "Wiki".
This is the R graphics archive in "RjpWiki", the Japanese R user's
homeground. Most examples are mere outputs of example codes of R
graphics functions and you can reproduce them easily. But, I think,
it is very instructive to have such collections together somewhere for
novice users to show the capability of R graphics.  Some examples are
taken from postings to r-help ML to which we (Japanese R users) want
to  express our thanks.  Of course, more thanks to R core teams.

PS.  Sure your browsers cannot correctly show contents with Japanese
characters.  I hope at least R codes and graphics are correctly displayed.

Cheers

Shigeru Mase



From maechler at stat.math.ethz.ch  Thu Mar  4 16:50:09 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 4 Mar 2004 16:50:09 +0100
Subject: [R] Resolution of problem re location polr and of MASS
In-Reply-To: <s046e7c8.051@MAIL.NDRI.ORG>
References: <s046e7c8.051@MAIL.NDRI.ORG>
Message-ID: <16455.20529.846628.468560@gargle.gargle.HOWL>

>>>>> "Peter" == Peter Flom <flom at ndri.org>
>>>>>     on Thu, 04 Mar 2004 08:24:26 -0500 writes:

    Peter> First, my apologies for not giving all the details in my first post.  
    Peter> Second, the problem was somewhat more complicated than simply using
    Peter> help.search('polr') or library(MASS).  When I tried these, I got
    Peter> notices that polr could not be found anywhere, and that no package
    Peter> called MASS existed.  When I went to the  CRAN site, there was no
    Peter> package called MASS.  I am not sure how I installed R 1.8.1 on a Windows
    Peter> machine without installing MASS and without getting any errors, but I
    Peter> did.

In that case, I'd definitely recommend to wipe your R version
and reinstall R 1.8.1.   When MASS has been missing who knows
what else is wrong with your R installation....

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From stephen at inf.ed.ac.uk  Thu Mar  4 17:04:01 2004
From: stephen at inf.ed.ac.uk (Stephen Eglen)
Date: Thu, 4 Mar 2004 16:04:01 +0000
Subject: [R] New beta release (5.2.0beta4) of Emacs Speaks Statistics
Message-ID: <16455.21361.632346.772134@bushmills.inf.ed.ac.uk>

We are nearly ready to release Emacs Speaks Statistics (ESS) version
5.2.  The latest beta, 5.2.0beta4, is available at:

http://www.analytics.washington.edu/downloads/ess/ess-5.2.0beta4.tar.gz
or
http://www.analytics.washington.edu/downloads/ess/ess-5.2.0beta4.zip

If you know of any release-critical bugs, please report them with
Emacs using "M-x ess-submit-bug-report".  If no bugs are forthcoming,
we hope to release 5.2 in the middle of next week.

Changes since 5.1.24 are listed in the top-level README.

Thanks, 
The ESS Core Team.



From Carlisle.Thacker at noaa.gov  Thu Mar  4 17:30:52 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Thu, 04 Mar 2004 11:30:52 -0500
Subject: [R] Re: No attachments for R-help ("need help with smooth.spline")
References: <3A822319EB35174CA3714066D590DCD504AF7905@usrymx25.merck.com>
	<40473383.47DF2F8B@noaa.gov>
	<16455.20176.152717.358324@gargle.gargle.HOWL>
Message-ID: <404759BC.D53C118E@noaa.gov>

Sorry.  Didn't realize about attachments.

Here are the data for four profiles that had problems with
smooth.spline():

     id     p     t      s
3322643   0.0 23.26 35.490
3322643  42.0 23.26 35.500
3322643  44.0 23.19 35.550
3322643  47.0 22.12 35.600
3322643  49.0 21.34 35.639
3322643  53.0 17.34 35.610
3322643  55.0 16.23 35.580
3322643  58.0 15.51 35.520
3322643  59.0 15.39 35.460
3322643  63.0 14.82 35.450
3322643  67.0 14.67 35.470
3322643  68.0 14.66 35.480
3322643  72.0 14.37 35.450
3322643  76.0 14.26 35.420
3322643  82.0 13.92 35.380
3322643  86.0 13.83 35.370
3322643  94.0 13.49 35.350
3322643 104.0 13.23 35.340
3322643 112.0 13.09 35.310
3322643 127.0 12.76 35.270
3322643 134.0 12.71 35.270
3322643 138.0 12.63 35.270
3322643 142.0 12.60 35.260
3322643 146.0 12.47 35.240
3322643 155.0 12.39 35.240
3322643 164.0 12.34 35.230
3322643 170.0 12.18 35.210
3322643 172.0 12.16 35.210
3322643 175.0 12.09 35.210
3322643 201.0 11.74 35.160
3322643 206.0 11.67 35.150
3322643 218.0 11.57 35.130
3322643 223.0 11.47 35.130
3322643 232.0 11.38 35.120
3322643 243.0 11.29 35.110
3322643 253.0 11.16 35.100
3322643 265.0 11.10 35.100
3322643 277.0 11.10 35.110
3322643 285.0 10.98 35.100
3322643 295.0 10.87 35.090
3322643 305.0 10.78 35.090
3322643 320.0 10.68 35.090
3322643 330.0 10.58 35.080
3322643 340.0 10.62 35.090
3322643 355.0 10.53 35.090
3322643 375.0 10.55 35.150
3322643 395.0 10.46 35.140
3322643 405.0 10.36 35.140
3322643 425.0 10.25 35.150
3322643 470.0  9.80 35.120
3322643 480.0  9.78 35.120
3322643 515.0  9.39 35.090
3322643 540.0  8.95 35.060
3322643 545.0  8.94 35.050
3322643 565.0  8.66 35.030
3322643 570.0  8.64 35.020
3322643 610.0  8.13 34.970
3322643 630.0  7.99 34.950
3322643 660.0  7.70 34.930
3322643 695.0  7.20 34.900
3322643 745.0  6.62 34.840
3322643 780.0  6.46 34.820
3322643 795.0  6.29 34.820
3322643 810.0  6.26 34.820
3322643 860.0  5.91 34.800
3322643 905.0  5.72 34.810
3322643 920.0  5.67 34.820
3322643 940.0  5.64 34.830
3322643 975.0  5.57 34.840
3322643 985.0  5.57 34.850
3336561   0.0 24.48 35.330
3336561  37.0 24.46 35.340
3336561  39.0 24.43 35.340
3336561  40.0 24.40 35.400
3336561  43.0 23.81 35.500
3336561  47.0 20.90 35.749
3336561  49.0 17.27 35.829
3336561  50.0 16.60 35.799
3336561  52.0 16.18 35.709
3336561  53.0 16.11 35.699
3336561  56.0 15.83 35.590
3336561  60.0 15.14 35.550
3336561  70.0 14.76 35.530
3336561  71.0 14.73 35.520
3336561  73.0 14.62 35.510
3336561  75.0 14.52 35.500
3336561  85.0 14.38 35.480
3336561 106.0 14.29 35.460
3336561 109.0 14.19 35.450
3336561 112.0 14.09 35.420
3336561 113.0 14.09 35.430
3336561 118.0 13.83 35.400
3336561 127.0 13.75 35.380
3336561 137.0 13.58 35.370
3336561 148.0 13.49 35.360
3336561 166.0 13.43 35.350
3336561 179.0 13.38 35.340
3336561 192.0 13.25 35.300
3336561 197.0 13.08 35.300
3336561 214.0 12.87 35.270
3336561 223.0 12.77 35.250
3336561 229.0 12.44 35.210
3336561 235.0 12.02 35.140
3336561 237.0 11.79 35.130
3336561 239.0 11.53 35.100
3336561 242.0 11.34 35.100
3336561 247.0 11.21 35.090
3336561 248.0 11.19 35.040
3336561 252.0 10.87 35.060
3336561 256.0 10.71 34.980
3336561 258.0 10.63 35.040
3336561 269.0 10.51 34.990
3336561 276.0 10.32 34.980
3336561 287.0 10.25 34.980
3336561 294.0 10.07 34.960
3336561 298.0  9.86 34.920
3336561 300.0  9.80 34.920
3336561 310.0  9.76 34.900
3336561 320.0  9.48 34.890
3336561 325.0  9.46 34.890
3336561 350.0  9.07 34.860
3336561 355.0  8.93 34.840
3336561 375.0  8.53 34.780
3336561 380.0  8.33 34.770
3336561 390.0  8.09 34.750
3336561 400.0  8.00 34.730
3336561 410.0  7.65 34.700
3336561 430.0  7.29 34.650
3336561 435.0  7.21 34.640
3336561 470.0  6.93 34.620
3336561 495.0  6.78 34.600
3336561 510.0  6.70 34.590
3337798   0.0 27.46 34.600
3337798   4.0 27.44 34.630
3337798  11.0 27.44 34.640
3337798  15.0 27.28 34.640
3337798  43.0 27.16 34.680
3337798  44.0 27.13 34.820
3337798  45.0 26.80 34.970
3337798  47.0 24.45 35.300
3337798  50.0 22.53 35.430
3337798  57.0 18.07 35.460
3337798  59.0 16.97 35.580
3337798  60.0 16.69 35.580
3337798  62.0 16.23 35.610
3337798  65.0 15.89 35.550
3337798  73.0 14.63 35.500
3337798  77.0 14.44 35.470
3337798  84.0 13.97 35.450
3337798  90.0 13.79 35.450
3337798  94.0 13.69 35.410
3337798  97.0 13.65 35.410
3337798 120.0 13.54 35.400
3337798 144.0 13.34 35.370
3337798 164.0 13.20 35.340
3337798 177.0 13.09 35.340
3337798 181.0 12.97 35.340
3337798 183.0 12.93 35.320
3337798 201.0 12.63 35.270
3337798 203.0 12.61 35.280
3337798 216.0 12.46 35.240
3337798 221.0 12.35 35.200
3337798 224.0 12.15 35.200
3337798 226.0 11.97 35.100
3337798 231.0 11.57 35.140
3337798 233.0 11.55 35.140
3337798 234.0 11.50 35.090
3337798 240.0 11.12 35.020
3337798 244.0 10.84 35.040
3337798 248.0 10.81 35.020
3337798 253.0 10.62 34.990
3337798 256.0 10.25 34.960
3337798 257.0 10.18 34.970
3337798 265.0 10.09 34.950
3337798 271.0  9.93 34.940
3337798 274.0  9.91 34.930
3337798 279.0  9.54 34.880
3337798 283.0  9.38 34.880
3337798 287.0  9.31 34.850
3337798 292.0  9.03 34.840
3337798 301.0  8.79 34.830
3337798 310.0  8.74 34.820
3337798 350.0  7.81 34.740
3337798 355.0  7.73 34.720
3337798 400.0  7.55 34.700
3337798 415.0  7.52 34.690
3337798 445.0  7.25 34.660
3337798 460.0  6.99 34.640
3337798 480.0  6.78 34.610
3337798 505.0  6.32 34.580
3337798 515.0  6.30 34.580
3281507   0.0 27.52 35.694
3281507  44.0 27.55 35.694
3281507  47.9 27.55 35.694
3281507  68.9 27.54 35.724
3281507  69.9 27.50 35.764
3281507  71.9 26.67 36.014
3281507  72.9 26.36 36.054
3281507  73.9 25.97 36.034
3281507  74.9 25.24 36.044
3281507  75.9 24.54 36.094
3281507  78.9 24.04 36.124
3281507  79.9 23.76 36.114
3281507  80.9 23.67 36.104
3281507  84.9 23.13 36.084
3281507  86.9 22.74 36.054
3281507  87.9 22.62 36.064
3281507  89.9 22.28 36.034
3281507  90.9 22.18 36.044
3281507  91.9 22.08 36.044
3281507  92.9 21.76 35.974
3281507  93.9 21.00 35.794
3281507  94.8 20.40 35.754
3281507  95.8 19.08 35.544
3281507  96.8 18.39 35.594
3281507  97.8 17.97 35.534
3281507  99.8 16.86 35.594
3281507 101.8 16.65 35.634
3281507 104.8 16.28 35.624
3281507 105.8 16.22 35.634
3281507 107.8 16.17 35.644
3281507 108.8 16.15 35.634
3281507 115.8 15.75 35.604
3281507 119.7 15.56 35.604
3281507 127.7 15.40 35.584
3281507 134.7 15.24 35.564
3281507 141.6 15.14 35.554
3281507 147.6 14.98 35.534
3281507 158.6 14.79 35.514
3281507 165.5 14.71 35.514
3281507 174.5 14.58 35.494
3281507 190.4 14.46 35.474
3281507 209.3 14.13 35.434
3281507 218.2 13.99 35.414
3281507 228.2 13.82 35.394
3281507 235.2 13.72 35.384
3281507 246.1 13.59 35.364
3281507 256.0 13.49 35.354
3281507 269.0 13.36 35.334
3281507 276.9 13.32 35.334
3281507 284.9 13.21 35.314
3281507 297.8 12.88 35.274
3281507 311.7 12.71 35.254
3281507 325.6 12.42 35.234
3281507 342.5 11.82 35.154
3281507 351.5 11.25 35.094
3281507 357.4 11.09 35.084
3281507 362.4 11.05 35.084
3281507 367.4 10.80 35.064
3281507 371.4 10.73 35.054
3281507 379.3 10.40 35.034
3281507 392.2 10.38 35.044
3281507 402.2 10.12 35.014
3281507 405.1  9.98 34.994
3281507 412.1  9.69 34.964
3281507 415.1  9.44 34.934
3281507 427.0  8.98 34.884
3281507 431.0  8.87 34.874
3281507 437.9  8.68 34.854
3281507 441.9  8.61 34.844
3281507 446.9  8.49 34.834
3281507 455.8  8.28 34.804
3281507 468.7  8.11 34.784
3281507 473.7  8.07 34.774
3281507 484.6  7.87 34.754
3281507 495.5  7.74 34.754
3281507 501.5  7.68 34.734
3281507 508.4  7.59 34.734


Martin Maechler wrote:
> 
> I hope you see that almost all binary attachments are not allowed  (and
> silently deleted) on R-help, as it clearly says in the posting
> guide, or directly on http://www.r-project.org/mail.html
> 
> You should typically use  dump(<dataframe>)
> and paste ( not attach !) the result of that to (the end of)
> your message {or do something even more readable -- plain text
> in any case!}
> 
> With regards from your list maintainer,
> Martin Maechler <maechler at stat.math.ethz.ch>    http://stat.ethz.ch/~maechler/
> Seminar fuer Statistik, ETH-Zentrum  LEO C16    Leonhardstr. 27
> ETH (Federal Inst. Technology)  8092 Zurich     SWITZERLAND
> phone: x-41-1-632-3408          fax: ...-1228                   <><

-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From Icabalceta_j at wlf.state.la.us  Thu Mar  4 17:57:42 2004
From: Icabalceta_j at wlf.state.la.us (Icabalceta, Jorge L.)
Date: Thu, 4 Mar 2004 10:57:42 -0600 
Subject: [R] Gelman-Rubin Convergence test
Message-ID: <FF01C406D3A336489C58B9D0AE8E8E3702A0B0DC@wlfnt1.wlf.state.la.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040304/7dde2869/attachment.pl

From feh3k at spamcop.net  Thu Mar  4 15:33:38 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 4 Mar 2004 09:33:38 -0500
Subject: [R] "Statistiques avec R"
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
References: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <20040304093338.521d60fe.feh3k@spamcop.net>

On Thu, 04 Mar 2004 20:36:22 +0900
Shigeru Mase <mase at is.titech.ac.jp> wrote:

> Dear R users,
> 
> I want to share my joy with you. Please see the following
> excellent introduction to R "Statistiques avec R " by
> Vincent Zoonekynd
> 
> http://zoonek2.free.fr/UNIX/48_R/all.html
> 
> In paticular, you can see a lot of fascinating graphics
> examples of R from which you can get many hints.
> 
> Soryy if this is already well-known, but the CRAN search
> did not show nothing with the keyword "Zoonekynd".
> 
> Cheers,
> 
> Shigeru Mase,
> Dept. Comp. and Math. Sciences
> Tokyo Institute of Technology,
> Meguro, Tokyo, Japan 152-8550

This looks really excellent.  I wish I could read more than 50 words of
French.  An English version would be most welcome.

Frank



From mrennie at utm.utoronto.ca  Thu Mar  4 19:30:26 2004
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Thu,  4 Mar 2004 13:30:26 -0500
Subject: [R] Testing significance in a design with unequal but proportional
	sample sizes
Message-ID: <1078425026.404775c27ef60@webmail.utm.utoronto.ca>


Hi, all

I have a rather un-ideal dataset that I am trying to work with, and would 
appreciate any advice you have on the matter.

I have 4 years worth of data taken at 3 depth-zones from which samples have 
been taken at random. I am looking at the abundance of organism A between depth 
zones and across years, and am interested in the possible interaction of 
organism A distributions shifting between depth zones over time. Unfortunately, 
the sample sizes (n) differ between depth zones, as follows:

             Year
             1   2   3   4
Depth Zone 1 15  15  15  15
           2 10  10  10  10
           3 5   5   5   5

As such, I have a 2-way anova with unequal but proportional subclass numbers.
Sokal and Rolf (3rd Ed., 1995) have a nifty method of working out sums of 
squares in this type of scenario (page 357, 358, box 11.6).  However, they 
don't tell you how to calculate the probabilities, but refer the reader on to 
Snedecor and Cochran (1967), which I am on my way to consult shortly.

I'm curious as to whether there is a more straightforward method of coding this 
into R, rather than having to more or less customize my own statistical test.  
I found some discussions in the archives revolving around type III sums of 
squares from 2001, but the lack of consensus around the discussion did little 
to assure me that I should try this approach.

Anyone with advice, code or suggestions, I'd love to hear any of it.

Cheers,

Mike

-- 
Michael Rennie
Ph.D. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From ripley at stats.ox.ac.uk  Thu Mar  4 19:43:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Mar 2004 18:43:42 +0000 (GMT)
Subject: [R] need help with smooth.spline
In-Reply-To: <404749F2.4333F6FF@noaa.gov>
Message-ID: <Pine.LNX.4.44.0403041841020.10837-100000@gannet.stats>

Can we distinguish

smooth.spline() with a fixed penalty lambda.

smooth.spline() with a particular smoothing selector, e.g. GCV (the 
default).

I doubt if the smoothing fails but the selection of lambda might if the 
true curve is not smooth.


On Thu, 4 Mar 2004, W. C. Thacker wrote:

> "Liaw, Andy" wrote:
> Andy,
> 
> Is it known that smooth.spline() has a problem handling sharp jumps? 
> That is part of the question. It seems to work fine for some sharp
> jumps, but I have not yet been able to determine for which cases it
> should work well and for which it should fail. 
> 
> Maybe at least part of the problem has to do with end-point behavior. 
> Or with sampling intervals.
> 
> The data are from an archive, so their quality and their sampling
> characteristics are far from uniform.  Moreover, the true variability
> is non-normal, so recognizing bad data is difficult.  As the objective
> is to identify models for estimating salinity from temperature and
> pressure, what is important is to avoid outliers, hoping that the
> contamination from bad data with believable values is small.
> 
> I'll try to get the packages installed and take a look at the function
> you mentioned.
> 
> In the meantime, do you know of some criteria for recognizing when
> smooth.spline might fail?  It seems to work quite well for the bulk of
> the data.
> 
> Thanks,
> 
> Carlisle
> 
> 
> 
> > Hi Carlisle,
> > 
> > If I understand you correctly, the problem is smooth.spline() not handling
> > sharp jump(s), right?  If so, it's probably easier to try something that can
> > handle such features.  Wavelet `denoising' (as opposed to `smoothing', and
> > available in the wavethresh package) is well known for being able to handle
> > abrupt changes (very `spatially adaptive').  Other things you might consider
> > are mars() in the `mda' package (which fits splines in an adaptive fashion)
> > and locfit() in the `locfit' package.  For locfit, you will want to specify
> > local smoothing parameter selection, via a call like
> > 
> >   locfit(..., alpha=c(0, 0, 2), acri="cp")
> > 
> > You might need to play with the `2' a bit to get the right amount of
> > smoothing.  The details are in Loader's book `Local regression and
> > Likelihood'.
> > 
> > HTH,
> > Andy
> > 
> > > From: W. C. Thacker
> > >
> > > Andy,
> > >
> > > As the data are often noisy, smoothing splines should be appropriate.
> > >
> > > The first example profile shows an isothermal (constant temperature)
> > > layer in the upper ocean followed by a sharp thermocline (large
> > > temperature gradient), but there are relatively few observations
> > > defining this sharp transition.  In this case simple linear
> > > interpolation works fine, but smooth.spline() with all defaults gives
> > > an absolutely absurd value in the isothermal layer.  With all.knots =
> > > TRUE, the values in the isothermal layer are much better but still
> > > peculiar.
> > >
> > > Given the sampling and the data, is it possible to get smooth.spline()
> > > do better?  If so, would that adversely impact its performance for
> > > other cases?  (There are thousands of profiles.)  If not, is there a
> > > simp[le way to select cases that smooth.spline() should not be
> > > expected to handle, so they can be treated separately?
> > >
> > > Thanks,
> > >
> > > Carlisle
> > >
> > > "Liaw, Andy" wrote:
> > > >
> > > > If you really want interpolation, should you be using
> > > spline() rather than
> > > > smooth.spline()?  The later is for smoothing data observed
> > > with noise, not
> > > > for interpolation.
> > > >
> > > > Andy
> > > >
> > > > > From: W. C. Thacker
> > > > >
> > > > > Dear R listers,
> > > > >
> > > > > When using smooth.spline to interpolate data, results are
> > > generally
> > > > > good.  However, some cases produce totally unreasonable results.
> > > > >
> > > > > The data are values of pressure, temperature, and salinity from a
> > > > > probe that is lowered into the ocean, and the objective is to
> > > > > interpolate temperature and salinity to specified
> > > pressures.  While
> > > > > smooth.spline provides excellent values at the observed pressures,
> > > > > there are cases when the values at the desired pressures are
> > > > > unusable.  A dataframe with four such profiles, indicated
> > > by values of
> > > > > id, is attached.  My target values for pressure are
> > > seq(25,1600,25),
> > > > > but 1:500 is also interesting.
> > > > >
> > > > > Setting all.knots = TRUE helps, but it would be nice to
> > > be able to do
> > > > > better.
> > > > >
> > > > > Any suggestions?
> > > > >
> > > > > Thanks,
> > > > >
> > > > > Carlisle
> > > > >
> > > > > > version
> > > > >          _
> > > > > platform sparc-sun-solaris2.9
> > > > > arch     sparc
> > > > > os       solaris2.9
> > > > > system   sparc, solaris2.9
> > > > > status
> > > > > major    1
> > > > > minor    8.0
> > > > > year     2003
> > > > > month    10
> > > > > day      08
> > > > > language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mvdv at spamcop.net  Thu Mar  4 20:37:41 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Fri, 5 Mar 2004 06:37:41 +1100
Subject: [R] Alternative mail archives?
In-Reply-To: <26D5AB9F6512D611A8610001FA7E136F03278454@se-drc-mail4.selu.astrazeneca.net>
Message-ID: <000001c40220$2a310080$344610ac@FEB0480>

Thanks for the suggestions - the google cache is a way out. I'll also look
at the Upenn site.
MV

> -----Original Message-----
> From: Lennart.Borgman at astrazeneca.com 
> [mailto:Lennart.Borgman at astrazeneca.com] 
> Sent: Thursday, March 04, 2004 9:51 PM
> To: mvdv at spamcop.net; r-help at stat.math.ethz.ch
> Subject: RE: [R] Alternative mail archives?
> 
> 
> Excuse me for my previous answers, they were not very well 
> thought out. I hope this one is better. Thanks to those that 
> made me realize this!
> 
> Mark, did you try to look at the "cached" link in the search 
> results? The search is carried out by Google and the "cached" 
> link is often a very useful feature. The "cached" link goes 
> to Google's stored copy of the page.
> 
> - Lennart
> 
> 
> -----Original Message-----
> From: Mark Van De Vyver [mailto:mvdv at spamcop.net]
> Sent: 4 mars 2004 07:10
> To: r-help at stat.math.ethz.ch
> Subject: [R] Alternative mail archives?
> 
> 
> Hi, 
> The searchable mail archive at 
> http://maths.newcastle.edu.au/~rking/R/ is > very useful.  
> Unfortunately it seems that many of the emails are not 
> available, many show up in the search results but then return 
> a "file not found" when following the link. Is anyone else 
> experiencing this, and is there an alternative mail archive 
> with a search facility? TIA Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Carlisle.Thacker at noaa.gov  Thu Mar  4 21:03:50 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Thu, 04 Mar 2004 15:03:50 -0500
Subject: [R] need help with smooth.spline
References: <Pine.LNX.4.44.0403041841020.10837-100000@gannet.stats>
Message-ID: <40478BA6.C41F2226@noaa.gov>

Dear Prof. Ripley,

I'm not sure what you are asking. 

Prof Brian Ripley wrote:
> 
> Can we distinguish
> 
> smooth.spline() with a fixed penalty lambda.
> 
> smooth.spline() with a particular smoothing selector, e.g. GCV (the
> default).
> 
> I doubt if the smoothing fails but the selection of lambda might if the
> true curve is not smooth.

I have used the default GCV to determine how much smoothing.  This
generally works fine, and for the cases where it has difficulties it
still works well for most of the data.  

My original posting had a few examples in an attachment that was
stripped off but later reposted.  Andy Liaw has verified (see below)
that smooth.spline() works well with these data at the original
points, and I have suggested pictures showing how it fails away from
the original points.  They do not show the extreme magnitudes of the
bad values, as they lie beyond the range of the pictures

Should I be using something other than the defaults?  I need something
that will work for many cases.

Is there a way to recognize which data are likely to cause problems
for smooth.spline(), which might be used to separate them for special
treatment? 

Thanks,

Carlisle
 
> Andy,
> 
> You are seeing that smooth.spline() reproduces the observations.  The
> problem is with predicting away from the observations.
> 
> > I tried looking at the data you posted but don't see anything wrong with
> > smooth.spline().  Am I looking at the right thing?
> > 
> > > dat <- read.table("clipboard", header=T) # Read data from the Windoze
> > clipboard
> > > dat <- split(x[,2:4], x$id)
> > > str(dat)
> > List of 4
> >  $ 3281507:`data.frame':        77 obs. of  3 variables:
> >   ..$ p: num [1:77] 0 44 47.9 68.9 69.9 71.9 72.9 73.9 74.9 75.9 ...
> >   ..$ t: num [1:77] 27.5 27.6 27.6 27.5 27.5 ...
> >   ..$ s: num [1:77] 35.7 35.7 35.7 35.7 35.8 ...
> >  $ 3322643:`data.frame':        70 obs. of  3 variables:
> >   ..$ p: num [1:70] 0 42 44 47 49 53 55 58 59 63 ...
> >   ..$ t: num [1:70] 23.3 23.3 23.2 22.1 21.3 ...
> >   ..$ s: num [1:70] 35.5 35.5 35.5 35.6 35.6 ...
> >  $ 3336561:`data.frame':        62 obs. of  3 variables:
> >   ..$ p: num [1:62] 0 37 39 40 43 47 49 50 52 53 ...
> >   ..$ t: num [1:62] 24.5 24.5 24.4 24.4 23.8 ...
> >   ..$ s: num [1:62] 35.3 35.3 35.3 35.4 35.5 ...
> >  $ 3337798:`data.frame':        59 obs. of  3 variables:
> >   ..$ p: num [1:59] 0 4 11 15 43 44 45 47 50 57 ...
> >   ..$ t: num [1:59] 27.5 27.4 27.4 27.3 27.2 ...
> >   ..$ s: num [1:59] 34.6 34.6 34.6 34.6 34.7 ...
> > > par(mfrow=c(2,2))
> > > for(i in 1:4) { plot(dat[[i]]$p, dat[[i]]$s);
> > lines(smooth.spline(dat[[i]]$p, dat[[i]]$s))}
> 
> Look at this:
> 
> par(mfrow=c(2,4))
> for(i in 1:4) {
>  plot(dat[[i]]$p, dat[[i]]$t);
>  lines(seq(25,1000,25),
>        predict(smooth.spline(dat[[i]]$p,
>                dat[[i]]$t),seq(25,1000,25))$y);
>  plot(dat[[i]]$p, dat[[i]]$s);
>  lines(seq(25,1000,25),
>        predict(smooth.spline(dat[[i]]$p,
>                dat[[i]]$s),seq(25,1000,25))$y)}
> 
> or at this:
> 
> par(mfrow=c(2,4))
> for(i in 1:4) {
>  plot(dat[[i]]$p, dat[[i]]$t);
>  lines(seq(25,1000,1),
>        predict(smooth.spline(dat[[i]]$p,
>                dat[[i]]$t),seq(25,1000,1))$y);
>  plot(dat[[i]]$p, dat[[i]]$s);
>  lines(seq(25,1000,1),
>        predict(smooth.spline(dat[[i]]$p,
>                dat[[i]]$s),seq(25,1000,1))$y)}
> 
> Thanks,
> 
> Carlisle
     id     p     t      s
3322643   0.0 23.26 35.490
3322643  42.0 23.26 35.500
3322643  44.0 23.19 35.550
3322643  47.0 22.12 35.600
3322643  49.0 21.34 35.639
3322643  53.0 17.34 35.610
3322643  55.0 16.23 35.580
3322643  58.0 15.51 35.520
3322643  59.0 15.39 35.460
3322643  63.0 14.82 35.450
3322643  67.0 14.67 35.470
3322643  68.0 14.66 35.480
3322643  72.0 14.37 35.450
3322643  76.0 14.26 35.420
3322643  82.0 13.92 35.380
3322643  86.0 13.83 35.370
3322643  94.0 13.49 35.350
3322643 104.0 13.23 35.340
3322643 112.0 13.09 35.310
3322643 127.0 12.76 35.270
3322643 134.0 12.71 35.270
3322643 138.0 12.63 35.270
3322643 142.0 12.60 35.260
3322643 146.0 12.47 35.240
3322643 155.0 12.39 35.240
3322643 164.0 12.34 35.230
3322643 170.0 12.18 35.210
3322643 172.0 12.16 35.210
3322643 175.0 12.09 35.210
3322643 201.0 11.74 35.160
3322643 206.0 11.67 35.150
3322643 218.0 11.57 35.130
3322643 223.0 11.47 35.130
3322643 232.0 11.38 35.120
3322643 243.0 11.29 35.110
3322643 253.0 11.16 35.100
3322643 265.0 11.10 35.100
3322643 277.0 11.10 35.110
3322643 285.0 10.98 35.100
3322643 295.0 10.87 35.090
3322643 305.0 10.78 35.090
3322643 320.0 10.68 35.090
3322643 330.0 10.58 35.080
3322643 340.0 10.62 35.090
3322643 355.0 10.53 35.090
3322643 375.0 10.55 35.150
3322643 395.0 10.46 35.140
3322643 405.0 10.36 35.140
3322643 425.0 10.25 35.150
3322643 470.0  9.80 35.120
3322643 480.0  9.78 35.120
3322643 515.0  9.39 35.090
3322643 540.0  8.95 35.060
3322643 545.0  8.94 35.050
3322643 565.0  8.66 35.030
3322643 570.0  8.64 35.020
3322643 610.0  8.13 34.970
3322643 630.0  7.99 34.950
3322643 660.0  7.70 34.930
3322643 695.0  7.20 34.900
3322643 745.0  6.62 34.840
3322643 780.0  6.46 34.820
3322643 795.0  6.29 34.820
3322643 810.0  6.26 34.820
3322643 860.0  5.91 34.800
3322643 905.0  5.72 34.810
3322643 920.0  5.67 34.820
3322643 940.0  5.64 34.830
3322643 975.0  5.57 34.840
3322643 985.0  5.57 34.850
3336561   0.0 24.48 35.330
3336561  37.0 24.46 35.340
3336561  39.0 24.43 35.340
3336561  40.0 24.40 35.400
3336561  43.0 23.81 35.500
3336561  47.0 20.90 35.749
3336561  49.0 17.27 35.829
3336561  50.0 16.60 35.799
3336561  52.0 16.18 35.709
3336561  53.0 16.11 35.699
3336561  56.0 15.83 35.590
3336561  60.0 15.14 35.550
3336561  70.0 14.76 35.530
3336561  71.0 14.73 35.520
3336561  73.0 14.62 35.510
3336561  75.0 14.52 35.500
3336561  85.0 14.38 35.480
3336561 106.0 14.29 35.460
3336561 109.0 14.19 35.450
3336561 112.0 14.09 35.420
3336561 113.0 14.09 35.430
3336561 118.0 13.83 35.400
3336561 127.0 13.75 35.380
3336561 137.0 13.58 35.370
3336561 148.0 13.49 35.360
3336561 166.0 13.43 35.350
3336561 179.0 13.38 35.340
3336561 192.0 13.25 35.300
3336561 197.0 13.08 35.300
3336561 214.0 12.87 35.270
3336561 223.0 12.77 35.250
3336561 229.0 12.44 35.210
3336561 235.0 12.02 35.140
3336561 237.0 11.79 35.130
3336561 239.0 11.53 35.100
3336561 242.0 11.34 35.100
3336561 247.0 11.21 35.090
3336561 248.0 11.19 35.040
3336561 252.0 10.87 35.060
3336561 256.0 10.71 34.980
3336561 258.0 10.63 35.040
3336561 269.0 10.51 34.990
3336561 276.0 10.32 34.980
3336561 287.0 10.25 34.980
3336561 294.0 10.07 34.960
3336561 298.0  9.86 34.920
3336561 300.0  9.80 34.920
3336561 310.0  9.76 34.900
3336561 320.0  9.48 34.890
3336561 325.0  9.46 34.890
3336561 350.0  9.07 34.860
3336561 355.0  8.93 34.840
3336561 375.0  8.53 34.780
3336561 380.0  8.33 34.770
3336561 390.0  8.09 34.750
3336561 400.0  8.00 34.730
3336561 410.0  7.65 34.700
3336561 430.0  7.29 34.650
3336561 435.0  7.21 34.640
3336561 470.0  6.93 34.620
3336561 495.0  6.78 34.600
3336561 510.0  6.70 34.590
3337798   0.0 27.46 34.600
3337798   4.0 27.44 34.630
3337798  11.0 27.44 34.640
3337798  15.0 27.28 34.640
3337798  43.0 27.16 34.680
3337798  44.0 27.13 34.820
3337798  45.0 26.80 34.970
3337798  47.0 24.45 35.300
3337798  50.0 22.53 35.430
3337798  57.0 18.07 35.460
3337798  59.0 16.97 35.580
3337798  60.0 16.69 35.580
3337798  62.0 16.23 35.610
3337798  65.0 15.89 35.550
3337798  73.0 14.63 35.500
3337798  77.0 14.44 35.470
3337798  84.0 13.97 35.450
3337798  90.0 13.79 35.450
3337798  94.0 13.69 35.410
3337798  97.0 13.65 35.410
3337798 120.0 13.54 35.400
3337798 144.0 13.34 35.370
3337798 164.0 13.20 35.340
3337798 177.0 13.09 35.340
3337798 181.0 12.97 35.340
3337798 183.0 12.93 35.320
3337798 201.0 12.63 35.270
3337798 203.0 12.61 35.280
3337798 216.0 12.46 35.240
3337798 221.0 12.35 35.200
3337798 224.0 12.15 35.200
3337798 226.0 11.97 35.100
3337798 231.0 11.57 35.140
3337798 233.0 11.55 35.140
3337798 234.0 11.50 35.090
3337798 240.0 11.12 35.020
3337798 244.0 10.84 35.040
3337798 248.0 10.81 35.020
3337798 253.0 10.62 34.990
3337798 256.0 10.25 34.960
3337798 257.0 10.18 34.970
3337798 265.0 10.09 34.950
3337798 271.0  9.93 34.940
3337798 274.0  9.91 34.930
3337798 279.0  9.54 34.880
3337798 283.0  9.38 34.880
3337798 287.0  9.31 34.850
3337798 292.0  9.03 34.840
3337798 301.0  8.79 34.830
3337798 310.0  8.74 34.820
3337798 350.0  7.81 34.740
3337798 355.0  7.73 34.720
3337798 400.0  7.55 34.700
3337798 415.0  7.52 34.690
3337798 445.0  7.25 34.660
3337798 460.0  6.99 34.640
3337798 480.0  6.78 34.610
3337798 505.0  6.32 34.580
3337798 515.0  6.30 34.580
3281507   0.0 27.52 35.694
3281507  44.0 27.55 35.694
3281507  47.9 27.55 35.694
3281507  68.9 27.54 35.724
3281507  69.9 27.50 35.764
3281507  71.9 26.67 36.014
3281507  72.9 26.36 36.054
3281507  73.9 25.97 36.034
3281507  74.9 25.24 36.044
3281507  75.9 24.54 36.094
3281507  78.9 24.04 36.124
3281507  79.9 23.76 36.114
3281507  80.9 23.67 36.104
3281507  84.9 23.13 36.084
3281507  86.9 22.74 36.054
3281507  87.9 22.62 36.064
3281507  89.9 22.28 36.034
3281507  90.9 22.18 36.044
3281507  91.9 22.08 36.044
3281507  92.9 21.76 35.974
3281507  93.9 21.00 35.794
3281507  94.8 20.40 35.754
3281507  95.8 19.08 35.544
3281507  96.8 18.39 35.594
3281507  97.8 17.97 35.534
3281507  99.8 16.86 35.594
3281507 101.8 16.65 35.634
3281507 104.8 16.28 35.624
3281507 105.8 16.22 35.634
3281507 107.8 16.17 35.644
3281507 108.8 16.15 35.634
3281507 115.8 15.75 35.604
3281507 119.7 15.56 35.604
3281507 127.7 15.40 35.584
3281507 134.7 15.24 35.564
3281507 141.6 15.14 35.554
3281507 147.6 14.98 35.534
3281507 158.6 14.79 35.514
3281507 165.5 14.71 35.514
3281507 174.5 14.58 35.494
3281507 190.4 14.46 35.474
3281507 209.3 14.13 35.434
3281507 218.2 13.99 35.414
3281507 228.2 13.82 35.394
3281507 235.2 13.72 35.384
3281507 246.1 13.59 35.364
3281507 256.0 13.49 35.354
3281507 269.0 13.36 35.334
3281507 276.9 13.32 35.334
3281507 284.9 13.21 35.314
3281507 297.8 12.88 35.274
3281507 311.7 12.71 35.254
3281507 325.6 12.42 35.234
3281507 342.5 11.82 35.154
3281507 351.5 11.25 35.094
3281507 357.4 11.09 35.084
3281507 362.4 11.05 35.084
3281507 367.4 10.80 35.064
3281507 371.4 10.73 35.054
3281507 379.3 10.40 35.034
3281507 392.2 10.38 35.044
3281507 402.2 10.12 35.014
3281507 405.1  9.98 34.994
3281507 412.1  9.69 34.964
3281507 415.1  9.44 34.934
3281507 427.0  8.98 34.884
3281507 431.0  8.87 34.874
3281507 437.9  8.68 34.854
3281507 441.9  8.61 34.844
3281507 446.9  8.49 34.834
3281507 455.8  8.28 34.804
3281507 468.7  8.11 34.784
3281507 473.7  8.07 34.774
3281507 484.6  7.87 34.754
3281507 495.5  7.74 34.754
3281507 501.5  7.68 34.734
3281507 508.4  7.59 34.734

-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From tblackw at umich.edu  Thu Mar  4 21:06:37 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Thu, 4 Mar 2004 15:06:37 -0500 (EST)
Subject: [R] Testing significance in a design with unequal but proportional
	sample sizes
In-Reply-To: <1078425026.404775c27ef60@webmail.utm.utoronto.ca>
References: <1078425026.404775c27ef60@webmail.utm.utoronto.ca>
Message-ID: <Pine.SOL.4.58.0403041458230.2921@millipede.gpcc.itd.umich.edu>

Michael  -

Since your email says that the data are "the abundance of organism A",
I am moved to ask whether the abundances are integer counts, sometimes
zero, and whether the "samples" are perhaps dips of a net, or the
contents of a filter after pumping a certain amount of water through it,
or something akin to 'quadrats' in forest sampling.

If the abundances are integer counts, then it would be natural to
analyze the data with a log-linear model using R's  glm()  rather
than with anova.  Snedecor and Cochran is an excellent book, but for
this purpose Venables and Ripley's MASS (Modern Applied Statistics
with S and S-plus) might be better.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 4 Mar 2004, Michael Rennie wrote:

> Hi, all
>
> I have a rather un-ideal dataset that I am trying to work with, and would
> appreciate any advice you have on the matter.
>
> I have 4 years worth of data taken at 3 depth-zones from which samples have
> been taken at random. I am looking at the abundance of organism A between depth
> zones and across years, and am interested in the possible interaction of
> organism A distributions shifting between depth zones over time. Unfortunately,
> the sample sizes (n) differ between depth zones, as follows:
>
>              Year
>              1   2   3   4
> Depth Zone 1 15  15  15  15
>            2 10  10  10  10
>            3 5   5   5   5
>
> As such, I have a 2-way anova with unequal but proportional subclass numbers.
> Sokal and Rolf (3rd Ed., 1995) have a nifty method of working out sums of
> squares in this type of scenario (page 357, 358, box 11.6).  However, they
> don't tell you how to calculate the probabilities, but refer the reader on to
> Snedecor and Cochran (1967), which I am on my way to consult shortly.
>
> I'm curious as to whether there is a more straightforward method of coding this
> into R, rather than having to more or less customize my own statistical test.
> I found some discussions in the archives revolving around type III sums of
> squares from 2001, but the lack of consensus around the discussion did little
> to assure me that I should try this approach.
>
> Anyone with advice, code or suggestions, I'd love to hear any of it.
>
> Cheers,
>
> Mike
> --
> Michael Rennie
> Ph.D. Candidate
> University of Toronto at Mississauga
> 3359 Mississauga Rd. N.
> Mississauga ON  L5L 1C6
> Ph: 905-828-5452  Fax: 905-828-3792
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From loraine at loraine.net  Thu Mar  4 21:12:23 2004
From: loraine at loraine.net (Ann Loraine)
Date: Thu, 4 Mar 2004 12:12:23 -0800
Subject: [R] regular expressions in R?
Message-ID: <4006D8EA-6E18-11D8-977E-000A959EED5E@loraine.net>

Hello!

Are there any additional packages (besides "regex") I could use to 
incorporate regular expressions in my R code?

I would be grateful for any tips or help you could provide!

Yours,

Ann Loraine



From katkih at mail.nih.gov  Thu Mar  4 21:18:07 2004
From: katkih at mail.nih.gov (Katki, Hormuzd (NIH/NCI))
Date: Thu, 4 Mar 2004 15:18:07 -0500 
Subject: [R] passing a formula to coxph() along with a ...
Message-ID: <9D7EF737FA4C6F4FBBFC52FC30B83690D4A4C3@nihexchange7.nih.gov>

	Hello.  I am making a program that allows a user to enter a formula
to be used in coxph() and then does some other stuff to it, and also allows
the user to set other options within coxph().  

What is the proper way to pass a formula to a function along with a ...?  I
have a function:

func <-  function(coxformula, data = parent.frame(),...) {
# Cut to relevant part of function

coxmod <- coxph(formula=coxformula, data=dataframe,...)

resids <- residuals(coxmod,'dfbeta')
}

I use the function call:
func(Surv(failtime,cancer)~smoke,dataframe=dataset,robust=TRUE)

The coxph() evaluates properly.  But the error is in evaluating the
residuals(): 

Error in model.frame(formula = coxformula, data = dataframe, : 
	Object "coxformula" not found

When using recover(), the error upon running the residuals function is:
Error during wrapup: Object "coxformula" not found

But there are 2 times when it does work:
1.  If I do residuals.coxph(coxmod,type='martingale'), then it does work!
(But for no other residual type)
2.  If I don't include anything in the ... (i.e. I don't ask for robust=TRUE
in the call), then it works fine.

Any help is greatly appreciated.  Thank you,
Hormuzd Katki

Hormuzd Katki
Biostatistics Branch, Division of Cancer Epidemiology and Genetics
National Cancer Institute, NIH, DHHS
6120 Executive Blvd. Room 8044 MSC 7244
Rockville, MD 20852-4910
301-594-7818 (voice)
301-402-0081 (fax)
katkih at mail.nih.gov



From ozric at web.de  Thu Mar  4 21:30:50 2004
From: ozric at web.de (Christian Schulz)
Date: Thu, 4 Mar 2004 21:30:50 +0100
Subject: [R] regular expressions in R?
In-Reply-To: <4006D8EA-6E18-11D8-977E-000A959EED5E@loraine.net>
References: <4006D8EA-6E18-11D8-977E-000A959EED5E@loraine.net>
Message-ID: <200403042130.51248.ozric@web.de>

Ann,

there  is something in base:

regexpr(pattern, text,  extended = TRUE, perl = FALSE, fixed = FALSE)
IMHO show under ?grep

christian


Am Donnerstag, 4. M?rz 2004 21:12 schrieb Ann Loraine:
> Hello!
>
> Are there any additional packages (besides "regex") I could use to
> incorporate regular expressions in my R code?
>
> I would be grateful for any tips or help you could provide!
>
> Yours,
>
> Ann Loraine
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From pocernic at rap.ucar.edu  Thu Mar  4 21:28:15 2004
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Thu, 4 Mar 2004 13:28:15 -0700 (MST)
Subject: [R] lattice, different plotting symbols 
Message-ID: <Pine.LNX.4.44.0403041327470.32127-100000@albedo.rap.ucar.edu>

Hi,

I am trying to plot different variables from a data.frame using lattice's
xyplot using code like that below.  How do I specify a symbol and color
for the variable 'prob' and different one's for 'll.prob'?

Thanks,  Matt

xyplot( prob +  ll.prob ~ time.eff |stat.id + time.out ,data = OUT,
       allow.multiple = TRUE,
       layout = c(6,3), as.table = TRUE ,

       panel = function(x,y){panel.abline(h = 0)
       panel.xyplot(x,y)},
       strip = TRUE,
       )

Matt Pocernich
NCAR - Research Applications Program
303-497-8312



From solares at unsl.edu.ar  Thu Mar  4 22:15:00 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Thu, 4 Mar 2004 18:15:00 -0300 (ART)
Subject: [R] interactive plots
Message-ID: <55774.170.210.173.216.1078434900.squirrel@inter17.unsl.edu.ar>

Hi, i dont understand how i draw graphics interactive, by example
double click in axis and change the scale and zoom in or zoom out.Ruben



From robert.kissell at citigroup.com  Thu Mar  4 23:19:11 2004
From: robert.kissell at citigroup.com (Kissell, Robert [EQRE])
Date: Thu, 4 Mar 2004 17:19:11 -0500
Subject: [R] Command Line Programs
Message-ID: <4115749EFC8862458D6FE4F04F5F7DE701E82EE7@EXCHNY37.ny.ssmb.com>

Hi,
I have recently started using R again (switched from MatLab) and have a question regarding programming. How can I set up a file that will run several lines of R code? For example, in Fortran and DOS this used to be done via a batch file, and in MatLab I would write the code and save it in a script file, e.g., Test.m.

An example of what I am trying to do is the following:

x=matrix(c(1,2,3,4,5,6,7,8,9),3,3)
y=matrix(c(rnorm(9),3,3)
z1=x*y
z2=x%*%y

eigen(z1)
eigen(z2)


I would like to store these commands in a file and simply run/call/invoke that file rather than have to type in the line commands everyday. The above is a very simple snapahot of what I am doing.

Right Now I have a function that runs these commands. Also, I can easily COPY the commands from some editor (e.g., Word) and PASTE to the R Console, and R will automatically run the commands.

The question is, Is there anyway to invoke a series of commands outside of the function? In MatLab, as I mentioned, I would save the commands in a script file say TEST.m and run the commands from the MatLab command line prompt simply by typing TEST <enter>.

Thanks in advance.


Rob



From sgamibo at hotmail.com  Thu Mar  4 23:41:17 2004
From: sgamibo at hotmail.com (Chou Andy)
Date: Thu, 04 Mar 2004 22:41:17 +0000
Subject: [R] R commands
Message-ID: <Law11-F106DiRh4xzdI00029216@hotmail.com>

I'm a R beginner.  Recently I conduct a project about expected return that 
involves using R for calculations.  I encounter three questions as follow:

Q1.  How to use R to demonstrate bootstrapping and Monte Carlo simulation 
under 10,000 times of random selection?
Q2.  How to do a loop in R?
Q3.  How to use R to do binominal tree calculation under multiple periods?

I'm perplexed by the R commands, so any hints or examples will be highly 
appreciated.


Andy

_________________________________________________________________



From acarr at saskforestcentre.ca  Thu Mar  4 23:43:38 2004
From: acarr at saskforestcentre.ca (Angus Carr)
Date: Thu, 04 Mar 2004 16:43:38 -0600
Subject: [R] Extracting Krig results
Message-ID: <4047B11A.3020707@saskforestcentre.ca>

I'm new at this, so please bear with me.

I am trying to get Krig (actually tps) results into a data frame at a 
set of points.
Frankly, all I really need is to query the Krig on a specific XY pair 
and return a result. I can carry the rest. I am using tps from the 
fields package.

Here's the code I use to create the Krig object:
=========== Snip! ===============================
dCurr <- sqlQuery(ds,"select x,y,acmi50 from public.cc_output" )
fitCurr <- Tps(dCurr[1:2],dCurr$acmi50, scale.type="unscaled")
surface(fitCurr,type="C", extrap=TRUE, main="Median ACMI, 1961-1990", 
xlab="", ylab="", levels=evens)
===============================================

Ultimately, I want a table of XY locations and their appropriate value 
from the surface. From that, I can do further statistics.

The XY locations are a regular grid, although they may not be in the 
final product. If they are a regular grid, I could use a loop to drive 
the process.

How can I do this? I am baffled by the help file, and cannot tell what 
to do.

In principle, I could read the code of the surface function and figure 
out what it does, then do the same, but I am sure I am not the only 
person who wants to do some analysis on the output of an interpolation 
function.

I want to run rpart on a data frame containing 20 variables (or so), 
some of which are interpolated from weather stations.

Thanks for any help you can provide,
Angus Carr.



From abunn at montana.edu  Thu Mar  4 23:52:23 2004
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 4 Mar 2004 15:52:23 -0700
Subject: [R] Command Line Programs
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82EE7@EXCHNY37.ny.ssmb.com>
Message-ID: <000401c4023b$5e54ac70$78f05a99@msu.montana.edu>

?source

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Kissell, Robert [EQRE]
> Sent: Thursday, March 04, 2004 3:19 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Command Line Programs
> 
> 
> Hi,
> I have recently started using R again (switched from MatLab) 
> and have a question regarding programming. How can I set up a 
> file that will run several lines of R code? For example, in 
> Fortran and DOS this used to be done via a batch file, and in 
> MatLab I would write the code and save it in a script file, 
> e.g., Test.m.
> 
> An example of what I am trying to do is the following:
> 
> x=matrix(c(1,2,3,4,5,6,7,8,9),3,3)
> y=matrix(c(rnorm(9),3,3)
> z1=x*y
> z2=x%*%y
> 
> eigen(z1)
> eigen(z2)
> 
> 
> I would like to store these commands in a file and simply 
> run/call/invoke that file rather than have to type in the 
> line commands everyday. The above is a very simple snapahot 
> of what I am doing.
> 
> Right Now I have a function that runs these commands. Also, I 
> can easily COPY the commands from some editor (e.g., Word) 
> and PASTE to the R Console, and R will automatically run the commands.
> 
> The question is, Is there anyway to invoke a series of 
> commands outside of the function? In MatLab, as I mentioned, 
> I would save the commands in a script file say TEST.m and run 
> the commands from the MatLab command line prompt simply by 
> typing TEST <enter>.
> 
> Thanks in advance.
> 
> 
> Rob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Thu Mar  4 23:57:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 4 Mar 2004 17:57:44 -0500
Subject: [R] Command Line Programs
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7918@usrymx25.merck.com>

Save the lines in a file.  At the R prompt, you would type
source("myscript.R").  Outside of R, it depends on which OS you're using.
On *nix, you can do R CMD BATCH script.R.  On Windows, read the rw-FAQ.

Andy

> From: Kissell, Robert [EQRE]
> 
> Hi,
> I have recently started using R again (switched from MatLab) 
> and have a question regarding programming. How can I set up a 
> file that will run several lines of R code? For example, in 
> Fortran and DOS this used to be done via a batch file, and in 
> MatLab I would write the code and save it in a script file, 
> e.g., Test.m.
> 
> An example of what I am trying to do is the following:
> 
> x=matrix(c(1,2,3,4,5,6,7,8,9),3,3)
> y=matrix(c(rnorm(9),3,3)
> z1=x*y
> z2=x%*%y
> 
> eigen(z1)
> eigen(z2)
> 
> 
> I would like to store these commands in a file and simply 
> run/call/invoke that file rather than have to type in the 
> line commands everyday. The above is a very simple snapahot 
> of what I am doing.
> 
> Right Now I have a function that runs these commands. Also, I 
> can easily COPY the commands from some editor (e.g., Word) 
> and PASTE to the R Console, and R will automatically run the commands.
> 
> The question is, Is there anyway to invoke a series of 
> commands outside of the function? In MatLab, as I mentioned, 
> I would save the commands in a script file say TEST.m and run 
> the commands from the MatLab command line prompt simply by 
> typing TEST <enter>.
> 
> Thanks in advance.
> 
> 
> Rob
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From pallier at lscp.ehess.fr  Fri Mar  5 00:08:09 2004
From: pallier at lscp.ehess.fr (pallier)
Date: Fri, 05 Mar 2004 00:08:09 +0100
Subject: [R] Testing significance in a design with unequal but proportional
	sample sizes
In-Reply-To: <1078425026.404775c27ef60@webmail.utm.utoronto.ca>
References: <1078425026.404775c27ef60@webmail.utm.utoronto.ca>
Message-ID: <4047B6D9.6030603@lscp.ehess.fr>

Hello,

This is a follow up on the question about the analysis of unbalanced
data, based on my (limited) understanding of what goes in such cases.

When the data is unbalanced in a factorial design,
the main effect of a given factor can be defined in several ways.
Which type of main effet is relevant depends on the scientific question.

Some textbooks distinguish between weighted and unweighted mean effects.

If you use the 'aov' function with an unbalanced design, it will report 
(for the
first factor in the formula), the f-ratio associated to  the "weighted
means" solution. That is, the computation of the main effect ignores the
unbalance: The effect size of a factor 'a' is computed regardless of the
distributions of the units among other factors.

Consider:

 > x<-scan()
1: 1 2 3
4: 4 5 6 7 8
9: 1 2 3 4 5
14: 6 7 8
17:
Read 16 items
 > a<-factor(rep(c(1,2),c(8,8)))
 > b<-factor(rep(c(1,2,1,2),c(3,5,5,3)))
 >
 > tapply(x,list(a=a,b=b),mean)
   b
a   1 2
  1 2 6
  2 3 7
 > tapply(x,a,mean)
  1   2
4.5 4.5


If all units are given the same weights (that is we ignore the factor 'b'),
then the main effect of a is 0.
This is confirmed by:

 > summary(aov(x~a*b))
            Df    Sum Sq   Mean Sq   F value    Pr(>F)
a            1 2.417e-32 2.417e-32 1.209e-32 1.0000000
b            1        60        60        30 0.0001413 ***
a:b          1 5.621e-31 5.621e-31 2.810e-31 1.0000000
Residuals   12        24         2

This is called the weighted means approach because the subgroups defined 
by the
crossing of a and b are given weights proportional the their size.


Now, another approach is to forget about the individual units
and just consider the table of means:

 > tapply(x,list(a=a,b=b),mean)
   b
a   1 2
  1 2 6
  2 3 7


Forgetting about the samples' sizes, one way to defined the main effect 
of 'a'
is as the mean of 2 and 6 versus the mean of 3 and 7:

 > t=tapply(x,list(a=a,b=b),mean)
 > diff(apply(t,1,mean))
2
1


That is '1'

One can compute a "fake" Mean Square associated to 'a' as 
(n-1)*effect-size=15*1=15,
and compare it to the MSE from the previous ANOVA (2 with 12 d.f.)

The f-ratio=15/2=7.5 reaches significance:
 > pf(7.5,1,12)
[1] 0.9820225
 >

If I am correct, this is what textbooks call the "unweighted means" 
approach.
In many cases, it is this type of main effect which is relevant.
(Especialy when the unbalance is due to random missing observations.)

I do not know if there is a solution with R
for easily computing the unweigthed main effects and assessing
their significance. (Anyone?)

Actually, the different types of main effects defined above just 
correspond to different
contrasts on the cell means. So if there is an easy solution to compute 
arbitrary contrasts
on the cell means in a factorial design, this could an approach to this
question. (Anyone?)


Christophe



From p.dalgaard at biostat.ku.dk  Fri Mar  5 00:09:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Mar 2004 00:09:52 +0100
Subject: [R] Command Line Programs
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82EE7@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82EE7@EXCHNY37.ny.ssmb.com>
Message-ID: <x2llmg1a67.fsf@biostat.ku.dk>

"Kissell, Robert [EQRE]" <robert.kissell at citigroup.com> writes:

> Right Now I have a function that runs these commands. Also, I can
> easily COPY the commands from some editor (e.g., Word) and PASTE to
> the R Console, and R will automatically run the commands.
> 
> The question is, Is there anyway to invoke a series of commands
> outside of the function? In MatLab, as I mentioned, I would save the
> commands in a script file say TEST.m and run the commands from the
> MatLab command line prompt simply by typing TEST <enter>.
> 
> Thanks in advance.

source("foo.R")   # in R
R CMD BATCH foo.R # from a command line interface
Rcmd BATCH foo.R  # Windows variation of same

Was this really not to be found in any of the documents available to
you?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pallier at lscp.ehess.fr  Fri Mar  5 00:11:43 2004
From: pallier at lscp.ehess.fr (pallier)
Date: Fri, 05 Mar 2004 00:11:43 +0100
Subject: [R] Command Line Programs
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82EE7@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82EE7@EXCHNY37.ny.ssmb.com>
Message-ID: <4047B7AF.5030406@lscp.ehess.fr>

Kissell, Robert [EQRE] wrote:

>How can I set up a file that will run several lines of R code? For example, in Fortran and DOS this used to be done via a batch file, and in MatLab I would write the code and save it in a script file, e.g., Test.m.
>  
>
 From within R:

source('yourscript.R',echo=T)


 From the command line (under unix/Linux):

R BATCH yourscript.R

or

R <yourscript.R

--
Christophe



From deepayan at stat.wisc.edu  Fri Mar  5 00:21:23 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 4 Mar 2004 17:21:23 -0600
Subject: [R] lattice, different plotting symbols
In-Reply-To: <Pine.LNX.4.44.0403041327470.32127-100000@albedo.rap.ucar.edu>
References: <Pine.LNX.4.44.0403041327470.32127-100000@albedo.rap.ucar.edu>
Message-ID: <200403041721.23869.deepayan@stat.wisc.edu>

On Thursday 04 March 2004 14:28, Matt Pocernich wrote:
> Hi,
>
> I am trying to plot different variables from a data.frame using
> lattice's xyplot using code like that below.  How do I specify a symbol
> and color for the variable 'prob' and different one's for 'll.prob'?
>
> Thanks,  Matt
>
> xyplot( prob +  ll.prob ~ time.eff |stat.id + time.out ,data = OUT,
>        allow.multiple = TRUE,
>        layout = c(6,3), as.table = TRUE ,
>
>        panel = function(x,y){panel.abline(h = 0)
>        panel.xyplot(x,y)},
>        strip = TRUE,
>        )

What you are plotting is a grouped display (equivalent to having a 
non-trivial 'groups' argument), which means your panel function must be 
able to handle arguments called 'groups' and 'subscripts'. The standard 
panel function meant to be used for this is panel.superpose. This would be 
the default if you didn't specify a panel function, but since you also 
want a horizontal line, you need to do something like

        panel = function(x,y,...) {
            panel.abline(h = 0)
            panel.superpose(x,y,...)
        }

(using ... is the easiest way to specify 'groups' and 'subscripts', you 
could also supply them explicitly.)

This would use different symbols and colors obtained from the global 
lattice settings (as displayed by show.settings()), which you could 
override by explicitly specifying 'col' and 'pch' (for color and symbol 
respectively) to the xyplot call. (Both should be of length 2 in this 
case)

Deepayan



From roger at ysidro.econ.uiuc.edu  Fri Mar  5 01:10:26 2004
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 4 Mar 2004 18:10:26 -0600 (CST)
Subject: [R] need help with smooth.spline
In-Reply-To: <40478BA6.C41F2226@noaa.gov>
Message-ID: <Pine.SOL.4.30.0403041808560.20346-100000@ysidro.econ.uiuc.edu>


If one repeats the experiments in Craven and Wahba, the paper that
"invented" GCV you find, or at least I found, when I tried to do this
some years ago,  that GCV fails in about 10%
of cases rather catastrophically, and this is a fairly innocuous
setting.  So one way to interpret Brian's comment would be that maybe
it is GCV that is failing, and another choice of lambda might do better.

Obviously, Brian can interpret for himself.  I would only add that in cases
where you really want something with sharp breaks in derivatives then
then the usual L_2 roughness penalties are not very appropriate however
you choose to do the smoothing.

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From andy_liaw at merck.com  Fri Mar  5 01:30:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 4 Mar 2004 19:30:39 -0500
Subject: [R] need help with smooth.spline
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7919@usrymx25.merck.com>

The strange thing (to me, and to Carlisle) is the behavior of
predict.smooth.spline() on the data he posted:  part of the predictions are
way off from the data.  If one just plot prediction at the input data, it
looks just fine.  What am I missing?

Andy

> From: Roger Koenker [mailto:roger at ysidro.econ.uiuc.edu] 
> 
> If one repeats the experiments in Craven and Wahba, the paper that
> "invented" GCV you find, or at least I found, when I tried to do this
> some years ago,  that GCV fails in about 10%
> of cases rather catastrophically, and this is a fairly innocuous
> setting.  So one way to interpret Brian's comment would be that maybe
> it is GCV that is failing, and another choice of lambda might 
> do better.
> 
> Obviously, Brian can interpret for himself.  I would only add 
> that in cases
> where you really want something with sharp breaks in derivatives then
> then the usual L_2 roughness penalties are not very 
> appropriate however
> you choose to do the smoothing.
> 
> url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				
> Champaign, IL 61820
> 
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From h.wickham at auckland.ac.nz  Fri Mar  5 01:36:24 2004
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Fri, 05 Mar 2004 13:36:24 +1300
Subject: [R] lattice, different plotting symbols
In-Reply-To: <Pine.LNX.4.44.0403041327470.32127-100000@albedo.rap.ucar.edu>
References: <Pine.LNX.4.44.0403041327470.32127-100000@albedo.rap.ucar.edu>
Message-ID: <4047CB88.2090207@auckland.ac.nz>

Hi Matt,

> I am trying to plot different variables from a data.frame using lattice's
> xyplot using code like that below.  How do I specify a symbol and color
> for the variable 'prob' and different one's for 'll.prob'?

I think you probably want to use panel.superpose instead of 
panel.xyplot.  Behind the scenes xyplot is converting prob + ll.prob to 
groups - and panel.superpose knows how to plot different groups with 
different symbols.  Normally xyplot changes the default panel from 
panel.xyplot to panel.superpose when you use that formula construct, but 
because you've specified your own panel function it can't.

Hadley

> 
> Thanks,  Matt
> 
> xyplot( prob +  ll.prob ~ time.eff |stat.id + time.out ,data = OUT,
>        allow.multiple = TRUE,
>        layout = c(6,3), as.table = TRUE ,
> 
>        panel = function(x,y){panel.abline(h = 0)
>        panel.xyplot(x,y)},
>        strip = TRUE,
>        )
> 
> Matt Pocernich
> NCAR - Research Applications Program
> 303-497-8312
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mvdv at spamcop.net  Fri Mar  5 04:03:58 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Fri, 5 Mar 2004 14:03:58 +1100
Subject: [R] ISO country codes available in R's {map}?
In-Reply-To: <D5E776D7B9EAA14AA68A5E004C7A9B698531D1@evs1.econ.usyd.edu.au>
Message-ID: <000701c4025e$81914860$344610ac@FEB0480>

Hi
Does any one know wether it is possible to get the Country codes from the
Map package - I've not found anything in the docs and only seen a post re.
how to get country names.  I'd like to programmatically marry some data
indexed by ISO country code to the map package contry names.  If not then
I'll need to hunt for a listing and try and marry that to the {map}
names.....

TIA
Mark



From ray at mcs.vuw.ac.nz  Fri Mar  5 04:35:33 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 5 Mar 2004 16:35:33 +1300 (NZDT)
Subject: [R] ISO country codes available in R's {map}?
Message-ID: <200403050335.i253ZXuM002881@tahi.mcs.vuw.ac.nz>

> Does any one know wether it is possible to get the Country codes from the
> Map package - I've not found anything in the docs and only seen a post re.
> how to get country names.  I'd like to programmatically marry some data
> indexed by ISO country code to the map package contry names.  If not then
> I'll need to hunt for a listing and try and marry that to the {map}
> names.....
> 
No part of the maps package knows anything about ISO country codes.  I
guess if there was a database of ISO country codes somewhere, it
wouldn't be too difficult to provide some add-on functionality to merge
the internal polygon numbers with country code numbers.  The hardest
part would be what to do with mismatches.

Ray Brownrigg



From hodgess at gator.uhd.edu  Fri Mar  5 05:15:25 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 4 Mar 2004 22:15:25 -0600
Subject: [R] country codes
Message-ID: <200403050415.i254FPY09119@gator.dt.uh.edu>

Dear R People:

Someone was looking for a country code list:

http://kropla.com/dialcode.htm#table

Have fun!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From lanstin at aol.net  Fri Mar  5 05:31:02 2004
From: lanstin at aol.net (Christopher Austin-Lane)
Date: Thu, 4 Mar 2004 23:31:02 -0500 (EST)
Subject: [R] Slow reshape from 5x600000 to 6311 x 132
Message-ID: <40480285.7080808@aol.net>

I have a dataset that's a few hundred thousand rows from a database 
(read in via dbreadTable).  The database is like:

 > str(measures)
`data.frame':   609363 obs. of  5 variables:
  $ vih.id   : int  1 2 3 4 5 6 7 8 9 10 ... 

  $ vi.id    : int  1 2 3 4 5 6 7 8 9 10 ... 

  $ vih.value: chr  "0" "1989" "0" "N/A" ... 

  $ vih.date : chr  "20040226012314" "20040226012315" "20040226012315" 
"20040226012315" ... 

  $ vih.run.n: int  1 1 1 1 1 1 1 1 1 1 ..
I'm reshaping it to be like

 > str(better)
`data.frame':   132 obs. of  6311 variables:
  $ vih.run.n     : int  1 2 4 5 6 7 8 9 10 11 ...
  $ vih.value.1   : chr  "0" "0" "0" "0" ...
  $ vih.value.2   : chr  "1989" "1989" "1989" "1989" ...
  $ vih.value.3   : chr  "0" "0" "0" "0" ...
  $ vih.value.4   : chr  "N/A" "N/A" "N/A" "N/A" ...
  $ vih.value.5   : chr  "3163979" "3163979" "3163979" "3163979" ...
  $ vih.value.6   : chr  "5500073" "5500073" "5500073" "5500073" ...

(etc., etc.)

This takes about 4-8 hours to accomplish.  Should I

a) try to put it into the wide format row by row as I get the data from 
the DB instead of using dbReadTable,

or

b) try to tune something in R?  (I'm trying it now with  R 
--min-vsize=600M --min-nsize=6M although it's not seeming fast; I won't 
know if it's faster for a while).

(Using home compiled R 1.8.1 on Mac OS X 10.3.2, under emacs/ESS, 
although my R 1.8.1 on Solaris 2.8 has been churning for a few hours as 
well (on a split of the data that is 630 variables by 1000 obs).

--Chris



From klealambrou at hotmail.com  Fri Mar  5 06:05:26 2004
From: klealambrou at hotmail.com (klea lambrou)
Date: Fri, 05 Mar 2004 05:05:26 +0000
Subject: [R] (no subject)
Message-ID: <Law11-F101i0SYNIPuk000431ef@hotmail.com>


   hello R-users.could you please help me on this one?i have 2 vectors x1
   and y1 and both have the same size.i want to use the locpoly function
   from the kernsmooth package in order to estimate a regression
   function.at each estimation i want to leave one observation out,and i
   want my estimation to be at the observation i left out.i tried :

    for (j in 1:length(x1))
   + A<-locpoly(x1[-1],y1[-1],bandwidth=4,gridsize=1,range.x=x1[1])
   Error in rep.default(0, dimfkap) : invalid number of copies in "rep"

   i want the gridsize to be equal to one,and i will choose the bandwidth
   at each time.what am i doing wrong?can you help me?


From ckm-r at loafer.org  Fri Mar  5 06:44:54 2004
From: ckm-r at loafer.org (Christopher Mahmood)
Date: Thu, 4 Mar 2004 21:44:54 -0800
Subject: [R] Alternative mail archives?
In-Reply-To: <000101c401ad$ffb70150$344610ac@FEB0480>
References: <3A822319EB35174CA3714066D590DCD504AF78F7@usrymx25.merck.com>
	<000101c401ad$ffb70150$344610ac@FEB0480>
Message-ID: <20040305054454.GA10965@oogabooga.loafer.org>

* Mark Van De Vyver (mvdv at spamcop.net) [040303 22:11]:
> Is anyone else experiencing this, and is there an alternative mail archive
> with a search facility?

http://marc.theaimsgroup.com

-- 

-ckm



From seanpor at acm.org  Fri Mar  5 07:40:17 2004
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 05 Mar 2004 06:40:17 +0000
Subject: [R] R commands
In-Reply-To: <Law11-F106DiRh4xzdI00029216@hotmail.com>
References: <Law11-F106DiRh4xzdI00029216@hotmail.com>
Message-ID: <404820D1.2050709@acm.org>

Hi Andy,

I know it doesn't answer all of your questions but have you read the
Posting guide (see the bottom of all posts) ?
http://www.R-project.org/posting-guide.html and in particular, try
reading "An Introduction to R", chapter 9?

Note that in R, looping is relatively "slow" - if you can do things
using a vector, this is much better.

Try going to the R-project.org homepage and on the left hand side you'll
see Search - does searching for "bootstrap monte carlo" answer your
first question?

cheers,
Sean



Chou Andy wrote:

> I'm a R beginner. Recently I conduct a project about expected return
> that involves using R for calculations. I encounter three questions as
> follow:
>
> Q1. How to use R to demonstrate bootstrapping and Monte Carlo
> simulation under 10,000 times of random selection?
> Q2. How to do a loop in R?
> Q3. How to use R to do binominal tree calculation under multiple periods?
>
> I'm perplexed by the R commands, so any hints or examples will be
> highly appreciated.
>
>
> Andy
>
> _________________________________________________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From shitao at hotmail.com  Fri Mar  5 07:43:18 2004
From: shitao at hotmail.com (Tao Shi)
Date: Fri, 05 Mar 2004 06:43:18 +0000
Subject: [R] a question regarding 'cor' function
Message-ID: <Sea2-F60pbY0dwLpczr00010b9f@hotmail.com>

I'm a little bit confused with how exactly the 'cor' function handles 
missing values.  Should the last two function calls give the same results?   
Thanks,
...Tao

==========================================
>x=rnorm(10)
>y=rnorm(10)
>x[1]=NA
>y[2]=NA
>cor(x,y,method="spearman")
[1] -0.006060606

>cor(x,y,use="pairwise.complete.obs",method="spearman")
[1] -0.006060606
>cor(x[3:10],y[3:10],method="spearman")
[1] 0.1666667
==========================================

_________________________________________________________________
Create a Job Alert on MSN Careers and enter for a chance to win $1000! 
http://msn.careerbuilder.com/promo/kaday.htm?siteid=CBMSN_1K&sc_extcmp=JS_JASweep_MSNHotm2



From Catherine.Wang at infotech.monash.edu.au  Fri Mar  5 04:39:53 2004
From: Catherine.Wang at infotech.monash.edu.au (Xiaozhe (Catherine) Wang)
Date: Fri, 05 Mar 2004 14:39:53 +1100
Subject: [R] Lyapunov exponent code for time series
Message-ID: <4047F689.C9B6E49C@infotech.monash.edu.au>

Dear all, 
Has anyone worked on coding for calculating Lyapunov Exponent for a time
series data? or any package is available for computing Lyapunov?

Please advice and many thanks in advance.

Catherine X Wang



From Catherine.Wang at infotech.monash.edu.au  Fri Mar  5 04:39:53 2004
From: Catherine.Wang at infotech.monash.edu.au (Xiaozhe (Catherine) Wang)
Date: Fri, 05 Mar 2004 14:39:53 +1100
Subject: [R] Lyapunov exponent code for time series
Message-ID: <4047F689.C9B6E49C@infotech.monash.edu.au>

Dear all, 
Has anyone worked on coding for calculating Lyapunov Exponent for a time
series data? or any package is available for computing Lyapunov?

Please advice and many thanks in advance.

Catherine X Wang



From ozric at web.de  Fri Mar  5 08:28:43 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 5 Mar 2004 08:28:43 +0100
Subject: [R] Slow reshape from 5x600000 to 6311 x 132
In-Reply-To: <40480285.7080808@aol.net>
References: <40480285.7080808@aol.net>
Message-ID: <200403050828.45392.ozric@web.de>

Hi,

my reshape's  from ~1.4 million obs. to  ~150.00 obs. & 50 attr. goes 
surprinsing fast (1-2 miniutes), but is less complex then yours. Perhaps it 
is faster if you have no character.string as value - if it's
possible for your data?

Reshaping in the database is possible with
innerselects  ,but i prefer reshape because it take
in the db really long time?

christian
 

Am Freitag, 5. M?rz 2004 05:31 schrieb Christopher Austin-Lane:
> I have a dataset that's a few hundred thousand rows from a database
>
> (read in via dbreadTable).  The database is like:
>  > str(measures)
>
> `data.frame':   609363 obs. of  5 variables:
>   $ vih.id   : int  1 2 3 4 5 6 7 8 9 10 ...
>
>   $ vi.id    : int  1 2 3 4 5 6 7 8 9 10 ...
>
>   $ vih.value: chr  "0" "1989" "0" "N/A" ...
>
>   $ vih.date : chr  "20040226012314" "20040226012315" "20040226012315"
> "20040226012315" ...
>
>   $ vih.run.n: int  1 1 1 1 1 1 1 1 1 1 ..
> I'm reshaping it to be like
>
>  > str(better)
>
> `data.frame':   132 obs. of  6311 variables:
>   $ vih.run.n     : int  1 2 4 5 6 7 8 9 10 11 ...
>   $ vih.value.1   : chr  "0" "0" "0" "0" ...
>   $ vih.value.2   : chr  "1989" "1989" "1989" "1989" ...
>   $ vih.value.3   : chr  "0" "0" "0" "0" ...
>   $ vih.value.4   : chr  "N/A" "N/A" "N/A" "N/A" ...
>   $ vih.value.5   : chr  "3163979" "3163979" "3163979" "3163979" ...
>   $ vih.value.6   : chr  "5500073" "5500073" "5500073" "5500073" ...
>
> (etc., etc.)
>
> This takes about 4-8 hours to accomplish.  Should I
>
> a) try to put it into the wide format row by row as I get the data from
> the DB instead of using dbReadTable,
>
> or
>
> b) try to tune something in R?  (I'm trying it now with  R
> --min-vsize=600M --min-nsize=6M although it's not seeming fast; I won't
> know if it's faster for a while).
>
> (Using home compiled R 1.8.1 on Mac OS X 10.3.2, under emacs/ESS,
> although my R 1.8.1 on Solaris 2.8 has been churning for a few hours as
> well (on a split of the data that is 630 variables by 1000 obs).
>
> --Chris
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Mar  5 08:29:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 05 Mar 2004 08:29:43 +0100
Subject: [R] Lyapunov exponent code for time series
In-Reply-To: <4047F689.C9B6E49C@infotech.monash.edu.au>
References: <4047F689.C9B6E49C@infotech.monash.edu.au>
Message-ID: <40482C67.1080102@statistik.uni-dortmund.de>

Xiaozhe (Catherine) Wang wrote:
> Dear all, 
> Has anyone worked on coding for calculating Lyapunov Exponent for a time
> series data? or any package is available for computing Lyapunov?
> 
> Please advice and many thanks in advance.
> 
> Catherine X Wang


I don't think there is code available.

You might want to "google" for "TISEAN" and/or contribute the desired 
code yourself ...

Uwe Ligges



From arnab at myrealbox.com  Fri Mar  5 08:48:15 2004
From: arnab at myrealbox.com (Arnab mukherji)
Date: Fri, 05 Mar 2004 07:48:15 +0000
Subject: [R] Probit predictions outside (0,1) interval
Message-ID: <1078472895.c3b6129carnab@myrealbox.com>

Hi!

I was trying to implement a probit model on a dichotomous outcome variable and found that the predictions were outside the (0,1) interval that one should get. I later tried it with some simulated data with a similar result. 

Here is a toy program I wrote and I cant figure why I should be getting such odd predictions.

x1<-rnorm(1000)
x2<-rnorm(1000)
x3<-rnorm(1000)
x4<-rnorm(1000)
x5<-rnorm(1000)
x6<-rnorm(1000)
e1<-rnorm(1000)/3
e2<-rnorm(1000)/3
e3<-rnorm(1000)/3
y<-1-(1-pnorm(-2+0.33*x1+0.66*x2+1*x3+e1)*1-(pnorm(1+1.5*x4-0.25*x5+e2)*pnorm(1+0.2*x6+e3)))
y <- y>runif(1000)
dat<-data.frame(y = y, x1 = x1, x2 = x2, x3 = x3)
g<-glm(y~., data = dat, family = binomial)
summary(g)
yhat<-predict(g, dat)


Call:
glm(formula = y ~ ., family = binomial, data = dat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8383  -1.3519   0.7638   0.9249   1.3698  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  0.71749    0.06901  10.397  < 2e-16 ***
x1           0.10211    0.07057   1.447  0.14791    
x2           0.21068    0.07177   2.936  0.00333 ** 
x3           0.35162    0.07070   4.974 6.57e-07 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1275.3  on 999  degrees of freedom
Residual deviance: 1239.4  on 996  degrees of freedom
AIC: 1247.4

Number of Fisher Scoring iterations: 4

> yhat<-predict(g, dat)
> 
> range(yhat)
[1] -0.4416826  2.0056527
> range(y)
[1] 0 1

Any advice would be really helpful.

thanks
Arnab



From ripley at stats.ox.ac.uk  Fri Mar  5 08:55:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 07:55:49 +0000 (GMT)
Subject: [R] need help with smooth.spline
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7919@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0403050747030.15022-100000@gannet.stats>

On Thu, 4 Mar 2004, Liaw, Andy wrote:

> The strange thing (to me, and to Carlisle) is the behavior of
> predict.smooth.spline() on the data he posted:  part of the predictions are
> way off from the data.  If one just plot prediction at the input data, it
> looks just fine.  What am I missing?

This is what can happen if lambda is estimated as far too small.

It is not clear to me that lambda should be being estimated and if it 
should be, that GCV is preferable to CV.  Also, there can be multiple 
local minima to either the GCV or CV criteria, and you may well want to 
set control.spar to control the range of search (which by default is a 
large range, perhaps over-large).

I was also pointing out that if the true `curve' is not smooth and you 
estimate lambda, you are not likely to get a sensible answer (for the 
reason Roger says).

But `I used smooth.spline()' without telling us how and why the particular 
arguments were chosen is exactly the sort of thing the posting guide warns 
against.

> 
> Andy
> 
> > From: Roger Koenker [mailto:roger at ysidro.econ.uiuc.edu] 
> > 
> > If one repeats the experiments in Craven and Wahba, the paper that
> > "invented" GCV you find, or at least I found, when I tried to do this
> > some years ago,  that GCV fails in about 10%
> > of cases rather catastrophically, and this is a fairly innocuous
> > setting.  So one way to interpret Brian's comment would be that maybe
> > it is GCV that is failing, and another choice of lambda might 
> > do better.
> > 
> > Obviously, Brian can interpret for himself.  I would only add 
> > that in cases
> > where you really want something with sharp breaks in derivatives then
> > then the usual L_2 roughness penalties are not very 
> > appropriate however
> > you choose to do the smoothing.
> > 
> > url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> > email	rkoenker at uiuc.edu			Department of Economics
> > vox: 	217-333-4558				University of Illinois
> > fax:   	217-244-6678				
> > Champaign, IL 61820
> > 
> > 
> > 
> > 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Mar  5 08:59:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 07:59:33 +0000 (GMT)
Subject: [R] Testing significance in a design with unequal but proportional
	sample sizes
In-Reply-To: <4047B6D9.6030603@lscp.ehess.fr>
Message-ID: <Pine.LNX.4.44.0403050756580.15022-100000@gannet.stats>

On Fri, 5 Mar 2004, pallier wrote:

...

> Actually, the different types of main effects defined above just 
> correspond to different
> contrasts on the cell means. So if there is an easy solution to compute 
> arbitrary contrasts
> on the cell means in a factorial design, this could an approach to this
> question. (Anyone?)

There are at least three such ways.  ?contrasts (for the assignment
function contrasts<-)  and ?C, as well as the contrasts= argument to aov 
(the function you were discussing ...).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Mar  5 09:02:08 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 5 Mar 2004 09:02:08 +0100
Subject: [R] Gelman-Rubin Convergence test
In-Reply-To: <FF01C406D3A336489C58B9D0AE8E8E3702A0B0DC@wlfnt1.wlf.state.la.us>
References: <FF01C406D3A336489C58B9D0AE8E8E3702A0B0DC@wlfnt1.wlf.state.la.us>
Message-ID: <16456.13312.217337.161219@gargle.gargle.HOWL>

Hola Jorge,

>>>>> "JLI" == Icabalceta, Jorge L <Icabalceta_j at wlf.state.la.us>
>>>>>     on Thu, 4 Mar 2004 10:57:42 -0600  writes:

    JLI> Dear friends, I run the Gelman-Rubin Convergence test
    JLI> for a MCMC object
which R package ?
    JLI> for a MCMC object I have and I got the following result
which function ?
    JLI> Multivariate psrf 1.07+0i, What does this mean? I guess
why don't you explain "psrf" ?

    JLI>  What does this mean? I guess
    JLI> (if I am not mistaken) that I should get a psrf close
    JLI> to 1.00 but what is 1.07+0i? Is that convergence or
    JLI> something else?  Jorge

In any case,  1.07+0i  is  "1.07 + 0i", a complex number with 0
imaginary part and indeed quite close to the real number "1".

I guess that whoever wrote the function you are using (unknown to us)
could improve his/her code by only return Re(*), the real part,
if that (real numbers) is what is expected here.

    JLI> 	[[alternative HTML version deleted]]

(do read the posting guide, indicated at the end of every R-help message!)

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From rkoenker at uiuc.edu  Thu Mar  4 16:54:11 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 4 Mar 2004 09:54:11 -0600
Subject: [R] need help with smooth.spline
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF790B@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF790B@usrymx25.merck.com>
Message-ID: <2D92C45D-6DF4-11D8-AFA5-000A95A7E3AA@uiuc.edu>

Another option for fitting smoothing spline models to data with abrupt  
changes
in derivative is the total variation penalty methods that are  
incorporated into
the R package "nprq" .   The models there are fitting piecewise linear  
models
both univariate and bivariate components are allowed, but the roughness
penalty is total variation of the derivative, or gradient in the  
bivariate case,
so sharp kinks are permitted.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Mar 4, 2004, at 8:30 AM, Liaw, Andy wrote:

> Hi Carlisle,
>
> If I understand you correctly, the problem is smooth.spline() not  
> handling
> sharp jump(s), right?  If so, it's probably easier to try something  
> that can
> handle such features.  Wavelet `denoising' (as opposed to `smoothing',  
> and
> available in the wavethresh package) is well known for being able to  
> handle
> abrupt changes (very `spatially adaptive').  Other things you might  
> consider
> are mars() in the `mda' package (which fits splines in an adaptive  
> fashion)
> and locfit() in the `locfit' package.  For locfit, you will want to  
> specify
> local smoothing parameter selection, via a call like
>
>   locfit(..., alpha=c(0, 0, 2), acri="cp")
>
> You might need to play with the `2' a bit to get the right amount of
> smoothing.  The details are in Loader's book `Local regression and
> Likelihood'.
>
> HTH,
> Andy
>
>> From: W. C. Thacker
>>
>> Andy,
>>
>> As the data are often noisy, smoothing splines should be appropriate.
>>
>> The first example profile shows an isothermal (constant temperature)
>> layer in the upper ocean followed by a sharp thermocline (large
>> temperature gradient), but there are relatively few observations
>> defining this sharp transition.  In this case simple linear
>> interpolation works fine, but smooth.spline() with all defaults gives
>> an absolutely absurd value in the isothermal layer.  With all.knots =
>> TRUE, the values in the isothermal layer are much better but still
>> peculiar.
>>
>> Given the sampling and the data, is it possible to get smooth.spline()
>> do better?  If so, would that adversely impact its performance for
>> other cases?  (There are thousands of profiles.)  If not, is there a
>> simp[le way to select cases that smooth.spline() should not be
>> expected to handle, so they can be treated separately?
>>
>> Thanks,
>>
>> Carlisle
>>
>> "Liaw, Andy" wrote:
>>>
>>> If you really want interpolation, should you be using
>> spline() rather than
>>> smooth.spline()?  The later is for smoothing data observed
>> with noise, not
>>> for interpolation.
>>>
>>> Andy
>>>
>>>> From: W. C. Thacker
>>>>
>>>> Dear R listers,
>>>>
>>>> When using smooth.spline to interpolate data, results are
>> generally
>>>> good.  However, some cases produce totally unreasonable results.
>>>>
>>>> The data are values of pressure, temperature, and salinity from a
>>>> probe that is lowered into the ocean, and the objective is to
>>>> interpolate temperature and salinity to specified
>> pressures.  While
>>>> smooth.spline provides excellent values at the observed pressures,
>>>> there are cases when the values at the desired pressures are
>>>> unusable.  A dataframe with four such profiles, indicated
>> by values of
>>>> id, is attached.  My target values for pressure are
>> seq(25,1600,25),
>>>> but 1:500 is also interesting.
>>>>
>>>> Setting all.knots = TRUE helps, but it would be nice to
>> be able to do
>>>> better.
>>>>
>>>> Any suggestions?
>>>>
>>>> Thanks,
>>>>
>>>> Carlisle
>>>>
>>>>> version
>>>>          _
>>>> platform sparc-sun-solaris2.9
>>>> arch     sparc
>>>> os       solaris2.9
>>>> system   sparc, solaris2.9
>>>> status
>>>> major    1
>>>> minor    8.0
>>>> year     2003
>>>> month    10
>>>> day      08
>>>> language R
>>>>
>>>>
>>>> --
>>>>
>>>> William Carlisle Thacker
>>>>
>>>> Atlantic Oceanographic and Meteorological Laboratory
>>>> 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
>>>> Office: (305) 361-4323           Fax: (305) 361-4392
>>>>
>>>> "Too many have dispensed with generosity
>>>>      in order to practice charity."     Albert Camus
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>
>>>
>> --------------------------------------------------------------
>> ----------------
>>> Notice:  This e-mail message, together with any
>> attachments,...{{dropped}}
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>> --  
>>
>> William
>> Carlisle Thacker
>>
>> Atlantic Oceanographic and Meteorological Laboratory
>> 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
>> Office: (305) 361-4323           Fax: (305) 361-4392
>>
>> "Too many have dispensed with generosity
>>      in order to practice charity."     Albert Camus
>>
>>
>
>
> ----------------------------------------------------------------------- 
> -------
> Notice:  This e-mail message, together with any  
> attachments,...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html


From berwin at jacksonia.maths.uwa.edu.au  Fri Mar  5 09:23:53 2004
From: berwin at jacksonia.maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 5 Mar 2004 16:23:53 +0800
Subject: [R] Probit predictions outside (0,1) interval
In-Reply-To: <1078472895.c3b6129carnab@myrealbox.com>
References: <1078472895.c3b6129carnab@myrealbox.com>
Message-ID: <16456.14617.600047.444172@bossiaea.maths.uwa.edu.au>

>>>>> "AM" == Arnab mukherji <arnab at myrealbox.com> writes:

    AM> Any advice would be really helpful.
Read the documentation of predict and then the one of predict.glm? ;-) 

I guess you actually wanted to do one of the following:

> yhat<-predict(g, dat, type="response")
> range(yhat)
[1] 0.2760238 0.9229622

or,

> yhat<-fitted(g)
> range(yhat)
[1] 0.2760238 0.9229622

Cheers,

        Berwin



From ripley at stats.ox.ac.uk  Fri Mar  5 09:24:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 08:24:45 +0000 (GMT)
Subject: [R] Probit predictions outside (0,1) interval
In-Reply-To: <1078472895.c3b6129carnab@myrealbox.com>
Message-ID: <Pine.LNX.4.44.0403050822090.15104-100000@gannet.stats>

On Fri, 5 Mar 2004, Arnab mukherji wrote:

> I was trying to implement a probit model on a dichotomous outcome
> variable and found that the predictions were outside the (0,1) interval
> that one should get. I later tried it with some simulated data with a
> similar result.
> 
> Here is a toy program I wrote and I cant figure why I should be getting
> such odd predictions.

Did it occur to you to read the help page?  The default type of prediction 
is "link", not "response".  Do try ?predict.glm.

...
> yhat<-predict(g, dat)
...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bob.ohara at helsinki.fi  Fri Mar  5 09:42:05 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Fri, 05 Mar 2004 10:42:05 +0200
Subject: [R] Probit predictions outside (0,1) interval
References: <1078472895.c3b6129carnab@myrealbox.com>
Message-ID: <40483D5D.20206@helsinki.fi>

Arnab mukherji wrote:
> Hi!
> 
> I was trying to implement a probit model on a dichotomous outcome variable and found that the predictions were outside the (0,1) interval that one should get. I later tried it with some simulated data with a similar result. 
> 
> Here is a toy program I wrote and I cant figure why I should be getting such odd predictions.
> 
Let me be the first to write "read the help file".

There are several scales th at you can predict on in a GLM.  The 
relevant part of the help file is this:

  type: the type of prediction required.  The default is on the scale
           of the linear predictors; the alternative `"response"' is on
           the scale of the response variable.  Thus for a default
           binomial model the default predictions are of log-odds
           (probabilities on logit scale) and `type = "response"' gives
           the predicted probabilities.

Your predictions are on the probit scale.

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From angel_lul at hotmail.com  Fri Mar  5 10:48:04 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Fri, 05 Mar 2004 09:48:04 +0000
Subject: [R] Lyapunov exponent code for time series
In-Reply-To: <40482C67.1080102@statistik.uni-dortmund.de>
References: <4047F689.C9B6E49C@infotech.monash.edu.au>
	<40482C67.1080102@statistik.uni-dortmund.de>
Message-ID: <40484CD4.603@hotmail.com>

"Frozen" package Funfits was able to. Although its succesor package is 
Fields, it has not lyapunov exponent calculations.
You can get funfits from :
http://www.cgd.ucar.edu/stats/Software/Funfits/index.shtml
I haven't got the statistical or programming knowledge to revise that 
code, if you are able to I would be keen to know of any updates.
Cheers,
Angel

Uwe Ligges wrote:

> Xiaozhe (Catherine) Wang wrote:
>
>> Dear all, Has anyone worked on coding for calculating Lyapunov 
>> Exponent for a time
>> series data? or any package is available for computing Lyapunov?
>>
>> Please advice and many thanks in advance.
>>
>> Catherine X Wang
>
>
>
> I don't think there is code available.
>
> You might want to "google" for "TISEAN" and/or contribute the desired 
> code yourself ...
>
> Uwe Ligges
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www..R-project.org/posting-guide.html
>
> .
>



From arc at arcriswell.com  Fri Mar  5 10:13:28 2004
From: arc at arcriswell.com (Andrew Criswell)
Date: Fri, 05 Mar 2004 16:13:28 +0700
Subject: [R] Lyapunov exponent code for time series
References: <4047F689.C9B6E49C@infotech.monash.edu.au>
Message-ID: <404844B8.8020806@arcriswell.com>

Dear Catherine:

You may wish to look at the website 
http://www.mpipks-dresden.mpg.de/~tisean/ which houses an open-source 
program and documentation that will do what you want.

ANDREW

--
Andrew R. Criswell, Ph.D.
Graduate School, Bangkok University

Xiaozhe (Catherine) Wang wrote:

>Dear all, 
>Has anyone worked on coding for calculating Lyapunov Exponent for a time
>series data? or any package is available for computing Lyapunov?
>
>Please advice and many thanks in advance.
>
>Catherine X Wang
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From Simon.Fear at synequanon.com  Fri Mar  5 10:09:19 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 5 Mar 2004 09:09:19 -0000
Subject: [R] "Statistiques avec R"
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02210@synequanon01>


> -----Original Message-----
> From: Frank E Harrell Jr [mailto:feh3k at spamcop.net]
> Sent: 04 March 2004 14:34
> To: r-help
> Subject: Re: [R] "Statistiques avec R"
> 
> 
> This looks really excellent.  I wish I could read more than 
> 50 words of
> French.  An English version would be most welcome.
> 

How great is the demand? I was a translator before I became
a statistician but I think this would be a massive undertaking ...

Maybe Shigeru Mase's point about the Japanese wiki site
holds here too: even without being able to read the text, 
you can still read the R code and look at the pictures ...

Anyone who wants to encourage me to look into the
idea of a translation - or anyone who thinks they could do
it easily and quickly - please email me, not the list.  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From Simon.Fear at synequanon.com  Fri Mar  5 10:59:43 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 5 Mar 2004 09:59:43 -0000
Subject: [R]  row-echelon form (was no subject)
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02211@synequanon01>

I think one needs an LU decomposition rather than QR. 
However, I couldn't find anything off the shelf to do 
an LU, other than learning that determinant() now 
uses LU instead of QR or SVD, so the code to do it must
be in there for those that want it.

You'll probably need to divide rows of U by the first
entry if you insist on the unique reduced REF.

However, I can't see any reason not to use John Fox'
suggested R function for small and well-conditioned
matrices, such as one might have to reduce for
a homework assignment in an introductory linear algebra 
course ...

Did anyone ever need a REF in any statistical application?

> -----Original Message-----
> From: John Fox [mailto:jfox at mcmaster.ca]
> Sent: 04 March 2004 15:38
> To: 'Spencer Graves'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] row-echelon form (was no subject)
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Dear Spencer,
> 
> The R matrix from the qr decomposition isn't quite in 
> row-echelon form,
> because the leading entry in each row is not 1, and the other 
> entries in a
> column with a leading entry aren't all 0. Some more examples:
> 
> > A  # nonsingular
>      [,1] [,2] [,3]
> [1,]    2   -2    0
> [2,]    1   -1    1
> [3,]    4    4   -4
> > rowEchelonForm(A)
>      [,1] [,2] [,3]
> [1,]    1    0    0
> [2,]    0    1    0
> [3,]    0    0    1
> > qr.R(qr(A))
>           [,1]      [,2]       [,3]
> [1,] -4.582576 -2.400397  3.2732684
> [2,]  0.000000  3.903600 -2.3421602
> [3,]  0.000000  0.000000  0.8944272
> 
> 
> > B  # rank 2
>      [,1] [,2] [,3] [,4]
> [1,]   -2    0   -1    2
> [2,]    4    0    1    0
> [3,]    6    0    1    2
> > rowEchelonForm(B)
>      [,1] [,2] [,3] [,4]
> [1,]    1    0    0    1
> [2,]    0    0    1   -4
> [3,]    0    0    0    0
> > qr.R(qr(B))
>          [,1]      [,2] [,3]          [,4]
> [1,] 7.483315 1.6035675    0  1.069045e+00
> [2,] 0.000000 0.6546537    0 -2.618615e+00
> [3,] 0.000000 0.0000000    0  6.336077e-16 
> 
> 
> Regards,
>  John
> 
> > -----Original Message-----
> > From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> > Sent: Thursday, March 04, 2004 9:31 AM
> > To: John Fox
> > Cc: 'Aimin Yan'; r-help at stat.math.ethz.ch
> > Subject: Re: [R] row-echelon form (was no subject)
> > 
> >       How about the following: 
> > 
> >  > A <- array(1:6, dim=c(3, 2))
> >  > A.qr <- qr(A)
> >  > qr.R(A.qr)
> >           [,1]      [,2]
> > [1,] -3.741657 -8.552360
> > [2,]  0.000000  1.963961
> >  >
> >     I'm no expert, either, and I don't have time now to 
> > research this further.  Perhaps someone else can further 
> > enlighten us both. 
> > spencer graves
> > 
> > John Fox wrote:
> > 
> > >Dear Spencer,
> > >
> > >I'd be surprised if the qr decomposition as computed in R 
> > weren't a lot 
> > >more stable numerically, but I'm no expert. As well, I don't 
> > know how 
> > >to get the row-echelon form from the qr decomposition -- though I 
> > >suspect that you or someone else on the list is about to 
> > enlighten me.
> > >
> > >Regards,
> > > John
> > >
> > >  
> > >
> > >>-----Original Message-----
> > >>From: Spencer Graves [mailto:spencer.graves at pdf.com]
> > >>Sent: Wednesday, March 03, 2004 10:45 PM
> > >>To: John Fox
> > >>Cc: 'Aimin Yan'; r-help at stat.math.ethz.ch
> > >>Subject: Re: [R] row-echelon form (was no subject)
> > >>
> > >>How does this compare with R of the qr decomposition?  
> > spencer graves
> > >>
> > >>John Fox wrote:
> > >>
> > >>    
> > >>
> > >>>Dear Amin,
> > >>>
> > >>>I have a function (created just for demonstration, and reproduced
> > >>>below) for finding the row-echelon form of a matrix. I'm
> > >>>      
> > >>>
> > >>sure that many
> > >>    
> > >>
> > >>>list members could produce something that's better 
> > numerically, but 
> > >>>this should be OK at least for toy problems.
> > >>>
> > >>>John
> > >>>
> > >>>--------- snip -------------
> > >>>
> > >>>rowEchelonForm <- function(X, tol=.Machine$double.eps){
> > >>>   if ((!is.matrix(X)) || (!is.numeric(X))) stop("argument
> > >>>      
> > >>>
> > >>must be a
> > >>    
> > >>
> > >>>numeric matrix")
> > >>>   Z <- X
> > >>>   for (i in 1:min(dim(X))){
> > >>>       if (i > 1) Z[i-1,] <- 0
> > >>>       which <- which.max(abs(Z[,i]))  # find maximum pivot
> > >>>      
> > >>>
> > >>in current
> > >>    
> > >>
> > >>>column at or below current row
> > >>>       pivot <- X[which, i]
> > >>>       if (abs(pivot) <= tol) next     # check for 0 pivot
> > >>>       if (which > i) X[c(i,which),] <- X[c(which,i),]  #
> > >>>      
> > >>>
> > >>exchange rows
> > >>    
> > >>
> > >>>       X[i,] <- X[i,]/pivot            # pivot
> > >>>       row <- X[i,]                    
> > >>>       X <- X - outer(X[,i], row)      # sweep
> > >>>       X[i,] <- row                    # restore current row
> > >>>       }
> > >>>   n <- nrow(X)
> > >>>   for (i in 1:n) if (max(abs(X[i,])) <= tol) X[c(i,n),] <-
> > >>>      
> > >>>
> > >>X[c(n,i),]   #
> > >>    
> > >>
> > >>>0 rows to bottom
> > >>>   X
> > >>>   }
> > >>>       
> > >>>
> > >>> 
> > >>>
> > >>>      
> > >>>
> > >>>>-----Original Message-----
> > >>>>From: r-help-bounces at stat.math.ethz.ch 
> > >>>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aimin Yan
> > >>>>Sent: Wednesday, March 03, 2004 2:42 PM
> > >>>>To: r-help at stat.math.ethz.ch
> > >>>>Subject: [R] (no subject)
> > >>>>
> > >>>>how to produce a  Row Reduced Echelon Form for a matrix in R?
> > >>>>Aimin Yan
> > >>>>
> > >>>>______________________________________________
> > >>>>R-help at stat.math.ethz.ch mailing list 
> > >>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>>>PLEASE do read the posting guide! 
> > >>>>http://www.R-project.org/posting-guide.html
> > >>>>   
> > >>>>
> > >>>>        
> > >>>>
> > >>>______________________________________________
> > >>>R-help at stat.math.ethz.ch mailing list 
> > >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>>PLEASE do read the posting guide! 
> > >>>http://www.R-project.org/posting-guide.html
> > >>> 
> > >>>
> > >>>      
> > >>>
> > >
> > >  
> > >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Mar  5 11:59:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 10:59:41 +0000 (GMT)
Subject: [R] browseURL question
In-Reply-To: <Pine.LNX.4.44.0402280756040.17994-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0403051037550.18999-100000@gannet.stats>

On further checking, this does not happen if netscape/mozilla/firefox are
used locally, but it does happen if they are used remotely, as well as 
with gnome-moz-remote and kfmclient (I do not have galeon).  There were 
other problems: it tries to use -remote "openURL()" for unknown browsers 
(which is why firefox worked) and there it was not quoting $ which is not 
protected by the "".

Under exactly what circumstances with `UNIX' were you finding the 
problem?

The problems I have positively identified are fixed in R-devel now.

On Sat, 28 Feb 2004, Prof Brian Ripley wrote:

> The problem is the interpretation by the shell used on Unix: on Windows no
> shell is used. It seems to me that the Unix version of browseURl should be
> quoting `url' in
> 
>     remoteCmd <- if (isLocal)
>         switch(basename(browser), "gnome-moz-remote" = , open = url,
>             galeon = paste("-x", url), kfmclient = paste("openURL",
>                 url), netscape = , mozilla = , opera = , {
>                 paste("-remote \"openURL(", gsub("([,)])", "%\\1",
>                   url), ")\"", sep = "")
>             })
>     else url
> 
> either by escaping & by \ or surrounding it by single quotes.
> 
> I can't see a simple solution for you with the present R codebase.
> 
> On Fri, 27 Feb 2004 jtleek at u.washington.edu wrote:
> 
> > I have a quick question about the browseURL function. When I use the
> > function in a UNIX environment, I have to use two sets of quotations if
> > I have the & symbol in the URL. For Windows I only need to use the first
> > set. For example, on Windows:
> > browseURL("http://search.yahoo.com/search?p=Bioconductor&ei=UTF-8&fr=fp-tab-web-t&n=20&fl=0&x=wrt")
> > 
> > will call up the appropriate website. However, if I use the same command
> > under UNIX, the website will be truncated after the first &, but the
> > call will work if I use:
> > browseURL("'http://search.yahoo.com/search?p=Bioconductor&ei=UTF-8&fr=fp-tab-web-t&n=20&fl=0&x=wrt'")
> > 
> > Where I have added single quotes around the URL.Is there any quick method for solving this problem? Thank you very much for your help.
> > 
> > 
> > Jeff Leek
> > Graduate Student
> > University of Washington
> > jtleek at u.washington.edu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pingping.zheng at lancaster.ac.uk  Fri Mar  5 12:10:57 2004
From: pingping.zheng at lancaster.ac.uk (Pingping Zheng)
Date: Fri, 05 Mar 2004 11:10:57 +0000
Subject: [R] Command Line Expressions
Message-ID: <40486041.20807@lancs.ac.uk>

Hi,

Is it possible to run R in command line to evalute R expressions
and return results to stdout, something like

 >R CMD -e "R.version$minor"
then you got return
 >"8.1"

Or do a simple calculation
 >R CMD -e "sin(1.2)"
 >0.932039

Thanks.

-- 
Pingping Zheng
Department of Mathematics and Statistics
Fylde College
Lancaster University
Lancaster LA1 4YF
UK



From monica.palaseanu-lovejoy at stud.man.ac.uk  Fri Mar  5 12:17:31 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Fri, 5 Mar 2004 11:17:31 -0000
Subject: [R] selecting certain rows from a data.frame
In-Reply-To: <200403041116.i24B8bhI024821@hypatia.math.ethz.ch>
Message-ID: <E1AzDKn-000JgE-84@probity.mcc.ac.uk>

Hi,

First of all - thank you for the answers regarding shared library. If i 
am attempting to translate the library from Linux to Windows - i 
think i will need some heavy baby-sitting ;-))) It is way out of my 
league - for now.

Meanwhile i have this new question: suppose i have a data.frame 
with x and y columns and 10 rows, 1 to 10. I also have a variable m 
(or an array if you like) with 5 numbers that represent some of the 
row order numbers in my data.frame. Let's make m(1, 4, 5, 8, 10). 
How can i select from my data.frame the rows number 1, 4, 5, 8, 
and 10?

Thank you in advance for any help,

Monica



From arc at arcriswell.com  Fri Mar  5 12:32:47 2004
From: arc at arcriswell.com (Andrew Criswell)
Date: Fri, 05 Mar 2004 18:32:47 +0700
Subject: [R] Constraining coefficients
References: <20040303042031.68883.qmail@web10310.mail.yahoo.com>
Message-ID: <4048655F.3070005@arcriswell.com>

Hello All:

I have a binomial model with one covariate, x1, treated as a factor with 
3 levels. The other covariate is measured x2 <- 1:30. The response, y, 
is the proportion of successes out of 20 trials.

glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial)

Now, I would like to constrain the cofficients on 2 levels of the 
factor, x1, to be identical and test the difference between these models 
by a likelihood ratio test.

How can I get glm() to constrain the coefficients on 2 levels to be the 
same?

Thanks,
ANDREW



From andy_liaw at merck.com  Fri Mar  5 12:26:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 5 Mar 2004 06:26:44 -0500
Subject: [R] "Statistiques avec R"
Message-ID: <3A822319EB35174CA3714066D590DCD504AF791A@usrymx25.merck.com>

The site does look very interesting!  I'd ask any effort any translation to
also include update to more recent R version.  1.5.1 _is_ rather old...

Best,
andy

> From: Simon Fear
> 
> > -----Original Message-----
> > From: Frank E Harrell Jr [mailto:feh3k at spamcop.net]
> > Sent: 04 March 2004 14:34
> > To: r-help
> > Subject: Re: [R] "Statistiques avec R"
> > 
> > 
> > This looks really excellent.  I wish I could read more than 
> > 50 words of
> > French.  An English version would be most welcome.
> > 
> 
> How great is the demand? I was a translator before I became
> a statistician but I think this would be a massive undertaking ...
> 
> Maybe Shigeru Mase's point about the Japanese wiki site
> holds here too: even without being able to read the text, 
> you can still read the R code and look at the pictures ...
> 
> Anyone who wants to encourage me to look into the
> idea of a translation - or anyone who thinks they could do
> it easily and quickly - please email me, not the list.  
>  
> Simon Fear 
> Senior Statistician 
> Syne qua non Ltd 
> Tel: +44 (0) 1379 644449 
> Fax: +44 (0) 1379 644445 
> email: Simon.Fear at synequanon.com 
> web: http://www.synequanon.com 
>   
> Number of attachments included with this message: 0 
>   
> This message (and any associated files) is confidential 
> and\...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From arc at arcriswell.com  Fri Mar  5 13:01:39 2004
From: arc at arcriswell.com (Andrew Criswell)
Date: Fri, 05 Mar 2004 19:01:39 +0700
Subject: [R] Constraining coefficients
Message-ID: <40486C23.4070108@arcriswell.com>

Hello All:

I have a binomial model with one covariate, x1, treated as a factor with 
3 levels. The other covariate is measured x2 <- 1:30. The response, y, 
is the proportion of successes out of 20 trials.

glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial)

Now, I would like to constrain the cofficients on 2 levels of the 
factor, x1, to be identical and test the difference between these models 
by a likelihood ratio test.

How can I get glm() to constrain the coefficients on 2 levels to be the 
same?

Thanks,
ANDREW



From B.Rowlingson at lancaster.ac.uk  Fri Mar  5 13:18:11 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 05 Mar 2004 12:18:11 +0000
Subject: [R] Command Line Expressions
In-Reply-To: <40486041.20807@lancs.ac.uk>
References: <40486041.20807@lancs.ac.uk>
Message-ID: <40487003.2030605@lancaster.ac.uk>

Pingping Zheng wrote:

> Is it possible to run R in command line to evalute R expressions
> and return results to stdout, something like
> 
> Or do a simple calculation
>  >R CMD -e "sin(1.2)"
>  >0.932039

Yes, with a bit of trickery!

  R on Unix will read from standard in, so you need to feed your R from 
stdin - typically use 'echo' to send a string to stdin.

  You'll also want to use --slave to stop all of R's startup messages, 
and probably --no-save as well.

  Also, you may need to cat() the expression:

$ echo "cat(sin(1.2))" | R --no-save --slave
0.932039

  ...otherwise you get R's default print labelling:

$echo "sin(1.2)" | R --no-save --slave
[1] 0.932039


Baz



From sdavis2 at mail.nih.gov  Fri Mar  5 13:21:30 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 05 Mar 2004 07:21:30 -0500
Subject: [R] selecting certain rows from a data.frame
In-Reply-To: <E1AzDKn-000JgE-84@probity.mcc.ac.uk>
Message-ID: <BC6DDAFA.56AA%sdavis2@mail.nih.gov>

On 3/5/04 6:17 AM, "Monica Palaseanu-Lovejoy"
<monica.palaseanu-lovejoy at stud.man.ac.uk> wrote:

> Meanwhile i have this new question: suppose i have a data.frame
> with x and y columns and 10 rows, 1 to 10. I also have a variable m
> (or an array if you like) with 5 numbers that represent some of the
> row order numbers in my data.frame. Let's make m(1, 4, 5, 8, 10).
> How can i select from my data.frame the rows number 1, 4, 5, 8,
> and 10?

If df is the name of your dataframe:

df[c(1,4,5,8,10),]

Will select only those rows.

I would highly recommend looking at the R manual, An Introduction to R.  It
can be obtained from the CRAN homepage (http://cran.us.r-project.org/).
There is a link on the left side called Manual.

Sean



From chrysopa at insecta.ufv.br  Fri Mar  5 13:16:25 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Fri, 5 Mar 2004 09:16:25 -0300
Subject: [R] Problems with SJava instalation
Message-ID: <200403050916.25758.chrysopa@insecta.ufv.br>

Hi,

I'm try to use the SJava package. The install is OK. In R I have this error 
message:

--------------------------
> library(SJava)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 
"/opt/lib/R/site-library/SJava/libs/SJava.so":
  libRSNativeJava.so: cannot open shared object file: No such file or 
directory
Error in library(SJava) : .First.lib failed
---------------------------

The R say that file libRSNativeJava.so dont exist, but it exist.

---------------------------
[root at zeus Rwork]# ls /opt/lib/R/site-library/SJava/libs/*
/opt/lib/R/site-library/SJava/libs/SJava.so
/opt/lib/R/site-library/SJava/libs/libRSNativeJava.so
---------------------------

When I run the RJava script I have this error:

---------------------------
/opt/lib/R/site-library/SJava/scripts/RJava
Loading RInterpreter library
Exception in thread "main" java.lang.UnsatisfiedLinkError: no RInterpreter in 
java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1403)
        at java.lang.Runtime.loadLibrary0(Runtime.java:788)
        at java.lang.System.loadLibrary(System.java:832)
        at 
org.omegahat.R.Java.ROmegahatInterpreter.<clinit>(ROmegahatInterpreter.java:34)
        at org.omegahat.R.Java.Examples.JavaRCall.main(JavaRCall.java:11)
---------------------------

I'm using GNU/Debian stable/testing with j2sdk 1.4:

---------------------------
ii  j2sdk1.4       1.4.1-6        Blackdown Java(TM) 2 SDK, Standard Edition
ii  j2sdk1.4-src   1.4.1-6        Blackdown Java(TM) 2 SDK, Standard Edition, 
ii  j2se-common    1.1            Common facilities for all Java2 Standard Edi
---------------------------

What is my problem?

Thanks
Ronaldo
-- 
Eeny, Meeny, Jelly Beanie, the spirits are about to speak!
		-- Bullwinkle Moose
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From Jan.Verbesselt at agr.kuleuven.ac.be  Fri Mar  5 13:50:15 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Fri,  5 Mar 2004 13:50:15 +0100
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
Message-ID: <1078491014.4048778700ac0@webmail1.kuleuven.be>

Hi R specialists,

The na.omit.ts() method fails when the time series contains internal
NA's. How can these automatically be removed?

> spectrum(ts.mNDII, na.action=na.omit)
Error in na.omit.ts(as.ts(x)) : time series contains internal NAs

How can the na.action be activated correctly?

> acf(ts.Lin, type=c("correlation"), na.action=na.omit)
Error in na.omit.ts(as.ts(x)) : time series contains internal NAs

((ts.Lin contains two time series, where one contains internal NAs
(-->an NA not a the end/beginning of a time serie)))

Thanks a lot!

Jan Verbesselt



From ripley at stats.ox.ac.uk  Fri Mar  5 13:53:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 12:53:01 +0000 (GMT)
Subject: [R] Constraining coefficients
In-Reply-To: <4048655F.3070005@arcriswell.com>
Message-ID: <Pine.LNX.4.44.0403051250340.21801-100000@gannet.stats>

On Fri, 5 Mar 2004, Andrew Criswell wrote:

> I have a binomial model with one covariate, x1, treated as a factor with 
> 3 levels. The other covariate is measured x2 <- 1:30. The response, y, 
> is the proportion of successes out of 20 trials.
> 
> glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial)
> 
> Now, I would like to constrain the cofficients on 2 levels of the 
> factor, x1, to be identical and test the difference between these models 
> by a likelihood ratio test.
> 
> How can I get glm() to constrain the coefficients on 2 levels to be the 
> same?

Merge the levels of the factor: see ?levels.

You could also set up a custom contrasts matrix: either way the natural S
approach is to reparametrize rather than constrain.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Mar  5 13:55:52 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 5 Mar 2004 07:55:52 -0500
Subject: [R] Command Line Expressions
Message-ID: <3A822319EB35174CA3714066D590DCD504AF791B@usrymx25.merck.com>

Use echo in a Unix shell and pipe it to R.  At the Windows command prompt,
you could try:

c:\home>echo R.version | Rterm --vanilla -q
> R.version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R

HTH,
Andy

> From: Pingping Zheng
> 
> Hi,
> 
> Is it possible to run R in command line to evalute R expressions
> and return results to stdout, something like
> 
>  >R CMD -e "R.version$minor"
> then you got return
>  >"8.1"
> 
> Or do a simple calculation
>  >R CMD -e "sin(1.2)"
>  >0.932039
> 
> Thanks.
> 
> -- 
> Pingping Zheng
> Department of Mathematics and Statistics
> Fylde College
> Lancaster University
> Lancaster LA1 4YF
> UK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From garbade at mip.paed.uni-muenchen.de  Fri Mar  5 13:56:06 2004
From: garbade at mip.paed.uni-muenchen.de (Sven Garbade)
Date: Fri, 5 Mar 2004 13:56:06 +0100
Subject: [R] Line length in legend
Message-ID: <20040305135606.294e743d.garbade@psy.uni-muenchen.de>

Hi all,

is it posible to alter the length of the lines in the legend() function?

I think they are a little bit to short, so I changed the default value
of seg.len from 2 to 6. But maybe it would be nice to have an argument,
so users can change the default computed line length as they like. 

Sven



From andy_liaw at merck.com  Fri Mar  5 14:02:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 5 Mar 2004 08:02:55 -0500
Subject: [R] Constraining coefficients
Message-ID: <3A822319EB35174CA3714066D590DCD504AF791D@usrymx25.merck.com>

How about collapsing those two levels into one?  Wouldn't that work?

Cheers,
Andy

> From: Andrew Criswell
> 
> Hello All:
> 
> I have a binomial model with one covariate, x1, treated as a 
> factor with 
> 3 levels. The other covariate is measured x2 <- 1:30. The 
> response, y, 
> is the proportion of successes out of 20 trials.
> 
> glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial)
> 
> Now, I would like to constrain the cofficients on 2 levels of the 
> factor, x1, to be identical and test the difference between 
> these models 
> by a likelihood ratio test.
> 
> How can I get glm() to constrain the coefficients on 2 levels 
> to be the 
> same?
> 
> Thanks,
> ANDREW


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Fri Mar  5 14:14:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 5 Mar 2004 08:14:07 -0500
Subject: [R] locpoly (was: no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF791E@usrymx25.merck.com>

1. Please do make use of the subject line.

2. Please (re-)read the description of the `range.x' argument in ?locpoly:
it's suppose to be a vector of min and max x values.

3. I hope you realize how grossly inefficient this computation is...

Andy


> From: klea lambrou
> 
> 
>    hello R-users.could you please help me on this one?i have 
> 2 vectors x1
>    and y1 and both have the same size.i want to use the 
> locpoly function
>    from the kernsmooth package in order to estimate a regression
>    function.at each estimation i want to leave one 
> observation out,and i
>    want my estimation to be at the observation i left out.i tried :
> 
>     for (j in 1:length(x1))
>    + A<-locpoly(x1[-1],y1[-1],bandwidth=4,gridsize=1,range.x=x1[1])
>    Error in rep.default(0, dimfkap) : invalid number of 
> copies in "rep"
> 
>    i want the gridsize to be equal to one,and i will choose 
> the bandwidth
>    at each time.what am i doing wrong?can you help me?
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From bates at stat.wisc.edu  Fri Mar  5 14:26:04 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 05 Mar 2004 07:26:04 -0600
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572F02211@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572F02211@synequanon01>
Message-ID: <6rvflj8lxv.fsf@bates4.stat.wisc.edu>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> I think one needs an LU decomposition rather than QR. 
> However, I couldn't find anything off the shelf to do 
> an LU, other than learning that determinant() now 
> uses LU instead of QR or SVD, so the code to do it must
> be in there for those that want it.

I was about to write that the version of the Matrix package for
r-devel (to be R-1.9.0) has an LU decomposition but then I checked and
found that, although the underlying C code is there, I haven't written
an accessor function.  In any case the LU decomposition is computed
and stored whenever it is needed, say for calculating a condition
number or for solving a linear system.

> library(Matrix)   # version 0.7-3 for R-1.9.0
> mm = Matrix(rnorm(9), nc = 3)
> mm
           [,1]       [,2]       [,3]
[1,] -0.4186151 -0.3959071 -0.1717103
[2,] -0.5334526 -2.2817253 -1.5398915
[3,] -0.5993042 -1.3396313  0.2062784
> rcond(mm)
[1] 0.04505896
> str(mm)
 list()
 - attr(*, "x")= num [1:9] -0.419 -0.533 -0.599 -0.396 -2.282 ...
 - attr(*, "Dim")= int [1:2] 3 3
 - attr(*, "rcond")= Named num 0.0451
  ..- attr(*, "names")= chr "O"
 - attr(*, "factorization")=List of 1
  ..$ LU: list()
  .. ..- attr(*, "x")= num [1:9] -0.599  0.890  0.699 -1.340 -1.089 ...
  .. ..- attr(*, "Dim")= int [1:2] 3 3
  .. ..- attr(*, "pivot")= int [1:3] 3 2 3
  .. ..- attr(*, "class")= atomic [1:1] LU
  .. .. ..- attr(*, "package")= chr "Matrix"
 - attr(*, "class")= atomic [1:1] geMatrix
  ..- attr(*, "package")= chr "Matrix"
> expand(mm at factorization$LU)
$L
          [,1]       [,2] [,3]
[1,] 1.0000000  0.0000000    0
[2,] 0.8901200  1.0000000    0
[3,] 0.6985019 -0.4955766    1

$U
           [,1]      [,2]       [,3]
[1,] -0.5993042 -1.339631  0.2062784
[2,]  0.0000000 -1.089293 -1.7235040
[3,]  0.0000000  0.000000 -1.1699244

As I understand it the LU decomposition is much more widely used in
numerical linear algebra than is the row-reduced echelon form.



From mkondrin at hppi.troitsk.ru  Sat Mar  6 01:47:56 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 05 Mar 2004 16:47:56 -0800
Subject: [R] Reflecting R-commands in command line
Message-ID: <40491FBC.4080201@hppi.troitsk.ru>

Hello!
Somewhat strange question.
Suppose I have some sort of GUI. Clicking for example button in it I 
expect some R-commands to be executed in R. But I want this commands to 
be logged into R command line and the output printed there too (I want 
to save this later in transcript file). How this can be done?



From chrysopa at insecta.ufv.br  Fri Mar  5 15:17:09 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Fri, 5 Mar 2004 11:17:09 -0300
Subject: [R] Books
Message-ID: <200403051117.09149.chrysopa@insecta.ufv.br>

Hi,

I am seeking some books about spatial statistics and mixed models using R. I 
need books that approach binomial (proportion and binary) and poisson (count) 
data. The books will be used basically for ecologist. Would anybody have 
suggestions?  
  
Thank you  
Ronaldo
-- 
Newton's Little-Known Seventh Law:
	A bird in the hand is safer than one overhead.
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From duanewinner at att.net  Fri Mar  5 15:34:02 2004
From: duanewinner at att.net (Duane Winner)
Date: Fri, 05 Mar 2004 09:34:02 -0500
Subject: [R] unixodbc - postgresql: broken pipes, malloc,
	ksqo and other errors
Message-ID: <1078497242.294.31.camel@localhost>

Hello all. I am posting this on unixodbc-dev and pgsql-odbc as well as
r-help to make sure all of my bases are covered (and because I'm not
sure which one of these is the source of the issue.)

Unixodbc with Postgresql seems to be working, but I am getting some
errors that I don't understand and want to know why I am getting these
errors before I put everything into production.

I am running:
FreeBSD 4.9-RELEASE
unixODBC-2.2.8 (installed from FreeBSD port)
postgresql-7.3.5_1 (installed from FreeBSD port)
R 1.8.1 (installed from R-letter FreeBSD port)
RODBC (installed from CRAN)

Here is what is happening:

When I run "isql":
# isql mydb -v
+---------------------------------------+
| Connected!                            |
|                                       |
| sql-statement                         |
| help [tablename]                      |
| quit                                  |
|                                       |
+---------------------------------------+
SQL> q
Broken pipe

*** Why am I getting "broken pipe"? What does it mean and does it
matter?


Also in R:
> library(RODBC)
> channel <- odbcConnect("mydb")
> weather <-sqlQuery(channel,"select * from weather")
> odbcClose(channel)
R.bin in free(): warning: chunk is already free

*** Why am I getting this warning? This one concerns me the most, as I
need to make sure that it doesn't cause problems on a production system.
I don't understand much about programming, but I have been told that
this is essentially a 'malloc' error. 


Also, in ANY application (isql, OpenOffice, R) that opens a connection
using unixODBC, I see in my postgresql log this message:

ERROR:  'ksqo' is not a valid option name

*** Why am I getting this error, and again, should I be concerned?
Google actually gave me some results on this one, and to the best of my
discerning, it looks as if it is some Microsoft query option that just
isn't supported, in which case, fine. I'm just looking for assurances.


Again, like I said, I am able to run queries and utilize unixodbc and
the postgresql driver as best as I can tell. But I would hate to
implement this into production and find out that there is some critical
problem that will bite me once the production apps start to take some
hits.

Thanks for any info/assistance/guidance you can provide.

-DW



From jfox at mcmaster.ca  Fri Mar  5 15:39:24 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 5 Mar 2004 09:39:24 -0500
Subject: [R] Probit predictions outside (0,1) interval
In-Reply-To: <1078472895.c3b6129carnab@myrealbox.com>
Message-ID: <20040305143921.ZJCF2607.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Arnab,

Several people have already noted that you're getting predicted values on
the wrong scale. Note, as well, that you fit a logit model rather than a
probit model; for a probit model, you need family=binomial(probit), since
the logit link is the canonical link for the binomial family.

John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Arnab mukherji
> Sent: Friday, March 05, 2004 2:48 AM
> To: r-help at stat.math.ethz.ch
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] Probit predictions outside (0,1) interval
> 
> Hi!
> 
> I was trying to implement a probit model on a dichotomous 
> outcome variable and found that the predictions were outside 
> the (0,1) interval that one should get. I later tried it with 
> some simulated data with a similar result. 
> 
> Here is a toy program I wrote and I cant figure why I should 
> be getting such odd predictions.
> 
> x1<-rnorm(1000)
> x2<-rnorm(1000)
> x3<-rnorm(1000)
> x4<-rnorm(1000)
> x5<-rnorm(1000)
> x6<-rnorm(1000)
> e1<-rnorm(1000)/3
> e2<-rnorm(1000)/3
> e3<-rnorm(1000)/3
> y<-1-(1-pnorm(-2+0.33*x1+0.66*x2+1*x3+e1)*1-(pnorm(1+1.5*x4-0.
> 25*x5+e2)*pnorm(1+0.2*x6+e3)))
> y <- y>runif(1000)
> dat<-data.frame(y = y, x1 = x1, x2 = x2, x3 = x3) g<-glm(y~., 
> data = dat, family = binomial)
> summary(g)
> yhat<-predict(g, dat)
> 
> 
> Call:
> glm(formula = y ~ ., family = binomial, data = dat)
> 
> Deviance Residuals: 
>     Min       1Q   Median       3Q      Max  
> -1.8383  -1.3519   0.7638   0.9249   1.3698  
> 
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)    
> (Intercept)  0.71749    0.06901  10.397  < 2e-16 ***
> x1           0.10211    0.07057   1.447  0.14791    
> x2           0.21068    0.07177   2.936  0.00333 ** 
> x3           0.35162    0.07070   4.974 6.57e-07 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> (Dispersion parameter for binomial family taken to be 1)
> 
>     Null deviance: 1275.3  on 999  degrees of freedom 
> Residual deviance: 1239.4  on 996  degrees of freedom
> AIC: 1247.4
> 
> Number of Fisher Scoring iterations: 4
> 
> > yhat<-predict(g, dat)
> > 
> > range(yhat)
> [1] -0.4416826  2.0056527
> > range(y)
> [1] 0 1
> 
> Any advice would be really helpful.
>



From ripley at stats.ox.ac.uk  Fri Mar  5 15:40:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 14:40:48 +0000 (GMT)
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
In-Reply-To: <1078491014.4048778700ac0@webmail1.kuleuven.be>
Message-ID: <Pine.LNX.4.44.0403051439570.22483-100000@gannet.stats>

On Fri, 5 Mar 2004, Jan Verbesselt wrote:

> The na.omit.ts() method fails when the time series contains internal
> NA's. How can these automatically be removed?

It is impossible, as you have been told recently.  You cannot have a 
regular time series with gaps.

> > spectrum(ts.mNDII, na.action=na.omit)
> Error in na.omit.ts(as.ts(x)) : time series contains internal NAs
> 
> How can the na.action be activated correctly?
> 
> > acf(ts.Lin, type=c("correlation"), na.action=na.omit)
> Error in na.omit.ts(as.ts(x)) : time series contains internal NAs
> 
> ((ts.Lin contains two time series, where one contains internal NAs
> (-->an NA not a the end/beginning of a time serie)))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bxc at steno.dk  Fri Mar  5 15:41:31 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 5 Mar 2004 15:41:31 +0100
Subject: [R] Command Line Expressions
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179DF36@exdkba022.novo.dk>

The natural thing would be to pack this into script, but
if you in windows do:

echo %1 | Rterm --vanilla -q

you run into the problem that everything after the first comma is 
discarded, unless you use:

echo %~1 | Rterm --vanilla -q

in which case you will have to quote any R-commands with commas in them.

Bendix Carstensen

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Friday, March 05, 2004 1:56 PM
> To: 'Pingping Zheng'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Command Line Expressions
> 
> 
> Use echo in a Unix shell and pipe it to R.  At the Windows 
> command prompt, you could try:
> 
> c:\home>echo R.version | Rterm --vanilla -q
> > R.version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
> 
> HTH,
> Andy
> 
> > From: Pingping Zheng
> > 
> > Hi,
> > 
> > Is it possible to run R in command line to evalute R 
> expressions and 
> > return results to stdout, something like
> > 
> >  >R CMD -e "R.version$minor"
> > then you got return
> >  >"8.1"
> > 
> > Or do a simple calculation
> >  >R CMD -e "sin(1.2)"
> >  >0.932039
> > 
> > Thanks.
> > 
> > --
> > Pingping Zheng
> > Department of Mathematics and Statistics
> > Fylde College
> > Lancaster University
> > Lancaster LA1 4YF
> > UK
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Mar  5 15:54:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 14:54:41 +0000 (GMT)
Subject: [R] Reflecting R-commands in command line
In-Reply-To: <40491FBC.4080201@hppi.troitsk.ru>
Message-ID: <Pine.LNX.4.44.0403051452240.22483-100000@gannet.stats>

Are you looking for options(echo=TRUE)?

Otherwise we need to know more about how your GUI is submitting R code.

On Fri, 5 Mar 2004, M.Kondrin wrote:

> Somewhat strange question.
> Suppose I have some sort of GUI. Clicking for example button in it I 
> expect some R-commands to be executed in R. But I want this commands to 
> be logged into R command line and the output printed there too (I want 
> to save this later in transcript file). How this can be done?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jmacdon at med.umich.edu  Fri Mar  5 15:54:40 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Fri, 05 Mar 2004 09:54:40 -0500
Subject: [R] "Statistiques avec R"
Message-ID: <s0484e79.099@med-gwia-01a.med.umich.edu>

You can get a reasonable (if relatively comedic) translation of the page
by doing a google search for 'Statistiques avec R', and then choosing
Translate this page.

Although oddly enough the translator only seems to do each page up to a
point and then it reverts to French...

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Simon Fear" <Simon.Fear at synequanon.com> 03/05/04 04:09AM >>>

> -----Original Message-----
> From: Frank E Harrell Jr [mailto:feh3k at spamcop.net] 
> Sent: 04 March 2004 14:34
> To: r-help
> Subject: Re: [R] "Statistiques avec R"
> 
> 
> This looks really excellent.  I wish I could read more than 
> 50 words of
> French.  An English version would be most welcome.
> 

How great is the demand? I was a translator before I became
a statistician but I think this would be a massive undertaking ...

Maybe Shigeru Mase's point about the Japanese wiki site
holds here too: even without being able to read the text, 
you can still read the R code and look at the pictures ...

Anyone who wants to encourage me to look into the
idea of a translation - or anyone who thinks they could do
it easily and quickly - please email me, not the list.  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential\ and...{{dropped}}



From ligges at statistik.uni-dortmund.de  Fri Mar  5 15:58:15 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 05 Mar 2004 15:58:15 +0100
Subject: [R] Line length in legend
In-Reply-To: <20040305135606.294e743d.garbade@psy.uni-muenchen.de>
References: <20040305135606.294e743d.garbade@psy.uni-muenchen.de>
Message-ID: <40489587.1070300@statistik.uni-dortmund.de>

Sven Garbade wrote:

> Hi all,
> 
> is it posible to alter the length of the lines in the legend() function?
> 
> I think they are a little bit to short, so I changed the default value
> of seg.len from 2 to 6. But maybe it would be nice to have an argument,
> so users can change the default computed line length as they like. 
> 
> Sven

See ?legend and its argument "text.width".

Uwe Ligges



From spe2 at cornell.edu  Fri Mar  5 15:57:09 2004
From: spe2 at cornell.edu (Stephen Ellner)
Date: Fri, 05 Mar 2004 09:57:09 -0500
Subject: [R] Lyapunov exponent code for time series
Message-ID: <5.2.1.1.2.20040305094103.00ae3008@postoffice6.mail.cornell.edu>

The lyapunov exponent part of Funfits is semi-available,
but only for Windows. Doug Nychka and I, who wrote that part 
of Funfits, started trying to make it a CRANworthy package but 
never got it done. I've just linked it to my web page 
(www.eeb.cornell.edu/Ellner); follow the Software link on 
that page and get LENNS.zip. 

Steve

Angel Lopez wrote: 

>"Frozen" package Funfits was able to. Although its succesor package is 
>Fields, it has not lyapunov exponent calculations.
>You can get funfits from :
>http://www.cgd.ucar.edu/stats/Software/Funfits/index.shtml
>I haven't got the statistical or programming knowledge to revise that 
>code, if you are able to I would be keen to know of any updates.
>Cheers,
>
>Angel

> Xiaozhe (Catherine) Wang wrote:
>
>> Dear all, Has anyone worked on coding for calculating Lyapunov 
>> Exponent for a time
>> series data? or any package is available for computing Lyapunov?
>>
>> Please advice and many thanks in advance.
>>
>> Catherine X Wang
>

Stephen P. Ellner (spe2 at cornell.edu)
Department of Ecology and Evolutionary Biology
Corson Hall, Cornell University, Ithaca NY 14853-2701
Phone (607) 254-4221    FAX (607) 255-8088



From jfox at mcmaster.ca  Fri Mar  5 15:59:18 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 5 Mar 2004 09:59:18 -0500
Subject: [R]  row-echelon form (was no subject)
In-Reply-To: <6rvflj8lxv.fsf@bates4.stat.wisc.edu>
Message-ID: <20040305145915.ZSZZ2607.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Doug and Simon,

The U matrix in the LU decomposition is upper triangular, but (as I
understand it) it's not in row-echelon because the leading entries aren't 1
and the columns with leading entries aren't fully swept.

I'm certainly not making any claims, other than pedagogical, for the REF. In
fact, the function that I sent originated as a demonstration for a class. By
the way, I think that setting the tolerance in the function to
Machine$double.eps (which is what I did) is too small: Gabor Grothendieck
sent me an example in which my function failed with the default tolerance.

Regards,
 John

> -----Original Message-----
> From: Douglas Bates [mailto:bates at bates4.stat.wisc.edu] On 
> Behalf Of Douglas Bates
> Sent: Friday, March 05, 2004 8:26 AM
> To: Simon Fear
> Cc: John Fox; Spencer Graves; r-help at stat.math.ethz.ch
> Subject: Re: [R] row-echelon form (was no subject)
> 
> "Simon Fear" <Simon.Fear at synequanon.com> writes:
> 
> > I think one needs an LU decomposition rather than QR. 
> > However, I couldn't find anything off the shelf to do an LU, other 
> > than learning that determinant() now uses LU instead of QR 
> or SVD, so 
> > the code to do it must be in there for those that want it.
> 
> I was about to write that the version of the Matrix package 
> for r-devel (to be R-1.9.0) has an LU decomposition but then 
> I checked and found that, although the underlying C code is 
> there, I haven't written an accessor function.  In any case 
> the LU decomposition is computed and stored whenever it is 
> needed, say for calculating a condition number or for solving 
> a linear system.
> 
> > library(Matrix)   # version 0.7-3 for R-1.9.0
> > mm = Matrix(rnorm(9), nc = 3)
> > mm
>            [,1]       [,2]       [,3]
> [1,] -0.4186151 -0.3959071 -0.1717103
> [2,] -0.5334526 -2.2817253 -1.5398915
> [3,] -0.5993042 -1.3396313  0.2062784
> > rcond(mm)
> [1] 0.04505896
> > str(mm)
>  list()
>  - attr(*, "x")= num [1:9] -0.419 -0.533 -0.599 -0.396 -2.282 ...
>  - attr(*, "Dim")= int [1:2] 3 3
>  - attr(*, "rcond")= Named num 0.0451
>   ..- attr(*, "names")= chr "O"
>  - attr(*, "factorization")=List of 1
>   ..$ LU: list()
>   .. ..- attr(*, "x")= num [1:9] -0.599  0.890  0.699 -1.340 
> -1.089 ...
>   .. ..- attr(*, "Dim")= int [1:2] 3 3
>   .. ..- attr(*, "pivot")= int [1:3] 3 2 3
>   .. ..- attr(*, "class")= atomic [1:1] LU
>   .. .. ..- attr(*, "package")= chr "Matrix"
>  - attr(*, "class")= atomic [1:1] geMatrix
>   ..- attr(*, "package")= chr "Matrix"
> > expand(mm at factorization$LU)
> $L
>           [,1]       [,2] [,3]
> [1,] 1.0000000  0.0000000    0
> [2,] 0.8901200  1.0000000    0
> [3,] 0.6985019 -0.4955766    1
> 
> $U
>            [,1]      [,2]       [,3]
> [1,] -0.5993042 -1.339631  0.2062784
> [2,]  0.0000000 -1.089293 -1.7235040
> [3,]  0.0000000  0.000000 -1.1699244
> 
> As I understand it the LU decomposition is much more widely 
> used in numerical linear algebra than is the row-reduced 
> echelon form.



From christian.hoffmann at wsl.ch  Fri Mar  5 16:02:23 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Fri, 05 Mar 2004 16:02:23 +0100
Subject: [R] unusual name in .Rd file: documenting an infix function
In-Reply-To: <40486041.20807@lancs.ac.uk>
References: <40486041.20807@lancs.ac.uk>
Message-ID: <4048967F.7080109@wsl.ch>

Hi

I am having difficulties documenting an infix function:

paste.infix.r
"%&%" <- function(x,y) { paste(x,y,sep="") }

paste.infix.Rd
\name{pasteInfix}

  # I wanted to write \name{"\%\&\%"} or some variation of it, but &, % 
not allowed in LaTeX, see Guide.

\alias{pasteInfix}
\title{Paste(infix)}
\description{
   Paste as infix
}
\usage{
a %&% b   # or \%&\%

# this results in:
* checking Rd files ... OK
* checking for missing documentation entries ... WARNING
Undocumented code objects:
   %&%
All user-level objects in a package should have documentation entries.
See chapter 'Writing R documentation files' in manual 'Writing R
Extensions'.

* checking Rd \usage sections ... WARNING
Objects in \usage without \alias in documentation object 'pasteInfix':
   %&%
}
\arguments{
   \item{a}{character (1-dim)}
   \item{b}{character (1-dim)}
}
\value{
The concatenation of \code{a} and \code{b}, same as \code{ paste(a, b, 
sep="") }
}
\examples{
"I am" \%&\% " hungry" # [1] "I am hungry"
}
\author{Christian W. Hoffmann, \email{christian.hoffmann at wsl.ch}}
\keyword{misc}
\keyword{documentation}


Does there exist a solution to this problem?
Christian
-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From jfox at mcmaster.ca  Fri Mar  5 16:01:41 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 5 Mar 2004 10:01:41 -0500
Subject: [R] Reflecting R-commands in command line
In-Reply-To: <40491FBC.4080201@hppi.troitsk.ru>
Message-ID: <20040305150138.DWSW21160.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear M. Kondrin,

If your GUI is written with the tcltk package, then you might take a look at
the Rcdmr package, which does approximately what you describe. You should be
able to adapt the approach taken there.

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces+jfox=mcmaster.ca at stat.math.ethz.ch 
> [mailto:r-help-bounces+jfox=mcmaster.ca at stat.math.ethz.ch] On 
> Behalf Of M.Kondrin
> Sent: Friday, March 05, 2004 7:48 PM
> To: R-Help
> Subject: [R] Reflecting R-commands in command line
> 
> Hello!
> Somewhat strange question.
> Suppose I have some sort of GUI. Clicking for example button 
> in it I expect some R-commands to be executed in R. But I 
> want this commands to be logged into R command line and the 
> output printed there too (I want to save this later in 
> transcript file). How this can be done?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dadler at uni-goettingen.de  Fri Mar  5 10:01:28 2004
From: dadler at uni-goettingen.de (Daniel Adler)
Date: Fri, 5 Mar 2004 10:01:28 +0100
Subject: [R] [R-pkgs] rgl v0.64-10 released
Message-ID: <004e01c40290$728a50f0$0501a8c0@crush>

ANNOUNCEMENT
rgl - 3d visualization device system for R using OpenGL

DESCRIPTION
The rgl package is a visualization device system for R, using 
OpenGL as the rendering backend. An rgl device at its core 
is a real-time 3D engine written in C++. It provides an 
interactive viewpoint navigation facility (mouse + wheel support)
and an R programming interface.
  
VERSION
v0.64-10
  
CHANGELOG 
- MacOS X 'Panther' G5 fix for OpenGL library loading in .first.lib
- removed lpng and zlib from source tree 
- support for automatic downloading of zlib and lpng on win32 
- added demo directory with several examples using demo(rgl)
- CRAN R check fixes (v0.64-9 to v0.64-10)
    
KNOWN BUGS
- rgl.close() and rgl.quit() might crash on X11 platforms 

HOMEPAGE
http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler

AUTHOR
Daniel Adler <dadler at uni-goettingen.de>
Department of Statistics and Econometrics
University of Goettingen, Germany

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From ru68y7s at myrealbox.com  Fri Mar  5 16:25:53 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Fri, 05 Mar 2004 08:25:53 -0700
Subject: [R] question on integrate function & request for consultant
Message-ID: <1078500353.c3b00fbcru68y7s@myrealbox.com>

Dear R helpers,

i am using the ecdf(emp. cumulative distribution function,and am getting the following error on daily returns of stock data (about 2000 data points). I am not sure what this means, I would expect the result of the code to be 1  (the integral from -infinity to +infinity  of the emp. cumulative distribution). Also when I try to integrate a smaller subset(from say 0 to 1, i get an incorrect result).

does anyone have any suggestions on a possible  error in my formula?


>integrate(ecdf(ret),-Inf,Inf)
Error in integrate(ecdf(ret), -Inf, Inf) : 
        the integral is probably divergent


Also I am looking to hire a statistical consultant for a few hours to help with a project that I am working on.

Please contact me via email at ru68y7s at myrealbox.com

Thank you ,

Sri Viswanath



From feh3k at spamcop.net  Fri Mar  5 13:52:51 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 5 Mar 2004 07:52:51 -0500
Subject: [R] "Statistiques avec R"
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572F02210@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572F02210@synequanon01>
Message-ID: <20040305075251.2b490dae.feh3k@spamcop.net>

On Fri, 5 Mar 2004 09:09:19 -0000
"Simon Fear" <Simon.Fear at synequanon.com> wrote:

> 
> > -----Original Message-----
> > From: Frank E Harrell Jr [mailto:feh3k at spamcop.net]
> > Sent: 04 March 2004 14:34
> > To: r-help
> > Subject: Re: [R] "Statistiques avec R"
> > 
> > 
> > This looks really excellent.  I wish I could read more than 
> > 50 words of
> > French.  An English version would be most welcome.
> > 
> 
> How great is the demand? I was a translator before I became
> a statistician but I think this would be a massive undertaking ...

Simon - This would be a nice resource for people to have in english, but I
would be a poor judge of how much of your time it would be worth.

> 
> Maybe Shigeru Mase's point about the Japanese wiki site
> holds here too: even without being able to read the text, 
> you can still read the R code and look at the pictures ...

I was thinking of using this tutorial for students and I think the
language would really get in their way.

I tried running the site through the google.com translator and the results
were not terrible.  There are some panels of text that are images that are
not translated, and some of the highlighted panels of text expand outside
of the panel upon translation.

Frank

> 
> Anyone who wants to encourage me to look into the
> idea of a translation - or anyone who thinks they could do
> it easily and quickly - please email me, not the list.  
>  
> Simon Fear 
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From N.L.Pace at m.cc.utah.edu  Fri Mar  5 17:02:50 2004
From: N.L.Pace at m.cc.utah.edu (Nathan Leon Pace, MD, MStat)
Date: Fri, 05 Mar 2004 09:02:50 -0700
Subject: [R] Fwd:
Message-ID: <8DD498E4-6EBE-11D8-8BD5-000393B3E9D0@utah.edu>



> Hello,
>
> I'm running R 1.8.1 on a Mac G4 with Dual 800 MHz CPU, 1.5 GB RAM, and 
> sufficient disk space.
>
> I have loaded the boot package version 1.2-15.
>
> I defined a function to pass to the statistic argument which estimates 
> a vector (length = 6) of parameter estimates; none are variances.
>
> I defined a function to pass to the ran.gen argument for parametric 
> resampling.
>
> I created a boot object with R = 10000; the boot object appears to 
> have the appropriate components of a parametric bootstrap object.
>
> When I pass the boot object to the boot.ci function I can get CIs of 
> types = norm, basic, and perc on each parameter estimate.
>
> However, bca CIs are not returned and there is the following error 
> message:
>
> Error in empinf(boot.out, index = index, t = t.o, ...) :
> 	Influence values cannot be found from  a parametric bootstrap
>
> To knowledge I should be able to get bca CIs on a parametric bootstrap 
> object.
>
> Any suggestions?
>
> Nathan
>
>
> Nathan Leon Pace, MD, MStat	Work:n.l.pace at utah.edu
> Department of Anesthesiology	Home:nlpaces at comcast.net
> University of Utah			Work:801.581.6393
> Salt Lake City, Utah			    Home:801.467.2925
> 					Fax:801.581.4367										Cell:801.558.3987



From pallier at lscp.ehess.fr  Fri Mar  5 18:30:29 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Fri, 05 Mar 2004 18:30:29 +0100
Subject: [R] Testing significance in a design with unequal but proportional
	sample sizes
In-Reply-To: <Pine.LNX.4.44.0403050756580.15022-100000@gannet.stats>
References: <Pine.LNX.4.44.0403050756580.15022-100000@gannet.stats>
Message-ID: <4048B935.9050003@lscp.ehess.fr>



Prof Brian Ripley wrote:

>On Fri, 5 Mar 2004, pallier wrote:
>
>...
>
>  
>
>>Actually, the different types of main effects defined above just 
>>correspond to different
>>contrasts on the cell means. So if there is an easy solution to compute 
>>arbitrary contrasts
>>on the cell means in a factorial design, this could an approach to this
>>question. (Anyone?)
>>    
>>
>
>There are at least three such ways.  ?contrasts (for the assignment
>function contrasts<-)  and ?C, as well as the contrasts= argument to aov 
>(the function you were discussing ...).
>  
>
Thanks.
I know the existence of 'contrasts' and I read the  section about 
contrasts matrix in your book (MASS 3rd edition), as well as
in the R online documentation, but I probably do not understand them 
well: It still escapes me how to proceed to compute
"arbitrary" contrasts, such as, say:

a1b1 a1b2  a2b1 a2b2
   1       1      -1      -1

a1b1 a1b2  a2b1 a2b2
  .5      .5       -1       0

in a model "x~ a * b"  where a and b are two binary factors.

(the contrasts should be on the cell means, ignoring the sample size of 
subgroups. I know how to compute the size of the contrasts from the 
table of means returned by tapply, but I whould also need the associated 
MSE).

Sorry if the solution is obvious.

Christophe Pallier



From Brian.Beckage at uvm.edu  Fri Mar  5 17:35:45 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Fri, 5 Mar 2004 11:35:45 -0500
Subject: [R] as.POSIXct problem
Message-ID: <p06020404bc6e55dd3921@[132.198.177.56]>

Hi all,

I'm having difficulty converting a 'dates' object to a POSIXct object:

testDATES<-c(35947,35971,36004,36008,36053,36066)

testDATES<-chron(dates=testDATES, format = c(dates = "m/d/y"), 
origin=c(month = 12, day = 30, year = 1899))

>[1] 06/01/98 06/25/98 07/28/98 08/01/98 09/15/98 09/28/98

>  as.POSIXct(testDATES)
[1] NA NA NA NA NA NA

>format(as.POSIXct(testDATES), "%m/%d/%Y")
[1] NA NA NA NA NA NA

I've looked at the as.POSIXct function, traced the problem to ISOdate 
and then to strptime, e.g.,

x<-paste(year=1899, month=12, day=30, hour=12, min=0, sec=0)
>x
>"1899 12 30 12 0 0"

strptime(x, "%Y %m %d %H %M %S")
[1] NA NA NA NA

However, strptime works fine below.

dates <- c("02/27/92", "02/27/92", "01/14/92",
                 "02/28/92", "02/01/92")
strptime(dates, "%m/%d/%y")
>"02/27/92" "02/27/92" "01/14/92" "02/28/92" "02/01/92"

I've been reading through a whole suite of emails on the R archives 
regarding as.POSIXct problems but I have not found anything that 
solves my particular problem.  I need the dates to be of class 
POSIXct for use in the its package.

I'm using R 1.8.1 on Mac OS 10.3.2.  Any help is greatly appreciated.

Thanks,
Brian


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405



From ripley at stats.ox.ac.uk  Fri Mar  5 17:40:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 16:40:49 +0000 (GMT)
Subject: [R] Testing significance in a design with unequal but proportional
	sample sizes
In-Reply-To: <4048B935.9050003@lscp.ehess.fr>
Message-ID: <Pine.LNX.4.44.0403051638470.32028-100000@gannet.stats>

Those are contrasts treating the interaction as a set of four treatments, 
not an interaction between two factors.  So you would need to set 
contrasts on the factor ab <- a:b (outside a formula), with formula 
a+b+ab.

On Fri, 5 Mar 2004, Christophe Pallier wrote:

> 
> 
> Prof Brian Ripley wrote:
> 
> >On Fri, 5 Mar 2004, pallier wrote:
> >
> >...
> >
> >  
> >
> >>Actually, the different types of main effects defined above just 
> >>correspond to different
> >>contrasts on the cell means. So if there is an easy solution to compute 
> >>arbitrary contrasts
> >>on the cell means in a factorial design, this could an approach to this
> >>question. (Anyone?)
> >>    
> >>
> >
> >There are at least three such ways.  ?contrasts (for the assignment
> >function contrasts<-)  and ?C, as well as the contrasts= argument to aov 
> >(the function you were discussing ...).
> >  
> >
> Thanks.
> I know the existence of 'contrasts' and I read the  section about 
> contrasts matrix in your book (MASS 3rd edition), as well as
> in the R online documentation, but I probably do not understand them 
> well: It still escapes me how to proceed to compute
> "arbitrary" contrasts, such as, say:
> 
> a1b1 a1b2  a2b1 a2b2
>    1       1      -1      -1
> 
> a1b1 a1b2  a2b1 a2b2
>   .5      .5       -1       0
> 
> in a model "x~ a * b"  where a and b are two binary factors.
> 
> (the contrasts should be on the cell means, ignoring the sample size of 
> subgroups. I know how to compute the size of the contrasts from the 
> table of means returned by tapply, but I whould also need the associated 
> MSE).
> 
> Sorry if the solution is obvious.
> 
> Christophe Pallier
> 
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mkondrin at hppi.troitsk.ru  Sat Mar  6 04:50:47 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 05 Mar 2004 19:50:47 -0800
Subject: [R] Reflecting R-commands in command line
In-Reply-To: <20040305150138.DWSW21160.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20040305150138.DWSW21160.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <40494A97.6030404@hppi.troitsk.ru>

Thank you for your replies. I will look into Rcmdr, of course, but not 
in the moment.
I think about making some replacement of R edit() command with GTK toolkit.
I suppose what I would have a script where interface is stored. It is 
either source()-d into R session or loaded as library and called with 
command like gtkedit.start.gui() (so echo will not help because it works 
only for non-interactive sessions). When I would edit some entries in 
data.frame loaded into GUI table, the commands I use to change values of 
data.frame is send into R-session. So if some errors are introduced into 
the frame they can be tracked in R-session transcript and a rollback can 
be done.
Here is an example of what I actually mean. This tkGUI just does an 
assignment Rvar<-Rvalue, with Rvar and Rvalue are taken from entries in 
tk window. The question is how the line evaluated in callback can be 
reflected in command line? Sending the string to stdin() is impossible 
as stdin is opened as read-only.

library(tcltk)
tclVar()->h
tclVar()->z
#set a callback for a button - glue togather string from first and 
second entries  with assignment operator, parse resulting string and 
evaluate it. This string I want to be sent to R-session and somehow eval 
it there.
.Tcl.callback(function(...){eval(parse(text=paste(tclvalue(h),"<-",tclvalue(z))),envir=.GlobalEnv)})->l
# The interface itself - Button and two entries.
.Tcl(paste("toplevel .0 ; grid [button .0.1 -text \"Set var\" -command 
{",l,"}] [entry .0.2 -textvariable ",as.character(h),"] [entry .0.3 
-textvariable ",as.character(z),"]"))



From ripley at stats.ox.ac.uk  Fri Mar  5 18:03:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 17:03:35 +0000 (GMT)
Subject: [R] question on integrate function & request for consultant
In-Reply-To: <1078500353.c3b00fbcru68y7s@myrealbox.com>
Message-ID: <Pine.LNX.4.44.0403051701160.5208-100000@gannet.stats>

You are trying to integrate a discontinous function, and the answer is 
+Inf.  So the message is correct.

An ECDF is one to the right of the largest data point.  I wonder what you 
really want to do.

On Fri, 5 Mar 2004, s viswanath wrote:

> Dear R helpers,
> 
> i am using the ecdf(emp. cumulative distribution function,and am getting the following error on daily returns of stock data (about 2000 data points). I am not sure what this means, I would expect the result of the code to be 1  (the integral from -infinity to +infinity  of the emp. cumulative distribution). Also when I try to integrate a smaller subset(from say 0 to 1, i get an incorrect result).
> 
> does anyone have any suggestions on a possible  error in my formula?
> 
> 
> >integrate(ecdf(ret),-Inf,Inf)
> Error in integrate(ecdf(ret), -Inf, Inf) : 
>         the integral is probably divergent
> 
> 
> Also I am looking to hire a statistical consultant for a few hours to help with a project that I am working on.
> 
> Please contact me via email at ru68y7s at myrealbox.com
> 
> Thank you ,
> 
> Sri Viswanath
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Mar  5 18:12:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 17:12:28 +0000 (GMT)
Subject: [R] unusual name in .Rd file: documenting an infix function
In-Reply-To: <4048967F.7080109@wsl.ch>
Message-ID: <Pine.LNX.4.44.0403051705210.5208-100000@gannet.stats>

LaTeX has nothing to do with it: this is Rd format, and you need to read 
its description.

Your Rd file has no \alias for your operator.  \name is not an alias, and 
\name must not contain % or &.  Look for example at the file outer.Rd:
you should escape % in \alias and \usage (and I suspect & only in \alias).

On Fri, 5 Mar 2004, Christian Hoffmann wrote:

> Hi
> 
> I am having difficulties documenting an infix function:
> 
> paste.infix.r
> "%&%" <- function(x,y) { paste(x,y,sep="") }
> 
> paste.infix.Rd
> \name{pasteInfix}
> 
>   # I wanted to write \name{"\%\&\%"} or some variation of it, but &, % 
> not allowed in LaTeX, see Guide.
> 
> \alias{pasteInfix}
> \title{Paste(infix)}
> \description{
>    Paste as infix
> }
> \usage{
> a %&% b   # or \%&\%
> 
> # this results in:
> * checking Rd files ... OK
> * checking for missing documentation entries ... WARNING
> Undocumented code objects:
>    %&%
> All user-level objects in a package should have documentation entries.
> See chapter 'Writing R documentation files' in manual 'Writing R
> Extensions'.
> 
> * checking Rd \usage sections ... WARNING
> Objects in \usage without \alias in documentation object 'pasteInfix':
>    %&%
> }
> \arguments{
>    \item{a}{character (1-dim)}
>    \item{b}{character (1-dim)}
> }
> \value{
> The concatenation of \code{a} and \code{b}, same as \code{ paste(a, b, 
> sep="") }
> }
> \examples{
> "I am" \%&\% " hungry" # [1] "I am hungry"
> }
> \author{Christian W. Hoffmann, \email{christian.hoffmann at wsl.ch}}
> \keyword{misc}
> \keyword{documentation}
> 
> 
> Does there exist a solution to this problem?
> Christian
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sam.kemp2 at ntlworld.com  Fri Mar  5 18:28:04 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Fri, 05 Mar 2004 17:28:04 +0000
Subject: [R] pausing a script
Message-ID: <4048B8A4.3030405@ntlworld.com>

Hi,

Does anyone know how to pause an executing script and then resume 
executing after a key has been pressed?

What I am looking to do is pause the script when a graph appears so I 
can look at them before resuming the script any further!

Any help appreciated.

Cheers,

Sam.



From sebastiendurand at videotron.ca  Fri Mar  5 18:45:23 2004
From: sebastiendurand at videotron.ca (Sebastien Durand)
Date: Fri, 05 Mar 2004 12:45:23 -0500
Subject: [R] Interpolation or Kriging
Message-ID: <a06020400bc6e6972d1f5@[192.168.2.3]>

Hello!

Here is my problem, I have got a data set that contains values of 
intensity over a regular interval of time.
What I need to do is to resample this data set (or curve) at a 
different interval of time x (1000 predictions will be made to 
resample the curve).
So in order to perform such an operation I am required to do some 
kind of interpolation.
I know that through kriging, the interpolation result passes through 
the original data points.
I would be very incline to use such methods, but since I am not very 
used to work with R, I would like to know which function should I use 
and how.
Moreover I don't know if it is feasable, but I am looking for a fast 
routine since I got a total of 100000 data sets on which I will 
perform the operation just described.

Thanks a lot,
I will anxiously wait for a reply

Sebastien Durand



From amurta at ipimar.pt  Fri Mar  5 18:49:37 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Fri, 5 Mar 2004 17:49:37 +0000
Subject: [R] "Statistiques avec R"
In-Reply-To: <20040305075251.2b490dae.feh3k@spamcop.net>
References: <6C8A8033ABC1E3468048ABC4F13CE572F02210@synequanon01>
	<20040305075251.2b490dae.feh3k@spamcop.net>
Message-ID: <200403051749.37277.amurta@ipimar.pt>

The following note was placed today in the web site: 

" Note to english-speaking readers (2004/03/05): Many people have asked me if 
an English version of this document was on the way. The French version is 
still unfinished, but I hope it will be over before summer. I shall then 
consider translating it into English. "

So, an english version may appear in some months.

Alberto

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From ripley at stats.ox.ac.uk  Fri Mar  5 19:14:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Mar 2004 18:14:48 +0000 (GMT)
Subject: [R] as.POSIXct problem
In-Reply-To: <p06020404bc6e55dd3921@[132.198.177.56]>
Message-ID: <Pine.LNX.4.44.0403051804090.5385-100000@gannet.stats>

This is a problem of your OS: your example works on all of mine.

Can't you change the origin in chron?  If not, you should certainly be 
able to do

as.POSIXct(strptime(as.character(testDATES), "%m/%d/%y"))

Here's what I think you can do most easily:

shift <- julian(1,1,1970, origin=c(month = 12, day = 30, year = 1899))
as.POSIXct(chron(dates=unclass(testDATES) - shift))


On Fri, 5 Mar 2004, Brian Beckage wrote:

> Hi all,
> 
> I'm having difficulty converting a 'dates' object to a POSIXct object:
> 
> testDATES<-c(35947,35971,36004,36008,36053,36066)
> 
> testDATES<-chron(dates=testDATES, format = c(dates = "m/d/y"), 
> origin=c(month = 12, day = 30, year = 1899))
> 
> >[1] 06/01/98 06/25/98 07/28/98 08/01/98 09/15/98 09/28/98
> 
> >  as.POSIXct(testDATES)
> [1] NA NA NA NA NA NA
> 
> >format(as.POSIXct(testDATES), "%m/%d/%Y")
> [1] NA NA NA NA NA NA
> 
> I've looked at the as.POSIXct function, traced the problem to ISOdate 
> and then to strptime, e.g.,
> 
> x<-paste(year=1899, month=12, day=30, hour=12, min=0, sec=0)
> >x
> >"1899 12 30 12 0 0"
> 
> strptime(x, "%Y %m %d %H %M %S")
> [1] NA NA NA NA
> 
> However, strptime works fine below.
> 
> dates <- c("02/27/92", "02/27/92", "01/14/92",
>                  "02/28/92", "02/01/92")
> strptime(dates, "%m/%d/%y")
> >"02/27/92" "02/27/92" "01/14/92" "02/28/92" "02/01/92"
> 
> I've been reading through a whole suite of emails on the R archives 
> regarding as.POSIXct problems but I have not found anything that 
> solves my particular problem.  I need the dates to be of class 
> POSIXct for use in the its package.
> 
> I'm using R 1.8.1 on Mac OS 10.3.2.  Any help is greatly appreciated.
> 
> Thanks,
> Brian
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Fri Mar  5 19:17:49 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri,  5 Mar 2004 13:17:49 -0500 (EST)
Subject: [R] as.POSIXct problem
Message-ID: <20040305181749.04AE83995@mprdmxin.myway.com>



I was unable to reproduce your error on Windows 2000 R 1.8.1:

> testDATES<-c(35947,35971,36004,36008,36053,36066)
> 
> testDATES<-chron(dates=testDATES, format = c(dates = "m/d/y"), 
+ origin=c(month = 12, day = 30, year = 1899))
> 
> as.POSIXct(testDATES)
[1] "1998-05-31 20:00:00 Eastern Daylight Time"
[2] "1998-06-24 20:00:00 Eastern Daylight Time"
[3] "1998-07-27 20:00:00 Eastern Daylight Time"
[4] "1998-07-31 20:00:00 Eastern Daylight Time"
[5] "1998-09-14 20:00:00 Eastern Daylight Time"
[6] "1998-09-27 20:00:00 Eastern Daylight Time"
> 

At any rate, one idea is to create the chron dates 
relative to the default origin like this:

> testDATES<-c(35947,35971,36004,36008,36053,36066)
> testDATES.chron <- chron("12/30/1899") + testDATES
> as.POSIXct(testDATES.chron)
[1] "1998-05-31 20:00:00 Eastern Daylight Time"
[2] "1998-06-24 20:00:00 Eastern Daylight Time"
[3] "1998-07-27 20:00:00 Eastern Daylight Time"
[4] "1998-07-31 20:00:00 Eastern Daylight Time"
[5] "1998-09-14 20:00:00 Eastern Daylight Time"
[6] "1998-09-27 20:00:00 Eastern Daylight Time"

Does that make any difference on your system?

---
Date:   Fri, 5 Mar 2004 11:35:45 -0500 
From:   Brian Beckage <Brian.Beckage at uvm.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] as.POSIXct problem 

 
Hi all,

I'm having difficulty converting a 'dates' object to a POSIXct object:

testDATES<-c(35947,35971,36004,36008,36053,36066)

testDATES<-chron(dates=testDATES, format = c(dates = "m/d/y"), 
origin=c(month = 12, day = 30, year = 1899))

>[1] 06/01/98 06/25/98 07/28/98 08/01/98 09/15/98 09/28/98

> as.POSIXct(testDATES)
[1] NA NA NA NA NA NA

>format(as.POSIXct(testDATES), "%m/%d/%Y")
[1] NA NA NA NA NA NA

I've looked at the as.POSIXct function, traced the problem to ISOdate 
and then to strptime, e.g.,

x<-paste(year=1899, month=12, day=30, hour=12, min=0, sec=0)
>x
>"1899 12 30 12 0 0"

strptime(x, "%Y %m %d %H %M %S")
[1] NA NA NA NA

However, strptime works fine below.

dates <- c("02/27/92", "02/27/92", "01/14/92",
"02/28/92", "02/01/92")
strptime(dates, "%m/%d/%y")
>"02/27/92" "02/27/92" "01/14/92" "02/28/92" "02/01/92"

I've been reading through a whole suite of emails on the R archives 
regarding as.POSIXct problems but I have not found anything that 
solves my particular problem. I need the dates to be of class 
POSIXct for use in the its package.

I'm using R 1.8.1 on Mac OS 10.3.2. Any help is greatly appreciated.

Thanks,
Brian


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405



From Carlisle.Thacker at noaa.gov  Fri Mar  5 19:26:44 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Fri, 05 Mar 2004 13:26:44 -0500
Subject: [R] need help with smooth.spline
References: <Pine.LNX.4.44.0403041841020.10837-100000@gannet.stats>
	<40478BA6.C41F2226@noaa.gov>
	<75641684-6E37-11D8-A68F-000A95A7E3AA@uiuc.edu>
Message-ID: <4048C664.6A38BA61@noaa.gov>

roger koenker wrote:
> 
> If one repeats the experiments in Craven and Wahba, the paper that
> "invented" GCV you find, or at least I found, when I tried to do this
> some years ago, that GCV in about 10%
> of cases fails rather catastrophically, and this is a fairly innocuous
> setting. So one way to interpret Brian's comment would be that maybe
> it is GCV that is failing, and another choice of lambda might do better.

Making spar large enough avoids the outrageous values but gives a poor
approximation to much of the data.

The problems seem to occur in a region where the data indicate the
ocean should be well-mixed, i.e. the curve should be constant.  What's
more, the data look as though they have been edited to remove the
boring repeated values, keeping only the first and last values within
the mixed layer.

When constant t and s values are inserted into the data at integer
values for p within the mixed layer (interpolating by hand),
smooth.spline() with GCV gives a very different result: much smoother
and less faithful to the observations.  Much more reasonable.  It
seems that the problem might be expected when the density of points
changes abruptly at pretty much the same place where the gradient is
changing.  Maybe these cases can be captured and treated separately.

Maybe the total variation penalty methods or some of Andy's
suggestions (polymars(), mars(), locfit(), denoising with wavelets)
will work better.  I'll have to explore some packages.

Thanks to everybody for the gracious help.

Carlisle
-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From phddas at yahoo.com  Fri Mar  5 19:28:09 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 5 Mar 2004 10:28:09 -0800 (PST)
Subject: [R] fractal function
Message-ID: <20040305182809.85302.qmail@web20514.mail.yahoo.com>

Hello
Does any one know what is the complexity scale of fdim
"fractal calculation function", I need to find O(N)
function to do fractal calculation.

Thanks a lot



From rbaer at atsu.edu  Fri Mar  5 19:39:15 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Fri, 5 Mar 2004 12:39:15 -0600
Subject: [R] "Statistiques avec R"
References: <6C8A8033ABC1E3468048ABC4F13CE572F02210@synequanon01>
	<20040305075251.2b490dae.feh3k@spamcop.net>
Message-ID: <004001c402e1$2986be10$2e80010a@BigBaer>

> On Fri, 5 Mar 2004 09:09:19 -0000
> "Simon Fear" <Simon.Fear at synequanon.com> wrote:
> > How great is the demand? I was a translator before I became
> > a statistician but I think this would be a massive undertaking ...

In the version of the site at: http://zoonek2.free.fr/UNIX/48_R/all.html ,
the author makes the following note:

Note to english-speaking readers (2004/03/05): Many people have asked me if
an English version of this document was on the way. The French version is
still unfinished, but I hope it will be over before summer. I shall then
consider translating it into English.



From edarmas at esalq.usp.br  Fri Mar  5 19:39:30 2004
From: edarmas at esalq.usp.br (Eduardo Dutra de Armas)
Date: Fri, 5 Mar 2004 15:39:30 -0300
Subject: [R] time-series
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAsvzEnbBALEy+EUyF5G42lAEAAAAA@esalq.usp.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040305/00313af1/attachment.pl

From andy_liaw at merck.com  Fri Mar  5 19:47:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 5 Mar 2004 13:47:35 -0500
Subject: [R] need help with smooth.spline
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7926@usrymx25.merck.com>

I looked at rqss() in nprq, as Prof. Koenker suggested, but that doesn't
have a predict() method, so I don't know how you'd get the smooth at values
other than the observed...

The criteria (CV, GCV, etc.) could have multiple local minima for some data,
as Prof. Ripley and Prof. Koenker pointed out, so relying on those
`automatic' selection procedure may not be the best thing to do.
Theoretically as spar (lambda) goes to 0, smooth.spline should linearly
interpolate the data.  I guess the routine could run into numerical problems
before that.

Here's yet another thing to try (thanks to Martin for the `lokern' package):

library(lokerns)
par(mfrow=c(2,4))
for (i in 1:4) {
  plot(dat[[i]]$p, dat[[i]]$t);
  lines(lokerns(dat[[i]]$p, dat[[i]]$t, x.out=seq(25,1000,25)))
  plot(dat[[i]]$p, dat[[i]]$s)
  lines(lokerns(dat[[i]]$p, dat[[i]]$s, x.out=seq(25,1000,25)))
}

Best,
Andy

> From: W. C. Thacker
> 
> roger koenker wrote:
> > 
> > If one repeats the experiments in Craven and Wahba, the paper that
> > "invented" GCV you find, or at least I found, when I tried 
> to do this
> > some years ago, that GCV in about 10%
> > of cases fails rather catastrophically, and this is a 
> fairly innocuous
> > setting. So one way to interpret Brian's comment would be that maybe
> > it is GCV that is failing, and another choice of lambda 
> might do better.
> 
> Making spar large enough avoids the outrageous values but gives a poor
> approximation to much of the data.
> 
> The problems seem to occur in a region where the data indicate the
> ocean should be well-mixed, i.e. the curve should be constant.  What's
> more, the data look as though they have been edited to remove the
> boring repeated values, keeping only the first and last values within
> the mixed layer.
> 
> When constant t and s values are inserted into the data at integer
> values for p within the mixed layer (interpolating by hand),
> smooth.spline() with GCV gives a very different result: much smoother
> and less faithful to the observations.  Much more reasonable.  It
> seems that the problem might be expected when the density of points
> changes abruptly at pretty much the same place where the gradient is
> changing.  Maybe these cases can be captured and treated separately.
> 
> Maybe the total variation penalty methods or some of Andy's
> suggestions (polymars(), mars(), locfit(), denoising with wavelets)
> will work better.  I'll have to explore some packages.
> 
> Thanks to everybody for the gracious help.
> 
> Carlisle
> -- 
> 
> William Carlisle Thacker                            
>                                                     
> Atlantic Oceanographic and Meteorological Laboratory
> 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
> Office: (305) 361-4323           Fax: (305) 361-4392
> 
> "Too many have dispensed with generosity 
>      in order to practice charity."     Albert Camus
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From dfs at research.att.com  Fri Mar  5 20:02:57 2004
From: dfs at research.att.com (Deborah Swayne)
Date: Fri, 5 Mar 2004 14:02:57 -0500
Subject: [R] Re: [Rd] Yes you can (was: can you library(MASS) with R 1.9.0?)
In-Reply-To: <85vflksa4l.fsf_-_@servant.blindglobe.net>
References: <858yigv8ia.fsf@servant.blindglobe.net>
	<20040304172040.GA23617@sonny.eddelbuettel.com>
	<85vflksa4l.fsf_-_@servant.blindglobe.net>
Message-ID: <16456.52961.631309.210365@fry.research.att.com>

A.J. Rossini writes:
 > 
 > And it was just me.
 > 
 > Solution (thanks Deepayan, with hints from Dirk):
 > 
 >          ./tools/rsync-recommended  

Not just you -- it helped me out a lot to know that, too.  I suggest
that rsync-recommended be mentioned on web site along with rsync.

Debby



From Carlisle.Thacker at noaa.gov  Fri Mar  5 20:03:23 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Fri, 05 Mar 2004 14:03:23 -0500
Subject: [R] need help with smooth.spline
References: <3A822319EB35174CA3714066D590DCD504AF7926@usrymx25.merck.com>
Message-ID: <4048CEFB.E07FDEE6@noaa.gov>

"Liaw, Andy" wrote:
> 
> I looked at rqss() in nprq, as Prof. Koenker suggested, but that doesn't
> have a predict() method, so I don't know how you'd get the smooth at values
> other than the observed...
> 
> The criteria (CV, GCV, etc.) could have multiple local minima for some data,
> as Prof. Ripley and Prof. Koenker pointed out, so relying on those
> `automatic' selection procedure may not be the best thing to do.
> Theoretically as spar (lambda) goes to 0, smooth.spline should linearly
> interpolate the data.  I guess the routine could run into numerical problems
> before that.
> 
> Here's yet another thing to try (thanks to Martin for the `lokern' package):
> 
> library(lokerns)
> par(mfrow=c(2,4))
> for (i in 1:4) {
>   plot(dat[[i]]$p, dat[[i]]$t);
>   lines(lokerns(dat[[i]]$p, dat[[i]]$t, x.out=seq(25,1000,25)))
>   plot(dat[[i]]$p, dat[[i]]$s)
>   lines(lokerns(dat[[i]]$p, dat[[i]]$s, x.out=seq(25,1000,25)))
> }
> 
> Best,
> Andy

Thanks for still another suggestion.  I'm keeping our system manager
busy loading packages.  Is there a way he can get them all at once?

Also, do you know whether any of these packages come with a manual ---
other than the help pages?

Regards,

Carlisle



From andy_liaw at merck.com  Fri Mar  5 20:08:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 5 Mar 2004 14:08:19 -0500
Subject: [R] need help with smooth.spline
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7927@usrymx25.merck.com>

1.  It may not be the thing to do, but if you really want to, try:
   install.packages(CRAN.packages()[,1], lib.loc=...)

2.  No.  Some packages have vignettes, but most not.  Any additional info
should have been cited in the `Reference' section of the help page.

Cheers,
Andy

> From: thacker at aoml.noaa.gov [mailto:thacker at aoml.noaa.gov] 
> 
> "Liaw, Andy" wrote:
> > 
> > I looked at rqss() in nprq, as Prof. Koenker suggested, but 
> that doesn't
> > have a predict() method, so I don't know how you'd get the 
> smooth at values
> > other than the observed...
> > 
> > The criteria (CV, GCV, etc.) could have multiple local 
> minima for some data,
> > as Prof. Ripley and Prof. Koenker pointed out, so relying on those
> > `automatic' selection procedure may not be the best thing to do.
> > Theoretically as spar (lambda) goes to 0, smooth.spline 
> should linearly
> > interpolate the data.  I guess the routine could run into 
> numerical problems
> > before that.
> > 
> > Here's yet another thing to try (thanks to Martin for the 
> `lokern' package):
> > 
> > library(lokerns)
> > par(mfrow=c(2,4))
> > for (i in 1:4) {
> >   plot(dat[[i]]$p, dat[[i]]$t);
> >   lines(lokerns(dat[[i]]$p, dat[[i]]$t, x.out=seq(25,1000,25)))
> >   plot(dat[[i]]$p, dat[[i]]$s)
> >   lines(lokerns(dat[[i]]$p, dat[[i]]$s, x.out=seq(25,1000,25)))
> > }
> > 
> > Best,
> > Andy
> 
> Thanks for still another suggestion.  I'm keeping our system manager
> busy loading packages.  Is there a way he can get them all at once?
> 
> Also, do you know whether any of these packages come with a manual ---
> other than the help pages?
> 
> Regards,
> 
> Carlisle
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From sudar_80 at neo.tamu.edu  Fri Mar  5 20:11:53 2004
From: sudar_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Fri, 5 Mar 2004 19:11:53 -0000
Subject: [R] van der waerden test
Message-ID: <200403051911.i25JBriV092509@xyzzy-4.tamu.edu>

Hello,

I need some help in performing a Van_der_Waerden normal scores test in R. I 
have two arrays of scores(final on therapy scores from drug and placebo) and 
want to use the normal scores procdeure to test for significance. 
(observations are unequal in number - due to dropouts).

Thanks in advance

Regards

~S



From MSchwartz at medanalytics.com  Fri Mar  5 20:12:04 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 05 Mar 2004 13:12:04 -0600
Subject: [R] pausing a script
In-Reply-To: <4048B8A4.3030405@ntlworld.com>
References: <4048B8A4.3030405@ntlworld.com>
Message-ID: <1078513924.6984.9.camel@localhost.localdomain>

On Fri, 2004-03-05 at 11:28, Samuel Kemp wrote:
> Hi,
> 
> Does anyone know how to pause an executing script and then resume 
> executing after a key has been pressed?
> 
> What I am looking to do is pause the script when a graph appears so I 
> can look at them before resuming the script any further!
> 
> Any help appreciated.
> 
> Cheers,
> 
> Sam.


See ?par specifically for par("ask"), which enables you to do exactly
what you wish.

HTH,

Marc Schwartz



From e.pebesma at geog.uu.nl  Fri Mar  5 20:37:39 2004
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 05 Mar 2004 20:37:39 +0100
Subject: [R] Interpolation or Kriging
Message-ID: <4048D703.6080508@geog.uu.nl>

Sebastien,

Please have a look at the geostatistics packages refered to from

http://sal.agecon.uiuc.edu/csiss/Rgeo/

BTW -- is this page referenced from the R or CRAN home
pages? I couldn't find a link a few days ago.

Most of them may do what you want. I assume they can all
be fooled by providing one-dimensional data having a constant
second coordinate. Package gstat allows explicit specification
of one-dimensional data sets, but has no special provisions
for 1D data in terms of efficiency, compared to 2D data.
--
Edzer



From rossini at blindglobe.net  Fri Mar  5 22:23:57 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 05 Mar 2004 13:23:57 -0800
Subject: [R] Re: [Rd] Yes you can
In-Reply-To: <16456.52961.631309.210365@fry.research.att.com> (Deborah
	Swayne's message of "Fri, 5 Mar 2004 14:02:57 -0500")
References: <858yigv8ia.fsf@servant.blindglobe.net>
	<20040304172040.GA23617@sonny.eddelbuettel.com>
	<85vflksa4l.fsf_-_@servant.blindglobe.net>
	<16456.52961.631309.210365@fry.research.att.com>
Message-ID: <85ad2v56oi.fsf@servant.blindglobe.net>

Deborah Swayne <dfs at research.att.com> writes:

> A.J. Rossini writes:
>  > 
>  > And it was just me.
>  > 
>  > Solution (thanks Deepayan, with hints from Dirk):
>  > 
>  >          ./tools/rsync-recommended  
>
> Not just you -- it helped me out a lot to know that, too.  I suggest
> that rsync-recommended be mentioned on web site along with rsync.

Well, maybe the suggestion should be to run rsync-recommended upon
checkout or rsync-out.  (is there a good name for that behavior?)

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From q.liu at unn.ac.uk  Fri Mar  5 23:57:04 2004
From: q.liu at unn.ac.uk (Qin Liu)
Date: Fri, 5 Mar 2004 22:57:04 -0000
Subject: [R] building a multi-output neural network
Message-ID: <61B1A61B4F4AD711B3450008C791F6FA01BA202F@clearwater.unn.ac.uk>

Hi, there:

Any body has experience in building a multi-output neural network? 

<net3<-nnet(train[,c(1:8,10,11)],train[,c(9,12)],size=5,
maxit=10000,linout=T)
# build a 10-5-2 neural network 

<pred<-predict(net3, test[,c(1:8,10,11)])
#I am trying to predict two columns of test, which means test[,c(9,12)]

I was expecting "pred" be a two-column array. However, it seems the results
were mixed up, i.e. the length of "test" data set is 10, instead of getting
a 10x2 array of predicting results, I got a 20x1 array. How can I
distinguish which of them are predicting results for which column?

Thank you very much for helping me out.

Qin



From mase at is.titech.ac.jp  Sat Mar  6 01:33:18 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Sat, 06 Mar 2004 09:33:18 +0900
Subject: [R] "Statistiques avec R"
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
References: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <40491C4E.9000203@is.titech.ac.jp>

If you want to read in English, try the following machine translation.
Although curious here and there, you can understand the essence.

http://babelfish.altavista.com/babelfish/urltrurl?lp=fr_en&url=http%3A%2F%2Fzoonek2.free.fr%2FUNIX%2F48_R%2Fall.html


PS. To Michael Grant,

dou itasi masite (you are welcome, in Japanese)

Shigeru Mase wrote:

>Dear R users,
>
>I want to share my joy with you. Please see the following
>excellent introduction to R "Statistiques avec R " by
>Vincent Zoonekynd
>
>http://zoonek2.free.fr/UNIX/48_R/all.html
>
>In paticular, you can see a lot of fascinating graphics
>examples of R from which you can get many hints.
>
>Soryy if this is already well-known, but the CRAN search
>did not show nothing with the keyword "Zoonekynd".
>
>Cheers,
>
>Shigeru Mase,
>Dept. Comp. and Math. Sciences
>Tokyo Institute of Technology,
>Meguro, Tokyo, Japan 152-8550
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ndrew at efn.org  Sat Mar  6 02:00:52 2004
From: ndrew at efn.org (Nick Drew)
Date: Fri, 5 Mar 2004 17:00:52 -0800
Subject: [R] Statistical Quality Control
In-Reply-To: <E5F693155C10D61195EA00065B3815B77E96CD@computer.nestechemicals.com>
Message-ID: <MBEPIIKLODKFDMIPFOGACEFACDAA.ndrew@efn.org>

Just the other day I found this site, obtained the package, and tried some
of the examples.


http://www.cmis.csiro.au/S-PLUS/qtoolbox/index.html

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Holton, Blake
Sent: Tuesday, February 24, 2004 11:23 AM
To: R-Help (E-mail)
Subject: [R] Statistical Quality Control


Greetings,

I've been familiarizing myself with the features of R over the past few
days. I'm impressed with the quality and quantity of the features and
packages. One feature that I would be interested in would be a package for
statistical quality control. Does a package for statistical quality control
exist that I've been unable to locate?

If not, is anyone aware of efforts to develop a statistical quality control
package? It has been awhile since I've coded in C, but I would be willing to
contribute.

Regards,

Blake

--------------------------------
Blake Holton
Technical Service Representative
Dynea
475 28th Street
Springfield, OR 97477-5100

Desk: 541.744.7238
Toll Free: 800.547.9525
FAX: 541.744.7249
Cell: 541.954.2696

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From arc at arcriswell.com  Sat Mar  6 04:02:03 2004
From: arc at arcriswell.com (Andrew Criswell)
Date: Sat, 06 Mar 2004 10:02:03 +0700
Subject: [R] Constraining coefficients
References: <Pine.LNX.4.44.0403051250340.21801-100000@gannet.stats>
Message-ID: <40493F2B.1090205@arcriswell.com>


Thank you all for the advice.

So, to confirm: If I have

    data.0 <- data.frame(y, x1, x2)
    levels(data.0$x1) <- c('one', 'two', 'three')  ### three levels

    fm1 <- glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial, data = data.0)

then, what I need to do is merge the last two factors to one:

    data.0 <- data.1
    levels(data.1$x1) <- c('one', 'two', 'two')   ### two levels for factor

    fm2 <- glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial, data = data.1)
    anova(fm2, fm1, test = 'Chisq')

Although I understand it achieves the same result, I am not sure what 
numbers to plug into the contrast. The constrat, as it currently stands, is

contrasts(data.0$x1) <- matrix(c(0, 0,
                                 1, 0,
                                 0, 1), ncol = 2, byrow = T)


Andrew




Prof Brian Ripley wrote:

>On Fri, 5 Mar 2004, Andrew Criswell wrote:
>
>  
>
>>I have a binomial model with one covariate, x1, treated as a factor with 
>>3 levels. The other covariate is measured x2 <- 1:30. The response, y, 
>>is the proportion of successes out of 20 trials.
>>
>>glm(cbind(y, 20 - y) ~ x1 * x2, family = binomial)
>>
>>Now, I would like to constrain the cofficients on 2 levels of the 
>>factor, x1, to be identical and test the difference between these models 
>>by a likelihood ratio test.
>>
>>How can I get glm() to constrain the coefficients on 2 levels to be the 
>>same?
>>    
>>
>
>Merge the levels of the factor: see ?levels.
>
>You could also set up a custom contrasts matrix: either way the natural S
>approach is to reparametrize rather than constrain.
>
>  
>



From Brian.Beckage at uvm.edu  Sat Mar  6 04:32:56 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Fri, 5 Mar 2004 22:32:56 -0500
Subject: [R] as.POSIXct problem
In-Reply-To: <20040305181749.04AE83995@mprdmxin.myway.com>
References: <20040305181749.04AE83995@mprdmxin.myway.com>
Message-ID: <p06020408bc6ef47c3fa4@[132.198.177.56]>

>
>
>At any rate, one idea is to create the chron dates
>relative to the default origin like this:
>
>  > testDATES<-c(35947,35971,36004,36008,36053,36066)
>>  testDATES.chron <- chron("12/30/1899") + testDATES
>  > as.POSIXct(testDATES.chron)
>[1] "1998-05-31 20:00:00 Eastern Daylight Time"
>[2] "1998-06-24 20:00:00 Eastern Daylight Time"
>[3] "1998-07-27 20:00:00 Eastern Daylight Time"
>[4] "1998-07-31 20:00:00 Eastern Daylight Time"
>[5] "1998-09-14 20:00:00 Eastern Daylight Time"
>[6] "1998-09-27 20:00:00 Eastern Daylight Time"
>
>Does that make any difference on your system?


This worked on my machine as well!  I'm not sure why this made a 
difference...but thanks.

Brian


>
>---
>Date:   Fri, 5 Mar 2004 11:35:45 -0500
>From:   Brian Beckage <Brian.Beckage at uvm.edu>
>To:   <r-help at stat.math.ethz.ch>
>Subject:   [R] as.POSIXct problem
>
>
>Hi all,
>
>I'm having difficulty converting a 'dates' object to a POSIXct object:
>
>testDATES<-c(35947,35971,36004,36008,36053,36066)
>
>testDATES<-chron(dates=testDATES, format = c(dates = "m/d/y"),
>origin=c(month = 12, day = 30, year = 1899))
>
>>[1] 06/01/98 06/25/98 07/28/98 08/01/98 09/15/98 09/28/98
>
>>  as.POSIXct(testDATES)
>[1] NA NA NA NA NA NA
>
>>format(as.POSIXct(testDATES), "%m/%d/%Y")
>[1] NA NA NA NA NA NA
>
>I've looked at the as.POSIXct function, traced the problem to ISOdate
>and then to strptime, e.g.,
>
>x<-paste(year=1899, month=12, day=30, hour=12, min=0, sec=0)
>>x
>>"1899 12 30 12 0 0"
>
>strptime(x, "%Y %m %d %H %M %S")
>[1] NA NA NA NA
>
>However, strptime works fine below.
>
>dates <- c("02/27/92", "02/27/92", "01/14/92",
>"02/28/92", "02/01/92")
>strptime(dates, "%m/%d/%y")
>>"02/27/92" "02/27/92" "01/14/92" "02/28/92" "02/01/92"
>
>I've been reading through a whole suite of emails on the R archives
>regarding as.POSIXct problems but I have not found anything that
>solves my particular problem. I need the dates to be of class
>POSIXct for use in the its package.
>
>I'm using R 1.8.1 on Mac OS 10.3.2. Any help is greatly appreciated.
>
>Thanks,
>Brian
>
>
>--
>*********************************************************************
>Brian Beckage
>Department of Botany
>University of Vermont
>Marsh Life Science Building
>Burlington, VT 05405
>
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From Brian.Beckage at uvm.edu  Sat Mar  6 04:59:38 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Fri, 5 Mar 2004 22:59:38 -0500
Subject: [R] Summary: as.POSIXct
Message-ID: <p0602040bbc6ef96a6760@[132.198.177.56]>

Suggestions from both Prof Brian Ripley and Gabor Grothendiec solved 
my problem.

 From Prof Ripley:

>This is a problem of your OS: your example works on all of mine.
>
>Can't you change the origin in chron?  If not, you should certainly be
>able to do
>
>as.POSIXct(strptime(as.character(testDATES), "%m/%d/%y"))

This still resulted in NA's on my system.

>
>Here's what I think you can do most easily:
>
>shift <- julian(1,1,1970, origin=c(month = 12, day = 30, year = 1899))
>as.POSIXct(chron(dates=unclass(testDATES) - shift))

But this worked.

 From Gabor Grothendiec:

>At any rate, one idea is to create the chron dates
>relative to the default origin like this:
>
>  > testDATES<-c(35947,35971,36004,36008,36053,36066)
>>  testDATES.chron <- chron("12/30/1899") + testDATES
>  > as.POSIXct(testDATES.chron)
>[1] "1998-05-31 20:00:00 Eastern Daylight Time"
>[2] "1998-06-24 20:00:00 Eastern Daylight Time"
>[3] "1998-07-27 20:00:00 Eastern Daylight Time"
>[4] "1998-07-31 20:00:00 Eastern Daylight Time"
>[5] "1998-09-14 20:00:00 Eastern Daylight Time"
>[6] "1998-09-27 20:00:00 Eastern Daylight Time"
>
>Does that make any difference on your system?


This worked on my machine as well.  I'm not sure why these solutions 
avoided the previous NA's, but I guess it had to do with my use of 
the 'origin' argument within chron on my particular os...

Thanks again,
Brian





Thanks,
Brian




-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From phddas at yahoo.com  Sat Mar  6 06:46:56 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 5 Mar 2004 21:46:56 -0800 (PST)
Subject: [R] (no subject)
Message-ID: <20040306054656.57940.qmail@web20512.mail.yahoo.com>

Hello group.
I am trying to install fdim on my win2000 os. this
command is giving error and I feel I am not using it
correctly, could some one show me why. 

install.packages(fdim, 
"C:\Program Files\R\rw1081\library" , 
CRAN = "http://cran.r-project.org", 
"internal", 
available = NULL, 
destdir = "C:\Program Files\R\rw1081\library",
installWithVers = FALSE)

thanks



From vograno at evafunds.com  Sat Mar  6 07:26:35 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 5 Mar 2004 22:26:35 -0800
Subject: [R] .Call: is  new attribute of protected object auto-protected
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3A8D@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040305/d40ca534/attachment.pl

From ggrothendieck at myway.com  Sat Mar  6 07:44:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat,  6 Mar 2004 01:44:21 -0500 (EST)
Subject: [R] as.POSIXct problem
Message-ID: <20040306064421.AC5D639F8@mprdmxin.myway.com>



Regarding your question about why it worked, your example of
strptime not returning correct results on 100+ year old dates on
your machine, suggests POSIXt might have problems, in general
on your machine, with 100+ year old dates.

By using chron's default origin, everything handed to POSIXt 
is a recent date whereas with respect to the original origin, 
POSIXt had to decode time periods of 100+ years.

Date:   Fri, 5 Mar 2004 22:32:56 -0500 
From:   Brian Beckage <Brian.Beckage at uvm.edu>
To:   <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch> 
Subject:   RE: [R] as.POSIXct problem 
 
>
>
>At any rate, one idea is to create the chron dates
>relative to the default origin like this:
>
> > testDATES<-c(35947,35971,36004,36008,36053,36066)
>> testDATES.chron <- chron("12/30/1899") + testDATES
> > as.POSIXct(testDATES.chron)
>[1] "1998-05-31 20:00:00 Eastern Daylight Time"
>[2] "1998-06-24 20:00:00 Eastern Daylight Time"
>[3] "1998-07-27 20:00:00 Eastern Daylight Time"
>[4] "1998-07-31 20:00:00 Eastern Daylight Time"
>[5] "1998-09-14 20:00:00 Eastern Daylight Time"
>[6] "1998-09-27 20:00:00 Eastern Daylight Time"
>
>Does that make any difference on your system?


This worked on my machine as well! I'm not sure why this made a 
difference...but thanks.

Brian


>
>---
>Date: Fri, 5 Mar 2004 11:35:45 -0500
>From: Brian Beckage <Brian.Beckage at uvm.edu>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] as.POSIXct problem
>
>
>Hi all,
>
>I'm having difficulty converting a 'dates' object to a POSIXct object:
>
>testDATES<-c(35947,35971,36004,36008,36053,36066)
>
>testDATES<-chron(dates=testDATES, format = c(dates = "m/d/y"),
>origin=c(month = 12, day = 30, year = 1899))
>
>>[1] 06/01/98 06/25/98 07/28/98 08/01/98 09/15/98 09/28/98
>
>> as.POSIXct(testDATES)
>[1] NA NA NA NA NA NA
>
>>format(as.POSIXct(testDATES), "%m/%d/%Y")
>[1] NA NA NA NA NA NA
>
>I've looked at the as.POSIXct function, traced the problem to ISOdate
>and then to strptime, e.g.,
>
>x<-paste(year=1899, month=12, day=30, hour=12, min=0, sec=0)
>>x
>>"1899 12 30 12 0 0"
>
>strptime(x, "%Y %m %d %H %M %S")
>[1] NA NA NA NA
>
>However, strptime works fine below.
>
>dates <- c("02/27/92", "02/27/92", "01/14/92",
>"02/28/92", "02/01/92")
>strptime(dates, "%m/%d/%y")
>>"02/27/92" "02/27/92" "01/14/92" "02/28/92" "02/01/92"
>
>I've been reading through a whole suite of emails on the R archives
>regarding as.POSIXct problems but I have not found anything that
>solves my particular problem. I need the dates to be of class
>POSIXct for use in the its package.
>
>I'm using R 1.8.1 on Mac OS 10.3.2. Any help is greatly appreciated.
>
>Thanks,
>Brian
>
>
>--
>*********************************************************************
>Brian Beckage
>Department of Botany
>University of Vermont
>Marsh Life Science Building
>Burlington, VT 05405



From ripley at stats.ox.ac.uk  Sat Mar  6 08:45:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Mar 2004 07:45:43 +0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <20040306054656.57940.qmail@web20512.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403060740420.6353-100000@gannet.stats>

On Fri, 5 Mar 2004, Fred J. wrote:

> Hello group.
> I am trying to install fdim on my win2000 os. this
> command is giving error and I feel I am not using it

At least four errors, unfortunately.

> correctly, could some one show me why. 
> 
> install.packages(fdim,

  pkgs: character vector of the short names of packages ....
        ^^^^^^^^^^^^^^^^
 
> "C:\Program Files\R\rw1081\library" , 

You need to double backslashes: see the rw-FAQ.  You also need to use 
proper quotes not strange Microsoft `smart quotes', as in the next one.

> CRAN = "http://cran.r-project.org", 
> "internal", 
> available = NULL, 
> destdir = "C:\Program Files\R\rw1081\library",

That's wrong: you do not want the package zip files there.

> installWithVers = FALSE)

Why not use the defaults?

install.packages("fdim", .Library)

or use the Packages menu.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Mar  6 09:02:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Mar 2004 08:02:05 +0000 (GMT)
Subject: [R] .Call: is  new attribute of protected object auto-protected
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3A8D@phost015.intermedia.net>
Message-ID: <Pine.LNX.4.44.0403060748070.6353-100000@gannet.stats>

One again, please read the source code to find out.  You are using the
Rdefines.h back-compatibility macros that very few of us use and so almost
all of your audience has no idea exactly what they do (nor AFAIK is that
documented anywhere).  We would have to read the source code for you, and
that is unreasonable.

Since the names<- function does this in R, this should be easy for you to
ascertain.  Note that most of these replacement functions are not quite
the same as setting an attribute, and indeed setting some attributes
(including "names") have special-purpose code.  So your example is
potentially not an example of your subject line.

On Fri, 5 Mar 2004, Vadim Ogranovich wrote:

> I have an SEXP obj in a C function called via .Call(). The obj is
> protected (in fact it is an argument to .Call and therefore
> automatically protected). If I set an attribute of obj does the
> attribute become protected too? Here is an example
>  
> SEXP foo(SEXP obj) {
>     SET_NAMES(obj, NEW_CHARACTER(3));  /* are names protected or not? */
> ...
> }

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From renaud.lancelot at cirad.fr  Sat Mar  6 10:46:38 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sat, 06 Mar 2004 12:46:38 +0300
Subject: [R] problem with install.packages and  update.packages
Message-ID: <40499DFE.9090108@cirad.fr>

Dear all,

I am working with MS Windows XP Pro (latest update) and a pre-compiled 
version of R:

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R

I meet a strange problem with install.packages and update.packages 
(called from the menu), that did not occur until today:

######

 > local({a <- CRAN.packages()
+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
         cannot open URL 
`http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
In addition: Warning message:
InternetOpenUrl failed: `'


 > update.packages()
trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
         cannot open URL 
`http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
In addition: Warning message:
InternetOpenUrl failed: `'

######

I still can access http://www.r-project.org/ and any of its section 
(including download) with my browser:

Mozilla 1.6
Mozilla/5.0 (Windows; U; Windows NT 5.1; fr-FR; rv:1.6) Gecko/20040113

The only thing I recently changed on my computer was to update Mozilla 
(from 1.5 to 1.6).

Any hint will be highly appreciated.

Best regards,

Renaud
-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 04 824 55 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From ripley at stats.ox.ac.uk  Sat Mar  6 11:04:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Mar 2004 10:04:52 +0000 (GMT)
Subject: [R] problem with install.packages and  update.packages
In-Reply-To: <40499DFE.9090108@cirad.fr>
Message-ID: <Pine.LNX.4.44.0403060957240.6528-100000@gannet.stats>

Well, perhaps the site (which is not www.r-project.org) was unavailable or
inaccessible when you tried.  Internet caches can make sites appear
available when they are not, and the download processes do try to avoid
cached copies.

You seem to have --internet2 as your command-line flag (from the error 
message), so you do need to check you can access the exact URL *from 
Internet Explorer*.  (BTW, that is a crucial piece of information that you 
did not supply.)  If you use Mozilla, why do you have --internet2?

Note too that if R has not been changed, this is not a problem with R but 
with your own computer, and we are not going to be able to debug something 
you changed on your own machine, are we?


On Sat, 6 Mar 2004, Renaud Lancelot wrote:

> Dear all,
> 
> I am working with MS Windows XP Pro (latest update) and a pre-compiled 
> version of R:
> 
>  > version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
> 
> I meet a strange problem with install.packages and update.packages 
> (called from the menu), that did not occur until today:
> 
> ######
> 
>  > local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
> Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
>          cannot open URL 
> `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
> In addition: Warning message:
> InternetOpenUrl failed: `'
> 
> 
>  > update.packages()
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
> Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
>          cannot open URL 
> `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
> In addition: Warning message:
> InternetOpenUrl failed: `'
> 
> ######
> 
> I still can access http://www.r-project.org/ and any of its section 
> (including download) with my browser:
> 
> Mozilla 1.6
> Mozilla/5.0 (Windows; U; Windows NT 5.1; fr-FR; rv:1.6) Gecko/20040113
> 
> The only thing I recently changed on my computer was to update Mozilla 
> (from 1.5 to 1.6).
> 
> Any hint will be highly appreciated.
> 
> Best regards,
> 
> Renaud
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s0129600 at sms.ed.ac.uk  Sat Mar  6 13:52:52 2004
From: s0129600 at sms.ed.ac.uk (O Tosas Auguet)
Date: Sat,  6 Mar 2004 12:52:52 +0000
Subject: [R] GlmmPQL with binomial errors
Message-ID: <1078577572.4049c9a442bac@sms.ed.ac.uk>



Hi all!

I hope somebody can help me solve some doubts which must be very basic, 
but I haven't been able to solve by myself.

The first one, is how to assess for overdispersion in GlmmPQL when fitting 
binomial or poisson errors. The second one is whether GlmmPQL can compare 
models with different fixed effects.

The third doubt, regards the way I should arrange my data in a GlmmPQL with 
binomial errors. In glm, I am supposed to create cbind vector joining 
the "number of successes" and the "total-the number of successes". Should I 
proceed the same way for GlmmPQL of can I use a single column which, intead of 
containing the numbers, simply contains 0 or 1?. The reason for this question, 
is that I am trying to fit a variance components analysis with a single random 
effect and no fixed effects. The only way I know to test for the significance 
of the single level of random effects is by comparing the model with a glm 
without fixed effects and do a ChiSquare test. So, should the data of both 
models be arranged the same way? or is it possible to compare the model with 
random effects and response "0,1" whith that of a glm without fixed effects 
where the response is arranged as cbind(successes,total-successes)? My concern 
of using cbind in GlmmPQL, is that lose replication of the variable of 
interest, and I have realized that when fitting the random variable as a fixed 
effect in a simple glm, I end up with 0 residual degrees of freedom.



Thanks in advance for your help,

Olga



From mase at is.titech.ac.jp  Sat Mar  6 13:59:54 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Sat, 06 Mar 2004 21:59:54 +0900
Subject: [R] "Statistiques avec R"
In-Reply-To: <404714B6.8040909@is.titech.ac.jp>
References: <404714B6.8040909@is.titech.ac.jp>
Message-ID: <4049CB4A.2030706@is.titech.ac.jp>

Dear R users,

Forgive me. This is an off-line topic.

Probably you may be curious about the mysterious author
of "Statistiques avec R" as well as me.  He seems a mathematician.
I found one more extraordinary work of him.

http://tex.loria.fr/prod-graph/zoonekynd/metapost/metapost.html

He made Metapost (a kind of Metafont software which produces
ps outputs) a statistical graphics software. Look and have a fun!

Cheers

Shigeru Mase

Shigeru Mase wrote:

>Dear R users,
>
>I want to share my joy with you. Please see the following
>excellent introduction to R "Statistiques avec R " by
>Vincent Zoonekynd
>
>http://zoonek2.free.fr/UNIX/48_R/all.html
>
>In paticular, you can see a lot of fascinating graphics
>examples of R from which you can get many hints.
>
>Soryy if this is already well-known, but the CRAN search
>did not show nothing with the keyword "Zoonekynd".
>
>Cheers,
>
>Shigeru Mase,
>Dept. Comp. and Math. Sciences
>Tokyo Institute of Technology,
>Meguro, Tokyo, Japan 152-8550
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From renaud.lancelot at cirad.fr  Sat Mar  6 14:05:02 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sat, 06 Mar 2004 16:05:02 +0300
Subject: [R] problem with install.packages and  update.packages
In-Reply-To: <Pine.LNX.4.44.0403060957240.6528-100000@gannet.stats>
References: <Pine.LNX.4.44.0403060957240.6528-100000@gannet.stats>
Message-ID: <4049CC7E.5050907@cirad.fr>

Prof Brian Ripley wrote:

> Well, perhaps the site (which is not www.r-project.org) was unavailable or
> inaccessible when you tried.

Happily it works again, now, but I don't know why...

This morning, I browsed (with IE or Mozilla) 
http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES when R was 
not able to find it. I tried several times (and was able to download 
contributed packages with my browser) and the same problem occurred each 
time.

>  Internet caches can make sites appear
> available when they are not, and the download processes do try to avoid
> cached copies.
> You seem to have --internet2 as your command-line flag (from the error 
> message), so you do need to check you can access the exact URL *from 
> Internet Explorer*.  (BTW, that is a crucial piece of information that you 
> did not supply.)  If you use Mozilla, why do you have --internet2?

Because I also use this machine in my office. Our network administrator 
wants us to use IE and he configured it to work with the local network 
and the proxy server. When I am home (where I use a dial-up modem), I 
prefer to use Mozilla.

> Note too that if R has not been changed, this is not a problem with R but 
> with your own computer, and we are not going to be able to debug something 
> you changed on your own machine, are we?

Sure, and I did not mean that ! I was just puzzled and worried by this 
unusual behaviour.

Thanks for your reply.

Best regards,

Renaud

> 
> 
> On Sat, 6 Mar 2004, Renaud Lancelot wrote:
> 
> 
>>Dear all,
>>
>>I am working with MS Windows XP Pro (latest update) and a pre-compiled 
>>version of R:
>>
>> > version
>>          _
>>platform i386-pc-mingw32
>>arch     i386
>>os       mingw32
>>system   i386, mingw32
>>status
>>major    1
>>minor    8.1
>>year     2003
>>month    11
>>day      21
>>language R
>>
>>I meet a strange problem with install.packages and update.packages 
>>(called from the menu), that did not occur until today:
>>
>>######
>>
>> > local({a <- CRAN.packages()
>>+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
>>trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
>>Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
>>         cannot open URL 
>>`http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
>>In addition: Warning message:
>>InternetOpenUrl failed: `'
>>
>>
>> > update.packages()
>>trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
>>Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  :
>>         cannot open URL 
>>`http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
>>In addition: Warning message:
>>InternetOpenUrl failed: `'
>>
>>######
>>
>>I still can access http://www.r-project.org/ and any of its section 
>>(including download) with my browser:
>>
>>Mozilla 1.6
>>Mozilla/5.0 (Windows; U; Windows NT 5.1; fr-FR; rv:1.6) Gecko/20040113
>>
>>The only thing I recently changed on my computer was to update Mozilla 
>>(from 1.5 to 1.6).
>>
>>Any hint will be highly appreciated.
>>
>>Best regards,
>>
>>Renaud
>>
> 
> 


-- 
Dr Renaud Lancelot, v?t?rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 04 824 55 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From bates at stat.wisc.edu  Sat Mar  6 14:28:00 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 06 Mar 2004 07:28:00 -0600
Subject: [R] .Call: is  new attribute of protected object auto-protected
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3A8D@phost015.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A50C3A8D@phost015.intermedia.net>
Message-ID: <6ru112nlzz.fsf@bates4.stat.wisc.edu>

"Vadim Ogranovich" <vograno at evafunds.com> writes:

> I have an SEXP obj in a C function called via .Call(). The obj is
> protected (in fact it is an argument to .Call and therefore
> automatically protected). If I set an attribute of obj does the
> attribute become protected too? Here is an example
>  
> SEXP foo(SEXP obj) {
>     SET_NAMES(obj, NEW_CHARACTER(3));  /* are names protected or not? */
> ...
> }

Yes.

I use that property extensively when I am writing code for the .Call
interface.  Take a look at the C code in the Matrix package for
r-devel (available in the src/contrib/1.9.0 directory on CRAN) and you
will see that in most functions that return an SEXP I create and
PROTECT the value to be returned then do all the other allocations
within SET_SLOT or setAttrib or SET_VECTOR_ELT.  This results in a lot of
what may be slightly strange looking code like 

   SEXP val = PROTECT(NEW_OBJECT(MAKE_CLASS("myclass")));
   int n;
   double *realslot;

   ...
   SET_SLOT(val, install("slotname"), allocVector(REALSXP, n));
   realslot = REAL(GET_SLOT(val, install("slotname")));
   ...   /* do something to the elements of the realslot vector here */
   UNPROTECT(1);
   return val;

It looks a bit strange because I am getting the slot immediately after
setting it.  (There are ways around this involving saving some
temporary results but I find them more confusing to read.)  The
important point is that I set the slot directly on allocating so that
the value of the allocation is protected and I don't have to PROTECT
and UNPROTECT it explicitly.  I find that having to keep track of how
many objects I have protected is a source of errors for me so I try to
ensure that I only PROTECT one object and do all allocations as
components of that object.  Usually I can manage to count up to 1
without making mistakes :-)



From ripley at stats.ox.ac.uk  Sat Mar  6 15:04:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Mar 2004 14:04:30 +0000 (GMT)
Subject: [R] GlmmPQL with binomial errors
In-Reply-To: <1078577572.4049c9a442bac@sms.ed.ac.uk>
Message-ID: <Pine.LNX.4.44.0403061403010.10990-100000@gannet.stats>

glmmPQL is part of MASS, and that is support software for a book.  Do look 
in the book and its references ....

On Sat, 6 Mar 2004, O Tosas Auguet wrote:

> I hope somebody can help me solve some doubts which must be very basic, 
> but I haven't been able to solve by myself.
> 
> The first one, is how to assess for overdispersion in GlmmPQL when fitting 
> binomial or poisson errors. The second one is whether GlmmPQL can compare 
> models with different fixed effects.
> 
> The third doubt, regards the way I should arrange my data in a GlmmPQL with 
> binomial errors. In glm, I am supposed to create cbind vector joining 
> the "number of successes" and the "total-the number of successes". Should I 
> proceed the same way for GlmmPQL of can I use a single column which, intead of 
> containing the numbers, simply contains 0 or 1?. The reason for this question, 
> is that I am trying to fit a variance components analysis with a single random 
> effect and no fixed effects. The only way I know to test for the significance 
> of the single level of random effects is by comparing the model with a glm 
> without fixed effects and do a ChiSquare test. So, should the data of both 
> models be arranged the same way? or is it possible to compare the model with 
> random effects and response "0,1" whith that of a glm without fixed effects 
> where the response is arranged as cbind(successes,total-successes)? My concern 
> of using cbind in GlmmPQL, is that lose replication of the variable of 
> interest, and I have realized that when fitting the random variable as a fixed 
> effect in a simple glm, I end up with 0 residual degrees of freedom.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sudar_80 at neo.tamu.edu  Sat Mar  6 20:33:04 2004
From: sudar_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Sat, 6 Mar 2004 19:33:04 -0000
Subject: [R] normal scores test
Message-ID: <200403061933.i26JX4St011251@xyzzy-3.tamu.edu>


Hello,

I need help in performing a Van_der_Waerden normal scores test in R. I 
have two arrays of scores(final on therapy scores from drug and placebo) and 
want to use the normal scores procdeure to test for significance. 
(observations are unequal in number - due to dropouts). Could you please help 
me out with the coding or let me know if there is a package that can be used 
(for example, suppdist etc.)

Thanks in advance

Regards

~S



From phddas at yahoo.com  Sat Mar  6 22:30:25 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 6 Mar 2004 13:30:25 -0800 (PST)
Subject: [R] Installing packages
In-Reply-To: <01C40394.D245A800.acward@uqconnect.net.au>
Message-ID: <20040306213026.52804.qmail@web20513.mail.yahoo.com>

> local({a <- CRAN.packages()
+ install.packages(select.list(a[,1],,TRUE),
.libPaths()[1], available=a)})
trying URL
`http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length
16079 bytes
opened URL
downloaded 15Kb

trying URL
`http://cran.r-project.org/bin/windows/contrib/1.8/fdim_1.0-2.zip'
Content type `application/zip' length 71062 bytes
opened URL
downloaded 69Kb

Error in int.unzip(zipname, NULL, dest) : destination
does not exist

--- "Andrew C. Ward" <acward at uqconnect.net.au> wrote:
> You won't have to type anything if you install CRAN
> packages through the menu system (Packages->Install
> package(s) from CRAN..). Failing that, please tell
> us more about what the error was.
> 
> Regards,
> 
> Andrew C. Ward
> 
> 56 Azalea Crescent
> Fitzgibbon Qld 4018
> acward at uqconnect.net.au
> 
> 
> On Saturday, March 06, 2004 3:47 PM, Fred J. 
> [SMTP:phddas at yahoo.com] wrote:



From krcabrer at epm.net.co  Sat Mar  6 23:02:07 2004
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Sat, 06 Mar 2004 17:02:07 -0500
Subject: [Fwd: Re: [R] normal scores test]
Message-ID: <404A4A5F.6010203@epm.net.co>


-- 
Kenneth Roy Cabrera Torres
Celular +57 (315) 504 9339

-------------- next part --------------
An embedded message was scrubbed...
From: Kenneth Cabrera <krcabrer at epm.net.co>
Subject: Re: [R] normal scores test
Date: Sat, 06 Mar 2004 17:00:01 -0500
Size: 1437
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040306/2c17eef0/Rnormalscorestest.mht

From p.dalgaard at biostat.ku.dk  Sat Mar  6 23:08:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Mar 2004 23:08:53 +0100
Subject: [R] normal scores test
In-Reply-To: <200403061933.i26JX4St011251@xyzzy-3.tamu.edu>
References: <200403061933.i26JX4St011251@xyzzy-3.tamu.edu>
Message-ID: <x23c8l1vd6.fsf@biostat.ku.dk>

"Padmanabhan, Sudharsha" <sudar_80 at neo.tamu.edu> writes:

> Hello,
> 
> I need help in performing a Van_der_Waerden normal scores test in R. I 
> have two arrays of scores(final on therapy scores from drug and placebo) and 
> want to use the normal scores procdeure to test for significance. 
> (observations are unequal in number - due to dropouts). Could you please help 
> me out with the coding or let me know if there is a package that can be used 
> (for example, suppdist etc.)
> 
> Thanks in advance
> 
> Regards
> 
> ~S

I don't think we have the test pre-cooked (someone correct me if I'm
wrong) although I vaguely recall some e-conversations with Kurt Hornik
ages ago about doing a generic score test function for his ctest
package.

There are two things to it: calculating the actual scores and finding
the distribution of the test statistic. 

As far as I remember, the definition of the normal scores is that they
are the expected value of the jth order statistic in the normal
distribution. This is pretty close to qnorm(ppoints(n)), but not
exactly. However, the difference is likely to be immaterial. So your
scores would be qnorm(ppoints(n))[order(x)] or, in order to better
cope with ties, you might do 

n <- length(x)
s <- qnorm(rank(x)/(n+1)).

(make sure to remove NAs since rank() will otherwise have surprises
for you...)

The test statistic is 

T <- sum(s[group==1])

Once you have the scores, you need to work out the correlation
structure of s so that you can calculate the mean and variance of the
sum of the k observations in group 1. This is the same basic idea for
all scoring schemes. Keep the set of scores fixed and calculate the
permutation distribution.

The mean is obvious:

m <- mean(s)

and actually, so is the variance, but you need to know that you need
the "finite population variance" which divides by n rather than n-1.

V <- var(s) * (n - 1)/n

Now what about the covariance between two elements of s? This is
actually dead easy once you realize that the sum(s) is a constant and
that all covariances are identical for symmetry reasons: 

n * V + (n^2 - n) * C = 0  ==>  C = - 1/(n-1) * V

so, we get that T has mean k*m and variance k * (1 - k/(n-1)) * V

So, look up T in the normal distribution with those parameters and
you're basically done.

Note also that you can rather easily simulate from the permutation
distribution of T, e.g.:

replicate(10000, sum(s[sample(n,k)]))



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From henric.nilsson at statisticon.se  Sun Mar  7 00:58:02 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Sun, 07 Mar 2004 00:58:02 +0100
Subject: [R] GlmmPQL with binomial errors
In-Reply-To: <1078577572.4049c9a442bac@sms.ed.ac.uk>
References: <1078577572.4049c9a442bac@sms.ed.ac.uk>
Message-ID: <6.0.3.0.0.20040306212823.04bb0210@10.0.10.66>

At 13:52 2004-03-06, you wrote:

>The reason for this question, is that I am trying to fit a variance 
>components analysis with a single random effect and no fixed effects.The 
>only way I know to test for the significance of the single level of random 
>effects is by comparing the model with a glm without fixed effects and do 
>a ChiSquare test.

This may or may not be known to you:

If one model is a special case of the other, i.e. its variance component 
equals zero, then the less complex model falls on the boundary of the 
parameter space relative to the more complex model. Hence the LR test needs 
to be adjusted, and in this simple case the adjusted p-value may be taken 
as half the p-value from the usual one degree of freedom chi-squared test, 
i.e. using a 50-50 mixture of chi-squared distributions with zero and one 
df as the reference.

In other cases, e.g. when simultaneously testing if two or more variance 
components equals zero, the asymptotic distribution isn't generally known. 
You may use a score test (Lin, Biometrika, 1997), but I don't think it's 
implemented in R or any of the add-on packages.

Pinheiro & Bates "Mixed-effects models in S and S-PLUS" p. 82-87 provide a 
good discussion in the context of linear mixed-effects models.

//Henric



From dmurdoch at pair.com  Sun Mar  7 04:49:00 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 06 Mar 2004 22:49:00 -0500
Subject: [R] Installing packages
In-Reply-To: <20040306213026.52804.qmail@web20513.mail.yahoo.com>
References: <01C40394.D245A800.acward@uqconnect.net.au>
	<20040306213026.52804.qmail@web20513.mail.yahoo.com>
Message-ID: <626l409jk3ps14gksfkp4s9d5k4f9aamf0@4ax.com>

On Sat, 6 Mar 2004 13:30:25 -0800 (PST), you wrote:

>Error in int.unzip(zipname, NULL, dest) : destination
>does not exist
>

R tries to build a temporary directory to hold the files, and it
sounds as though it's failing when it does that.  You can see the sort
of name it is trying to use with this command:

> tempfile(,.libPaths()[1])
[1] "F:/R/cvs/r-devel/library\\file27447"

If you don't have permission to create a directory with that name,
then the command will fail.  I'd guess that's your problem.

The way around this is to install the library in a local path, instead
of one where you don't have write access.  You can do this within a
session by calling

.libPaths('c:/newpath')

and on the command line that invoked R by adding the argument
"R_LIBS=c:/newpath".

Notice the use of the forward slash, not a backslash, in the path
name.

Duncan Murdoch



From phddas at yahoo.com  Sun Mar  7 06:04:02 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 6 Mar 2004 21:04:02 -0800 (PST)
Subject: [R] Installing packages
In-Reply-To: <626l409jk3ps14gksfkp4s9d5k4f9aamf0@4ax.com>
Message-ID: <20040307050402.72377.qmail@web20504.mail.yahoo.com>

yes, I loged in as admin. and that fixed the problem,
but when I type ?fdim I don't get the help docs, why?
even though I have all the htmls under 
C:\Program Files\R\rw1081\library\fdim\html

thanks
--- Duncan Murdoch <dmurdoch at pair.com> wrote:
> On Sat, 6 Mar 2004 13:30:25 -0800 (PST), you wrote:
> 
> >Error in int.unzip(zipname, NULL, dest) :
> destination
> >does not exist
> >
> 
> R tries to build a temporary directory to hold the
> files, and it
> sounds as though it's failing when it does that. 
> You can see the sort
> of name it is trying to use with this command:
> 
> > tempfile(,.libPaths()[1])
> [1] "F:/R/cvs/r-devel/library\\file27447"
> 
> If you don't have permission to create a directory
> with that name,
> then the command will fail.  I'd guess that's your
> problem.
> 
> The way around this is to install the library in a
> local path, instead
> of one where you don't have write access.  You can
> do this within a
> session by calling
> 
> .libPaths('c:/newpath')
> 
> and on the command line that invoked R by adding the
> argument
> "R_LIBS=c:/newpath".
> 
> Notice the use of the forward slash, not a
> backslash, in the path
> name.
> 
> Duncan Murdoch
> 
>



From ayalahec at msu.edu  Sun Mar  7 06:36:12 2004
From: ayalahec at msu.edu (Hector L. Ayala-del-Rio)
Date: Sun, 07 Mar 2004 00:36:12 -0500
Subject: [R] Installing packages
In-Reply-To: <20040307050402.72377.qmail@web20504.mail.yahoo.com>
References: <626l409jk3ps14gksfkp4s9d5k4f9aamf0@4ax.com>
Message-ID: <5.2.1.1.0.20040307003515.02aa9ec8@mail.msu.edu>

Fred,
   Did you loaded the library before typing "?fdim"
 >
 > ?fdim
Error in help("fdim") : No documentation for `fdim' in specified packages 
and libraries:
   you could try `help.search("fdim")'
 > library(fdim)
 > ?fdim
 >
fdim                  package:fdim                  R Documentation

Calculation of generalizated fractal dimension of Data-Sets.

Description:

      Returns the Fractal Dimension of data-frames.

Usage:

      fdim(X, BaseR=2, Mnmax=TRUE, nMax=9, NumMinP=1, q=0, Alpha=0.2, 
PlotF=FALSE)

Arguments:


Hector



At 12:04 AM 3/7/2004, Fred J. wrote:
>yes, I loged in as admin. and that fixed the problem,
>but when I type ?fdim I don't get the help docs, why?
>even though I have all the htmls under
>C:\Program Files\R\rw1081\library\fdim\html
>
>thanks
>--- Duncan Murdoch <dmurdoch at pair.com> wrote:
> > On Sat, 6 Mar 2004 13:30:25 -0800 (PST), you wrote:
> >
> > >Error in int.unzip(zipname, NULL, dest) :
> > destination
> > >does not exist
> > >
> >
> > R tries to build a temporary directory to hold the
> > files, and it
> > sounds as though it's failing when it does that.
> > You can see the sort
> > of name it is trying to use with this command:
> >
> > > tempfile(,.libPaths()[1])
> > [1] "F:/R/cvs/r-devel/library\\file27447"
> >
> > If you don't have permission to create a directory
> > with that name,
> > then the command will fail.  I'd guess that's your
> > problem.
> >
> > The way around this is to install the library in a
> > local path, instead
> > of one where you don't have write access.  You can
> > do this within a
> > session by calling
> >
> > .libPaths('c:/newpath')
> >
> > and on the command line that invoked R by adding the
> > argument
> > "R_LIBS=c:/newpath".
> >
> > Notice the use of the forward slash, not a
> > backslash, in the path
> > name.
> >
> > Duncan Murdoch
> >
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

********************************************
H?ctor L. Ayala-del-R?o, Ph.D.
Center for Microbial Ecology
Michigan State University
540 Plant & Soil Sciences Building
East Lansing, MI 48824
Tel: 517-353-9021
Fax: 517-353-2917
ayalahec at msu.edu



From alexpegucci at yahoo.com  Sun Mar  7 06:42:04 2004
From: alexpegucci at yahoo.com (alex pegucci)
Date: Sat, 6 Mar 2004 21:42:04 -0800 (PST)
Subject: [R] Using character vectors to call data in the "cbind" command
Message-ID: <20040307054204.21543.qmail@web14421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040306/7b4fb4e9/attachment.pl

From Don.Driscoll at flinders.edu.au  Sun Mar  7 07:19:50 2004
From: Don.Driscoll at flinders.edu.au (Don Driscoll)
Date: Sun, 07 Mar 2004 16:49:50 +1030
Subject: [R] Plot is lost when fitting block as error
Message-ID: <6.0.1.1.1.20040307164258.01b32c58@mail.flinders.edu.au>

G'day,

Plot doesn't seem to work when I specify block as an error term, but does 
if I load block into the model as the first factor (see code below).

Is there some way to make plot work with the fancier model specification?

Also, if anyone knows how to get SEDs for this model I would really like to 
know.


response<-c(rnorm(5,5,2),rnorm(5,12,2),rnorm(5,18,2),rnorm(5,12,2))
factor1<-factor(rep(c(1,2),each=10))
factor2 <- factor(rep(rep(c(1,2),each=5),2))
block<-gl(5,1,20)
model<-aov(response~factor1*factor2+Error(block))
plot(model)


#THE RESULT IS:

Error in plot.window(xlim, ylim, log, asp, ...) :
         need finite xlim values
In addition: Warning messages:
1: no finite arguments to min; returning Inf
2: no finite arguments to max; returning -Inf
3: no finite arguments to min; returning Inf
4: no finite arguments to max; returning -Inf

BUT IF YOU SPECIFY THE MODEL AS

model<-aov(response~block+factor1*factor2)

THEN PLOT(MODEL) WORKS AS EXPECTED.


Don.


Dr Don Driscoll
Lecturer in Biodiversity
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide SA 5001
(08) 8201 2165



From ripley at stats.ox.ac.uk  Sun Mar  7 09:23:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Mar 2004 08:23:53 +0000 (GMT)
Subject: [R] Using character vectors to call data in the "cbind" command
In-Reply-To: <20040307054204.21543.qmail@web14421.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403070815140.23581-100000@gannet.stats>

On Sat, 6 Mar 2004, alex pegucci wrote:

>  I have searched the forum but could not find a thread about the way to
> solve my problem. I am trying to find a way to use a subset of a list of
> variable names I have when I call the "cbind" command to create a data
> matrix, after I have attached a dataset. The nature of my program
> necessitates that I create different data matrices using a subset of
> variable names multiple times. So, when I change the variables for the
> analysis, I need to make 5-6 manipulations to my program and it is
> painful. Is there a way to use a character vector containing variable
> names in the cbind command? What I tried is

> 1.I loaded and attached a data set.
> 2.I have a character vector containing the variables I want to use in
> the analysis. Say
>  name <- c("a1","a2","a3", "a4")

> I have several vectors of this type to use in the analysis, so to
> create a data matrix, I use the cbind command but as expected the
> following does not work:
>
> X <- cbind(1, name)
> cbind requires "," between the variable names so when I do
> X <- cbind(1, paste(",", name, collapse=""))
> it still doesn't work although the printed version of the second portion
> is more or less what I need.
>  
> Is there a way to do this?

Several.  Perhaps the easiest is

eval(parse(text=paste("cbind(1,", paste(name, collapse=","), ")")))

which constructs the command you would type in and submits it.

Another way would be to use do.call(), as in

do.call("cbind", c(1, lapply(name, get)))

although if you want dimnames

tmp <- lapply(name, get); names(tmp) <- name
do.call("cbind", c(1, tmp))

is needed.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Mar  7 09:37:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Mar 2004 08:37:39 +0000 (GMT)
Subject: [R] Plot is lost when fitting block as error
In-Reply-To: <6.0.1.1.1.20040307164258.01b32c58@mail.flinders.edu.au>
Message-ID: <Pine.LNX.4.44.0403070824090.23581-100000@gannet.stats>

You second fit is of class c("aov", "lm"), the first of class "aovlist".
There is no way to plot a series of aov fits in different strata, but you
can plot aspects of the individual fits: there is an example in (MASS4,
p.284).

In your case there is only one stratum of interest and
plot(model[["Within"]]) ought to work, but for some reason plot.lm calls
predict() to get the fitted values.  I am not sure that the lm method for
plot is appropriate for aov fits, though: in particular leverage (and
hence Cook's distance) is not usually of interest.

On Sun, 7 Mar 2004, Don Driscoll wrote:

> G'day,
> 
> Plot doesn't seem to work when I specify block as an error term, but does 
> if I load block into the model as the first factor (see code below).
> 
> Is there some way to make plot work with the fancier model specification?
> 
> Also, if anyone knows how to get SEDs for this model I would really like to 
> know.

SEDs are what?

> response<-c(rnorm(5,5,2),rnorm(5,12,2),rnorm(5,18,2),rnorm(5,12,2))
> factor1<-factor(rep(c(1,2),each=10))
> factor2 <- factor(rep(rep(c(1,2),each=5),2))
> block<-gl(5,1,20)
> model<-aov(response~factor1*factor2+Error(block))
> plot(model)
> 
> 
> #THE RESULT IS:
> 
> Error in plot.window(xlim, ylim, log, asp, ...) :
>          need finite xlim values
> 
> BUT IF YOU SPECIFY THE MODEL AS
> 
> model<-aov(response~block+factor1*factor2)
> 
> THEN PLOT(MODEL) WORKS AS EXPECTED.

Does it?  It is not what I think is sensible.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zh_jinsong at yahoo.com.cn  Sun Mar  7 11:36:34 2004
From: zh_jinsong at yahoo.com.cn (=?gb2312?q?Jinsong=20Zhao?=)
Date: Sun, 7 Mar 2004 18:36:34 +0800 (CST)
Subject: [R] graphic device MetaPost
Message-ID: <20040307103634.86081.qmail@web15406.mail.cnb.yahoo.com>

Hi all,

By default, MetaPost passes all text through TeX. This has the
advantage of allowing essentially any TeX symbols in titles and labels.
It give us, who use the multibyte character in ordinary communication,
much convenience. Gnuplot has fulfilled this function, and it give me a
deep impression for I could use Chinese character in plots with a minor
modification to the MetaPost file.

I hope the R Development Core Team could consider MetaPost as a graphic
device in future R version.

Best wishes,

Jinsong Zhao
_________________________________________________________



From ligges at statistik.uni-dortmund.de  Sun Mar  7 11:42:32 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 07 Mar 2004 11:42:32 +0100
Subject: [R] Installing packages
References: <20040307050402.72377.qmail@web20504.mail.yahoo.com>
Message-ID: <404AFC98.3F3F2FB1@statistik.uni-dortmund.de>

"Fred J." wrote:
> 
> yes, I loged in as admin. and that fixed the problem,
> but when I type ?fdim I don't get the help docs, why?
> even though I have all the htmls under
> C:\Program Files\R\rw1081\library\fdim\html
> 
> thanks

Either you forgot to load the package
  library(fdim)
or your installation is still broken (please check whether you have got
read permissions) ...


BTW: Without having changed the defaults, you will need the files in
...\fdim\help, those in ...\fdim\html will be displayed after setting
options(htmlhelp=TRUE).

Uwe Ligges


> --- Duncan Murdoch <dmurdoch at pair.com> wrote:
> > On Sat, 6 Mar 2004 13:30:25 -0800 (PST), you wrote:
> >
> > >Error in int.unzip(zipname, NULL, dest) :
> > destination
> > >does not exist
> > >
> >
> > R tries to build a temporary directory to hold the
> > files, and it
> > sounds as though it's failing when it does that.
> > You can see the sort
> > of name it is trying to use with this command:
> >
> > > tempfile(,.libPaths()[1])
> > [1] "F:/R/cvs/r-devel/library\\file27447"
> >
> > If you don't have permission to create a directory
> > with that name,
> > then the command will fail.  I'd guess that's your
> > problem.
> >
> > The way around this is to install the library in a
> > local path, instead
> > of one where you don't have write access.  You can
> > do this within a
> > session by calling
> >
> > .libPaths('c:/newpath')
> >
> > and on the command line that invoked R by adding the
> > argument
> > "R_LIBS=c:/newpath".
> >
> > Notice the use of the forward slash, not a
> > backslash, in the path
> > name.
> >
> > Duncan Murdoch
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From pallier at lscp.ehess.fr  Sun Mar  7 13:21:56 2004
From: pallier at lscp.ehess.fr (pallier)
Date: Sun, 07 Mar 2004 13:21:56 +0100
Subject: [R] "Statistiques avec R"
In-Reply-To: <4049CB4A.2030706@is.titech.ac.jp>
References: <404714B6.8040909@is.titech.ac.jp>
	<4049CB4A.2030706@is.titech.ac.jp>
Message-ID: <404B13E4.8010909@lscp.ehess.fr>

Shigeru Mase wrote:

>
> Probably you may be curious about the mysterious author
> of "Statistiques avec R" as well as me.  He seems a mathematician.
> I found one more extraordinary work of him.
>
> http://tex.loria.fr/prod-graph/zoonekynd/metapost/metapost.html
>
> He made Metapost (a kind of Metafont software which produces
> ps outputs) a statistical graphics software. Look and have a fun!


Vincent Zoonekynd is *not* the creator of Metapost. John D. Hobby is 
(cf. http://cm.bell-labs.com/who/hobby/index.html).


Vincent Zoonekynd indeed has a ph.D. in maths from the university of 
Paris 7 (Jussieu). On his homepage (http://zoonek.free.fr/), he says he 
is looking for a job in computer science or bioinformatics (last update: 
october 2003). The job market is very gloomy for young scientists in 
France. I do not know if he's looking for a job in a foreign country, 
but his CV indicates that he speaks English, and has a medium level in 
Japanese.

He wrote the "Statistiques avec R" notes while learning R. I like these 
pages very much too, but they are notes taken by a mathematician 
learning statistics. In some parts, he has not yet fully grasped all the 
concepts (but he indicates this, and he's stressing that this is a work 
in progress). This remark does not detract from the quality and the 
usefulness of his work.

Christophe Pallier
http://www.pallier.org



From ripley at stats.ox.ac.uk  Sun Mar  7 15:42:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Mar 2004 14:42:17 +0000 (GMT)
Subject: [R] graphic device MetaPost
In-Reply-To: <20040307103634.86081.qmail@web15406.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403071430370.24043-100000@gannet.stats>

On Sun, 7 Mar 2004, [gb2312] Jinsong Zhao wrote:

> By default, MetaPost passes all text through TeX. This has the
> advantage of allowing essentially any TeX symbols in titles and labels.
> It give us, who use the multibyte character in ordinary communication,
> much convenience. Gnuplot has fulfilled this function, and it give me a
> deep impression for I could use Chinese character in plots with a minor
> modification to the MetaPost file.

I don't think so: you would still need character metrics for the fonts you 
use to be able to centre them, for example.

> I hope the R Development Core Team could consider MetaPost as a graphic
> device in future R version.

We would welcome your contributing such a device.  Note though that 
there is a public API for graphics devices, and so you could just 
contribute the device to CRAN.

If perchance you meant `I want the R core team to write a metapost device
for me', then you have not grasped how Open Source projects work.

There are some plans for internationalization of R via UTF-8, but this
will be a considerable amount (man months?) of work, and of very little
benefit to any of the core developers.  Volunteers would be welcome.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sun Mar  7 17:41:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun,  7 Mar 2004 11:41:23 -0500 (EST)
Subject: [R] graphic device MetaPost
Message-ID: <20040307164123.5B27C399F@mprdmxin.myway.com>




Users may have insights that the developers may not have and 
those ideas may usefully find their way into R, even if the 
promoter of the idea does not have the capabilities or 
resources to implement it.

Surely, good ideas such as this one should be mentioned.  
Maybe that user will implement it or maybe someone from 
the R core will or maybe some other user will.  Who knows?  
No one will if no one knows about it.

Date:   Sun, 7 Mar 2004 14:42:17 +0000 (GMT) 
From:   Prof Brian Ripley <ripley at stats.ox.ac.uk>
To:   =?gb2312?q?Jinsong=20Zhao?= <zh_jinsong at yahoo.com.cn> 
Cc:   rhelp <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] graphic device MetaPost 

 
On Sun, 7 Mar 2004, [gb2312] Jinsong Zhao wrote:

> By default, MetaPost passes all text through TeX. This has the
> advantage of allowing essentially any TeX symbols in titles and labels.
> It give us, who use the multibyte character in ordinary communication,
> much convenience. Gnuplot has fulfilled this function, and it give me a
> deep impression for I could use Chinese character in plots with a minor
> modification to the MetaPost file.

I don't think so: you would still need character metrics for the fonts you 
use to be able to centre them, for example.

> I hope the R Development Core Team could consider MetaPost as a graphic
> device in future R version.

We would welcome your contributing such a device. Note though that 
there is a public API for graphics devices, and so you could just 
contribute the device to CRAN.

If perchance you meant `I want the R core team to write a metapost device
for me', then you have not grasped how Open Source projects work.

There are some plans for internationalization of R via UTF-8, but this
will be a considerable amount (man months?) of work, and of very little
benefit to any of the core developers. Volunteers would be welcome.

-- 
Brian D. Ripley, ripley at stats.ox.ac.uk
Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
University of Oxford, Tel: +44 1865 272861 (self)
1 South Parks Road, +44 1865 272866 (PA)
Oxford OX1 3TG, UK Fax: +44 1865 272595



From itayf at fhcrc.org  Sun Mar  7 20:12:11 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Sun, 7 Mar 2004 11:12:11 -0800 (PST)
Subject: [R] graphic device MetaPost
In-Reply-To: <20040307164123.5B27C399F@mprdmxin.myway.com>
Message-ID: <Pine.LNX.4.44.0403071054280.866-100000@cezanne.fhcrc.org>


The TeX world offers several comapnion systems to typeset 
graphics; metapost, xy-pic, and PSTricks are the most powerful, 
and have facilities for typesetting data plots. Recently, the pgf 
LaTeX package, that can coop with PDF and PS from the same 
source, was submitted to CTAN; but it is more restricted than the 
others.

>From time-to-time I played with the idea of writing such 
mentioned device. The trouble is, that I feel it is beyond my R 
and TeX skills (for the moment, at least).
If no one picks up this glove -- then I will attempt this 
venture.
If you would like to see such a device(s), and would like to 
contribute, maybe we can team up.

To the original poster: could you use the pictex() device
for your needs?

	Thanks,
	Itay Furman


On Sun, 7 Mar 2004, Gabor Grothendieck wrote:

> 
> Users may have insights that the developers may not have and 
> those ideas may usefully find their way into R, even if the 
> promoter of the idea does not have the capabilities or 
> resources to implement it.
> 
> Surely, good ideas such as this one should be mentioned.  
> Maybe that user will implement it or maybe someone from 
> the R core will or maybe some other user will.  Who knows?  
> No one will if no one knows about it.
> 
> Date:   Sun, 7 Mar 2004 14:42:17 +0000 (GMT) 
> From:   Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To:   =?gb2312?q?Jinsong=20Zhao?= <zh_jinsong at yahoo.com.cn> 
> Cc:   rhelp <r-help at stat.math.ethz.ch> 
> Subject:   Re: [R] graphic device MetaPost 
> 
>  
> On Sun, 7 Mar 2004, [gb2312] Jinsong Zhao wrote:
> 
> > By default, MetaPost passes all text through TeX. This has the
> > advantage of allowing essentially any TeX symbols in titles and labels.
> > It give us, who use the multibyte character in ordinary communication,
> > much convenience. Gnuplot has fulfilled this function, and it give me a
> > deep impression for I could use Chinese character in plots with a minor
> > modification to the MetaPost file.
> 
> I don't think so: you would still need character metrics for the fonts you 
> use to be able to centre them, for example.
> 
> > I hope the R Development Core Team could consider MetaPost as a graphic
> > device in future R version.
> 
> We would welcome your contributing such a device. Note though that 
> there is a public API for graphics devices, and so you could just 
> contribute the device to CRAN.
> 
> If perchance you meant `I want the R core team to write a metapost device
> for me', then you have not grasped how Open Source projects work.
> 
> There are some plans for internationalization of R via UTF-8, but this
> will be a considerable amount (man months?) of work, and of very little
> benefit to any of the core developers. Volunteers would be welcome.
> 
>



From phddas at yahoo.com  Sun Mar  7 20:54:50 2004
From: phddas at yahoo.com (Fred J.)
Date: Sun, 7 Mar 2004 11:54:50 -0800 (PST)
Subject: [R] applying data generating function
Message-ID: <20040307195450.34920.qmail@web20516.mail.yahoo.com>

Hello

Coming from matlab background, I could use some help
on this one.

I need to generate a data set based on this equation
X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
N(0,0,001) random variable
I need say 100 values.

How do I do this? 

Thanks



From gracestat at yahoo.com  Sun Mar  7 21:45:13 2004
From: gracestat at yahoo.com (Grace Conlon)
Date: Sun, 7 Mar 2004 12:45:13 -0800 (PST)
Subject: [R] missing values
Message-ID: <20040307204513.11274.qmail@web21401.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040307/86f3cd5f/attachment.pl

From jens_hainmueller at ksg05.harvard.edu  Mon Mar  8 00:49:16 2004
From: jens_hainmueller at ksg05.harvard.edu (Jens Hainmueller)
Date: Sun, 7 Mar 2004 15:49:16 -0800
Subject: [R] drawing filled countries according to data using map('world')?
Message-ID: <HCECJPLNNGBBJIOJMJJAIEHKCEAA.jens_hainmueller@ksg05.harvard.edu>

Hello,

I am looking for somebody who has experience with the map library (Becker
and Wilks 1993) and might be able to help me with the following problem:

Using the 'world' database I would like to draw filled countries in a world
map so that the filling colors of each country corresponds to the value of a
policy variable X at time t (the goal is to visualize a policy diffusion
pattern over time using different maps for t=1985, 1990, etc.).

In their explanatory note, Becker and Wilks show how to accomplish this with
the 'states' database, for filling US states with color according to the
republican vote in 1900.

> state.names <- unix(?tr "[A-Z]" "[a-z]"?, state.name)
> map.states <- unix(?sed "s/:.*//"?, map(names=T, plot=F))
> state.to.map <- match(map.states, state.names)
> color <- votes.repub[state.to.map, votes.year == 1900] / 100
> map(?state?, fill=T, col=color)
> map(?state?, add=T)

"The first expression changes uppercase to lowercase in the standard S
dataset giving state names,
so that these can be compared with the names returned by map. Next the
complete set of state
polygon names is requested (using map(names=T,plot=F); the default database
is
?state?) and the trailing portions (from the '':?? onwards) are removed so
that we have a list of
the state for which each polygon is a part or the whole. Then we create
state.to.map that
gives the translation from the ordering of the states known to S
(alphabetical) to the ordering
known to the mapping mechanism. By using this vector, as in the next
expression, all the pieces
of a state will be colored the same color. The state.to.map vector is a
useful one to keep
around, for it will work in any context where the ordering of the state data
is as here. Notice that
unless such a vector is being reused, it will usually be the case that there
will be a step like this
one, finding the translation between the ordering for the regions in your
data and the ordering
according to map. In general, the translation will have to be computed each
time the set of
selected polygons changes."

My question then is, how to compute a similar procedure using the 'world'
database. Specifically, how can I access the country names in the 'world'
database to accomplish the translation to the country names in my dataset?
Is there any way to unpack the 'world' database to do the matching in an
external program? And does anybody now of other (more recent) world maps
that I could use?

Thanks very much!

Best,
Jens



From p.dalgaard at biostat.ku.dk  Sun Mar  7 22:04:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Mar 2004 22:04:04 +0100
Subject: [R] missing values
In-Reply-To: <20040307204513.11274.qmail@web21401.mail.yahoo.com>
References: <20040307204513.11274.qmail@web21401.mail.yahoo.com>
Message-ID: <x2ptboz7wb.fsf@biostat.ku.dk>

Grace Conlon <gracestat at yahoo.com> writes:

> How can I deal with missing values in the excel file? 
> I used read.csv to imports data, how ever there are missing values in the csv file. 
> When I use names(), it turns out a error message: " names attribute must be the same length as the vector" 
> What can i do with the missing values?

What were you trying to do with names and what has it got to do with
missing values??

How are the missing values coded in the csv file? If they are empty
fields, read.csv (btw, isn't it easier to export as delimited file and
use read.delim?) should handle them automatically, if not, try using
the na.strings argument.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pcampbell at econ.bbk.ac.uk  Sun Mar  7 22:10:56 2004
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Sun, 7 Mar 2004 21:10:56 -0000
Subject: [R] applying data generating function
In-Reply-To: <20040307195450.34920.qmail@web20516.mail.yahoo.com>
Message-ID: <NGECIFANPOJAGABBAEAPEEFJCLAA.pcampbell@econ.bbk.ac.uk>

It may be possible to do this without a loop but I haven't found a way


###Generate an array of 100 N(0,1) RVs
z<-rnorm(100)
###Build the array to store output
x<-vector(length=100)
###Create initial value
x[1]<-z[1]
###Loop though building series
for(i in 2:100){
	x[i]<-3.8*x[i-1]*(1-x[i-1])+z[i]
}



As I understand it you take an overhead hit when using loops, so they are
best avoided.
HTH
Phineas
http://www.phineas.pwp.blueyonder.co.uk/

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Fred J.
Sent: Sunday, March 07, 2004 7:55 PM
To: r help
Subject: [R] applying data generating function


Hello

Coming from matlab background, I could use some help
on this one.

I need to generate a data set based on this equation
X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
N(0,0,001) random variable
I need say 100 values.

How do I do this?

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pallier at lscp.ehess.fr  Sun Mar  7 23:24:48 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Sun, 07 Mar 2004 23:24:48 +0100
Subject: [R] applying data generating function
In-Reply-To: <20040307195450.34920.qmail@web20516.mail.yahoo.com>
References: <20040307195450.34920.qmail@web20516.mail.yahoo.com>
Message-ID: <404BA130.9060703@lscp.ehess.fr>



Fred J. wrote:

>I need to generate a data set based on this equation
>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>N(0,0,001) random variable
>I need say 100 values.
>
>How do I do this? 
>
>
>  
>

I assume X(t) and x(t) are the same (?).

f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
v=c()
x=.1 # starting point
for (i in 1:100) { x=f(x); v=append(v,x) }

There may be smarter ways...

Christophe Pallier



From spencer.graves at pdf.com  Sun Mar  7 22:32:47 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 07 Mar 2004 13:32:47 -0800
Subject: [R] applying data generating function
In-Reply-To: <20040307195450.34920.qmail@web20516.mail.yahoo.com>
References: <20040307195450.34920.qmail@web20516.mail.yahoo.com>
Message-ID: <404B94FF.9040405@pdf.com>

      I'd use a for loop: 

set.seed(123)
e <- 0.001*rnorm(100)
x <- rep(0, 100)
for(i in 2:100)
    x[i] <- (3.8*x[i-1]*(1-x[i-1])+e[i])
plot(x, type="l")
plot(x[-100], x[-1])

      "R" is great for standard vector and matrix operations, but 
recursions are not so easy.  

      hope this helps.  spencer graves

Fred J. wrote:

>Hello
>
>Coming from matlab background, I could use some help
>on this one.
>
>I need to generate a data set based on this equation
>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>N(0,0,001) random variable
>I need say 100 values.
>
>How do I do this? 
>
>Thanks
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From rbaer at atsu.edu  Sun Mar  7 22:50:01 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Sun, 7 Mar 2004 15:50:01 -0600
Subject: [R] Frequency ploygon help
Message-ID: <000f01c4048e$2498fe50$6401a8c0@meadow>

I can't figure out how to get the x-axis to contain the category lables for
my frequency polygon.  I'm also not sure if there is a more elegant
approach.  Any insights on the labels?

I tried this:
#generate some pseudo data

x=c(sort(sample(1:1500,5)),sort(sample(1:1500,3),dec=T))

# assign names to the vector

names(x)=c("0-13","14-19","50-99","100-149","150-199","200-249","250-299","3
00+")



#Plot a frequency polygon

# This displays as inicies, not labels:

plot(x,type="b",col="red")



#This almost works (adds labels) but produces errors and scrambles y-axis

plot(x,type="b",col="red",labels=labels(x))



I looked at ?axis but could not see a solution there.  Thanks in advance.



Rob



From p.dalgaard at biostat.ku.dk  Sun Mar  7 23:04:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Mar 2004 23:04:23 +0100
Subject: [R] applying data generating function
In-Reply-To: <404BA130.9060703@lscp.ehess.fr>
References: <20040307195450.34920.qmail@web20516.mail.yahoo.com>
	<404BA130.9060703@lscp.ehess.fr>
Message-ID: <x2hdx0z53s.fsf@biostat.ku.dk>

Christophe Pallier <pallier at lscp.ehess.fr> writes:

> Fred J. wrote:
> 
> >I need to generate a data set based on this equation
> >X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
> >N(0,0,001) random variable
> >I need say 100 values.
> >
> > How do I do this?
> 
> I assume X(t) and x(t) are the same (?).
> 
> f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
> v=c()
> x=.1 # starting point
> for (i in 1:100) { x=f(x); v=append(v,x) }
> 
> There may be smarter ways...

Yes, but the only really crucial one is to avoid the inefficient append  by
preallocating the v: 

v <- numeric(100)
x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }

apart from that you can use implicit loops:

x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))

or

z <- .1 ; v <- replicate(100, z <<- f(z))

(You cannot use x there because of a variable capture issue which is a
bit of a bug. I intend to fix it for 1.9.0.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ray at mcs.vuw.ac.nz  Sun Mar  7 23:46:51 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 8 Mar 2004 11:46:51 +1300 (NZDT)
Subject: [R] drawing filled countries according to data using map('world')?
Message-ID: <200403072246.i27Mkp5i027455@tahi.mcs.vuw.ac.nz>

> My question then is, how to compute a similar procedure using the 'world'
> database. Specifically, how can I access the country names in the 'world'
> database to accomplish the translation to the country names in my dataset?
> Is there any way to unpack the 'world' database to do the matching in an
> external program? And does anybody now of other (more recent) world maps
> that I could use?
> 
The short answer is that the file world.N in .../library/maps/mapdata/
contains the mapping from polygon numbers to names.

I think the only other thing you need to know is that the colours are
allocated sequentially in increasing numerical order of polygon number.

Ray Brownrigg



From gracestat at yahoo.com  Mon Mar  8 00:43:48 2004
From: gracestat at yahoo.com (Grace Conlon)
Date: Sun, 7 Mar 2004 15:43:48 -0800 (PST)
Subject: [R] Excel files
Message-ID: <20040307234348.53628.qmail@web21401.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040307/cfd9883c/attachment.pl

From jens_hainmueller at ksg05.harvard.edu  Mon Mar  8 05:18:16 2004
From: jens_hainmueller at ksg05.harvard.edu (Jens Hainmueller)
Date: Sun, 7 Mar 2004 20:18:16 -0800
Subject: [R] drawing filled countries according to data using map('world')?
	- follow up
Message-ID: <HCECJPLNNGBBJIOJMJJAMEHNCEAA.jens_hainmueller@ksg05.harvard.edu>

Hello,

this is a follow up on my previous inquiry regarding the use of the map
library (Becker and Wilks 1993).

Using the 'world' database I would like to draw filled countries in a world
map so that the filling colors of each country corresponds to the value of a
policy variable "fix.float" at a specific "year" (the goal is to visualize a
policy diffusion pattern over time using different maps for year=1985, 1990,
etc.).

In my dataset [Test] I have created a vector 'map.name' that contains
country names that I have made identical to the country names in file
world.N in .../library/maps/mapdata/.

> Test[1:10,]
   region fix.float wbcode  name year dv dv.lag map.name polygon
1     lac        NA    ABW Aruba 1973 NA     NA    Aruba    1936
2     lac        NA    ABW Aruba 1974 NA     NA    Aruba    1936
3     lac        NA    ABW Aruba 1975 NA     NA    Aruba    1936
4     lac        NA    ABW Aruba 1976 NA     NA    Aruba    1936
5     lac        NA    ABW Aruba 1977 NA     NA    Aruba    1936
6     lac        NA    ABW Aruba 1978 NA     NA    Aruba    1936
7     lac        NA    ABW Aruba 1979 NA     NA    Aruba    1936
8     lac        NA    ABW Aruba 1980 NA     NA    Aruba    1936
9     lac        NA    ABW Aruba 1981 NA     NA    Aruba    1936
10    lac        NA    ABW Aruba 1982 NA     NA    Aruba    1936

Now I would like to translate the country names in the 'world' database to
the country names in my dataset (following Becker and Wilks 1993). For some
reason, the translation does not work.

> map.country<-  map(database = "world", names=T,plot=F)
> state.to.map <- match(map.name,map.country)
> color <- dv[state.to.map, year == 1980]
Error in dv[state.to.map, year == 1980] : incorrect number of dimensions

> color <- dv[state.to.map, year == 1980]/100
Error in dv[state.to.map, year == 1980] : incorrect number of dimensions

What am I doing wrong? (there are a few values missing in "fix.float")

Thanks for your help!

Best
Jens Hainmueller



From rbaer at atsu.edu  Mon Mar  8 02:43:49 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Sun, 7 Mar 2004 19:43:49 -0600
Subject: [R] Excel files
References: <20040307234348.53628.qmail@web21401.mail.yahoo.com>
Message-ID: <002a01c404ae$ce1b9030$2e80010a@BigBaer>

This might get you started on reading and plotting the dates and times for
levels of a gender factor:
# I assume the following Excel data
date time Sex Value
1 5/5/1999 10:00:00 male 14.987685
2 7/3/1998 20:00:00 female 17.667527
3 8/6/1999 3:23:00 male 3.428401
4 12/7/1997 6:36:00 male 14.977503
5 3/4/2004 9:49:00 male 6.704703
6 11/15/1999 23:04:00 female 5.536046
7 10/16/1998 6:05:00 male 12.153291
8 2/5/1999 3:06:00 female 12.121168
9 3/3/1997 3:07:00 male 4.641686
10 8/6/2004 7:08:00 male 6.649273
11 8/7/1999 22:10:00 male 11.158278
12 7/4/1998 21:11:00 female 8.098113
13 7/6/1999 5:15:00 male 6.220476
14 9/1/1997 6:16:00 female 1.939658
15 3/15/2004 7:05:00 male 2.032969
16 5/16/1999 2:23:00 male 16.436000
#Simulate Excel read
x<-read.table("clipboard")
x

# Do your plotting
library(chron)
timedate=chron(as.character(x$date),as.character(x$time),format=c(dates="m/d
/y",times="h:m:s"))
plot(x$Value~timedate,pch=as.numeric(x$Sex))

1. How can I access the data of certain column? I mean how can I refer it in
R?
Notice how the read in data is in a dataframe.  To access a given column of
the data frame use a $ plus the column name.
2. How can I make the two column character values(the time variable I
mentioned) to the X ases values?
see the construction of timedate
3. If I have three variables, one is y axes, one is x axes,  the third one
is a logic var, how can I make the logic varaible on the plot also?
I don't know precisely what you want but see how I show gender using the
pch= tag to get different plot symbols.  The details depend what you want
here.


HTH,

Rob Baer
----- Original Message ----- 
From: "Grace Conlon" <gracestat at yahoo.com>
To: <R-help at stat.math.ethz.ch>
Sent: Sunday, March 07, 2004 5:43 PM
Subject: [R] Excel files


Hello,

I was trying to import data from an Excel file. After I imported the data, I
was trying to make a scatter plot.

The X axes variable is a time variable, which occupies two columns, one is
date, another one is time. Example 21-Apr-03, 4:10 PM.   My qestion is:


4. I have a column of number values, however I want to creat a new column,
the value of each cell is the average of every 10 values of the first
column. How can I do it in R?

I tried to figure it out in the help section, however, it spent me a lot of
times, I still can not get the way out, please help. THank you!



---------------------------------


[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Mon Mar  8 02:56:46 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 07 Mar 2004 17:56:46 -0800
Subject: [R] applying data generating function
In-Reply-To: <x2hdx0z53s.fsf@biostat.ku.dk>
References: <20040307195450.34920.qmail@web20516.mail.yahoo.com>	<404BA130.9060703@lscp.ehess.fr>
	<x2hdx0z53s.fsf@biostat.ku.dk>
Message-ID: <404BD2DE.7090001@pdf.com>

      Peter's enumeration of alternatives inspired me to compare compute 
times for N = 10^(2:5), with the following results:      

*** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
                          10  100 1000 10000  1e+05
for loop                   0 0.01 0.09  1.27 192.05
gen e + for loop           0 0.00 0.03  0.22   2.58
create storage + for loop  0 0.01 0.05  0.34   3.45
sapply                     0 0.00 0.04  0.28   3.82
replicate                  0 0.01 0.05  0.29   4.02

      I repeated this with the "for loop" both first and last.  The 
times tended to decline on replication, with the "for loop" time for N = 
1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
last), and 118.64 ("for loop" first again). 

      Conclusions: 
     
      (1) Apparently, in some cases, R picks up speed upon replication

      (2) The first 3 times for the "for loop" with N = 1e5 made me 
wonder if there was an order effect, with the "for loop" being longer in 
the first position.  However, the last run with the "for loop" again 
first had the shortest time of 118.64, contradicting that hypothesis. 

      By comparison, I also tried this under S-Plus 6.2: 

*** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
                           10  100  1000 10000  100000
                 for loop 0.01 0.05 0.331 3.976 273.073
         gen e + for loop 0.00 0.04 0.320 3.154  29.112
create storage + for loop 0.01 0.03 0.231 2.113  22.242
                   sapply 0.00 0.04 0.380 4.757  23.003

      The script I used appears below.  As Peter said, "the only really 
crucial [issue] is to avoid the inefficient append by preallocating" the 
vectors to be generated.  Moreover, this is only an issue for long loop, 
with a threshold of between 1e4 and 1e5 in this example.  For shorter 
loops, the programmers' time is far more valuable. 

Enjoy.  spencer graves
####################


N.gen <- c(10, 100, 1000, 10000, 1e5)
mtds <- c("for loop", "gen e + for loop", "create storage + for loop",
    "sapply", "replicate")
m <- length(N.gen)   
ellapsed.time <- array(NA, dim=c(m, length(mtds)))
dimnames(ellapsed.time) <- list(N.gen, mtds)
   
for(iN in 1:m){
    cat("\n", iN, "")
    N <- N.gen[iN]
#for loop
set.seed(123)
start.time <- proc.time()
f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
v=c()
x=.1 # starting point
for (i in 1:N) { x=f(x); v=append(v,x) }
ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3]   
cat(mtds[1], "")

#gen e + for loop
set.seed(123)
start.time <- proc.time()
e <- 0.001*rnorm(N)
X <- rep(0.1, N+1)
for(i in 2:(N+1))
    X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
cat(mtds[2], "")

#create storage + for loop 
set.seed(123)
start.time <- proc.time()
V <- numeric(N)
xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
ellapsed.time[iN, "create storage + for loop"] <- 
(proc.time()-start.time)[3]
cat(mtds[3], "")

#sapply
set.seed(123)
start.time <- proc.time()
xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3]   
cat(mtds[4], "")

if(!is.null(version$language)){
#replicate
set.seed(123)
start.time <- proc.time()
z <- .1 ; vr <- replicate(N, z <<- f(z))
ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
cat(mtds[5], "")
}

}

t(ellapsed.time)
#############################
Peter Dalgaard wrote:

>Christophe Pallier <pallier at lscp.ehess.fr> writes:
>
>  
>
>>Fred J. wrote:
>>
>>    
>>
>>>I need to generate a data set based on this equation
>>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>>>N(0,0,001) random variable
>>>I need say 100 values.
>>>
>>>How do I do this?
>>>      
>>>
>>I assume X(t) and x(t) are the same (?).
>>
>>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
>>v=c()
>>x=.1 # starting point
>>for (i in 1:100) { x=f(x); v=append(v,x) }
>>
>>There may be smarter ways...
>>    
>>
>
>Yes, but the only really crucial one is to avoid the inefficient append  by
>preallocating the v: 
>
>v <- numeric(100)
>x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
>
>apart from that you can use implicit loops:
>
>x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
>
>or
>
>z <- .1 ; v <- replicate(100, z <<- f(z))
>
>(You cannot use x there because of a variable capture issue which is a
>bit of a bug. I intend to fix it for 1.9.0.)
>
>  
>



From ggrothendieck at myway.com  Mon Mar  8 03:12:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun,  7 Mar 2004 21:12:51 -0500 (EST)
Subject: [R] Excel files
Message-ID: <20040308021251.A23043977@mprdmxin.myway.com>


Just a minor correction and a simplification.   The header=T
is missing from read.table and as.is=1:2 could be added to
avoid having to use as.character in chron.  Also in your data
(but not in Grace's) the default chron format is used so the
format specifier can be omitted:

require(chron)
x <- read.table("clipboard", header=T, as.is=1:2)
timedate <- chron( x$date, x$time )
plot( x$Value~timedate, pch=as.numeric(x$Sex) )



---
Date:   Sun, 7 Mar 2004 19:43:49 -0600 
From:   Robert W. Baer, Ph.D. <rbaer at atsu.edu>
[ Add to Address Book | Block Address | Report as Spam ] 
To:   <R-help at stat.math.ethz.ch> 
Cc:   Grace Conlon <gracestat at yahoo.com> 
Subject:   Re: [R] Excel files 

 
This might get you started on reading and plotting the dates and times for
levels of a gender factor:
# I assume the following Excel data
date time Sex Value
1 5/5/1999 10:00:00 male 14.987685
2 7/3/1998 20:00:00 female 17.667527
3 8/6/1999 3:23:00 male 3.428401
4 12/7/1997 6:36:00 male 14.977503
5 3/4/2004 9:49:00 male 6.704703
6 11/15/1999 23:04:00 female 5.536046
7 10/16/1998 6:05:00 male 12.153291
8 2/5/1999 3:06:00 female 12.121168
9 3/3/1997 3:07:00 male 4.641686
10 8/6/2004 7:08:00 male 6.649273
11 8/7/1999 22:10:00 male 11.158278
12 7/4/1998 21:11:00 female 8.098113
13 7/6/1999 5:15:00 male 6.220476
14 9/1/1997 6:16:00 female 1.939658
15 3/15/2004 7:05:00 male 2.032969
16 5/16/1999 2:23:00 male 16.436000
#Simulate Excel read
x<-read.table("clipboard")
x

# Do your plotting
library(chron)
timedate=chron(as.character(x$date),as.character(x$time),format=c(dates="m/d/y",times="h:m:s"))
plot(x$Value~timedate,pch=as.numeric(x$Sex))

1. How can I access the data of certain column? I mean how can I refer it in
R?
Notice how the read in data is in a dataframe. To access a given column of
the data frame use a $ plus the column name.
2. How can I make the two column character values(the time variable I
mentioned) to the X ases values?
see the construction of timedate
3. If I have three variables, one is y axes, one is x axes, the third one
is a logic var, how can I make the logic varaible on the plot also?
I don't know precisely what you want but see how I show gender using the
pch= tag to get different plot symbols. The details depend what you want
here.


HTH,

Rob Baer
----- Original Message ----- 
From: "Grace Conlon" <gracestat at yahoo.com>
To: <R-help at stat.math.ethz.ch>
Sent: Sunday, March 07, 2004 5:43 PM
Subject: [R] Excel files


Hello,

I was trying to import data from an Excel file. After I imported the data, I
was trying to make a scatter plot.

The X axes variable is a time variable, which occupies two columns, one is
date, another one is time. Example 21-Apr-03, 4:10 PM. My qestion is:


4. I have a column of number values, however I want to creat a new column,
the value of each cell is the average of every 10 values of the first
column. How can I do it in R?

I tried to figure it out in the help section, however, it spent me a lot of
times, I still can not get the way out, please help. THank you!



From ggrothendieck at myway.com  Mon Mar  8 05:02:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun,  7 Mar 2004 23:02:17 -0500 (EST)
Subject: [R] applying data generating function
Message-ID: <20040308040217.567C13978@mprdmxin.myway.com>



Regarding your comment on speed varying when replicating the
runs, try running gc() first.

---
Date:   Sun, 07 Mar 2004 17:56:46 -0800 
From:   Spencer Graves <spencer.graves at pdf.com>
To:   Peter Dalgaard <p.dalgaard at biostat.ku.dk> 
Cc:   Fred J. <phddas at yahoo.com>,r-help <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] applying data generating function 

 
Peter's enumeration of alternatives inspired me to compare compute 
times for N = 10^(2:5), with the following results: 

*** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
10 100 1000 10000 1e+05
for loop 0 0.01 0.09 1.27 192.05
gen e + for loop 0 0.00 0.03 0.22 2.58
create storage + for loop 0 0.01 0.05 0.34 3.45
sapply 0 0.00 0.04 0.28 3.82
replicate 0 0.01 0.05 0.29 4.02

I repeated this with the "for loop" both first and last. The 
times tended to decline on replication, with the "for loop" time for N = 
1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
last), and 118.64 ("for loop" first again). 

Conclusions: 

(1) Apparently, in some cases, R picks up speed upon replication

(2) The first 3 times for the "for loop" with N = 1e5 made me 
wonder if there was an order effect, with the "for loop" being longer in 
the first position. However, the last run with the "for loop" again 
first had the shortest time of 118.64, contradicting that hypothesis. 

By comparison, I also tried this under S-Plus 6.2: 

*** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
10 100 1000 10000 100000
for loop 0.01 0.05 0.331 3.976 273.073
gen e + for loop 0.00 0.04 0.320 3.154 29.112
create storage + for loop 0.01 0.03 0.231 2.113 22.242
sapply 0.00 0.04 0.380 4.757 23.003

The script I used appears below. As Peter said, "the only really 
crucial [issue] is to avoid the inefficient append by preallocating" the 
vectors to be generated. Moreover, this is only an issue for long loop, 
with a threshold of between 1e4 and 1e5 in this example. For shorter 
loops, the programmers' time is far more valuable. 

Enjoy. spencer graves
####################


N.gen <- c(10, 100, 1000, 10000, 1e5)
mtds <- c("for loop", "gen e + for loop", "create storage + for loop",
"sapply", "replicate")
m <- length(N.gen) 
ellapsed.time <- array(NA, dim=c(m, length(mtds)))
dimnames(ellapsed.time) <- list(N.gen, mtds)

for(iN in 1:m){
cat("\n", iN, "")
N <- N.gen[iN]
#for loop
set.seed(123)
start.time <- proc.time()
f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
v=c()
x=.1 # starting point
for (i in 1:N) { x=f(x); v=append(v,x) }
ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3] 
cat(mtds[1], "")

#gen e + for loop
set.seed(123)
start.time <- proc.time()
e <- 0.001*rnorm(N)
X <- rep(0.1, N+1)
for(i in 2:(N+1))
X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
cat(mtds[2], "")

#create storage + for loop 
set.seed(123)
start.time <- proc.time()
V <- numeric(N)
xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
ellapsed.time[iN, "create storage + for loop"] <- 
(proc.time()-start.time)[3]
cat(mtds[3], "")

#sapply
set.seed(123)
start.time <- proc.time()
xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3] 
cat(mtds[4], "")

if(!is.null(version$language)){
#replicate
set.seed(123)
start.time <- proc.time()
z <- .1 ; vr <- replicate(N, z <<- f(z))
ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
cat(mtds[5], "")
}

}

t(ellapsed.time)
#############################
Peter Dalgaard wrote:

>Christophe Pallier <pallier at lscp.ehess.fr> writes:
>
> 
>
>>Fred J. wrote:
>>
>> 
>>
>>>I need to generate a data set based on this equation
>>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>>>N(0,0,001) random variable
>>>I need say 100 values.
>>>
>>>How do I do this?
>>> 
>>>
>>I assume X(t) and x(t) are the same (?).
>>
>>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
>>v=c()
>>x=.1 # starting point
>>for (i in 1:100) { x=f(x); v=append(v,x) }
>>
>>There may be smarter ways...
>> 
>>
>
>Yes, but the only really crucial one is to avoid the inefficient append by
>preallocating the v: 
>
>v <- numeric(100)
>x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
>
>apart from that you can use implicit loops:
>
>x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
>
>or
>
>z <- .1 ; v <- replicate(100, z <<- f(z))
>
>(You cannot use x there because of a variable capture issue which is a
>bit of a bug. I intend to fix it for 1.9.0.)
>
> 
>



From spencer.graves at pdf.com  Mon Mar  8 05:10:51 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 07 Mar 2004 20:10:51 -0800
Subject: [R] drawing filled countries according to data using map('world')?
	- follow up
In-Reply-To: <HCECJPLNNGBBJIOJMJJAMEHNCEAA.jens_hainmueller@ksg05.harvard.edu>
References: <HCECJPLNNGBBJIOJMJJAMEHNCEAA.jens_hainmueller@ksg05.harvard.edu>
Message-ID: <404BF24B.20008@pdf.com>

      I have no experience with maps, but I can see a problem with 
"dv[state.to.map, year == 1980]" that would generate the error you got, 
"incorrect number of dimensions":  This expression assumes "dv" is a 
2-dimensional array, and R thinks you want the rows specified by 
"state.to.map" and the columns specified by "year == 1980". 

      If "dv" is a vector, I'm guessing that the following might give 
you what you want: 

      dv[state.to.map][(year == 1980)[state.to.map]]

      I assume here that "state.to.map" is a numeric vector of the same 
length as "dv", and "year" is a numeric vector of the same length.  If 
"dv" is a 3-dimensional array, then the following might return what you 
want: 

      dv[state.to.map, drop=F][(year == 1980)[state.to.map],,]

      Please check help.start() -> "An Introduction to R" ->

    * Simple manipulations numbers and vectors
      <#Simple%20manipulations%20numbers%20and%20vectors>:
    * Objects <#Objects>:
    * Factors <#Factors>:
    * Arrays and matrices <#Arrays%20and%20matrices>:
    * Lists and data frames <#Lists%20and%20data%20frames>:

      Hope this helps.  spencer graves

Jens Hainmueller wrote:

>Hello,
>
>this is a follow up on my previous inquiry regarding the use of the map
>library (Becker and Wilks 1993).
>
>Using the 'world' database I would like to draw filled countries in a world
>map so that the filling colors of each country corresponds to the value of a
>policy variable "fix.float" at a specific "year" (the goal is to visualize a
>policy diffusion pattern over time using different maps for year=1985, 1990,
>etc.).
>
>In my dataset [Test] I have created a vector 'map.name' that contains
>country names that I have made identical to the country names in file
>world.N in .../library/maps/mapdata/.
>
>  
>
>>Test[1:10,]
>>    
>>
>   region fix.float wbcode  name year dv dv.lag map.name polygon
>1     lac        NA    ABW Aruba 1973 NA     NA    Aruba    1936
>2     lac        NA    ABW Aruba 1974 NA     NA    Aruba    1936
>3     lac        NA    ABW Aruba 1975 NA     NA    Aruba    1936
>4     lac        NA    ABW Aruba 1976 NA     NA    Aruba    1936
>5     lac        NA    ABW Aruba 1977 NA     NA    Aruba    1936
>6     lac        NA    ABW Aruba 1978 NA     NA    Aruba    1936
>7     lac        NA    ABW Aruba 1979 NA     NA    Aruba    1936
>8     lac        NA    ABW Aruba 1980 NA     NA    Aruba    1936
>9     lac        NA    ABW Aruba 1981 NA     NA    Aruba    1936
>10    lac        NA    ABW Aruba 1982 NA     NA    Aruba    1936
>
>Now I would like to translate the country names in the 'world' database to
>the country names in my dataset (following Becker and Wilks 1993). For some
>reason, the translation does not work.
>
>  
>
>>map.country<-  map(database = "world", names=T,plot=F)
>>state.to.map <- match(map.name,map.country)
>>color <- dv[state.to.map, year == 1980]
>>    
>>
>Error in dv[state.to.map, year == 1980] : incorrect number of dimensions
>
>  
>
>>color <- dv[state.to.map, year == 1980]/100
>>    
>>
>Error in dv[state.to.map, year == 1980] : incorrect number of dimensions
>
>What am I doing wrong? (there are a few values missing in "fix.float")
>
>Thanks for your help!
>
>Best
>Jens Hainmueller
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From spencer.graves at pdf.com  Mon Mar  8 05:15:41 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 07 Mar 2004 20:15:41 -0800
Subject: [R] applying data generating function
In-Reply-To: <20040308040217.567C13978@mprdmxin.myway.com>
References: <20040308040217.567C13978@mprdmxin.myway.com>
Message-ID: <404BF36D.8080501@pdf.com>

Hi, Gabor: 

      Thanks for the "garbage collection" suggestion.  In this case, I 
can't imagine how it would change the results:  I developed the script 
in an S-Plus script window, then copied it into an R session that had 
recently just been started.  Moreover, the times generally declined upon 
replication.  Do you think the time might INCREASE after "gc"? 

      Best Wishes,
      spencer graves

Gabor Grothendieck wrote:

>Regarding your comment on speed varying when replicating the
>runs, try running gc() first.
>
>---
>Date:   Sun, 07 Mar 2004 17:56:46 -0800 
>From:   Spencer Graves <spencer.graves at pdf.com>
>To:   Peter Dalgaard <p.dalgaard at biostat.ku.dk> 
>Cc:   Fred J. <phddas at yahoo.com>,r-help <r-help at stat.math.ethz.ch> 
>Subject:   Re: [R] applying data generating function 
>
> 
>Peter's enumeration of alternatives inspired me to compare compute 
>times for N = 10^(2:5), with the following results: 
>
>*** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
>10 100 1000 10000 1e+05
>for loop 0 0.01 0.09 1.27 192.05
>gen e + for loop 0 0.00 0.03 0.22 2.58
>create storage + for loop 0 0.01 0.05 0.34 3.45
>sapply 0 0.00 0.04 0.28 3.82
>replicate 0 0.01 0.05 0.29 4.02
>
>I repeated this with the "for loop" both first and last. The 
>times tended to decline on replication, with the "for loop" time for N = 
>1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
>last), and 118.64 ("for loop" first again). 
>
>Conclusions: 
>
>(1) Apparently, in some cases, R picks up speed upon replication
>
>(2) The first 3 times for the "for loop" with N = 1e5 made me 
>wonder if there was an order effect, with the "for loop" being longer in 
>the first position. However, the last run with the "for loop" again 
>first had the shortest time of 118.64, contradicting that hypothesis. 
>
>By comparison, I also tried this under S-Plus 6.2: 
>
>*** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
>10 100 1000 10000 100000
>for loop 0.01 0.05 0.331 3.976 273.073
>gen e + for loop 0.00 0.04 0.320 3.154 29.112
>create storage + for loop 0.01 0.03 0.231 2.113 22.242
>sapply 0.00 0.04 0.380 4.757 23.003
>
>The script I used appears below. As Peter said, "the only really 
>crucial [issue] is to avoid the inefficient append by preallocating" the 
>vectors to be generated. Moreover, this is only an issue for long loop, 
>with a threshold of between 1e4 and 1e5 in this example. For shorter 
>loops, the programmers' time is far more valuable. 
>
>Enjoy. spencer graves
>####################
>
>
>N.gen <- c(10, 100, 1000, 10000, 1e5)
>mtds <- c("for loop", "gen e + for loop", "create storage + for loop",
>"sapply", "replicate")
>m <- length(N.gen) 
>ellapsed.time <- array(NA, dim=c(m, length(mtds)))
>dimnames(ellapsed.time) <- list(N.gen, mtds)
>
>for(iN in 1:m){
>cat("\n", iN, "")
>N <- N.gen[iN]
>#for loop
>set.seed(123)
>start.time <- proc.time()
>f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
>v=c()
>x=.1 # starting point
>for (i in 1:N) { x=f(x); v=append(v,x) }
>ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3] 
>cat(mtds[1], "")
>
>#gen e + for loop
>set.seed(123)
>start.time <- proc.time()
>e <- 0.001*rnorm(N)
>X <- rep(0.1, N+1)
>for(i in 2:(N+1))
>X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
>ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
>cat(mtds[2], "")
>
>#create storage + for loop 
>set.seed(123)
>start.time <- proc.time()
>V <- numeric(N)
>xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
>ellapsed.time[iN, "create storage + for loop"] <- 
>(proc.time()-start.time)[3]
>cat(mtds[3], "")
>
>#sapply
>set.seed(123)
>start.time <- proc.time()
>xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
>ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3] 
>cat(mtds[4], "")
>
>if(!is.null(version$language)){
>#replicate
>set.seed(123)
>start.time <- proc.time()
>z <- .1 ; vr <- replicate(N, z <<- f(z))
>ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
>cat(mtds[5], "")
>}
>
>}
>
>t(ellapsed.time)
>#############################
>Peter Dalgaard wrote:
>
>  
>
>>Christophe Pallier <pallier at lscp.ehess.fr> writes:
>>
>>
>>
>>    
>>
>>>Fred J. wrote:
>>>
>>>
>>>
>>>      
>>>
>>>>I need to generate a data set based on this equation
>>>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>>>>N(0,0,001) random variable
>>>>I need say 100 values.
>>>>
>>>>How do I do this?
>>>>
>>>>
>>>>        
>>>>
>>>I assume X(t) and x(t) are the same (?).
>>>
>>>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
>>>v=c()
>>>x=.1 # starting point
>>>for (i in 1:100) { x=f(x); v=append(v,x) }
>>>
>>>There may be smarter ways...
>>>
>>>
>>>      
>>>
>>Yes, but the only really crucial one is to avoid the inefficient append by
>>preallocating the v: 
>>
>>v <- numeric(100)
>>x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
>>
>>apart from that you can use implicit loops:
>>
>>x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
>>
>>or
>>
>>z <- .1 ; v <- replicate(100, z <<- f(z))
>>
>>(You cannot use x there because of a variable capture issue which is a
>>bit of a bug. I intend to fix it for 1.9.0.)
>>
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at myway.com  Mon Mar  8 07:46:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Mar 2004 01:46:21 -0500 (EST)
Subject: [R] applying data generating function
Message-ID: <20040308064621.701CD3953@mprdmxin.myway.com>



Its possible that there was a garbage collection at the
beginning or maybe this suggestion does not apply, given
the precautions you took.  As far as I know, all you can
do is try it and see if it gives more consistent results.

---
Date:   Sun, 07 Mar 2004 20:15:41 -0800 
From:   Spencer Graves <spencer.graves at pdf.com>
To:   <ggrothendieck at myway.com> 
Cc:   <p.dalgaard at biostat.ku.dk>, <phddas at yahoo.com>, <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] applying data generating function 

 
Hi, Gabor: 

Thanks for the "garbage collection" suggestion. In this case, I 
can't imagine how it would change the results: I developed the script 
in an S-Plus script window, then copied it into an R session that had 
recently just been started. Moreover, the times generally declined upon 
replication. Do you think the time might INCREASE after "gc"? 

Best Wishes,
spencer graves

Gabor Grothendieck wrote:

>Regarding your comment on speed varying when replicating the
>runs, try running gc() first.
>
>---
>Date: Sun, 07 Mar 2004 17:56:46 -0800 
>From: Spencer Graves <spencer.graves at pdf.com>
>To: Peter Dalgaard <p.dalgaard at biostat.ku.dk> 
>Cc: Fred J. <phddas at yahoo.com>,r-help <r-help at stat.math.ethz.ch> 
>Subject: Re: [R] applying data generating function 
>
> 
>Peter's enumeration of alternatives inspired me to compare compute 
>times for N = 10^(2:5), with the following results: 
>
>*** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
>10 100 1000 10000 1e+05
>for loop 0 0.01 0.09 1.27 192.05
>gen e + for loop 0 0.00 0.03 0.22 2.58
>create storage + for loop 0 0.01 0.05 0.34 3.45
>sapply 0 0.00 0.04 0.28 3.82
>replicate 0 0.01 0.05 0.29 4.02
>
>I repeated this with the "for loop" both first and last. The 
>times tended to decline on replication, with the "for loop" time for N = 
>1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
>last), and 118.64 ("for loop" first again). 
>
>Conclusions: 
>
>(1) Apparently, in some cases, R picks up speed upon replication
>
>(2) The first 3 times for the "for loop" with N = 1e5 made me 
>wonder if there was an order effect, with the "for loop" being longer in 
>the first position. However, the last run with the "for loop" again 
>first had the shortest time of 118.64, contradicting that hypothesis. 
>
>By comparison, I also tried this under S-Plus 6.2: 
>
>*** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
>10 100 1000 10000 100000
>for loop 0.01 0.05 0.331 3.976 273.073
>gen e + for loop 0.00 0.04 0.320 3.154 29.112
>create storage + for loop 0.01 0.03 0.231 2.113 22.242
>sapply 0.00 0.04 0.380 4.757 23.003
>
>The script I used appears below. As Peter said, "the only really 
>crucial [issue] is to avoid the inefficient append by preallocating" the 
>vectors to be generated. Moreover, this is only an issue for long loop, 
>with a threshold of between 1e4 and 1e5 in this example. For shorter 
>loops, the programmers' time is far more valuable. 
>
>Enjoy. spencer graves
>####################
>
>
>N.gen <- c(10, 100, 1000, 10000, 1e5)
>mtds <- c("for loop", "gen e + for loop", "create storage + for loop",
>"sapply", "replicate")
>m <- length(N.gen) 
>ellapsed.time <- array(NA, dim=c(m, length(mtds)))
>dimnames(ellapsed.time) <- list(N.gen, mtds)
>
>for(iN in 1:m){
>cat("\n", iN, "")
>N <- N.gen[iN]
>#for loop
>set.seed(123)
>start.time <- proc.time()
>f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
>v=c()
>x=.1 # starting point
>for (i in 1:N) { x=f(x); v=append(v,x) }
>ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3] 
>cat(mtds[1], "")
>
>#gen e + for loop
>set.seed(123)
>start.time <- proc.time()
>e <- 0.001*rnorm(N)
>X <- rep(0.1, N+1)
>for(i in 2:(N+1))
>X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
>ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
>cat(mtds[2], "")
>
>#create storage + for loop 
>set.seed(123)
>start.time <- proc.time()
>V <- numeric(N)
>xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
>ellapsed.time[iN, "create storage + for loop"] <- 
>(proc.time()-start.time)[3]
>cat(mtds[3], "")
>
>#sapply
>set.seed(123)
>start.time <- proc.time()
>xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
>ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3] 
>cat(mtds[4], "")
>
>if(!is.null(version$language)){
>#replicate
>set.seed(123)
>start.time <- proc.time()
>z <- .1 ; vr <- replicate(N, z <<- f(z))
>ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
>cat(mtds[5], "")
>}
>
>}
>
>t(ellapsed.time)
>#############################
>Peter Dalgaard wrote:
>
> 
>
>>Christophe Pallier <pallier at lscp.ehess.fr> writes:
>>
>>
>>
>> 
>>
>>>Fred J. wrote:
>>>
>>>
>>>
>>> 
>>>
>>>>I need to generate a data set based on this equation
>>>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>>>>N(0,0,001) random variable
>>>>I need say 100 values.
>>>>
>>>>How do I do this?
>>>>
>>>>
>>>> 
>>>>
>>>I assume X(t) and x(t) are the same (?).
>>>
>>>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
>>>v=c()
>>>x=.1 # starting point
>>>for (i in 1:100) { x=f(x); v=append(v,x) }
>>>
>>>There may be smarter ways...
>>>
>>>
>>> 
>>>
>>Yes, but the only really crucial one is to avoid the inefficient append by
>>preallocating the v: 
>>
>>v <- numeric(100)
>>x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
>>
>>apart from that you can use implicit loops:
>>
>>x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
>>
>>or
>>
>>z <- .1 ; v <- replicate(100, z <<- f(z))
>>
>>(You cannot use x there because of a variable capture issue which is a
>>bit of a bug. I intend to fix it for 1.9.0.)
>>



From ripley at stats.ox.ac.uk  Mon Mar  8 08:29:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Mar 2004 07:29:48 +0000 (GMT)
Subject: [R] applying data generating function
In-Reply-To: <404BD2DE.7090001@pdf.com>
Message-ID: <Pine.LNX.4.44.0403080726240.28242-100000@gannet.stats>

You need to run gc() before running such timings in R, as the first run 
often has to pay for a level-0 garbage collection.  That is normally the 
cause of (1), although I haven't seen differences as large as 10 secs (but 
have no idea of the speed of your machine, and have seen 3 secs).

On Sun, 7 Mar 2004, Spencer Graves wrote:

>       Peter's enumeration of alternatives inspired me to compare compute 
> times for N = 10^(2:5), with the following results:      
> 
> *** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
>                           10  100 1000 10000  1e+05
> for loop                   0 0.01 0.09  1.27 192.05
> gen e + for loop           0 0.00 0.03  0.22   2.58
> create storage + for loop  0 0.01 0.05  0.34   3.45
> sapply                     0 0.00 0.04  0.28   3.82
> replicate                  0 0.01 0.05  0.29   4.02
> 
>       I repeated this with the "for loop" both first and last.  The 
> times tended to decline on replication, with the "for loop" time for N = 
> 1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
> last), and 118.64 ("for loop" first again). 
> 
>       Conclusions: 
>      
>       (1) Apparently, in some cases, R picks up speed upon replication
> 
>       (2) The first 3 times for the "for loop" with N = 1e5 made me 
> wonder if there was an order effect, with the "for loop" being longer in 
> the first position.  However, the last run with the "for loop" again 
> first had the shortest time of 118.64, contradicting that hypothesis. 
> 
>       By comparison, I also tried this under S-Plus 6.2: 
> 
> *** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
>                            10  100  1000 10000  100000
>                  for loop 0.01 0.05 0.331 3.976 273.073
>          gen e + for loop 0.00 0.04 0.320 3.154  29.112
> create storage + for loop 0.01 0.03 0.231 2.113  22.242
>                    sapply 0.00 0.04 0.380 4.757  23.003
> 
>       The script I used appears below.  As Peter said, "the only really 
> crucial [issue] is to avoid the inefficient append by preallocating" the 
> vectors to be generated.  Moreover, this is only an issue for long loop, 
> with a threshold of between 1e4 and 1e5 in this example.  For shorter 
> loops, the programmers' time is far more valuable. 
> 
> Enjoy.  spencer graves
> ####################
> 
> 
> N.gen <- c(10, 100, 1000, 10000, 1e5)
> mtds <- c("for loop", "gen e + for loop", "create storage + for loop",
>     "sapply", "replicate")
> m <- length(N.gen)   
> ellapsed.time <- array(NA, dim=c(m, length(mtds)))
> dimnames(ellapsed.time) <- list(N.gen, mtds)
>    
> for(iN in 1:m){
>     cat("\n", iN, "")
>     N <- N.gen[iN]
> #for loop
> set.seed(123)
> start.time <- proc.time()
> f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
> v=c()
> x=.1 # starting point
> for (i in 1:N) { x=f(x); v=append(v,x) }
> ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3]   
> cat(mtds[1], "")
> 
> #gen e + for loop
> set.seed(123)
> start.time <- proc.time()
> e <- 0.001*rnorm(N)
> X <- rep(0.1, N+1)
> for(i in 2:(N+1))
>     X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
> ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
> cat(mtds[2], "")
> 
> #create storage + for loop 
> set.seed(123)
> start.time <- proc.time()
> V <- numeric(N)
> xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
> ellapsed.time[iN, "create storage + for loop"] <- 
> (proc.time()-start.time)[3]
> cat(mtds[3], "")
> 
> #sapply
> set.seed(123)
> start.time <- proc.time()
> xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
> ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3]   
> cat(mtds[4], "")
> 
> if(!is.null(version$language)){
> #replicate
> set.seed(123)
> start.time <- proc.time()
> z <- .1 ; vr <- replicate(N, z <<- f(z))
> ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
> cat(mtds[5], "")
> }
> 
> }
> 
> t(ellapsed.time)
> #############################
> Peter Dalgaard wrote:
> 
> >Christophe Pallier <pallier at lscp.ehess.fr> writes:
> >
> >  
> >
> >>Fred J. wrote:
> >>
> >>    
> >>
> >>>I need to generate a data set based on this equation
> >>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
> >>>N(0,0,001) random variable
> >>>I need say 100 values.
> >>>
> >>>How do I do this?
> >>>      
> >>>
> >>I assume X(t) and x(t) are the same (?).
> >>
> >>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
> >>v=c()
> >>x=.1 # starting point
> >>for (i in 1:100) { x=f(x); v=append(v,x) }
> >>
> >>There may be smarter ways...
> >>    
> >>
> >
> >Yes, but the only really crucial one is to avoid the inefficient append  by
> >preallocating the v: 
> >
> >v <- numeric(100)
> >x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
> >
> >apart from that you can use implicit loops:
> >
> >x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
> >
> >or
> >
> >z <- .1 ; v <- replicate(100, z <<- f(z))
> >
> >(You cannot use x there because of a variable capture issue which is a
> >bit of a bug. I intend to fix it for 1.9.0.)
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From a.trapletti at bluewin.ch  Mon Mar  8 09:01:14 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Mon, 08 Mar 2004 09:01:14 +0100
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
Message-ID: <404C284A.1060003@bluewin.ch>

>
> Hi R specialists,
>
> The na.omit.ts() method fails when the time series contains internal
> NA's. How can these automatically be removed?


try na.remove from tseries. This is, e.g., useful when removing weekends 
(NA prices) from financial data, i.e., switching from physical time to 
business time.

best
Adrian

>
>>> spectrum(ts.mNDII, na.action=na.omit)
>
> Error in na.omit.ts(as.ts(x)) : time series contains internal NAs
>
> How can the na.action be activated correctly?
>
>>> acf(ts.Lin, type=c("correlation"), na.action=na.omit)
>
>Error in na.omit.ts(as.ts(x)) : time series contains internal NAs
>
>((ts.Lin contains two time series, where one contains internal NAs
>(-->an NA not a the end/beginning of a time serie)))
>
>Thanks a lot!
>
>Jan Verbesselt
>  
>
>
>

Dr. Adrian Trapletti
Trapletti Statistical Computing
Wildsbergstrasse 31, 8610 Uster
Switzerland
Phone & Fax : +41 (0) 1 994 5631
Mobile : +41 (0) 76 370 5631
Email : mailto:a.trapletti at bluewin.ch
WWW : http://trapletti.homelinux.com



From ripley at stats.ox.ac.uk  Mon Mar  8 09:24:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Mar 2004 08:24:42 +0000 (GMT)
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
In-Reply-To: <404C284A.1060003@bluewin.ch>
Message-ID: <Pine.LNX.4.44.0403080818360.28424-100000@gannet.stats>

On Mon, 8 Mar 2004, Adrian Trapletti wrote:

> > The na.omit.ts() method fails when the time series contains internal
> > NA's. How can these automatically be removed?
> 
> 
> try na.remove from tseries. This is, e.g., useful when removing weekends 
> (NA prices) from financial data, i.e., switching from physical time to 
> business time.

That may well not be appropriate though.  NA denotes a missing and not a 
non-existent value, and for example removing non-trading days in just one 
market it probably not appropriate (as information accrues in other 
markets).

The issue is not `How can the na.action be activated correctly?', but
`What is the appropriate na.action?', and the answer is usually `none of 
them'.

> >>> spectrum(ts.mNDII, na.action=na.omit)
> >
> > Error in na.omit.ts(as.ts(x)) : time series contains internal NAs
> >
> > How can the na.action be activated correctly?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From a.trapletti at bluewin.ch  Mon Mar  8 09:49:16 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Mon, 08 Mar 2004 09:49:16 +0100
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
In-Reply-To: <Pine.LNX.4.44.0403080818360.28424-100000@gannet.stats>
References: <Pine.LNX.4.44.0403080818360.28424-100000@gannet.stats>
Message-ID: <404C338C.7050800@bluewin.ch>

Prof Brian Ripley wrote:

>On Mon, 8 Mar 2004, Adrian Trapletti wrote:
>
>  
>
>>>The na.omit.ts() method fails when the time series contains internal
>>>NA's. How can these automatically be removed?
>>>      
>>>
>>try na.remove from tseries. This is, e.g., useful when removing weekends 
>>(NA prices) from financial data, i.e., switching from physical time to 
>>business time.
>>    
>>
>
>That may well not be appropriate though.  NA denotes a missing and not a 
>non-existent value, 
>
I think, this is more a philosophical question, if weekends represent 
missing or non-existent values. One can find arguments for both.

>and for example removing non-trading days in just one 
>market it probably not appropriate (as information accrues in other 
>markets).
>  
>
>The issue is not `How can the na.action be activated correctly?', but
>`What is the appropriate na.action?', and the answer is usually `none of 
>them'.
>
>  
>
I partly agree with you. Sometimes it is not appropriate and it might be 
better to use, e.g., a state-space model which can work with NA's. But 
from my experience in financial markets, it is often not worth 
considering these more complex approaches, since they don't generate 
additional profits. Other effects often dominate...
But anyhow, I find it good if the user can choose between the different 
options and decide himself.

>>>>>spectrum(ts.mNDII, na.action=na.omit)
>>>>>          
>>>>>
>>>Error in na.omit.ts(as.ts(x)) : time series contains internal NAs
>>>
>>>How can the na.action be activated correctly?
>>>      
>>>
>
>  
>
best
Adrian

Dr. Adrian Trapletti
Trapletti Statistical Computing
Wildsbergstrasse 31, 8610 Uster
Switzerland
Phone & Fax : +41 (0) 1 994 5631
Mobile : +41 (0) 76 370 5631
Email : mailto:a.trapletti at bluewin.ch
WWW : http://trapletti.homelinux.com



From stuart.leask at nottingham.ac.uk  Mon Mar  8 09:57:26 2004
From: stuart.leask at nottingham.ac.uk (Stuart Leask)
Date: Mon, 8 Mar 2004 08:57:26 -0000
Subject: [R] lm - significance disappears
References: <Pine.LNX.4.44.0403080818360.28424-100000@gannet.stats>
Message-ID: <009301c404eb$60f34d80$f2e1f380@OPENZAURUS>

Hi there.
I am trying to model the effect of "Age at onset of a condition" (AGEONSET)
upon IQ (ie. do they have more problems if they get ill younger?). I also
want to see if there is an interaction with the TYPE of test (reading &
maths).

I am struggling to understand why the effect of AGEONSET, so very
significant in the non-interaction models, ceases to be so in the
interacting model? Am I using lm() wrongly here?


Stuart

****************************

# Regress IQ on AGEONSET, AGEONSET + TYPE, & AGEONSET*TYPE.

> fit.f<-lm(IQ~AGEONSET)
> summary(fit.f)

Call:
lm(formula = IQ ~ AGEONSET)

Residuals:
     Min       1Q   Median       3Q      Max
-3.07434 -0.78350  0.04031  0.71869  2.82266

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.975267   0.077362 -12.607  < 2e-16 ***
AGEONSET     0.037158   0.006876   5.404 9.57e-08 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 1.109 on 574 degrees of freedom
Multiple R-Squared: 0.04841,    Adjusted R-squared: 0.04675
F-statistic:  29.2 on 1 and 574 DF,  p-value: 9.571e-08


> fit.f<-lm(IQ~AGEONSET+TYPE)
> summary(fit.f)

Call:
lm(formula = IQ ~ AGEONSET + TYPE)

Residuals:
     Min       1Q   Median       3Q      Max
-3.03301 -0.80625  0.03662  0.72066  2.86379

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -0.852151   0.158755  -5.368 1.16e-07 ***
AGEONSET     0.037147   0.006877   5.401 9.71e-08 ***
TYPE        -0.082107   0.092447  -0.888    0.375
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 1.109 on 573 degrees of freedom
Multiple R-Squared: 0.04972,    Adjusted R-squared: 0.0464
F-statistic: 14.99 on 2 and 573 DF,  p-value: 4.513e-07


> fit.f<-lm(IQ~AGEONSET*TYPE)
> summary(fit.f)

Call:
lm(formula = IQ ~ AGEONSET * TYPE)

Residuals:
     Min       1Q   Median       3Q      Max
-3.04133 -0.80869  0.03557  0.72420  2.86796

Coefficients:
               Estimate Std. Error t value Pr(>|t|)
(Intercept)   -0.833399   0.244866  -3.403 0.000712 ***
AGEONSET       0.035070   0.021758   1.612 0.107555
TYPE          -0.094608   0.154887  -0.611 0.541562
AGEONSET:TYPE  0.001386   0.013767   0.101 0.919868
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 1.11 on 572 degrees of freedom
Multiple R-Squared: 0.04974,    Adjusted R-squared: 0.04475
F-statistic: 9.979 on 3 and 572 DF,  p-value: 2.026e-06



From copellifulvio at yahoo.it  Mon Mar  8 11:17:04 2004
From: copellifulvio at yahoo.it (=?iso-8859-1?q?Fulvio=20Copex?=)
Date: Mon, 8 Mar 2004 11:17:04 +0100 (CET)
Subject: [R] getting the std errors in the lm function 
Message-ID: <20040308101704.97085.qmail@web25204.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040308/49545610/attachment.pl

From migreja at med.up.pt  Mon Mar  8 11:26:13 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?J=FAlia?= Rodrigues Igreja)
Date: Mon, 08 Mar 2004 10:26:13 -0000
Subject: [R] dsn
Message-ID: <4.3.2.7.1.19970301204058.00ac7dc0@mail.med.up.pt>

Hi,

I have a data base in oracle and need to link R using ODBC.
When i use the command odbcConnect a dsn is needed.
I would like to create a dsn.Do you know how can i do it?
Thank you.

Margarida,Portugal



From ripley at stats.ox.ac.uk  Mon Mar  8 11:35:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Mar 2004 10:35:11 +0000 (GMT)
Subject: [R] lm - significance disappears
In-Reply-To: <009301c404eb$60f34d80$f2e1f380@OPENZAURUS>
Message-ID: <Pine.LNX.4.44.0403081028590.5954-100000@gannet.stats>

On Mon, 8 Mar 2004, Stuart Leask wrote:

> Hi there.
> I am trying to model the effect of "Age at onset of a condition" (AGEONSET)
> upon IQ (ie. do they have more problems if they get ill younger?). I also
> want to see if there is an interaction with the TYPE of test (reading &
> maths).
> 
> I am struggling to understand why the effect of AGEONSET, so very
> significant in the non-interaction models, ceases to be so in the
> interacting model? Am I using lm() wrongly here?

First, the meaning of AGEONSET changes.  Assuming TYPE is a 2-level factor 
and AGEONSET is continuous, this is not an interaction per se but a model 
with one line for both types vs separate lines for both types.  

>From now on I need to assume standard (that is treatment) contrasts.
In your final model, AGEONSET is the slope for the line in the first
group: maybe that group is the smaller, as the standard error is 3x
larger?  AGEONSET:TYPE is the difference in slopes: that is determined 
well but is small.

It helps to look at both the size of the coefficient and its se, as well 
as their ratio.

> 
> 
> Stuart
> 
> ****************************
> 
> # Regress IQ on AGEONSET, AGEONSET + TYPE, & AGEONSET*TYPE.
> 
> > fit.f<-lm(IQ~AGEONSET)
> > summary(fit.f)
> 
> Call:
> lm(formula = IQ ~ AGEONSET)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -3.07434 -0.78350  0.04031  0.71869  2.82266
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0.975267   0.077362 -12.607  < 2e-16 ***
> AGEONSET     0.037158   0.006876   5.404 9.57e-08 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 1.109 on 574 degrees of freedom
> Multiple R-Squared: 0.04841,    Adjusted R-squared: 0.04675
> F-statistic:  29.2 on 1 and 574 DF,  p-value: 9.571e-08
> 
> 
> > fit.f<-lm(IQ~AGEONSET+TYPE)
> > summary(fit.f)
> 
> Call:
> lm(formula = IQ ~ AGEONSET + TYPE)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -3.03301 -0.80625  0.03662  0.72066  2.86379
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) -0.852151   0.158755  -5.368 1.16e-07 ***
> AGEONSET     0.037147   0.006877   5.401 9.71e-08 ***
> TYPE        -0.082107   0.092447  -0.888    0.375
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 1.109 on 573 degrees of freedom
> Multiple R-Squared: 0.04972,    Adjusted R-squared: 0.0464
> F-statistic: 14.99 on 2 and 573 DF,  p-value: 4.513e-07
> 
> 
> > fit.f<-lm(IQ~AGEONSET*TYPE)
> > summary(fit.f)
> 
> Call:
> lm(formula = IQ ~ AGEONSET * TYPE)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -3.04133 -0.80869  0.03557  0.72420  2.86796
> 
> Coefficients:
>                Estimate Std. Error t value Pr(>|t|)
> (Intercept)   -0.833399   0.244866  -3.403 0.000712 ***
> AGEONSET       0.035070   0.021758   1.612 0.107555
> TYPE          -0.094608   0.154887  -0.611 0.541562
> AGEONSET:TYPE  0.001386   0.013767   0.101 0.919868
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 1.11 on 572 degrees of freedom
> Multiple R-Squared: 0.04974,    Adjusted R-squared: 0.04475
> F-statistic: 9.979 on 3 and 572 DF,  p-value: 2.026e-06
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Mar  8 12:22:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Mar 2004 11:22:38 +0000 (GMT)
Subject: [R] getting the std errors in the lm function 
In-Reply-To: <20040308101704.97085.qmail@web25204.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403081121200.6205-100000@gannet.stats>

On Mon, 8 Mar 2004, Fulvio Copex wrote:

> Hello,
> I have a simple question for you:
> making:
> mylm<-lm(y~x)
> summary(mylm)
> I get the following results:
> ******************************************************
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 16.54087    0.19952   82.91   <2e-16 ***
> x[1:19]     -2.32337    0.04251  -54.66   <2e-16 ***
> ******************************************************
> now I can access easily with
> mylm$coefficients[[1]]
> mylm$coefficients[[2]]

Use [] not [[]] for a non-list vector, and better

coef(mylm)[1] etc

> at the Estimate 16.54087 and -2.32337.
> how can I access at the Std. Error 0.19952 and 0.04251 ?

coef(summary(mylm))[, 2]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Mar  8 12:32:54 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Mar 2004 12:32:54 +0100
Subject: [R] getting the std errors in the lm function
In-Reply-To: <20040308101704.97085.qmail@web25204.mail.ukl.yahoo.com>
References: <20040308101704.97085.qmail@web25204.mail.ukl.yahoo.com>
Message-ID: <x27jxvo9p5.fsf@biostat.ku.dk>

Fulvio Copex <copellifulvio at yahoo.it> writes:

> Hello,
> I have a simple question for you:
> making:
> mylm<-lm(y~x)
> summary(mylm)
> I get the following results:
> ******************************************************
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 16.54087    0.19952   82.91   <2e-16 ***
> x[1:19]     -2.32337    0.04251  -54.66   <2e-16 ***
> ******************************************************
> now I can access easily with
> mylm$coefficients[[1]]
> mylm$coefficients[[2]]
> at the Estimate 16.54087 and -2.32337.

[coefficients(mylm)[[1]] would be preferable]

> how can I access at the Std. Error 0.19952 and 0.04251 ?


m <- coefficients(summary(mylm))
m[1,2]
m[2,2]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From JonesW at kssg.com  Mon Mar  8 12:42:03 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 8 Mar 2004 11:42:03 -0000 
Subject: [R] dsn
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0FDF@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040308/387287ed/attachment.pl

From jvpl2000 at yahoo.com  Mon Mar  8 15:08:30 2004
From: jvpl2000 at yahoo.com (JVP L)
Date: Mon, 8 Mar 2004 06:08:30 -0800 (PST)
Subject: [R] Graphical Test
Message-ID: <20040308140830.63685.qmail@web10203.mail.yahoo.com>

Dear R mailing list,

Greetings!!!

I'm doing a graphical test of suitability for the
aircondit data.  I actually just followed the 
exercise in Davison and Hinkley's book of Graphical
Test for model checking.  The program for
testing if it comes from an exponential distribution
seems to be running okay but the program for
the gamma distribution distribution is running away
:-).  I hope you can spare sometime to check
what I have written below.  I am also in doubt about
the mle and functions I have written.

Thank you in advance for your help.

:-)
Jei


  

> ###  Graphical test of suitability of the
exponential and gamma model for the aircondit data 
> 
> 
> library(boot)
> 
> data(aircondit)
> 
> 
> 
> # Testing if the aircondit data comes from an
exponential distribution
> 
> expqq.fun <- function(data, q) sort(data)/mean(data)
> exp.gen <- function(data, mle) rexp(length(data),
mle)
> n <- nrow(aircondit)
> qq <- qexp((1:n)/(n+1))
> exp.boot <- boot(aircondit$hours, expqq.fun,R=999,
sim="parametric",
+     ran.gen=exp.gen, mle=1/mean(aircondit$hours),
q=qq)
> env <- envelope(exp.boot, level=0.95)
> plot(qq, exp.boot$t0,xlab="Exponential quantiles",
+      ylab="Scaled order statistics",
xlim=c(0,max(qq)),
+     
ylim=c(0,max(c(exp.boot$t0,env$overall[2,]))),pch=1)
> lines(qq,env$overall[1,]); lines(qq,env$overall[2,])
> lines(qq,env$point[1,],lty=2);
lines(qq,env$point[2,],lty=2)
> 
> 
> 
> # Testing if the aircondit data comes from a gamma
distribution
> 
> gamqq.fun <- function(data, q) sort(data)/mean(data)
> gam.gen <- function(data, mle)
rgamma(length(data),shape=12, scale=108.0833/12)
> n <- nrow(aircondit)
> p<-seq(.01,.99, by=.01)
> qq <- qgamma(p,shape=12,scale=108.0833/12)
> gam.boot <- boot(aircondit$hours, gamqq.fun,R=999,
sim="parametric",
+     ran.gen=gam.gen, mle=1/mean(aircondit$hours),
q=qq)
> env <- envelope(gam.boot, level=0.95)
> plot(qq, gam.boot$t0,xlab="Gamma quantiles",
+      ylab="Scaled order statistics",
xlim=c(0,max(qq)),
+     
ylim=c(0,max(c(gam.boot$t0,env$overall[2,]))),pch=1)
Error in xy.coords(x, y, xlabel, ylabel, log) : 
        x and y lengths differ
> lines(qq,env$overall[1,]); lines(qq,env$overall[2,])
Error in xy.coords(x, y) : x and y lengths differ
> lines(qq,env$point[1,],lty=2);
lines(qq,env$point[2,],lty=2)
Error in xy.coords(x, y) : x and y lengths differ
>



From poolloopus at yahoo.com  Mon Mar  8 16:34:01 2004
From: poolloopus at yahoo.com (S P)
Date: Mon, 8 Mar 2004 07:34:01 -0800 (PST)
Subject: [R] Rank /core test simulation
Message-ID: <20040308153401.42172.qmail@web41008.mail.yahoo.com>

Hi all,

I am a biostatistician and I have developed my own
ranking system for clinical data. I would like to test
the efficiency of it w.r.t. to other ranking systems.
I would like to simulate the data and after assigning
ranks to my observed scores(after neglecting
dropouts), observe the type I error. If I want to do a
Kruuskal Wallis type of test, what test statistic
should I use to test for a difference of "delta"
between the two groups (I know "delta" since the data
is generated with that difference). The default K-W
test statistics test for a NULL difference and I want
to see how frequently my ranking system rejects the
correct null hypothesis of a "delta" difference.

Thank you in advance for your help.

~S



From HankeA at mar.dfo-mpo.gc.ca  Mon Mar  8 18:56:32 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Mon, 08 Mar 2004 13:56:32 -0400
Subject: [R] boot package
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124956@msgmarsta01.bio.dfo.ca>

Hi,
The function anosim() in vegan package or sample() in base may be of help to
you.
Alex

-----Original Message-----
From: Rog?rio Rosa da Silva [mailto:rrsilva at ib.usp.br] 
Sent: March 4, 2004 4:57 AM
To: rhelp
Subject: [R] boot package 



Dear all 

As part of an ongoing study on the ecomorphology of ant communities, I have 
obtained a matrix with 156 row (species) and 20 columns (several
measurements 
of body shape) for 4 localities.

For each community, I calculated a matrix of Euclidean distances between all
pairs of species. From this matrix, I extracted two measures of community
structure: i) I identified the distance from a individual to its nearest
neighbor (NND) in the morphological space and then calculated the averages
of 
the NND (MNND); ii) the mean of the Euclidean distances (MED). NND and MED 
are of practical use in describing spatial relations between species. The 
results of Euclidean distance studies in each of the four localities were 
compared with each other for evidence repeating patterns.

To determine wheter ou not the morphological arrangement of species in 
communities reflected internal organization, MNND and MED should be compared

with randomly generated communities.

It is possible to write a function in th boot package to evaluate the values

of average nearest-neighbor distance and of the mean of Euclidean distances 
in randomly generated communities ? One set of 1000 random communities each 
with 78, 90, 100 and 102 species must be generaded from the pool of species 
(156 species). The only restrictions on species composition  is that no 
species could be placed in a community more than once, and only rows values 
can be permuted.

Sorry, I don't know how to define the statistic term in the boot() function 
from the boot library in R. I have studied the boot package, argument list
of 
functions and the functions definition. Can someone point me the correct 
syntax?

I would very much appreciate any help

Best regards

Rog?rio

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From nali at umn.edu  Mon Mar  8 19:51:55 2004
From: nali at umn.edu (Na Li)
Date: Mon, 08 Mar 2004 12:51:55 -0600
Subject: [R] graphic device MetaPost
In-Reply-To: <20040307103634.86081.qmail@web15406.mail.cnb.yahoo.com> (Jinsong
	Zhao's message of "Sun, 7 Mar 2004 18:36:34 +0800 (CST)")
References: <20040307103634.86081.qmail@web15406.mail.cnb.yahoo.com>
Message-ID: <gkn06rp3xw.fsf@bass.biostat.umn.edu>

On 7 Mar 2004, Jinsong Zhao said:

> use Chinese character in plots with a minor modification to the MetaPost
> file.

I never tried this, but maybe it is possible to use the psfrag LaTeX
package to insert Chinese characters into an eps file (with the help of
proper ps fonts and the CJK packages of course)?

Michael



From cindy197901 at yahoo.com  Mon Mar  8 17:54:09 2004
From: cindy197901 at yahoo.com (Cindy Juncker)
Date: Mon, 8 Mar 2004 08:54:09 -0800 (PST)
Subject: [R] a question on CSV file
Message-ID: <20040308165410.4412.qmail@web61009.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040308/71824acb/attachment.pl

From joshini at mail.nih.gov  Mon Mar  8 18:15:30 2004
From: joshini at mail.nih.gov (Joshi, Nina (NIH/NCI))
Date: Mon, 8 Mar 2004 12:15:30 -0500 
Subject: [R] memory problem
Message-ID: <27C204BD76CBC142BA1AE46D62A8548ED9E906@nihexchange9.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040308/4bb753bf/attachment.pl

From apjaworski at mmm.com  Mon Mar  8 17:03:33 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Mon, 8 Mar 2004 10:03:33 -0600
Subject: [R] Statistical Quality Control
Message-ID: <OFDE31B877.0E158B9F-ON86256E51.0057FEC1-86256E51.0058378F@mmm.com>






A new package named qcc just appeared on CRAN.  It has standard QC charts,
OC curves and process capability stuff.  It even has a Minitab-like process
capability sixpack.  It looks like it is pretty nicely done.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "Nick Drew"          |
|         |           <ndrew at efn.org>      |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           03/05/2004 19:00     |
|         |           Please respond to    |
|         |           ndrew                |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       "Holton, Blake" <Blake.Holton at dynea.com>                                                                     |
  |        "R-Help (E-mail)" <r-help at stat.math.ethz.ch>                                                                         |
  |      cc:                                                                                                                    |
  |      Subject:  RE: [R] Statistical Quality Control                                                                          |
  >-----------------------------------------------------------------------------------------------------------------------------|




Just the other day I found this site, obtained the package, and tried some
of the examples.


http://www.cmis.csiro.au/S-PLUS/qtoolbox/index.html

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Holton, Blake
Sent: Tuesday, February 24, 2004 11:23 AM
To: R-Help (E-mail)
Subject: [R] Statistical Quality Control


Greetings,

I've been familiarizing myself with the features of R over the past few
days. I'm impressed with the quality and quantity of the features and
packages. One feature that I would be interested in would be a package for
statistical quality control. Does a package for statistical quality control
exist that I've been unable to locate?

If not, is anyone aware of efforts to develop a statistical quality control
package? It has been awhile since I've coded in C, but I would be willing
to
contribute.

Regards,

Blake

--------------------------------
Blake Holton
Technical Service Representative
Dynea
475 28th Street
Springfield, OR 97477-5100

Desk: 541.744.7238
Toll Free: 800.547.9525
FAX: 541.744.7249
Cell: 541.954.2696

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Jan.Verbesselt at agr.kuleuven.ac.be  Mon Mar  8 15:29:35 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Mon,  8 Mar 2004 15:29:35 +0100
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
In-Reply-To: <404C338C.7050800@bluewin.ch>
References: <Pine.LNX.4.44.0403080818360.28424-100000@gannet.stats>
	<404C338C.7050800@bluewin.ch>
Message-ID: <1078756175.404c834f94ec3@webmail2.kuleuven.be>

Thanks for the tips and advice!

I found out how the NA's came into the time series.  By a threshold
mechanism, extreme values (outliers) are removed from the time series
(environmental data, remote sensing data) and set as NA.  A solution
could be to detect an outlier and replace it by the quadratic fit of 2
values before and 2 values after the outlier.

In R, 'lsfit? does a least squares fit on the available data but can
this be used on time series since values are auto correlated?

Are there R-functions that "fit or interpolate" a new value based on a
set window around the outlier valid for seasonal time series? Which
function is the best to use? 

Thanks,
Jan



From lisas at salford-systems.com  Mon Mar  8 14:28:45 2004
From: lisas at salford-systems.com (Lisa Solomon)
Date: Mon, 08 Mar 2004 05:28:45 -0800
Subject: [R] 
 Creating CART: A Personal Retrospective, Opening Session of CART
 Data Mining 2004
Message-ID: <404C750D.7090705@salford-systems.com>

OPENING SESSION OF CART DATA MINING 2004, Monday March 22nd (no charge)
Welcoming Reception: 5:30 - 7:30 PM
Session Begins promptly at 7:30 PM

Personal recollections of Leo Breiman, Jerome Friedman, Richard Olshen,
Charles Stone. Join us to hear Breiman, Friedman, Olshen and Stone
discuss their early research interests and trace the ideas,
decisions, and chance events that culminated in CART, their landmark work.

RSVP to:
Lisa Solomon
lisas at salford-systems.com
619-543-8880 x14
http://www.cartdatamining.com

Location: Crowne Plaza San Francisco-Union Square
480 Sutter Street
San Francisco, CA 94108



From ripley at stats.ox.ac.uk  Mon Mar  8 18:33:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Mar 2004 17:33:54 +0000 (GMT)
Subject: [R] applying data generating function
In-Reply-To: <404C9D77.8060405@pdf.com>
Message-ID: <Pine.LNX.4.44.0403081708450.1285-100000@gannet.stats>

On Mon, 8 Mar 2004, Spencer Graves wrote:

>       With "gc()" right before each call to "proc.time", as Brian Ripley 
> and Gabor Grothendieck suggested, the times were substantially more 
> stable.  For the for loop, extending the vector with each of 1e5 
> iterations, I got 181.25, 181.27, 182.72, 182.44, and 182.56.  The 
> averages of the last 3 of these tests are as follows: 
> 
>                           10  100 1000 10000  1e+05
> for loop                   0 0.01 0.05  1.13 182.14
> gen e + for loop           0 0.00 0.03  0.26   2.58
> create storage + for loop  0 0.00 0.04  0.39   3.94
> sapply                     0 0.00 0.03  0.32   4.05
> replicate                  0 0.00 0.03  0.31   3.55
> 
> Without "gc()", I got 192.05, 182.02, 126.04, 130.30, and 118.64 for 
> extending the vector with each for loop iteration. 
> 
>       Three more observations about this: 
> 
>       1.  Without "gc()", the times started higher but declined by 
> roughly a third.  This suggests that R may actually be storing 
> intermediate "semi-compiled" code in "garbage" and using it when the 
> situation warrants -- but "gc()" discards it. 

I don't see anything in the code to allow for that possibility.

I believe it's down to the vagarities of garbage collection, and in
particular how the tuning of limits and the mix of level 0,1,2 gc's gets
adjusted during runs.  Here is a small experimental setup:

foo <- function(N)
{
  set.seed(123)
  gct <- gc.time()
  res <- system.time({
    f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
    v=c()
    x=.1 # starting point
    for (i in 1:N) { x=f(x); v=append(v,x) }
  })
  gct <- gc.time() - gct
  cbind(res, gct)
}

gc.time(TRUE)
gc()
> foo(1e4)
      res  gct
[1,] 1.39 1.12
[2,] 0.01 0.92
[3,] 1.41 1.07
[4,] 0.00 0.00
[5,] 0.00 0.00
> foo(1e5)
        res    gct
[1,] 218.68 242.86
[2,]  19.98 162.12
[3,] 238.83 246.10
[4,]   0.00   0.00
[5,]   0.00   0.00

so most (if not more than all) of the time is going on garbage collection,
something like 18000 gc's in the second run.

>       2.  Increasing N from 1e4 to 1e5 increased the time NOT by a 
> factor of 10 but by a factor of 161 = 182/1.13 when the length of the 
> vector was extended in each iteration. 

Right, but 9/10 of those additional allocations/garbage collections are of
longer objects than before and so that will take more time.  In
particular, objects of non-small size are directly allocated and freed, so
this will also depend on the speed of your malloc.  How the time to alloc 
n bytes depends on n will be very system-specific.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Mar  8 17:21:11 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 08 Mar 2004 08:21:11 -0800
Subject: [R] applying data generating function
In-Reply-To: <Pine.LNX.4.44.0403080726240.28242-100000@gannet.stats>
References: <Pine.LNX.4.44.0403080726240.28242-100000@gannet.stats>
Message-ID: <404C9D77.8060405@pdf.com>

      With "gc()" right before each call to "proc.time", as Brian Ripley 
and Gabor Grothendieck suggested, the times were substantially more 
stable.  For the for loop, extending the vector with each of 1e5 
iterations, I got 181.25, 181.27, 182.72, 182.44, and 182.56.  The 
averages of the last 3 of these tests are as follows: 

                          10  100 1000 10000  1e+05
for loop                   0 0.01 0.05  1.13 182.14
gen e + for loop           0 0.00 0.03  0.26   2.58
create storage + for loop  0 0.00 0.04  0.39   3.94
sapply                     0 0.00 0.03  0.32   4.05
replicate                  0 0.00 0.03  0.31   3.55

Without "gc()", I got 192.05, 182.02, 126.04, 130.30, and 118.64 for 
extending the vector with each for loop iteration. 

      Three more observations about this: 

      1.  Without "gc()", the times started higher but declined by 
roughly a third.  This suggests that R may actually be storing 
intermediate "semi-compiled" code in "garbage" and using it when the 
situation warrants -- but "gc()" discards it. 

      2.  Increasing N from 1e4 to 1e5 increased the time NOT by a 
factor of 10 but by a factor of 161 = 182/1.13 when the length of the 
vector was extended in each iteration. 

      3.  The fastest was indeed to generate "e" and allocate all 
required storage before entering the loop, but the major improvement was 
due to allocation of storage before initiating the for loop.  However, 
with 1000 or fewer iterations, the difference was hardly detectable. 

      Best Wishes,
      spencer graves

Prof Brian Ripley wrote:

>You need to run gc() before running such timings in R, as the first run 
>often has to pay for a level-0 garbage collection.  That is normally the 
>cause of (1), although I haven't seen differences as large as 10 secs (but 
>have no idea of the speed of your machine, and have seen 3 secs).
>
>On Sun, 7 Mar 2004, Spencer Graves wrote:
>
>  
>
>>      Peter's enumeration of alternatives inspired me to compare compute 
>>times for N = 10^(2:5), with the following results:      
>>
>>*** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
>>                          10  100 1000 10000  1e+05
>>for loop                   0 0.01 0.09  1.27 192.05
>>gen e + for loop           0 0.00 0.03  0.22   2.58
>>create storage + for loop  0 0.01 0.05  0.34   3.45
>>sapply                     0 0.00 0.04  0.28   3.82
>>replicate                  0 0.01 0.05  0.29   4.02
>>
>>      I repeated this with the "for loop" both first and last.  The 
>>times tended to decline on replication, with the "for loop" time for N = 
>>1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
>>last), and 118.64 ("for loop" first again). 
>>
>>      Conclusions: 
>>     
>>      (1) Apparently, in some cases, R picks up speed upon replication
>>
>>      (2) The first 3 times for the "for loop" with N = 1e5 made me 
>>wonder if there was an order effect, with the "for loop" being longer in 
>>the first position.  However, the last run with the "for loop" again 
>>first had the shortest time of 118.64, contradicting that hypothesis. 
>>
>>      By comparison, I also tried this under S-Plus 6.2: 
>>
>>*** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
>>                           10  100  1000 10000  100000
>>                 for loop 0.01 0.05 0.331 3.976 273.073
>>         gen e + for loop 0.00 0.04 0.320 3.154  29.112
>>create storage + for loop 0.01 0.03 0.231 2.113  22.242
>>                   sapply 0.00 0.04 0.380 4.757  23.003
>>
>>      The script I used appears below.  As Peter said, "the only really 
>>crucial [issue] is to avoid the inefficient append by preallocating" the 
>>vectors to be generated.  Moreover, this is only an issue for long loop, 
>>with a threshold of between 1e4 and 1e5 in this example.  For shorter 
>>loops, the programmers' time is far more valuable. 
>>
>>Enjoy.  spencer graves
>>####################
>>
>>
>>N.gen <- c(10, 100, 1000, 10000, 1e5)
>>mtds <- c("for loop", "gen e + for loop", "create storage + for loop",
>>    "sapply", "replicate")
>>m <- length(N.gen)   
>>ellapsed.time <- array(NA, dim=c(m, length(mtds)))
>>dimnames(ellapsed.time) <- list(N.gen, mtds)
>>   
>>for(iN in 1:m){
>>    cat("\n", iN, "")
>>    N <- N.gen[iN]
>>#for loop
>>set.seed(123)
>>start.time <- proc.time()
>>f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
>>v=c()
>>x=.1 # starting point
>>for (i in 1:N) { x=f(x); v=append(v,x) }
>>ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3]   
>>cat(mtds[1], "")
>>
>>#gen e + for loop
>>set.seed(123)
>>start.time <- proc.time()
>>e <- 0.001*rnorm(N)
>>X <- rep(0.1, N+1)
>>for(i in 2:(N+1))
>>    X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
>>ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
>>cat(mtds[2], "")
>>
>>#create storage + for loop 
>>set.seed(123)
>>start.time <- proc.time()
>>V <- numeric(N)
>>xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
>>ellapsed.time[iN, "create storage + for loop"] <- 
>>(proc.time()-start.time)[3]
>>cat(mtds[3], "")
>>
>>#sapply
>>set.seed(123)
>>start.time <- proc.time()
>>xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
>>ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3]   
>>cat(mtds[4], "")
>>
>>if(!is.null(version$language)){
>>#replicate
>>set.seed(123)
>>start.time <- proc.time()
>>z <- .1 ; vr <- replicate(N, z <<- f(z))
>>ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
>>cat(mtds[5], "")
>>}
>>
>>}
>>
>>t(ellapsed.time)
>>#############################
>>Peter Dalgaard wrote:
>>
>>    
>>
>>>Christophe Pallier <pallier at lscp.ehess.fr> writes:
>>>
>>> 
>>>
>>>      
>>>
>>>>Fred J. wrote:
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>>>I need to generate a data set based on this equation
>>>>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
>>>>>N(0,0,001) random variable
>>>>>I need say 100 values.
>>>>>
>>>>>How do I do this?
>>>>>     
>>>>>
>>>>>          
>>>>>
>>>>I assume X(t) and x(t) are the same (?).
>>>>
>>>>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
>>>>v=c()
>>>>x=.1 # starting point
>>>>for (i in 1:100) { x=f(x); v=append(v,x) }
>>>>
>>>>There may be smarter ways...
>>>>   
>>>>
>>>>        
>>>>
>>>Yes, but the only really crucial one is to avoid the inefficient append  by
>>>preallocating the v: 
>>>
>>>v <- numeric(100)
>>>x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
>>>
>>>apart from that you can use implicit loops:
>>>
>>>x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
>>>
>>>or
>>>
>>>z <- .1 ; v <- replicate(100, z <<- f(z))
>>>
>>>(You cannot use x there because of a variable capture issue which is a
>>>bit of a bug. I intend to fix it for 1.9.0.)
>>>
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>  
>



From ozric at web.de  Mon Mar  8 15:50:01 2004
From: ozric at web.de (Christian Schulz)
Date: Mon, 8 Mar 2004 15:50:01 +0100
Subject: [R] years from as.POSIXlt 
Message-ID: <200403081550.01997.ozric@web.de>

Hi,

how it's possible to extract the year and  the number
of days from Julian date. i'm little confused about the last two
functions and ?years .

EDATE  comes from sqlQuery with as.is=T
EDATE <- as.POSIXlt(datvears$ENROLLDAY)

Many thanks, Christian


> EDATE[1:5] 
[1] "2000-06-30 11:25:01" "2000-06-30 11:39:55" "2000-06-30 12:11:11"
[4] "2000-06-30 12:13:32" "2000-06-30 12:50:12"
> weekdays(EDATE[1:5])
[1] "Freitag" "Freitag" "Freitag" "Freitag" "Freitag"
> months(EDATE[1:5])
[1] "Juni" "Juni" "Juni" "Juni" "Juni"
> years(EDATE[1:5])
NULL
> days(EDATE[1:5])
NULL
>



From cfinet at ens-lyon.fr  Mon Mar  8 16:38:26 2004
From: cfinet at ens-lyon.fr (cfinet@ens-lyon.fr)
Date: Mon, 08 Mar 2004 16:38:26 +0100 (CET)
Subject: [R] question
Message-ID: <1078760306.404c937277a2c@mouette.ens-lyon.fr>

I do not manage to make a Fisher?s exact test with the next matrix :

    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]    1    3    0    1    2    9    0    0    2     5     8     6
[2,]    0    3    3    0    0    0    0    5    0     3     0     0
[3,]    0    0    0    0    0   10    0    0    0     0    10     0
[4,]    0    2    0    2    6   14    0    0    6     0    10     6
[5,]    0    5    0    0    0    7    0    0    0     2     8     0
[6,]    0    0    1    9    4    7    2    1    4     2    12     5
[7,]    0    6    0    0    0    0    0    0    0     5     1     3

but I do not understand why it does not work since I obtain the next error 
message :



> fisher.test(enfin.matrix)
Error in fisher.test(enfin.matrix) : Bug in FEXACT: gave negative key

thank you for considering my application

c?dric finet



From brahm at alum.mit.edu  Mon Mar  8 17:22:39 2004
From: brahm at alum.mit.edu (David Brahm)
Date: Mon, 8 Mar 2004 11:22:39 -0500
Subject: [R] unusual name in .Rd file: documenting an infix function
References: <4048967F.7080109@wsl.ch>
Message-ID: <16460.40399.737818.711938@arbres1a.fmr.com>

Christian Hoffmann <christian.hoffmann at wsl.ch> wrote:
> I am having difficulties documenting an infix function:
> "%&%" <- function(x,y) { paste(x,y,sep="") }

We have the exact same function, and the following documentation works fine.
Note the documentation filename is "g.am.Rd", but that's just a random name,
unrelated to the function.  Also note that "&" is escaped in the name but not
in the alias!

\name{\%\&\%}
\alias{\%&\%}
\title{Concatenate strings}
\description{a \%&\% b  converts a and b into strings and concatenates them.}
\usage{a \%&\% b}
\value{A string, the concatenation of a and b.}
\examples{3 \%&\% "RAJA"}
\keyword{character}

-- 
                              -- David Brahm (brahm at alum.mit.edu)



From gracestat at yahoo.com  Mon Mar  8 18:54:08 2004
From: gracestat at yahoo.com (Grace Conlon)
Date: Mon, 8 Mar 2004 09:54:08 -0800 (PST)
Subject: [R] abline
Message-ID: <20040308175408.98816.qmail@web21403.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040308/8b2100e4/attachment.pl

From Benjamin.STABLER at odot.state.or.us  Mon Mar  8 21:07:56 2004
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Mon, 8 Mar 2004 12:07:56 -0800 
Subject: [R] Decision Trees
Message-ID: <76A000A82289D411952F001083F9DD06047FE500@exsalem4-bu.odot.state.or.us>

I am familiar with the rpart and tree packages for classification and
regression trees.  However, quite a bit of the research in the
transportation community relating to decision trees uses the C4.5 family of
algorithms by Quinlan.  Are there any plans to make a C4.5 (or a derivative
of it) available to R?  If not, then I might use the WEKA Java package (
http://www.cs.waikato.ac.nz/ml/weka) that implements a Java version of C4.5.
Thanks.  

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Mar  8 17:54:43 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 8 Mar 2004 17:54:43 +0100 (CET)
Subject: [R] normal scores test
In-Reply-To: <200403061933.i26JX4St011251@xyzzy-3.tamu.edu>
References: <200403061933.i26JX4St011251@xyzzy-3.tamu.edu>
Message-ID: <Pine.LNX.4.51.0403081754090.989@artemis.imbe.med.uni-erlangen.de>


On Sat, 6 Mar 2004, Padmanabhan, Sudharsha wrote:

>
> Hello,
>
> I need help in performing a Van_der_Waerden normal scores test in R. I
> have two arrays of scores(final on therapy scores from drug and placebo) and
> want to use the normal scores procdeure to test for significance.
> (observations are unequal in number - due to dropouts). Could you please help
> me out with the coding or let me know if there is a package that can be used
> (for example, suppdist etc.)
>


?dperm in package `exactRankTests' has an example:

R> library(exactRankTests)
R>
R> n <- 10
R> x <- rnorm(n)
R> y <- rnorm(n)
R> scores <- cscores(c(x,y), type="NormalQuantile")
R>
R> X <- sum(scores[seq(along=x)])  # <- v.d. Waerden normal quantile statistic
R>
R> # critical value, two-sided test
R>
R> abs(qperm(0.025, scores, length(x), simulate = TRUE, B = 10000))
[1] 3.854136
R>
R> # p-value
R>
R> pperm(X, scores, length(x), alternative="two.sided",
+       simulate = TRUE, B = 10000)
[1] 0.1701

Best,

Torsten

> Thanks in advance
>
> Regards
>
> ~S
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From apjaworski at mmm.com  Mon Mar  8 23:47:18 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Mon, 8 Mar 2004 16:47:18 -0600
Subject: [R] test
Message-ID: <OF779FA7E1.2F65A657-ON86256E51.007D07E5-86256E51.007D2E6B@mmm.com>





I am trying to test whether I can post to the list.  I responded to a
couple of messages recently and I have not seen my response posted.

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122



From p.dalgaard at biostat.ku.dk  Tue Mar  9 00:13:13 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Mar 2004 00:13:13 +0100
Subject: [R] memory problem
In-Reply-To: <27C204BD76CBC142BA1AE46D62A8548ED9E906@nihexchange9.nih.gov>
References: <27C204BD76CBC142BA1AE46D62A8548ED9E906@nihexchange9.nih.gov>
Message-ID: <x2vflfx792.fsf@biostat.ku.dk>

"Joshi, Nina (NIH/NCI)" <joshini at mail.nih.gov> writes:

> I am trying to upload into R 143 Affymetrix chips onto using R on the NIH
> Nimbus server.  I can load 10 chips without a problem, however, when I try
> to load 143 I receive a error message: cannot create a vector of 523263 KB.
> I have expanded the memory of R as follows:  R --min-vsize=10M
> --max-vsize=2500M --min-nsize=10M -max-nsize=50M (as specified in help in
> R).  After running this command the memory in R is as follows:  
> 
>  
> 
>             Used     (Mb)      gc    trigger        (Mb)        limit
> (Mb)      
> 
> Ncells   513502 13.8        10485760        280.0                1400
> 
> Vcells   142525   1.1        162625696     1240.8                2500
> 
>  
> 
> However, I am still getting the error cannot create a vector of 523263 KB.
> Any suggestions/ideas? 

Well, it basically means that you're running out of memory. The 523263
KB is just the last request that R tries to honour when it fails, but
it is in fact a whole half gigabyte. I forget what the memory usage per
Affy chip is (and it depends on the chip, I suppose), but how much did
the 10 chips take? Do you have space for 14 times that amount? 

It is not unlikely that you're running into the limits of the 32bit
address space, in which case there is little to do but to move to a
64bit platform (with plenty of swap space).

(Note that you may get replies from more specifically experienced
people on the Bioconductor lists.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jgentry at jimmy.harvard.edu  Tue Mar  9 00:20:29 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Mon, 8 Mar 2004 18:20:29 -0500 (EST)
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
Message-ID: <Pine.SOL.4.20.0403081812080.25526-100000@santiam.dfci.harvard.edu>

Hello ...

Using R-1.9.0 alpha, I'm having some problem getting a few packages to
pass check under Windows - specifically with the 'missing link(s)' section
of the package install phase.

I started trying to track down how the missing link was showing up as in
some cases I could not see why the link was considered missing.  For
instance, in one package it was looking for 'makeViewers' although when
the packages is loaded in R, 'makeViewers' is even in the search path (by
way of a dependent package which was loaded initially).

In trying to figure out how these things are determiend, I noticed that in
Windows there was a difference in the reported missing links when one does
'Rcmd check' vs 'Rcmd install' (or 'Rcmd install --build').  In the
example above, using either of the 'install' methods results in that link
not being reported as missing (and if I put in an intentionally missing
link it gets picked up as such), but 'check' reports it as missing.  I'm
wondering what the difference in environment is between check & install on
Windows as that might help me to figure out why check reports these as
missing links.

Thanks
-Jeff



From andy_liaw at merck.com  Tue Mar  9 00:50:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 8 Mar 2004 18:50:29 -0500
Subject: [R] applying data generating function
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7942@usrymx25.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Monday, March 08, 2004 11:21 AM
> To: Prof Brian Ripley
> Cc: Fred J.; r-help; Peter Dalgaard
> Subject: Re: [R] applying data generating function
> 
> 
>       With "gc()" right before each call to "proc.time", as 
> Brian Ripley 
> and Gabor Grothendieck suggested, the times were substantially more 
> stable.  For the for loop, extending the vector with each of 1e5 
> iterations, I got 181.25, 181.27, 182.72, 182.44, and 182.56.  The 
> averages of the last 3 of these tests are as follows: 
> 
>                           10  100 1000 10000  1e+05
> for loop                   0 0.01 0.05  1.13 182.14
> gen e + for loop           0 0.00 0.03  0.26   2.58
> create storage + for loop  0 0.00 0.04  0.39   3.94
> sapply                     0 0.00 0.03  0.32   4.05
> replicate                  0 0.00 0.03  0.31   3.55
> 
> Without "gc()", I got 192.05, 182.02, 126.04, 130.30, and 118.64 for 
> extending the vector with each for loop iteration. 
> 
>       Three more observations about this: 
> 
>       1.  Without "gc()", the times started higher but declined by 
> roughly a third.  This suggests that R may actually be storing 
> intermediate "semi-compiled" code in "garbage" and using it when the 
> situation warrants -- but "gc()" discards it. 
> 
>       2.  Increasing N from 1e4 to 1e5 increased the time NOT by a 
> factor of 10 but by a factor of 161 = 182/1.13 when the length of the 
> vector was extended in each iteration. 

Growing a vector inside for loop is expensive and inefficient because R
needs to allocate a new vector of sufficient length, copy the data over,
then de-reference the old vector.  (This my understanding of how it works.
Hopefully I'm not too far off...)

HTH,
Andy
 
>       3.  The fastest was indeed to generate "e" and allocate all 
> required storage before entering the loop, but the major 
> improvement was 
> due to allocation of storage before initiating the for loop.  
> However, 
> with 1000 or fewer iterations, the difference was hardly detectable. 
> 
>       Best Wishes,
>       spencer graves
> 
> Prof Brian Ripley wrote:
> 
> >You need to run gc() before running such timings in R, as 
> the first run 
> >often has to pay for a level-0 garbage collection.  That is 
> normally the 
> >cause of (1), although I haven't seen differences as large 
> as 10 secs (but 
> >have no idea of the speed of your machine, and have seen 3 secs).
> >
> >On Sun, 7 Mar 2004, Spencer Graves wrote:
> >
> >  
> >
> >>      Peter's enumeration of alternatives inspired me to 
> compare compute 
> >>times for N = 10^(2:5), with the following results:      
> >>
> >>*** R 1.8.1 under Windows 2000, IBM Thinkpad T30: 
> >>                          10  100 1000 10000  1e+05
> >>for loop                   0 0.01 0.09  1.27 192.05
> >>gen e + for loop           0 0.00 0.03  0.22   2.58
> >>create storage + for loop  0 0.01 0.05  0.34   3.45
> >>sapply                     0 0.00 0.04  0.28   3.82
> >>replicate                  0 0.01 0.05  0.29   4.02
> >>
> >>      I repeated this with the "for loop" both first and last.  The 
> >>times tended to decline on replication, with the "for loop" 
> time for N = 
> >>1e5 = 182.02, 126.04 (with the "for loop" last), 130.30 ("for loop" 
> >>last), and 118.64 ("for loop" first again). 
> >>
> >>      Conclusions: 
> >>     
> >>      (1) Apparently, in some cases, R picks up speed upon 
> replication
> >>
> >>      (2) The first 3 times for the "for loop" with N = 1e5 made me 
> >>wonder if there was an order effect, with the "for loop" 
> being longer in 
> >>the first position.  However, the last run with the "for 
> loop" again 
> >>first had the shortest time of 118.64, contradicting that 
> hypothesis. 
> >>
> >>      By comparison, I also tried this under S-Plus 6.2: 
> >>
> >>*** S-Plus 6.2, Windows 2000, IBM Thinkpad T30 ("for loop" first): 
> >>                           10  100  1000 10000  100000
> >>                 for loop 0.01 0.05 0.331 3.976 273.073
> >>         gen e + for loop 0.00 0.04 0.320 3.154  29.112
> >>create storage + for loop 0.01 0.03 0.231 2.113  22.242
> >>                   sapply 0.00 0.04 0.380 4.757  23.003
> >>
> >>      The script I used appears below.  As Peter said, "the 
> only really 
> >>crucial [issue] is to avoid the inefficient append by 
> preallocating" the 
> >>vectors to be generated.  Moreover, this is only an issue 
> for long loop, 
> >>with a threshold of between 1e4 and 1e5 in this example.  
> For shorter 
> >>loops, the programmers' time is far more valuable. 
> >>
> >>Enjoy.  spencer graves
> >>####################
> >>
> >>
> >>N.gen <- c(10, 100, 1000, 10000, 1e5)
> >>mtds <- c("for loop", "gen e + for loop", "create storage + 
> for loop",
> >>    "sapply", "replicate")
> >>m <- length(N.gen)   
> >>ellapsed.time <- array(NA, dim=c(m, length(mtds)))
> >>dimnames(ellapsed.time) <- list(N.gen, mtds)
> >>   
> >>for(iN in 1:m){
> >>    cat("\n", iN, "")
> >>    N <- N.gen[iN]
> >>#for loop
> >>set.seed(123)
> >>start.time <- proc.time()
> >>f<-function (x.) { 3.8*x.*(1-x.) + rnorm(1,0,.001) }
> >>v=c()
> >>x=.1 # starting point
> >>for (i in 1:N) { x=f(x); v=append(v,x) }
> >>ellapsed.time[iN, "for loop"] <- (proc.time()-start.time)[3]   
> >>cat(mtds[1], "")
> >>
> >>#gen e + for loop
> >>set.seed(123)
> >>start.time <- proc.time()
> >>e <- 0.001*rnorm(N)
> >>X <- rep(0.1, N+1)
> >>for(i in 2:(N+1))
> >>    X[i] <- (3.8*X[i-1]*(1-X[i-1])+e[i-1])
> >>ellapsed.time[iN, "gen e + for loop"] <- (proc.time()-start.time)[3]
> >>cat(mtds[2], "")
> >>
> >>#create storage + for loop 
> >>set.seed(123)
> >>start.time <- proc.time()
> >>V <- numeric(N)
> >>xv <- .1 ; for (i in 1:N) { xv <- f(xv); V[i] <- xv }
> >>ellapsed.time[iN, "create storage + for loop"] <- 
> >>(proc.time()-start.time)[3]
> >>cat(mtds[3], "")
> >>
> >>#sapply
> >>set.seed(123)
> >>start.time <- proc.time()
> >>xa <- .1 ; va <- sapply(1:N, function(i) xa <<- f(xa))
> >>ellapsed.time[iN, "sapply"] <- (proc.time()-start.time)[3]   
> >>cat(mtds[4], "")
> >>
> >>if(!is.null(version$language)){
> >>#replicate
> >>set.seed(123)
> >>start.time <- proc.time()
> >>z <- .1 ; vr <- replicate(N, z <<- f(z))
> >>ellapsed.time[iN, "replicate"] <- (proc.time()-start.time)[3]
> >>cat(mtds[5], "")
> >>}
> >>
> >>}
> >>
> >>t(ellapsed.time)
> >>#############################
> >>Peter Dalgaard wrote:
> >>
> >>    
> >>
> >>>Christophe Pallier <pallier at lscp.ehess.fr> writes:
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>>>Fred J. wrote:
> >>>>
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>>>I need to generate a data set based on this equation
> >>>>>X(t) = 3.8x(t-1) (1-x(t-1)) + e(t), where e(t) is a
> >>>>>N(0,0,001) random variable
> >>>>>I need say 100 values.
> >>>>>
> >>>>>How do I do this?
> >>>>>     
> >>>>>
> >>>>>          
> >>>>>
> >>>>I assume X(t) and x(t) are the same (?).
> >>>>
> >>>>f<-function (x) { 3.8*x*(1-x) + rnorm(1,0,.001) }
> >>>>v=c()
> >>>>x=.1 # starting point
> >>>>for (i in 1:100) { x=f(x); v=append(v,x) }
> >>>>
> >>>>There may be smarter ways...
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>Yes, but the only really crucial one is to avoid the 
> inefficient append  by
> >>>preallocating the v: 
> >>>
> >>>v <- numeric(100)
> >>>x <- .1 ; for (i in 1:100) { x <- f(x); v[i] <- x }
> >>>
> >>>apart from that you can use implicit loops:
> >>>
> >>>x <- .1 ; v <- sapply(1:100, function(i) x <<- f(x))
> >>>
> >>>or
> >>>
> >>>z <- .1 ; v <- replicate(100, z <<- f(z))
> >>>
> >>>(You cannot use x there because of a variable capture 
> issue which is a
> >>>bit of a bug. I intend to fix it for 1.9.0.)
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >>
> >>
> >>    
> >>
> >
> 
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From spencer.graves at pdf.com  Tue Mar  9 01:04:08 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 08 Mar 2004 16:04:08 -0800
Subject: [R] abline
In-Reply-To: <20040308175408.98816.qmail@web21403.mail.yahoo.com>
References: <20040308175408.98816.qmail@web21403.mail.yahoo.com>
Message-ID: <404D09F8.8010504@pdf.com>

?abline in R 1.8.1 includes the following: 

Usage:

     abline(a, b, untf = FALSE, ...)
     abline(h=, untf = FALSE, ...)
     abline(v=, untf = FALSE, ...)
     abline(coef=, untf = FALSE, ...)
     abline(reg=, untf = FALSE, ...)

      It looks to me like you want "abline(h=h)".  hope this helps.  
spencer graves

Grace Conlon wrote:

>if I want to specify y-coordinates for the heights of horizontal lines to go across a plot.
> 
>x <- c(1,2,3,6,4,8,4,7)
>y <- c(3,2,7,4,5,4,5,6)
>h <- c(3,5,7)
>plot(x,y)
>abline(y=h)
>
>However I got error message:
>Warning message: 
>parameter "y" couldn't be set in high-level plot() function 
>(I tried abline(h=y) , it;s not what I want also)
>
>Do u know why?
> 
>
>
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From andy_liaw at merck.com  Tue Mar  9 01:04:17 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 8 Mar 2004 19:04:17 -0500
Subject: [R] abline
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7943@usrymx25.merck.com>

So exactly what do you want?  

h <- c(3,5,7)
abline(h=h)

draws three horizontal lines at y=3, 5, and 7, as the help page for abline
says it should.

[Note: that's abline(h=h), not abline(y=h).]

Andy

> From: Grace Conlon
> 
> if I want to specify y-coordinates for the heights of 
> horizontal lines to go across a plot.
>  
> x <- c(1,2,3,6,4,8,4,7)
> y <- c(3,2,7,4,5,4,5,6)
> h <- c(3,5,7)
> plot(x,y)
> abline(y=h)
> 
> However I got error message:
> Warning message: 
> parameter "y" couldn't be set in high-level plot() function 
> (I tried abline(h=y) , it;s not what I want also)
> 
> Do u know why?
>  
> 
> 
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From wang at galton.uchicago.edu  Tue Mar  9 01:05:47 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Mon, 8 Mar 2004 18:05:47 -0600 (CST)
Subject: [R] Re: R-help Digest, Vol 13, Issue 8
In-Reply-To: <200403081104.i28B2x04014300@hypatia.math.ethz.ch>
References: <200403081104.i28B2x04014300@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0403081805150.5785@galton.uchicago.edu>



On Mon, 8 Mar 2004 r-help-request at stat.math.ethz.ch wrote:

> Send R-help mailing list submissions to
> 	r-help at stat.math.ethz.ch
> 
> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> or, via email, send a message with subject or body 'help' to
> 	r-help-request at stat.math.ethz.ch
> 
> You can reach the person managing the list at
> 	r-help-owner at stat.math.ethz.ch
> 
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of R-help digest..."
> 
> 
> Today's Topics:
> 
>    1. Re: "Statistiques avec R" (pallier)
>    2. Re: graphic device MetaPost (Prof Brian Ripley)
>    3. Re: graphic device MetaPost (Gabor Grothendieck)
>    4. Re: graphic device MetaPost (Itay Furman)
>    5. applying data generating function (Fred J.)
>    6. missing values (Grace Conlon)
>    7. drawing filled countries according to data using
>       map('world')? (Jens Hainmueller)
>    8. Re: missing values (Peter Dalgaard)
>    9. RE: applying data generating function (Phineas Campbell)
>   10. Re: applying data generating function (Christophe Pallier)
>   11. Re: applying data generating function (Spencer Graves)
>   12. Frequency ploygon help (Robert W. Baer, Ph.D.)
>   13. Re: applying data generating function (Peter Dalgaard)
>   14. Re: drawing filled countries according to data using
>       map('world')? (Ray Brownrigg)
>   15. Excel files (Grace Conlon)
>   16. drawing filled countries according to data using
>       map('world')?	- follow up (Jens Hainmueller)
>   17. Re: Excel files (Robert W. Baer, Ph.D.)
>   18. Re: applying data generating function (Spencer Graves)
>   19. Re: Excel files (Gabor Grothendieck)
>   20. Re: applying data generating function (Gabor Grothendieck)
>   21. Re: drawing filled countries according to data using
>       map('world')?	- follow up (Spencer Graves)
>   22. Re: applying data generating function (Spencer Graves)
>   23. Re: applying data generating function (Gabor Grothendieck)
>   24. Re: applying data generating function (Prof Brian Ripley)
>   25. Re: Internal NA removal out of Time Series with na.omit.ts()
>       (Adrian Trapletti)
>   26. Re: Internal NA removal out of Time Series with na.omit.ts()
>       (Prof Brian Ripley)
>   27. Re: Internal NA removal out of Time Series with na.omit.ts()
>       (Adrian Trapletti)
>   28. lm - significance disappears (Stuart Leask)
>   29. getting the std errors in the lm function  (Fulvio Copex)
>   30. dsn (Margarida J?lia Rodrigues Igreja)
>   31. Re: lm - significance disappears (Prof Brian Ripley)
> 
> 
> ----------------------------------------------------------------------
> 
> Message: 1
> Date: Sun, 07 Mar 2004 13:21:56 +0100
> From: pallier <pallier at lscp.ehess.fr>
> Subject: Re: [R] "Statistiques avec R"
> To: r-help <r-help at stat.math.ethz.ch>
> Message-ID: <404B13E4.8010909 at lscp.ehess.fr>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> Shigeru Mase wrote:
> 
> >
> > Probably you may be curious about the mysterious author
> > of "Statistiques avec R" as well as me.  He seems a mathematician.
> > I found one more extraordinary work of him.
> >
> > http://tex.loria.fr/prod-graph/zoonekynd/metapost/metapost.html
> >
> > He made Metapost (a kind of Metafont software which produces
> > ps outputs) a statistical graphics software. Look and have a fun!
> 
> 
> Vincent Zoonekynd is *not* the creator of Metapost. John D. Hobby is 
> (cf. http://cm.bell-labs.com/who/hobby/index.html).
> 
> 
> Vincent Zoonekynd indeed has a ph.D. in maths from the university of 
> Paris 7 (Jussieu). On his homepage (http://zoonek.free.fr/), he says he 
> is looking for a job in computer science or bioinformatics (last update: 
> october 2003). The job market is very gloomy for young scientists in 
> France. I do not know if he's looking for a job in a foreign country, 
> but his CV indicates that he speaks English, and has a medium level in 
> Japanese.
> 
> He wrote the "Statistiques avec R" notes while learning R. I like these 
> pages very much too, but they are notes taken by a mathematician 
> learning statistics. In some parts, he has not yet fully grasped all the 
> concepts (but he indicates this, and he's stressing that this is a work 
> in progress). This remark does not detract from the quality and the 
> usefulness of his work.
> 
> Christophe Pallier
> http://www.pallier.org
> 
> 
> 
> ------------------------------
> 
> Message: 2
> Date: Sun, 7 Mar 2004 14:42:17 +0000 (GMT)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Subject: Re: [R] graphic device MetaPost
> To: =?gb2312?q?Jinsong=20Zhao?= <zh_jinsong at yahoo.com.cn>
> Cc: rhelp <r-help at stat.math.ethz.ch>
> Message-ID: <Pine.LNX.4.44.0403071430370.24043-100000 at gannet.stats>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Sun, 7 Mar 2004, [gb2312] Jinsong Zhao wrote:
> 
> > By default, MetaPost passes all text through TeX. This has the
> > advantage of allowing essentially any TeX symbols in titles and labels.
> > It give us, who use the multibyte character in ordinary communication,
> > much convenience. Gnuplot has fulfilled this function, and it give me a
> > deep impression for I could use Chinese character in plots with a minor
> > modification to the MetaPost file.
> 
> I don't think so: you would still need character metrics for the fonts you 
> use to be able to centre them, for example.
> 
> > I hope the R Development Core Team could consider MetaPost as a graphic
> > device in future R version.
> 
> We would welcome your contributing such a device.  Note though that 
> there is a public API for graphics devices, and so you could just 
> contribute the device to CRAN.
> 
> If perchance you meant `I want the R core team to write a metapost device
> for me', then you have not grasped how Open Source projects work.
> 
> There are some plans for internationalization of R via UTF-8, but this
> will be a considerable amount (man months?) of work, and of very little
> benefit to any of the core developers.  Volunteers would be welcome.
> 
>



From wang at galton.uchicago.edu  Tue Mar  9 01:18:28 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Mon, 8 Mar 2004 18:18:28 -0600 (CST)
Subject: [R] how to replace NaN in a vector
In-Reply-To: <200403081104.i28B2x04014300@hypatia.math.ethz.ch>
References: <200403081104.i28B2x04014300@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0403081806260.5785@galton.uchicago.edu>

Hi, all

a vector such as

(1,2,4,-1,NaN,2,4,NaN)

if try to replace all the NaN with a numerical value, what's the easiest 
way?
thanks a lot



best
yong



From jens_hainmueller at ksg05.harvard.edu  Tue Mar  9 04:36:45 2004
From: jens_hainmueller at ksg05.harvard.edu (Jens Hainmueller)
Date: Mon, 8 Mar 2004 19:36:45 -0800
Subject: [R] vector extraction
Message-ID: <HCECJPLNNGBBJIOJMJJAIEIOCEAA.jens_hainmueller@ksg05.harvard.edu>

Hello,

I could need some help on this one:

>From the data.frame "Test.dataset2" below (TSCS data for 151
"countries.to.map" for "year" 1973-95; each "country.to.map" is described by
a unique code), I would like to extract a vector "color" that for each
"country.to.map" takes on the value of "dv" (a categorical variable with
values 1,2,..4) for a specified "year". Thus, for a specified "year",
"color" should have 151 obs - one for each "country.to.map" represented by
its respective value of "dv").

I tried this:

> color <-  dv[country.to.map][(year == 1980)[country.to.map]]

but it does not give me what I need. I can't figure out where my error is,
however.

Thanks,
Jens

> Test.dataset2[1:40,]
 country.to.map dv year
1          1936 NA 1973
2          1936 NA 1974
3          1936 NA 1975
4          1936 NA 1976
5          1936 NA 1977
6          1936 NA 1978
7          1936 NA 1979
8          1936 NA 1980
9          1936 NA 1981
10         1936 NA 1982
11         1936 NA 1983
12         1936 NA 1984
13         1936 NA 1985
14         1936 NA 1986
15         1936 NA 1987
16         1936 NA 1988
17         1936  4 1989
18         1936  4 1990
19         1936  4 1991
20         1936  4 1992
21         1936  4 1993
22         1936  4 1994
23         1936  4 1995
24           56  4 1973
25           56  4 1974
26           56  2 1975
27           56  2 1976
28           56  2 1977
29           56  2 1978
30           56  2 1979
31           56  2 1980
32           56  2 1981
33           56  2 1982
34           56  4 1983
35           56  4 1984
36           56  4 1985
37           56  4 1986
38           56  4 1987
39           56  4 1988
40           56  4 1989



From xiaoliu at jhmi.edu  Tue Mar  9 01:53:40 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Mon, 08 Mar 2004 19:53:40 -0500
Subject: [R] abline
Message-ID: <7aa0b27abc09.7abc097aa0b2@jhmimail.jhmi.edu>

> x <- c(1,2,3,6,4,8,4,7)
> y <- c(3,2,7,4,5,4,5,6)
> h <- c(3,5,7)
> plot(x,y)
> abline(y=h)

abline(h=h) will work.



From ggrothendieck at myway.com  Tue Mar  9 02:14:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Mar 2004 20:14:08 -0500 (EST)
Subject: [R] years from as.POSIXlt 
Message-ID: <20040309011408.68403399A@mprdmxin.myway.com>



days and years are from the chron package and operate on
chron objects, not POSIXlt objects.

If you have a POSIXlt object, EDATE, you can get the years 
and julian day of the year like this:

as.numeric(format(EDATE,"%Y"))
as.numeric(format(EDATE,"%j"))

See ?strptime for those % codes and many others.

Alternately you can convert your dates to chron objects
instead of POSIXlt objects and then you can use years and 
days as you did below.  See the chron package for that.

---

Date:   Mon, 8 Mar 2004 15:50:01 +0100 
From:   Christian Schulz <ozric at web.de>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] years from as.POSIXlt  

 
Hi,

how it's possible to extract the year and the number
of days from Julian date. i'm little confused about the last two
functions and ?years .

EDATE comes from sqlQuery with as.is=T
EDATE <- as.POSIXlt(datvears$ENROLLDAY)

Many thanks, Christian


> EDATE[1:5] 
[1] "2000-06-30 11:25:01" "2000-06-30 11:39:55" "2000-06-30 12:11:11"
[4] "2000-06-30 12:13:32" "2000-06-30 12:50:12"
> weekdays(EDATE[1:5])
[1] "Freitag" "Freitag" "Freitag" "Freitag" "Freitag"
> months(EDATE[1:5])
[1] "Juni" "Juni" "Juni" "Juni" "Juni"
> years(EDATE[1:5])
NULL
> days(EDATE[1:5])
NULL
>



From ggrothendieck at myway.com  Tue Mar  9 02:26:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Mar 2004 20:26:57 -0500 (EST)
Subject: [R] abline
Message-ID: <20040309012657.1EB433978@mprdmxin.myway.com>



abline does not have a y parameter so it is letting
you know that you have specified an invalid parameter.
See

?abline

Issuing the command

args(abline)

is another way to find out the arguments to a command.

At any rate, based on your description I think you want
segments. See

?segments

---
Date:   Mon, 8 Mar 2004 09:54:08 -0800 (PST) 
From:   Grace Conlon <gracestat at yahoo.com>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] abline 

 
if I want to specify y-coordinates for the heights of horizontal lines to go across a plot.

x <- c(1,2,3,6,4,8,4,7)
y <- c(3,2,7,4,5,4,5,6)
h <- c(3,5,7)
plot(x,y)
abline(y=h)

However I got error message:
Warning message: 
parameter "y" couldn't be set in high-level plot() function 
(I tried abline(h=y) , it;s not what I want also)

Do u know why?



From ggrothendieck at myway.com  Tue Mar  9 02:53:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Mar 2004 20:53:48 -0500 (EST)
Subject: [R] a question on CSV file
Message-ID: <20040309015348.AC1123973@mprdmxin.myway.com>



Note that a CSV file is one in which data elements
are separated by commas.  There are no commas in
what you display so I assume that you are displaying
the data after you have read it in, not the CSV file.

If you read a file into a data frame x you can refer to
row i column j of the data frame as x[i,j].

The subset of rows of data frame x with column Med equal to 1 is
   x[,x$Med == 1]
or equivalently
   subset(x, Med == 1)
See ?subset for more info.

See ?save and ?write 

You can plot column heartrate vs. column ex in data frame x
like this:

plot( heartrate ~ ex, data = x )

You might find it helpful to read or re-read the document
An Introduction to R .

---
Date:   Mon, 8 Mar 2004 08:54:09 -0800 (PST) 
From:   Cindy Juncker <cindy197901 at yahoo.com>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] a question on CSV file 

 
I have a csv file,

heartrate excercise medicine Time
12 0 0 4:30 am
23 5 0 5:00 am
34 6 0 5:30 am
56 3 0 6:00 am
24 12 1 6:30 am
35 2 0 7:00 am
26 5 0 7:30 am
63 4 0 8:00 am
24 3 1 8:30 am 
45 9 0 9:00 am

I did the folllowing:
x <- read.table("heart.csv",sep=",")
names(x) <- c("heartrate","ex","Med","Time")
x.newtime <- strptime(x$Time, format= "%H:%M %p")


What I am tring to do is: 
First I am trying to get all the data from when there was a medicine taken in previous hour.
Second I will plot heartrate VS median Excercise in previous hour.


In the first problem, how can I get the desired data? after I got it, should I put it in a new file? How? 

How can I refer to a specific cell in a csv file? 

I appreciate your kindly help!



From yunfang at yahoo-inc.com  Tue Mar  9 02:57:00 2004
From: yunfang at yahoo-inc.com (Yun-Fang Juan)
Date: Mon, 8 Mar 2004 17:57:00 -0800
Subject: [R] SSfpl question
Message-ID: <029701c40579$cfe43e00$90ea7ecf@YUNFANG2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040308/66d2ebb1/attachment.pl

From kjetil at entelnet.bo  Tue Mar  9 03:20:00 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Mon, 08 Mar 2004 22:20:00 -0400
Subject: [R] Internal NA removal out of Time Series with na.omit.ts()
In-Reply-To: <1078756175.404c834f94ec3@webmail2.kuleuven.be>
References: <404C338C.7050800@bluewin.ch>
Message-ID: <404CF190.25873.9217C7@localhost>

On 8 Mar 2004 at 15:29, Jan Verbesselt wrote:

> Thanks for the tips and advice!
> 
> I found out how the NA's came into the time series.  By a threshold
> mechanism, extreme values (outliers) are removed from the time series
> (environmental data, remote sensing data) and set as NA.  A solution
> could be to detect an outlier and replace it by the quadratic fit of 2
> values before and 2 values after the outlier.
> 
> In R, 'lsfit? does a least squares fit on the available data but can
> this be used on time series since values are auto correlated?
> 
> Are there R-functions that "fit or interpolate" a new value based on a
> set window around the outlier valid for seasonal time series? Which
> function is the best to use? 
> 


Can something along this lines be of use?

(R1.8.1, on windows XP)

test <- arima.sim(model=list(ar=0.4, ma=0.4), n=100)
test[51:53] <- NA
test.m1 <- arima(test, order=c(1,0,1) )
test.sm <- KalmanSmooth(test, test.m1$model)$smooth
> cbind(test, test.sm)[50:55, ]
          test test.sm.Series 1 test.sm.Series 2
[1,] 0.8719321        0.8719321      0.251871564
[2,]        NA        0.7359504      0.000512806
[3,]        NA        0.4116944      0.000928828
[4,]        NA        0.2370341      0.001682354
[5,] 0.2366336        0.2366336      0.019878832
[6,] 0.5924668        0.5924668      0.084405316

?

Kjetil Halvorsen

> Thanks,
> Jan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Mar  9 03:24:45 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  8 Mar 2004 21:24:45 -0500 (EST)
Subject: [R] a question on CSV file
Message-ID: <20040309022445.6CD1D399F@mprdmxin.myway.com>


In rereading what I had written there is an error.
The first statement selecting Med as 1 should be as follows:
x[x$Med == 1,]

---
Date:   Mon, 8 Mar 2004 20:53:48 -0500 (EST) 
From:   Gabor Grothendieck <ggrothendieck at myway.com>
To:   <cindy197901 at yahoo.com>, <R-help at stat.math.ethz.ch> 
Subject:   RE: [R] a question on CSV file 

 


Note that a CSV file is one in which data elements
are separated by commas. There are no commas in
what you display so I assume that you are displaying
the data after you have read it in, not the CSV file.

If you read a file into a data frame x you can refer to
row i column j of the data frame as x[i,j].

The subset of rows of data frame x with column Med equal to 1 is
x[,x$Med == 1]
or equivalently
subset(x, Med == 1)
See ?subset for more info.

See ?save and ?write 

You can plot column heartrate vs. column ex in data frame x
like this:

plot( heartrate ~ ex, data = x )

You might find it helpful to read or re-read the document
An Introduction to R .

---
Date: Mon, 8 Mar 2004 08:54:09 -0800 (PST) 
From: Cindy Juncker <cindy197901 at yahoo.com>
To: <R-help at stat.math.ethz.ch> 
Subject: [R] a question on CSV file 


I have a csv file,

heartrate excercise medicine Time
12 0 0 4:30 am
23 5 0 5:00 am
34 6 0 5:30 am
56 3 0 6:00 am
24 12 1 6:30 am
35 2 0 7:00 am
26 5 0 7:30 am
63 4 0 8:00 am
24 3 1 8:30 am 
45 9 0 9:00 am

I did the folllowing:
x <- read.table("heart.csv",sep=",")
names(x) <- c("heartrate","ex","Med","Time")
x.newtime <- strptime(x$Time, format= "%H:%M %p")


What I am tring to do is: 
First I am trying to get all the data from when there was a medicine taken in previous hour.
Second I will plot heartrate VS median Excercise in previous hour.


In the first problem, how can I get the desired data? after I got it, should I put it in a new file? How? 

How can I refer to a specific cell in a csv file? 

I appreciate your kindly help!



From kjetil at entelnet.bo  Tue Mar  9 03:20:00 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Mon, 08 Mar 2004 22:20:00 -0400
Subject: [R] question
In-Reply-To: <1078760306.404c937277a2c@mouette.ens-lyon.fr>
Message-ID: <404CF190.16752.921798@localhost>

On 8 Mar 2004 at 16:38, cfinet at ens-lyon.fr wrote:

> I do not manage to make a Fisher?s exact test with the next matrix :
> 

You can consider using chisq.test() with the argument sim=TRUE:

> mat <- matrix(scan(), 7, 12, byrow=TRUE)
1:  1    3    0    1    2    9    0    0    2     5     8     6
13:    0    3    3    0    0    0    0    5    0     3     0     0
25:    0    0    0    0    0   10    0    0    0     0    10     0
37:    0    2    0    2    6   14    0    0    6     0    10     6
49:    0    5    0    0    0    7    0    0    0     2     8     0
61:    0    0    1    9    4    7    2    1    4     2    12     5
73:    0    6    0    0    0    0    0    0    0     5     1     3
85: 
Read 84 items

> chisq.test(mat, sim=TRUE)

        Pearson's Chi-squared test with simulated p-value (based
        on 2000 replicates)

data:  mat 
X-squared = 218.3366, df = NA, p-value = < 2.2e-16

Kjetil Halvorsen



>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,]    1    3    0    1    2    9    0    0    2     5     8     6
> [2,]    0    3    3    0    0    0    0    5    0     3     0     0
> [3,]    0    0    0    0    0   10    0    0    0     0    10     0
> [4,]    0    2    0    2    6   14    0    0    6     0    10     6
> [5,]    0    5    0    0    0    7    0    0    0     2     8     0
> [6,]    0    0    1    9    4    7    2    1    4     2    12     5
> [7,]    0    6    0    0    0    0    0    0    0     5     1     3
> 
> but I do not understand why it does not work since I obtain the next
> error message :
> 
> 
> 
> > fisher.test(enfin.matrix)
> Error in fisher.test(enfin.matrix) : Bug in FEXACT: gave negative key
> 
> thank you for considering my application
> 
> c?dric finet
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From rsong at cs.wisc.edu  Tue Mar  9 03:27:48 2004
From: rsong at cs.wisc.edu (Rui Song)
Date: Mon, 8 Mar 2004 20:27:48 -0600 (CST)
Subject: [R] About reading data into R
In-Reply-To: <6rptbmlrh8.fsf@bates4.stat.wisc.edu>
References: <Pine.LNX.4.58.0403081751230.30323@matrice.stat.wisc.edu>
	<6rptbmlrh8.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.58.0403082023580.30584@matrice.stat.wisc.edu>

I have a problem about reading data into R. There is a "\n"
between each pair of data, like:

      -155.65

      -155.77

      -155.40

      -155.46

      -155.52

      -155.34

	...
Could anyone tell me how to read in such data? Thanks!

Rui



From savhou at bechtel.com  Tue Mar  9 04:15:04 2004
From: savhou at bechtel.com (Symantec Alerts)
Date: Mon, 08 Mar 2004 21:15:04 -0600
Subject: [R] Content violation
Message-ID: <200403090314.i293ElT7009473@hypatia.math.ethz.ch>

The email message:

From: r-help at lists.r-project.org
To: dvelpmnt at bechtel.com

includes the file <your_document.pif> which is prohibited by Bechtel Information Security policy.  The file was blocked and did not reach the intended recipient.

****************************************************************
If you did not send an email message containing this information, your email address may have been spoofed by a third party or virus. If you believe you've received this notification in error, please delete it and disregard the following instructions.
****************************************************************

To protect Bechtel's network from viruses or other potentially harmful files, the following types of attachments are not allowed to enter or leave the Bechtel network, as well as certain subject lines known to be associated with viruses:

. ADE	 Microsoft Access Project Extension
. ADP	 Microsoft Access Project
. BAS	 Visual Basic Class Module
. BAT	 Batch File
. CHM	 Compiled HTML Help File
. CMD	 Windows NT Command Script
. COM	 MS-DOS Application
. CPL	 Control Panel Extension
. CRT	 Security Certificate
. EXE	 Application
. HLP	 Windows Help File
. HTA	 HTML Applications
. INF	 Setup Information File
. INS	 Internet Communication Settings
. ISP	 Internet Communication Settings
. JS	 JScript File
. JSE	 JScript Encoded Script File
. LNK	 Shortcut
. MSC	 Microsoft Common Console Document
. MSI	 Windows Installer Package
. MSP	 Windows Installer Patch
. MST	 Visual Test Source File
. PCD	 Photo CD Image
. PIF	 Shortcut to MS-DOS Program
. REG	 Registration Entries
. SCR	 Screen Saver
. SCT	 Windows Script Component
. SHS	 Shell Scrap Object
. URL	 Internet Shortcut (Uniform Resource Locator)
. VB	 VBScript File
. VBE	 VBScript Encoded Script File
. VBS	 VBScript Script File
. WSC	 Windows Script Component
. WSF	 Windows Script File
. WSH	 Windows Scripting Host Settings File

To successfully send an attachment listed above to or from the Bechtel network, try one of the following options:
1.) Rename the file before attaching it to the email.  For example, rename EXAMPLE.EXE to EXAMPLE_EXE.  Make sure to add a note in the email to rename the extension back to .EXE when it arrives.
2.) ZIP the file with password protection.  Attached the ZIP file to the email.
3.) Instead of sending URL attachments, copy and paste the Web address into the email (i.e. http://www.example.com/default.htm).

For further assistance within Bechtel, please contact your local helpdesk.



From dmurdoch at pair.com  Tue Mar  9 04:26:40 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 08 Mar 2004 22:26:40 -0500
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <Pine.SOL.4.20.0403081812080.25526-100000@santiam.dfci.harvard.edu>
References: <Pine.SOL.4.20.0403081812080.25526-100000@santiam.dfci.harvard.edu>
Message-ID: <i1eq40plr4jm2bnrp3bvnjjdt976ipu4iq@4ax.com>

On Mon, 8 Mar 2004 18:20:29 -0500 (EST), you wrote:

>In trying to figure out how these things are determiend, I noticed that in
>Windows there was a difference in the reported missing links when one does
>'Rcmd check' vs 'Rcmd install' (or 'Rcmd install --build').  In the
>example above, using either of the 'install' methods results in that link
>not being reported as missing (and if I put in an intentionally missing
>link it gets picked up as such), but 'check' reports it as missing.  I'm
>wondering what the difference in environment is between check & install on
>Windows as that might help me to figure out why check reports these as
>missing links.

This is something we should fix. I haven't checked the code, but I
believe the rule is that check will only find links to base packages,
but install will find links to any package installed on the system.

You can avoid the errors in check by using the
\link[package:topic]{foo} syntax, to tell R where to find the link:
but you need to give the filename of the Rd file as the topic, not an
alias to it.

Duncan Murdoch



From spencer.graves at pdf.com  Tue Mar  9 04:43:00 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 08 Mar 2004 19:43:00 -0800
Subject: [R] Monte Carlo p-value (was "question")
In-Reply-To: <404CF190.16752.921798@localhost>
References: <404CF190.16752.921798@localhost>
Message-ID: <404D3D44.3030107@pdf.com>

      What is the standard convention for a Monte Carlo p-value when the 
observed outcome is more extreme than all simulations?  The example 
provided by C?dric Fine produced a Monte Carlo p-value, according to 
Kjetil Halvorsen, of "2.2e-16 (based on 2000 replicates)".  This seems 
inappropriate to me. 

      By reading the code, I found that for this case,

            PVAL <- sum(tmp$results >= STATISTIC)/B

where STATISTIC is the observed chi-square while "tmp$results" is a 
vector of length B of chi-squares from Monte Carlo simulated tables with 
the same marginals.  Thus, PVAL ranges over seq(0, 1, length=(B+1)).  
For the observed table, presumably, PVAL = 0.  The function "chisq.test" 
apparently returns an object of class "htest", and "print.htest" calls 
"format.pval(x$p.value, digits = digits)", for which "format.pval(0, 4)" 
is "< 2.2e-16". 

      Can someone provide an appropriate reference or sense of the 
literature on the appropriate number to report for a Monte Carlo p-value 
when the observed is more extreme than all the simulations?  A value of 
0 or "< 2.2e-16" violates my sense of the logic of this situation.  If 
the appropriate number is 0.5/B, then the line "PVAL <- sum(tmp$results 
 >= STATISTIC)/B" could be followed immediately by something like the 
following: 

      if(PVAL==0) PVAL <- 0.5/B

      Comments?
      Best Wishes,
      Spencer Graves

kjetil at entelnet.bo wrote:

>On 8 Mar 2004 at 16:38, cfinet at ens-lyon.fr wrote:
>
>  
>
>>I do not manage to make a Fisher?s exact test with the next matrix :
>>
>>    
>>
>
>You can consider using chisq.test() with the argument sim=TRUE:
>
>  
>
>>mat <- matrix(scan(), 7, 12, byrow=TRUE)
>>    
>>
>1:  1    3    0    1    2    9    0    0    2     5     8     6
>13:    0    3    3    0    0    0    0    5    0     3     0     0
>25:    0    0    0    0    0   10    0    0    0     0    10     0
>37:    0    2    0    2    6   14    0    0    6     0    10     6
>49:    0    5    0    0    0    7    0    0    0     2     8     0
>61:    0    0    1    9    4    7    2    1    4     2    12     5
>73:    0    6    0    0    0    0    0    0    0     5     1     3
>85: 
>Read 84 items
>
>  
>
>>chisq.test(mat, sim=TRUE)
>>    
>>
>
>        Pearson's Chi-squared test with simulated p-value (based
>        on 2000 replicates)
>
>data:  mat 
>X-squared = 218.3366, df = NA, p-value = < 2.2e-16
>
>Kjetil Halvorsen
>
>
>
>  
>
>>    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
>>[1,]    1    3    0    1    2    9    0    0    2     5     8     6
>>[2,]    0    3    3    0    0    0    0    5    0     3     0     0
>>[3,]    0    0    0    0    0   10    0    0    0     0    10     0
>>[4,]    0    2    0    2    6   14    0    0    6     0    10     6
>>[5,]    0    5    0    0    0    7    0    0    0     2     8     0
>>[6,]    0    0    1    9    4    7    2    1    4     2    12     5
>>[7,]    0    6    0    0    0    0    0    0    0     5     1     3
>>
>>but I do not understand why it does not work since I obtain the next
>>error message :
>>
>>
>>
>>    
>>
>>>fisher.test(enfin.matrix)
>>>      
>>>
>>Error in fisher.test(enfin.matrix) : Bug in FEXACT: gave negative key
>>
>>thank you for considering my application
>>
>>c?dric finet
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From dmurdoch at pair.com  Tue Mar  9 04:57:43 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 08 Mar 2004 22:57:43 -0500
Subject: [R] how to replace NaN in a vector
In-Reply-To: <Pine.LNX.4.58.0403081806260.5785@galton.uchicago.edu>
References: <200403081104.i28B2x04014300@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0403081806260.5785@galton.uchicago.edu>
Message-ID: <e4gq401qjg5jjfq608j3hlb087mb4188o7@4ax.com>

On Mon, 8 Mar 2004 18:18:28 -0600 (CST), you wrote:

>Hi, all
>
>a vector such as
>
>(1,2,4,-1,NaN,2,4,NaN)
>
>if try to replace all the NaN with a numerical value, what's the easiest 
>way?

x[is.nan(x)] <- 42

See ?NaN for related functions.

Duncan Murdoch



From d.scott at auckland.ac.nz  Tue Mar  9 05:05:49 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Tue, 9 Mar 2004 17:05:49 +1300 (NZDT)
Subject: [R] how to replace NaN in a vector
In-Reply-To: <Pine.LNX.4.58.0403081806260.5785@galton.uchicago.edu>
Message-ID: <Pine.LNX.4.44.0403091704430.21102-100000@stat71.stat.auckland.ac.nz>

On Mon, 8 Mar 2004, Yong Wang wrote:

> Hi, all
> 
> a vector such as
> 
> (1,2,4,-1,NaN,2,4,NaN)
> 
> if try to replace all the NaN with a numerical value, what's the easiest 
> way?
> thanks a lot
> 
> 

Is this what you wanted?

> x<-c(1,2,4,-1,NaN,2,4,NaN)
> y<-ifelse(is.nan(x),-999,x)
> y
[1]    1    2    4   -1 -999    2    4 -999



David Scott


_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From ok at cs.otago.ac.nz  Tue Mar  9 05:09:23 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 9 Mar 2004 17:09:23 +1300 (NZDT)
Subject: [R] More factor names in x axis, how?
Message-ID: <200403090409.i2949NGZ058200@atlas.otago.ac.nz>

I have some data which records, amongst other things,
age (recoded by me to be in months), and
drug group (a factor).
The drug group is a 2 digit number,
but there is no numeric relationship, and only 50 of the possible
groups occur

So

> asd <- read.table("asd.dat", header=TRUE)
> asd$group <- as.factor(asd$group)
> plot(age ~ group, data=asd)

gives me a series of boxplots which suggests that the age
distributions are worth examining further,
BUT while every factor level in the data is represented by
a tick mark on the x axis, only about half of the ticks
are labelled with the factor level.  Because the factor
levels which occur are not consecutive integers, I cannot
tell what some of the tick marks represent.

There is enough room for all the factor levels to be printed,
but I cannot figure out how to make that happen.  Shrinking
the labels with cex=0.5 (or even smaller) was the obvious thing
to try, but while making the labels bigger reduced the number of
factor levels shown, making the labels smaller didn't increase it.

I have checkd ?par and ?plot and ?plot.default and don't see anything
obvious.



From ajayshah at mayin.org  Mon Mar  8 17:37:15 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 8 Mar 2004 22:07:15 +0530
Subject: [R] Am failing on making lagged residual after regression
Message-ID: <20040308163715.GA23660@igidr.ac.in>

Folks,

I'm most confused in trying to do something that (I thought) out to be
mainstream and straightforward R. :-) Could you please help?

I am doing an ordinary linear regression. My goal is: After a
regression, to make residuals, and make a new variable which is the
lagged residuals (lagged by 1). I will use this variable in a 2nd
stage regression (for an error-correcting model).

This sounds simple and reasonable, and should be right up R's alley,
but I am just not able to do this. Can I please show you the steps
which I'm trying and failing in?

I start with:

> m = lm(NNDA ~ NFA + NFA.x.d1 + NFA.x.d2 + IIP.n + CRR, D.f)
> e = residuals(m)
> print(e)
          34           35           36           37           38           39 
 -5073.24843  -4210.27886  -8218.01782  -1489.10583  -4426.11738 -11332.56052 
  (lines deleted)
          64           65           66           67           68           69 
  8362.93776   7564.14324   2311.41208   7660.00638  -1271.04645 -10917.29418 
  (lines deleted)
         160          161          162          163          164          165 
  3858.94591 -11783.04370 -21438.33646   1859.49628  -4988.82853 -25172.43241 

Here, the residuals only started at the 34th observation owing to
missing data in my data frame. This is correct and sensible. The
dataset is 167 observations, but 166 and 167 are also missing data and
dropped.

I tried to use lag(e,1) to make a new vector and failed. I think I am
just not understanding the R concept of lag(). In my notion of a
lagged vector, I want a vector f where f[35] is e[34], i.e. is the
first residual above of -5073.24843. This is just not what I get by
saying lag(e,1) - I am just not understanding lag(). I would be very
happy if someone could educate me on how to utilise lag().

Okay, I try to get my way in a different way:

> print(T)
[1] 167
> f = numeric(T)
> f[1] = NA
> f[2:T] = e[1:(T-1)]

This looks reasonable? I thought this should do the trick. I am
hand-initialising a T-length vector with NA in the 1st elem, and I
copy out the values of e[] from 1 till 166 into f[2:T]. I thought this
should give me a lagged e. It doesn't --

> print(f)
  [1]           NA  -5073.24843  -4210.27886  -8218.01782  -1489.10583
  (lines deleted)
[131]   1859.49628  -4988.82853 -25172.43241           NA           NA
  (lines deleted)
[166]           NA           NA

I thought "Okay, what seems to be happening is that the e[1] that I
have is `actually' the e[34] of my thoughts". So I try:

> f=rep(NA, T)               # zap out f
> f[35:T] = e[34:(T-1)]      # copy out useful stuff into 35..T
> print(f)
  [1]           NA           NA           NA           NA           NA
  (lines deleted)
 [31]           NA           NA           NA           NA   7660.00638
 [36]  -1271.04645 -10917.29418 -11111.60144  -1597.98355  -1066.01901
  (lines deleted)
[131]   1859.49628  -4988.82853 -25172.43241           NA           NA
  (lines deleted)
[166]           NA           NA

This is wrong!!

Recall (from upstairs) that e[34] was -5073.24843. That value seems to
have mysteriously vanished. Instead, the first non-NA in f - which is
f[35] - is 7660.00638, which (incidentally) was e[67]. I just don't
know how that value got here. And, the values in f[] seem to peter out
at 133!  After 133, they are all NA until the end.

I guess I'm _just_ not understanding what is the animal that is
returned by residual(lm()). I know I am missing something basic,
because lots of people must be doing what I am trying: I.e. to run a
regression, extract a residual, lag it, and use it for a 2nd stage
regression.

I know that the vector e (returned by residual(lm())) is different
from a simple vector, for when I say:

> print(f[35])
[1] 7660.006
> print(e[35])
       68 
-1271.046 

the two animals seem to be different. I don't understand e[35] - why
is it not just a number - there seems to be some index tagging along?
How do I get at the pure numbers of the residuals?

Thanks much,

       -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Tue Mar  9 07:02:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  9 Mar 2004 01:02:19 -0500 (EST)
Subject: [R] vector extraction
Message-ID: <20040309060219.33D103992@mprdmxin.myway.com>



My understanding is that you want to extract the
dv values for year 1980:

     color <- with( Test.dataset2, dv[ year == 1980 ] )

---

Date:   Mon, 8 Mar 2004 19:36:45 -0800 
From:   Jens Hainmueller <jens_hainmueller at ksg05.harvard.edu>
To:   R-Help <r-help at stat.math.ethz.ch> 
Subject:   [R] vector extraction 

 
Hello,

I could need some help on this one:

>From the data.frame "Test.dataset2" below (TSCS data for 151
"countries.to.map" for "year" 1973-95; each "country.to.map" is described by
a unique code), I would like to extract a vector "color" that for each
"country.to.map" takes on the value of "dv" (a categorical variable with
values 1,2,..4) for a specified "year". Thus, for a specified "year",
"color" should have 151 obs - one for each "country.to.map" represented by
its respective value of "dv").

I tried this:

> color <- dv[country.to.map][(year == 1980)[country.to.map]]

but it does not give me what I need. I can't figure out where my error is,
however.

Thanks,
Jens

> Test.dataset2[1:40,]
country.to.map dv year
1 1936 NA 1973
2 1936 NA 1974
3 1936 NA 1975
4 1936 NA 1976
5 1936 NA 1977
6 1936 NA 1978
7 1936 NA 1979
8 1936 NA 1980
9 1936 NA 1981
10 1936 NA 1982
11 1936 NA 1983
12 1936 NA 1984
13 1936 NA 1985
14 1936 NA 1986
15 1936 NA 1987
16 1936 NA 1988
17 1936 4 1989
18 1936 4 1990
19 1936 4 1991
20 1936 4 1992
21 1936 4 1993
22 1936 4 1994
23 1936 4 1995
24 56 4 1973
25 56 4 1974
26 56 2 1975
27 56 2 1976
28 56 2 1977
29 56 2 1978
30 56 2 1979
31 56 2 1980
32 56 2 1981
33 56 2 1982
34 56 4 1983
35 56 4 1984
36 56 4 1985
37 56 4 1986
38 56 4 1987
39 56 4 1988
40 56 4 1989



From ggrothendieck at myway.com  Tue Mar  9 07:23:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  9 Mar 2004 01:23:23 -0500 (EST)
Subject: [R] More factor names in x axis, how?
Message-ID: <20040309062323.B7E9039B9@mprdmxin.myway.com>



You can omit the x axis from plot and create it yourself
using axis:

   plot(age ~ group, data=asd, xaxt="n")
   axis(1, 1:nlevels(group), levels(group))

---

Date:   Tue, 9 Mar 2004 17:09:23 +1300 (NZDT) 
From:   Richard A. O'Keefe <ok at cs.otago.ac.nz>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] More factor names in x axis, how? 

 
I have some data which records, amongst other things,
age (recoded by me to be in months), and
drug group (a factor).
The drug group is a 2 digit number,
but there is no numeric relationship, and only 50 of the possible
groups occur

So

> asd <- read.table("asd.dat", header=TRUE)
> asd$group <- as.factor(asd$group)
> plot(age ~ group, data=asd)

gives me a series of boxplots which suggests that the age
distributions are worth examining further,
BUT while every factor level in the data is represented by
a tick mark on the x axis, only about half of the ticks
are labelled with the factor level. Because the factor
levels which occur are not consecutive integers, I cannot
tell what some of the tick marks represent.

There is enough room for all the factor levels to be printed,
but I cannot figure out how to make that happen. Shrinking
the labels with cex=0.5 (or even smaller) was the obvious thing
to try, but while making the labels bigger reduced the number of
factor levels shown, making the labels smaller didn't increase it.

I have checkd ?par and ?plot and ?plot.default and don't see anything
obvious.



From ggrothendieck at myway.com  Tue Mar  9 07:52:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  9 Mar 2004 01:52:22 -0500 (EST)
Subject: [R] About reading data into R
Message-ID: <20040309065222.961163971@mprdmxin.myway.com>



scan has no problem with blank lines.  read.table has
an argument that controls how it handles blank lines
and the default setting is to ignore them.


---
Date:   Mon, 8 Mar 2004 20:27:48 -0600 (CST) 
From:   Rui Song <rsong at cs.wisc.edu>
To:   <R-help at R-project.org> 
Subject:   [R] About reading data into R 

 
I have a problem about reading data into R. There is a "\n"
between each pair of data, like:

-155.65

-155.77

-155.40

-155.46

-155.52

-155.34

     ...
Could anyone tell me how to read in such data? Thanks!

Rui



From arv at ono.com  Tue Mar  9 08:27:55 2004
From: arv at ono.com (antonio rodriguez)
Date: Tue, 9 Mar 2004 08:27:55 +0100
Subject: [R] time-series
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAsvzEnbBALEy+EUyF5G42lAEAAAAA@esalq.usp.br>
Message-ID: <IPEFKICOHOECENGJBAGLMEIODAAA.arv@ono.com>

Hi Eduardo,

Probably a good idea is to do a multivariate analysis through spectral
analysis. With function spectrum you can look for coherence and phase
between several time series. In your case with different lengths, you can
use the option: na.action=na.omit

>rainfall5<-spectrum(ts.union(s1,s2,s3,s4,s5),method="pgram",kernell("daniel
l",m=c(3,5,7)),taper=0.1,na.action=na.omit)
>plot(rainfall5,plot.type="c")	#looks for coherence
>plot(rainfall5,plot.type="p")	#looks for phase

Hope this helps,

Antonio

> -----Mensaje original-----
> De: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Eduardo Dutra de
> Armas
> Enviado el: viernes, 05 de marzo de 2004 19:40
> Para: r-help at stat.math.ethz.ch
> Asunto: [R] time-series
>
>
> Dear R helpers
> I have a daily rainfall dataset of 5 stations, but the time-series are
> of different lengths. Three stations measured precipitation from 1917 to
> 2004, one from 1930 to 1979, and other from 1975 to 1998. I'd like to
> know how to analyse if rainfall is similiar at all stations. Is it
> possible to compare time-series like these?
>
> Best Regards!
>
> __________________________________________________________
> Eng. Agr., M.Sc. Eduardo Dutra de Armas
> __________________________________________________________
> Centro de Energia Nuclear na Agricultura (CENA/USP)
> Laborat?rio de Ecotoxicologia
> Av.Centen?rio 303, C.P. 96, CEP 13400-970, Piracicaba, SP, Brasil
> Fone: (55-19)34294761 - Fax: (55-19)34294610 - Cel: (55-19)81155564
> (?reas de atua??o: Polui??o de solo e ?gua, Din?mica Ambiental de
> pesticidas, Biodegrada??o, Microbiologia Ambiental)
> __________________________________________________________
>
>
> ---
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
---
Incoming mail is certified Virus Free.



---



From ripley at stats.ox.ac.uk  Tue Mar  9 08:40:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 07:40:33 +0000 (GMT)
Subject: [R] years from as.POSIXlt 
In-Reply-To: <200403081550.01997.ozric@web.de>
Message-ID: <Pine.LNX.4.44.0403090738300.6161-100000@gannet.stats>

On Mon, 8 Mar 2004, Christian Schulz wrote:

> how it's possible to extract the year and  the number
> of days from Julian date. i'm little confused about the last two
> functions and ?years .
> 
> EDATE  comes from sqlQuery with as.is=T
> EDATE <- as.POSIXlt(datvears$ENROLLDAY)
> > EDATE[1:5] 
> [1] "2000-06-30 11:25:01" "2000-06-30 11:39:55" "2000-06-30 12:11:11"
> [4] "2000-06-30 12:13:32" "2000-06-30 12:50:12"
> > weekdays(EDATE[1:5])
> [1] "Freitag" "Freitag" "Freitag" "Freitag" "Freitag"
> > months(EDATE[1:5])
> [1] "Juni" "Juni" "Juni" "Juni" "Juni"
> > years(EDATE[1:5])
> NULL
> > days(EDATE[1:5])
> NULL

years() and days() are not part of base R: from chron, perhaps?

See ?julian, which says 

Note:

     Other components such as the day of the month or the year are very
     easy to computes: just use 'as.POSIXlt' and extract the relevant
     component.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar  9 08:48:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 07:48:33 +0000 (GMT)
Subject: [R] Decision Trees
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE500@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0403090742310.6161-100000@gannet.stats>

Please check the licence conditions for C4.5: it is basically commercial.
You are not allowed to use the published source code, and I am not allowed 
to show you the source code I bought, which I am not allowed to `use for 
commercial purposes or gain'.

WEKA includes a re-implementation of the ideas behind C4.5, but not C4.5.


On Mon, 8 Mar 2004 Benjamin.STABLER at odot.state.or.us wrote:

> I am familiar with the rpart and tree packages for classification and
> regression trees.  However, quite a bit of the research in the
> transportation community relating to decision trees uses the C4.5 family of
> algorithms by Quinlan.  Are there any plans to make a C4.5 (or a derivative
> of it) available to R?  If not, then I might use the WEKA Java package (
> http://www.cs.waikato.ac.nz/ml/weka) that implements a Java version of C4.5.
> Thanks.  

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar  9 08:53:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 07:53:26 +0000 (GMT)
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <Pine.SOL.4.20.0403081812080.25526-100000@santiam.dfci.harvard.edu>
Message-ID: <Pine.LNX.4.44.0403090749460.6161-100000@gannet.stats>

Note that on Windows only packages in the same library are checked for 
links, as on Windows the HTML help cannot link across libraries.  (This is 
done under Unix via symbolic links.)

By default check installs in a private library, so you may well see 
(correct) reports of missing links under Windows.  You can check an 
already installed version, though.

On Mon, 8 Mar 2004, Jeff Gentry wrote:

> Hello ...
> 
> Using R-1.9.0 alpha, I'm having some problem getting a few packages to
> pass check under Windows - specifically with the 'missing link(s)' section
> of the package install phase.
> 
> I started trying to track down how the missing link was showing up as in
> some cases I could not see why the link was considered missing.  For
> instance, in one package it was looking for 'makeViewers' although when
> the packages is loaded in R, 'makeViewers' is even in the search path (by
> way of a dependent package which was loaded initially).

Not relevant, as this is Perl code and no R process is running.

> In trying to figure out how these things are determiend, I noticed that in
> Windows there was a difference in the reported missing links when one does
> 'Rcmd check' vs 'Rcmd install' (or 'Rcmd install --build').  In the
> example above, using either of the 'install' methods results in that link
> not being reported as missing (and if I put in an intentionally missing
> link it gets picked up as such), but 'check' reports it as missing.  I'm
> wondering what the difference in environment is between check & install on
> Windows as that might help me to figure out why check reports these as
> missing links.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrewjabel at hotmail.com  Tue Mar  9 08:56:58 2004
From: andrewjabel at hotmail.com (Andrew Abel)
Date: Tue, 09 Mar 2004 17:56:58 +1000
Subject: [R] (no subject)
Message-ID: <BAY8-F107tbRUKUAKrc00007aae@hotmail.com>

Hi,

I keep getting this error code when I try to run an R file that reads in 
data from another file. I know the file is there.

source("C:/R_Data/typhoon.r")
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file `typhoon.txt'


I hope you can help.

Regards

Andrew Abel

_________________________________________________________________

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: typhoon.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040309/6f7b0ac3/typhoon.txt

From ripley at stats.ox.ac.uk  Tue Mar  9 09:03:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 08:03:44 +0000 (GMT)
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <i1eq40plr4jm2bnrp3bvnjjdt976ipu4iq@4ax.com>
Message-ID: <Pine.LNX.4.44.0403090759070.6161-100000@gannet.stats>

On Mon, 8 Mar 2004, Duncan Murdoch wrote:

> On Mon, 8 Mar 2004 18:20:29 -0500 (EST), you wrote:
> 
> >In trying to figure out how these things are determiend, I noticed that in
> >Windows there was a difference in the reported missing links when one does
> >'Rcmd check' vs 'Rcmd install' (or 'Rcmd install --build').  In the
> >example above, using either of the 'install' methods results in that link
> >not being reported as missing (and if I put in an intentionally missing
> >link it gets picked up as such), but 'check' reports it as missing.  I'm
> >wondering what the difference in environment is between check & install on
> >Windows as that might help me to figure out why check reports these as
> >missing links.
> 
> This is something we should fix. I haven't checked the code, but I
> believe the rule is that check will only find links to base packages,
> but install will find links to any package installed on the system.

No, both will find links in the same library as installing into (plus
those which are fixed up on installation, e.g. to the base package).

Several of us have looked for years for a fix, and this is the best scheme 
we have come up with.  You can't put in absolute paths in the HTML as e.g. 
a private library may be used with more than one version of R (or R may be 
updated later).  Short of adding symbolic links to Windows (and getting 
browsers to follow them), how do you propose `we should fix' it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From k.wang at auckland.ac.nz  Tue Mar  9 09:07:39 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 9 Mar 2004 21:07:39 +1300
Subject: [R] Decision Trees
In-Reply-To: <Pine.LNX.4.44.0403090742310.6161-100000@gannet.stats>
Message-ID: <20040309080751.UHVL22869.web1-rme.xtra.co.nz@kevinlpt>

Hi,

> -----Original Message-----
>
> WEKA includes a re-implementation of the ideas behind C4.5,
> but not C4.5.

If my memory serves me right, WEKA people called this
"re-implementation" J4.8.

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
Statistics Workshops Co-ordinator
Student Learning Centre
University of Auckland
New Zealand



From ripley at stats.ox.ac.uk  Tue Mar  9 09:17:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 08:17:02 +0000 (GMT)
Subject: [R] Monte Carlo p-value (was "question")
In-Reply-To: <404D3D44.3030107@pdf.com>
Message-ID: <Pine.LNX.4.44.0403090804330.6161-100000@gannet.stats>

1/(B+1) is the significance level of the Monte Carlo test if the data 
give the most extreme value.  (E.g. Ripley, 1987, p. 171.)  This is a 
calculation, not a convention, and assumes a continuously distributed 
statistic.

On Mon, 8 Mar 2004, Spencer Graves wrote:

>       What is the standard convention for a Monte Carlo p-value when the 
> observed outcome is more extreme than all simulations?  The example 
> provided by C?dric Fine produced a Monte Carlo p-value, according to 
> Kjetil Halvorsen, of "2.2e-16 (based on 2000 replicates)".  This seems 
> inappropriate to me. 
> 
>       By reading the code, I found that for this case,
> 
>             PVAL <- sum(tmp$results >= STATISTIC)/B
> 
> where STATISTIC is the observed chi-square while "tmp$results" is a 
> vector of length B of chi-squares from Monte Carlo simulated tables with 
> the same marginals.  Thus, PVAL ranges over seq(0, 1, length=(B+1)).  
> For the observed table, presumably, PVAL = 0.  The function "chisq.test" 
> apparently returns an object of class "htest", and "print.htest" calls 
> "format.pval(x$p.value, digits = digits)", for which "format.pval(0, 4)" 
> is "< 2.2e-16". 
> 
>       Can someone provide an appropriate reference or sense of the 
> literature on the appropriate number to report for a Monte Carlo p-value 
> when the observed is more extreme than all the simulations?  A value of 
> 0 or "< 2.2e-16" violates my sense of the logic of this situation.  If 
> the appropriate number is 0.5/B, then the line "PVAL <- sum(tmp$results 
>  >= STATISTIC)/B" could be followed immediately by something like the 
> following: 
> 
>       if(PVAL==0) PVAL <- 0.5/B
> 
>       Comments?
>       Best Wishes,
>       Spencer Graves
> 
> kjetil at entelnet.bo wrote:
> 
> >On 8 Mar 2004 at 16:38, cfinet at ens-lyon.fr wrote:
> >
> >  
> >
> >>I do not manage to make a Fisher?s exact test with the next matrix :
> >>
> >>    
> >>
> >
> >You can consider using chisq.test() with the argument sim=TRUE:
> >
> >  
> >
> >>mat <- matrix(scan(), 7, 12, byrow=TRUE)
> >>    
> >>
> >1:  1    3    0    1    2    9    0    0    2     5     8     6
> >13:    0    3    3    0    0    0    0    5    0     3     0     0
> >25:    0    0    0    0    0   10    0    0    0     0    10     0
> >37:    0    2    0    2    6   14    0    0    6     0    10     6
> >49:    0    5    0    0    0    7    0    0    0     2     8     0
> >61:    0    0    1    9    4    7    2    1    4     2    12     5
> >73:    0    6    0    0    0    0    0    0    0     5     1     3
> >85: 
> >Read 84 items
> >
> >  
> >
> >>chisq.test(mat, sim=TRUE)
> >>    
> >>
> >
> >        Pearson's Chi-squared test with simulated p-value (based
> >        on 2000 replicates)
> >
> >data:  mat 
> >X-squared = 218.3366, df = NA, p-value = < 2.2e-16
> >
> >Kjetil Halvorsen
> >
> >
> >
> >  
> >
> >>    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> >>[1,]    1    3    0    1    2    9    0    0    2     5     8     6
> >>[2,]    0    3    3    0    0    0    0    5    0     3     0     0
> >>[3,]    0    0    0    0    0   10    0    0    0     0    10     0
> >>[4,]    0    2    0    2    6   14    0    0    6     0    10     6
> >>[5,]    0    5    0    0    0    7    0    0    0     2     8     0
> >>[6,]    0    0    1    9    4    7    2    1    4     2    12     5
> >>[7,]    0    6    0    0    0    0    0    0    0     5     1     3
> >>
> >>but I do not understand why it does not work since I obtain the next
> >>error message :
> >>
> >>
> >>
> >>    
> >>
> >>>fisher.test(enfin.matrix)
> >>>      
> >>>
> >>Error in fisher.test(enfin.matrix) : Bug in FEXACT: gave negative key
> >>
> >>thank you for considering my application
> >>
> >>c?dric finet
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar  9 09:28:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 08:28:24 +0000 (GMT)
Subject: [R] Am failing on making lagged residual after regression
In-Reply-To: <20040308163715.GA23660@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0403090823580.6161-100000@gannet.stats>

If you have missing data in your data frame and want residuals for all 
observations, you need to use na.action=na.exclude, not the default 
na.omit.

As for lag, its description says

Description:

     Compute a lagged version of a time series, shifting the time base
     back by a given number of observations.

and you don't have a time series.  It works by shifting the time base for 
a time series, not by moving the contents of a vector.

On Mon, 8 Mar 2004, Ajay Shah wrote:

> Folks,
> 
> I'm most confused in trying to do something that (I thought) out to be
> mainstream and straightforward R. :-) Could you please help?
> 
> I am doing an ordinary linear regression. My goal is: After a
> regression, to make residuals, and make a new variable which is the
> lagged residuals (lagged by 1). I will use this variable in a 2nd
> stage regression (for an error-correcting model).
> 
> This sounds simple and reasonable, and should be right up R's alley,
> but I am just not able to do this. Can I please show you the steps
> which I'm trying and failing in?
> 
> I start with:
> 
> > m = lm(NNDA ~ NFA + NFA.x.d1 + NFA.x.d2 + IIP.n + CRR, D.f)
> > e = residuals(m)
> > print(e)
>           34           35           36           37           38           39 
>  -5073.24843  -4210.27886  -8218.01782  -1489.10583  -4426.11738 -11332.56052 
>   (lines deleted)
>           64           65           66           67           68           69 
>   8362.93776   7564.14324   2311.41208   7660.00638  -1271.04645 -10917.29418 
>   (lines deleted)
>          160          161          162          163          164          165 
>   3858.94591 -11783.04370 -21438.33646   1859.49628  -4988.82853 -25172.43241 
> 
> Here, the residuals only started at the 34th observation owing to
> missing data in my data frame. This is correct and sensible. The
> dataset is 167 observations, but 166 and 167 are also missing data and
> dropped.
> 
> I tried to use lag(e,1) to make a new vector and failed. I think I am
> just not understanding the R concept of lag(). In my notion of a
> lagged vector, I want a vector f where f[35] is e[34], i.e. is the
> first residual above of -5073.24843. This is just not what I get by
> saying lag(e,1) - I am just not understanding lag(). I would be very
> happy if someone could educate me on how to utilise lag().
> 
> Okay, I try to get my way in a different way:
> 
> > print(T)
> [1] 167
> > f = numeric(T)
> > f[1] = NA
> > f[2:T] = e[1:(T-1)]
> 
> This looks reasonable? I thought this should do the trick. I am
> hand-initialising a T-length vector with NA in the 1st elem, and I
> copy out the values of e[] from 1 till 166 into f[2:T]. I thought this
> should give me a lagged e. It doesn't --
> 
> > print(f)
>   [1]           NA  -5073.24843  -4210.27886  -8218.01782  -1489.10583
>   (lines deleted)
> [131]   1859.49628  -4988.82853 -25172.43241           NA           NA
>   (lines deleted)
> [166]           NA           NA
> 
> I thought "Okay, what seems to be happening is that the e[1] that I
> have is `actually' the e[34] of my thoughts". So I try:
> 
> > f=rep(NA, T)               # zap out f
> > f[35:T] = e[34:(T-1)]      # copy out useful stuff into 35..T
> > print(f)
>   [1]           NA           NA           NA           NA           NA
>   (lines deleted)
>  [31]           NA           NA           NA           NA   7660.00638
>  [36]  -1271.04645 -10917.29418 -11111.60144  -1597.98355  -1066.01901
>   (lines deleted)
> [131]   1859.49628  -4988.82853 -25172.43241           NA           NA
>   (lines deleted)
> [166]           NA           NA
> 
> This is wrong!!
> 
> Recall (from upstairs) that e[34] was -5073.24843. That value seems to
> have mysteriously vanished. Instead, the first non-NA in f - which is
> f[35] - is 7660.00638, which (incidentally) was e[67]. I just don't
> know how that value got here. And, the values in f[] seem to peter out
> at 133!  After 133, they are all NA until the end.
> 
> I guess I'm _just_ not understanding what is the animal that is
> returned by residual(lm()). I know I am missing something basic,
> because lots of people must be doing what I am trying: I.e. to run a
> regression, extract a residual, lag it, and use it for a 2nd stage
> regression.
> 
> I know that the vector e (returned by residual(lm())) is different
> from a simple vector, for when I say:
> 
> > print(f[35])
> [1] 7660.006
> > print(e[35])
>        68 
> -1271.046 
> 
> the two animals seem to be different. I don't understand e[35] - why
> is it not just a number - there seems to be some index tagging along?
> How do I get at the pure numbers of the residuals?
> 
> Thanks much,
> 
>        -ans.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cfinet at ens-lyon.fr  Tue Mar  9 10:05:48 2004
From: cfinet at ens-lyon.fr (cfinet@ens-lyon.fr)
Date: Tue, 09 Mar 2004 10:05:48 +0100 (CET)
Subject: [R] Monte Carlo p-value (was "question")
In-Reply-To: <404D3D44.3030107@pdf.com>
References: <404CF190.16752.921798@localhost> <404D3D44.3030107@pdf.com>
Message-ID: <1078823148.404d88ece8606@mouette.ens-lyon.fr>

I thank you for your answer but I do not understand yet why the Fisher?s exact 
test does not work. And why is a "negative key".

C?dric Finet



En r?ponse ? Spencer Graves <spencer.graves at pdf.com>:

>       What is the standard convention for a Monte Carlo p-value when the
> 
> observed outcome is more extreme than all simulations?  The example 
> provided by C?dric Fine produced a Monte Carlo p-value, according to 
> Kjetil Halvorsen, of "2.2e-16 (based on 2000 replicates)".  This seems
> 
> inappropriate to me. 
> 
>       By reading the code, I found that for this case,
> 
>             PVAL <- sum(tmp$results >= STATISTIC)/B
> 
> where STATISTIC is the observed chi-square while "tmp$results" is a 
> vector of length B of chi-squares from Monte Carlo simulated tables with
> 
> the same marginals.  Thus, PVAL ranges over seq(0, 1, length=(B+1)).  
> For the observed table, presumably, PVAL = 0.  The function "chisq.test"
> 
> apparently returns an object of class "htest", and "print.htest" calls
> 
> "format.pval(x$p.value, digits = digits)", for which "format.pval(0, 4)"
> 
> is "< 2.2e-16". 
> 
>       Can someone provide an appropriate reference or sense of the 
> literature on the appropriate number to report for a Monte Carlo p-value
> 
> when the observed is more extreme than all the simulations?  A value of
> 
> 0 or "< 2.2e-16" violates my sense of the logic of this situation.  If
> 
> the appropriate number is 0.5/B, then the line "PVAL <- sum(tmp$results
> 
>  >= STATISTIC)/B" could be followed immediately by something like the 
> following: 
> 
>       if(PVAL==0) PVAL <- 0.5/B
> 
>       Comments?
>       Best Wishes,
>       Spencer Graves
> 
> kjetil at entelnet.bo wrote:
> 
> >On 8 Mar 2004 at 16:38, cfinet at ens-lyon.fr wrote:
> >
> >  
> >
> >>I do not manage to make a Fisher?s exact test with the next matrix :
> >>
> >>    
> >>
> >
> >You can consider using chisq.test() with the argument sim=TRUE:
> >
> >  
> >
> >>mat <- matrix(scan(), 7, 12, byrow=TRUE)
> >>    
> >>
> >1:  1    3    0    1    2    9    0    0    2     5     8     6
> >13:    0    3    3    0    0    0    0    5    0     3     0     0
> >25:    0    0    0    0    0   10    0    0    0     0    10     0
> >37:    0    2    0    2    6   14    0    0    6     0    10     6
> >49:    0    5    0    0    0    7    0    0    0     2     8     0
> >61:    0    0    1    9    4    7    2    1    4     2    12     5
> >73:    0    6    0    0    0    0    0    0    0     5     1     3
> >85: 
> >Read 84 items
> >
> >  
> >
> >>chisq.test(mat, sim=TRUE)
> >>    
> >>
> >
> >        Pearson's Chi-squared test with simulated p-value (based
> >        on 2000 replicates)
> >
> >data:  mat 
> >X-squared = 218.3366, df = NA, p-value = < 2.2e-16
> >
> >Kjetil Halvorsen
> >
> >
> >
> >  
> >
> >>    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> >>[1,]    1    3    0    1    2    9    0    0    2     5     8     6
> >>[2,]    0    3    3    0    0    0    0    5    0     3     0     0
> >>[3,]    0    0    0    0    0   10    0    0    0     0    10     0
> >>[4,]    0    2    0    2    6   14    0    0    6     0    10     6
> >>[5,]    0    5    0    0    0    7    0    0    0     2     8     0
> >>[6,]    0    0    1    9    4    7    2    1    4     2    12     5
> >>[7,]    0    6    0    0    0    0    0    0    0     5     1     3
> >>
> >>but I do not understand why it does not work since I obtain the next
> >>error message :
> >>
> >>
> >>
> >>    
> >>
> >>>fisher.test(enfin.matrix)
> >>>      
> >>>
> >>Error in fisher.test(enfin.matrix) : Bug in FEXACT: gave negative
> key
> >>
> >>thank you for considering my application
> >>
> >>c?dric finet
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
>



From petr.pikal at precheza.cz  Tue Mar  9 10:09:16 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 09 Mar 2004 10:09:16 +0100
Subject: [R] abline
In-Reply-To: <20040308175408.98816.qmail@web21403.mail.yahoo.com>
Message-ID: <404D97CC.26390.871E06@localhost>

Hi

On 8 Mar 2004 at 9:54, Grace Conlon wrote:

> if I want to specify y-coordinates for the heights of horizontal lines
> to go across a plot.
> 
> x <- c(1,2,3,6,4,8,4,7)
> y <- c(3,2,7,4,5,4,5,6)
> h <- c(3,5,7)
> plot(x,y)
> abline(y=h)
try abline(h=h)

see ?abline

Cheers


> 
> However I got error message:
> Warning message: 
> parameter "y" couldn't be set in high-level plot() function 
> (I tried abline(h=y) , it;s not what I want also)
> 
> Do u know why?
> 
> 
> 
> ---------------------------------
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From nusbj at hotmail.com  Tue Mar  9 10:55:48 2004
From: nusbj at hotmail.com (Z P)
Date: Tue, 09 Mar 2004 17:55:48 +0800
Subject: [R] maxima
Message-ID: <SEA2-F327Ud9BViWG430007a09c@hotmail.com>

Dear all,

suppose I have a bi-variate function f(x,y), I want to find the maxima. I 
define x and y vector, and get matrix z=f(x,y). how can I get which (x0,y0) 
makes z become the maxima?

I can do two loops to get the x0 and y0, but I think there may exist a 
function to do this.



From fzoellne at TechFak.Uni-Bielefeld.DE  Tue Mar  9 11:29:49 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Tue, 9 Mar 2004 11:29:49 +0100
Subject: [R] SVM unbalanced classes
Message-ID: <20040309102949.GB15856@hindemith.TechFak.Uni-Bielefeld.DE>

Hi!

I am using R 1.8.1 and the svm of the e1071 package for classification.
The problem is that I have unbalanced classes e.g. the first one is much bigger than the second one and therfore the svm is biased to the first class.
If I manually adjust the class size the bias disappears.
The question is then how to include this unequal class distribution to the svm (e.g. via wheights or costs)?

Yours,
Frank
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From thpe at hhbio.wasser.tu-dresden.de  Tue Mar  9 11:31:28 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 09 Mar 2004 11:31:28 +0100
Subject: [R] maxima
In-Reply-To: <SEA2-F327Ud9BViWG430007a09c@hotmail.com>
References: <SEA2-F327Ud9BViWG430007a09c@hotmail.com>
Message-ID: <404D9D00.1020103@hhbio.wasser.tu-dresden.de>

Z P wrote:
> Dear all,
> 
> suppose I have a bi-variate function f(x,y), I want to find the maxima. 
> I define x and y vector, and get matrix z=f(x,y). how can I get which 
> (x0,y0) makes z become the maxima?

If you have function values as matrix, you may simply use:

which(z==max(z), arr.ind=TRUE)

or you can consider to use an optimization algorithm directly on your 
function, see ?optim or ?nlm.

Hope it helps

Thomas P.



From andy_liaw at merck.com  Tue Mar  9 11:59:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Mar 2004 05:59:31 -0500
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7945@usrymx25.merck.com>

Make sure you either use full path to `typhoon.txt' as you did with
`typhoon.r', or setwd() to the directory where `typhoon.txt' lives before
reading it.

[Please use an informative subject for your message.]

Andy

> From: Andrew Abel
> 
> Hi,
> 
> I keep getting this error code when I try to run an R file 
> that reads in 
> data from another file. I know the file is there.
> 
> source("C:/R_Data/typhoon.r")
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `typhoon.txt'
> 
> 
> I hope you can help.
> 
> Regards
> 
> Andrew Abel
> 
> _________________________________________________________________

> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From asemeria at cramont.it  Tue Mar  9 12:26:14 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 9 Mar 2004 12:26:14 +0100
Subject: [R] (no subject)
Message-ID: <OF32780FE7.C9241ECB-ONC1256E52.003ED35E@tomware.it>





May be you aren't on the right working dir:
getwd(),setwd().
Your R-script are able to find the thyphoon.txt path?
Best!

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ozric at web.de  Tue Mar  9 12:34:00 2004
From: ozric at web.de (Christian Schulz)
Date: Tue, 9 Mar 2004 12:34:00 +0100
Subject: [R] SVM unbalanced classes
In-Reply-To: <20040309102949.GB15856@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040309102949.GB15856@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <200403091234.02169.ozric@web.de>

Hi,

SVM have got the option class.weights to put the 
apriori-distribution for your target variable.

To oversample, or undersample a specific class
you have imho write a own function!?

Perhaps it helps ,christian

 

Am Dienstag, 9. M?rz 2004 11:29 schrieb Frank Gerrit Zoellner:
> Hi!
>
> I am using R 1.8.1 and the svm of the e1071 package for classification.
> The problem is that I have unbalanced classes e.g. the first one is much
> bigger than the second one and therfore the svm is biased to the first
> class. If I manually adjust the class size the bias disappears.
> The question is then how to include this unequal class distribution to the
> svm (e.g. via wheights or costs)?
>
> Yours,
> Frank



From mangel at uco.es  Tue Mar  9 12:51:17 2004
From: mangel at uco.es (=?iso-8859-1?Q?Miguel_=C1ngel_G=F3mez-Nieto?=)
Date: Tue, 9 Mar 2004 12:51:17 +0100
Subject: [R] asking for information
Message-ID: <012f01c405cc$d6af5aa0$a575d696@rabinf35>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040309/ead6e044/attachment.pl

From hdopazo at cnio.es  Tue Mar  9 12:46:51 2004
From: hdopazo at cnio.es (Hernan Dopazo)
Date: Tue, 9 Mar 2004 12:46:51 +0100
Subject: [R] levelplot problems !!!
Message-ID: <200403091246.51972.hdopazo@cnio.es>

Dear R users,

I have changed my R version to the new 1.8.1 and some problems appears when 
using the previous levelplot code. 

This is a simple example:

a <-1:10
b <-11:20
j <- rnorm(100)
grid<-expand.grid(a = a, b = b)
levelplot(j~a*b, grid)

Normaly in my previous vs this was suffice to produce the levelplot.
Now, an empty  R graphics device appears with the following error message:

Error in grid.pack(frame = key.gf, row = 1, col = 1, grob = grid.rect(x 
=rep(0.5,  :        unused argument(s) (frame ...)

Are there any solution?
Thanks in advance
H


-- 
.........................................................
Hernan J. Dopazo
Bioinformatica. CNIO 
c/ Melchor Fern?ndez Almagro 3 
28029, Madrid, Espa?a 
Tfn: (34) 91 224 69 00 ext: 2428 
Fax: (34) 91 224 69 23 
http://bioinfo.cnio.es 
.........................................................



From dmurdoch at pair.com  Tue Mar  9 12:54:00 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 09 Mar 2004 06:54:00 -0500
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <Pine.LNX.4.44.0403090759070.6161-100000@gannet.stats>
References: <i1eq40plr4jm2bnrp3bvnjjdt976ipu4iq@4ax.com>
	<Pine.LNX.4.44.0403090759070.6161-100000@gannet.stats>
Message-ID: <tpbr401mkmtvahmjc7da9umkgq229710qr@4ax.com>

On Tue, 9 Mar 2004 08:03:44 +0000 (GMT), you wrote:

>No, both will find links in the same library as installing into (plus
>those which are fixed up on installation, e.g. to the base package).
>
>Several of us have looked for years for a fix, and this is the best scheme 
>we have come up with.  You can't put in absolute paths in the HTML as e.g. 
>a private library may be used with more than one version of R (or R may be 
>updated later).  Short of adding symbolic links to Windows (and getting 
>browsers to follow them), how do you propose `we should fix' it?

Here's a proposal:

All of check and build and install should default to the same library
location.  Check and build aren't meant to be permanent installs, so
if the package already exists there, it'll have to be temporarily
moved out of the way.

Duncan Murdoch



From Lennart.Borgman at astrazeneca.com  Tue Mar  9 13:34:30 2004
From: Lennart.Borgman at astrazeneca.com (Lennart.Borgman@astrazeneca.com)
Date: Tue, 9 Mar 2004 13:34:30 +0100 
Subject: [R] error() and C++ destructors
Message-ID: <26D5AB9F6512D611A8610001FA7E136F03278475@se-drc-mail4.selu.astrazeneca.net>

I am sending this reply on behalf of Erik (who is not a member of this
list).

- Lennart


-----Original Message-----
From: K?llen, Erik 
Sent: 9 mars 2004 11:37
To: Borgman, Lennart
Subject: RE: [R] error() and C++ destructors


I would do something like:

class error_exception {
public:
	error_exception(const string &str) : msg(str) {}
	string msg;
};

void my_error(const string &str) {
	throw error_exception(str);
}

int real_my_method(int a, char *b) {
	/*
	some code...
	*/
	return 0;
}

// this is the public method:
int my_method(int a, char *b) {
	try {
		return real_my_method(a, b);
	}
	catch (error_exception &e) {
		error(e.msg);
	}
}

You could probably even create a macro like:
#define R_METHOD_IMPL(rettype, name, paramlist) \
rettype real_##name paramlist; \
rettype name paramlist { \
	try { \
		return real_##name paramlist; \
	} \
	catch (error_exception &e) { \
		error(e.msg); \
	} \
} \
rettype real_##name paramlist


You would use this macro like:
R_METHOD_IMPL(int, my_method, (int a, char *b)) {
	// source code here
}

I think it would work, but I'm not sure (untested).


/Erik K?ll?n


-----Original Message-----
From: Vadim Ogranovich [mailto:vograno at evafunds.com]
Sent: 2 mars 2004 22:00
To: R Help List
Subject: [R] error() and C++ destructors


Hi,
 
I am writing C++ functions that are to be called via .Call() interface.
I'd been using error() (from R.h) to return to R if there is an error,
but then I realized that this might be not safe as supposedly error()
doesn't throw an exception and therefore some destructors do not get
called and some memory may leak. Here is a simple example
 
extern "C" void foo() {
    string str = "hello";
    error("message");
}
 
The memory allocated for str is leaked.
 
Did anyone think about this and find a way to work around the problem?
 
Thanks,
Vadim

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Mar  9 13:38:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 12:38:33 +0000 (GMT)
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <tpbr401mkmtvahmjc7da9umkgq229710qr@4ax.com>
Message-ID: <Pine.LNX.4.44.0403091235510.7714-100000@gannet.stats>

On Tue, 9 Mar 2004, Duncan Murdoch wrote:

> On Tue, 9 Mar 2004 08:03:44 +0000 (GMT), you wrote:
> 
> >No, both will find links in the same library as installing into (plus
> >those which are fixed up on installation, e.g. to the base package).
> >
> >Several of us have looked for years for a fix, and this is the best scheme 
> >we have come up with.  You can't put in absolute paths in the HTML as e.g. 
> >a private library may be used with more than one version of R (or R may be 
> >updated later).  Short of adding symbolic links to Windows (and getting 
> >browsers to follow them), how do you propose `we should fix' it?
> 
> Here's a proposal:
> 
> All of check and build and install should default to the same library
> location.  Check and build aren't meant to be permanent installs, so
> if the package already exists there, it'll have to be temporarily
> moved out of the way.

You may not own the main library and so not have permission to 
install/check/build there.  If you do, you can use check on an installed 
copy of the package.  As for build, this is one of the reasons why
Rcmd INSTALL --build was needed, as that installs in the standard place 
and then wraps up the installed copy.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angel_lul at hotmail.com  Tue Mar  9 14:39:50 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Tue, 09 Mar 2004 13:39:50 +0000
Subject: [R] asking for information
In-Reply-To: <012f01c405cc$d6af5aa0$a575d696@rabinf35>
References: <012f01c405cc$d6af5aa0$a575d696@rabinf35>
Message-ID: <404DC926.4020709@hotmail.com>

Hola,
Yes you can call R from C and viceversa, you should read the manual 
"Writing R extensions":
available in the section "manuals" from the r-project website
http://cran.r-project.org/doc/manuals/R-exts.pdf
Suerte,
Angel

Miguel ?ngel G?mez-Nieto wrote:

>Dear Sir:
>
>I am interesting in to use some R functions in some of our developments. But I have a problem which I have not found information in R documentation, for that I ask for help.
>
>Can I to call R functions from a C (or another language) program ?. ANd if the answer if yes, could you help me informe me where I can found information to respect?
>
>I will be very thanks to you, if you could help me
>
>Thanks a lot
>
>Best Regards
>
>---------------------------------------------------------------------
>Miguel ?ngel G?mez-Nieto
>University of C?rdoba
>Dept. Computing and Numerical Analysis
>Software and Knowledge Engineering and Database Researh Group
>Campus Universitario de Rabanales
>Building C2, Plant 3. E-14071 C?rdoba (Spain)
>Phone: +34-957-212082
>Fax: +34-957-218630
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>.
>
>  
>



From Lennart.Borgman at astrazeneca.com  Tue Mar  9 13:59:01 2004
From: Lennart.Borgman at astrazeneca.com (Lennart.Borgman@astrazeneca.com)
Date: Tue, 9 Mar 2004 13:59:01 +0100 
Subject: [R] error() and C++ destructors
Message-ID: <26D5AB9F6512D611A8610001FA7E136F03278477@se-drc-mail4.selu.astrazeneca.net>

And maybe I stressed Erik a bit, because he corrected himself some minutes
later (when I was no longer looking over his shoulder). Again on behalf of
Erik.

- Lennart


-----Original Message-----
From: K?llen, Erik 
Sent: 9 mars 2004 13:40
To: Borgman, Lennart
Subject: RE: [R] error() and C++ destructors


Whoops.

This code leaks memory for the exception.
It would be better to throw a pointer:

void my_error(const string &str) {
	throw new error_exception(str);
}

int my_method(int a, char *b) {
	try {
		return real_my_method(a, b);
	}
	catch (error_exception *pe) {
		static char error_msg[SOME_LARGE_NUMBER];
		strncpy(error_msg, pe->msg.c_str(), sizeof(error_msg));
		delete pe;
		error(error_msg);
	}
}

-----Original Message-----
From: Borgman, Lennart 
Sent: 9 mars 2004 13:35
To: 'r-help at stat.math.ethz.ch'
Subject: RE: [R] error() and C++ destructors


I am sending this reply on behalf of Erik (who is not a member of this
list).

- Lennart


-----Original Message-----
From: K?llen, Erik 
Sent: 9 mars 2004 11:37
To: Borgman, Lennart
Subject: RE: [R] error() and C++ destructors


I would do something like:

class error_exception {
public:
	error_exception(const string &str) : msg(str) {}
	string msg;
};

void my_error(const string &str) {
	throw error_exception(str);
}

int real_my_method(int a, char *b) {
	/*
	some code...
	*/
	return 0;
}

// this is the public method:
int my_method(int a, char *b) {
	try {
		return real_my_method(a, b);
	}
	catch (error_exception &e) {
		error(e.msg);
	}
}

You could probably even create a macro like:
#define R_METHOD_IMPL(rettype, name, paramlist) \
rettype real_##name paramlist; \
rettype name paramlist { \
	try { \
		return real_##name paramlist; \
	} \
	catch (error_exception &e) { \
		error(e.msg); \
	} \
} \
rettype real_##name paramlist


You would use this macro like:
R_METHOD_IMPL(int, my_method, (int a, char *b)) {
	// source code here
}

I think it would work, but I'm not sure (untested).


/Erik K?ll?n


-----Original Message-----
From: Vadim Ogranovich [mailto:vograno at evafunds.com]
Sent: 2 mars 2004 22:00
To: R Help List
Subject: [R] error() and C++ destructors


Hi,
 
I am writing C++ functions that are to be called via .Call() interface.
I'd been using error() (from R.h) to return to R if there is an error,
but then I realized that this might be not safe as supposedly error()
doesn't throw an exception and therefore some destructors do not get
called and some memory may leak. Here is a simple example
 
extern "C" void foo() {
    string str = "hello";
    error("message");
}
 
The memory allocated for str is leaked.
 
Did anyone think about this and find a way to work around the problem?
 
Thanks,
Vadim

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rgentlem at jimmy.harvard.edu  Tue Mar  9 14:20:13 2004
From: rgentlem at jimmy.harvard.edu (Robert Gentleman)
Date: Tue, 9 Mar 2004 08:20:13 -0500
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <Pine.LNX.4.44.0403091235510.7714-100000@gannet.stats>;
	from ripley@stats.ox.ac.uk on Tue, Mar 09, 2004 at 12:38:33PM
	+0000
References: <tpbr401mkmtvahmjc7da9umkgq229710qr@4ax.com>
	<Pine.LNX.4.44.0403091235510.7714-100000@gannet.stats>
Message-ID: <20040309082013.M26851@jimmy.harvard.edu>

On Tue, Mar 09, 2004 at 12:38:33PM +0000, Prof Brian Ripley wrote:
> On Tue, 9 Mar 2004, Duncan Murdoch wrote:
> 
> > On Tue, 9 Mar 2004 08:03:44 +0000 (GMT), you wrote:
> > 
> > >No, both will find links in the same library as installing into (plus
> > >those which are fixed up on installation, e.g. to the base package).
> > >
> > >Several of us have looked for years for a fix, and this is the best scheme 
> > >we have come up with.  You can't put in absolute paths in the HTML as e.g. 
> > >a private library may be used with more than one version of R (or R may be 
> > >updated later).  Short of adding symbolic links to Windows (and getting 
> > >browsers to follow them), how do you propose `we should fix' it?
> > 
> > Here's a proposal:
> > 
> > All of check and build and install should default to the same library
> > location.  Check and build aren't meant to be permanent installs, so
> > if the package already exists there, it'll have to be temporarily
> > moved out of the way.
> 
> You may not own the main library and so not have permission to 
> install/check/build there.  If you do, you can use check on an installed 
> copy of the package.  As for build, this is one of the reasons why
> Rcmd INSTALL --build was needed, as that installs in the standard place 
> and then wraps up the installed copy.

 Yes, but for automated checking of many packages (60+ in Bioconductor
 and over 200 in CRAN) it would be nice to be able to separate out the
 real issues from the supposed ones.

 Perhaps we need a mechanism to generate some sort of database of
 these (which is updated/modified by R INSTALL, for example). Then R
 CMD check could generate an index file of resolved/supplied 
 links for the library path that R is using, generally
 $R_HOME/library, and then we simply compare against that. It seems
 that we are currently making the wrong comparison. What might
 make more sense (it seems to me) is to check and see if the  links
 are resolved only against base R + any packages in the
 depends/suggests field of the package being  checked. Since these are
 the ones that we will try to ensure are  available at run
 time. References outside of that set could be warnings. 

 Here we are not trying to physically resolve the links, just make
 sure that they are resolvable given the version of R and the package
 dependencies. 

 Robert


> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+---------------------------------------------------------------------------+
| Robert Gentleman                 phone : (617) 632-5250                   |
| Associate Professor              fax:   (617)  632-2444                   |
| Department of Biostatistics      office: M1B20                            |
| Harvard School of Public Health  email: rgentlem at jimmy.harvard.edu        |
+---------------------------------------------------------------------------+



From jmacdon at med.umich.edu  Tue Mar  9 14:36:11 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 09 Mar 2004 08:36:11 -0500
Subject: [R] memory problem
Message-ID: <s04d8204.013@med-gwia-01a.med.umich.edu>

How many chips you can read is a function of how much RAM you have and
what chip it is. On a unix/linux box you will be able to read in and
process 143 of the HG-u95aV2 chips if you have about 2 Gb RAM. For the
larger U133A chips (RAE/MOE are about the same size), you will probably
need almost twice as much RAM. For the new version 2 chips you will
likely need about four times as much RAM (yep, that's 8 Gb!).

Some information about memory requirements and timings can be found
here:

http://stat-www.berkeley.edu/users/bolstad/ComputeRMAFAQ/size.html

HTH,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Joshi, Nina (NIH/NCI)" <joshini at mail.nih.gov> 03/08/04 12:15PM
>>>
I am trying to upload into R 143 Affymetrix chips onto using R on the
NIH
Nimbus server.  I can load 10 chips without a problem, however, when I
try
to load 143 I receive a error message: cannot create a vector of 523263
KB.
I have expanded the memory of R as follows:  R --min-vsize=10M
--max-vsize=2500M --min-nsize=10M -max-nsize=50M (as specified in help
in
R).  After running this command the memory in R is as follows:  

 

            Used     (Mb)      gc    trigger        (Mb)        limit
(Mb)      

Ncells   513502 13.8        10485760        280.0                1400

Vcells   142525   1.1        162625696     1240.8                2500

 

However, I am still getting the error cannot create a vector of 523263
KB.
Any suggestions/ideas? 

 

Thanks.,

 

Nina

 

Nina Joshi, PhD

NIH/NCI/ Genetics Branch

National Naval Medical Center, Bldg. 8, Rm. 5101

8901 Wisconsin Ave.

Bethesda, MD. 20889-5101

(301) 435-5436 - phone

(301) 496-0047 - fax

 

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From david.meyer at wu-wien.ac.at  Tue Mar  9 15:42:34 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Tue, 9 Mar 2004 15:42:34 +0100
Subject: [R] SVM unbalanced classes
In-Reply-To: <200403091141.i29BL0Ts000655@hypatia.math.ethz.ch>
References: <200403091141.i29BL0Ts000655@hypatia.math.ethz.ch>
Message-ID: <20040309154234.56ad63e3.david.meyer@wu-wien.ac.at>

You might consider using the `weight' argument of svm().

Best,

David.


Hi!

I am using R 1.8.1 and the svm of the e1071 package for classification.
The problem is that I have unbalanced classes e.g. the first one is much
bigger than the second one and therfore the svm is biased to the first
class.
If I manually adjust the class size the bias disappears.
The question is then how to include this unequal class distribution to
the svm (e.g. via wheights or costs)?

Yours,
Frank
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From maechler at stat.math.ethz.ch  Tue Mar  9 14:47:20 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 9 Mar 2004 14:47:20 +0100
Subject: [R] levelplot problems !!!
In-Reply-To: <200403091246.51972.hdopazo@cnio.es>
References: <200403091246.51972.hdopazo@cnio.es>
Message-ID: <16461.51944.352782.328727@gargle.gargle.HOWL>

>>>>> "Hernan" == Hernan Dopazo <hdopazo at cnio.es>
>>>>>     on Tue, 9 Mar 2004 12:46:51 +0100 writes:

    Hernan> Dear R users,
    Hernan> I have changed my R version to the new 1.8.1 and some problems appears when 
    Hernan> using the previous levelplot code. 

    Hernan> This is a simple example:

after
	library(lattice)
!

    Hernan> a <-1:10
    Hernan> b <-11:20
    Hernan> j <- rnorm(100)
    Hernan> grid<-expand.grid(a = a, b = b)
    Hernan> levelplot(j~a*b, grid)

This works fine for me.

    Hernan> Normaly in my previous vs this was suffice to
    Hernan> produce the levelplot.  Now, an empty R graphics
    Hernan> device appears with the following error message:

    Hernan> Error in grid.pack(frame = key.gf, row = 1, col = 1,
    Hernan> grob = grid.rect(x =rep(0.5, : unused argument(s)
    Hernan> (frame ...)


I assume that you somehow managed to use an old version of
"grid" or "lattice" instead of the ones that ``come with R 1.8.1''.

Can you try to look at 
    .path.package()

after library(grid) ; library(lattice) 
and make sure that both packages come from the same place as
"base" e.g. ?

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From sebastiendurand at videotron.ca  Tue Mar  9 15:09:18 2004
From: sebastiendurand at videotron.ca (Sebastien Durand)
Date: Tue, 09 Mar 2004 09:09:18 -0500
Subject: [R] Package cclust error
Message-ID: <a06020401bc737f29021b@[192.168.2.3]>

Hello, here is my problem,

After looking at the mail archives, I found a 
description of the error I get when I use this 
package. 
At first I even tought that they were showing how to solve it.

But the thing is that by saying "the programmer 
forgot drop=FALSE" doesn't show me how I should 
get rid of the problem
I have looked inside the package very quickly and 
I found three files, I am not sure what to do to 
fix this (the problem is describe bellow)

I would be very happy to have someone show me how to solve my problem....

Thanks a lot

S?bastien Durand



On Wed, 20 Feb 2002, David Wartel wrote:

>
>  I have to solve a clustering problem.
>  My first step is to determinate the number of clusters, that's why I 'm using
...snip...
>... number of clusters.
>  A function is already implemented in R to calculate this index :
>
>  clustIndex(cl,x, index="calinski")

Where is that from? It's not part of R -- package cclust, perhaps?

>  where cl is the result of a clustering method , for instance:
>
>  cclust(x,k,itermax,verbose=TRUE,method="kmeans")
>
>  My probleme is that I can't calculate the Calinski's index when a cluster
>  contains only one datapoint :
>
>  Error in cov(x[cluster == l, ]) : supply both x and y or a matrix-like x

The programmer forgot drop=FALSE, it would seem.



From deepayan at stat.wisc.edu  Tue Mar  9 15:19:13 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 9 Mar 2004 08:19:13 -0600
Subject: [R] levelplot problems !!!
In-Reply-To: <200403091246.51972.hdopazo@cnio.es>
References: <200403091246.51972.hdopazo@cnio.es>
Message-ID: <200403090819.13821.deepayan@stat.wisc.edu>

On Tuesday 09 March 2004 05:46, Hernan Dopazo wrote:
> Dear R users,
>
> I have changed my R version to the new 1.8.1 and some problems appears
> when using the previous levelplot code.
>
> This is a simple example:
>
> a <-1:10
> b <-11:20
> j <- rnorm(100)
> grid<-expand.grid(a = a, b = b)
> levelplot(j~a*b, grid)
>
> Normaly in my previous vs this was suffice to produce the levelplot.
> Now, an empty  R graphics device appears with the following error
> message:
>
> Error in grid.pack(frame = key.gf, row = 1, col = 1, grob = grid.rect(x
> =rep(0.5,  :        unused argument(s) (frame ...)

This shouldn't happen (and doesn't happen for me). What OS and what version 
of lattice are you using ?

Deepayan



From poolloopus at yahoo.com  Tue Mar  9 15:53:38 2004
From: poolloopus at yahoo.com (S P)
Date: Tue, 9 Mar 2004 06:53:38 -0800 (PST)
Subject: [R] New Ranking Scheme - Help
Message-ID: <20040309145338.86656.qmail@web41010.mail.yahoo.com>

Hi all,

I am a biostatistician and I have developed my own
ranking system for clinical data. I would like to test
the efficiency of it w.r.t. to other ranking systems.
I would like to simulate the data and after assigning
ranks to my observed scores(after neglecting
dropouts), observe the type I error. If I want to do a
Kruuskal Wallis type of test, what test statistic
should I use to test for a difference of "delta"
between the two groups (I know "delta" since the data
is generated with that difference). The default K-W
test statistics test for a NULL difference and I want
to see how frequently my ranking system rejects the
correct null hypothesis of a "delta" difference.

(So my data is now in 2 arrays of unequal lengths due
to dropouts, which have the ranks for drug and
placebo.)

Thank you in advance for your help.

~S



From sje at shannon.mast.queensu.ca  Tue Mar  9 16:13:41 2004
From: sje at shannon.mast.queensu.ca (Stephen Dicey)
Date: Tue, 09 Mar 2004 10:13:41 -0500
Subject: [R] aic calculation
Message-ID: <404DDF25.61A3016B@shannon.mast.queensu.ca>

hello,
could somebody refer me to the reason R uses

-2*loglik + 2*(#param)+2

to calculate AIC?
thank you

--
Stoyan Iliev



From dray at biomserv.univ-lyon1.fr  Tue Mar  9 16:14:34 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Tue, 09 Mar 2004 10:14:34 -0500
Subject: [R] Package cclust error
In-Reply-To: <a06020401bc737f29021b@[192.168.2.3]>
Message-ID: <5.2.1.1.0.20040309101420.038b2008@biomserv.univ-lyon1.fr>

The problem is with this line of  clustIndex
zttw <- ttww(x, clres$size, clres$cluster) because it use cov function 
which can not be used on only one value

And the problem is that some indexes (e.g. "calinsky") do not use this value.
replace this line by #zttw <- ttww(x, clres$size, clres$cluster)
and it will work.
However, some indexes used this quantity ("scott", "marriot"...)
I think that the author must modify the function because it stops the 
function although the statistic could be computed


Sincerely,

At 09:09 09/03/2004, Sebastien Durand wrote:
>Hello, here is my problem,
>
>After looking at the mail archives, I found a description of the error I 
>get when I use this package. At first I even tought that they were showing 
>how to solve it.
>
>But the thing is that by saying "the programmer forgot drop=FALSE" doesn't 
>show me how I should get rid of the problem
>I have looked inside the package very quickly and I found three files, I 
>am not sure what to do to fix this (the problem is describe bellow)
>
>I would be very happy to have someone show me how to solve my problem....
>
>Thanks a lot
>
>S?bastien Durand
>
>
>
>On Wed, 20 Feb 2002, David Wartel wrote:
>
>>
>>  I have to solve a clustering problem.
>>  My first step is to determinate the number of clusters, that's why I 'm 
>> using
>...snip...
>>... number of clusters.
>>  A function is already implemented in R to calculate this index :
>>
>>  clustIndex(cl,x, index="calinski")
>
>Where is that from? It's not part of R -- package cclust, perhaps?
>
>>  where cl is the result of a clustering method , for instance:
>>
>>  cclust(x,k,itermax,verbose=TRUE,method="kmeans")
>>
>>  My probleme is that I can't calculate the Calinski's index when a cluster
>>  contains only one datapoint :
>>
>>  Error in cov(x[cluster == l, ]) : supply both x and y or a matrix-like x
>
>The programmer forgot drop=FALSE, it would seem.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From beskow at speech.kth.se  Tue Mar  9 16:17:45 2004
From: beskow at speech.kth.se (Jonas Beskow)
Date: Tue, 09 Mar 2004 16:17:45 +0100
Subject: [R] Significance of differences in RMS?
Message-ID: <404DE019.4050908@speech.kth.se>

Greetings,

I have the following problem:
I want to compare a "parameter trajectory", i.e. a series of real 
numbers (representing equidistant samples of a time-varying parameter) 
produced by some "model", to a reference trajectory, measured from the 
real world, in order to get a rating of how good the model that produced 
the first trajectory is. Ok, so I use the RMS of the difference between 
the two trajectories at each sample.

Then I have another model, producing another trajectory, leading to 
another RMS-value. Good. Now I want to clame that the two models are 
"equally good" on the basis of the RMS-values being similar, and require 
some sort of significance test to support this claim. But I can't assume 
normal distribution in this case since the values are squared (or can 
I?) so with my limited knowledge of statistics I'm stuck...

Any help is appreciated!

regards

- Jonas

PS I apologize for posting a question entirely unrelated to R but any 
R-related answers are of course welcome!

-- 
Jonas Beskow, Ph.D.                     Tel: +46 8 790 8965
Centre for Speech Technology            Fax: +46 8 790 7854
KTH                                    beskow at speech.kth.se
SE-10044 Stockholm, Sweden        www.speech.kth.se/~beskow



From jiafucang at hotmail.com  Tue Mar  9 16:29:50 2004
From: jiafucang at hotmail.com (Fucang Jia)
Date: Tue, 09 Mar 2004 23:29:50 +0800
Subject: [R] How to ascertain the number of clusters automatically?
Message-ID: <LAW11-F16YK2o9LyreW0001582d@hotmail.com>

Hi, everyone,

There is many small cells which can be classified into several big cells 
from the scanned image. K-means clustering does not work well in this 
condition. I have done hierarchical clustering on cells successfully which 
uses shortest distance between classes. The number of clusters is about 3, 
4, 5, 6, 7 generally. One can ascertain the number of clusters visually.  
But because there are thousands of images to be clustered. So it is humdrum 
to me. I want to know if there are any methods that can be used to ascertain 
the number of clusters automatically, especially in this case, only several 
clusters?

Thank you very much!

Best,

Fucang



From jschum at bgc-jena.mpg.de  Tue Mar  9 15:49:15 2004
From: jschum at bgc-jena.mpg.de (Jens Schumacher)
Date: Tue, 09 Mar 2004 15:49:15 +0100
Subject: [R] aic calculation
References: <404DDF25.61A3016B@shannon.mast.queensu.ca>
Message-ID: <404DD96B.8F1310C3@bgc-jena.mpg.de>

R calculates

AIC = -2*loglik + 2*(#param)

but you probably missed the variance parameter when counting the parameters.

Jens Schumacher
--

--------------------------------------------
Dr. Jens Schumacher
Max-Planck-Institut f. Biogeochemie
Winzerlaer Str. 10
D-07745 Jena
Germany

Tel: +49 (0)3641/576181
Fax: +49 (0)3641/577100
email: jens.schumacher at bgc-jena.mpg.de



From ripley at stats.ox.ac.uk  Tue Mar  9 16:42:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 15:42:58 +0000 (GMT)
Subject: [R] aic calculation
In-Reply-To: <404DDF25.61A3016B@shannon.mast.queensu.ca>
Message-ID: <Pine.LNX.4.44.0403091530320.12781-100000@gannet.stats>

AIC is calculated in many places in R, but I do not believe any use that 
formula.

Here is a guess as to your confusion: in linear models there are p
coefficients plus sigma^2 to be estimated and hence there is often an
extra 2 associated with the scale parameter.  For example, in

> gaussian()$aic
function (y, n, mu, wt, dev)
sum(wt) * (log(dev/sum(wt) * 2 * pi) + 1) + 2

Beyond that, additive constants do not matter in comparing AIC between 
models and the defn of log-likelihood is only up to an additive constant.  
So sometimes calculations omit constants common to all models: 
extractAIC.lm does, for example.

On Tue, 9 Mar 2004, Stephen Dicey wrote:

> could somebody refer me to the reason R uses
> 
> -2*loglik + 2*(#param)+2
> 
> to calculate AIC?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andel at ifi.unizh.ch  Tue Mar  9 16:56:43 2004
From: andel at ifi.unizh.ch (David Andel)
Date: 9 Mar 2004 16:56:43 +0100
Subject: [R] use of split lines in ess
Message-ID: <404DE93B.5050707@ifi.unizh.ch>

Hi all

I am very astonished that R generates a "syntax error" when I want to 
split up a line with a backslash, which usually works in any shell script.

R itself generates the "+" symbols at the beginning of following lines 
in a splitted line so I've tried with them as well, but also without 
success.

Searching in the h-help archive did not reveal any answer either.

How is this accomplished?

Thanks,
David



From rpeng at jhsph.edu  Tue Mar  9 16:43:48 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 09 Mar 2004 10:43:48 -0500
Subject: [R] aic calculation
In-Reply-To: <404DDF25.61A3016B@shannon.mast.queensu.ca>
References: <404DDF25.61A3016B@shannon.mast.queensu.ca>
Message-ID: <404DE634.1010307@jhsph.edu>

It doesn't, as far as I know.  The function AIC.logLik is defined as

 > AIC.logLik
function (object, ..., k = 2)
-2 * c(object) + k * attr(object, "df")
<environment: namespace:base>

-roger

Stephen Dicey wrote:
> hello,
> could somebody refer me to the reason R uses
> 
> -2*loglik + 2*(#param)+2
> 
> to calculate AIC?
> thank you
> 
> --
> Stoyan Iliev
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Tue Mar  9 17:04:52 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 09 Mar 2004 08:04:52 -0800
Subject: [R] aic calculation
In-Reply-To: <404DDF25.61A3016B@shannon.mast.queensu.ca>
References: <404DDF25.61A3016B@shannon.mast.queensu.ca>
Message-ID: <404DEB24.9020205@pdf.com>

      That quantity is called the "Akaike Information Criterion."  It 
dates back to original work of Akaike.  For more recent discussions and 
citations to earlier literature see, e.g.: 

      Brian Ripley (1996) Pattern Recognition and Neural Networks 
(Cambridge U. Pr.)

      Burnham and Anderson (2002) Model Selection and Multimodel 
Inference (Springer). 

      The latter contains errors, as Prof. Ripley indicated in earlier 
posts to this listserve.  However, it seems for me to still be useful. 

      hope this helps.  spencer graves

Stephen Dicey wrote:

>hello,
>could somebody refer me to the reason R uses
>
>-2*loglik + 2*(#param)+2
>
>to calculate AIC?
>thank you
>
>--
>Stoyan Iliev
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From dmurdoch at pair.com  Tue Mar  9 17:26:49 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 09 Mar 2004 11:26:49 -0500
Subject: [R] Different missing links on Windows in 'check' vs. 'install'
In-Reply-To: <Pine.LNX.4.44.0403091235510.7714-100000@gannet.stats>
References: <tpbr401mkmtvahmjc7da9umkgq229710qr@4ax.com>
	<Pine.LNX.4.44.0403091235510.7714-100000@gannet.stats>
Message-ID: <d5kr4051tlrsq1ce8tpvndi37u9ih6d0cc@4ax.com>

On Tue, 9 Mar 2004 12:38:33 +0000 (GMT), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote :

>On Tue, 9 Mar 2004, Duncan Murdoch wrote:
>> Here's a proposal:
>> 
>> All of check and build and install should default to the same library
>> location.  Check and build aren't meant to be permanent installs, so
>> if the package already exists there, it'll have to be temporarily
>> moved out of the way.
>
>You may not own the main library and so not have permission to 
>install/check/build there.  If you do, you can use check on an installed 
>copy of the package.  As for build, this is one of the reasons why
>Rcmd INSTALL --build was needed, as that installs in the standard place 
>and then wraps up the installed copy.

This isn't a revolutionary proposal, I'm mainly suggesting that we
regularize it a bit.  If I want to build a binary package, I should be
able to expect to use "Rcmd build --binary".  If "Rcmd INSTALL
--build" does a better job, then shouldn't "Rcmd build --binary" be
modified to do the same?

The only new part of my suggestion is to move existing packages out of
the way so that the new one can be installed temporarily.  As you say,
this won't always be possible, but if "Rcmd INSTALL --build" could
work, then it should be possible to do a temporary version of the same
thing.

So here's a refinement of my suggestion:

By default, Rcmd build and Rcmd check should attempt to do their work
in the default library install location.  If that fails, then they
could fail with a message telling you to use a  --library= option, or
they could automatically try working in a temporary directory, with a
warning that this may produce suboptimal results.

In either case, if a package with the given name is already installed,
they should attempt to rename it to a temporary name to get it out of
the way.  If this fails, then proceed as above.

Duncan Murdoch



From martinol at ensam.inra.fr  Tue Mar  9 17:35:51 2004
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Tue, 09 Mar 2004 17:35:51 +0100
Subject: [R] R and tmp directroy
Message-ID: <404DF267.9040203@ensam.inra.fr>

Hi all,

When I start R, it gives

mkdir: Ne peut cr?er le r?pertoire `/home/mart/tmp/Rtmp11729'.: No such file or directory


(I specify that my home path is /home/mart and the R version is 1.7.1)

How I have to configure R to use the directory /tmp instead of 
/home/mart/tmp ?
What is utility of this directory tmp for R?


Thanks,
Olivier



From susan_lin99 at yahoo.com  Tue Mar  9 18:11:55 2004
From: susan_lin99 at yahoo.com (Susan Lin)
Date: Tue, 9 Mar 2004 09:11:55 -0800 (PST)
Subject: [R] how to use conditional statements to handle exceptions?
Message-ID: <20040309171155.48562.qmail@web21202.mail.yahoo.com>

Hello,

I have a problem to handle the following statements.
  
  for(i in [1:3])
  {
    file=paste("file", i, ".dat")
    bb <- read.table(file)
    x11()
    plot(bb)
    dev.off()

  }
When the input .dat file is empty, the program stops
running and an error message appears. Could someone
tell me how to handle this exception?

Thanks



From rossini at blindglobe.net  Tue Mar  9 18:24:14 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 09 Mar 2004 09:24:14 -0800
Subject: [R] use of split lines in ess
In-Reply-To: <404DE93B.5050707@ifi.unizh.ch> (David Andel's message of "9
	Mar 2004 16:56:43 +0100")
References: <404DE93B.5050707@ifi.unizh.ch>
Message-ID: <85wu5u9bnl.fsf@servant.blindglobe.net>


it looks like you are wondering about something with ESS.  Could you
be more precise about the exact conditions?  Also, the ess-help
mailing list might be a better place to post.  Version #'s of R, and
if it is an ESS problem, ESS and (X)Emacs would be useful.


"David Andel" <andel at ifi.unizh.ch> writes:

> I am very astonished that R generates a "syntax error" when I want to
> split up a line with a backslash, which usually works in any shell
> script.
>
> R itself generates the "+" symbols at the beginning of following lines
> in a splitted line so I've tried with them as well, but also without
> success.
>
> Searching in the h-help archive did not reveal any answer either.
>
> How is this accomplished?
>
> Thanks,
> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From stephane.dray at umontreal.ca  Tue Mar  9 18:33:34 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Tue, 09 Mar 2004 12:33:34 -0500
Subject: [R] Creating its own family
Message-ID: <5.2.1.1.0.20040309122939.037b1478@magellan.umontreal.ca>

Hello,
I would like to create my own family for glm modelling

If we consider a matrix Y, I would like to model Yij/var(Yj) with an 
inverse variance link.
I have create my own family inspired by the negative.binomial of MASS:

#########################################################
myfamily=function (varcol)
{
     env <- new.env(parent = .GlobalEnv)

     assign(".varcol",varcol,envir=env)
     famname="myfamily"
     link="inv col var"
     linkfun=function(.varcol,mu){mu/(.varcol)}
     linkinv=function(.varcol,eta){eta*(.varcol)}
     variance <- function (mu) rep.int(1, length(mu))
     validmu <- function (mu) TRUE

     dev.resids <- function (y, mu, wt) wt * ((y - mu)^2)
     aic <- function (y, n, mu, wt, dev) sum(wt) * (log(dev/sum(wt) * 2 * 
pi) + 1) + 2
     mu.eta=function (eta) rep.int(1, length(eta))
     initialize <- expression({
     n <- rep.int(1, nobs)
     mustart <- y
     })

     environment(variance) <- environment(validmu) <- 
environment(dev.resids) <- environment(aic) <- env

     structure(list(family = famname, link = link, linkfun = linkfun,
         linkinv = linkinv, variance = variance, dev.resids = dev.resids,
         aic = aic, mu.eta = mu.eta, initialize = initialize,
         validmu = validmu), class = "family")
}
#####################################################

But it does not work when I try it

 > Y=matrix(runif(50),10,5)
 > glm(as.vector(t(Y))~x,family=myfamily(rep(apply(Y,2,var),10)))
Error in family$linkfun(mustart) : Argument "mu" is missing, with no default
 >

Hope that someone could help me,

Thanks in advances,
Sincerely.

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From bates at stat.wisc.edu  Tue Mar  9 18:36:32 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 09 Mar 2004 11:36:32 -0600
Subject: [R] use of split lines in ess
In-Reply-To: <404DE93B.5050707@ifi.unizh.ch>
References: <404DE93B.5050707@ifi.unizh.ch>
Message-ID: <6r3c8hzzvj.fsf@bates4.stat.wisc.edu>

"David Andel" <andel at ifi.unizh.ch> writes:

> I am very astonished that R generates a "syntax error" when I want to
> split up a line with a backslash, which usually works in any shell
> script.

One way to diminish the astonishment factor when using software is to
try reading the documentation. :-)

The backslash in part of the R syntax (except within quoted strings)
so it generates a syntax error.

If you want to force continuation of an expression to the next input
line then ensure that the partial expression that you type is not
syntactically complete.  Often you can use { or ( on the first line
with the pairing } or ) on the last line.

> R itself generates the "+" symbols at the beginning of following lines
> in a splitted line so I've tried with them as well, but also without
> success.

> Searching in the h-help archive did not reveal any answer either.

> How is this accomplished?



From monica.palaseanu-lovejoy at stud.man.ac.uk  Tue Mar  9 18:44:17 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Tue, 9 Mar 2004 17:44:17 -0000
Subject: [R] Error message - what does it mean???
In-Reply-To: <200403091127.i29BL0Ta000655@hypatia.math.ethz.ch>
Message-ID: <E1B0lHB-0006K3-Do@probity.mcc.ac.uk>

Hi,

I am trying to calculate mahalanobis distances for a matrix x with 
n*p variables. I am getting the following error:

md2 <- mahalanobis(x, center, cov)
Error in solve.default(cov, tol = tol.inv) : 
        system is computationally singular: reciprocal condition 
number = 2.11165e-009

What does it means?

Thank you so much for any help,

Monica



From ripley at stats.ox.ac.uk  Tue Mar  9 19:07:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 18:07:40 +0000 (GMT)
Subject: [R] Error message - what does it mean???
In-Reply-To: <E1B0lHB-0006K3-Do@probity.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0403091803250.13153-100000@gannet.stats>

On Tue, 9 Mar 2004, Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> I am trying to calculate mahalanobis distances for a matrix x with 
> n*p variables. I am getting the following error:
> 
> md2 <- mahalanobis(x, center, cov)
> Error in solve.default(cov, tol = tol.inv) : 
>         system is computationally singular: reciprocal condition 
> number = 2.11165e-009
> 
> What does it means?

Error messages usually mean what they say, and it hard to see what you
mind to understand here.  Whatever you supplied for `cov' is a singular
matrix to the default tolerance, described on the help page.  If you know 
what is going on with your data, you could reduce the argument `tol.inv'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Benjamin.STABLER at odot.state.or.us  Tue Mar  9 19:11:22 2004
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 9 Mar 2004 10:11:22 -0800 
Subject: [R] Decision Trees
Message-ID: <76A000A82289D411952F001083F9DD06047FE505@exsalem4-bu.odot.state.or.us>

Thanks for the insight.  I think we will try both CART (rpart) and C4.5
(J4.8) and see what happens.  As always, the R community is so helpful.

Thanks,
Ben

>-----Original Message-----
>From: Ko-Kang Kevin Wang [mailto:k.wang at auckland.ac.nz]
>Sent: Tuesday, March 09, 2004 12:08 AM
>To: 'Prof Brian Ripley'; STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch
>Subject: RE: [R] Decision Trees
>
>
>Hi,
>
>> -----Original Message-----
>>
>> WEKA includes a re-implementation of the ideas behind C4.5,
>> but not C4.5.
>
>If my memory serves me right, WEKA people called this
>"re-implementation" J4.8.
>
>Kevin
>
>--------------------------------------------
>Ko-Kang Kevin Wang, MSc(Hon)
>Statistics Workshops Co-ordinator
>Student Learning Centre
>University of Auckland
>New Zealand
>
>



From martinol at ensam.inra.fr  Tue Mar  9 19:09:36 2004
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Tue, 09 Mar 2004 19:09:36 +0100
Subject: [R] R and tmp directroy
References: <404DF267.9040203@ensam.inra.fr>
Message-ID: <404E0860.7090501@ensam.inra.fr>

Finally, i found the answer for my question...
it is only necessary to add
setenv TMPDIR /tmp/
in my .cshrc



Martin Olivier wrote:

> Hi all,
>
> When I start R, it gives
>
> mkdir: Ne peut cr?er le r?pertoire `/home/mart/tmp/Rtmp11729'.: No 
> such file or directory
>
>
> (I specify that my home path is /home/mart and the R version is 1.7.1)
>
> How I have to configure R to use the directory /tmp instead of 
> /home/mart/tmp ?
> What is utility of this directory tmp for R?
>
>
> Thanks,
> Olivier
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>

-- 

-------------------------------------------------------------
Martin Olivier
INRA - Unit? prot?omique           LIRMM - IFA/MAB
2, Place Viala                     161, rue Ada
34060 Montpellier C?dex 1          34392 Montpellier C?dex 5	

Tel : 04 99 61 26 14               Tel : O4 67 41 86 71
martinol at ensam.inra.fr             martin at lirmm.fr



From rpeng at jhsph.edu  Tue Mar  9 19:17:02 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 09 Mar 2004 13:17:02 -0500
Subject: [R] how to use conditional statements to handle exceptions?
In-Reply-To: <20040309171155.48562.qmail@web21202.mail.yahoo.com>
References: <20040309171155.48562.qmail@web21202.mail.yahoo.com>
Message-ID: <404E0A1E.9050300@jhsph.edu>

Take a look at try().

-roger

Susan Lin wrote:
> Hello,
> 
> I have a problem to handle the following statements.
>   
>   for(i in [1:3])
>   {
>     file=paste("file", i, ".dat")
>     bb <- read.table(file)
>     x11()
>     plot(bb)
>     dev.off()
> 
>   }
> When the input .dat file is empty, the program stops
> running and an error message appears. Could someone
> tell me how to handle this exception?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Mar  9 19:18:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 18:18:29 +0000 (GMT)
Subject: [R] R and tmp directroy
In-Reply-To: <404DF267.9040203@ensam.inra.fr>
Message-ID: <Pine.LNX.4.44.0403091811330.13241-100000@gannet.stats>

Probably you have TMPDIR set in your enviroment to point to 
/home/mart/tmp and that does not exist or is not writable.

Solution: don't have variables pointing to inappropriate places?

On Tue, 9 Mar 2004, Martin Olivier wrote:

> When I start R, it gives
> 
> mkdir: Ne peut cr?er le r?pertoire `/home/mart/tmp/Rtmp11729'.: No such file or directory
> 
> 
> (I specify that my home path is /home/mart and the R version is 1.7.1)
> 
> How I have to configure R to use the directory /tmp instead of 
> /home/mart/tmp ?
> What is utility of this directory tmp for R?

For temporary files, like all other Unix/Linux applications.
See ?tempdir as well as internal uses (for HTML links, the editor temp 
file and so on).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From abunn at montana.edu  Tue Mar  9 19:23:14 2004
From: abunn at montana.edu (Andy Bunn)
Date: Tue, 9 Mar 2004 11:23:14 -0700
Subject: [R] how to use conditional statements to handle exceptions?
In-Reply-To: <20040309171155.48562.qmail@web21202.mail.yahoo.com>
Message-ID: <001001c40603$99468260$78f05a99@msu.montana.edu>

If you mean to put a check in to see if the file exists then something
like this would work:

  for(i in 1:3){
    aFile <- paste("file", i, ".dat", sep = "")
    if(file.exists(aFile) == T){
        bb <- read.table(aFile, header = F)
        x11()
        plot(bb)
        dev.off()
    }
  }


If you mean that you want to check to see that it has data in it you
could ammend the if statement to something like 

if(nrow(aFile) > 0)...


HTH, Andy



From ripley at stats.ox.ac.uk  Tue Mar  9 19:33:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 18:33:48 +0000 (GMT)
Subject: [R] how to use conditional statements to handle exceptions?
In-Reply-To: <20040309171155.48562.qmail@web21202.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403091830410.15491-100000@gannet.stats>

On Tue, 9 Mar 2004, Susan Lin wrote:

> I have a problem to handle the following statements.
>   
>   for(i in [1:3])
>   {
>     file=paste("file", i, ".dat")

Don't you mean paste("file", i, ".dat", sep="") ?  Or are your files
called `file 1 .dat'?

>     bb <- read.table(file)
>     x11()
>     plot(bb)
>     dev.off()

This opens a device, plots and closes it instantly.  Is that what you 
want?  I would have had

par(ask=TRUE)

outside the loop, which will automatically launch as graphics device if 
none is open.

> 
>   }
> When the input .dat file is empty, the program stops
> running and an error message appears. Could someone
> tell me how to handle this exception?

?try

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From susan_lin99 at yahoo.com  Tue Mar  9 19:49:52 2004
From: susan_lin99 at yahoo.com (Susan Lin)
Date: Tue, 9 Mar 2004 10:49:52 -0800 (PST)
Subject: [R] read.table() question
Message-ID: <20040309184952.17228.qmail@web21201.mail.yahoo.com>

Hi,

In the following code, I got an error meesage if an
input file is empty, and the program stopped running.
Could someone to tell me how to handle this problem. I
want the program to keep running. Thanks.
for i in [1:3]
{  
    file=paste("file", i, ".dat")
    x <- read.data(file)
    x(11)
    plot(x);
    dev.off()

}



From ripley at stats.ox.ac.uk  Tue Mar  9 20:04:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Mar 2004 19:04:27 +0000 (GMT)
Subject: [R] Creating its own family
In-Reply-To: <5.2.1.1.0.20040309122939.037b1478@magellan.umontreal.ca>
Message-ID: <Pine.LNX.4.44.0403091853540.15568-100000@gannet.stats>

I think you want

linkfun <- function(mu){mu/varcol}
linkinv <- function(eta){eta*varcol)}

etc, that is a function of a single argument.  Also, you can use lexical 
scope to capture varcol, as in the above: it will be in the environment of 
linkfun/inv.  You did not set its environment as MASS does, and you do not 
need to.  (MASS could have been written differently if it did not share 
code with S.)

On Tue, 9 Mar 2004, Stephane DRAY wrote:

> Hello,
> I would like to create my own family for glm modelling
> 
> If we consider a matrix Y, I would like to model Yij/var(Yj) with an 
> inverse variance link.
> I have create my own family inspired by the negative.binomial of MASS:
> 
> #########################################################
> myfamily=function (varcol)
> {
>      env <- new.env(parent = .GlobalEnv)
> 
>      assign(".varcol",varcol,envir=env)
>      famname="myfamily"
>      link="inv col var"
>      linkfun=function(.varcol,mu){mu/(.varcol)}
>      linkinv=function(.varcol,eta){eta*(.varcol)}
>      variance <- function (mu) rep.int(1, length(mu))
>      validmu <- function (mu) TRUE
> 
>      dev.resids <- function (y, mu, wt) wt * ((y - mu)^2)
>      aic <- function (y, n, mu, wt, dev) sum(wt) * (log(dev/sum(wt) * 2 * 
> pi) + 1) + 2
>      mu.eta=function (eta) rep.int(1, length(eta))
>      initialize <- expression({
>      n <- rep.int(1, nobs)
>      mustart <- y
>      })
> 
>      environment(variance) <- environment(validmu) <- 
> environment(dev.resids) <- environment(aic) <- env
> 
>      structure(list(family = famname, link = link, linkfun = linkfun,
>          linkinv = linkinv, variance = variance, dev.resids = dev.resids,
>          aic = aic, mu.eta = mu.eta, initialize = initialize,
>          validmu = validmu), class = "family")
> }
> #####################################################
> 
> But it does not work when I try it
> 
>  > Y=matrix(runif(50),10,5)
>  > glm(as.vector(t(Y))~x,family=myfamily(rep(apply(Y,2,var),10)))
> Error in family$linkfun(mustart) : Argument "mu" is missing, with no default
>  >
> 
> Hope that someone could help me,
> 
> Thanks in advances,
> Sincerely.
> 
> St?phane DRAY
> -------------------------------------------------------------------------------------------------- 
> 
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
> 
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> -------------------------------------------------------------------------------------------------- 
> 
> Web                                          http://www.steph280.freesurf.fr/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vograno at evafunds.com  Tue Mar  9 20:32:48 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 9 Mar 2004 11:32:48 -0800
Subject: [R] error() and C++ destructors
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3A9C@phost015.intermedia.net>

One shortcoming of Erik's solution is that it can only catch the exceptions of type error_exception. For example it won't work if my code calls some third party library that can throw exceptions of some other types.

In case it's of interest to someone here is the boilerplate that I ended up using (note the similarity between my errMsg and error_message from Erik's solution):

extern "C" SEXP foo(SEXP x) {
	int hasFailed=0;
	char errMsg[2048];

	// NO C++ objects above this line
	try {
		// do the work
		...
	}
	cach (std::exception e) {
		hasFailed = 1;
		strncopy(errMsg, e.what(), sizeof(errMsg));
	}
	catch (OtherException e) {
		hasFailed = 1;
		...
	}

	// NO C++ objects below this line

	if (hasFailed) {
		error(errMsg);
	}

	return x;
}

One thing I don't like about my solution is that I need to remember to set hasFailed in each and every catch block (and "I often remember to forget these sort of things" :-)

> -----Original Message-----
> From: Lennart.Borgman at astrazeneca.com 
> [mailto:Lennart.Borgman at astrazeneca.com] 
> Sent: Tuesday, March 09, 2004 4:59 AM
> To: r-help at stat.math.ethz.ch
> Subject: RE: [R] error() and C++ destructors
> 
> 
> And maybe I stressed Erik a bit, because he corrected himself 
> some minutes later (when I was no longer looking over his 
> shoulder). Again on behalf of Erik.
> 
> - Lennart
> 
> 
> -----Original Message-----
> From: K?llen, Erik 
> Sent: 9 mars 2004 13:40
> To: Borgman, Lennart
> Subject: RE: [R] error() and C++ destructors
> 
> 
> Whoops.
> 
> This code leaks memory for the exception.
> It would be better to throw a pointer:
> 
> void my_error(const string &str) {
> 	throw new error_exception(str);
> }
> 
> int my_method(int a, char *b) {
> 	try {
> 		return real_my_method(a, b);
> 	}
> 	catch (error_exception *pe) {
> 		static char error_msg[SOME_LARGE_NUMBER];
> 		strncpy(error_msg, pe->msg.c_str(), sizeof(error_msg));
> 		delete pe;
> 		error(error_msg);
> 	}
> }
> 
> -----Original Message-----
> From: Borgman, Lennart 
> Sent: 9 mars 2004 13:35
> To: 'r-help at stat.math.ethz.ch'
> Subject: RE: [R] error() and C++ destructors
> 
> 
> I am sending this reply on behalf of Erik (who is not a 
> member of this list).
> 
> - Lennart
> 
> 
> -----Original Message-----
> From: K?llen, Erik 
> Sent: 9 mars 2004 11:37
> To: Borgman, Lennart
> Subject: RE: [R] error() and C++ destructors
> 
> 
> I would do something like:
> 
> class error_exception {
> public:
> 	error_exception(const string &str) : msg(str) {}
> 	string msg;
> };
> 
> void my_error(const string &str) {
> 	throw error_exception(str);
> }
> 
> int real_my_method(int a, char *b) {
> 	/*
> 	some code...
> 	*/
> 	return 0;
> }
> 
> // this is the public method:
> int my_method(int a, char *b) {
> 	try {
> 		return real_my_method(a, b);
> 	}
> 	catch (error_exception &e) {
> 		error(e.msg);
> 	}
> }
> 
> You could probably even create a macro like:
> #define R_METHOD_IMPL(rettype, name, paramlist) \
> rettype real_##name paramlist; \
> rettype name paramlist { \
> 	try { \
> 		return real_##name paramlist; \
> 	} \
> 	catch (error_exception &e) { \
> 		error(e.msg); \
> 	} \
> } \
> rettype real_##name paramlist
> 
> 
> You would use this macro like:
> R_METHOD_IMPL(int, my_method, (int a, char *b)) {
> 	// source code here
> }
> 
> I think it would work, but I'm not sure (untested).
> 
> 
> /Erik K?ll?n
> 
> 
> -----Original Message-----
> From: Vadim Ogranovich [mailto:vograno at evafunds.com]
> Sent: 2 mars 2004 22:00
> To: R Help List
> Subject: [R] error() and C++ destructors
> 
> 
> Hi,
>  
> I am writing C++ functions that are to be called via .Call() 
> interface. I'd been using error() (from R.h) to return to R 
> if there is an error, but then I realized that this might be 
> not safe as supposedly error() doesn't throw an exception and 
> therefore some destructors do not get called and some memory 
> may leak. Here is a simple example
>  
> extern "C" void foo() {
>     string str = "hello";
>     error("message");
> }
>  
> The memory allocated for str is leaked.
>  
> Did anyone think about this and find a way to work around the problem?
>  
> Thanks,
> Vadim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From hb at maths.lth.se  Tue Mar  9 21:20:16 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 9 Mar 2004 21:20:16 +0100
Subject: [R] how to use conditional statements to handle exceptions?
In-Reply-To: <20040309171155.48562.qmail@web21202.mail.yahoo.com>
Message-ID: <008801c40613$ef756510$e502eb82@maths.lth.se>

For R v1.8.0 you can use tryCatch() (it's great) to catch objects of
class 'condition' and just print() them (instead of letting them
generate errors) like this:

for(i in 1:3) {
  file <- paste("file", i, ".dat", sep="")
  tryCatch({
    bb <- read.table(file)
    x11()
    plot(bb)
    dev.off()
  }, condition=function(ex) {
    print(ex);
  })
}

As soon as/if an error is generated, tryCatch() will catch it an call
the condition function, which will print it.

Details:
If the file does not exists when calling read.table() both a warning
and an error is generated. Since warnings are now of class
simpleWarning and (stop) errors are of class simpleError and both
these are subclasses (via the abstract classes 'warning' and 'error',
respectively) of class 'condition', which is the root class of all
exception classes, it is best to catch by using 'condition' above. If
one use 'error=...' instead, warnings will still be shown.

Moreover, a shorter version of the above would be to use
'condition=print'.
 

Some further comments:
 1) Your code is not correct, e.g. "[1:3]".
    Please submit example code that is executable!
 2) Did you forget sep="" in the paste line?
 1) avoid using "=" to assign; or at least be consistent.

Cheers

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Susan Lin
> Sent: den 9 mars 2004 18:12
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to use conditional statements to handle exceptions?
> 
> 
> Hello,
> 
> I have a problem to handle the following statements.
>   
>   for(i in [1:3])
>   {
>     file=paste("file", i, ".dat")
>     bb <- read.table(file)
>     x11()
>     plot(bb)
>     dev.off()
> 
>   }
> When the input .dat file is empty, the program stops
> running and an error message appears. Could someone
> tell me how to handle this exception?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From GPetris at uark.edu  Tue Mar  9 22:11:51 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 9 Mar 2004 15:11:51 -0600 (CST)
Subject: [R] read.table() question
In-Reply-To: <20040309184952.17228.qmail@web21201.mail.yahoo.com> (message
	from Susan Lin on Tue, 09 Mar 2004 10:49:52 -0800 (PST))
References: <20040309184952.17228.qmail@web21201.mail.yahoo.com>
Message-ID: <200403092111.i29LBpHK014622@definetti.uark.edu>


see

?try

HTH,
Giovanni

> Date: Tue, 09 Mar 2004 10:49:52 -0800 (PST)
> From: Susan Lin <susan_lin99 at yahoo.com>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> 
> Hi,
> 
> In the following code, I got an error meesage if an
> input file is empty, and the program stopped running.
> Could someone to tell me how to handle this problem. I
> want the program to keep running. Thanks.
> for i in [1:3]
> {  
>     file=paste("file", i, ".dat")
>     x <- read.data(file)
>     x(11)
>     plot(x);
>     dev.off()
> 
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From vograno at evafunds.com  Tue Mar  9 22:20:58 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 9 Mar 2004 13:20:58 -0800
Subject: [R] how to continue develop package
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3A9E@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040309/be6e0a0a/attachment.pl

From JSweval at illumigen.com  Tue Mar  9 23:41:17 2004
From: JSweval at illumigen.com (John Sweval)
Date: Tue, 9 Mar 2004 14:41:17 -0800
Subject: [R] Adding data.frames together
Message-ID: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040309/6678f09a/attachment.pl

From lanstin at aol.net  Tue Mar  9 23:44:18 2004
From: lanstin at aol.net (Christopher Austin-Lane)
Date: Tue, 9 Mar 2004 17:44:18 -0500 (EST)
Subject: [R] Slow reshape from 5x600000 to 6311 x 132
In-Reply-To: <200403050828.45392.ozric@web.de>
References: <40480285.7080808@aol.net> <200403050828.45392.ozric@web.de>
Message-ID: <404E48C2.7080601@aol.net>

Thanks for the info - it inspired me to keep tweaking.

R --min-vsize=400M --min-nsize=6M

allowed me to run in a few minutes - I had been trying larger values of 
--min-vsize (600M) (I have 1 Gig of RAM, but a lot of stuff running), 
but that was evidentally causing R to swap during the reshape, which 
caused it to really slow down.

--Chris

Christian Schulz wrote on 3/5/04, 2:28 AM:

 > Hi,
 >
 > my reshape's  from ~1.4 million obs. to  ~150.00 obs. & 50 attr. goes
 > surprinsing fast (1-2 miniutes), but is less complex then yours.



From andy_liaw at merck.com  Tue Mar  9 23:57:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Mar 2004 17:57:23 -0500
Subject: [R] how to continue develop package
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7957@usrymx25.merck.com>

My suggestion is to treat the installed package as `binary', even if the
code is purely in R.  This way you will only make modifications to the
_source_ and make up the `binary' as needed.  It's really not burdensome.

Your second question has nothing to do with ESS.  When you modify objects on
the search list, the modified copy is placed in the global environment
(pos=1).  After you experimented with the modification and are happy with
it, just place it in the package and re-install.

It doesn't make sense to edit code in the installed package directly:  That
will be the only copy of the changed package, and you won't be able to
package that up for installation by others, at least not easily.

HTH,
Andy


> From: Vadim Ogranovich
> 
> Hi,
>  
> I have completed a prototype of a package, say FOO, and now I want to
> start using it as an ordinary R package, i.e. attach it via
> library("FOO"). On the other hand I will be adding functionality and
> fixing bugs so the code is going to change a lot. There is a couple of
> problems that don't know how to solve:
>  
> 1. To be able to use library() the package must be INSTALLED. The
> installation creates a copy of the code. So it seems each 
> time I change
> the code I need to repeat R CMD INSTALL. This is not too bad, 
> but maybe
> there is a better way
> 2. (This is probably more of an ESS question) Suppose I have attached
> FOO via library("foo") which is now at the second position of 
> the search
> list. Now if I use ESS to modify the definition of some function goo()
> from the package "FOO" and send the new definition to R it will NOT
> replace the old definition, rather creates a new function at the first
> position of the search list. The new function will of course 
> overshadow
> the old one so I'll achieve the effect I want, but I'd feel 
> better if I
> could replace the old definition rather than overshadow it. Does ESS
> allow this sort of things?
>  
> These are my specific questions, however if you can suggest a 
> different
> way of on-going development of a package I'd love to hear from you.
>  
> Thanks,
> Vadim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From thpe at hhbio.wasser.tu-dresden.de  Wed Mar 10 00:04:30 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 10 Mar 2004 00:04:30 +0100
Subject: [R] years from as.POSIXlt
In-Reply-To: <Pine.LNX.4.44.0403090738300.6161-100000@gannet.stats>
References: <Pine.LNX.4.44.0403090738300.6161-100000@gannet.stats>
Message-ID: <404E4D7E.5040105@hhbio.wasser.tu-dresden.de>

Prof Brian Ripley wrote:

> See ?julian, which says 
> 
> Note:
> 
>      Other components such as the day of the month or the year are very
>      easy to computes: just use 'as.POSIXlt' and extract the relevant
>      component.
> 

Hello,

unfortunately not all mentioned functions work on all machines. Where

 > months(.leap.seconds)

works on all systems, a call to

 > julian(leap.seconds)

workes perfectly only on Linux (R 1.8.0) but failed on two different XP 
Machines (German XP version):

Error in fromchar(x) : character string is not in a standard unambiguous 
format

I've tested several things, several versions of msvcrt.dll, different 
PATH settings, locale set to German or USA, R 1.8.1 and R 1.7.1, but the 
  problem remains.

On the other hand

 >as.numeric(format(x, f="%d"))

works perfectly, so I do not understand what is wrong with julian()



Thomas P.



From thpe at hhbio.wasser.tu-dresden.de  Wed Mar 10 00:12:45 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 10 Mar 2004 00:12:45 +0100
Subject: [R] years from as.POSIXlt
In-Reply-To: <Pine.LNX.4.44.0403090738300.6161-100000@gannet.stats>
References: <Pine.LNX.4.44.0403090738300.6161-100000@gannet.stats>
Message-ID: <404E4F6D.1080703@hhbio.wasser.tu-dresden.de>

>as.numeric(format(x, f="%j"))

which is the right code, works perfectly, too.

Thomas P.



From jcjorgensen at wisc.edu  Wed Mar 10 00:13:07 2004
From: jcjorgensen at wisc.edu (Jeff Jorgensen)
Date: Tue, 09 Mar 2004 17:13:07 -0600
Subject: [R] corARMA and ACF in nlme
Message-ID: <5.2.1.1.2.20040309165714.01969288@wiscmail.wisc.edu>

Hi R-sters,

Just wondering what I might be doing wrong.  I'm trying to fit a multiple 
linear regression model, and being ever mindful about the possibilities of 
autocorrelation in the errors (it's a time series), the errors appear to 
follow an AR1 process (ar(ts(glsfit$residuals)) selected order 1).  So, 
when I go back and try to do the simultaneous regression and error fit with 
gls, the acf and pacf plots of residuals from the old model (glsfit) and 
those plots of the new model (glsAR1fit, below)  look exactly the same (a 
significant autocorrelation at lag of 1).

Any ideas out there as to what I may be doing wrong?  Is there an error in 
my code?

Here's my R code for the simultaneous model fit (taking a phi estimate=0.6 
from a previous step <ACF(glsfit)>):

glsAR1fit<-gls(y~x1+x2+x3+x4, na.action = na.omit, subset=12:54,
                 correlation = corARMA(0.6, p=1, q=0, fixed = FALSE))

Thanks much,

Jeff

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Jeff Jorgensen

Center for Limnology
University of Wisconsin Madison           ph (608) 263-2304
680 North Park Street                           fx (608) 265-2340
Madison, Wisconsin 53706                http://limnology.wisc.edu

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From andy_liaw at merck.com  Wed Mar 10 00:27:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Mar 2004 18:27:27 -0500
Subject: [R] Adding data.frames together
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7959@usrymx25.merck.com>

Use rbind() (for row-bind).  Here are some examples:

> d1 <- data.frame(x=1, y=1)
> d2 <- data.frame(x=1:2, y=1:2)
> d3 <- data.frame(x=3, y=3)
> rbind(d1,d2,d3)
   x y
1  1 1
11 1 1
2  2 2
12 3 3
> do.call("rbind", list(d1,d2,d3))
   x y
1  1 1
11 1 1
2  2 2
12 3 3

[or even this:]

> do.call("rbind", eval(parse(text=paste(paste("list(",
paste("d",1:3,sep="",collapse=","), ")")))))
   x y
1  1 1
11 1 1
2  2 2
12 3 3

HTH,
Andy

> From: John Sweval
> 
> I have a series of data frames that are identical structurally, i.e. -
> made with the same code, but I need to add them together so that they
> become one, longer, data frame, i.e. - each of the slot vectors are
> increased in length by the length of the added data frame vectors.
> 
> So if I have df1 with a slot A so that length(df1$A) = 100 and I have
> df2 with a slot A so that length(df2$A)=200 then I need a method to
> create df3 its slot A is the df1$A plus df2$A such that 
> length(df3$A) =
> 300.
> 
> It does not appear that if you use data.frame to join two 
> data frames it
> just adds the slots of both sources to the destination data frame and
> that is not what I want.
> 
> In my finally solution, I need to do this with multiple 
> data.frames that
> slot-wise are identical, but each slot length is different 
> between data
> frames.
> 
> Seems like there should be an easy solution, but I just have not
> stumbled across it in the documentation.
> 
> Thanks,
> John C. Sweval
>   Database Architect
>   Illumigen Biosciences, Inc.
>     Email: jsweval at illumigen.com
>     Phone: 206-378-0400
>     Fax: 206-378-0408
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From tblackw at umich.edu  Wed Mar 10 01:00:06 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Tue, 9 Mar 2004 19:00:06 -0500 (EST)
Subject: [R] Adding data.frames together
In-Reply-To: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
References: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
Message-ID: <Pine.SOL.4.58.0403091824160.18716@millipede.gpcc.itd.umich.edu>

John  -

The function  rbind()  operates on pairs of data frames, and
(somewhat arcane and definitely NOT for beginning users)

   do.call("rbind", list(df1, df2, df3, df4, df5)))

will combine any number (in this case 5) of data frames.

As of February 2003, the  do.call()  approach did not deal
gracefully with columns in a data frame which had been
converted to factors (happens by default in  read.table()).
In my own code I had to go through some gyrations to protect
against that.

For futher information on the  do.call()  approach, search
the r-help archives for "do.call" AND "rbind".  In particular,
there was a thread involving jerosenb and rpeng titled
"[R] quotes within quotes" with one email dated Wed, 9 Apr 2003.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 9 Mar 2004, John Sweval wrote:

> I have a series of data frames that are identical structurally, i.e. -
> made with the same code, but I need to add them together so that they
> become one, longer, data frame, i.e. - each of the slot vectors are
> increased in length by the length of the added data frame vectors.
>
> So if I have df1 with a slot A so that length(df1$A) = 100 and I have
> df2 with a slot A so that length(df2$A)=200 then I need a method to
> create df3 its slot A is the df1$A plus df2$A such that length(df3$A) =
> 300.
>
> It does not appear that if you use data.frame to join two data frames it
> just adds the slots of both sources to the destination data frame and
> that is not what I want.
>
> In my finally solution, I need to do this with multiple data.frames that
> slot-wise are identical, but each slot length is different between data
> frames.
>
> Seems like there should be an easy solution, but I just have not
> stumbled across it in the documentation.
>
> Thanks,
> John C. Sweval
>   Database Architect
>   Illumigen Biosciences, Inc.
>     Email: jsweval at illumigen.com
>     Phone: 206-378-0400
>     Fax: 206-378-0408
>



From tblackw at umich.edu  Wed Mar 10 01:10:07 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Tue, 9 Mar 2004 19:10:07 -0500 (EST)
Subject: [R] Adding data.frames together - correction
In-Reply-To: <Pine.SOL.4.58.0403091824160.18716@millipede.gpcc.itd.umich.edu>
References: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
	<Pine.SOL.4.58.0403091824160.18716@millipede.gpcc.itd.umich.edu>
Message-ID: <Pine.SOL.4.58.0403091906470.9566@tetris.gpcc.itd.umich.edu>


Correction to my reply below:

If my memory serves correctly, one year and many projects later,
I wanted to keep character data as character, not factors,
throughout.  The gyrations were to maintain this despite
using  rbind(),  not because  rbind()  behaved badly with
factors.  Please excuse my foggy memory.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 9 Mar 2004, Tom Blackwell wrote:

> John  -
>
> The function  rbind()  operates on pairs of data frames, and
> (somewhat arcane and definitely NOT for beginning users)
>
>    do.call("rbind", list(df1, df2, df3, df4, df5)))
>
> will combine any number (in this case 5) of data frames.
>
> As of February 2003, the  do.call()  approach did not deal
> gracefully with columns in a data frame which had been
> converted to factors (happens by default in  read.table()).
> In my own code I had to go through some gyrations to protect
> against that.
>
> For futher information on the  do.call()  approach, search
> the r-help archives for "do.call" AND "rbind".  In particular,
> there was a thread involving jerosenb and rpeng titled
> "[R] quotes within quotes" with one email dated Wed, 9 Apr 2003.
>
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
> On Tue, 9 Mar 2004, John Sweval wrote:
>
> > I have a series of data frames that are identical structurally, i.e. -
> > made with the same code, but I need to add them together so that they
> > become one, longer, data frame, i.e. - each of the slot vectors are
> > increased in length by the length of the added data frame vectors.
> >
> > So if I have df1 with a slot A so that length(df1$A) = 100 and I have
> > df2 with a slot A so that length(df2$A)=200 then I need a method to
> > create df3 its slot A is the df1$A plus df2$A such that length(df3$A) =
> > 300.
> >
> > It does not appear that if you use data.frame to join two data frames it
> > just adds the slots of both sources to the destination data frame and
> > that is not what I want.
> >
> > In my finally solution, I need to do this with multiple data.frames that
> > slot-wise are identical, but each slot length is different between data
> > frames.
> >
> > Seems like there should be an easy solution, but I just have not
> > stumbled across it in the documentation.
> >
> > Thanks,
> > John C. Sweval
> >   Database Architect
> >   Illumigen Biosciences, Inc.
> >     Email: jsweval at illumigen.com
> >     Phone: 206-378-0400
> >     Fax: 206-378-0408
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tplate at blackmesacapital.com  Wed Mar 10 01:21:34 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 09 Mar 2004 17:21:34 -0700
Subject: [R] Adding data.frames together
In-Reply-To: <Pine.SOL.4.58.0403091824160.18716@millipede.gpcc.itd.umich .edu>
References: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
	<Pine.SOL.4.58.0403091824160.18716@millipede.gpcc.itd.umich.edu>
Message-ID: <6.0.3.0.2.20040309170722.04995c50@mailhost.blackmesacapital.com>

Actually, rbind() operates on any number of arguments, as stated in the 
documentation ?rbind.  (If it only operated on pairs of arguments, the 
do.call() approach wouldn't get around that anyway.  The do.call() approach 
can be very useful when one has a list of data frames to be rbind'ed 
together, but it's not necessary when the data frames are in separate objects.)

Also, rbind() seems to cope with factor columns now - at least in the 
simple example below.  Are there other examples where it does not cope?

Here are some examples:

 > df1 <- data.frame(a=1:3,b=101:103)
 > df2 <- data.frame(a=4:5,b=104:105)
 > df3 <- data.frame(a=6,b=106)
 > rbind(df1, df2, df3)
    a   b
1  1 101
2  2 102
3  3 103
11 4 104
21 5 105
12 6 106
 > df1 <- data.frame(a=1:3,b=letters[1:3])
 > df2 <- data.frame(a=4:5,b=letters[4:5])
 > df <- rbind(df1, df2)
 > sapply(df, class)
         a         b
"integer"  "factor"
 > df
    a b
1  1 a
2  2 b
3  3 c
11 4 d
21 5 e
 >

-- Tony Plate

At Tuesday 05:00 PM 3/9/2004, Tom Blackwell wrote:
>John  -
>
>The function  rbind()  operates on pairs of data frames, and
>(somewhat arcane and definitely NOT for beginning users)
>
>    do.call("rbind", list(df1, df2, df3, df4, df5)))
>
>will combine any number (in this case 5) of data frames.
>
>As of February 2003, the  do.call()  approach did not deal
>gracefully with columns in a data frame which had been
>converted to factors (happens by default in  read.table()).
>In my own code I had to go through some gyrations to protect
>against that.
>
>For futher information on the  do.call()  approach, search
>the r-help archives for "do.call" AND "rbind".  In particular,
>there was a thread involving jerosenb and rpeng titled
>"[R] quotes within quotes" with one email dated Wed, 9 Apr 2003.
>
>-  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
>On Tue, 9 Mar 2004, John Sweval wrote:
>
> > I have a series of data frames that are identical structurally, i.e. -
> > made with the same code, but I need to add them together so that they
> > become one, longer, data frame, i.e. - each of the slot vectors are
> > increased in length by the length of the added data frame vectors.
> >
> > So if I have df1 with a slot A so that length(df1$A) = 100 and I have
> > df2 with a slot A so that length(df2$A)=200 then I need a method to
> > create df3 its slot A is the df1$A plus df2$A such that length(df3$A) =
> > 300.
> >
> > It does not appear that if you use data.frame to join two data frames it
> > just adds the slots of both sources to the destination data frame and
> > that is not what I want.
> >
> > In my finally solution, I need to do this with multiple data.frames that
> > slot-wise are identical, but each slot length is different between data
> > frames.
> >
> > Seems like there should be an easy solution, but I just have not
> > stumbled across it in the documentation.
> >
> > Thanks,
> > John C. Sweval
> >   Database Architect
> >   Illumigen Biosciences, Inc.
> >     Email: jsweval at illumigen.com
> >     Phone: 206-378-0400
> >     Fax: 206-378-0408
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vcs at stat.stanford.edu  Wed Mar 10 01:33:44 2004
From: vcs at stat.stanford.edu (Victoria Stodden)
Date: Tue, 9 Mar 2004 16:33:44 -0800
Subject: [R] accuracy of chi-square distribution approximations
Message-ID: <Pine.SGI.4.44.0403091517350.33252444-100000@rgmiller.Stanford.EDU>


Hi there,

How accurate is the aproximation R makes to the Chi-Square distribution?
For example, if I run:

> qchisq(1/1000000,6)
[1] 0.03650857

how accurate is 0.0365 compared to the theoretical percentile? What kind
of approximations have been made in the software's algorithm? It woudl be
useful to know since I am working with tiny percentiles such as one
one-millionth and one one-billionth, and I am not sure how R comes up
with the percentile numbers.

Thanks for any help,
-Victoria



From andy_liaw at merck.com  Wed Mar 10 01:45:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Mar 2004 19:45:36 -0500
Subject: [R] accuracy of chi-square distribution approximations
Message-ID: <3A822319EB35174CA3714066D590DCD504AF795A@usrymx25.merck.com>

You have full access to the R source code, so that would be the place to
look.

In R-1.8.1, in the file .../src/nmath/qchisq.c, the function just calls
qgamma() (of course!).  In qgamma.c, it says:

 *  DESCRIPTION
 *
 *      Compute the quantile function of the gamma distribution.
 *
 *  NOTES
 *
 *      This function is based on the Applied Statistics
 *      Algorithm AS 91 ("ppchi2") and via pgamma(.) AS 239.
 *
 *  REFERENCES
 *
 *      Best, D. J. and D. E. Roberts (1975).
 *      Percentage Points of the Chi-Squared Distribution.
 *      Applied Statistics 24, page 385.  */

so you should consult those references.

Andy

> From: Victoria Stodden
> 
> Hi there,
> 
> How accurate is the aproximation R makes to the Chi-Square 
> distribution?
> For example, if I run:
> 
> > qchisq(1/1000000,6)
> [1] 0.03650857
> 
> how accurate is 0.0365 compared to the theoretical 
> percentile? What kind
> of approximations have been made in the software's algorithm? 
> It woudl be
> useful to know since I am working with tiny percentiles such as one
> one-millionth and one one-billionth, and I am not sure how R comes up
> with the percentile numbers.
> 
> Thanks for any help,
> -Victoria
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From yschen at jhu.edu  Wed Mar 10 01:46:34 2004
From: yschen at jhu.edu (YIHSU CHEN)
Date: Tue, 09 Mar 2004 19:46:34 -0500
Subject: [R] How to use MLE-class?
Message-ID: <000801c40639$23551ea0$cb1bdc80@yschen>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040309/dbef6760/attachment.pl

From davidD at qimr.edu.au  Wed Mar 10 01:56:32 2004
From: davidD at qimr.edu.au (David Duffy)
Date: Wed, 10 Mar 2004 10:56:32 +1000 (EST)
Subject: [R] Re: R-help Digest, Vol 13, Issue 9
In-Reply-To: <200403091131.i29BL0Td000655@hypatia.math.ethz.ch>
References: <200403091131.i29BL0Td000655@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0403101048200.10284@orpheus.qimr.edu.au>

C?dric Finet wrote:
>
> I thank you for your answer but I do not understand yet why the Fisher?s exact
> test does not work. And why is a "negative key".
>
> C?dric Finet
>

Running the original TOMS643 fortran code (R uses an f2c translation of
this) says:

 FEXACT ERROR:  30
 Stack length exceeded in f3xact.  This problem should not occur.

The integer hash key is bigger than the largest allowable integer,
and so appears as a negative number.  Table is too big.


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From andy_liaw at merck.com  Wed Mar 10 02:35:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Mar 2004 20:35:54 -0500
Subject: [R] How to use MLE-class?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF795B@usrymx25.merck.com>

Do you mean mle() in the package `mle'?  If so, see ?"mle-class" for a
description.

Andy

> From: YIHSU CHEN
> 
> Hi there,
>  
> I had successfully use "MLE" function to solve my problem.  Is there
> anyone knows how to get related information? i.e., value of likelihood
> function, information matrix, and etc. I know MLE-class can 
> do it but I
> can not find any information tells me how to do it.
>  
> Thanks a billions,
>  
> Yihsu 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From kjetil at entelnet.bo  Wed Mar 10 02:43:09 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 09 Mar 2004 21:43:09 -0400
Subject: [R] Adding data.frames together
In-Reply-To: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
Message-ID: <404E3A6D.26623.134EDA1@localhost>

On 9 Mar 2004 at 14:41, John Sweval wrote:

If you have the dataframes as components of a list dfs, maybe you can 
do something like

do.call("cbind", dfs)

Kjetil Halvorsen

> I have a series of data frames that are identical structurally, i.e. -
> made with the same code, but I need to add them together so that they
> become one, longer, data frame, i.e. - each of the slot vectors are
> increased in length by the length of the added data frame vectors.
> 
> So if I have df1 with a slot A so that length(df1$A) = 100 and I have
> df2 with a slot A so that length(df2$A)=200 then I need a method to
> create df3 its slot A is the df1$A plus df2$A such that length(df3$A)
> = 300.
> 
> It does not appear that if you use data.frame to join two data frames
> it just adds the slots of both sources to the destination data frame
> and that is not what I want.
> 
> In my finally solution, I need to do this with multiple data.frames
> that slot-wise are identical, but each slot length is different
> between data frames.
> 
> Seems like there should be an easy solution, but I just have not
> stumbled across it in the documentation.
> 
> Thanks,
> John C. Sweval
>   Database Architect
>   Illumigen Biosciences, Inc.
>     Email: jsweval at illumigen.com
>     Phone: 206-378-0400
>     Fax: 206-378-0408
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Mar 10 02:54:07 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 09 Mar 2004 17:54:07 -0800
Subject: [R] How to use MLE-class?
In-Reply-To: <000801c40639$23551ea0$cb1bdc80@yschen>
References: <000801c40639$23551ea0$cb1bdc80@yschen>
Message-ID: <404E753F.8@pdf.com>

      Have you read Pinheiro and Bates (2000) Mixed-Effects Models in S 
and S-Plus (Springer)?  I learned much from that book.  Also, 
help.search("lme") gave interesting results. 

      hope this helps.  spencer graves

YIHSU CHEN wrote:

>Hi there,
> 
>I had successfully use "MLE" function to solve my problem.  Is there
>anyone knows how to get related information? i.e., value of likelihood
>function, information matrix, and etc. I know MLE-class can do it but I
>can not find any information tells me how to do it.
> 
>Thanks a billions,
> 
>Yihsu 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Wed Mar 10 02:57:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 09 Mar 2004 17:57:27 -0800
Subject: [R] How to use MLE-class?
In-Reply-To: <000801c40639$23551ea0$cb1bdc80@yschen>
References: <000801c40639$23551ea0$cb1bdc80@yschen>
Message-ID: <404E7607.9040502@pdf.com>

	  Please ignore my earlier reply to this post:  I read it as "lme", not 
"mle".  spencer graves
################
      Have you read Pinheiro and Bates (2000) Mixed-Effects Models in S
and S-Plus (Springer)?  I learned much from that book.  Also,
help.search("lme") gave interesting results.

      hope this helps.  spencer graves

YIHSU CHEN wrote:

>Hi there,
> 
>I had successfully use "MLE" function to solve my problem.  Is there
>anyone knows how to get related information? i.e., value of likelihood
>function, information matrix, and etc. I know MLE-class can do it but I
>can not find any information tells me how to do it.
> 
>Thanks a billions,
> 
>Yihsu 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From service at paypal.com  Wed Mar 10 03:04:12 2004
From: service at paypal.com (PayPal Customer Service 1)
Date: Tue, 09 Mar 2004 20:04:12 -0600
Subject: [R] AutoResponse - Email Returned SAXK  (KMM44933619V84198L0KM)
Message-ID: <20040310015429.AAD2D408B1@smtp1.nix.paypal.com>



Thank you for contacting PayPal Customer Service.  

In an effort to assist you as quickly and efficiently as possible, please 
direct all customer service inquires through our website. Click on the 
hyperlink below to go to the PayPal website. After entering your email 
address and password into the Member Log In box, you can submit your 
inquiry via our Customer Service Contact form. If you indicate the type of 
question you have with as much detail as you can, we will be able to 
provide you with the best customer service possible.

If your email program is unable to open hyperlinks, please copy and paste 
this URL into the address bar of your browser.

https://www.paypal.com/wf/f=default	

If you are contacting PayPal because you are unable to log into your 
account, please use the contact form below.

https://www.paypal.com/ewf/f=default

Thank you for choosing PayPal!

------------------------------------------------------------------------
Note: When you click on links in this email, you will be asked to log into 
your PayPal Account. As always, make sure that you are logging into a 
secure PayPal page by looking for 'https://www.paypal.com/' at the 
beginning of the URL.

Please do not reply to this e-mail.  Mail sent to this address will not be 
answered.

********************************************
Original Email:
Please read the attached file.



From Benjamin.STABLER at odot.state.or.us  Wed Mar 10 03:46:27 2004
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Tue, 9 Mar 2004 18:46:27 -0800 
Subject: [R] Rcmd BATCH command line arguments 
Message-ID: <76A000A82289D411952F001083F9DD06047FE50A@exsalem4-bu.odot.state.or.us>

I want to run Rcmd BATCH with R_DEFAULT_PACKAGE=base so it doesn't load any
packages, but it seems to reject this argument because it does not start
with a '-' or '--'.  Is there a different argument that will work?  Thanks.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From ggrothendieck at myway.com  Wed Mar 10 04:57:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  9 Mar 2004 22:57:43 -0500 (EST)
Subject: [R] years from as.POSIXlt
Message-ID: <20040310035743.A4562399F@mprdmxin.myway.com>



If you search the mail archives for fromchar there are a number
of discussions of similar bugs that all seem to come from 
German or other European users, suggesting locale problems.

I suspect it won't be easy for others to reproduce this
so you might try to see what you can track down yourself.
fromchar is in as.POSIXlt so perhaps you could
try to get the bug to appear by using as.POSIXlt 
directly and then debugging it using 

    debug(as.POSIXlt)

and as soon as it defines fromchar

    debug(fromchar)

In terms of an interim solution, you could use chron.
For example, for the .leap.seconds problem:

   require(chron)
   .leap.seconds.chron <- chron( format(.leap.seconds,"%m/%d/%Y"), 
                                 format(.leap.seconds,"%H:%M:%S") )
   as.numeric(.leap.seconds.chron) 

where the last line uses the fact that chron stores its dates 
internally as the number of days since January 1, 1970.

---
Date:   Wed, 10 Mar 2004 00:04:30 +0100 
From:   Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de>
To:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] years from as.POSIXlt 

 
Prof Brian Ripley wrote:

> See ?julian, which says 
> 
> Note:
> 
> Other components such as the day of the month or the year are very
> easy to computes: just use 'as.POSIXlt' and extract the relevant
> component.
> 

Hello,

unfortunately not all mentioned functions work on all machines. Where

> months(.leap.seconds)

works on all systems, a call to

> julian(leap.seconds)

workes perfectly only on Linux (R 1.8.0) but failed on two different XP 
Machines (German XP version):

Error in fromchar(x) : character string is not in a standard unambiguous 
format

I've tested several things, several versions of msvcrt.dll, different 
PATH settings, locale set to German or USA, R 1.8.1 and R 1.7.1, but the 
problem remains.

On the other hand

>as.numeric(format(x, f="%d"))

works perfectly, so I do not understand what is wrong with julian()



Thomas P.



From ripley at stats.ox.ac.uk  Wed Mar 10 08:13:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 07:13:46 +0000 (GMT)
Subject: [R] years from as.POSIXlt
In-Reply-To: <404E4F6D.1080703@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0403100710310.16941-100000@gannet.stats>

On Wed, 10 Mar 2004, Thomas Petzoldt wrote:

> >as.numeric(format(x, f="%j"))
> 
> which is the right code, works perfectly, too.

but the recommended procedure is on the help page ?julian, and of course 
works perfectly.  Just use 1+x$yday if you want 1-based day of the year.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Christine.Tuleau at math.u-psud.fr  Tue Mar  9 16:23:16 2004
From: Christine.Tuleau at math.u-psud.fr (Christine Tuleau)
Date: Tue, 9 Mar 2004 15:23:16 +0000 (WET)
Subject: [R] help
Message-ID: <200403091523.i29FNF1P005800@diamant.math.u-psud.fr>


Hello,

I am a new member, and I need your help.
For my work (thesis), I use the package rpart to construct trees.
But, to continu my studies, I need to calcule the 'variable importance'. But, I 
don't find a program to do this in the implementation of R.
Is there someone who know if there exist a program which calculate the 'variable 
importance', notion defined by Breiman and al. in CART ?
If this program exists, could you tell me where I can find it, to use it?

Thanks a lot, and sorry for my english.

Christine Tuleau



From ripley at stats.ox.ac.uk  Wed Mar 10 08:21:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 07:21:49 +0000 (GMT)
Subject: [R] years from as.POSIXlt
In-Reply-To: <404E4D7E.5040105@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0403100715240.16941-100000@gannet.stats>

On Wed, 10 Mar 2004, Thomas Petzoldt wrote:

(without copying me)

> Prof Brian Ripley wrote:
> 
> > See ?julian, which says 
> > 
> > Note:
> > 
> >      Other components such as the day of the month or the year are very
> >      easy to computes: just use 'as.POSIXlt' and extract the relevant
> >      component.
> > 
> 
> Hello,
> 
> unfortunately not all mentioned functions work on all machines. Where

No function is mentioned in that note!

>  > months(.leap.seconds)
> 
> works on all systems, a call to
> 
>  > julian(leap.seconds)

What is `leap.seconds'?

> julian(.leap.seconds)

works on Windows XP, of course.

> workes perfectly only on Linux (R 1.8.0) but failed on two different XP 
> Machines (German XP version):
> 
> Error in fromchar(x) : character string is not in a standard unambiguous 
> format

What is `leap.seconds' on your machine?  It is an object that as.POSIXct 
is failing on, I suspect (try traceback() to be sure).

> I've tested several things, several versions of msvcrt.dll, different 
> PATH settings, locale set to German or USA, R 1.8.1 and R 1.7.1, but the 
>   problem remains.
> 
> On the other hand
> 
>  >as.numeric(format(x, f="%d"))
> 
> works perfectly, so I do not understand what is wrong with julian()

Do *read* the advice on how to get yday quoted above.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Mar 10 08:48:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 07:48:34 +0000 (GMT)
Subject: [R] Rcmd BATCH command line arguments 
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE50A@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0403100739450.16941-100000@gannet.stats>

On Tue, 9 Mar 2004 Benjamin.STABLER at odot.state.or.us wrote:

> I want to run Rcmd BATCH with R_DEFAULT_PACKAGE=base so it doesn't load any
> packages, but it seems to reject this argument because it does not start
> with a '-' or '--'.  Is there a different argument that will work?  Thanks.

It is not a command-line argument!  In Rterm.exe you can set environment
variables this way, but not in Rcmd.exe.  So set the environment variable
R_DEFAULT_PACKAGE>S< to NULL, not base which is always loaded.  See 
?Startup for the documentation (which is correct) and the rw-FAQ for how 
to set environment variables.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Joris.DeWolf at cropdesign.com  Wed Mar 10 08:58:00 2004
From: Joris.DeWolf at cropdesign.com (Joris DeWolf)
Date: Wed, 10 Mar 2004 08:58:00 +0100
Subject: [R] Adding data.frames together
In-Reply-To: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
References: <E4E4C6B0D39DAC4CAA15D09B594DD98C188BAE@exon.illumigen.com>
Message-ID: <404ECA88.7050704@cropdesign.com>


check rbind
Joris

John Sweval wrote:

>I have a series of data frames that are identical structurally, i.e. -
>made with the same code, but I need to add them together so that they
>become one, longer, data frame, i.e. - each of the slot vectors are
>increased in length by the length of the added data frame vectors.
>
>So if I have df1 with a slot A so that length(df1$A) = 100 and I have
>df2 with a slot A so that length(df2$A)=200 then I need a method to
>create df3 its slot A is the df1$A plus df2$A such that length(df3$A) =
>300.
>
>It does not appear that if you use data.frame to join two data frames it
>just adds the slots of both sources to the destination data frame and
>that is not what I want.
>
>In my finally solution, I need to do this with multiple data.frames that
>slot-wise are identical, but each slot length is different between data
>frames.
>
>Seems like there should be an easy solution, but I just have not
>stumbled across it in the documentation.
>
>Thanks,
>John C. Sweval
>  Database Architect
>  Illumigen Biosciences, Inc.
>    Email: jsweval at illumigen.com
>    Phone: 206-378-0400
>    Fax: 206-378-0408
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 

====================================================================== 
Joris De Wolf
CropDesign N.V. 
Plant Evaluation Group
Technologiepark 3 
B-9052 Zwijnaarde 
Belgium 
Tel. : +32 9 242 91 55
Fax  : +32 9 241 91 73
====================================================================== 


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From asemeria at cramont.it  Wed Mar 10 09:12:19 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Wed, 10 Mar 2004 09:12:19 +0100
Subject: [R] read.table() question
Message-ID: <OF2942264C.0E9095F0-ONC1256E53.002D1262@tomware.it>





I don't know "read.data" function, it is
a function that you have defined?
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Wed Mar 10 09:27:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 08:27:03 +0000 (GMT)
Subject: [R] corARMA and ACF in nlme
In-Reply-To: <5.2.1.1.2.20040309165714.01969288@wiscmail.wisc.edu>
Message-ID: <Pine.LNX.4.44.0403100820050.17082-100000@gannet.stats>

On Tue, 9 Mar 2004, Jeff Jorgensen wrote:

> Just wondering what I might be doing wrong.  I'm trying to fit a multiple 
> linear regression model, and being ever mindful about the possibilities of 
> autocorrelation in the errors (it's a time series), the errors appear to 
> follow an AR1 process (ar(ts(glsfit$residuals)) selected order 1).  So, 
> when I go back and try to do the simultaneous regression and error fit with 
> gls, 

That's not really what you did: gls fits a multivariate normal 
distribution with covariance matrix like that of observations from an AR1 
rather than fit an AR model.  A fine distinction, but as the rest of the 
para shows, it does matter.

> the acf and pacf plots of residuals from the old model (glsfit) and 
> those plots of the new model (glsAR1fit, below)  look exactly the same (a 
> significant autocorrelation at lag of 1).

They should.  Those are the residuals from the regression, not the 
innovations of the autoregression fitted to the residuals.  So the problem 
lies in how you interpreted what you did, I believe.

You may find this clearer if you use arima which does fit an ARIMA model.

> Any ideas out there as to what I may be doing wrong?  Is there an error in 
> my code?
> 
> Here's my R code for the simultaneous model fit (taking a phi estimate=0.6 
> from a previous step <ACF(glsfit)>):
> 
> glsAR1fit<-gls(y~x1+x2+x3+x4, na.action = na.omit, subset=12:54,
>                  correlation = corARMA(0.6, p=1, q=0, fixed = FALSE))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Mar 10 09:37:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Mar 2004 09:37:14 +0100
Subject: [R] corARMA and ACF in nlme
In-Reply-To: <5.2.1.1.2.20040309165714.01969288@wiscmail.wisc.edu>
References: <5.2.1.1.2.20040309165714.01969288@wiscmail.wisc.edu>
Message-ID: <x27jxtw11h.fsf@biostat.ku.dk>

Jeff Jorgensen <jcjorgensen at wisc.edu> writes:

> Hi R-sters,
> 
> Just wondering what I might be doing wrong.  I'm trying to fit a
> multiple linear regression model, and being ever mindful about the
> possibilities of autocorrelation in the errors (it's a time series),
> the errors appear to follow an AR1 process (ar(ts(glsfit$residuals))
> selected order 1).  So, when I go back and try to do the simultaneous
> regression and error fit with gls, the acf and pacf plots of residuals
> from the old model (glsfit) and those plots of the new model
> (glsAR1fit, below)  look exactly the same (a significant
> autocorrelation at lag of 1).
> 
> Any ideas out there as to what I may be doing wrong?  Is there an
> error in my code?

This is one of the dangers of accessing model fit structures directly:
There is more than one way to define residuals for correlated data. Try
looking at help(residuals.gls).

(It's been a while, but as far as I remember, you can plot the
Variogram of the raw residuals and overlay the theoretical Variogram
of the fitted model, so raw residuals are useful too. Also,
standardized residuals are not uniquely defined since matrix square
roots aren't.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From martinol at ensam.inra.fr  Wed Mar 10 09:40:25 2004
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Wed, 10 Mar 2004 09:40:25 +0100
Subject: [R] Writing Rd files dor package
Message-ID: <404ED479.50304@ensam.inra.fr>

Hi all,

I have some problems to write help files for a package. I don't 
understant how to include help
files in a package that I develop.

I have written the Rd files that I have put in a man directory. So after 
this, I built the package with the command
R CMD build (for linux) and I obtained my package (I have also used the 
commands  R CMD Rdconv -t txt and
R CMD Rdconv -t html to convert my Rd files in txt and html files)

I succesfully install the package with R INSTALL and load the library 
with the command library()
in a R session. But there is  no available help for the functions, enven 
if in the /path/R/library/mypacakge/ directory, there are the 
directories html,  man and help.

I suppose that I have made some mistakes. I read "Writing R Extensions" 
, but  I do not find my mistakes.
Could you help me.

Best regards,
Olivier



From m.mader at gsf.de  Wed Mar 10 09:56:19 2004
From: m.mader at gsf.de (Michael Mader)
Date: Wed, 10 Mar 2004 09:56:19 +0100
Subject: [R] Writing Rd files dor package
References: <404ED479.50304@ensam.inra.fr>
Message-ID: <404ED833.7E24C7FD@gsf.de>

In general, it's advantageous to do a check before the build (R CMD
check mypackage on Linux/UNIX).
R CMD check extensively checks the Rd-formatting and might give you a
hint on what's wrong with your Rd-files. 

However, it's quite hard to diagnose problems without having the
problem-causing code/Rd files. Can you reduce one of your Rd-files to a
minimum-length example?

Regards

Michael

Martin Olivier wrote:
> 
> Hi all,
> 
> I have some problems to write help files for a package. I don't
> understant how to include help
> files in a package that I develop.
> 
> I have written the Rd files that I have put in a man directory. So after
> this, I built the package with the command
> R CMD build (for linux) and I obtained my package (I have also used the
> commands  R CMD Rdconv -t txt and
> R CMD Rdconv -t html to convert my Rd files in txt and html files)
> 
> I succesfully install the package with R INSTALL and load the library
> with the command library()
> in a R session. But there is  no available help for the functions, enven
> if in the /path/R/library/mypacakge/ directory, there are the
> directories html,  man and help.
> 
> I suppose that I have made some mistakes. I read "Writing R Extensions"
> , but  I do not find my mistakes.
> Could you help me.
> 
> Best regards,
> Olivier
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Michael T. Mader
Institute for Bioinformatics/MIPS, GSF
Ingolstaedter Landstrasse 1
D-85764 Neuherberg
0049-89-3187-3576

response time (n.) An unbounded, random variable Tr associated with a
given TIMESHARING system and representing the putative time which
elapses between Ts, the time of sending a message, and Te, the time when
the resulting error diagnostic is received.	
	S. Kelly-Bootle, The Devil's DP Dictionary



From cdeclercq at nordnet.fr  Wed Mar 10 10:37:13 2004
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Wed, 10 Mar 2004 10:37:13 +0100
Subject: [R] years from as.POSIXlt
In-Reply-To: <Pine.LNX.4.44.0403100715240.16941-100000@gannet.stats>
Message-ID: <MJELLLFFFCNHMHOOLCMBEEADCEAA.cdeclercq@nordnet.fr>

As I understand it, there is perhaps something bad with "1970-01-01 00:00",
not in R, but in (at least some versions of) MS-Windows (e.g.
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26357.html).


> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R

> Sys.info()[c(1:3,5)]
                      sysname                       release
                    "Windows"                      "NT 5.1"
                      version                       machine
"(build 2600) Service Pack 1"                         "x86"

> Sys.getlocale("LC_TIME")
[1] "French_France.1252"

> as.POSIXct("1970-01-01 01:00:00")
[1] "1970-01-01 01:00:00 Paris, Madrid"
> as.POSIXct("1970-01-01 00:00:00")
Error in fromchar(x) : character string is not in a standard unambiguous
format
> julian(.leap.seconds)
Error in fromchar(x) : character string is not in a standard unambiguous
format

but, of course, 'julian' is OK with another origin

> julian(.leap.seconds, origin=ISOdate(1900,1,1))
Time differences of 26478.5, 26662.5, 27027.5, 27392 [truncated]

Christophe
--
Christophe Declercq, MD
Observatoire regional de la sante Nord-Pas-de-Calais
13, rue Faidherbe
F-59046 LILLE Cedex
Phone 33 3 20 15 49 24
Fax 33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org




> -----Message d'origine-----
> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Prof Brian Ripley
> Envoye : mercredi 10 mars 2004 08:22
> A : Thomas Petzoldt
> Cc : r-help at stat.math.ethz.ch
> Objet : Re: [R] years from as.POSIXlt
>
>
> On Wed, 10 Mar 2004, Thomas Petzoldt wrote:
>
> (without copying me)
>
> > Prof Brian Ripley wrote:
> >
> > > See ?julian, which says
> > >
> > > Note:
> > >
> > >      Other components such as the day of the month or the
> year are very
> > >      easy to computes: just use 'as.POSIXlt' and extract the relevant
> > >      component.
> > >
> >
> > Hello,
> >
> > unfortunately not all mentioned functions work on all machines. Where
>
> No function is mentioned in that note!
>
> >  > months(.leap.seconds)
> >
> > works on all systems, a call to
> >
> >  > julian(leap.seconds)
>
> What is `leap.seconds'?
>
> > julian(.leap.seconds)
>
> works on Windows XP, of course.
>
> > workes perfectly only on Linux (R 1.8.0) but failed on two different XP
> > Machines (German XP version):
> >
> > Error in fromchar(x) : character string is not in a standard
> unambiguous
> > format
>
> What is `leap.seconds' on your machine?  It is an object that as.POSIXct
> is failing on, I suspect (try traceback() to be sure).
>
> > I've tested several things, several versions of msvcrt.dll, different
> > PATH settings, locale set to German or USA, R 1.8.1 and R
> 1.7.1, but the
> >   problem remains.
> >
> > On the other hand
> >
> >  >as.numeric(format(x, f="%d"))
> >
> > works perfectly, so I do not understand what is wrong with julian()
>
> Do *read* the advice on how to get yday quoted above.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Wed Mar 10 11:12:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 10:12:02 +0000 (GMT)
Subject: [R] julian.default on Windows (was years from as.POSIXlt)
In-Reply-To: <MJELLLFFFCNHMHOOLCMBEEADCEAA.cdeclercq@nordnet.fr>
Message-ID: <Pine.LNX.4.44.0403100951280.17273-100000@gannet.stats>

On Wed, 10 Mar 2004, Christophe Declercq wrote:

> As I understand it, there is perhaps something bad with "1970-01-01 00:00",
> not in R, but in (at least some versions of) MS-Windows (e.g.
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26357.html).

Ok, I set my machine to CET and I get the problem.  It is indeed a
Microsoft bug: it seems its mktime can only regards as valid times after
1970-01-01 00:00:00 GMT (and not after that time in the current timezone).  
Hence R's strptime has trouble in the period of 1970 when CMT was in 1969.  
That's pretty esoteric!

Fixed now in R-devel.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thpe at hhbio.wasser.tu-dresden.de  Wed Mar 10 12:35:06 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 10 Mar 2004 12:35:06 +0100
Subject: [R] julian.default on Windows (was years from as.POSIXlt)
In-Reply-To: <Pine.LNX.4.44.0403100951280.17273-100000@gannet.stats>
References: <Pine.LNX.4.44.0403100951280.17273-100000@gannet.stats>
Message-ID: <404EFD6A.90209@hhbio.wasser.tu-dresden.de>

Prof Brian Ripley wrote:

> 
> Ok, I set my machine to CET and I get the problem.  It is indeed a
> Microsoft bug: it seems its mktime can only regards as valid times after
> 1970-01-01 00:00:00 GMT (and not after that time in the current timezone).  
> Hence R's strptime has trouble in the period of 1970 when CMT was in 1969.  
> That's pretty esoteric!

Really!

> Fixed now in R-devel.

Thank you very much. It was my intention to point out that there was 
really a problem with some platforms, not that I have this problem. As 
we know and as was discussed earlier there are also several workarounds.

Thomas P.



From spencer.graves at pdf.com  Wed Mar 10 12:49:53 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 10 Mar 2004 03:49:53 -0800
Subject: [R] Map of British Colonial America 1775
Message-ID: <404F00E1.7070002@pdf.com>

      Does anyone have suggestions about how to produce a map of British 
Colonial America 1775?  At that time, Great Britain had 26 colonies in 
the Americas, including Bermuda, several Caribbean islands, "Quebec" 
(extending then almost to New Orleans), Nova Scotia, Newfoundland, 
Belize, and the 13 that declared independence in 1776. 

      I've reviewed the "map" documentation I've found so far with 
S-Plus 6.2 and R including "www.r-project.org" -> search -> "R site 
search" -> "world map" and Becker & Wilkes (1993) "Maps in S" cited in 
the "map" documentation.  I've noticed that, for example, 
'library(maps);  map("world", "UK:Bermuda")' produces an 83-point 
polygon sketch in S-Plus 6.2 but only a degenerate 3-point polygon (with 
the third point = the first) in R 1.8.1. 

      Thanks, Spencer Graves



From rksh at soc.soton.ac.uk  Wed Mar 10 13:29:09 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 10 Mar 2004 12:29:09 +0000
Subject: [R] aperm() and as.list() args to "["
Message-ID: <a06002005bc74b97fe154@[139.166.242.29]>


Hi everyone.

I'm playing with aperm():

a <- 1:24
dim(a) <- c(2,3,2,2)
permutation <-  c(1,2,4,3)
b <- aperm(a,permutation)


So if my understanding is right,

a[1,3,2,1] ==  b[c(1,3,2,1)[permutation] ]

but this isn't what I want because the RHS evaluates to a vector, and
I am trying to identify a single element of b.

How do I modify the RHS to give what I want?


Following aren't right  either:
b[as.vector(c(1,3,2,1)[permutation]) ]
b[as.list(c(1,3,2,1)[permutation]) ]


OBattempt:
eval(parse(text=paste("b[",paste(c(1,3,2,1)[permutation],collapse=","),"]")))

which DOES work, but is ghastly!

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From spencer.graves at pdf.com  Wed Mar 10 13:48:44 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 10 Mar 2004 04:48:44 -0800
Subject: [R] aperm() and as.list() args to "["
In-Reply-To: <a06002005bc74b97fe154@[139.166.242.29]>
References: <a06002005bc74b97fe154@[139.166.242.29]>
Message-ID: <404F0EAC.7050509@pdf.com>

    How about: 

 > a <- 1:24
 > dim(a) <- c(2,3,2,2)
 > permutation <-  c(1,2,4,3)
 > b <- aperm(a,permutation)
 > a[1,3,2,1]
[1] 11
 >
 > Expr <- paste("b[", paste(c(1,3,2,1)[permutation], collapse=","), "]")
 > eval(parse(text=Expr))
[1] 11

      Ugly, I think, but effective. 
      hope this helps.  spencer graves

Robin Hankin wrote:

>
> Hi everyone.
>
> I'm playing with aperm():
>
> a <- 1:24
> dim(a) <- c(2,3,2,2)
> permutation <-  c(1,2,4,3)
> b <- aperm(a,permutation)
>
>
> So if my understanding is right,
>
> a[1,3,2,1] ==  b[c(1,3,2,1)[permutation] ]
>
> but this isn't what I want because the RHS evaluates to a vector, and
> I am trying to identify a single element of b.
>
> How do I modify the RHS to give what I want?
>
>
> Following aren't right  either:
> b[as.vector(c(1,3,2,1)[permutation]) ]
> b[as.list(c(1,3,2,1)[permutation]) ]
>
>
> OBattempt:
> eval(parse(text=paste("b[",paste(c(1,3,2,1)[permutation],collapse=","),"]"))) 
>
>
> which DOES work, but is ghastly!
>



From angel_lul at hotmail.com  Wed Mar 10 14:50:38 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 10 Mar 2004 13:50:38 +0000
Subject: [R] avoid step appearance in plot lines
Message-ID: <404F1D2E.5070104@hotmail.com>

How can I improve the aspect of the lines in a line plot?
I want to avoid that between two points the line joining them is breaked 
into "steps".
For example:
plot(1:100,sin(1:100))
lines(1:100,sin(1:100),type='l',lwd=2)
I get that the line joining two points is made up of vertical segments, 
what I basically want is that there are more segments between two points 
so I get the aspect of a true line.
Thanks,
Angel



From ripley at stats.ox.ac.uk  Wed Mar 10 13:50:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 12:50:55 +0000 (GMT)
Subject: [R] aperm() and as.list() args to "["
In-Reply-To: <a06002005bc74b97fe154@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0403101244480.17793-100000@gannet.stats>

On Wed, 10 Mar 2004, Robin Hankin wrote:

> 
> Hi everyone.
> 
> I'm playing with aperm():
> 
> a <- 1:24
> dim(a) <- c(2,3,2,2)
> permutation <-  c(1,2,4,3)
> b <- aperm(a,permutation)
> 
> 
> So if my understanding is right,
> 
> a[1,3,2,1] ==  b[c(1,3,2,1)[permutation] ]
> 
> but this isn't what I want because the RHS evaluates to a vector, and
> I am trying to identify a single element of b.
> 
> How do I modify the RHS to give what I want?

ind <- as.list(c(1,3,2,1)[permutation])
do.call("[", c(list(b), ind))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From busscher at uni-kassel.de  Wed Mar 10 14:13:50 2004
From: busscher at uni-kassel.de (Nicolaas Busscher)
Date: Wed, 10 Mar 2004 14:13:50 +0100
Subject: [R] gauge R&R and repeated measurements with R
In-Reply-To: <404B6386.3080604@lscp.ehess.fr>
References: <400E7369.5060405@lscp.ehess.fr>
	<20040305205202.4b57d141.busscher@uni-kassel.de>
	<404B6386.3080604@lscp.ehess.fr>
Message-ID: <20040310141350.48203044.busscher@uni-kassel.de>

Dear List Members,
Chistophe Pallier was so kind to help getting my question more clear,
so:

I have the following problem , which is a so called gauge R&R
(repeatebility & reproducibility) question. To get things clear (also
for myself) i draw a small graph, which i attached. 

To describe it short: we have grain samples grown under different
conditons which we process all in the same way . for this processed
grain we have a measurement procedure from which we get a value. we do
this measurement process 6 time (repeated measurements) foreach
processed sample . the process is done 6 time for each sample, we have
two different samples. the data plot looks like: sample proces values
1       1       8.0 7.8 9.0 6,5 5.5 8.9
1       2       ...
...
2       7      ...
and so on

sample is a fixed factor (we alllways use the sample from the same
bag), while process and value are random factors.

the question we have is:
1. how significant can we see the difference between sample 1 and 2,
by generating as much variation as possible, by doing 6 times the
process and each processed sample "measuring" 6 times as far as i
understand it this means: aov(value~sample+Error(proces)) Is this ok
for this kombination of fixed and random factors? from the data types
in R value is a float number, while sample and process are factors. is
that ok?

i have to generate the process information from different data,
because we combine the data from different days. as far as i
understand the Error() function, it reduces the influence of repeated
measurements on the degree of freedom , so the significance is not so
high as without.

if we would expect that there would be a time influence in the process
data (f.e. a degradation of the samples), how could we check this in
terms of this formula?

2. for our development of the complete process : is the variation from
process bigger or from values? do we get this from
aov(value~sample/process)?

the following i get out of my data, process is gathering 4 groups of
repeated measurements, (for a start i take the date of the day of the
experiment), we did the process more than one time a day.> 
>     print(summary(aov(value~sample+Error(process))))

Error: process
          Df Sum Sq Mean Sq F value  Pr(>F)  
sample     1  36726   36726  6.1457 0.04787 *
Residuals  6  35855    5976                  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Error: Within
           Df Sum Sq Mean Sq F value    Pr(>F)    
sample      1  35596   35596  39.086 2.040e-09 ***
Residuals 223 203092     911                      
---

Does this mean that from the Error:process line we get the
information, that sample is with one * significant , taking into
account that values are repeated measurents? what means Eror:within,
where can i read about this , beside Peinhiero/Bates and the MASS book
from Venables/Ripley?

Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
>     print(summary(aov(value~sample/process)))
                Df Sum Sq Mean Sq F value    Pr(>F)    
sample           1  34068   34068  41.719 6.872e-10 ***
sample:process  14 100812    7201   8.818 4.271e-15 ***
Residuals      216 176389     817                      
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
are here the f values more interesting? 
F sample ist 41, so it has a stronger influence as process, whos F is
8. so the process has a weaker influence as sample?

thanks
Nicolaas Busscher

-- 
#---------------------------------------------------
##!! Achtung neue E-Mail adresse: busscher at uni-kassel.de
#---------------------------------------------------
Dr.Nicolaas Busscher Universit?t GH Kassel
Nordbahnhofstrasse: 1a, D-37213 Witzenhausen
Phone: 0049-(0)5542-98-1715, Fax: 0049-(0)5542-98-1713









From HankeA at mar.dfo-mpo.gc.ca  Wed Mar 10 14:05:44 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 10 Mar 2004 09:05:44 -0400
Subject: [R] How to ascertain the number of clusters automatically?
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12495C@msgmarsta01.bio.dfo.ca>

Hi,
You may be interested in a clustering algorithm called OPTICS. It is both
interactive and automatic and does not require a lot of input parameters. It
is described as creating " an augmented ordering of the data representing
its density-based clustering structure". It "automatically and efficiently
extracts not only traditional clustering information but also the intrinsic
clustering structure".
Check out this site:
http://www.dbs.informatik.uni-muenchen.de/Forschung/KDD/Clustering/
Alex

-----Original Message-----
From: Fucang Jia [mailto:jiafucang at hotmail.com] 
Sent: March 9, 2004 11:30 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to ascertain the number of clusters automatically?


Hi, everyone,

There is many small cells which can be classified into several big cells 
from the scanned image. K-means clustering does not work well in this 
condition. I have done hierarchical clustering on cells successfully which 
uses shortest distance between classes. The number of clusters is about 3, 
4, 5, 6, 7 generally. One can ascertain the number of clusters visually.  
But because there are thousands of images to be clustered. So it is humdrum 
to me. I want to know if there are any methods that can be used to ascertain

the number of clusters automatically, especially in this case, only several 
clusters?

Thank you very much!

Best,

Fucang

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Brandon.J.Whitcher at gsk.com  Wed Mar 10 14:26:52 2004
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher@gsk.com)
Date: Wed, 10 Mar 2004 13:26:52 +0000
Subject: [R] PDF and Postscript figures in Solaris 9
Message-ID: <OFF4D93AFA.836AFAB1-ON80256E53.00490BAC@sb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040310/78075a2f/attachment.pl

From poolloopus at yahoo.com  Wed Mar 10 14:58:57 2004
From: poolloopus at yahoo.com (S P)
Date: Wed, 10 Mar 2004 05:58:57 -0800 (PST)
Subject: [R] Rank Simulations - Test statistic Help
Message-ID: <20040310135857.75997.qmail@web41010.mail.yahoo.com>

Hi all,

I am a biostatistician and I have developed my own
ranking system for clinical data. I would like to test
the efficiency of it w.r.t. to other ranking systems.
I would like to simulate the data and after assigning
ranks to my observed scores(after neglecting
dropouts), observe the type I error. If I want to do a
Kruuskal Wallis type of test, what test statistic
should I use to test for a difference of "delta"
between the two groups (I know "delta" since the data
is generated with that difference). The default K-W
test statistics test for a NULL difference and I want
to see how frequently my ranking system rejects the
correct null hypothesis of a "delta" difference.

(So my data is now in 2 arrays of unequal lengths due
to dropouts, which have the ranks for drug and
placebo.)

Thank you in advance for your help.

~S



From r.alberts at cs.rug.nl  Thu Mar 11 00:31:25 2004
From: r.alberts at cs.rug.nl (Rudi Alberts)
Date: 10 Mar 2004 15:31:25 -0800
Subject: [R] change the default separator
Message-ID: <1078961485.2229.2.camel@gbic04>

Hello,

How do I permanently change the separator in "paste" and "cat"
that by default is set to " " ?
(so that I dont have to say sep="" any time I use paste or cat)



regards, R. Alberts


-- 
Drs. R. Alberts
Groningen Bioinformatics Centre (GBIC)
Dep. of Mathematics and Computing Science
University of Groningen (RuG)
 
P.O. Box 800
9700 AV  Groningen
The Netherlands

tel. Haren     +31 50 363 8082
tel. Groningen +31 50 363 3957



From p.dalgaard at biostat.ku.dk  Wed Mar 10 15:48:54 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Mar 2004 15:48:54 +0100
Subject: [R] aperm() and as.list() args to "["
In-Reply-To: <a06002005bc74b97fe154@[139.166.242.29]>
References: <a06002005bc74b97fe154@[139.166.242.29]>
Message-ID: <x28yi8vju1.fsf@biostat.ku.dk>

[Ooops. Forgot to copy the list 1st time around... Also missed the
possibility of using t()]

Robin Hankin <rksh at soc.soton.ac.uk> writes:

> Hi everyone.
> 
> I'm playing with aperm():
> 
> a <- 1:24
> dim(a) <- c(2,3,2,2)
> permutation <-  c(1,2,4,3)
> b <- aperm(a,permutation)
> 
> 
> So if my understanding is right,
> 
> a[1,3,2,1] ==  b[c(1,3,2,1)[permutation] ]
> 
> but this isn't what I want because the RHS evaluates to a vector, and
> I am trying to identify a single element of b.
> 
> How do I modify the RHS to give what I want?

Matrix indexing:

> a[1,3,2,1]
[1] 11
>  b[rbind(c(1,3,2,1)[permutation])]
[1] 11

or, equivalently:
>  b[matrix(c(1,3,2,1)[permutation],1)]
[1] 11

or (neat, once you get it)

>  b[t(c(1,3,2,1)[permutation])]
[1] 11


> Following aren't right  either:
> b[as.vector(c(1,3,2,1)[permutation]) ]
> b[as.list(c(1,3,2,1)[permutation]) ]
> 
> 
> OBattempt:
> eval(parse(text=paste("b[",paste(c(1,3,2,1)[permutation],collapse=","),"]")))
> 
> which DOES work, but is ghastly!

Less ghastly (but only slightly so):

> do.call("[",c(list(b),as.list(c(1,3,2,1)[permutation])))
[1] 11

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wolfram at fischer-zim.ch  Wed Mar 10 15:58:33 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Wed, 10 Mar 2004 15:58:33 +0100
Subject: [R] converting lists got by tapply to dataframes
Message-ID: <20040310145833.GA5386@s1x.local>

I have two lists:

    xa <- list( X=c(1,2,3), Y=c(4,5,6), Z=c(7,8,9) )

    xb <- with( barley, tapply( X=seq(1:nrow(barley)), INDEX=site
        , FUN=function(z)yield[z]))

I can convert xa to a dataframe easily with:
    as.data.frame(xa)

But if i try the same with xb I get:
    as.data.frame(xb)
    Error in as.data.frame.default(xb) :
    can't coerce array into a data.frame

What helps?

(NB: I know the formula for xb is stupid, but it generates the same
type of list as the list I get from my real problem.)

Wolfram



From ggrothendieck at myway.com  Wed Mar 10 16:05:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 10 Mar 2004 10:05:17 -0500 (EST)
Subject: [R] change the default separator
Message-ID: <20040310150517.8063D39E5@mprdmxin.myway.com>


You could define your own function:

   paste0 <- function( ..., sep="" ) paste( ..., sep = sep )

which is used like this:

   > paste0("large","st")
   [1] "largest"

or perhaps you might want to define an infix operator to do
this:

   "%+%" <- function(x,y) paste(x,y,sep="")

which is used like this:

   > "large" %+% "st"
   [1] "largest"


It would be nice if a convenience wrapper such as paste0 were 
part of the base since I too find that I often use sep="" on 
paste.  Other areas of R such as ISOdate and table.csv are 
wrappers.

---

Hello,

How do I permanently change the separator in "paste" and "cat"
that by default is set to " " ?
(so that I dont have to say sep="" any time I use paste or cat)



regards, R. Alberts


-- 
Drs. R. Alberts
Groningen Bioinformatics Centre (GBIC)
Dep. of Mathematics and Computing Science
University of Groningen (RuG)
 
P.O. Box 800
9700 AV  Groningen
The Netherlands

tel. Haren     +31 50 363 8082
tel. Groningen +31 50 363 3957



From andy_liaw at merck.com  Wed Mar 10 16:16:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 10 Mar 2004 10:16:12 -0500
Subject: [R] change the default separator
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7963@usrymx25.merck.com>

You can use a wrapper:

mypaste <- function(...) paste(..., sep="")

HTH,
Andy

> From: Rudi Alberts
> 
> Hello,
> 
> How do I permanently change the separator in "paste" and "cat"
> that by default is set to " " ?
> (so that I dont have to say sep="" any time I use paste or cat)
> 
> 
> 
> regards, R. Alberts
> 
> 
> -- 
> Drs. R. Alberts
> Groningen Bioinformatics Centre (GBIC)
> Dep. of Mathematics and Computing Science
> University of Groningen (RuG)
>  
> P.O. Box 800
> 9700 AV  Groningen
> The Netherlands
> 
> tel. Haren     +31 50 363 8082
> tel. Groningen +31 50 363 3957
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From hardouin at cebc.cnrs.fr  Wed Mar 10 16:16:24 2004
From: hardouin at cebc.cnrs.fr (=?ISO-8859-1?Q?Hardouin_Lo=EFc?=)
Date: Wed, 10 Mar 2004 16:16:24 +0100
Subject: [R] performing type III
Message-ID: <404F3148.4030501@cebc.cnrs.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040310/2f0250b7/attachment.pl

From Virgilio.Gomez at uv.es  Wed Mar 10 16:18:48 2004
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Wed, 10 Mar 2004 16:18:48 +0100
Subject: [R] [R-pkgs] New package: DCluster
Message-ID: <1078931928.2622.36.camel@chomsky.estadi.uv.es>

Dear R users,

I am glad to announce the release of a new package for R: DCluster. It
implements a number of methods for the detection of clusters of disease.
It includes methods to test Poisson extra-variation (Chi-square test and
Potthoff-Whittinghill's test), Spatial Autocorrelation (Moran's I and
Geary's c), general clustering (Whittermore's statistic and Tango's
statistic), scan methods (Openshaw's GAM, Besag & Newell's methods and
Kulldorff & Nagarwalla's statistic) and a focused test (Stone's Test).

Bootstrap is used to estimate significance and a number of models are
proposed to simulate data: permutations and Poisson, Multinomial or Neg.
Binomial distributions.

I hope the package will be useful to epidemiologists and statisticians
involved in spatial epidemiological studies. As usual, feedback and
comments are welcome. Our hope is to improve the package, so suggestions
about new methods to be added are of interest.

With best regards,

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From andrewr at uidaho.edu  Wed Mar 10 17:01:54 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 10 Mar 2004 08:01:54 -0800
Subject: [R] converting lists got by tapply to dataframes
In-Reply-To: <20040310145833.GA5386@s1x.local>
References: <20040310145833.GA5386@s1x.local>
Message-ID: <200403100801.54685.andrewr@uidaho.edu>

Wolfram,

try 

unlist(xb) 

Andrew

On Wednesday 10 March 2004 06:58, Wolfram Fischer wrote:
> I have two lists:
>
>     xa <- list( X=c(1,2,3), Y=c(4,5,6), Z=c(7,8,9) )
>
>     xb <- with( barley, tapply( X=seq(1:nrow(barley)), INDEX=site
>         , FUN=function(z)yield[z]))
>
> I can convert xa to a dataframe easily with:
>     as.data.frame(xa)
>
> But if i try the same with xb I get:
>     as.data.frame(xb)
>     Error in as.data.frame.default(xb) :
>     can't coerce array into a data.frame
>
> What helps?
>
> (NB: I know the formula for xb is stupid, but it generates the same
> type of list as the list I get from my real problem.)
>
> Wolfram
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From thpe at hhbio.wasser.tu-dresden.de  Wed Mar 10 17:05:14 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 10 Mar 2004 17:05:14 +0100
Subject: [R] do.call and environments
Message-ID: <404F3CBA.2090107@hhbio.wasser.tu-dresden.de>

Hello,

I want to call a function "fx" given by name, where some "global" 
variables (in the environment of fx) are passed to the function. For 
compatibility reasons I cannot modify the parameter list of fx and I 
want to avoid setting variables in the global environment (e.g. via <<-)

Is there a way, how to do this?

Thomas P.

The example:

fx <- function(y) print(x*y)

f <- function(fun, xx) {
   fxx <- function() {do.call(fun, list(y=3))}
   x <- x
   fxx()
}

f("fx", 13)

## does not work, because fx does not find x



From ali at hmg.inpg.fr  Wed Mar 10 17:11:51 2004
From: ali at hmg.inpg.fr (Abdou Ali)
Date: Wed, 10 Mar 2004 17:11:51 +0100
Subject: [R] numerical equation
Message-ID: <20040310161139.7D1F83349F@ltheln11.hmg.inpg.fr>

Hello,
Is there R command to solve a nonlinear numerical equation.
Thank you.



From HankeA at mar.dfo-mpo.gc.ca  Wed Mar 10 16:51:49 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 10 Mar 2004 11:51:49 -0400
Subject: [R] Rank Simulations - Test statistic Help
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12495F@msgmarsta01.bio.dfo.ca>

Dear S,
Try rephrasing your question instead of altering the subject line. I can see
why you haven't any takers.
Alex

-----Original Message-----
From: S P [mailto:poolloopus at yahoo.com] 
Sent: March 10, 2004 9:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Rank Simulations - Test statistic Help


Hi all,

I am a biostatistician and I have developed my own
ranking system for clinical data. I would like to test
the efficiency of it w.r.t. to other ranking systems.
I would like to simulate the data and after assigning
ranks to my observed scores(after neglecting
dropouts), observe the type I error. If I want to do a
Kruuskal Wallis type of test, what test statistic
should I use to test for a difference of "delta"
between the two groups (I know "delta" since the data
is generated with that difference). The default K-W
test statistics test for a NULL difference and I want
to see how frequently my ranking system rejects the
correct null hypothesis of a "delta" difference.

(So my data is now in 2 arrays of unequal lengths due
to dropouts, which have the ranks for drug and
placebo.)

Thank you in advance for your help.

~S

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andrewr at uidaho.edu  Wed Mar 10 17:19:25 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 10 Mar 2004 08:19:25 -0800
Subject: [R] Non-linear regression problem: R vs JMP (long)
Message-ID: <200403100819.25760.andrewr@uidaho.edu>

Dear R friends,

I know that this topic has been mulled over before, and that there is a 
substantial difference between the convergence criteria for JMP and those for 
R.  I apologize that this is somwehat raking cold coals.

Summary: 

A model/data combination achieves convergence in JMP, and survives a 
reasonably rigorous examination (sensible parameter estimates, well-behaved 
surface, confidence intervals exclude 0).  The same combination fails to 
achieve convergence in R despite starting from the estimates reported in JMP.  
What can I do?

Full story:

I am collaborating on a project which presently requires the fit of a seven 
parameter non-linear function to 874 observations.  The function is:

freeze.d <- deriv3(~ dbh^b * u^dbh * a1 * smi^a2 * exp(a3*pbal) *
  (1 + a4*exp(a5*ba)) * cr^a6 * dh5^a7,
  c("a1","a2","a3","a4","a5","a6","a7","b","u"),
  function(dbh, smi, pbal, ba, cr, dh5, a1, a2, a3, a4, a5, a6, a7, b, u){})

The dbh, smi, pbal, ba, cr, and dh5, are known.

The data do not require the level of intricacy reflected in the function. It 
is debatable whether they support it. However, my collaborator is anxious to 
avoid linear models because of their relatively poor extrapolative 
properties.  So, that is an ongoing discussion.

He has achieved convergence using non-linear least squares in JMP.  JMP uses 
the union of three criteria: small change in objective function, small change 
in parameter values, and small change in gradient.  He can achieve 
convergence in any of the three by reducing the other two sufficiently. He 
arrives at the same point of convergence from a range of different starting 
values.  He gets garbage solutions for other starting points, but recognizes 
these as such and explores elsewhere.  At the point of convergence the model 
has reasonable behavior, the confidence intervals for all the parameters 
exclude 0, the surface seems to fit the data just fine, and the parameter 
estimates all make sense. Some of them are highly correlated (up to -0.95) 
but by no means all.  In short, it seems to me to be a very thorough and 
careful effort.

I'm trying to reproduce the model in R using nls.  The convergence criterion 
is different than JMP: it's a relative-offset convergence criterion. Even if 
I start with his converged parameter estimates, nls will consume as many 
iterations as I allow it and produce an error, e.g. "number of iterations 
exceeded maximum of 50000".  I have the trace on, and for the last n-2 
iterations the parameter estimates do not change, at least in the digits that 
trace provides. Increasing the tolerance does not seem to help.  

So, I wonder: is it possible that JMP could produce results that are garbage 
but stand up to reasonable scrutiny? Alternatively, is there some further 
element of the fit that I should advise him to examine?  I've been reading 
Bruce McCullough's work and I know that for example S-plus was one of only 
two packages to declare "ns" instead of providing estimates with 0 accurate 
digits.  But, how does one know that is what is happening in any given 
situation, if the model looks good?  Finally, it's possible that I am using 
nls wrongly: here is my call

freeze.nls.d.1 <-
    nls(dd.5 ~ freeze.d(dbh.0, smi, pbal.0, sba.0, cr.0, dh.5,
                        a1, a2, a3, a4, a5, a6, a7, b, u),
        data = trees, control=ctl.obj, trace=T,
        start = list(b = 0.2, u = 1, a1 = 0.4, a2 = -0.1, a3 = -0.01, a4 = 3, 
a5 = -0.02, a6 = 0.25, a7 = 0.40))

Any insights will be appreciated!

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From statho3 at web.de  Wed Mar 10 17:15:50 2004
From: statho3 at web.de (Thomas Stabla)
Date: Wed, 10 Mar 2004 17:15:50 +0100 (CET)
Subject: [R] R CMD check errors
Message-ID: <Pine.LNX.4.44.0403101712400.1319-100000@spock.vulcan>

Hello,

I'm getting some error messages from R CMD check I can't deal with.
I'm working under Linux with R 1.8.1
The package working directory can be found at:

http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/distr.tar.gz


Here's the 00check.log produced by R CMD check distr

* using log directory '/home/tom/studium/R/swp/swp/package/distr.Rcheck'
* checking for file 'distr/DESCRIPTION' ... OK
* checking if this is a source package ... OK
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking index information ... OK
* checking package subdirectories ... WARNING
Subdirectory 'data' contains no data sets.
Subdirectory 'src' contains no source files.
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... WARNING
Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
        package/namespace load failed
Execution halted
* checking for replacement functions with final arg not named 'value' ... WARNING
Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
        package/namespace load failed
Execution halted
* checking Rd files ... OK
* checking for missing documentation entries ... ERROR
Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :


Thanks for your help,
Thomas Stabla



From rksh at soc.soton.ac.uk  Wed Mar 10 17:21:08 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 10 Mar 2004 16:21:08 +0000
Subject: [R] aperm() and as.list() args to "["
In-Reply-To: <x28yi8vju1.fsf@biostat.ku.dk>
References: <a06002005bc74b97fe154@[139.166.242.29]>
	<x28yi8vju1.fsf@biostat.ku.dk>
Message-ID: <a06002008bc74e30f9f50@[139.166.242.29]>

Many thanks to everyone who replied (online and offline) to my query 
about permutations.
There appear to be two independent ways of solving it:

(1) matrix indexing, as in
       b[rbind(c(1,3,2,1)[permutation])]

(2) do.call(), as in

ind <- as.list(c(1,3,2,1)[permutation])
do.call("[", c(list(b), ind))


I found both these very instructive (us ex-matlab users often 
overlook matrix indexing!) but to me the
do.call() solution is positively sublime (Peter, why didn't you like 
it? let's get an aesthetics discussion going!)

I spent some considerable time over lunch today reading the helppages 
for do.call(), "[", and c().
There is no way under the Sun that I would have come up with the 
second command above.

Could we add this example or one like it to help(do.call)?

Also, the help page for "[" doesn't include what do.call() did, namely

"["(b,1,3,1,2)

could we add this or some similar example to ?"[" please?


cheers

rksh




>
>  >
>>  a <- 1:24
>>  dim(a) <- c(2,3,2,2)
>>  permutation <-  c(1,2,4,3)
>>  b <- aperm(a,permutation)
>  >
>  >

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From ocbruno at netscape.net  Wed Mar 10 14:38:16 2004
From: ocbruno at netscape.net (Marcelo Bruno)
Date: Wed, 10 Mar 2004 13:38:16 +0000
Subject: [R] Fisheries acoustics
Message-ID: <404F1A48.5080103@netscape.net>

Hi, member-list

i'm searching for packages or routines to analise fisheries acoustics data.
Something make  import data of scientific ecosound (EK500) of paralel 
port, filter signal, analyse pattern of pixel fish shoals...

Is there  someone work with this ? If no, why dont make colective effort 
to do this?? (package fish_acoustic).

Thanks
Marcelo



From Benjamin.STABLER at odot.state.or.us  Wed Mar 10 17:29:24 2004
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Wed, 10 Mar 2004 08:29:24 -0800
Subject: [R] Rcmd BATCH command line arguments 
Message-ID: <76A000A82289D411952F001083F9DD06047FE50C@exsalem4-bu.odot.state.or.us>

Okay, thanks.  But it seems inconsistent that Rterm and Rgui support setting
environment variables this way while Rcmd does not.  Also, I don't think the
rw-FAQ is all that helpful on this issue.  It says:

Environment variables can be set in R in three different ways.
  1. On the command line as name=value pairs.  For example in the
     shortcut to `RGui' you could have

          path_to_R\bin\Rgui.exe HOME=p:/ R_LIBS=p:/myRlib

  2. In an environment file ..........

It would be clearer if point one said that it only applies to Rterm and Rgui
but not Rcmd.  

Ben Stabler

>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Tuesday, March 09, 2004 11:49 PM
>To: STABLER Benjamin
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] Rcmd BATCH command line arguments 
>
>
>On Tue, 9 Mar 2004 Benjamin.STABLER at odot.state.or.us wrote:
>
>> I want to run Rcmd BATCH with R_DEFAULT_PACKAGE=base so it 
>doesn't load any
>> packages, but it seems to reject this argument because it 
>does not start
>> with a '-' or '--'.  Is there a different argument that will 
>work?  Thanks.
>
>It is not a command-line argument!  In Rterm.exe you can set 
>environment
>variables this way, but not in Rcmd.exe.  So set the 
>environment variable
>R_DEFAULT_PACKAGE>S< to NULL, not base which is always loaded.  See 
>?Startup for the documentation (which is correct) and the 
>rw-FAQ for how 
>to set environment variables.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From abunn at montana.edu  Wed Mar 10 17:34:54 2004
From: abunn at montana.edu (Andy Bunn)
Date: Wed, 10 Mar 2004 09:34:54 -0700
Subject: [R] Center labels on a boxplot
Message-ID: <002301c406bd$a143feb0$78f05a99@msu.montana.edu>

Suppose that I have data on three species for a variable and datasets
from two time periods. I want to make a boxplot of the first dataset and
then add the second using 'at = ' and 'add = T' as in the example for
'boxplot.'

Since the boxes are paired by species, I want to do is have the x labels
be centered between the boxes. I'm doing this now with mtext and
entering spaces to make them center. There must be a better way.

Much thanks, Andy

   #######################
   myDF1 <- data.frame(spp1 = c(0,0,0,0,1,0,1,1,0,0,0,1), 
                        spp2 = c(1,0,1,1,0,0,0,0,1,0,0,0),
                        spp3 = c(0,1,0,0,0,1,0,0,0,1,1,0),
                        aVar = runif(12, 1, 10) + 1:12)

    myDF2 <- data.frame(spp1 = c(1,1,0,0,0,0,0,0,1,0,0,1), 
                        spp2 = c(0,0,1,1,0,0,0,1,0,0,1,0),
                        spp3 = c(0,0,0,0,1,1,1,0,0,1,0,0),
                        aVar = runif(12, 1, 10) + 1:12)

    boxplot(aVar ~ spp1 + spp2 + spp3, data = myDF1, boxwex = 0.25, at =
1:3 - 0.2,
                       names = c("","",""), 
                       ylab = "A Varirable of Extreme Interest",
                       ylim = c(0,24))

    boxplot(aVar ~ spp1 + spp2 + spp3, data = myDF2, boxwex = 0.25, at =
1:3 + 0.2,
                       names = c("","",""), add = T, col="gray")
 
    legend(0.5, 24, c("Time 1", "Time 2"),
           fill = c("white", "gray"))    
 
    
   mtext("SPP1, SPP2, SPP3",.....)
   #########################



From Jens_Praestgaard at hgsi.com  Wed Mar 10 17:13:47 2004
From: Jens_Praestgaard at hgsi.com (Jens_Praestgaard@hgsi.com)
Date: Wed, 10 Mar 2004 11:13:47 -0500
Subject: [R] Question concerning library function "nlme" and lexical scoping
Message-ID: <OFE8A84E07.EA57370E-ON85256E53.0057B09A-85256E53.0059277B@hgsi.com>




I am running into problems calling the library function nlme from within
another function.  Basically,the following function (with v a list
containing data and initialization values)

> testfunc
function(dat=v) {
test<-nlsList(result~a+(b-a)/(1+(conc/(c+z*cdiff))^d)
|rep,start=dat$init,data=dat$mixeddat)
return(nlme(test,random=b~1))
}
 produces the error message

Error in eval(expr, envir, enclos) : Object "dat" not found.

However, if the assignment dat<-v is done globally, then testfunc()
produces the desired mixed model analysis.

As I understand lexical scoping, the nlme function should take the value of
"dat" from the calling environment through its "data = sys.frame(sys.parent
())" argument. Instead it seems to only recognize dat if it is defined
globally.

Thank you.

Jens Praestgaard
Human Genome Sciences



From andy_liaw at merck.com  Wed Mar 10 17:43:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 10 Mar 2004 11:43:01 -0500
Subject: [R] converting lists got by tapply to dataframes
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7966@usrymx25.merck.com>

That gives you a vector, not a data frame.

The problem is that the object returned by tapply is a 1-d array of list.
When as.data.frame() is called, the default method doesn't know how to
handle arrays and gives the error.  What you really want for this particular
case is to treat the object as a list, so the unorthodox way is:

  as.data.frame.list(xb)

OK, I didn't really say that...

Andy

> From: Andrew Robinson
> 
> Wolfram,
> 
> try 
> 
> unlist(xb) 
> 
> Andrew
> 
> On Wednesday 10 March 2004 06:58, Wolfram Fischer wrote:
> > I have two lists:
> >
> >     xa <- list( X=c(1,2,3), Y=c(4,5,6), Z=c(7,8,9) )
> >
> >     xb <- with( barley, tapply( X=seq(1:nrow(barley)), INDEX=site
> >         , FUN=function(z)yield[z]))
> >
> > I can convert xa to a dataframe easily with:
> >     as.data.frame(xa)
> >
> > But if i try the same with xb I get:
> >     as.data.frame(xb)
> >     Error in as.data.frame.default(xb) :
> >     can't coerce array into a data.frame
> >
> > What helps?
> >
> > (NB: I know the formula for xb is stupid, but it generates the same
> > type of list as the list I get from my real problem.)
> >
> > Wolfram
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : 
> http://www.uidaho.edu/~andrewr
> Moscow ID 83843                
>       Or: http://www.biometrics.uidaho.edu
> No statement above necessarily represents my employer's opinion.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ggrothendieck at myway.com  Wed Mar 10 17:55:52 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 10 Mar 2004 11:55:52 -0500 (EST)
Subject: [R] do.call and environments
Message-ID: <20040310165552.BE61239B3@mprdmxin.myway.com>



Try this:


x <- 7
fx <- function(y) print(x*y)
f <- function(fun, x) {
	myfx <- eval(parse(text=deparse(fun)))
	myfx(3)
}
f(fx,2)  # uses 2, not 7, for x

This creates a new function myfx which has the same functionality
as fx but is created in the body of f and therefore has that body
as its environment -- with the result that it finds the x arg
to f.  myfx is not fx (it just has the same functionality) so 
fx's environment is irrelevant.

Just one caveat.  If fx is recursive it should use Recall
to call itself.  See ?Recall 



---
Hello,

I want to call a function "fx" given by name, where some "global" 
variables (in the environment of fx) are passed to the function. For 
compatibility reasons I cannot modify the parameter list of fx and I 
want to avoid setting variables in the global environment (e.g. via <<-)

Is there a way, how to do this?

Thomas P.

The example:

fx <- function(y) print(x*y)

f <- function(fun, xx) {
   fxx <- function() {do.call(fun, list(y=3))}
   x <- x
   fxx()
}

f("fx", 13)

## does not work, because fx does not find x



From kjetil at entelnet.bo  Wed Mar 10 18:02:49 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 10 Mar 2004 13:02:49 -0400
Subject: [R] converting lists got by tapply to dataframes
In-Reply-To: <20040310145833.GA5386@s1x.local>
Message-ID: <404F11F9.30419.DADB19@localhost>

On 10 Mar 2004 at 15:58, Wolfram Fischer wrote:

It's a bit strange. The following should work:

as.data.frame(as.matrix(xb))

Kjetil Halvorsen


> I have two lists:
> 
>     xa <- list( X=c(1,2,3), Y=c(4,5,6), Z=c(7,8,9) )
> 
>     xb <- with( barley, tapply( X=seq(1:nrow(barley)), INDEX=site
>         , FUN=function(z)yield[z]))
> 
> I can convert xa to a dataframe easily with:
>     as.data.frame(xa)
> 
> But if i try the same with xb I get:
>     as.data.frame(xb)
>     Error in as.data.frame.default(xb) :
>     can't coerce array into a data.frame
> 
> What helps?
> 
> (NB: I know the formula for xb is stupid, but it generates the same
> type of list as the list I get from my real problem.)
> 
> Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From lixian7001 at yahoo.com  Wed Mar 10 18:22:52 2004
From: lixian7001 at yahoo.com (li xian)
Date: Wed, 10 Mar 2004 09:22:52 -0800 (PST)
Subject: [R] plot date
Message-ID: <20040310172252.53612.qmail@web11509.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040310/942e5eaf/attachment.pl

From thpe at hhbio.wasser.tu-dresden.de  Wed Mar 10 18:27:25 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 10 Mar 2004 18:27:25 +0100
Subject: [R] do.call and env
In-Reply-To: <200403101722.i2AHMsf23051@gator.dt.uh.edu>
References: <200403101722.i2AHMsf23051@gator.dt.uh.edu>
Message-ID: <404F4FFD.20805@hhbio.wasser.tu-dresden.de>

Erin Hodgess wrote:

> Hi Thomas!
> 
> Try something like this:
> 
> fx <- function(y) print(x*y)
> 
> f <- function(fun,x) {
> assign("x",x,env=.GlobalEnv)
> fxx <- function() {
> do.call(fun,list(y=3))
> }
> fxx()
> }
> 
> f("fx",13)
> 
> [1] 39
> 
> 
> Hope this helps!

Thank you it works, but sorry, this is not what I want, as it assigns 
"x" to the global environment.

Thomas



From ligges at statistik.uni-dortmund.de  Wed Mar 10 18:32:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 10 Mar 2004 18:32:35 +0100
Subject: [R] Writing Rd files dor package
In-Reply-To: <404ED479.50304@ensam.inra.fr>
References: <404ED479.50304@ensam.inra.fr>
Message-ID: <404F5133.50003@statistik.uni-dortmund.de>

Martin Olivier wrote:
> Hi all,
> 
> I have some problems to write help files for a package. I don't 
> understant how to include help
> files in a package that I develop.
> 
> I have written the Rd files that I have put in a man directory. So after 
> this, I built the package with the command
> R CMD build (for linux) and I obtained my package (I have also used the 
> commands  R CMD Rdconv -t txt and
> R CMD Rdconv -t html to convert my Rd files in txt and html files)
>

R CMD Rdconv is not necessary, if you are building a package. The Rd 
files will be converted automatically during R CMD INSTALL.


> I succesfully install the package with R INSTALL and load the library 
> with the command library()
> in a R session. But there is  no available help for the functions, enven 
> if in the /path/R/library/mypacakge/ directory, there are the 
> directories html,  man and help.

If the Rd files have the correct syntax and corresoponding files are in 
the mentioned directories (e.g. ./help), everything should work fine ....


> I suppose that I have made some mistakes. I read "Writing R Extensions" 
> , but I do not find my mistakes.
> Could you help me.
> 
> Best regards,
> Olivier


Uwe Ligges



From tplate at blackmesacapital.com  Wed Mar 10 18:56:28 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 10 Mar 2004 10:56:28 -0700
Subject: [R] do.call and environments
In-Reply-To: <404F3CBA.2090107@hhbio.wasser.tu-dresden.de>
References: <404F3CBA.2090107@hhbio.wasser.tu-dresden.de>
Message-ID: <6.0.3.0.2.20040310104803.049e4160@mailhost.blackmesacapital.com>

The reason in your example that fx() doesn't find 'x' is that the lexical 
scope of fx() does not include 'x'.  So, this is what must be fixed, in one 
way or another.

One simple way to make your example work is to define fx() in a place where 
its lexical scope includes the variables you want it to see:

 > f <- function(fun, xx) {
+   x <- xx
+   fxx <- function() do.call(fun, list(y=3))
+   fx <- function(y) print(x*y)
+   fxx()
+ }
 > x # verify that x is not a global variable!
Error: Object "x" not found
 > f("fx", 13)
[1] 39
 >

Another way, that is probably more suited to what you want (since it sounds 
like you don't have control over where fx() is defined), is to make a local 
copy of fx(), and change its environment:

 > fx <- function(y) print(x*y)
 >
 > f <- function(fun, xx) {
+   x <- xx
+   fxx <- function() do.call(fun, list(y=3))
+   fx <- fx
+   environment(fx) <- environment()
+   fxx()
+ }
 > x # verify that x is not a global variable!
Error: Object "x" not found
 > f("fx", 13)
[1] 39
 >

I suspect there is also some way to do this using eval() and its envir= 
and/or enclos= arguments, but I couldn't get anything like this to work.

The above solutions were generated by manual genetic programming (i.e., 
trial and error), so they may not be particularly good or 
elegant.  However, I'm sure that others will point out both any problems 
with these suggestions and some better solutions!

hope this helps,

Tony Plate

At Wednesday 09:05 AM 3/10/2004, Thomas Petzoldt wrote:
>Hello,
>
>I want to call a function "fx" given by name, where some "global" 
>variables (in the environment of fx) are passed to the function. For 
>compatibility reasons I cannot modify the parameter list of fx and I want 
>to avoid setting variables in the global environment (e.g. via <<-)
>
>Is there a way, how to do this?
>
>Thomas P.
>
>The example:
>
>fx <- function(y) print(x*y)
>
>f <- function(fun, xx) {
>   fxx <- function() {do.call(fun, list(y=3))}
>   x <- x
>   fxx()
>}
>
>f("fx", 13)
>
>## does not work, because fx does not find x
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Mar 10 19:03:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 10 Mar 2004 13:03:14 -0500
Subject: [R] do.call and environments
Message-ID: <3A822319EB35174CA3714066D590DCD504AF796B@usrymx25.merck.com>

Seems to me what you want is dynamic scoping: `x' is not defined in `fx'.
You want `x' to be found in the scope of the function(s) that calls `fx',
rather than the environment where `fx' is defined.  I was told (thanks,
Robert!) that that is a very bad idea:  as the author of `fx', you want some
assurance of what `x' might be.  This is done via R's lexical scope.  With
dynamic scope, there is absolutely no way to do that.  For example, I might
write a function `g' that define `x' as a character, or a data frame, or a
list, or an `lm' object, or a connection, ....  How would you write `fx' to
deal with that nightmare if you have dynamic scope?

Andy

> From: Thomas Petzoldt
> 
> Hello,
> 
> I want to call a function "fx" given by name, where some "global" 
> variables (in the environment of fx) are passed to the function. For 
> compatibility reasons I cannot modify the parameter list of fx and I 
> want to avoid setting variables in the global environment 
> (e.g. via <<-)
> 
> Is there a way, how to do this?
> 
> Thomas P.
> 
> The example:
> 
> fx <- function(y) print(x*y)
> 
> f <- function(fun, xx) {
>    fxx <- function() {do.call(fun, list(y=3))}
>    x <- x
>    fxx()
> }
> 
> f("fx", 13)
> 
> ## does not work, because fx does not find x
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Mar 10 19:07:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 18:07:18 +0000 (GMT)
Subject: [R] Question concerning library function "nlme" and lexical
	scoping
In-Reply-To: <OFE8A84E07.EA57370E-ON85256E53.0057B09A-85256E53.0059277B@hgsi.com>
Message-ID: <Pine.LNX.4.44.0403101757200.22539-100000@gannet.stats>

On Wed, 10 Mar 2004 Jens_Praestgaard at hgsi.com wrote:

> I am running into problems calling the library function nlme from within
> another function.  Basically,the following function (with v a list
> containing data and initialization values)
> 
> > testfunc
> function(dat=v) {
> test<-nlsList(result~a+(b-a)/(1+(conc/(c+z*cdiff))^d)
> |rep,start=dat$init,data=dat$mixeddat)
> return(nlme(test,random=b~1))
> }
>  produces the error message
> 
> Error in eval(expr, envir, enclos) : Object "dat" not found.
> 
> However, if the assignment dat<-v is done globally, then testfunc()
> produces the desired mixed model analysis.
> 
> As I understand lexical scoping, the nlme function should take the value of
> "dat" from the calling environment through its "data = sys.frame(sys.parent
> ())" argument. Instead it seems to only recognize dat if it is defined
> globally.

Let's be careful. Lexical scoping is to do with where a function is 
defined, so the lexical scope for nlme() is namespace:nlme.  That's not 
pertinent here.

But if you do traceback() you will see that nlme is not looking in data 
for dat, but using the result `test'.  I would need a working example to 
be sure enough of what is going on.

http://developer.r-project.org/nonstandard-eval.pdf
may be instructive.  nlme is particularly non-standard as it has to deal 
with several formulae, and get them in several ways.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Mar 10 19:27:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 18:27:38 +0000 (GMT)
Subject: [R] converting lists got by tapply to dataframes
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7966@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0403101821280.22567-100000@gannet.stats>

On Wed, 10 Mar 2004, Liaw, Andy wrote:

> That gives you a vector, not a data frame.
> 
> The problem is that the object returned by tapply is a 1-d array of list.
> When as.data.frame() is called, the default method doesn't know how to
> handle arrays and gives the error.  What you really want for this particular
> case is to treat the object as a list, so the unorthodox way is:
> 
>   as.data.frame.list(xb)
> 
> OK, I didn't really say that...

If this dataset barley from package lattice, it works in 1.9.0 alpha
which does have as.data.frame.array.  So the issue is about to go away.


> > On Wednesday 10 March 2004 06:58, Wolfram Fischer wrote:
> > > I have two lists:
> > >
> > >     xa <- list( X=c(1,2,3), Y=c(4,5,6), Z=c(7,8,9) )
> > >
> > >     xb <- with( barley, tapply( X=seq(1:nrow(barley)), INDEX=site
> > >         , FUN=function(z)yield[z]))
> > >
> > > I can convert xa to a dataframe easily with:
> > >     as.data.frame(xa)
> > >
> > > But if i try the same with xb I get:
> > >     as.data.frame(xb)
> > >     Error in as.data.frame.default(xb) :
> > >     can't coerce array into a data.frame
> > >
> > > What helps?
> > >
> > > (NB: I know the formula for xb is stupid, but it generates the same
> > > type of list as the list I get from my real problem.)
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Wed Mar 10 19:30:13 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 10 Mar 2004 13:30:13 -0500 (EST)
Subject: [R] Center labels on a boxplot
Message-ID: <20040310183013.91DA139B9@mprdmxin.myway.com>



How about something like this suitably fixed up:

 par(mfrow=c(1,3)
 for(i in 1:3)
   boxplot( data.frame( a=myDF1[myDF1[i]==1,4],
                        b=myDF2[myDF2[i]==1,4]),xlab=i,ylim=c(0,25) )



From bates at stat.wisc.edu  Wed Mar 10 19:40:00 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 10 Mar 2004 12:40:00 -0600
Subject: [R] Non-linear regression problem: R vs JMP (long)
In-Reply-To: <200403100819.25760.andrewr@uidaho.edu>
References: <200403100819.25760.andrewr@uidaho.edu>
Message-ID: <6rbrn4czr3.fsf@bates4.stat.wisc.edu>

Andrew Robinson <andrewr at uidaho.edu> writes:

> Summary: 

> A model/data combination achieves convergence in JMP, and survives a
> reasonably rigorous examination (sensible parameter estimates,
> well-behaved surface, confidence intervals exclude 0).  The same
> combination fails to achieve convergence in R despite starting from
> the estimates reported in JMP.  What can I do?

> Full story:

> I am collaborating on a project which presently requires the fit of a seven 
> parameter non-linear function to 874 observations.  The function is:

> freeze.d <- deriv3(~ dbh^b * u^dbh * a1 * smi^a2 * exp(a3*pbal) *
>   (1 + a4*exp(a5*ba)) * cr^a6 * dh5^a7,
>   c("a1","a2","a3","a4","a5","a6","a7","b","u"),
>   function(dbh, smi, pbal, ba, cr, dh5, a1, a2, a3, a4, a5, a6, a7, b, u){})

Isn't that 9 parameters - counting b and u?

> The dbh, smi, pbal, ba, cr, and dh5, are known.

I would not be surprised at the inability to converge on a 9 parameter
nonlinear model with so many multiplicative terms.  Would it be
reasonable to try to fit the logarithm of the response to the
logarithm of the prediction instead? 

Try evaluating the gradient matrix at JMP's converged parameter
estimates and checking the condition number (using the kappa
function).  I expect it will be very large, indicating very poor
conditioning in the gradient.

> The data do not require the level of intricacy reflected in the function. It 
> is debatable whether they support it. However, my collaborator is anxious to 
> avoid linear models because of their relatively poor extrapolative 
> properties.  So, that is an ongoing discussion.

I don't think that an overparameterized nonlinear model is necessarily
the answer.

You could try to reduce the number of parameters by 1 in R using the
plinear algorithm (remove a1 as it is a simple multiplier but state
alg='plinear' in the call to nls).



From angel_lul at hotmail.com  Wed Mar 10 20:41:55 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 10 Mar 2004 19:41:55 +0000
Subject: [R] reading binary file with different modes 
Message-ID: <404F6F83.1050206@hotmail.com>

I have a binary file representing a matrix with columns of different 
variable type/mode (i.e. the file was saved from C using double and int 
variables)
I want to import it into R, I've been reading the R Import/Export and I 
am able to use readBin for a file that contains only one variable type 
(either all doubles or all ints) but I can not find the way to use it 
when you have a file with mixture of the two.
Could somebody point me to a solution or appropiatte reference?
Thanks,
Angel



From ray at mcs.vuw.ac.nz  Wed Mar 10 20:32:21 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Thu, 11 Mar 2004 08:32:21 +1300 (NZDT)
Subject: [R] Map of British Colonial America 1775
Message-ID: <200403101932.i2AJWLL0028333@tahi.mcs.vuw.ac.nz>

>       I've reviewed the "map" documentation I've found so far with 
> S-Plus 6.2 and R including "www.r-project.org" -> search -> "R site 
> search" -> "world map" and Becker & Wilkes (1993) "Maps in S" cited in 
> the "map" documentation.  I've noticed that, for example, 
> 'library(maps);  map("world", "UK:Bermuda")' produces an 83-point 
> polygon sketch in S-Plus 6.2 but only a degenerate 3-point polygon (with 
> the third point = the first) in R 1.8.1. 
> 
You need the mapdata package.  The default "world" database in S-Plus is
the high-resolution one (with also "world.thin").  In R the high
resolution one is "worldHires" from mapdata (mainly so that the
standard maps package is a manageable size).

Ray Brownrigg



From MSchwartz at medanalytics.com  Wed Mar 10 20:39:17 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 10 Mar 2004 13:39:17 -0600
Subject: [R] Center labels on a boxplot
In-Reply-To: <002301c406bd$a143feb0$78f05a99@msu.montana.edu>
References: <002301c406bd$a143feb0$78f05a99@msu.montana.edu>
Message-ID: <1078947557.10736.69.camel@localhost.localdomain>

On Wed, 2004-03-10 at 10:34, Andy Bunn wrote:
> Suppose that I have data on three species for a variable and datasets
> from two time periods. I want to make a boxplot of the first dataset and
> then add the second using 'at = ' and 'add = T' as in the example for
> 'boxplot.'
> 
> Since the boxes are paired by species, I want to do is have the x labels
> be centered between the boxes. I'm doing this now with mtext and
> entering spaces to make them center. There must be a better way.
> 
> Much thanks, Andy
> 
>    #######################
>    myDF1 <- data.frame(spp1 = c(0,0,0,0,1,0,1,1,0,0,0,1), 
>                         spp2 = c(1,0,1,1,0,0,0,0,1,0,0,0),
>                         spp3 = c(0,1,0,0,0,1,0,0,0,1,1,0),
>                         aVar = runif(12, 1, 10) + 1:12)
> 
>     myDF2 <- data.frame(spp1 = c(1,1,0,0,0,0,0,0,1,0,0,1), 
>                         spp2 = c(0,0,1,1,0,0,0,1,0,0,1,0),
>                         spp3 = c(0,0,0,0,1,1,1,0,0,1,0,0),
>                         aVar = runif(12, 1, 10) + 1:12)
> 
>     boxplot(aVar ~ spp1 + spp2 + spp3, data = myDF1, boxwex = 0.25, at =
> 1:3 - 0.2,
>                        names = c("","",""), 
>                        ylab = "A Varirable of Extreme Interest",
>                        ylim = c(0,24))
> 
>     boxplot(aVar ~ spp1 + spp2 + spp3, data = myDF2, boxwex = 0.25, at =
> 1:3 + 0.2,
>                        names = c("","",""), add = T, col="gray")
>  
>     legend(0.5, 24, c("Time 1", "Time 2"),
>            fill = c("white", "gray"))    
>  
>     
>    mtext("SPP1, SPP2, SPP3",.....)
>    #########################


Andy,

Your boxplots are at (1:3 - 0.2) and (1:3 + 0.2). Thus, the centers of
the pairs are at 1:3.

You can use:

mtext(side = 1, text = c("SPP1", "SPP2", "SPP3"), at = 1:3, line = 2)

To place the pair labels in the centers along the x axis.

HTH,

Marc Schwartz



From thpe at hhbio.wasser.tu-dresden.de  Wed Mar 10 20:41:31 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 10 Mar 2004 20:41:31 +0100
Subject: [R] Center labels on a boxplot
In-Reply-To: <002301c406bd$a143feb0$78f05a99@msu.montana.edu>
References: <002301c406bd$a143feb0$78f05a99@msu.montana.edu>
Message-ID: <404F6F6B.3090609@hhbio.wasser.tu-dresden.de>

Hello Andy,

does

mtext(c("SPP1", "SPP2", "SPP3"), side=1, at=c(1,2,3))

complete your script?

Thomas P.



From ripley at stats.ox.ac.uk  Wed Mar 10 21:56:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Mar 2004 20:56:19 +0000 (GMT)
Subject: [R] plot date
In-Reply-To: <20040310172252.53612.qmail@web11509.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403102054310.1075-100000@gannet.stats>

On Wed, 10 Mar 2004, li xian wrote:

> I was trying to plot date time  vs numeric values.
> Number of My date value are about 4000,  It is continuous time series, 3 days, the interveal is 5 minutes, (2002-09-22 12:35:00 pm to 2002-09-22 3:40:00 pm) How can i plot it in R? 
> I tried the regular way:
>  
> formattime <- strptime(time, format= "%d-%b-%y %I:%M %p")
> plot(formattime, outNum)

>  However the x axes doen't give me the hours by each three days, instead
> it is "mon" "tue" etc. What should I do?

Read ?axis.POSIXct and its examples.  The plot method has to guess how you 
want the axis labelled, but by taking charge you can have your way.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From apjaworski at mmm.com  Wed Mar 10 22:27:22 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Wed, 10 Mar 2004 15:27:22 -0600
Subject: [R] numerical equation
Message-ID: <OF88FC3A0F.A8C5C6C4-ON86256E53.00736811-86256E53.0075DCF7@mmm.com>






Check out the uniroot function.  It will solve a single equation.

There is also a packge RMinpack.  It implements the original Fortran
Minpack library and will solve systems of nonlinear equations.  It is only
available in source form (search Google for it).

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           Abdou Ali            |
|         |           <ali at hmg.inpg.fr>    |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           03/10/2004 10:11     |
|         |           Please respond to    |
|         |           Abdou.Ali            |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       r-help at stat.math.ethz.ch                                                                                     |
  |      cc:                                                                                                                    |
  |      Subject:  [R] numerical equation                                                                                       |
  >-----------------------------------------------------------------------------------------------------------------------------|




Hello,
Is there R command to solve a nonlinear numerical equation.
Thank you.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Mar 10 22:44:57 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 10 Mar 2004 16:44:57 -0500
Subject: [R] performing type III
In-Reply-To: <404F3148.4030501@cebc.cnrs.fr>
Message-ID: <20040310214459.HYOU27950.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Alex,

The anova() function doesn't take a type argument, producing the error.
Perhaps the Anova() function in the car package will do what you want.

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hardouin Lo?c
> Sent: Wednesday, March 10, 2004 10:16 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] performing type III
> 
> Hi,
> 
> I meet some tecnhical problem when performing type III test 
> in an analysis of covariance.
> I code with contrast=sum and precise type="III" for the anova table.
> There is a warning message like this:
> Models with response "NULL" removed and type I is performed 
> by default. 
> And R-Help is not very clear on use of contrasts.
> 
> Please don't rush in the usual holy war about this topic. And 
> if somebody as a concrete example...
> 
> Thank you
> 
> Alex
>



From SLiang at wyeth.com  Wed Mar 10 23:09:23 2004
From: SLiang at wyeth.com (Sean Liang)
Date: Wed, 10 Mar 2004 17:09:23 -0500
Subject: [R] data frame filtration
Message-ID: <s04f4bcd.096@gwsmtp.genetics.com>

Hi, R-help:

I am a new user of R and am very pleased with R's features. Here I have
one question regarding data frame manipulation. I have a data frame look
like this:

    Fruit Condition
1  Orange   Good
2  Orange    Bad
3  Orange   Good
4  Orange   Good
5  Orange    Bad
6   Apple   Good
7   Apple    Bad
8   Apple   Good
9   Apple   Good
10  Apple    Bad
11  Apple   Good
12  Apple    Bad
13  Mango   Good
14  Mango   Good
15  Mango    Bad

and I like to remove fruit group(s) with three or more "Bad" pieces. In
this case, I want to remove Apple group. Is there an easy way to count
the "Good" and "Bad" in each group then remove the ones that meet the
criteria? 

Thanks for your help.

Sean



From smalladi at lexgen.com  Wed Mar 10 23:29:37 2004
From: smalladi at lexgen.com (Malladi, Sukhaswami)
Date: Wed, 10 Mar 2004 16:29:37 -0600
Subject: [R] Inserting Date Field into Oracle table using ROracle
Message-ID: <2D42B4DF9845F74E9318DF707D5956D93889CA@wdexch02.lexgen.com>

Hello,

Attached is a mail regarding question how to insert Date field using ROracle

package. I am stuck with this problem and appreciate receiving help from 
gurus on this list.

Code used mainly is:

library(ROracle) ### --- Version 0.53
drv <- dbDriver("Oracle") 
con <- dbConnect( drv, "user/passwd") 
d <- data.frame(CDATE = "2004-03-10 10:12:00")
ps <- dbPrepareStatement(con, 
		"INSERT into DATEST (CDATE) VALUES ( :1 ) ", 
		bind=c( "character"))  ## -- c("date") does not work
sapply(d, class)
d$CDATE <- as.character(d$CDATE)
sapply(d, class)
dbExecStatement(ps,d) 

Error in oraExecStatement(ps, data, ...) : 
        RS-DBI driver: (ORA-01861: literal does not match format string )

Thanks for your help in advance,
Swami
(smalladi at lexgen.com)

----------------------------- Correspondence with David James
-----------------------

Dear David,

Thanks for your kind reply. I did what you suggested, coerced
d into a character vector. Now I get an Oracle error -

d <- data.frame(CDATE = "TO_DATE('2004-03-10 10:12:00','YYYY-MM-DD
HH:MI:SS')")
sapply(d, class)
d$CDATE <- as.character(d$CDATE)
sapply(d, class)
dbExecStatement(ps,d) 
Error in oraExecStatement(ps, data, ...) : 
        RS-DBI driver: (ORA-01858: a non-numeric character was found where a
numeric was expected
-----------------------------
ORA-01858 a non-numeric character was found where a numeric was expected

Cause: The input data to be converted using a date format model was
incorrect. The input data did not contain a number where a number was
required by the format model.

Action: Fix the input data or the date format model to make sure the
elements match in number and type. Then retry the operation
------------------

If I do 
d <- data.frame(CDATE = "2004-03-10 10:12:00")
instead of line 1 above, I get error :
Error in oraExecStatement(ps, data, ...) : 
        RS-DBI driver: (ORA-01861: literal does not match format string )
----------------
Cause: Literals in the input must be the same length as literals in the
format string (with the exception of leading white space). If the "FX"
modifier has been toggled on, the literal must match exactly, with no extra
white space.

Action: Correct the format string to match the literal.
------------

I do not know what I am doing wrongly. I will definitely post the experience
in R-help.

Kindly help,
Thanks
Swami



> -----Original Message-----
> From: David James [mailto:dj at research.bell-labs.com]
> Sent: Wednesday, March 10, 2004 8:15 AM
> To: Malladi, Sukhaswami
> Cc: David James
> Subject: Re: ROracle : insert dates
> 
> 
> Dear Swami,
> 
> One possible cause of your problem is that the dataframe "d"
> that you create may not have the date field "CDATE" as a string,
> but rather as a factor.  If this is the case, then you need to 
> coerce it to be a character vector, e.g.,
>   > d <- data.frame(CDATE = "2004-03-10")
>   d
>   CDATE
>   1 2004-03-10
>   > sapply(d, class)
>      CDATE
>   "factor"
>   > ## coerce CDATE to character
>   > d$CDATE <- as.character(d$CDATE)
>   > sapply(d, class)
>         CDATE
>   "character"
> 
> If this is indeed the problem, could you summary the result and
> post it to r-help so other people may be able to learn from your
> experience?
> 
> Regards,
> 
> --
> David
> 
> 
> Malladi, Sukhaswami wrote:
> > Hi
> > 
> > I am using ROracle for interacting between ORACLE and R. I 
> am able to insert
> > character and numeric data.
> > However, I am unable to insert date into a table despite 
> attempting many
> > methods. The code I used is as follows:
> > 
> > 	library(ROracle) ### --- Version 0.53
> > 	drv <- dbDriver("Oracle") 
> > 	con <- dbConnect( drv, "user/passwd") 
> > 
> > 	d <- data.frame( cbind( CDATE="TO_DATE('02-02-2004
> > 10:12:00','DD-MM-YYYY HH:MI:SS' )" ) )
> > 
> > 	lQry <- "INSERT into DATEST (CDATE) VALUES ( :1 ) "
> > 
> > 	ps <- dbPrepareStatement(con, "INSERT into DATEST 
> (CDATE) VALUES (
> > :1 ) ",
> > 			 bind=c( "character"))  ## --------- c("date")
> > gives error shown below
> > 
> > 	dbExecStatement(ps,d) 
> > 
> > Error in oraExecStatement(ps, data, ...) : 
> >         RS-DBI driver: (unrecognized S class factor )
> > 
> > > ps <- dbPrepareStatement(con, lQry, bind=c("date")) 
> > Error in oraPrepareStatement(conn, statement, bind, ...) : 
> >         RS-DBI driver: (unrecognized S class date )
> > > 
> > 
> > My question is : how do I insert date in the oracle table DATEST ?
> > 
> > SQL> desc DATEST;
> > 
> >  Name                Type
> >  ------------------ -------- 
> >  CDATE               DATE
> > 
> > 
> > platform i686-pc-linux-gnu
> > arch     i686             
> > os       linux-gnu        
> > system   i686, linux-gnu  
> > status                    
> > major    1                
> > minor    8.1              
> > year     2003             
> > month    11               
> > day      21               
> > language R                
> > 
> > I would be grateful for your kind help,
> > 
> > Thanks,
> > Swami
> > 
> > 
> > 
> **************************************************************
> ************* 
> >  The contents of this communication are intended only for 
> the addressee and
> > may contain confidential and/or privileged material. If you 
> are not the
> > intended recipient, please do not read, copy, use or disclose this
> > communication and notify the sender.  Opinions, conclusions 
> and other
> > information in this communication that do not relate to the official
> > business of my company shall be understood as neither given 
> nor endorsed by
> > it.  
> > 
> **************************************************************
> ************* 
> > 
> 


*************************************************************************** 
 The contents of this communication are intended only for th...{{dropped}}



From vograno at evafunds.com  Thu Mar 11 00:09:20 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Wed, 10 Mar 2004 15:09:20 -0800
Subject: [R] dyn.unload fails after some C++ code.
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3AA6@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040310/50af1a12/attachment.pl

From mike.campana at freesurf.ch  Thu Mar 11 00:20:30 2004
From: mike.campana at freesurf.ch (Mike Campana)
Date: Thu, 11 Mar 2004 00:20:30 +0100
Subject: [R] failure in simple calculation!!
Message-ID: <MDBBLPJFFFANKHOFDHBFAEGFCCAA.mike.campana@freesurf.ch>

Hello

I have a dataframe, at least I think I created it by using:

DELTA <- as.data.frame(DELTA)

The dataframe has 2 columns made of numbers("data" and "delta")

If I use   DELTA$delta + 12
I get a sequence of NA  Warning message:
"+" not meaningful for factors in: Ops.factor(DELTA$deltatemp, 12)

What does this mean? Can you explain me please why I can not make this
simple calculation? And what can I do to the calculation?



further information:
mode(DELTA)
[1] "list"

DELTA$delta
.....
[685] 6.04  7.84  8.01  8.43  6.6   6.33  8.42  8.86  10.25 8.2   9.32  9.77
[697] 7.9   6.42  5.95
474 Levels: -0.02 -0.13 -0.22 -0.46 -0.55 -1.26 -1.83 -1.86 -2.45 -3.2 ...
9.9
What does this last row mean?

Thanks a lot for the answer and on general for the help contaned in the
mailing list

Mike



From Wanzare at HCJP.com  Thu Mar 11 01:36:58 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Thu, 11 Mar 2004 09:36:58 +0900
Subject: [R] data frame filtration
Message-ID: <1CBA12F2D414914989C723D196B287DC0555B3@jp-svr-ex1.HCJP.COM>

?table

Assuming that your data frame is named as x.

tbl <- table(x)		 
tbl[tbl[,"Bad"]<3,]

HTH
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Liang
Sent: Thursday, March 11, 2004 7:09 AM
To: R-help at stat.math.ethz.ch
Subject: [R] data frame filtration

Hi, R-help:

I am a new user of R and am very pleased with R's features. Here I have
one question regarding data frame manipulation. I have a data frame look
like this:

    Fruit Condition
1  Orange   Good
2  Orange    Bad
3  Orange   Good
4  Orange   Good
5  Orange    Bad
6   Apple   Good
7   Apple    Bad
8   Apple   Good
9   Apple   Good
10  Apple    Bad
11  Apple   Good
12  Apple    Bad
13  Mango   Good
14  Mango   Good
15  Mango    Bad

and I like to remove fruit group(s) with three or more "Bad" pieces. In
this case, I want to remove Apple group. Is there an easy way to count
the "Good" and "Bad" in each group then remove the ones that meet the
criteria? 

Thanks for your help.

Sean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From k.wang at auckland.ac.nz  Thu Mar 11 01:37:34 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 11 Mar 2004 13:37:34 +1300
Subject: [R] failure in simple calculation!!
References: <MDBBLPJFFFANKHOFDHBFAEGFCCAA.mike.campana@freesurf.ch>
Message-ID: <003e01c40701$0b7f9990$302ed882@cs.auckland.ac.nz>

Hi,

----- Original Message ----- 
From: "Mike Campana" <mike.campana at freesurf.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 11, 2004 12:20 PM
Subject: [R] failure in simple calculation!!


> Hello
>
> I have a dataframe, at least I think I created it by using:
>
> DELTA <- as.data.frame(DELTA)
>
> The dataframe has 2 columns made of numbers("data" and "delta")
>
> If I use   DELTA$delta + 12
> I get a sequence of NA  Warning message:
> "+" not meaningful for factors in: Ops.factor(DELTA$deltatemp, 12)
>
> What does this mean? Can you explain me please why I can not make this
> simple calculation? And what can I do to the calculation?
>
> further information:
> mode(DELTA)
> [1] "list"
>
> DELTA$delta
> .....
> [685] 6.04  7.84  8.01  8.43  6.6   6.33  8.42  8.86  10.25 8.2   9.32
9.77
> [697] 7.9   6.42  5.95
> 474 Levels: -0.02 -0.13 -0.22 -0.46 -0.55 -1.26 -1.83 -1.86 -2.45 -3.2 ...
> 9.9
> What does this last row mean?


Several things.

1) DELTA is not a data frame here.  It is a list: mode(DELTA)

2) DELTA$delta is not a vector.  It is a factor.  For some reason when you
generated DELTA, your delta has become a factor.  Arithematic operations on
factors are meaningless.

HTH.

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
Statistics Workshops Co-ordinator
Student Learning Centre
University of Auckland
New Zealand



From Meredith.Briggs at team.telstra.com  Thu Mar 11 01:43:20 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Thu, 11 Mar 2004 11:43:20 +1100
Subject: [R] Identifying Multicollinearity in sparse datasets
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35346@ntmsg0092.corpmail.telstra.com.au>

Hello

Is there any package or method to identify multicollinearity in data of the sort below?
thanks.
Meredith

 <<DATA2.txt>> 





-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: DATA2.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040311/28add610/DATA2.txt

From k.wang at auckland.ac.nz  Thu Mar 11 01:59:55 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 11 Mar 2004 13:59:55 +1300
Subject: [R] Identifying Multicollinearity in sparse datasets
References: <3B5823541A25D311B3B90008C7F9056410E35346@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <007801c40704$2b2d28e0$302ed882@cs.auckland.ac.nz>

Hi,

Take a look at the car package, there is a function called vif.

HTH,

Kevin

----- Original Message ----- 
From: "Briggs, Meredith M" <Meredith.Briggs at team.telstra.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, March 11, 2004 1:43 PM
Subject: [R] Identifying Multicollinearity in sparse datasets


Hello

Is there any package or method to identify multicollinearity in data of the
sort below?
thanks.
Meredith

 <<DATA2.txt>>








----------------------------------------------------------------------------
----


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Mar 11 02:41:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 10 Mar 2004 20:41:04 -0500 (EST)
Subject: [R] do.call and environments
Message-ID: <20040311014104.63D8D3947@mprdmxin.myway.com>


Tony,

I played with your solution some more and found
that it can be reduced to:

fx <- function(y) print(x*y)
f <- function(fun, x) {
        environment(fun) <- environment()
        fun(3)
}
f(fx,2)

---

Date:   Wed, 10 Mar 2004 10:56:28 -0700 
From:   Tony Plate <tplate at blackmesacapital.com>
To:   <petzoldt at rcs.urz.tu-dresden.de>,R-Help <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] do.call and environments 

 
The reason in your example that fx() doesn't find 'x' is that the lexical 
scope of fx() does not include 'x'. So, this is what must be fixed, in one 
way or another.

One simple way to make your example work is to define fx() in a place where 
its lexical scope includes the variables you want it to see:

> f <- function(fun, xx) {
+ x <- xx
+ fxx <- function() do.call(fun, list(y=3))
+ fx <- function(y) print(x*y)
+ fxx()
+ }
> x # verify that x is not a global variable!
Error: Object "x" not found
> f("fx", 13)
[1] 39
>

Another way, that is probably more suited to what you want (since it sounds 
like you don't have control over where fx() is defined), is to make a local 
copy of fx(), and change its environment:

> fx <- function(y) print(x*y)
>
> f <- function(fun, xx) {
+ x <- xx
+ fxx <- function() do.call(fun, list(y=3))
+ fx <- fx
+ environment(fx) <- environment()
+ fxx()
+ }
> x # verify that x is not a global variable!
Error: Object "x" not found
> f("fx", 13)
[1] 39
>

I suspect there is also some way to do this using eval() and its envir= 
and/or enclos= arguments, but I couldn't get anything like this to work.

The above solutions were generated by manual genetic programming (i.e., 
trial and error), so they may not be particularly good or 
elegant. However, I'm sure that others will point out both any problems 
with these suggestions and some better solutions!

hope this helps,

Tony Plate

At Wednesday 09:05 AM 3/10/2004, Thomas Petzoldt wrote:
>Hello,
>
>I want to call a function "fx" given by name, where some "global" 
>variables (in the environment of fx) are passed to the function. For 
>compatibility reasons I cannot modify the parameter list of fx and I want 
>to avoid setting variables in the global environment (e.g. via <<-)
>
>Is there a way, how to do this?
>
>Thomas P.
>
>The example:
>
>fx <- function(y) print(x*y)
>
>f <- function(fun, xx) {
> fxx <- function() {do.call(fun, list(y=3))}
> x <- x
> fxx()
>}
>
>f("fx", 13)
>
>## does not work, because fx does not find x
>



From ggrothendieck at myway.com  Thu Mar 11 02:53:06 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 10 Mar 2004 20:53:06 -0500 (EST)
Subject: [R] data frame filtration
Message-ID: <20040311015306.37B023971@mprdmxin.myway.com>



Suppose the data frame is called fruit:

do.call("rbind", 
 by(fruit, fruit$Fruit, function(x)if (sum(x$Condition=="Bad")<3) x))

by splits the data frame into groups and then applies the 
indicated function to each group.  If the condition is met then
that group is returned; otherwise, NULL is returned since there
is no else leg to the if.  rbind then binds the groups back into
a data frame.

---
Date:   Wed, 10 Mar 2004 17:09:23 -0500 
From:   Sean Liang <SLiang at wyeth.com>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] data frame filtration 

 
Hi, R-help:

I am a new user of R and am very pleased with R's features. Here I have
one question regarding data frame manipulation. I have a data frame look
like this:

Fruit Condition
1 Orange Good
2 Orange Bad
3 Orange Good
4 Orange Good
5 Orange Bad
6 Apple Good
7 Apple Bad
8 Apple Good
9 Apple Good
10 Apple Bad
11 Apple Good
12 Apple Bad
13 Mango Good
14 Mango Good
15 Mango Bad

and I like to remove fruit group(s) with three or more "Bad" pieces. In
this case, I want to remove Apple group. Is there an easy way to count
the "Good" and "Bad" in each group then remove the ones that meet the
criteria? 

Thanks for your help.

Sean



From Meredith.Briggs at team.telstra.com  Thu Mar 11 05:08:15 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Thu, 11 Mar 2004 15:08:15 +1100
Subject: [R] Function OPTIM()  
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35347@ntmsg0092.corpmail.telstra.com.au>

Hello

I'm trying to reformat my problem below so there is less data entry each time the package is run.

This is the 'inefficient' version and below in blue is what I would like it to look like but can't get it to work.

InDATA<- read.table ("C:/Data/March 2004/DATA2.txt",header=T)

WO=dim(InDATA)[1]
DI=dim(InDATA)[2]-1


B <- matrix(rep(0,WO*DI), c(WO,DI))
j=1
while (j  <DI+1) {B[,j]=InDATA[,j] 
j=j+1
next}
print(B)

y <- matrix(c(InDATA[,DI+1]),c(WO,1))
fr <- function(X) {

for (j  in 1:DI) { j <- X[j]}


p <- matrix(c(X[1],X[2],X[3],X[4],X[5],X[6],X[7],X[8],X[9],X[10],X[11],X[12],X[13],X[14],X[15],X[16],X[17],X[18],X[19],X[20],X[21],X[22],X[23],X[24],X[25],X[26],X[27],X[28],X[29],X[30]),c(30,1))

t(B %*% p-y) %*% (B %*% p-y)

}


optim(c(rep(1,DI)),fr,method="L-BFGS-B",lower=c(488,716,327,77,40,85,1,4,21,4,462,256,366,4697,654,373,664,100,601,1641,700,516,769,617,5372,8079,432,62,4155,2954), upper=c(1540,859,382,95,50,95,10,5,26,6,538,309,443,5310,757,453,757,424,741,1971,1425,600,1191,720,5373,9329,570,75,5559,6351))


InDATA<- read.table ("C:/Data/March 2004/DATA2.txt",header=T)

WO=dim(InDATA)[1]
DI=dim(InDATA)[2]-1


B <- matrix(rep(0,WO*DI), c(WO,DI))
j=1
while (j  <DI+1) {B[,j]=InDATA[,j] 
j=j+1
next}
print(B)

y <- matrix(c(InDATA[,DI+1]),c(WO,1))
fr <- function(X) {

for (j  in 1:DI) { j <- X[j]}


p <- matrix(cbind(InDATA[1,1:DI]))

t(B %*% p-y) %*% (B %*% p-y)

}

optim(c(rep(1,DI)),fr,method="L-BFGS-B",Lower,Upper)



From ajayshah at mayin.org  Thu Mar 11 02:44:21 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 11 Mar 2004 07:14:21 +0530
Subject: [R] Difficulties in interaction between R and latex (prosper)
Message-ID: <20040311014421.GA10239@igidr.ac.in>

Hello, folks! I'm trying to use R as a graphics program, to make some
pretty graphs that will go into prosper slideshows.

I wrote this fragment, from the R manual, into a file demo.R:

   x=seq(-3,3,0.1)
   postscript("cm_test.eps", width = 4.0, height = 3.0,
               horizontal = FALSE, onefile = FALSE, paper = "special",
               family = "ComputerModern")
   plot(x, sin(x), type="l")

I fed this into a simplest-possible tex file, named sl_demo.tex, which
uses prosper:

   \documentclass[pdf,serpaggi,slideColor,colorBG]{prosper}
   \usepackage[latin1]{inputenc}
   \usepackage{graphicx}
   \begin{document}
     \begin{slide}{Demo}
       \includegraphics[width=\linewidth]{cm_test.eps}
     \end{slide}
     \begin{slide}{This one works}
       \includegraphics[width=\linewidth]{thisworks.eps}
     \end{slide}
   \end{document}

The file cm_test.eps is produced using R. I left a file
"thisworks.eps" there as a counterpoint (it was made using jgraph and
it works fine).

The resulting sl_demo.pdf is attached. It's supposed to be a
slideshow. Under Adobe acrobat, when I say Ctrl-L it must go
fullscreen. That works correctly for thisworks.eps but not for the
eps file that's made using R.

Any ideas what I'm doing wrong? How do I get the graph made using R
to sit horizantally (i.e. landscape), and fill the screen? I tried to
say "horizontal=T" and that doesn't work.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sl_demo.pdf
Type: application/pdf
Size: 18435 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040311/edfd56bf/sl_demo.pdf
-------------- next part --------------
A non-text attachment was scrubbed...
Name: thisworks.eps
Type: application/postscript
Size: 10672 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040311/edfd56bf/thisworks.eps
-------------- next part --------------
x=seq(-3,3,0.1)

# Example from the manual, obtained when we say ?postscript
postscript("cm_test.eps", width = 4.0, height = 3.0,
            horizontal = FALSE, onefile = FALSE, paper = "special",
            family = "ComputerModern")
plot(x, sin(x), type="l")

From rsong at cs.wisc.edu  Thu Mar 11 05:42:45 2004
From: rsong at cs.wisc.edu (Rui Song)
Date: Wed, 10 Mar 2004 22:42:45 -0600 (CST)
Subject: [R] About reading data into R
In-Reply-To: <20040309065222.961163971@mprdmxin.myway.com>
References: <20040309065222.961163971@mprdmxin.myway.com>
Message-ID: <Pine.LNX.4.58.0403102239570.6484@matrice.stat.wisc.edu>

Thanks for your reply, I chose to use scan here. So can I ask another
question, which function works faster, scan or read.table?

Thanks,

Rui

On Tue, 9 Mar 2004, Gabor Grothendieck wrote:

>
>
> scan has no problem with blank lines.  read.table has
> an argument that controls how it handles blank lines
> and the default setting is to ignore them.
>
>
> ---
> Date:   Mon, 8 Mar 2004 20:27:48 -0600 (CST)
> From:   Rui Song <rsong at cs.wisc.edu>
> To:   <R-help at R-project.org>
> Subject:   [R] About reading data into R
>
>
> I have a problem about reading data into R. There is a "\n"
> between each pair of data, like:
>
> -155.65
>
> -155.77
>
> -155.40
>
> -155.46
>
> -155.52
>
> -155.34
>
>      ...
> Could anyone tell me how to read in such data? Thanks!
>
> Rui
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Thu Mar 11 06:15:35 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 11 Mar 2004 00:15:35 -0500 (EST)
Subject: [R] About reading data into R
Message-ID: <20040311051535.BF21C3A04@mprdmxin.myway.com>


read.table uses scan.  Issue the command

   read.table

to see the R source code.

You can compare timings with your data set like this:

   gc(); system.time( z1 <- read.table(myfile) )
   gc(); system.time( z2 <- scan(myfile) )

(where you may need to fix up the function calls above
to reflect any additional arguments needed to read your 
dataset correctly).  

---
Date:   Wed, 10 Mar 2004 22:42:45 -0600 (CST) 
From:   Rui Song <rsong at cs.wisc.edu>
To:   Gabor Grothendieck <ggrothendieck at myway.com> 
Cc:   <rsong at cs.wisc.edu>, <R-help at R-project.org> 
Subject:   RE: [R] About reading data into R 

 
Thanks for your reply, I chose to use scan here. So can I ask another
question, which function works faster, scan or read.table?

Thanks,

Rui

On Tue, 9 Mar 2004, Gabor Grothendieck wrote:

>
>
> scan has no problem with blank lines. read.table has
> an argument that controls how it handles blank lines
> and the default setting is to ignore them.
>
>
> ---
> Date: Mon, 8 Mar 2004 20:27:48 -0600 (CST)
> From: Rui Song <rsong at cs.wisc.edu>
> To: <R-help at R-project.org>
> Subject: [R] About reading data into R
>
>
> I have a problem about reading data into R. There is a "\n"
> between each pair of data, like:
>
> -155.65
>
> -155.77
>
> -155.40
>
> -155.46
>
> -155.52
>
> -155.34
>
> ...
> Could anyone tell me how to read in such data? Thanks!
>
> Rui
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Mar 11 07:45:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Mar 2004 06:45:24 +0000 (GMT)
Subject: [R] failure in simple calculation!!
In-Reply-To: <003e01c40701$0b7f9990$302ed882@cs.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0403110641410.1977-100000@gannet.stats>

On Thu, 11 Mar 2004, Ko-Kang Kevin Wang wrote:

> Hi,
> 
> ----- Original Message ----- 
> From: "Mike Campana" <mike.campana at freesurf.ch>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, March 11, 2004 12:20 PM
> Subject: [R] failure in simple calculation!!
> 
> 
> > Hello
> >
> > I have a dataframe, at least I think I created it by using:
> >
> > DELTA <- as.data.frame(DELTA)
> >
> > The dataframe has 2 columns made of numbers("data" and "delta")
> >
> > If I use   DELTA$delta + 12
> > I get a sequence of NA  Warning message:
> > "+" not meaningful for factors in: Ops.factor(DELTA$deltatemp, 12)
> >
> > What does this mean? Can you explain me please why I can not make this
> > simple calculation? And what can I do to the calculation?
> >
> > further information:
> > mode(DELTA)
> > [1] "list"
> >
> > DELTA$delta
> > .....
> > [685] 6.04  7.84  8.01  8.43  6.6   6.33  8.42  8.86  10.25 8.2   9.32
> 9.77
> > [697] 7.9   6.42  5.95
> > 474 Levels: -0.02 -0.13 -0.22 -0.46 -0.55 -1.26 -1.83 -1.86 -2.45 -3.2 ...
> > 9.9
> > What does this last row mean?
> 
> 
> Several things.
> 
> 1) DELTA is not a data frame here.  It is a list: mode(DELTA)

A data frame is a list (almost all classed objects are), so the mode is
reported as a list.  Asking the class() would have been more revealing.

> 2) DELTA$delta is not a vector.  It is a factor.  For some reason when you
> generated DELTA, your delta has become a factor.  Arithematic operations on
> factors are meaningless.

as.data.frame will coerce character columns to factors.  The fix is 
probably

DELTA$delta <- as.numeric(as.character(DELTA$delta))

see the FAQ, Q7.12.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Mar 11 08:00:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Mar 2004 07:00:04 +0000 (GMT)
Subject: [R] About reading data into R
In-Reply-To: <Pine.LNX.4.58.0403102239570.6484@matrice.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0403110656220.1977-100000@gannet.stats>

On Wed, 10 Mar 2004, Rui Song wrote:

> Thanks for your reply, I chose to use scan here. So can I ask another
> question, which function works faster, scan or read.table?

read.table calls scan, and the two functions do different jobs.

read.table reads in a data frame.
scan reads a vector or list

Since neither can do exactly what the other does, neither can be faster.

For further discussion, including on speed, please read ?read.table and
the `R Data Import/Export' manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petzoldt at rcs.urz.tu-dresden.de  Thu Mar 11 08:22:45 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 11 Mar 2004 08:22:45 +0100
Subject: [R] Summary: do.call and environments
In-Reply-To: <20040311014104.63D8D3947@mprdmxin.myway.com>
References: <20040311014104.63D8D3947@mprdmxin.myway.com>
Message-ID: <405013C5.50300@rcs.urz.tu-dresden.de>

Dear R users,

thank you very much for your suggestions. I've learned much, especially
that the problem was not as simple as I thought. There have been several
proposals, but most of them solved the problem only partly.

The proposal(s) of Gabor and Tony (different versions) seemed to be very 
promising:

> fx <- function(y) print(x*y)
> f <- function(fun, x) {
>         environment(fun) <- environment()
>         fun(3)
> }
> f(fx,2)

... but unfortunately they miss the "do.call" mechanism. I intended to 
call a function given as character with f("fx", value), and not f(fx, 
value).

Another proposal (from Robert Gentleman, thank you) works. It has the 
only disadvantage, that I have to set an environment outside of my own
function. If I understand this correctly, this means that the new
environment ist set persistently (*globally*) and may have side-effects 
to other calls:

> fx <- function(y) print(x*y)
>     environment(fx) <- new.env()
>
> ff <- function(fun, x) {
>     assign("x", x, environment(get(fun, mode="function")))
>     do.call(fun, list(y=3))
> }

Furthermore, Andy pointed out that my idea was a bad one, but why I am 
trying such weired things? My problem is, that I want to define list 
objects which contain informations, about how they are processed (as 
character, not as copy of a function). This solver function (e.g. 
"lsoda" from the odesolve package) then calls a third function (my model 
equations), provided from my side again, but unfortunately does not 
"pass through" some additonal argument(s), needed by the model 
equations. So I wanted to call lsoda or my own function provided with 
the additional arguments within an own environment.

Now, as it comes out, that this was really a bad idea, I should focus on 
the pass-trough method (...) and use one of the 99% workarounds 
mentioned above in the meantime.

Thank you again

Thomas P.



From knoblauch at lyon.inserm.fr  Thu Mar 11 08:44:19 2004
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Thu, 11 Mar 2004 08:44:19 +0100
Subject: [R] Difficulties in interaction between R and latex (prosper)
Message-ID: <1078991059.405018d39fb5b@webmail.lyon.inserm.fr>

I suspect that this is not really an R question but one for the
graphicx package of LaTeX.  Did you try the angle= argument
for the includegraphics command? eg,

\includegraphics[width=\linewidth,angle=90]{cm_test.eps}


Quoting Ajay Shah <ajayshah at mayin.org>:

> Hello, folks! I'm trying to use R as a graphics program, to make some
> pretty graphs that will go into prosper slideshows.
> 
> I wrote this fragment, from the R manual, into a file demo.R:
> 
>    x=seq(-3,3,0.1)
>    postscript("cm_test.eps", width = 4.0, height = 3.0,
>                horizontal = FALSE, onefile = FALSE, paper = "special",
>                family = "ComputerModern")
>    plot(x, sin(x), type="l")
> 
> I fed this into a simplest-possible tex file, named sl_demo.tex, which
> uses prosper:
> 
>    \documentclass[pdf,serpaggi,slideColor,colorBG]{prosper}
>    \usepackage[latin1]{inputenc}
>    \usepackage{graphicx}
>    \begin{document}
>      \begin{slide}{Demo}
>        \includegraphics[width=\linewidth]{cm_test.eps}
>      \end{slide}
>      \begin{slide}{This one works}
>        \includegraphics[width=\linewidth]{thisworks.eps}
>      \end{slide}
>    \end{document}
> 
> The file cm_test.eps is produced using R. I left a file
> "thisworks.eps" there as a counterpoint (it was made using jgraph and
> it works fine).
> 
> The resulting sl_demo.pdf is attached. It's supposed to be a
> slideshow. Under Adobe acrobat, when I say Ctrl-L it must go
> fullscreen. That works correctly for thisworks.eps but not for the
> eps file that's made using R.
> 
> Any ideas what I'm doing wrong? How do I get the graph made using R
> to sit horizantally (i.e. landscape), and fill the screen? I tried to
> say "horizontal=T" and that doesn't work.
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 



____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 92 34 61
portable: 06 84 10 64 10



From ggrothendieck at myway.com  Thu Mar 11 08:57:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 11 Mar 2004 02:57:57 -0500 (EST)
Subject: [R] Summary: do.call and environments
Message-ID: <20040311075757.906A039AC@mprdmxin.myway.com>


To use the modify the solution from Tony and I 
so that you can pass the name of the function, rather
than the function itself, like this:

x <- 7
fx <- function(y) print(x*y)
f <- function(fun, x) {
	 fun <- get(fun)
         environment(fun) <- environment()
         do.call("fun",list(3))
}
f("fx",2)


---
Date:   Thu, 11 Mar 2004 08:22:45 +0100 
From:   Thomas Petzoldt <petzoldt at rcs.urz.tu-dresden.de>
Cc:   <petzoldt at rcs.urz.tu-dresden.de>, <r-help at stat.math.ethz.ch> 
Subject:   [R] Summary: do.call and environments 

 
Dear R users,

thank you very much for your suggestions. I've learned much, especially
that the problem was not as simple as I thought. There have been several
proposals, but most of them solved the problem only partly.

The proposal(s) of Gabor and Tony (different versions) seemed to be very 
promising:

> fx <- function(y) print(x*y)
> f <- function(fun, x) {
> environment(fun) <- environment()
> fun(3)
> }
> f(fx,2)

... but unfortunately they miss the "do.call" mechanism. I intended to 
call a function given as character with f("fx", value), and not f(fx, 
value).

Another proposal (from Robert Gentleman, thank you) works. It has the 
only disadvantage, that I have to set an environment outside of my own
function. If I understand this correctly, this means that the new
environment ist set persistently (*globally*) and may have side-effects 
to other calls:

> fx <- function(y) print(x*y)
> environment(fx) <- new.env()
>
> ff <- function(fun, x) {
> assign("x", x, environment(get(fun, mode="function")))
> do.call(fun, list(y=3))
> }

Furthermore, Andy pointed out that my idea was a bad one, but why I am 
trying such weired things? My problem is, that I want to define list 
objects which contain informations, about how they are processed (as 
character, not as copy of a function). This solver function (e.g. 
"lsoda" from the odesolve package) then calls a third function (my model 
equations), provided from my side again, but unfortunately does not 
"pass through" some additonal argument(s), needed by the model 
equations. So I wanted to call lsoda or my own function provided with 
the additional arguments within an own environment.

Now, as it comes out, that this was really a bad idea, I should focus on 
the pass-trough method (...) and use one of the 99% workarounds 
mentioned above in the meantime.

Thank you again

Thomas P.



From spencer.graves at pdf.com  Thu Mar 11 09:28:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 00:28:35 -0800
Subject: [R] Summary: do.call and environments
In-Reply-To: <20040311075757.906A039AC@mprdmxin.myway.com>
References: <20040311075757.906A039AC@mprdmxin.myway.com>
Message-ID: <40502333.1050006@pdf.com>

      That looks great, but I'm confused.  In R 1.8.1 under Windows 
2000, the suggested script produced for me the following:

 > f("fx",2)
[1] 6

      I would have naively expected 14.  From whence cometh "6"? 

      Also, I prefer to use transportable code wherever feasible.  The 
same script generated the following response from S-Plus 6.2: 

 > f("fx", 2)
Problem in f("fx", 2): Couldn't find a function definition for 
"environment"
Use traceback() to see the call stack

      Thanks,
      Spencer Graves

Gabor Grothendieck wrote:

>To use the modify the solution from Tony and I 
>so that you can pass the name of the function, rather
>than the function itself, like this:
>
>x <- 7
>fx <- function(y) print(x*y)
>f <- function(fun, x) {
>	 fun <- get(fun)
>         environment(fun) <- environment()
>         do.call("fun",list(3))
>}
>f("fx",2)
>
>
>---
>Date:   Thu, 11 Mar 2004 08:22:45 +0100 
>From:   Thomas Petzoldt <petzoldt at rcs.urz.tu-dresden.de>
>Cc:   <petzoldt at rcs.urz.tu-dresden.de>, <r-help at stat.math.ethz.ch> 
>Subject:   [R] Summary: do.call and environments 
>
> 
>Dear R users,
>
>thank you very much for your suggestions. I've learned much, especially
>that the problem was not as simple as I thought. There have been several
>proposals, but most of them solved the problem only partly.
>
>The proposal(s) of Gabor and Tony (different versions) seemed to be very 
>promising:
>
>  
>
>>fx <- function(y) print(x*y)
>>f <- function(fun, x) {
>>environment(fun) <- environment()
>>fun(3)
>>}
>>f(fx,2)
>>    
>>
>
>... but unfortunately they miss the "do.call" mechanism. I intended to 
>call a function given as character with f("fx", value), and not f(fx, 
>value).
>
>Another proposal (from Robert Gentleman, thank you) works. It has the 
>only disadvantage, that I have to set an environment outside of my own
>function. If I understand this correctly, this means that the new
>environment ist set persistently (*globally*) and may have side-effects 
>to other calls:
>
>  
>
>>fx <- function(y) print(x*y)
>>environment(fx) <- new.env()
>>
>>ff <- function(fun, x) {
>>assign("x", x, environment(get(fun, mode="function")))
>>do.call(fun, list(y=3))
>>}
>>    
>>
>
>Furthermore, Andy pointed out that my idea was a bad one, but why I am 
>trying such weired things? My problem is, that I want to define list 
>objects which contain informations, about how they are processed (as 
>character, not as copy of a function). This solver function (e.g. 
>"lsoda" from the odesolve package) then calls a third function (my model 
>equations), provided from my side again, but unfortunately does not 
>"pass through" some additonal argument(s), needed by the model 
>equations. So I wanted to call lsoda or my own function provided with 
>the additional arguments within an own environment.
>
>Now, as it comes out, that this was really a bad idea, I should focus on 
>the pass-trough method (...) and use one of the 99% workarounds 
>mentioned above in the meantime.
>
>Thank you again
>
>Thomas P.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From petzoldt at rcs.urz.tu-dresden.de  Thu Mar 11 09:45:32 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 11 Mar 2004 09:45:32 +0100
Subject: [R] Summary: do.call and environments
In-Reply-To: <40502333.1050006@pdf.com>
References: <20040311075757.906A039AC@mprdmxin.myway.com>
	<40502333.1050006@pdf.com>
Message-ID: <4050272C.9070902@rcs.urz.tu-dresden.de>

Hello,

>  > f("fx",2)
> [1] 6
> 
>      I would have naively expected 14.  From whence cometh "6"?
>      Also, I prefer to use transportable code wherever feasible.  The 

2*3=6, which was the intention. It is in fact only a proof of 
correctness, that "7" is not used here. The proposal of Gabor does 
exactly, what I want, but if it does not work on S-PLUS, it's a serious 
disadvantage and I should check this too.

Thomas P.



From ggrothendieck at myway.com  Thu Mar 11 10:03:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 11 Mar 2004 04:03:23 -0500 (EST)
Subject: [R] Summary: do.call and environments
Message-ID: <20040311090323.E88EC39AC@mprdmxin.myway.com>



Note that R and S are fundamentally different when it comes to
scoping.  

R uses lexical scoping, i.e. the parent environment of a function
is the environment at the point where it is *defined* whereas
S uses dynamic scoping, i.e. the parent environment in a function 
is the environment at the point where the function is *called*.

Thus, anything regarding scoping will be different in the 
two systems.

Note that in S, the question is easy since all you need is:

x <- 7
fx <- function(y) print(x*y)
f <- function(fx, x) do.call(fx,list(3))
f("fx",2)

This gives 6 in S but 21 in R.  (Better check this since I
don't have access to S and am going by my understanding.)

In fact, this entire exercise can be regarded as simulating
S-style dynamic scoping in R.

---
Date:   Thu, 11 Mar 2004 09:45:32 +0100 
From:   Thomas Petzoldt <petzoldt at rcs.urz.tu-dresden.de>
To:   Spencer Graves <spencer.graves at pdf.com> 
Cc:   <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Summary: do.call and environments 

 
Hello,

> > f("fx",2)
> [1] 6
> 
> I would have naively expected 14. From whence cometh "6"?
> Also, I prefer to use transportable code wherever feasible. The 

2*3=6, which was the intention. It is in fact only a proof of 
correctness, that "7" is not used here. The proposal of Gabor does 
exactly, what I want, but if it does not work on S-PLUS, it's a serious 
disadvantage and I should check this too.

Thomas P.



From spencer.graves at pdf.com  Thu Mar 11 10:21:18 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 01:21:18 -0800
Subject: [R] Summary: do.call and environments
In-Reply-To: <20040311090323.E88EC39AC@mprdmxin.myway.com>
References: <20040311090323.E88EC39AC@mprdmxin.myway.com>
Message-ID: <40502F8E.9020903@pdf.com>

      I got the following from S-Plus 6.2: 

 > x <- 7
 > fx <- function(y)
print(x * y)
 > f <- function(fx, x)
do.call(fx, list(3))
 > f("fx", 2)
[1] 21
[1] 21

      This is the same as R 1.8.1 except for that S-Plus printed the 
result in addition to returning it. 

      Thanks,
      Spencer Graves

Gabor Grothendieck wrote:

>Note that R and S are fundamentally different when it comes to
>scoping.  
>
>R uses lexical scoping, i.e. the parent environment of a function
>is the environment at the point where it is *defined* whereas
>S uses dynamic scoping, i.e. the parent environment in a function 
>is the environment at the point where the function is *called*.
>
>Thus, anything regarding scoping will be different in the 
>two systems.
>
>Note that in S, the question is easy since all you need is:
>
>x <- 7
>fx <- function(y) print(x*y)
>f <- function(fx, x) do.call(fx,list(3))
>f("fx",2)
>
>This gives 6 in S but 21 in R.  (Better check this since I
>don't have access to S and am going by my understanding.)
>
>In fact, this entire exercise can be regarded as simulating
>S-style dynamic scoping in R.
>
>---
>Date:   Thu, 11 Mar 2004 09:45:32 +0100 
>From:   Thomas Petzoldt <petzoldt at rcs.urz.tu-dresden.de>
>To:   Spencer Graves <spencer.graves at pdf.com> 
>Cc:   <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch> 
>Subject:   Re: [R] Summary: do.call and environments 
>
> 
>Hello,
>
>  
>
>>>f("fx",2)
>>>      
>>>
>>[1] 6
>>
>>I would have naively expected 14. From whence cometh "6"?
>>Also, I prefer to use transportable code wherever feasible. The 
>>    
>>
>
>2*3=6, which was the intention. It is in fact only a proof of 
>correctness, that "7" is not used here. The proposal of Gabor does 
>exactly, what I want, but if it does not work on S-PLUS, it's a serious 
>disadvantage and I should check this too.
>
>Thomas P.
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com
>  
>



From spencer.graves at pdf.com  Thu Mar 11 10:22:38 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 01:22:38 -0800
Subject: [R] Summary: do.call and environments
In-Reply-To: <4050272C.9070902@rcs.urz.tu-dresden.de>
References: <20040311075757.906A039AC@mprdmxin.myway.com>
	<40502333.1050006@pdf.com> <4050272C.9070902@rcs.urz.tu-dresden.de>
Message-ID: <40502FDE.3070600@pdf.com>

ff1 <- function(fun, x) {
  assign(fun, get(fun), 1)
  assign("x", x, 1)
  do.call(fun, list(y=3))
}
tst <- ff1("fx", 2)

      This assigns "6" to "tst" in both S-Plus 6.2 as R 1.8.1.  However, 
it also prints "[1] 6" in S-Plus 6.2 but not R.  ???

      Best Wishes,
      Spencer Graves

Thomas Petzoldt wrote:

> Hello,
>
>>  > f("fx",2)
>> [1] 6
>>
>>      I would have naively expected 14.  From whence cometh "6"?
>>      Also, I prefer to use transportable code wherever feasible.  The 
>
>
> 2*3=6, which was the intention. It is in fact only a proof of 
> correctness, that "7" is not used here. The proposal of Gabor does 
> exactly, what I want, but if it does not work on S-PLUS, it's a 
> serious disadvantage and I should check this too.
>
> Thomas P.



From spencer.graves at pdf.com  Thu Mar 11 10:23:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 01:23:54 -0800
Subject: [R] Map of British Colonial America 1775
In-Reply-To: <200403101932.i2AJWLL0028333@tahi.mcs.vuw.ac.nz>
References: <200403101932.i2AJWLL0028333@tahi.mcs.vuw.ac.nz>
Message-ID: <4050302A.7020607@pdf.com>

      Thanks for the reply.  I still didn't understand after reading the 
documentation, but the following examples resolved the questions for me: 

library(mapdata)
tst <- map("worldHires", "UK:Bermuda")

      This produced a 57-point polygon from R 1.8.1 (vs. the 83 point 
polygon from S-Plus 6.2 mentioned previously).  To get evn more 
resolution, I tried the following: 

tst0 <- map("worldHires", "UK:Bermuda", res=0)

      This produced a 91-point polygon from R 1.8.1, which matches what 
I got from "tst0 <- map("worldHires", "UK:Bermuda", res=0)" in S-Plus. 

      Thanks again,
      Spencer Graves

Ray Brownrigg wrote:

>>      I've reviewed the "map" documentation I've found so far with 
>>S-Plus 6.2 and R including "www.r-project.org" -> search -> "R site 
>>search" -> "world map" and Becker & Wilkes (1993) "Maps in S" cited in 
>>the "map" documentation.  I've noticed that, for example, 
>>'library(maps);  map("world", "UK:Bermuda")' produces an 83-point 
>>polygon sketch in S-Plus 6.2 but only a degenerate 3-point polygon (with 
>>the third point = the first) in R 1.8.1. 
>>
>>    
>>
>You need the mapdata package.  The default "world" database in S-Plus is
>the high-resolution one (with also "world.thin").  In R the high
>resolution one is "worldHires" from mapdata (mainly so that the
>standard maps package is a manageable size).
>
>Ray Brownrigg
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From duchesnay at shfj.cea.fr  Thu Mar 11 12:08:00 2004
From: duchesnay at shfj.cea.fr (Edouard DUCHESNAY)
Date: Thu, 11 Mar 2004 12:08:00 +0100
Subject: [R] No traceback available when using try(...)
Message-ID: <200403111208.00088.duchesnay@shfj.cea.fr>

Hello,

1. The Situation :
------------------------
The stack traceback is not available when error ouccured in a try(....)
-- test.R --------------------------------
f<-function(a){
  return ( log(a) )
}
try(f("A"))
traceback()
-------------------------------------------

I get the following message :
> try(f("A"))
Error in log(x) : Non-numeric argument to mathematical function
> traceback()
No traceback available

I try to use tryCatch(...) instead, I could get the stack traceback (using the finally), 
but the R program stop.

2. My goal
--------------------
All I need is to prevent a error to stop a program (like try() does), print the traceback 
just after the try(....), and get the error message (geterrmessage()).


3. Question
-------------------
A) stack traceback is not available when error ouccured in a try is a normal behavior ?
B) Do you have any sugestion to fill my goal ?
C) If not is it a good idea to investigate around withRestarts(expr, ...) ?


Best regards


PS : My R Version 1.8.1  (2003-11-21)
-- 
Edouard Duchesnay                      Tel: +33 1 69 86 78 52
CEA - SHFJ                             Fax: +33 1 69 86 77 86
4, place du G?n?ral Leclerc
91401 Orsay Cedex France



From fzoellne at TechFak.Uni-Bielefeld.DE  Thu Mar 11 12:19:10 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Thu, 11 Mar 2004 12:19:10 +0100
Subject: [R] Changingvalues in data frame
Message-ID: <20040311111910.GA19588@hindemith.TechFak.Uni-Bielefeld.DE>

Hi!

I have a question concerning data frames and changing particular values in it.

I have set up a data frame containing n rows of observations od dimension m, so in each row there is a vector of size m. Now I want to introduce a cut off to tha data. Therfore I caluclated the mean and variance within each column using apply:

me<-apply(daten[1:72],2,mean)
st<-apply(daten[1:72],2,var)	

Now I want use the mean and the var to form a cut off value, and apply it to each row of the data frame.

I tried the following

cutoff<-function(x){
      if(x>me+2*st){
        x<-me+2*st
      }
    }

cutd<-apply(daten[1:72],1,cutoff)

but I have to supply somehow the column index to me and st.
Any Ideas ?
Thanks,
Frank
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From sam.kemp2 at ntlworld.com  Thu Mar 11 12:24:14 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Thu, 11 Mar 2004 11:24:14 +0000
Subject: [R] non-uniform time series embeddings
Message-ID: <40504C5E.80505@ntlworld.com>

Hi,

Does anyone know of a function (or a method) of generating simulated 
time series with non-uniform embeddings?

Any help will be greatly appreciated.

Cheers,

Sam.



From mros at toulouse.inra.fr  Thu Mar 11 12:24:36 2004
From: mros at toulouse.inra.fr (Mathieu Ros)
Date: Thu, 11 Mar 2004 12:24:36 +0100
Subject: [R] =?iso-8859-1?q?Une_introduction_=E0_R/An_introduction_to_R?=
Message-ID: <AD79D68D-734E-11D8-8DAA-000A95C5B248@toulouse.inra.fr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Dear all,
it's a long time now since I started to translate "an introduction to 
R" in french and the document was sleeping in a quite hidden place on 
the web. These times I had an increasing demand from users around the 
(french speaking) world who were asking me what was going on.
The translation is about half the way, but considering my present 
duties (a phD thesis to write, among other things), I'm afraid to take 
3 more years of part-time work to finish it.
Thus I wonder if some english to french translators would be interested 
in giving me a hand, we could even create a "R french translation team" 
(cf the "statistiques avec R" thread).
Comments welcome...
best,

- --Mathieu Ros
Ph D student, Cell Genetics unit
National Institute of Agronomical Research
Chemin de borde rouge
31326 Castanet-Tolosan, France
tel : 0(+033)5 61 28 53 05
fax : 0(+033)5 61 28 53 08
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.4 (Darwin)

iD8DBQFAUEx14aMFr2geGWARAsLoAJ9BWzDhlhfgiIpcAuD2YHo+C36xewCfcztv
hnOEVpNtbKD7KdIMJ3UWHds=
=2zWU
-----END PGP SIGNATURE-----



From jonathan.williams at pharmacology.oxford.ac.uk  Thu Mar 11 13:13:56 2004
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Thu, 11 Mar 2004 12:13:56 -0000
Subject: [R] making operators act on rows of a data frame
Message-ID: <NGBBKJEMOMLJFCOIEGCEOEOJJKAA.jonathan.williams@pharm.ox.ac.uk>

Dear R helpers,
I wish to use the "sum" operator for each row of a data frame.
However, it appears that the operator acts on the entire data
frame, over all columns. What is the best way to obtain row-
wise operation?

The following code shows my attempts so far, and their problems:-

test1=array(rbinom(120,1,0.5),c(20,3))
test1[,3]=NA
sum(test1[,1:2])
test1[,3][sum(test1[,1:2])>=2]=1
test1[,3][sum(test1[,1:2])]
test1

test2=array(rbinom(120,1,0.5),c(20,3))
test2[,3]=NA
sum(test2[,1:2])
test2[,3][(test2[,1]+test2[,2])>=2]=1
test2[,3][sum(test2[,1:2])]
test2

In the 1st section, I try to use "sum" to add the first two columns 
of a data frame. Here, sum(test1[,1:2]) evaluates to a single integer
but this modifies *all* the rows of test1.

In the 2nd section, specifying addition of the first two columns with
a '+' acts row-by row, as I want. This is OK for this demonstration,
but would be impractical in the program I am trying to write (where
the columns I wish to sum are numerous and change from time to time).

I would be very grateful to know if it is possible to get operators
to act on rows of a data frame, and if so, how.

I am running R1.8.1 on Windows NT.

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From thpe at hhbio.wasser.tu-dresden.de  Thu Mar 11 13:05:21 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 11 Mar 2004 13:05:21 +0100
Subject: [R] Changingvalues in data frame
In-Reply-To: <20040311111910.GA19588@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040311111910.GA19588@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <40505601.3050008@hhbio.wasser.tu-dresden.de>

Hello Frank,

does the following example, what you want?

Thomas P.

## some test data
x <- data.frame(matrix(rnorm(72*10, mean=50, sd=20), ncol=10))

me <- colMeans(x)
sd <- apply(x, 2, sd)

coff <- me + 2*sd

# see ?t and ?pmax
x2 <- t(pmin(t(x),coff))

# test it
x-x2



From petzoldt at rcs.urz.tu-dresden.de  Thu Mar 11 13:07:05 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 11 Mar 2004 13:07:05 +0100
Subject: [R] Changingvalues in data frame
In-Reply-To: <20040311111910.GA19588@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040311111910.GA19588@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <40505669.2020904@rcs.urz.tu-dresden.de>

Hello Frank,

does the following example, what you want?

Thomas P.

## some test data
x <- data.frame(matrix(rnorm(72*10, mean=50, sd=20), ncol=10))

me <- colMeans(x)
sd <- apply(x, 2, sd)

coff <- me + 2*sd

# see ?t and ?pmax
x2 <- t(pmin(t(x),coff))

# test it
x-x2



From s-plus at wiwi.uni-bielefeld.de  Thu Mar 11 13:10:42 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 11 Mar 2004 13:10:42 +0100
Subject: [R] Changingvalues in data frame
References: <20040311111910.GA19588@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <40505742.1020307@wiwi.uni-bielefeld.de>

Frank Gerrit Zoellner wrote:

>Hi!
>
>I have a question concerning data frames and changing particular values in it.
>
>I have set up a data frame containing n rows of observations od dimension m, so in each row there is a vector of size m. Now I want to introduce a cut off to tha data. Therfore I caluclated the mean and variance within each column using apply:
>
>me<-apply(daten[1:72],2,mean)
>st<-apply(daten[1:72],2,var)	
>
>Now I want use the mean and the var to form a cut off value, and apply it to each row of the data frame.
>
>I tried the following
>
>cutoff<-function(x){
>      if(x>me+2*st){
>        x<-me+2*st
>      }
>    }
>
>cutd<-apply(daten[1:72],1,cutoff)
>
>but I have to supply somehow the column index to me and st.
>Any Ideas ?
>Thanks,
>Frank
>  
>
what about:

x<-as.data.frame(matrix(rnorm(100),20,5))
apply(x,2,function(x){limit<-mean(x)+1*sqrt(var(x)); 
ifelse(x>limit,1000*limit,x)})

or

apply(x,2,function(x){limit<-mean(x)+2*sqrt(var(x)); 
ifelse(x>limit,limit,x)})



Peter



From martinsrui at prof2000.pt  Thu Mar 11 13:28:16 2004
From: martinsrui at prof2000.pt (Rui  Martins )
Date: Thu, 11 Mar 2004 12:28:16 +0000
Subject: [R] Adaptive kernel estimation
Message-ID: <200403111228.AA177340704@mail.prof2000.pt>


My name is Rui Martins and I would like to know if there exist some routines in R code to perform the adaptive kernel density estimation.


Thank you


Rui



From pvremort at vub.ac.be  Thu Mar 11 14:31:45 2004
From: pvremort at vub.ac.be (Piet van Remortel)
Date: Thu, 11 Mar 2004 14:31:45 +0100
Subject: [R] tics and grids
Message-ID: <opr4o867u6gzo14j@mach.vub.ac.be>

Hi,

Whats the easiest way to set a desired interval for tics on axes in R ?
And will the 'grid' command put gridlines on all tics automatically ?

I tend to get stuck with graphs with ranges 0-1 with only 2 tics and 2 
gridlines in the 0-1 range, while I would like 10 tics (or every 0.1)

tnx,


Piet
please reply to pvremortNOSPAM at vub.ac.be



From petr.pikal at precheza.cz  Thu Mar 11 14:52:29 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 11 Mar 2004 14:52:29 +0100
Subject: [R] making operators act on rows of a data frame
In-Reply-To: <NGBBKJEMOMLJFCOIEGCEOEOJJKAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <40507D2D.4255.17D17D0@localhost>


   Hi

   On 11 Mar 2004 at 12:13, Jonathan Williams wrote:

   > Dear R helpers,

   > I wish to use the "sum" operator for each row of a data frame.

   > However, it appears that the operator acts on the entire data

   > frame, over all columns. What is the best way to obtain row-

   > wise operation?

   >

   > The following code shows my attempts so far, and their problems:-

   >

   > test1=array(rbinom(120,1,0.5),c(20,3))

   > test1[,3]=NA

   > sum(test1[,1:2])

   Try rowSums()

   rowSums(test1, na.rm=T)

   > test1[,3][sum(test1[,1:2])>=2]=1

   > test1[,3][sum(test1[,1:2])]

   > test1

   >

   > test2=array(rbinom(120,1,0.5),c(20,3))

   > test2[,3]=NA

   This can be done by

   test2[rowSums(test2, na.rm=T)>=2,3]<-1

   Cheers

   Petr

   > sum(test2[,1:2])

   > test2[,3][(test2[,1]+test2[,2])>=2]=1

   > test2[,3][sum(test2[,1:2])]

   > test2

   >

   > In the 1st section, I try to use "sum" to add the first two columns
   of

   > a data frame. Here, sum(test1[,1:2]) evaluates to a single integer
   but

   > this modifies *all* the rows of test1.

   >

   > In the 2nd section, specifying addition of the first two columns
   with

   > a '+' acts row-by row, as I want. This is OK for this demonstration,

   > but would be impractical in the program I am trying to write (where

   > the columns I wish to sum are numerous and change from time to
   time).

   >

   > I would be very grateful to know if it is possible to get operators
   to

   > act on rows of a data frame, and if so, how.

   >

   > I am running R1.8.1 on Windows NT.

   >

   > Jonathan Williams

   > OPTIMA

   > Radcliffe Infirmary

   > Woodstock Road

   > OXFORD OX2 6HE

   > Tel +1865 (2)24356

   >

   > ______________________________________________

   > R-help at stat.math.ethz.ch mailing list

   > https://www.stat.math.ethz.ch/mailman/listinfo/r-help

   > PLEASE do read the posting guide!

   > http://www.R-project.org/posting-guide.html

   Petr Pikal

   petr.pikal at precheza.cz


From petr.pikal at precheza.cz  Thu Mar 11 14:56:33 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 11 Mar 2004 14:56:33 +0100
Subject: [R] tics and grids
In-Reply-To: <opr4o867u6gzo14j@mach.vub.ac.be>
Message-ID: <40507E21.26492.180D2B5@localhost>

Hallo

On 11 Mar 2004 at 14:31, Piet van Remortel wrote:

> Hi,
> 
> Whats the easiest way to set a desired interval for tics on axes in R
> ? And will the 'grid' command put gridlines on all tics automatically
> ?

Probably making graph without axes and specify your own axes 
with

axis()

see
?axis
?grid

Cheers
Petr

> 
> I tend to get stuck with graphs with ranges 0-1 with only 2 tics and 2
> gridlines in the 0-1 range, while I would like 10 tics (or every 0.1)
> 
> tnx,
> 
> 
> Piet
> please reply to pvremortNOSPAM at vub.ac.be
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From andy_liaw at merck.com  Thu Mar 11 15:00:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Mar 2004 09:00:53 -0500
Subject: [R] Summary: do.call and environments
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7979@usrymx25.merck.com>

Gabor,

> From: Gabor Grothendieck
> 
> Note that R and S are fundamentally different when it comes to
> scoping.  
> 
> R uses lexical scoping, i.e. the parent environment of a function
> is the environment at the point where it is *defined* whereas
> S uses dynamic scoping, i.e. the parent environment in a function 
> is the environment at the point where the function is *called*.

I don't think that's quite right.  S does not use dynamic scope.  This
simple example fails in S-PLUS (6.1):

> g <- function() x + y
> f <- function() {
+   x <- 2
+   y <- 3
+   g()
+ }
> f()
Problem in g(): Object "y" not found 
Use traceback() to see the call stack

It would have worked if S has dynamic scope: variables defined in the
caller's (f in this case) frame is visible to the function being called (g
in this case).  The S scoping rule is described in the S-PLUS programmer's
guide, as well as V&R's `S Programming' (which also describes R's lexical
scope).

And of course, the example doesn't work in R, either:

> g <- function() x + y
> f <- function() {
+   x <- 3
+   y <- 2
+   g()
+ }
> f()
Error in g() : Object "x" not found

Cheers,
Andy
 
> Thus, anything regarding scoping will be different in the 
> two systems.
> 
> Note that in S, the question is easy since all you need is:
> 
> x <- 7
> fx <- function(y) print(x*y)
> f <- function(fx, x) do.call(fx,list(3))
> f("fx",2)
> 
> This gives 6 in S but 21 in R.  (Better check this since I
> don't have access to S and am going by my understanding.)
> 
> In fact, this entire exercise can be regarded as simulating
> S-style dynamic scoping in R.
> 
> ---
> Date:   Thu, 11 Mar 2004 09:45:32 +0100 
> From:   Thomas Petzoldt <petzoldt at rcs.urz.tu-dresden.de>
> To:   Spencer Graves <spencer.graves at pdf.com> 
> Cc:   <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch> 
> Subject:   Re: [R] Summary: do.call and environments 
> 
>  
> Hello,
> 
> > > f("fx",2)
> > [1] 6
> > 
> > I would have naively expected 14. From whence cometh "6"?
> > Also, I prefer to use transportable code wherever feasible. The 
> 
> 2*3=6, which was the intention. It is in fact only a proof of 
> correctness, that "7" is not used here. The proposal of Gabor does 
> exactly, what I want, but if it does not work on S-PLUS, it's 
> a serious 
> disadvantage and I should check this too.
> 
> Thomas P.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Thu Mar 11 15:02:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Mar 2004 09:02:22 -0500
Subject: [R] Adaptive kernel estimation
Message-ID: <3A822319EB35174CA3714066D590DCD504AF797A@usrymx25.merck.com>

At least packages `lokern' and `locfit' can do this.

HTH,
Andy

> From: Rui Martins 
> 
> My name is Rui Martins and I would like to know if there 
> exist some routines in R code to perform the adaptive kernel 
> density estimation.
> 
> 
> Thank you
> 
> 
> Rui


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Simon.Fear at synequanon.com  Thu Mar 11 15:06:32 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Thu, 11 Mar 2004 14:06:32 -0000
Subject: [R] making operators act on rows of a data frame
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02218@synequanon01>

The generic solution is apply(myArray, 1, myfun)
- see ?apply (while you're there see ?lapply and ?sapply
as well!).

For you particular case, you can use rowSums().

There is a distinction between `array`, `matrix` and
`data.frame` - these are not interchangeable terms.

As it happens, both apply() and rowSums() will coerce
the first argument to an array if possible - thus they
might work on a data.frame. But be very sure
your data.frame does not contain any factors or
character values.

> -----Original Message-----
> From: Jonathan Williams
> [mailto:jonathan.williams at pharmacology.oxford.ac.uk]
> Sent: 11 March 2004 12:14
> To: Ethz. Ch
> Subject: [R] making operators act on rows of a data frame
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Dear R helpers,
> I wish to use the "sum" operator for each row of a data frame.
> However, it appears that the operator acts on the entire data
> frame, over all columns. What is the best way to obtain row-
> wise operation?
> 
> The following code shows my attempts so far, and their problems:-
> 
> test1=array(rbinom(120,1,0.5),c(20,3))
> test1[,3]=NA
> sum(test1[,1:2])
> test1[,3][sum(test1[,1:2])>=2]=1
> test1[,3][sum(test1[,1:2])]
> test1
> 
> test2=array(rbinom(120,1,0.5),c(20,3))
> test2[,3]=NA
> sum(test2[,1:2])
> test2[,3][(test2[,1]+test2[,2])>=2]=1
> test2[,3][sum(test2[,1:2])]
> test2
> 
> In the 1st section, I try to use "sum" to add the first two columns 
> of a data frame. Here, sum(test1[,1:2]) evaluates to a single integer
> but this modifies *all* the rows of test1.
> 
> In the 2nd section, specifying addition of the first two columns with
> a '+' acts row-by row, as I want. This is OK for this demonstration,
> but would be impractical in the program I am trying to write (where
> the columns I wish to sum are numerous and change from time to time).
> 
> I would be very grateful to know if it is possible to get operators
> to act on rows of a data frame, and if so, how.
> 
> I am running R1.8.1 on Windows NT.
> 
> Jonathan Williams
> OPTIMA
> Radcliffe Infirmary
> Woodstock Road
> OXFORD OX2 6HE
> Tel +1865 (2)24356
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From Matthias.Schmidt at forst.bwl.de  Thu Mar 11 15:15:07 2004
From: Matthias.Schmidt at forst.bwl.de (Schmidt.Matthias (FORST))
Date: Thu, 11 Mar 2004 15:15:07 +0100
Subject: [R] query with parameters using RODBC
Message-ID: <855D381F618DE84FA58C035BA803FCE6B9A6BD@FVAFR-SE1.FORST.BWL.DE>

Dear all,
I am using RODBC to conduct Queries in a MSACCESS data base. This works
quite well, but is it also possible to conduct Queries including parameters
which can be specified flexibly if the Query is used for example in a R
function. Something like the function .parambyname in Borland Delphi for
example.

thanx for help

Matthias

***********************************************************
Matthias Schmidt
Forstliche Versuchs- und Forschungsanstalt Baden-W?rttemberg (FVA) 
Abteilung Biometrie und Informatik
Wonnhaldestr. 4
79100 Freiburg i. Br
Tel.: + 49 (0)761 / 4018 -187
Fax: + 49 (0)761 / 4018 - 333



From YanZhang at paloma.com  Thu Mar 11 15:34:35 2004
From: YanZhang at paloma.com (Yan Zhang)
Date: Thu, 11 Mar 2004 09:34:35 -0500
Subject: [R] widen the screen place
Message-ID: <48027968DDD36F40B70C261D6FE78837029F8E@titan3.paloma.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040311/39553316/attachment.pl

From fzoellne at TechFak.Uni-Bielefeld.DE  Thu Mar 11 16:04:27 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Thu, 11 Mar 2004 16:04:27 +0100
Subject: [R] fft question
Message-ID: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>

Hi!

I am using the fft() function the base package to transform some 1d signal. 
If I use this standar fucntion I get a very huge first fourier coeficient.
I think this dues to the handling of the borders of the signal.
Usually in fft especially in image processing the signal is simulated to be continuous by adding the signal several times periodically. My question is, is there some function implemented in R handling this or do I have to combine my data manually ?

Thanks, 
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From shayna.strom at new.oxford.ac.uk  Thu Mar 11 16:05:46 2004
From: shayna.strom at new.oxford.ac.uk (Shayna Strom)
Date: Thu, 11 Mar 2004 15:05:46 +0000 (GMT)
Subject: [R] recoding variables
Message-ID: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040311/0c74fcb7/attachment.pl

From sundar.dorai-raj at pdf.com  Thu Mar 11 16:15:48 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 11 Mar 2004 09:15:48 -0600
Subject: [R] recoding variables
In-Reply-To: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
References: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
Message-ID: <405082A4.2070507@pdf.com>



Shayna Strom wrote:

> Hi,
> 
> I was hoping someone could help me.  I am a graduate student new to using R, and I'm trying to figure out how to recode a continuous variable to make it into an ordinal variable with 3 categories.  I literally have no idea how to proceed--could anyone possibly advise me?  Please copy me on any responses, as I have just subscribed to the R-help email list but don't know whether the subscription has gone through yet.
> 
> Thanks so much-
> Shayna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

See ?ordered.

x <- rep(runif(3), 10)
xord <- ordered(x)



From sundar.dorai-raj at pdf.com  Thu Mar 11 16:18:27 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 11 Mar 2004 09:18:27 -0600
Subject: [R] recoding variables
In-Reply-To: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
References: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
Message-ID: <40508343.7010507@pdf.com>



Shayna Strom wrote:

> Hi,
> 
> I was hoping someone could help me.  I am a graduate student new to using R, and I'm trying to figure out how to recode a continuous variable to make it into an ordinal variable with 3 categories.  I literally have no idea how to proceed--could anyone possibly advise me?  Please copy me on any responses, as I have just subscribed to the R-help email list but don't know whether the subscription has gone through yet.
> 
> Thanks so much-
> Shayna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Sorry sent too quickly. Probably what you want is ?cut too.

x <- runif(30)
xcut <- cut(x, c(0, 1/3, 2/3, 1))
xord <- ordered(xcut)



From rpeng at jhsph.edu  Thu Mar 11 16:19:39 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 11 Mar 2004 10:19:39 -0500
Subject: [R] recoding variables
In-Reply-To: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
References: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
Message-ID: <4050838B.8050002@jhsph.edu>

Basically, you want to use cut().  For example,

x <- 1:20
cut(x, c(0, 8, 15, 20))

This divides x into a factor with levels (0,8], (8,15], and (15,20].

-roger

Shayna Strom wrote:
> Hi,
> 
> I was hoping someone could help me.  I am a graduate student new to using R, and I'm trying to figure out how to recode a continuous variable to make it into an ordinal variable with 3 categories.  I literally have no idea how to proceed--could anyone possibly advise me?  Please copy me on any responses, as I have just subscribed to the R-help email list but don't know whether the subscription has gone through yet.
> 
> Thanks so much-
> Shayna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Thu Mar 11 16:26:42 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 07:26:42 -0800
Subject: [R] recoding variables
In-Reply-To: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
References: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
Message-ID: <40508532.7000005@pdf.com>

(tst <- cut(1:11, c(-Inf, 3, 5, Inf)))
 [1] (-Inf,3] (-Inf,3] (-Inf,3] (3,5]    (3,5]    (5,Inf]  (5,Inf]  (5,Inf]
 [9] (5,Inf]  (5,Inf]  (5,Inf]
Levels: (-Inf,3] (3,5] (5,Inf]
 >
 > class(tst)
[1] "factor"

      hope this helps.  spencer graves

Shayna Strom wrote:

>Hi,
>
>I was hoping someone could help me.  I am a graduate student new to using R, and I'm trying to figure out how to recode a continuous variable to make it into an ordinal variable with 3 categories.  I literally have no idea how to proceed--could anyone possibly advise me?  Please copy me on any responses, as I have just subscribed to the R-help email list but don't know whether the subscription has gone through yet.
>
>Thanks so much-
>Shayna
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From chrysopa at insecta.ufv.br  Thu Mar 11 16:18:35 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Thu, 11 Mar 2004 12:18:35 -0300
Subject: [R] Questions about spatial data
Message-ID: <200403111218.35905.chrysopa@insecta.ufv.br>

Hi,

I have two questions about spatial data.

1) I have this grid

x <- rep(c(1:8),c(rep(6,8)))
y <- rep(c(1:6),8)

I need to make some others grids with same length and different numbers of 
points whit a random spatial pattern.

How is the best method to make this in R.

2) I need to produce some grids with the same numbers of points in different 
degrees of aggregation, the level of aggregation for all grids must be 
statistically different.

How to do these grids in R and how to test the significance between these 
grids?

Thanks for all
Ronaldo
-- 
FLASH!
Intelligence of mankind decreasing.
Details at ... uh, when the little hand is on the ....
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From tblackw at umich.edu  Thu Mar 11 16:26:26 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Thu, 11 Mar 2004 10:26:26 -0500 (EST)
Subject: [R] making operators act on rows of a data frame
In-Reply-To: <NGBBKJEMOMLJFCOIEGCEOEOJJKAA.jonathan.williams@pharm.ox.ac.uk>
References: <NGBBKJEMOMLJFCOIEGCEOEOJJKAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <Pine.SOL.4.58.0403111019460.5345@millipede.gpcc.itd.umich.edu>

Jonathan  -

Petr Pikal suggests  RowSums()  for your specific task.

A more general solution is given by  apply()  and  sapply().
These functions will apply any R function (even one that you
would write yourself) to individual members of any dimension
of an array, matrix or data frame, or any combination of
dimensions (in the context of a three or four-dimensional
[or more] array).  See  help("apply"), help("sapply").

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 11 Mar 2004, Jonathan Williams wrote:

> Dear R helpers,
> I wish to use the "sum" operator for each row of a data frame.
> However, it appears that the operator acts on the entire data
> frame, over all columns. What is the best way to obtain row-
> wise operation?
>
> The following code shows my attempts so far, and their problems:-
>
> test1=array(rbinom(120,1,0.5),c(20,3))
> test1[,3]=NA
> sum(test1[,1:2])
> test1[,3][sum(test1[,1:2])>=2]=1
> test1[,3][sum(test1[,1:2])]
> test1
>
> test2=array(rbinom(120,1,0.5),c(20,3))
> test2[,3]=NA
> sum(test2[,1:2])
> test2[,3][(test2[,1]+test2[,2])>=2]=1
> test2[,3][sum(test2[,1:2])]
> test2
>
> In the 1st section, I try to use "sum" to add the first two columns
> of a data frame. Here, sum(test1[,1:2]) evaluates to a single integer
> but this modifies *all* the rows of test1.
>
> In the 2nd section, specifying addition of the first two columns with
> a '+' acts row-by row, as I want. This is OK for this demonstration,
> but would be impractical in the program I am trying to write (where
> the columns I wish to sum are numerous and change from time to time).
>
> I would be very grateful to know if it is possible to get operators
> to act on rows of a data frame, and if so, how.
>
> I am running R1.8.1 on Windows NT.
>
> Jonathan Williams
> OPTIMA
> Radcliffe Infirmary
> Woodstock Road
> OXFORD OX2 6HE
> Tel +1865 (2)24356
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.pagel at gsf.de  Thu Mar 11 16:27:32 2004
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 11 Mar 2004 16:27:32 +0100
Subject: [R] recoding variables
In-Reply-To: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
References: <E1B1Rko-00059W-00@wing1.herald.ox.ac.uk>
Message-ID: <20040311152732.GA11030@porcupine.gsf.de>


	Hi!

> using R, and I'm trying to figure out how to recode a continuous
> variable to make it into an ordinal variable with 3 categories.

You are looking for cut():

> a <- runif(15)

> a
 [1] 0.19109987 0.78808597 0.78458256 0.31355035 0.02076274 0.82287287 0.75260382 0.82627690 0.14775167
[10] 0.59427620 0.12314764 0.44151537 0.05123785 0.88879744 0.25552054

> b = cut(a, breaks=c(0, 0.33, 0.66, 1), labels=c('low', 'medium', 'high'))

> b
 [1] low    high   high   low    low    high   high   high   low medium low    medium low    high   low
Levels: low medium high

cu
	Philipp


-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/~pagel



From tlumley at u.washington.edu  Thu Mar 11 16:47:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Mar 2004 07:47:07 -0800 (PST)
Subject: [R] Summary: do.call and environments
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7979@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7979@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0403110733020.60588@homer10.u.washington.edu>

On Thu, 11 Mar 2004, Liaw, Andy wrote:

> Gabor,
>
> > From: Gabor Grothendieck
> >
> > Note that R and S are fundamentally different when it comes to
> > scoping.
> >
> > R uses lexical scoping, i.e. the parent environment of a function
> > is the environment at the point where it is *defined* whereas
> > S uses dynamic scoping, i.e. the parent environment in a function
> > is the environment at the point where the function is *called*.
>
> I don't think that's quite right.  S does not use dynamic scope.  This
> simple example fails in S-PLUS (6.1):

I'm not sure if there is standard jargon for the scoping rules that S
uses, but it certainly isn't dynamic scope.  The scope of a name does
not change: it's always the local frame plus the global workspace, so we
have static scope.

One way to describe the difference between S and R scoping is that in S
environment(fun) would always be the global workspace (frame 0), which is
why the environment() function isn't needed in S.

As a final note, it is "obvious" that dynamic scope is useful and easy to
implement, so people often try to fake it. Lisp developers also used to
think that dynamic scope was more useful and easier than dynamic scope.
They don't now.


	-thomas



From m_nica at hotmail.com  Thu Mar 11 16:49:49 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Thu, 11 Mar 2004 09:49:49 -0600
Subject: [R] saving a data.frame to "\t" files
Message-ID: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040311/15de890e/attachment.pl

From tplate at blackmesacapital.com  Thu Mar 11 17:03:51 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 11 Mar 2004 09:03:51 -0700
Subject: [R] do.call and environments
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF796B@usrymx25.merck.co
 m>
References: <3A822319EB35174CA3714066D590DCD504AF796B@usrymx25.merck.com>
Message-ID: <6.0.3.0.2.20040310214343.049dbd78@mailhost.blackmesacapital.com>

Andy, I think you're correct that Thomas Petzoldt wants dynamic scoping, but
isn't your argument here about strong typing vs weak typing?  To 
paraphrase, one
could say: For example, if weak typing were allowed I might write a 
function `g' that
passes argument `x' to fx() as a character, or a data frame, or a list, or 
an `lm' object,
or a connection, ....  How would you write fx() to deal with that nightmare 
if you have
weak typing? (Actually, the S language does have weak typing, and some 
functions
deal with the "nightmare" by checking the types of their arguments.  Other 
functions
just pass their arguments along without type checking, resulting in cryptic 
error
messages).

I think the issues around lexical vs dynamic scope are more to do with dynamic
scope making programming accidents more likely, because of the risk that
a variable in a higher frame is masked unintentionally by another variable of
the same name in an intervening frame, and more difficult to track down,
because the error can depend on the exact calling sequence when it occurred.

cheers,

Tony Plate

At Wednesday 11:03 AM 3/10/2004, Liaw, Andy wrote:
>Seems to me what you want is dynamic scoping: `x' is not defined in `fx'.
>You want `x' to be found in the scope of the function(s) that calls `fx',
>rather than the environment where `fx' is defined.  I was told (thanks,
>Robert!) that that is a very bad idea:  as the author of `fx', you want some
>assurance of what `x' might be.  This is done via R's lexical scope.  With
>dynamic scope, there is absolutely no way to do that.  For example, I might
>write a function `g' that define `x' as a character, or a data frame, or a
>list, or an `lm' object, or a connection, ....  How would you write `fx' to
>deal with that nightmare if you have dynamic scope?
>
>Andy
>
> > From: Thomas Petzoldt
> >
> > Hello,
> >
> > I want to call a function "fx" given by name, where some "global"
> > variables (in the environment of fx) are passed to the function. For
> > compatibility reasons I cannot modify the parameter list of fx and I
> > want to avoid setting variables in the global environment
> > (e.g. via <<-)
> >
> > Is there a way, how to do this?
> >
> > Thomas P.
> >
> > The example:
> >
> > fx <- function(y) print(x*y)
> >
> > f <- function(fun, xx) {
> >    fxx <- function() {do.call(fun, list(y=3))}
> >    x <- x
> >    fxx()
> > }
> >
> > f("fx", 13)
> >
> > ## does not work, because fx does not find x
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments,...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andrewr at uidaho.edu  Thu Mar 11 17:44:39 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 11 Mar 2004 08:44:39 -0800
Subject: [R] widen the screen place
In-Reply-To: <48027968DDD36F40B70C261D6FE78837029F8E@titan3.paloma.com>
References: <48027968DDD36F40B70C261D6FE78837029F8E@titan3.paloma.com>
Message-ID: <200403110844.39563.andrewr@uidaho.edu>

Try 

?options

Andrew

On Thursday 11 March 2004 06:34, Yan Zhang wrote:
> I have this problem: my screen seems not wide enough in R. My true
> computer monitor is very wide, so I wish that I can line up the text
> when I stretch it wide. i.e.:
>
> This is typical R returns:
> > summary(a)
>
>  V1            V2                 V3             V4
>  F:625   Min.   :20020103   EMC    :  34   Min.   : 9300300
>  L:944   1st Qu.:20020530   BRW    :  27   1st Qu.: 9323865
>  N:615   Median :20020807   Q      :  23   Median : 9355230
>          Mean   :20020761   DSS    :  22   Mean   : 9404488
>          3rd Qu.:20021015   LU     :  20   3rd Qu.: 9410738
>          Max.   :20021231   SLR    :  20   Max.   :13191609
>                             (Other):2038
>        V5                 V6              V7                V8
>  Min.   : 9300600   Min.   : 0.00   Min.   :0.03000   Min.   :  0.62
>  1st Qu.: 9401610   1st Qu.: 2.00   1st Qu.:0.03500   1st Qu.:  9.00
>  Median : 9534786   Median :11.00   Median :0.04500   Median : 17.50
>  Mean   : 9818852   Mean   :24.36   Mean   :0.06215   Mean   : 21.01
>  3rd Qu.:10313592   3rd Qu.:59.00   3rd Qu.:0.06500   3rd Qu.: 28.32
>  Max.   :13205647   Max.   :60.00   Max.   :0.57400   Max.   :121.20
>
>        V9                V10                V11              V12
>  Min.   :  202200   Min.   :     100   Min.   :  0.61   Min.   :  50
>  1st Qu.:  748350   1st Qu.:   38775   1st Qu.:  9.10   1st Qu.:2000
>  Median : 1742450   Median :   98750   Median : 17.62   Median :2000
>  Mean   : 4159455   Mean   :  267184   Mean   : 21.07   Mean   :1901
>  3rd Qu.: 4556200   3rd Qu.:  241250   3rd Qu.: 28.48   3rd Qu.:2000
>  Max.   :90155800   Max.   :12505300   Max.   :120.60   Max.   :2000
>
>       V13
>  Min.   :-3750.0
>  1st Qu.: -300.0
>  Median : -100.0
>  Mean   :  140.2
>  3rd Qu.:  460.0
>  Max.   : 8700.0
>
> But I want to see:
> > summary(a)
>
> V1            V2                 V3             V4                  V5
> V6              V7                V8
>  F:625   Min.   :20020103   EMC    :  34   Min.   : 9300300    Min.   :
> 9300600   Min.   : 0.00   Min.   :0.03000   Min.   :  0.62
>  L:944   1st Qu.:20020530   BRW    :  27   1st Qu.: 9323865    1st Qu.:
> 9401610   1st Qu.: 2.00   1st Qu.:0.03500   1st Qu.:  9.00
>  N:615   Median :20020807   Q      :  23   Median : 9355230    Median :
> 9534786   Median :11.00   Median :0.04500   Median : 17.50
>          Mean   :20020761   DSS    :  22   Mean   : 9404488    Mean   :
> 9818852   Mean   :24.36   Mean   :0.06215   Mean   : 21.01
>          3rd Qu.:20021015   LU     :  20   3rd Qu.: 9410738    3rd
> Qu.:10313592   3rd Qu.:59.00   3rd Qu.:0.06500   3rd Qu.: 28.32
>          Max.   :20021231   SLR    :  20   Max.   :13191609    Max.
>
> :13205647   Max.   :60.00   Max.   :0.57400   Max.   :121.20
>
>                             (Other):2038
>
> They all lines in unix not on my email, here.
>
> I hope you understand what I mean.
>
> Thanks very much.
>
> Yan
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From maechler at stat.math.ethz.ch  Thu Mar 11 18:14:02 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 11 Mar 2004 18:14:02 +0100
Subject: [R] fft question
In-Reply-To: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <16464.40538.560785.903625@gargle.gargle.HOWL>

>>>>> "Frank" == Frank Gerrit Zoellner <fzoellne at TechFak.Uni-Bielefeld.DE>
>>>>>     on Thu, 11 Mar 2004 16:04:27 +0100 writes:

    Frank> Hi!  I am using the fft() function the base package
    Frank> to transform some 1d signal.  If I use this standar
    Frank> fucntion I get a very huge first fourier coeficient.
    Frank> I think this dues to the handling of the borders of
    Frank> the signal.  Usually in fft especially in image
    Frank> processing the signal is simulated to be continuous
    Frank> by adding the signal several times periodically. My
    Frank> question is, is there some function implemented in R
    Frank> handling this or do I have to combine my data
    Frank> manually ?

Yes, manually, like

  fx <- fft(rep(x, 4))

when you want to quadruple your signal.
Martin



From maechler at stat.math.ethz.ch  Thu Mar 11 18:26:50 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 11 Mar 2004 18:26:50 +0100
Subject: [R] R CMD check errors
In-Reply-To: <Pine.LNX.4.44.0403101712400.1319-100000@spock.vulcan>
References: <Pine.LNX.4.44.0403101712400.1319-100000@spock.vulcan>
Message-ID: <16464.41306.384076.770887@gargle.gargle.HOWL>

>>>>> "Thomas" == Thomas Stabla <statho3 at web.de>
>>>>>     on Wed, 10 Mar 2004 17:15:50 +0100 (CET) writes:

    Thomas> Hello,

    Thomas> I'm getting some error messages from R CMD check I can't deal with.
    Thomas> I'm working under Linux with R 1.8.1
    Thomas> The package working directory can be found at:

    Thomas> http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/distr.tar.gz


    Thomas> Here's the 00check.log produced by R CMD check distr

I got it and I can confirm to get warnings / errors along these
lines, also with today's  "R-1.9.0 alpha"


    Thomas> * using log directory '/home/tom/studium/R/swp/swp/package/distr.Rcheck'
    Thomas> * checking for file 'distr/DESCRIPTION' ... OK
    Thomas> * checking if this is a source package ... OK
    Thomas> * checking package directory ... OK
    Thomas> * checking for portable file names ... OK
    Thomas> * checking for sufficient/correct file permissions ... OK
    Thomas> * checking DESCRIPTION meta-information ... OK
    Thomas> * checking index information ... OK
    Thomas> * checking package subdirectories ... WARNING
    Thomas> Subdirectory 'data' contains no data sets.
    Thomas> Subdirectory 'src' contains no source files.

(so just delete these two directories for now; but that doesn't
 help for the real problem below !)

    Thomas> * checking R files for syntax errors ... OK
    Thomas> * checking R files for library.dynam ... OK
    Thomas> * checking S3 generic/method consistency ... WARNING
    Thomas> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
    Thomas> package/namespace load failed
    Thomas> Execution halted
    Thomas> * checking for replacement functions with final arg not named 'value' ... WARNING
    Thomas> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
    Thomas> package/namespace load failed
    Thomas> Execution halted
    Thomas> * checking Rd files ... OK
    Thomas> * checking for missing documentation entries ... ERROR
    Thomas> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :


Yes, unfortunately, you don't get better errors here.

     [[ -> feature request ;-) ]]

But since "R CMD check" *does* install the package , you just
load/attach it manually 
(using 'lib.loc = "..." !) and can see what the matter is :

  > library(distr, lib.loc="/u/maechler/R/other-people/distr.Rcheck")
  Error in loadNamespace(i[[1]], c(lib.loc, .libPaths()), keep.source) : 
	  package 'stepfun' does not have a name space
  Error in library(distr, lib.loc = "/u/maechler/R/other-people/distr.Rcheck") : 
	  package/namespace load failed


Now this does help you further.
Particularly for the above case, I'd recommend to

** Get and install "R-1.9.0 alpha" and exclusively work with R-1.9.x
** for the "distr" package.

Reason: From 1.9.0 on, "stepfun" is part of "stats"  
	- which *is* a namespace
	- and a "default package", i.e. you don't even have to
	  say "library(stats)" by default.

With high regards to the "distr" developers,
Martin



From bates at stat.wisc.edu  Thu Mar 11 18:27:20 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Mar 2004 11:27:20 -0600
Subject: [R] widen the screen place
In-Reply-To: <48027968DDD36F40B70C261D6FE78837029F8E@titan3.paloma.com>
References: <48027968DDD36F40B70C261D6FE78837029F8E@titan3.paloma.com>
Message-ID: <6rsmgfi9af.fsf@bates4.stat.wisc.edu>

See 

?options

and especially the "width" option.  

Options used in base R:
...
     'width': controls the number of characters on a line. You may want
          to change this if you re-size the window that R is running
          in.  Valid values are 10...10000 with default normally 80. 
          (The valid values are in file 'Print.h' and can be changed by
          re-compiling R.)

You can get wide lines with

options(width=132)

for example.

"Yan Zhang" <YanZhang at paloma.com> writes:

> I have this problem: my screen seems not wide enough in R. My true
> computer monitor is very wide, so I wish that I can line up the text
> when I stretch it wide. i.e.:



From abunn at montana.edu  Thu Mar 11 18:39:42 2004
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 11 Mar 2004 10:39:42 -0700
Subject: [R] Questions about spatial data
In-Reply-To: <200403111218.35905.chrysopa@insecta.ufv.br>
Message-ID: <000e01c4078f$d8c34490$78f05a99@msu.montana.edu>

See the splancs library and the functions

gridpts 
pcp.sim 
csr

in particular. However, what you want to do is not completely trivial.



From dj at research.bell-labs.com  Thu Mar 11 18:42:31 2004
From: dj at research.bell-labs.com (David James)
Date: Thu, 11 Mar 2004 12:42:31 -0500
Subject: [R] Inserting Date Field into Oracle table using ROracle
In-Reply-To: <2D42B4DF9845F74E9318DF707D5956D93889CA@wdexch02.lexgen.com>;
	from smalladi@lexgen.com on Wed, Mar 10, 2004 at 04:29:37PM -0600
References: <2D42B4DF9845F74E9318DF707D5956D93889CA@wdexch02.lexgen.com>
Message-ID: <20040311124230.A6118@jessie.research.bell-labs.com>

I could not reproduce your problem, and I suspect that if you
try your example from the sqlplus utility you'd get the same
problem.  I'd suggest you try to insert into your table directly
from sqlplus, and once you get it working then try the same
syntax from R.  Oracle automatically coerces strings to DATE
by using the default format for your locale, so you need to make 
sure that the strings you pass to INSERT conform to such default 
format; e.g., to find out you could try 

   $ sqlplus user/password
   SQL> select sysdate from dual;

   SYSDATE
   ---------
   11-MAR-04

   SQL>

The following R session mimics your script:

   require(ROracle)
   con <- dbConnect("Oracle", "user/password")

   dbGetQuery(con, "create table tst (d date, i integer)")

   ## make sure the input data.frame has the correct types
   d <- data.frame(d = "11-MAR-04", i = as.integer(100))
   d$d <- as.character(d$d)         ## should *not* be a factor

   ## prepared statements automatically begin a new transaction
   ps <- dbPrepareStatement(con, "insert into tst values (:1, :2)", 
            bind = c("character", "integer"))
   dbExecStatement(ps, d)     ## do the actual insert

   ## close the prepared statement to force a commit (otherwise you
   ## won't see the changes to the table)
   dbClearResult(ps)
   [1] TRUE

   > dbReadTable(con, "tst")
             D   I
   0 11-MAR-04 100

Hope this helps,

--
David

Malladi, Sukhaswami wrote:
> Hello,
> 
> Attached is a mail regarding question how to insert Date field using ROracle
> 
> package. I am stuck with this problem and appreciate receiving help from 
> gurus on this list.
> 
> Code used mainly is:
> 
> library(ROracle) ### --- Version 0.53
> drv <- dbDriver("Oracle") 
> con <- dbConnect( drv, "user/passwd") 
> d <- data.frame(CDATE = "2004-03-10 10:12:00")
> ps <- dbPrepareStatement(con, 
> 		"INSERT into DATEST (CDATE) VALUES ( :1 ) ", 
> 		bind=c( "character"))  ## -- c("date") does not work
> sapply(d, class)
> d$CDATE <- as.character(d$CDATE)
> sapply(d, class)
> dbExecStatement(ps,d) 
> 
> Error in oraExecStatement(ps, data, ...) : 
>         RS-DBI driver: (ORA-01861: literal does not match format string )
> 
> Thanks for your help in advance,
> Swami
> (smalladi at lexgen.com)
> 
> ----------------------------- Correspondence with David James
> -----------------------
> 
> Dear David,
> 
> Thanks for your kind reply. I did what you suggested, coerced
> d into a character vector. Now I get an Oracle error -
> 
> d <- data.frame(CDATE = "TO_DATE('2004-03-10 10:12:00','YYYY-MM-DD
> HH:MI:SS')")
> sapply(d, class)
> d$CDATE <- as.character(d$CDATE)
> sapply(d, class)
> dbExecStatement(ps,d) 
> Error in oraExecStatement(ps, data, ...) : 
>         RS-DBI driver: (ORA-01858: a non-numeric character was found where a
> numeric was expected
> -----------------------------
> ORA-01858 a non-numeric character was found where a numeric was expected
> 
> Cause: The input data to be converted using a date format model was
> incorrect. The input data did not contain a number where a number was
> required by the format model.
> 
> Action: Fix the input data or the date format model to make sure the
> elements match in number and type. Then retry the operation
> ------------------
> 
> If I do 
> d <- data.frame(CDATE = "2004-03-10 10:12:00")
> instead of line 1 above, I get error :
> Error in oraExecStatement(ps, data, ...) : 
>         RS-DBI driver: (ORA-01861: literal does not match format string )
> ----------------
> Cause: Literals in the input must be the same length as literals in the
> format string (with the exception of leading white space). If the "FX"
> modifier has been toggled on, the literal must match exactly, with no extra
> white space.
> 
> Action: Correct the format string to match the literal.
> ------------
> 
> I do not know what I am doing wrongly. I will definitely post the experience
> in R-help.
> 
> Kindly help,
> Thanks
> Swami
> 
> 
> 
> > -----Original Message-----
> > From: David James [mailto:dj at research.bell-labs.com]
> > Sent: Wednesday, March 10, 2004 8:15 AM
> > To: Malladi, Sukhaswami
> > Cc: David James
> > Subject: Re: ROracle : insert dates
> > 
> > 
> > Dear Swami,
> > 
> > One possible cause of your problem is that the dataframe "d"
> > that you create may not have the date field "CDATE" as a string,
> > but rather as a factor.  If this is the case, then you need to 
> > coerce it to be a character vector, e.g.,
> >   > d <- data.frame(CDATE = "2004-03-10")
> >   d
> >   CDATE
> >   1 2004-03-10
> >   > sapply(d, class)
> >      CDATE
> >   "factor"
> >   > ## coerce CDATE to character
> >   > d$CDATE <- as.character(d$CDATE)
> >   > sapply(d, class)
> >         CDATE
> >   "character"
> > 
> > If this is indeed the problem, could you summary the result and
> > post it to r-help so other people may be able to learn from your
> > experience?
> > 
> > Regards,
> > 
> > --
> > David
> > 
> > 
> > Malladi, Sukhaswami wrote:
> > > Hi
> > > 
> > > I am using ROracle for interacting between ORACLE and R. I 
> > am able to insert
> > > character and numeric data.
> > > However, I am unable to insert date into a table despite 
> > attempting many
> > > methods. The code I used is as follows:
> > > 
> > > 	library(ROracle) ### --- Version 0.53
> > > 	drv <- dbDriver("Oracle") 
> > > 	con <- dbConnect( drv, "user/passwd") 
> > > 
> > > 	d <- data.frame( cbind( CDATE="TO_DATE('02-02-2004
> > > 10:12:00','DD-MM-YYYY HH:MI:SS' )" ) )
> > > 
> > > 	lQry <- "INSERT into DATEST (CDATE) VALUES ( :1 ) "
> > > 
> > > 	ps <- dbPrepareStatement(con, "INSERT into DATEST 
> > (CDATE) VALUES (
> > > :1 ) ",
> > > 			 bind=c( "character"))  ## --------- c("date")
> > > gives error shown below
> > > 
> > > 	dbExecStatement(ps,d) 
> > > 
> > > Error in oraExecStatement(ps, data, ...) : 
> > >         RS-DBI driver: (unrecognized S class factor )
> > > 
> > > > ps <- dbPrepareStatement(con, lQry, bind=c("date")) 
> > > Error in oraPrepareStatement(conn, statement, bind, ...) : 
> > >         RS-DBI driver: (unrecognized S class date )
> > > > 
> > > 
> > > My question is : how do I insert date in the oracle table DATEST ?
> > > 
> > > SQL> desc DATEST;
> > > 
> > >  Name                Type
> > >  ------------------ -------- 
> > >  CDATE               DATE
> > > 
> > > 
> > > platform i686-pc-linux-gnu
> > > arch     i686             
> > > os       linux-gnu        
> > > system   i686, linux-gnu  
> > > status                    
> > > major    1                
> > > minor    8.1              
> > > year     2003             
> > > month    11               
> > > day      21               
> > > language R                
> > > 
> > > I would be grateful for your kind help,
> > > 
> > > Thanks,
> > > Swami
> > > 
> > > 
> > > 
> > **************************************************************
> > ************* 
> > >  The contents of this communication are intended only for 
> > the addressee and
> > > may contain confidential and/or privileged material. If you 
> > are not the
> > > intended recipient, please do not read, copy, use or disclose this
> > > communication and notify the sender.  Opinions, conclusions 
> > and other
> > > information in this communication that do not relate to the official
> > > business of my company shall be understood as neither given 
> > nor endorsed by
> > > it.  
> > > 
> > **************************************************************
> > ************* 
> > > 
> > 
> 
> 
> *************************************************************************** 
>  The contents of this communication are intended only for th...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From YanZhang at paloma.com  Thu Mar 11 18:51:32 2004
From: YanZhang at paloma.com (Yan Zhang)
Date: Thu, 11 Mar 2004 12:51:32 -0500
Subject: [R] widen the screen place
Message-ID: <48027968DDD36F40B70C261D6FE78837019D14@titan3.paloma.com>

I got it already , thanks very much everyone.



-----Original Message-----
From: Douglas Bates [mailto:bates at bates4.stat.wisc.edu] On Behalf Of
Douglas Bates
Sent: Thursday, March 11, 2004 12:27 PM
To: Yan Zhang
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] widen the screen place

See 

?options

and especially the "width" option.  

Options used in base R:
...
     'width': controls the number of characters on a line. You may want
          to change this if you re-size the window that R is running
          in.  Valid values are 10...10000 with default normally 80. 
          (The valid values are in file 'Print.h' and can be changed by
          re-compiling R.)

You can get wide lines with

options(width=132)

for example.

"Yan Zhang" <YanZhang at paloma.com> writes:

> I have this problem: my screen seems not wide enough in R. My true
> computer monitor is very wide, so I wish that I can line up the text
> when I stretch it wide. i.e.:



From abunn at montana.edu  Thu Mar 11 18:49:59 2004
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 11 Mar 2004 10:49:59 -0700
Subject: [R] saving a data.frame to "\t" files
In-Reply-To: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
Message-ID: <001601c40791$48e7a710$78f05a99@msu.montana.edu>

?write.table

setwd("c:\\temp")
myDF <- data.frame(A = rnorm(100), B = rnorm(100))
write.table(myDF, file = "myDF.dat", sep = "\t", quote = F, row.names =
F)

HTH, Andy



From ligges at statistik.uni-dortmund.de  Thu Mar 11 18:56:13 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 11 Mar 2004 18:56:13 +0100
Subject: [R] R CMD check errors
In-Reply-To: <Pine.LNX.4.44.0403101712400.1319-100000@spock.vulcan>
References: <Pine.LNX.4.44.0403101712400.1319-100000@spock.vulcan>
Message-ID: <4050A83D.2080603@statistik.uni-dortmund.de>

Thomas Stabla wrote:

> Hello,
> 
> I'm getting some error messages from R CMD check I can't deal with.
> I'm working under Linux with R 1.8.1
> The package working directory can be found at:
> 
> http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/distr.tar.gz
> 
> 
> Here's the 00check.log produced by R CMD check distr
> 
> * using log directory '/home/tom/studium/R/swp/swp/package/distr.Rcheck'
> * checking for file 'distr/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking index information ... OK
> * checking package subdirectories ... WARNING
> Subdirectory 'data' contains no data sets.
> Subdirectory 'src' contains no source files.

So, if you don't want to provide data files or C/C++/Fortran sources, 
you want to delete those directories.
BTW: There's no documentation in that package ...


> * checking R files for syntax errors ... OK
> * checking R files for library.dynam ... OK
> * checking S3 generic/method consistency ... WARNING
> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
>         package/namespace load failed

Your NAMESPACE file is syntactically incorrect (hint: you need to quote 
in exportPattern()). Please read "Writing R extensions", and please read 
the ReadMe files in all directories which have been created by 
package.skeleton().

Uwe Ligges




> Execution halted
> * checking for replacement functions with final arg not named 'value' ... WARNING
> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
>         package/namespace load failed
> Execution halted
> * checking Rd files ... OK
> * checking for missing documentation entries ... ERROR
> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
> 
> 
> Thanks for your help,
> Thomas Stabla
>



From bates at stat.wisc.edu  Thu Mar 11 18:55:04 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Mar 2004 11:55:04 -0600
Subject: [R] Summary: do.call and environments
In-Reply-To: <Pine.A41.4.58.0403110733020.60588@homer10.u.washington.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7979@usrymx25.merck.com>
	<Pine.A41.4.58.0403110733020.60588@homer10.u.washington.edu>
Message-ID: <6rekrzi807.fsf@bates4.stat.wisc.edu>

Thomas Lumley <tlumley at u.washington.edu> writes:

> Lisp developers also used to
> think that dynamic scope was more useful and easier than dynamic scope.
> They don't now.

Not surprising, given the way that you wrote the first sentence. :-)

Did you mean for one of the "dynamic scope" phrases to be "lexical
scope"?



From bates at stat.wisc.edu  Thu Mar 11 18:58:00 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Mar 2004 11:58:00 -0600
Subject: [R] saving a data.frame to "\t" files
In-Reply-To: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
References: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
Message-ID: <6rad2ni7vb.fsf@bates4.stat.wisc.edu>

"Mihai Nica" <m_nica at hotmail.com> writes:

> Windows 2000, updated R and packages.
> 
> could somebody pleaseeeeeeeee help with saving a data.frame with
> column names into "\t" text files for later importing in other
> programs? It seems an easy task, yet... it beats me.

See

?write.table

You want to use something like

write.table(myDataFrame, file = "myTabSeparatedFile.txt", sep = "\t")



From ligges at statistik.uni-dortmund.de  Thu Mar 11 19:02:16 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 11 Mar 2004 19:02:16 +0100
Subject: [R] reading binary file with different modes
In-Reply-To: <404F6F83.1050206@hotmail.com>
References: <404F6F83.1050206@hotmail.com>
Message-ID: <4050A9A8.7000505@statistik.uni-dortmund.de>

Angel Lopez wrote:

> I have a binary file representing a matrix with columns of different 
> variable type/mode (i.e. the file was saved from C using double and int 
> variables)
> I want to import it into R, I've been reading the R Import/Export and I 
> am able to use readBin for a file that contains only one variable type 
> (either all doubles or all ints) but I can not find the way to use it 
> when you have a file with mixture of the two.
> Could somebody point me to a solution or appropiatte reference?
> Thanks,
> Angel


You have to loop over something like:

  readBin(x, "double")
  readBin(x, "integer", size=2)


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Mar 11 19:04:54 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 11 Mar 2004 19:04:54 +0100
Subject: [R] saving a data.frame to "\t" files
In-Reply-To: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
References: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
Message-ID: <4050AA46.1030004@statistik.uni-dortmund.de>

Mihai Nica wrote:
> Windows 2000, updated R and packages.
> 
> could somebody pleaseeeeeeeee help with saving a data.frame with column
> names into "\t" text files for later importing in other programs? It seems an easy task, yet... it beats me.

It's in the R Data Import/Export manual and in the help file 
?write.table (hint: argument sep = "\t").

Uwe Ligges



From m_nica at hotmail.com  Thu Mar 11 19:04:13 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Thu, 11 Mar 2004 12:04:13 -0600
Subject: [R] saving a data.frame to "\t" files
Message-ID: <Law10-OE47DEjspyhjC00044355@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040311/49557ce1/attachment.pl

From spencer.graves at pdf.com  Thu Mar 11 19:12:56 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 10:12:56 -0800
Subject: [R] Summary: do.call and environments
In-Reply-To: <Pine.A41.4.58.0403110733020.60588@homer10.u.washington.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7979@usrymx25.merck.com>
	<Pine.A41.4.58.0403110733020.60588@homer10.u.washington.edu>
Message-ID: <4050AC28.7030904@pdf.com>

Hi, Thomas: 

(see inline)

Thomas Lumley wrote:

>On Thu, 11 Mar 2004, Liaw, Andy wrote:
>
>  
>
>>Gabor,
>>
>>    
>>
>>>From: Gabor Grothendieck
>>>
>>>Note that R and S are fundamentally different when it comes to
>>>scoping.
>>>
>>>R uses lexical scoping, i.e. the parent environment of a function
>>>is the environment at the point where it is *defined* whereas
>>>S uses dynamic scoping, i.e. the parent environment in a function
>>>is the environment at the point where the function is *called*.
>>>      
>>>
>>I don't think that's quite right.  S does not use dynamic scope.  This
>>simple example fails in S-PLUS (6.1):
>>    
>>
>
>I'm not sure if there is standard jargon for the scoping rules that S
>uses, but it certainly isn't dynamic scope.  The scope of a name does
>not change: it's always the local frame plus the global workspace, so we
>have static scope.
>
>One way to describe the difference between S and R scoping is that in S
>environment(fun) would always be the global workspace (frame 0), which is
>why the environment() function isn't needed in S.
>
>As a final note, it is "obvious" that dynamic scope is useful and easy to
>implement, so people often try to fake it. Lisp developers also used to
>think that dynamic scope was more useful and easier than dynamic scope.
>They don't now.
>
Do you mean, "Lisp developers used to think that dynamic scope was ... 
easier than static scope" or "... static scope was ... easier than 
dynamic scope"? 

Thanks for the clarification.  Spencer Graves

>
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Thu Mar 11 19:17:48 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Mar 2004 10:17:48 -0800
Subject: [R] saving a data.frame to "\t" files
In-Reply-To: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
References: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
Message-ID: <4050AD4C.2020909@pdf.com>

Maybe I don't understand your questions, but the documentation for 
"write.table" in S-Plus 2000, 6.2, and R 1.8.1 all mention a parameter 
"sep".  Will this fix your problem? 

      hope this helps.  spencer graves

Mihai Nica wrote:

>Windows 2000, updated R and packages.
>
>could somebody pleaseeeeeeeee help with saving a data.frame with column
>names into "\t" text files for later importing in other programs? It seems an easy task, yet... it beats me.
>
>thanks,
>
>Mihai Nica
>Jackson State University
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From xiaoliu at jhmi.edu  Thu Mar 11 19:16:15 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Thu, 11 Mar 2004 13:16:15 -0500
Subject: [R] Receiver Operator Characteristic curve
Message-ID: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>

Dear R-helpers:

I want to calculate area under a Receiver Operator Characteristic curve.  Where can I find related functions?

Thank you in advance

Xiao



From p.pagel at gsf.de  Thu Mar 11 19:16:27 2004
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 11 Mar 2004 19:16:27 +0100
Subject: [R] saving a data.frame to "\t" files
In-Reply-To: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
References: <Law10-OE42Cn9RI4QAS0003f403@hotmail.com>
Message-ID: <20040311181627.GA1438@porcupine.gsf.de>

> could somebody pleaseeeeeeeee help with saving a data.frame with
> column names into "\t" text files for later importing in other
> programs? It seems an easy task, yet... it beats me.

write.table(mydf, file="foo.tbl", sep="\t", col.names=T)

Where mydf is your data frame.

If you don't want quoting you can also add 'quote=FALSE'

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/~pagel



From andy_liaw at merck.com  Thu Mar 11 19:34:58 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Mar 2004 13:34:58 -0500
Subject: [R] do.call and environments
Message-ID: <3A822319EB35174CA3714066D590DCD504AF797D@usrymx25.merck.com>

Tony,

Thanks very much for the correction.  I agree completely.  The real
nightmare is in not being able to ensure which object, if there can be
several of the same name within the scope, that the code is going to pickup.
With lexical scope, the developer has the responsibility and control over
that, but not with dynamic scope.

Best,
Andy

> From: Tony Plate
> 
> Andy, I think you're correct that Thomas Petzoldt wants 
> dynamic scoping, but
> isn't your argument here about strong typing vs weak typing?  To 
> paraphrase, one
> could say: For example, if weak typing were allowed I might write a 
> function `g' that
> passes argument `x' to fx() as a character, or a data frame, 
> or a list, or 
> an `lm' object,
> or a connection, ....  How would you write fx() to deal with 
> that nightmare 
> if you have
> weak typing? (Actually, the S language does have weak typing, 
> and some 
> functions
> deal with the "nightmare" by checking the types of their 
> arguments.  Other 
> functions
> just pass their arguments along without type checking, 
> resulting in cryptic 
> error
> messages).
> 
> I think the issues around lexical vs dynamic scope are more 
> to do with dynamic
> scope making programming accidents more likely, because of 
> the risk that
> a variable in a higher frame is masked unintentionally by 
> another variable of
> the same name in an intervening frame, and more difficult to 
> track down,
> because the error can depend on the exact calling sequence 
> when it occurred.
> 
> cheers,
> 
> Tony Plate
> 
> At Wednesday 11:03 AM 3/10/2004, Liaw, Andy wrote:
> >Seems to me what you want is dynamic scoping: `x' is not 
> defined in `fx'.
> >You want `x' to be found in the scope of the function(s) 
> that calls `fx',
> >rather than the environment where `fx' is defined.  I was 
> told (thanks,
> >Robert!) that that is a very bad idea:  as the author of 
> `fx', you want some
> >assurance of what `x' might be.  This is done via R's 
> lexical scope.  With
> >dynamic scope, there is absolutely no way to do that.  For 
> example, I might
> >write a function `g' that define `x' as a character, or a 
> data frame, or a
> >list, or an `lm' object, or a connection, ....  How would 
> you write `fx' to
> >deal with that nightmare if you have dynamic scope?
> >
> >Andy
> >
> > > From: Thomas Petzoldt
> > >
> > > Hello,
> > >
> > > I want to call a function "fx" given by name, where some "global"
> > > variables (in the environment of fx) are passed to the 
> function. For
> > > compatibility reasons I cannot modify the parameter list 
> of fx and I
> > > want to avoid setting variables in the global environment
> > > (e.g. via <<-)
> > >
> > > Is there a way, how to do this?
> > >
> > > Thomas P.
> > >
> > > The example:
> > >
> > > fx <- function(y) print(x*y)
> > >
> > > f <- function(fun, xx) {
> > >    fxx <- function() {do.call(fun, list(y=3))}
> > >    x <- x
> > >    fxx()
> > > }
> > >
> > > f("fx", 13)
> > >
> > > ## does not work, because fx does not find x
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> >-------------------------------------------------------------
> -----------------
> >Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From malayus at hotmail.com  Thu Mar 11 19:36:30 2004
From: malayus at hotmail.com (Mario Lara Yuste)
Date: Thu, 11 Mar 2004 18:36:30 +0000
Subject: [R] A question about function "cor" in code C
Message-ID: <Law11-F29Hs9WOv8Ql20003e4b6@hotmail.com>

Dear Sir:

   I am interesting in to call function "cor" from C code. The reason for 
this, is that I need to obtain a matrix of correlation in a C program, and I 
have think to link my program in C code with R. But I don?t  know make this 
link. I have read R-exts.pdf document, but I have not found a direct way to 
link.

      But I have a problem which I have not found information in R 
documentation,
for that I ask for help.

    Thanking you in advance,

                                                   Mario Lara Yuste
                                                   malayus at hotmail.com
                                                   University of Cordoba 
(Spain)

_________________________________________________________________
Reparaciones, servicios a domicilio, empresas, profesionales... Todo en la 
gu?a telef?nica de QDQ. http://qdq.msn.es/msn.cfm



From B.Rowlingson at lancaster.ac.uk  Thu Mar 11 20:10:40 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 11 Mar 2004 19:10:40 +0000
Subject: [R] Questions about spatial data
In-Reply-To: <200403111218.35905.chrysopa@insecta.ufv.br>
References: <200403111218.35905.chrysopa@insecta.ufv.br>
Message-ID: <4050B9B0.5020400@lancaster.ac.uk>

Ronaldo Reis Jr. wrote:

> 1) I have this grid
> 
> x <- rep(c(1:8),c(rep(6,8)))
> y <- rep(c(1:6),8)
> 
> I need to make some others grids with same length and different numbers of 
> points whit a random spatial pattern.

  This is a bit imprecise. What do you mean by 'same length and 
different numbers of points'?

  Your grid above has 8*6 = 42 points. Do you want to generate a random 
subset of these grid points, selecting each point with a fixed 
probability, or selecting exactly N points?

  Firstly, I would make your grid above using 'expand.grid':

fullGrid <- expand.grid(1:8,1:6)
plot(fullGrid)

  Then you can use the sample() function to choose 10 different integers 
from 1 to 42, and use that to choose rows of the fullGrid data frame:

thinGrid <- fullGrid[sample(42,10),]
plot(thinGrid)

  This gives you exactly 10 points.

  Or you could choose each point with some probability, using runif(42) 
to generate uniform random numbers and compare this with a threshold:

thin2 <- fullGrid[runif(42)>.8,]
plot(thin2)

  This gives grids with differing numbers of points, but each of the 42 
points has a 0.2 probability of being included.

> 2) I need to produce some grids with the same numbers of points in different 
> degrees of aggregation, the level of aggregation for all grids must be 
> statistically different.
> 
> How to do these grids in R and how to test the significance between these 
> grids?

  I think at this point you need to find a good textbook on spatial 
point processes, then come back and look at R packages spatstat and to a 
lesser extent, splancs.

  Hope this helps,

Baz



From tlumley at u.washington.edu  Thu Mar 11 20:26:09 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Mar 2004 11:26:09 -0800 (PST)
Subject: [R] No traceback available when using try(...)
In-Reply-To: <200403111208.00088.duchesnay@shfj.cea.fr>
References: <200403111208.00088.duchesnay@shfj.cea.fr>
Message-ID: <Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>

On Thu, 11 Mar 2004, Edouard DUCHESNAY wrote:

> Hello,
>
> 1. The Situation :
> ------------------------
> The stack traceback is not available when error ouccured in a try(....)

It's a bug in 1.8.1. It has been fixed.

	-thomas



From ihaka at stat.auckland.ac.nz  Thu Mar 11 20:40:38 2004
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Fri, 12 Mar 2004 08:40:38 +1300
Subject: [R] fft question
In-Reply-To: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <4050C0B6.9050906@stat.auckland.ac.nz>

Frank Gerrit Zoellner wrote:
> Hi!
> 
> I am using the fft() function the base package to transform some 1d signal. 
> If I use this standar fucntion I get a very huge first fourier coeficient.
> I think this dues to the handling of the borders of the signal.
> Usually in fft especially in image processing the signal is simulated to be continuous by adding the signal several times periodically. My question is, is there some function implemented in R handling this or do I have to combine my data manually ?
> 
> Thanks, 

The fft function computes the discrete transform

	d(lambda) = SUM x(t) exp(-i * lambda * t)

(for a discrete set of lambda values).

The first coefficient is just SUM x(t).  This means that the
problem is not end-point discontinuity, but the fact that the
average level of the signal is non-zero.

Replicating the series k times won't help, you'll just
make the first coefficent k times bigger.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From p.connolly at hortresearch.co.nz  Thu Mar 11 21:00:45 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 12 Mar 2004 09:00:45 +1300
Subject: [R] Summary: do.call and environments
In-Reply-To: <40502FDE.3070600@pdf.com>;
	from spencer.graves@pdf.com on Thu, Mar 11, 2004 at 01:22:38AM
	-0800
References: <20040311075757.906A039AC@mprdmxin.myway.com>
	<40502333.1050006@pdf.com> <4050272C.9070902@rcs.urz.tu-dresden.de>
	<40502FDE.3070600@pdf.com>
Message-ID: <20040312090045.O2137@hortresearch.co.nz>

On Thu, 11-Mar-2004 at 01:22AM -0800, Spencer Graves wrote:

|> ff1 <- function(fun, x) {
|>   assign(fun, get(fun), 1)
|>   assign("x", x, 1)
|>   do.call(fun, list(y=3))
|> }
|> tst <- ff1("fx", 2)
|> 


|>       This assigns "6" to "tst" in both S-Plus 6.2 as R 1.8.1.
|> However, it also prints "[1] 6" in S-Plus 6.2 but not R.  ???

Good ol' Splus 3.4 gives the same result as R 1.8.1.



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ryszard.czerminski at pharma.novartis.com  Thu Mar 11 21:03:10 2004
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Thu, 11 Mar 2004 15:03:10 -0500
Subject: [R] how to pass extra parameters using call() or similar mechanism ?
Message-ID: <OF5A4D4945.19CA16E3-ON85256E54.006C9CE3-85256E54.006E4E04@EU.novartis.net>

I am trying to write a function, which would allow to call various methods
and would pass to them extra arbitrary parameters.
My first attempt was to use call() as illustrated below, but apparently
'...' cannot be used in such context.

How can this be achieved ?

Best regards,

Ryszard

> myfun <- function(method, x, ...) {
+   v <- eval(call(method, x, ...))
+ }
> method = 'sqrt'
> myfun('sqrt',2)
Error in eval(call(method, x, ...)) : ... used in an incorrect context
> eval(call(method, 2, ...))
Error in eval(call(method, 2, ...)) : ... used in an incorrect context
> eval(call(method, 2))
[1] 1.414214



From smalladi at lexgen.com  Thu Mar 11 21:35:29 2004
From: smalladi at lexgen.com (Malladi, Sukhaswami)
Date: Thu, 11 Mar 2004 14:35:29 -0600
Subject: [R] Inserting Date Field into Oracle table using ROracle
Message-ID: <2D42B4DF9845F74E9318DF707D5956D93889CE@wdexch02.lexgen.com>

Thank you very much for the insight into the problem.

I am able to insert dates into oracle date field with 
ROracle in the given date format with the restriction 
of not being able to include time. The default time is 
12:00:00 AM for all dates inserted. 

However, when I use a timestamp field instead of date field, 
I can insert time as well. The following code works :

....
require(ROracle)
con <- dbConnect("Oracle", "user/password")
dbGetQuery(con, "create table datest (d timestamp)")
ps <- dbPrepareStatement(con,"insert into DATEST values
(:1)",bind=c("character"))
d <- data.frame( d='14-MAR-00 11.16.22 AM' )
d$d <- as.character(d$d)
dbExecStatement( ps, d )
<OraPreparedStatement:(18793,0,1)> 
dbCommit(con);
...

The date/time format is fixed ( DD-MON-YY HH.MI.SS AM )
(pl. note the 2 digit year )

Thanks once again for the help,

Regards,
Swami


> -----Original Message-----
> From: David James [mailto:dj at research.bell-labs.com]
> Sent: Thursday, March 11, 2004 11:43 AM
> To: Malladi, Sukhaswami
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Inserting Date Field into Oracle table using ROracle
> 
> 
> I could not reproduce your problem, and I suspect that if you
> try your example from the sqlplus utility you'd get the same
> problem.  I'd suggest you try to insert into your table directly
> from sqlplus, and once you get it working then try the same
> syntax from R.  Oracle automatically coerces strings to DATE
> by using the default format for your locale, so you need to make 
> sure that the strings you pass to INSERT conform to such default 
> format; e.g., to find out you could try 
> 
>    $ sqlplus user/password
>    SQL> select sysdate from dual;
> 
>    SYSDATE
>    ---------
>    11-MAR-04
> 
>    SQL>
> 
> The following R session mimics your script:
> 
>    require(ROracle)
>    con <- dbConnect("Oracle", "user/password")
> 
>    dbGetQuery(con, "create table tst (d date, i integer)")
> 
>    ## make sure the input data.frame has the correct types
>    d <- data.frame(d = "11-MAR-04", i = as.integer(100))
>    d$d <- as.character(d$d)         ## should *not* be a factor
> 
>    ## prepared statements automatically begin a new transaction
>    ps <- dbPrepareStatement(con, "insert into tst values (:1, :2)", 
>             bind = c("character", "integer"))
>    dbExecStatement(ps, d)     ## do the actual insert
> 
>    ## close the prepared statement to force a commit (otherwise you
>    ## won't see the changes to the table)
>    dbClearResult(ps)
>    [1] TRUE
> 
>    > dbReadTable(con, "tst")
>              D   I
>    0 11-MAR-04 100
> 
> Hope this helps,
> 
> --
> David
> 
> Malladi, Sukhaswami wrote:
> > Hello,
> > 
> > Attached is a mail regarding question how to insert Date 
> field using ROracle
> > 
> > package. I am stuck with this problem and appreciate 
> receiving help from 
> > gurus on this list.
> > 
> > Code used mainly is:
> > 
> > library(ROracle) ### --- Version 0.53
> > drv <- dbDriver("Oracle") 
> > con <- dbConnect( drv, "user/passwd") 
> > d <- data.frame(CDATE = "2004-03-10 10:12:00")
> > ps <- dbPrepareStatement(con, 
> > 		"INSERT into DATEST (CDATE) VALUES ( :1 ) ", 
> > 		bind=c( "character"))  ## -- c("date") does not work
> > sapply(d, class)
> > d$CDATE <- as.character(d$CDATE)
> > sapply(d, class)
> > dbExecStatement(ps,d) 
> > 
> > Error in oraExecStatement(ps, data, ...) : 
> >         RS-DBI driver: (ORA-01861: literal does not match 
> format string )
> > 
> > Thanks for your help in advance,
> > Swami
> > (smalladi at lexgen.com)
> > 
> > ----------------------------- Correspondence with David James
> > -----------------------
> > 
> > Dear David,
> > 
> > Thanks for your kind reply. I did what you suggested, coerced
> > d into a character vector. Now I get an Oracle error -
> > 
> > d <- data.frame(CDATE = "TO_DATE('2004-03-10 10:12:00','YYYY-MM-DD
> > HH:MI:SS')")
> > sapply(d, class)
> > d$CDATE <- as.character(d$CDATE)
> > sapply(d, class)
> > dbExecStatement(ps,d) 
> > Error in oraExecStatement(ps, data, ...) : 
> >         RS-DBI driver: (ORA-01858: a non-numeric character 
> was found where a
> > numeric was expected
> > -----------------------------
> > ORA-01858 a non-numeric character was found where a numeric 
> was expected
> > 
> > Cause: The input data to be converted using a date format model was
> > incorrect. The input data did not contain a number where a 
> number was
> > required by the format model.
> > 
> > Action: Fix the input data or the date format model to make sure the
> > elements match in number and type. Then retry the operation
> > ------------------
> > 
> > If I do 
> > d <- data.frame(CDATE = "2004-03-10 10:12:00")
> > instead of line 1 above, I get error :
> > Error in oraExecStatement(ps, data, ...) : 
> >         RS-DBI driver: (ORA-01861: literal does not match 
> format string )
> > ----------------
> > Cause: Literals in the input must be the same length as 
> literals in the
> > format string (with the exception of leading white space). 
> If the "FX"
> > modifier has been toggled on, the literal must match 
> exactly, with no extra
> > white space.
> > 
> > Action: Correct the format string to match the literal.
> > ------------
> > 
> > I do not know what I am doing wrongly. I will definitely 
> post the experience
> > in R-help.
> > 
> > Kindly help,
> > Thanks
> > Swami
> > 
> > 
> > 
> > > -----Original Message-----
> > > From: David James [mailto:dj at research.bell-labs.com]
> > > Sent: Wednesday, March 10, 2004 8:15 AM
> > > To: Malladi, Sukhaswami
> > > Cc: David James
> > > Subject: Re: ROracle : insert dates
> > > 
> > > 
> > > Dear Swami,
> > > 
> > > One possible cause of your problem is that the dataframe "d"
> > > that you create may not have the date field "CDATE" as a string,
> > > but rather as a factor.  If this is the case, then you need to 
> > > coerce it to be a character vector, e.g.,
> > >   > d <- data.frame(CDATE = "2004-03-10")
> > >   d
> > >   CDATE
> > >   1 2004-03-10
> > >   > sapply(d, class)
> > >      CDATE
> > >   "factor"
> > >   > ## coerce CDATE to character
> > >   > d$CDATE <- as.character(d$CDATE)
> > >   > sapply(d, class)
> > >         CDATE
> > >   "character"
> > > 
> > > If this is indeed the problem, could you summary the result and
> > > post it to r-help so other people may be able to learn from your
> > > experience?
> > > 
> > > Regards,
> > > 
> > > --
> > > David
> > > 
> > > 
> > > Malladi, Sukhaswami wrote:
> > > > Hi
> > > > 
> > > > I am using ROracle for interacting between ORACLE and R. I 
> > > am able to insert
> > > > character and numeric data.
> > > > However, I am unable to insert date into a table despite 
> > > attempting many
> > > > methods. The code I used is as follows:
> > > > 
> > > > 	library(ROracle) ### --- Version 0.53
> > > > 	drv <- dbDriver("Oracle") 
> > > > 	con <- dbConnect( drv, "user/passwd") 
> > > > 
> > > > 	d <- data.frame( cbind( CDATE="TO_DATE('02-02-2004
> > > > 10:12:00','DD-MM-YYYY HH:MI:SS' )" ) )
> > > > 
> > > > 	lQry <- "INSERT into DATEST (CDATE) VALUES ( :1 ) "
> > > > 
> > > > 	ps <- dbPrepareStatement(con, "INSERT into DATEST 
> > > (CDATE) VALUES (
> > > > :1 ) ",
> > > > 			 bind=c( "character"))  ## 
> --------- c("date")
> > > > gives error shown below
> > > > 
> > > > 	dbExecStatement(ps,d) 
> > > > 
> > > > Error in oraExecStatement(ps, data, ...) : 
> > > >         RS-DBI driver: (unrecognized S class factor )
> > > > 
> > > > > ps <- dbPrepareStatement(con, lQry, bind=c("date")) 
> > > > Error in oraPrepareStatement(conn, statement, bind, ...) : 
> > > >         RS-DBI driver: (unrecognized S class date )
> > > > > 
> > > > 
> > > > My question is : how do I insert date in the oracle 
> table DATEST ?
> > > > 
> > > > SQL> desc DATEST;
> > > > 
> > > >  Name                Type
> > > >  ------------------ -------- 
> > > >  CDATE               DATE
> > > > 
> > > > 
> > > > platform i686-pc-linux-gnu
> > > > arch     i686             
> > > > os       linux-gnu        
> > > > system   i686, linux-gnu  
> > > > status                    
> > > > major    1                
> > > > minor    8.1              
> > > > year     2003             
> > > > month    11               
> > > > day      21               
> > > > language R                
> > > > 
> > > > I would be grateful for your kind help,
> > > > 
> > > > Thanks,
> > > > Swami
> > > > 
> > > > 
> > > > 
> > > **************************************************************
> > > ************* 
> > > >  The contents of this communication are intended only for 
> > > the addressee and
> > > > may contain confidential and/or privileged material. If you 
> > > are not the
> > > > intended recipient, please do not read, copy, use or 
> disclose this
> > > > communication and notify the sender.  Opinions, conclusions 
> > > and other
> > > > information in this communication that do not relate to 
> the official
> > > > business of my company shall be understood as neither given 
> > > nor endorsed by
> > > > it.  
> > > > 
> > > **************************************************************
> > > ************* 
> > > > 
> > > 
> > 
> > 
> > 
> **************************************************************
> ************* 
> >  The contents of this communication are intended only for 
> th...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


*************************************************************************** 
 The contents of this communication are intended only for th...{{dropped}}



From p.dalgaard at biostat.ku.dk  Thu Mar 11 23:11:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Mar 2004 23:11:49 +0100
Subject: [R] Summary: do.call and environments
In-Reply-To: <20040311090323.E88EC39AC@mprdmxin.myway.com>
References: <20040311090323.E88EC39AC@mprdmxin.myway.com>
Message-ID: <x2brn32fve.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> S uses dynamic scoping, i.e. the parent environment in a function 
> is the environment at the point where the function is *called*.

Not really. The parent env. in S (or should I say: the other
implementation of S) is essentially the global environment only.
Actually, a little more complex since S operates with an "expression
frame" which is kind of sandwiched in between the function frame and
the global frame. However you don't have automatic access to the
variables of the caller as you would have in dynamic scoping. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From j.nocera at unb.ca  Thu Mar 11 23:24:48 2004
From: j.nocera at unb.ca (Joe Nocera)
Date: Thu, 11 Mar 2004 18:24:48 -0400
Subject: [R] Receiver Operator Characteristic curve
In-Reply-To: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>
Message-ID: <NGBBIMKLALDJHADGCBBBCEMJCOAA.j.nocera@unb.ca>

Xiao,

The Mayo Clinic website has an archive of functions written in S to plot ROC
and calculate the AUC for those plots.  The functions are self-extracting,
and easily imported into R.

http://www.mayo.edu/hsr/Sfunc.html

Cheers,
Joe Nocera

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of XIAO LIU
Sent: March 11, 2004 2:16 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Receiver Operator Characteristic curve


Dear R-helpers:

I want to calculate area under a Receiver Operator Characteristic curve.
Where can I find related functions?

Thank you in advance

Xiao

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rgentlem at jimmy.harvard.edu  Thu Mar 11 23:34:59 2004
From: rgentlem at jimmy.harvard.edu (Robert Gentleman)
Date: Thu, 11 Mar 2004 17:34:59 -0500
Subject: [R] Receiver Operator Characteristic curve
In-Reply-To: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>;
	from xiaoliu@jhmi.edu on Thu, Mar 11, 2004 at 01:16:15PM -0500
References: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>
Message-ID: <20040311173459.M26851@jimmy.harvard.edu>

well one stop is at
  www.bioconductor.org/repository/release1.3/package/html/index.html

 where you will find an ROC package


On Thu, Mar 11, 2004 at 01:16:15PM -0500, XIAO LIU wrote:
> Dear R-helpers:
> 
> I want to calculate area under a Receiver Operator Characteristic curve.  Where can I find related functions?
> 
> Thank you in advance
> 
> Xiao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+---------------------------------------------------------------------------+
| Robert Gentleman                 phone : (617) 632-5250                   |
| Associate Professor              fax:   (617)  632-2444                   |
| Department of Biostatistics      office: M1B20                            |
| Harvard School of Public Health  email: rgentlem at jimmy.harvard.edu        |
+---------------------------------------------------------------------------+



From feh3k at spamcop.net  Thu Mar 11 23:48:51 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 11 Mar 2004 16:48:51 -0600
Subject: [R] Receiver Operator Characteristic curve
In-Reply-To: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>
References: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>
Message-ID: <20040311164851.3af0ef66.feh3k@spamcop.net>

On Thu, 11 Mar 2004 13:16:15 -0500
XIAO LIU <xiaoliu at jhmi.edu> wrote:

> Dear R-helpers:
> 
> I want to calculate area under a Receiver Operator Characteristic curve.
>  Where can I find related functions?
> 
> Thank you in advance
> 
> Xiao
> 

install.packages('Hmisc')
library(Hmisc)
w <- somers2(predicted probability, 0/1 diagnosis)

Convert Somers' Dxy rank correlation to ROC area (C) using Dxy=2*(C-.5).

To get standard error of Dxy (and hence C) type ?rcorr.cens (another Hmisc
function).

This is the nonparametric Wilcoxon-Mann-Whitney approach.
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From partha_bagchi at hgsi.com  Thu Mar 11 23:31:06 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Thu, 11 Mar 2004 17:31:06 -0500
Subject: [R] Receiver Operator Characteristic curve
Message-ID: <OFA318C88F.1B354AE3-ON85256E54.007BA4D7-85256E54.007BB2CE@hgsi.com>

Look over here:

http://www.bioconductor.org/faq.html#Bioconductor%20Packages






XIAO LIU <xiaoliu at jhmi.edu>
Sent by: r-help-bounces at stat.math.ethz.ch
03/11/2004 01:16 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] Receiver Operator Characteristic curve


Dear R-helpers:

I want to calculate area under a Receiver Operator Characteristic curve. 
Where can I find related functions?

Thank you in advance

Xiao

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

--
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From andy_liaw at merck.com  Fri Mar 12 00:16:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Mar 2004 18:16:02 -0500
Subject: [R] how to pass extra parameters using call() or similar
	mech anism ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7987@usrymx25.merck.com>

See if this helps:

> callfun <- function(FUN, x, ...) {
+   do.call(FUN, c(list(x), list(...)))
+ }
> x <- c(1, 2, 5, NA)
> callfun("sum", x)
[1] NA
> callfun("sum", x, na.rm=TRUE)
[1] 8

Andy

> From: ryszard.czerminski at pharma.novartis.com
> 
> I am trying to write a function, which would allow to call 
> various methods
> and would pass to them extra arbitrary parameters.
> My first attempt was to use call() as illustrated below, but 
> apparently
> '...' cannot be used in such context.
> 
> How can this be achieved ?
> 
> Best regards,
> 
> Ryszard
> 
> > myfun <- function(method, x, ...) {
> +   v <- eval(call(method, x, ...))
> + }
> > method = 'sqrt'
> > myfun('sqrt',2)
> Error in eval(call(method, x, ...)) : ... used in an incorrect context
> > eval(call(method, 2, ...))
> Error in eval(call(method, 2, ...)) : ... used in an incorrect context
> > eval(call(method, 2))
> [1] 1.414214
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From tplate at blackmesacapital.com  Fri Mar 12 01:23:44 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 11 Mar 2004 17:23:44 -0700
Subject: [R] how to pass extra parameters using call() or similar
	mechanism ?
In-Reply-To: <OF5A4D4945.19CA16E3-ON85256E54.006C9CE3-85256E54.006E4E04@
	EU.novartis.net>
References: <OF5A4D4945.19CA16E3-ON85256E54.006C9CE3-85256E54.006E4E04@EU.novartis.net>
Message-ID: <6.0.3.0.2.20040311172115.04a334d0@mailhost.blackmesacapital.com>

If you want to use call() with a list of arguments, the easiest way I know 
of is to use do.call() (if anyone knows a better way, please say so!)

 > myfun2 <- function(method, x, ...) {
+    method.call <- do.call("call", list(method, x, ...))
+    eval(method.call)
+ }
 > myfun2('sqrt',2)
[1] 1.414214
 >

If you don't need the call object, you can just use do.call() directly 
(this is the same as Andy Liaw's suggestion):

 > myfun <- function(method, x, ...) {
+    do.call(method, list(x, ...))
+ }
 > myfun('sqrt',2)
[1] 1.414214


At Thursday 01:03 PM 3/11/2004, ryszard.czerminski at pharma.novartis.com wrote:
>I am trying to write a function, which would allow to call various methods
>and would pass to them extra arbitrary parameters.
>My first attempt was to use call() as illustrated below, but apparently
>'...' cannot be used in such context.
>
>How can this be achieved ?
>
>Best regards,
>
>Ryszard
>
> > myfun <- function(method, x, ...) {
>+   v <- eval(call(method, x, ...))
>+ }
> > method = 'sqrt'
> > myfun('sqrt',2)
>Error in eval(call(method, x, ...)) : ... used in an incorrect context
> > eval(call(method, 2, ...))
>Error in eval(call(method, 2, ...)) : ... used in an incorrect context
> > eval(call(method, 2))
>[1] 1.414214
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From prefectf at yahoo.com  Fri Mar 12 05:03:41 2004
From: prefectf at yahoo.com (Ford Prefect)
Date: Thu, 11 Mar 2004 20:03:41 -0800 (PST)
Subject: [R] power / sample size for ONE-SAMPLE binomial?
Message-ID: <20040312040341.85785.qmail@web13912.mail.yahoo.com>

Hi,

Is there an R function for calculating sample sizes
and power for a one-sample binomial experiment?
I have seen a bunch of function for two-sample
binomial (comparing two proportions) but can't
seem to find any for tests about a single proportion.

-Thanks



From ajayshah at mayin.org  Thu Mar 11 14:05:57 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 11 Mar 2004 18:35:57 +0530
Subject: [R] Difficulties in interaction between R and latex (prosper)
In-Reply-To: <1078991059.405018d39fb5b@webmail.lyon.inserm.fr>
References: <1078991059.405018d39fb5b@webmail.lyon.inserm.fr>
Message-ID: <20040311130557.GX689@igidr.ac.in>

On Thu, Mar 11, 2004 at 08:44:19AM +0100, Ken Knoblauch wrote:
> I suspect that this is not really an R question but one for the
> graphicx package of LaTeX.  Did you try the angle= argument
> for the includegraphics command? eg,
> 
> \includegraphics[width=\linewidth,angle=90]{cm_test.eps}

Yes, I agree that it's about the interfaces between R and prosper.

:( I tried using angle and it didn't work.

BTW it's not a generic graphicx issue : I wrote a simple latex
document which uses cm_test.eps and that works.

> > Hello, folks! I'm trying to use R as a graphics program, to make some
> > pretty graphs that will go into prosper slideshows.
> > 
> > I wrote this fragment, from the R manual, into a file demo.R:
> > 
> >    x=seq(-3,3,0.1)
> >    postscript("cm_test.eps", width = 4.0, height = 3.0,
> >                horizontal = FALSE, onefile = FALSE, paper = "special",
> >                family = "ComputerModern")
> >    plot(x, sin(x), type="l")
> > 
> > I fed this into a simplest-possible tex file, named sl_demo.tex, which
> > uses prosper:
> > 
> >    \documentclass[pdf,serpaggi,slideColor,colorBG]{prosper}
> >    \usepackage[latin1]{inputenc}
> >    \usepackage{graphicx}
> >    \begin{document}
> >      \begin{slide}{Demo}
> >        \includegraphics[width=\linewidth]{cm_test.eps}
> >      \end{slide}
> >      \begin{slide}{This one works}
> >        \includegraphics[width=\linewidth]{thisworks.eps}
> >      \end{slide}
> >    \end{document}
> > 
> > The file cm_test.eps is produced using R. I left a file
> > "thisworks.eps" there as a counterpoint (it was made using jgraph and
> > it works fine).
> > 
> > The resulting sl_demo.pdf is attached. It's supposed to be a
> > slideshow. Under Adobe acrobat, when I say Ctrl-L it must go
> > fullscreen. That works correctly for thisworks.eps but not for the
> > eps file that's made using R.
> > 
> > Any ideas what I'm doing wrong? How do I get the graph made using R
> > to sit horizantally (i.e. landscape), and fill the screen? I tried to
> > say "horizontal=T" and that doesn't work.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From arrayprofile at yahoo.com  Fri Mar 12 07:30:14 2004
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 11 Mar 2004 22:30:14 -0800 (PST)
Subject: [R] access R remotely
In-Reply-To: <20040311164851.3af0ef66.feh3k@spamcop.net>
Message-ID: <20040312063014.4660.qmail@web41203.mail.yahoo.com>

Hi,

not sure if this is kind of question that should be
asked here, but here it is:

I am trying to access R installed on a remote cluster
(Linus), but I got the error message that R command
not found when I simply typed "R" after the prompt
after having successfully accessed the remote cluster
using SSH. what could be the reason of the problem? is
the problem on the cluster side or on my side? I am
not a IT person at all, so I don't know much detail...
Our IT guy on the cluster side is working on the
problem, but seems not very efficiently targeting the
problem.

Thanks for any suggestion.



From nusbj at hotmail.com  Fri Mar 12 07:40:00 2004
From: nusbj at hotmail.com (Z P)
Date: Fri, 12 Mar 2004 14:40:00 +0800
Subject: [R] confidence interval in local polynomial regression
Message-ID: <Sea2-F18lEeGjOHJg3s0002d2df@hotmail.com>

Dear all,

Is there any package or function can do the pointwise confidence interval 
and confidence band for the local polynomial regression? Maybe the local 
linear regression is enough. Thanks.

Regards,

Zhen



From Hendri at uvt.nl  Fri Mar 12 08:28:29 2004
From: Hendri at uvt.nl (Hendri Adriaens)
Date: Fri, 12 Mar 2004 08:28:29 +0100
Subject: [Prosper-users] Re: [R] Difficulties in interaction between R and
	latex (prosper)
In-Reply-To: <20040311130557.GX689@igidr.ac.in>
Message-ID: <AIEMLDLODGHKHCOBLEFCIEJPCFAA.Hendri@uvt.nl>

> BTW it's not a generic graphicx issue : I wrote a simple latex
> document which uses cm_test.eps and that works.

Probably there is a problem with the eps. Run it through ghostscript
epswrite to (probably & hopefully) solve the problem.

Best,
-Hendri Adriaens.



From mros at toulouse.inra.fr  Fri Mar 12 08:31:21 2004
From: mros at toulouse.inra.fr (Mathieu Ros)
Date: Fri, 12 Mar 2004 08:31:21 +0100
Subject: [R] [in the press] Raqua and O'reilly
Message-ID: <425A5E25-73F7-11D8-8DAA-000A95C5B248@toulouse.inra.fr>

I've been nicely surprised, glancing through O'reilly's "Mac OS X 
Panther for Unix Geeks" (by Brian Jepson and Ernest E. Rothman), to 
discover that the sixth section of chapter seven is dedicated to Raqua. 
The authors give a few lines to describe R and Stefano's port to mac OS 
X, along with figures (graphic windows and R console), and even a small 
applescript example to interact with Raqua.
BTW, the book is great.

--Mathieu



From asemeria at cramont.it  Fri Mar 12 09:09:50 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Fri, 12 Mar 2004 09:09:50 +0100
Subject: [R] access R remotely
Message-ID: <OF3F5617D6.0A414240-ONC1256E55.002CD84F@tomware.it>





This is not the right place for this question, but however:
when you connect to a remote machine with ssh
with a user name "user" you aren't able to
use the specific shell profile ($PATH,....) of "user", then you have to
specify the complete PATH of all command (i.e. /usr/local/bin/R).
Best

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From seanpor at acm.org  Fri Mar 12 09:03:39 2004
From: seanpor at acm.org (Sean O'Riordain)
Date: Fri, 12 Mar 2004 08:03:39 +0000
Subject: [R] access R remotely
In-Reply-To: <20040312063014.4660.qmail@web41203.mail.yahoo.com>
References: <20040312063014.4660.qmail@web41203.mail.yahoo.com>
Message-ID: <40516EDB.2030906@acm.org>

This likely means that either R is not installed on that machine or the 
R binary is not in your PATH. : try asking your IT guy where R is 
installed... say Mr.IT.Guy says that R is installed in /usr/local/bin... 
then

/usr/local/bin/R

to start R...

cheers,
Sean.

array chip wrote:

>Hi,
>
>not sure if this is kind of question that should be
>asked here, but here it is:
>
>I am trying to access R installed on a remote cluster
>(Linus), but I got the error message that R command
>not found when I simply typed "R" after the prompt
>after having successfully accessed the remote cluster
>using SSH. what could be the reason of the problem? is
>the problem on the cluster side or on my side? I am
>not a IT person at all, so I don't know much detail...
>Our IT guy on the cluster side is working on the
>problem, but seems not very efficiently targeting the
>problem.
>
>Thanks for any suggestion.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From fzoellne at TechFak.Uni-Bielefeld.DE  Fri Mar 12 09:42:17 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Fri, 12 Mar 2004 09:42:17 +0100
Subject: [R] fft question
In-Reply-To: <16464.40538.560785.903625@gargle.gargle.HOWL>
References: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
	<16464.40538.560785.903625@gargle.gargle.HOWL>
Message-ID: <20040312084217.GA26201@hindemith.TechFak.Uni-Bielefeld.DE>

On Thu, Mar 11, 2004 at 06:14:02PM +0100, Martin Maechler wrote:

> 
> Yes, manually, like
> 
>   fx <- fft(rep(x, 4))
> 

I think rep works on a vector but in my case x is a dataframe/matrix with the signal along the rows.

Does rep work on dataframes ?

Thanks,

-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From fzoellne at TechFak.Uni-Bielefeld.DE  Fri Mar 12 09:43:21 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Fri, 12 Mar 2004 09:43:21 +0100
Subject: [R] Receiver Operator Characteristic curve
In-Reply-To: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>
References: <951d639513b6.9513b6951d63@jhmimail.jhmi.edu>
Message-ID: <20040312084321.GB26201@hindemith.TechFak.Uni-Bielefeld.DE>

On Thu, Mar 11, 2004 at 01:16:15PM -0500, XIAO LIU wrote:
> Dear R-helpers:
> 
> I want to calculate area under a Receiver Operator Characteristic curve.  Where can I find related functions?

There is an ROC package within the Bioconductor Project that works with R.
I use it, too.

> 
> Thank you in advance
> 
> Xiao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From hardouin at cebc.cnrs.fr  Fri Mar 12 09:45:36 2004
From: hardouin at cebc.cnrs.fr (=?ISO-8859-1?Q?Hardouin_Lo=EFc?=)
Date: Fri, 12 Mar 2004 09:45:36 +0100
Subject: [R] performing type III
In-Reply-To: <OF3F5617D6.0A414240-ONC1256E55.002CD84F@tomware.it>
References: <OF3F5617D6.0A414240-ONC1256E55.002CD84F@tomware.it>
Message-ID: <405178B0.4090203@cebc.cnrs.fr>


Sorry to have not precised that I used the anova function in the car 
package but I did.
I try to fit clutch size with two covariables and a factor (with two 
levels). My purpose is to compare results with type I and III sum of 
squares and furhter perform model simplification. The data set contains 
no missing value.
The model run and provided only type I response with the warning message 
(see below)

 > sq<-lm(tponte~logavril*reldponte*exp,data=wa,contrasts=sum)
 > anova(sq,type=c("I","III"))
Analysis of Variance Table

Response: tponte
                       Df Sum Sq Mean Sq F value    Pr(>F) 
 logavril                 1 27.930  27.930 52.5041 1.385e-11 ***
reldponte                1 23.442  23.442 44.0670 3.984e-10 ***
exp                      1  2.657   2.657  4.9940   0.02672 *
logavril:reldponte       1  0.281   0.281  0.5292   0.46795  
logavril:exp             1  0.925   0.925  1.7383   0.18911 
reldponte:exp            1  0.016   0.016  0.0307   0.86105  
logavril:reldponte:exp   1  1.052   1.052  1.9771   0.16150  
Residuals              172 91.497   0.532                     ---
 
Warning message:
Models with response "NULL" removed because response differs from model 
1 in: anova.lmlist(object, ...)
 >

Hope to be clearer,
Thanks,

Alex

>  
>



From rksh at soc.soton.ac.uk  Fri Mar 12 10:10:51 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 12 Mar 2004 09:10:51 +0000
Subject: [R] another do.call() problem.
Message-ID: <a0600200dbc772d3d048f@[139.166.242.29]>

Hi everyone

suppose I have

a <- array(1:256,rep(4,4))

and want to access a[1,2,3,1] by the vector c(1,2,3,1).  As per 
yesterday, I can use do.call():

a[1,2,3,1] == do.call("[",c(list(a),c(1,2,3,1)))

Now how do I apply the above technique (or indeed any other technique!)  to get

a[1,2,3,]

[1]  37 101 165 229

from a vector like c(1,2,3,0) or c(1,2,3,NULL) or c(1,2,3,NA)?



OBattempts:

do.call("[",c(list(a),c(1,2,3)))
do.call("[",c(list(a),c(1,2,3),NULL))
do.call("[",c(list(a),c(1,2,3,NULL)))
do.call("[",c(list(a),c(1,2,3,",")))

none of these give what I want.

Anyone?
-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From albedo at pisem.net  Fri Mar 12 10:23:30 2004
From: albedo at pisem.net (Albedo)
Date: Fri, 12 Mar 2004 11:23:30 +0200
Subject: [R] two instances of R 
Message-ID: <40518192.105@pisem.net>

Hello.

Does anoby know if it's safe to run two instances of R
(in R CMD BATCH mode) in the same directory ?

If so, is it safe to run them on different computers
(with access to the same filesystem via NFS) ?

Thanks.



From Simon.Fear at synequanon.com  Fri Mar 12 10:43:27 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 12 Mar 2004 09:43:27 -0000
Subject: [R] Summary: do.call and environments
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F0221C@synequanon01>

It is possible to fake something very like dynamic scoping 
in S-PLUS as in the following example:

fx <- function(y) print(get("x", inherit=T) * y)
fxx <- function(fun,x) fun(3)
fxx(fx,2)
[1] 6

As several posters already pointed out; this strategy will
not always do what you want (it will pick up the first x defined in
the frames stack, rather than necessarily the x passed to fxx, 
in a more complicated nested call) - that's the downside of 
dynamic scoping.

In R the above will not do what you want at all; fx will still be 
evaluated in the environment in which it was defined, and will
never read the x passed to fxx.

You COULD do this in R:

fx <- function(y) print(get("x", envir=sys.parent()) * y)

but that will only work for fx called from fxx directly - you cannot
set both envir and inherits arguments in R (nor both frame and 
inherit in S-PLUS).

But in R you can do (thanks Tony, mod Gabor)

fx <- function(y) print(x*y)
fxx <- function(fun, x) {
        environment(fun) <- environment()
        fun(3)
}

And that's a lovely example of the power of lexical scoping,
guaranteeing that the x of the fxx call will be used. [Provided
you don't reset the environment again before calling fun, but 
why would you ever do that?]

Incidentally, anyone for golf? How about

fxx <- function(fun,x) eval(parse(text=deparse(fun)))(3)

The above won't work in S-PLUS by the way, not even if you
force the eval in sys.parent(). That's the kind of thing that used
to drive me mad in S-PLUS - why, oh why, doesn't it work? [Don't
answer that.]

IMHO there isn't going to be a solution that works in both R and
S-PLUS - now there's asking to be shot down in flames - or if there is,
it would not be natural to either dialect. Scoping is precisely where
R and S-PLUS do not match.

Having said that I wonder if the original poster really needed to
get into this problem at all, what is wrong with

fxy <- function(x,y) print(x*y)
ffxy <- function(fun,x,y) fun(x,y)
ffxy(fxy,2,3)

IE just pass x as an argument. That works in all S.

HTH
Simon  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From B.Rowlingson at lancaster.ac.uk  Fri Mar 12 11:01:41 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 12 Mar 2004 10:01:41 +0000
Subject: [R] Questions about spatial data
In-Reply-To: <4050B9B0.5020400@lancaster.ac.uk>
References: <200403111218.35905.chrysopa@insecta.ufv.br>
	<4050B9B0.5020400@lancaster.ac.uk>
Message-ID: <40518A85.5020903@lancaster.ac.uk>


>  Your grid above has 8*6 = 42 points. Do you want to generate a random 

  That was a subtle Hitchhikers Guide To The Galaxy reference there, 
honest,  and not a stupid dumb multiplication mistake on my part after 
working four 18-hour days on the trot...

  I will now write out 1000 times, 6 times 8 is 48, seven eights are 
fifty-six, eight eights are.. oh hang on I've run out of fingers...

Barry



From p.dalgaard at biostat.ku.dk  Fri Mar 12 11:20:31 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 11:20:31 +0100
Subject: [R] performing type III
In-Reply-To: <405178B0.4090203@cebc.cnrs.fr>
References: <OF3F5617D6.0A414240-ONC1256E55.002CD84F@tomware.it>
	<405178B0.4090203@cebc.cnrs.fr>
Message-ID: <x2ekrytlhs.fsf@biostat.ku.dk>

Hardouin Lo?c <hardouin at cebc.cnrs.fr> writes:

> Sorry to have not precised that I used the anova function in the car
> package but I did.

No you didn't. There's no anova() function in the car package. There
is an Anova() function, though.

>  > sq<-lm(tponte~logavril*reldponte*exp,data=wa,contrasts=sum)
>  > anova(sq,type=c("I","III"))

and that gives you the non-car anova() results.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From migreja at med.up.pt  Fri Mar 12 11:42:28 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?J=FAlia?= Rodrigues Igreja)
Date: Fri, 12 Mar 2004 10:42:28 -0000
Subject: [R] read.spss
Message-ID: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040312/ac25f4e1/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Mar 12 11:45:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 11:45:38 +0100
Subject: [R] another do.call() problem.
In-Reply-To: <a0600200dbc772d3d048f@[139.166.242.29]>
References: <a0600200dbc772d3d048f@[139.166.242.29]>
Message-ID: <x2ad2mtkbx.fsf@biostat.ku.dk>

Robin Hankin <rksh at soc.soton.ac.uk> writes:

> Hi everyone
> 
> suppose I have
> 
> a <- array(1:256,rep(4,4))
> 
> and want to access a[1,2,3,1] by the vector c(1,2,3,1).  As per
> yesterday, I can use do.call():
> 
> a[1,2,3,1] == do.call("[",c(list(a),c(1,2,3,1)))
> 
> Now how do I apply the above technique (or indeed any other technique!)  to get
> 
> a[1,2,3,]
> 
> [1]  37 101 165 229
> 
> from a vector like c(1,2,3,0) or c(1,2,3,NULL) or c(1,2,3,NA)?

The middle one is impossible. The other two can be done like this

  v <- c(1,2,3,NA)
  al <- lapply(v,function(x) if(is.na(x)) alist(a=)$a else x )
  do.call("[",c(list(a),al))

This is in treacherous territory: The missing argument indicator has
some very strange semantics, for instance if you save it in a
variable, then any subsequent use of that variable will trigger the
same error as when you access a missing argument in a function:

  mis <- alist(a=)$a
  lapply(v,function(x)if(is.na(x))mis else x )
Error in FUN(X[[4]], ...) : Argument "mis" is missing, with no default

Technically, the missing argument is implemented as a zero-length
variable name 

> as.character(alist(a=)$a)
[1] ""
> mode(alist(a=)$a)
[1] "name"

However, I don't think it is a good idea to rely on that, and anyway,
as.name("") is forbidden:

> as.name("")
Error in as.name("") : attempt to use zero-length variable name

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Mar 12 11:51:56 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 11:51:56 +0100
Subject: [R] Questions about spatial data
In-Reply-To: <40518A85.5020903@lancaster.ac.uk>
References: <200403111218.35905.chrysopa@insecta.ufv.br>
	<4050B9B0.5020400@lancaster.ac.uk> <40518A85.5020903@lancaster.ac.uk>
Message-ID: <x265datk1f.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> >  Your grid above has 8*6 = 42 points. Do you want to generate a
> > random
> 
>   That was a subtle Hitchhikers Guide To The Galaxy reference there,
> honest,  and not a stupid dumb multiplication mistake on my part after
> working four 18-hour days on the trot...

Maybe that's why we got a letter from "Ford Prefect" today? 

Don't panic, just throw yourself at the ground and miss...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ccleland at optonline.net  Fri Mar 12 11:53:17 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 12 Mar 2004 05:53:17 -0500
Subject: [R] read.spss
In-Reply-To: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
References: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
Message-ID: <4051969D.6080600@optonline.net>

Margarida J?lia Rodrigues Igreja wrote:
> Hi,
> I would like to read a spss file in R.
> When i type read.spss("...")
> Comes the error: couldn't find function "read.spss"
> What shall i do?

   Doing help.search("read.spss") would tell you that it's in the 
foreign package.  You need to do

library(foreign)
read.spss("...")

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From mmarques at inescporto.pt  Fri Mar 12 11:58:59 2004
From: mmarques at inescporto.pt (MMarques Power)
Date: Fri, 12 Mar 2004 10:58:59 +0000
Subject: [R] read.spss
In-Reply-To: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
References: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
Message-ID: <15967963656.20040312105859@power.inescn.pt>

Hello Margarida,

Tuesday, March 4, 1997, 2:21:00 AM, you wrote:

> Hi,
> I would like to read a spss file in R.
> When i type read.spss("...")
> Comes the error: couldn't find function "read.spss"
> What shall i do?
> Margarida


"read.spss()" does not exists in the plain R,
it requires the correct package "foreign package" ...

A simple solution would be to export the data from Spss
to CSV format and then read something with

scan() or read.table()  functions

But if not possible to export the foreign package is the right way to
go.

try ?scan or ?read.table parameters .




Marco Marques
INESC Porto



From Ivar.Herfindal at bio.ntnu.no  Fri Mar 12 11:59:59 2004
From: Ivar.Herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Fri, 12 Mar 2004 11:59:59 +0100
Subject: [R] read.spss
In-Reply-To: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
References: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
Message-ID: <opr4qwt9q4ndboo6@mail.bio.ntnu.no>

perhaps

library(foreign)
?read.spss


Ivar Herfindal


On Tue, 04 Mar 1997 02:21:00 +0000, Margarida J?lia Rodrigues Igreja 
<migreja at med.up.pt> wrote:

> Hi,
> I would like to read a spss file in R.
> When i type read.spss("...")
> Comes the error: couldn't find function "read.spss"
> What shall i do?
>
> Margarida
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Mar 12 12:04:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Mar 2004 12:04:08 +0100
Subject: [R] read.spss
In-Reply-To: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
References: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
Message-ID: <40519928.1020504@statistik.uni-dortmund.de>

Margarida J?lia Rodrigues Igreja wrote:
> Hi,
> I would like to read a spss file in R.
> When i type read.spss("...")
> Comes the error: couldn't find function "read.spss"
> What shall i do?

It's in package "foreign", so use
  library(foreign)
before calling read.spss().

Uwe Ligges


> Margarida
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Mar 12 12:05:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Mar 2004 12:05:08 +0100
Subject: [R] two instances of R
In-Reply-To: <40518192.105@pisem.net>
References: <40518192.105@pisem.net>
Message-ID: <40519964.3080406@statistik.uni-dortmund.de>

Albedo wrote:

> Hello.
> 
> Does anoby know if it's safe to run two instances of R
> (in R CMD BATCH mode) in the same directory ?
> 
> If so, is it safe to run them on different computers
> (with access to the same filesystem via NFS) ?

Yes, if you don't use the same files to write anything into ...

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Fri Mar 12 12:04:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 12:04:51 +0100
Subject: [R] two instances of R
In-Reply-To: <40518192.105@pisem.net>
References: <40518192.105@pisem.net>
Message-ID: <x21xnytjfw.fsf@biostat.ku.dk>

Albedo <albedo at pisem.net> writes:

> Hello.
> 
> Does anoby know if it's safe to run two instances of R
> (in R CMD BATCH mode) in the same directory ?
> 
> If so, is it safe to run them on different computers
> (with access to the same filesystem via NFS) ?

You have to worry about race conditions on file writes (e.g. running
the *same* script twice could be asking for trouble if both processes
want to write foo.Rout). Also watch out for seed issues when
simulating, if both processes restore the same seed from a saved
workspace, they'll be generating exactly the same sequence of random
numbers, which may or may not be what you want. Apart from that you
should be OK.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tobias.verbeke at bivv.be  Fri Mar 12 12:06:37 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Fri, 12 Mar 2004 12:06:37 +0100
Subject: [R] read.spss
In-Reply-To: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
Message-ID: <OF18694C28.E5250ED6-ONC1256E55.003C66F3-C1256E55.003D06E0@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 04/03/1997 03:21:00:

> Hi,
> I would like to read a spss file in R.
> When i type read.spss("...")
> Comes the error: couldn't find function "read.spss"
> What shall i do?
>

Hi Margarida,

Did you load the foreign package by typing
      library(foreign)
before using the
      read.spss()-command
and did you install the foreign package by typing e.g.
      install.packages(foreign)
before loading the foreign package ?

HTH,
Tobias



From p.dalgaard at biostat.ku.dk  Fri Mar 12 12:06:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 12:06:32 +0100
Subject: [R] read.spss
In-Reply-To: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
References: <4.3.2.7.1.19970304021739.00ad0330@mail.med.up.pt>
Message-ID: <x2wu5qs4sn.fsf@biostat.ku.dk>

Margarida J?lia Rodrigues Igreja <migreja at med.up.pt> writes:

> Hi,
> I would like to read a spss file in R.
> When i type read.spss("...")
> Comes the error: couldn't find function "read.spss"
> What shall i do?

library(foreign)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Mar 12 12:19:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Mar 2004 11:19:54 +0000 (GMT)
Subject: [R] query with parameters using RODBC
In-Reply-To: <855D381F618DE84FA58C035BA803FCE6B9A6BD@FVAFR-SE1.FORST.BWL.DE>
Message-ID: <Pine.LNX.4.44.0403121118440.3583-100000@gannet.stats>

On Thu, 11 Mar 2004, Schmidt.Matthias (FORST) wrote:

> I am using RODBC to conduct Queries in a MSACCESS data base. This works
> quite well, but is it also possible to conduct Queries including parameters
> which can be specified flexibly if the Query is used for example in a R
> function. Something like the function .parambyname in Borland Delphi for
> example.

Perhaps you could tell us what that does?

RODBC is just a wrapper to the basic ODBC routines, so please tell us what 
Delphi is passing to ODBC and how.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From duchesnay at shfj.cea.fr  Fri Mar 12 12:28:48 2004
From: duchesnay at shfj.cea.fr (Edouard DUCHESNAY)
Date: Fri, 12 Mar 2004 12:28:48 +0100
Subject: [R] No traceback available when using try(...)
In-Reply-To: <Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>
References: <200403111208.00088.duchesnay@shfj.cea.fr>
	<Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>
Message-ID: <200403121228.48061.duchesnay@shfj.cea.fr>

> > 1. The Situation :
> > ------------------------
> > The stack traceback is not available when error ouccured in a try(....)
>
> It's a bug in 1.8.1. It has been fixed.
>
> 	-thomas
I have patched R (Version 1.8.1 Patched (2004-03-11)) and It stills not working.
- R was compiled without any option (only a --prefix=)
- I use a Linux RedHat 9


-- 
Edouard Duchesnay                      Tel: +33 1 69 86 78 52
CEA - SHFJ                             Fax: +33 1 69 86 77 86
4, place du G?n?ral Leclerc
91401 Orsay Cedex France



From B.Rowlingson at lancaster.ac.uk  Fri Mar 12 12:38:46 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 12 Mar 2004 11:38:46 +0000
Subject: [R] two instances of R
In-Reply-To: <40518192.105@pisem.net>
References: <40518192.105@pisem.net>
Message-ID: <4051A146.1000604@lancaster.ac.uk>

Albedo wrote:
> Hello.
> 
> Does anoby know if it's safe to run two instances of R
> (in R CMD BATCH mode) in the same directory ?
> 
> If so, is it safe to run them on different computers
> (with access to the same filesystem via NFS) ?
> 

  "Is it safe?". Heh. Marathon Man was on TV last night...

  Depends on your definition of 'safe'. The processes will run quite 
happily, they will read whichever .RData they find by the usual search 
methods.

  However, if they come to _write_ a .RData, you should make sure they 
dont try to write to the same file, since if they do that at the same 
time bad things could happen, and if they do it sequentially then the 
first one will get overwritten by the last one.

  If you do "R CMD BATCH --no-save" then R won't try and save the .RData 
at the end, so if all your results are going to the output file then you 
dont need to save .RData and can use this option.

Barry



From dnogues at ipe.csic.es  Fri Mar 12 12:47:14 2004
From: dnogues at ipe.csic.es (=?ISO-8859-1?Q?David_Nogu=E9s?=)
Date: Fri, 12 Mar 2004 12:47:14 +0100
Subject: [R] GCV UBRE score in GAM models
Message-ID: <4051A342.7070805@ipe.csic.es>

hello to everybody:

I would to know with ranges  of GCV or UBRE values can be considered as 
adequate to consider a GAM as correct

Thanks in advance

-- 
David Nogu?s Bravo

Functional Ecology and Biodiversity Department
Pyrenean Institute of Ecology
Spanish Research Council

Av. Monta?ana 1005
Zaragoza - CP 50059
976716030 - 976716019 (fax)



From ripley at stats.ox.ac.uk  Fri Mar 12 12:59:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Mar 2004 11:59:42 +0000 (GMT)
Subject: [R] fft question
In-Reply-To: <20040312084217.GA26201@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <Pine.LNX.4.44.0403121156260.3583-100000@gannet.stats>

On Fri, 12 Mar 2004, Frank Gerrit Zoellner wrote:

> On Thu, Mar 11, 2004 at 06:14:02PM +0100, Martin Maechler wrote:
> 
> > 
> > Yes, manually, like
> > 
> >   fx <- fft(rep(x, 4))
> > 
> 
> I think rep works on a vector but in my case x is a dataframe/matrix with the signal along the rows.
> 
> Does rep work on dataframes ?

Yes, but it does not do what you want I believe.

A data frame (sic) is a list, and so the columns are replicated to form a 
list.

Row indexing will work for a data frame or matrix. Say

fft(x[rep(1:now(x), 4), ])

?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rasch_sub at med1.med.tum.de  Fri Mar 12 13:15:24 2004
From: rasch_sub at med1.med.tum.de (Raphael Schneider)
Date: Fri, 12 Mar 2004 13:15:24 +0100
Subject: [R] fft question
In-Reply-To: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040311150427.GA24788@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <200403121315.25442.rasch_sub@med1.med.tum.de>

Hi,

On Thursday 11 March 2004 16:04, Frank Gerrit Zoellner wrote:
> I am using the fft() function the base package to transform some 1d signal.
> If I use this standar fucntion I get a very huge first fourier coeficient.
In electronics, the first coefficient is the dc-component of the signal. If 
you remove the mean from the signal, the first coefficient should be zero.

> I think this dues to the handling of the borders of the signal.
> Usually in fft especially in image processing the signal is simulated to be
> continuous by adding the signal several times periodically.
If you doing a signal analysis of real signals with FFT, you normaly apply a  
window function to the data (e.g. Hamming window) so the begin and end of the 
signal is smoothly going down to zero. The reason doing this is that the 
Fourier transformation "thinks" it analyses a signal with infinit length. So 
the Fourier transformation "adds" the signal infinitely at the end of the 
signal. If the start and end of the signal is not the same value, then you 
have signal with a dis-continuity (the step from the end of the signal to the 
begin of the "added" signal). which adds a lot of noise in your frequency 
components.

Raphael
www.librasch.org



From Christoph.Scherber at uni-jena.de  Fri Mar 12 13:38:02 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 12 Mar 2004 13:38:02 +0100
Subject: [R] quantile regression
Message-ID: <4051AF2A.7060607@uni-jena.de>

Dear colleagues,

How can I do quantile regression with R?

Best regards
Chris.



From andy_liaw at merck.com  Fri Mar 12 14:06:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Mar 2004 08:06:55 -0500
Subject: [R] confidence interval in local polynomial regression
Message-ID: <3A822319EB35174CA3714066D590DCD504AF798D@usrymx25.merck.com>

The predict method for locfit object (in the `locfit' package) can do this.
For details, see Loader's book.

HTH,
Andy

> From: Z P
> 
> Dear all,
> 
> Is there any package or function can do the pointwise 
> confidence interval 
> and confidence band for the local polynomial regression? 
> Maybe the local 
> linear regression is enough. Thanks.
> 
> Regards,
> 
> Zhen


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From jfox at mcmaster.ca  Fri Mar 12 14:45:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 12 Mar 2004 08:45:21 -0500
Subject: [R] performing type III
In-Reply-To: <405178B0.4090203@cebc.cnrs.fr>
Message-ID: <20040312134521.NUPL7291.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Hardouin,

It has already been mentioned that the Anova() function in the car package
has a name beginning with an uppercase "A".

There are some other problems here as well:

(1) Unless you've written a contrasts function called sum(), and even if you
have, contrasts=sum isn't going to do anything. The contrasts argument is
supposed to be a list. See ?lm and ?contr.sum.

(2) The Anova() function in the car library doesn't calculate "type-I" sums
of squares; you can use anova() for this -- as you've done inadvertently.

(3) The type argument to the Anova() function takes *one* of "II" or "III"
as a value. See ?Anova.

(4) You describe this as an analysis of covariance. Type III sums of squares
for the "main" effects won't be sensible here unless the covariates are
expressed as deviations from their means (or some other reference point).

John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hardouin Lo?c
> Sent: Friday, March 12, 2004 3:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] performing type III
> 
> 
> Sorry to have not precised that I used the anova function in 
> the car package but I did.
> I try to fit clutch size with two covariables and a factor 
> (with two levels). My purpose is to compare results with type 
> I and III sum of squares and furhter perform model 
> simplification. The data set contains no missing value.
> The model run and provided only type I response with the 
> warning message (see below)
> 
>  > sq<-lm(tponte~logavril*reldponte*exp,data=wa,contrasts=sum)
>  > anova(sq,type=c("I","III"))
> Analysis of Variance Table
> 
> Response: tponte
>                        Df Sum Sq Mean Sq F value    Pr(>F) 
>  logavril                 1 27.930  27.930 52.5041 1.385e-11 ***
> reldponte                1 23.442  23.442 44.0670 3.984e-10 ***
> exp                      1  2.657   2.657  4.9940   0.02672 *
> logavril:reldponte       1  0.281   0.281  0.5292   0.46795  
> logavril:exp             1  0.925   0.925  1.7383   0.18911 
> reldponte:exp            1  0.016   0.016  0.0307   0.86105  
> logavril:reldponte:exp   1  1.052   1.052  1.9771   0.16150  
> Residuals              172 91.497   0.532                     ---
>  
> Warning message:
> Models with response "NULL" removed because response differs 
> from model
> 1 in: anova.lmlist(object, ...)
>  >
> 
> Hope to be clearer,
> Thanks,
> 
> Alex
> 
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From =?iso-8859-1?Q?Micha=EBl?=  Fri Mar 12 14:45:46 2004
From: =?iso-8859-1?Q?Micha=EBl?= (=?iso-8859-1?Q?Micha=EBl?=)
Date: Fri, 12 Mar 2004 14:45:46 +0100
Subject: [R] Basic questions on nls and bootstrap
Message-ID: <5.0.2.1.2.20040312144322.00b78c60@utinam.univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040312/351bd50d/attachment.pl

From dmurdoch at pair.com  Fri Mar 12 15:07:20 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 12 Mar 2004 09:07:20 -0500
Subject: [R] No traceback available when using try(...)
In-Reply-To: <200403121228.48061.duchesnay@shfj.cea.fr>
References: <200403111208.00088.duchesnay@shfj.cea.fr>
	<Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>
	<200403121228.48061.duchesnay@shfj.cea.fr>
Message-ID: <cjg350pukspus4rmfrshbuofqnf77218fm@4ax.com>

On Fri, 12 Mar 2004 12:28:48 +0100, Edouard DUCHESNAY
<duchesnay at shfj.cea.fr> wrote :

>> > 1. The Situation :
>> > ------------------------
>> > The stack traceback is not available when error ouccured in a try(....)
>>
>> It's a bug in 1.8.1. It has been fixed.
>>
>> 	-thomas
>I have patched R (Version 1.8.1 Patched (2004-03-11)) and It stills not working.
>- R was compiled without any option (only a --prefix=)
>- I use a Linux RedHat 9

Edouard is right, I still see the error in a current 1.9.0 alpha
build:

> f<-function(a){
+   return ( log(a) )
+ }
> f("A")
Error in log(x) : Non-numeric argument to mathematical function
> traceback()
2: log(a)
1: f("A")
> try(f("A"))
Error in log(x) : Non-numeric argument to mathematical function
> traceback()
No traceback available



Duncan Murdoch



From simon at stats.gla.ac.uk  Fri Mar 12 15:15:26 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Fri, 12 Mar 2004 14:15:26 +0000 (GMT)
Subject: [R] GCV UBRE score in GAM models
In-Reply-To: <4051A342.7070805@ipe.csic.es>
References: <4051A342.7070805@ipe.csic.es>
Message-ID: <Pine.SOL.4.58.0403121407140.22985@moon.stats.gla.ac.uk>

> I would to know with ranges  of GCV or UBRE values can be considered as
> adequate to consider a GAM as correct

I don't think that there is really an answer to this. It's a bit like
asking what range of values of the residual sum of squares means that a
linear model is correct. I've only seen the GCV score used as a statistic
for judging the relative appropriateness of models, not the `absolute'
appropriateness.

For detecting `incorrectness' of a GAM I'd look at residual plots and in
cases where the scale parameter ought to be one, check that the residuals
are consistent with this.

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From andel at ifi.unizh.ch  Fri Mar 12 15:30:42 2004
From: andel at ifi.unizh.ch (David Andel)
Date: 12 Mar 2004 15:30:42 +0100
Subject: [R] double vs. list
Message-ID: <4051C992.8000508@ifi.unizh.ch>

Hi all

I've got trouble with getting a "list" object and not being able to 
change that:

Lets say x is a simple matrix and I want to remove all elements <3 to get
3
5 7

 > x <- matrix(c(1,2,3,5,1,7), nrow=2, ncol=3, byrow=T)
 > x
      [,1] [,2] [,3]
[1,]    1    2    3
[2,]    5    1    7
 > typeof(x)
[1] "double"
 > y <- sapply(c(1,2), function(i) which(x[i,]<3))
 > y
[[1]]
[1] 1 2

[[2]]
[1] 2

 > typeof(y)
[1] "list"

Of course I could remove these elements right away and produce a "x" 
without elements <3, but also of type "list".
But because rule will be more complicated than this I need to know the 
elements to remove explicitely first.

Now when trying to remove elements 1 and 2 from x[1,], I get an error:
 > y[1]
[[1]]
[1] 1 2

 > x[1,][-y[1]]
Error in -y[1] : Invalid argument to unary operator

But this works:

 > x[1,][-c(1,2)]
[1] 3

What can I do differently?


Thanks,
David



From andy_liaw at merck.com  Fri Mar 12 15:44:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Mar 2004 09:44:44 -0500
Subject: [R] quantile regression
Message-ID: <3A822319EB35174CA3714066D590DCD504AF798E@usrymx25.merck.com>

Please read the footer of the message, and follow the link.  Besides, you
don't need people googling for you, do you?

Andy

> From: Christoph Scherber
> 
> Dear colleagues,
> 
> How can I do quantile regression with R?
> 
> Best regards
> Chris.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From p.dalgaard at biostat.ku.dk  Fri Mar 12 15:48:27 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 15:48:27 +0100
Subject: [R] quantile regression
In-Reply-To: <4051AF2A.7060607@uni-jena.de>
References: <4051AF2A.7060607@uni-jena.de>
Message-ID: <x2oer2ruis.fsf@biostat.ku.dk>

Christoph Scherber <Christoph.Scherber at uni-jena.de> writes:

> Dear colleagues,
> 
> How can I do quantile regression with R?

Package quantreg springs to mind...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ma at bioinf.uni-hannover.de  Fri Mar 12 16:04:55 2004
From: ma at bioinf.uni-hannover.de (Dong Hui Ma)
Date: Fri, 12 Mar 2004 16:04:55 +0100
Subject: [R] add a vertical line in a 2d histogram !
Message-ID: <5.1.0.14.0.20040312160305.00a74008@Mail.bioinf.uni-hannover.de>

Dear all,
I have a stupid question!
Just want to know is there some function like "abline" can be used
in 3 dimensional case?

Thank you in advance!"

Donghui Ma



From migreja at med.up.pt  Fri Mar 12 16:08:32 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?J=FAlia?= Rodrigues Igreja)
Date: Fri, 12 Mar 2004 15:08:32 -0000
Subject: [R] spss
Message-ID: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>

hi,
i can?t download a file in access.

when i type:
 > library(foreign)
 >  read.spss("H:\Desktop\bd1\experiencia1")

comes the error:
Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file


do you know what is the problem?
can you help me?

margarida,porto,portugal



From andy_liaw at merck.com  Fri Mar 12 16:08:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Mar 2004 10:08:09 -0500
Subject: [R] double vs. list
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7990@usrymx25.merck.com>

> From: David Andel
> 
> Hi all
> 
> I've got trouble with getting a "list" object and not being able to 
> change that:
> 
> Lets say x is a simple matrix and I want to remove all 
> elements <3 to get
> 3
> 5 7
> 
>  > x <- matrix(c(1,2,3,5,1,7), nrow=2, ncol=3, byrow=T)
>  > x
>       [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    5    1    7
>  > typeof(x)
> [1] "double"
>  > y <- sapply(c(1,2), function(i) which(x[i,]<3))
>  > y
> [[1]]
> [1] 1 2
> 
> [[2]]
> [1] 2
> 
>  > typeof(y)
> [1] "list"
> 
> Of course I could remove these elements right away and produce a "x" 
> without elements <3, but also of type "list".
> But because rule will be more complicated than this I need to 
> know the 
> elements to remove explicitely first.
> 
> Now when trying to remove elements 1 and 2 from x[1,], I get an error:
>  > y[1]
> [[1]]
> [1] 1 2
> 
>  > x[1,][-y[1]]
> Error in -y[1] : Invalid argument to unary operator
> 
> But this works:
> 
>  > x[1,][-c(1,2)]
> [1] 3
> 
> What can I do differently?

Use double brackets to access a list component.  Single bracket gives you
back a list with one component.

I.e., try:

x[1,][-y[[1]]]

And BTW, isn't this really what you want?

> apply(x, 1, function(x) x[x>=3])
[[1]]
[1] 3

[[2]]
[1] 5 7

HTH,
Andy

> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Christoph.Scherber at uni-jena.de  Fri Mar 12 16:13:03 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Fri, 12 Mar 2004 16:13:03 +0100
Subject: [R] quantile regression
In-Reply-To: <4051AF2A.7060607@uni-jena.de>
References: <4051AF2A.7060607@uni-jena.de>
Message-ID: <4051D37F.7000509@uni-jena.de>

OK, Thank you all very much for the help!

Best regards
Chris.



Christoph Scherber wrote:

> Dear colleagues,
>
> How can I do quantile regression with R?
>
> Best regards
> Chris.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From statho3 at web.de  Fri Mar 12 16:15:26 2004
From: statho3 at web.de (Thomas Stabla)
Date: Fri, 12 Mar 2004 16:15:26 +0100 (CET)
Subject: [R] R CMD check errors
In-Reply-To: <4050A83D.2080603@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0403121605370.1936-100000@spock.vulcan>

Hello again,

I tried to isolate the source code, which causes the error messages:

* checking S3 generic/method consistency ... WARNING
Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
        package/namespace load failed
* checking for replacement functions with final arg not named 'value' ... WARNING
Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
        package/namespace load failed


I created a new package skeleton by calling package.skeleton(name =
"error") from R 1.8.1.

I deleted the subdirectories 'data', 'src' and 'man'.

The NAMESPACE file contains:

  exportPattern("^[^\\.]")

The Error.R file in the subdirectory 'R' contains

  x <- 1
  setClass("Parameter", setClass("Parameter", representation(name = "character"))


This small "package" produces already the errors mentioned above, if R
CMD check Error is called (R 1.8.1 and also R 1.9.x).
The point is, as soon as I remove the line with the setClass-call,
then R CMD check Error runs "without" errors (there is a warning about
an empty 'INDEX' file).

If I remove the NAMESPACE file and call R CMD check (after build), I get
these errors:

* checking S3 generic/method consistency ... WARNING
Error in .tryQuietly({ : Error in eval(expr, envir, enclos) : couldn't find function "setClass"
Execution halted
* checking for replacement functions with final arg not named 'value' ... WARNING
Error in .tryQuietly({ : Error in eval(expr, envir, enclos) : couldn't find function "setClass"
Execution halted


Thanks for your help,
Thomas Stabla


Error directory available at:
http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/Error.tar.gz



From ccleland at optonline.net  Fri Mar 12 16:27:17 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 12 Mar 2004 10:27:17 -0500
Subject: [R] spss
In-Reply-To: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
References: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
Message-ID: <4051D6D5.9090709@optonline.net>

Margarida J?lia Rodrigues Igreja wrote:
> i can?t download a file in access.
> 
> when i type:
>  > library(foreign)
>  >  read.spss("H:\Desktop\bd1\experiencia1")
> 
> comes the error:
> Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file

   From the R for Windows FAQ...

R can't find my file, but I know it is there!

How did you specify it? Backslashes have to be doubled in R 
character strings, so for example one needs 
"d:\\rw1081\\library\\xgobi\\scripts\\xgobi.bat". Make life 
easier for yourself by using forward slashes as path separators: 
they do work under Windows.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Ivar.Herfindal at bio.ntnu.no  Fri Mar 12 16:29:14 2004
From: Ivar.Herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Fri, 12 Mar 2004 16:29:14 +0100
Subject: [R] spss
In-Reply-To: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
References: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
Message-ID: <opr4q9a0sindboo6@mail.bio.ntnu.no>

Hello

>From the error message, it seems like you should write something like this:

library(foreign)
read.spss("H:\\Desktop\\bd1\\experiencia1")

Ivar Herfindal

On Tue, 04 Mar 1997 06:47:01 +0000, Margarida J?lia Rodrigues Igreja 
<migreja at med.up.pt> wrote:

> hi,
> i can?t download a file in access.
>
> when i type:
> > library(foreign)
> >  read.spss("H:\Desktop\bd1\experiencia1")
>
> comes the error:
> Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file
>
>
> do you know what is the problem?
> can you help me?
>
> margarida,porto,portugal
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From luke at stat.uiowa.edu  Fri Mar 12 16:38:51 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Fri, 12 Mar 2004 09:38:51 -0600 (CST)
Subject: [R] No traceback available when using try(...)
In-Reply-To: <200403111208.00088.duchesnay@shfj.cea.fr>
Message-ID: <Pine.LNX.4.44.0403120844560.11959-100000@itasca.stat.uiowa.edu>

I believe this is the way try() has interacted with the traditional
error handling in R for many years, for better or worse.  The check
for a try() on the stack, and jump to it if one is found, happens
before the traceback is written.  In some cases this behavior is what
one wants, in others it may not be.  Writing the traceback is also
turned off by defining an error option.  That seems a bit less
sensible to me, but again has been that way for some time, and it may
be what is wanted in some cases.

The new error handling system is intended to provide more flexibility.
If you want to silence the error but capture traceback information you
can use a calling handler to capture the calls on the stack with
sys.calls and then jump to a restart.  Something like this:

myTry <- function(expr) {
    withRestarts(
        withCallingHandlers(expr,
                            error = function(e) {
                                t <- sys.calls()
                                invokeRestart("myAbort", e, t)
                            }),
        myAbort = function(e, t)
            list(error = e, traceback = t))
}

[Because of lazy argumetn evaluation you have to force evaluation of t
in the handler.]

[It might be safer to use a restart naming convention like
"mypackage::myAport" to avoid conflicts.]

Then you get
                    
> myTry(1+2)
[1] 3
> myTry(f("A"))
$error
<simpleError in log(x): Non-numeric argument to mathematical function>
 
$traceback
$traceback[[1]]
myTry(f("A"))
 
$traceback[[2]]
withRestarts(withCallingHandlers(expr, error = function(e) {
    t <- sys.calls()
    invokeRestart("myAbort", e, t)
}), myAbort = function(e, t) list(error = e, traceback = t))
 
$traceback[[3]]
withOneRestart(expr, restarts[[1]])
 
$traceback[[4]]
doWithOneRestart(return(expr), restart)
 
$traceback[[5]]
withCallingHandlers(expr, error = function(e) {
    t <- sys.calls()
    invokeRestart("myAbort", e, t)
})
 
$traceback[[6]]
f("A")
 
$traceback[[7]]
log(a)
 
$traceback[[8]]
.handleSimpleError(function (e)
{
    t <- sys.calls()
    invokeRestart("myAbort", e, t)
}, "Non-numeric argument to mathematical function", quote(log(x)))
 
$traceback[[9]]
h(simpleError(msg, call))

You may want to trim some of the error handling internals out of the
sys.calls or something like that (we might want to think about
providing a function that does this at some point).

Hope that helps.

luke

On Thu, 11 Mar 2004, Edouard DUCHESNAY wrote:

> Hello,
> 
> 1. The Situation :
> ------------------------
> The stack traceback is not available when error ouccured in a try(....)
> -- test.R --------------------------------
> f<-function(a){
>   return ( log(a) )
> }
> try(f("A"))
> traceback()
> -------------------------------------------
> 
> I get the following message :
> > try(f("A"))
> Error in log(x) : Non-numeric argument to mathematical function
> > traceback()
> No traceback available
> 
> I try to use tryCatch(...) instead, I could get the stack traceback (using the finally), 
> but the R program stop.
> 
> 2. My goal
> --------------------
> All I need is to prevent a error to stop a program (like try() does), print the traceback 
> just after the try(....), and get the error message (geterrmessage()).
> 
> 
> 3. Question
> -------------------
> A) stack traceback is not available when error ouccured in a try is a normal behavior ?
> B) Do you have any sugestion to fill my goal ?
> C) If not is it a good idea to investigate around withRestarts(expr, ...) ?
> 
> 
> Best regards
> 
> 
> PS : My R Version 1.8.1  (2003-11-21)
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From migreja at med.up.pt  Fri Mar 12 16:45:05 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?J=FAlia?= Rodrigues Igreja)
Date: Fri, 12 Mar 2004 15:45:05 -0000
Subject: [R] still spss
Message-ID: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>

hi again,

i still cannot open the file in spss :(

i type:
library(foreign)
read.spss("H:\\Desktop\\bd1\\experiencia1")

and the error comes:
Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable to open file

can you help me?

margarida,portugal



From tlumley at u.washington.edu  Fri Mar 12 16:47:03 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Mar 2004 07:47:03 -0800 (PST)
Subject: [R] spss
In-Reply-To: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
References: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
Message-ID: <Pine.A41.4.58.0403120746300.41302@homer29.u.washington.edu>

On Tue, 4 Mar 1997, Margarida [iso-8859-1] J?lia Rodrigues Igreja wrote:

> hi,
> i can?t download a file in access.
>
> when i type:
>  > library(foreign)
>  >  read.spss("H:\Desktop\bd1\experiencia1")
>
> comes the error:
> Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file
>
>
> do you know what is the problem?

Yes. Read the entry in the Windows FAQ
   "R can't find my file, but I know it is there!"


	-thomas



From rasch at med1.med.tum.de  Fri Mar 12 17:05:51 2004
From: rasch at med1.med.tum.de (Raphael Schneider)
Date: Fri, 12 Mar 2004 17:05:51 +0100
Subject: [R] spss
In-Reply-To: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
References: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
Message-ID: <200403121705.51520.rasch@med1.med.tum.de>

Hi,

On Tuesday 04 March 1997 07:47, Margarida J?lia Rodrigues Igreja wrote:
> when i type:
>  > library(foreign)
>  >  read.spss("H:\Desktop\bd1\experiencia1")
>
> comes the error:
> Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file
I think you should use 'experiencia1.sav' as the filename (add '.sav' to the 
filename). SPSS documents have normaly 'sav' as the extension.

Raphael
www.librasch.org

PS: The first thing I change when installing Windows is that file extensions 
are always shown.
PS2: Could you please check your date and time settings.



From ligges at statistik.uni-dortmund.de  Fri Mar 12 17:29:57 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Mar 2004 17:29:57 +0100
Subject: [R] R CMD check errors
In-Reply-To: <Pine.LNX.4.44.0403121605370.1936-100000@spock.vulcan>
References: <Pine.LNX.4.44.0403121605370.1936-100000@spock.vulcan>
Message-ID: <4051E585.2010909@statistik.uni-dortmund.de>

Thomas Stabla wrote:
> Hello again,
> 
> I tried to isolate the source code, which causes the error messages:
> 
> * checking S3 generic/method consistency ... WARNING
> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
>         package/namespace load failed
> * checking for replacement functions with final arg not named 'value' ... WARNING
> Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc, character.only = TRUE, verbose = FALSE) :
>         package/namespace load failed
> 
> 
> I created a new package skeleton by calling package.skeleton(name =
> "error") from R 1.8.1.
> 
> I deleted the subdirectories 'data', 'src' and 'man'.
> 
> The NAMESPACE file contains:
> 
>   exportPattern("^[^\\.]")


and lacks:

    import("methods")

and for sure you want as well:

    exportClass("Parameter")


Thus, the complete file is:


    import("methods")
    exportClass("Parameter")
    exportPattern("^[^\\.]")


> The Error.R file in the subdirectory 'R' contains
> 
>   x <- 1
>   setClass("Parameter", setClass("Parameter", representation(name = "character"))


Well, above, you mean:
   setClass("Parameter", representation(name = "character"))


Uwe Ligges



> This small "package" produces already the errors mentioned above, if R
> CMD check Error is called (R 1.8.1 and also R 1.9.x).
> The point is, as soon as I remove the line with the setClass-call,
> then R CMD check Error runs "without" errors (there is a warning about
> an empty 'INDEX' file).
> 
> If I remove the NAMESPACE file and call R CMD check (after build), I get
> these errors:
> 
> * checking S3 generic/method consistency ... WARNING
> Error in .tryQuietly({ : Error in eval(expr, envir, enclos) : couldn't find function "setClass"
> Execution halted
> * checking for replacement functions with final arg not named 'value' ... WARNING
> Error in .tryQuietly({ : Error in eval(expr, envir, enclos) : couldn't find function "setClass"
> Execution halted
> 
> 
> Thanks for your help,
> Thomas Stabla
> 
> 
> Error directory available at:
> http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/Error.tar.gz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Mar 12 17:32:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Mar 2004 17:32:12 +0100
Subject: [R] spss
In-Reply-To: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
References: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
Message-ID: <4051E60C.6090707@statistik.uni-dortmund.de>

Margarida J?lia Rodrigues Igreja wrote:

> hi,
> i can?t download a file in access.
> 
> when i type:
>  > library(foreign)
>  >  read.spss("H:\Desktop\bd1\experiencia1")
> 
> comes the error:
> Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file
> 
> 
> do you know what is the problem?
> can you help me?

Reading the R for Windows FAQs helps:

"2.14 R can't find my file, but I know it is there!

How did you specify it? Backslashes have to be doubled in R character 
strings, so for example one needs 
"d:\\rw1081\\library\\xgobi\\scripts\\xgobi.bat". Make life easier for 
yourself by using forward slashes as path separators: they do work under 
Windows."

Uwe Ligges


> margarida,porto,portugal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Mar 12 17:33:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Mar 2004 17:33:55 +0100
Subject: [R] add a vertical line in a 2d histogram !
In-Reply-To: <5.1.0.14.0.20040312160305.00a74008@Mail.bioinf.uni-hannover.de>
References: <5.1.0.14.0.20040312160305.00a74008@Mail.bioinf.uni-hannover.de>
Message-ID: <4051E673.4070208@statistik.uni-dortmund.de>

Dong Hui Ma wrote:

> Dear all,
> I have a stupid question!
> Just want to know is there some function like "abline" can be used
> in 3 dimensional case?
> 
> Thank you in advance!"

May I ask which kind of a histogram in 3 dim. case you mean? Which 
function from which package in which version of R under what OS?

Uwe Ligges



From Scott.Waichler at pnl.gov  Fri Mar 12 17:38:53 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Fri, 12 Mar 2004 08:38:53 -0800
Subject: [R] Sweave and R output: possible to suppress "Schunk" tags in
 *.tex file output?
Message-ID: <62AE0CF1D4875C4BBDEC29DB9924ACE87F21AE@pnlmse25.pnl.gov>

Hello,

I would like to fill the rows of a Latex tabular environment with output from
R, as in

\begin{table}
  \caption{Table caption.} 
  \label{tab:events}
  \begin{tabular}{c r r r r r}
  \hline

<<echo=false,results=tex>>=
fill.my.table.rows()
@

  \end{tabular}
\end{table}

Sweave produces the output inside \begin{Schunk} and \end{Schunk} commands,
which latex doesn't tolerate within a table.  I would prefer to avoid writing all of
the Latex table code from R.  Is my only other option to produce a separate
text file, e.g. under Sweave with results=hide, then use \input{} on the file?

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov



From dmurdoch at pair.com  Fri Mar 12 17:41:00 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 12 Mar 2004 11:41:00 -0500
Subject: [R] add a vertical line in a 2d histogram !
In-Reply-To: <5.1.0.14.0.20040312160305.00a74008@Mail.bioinf.uni-hannover.de>
References: <5.1.0.14.0.20040312160305.00a74008@Mail.bioinf.uni-hannover.de>
Message-ID: <1vp350tu1jc2c5rmm7hj7bmg8ci4coq89p@4ax.com>

On Fri, 12 Mar 2004 16:04:55 +0100, Dong Hui Ma
<ma at bioinf.uni-hannover.de> wrote :

>Dear all,
>I have a stupid question!
>Just want to know is there some function like "abline" can be used
>in 3 dimensional case?

Your question and subject line contradict each other.  Could you give
an example of how you produce your 2d or 3d histogram, and describe
where you want the line to appear?  You can almost certainly do so.

Duncan Murdoch



From rpeng at jhsph.edu  Fri Mar 12 17:44:14 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 12 Mar 2004 11:44:14 -0500
Subject: [R] No traceback available when using try(...)
In-Reply-To: <cjg350pukspus4rmfrshbuofqnf77218fm@4ax.com>
References: <200403111208.00088.duchesnay@shfj.cea.fr>
	<Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>
	<200403121228.48061.duchesnay@shfj.cea.fr>
	<cjg350pukspus4rmfrshbuofqnf77218fm@4ax.com>
Message-ID: <4051E8DE.4010807@jhsph.edu>

Funny, it works for me on R-patched

 > f <- function(a) { return(log(a)) }
 > f("A")
Error in log(x) : Non-numeric argument to mathematical function
 > traceback()
2: log(a)
1: f("A")
 > try(f("A"))
Error in log(x) : Non-numeric argument to mathematical function
 > traceback()
2: log(a)
1: f("A")

This is on

 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   Patched
major    1
minor    8.1
year     2004
month    02
day      26
language R

-roger

Duncan Murdoch wrote:
> On Fri, 12 Mar 2004 12:28:48 +0100, Edouard DUCHESNAY
> <duchesnay at shfj.cea.fr> wrote :
> 
> 
>>>>1. The Situation :
>>>>------------------------
>>>>The stack traceback is not available when error ouccured in a try(....)
>>>
>>>It's a bug in 1.8.1. It has been fixed.
>>>
>>>	-thomas
>>
>>I have patched R (Version 1.8.1 Patched (2004-03-11)) and It stills not working.
>>- R was compiled without any option (only a --prefix=)
>>- I use a Linux RedHat 9
> 
> 
> Edouard is right, I still see the error in a current 1.9.0 alpha
> build:
> 
> 
>>f<-function(a){
> 
> +   return ( log(a) )
> + }
> 
>>f("A")
> 
> Error in log(x) : Non-numeric argument to mathematical function
> 
>>traceback()
> 
> 2: log(a)
> 1: f("A")
> 
>>try(f("A"))
> 
> Error in log(x) : Non-numeric argument to mathematical function
> 
>>traceback()
> 
> No traceback available
> 
> 
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rasch_sub at med1.med.tum.de  Fri Mar 12 17:45:23 2004
From: rasch_sub at med1.med.tum.de (Raphael Schneider)
Date: Fri, 12 Mar 2004 17:45:23 +0100
Subject: [R] spss
In-Reply-To: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
References: <4.3.2.7.1.19970304064326.00ad3390@mail.med.up.pt>
Message-ID: <200403121745.23085.rasch_sub@med1.med.tum.de>

Hi,

(I am not sure if this worked the first time I send it. So 2nd try.)

On Tuesday 04 March 1997 07:47, Margarida J?lia Rodrigues Igreja wrote:
> when i type:
>  > library(foreign)
>  >  read.spss("H:\Desktop\bd1\experiencia1")
>
> comes the error:
> Error in read.spss("H:Desktop\bd1experiencia1") : unable to open file
Perhaps your file has the extension 'sav' which is not shown. So try to us 
'experiencia1.sav' as the filename (add '.sav' to the filename). SPSS 
documents have normaly 'sav' as the extension.

Raphael
www.librasch.org

PS: The first thing I change when installing Windows is that file extensions 
are always shown.
PS2: Could you please check your date and time settings.



From hdoran at nasdc.org  Fri Mar 12 17:50:59 2004
From: hdoran at nasdc.org (Harold Doran)
Date: Fri, 12 Mar 2004 11:50:59 -0500
Subject: [R] still spss
Message-ID: <66578BFC0BA55348B5907A0F798EE93066357F@ernesto.NASDC.ORG>

It appears you may have left off the file extension, "experiencial.sav"

-----Original Message-----
From: Margarida J?lia Rodrigues Igreja [mailto:migreja at med.up.pt]
Sent: Tuesday, March 04, 1997 2:24 AM
To: R-help at stat.math.ethz.ch
Subject: [R] still spss


hi again,

i still cannot open the file in spss :(

i type:
library(foreign)
read.spss("H:\\Desktop\\bd1\\experiencia1")

and the error comes:
Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable to open file

can you help me?

margarida,portugal

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Fri Mar 12 17:53:19 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 12 Mar 2004 11:53:19 -0500
Subject: [R] still spss
In-Reply-To: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>
Message-ID: <20040312165319.PSTI2607.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Margarida,

Are you sure that the file is named experiencia1. Might it be named
experiencia1.sav or experiencia1.por?

I hope that this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Margarida J?lia Rodrigues Igreja
> Sent: Tuesday, March 04, 1997 2:24 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] still spss
> 
> hi again,
> 
> i still cannot open the file in spss :(
> 
> i type:
> library(foreign)
> read.spss("H:\\Desktop\\bd1\\experiencia1")
> 
> and the error comes:
> Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable 
> to open file
> 
> can you help me?
> 
> margarida,portugal
>



From spencer.graves at pdf.com  Fri Mar 12 17:58:08 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 12 Mar 2004 08:58:08 -0800
Subject: [R] aic calculation
In-Reply-To: <404DEB24.9020205@pdf.com>
References: <404DDF25.61A3016B@shannon.mast.queensu.ca>
	<404DEB24.9020205@pdf.com>
Message-ID: <4051EC20.2060305@pdf.com>

www.r-project.org -> search -> "R site search" -> "Burnham and Anderson" 
just produced 12 hits for me, all but 4 of which were either from me or 
from Brian Ripley.  A similar search for "AIC.c" produced 11 hits, some 
on the "Burnham and Anderson" list, some not. 

      hope this helps.  spencer graves

Anne York wrote:

On Tue, 09 Mar 2004 Spencer Graves <spencer.graves at pdf.com>
wrote: 


 ... most of message deleted ...                                                                                



>     Brian Ripley (1996) Pattern Recognition and Neural 
>Networks (Cambridge U. Pr.)
>  
>

>     Burnham and Anderson (2002) Model Selection and 
>Multimodel Inference (Springer).
>  
>
                                                                                
                                                                                

>     The latter contains errors, as Prof. Ripley indicated 
>in earlier posts to this listserve.  However, it seems for 
>me to still be useful.
>  
>
 

Hi Spencer                                                     
Somehow I missed that most, do you have any idea when it 
was. I'd like to read it. 

Thanks, 

Anne



From ernesto at ipimar.pt  Fri Mar 12 18:08:09 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Fri, 12 Mar 2004 17:08:09 +0000
Subject: [R] grep
Message-ID: <1079111289.12588.49.camel@gandalf.local>

Hi,

I want to use the first digit of the elements of a vector.

I've tried grep but didn't work.

Any help is welcome.

Thanks

EJ

> grep("^[0-9]",as.character(runif(100,0,2)))
  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
17  18
 [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34 
35  36
 [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52 
53  54
 [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 
71  72
 [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 
89  90
 [91]  91  92  93  94  95  96  97  98  99 100



From apjaworski at mmm.com  Fri Mar 12 18:02:39 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Fri, 12 Mar 2004 11:02:39 -0600
Subject: [R] Xeon CPU and ATLAS
Message-ID: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>





I am about to get a new machine at work - an IBM Intellistation with the
Xeon 2.8 GB processor.  It will run Windows 2000.  I would like to install
the proper ATLAS dll for this machine, but I am not sure if Xeon is P4?
Does anybody have any experience with Xeon?

Thanks in advance,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122



From thpe at hhbio.wasser.tu-dresden.de  Fri Mar 12 18:12:26 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 12 Mar 2004 18:12:26 +0100
Subject: [R] still spss
In-Reply-To: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>
References: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>
Message-ID: <4051EF7A.8050307@hhbio.wasser.tu-dresden.de>

Margarida J?lia Rodrigues Igreja wrote:

> library(foreign)
> read.spss("H:\\Desktop\\bd1\\experiencia1")
> Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable to open file

I suspect, you make still a path error. Does your file really have no 
extension (e.g. .sav)? The default behaviour of Windows is to hide 
extensions.

Maybe, read.spss("H:\\Desktop\\bd1\\experiencia1.sav") will help, 
otherwise you may look for a computer expert near you.

Thomas P.



From tlumley at u.washington.edu  Fri Mar 12 18:16:26 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Mar 2004 09:16:26 -0800 (PST)
Subject: [R] No traceback available when using try(...)
In-Reply-To: <4051E8DE.4010807@jhsph.edu>
References: <200403111208.00088.duchesnay@shfj.cea.fr>
	<Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>
	<200403121228.48061.duchesnay@shfj.cea.fr>
	<cjg350pukspus4rmfrshbuofqnf77218fm@4ax.com>
	<4051E8DE.4010807@jhsph.edu>
Message-ID: <Pine.A41.4.58.0403120915140.42486@homer14.u.washington.edu>

On Fri, 12 Mar 2004, Roger D. Peng wrote:

> Funny, it works for me on R-patched
>
>  > f <- function(a) { return(log(a)) }
>  > f("A")
> Error in log(x) : Non-numeric argument to mathematical function
>  > traceback()
> 2: log(a)
> 1: f("A")
>  > try(f("A"))
> Error in log(x) : Non-numeric argument to mathematical function
>  > traceback()
> 2: log(a)
> 1: f("A")
>

No, it doesn't.  The second traceback() call is printing the *previous*
trace information, which in this case is the same.

The bug in 1.8.1, which *is* fixed, is that you didn't get traceback
information anywhere, rather than not getting it with try()

	-thomas



From abderrahim.oulhaj at pharmacology.oxford.ac.uk  Fri Mar 12 18:25:03 2004
From: abderrahim.oulhaj at pharmacology.oxford.ac.uk (Abderrahim Oulhaj)
Date: Fri, 12 Mar 2004 17:25:03 -0000
Subject: [R] Latent trait models
Message-ID: <001901c40856$f444dd60$abca01a3@optima.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040312/67c073a0/attachment.pl

From apv at capital.net  Fri Mar 12 18:28:44 2004
From: apv at capital.net (Arend P. van der Veen)
Date: Fri, 12 Mar 2004 12:28:44 -0500
Subject: [R] RMySQL under FreeBSD 4.8 - is solution acceptable ????
Message-ID: <1079112524.32093.389.camel@freebsd>

Hi,

I had been having problems with RMySQL 0.5-3 under FreeBSD 4.8.  I think
I have found a work around but wanted to see if anybody had any
comments.  This may also be applicable to OS X.  The error I got was:

> library(RMySQL)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"/usr/local/R/lib/R/library/RMySQL/libs/RMySQL.so":
  /usr/local/R/lib/R/library/RMySQL/libs/RMySQL.so: Undefined symbol
"getopt_long"
Error in library(RMySQL) : .First.lib failed
>

The problem was that getopt_long is undefined.  I read the README files
and found that this is may be a problem with non-gnu systems like
FreeBSD.  I have also heard that OS-X users have the same problem.

In the RMySQL distribution there is a new version of getopt.c.  I tried
to compile this with RMySQL by moving it to the src directory but was
not very successful. 

I finally decided to remove the code associate with getopt_long from the
driver and see what impact it would have.  I analyzed RS-MySQL and
disable the section of the program that uses getopt_long.  I made the
following patch to RS-MySQL.c:

243c243,244
< #ifndef WIN32
---
>   /*#ifndef WIN32*/
> #if 0 /* Arend van der Veen, Mar 12, 2004 */

I have been able to competely determine what impact this will have.  I
use a ~/.my.cnf file with connection information in it to connect to my
mysql database:

lConnection <- dbConnect("MySQL",group="mysection")

I initially thought that this may stop working based on the modification
that I made but it still works.

What I am hoping to find out is what is the impact of this change on the
functionality of the driver.

Thanks,
Arend van der Veen



From ligges at statistik.uni-dortmund.de  Fri Mar 12 19:19:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 12 Mar 2004 19:19:05 +0100
Subject: [R] still spss
In-Reply-To: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>
References: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>
Message-ID: <4051FF19.9080507@statistik.uni-dortmund.de>

Margarida J?lia Rodrigues Igreja wrote:

> hi again,
> 
> i still cannot open the file in spss :(
> 
> i type:
> library(foreign)
> read.spss("H:\\Desktop\\bd1\\experiencia1")
> 
> and the error comes:
> Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable to open file

For sure the file there? Does the file has an extension, .sav for 
example? You will need to specify the latter as in: 
read.spss("H:\\Desktop\\bd1\\experiencia1.sav")


Uwe Ligges



> can you help me?
> 
> margarida,portugal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From mifek at gmx.net  Fri Mar 12 19:42:48 2004
From: mifek at gmx.net (Michael Mifek)
Date: Fri, 12 Mar 2004 19:42:48 +0100
Subject: [R] Problems startin RAqua for Mac
Message-ID: <0F4F4A31-7455-11D8-B2BA-000A95B1541C@gmx.net>

I am a user of a i book G4 with OS X 10.3 and I installed RAqua 1.8.1 
without any problems. When I want to start RStart the "R" Symbol appers 
for one second and then disappers again and nothing else happens. I've 
tried to reinstall the program and I also reinstalled my OS X wich did 
not help. I really need this program for my studies. If someone knows 
whats wrong please eMail me!!

mifek at gmx.net

thanks a lot
Michael



From sundar.dorai-raj at pdf.com  Fri Mar 12 19:50:47 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 12 Mar 2004 12:50:47 -0600
Subject: [R] grep
In-Reply-To: <1079111289.12588.49.camel@gandalf.local>
References: <1079111289.12588.49.camel@gandalf.local>
Message-ID: <40520687.6050206@pdf.com>



Ernesto Jardim wrote:

> Hi,
> 
> I want to use the first digit of the elements of a vector.
> 
> I've tried grep but didn't work.
> 
> Any help is welcome.
> 
> Thanks
> 
> EJ
> 
> 
>>grep("^[0-9]",as.character(runif(100,0,2)))
> 
>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
> 17  18
>  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34 
> 35  36
>  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52 
> 53  54
>  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 
> 71  72
>  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 
> 89  90
>  [91]  91  92  93  94  95  96  97  98  99 100
> 

Not surprising. Try ?substring instead.

substring(runif(100, 0, 2), 1, 1)

-sundar



From ccleland at optonline.net  Fri Mar 12 19:57:47 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 12 Mar 2004 13:57:47 -0500
Subject: [R] grep
In-Reply-To: <1079111289.12588.49.camel@gandalf.local>
References: <1079111289.12588.49.camel@gandalf.local>
Message-ID: <4052082B.4060807@optonline.net>

Ernesto Jardim wrote:
> I want to use the first digit of the elements of a vector.
> 
> I've tried grep but didn't work.
> 
> Any help is welcome.

substr(as.character(runif(100,0,2)), 1, 1)

see ?substr

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From MSchwartz at medanalytics.com  Fri Mar 12 20:01:40 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 12 Mar 2004 13:01:40 -0600
Subject: [R] grep
In-Reply-To: <1079111289.12588.49.camel@gandalf.local>
References: <1079111289.12588.49.camel@gandalf.local>
Message-ID: <1079118100.20461.28.camel@localhost.localdomain>

On Fri, 2004-03-12 at 11:08, Ernesto Jardim wrote:
> Hi,
> 
> I want to use the first digit of the elements of a vector.
> 
> I've tried grep but didn't work.
> 
> Any help is welcome.
> 
> Thanks
> 
> EJ
> 
> > grep("^[0-9]",as.character(runif(100,0,2)))
>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
> 17  18
>  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34 
> 35  36
>  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52 
> 53  54
>  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 
> 71  72
>  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 
> 89  90
>  [91]  91  92  93  94  95  96  97  98  99 100


How about ?substr

> substr(as.character(runif(100, 0, 2)), 1, 1)
  [1] "0" "1" "0" "1" "0" "0" "0" "0" "0" "1" "1" "1" "0" "0" "1" "0"
 [17] "1" "0" "1" "1" "1" "1" "0" "1" "0" "1" "0" "1" "1" "0" "1" "0"
 [33] "1" "0" "1" "0" "0" "1" "0" "0" "0" "0" "0" "0" "0" "0" "1" "0"
 [49] "1" "1" "0" "0" "0" "1" "1" "1" "0" "1" "0" "1" "0" "1" "1" "1"
 [65] "1" "0" "1" "1" "1" "1" "1" "0" "1" "1" "0" "1" "0" "1" "0" "0"
 [81] "1" "1" "0" "0" "1" "1" "0" "1" "0" "0" "0" "0" "0" "0" "0" "1"
 [97] "0" "0" "0" "1"

or

> substr(as.character(1:100), 1, 1)
  [1] "1" "2" "3" "4" "5" "6" "7" "8" "9" "1" "1" "1" "1" "1" "1" "1"
 [17] "1" "1" "1" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "3" "3" "3"
 [33] "3" "3" "3" "3" "3" "3" "3" "4" "4" "4" "4" "4" "4" "4" "4" "4"
 [49] "4" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "6" "6" "6" "6" "6"
 [65] "6" "6" "6" "6" "6" "7" "7" "7" "7" "7" "7" "7" "7" "7" "7" "8"
 [81] "8" "8" "8" "8" "8" "8" "8" "8" "8" "9" "9" "9" "9" "9" "9" "9"
 [97] "9" "9" "9" "1"


HTH,

Marc Schwartz



From olefc at daimi.au.dk  Fri Mar 12 20:04:26 2004
From: olefc at daimi.au.dk (Ole F. Christensen)
Date: Fri, 12 Mar 2004 20:04:26 +0100
Subject: [R] Sweave and R output: possible to suppress "Schunk" tags in *.tex
	file output?
Message-ID: <405209BA.9020404@daimi.au.dk>

Dear Scott

Why not use the function xtable() in the library of the same name for this ?

Cheers Ole

Hello,

I would like to fill the rows of a Latex tabular environment with output from
R, as in

\begin{table}
  \caption{Table caption.} 
  \label{tab:events}
  \begin{tabular}{c r r r r r}
  \hline

<<echo=false,results=tex>>=
fill.my.table.rows()
@

  \end{tabular}
\end{table}

Sweave produces the output inside \begin{Schunk} and \end{Schunk} commands,
which latex doesn't tolerate within a table.  I would prefer to avoid writing all of
the Latex table code from R.  Is my only other option to produce a separate
text file, e.g. under Sweave with results=hide, then use \input{} on the file?

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
scott.waichler _at_ pnl.gov













    

------------------------------------------------------------------------

    *



From tblackw at umich.edu  Fri Mar 12 20:05:52 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Fri, 12 Mar 2004 14:05:52 -0500 (EST)
Subject: [R] grep
In-Reply-To: <1079111289.12588.49.camel@gandalf.local>
References: <1079111289.12588.49.camel@gandalf.local>
Message-ID: <Pine.SOL.4.58.0403121359410.20144@mspacman.gpcc.itd.umich.edu>

Ernesto  -

Use  as.numeric(substr(as.character(x), 1, 1)).

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 12 Mar 2004, Ernesto Jardim wrote:

> Hi,
>
> I want to use the first digit of the elements of a vector.
>
> I've tried grep but didn't work.
>
> Any help is welcome.
>
> Thanks
>
> EJ
>
> > grep("^[0-9]",as.character(runif(100,0,2)))
>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16
> 17  18
>  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34
> 35  36
>  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52
> 53  54
>  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70
> 71  72
>  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88
> 89  90
>  [91]  91  92  93  94  95  96  97  98  99 100
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Fri Mar 12 20:14:45 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Mar 2004 11:14:45 -0800 (PST)
Subject: [R] Xeon CPU and ATLAS
In-Reply-To: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>
References: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>
Message-ID: <Pine.A41.4.58.0403121113350.42486@homer14.u.washington.edu>

On Fri, 12 Mar 2004 apjaworski at mmm.com wrote:

>
> I am about to get a new machine at work - an IBM Intellistation with the
> Xeon 2.8 GB processor.  It will run Windows 2000.  I would like to install
> the proper ATLAS dll for this machine, but I am not sure if Xeon is P4?
> Does anybody have any experience with Xeon?
>

The Xeon line has larger cache than the other P4 chips, and since Atlas
gets much of its power from optimising cache use I would expect a P4 Atlas
not to be optimal.

	-thomas



From FWS4 at CDRH.FDA.GOV  Fri Mar 12 20:19:13 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Fri, 12 Mar 2004 14:19:13 -0500
Subject: [R] grep
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB79F@drm556>


as.integer(x/10^(as.integer(log10(x))))



-----Original Message-----
From: Ernesto Jardim [mailto:ernesto at ipimar.pt] 
Sent: Friday, March 12, 2004 12:08 PM
To: Mailing List R
Subject: [R] grep


Hi,

I want to use the first digit of the elements of a vector.

I've tried grep but didn't work.

Any help is welcome.

Thanks

EJ

> grep("^[0-9]",as.character(runif(100,0,2)))
  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
17  18
 [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34 
35  36
 [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52 
53  54
 [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 
71  72
 [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 
89  90
 [91]  91  92  93  94  95  96  97  98  99 100

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Fri Mar 12 20:32:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 12 Mar 2004 14:32:36 -0500 (EST)
Subject: [R] still spss
Message-ID: <20040312193236.2F7D23999@mprdmxin.myway.com>


Another thing to try if you just can't get the other
suggestions to work is:

read.spss(file.choose())

This will bring up up a Windows dialogue that allows you
to navigate to and select the file.

---
Date:   Fri, 12 Mar 2004 18:12:26 +0100 
From:   Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de>
To:   =?ISO-8859-1?Q?Margarida_J=FAlia_Rodrigues_Igreja?= <migreja at med.up.pt> 
Cc:   <R-help at stat.math.ethz.ch> 
Subject:   Re: [R] still spss 

 
Margarida Júlia Rodrigues Igreja wrote:

> library(foreign)
> read.spss("H:\\Desktop\\bd1\\experiencia1")
> Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable to open file

I suspect, you make still a path error. Does your file really have no 
extension (e.g. .sav)? The default behaviour of Windows is to hide 
extensions.

Maybe, read.spss("H:\\Desktop\\bd1\\experiencia1.sav") will help, 
otherwise you may look for a computer expert near you.

Thomas P.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Mar 12 20:49:55 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 20:49:55 +0100
Subject: [R] Xeon CPU and ATLAS
In-Reply-To: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>
References: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>
Message-ID: <x2hdwtvo9o.fsf@biostat.ku.dk>

apjaworski at mmm.com writes:

> I am about to get a new machine at work - an IBM Intellistation with the
> Xeon 2.8 GB processor.  It will run Windows 2000.  I would like to install
> the proper ATLAS dll for this machine, but I am not sure if Xeon is P4?
> Does anybody have any experience with Xeon?

Yes, we have a dual 2.8 Xeon running Linux. Very nice machine. We've
had it for a bit more than a year and it's been of very good use. I
can see that I did two ATLAS builds in feb 2003, one is called
Linux_P4SSE2_2 and the other is Linux_Xeon_2. AFAIR (but it's been a
while), the former is the preconfigured thing that came out of a
quick-install and the other one is fully tuned version, and there was
no difference worth speaking of between the two. 

I think the difference between P4 and Xeon is the SSE1 vs SSE2 thing,
so you'd want the P4SSE2 DLL, but others may be able to speak more
authoritatively. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Mar 12 20:54:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Mar 2004 20:54:23 +0100
Subject: [R] No traceback available when using try(...)
In-Reply-To: <Pine.A41.4.58.0403120915140.42486@homer14.u.washington.edu>
References: <200403111208.00088.duchesnay@shfj.cea.fr>
	<Pine.A41.4.58.0403111125460.45338@homer09.u.washington.edu>
	<200403121228.48061.duchesnay@shfj.cea.fr>
	<cjg350pukspus4rmfrshbuofqnf77218fm@4ax.com>
	<4051E8DE.4010807@jhsph.edu>
	<Pine.A41.4.58.0403120915140.42486@homer14.u.washington.edu>
Message-ID: <x2d67hvo28.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Fri, 12 Mar 2004, Roger D. Peng wrote:
> 
> > Funny, it works for me on R-patched
> >
> >  > f <- function(a) { return(log(a)) }
> >  > f("A")
> > Error in log(x) : Non-numeric argument to mathematical function
> >  > traceback()
> > 2: log(a)
> > 1: f("A")
> >  > try(f("A"))
> > Error in log(x) : Non-numeric argument to mathematical function
> >  > traceback()
> > 2: log(a)
> > 1: f("A")
> >
> 
> No, it doesn't.  The second traceback() call is printing the *previous*
> trace information, which in this case is the same.

Right, same thing here.
 
> The bug in 1.8.1, which *is* fixed, is that you didn't get traceback
> information anywhere, rather than not getting it with try()

(Is is obvious that you *should* get a traceback from inside try()?)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Fri Mar 12 20:58:14 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 12 Mar 2004 13:58:14 -0600
Subject: [R] still spss
In-Reply-To: <4051FF19.9080507@statistik.uni-dortmund.de>
References: <4.3.2.7.1.19970304072106.00ad6370@mail.med.up.pt>
	<4051FF19.9080507@statistik.uni-dortmund.de>
Message-ID: <6r8yi5x2g9.fsf@bates4.stat.wisc.edu>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Margarida J?lia Rodrigues Igreja wrote:
> 
> > hi again,
> > i still cannot open the file in spss :(
> 
> > i type:
> 
> > library(foreign)
> > read.spss("H:\\Desktop\\bd1\\experiencia1")
> > and the error comes:
> 
> > Error in read.spss("H:\\Desktop\\bd1\\experiencia1") : unable to open file
> 
> For sure the file there? Does the file has an extension, .sav for
> example? You will need to specify the latter as in:
> read.spss("H:\\Desktop\\bd1\\experiencia1.sav")

In Windows I recommend the use of file.choose() in place of explicit
file names.  The function brings up a chooser panel in which you can
use the familiar Windows point-and-click style of selecting a file.

That is, invoke read.spss as

read.spss(file.choose())



From tlumley at u.washington.edu  Fri Mar 12 22:00:33 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Mar 2004 13:00:33 -0800 (PST)
Subject: [R] Problems startin RAqua for Mac
In-Reply-To: <0F4F4A31-7455-11D8-B2BA-000A95B1541C@gmx.net>
References: <0F4F4A31-7455-11D8-B2BA-000A95B1541C@gmx.net>
Message-ID: <Pine.A41.4.58.0403121257560.42486@homer14.u.washington.edu>

On Fri, 12 Mar 2004, Michael Mifek wrote:

> I am a user of a i book G4 with OS X 10.3 and I installed RAqua 1.8.1
> without any problems. When I want to start RStart the "R" Symbol appers
> for one second and then disappers again and nothing else happens. I've
> tried to reinstall the program and I also reinstalled my OS X wich did
> not help. I really need this program for my studies. If someone knows
> whats wrong please eMail me!!
>

Look at the Console log as you try to start RAqua.  You can find the
Console log viewer by going to the Applications folder and then the
Utilities folder inside Applications. It should give more information
about why RAqua is failing.

One fairly frequent reason for RAqua to fail is if you didn't install the
readline library that comes packaged with RAqua.  This is not optional.

	-thomas



From andrewr at uidaho.edu  Fri Mar 12 22:04:55 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 12 Mar 2004 13:04:55 -0800 (PST)
Subject: [R] Problems startin RAqua for Mac
In-Reply-To: <0F4F4A31-7455-11D8-B2BA-000A95B1541C@gmx.net>
References: <0F4F4A31-7455-11D8-B2BA-000A95B1541C@gmx.net>
Message-ID: <Pine.GSO.4.56.0403121304020.15660@tornado.csrv.uidaho.edu>

Michael,

check the logs.  I suspect that you need to install libreadline, which you
will find in the RAqua disk image that you downloaded.

Andrew


On Fri, 12 Mar 2004, Michael Mifek wrote:

> I am a user of a i book G4 with OS X 10.3 and I installed RAqua 1.8.1
> without any problems. When I want to start RStart the "R" Symbol appers
> for one second and then disappers again and nothing else happens. I've
> tried to reinstall the program and I also reinstalled my OS X wich did
> not help. I really need this program for my studies. If someone knows
> whats wrong please eMail me!!
>
> mifek at gmx.net
>
> thanks a lot
> Michael
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Andrew Robinson			     Ph: 208 885 7115
Department of Forest Resources	     Fa: 208 885 6226
University of Idaho		     E : andrewr at uidaho.edu
PO Box 441133			     W : http://www.uidaho.edu/~andrewr
Moscow ID 83843			     Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From sudar_80 at neo.tamu.edu  Fri Mar 12 22:21:44 2004
From: sudar_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Fri, 12 Mar 2004 21:21:44 -0000
Subject: [R] (no subject)
Message-ID: <200403122121.i2CLLibW036013@xyzzy-1.tamu.edu>


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
hello all,


I have an array c as follows

1	23	12
1	34	15			
1	45	67
1	45	87
2	78	23
2	65	19
2	45	90
2	70	32


Col 1 indicates treatment, col2 is the observed readings and col3 
is some summary statistic I have calculated after breaking the ties in col 2
arbitrarily.

Now I want to average the statistics of the tied values. So, in col 3 for
each of the reading of 45 , I now want (67+87+90)/3 = 81.33


1	23	12
1	34	15			
1	45	81.33
1	45	81.33
2	78	23
2	65	19
2	45	81.33
2	70	32


Can someone please help me do this in R?

Thanks in advance

Warm regards

~S



From sudar_80 at neo.tamu.edu  Fri Mar 12 22:22:00 2004
From: sudar_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Fri, 12 Mar 2004 21:22:00 -0000
Subject: [R] (no subject)
Message-ID: <200403122122.i2CLM0iV082947@xyzzy-4.tamu.edu>


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
hello all,


I have an array c as follows

1	23	12
1	34	15			
1	45	67
1	45	87
2	78	23
2	65	19
2	45	90
2	70	32


Col 1 indicates treatment, col2 is the observed readings and col3 
is some summary statistic I have calculated after breaking the ties in col 2
arbitrarily.

Now I want to average the statistics of the tied values. So, in col 3 for
each of the reading of 45 , I now want (67+87+90)/3 = 81.33


1	23	12
1	34	15			
1	45	81.33
1	45	81.33
2	78	23
2	65	19
2	45	81.33
2	70	32


Can someone please help me do this in R?

Thanks in advance

Warm regards

~S



From ripley at stats.ox.ac.uk  Fri Mar 12 23:16:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Mar 2004 22:16:03 +0000 (GMT)
Subject: [R] Xeon CPU and ATLAS
In-Reply-To: <x2hdwtvo9o.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0403122206570.5656-100000@gannet.stats>

AFAIK all P4s are SSE2.

He wants a Windows ATLAS build, and that is not hard, but you have to do 
that under Cygwin not MinGW.

We have a Xeon, but have found Athlons and now Opterons better value.
I don't have access to Windows Xeon machine: anyone who does could 
contribute a Xeon version of Rblas.dll (to Uwe Ligges, please).

Note that for 1.9.0 you can use Goto's BLAS and he as a P4 version that 
is faster than ATLAS (under Windows).  So I would start with that.

Although Thomas Lumley is right about cache sizes, using a P4 512kb ATLAS 
or Goto BLAS is pretty good on a Xeon (only tested by me under Linux).  
Using a BLAS for too large a cache is disastrous, but underestimating by a 
factor of 2 is not bad at all. (I think most Xeons are 1Mb cache, with 
Xeon MP being 4Mb.)

On 12 Mar 2004, Peter Dalgaard wrote:

> apjaworski at mmm.com writes:
> 
> > I am about to get a new machine at work - an IBM Intellistation with the
> > Xeon 2.8 GB processor.  It will run Windows 2000.  I would like to install
> > the proper ATLAS dll for this machine, but I am not sure if Xeon is P4?
> > Does anybody have any experience with Xeon?
> 
> Yes, we have a dual 2.8 Xeon running Linux. Very nice machine. We've
> had it for a bit more than a year and it's been of very good use. I
> can see that I did two ATLAS builds in feb 2003, one is called
> Linux_P4SSE2_2 and the other is Linux_Xeon_2. AFAIR (but it's been a
> while), the former is the preconfigured thing that came out of a
> quick-install and the other one is fully tuned version, and there was
> no difference worth speaking of between the two. 
> 
> I think the difference between P4 and Xeon is the SSE1 vs SSE2 thing,
> so you'd want the P4SSE2 DLL, but others may be able to speak more
> authoritatively. 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From abunn at montana.edu  Fri Mar 12 23:24:45 2004
From: abunn at montana.edu (Andy Bunn)
Date: Fri, 12 Mar 2004 15:24:45 -0700
Subject: [R] (no subject)
In-Reply-To: <200403122122.i2CLM0iV082947@xyzzy-4.tamu.edu>
Message-ID: <000b01c40880$d55bb610$78f05a99@msu.montana.edu>

Look at ?Extract

myDF <- data.frame(c1 = c(1,1,1,1,2,2,2,2), 
                   c2 = c(23,34,45,45,78,65,45,70),
                   c3 = c(12,15,67,87,23,19,90,32))
mean(myDF[myDF$c2 == 45,3])


HTH, Andy



From rossini at blindglobe.net  Fri Mar 12 23:42:37 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 12 Mar 2004 14:42:37 -0800
Subject: [R] Xeon CPU and ATLAS
In-Reply-To: <x2hdwtvo9o.fsf@biostat.ku.dk> (Peter Dalgaard's message of "12
	Mar 2004 20:49:55 +0100")
References: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>
	<x2hdwtvo9o.fsf@biostat.ku.dk>
Message-ID: <858yi54rhe.fsf@servant.blindglobe.net>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> apjaworski at mmm.com writes:
>
>> I am about to get a new machine at work - an IBM Intellistation with the
>> Xeon 2.8 GB processor.  It will run Windows 2000.  I would like to install
>> the proper ATLAS dll for this machine, but I am not sure if Xeon is P4?
>> Does anybody have any experience with Xeon?
>
> Yes, we have a dual 2.8 Xeon running Linux. Very nice machine. We've
> had it for a bit more than a year and it's been of very good use. I
> can see that I did two ATLAS builds in feb 2003, one is called
> Linux_P4SSE2_2 and the other is Linux_Xeon_2. AFAIR (but it's been a
> while), the former is the preconfigured thing that came out of a
> quick-install and the other one is fully tuned version, and there was
> no difference worth speaking of between the two. 
>
> I think the difference between P4 and Xeon is the SSE1 vs SSE2 thing,
> so you'd want the P4SSE2 DLL, but others may be able to speak more
> authoritatively. 

SSE1 vs SSE2  is PIII vs PIV (so depends if your xeons are p3 or p4 (?
not sure if there is such a thing) based.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From apjaworski at mmm.com  Fri Mar 12 23:44:33 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Fri, 12 Mar 2004 16:44:33 -0600
Subject: [R] Xeon CPU and ATLAS
Message-ID: <OF757B3B87.42DE812D-ON86256E55.007B4B89-86256E55.007CEDE2@mmm.com>






First of all thanks to Peter Dalgaard, Thomas Lumley and Prof Brian Ripley
for quick responses.

Indeed, I will be running Windows2000 on this machine (my company's
standard).  I will also install the full version of cygwin on it.

When I asked my original question I thought about just copying the P4 dll
from the ATLAS directory on CRAN.  As I understand from the three answers I
got, this should work, but perhaps not be the fastest BLAS possible.  When
I get the machine (beginning of April) I will try to configure and compile
ATLAS and Goto and compare the results.

Thanks again,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+----------------------------->
|         |           Prof Brian Ripley |
|         |           <ripley at stats.ox.a|
|         |           c.uk>             |
|         |                             |
|         |           03/12/2004 16:16  |
|         |                             |
|---------+----------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       Peter Dalgaard <p.dalgaard at biostat.ku.dk>                                                                    |
  |      cc:       apjaworski at mmm.com                                                                                           |
  |        <R-help at stat.math.ethz.ch>                                                                                           |
  |      Subject:  Re: [R] Xeon CPU and ATLAS                                                                                   |
  >-----------------------------------------------------------------------------------------------------------------------------|




AFAIK all P4s are SSE2.

He wants a Windows ATLAS build, and that is not hard, but you have to do
that under Cygwin not MinGW.

We have a Xeon, but have found Athlons and now Opterons better value.
I don't have access to Windows Xeon machine: anyone who does could
contribute a Xeon version of Rblas.dll (to Uwe Ligges, please).

Note that for 1.9.0 you can use Goto's BLAS and he as a P4 version that
is faster than ATLAS (under Windows).  So I would start with that.

Although Thomas Lumley is right about cache sizes, using a P4 512kb ATLAS
or Goto BLAS is pretty good on a Xeon (only tested by me under Linux).
Using a BLAS for too large a cache is disastrous, but underestimating by a
factor of 2 is not bad at all. (I think most Xeons are 1Mb cache, with
Xeon MP being 4Mb.)

On 12 Mar 2004, Peter Dalgaard wrote:

> apjaworski at mmm.com writes:
>
> > I am about to get a new machine at work - an IBM Intellistation with
the
> > Xeon 2.8 GB processor.  It will run Windows 2000.  I would like to
install
> > the proper ATLAS dll for this machine, but I am not sure if Xeon is P4?
> > Does anybody have any experience with Xeon?
>
> Yes, we have a dual 2.8 Xeon running Linux. Very nice machine. We've
> had it for a bit more than a year and it's been of very good use. I
> can see that I did two ATLAS builds in feb 2003, one is called
> Linux_P4SSE2_2 and the other is Linux_Xeon_2. AFAIR (but it's been a
> while), the former is the preconfigured thing that came out of a
> quick-install and the other one is fully tuned version, and there was
> no difference worth speaking of between the two.
>
> I think the difference between P4 and Xeon is the SSE1 vs SSE2 thing,
> so you'd want the P4SSE2 DLL, but others may be able to speak more
> authoritatively.
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Fri Mar 12 23:52:24 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 12 Mar 2004 22:52:24 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <200403122122.i2CLM0iV082947@xyzzy-4.tamu.edu>
Message-ID: <XFMail.040312225224.Ted.Harding@nessie.mcc.ac.uk>

On 12-Mar-04 Padmanabhan, Sudharsha wrote:
> I have an array c as follows
> 
> 1     23      12
> 1     34      15                      
> 1     45      67
> 1     45      87
> 2     78      23
> 2     65      19
> 2     45      90
> 2     70      32
> 
> 
> Col 1 indicates treatment, col2 is the observed readings and col3 
> is some summary statistic I have calculated after breaking the ties
> in col 2 arbitrarily.
> 
> Now I want to average the statistics of the tied values. So, in col 3
> for each of the reading of 45 , I now want (67+87+90)/3 = 81.33
> 
> 
> 1     23      12
> 1     34      15                      
> 1     45      81.33
> 1     45      81.33
> 2     78      23
> 2     65      19
> 2     45      81.33
> 2     70      32

tmp<-matrix(c(
1,23,12,
1,34,15,
1,45,67,
1,45,87,
2,78,23,
2,65,19,
2,45,90,
2,70,32),ncol=3,byrow=TRUE)

for( i in (unique(tmp[,2])) ){
     m <- mean(tmp[tmp[,2]==i,3]);
     tmp[tmp[,2]==i,3] <- m
}

tmp
     [,1] [,2]     [,3]
[1,]    1   23 12.00000
[2,]    1   34 15.00000
[3,]    1   45 81.33333
[4,]    1   45 81.33333
[5,]    2   78 23.00000
[6,]    2   65 19.00000
[7,]    2   45 81.33333
[8,]    2   70 32.00000

(You could make the assignment within the for-loop even more
 concise but the above shows explicitly what is going on).

Hope this helps!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 12-Mar-04                                       Time: 22:52:24
------------------------------ XFMail ------------------------------



From thoar at cgd.ucar.edu  Sat Mar 13 01:08:25 2004
From: thoar at cgd.ucar.edu (Tim Hoar)
Date: Fri, 12 Mar 2004 17:08:25 -0700 (MST)
Subject: [R] 64bit build on IBM
Message-ID: <Pine.GSO.4.30.0403121200300.23790-200000@sunray1>

I am trying to build R with 64bit support so I can access more than
2GB of memory on an IBM running AIX. (IBM NightHawk II RS/6000 node with 16
64-bit, 375-MHz POWER3 CPUs and 32 GB of memory, if you want specifics)

The 32bit install required little tweaking, but the 64 bit is sufficiently
complicated that I thought I would report my findings here and ask for
help with the latest (and hopefully last) hurdle.

Let me also state I have been amazed at the ease of installations
despite the obvious complexity of the install process. I have built R
on at least 3 different O/S's with several different compilers, etc. on
each platform. I wanted to use R's configure process for some software
I am working with and realized just how hard the configure process is --
so my hats off to the developers!

#----------------------------------------------------------------------
# Settings in the config.site file
# Fortran or C options aren't making it everywhere.
#----------------------------------------------------------------------
CC='/usr/bin/xlc'
CFLAGS='-q64 ...'
F77='/usr/bin/xlf'
FFLAGS='-q64 ...'
CXX='/usr/bin/xlc'
CXXFLAGS='-q64 ...'
SHLIB_LDFLAGS=-b64
LDFLAGS='-L/usr/local/lib64/r4i4 -L/usr/lib'

    bombs out. config.log:

configure:15817: checking whether mixed C/Fortran code can be run
** cftest   === End of Compilation 1 ===
1501-510  Compilation successful for file conftestf.f.
ld: 0711-736 ERROR: Input file conftest.o:
        XCOFF64 object files are not allowed in 32-bit mode.

CC='/usr/bin/xlc -q64'
F77='/usr/bin/xlf -q64'
CXX='/usr/bin/xlc -q64'   makes it through.

I believe that somewhere there should be a SHLIB_LDFLAGS where there is not.

#----------------------------------------------------------------------
#2 const char ... vs char
#----------------------------------------------------------------------

had to compile with -qinitauto=00
to get the program to run the program to check for POSIX handling of
leapseconds. Without it, the program hangs!

#----------------------------------------------------------------------
# There needs to be some way to set an AR_FLAGS variable, ...
#----------------------------------------------------------------------

I've tried setting environement variables, running configure and then
modifying the top-level Makeconf file to redefine $(AR) to contain
the necessary options   (see below)   but the resulting makefiles all
keep getting a (bogus under AIX) "cr" option.

So I have to manually edit the top-level Makeconf and manually remove
the "cr" from the following makefiles:  src/appl/Makefile, src/nmath/Makefile,
src/unix/Makefile, src/extra/bzip2/Makefile, src/extra/pcre/Makefile,
src/extra/zlib/Makefile ... but that's not all.
I edit them "all at once", before I issue the first "make" command, and
then I have to edit (just) src/unix/Makefile again -- seems it gets remade
at some point.

(before configure) setenv AR_FLAGS '-X64 -v -q'             did nothing
adding Makeconf:AR_FLAGS = -X64 -q -v    did nothing

'$(AR) cr' hardwired in src/appl/Makefile
'$(AR) cr' hardwired in src/nmath/Makefile
'$(AR) cr' hardwired in src/unix/Makefile
'$(AR) cr' hardwired in src/extra/bzip2/Makefile
'$(AR) cr' hardwired in src/extra/pcre/Makefile
'$(AR) cr' hardwired in src/extra/zlib/Makefile

#----------------------------------------------------------------------
# my problem ...
#----------------------------------------------------------------------

Our /lib/crt0.o   is the 32bit flavor. We also have a
    /lib/crt0_64.o   so -- I changed the top-level Makeconf to
reflect this.

R_XTRA_LIBS = /lib/crt0_64.o

#----------------------------------------------------------------------
# Something with the X11 support is not working.
#----------------------------------------------------------------------

src/modules/X11/Makefile.in   defines rules to create .lo objects,
which (in 64 bit mode) the AIX compiler does not like.

I manually edited the
Makefile to reflect the advice from our consultants. The result is
attached as a screen dump of the problem area.


Any help would be appreciated.


Tim


## Tim Hoar, Associate Scientist              email: thoar at ucar.edu     ##
## Geophysical Statistics Project             phone: 303-497-1708       ##
## National Center for Atmospheric Research   FAX  : 303-497-1333       ##
## Boulder, CO  80307                    http://www.cgd.ucar.edu/~thoar ##



-------------- next part --------------
exec: export(export,XL_CONFIG=/etc/vac.cfg:xlc,NULL) 
exec: /usr/vac/bin/CreateExportList(/usr/vac/bin/CreateExportList,/ptmp/thoar/xlcSEtLbjqe,-f,/ptmp/thoar/xlcSFtDbjqd,-X,64,NULL) 
exec: /bin/ld(/bin/ld,-bM:SRE,-bnoentry,-bM:SRE,-H512,-T512,-bnoentry,-bexpall,-bI:../../../etc/R.exp,-b64,-bpT:0x10000000,-bpD:0x20000000,-b64,-L/usr/lib,-oR_X11.so,dataentry.o,devX11.o,rotated.o,rbitmap.o,-lSM,-lICE,-lX11,-ldl,-ltermcap,-lm,-lc,-L/usr/lpp/xlopt,-lxlopt,-lc,-bE:/ptmp/thoar/xlcSEtLbjqe,NULL) 
ld: 0711-317 ERROR: Undefined symbol: .Rf_error
ld: 0711-317 ERROR: Undefined symbol: .Rf_warning
ld: 0711-317 ERROR: Undefined symbol: .Rf_lengthgets
ld: 0711-317 ERROR: Undefined symbol: .R_Reprotect
ld: 0711-317 ERROR: Undefined symbol: .Rf_mkChar
ld: 0711-317 ERROR: Undefined symbol: .SET_STRING_ELT
ld: 0711-317 ERROR: Undefined symbol: .Rf_isNull
ld: 0711-317 ERROR: Undefined symbol: .SET_VECTOR_ELT
ld: 0711-317 ERROR: Undefined symbol: .Rf_isVector
ld: 0711-317 ERROR: Undefined symbol: R_NaString
ld: 0711-317 ERROR: Undefined symbol: R_NilValue
ld: 0711-317 ERROR: Undefined symbol: .Rf_PrintDefaults
ld: 0711-317 ERROR: Undefined symbol: .Rf_EncodeElement
ld: 0711-317 ERROR: Undefined symbol: .Rf_allocVector
ld: 0711-317 ERROR: Undefined symbol: .Rf_coerceVector
ld: 0711-317 ERROR: Undefined symbol: .Rf_install
ld: 0711-317 ERROR: Undefined symbol: R_GlobalEnv
ld: 0711-317 ERROR: Undefined symbol: .Rf_GetOption
ld: 0711-317 ERROR: Undefined symbol: .Rf_asInteger
ld: 0711-317 ERROR: Undefined symbol: R_NaInt
ld: 0711-317 ERROR: Undefined symbol: .Rf_length
ld: 0711-317 ERROR: Undefined symbol: .Rf_nthcdr
ld: 0711-317 ERROR: Undefined symbol: .R_strtod
ld: 0711-317 ERROR: Undefined symbol: .Rf_isBlankString
ld: 0711-317 ERROR: Undefined symbol: R_NaReal
ld: 0711-317 ERROR: Undefined symbol: .UNIMPLEMENTED
ld: 0711-317 ERROR: Undefined symbol: .Rf_duplicate
ld: 0711-317 ERROR: Undefined symbol: .R_ProtectWithIndex
ld: 0711-317 ERROR: Undefined symbol: R_NamesSymbol
ld: 0711-317 ERROR: Undefined symbol: .Rf_getAttrib
ld: 0711-317 ERROR: Undefined symbol: .Rf_errorcall
ld: 0711-317 ERROR: Undefined symbol: .Rf_protect
ld: 0711-317 ERROR: Undefined symbol: .Rf_str2type
ld: 0711-317 ERROR: Undefined symbol: .Rf_begincontext
ld: 0711-317 ERROR: Undefined symbol: .Rf_endcontext
ld: 0711-317 ERROR: Undefined symbol: .Rf_setAttrib
ld: 0711-317 ERROR: Undefined symbol: .Rf_unprotect
ld: 0711-317 ERROR: Undefined symbol: .R_setX11Routines
ld: 0711-317 ERROR: Undefined symbol: .vmaxget
ld: 0711-317 ERROR: Undefined symbol: .Rf_asReal
ld: 0711-317 ERROR: Undefined symbol: .Rf_isValidString
ld: 0711-317 ERROR: Undefined symbol: .Rf_warningcall
ld: 0711-317 ERROR: Undefined symbol: .Rf_isString
ld: 0711-317 ERROR: Undefined symbol: .Rf_isInteger
ld: 0711-317 ERROR: Undefined symbol: .Rf_isLogical
ld: 0711-317 ERROR: Undefined symbol: .Rf_isReal
ld: 0711-317 ERROR: Undefined symbol: .Rf_RGBpar
ld: 0711-317 ERROR: Undefined symbol: .vmaxset
ld: 0711-317 ERROR: Undefined symbol: .R_CheckDeviceAvailable
ld: 0711-317 ERROR: Undefined symbol: R_interrupts_suspended
ld: 0711-317 ERROR: Undefined symbol: .Rf_mkString
ld: 0711-317 ERROR: Undefined symbol: .Rf_gsetVar
ld: 0711-317 ERROR: Undefined symbol: .GEcreateDevDesc
ld: 0711-317 ERROR: Undefined symbol: .Rf_addDevice
ld: 0711-317 ERROR: Undefined symbol: .GEinitDisplayList
ld: 0711-317 ERROR: Undefined symbol: R_interrupts_pending
ld: 0711-317 ERROR: Undefined symbol: .Rf_onintr
ld: 0711-317 ERROR: Undefined symbol: .R_alloc
ld: 0711-317 ERROR: Undefined symbol: .Rf_findVar
ld: 0711-317 ERROR: Undefined symbol: .Rf_elt
ld: 0711-317 ERROR: Undefined symbol: .Rf_GetDevice
ld: 0711-317 ERROR: Undefined symbol: R_InputHandlers
ld: 0711-317 ERROR: Undefined symbol: .getInputHandler
ld: 0711-317 ERROR: Undefined symbol: .removeInputHandler
ld: 0711-317 ERROR: Undefined symbol: .Rf_devNumber
ld: 0711-317 ERROR: Undefined symbol: .Rf_KillDevice
ld: 0711-317 ERROR: Undefined symbol: .GEplayDisplayList
ld: 0711-317 ERROR: Undefined symbol: .R_ExpandFileName
ld: 0711-317 ERROR: Undefined symbol: .R_fopen
ld: 0711-317 ERROR: Undefined symbol: .Rf_asLogical
ld: 0711-317 ERROR: Undefined symbol: .addInputHandler
ld: 0711-317 ERROR: Undefined symbol: .Rprintf
ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more information.
unlink: /ptmp/thoar/xlcW0t0bjqa
unlink: /ptmp/thoar/xlcW1tBbjqb
unlink: /ptmp/thoar/xlcW2tBbjqc
unlink: /ptmp/thoar/xlcSEtLbjqe
unlink: /ptmp/thoar/xlcSFtDbjqd

From wolfgangpauli at web.de  Sat Mar 13 01:18:19 2004
From: wolfgangpauli at web.de (Wolfgang Pauli)
Date: Sat, 13 Mar 2004 01:18:19 +0100
Subject: [R] Error in "names<-.default"(`*tmp*`, value = nmstrata) :
Message-ID: <200403130118.19251.wolfgangpauli@web.de>

Hi,

I have a problem with aov(). I used it many times, but now I have new data, 
tried to use it in the same way. I get I strange Error message that i can't 
understand (see below). I guess it is caused by incompatible data types. The 
dataframe also looks ok to me.

I use:
spk =factor(rep(c("spk1","spk2","spk3","spk4","spk5","spk6","spk7","spk8"), 
c(2,2,2,2,2,2,2,2)))
cs = factor(rep(c("yes","no"),c(8,8)))
sub = factor(sort(spaeVP))
rati = stack(data.frame(arousal))
rati = rati[,1]
rating.df <-  data.frame(sub, cs, spk, rati)
summary(aov(rati ~ cs*spk + Error(sub/(cs*spk)), data=rating.df))

I get this Error (after about 5sec):
Error in "names<-.default"(`*tmp*`, value = nmstrata) : 
	names attribute must be the same length as the vector

> rating.df
    sub  cs  spk rati
1     5 yes spk1   -1
2     5 yes spk1    0
3     5 yes spk2    1
4     5 yes spk2    0
5     5 yes spk3    0
6     5 yes spk3   -1
7     5 yes spk4   -1
8     5 yes spk4    1
9     5  no spk5    0
...

Hope somebody can help me.

with best regards,

Wolfgang



From p.dalgaard at biostat.ku.dk  Sat Mar 13 02:16:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Mar 2004 02:16:35 +0100
Subject: [R] Xeon CPU and ATLAS
In-Reply-To: <858yi54rhe.fsf@servant.blindglobe.net>
References: <OFF2947DCA.9C5DC3B0-ON86256E55.005D23EC-86256E55.005DA094@mmm.com>
	<x2hdwtvo9o.fsf@biostat.ku.dk> <858yi54rhe.fsf@servant.blindglobe.net>
Message-ID: <x28yi5v958.fsf@biostat.ku.dk>

rossini at blindglobe.net (A.J. Rossini) writes:

> > I think the difference between P4 and Xeon is the SSE1 vs SSE2 thing,
> > so you'd want the P4SSE2 DLL, but others may be able to speak more
> > authoritatively. 
> 
> SSE1 vs SSE2  is PIII vs PIV (so depends if your xeons are p3 or p4 (?
> not sure if there is such a thing) based.

Sure? I have

turmalin:/usr/local/src/ATLAS/>ls  CONFIG/ARCHS/
21164.tgz         ConfDump.log   Makefile    PIIISSE1.tgz    PPCG4AltiVec.tgz
21164GOTO.tgz     CreateDef.sh   P4SSE1.tgz  POWER.tgz       PPRO.tgz
21264.tgz         CreateDirs.sh  P4SSE2      POWER2Thin.tgz  SGIIP28.tgz
21264GOTO.tgz     CreateTar.sh   P4SSE2.tgz  POWER3.tgz      SGIIP30.tgz
ATHLON.tgz        HP9735.tgz     P5MMX.tgz   PPC604.tgz      SunUS2.tgz
ATHLON3DNow2.tgz  IA64Itan.tgz   PII.tgz     PPC604e.tgz     SunUS5.tgz
ATHLONSSE1.tgz    KillDirs.sh    PIII.tgz    PPCG4.tgz       negflt.c

so it would seem that there's both P4SSE1 and PIIISSE1. However,
"lannerfalk" (which you shared an office with for a while) is a P4,
with SSE2, and actually quite difficult to tell apart from the Xeon
chips in "turmalin":

CPU: L1 I cache: 12K, L1 D cache: 8K
CPU: L2 cache: 512K
CPU: Hyper-Threading is disabled
CPU:             Common caps: 3febfbff 00000000 00000000 00000000
CPU0: Intel(R) Pentium(R) 4 CPU 2.20GHz stepping 04

vs.

CPU: Trace cache: 12K uops, L1 D cache: 8K
CPU: L2 cache: 512K
CPU: Physical Processor ID: 0
Intel machine check reporting enabled on CPU#1.
CPU:     After generic, caps: bfebfbff 00000000 00000000 00000000
CPU:             Common caps: bfebfbff 00000000 00000000 00000000
CPU1: Intel(R) Xeon(TM) CPU 2.80GHz stepping 07

And their /proc/cpuinfo entries are also similar except for the clock
speed and number of siblings. So SSE2 is certainly not the difference
between P4 and Xeon - stepping number issue, perhaps?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wang at galton.uchicago.edu  Sat Mar 13 02:37:18 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Fri, 12 Mar 2004 19:37:18 -0600 (CST)
Subject: [R] a replacement problem
In-Reply-To: <200403121128.i2CBQnTH004555@hypatia.math.ethz.ch>
References: <200403121128.i2CBQnTH004555@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0403121933140.11884@galton.uchicago.edu>


hi,all
a quick question:  

a vector

v<-c(g,r,y,g,y,y,r)

needs to be

v<-c(1,2,3,1,3,3,2)

i.e., replace g,r,y as 1,2,3

how to do that, by the way, what key words I should use to search for such 
basic operation command?
thank you 

best



From MSchwartz at medanalytics.com  Sat Mar 13 03:38:26 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 12 Mar 2004 20:38:26 -0600
Subject: [R] a replacement problem
In-Reply-To: <Pine.LNX.4.58.0403121933140.11884@galton.uchicago.edu>
References: <200403121128.i2CBQnTH004555@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0403121933140.11884@galton.uchicago.edu>
Message-ID: <1079145506.6993.32.camel@localhost.localdomain>

On Fri, 2004-03-12 at 19:37, Yong Wang wrote:
> hi,all
> a quick question:  
> 
> a vector
> 
> v<-c(g,r,y,g,y,y,r)
> 
> needs to be
> 
> v<-c(1,2,3,1,3,3,2)
> 
> i.e., replace g,r,y as 1,2,3


A couple of options and probably more:

1. Using a loop and a quick function I wrote called "gsr" (for Global
Search and Replace):

gsr <- function(Source, Search, Replace)
{
  for (i in 1:length(Search))
  {
    Source <- replace(Source, Source == Search[i], Replace[i])
  }

 Source
}


v <- c("g", "r", "y", "g", "y", "y", "r")

gsr(v, c("g", "r", "y"), 1:3)
[1] "1" "2" "3" "1" "3" "3" "2"

Note the above returns character values, so if you want numerics:

> as.numeric(gsr(v, c("g", "r", "y"), 1:3))
[1] 1 2 3 1 3 3 2


Also, in the above function, you would want to be sure that both Search
and Replace are vectors of equal length and could add some error
checking code for that prior to the for() loop:

  if (length(Search) != length(Replace))
    stop("Search and Replace Must Have Equal Number of Items\n")



2. Using a "trick" with unclass() and factor():

> as.vector(unclass(factor(v, levels = c("g", "r", "y"))))
[1] 1 2 3 1 3 3 2


> how to do that, by the way, what key words I should use to search for such 
> basic operation command?


Strangely enough:

help.search("replace")

;-)

See ?replace for more info and there is also the grep family for complex
search and replace operations based on regex. See ?grep.

HTH,

Marc Schwartz



From hodgess at gator.uhd.edu  Sat Mar 13 04:32:17 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Fri, 12 Mar 2004 21:32:17 -0600
Subject: [R] vector and replace.
Message-ID: <200403130332.i2D3WHM00871@gator.dt.uh.edu>

Hi Yong!

How about this:

> gsr <- sample(c("g","r","y"),replace=T,15)
> gsr
 [1] "r" "g" "r" "g" "y" "y" "r" "r" "y" "g" "r" "y" "r" "r" "g"
> as.numeric(as.factor(gsr))
 [1] 2 1 2 1 3 3 2 2 3 1 2 3 2 2 1
> 

Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu


Date: Fri, 12 Mar 2004 19:37:18 -0600 (CST)
From: Yong Wang <wang at galton.uchicago.edu>
To: r-help-request at stat.math.ethz.ch
cc: r-help at stat.math.ethz.ch


hi,all
a quick question:  

a vector

v<-c(g,r,y,g,y,y,r)

needs to be

v<-c(1,2,3,1,3,3,2)

i.e., replace g,r,y as 1,2,3

how to do that, by the way, what key words I should use to search for such 
basic operation command?
thank you 

best

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Mar 13 10:08:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Mar 2004 09:08:12 +0000 (GMT)
Subject: [R] Error in "names<-.default"(`*tmp*`, value = nmstrata) :
In-Reply-To: <200403130118.19251.wolfgangpauli@web.de>
Message-ID: <Pine.LNX.4.44.0403130904510.6467-100000@gannet.stats>

This is not reproducible. The last report of this type came about from an
incorrectly specified model (the Error model was rank-deficient), and
1.9.0 alpha gave an accurate diagnosis of the error.  So please

1) try your example in 1.9.0 alpha

2) if is still fails, supply a reproducible example (what is spaeVP and 
why is it sorted?) including what you are trying to do with a term like
Error(sub/(cs*spk)).

On Sat, 13 Mar 2004, Wolfgang Pauli wrote:

> Hi,
> 
> I have a problem with aov(). I used it many times, but now I have new data, 
> tried to use it in the same way. I get I strange Error message that i can't 
> understand (see below). I guess it is caused by incompatible data types. The 
> dataframe also looks ok to me.
> 
> I use:
> spk =factor(rep(c("spk1","spk2","spk3","spk4","spk5","spk6","spk7","spk8"), 
> c(2,2,2,2,2,2,2,2)))
> cs = factor(rep(c("yes","no"),c(8,8)))
> sub = factor(sort(spaeVP))
> rati = stack(data.frame(arousal))
> rati = rati[,1]
> rating.df <-  data.frame(sub, cs, spk, rati)
> summary(aov(rati ~ cs*spk + Error(sub/(cs*spk)), data=rating.df))
> 
> I get this Error (after about 5sec):
> Error in "names<-.default"(`*tmp*`, value = nmstrata) : 
> 	names attribute must be the same length as the vector
> 
> > rating.df
>     sub  cs  spk rati
> 1     5 yes spk1   -1
> 2     5 yes spk1    0
> 3     5 yes spk2    1
> 4     5 yes spk2    0
> 5     5 yes spk3    0
> 6     5 yes spk3   -1
> 7     5 yes spk4   -1
> 8     5 yes spk4    1
> 9     5  no spk5    0
> ...
> 
> Hope somebody can help me.
> 
> with best regards,
> 
> Wolfgang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Mar 13 10:22:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Mar 2004 09:22:56 +0000 (GMT)
Subject: [R] Xeon CPU and ATLAS
In-Reply-To: <x28yi5v958.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0403130912140.6467-100000@gannet.stats>

On 13 Mar 2004, Peter Dalgaard wrote:

> rossini at blindglobe.net (A.J. Rossini) writes:
> 
> > > I think the difference between P4 and Xeon is the SSE1 vs SSE2 thing,
> > > so you'd want the P4SSE2 DLL, but others may be able to speak more
> > > authoritatively. 
> > 
> > SSE1 vs SSE2  is PIII vs PIV (so depends if your xeons are p3 or p4 (?
> > not sure if there is such a thing) based.
> 
> Sure? I have
> 
> turmalin:/usr/local/src/ATLAS/>ls  CONFIG/ARCHS/
> 21164.tgz         ConfDump.log   Makefile    PIIISSE1.tgz    PPCG4AltiVec.tgz
> 21164GOTO.tgz     CreateDef.sh   P4SSE1.tgz  POWER.tgz       PPRO.tgz
> 21264.tgz         CreateDirs.sh  P4SSE2      POWER2Thin.tgz  SGIIP28.tgz
> 21264GOTO.tgz     CreateTar.sh   P4SSE2.tgz  POWER3.tgz      SGIIP30.tgz
> ATHLON.tgz        HP9735.tgz     P5MMX.tgz   PPC604.tgz      SunUS2.tgz
> ATHLON3DNow2.tgz  IA64Itan.tgz   PII.tgz     PPC604e.tgz     SunUS5.tgz
> ATHLONSSE1.tgz    KillDirs.sh    PIII.tgz    PPCG4.tgz       negflt.c
> 
> so it would seem that there's both P4SSE1 and PIIISSE1. However,

That's because ATLAS did not always support SSE2, and it might be
faster not to use it on some systems.

>From Intel's Xeon FAQ
http://www.intel.com/cd/ids/developer/asmo-na/eng/19250.htm

   We had XMM on Pentium? III. What is the difference on Intel Xeon?

   Intel Xeon offers SSE2 technology. In addition to the features of SSE, 
   SSE2 provides support for additional simultaneous 128-bit integer and 
   double-precision floating point calculations.

which seems the definitive answer re Xeons.  (I am 99% sure all P4s have 
the same instructions set which includes SSE2.  The main Xeon differences 
are that multi-processor systems are allowed and the caches are usually 
larger than contemporaneous P4s.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From merolagio at tiscali.co.uk  Fri Mar 12 11:16:33 2004
From: merolagio at tiscali.co.uk (giovanni merola)
Date: Fri, 12 Mar 2004 10:16:33 +0000
Subject: [R] R-business case
Message-ID: <40518E01.10608@tiscali.co.uk>

Dear all,
I am putting together a business case for R and I would like to know if 
there exist statistics on the distribution of R in academia, research 
and commercial institutions. The same information about S-plus would be 
welcome. Please reply only if you have consistent and certain information.

Thank you very much, giovanni



From S.Nyangoma at cs.rug.nl  Sat Mar 13 12:07:17 2004
From: S.Nyangoma at cs.rug.nl (Stephen Nyangoma)
Date: 13 Mar 2004 12:07:17 +0100
Subject: [R] LC-MS data & netCEL files
Message-ID: <1079176037.14712.5.camel@iwi142>

Hi,
I have LS-MS cel files that I want to acess. Is there anyone who has
experience with handling of these files. I am unable to install the
netCEL sofware also. Can someone explain how to install this in R
(linux).

Is someone having R software/resources for handling LC-MS data? 

Thanks for your help.

Stephen.



From ozric at web.de  Sun Mar 14 15:39:19 2004
From: ozric at web.de (Christian Schulz)
Date: Sun, 14 Mar 2004 15:39:19 +0100
Subject: [R] levels x-axis (histogram)
Message-ID: <200403141539.19774.ozric@web.de>

Hi,

how it's possible to get the level values (not the numeric values!) 
on the x-axis in a lattice histogram?  I found nothing related
in archives and my my attempts didn't success , but AKTIVE is a
factor?

Many thanks, christian

histogram( ~AKTIVE | BIRTHMONTH,aspect=1,xlab="Geburtsmonat",data=dmy)

> is.factor(dmy$AKTIVE)
[1] TRUE
> fr(dmy$AKTIVE)
      Count     Prcnt
Nein  53954  77.00234
Ja    16114  22.99766
Total 70068 100.00000



From deepayan at stat.wisc.edu  Sat Mar 13 15:54:59 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 13 Mar 2004 08:54:59 -0600
Subject: [R] levels x-axis (histogram)
In-Reply-To: <200403141539.19774.ozric@web.de>
References: <200403141539.19774.ozric@web.de>
Message-ID: <200403130854.59842.deepayan@stat.wisc.edu>

On Sunday 14 March 2004 08:39, Christian Schulz wrote:
> Hi,
>
> how it's possible to get the level values (not the numeric values!)
> on the x-axis in a lattice histogram?  I found nothing related
> in archives and my my attempts didn't success , but AKTIVE is a
> factor?

I guess this would qualify as a bug. It should be fixed by R 1.9.0, for 
now, a possible workaround is to add

histogram( ~AKTIVE | BIRTHMONTH,aspect=1,xlab="Geburtsmonat",data=dmy,
          xlim = levels(dmy$AKTIVE))

Deepayan



From S.Nyangoma at cs.rug.nl  Sat Mar 13 16:33:46 2004
From: S.Nyangoma at cs.rug.nl (Stephen Nyangoma)
Date: 13 Mar 2004 16:33:46 +0100
Subject: [R] Installing ncdf package
Message-ID: <1079192026.14712.20.camel@iwi142>

Hi
I used the command 

R CMD INSTALL ncdf

to install ncdf package in linux. Can somebody explain to me what might
be wrong. Thanks. Below is the error.



* Installing *source* package 'ncdf' ...
Special note:
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking /usr/local/include/netcdf.h usability... no
checking /usr/local/include/netcdf.h presence... no
checking for /usr/local/include/netcdf.h... no
checking /usr/include/netcdf.h usability... no
checking /usr/include/netcdf.h presence... no
checking for /usr/include/netcdf.h... no
checking /iwi200/home/users/stephen/include/netcdf.h usability... no
checking /iwi200/home/users/stephen/include/netcdf.h presence... no
checking for /iwi200/home/users/stephen/include/netcdf.h... no
checking /iwi200/home/users/stephen/src/netcdf/netcdf.h usability... no
checking /iwi200/home/users/stephen/src/netcdf/netcdf.h presence... no
checking for /iwi200/home/users/stephen/src/netcdf/netcdf.h... no
checking /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/netcdf.h
usability... no
checking /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/netcdf.h
presence... no
checking for
/iwi200/home/users/stephen/src/netcdf/netcdf-3.4/netcdf.h... no
checking
/iwi200/home/users/stephen/src/netcdf/netcdf-3.4/src/libsrc/netcdf.h
usability... no
checking
/iwi200/home/users/stephen/src/netcdf/netcdf-3.4/src/libsrc/netcdf.h
presence... no
checking for
/iwi200/home/users/stephen/src/netcdf/netcdf-3.4/src/libsrc/netcdf.h...
no

Fatal error: I cannot find the directory that holds the netcdf include
file netcdf.h!
You can specify it as follows:
      ./configure --with-netcdf_incdir=directory_with_file_netcdf.h

 *** Special note for R CMD INSTALL users:
*********************************
     The syntax for specifying multiple --configure-args does not seem
to be
     well documented in R.  If you have installed the netcdf include and
library
     directories in some non-standard location, you can specify BOTH
these
     during the R CMD INSTALL process using the following syntax:

   R CMD INSTALL
--configure-args="-with-netcdf_incdir=/path/to/netcdf/incdir
-with-netcdf_libdir=/path/to/netcdf/libdir" ncdf_1.1.tar.gz

     where you should, of course, specify your own netcdf include and
library
     directories, and the actual package name.

***************************************************************************

ERROR: configuration failed for package 'ncdf'



From wolfgangpauli at web.de  Sat Mar 13 17:56:27 2004
From: wolfgangpauli at web.de (Wolfgang Pauli)
Date: Sat, 13 Mar 2004 17:56:27 +0100
Subject: [R] ... Error in "names<-.default"(`*tmp*`, value = nmstrata) 
Message-ID: <200403131756.27759.wolfgangpauli@web.de>

Hi,
OK, I will describe it in more detail.In the meantime I installed the newest debian package of r-base etc. and the error message changed from:Error in "names<-.default"(`*tmp*`, value = nmstrata) :         names attribute must be the same length as the vectorto:Error model is singular in: aov(rati ~ cs * spk + Error(sub/(cs * spk)), data = rating.df) 
The model is a repeated measures anova that's why I use the Error() term. I found the hint for this usage in "Notes on the use of R for psychology experiments and questionaires" (Jonathan Baron, 2003).cs and speaker are within-subjects factors. rati is the dependent variable: ratings done by subjects. sub are the subject-codes (between-subjects factor). SpaeVP is sorted because in the preprocessing I needed to recoded data of each subject and did:
>testfac <- as.factor(spaeVP)>levels(testfac) -> vpnso I could do "for ( i in vpn)".after that data (rati) was sorted so I sorted spaeVp too, before putting it into the dataframe. (I found no better solution (newbie))
Now follows the syntax I already sent last time. After that is more data of my dataframe. 
with best regards,
Wolfgang Pauli
I use:spk =factor(rep(c("spk1","spk2","spk3","spk4","spk5","spk6","spk7","spk8"), c(2,2,2,2,2,2,2,2)))cs = factor(rep(c("yes","no"),c(8,8)))sub = factor(sort(spaeVP))rati = stack(data.frame(arousal))rati = rati[,1]rating.df <-  data.frame(sub, cs, spk, rati)summary(aov(rati ~ cs*spk + Error(sub/(cs*spk)), data=rating.df))
> arousal (30 subjects, and 16 stimuli) before using stack() and putting it into "rati"      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [1,]   -1   -2   -1   -1   -2    0   -1    0   -2    -1     0     0    -2 [2,]    0    0    0    0   -1    1    0   -1   -2     0     1    -1    -1 [3,]    1   -1   -1   -2   -1    0    0    0   -3     0    -1     0     2 [4,]    0    0    2    0   -1    1    0    1    0     2    -2     1    -2 [5,]    0    0   -1   -1   -1    0    0    0    0     0     1     0     1 [6,]   -1    0   -2    0   -2    0    0    0    0     0    -1     1     0 [7,]   -1    0    0    0   -1    0    0   -1    0     1    -1     1    -1 [8,]    1    0    0    0   -1    0    0    1    0    -1    -3     0     1 [9,]    0   -1    0    0   -1    0    0    1    0    -1     0     1     3[10,]    0   -2   -1   -1    0    2    0   -2   -2     1    -1     2     1[11,]    0   -1    0    0    0   -1   -1    0   -1    -1     1     1     0[12,]   -1   -3    0   -2    0    0    0    0   -1     0     1     2     1[13,]    0   -1   -1   -1    0   -1    0   -1   -1    -1    -1     1     0[14,]    1    2    2   -1    0    0    0    0    1    -1     0     1     1[15,]    0    0    0   -1    0    1   -1   -1   -1     0    -1    -2     1[16,]    1    0   -2   -1    0   -1    0    0   -1     1     1     1     0      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [1,]     0     0     0     0     0     0     1     1     0     0     1     1 [2,]    -2     0    -1    -1     1    -3     3     0     0    -1     0     0 [3,]     1     0     0     0     1    -2     0    -1    -1     0     0    -1 [4,]     2     0     1     0     0     0     3    -1     1     0    -1     0 [5,]    -2     1    -1     1     0    -2     1     0     0     0    -1    -1 [6,]     0     1    -1     0    -1    -2     3     3     2     0    -1     0 [7,]     0     1     1     0     0    -1     0     1    -1     1     0     0 [8,]     1     0     0    -2     0     0    -2     1     1     0     0    -1 [9,]     1     0    -2     0     0    -2     2     2     0     1     0     0[10,]     1     0     0     0     0    -1     1     1     0     1     1     2[11,]     0    -1     1     1     0     0     0     1    -2     1     0     1[12,]     1     1     0     1    -1    -1     1     2     0     1     0     0[13,]     2     0    -1     1    -1    -3     4     1     0     1    -1    -2[14,]    -1     0    -1     2    -1    -1     1     0    -1     1     2     0[15,]     2     0     0    -2    -2     0     0     2     2     2     1    -1[16,]     0     0     0     1     0    -1     1     0     1     1     1    -1      [,26] [,27] [,28] [,29] [,30] [,31] [,32] [1,]     0    -1    -1    -1    -2     0     1 [2,]    -2    -1    -1     1     0     1     2 [3,]     0     0    -1    -2     2    -2     1 [4,]     1     2     0     1    -1     0     1 [5,]     0     0     1     2    -4    -2     0 [6,]     0     1    -1     0    -1     2     0 [7,]    -1    -2    -1     1     0    -2     1 [8,]     1    -2     0     0    -2     1     0 [9,]    -2     0     0     0    -3     1     0[10,]     2    -2    -1    -1     1     2     0[11,]    -1    -1    -1    -1     1     1     2[12,]    -1     0     0     0    -1     0     1[13,]    -2     1    -1    -1    -2     2     0[14,]     0    -1    -1     0     0     2     2[15,]     1    -1     0     0     0     3     1[16,]     0     0     2     1     0     2     1> 


> rating.df    sub  cs  spk rati1     5 yes spk1   -12     5 yes spk1    03     5 yes spk2    14     5 yes spk2    05     5 yes spk3    06     5 yes spk3   -17     5 yes spk4   -18     5 yes spk4    19     5  no spk5    010    5  no spk5    011    5  no spk6    012    5  no spk6   -113    5  no spk7    014    5  no spk7    115    5  no spk8    016    5  no spk8    117    6 yes spk1   -218    6 yes spk1    019    6 yes spk2   -120    6 yes spk2    021    6 yes spk3    022    6 yes spk3    023    6 yes spk4    024    6 yes spk4    025    6  no spk5   -126    6  no spk5   -227    6  no spk6   -128    6  no spk6   -329    6  no spk7   -130    6  no spk7    231    6  no spk8    032    6  no spk8    033    7 yes spk1   -134    7 yes spk1    035    7 yes spk2   -136    7 yes spk2    237    7 yes spk3   -138    7 yes spk3   -239    7 yes spk4    040    7 yes spk4    041    7  no spk5    042    7  no spk5   -143    7  no spk6    044    7  no spk6    045    7  no spk7   -146    7  no spk7    247    7  no spk8    048    7  no spk8   -249    8 yes spk1   -150    8 yes spk1    051    8 yes spk2   -252    8 yes spk2    053    8 yes spk3   -154    8 yes spk3    055    8 yes spk4    056    8 yes spk4    057    8  no spk5    058    8  no spk5   -159    8  no spk6    060    8  no spk6   -261    8  no spk7   -162    8  no spk7   -163    8  no spk8   -164    8  no spk8   -165    9 yes spk1   -266    9 yes spk1   -167    9 yes spk2   -168    9 yes spk2   -169    9 yes spk3   -170    9 yes spk3   -271    9 yes spk4   -172    9 yes spk4   -173    9  no spk5   -174    9  no spk5    075    9  no spk6    076    9  no spk6    077    9  no spk7    078    9  no spk7    079    9  no spk8    080    9  no spk8    081   10 yes spk1    082   10 yes spk1    183   10 yes spk2    084   10 yes spk2    185   10 yes spk3    086   10 yes spk3    087   10 yes spk4    088   10 yes spk4    089   10  no spk5    090   10  no spk5    291   10  no spk6   -192   10  no spk6    093   10  no spk7   -194   10  no spk7    095   10  no spk8    196   10  no spk8   -197   11 yes spk1   -198   11 yes spk1    099   11 yes spk2    0100  11 yes spk2    0101  11 yes spk3    0102  11 yes spk3    0103  11 yes spk4    0104  11 yes spk4    0105  11  no spk5    0106  11  no spk5    0107  11  no spk6   -1108  11  no spk6    0109  11  no spk7    0110  11  no spk7    0111  11  no spk8   -1112  11  no spk8    0113  12 yes spk1    0114  12 yes spk1   -1115  12 yes spk2    0116  12 yes spk2    1117  12 yes spk3    0118  12 yes spk3    0119  12 yes spk4   -1120  12 yes spk4    1121  12  no spk5    1122  12  no spk5   -2123  12  no spk6    0124  12  no spk6    0125  12  no spk7   -1126  12  no spk7    0127  12  no spk8   -1128  12  no spk8    0129  13 yes spk1   -2130  13 yes spk1   -2131  13 yes spk2   -3132  13 yes spk2    0133  13 yes spk3    0134  13 yes spk3    0135  13 yes spk4    0136  13 yes spk4    0137  13  no spk5    0138  13  no spk5   -2139  13  no spk6   -1140  13  no spk6   -1141  13  no spk7   -1142  13  no spk7    1143  13  no spk8   -1144  13  no spk8   -1145  14 yes spk1   -1146  14 yes spk1    0147  14 yes spk2    0148  14 yes spk2    2149  14 yes spk3    0150  14 yes spk3    0151  14 yes spk4    1152  14 yes spk4   -1153  14  no spk5   -1154  14  no spk5    1155  14  no spk6   -1156  14  no spk6    0157  14  no spk7   -1158  14  no spk7   -1159  14  no spk8    0160  14  no spk8    1161  16 yes spk1    0162  16 yes spk1    1163  16 yes spk2   -1164  16 yes spk2   -2165  16 yes spk3    1166  16 yes spk3   -1167  16 yes spk4   -1168  16 yes spk4   -3169  16  no spk5    0170  16  no spk5   -1171  16  no spk6    1172  16  no spk6    1173  16  no spk7   -1174  16  no spk7    0175  16  no spk8   -1176  16  no spk8    1177  17 yes spk1    0178  17 yes spk1   -1179  17 yes spk2    0180  17 yes spk2    1181  17 yes spk3    0182  17 yes spk3    1183  17 yes spk4    1184  17 yes spk4    0185  17  no spk5    1186  17  no spk5    2187  17  no spk6    1188  17  no spk6    2189  17  no spk7    1190  17  no spk7    1191  17  no spk8   -2192  17  no spk8    1193  19 yes spk1   -2194  19 yes spk1   -1195  19 yes spk2    2196  19 yes spk2   -2197  19 yes spk3    1198  19 yes spk3    0199  19 yes spk4   -1200  19 yes spk4    1201  19  no spk5    3202  19  no spk5    1203  19  no spk6    0204  19  no spk6    1205  19  no spk7    0206  19  no spk7    1207  19  no spk8    1208  19  no spk8    0209  20 yes spk1    0210  20 yes spk1   -2211  20 yes spk2    1212  20 yes spk2    2213  20 yes spk3   -2214  20 yes spk3    0215  20 yes spk4    0216  20 yes spk4    1217  20  no spk5    1218  20  no spk5    1219  20  no spk6    0220  20  no spk6    1221  20  no spk7    2222  20  no spk7   -1223  20  no spk8    2224  20  no spk8    0225  23 yes spk1    0226  23 yes spk1    0227  23 yes spk2    0228  23 yes spk2    0229  23 yes spk3    1230  23 yes spk3    1231  23 yes spk4    1232  23 yes spk4    0233  23  no spk5    0234  23  no spk5    0235  23  no spk6   -1236  23  no spk6    1237  23  no spk7    0238  23  no spk7    0239  23  no spk8    0240  23  no spk8    0241  24 yes spk1    0242  24 yes spk1   -1243  24 yes spk2    0244  24 yes spk2    1245  24 yes spk3   -1246  24 yes spk3   -1247  24 yes spk4    1248  24 yes spk4    0249  24  no spk5   -2250  24  no spk5    0251  24  no spk6    1252  24  no spk6    0253  24  no spk7   -1254  24  no spk7   -1255  24  no spk8    0256  24  no spk8    0257  25 yes spk1    0258  25 yes spk1   -1259  25 yes spk2    0260  25 yes spk2    0261  25 yes spk3    1262  25 yes spk3    0263  25 yes spk4    0264  25 yes spk4   -2265  25  no spk5    0266  25  no spk5    0267  25  no spk6    1268  25  no spk6    1269  25  no spk7    1270  25  no spk7    2271  25  no spk8   -2272  25  no spk8    1273  27 yes spk1    0274  27 yes spk1    1275  27 yes spk2    1276  27 yes spk2    0277  27 yes spk3    0278  27 yes spk3   -1279  27 yes spk4    0280  27 yes spk4    0281  27  no spk5    0282  27  no spk5    0283  27  no spk6    0284  27  no spk6   -1285  27  no spk7   -1286  27  no spk7   -1287  27  no spk8   -2288  27  no spk8    0289  28 yes spk1    0290  28 yes spk1   -3291  28 yes spk2   -2292  28 yes spk2    0293  28 yes spk3   -2294  28 yes spk3   -2295  28 yes spk4   -1296  28 yes spk4    0297  28  no spk5   -2298  28  no spk5   -1299  28  no spk6    0300  28  no spk6   -1301  28  no spk7   -3302  28  no spk7   -1303  28  no spk8    0304  28  no spk8   -1305  91 yes spk1    1306  91 yes spk1    3307  91 yes spk2    0308  91 yes spk2    3309  91 yes spk3    1310  91 yes spk3    3311  91 yes spk4    0312  91 yes spk4   -2313  91  no spk5    2314  91  no spk5    1315  91  no spk6    0316  91  no spk6    1317  91  no spk7    4318  91  no spk7    1319  91  no spk8    0320  91  no spk8    1321  92 yes spk1    1322  92 yes spk1    0323  92 yes spk2   -1324  92 yes spk2   -1325  92 yes spk3    0326  92 yes spk3    3327  92 yes spk4    1328  92 yes spk4    1329  92  no spk5    2330  92  no spk5    1331  92  no spk6    1332  92  no spk6    2333  92  no spk7    1334  92  no spk7    0335  92  no spk8    2336  92  no spk8    0337  93 yes spk1    0338  93 yes spk1    0339  93 yes spk2   -1340  93 yes spk2    1341  93 yes spk3    0342  93 yes spk3    2343  93 yes spk4   -1344  93 yes spk4    1345  93  no spk5    0346  93  no spk5    0347  93  no spk6   -2348  93  no spk6    0349  93  no spk7    0350  93  no spk7   -1351  93  no spk8    2352  93  no spk8    1353  94 yes spk1    0354  94 yes spk1   -1355  94 yes spk2    0356  94 yes spk2    0357  94 yes spk3    0358  94 yes spk3    0359  94 yes spk4    1360  94 yes spk4    0361  94  no spk5    1362  94  no spk5    1363  94  no spk6    1364  94  no spk6    1365  94  no spk7    1366  94  no spk7    1367  94  no spk8    2368  94  no spk8    1369  96 yes spk1    1370  96 yes spk1    0371  96 yes spk2    0372  96 yes spk2   -1373  96 yes spk3   -1374  96 yes spk3   -1375  96 yes spk4    0376  96 yes spk4    0377  96  no spk5    0378  96  no spk5    1379  96  no spk6    0380  96  no spk6    0381  96  no spk7   -1382  96  no spk7    2383  96  no spk8    1384  96  no spk8    1385  97 yes spk1    1386  97 yes spk1    0387  97 yes spk2   -1388  97 yes spk2    0389  97 yes spk3   -1390  97 yes spk3    0391  97 yes spk4    0392  97 yes spk4   -1393  97  no spk5    0394  97  no spk5    2395  97  no spk6    1396  97  no spk6    0397  97  no spk7   -2398  97  no spk7    0399  97  no spk8   -1400  97  no spk8   -1401  98 yes spk1    0402  98 yes spk1   -2403  98 yes spk2    0404  98 yes spk2    1405  98 yes spk3    0406  98 yes spk3    0407  98 yes spk4   -1408  98 yes spk4    1409  98  no spk5   -2410  98  no spk5    2411  98  no spk6   -1412  98  no spk6   -1413  98  no spk7   -2414  98  no spk7    0415  98  no spk8    1416  98  no spk8    0417  99 yes spk1   -1418  99 yes spk1   -1419  99 yes spk2    0420  99 yes spk2    2421  99 yes spk3    0422  99 yes spk3    1423  99 yes spk4   -2424  99 yes spk4   -2425  99  no spk5    0426  99  no spk5   -2427  99  no spk6   -1428  99  no spk6    0429  99  no spk7    1430  99  no spk7   -1431  99  no spk8   -1432  99  no spk8    0433 911 yes spk1   -1434 911 yes spk1   -1435 911 yes spk2   -1436 911 yes spk2    0437 911 yes spk3    1438 911 yes spk3   -1439 911 yes spk4   -1440 911 yes spk4    0441 911  no spk5    0442 911  no spk5   -1443 911  no spk6   -1444 911  no spk6    0445 911  no spk7   -1446 911  no spk7   -1447 911  no spk8    0448 911  no spk8    2449 912 yes spk1   -1450 912 yes spk1    1451 912 yes spk2   -2452 912 yes spk2    1453 912 yes spk3    2454 912 yes spk3    0455 912 yes spk4    1456 912 yes spk4    0457 912  no spk5    0458 912  no spk5   -1459 912  no spk6   -1460 912  no spk6    0461 912  no spk7   -1462 912  no spk7    0463 912  no spk8    0464 912  no spk8    1465 914 yes spk1   -2466 914 yes spk1    0467 914 yes spk2    2468 914 yes spk2   -1469 914 yes spk3   -4470 914 yes spk3   -1471 914 yes spk4    0472 914 yes spk4   -2473 914  no spk5   -3474 914  no spk5    1475 914  no spk6    1476 914  no spk6   -1477 914  no spk7   -2478 914  no spk7    0479 914  no spk8    0480 914  no spk8    0481 919 yes spk1    0482 919 yes spk1    1483 919 yes spk2   -2484 919 yes spk2    0485 919 yes spk3   -2486 919 yes spk3    2487 919 yes spk4   -2488 919 yes spk4    1489 919  no spk5    1490 919  no spk5    2491 919  no spk6    1492 919  no spk6    0493 919  no spk7    2494 919  no spk7    2495 919  no spk8    3496 919  no spk8    2497 922 yes spk1    1498 922 yes spk1    2499 922 yes spk2    1500 922 yes spk2    1501 922 yes spk3    0502 922 yes spk3    0503 922 yes spk4    1504 922 yes spk4    0505 922  no spk5    0506 922  no spk5    0507 922  no spk6    2508 922  no spk6    1509 922  no spk7    0510 922  no spk7    2511 922  no spk8    1512 922  no spk8    1>



From tblackw at umich.edu  Sat Mar 13 18:25:30 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Sat, 13 Mar 2004 12:25:30 -0500 (EST)
Subject: [R] Installing ncdf package
In-Reply-To: <1079192026.14712.20.camel@iwi142>
References: <1079192026.14712.20.camel@iwi142>
Message-ID: <Pine.SOL.4.58.0403131223110.13149@zektor.gpcc.itd.umich.edu>

Stephen  -

The error message suggests that the 'ncdf' package depends on
having the 'netcdf' source code.  This is available from

http://www.unidata.ucar.edu/pakages/netcdf

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Sat, 13 Mar 2004, Stephen Nyangoma wrote:

> Hi
> I used the command
>
> R CMD INSTALL ncdf
>
> to install ncdf package in linux. Can somebody explain to me what might
> be wrong. Thanks. Below is the error.
>
>
>
> * Installing *source* package 'ncdf' ...
> Special note:
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking /usr/local/include/netcdf.h usability... no
> checking /usr/local/include/netcdf.h presence... no
> checking for /usr/local/include/netcdf.h... no
> checking /usr/include/netcdf.h usability... no
> checking /usr/include/netcdf.h presence... no
> checking for /usr/include/netcdf.h... no
> checking /iwi200/home/users/stephen/include/netcdf.h usability... no
> checking /iwi200/home/users/stephen/include/netcdf.h presence... no
> checking for /iwi200/home/users/stephen/include/netcdf.h... no
> checking /iwi200/home/users/stephen/src/netcdf/netcdf.h usability... no
> checking /iwi200/home/users/stephen/src/netcdf/netcdf.h presence... no
> checking for /iwi200/home/users/stephen/src/netcdf/netcdf.h... no
> checking /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/netcdf.h
> usability... no
> checking /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/netcdf.h
> presence... no
> checking for
> /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/netcdf.h... no
> checking
> /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/src/libsrc/netcdf.h
> usability... no
> checking
> /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/src/libsrc/netcdf.h
> presence... no
> checking for
> /iwi200/home/users/stephen/src/netcdf/netcdf-3.4/src/libsrc/netcdf.h...
> no
>
> Fatal error: I cannot find the directory that holds the netcdf include
> file netcdf.h!
> You can specify it as follows:
>       ./configure --with-netcdf_incdir=directory_with_file_netcdf.h
>
>  *** Special note for R CMD INSTALL users:
> *********************************
>      The syntax for specifying multiple --configure-args does not seem
> to be
>      well documented in R.  If you have installed the netcdf include and
> library
>      directories in some non-standard location, you can specify BOTH
> these
>      during the R CMD INSTALL process using the following syntax:
>
>    R CMD INSTALL
> --configure-args="-with-netcdf_incdir=/path/to/netcdf/incdir
> -with-netcdf_libdir=/path/to/netcdf/libdir" ncdf_1.1.tar.gz
>
>      where you should, of course, specify your own netcdf include and
> library
>      directories, and the actual package name.
>
> ***************************************************************************
>
> ERROR: configuration failed for package 'ncdf'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andrewr at uidaho.edu  Sat Mar 13 18:55:32 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Sat, 13 Mar 2004 09:55:32 -0800
Subject: [R] Assessing Kappa values (was: Non-linear regression problem: R
	vs JMP (long))
Message-ID: <200403130955.32398.andrewr@uidaho.edu>

Dear R-friends,

I've been able to induce R to fit the non-linear model in my previous post, by 
changing the tolerance to 0.475.  I've calculated the kappa of the gradient 
matrix from the model as being 2487.294.  Is that very large?  I cannot find 
any reference to a range of suitable values.  The matrix dimension is 875 x 
9.

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From albedo at pisem.net  Sat Mar 13 22:06:09 2004
From: albedo at pisem.net (Albedo)
Date: Sat, 13 Mar 2004 23:06:09 +0200
Subject: [R] nnet classification accuracy vs. other models
Message-ID: <405377C1.50305@pisem.net>

I was wandering if anybody ever tried to compare the classification
accuracy of nnet to other (rpart, tree, bagging) models. From what I
know, there is no reason to expect a significant difference in 
classification accuracy between these models, yet in my particular case
I get about 10% error rate for tree, rpart and bagging model and 80% 
error rate for nnet, applied to the same data.

Thanks.



From edgar at cs.uprm.edu  Sun Mar 14 01:11:27 2004
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Sat, 13 Mar 2004 19:11:27 -0500 (EST)
Subject: [R] nnet classification accuracy vs. other models
In-Reply-To: <405377C1.50305@pisem.net>
Message-ID: <Pine.GSO.4.33.0403131904120.24094-100000@cs.uprm.edu>

I think that you are using nnet incorrectly. I have compared several
classifiers (including that ones that you mention in your e-mail) on the
same dataset and I have never found more of a 20% of difference in the
missclassification error. Of course, I estimated the misclassification
error by cross validation.

Regards
Edgar Acuna
UPR-MATH

On Sat, 13 Mar 2004, Albedo wrote:

> I was wandering if anybody ever tried to compare the classification
> accuracy of nnet to other (rpart, tree, bagging) models. From what I
> know, there is no reason to expect a significant difference in
> classification accuracy between these models, yet in my particular case
> I get about 10% error rate for tree, rpart and bagging model and 80%
> error rate for nnet, applied to the same data.
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ivankautter at hotmail.com  Sun Mar 14 00:38:34 2004
From: ivankautter at hotmail.com (Ivan Kautter)
Date: Sat, 13 Mar 2004 15:38:34 -0800
Subject: [R] recursive matrix multiplication question
Message-ID: <BAY2-F133Wk0jVKyNzq00038d54@hotmail.com>

List serv subscribers,

I am wondering if there is an efficient means of doing recursive matrix 
multiplication in R.  My data resides in a 4 X 2541 matrix from which I need 
to extract 2541 2X2 matrices based on each row.  If I attempt something like 
this:

function(AO)
{A<-AO
{for(t in 1:4)
A[t+1]<-matrix(h0m[t,1:4],nrow=2,ncol=2,byrow=FALSE)%*%matrix(h0m[t+1,1:4],nrow=2,ncol=2,byrow=FALSE)
}
}

I get this type of error:

1: number of items to replace is not a multiple of replacement length
2: number of items to replace is not a multiple of replacement length
3: number of items to replace is not a multiple of replacement length
4: number of items to replace is not a multiple of replacement length

I believe I understand the error here in that I am attempting to add more 
elements to an object that is only a 2x2 matrix.  So is there some way to 
just update the value of the matrix recursively?  If you are asking yourself 
why I am bothering to do this if I mean to multiple only 4 matrices, I 
actually intend to multiple up to all 2541 of them, which seems no small 
task.  Ultimately, I would like to define the time period over which I am 
multiplyig these essentially time-dependent matrices and obtain the 
eigenvalues of the final matrix product.

If anyone can offer any assistance, I would greatly appreciate it.  Thanks.

Ivan Kautter




From spencer.graves at pdf.com  Sun Mar 14 01:39:52 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 13 Mar 2004 16:39:52 -0800
Subject: [R] Sequential matrix multiplactions (was:  recursive matrix
	multiplication question)
In-Reply-To: <BAY2-F133Wk0jVKyNzq00038d54@hotmail.com>
References: <BAY2-F133Wk0jVKyNzq00038d54@hotmail.com>
Message-ID: <4053A9D8.1080209@pdf.com>

  Is the following what you want:

A1 <- array(1:20, dim=c(5, 4))
seq.mult <-
function(AO){
A<-AO
N <- dim(A)[1]
for(i in 1:(N-1)){
A[i+1, ] <- (matrix(AO[i,1:4],nrow=2,ncol=2,byrow=FALSE)
%*% matrix(AO[i+1,1:4],nrow=2,ncol=2,byrow=FALSE))
}
A
}
seq.mult(A1)
[,1] [,2] [,3] [,4]
[1,] 1 6 11 16
[2,] 79 124 199 344
[3,] 102 157 242 397
[4,] 129 194 289 454
[5,] 160 235 340 515

You got the error (or warning) by omitting the comma, writing A[t+1], 
where I wrote A[i+1,]. Any matrix is also a vector and can be addressed 
as such. If A is a 5 x 4 matrix, as in this example, then A[t+1] 
interprets it as a 1 x 20 vector.

I hope you won't object if I comment on a few other things I noticed: 
First, I assume your data reside in a 2541 x 4 matrix, not a 4 x 2541 
matrix. The modified version of the function should work for the former 
but not the latter. Second, I don't know where "h0m" is defined; I 
assume you meant to write AO there.

Also, "t" is the function to transpose a matrix. While R is capable of 
distinguishing matrices from functions in most contexts, there are some 
contexts where it is not obvious what is requested, and it is therefore 
usually not prudent to use the name of a function for some other 
purpose. This is done routinely for function arguments. For example, the 
density function for Student's t distribution, dt, has an argument df 
for degrees of freedom, but df is also the density function for the F 
distribution. However, I try to avoid that, to the point of trying names 
I plan to use to see if they are already defined.

Your subject line is misleading: I saw no recursion in your question. A 
"recursion", according to the third (and seemingly most relevant here) 
definition in "www.m-w.com", is "a computer programming technique 
involving the use of a procedure, subroutine, function, or algorithm 
that calls itself".

You may already know that the function to get eigenvalues and vectors is 
"eigen".

hope this helps.
spencer graves

Ivan Kautter wrote:

> List serv subscribers,
>
> I am wondering if there is an efficient means of doing recursive 
> matrix multiplication in R. My data resides in a 4 X 2541 matrix from 
> which I need to extract 2541 2X2 matrices based on each row. If I 
> attempt something like this:
>
> function(AO)
> {A<-AO
> {for(t in 1:4)
> A[t+1]<-matrix(h0m[t,1:4],nrow=2,ncol=2,byrow=FALSE)%*%matrix(h0m[t+1,1:4],nrow=2,ncol=2,byrow=FALSE) 
>
> }
> }
>
> I get this type of error:
>
> 1: number of items to replace is not a multiple of replacement length
> 2: number of items to replace is not a multiple of replacement length
> 3: number of items to replace is not a multiple of replacement length
> 4: number of items to replace is not a multiple of replacement length
>
> I believe I understand the error here in that I am attempting to add 
> more elements to an object that is only a 2x2 matrix. So is there some 
> way to just update the value of the matrix recursively? If you are 
> asking yourself why I am bothering to do this if I mean to multiple 
> only 4 matrices, I actually intend to multiple up to all 2541 of them, 
> which seems no small task. Ultimately, I would like to define the time 
> period over which I am multiplyig these essentially time-dependent 
> matrices and obtain the eigenvalues of the final matrix product.
>
> If anyone can offer any assistance, I would greatly appreciate it. 
> Thanks.
>
> Ivan Kautter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From mash at econs.umass.edu  Sun Mar 14 11:24:25 2004
From: mash at econs.umass.edu (Michael Ash)
Date: Sun, 14 Mar 2004 05:24:25 -0500 (EST)
Subject: [R] 1.8.1 Make problem on SunOS
Message-ID: <Pine.GSO.4.55.0403140512110.6887@shell1.oit.umass.edu>


I am trying to make R-1.8.1 on (SunOS shell1 5.8 Generic_108528-15 sun4u sparc SUNW,UltraAX-i2). I did
./configure
make
Configure output seems ok.  The make proceeds until the following line appears, repeated indefinitely (until I break):

./config.status: ./confstat28489-19881/subs.frag: cannot overwrite existing file

I suspect that this may involve write permissions (and maybe
the umask set in config.status). I am attempting the make in
a directory where I have rwx permission.  (I've also tried
this with "./configure --prefix=/home/me" with the same
result.  I don't have superuser permission on the machine in
question and am planning to install R to my home directory.)

Any suggestions?

Thank you very much.

Best regards,

Michael Ash, Assistant Professor
  of Economics and Public Policy
Department of Economics and CPPA
University of Massachusetts
Amherst, MA 01003
Tel 413-545-6329 Fax 413-545-2921
Email mash at econs.umass.edu
http://people.umass.edu/maash



From chrysopa at insecta.ufv.br  Sun Mar 14 11:26:20 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Sun, 14 Mar 2004 07:26:20 -0300
Subject: [R] Problems with SJava instalation
In-Reply-To: <200403051329.OAA22888@jacaranda.math.u-psud.fr>
References: <200403051329.OAA22888@jacaranda.math.u-psud.fr>
Message-ID: <200403140722.01121.chrysopa@insecta.ufv.br>

Em Sex 05 Mar 2004 10:29, Jean Coursol escreveu:
> On  5 Mar, Ronaldo Reis Jr. wrote:
> > Hi,
> >
> > I'm try to use the SJava package. The install is OK. In R I have this
> > error message:
> >
> > --------------------------
> >
> >> library(SJava)
> >
> > Error in dyn.load(x, as.logical(local), as.logical(now)) :
> >         unable to load shared library
> > "/opt/lib/R/site-library/SJava/libs/SJava.so":
> >   libRSNativeJava.so: cannot open shared object file: No such file or
> > directory
> > Error in library(SJava) : .First.lib failed
> > ---------------------------
> >
> > The R say that file libRSNativeJava.so dont exist, but it exist.
> >
> > ---------------------------
> > [root at zeus Rwork]# ls /opt/lib/R/site-library/SJava/libs/*
> > /opt/lib/R/site-library/SJava/libs/SJava.so
> > /opt/lib/R/site-library/SJava/libs/libRSNativeJava.so
> > ---------------------------
> >
> > When I run the RJava script I have this error:
> >
> > ---------------------------
> > /opt/lib/R/site-library/SJava/scripts/RJava
> > Loading RInterpreter library
> > Exception in thread "main" java.lang.UnsatisfiedLinkError: no
> > RInterpreter in java.library.path
> >         at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1403)
> >         at java.lang.Runtime.loadLibrary0(Runtime.java:788)
> >         at java.lang.System.loadLibrary(System.java:832)
> >         at
> > org.omegahat.R.Java.ROmegahatInterpreter.<clinit>(ROmegahatInterpreter.ja
> >va:34) at org.omegahat.R.Java.Examples.JavaRCall.main(JavaRCall.java:11)
> > ---------------------------
> >
> > I'm using GNU/Debian stable/testing with j2sdk 1.4:
> >
> > ---------------------------
> > ii  j2sdk1.4       1.4.1-6        Blackdown Java(TM) 2 SDK, Standard
> > Edition ii  j2sdk1.4-src   1.4.1-6        Blackdown Java(TM) 2 SDK,
> > Standard Edition, ii  j2se-common    1.1            Common facilities for
> > all Java2 Standard Edi ---------------------------
> >
> > What is my problem?
> >
> > Thanks
> > Ronaldo
>
> Executing ldconfig may help.
>
> Jean Coursol

Hi,

I try to execute ldconfig and the SJava dont work. I try to install with 
gcc-3.3 (was gcc-2.95 in my first instalation). Where is the problem?

Thanks
Ronaldo
-- 
If all the world's economists were laid end to end, we wouldn't reach a
conclusion.
		-- William Baumol
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From pburns at pburns.seanet.com  Sun Mar 14 12:15:35 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sun, 14 Mar 2004 11:15:35 +0000
Subject: [R] recursive matrix multiplication question
References: <BAY2-F133Wk0jVKyNzq00038d54@hotmail.com>
Message-ID: <40543ED7.4010107@pburns.seanet.com>

It would be much easier if you started out with a 3-dimensional
array. A large part of what makes R a good environment is that
it gives you the flexibility to have data structures that match how
you think about the data.

h0arr <- h0m
dim(h0arr) <- c(2, 2, ncol(h0m))

Then you can write a function to do the entire task that you want:

tseqeigen <- function(x, times=1:(nm-1)) {
dx <- dim(x)
if(length(dx) != 3) stop("x must be 3-dimensional array")
if(dx[1] != dx[2]) stop("first two dimensions of x must be equal")
nm <- dx[3]
ans <- array(NA, c(length(times), dx[1]))
for(i in seq(along=times)) {
this.t <- times[i]
this.mm <- x[,, this.t] %*% x[,, this.t + 1]
ans[i,] <- eigen(this.mm)$values
}
ans
}

This could then be used like:

tseqeigen(h0arr)

or

tseqeigen(h0arr, 1:4)

Enhancements to the function might include giving it a better
name, and dealing with dimnames.

There is some material in S Poetry on dealing with higher
dimensional arrays.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ivan Kautter wrote:

> List serv subscribers,
>
> I am wondering if there is an efficient means of doing recursive 
> matrix multiplication in R. My data resides in a 4 X 2541 matrix from 
> which I need to extract 2541 2X2 matrices based on each row. If I 
> attempt something like this:
>
> function(AO)
> {A<-AO
> {for(t in 1:4)
> A[t+1]<-matrix(h0m[t,1:4],nrow=2,ncol=2,byrow=FALSE)%*%matrix(h0m[t+1,1:4],nrow=2,ncol=2,byrow=FALSE) 
>
> }
> }
>
> I get this type of error:
>
> 1: number of items to replace is not a multiple of replacement length
> 2: number of items to replace is not a multiple of replacement length
> 3: number of items to replace is not a multiple of replacement length
> 4: number of items to replace is not a multiple of replacement length
>
> I believe I understand the error here in that I am attempting to add 
> more elements to an object that is only a 2x2 matrix. So is there some 
> way to just update the value of the matrix recursively? If you are 
> asking yourself why I am bothering to do this if I mean to multiple 
> only 4 matrices, I actually intend to multiple up to all 2541 of them, 
> which seems no small task. Ultimately, I would like to define the time 
> period over which I am multiplyig these essentially time-dependent 
> matrices and obtain the eigenvalues of the final matrix product.
>
> If anyone can offer any assistance, I would greatly appreciate it. 
> Thanks.
>
> Ivan Kautter
>
> _________________________________________________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From albedo at pisem.net  Sun Mar 14 12:49:07 2004
From: albedo at pisem.net (Albedo)
Date: Sun, 14 Mar 2004 14:49:07 +0300 (MSK)
Subject: [R] nnet classification accuracy vs. other models
Message-ID: <200403141149.i2EBn7du099127@www5.hotbox.ru>

The only thing that I could have done wrong with nnet
(that I 
can think of) is not enough nuerons in hidden layer,
but then
again this is actually limited by my computer memory.

However, I did estimate the error a little bit
different - I have
enough data for test set, which I used for classification
accuracy estimation only.

 Edgar Acuna <edgar at cs.uprm.edu>:

> I think that you are using nnet incorrectly. I have
compared several
> classifiers (including that ones that you mention in
your e-mail) on the
> same dataset and I have never found more of a 20% of
difference in the
> missclassification error. Of course, I estimated the
misclassification
> error by cross validation.
>
> Regards
> Edgar Acuna
> UPR-MATH
>
> On Sat, 13 Mar 2004, Albedo wrote:
>
> > I was wandering if anybody ever tried to compare
the classification
> > accuracy of nnet to other (rpart, tree, bagging)
models. From what I
> > know, there is no reason to expect a significant
difference in
> > classification accuracy between these models, yet
in my particular case
> > I get about 10% error rate for tree, rpart and
bagging model and 80%
> > error rate for nnet, applied to the same data.
> >
> > Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>



From ripley at stats.ox.ac.uk  Sun Mar 14 13:44:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Mar 2004 12:44:57 +0000 (GMT)
Subject: [R] nnet classification accuracy vs. other models
In-Reply-To: <200403141149.i2EBn7du099127@www5.hotbox.ru>
Message-ID: <Pine.LNX.4.44.0403141237200.30510-100000@gannet.stats>

On Sun, 14 Mar 2004, Albedo wrote:

> The only thing that I could have done wrong with nnet
> (that I 
> can think of) is not enough nuerons in hidden layer,
> but then
> again this is actually limited by my computer memory.

Perhaps you had too many, not too few?  Perhaps you didn't choose the 
weight decay correctly?  Perhaps you made an error in the code you used?
Who knows?  You haven't told us anything useful about what you did ....

> However, I did estimate the error a little bit
> different - I have
> enough data for test set, which I used for classification
> accuracy estimation only.

So you may have the tree results wrong, instead?

I suggest that you seek expert statistical help from a paid consultant:  
these are not R questions and you seem to need a face-to-face dialogue
even to help you ask answerable questions.  Your basic hypothesis is
wrong:  there is good reason to expect nnet to outperform tree models
(even after bagging the latter, and `bagging' is not a model), *in the
hands of a competent user*.


>  Edgar Acuna <edgar at cs.uprm.edu>:
> 
> > I think that you are using nnet incorrectly. I have
> compared several
> > classifiers (including that ones that you mention in
> your e-mail) on the
> > same dataset and I have never found more of a 20% of
> difference in the
> > missclassification error. Of course, I estimated the
> misclassification
> > error by cross validation.
> >
> > Regards
> > Edgar Acuna
> > UPR-MATH
> >
> > On Sat, 13 Mar 2004, Albedo wrote:
> >
> > > I was wandering if anybody ever tried to compare
> the classification
> > > accuracy of nnet to other (rpart, tree, bagging)
> models. From what I
> > > know, there is no reason to expect a significant
> difference in
> > > classification accuracy between these models, yet
> in my particular case
> > > I get about 10% error rate for tree, rpart and
> bagging model and 80%
> > > error rate for nnet, applied to the same data.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Regestrierung at domain.at  Sun Mar 14 18:31:10 2004
From: Regestrierung at domain.at (Regestrierung@domain.at)
Date: Sun, 14 Mar 2004 18:31:10 +0100
Subject: [R] Regestrierung 
Message-ID: <200403141731.i2EHVAi26548@almacen.cet.at>

Herzlichen Gl?ckwunsch !

Ihre Regestrierung auf www.ianta-ki.de war erfolgreich .

Ben?tzername = Sabine
Passwort = 28142678

Sie k?nnen sich ab sofort mit den daten unter www.ianta-ki.de einlggen !

Ihr Webmaster at domaine.de



From Jan.Verbesselt at agr.kuleuven.ac.be  Sun Mar 14 18:04:02 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Sun, 14 Mar 2004 18:04:02 +0100
Subject: [R] High/low level: Plot 2 time series with different axis (left
	and right)
Message-ID: <1079283842.405490821c6ea@webmail1.kuleuven.be>

Dear R specialists,

I have two time series in a data.frame and want to plot them in the same
plot(), with the left axis scaled to time series 1 (-700,0) and the
right axis scaled to time series 2 (-0.2, 0.4). 

plot(timeserie1)
lines(timeserie2, col=c(2)) => this one should be scaled differently
with a new axis on the right handside.

How can these be visualised such that the fit is optimal for
visualisation of the two time series? Which commands can I use?

Thanks,
Jan



From ripley at stats.ox.ac.uk  Sun Mar 14 18:28:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Mar 2004 17:28:55 +0000 (GMT)
Subject: [R] High/low level: Plot 2 time series with different axis (left
	and right)
In-Reply-To: <1079283842.405490821c6ea@webmail1.kuleuven.be>
Message-ID: <Pine.LNX.4.44.0403141725530.1546-100000@gannet.stats>

On Sun, 14 Mar 2004, Jan Verbesselt wrote:

> Dear R specialists,
> 
> I have two time series in a data.frame and want to plot them in the same
> plot(), with the left axis scaled to time series 1 (-700,0) and the
> right axis scaled to time series 2 (-0.2, 0.4). 
> 
> plot(timeserie1)
> lines(timeserie2, col=c(2)) => this one should be scaled differently

Why c(2)?

> with a new axis on the right handside.
> 
> How can these be visualised such that the fit is optimal for
> visualisation of the two time series? Which commands can I use?

plot(timeserie1)
par(new=T)
plot(timeserie2, yaxt="n", type="l", col=2, ylab="")
axis(4)

provided the two series have the same timebase.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ian at ianlong.co.uk  Sun Mar 14 21:02:07 2004
From: ian at ianlong.co.uk (Ian Long)
Date: Sun, 14 Mar 2004 20:02:07 +0000
Subject: [R] Troubles installing RMySQL on Win2K
Message-ID: <5.2.0.9.0.20040314195603.02407008@pop.freeserve.net>

What do I need to do to load the RMySQL library on Win 2K using version 1.8.1

I have Mysql v4.0.16 up and running on the local machine with several 
databases running fine.

Using RGUI I have loaded RMySQL and an appropriate directory structure has 
appeared under
C:\Program Files\R\rw1081\library\RMySQL

I have added C:\Program Files\R\rw1081\library\RMySQL\inst\libs to my path 
variable so that libmySQL.dll can be found.

If I type library(), then I see RMySQL in the list of available packages.

If I try and load the library, I get
 > library(RMySQL)
Error in testRversion(descfields) : This package has not been installed 
properly
  See the Note in ?library

Where am I going wrong?

Thanks,
Ian.



From ripley at stats.ox.ac.uk  Sun Mar 14 23:08:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Mar 2004 22:08:16 +0000 (GMT)
Subject: [R] Troubles installing RMySQL on Win2K
In-Reply-To: <5.2.0.9.0.20040314195603.02407008@pop.freeserve.net>
Message-ID: <Pine.LNX.4.44.0403142204050.1707-100000@gannet.stats>

Where did you get RMySQL from and how did you `load' it?

Most likely you unpacked a source distribution instead of installing.
There is a binary version of RMySQL available at

http://stat.bell-labs.com/RS-DBI/download

If you didn't use that, please try it.

On Sun, 14 Mar 2004, Ian Long wrote:

> What do I need to do to load the RMySQL library on Win 2K using version 1.8.1
> 
> I have Mysql v4.0.16 up and running on the local machine with several 
> databases running fine.
> 
> Using RGUI I have loaded RMySQL and an appropriate directory structure has 
> appeared under
> C:\Program Files\R\rw1081\library\RMySQL
> 
> I have added C:\Program Files\R\rw1081\library\RMySQL\inst\libs to my path 
> variable so that libmySQL.dll can be found.
> 
> If I type library(), then I see RMySQL in the list of available packages.
> 
> If I try and load the library, I get
>  > library(RMySQL)
> Error in testRversion(descfields) : This package has not been installed 
> properly
>   See the Note in ?library
> 
> Where am I going wrong?
> 
> Thanks,
> Ian.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mcclatchie.sam at saugov.sa.gov.au  Mon Mar 15 04:47:39 2004
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Mon, 15 Mar 2004 14:17:39 +1030
Subject: [R] fisheries acoustics
Message-ID: <032A8573186A2B4EBBAEFA5784D0523555A064@sagemsg0007.sagemsmrd01.sa.gov.au>

Hello Bruno

Your query is very broad, but it sounds as if you have not heard of a Windoz
based application package called Echoview
<http://www.sonardata.com/sonardata/default.shtm>. Of course there are about
half a dozen to a dozen or more analysis packages out there that have been
developed by acoustics groups (from the IMR Bergen Integrator to NIWA's
ESP2), but Echoview is becoming something of a standard.

R is not the best tool to deal with the very large files that consitute raw
acoustic data, and there isn't much point when there are good packages out
there already.

Sam
----
Sam McClatchie,
Sub-program leader, Pelagic Fisheries
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Telephone: (61-8) 8200 2448
FAX: (61-8) 8200 2481
Research home page <http://www.smcc.150m.com/>
  
                   /\
      ...>><xX(?> 
                //// \\\\
                   <?)Xx><<
              /////  \\\\\\
                        ><(((?> 
  >><(((?>   ...>><xX(?>O<?)Xx><<



-------------------Message: 25
Date: Wed, 10 Mar 2004 13:38:16 +0000
From: Marcelo Bruno <ocbruno at netscape.net>
Subject: [R] Fisheries acoustics
To: R-help at stat.math.ethz.ch
Message-ID: <404F1A48.5080103 at netscape.net>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi, member-list

i'm searching for packages or routines to analise fisheries acoustics data.
Something make  import data of scientific ecosound (EK500) of paralel 
port, filter signal, analyse pattern of pixel fish shoals...

Is there  someone work with this ? If no, why dont make colective effort 
to do this?? (package fish_acoustic).

Thanks
Marcelo



From hungy at stat.ncu.edu.tw  Mon Mar 15 12:57:51 2004
From: hungy at stat.ncu.edu.tw (hungy@stat.ncu.edu.tw)
Date: Mon, 15 Mar 2004 12:57:51 +0800(CST)
Subject: [R] 2D or 3D B-spline
Message-ID: <20040315045751.8935168A01@stat.ncu.edu.tw>

Hi,
could someone tell me which package in R can do 2D or 3D B-spline ?
Thanks a lot.

Ying-Chao Hung

From mvdv at spamcop.net  Mon Mar 15 06:41:01 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Mon, 15 Mar 2004 16:41:01 +1100
Subject: [R] Correct Computer Modern font in postscript(..) output
Message-ID: <000001c40a50$1a58b8a0$344610ac@FEB0480>

Hi,
I'm trying to get the correct font used when generating italic text in an R
grahic.  I have a set of labels that print correctly except it seems the
italic text is justr a slanted version of the TeX computer modern normal
font...  I'm using R v1.8.1 on Windows XP, and I get the same result if I
build the pdf using Adobe Acrobat or using MikTeX

The labels:

grpNames<-c(expression(paste("lo ", italic(d[n]))),
expression(italic(d[n])), expression(paste("hi ", italic(d[n]))),
            expression(paste("lo ", italic(LM[1]))),
expression(italic(LM[1])), expression(paste("hi ", italic(LM[1]))),
            expression(italic(d[n])), expression(italic(LM[1])),
expression(italic(LM[2])))

The postscript command:

postscript(file = "Rplots.ps",
           onefile = F, height=6,width=10,
           paper="a4", family="ComputerModern",bg="transparent",
           horizontal=F, pointsize=10,
           print.it=F)

Is this the best currently available, or is there some way to get the
correct font used when using italics?

TIA
Mark



From petr.pikal at precheza.cz  Mon Mar 15 08:06:47 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 15 Mar 2004 08:06:47 +0100
Subject: [R] High/low level: Plot 2 time series with different axis
	(left	and right)
In-Reply-To: <1079283842.405490821c6ea@webmail1.kuleuven.be>
Message-ID: <40556417.18005.1729EE@localhost>


   Hi

   On 14 Mar 2004 at 18:04, Jan Verbesselt wrote:

   > Dear R specialists,

   >

   > I have two time series in a data.frame and want to plot them in the

   > same plot(), with the left axis scaled to time series 1 (-700,0) and

   > the right axis scaled to time series 2 (-0.2, 0.4).

   >

   > plot(timeserie1)

   > lines(timeserie2, col=c(2)) => this one should be scaled differently

   > with a new axis on the right handside.

   I am not really a R specialist but for this task I use function:

   plot.yy<-function(x,yright,yleft, yleftlim=NULL, yrightlim = NULL,
   xlab = NULL ,yylab=c("",""),pch=c(1,2),col=c(1,2), linky=F, smooth=0,
   lwds=1, length=10, format="%d-%H:%M", ...)

   {

   par(mar=c(5,4,4,2),oma=c(0,0,0,3))

   plot(x, yright, ylim=yrightlim, axes=F,ylab="", xlab=xlab,
   pch=pch[1],col=col[1], ...)

   axis(4,pretty(range(yright,na.rm=T),10),col=col[1])

   if (linky) lines(x,yright,col=col[1], ...)

   if (smooth!=0) lines(supsmu(x,yright,span=smooth),col=col[1],
   lwd=lwds, ...)

   if(yylab[1]=="")

   mtext(deparse(substitute(yright)),side=4,outer=T,line=1, col=col[1],
   ...)

   else

   mtext(yylab[1],side=4,outer=T,line=1, col=col[1], ...)

   par(new=T)

   plot(x,yleft, ylim=yleftlim, ylab="", axes=F ,xlab=xlab,
   pch=pch[2],col=col[2], ...)

   box()

   axis(2,pretty(range(yleft,na.rm=T),10),col=col[2], col.axis=col[2])

   if (is.null(class(x))) axis(1,pretty(range(x,na.rm=T),10)) else

   {

   l<-length(x)

   axis(1,at=x[seq(1,l,length=length)],labels=format(as.POSIXct(x[seq(1,l
   ,length=length)]),format=format))

   }

   if(yylab[2]=="")

   mtext(deparse(substitute(yleft)),side=2,line=2, col=col[2], ...)

   else

   mtext(yylab[2],side=2,line=2, col=col[2], ...)

   if (linky) lines(x,yleft,col=col[2], lty=2, ...)

   if (smooth!=0) lines(supsmu(x,yleft,span=smooth),col=col[2], lty=2,
   lwd=lwds, ...)

   }

   ### End of a function

   It gives you a limited possibility to do some smoothing, add lines
   change colors and points and add some axes annotation and it handels x
   in POSIX class }just a little bit :).

   Cheers

   Petr

   >

   > How can these be visualised such that the fit is optimal for

   > visualisation of the two time series? Which commands can I use?

   >

   > Thanks,

   > Jan

   >

   > ______________________________________________

   > R-help at stat.math.ethz.ch mailing list

   > https://www.stat.math.ethz.ch/mailman/listinfo/r-help

   > PLEASE do read the posting guide!

   > http://www.R-project.org/posting-guide.html

   Petr Pikal

   petr.pikal at precheza.cz


From ian at ianlong.co.uk  Mon Mar 15 08:31:27 2004
From: ian at ianlong.co.uk (Ian Long)
Date: Mon, 15 Mar 2004 07:31:27 +0000
Subject: [R] Troubles installing RMySQL on Win2K
In-Reply-To: <Pine.LNX.4.44.0403142204050.1707-100000@gannet.stats>
References: <5.2.0.9.0.20040314195603.02407008@pop.freeserve.net>
Message-ID: <5.2.0.9.0.20040315072842.02440e18@pop.freeserve.net>

That works perfectly, thank you.

The question of how did I "load it" is a hard one to answer - over the past 
two weeks I have had a variety of different attempts. I believe as you 
suggest, I had a source distribution and was struggling to find out how to 
compile it.

Many thanks for your efforts,
Ian.

At 22:08 14/03/2004 +0000, Prof Brian Ripley wrote:
>Where did you get RMySQL from and how did you `load' it?
>
>Most likely you unpacked a source distribution instead of installing.
>There is a binary version of RMySQL available at
>
>http://stat.bell-labs.com/RS-DBI/download
>
>If you didn't use that, please try it.
>
>On Sun, 14 Mar 2004, Ian Long wrote:
>
> > What do I need to do to load the RMySQL library on Win 2K using version 
> 1.8.1
> >
> > I have Mysql v4.0.16 up and running on the local machine with several
> > databases running fine.
> >
> > Using RGUI I have loaded RMySQL and an appropriate directory structure has
> > appeared under
> > C:\Program Files\R\rw1081\library\RMySQL
> >
> > I have added C:\Program Files\R\rw1081\library\RMySQL\inst\libs to my path
> > variable so that libmySQL.dll can be found.
> >
> > If I type library(), then I see RMySQL in the list of available packages.
> >
> > If I try and load the library, I get
> >  > library(RMySQL)
> > Error in testRversion(descfields) : This package has not been installed
> > properly
> >   See the Note in ?library
> >
> > Where am I going wrong?
> >
> > Thanks,
> > Ian.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mazevedo at ipimar.pt  Fri Mar 12 15:14:52 2004
From: mazevedo at ipimar.pt (Manica)
Date: Fri, 12 Mar 2004 14:14:52 +0000
Subject: [R] help
Message-ID: <4051C5DC.85C14B64@ipimar.pt>

I want to extract the first non-zero digit of any vector, as for example
from a<-runif(100,0,1).
Tried it using grep (...) but didn?t work.

Any help is very much appreciated.
Thanks,
Manica


--
visite
http://ipimar-iniap.ipimar.pt/neomav/



From Mark.Bravington at csiro.au  Mon Mar 15 02:09:03 2004
From: Mark.Bravington at csiro.au (Mark.Bravington@csiro.au)
Date: Mon, 15 Mar 2004 12:09:03 +1100
Subject: [R] [R-pkgs] New versions: mvbutils and debug packages
Message-ID: <C4178DC99E08604EA5E2BDB989F09380090C8D@EXTAS2-HBA.tas.csiro.au>

Dear R users

New versions of the 'mvbutils' and 'debug' packages are now available on CRAN, both in source form and as precompiled binaries.

'mvbutils' offers the following (as well as many miscellaneous utilities):

  ?  hiearchical, searchable project organization, with workspaces switchable inside a single R session, and objects in "ancestor" projects always visible in R from "child" projects;
  ?  function editing (interface to text editors), with multiple simultaneous edits, an "unfrozen" R prompt, and automatic backup;
  ?  function code and plain-text documentation stored in the same R object, and editable in the same file;
  ?  informal plain-text documentation via help, and conversion to Rd format;
  ?  nested 'source'ing, and interspersal of R code and data in the same file;
  ?  macro-like functions, executing inside their caller's environment;
  ?  untangling and display of "what calls what" within groups of functions.

'debug' is for debugging functions (yours or other people's). It requires 'mvbutils'. 'debug' offers:

  ?  a visible code window with line-numbered code and highlighted execution point;
  ?  the ability to set (conditional) breakpoints in advance, at any line number;
  ?  the opportunity to keep going after errors;
  ?  multiple debugging windows open at once (when one debuggee calls another, or itself);
  ?  full debugging of 'on.exit' code;
  ?  the ability to move the execution point around without executing intervening statements;
  ?  direct interpretation of typed-in statements, as if they were in the function itself.

For further information on 'debug', see the article "Debugging without (too many) tears" in R-news 3/3.

The new versions fix a few minor bugs in the 1.0.0 releases, as documented in the CHANGES.TXT files in each package.

Mark

*******************************

Mark Bravington
CSIRO (CMIS)
PO Box 1538
Castray Esplanade
Hobart
TAS 7001

Email name: Mark.Bravington
Email location: csiro.au

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Mon Mar 15 09:11:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Mar 2004 08:11:33 +0000 (GMT)
Subject: [R] Correct Computer Modern font in postscript(..) output
In-Reply-To: <000001c40a50$1a58b8a0$344610ac@FEB0480>
Message-ID: <Pine.LNX.4.44.0403150801530.20184-100000@gannet.stats>

family="ComputerModern" gives you CMSL10, which is the `correct font'.
>From src/main/devPS.c:

    /* Computer Modern as recoded by Brian D'Urso */
    { "ComputerModern",
      {"CM_regular_10.afm", "CM_boldx_10.afm", "CM_italic_10.afm",
       "CM_boldx_italic_10.afm", "CM_symbol_10.afm"}
    },

and CM_italic_10.afm specifies CMSL10: there is no italic font in standard 
Computer Modern, and the `correct font' is the one the designer intended.

What did you think the `correct font' was?

Please do read the posting guide and its references, and try to write more 
informative (and less accusatory) messages.


On Mon, 15 Mar 2004, Mark Van De Vyver wrote:

> Hi,
> I'm trying to get the correct font used when generating italic text in an R
> grahic.  I have a set of labels that print correctly except it seems the
> italic text is justr a slanted version of the TeX computer modern normal
> font...  I'm using R v1.8.1 on Windows XP, and I get the same result if I
> build the pdf using Adobe Acrobat or using MikTeX
> 
> The labels:
> 
> grpNames<-c(expression(paste("lo ", italic(d[n]))),
> expression(italic(d[n])), expression(paste("hi ", italic(d[n]))),
>             expression(paste("lo ", italic(LM[1]))),
> expression(italic(LM[1])), expression(paste("hi ", italic(LM[1]))),
>             expression(italic(d[n])), expression(italic(LM[1])),
> expression(italic(LM[2])))
> 
> The postscript command:
> 
> postscript(file = "Rplots.ps",
>            onefile = F, height=6,width=10,
>            paper="a4", family="ComputerModern",bg="transparent",
>            horizontal=F, pointsize=10,
>            print.it=F)
> 
> Is this the best currently available, or is there some way to get the
> correct font used when using italics?

Well, that is `building' a postscript file, not PDF: if you want PDF you 
should be using pdf().


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Friedrich.Leisch at ci.tuwien.ac.at  Mon Mar 15 10:11:09 2004
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Mon, 15 Mar 2004 10:11:09 +0100
Subject: [R] Sweave and R output: possible to suppress "Schunk" tags in
	*.tex file output?
In-Reply-To: <62AE0CF1D4875C4BBDEC29DB9924ACE87F21AE@pnlmse25.pnl.gov>
References: <62AE0CF1D4875C4BBDEC29DB9924ACE87F21AE@pnlmse25.pnl.gov>
Message-ID: <16469.29485.718151.188553@galadriel.ci.tuwien.ac.at>

>>>>> On Fri, 12 Mar 2004 08:38:53 -0800,
>>>>> Waichler, Scott R (WSR) wrote:

  > Hello,
  > I would like to fill the rows of a Latex tabular environment with output from
  > R, as in

  > \begin{table}
  >   \caption{Table caption.} 
  >   \label{tab:events}
  >   \begin{tabular}{c r r r r r}
  >   \hline

  > <<echo=false,results=tex>>=
  > fill.my.table.rows()
  > @

  >   \end{tabular}
  > \end{table}

  > Sweave produces the output inside \begin{Schunk} and \end{Schunk} commands,
  > which latex doesn't tolerate within a table.  I would prefer to avoid writing all of
  > the Latex table code from R.  Is my only other option to produce a separate
  > text file, e.g. under Sweave with results=hide, then use \input{} on the file?

results=tex will do what you want, i.e., produce no Soutput and Schunk
environments. you may also want to look at package xtable.

best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From david.meyer at wu-wien.ac.at  Mon Mar 15 11:12:37 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 15 Mar 2004 11:12:37 +0100
Subject: [R] Re: R-help Digest, Vol 13, Issue 14
In-Reply-To: <200403141219.i2EB7wgf008033@hypatia.math.ethz.ch>
References: <200403141219.i2EB7wgf008033@hypatia.math.ethz.ch>
Message-ID: <20040315111237.7d51257d.david.meyer@wu-wien.ac.at>

You could look at

@Article{         e1071-papers:meyer+leisch+hornik:2003,
  author        = {David Meyer and Friedrich Leisch and Kurt Hornik},
  title         = {The Support Vector Machine under Test},
  journal       = {Neurocomputing},
  year          = 2003,
  month         = {September},
  pages         = {169--186},
  volume        = 55
}

which compares a lot of classifiction and regression methods available
in R. The purpose obviously was to assess SVMs, but the error rates can
be compared independently from that. Generally, the performance of
nnet() was acceptable, but ensemble methods have been quite competitive
as well.

Best,
David

---

I was wandering if anybody ever tried to compare the classification
accuracy of nnet to other (rpart, tree, bagging) models. From what I
know, there is no reason to expect a significant difference in 
classification accuracy between these models, yet in my particular case
I get about 10% error rate for tree, rpart and bagging model and 80% 
error rate for nnet, applied to the same data.

Thanks.



From fm3a004 at math.uni-hamburg.de  Mon Mar 15 10:19:10 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 15 Mar 2004 10:19:10 +0100 (MET)
Subject: [R] nnet classification accuracy vs. other models
In-Reply-To: <405377C1.50305@pisem.net>
Message-ID: <Pine.GSO.3.95q.1040315101109.7135A-100000@sun11.math.uni-hamburg.de>

My experience is that nnet needs a lot of tuning, not only in terms of
numbers of layers, but also in terms of the other parameters. My first
results where I kept very much of the default parameter values with nnet
have been very bad, as bad as you say. (But as Brian Ripley already wrote,
it's not straight forward to say via the net how to do it better.)

Apart from that, such a large difference of classification accuracy
between different methods is strange, but possible in principle. 
Very different structures of data exist (which means again that nobody can
assess your problem without knowing the data).

Christian

On Sat, 13 Mar 2004, Albedo wrote:

> I was wandering if anybody ever tried to compare the classification
> accuracy of nnet to other (rpart, tree, bagging) models. From what I
> know, there is no reason to expect a significant difference in 
> classification accuracy between these models, yet in my particular case
> I get about 10% error rate for tree, rpart and bagging model and 80% 
> error rate for nnet, applied to the same data.
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From rksh at soc.soton.ac.uk  Mon Mar 15 10:53:49 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Mon, 15 Mar 2004 09:53:49 +0000
Subject: [R] optim() and La.svd()  error code 1
Message-ID: <a0600200fbc7b250c1d13@[139.166.242.29]>

Hello everybody.

I am using optim() to minimize a function of 19 variables and I 
repeatably get the following error
message:


R> source("/users/sat/rksh/goldstein/emulator/optimizer.R")
sann objective function values
initial       value 5044.955275
Error in La.svd(x, nu, nv, method) : error code 1 from Lapack routine dgesdd
R>

This error occurs whether I use SANN or Nelder-Mead.  My objective 
function doesn't seem
to do anything odd near my starting value.

Does anyone have any experience of this error message?

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From jcadima at isa.utl.pt  Thu Mar 11 13:11:04 2004
From: jcadima at isa.utl.pt (Jorge Cadima)
Date: Thu, 11 Mar 2004 12:11:04 +0000
Subject: [R] [R-pkgs] Subselect package - Version 0.7.1
Message-ID: <E1B1P1l-0003PO-00@DMjcadima>


A new  version (0.7.1) of package 'subselect' has been uploaded to CRAN.

Package 'subselect' provides functions which assess the quality of
variable subsets as surrogates for a full data set, in an exploratory
data analysis, and search for subsets which are optimal under various
criteria. 

As of version 0.7 a new function 'leaps' has been added. 'Leaps'
performs a branch and bound search for the best variable subsets,
according to a specified criterion. 'Leaps' implements Duarte Silva's
adaptation (Reference 3) of Furnival and Wilson's Leaps and Bounds
Algorithm for variable selection in Regression Analysis. It is viable in
identifying optimal subsets for data sets with a moderate number
(up to about 30-35) of variables, and very fast for small data sets
(up to about 20-25 variables).

In package subselect, the quality of given k-subsets of variables are
assessed under three criteria (Reference 2). 

Three additional functions, 'anneal', 'genetic' and 'improve', search for
optimal k-variable subsets under those criteria, using three different
algorithms: a simulated annealing algorithm, a genetic algorithm and a
restricted local improvement algorithm (Reference 1). Among the
options, the user can control number of iterations, initial
temperature, cooling factors and cooling frequency in simulated
annealing, and number of generations, population size, admissibility
of clones and presence and frequency of mutations in the 
genetic algorithm. 

For all algorithms, it is possible to specify the number
of solutions required in one or more cardinalities and to force the solutions 
to include and/or to exclude given subsets of variables.


Here is the DESCRIPTION file for the package:

Package: subselect
Version: 0.7.1
Date: 2004/03/10
Title: Selecting variable subsets.
Author: Jorge Orestes Cerdeira <orestes at isa.utl.pt> Pedro Duarte Silva
        <psilva at porto.ucp.pt> Jorge Cadima <jcadima at isa.utl.pt> Manuel
        Minhoto <minhoto at uevora.pt>
Maintainer: Jorge Cadima <jcadima at isa.utl.pt>
Description: A collection of functions which assess the quality of
        variable subsets as surrogates for a full data set, in an
        exploratory data analysis, and search for subsets which are
        optimal under various criteria.
License: GPL


There is a CHANGELOG file in subdirectory 'inst' documenting changes
since Version 0.1.



BIBLIOGRAPHY:

1) Cadima, J., Cerdeira, J. Orestes and Minhoto, M. (2004)
Computational aspects of algorithms for variable selection in the
context of principal components. To appear in 
_Computational Statistics & Data Analysis_ (Special Issue on
Applications of Optimization Heuristics to Estimation and Modelling Problems).

2) Cadima, J. and Jolliffe, I.T. (2001). Variable Selection and
the Interpretation of Principal Subspaces, _Journal of
Agricultural, Biological and Environmental Statistics_, Vol. 6, 62-79.

3) Duarte Silva, A.P. (2002) Discarding Variables in a Principal 
Component Analysis: Algorithms for All-Subsets Comparisons,
_Computational Statistics_, Vol. 17, 251-271.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From simon at stats.gla.ac.uk  Mon Mar 15 11:18:58 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 15 Mar 2004 10:18:58 +0000 (GMT)
Subject: [R] help
In-Reply-To: <4051C5DC.85C14B64@ipimar.pt>
References: <4051C5DC.85C14B64@ipimar.pt>
Message-ID: <Pine.SOL.4.58.0403151018120.23941@moon.stats.gla.ac.uk>

> I want to extract the first non-zero digit of any vector, as for example
> from a<-runif(100,0,1).

a[min((1:length(a))[a!=0])]

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From Ted.Harding at nessie.mcc.ac.uk  Mon Mar 15 11:40:44 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 15 Mar 2004 10:40:44 -0000 (GMT)
Subject: [R] help
In-Reply-To: <4051C5DC.85C14B64@ipimar.pt>
Message-ID: <XFMail.040315104044.Ted.Harding@nessie.mcc.ac.uk>

On 12-Mar-04 Manica wrote:
> I want to extract the first non-zero digit of any vector, as for
> example
> from a<-runif(100,0,1).
> Tried it using grep (...) but didn?t work.
> 
> Any help is very much appreciated.
> Thanks,
> Manica

runif will put "real" numbers in 'a'. It is highly likely that
any of them will be zero, and in any case they are not "digits"
(though in their decimal representation many of them will have
at least one "0" in the string of digits for each one).

So it is not clear what you mean by "the first non-zero digit
of any vector". Please clarify!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Mar-04                                       Time: 10:40:44
------------------------------ XFMail ------------------------------



From petr.pikal at precheza.cz  Mon Mar 15 11:57:55 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 15 Mar 2004 11:57:55 +0100
Subject: [R] expanding some values in logical vector
Message-ID: <40559A43.8581.EB03E1@localhost>

Dear all

In automatic dropout evaluation function I construct an index (pointer), which 
will be TRUE at "unusual" values. Then I need to expand these TRUE values a 
little bit forward and backward.

Example:

having span=5, from vector

idx<-rep(F,10)
idx[4]<-T

> idx
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
FALSE

I need

> idx
 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE 
FALSE


I was using embed() for this task (myfun1):

idx <- rep(F,20)
idx[c(4,11,13)] <- T

> idx
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE 
FALSE

iii <- myfun1(idx)

> iii
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  
TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE 
FALSE


It is close to what I want, but it is slow and when idx vector is long enough it 
goes short of memory. Then I tried to accomplish it with rle() and some fiddling 
with $values and $lengths (myfun2), which works as long as the true values are 
completely separated.

idx <- rep(F,20)
idx[c(4,12)] <- T

> idx
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 
FALSE

iii <- myfun2(idx)
> iii
 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  
TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE 
FALSE
	
But when using previous idx

idx <- rep(F,20)
idx[c(4,11,13)] <- T

iii <- myfun2(idx)
Error in rep.default(kody$values, opak) : invalid number of copies in "rep"

Problem is, if TRUE values are closer than span allows. Then some values in 
"opak" for FALSE kody$values are negative, what is not allowed. Setting them 
to zero will expand the length of index vector. Instead number of repetitions of 
neighbour TRUE values should be decreased accordingly. 

I greatly appreciated, if somebody could give me a hint if there is some another 
built in function which can help me or what to do with myfun2 or how to get 
properly expanded index values by some another way.

Sorry for the long post but I was not able to explain my problem and what I have 
done yet to solve it in shorter.

Thank you and best regards.

Petr Pikal




##### Here are functions used #####



myfun1 <- function(idx,span=5)

{
n <- length(idx)
s <- span%/%2
z <- embed(idx,span)
sumy <- rowSums(z)>0
index <- c(rep(sumy[1],s),sumy,rep(sumy[n-span+1],s))
}



myfun2 <- function(idx,span=5)

{
n <- length(idx)

kody <- rle(idx)
test <- letters[sum(cumsum(c(kody$values[1],idx[n])))+1] ####is some of true 
values at the end of vector idx?
opak <- kody$values*(span-1)*2-(span-1)+kody$lengths #### enlarge number of 
TRUE repetitions according to the span
delka <- length(opak)

#### some ifs to ensure the end points will have correct number of repetitions

opak[c(1,delka)] <- opak[c(1,delka)]+span%/%2
if (sum(kody$values)==0) opak <- n
if (opak[1]<0) {opak[2] <- opak[2]+opak[1]; opak[1] <- 0}
if (opak[delka]<0) {opak[delka-1] <- opak[delka-1]+opak[delka]; opak[delka] <- 
0}

switch(test,

a = opak<-opak,
b = opak[delka]<-opak[delka]-(span-1), 
c = opak[1]<-opak[1]-(span-1), 
d = opak[c(1,delka)]<-opak[c(1,delka)]-(span-1),

)

#### here should opak contain correct number of repetitions but does not

index<-rep(kody$values,opak)

}
Petr Pikal
petr.pikal at precheza.cz



From sundar.dorai-raj at pdf.com  Mon Mar 15 12:26:27 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 15 Mar 2004 05:26:27 -0600
Subject: [R] help
In-Reply-To: <4051C5DC.85C14B64@ipimar.pt>
References: <4051C5DC.85C14B64@ipimar.pt>
Message-ID: <405592E3.3080006@pdf.com>



Manica wrote:
> I want to extract the first non-zero digit of any vector, as for example
> from a<-runif(100,0,1).
> Tried it using grep (...) but didn?t work.
> 
> Any help is very much appreciated.
> Thanks,
> Manica
> 
> 

How about:

a <- runif(10, 0, 1)
b <- as.character(a)
nonzero <- regexpr("[1-9]", b)
as.numeric(substr(b, nonzero, nonzero))

-sundar

P.S. in the future, it would be helpful to see an example of the output 
you expect to avoid any confusion in the question.



From mike.campana at freesurf.ch  Mon Mar 15 12:37:02 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Mon, 15 Mar 2004 12:37:02 +0100
Subject: [R] creating a ps. file
Message-ID: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>

Dear all

I wrote a routine. At the end of each cycle of the loop I would like to 
save the result (plot) in a postcriptfile.
Of course if I just use dev.print in the following way: 
dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
I overwrite my results with the second cycle of the loop. I suppose 
there is a way to define the file name so that several plots are 
created(plot_1,plot_2...).

Could you give me an advice? Thanks a lot

Best Regards 
Mike

---
---



From bxc at steno.dk  Mon Mar 15 12:53:17 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 15 Mar 2004 12:53:17 +0100
Subject: [R] help
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E0D2@exdkba022.novo.dk>

I guess what you want is:

a <- abs(a)
floor( a / 10^floor( log10( a ) ) )

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------





> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Manica
> Sent: Friday, March 12, 2004 3:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] help
> Importance: High
> 
> 
> I want to extract the first non-zero digit of any vector, as 
> for example from a<-runif(100,0,1). Tried it using grep (...) 
> but didn?t work.
> 
> Any help is very much appreciated.
> Thanks,
> Manica
> 
> 
> --
> visite
> http://ipimar-iniap.ipimar.pt/neomav/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jonathan.williams at pharmacology.oxford.ac.uk  Mon Mar 15 13:48:13 2004
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Mon, 15 Mar 2004 12:48:13 -0000
Subject: [R] imputation of sub-threshold values
Message-ID: <NGBBKJEMOMLJFCOIEGCEAEOOJKAA.jonathan.williams@pharm.ox.ac.uk>

Is there a good way in R to impute values which exist, 
but are less than the detection level for an assay?
Thanks,

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From s-plus at wiwi.uni-bielefeld.de  Mon Mar 15 13:51:47 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Mon, 15 Mar 2004 13:51:47 +0100
Subject: [R] expanding some values in logical vector
References: <40559A43.8581.EB03E1@localhost>
Message-ID: <4055A6E3.6090709@wiwi.uni-bielefeld.de>

Try:

 > x<-rep(FALSE,20); x[c(4,10,15)]<-TRUE
 > x
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
[13] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
 > x[outer(which(x),-1:1,"+")]<-T
 > x
 [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE
[13] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE
 > x<-rep(FALSE,20); x[c(4,10,15)]<-TRUE
 > x[outer(which(x),-2:2,"+")]<-T
 > x
 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE
# now we definie
 > expand.true<-function(x,span=1){
  m<-floor(span/2)
  ind<-outer(which(x),(-m):m,"+")
  ind<-ind[ind>0 & ind <= length(x)]
  x[ind]<-TRUE
  x
}
 > x<-rep(FALSE,20); x[c(4,10,19)]<-TRUE
 > expand.true(x,span=5)
 [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
[13] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE



Peter Wolf


Petr Pikal wrote:

>Dear all
>
>In automatic dropout evaluation function I construct an index (pointer), which 
>will be TRUE at "unusual" values. Then I need to expand these TRUE values a 
>little bit forward and backward.
>
>Example:
>
>having span=5, from vector
>
>idx<-rep(F,10)
>idx[4]<-T
>
>  
>
>>idx
>>    
>>
> [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
>FALSE
>
>I need
>
>  
>
>>idx
>>    
>>
> [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE 
>FALSE
>
>
>I was using embed() for this task (myfun1):
>
>idx <- rep(F,20)
>idx[c(4,11,13)] <- T
>
>  
>
>>idx
>>    
>>
> [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
>FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE 
>FALSE
>
>iii <- myfun1(idx)
>
>  
>
>>iii
>>    
>>
> [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  
>TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE 
>FALSE
>
>
>It is close to what I want, but it is slow and when idx vector is long enough it 
>goes short of memory. Then I tried to accomplish it with rle() and some fiddling 
>with $values and $lengths (myfun2), which works as long as the true values are 
>completely separated.
>
>idx <- rep(F,20)
>idx[c(4,12)] <- T
>
>  
>
>>idx
>>    
>>
> [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
>FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 
>FALSE
>
>iii <- myfun2(idx)
>  
>
>>iii
>>    
>>
> [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  
>TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE 
>FALSE
>	
>But when using previous idx
>
>idx <- rep(F,20)
>idx[c(4,11,13)] <- T
>
>iii <- myfun2(idx)
>Error in rep.default(kody$values, opak) : invalid number of copies in "rep"
>
>Problem is, if TRUE values are closer than span allows. Then some values in 
>"opak" for FALSE kody$values are negative, what is not allowed. Setting them 
>to zero will expand the length of index vector. Instead number of repetitions of 
>neighbour TRUE values should be decreased accordingly. 
>
>I greatly appreciated, if somebody could give me a hint if there is some another 
>built in function which can help me or what to do with myfun2 or how to get 
>properly expanded index values by some another way.
>
>Sorry for the long post but I was not able to explain my problem and what I have 
>done yet to solve it in shorter.
>
>Thank you and best regards.
>
>Petr Pikal
>
>
>
>
>##### Here are functions used #####
>
>
>
>myfun1 <- function(idx,span=5)
>
>{
>n <- length(idx)
>s <- span%/%2
>z <- embed(idx,span)
>sumy <- rowSums(z)>0
>index <- c(rep(sumy[1],s),sumy,rep(sumy[n-span+1],s))
>}
>
>
>
>myfun2 <- function(idx,span=5)
>
>{
>n <- length(idx)
>
>kody <- rle(idx)
>test <- letters[sum(cumsum(c(kody$values[1],idx[n])))+1] ####is some of true 
>values at the end of vector idx?
>opak <- kody$values*(span-1)*2-(span-1)+kody$lengths #### enlarge number of 
>TRUE repetitions according to the span
>delka <- length(opak)
>
>#### some ifs to ensure the end points will have correct number of repetitions
>
>opak[c(1,delka)] <- opak[c(1,delka)]+span%/%2
>if (sum(kody$values)==0) opak <- n
>if (opak[1]<0) {opak[2] <- opak[2]+opak[1]; opak[1] <- 0}
>if (opak[delka]<0) {opak[delka-1] <- opak[delka-1]+opak[delka]; opak[delka] <- 
>0}
>
>switch(test,
>
>a = opak<-opak,
>b = opak[delka]<-opak[delka]-(span-1), 
>c = opak[1]<-opak[1]-(span-1), 
>d = opak[c(1,delka)]<-opak[c(1,delka)]-(span-1),
>
>)
>
>#### here should opak contain correct number of repetitions but does not
>
>index<-rep(kody$values,opak)
>
>}
>Petr Pikal
>petr.pikal at precheza.cz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Jan.Verbesselt at agr.kuleuven.ac.be  Mon Mar 15 14:13:24 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Mon, 15 Mar 2004 14:13:24 +0100
Subject: [R] Test Relation between Time Series? => Cross Correlation
	Analysis with ccf()
Message-ID: <002a01c40a8f$4c310aa0$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear R world,

When investigating two time series and applying a cross correlation
ccf() function, the results show that correlation is maximal (ccf=0.8)
at a lag of 0.1.

e.g. 
ccf(Inv.KBDIn,ts.medNDII, type=c("correlation"), na.action=na.omit,
main=c("Cross-Correlation of Inverse KBDI against NDII"), ylab="CCF")

1.  Does this mean that there is a positive shift of the first time
series Inv.KBDI against the second?

2.   A lag of 0.1 means that 0.1*Frequency=> amount of time steps. 
e.g.
Frequency=36, => 36 time steps in a year.  So a lag=0.1, multiplied with
36 give us: 3.6 time steps (decades in this case).

*Is this correct?  

*Has anyone other tips for interpretation of (partial)
cross/auto-correlation results? (shape, etc.) (pdf.) or examples of
scripts of how one can extract useful information out of a cross-
correlation plot. Etc. 

*What is the validity of using this as a tool to test the relationship
between to time series vs dependence of results? (max. correlation
coefficient, lag)

Thanks a lot,
Jan


_______________________________________________________________________
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From chrysopa at insecta.ufv.br  Mon Mar 15 13:27:52 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Mon, 15 Mar 2004 09:27:52 -0300
Subject: [R] Problems with SJava instalation
In-Reply-To: <200403140722.01121.chrysopa@insecta.ufv.br>
References: <200403051329.OAA22888@jacaranda.math.u-psud.fr>
	<200403140722.01121.chrysopa@insecta.ufv.br>
Message-ID: <200403150927.52022.chrysopa@insecta.ufv.br>

Em Dom 14 Mar 2004 07:26, Ronaldo Reis Jr. escreveu:
> Em Sex 05 Mar 2004 10:29, Jean Coursol escreveu:
>
> Hi,
>
> I try to execute ldconfig and the SJava dont work. I try to install with
> gcc-3.3 (was gcc-2.95 in my first instalation). Where is the problem?
>
> Thanks
> Ronaldo

Hi,

I finally got to cross that mistake that was having with SJava. 
I am installing SJava to use the package iplots. If I execute the command:   

------------------------
[ronaldo at zeus ronaldo]$ /opt/lib/R/site-library/SJava/scripts/RJava --example 
--gui-none
------------------------

He go to terminal     

------------------------
Type 'q()' to quit R.

[Previously saved workspace restored]

[omegahat->R] 
------------------------

If I give Enter in this terminal him of the the following mistake:

------------------------
[omegahat->R] 
Problems: unrecognized user-level object of mode 1
Error: No default Java type for S class ???

An unexpected exception has been detected in native code outside the VM.
Unexpected Signal : 11 occurred at PC=0xBFFFD330
Function=[Unknown.]
Library=(N/A)

NOTE: We are unable to locate the function name symbol for the error
      just occurred. Please refer to release documentation for possible
      reason and solutions.
--------------------------

If I try to call the iplots have the following message:    

--------------------------
> library(iplots)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 
"/opt/lib/R/site-library/SJava/libs/SJava.so":
  libRSNativeJava.so: cannot open shared object file: No such file or 
directory
Error in library(SJava) : .First.lib failed
> 
--------------------------

This is the mistake that gave.

What am making wrong? How to do the iplots to work?

Thanks
Ronaldo
-- 
Basic ? como var?ola, pega-se quando crian?a.
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From solares at unsl.edu.ar  Mon Mar 15 14:46:32 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Mon, 15 Mar 2004 10:46:32 -0300 (ART)
Subject: [R] printing graphics whithout save us
Message-ID: <46852.170.210.173.216.1079358392.squirrel@inter17.unsl.edu.ar>

hello, it wanted to know as printing a graphics on a screen, without to
have save in a file, already use dev.print and give me an error  in PS ,
unable to start device postscript and a warning printing via file =" " not
implemented.Thanks Ruben



From bxc at steno.dk  Mon Mar 15 14:57:59 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 15 Mar 2004 14:57:59 +0100
Subject: [R] creating a ps. file
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E0E4@exdkba022.novo.dk>

paste

is the function you need.

Bendix C.

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------





> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> mike.campana at freesurf.ch
> Sent: Monday, March 15, 2004 12:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] creating a ps. file
> 
> 
> Dear all
> 
> I wrote a routine. At the end of each cycle of the loop I 
> would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).
> 
> Could you give me an advice? Thanks a lot
> 
> Best Regards 
> Mike
> 
> ---
> ---
> 
> 
> ______________________________________________
> 
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From duchesnay at shfj.cea.fr  Mon Mar 15 14:58:22 2004
From: duchesnay at shfj.cea.fr (Edouard DUCHESNAY)
Date: Mon, 15 Mar 2004 14:58:22 +0100
Subject: [R] No traceback available when using try(...)
In-Reply-To: <Pine.LNX.4.44.0403120844560.11959-100000@itasca.stat.uiowa.edu>
References: <Pine.LNX.4.44.0403120844560.11959-100000@itasca.stat.uiowa.edu>
Message-ID: <200403151458.22147.duchesnay@shfj.cea.fr>

The Luke's proposition it works fine.

I have extended the code proposed by Luke, to implemtent a "myTry()" and a "mytraceback()".
They print the function stack and the variables of each functions, it may helps.
---------- mytry.R -------------------------------------------------------------------------------------------------------------------------------------------------------
myTry <- function(expr) {
  withRestarts(
               withCallingHandlers(expr,
                                   error = function(e) {
                                     error  = e
                                     calls  = sys.calls()
                                     frames = sys.frames()
                                     invokeRestart("myAbort", e, calls, frames)
                                   }),
               myAbort = function(e, calls, frames){
                 err = list(error = e, calls=calls, frames=frames)
                 class(err)<-c("try-error")
                 return(err)
               }
               )
}

myTraceback <- function(trace, optFunctions2Skip=c() ){
  functions2Skip = c('withRestarts','withCallingHandlers','invokeRestart', 'withOneRestart' ,'myTry', 'doWithOneRestart', '.handleSimpleError', optFunctions2Skip)
  for( i in 1:length(trace$frames)){
    env  = trace$frames[[ i ]]
    func = trace$calls [[ i ]]
    funcStr  = as.character(trace$calls[[i]])
    
    funcNameStr = funcStr[1]

    args=FALSE
    simple.Error=FALSE
    
    if(length(funcStr) > 1){
      funcArgs    = funcStr[2:length(funcStr)]
      args = TRUE
      if(any(grep('simpleError',funcArgs))){simple.Error=TRUE}
    }
    if ( !(funcNameStr %in% functions2Skip) && !simple.Error){
      cat('\n---------------------------------\n')
      if(args) cat(funcNameStr,'(',funcArgs,')','\n')
      else     cat(funcNameStr,'()','\n')
      cat('- - - - - - - - - - - - - - - - -\n')
      
      vars=ls(env)
      for(v in vars){
        cat(v,'=')
        print(get(v, envir=env))
      }
    }
  }
}
-------------------------------------------------------------------------------------------------------------------------------------------------------
---- Exemple.R ---------------------------------------------------------------------------------------------------------------------------------

f<-function(a,b){
  return(regexpr(a,b))
}
g<-function(a,b){f(a,b)}

TEST<-function(){
  myTry(1+2)
  r = myTry(g("A",2))
  return( r )
}
r=TEST()
if(inherits(r,'try-error')){
  print(r$error)
  myTraceback(r, optFunctions2Skip='TEST')
}
-------------------------------------------------------------------------------------------------------------------------------------------------------

-- 
Edouard Duchesnay                      Tel: +33 1 69 86 78 52
CEA - SHFJ                             Fax: +33 1 69 86 77 86
4, place du G?n?ral Leclerc
91401 Orsay Cedex France



From jangann at mote.org  Mon Mar 15 15:04:05 2004
From: jangann at mote.org (Janet Gannon)
Date: Mon, 15 Mar 2004 09:04:05 -0500
Subject: [R] Simple numeric "as.is" question
Message-ID: <4055B7D5.4040209@mote.org>

I am reading a list of numbers from my clipboard, and have been 
successful, except I can't make a histogram as R doesn't recognize my 
variable as numeric.  I know I need to use "as.is", but the specifics 
escape me. 

I have used x<-read.table("clipboard", header=F) to import from a txt 
file.  How do make this numeric?  Thanks, J.



From MSchwartz at medanalytics.com  Mon Mar 15 15:07:00 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 15 Mar 2004 08:07:00 -0600
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <1079359620.7655.104.camel@localhost.localdomain>

On Mon, 2004-03-15 at 05:37, mike.campana at freesurf.ch wrote:
> Dear all
> 
> I wrote a routine. At the end of each cycle of the loop I would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).
> 
> Could you give me an advice? Thanks a lot
> 
> Best Regards 
> Mike


You can use paste() within a loop to construct the name of the resultant
ps files. An example:

number.graphics <- 5
for(i in 1:number.graphics)
  {
    postscript(file = paste("C:/Rfigures/plot_", i, ".ps", sep = ""))
    barplot(1:i)
    dev.off()
  }

The above will create 5 ps files called plot_1.ps through plot_5.ps in
the defined directory.

See ?paste for additional information.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Mon Mar 15 15:07:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 15 Mar 2004 15:07:30 +0100
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4055B8A2.8040501@statistik.uni-dortmund.de>

mike.campana at freesurf.ch wrote:
> Dear all
> 
> I wrote a routine. At the end of each cycle of the loop I would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).

Two comments:

a) You might want to use postscript() directly with its argument 
onefile=FALSE.

b) construct the filename with an index you are using in the loop (say "i"):
   dev.print(device=postscript,
     file = paste("c:/Rfigures/plot_", i, ".ps", sep = ""))


Uwe Ligges


> Could you give me an advice? Thanks a lot
> 
> Best Regards 
> Mike
> 
> ---
> ---
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From asemeria at cramont.it  Mon Mar 15 15:22:59 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Mon, 15 Mar 2004 15:22:59 +0100
Subject: [R] creating a ps. file
Message-ID: <OF7AF33AAB.DB9F31FC-ONC1256E58.004F021E@tomware.it>





for (i in 1:n){
dev.print(postcript,file=paste("c:/Rfigures/plot_",i,".ps",sep=""))
}

Best!

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From chris_ciotti at yahoo.com  Mon Mar 15 15:17:13 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Mon, 15 Mar 2004 09:17:13 -0500
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4055BAE9.8000908@yahoo.com>

mike.campana at freesurf.ch wrote:

>Dear all
>
>I wrote a routine. At the end of each cycle of the loop I would like to 
>save the result (plot) in a postcriptfile.
>Of course if I just use dev.print in the following way: 
>dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
>I overwrite my results with the second cycle of the loop. I suppose 
>there is a way to define the file name so that several plots are 
>created(plot_1,plot_2...).
>
>Could you give me an advice? Thanks a lot
>
>Best Regards 
>Mike
>
>  
>

Hello -

I've never done this in R (I'm a total 'R' newbie) but in C or Java, 
I've used the loop counter as a postfix (or prefix) for the file name.  
You could do it like this (non-R code):

int max = 10;
for(int i = 1; i < max; i++){
    [ . . .] //code for plotting etc  
    writer = PdfWriter.getInstance(document, new 
FileOutputStream(pdfTitle + "_"
                                      + i + ".pdf"));

}

This would give (assuming pdfTitle == "plot"): plot_1.pdf, plot_2.pdf 
and so and so forth

Others may be able to provide pointers etc for writing loops in R.  Hope 
this helps.


-- 
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7



From ma at ne.su.se  Mon Mar 15 15:15:13 2004
From: ma at ne.su.se (Mahmood Arai)
Date: Mon, 15 Mar 2004 15:15:13 +0100
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4055BA71.5060001@ne.su.se>

mike.campana at freesurf.ch wrote:

>Dear all
>
>I wrote a routine. At the end of each cycle of the loop I would like to 
>save the result (plot) in a postcriptfile.
>Of course if I just use dev.print in the following way: 
>dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
>I overwrite my results with the second cycle of the loop. I suppose 
>there is a way to define the file name so that several plots are 
>created(plot_1,plot_2...).
>
>Could you give me an advice? Thanks a lot
>
>  
>

see ?postscript

The following example saves two plots in a postscriptfile.

R> postscript("test.ps")
R> hist(1:100)
R> hist(rnorm(100))
R> dev.off()
null device
          1

/mahmood arai



>Best Regards 
>Mike
>
>---
>---
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From frohde_home at yahoo.com  Mon Mar 15 15:28:14 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Mon, 15 Mar 2004 06:28:14 -0800 (PST)
Subject: [R] R equiv to proc gremove in maps package
Message-ID: <20040315142814.99284.qmail@web60103.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040315/46ecd158/attachment.pl

From thpe at hhbio.wasser.tu-dresden.de  Mon Mar 15 15:33:04 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 15 Mar 2004 15:33:04 +0100
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4055BEA0.5050604@hhbio.wasser.tu-dresden.de>

mike.campana at freesurf.ch wrote:

> Dear all
> 
> I wrote a routine. At the end of each cycle of the loop I would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).
> 
> Could you give me an advice? Thanks a lot

Hello,

if you use:

nruns <- 200

for(i in 1:nruns) {
   postscript(filename=paste("plot_", i+1000, ".ps", sep=""))
   # some plotting commands here
   #
   dev.off()
}

then you get your files in a sortable order without too much string 
manipulation. Of course, you can also use 10000 or whatever, if you want 
more figures.

Thomas P.



From Simon.Fear at synequanon.com  Mon Mar 15 15:38:33 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 15 Mar 2004 14:38:33 -0000
Subject: [R] imputation of sub-threshold values
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02222@synequanon01>

> -----Original Message-----
> From: Jonathan Williams
> [mailto:jonathan.williams at pharmacology.oxford.ac.uk]
> Sent: 15 March 2004 12:48
> To: Ethz. Ch
> Subject: [R] imputation of sub-threshold values
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Is there a good way in R to impute values which exist, 
> but are less than the detection level for an assay?
> Thanks,
> 

There's no failsafe way to do this in ANY language. Imputing
using the value of the detection level, or half way between
there and zero (for strictly positive parameters) are two
methods commonly used in the pharma industry, if the
proportion of results below detection level is small.  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From petr.pikal at precheza.cz  Mon Mar 15 15:40:27 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 15 Mar 2004 15:40:27 +0100
Subject: [R] expanding some values in logical vector
In-Reply-To: <4055A6E3.6090709@wiwi.uni-bielefeld.de>
Message-ID: <4055CE6B.9440.1B6C2E0@localhost>

Thank you very much. I myself thought about something similar but I was not so 
smart to use outer.

Cheers
Petr


On 15 Mar 2004 at 13:51, Peter Wolf wrote:

> Try:
> 
>  > x<-rep(FALSE,20); x[c(4,10,15)]<-TRUE
>  > x
>  [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE
>  FALSE
> [13] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
>  > x[outer(which(x),-1:1,"+")]<-T
>  > x
>  [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE
>  FALSE
> [13] FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE
>  > x<-rep(FALSE,20); x[c(4,10,15)]<-TRUE
>  > x[outer(which(x),-2:2,"+")]<-T
>  > x
>  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
>   TRUE
> [13]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE
> # now we definie
>  > expand.true<-function(x,span=1){
>   m<-floor(span/2)
>   ind<-outer(which(x),(-m):m,"+")
>   ind<-ind[ind>0 & ind <= length(x)]
>   x[ind]<-TRUE
>   x
> }
>  > x<-rep(FALSE,20); x[c(4,10,19)]<-TRUE
>  > expand.true(x,span=5)
>  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
>   TRUE
> [13] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
> 
> 
> 
> Peter Wolf
> 
> 
> Petr Pikal wrote:
> 
> >Dear all
> >
> >In automatic dropout evaluation function I construct an index
> >(pointer), which will be TRUE at "unusual" values. Then I need to
> >expand these TRUE values a little bit forward and backward.
> >
> >Example:
> >
> >having span=5, from vector
> >
> >idx<-rep(F,10)
> >idx[4]<-T
> >
> >  
> >
> >>idx
> >>    
> >>
> > [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
> >FALSE
> >
> >I need
> >
> >  
> >
> >>idx
> >>    
> >>
> > [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE 
> >FALSE
> >
> >
> >I was using embed() for this task (myfun1):
> >
> >idx <- rep(F,20)
> >idx[c(4,11,13)] <- T
> >
> >  
> >
> >>idx
> >>    
> >>
> > [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
> >FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE 
> >FALSE
> >
> >iii <- myfun1(idx)
> >
> >  
> >
> >>iii
> >>    
> >>
> > [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  
> >TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE 
> >FALSE
> >
> >
> >It is close to what I want, but it is slow and when idx vector is
> >long enough it goes short of memory. Then I tried to accomplish it
> >with rle() and some fiddling with $values and $lengths (myfun2),
> >which works as long as the true values are completely separated.
> >
> >idx <- rep(F,20)
> >idx[c(4,12)] <- T
> >
> >  
> >
> >>idx
> >>    
> >>
> > [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE 
> >FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 
> >FALSE
> >
> >iii <- myfun2(idx)
> >  
> >
> >>iii
> >>    
> >>
> > [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  
> >TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE 
> >FALSE
> >	
> >But when using previous idx
> >
> >idx <- rep(F,20)
> >idx[c(4,11,13)] <- T
> >
> >iii <- myfun2(idx)
> >Error in rep.default(kody$values, opak) : invalid number of copies in
> >"rep"
> >
> >Problem is, if TRUE values are closer than span allows. Then some
> >values in "opak" for FALSE kody$values are negative, what is not
> >allowed. Setting them to zero will expand the length of index vector.
> >Instead number of repetitions of neighbour TRUE values should be
> >decreased accordingly. 
> >
> >I greatly appreciated, if somebody could give me a hint if there is
> >some another built in function which can help me or what to do with
> >myfun2 or how to get properly expanded index values by some another
> >way.
> >
> >Sorry for the long post but I was not able to explain my problem and
> >what I have done yet to solve it in shorter.
> >
> >Thank you and best regards.
> >
> >Petr Pikal
> >
> >
> >
> >
> >##### Here are functions used #####
> >
> >
> >
> >myfun1 <- function(idx,span=5)
> >
> >{
> >n <- length(idx)
> >s <- span%/%2
> >z <- embed(idx,span)
> >sumy <- rowSums(z)>0
> >index <- c(rep(sumy[1],s),sumy,rep(sumy[n-span+1],s))
> >}
> >
> >
> >
> >myfun2 <- function(idx,span=5)
> >
> >{
> >n <- length(idx)
> >
> >kody <- rle(idx)
> >test <- letters[sum(cumsum(c(kody$values[1],idx[n])))+1] ####is some
> >of true values at the end of vector idx? opak <-
> >kody$values*(span-1)*2-(span-1)+kody$lengths #### enlarge number of
> >TRUE repetitions according to the span delka <- length(opak)
> >
> >#### some ifs to ensure the end points will have correct number of
> >#### repetitions
> >
> >opak[c(1,delka)] <- opak[c(1,delka)]+span%/%2
> >if (sum(kody$values)==0) opak <- n
> >if (opak[1]<0) {opak[2] <- opak[2]+opak[1]; opak[1] <- 0}
> >if (opak[delka]<0) {opak[delka-1] <- opak[delka-1]+opak[delka];
> >opak[delka] <- 0}
> >
> >switch(test,
> >
> >a = opak<-opak,
> >b = opak[delka]<-opak[delka]-(span-1), 
> >c = opak[1]<-opak[1]-(span-1), 
> >d = opak[c(1,delka)]<-opak[c(1,delka)]-(span-1),
> >
> >)
> >
> >#### here should opak contain correct number of repetitions but does
> >#### not
> >
> >index<-rep(kody$values,opak)
> >
> >}
> >Petr Pikal
> >petr.pikal at precheza.cz
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >  
> >
> 
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From petzoldt at rcs.urz.tu-dresden.de  Mon Mar 15 15:40:32 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 15 Mar 2004 15:40:32 +0100
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4055C060.6020301@rcs.urz.tu-dresden.de>

mike.campana at freesurf.ch wrote:

> Dear all
> 
> I wrote a routine. At the end of each cycle of the loop I would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).
> 
> Could you give me an advice? Thanks a lot

Hello,

if you use:

nruns <- 200

for(i in 1:nruns) {
   postscript(file=paste("plot_", i+1000, ".ps", sep=""))
   # some plotting commands here
   #
   dev.off()
}

then you get your files in a sortable order without too much string
manipulation. Of course, you can also use 10000 or whatever, if you want
more figures.

Thomas P.

Sorry: in the postscript() function the option is called "file" and not 
"filename" like in bmp() and png()



From ramasamy at cancer.org.uk  Mon Mar 15 15:43:12 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 15 Mar 2004 14:43:12 -0000
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <ODEPICOHNDBJEHIFCIMPCEEGCBAA.ramasamy@cancer.org.uk>

paste("c:/Rfigures/plot", i, ".ps", sep="")

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> mike.campana at freesurf.ch
> Sent: 15 March 2004 11:37
> To: r-help at stat.math.ethz.ch
> Subject: [R] creating a ps. file
>
>
> Dear all
>
> I wrote a routine. At the end of each cycle of the loop I would like to
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way:
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose
> there is a way to define the file name so that several plots are
> created(plot_1,plot_2...).
>
> Could you give me an advice? Thanks a lot
>
> Best Regards
> Mike
>
> ---
> ---
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From chris_ciotti at yahoo.com  Mon Mar 15 15:45:21 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Mon, 15 Mar 2004 09:45:21 -0500
Subject: [R] setting x-y axis at origin
Message-ID: <4055C181.7030407@yahoo.com>

Hello -

I'm just getting into 'R' and am having trouble setting up the x-y axis 
to share (0,0).  In the example posted here: 
http://geocities.com/chris_ciotti/Images/part1.pdf, each axis has a 0 
which I do not want. 

Any help on getting a graph starting at (0,0) would be greatly 
appreciated. 


-- 
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7



From ripley at stats.ox.ac.uk  Mon Mar 15 15:52:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Mar 2004 14:52:18 +0000 (GMT)
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <Pine.LNX.4.44.0403151449510.23933-100000@gannet.stats>

Before the loop, open a postscript device with onefile=FALSE, and see the
advice in the postscript help page for how to specify the file argument.  
Then each plot will produce a separate file.  (Remember to do dev.off() 
after the loop.)

On Mon, 15 Mar 2004 mike.campana at freesurf.ch wrote:

> I wrote a routine. At the end of each cycle of the loop I would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).
> 
> Could you give me an advice? Thanks a lot
> 
> Best Regards 
> Mike

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Mon Mar 15 12:31:08 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 15 Mar 2004 12:31:08 +0100
Subject: [R] nnet classification accuracy vs. other models
In-Reply-To: <Pine.GSO.3.95q.1040315101109.7135A-100000@sun11.math.uni-hamburg.de>
Message-ID: <MABBLJDICACNFOLGIHJOCEJHEDAA.phgrosjean@sciviews.org>

I did a comparison of 15 different methods with a particularly difficult
real dataset. I tried to fine-tune all methods the best as I could. I must
admit that fine-tuning of nnet was by far the longest process, and I am
pretty sure it is still not optimal! I did 100 replicates of random
selection of 2/3 of the data for the training. The rest being used for
estimating accuracy (I could have done a n-fold cross-validation as well) of
the methods. (I got the timing too for the methods, because it is an
important factor in my application). The results are presented in the
attached graph. Here is the legend of the labels:
- lda = linear discriminant analysis,
- qda = quadratic discriminant analysis,
- mda = mixture discriminant analysis,
- fda = flexible discriminant analysis,
- knn = k-nearest neighbourg,
- lvq = learning vector quantization,
- tree = tree method (from tree package),
- rpart = tree method (from rpart package),
- bagg = bagging (from ipred),
- db.l = double bagging with lda (ipred),
- db.k = double bagging with knn(ipred),
- rfor = random forest,
- svm = support vector machine,
- nnet = neural network,
- dvf = discriminant vector forest (a new method I have set up).

You see that, if most median values for total accuracy are around between
77% and 85%, there are a few methods that are a little bit less performant
(tree, rpart, svm and nnet). However, a method like svm (from package e1071)
is usually performing better than that and compares often favorably with
other machine learning techniques. So, I checked with its author (David
Meyer) if it was possible to improve these results, and if I did something
wrong. He did not found a way to improve it much more. However, I did not
the same exercice for nnet.

Anyway, this shows that some differences between methods could be expected
and results are not always as expected (that is, application of these
methods on real data do not always match the results obtained with the
various benchmark artificial data often used to compare these methods)!

Best,

Philippe Grosjean

.......................................................<?}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Christian Hennig
Sent: Monday, 15 March, 2004 10:19
To: Albedo
Cc: s-news at lists.biostat.wustl.edu; r-help at stat.math.ethz.ch
Subject: Re: [R] nnet classification accuracy vs. other models


My experience is that nnet needs a lot of tuning, not only in terms of
numbers of layers, but also in terms of the other parameters. My first
results where I kept very much of the default parameter values with nnet
have been very bad, as bad as you say. (But as Brian Ripley already wrote,
it's not straight forward to say via the net how to do it better.)

Apart from that, such a large difference of classification accuracy
between different methods is strange, but possible in principle.
Very different structures of data exist (which means again that nobody can
assess your problem without knowing the data).

Christian

On Sat, 13 Mar 2004, Albedo wrote:

> I was wandering if anybody ever tried to compare the classification
> accuracy of nnet to other (rpart, tree, bagging) models. From what I
> know, there is no reason to expect a significant difference in
> classification accuracy between these models, yet in my particular case
> I get about 10% error rate for tree, rpart and bagging model and 80%
> error rate for nnet, applied to the same data.
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

-------------- next part --------------
A non-text attachment was scrubbed...
Name: compalm.pdf
Type: application/pdf
Size: 80850 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040315/026c4fbf/compalm.pdf

From Timur.Elzhov at jinr.ru  Mon Mar 15 16:18:56 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 15 Mar 2004 18:18:56 +0300
Subject: [R] creating a ps. file
In-Reply-To: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079350622.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <20040315151856.GA24354@nf034.jinr.ru>

On Mon, Mar 15, 2004 at 12:37:02PM +0100, mike.campana at freesurf.ch wrote:

> I wrote a routine. At the end of each cycle of the loop I would like to 
> save the result (plot) in a postcriptfile.
> Of course if I just use dev.print in the following way: 
> dev.print(device=postcript, 'c:/Rfigures/plot_1.ps")
> I overwrite my results with the second cycle of the loop. I suppose 
> there is a way to define the file name so that several plots are 
> created(plot_1,plot_2...).
> 
> Could you give me an advice? Thanks a lot

for (i in 1:10) {
    x11(); plot(...)
    ...
    dev.copy2eps(file = paste("plot_", i, ".eps", sep = ""))
}

--
WBR,
Timur



From andy_liaw at merck.com  Mon Mar 15 16:16:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 15 Mar 2004 10:16:45 -0500
Subject: [R] Simple numeric "as.is" question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79A7@usrymx25.merck.com>

If read.table() is not recognizing the data as numeric, there's probably
something in the content that confused read.table().  You can try using 

x <- scan("clipboard") 

instead and see if and how it chokes.

Andy

> From: Janet Gannon
> 
> I am reading a list of numbers from my clipboard, and have been 
> successful, except I can't make a histogram as R doesn't recognize my 
> variable as numeric.  I know I need to use "as.is", but the specifics 
> escape me. 
> 
> I have used x<-read.table("clipboard", header=F) to import from a txt 
> file.  How do make this numeric?  Thanks, J.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From sundar.dorai-raj at pdf.com  Mon Mar 15 16:22:25 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 15 Mar 2004 09:22:25 -0600
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <4055B7D5.4040209@mote.org>
References: <4055B7D5.4040209@mote.org>
Message-ID: <4055CA31.1020005@pdf.com>



Janet Gannon wrote:

> I am reading a list of numbers from my clipboard, and have been 
> successful, except I can't make a histogram as R doesn't recognize my 
> variable as numeric.  I know I need to use "as.is", but the specifics 
> escape me.
> I have used x<-read.table("clipboard", header=F) to import from a txt 
> file.  How do make this numeric?  Thanks, J.
> 

Please (re-)read ?read.table. Specifically, the "as.is" and "colClasses" 
arguments.

-sundar



From abunn at montana.edu  Mon Mar 15 16:23:57 2004
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 15 Mar 2004 08:23:57 -0700
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <4055B7D5.4040209@mote.org>
Message-ID: <001301c40aa1$8c190980$78f05a99@msu.montana.edu>

Look at the help for:
?as.numeric
HTH, Andy
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Janet Gannon
> Sent: Monday, March 15, 2004 7:04 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Simple numeric "as.is" question
> 
> 
> I am reading a list of numbers from my clipboard, and have been 
> successful, except I can't make a histogram as R doesn't recognize my 
> variable as numeric.  I know I need to use "as.is", but the specifics 
> escape me. 
> 
> I have used x<-read.table("clipboard", header=F) to import from a txt 
> file.  How do make this numeric?  Thanks, J.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Timur.Elzhov at jinr.ru  Mon Mar 15 16:30:06 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 15 Mar 2004 18:30:06 +0300
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <4055B7D5.4040209@mote.org>
References: <4055B7D5.4040209@mote.org>
Message-ID: <20040315153006.GA24469@nf034.jinr.ru>

On Mon, Mar 15, 2004 at 09:04:05AM -0500, Janet Gannon wrote:

> I am reading a list of numbers from my clipboard, and have been 
> successful, except I can't make a histogram as R doesn't recognize my 
> variable as numeric.  I know I need to use "as.is", but the specifics 
> escape me. 
> 
> I have used x<-read.table("clipboard", header=F) to import from a txt 
> file.  How do make this numeric?  Thanks, J.

x <- read.table("clipboard", header=F)
x <- as.data.frame(lapply(x, as.numeric))

--
WBR,
Timur



From fzoellne at TechFak.Uni-Bielefeld.DE  Mon Mar 15 16:31:58 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Mon, 15 Mar 2004 16:31:58 +0100
Subject: [R] norm of complex number
Message-ID: <20040315153158.GA12408@hindemith.TechFak.Uni-Bielefeld.DE>

Hi!

I want o calc the norm of a complex number ( |c| where c is complex). Does the function abs() this ?

Thanks,
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From tlumley at u.washington.edu  Mon Mar 15 16:38:12 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 15 Mar 2004 07:38:12 -0800 (PST)
Subject: [R] imputation of sub-threshold values
In-Reply-To: <NGBBKJEMOMLJFCOIEGCEAEOOJKAA.jonathan.williams@pharm.ox.ac.uk>
References: <NGBBKJEMOMLJFCOIEGCEAEOOJKAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <Pine.A41.4.58.0403150730010.58628@homer03.u.washington.edu>

On Mon, 15 Mar 2004, Jonathan Williams wrote:

> Is there a good way in R to impute values which exist,
> but are less than the detection level for an assay?


If there were a good way to do it, it would probably be implementable or
implemented in R.


If you can persuade the people measuring the values to give you the
numbers (assuming they are just below `limit of detection' rather
than genuine non-detects) you will reduce the need for imputation. This is
often the most powerful technique -- analytical chemists are trained not
to give out numbers less than some multiple of the measurement error, but
they can often be persuaded that statisticians can be trusted with these
numbers.

If you think your data are close to one of the parametric survival models
in the survival package, you can analyse the data as left-censored,
without imputation. You could impute from the fitted model, if you need
imputations.

Otherwise you may be stuck with some ad hoc imputation with a sensitivity
analysis to see how the results depend on the imputed value.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tblackw at umich.edu  Mon Mar 15 17:04:08 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Mon, 15 Mar 2004 11:04:08 -0500 (EST)
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <4055B7D5.4040209@mote.org>
References: <4055B7D5.4040209@mote.org>
Message-ID: <Pine.SOL.4.58.0403151059550.18305@rygar.gpcc.itd.umich.edu>

Janet  -

Try  x2 <- as.numeric(as.character(x))
     hist(x2)

I'm not a Windows user, so I can't test this before sending.
It might solve the problem, might not.

(Flame !! :  This is just ONE MORE example of the difficulties
caused by the default behavior of  read.table()  to make things
into factors.  I sincerely wish the default were to preserve
character data as character.)

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 15 Mar 2004, Janet Gannon wrote:

> I am reading a list of numbers from my clipboard, and have been
> successful, except I can't make a histogram as R doesn't recognize my
> variable as numeric.  I know I need to use "as.is", but the specifics
> escape me.
>
> I have used x<-read.table("clipboard", header=F) to import from a txt
> file.  How do make this numeric?  Thanks, J.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From thpe at hhbio.wasser.tu-dresden.de  Mon Mar 15 17:05:01 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 15 Mar 2004 17:05:01 +0100
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <4055B7D5.4040209@mote.org>
References: <4055B7D5.4040209@mote.org>
Message-ID: <4055D42D.8050007@hhbio.wasser.tu-dresden.de>

Janet Gannon wrote:

> I am reading a list of numbers from my clipboard, and have been 
> successful, except I can't make a histogram as R doesn't recognize my 
> variable as numeric.  I know I need to use "as.is", but the specifics 
> escape me.
> I have used x<-read.table("clipboard", header=F) to import from a txt 

In normal circumstances, your example works, but sometimes you need 
special options, e.g sep="\t" if you have missing values or dec="," in 
countries with comma as decimal delimiter. Another possibility is, that 
read.table has converted your data into factors, then convert them back, 
  via:

x$V1 <- as.numeric(as.character(x$V1))

etc...

For more options please read the R Data Import/Export manual.

Thomas



From ligges at statistik.uni-dortmund.de  Mon Mar 15 17:10:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 15 Mar 2004 17:10:37 +0100
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <4055B7D5.4040209@mote.org>
References: <4055B7D5.4040209@mote.org>
Message-ID: <4055D57D.5000104@statistik.uni-dortmund.de>

Janet Gannon wrote:

> I am reading a list of numbers from my clipboard, and have been 
> successful, except I can't make a histogram as R doesn't recognize my 
> variable as numeric.  I know I need to use "as.is", but the specifics 
> escape me.
> I have used x<-read.table("clipboard", header=F) to import from a txt 
> file.  How do make this numeric?  Thanks, J.


So the elements of your data frame "x" are not numeric? read.table() 
auto-detects which variables are numeric or not, so there's something 
strange with your textfile, I guess.

   as.numeric(x$V1)
converts the first element in your data.frame x to numeric.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Mon Mar 15 17:12:38 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 15 Mar 2004 17:12:38 +0100
Subject: [R] printing graphics whithout save us
In-Reply-To: <46852.170.210.173.216.1079358392.squirrel@inter17.unsl.edu.ar>
References: <46852.170.210.173.216.1079358392.squirrel@inter17.unsl.edu.ar>
Message-ID: <4055D5F6.1010206@statistik.uni-dortmund.de>

solares at unsl.edu.ar wrote:

> hello, it wanted to know as printing a graphics on a screen, without to
> have save in a file, already use dev.print and give me an error  in PS ,
> unable to start device postscript and a warning printing via file =" " not
> implemented.Thanks Ruben


Please read ?dev.print completely, which tells you (among other things):

"The default for dev.print is to produce and print a postscript copy. 
This will not work unless options("printcmd") is set suitably and you 
have a PostScript printer: see postscript for how to set this up. 
Windows users may prefer to use dev.print(win.print)."


Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From statho3 at web.de  Mon Mar 15 18:09:45 2004
From: statho3 at web.de (Thomas Stabla)
Date: Mon, 15 Mar 2004 18:09:45 +0100 (CET)
Subject: [R] R CMD check errors
In-Reply-To: <4051E585.2010909@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0403151805160.1166-100000@spock.vulcan>

On Fri, 12 Mar 2004, Uwe Ligges wrote:

> Thomas Stabla wrote:
> > Hello again,
> >
> > I tried to isolate the source code, which causes the error messages:
> >
> > * checking S3 generic/method consistency ... WARNING
> > Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
character.only = TRUE,
verbose = FALSE) :
> >         package/namespace load failed
> > * checking for replacement functions with final arg not named 'value'
... WARNING
> > Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
character.only = TRUE,
verbose = FALSE) :
> >         package/namespace load failed
> >
> >
> > I created a new package skeleton by calling package.skeleton(name =
> > "error") from R 1.8.1.
> >
> > I deleted the subdirectories 'data', 'src' and 'man'.
> >
> > The NAMESPACE file contains:
> >
> >   exportPattern("^[^\\.]")
>
>
> and lacks:
>
>     import("methods")
>
> and for sure you want as well:
>
>     exportClass("Parameter")
>
>
> Thus, the complete file is:
>
>
>     import("methods")
>     exportClass("Parameter")
>     exportPattern("^[^\\.]")
>

sounds logical.
BTW, I read "Writing R Extensions" in HMTL form under Windows R 1.8.1, and
it doesn't mention exportClass.
Also it doesn't describe how to use S4 classes (or package "methods") and
namepaces together.
Hence, I think it is not up to date.

>
> > The Error.R file in the subdirectory 'R' contains
> >
> >   x <- 1
> >   setClass("Parameter", setClass("Parameter", representation(name =
"character"))
>
>
> Well, above, you mean:
>    setClass("Parameter", representation(name = "character"))
>

copy & paste error, sorry

Now R CMD check distr runs smoothly, thanks for your help.

Thomas Stabla



From ramasamy at cancer.org.uk  Mon Mar 15 18:10:30 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 15 Mar 2004 17:10:30 -0000
Subject: [R] make check on Solaris 8 fails due to plot
Message-ID: <ODEPICOHNDBJEHIFCIMPAEENCBAA.ramasamy@cancer.org.uk>

Dear all,

I am having trouble trying to install R-1.8.1 on a Sun Solaris 8
(Generic_108528-23 version) machine. The configuration was successful but
make check fails. I traced the the problem to the plot() function.

> 1 + 1
[1] 2
> capabilities()
    jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
    TRUE    FALSE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
  libxml     fifo   cledit  IEEE754    bzip2     PCRE
    TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
> plot( 1:10 )
Bus Error

and R terminates to the shell prompt. I have searched the archives with no
success. Can anyone kindly help me fix the problem or at least show me how
to get more information about the error.

Thank you.

--
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Medical Statistician
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Medical Statistics Group                Tel : 01865 226 677
Cancer Research UK                      Fax : 01865 226 962



From ligges at statistik.uni-dortmund.de  Mon Mar 15 18:30:17 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 15 Mar 2004 18:30:17 +0100
Subject: [R] R CMD check errors
In-Reply-To: <Pine.LNX.4.44.0403151805160.1166-100000@spock.vulcan>
References: <Pine.LNX.4.44.0403151805160.1166-100000@spock.vulcan>
Message-ID: <4055E829.8000708@statistik.uni-dortmund.de>

Thomas Stabla wrote:

> On Fri, 12 Mar 2004, Uwe Ligges wrote:
> 
> 
>>Thomas Stabla wrote:
>>
>>>Hello again,
>>>
>>>I tried to isolate the source code, which causes the error messages:
>>>
>>>* checking S3 generic/method consistency ... WARNING
>>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
> 
> character.only = TRUE,
> verbose = FALSE) :
> 
>>>        package/namespace load failed
>>>* checking for replacement functions with final arg not named 'value'
> 
> ... WARNING
> 
>>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
> 
> character.only = TRUE,
> verbose = FALSE) :
> 
>>>        package/namespace load failed
>>>
>>>
>>>I created a new package skeleton by calling package.skeleton(name =
>>>"error") from R 1.8.1.
>>>
>>>I deleted the subdirectories 'data', 'src' and 'man'.
>>>
>>>The NAMESPACE file contains:
>>>
>>>  exportPattern("^[^\\.]")
>>
>>
>>and lacks:
>>
>>    import("methods")
>>
>>and for sure you want as well:
>>
>>    exportClass("Parameter")
>>
>>
>>Thus, the complete file is:
>>
>>
>>    import("methods")
>>    exportClass("Parameter")
>>    exportPattern("^[^\\.]")
>>
> 
> 
> sounds logical.
> BTW, I read "Writing R Extensions" in HMTL form under Windows R 1.8.1, and
> it doesn't mention exportClass.
> Also it doesn't describe how to use S4 classes (or package "methods") and
> namepaces together.
> Hence, I think it is not up to date.

You are right, sorry. The version in R-devel (R-1.9.0 alpha) is up to 
date and mentions how to handle S4 classes. So you might want to check 
that one.

Best,
Uwe




> 
>>>The Error.R file in the subdirectory 'R' contains
>>>
>>>  x <- 1
>>>  setClass("Parameter", setClass("Parameter", representation(name =
> 
> "character"))
> 
>>
>>Well, above, you mean:
>>   setClass("Parameter", representation(name = "character"))
>>
> 
> 
> copy & paste error, sorry
> 
> Now R CMD check distr runs smoothly, thanks for your help.
> 
> Thomas Stabla
> 
> 
> 
>



From B.Rowlingson at lancaster.ac.uk  Mon Mar 15 19:12:54 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 15 Mar 2004 18:12:54 +0000
Subject: [R] R equiv to proc gremove in maps package
In-Reply-To: <20040315142814.99284.qmail@web60103.mail.yahoo.com>
References: <20040315142814.99284.qmail@web60103.mail.yahoo.com>
Message-ID: <4055F226.3080804@lancaster.ac.uk>

Fred Rohde wrote:
> Is there an R equivalent to SAS's proc gremove?  You would use this procedure to combine
> the units on an existing map, for example to build a map of Metropolitan Statistical Areas
 > (MSAs) from the [US] counties dataset where the internal boundries 
surround the MSAs (which
 > are groups of counties) rather than the individual counties.

  This sounds like the standard GIS 'dissolve' function. Its probably a 
bad idea to try and give R all the GIS functions on the planet - better 
to link it with a GIS and use that instead.

  GRASS-GIS can do a dissolve operation, using v.reclass with the -d 
option. See http://grass.itc.it/gdp/html_grass5/html/v.reclass.html

  Of course you'll then need some way of getting your data into and out 
of GRASS, there's a GRASS R library by Roger Bivand that might help and 
an R-Arc/Info lib by Virgilio Gomez-Rubio that also might help.

  Doing it from R's 'maps' format could be fun though... for some 
definition of 'fun'...

Barry



From ripley at stats.ox.ac.uk  Mon Mar 15 19:26:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Mar 2004 18:26:47 +0000 (GMT)
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <001301c40aa1$8c190980$78f05a99@msu.montana.edu>
Message-ID: <Pine.LNX.4.44.0403151824110.3984-100000@gannet.stats>

On Mon, 15 Mar 2004, Andy Bunn wrote:

> Look at the help for:
> ?as.numeric

and make sure you read there that it is the wrong thing (as a column of a
data frame will be a factor).  Better to look this up in the FAQ!

> HTH, Andy
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Janet Gannon
> > Sent: Monday, March 15, 2004 7:04 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Simple numeric "as.is" question
> > 
> > 
> > I am reading a list of numbers from my clipboard, and have been 
> > successful, except I can't make a histogram as R doesn't recognize my 
> > variable as numeric.  I know I need to use "as.is", but the specifics 
> > escape me. 
> > 
> > I have used x<-read.table("clipboard", header=F) to import from a txt 
> > file.  How do make this numeric?  Thanks, J.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Mar 15 19:30:50 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 15 Mar 2004 10:30:50 -0800
Subject: [R] norm of complex number
In-Reply-To: <20040315153158.GA12408@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040315153158.GA12408@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <4055F65A.7080207@pdf.com>

?complex reveals "Mod": 

 > Mod(1+1i)
[1] 1.414214
 >
      hope this helps.  spencer graves
p.s.  help.search("complex") reveals "complex", etc. 

Frank Gerrit Zoellner wrote:

>Hi!
>
>I want o calc the norm of a complex number ( |c| where c is complex). Does the function abs() this ?
>
>Thanks,
>  
>



From ripley at stats.ox.ac.uk  Mon Mar 15 19:31:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Mar 2004 18:31:05 +0000 (GMT)
Subject: [R] setting x-y axis at origin
In-Reply-To: <4055C181.7030407@yahoo.com>
Message-ID: <Pine.LNX.4.44.0403151830101.3984-100000@gannet.stats>

?par, look at xaxs and yaxs, especially style "i".

On Mon, 15 Mar 2004, christopher ciotti wrote:

> I'm just getting into 'R' and am having trouble setting up the x-y axis 
> to share (0,0).  In the example posted here: 
> http://geocities.com/chris_ciotti/Images/part1.pdf, each axis has a 0 
> which I do not want. 
> 
> Any help on getting a graph starting at (0,0) would be greatly 
> appreciated. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Mar 15 19:32:57 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 15 Mar 2004 10:32:57 -0800
Subject: [R] setting x-y axis at origin
In-Reply-To: <4055C181.7030407@yahoo.com>
References: <4055C181.7030407@yahoo.com>
Message-ID: <4055F6D9.2080400@pdf.com>

      I don't understand exactly what you want but the following might 
help: 

plot(1:11, 1:11, xlim=c(2, 5), ylim=c(3, 7))

      Is this what you want?  spencer graves

christopher ciotti wrote:

> Hello -
>
> I'm just getting into 'R' and am having trouble setting up the x-y 
> axis to share (0,0).  In the example posted here: 
> http://geocities.com/chris_ciotti/Images/part1.pdf, each axis has a 0 
> which I do not want.
> Any help on getting a graph starting at (0,0) would be greatly 
> appreciated.
>



From MSchwartz at medanalytics.com  Mon Mar 15 19:35:26 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 15 Mar 2004 12:35:26 -0600
Subject: [R] setting x-y axis at origin
In-Reply-To: <4055C181.7030407@yahoo.com>
References: <4055C181.7030407@yahoo.com>
Message-ID: <1079375726.6991.17.camel@localhost.localdomain>

On Mon, 2004-03-15 at 08:45, christopher ciotti wrote:
> Hello -
> 
> I'm just getting into 'R' and am having trouble setting up the x-y axis 
> to share (0,0).  In the example posted here: 
> http://geocities.com/chris_ciotti/Images/part1.pdf, each axis has a 0 
> which I do not want. 
> 
> Any help on getting a graph starting at (0,0) would be greatly 
> appreciated. 


What you are seeing is the default behavior for axis ranges, which are
extended by 4% under default circumstances. This is described in the
help for 'par' under 'xaxs' when using the default axis style "r". See
?par for additional information.

So for example, if you use:

plot(c(0, 100), c(0,140))

You will see a generic scatter plot that has roughly the same axis
ranges as your plot.

You have a simple approach to alter this behavior by changing the axis
styles to "i".

plot(c(0, 100), c(0,140), xaxs = "i", yaxs = "i")

If your x and y values do not actually contain (0,0), you can explicitly
define the axis ranges by using 'xlim' and 'ylim' respectively:

plot(c(1, 100), c(1,140), xaxs = "i", yaxs = "i")

Note the above origin is at (1, 1). Use the following instead:

plot(c(1, 100), c(1,140), xaxs = "i", yaxs = "i", 
     xlim = c(0, 100), ylim = c(0, 140))

You can also adjust the xlim and ylim values as you require based upon
you actual data (ie. xlim = c(0, max(x)) )

You might want to review ?plot.default and ?axis for additional
information on some of the arguments available for fine tuning plots and
axes along with the help for par.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Mon Mar 15 19:35:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 15 Mar 2004 13:35:05 -0500
Subject: [R] norm of complex number
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79B0@usrymx25.merck.com>

Not abs(), but Mod().  See ?complex.

HTH,
Andy

> From: Frank Gerrit Zoellner
> 
> Hi!
> 
> I want o calc the norm of a complex number ( |c| where c is 
> complex). Does the function abs() this ?
> 
> Thanks,
> -- 
> Frank G. Zoellner
> AG Angewandte Informatik
> Technische Fakult"at
> Universit"at Bielefeld
> phone: +49(0)521-106-2951
> fax:   +49(0)521-106-2992
> email: fzoellne at techfak.uni-bielefeld.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From chris_ciotti at yahoo.com  Mon Mar 15 19:56:50 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Mon, 15 Mar 2004 13:56:50 -0500
Subject: [R] setting x-y axis at origin
In-Reply-To: <4055C181.7030407@yahoo.com>
References: <4055C181.7030407@yahoo.com>
Message-ID: <4055FC72.7070404@yahoo.com>

christopher ciotti wrote:

> Hello -
>
> I'm just getting into 'R' and am having trouble setting up the x-y 
> axis to share (0,0).  In the example posted here: 
> http://geocities.com/chris_ciotti/Images/part1.pdf, each axis has a 0 
> which I do not want.
> Any help on getting a graph starting at (0,0) would be greatly 
> appreciated.
>

Thank you all for the info. 

-- 
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7



From ripley at stats.ox.ac.uk  Mon Mar 15 20:16:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Mar 2004 19:16:33 +0000 (GMT)
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <Pine.SOL.4.58.0403151059550.18305@rygar.gpcc.itd.umich.edu>
Message-ID: <Pine.LNX.4.44.0403151911470.4073-100000@gannet.stats>

On Mon, 15 Mar 2004, Tom Blackwell wrote:

> Try  x2 <- as.numeric(as.character(x))
>      hist(x2)
> 
> I'm not a Windows user, so I can't test this before sending.
> It might solve the problem, might not.
> 
> (Flame !! :  This is just ONE MORE example of the difficulties
> caused by the default behavior of  read.table()  to make things
> into factors.  I sincerely wish the default were to preserve
> character data as character.)

Eh?  Misusing read.table where scan() is appropriate is hardly a cause for 
flaming read.table.  The point is that if this were numeric data, 
read.table would have read it as numeric, so it is not and that should be 
resolved first.

Be careful what you wish: *all* data on a file is character, so you 
would always get character columns -- you might as well use a character 
matrix.

> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Mon, 15 Mar 2004, Janet Gannon wrote:
> 
> > I am reading a list of numbers from my clipboard, and have been
> > successful, except I can't make a histogram as R doesn't recognize my
> > variable as numeric.  I know I need to use "as.is", but the specifics
> > escape me.
> >
> > I have used x<-read.table("clipboard", header=F) to import from a txt
> > file.  How do make this numeric?  Thanks, J.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Mar 15 20:18:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Mar 2004 19:18:18 +0000 (GMT)
Subject: [R] norm of complex number
In-Reply-To: <20040315153158.GA12408@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <Pine.LNX.4.44.0403151917220.4073-100000@gannet.stats>

That's called the modulus, and computed by function Mod().

On Mon, 15 Mar 2004, Frank Gerrit Zoellner wrote:

> I want o calc the norm of a complex number ( |c| where c is complex).
> Does the function abs() this ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ray at mcs.vuw.ac.nz  Mon Mar 15 20:25:26 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 16 Mar 2004 08:25:26 +1300 (NZDT)
Subject: [R] R equiv to proc gremove in maps package
Message-ID: <200403151925.i2FJPQgU005916@tahi.mcs.vuw.ac.nz>

> Is there an R equivalent to SAS's proc gremove?  You would use this procedure to combine the units on an existing map, for example to build a map of Metropolitan Statistical Areas (MSAs) from the [US] counties dataset where the internal boundries surround the MSAs (which are groups of counties) rather than the individual counties.  I can imagine the mechanism would be to find and erase the boundries within each group.  Is there a way to do that in R?  I looked at the S reference on building geographical databases, and I could probably fumble through the C code to build the binary files of lines and polygons, but have no idea how to implement the shell program to extract the datapoints.  Thanks.
> 
It's already built in to the maps package.

Try e.g.:
map("county", c("arizona,gila", "arizona,maricopa", "arizona,yavapai"), interior=F)

[I wouldn't have believed that it worked until I tried!]

Hope this helps,
Ray Brownrigg



From ggrothendieck at myway.com  Mon Mar 15 20:37:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 15 Mar 2004 14:37:09 -0500 (EST)
Subject: [R] Simple numeric 
Message-ID: <20040315193709.BD21E39AB@mprdmxin.myway.com>



Regarding the flame, your might be interested in the following:

   read.table(myfile, as.is=T)

which will interpret character columns as character rather
than factor and still reads in the numeric columns as numbers.
You can also specify specific columns such as as.is=3:4 if you
want columns 3 and 4 to be character but other character columns
to be factors.

---
Date:   Mon, 15 Mar 2004 11:04:08 -0500 (EST) 
From:   Tom Blackwell <tblackw at umich.edu>
To:   Janet Gannon <jangann at mote.org> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Simple numeric "as.is" question 

 
Janet -

Try x2 <- as.numeric(as.character(x))
hist(x2)

I'm not a Windows user, so I can't test this before sending.
It might solve the problem, might not.

(Flame !! : This is just ONE MORE example of the difficulties
caused by the default behavior of read.table() to make things
into factors. I sincerely wish the default were to preserve
character data as character.)

- tom blackwell - u michigan medical school - ann arbor -

On Mon, 15 Mar 2004, Janet Gannon wrote:

> I am reading a list of numbers from my clipboard, and have been
> successful, except I can't make a histogram as R doesn't recognize my
> variable as numeric. I know I need to use "as.is", but the specifics
> escape me.
>
> I have used x<-read.table("clipboard", header=F) to import from a txt
> file. How do make this numeric? Thanks, J.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wolski at molgen.mpg.de  Mon Mar 15 20:53:02 2004
From: wolski at molgen.mpg.de (wolski)
Date: Mon, 15 Mar 2004 20:53:02 +0100
Subject: [R] creating vignettes ... ERROR
Message-ID: <200403152053020585.26664F3E@harry.molgen.mpg.de>

The package build fails with:


* creating vignettes ... ERROR
/package/R/R-1.8.1/linux/lib/R/bin/texi2dvi: pdflatex exited with bad status, quitting.
/package/R/R-1.8.1/linux/lib/R/bin/texi2dvi: see SamplesSession.log for errors.
Error in texi2dvi(file = bft, pdf = TRUE, clean = TRUE, quiet = quiet) : 
        running texi2dvi on SamplesSession.tex failed


But on the other hand the check passes and running pdflatex on SamplesSession.tex directly
gives in the SamplesSession.log no error. Whats wrong?

0.pfb><cmmi10.pfb><cmbx10.pfb><cmtt10.pfb><cmr10.pfb><cmr12.pfb><cmbx12.pfb><cm
r17.pfb>
Output written on SamplesSession.pdf (9 pages, 139065 bytes).

Eryk



From maechler at stat.math.ethz.ch  Mon Mar 15 21:03:16 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 Mar 2004 21:03:16 +0100
Subject: [R] norm of complex number
In-Reply-To: <20040315153158.GA12408@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040315153158.GA12408@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <16470.3076.656590.600506@gargle.gargle.HOWL>

>>>>> "Frank" == Frank Gerrit Zoellner <fzoellne at techfak.uni-bielefeld.de>
>>>>>     on Mon, 15 Mar 2004 16:31:58 +0100 writes:

    Frank> Hi!  I want o calc the norm of a complex number ( |c|
    Frank> where c is complex). Does the function abs() this ?

Yes, it does - and help(abs) will tell you so in the next
version of R.

For complex numbers, the more usual function name is Mod().

Regards,
Martin Maechler



From spencer.graves at pdf.com  Mon Mar 15 21:20:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 15 Mar 2004 12:20:12 -0800
Subject: [R] norm of complex number
In-Reply-To: <Pine.LNX.4.44.0403151917220.4073-100000@gannet.stats>
References: <Pine.LNX.4.44.0403151917220.4073-100000@gannet.stats>
Message-ID: <40560FFC.3010605@pdf.com>

      "abs" also works (in R 1.8.1 under Windows 2000), as we see just 
by trying it: 

      abs(1+1i) = 1.414214

      hope this helps.  spencer graves

Prof Brian Ripley wrote:

>That's called the modulus, and computed by function Mod().
>
>On Mon, 15 Mar 2004, Frank Gerrit Zoellner wrote:
>
>  
>
>>I want o calc the norm of a complex number ( |c| where c is complex).
>>Does the function abs() this ?
>>    
>>
>
>  
>



From mvdv at spamcop.net  Mon Mar 15 21:37:33 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Tue, 16 Mar 2004 07:37:33 +1100
Subject: [R] Correct Computer Modern font in postscript(..) output
In-Reply-To: <Pine.LNX.4.44.0403150801530.20184-100000@gannet.stats>
Message-ID: <000001c40acd$594257b0$344610ac@FEB0480>

Brian,
Thanks for the reponse.  I apologise if I gave the impression of being
accusatory - not my intention.  My point was that when I use italic(), in R
with ComputerModern, I get output 
different to the output I get when I use \italic in TeX/LaTex.  I'm neither
an R nor TeX pro (for want of time not interest), so I simply expected the
two to be the same.  Brian states the current output is the intended
behaviour, which is fine.
Thanks
Mark
 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, March 15, 2004 7:12 PM
> To: Mark Van De Vyver
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Correct Computer Modern font in postscript(..) output
> 
> 
> family="ComputerModern" gives you CMSL10, which is the 
> `correct font'. From src/main/devPS.c:
> 
>     /* Computer Modern as recoded by Brian D'Urso */
>     { "ComputerModern",
>       {"CM_regular_10.afm", "CM_boldx_10.afm", "CM_italic_10.afm",
>        "CM_boldx_italic_10.afm", "CM_symbol_10.afm"}
>     },
> 
> and CM_italic_10.afm specifies CMSL10: there is no italic 
> font in standard 
> Computer Modern, and the `correct font' is the one the 
> designer intended.
> 
> What did you think the `correct font' was?
> 
> Please do read the posting guide and its references, and try 
> to write more 
> informative (and less accusatory) messages.
> 
> 
> On Mon, 15 Mar 2004, Mark Van De Vyver wrote:
> 
> > Hi,
> > I'm trying to get the correct font used when generating 
> italic text in 
> > an R grahic.  I have a set of labels that print correctly except it 
> > seems the italic text is justr a slanted version of the TeX 
> computer 
> > modern normal font...  I'm using R v1.8.1 on Windows XP, 
> and I get the 
> > same result if I build the pdf using Adobe Acrobat or using MikTeX
> > 
> > The labels:
> > 
> > grpNames<-c(expression(paste("lo ", italic(d[n]))), 
> > expression(italic(d[n])), expression(paste("hi ", italic(d[n]))),
> >             expression(paste("lo ", italic(LM[1]))), 
> > expression(italic(LM[1])), expression(paste("hi ", italic(LM[1]))),
> >             expression(italic(d[n])), expression(italic(LM[1])),
> > expression(italic(LM[2])))
> > 
> > The postscript command:
> > 
> > postscript(file = "Rplots.ps",
> >            onefile = F, height=6,width=10,
> >            paper="a4", family="ComputerModern",bg="transparent",
> >            horizontal=F, pointsize=10,
> >            print.it=F)
> > 
> > Is this the best currently available, or is there some way 
> to get the 
> > correct font used when using italics?
> 
> Well, that is `building' a postscript file, not PDF: if you 
> want PDF you 
> should be using pdf().
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From david at elseware.nl  Mon Mar 15 21:41:16 2004
From: david at elseware.nl (David A. van Leeuwen)
Date: Mon, 15 Mar 2004 21:41:16 +0100
Subject: [R] effect size
Message-ID: <405614EC.1080104@elseware.nl>

Hi,

Having searched google '[R] aov effect size' without any results I 
wonder if I not completely miss something. 

Is there any R function that calculates the effect size of an AOV's main 
effect or interaction effect?  It should be related to the F's and the 
degree of freedom of the error, but the multitude in numbers in aov() 
baffle me a bit.

Thanks,

---david



From p.dalgaard at biostat.ku.dk  Mon Mar 15 21:51:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Mar 2004 21:51:17 +0100
Subject: [R] Simple numeric "as.is" question
In-Reply-To: <20040315153006.GA24469@nf034.jinr.ru>
References: <4055B7D5.4040209@mote.org> <20040315153006.GA24469@nf034.jinr.ru>
Message-ID: <x21xntu94q.fsf@biostat.ku.dk>

Timur Elzhov <Timur.Elzhov at jinr.ru> writes:

> On Mon, Mar 15, 2004 at 09:04:05AM -0500, Janet Gannon wrote:
> 
> > I am reading a list of numbers from my clipboard, and have been 
> > successful, except I can't make a histogram as R doesn't recognize my 
> > variable as numeric.  I know I need to use "as.is", but the specifics 
> > escape me. 
> > 
> > I have used x<-read.table("clipboard", header=F) to import from a txt 
> > file.  How do make this numeric?  Thanks, J.
> 
> x <- read.table("clipboard", header=F)
> x <- as.data.frame(lapply(x, as.numeric))

This is almost always wrong. If a column was erroneously converted to
a factor, it comes out as 1,2,3,4 even if the original values were
3.5,4,5.6,10.2. as.numeric(as.character(....)) is a better bet but a
bit silly for those columns that were numeric to begin with.

You might try 

lapply(x,function(x) if (is.numeric(x)) x else
        as.numeric(as.character(x)) )

but probably better is to find out *why* a variable is not read as
numeric. It's usually because an least one value is not a number.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rossini at blindglobe.net  Mon Mar 15 21:53:19 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 15 Mar 2004 12:53:19 -0800
Subject: [R] imputation of sub-threshold values
In-Reply-To: <Pine.A41.4.58.0403150730010.58628@homer03.u.washington.edu>
	(Thomas
	Lumley's message of "Mon, 15 Mar 2004 07:38:12 -0800 (PST)")
References: <NGBBKJEMOMLJFCOIEGCEAEOOJKAA.jonathan.williams@pharm.ox.ac.uk>
	<Pine.A41.4.58.0403150730010.58628@homer03.u.washington.edu>
Message-ID: <85llm1q1c0.fsf@servant.blindglobe.net>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Mon, 15 Mar 2004, Jonathan Williams wrote:
>
>> Is there a good way in R to impute values which exist,
>> but are less than the detection level for an assay?
>
>
> If there were a good way to do it, it would probably be implementable or
> implemented in R.
>
>
> If you can persuade the people measuring the values to give you the
> numbers (assuming they are just below `limit of detection' rather
> than genuine non-detects) you will reduce the need for imputation. This is
> often the most powerful technique -- analytical chemists are trained not
> to give out numbers less than some multiple of the measurement error, but
> they can often be persuaded that statisticians can be trusted with these
> numbers.
>
> If you think your data are close to one of the parametric survival models
> in the survival package, you can analyse the data as left-censored,
> without imputation. You could impute from the fitted model, if you need
> imputations.
>
> Otherwise you may be stuck with some ad hoc imputation with a sensitivity
> analysis to see how the results depend on the imputed value.

Jim Hughes, in Thomas' department, had written a paper on this for
Biometrics and implemented an approach for this problem in S-PLUS.  It
used to be on Jim's WWW site.  Not sure where that is at UW.  It was
mostly in the context of repeated assays, but probably could be
trimmed down or reimplemented.

I'm not claiming it's a good way, either.  But it does what is
requested (and similar to Thomas' comment, I'd suggest being careful
with this approach).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From andy_liaw at merck.com  Mon Mar 15 22:03:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 15 Mar 2004 16:03:19 -0500
Subject: [R] Simple numeric "as.is" question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79C0@usrymx25.merck.com>

> From: Peter Dalgaard
> Sent: Monday, March 15, 2004 3:51 PM
> To: Timur Elzhov
> Cc: r-help at stat.math.ethz.ch; Janet Gannon
> Subject: Re: [R] Simple numeric "as.is" question
> 
> 
> Timur Elzhov <Timur.Elzhov at jinr.ru> writes:
> 
> > On Mon, Mar 15, 2004 at 09:04:05AM -0500, Janet Gannon wrote:
> > 
> > > I am reading a list of numbers from my clipboard, and have been 
> > > successful, except I can't make a histogram as R doesn't 
> recognize my 
> > > variable as numeric.  I know I need to use "as.is", but 
> the specifics 
> > > escape me. 
> > > 
> > > I have used x<-read.table("clipboard", header=F) to 
> import from a txt 
> > > file.  How do make this numeric?  Thanks, J.
> > 
> > x <- read.table("clipboard", header=F)
> > x <- as.data.frame(lapply(x, as.numeric))
> 
> This is almost always wrong. If a column was erroneously converted to
> a factor, it comes out as 1,2,3,4 even if the original values were
> 3.5,4,5.6,10.2. as.numeric(as.character(....)) is a better bet but a
> bit silly for those columns that were numeric to begin with.
> 
> You might try 
> 
> lapply(x,function(x) if (is.numeric(x)) x else
>         as.numeric(as.character(x)) )
> 
> but probably better is to find out *why* a variable is not read as
> numeric. It's usually because an least one value is not a number.

... which is why I suggested use scan() instead of read.table(), which
defaults to numeric input, and would complain about non-numeric input.

Re-reading the original message, there's really not enough info to tell
what's wrong.  How does Janet know that the data read in are not numeric?
What types were they exactly?  Are there more than one variable in the
input?

Andy

 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From wmorgan at mitre.org  Mon Mar 15 22:37:08 2004
From: wmorgan at mitre.org (William T Morgan)
Date: 15 Mar 2004 16:37:08 -0500
Subject: [R] spearman rank correlation problem
Message-ID: <1079386628.17351.36.camel@paradise.mitre.org>

Hello R gurus,

I want to calculate the Spearman rho between two ranked lists. I am
getting results with cor.test that differ in comparison to my own
spearman function:

  > my.spearman
  function(l1, l2) {
    if(length(l1) != length(l2)) stop("lists must have same length")
    r1 <- rank(l1)
    r2 <- rank(l2)
    dsq <- sapply(r1-r2,function(x) x^2)
    1 - ((6 * sum(dsq)) / (length(l1) * (length(l1)^2 - 1)))
  }

Perhaps I'm doing something wrong in that code, but it's a pretty
straightforward calculation, so it's hard to see what, especially with
rank() handling the ties correctly. One example difference:

  > a
   [1]  0.112761940  0.130260949 -0.010567817 -0.411906701  0.004588443
   [6] -0.034337846 -0.148082981 -0.243724351  0.186690390  0.408983820
  > b
   [1]  8 13 14 15  5  7  8  2 19 19
  > cor.test(a,b,method="spearman")

  	Spearman's rank correlation rho

  data:  a and b 
  S = 85, p-value = 0.1544
  alternative hypothesis: true rho is not equal to 0 
  sample estimates:
        rho 
  0.4878139 

  Warning message: 
  p-values may be incorrect due to ties in: cor.test.default(a, b, method = "spearman") 
  > my.spearman(a,b)
  [1] 0.4909091

Which, as you can see, isn't quite the same. And also:

  > c
   [1] 0 0 0 0 0 0 0 0 0 0
  > cor.test(a,c,method="spearman")

  	Spearman's rank correlation rho

  data:  a and c 
  S = NA, p-value = NA
  alternative hypothesis: true rho is not equal to 0 
  sample estimates:
  rho 
   NA 

  Warning message: 
  The standard deviation is zero in: cor(x, y, na.method, method == "kendall") 
  > my.spearman(a,c)
  [1] 0.5

Any suggestions as to what I'm doing wrong?

Thanks in advance,

-- 
William Morgan
wmorgan at mitre dot org



From p.dalgaard at biostat.ku.dk  Tue Mar 16 00:22:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Mar 2004 00:22:04 +0100
Subject: [R] make check on Solaris 8 fails due to plot
In-Reply-To: <ODEPICOHNDBJEHIFCIMPAEENCBAA.ramasamy@cancer.org.uk>
References: <ODEPICOHNDBJEHIFCIMPAEENCBAA.ramasamy@cancer.org.uk>
Message-ID: <x2oeqxsnkz.fsf@biostat.ku.dk>

"Adaikalavan Ramasamy" <ramasamy at cancer.org.uk> writes:

> Dear all,
> 
> I am having trouble trying to install R-1.8.1 on a Sun Solaris 8
> (Generic_108528-23 version) machine. The configuration was successful but
> make check fails. I traced the the problem to the plot() function.
> 
> > 1 + 1
> [1] 2
> > capabilities()
>     jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
>     TRUE    FALSE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
>   libxml     fifo   cledit  IEEE754    bzip2     PCRE
>     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
> > plot( 1:10 )
> Bus Error
> 
> and R terminates to the shell prompt. I have searched the archives with no
> success. Can anyone kindly help me fix the problem or at least show me how
> to get more information about the error.

AFAIR, some gcc versions could cause this due to incorrect code
generation. (See the platform notes in the Admin manual - sec'n B.7.3
to be specific.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Murray.Keir at macquarie.com  Tue Mar 16 00:45:51 2004
From: Murray.Keir at macquarie.com (Murray Keir)
Date: Tue, 16 Mar 2004 10:45:51 +1100
Subject: [R] A beginners question
Message-ID: <0638B6AA309DA441AA45CC45DB77954602E998B6@nt_syd_exm02.pc.internal.macquarie.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/8c61e03d/attachment.pl

From phddas at yahoo.com  Tue Mar 16 04:20:27 2004
From: phddas at yahoo.com (Fred J.)
Date: Mon, 15 Mar 2004 19:20:27 -0800 (PST)
Subject: [R] rate of change
Message-ID: <20040316032027.64446.qmail@web20504.mail.yahoo.com>

Hello
I am wondering, how do I find if R has a certain
funciton to do a given task. do I just type
help.search("rate").
I am just trying to find a function to calculate the
rate of change for a variable. I could come up with
one if there isn't any allready builtin.

thanks



From ok at cs.otago.ac.nz  Tue Mar 16 05:53:57 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 16 Mar 2004 17:53:57 +1300 (NZDT)
Subject: [R] How to fit a classification model in R?
Message-ID: <200403160453.i2G4rvT9178723@atlas.otago.ac.nz>

I have about 20 000 cases with discrete variables (some are counts,
some are factors).  I'm interested in fitting a series of models
outcome ~ 1
outcome ~ sex
outcome ~ sex + age
outcome ~ age * sex
outcome ~ age * sex + location
...

I do NOT expect to get any statistical significance out of this at all;
it's purely exploratory (this is a small sample of the full data set).

I'm trying multinom (present continuous because it is not a fast method);
what are recommended ways of doing it?

As part of this question, what would fitting a "Naive Bayes" model look
like in R?



From pauljohn at ku.edu  Tue Mar 16 07:00:34 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Tue, 16 Mar 2004 00:00:34 -0600
Subject: [R] glm questions
Message-ID: <40569802.4060509@ku.edu>

Greetings, everybody. Can I ask some glm questions?

1. How do you find out -2*lnL(saturated model)?

In the output from glm, I find:

 Null deviance:  which I think is  -2[lnL(null) - lnL(saturated)]
 Residual deviance:   -2[lnL(fitted) - lnL(saturated)]

The Null model is the one that includes the constant only (plus offset 
if specified). Right?

I can use the Null and Residual deviance to calculate the "usual model 
Chi-squared" statistic
-2[lnL(null) - lnL(fitted)].

But, just for curiosity's sake, what't the saturated model's -2lnL ?

2. Why no 'scaled deviance' in output?  Or, how are you supposed to tell 
if there is over-dispersion?
I just checked andSAS gives us the scaled and nonscaled deviance. 

I have read the Venables & Ripley (MASS 4ed) chapter on GLM . I believe 
I understand the cautionary point about overdispersion toward the end 
(p. 408).  Since I'm comparing lots of other books at the moment, I 
believe I see people using the practice that is being criticized.  The 
Pearson Chi-square based estimate of dispersion is recommended and one 
uses an F test to decide if the fitted model is significantly worse than 
the saturated model.  But don't we still assess over-dispersion by 
looking at the scaled deviance (after it is calculated properly)?

Can I make a guess why glm does not report scaled deviance?  Are the glm 
authors trying to discourage us from making the lazy assessment in which 
one concludes over-dispersion is present if the scaled deviance exceeds 
the degrees of freedom?

3. When I run "example(glm)" at the end there's a Gamma model and the 
printout says:

(Dispersion parameter for Gamma family taken to be 0.001813340) 

I don't find an estimate for the Gamma distribution's shape paremeter in 
the output.  I'm uncertain what the reported dispersion parameter refers 
to.  Its the denominator (phi) in the exponential family formula, isn't 
it? 

            y*theta - c(theta)                         
   exp [   ---------------------    - h(y,phi)     ]
                    phi


4. For GLM teaching purposes, can anybody point me at some good examples 
of GLM that do not use Normal, Poisson, Negative Binomial, and/or 
Logistic Regression?  I want to justify the effort to understand the GLM 
as a framework.  We have already in previous semesters followed the 
usual "econometric" approach in which OLS, Poisson/Count, and Logistic 
regression are treated as special cases.  Some of the students don't see 
any benefit from tackling the GLM's new notation/terminology.

What I'm lacking is some persuasive evidence that the effort to master 
the details of the GLM is worthwhile.  I could really use some data and 
reference articles that have applications of Gamma distributed (or 
exponential) variables, say, or Weibull, or whatever.

I've been dropping my course notes in this directory:

http://lark.cc.ku.edu/~pauljohn/ps909/AdvancedRegression. 

The documents GLM1 and GLM2 are pretty good theoretical surveys <patting 
self on back/>.  But  I need to work harder to justify the effort by 
providing examples. 

I'd appreciate any feedback, if you have any. And, of course, if you 
want to take these documents and use them for your own purposes, be my 
guest.

4. Is it possible to find all methods that an object inherits?

I found out by reading the source code for J Fox's car package that model.matrix() returns the X matrix of coded input variables, so one can do fun things like calculate robust standard errors and such. That's really useful, because before I found that, I was recoding up a storm to re-create the X matrix used in a model.

Is there a direct way to find a list of all the other methods that would apply to an object?

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700s



From ozric at web.de  Tue Mar 16 07:09:36 2004
From: ozric at web.de (Christian Schulz)
Date: Tue, 16 Mar 2004 07:09:36 +0100
Subject: [R] How to fit a classification model in R?
In-Reply-To: <200403160453.i2G4rvT9178723@atlas.otago.ac.nz>
References: <200403160453.i2G4rvT9178723@atlas.otago.ac.nz>
Message-ID: <200403160709.37544.ozric@web.de>

Perhaps  step(lm.object)  
helps, what use a backward elimination of attributes , 
further i remind in library(leaps)  some helpfuel things
for modellimg selection.

christian


Am Dienstag, 16. M?rz 2004 05:53 schrieb Richard A. O'Keefe:
> I have about 20 000 cases with discrete variables (some are counts,
> some are factors).  I'm interested in fitting a series of models
> outcome ~ 1
> outcome ~ sex
> outcome ~ sex + age
> outcome ~ age * sex
> outcome ~ age * sex + location
> ...
>
> I do NOT expect to get any statistical significance out of this at all;
> it's purely exploratory (this is a small sample of the full data set).
>
> I'm trying multinom (present continuous because it is not a fast method);
> what are recommended ways of doing it?
>
> As part of this question, what would fitting a "Naive Bayes" model look
> like in R?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Mar 16 08:13:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Mar 2004 07:13:18 +0000 (GMT)
Subject: [R] make check on Solaris 8 fails due to plot
In-Reply-To: <ODEPICOHNDBJEHIFCIMPAEENCBAA.ramasamy@cancer.org.uk>
Message-ID: <Pine.LNX.4.44.0403160711170.5148-100000@gannet.stats>

See the R-admin manual: this happens for certain broken versions of 
Solaris-sparc gcc.  If that is not the cause we would need to know much 
more about what you used to build R.

We do regularly test on Solaris 8, with gcc 3.3.3 and Forte 7.


On Mon, 15 Mar 2004, Adaikalavan Ramasamy wrote:

> Dear all,
> 
> I am having trouble trying to install R-1.8.1 on a Sun Solaris 8
> (Generic_108528-23 version) machine. The configuration was successful but
> make check fails. I traced the the problem to the plot() function.
> 
> > 1 + 1
> [1] 2
> > capabilities()
>     jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
>     TRUE    FALSE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
>   libxml     fifo   cledit  IEEE754    bzip2     PCRE
>     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
> > plot( 1:10 )
> Bus Error
> 
> and R terminates to the shell prompt. I have searched the archives with no
> success. Can anyone kindly help me fix the problem or at least show me how
> to get more information about the error.
> 
> Thank you.
> 
> --
> Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
> Medical Statistician
> Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
> Medical Statistics Group                Tel : 01865 226 677
> Cancer Research UK                      Fax : 01865 226 962
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thpe at hhbio.wasser.tu-dresden.de  Tue Mar 16 09:06:55 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 16 Mar 2004 09:06:55 +0100
Subject: [R] imputation of sub-threshold values [off-topic]
In-Reply-To: <Pine.A41.4.58.0403150730010.58628@homer03.u.washington.edu>
References: <NGBBKJEMOMLJFCOIEGCEAEOOJKAA.jonathan.williams@pharm.ox.ac.uk>
	<Pine.A41.4.58.0403150730010.58628@homer03.u.washington.edu>
Message-ID: <4056B59F.9080704@hhbio.wasser.tu-dresden.de>

Thomas Lumley wrote:

[...]

> If you can persuade the people measuring the values to give you the 
> numbers (assuming they are just below `limit of detection' rather 
> than genuine non-detects) you will reduce the need for imputation.
> This is often the most powerful technique -- analytical chemists are
> trained not to give out numbers less than some multiple of the
> measurement error, but they can often be persuaded that statisticians
> can be trusted with these numbers.

[...]

Dear Thomas and dear R people,

thank you very much for these words. Similar discussions are frequently 
observed with people of the "applied side". I repeatedly point out, that 
scientific data should include also values lower than the "measurement 
limit", but in most cases they do are not willing, sometimes because of 
legal reasons and in other cases simply, because someone told them, that 
this is would be an unsound behaviour. Are there citable references, I 
can use as antidote?

Thomas P.



From ripley at stats.ox.ac.uk  Tue Mar 16 09:24:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Mar 2004 08:24:35 +0000 (GMT)
Subject: [R] effect size
In-Reply-To: <405614EC.1080104@elseware.nl>
Message-ID: <Pine.LNX.4.44.0403160822020.5148-100000@gannet.stats>

I think you want to call summary.lm on the aov object, but this depends on 
what you mean by `effect size'.

For example, R does have an effects() function, and that might be what you 
want.

On Mon, 15 Mar 2004, David A. van Leeuwen wrote:

> Having searched google '[R] aov effect size' without any results I 
> wonder if I not completely miss something. 
> 
> Is there any R function that calculates the effect size of an AOV's main 
> effect or interaction effect?  It should be related to the F's and the 
> degree of freedom of the error, but the multitude in numbers in aov() 
> baffle me a bit.

Given that, I wonder if you are used to standard terminology.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue Mar 16 10:25:49 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 16 Mar 2004 10:25:49 +0100
Subject: [R] spearman rank correlation problem
In-Reply-To: <1079386628.17351.36.camel@paradise.mitre.org>
References: <1079386628.17351.36.camel@paradise.mitre.org>
Message-ID: <16470.51229.281145.507762@gargle.gargle.HOWL>

>>>>> "William" == William T Morgan <wmorgan at mitre.org>
>>>>>     on 15 Mar 2004 16:37:08 -0500 writes:

    William> Hello R gurus,
    William> I want to calculate the Spearman rho between two ranked lists. I am
    William> getting results with cor.test that differ in comparison to my own
    William> spearman function:

    >> my.spearman
    William> function(l1, l2) {
    William>   if(length(l1) != length(l2)) stop("lists must have same length")
    William>   r1 <- rank(l1)
    William>   r2 <- rank(l2)
    William>   dsq <- sapply(r1-r2,function(x) x^2)
    William>   1 - ((6 * sum(dsq)) / (length(l1) * (length(l1)^2 - 1)))
    William> }

    William> Perhaps I'm doing something wrong in that code, but it's a pretty
    William> straightforward calculation, so it's hard to see what, especially with
    William> rank() handling the ties correctly. 

Well, the "ties" in your example are really the "problem".
The formula you use,  
    1 - 6 S(d^2) / (n^3 - n)    ( d = r1 - r2 ; r{12} := rank(x{12}) )
is only equal to the more natural definition,  
cor(r1, r2),  in the situation when there are no ties
[plus in a few "lucky" situations with ties].

cor.test() and now  cor(*, method = "spearman")  in R have always used
the correlation of the ranks.
It seems that this needs to be documented, since you are right,
the "1 - 6 S / (..)"  formula is also in use as *definition* for
Spearman's rank correlation.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ahenningsen at email.uni-kiel.de  Tue Mar 16 10:39:39 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 16 Mar 2004 10:39:39 +0100
Subject: [R] R CMD check warning on predict.systemfit
Message-ID: <200403161039.39787.ahenningsen@email.uni-kiel.de>

Hi,

I added a new function "predict.systemfit" to our package "systemfit" to make 
it closer to other packages (e.g. lm). Now "R CMD check" complains that the 
generic function "predict" has only the argument "object", while our function 
"predict.systemfit" has more arguments. However, the function "predict.lm" 
has also more arguments and they are almost the same as in 
"predict.systemfit". Thus, I think that our way to specify 
"predict.systemfit" might be OK in spite of this warning. 
What should I do? Can I ignore this warning? 
What will Kurt answer when we submit it ;-) ?

Best wishes,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ligges at statistik.uni-dortmund.de  Tue Mar 16 11:06:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Mar 2004 11:06:43 +0100
Subject: [R] R CMD check warning on predict.systemfit
In-Reply-To: <200403161039.39787.ahenningsen@email.uni-kiel.de>
References: <200403161039.39787.ahenningsen@email.uni-kiel.de>
Message-ID: <4056D1B3.4020804@statistik.uni-dortmund.de>

Arne Henningsen wrote:

> Hi,
> 
> I added a new function "predict.systemfit" to our package "systemfit" to make 
> it closer to other packages (e.g. lm). Now "R CMD check" complains that the 
> generic function "predict" has only the argument "object", while our function 
> "predict.systemfit" has more arguments. However, the function "predict.lm" 
> has also more arguments and they are almost the same as in 
> "predict.systemfit". Thus, I think that our way to specify 
> "predict.systemfit" might be OK in spite of this warning. 
> What should I do? Can I ignore this warning? 
> What will Kurt answer when we submit it ;-) ?
> 
> Best wishes,
> Arne
> 

Well, you must use "object" and "..." as arguments, but you are allowed 
to use further arguments. That should not result in a warning.
How does your function looks like exactly?

Uwe Ligges



From paolo.bulla at unibocconi.it  Tue Mar 16 11:12:03 2004
From: paolo.bulla at unibocconi.it (Paolo Bulla)
Date: Tue, 16 Mar 2004 11:12:03 +0100
Subject: [R] multiple summation
Message-ID: <1079431923.4056d2f32959c@webmail.uni-bocconi.it>


Hello,
I have to compute a multiple summation (not an integration because the 
independent variables a
are discrete) for all the values of a function of several variables f
(x_1,...,x_n), that is

sum ... sum f(x_1,...,x_n)
x_1     x_n

have you some suggestion? Is it possible?
I know that for multiple integration there is the function adapt, but it has at 
most n=20. In my case n depends on the dimension of the dataset and, hence, it 
could be bigger.

Thank you,
Paolo



From ahenningsen at email.uni-kiel.de  Tue Mar 16 11:33:57 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 16 Mar 2004 11:33:57 +0100
Subject: [R] R CMD check warning on predict.systemfit
In-Reply-To: <1079432090.7665.17.camel@biol102145.oulu.fi>
References: <200403161039.39787.ahenningsen@email.uni-kiel.de>
	<1079432090.7665.17.camel@biol102145.oulu.fi>
Message-ID: <200403161133.57219.ahenningsen@email.uni-kiel.de>

Hi,

thanks, Uwe and Jari, for your helpful comments! Indeed, I forgot the "..." 
argument. R did not exactly say what was wrong. It just said:

* checking S3 generic/method consistency ... WARNING
predict:
  function(object, ...)
predict.systemfit:
  function(object, data, se.fit, se.pred, interval, level)

and I wrongly assumed that "data, se.fit, se.pred, interval" shouldn't be 
there rather than that "..." was missing.

Thanks again,
Arne

On Tuesday 16 March 2004 11:14, Jari Oksanen wrote:
> Arne,
>
> Are you sure that R warns about extra variables, or does it warn about
> missing parameter "..."?
>
> The syntax of the generic is
>
> predict(object, ...)
>
> and both these should be in your function. You should have "..." even if
> you do not pass any extra parameters to other functions (been there,
> seen that).
>
> cheers, jari oksanen
>
> On Tue, 2004-03-16 at 11:39, Arne Henningsen wrote:
> > Hi,
> >
> > I added a new function "predict.systemfit" to our package "systemfit" to
> > make it closer to other packages (e.g. lm). Now "R CMD check" complains
> > that the generic function "predict" has only the argument "object", while
> > our function "predict.systemfit" has more arguments. However, the
> > function "predict.lm" has also more arguments and they are almost the
> > same as in
> > "predict.systemfit". Thus, I think that our way to specify
> > "predict.systemfit" might be OK in spite of this warning.
> > What should I do? Can I ignore this warning?
> > What will Kurt answer when we submit it ;-) ?
> >
> > Best wishes,
> > Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From j.fu at cs.rug.nl  Tue Mar 16 11:34:21 2004
From: j.fu at cs.rug.nl (J Fu)
Date: Tue, 16 Mar 2004 11:34:21 +0100
Subject: [R] lme(nlme) error message
Message-ID: <000801c40b42$3e81af50$a88f7d81@129125143168>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/b9be5b03/attachment.pl

From JensHenrik.Badsberg at agrsci.dk  Tue Mar 16 11:37:02 2004
From: JensHenrik.Badsberg at agrsci.dk (Jens Henrik Badsberg)
Date: Tue, 16 Mar 2004 11:37:02 +0100
Subject: [R] Documentation on how to put classes and methods in packages
	with namespace?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAECECD506@DJFPOST01.djf.agrsci.dk>


 Documentation on how to put classes and methods in packages with namespace?

 How should I define my classes and methods in "dynamicGraph"???

 (That is - Can some one point me to the documentation on
 how to put classes and methods in packages with a namespace? )

 Currently it is done by the code below.

 This gives problems, when a workspace with a "dynmaicGraph" is restored:

 1. run:

     local({a <- CRAN.packages()
     install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
     search()
     library(dynamicGraph)
     a <- DynamicGraph(1:10)
     search()
     ls(pos=3, all.names=TRUE)
     history()
     q(save="yes")

 2. run:

     search()
     ls()
     library(dynamicGraph)
     search()
     ls(pos=3, all.names=TRUE)
     b <- DynamicGraph(1:10)
     history()

 The classes and methods of dynamicGraph is not assigned, see the attached "run2.txt",
 and compare with the list of the objects of dynamicGraph in "run1.txt".

 I have experimented with the argument "where" to setClass, etc., see the attached
 "onLoad.R".

 Regards,

 Jens Henrik Badsberg



".onAttach" <-
function (lib, pkg) 
{
    require(tcltk)
}
".onLoad" <-
function (lib, pkg) 
{
    .onLoad.dynamicGraph()
}
".onLoad.dynamicGraph" <-
function () 
{
    library(methods)
    setClass("GraphLatticeProto", representation(vertices = "list", 
        blocks = "list", blockTree = "list", graphs = "list"))
    setClass("CanvasProto", representation(top = "tkwin", canvas = "tkwin", 
        tags = "list", id = "numeric", visibleVertices = "numeric", 
        graphEdges = "list", blockEdges = "list", factorVertices = "list", 
        factorEdges = "list"))
    setClass("NodeProto", representation(color = "character", 
        label = "character", label.position = "numeric"), prototype(color = "black", 
        label = "Label", label.position = c(0, 0, 0)))

    ### .... Many lines deleted ....

    if (!isGeneric("label")) {
        if (is.function("label")) 
            fun <- label
        else fun <- function(object) standardGeneric("label")
        setGeneric("label", fun)
    }
    setMethod("label", "NodeProto", function(object) object at label)
    setGeneric("label<-", function(x, value) standardGeneric("label<-"))
    setReplaceMethod("label", "NodeProto", function(x, value) {
        x at label <- value
        x
    })

    ### .... Many lines deleted ....

}
 <<run1.txt>>  <<run2.txt>>  <<onLoad.R>> 
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: run1.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/9729437f/run1.txt
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: run2.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/9729437f/run2.txt

From saavedra at itc.nl  Tue Mar 16 12:27:40 2004
From: saavedra at itc.nl (Carlos Saavedra)
Date: Tue, 16 Mar 2004 12:27:40 +0100
Subject: [R] graphical interface
Message-ID: <EF2DA3EEE4E2D31199710008C7EBFF1903FB87DD@itcnt09.itc.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/e59493f6/attachment.pl

From ripley at stats.ox.ac.uk  Tue Mar 16 12:37:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Mar 2004 11:37:15 +0000 (GMT)
Subject: [R] Documentation on how to put classes and methods in packages
	with namespace?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAECECD506@DJFPOST01.djf.agrsci.dk>
Message-ID: <Pine.LNX.4.44.0403161134300.8633-100000@gannet.stats>

See `Writing R Extensions' in 1.9.0 alpha and the stats4 package there as 
an example.  At this point you probably only want to check a package 
against 1.9.0 alpha, anyway.

On Tue, 16 Mar 2004, Jens Henrik Badsberg wrote:

> 
>  Documentation on how to put classes and methods in packages with namespace?
> 
>  How should I define my classes and methods in "dynamicGraph"???
> 
>  (That is - Can some one point me to the documentation on
>  how to put classes and methods in packages with a namespace? )

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From prechelt at pcpool.mi.fu-berlin.de  Tue Mar 16 12:44:18 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Tue, 16 Mar 2004 12:44:18 +0100
Subject: [R] Terminology and canonical statistical user literature
Message-ID: <85D25331FFB7AE4C900EA467D4ADA39201211F@circle.pcpool.mi.fu-berlin.de>

Brian Ripley wrote (to somebody asking about "effect sizes"):
> ...
> Given that, I wonder if you are used to standard terminology.

Good point. But I think for many of us there is more behind that.

I personally belong to an (apparently fairly large) group of
R users who may be enthusiastic, but are statistical laymen
due to a lack of formal education in the area.

The half-knowledge that I have is often sufficient to see that 
many otherwise nice sources of statistical knowledge are 
dangerously incomplete when it comes to explaining the
preconditions required for applying a certain technique
(One example: The extensive NIST handbook at
 http://www.itl.nist.gov/div898/handbook/
 fails to mention that the Wilcoxon rank sum test assumes a
 continuous distribution underlying the sample)
This is not to speak of how to correctly interpret the results.

My situation is this:
- I often have a hard time understanding the R documentation 
due to lack of background.
- I am not in a position to obtain a full background like 
a statistics student would get it.
- I am very interested in carefully checking/validating my 
application of statistical techniques.
- I cannot usually get a consulting statistician to help me.

My question:
Could some of the R gurus maybe agree on a book 
(or very small set of books) with the following properties?:
- explains typical approaches of statistical analysis
  (like MASS, but not as condensed)
- carefully describes preconditions, how to check them,
  robustness if they are violated, interpretation of results
- avoids explaining the innards of the techniques
  (and generally uses the perspective of the computer age)
- uses terminology that is easily mapped to R

If yes, I would be very interested in seeing this list.

I understand that one book cannot cover it all,
but maybe there is at least something like "CAS-"
(Conservative Applied Statistics without S) that
is of this type?  :-)

  Lutz Prechelt

Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
Institut f?r Informatik; Freie Universit?t Berlin
Takustr. 9; 14195 Berlin; Germany
+49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/



From Brandon.J.Whitcher at gsk.com  Tue Mar 16 12:56:07 2004
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher@gsk.com)
Date: Tue, 16 Mar 2004 11:56:07 +0000
Subject: [R] R install/configure problem on Sun Solaris
Message-ID: <OFBCC755B8.2228F740-ON80256E59.00413B90@sb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/679d8dc4/attachment.pl

From d.firth at warwick.ac.uk  Tue Mar 16 13:11:45 2004
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 16 Mar 2004 12:11:45 +0000
Subject: [R] glm questions
In-Reply-To: <40569802.4060509@ku.edu>
Message-ID: <17C7D966-7743-11D8-A7C1-000A95A6625E@warwick.ac.uk>

Dear Paul

Here are some attempts at your questions.  I hope it's of some help.

On Tuesday, Mar 16, 2004, at 06:00 Europe/London, Paul Johnson wrote:

> Greetings, everybody. Can I ask some glm questions?
>
> 1. How do you find out -2*lnL(saturated model)?
>
> In the output from glm, I find:
>
> Null deviance:  which I think is  -2[lnL(null) - lnL(saturated)]
> Residual deviance:   -2[lnL(fitted) - lnL(saturated)]
>
> The Null model is the one that includes the constant only (plus offset 
> if specified). Right?
>
> I can use the Null and Residual deviance to calculate the "usual model 
> Chi-squared" statistic
> -2[lnL(null) - lnL(fitted)].
>
> But, just for curiosity's sake, what't the saturated model's -2lnL ?

It's important to remember that lnL is defined only up to an additive 
constant.  For example a Poisson model has lnL contributions -mu + 
y*log(mu) + constant, and the constant is arbitrary.  The differencing 
in the deviance calculation eliminates it.  What constant would you 
like to use??

>
> 2. Why no 'scaled deviance' in output?  Or, how are you supposed to 
> tell if there is over-dispersion?
> I just checked andSAS gives us the scaled and nonscaled deviance.
> I have read the Venables & Ripley (MASS 4ed) chapter on GLM . I 
> believe I understand the cautionary point about overdispersion toward 
> the end (p. 408).  Since I'm comparing lots of other books at the 
> moment, I believe I see people using the practice that is being 
> criticized.  The Pearson Chi-square based estimate of dispersion is 
> recommended and one uses an F test to decide if the fitted model is 
> significantly worse than the saturated model.  But don't we still 
> assess over-dispersion by looking at the scaled deviance (after it is 
> calculated properly)?
>
> Can I make a guess why glm does not report scaled deviance?  Are the 
> glm authors trying to discourage us from making the lazy assessment in 
> which one concludes over-dispersion is present if the scaled deviance 
> exceeds the degrees of freedom?

I am unclear what you are asking here.  I assume by "scaled deviance" 
you mean deviance divided by phi, a (known) scale parameter?  (I'm 
sorry, I don't know SAS's definition.)    In many applications (eg 
binomial, Poisson) deviance and scaled deviance are the same thing, 
since phi is 1.  Yes, if you wanted to judge overdispersion relative to 
some other value of phi you would scale the deviance.  What other value 
of phi would you like?

>
> 3. When I run "example(glm)" at the end there's a Gamma model and the 
> printout says:
>
> (Dispersion parameter for Gamma family taken to be 0.001813340)
> I don't find an estimate for the Gamma distribution's shape paremeter 
> in the output.  I'm uncertain what the reported dispersion parameter 
> refers to.  Its the denominator (phi) in the exponential family 
> formula, isn't it?
>            y*theta - c(theta)                           exp [   
> ---------------------    - h(y,phi)     ]
>                    phi
>

Phi is the coefficient of variation, ie variance/(mean^2).  Thus it is 
a shape parameter.  If you are used to some other parameterization of 
the gamma family, just express the mean and variance in that 
parameterization to see the relation between your parameters and phi.

>
> 4. For GLM teaching purposes, can anybody point me at some good 
> examples of GLM that do not use Normal, Poisson, Negative Binomial, 
> and/or Logistic Regression?  I want to justify the effort to 
> understand the GLM as a framework.  We have already in previous 
> semesters followed the usual "econometric" approach in which OLS, 
> Poisson/Count, and Logistic regression are treated as special cases.  
> Some of the students don't see any benefit from tackling the GLM's new 
> notation/terminology.

McCullagh and Nelder (1989) has some I believe, eg gamma models.  Also 
quasi-likelihood models, such as the Wedderburn (1974) approach to 
analysis of 2-component compositional data (the leaf blotch example in 
McC&N).

On the more general point: yes, if all that students need to know is 
OLS, Poisson rate models and logistic regression, then GLM is overkill. 
  The point, surely, is that GLM opens up a way of thinking in which 
mean function and variance function are specified separately?  This 
becomes most clear through a presentation of GLMs via quasi-likelihood 
(as a the "right" generalization of weighted least squares) rather than 
via the exponential-family likelihoods.  In my opinion.

>
> 4. Is it possible to find all methods that an object inherits?
>
> I found out by reading the source code for J Fox's car package that 
> model.matrix() returns the X matrix of coded input variables, so one 
> can do fun things like calculate robust standard errors and such. 
> That's really useful, because before I found that, I was recoding up a 
> storm to re-create the X matrix used in a model.
>
> Is there a direct way to find a list of all the other methods that 
> would apply to an object?

methods(class="glm")
methods(class="lm")

is probably not as direct as you had in mind!  But it's a start.

Best wishes,
David



From Friedrich.Leisch at ci.tuwien.ac.at  Tue Mar 16 13:10:43 2004
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Tue, 16 Mar 2004 13:10:43 +0100
Subject: [R] Sweave and R output: possible to suppress "Schunk" tags in
	*.tex file output?
In-Reply-To: <62AE0CF1D4875C4BBDEC29DB9924ACE87F21AE@pnlmse25.pnl.gov>
References: <62AE0CF1D4875C4BBDEC29DB9924ACE87F21AE@pnlmse25.pnl.gov>
Message-ID: <16470.61123.522584.990023@galadriel.ci.tuwien.ac.at>

>>>>> On Fri, 12 Mar 2004 08:38:53 -0800,
>>>>> Waichler, Scott R (WSR) wrote:

  > Hello,
  > I would like to fill the rows of a Latex tabular environment with output from
  > R, as in

  > \begin{table}
  >   \caption{Table caption.} 
  >   \label{tab:events}
  >   \begin{tabular}{c r r r r r}
  >   \hline

  > <<echo=false,results=tex>>=
  > fill.my.table.rows()
  > @

  >   \end{tabular}
  > \end{table}

  > Sweave produces the output inside \begin{Schunk} and \end{Schunk} commands,
  > which latex doesn't tolerate within a table.  I would prefer to avoid writing all of
  > the Latex table code from R.  Is my only other option to produce a separate
  > text file, e.g. under Sweave with results=hide, then use \input{}
  > on the file?

Oops, that's a bug in Sweave ... the Schunk shouldn't be there in that
case. I will fix it for 1.9.0.

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ligges at statistik.uni-dortmund.de  Tue Mar 16 13:51:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Mar 2004 13:51:27 +0100
Subject: [R] Documentation on how to put classes and methods in packages
	with namespace?
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAECECD506@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAECECD506@DJFPOST01.djf.agrsci.dk>
Message-ID: <4056F84F.9040709@statistik.uni-dortmund.de>

Jens Henrik Badsberg wrote:

>  Documentation on how to put classes and methods in packages with namespace?
> 
>  How should I define my classes and methods in "dynamicGraph"???


Jens Henrik,

without reading all the lines below, I'd like to point you to the 
"Writing R Extensions" manual of R-devel (1.9.0 alpha), which includes 
some lines on handling S4 classes/methods in Namespaces.

Uwe




>  (That is - Can some one point me to the documentation on
>  how to put classes and methods in packages with a namespace? )
> 
>  Currently it is done by the code below.
> 
>  This gives problems, when a workspace with a "dynmaicGraph" is restored:
> 
>  1. run:
> 
>      local({a <- CRAN.packages()
>      install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
>      search()
>      library(dynamicGraph)
>      a <- DynamicGraph(1:10)
>      search()
>      ls(pos=3, all.names=TRUE)
>      history()
>      q(save="yes")
> 
>  2. run:
> 
>      search()
>      ls()
>      library(dynamicGraph)
>      search()
>      ls(pos=3, all.names=TRUE)
>      b <- DynamicGraph(1:10)
>      history()
> 
>  The classes and methods of dynamicGraph is not assigned, see the attached "run2.txt",
>  and compare with the list of the objects of dynamicGraph in "run1.txt".
> 
>  I have experimented with the argument "where" to setClass, etc., see the attached
>  "onLoad.R".
> 
>  Regards,
> 
>  Jens Henrik Badsberg
> 
> 
> 
> ".onAttach" <-
> function (lib, pkg) 
> {
>     require(tcltk)
> }
> ".onLoad" <-
> function (lib, pkg) 
> {
>     .onLoad.dynamicGraph()
> }
> ".onLoad.dynamicGraph" <-
> function () 
> {
>     library(methods)
>     setClass("GraphLatticeProto", representation(vertices = "list", 
>         blocks = "list", blockTree = "list", graphs = "list"))
>     setClass("CanvasProto", representation(top = "tkwin", canvas = "tkwin", 
>         tags = "list", id = "numeric", visibleVertices = "numeric", 
>         graphEdges = "list", blockEdges = "list", factorVertices = "list", 
>         factorEdges = "list"))
>     setClass("NodeProto", representation(color = "character", 
>         label = "character", label.position = "numeric"), prototype(color = "black", 
>         label = "Label", label.position = c(0, 0, 0)))
> 
>     ### .... Many lines deleted ....
> 
>     if (!isGeneric("label")) {
>         if (is.function("label")) 
>             fun <- label
>         else fun <- function(object) standardGeneric("label")
>         setGeneric("label", fun)
>     }
>     setMethod("label", "NodeProto", function(object) object at label)
>     setGeneric("label<-", function(x, value) standardGeneric("label<-"))
>     setReplaceMethod("label", "NodeProto", function(x, value) {
>         x at label <- value
>         x
>     })
> 
>     ### .... Many lines deleted ....
> 
> }
>  <<run1.txt>>  <<run2.txt>>  <<onLoad.R>> 
> 
> 
> ------------------------------------------------------------------------
> 
> 
> 
> local({a <- CRAN.packages()
> install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
> search()
> library(dynamicGraph)
> a <- DynamicGraph(1:10)
> search()
> ls(pos=3, all.names=TRUE)
> history()
> q(save="yes")
> 
> 
> 
> R : Copyright 2003, The R Foundation for Statistical Computing
> Version 1.8.1  (2003-11-21), ISBN 3-900051-00-3
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> 
>>local({a <- CRAN.packages()
> 
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 16234 bytes
> opened URL
> downloaded 15Kb
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.8/dynamicGraph_0.1.5.9.9.8.8.zip'
> Content type `application/zip' length 347067 bytes
> opened URL
> downloaded 338Kb
> 
> package  dynamicGraph  successfully unpacked and MD5 sums checked
> 
> Delete downloaded files (y/N)? y
> 
> updating HTML package descriptions
> 
>>search()
> 
> [1] ".GlobalEnv"      "package:methods" "package:ctest"   "package:mva"    
> [5] "package:modreg"  "package:nls"     "package:ts"      "Autoloads"      
> [9] "package:base"   
> 
>>library(dynamicGraph)
> 
> Loading required package: tcltk 
> 
>>a <- DynamicGraph(1:10)
>>search()
> 
>  [1] ".GlobalEnv"           "package:tcltk"        "package:dynamicGraph"
>  [4] "package:methods"      "package:ctest"        "package:mva"         
>  [7] "package:modreg"       "package:nls"          "package:ts"          
> [10] "Autoloads"            "package:base"        
> 
>>ls(pos=3, all.names=TRUE)
> 
>   [1] ".__C__BlockEdgeProto"                   ".__C__BlockProto"                      
>   [3] ".__C__CanvasProto"                      ".__C__ContinuousVertexProto"           
>   [5] ".__C__defaultModelObjectProto"          ".__C__defaultTestObjectProto"          
>   [7] ".__C__DiscreteGeneratorProto"           ".__C__DiscreteVertexProto"             
>   [9] ".__C__EdgeProto"                        ".__C__FactorEdgeProto"                 
>  [11] ".__C__FactorVertexProto"                ".__C__GeneratorProto"                  
>  [13] ".__C__GraphLatticeProto"                ".__C__LinearGeneratorProto"            
>  [15] ".__C__NodeProto"                        ".__C__OrdinalVertexProto"              
>  [17] ".__C__QuadraticGeneratorProto"          ".__C__VertexEdgeProto"                 
>  [19] ".__C__VertexProto"                      ".__M__addToPopups:dynamicGraph"        
>  [21] ".__M__ancestors:dynamicGraph"           ".__M__ancestors<-:dynamicGraph"        
>  [23] ".__M__color:dynamicGraph"               ".__M__color<-:dynamicGraph"            
>  [25] ".__M__Colors:dynamicGraph"              ".__M__Colors<-:dynamicGraph"           
>  [27] ".__M__descendants:dynamicGraph"         ".__M__descendants<-:dynamicGraph"      
>  [29] ".__M__draw:dynamicGraph"                ".__M__edgeLabel:dynamicGraph"          
>  [31] ".__M__edgeLabel<-:dynamicGraph"         ".__M__edgeWidth:dynamicGraph"          
>  [33] ".__M__edgeWidth<-:dynamicGraph"         ".__M__index:dynamicGraph"              
>  [35] ".__M__index<-:dynamicGraph"             ".__M__Indices:dynamicGraph"            
>  [37] ".__M__label:dynamicGraph"               ".__M__label<-:dynamicGraph"            
>  [39] ".__M__labelOfEdge:dynamicGraph"         ".__M__labelOfEdge<-:dynamicGraph"      
>  [41] ".__M__labelPosition:dynamicGraph"       ".__M__labelPosition<-:dynamicGraph"    
>  [43] ".__M__LabelPositions:dynamicGraph"      ".__M__LabelPositions<-:dynamicGraph"   
>  [45] ".__M__Labels:dynamicGraph"              ".__M__Labels<-:dynamicGraph"           
>  [47] ".__M__name:dynamicGraph"                ".__M__name<-:dynamicGraph"             
>  [49] ".__M__Names:dynamicGraph"               ".__M__Names<-:dynamicGraph"            
>  [51] ".__M__NodeAncestors:dynamicGraph"       ".__M__NodeAncestors<-:dynamicGraph"    
>  [53] ".__M__NodeDescendants:dynamicGraph"     ".__M__NodeDescendants<-:dynamicGraph"  
>  [55] ".__M__nodeIndices:dynamicGraph"         ".__M__NodeIndices:dynamicGraph"        
>  [57] ".__M__nodeIndices<-:dynamicGraph"       ".__M__nodeIndicesOfEdge:dynamicGraph"  
>  [59] ".__M__nodeIndicesOfEdge<-:dynamicGraph" ".__M__NodeTypes:dynamicGraph"          
>  [61] ".__M__nodeTypesOfEdge:dynamicGraph"     ".__M__oriented:dynamicGraph"           
>  [63] ".__M__Oriented:dynamicGraph"            ".__M__oriented<-:dynamicGraph"         
>  [65] ".__M__Oriented<-:dynamicGraph"          ".__M__position:dynamicGraph"           
>  [67] ".__M__position<-:dynamicGraph"          ".__M__Positions:dynamicGraph"          
>  [69] ".__M__Positions<-:dynamicGraph"         ".__M__Strata:dynamicGraph"             
>  [71] ".__M__Strata<-:dynamicGraph"            ".__M__stratum:dynamicGraph"            
>  [73] ".__M__stratum<-:dynamicGraph"           ".__M__visible:dynamicGraph"            
>  [75] ".__M__Visible:dynamicGraph"             ".__M__visible<-:dynamicGraph"          
>  [77] ".__M__Visible<-:dynamicGraph"           ".__M__width:dynamicGraph"              
>  [79] ".__M__width<-:dynamicGraph"             ".__M__widthOfEdge:dynamicGraph"        
>  [81] ".__M__widthOfEdge<-:dynamicGraph"       ".__M__Widths:dynamicGraph"             
>  [83] ".__M__Widths<-:dynamicGraph"            ".onLoad.dynamicGraph"                  
>  [85] ".onLoad.dynamicGraphInterface"          "addToPopups"                           
>  [87] "ancestors"                              "ancestors<-"                           
>  [89] "blockTreeToList"                        "color"                                 
>  [91] "color<-"                                "Colors"                                
>  [93] "Colors<-"                               "descendants"                           
>  [95] "descendants<-"                          "draw"                                  
>  [97] "DynamicGraph"                           "dynamicGraphMain"                      
>  [99] "edgeLabel"                              "edgeLabel<-"                           
> [101] "edgeWidth"                              "edgeWidth<-"                           
> [103] "index"                                  "index<-"                               
> [105] "Indices"                                "label"                                 
> [107] "label<-"                                "labelOfEdge"                           
> [109] "labelOfEdge<-"                          "labelPosition"                         
> [111] "labelPosition<-"                        "LabelPositions"                        
> [113] "LabelPositions<-"                       "Labels"                                
> [115] "Labels<-"                               "modalDialog"                           
> [117] "name"                                   "name<-"                                
> [119] "Names"                                  "Names<-"                               
> [121] "nameToVertexIndex"                      "newBlock"                              
> [123] "newBlockEdge"                           "newDefaultModelObject"                 
> [125] "newDefaultTestObject"                   "newFactor"                             
> [127] "newFactorEdge"                          "newVertex"                             
> [129] "newVertexEdge"                          "NodeAncestors"                         
> [131] "NodeAncestors<-"                        "NodeDescendants"                       
> [133] "NodeDescendants<-"                      "nodeIndices"                           
> [135] "NodeIndices"                            "nodeIndices<-"                         
> [137] "nodeIndicesOfEdge"                      "nodeIndicesOfEdge<-"                   
> [139] "NodeTypes"                              "nodeTypesOfEdge"                       
> [141] "oriented"                               "Oriented"                              
> [143] "oriented<-"                             "Oriented<-"                            
> [145] "position"                               "position<-"                            
> [147] "Positions"                              "Positions<-"                           
> [149] "returnBlockEdgeList"                    "returnEdgeList"                        
> [151] "returnFactorEdgeList"                   "returnFactorVerticesAndEdges"          
> [153] "returnVertexList"                       "setBlocks"                             
> [155] "setTreeBlocks"                          "Strata"                                
> [157] "Strata<-"                               "stratum"                               
> [159] "stratum<-"                              "validFactorClasses"                    
> [161] "validVertexClasses"                     "visible"                               
> [163] "Visible"                                "visible<-"                             
> [165] "Visible<-"                              "width"                                 
> [167] "width<-"                                "widthOfEdge"                           
> [169] "widthOfEdge<-"                          "Widths"                                
> [171] "Widths<-"                              
> 
>>history()
>>q(save="yes")
> 
> 
> 
> ------------------------------------------------------------------------
> 
> search()
> ls()
> library(dynamicGraph)
> search()
> ls(pos=3, all.names=TRUE)
> b <- DynamicGraph(1:10)
> history()
> 
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> [Previously saved workspace restored]
> 
> 
>>search()
> 
> [1] ".GlobalEnv"      "package:ctest"   "package:mva"     "package:modreg"  "package:nls"    
> [6] "package:ts"      "package:methods" "Autoloads"       "package:base"   
> 
>>ls()
> 
> [1] "a"
> 
>>library(dynamicGraph)
> 
> Loading required package: tcltk 
> 
>>search()
> 
>  [1] ".GlobalEnv"           "package:tcltk"        "package:dynamicGraph"
>  [4] "package:ctest"        "package:mva"          "package:modreg"      
>  [7] "package:nls"          "package:ts"           "package:methods"     
> [10] "Autoloads"            "package:base"        
> 
>>ls(pos=3, all.names=TRUE)
> 
>  [1] ".onLoad.dynamicGraph"          ".onLoad.dynamicGraphInterface"
>  [3] "blockTreeToList"               "DynamicGraph"                 
>  [5] "dynamicGraphMain"              "modalDialog"                  
>  [7] "nameToVertexIndex"             "newBlock"                     
>  [9] "newBlockEdge"                  "newDefaultModelObject"        
> [11] "newDefaultTestObject"          "newFactor"                    
> [13] "newFactorEdge"                 "newVertex"                    
> [15] "newVertexEdge"                 "returnBlockEdgeList"          
> [17] "returnEdgeList"                "returnFactorEdgeList"         
> [19] "returnFactorVerticesAndEdges"  "returnVertexList"             
> [21] "setBlocks"                     "setTreeBlocks"                
> [23] "validFactorClasses"            "validVertexClasses"           
> 
>>b <- DynamicGraph(1:10)
> 
> Error in getClass(superClass) : "NodeProto" is not a defined class
> 
>>history()
>>q()
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Mar 16 13:54:17 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 16 Mar 2004 13:54:17 +0100
Subject: [R] multiple summation
In-Reply-To: <1079431923.4056d2f32959c@webmail.uni-bocconi.it>
References: <1079431923.4056d2f32959c@webmail.uni-bocconi.it>
Message-ID: <4056F8F9.7070804@statistik.uni-dortmund.de>

Paolo Bulla wrote:

> Hello,
> I have to compute a multiple summation (not an integration because the 
> independent variables a
> are discrete) for all the values of a function of several variables f
> (x_1,...,x_n), that is
> 
> sum ... sum f(x_1,...,x_n)
> x_1     x_n
> 
> have you some suggestion? Is it possible?
> I know that for multiple integration there is the function adapt, but it has at 
> most n=20. In my case n depends on the dimension of the dataset and, hence, it 
> could be bigger.
> 
> Thank you,
> Paolo


I's try to vectorizing f() and apply it on the set of all permutations.

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Tue Mar 16 13:57:30 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 16 Mar 2004 12:57:30 -0000
Subject: [R] make check on Solaris 8 fails due to plot
In-Reply-To: <Pine.LNX.4.44.0403160711170.5148-100000@gannet.stats>
Message-ID: <ODEPICOHNDBJEHIFCIMPMEFNCBAA.ramasamy@cancer.org.uk>

Many thanks to Prof.Ripley and Dr.Dalgaard. Yes, I am using gcc-3.2.2
(correctly configured for solaris 2.8) and this could be the problem.

Is there a way to fix this myself ? My machine specs are 'SunOS enterprise
5.8 Generic_108528-23 sun4u sparc SUNW,Ultra-Enterprise'.

The R-admin seems to suggest that I can solve this by redefining the CC,
FFLAGS, CXXFLAGS and LDFLAGS in the config.site file but I am not sure if I
am reading it correctly and am unable to find the config.site file.

Thanks again.

Regards, Adai.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
> Sent: 16 March 2004 07:13
> To: Adaikalavan Ramasamy
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] make check on Solaris 8 fails due to plot
>
>
> See the R-admin manual: this happens for certain broken versions of
> Solaris-sparc gcc.  If that is not the cause we would need to know much
> more about what you used to build R.
>
> We do regularly test on Solaris 8, with gcc 3.3.3 and Forte 7.
>
>
> On Mon, 15 Mar 2004, Adaikalavan Ramasamy wrote:
>
> > Dear all,
> >
> > I am having trouble trying to install R-1.8.1 on a Sun Solaris 8
> > (Generic_108528-23 version) machine. The configuration was
> successful but
> > make check fails. I traced the the problem to the plot() function.
> >
> > > 1 + 1
> > [1] 2
> > > capabilities()
> >     jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
> >     TRUE    FALSE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
> >   libxml     fifo   cledit  IEEE754    bzip2     PCRE
> >     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
> > > plot( 1:10 )
> > Bus Error
> >
> > and R terminates to the shell prompt. I have searched the
> archives with no
> > success. Can anyone kindly help me fix the problem or at least
> show me how
> > to get more information about the error.
> >
> > Thank you.
> >
> > --
> > Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
> > Medical Statistician
> > Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
> > Medical Statistics Group                Tel : 01865 226 677
> > Cancer Research UK                      Fax : 01865 226 962
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Mar 16 14:12:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Mar 2004 13:12:35 +0000 (GMT)
Subject: [R] make check on Solaris 8 fails due to plot
In-Reply-To: <ODEPICOHNDBJEHIFCIMPMEFNCBAA.ramasamy@cancer.org.uk>
Message-ID: <Pine.LNX.4.44.0403161308250.8898-100000@gannet.stats>

On Tue, 16 Mar 2004, Adaikalavan Ramasamy wrote:

> Many thanks to Prof.Ripley and Dr.Dalgaard. Yes, I am using gcc-3.2.2
> (correctly configured for solaris 2.8) and this could be the problem.

It is a problem.

> Is there a way to fix this myself ? My machine specs are 'SunOS enterprise
> 5.8 Generic_108528-23 sun4u sparc SUNW,Ultra-Enterprise'.
> 
> The R-admin seems to suggest that I can solve this by redefining the CC,
> FFLAGS, CXXFLAGS and LDFLAGS in the config.site file but I am not sure if I
> am reading it correctly and am unable to find the config.site file.

You can probably solve it by turning optimization off (via CFLAGS and
FFLAGS), but a better way to solve it is to install/build a working
compiler: gcc-3.2.2 is quite old now, too.

config.site is in the top level of the sources.

> > -----Original Message-----
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
> > Sent: 16 March 2004 07:13
> >
> > See the R-admin manual: this happens for certain broken versions of
> > Solaris-sparc gcc.  If that is not the cause we would need to know much
> > more about what you used to build R.
> >
> > We do regularly test on Solaris 8, with gcc 3.3.3 and Forte 7.
> >
> > On Mon, 15 Mar 2004, Adaikalavan Ramasamy wrote:
> >
> > > Dear all,
> > >
> > > I am having trouble trying to install R-1.8.1 on a Sun Solaris 8
> > > (Generic_108528-23 version) machine. The configuration was
> > successful but
> > > make check fails. I traced the the problem to the plot() function.
> > >
> > > > 1 + 1
> > > [1] 2
> > > > capabilities()
> > >     jpeg      png    tcltk      X11    GNOME     libz http/ftp  sockets
> > >     TRUE    FALSE     TRUE     TRUE    FALSE     TRUE     TRUE     TRUE
> > >   libxml     fifo   cledit  IEEE754    bzip2     PCRE
> > >     TRUE     TRUE     TRUE     TRUE     TRUE     TRUE
> > > > plot( 1:10 )
> > > Bus Error
> > >
> > > and R terminates to the shell prompt. I have searched the
> > archives with no
> > > success. Can anyone kindly help me fix the problem or at least
> > show me how
> > > to get more information about the error.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Mar 16 14:17:42 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Mar 2004 08:17:42 -0500
Subject: [R] rate of change
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79C5@usrymx25.merck.com>

help.search() would be a good place to start for this kind of question, but
it's unlikely to be helpful in this particular case...

When I see "rate of change", I think of derivatives.  However, it's
difficult to say much without more detail on what you are looking for.
(What exactly do you mean by rate of change of a variable?)  There are many
ways to estimate derivatives in R, but which to recommend depends on your
particular application.

Cheers,
Andy

> From: Fred J.
> 
> Hello
> I am wondering, how do I find if R has a certain
> funciton to do a given task. do I just type
> help.search("rate").
> I am just trying to find a function to calculate the
> rate of change for a variable. I could come up with
> one if there isn't any allready builtin.
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Tue Mar 16 14:19:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Mar 2004 08:19:43 -0500
Subject: [R] A beginners question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79C6@usrymx25.merck.com>

I guess you're looking for reshape()?

HTH,
Andy

> From: Murray Keir
> 
> Hi.  I used R years ago at Uni but haven't touched it since, 
> so I'm back to beginner level now.  After looking through the 
> manuals for a while I've come to the conclusion I don't know 
> where to start looking for what I'm looking for, so I decided 
> to ask for help.
> 
> I've got a fairly simple data frame with an identifier, 
> period and score.  There are a few hundred seperate 
> identifiers and the period is from 0 to 36.  I'd like to 
> pivot this so that it's a matrix of scores, with identifiers 
> as column headers and periods as row headers.  I'd then like 
> to run an ARIMA on each of the columns.
> 
> Thanks
> Murray


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From JensHenrik.Badsberg at agrsci.dk  Tue Mar 16 14:21:29 2004
From: JensHenrik.Badsberg at agrsci.dk (Jens Henrik Badsberg)
Date: Tue, 16 Mar 2004 14:21:29 +0100
Subject: SV: [R] Documentation on how to put classes and methods in
	packageswith namespace?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAECECD508@DJFPOST01.djf.agrsci.dk>


 Re. the Kings Quest game called R,

 Inspired by the "stats4" of R-1.9.0 (alpha of 2004-03-15)
 I moved the body af my function ".onLoad.dynamicGraph"
 to the end of ".../dynamicGraph/R/dynamicGraph"
 (replacing "library(methods)" by "require(methods)").

 The secound run, with [Previously saved workspace restored],
 produces the same result as in my first letter of today:

> library(dynamicGraph)
Loading required package: tcltk 
> b <- DynamicGraph(1:10)
Error in getClass(superClass) : "NodeProto" is not a defined class

 I also (again) read pages 13 and 16-17 (and 73) of
 "Writing R Extensions" of 2004-03-15,
 and the manual pages of "setClass":

 where: For 'setClass' and 'removeClass', the environment in which to
          store or remove the definition.  Defaults to the top-level
          environment of the calling function (the global environment
          for ordinary computations, but the environment or namespace
          of a package when loading that package).

          For other functions, 'where' defines where to do the search
          for the class definition, and the default is to search from
          the top-level environment or namespace of the caller to this
          function. 

 "setMethod":

   where: the database in which to store the definition of the method; 

 "isGeneric":

   where: The environment, namespace, or search-list position from
          which to search for objects.  By default, start at the
          top-level environment of the calling function, typically the
          global environment (i.e., use the search list), or the
          namespace of a package from which the call came.  It is
          important to supply this argument when calling any of these
          functions indirectly.  With package namespaces, the default
          is likely to be wrong in such calls.

 It does not help me. Maybe this should be move to "a bug report".

 Regards,

 Jens Henrik



-----Oprindelig meddelelse-----
Fra: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sendt: 16. marts 2004 12:37
Til: Jens Henrik Badsberg
Cc: r-help at stat.math.ethz.ch
Emne: Re: [R] Documentation on how to put classes and methods in
packageswith namespace?


See `Writing R Extensions' in 1.9.0 alpha and the stats4 package there as 
an example.  At this point you probably only want to check a package 
against 1.9.0 alpha, anyway.

On Tue, 16 Mar 2004, Jens Henrik Badsberg wrote:

> 
>  Documentation on how to put classes and methods in packages with namespace?
> 
>  How should I define my classes and methods in "dynamicGraph"???
> 
>  (That is - Can some one point me to the documentation on
>  how to put classes and methods in packages with a namespace? )

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From JensHenrik.Badsberg at agrsci.dk  Tue Mar 16 14:32:42 2004
From: JensHenrik.Badsberg at agrsci.dk (Jens Henrik Badsberg)
Date: Tue, 16 Mar 2004 14:32:42 +0100
Subject: SV: [R] Documentation on how to put classes and methods in
	packageswith namespace?
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAECECD509@DJFPOST01.djf.agrsci.dk>


 Please disregard the previous mail.

 "a bug report" made me test in version 1.9.0 (alpha).

 Here it works fine, e.i. without changing the version of
 "dynamicGraph" on CRAN.

 Regards,

 Jens Henrik


R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0 alpha (2004-03-15), ISBN 3-900051-00-3

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> search()
[1] ".GlobalEnv"       "package:stats"    "package:graphics" "package:utils"   
[5] "package:methods"  "Autoloads"        "package:base"    
> library(dynamicGraph)
Loading required package: tcltk 
> search()
[1] ".GlobalEnv"           "package:tcltk"        "package:dynamicGraph"
[4] "package:stats"        "package:graphics"     "package:utils"       
[7] "package:methods"      "Autoloads"            "package:base"        
> ls(pos=3, all.names=TRUE)
  [1] ".__C__BlockEdgeProto"                   ".__C__BlockProto"                      
  [3] ".__C__CanvasProto"                      ".__C__ContinuousVertexProto"           
  [5] ".__C__defaultModelObjectProto"          ".__C__defaultTestObjectProto"          
  [7] ".__C__DiscreteGeneratorProto"           ".__C__DiscreteVertexProto"             
  [9] ".__C__EdgeProto"                        ".__C__FactorEdgeProto"                 
 [11] ".__C__FactorVertexProto"                ".__C__GeneratorProto"                  
 [13] ".__C__GraphLatticeProto"                ".__C__LinearGeneratorProto"            
 [15] ".__C__NodeProto"                        ".__C__OrdinalVertexProto"              
 [17] ".__C__QuadraticGeneratorProto"          ".__C__VertexEdgeProto"                 
 [19] ".__C__VertexProto"                      ".__M__addToPopups:dynamicGraph"        
 [21] ".__M__ancestors:dynamicGraph"           ".__M__ancestors<-:dynamicGraph"        
 [23] ".__M__color:dynamicGraph"               ".__M__color<-:dynamicGraph"            
 [25] ".__M__Colors:dynamicGraph"              ".__M__Colors<-:dynamicGraph"           
 [27] ".__M__descendants:dynamicGraph"         ".__M__descendants<-:dynamicGraph"      
 [29] ".__M__draw:dynamicGraph"                ".__M__edgeLabel:dynamicGraph"          
 [31] ".__M__edgeLabel<-:dynamicGraph"         ".__M__edgeWidth:dynamicGraph"          
 [33] ".__M__edgeWidth<-:dynamicGraph"         ".__M__index:dynamicGraph"              
 [35] ".__M__index<-:dynamicGraph"             ".__M__Indices:dynamicGraph"            
 [37] ".__M__label:dynamicGraph"               ".__M__label<-:dynamicGraph"            
 [39] ".__M__labelOfEdge:dynamicGraph"         ".__M__labelOfEdge<-:dynamicGraph"      
 [41] ".__M__labelPosition:dynamicGraph"       ".__M__labelPosition<-:dynamicGraph"    
 [43] ".__M__LabelPositions:dynamicGraph"      ".__M__LabelPositions<-:dynamicGraph"   
 [45] ".__M__Labels:dynamicGraph"              ".__M__Labels<-:dynamicGraph"           
 [47] ".__M__name:dynamicGraph"                ".__M__name<-:dynamicGraph"             
 [49] ".__M__Names:dynamicGraph"               ".__M__Names<-:dynamicGraph"            
 [51] ".__M__NodeAncestors:dynamicGraph"       ".__M__NodeAncestors<-:dynamicGraph"    
 [53] ".__M__NodeDescendants:dynamicGraph"     ".__M__NodeDescendants<-:dynamicGraph"  
 [55] ".__M__nodeIndices:dynamicGraph"         ".__M__NodeIndices:dynamicGraph"        
 [57] ".__M__nodeIndices<-:dynamicGraph"       ".__M__nodeIndicesOfEdge:dynamicGraph"  
 [59] ".__M__nodeIndicesOfEdge<-:dynamicGraph" ".__M__NodeTypes:dynamicGraph"          
 [61] ".__M__nodeTypesOfEdge:dynamicGraph"     ".__M__oriented:dynamicGraph"           
 [63] ".__M__Oriented:dynamicGraph"            ".__M__oriented<-:dynamicGraph"         
 [65] ".__M__Oriented<-:dynamicGraph"          ".__M__position:dynamicGraph"           
 [67] ".__M__position<-:dynamicGraph"          ".__M__Positions:dynamicGraph"          
 [69] ".__M__Positions<-:dynamicGraph"         ".__M__Strata:dynamicGraph"             
 [71] ".__M__Strata<-:dynamicGraph"            ".__M__stratum:dynamicGraph"            
 [73] ".__M__stratum<-:dynamicGraph"           ".__M__visible:dynamicGraph"            
 [75] ".__M__Visible:dynamicGraph"             ".__M__visible<-:dynamicGraph"          
 [77] ".__M__Visible<-:dynamicGraph"           ".__M__width:dynamicGraph"              
 [79] ".__M__width<-:dynamicGraph"             ".__M__widthOfEdge:dynamicGraph"        
 [81] ".__M__widthOfEdge<-:dynamicGraph"       ".__M__Widths:dynamicGraph"             
 [83] ".__M__Widths<-:dynamicGraph"            ".onLoad.dynamicGraph"                  
 [85] ".onLoad.dynamicGraphInterface"          "addToPopups"                           
 [87] "ancestors"                              "ancestors<-"                           
 [89] "blockTreeToList"                        "color"                                 
 [91] "color<-"                                "Colors"                                
 [93] "Colors<-"                               "descendants"                           
 [95] "descendants<-"                          "draw"                                  
 [97] "DynamicGraph"                           "dynamicGraphMain"                      
 [99] "edgeLabel"                              "edgeLabel<-"                           
[101] "edgeWidth"                              "edgeWidth<-"                           
[103] "index"                                  "index<-"                               
[105] "Indices"                                "label"                                 
[107] "label<-"                                "labelOfEdge"                           
[109] "labelOfEdge<-"                          "labelPosition"                         
[111] "labelPosition<-"                        "LabelPositions"                        
[113] "LabelPositions<-"                       "Labels"                                
[115] "Labels<-"                               "modalDialog"                           
[117] "name"                                   "name<-"                                
[119] "Names"                                  "Names<-"                               
[121] "nameToVertexIndex"                      "newBlock"                              
[123] "newBlockEdge"                           "newDefaultModelObject"                 
[125] "newDefaultTestObject"                   "newFactor"                             
[127] "newFactorEdge"                          "newVertex"                             
[129] "newVertexEdge"                          "NodeAncestors"                         
[131] "NodeAncestors<-"                        "NodeDescendants"                       
[133] "NodeDescendants<-"                      "nodeIndices"                           
[135] "NodeIndices"                            "nodeIndices<-"                         
[137] "nodeIndicesOfEdge"                      "nodeIndicesOfEdge<-"                   
[139] "NodeTypes"                              "nodeTypesOfEdge"                       
[141] "oriented"                               "Oriented"                              
[143] "oriented<-"                             "Oriented<-"                            
[145] "position"                               "position<-"                            
[147] "Positions"                              "Positions<-"                           
[149] "returnBlockEdgeList"                    "returnEdgeList"                        
[151] "returnFactorEdgeList"                   "returnFactorVerticesAndEdges"          
[153] "returnVertexList"                       "setBlocks"                             
[155] "setTreeBlocks"                          "Strata"                                
[157] "Strata<-"                               "stratum"                               
[159] "stratum<-"                              "validFactorClasses"                    
[161] "validVertexClasses"                     "visible"                               
[163] "Visible"                                "visible<-"                             
[165] "Visible<-"                              "width"                                 
[167] "width<-"                                "widthOfEdge"                           
[169] "widthOfEdge<-"                          "Widths"                                
[171] "Widths<-"                              
> ls()
[1] "a"
> b <- DynamicGraph(1:5)
> 

-----Oprindelig meddelelse-----
Fra: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sendt: 16. marts 2004 12:37
Til: Jens Henrik Badsberg
Cc: r-help at stat.math.ethz.ch
Emne: Re: [R] Documentation on how to put classes and methods in
packageswith namespace?


See `Writing R Extensions' in 1.9.0 alpha and the stats4 package there as 
an example.  At this point you probably only want to check a package 
against 1.9.0 alpha, anyway.

On Tue, 16 Mar 2004, Jens Henrik Badsberg wrote:

> 
>  Documentation on how to put classes and methods in packages with namespace?
> 
>  How should I define my classes and methods in "dynamicGraph"???
> 
>  (That is - Can some one point me to the documentation on
>  how to put classes and methods in packages with a namespace? )

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Tue Mar 16 14:57:48 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 16 Mar 2004 07:57:48 -0600
Subject: [R] lme(nlme) error message
In-Reply-To: <000801c40b42$3e81af50$a88f7d81@129125143168>
References: <000801c40b42$3e81af50$a88f7d81@129125143168>
Message-ID: <6r65d4uc6b.fsf@bates4.stat.wisc.edu>

"J Fu" <j.fu at cs.rug.nl> writes:

> I am writing to seek any help on "lme" error message. I am using lme
> to do Mixed-model linear regression. I use my simulated
> data. However, sometimes, I get the following error message, which I
> do not understand.
> 
> "Error in solve.default(pdMatrix(a, fact=TRUE)): system is computationally sigular"
> 
> I would appreciate any help about it.

This is usually an indication that the model is overfitting the data,
which is not an unusual situation for simulated data.  A simple
example would be a variance component model being simulated under the
null hypothesis that the variance component for the random effects is
zero.  In a large proportion of the cases (roughly half) the simulated
data will be such that the MLE and REML estimates of the variance
component are exactly zero.  The code that is failing is trying to
invert the factor of the relative variance-covariance matrix, which is
approaching singularity.

You could wrap your call to lme in try() or tryCatch() so you can
detect the situations where lme fails to converge and decide what to
do with them.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From JonesW at kssg.com  Tue Mar 16 14:56:41 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Tue, 16 Mar 2004 13:56:41 -0000
Subject: [R] rate of change
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0FF9@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040316/32a97096/attachment.pl

From GPetris at uark.edu  Tue Mar 16 15:32:01 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 16 Mar 2004 08:32:01 -0600 (CST)
Subject: [R] multiple summation
In-Reply-To: <1079431923.4056d2f32959c@webmail.uni-bocconi.it> (message from
	Paolo Bulla on Tue, 16 Mar 2004 11:12:03 +0100)
References: <1079431923.4056d2f32959c@webmail.uni-bocconi.it>
Message-ID: <200403161432.i2GEW1PD020775@definetti.uark.edu>


sum, I guess, as in 

> x <- seq(0,2*pi,length=10)
> y <- x <- seq(0,2*pi,length=10)
> X <- cbind(x,y)
> sum(sapply(1:NROW(X), function(i) sin(X[i,1])*cos(X[i,2])))
[1] 1.991599e-16

There may be more efficient ways to deal with the `sapply' part,
especially if your function is vectorized. 

HTH,
Giovanni

> Date: Tue, 16 Mar 2004 11:12:03 +0100
> From: Paolo Bulla <paolo.bulla at unibocconi.it>
> Sender: r-help-bounces at stat.math.ethz.ch
> Cc: 
> Precedence: list
> User-Agent: Internet Messaging Program (IMP) 3.2.1
> 
> 
> Hello,
> I have to compute a multiple summation (not an integration because the 
> independent variables a
> are discrete) for all the values of a function of several variables f
> (x_1,...,x_n), that is
> 
> sum ... sum f(x_1,...,x_n)
> x_1     x_n
> 
> have you some suggestion? Is it possible?
> I know that for multiple integration there is the function adapt, but it has at 
> most n=20. In my case n depends on the dimension of the dataset and, hence, it 
> could be bigger.
> 
> Thank you,
> Paolo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From rbaer at atsu.edu  Tue Mar 16 15:43:44 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Tue, 16 Mar 2004 08:43:44 -0600
Subject: [R] spearman rank correlation problem
References: <1079386628.17351.36.camel@paradise.mitre.org>
	<16470.51229.281145.507762@gargle.gargle.HOWL>
Message-ID: <003801c40b65$153fed50$2e80010a@BigBaer>

> Martin Maechler writes:
> It seems that this needs to be documented, since you are right,
> the    1 - 6 S(d^2) / (n^3 - n)    ( d = r1 - r2 ; r{12} := rank(x{12}) )
> formula is also in use as *definition* for Spearman's rank correlation.

Since this topic is here...

Although I seen the above formula in a couple of books, but I have never
seen a derivation.  Does anyone have a reference where it is derived
(apologies to math and statistics types for whom this must seem child's
play)?  Examining the derivation is probably the best way to understand the
effect of ties when using William Morgan's function for Spearman.

Rob Baer



From bxc at steno.dk  Tue Mar 16 15:51:50 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 16 Mar 2004 15:51:50 +0100
Subject: [R] glm questions  --- saturated model
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E11E@exdkba022.novo.dk>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Firth
> Sent: Tuesday, March 16, 2004 1:12 PM
> To: Paul Johnson
> Cc: r-help at r-project.org
> Subject: Re: [R] glm questions
> 
> 
> Dear Paul
> 
> Here are some attempts at your questions.  I hope it's of some help.
> 
> On Tuesday, Mar 16, 2004, at 06:00 Europe/London, Paul Johnson wrote:
> 
> > Greetings, everybody. Can I ask some glm questions?
> >
> > 1. How do you find out -2*lnL(saturated model)?
> >
> > In the output from glm, I find:
> >
> > Null deviance:  which I think is  -2[lnL(null) - lnL(saturated)]
> > Residual deviance:   -2[lnL(fitted) - lnL(saturated)]
> >
> > The Null model is the one that includes the constant only 
> (plus offset
> > if specified). Right?
> >
> > I can use the Null and Residual deviance to calculate the 
> "usual model
> > Chi-squared" statistic
> > -2[lnL(null) - lnL(fitted)].
> >
> > But, just for curiosity's sake, what't the saturated model's -2lnL ?
> 
> It's important to remember that lnL is defined only up to an additive 
> constant.  For example a Poisson model has lnL contributions -mu + 
> y*log(mu) + constant, and the constant is arbitrary.  The 
> differencing 
> in the deviance calculation eliminates it.  What constant would you 
> like to use??
> 

I have always been und the impression that the constant chosen by glm is
that which makes the deviance of the saturated model 0, the saturated
model being the one with one parameter per observation in the dataset.

For example:

> y <- sample( 0:10, 15, replace=T )
> A <- factor( rep( 1:5, 3 ) )
> B <- factor( rep( 1:3, each=5 ) )
> data.frame( y, A, B )
    y A B
1   1 1 1
2   4 2 1
3   3 3 1
4   7 4 1
5   1 5 1
6   0 1 2
7   5 2 2
8   8 3 2
9   4 4 2
10  2 5 2
11  6 1 3
12 10 2 3
13  6 3 3
14  0 4 3
15  1 5 3
> glm( y ~ A + B, family=poisson )

Call:  glm(formula = y ~ A + B, family = poisson) 

Coefficients:
(Intercept)           A2           A3           A4           A5
B2  
     0.6581       0.9985       0.8873       0.4520      -0.5596
0.1719  
         B3  
     0.3629  

Degrees of Freedom: 14 Total (i.e. Null);  8 Residual
Null Deviance:      40.33 
Residual Deviance: 24.07        AIC: 78.9 
> glm( y ~ A * B, family=poisson )

Call:  glm(formula = y ~ A * B, family = poisson) 

Coefficients:
(Intercept)           A2           A3           A4           A5
B2  
  2.535e-15    1.386e+00    1.099e+00    1.946e+00   -1.293e-14
-2.330e+01  
         B3        A2:B2        A3:B2        A4:B2        A5:B2
A2:B3  
  1.792e+00    2.353e+01    2.428e+01    2.274e+01    2.400e+01
-8.755e-01  
      A3:B3        A4:B3        A5:B3  
 -1.099e+00   -2.704e+01   -1.792e+00  

Degrees of Freedom: 14 Total (i.e. Null);  0 Residual
Null Deviance:      40.33 
Residual Deviance: 3.033e-10    AIC: 70.84 

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From tblackw at umich.edu  Tue Mar 16 16:10:08 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Tue, 16 Mar 2004 10:10:08 -0500 (EST)
Subject: [R] Terminology and canonical statistical user literature
In-Reply-To: <85D25331FFB7AE4C900EA467D4ADA39201211F@circle.pcpool.mi.fu-berlin.de>
References: <85D25331FFB7AE4C900EA467D4ADA39201211F@circle.pcpool.mi.fu-berlin.de>
Message-ID: <Pine.SOL.4.58.0403160953591.354@rygar.gpcc.itd.umich.edu>

Dr. Prechelt  -

It's been my observation that there IS no book of the sort
you have asked for.  There have been many attempts over the
last 75 years to write such a book.  Attempts by some very
smart and articulate people  . . .  and no such attempt that
I know of has succeeded.

I am forced to conclude that there is something intrinsic to
the subject matter which makes it refractory to a good textbook
exposition.  We can speculate about what that is, but I think
the evidence is plain that there is some inherent difficulty.

The way which does seem to work in learning this material is
a spiral - do an introductory look with a limited set of basic
statistical procedures.  Don't worry if you didn't understand
quite all of the details.  Let it sit for a few months;  find
an applied situation where you just HAVE to make use of what
you know.  Then, three months later, go back and view all of
the same procedures again, from a somewhat more sophisticated
or abstract viewpoint, and extend your knowledge to a few more
procedures.

This approach to learning statistics does seem to work, but
it's not a quick process.  I don't know of any other.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 16 Mar 2004, Lutz Prechelt wrote:

> Brian Ripley wrote (to somebody asking about "effect sizes"):
> > ...
> > Given that, I wonder if you are used to standard terminology.
>
> Good point. But I think for many of us there is more behind that.
>
> I personally belong to an (apparently fairly large) group of
> R users who may be enthusiastic, but are statistical laymen
> due to a lack of formal education in the area.
>
> The half-knowledge that I have is often sufficient to see that
> many otherwise nice sources of statistical knowledge are
> dangerously incomplete when it comes to explaining the
> preconditions required for applying a certain technique
> (One example: The extensive NIST handbook at
>  http://www.itl.nist.gov/div898/handbook/
>  fails to mention that the Wilcoxon rank sum test assumes a
>  continuous distribution underlying the sample)
> This is not to speak of how to correctly interpret the results.
>
> My situation is this:
> - I often have a hard time understanding the R documentation
> due to lack of background.
> - I am not in a position to obtain a full background like
> a statistics student would get it.
> - I am very interested in carefully checking/validating my
> application of statistical techniques.
> - I cannot usually get a consulting statistician to help me.
>
> My question:
> Could some of the R gurus maybe agree on a book
> (or very small set of books) with the following properties?:
> - explains typical approaches of statistical analysis
>   (like MASS, but not as condensed)
> - carefully describes preconditions, how to check them,
>   robustness if they are violated, interpretation of results
> - avoids explaining the innards of the techniques
>   (and generally uses the perspective of the computer age)
> - uses terminology that is easily mapped to R
>
> If yes, I would be very interested in seeing this list.
>
> I understand that one book cannot cover it all,
> but maybe there is at least something like "CAS-"
> (Conservative Applied Statistics without S) that
> is of this type?  :-)
>
>   Lutz Prechelt
>
> Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
> Institut f?r Informatik; Freie Universit?t Berlin
> Takustr. 9; 14195 Berlin; Germany
> +49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Tue Mar 16 16:15:06 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 16 Mar 2004 11:15:06 -0400 (AST)
Subject: [R] glm questions
Message-ID: <200403161515.i2GFF6ZI002812@erdos.math.unb.ca>


David Firth wrote (in response to a question from Paul Johnson):

> On the more general point: yes, if all that students need to know is 
> OLS, Poisson rate models and logistic regression, then GLM is overkill. 

	I couldn't agree less.  The glm (not GLM!) framework gives a
	coherence to the structure and changes a collection of ad hoc
	(and thereby essentially meaningless cook-book) techniques
	into a single meaningful technique:

	A parameter (the mean) of a distribution is a transformation
	of a linear function of some predictors.  One seeks to
	estimate the linear coefficients via maximum likelihood.  In
	a broad array of circumstances the maximization can be
	carried out by the glm() function (using iteratively
	reweighted least squares).  The process is quick and
	efficient and the notation is about as transparent as can be
	imagined.

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From p.dalgaard at biostat.ku.dk  Tue Mar 16 16:27:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Mar 2004 16:27:14 +0100
Subject: [R] make check on Solaris 8 fails due to plot
In-Reply-To: <ODEPICOHNDBJEHIFCIMPMEFNCBAA.ramasamy@cancer.org.uk>
References: <ODEPICOHNDBJEHIFCIMPMEFNCBAA.ramasamy@cancer.org.uk>
Message-ID: <x2znagolrh.fsf@biostat.ku.dk>

"Adaikalavan Ramasamy" <ramasamy at cancer.org.uk> writes:

> Many thanks to Prof.Ripley and Dr.Dalgaard. Yes, I am using gcc-3.2.2
> (correctly configured for solaris 2.8) and this could be the problem.
> 
> Is there a way to fix this myself ? My machine specs are 'SunOS enterprise
> 5.8 Generic_108528-23 sun4u sparc SUNW,Ultra-Enterprise'.
> 
> The R-admin seems to suggest that I can solve this by redefining the CC,
> FFLAGS, CXXFLAGS and LDFLAGS in the config.site file but I am not sure if I
> am reading it correctly and am unable to find the config.site file.

It's in the top level source directory. If you can't easily stage a
compiler upgrade, the most obvious change to try is to drop the
optimization level from -O2 to -O or leave it off entirely.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Tue Mar 16 15:42:34 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 16 Mar 2004 14:42:34 -0000 (GMT)
Subject: [R] multiple summation
In-Reply-To: <1079431923.4056d2f32959c@webmail.uni-bocconi.it>
Message-ID: <XFMail.040316144234.Ted.Harding@nessie.mcc.ac.uk>

On 16-Mar-04 Paolo Bulla wrote:
> 
> Hello,
> I have to compute a multiple summation (not an integration
> because the independent variables are discrete) for all
> the values of a function of several variables f(x_1,...,x_n),
> that is
> 
> sum ... sum f(x_1,...,x_n)
> x_1     x_n
> 
> have you some suggestion? Is it possible?
> I know that for multiple integration there is the function adapt,
> but it has at most n=20. In my case n depends on the dimension of the
> dataset and, hence, it could be bigger.

If you can conveniently encapsulate the values of f(x_1,...,x_n)
as an n-dimensional array (which presumes in effect that the ranges
of x_1,...,x_n form a "product space"), then simple 'sum' should
do it, e.g.

  A<-array(c(1,2,3,4,5,6,7,8,9,10,11,12),dim=c(2,2,3))

  A
  , , 1
       [,1] [,2]
  [1,]    1    3
  [2,]    2    4

  , , 2
       [,1] [,2]
  [1,]    5    7
  [2,]    6    8

  , , 3
       [,1] [,2]
  [1,]    9   11
  [2,]   10   12

  sum(A)
  [1] 78

However, I am concerned about your possibility that 'n' could be
bigger than 20, since this would generate a huge array (at least
8*2^21 = 16,777,216 bytes even with only 2 distinct x_i values per
dimension; with 3 you get  8*3^21 = 83,682,825,624 or 83GB).

While it does work if you can get it into memory and avoid overflow:

  B<-array((1:2^21)/2^22,dim=rep(2,21))
  sum(B)
  [1] 524288.2

you are clearly pushing hard at machine limits here. What sort
of context requires summing over so many dimensions?

An alternative, if 'f' is given by a formula, could be to simply
and crudely iterate over the dimensions, but in its own way that
is even more daunting!

Another alternative, if you don't need a mathematically exact
result, is to estimate the sum by Monte Carlo simulation: sample
the points in each dimension, evaluate the function, accumulate
these mean of these values, and finally scale up appropriately
according to the number of points in the "box" (stop when the SE
of your result is small enough).

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 16-Mar-04                                       Time: 14:42:34
------------------------------ XFMail ------------------------------



From tlumley at u.washington.edu  Tue Mar 16 16:32:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 16 Mar 2004 07:32:29 -0800 (PST)
Subject: [R] glm questions
In-Reply-To: <17C7D966-7743-11D8-A7C1-000A95A6625E@warwick.ac.uk>
References: <17C7D966-7743-11D8-A7C1-000A95A6625E@warwick.ac.uk>
Message-ID: <Pine.A41.4.58.0403160722090.43474@homer27.u.washington.edu>

On Tue, 16 Mar 2004, David Firth wrote:

> I am unclear what you are asking here.  I assume by "scaled deviance"
> you mean deviance divided by phi, a (known) scale parameter?  (I'm
> sorry, I don't know SAS's definition.)    In many applications (eg
> binomial, Poisson) deviance and scaled deviance are the same thing,
> since phi is 1.  Yes, if you wanted to judge overdispersion relative to
> some other value of phi you would scale the deviance.  What other value
> of phi would you like?

For the quasibinomial() and quasipoisson() families you do get a scale
parameter estimate, computed from the variance of the Pearson residuals.

>
> On Tuesday, Mar 16, 2004, at 06:00 Europe/London, Paul Johnson wrote:
> >
> > 4. For GLM teaching purposes, can anybody point me at some good
> > examples of GLM that do not use Normal, Poisson, Negative Binomial,
> > and/or Logistic Regression?  I want to justify the effort to
> > understand the GLM as a framework.  We have already in previous
> > semesters followed the usual "econometric" approach in which OLS,
> > Poisson/Count, and Logistic regression are treated as special cases.
> > Some of the students don't see any benefit from tackling the GLM's new
> > notation/terminology.
>

David pointed out the Gamma models (both log link and reciprocal link). I
like the binomial with log link, which gives relative risks rather than
odds ratios.  These give examples where you might want two different link
functions for the same variance function.  There's also a paper by Nelder
in Applied Statistics (1994) talking about power link and variance for
skewed non-negative data.

	-thomas



From hdoran at nasdc.org  Tue Mar 16 16:34:05 2004
From: hdoran at nasdc.org (Harold Doran)
Date: Tue, 16 Mar 2004 10:34:05 -0500
Subject: [R] rate of change
Message-ID: <66578BFC0BA55348B5907A0F798EE930663599@ernesto.NASDC.ORG>

Type help(deriv). However, it may be easier to compute the derivative by hand for a simple expression. 

HCD

-----Original Message-----
From: Fred J. [mailto:phddas at yahoo.com]
Sent: Monday, March 15, 2004 10:20 PM
To: r help
Subject: [R] rate of change


Hello
I am wondering, how do I find if R has a certain
funciton to do a given task. do I just type
help.search("rate").
I am just trying to find a function to calculate the
rate of change for a variable. I could come up with
one if there isn't any allready builtin.

thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From d.firth at warwick.ac.uk  Tue Mar 16 16:44:30 2004
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 16 Mar 2004 15:44:30 +0000
Subject: [R] glm questions  --- saturated model
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E11E@exdkba022.novo.dk>
Message-ID: <D0B5C981-7760-11D8-A7C1-000A95A6625E@warwick.ac.uk>

On Tuesday, Mar 16, 2004, at 14:51 Europe/London, BXC (Bendix 
Carstensen) wrote:

>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Firth
>> Sent: Tuesday, March 16, 2004 1:12 PM
>> To: Paul Johnson
>> Cc: r-help at r-project.org
>> Subject: Re: [R] glm questions
>>
>>
>> Dear Paul
>>
>> Here are some attempts at your questions.  I hope it's of some help.
>>
>> On Tuesday, Mar 16, 2004, at 06:00 Europe/London, Paul Johnson wrote:
>>
>>> Greetings, everybody. Can I ask some glm questions?
>>>
>>> 1. How do you find out -2*lnL(saturated model)?
>>>
>>> In the output from glm, I find:
>>>
>>> Null deviance:  which I think is  -2[lnL(null) - lnL(saturated)]
>>> Residual deviance:   -2[lnL(fitted) - lnL(saturated)]
>>>
>>> The Null model is the one that includes the constant only
>> (plus offset
>>> if specified). Right?
>>>
>>> I can use the Null and Residual deviance to calculate the
>> "usual model
>>> Chi-squared" statistic
>>> -2[lnL(null) - lnL(fitted)].
>>>
>>> But, just for curiosity's sake, what't the saturated model's -2lnL ?
>>
>> It's important to remember that lnL is defined only up to an additive
>> constant.  For example a Poisson model has lnL contributions -mu +
>> y*log(mu) + constant, and the constant is arbitrary.  The
>> differencing
>> in the deviance calculation eliminates it.  What constant would you
>> like to use??
>>
>
> I have always been und the impression that the constant chosen by glm 
> is
> that which makes the deviance of the saturated model 0, the saturated
> model being the one with one parameter per observation in the dataset.
> ...

But a look at the deviance formula above ---
    -2[lnL(fitted) - lnL(saturated)]
--- shows us that *any* constant can be added to lnL, and the deviance 
for the saturated model will still be zero.

David



From andy_liaw at merck.com  Tue Mar 16 16:52:21 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Mar 2004 10:52:21 -0500
Subject: [R] graphical interface
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79CA@usrymx25.merck.com>

> From: Carlos Saavedra
> 
> Dear R users,,
>  
> I'm having difficulties when i use the gstat extensions in R
> especially with the graphical interface, as the
> variograms plots are depicted with gray background and cyan 
> points for the number of pairs, Consequently the fitted variogram
> is drawn in cyan, 
>  
> How can i change that so i could have,  white background and 
> black lines for  the fitted variogram, for the complete plot. 

Try:

lset(theme=col.whitebg())

right before the plot command.  See ?lset for more details.

[Plead to the maintainer of lattice:  Is it possible to make the theme an
option that can be set for the session, not just for the device?]

Best,
Andy
  
> looking forward for hearing from you,
>  
> carlos saavedra
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ferraria at ensisun.imag.fr  Tue Mar 16 17:21:26 2004
From: ferraria at ensisun.imag.fr (anthony.ferrari@ensimag.imag.fr)
Date: Tue, 16 Mar 2004 17:21:26 +0100 (MET)
Subject: [R] X11 and utils
Message-ID: <Pine.GSO.4.40.0403161701100.15543-100000@ensisun>


Hello,
I'm a french student working on a project concerning cDNA microarray.
I've chosen to work with R and use the marrayClasses, marrayInput (...)
packages.

First, when I enter "library(marrayInput)", R warns me about a missing
package called "utils" but I can't find it anywhere. Normally this package
is installed when you install "base" package. Moreover when I call a
function of the "utils" package, it works so it seems to be installed.
Why doesn't R see it when I enter "library(marrayInput)" ?
I hope, I am clear enough.

Secondly, once R is installed, X11() does not work. I can't see anything
when I invoke the plot functions.
Maybe I have to precise that :
      - I work on a Linux platform. But I havn't got the root's password.
      - So I can only install R locally
My first idea is that the problem comes from the intallation step. Is
there something to tell R about the system X11R6 path ? May I give a
parameter to ./configure ? Or does R normally find all of these itself ?


waiting for an answer,
thank you very much

kind regards,
Anthony Ferrari
anthony.ferrari at ensimag.imag.fr



From washington.santos at posgrad.ufla.br  Tue Mar 16 17:52:22 2004
From: washington.santos at posgrad.ufla.br (Washington Santos Da Silva)
Date: Tue, 16 Mar 2004 13:52:22 -0300
Subject: [R] A simple question about simulation/estimation. 
Message-ID: <20040316164336.M83405@posgrad.ufla.br>

Dear list,

 What is the best form to, given a matrix of simulated data (120x80), 
estimate the parameters of a weibull distribution for each column, store the 
estimates and plot them?

Thanks in advance.

Washington Santos da Silva



From p.dalgaard at biostat.ku.dk  Tue Mar 16 18:42:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Mar 2004 18:42:38 +0100
Subject: [R] glm questions  --- saturated model
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E11E@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90179E11E@exdkba022.novo.dk>
Message-ID: <x2n06gofht.fsf@biostat.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> > It's important to remember that lnL is defined only up to an additive 
> > constant.  For example a Poisson model has lnL contributions -mu + 
> > y*log(mu) + constant, and the constant is arbitrary.  The 
> > differencing 
> > in the deviance calculation eliminates it.  What constant would you 
> > like to use??
> > 
> 
> I have always been und the impression that the constant chosen by glm is
> that which makes the deviance of the saturated model 0, the saturated
> model being the one with one parameter per observation in the dataset.

As David pointed out, the deviance of a saturated model is zero by
definition. However, there's nothing arbitrary about the constant in a
likelihood either since it is supposed to be a density if seen as a
function of y (well, if you *really* want to quibble, it's a density
with respect to an arbitrary measure, so you could get an arbitrary
constant in if you insist, I suppose). The point is that the constant
is *uniformative* since it depends on y only, not mu, and hence that
people tend to throw some bits of the likelihood away, and not always
the same bits.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From partha_bagchi at hgsi.com  Tue Mar 16 19:14:08 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Tue, 16 Mar 2004 13:14:08 -0500
Subject: [R] Terminology and canonical statistical user literature
Message-ID: <OF071C76B1.BDAC2E35-ON85256E59.0063DD43-85256E59.00642C01@hgsi.com>

I would recommend Peter Dalgaard's book for an introduction to Statistics 
with R. Also, a resource that maybe more aligned with what you are asking 
for may be the so called ARTIST project. Details at
www.gen.umn.edu/artist/






"Lutz Prechelt" <prechelt at pcpool.mi.fu-berlin.de>
Sent by: r-help-bounces at stat.math.ethz.ch
03/16/2004 06:44 AM

 
        To:     "R Help" <r-help at stat.math.ethz.ch>
        cc: 
        Subject:        [R] Terminology and canonical statistical user literature


Brian Ripley wrote (to somebody asking about "effect sizes"):
> ...
> Given that, I wonder if you are used to standard terminology.

Good point. But I think for many of us there is more behind that.

I personally belong to an (apparently fairly large) group of
R users who may be enthusiastic, but are statistical laymen
due to a lack of formal education in the area.

The half-knowledge that I have is often sufficient to see that
many otherwise nice sources of statistical knowledge are
dangerously incomplete when it comes to explaining the
preconditions required for applying a certain technique
(One example: The extensive NIST handbook at
http://www.itl.nist.gov/div898/handbook/
fails to mention that the Wilcoxon rank sum test assumes a
continuous distribution underlying the sample)
This is not to speak of how to correctly interpret the results.

My situation is this:
- I often have a hard time understanding the R documentation
due to lack of background.
- I am not in a position to obtain a full background like
a statistics student would get it.
- I am very interested in carefully checking/validating my
application of statistical techniques.
- I cannot usually get a consulting statistician to help me.

My question:
Could some of the R gurus maybe agree on a book
(or very small set of books) with the following properties?:
- explains typical approaches of statistical analysis
(like MASS, but not as condensed)
- carefully describes preconditions, how to check them,
robustness if they are violated, interpretation of results
- avoids explaining the innards of the techniques
(and generally uses the perspective of the computer age)
- uses terminology that is easily mapped to R

If yes, I would be very interested in seeing this list.

I understand that one book cannot cover it all,
but maybe there is at least something like "CAS-"
(Conservative Applied Statistics without S) that
is of this type?  :-)

Lutz Prechelt

Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
Institut f?r Informatik; Freie Universit?t Berlin
Takustr. 9; 14195 Berlin; Germany
+49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

--
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From d.firth at warwick.ac.uk  Tue Mar 16 19:40:23 2004
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 16 Mar 2004 18:40:23 +0000
Subject: [R] glm questions
In-Reply-To: <200403161515.i2GFF6ZI002812@erdos.math.unb.ca>
Message-ID: <62A3DA7C-7779-11D8-A7C1-000A95A6625E@warwick.ac.uk>

On Tuesday, Mar 16, 2004, at 15:15 Europe/London, Rolf Turner wrote:

>
> David Firth wrote (in response to a question from Paul Johnson):
>
>> On the more general point: yes, if all that students need to know is
>> OLS, Poisson rate models and logistic regression, then GLM is 
>> overkill.
>
> 	I couldn't agree less.  The glm (not GLM!) framework gives a
>

Well, I really did _intend_ GLM when I wrote GLM, meaning the sort of 
theoretical thing that Paul described, involving the presentation to 
(political-science etc) students of the general (linear) exponential 
family.  All that stuff is not needed for a proper understanding of the 
three models mentioned, and certainly those three models are all 
meaningful without it.  Your second paragraph below is one that I 
wholeheartedly agree with though (except that it should be linear 
function of the parameters, not the predictors), and it seems to agree 
also with what I wrote in the second part of the paragraph which you 
quote above (the part that you cut) from my earlier reply.

David

> 	coherence to the structure and changes a collection of ad hoc
> 	(and thereby essentially meaningless cook-book) techniques
> 	into a single meaningful technique:
>
> 	A parameter (the mean) of a distribution is a transformation
> 	of a linear function of some predictors.  One seeks to
> 	estimate the linear coefficients via maximum likelihood.  In
> 	a broad array of circumstances the maximization can be
> 	carried out by the glm() function (using iteratively
> 	reweighted least squares).  The process is quick and
> 	efficient and the notation is about as transparent as can be
> 	imagined.
>
> 					cheers,
>
> 						Rolf Turner
> 						rolf at math.unb.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Mar 16 20:18:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Mar 2004 19:18:32 +0000 (GMT)
Subject: [R] Terminology and canonical statistical user literature
In-Reply-To: <85D25331FFB7AE4C900EA467D4ADA39201211F@circle.pcpool.mi.fu-berlin.de>
Message-ID: <Pine.LNX.4.44.0403161910360.11887-100000@gannet.stats>

On Tue, 16 Mar 2004, Lutz Prechelt wrote:

> Brian Ripley wrote (to somebody asking about "effect sizes"):
> > ...
> > Given that, I wonder if you are used to standard terminology.
> 
> Good point. But I think for many of us there is more behind that.

But you have completely missed my point. Asking for how to do X, where X 
is a word or two, without any reference or explanation as to what X is, 
will be insufficient unless completely standard jargon is used.  Jargon is 
great for concise and accurate transmission of ideas, but only if both 
sides have the same dictionary.

This is particularly important with a one-way transmission medium like 
posting to a list.

It is like the Q yesterday about the `norm of a complex number'.  That's 
not standard, but saying it was |x| told us he meant the modulus.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pauljohn at ku.edu  Tue Mar 16 21:28:48 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Tue, 16 Mar 2004 14:28:48 -0600
Subject: [R] glm questions  --- saturated model
In-Reply-To: <x2n06gofht.fsf@biostat.ku.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90179E11E@exdkba022.novo.dk>
	<x2n06gofht.fsf@biostat.ku.dk>
Message-ID: <40576380.3070101@ku.edu>

I'm confused going back and forth between the textbooks and these 
emails.  Please pardon me that I seem so pedantic.

I am pretty certain that -2lnL(saturated) is not 0 by definition.  In 
the binomial model with groups of size=1, then the observed scores will 
be {0,1} but the predicted mean will be some number in [0,1], and so 
-2lnL will not be 0.  I'm reading, for example, Annette Dobson, An 
Introduction to Generalized Linear Models, 2ed (2002  p. 77), where it 
gives the formula one can use for -2lnL(saturated) in the binomial model.

For the Normal distribution, Dobson says

-2lnL(saturated) = N log(2 pi sigma^2)

She gives the saturated model -2lnL(saturated) for lots of 
distributions, actually.

I thought the point in the first note from Prof. Firth was that the 
deviance is defined up to an additive constant because you can add or 
subtract from lnL in the deviance formula

D = -2[lnL(full) - lnL(subset)]

and the deviance is unaffected.  But I don't think that means there is a 
completely free quantity in lnL(saturated).

I agree that the deviance of the saturated model is 0 by definition, if 
by that one means to say

-2[lnL(saturated)-lnL(saturated)]

but of course, that's just a tautology.

Respectfully yours,

pj


Peter Dalgaard wrote:

>"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:
>
>  
>
>>>It's important to remember that lnL is defined only up to an additive 
>>>constant.  For example a Poisson model has lnL contributions -mu + 
>>>y*log(mu) + constant, and the constant is arbitrary.  The 
>>>differencing 
>>>in the deviance calculation eliminates it.  What constant would you 
>>>like to use??
>>>
>>>      
>>>
>>I have always been und the impression that the constant chosen by glm is
>>that which makes the deviance of the saturated model 0, the saturated
>>model being the one with one parameter per observation in the dataset.
>>    
>>
>
>As David pointed out, the deviance of a saturated model is zero by
>definition. However, there's nothing arbitrary about the constant in a
>likelihood either since it is supposed to be a density if seen as a
>function of y (well, if you *really* want to quibble, it's a density
>with respect to an arbitrary measure, so you could get an arbitrary
>constant in if you insist, I suppose). The point is that the constant
>is *uniformative* since it depends on y only, not mu, and hence that
>people tend to throw some bits of the likelihood away, and not always
>the same bits.
>
>  
>


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From jasont at indigoindustrial.co.nz  Tue Mar 16 22:16:02 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 17 Mar 2004 10:16:02 +1300
Subject: [R] Sweave and R output: possible to suppress "Schunk" tags in
	*.tex file output?
In-Reply-To: <16469.29485.718151.188553@galadriel.ci.tuwien.ac.at>
References: <62AE0CF1D4875C4BBDEC29DB9924ACE87F21AE@pnlmse25.pnl.gov>
	<16469.29485.718151.188553@galadriel.ci.tuwien.ac.at>
Message-ID: <40576E92.4010308@indigoindustrial.co.nz>

Friedrich.Leisch at ci.tuwien.ac.at wrote:
> results=tex will do what you want, i.e., produce no Soutput and Schunk
> environments. you may also want to look at package xtable.

Just to add, the function latex() in the Hmisc package is also quite good.

Cheers

Jason



From jasont at indigoindustrial.co.nz  Tue Mar 16 22:16:28 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 17 Mar 2004 10:16:28 +1300
Subject: [R] R-business case
In-Reply-To: <40518E01.10608@tiscali.co.uk>
References: <40518E01.10608@tiscali.co.uk>
Message-ID: <40576EAC.2030806@indigoindustrial.co.nz>

giovanni merola wrote:
> Dear all,
> I am putting together a business case for R and I would like to know if 
> there exist statistics on the distribution of R in academia, research 
> and commercial institutions. The same information about S-plus would be 
> welcome. Please reply only if you have consistent and certain information.
> 

For the usual commercial reasons, getting hard data about commercial use 
is difficult.  I've usually just listed the customers' needs 
(compatibility? particular OS/platform as standard? extensibility? 
support?), and compared the strengths and weaknesses of R vs. other apps.

Cheers

Jason



From d.firth at warwick.ac.uk  Tue Mar 16 22:37:20 2004
From: d.firth at warwick.ac.uk (David Firth)
Date: Tue, 16 Mar 2004 21:37:20 +0000
Subject: [R] glm questions  --- saturated model
In-Reply-To: <40576380.3070101@ku.edu>
Message-ID: <1AF53C90-7792-11D8-B075-0050E4C03977@warwick.ac.uk>

On Tuesday, Mar 16, 2004, at 20:28 Europe/London, Paul Johnson wrote:

> I'm confused going back and forth between the textbooks and these 
> emails.  Please pardon me that I seem so pedantic.
>
> I am pretty certain that -2lnL(saturated) is not 0 by definition.

I'm pretty certain of that too.

> In the binomial model with groups of size=1, then the observed scores 
> will be {0,1} but the predicted mean will be some number in [0,1], and 
> so -2lnL will not be 0.  I'm reading, for example, Annette Dobson, An 
> Introduction to Generalized Linear Models, 2ed (2002  p. 77), where it 
> gives the formula one can use for -2lnL(saturated) in the binomial 
> model.
>
> For the Normal distribution, Dobson says
>
> -2lnL(saturated) = N log(2 pi sigma^2)
>
> She gives the saturated model -2lnL(saturated) for lots of 
> distributions, actually.
>
> I thought the point in the first note from Prof. Firth was that the 
> deviance is defined up to an additive constant because you can add or 
> subtract from lnL in the deviance formula
>
> D = -2[lnL(full) - lnL(subset)]
>
> and the deviance is unaffected.  But I don't think that means there is 
> a completely free quantity in lnL(saturated).

No, that's not what I said earlier.  Nor what I meant.

lnL(any model, including saturated) is defined only up to an additive 
constant.  Dobson's formulae are presumably correct, and would remain 
so if we added a constant.

For discrete distributions we can probably all agree to define 
likelihood as the probability of getting the observed data (ie to use 
counting measure on 0,1,2,... or whatever to define our "density") --- 
in which case the arbitrariness is resolved.  For continuous 
distributions we can't do that: the probability is zero for every set 
of data.  So we use density with respect to some measure, the choice of 
which measure leads to an arbitrary constant (as Peter said).  But the 
value of the constant doesn't matter -- and that really is the point.

I hope that's somehow clearer.

Best wishes,
David



From spencer.graves at pdf.com  Tue Mar 16 22:41:38 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 16 Mar 2004 13:41:38 -0800
Subject: [R] R-business case
In-Reply-To: <40576EAC.2030806@indigoindustrial.co.nz>
References: <40518E01.10608@tiscali.co.uk>
	<40576EAC.2030806@indigoindustrial.co.nz>
Message-ID: <40577492.6090507@pdf.com>

      Some web sites have "hit counters".  It should be possible to get 
a counts of the numbers of times different parts of R are downloaded.  
Do the CRAN web sites include any such? 

      Best Wishes,   
      spencer graves

Jason Turner wrote:

> giovanni merola wrote:
>
>> Dear all,
>> I am putting together a business case for R and I would like to know 
>> if there exist statistics on the distribution of R in academia, 
>> research and commercial institutions. The same information about 
>> S-plus would be welcome. Please reply only if you have consistent and 
>> certain information.
>>
>
> For the usual commercial reasons, getting hard data about commercial 
> use is difficult.  I've usually just listed the customers' needs 
> (compatibility? particular OS/platform as standard? extensibility? 
> support?), and compared the strengths and weaknesses of R vs. other apps.
>
> Cheers
>
> Jason
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jgentry at jimmy.harvard.edu  Tue Mar 16 22:50:06 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Tue, 16 Mar 2004 16:50:06 -0500 (EST)
Subject: [R] R-business case
In-Reply-To: <40577492.6090507@pdf.com>
Message-ID: <Pine.SOL.4.20.0403161648460.1867-100000@santiam.dfci.harvard.edu>

>       Some web sites have "hit counters".  It should be possible to get 
> a counts of the numbers of times different parts of R are downloaded.  
> Do the CRAN web sites include any such? 

But how would CRAN know what sort of activity its mirrors is
receiving?  And what does a 'download' mean in an environment like this -
I could download R 10 times, or 10 people could download it ... or even
one person could download it and then give it to 50 people.  The numbers
don't necessarily reflect usage patterns, IMO.

-J



From david at elseware.nl  Tue Mar 16 22:58:12 2004
From: david at elseware.nl (David A. van Leeuwen)
Date: Tue, 16 Mar 2004 22:58:12 +0100
Subject: [R] effect size
In-Reply-To: <Pine.LNX.4.44.0403160822020.5148-100000@gannet.stats>
References: <Pine.LNX.4.44.0403160822020.5148-100000@gannet.stats>
Message-ID: <40577874.1090302@elseware.nl>

Prof Brian Ripley wrote:

>I think you want to call summary.lm on the aov object, but this depends on 
>what you mean by `effect size'.
>
>  
>
I guess this is what we wanted.

>Given that, I wonder if you are used to standard terminology.
>  
>
No, I am not, unfortunately.  We are doing lots of statistical analyses, 
using R because it is fab and such, but reviewers are looking for SPSS 
output using terminology that we can't find in the R bundle---but our 
general impression is that R does things way more cleverer and better 
than click-until-you-seed-red-signifficant-effect tools found elsewhere. 

Reactions on r-help caused us to request for a better specification of 
the `effect size' that people wanted, and it turns out to be
$$ SS(effect) / \Sum SS $$ (SS being sums-of-squares).  To me, a simple 
physicist, that sounds as `the fraction of explained variance' by the 
factor.  Looking at the formulas in help(summary.lm) is seems that 
summary.lm()$r.squared  is exactly what we want (for a one-way aov).

Is there a way to quickly tabulate the expression $$ SS(effect) / 
(\Sum_{effects} SS(effect) + SS(residuals)) $$ ?

The numbers are practically there in the summary.aov() table.  Only the 
grand total SS needs to be calculated.

>For example, R does have an effects() function, and that might be what you 
>want.
>
>  
>
I don't really understand the effects()---it must be related to 
coefficients() but it obviously is different. There is 
model.tables.aov() which is also enlightening, but I think it is really 
the $ r^2 $ that we were looking for (our reviewers calling this an $ 
\eta^2 $---if that clarifies things).

Thanks,

---david

>On Mon, 15 Mar 2004, David A. van Leeuwen wrote:
>
>  
>
>>Having searched google '[R] aov effect size' without any results I 
>>wonder if I not completely miss something. 
>>
>>Is there any R function that calculates the effect size of an AOV's main 
>>effect or interaction effect?  It should be related to the F's and the 
>>degree of freedom of the error, but the multitude in numbers in aov() 
>>baffle me a bit.
>>    
>>



From p.connolly at hortresearch.co.nz  Tue Mar 16 23:04:07 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 17 Mar 2004 11:04:07 +1300
Subject: [R] R-business case
In-Reply-To: <Pine.SOL.4.20.0403161648460.1867-100000@santiam.dfci.harvard.edu>;
	from jgentry@jimmy.harvard.edu on Tue, Mar 16, 2004 at 04:50:06PM -0500
References: <40577492.6090507@pdf.com>
	<Pine.SOL.4.20.0403161648460.1867-100000@santiam.dfci.harvard.edu>
Message-ID: <20040317110406.B2137@hortresearch.co.nz>

On Tue, 16-Mar-2004 at 04:50PM -0500, Jeff Gentry wrote:

|> >       Some web sites have "hit counters".  It should be possible to get 
|> > a counts of the numbers of times different parts of R are downloaded.  
|> > Do the CRAN web sites include any such? 
|> 
|> But how would CRAN know what sort of activity its mirrors is
|> receiving?  And what does a 'download' mean in an environment like this -
|> I could download R 10 times, or 10 people could download it ... or even
|> one person could download it and then give it to 50 people.  The numbers
|> don't necessarily reflect usage patterns, IMO.

Yes.  Such meters are almost, but not quite completely useless --
rather like Arthur Dent's attempts at getting a cup of tea from the
Nutrimatix drink dispenser.



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From edd at debian.org  Tue Mar 16 23:21:33 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 16 Mar 2004 16:21:33 -0600
Subject: [R] R-business case
In-Reply-To: <Pine.SOL.4.20.0403161648460.1867-100000@santiam.dfci.harvard.edu>
References: <40577492.6090507@pdf.com>
	<Pine.SOL.4.20.0403161648460.1867-100000@santiam.dfci.harvard.edu>
Message-ID: <20040316222133.GA10404@sonny.eddelbuettel.com>

On Tue, Mar 16, 2004 at 04:50:06PM -0500, Jeff Gentry wrote:
> >       Some web sites have "hit counters".  It should be possible to get 
> > a counts of the numbers of times different parts of R are downloaded.  
> > Do the CRAN web sites include any such? 
> 
> But how would CRAN know what sort of activity its mirrors is
> receiving?  And what does a 'download' mean in an environment like this -

How about this: R will presumably be used most often on Windoze. As shipped,
the binary will contact the Austrian site by default. If we started with
that, we could have a reasonable snapshot of activity.  

I would really like to see download numbers of source or binary CRAN
packages. 

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From mandevip at uaslp.mx  Tue Mar 16 23:40:38 2004
From: mandevip at uaslp.mx (Peter B. Mandeville)
Date: Tue, 16 Mar 2004 16:40:38 -0600
Subject: [R] effect size
Message-ID: <5.1.0.14.0.20040316164025.03230eb0@uaslp.mx>


ETA2 <- function(lm.obj){
   cat("\n       ETA2:\n\n")
   tmp <- anova(lm.obj)
   print(round(tmp$'Sum Sq'/sum(tmp$'Sum Sq'),4))
}

At 10:58 p.m. 16/03/04 +0100, you wrote:
>Prof Brian Ripley wrote:
>
>>I think you want to call summary.lm on the aov object, but this depends 
>>on what you mean by `effect size'.
>>
>>
>I guess this is what we wanted.
>
>>Given that, I wonder if you are used to standard terminology.
>>
>No, I am not, unfortunately.  We are doing lots of statistical analyses, 
>using R because it is fab and such, but reviewers are looking for SPSS 
>output using terminology that we can't find in the R bundle---but our 
>general impression is that R does things way more cleverer and better than 
>click-until-you-seed-red-signifficant-effect tools found elsewhere.
>Reactions on r-help caused us to request for a better specification of the 
>`effect size' that people wanted, and it turns out to be
>$$ SS(effect) / \Sum SS $$ (SS being sums-of-squares).  To me, a simple 
>physicist, that sounds as `the fraction of explained variance' by the 
>factor.  Looking at the formulas in help(summary.lm) is seems that 
>summary.lm()$r.squared  is exactly what we want (for a one-way aov).
>
>Is there a way to quickly tabulate the expression $$ SS(effect) / 
>(\Sum_{effects} SS(effect) + SS(residuals)) $$ ?
>
>The numbers are practically there in the summary.aov() table.  Only the 
>grand total SS needs to be calculated.
>
>>For example, R does have an effects() function, and that might be what 
>>you want.
>>
>>
>I don't really understand the effects()---it must be related to 
>coefficients() but it obviously is different. There is model.tables.aov() 
>which is also enlightening, but I think it is really the $ r^2 $ that we 
>were looking for (our reviewers calling this an $ \eta^2 $---if that 
>clarifies things).
>
>Thanks,
>
>---david
>
>>On Mon, 15 Mar 2004, David A. van Leeuwen wrote:
>>
>>
>>
>>>Having searched google '[R] aov effect size' without any results I 
>>>wonder if I not completely miss something.
>>>Is there any R function that calculates the effect size of an AOV's main 
>>>effect or interaction effect?  It should be related to the F's and the 
>>>degree of freedom of the error, but the multitude in numbers in aov() 
>>>baffle me a bit.
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Tue Mar 16 23:41:29 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 16 Mar 2004 16:41:29 -0600
Subject: [R] graphical interface
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF79CA@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF79CA@usrymx25.merck.com>
Message-ID: <200403161641.29084.deepayan@stat.wisc.edu>

On Tuesday 16 March 2004 09:52, Liaw, Andy wrote:
> > From: Carlos Saavedra
> >
> > Dear R users,,
> >
> > I'm having difficulties when i use the gstat extensions in R
> > especially with the graphical interface, as the
> > variograms plots are depicted with gray background and cyan
> > points for the number of pairs, Consequently the fitted variogram
> > is drawn in cyan,
> >
> > How can i change that so i could have,  white background and
> > black lines for  the fitted variogram, for the complete plot.
>
> Try:
>
> lset(theme=col.whitebg())
>
> right before the plot command.  See ?lset for more details.


Another option (if the intention is to have a black and white plot), is to 
start the device by 

trellis.device(color = FALSE)

before calling the plot command.


> [Plead to the maintainer of lattice:  Is it possible to make the theme
> an option that can be set for the session, not just for the device?]


I'm not completely sure if this is what you are looking for, but the device  
settings are ultimately determined by trellis.device(), which has a 
'theme' argument. It essentially has the effect of calling lset() with 
that theme immediately after the device is set up. This 'theme' argument 
defaults to getOption("lattice.theme"), which you can set to modify the 
default behaviour. From ?trellis.device:


   theme: list of components that change the settings of the device
          opened, or, a function that when called produces such a list.
          The function name can be supplied as a quoted string. A
          possible usage is to change the default settings at session
          startup, for example by setting 'options(lattice.theme =
          "col.whitebg")'. If 'theme' is a function, it will not be
          supplied any arguments, however, it is guaranteed that a
          device will already be open when it is called, so one may use
          '.Device' inside the function to ascertain what device has
          been opened.


If this is indeed what you are looking for, I would appreciate hearing 
about any ideas to make this information easier to get at (I guess I could 
add a link from ?lset to ?trellis.device for starters).

Deepayan



From risdpizza at yahoo.co.jp  Wed Mar 17 00:13:11 2004
From: risdpizza at yahoo.co.jp (SI)
Date: Wed, 17 Mar 2004 08:13:11 +0900 (JST)
Subject: [R] save GUI window position in windows version?
Message-ID: <20040316231311.28006.qmail@web501.mail.yahoo.co.jp>

Hi,

I'm using windows GUI version.  Is there any way 
to save window position from previous session?Everytime I
open GUI starts as maximized and I 
have to resize it.

Thanks for the help.

SI



From Don.Driscoll at flinders.edu.au  Wed Mar 17 00:14:49 2004
From: Don.Driscoll at flinders.edu.au (Don Driscoll)
Date: Wed, 17 Mar 2004 09:44:49 +1030
Subject: [R] bray-curtis?
Message-ID: <6.0.1.1.1.20040317094128.01bb9950@mail.flinders.edu.au>

Does R have a function to calculate Bray-Curtis distance measures, which is 
probably one of the most frequently used and recommended dissimilarity 
measures in ecology?  It isn't mentioned in dist().



From Ted.Harding at nessie.mcc.ac.uk  Tue Mar 16 23:02:10 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 16 Mar 2004 22:02:10 -0000 (GMT)
Subject: [R] R-business case
In-Reply-To: <40576EAC.2030806@indigoindustrial.co.nz>
Message-ID: <XFMail.040316220210.Ted.Harding@nessie.mcc.ac.uk>

On 16-Mar-04 Jason Turner wrote:
> giovanni merola wrote:
>> Dear all,
>> I am putting together a business case for R and I would like
>> to know if there exist statistics on the distribution of R in
>> academia, research and commercial institutions. The same
>> information about S-plus would be welcome. Please reply only if
>> you have consistent and certain information.
>> 
> 
> For the usual commercial reasons, getting hard data about commercial
> use is difficult.  I've usually just listed the customers' needs 
> (compatibility? particular OS/platform as standard? extensibility? 
> support?), and compared the strengths and weaknesses of R vs. other
> apps.

This is a question which could possibly fascinate members of this
list os statisticians (or possibly not ... ).

I gave quite some thought to a similar question some time ago:
how could one estimate the usage of Linux?

I came to the conclusion that it is probably impossible by reasonable
means to get an estimate, with any quantified reliability, of the
usage of Open Source software, except in restricted contexts.

Even with commercial software, while the companies will know (or
should) how many licences have been sold, this is not informative
about how many people use pirate copies, nor about how many people
have stopped using it. Nevertheless, it gives a good estimate
compared with what you could get for Open Source.

Whatever sampling trick you think up, there will be aspects
of the availability of OS, or of its dissemination, which will
be almost completely intangible. This, in my view, is why
although a weak lower bound may be feasible (e.g. the number of
active members of this list, for R), putting a margin of error
on it is not feasible.

However, if any of you have different ideas (or, even better, know
of good studies of this sort of question), then it would be very
interesting to hear of them!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 16-Mar-04                                       Time: 22:02:10
------------------------------ XFMail ------------------------------



From abunn at montana.edu  Wed Mar 17 00:23:16 2004
From: abunn at montana.edu (Andy Bunn)
Date: Tue, 16 Mar 2004 16:23:16 -0700
Subject: [R] bray-curtis?
In-Reply-To: <6.0.1.1.1.20040317094128.01bb9950@mail.flinders.edu.au>
Message-ID: <000001c40bad$ac174370$78f05a99@msu.montana.edu>

Look at the function vegdist in the library vegan.

It does Bray-Curtis and other common ecological distance measures.

HTH, Andy



From dray at biomserv.univ-lyon1.fr  Wed Mar 17 00:28:43 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Tue, 16 Mar 2004 18:28:43 -0500
Subject: [R] bray-curtis?
In-Reply-To: <6.0.1.1.1.20040317094128.01bb9950@mail.flinders.edu.au>
Message-ID: <5.2.1.1.0.20040316182801.00b60df8@biomserv.univ-lyon1.fr>

see vegdist in vegan package

At 18:14 16/03/2004, Don Driscoll wrote:
>Does R have a function to calculate Bray-Curtis distance measures, which 
>is probably one of the most frequently used and recommended dissimilarity 
>measures in ecology?  It isn't mentioned in dist().
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From dmurdoch at pair.com  Wed Mar 17 00:57:26 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 16 Mar 2004 18:57:26 -0500
Subject: [R] save GUI window position in windows version?
In-Reply-To: <20040316231311.28006.qmail@web501.mail.yahoo.co.jp>
References: <20040316231311.28006.qmail@web501.mail.yahoo.co.jp>
Message-ID: <mn4f50p2nuc8bkrqtb4a6hu6ds6p4r0rkm@4ax.com>

On Wed, 17 Mar 2004 08:13:11 +0900 (JST), SI <risdpizza at yahoo.co.jp>
wrote :

>Hi,
>
>I'm using windows GUI version.  Is there any way 
>to save window position from previous session?Everytime I
>open GUI starts as maximized and I 
>have to resize it.

Set up the console the way you want it, then go to 

Edit|GUI preferences...

and save them to a file.  If you use the default file, it'll apply
every time you start Rgui.

To do the same for graphics windows, you can set up your own function
and set it to be the default graphics device function.  E.g.

> tinywindows <- function(...) windows(width=1, height=1, ...)
> options(device="tinywindows")
> plot(1,1)
Error in plot.new() : Figure margins too large

Duncan Murdoch



From andy_liaw at merck.com  Wed Mar 17 01:18:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Mar 2004 19:18:32 -0500
Subject: [R] graphical interface
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79DA@usrymx25.merck.com>

> From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
> On Tuesday 16 March 2004 09:52, Liaw, Andy wrote:
> > [Plead to the maintainer of lattice:  Is it possible to 
> make the theme
> > an option that can be set for the session, not just for the device?]
> 
> 
> I'm not completely sure if this is what you are looking for, 
> but the device  
> settings are ultimately determined by trellis.device(), which has a 
> 'theme' argument. It essentially has the effect of calling 
> lset() with 
> that theme immediately after the device is set up. This 
> 'theme' argument 
> defaults to getOption("lattice.theme"), which you can set to 
> modify the 
> default behaviour. From ?trellis.device:
> 
> 
>    theme: list of components that change the settings of the device
>           opened, or, a function that when called produces 
> such a list.
>           The function name can be supplied as a quoted string. A
>           possible usage is to change the default settings at session
>           startup, for example by setting 'options(lattice.theme =
>           "col.whitebg")'. If 'theme' is a function, it will not be
>           supplied any arguments, however, it is guaranteed that a
>           device will already be open when it is called, so 
> one may use
>           '.Device' inside the function to ascertain what device has
>           been opened.
> 
> 
> If this is indeed what you are looking for, I would 
> appreciate hearing 
> about any ideas to make this information easier to get at (I 
> guess I could 
> add a link from ?lset to ?trellis.device for starters).

Hallelujah!!  This is it!  I just have trouble knowing about new features in
packages that I had some familiarity with (especially those that I've gotten
too used too from my S-PLUS days...).  Thanks X 1e6!!

Cheers,
Andy

 
> Deepayan
> 
> 
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Wed Mar 17 02:16:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Mar 2004 20:16:04 -0500
Subject: [R] effect size
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79DB@usrymx25.merck.com>

> From: David A. van Leeuwen
> 
> Prof Brian Ripley wrote:
> 
> >I think you want to call summary.lm on the aov object, but 
> this depends on 
> >what you mean by `effect size'.
> >
> I guess this is what we wanted.
> 
> >Given that, I wonder if you are used to standard terminology.
> >  
> >
> No, I am not, unfortunately.  We are doing lots of 
> statistical analyses, 
> using R because it is fab and such, but reviewers are looking 
> for SPSS 
> output using terminology that we can't find in the R bundle---but our 
> general impression is that R does things way more cleverer and better 
> than click-until-you-seed-red-signifficant-effect tools found 
> elsewhere. 

I believe there is `effect size' in DOE, e.g., for two-level, main effect
only designs, effect sizes are something like 0.5*(mean(high) - mean(low)).
This is obviously not what you meant.
 
> Reactions on r-help caused us to request for a better 
> specification of 
> the `effect size' that people wanted, and it turns out to be
> $$ SS(effect) / \Sum SS $$ (SS being sums-of-squares).  To 
> me, a simple 
> physicist, that sounds as `the fraction of explained variance' by the 
> factor.  Looking at the formulas in help(summary.lm) is seems that 
> summary.lm()$r.squared  is exactly what we want (for a one-way aov).
> 
> Is there a way to quickly tabulate the expression $$ SS(effect) / 
> (\Sum_{effects} SS(effect) + SS(residuals)) $$ ?
> 
> The numbers are practically there in the summary.aov() table. 
>  Only the 
> grand total SS needs to be calculated.

I guess you want a partition of R^2 by factors in the model.  The only
situations where I can see this being sensible are:

- One-way classification; i.e., only one factor.
- Completely balanced design.

For designs with more than one factor, the balanceness guarantees the
orthogonality among the sums of squares, so it makes sense to partition R^2
that way.  Short of that, there's no unique way to compute the SS (that's
where the infamous 4 types of SS come from), and I don't see a sensible way
to go about it.

Think about the analogous situation in multiple linear regression:  unless
all predictor variables are orthogonal, there's no sensible way to compute
proportion of variance explained by one variable, because that variable is
correlated with other variables, and it's contribution to R^2 depends on
what other variables are in the model.

Andy 

 
> >For example, R does have an effects() function, and that 
> might be what you 
> >want.
> >
> >  
> >
> I don't really understand the effects()---it must be related to 
> coefficients() but it obviously is different. There is 
> model.tables.aov() which is also enlightening, but I think it 
> is really 
> the $ r^2 $ that we were looking for (our reviewers calling this an $ 
> \eta^2 $---if that clarifies things).
> 
> Thanks,
> 
> ---david
> 
> >On Mon, 15 Mar 2004, David A. van Leeuwen wrote:
> >
> >  
> >
> >>Having searched google '[R] aov effect size' without any results I 
> >>wonder if I not completely miss something. 
> >>
> >>Is there any R function that calculates the effect size of 
> an AOV's main 
> >>effect or interaction effect?  It should be related to the 
> F's and the 
> >>degree of freedom of the error, but the multitude in 
> numbers in aov() 
> >>baffle me a bit.
> >>    
> >>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andrewr at uidaho.edu  Wed Mar 17 02:39:42 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 16 Mar 2004 17:39:42 -0800
Subject: [R] effect size
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF79DB@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF79DB@usrymx25.merck.com>
Message-ID: <200403161739.42470.andrewr@uidaho.edu>

Thinking about effect sizes, a plausible alternative may be found in the 
relimp package.   

>From CRAN:  relimp: Relative Contribution of Effects in a Regression Model

Functions to facilitate inference on the relative importance of predictors in 
a linear or generalized linear model
    Version:	0.8-2
    Depends:	R (>= 1.8.0), tcltk, MASS
    Author:	David Firth
    Maintainer:	David Firth <d.firth at warwick.ac.uk>
    License:	GPL (version 2 or later)
URL:		http://www.warwick.ac.uk/go/relimp
		http://www.warwick.ac.uk/go/dfirth

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From andy_liaw at merck.com  Wed Mar 17 03:07:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Mar 2004 21:07:43 -0500
Subject: [R] effect size
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79DC@usrymx25.merck.com>

>From help(relimp, package="relimp"):

Details:

     If 'set1' and 'set2' both have length 1, relative importance is
     measured by the ratio of the two standardized coefficients.
     Equivalently this is the ratio of the standard deviations of the
     two contributions to the linear predictor, and this provides the
     generalization to comparing two sets rather than just a pair of
     predictors.

Doesn't look like what David want (he's not comparing factor to factor).

Andy

> From: Andrew Robinson [mailto:andrewr at uidaho.edu] 
> 
> Thinking about effect sizes, a plausible alternative may be 
> found in the 
> relimp package.   
> 
> From CRAN:  relimp: Relative Contribution of Effects in a 
> Regression Model
> 
> Functions to facilitate inference on the relative importance 
> of predictors in 
> a linear or generalized linear model
>     Version:	0.8-2
>     Depends:	R (>= 1.8.0), tcltk, MASS
>     Author:	David Firth
>     Maintainer:	David Firth <d.firth at warwick.ac.uk>
>     License:	GPL (version 2 or later)
> URL:		http://www.warwick.ac.uk/go/relimp
> 		http://www.warwick.ac.uk/go/dfirth
> 
> Andrew
> -- 
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : 
> http://www.uidaho.edu/~andrewr
> Moscow ID 83843                
>       Or: http://www.biometrics.uidaho.edu
> No statement above necessarily represents my employer's opinion.
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From mash at econs.umass.edu  Wed Mar 17 03:47:13 2004
From: mash at econs.umass.edu (Michael Ash)
Date: Tue, 16 Mar 2004 21:47:13 -0500 (EST)
Subject: [R] Re: 1.8.1 Make problem on SunOS
In-Reply-To: <Pine.GSO.4.55.0403140512110.6887@shell1.oit.umass.edu>
References: <Pine.GSO.4.55.0403140512110.6887@shell1.oit.umass.edu>
Message-ID: <Pine.GSO.4.55.0403162143290.6431@shell1.oit.umass.edu>

OK, I solved my problem in case anyone is still interested.

I have set noclobber (don't redirect ">" on top of an
existing file) in .bashrc.  The problem didn't clear up
until I commented the line out of .bashrc.  I tried turning
the option off in the shell in which I was attempting to
make R, but to no avail.  Only commenting the noclobber line
out of .bashrc resolved the problem.  Then "make" worked
like a charm.

It took way too long, but it's so satisfying when you crack
a problem like that.

Best,

Michael Ash
mash at econs.umass.edu

On Sun, 14 Mar 2004, Michael Ash wrote:

>
> I am trying to make R-1.8.1 on (SunOS shell1 5.8 Generic_108528-15 sun4u sparc SUNW,UltraAX-i2). I did
> ./configure
> make
> Configure output seems ok.  The make proceeds until the following line appears, repeated indefinitely (until I break):
>
> ./config.status: ./confstat28489-19881/subs.frag: cannot overwrite existing file
>
> I suspect that this may involve write permissions (and maybe
> the umask set in config.status). I am attempting the make in
> a directory where I have rwx permission.  (I've also tried
> this with "./configure --prefix=/home/me" with the same
> result.  I don't have superuser permission on the machine in
> question and am planning to install R to my home directory.)
>
> Any suggestions?
>
> Thank you very much.
>
> Best regards,
>
> Michael Ash, Assistant Professor
>   of Economics and Public Policy
> Department of Economics and CPPA
> University of Massachusetts
> Amherst, MA 01003
> Tel 413-545-6329 Fax 413-545-2921
> Email mash at econs.umass.edu
> http://people.umass.edu/maash



From ajayshah at mayin.org  Wed Mar 17 03:00:17 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 17 Mar 2004 07:30:17 +0530
Subject: [R] Plot 2 time series with different y axes (left and right)
Message-ID: <20040317020017.GA25085@igidr.ac.in>

Petr Pikal said:

> I am not really a R specialist but for this task I use function:

and he pasted his code into the email. I reindented the code, and
wrote a fragment to experiment with it. Here it is:

---------------------------------------------------------------------------

plot.yy <- function(x, yright, yleft,
                    yleftlim=NULL, yrightlim = NULL,
                    xlab = NULL, yylab=c("",""),
                    pch=c(1,2), col=c(1,2),
                    linky=F, smooth=0,
                    lwds=1, length=10, format="%d-%H:%M", ...
                    )
{
  par(mar=c(5,4,4,2),oma=c(0,0,0,3))

  plot(x, yright, ylim=yrightlim, axes=F,ylab="", xlab=xlab,
       pch=pch[1],col=col[1], ...)
  axis(4,pretty(range(yright,na.rm=T),10),col=col[1])
  if (linky) lines(x,yright,col=col[1], ...)
  if (smooth!=0) lines(supsmu(x,yright,span=smooth),col=col[1], lwd=lwds, ...)
  if(yylab[1]=="")
    mtext(deparse(substitute(yright)),side=4,outer=T,line=1, col=col[1], ...)
  else
    mtext(yylab[1],side=4,outer=T,line=1, col=col[1], ...)

  par(new=T)
  plot(x,yleft, ylim=yleftlim, ylab="", axes=F ,xlab=xlab,
       pch=pch[2],col=col[2], ...)
  box()
  axis(2,pretty(range(yleft,na.rm=T),10),col=col[2], col.axis=col[2])

  if (is.null(class(x)))
    axis(1,pretty(range(x,na.rm=T),10))
  else {
    l<-length(x)
    axis(1,at=x[seq(1,l,length=length)],
         labels=format(as.POSIXct(x[seq(1,l,length=length)]),
           format=format))
  }

  if(yylab[2]=="")
    mtext(deparse(substitute(yleft)),side=2,line=2, col=col[2], ...)
  else
    mtext(yylab[2],side=2,line=2, col=col[2], ...)
  if (linky)
    lines(x,yleft,col=col[2], lty=2, ...)
  if (smooth!=0)
    lines(supsmu(x,yleft,span=smooth),col=col[2], lty=2, lwd=lwds, ...)
}

# I wrote some code in order to try out the plot.yy() function --
x = 1990:2000;
y1 = 100*rnorm(11)
y2 = 200*rnorm(11)
plot.yy(x, y1, y2, yylab=c("Y axis 1", "Y axis 2"),
        linky=T, smooth=0,
        lwds=3, length=10)

---------------------------------------------------------------------------

Thanks! This gave me lots of ideas.

When I run this code, I get the error --

Error in as.POSIXct.default(x[seq(1, l, length = length)]) : 
  Don't know how to convert `x[seq(1, l, length = length)]' to class "POSIXct"
Execution halted

I also find that one of the two series is not `linked' (in your
notation). I also find that the y label "Y axis 2" does not appear.
Any ideas?

Thanks again,

          -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From jlozano at apoy.upm.edu.ph  Wed Mar 17 07:36:15 2004
From: jlozano at apoy.upm.edu.ph (jlozano@apoy.upm.edu.ph)
Date: Wed, 17 Mar 2004 14:36:15 +0800
Subject: [R] What library is coxreg?
Message-ID: <1079505375.4057f1df1130e@mail.upm.edu.ph>

Dear mailing list,

I'm trying to run the sample program about coxreg in the eha package.  But the
command is not recognized.  I tried the boot and also the survival library. 
Hope you can help me solve this little problem. 

Thank you.
Jei


----------------------------------------------------------------
This message was sent using IMP, the Internet Messaging Program.
University of the Philippines Manila (http://mail.upm.edu.ph)



From ozric at web.de  Wed Mar 17 08:25:27 2004
From: ozric at web.de (Christian Schulz)
Date: Wed, 17 Mar 2004 08:25:27 +0100
Subject: [R] R-business case
In-Reply-To: <40518E01.10608@tiscali.co.uk>
References: <40518E01.10608@tiscali.co.uk>
Message-ID: <200403170825.28916.ozric@web.de>

My humble idea from perspective of a "market researcher".
Counting the different mail-adresses with *.com
in the help-archives of the last years (nice Text-Mining-Archive) with a bot.

Ok, perhaps somebody like me use a freemail, so it's biased ,too.
Reasons may be different - my is a technical one.

christian


Am Freitag, 12. M?rz 2004 11:16 schrieb giovanni merola:
> Dear all,
> I am putting together a business case for R and I would like to know if
> there exist statistics on the distribution of R in academia, research
> and commercial institutions. The same information about S-plus would be
> welcome. Please reply only if you have consistent and certain information.
>
> Thank you very much, giovanni
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From bernd.weiss at uni-koeln.de  Wed Mar 17 08:31:46 2004
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Wed, 17 Mar 2004 08:31:46 +0100
Subject: [R] What library is coxreg?
In-Reply-To: <1079505375.4057f1df1130e@mail.upm.edu.ph>
Message-ID: <40580CF2.19038.3F7210@localhost>

On 17 Mar 2004 at 14:36, jlozano at apoy.upm.edu.ph wrote:

> Dear mailing list,
> 
> I'm trying to run the sample program about coxreg in the eha package. 
> But the command is not recognized.  I tried the boot and also the
> survival library. Hope you can help me solve this little problem. 
> 
> Thank you.
> Jei
> 

Hi Jei,

what's wrong with the package 'eha'? When I call  help.search("coxreg"), I get...

> help.search("coxreg")
Help files with alias or concept or title matching 'coxreg' using
fuzzy matching:



coxreg.fit(eha)         Cox regression
coxreg(eha)             Cox regression
print.coxreg(eha)       Prints coxreg objects
summary.coxreg(eha)     Prints coxreg objects
fit.lmc(gstat)          Fit a Linear Model of Coregionalization to a
                        Multivariable Sample Variogram
cox.zph(survival)       Test the Proportional Hazards Assumption of a
                        Cox Regression



Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.



Hope it helps,

Bernd
-- 
Bernd Weiss, M.A.
Universitaet zu Koeln / University of Cologne
Forschungsinstitut fuer Soziologie / Research Institute for Sociology
Greinstr. 2 / 50 939 Cologne / Germany
Phone: +49 221 / 470-4234
E-Mail: <bernd.weiss at uni-koeln.de>



From ripley at stats.ox.ac.uk  Wed Mar 17 08:46:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Mar 2004 07:46:24 +0000 (GMT)
Subject: [R] Re: 1.8.1 Make problem on SunOS
In-Reply-To: <Pine.GSO.4.55.0403162143290.6431@shell1.oit.umass.edu>
Message-ID: <Pine.LNX.4.44.0403170743080.12784-100000@gannet.stats>

Do you somehow have bash set as your shell for batch operations?
On that platform you get sh or ksh by default, and bash is not even 
installed by default.

If so, it was a crucial piece of information you haven't told us and is 
something to bear in mind if you need future help.

On Tue, 16 Mar 2004, Michael Ash wrote:

> OK, I solved my problem in case anyone is still interested.
> 
> I have set noclobber (don't redirect ">" on top of an
> existing file) in .bashrc.  The problem didn't clear up
> until I commented the line out of .bashrc.  I tried turning
> the option off in the shell in which I was attempting to
> make R, but to no avail.  Only commenting the noclobber line
> out of .bashrc resolved the problem.  Then "make" worked
> like a charm.
> 
> It took way too long, but it's so satisfying when you crack
> a problem like that.
> 
> Best,
> 
> Michael Ash
> mash at econs.umass.edu
> 
> On Sun, 14 Mar 2004, Michael Ash wrote:
> 
> >
> > I am trying to make R-1.8.1 on (SunOS shell1 5.8 Generic_108528-15 sun4u sparc SUNW,UltraAX-i2). I did
> > ./configure
> > make
> > Configure output seems ok.  The make proceeds until the following line appears, repeated indefinitely (until I break):
> >
> > ./config.status: ./confstat28489-19881/subs.frag: cannot overwrite existing file
> >
> > I suspect that this may involve write permissions (and maybe
> > the umask set in config.status). I am attempting the make in
> > a directory where I have rwx permission.  (I've also tried
> > this with "./configure --prefix=/home/me" with the same
> > result.  I don't have superuser permission on the machine in
> > question and am planning to install R to my home directory.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nusbj at hotmail.com  Wed Mar 17 09:10:25 2004
From: nusbj at hotmail.com (Z P)
Date: Wed, 17 Mar 2004 16:10:25 +0800
Subject: [R] GEE2
Message-ID: <Sea2-F20XDcrYDCuJg0000472b2@hotmail.com>


Dear all,

Is there any package in R can do the GEE2? Thanks.



From angel_lul at hotmail.com  Wed Mar 17 10:35:48 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 17 Mar 2004 09:35:48 +0000
Subject: [R] reading short int and float from binary connection
Message-ID: <40581BF4.1060503@hotmail.com>

Hi,
I have a binary file with mixture of short integers (2 bytes) and floats 
(4 bytes).
To get the data into R I use readBin reading each value one at a time 
(n=1) but it is giving me headaches, two questions:
-In the "what" option is there any difference between "integer" and "int"?
-To read the short int I use what="int" and size=2 and to read the 
floats I use what="double" and size=4, the strange thing is that 
sometimes it works but other times it reads wrong values.
Any clues where my mistake might be?
Thanks
Angel



From cristian at biometria.univr.it  Wed Mar 17 11:50:18 2004
From: cristian at biometria.univr.it (Cristian Pattaro)
Date: Wed, 17 Mar 2004 11:50:18 +0100
Subject: [R] Q: Odds Ratio
Message-ID: <40582D6A.8030008@biometria.univr.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040317/82910eb3/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Mar 17 12:11:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Mar 2004 12:11:06 +0100
Subject: [R] reading short int and float from binary connection
In-Reply-To: <40581BF4.1060503@hotmail.com>
References: <40581BF4.1060503@hotmail.com>
Message-ID: <4058324A.9020309@statistik.uni-dortmund.de>

Angel Lopez wrote:

> Hi,
> I have a binary file with mixture of short integers (2 bytes) and floats 
> (4 bytes).
> To get the data into R I use readBin reading each value one at a time 
> (n=1) but it is giving me headaches, two questions:
> -In the "what" option is there any difference between "integer" and "int"?
> -To read the short int I use what="int" and size=2 and to read the 
> floats I use what="double" and size=4, the strange thing is that 
> sometimes it works but other times it reads wrong values.
> Any clues where my mistake might be?
> Thanks
> Angel


Are the inetgers signed or not? You might want to set signed = FALSE for 
unsigned integers.
Are you sure you are reading each time from the beginning of the connection?
Are you really sure about the file format specifications?

Difficult to track down without your code, the file format's 
specification, and an example file..

Uwe Ligges



From angel_lul at hotmail.com  Wed Mar 17 13:31:46 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 17 Mar 2004 12:31:46 +0000
Subject: [R] reading short int and float from binary connection
In-Reply-To: <4058324A.9020309@statistik.uni-dortmund.de>
References: <40581BF4.1060503@hotmail.com>
	<4058324A.9020309@statistik.uni-dortmund.de>
Message-ID: <40584532.3080509@hotmail.com>

Thanks for the advise.
I think I've found the solution, the problem was that the file had been 
written from a C program using a structure containing floats and int, if 
I rewrite the C code not using a structure but independent floats and 
ints the data then is read smoothly into R. I still can't figure out why 
a structure is different or how should I have read it into R but it is 
probably more a C than an R question .
Thanks,
Angel

Uwe Ligges wrote:

> Angel Lopez wrote:
>
>> Hi,
>> I have a binary file with mixture of short integers (2 bytes) and 
>> floats (4 bytes).
>> To get the data into R I use readBin reading each value one at a time 
>> (n=1) but it is giving me headaches, two questions:
>> -In the "what" option is there any difference between "integer" and 
>> "int"?
>> -To read the short int I use what="int" and size=2 and to read the 
>> floats I use what="double" and size=4, the strange thing is that 
>> sometimes it works but other times it reads wrong values.
>> Any clues where my mistake might be?
>> Thanks
>> Angel
>
>
>
> Are the inetgers signed or not? You might want to set signed = FALSE 
> for unsigned integers.
> Are you sure you are reading each time from the beginning of the 
> connection?
> Are you really sure about the file format specifications?
>
> Difficult to track down without your code, the file format's 
> specification, and an example file..
>
> Uwe Ligges
>
> .
>



From bitwrit at ozemail.com.au  Wed Mar 17 06:20:06 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 17 Mar 2004 16:20:06 +1100
Subject: [R] R-business case
In-Reply-To: <XFMail.040316220210.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040316220210.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20040317113407.ZQKC14485.smta08.mail.ozemail.net@there>

Haven't there been one or two people who asked list members to submit 
peer-reviewed, published papers in which R was used for the analysis? I 
certainly have sent an email or two like this.

Jim



From bxc at steno.dk  Wed Mar 17 12:44:58 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 17 Mar 2004 12:44:58 +0100
Subject: [R] Q: Odds Ratio
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E150@exdkba022.novo.dk>

lr.mod <- glm( y ~ x + w, family=binomial )
exp( summary( lr.mod )$coef[,1:2] %*% rbind( c(1,1,1), 1.96*c(0,-1,-1) )
)

should do the job. 

Pack it in a function if you like, see e.g. the
(so far) undocumented function ci.lin in:
http://www.biostat.ku.dk/~bxc/R/ci.lin.R
(depends on)
http://www.biostat.ku.dk/~bxc/R/ci.mat.R

Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------





> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Cristian Pattaro
> Sent: Wednesday, March 17, 2004 11:50 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Q: Odds Ratio
> 
> 
> Dear all,
> 
> is there a automatic method to obtain Odds Ratio estimates and their 
> confidence intervals from a GLM Logistic model?
> 
> Thanks
> Cristian
> 
> =============================================
> Cristian Pattaro
> =============================================
> Unit of Epidemiology & Medical Statistics
> Department of Medicines and Public Health
> University of Verona
> cristian at biometria.univr.it 
> =============================================
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From r.knell at qmul.ac.uk  Wed Mar 17 12:53:39 2004
From: r.knell at qmul.ac.uk (Rob Knell)
Date: Wed, 17 Mar 2004 11:53:39 +0000
Subject: [R] ANCOVA when you don't know factor levels
Message-ID: <BAE0D95E-7809-11D8-BCDC-0003936DB7E2@qmul.ac.uk>

Hello people

I am doing some thinking about how to analyse data on dimorphic animals  
- where different individuals of the same species have rather different  
morphology. An example of this is that some male beetles have large  
horns and small wings, and rely on beating the other guys up to get  
access to mates, whereas others have smaller horns and larger wings,  
and rely on mobility to find mates before the guys with the big horns  
turn up and beat them up. Normally what we do to see if this is  
happening is to plot a scatter plot of horn length vs body size. If  
it's a simple straight line relationship then no dimorphism, but if  
it's either a line with an obvious change of slope or two separate  
lines then you've got a dimorphic animal.

If you've just got a change of slope then it's relatively easy to test  
for significance by breakpoint regression, which will also tell you the  
body size at which the beetles switch from 'big wings, small horns'  
strategy to the other. What is more problematic, however, is if you  
have a dataset that is essentially two linear regressions that overlap  
in both X and Y. Here you can't do a breakpoint regression. The  
solution that I have come up with is to put a straight line between the  
two clouds of data and to use that line to define a two-level factor  
and fit a model with body size, the factor and the interaction to the  
horn size data. The line can then be moved up and down, and the slope  
varied, and the fit of the model for each intercept/slope combination  
compared. The best fit model can then be compared with a straight  
linear model with an F-test, which gives you a significance test, and  
the line allows you to decide which morph a particular beetle conforms  
to. I've written some R code to do this (see below). What i would like  
to know is a) if this problem has been dealt with before elsewhere, b)  
if there's a better way to do it and c) if my R function could be  
improved at all - it's my first attempt at such a thing, so any input  
would be really helpful. If anyone wants a sample data set then I can  
email you one.

Thanks for any input

Rob Knell

School of Biological Sciences
Queen Mary, University of London

'Phone +44 (0)20 7882 7720
http://www.qmw.ac.uk/~ugbt794
http://www.mopane.org

Here's the code:

switchline<- 
function(X,Y,Intmin,Intmax,Intgap,Slopemin,Slopemax,Slopegap)

 

{

 

#   X       = unimodal variable (e.g. filename$bodysize)

 

#   Y       = bimodal variable (e.g. filename$hornsize)

 

#    Intmin       = Minimum switchline intercept

 

#    Slopemin = Minimum switchline slope

#    Intmax       = Maximum switchline intercept

#    Slopemax = Maximum switchline slope

#    Intgap       = Interval between each intercept

#    Slopegap = Interval between each slope

 

 

 

#   This function written by Rob Knell 2003. It is an attempt to  
produce an

#    objective analysis for datasets of apparently dimorphic characters  
in which

#   there is overlap in both the X and Y variable. The routine divides  
the

#    individuals into one of two classes based on whether they are  
above or below

#   a line, and fits a linear model to the Y variable with the X  
variable as a

#    continuous explanatory variable and a single factor, "Morph",  
which is based

#   on whether they were above the line or below it, plus an  
interaction term.

#   The line is then adjusted, the individuals reclassified and the  
process

#    repeated. The output is of the form "Int" - intercept of the line  
dividing

#   one morph from another, "Slp" - the slope and "Rsqr", which gives  
the R-

#    squared value for the fitted model when the division into factors  
is based on

#   that particular line.

 

#   An error message probably means that one of your lines has missed  
the data

#    altogether - in other words, all of your data points are  
classified into a

#    single factor.

#

 

 

 

Lines.grid<-  
expand.grid(Intercept=seq(Intmin,Intmax,Intgap),Slope=seq(Slopemin,Slope 
max,Slopegap))

 

bandwidth<-length(Lines.grid$Intercept)

 

 

Lines<-matrix(nrow=bandwidth,ncol=3,data=NA)

dimnames(Lines)<-list(seq(1,bandwidth),c("Int","Slp","Rsqr"))

 

Lines[,1]<-Lines.grid$Intercept

Lines[,2]<-Lines.grid$Slope

 

for (i in 1:bandwidth)

 

    {

    temp.data<-matrix(nrow=length(X),ncol=4,data=NA)

    colnames(temp.data)<-c("X1","Y1","switchvalue","Morph")

    temp.data[,1]<-X

    temp.data[,2]<-Y

        for(j in 1:length(X))

        {

        temp.data[j,3]<-(Lines[i,1]+Lines[i,2]*temp.data[j,1])

        ifelse(temp.data[j,2]>=temp.data[j,3], temp.data[j,4]<-1,  
        temp.data[j,4]<-2)

        }

    

    temp.data<-data.frame(temp.data)

    temp.data$Morph<-factor(temp.data$Morph)

    model<-lm(temp.data$Y1~temp.data$X1*temp.data$Morph)

    Lines[i,3]<-round(summary(model)$r.squared,6)

    }

 

print(Lines)

 

Lines<-data.frame(Lines)

Max<-max(Lines$Rsqr)

MaxI<-Lines$Int[Lines$Rsqr==Max]

MaxS<-Lines$Slp[Lines$Rsqr==Max]

Maxmat<-cbind(MaxI,MaxS)

 

print(Maxmat)

}

                       



From simon at stats.gla.ac.uk  Tue Mar 16 12:00:52 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 16 Mar 2004 11:00:52 +0000 (GMT)
Subject: [R] [R-pkgs] mgcv 1.0 
Message-ID: <Pine.SOL.4.58.0403161058130.29943@moon.stats.gla.ac.uk>

mgcv 1.0 (package providing gams etc) will be released with R 1.9.0.
(R 1.8.x compatible versions can be found at:
http://www.stats.gla.ac.uk/~simon/simon/mgcv.html)
There are quite a few changes from mgcv 0.9: hence this message.

The main new features are:

* A generalized additive mixed modelling function `gamm' (which uses lme
  from the nlme library of glmmPQL from the MASS library for fitting).

* Tensor product smooths as an alternative way of representing smooths of
  more than one variable. These are useful when isotropic smoothing is
  inappropriate. Tensor products of any available smooth can be used.
  See ?te

* An object oriented approach to smooth terms that allows new classes of
  smooths to be added easily. See ?smooth.construct for details and
  example R code implementing `p-splines'.

* A built in cyclic smooth class for terms where the beginning is the
  end...

Partly as a result of the above the code has undergone some fairly major
re-organization, which means that ...

1. The new version is not fully back compatible with previous versions.
   For example version 1.0 plotting or summary functions will not work with
   gam objects from previous versions of mgcv.

2. There's likely to be the odd teething problem - please let me know
   about them so they can get fixed.

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From pxt at ph.adfa.edu.au  Wed Mar 17 13:50:42 2004
From: pxt at ph.adfa.edu.au (Pisut Tempatarachoke)
Date: Wed, 17 Mar 2004 23:50:42 +1100
Subject: [R] R-1.8.1-4: Font family, ticks and mathematical expression??
Message-ID: <405849A2.2050506@ph.adfa.edu.au>

Hi all,

Could anyone help answer the following questions, please?
(I'm using R-1.8.1-4 on Fedora Core 1 and very new to R)

(i) Is it possible to specify a font family (e.g. courier or helvetica) 
when graphing?

(ii) How can I make ticks point inwards on all four sides of a plot?  Is 
it possible to have minor and major ticks?

(iii) How would I specify a symbol "\sim" (i.e. "~") in a mathematical 
expression?  So far, I've only seen "%~~%", but that's not exactly what 
I'm after.  (I tried "%~%" but that didn't work.)

Thanks so much in advance, everyone.

Regards
Tempo



From MailMonitor_on_BC1 at barker.nsw.edu.au  Wed Mar 17 13:49:38 2004
From: MailMonitor_on_BC1 at barker.nsw.edu.au (MailMonitor on BC1)
Date: Wed, 17 Mar 2004 23:49:38 +1100
Subject: [R] [Virus detected]
Message-ID: <OFD3CAB9CB.096E79E1-ONCA256E5A.0046769A@nsw.edu.au>

Sophos Plc MailMonitor for Domino/D R1.0(4.003c)
Server: BC1
-----------------------------------------------------------------------

Your email contained infected attachment(s).  For advice consult your
system administrator.

-----------------------------------------------------------------------
Mail-Info

From:       r-help at stat.math.ethz.ch
To:         ndrews at barker.nsw.edu.au
Rec.: ndrews at barker.nsw.edu.au
Date:       17/03/2004 23:48:55
Subject:    Re: Your text

-----------------------------------------------------------------------
File: [your_text.pif]   State: [file contains virus]



From v_spiesser at yahoo.fr  Wed Mar 17 13:48:47 2004
From: v_spiesser at yahoo.fr (=?iso-8859-1?q?vincent=20spiesser?=)
Date: Wed, 17 Mar 2004 13:48:47 +0100 (CET)
Subject: [R] library tcltk
Message-ID: <20040317124847.95859.qmail@web14914.mail.yahoo.com>

Hi,
At the moment, I am working with the "tcltk" library.

I wonder if it is possible to create a window, destroy
it and open it a new time.

For example

tt <- tktoplevel()
 #... instructions
tkdestroy(tt)
 #... other instructions

A this point, I would like to open tt from the tt
object. Is it possible ?

Thanks.

Vincent Spiesser



From andy_liaw at merck.com  Wed Mar 17 14:06:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 08:06:11 -0500
Subject: [R] ANCOVA when you don't know factor levels
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79E0@usrymx25.merck.com>

This sounds like a good case for mixture regression, for which there's Fritz
Leisch's `flexmix' package.  However, I don't think flexmix has facility for
testing whether the mixture has one vs. two components.  Others on the list
surely would know more than I do.

HTH,
Andy

> From: Rob Knell
> 
> Hello people
> 
> I am doing some thinking about how to analyse data on 
> dimorphic animals  
> - where different individuals of the same species have rather 
> different  
> morphology. An example of this is that some male beetles have large  
> horns and small wings, and rely on beating the other guys up to get  
> access to mates, whereas others have smaller horns and larger wings,  
> and rely on mobility to find mates before the guys with the 
> big horns  
> turn up and beat them up. Normally what we do to see if this is  
> happening is to plot a scatter plot of horn length vs body size. If  
> it's a simple straight line relationship then no dimorphism, but if  
> it's either a line with an obvious change of slope or two separate  
> lines then you've got a dimorphic animal.
> 
> If you've just got a change of slope then it's relatively 
> easy to test  
> for significance by breakpoint regression, which will also 
> tell you the  
> body size at which the beetles switch from 'big wings, small horns'  
> strategy to the other. What is more problematic, however, is if you  
> have a dataset that is essentially two linear regressions 
> that overlap  
> in both X and Y. Here you can't do a breakpoint regression. The  
> solution that I have come up with is to put a straight line 
> between the  
> two clouds of data and to use that line to define a two-level factor  
> and fit a model with body size, the factor and the 
> interaction to the  
> horn size data. The line can then be moved up and down, and 
> the slope  
> varied, and the fit of the model for each intercept/slope 
> combination  
> compared. The best fit model can then be compared with a straight  
> linear model with an F-test, which gives you a significance 
> test, and  
> the line allows you to decide which morph a particular beetle 
> conforms  
> to. I've written some R code to do this (see below). What i 
> would like  
> to know is a) if this problem has been dealt with before 
> elsewhere, b)  
> if there's a better way to do it and c) if my R function could be  
> improved at all - it's my first attempt at such a thing, so 
> any input  
> would be really helpful. If anyone wants a sample data set 
> then I can  
> email you one.
> 
> Thanks for any input
> 
> Rob Knell
> 
> School of Biological Sciences
> Queen Mary, University of London
> 
> 'Phone +44 (0)20 7882 7720
> http://www.qmw.ac.uk/~ugbt794
> http://www.mopane.org
> 
> Here's the code:
> 
> switchline<- 
> function(X,Y,Intmin,Intmax,Intgap,Slopemin,Slopemax,Slopegap)
> 
>  
> 
> {
> 
>  
> 
> #   X       = unimodal variable (e.g. filename$bodysize)
> 
>  
> 
> #   Y       = bimodal variable (e.g. filename$hornsize)
> 
>  
> 
> #    Intmin       = Minimum switchline intercept
> 
>  
> 
> #    Slopemin = Minimum switchline slope
> 
> #    Intmax       = Maximum switchline intercept
> 
> #    Slopemax = Maximum switchline slope
> 
> #    Intgap       = Interval between each intercept
> 
> #    Slopegap = Interval between each slope
> 
>  
> 
>  
> 
>  
> 
> #   This function written by Rob Knell 2003. It is an attempt to  
> produce an
> 
> #    objective analysis for datasets of apparently dimorphic 
> characters  
> in which
> 
> #   there is overlap in both the X and Y variable. The 
> routine divides  
> the
> 
> #    individuals into one of two classes based on whether they are  
> above or below
> 
> #   a line, and fits a linear model to the Y variable with the X  
> variable as a
> 
> #    continuous explanatory variable and a single factor, "Morph",  
> which is based
> 
> #   on whether they were above the line or below it, plus an  
> interaction term.
> 
> #   The line is then adjusted, the individuals reclassified and the  
> process
> 
> #    repeated. The output is of the form "Int" - intercept of 
> the line  
> dividing
> 
> #   one morph from another, "Slp" - the slope and "Rsqr", 
> which gives  
> the R-
> 
> #    squared value for the fitted model when the division 
> into factors  
> is based on
> 
> #   that particular line.
> 
>  
> 
> #   An error message probably means that one of your lines 
> has missed  
> the data
> 
> #    altogether - in other words, all of your data points are  
> classified into a
> 
> #    single factor.
> 
> #
> 
>  
> 
>  
> 
>  
> 
> Lines.grid<-  
> expand.grid(Intercept=seq(Intmin,Intmax,Intgap),Slope=seq(Slop
> emin,Slope 
> max,Slopegap))
> 
>  
> 
> bandwidth<-length(Lines.grid$Intercept)
> 
>  
> 
>  
> 
> Lines<-matrix(nrow=bandwidth,ncol=3,data=NA)
> 
> dimnames(Lines)<-list(seq(1,bandwidth),c("Int","Slp","Rsqr"))
> 
>  
> 
> Lines[,1]<-Lines.grid$Intercept
> 
> Lines[,2]<-Lines.grid$Slope
> 
>  
> 
> for (i in 1:bandwidth)
> 
>  
> 
>     {
> 
>     temp.data<-matrix(nrow=length(X),ncol=4,data=NA)
> 
>     colnames(temp.data)<-c("X1","Y1","switchvalue","Morph")
> 
>     temp.data[,1]<-X
> 
>     temp.data[,2]<-Y
> 
>         for(j in 1:length(X))
> 
>         {
> 
>         temp.data[j,3]<-(Lines[i,1]+Lines[i,2]*temp.data[j,1])
> 
>         ifelse(temp.data[j,2]>=temp.data[j,3], temp.data[j,4]<-1,  
>         temp.data[j,4]<-2)
> 
>         }
> 
>     
> 
>     temp.data<-data.frame(temp.data)
> 
>     temp.data$Morph<-factor(temp.data$Morph)
> 
>     model<-lm(temp.data$Y1~temp.data$X1*temp.data$Morph)
> 
>     Lines[i,3]<-round(summary(model)$r.squared,6)
> 
>     }
> 
>  
> 
> print(Lines)
> 
>  
> 
> Lines<-data.frame(Lines)
> 
> Max<-max(Lines$Rsqr)
> 
> MaxI<-Lines$Int[Lines$Rsqr==Max]
> 
> MaxS<-Lines$Slp[Lines$Rsqr==Max]
> 
> Maxmat<-cbind(MaxI,MaxS)
> 
>  
> 
> print(Maxmat)
> 
> }
> 
>                        
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Mar 17 14:07:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Mar 2004 13:07:23 +0000 (GMT)
Subject: [R] R-1.8.1-4: Font family, ticks and mathematical expression??
In-Reply-To: <405849A2.2050506@ph.adfa.edu.au>
Message-ID: <Pine.LNX.4.44.0403171256210.6357-100000@gannet.stats>

On Wed, 17 Mar 2004, Pisut Tempatarachoke wrote:

> Hi all,
> 
> Could anyone help answer the following questions, please?
> (I'm using R-1.8.1-4 on Fedora Core 1 and very new to R)
> 
> (i) Is it possible to specify a font family (e.g. courier or helvetica) 
> when graphing?

`when graphing' means what?  Some graphics devices, e.g. postscript and
pdf support families.  X11() does not in 1.8.1 but will in 1.9.0, due
early April.

> (ii) How can I make ticks point inwards on all four sides of a plot?  Is 
> it possible to have minor and major ticks?

Their height (including direction) is controlled by par's tcl and tck --
this is in `An Introduction to R'. See ?par.

You cna get major and minor ticks by calling axis twice with different par 
values, if I understand you aright.

> (iii) How would I specify a symbol "\sim" (i.e. "~") in a mathematical 
> expression?  So far, I've only seen "%~~%", but that's not exactly what 
> I'm after.  (I tried "%~%" but that didn't work.)

Try demo(plotmath) for what is supported.  \sim is just a character in the 
symbol font, but I don't know how to access those directly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dlc at mbx.unige.ch  Wed Mar 17 14:09:12 2004
From: dlc at mbx.unige.ch (dlc@mbx.unige.ch)
Date: Wed, 17 Mar 2004 14:09:12 +0100
Subject: [R] library tcltk
In-Reply-To: <20040317124847.95859.qmail@web14914.mail.yahoo.com>
References: <20040317124847.95859.qmail@web14914.mail.yahoo.com>
Message-ID: <48A731B0-7814-11D8-9CD8-0003931DD6AE@local>


hello,
try:

tt=tktoplevel()
#...
tkwm.state(tt,"withdrawn")
# the tt window disapperas but is not destroyed
#...
tkwm.state(tt,"normal")
# the tt window appears again.

hope this help,
DLC

> Hi,
> At the moment, I am working with the "tcltk" library.
>
> I wonder if it is possible to create a window, destroy
> it and open it a new time.
>
> For example
>
> tt <- tktoplevel()
>  #... instructions
> tkdestroy(tt)
>  #... other instructions
>
> A this point, I would like to open tt from the tt
> object. Is it possible ?
>
> Thanks.
>
> Vincent Spiesser
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mash at econs.umass.edu  Wed Mar 17 14:21:49 2004
From: mash at econs.umass.edu (Michael Ash)
Date: Wed, 17 Mar 2004 08:21:49 -0500 (EST)
Subject: [R] Re: 1.8.1 Make problem on SunOS
In-Reply-To: <Pine.LNX.4.44.0403170743080.12784-100000@gannet.stats>
References: <Pine.LNX.4.44.0403170743080.12784-100000@gannet.stats>
Message-ID: <Pine.GSO.4.55.0403170815150.17159@shell1.oit.umass.edu>


On Wed, 17 Mar 2004, Prof Brian Ripley wrote:

> Do you somehow have bash set as your shell for batch
> operations? On that platform you get sh or ksh by default,
> and bash is not even installed by default.

I didn't know that bash was set as the shell for batch
operations and it wouldn't have occurred to me to check.  I
am a user without superuser privileges on this computer.  I
did use chsh to set bash as the shell for terminals, but I
have no idea if that operation sets sets bash as the shell
for batch operations.  Does anyone know how I can tell if I
have [accidentally] set bash as the shell for batch
operations or if this is a system default?  (And how to
change the shell for batch operations?)

> If so, it was a crucial piece of information you haven't
> told us and is something to bear in mind if you need
> future help.

True, but it wasn't obvious that was the missing piece of
crucial info.  The key evidence was:

./config.status: ./confstat28489-19881/subs.frag: cannot overwrite existing file

which pointed to noclobber, and then I got the answer by
trial and error.  I had no idea .bashrc would affect batch
operations or that I should check which shell was set for
batch operations.  Perhaps an addition to the R installation
notes...

Best regards,
Michael Ash



> On Tue, 16 Mar 2004, Michael Ash wrote:
>
> > OK, I solved my problem in case anyone is still interested.
> >
> > I have set noclobber (don't redirect ">" on top of an
> > existing file) in .bashrc.  The problem didn't clear up
> > until I commented the line out of .bashrc.  I tried turning
> > the option off in the shell in which I was attempting to
> > make R, but to no avail.  Only commenting the noclobber line
> > out of .bashrc resolved the problem.  Then "make" worked
> > like a charm.
> >
> > It took way too long, but it's so satisfying when you crack
> > a problem like that.
> >
> > Best,
> >
> > Michael Ash
> > mash at econs.umass.edu
> >
> > On Sun, 14 Mar 2004, Michael Ash wrote:
> >
> > >
> > > I am trying to make R-1.8.1 on (SunOS shell1 5.8 Generic_108528-15 sun4u sparc SUNW,UltraAX-i2). I did
> > > ./configure
> > > make
> > > Configure output seems ok.  The make proceeds until the following line appears, repeated indefinitely (until I break):
> > >
> > > ./config.status: ./confstat28489-19881/subs.frag: cannot overwrite existing file
> > >
> > > I suspect that this may involve write permissions (and maybe
> > > the umask set in config.status). I am attempting the make in
> > > a directory where I have rwx permission.  (I've also tried
> > > this with "./configure --prefix=/home/me" with the same
> > > result.  I don't have superuser permission on the machine in
> > > question and am planning to install R to my home directory.)
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From e.pebesma at geog.uu.nl  Wed Mar 17 14:51:49 2004
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Wed, 17 Mar 2004 14:51:49 +0100
Subject: [R] graphical interface -- gstat/lattice
Message-ID: <405857F5.20804@geog.uu.nl>

Carlos, what you see are the lattice default settings;
use trellis.par.get/set to get and change them:

library(gstat) # requires lattice
data(meuse)
trellis.par.set("background", list(col="#ffffff"))
trellis.par.set("plot.symbol", list(cex=1,col="#000000",font=1,pch="+"))
plot(variogram(zinc~1,~x+y,meuse))

for more help, try:
?Lattice
?trellis.par.set

Please use a more descriptive subject, next time.

Best regards,
--
Edzer



From dmurdoch at pair.com  Wed Mar 17 15:02:51 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 17 Mar 2004 09:02:51 -0500
Subject: [R] R-business case
In-Reply-To: <40577492.6090507@pdf.com>
References: <40518E01.10608@tiscali.co.uk>
	<40576EAC.2030806@indigoindustrial.co.nz>
	<40577492.6090507@pdf.com>
Message-ID: <t3mg50lq3elic358qsobm4u4fuogf328m5@4ax.com>

On Tue, 16 Mar 2004 13:41:38 -0800, Spencer Graves
<spencer.graves at pdf.com> wrote :

>      Some web sites have "hit counters".  It should be possible to get 
>a counts of the numbers of times different parts of R are downloaded.  
>Do the CRAN web sites include any such? 

The logs aren't currently available for general viewing, but I took a
look at what we do have, and it shows rw1081.exe (the Windows binary
build of R 1.8.1) being downloaded from the main CRAN site about 28000
times.  Not all of those downloads were successful; they average
around 10 Meg each, and the file is 22 Meg in size.  On the other
hand, traffic to the mirrors isn't included at all.

Duncan Murdoch



From j.zutt at ewi.tudelft.nl  Wed Mar 17 15:24:49 2004
From: j.zutt at ewi.tudelft.nl (Jonne Zutt)
Date: Wed, 17 Mar 2004 15:24:49 +0100
Subject: [R] mva :: prcomp
Message-ID: <1079533489.2150.63.camel@dutiih.twi.tudelft.nl>

Dear R-list users,

I'm new to principal components and factor analysis.
I thought this method can be very useful for me to find relationships
between several variables (which I know there is, only don't know which
variables exactly and what kind of relation), so as a structure
detection method.

Now, I'm experimenting with the function prcomp from the mva package.
In my source code below, I of course expect one of the column to be
useless (I provided one duplicate column). I know both avg.EDGE.etc and
avg.DEGREE have a relation with sum.delivery.penalty.
E.g. the bigger avg.DEGREE, the smaller sum.delivery.penalty.

My question is about the output of prcomp.
I understand the cumulative proportion of variance of the third
principal component is 100%. Just like I expected.
I see the components are sorted. The one that explains the most variance
is listed first.

But, how can I figure out what these principal components are exactly?
For example PC1. Was is the exact meaning of it?
I assumed it is some linear combination of the variables I provided in
the call to prcomp, but how can i obtain this linear combination?

ps > i used http://www.statsoftinc.com/textbook/stfacan.html as a
reference, and help(prcomp/princomp) of course.

Thanks for any help!
Jonne.


# Read a table
dir = "..."
file = "..." # huge file, 12 Mb
stats = read.table(paste(dir, file, sep=""), header=TRUE)

# Select several columns
data = subset(stats, select =
         c(sum.delivery.penalty,
           avg.EDGE.IN.SHORTEST.PATH.COUNT,
           avg.EDGE.IN.SHORTEST.PATH.COUNT,
           avg.DEGREE))

require(mva)
pc2 = prcomp(data, retx = TRUE, center = TRUE,
             scale. = TRUE, tol = NULL)
pc2
summary(pc2)

--- gives the following output

> pc2
Standard deviations:
[1] 1.424074e+00 1.000000e-00 9.859080e-01 5.711682e-17

Rotation:
                                            PC1           PC2          
PC3
sum.delivery.penalty              -1.627945e-01 -1.539887e-12 
9.866600e-01
avg.EDGE.IN.SHORTEST.PATH.COUNT   -6.976740e-01  2.413866e-16
-1.151131e-01
avg.EDGE.IN.SHORTEST.PATH.COUNT.1 -6.976740e-01  2.013413e-17
-1.151131e-01
avg.DEGREE                         2.505027e-13 -1.000000e+00
-1.519375e-12
                                            PC4
sum.delivery.penalty              -1.118300e-17
avg.EDGE.IN.SHORTEST.PATH.COUNT    7.071068e-01
avg.EDGE.IN.SHORTEST.PATH.COUNT.1 -7.071068e-01
avg.DEGREE                        -3.253830e-18
> summary(pc2)
Importance of components:
                         PC1   PC2   PC3      PC4
Standard deviation     1.424 1.000 0.986 5.71e-17
Proportion of Variance 0.507 0.250 0.243 0.00e+00
Cumulative Proportion  0.507 0.757 1.000 1.00e+00



From hendry at cs.helsinki.fi  Wed Mar 17 15:55:19 2004
From: hendry at cs.helsinki.fi (Kai Hendry)
Date: Wed, 17 Mar 2004 16:55:19 +0200
Subject: [R] Frequency table
Message-ID: <20040317145519.GD5021@cs.helsinki.fi>

This must be FAQ, but I can't find it in archives or with a site search.

I am trying to construct a frequency table. I guess this should be done with
table. Or perhaps factor and split. Or prop.table. cut? findInterval? Argh!

Please correct me if what I am looking for is not called a "frequency table".
Perhaps it's called grouped data.

> zz$x9
 [1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 64 75 58 60 56 60
[26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 72 87 52 72 80 85
[51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84

I (think) I want it to look like:

40-49   2
50-59   15
60-69   20
70-79   19
80-89   12
90-99   2

Or the other way around with transpose.

classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
For the rownames

sum(zz$x9 > 40 & zz$x9 < 50)
For getting frequency counts is very laborious...

I got this far:
> table(cut(zz$x9, brk))

 (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
       2       19       21       19        8        1
> brk
[1]  40  50  60  70  80  90 100
> 
> t(table(cut(zz$x9, brk)))
     (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
[1,]  2      19      21      19       8       1

Still feels a million miles off.

Now I could do with a little help please after spending a couple of hours
working this out.



From lmsilva at fe.up.pt  Wed Mar 17 15:59:57 2004
From: lmsilva at fe.up.pt (lmsilva@fe.up.pt)
Date: Wed, 17 Mar 2004 14:59:57 +0000
Subject: [R] projection pursuit
Message-ID: <1079535597.405867ed8948a@webmail.fe.up.pt>


Dear helpers

Does R have a package that performs projection pursuit density estimation? Or 
anyone knows code in Matlab or C for example to do this?

Thank you all

Luis



From ramasamy at cancer.org.uk  Wed Mar 17 16:06:22 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 17 Mar 2004 15:06:22 -0000
Subject: [R] R-business case
In-Reply-To: <t3mg50lq3elic358qsobm4u4fuogf328m5@4ax.com>
Message-ID: <ODEPICOHNDBJEHIFCIMPCEHKCBAA.ramasamy@cancer.org.uk>

I like the previous suggestion of counting the number of unique e-mails in
the archive.

Another useful thing would be to count and plot the growth of number of R
(and Bioconductor) packages over the years. I know not all packages are
created equal.

Regards, Adai.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Duncan Murdoch
> Sent: 17 March 2004 14:03
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch; giovanni merola
> Subject: Re: [R] R-business case
>
>
> On Tue, 16 Mar 2004 13:41:38 -0800, Spencer Graves
> <spencer.graves at pdf.com> wrote :
>
> >      Some web sites have "hit counters".  It should be possible to get
> >a counts of the numbers of times different parts of R are downloaded.
> >Do the CRAN web sites include any such?
>
> The logs aren't currently available for general viewing, but I took a
> look at what we do have, and it shows rw1081.exe (the Windows binary
> build of R 1.8.1) being downloaded from the main CRAN site about 28000
> times.  Not all of those downloads were successful; they average
> around 10 Meg each, and the file is 22 Meg in size.  On the other
> hand, traffic to the mirrors isn't included at all.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Mar 17 16:14:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 10:14:09 -0500
Subject: [R] Frequency table
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79EA@usrymx25.merck.com>

I guess you want something like:

table(cut(zz$x9, c(-Inf, seq(40, 90, by=10), Inf)))

HTH,
Andy

> From: Kai Hendry
> 
> This must be FAQ, but I can't find it in archives or with a 
> site search.
> 
> I am trying to construct a frequency table. I guess this 
> should be done with
> table. Or perhaps factor and split. Or prop.table. cut? 
> findInterval? Argh!
> 
> Please correct me if what I am looking for is not called a 
> "frequency table".
> Perhaps it's called grouped data.
> 
> > zz$x9
>  [1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 
> 64 75 58 60 56 60
> [26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 
> 72 87 52 72 80 85
> [51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84
> 
> I (think) I want it to look like:
> 
> 40-49   2
> 50-59   15
> 60-69   20
> 70-79   19
> 80-89   12
> 90-99   2
> 
> Or the other way around with transpose.
> 
> classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
> For the rownames
> 
> sum(zz$x9 > 40 & zz$x9 < 50)
> For getting frequency counts is very laborious...
> 
> I got this far:
> > table(cut(zz$x9, brk))
> 
>  (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
>        2       19       21       19        8        1
> > brk
> [1]  40  50  60  70  80  90 100
> > 
> > t(table(cut(zz$x9, brk)))
>      (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
> [1,]  2      19      21      19       8       1
> 
> Still feels a million miles off.
> 
> Now I could do with a little help please after spending a 
> couple of hours
> working this out.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ramasamy at cancer.org.uk  Wed Mar 17 16:16:01 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 17 Mar 2004 15:16:01 -0000
Subject: [R] Frequency table
In-Reply-To: <20040317145519.GD5021@cs.helsinki.fi>
Message-ID: <ODEPICOHNDBJEHIFCIMPMEHKCBAA.ramasamy@cancer.org.uk>

?data.frame

data.frame( table(cut(x, seq(0, 1, by=0.1))) )

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kai Hendry
> Sent: 17 March 2004 14:55
> To: r-help at stat.math.ethz.ch
> Subject: [R] Frequency table
> 
> 
> This must be FAQ, but I can't find it in archives or with a site search.
> 
> I am trying to construct a frequency table. I guess this should 
> be done with
> table. Or perhaps factor and split. Or prop.table. cut? 
> findInterval? Argh!
> 
> Please correct me if what I am looking for is not called a 
> "frequency table".
> Perhaps it's called grouped data.
> 
> > zz$x9
>  [1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 64 
> 75 58 60 56 60
> [26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 72 
> 87 52 72 80 85
> [51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84
> 
> I (think) I want it to look like:
> 
> 40-49   2
> 50-59   15
> 60-69   20
> 70-79   19
> 80-89   12
> 90-99   2
> 
> Or the other way around with transpose.
> 
> classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
> For the rownames
> 
> sum(zz$x9 > 40 & zz$x9 < 50)
> For getting frequency counts is very laborious...
> 
> I got this far:
> > table(cut(zz$x9, brk))
> 
>  (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
>        2       19       21       19        8        1
> > brk
> [1]  40  50  60  70  80  90 100
> > 
> > t(table(cut(zz$x9, brk)))
>      (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
> [1,]  2      19      21      19       8       1
> 
> Still feels a million miles off.
> 
> Now I could do with a little help please after spending a couple of hours
> working this out.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Mar 17 16:25:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Mar 2004 16:25:41 +0100
Subject: [R] Frequency table
In-Reply-To: <20040317145519.GD5021@cs.helsinki.fi>
References: <20040317145519.GD5021@cs.helsinki.fi>
Message-ID: <40586DF5.2050809@statistik.uni-dortmund.de>

Kai Hendry wrote:

> This must be FAQ, but I can't find it in archives or with a site search.
> 
> I am trying to construct a frequency table. I guess this should be done with
> table. Or perhaps factor and split. Or prop.table. cut? findInterval? Argh!
> 
> Please correct me if what I am looking for is not called a "frequency table".
> Perhaps it's called grouped data.
> 
> 
>>zz$x9
> 
>  [1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 64 75 58 60 56 60
> [26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 72 87 52 72 80 85
> [51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84
> 
> I (think) I want it to look like:
> 
> 40-49   2
> 50-59   15
> 60-69   20
> 70-79   19
> 80-89   12
> 90-99   2
> 
> Or the other way around with transpose.
> 
> classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
> For the rownames
> 
> sum(zz$x9 > 40 & zz$x9 < 50)
> For getting frequency counts is very laborious...
> 
> I got this far:
> 
>>table(cut(zz$x9, brk))


  table(cut(zz$x9, brk, right = FALSE))

should do the trick.

Uwe Ligges


> 
>  (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
>        2       19       21       19        8        1
> 
>>brk
> 
> [1]  40  50  60  70  80  90 100
> 
>>t(table(cut(zz$x9, brk)))
> 
>      (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
> [1,]  2      19      21      19       8       1
> 
> Still feels a million miles off.
> 
> Now I could do with a little help please after spending a couple of hours
> working this out.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Mar 17 16:27:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Mar 2004 16:27:35 +0100
Subject: [R] Frequency table
In-Reply-To: <20040317145519.GD5021@cs.helsinki.fi>
References: <20040317145519.GD5021@cs.helsinki.fi>
Message-ID: <x24qsnlcig.fsf@biostat.ku.dk>

Kai Hendry <hendry at cs.helsinki.fi> writes:

> This must be FAQ, but I can't find it in archives or with a site search.
> 
> I am trying to construct a frequency table. I guess this should be done with
> table. Or perhaps factor and split. Or prop.table. cut? findInterval? Argh!
> 
> Please correct me if what I am looking for is not called a "frequency table".
> Perhaps it's called grouped data.
> 
> > zz$x9
>  [1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 64 75 58 60 56 60
> [26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 72 87 52 72 80 85
> [51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84
> 
> I (think) I want it to look like:
> 
> 40-49   2
> 50-59   15
> 60-69   20
> 70-79   19
> 80-89   12
> 90-99   2
> 
> Or the other way around with transpose.
> 
> classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
> For the rownames
> 
> sum(zz$x9 > 40 & zz$x9 < 50)
> For getting frequency counts is very laborious...
> 
> I got this far:
> > table(cut(zz$x9, brk))
> 
>  (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
>        2       19       21       19        8        1
> > brk
> [1]  40  50  60  70  80  90 100
> > 
> > t(table(cut(zz$x9, brk)))
>      (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
> [1,]  2      19      21      19       8       1
> 
> Still feels a million miles off.
> 
> Now I could do with a little help please after spending a couple of hours
> working this out.

Hmm, interesting complication of the convention that tables are 1D
arrays there...

You got this far:


classes <- c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
brk <- seq(40,100,10)

However, your intervals include the wrong end and the labels are ugly,
so try

table(cut(zz,breaks=brk,right=FALSE,labels=classes))

This at least gives you the right counts and labels:

40-49 50-59 60-69 70-79 80-89 90-99
    2    15    20    19    12     2

for a column display, you need to convert to a matrix somehow.
Transposing twice will actually do it, but I think I prefer

matrix(table(cut(zz,breaks=brk,right=FALSE)),dimnames=list(age=classes,""))

which gives this:

age
  40-49  2
  50-59 15
  60-69 20
  70-79 19
  80-89 12
  90-99  2



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at medanalytics.com  Wed Mar 17 16:32:52 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 17 Mar 2004 09:32:52 -0600
Subject: [R] Frequency table
In-Reply-To: <20040317145519.GD5021@cs.helsinki.fi>
References: <20040317145519.GD5021@cs.helsinki.fi>
Message-ID: <1079537572.7699.613.camel@localhost.localdomain>

On Wed, 2004-03-17 at 08:55, Kai Hendry wrote:
> This must be FAQ, but I can't find it in archives or with a site search.
> 
> I am trying to construct a frequency table. I guess this should be done with
> table. Or perhaps factor and split. Or prop.table. cut? findInterval? Argh!
> 
> Please correct me if what I am looking for is not called a "frequency table".
> Perhaps it's called grouped data.
> 
> > zz$x9
>  [1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 64 75 58 60 56 60
> [26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 72 87 52 72 80 85
> [51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84
> 
> I (think) I want it to look like:
> 
> 40-49   2
> 50-59   15
> 60-69   20
> 70-79   19
> 80-89   12
> 90-99   2
> 
> Or the other way around with transpose.
> 
> classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
> For the rownames
> 
> sum(zz$x9 > 40 & zz$x9 < 50)
> For getting frequency counts is very laborious...
> 
> I got this far:
> > table(cut(zz$x9, brk))
> 
>  (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
>        2       19       21       19        8        1
> > brk
> [1]  40  50  60  70  80  90 100
> > 
> > t(table(cut(zz$x9, brk)))
>      (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
> [1,]  2      19      21      19       8       1
> 
> Still feels a million miles off.
> 
> Now I could do with a little help please after spending a couple of hours
> working this out.


Try this:

table(cut(zz$x9, brk,
          labels = c("40 - 49", "50 - 59", "60 - 69", 
                     "70 - 79", "80 - 89", "90 - 99"),
          right = FALSE))


This should give you something like:

40 - 49 50 - 59 60 - 69 70 - 79 80 - 89 90 - 99 
      2      15      20      19      12       2 

You can use the labels argument in cut to define the group labels and
'right = FALSE' "closes" the intervals to the right side of the range.

HTH,

Marc Schwartz



From p.pagel at gsf.de  Wed Mar 17 16:34:39 2004
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 17 Mar 2004 16:34:39 +0100
Subject: [R] Frequency table
In-Reply-To: <20040317145519.GD5021@cs.helsinki.fi>
References: <20040317145519.GD5021@cs.helsinki.fi>
Message-ID: <20040317153439.GA5796@porcupine.gsf.de>


On Wed, Mar 17, 2004 at 04:55:19PM +0200, Kai Hendry wrote:
> I am trying to construct a frequency table. I guess this should be done with
> table. Or perhaps factor and split. Or prop.table. cut? findInterval? Argh!

> I got this far:
> > table(cut(zz$x9, brk))
> 
>  (40,50]  (50,60]  (60,70]  (70,80]  (80,90] (90,100]
>        2       19       21       19        8        1
> > brk
> [1]  40  50  60  70  80  90 100
> > 
> > t(table(cut(zz$x9, brk)))
>      (40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
> [1,]  2      19      21      19       8       1
> 
> Still feels a million miles off.

Why? To me it looks like you figured it all out. You found out how to
use cut() to get the appropriate factor and you used table() to
compute the counts. Nothing wrong with that...

The only difference to what you wanted to get is that your example looked
more like a data frame. Try

as.data.frame(yourtable)

which will give you something like this:

> as.data.frame(tbl)
         b Freq
1   (0,20]   17
2  (20,40]   28
3  (40,60]   19
4  (60,80]   15
5 (80,100]   21

Is that what you wanted?

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/~pagel



From p.dalgaard at biostat.ku.dk  Wed Mar 17 16:41:01 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Mar 2004 16:41:01 +0100
Subject: [R] Frequency table
In-Reply-To: <20040317145519.GD5021@cs.helsinki.fi>
References: <20040317145519.GD5021@cs.helsinki.fi>
Message-ID: <x2oeqvjxbm.fsf@biostat.ku.dk>

Kai Hendry <hendry at cs.helsinki.fi> writes:


> 40-49   2
> 50-59   15
> 60-69   20
> 70-79   19
> 80-89   12
> 90-99   2

Here's another solution for this 10-year age group thing:

tt<-table(zz%/%10)
n <- names(tt)
names(tt) <- paste(n,0,"-",n,9,sep="")
tt
data.frame(count=c(tt))

Beware that empty groups are silently zapped, though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sdavis2 at mail.nih.gov  Wed Mar 17 16:43:50 2004
From: sdavis2 at mail.nih.gov (Davis, Sean (NIH/NHGRI))
Date: Wed, 17 Mar 2004 10:43:50 -0500
Subject: [R] projection pursuit
Message-ID: <0E3E7E8F6E23DF4C8127A063568356B50473B0C6@nihexchange12.nih.gov>

Luis,

If I am not mistaken, the xgobi package (which requires xgobi executable)
does interactive projection pursuit.  Others can correct me if I'm wrong and
there may be other packages, as well.

Sean

-----Original Message-----
From: lmsilva at fe.up.pt
To: r-help at stat.math.ethz.ch
Sent: 3/17/2004 9:59 AM
Subject: [R] projection pursuit


Dear helpers

Does R have a package that performs projection pursuit density
estimation? Or 
anyone knows code in Matlab or C for example to do this?

Thank you all

Luis

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MARGARET_GALIETTI at fmc.com  Wed Mar 17 17:06:40 2004
From: MARGARET_GALIETTI at fmc.com (MARGARET GALIETTI)
Date: Wed, 17 Mar 2004 11:06:40 -0500
Subject: [R] help with file
Message-ID: <OF75FFD73F.88537F3D-ON85256E5A.0057431D-85256E5A.00587FB5@fmc.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040317/b2f20a74/attachment.pl

From ripley at stats.ox.ac.uk  Wed Mar 17 17:12:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Mar 2004 16:12:06 +0000 (GMT)
Subject: [R] Frequency table
In-Reply-To: <x2oeqvjxbm.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0403171606300.24465-100000@gannet.stats>

On 17 Mar 2004, Peter Dalgaard wrote:

> Kai Hendry <hendry at cs.helsinki.fi> writes:
> 
> 
> > 40-49   2
> > 50-59   15
> > 60-69   20
> > 70-79   19
> > 80-89   12
> > 90-99   2
> 
> Here's another solution for this 10-year age group thing:
> 
> tt<-table(zz%/%10)
> n <- names(tt)
> names(tt) <- paste(n,0,"-",n,9,sep="")
> tt
> data.frame(count=c(tt))
> 
> Beware that empty groups are silently zapped, though.

FWIW, table(factor(zz%/%10, levels=0:9))  avoids that

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From FWS4 at CDRH.FDA.GOV  Wed Mar 17 17:51:01 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Wed, 17 Mar 2004 11:51:01 -0500
Subject: [R] projection pursuit
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB7A5@drm556>

Yes, xgobi does projection pursuit.  Though 
I'm not so sure about 'projection pursuit density estimation'.
Not that I know what that is.


-----Original Message-----
From: Davis, Sean (NIH/NHGRI) [mailto:sdavis2 at mail.nih.gov] 
Sent: Wednesday, March 17, 2004 10:44 AM
To: 'lmsilva at fe.up.pt '; 'r-help at stat.math.ethz.ch '
Subject: RE: [R] projection pursuit


Luis,

If I am not mistaken, the xgobi package (which requires xgobi executable)
does interactive projection pursuit.  Others can correct me if I'm wrong and
there may be other packages, as well.

Sean

-----Original Message-----
From: lmsilva at fe.up.pt
To: r-help at stat.math.ethz.ch
Sent: 3/17/2004 9:59 AM
Subject: [R] projection pursuit


Dear helpers

Does R have a package that performs projection pursuit density
estimation? Or 
anyone knows code in Matlab or C for example to do this?

Thank you all

Luis

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From cp133 at york.ac.uk  Wed Mar 17 17:55:31 2004
From: cp133 at york.ac.uk (C Peroni)
Date: Wed, 17 Mar 2004 16:55:31 -0000
Subject: [R] Persp plotting of kernel density estimate.
Message-ID: <000001c40c40$a898c140$add82090@csrv.ad.york.ac.uk>

Dear All, 

I am trying to visualize the surface of a bivariate kernel density
estimate. 
I have a vector of bivariate observations(x,y), and a function which
computes the kernel density estimate z corresponding to each
observation.
I cannot generate the (x,y) data in the ascending order needed by
persp(x,y,z). 
I was wondering whether there is an R version of the S function interp.

Would anybody be able to help.
Thank you in advance.

Chiara



From feh3k at spamcop.net  Wed Mar 17 15:36:05 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 17 Mar 2004 09:36:05 -0500
Subject: [R] R-business case
In-Reply-To: <20040317113407.ZQKC14485.smta08.mail.ozemail.net@there>
References: <XFMail.040316220210.Ted.Harding@nessie.mcc.ac.uk>
	<20040317113407.ZQKC14485.smta08.mail.ozemail.net@there>
Message-ID: <20040317093605.5a3df496.feh3k@spamcop.net>

On Wed, 17 Mar 2004 16:20:06 +1100
Jim Lemon <bitwrit at ozemail.com.au> wrote:

> Haven't there been one or two people who asked list members to submit 
> peer-reviewed, published papers in which R was used for the analysis? I 
> certainly have sent an email or two like this.
> 
> Jim
>

Just to be provocative, it would be best to state the ultimate goals, then
R users could be of more help.  We have submitted and published articles
using R and are using R in production work on contracts from
pharmaceutical companies.  It's difficult to know from the original note
why we should spend time compiling such data.  Is anyone finding that R
has some deficiencies with respect to their own work?

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Wed Mar 17 18:13:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 12:13:24 -0500
Subject: [R] build on Irix failed reg-tests
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79F2@usrymx25.merck.com>

Dear R-devel,

Has anyone seen this problem?  We tried building R-1.8.1 (and R-1.9.0 alpha
2004-03-17) on an Irix 6.5 box using

./configure CC="cc -64" F77="f77 -64" --with-tcltk=no --enable-R-shlib

make check failed because NA + 0 gave NaN instead of NA.  I've tried both
32- and 64-bit build, with and without --enable-R-shlib.  The same symptom
occur in all cases.

Any pointer much appreciated.

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From spencer.graves at pdf.com  Wed Mar 17 18:21:33 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 Mar 2004 09:21:33 -0800
Subject: [R] R-business case
In-Reply-To: <20040317113407.ZQKC14485.smta08.mail.ozemail.net@there>
References: <XFMail.040316220210.Ted.Harding@nessie.mcc.ac.uk>
	<20040317113407.ZQKC14485.smta08.mail.ozemail.net@there>
Message-ID: <4058891D.8010709@pdf.com>

Also, how many books explicitly discuss R -- and especially how many 
mention R (and not S-Plus) in the title?  spencer graves

Jim Lemon wrote:

>Haven't there been one or two people who asked list members to submit 
>peer-reviewed, published papers in which R was used for the analysis? I 
>certainly have sent an email or two like this.
>
>Jim
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From cp133 at york.ac.uk  Wed Mar 17 18:39:30 2004
From: cp133 at york.ac.uk (C Peroni)
Date: Wed, 17 Mar 2004 17:39:30 -0000
Subject: [R] (no subject)
Message-ID: <000001c40c46$cd1efa60$add82090@csrv.ad.york.ac.uk>

Dear All, 

I am trying to visualize the surface of a bivariate kernel density
estimate. 
I have a vector of bivariate observations(x,y), and a function which
computes the kernel density estimate z corresponding to each
observation. I cannot generate the (x,y) data in the ascending order
needed by persp(x,y,z). 
I was wondering whether there is an R version of the S function interp.

Would anybody be able to help.
Thank you in advance.

Chiara



From cp133 at york.ac.uk  Wed Mar 17 18:41:00 2004
From: cp133 at york.ac.uk (C Peroni)
Date: Wed, 17 Mar 2004 17:41:00 -0000
Subject: [R] Persp plotting of kernel density estimate.
Message-ID: <000101c40c47$0318ba20$add82090@csrv.ad.york.ac.uk>

Dear All, 

I am trying to visualize the surface of a bivariate kernel density
estimate. 
I have a vector of bivariate observations(x,y), and a function which
computes the kernel density estimate z corresponding to each
observation. I cannot generate the (x,y) data in the ascending order
needed by persp(x,y,z). 
I was wondering whether there is an R version of the S function interp.

Would anybody be able to help.
Thank you in advance.

Chiara



From klealambrou at hotmail.com  Wed Mar 17 18:42:06 2004
From: klealambrou at hotmail.com (klea lambrou)
Date: Wed, 17 Mar 2004 17:42:06 +0000
Subject: [R] remove pairs of missing values
Message-ID: <BAY16-F44pgpZ5PM6LB00014307@hotmail.com>


   hello R-users.I really need your help on these one.i have two vectors
   x and y of equal length.y contains missing values,so i need to remove
   them.i know how to do that for y but i also need the corresponding x
   value to be removed too.i cannot find or at least think of a command
   which will do this.can you please help me?


From Sinnwell.Jason at mayo.edu  Wed Mar 17 18:42:32 2004
From: Sinnwell.Jason at mayo.edu (Jason Sinnwell)
Date: Wed, 17 Mar 2004 11:42:32 -0600 (CST)
Subject: [R] R CMD Sd2Rd 
Message-ID: <200403171742.i2HHgW525705@rocky.mayo.edu>

Using R 1.8.1 on Solaris

Developing an Splus and R package from the same files.  
Creating an alias for multiple functions to the same file in .sgml:  
<s-topics>
<s-topic> my.function </s-topic>
<s-topic> print.my.function </s-topic>
</s-topics>

using R CMD Sd2Rd would convert to:

\name{my.function}
\alias{my.function}
\name{print.my.function}
\alias{print.my.function}

which is a syntax error.  Rd format would understand it if the third line were 
not there. (R-exts 2.1.1)



+--------------------------+
|Jason P. Sinnwell, M.S.   |
|Mayo Clinic, Rochester    |
|Health Sciences Research  |
|Division of Biostatistics |
|Harwick 7-97		   |
|507.284.3270              |
+--------------------------+



From andy_liaw at merck.com  Wed Mar 17 18:45:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 12:45:29 -0500
Subject: [R] Persp plotting of kernel density estimate.
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79F6@usrymx25.merck.com>

See ?interp in the `akima' package.  Also, I believe the `sm' package does
have function that produce the surface plot of 2D kernel density surface.

HTH,
Andy

> From: C Peroni
> 
> Dear All, 
> 
> I am trying to visualize the surface of a bivariate kernel density
> estimate. 
> I have a vector of bivariate observations(x,y), and a function which
> computes the kernel density estimate z corresponding to each
> observation.
> I cannot generate the (x,y) data in the ascending order needed by
> persp(x,y,z). 
> I was wondering whether there is an R version of the S 
> function interp.
> 
> Would anybody be able to help.
> Thank you in advance.
> 
> Chiara
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ggrothendieck at myway.com  Wed Mar 17 19:13:30 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Mar 2004 13:13:30 -0500 (EST)
Subject: [R] Frequency table
Message-ID: <20040317181330.2C47E39D5@mprdmxin.myway.com>



Assuming x contains the data, taking the solutions so far and 
adding some minor improvements gives:

   
   groups <- x %/% 10
   lev <- min(groups):max(groups)
   lab <- factor( paste( lev, "0-", lev, "9", sep = "" ) )
   groups <- factor( groups, lev = lev, lab = lab )
   tab <- table( groups, dnn = "Range" )
   as.data.frame( tab )

   # for graphical output:

   bp <- barplot( tab )
   text( bp, tab, tab, pos = 3, xpd = TRUE )




---
Date:   Wed, 17 Mar 2004 16:55:19 +0200 
From:   Kai Hendry <hendry at cs.helsinki.fi>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Frequency table 

 
This must be FAQ, but I can't find it in archives or with a site search.

I am trying to construct a frequency table. I guess this should be done with
table. Or perhaps factor and split. Or prop.table. cut? findInterval? Argh!

Please correct me if what I am looking for is not called a "frequency table".
Perhaps it's called grouped data.

> zz$x9
[1] 65 70 85 65 65 65 62 55 82 59 55 66 74 55 65 56 80 73 45 64 75 58 60 56 60
[26] 65 53 63 72 80 90 95 55 70 79 62 57 65 60 47 61 53 80 75 72 87 52 72 80 85
[51] 75 70 84 60 72 70 76 70 79 72 69 80 62 74 54 58 58 69 81 84

I (think) I want it to look like:

40-49 2
50-59 15
60-69 20
70-79 19
80-89 12
90-99 2

Or the other way around with transpose.

classes = c("40-49", "50-59", "60-69", "70-79", "80-89", "90-99")
For the rownames

sum(zz$x9 > 40 & zz$x9 < 50)
For getting frequency counts is very laborious...

I got this far:
> table(cut(zz$x9, brk))

(40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
2 19 21 19 8 1
> brk
[1] 40 50 60 70 80 90 100
> 
> t(table(cut(zz$x9, brk)))
(40,50] (50,60] (60,70] (70,80] (80,90] (90,100]
[1,] 2 19 21 19 8 1

Still feels a million miles off.

Now I could do with a little help please after spending a couple of hours
working this out.



From MSchwartz at medanalytics.com  Wed Mar 17 19:15:35 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 17 Mar 2004 12:15:35 -0600
Subject: [R] remove pairs of missing values
In-Reply-To: <BAY16-F44pgpZ5PM6LB00014307@hotmail.com>
References: <BAY16-F44pgpZ5PM6LB00014307@hotmail.com>
Message-ID: <1079547335.4299.6.camel@localhost.localdomain>

On Wed, 2004-03-17 at 11:42, klea lambrou wrote:
>    hello R-users.I really need your help on these one.i have two vectors
>    x and y of equal length.y contains missing values,so i need to remove
>    them.i know how to do that for y but i also need the corresponding x
>    value to be removed too.i cannot find or at least think of a command
>    which will do this.can you please help me?


Use complete.cases():

x <- c(1:6)
y <- c(1, 2, NA, 4, 5, NA)
z <- cbind(x, y)

> z
     x  y
[1,] 1  1
[2,] 2  2
[3,] 3 NA
[4,] 4  4
[5,] 5  5
[6,] 6 NA

> z[complete.cases(z), ]
     x y
[1,] 1 1
[2,] 2 2
[3,] 4 4
[4,] 5 5

See ?complete.cases for more information.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Wed Mar 17 19:06:50 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 13:06:50 -0500
Subject: [R] remove pairs of missing values
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79F7@usrymx25.merck.com>

miss <- is.na(y)
y.good <- y[!miss]
x.good <- x[!miss]

HTH,
Andy

> From: klea lambrou
 
> 
>    hello R-users.I really need your help on these one.i have 
> two vectors
>    x and y of equal length.y contains missing values,so i 
> need to remove
>    them.i know how to do that for y but i also need the 
> corresponding x
>    value to be removed too.i cannot find or at least think of 
> a command
>    which will do this.can you please help me?
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From tlumley at u.washington.edu  Wed Mar 17 19:18:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 17 Mar 2004 10:18:29 -0800 (PST)
Subject: [R] build on Irix failed reg-tests
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF79F2@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF79F2@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0403170943510.98114@homer35.u.washington.edu>

On Wed, 17 Mar 2004, Liaw, Andy wrote:

> Dear R-devel,
>
> Has anyone seen this problem?  We tried building R-1.8.1 (and R-1.9.0 alpha
> 2004-03-17) on an Irix 6.5 box using
>
> ./configure CC="cc -64" F77="f77 -64" --with-tcltk=no --enable-R-shlib
>
> make check failed because NA + 0 gave NaN instead of NA.  I've tried both
> 32- and 64-bit build, with and without --enable-R-shlib.  The same symptom
> occur in all cases.
>

I haven't seen it before, but it looks to me as though arithmetic.c
assumes that adding a number to a NaN gives the same NaN back, (NA is the
NaN with lower word 1954).  That is, we just do
#ifdef IEEE_754
            REAL(ans)[i] = REAL(s1)[i1] + REAL(s2)[i2];
#else

This doesn't look as thought it is guaranteed to work, though it does on
most machines.  It clearly can't work where both operands are NaN, so
under OS X I get
> NA+NaN
[1] NA
> NaN+NA
[1] NaN


	-thomas



From sdavis2 at mail.nih.gov  Wed Mar 17 19:21:50 2004
From: sdavis2 at mail.nih.gov (Davis, Sean (NIH/NHGRI))
Date: Wed, 17 Mar 2004 13:21:50 -0500
Subject: [R] projection pursuit
Message-ID: <0E3E7E8F6E23DF4C8127A063568356B50473B0CA@nihexchange12.nih.gov>

 Good point....

Sean

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
To: 'r-help at stat.math.ethz.ch '
Sent: 3/17/2004 11:51 AM
Subject: RE: [R] projection pursuit

Yes, xgobi does projection pursuit.  Though 
I'm not so sure about 'projection pursuit density estimation'.
Not that I know what that is.


-----Original Message-----
From: Davis, Sean (NIH/NHGRI) [mailto:sdavis2 at mail.nih.gov] 
Sent: Wednesday, March 17, 2004 10:44 AM
To: 'lmsilva at fe.up.pt '; 'r-help at stat.math.ethz.ch '
Subject: RE: [R] projection pursuit


Luis,

If I am not mistaken, the xgobi package (which requires xgobi
executable)
does interactive projection pursuit.  Others can correct me if I'm wrong
and
there may be other packages, as well.

Sean

-----Original Message-----
From: lmsilva at fe.up.pt
To: r-help at stat.math.ethz.ch
Sent: 3/17/2004 9:59 AM
Subject: [R] projection pursuit


Dear helpers

Does R have a package that performs projection pursuit density
estimation? Or 
anyone knows code in Matlab or C for example to do this?

Thank you all

Luis

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Wed Mar 17 19:22:49 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 17 Mar 2004 18:22:49 +0000
Subject: [R] (no subject)
In-Reply-To: <000001c40c46$cd1efa60$add82090@csrv.ad.york.ac.uk>
References: <000001c40c46$cd1efa60$add82090@csrv.ad.york.ac.uk>
Message-ID: <40589779.6020107@lancaster.ac.uk>

C Peroni wrote:

> I am trying to visualize the surface of a bivariate kernel density
> estimate. 
> I have a vector of bivariate observations(x,y), and a function which
> computes the kernel density estimate z corresponding to each
> observation. I cannot generate the (x,y) data in the ascending order
> needed by persp(x,y,z). 

  That's not a very useful kernel density estimate function. The 
kernel2d function in library(splancs) takes some x,y points, a kernel 
width, and a number of grid point in x and y and returns something that 
you can feed into persp() or image(). Can't you edit your kernel 
function to give you values away from the observations?

> I was wondering whether there is an R version of the S function interp.

  A simple visualisation can be acheived by plotting the points with a 
symbol size related to the value at the point. e.g.

xyz=data.frame(x=runif(10),y=runif(10),z=runif(10)^2)
plot(xyz$x,xyz$y,cex=xyz$z*3)

  You'll have to make sure the cex parameter is scaled to a range that 
looks good on your graphics device, which is where the '*3' multiplier 
comes from in my example. Normal points have cex=1. Points with cex=0 
are invisible to you may want to add a small offset.

Barry



From jgoebel at diw.de  Wed Mar 17 19:23:15 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Wed, 17 Mar 2004 19:23:15 +0100
Subject: [R] remove pairs of missing values
In-Reply-To: <BAY16-F44pgpZ5PM6LB00014307@hotmail.com>
References: <BAY16-F44pgpZ5PM6LB00014307@hotmail.com>
Message-ID: <20040317182315.GA22428@diw138134.diw-berlin.de>

not.ok <- is.na(y)
y.new <- y[! not.ok]
x.new <- x[! not.ok]

should work.

jan

On Wed, 17 Mar 2004, klea lambrou wrote:

> 
>    hello R-users.I really need your help on these one.i have two vectors
>    x and y of equal length.y contains missing values,so i need to remove
>    them.i know how to do that for y but i also need the corresponding x
>    value to be removed too.i cannot find or at least think of a command
>    which will do this.can you please help me?
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K?nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From andy_liaw at merck.com  Wed Mar 17 19:33:26 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 13:33:26 -0500
Subject: [R] build on Irix failed reg-tests
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79F8@usrymx25.merck.com>

> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> 
> On Wed, 17 Mar 2004, Liaw, Andy wrote:
> 
> > Dear R-devel,
> >
> > Has anyone seen this problem?  We tried building R-1.8.1 
> (and R-1.9.0 alpha
> > 2004-03-17) on an Irix 6.5 box using
> >
> > ./configure CC="cc -64" F77="f77 -64" --with-tcltk=no 
> --enable-R-shlib
> >
> > make check failed because NA + 0 gave NaN instead of NA.  
> I've tried both
> > 32- and 64-bit build, with and without --enable-R-shlib.  
> The same symptom
> > occur in all cases.
> >
> 
> I haven't seen it before, but it looks to me as though arithmetic.c
> assumes that adding a number to a NaN gives the same NaN 
> back, (NA is the
> NaN with lower word 1954).  That is, we just do
> #ifdef IEEE_754
>             REAL(ans)[i] = REAL(s1)[i1] + REAL(s2)[i2];
> #else
> 
> This doesn't look as thought it is guaranteed to work, though 
> it does on
> most machines.  It clearly can't work where both operands are NaN, so
> under OS X I get
> > NA+NaN
> [1] NA
> > NaN+NA
> [1] NaN
> 
> 
> 	-thomas
> 

On our Opteron I get:

> NA+NaN
[1] NaN
> NaN+NA
[1] NA

OK, now I'm really confused...

Andy
 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Mar 17 19:40:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Mar 2004 18:40:10 +0000 (GMT)
Subject: [R] Persp plotting of kernel density estimate.
In-Reply-To: <000101c40c47$0318ba20$add82090@csrv.ad.york.ac.uk>
Message-ID: <Pine.LNX.4.44.0403171839130.1379-100000@gannet.stats>

On Wed, 17 Mar 2004, C Peroni wrote:

> Dear All, 
> 
> I am trying to visualize the surface of a bivariate kernel density
> estimate. 
> I have a vector of bivariate observations(x,y), and a function which
> computes the kernel density estimate z corresponding to each
> observation. I cannot generate the (x,y) data in the ascending order
> needed by persp(x,y,z). 
> I was wondering whether there is an R version of the S function interp.

There is, in package akima.

But, you really want to be interpolating in the kde: both MASS and 
KernSmooth have functions to do 2D kde for you.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Mar 17 19:53:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Mar 2004 18:53:20 +0000 (GMT)
Subject: [R] R CMD Sd2Rd 
In-Reply-To: <200403171742.i2HHgW525705@rocky.mayo.edu>
Message-ID: <Pine.LNX.4.44.0403171849470.1379-100000@gannet.stats>

Do you not have an editor?

Sd2Rd is designed for rough conversion, especially for Ssgml files, not 
for accurate and complete conversion.  It is purely for convenience.

In this case, prompt() in S-PLUS will never create the file you quote.  We 
don't actually know exactly what the allowed format of Ssgml files is.

On Wed, 17 Mar 2004, Jason Sinnwell wrote:

> Using R 1.8.1 on Solaris
> 
> Developing an Splus and R package from the same files.  

We would strongly recommend you to use .Rd files as the master copies if
you are using R tools.

> Creating an alias for multiple functions to the same file in .sgml:  
> <s-topics>
> <s-topic> my.function </s-topic>
> <s-topic> print.my.function </s-topic>
> </s-topics>
> 
> using R CMD Sd2Rd would convert to:
> 
> \name{my.function}
> \alias{my.function}
> \name{print.my.function}
> \alias{print.my.function}
> 
> which is a syntax error.  Rd format would understand it if the third line were 
> not there. (R-exts 2.1.1)
> 
> 
> 
> +--------------------------+
> |Jason P. Sinnwell, M.S.   |
> |Mayo Clinic, Rochester    |
> |Health Sciences Research  |
> |Division of Biostatistics |
> |Harwick 7-97		   |
> |507.284.3270              |
> +--------------------------+
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Mar 17 20:02:55 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Mar 2004 20:02:55 +0100
Subject: [R] Frequency table
In-Reply-To: <Pine.LNX.4.44.0403171606300.24465-100000@gannet.stats>
References: <Pine.LNX.4.44.0403171606300.24465-100000@gannet.stats>
Message-ID: <x2oeqv1elc.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > tt<-table(zz%/%10)
> > n <- names(tt)
> > names(tt) <- paste(n,0,"-",n,9,sep="")
> > tt
> > data.frame(count=c(tt))
> > 
> > Beware that empty groups are silently zapped, though.
> 
> FWIW, table(factor(zz%/%10, levels=0:9))  avoids that

I knew, but then you'd basically be back to using cut. Your fix will
lose if there are any 107-year olds, by the way. It will also include
a number of empty groups below the range of data, which might not be
what you want.

Here's a first stab at a more general solution:

sz <- 10 
g <- zz %/% sz
lv <- min(g):max(g)
lb <- paste(lv * sz, "-", lv * sz + sz - 1, sep="")
f <- factor(g,levels=lv,labels=lb)
(tt <- table(f))
data.frame(count=c(tt))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Mar 17 21:02:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Mar 2004 20:02:56 +0000 (GMT)
Subject: [R] build on Irix failed reg-tests
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF79F8@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0403171958420.1547-100000@gannet.stats>

I think NA+0 should be special, though.  All of mine give

> NA+NaN
[1] NA
> NaN+NA
[1] NA

That's with four separate compilers on Sparc-Solaris as well as Windows
and Linux i686.

I have long wondered why it works ....


On Wed, 17 Mar 2004, Liaw, Andy wrote:

> > From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> > 
> > On Wed, 17 Mar 2004, Liaw, Andy wrote:
> > 
> > > Dear R-devel,
> > >
> > > Has anyone seen this problem?  We tried building R-1.8.1 
> > (and R-1.9.0 alpha
> > > 2004-03-17) on an Irix 6.5 box using
> > >
> > > ./configure CC="cc -64" F77="f77 -64" --with-tcltk=no 
> > --enable-R-shlib
> > >
> > > make check failed because NA + 0 gave NaN instead of NA.  
> > I've tried both
> > > 32- and 64-bit build, with and without --enable-R-shlib.  
> > The same symptom
> > > occur in all cases.
> > >
> > 
> > I haven't seen it before, but it looks to me as though arithmetic.c
> > assumes that adding a number to a NaN gives the same NaN 
> > back, (NA is the
> > NaN with lower word 1954).  That is, we just do
> > #ifdef IEEE_754
> >             REAL(ans)[i] = REAL(s1)[i1] + REAL(s2)[i2];
> > #else
> > 
> > This doesn't look as thought it is guaranteed to work, though 
> > it does on
> > most machines.  It clearly can't work where both operands are NaN, so
> > under OS X I get
> > > NA+NaN
> > [1] NA
> > > NaN+NA
> > [1] NaN
> > 
> > 
> > 	-thomas
> > 
> 
> On our Opteron I get:
> 
> > NA+NaN
> [1] NaN
> > NaN+NA
> [1] NA
> 
> OK, now I'm really confused...
> 
> Andy
>  
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mvdv at spamcop.net  Wed Mar 17 21:31:46 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 18 Mar 2004 07:31:46 +1100
Subject: [R] Substitute for xlim and usr in boxplot(...) & bxp(...)?
In-Reply-To: <Pine.LNX.4.44.0403041009140.29175-100000@reclus.nhh.no>
Message-ID: <000201c40c5e$de7bf020$344610ac@FEB0480>

Hi,
Could anyone hint at how to set the x axis plot range in boxplot/bxp?  The
docs are clear that usr and xlim are not passed through/down
TIA
Mark



From ecashin at uga.edu  Wed Mar 17 21:28:24 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 17 Mar 2004 15:28:24 -0500
Subject: [R] Time for Usenet R Group?
References: <5.2.0.9.2.20030530075203.01c3cfe8@pop4.attglobal.net>
	<3ED799CB.1090602@stat.ucla.edu>
Message-ID: <873c876wwn.fsf@uga.edu>

"Roger D. Peng" <rpeng at stat.ucla.edu> writes:

> Marc R. Feldesman wrote:
>
>> I agree with you on the "flood" of messages lately.  Often this
>> flood accompanies a new release, but this flood has continued
>> unabated for longer than I would have imagined.  The good news is
>> that R is becoming more popular and this (hopefully) attracts more
>> developers, which results in more libraries, etc.  The bad news is
>> that with more users come more questions.
>
> I don't consider that "bad news".  That's just "how it is".  As far as
> I can see, it's all good news :)

I never used the mailing list directly, but always via NNTP, since
gmane acts as a gateway for the R mailing list.

  www.gmane.org

It has several anti-spam features in place already.  So if you want to
switch, it's very easy.  Just point your favorite news client to
news.gmane.org and start reading gmane.comp.lang.r.general.

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From p.murrell at auckland.ac.nz  Wed Mar 17 21:51:32 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 18 Mar 2004 09:51:32 +1300
Subject: [R] R-1.8.1-4: Font family, ticks and mathematical expression??
References: <Pine.LNX.4.44.0403171256210.6357-100000@gannet.stats>
Message-ID: <4058BA54.2080206@stat.auckland.ac.nz>

Hi


Prof Brian Ripley wrote:
> On Wed, 17 Mar 2004, Pisut Tempatarachoke wrote:
> 
> 
>>Hi all,
>>
>>Could anyone help answer the following questions, please?
>>(I'm using R-1.8.1-4 on Fedora Core 1 and very new to R)
>>
>>(i) Is it possible to specify a font family (e.g. courier or helvetica) 
>>when graphing?
> 
> 
> `when graphing' means what?  Some graphics devices, e.g. postscript and
> pdf support families.  X11() does not in 1.8.1 but will in 1.9.0, due
> early April.
> 
> 
>>(ii) How can I make ticks point inwards on all four sides of a plot?  Is 
>>it possible to have minor and major ticks?
> 
> 
> Their height (including direction) is controlled by par's tcl and tck --
> this is in `An Introduction to R'. See ?par.
> 
> You cna get major and minor ticks by calling axis twice with different par 
> values, if I understand you aright.
> 
> 
>>(iii) How would I specify a symbol "\sim" (i.e. "~") in a mathematical 
>>expression?  So far, I've only seen "%~~%", but that's not exactly what 
>>I'm after.  (I tried "%~%" but that didn't work.)
> 
> 
> Try demo(plotmath) for what is supported.  \sim is just a character in the 
> symbol font, but I don't know how to access those directly.


Does this do what you want ...?

plot(1:10)
text(4, 2, expression(x*tilde("   ")*y))

Depending on the complexity of your expression, Hershey fonts may be an 
alternative ...

plot(1:10)
text(4, 2, "x \\ap y", vfont=c("sans serif", "italic"))

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From kjetil at entelnet.bo  Wed Mar 17 21:58:58 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 17 Mar 2004 16:58:58 -0400
Subject: [R] Q: Odds Ratio
In-Reply-To: <40582D6A.8030008@biometria.univr.it>
Message-ID: <405883D2.4539.1C4C91@localhost>

On 17 Mar 2004 at 11:50, Cristian Pattaro wrote:

> Dear all,
> 
> is there a automatic method to obtain Odds Ratio estimates and their
> confidence intervals from a GLM Logistic model?
> 

The parameters in the linear predictor in logistic regression 
are log odds ratios (assuming the default contrast, contr.treatment). 
So just exponentiate it!, and do the same with the confidence limits.

Maybe the best way to obtain confidence limits are 
library(MASS)
mod <- glm(...)
confint(mod)
which uses likelihood profiling

Kjetil Halvorsen

> Thanks
> Cristian
> 
> =============================================
> Cristian Pattaro
> =============================================
> Unit of Epidemiology & Medical Statistics
> Department of Medicines and Public Health
> University of Verona
> cristian at biometria.univr.it
> =============================================
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From feldesmanm at pdx.edu  Wed Mar 17 22:11:34 2004
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Wed, 17 Mar 2004 13:11:34 -0800
Subject: [R] Time for Usenet R Group?
In-Reply-To: <873c876wwn.fsf@uga.edu>
References: <5.2.0.9.2.20030530075203.01c3cfe8@pop4.attglobal.net>
	<3ED799CB.1090602@stat.ucla.edu> <873c876wwn.fsf@uga.edu>
Message-ID: <6.0.3.0.2.20040317131036.02101ec0@pop4.attglobal.net>

At 12:28 PM 3/17/2004, Ed L Cashin wrote:

I hope this response isn't indicative of the speed with which gmane posts 
messages.  I think this entire thread was more than 7 or 8 months ago, 
possibly longer.


 >"Roger D. Peng" <rpeng at stat.ucla.edu> writes:
 >
 >> Marc R. Feldesman wrote:
 >>
 >>> I agree with you on the "flood" of messages lately.  Often this
 >>> flood accompanies a new release, but this flood has continued
 >>> unabated for longer than I would have imagined.  The good news is
 >>> that R is becoming more popular and this (hopefully) attracts more
 >>> developers, which results in more libraries, etc.  The bad news is
 >>> that with more users come more questions.
 >>
 >> I don't consider that "bad news".  That's just "how it is".  As far as
 >> I can see, it's all good news :)
 >
 >I never used the mailing list directly, but always via NNTP, since
 >gmane acts as a gateway for the R mailing list.
 >
 >  www.gmane.org
 >
 >It has several anti-spam features in place already.  So if you want to
 >switch, it's very easy.  Just point your favorite news client to
 >news.gmane.org and start reading gmane.comp.lang.r.general.
 >
 >--
 >--Ed L Cashin            |   PGP public key:
 >  ecashin at uga.edu        |   http://noserose.net/e/pgp/
 >
 >______________________________________________
 >R-help at stat.math.ethz.ch mailing list
 >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesman at attglobal.net
fax:    503-725-3905


"Don't knock on my door if you don't know my Rottweiler's name"  Warren Zevon
"Its midnight and I'm not famous yet"  Jimmy Buffett



From andy_liaw at merck.com  Wed Mar 17 22:15:33 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 16:15:33 -0500
Subject: [R] build on Irix failed reg-tests
Message-ID: <3A822319EB35174CA3714066D590DCD504AF79FC@usrymx25.merck.com>

Sorry for being dense:  So What is the likely source of the problem, then?
Is it the compiler, the OS, R itself, or some combination of these?  Any
suggestion on how to resolve this?

Best,
Andy

> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> 
> I think NA+0 should be special, though.  All of mine give
> 
> > NA+NaN
> [1] NA
> > NaN+NA
> [1] NA
> 
> That's with four separate compilers on Sparc-Solaris as well 
> as Windows
> and Linux i686.
> 
> I have long wondered why it works ....
> 
> 
> On Wed, 17 Mar 2004, Liaw, Andy wrote:
> 
> > > From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> > > 
> > > On Wed, 17 Mar 2004, Liaw, Andy wrote:
> > > 
> > > > Dear R-devel,
> > > >
> > > > Has anyone seen this problem?  We tried building R-1.8.1 
> > > (and R-1.9.0 alpha
> > > > 2004-03-17) on an Irix 6.5 box using
> > > >
> > > > ./configure CC="cc -64" F77="f77 -64" --with-tcltk=no 
> > > --enable-R-shlib
> > > >
> > > > make check failed because NA + 0 gave NaN instead of NA.  
> > > I've tried both
> > > > 32- and 64-bit build, with and without --enable-R-shlib.  
> > > The same symptom
> > > > occur in all cases.
> > > >
> > > 
> > > I haven't seen it before, but it looks to me as though 
> arithmetic.c
> > > assumes that adding a number to a NaN gives the same NaN 
> > > back, (NA is the
> > > NaN with lower word 1954).  That is, we just do
> > > #ifdef IEEE_754
> > >             REAL(ans)[i] = REAL(s1)[i1] + REAL(s2)[i2];
> > > #else
> > > 
> > > This doesn't look as thought it is guaranteed to work, though 
> > > it does on
> > > most machines.  It clearly can't work where both operands 
> are NaN, so
> > > under OS X I get
> > > > NA+NaN
> > > [1] NA
> > > > NaN+NA
> > > [1] NaN
> > > 
> > > 
> > > 	-thomas
> > > 
> > 
> > On our Opteron I get:
> > 
> > > NA+NaN
> > [1] NaN
> > > NaN+NA
> > [1] NA
> > 
> > OK, now I'm really confused...
> > 
> > Andy
> >  
> > 
> > 
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> Brian 
> D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From jahernan at umn.edu  Wed Mar 17 22:35:23 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Wed, 17 Mar 2004 15:35:23 -0600
Subject: [R] NLS question:Quadratic plus plateau fit
Message-ID: <6.0.0.22.0.20040317152324.01eeeb80@jahernan.email.umn.edu>

Dear R colleagues:

Am trying to fit a simple NL model to determine Economical Optimum Nitrogen 
Rates.
The segmented (quadratic + plateau) model only works with some y's, in some 
cases I get a "singular gradient" error.
I'll appreciate any ideas in how to solve the singular gradient error.

Thanks,

Jose

# The following code works using yield2 in the nls model but not using yield.

# Economical constants, nitrogen price in dollars per lb and corn price in 
dollars per bushel
nprice <- 0.17
cprice <- 2.25
ratio <- nprice/cprice

# Example data,

nrate <- c(0,60,90,120,150,180)
yield2 <- c(161.7,187.1,188.5,196.6,196.0,196.5)
yield <- c(163.4,178.1,179.6,178.2,184.4,184.5)

data.1 <- data.frame(nrate = nrate, yield = yield)
plot(data.1)

qp.nls.fit<- nls(yield ~ (b0 + b1*nrate + b2*I(nrate^2))*(nrate <= x0)
                 + (b0 + b1*x0 + b2*I(x0^2))*(nrate > x0),
                 data=data.1,
                 start=list(b0=125, b1=0.5, b2=-0.001, x0=135),
                 trace=T)
qp.nls.fit
summary(qp.nls.fit)
c.qp.fit <- coefficients(qp.nls.fit)
attach(as.list(c.qp.fit))

yld.x0 <- b0 + b1*x0 + b2*x0^2
maxn <- -b1/(2*b2)
eonr <- (ratio - b1)/(2*b2)
eonr2 <- round(eonr, digits = 0)
yldmaxn <- b0 + b1*maxn + b2*maxn^2
yldeonr <- b0 + b1*eonr + b2*eonr^2
yldeonr2 <- round(yldeonr, digits = 0)

plot(yield ~ nrate,
     pch = 16,
     main = "Quadratic plus Plateau Model",
     xlab = expression(paste("Nitrogen rate [lbs ac" ^-1,"]")),
     ylab = expression(paste("Corn yield [bu ac"^-1,"]")))
curve(predict(qp.nls.fit, data.frame(nrate = x)), add = T)
text(100,175, paste("EONR=",eonr2))
text(100,173, paste("Yield at EONR=",yldeonr2))


-- 
Jose A. Hernandez
Ph.D. Candidate
Precision Agriculture Center

Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From MSchwartz at medanalytics.com  Wed Mar 17 22:42:46 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 17 Mar 2004 15:42:46 -0600
Subject: [R] Substitute for xlim and usr in boxplot(...) & bxp(...)?
Message-ID: <1079559766.4299.67.camel@localhost.localdomain>

On Wed, 2004-03-17 at 14:31, Mark Van De Vyver wrote: 
> Hi,
> Could anyone hint at how to set the x axis plot range in boxplot/bxp?  The
> docs are clear that usr and xlim are not passed through/down
> TIA
> Mark


It is not entirely clear what you are looking to do here.

Are you attempting to plot a horizontal boxplot and adjust the range of
the horizontal axis?

If so, use 'ylim':

boxplot(1:10, ylim = c(0, 100), horizontal = TRUE)

If by chance, you are looking to play around with the location(s) of the
boxplots in a vertical configuration, see the 'at' and 'add' arguments
in ?boxplot. The last example in the help gives some hints on that,
where the 'boxwex' argument is also helpful.

Also, please be sure to post using a new e-mail. Your initial post is
"buried" in a thread from the beginning of the month in the archive. I
noted it when looking to see if anyone had responded to it yet.

HTH,

Marc Schwartz



From ecashin at uga.edu  Wed Mar 17 22:52:51 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 17 Mar 2004 16:52:51 -0500
Subject: [R] Time for Usenet R Group?
References: <5.2.0.9.2.20030530075203.01c3cfe8@pop4.attglobal.net>
	<3ED799CB.1090602@stat.ucla.edu> <873c876wwn.fsf@uga.edu>
	<6.0.3.0.2.20040317131036.02101ec0@pop4.attglobal.net>
Message-ID: <87u10n5efg.fsf@uga.edu>

"Marc R. Feldesman" <feldesmanm at pdx.edu> writes:

> At 12:28 PM 3/17/2004, Ed L Cashin wrote:
>
> I hope this response isn't indicative of the speed with which gmane
> posts messages.  I think this entire thread was more than 7 or 8
> months ago, possibly longer.

Your hopes are fulfilled.  The reason is simply that I am catching up
and just got around to this thread.

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From JensHenrik.Badsberg at agrsci.dk  Wed Mar 17 23:10:42 2004
From: JensHenrik.Badsberg at agrsci.dk (Jens Henrik Badsberg)
Date: Wed, 17 Mar 2004 23:10:42 +0100
Subject: [R] Bundles and 1.9.0 (alpha)
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAECBB23D2@DJFPOST01.djf.agrsci.dk>


What is it "Package bundles" and 1.9.0 (alpha) ?
'Rcmd check Bundle' aborts with:
Error: cannot open file '.../Bundle.Rcheck/Package/R/Package' for reading
In ".../Bundle.Rcheck/" the packages of the bundle are "merged" into a single
"package".  ('Rcmd INSTALL' does also "merge" the packages of the bundle.)
I do not in "Writing R Extensions", version 1.9.0, section 1.1.5, see any
changes (relative 1.8.1) on how bundles should be handled.
I get the same on abort on the "VR" (7.1-14) bundle.
Jens Henrik Badsberg



From jahernan at umn.edu  Wed Mar 17 23:12:24 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Wed, 17 Mar 2004 16:12:24 -0600
Subject: [R] remove pairs of missing values
Message-ID: <6.0.0.22.0.20040317161145.01f52e10@jahernan.email.umn.edu>

I think the easiest way is using na.omit function which omits pairs of data

Look at the following example:

 > x <- c(0,1,2,3,4,5)
 > y <- c(NA,2, 3, 5, NA, 6)
 > data.1 <- data.frame(x,y)
 > data.1
   x  y
1 0 NA
2 1  2
3 2  3
4 3  5
5 4 NA
6 5  6
 > new.data <- na.omit(data.1)
 > new.data
   x y
2 1 2
3 2 3
4 3 5
6 5 6
 >

-- 
Jose A. Hernandez
Ph.D. Candidate
Precision Agriculture Center

Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From mvdv at spamcop.net  Wed Mar 17 23:32:18 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 18 Mar 2004 09:32:18 +1100
Subject: [R] (no subject)
Message-ID: <000101c40c6f$b4f3db80$344610ac@FEB0480>

Hi Marc,
Thanks for the response.  Apologies for the lack of detail... I have used
'boxwex', 'at' and ylim, to place things nicely, except I know have some
spare room at the end of the plot, which I would like to get rid of.... Your
suggestion to use 'add' per the example is one I had not thought of.  Does
any one know of a simpler approach, to use add I'm need to create two
boxplots, a dummy (with x taking the correct range of values, say
boxplot(1:10, ylim = c(0, 100))) and the real box plot with, say 11 box
plots...

Thanks again
Mark

On Wed, 2004-03-17 at 14:31, Mark Van De Vyver wrote: 
> Hi,
> Could anyone hint at how to set the x axis plot range in boxplot/bxp?  
> The docs are clear that usr and xlim are not passed through/down TIA
> Mark


It is not entirely clear what you are looking to do here.

Are you attempting to plot a horizontal boxplot and adjust the range of
the horizontal axis?

If so, use 'ylim':

boxplot(1:10, ylim = c(0, 100), horizontal = TRUE)

If by chance, you are looking to play around with the location(s) of the
boxplots in a vertical configuration, see the 'at' and 'add' arguments
in ?boxplot. The last example in the help gives some hints on that,
where the 'boxwex' argument is also helpful.

Also, please be sure to post using a new e-mail. Your initial post is
"buried" in a thread from the beginning of the month in the archive. I
noted it when looking to see if anyone had responded to it yet.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Wed Mar 17 23:35:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Mar 2004 23:35:37 +0100
Subject: [R] build on Irix failed reg-tests
In-Reply-To: <Pine.LNX.4.44.0403171958420.1547-100000@gannet.stats>
References: <Pine.LNX.4.44.0403171958420.1547-100000@gannet.stats>
Message-ID: <x27jxj14qu.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> I think NA+0 should be special, though.  All of mine give
> 
> > NA+NaN
> [1] NA
> > NaN+NA
> [1] NA
> 
> That's with four separate compilers on Sparc-Solaris as well as Windows
> and Linux i686.
> 
> I have long wondered why it works ....

I get the same as Andy on Opteron, with another curious twist:

> NA-NaN
[1] NA
> NA+NaN
[1] NaN
> NaN+NA
[1] NA
> NaN-NA
[1] NaN

Looks like Thomas is right and we need to fix up the logic of all the
vectorized ops. Drats! 

        -p

> > > #ifdef IEEE_754
> > >             REAL(ans)[i] = REAL(s1)[i1] + REAL(s2)[i2];
> > > #else
> > > 
> > > This doesn't look as thought it is guaranteed to work, though 
> > > it does on
> > > most machines.  It clearly can't work where both operands are NaN, so
> > > under OS X I get
> > > > NA+NaN
> > > [1] NA
> > > > NaN+NA
> > > [1] NaN
> > > 
> > > 
> > > 	-thomas
> > > 
> > 
> > On our Opteron I get:
> > 
> > > NA+NaN
> > [1] NaN
> > > NaN+NA
> > [1] NA
> > 
> > OK, now I'm really confused...
> > 
> > Andy
> >  
> > 
> > 
> > ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachments,...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Icabalceta_j at wlf.state.la.us  Wed Mar 17 23:35:41 2004
From: Icabalceta_j at wlf.state.la.us (Icabalceta, Jorge L.)
Date: Wed, 17 Mar 2004 16:35:41 -0600
Subject: [R] save to file
Message-ID: <FF01C406D3A336489C58B9D0AE8E8E3702A0B0F9@wlfnt1.wlf.state.la.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040317/6d6722d0/attachment.pl

From tlumley at u.washington.edu  Wed Mar 17 23:47:34 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 17 Mar 2004 14:47:34 -0800 (PST)
Subject: [R] build on Irix failed reg-tests
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF79FC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF79FC@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0403171435140.65192@homer10.u.washington.edu>

On Wed, 17 Mar 2004, Liaw, Andy wrote:

> Sorry for being dense:  So What is the likely source of the problem, then?
> Is it the compiler, the OS, R itself, or some combination of these?  Any
> suggestion on how to resolve this?

In a sense it is R itself, making assumptions that hold on many, but not
all systems.  There are many different NaN values, all of which are
guaranteed to give NaN results in arithmetic observations.  On many
systems, if one operand is NaN and the other is a valid number the result
is not only NaN, but *the same NaN* as the operand.

In R we call one particular one of these NaN values "NA".  The standards
guarantee that any operation on NA gives some NaN, but we have assumed it
gives the right NaN, the NA one.  On Irix this seems not to be the case.

The solution is to change arithmetic.c not to make this assumption, which
will be slower. It would be nice to detect at build time whether the
assumption holds, so as to keep the current behaviour on other systems.

It should be rare for the error to make any difference, since is.na() is
still true for NaN.  AFAICT the only problem would be that is.nan(0+NA)
can be TRUE when it should be FALSE.

	-thomas



From vograno at evafunds.com  Wed Mar 17 23:48:51 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Wed, 17 Mar 2004 14:48:51 -0800
Subject: [R] why-s of method dispatching
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3AB3@phost015.intermedia.net>

Hi,

I am having a problem to understand why as.data.frame method doesn't
dispatch properly on my class:

> setClass("Foo", "character")
[1] "Foo"
> as.data.frame(list(foo=new("Foo", .Data="a")))
Error in as.data.frame.default(x[[i]], optional = TRUE) : 
 can't coerce Foo into a data.frame

I was expecting that this would call as.data.frame.character.


Another puzzle. If I explicitly call as.data.frame.character() it would
fail but for a different reason:

> as.data.frame.character(list(foo=new("Foo", .Data="a")))
Error in unique.default(x) : unique() applies only to vectors


I was under an impression that an instance of "Foo" would be welcome
anywhere a "character" was, but it seems to be more subtle. What am I
missing?


Thanks,
Vadim



From jasont at indigoindustrial.co.nz  Thu Mar 18 00:31:26 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 18 Mar 2004 12:31:26 +1300
Subject: [R] save to file
In-Reply-To: <FF01C406D3A336489C58B9D0AE8E8E3702A0B0F9@wlfnt1.wlf.state.la.us>
References: <FF01C406D3A336489C58B9D0AE8E8E3702A0B0F9@wlfnt1.wlf.state.la.us>
Message-ID: <4058DFCE.9090008@indigoindustrial.co.nz>

?dump
?savehistory

Icabalceta, Jorge L. wrote:

> Dear friends,
>     In R there is a file | save to file option which takes a "picture" of
> the text in your screen and saves it to a text file. Is there any way to do
> the same with a function?



From MSchwartz at medanalytics.com  Thu Mar 18 00:40:46 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 17 Mar 2004 17:40:46 -0600
Subject: [R] (no subject)
In-Reply-To: <000101c40c6f$b4f3db80$344610ac@FEB0480>
References: <000101c40c6f$b4f3db80$344610ac@FEB0480>
Message-ID: <1079566846.4299.135.camel@localhost.localdomain>

On Wed, 2004-03-17 at 16:32, Mark Van De Vyver wrote:
> Hi Marc,
> Thanks for the response.  Apologies for the lack of detail... I have used
> 'boxwex', 'at' and ylim, to place things nicely, except I know have some
> spare room at the end of the plot, which I would like to get rid of.... Your
> suggestion to use 'add' per the example is one I had not thought of.  Does
> any one know of a simpler approach, to use add I'm need to create two
> boxplots, a dummy (with x taking the correct range of values, say
> boxplot(1:10, ylim = c(0, 100))) and the real box plot with, say 11 box
> plots...
> 
> Thanks again
> Mark


Mark,

I am perhaps still a bit confused. Do you have 11 'unrelated' groups of
data or is it perhaps 5 pairs (10 total) plus one additional that is
separate and off by itself?

If it is 11 separate groups, they should space evenly across the x axis
based upon the default way boxplot() handles such things. The x axis
positions will be 1:11, with the same amount of space on the left and
right hand sides of the plot.

For example:

# Create a 11 column dataframe
x <- data.frame(matrix(rnorm(50 * 11), ncol = 11))

# Do the default boxplot
boxplot(x)



On the other hand, if you perhaps have the second scenario of 5 pairs
and a separate single, you could do something like the following:

# Create a plot window with x from 0.5 to 6.5 and y reflecting the
# range of values in 'x'
plot(c(0.5, 6.5), range(x), type = "n", ann = FALSE, axes = FALSE)

# Now add boxplots for the 'odd' columns in 'x', adding the 11th
# column. Set the 'at' values to pair up the first 5 cols and
# plot the 11th column at x = 6.2 for symmetry.
boxplot(x[, c(1, 3, 5, 7, 9, 11)], at = c(1:5 - 0.2, 6.2), 
        boxwex = 0.2, add = TRUE, xaxt = "n")

# Now add boxplots for the 'even' columns in 'x'
# Set the 'at' values to pair these up with the above
boxplot(x[, c(2, 4, 6, 8, 10)], at = 1:5 + 0.2, 
        boxwex = 0.2, add = TRUE, xaxt = "n")

Not sure if that is what you are looking for, but it might provide some
food for thought.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Thu Mar 18 00:54:56 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Mar 2004 18:54:56 -0500
Subject: [R] build on Irix failed reg-tests
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A00@usrymx25.merck.com>

> From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
> 
> On Wed, 17 Mar 2004, Liaw, Andy wrote:
> 
> > Sorry for being dense:  So What is the likely source of the 
> problem, then?
> > Is it the compiler, the OS, R itself, or some combination 
> of these?  Any
> > suggestion on how to resolve this?
> 
> In a sense it is R itself, making assumptions that hold on 
> many, but not
> all systems.  There are many different NaN values, all of which are
> guaranteed to give NaN results in arithmetic observations.  On many
> systems, if one operand is NaN and the other is a valid 
> number the result
> is not only NaN, but *the same NaN* as the operand.
> 
> In R we call one particular one of these NaN values "NA".  
> The standards
> guarantee that any operation on NA gives some NaN, but we 
> have assumed it
> gives the right NaN, the NA one.  On Irix this seems not to 
> be the case.
> 
> The solution is to change arithmetic.c not to make this 
> assumption, which
> will be slower. It would be nice to detect at build time whether the
> assumption holds, so as to keep the current behaviour on 
> other systems.
> 
> It should be rare for the error to make any difference, since 
> is.na() is
> still true for NaN.  AFAICT the only problem would be that 
> is.nan(0+NA)
> can be TRUE when it should be FALSE.
> 
> 	-thomas

Thanks very much for the clarification, Thomas.  So would it be fairly safe
for us to edit that part of reg-tests-1.R so it passes this step, so make
check can go on from there?

Best,
Andy


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From davidD at qimr.edu.au  Thu Mar 18 01:23:19 2004
From: davidD at qimr.edu.au (David Duffy)
Date: Thu, 18 Mar 2004 10:23:19 +1000 (EST)
Subject: [R] Re: R-help Digest, Vol 13, Issue 17
In-Reply-To: <200403171111.i2HB0vH2022471@hypatia.math.ethz.ch>
References: <200403171111.i2HB0vH2022471@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0403181013070.21290@orpheus.qimr.edu.au>

Cristian Pattaro asked:
> is there a automatic method to obtain Odds Ratio estimates and their
> confidence intervals from a GLM Logistic model?

something like:

 oddsr<-function(x) {
   require(MASS)
   round(exp(cbind(Odds.Ratio=coef(x), confint(x))),3)
 }
 oddsr(glm(y ~ x, family=binomial))

David Duffy.



From mvdv at spamcop.net  Thu Mar 18 01:34:07 2004
From: mvdv at spamcop.net (Mark Van De Vyver)
Date: Thu, 18 Mar 2004 11:34:07 +1100
Subject: [R] Substitute for xlim and usr in boxplot(...) & bxp(...)?
In-Reply-To: <1079566846.4299.135.camel@localhost.localdomain>
Message-ID: <000001c40c80$ba5730c0$344610ac@FEB0480>

Hi Marc, 
As you guessed my current point is like your second example.  I actually
have three groups of three, so the normal box plot drws them at positions,
1:9 by the time I've placed them where I'd like they plot at positions
(roughly) 1:8, leaving me with a large right hand space that I'd like to get
rid of.....

I think your code will work :)  I've not seen the mixing of plot and boxplot
before, so I'll have to play for a while..
Thanks again
Mark

> -----Original Message-----
> From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
> Sent: Thursday, March 18, 2004 10:41 AM
> To: Mark Van De Vyver
> Cc: 'R-Help'
> Subject: Re: [R] (no subject)
> 
> 
> On Wed, 2004-03-17 at 16:32, Mark Van De Vyver wrote:
> > Hi Marc,
> > Thanks for the response.  Apologies for the lack of 
> detail... I have 
> > used 'boxwex', 'at' and ylim, to place things nicely, except I know 
> > have some spare room at the end of the plot, which I would 
> like to get 
> > rid of.... Your suggestion to use 'add' per the example is 
> one I had 
> > not thought of.  Does any one know of a simpler approach, 
> to use add 
> > I'm need to create two boxplots, a dummy (with x taking the correct 
> > range of values, say boxplot(1:10, ylim = c(0, 100))) and 
> the real box 
> > plot with, say 11 box plots...
> > 
> > Thanks again
> > Mark
> 
> 
> Mark,
> 
> I am perhaps still a bit confused. Do you have 11 'unrelated' 
> groups of data or is it perhaps 5 pairs (10 total) plus one 
> additional that is separate and off by itself?
> 
> If it is 11 separate groups, they should space evenly across 
> the x axis based upon the default way boxplot() handles such 
> things. The x axis positions will be 1:11, with the same 
> amount of space on the left and right hand sides of the plot.
> 
> For example:
> 
> # Create a 11 column dataframe
> x <- data.frame(matrix(rnorm(50 * 11), ncol = 11))
> 
> # Do the default boxplot
> boxplot(x)
> 
> 
> 
> On the other hand, if you perhaps have the second scenario of 
> 5 pairs and a separate single, you could do something like 
> the following:
> 
> # Create a plot window with x from 0.5 to 6.5 and y 
> reflecting the # range of values in 'x' plot(c(0.5, 6.5), 
> range(x), type = "n", ann = FALSE, axes = FALSE)
> 
> # Now add boxplots for the 'odd' columns in 'x', adding the 
> 11th # column. Set the 'at' values to pair up the first 5 
> cols and # plot the 11th column at x = 6.2 for symmetry. 
> boxplot(x[, c(1, 3, 5, 7, 9, 11)], at = c(1:5 - 0.2, 6.2), 
>         boxwex = 0.2, add = TRUE, xaxt = "n")
> 
> # Now add boxplots for the 'even' columns in 'x'
> # Set the 'at' values to pair these up with the above 
> boxplot(x[, c(2, 4, 6, 8, 10)], at = 1:5 + 0.2, 
>         boxwex = 0.2, add = TRUE, xaxt = "n")
> 
> Not sure if that is what you are looking for, but it might 
> provide some food for thought.
> 
> HTH,
> 
> Marc Schwartz
> 
>



From risdpizza at yahoo.co.jp  Thu Mar 18 01:34:38 2004
From: risdpizza at yahoo.co.jp (SI)
Date: Thu, 18 Mar 2004 09:34:38 +0900 (JST)
Subject: [R] save GUI window position in windows version?
Message-ID: <20040318003438.33358.qmail@web504.mail.yahoo.co.jp>


Hi,

thanks for your reply.  But I guess I misfired a
question.

I want to save the size of _GUI_ window, NOT console
window.  GUI window is the window with "RGui"
writtenon the left of title bar.  Console window 
is inside of the GUI window with "R Console" on 
its title bar. I save my preference of console 
window in the default Rconsole file.
 
Every time I start an R GUI, that is, every time I 
start R on my Windows XP, it is started as
maximized, showing all over the screen. I 
always resize and move its position on the 
screen.

Is there a way to save GUI window size and position?

I close R after running a script involving a big
data, and restart R, as R seems to slow down
considerably even I eliminate the big object 
on memory with rm(). So saving size and location
of GUI saves me some hustle.

Thanks in advance.

SI



> --- Duncan Murdoch <dmurdoch at pair.com> 
> > On Wed, 17 Mar 2004 08:13:11 +0900 (JST), SI
> > <risdpizza at yahoo.co.jp>
> > wrote :
> > 
> > >Hi,
> > >
> > >I'm using windows GUI version.  Is there any way 
> > >to save window position from previous
> > session?Everytime I
> > >open GUI starts as maximized and I 
> > >have to resize it.
> > 
> > Set up the console the way you want it, then go to
> 
> > 
> > Edit|GUI preferences...
> > 
> > and save them to a file.  If you use the default
> > file, it'll apply
> > every time you start Rgui.
> > 
> > To do the same for graphics windows, you can set
> up
> > your own function
> > and set it to be the default graphics device
> > function.  E.g.
> > 
> > > tinywindows <- function(...) windows(width=1,
> > height=1, ...)
> > > options(device="tinywindows")
> > > plot(1,1)
> > Error in plot.new() : Figure margins too large
> > 
> > Duncan Murdoch
> 
> __________________________________________________

>



From jasont at indigoindustrial.co.nz  Thu Mar 18 02:01:00 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 18 Mar 2004 14:01:00 +1300
Subject: [R] save GUI window position in windows version?
In-Reply-To: <20040318003438.33358.qmail@web504.mail.yahoo.co.jp>
References: <20040318003438.33358.qmail@web504.mail.yahoo.co.jp>
Message-ID: <4058F4CC.9000604@indigoindustrial.co.nz>

SI wrote:

> Hi,
> 
> thanks for your reply.  But I guess I misfired a
> question.
> 
> I want to save the size of _GUI_ window, NOT console
> window.  GUI window is the window with "RGui"

One option to consider is to start R in "sdi" mode.  My shortcut for
R-1.8.1 on WinXP is...

"C:\Program Files\R\rw1081\bin\Rgui.exe" --sdi

Hmmm...  looks like ISO-2002-JP doesn't believe in the usual backslash.
 Substitute a back-slash for \.

This opens only a console window.  Help and graphics windows open as
windows of their own, not sub-windows of the GUI.  See if that helps.

Cheers

Jason



From ggrothendieck at myway.com  Thu Mar 18 02:10:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Mar 2004 20:10:43 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040318011043.6CEA539C8@mprdmxin.myway.com>





Consider the following example:

# substitute a with b in the indicated function. Seems to work.
> z <- substitute( function()a+1, list(a=quote(b)) )
> z
function() b + 1

# z is an object of class call so use eval
# to turn it into an object of class expression; however,
# when z is evaluated, the variable  a  returns.  
> eval(z)
function()a+1

Why did  a  suddenly reappear again after it had already been replaced?



From Wanzare at HCJP.com  Thu Mar 18 02:31:01 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Thu, 18 Mar 2004 10:31:01 +0900
Subject: [R] Lag length
Message-ID: <1CBA12F2D414914989C723D196B287DC0555B9@jp-svr-ex1.HCJP.COM>

Hello,
	Is their any function in R to calculate multivariate
generalization of AIC or SBC?

	Ideally, I am looking for a function which could generate a
tabular results set of AIC & SBC values for different lag length.

	I think a combination of glm & AIC function could do the trick
but I would have to repeat the procedure for different lag lengths so
was wondering if there is any clean way of doing this. 

TIA

Manoj



From loveland.1 at nd.edu  Thu Mar 18 03:58:34 2004
From: loveland.1 at nd.edu (Matt Loveland)
Date: Wed, 17 Mar 2004 21:58:34 -0500
Subject: [R] cannot allocate vector
Message-ID: <011701c40c94$e8924650$7ffc4a81@loveland>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040317/ea5d40db/attachment.pl

From ggrothendieck at myway.com  Thu Mar 18 04:09:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Mar 2004 22:09:50 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040318030950.7D0C839BE@mprdmxin.myway.com>



I left out the brackets in my last email but the problem
(a reappears after have been substituted out) still remains:

> z <- substitute( function(){a+1}, list(a=quote(b)) )
> z
function() {
    b + 1
}
> eval(z)
function(){a+1}



---
Date:   Wed, 17 Mar 2004 20:10:43 -0500 (EST) 
From:   Gabor Grothendieck <ggrothendieck at myway.com>
[ Add to Address Book | Block Address | Report as Spam ] 
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] substitute question 

 




Consider the following example:

# substitute a with b in the indicated function. Seems to work.
> z <- substitute( function()a+1, list(a=quote(b)) )
> z
function() b + 1

# z is an object of class call so use eval
# to turn it into an object of class expression; however,
# when z is evaluated, the variable a returns. 
> eval(z)
function()a+1

Why did a suddenly reappear again after it had already been replaced?



From risdpizza at yahoo.co.jp  Thu Mar 18 08:36:26 2004
From: risdpizza at yahoo.co.jp (SI)
Date: Thu, 18 Mar 2004 16:36:26 +0900 (JST)
Subject: [R] save GUI window position in windows version?
Message-ID: <20040318073626.21062.qmail@web503.mail.yahoo.co.jp>

Thanks a ton for your help.  Now I'm totally 
an SDI person.

Cheers, 

SI

FYI 
ISO-2002-JP and us all believe in \, which is our 
currency symbol.  We use it in place of a backslash.
For some good reasons, ancient programmers 
assigned the same code in US-ASCII for backslash
to \ in JIS X 0201.

>Hi,
>
>thanks for your reply.  But I guess I misfired a
question.
>
>I want to save the size of _GUI_ window, NOT console
>window.  GUI window is the window with "RGui" written
>on the left of title bar.  Console window is inside
>of the GUI window with "R Console" on its title bar.
>I save my preference of console window in the
>default Rconsole file.

Okay, I understand now.  I don't think that window size is
saved.  I
usually use R in SDI mode (option --sdi on the command
line), so I
don't see that.

If you want to save that window's position, you'll need to
play around
with the R source code (look in src/gnuwin32/rui.c, I
think; or
possibly one of the files in src/gnuwin32/front-ends).  I
think it's
easier to switch to SDI mode!

Duncan Murdoch


-----------
One option to consider is to start R in "sdi" mode.  My
shortcut for
R-1.8.1 on WinXP is...

"C:\Program Files\R\rw1081\bin\Rgui.exe" --sdi

Hmmm...  looks like ISO-2002-JP doesn't believe in the
usual backslash.
 Substitute a back-slash for \.

This opens only a console window.  Help and graphics
windows open as
windows of their own, not sub-windows of the GUI.  See if
that helps.

Cheers

Jason



From ripley at stats.ox.ac.uk  Thu Mar 18 09:24:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 08:24:26 +0000 (GMT)
Subject: [R] why-s of method dispatching
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3AB3@phost015.intermedia.net>
Message-ID: <Pine.LNX.4.44.0403180817530.2364-100000@gannet.stats>

On Wed, 17 Mar 2004, Vadim Ogranovich wrote:

> I am having a problem to understand why as.data.frame method doesn't
> dispatch properly on my class:
> 
> > setClass("Foo", "character")
> [1] "Foo"
> > as.data.frame(list(foo=new("Foo", .Data="a")))
> Error in as.data.frame.default(x[[i]], optional = TRUE) : 
>  can't coerce Foo into a data.frame
> 
> I was expecting that this would call as.data.frame.character.

You have set an S4 class and as.data.frame is an S3 generic.

> list(foo=new("Foo", .Data="a"))
$foo
An object of class "Foo"
[1] "a"

and what as.data.frame sees is

> attributes(list(foo=new("Foo", .Data="a"))$foo)
$class
[1] "Foo"
attr(,"package")
[1] ".GlobalEnv"

so thinks this is an S3 class it knows nothing about.

> Another puzzle. If I explicitly call as.data.frame.character() it would
> fail but for a different reason:
> 
> > as.data.frame.character(list(foo=new("Foo", .Data="a")))
> Error in unique.default(x) : unique() applies only to vectors
> 
> I was under an impression that an instance of "Foo" would be welcome
> anywhere a "character" was, but it seems to be more subtle. What am I
> missing?

The difference between S3 and S4 classes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ross at biostat.ucsf.edu  Thu Mar 18 09:26:45 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 18 Mar 2004 00:26:45 -0800
Subject: [R] R and package don't know they're built on same system
Message-ID: <20040318082645.GW1895@wheat.boylan.org>

I just had the interesting experience of building a package and R on
the same system, and having R refuse to load the resultant dynamic
library because it was thought to be for a different system.

The system was non-standard and beta, being a Linux-based 64 bit
Opteron system.  It uses the gnu tool chain.  The dynamic library was
built from C source.

When I tried to load the library R (1.8.1) complained
"package Rpmi was built for i686-pc-linux-gnu"
Inspection of R.version$platform, which is the thing the i686.... is
being compared to, shows that it is x86_64-unknown-linux-gnu.

I worked around this by removing the test causing the error (in
library.R), but, considering I built both R and Rmpi (an unofficial
version 0.4.6 from the author) within minutes of each other, it was
very surprising to find they had different notions of their system.

Any idea what's behind this, or how to fix it?



From ripley at stats.ox.ac.uk  Thu Mar 18 09:31:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 08:31:19 +0000 (GMT)
Subject: [R] cannot allocate vector
In-Reply-To: <011701c40c94$e8924650$7ffc4a81@loveland>
Message-ID: <Pine.LNX.4.44.0403180825400.2364-100000@gannet.stats>

On Wed, 17 Mar 2004, Matt Loveland wrote:

> I'm having trouble with glmmPQL.

I think you are having trouble with memory limits, actually.  As the 
author of glmmPQL, I don't appreciate my code being blamed for something 
else.

> I'm fitting a 2 level random intercept model, with 90,000 cases and about 330 groups.  I'm unable to get any results on the full data set.  I can get it to work if I sample down to about 30,000 cases.  But for models with N's much larger than that I get the following warning message:
> 
>  m3=glmmPQL(prepfood~iage+iemployed+iwhite+ieduclevl+imarried+servcomm+leadgrup+leadsty4, family=binomial, random=~1|congrega1,data=data)
> Error: cannot allocate vector of size 4135 Kb
> In addition: Warning message: 
> Reached total allocation of 253Mb: see help(memory.size) 
> 
> I've tried increasing my virtual memory size, and also defragmenting my
> hard drive.  It hasn't helped.  I've seen other people asking similar
> questions on the archive, but it seems that this problem should have
> gone away after earlier versions of R, is that right?

Do read the page it asks you too.  You are on Windows, and you need to use 
the --max-mem-size flag when starting R to increase the memory available 
to R.  However, if you do swapping may make your machine nigh unusable.

What did you not understand about help(memory.size)?
This is also in the rw-FAQ: what in that did you not understand?

> Is this a data problem, am I fitting a bad model, or is it a memory size
> problem.  I'm hoping the last one, and any help is appreciated.

Yes, so try a machine with 2Gb RAM.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Helge.Daebel at eawag.ch  Thu Mar 18 08:36:40 2004
From: Helge.Daebel at eawag.ch (Daebel Helge)
Date: Thu, 18 Mar 2004 08:36:40 +0100
Subject: [R] Question: Assigning the values of a time series to another one
	with dif. length and resolution
Message-ID: <79EACAE180E2CD48A096E486F53334AB390031@hathor.eawag.wroot.emp-eaw.ch>

Hi,
I have two time series organized in matixes data.1 and data.2. 
They have different length and resolution.

matrix data.1
t1  v1
t3  v3
t5  v5
...

matrix data.2
t1  0
t2  0
t3  0
...

desired result:
data.2
t1  v1
t2  0
t3  v3
...

What is the MOST EFFECTIVE way (since the matrixes are very large) to assign the values of data.1 to data.2 whenever time values are equal?

Thanks for your help,
Helge

P.S: My solution so far:

for ( i in 1 : nrow(data.1) )
    {
    index  <- which ( data.2[,1] == data.1[i,1] )
    data.2[index,2] <- data.1[i,2]
    }

_________________________________________ 
Swiss Federal Institute for Environmental
Science and Technology (EAWAG)

Dept. of Engineering Sciences

Helge Daebel
EAWAG, ING - CB D35
Ueberlandstrasse 133
CH-8600 D?bendorf, Switzerland

phone: +41 1 823 5052          
fax:   +41 1 823 5389
mail:  helge.daebel at eawag.ch
http:  www.eawag.ch/research_e/ing/uwe/daebel/bivariate_probability.htm



From merser at tiscali.dk  Thu Mar 18 09:36:48 2004
From: merser at tiscali.dk (merser@tiscali.dk)
Date: Thu, 18 Mar 2004 09:36:48 +0100
Subject: [R] logistic regression with temporal correlation
Message-ID: <404493BB000023BA@cpfe8.be.tisc.dk>

Hello

We would like to perform a logistic regression analysis weighting the
independent variable in  a temporal fashion, i.e. events occuring most
recent get highest weight. Does anyone know how to do this in R??

Regards
S. Merser and S. Lophaven



From mike.campana at freesurf.ch  Thu Mar 18 09:39:02 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Thu, 18 Mar 2004 09:39:02 +0100
Subject: [R] tapply
Message-ID: <1079599142.webexpressdV3.1.f@smtp.freesurf.ch>

Dear all
I have a dataframe containing hourly data of 3 parameters. 
I would like to create a dataframe containg daily mean values of these 
parameters. Additionally I want to keep information about time of 
measurement ("year","month","day"). 
With the function tapply I can average  over a column of the dataframe. 
I can repeat the function 2 time and  merge the vectors. In this way I 
obtain my new dataframe (see below).If I want to add the column day, 
month and year I can repeat tapply other three time. This system works.  


Question: is there a function that average in a single step over the 3 
columns?

Thanks a lot for your answer!
Regards
Mike Campana   

#### read the data
setwd("c:/R")
data <- NULL
data <- as.data.frame(read.table(file="Montreal.txt",header=F,skip=15))
colnames(data) 
<-c("year","month","day","hour","min","temp","press","ozone")
### create  mean value
temp_daily <- 
tapply(data$temp,data$year*10000+data$month*100+data$day,FUN=mean)
press_daily <- 
tapply(data$press,data$year*10000+data$month*100+data$day,FUN=mean)
ozone_daily <- 
tapply(data$ozone,data$year*10000+data$month*100+data$day,FUN=mean)
### merge the data
newdata <- as.data.frame (cbind(temp_daily,temp_daily,temp_daily))

---



---



---



---



---



From JonesW at kssg.com  Thu Mar 18 09:43:10 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Thu, 18 Mar 2004 08:43:10 -0000
Subject: [R] logistic regression with temporal correlation
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0FFC@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040318/38f6f854/attachment.pl

From ripley at stats.ox.ac.uk  Thu Mar 18 10:01:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 09:01:21 +0000 (GMT)
Subject: [R] logistic regression with temporal correlation
In-Reply-To: <404493BB000023BA@cpfe8.be.tisc.dk>
Message-ID: <Pine.LNX.4.44.0403180859070.2541-100000@gannet.stats>

This is not `temporal correlation'.  You can supply a weights argument to 
glm and so downweight older observations.  Since the theory is no longer 
exactly apposite, treat the standard errors etc with some caution.

On Thu, 18 Mar 2004 merser at tiscali.dk wrote:

> We would like to perform a logistic regression analysis weighting the
> independent variable in  a temporal fashion, i.e. events occuring most
> recent get highest weight. Does anyone know how to do this in R??
> 
> Regards
> S. Merser and S. Lophaven

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Mar 18 10:46:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Mar 2004 10:46:32 +0100
Subject: [R] R and package don't know they're built on same system
In-Reply-To: <20040318082645.GW1895@wheat.boylan.org>
References: <20040318082645.GW1895@wheat.boylan.org>
Message-ID: <x27jxiebd3.fsf@biostat.ku.dk>

Ross Boylan <ross at biostat.ucsf.edu> writes:

> I just had the interesting experience of building a package and R on
> the same system, and having R refuse to load the resultant dynamic
> library because it was thought to be for a different system.
> 
> The system was non-standard and beta, being a Linux-based 64 bit
> Opteron system.  It uses the gnu tool chain.  The dynamic library was
> built from C source.
> 
> When I tried to load the library R (1.8.1) complained
> "package Rpmi was built for i686-pc-linux-gnu"
> Inspection of R.version$platform, which is the thing the i686.... is
> being compared to, shows that it is x86_64-unknown-linux-gnu.
> 
> I worked around this by removing the test causing the error (in
> library.R), but, considering I built both R and Rmpi (an unofficial
> version 0.4.6 from the author) within minutes of each other, it was
> very surprising to find they had different notions of their system.
> 
> Any idea what's behind this, or how to fix it?

Hmm... It's probably not grabbing the i686-pc-linux-gnu out of
nowhere, so consider whether it might be picking up another package of
the same name somewhere along your library path, or see if the string
appears in the package you built. In the latter case, you need to
investigate the build process in detail.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tor.strand at cih.uib.no  Thu Mar 18 11:01:38 2004
From: tor.strand at cih.uib.no (Tor A Strand)
Date: Thu, 18 Mar 2004 11:01:38 +0100
Subject: [R] Install R on Mac OS.X
Message-ID: <3F5BCBD2-78C3-11D8-B2A8-000A9568DB4C@cih.uib.no>

I am running R 1.8.1 flawlessly on my mac with OS.X.3

My friend is running 1.6.1 but it is impossible for him to make 1.8.1 
work. I do not understand why it should work better in my computer. I 
do, however, have X.11 installed, he doesnt, could that be an issue and 
if it is, does it suffice to install X.11 after RAqua has been 
installed?

Tor A Strand, MD PhD
Centre for International Health
University of Bergen
Norway



From bitwrit at ozemail.com.au  Thu Mar 18 11:31:03 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 18 Mar 2004 21:31:03 +1100
Subject: [R] reading short int and float from binary connection
In-Reply-To: <40584532.3080509@hotmail.com>
References: <40581BF4.1060503@hotmail.com>
	<4058324A.9020309@statistik.uni-dortmund.de>
	<40584532.3080509@hotmail.com>
Message-ID: <20040318103902.WBHA15392.smta10.mail.ozemail.net@there>

Angel Lopez wrote:
> Thanks for the advise.
> I think I've found the solution, the problem was that the file had been
> written from a C program using a structure containing floats and int, if
> I rewrite the C code not using a structure but independent floats and
> ints the data then is read smoothly into R. I still can't figure out why
> a structure is different or how should I have read it into R but it is
> probably more a C than an R question .
> 
You're probably right there. A C structure may be silently aligned by the 
compiler so that value fields are not contiguous in memory. A binary write of 
such a structure cannot be assumed to have bytes in the same order described 
in the structure definition.

Jim



From andy_liaw at merck.com  Thu Mar 18 12:37:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Mar 2004 06:37:22 -0500
Subject: [R] R and package don't know they're built on same system
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A07@usrymx25.merck.com>

One thing to make sure is that all compilers used match; i.e., all 32-bit or
all 64-bit.  On our SLES8 pre-installed on Opteron, g++ was left out of the
64-bit toolchain.  R itself compiled fine (as 64-bit), but some packages
failed to install.  It was strange because the packages were built, but
failed to load.  I was only able to track it down because I happen to know
that the packages that failed contain C++ code, so they were compiled with
the 32-bit g++, and thus cannot be loaded into 64-bit R.

HTH,
Andy

> From: Ross Boylan
> 
> I just had the interesting experience of building a package and R on
> the same system, and having R refuse to load the resultant dynamic
> library because it was thought to be for a different system.
> 
> The system was non-standard and beta, being a Linux-based 64 bit
> Opteron system.  It uses the gnu tool chain.  The dynamic library was
> built from C source.
> 
> When I tried to load the library R (1.8.1) complained
> "package Rpmi was built for i686-pc-linux-gnu"
> Inspection of R.version$platform, which is the thing the i686.... is
> being compared to, shows that it is x86_64-unknown-linux-gnu.
> 
> I worked around this by removing the test causing the error (in
> library.R), but, considering I built both R and Rmpi (an unofficial
> version 0.4.6 from the author) within minutes of each other, it was
> very surprising to find they had different notions of their system.
> 
> Any idea what's behind this, or how to fix it?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From f.calboli at ucl.ac.uk  Thu Mar 18 13:53:31 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 18 Mar 2004 12:53:31 +0000
Subject: [R] R-business case
In-Reply-To: <200403181121.i2IB0hY3024253@hypatia.math.ethz.ch>
References: <200403181121.i2IB0hY3024253@hypatia.math.ethz.ch>
Message-ID: <1079614411.2944.18.camel@monkey>


> 
> Just to be provocative, it would be best to state the ultimate goals, then
> R users could be of more help.  We have submitted and published articles
> using R and are using R in production work on contracts from
> pharmaceutical companies.  It's difficult to know from the original note
> why we should spend time compiling such data.  Is anyone finding that R
> has some deficiencies with respect to their own work?
> 

All the analysis I do is done with R, so anything published where I am
in the author list is likely to have been done/revised with R.

The fact than R requires me to know what I am doing is more of an
advantage, IMHO, rather than a shortcoming... from a usability
standpoint I never felt I had any problem I could not deal with after
some thinking, or asking the list.

Regards,

Federico Calboli 

-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk



From fzoellne at TechFak.Uni-Bielefeld.DE  Thu Mar 18 13:02:51 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Thu, 18 Mar 2004 13:02:51 +0100
Subject: [R] SVM question
Message-ID: <20040318120251.GA26004@hindemith.TechFak.Uni-Bielefeld.DE>

Hi!

I have a question concerning the svm in the e1071 package.
I trained the svm by a set of samples, doing a 10 cross validation.
The summary function then prints out the total accuracy and single accuracies, works fine.

My question is then: Is it possible to get classification results per cross validation out the svm? I mean e.g. numbers about the true positives ,fp,fn,tf ? How do I get a list of the classified examples ? 

Thanks,
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From ripley at stats.ox.ac.uk  Thu Mar 18 13:12:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 12:12:04 +0000 (GMT)
Subject: [R] R and package don't know they're built on same system
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A07@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0403181159350.9107-100000@gannet.stats>

You may also want to check why the error message refers to Rpmi, since you 
said you have just built Rmpi, not the same thing!

Something is wrong about the story we are being told: if that is really 
the error message which was produced then pilot error is involved, and if 
not inaccurate reporting is involved.  My guess is that there is a 32-bit 
package (sic, not library) Rpmi.

On Thu, 18 Mar 2004, Liaw, Andy wrote:

> One thing to make sure is that all compilers used match; i.e., all 32-bit or
> all 64-bit.  On our SLES8 pre-installed on Opteron, g++ was left out of the
> 64-bit toolchain.  R itself compiled fine (as 64-bit), but some packages
> failed to install.  It was strange because the packages were built, but
> failed to load.  I was only able to track it down because I happen to know
> that the packages that failed contain C++ code, so they were compiled with
> the 32-bit g++, and thus cannot be loaded into 64-bit R.
> 
> HTH,
> Andy
> 
> > From: Ross Boylan
> > 
> > I just had the interesting experience of building a package and R on
> > the same system, and having R refuse to load the resultant dynamic
> > library because it was thought to be for a different system.
> > 
> > The system was non-standard and beta, being a Linux-based 64 bit
> > Opteron system.  It uses the gnu tool chain.  The dynamic library was
> > built from C source.
> > 
> > When I tried to load the library R (1.8.1) complained
> > "package Rpmi was built for i686-pc-linux-gnu"
> > Inspection of R.version$platform, which is the thing the i686.... is
> > being compared to, shows that it is x86_64-unknown-linux-gnu.
> > 
> > I worked around this by removing the test causing the error (in
> > library.R), but, considering I built both R and Rmpi (an unofficial
> > version 0.4.6 from the author) within minutes of each other, it was
> > very surprising to find they had different notions of their system.
> > 
> > Any idea what's behind this, or how to fix it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Mar 18 14:14:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Mar 2004 14:14:21 +0100
Subject: [R] help with file
In-Reply-To: <OF75FFD73F.88537F3D-ON85256E5A.0057431D-85256E5A.00587FB5@fmc.com>
References: <OF75FFD73F.88537F3D-ON85256E5A.0057431D-85256E5A.00587FB5@fmc.com>
Message-ID: <4059A0AD.6070405@statistik.uni-dortmund.de>

MARGARET GALIETTI wrote:

> Hi,
> 
> I am new at R, and I need some help with some basic stuff.
> 
> I want to do clustering of data. It works perfectly, but now I want to 
> export my file form R to my Linux directory. The problem is that the file 
> is generated only with the cluster numbers, but it does not carry over the 
> identifiers. Let me show you what I did :
> 
> 1. demo<-read.table("my_table", sep=",", header=TRUE)          # I 
> imported my table into R as a data frame. I had header and one of my 
> columns was called KEYS; this was the unique identifier for my data.
> 2. #clustering part
>     >names(demo) 
>     >dm<-dist(demo[,c(3,4,5)])  
>     >names(dm)<-demo[,2]                #the unique Id called KEYS is in 
> my column number 2
>     >cluster<-hclust(dm)
>     >plot(cluster)                                    #my data is 
> displayed in dendogram
> 
> 3. Now I want to see the breakdowns; I want to see every unique KEYS and 
> corresponding cluster number
>  
>     >result<-cutree(cluster, h =1)
>    >result                                   #I can see KEYS and 
> corresponding cluster numbers
> 
> 4. Now I want to export this info out of R into a file
>     How do I do this??? I am going CRAZY. All I get is the cluster numbers 
> but not the KEYS.
> 
> HELP!!!
> 
> Margaret
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


See ?write.table.
Or ?sink for redirecting R's output, and ?dump for saving R objects ....

Uwe Ligges



From lochapoka at web.de  Thu Mar 18 14:52:33 2004
From: lochapoka at web.de (Volker Bahn)
Date: Thu, 18 Mar 2004 08:52:33 -0500
Subject: [R] Re: nnet classification accuracy vs. other models
References: <405377C1.50305@pisem.net>
Message-ID: <012b01c40cf0$44a64c10$c8a66f82@Context>

I believe the paper below did a comparison relevant to your question but
possibly not exactly what you are asking (I'm not sure that nnet is
artificial neural networks - on of the techniques compared in the paper).

Hope this helps

Volker

Moisen G. G., and T. S. Frescino. 2002. Comparing five modelling techniques
for predicting forest characteristics. Ecological Modelling
157(2-3):209-225.

_______________________________

Volker Bahn

Dept. of Wildlife Ecology - Rm. 210
University of Maine
5755 Nutting Hall
Orono, Maine
04469-5755, USA
Tel. (207) 581 2799
Fax: (207) 581 2858
volker.bahn at umit.maine.edu
http://www.wle.umaine.edu/used_text%20files/Volker%20Bahn/home.htm


----- Original Message ----- 
From: "Albedo" <albedo at pisem.net>
To: <r-help at stat.math.ethz.ch>; <s-news at lists.biostat.wustl.edu>
Sent: Saturday, March 13, 2004 16:06
Subject: nnet classification accuracy vs. other models


| I was wandering if anybody ever tried to compare the classification
| accuracy of nnet to other (rpart, tree, bagging) models. From what I
| know, there is no reason to expect a significant difference in
| classification accuracy between these models, yet in my particular case
| I get about 10% error rate for tree, rpart and bagging model and 80%
| error rate for nnet, applied to the same data.
|
| Thanks.
|



From rossini at blindglobe.net  Thu Mar 18 14:57:41 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 18 Mar 2004 05:57:41 -0800
Subject: [R] R and package don't know they're built on same system
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A07@usrymx25.merck.com>
	(Andy Liaw's message of "Thu, 18 Mar 2004 06:37:22 -0500")
References: <3A822319EB35174CA3714066D590DCD504AF7A07@usrymx25.merck.com>
Message-ID: <85k71i8dgq.fsf@servant.blindglobe.net>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> One thing to make sure is that all compilers used match; i.e., all 32-bit or
> all 64-bit.  On our SLES8 pre-installed on Opteron, g++ was left out of the
> 64-bit toolchain.  R itself compiled fine (as 64-bit), but some packages
> failed to install.  It was strange because the packages were built, but
> failed to load.  I was only able to track it down because I happen to know
> that the packages that failed contain C++ code, so they were compiled with
> the 32-bit g++, and thus cannot be loaded into 64-bit R.

Not sure which system Ross is referring to (i.e. built from scratch,
or a distribution?) but there are bi-arch systems for the Opteron
which get confused.  This, along with Andy's guess about g++, suggest
the possibility of a mix-up.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From andrewr at uidaho.edu  Thu Mar 18 15:04:15 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 18 Mar 2004 06:04:15 -0800
Subject: [R] Install R on Mac OS.X
In-Reply-To: <3F5BCBD2-78C3-11D8-B2A8-000A9568DB4C@cih.uib.no>
References: <3F5BCBD2-78C3-11D8-B2A8-000A9568DB4C@cih.uib.no>
Message-ID: <200403180604.15185.andrewr@uidaho.edu>

In what way does 1.8.1 not work for your friend?

Andrew

On Thursday 18 March 2004 02:01, Tor A Strand wrote:
> I am running R 1.8.1 flawlessly on my mac with OS.X.3
>
> My friend is running 1.6.1 but it is impossible for him to make 1.8.1
> work. I do not understand why it should work better in my computer. I
> do, however, have X.11 installed, he doesnt, could that be an issue and
> if it is, does it suffice to install X.11 after RAqua has been
> installed?
>
> Tor A Strand, MD PhD
> Centre for International Health
> University of Bergen
> Norway
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From ggrothendieck at myway.com  Thu Mar 18 15:11:56 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 09:11:56 -0500 (EST)
Subject: [R] tapply
Message-ID: <20040318141156.732613977@mprdmxin.myway.com>




Try this (untested):

aggregate( data[,6:8], list(date = as.matrix(data[,1:3]) %*% c(10000,100,1)), mean )

---
Date:   Thu, 18 Mar 2004 09:39:02 +0100 
From:   <mike.campana at freesurf.ch>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] tapply 

 
Dear all
I have a dataframe containing hourly data of 3 parameters. 
I would like to create a dataframe containg daily mean values of these 
parameters. Additionally I want to keep information about time of 
measurement ("year","month","day"). 
With the function tapply I can average over a column of the dataframe. 
I can repeat the function 2 time and merge the vectors. In this way I 
obtain my new dataframe (see below).If I want to add the column day, 
month and year I can repeat tapply other three time. This system works. 


Question: is there a function that average in a single step over the 3 
columns?

Thanks a lot for your answer!
Regards
Mike Campana 

#### read the data
setwd("c:/R")
data <- NULL
data <- as.data.frame(read.table(file="Montreal.txt",header=F,skip=15))
colnames(data) 
<-c("year","month","day","hour","min","temp","press","ozone")
### create mean value
temp_daily <- 
tapply(data$temp,data$year*10000+data$month*100+data$day,FUN=mean)
press_daily <- 
tapply(data$press,data$year*10000+data$month*100+data$day,FUN=mean)
ozone_daily <- 
tapply(data$ozone,data$year*10000+data$month*100+data$day,FUN=mean)
### merge the data
newdata <- as.data.frame (cbind(temp_daily,temp_daily,temp_daily))



From thpe at hhbio.wasser.tu-dresden.de  Thu Mar 18 15:12:54 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 18 Mar 2004 15:12:54 +0100
Subject: [R] tapply
In-Reply-To: <1079599142.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1079599142.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4059AE66.8060504@hhbio.wasser.tu-dresden.de>

mike.campana at freesurf.ch wrote:

> Question: is there a function that average in a single step over the 3 
> columns?

You may look for ?aggregate

Thomas P.



From bates at stat.wisc.edu  Thu Mar 18 15:22:56 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 18 Mar 2004 08:22:56 -0600
Subject: [R] cannot allocate vector
In-Reply-To: <011701c40c94$e8924650$7ffc4a81@loveland>
References: <011701c40c94$e8924650$7ffc4a81@loveland>
Message-ID: <6r65d2z133.fsf@bates4.stat.wisc.edu>

"Matt Loveland" <loveland.1 at nd.edu> writes:

> I'm having trouble with glmmPQL.
> 
> I'm fitting a 2 level random intercept model, with 90,000 cases and
> about 330 groups.  I'm unable to get any results on the full data
> set.  I can get it to work if I sample down to about 30,000 cases.
> But for models with N's much larger than that I get the following
> warning message:
> 
>  m3=glmmPQL(prepfood~iage+iemployed+iwhite+ieduclevl+imarried+servcomm+leadgrup+leadsty4, family=binomial, random=~1|congrega1,data=data)
> Error: cannot allocate vector of size 4135 Kb
> In addition: Warning message: 
> Reached total allocation of 253Mb: see help(memory.size) 

It may be possible to fit the model on your current machine with the
current setting using function GLMM from package lme4.  This function
by default uses essentially the same algorithm as glmmPQL from MASS
(iteratively weighted calls to lme) but it employs a different version
of lme (glmmPQL calls lme from the nlme package while GLMM calls a
more efficient representation in the lme4 package itself).

Finally, there is yet another implementation of lme in development - a
version that is more economical in storage, more flexible in the model
structures that can be fit (for those who have been waiting, yes it
can fit models with crossed and partially crossed random effects using
a reasonable syntax and data representation), and is fast.  On models
fit to large data sets this version is remarkably fast.

Our schedule is to release new versions of the lme4 and Matrix
packages with R-1.9.0 (2004-04-04).  Please contact me off-list if you
want to participate in testing these packages or if you can provide
data for our testing.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From pastaska1934 at free.fr  Thu Mar 18 15:36:20 2004
From: pastaska1934 at free.fr (pastaska1934)
Date: Thu, 18 Mar 2004 15:36:20 +0100
Subject: [R] don't stop when error occurs
Message-ID: <001c01c40cf6$619920d0$74e04252@gonzague>

hi,
i'm doing some bootstraping on a data set, using kmeans for each bootstrap,
i mean i do a loop(200 times) and in each loop i use kmeans.
i have to count some occurences in a matrix result,
but somentimes kmeans fail, cause of the algorithm.
so i would like to avoid my function to stop on kmeans error
is there a way to do such a thing?
like
if(kmeans()=error)continue
or
if(kmeans != -1) go on

thanks

sebeuuuuuuuuuu^^



From andy_liaw at merck.com  Thu Mar 18 15:36:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Mar 2004 09:36:39 -0500
Subject: [R] don't stop when error occurs
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A0E@usrymx25.merck.com>

Read ?try, ?tryCatch, as well as the posting guide mentioned in the footer.

Andy

> From: pastaska1934
> 
> hi,
> i'm doing some bootstraping on a data set, using kmeans for 
> each bootstrap,
> i mean i do a loop(200 times) and in each loop i use kmeans.
> i have to count some occurences in a matrix result,
> but somentimes kmeans fail, cause of the algorithm.
> so i would like to avoid my function to stop on kmeans error
> is there a way to do such a thing?
> like
> if(kmeans()=error)continue
> or
> if(kmeans != -1) go on
> 
> thanks
> 
> sebeuuuuuuuuuu^^
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From abunn at montana.edu  Thu Mar 18 15:39:15 2004
From: abunn at montana.edu (Andy Bunn)
Date: Thu, 18 Mar 2004 07:39:15 -0700
Subject: [R] don't stop when error occurs
In-Reply-To: <001c01c40cf6$619920d0$74e04252@gonzague>
Message-ID: <001c01c40cf6$cc33a000$78f05a99@msu.montana.edu>

You can do that easily with 'try'
?try
     'try' is a wrapper to run an expression that might fail and allow
     the user's code to handle error-recovery.
HTH, Andy



From JonesW at kssg.com  Thu Mar 18 15:35:17 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Thu, 18 Mar 2004 14:35:17 -0000
Subject: [R] don't stop when error occurs
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F100C@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040318/64b762d9/attachment.pl

From Jim_Garrett at bd.com  Thu Mar 18 16:21:03 2004
From: Jim_Garrett at bd.com (Jim_Garrett@bd.com)
Date: Thu, 18 Mar 2004 10:21:03 -0500
Subject: [R] Re:  projection pursuit
Message-ID: <OFB6BCBB0E.B6C2F7DF-ON85256E5B.00502BE2@bd.com>

Luis,

See the fastICA package, in particular the final example in the function
fastICA's help page.  This doesn't leave you with density estimates, but
with projection-pursuit directions; you still have to figure out how to fit
a density estimate to the rotated data.  Actually, as I understand it,
XGobi also finds directions but does not fit a density.  However, any
multivariate density estimator ought to be applicable.  I'm not aware at
the moment of the tools R offers for multivariate density estimation, but
I'm sure there are multiple possibilities.

_The Elements of Statistical Learning_ by Hastie, Tibshirani, and Friedman
mention a "trick" to use classification tools (which model class
probabilities) to estimate density.  Fundamentally, generate data from a
reference distribution, and use the classification tool to estimate
probability of observed data (as opposed to generated data) as a function
of the inputs.  These probabilities, normalized to integrate to 1, form a
density estimate.  Since there are so very many classification tools
available, this trick offers a lot of flexibility.

Good luck.

Jim Garrett
Baltimore, Maryland, USA


**********************************************************************
This message is intended only for the designated recipient(s...{{dropped}}



From migreja at med.up.pt  Thu Mar 18 16:32:29 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?Júlia?= Rodrigues Igreja)
Date: Thu, 18 Mar 2004 15:32:29 -0000
Subject: [R] dsn
Message-ID: <4.3.2.7.1.19970101054058.00ad68a0@mail.med.up.pt>

Hi,

I've finally got a DSN working on my ODBC installation and I've tested it 
with the ODBC DataManager so I'm sure it's working.
However when trying to connect to the DSN via R interface I get an error:

 > 
odbcConnect("H:\\Desktop\\bd1",uid="tiago",pwd="archi",case="nochange",believeNRows=TRUE)
[1] -1
Warning messages:
1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver 
Manager]Data source name not found, and no default driver specified
2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn, ";UID=", 
uid, ";PWD=", pwd,

Before the odbcCOnnect() command I gave the library(RODBC) command and it 
gave no error
What am I missing?

Margarida



From scott.rifkin at yale.edu  Thu Mar 18 16:38:40 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Thu, 18 Mar 2004 10:38:40 -0500 (EST)
Subject: [R] two lme questions
Message-ID: <Pine.LNX.4.44.0403181016230.7188-100000@ajax.its.yale.edu>

1) I have the following data situation:

96 plots
12 varieties
2 time points
2 technical treatments

the experiment is arranged as follows:

a single plot has two varieties tested on it. if variety A on plot #1 has 
treatment T1 applied to it, then variety B on plot #1 has treatment T2 
applied to it.  across the whole experiment variety A is exposed to 
treatment T1 the same number of times as treatment T2.

with respect to time points, plots come in 3 kinds. (1) varietyA,
timepoint#1 vs. variety B, timepoint#1 (2) varietyA timepoint #2 vs. 
varietyB timepoint #2, and (3) varietyA timepoint #1 vs. variety A 
timepoint#2

plots and varieties are random samples from a population of plots and 
varieties, so they are random effects.  The technical treatment and 
timepoints are fixed effects.

i am particularly interested in the variance components for variety and 
timepoint within variety, in the estimate for the fixed stage effect and 
in the predictions (BLUP) for variety and stage within variety.

My First Question is about specifying the random part of the lme() 
statement

the fixed part is Measurement~Treatment+Time
the random part, i think, should include random=~1|variety/time or 
equivalently(?) list(variety=~1,Time=~1)

but how do i also specify that plot should be a random effect?  it's not 
nested within variety, nor is variety nested within it.



2) I have fixed effects as above where each only has two kinds (2 
Treatments, 2 Times).  When I use lme and look at the estimates for the 
fixed effects I get output that looks like:

>summary(asdf.lme)
...

Fixed effects: Measurement ~ Treatment+Time
		Value	 Std.Error ...
(Intercept)	xxx	xxx
TreatmentHot	xxx	xxx
TimeEarly	xxx	xxx
  Correlation
...


where my two treatments are Hot and Cold and my two times are Early and 
Late and the xxx are actual numbers.

Why isnt there any line for the Cold treatment and the late time?  is it 
because these necessarily are the opposite of the Hot and Early ones so 
putting them in would be redundant (i.e. Hot+Cold=0, Early+Late=0)?


Thanks much,
Scott Rifkin
scott.rifkin at yale.edu



From tplate at blackmesacapital.com  Thu Mar 18 16:41:54 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 18 Mar 2004 08:41:54 -0700
Subject: [R] substitute question
In-Reply-To: <20040318030950.7D0C839BE@mprdmxin.myway.com>
References: <20040318030950.7D0C839BE@mprdmxin.myway.com>
Message-ID: <6.0.3.0.2.20040318082631.09088978@mailhost.blackmesacapital.com>

This is because of the saved attribute "source" on z (that doesn't get 
printed out before evaluating z, because z is not yet then a function).

To complete your example:

 > z <- substitute( function(){a+1}, list(a=quote(b)) )
 > z
function() {
     b + 1
}
 > eval(z)
function(){a+1}
 > ze <- eval(z)
 > attributes(ze)
$source
[1] "function(){a+1}"

 > attr(ze, "source") <- NULL
 > ze
function ()
{
     b + 1
}
 >

I previously wrote on this topic:

Date: Fri, 24 Oct 2003 09:42:55 -0600
To: R-help at stat.math.ethz.ch, Peter Dalgaard <p.dalgaard at biostat.ku.dk>
From: Tony Plate <tplate at blackmesacapital.com>
Subject: Re: [R] what's going on here with substitute() ?
Mime-Version: 1.0
Content-Type: text/plain; charset="us-ascii"; format=flowed

Peter, thank you for the explanation.  This is indeed what is happening.

Might I suggest the following passage for inclusion in the help page for 
"function", and possibly also "body", in the DETAILS or WARNING section:

"Note that the text of the original function definition is saved as an 
attribute "source" on the function, and this is printed out when the 
function is printed.  Hence, if the function body is changed in some way 
other than by assigning a value via body() (which removes the "source" 
attribute), the printed form of the function may not be the same as the 
actual function body."

Something along these lines could also go in the help for "eval", though if 
it were only there it might be very difficult to find if one were trying to 
look up puzzling behavior of a function.

Here is a transcript that shows what is happening, with another suggestion 
following it.

 > eval(substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]]))))
 > this.is.R
function() X
 > body(this.is.R)
[1] TRUE
 > attributes(this.is.R)
$source
[1] "function() X"
 > attributes(this.is.R) <- NULL
 > this.is.R
function ()
TRUE
 > # the "source" attribute comes from function definition:
 > attributes(function() X)
$source
[1] "function() X"
 > # and seems to be added by "eval":
 > attr(eval(parse(text="function() TRUE")[[1]]), "source")
[1] "function() TRUE"
 >

 > # we can assign bogus "source"
 > attr(this.is.R, "source") <- "a totally bogus function body"
 > this.is.R
a totally bogus function body
 > # assigning to body() removes "source"
 > body(this.is.R) <- list(666)
 > this.is.R
function ()
666
 > attr(this.is.R, "source")
NULL
 >

An even better approach might be something that gave a warning on printing 
if the parsed "source" attribute was not identical to the language object 
being printed.  This would probably belong in the code for "case LANGSXP:" 
in the function PrintValueRec in main/print.c (if it were written in R, I 
could contribute a patch, but right now I don't have time to try to 
understand the C there.)  R code to do the test could be something like this:

 > f <- this.is.R
 > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] FALSE
 > f <- function() TRUE
 > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] TRUE
 >

-- Tony Plate


At Wednesday 08:09 PM 3/17/2004, Gabor Grothendieck wrote:


>I left out the brackets in my last email but the problem
>(a reappears after have been substituted out) still remains:
>
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
>function() {
>     b + 1
>}
> > eval(z)
>function(){a+1}
>
>
>
>---
>Date:   Wed, 17 Mar 2004 20:10:43 -0500 (EST)
>From:   Gabor Grothendieck <ggrothendieck at myway.com>
>[ Add to Address Book | Block Address | Report as Spam ]
>To:   <R-help at stat.math.ethz.ch>
>Subject:   [R] substitute question
>
>
>
>
>
>
>Consider the following example:
>
># substitute a with b in the indicated function. Seems to work.
> > z <- substitute( function()a+1, list(a=quote(b)) )
> > z
>function() b + 1
>
># z is an object of class call so use eval
># to turn it into an object of class expression; however,
># when z is evaluated, the variable a returns.
> > eval(z)
>function()a+1
>
>Why did a suddenly reappear again after it had already been replaced?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vaclav.petricek at mff.cuni.cz  Thu Mar 18 16:42:15 2004
From: vaclav.petricek at mff.cuni.cz (Vaclav Petricek)
Date: Thu, 18 Mar 2004 16:42:15 +0100 (CET)
Subject: [R] Draw abbreviated key (lattice, xyplot)
In-Reply-To: <200403180604.15185.andrewr@uidaho.edu>
References: <3F5BCBD2-78C3-11D8-B2A8-000A9568DB4C@cih.uib.no>
	<200403180604.15185.andrewr@uidaho.edu>
Message-ID: <Pine.BSF.4.50.0403181517311.28462-100000@sec.ms.mff.cuni.cz>


Hello

I have been experimenting with xyplot, reading the help and googling but
I am still unable to draw an abbreviated key.

I would like to display key for a few particular countries.

My dataset looks like this

   year papers        country papers.total
1  1988    403            USA          551
2  1988     31 United Kingdom          551
3  1988     24         Canada          551
4  1988     20    Netherlands          551
5  1988     19         Israel          551
6  1988     16        Germany          551
7  1988     13         France          551
8  1988     10          Italy          551
9  1988      8    Switzerland          551
10 1988      5          Japan          551
11 1988      5        Denmark          551
12 1988      3          Spain          551
13 1988      3         Russia          551
14 1988      2         Sweden          551
15 1988      2         Poland          551
16 1988      2        Finland          551
17 1988      2         Brasil          551
18 1988      2      Australia          551
19 1988      1       Thailand          551
20 1988      1         Norway          551
21 1988      1         Mexico          551
22 1988      1        Ireland          551
23 1988      1         Greece          551
24 1989    649            USA          926
25 1989     53 United Kingdom          926
26 1989     43         France          926
27 1989     37         Canada          926
28 1989     36        Germany          926
29 1989     28    Netherlands          926
30 1989     19         Sweden          926
[...]

I use xyplot to show how many papers

> xyplot(papers~year,groups=country,type='l',auto.key=T)

produces an extremely long key

> xyplot(papers~year,groups=country,type='l',key=simpleKey(levels(as.factor(country))[1:5]))

shows *alphabetically* first five countries in the key

> xyplot(papers~year,groups=country,type='l',key=simpleKey(c('USA','Germany')))

displays correct countries but the colors obviously do not match.

Could you please point me in the right direction?

Vaclav



From ligges at statistik.uni-dortmund.de  Thu Mar 18 17:12:53 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Mar 2004 17:12:53 +0100
Subject: [R] dsn
In-Reply-To: <4.3.2.7.1.19970101054058.00ad68a0@mail.med.up.pt>
References: <4.3.2.7.1.19970101054058.00ad68a0@mail.med.up.pt>
Message-ID: <4059CA85.2070609@statistik.uni-dortmund.de>

Margarida =?iso-8859-1?Q?J?lia?= Rodrigues Igreja wrote:

> Hi,
> 
> I've finally got a DSN working on my ODBC installation and I've tested 
> it with the ODBC DataManager so I'm sure it's working.
> However when trying to connect to the DSN via R interface I get an error:
> 
>  > 
> odbcConnect("H:\\Desktop\\bd1",uid="tiago",pwd="archi",case="nochange",believeNRows=TRUE) 

If you have specified the DSN "bd1" using Windows' Control Panel, you 
don't need to specify any path - just specify the DSN itself, as in:

 
odbcConnect("bd1",uid="tiago",pwd="archi",case="nochange",believeNRows=TRUE) 



Uwe Ligges


> [1] -1
> Warning messages:
> 1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver 
> Manager]Data source name not found, and no default driver specified
> 2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn, 
> ";UID=", uid, ";PWD=", pwd,
> 
> Before the odbcCOnnect() command I gave the library(RODBC) command and 
> it gave no error
> What am I missing?
> 
> Margarida
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From DMiddleton at fisheries.gov.fk  Thu Mar 18 16:24:14 2004
From: DMiddleton at fisheries.gov.fk (David Middleton)
Date: Thu, 18 Mar 2004 13:24:14 -0200
Subject: [R] dsn
Message-ID: <AEF7C40954294D42990B3F90C6C73A457BCB67@figmail.fig.fk>


Margarida

> odbcConnect("H:\\Desktop\\bd1",uid="tiago",pwd="archi",case="nochange",believeNRows=TRUE)
> [1] -1
> Warning messages:
> 1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver 
> Manager]Data source name not found, and no default driver specified

You simply need to give the data source name (DSN), not a path, as the first argument to odbcConnect.  If you want to connect directly to a database file, rather than via a DSN, the RODBC package also provides odbcConnectAccess etc.

I am slightly surprised that you appear to be using unixODBC on what appears to be (from the path specification) a windows system.  I guess this may be possible via cygwin - maybe there's even a win32 release of unixODBC.  RODBC works fine with the standard MS ODBC setup on windows systems as far as I can tell.

David



From tlumley at u.washington.edu  Thu Mar 18 18:00:02 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Mar 2004 09:00:02 -0800 (PST)
Subject: [R] substitute question
In-Reply-To: <20040318030950.7D0C839BE@mprdmxin.myway.com>
References: <20040318030950.7D0C839BE@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0403180849540.168194@homer04.u.washington.edu>

On Wed, 17 Mar 2004, Gabor Grothendieck wrote:

>
>
> I left out the brackets in my last email but the problem
> (a reappears after have been substituted out) still remains:
>
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
> function() {
>     b + 1
> }
> > eval(z)
> function(){a+1}


Interesting.

Appearances are misleading, however:
> z <- substitute( function(){a+1}, list(a=quote(b)) )
> z
function() {
    b + 1
}
> f<-eval(z)
> f()
Error in f() : Object "b" not found
> f
function(){a+1}
> attr(f,"source")<-NULL
> f
function ()
{
    b + 1
}

So it isn't that eval(z) has a+1 inside, it just has a "source" attribute
with a+1.

Looking more carefully at z
> as.list(z)
[[1]]
`function`

[[2]]
NULL

[[3]]
{
    b + 1
}

[[4]]
[1] "function(){a+1}"

so the original construction of z has kept the source (not, however, as a
"source" attribute).

There is method to our madness here.  It is impossible (or at least too
complicated) to keep comments in the right place as a function is
parsed and deparsed.  In the old days, comments would occasionally move
around, sometimes in very misleading ways (IIRC with if(){}else{} cases)

Now we keep a copy of the source code with functions created interactively
or with source(), and drop the comments on parsing.  This is controlled by
options("keep.source").

If you do a lot of substitute()-style programming you may want
options(keep.source=FALSE).

	-thomas

PS: There are, of course, interesting possibilities for creative abuse of
the source attribute....



From ripley at stats.ox.ac.uk  Thu Mar 18 18:30:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 17:30:37 +0000 (GMT)
Subject: [R] projection pursuit
In-Reply-To: <1079535597.405867ed8948a@webmail.fe.up.pt>
Message-ID: <Pine.LNX.4.44.0403181728540.12577-100000@gannet.stats>

I don't think anyone has actually answered this.  PPDE is a technical 
term, defined in

Friedman, Steutzle and Schroeder (1984) Projection pursuit density
estimation. Journal of the American Statistical Association,
79(387):599-608, September 1984.

I don't know of an R implementation, but have known of S ones (to which I
no longer have access).

If that is not what you mean by PPDE, please give us the reference(s) you 
have in mind.


On Wed, 17 Mar 2004 lmsilva at fe.up.pt wrote:

> Does R have a package that performs projection pursuit density estimation? Or 
> anyone knows code in Matlab or C for example to do this?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From feh3k at spamcop.net  Thu Mar 18 18:36:02 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 18 Mar 2004 11:36:02 -0600
Subject: [R] .First.lib failed
Message-ID: <20040318113602.3e8250b3.feh3k@spamcop.net>

Using R-devel of 2Mar04 on debian linux I am getting an error when running
/usr/local/src/R-devel/bin/R CMD check --no-clean Hmisc:


* checking S3 generic/method consistency ... WARNING
Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
character.
only = TRUE, verbose = FALSE) : 
        .First.lib failed
Execution halted

I remember having to solve this problem previously but I can't remember
how.

Contents of R/Hmisc/s/First.lib.s (main functions in Hmisc are in
R/Hmisc/R/Hmisc.R):

.First.lib <- function(lib, pkg, verbose=TRUE, ...) {
  if(verbose)
    cat("Hmisc library by Frank E Harrell Jr\n\n",
        "Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')\n",
        "to see overall documentation.\n\n",
        "Hmisc redefines [.factor to drop unused levels of factor
variables\n",
        "when subscripting. To prevent this behaviour, issue the
command\n",
        "options(drop.unused.levels=F).\n\n",
        sep='')
  library.dynam("Hmisc", pkg, lib)
  invisible()
}

If I run /usr/local/src/R-devel/bin/R and type library(Hmisc) everything
is fine.

Thanks for any pointers.  -Frank Harrell



From andy_liaw at merck.com  Thu Mar 18 17:13:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Mar 2004 11:13:25 -0500
Subject: [R] Re:  projection pursuit
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A12@usrymx25.merck.com>

> From: Jim_Garrett at bd.com
> 
> Luis,
> 
> See the fastICA package, in particular the final example in 
> the function
> fastICA's help page.  This doesn't leave you with density 
> estimates, but
> with projection-pursuit directions; you still have to figure 
> out how to fit
> a density estimate to the rotated data.  Actually, as I understand it,
> XGobi also finds directions but does not fit a density.  However, any
> multivariate density estimator ought to be applicable.  I'm 
> not aware at
> the moment of the tools R offers for multivariate density 
> estimation, but
> I'm sure there are multiple possibilities.

I believe locfit() can handle up to 5 dimensions.


> _The Elements of Statistical Learning_ by Hastie, Tibshirani, 
> and Friedman
> mention a "trick" to use classification tools (which model class
> probabilities) to estimate density.  Fundamentally, generate 
> data from a
> reference distribution, and use the classification tool to estimate
> probability of observed data (as opposed to generated data) 
> as a function
> of the inputs.  These probabilities, normalized to integrate 
> to 1, form a
> density estimate.  Since there are so very many classification tools
> available, this trick offers a lot of flexibility.

Leo Breiman had talked about this `trick' in his early presentations on
random forest.  However, from my limited experiment, this is an extremely
poor way of estimating density.  Has anyone else tried it?

Cheers,
Andy

 
> Good luck.
> 
> Jim Garrett
> Baltimore, Maryland, USA
> 
> 
> **********************************************************************
> This message is intended only for the designated 
> recipient(s...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From gwiggner at lix.polytechnique.fr  Thu Mar 18 19:03:53 2004
From: gwiggner at lix.polytechnique.fr (Claus Gwiggner)
Date: 18 Mar 2004 19:03:53 +0100
Subject: [R] Goodness of fit
Message-ID: <1079633033.3125.42.camel@cpgwiggn-mw.eurocontrol.fr>

Hello,

where do I find a good document (on the web, if possible) on goodness of
fit tests ?



From martin at lirmm.fr  Thu Mar 18 19:17:10 2004
From: martin at lirmm.fr (Martin Olivier)
Date: Thu, 18 Mar 2004 19:17:10 +0100
Subject: [R] help with aov
Message-ID: <4059E7A6.4080601@lirmm.fr>

Hi all,

Suppose the following data and the simple model
y<-1:12+rnorm(12)
fac1<-c(rep("A",4),rep("B",4),rep("C",4))
fac2<-rep(c("D","C"),6)
dat<-data.frame(y,fac1,fac2)
tmp<-aov(y~fac1+fac2,dat)

the command tmp$coeff gives the fllowing results :

(Intercept)       fac1B       fac1C       fac2D
3.307888    2.898187    7.409010   -1.088588

But mean(y) gives 6.199327 and is different of Intercept..
So, I don't understand the parametrization with the aov() function.
I would like the estimations with the constraints
 fac1A+fac1B+fac1C =0 and fac2D+fac2C=0...

What is the solution?


Thanks,
Olivier.



From ripley at stats.ox.ac.uk  Thu Mar 18 19:36:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 18:36:46 +0000 (GMT)
Subject: [R] help with aov
In-Reply-To: <4059E7A6.4080601@lirmm.fr>
Message-ID: <Pine.LNX.4.44.0403181833080.12739-100000@gannet.stats>

This is nothing to do with aov: if you want to look at coefficients you 
may as well use lm().  The idea of aov is to look at factors as a whole.

You do need to understand codings: looks like you want to specify 
contr.sum contrasts.  The best place to read about this is chapter 6 of 
MASS.


On Thu, 18 Mar 2004, Martin Olivier wrote:

> Hi all,
> 
> Suppose the following data and the simple model
> y<-1:12+rnorm(12)
> fac1<-c(rep("A",4),rep("B",4),rep("C",4))
> fac2<-rep(c("D","C"),6)
> dat<-data.frame(y,fac1,fac2)
> tmp<-aov(y~fac1+fac2,dat)
> 
> the command tmp$coeff gives the fllowing results :
> 
> (Intercept)       fac1B       fac1C       fac2D
> 3.307888    2.898187    7.409010   -1.088588
> 
> But mean(y) gives 6.199327 and is different of Intercept..
> So, I don't understand the parametrization with the aov() function.
> I would like the estimations with the constraints
>  fac1A+fac1B+fac1C =0 and fac2D+fac2C=0...
> 
> What is the solution?
> 
> 
> Thanks,
> Olivier.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Thu Mar 18 20:00:03 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 14:00:03 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040318190003.E8DE939BF@mprdmxin.myway.com>


Tony, Thomas.  Thanks for your help.  Your comments were very
useful.

Unfortunately, my next step gives me a new round of problems.

The following is the same as the last example except that instead
of hard coding the function into the expression I wanted to 
pass it to the expression.  It seems like one has to do a double
substitute to get this effect but I am having problems getting this
to work.

In the code below we first show that the keep.source option has been
set to FALSE so that the source attribute does not mislead us.

Then we do a double substitute.  The outer substitute just creates
the substitute that was in my previous question.  This outer
substitute produces z, a call object.  So far its as expected.

We then do ze <- eval(z) but we get a function object right away.  I was
expecting that we get an expression object.  Even worse, the function does not
have  a  replaced with  b -- even though this did work in the previous example 
that I posted.  

What's wrong?


> options()$keep.source
[1] FALSE
> f <- function(){a+1}
> z <- substitute(substitute(f,list(a=quote(b))),list(f=f))
> class(z)
[1] "call"
> as.list(z)
[[1]]
substitute

[[2]]
function () 
{
    a + 1
}

[[3]]
list(a = quote(b))

> ze <- eval(z)
> class(ze)
[1] "function"
> ze
function () 
{
    a + 1
}
> attr(ze,"source")
NULL
> 

---

Date:   Thu, 18 Mar 2004 09:00:02 -0800 (PST) 
From:   Thomas Lumley <tlumley at u.washington.edu>
To:   Gabor Grothendieck <ggrothendieck at myway.com> 
Cc:   <R-help at stat.math.ethz.ch> 
Subject:   RE: [R] substitute question 

 
On Wed, 17 Mar 2004, Gabor Grothendieck wrote:

>
>
> I left out the brackets in my last email but the problem
> (a reappears after have been substituted out) still remains:
>
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
> function() {
> b + 1
> }
> > eval(z)
> function(){a+1}


Interesting.

Appearances are misleading, however:
> z <- substitute( function(){a+1}, list(a=quote(b)) )
> z
function() {
b + 1
}
> f<-eval(z)
> f()
Error in f() : Object "b" not found
> f
function(){a+1}
> attr(f,"source")<-NULL
> f
function ()
{
b + 1
}

So it isn't that eval(z) has a+1 inside, it just has a "source" attribute
with a+1.

Looking more carefully at z
> as.list(z)
[[1]]
`function`

[[2]]
NULL

[[3]]
{
b + 1
}

[[4]]
[1] "function(){a+1}"

so the original construction of z has kept the source (not, however, as a
"source" attribute).

There is method to our madness here. It is impossible (or at least too
complicated) to keep comments in the right place as a function is
parsed and deparsed. In the old days, comments would occasionally move
around, sometimes in very misleading ways (IIRC with if(){}else{} cases)

Now we keep a copy of the source code with functions created interactively
or with source(), and drop the comments on parsing. This is controlled by
options("keep.source").

If you do a lot of substitute()-style programming you may want
options(keep.source=FALSE).

     -thomas

PS: There are, of course, interesting possibilities for creative abuse of
the source attribute....

---

Date:   Thu, 18 Mar 2004 08:41:54 -0700 
From:   Tony Plate <tplate at blackmesacapital.com>
To:   <ggrothendieck at myway.com>, <R-help at stat.math.ethz.ch> 
Subject:   RE: [R] substitute question 

 
This is because of the saved attribute "source" on z (that doesn't get 
printed out before evaluating z, because z is not yet then a function).

To complete your example:

> z <- substitute( function(){a+1}, list(a=quote(b)) )
> z
function() {
b + 1
}
> eval(z)
function(){a+1}
> ze <- eval(z)
> attributes(ze)
$source
[1] "function(){a+1}"

> attr(ze, "source") <- NULL
> ze
function ()
{
b + 1
}
>

I previously wrote on this topic:

Date: Fri, 24 Oct 2003 09:42:55 -0600
To: R-help at stat.math.ethz.ch, Peter Dalgaard <p.dalgaard at biostat.ku.dk>
From: Tony Plate <tplate at blackmesacapital.com>
Subject: Re: [R] what's going on here with substitute() ?
Mime-Version: 1.0
Content-Type: text/plain; charset="us-ascii"; format=flowed

Peter, thank you for the explanation. This is indeed what is happening.

Might I suggest the following passage for inclusion in the help page for 
"function", and possibly also "body", in the DETAILS or WARNING section:

"Note that the text of the original function definition is saved as an 
attribute "source" on the function, and this is printed out when the 
function is printed. Hence, if the function body is changed in some way 
other than by assigning a value via body() (which removes the "source" 
attribute), the printed form of the function may not be the same as the 
actual function body."

Something along these lines could also go in the help for "eval", though if 
it were only there it might be very difficult to find if one were trying to 
look up puzzling behavior of a function.

Here is a transcript that shows what is happening, with another suggestion 
following it.

> eval(substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]]))))
> this.is.R
function() X
> body(this.is.R)
[1] TRUE
> attributes(this.is.R)
$source
[1] "function() X"
> attributes(this.is.R) <- NULL
> this.is.R
function ()
TRUE
> # the "source" attribute comes from function definition:
> attributes(function() X)
$source
[1] "function() X"
> # and seems to be added by "eval":
> attr(eval(parse(text="function() TRUE")[[1]]), "source")
[1] "function() TRUE"
>

> # we can assign bogus "source"
> attr(this.is.R, "source") <- "a totally bogus function body"
> this.is.R
a totally bogus function body
> # assigning to body() removes "source"
> body(this.is.R) <- list(666)
> this.is.R
function ()
666
> attr(this.is.R, "source")
NULL
>

An even better approach might be something that gave a warning on printing 
if the parsed "source" attribute was not identical to the language object 
being printed. This would probably belong in the code for "case LANGSXP:" 
in the function PrintValueRec in main/print.c (if it were written in R, I 
could contribute a patch, but right now I don't have time to try to 
understand the C there.) R code to do the test could be something like this:

> f <- this.is.R
> identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] FALSE
> f <- function() TRUE
> identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] TRUE
>

-- Tony Plate


At Wednesday 08:09 PM 3/17/2004, Gabor Grothendieck wrote:


>I left out the brackets in my last email but the problem
>(a reappears after have been substituted out) still remains:
>
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
>function() {
> b + 1
>}
> > eval(z)
>function(){a+1}
>
>
>
>---
>Date: Wed, 17 Mar 2004 20:10:43 -0500 (EST)
>From: Gabor Grothendieck <ggrothendieck at myway.com>
>[ Add to Address Book | Block Address | Report as Spam ]
>To: <R-help at stat.math.ethz.ch>
>Subject: [R] substitute question
>
>
>
>
>
>
>Consider the following example:
>
># substitute a with b in the indicated function. Seems to work.
> > z <- substitute( function()a+1, list(a=quote(b)) )
> > z
>function() b + 1
>
># z is an object of class call so use eval
># to turn it into an object of class expression; however,
># when z is evaluated, the variable a returns.
> > eval(z)
>function()a+1
>
>Why did a suddenly reappear again after it had already been replaced?



From tplate at blackmesacapital.com  Thu Mar 18 20:15:45 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 18 Mar 2004 12:15:45 -0700
Subject: [R] substitute question
In-Reply-To: <20040318190003.E8DE939BF@mprdmxin.myway.com>
References: <20040318190003.E8DE939BF@mprdmxin.myway.com>
Message-ID: <6.0.3.0.2.20040318121035.090ab120@mailhost.blackmesacapital.com>

Gabor, I suspect you are overlooking the fact that substitute() does not 
evaluate its first argument.  So, in the previous example you were giving 
an unevaluated expression as the first argument of substitute().  In your 
most recent example you are supplying a function object as the first 
argument of substitute() (by supplying it as the value to be substituted 
for f).  I think you can get what you want by first doing f <- 
Quote(function(){a+1}):

 > f <- Quote(function(){a+1})
 > f
function() {
     a + 1
}
 > z <- substitute(substitute(f,list(a=quote(b))),list(f=f))
 > z
substitute(function() {
     a + 1
}, list(a = quote(b)))
 > eval(z)
function() {
     b + 1
}
 > eval(eval(z)) # this displays the "source" attribute
function(){a+1}
 > body(eval(eval(z)))
{
     b + 1
}
 >

hope this helps,

Tony Plate

At Thursday 12:00 PM 3/18/2004, you wrote:

>Tony, Thomas.  Thanks for your help.  Your comments were very
>useful.
>
>Unfortunately, my next step gives me a new round of problems.
>
>The following is the same as the last example except that instead
>of hard coding the function into the expression I wanted to
>pass it to the expression.  It seems like one has to do a double
>substitute to get this effect but I am having problems getting this
>to work.
>
>In the code below we first show that the keep.source option has been
>set to FALSE so that the source attribute does not mislead us.
>
>Then we do a double substitute.  The outer substitute just creates
>the substitute that was in my previous question.  This outer
>substitute produces z, a call object.  So far its as expected.
>
>We then do ze <- eval(z) but we get a function object right away.  I was
>expecting that we get an expression object.  Even worse, the function does not
>have  a  replaced with  b -- even though this did work in the previous 
>example
>that I posted.
>
>What's wrong?
>
>
> > options()$keep.source
>[1] FALSE
> > f <- function(){a+1}
> > z <- substitute(substitute(f,list(a=quote(b))),list(f=f))
> > class(z)
>[1] "call"
> > as.list(z)
>[[1]]
>substitute
>
>[[2]]
>function ()
>{
>     a + 1
>}
>
>[[3]]
>list(a = quote(b))
>
> > ze <- eval(z)
> > class(ze)
>[1] "function"
> > ze
>function ()
>{
>     a + 1
>}
> > attr(ze,"source")
>NULL
> >
>
>---
>
>Date:   Thu, 18 Mar 2004 09:00:02 -0800 (PST)
>From:   Thomas Lumley <tlumley at u.washington.edu>
>To:   Gabor Grothendieck <ggrothendieck at myway.com>
>Cc:   <R-help at stat.math.ethz.ch>
>Subject:   RE: [R] substitute question
>
>
>On Wed, 17 Mar 2004, Gabor Grothendieck wrote:
>
> >
> >
> > I left out the brackets in my last email but the problem
> > (a reappears after have been substituted out) still remains:
> >
> > > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > > z
> > function() {
> > b + 1
> > }
> > > eval(z)
> > function(){a+1}
>
>
>Interesting.
>
>Appearances are misleading, however:
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
>function() {
>b + 1
>}
> > f<-eval(z)
> > f()
>Error in f() : Object "b" not found
> > f
>function(){a+1}
> > attr(f,"source")<-NULL
> > f
>function ()
>{
>b + 1
>}
>
>So it isn't that eval(z) has a+1 inside, it just has a "source" attribute
>with a+1.
>
>Looking more carefully at z
> > as.list(z)
>[[1]]
>`function`
>
>[[2]]
>NULL
>
>[[3]]
>{
>b + 1
>}
>
>[[4]]
>[1] "function(){a+1}"
>
>so the original construction of z has kept the source (not, however, as a
>"source" attribute).
>
>There is method to our madness here. It is impossible (or at least too
>complicated) to keep comments in the right place as a function is
>parsed and deparsed. In the old days, comments would occasionally move
>around, sometimes in very misleading ways (IIRC with if(){}else{} cases)
>
>Now we keep a copy of the source code with functions created interactively
>or with source(), and drop the comments on parsing. This is controlled by
>options("keep.source").
>
>If you do a lot of substitute()-style programming you may want
>options(keep.source=FALSE).
>
>      -thomas
>
>PS: There are, of course, interesting possibilities for creative abuse of
>the source attribute....
>
>---
>
>Date:   Thu, 18 Mar 2004 08:41:54 -0700
>From:   Tony Plate <tplate at blackmesacapital.com>
>To:   <ggrothendieck at myway.com>, <R-help at stat.math.ethz.ch>
>Subject:   RE: [R] substitute question
>
>
>This is because of the saved attribute "source" on z (that doesn't get
>printed out before evaluating z, because z is not yet then a function).
>
>To complete your example:
>
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
>function() {
>b + 1
>}
> > eval(z)
>function(){a+1}
> > ze <- eval(z)
> > attributes(ze)
>$source
>[1] "function(){a+1}"
>
> > attr(ze, "source") <- NULL
> > ze
>function ()
>{
>b + 1
>}
> >
>
>I previously wrote on this topic:
>
>Date: Fri, 24 Oct 2003 09:42:55 -0600
>To: R-help at stat.math.ethz.ch, Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>From: Tony Plate <tplate at blackmesacapital.com>
>Subject: Re: [R] what's going on here with substitute() ?
>Mime-Version: 1.0
>Content-Type: text/plain; charset="us-ascii"; format=flowed
>
>Peter, thank you for the explanation. This is indeed what is happening.
>
>Might I suggest the following passage for inclusion in the help page for
>"function", and possibly also "body", in the DETAILS or WARNING section:
>
>"Note that the text of the original function definition is saved as an
>attribute "source" on the function, and this is printed out when the
>function is printed. Hence, if the function body is changed in some way
>other than by assigning a value via body() (which removes the "source"
>attribute), the printed form of the function may not be the same as the
>actual function body."
>
>Something along these lines could also go in the help for "eval", though if
>it were only there it might be very difficult to find if one were trying to
>look up puzzling behavior of a function.
>
>Here is a transcript that shows what is happening, with another suggestion
>following it.
>
> > eval(substitute(this.is.R <- function() X,
>list(X=!is.null(options("CRAN")[[1]]))))
> > this.is.R
>function() X
> > body(this.is.R)
>[1] TRUE
> > attributes(this.is.R)
>$source
>[1] "function() X"
> > attributes(this.is.R) <- NULL
> > this.is.R
>function ()
>TRUE
> > # the "source" attribute comes from function definition:
> > attributes(function() X)
>$source
>[1] "function() X"
> > # and seems to be added by "eval":
> > attr(eval(parse(text="function() TRUE")[[1]]), "source")
>[1] "function() TRUE"
> >
>
> > # we can assign bogus "source"
> > attr(this.is.R, "source") <- "a totally bogus function body"
> > this.is.R
>a totally bogus function body
> > # assigning to body() removes "source"
> > body(this.is.R) <- list(666)
> > this.is.R
>function ()
>666
> > attr(this.is.R, "source")
>NULL
> >
>
>An even better approach might be something that gave a warning on printing
>if the parsed "source" attribute was not identical to the language object
>being printed. This would probably belong in the code for "case LANGSXP:"
>in the function PrintValueRec in main/print.c (if it were written in R, I
>could contribute a patch, but right now I don't have time to try to
>understand the C there.) R code to do the test could be something like this:
>
> > f <- this.is.R
> > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
>[1] FALSE
> > f <- function() TRUE
> > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
>[1] TRUE
> >
>
>-- Tony Plate
>
>
>At Wednesday 08:09 PM 3/17/2004, Gabor Grothendieck wrote:
>
>
> >I left out the brackets in my last email but the problem
> >(a reappears after have been substituted out) still remains:
> >
> > > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > > z
> >function() {
> > b + 1
> >}
> > > eval(z)
> >function(){a+1}
> >
> >
> >
> >---
> >Date: Wed, 17 Mar 2004 20:10:43 -0500 (EST)
> >From: Gabor Grothendieck <ggrothendieck at myway.com>
> >[ Add to Address Book | Block Address | Report as Spam ]
> >To: <R-help at stat.math.ethz.ch>
> >Subject: [R] substitute question
> >
> >
> >
> >
> >
> >
> >Consider the following example:
> >
> ># substitute a with b in the indicated function. Seems to work.
> > > z <- substitute( function()a+1, list(a=quote(b)) )
> > > z
> >function() b + 1
> >
> ># z is an object of class call so use eval
> ># to turn it into an object of class expression; however,
> ># when z is evaluated, the variable a returns.
> > > eval(z)
> >function()a+1
> >
> >Why did a suddenly reappear again after it had already been replaced?
>
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com



From tlumley at u.washington.edu  Thu Mar 18 20:29:39 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Mar 2004 11:29:39 -0800 (PST)
Subject: [R] substitute question
In-Reply-To: <20040318190003.E8DE939BF@mprdmxin.myway.com>
References: <20040318190003.E8DE939BF@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0403181107560.168194@homer04.u.washington.edu>

On Thu, 18 Mar 2004, Gabor Grothendieck wrote:

>
> Tony, Thomas.  Thanks for your help.  Your comments were very
> useful.
>
> Unfortunately, my next step gives me a new round of problems.
>
> The following is the same as the last example except that instead
> of hard coding the function into the expression I wanted to
> pass it to the expression.  It seems like one has to do a double
> substitute to get this effect but I am having problems getting this
> to work.
>

The problem is that f is a function, not an expression. You need to work
with body(f)

Either
> f<-function(){a+1}>
body(f)<-do.call("substitute",list(body(f),list(a=quote(b))))> f
function ()
{
    b + 1
}

or
> f<-function(){a+1}
> body(f)<-eval(substitute(substitute(expr,list(a=quote(b))),list(expr=body(f))))
> f
function ()
{
    b + 1
}


	-thomas



From lmassis at yahoo.com.br  Thu Mar 18 20:31:55 2004
From: lmassis at yahoo.com.br (Leonard Assis)
Date: Thu, 18 Mar 2004 16:31:55 -0300
Subject: [R] substitute question
In-Reply-To: <20040318190003.E8DE939BF@mprdmxin.myway.com>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAuza3AGpmKkqXuv4sp2m60sKAAAAQAAAApXvZKiCsikyEvV9YE+KwGAEAAAAA@yahoo.com.br>

 Anyone Knows How Shoud I Build Rasch Models (Partial Credit Models)
In R?


[]s
Leonard Assis
Estat?stico - CONFE 7439



From vograno at evafunds.com  Thu Mar 18 20:58:55 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 18 Mar 2004 11:58:55 -0800
Subject: [R] why-s of method dispatching
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3AB6@phost015.intermedia.net>

I see. Thank you very much!

This brings another question. Does R-Core have any plan to promote
data.frame to an S4 class? In general, is there any "road-map" (formal
or informal) to phasing out S3 classes?

Thanks,
Vadim

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Thursday, March 18, 2004 12:24 AM
> To: Vadim Ogranovich
> Cc: R Help List
> Subject: Re: [R] why-s of method dispatching
> 
> 
> On Wed, 17 Mar 2004, Vadim Ogranovich wrote:
> 
> > I am having a problem to understand why as.data.frame
> method doesn't
> > dispatch properly on my class:
> > 
> > > setClass("Foo", "character")
> > [1] "Foo"
> > > as.data.frame(list(foo=new("Foo", .Data="a")))
> > Error in as.data.frame.default(x[[i]], optional = TRUE) :  can't 
> > coerce Foo into a data.frame
> > 
> > I was expecting that this would call as.data.frame.character.
> 
> You have set an S4 class and as.data.frame is an S3 generic.
> 
> > list(foo=new("Foo", .Data="a"))
> $foo
> An object of class "Foo"
> [1] "a"
> 
> and what as.data.frame sees is
> 
> > attributes(list(foo=new("Foo", .Data="a"))$foo)
> $class
> [1] "Foo"
> attr(,"package")
> [1] ".GlobalEnv"
> 
> so thinks this is an S3 class it knows nothing about.
> 
> > Another puzzle. If I explicitly call as.data.frame.character() it
> > would fail but for a different reason:
> > 
> > > as.data.frame.character(list(foo=new("Foo", .Data="a")))
> > Error in unique.default(x) : unique() applies only to vectors
> > 
> > I was under an impression that an instance of "Foo" would
> be welcome
> > anywhere a "character" was, but it seems to be more subtle.
> What am I
> > missing?
> 
> The difference between S3 and S4 classes.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From Ramasubbu.Venkatesh at celeradiagnostics.com  Thu Mar 18 21:04:04 2004
From: Ramasubbu.Venkatesh at celeradiagnostics.com (Ramasubbu Venkatesh)
Date: Thu, 18 Mar 2004 12:04:04 -0800
Subject: [R] Problem getting SJava to work on windows
Message-ID: <OF25750EDA.A8C59EFD-ON88256E5B.006E2920-88256E5B.006E3C31@applera.com>





Hi,

I installed SJava on Windows. But I am not able to invoke SJava as when I
try to load the library(SJava) it complains saying JAVA_HOME is not set?
What is the best way to resolve this problem?

Thanks,

Venky



From vograno at evafunds.com  Thu Mar 18 21:10:09 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 18 Mar 2004 12:10:09 -0800
Subject: [R] why-s of method dispatching
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3AB7@phost015.intermedia.net>

I see. Thank you very much! 

Does R-Core have any plan to promote data.frame to an S4 class? In
general, is there any "road-map" (formal or informal) for phasing out S3
classes?

Thanks,
Vadim

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Thursday, March 18, 2004 12:24 AM
> To: Vadim Ogranovich
> Cc: R Help List
> Subject: Re: [R] why-s of method dispatching
> 
> 
> On Wed, 17 Mar 2004, Vadim Ogranovich wrote:
> 
> > I am having a problem to understand why as.data.frame 
> method doesn't 
> > dispatch properly on my class:
> > 
> > > setClass("Foo", "character")
> > [1] "Foo"
> > > as.data.frame(list(foo=new("Foo", .Data="a")))
> > Error in as.data.frame.default(x[[i]], optional = TRUE) :
> >  can't coerce Foo into a data.frame
> > 
> > I was expecting that this would call as.data.frame.character.
> 
> You have set an S4 class and as.data.frame is an S3 generic.
> 
> > list(foo=new("Foo", .Data="a"))
> $foo
> An object of class "Foo"
> [1] "a"
> 
> and what as.data.frame sees is
> 
> > attributes(list(foo=new("Foo", .Data="a"))$foo)
> $class
> [1] "Foo"
> attr(,"package")
> [1] ".GlobalEnv"
> 
> so thinks this is an S3 class it knows nothing about.
> 
> > Another puzzle. If I explicitly call as.data.frame.character() it 
> > would fail but for a different reason:
> > 
> > > as.data.frame.character(list(foo=new("Foo", .Data="a")))
> > Error in unique.default(x) : unique() applies only to vectors
> > 
> > I was under an impression that an instance of "Foo" would 
> be welcome 
> > anywhere a "character" was, but it seems to be more subtle. 
> What am I 
> > missing?
> 
> The difference between S3 and S4 classes.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From ggrothendieck at myway.com  Thu Mar 18 21:26:29 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 15:26:29 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040318202629.CA7A43971@mprdmxin.myway.com>



Tony, Thomas,  Thanks, again.

If the problem with my last example was just that I was passing a function
rather than an unevaluated expression, then why don't the following 
return f with b in place of a?  In both cases, a is still there in the
final output.

f <- function() { a + 1 }
z <- substitute(substitute(f=f,list(a=quote(b))),list(f=parse(text=deparse(f))))
eval(eval(z))

or

f <- function() { a + 1 }
z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
eval(eval(eval(z)))

---

Date:   Thu, 18 Mar 2004 11:29:39 -0800 (PST) 
From:   Thomas Lumley <tlumley at u.washington.edu>
To:   Gabor Grothendieck <ggrothendieck at myway.com> 
Cc:   <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch> 
Subject:   RE: [R] substitute question 

 
On Thu, 18 Mar 2004, Gabor Grothendieck wrote:

>
> Tony, Thomas. Thanks for your help. Your comments were very
> useful.
>
> Unfortunately, my next step gives me a new round of problems.
>
> The following is the same as the last example except that instead
> of hard coding the function into the expression I wanted to
> pass it to the expression. It seems like one has to do a double
> substitute to get this effect but I am having problems getting this
> to work.
>

The problem is that f is a function, not an expression. You need to work
with body(f)

Either
> f<-function(){a+1}>
body(f)<-do.call("substitute",list(body(f),list(a=quote(b))))> f
function ()
{
b + 1
}

or
> f<-function(){a+1}
> body(f)<-eval(substitute(substitute(expr,list(a=quote(b))),list(expr=body(f))))
> f
function ()
{
b + 1
}


     -thomas

---

Date:   Thu, 18 Mar 2004 12:15:45 -0700 
From:   Tony Plate <tplate at blackmesacapital.com>
To:   <ggrothendieck at myway.com> 
Cc:   <R-help at stat.math.ethz.ch> 
Subject:   RE: [R] substitute question 

 
Gabor, I suspect you are overlooking the fact that substitute() does not 
evaluate its first argument. So, in the previous example you were giving 
an unevaluated expression as the first argument of substitute(). In your 
most recent example you are supplying a function object as the first 
argument of substitute() (by supplying it as the value to be substituted 
for f). I think you can get what you want by first doing f <- 
Quote(function(){a+1}):

> f <- Quote(function(){a+1})
> f
function() {
a + 1
}
> z <- substitute(substitute(f,list(a=quote(b))),list(f=f))
> z
substitute(function() {
a + 1
}, list(a = quote(b)))
> eval(z)
function() {
b + 1
}
> eval(eval(z)) # this displays the "source" attribute
function(){a+1}
> body(eval(eval(z)))
{
b + 1
}
>

hope this helps,

Tony Plate

At Thursday 12:00 PM 3/18/2004, you wrote:

>Tony, Thomas. Thanks for your help. Your comments were very
>useful.
>
>Unfortunately, my next step gives me a new round of problems.
>
>The following is the same as the last example except that instead
>of hard coding the function into the expression I wanted to
>pass it to the expression. It seems like one has to do a double
>substitute to get this effect but I am having problems getting this
>to work.
>
>In the code below we first show that the keep.source option has been
>set to FALSE so that the source attribute does not mislead us.
>
>Then we do a double substitute. The outer substitute just creates
>the substitute that was in my previous question. This outer
>substitute produces z, a call object. So far its as expected.
>
>We then do ze <- eval(z) but we get a function object right away. I was
>expecting that we get an expression object. Even worse, the function does not
>have a replaced with b -- even though this did work in the previous 
>example
>that I posted.
>
>What's wrong?
>
>
> > options()$keep.source
>[1] FALSE
> > f <- function(){a+1}
> > z <- substitute(substitute(f,list(a=quote(b))),list(f=f))
> > class(z)
>[1] "call"
> > as.list(z)
>[[1]]
>substitute
>
>[[2]]
>function ()
>{
> a + 1
>}
>
>[[3]]
>list(a = quote(b))
>
> > ze <- eval(z)
> > class(ze)
>[1] "function"
> > ze
>function ()
>{
> a + 1
>}
> > attr(ze,"source")
>NULL
> >
>
>---
>
>Date: Thu, 18 Mar 2004 09:00:02 -0800 (PST)
>From: Thomas Lumley <tlumley at u.washington.edu>
>To: Gabor Grothendieck <ggrothendieck at myway.com>
>Cc: <R-help at stat.math.ethz.ch>
>Subject: RE: [R] substitute question
>
>
>On Wed, 17 Mar 2004, Gabor Grothendieck wrote:
>
> >
> >
> > I left out the brackets in my last email but the problem
> > (a reappears after have been substituted out) still remains:
> >
> > > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > > z
> > function() {
> > b + 1
> > }
> > > eval(z)
> > function(){a+1}
>
>
>Interesting.
>
>Appearances are misleading, however:
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
>function() {
>b + 1
>}
> > f<-eval(z)
> > f()
>Error in f() : Object "b" not found
> > f
>function(){a+1}
> > attr(f,"source")<-NULL
> > f
>function ()
>{
>b + 1
>}
>
>So it isn't that eval(z) has a+1 inside, it just has a "source" attribute
>with a+1.
>
>Looking more carefully at z
> > as.list(z)
>[[1]]
>`function`
>
>[[2]]
>NULL
>
>[[3]]
>{
>b + 1
>}
>
>[[4]]
>[1] "function(){a+1}"
>
>so the original construction of z has kept the source (not, however, as a
>"source" attribute).
>
>There is method to our madness here. It is impossible (or at least too
>complicated) to keep comments in the right place as a function is
>parsed and deparsed. In the old days, comments would occasionally move
>around, sometimes in very misleading ways (IIRC with if(){}else{} cases)
>
>Now we keep a copy of the source code with functions created interactively
>or with source(), and drop the comments on parsing. This is controlled by
>options("keep.source").
>
>If you do a lot of substitute()-style programming you may want
>options(keep.source=FALSE).
>
> -thomas
>
>PS: There are, of course, interesting possibilities for creative abuse of
>the source attribute....
>
>---
>
>Date: Thu, 18 Mar 2004 08:41:54 -0700
>From: Tony Plate <tplate at blackmesacapital.com>
>To: <ggrothendieck at myway.com>, <R-help at stat.math.ethz.ch>
>Subject: RE: [R] substitute question
>
>
>This is because of the saved attribute "source" on z (that doesn't get
>printed out before evaluating z, because z is not yet then a function).
>
>To complete your example:
>
> > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > z
>function() {
>b + 1
>}
> > eval(z)
>function(){a+1}
> > ze <- eval(z)
> > attributes(ze)
>$source
>[1] "function(){a+1}"
>
> > attr(ze, "source") <- NULL
> > ze
>function ()
>{
>b + 1
>}
> >
>
>I previously wrote on this topic:
>
>Date: Fri, 24 Oct 2003 09:42:55 -0600
>To: R-help at stat.math.ethz.ch, Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>From: Tony Plate <tplate at blackmesacapital.com>
>Subject: Re: [R] what's going on here with substitute() ?
>Mime-Version: 1.0
>Content-Type: text/plain; charset="us-ascii"; format=flowed
>
>Peter, thank you for the explanation. This is indeed what is happening.
>
>Might I suggest the following passage for inclusion in the help page for
>"function", and possibly also "body", in the DETAILS or WARNING section:
>
>"Note that the text of the original function definition is saved as an
>attribute "source" on the function, and this is printed out when the
>function is printed. Hence, if the function body is changed in some way
>other than by assigning a value via body() (which removes the "source"
>attribute), the printed form of the function may not be the same as the
>actual function body."
>
>Something along these lines could also go in the help for "eval", though if
>it were only there it might be very difficult to find if one were trying to
>look up puzzling behavior of a function.
>
>Here is a transcript that shows what is happening, with another suggestion
>following it.
>
> > eval(substitute(this.is.R <- function() X,
>list(X=!is.null(options("CRAN")[[1]]))))
> > this.is.R
>function() X
> > body(this.is.R)
>[1] TRUE
> > attributes(this.is.R)
>$source
>[1] "function() X"
> > attributes(this.is.R) <- NULL
> > this.is.R
>function ()
>TRUE
> > # the "source" attribute comes from function definition:
> > attributes(function() X)
>$source
>[1] "function() X"
> > # and seems to be added by "eval":
> > attr(eval(parse(text="function() TRUE")[[1]]), "source")
>[1] "function() TRUE"
> >
>
> > # we can assign bogus "source"
> > attr(this.is.R, "source") <- "a totally bogus function body"
> > this.is.R
>a totally bogus function body
> > # assigning to body() removes "source"
> > body(this.is.R) <- list(666)
> > this.is.R
>function ()
>666
> > attr(this.is.R, "source")
>NULL
> >
>
>An even better approach might be something that gave a warning on printing
>if the parsed "source" attribute was not identical to the language object
>being printed. This would probably belong in the code for "case LANGSXP:"
>in the function PrintValueRec in main/print.c (if it were written in R, I
>could contribute a patch, but right now I don't have time to try to
>understand the C there.) R code to do the test could be something like this:
>
> > f <- this.is.R
> > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
>[1] FALSE
> > f <- function() TRUE
> > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
>[1] TRUE
> >
>
>-- Tony Plate
>
>
>At Wednesday 08:09 PM 3/17/2004, Gabor Grothendieck wrote:
>
>
> >I left out the brackets in my last email but the problem
> >(a reappears after have been substituted out) still remains:
> >
> > > z <- substitute( function(){a+1}, list(a=quote(b)) )
> > > z
> >function() {
> > b + 1
> >}
> > > eval(z)
> >function(){a+1}
> >
> >
> >
> >---
> >Date: Wed, 17 Mar 2004 20:10:43 -0500 (EST)
> >From: Gabor Grothendieck <ggrothendieck at myway.com>
> >[ Add to Address Book | Block Address | Report as Spam ]
> >To: <R-help at stat.math.ethz.ch>
> >Subject: [R] substitute question
> >
> >
> >
> >
> >
> >
> >Consider the following example:
> >
> ># substitute a with b in the indicated function. Seems to work.
> > > z <- substitute( function()a+1, list(a=quote(b)) )
> > > z
> >function() b + 1
> >
> ># z is an object of class call so use eval
> ># to turn it into an object of class expression; however,
> ># when z is evaluated, the variable a returns.
> > > eval(z)
> >function()a+1
> >
> >Why did a suddenly reappear again after it had already been replaced?
>



From prefectf at yahoo.com  Thu Mar 18 21:28:52 2004
From: prefectf at yahoo.com (Ford Prefect)
Date: Thu, 18 Mar 2004 12:28:52 -0800 (PST)
Subject: [R] printf and sprintf: Where are they??
Message-ID: <20040318202852.53461.qmail@web13908.mail.yahoo.com>

Hi,

Others on this list have happily commented that there
exists printf and/or sprintf in R.  I can't find it.
help(sprintf) and help.search("sprintf") come up
empty!

Is it in some non-standard package?

Thanks!



From sundar.dorai-raj at pdf.com  Thu Mar 18 21:37:53 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 18 Mar 2004 14:37:53 -0600
Subject: [R] printf and sprintf: Where are they??
In-Reply-To: <20040318202852.53461.qmail@web13908.mail.yahoo.com>
References: <20040318202852.53461.qmail@web13908.mail.yahoo.com>
Message-ID: <405A08A1.1070901@pdf.com>



Ford Prefect wrote:
> Hi,
> 
> Others on this list have happily commented that there
> exists printf and/or sprintf in R.  I can't find it.
> help(sprintf) and help.search("sprintf") come up
> empty!
> 
> Is it in some non-standard package?
> 
> Thanks!
> 

On what platform/R-version?

on R-1.8.1 for Windows I get:

help.search("sprintf")

sprintf(base)           Use C-style String Formatting Commands
capture(gregmisc)       Capture printed output of an R expression in a
                         string

So "sprintf" is in the base package.

-sundar



From rolf at math.unb.ca  Thu Mar 18 21:42:32 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 18 Mar 2004 16:42:32 -0400 (AST)
Subject: [R] why-s of method dispatching
Message-ID: <200403182042.i2IKgWS7017120@erdos.math.unb.ca>

Vadim Ogranovich wrote:

> Does R-Core have any plan to promote data.frame to an S4 class? In
> general, is there any "road-map" (formal or informal) for phasing out
> S3 classes?

FOR GOD'S SAKE DON'T!!!  S4 classes and methods are incomprehensible
to the human mind, which is what I'm equipped with.  If S3 is phased
out, R will become effectively unusable as a programming language.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From joseclaudio.faria at terra.com.br  Thu Mar 18 22:00:35 2004
From: joseclaudio.faria at terra.com.br (joseclaudio.faria)
Date: Thu, 18 Mar 2004 18:00:35 -0300
Subject: [R] RE: Frequency table (JCFaria)
Message-ID: <004d01c40d2c$11f22320$01fea8c0@sapetinga>

Hi,

Same time ago I made this generic function for frequency table.
I think that it can help you.


data <- c(65, 70, 85, 65, 65, 65, 62, 55, 82, 59,
   55, 66, 74, 55, 65, 56, 80, 73, 45, 64,
   75, 58, 60, 56, 60, 65, 53, 63, 72, 80,
   90, 95, 55, 70, 79, 62, 57, 65, 60, 47,
   61, 53, 80, 75, 72, 87, 52, 72, 80, 85,
   75, 70, 84, 60, 72, 70, 76, 70, 79, 72,
   69, 80, 62, 74, 54, 58, 58, 69, 81, 84)

#------------ begin options of table---------------
min <-  40
max <- 100
h   <-  10
#-------------- end options of table---------------

#-------- begin declaration of variables-----------
Fi   <- numeric()
FacA <- numeric();  FacP <- numeric()
FrA  <- numeric();  FrP  <- numeric()
#-------- end declaration of variables-------------

#----------------- begin function------------------
Createtable <- function()
{
  Fi <<- table(cut(data, br = seq(min, max, h), right = FALSE))
  K <- length(names(Fi))

  for(i in 1:K)
  {
    FrA[i] = Fi[i] / 70
  }

  for(i in 1:K)
  {
    FrP[i] = (Fi[i] / 70) * 100
  }

  for(i in 1:K)
  {
    FacA[i] = sum(Fi[1:i])
  }

  for(i in 1:K)
  {
    FacP[i] = (sum(Fi[1:i]) / 70) * 100
  }

  table <- data.frame(Fi, FrA, FrP, FacA, FacP)
}
#----------------- end function------------------

tab <- Createtable()
print("Complete table:")
print(tab)


Jos? Cl?udio Faria
UESC/DCET
Brasil
73-634.2779
joseclaudio.faria at terra.com.br
jc_faria at uol.com.br



From p.dalgaard at biostat.ku.dk  Thu Mar 18 22:33:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Mar 2004 22:33:53 +0100
Subject: [R] substitute question
In-Reply-To: <20040318202629.CA7A43971@mprdmxin.myway.com>
References: <20040318202629.CA7A43971@mprdmxin.myway.com>
Message-ID: <x2lllxzvpa.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> Tony, Thomas,  Thanks, again.
> 
> If the problem with my last example was just that I was passing a function
> rather than an unevaluated expression, then why don't the following 
> return f with b in place of a?  In both cases, a is still there in the
> final output.
> 
> f <- function() { a + 1 }
> z <- substitute(substitute(f=f,list(a=quote(b))),list(f=parse(text=deparse(f))))
> eval(eval(z))
> 
> or
> 
> f <- function() { a + 1 }
> z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
> eval(eval(eval(z)))

As far as I can see you don't quote *the expression that creates f* in
either of the above

f <- quote(function() { a + 1 })
attr(f,"source") <- NULL
g <- eval(substitute(substitute(f, list(a=quote(b))),list(f=f)))
g
g() # oops, g is not a function but a call to "function"
g <- eval(g)
b <- 500
g()

If you have only the function to begin with, try something along these
lines 

f <- function() { a + 1 }
g <- f
attr(g,"source") <- NULL
body(g) <- eval(substitute(substitute(f, list(a=quote(b))),list(f=body(f))))
g
g()


(The real pain in these examples is that substitute autoquotes its
expr argument. Therefore, when you want to modify an expression that
is already stored in a variable, you need an extra outer layer of
eval(substitute(...)) to poke the content of the variable into the
inner substitute. An "esub" function  with standard evaluation
semantics would make this much easier.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Mar 18 22:44:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Mar 2004 21:44:38 +0000 (GMT)
Subject: [R] why-s of method dispatching
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3AB7@phost015.intermedia.net>
Message-ID: <Pine.LNX.4.44.0403182129001.12899-100000@gannet.stats>

On Thu, 18 Mar 2004, Vadim Ogranovich wrote:

> I see. Thank you very much! 
> 
> Does R-Core have any plan to promote data.frame to an S4 class? In
> general, is there any "road-map" (formal or informal) for phasing out S3
> classes?

No, no.  There is a vast amount of existing code depending on S3 classes.
As you will see from Rolf Turner's reaction, many users wish to keep it 
that way.

BTW, not all of us would see it as promotion, as S3 and S4 classes have
different strengths.  It is unfortunate that the particular way S4 classes 
are implemented at present that they get confused with S3 classes.


> 
> Thanks,
> Vadim
> 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: Thursday, March 18, 2004 12:24 AM
> > To: Vadim Ogranovich
> > Cc: R Help List
> > Subject: Re: [R] why-s of method dispatching
> > 
> > 
> > On Wed, 17 Mar 2004, Vadim Ogranovich wrote:
> > 
> > > I am having a problem to understand why as.data.frame 
> > method doesn't 
> > > dispatch properly on my class:
> > > 
> > > > setClass("Foo", "character")
> > > [1] "Foo"
> > > > as.data.frame(list(foo=new("Foo", .Data="a")))
> > > Error in as.data.frame.default(x[[i]], optional = TRUE) :
> > >  can't coerce Foo into a data.frame
> > > 
> > > I was expecting that this would call as.data.frame.character.
> > 
> > You have set an S4 class and as.data.frame is an S3 generic.
> > 
> > > list(foo=new("Foo", .Data="a"))
> > $foo
> > An object of class "Foo"
> > [1] "a"
> > 
> > and what as.data.frame sees is
> > 
> > > attributes(list(foo=new("Foo", .Data="a"))$foo)
> > $class
> > [1] "Foo"
> > attr(,"package")
> > [1] ".GlobalEnv"
> > 
> > so thinks this is an S3 class it knows nothing about.
> > 
> > > Another puzzle. If I explicitly call as.data.frame.character() it 
> > > would fail but for a different reason:
> > > 
> > > > as.data.frame.character(list(foo=new("Foo", .Data="a")))
> > > Error in unique.default(x) : unique() applies only to vectors
> > > 
> > > I was under an impression that an instance of "Foo" would 
> > be welcome 
> > > anywhere a "character" was, but it seems to be more subtle. 
> > What am I 
> > > missing?
> > 
> > The difference between S3 and S4 classes.
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From scott.rifkin at yale.edu  Thu Mar 18 23:17:47 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Thu, 18 Mar 2004 17:17:47 -0500 (EST)
Subject: [R] two lme questions
Message-ID: <Pine.LNX.4.44.0403181654380.28835-100000@argos.its.yale.edu>

This is an update to my previous post today after finding some previous 
posts about crossed random effects.  Any comments would be much 
appreciated.


I have two random factors (Plot and Variety), one fixed factor (Time).  I 
got rid of one of the fixed factors from my previous post for simplicity.  
Each Variety is tested at each of two Times, so I would like to include 
Time %in% Variety in my random factors.

I tried the following based on a suggestion from Douglas Bates 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26424.html

> Data = groupedData(Measurement ~Time|Variety, data = 
read.table("mvone.txt",header = TRUE))
> Data$vt = factor(paste(Data$Variety,Data$Time,sep="/"))
> Data$const=factor(rep(1,nrow(Data)))
> Data.lme=lme(Measurement~Time,data=Data,random=list(const=pdBlocked(list(pdIdent(~Plot-1),pdIdent(~Variety-1),pdIdent(~vt-1)))))

I'd like some help interpreting the results.(the ... mean that I deleted 
repetitive stuff)

>summary(Data.lme)
...
 Block 1: Plot18Ea18La, Plot18Ea49Ea, Plot18Ea59Ea, Plot18Ea67Ea, 
Plot18La18Ea, Plot18La21La, Plot18La46La, Plot18La71La, Plot21Ea21La, 
...
Plot71La67La, Plot71La71Ea
 Formula: ~Plot - 1 | const
 Structure: Multiple of an Identity
        Plot18Ea18La Plot18Ea49Ea Plot18Ea59Ea Plot18Ea67Ea Plot18La18Ea
StdDev:     0.973655     0.973655     0.973655     0.973655     0.973655
        Plot18La21La Plot18La46La Plot18La71La Plot21Ea21La Plot21Ea59Ea
StdDev:     0.973655     0.973655     0.973655     0.973655     0.973655
...

I take it that these StdDevs are the sqrt of the variance component for 
Plot.  They are the diagonal of the pdBlocked matrix corresponding to 
pdIdent(~Plot-1)

same thing for 

 Block 2: VarietyL23, VarietyL54, VarietyL59, VarietyL71, VarietyL49, 
...
 Formula: ~Variety - 1 | const
 Structure: Multiple of an Identity
        VarietyL23 VarietyL54 VarietyL59 VarietyL71 VarietyL49 VarietyL21
StdDev: 0.04296328 0.04296328 0.04296328 0.04296328 0.04296328 0.04296328
...
and

 Block 3: vtL18/Early, vtL18/Late, vtL21/Early, vtL21/Late, vtL23/Early, 
...
vtL71/Late
 Formula: ~vt - 1 | const
 Structure: Multiple of an Identity
        vtL18/Early  vtL18/Late vtL21/Early  vtL21/Late vtL23/Early  vtL23/Late
StdDev: 0.006307627 0.006307627 0.006307627 0.006307627 0.006307627 0.006307627
...

and 

         Residual
StdDev: 0.1299986

is just the sigma(e)

Fixed effects: Measurement ~ Time 
                Value  Std.Error  DF  t-value p-value
(Intercept)  9.762629 0.10228970 190 95.44097  <.0001
TimeLate    -0.222656 0.03712913 190 -5.99680  <.0001

I'm still wondering why I only get TimeLate...is it because TimeEarly is 
just 0.222656?

when I do:

> intervals(Data.lme)
Approximate 95% confidence intervals

 Fixed effects:
                 lower       est.      upper
(Intercept)  9.5608597  9.7626290  9.9643984
TimeLate    -0.2958942 -0.2226559 -0.1494177
attr(,"label")
[1] "Fixed effects:"

 Random Effects:
  Level: const 
                       lower        est.     upper
sd(Plot - 1)    8.436375e-01 0.973655068 1.1237103
sd(Variety - 1) 1.577451e-02 0.042963279 0.1170143
sd(vt - 1)      4.186565e-06 0.006307627 9.5032935

 Within-group standard error:
    lower      est.     upper 
0.1119908 0.1299986 0.1509019 


These are my confidence intervals on the variance components, I assume.  
I'm slightly confused on this because in one of the posts I found:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/21857.html

it seems to be much more complicated to find the variance componenets.

Thanks much,
Scott Rifkin
scott.rifkin at yale.edu



From ggrothendieck at myway.com  Thu Mar 18 23:31:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 17:31:40 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040318223140.2E3543962@mprdmxin.myway.com>



From:   Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> (The real pain in these examples is that substitute autoquotes its
> expr argument. Therefore, when you want to modify an expression that
> is already stored in a variable, you need an extra outer layer of
> eval(substitute(...)) to poke the content of the variable into the
> inner substitute. An "esub" function with standard evaluation
> semantics would make this much easier.)

That is one of the frustrations of using substitute.  

The other is that even if you do perform two levels of substitute,
as I have been trying, you still can't count on it working for
an arbitrary unevaluated expression, as my examples show.  

Even putting aside the source attribute which is super confusing
until you know about it, all the solutions that I can see to 
the problem I presented are ugly.

(1) One can either pick apart the function using body, or 

(2) I  assume one could convert the function to text and 
paste together the correct substitute command with the text of 
the function inserted in its argument.   

The quote mechanism works but is not applicable if all you have
is the function itself (as you point out).

This is sooo frustrating.

Thanks to you, Tony and Thomas for all your help.



From adrian_d at eskimo.com  Thu Mar 18 23:49:37 2004
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Thu, 18 Mar 2004 14:49:37 -0800 (PST)
Subject: [R] profile error on an nls object
Message-ID: <Pine.SUN.4.58.0403181444040.20898@eskimo.com>

Hello all,

This is the error message that I get.
>   hyp.res <- nls(log(y)~log(pdf.hyperb(theta,X)), data=dataModel,
+              start=list(theta=thetaE0),
+              trace=TRUE)
45.54325 :   0.1000000  1.3862944 -4.5577142  0.0005503
3.728302 :   0.0583857346  0.4757772859 -4.9156128701  0.0005563154
1.584317 :   0.0194149477  0.3444648833 -4.9365149150  0.0004105426
1.569333 :   0.0139310639  0.3824648048 -4.9024001228  0.0004089738
1.569311 :   0.0137155342  0.3888648619 -4.8979817546  0.0004137501
1.569311 :   0.0136895846  0.3893564152 -4.8976182201  0.0004141057
1.569311 :   0.0136876315  0.3894059947 -4.8975821760  0.0004141343
>   hyp.res.S <- summary(hyp.res)
>   hyp.res.S

Formula: log(y) ~ log(pdf.hyperb(theta, X))

Parameters:
         Estimate Std. Error t value Pr(>|t|)
theta1  0.0136876  0.0359964   0.380    0.705
theta2  0.3894060  0.3079860   1.264    0.211
theta3 -4.8975822  0.2219928 -22.062   <2e-16 ***
theta4  0.0004141  0.0005457   0.759    0.451
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.1542 on 66 degrees of freedom

Correlation of Parameter Estimates:
         theta1    theta2    theta3
theta2 -0.02168
theta3 -0.02029  0.997736
theta4 -0.97182 -0.008054 -0.008952

>   pr1 <- profile(hyp.res)
1.825584 :   0.3894059947 -4.8975821760  0.0004141343
1.58426 :   0.373691474 -4.909091289  0.000824045
1.583673 :   0.4176596873 -4.8774106487  0.0008176545
1.583670 :   0.4196944963 -4.8760375504  0.0008187918
1.583670 :   0.4199010211 -4.8758854269  0.0008188162
1.624899 :   0.449756713 -4.854643555  0.001215014
1.624743 :   0.46804752 -4.84185838  0.00122343
1.624741 :   0.470384534 -4.840195293  0.001224199
1.624741 :   0.470638282 -4.840013199  0.001224298
1.624741 :   0.470670500 -4.839990112  0.001224309
1.692158 :   0.522188258 -4.803565745  0.001635778
1.691853 :   0.540794581 -4.791027785  0.001650730
1.691847 :   0.544973564 -4.788090229  0.001652321
1.691847 :   0.545500818 -4.787718964  0.001652616
1.691847 :   0.545592388 -4.787654441  0.001652658
1.784749 :   0.622277872 -4.734086833  0.002091090
1.784039 :   0.642139831 -4.721442413  0.002115555
1.784022 :   0.649929188 -4.716068239  0.002119126
1.784021 :   0.651094995 -4.715267692  0.002119963
1.784021 :   0.65136956 -4.71507850  0.00212012
1.784021 :   0.651420684 -4.715043256  0.002120153
1.901667 :   0.760981513 -4.639871097  0.002604136
1.899765 :   0.782870773 -4.627113544  0.002644289
1.899703 :   0.798044378 -4.616913842  0.002653139
1.899699 :   0.800930030 -4.614996517  0.002655646
1.899699 :   0.801815115 -4.614404602  0.002656286
1.899699 :   0.802033012 -4.614258879  0.002656455
1.899699 :   0.802090888 -4.614220164  0.002656499
2.042311 :   0.960722487 -4.508069592  0.003221186
2.036028 :   0.985442669 -4.495703574  0.003291724
2.035767 :   1.017849320 -4.474692317  0.003317203
2.035736 :   1.026482531 -4.469203277  0.003326355
2.035733 :   1.029937702 -4.466987738  0.003329627
2.035732 :   1.031114541 -4.466233213  0.003330776
2.035732 :   1.03153247 -4.46596514  0.00333118
2.035732 :   1.031679019 -4.465871141  0.003331322
2.035732 :   1.031730150 -4.465838341  0.003331371
1.583425 :   3.595503e-01 -4.918824e+00  1.793668e-05
1.583270 :   3.783622e-01 -4.905413e+00  2.175254e-05
1.58327 :   3.793739e-01 -4.904683e+00  2.134353e-05
1.58327 :   3.794562e-01 -4.904622e+00  2.133204e-05
1.624847 :   0.3695765499 -4.9116128267 -0.0003687005
1.624748 :   0.3877816645 -4.8985581022 -0.0003699797
1.624747 :   0.3890277914 -4.8976645481 -0.0003706884
1.624747 :   0.3891837724 -4.8975508910 -0.0003707385
1.693266 :   0.3989123147 -4.8904787597 -0.0007628478
1.693170 :   0.4172349434 -4.8774484377 -0.0007692982
1.693168 :   0.4193992605 -4.8759045536 -0.0007705839
1.693168 :   0.4197576966 -4.8756463144 -0.0007707548
1.693168 :   0.4198094076 -4.8756090629 -0.0007707816
1.788041 :   0.450653198 -4.853510937 -0.001173674
1.787884 :   0.469850125 -4.840178495 -0.001186483
1.787878 :   0.473941725 -4.837285088 -0.001188994
1.787878 :   0.474783800 -4.836687703 -0.001189518
1.787878 :   0.474960093 -4.836562526 -0.001189627
1.787878 :   0.474996400 -4.836536741 -0.001189650
1.908362 :   0.531011661 -4.796878005 -0.001614805
1.907987 :   0.552282889 -4.782714884 -0.001637055
1.907965 :   0.560264832 -4.777154923 -0.001642437
1.907964 :   0.562346209 -4.775709348 -0.001644026
1.907963 :   0.56295255 -4.77528733 -0.00164447
1.907963 :   0.563123513 -4.775168321 -0.001644596
1.907963 :   0.563172014 -4.775134556 -0.001644632
2.053048 :   0.653571857 -4.712183501 -0.002111091
2.051874 :   0.678913471 -4.696423681 -0.002150202
2.051785 :   0.69544095 -4.68518302 -0.00216328
2.051772 :   0.701166962 -4.681323343 -0.002168564
2.051770 :   0.703471045 -4.679765756 -0.002170603
2.051769 :   0.704360670 -4.679164327 -0.002171397
2.051769 :   0.704707691 -4.678929683 -0.002171706
2.051769 :   0.704842651 -4.678838422 -0.002171826
2.051769 :   0.704895042 -4.678802995 -0.002171872
3.874239 :   0.0136876315 -4.8975821760  0.0004141343
1.683633 :   0.0140300885 -5.1010837614  0.0004228894
1.582383 :   0.0159635264 -5.0698805881  0.0003921865
1.582380 :   0.0156612906 -5.0699927701  0.0003961255
1.582380 :   0.0156714529 -5.0699887428  0.0003959888
1.62839 :   0.0177072933 -5.2469160871  0.0003773674
1.62302 :   0.0173703508 -5.2549785000  0.0003824169
1.623010 :   0.017454006 -5.254634273  0.000381311
1.623010 :   0.017450834 -5.254646856  0.000381349
1.693919 :   0.0192287068 -5.4391483575  0.0003667216
1.688721 :   0.0188383089 -5.4474923226  0.0003715871
1.688708 :   0.0189024936 -5.4470793675  0.0003708466
1.688708 :   0.0189011857 -5.4470970845  0.0003708537
1.781934 :   0.0203962009 -5.6454738631  0.0003600353
1.776866 :   0.0199673656 -5.6541185083  0.0003646648
1.776849 :   0.0200118977 -5.6536319416  0.0003642738
1.776849 :   0.0200100558 -5.6536559449  0.0003642825
1.889638 :   0.0211923804 -5.8738978163  0.0003572760
1.884404 :   0.0207378220 -5.8830916118  0.0003616144
1.884382 :   0.020765806 -5.882510312  0.000361524
1.884382 :   0.0207622023 -5.8825427275  0.0003615496
1.884382 :   0.0207620572 -5.8825409040  0.0003615529
2.013048 :   0.0215963002 -6.1364575433  0.0003585248
2.007339 :   0.0211300497 -6.1464738264  0.0003624619
2.00731 :   0.0211456623 -6.1457695801  0.0003626177
2.00731 :   0.0211391005 -6.1458133458  0.0003626743
2.00731 :   0.0211385176 -6.1458106028  0.0003626839
1.588746 :   0.0116517911 -4.7206548317  0.0004327556
1.58227 :   0.0113562764 -4.7282291715  0.0004385397
1.582266 :   0.0114927727 -4.7280477868  0.0004363126
1.582266 :   0.0114778523 -4.7280512493  0.0004365554
1.626601 :   0.0092002102 -4.5533139799  0.0004596651
1.619645 :   0.0089799845 -4.5607704466  0.0004657534
1.619642 :   0.0091267199 -4.5606310820  0.0004631665
1.619642 :   0.0091061083 -4.5606330852  0.0004635248
1.686974 :   0.0065887359 -4.3829352072  0.0004921501
1.679178 :   0.006431904 -4.390438771  0.000499185
1.679176 :   0.0065909364 -4.3903319603  0.0004961289
1.679176 :   0.0065644067 -4.3903330820  0.0004966265
1.679176 :   0.0065683565 -4.3903330201  0.0004965523
1.767647 :   0.003788939 -4.203815551  0.000532725
1.758289 :   0.0036705739 -4.2116327081  0.0005421454
1.758287 :   0.0038516622 -4.2115484360  0.0005382965
1.758287 :   0.0038192070 -4.2115490856  0.0005389643
1.865816 :   0.0006957633 -4.0084238869  0.0005871507
1.853701 :   0.0005961484 -4.0168687626  0.0006016352
1.853699 :   0.0008362140 -4.0167968055  0.0005958852
1.853699 :   0.0007620456 -4.0167975560  0.0005975896
1.853699 :   0.0007701809 -4.0167974548  0.0005974007
1.978174 :  -0.0028587603 -3.7850046811  0.0006669515
1.960889 :  -0.0030688672 -3.7945046007  0.0006955498
1.960887 :  -0.0027067062 -3.7944311799  0.0006854164
1.960887 :  -0.0027920434 -3.7944320124  0.0006877001
5.64713 :  0.0136876315 0.3894059947 0.0004141343
1.629748 :  0.0155012039 0.1892390960 0.0003968719
1.581812 :  0.0156784380 0.1604443768 0.0003956682
1.581806 :  0.0155981707 0.1607541098 0.0003966080
1.581806 :  0.0156011476 0.1607440033 0.0003965726
1.625034 :   0.0176093652 -0.0792346636  0.0003781418
1.619729 :   0.0172570301 -0.0690409623  0.0003829002
1.619717 :   0.0172925225 -0.0695061842  0.0003825291
1.619717 :   0.0172912517 -0.0694884984  0.0003825385
1.685930 :   0.0190500552 -0.3090794855  0.0003679339
1.680988 :   0.0186534576 -0.2991696578  0.0003725206
1.680974 :   0.0186835862 -0.2996838489  0.0003722919
1.680974 :   0.0186818890 -0.2996610614  0.0003723024
1.768212 :   0.0201697584 -0.5459270795  0.0003613506
1.763496 :   0.0197433946 -0.5361742949  0.0003657251
1.763479 :   0.0197679071 -0.5367429015  0.0003656357
1.763479 :   0.0197650941 -0.5367140691  0.0003656558
1.869533 :   0.0209584525 -0.7978734727  0.0003583334
1.864765 :   0.0205126475 -0.7879935819  0.0003624617
1.864745 :   0.0205327327 -0.7886349546  0.0003624954
1.864745 :   0.0205286611 -0.7885983504  0.0003625264
1.864745 :   0.0205285176 -0.7886004594  0.0003625299
1.986651 :   0.0213989344 -1.0757885915  0.0003589658
1.981542 :   0.0209450354 -1.0654801197  0.0003627591
1.981515 :   0.0209633543 -1.0662185762  0.0003628857
1.981515 :   0.020956299 -1.066171806  0.000362947
1.981515 :   0.0209561876 -1.0661748041  0.0003629514
1.589875 :  0.0116794138 0.6293846615 0.0004325651
1.583420 :  0.0113555311 0.6404016532 0.0004381207
1.583415 :  0.0114133891 0.6400920083 0.0004372227
1.583415 :  0.0114079388 0.6400983595 0.0004373103
1.631556 :  0.0091559922 0.8877395583 0.0004602042
1.624037 :  0.0088957207 0.8995559273 0.0004663601
1.624033 :  0.0089630852 0.8992803329 0.0004651855
1.624033 :  0.008952827 0.899284613 0.000465364
1.697006 :  0.0064519740 1.1632996957 0.0004939404
1.688459 :  0.0062555272 1.1758143022 0.0005012813
1.688456 :  0.0063357705 1.1755760587 0.0004997134
1.688456 :  0.0063225169 1.1755787128 0.0004999653
1.783962 :  0.0035487681 1.4669400022 0.0005364535
1.773585 :  0.0033867882 1.4806292714 0.0005467553
1.773583 :  0.0034981174 1.4804153720 0.0005443142
1.773583 :  0.0034734893 1.4804170613 0.0005448297
1.889003 :  0.0003437136 1.8152947117 0.0005941152
1.875415 :  0.0002107001 1.8307953466 0.0006103544
1.875413 :  0.0002868541 1.8305923056 0.0006083076
1.875413 :  0.0003182105 1.8305924268 0.0006075744
1.875413 :  0.000300086 1.830592703 0.000608001
Error in prof$getProfile() : step factor 0.000488281 reduced below
`minFactor' of 0.000976563
>

Why is there an error on profile, which should only evaluate the
objective function for different parameter values?

Thanks a lot.

Adrian



From p.dalgaard at biostat.ku.dk  Thu Mar 18 23:52:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Mar 2004 23:52:47 +0100
Subject: [R] substitute question
In-Reply-To: <20040318223140.2E3543962@mprdmxin.myway.com>
References: <20040318223140.2E3543962@mprdmxin.myway.com>
Message-ID: <x2brmtzs1s.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> From:   Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > (The real pain in these examples is that substitute autoquotes its
> > expr argument. Therefore, when you want to modify an expression that
> > is already stored in a variable, you need an extra outer layer of
> > eval(substitute(...)) to poke the content of the variable into the
> > inner substitute. An "esub" function with standard evaluation
> > semantics would make this much easier.)
> 
> That is one of the frustrations of using substitute.  
> 
> The other is that even if you do perform two levels of substitute,
> as I have been trying, you still can't count on it working for
> an arbitrary unevaluated expression, as my examples show.  

Er, I don't think so. All I have seen is a couple of cases where you
tried to pass something that was not a language object (e.g. a
function as opposed to an expression or call generating a function.)

> Even putting aside the source attribute which is super confusing
> until you know about it, all the solutions that I can see to 
> the problem I presented are ugly.
> 
> (1) One can either pick apart the function using body, or 
> 
> (2) I  assume one could convert the function to text and 
> paste together the correct substitute command with the text of 
> the function inserted in its argument.   
> 
> The quote mechanism works but is not applicable if all you have
> is the function itself (as you point out).
> 
> This is sooo frustrating.

Well, the certain road to frustration is to try to do the impossible.
A function is not a language object and you can only do substitutions
on language objects.

> f <- function()x
> g <- quote(function()x)
> f
function()x
> g
function() x
> mode(f)
[1] "function"
> mode(g)
[1] "call"
> is.language(f)
[1] FALSE
> is.language(g)
[1] TRUE

However, a function is basically a triplet consisting of an argument
list, a body, and an environment, the middle of which *is* a language
object. So I don't think your (1) is ugly, it's the logical way to
proceed. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Fri Mar 19 00:06:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 18:06:36 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040318230636.11049399E@mprdmxin.myway.com>


> Date:   18 Mar 2004 23:52:47 +0100 
> From:   Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> To:   <ggrothendieck at myway.com> 
> Cc:   <p.dalgaard at biostat.ku.dk>, <tlumley at u.washington.edu>, <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch> 
> Subject:   Re: [R] substitute question 
> 
>  
> "Gabor Grothendieck" <ggrothendieck at myway.com> writes:
> 
> > From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > > (The real pain in these examples is that substitute autoquotes its
> > > expr argument. Therefore, when you want to modify an expression that
> > > is already stored in a variable, you need an extra outer layer of
> > > eval(substitute(...)) to poke the content of the variable into the
> > > inner substitute. An "esub" function with standard evaluation
> > > semantics would make this much easier.)
> > 
> > That is one of the frustrations of using substitute. 
> > 
> > The other is that even if you do perform two levels of substitute,
> > as I have been trying, you still can't count on it working for
> > an arbitrary unevaluated expression, as my examples show. 
> 
> Er, I don't think so. All I have seen is a couple of cases where you
> tried to pass something that was not a language object (e.g. a
> function as opposed to an expression or call generating a function.)
> 
> 

The parse/deparse one definitely is an expression:

> z <- parse(text=deparse(f))
> class(z);mode(z);typeof(z)
[1] "expression"
[1] "expression"
[1] "expression"

In the other one, it is not an expression in the inner substitute
but should be by the time it gets to the outer one since I
explicitly added expression(...).  It is expanded which shows
it did do that part right.



From tplate at blackmesacapital.com  Fri Mar 19 00:41:31 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 18 Mar 2004 16:41:31 -0700
Subject: [R] substitute question
In-Reply-To: <20040318230636.11049399E@mprdmxin.myway.com>
References: <20040318230636.11049399E@mprdmxin.myway.com>
Message-ID: <6.0.3.0.2.20040318163434.090a70f8@mailhost.blackmesacapital.com>

Gabor, you might have less frustration if you work with Peter's esub() 
suggestion (use it instead of substitute()):

esub <- function(expr, subst.list) do.call("substitute", list(expr, 
subst.list))

When you say

"In the other one, it is not an expression in the inner substitute
but should be by the time it gets to the outer one since I explicitly added 
expression(...).  It is expanded which shows it did do that part right."

something to bear in mind is the "natural order" of evaluation is reversed 
with nested substitute()'s (because the inner substitute() is quoted by the 
outer substitute())  (but I haven't carefully reasoned through this being 
an explanation for your particular frustration here.)

hope this might help,

Tony Plate




At Thursday 04:06 PM 3/18/2004, Gabor Grothendieck wrote:

> > Date:   18 Mar 2004 23:52:47 +0100
> > From:   Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > To:   <ggrothendieck at myway.com>
> > Cc:   <p.dalgaard at biostat.ku.dk>, <tlumley at u.washington.edu>, 
> <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch>
> > Subject:   Re: [R] substitute question
> >
> >
> > "Gabor Grothendieck" <ggrothendieck at myway.com> writes:
> >
> > > From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > > > (The real pain in these examples is that substitute autoquotes its
> > > > expr argument. Therefore, when you want to modify an expression that
> > > > is already stored in a variable, you need an extra outer layer of
> > > > eval(substitute(...)) to poke the content of the variable into the
> > > > inner substitute. An "esub" function with standard evaluation
> > > > semantics would make this much easier.)
> > >
> > > That is one of the frustrations of using substitute.
> > >
> > > The other is that even if you do perform two levels of substitute,
> > > as I have been trying, you still can't count on it working for
> > > an arbitrary unevaluated expression, as my examples show.
> >
> > Er, I don't think so. All I have seen is a couple of cases where you
> > tried to pass something that was not a language object (e.g. a
> > function as opposed to an expression or call generating a function.)
> >
> >
>
>The parse/deparse one definitely is an expression:
>
> > z <- parse(text=deparse(f))
> > class(z);mode(z);typeof(z)
>[1] "expression"
>[1] "expression"
>[1] "expression"
>
>In the other one, it is not an expression in the inner substitute
>but should be by the time it gets to the outer one since I
>explicitly added expression(...).  It is expanded which shows
>it did do that part right.
>
>
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com



From andel at ifi.unizh.ch  Fri Mar 19 00:47:38 2004
From: andel at ifi.unizh.ch (David Andel)
Date: 19 Mar 2004 00:47:38 +0100
Subject: [R] character -> list
Message-ID: <405A351A.5070403@ifi.unizh.ch>

Hi

I have a matrix of type "character" (strangly) and am trying to apply 
the function "aggregate" to it, but unfortunately without success.
It seems to me that aggregate would work, if I only could get rid of the 
quotation marks around each item.

So for a very simple example which looks as my actual data look (of 
course here I put the quotation marks on purpose, but I have no idea 
where they come from in my actual data):

 > x <- matrix(c("a","b","1","a","c","3","b","b","2"), nr=3, byrow=T, 
dimnames=list(1:3, c("type","group","value")))
 > x
   type group value
1 "a"  "b"   "1"
2 "a"  "c"   "3"
3 "b"  "b"   "2"

I can't access the columns the way I think would be necessary for 
aggregate to work:
 > x$type
NULL

And this seems to be the problem:
 > typeof(x)
[1] "character"

I think I would need to have type "list" to be able to apply "aggregate".

How can I accomplish this?

Thanks,
David



From richard_raubertas at merck.com  Fri Mar 19 00:51:09 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Thu, 18 Mar 2004 18:51:09 -0500
Subject: [R] sd(): why  error with NA's?
Message-ID: <38C4C095FC35E5469BED686B42F40A1307045F4F@usrymx17.merck.com>

R 1.8.1 with Windows XP.

I have a question about how sd() behaves with NA's:

> mean(c(1,2,3,NA))
[1] NA
> median(c(1,2,3,NA))
[1] NA
> mad(c(1,2,3,NA))
[1] NA
> sd(c(1,2,3,NA))
Error in var(x, na.rm = na.rm) : missing observations in cov/cor
>

What is so special about the standard deviation, relative to
other descriptive statistics, that the presence of NA's 
deserves an error instead of simply returning NA?
(I know about na.rm=TRUE, but that is not the point here.)

A few small changes to sd() would seem to resolve the anomaly:

sd <- function(x, na.rm=FALSE)
# Function like built-in 'sd', but return NA instead of error when
# 'na.rm' is FALSE and 'x' has NA's.
{
    if (is.matrix(x)) {
        apply(x, 2, sd, na.rm = na.rm)
    } else if (is.vector(x)) {
        if (!na.rm && any(is.na(x)))  NA
        else sqrt(var(x, na.rm = na.rm))
    } else if (is.data.frame(x)) {
        sapply(x, sd, na.rm = na.rm)
    } else {
        x <- as.vector(x)
        if (!na.rm && any(is.na(x)))  NA
        else sqrt(var(x, na.rm = na.rm))
    }
}

Rich Raubertas
Merck & Co.


------------------------------------------------------------------------------Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.



From teresa at cimat.mx  Fri Mar 19 01:01:10 2004
From: teresa at cimat.mx (Teresa Alarcon)
Date: Thu, 18 Mar 2004 18:01:10 -0600
Subject: [R] Reading  image file formats
Message-ID: <5.2.0.9.0.20040318175948.02d44a70@pop.cimat.mx>

Hi!
Is there exists an instruction to read images (bmp, jpg, etc.) in R?



From tlumley at u.washington.edu  Fri Mar 19 00:56:59 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Mar 2004 15:56:59 -0800 (PST)
Subject: [R] substitute question
In-Reply-To: <20040318223140.2E3543962@mprdmxin.myway.com>
References: <20040318223140.2E3543962@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0403181551430.168194@homer04.u.washington.edu>

On Thu, 18 Mar 2004, Gabor Grothendieck wrote:

>
>
> From:   Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > (The real pain in these examples is that substitute autoquotes its
> > expr argument. Therefore, when you want to modify an expression that
> > is already stored in a variable, you need an extra outer layer of
> > eval(substitute(...)) to poke the content of the variable into the
> > inner substitute. An "esub" function with standard evaluation
> > semantics would make this much easier.)
>
> That is one of the frustrations of using substitute.
>
> The other is that even if you do perform two levels of substitute,
> as I have been trying, you still can't count on it working for
> an arbitrary unevaluated expression, as my examples show.

You can, but a function isn't an arbitrary unevaluated expression
 You still aren't passing an expression containg the symbol `a`. You are
passing an expression containing the function f, whose body is an
expression containing the symbol `a`.  Functions aren't expressions.
>
> f <- function() { a + 1 }
> z <- substitute(substitute(f=f,list(a=quote(b))),list(f=parse(text=deparse(f)$
> eval(eval(z))
>
> or
>
> f <- function() { a + 1 }
> z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
> eval(eval(eval(z)))


> (1) One can either pick apart the function using body, or

Actually, I think this is fairly natural -- it is only body(f) that is an
expression.  Certainly substitute() could have been written to operate on
functions as well, but it wasn't.



	-thomas



From andy_liaw at merck.com  Fri Mar 19 00:58:46 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Mar 2004 18:58:46 -0500
Subject: [R] character -> list
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A24@usrymx25.merck.com>

Use as.data.frame() on the matrix.

HTH,
Andy

> From: David Andel
> 
> Hi
> 
> I have a matrix of type "character" (strangly) and am trying to apply 
> the function "aggregate" to it, but unfortunately without success.
> It seems to me that aggregate would work, if I only could get 
> rid of the 
> quotation marks around each item.
> 
> So for a very simple example which looks as my actual data look (of 
> course here I put the quotation marks on purpose, but I have no idea 
> where they come from in my actual data):
> 
>  > x <- matrix(c("a","b","1","a","c","3","b","b","2"), nr=3, byrow=T, 
> dimnames=list(1:3, c("type","group","value")))
>  > x
>    type group value
> 1 "a"  "b"   "1"
> 2 "a"  "c"   "3"
> 3 "b"  "b"   "2"
> 
> I can't access the columns the way I think would be necessary for 
> aggregate to work:
>  > x$type
> NULL
> 
> And this seems to be the problem:
>  > typeof(x)
> [1] "character"
> 
> I think I would need to have type "list" to be able to apply 
> "aggregate".
> 
> How can I accomplish this?
> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.



From halper at health.nyc.gov  Fri Mar 19 01:29:22 2004
From: halper at health.nyc.gov (Howard Alper)
Date: Thu, 18 Mar 2004 19:29:22 -0500
Subject: [R] How to do a truncated regression
Message-ID: <s059f8b6.085@DPH-2LAF-GWIA.health.nycnet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040318/b4cd3d60/attachment.pl

From ggrothendieck at myway.com  Fri Mar 19 02:11:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 20:11:08 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040319011108.3A6D239C2@mprdmxin.myway.com>



I don't think I expressed myself very well on that.  

Looking at what we get from the example:

> z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))

> z
substitute(expression(function () 
{
    a + 1
}), list(a = quote(b)))

> class(z);mode(z);typeof(z)
[1] "call"
[1] "call"
[1] "language"


we see that the function seems to be expanded correctly and 
the statement does produce a call object.  However, 
applying eval one, two or three times does not give what 
you would think if you looked at z above.  In all cases, 
a is still there.  Some posters have claimed that the
problem is that list(f=f) cannot hold a function and must be
an expression but that's not what the help page says, the
examples are not all of class expression or call and
even if that were true then my parse/deparse example should 
work since it is unquestionably an expression.

Date:   Thu, 18 Mar 2004 16:41:31 -0700 
From:   Tony Plate <tplate at blackmesacapital.com>
To:   <ggrothendieck at myway.com>, <p.dalgaard at biostat.ku.dk> 
Cc:   <tlumley at u.washington.edu>, <R-help at stat.math.ethz.ch> 
Subject:   Re: [R] substitute question 

 
Gabor, you might have less frustration if you work with Peter's esub() 
suggestion (use it instead of substitute()):

esub <- function(expr, subst.list) do.call("substitute", list(expr, 
subst.list))

When you say

"In the other one, it is not an expression in the inner substitute
but should be by the time it gets to the outer one since I explicitly added 
expression(...). It is expanded which shows it did do that part right."

something to bear in mind is the "natural order" of evaluation is reversed 
with nested substitute()'s (because the inner substitute() is quoted by the 
outer substitute()) (but I haven't carefully reasoned through this being 
an explanation for your particular frustration here.)

hope this might help,

Tony Plate




At Thursday 04:06 PM 3/18/2004, Gabor Grothendieck wrote:

> > Date: 18 Mar 2004 23:52:47 +0100
> > From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > To: <ggrothendieck at myway.com>
> > Cc: <p.dalgaard at biostat.ku.dk>, <tlumley at u.washington.edu>, 
> <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch>
> > Subject: Re: [R] substitute question
> >
> >
> > "Gabor Grothendieck" <ggrothendieck at myway.com> writes:
> >
> > > From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> > > > (The real pain in these examples is that substitute autoquotes its
> > > > expr argument. Therefore, when you want to modify an expression that
> > > > is already stored in a variable, you need an extra outer layer of
> > > > eval(substitute(...)) to poke the content of the variable into the
> > > > inner substitute. An "esub" function with standard evaluation
> > > > semantics would make this much easier.)
> > >
> > > That is one of the frustrations of using substitute.
> > >
> > > The other is that even if you do perform two levels of substitute,
> > > as I have been trying, you still can't count on it working for
> > > an arbitrary unevaluated expression, as my examples show.
> >
> > Er, I don't think so. All I have seen is a couple of cases where you
> > tried to pass something that was not a language object (e.g. a
> > function as opposed to an expression or call generating a function.)
> >
> >
>
>The parse/deparse one definitely is an expression:
>
> > z <- parse(text=deparse(f))
> > class(z);mode(z);typeof(z)
>[1] "expression"
>[1] "expression"
>[1] "expression"
>
>In the other one, it is not an expression in the inner substitute
>but should be by the time it gets to the outer one since I
>explicitly added expression(...). It is expanded which shows
>it did do that part right.
>



From ggrothendieck at myway.com  Fri Mar 19 02:17:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 20:17:09 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040319011709.B257F39DE@mprdmxin.myway.com>


Date:   Thu, 18 Mar 2004 15:56:59 -0800 (PST) 
From:   Thomas Lumley <tlumley at u.washington.edu>
>  
> On Thu, 18 Mar 2004, Gabor Grothendieck wrote:
> > (1) One can either pick apart the function using body, or
>
> Actually, I think this is fairly natural -- it is only body(f) 
> that is an
> expression. Certainly substitute() could have been written to 
> operate on
> functions as well, but it wasn't.

In the context of R, natural is performing operations on whole
objects at once.  Its the same difference as indexing vs. vector
operations.

I am not sure where this came from about not operating on functions.
Are you sure?  I know just about everyone is saying this but
the help page does not refer to that and the 
example I gave where we do use list(f=f) for f a function does
seem to expand properly (see my last posting) although 
the subsitution never appears to take effect.



From tlumley at u.washington.edu  Fri Mar 19 02:27:20 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Mar 2004 17:27:20 -0800 (PST)
Subject: [R] substitute question
In-Reply-To: <20040319011108.3A6D239C2@mprdmxin.myway.com>
References: <20040319011108.3A6D239C2@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0403181716050.65426@homer10.u.washington.edu>

On Thu, 18 Mar 2004, Gabor Grothendieck wrote:

>
>
> I don't think I expressed myself very well on that.
>
> Looking at what we get from the example:
>
> > z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
>
> > z
> substitute(expression(function ()
> {
>     a + 1
> }), list(a = quote(b)))
>
> > class(z);mode(z);typeof(z)
> [1] "call"
> [1] "call"
> [1] "language"
>
>
> we see that the function seems to be expanded correctly and
> the statement does produce a call object.  However,
> applying eval one, two or three times does not give what
> you would think if you looked at z above.

Maybe we didn't express ourselves well enough.

Looking at z above isn't enough.  z is a call to substitute().
Its first operand is an expression. The expression contains a single term,
which is a function.

If you typed
notz<- quote(substitute(expression(function ()
 {
     a + 1
 }), list(a = quote(b))))

you would obtain something that deparsed the same as z, and so looked the
same, but was actually different.  In notz the first operand of substitute
is an expression containing multiple terms, which if evaluated would
return a function.

substitute() goes though this expression and checks each term to see if it
is `a`. In z there is only one term and it isn't `a`.  In notz there is
(after sufficient recursion) an `a` and it gets replaced.

So

> z[[2]][[2]]
function ()
{
    a + 1
}
> notz[[2]][[2]]
function() {
    a + 1
}

are the respective operands, and they still look the same. But

> mode(z[[2]][[2]])
[1] "function"
> mode(notz[[2]][[2]])
[1] "call"
> length(z[[2]][[2]])
[1] 1
> length(notz[[2]][[2]])
[1] 4

and if we try to find the actual `a` in there
> notz[[2]][[2]][[3]][[2]][[2]]
a
> z[[2]][[2]][[3]][[2]][[2]]
Error in z[[2]][[2]][[3]] : object is not subsettable
>


	-thomas



From tlumley at u.washington.edu  Fri Mar 19 02:29:52 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Mar 2004 17:29:52 -0800 (PST)
Subject: [R] substitute question
In-Reply-To: <20040319011709.B257F39DE@mprdmxin.myway.com>
References: <20040319011709.B257F39DE@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0403181728100.65426@homer10.u.washington.edu>

On Thu, 18 Mar 2004, Gabor Grothendieck wrote:
>
> I am not sure where this came from about not operating on functions.
> Are you sure?  I know just about everyone is saying this but
> the help page does not refer to that and the
> example I gave where we do use list(f=f) for f a function does
> seem to expand properly (see my last posting) although
> the subsitution never appears to take effect.
>

Yes, I am sure.

It's not that it is invalid to have a function as the first argument of
substitute, it's that substitute() does not recurse into either the body
or the formal arguments of the function (or into the environment, though
no-one would expect that).


	-thomas



From ccompton at ahc.co.nz  Fri Mar 19 02:42:26 2004
From: ccompton at ahc.co.nz (Chris Compton)
Date: Fri, 19 Mar 2004 14:42:26 +1300
Subject: [R] Specifying interaction terms in glm
Message-ID: <1B8862240867A4479484B3AE750AE7AA01E04C@AHCSBS01.ahc.co.nz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040319/58be61f8/attachment.pl

From ggrothendieck at myway.com  Fri Mar 19 02:46:16 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Mar 2004 20:46:16 -0500 (EST)
Subject: [R] character -> list
Message-ID: <20040319014616.4A8DE397E@mprdmxin.myway.com>


It would be helpful if you were more explicit on what you mean
by not successful.

Is the problem that:
1. you can't get aggregate to work with character data or 
2. you want to convert the numeric column to numbers or
3. you want to have a structure that has both numbers and character
columns

Also, I am not sure if you actually want the characters or if you
want the character columns converted to factors or, if applicable
to numbers.

In case 1, it actually does work:

> aggregate(x[,-2], list(x[,2]), length)
  Group.1 type value
1       b    2     2
2       c    1     1

In case 2, use as.numeric:

> aggregate(list(value=as.numeric(x[,3])),list(group=x[,2]), mean)
  group value
1     b   1.5
2     c   3.0

In case 3, create a data frame:
x.df <- data.frame(type = I(x[,1]), group = I(x[,2]), value = as.numeric(x[,3]))

If you want the character columns converted to factors then get
rid of the I's.


Date:   19 Mar 2004 00:47:38 +0100 
From:   David Andel <andel at ifi.unizh.ch>
To:   <R-help at stat.math.ethz.ch> 
Subject:   [R] character - > list 

 
Hi

I have a matrix of type "character" (strangly) and am trying to apply 
the function "aggregate" to it, but unfortunately without success.
It seems to me that aggregate would work, if I only could get rid of the 
quotation marks around each item.

So for a very simple example which looks as my actual data look (of 
course here I put the quotation marks on purpose, but I have no idea 
where they come from in my actual data):

> x <- matrix(c("a","b","1","a","c","3","b","b","2"), nr=3, byrow=T, 
dimnames=list(1:3, c("type","group","value")))
> x
type group value
1 "a" "b" "1"
2 "a" "c" "3"
3 "b" "b" "2"

I can't access the columns the way I think would be necessary for 
aggregate to work:
> x$type
NULL

And this seems to be the problem:
> typeof(x)
[1] "character"

I think I would need to have type "list" to be able to apply "aggregate".

How can I accomplish this?

Thanks,
David



From Bill.Venables at csiro.au  Fri Mar 19 02:36:24 2004
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Fri, 19 Mar 2004 12:36:24 +1100
Subject: [R] profile error on an nls object
Message-ID: <B998A44C8986644EA8029CFE6396A92401837B@exqld2-bne.qld.csiro.au>

Adrian Dragulescu asks:
 
> Hello all,
> 
> This is the error message that I get.
> >   hyp.res <- nls(log(y)~log(pdf.hyperb(theta,X)), data=dataModel,
> +              start=list(theta=thetaE0),
> +              trace=TRUE)
> 45.54325 :   0.1000000  1.3862944 -4.5577142  0.0005503
> 3.728302 :   0.0583857346  0.4757772859 -4.9156128701  0.0005563154
> 1.584317 :   0.0194149477  0.3444648833 -4.9365149150  0.0004105426
> 1.569333 :   0.0139310639  0.3824648048 -4.9024001228  0.0004089738
> 1.569311 :   0.0137155342  0.3888648619 -4.8979817546  0.0004137501
> 1.569311 :   0.0136895846  0.3893564152 -4.8976182201  0.0004141057
> 1.569311 :   0.0136876315  0.3894059947 -4.8975821760  0.0004141343
> >   hyp.res.S <- summary(hyp.res)
> >   hyp.res.S
> 
> Formula: log(y) ~ log(pdf.hyperb(theta, X))
> 
> Parameters:
> 	   Estimate Std. Error t value Pr(>|t|)
> theta1  0.0136876  0.0359964   0.380    0.705
> theta2  0.3894060  0.3079860   1.264    0.211
> theta3 -4.8975822  0.2219928 -22.062   <2e-16 ***
> theta4  0.0004141  0.0005457   0.759    0.451
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.1542 on 66 degrees of freedom
> 
> Correlation of Parameter Estimates:
> 	   theta1    theta2    theta3
> theta2 -0.02168
> theta3 -0.02029  0.997736
> theta4 -0.97182 -0.008054 -0.008952
> 
> >   pr1 <- profile(hyp.res)
> 1.825584 :   0.3894059947 -4.8975821760  0.0004141343
> ...
> 1.875413 :  0.000300086 1.830592703 0.000608001
> Error in prof$getProfile() : step factor 0.000488281 reduced below
`minFactor' of 0.000976563
> >
> 
> Why is there an error on profile, which should only evaluate the
> objective function for different parameter values?

profile() doesn't merely evaluate the objective function at
different parameter values. It holds one of the parameters constant
and optimises the objective function with respect to all the others.
This is repeated for a sequence of values passing through the mle, and
the same for each parameter.  

If you fix a parameter at a very unrealistic value (as can happen) it
is not surprising that the optimisation with respect to all the others
runs into trouble.  This is a computationally difficult area and I am
more surprised by the cases that work without a hitch than by the ones
that don't.  You can expect to have to tweak things a bit in many cases.

Bill Venables.

> 
> Thanks a lot.
> 
> Adrian
> 
> ______________________________________________



From acuster at nature.berkeley.edu  Fri Mar 19 03:47:54 2004
From: acuster at nature.berkeley.edu (Adrian Custer)
Date: Thu, 18 Mar 2004 18:47:54 -0800
Subject: [R] Incomplete Gamma Functions and GammaDistribution Doc errata.
Message-ID: <1079664474.19469.82.camel@tsetse.lab-net>

Hello all,

In the course of trying to implement the CDF of an
InverseGammaDistribution, I have run across the need for an igamma()
function. Several others have needed this function but the answers I
have found so far are not totally clear to me. I'm writing for three
reasons:
	1) to present a small error in the docs
	2) to clarify the approach we are expected to take
	3) to request,for the ease of use of myself and others, that the 	  R
developers implement igamma(), document the approach they 	  want users
to take or do both.

1) An error in the docs of the gamma distribution:
-------------------------------------------------

The GammaDistribution(param1,param2) is defined alternatively with two
versions of the second parameter which are inverses of one another.
'Rate' arises from one derivation such as that presented in Mathworld: 
http://mathworld.wolfram.com/GammaDistribution.html
(see just below equation (2) )
        "define $\theta \equiv 1 / \lambda$ to be the time between
        changes"
which makes \theta in equation (3) a rate parameter. In the R
documentation (i.e. ?pgamma) this same equation is presented but the
parameter \theta is now called s and is termed the scale. This goes
against general usage of mathworld, wikipaedia, the COLT library, and
others who call the "s" term a "rate". 

Please either change the equation or replace the name with "rate" and
the label with r or nu or theta.



2) Using existing functions to build the igamma(\alpha,\beta):
-------------------------------------------------------------

The incomplete gamma has been mentioned in two previous emails that I
could find:
http://www.r-project.org/nocvs/mail/r-help/2000/1006.html
and
http://www.r-project.org/nocvs/mail/r-help/2000/3618.html
both of which suggest that the upper incomplete gamma can be obtained 
using the pgamma() function. 

Is Prof. Ripley's message (the first above) saying that

	upperIncompletegamma(x,\rate) = pgamma(x,\rate) * gamma(\rate) ?

I have not yet been able to see why this would be true. I keep tripping
myself up on the differing notation of R, mathworld, wikipaedia and
Abramowitz and Stegun's "Handbook of mathematical functions".

Would someone please confirm that this is mathematically correct? 


3) Adding an igamma() function or documentation for how to generate it.
--------------------------------------------------------------------

Since I am at least the third person not clever enough to see the way to
derive the igamma directly, because the lack of an igamma() function has
cost me a chunk of my day, because using functions related to
distributions to derive pure mathematical functions is counter
intuitive, and because two implementations already exist, I hope the R
developers would be willing to incorporate an igamma() function directly
into R. 

The implementation could either use the code posted to the list:
http://www.r-project.org/nocvs/mail/r-help/1998/0295.html
or be a convinience function around the pgamma and gamma solution
suggested in the emails presented above. 

Alternatively, if this approach doesn't suit you, would someone please
add an explanation to the documentation for the gamma function
explaining how to generate an igamma(\alpha, \beta) using the current
functions?

Thanks to all,
adrian



From joseclaudio.faria at terra.com.br  Fri Mar 19 04:09:19 2004
From: joseclaudio.faria at terra.com.br (joseclaudio.faria)
Date: Fri, 19 Mar 2004 00:09:19 -0300
Subject: [R] Frequency table
Message-ID: <000201c40d5f$9de63320$01fea8c0@sapetinga>

Hi,

See if this generic function I made can help you.

data <- c(65, 70, 85, 65, 65, 65, 62, 55, 82, 59,
               55, 66, 74, 55, 65, 56, 80, 73, 45, 64,
               75, 58, 60, 56, 60, 65, 53, 63, 72, 80,
               90, 95, 55, 70, 79, 62, 57, 65, 60, 47,
               61, 53, 80, 75, 72, 87, 52, 72, 80, 85,
               75, 70, 84, 60, 72, 70, 76, 70, 79, 72,
               69, 80, 62, 74, 54, 58, 58, 69, 81, 84)

#------------ begin options of table---------------
min <-    40
max <- 100
h   <-     10
#-------------- end options of table---------------

#-------- begin declaration of variables-----------
Fi <- numeric(); FacA <- numeric(); FacP <- numeric(); FrA   <- numeric();
FrP    <- numeric()
#-------- end declaration of variables-------------

#----------------- begin function------------------
Createtable <- function()
{
  Fi <<- table(cut(data, br = seq(min, max, h), right = FALSE))
  K <- length(names(Fi))
  n <- length(data)

  for(i in 1:K)
  {
    FrA[i] = Fi[i] / n
  }

  for(i in 1:K)
  {
    FrP[i] = (Fi[i] / n) * 100
  }

  for(i in 1:K)
  {
    FacA[i] = sum(Fi[1:i])
  }

  for(i in 1:K)
  {
    FacP[i] = (sum(Fi[1:i]) / n) * 100
  }

  table <- data.frame(Fi, FrA, FrP, FacA, FacP)
}
#----------------- end function------------------

tab <- Createtable()
print("Complete table:")
print(tab)

Jos? Cl?udio Faria
UESC/DCET
Brasil
73-634.2779
joseclaudio.faria at terra.com.br
jc_faria at uol.com.br



From ok at cs.otago.ac.nz  Fri Mar 19 04:44:18 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 19 Mar 2004 16:44:18 +1300 (NZDT)
Subject: [R] Why is rpart() so slow?
Message-ID: <200403190344.i2J3iISP238216@atlas.otago.ac.nz>

I've had rpart running on a problem now for a couple of *days*,
but I'd expect a decision tree builder to run in minutes if not
seconds.  Why is rpart slow?  Is there anything I can do to make
it quicker?



From p.connolly at hortresearch.co.nz  Fri Mar 19 04:54:53 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 19 Mar 2004 16:54:53 +1300
Subject: [R] Why is rpart() so slow?
In-Reply-To: <200403190344.i2J3iISP238216@atlas.otago.ac.nz>;
	from ok@cs.otago.ac.nz on Fri, Mar 19, 2004 at 04:44:18PM +1300
References: <200403190344.i2J3iISP238216@atlas.otago.ac.nz>
Message-ID: <20040319165453.J2137@hortresearch.co.nz>

On Fri, 19-Mar-2004 at 04:44PM +1300, Richard A. O'Keefe wrote:

|> I've had rpart running on a problem now for a couple of *days*,
|> but I'd expect a decision tree builder to run in minutes if not
|> seconds.  Why is rpart slow?  Is there anything I can do to make
|> it quicker?

Yes.  You could give us an indication of just what you're trying to
do, with what, and to what, so we would be in a position to say what
improvements could be made.



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ok at cs.otago.ac.nz  Fri Mar 19 05:15:06 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 19 Mar 2004 17:15:06 +1300 (NZDT)
Subject: [R] Why is rpart() so slow?
Message-ID: <200403190415.i2J4F6mA239517@atlas.otago.ac.nz>

I asked why rpart is slow.
Patrick Connolly <p.connolly at hortresearch.co.nz> replied:
	You could give us an indication of just what you're trying to
	do, with what, and to what, so we would be in a position to say what
	improvements could be made.
	
The thing that is chugging away now is
	rpart(rgrp ~ y2 + sex, a.frame, a.frame$wt)

where

    rgrp has 21 levels
    y2 has 561 levels
    sex has 2 levels
    wt has values 1..9
    a.frame has 50,500 cases and other variables

I have written decision tree builders, in fact I've published a paper on
the technique, and I really would expect this to zip through in seconds.
instead of the 4 hours this one has taken so far today (500MHz machine).

Presuambly it's something to do with trying to do binary splits and find
good subsets, but I don't *want* binary splits, and I can't figure out from
?rpart how to tell rpart that I don't want binary splits.

(The idea of trying to find an optimal partition of a set of 561 elements
does not fill me with enthusiasm.)

Is there perhaps an alternative to rpart that does n-way splits instead of
binary splits?



From zelickr at pdx.edu  Fri Mar 19 06:00:08 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Thu, 18 Mar 2004 21:00:08 -0800 (PST)
Subject: [R] data frame output
Message-ID: <Pine.GSO.4.44.0403182038320.4711-100000@freke.odin.pdx.edu>

Hello list,

Is there a way to *not* supress leading zeros when printing (to the
console window or to a file) a dataframe?

Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From ripley at stats.ox.ac.uk  Fri Mar 19 09:39:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 08:39:39 +0000 (GMT)
Subject: [R] Incomplete Gamma Functions and GammaDistribution Doc errata.
In-Reply-To: <1079664474.19469.82.camel@tsetse.lab-net>
Message-ID: <Pine.LNX.4.44.0403190745170.13651-100000@gannet.stats>

On Thu, 18 Mar 2004, Adrian Custer wrote:

> In the course of trying to implement the CDF of an
> InverseGammaDistribution, I have run across the need for an igamma()
> function. Several others have needed this function but the answers I
> have found so far are not totally clear to me. I'm writing for three
> reasons:
> 	1) to present a small error in the docs
> 	2) to clarify the approach we are expected to take
> 	3) to request,for the ease of use of myself and others, that the 	  R
> developers implement igamma(), document the approach they 	  want users
> to take or do both.

We would rather you defer to someone who leaps to fewer false assumptions.

> 1) An error in the docs of the gamma distribution:
> -------------------------------------------------
> 
> The GammaDistribution(param1,param2) is defined alternatively with two
> versions of the second parameter which are inverses of one another.
> 'Rate' arises from one derivation such as that presented in Mathworld: 
> http://mathworld.wolfram.com/GammaDistribution.html
> (see just below equation (2) )
>         "define $\theta \equiv 1 / \lambda$ to be the time between
>         changes"

Do please read what your reference says.  It calls \lambda the `rate of 
change', and that is the only occurrence of the word `rate' on the page.

> which makes \theta in equation (3) a rate parameter. In the R

That seems an impossible interpretation of what is actually said.

> documentation (i.e. ?pgamma) this same equation is presented but the
> parameter \theta is now called s and is termed the scale. This goes
> against general usage of mathworld, wikipaedia, the COLT library, and
> others who call the "s" term a "rate". 

`mathworld' plainly does not, and you have given us no others to verify.

> Please either change the equation or replace the name with "rate" and
> the label with r or nu or theta.

Please look `rate' up in your dictionary, and look the gamma density up in 
a score of statistical references.  Your reference calls what R calls 
`rate' a `rate of change', and you insist it means that 1/rate is `a rate 
parameter'!


> 2) Using existing functions to build the igamma(\alpha,\beta):
> -------------------------------------------------------------
> 
> The incomplete gamma has been mentioned in two previous emails that I
> could find:
> http://www.r-project.org/nocvs/mail/r-help/2000/1006.html
> and
> http://www.r-project.org/nocvs/mail/r-help/2000/3618.html
> both of which suggest that the upper incomplete gamma can be obtained 
> using the pgamma() function. 

Neither mention the `upper incomplete gamma'.

> Is Prof. Ripley's message (the first above) saying that
> 
> 	upperIncompletegamma(x,\rate) = pgamma(x,\rate) * gamma(\rate) ?


Let me quote it to you, since by this point I have problems with your 
veracity.

  > Has anybody implemented the _incomplete_ gamma function in R?

  What do you think the cumulative distribution function of a gamma
  distribution is? (Now, there is a little room for disagreement here,
  but I think

  pgamma(x, nu)*gamma(nu)

  might be what you are looking for.) 

So

-  `Upper incomplete gamma' is not mentioned.
-  In your own reference, the parameters of an incomplete gamma do not 
contain a rate,

and so what makes you think nu is \rate?  The second argument of pgamma is
the *scale*.

I believe the `incomplete Gamma' of people like Pearson is what your 
reference calls the `lower incomplete Gamma', but note I did say

	there is a little room for disagreement here
and

	might be what you are looking for.

Let's look at Abramowitz and Stegun,

http://jove.prohosting.com/~skripty/page_260.htm

They have P(a, x) as the integral from 0 to x, whereas in Mathematica
notation \gamma(a, x) is the integral from 0 to a with shape parameter x.  
(That is pretty strange, since it is a that is normally varied for fixed
x.) So Abramowitz and Stegun's incomplete Gamma is the lower incomplete
Gamma of Mathematica with a more sensible set of parameter names.  Then in
your reference the gamma cdf for \lambda=1 is

D(x) = pgamma(x, shape = h) = P(h, x)/P(h, Inf)

and hence

P(nu, x)  = pgamma(x, shape = h) * P(h, Inf) 
          = pgamma(x, shape = h) * gamma(nu)

just as I suggested.

> I have not yet been able to see why this would be true. I keep tripping
> myself up on the differing notation of R, mathworld, wikipaedia and
> Abramowitz and Stegun's "Handbook of mathematical functions".
> 
> Would someone please confirm that this is mathematically correct? 

It is pretty discourteous to ask other people to confirm that an expert
posting is `mathematically correct', especially given the phrases you
selectively omitted.


> 3) Adding an igamma() function or documentation for how to generate it.
> --------------------------------------------------------------------
> 
> Since I am at least the third person not clever enough to see the way to
> derive the igamma directly, because the lack of an igamma() function has
> cost me a chunk of my day, because using functions related to
> distributions to derive pure mathematical functions is counter
> intuitive, and because two implementations already exist, I hope the R
> developers would be willing to incorporate an igamma() function directly
> into R. 
> 
> The implementation could either use the code posted to the list:
> http://www.r-project.org/nocvs/mail/r-help/1998/0295.html
> or be a convinience function around the pgamma and gamma solution
> suggested in the emails presented above. 
> 
> Alternatively, if this approach doesn't suit you, would someone please
> add an explanation to the documentation for the gamma function
> explaining how to generate an igamma(\alpha, \beta) using the current
> functions?

It is very easy, *once* you know what you mean by an incomplete Gamma
function.  Note that if you read mathworld, you are unlikely to be able to
communicate with either mathematicians or statisticians, even if you read
it accurately.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Mar 19 09:06:21 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Mar 2004 09:06:21 +0100
Subject: [R] profile error on an nls object
In-Reply-To: <B998A44C8986644EA8029CFE6396A92401837B@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A92401837B@exqld2-bne.qld.csiro.au>
Message-ID: <x27jxhz2f6.fsf@biostat.ku.dk>

<Bill.Venables at csiro.au> writes:

> > Why is there an error on profile, which should only evaluate the
> > objective function for different parameter values?
> 
> profile() doesn't merely evaluate the objective function at
> different parameter values. It holds one of the parameters constant
> and optimises the objective function with respect to all the others.
> This is repeated for a sequence of values passing through the mle, and
> the same for each parameter.  
> 
> If you fix a parameter at a very unrealistic value (as can happen) it
> is not surprising that the optimisation with respect to all the others
> runs into trouble.  This is a computationally difficult area and I am
> more surprised by the cases that work without a hitch than by the ones
> that don't.  You can expect to have to tweak things a bit in many cases.

True. I did get some experience with the profiler for the mle() code.
This was originally a fairly straightforward adaptation of the nls
profiler (which, I believe, in turn is an adaptation of profile.glm
which Bill wrote). It did turn out to be quite frail when confronted
with highly non-quadratic likelihoods and bounded domains. Brian
Ripley has since improved the mle profiler quite a bit, with step size
adjustments and try() constructs in critical places.

I must admit that I haven't studied Brian's news code too carefully,
but there's probably still some room for improvement. For instance, it
seems that the local optimization always starts from the global
parameter estimates, which could likely be faster and more stable if
we used the values from the previous step instead.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Fri Mar 19 09:46:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Mar 2004 09:46:14 +0100
Subject: [R] Reading  image file formats
In-Reply-To: <5.2.0.9.0.20040318175948.02d44a70@pop.cimat.mx>
References: <5.2.0.9.0.20040318175948.02d44a70@pop.cimat.mx>
Message-ID: <405AB356.4080803@statistik.uni-dortmund.de>

Teresa Alarcon wrote:

> Hi!
> Is there exists an instruction to read images (bmp, jpg, etc.) in R?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

See package "pixmap" for pixel map images.

Uwe Ligges



From pauljohn at ku.edu  Fri Mar 19 07:24:55 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Fri, 19 Mar 2004 00:24:55 -0600
Subject: [R] using "unstack" inside my function: that old scope problem again
Message-ID: <405A9237.1090402@ku.edu>

I've been reading the R mail archives and I've found  a lot of messages 
with this same kind of problem, but I can't understand the answers.  Can 
one of you try to explain this to me?

Here's my example. Given a regression model and a variable, I want to 
use unstack() on the vector of residuals and make some magic with the 
result. But unstack hates me.
 

PCSE <- function (tmodel,groupVar) {
  myres1 <- resid(tmodel)
  resUnstacked <- unstack(myres1, form = myres1 ~ groupVar));
  E <- as.matrix(resUnstacked)
  SIGMA <- (1/nrow(E))*(t(E) %*% E)
  OMEGA <- diag(x=1, nrow=nrow(E), ncol=nrow(E)) %x% SIGMA

  X <- model.matrix(tmodel)
  XPRIMEXINV <- solve(t(X) %*% X)
  PCSECOVB <- XPRIMEXINV %*%  (t(X) %*% OMEGA %*% X ) %*% XPRIMEXINV
}


The error is:
PCSE(eld.ols1,dat2$STATEID)
Error in eval(expr, envir, enclos) : Object "groupVar" not found

Here's what I don't understand the most.
 
If I hack this so that the "resUnstacked" is created by a "matrix" 
command, then the function works. Why would matrix() work when unstack 
does not.  And why does model.matrix() work if unstack does not.

Thanks in advance, as usual.

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From ripley at stats.ox.ac.uk  Fri Mar 19 09:47:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 08:47:53 +0000 (GMT)
Subject: [R] data frame output
In-Reply-To: <Pine.GSO.4.44.0403182038320.4711-100000@freke.odin.pdx.edu>
Message-ID: <Pine.LNX.4.44.0403190842260.13651-100000@gannet.stats>

On Thu, 18 Mar 2004, Randy Zelick wrote:

> Is there a way to *not* supress leading zeros when printing (to the
> console window or to a file) a dataframe?

If you mean via print() or autoprinting, no.
I am not sure why you would want to do this, but it seems that using
format() and then gsub should work.  For example

library(MASS)
fh <- format(hills)
fh[] <- lapply(fh, function(x) gsub(" ", "0", as.character(x)))
fh
                 dist climb    time
Greenmantle      02.5  0650 016.083
Carnethy         06.0  2500 048.350
Craig Dunain     06.0  0900 033.650
Ben Rha          07.5  0800 045.600
Ben Lomond       08.0  3070 062.267
...


See also ?sprintf.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Mar 19 09:55:54 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Mar 2004 09:55:54 +0100
Subject: [R] data frame output
In-Reply-To: <Pine.GSO.4.44.0403182038320.4711-100000@freke.odin.pdx.edu>
References: <Pine.GSO.4.44.0403182038320.4711-100000@freke.odin.pdx.edu>
Message-ID: <16474.46490.151946.699074@gargle.gargle.HOWL>

>>>>> "Randy" == Randy Zelick <zelickr at pdx.edu>
>>>>>     on Thu, 18 Mar 2004 21:00:08 -0800 (PST) writes:

    Randy> Hello list,
    Randy> Is there a way to *not* supress leading zeros when printing (to the
    Randy> console window or to a file) a dataframe?

Yes, e.g. use

  formatC(..., format = "f")

to produce a character matrix and write() that to your file.



From Stephanie.Mahevas at ifremer.fr  Fri Mar 19 09:52:29 2004
From: Stephanie.Mahevas at ifremer.fr (Stephanie MAHEVAS)
Date: Fri, 19 Mar 2004 09:52:29 +0100
Subject: [R] Dynamic Factor Analysis
Message-ID: <405AB4CD.5000105@ifremer.fr>

Dear users

I would like to detect and estimate  common patterns in a set of time 
series. I have heard about the Dynamic Factor Analysis (DFA) which is  a 
multivariate time series analysis method to perform such analysis.
Does anybody know if there is a librairy dedicated to DFA?
Thank in advance
stephanie Mahevas

-- 
......................................................................
Stephanie MAHEVAS (Stephanie.Mahevas at ifremer.fr)
  IFREMER/MAERHA   Tel: 02 40 37 41 81 Fax: 02 40 37 40 75
  (Math?matiques Appliqu?es ? l'Evaluation
   des Ressources  Halieutiques et Aquacoles)
   rue de l'?le d'Yeu
   BP 21105  44311 NANTES Cedex 03
   http://www.ifremer.fr/maerha
     o   \ o /  _ o         __|    \ /     |__        o _  \ o /   o
    /|\    |     /\   ___\o   \o    |    o/    o/__   /\     |    /|\
    / \   / \   | \  /)  |    ( \  /o\  / )    |  (\  / |   / \   / \
......................................................................



From maechler at stat.math.ethz.ch  Fri Mar 19 09:52:32 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 Mar 2004 09:52:32 +0100
Subject: [R] Incomplete Gamma Functions and GammaDistribution Doc errata.
In-Reply-To: <1079664474.19469.82.camel@tsetse.lab-net>
References: <1079664474.19469.82.camel@tsetse.lab-net>
Message-ID: <16474.46288.872673.87181@gargle.gargle.HOWL>

>>>>> "Adrian" == Adrian Custer <acuster at nature.berkeley.edu>
>>>>>     on Thu, 18 Mar 2004 18:47:54 -0800 writes:

    Adrian> Hello all,
    Adrian> In the course of trying to implement the CDF of an
    Adrian> InverseGammaDistribution, I have run across the need for an igamma()
    Adrian> function. Several others have needed this function but the answers I
    Adrian> have found so far are not totally clear to me. I'm writing for three
    Adrian> reasons:

    Adrian> 1) to present a small error in the docs
I don't agree.
Did you look at the postscript or PDF version of the help file?
{you can get with
     help(pgamma, offline=TRUE)
 when you have a working latex installation}

If you compare its formula with Abramowitz & Stegun 6.5.1
its pretty pretty very very obvious

that   P(a,x) == pgamma(x,a)

{so forget about 'rate' or 'scale' which are both 1 by default!)

Why should this be hard to see? Shuldn't you first do your
homework before starting to insult the R developers?

Regards,
Martin Maechler, Seminar fuer Statistik, ETH Zurich Switzerland



From ripley at stats.ox.ac.uk  Fri Mar 19 10:00:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 09:00:54 +0000 (GMT)
Subject: [R] Incomplete Gamma Functions and GammaDistribution Doc errata.
In-Reply-To: <Pine.LNX.4.44.0403190745170.13651-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0403190858180.13651-100000@gannet.stats>

Note corrections to two typos, since Master Custer seems to get confused
by tiny details.

On Fri, 19 Mar 2004, Prof Brian Ripley wrote:

> On Thu, 18 Mar 2004, Adrian Custer wrote:
> 
> > In the course of trying to implement the CDF of an
> > InverseGammaDistribution, I have run across the need for an igamma()
> > function. Several others have needed this function but the answers I
> > have found so far are not totally clear to me. I'm writing for three
> > reasons:
> > 	1) to present a small error in the docs
> > 	2) to clarify the approach we are expected to take
> > 	3) to request,for the ease of use of myself and others, that the 	  R
> > developers implement igamma(), document the approach they 	  want users
> > to take or do both.
> 
> We would rather you defer to someone who leaps to fewer false assumptions.
> 
> > 1) An error in the docs of the gamma distribution:
> > -------------------------------------------------
> > 
> > The GammaDistribution(param1,param2) is defined alternatively with two
> > versions of the second parameter which are inverses of one another.
> > 'Rate' arises from one derivation such as that presented in Mathworld: 
> > http://mathworld.wolfram.com/GammaDistribution.html
> > (see just below equation (2) )
> >         "define $\theta \equiv 1 / \lambda$ to be the time between
> >         changes"
> 
> Do please read what your reference says.  It calls \lambda the `rate of 
> change', and that is the only occurrence of the word `rate' on the page.
> 
> > which makes \theta in equation (3) a rate parameter. In the R
> 
> That seems an impossible interpretation of what is actually said.
> 
> > documentation (i.e. ?pgamma) this same equation is presented but the
> > parameter \theta is now called s and is termed the scale. This goes
> > against general usage of mathworld, wikipaedia, the COLT library, and
> > others who call the "s" term a "rate". 
> 
> `mathworld' plainly does not, and you have given us no others to verify.
> 
> > Please either change the equation or replace the name with "rate" and
> > the label with r or nu or theta.
> 
> Please look `rate' up in your dictionary, and look the gamma density up in 
> a score of statistical references.  Your reference calls what R calls 
> `rate' a `rate of change', and you insist it means that 1/rate is `a rate 
> parameter'!
> 
> 
> > 2) Using existing functions to build the igamma(\alpha,\beta):
> > -------------------------------------------------------------
> > 
> > The incomplete gamma has been mentioned in two previous emails that I
> > could find:
> > http://www.r-project.org/nocvs/mail/r-help/2000/1006.html
> > and
> > http://www.r-project.org/nocvs/mail/r-help/2000/3618.html
> > both of which suggest that the upper incomplete gamma can be obtained 
> > using the pgamma() function. 
> 
> Neither mention the `upper incomplete gamma'.
> 
> > Is Prof. Ripley's message (the first above) saying that
> > 
> > 	upperIncompletegamma(x,\rate) = pgamma(x,\rate) * gamma(\rate) ?
> 
> 
> Let me quote it to you, since by this point I have problems with your 
> veracity.
> 
>   > Has anybody implemented the _incomplete_ gamma function in R?
> 
>   What do you think the cumulative distribution function of a gamma
>   distribution is? (Now, there is a little room for disagreement here,
>   but I think
> 
>   pgamma(x, nu)*gamma(nu)
> 
>   might be what you are looking for.) 
> 
> So
> 
> -  `Upper incomplete gamma' is not mentioned.
> -  In your own reference, the parameters of an incomplete gamma do not 
> contain a rate,
> 
> and so what makes you think nu is \rate?  The second argument of pgamma is
> the *scale*.

the *shape*.

> 
> I believe the `incomplete Gamma' of people like Pearson is what your 
> reference calls the `lower incomplete Gamma', but note I did say
> 
> 	there is a little room for disagreement here
> and
> 
> 	might be what you are looking for.
> 
> Let's look at Abramowitz and Stegun,
> 
> http://jove.prohosting.com/~skripty/page_260.htm
> 
> They have P(a, x) as the integral from 0 to x, whereas in Mathematica
> notation \gamma(a, x) is the integral from 0 to a with shape parameter x.  
> (That is pretty strange, since it is a that is normally varied for fixed
> x.) So Abramowitz and Stegun's incomplete Gamma is the lower incomplete
> Gamma of Mathematica with a more sensible set of parameter names.  Then in
> your reference the gamma cdf for \lambda=1 is
> 
> D(x) = pgamma(x, shape = h) = P(h, x)/P(h, Inf)
> 
> and hence
> 
> P(nu, x)  = pgamma(x, shape = h) * P(h, Inf) 
>           = pgamma(x, shape = h) * gamma(nu)

P(nu, x)  = pgamma(x, shape = nu) * P(nu, Inf)
          = pgamma(x, shape = nu) * gamma(nu)

> just as I suggested.
> 
> > I have not yet been able to see why this would be true. I keep tripping
> > myself up on the differing notation of R, mathworld, wikipaedia and
> > Abramowitz and Stegun's "Handbook of mathematical functions".
> > 
> > Would someone please confirm that this is mathematically correct? 
> 
> It is pretty discourteous to ask other people to confirm that an expert
> posting is `mathematically correct', especially given the phrases you
> selectively omitted.
> 
> 
> > 3) Adding an igamma() function or documentation for how to generate it.
> > --------------------------------------------------------------------
> > 
> > Since I am at least the third person not clever enough to see the way to
> > derive the igamma directly, because the lack of an igamma() function has
> > cost me a chunk of my day, because using functions related to
> > distributions to derive pure mathematical functions is counter
> > intuitive, and because two implementations already exist, I hope the R
> > developers would be willing to incorporate an igamma() function directly
> > into R. 
> > 
> > The implementation could either use the code posted to the list:
> > http://www.r-project.org/nocvs/mail/r-help/1998/0295.html
> > or be a convinience function around the pgamma and gamma solution
> > suggested in the emails presented above. 
> > 
> > Alternatively, if this approach doesn't suit you, would someone please
> > add an explanation to the documentation for the gamma function
> > explaining how to generate an igamma(\alpha, \beta) using the current
> > functions?
> 
> It is very easy, *once* you know what you mean by an incomplete Gamma
> function.  Note that if you read mathworld, you are unlikely to be able to
> communicate with either mathematicians or statisticians, even if you read
> it accurately.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Mar 19 10:09:39 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Mar 2004 10:09:39 +0100
Subject: [R] using "unstack" inside my function: that old scope problem
	again
In-Reply-To: <405A9237.1090402@ku.edu>
References: <405A9237.1090402@ku.edu>
Message-ID: <x21xnpgq3w.fsf@biostat.ku.dk>

Paul Johnson <pauljohn at ku.edu> writes:

> I've been reading the R mail archives and I've found  a lot of
> messages with this same kind of problem, but I can't understand the
> answers.  Can one of you try to explain this to me?
> 
> Here's my example. Given a regression model and a variable, I want to
> use unstack() on the vector of residuals and make some magic with the
> result. But unstack hates me.
>  PCSE <- function (tmodel,groupVar) {
>   myres1 <- resid(tmodel)
>   resUnstacked <- unstack(myres1, form = myres1 ~ groupVar));
>   E <- as.matrix(resUnstacked)
>   SIGMA <- (1/nrow(E))*(t(E) %*% E)
>   OMEGA <- diag(x=1, nrow=nrow(E), ncol=nrow(E)) %x% SIGMA
> 
>   X <- model.matrix(tmodel)
>   XPRIMEXINV <- solve(t(X) %*% X)
>   PCSECOVB <- XPRIMEXINV %*%  (t(X) %*% OMEGA %*% X ) %*% XPRIMEXINV
> }
> 
> 
> The error is:
> PCSE(eld.ols1,dat2$STATEID)
> Error in eval(expr, envir, enclos) : Object "groupVar" not found
> 
> Here's what I don't understand the most.
>  If I hack this so that the "resUnstacked" is created by a "matrix"
> command, then the function works. Why would matrix() work when unstack
> does not.  And why does model.matrix() work if unstack does not.
> 
> Thanks in advance, as usual.

Look inside getAnywhere(unstack.default) and you'll find things like

x <- as.list(x)
...
res <- c(tapply(eval(form[[2]], x), eval(form[[3]], x), as.vector))


Now, "x" is myres1, form[[2]] is quote(myres1), and form[[3]] is
quote(groupVar). myres1 would appear to be a vector and I suspect it
doesn't have any elements of that name...

You might be able to unstack(data.frame(myres1,groupVar), ..etc..)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lmsilva at fe.up.pt  Fri Mar 19 10:11:59 2004
From: lmsilva at fe.up.pt (lmsilva@fe.up.pt)
Date: Fri, 19 Mar 2004 09:11:59 +0000
Subject: [R] projection pursuit
In-Reply-To: <Pine.LNX.4.44.0403181728540.12577-100000@gannet.stats>
References: <Pine.LNX.4.44.0403181728540.12577-100000@gannet.stats>
Message-ID: <1079687519.405ab95f9ae96@webmail.fe.up.pt>

This is exactly what I mean by PPDE. I was reading the paper you mention below 
and I was looking for already implemented code in languages like R, Matlab or 
C.

Thank you all
Luis


> I don't think anyone has actually answered this.  PPDE is a technical 
> term, defined in
> 
> Friedman, Steutzle and Schroeder (1984) Projection pursuit density
> estimation. Journal of the American Statistical Association,
> 79(387):599-608, September 1984.
> 
> I don't know of an R implementation, but have known of S ones (to which I
> no longer have access).
> 
> If that is not what you mean by PPDE, please give us the reference(s) you 
> have in mind.
> 
> 
> On Wed, 17 Mar 2004 lmsilva at fe.up.pt wrote:
> 
> > Does R have a package that performs projection pursuit density estimation?
> Or 
> > anyone knows code in Matlab or C for example to do this?
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From ripley at stats.ox.ac.uk  Fri Mar 19 10:14:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 09:14:00 +0000 (GMT)
Subject: [R] using "unstack" inside my function: that old scope problem
	again
In-Reply-To: <405A9237.1090402@ku.edu>
Message-ID: <Pine.LNX.4.44.0403190906000.13651-100000@gannet.stats>

The problem is the use of a formula, and hence non-standard evaluation 
(even non-standard for a formula).  Unstack contains

    res <- c(tapply(eval(form[[2]], x), eval(form[[3]], x), as.vector))

and those eval() calls are looking only in x and then along the search
path.  They should be something like

eval(form[[2]], x, parent.frame())

or

eval(form[[2]], x, environment(form))

depending on your preferences for scoping rules.

unstack.default is very simple: why not extract the part you want?


On Fri, 19 Mar 2004, Paul Johnson wrote:

> I've been reading the R mail archives and I've found  a lot of messages 
> with this same kind of problem, but I can't understand the answers.  Can 
> one of you try to explain this to me?
> 
> Here's my example. Given a regression model and a variable, I want to 
> use unstack() on the vector of residuals and make some magic with the 
> result. But unstack hates me.
>  
> 
> PCSE <- function (tmodel,groupVar) {
>   myres1 <- resid(tmodel)
>   resUnstacked <- unstack(myres1, form = myres1 ~ groupVar));
>   E <- as.matrix(resUnstacked)
>   SIGMA <- (1/nrow(E))*(t(E) %*% E)
>   OMEGA <- diag(x=1, nrow=nrow(E), ncol=nrow(E)) %x% SIGMA
> 
>   X <- model.matrix(tmodel)
>   XPRIMEXINV <- solve(t(X) %*% X)
>   PCSECOVB <- XPRIMEXINV %*%  (t(X) %*% OMEGA %*% X ) %*% XPRIMEXINV
> }
> 
> 
> The error is:
> PCSE(eld.ols1,dat2$STATEID)
> Error in eval(expr, envir, enclos) : Object "groupVar" not found
> 
> Here's what I don't understand the most.
>  
> If I hack this so that the "resUnstacked" is created by a "matrix" 
> command, then the function works. Why would matrix() work when unstack 
> does not.  And why does model.matrix() work if unstack does not.

Because the eval calls in model.matrix.default have been thought through.

> 
> Thanks in advance, as usual.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angel_lul at hotmail.com  Fri Mar 19 11:25:37 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Fri, 19 Mar 2004 10:25:37 +0000
Subject: [R] date conversions in pastecs
Message-ID: <405ACAA1.3020608@hotmail.com>

In function daystoyears in package pastecs, I get this (wrong?) result 
with 1995:

 >  daystoyears(1,datemin="1/1/1997",dateformat="m/d/Y")
        
1997.001
 >  daystoyears(1,datemin="1/1/1995",dateformat="m/d/Y")
        
1994.999

Any insights?
Thanks
Angel



From wb at arb-phys.uni-dortmund.de  Fri Mar 19 10:27:56 2004
From: wb at arb-phys.uni-dortmund.de (Wilhelm B. Kloke)
Date: Fri, 19 Mar 2004 10:27:56 +0100 (CET)
Subject: [R] Crossed Random Effects ?
Message-ID: <200403190927.i2J9Runs097044@yorikke.arb-phys.uni-dortmund.de>

I have been asked how to handle the following situation in R:

Given an unbalanced design of 3 crossed random effects, such as
subject, rater and item, how to estimate the variance components?

I know how to do it using lme, but this seems to be limited to
the nested case; or to use aov with error strata, when the
design is balanced. 
-- 
Dipl.-Math. Wilhelm Bernhard Kloke
Institut fuer Arbeitsphysiologie an der Universitaet Dortmund
Ardeystrasse 67, D-44139 Dortmund, Tel. 0231-1084-257



From ripley at stats.ox.ac.uk  Fri Mar 19 10:38:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 09:38:09 +0000 (GMT)
Subject: [R] using "unstack" inside my function: that old scope problem
	again
In-Reply-To: <x21xnpgq3w.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0403190936280.13651-100000@gannet.stats>

On 19 Mar 2004, Peter Dalgaard wrote:

> Paul Johnson <pauljohn at ku.edu> writes:
> 
> > I've been reading the R mail archives and I've found  a lot of
> > messages with this same kind of problem, but I can't understand the
> > answers.  Can one of you try to explain this to me?
> > 
> > Here's my example. Given a regression model and a variable, I want to
> > use unstack() on the vector of residuals and make some magic with the
> > result. But unstack hates me.
> >  PCSE <- function (tmodel,groupVar) {
> >   myres1 <- resid(tmodel)
> >   resUnstacked <- unstack(myres1, form = myres1 ~ groupVar));
> >   E <- as.matrix(resUnstacked)
> >   SIGMA <- (1/nrow(E))*(t(E) %*% E)
> >   OMEGA <- diag(x=1, nrow=nrow(E), ncol=nrow(E)) %x% SIGMA
> > 
> >   X <- model.matrix(tmodel)
> >   XPRIMEXINV <- solve(t(X) %*% X)
> >   PCSECOVB <- XPRIMEXINV %*%  (t(X) %*% OMEGA %*% X ) %*% XPRIMEXINV
> > }
> > 
> > 
> > The error is:
> > PCSE(eld.ols1,dat2$STATEID)
> > Error in eval(expr, envir, enclos) : Object "groupVar" not found
> > 
> > Here's what I don't understand the most.
> >  If I hack this so that the "resUnstacked" is created by a "matrix"
> > command, then the function works. Why would matrix() work when unstack
> > does not.  And why does model.matrix() work if unstack does not.
> > 
> > Thanks in advance, as usual.
> 
> Look inside getAnywhere(unstack.default) and you'll find things like
> 
> x <- as.list(x)
> ...
> res <- c(tapply(eval(form[[2]], x), eval(form[[3]], x), as.vector))
> 
> 
> Now, "x" is myres1, form[[2]] is quote(myres1), and form[[3]] is
> quote(groupVar). myres1 would appear to be a vector and I suspect it
> doesn't have any elements of that name...

But it is not documented that `form' refers only to columns of x.
Is that the intention?  I thought not.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andel at ifi.unizh.ch  Fri Mar 19 10:58:09 2004
From: andel at ifi.unizh.ch (David Andel)
Date: 19 Mar 2004 10:58:09 +0100
Subject: [R] line going across several graphs
Message-ID: <405AC431.5090609@ifi.unizh.ch>

Hi

Is there a way to draw a line across several graphs?
Say I have a few histograms one upon the other with "0" exactly on one line.
Now when I do abline(v=0, lty=2) I have only the short lines 
corresponding to each graph.
How can I draw one line through?

Thanks,
David



From p.dalgaard at biostat.ku.dk  Fri Mar 19 11:20:13 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Mar 2004 11:20:13 +0100
Subject: [R] using "unstack" inside my function: that old scope problem
	again
In-Reply-To: <Pine.LNX.4.44.0403190936280.13651-100000@gannet.stats>
References: <Pine.LNX.4.44.0403190936280.13651-100000@gannet.stats>
Message-ID: <x2wu5hf89u.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > > Thanks in advance, as usual.
> > 
> > Look inside getAnywhere(unstack.default) and you'll find things like
> > 
> > x <- as.list(x)
> > ...
> > res <- c(tapply(eval(form[[2]], x), eval(form[[3]], x), as.vector))
> > 
> > 
> > Now, "x" is myres1, form[[2]] is quote(myres1), and form[[3]] is
> > quote(groupVar). myres1 would appear to be a vector and I suspect it
> > doesn't have any elements of that name...
> 
> But it is not documented that `form' refers only to columns of x.
> Is that the intention?  I thought not.

Unstack() is defined as the reverse of stack(), so it is not really
well defined what to expect of the default method. However, I think
you're right that something is wrong: If you run unstack at the
command level you'd pick up items in the global environment, but the
similar thing doesn't happen inside a function. You'd likely want the
eval()s to use enclos=parent.frame(), or maybe the whole thing wants
to be eval(...,environment(form)).

I'm also puzzled about the as.list thing. It doesn't seem to do what I
would expect such a thing to do...

> x <- rnorm(6)
> g <- gl(2,3)
> unstack(x,x~g)
NULL data frame with 0 rows
> unstack(list(x),x~g)
Error in tapply(eval(form[[2]], x), eval(form[[3]], x), as.vector) :
        arguments must have same length
> unstack(list(x=x),x~g)
          X1          X2
1 -1.1093047 -0.02241302
2  0.6129984  0.22170428
3  1.1512541  0.17254144


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From e.kalmbach at biol.rug.nl  Fri Mar 19 11:56:50 2004
From: e.kalmbach at biol.rug.nl (Ellen Kalmbach)
Date: Fri, 19 Mar 2004 11:56:50 +0100
Subject: [R] yags, GEEs, and GLMMs
Message-ID: <004501c40da0$e17a3eb0$c3847d81@129125132195>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040319/2e6d82ce/attachment.pl

From chris1 at psyctc.org  Fri Mar 19 12:03:07 2004
From: chris1 at psyctc.org (Chris Evans)
Date: Fri, 19 Mar 2004 11:03:07 -0000
Subject: [R] 1.8.1 on Debian stable
Message-ID: <405AD36B.26919.81B501C@localhost>

This has probably been addressed here, but I can't find it if it has, 
and it may be up on the R project homepage or the CRAN pages, but if 
so ....

... so sorry if this has been answered before: question is, can I 
upgrade to 1.8.1 under Debian stable (a.k.a. "Woody")?  The deb 
packages in the CRAN archive are clearly for 1.8.0 and those in the 
"unstable" distro for Debian are 1.8.1.  Therefore I assume there are 
dependency problems to solve that didn't make it worth making 1.8.1 
available for Woody and if the package maintainers don't want to try, 
I certainly won't attempt it.  But I am sorry as this seems to mean 
that I can't get the ROC package in Bioconductor for my machine.

(No: too many security reasons why I don't want to go to "unstable" 
on the machine in question.)

Thanks in advance for any advice, pointers or clarifications.

Chris
PSYCTC: Psychotherapy, Psychology, Psychiatry, Counselling
   and Therapeutic Communities; practice, research, 
   teaching and consultancy.
Chris Evans & Jo-anne Carlyle
http://psyctc.org/ Email: chris at psyctc.org



From chris1 at psyctc.org  Fri Mar 19 12:45:53 2004
From: chris1 at psyctc.org (Chris Evans)
Date: Fri, 19 Mar 2004 11:45:53 -0000
Subject: [R] Moving to 1.8.1: can you transfer your package list?
Message-ID: <405ADD71.13510.842766A@localhost>

I prefer to use R on a linux box and ultimately need things to end up 
there to serve things up using David Firth's excellent CGIwithR and 
apache, but one step at a time and I've installed 1.8.1 under win2k.  
Another question that I'm sure is simple: is there a simple way to 
find the list of installed libraries I had in my 1.7.1 installation 
and use that to drive install.packages?

I've done it with a rather ugly bit of copy and paste using library() 
in the 1.7.1 installation and then using install.packages("xxx") on 
each of the package names from the first column of the output from 
library().  How should I have done it more elegantly gurus?

TIA,

Chris
PSYCTC: Psychotherapy, Psychology, Psychiatry, Counselling
   and Therapeutic Communities; practice, research, 
   teaching and consultancy.
Chris Evans & Jo-anne Carlyle
http://psyctc.org/ Email: chris at psyctc.org



From ripley at stats.ox.ac.uk  Fri Mar 19 13:02:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 12:02:36 +0000 (GMT)
Subject: [R] Moving to 1.8.1: can you transfer your package list?
In-Reply-To: <405ADD71.13510.842766A@localhost>
Message-ID: <Pine.LNX.4.44.0403191156040.17175-100000@gannet.stats>

On Fri, 19 Mar 2004, Chris Evans wrote:

> I prefer to use R on a linux box and ultimately need things to end up 
> there to serve things up using David Firth's excellent CGIwithR and 
> apache, but one step at a time and I've installed 1.8.1 under win2k.  
> Another question that I'm sure is simple: is there a simple way to 
> find the list of installed libraries I had in my 1.7.1 installation 
> and use that to drive install.packages?
> 
> I've done it with a rather ugly bit of copy and paste using library() 
> in the 1.7.1 installation and then using install.packages("xxx") on 
> each of the package names from the first column of the output from 
> library().  How should I have done it more elegantly gurus?

installed.packages(priority="NA")[, 1]

Can I suggest that with 1.9.0 imminent, you use that rather than 1.8.1.
The Windows build is at 
http://cran.r-project.org/bin/windows/base/rdevel.html

At the very least, use a version of R-patched (similar URL).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Mar 19 13:14:38 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Mar 2004 13:14:38 +0100
Subject: [R] Moving to 1.8.1: can you transfer your package list?
In-Reply-To: <405ADD71.13510.842766A@localhost>
References: <405ADD71.13510.842766A@localhost>
Message-ID: <405AE42E.1040009@statistik.uni-dortmund.de>

Chris Evans wrote:
> I prefer to use R on a linux box and ultimately need things to end up 
> there to serve things up using David Firth's excellent CGIwithR and 
> apache, but one step at a time and I've installed 1.8.1 under win2k.  
> Another question that I'm sure is simple: is there a simple way to 
> find the list of installed libraries I had in my 1.7.1 installation 
> and use that to drive install.packages?
> 
> I've done it with a rather ugly bit of copy and paste using library() 
> in the 1.7.1 installation and then using install.packages("xxx") on 
> each of the package names from the first column of the output from 
> library().  How should I have done it more elegantly gurus?
>

  ip <- installed.packages(lib.loc = "pathToYourOldLibraryDir")
  ip <- ip[ip[,1]!="base" ,1]
  install.packages(ip)

Uwe Ligges



> TIA,
> 
> Chris
> PSYCTC: Psychotherapy, Psychology, Psychiatry, Counselling
>    and Therapeutic Communities; practice, research, 
>    teaching and consultancy.
> Chris Evans & Jo-anne Carlyle
> http://psyctc.org/ Email: chris at psyctc.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Fri Mar 19 13:33:38 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 19 Mar 2004 13:33:38 +0100
Subject: [R] date conversions in pastecs
In-Reply-To: <405ACAA1.3020608@hotmail.com>
Message-ID: <MABBLJDICACNFOLGIHJOEENNEDAA.phgrosjean@sciviews.org>

You should read ?daystoyears more carefully. There is a details section
where you read:

Details:

     The "years" time-scale uses one unit for each year. We
     deliberately "linearized" time in this time-scale and each year
     has 365.25 days. There is thus no adjustment for bissextile years.
     Similarly, one month is considered to be 1/12 year, no mather if
     it has 28, 29, 30 or 31 days. This representation simplifies
     further calculations, especially regarding seasonal effects (a
     quarter is exactly 0.25 units for instance), but introduces some
     shifts in time (of up to one day, which is not significant when
     working on long-term series with years as units). However,
     converting it back to "days", using 'yearstodays()' restablishes
     correct initial days without errors.

Expect thus, as explained in that section, some shifts of up to one day in
the calculation.

Best,

Philippe Grosjean

.......................................................<?}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Angel Lopez
Sent: Friday, 19 March, 2004 11:26
To: R-help at stat.math.ethz.ch
Subject: [R] date conversions in pastecs


In function daystoyears in package pastecs, I get this (wrong?) result
with 1995:

 >  daystoyears(1,datemin="1/1/1997",dateformat="m/d/Y")

1997.001
 >  daystoyears(1,datemin="1/1/1995",dateformat="m/d/Y")

1994.999

Any insights?
Thanks
Angel

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ru68y7s at myrealbox.com  Fri Mar 19 13:49:06 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Fri, 19 Mar 2004 05:49:06 -0700
Subject: [R] cumulative density question
Message-ID: <1079700546.d6882e3cru68y7s@myrealbox.com>

Hi,

I am interested in looking at cumulative density functions. If F(x) is a cumulative density of monthly fund returns over the interval of a to b, and I am interested in returns above and below a specified point r, then I would like to find the number that is made up of
 
1.(integral from r to b)(1-F(x))dx  
2. (integral from a to r)(F(x)dx)

3. the ratio of #1/#2 above


In financial literature this ratio has been called the Omega function.

My first guess in obtaining this equation using R
is to use the integrate function but I am have two problems: 

I. can I use a nonparametric density in the integrate function(how?), 
II. how can i get the ratio of #3 above as the integrate function gives the number plus the absolute error
> integrate(dnorm,-4,1.96)
0.9749704 with absolute error < 2.1e-07
so using a ratio  in #3 above i get the following error:
 : non-numeric argument to binary operator



Thank you in advance.
Sri Viswanath



From bates at stat.wisc.edu  Fri Mar 19 13:56:58 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 19 Mar 2004 06:56:58 -0600
Subject: [R] Crossed Random Effects ?
In-Reply-To: <200403190927.i2J9Runs097044@yorikke.arb-phys.uni-dortmund.de>
References: <200403190927.i2J9Runs097044@yorikke.arb-phys.uni-dortmund.de>
Message-ID: <6r7jxhxaed.fsf@bates4.stat.wisc.edu>

"Wilhelm B. Kloke" <wb at arb-phys.uni-dortmund.de> writes:

> I have been asked how to handle the following situation in R:
> 
> Given an unbalanced design of 3 crossed random effects, such as
> subject, rater and item, how to estimate the variance components?
> 
> I know how to do it using lme, but this seems to be limited to
> the nested case; or to use aov with error strata, when the
> design is balanced. 

With the current version of lme it is difficult and inefficient to
work with crossed random effects.  A new version of lme being
developed makes this much easier.  I expect to release new versions of
the Matrix and lme4 packages with R-1.9.0 (early April). I will
describe the new lme capabilities in more detail at the useR!2004
conference in May.
-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From koen.hufkens at pandora.be  Fri Mar 19 14:06:29 2004
From: koen.hufkens at pandora.be (Koen Hufkens)
Date: Fri, 19 Mar 2004 14:06:29 +0100
Subject: [R] Beginners question
Message-ID: <opr431c3g90nfjvb@mail-out.pandora.be>

Dear list,

I've been messing around with coding functions in R and it just won't make 
sense to me.
Running my analysis by hand on command line is fine and works but because 
of the repetitive nature of the job I would like to code a function for it.

My problem:

I would like to read in data from a file in my current working dir.

so my code would look like:

myanalysis <- function(sample, samplenr){
sample.samplenr <- read.geodata("sample.samplenr", arg, arg etc.)
sample.samplenr.results <- someanalysis(sample.samplenr)
}

problem is that charater values aren't interpretated as they should. Any 
tips and tricks? This should be fairly simple but I just can't find any 
reference to it. Most input values in functions in R are numeric or are 
strings to be interpretated with bolean statements but aren't handled as 
input variable perse.

Suggestions and comments would be greatly appreciated,
Koen.



From krcabrer at perseus.unalmed.edu.co  Fri Mar 19 14:08:30 2004
From: krcabrer at perseus.unalmed.edu.co (Kenneth Cabrera)
Date: Fri, 19 Mar 2004 08:08:30 -0500
Subject: [R] model.tables (se=T) with interaction
Message-ID: <opr431ggkmfaouaq@200.24.8.4>


Hi R users.

Why if I have a two-way or multifactor analysis of variance and
I call the function model.tables with the se=T options there
is not estimation when I have an interaction on the model?


Thank you very much for your help


-- 
Kenneth Cabrera
Universidad Nacional de Colombia
Tel Of 430 9351
Cel 315 504 9339
Medell?n



From e.kalmbach at biol.rug.nl  Fri Mar 19 14:34:58 2004
From: e.kalmbach at biol.rug.nl (Ellen Kalmbach)
Date: Fri, 19 Mar 2004 14:34:58 +0100
Subject: [R] yags, GEEs and GLMMs
Message-ID: <007601c40db6$f9048b10$c3847d81@129125132195>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040319/2b455f85/attachment.pl

From ggrothendieck at myway.com  Fri Mar 19 14:36:45 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Mar 2004 08:36:45 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040319133645.3E9C23990@mprdmxin.myway.com>


[This appears to have bounced so I am sending it again.  Apologies
if it gets there twice.]

Thanks. Thus it seems that there are two types of expressions and calls:

1. fully expanded 
2. partially expanded

and that fully expanded ones are a prerequisite for substitution.
body() and quote() produce such fully expanded expressions.

Using a small utility function we can investigate this:

recurse <- function( x, idx = NULL )
     if ( length( x ) > 0 ) { 
          for( i in seq( along = x ) )
               if (length(x[[i]])>1) 
                    Recall( x[[i]], c(idx, i))
               else {
                    if (length(idx)) cat(idx,"")
                    cat( i, class(x[[i]]), ":" )
                    cat( rep("\t",length(idx) + 2) )
                    print( x[[i]] )
               }
     }

f <- function(){a+1}

eb <- body(f)
class(eb)
recurse(eb)

eq <- quote(function(){a+1})
class(eq)
recurse(eq)

ep <- parse(text=deparse(f))
class(ep)
recurse(ep)


The output that the above is shown below. It shows that
body() and quote() produce fully expanded expression style objects
although body's is of class { and quote is of class call.

However, parse(text=deparse(f)) also produces a fully expanded
expression style object of class expression yet substitution 
does not occur with that. Thus full vs. partial expansion is likely
a necessary but not a sufficient condition. There is something
else but I don't know what it is.


> f <- function(){a+1}
> 
> eb <- body(f)
> class(eb)
[1] "{"
> recurse(eb)
1 name : `{`
2 1 name : `+`
2 2 name : a
2 3 numeric : [1] 1
> 
> eq <- quote(function(){a+1})
> class(eq)
[1] "call"
> recurse(eq) # lines begin with list indices and class name
1 name : `function`
2 NULL : NULL
3 1 name : `{`
3 2 1 name : `+`
3 2 2 name : a
3 2 3 numeric : [1] 1
4 NULL : NULL
> 
> ep <- parse(text=deparse(f))
> class(ep)
[1] "expression"
> recurse(ep)
1 1 name : `function`
1 2 NULL : NULL
1 3 1 name : `{`
1 3 2 1 name : `+`
1 3 2 2 name : a
1 3 2 3 numeric : [1] 1
1 4 NULL : NULL


Date: Thu, 18 Mar 2004 17:27:20 -0800 (PST) 
From: Thomas Lumley <tlumley at u.washington.edu>
To: Gabor Grothendieck <ggrothendieck at myway.com> 
Cc: <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch> 
Subject: Re: [R] substitute question 


On Thu, 18 Mar 2004, Gabor Grothendieck wrote:

>
>
> I don't think I expressed myself very well on that.
>
> Looking at what we get from the example:
>
> > z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
>
> > z
> substitute(expression(function ()
> {
> a + 1
> }), list(a = quote(b)))
>
> > class(z);mode(z);typeof(z)
> [1] "call"
> [1] "call"
> [1] "language"
>
>
> we see that the function seems to be expanded correctly and
> the statement does produce a call object. However,
> applying eval one, two or three times does not give what
> you would think if you looked at z above.

Maybe we didn't express ourselves well enough.

Looking at z above isn't enough. z is a call to substitute().
Its first operand is an expression. The expression contains a single term,
which is a function.

If you typed
notz<- quote(substitute(expression(function ()
{
a + 1
}), list(a = quote(b))))

you would obtain something that deparsed the same as z, and so looked the
same, but was actually different. In notz the first operand of substitute
is an expression containing multiple terms, which if evaluated would
return a function.

substitute() goes though this expression and checks each term to see if it
is `a`. In z there is only one term and it isn't `a`. In notz there is
(after sufficient recursion) an `a` and it gets replaced.

So

> z[[2]][[2]]
function ()
{
a + 1
}
> notz[[2]][[2]]
function() {
a + 1
}

are the respective operands, and they still look the same. But

> mode(z[[2]][[2]])
[1] "function"
> mode(notz[[2]][[2]])
[1] "call"
> length(z[[2]][[2]])
[1] 1
> length(notz[[2]][[2]])
[1] 4

and if we try to find the actual `a` in there
> notz[[2]][[2]][[3]][[2]][[2]]
a
> z[[2]][[2]][[3]][[2]][[2]]
Error in z[[2]][[2]][[3]] : object is not subsettable
>


-thomas



From pauljohn at ku.edu  Fri Mar 19 14:31:23 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Fri, 19 Mar 2004 07:31:23 -0600
Subject: [R] Beginners question
In-Reply-To: <opr431c3g90nfjvb@mail-out.pandora.be>
References: <opr431c3g90nfjvb@mail-out.pandora.be>
Message-ID: <405AF62B.1010108@ku.edu>

I do this kind of thing all the time.  Here's an example program that 
iterates over files in a directory.  The files are called

DataCulture1
DataCulture2

and so forth.  I want to make a graph based on the first, then the 
second, and so forth.  This version will make a graph each time the 
function newgr() is run. This uses locator to make the user place labels 
on the graph. If you don't want it to interact that way, kill that line, 
or else the program will seem not to run.

run<-1
updateDataSet <- function(run)
  {
    aChar<-character(1)
    aChar<-as.character(run)
    dsname<-c(paste("DataCulture",aChar,sep=""))
    data<-read.table(dsname,header=T,as.is = TRUE)

  }

buildGraph <-function(data)
{
tmp1<-plot(data$acquaint~data$T,type='l', ylim=c(0,1),ylab="average 
proportion",xlab="PERIOD",lty=1,pch=1,main="")
par("new"=TRUE)
tmp2<-plot(data$harmony~data$T,type='l', ylim=c(0,1),ylab="average 
proportion",xlab="PERIOD",lty=2,pch=1,main="")
par("new"=TRUE)
tmp3<-plot(data$identical~data$T,type='l', ylim=c(0,1),ylab="average 
proportion",xlab="PERIOD",lty=3,pch=1,main="")
par("new"=TRUE)
tmp4<-plot(data$totalEntropy~data$T,type='l',ylim=c(0,1),ylab="",xlab="",lty=4,pch=1,main="", 
cex=10)

#want a legend?
#legend(max(data$T)/2,0.2,c("Acquaintance","Harmonious","Identical","Entropy"),lty=1:4,xjust=1,yjust=1)
#if you want to interact, do this:
#legend(locator(1),c("Acquaintance","Harmonious","Identical"),lty=1:3)
#or do this to place the lables, one after the other
text(locator(4),c("Acquaintance","Harmony","Identical","Entropy"))

}

redraw<-function()
  {
    dsname<-updateDataSet(run)
    buildGraph(dsname)
  }

newgr<-function()
{
  dsname<-updateDataSet(run)
  buildGraph(dsname)
  run <<- run+1
}

newgr()


Koen Hufkens wrote:

> Dear list,
>
> I've been messing around with coding functions in R and it just won't 
> make sense to me.
> Running my analysis by hand on command line is fine and works but 
> because of the repetitive nature of the job I would like to code a 
> function for it.
>
> My problem:
>
> I would like to read in data from a file in my current working dir.
>
> so my code would look like:
>
> myanalysis <- function(sample, samplenr){
> sample.samplenr <- read.geodata("sample.samplenr", arg, arg etc.)
> sample.samplenr.results <- someanalysis(sample.samplenr)
> }
>
> problem is that charater values aren't interpretated as they should. 
> Any tips and tricks? This should be fairly simple but I just can't 
> find any reference to it. Most input values in functions in R are 
> numeric or are strings to be interpretated with bolean statements but 
> aren't handled as input variable perse.
>
> Suggestions and comments would be greatly appreciated,
> Koen.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From adrian_d at eskimo.com  Fri Mar 19 15:15:37 2004
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Fri, 19 Mar 2004 06:15:37 -0800 (PST)
Subject: [R] cumulative density function
Message-ID: <Pine.SUN.4.58.0403190612540.15982@eskimo.com>


Because the result is a list, you can get your number like this:
integrate(dnorm,0,2)[[1]]

You don't have an expression for the probability function?  You can do
a nonlinear regression to a distribution you belive in, and then
integrate that.  Or, you can calculate the experimental cdf by simple
summation.

Best,
Adrian



From ramasamy at cancer.org.uk  Fri Mar 19 15:22:13 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 19 Mar 2004 14:22:13 -0000
Subject: [R] Beginners question
In-Reply-To: <opr431c3g90nfjvb@mail-out.pandora.be>
Message-ID: <ODEPICOHNDBJEHIFCIMPGEKBCBAA.ramasamy@cancer.org.uk>

1. Section 2.6 Character Vectors of R-intro (www.r-project.org -> Manual ->
Introduction to R)
2. Search the archives https://www.stat.math.ethz.ch/mailman/listinfo/r-help
which is appended as footnote on every R-help mail
3. help("paste")

> -----Original Message-----
> From:
> r-help-bounces+adaikalavan.ramasamy=cancer.org.uk at stat.math.ethz.ch
> [mailto:r-help-bounces+adaikalavan.ramasamy=cancer.org.uk at stat.math.ethz
> .ch]On Behalf Of Koen Hufkens
> Sent: 19 March 2004 13:06
> To: R help mailing list
> Subject: [R] Beginners question
>
>
> Dear list,
>
> I've been messing around with coding functions in R and it just
> won't make
> sense to me.
> Running my analysis by hand on command line is fine and works but because
> of the repetitive nature of the job I would like to code a
> function for it.
>
> My problem:
>
> I would like to read in data from a file in my current working dir.
>
> so my code would look like:
>
> myanalysis <- function(sample, samplenr){
> sample.samplenr <- read.geodata("sample.samplenr", arg, arg etc.)
> sample.samplenr.results <- someanalysis(sample.samplenr)
> }
>
> problem is that charater values aren't interpretated as they should. Any
> tips and tricks? This should be fairly simple but I just can't find any
> reference to it. Most input values in functions in R are numeric or are
> strings to be interpretated with bolean statements but aren't handled as
> input variable perse.
>
> Suggestions and comments would be greatly appreciated,
> Koen.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From berthold.stegemann at medtronic.com  Fri Mar 19 15:39:23 2004
From: berthold.stegemann at medtronic.com (Stegemann, Berthold)
Date: Fri, 19 Mar 2004 15:39:23 +0100
Subject: [R] Spatial Statistics: surf.gls
Message-ID: <C691EA62A179F04E87E33CEE4A1ADCF0B7BD49@MSTM1BMSGM50.ent.core.medtronic.com>

 In an experimental setup we obtain z-data samples at equidistant grid points.

The surf.gls (Kriging) algorithm produces an error under this circumstance when performing the Choleski decomposition.

A workaround is to dither the grid coordinates using  (x <- rnorm(length(x)) ; y<- rnowm(length(y))).

Question: Is this an expected behaviour  of the  surf.gls function ?

Regards,
Berthold 

---------------------------------
Dr. Berthold Stegemann 
Senior Scientist
Bakken Research Center
Heart Failure Management
Endepolsdomein 5
6229 GW Maastricht
The Netherlands
Office: +31.43.3566.613
Mobile: +31.6.5335.1669
Fax: +31.43.3566.509



From neil.skjodt at ualberta.ca  Fri Mar 19 15:49:26 2004
From: neil.skjodt at ualberta.ca (Neil Skjodt)
Date: Fri, 19 Mar 2004 07:49:26 -0700
Subject: [R] for loop or Hmisc library trap.rule function syntax error
Message-ID: <405B1A54@webmail.ualberta.ca>

Hello:

I am new R user stumped why the R code after this paragraph generates "Error: 
syntax error" messages after each of the last 2 lines. I have tried searching 
the manuals, Hmisc documentation, contributed manuals, help archives, and 
Internet. I am running R 1.7.1 under Windows 2000 (I will upgrade when my 
imminent OS upgrade happens). My data was successfully entered and displayed 
as data.df whose first row is column labels, whose subsequent rows are 
separate subject results, whose 2nd to 7th columns are numeric control results 
(for observations at 0, 30, 60, 90, 120, and 180 min), and whose 8th to 13th 
columns are numeric treatment results. I am trying to enter the control and 
test area under the curve values from the Hmisc trap.rule function into 
"control" and "test" for hypothesis testing. I tried posting this last PM, but 
my message seems to have been lost. I apologize if this appears as a duplicate 
message. Thanks. Neil

...
control = c();		 
test = c();

for (subj in 1:11) {

  rownum = subj+1	  # add one to subject as first row is column labels
  control(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,2:7))
  test(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,8:13))
}
...



From spencer.graves at pdf.com  Fri Mar 19 15:46:10 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Mar 2004 06:46:10 -0800
Subject: [R] cumulative density question
In-Reply-To: <1079700546.d6882e3cru68y7s@myrealbox.com>
References: <1079700546.d6882e3cru68y7s@myrealbox.com>
Message-ID: <405B07B2.6030803@pdf.com>

    1.  I assume you are looking at the cumulative DISTRIBUTION function 
(cdf), F(x), which is the probability that a random variable X is less 
than or equal to x;  the cdf is the integral of the density function. 

    2.  The maximum likelihood nonparametric estimate of the cdf is the 
"empirical cdf" (ecdf) in library(stepfun).  However, this is NOT a 
function appropriate for "integrate".  The following looks like it gives 
me the correct answer: 


pecdf <- function(x, y=1:3, lower.tail=TRUE){
   nx <- length(x)
   F. <- rep(NA, nx)
   for(i in 1:nx){
     F.[i] <- sum(y<=x[i])
   }
   F. <- F./length(y)
   if(lower.tail) F. else (1-F.)
}

pecdf(0:4)
pecdf(0:4, lower.tail=FALSE)

Omega <-
function(r){
  numerator <- integrate(pecdf, 1, r)
  denominator <- integrate(pecdf, r, 3, lower.tail=FALSE)
  numerator$value/denominator$value
}

Omega(2)

      3.  For "data" y = 1:3, I get the following expressions for the 
numerator and denominator: 

      numerator(r) = ((ifelse(r<=2, (5-2*r), (3-r))/3)

      denominator(r) = ((ifelse(r<=2, (r-1), (2*r-3))/3)

One could probably develop a more general form of this for arbitrary "y". 

      hope this helps.  spencer graves

s viswanath wrote:

>Hi,
>
>I am interested in looking at cumulative density functions. If F(x) is a cumulative density of monthly fund returns over the interval of a to b, and I am interested in returns above and below a specified point r, then I would like to find the number that is made up of
> 
>1.(integral from r to b)(1-F(x))dx  
>2. (integral from a to r)(F(x)dx)
>
>3. the ratio of #1/#2 above
>
>
>In financial literature this ratio has been called the Omega function.
>
>My first guess in obtaining this equation using R
>is to use the integrate function but I am have two problems: 
>
>I. can I use a nonparametric density in the integrate function(how?), 
>II. how can i get the ratio of #3 above as the integrate function gives the number plus the absolute error
>  
>
>>integrate(dnorm,-4,1.96)
>>    
>>
>0.9749704 with absolute error < 2.1e-07
>so using a ratio  in #3 above i get the following error:
> : non-numeric argument to binary operator
>
>
>
>Thank you in advance.
>Sri Viswanath
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From feh3k at spamcop.net  Fri Mar 19 15:50:19 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 19 Mar 2004 09:50:19 -0500
Subject: [R] for loop or Hmisc library trap.rule function syntax error
In-Reply-To: <405B1A54@webmail.ualberta.ca>
References: <405B1A54@webmail.ualberta.ca>
Message-ID: <20040319095019.4f7d8b11.feh3k@spamcop.net>

On Fri, 19 Mar 2004 07:49:26 -0700
Neil Skjodt <neil.skjodt at ualberta.ca> wrote:

> Hello:
> 
> I am new R user stumped why the R code after this paragraph generates
> "Error: syntax error" messages after each of the last 2 lines. I have
> tried searching the manuals, Hmisc documentation, contributed manuals,
> help archives, and Internet. I am running R 1.7.1 under Windows 2000 (I
> will upgrade when my imminent OS upgrade happens). My data was
> successfully entered and displayed as data.df whose first row is column
> labels, whose subsequent rows are separate subject results, whose 2nd to
> 7th columns are numeric control results (for observations at 0, 30, 60,
> 90, 120, and 180 min), and whose 8th to 13th columns are numeric
> treatment results. I am trying to enter the control and test area under
> the curve values from the Hmisc trap.rule function into "control" and
> "test" for hypothesis testing. I tried posting this last PM, but my
> message seems to have been lost. I apologize if this appears as a
> duplicate message. Thanks. Neil
> 
> ...
> control = c();		 
> test = c();

You need to spend time with the manuals.  Remove ; from end of line, set
aside vectors of full length, use [ ] for subscripting and subsetting;
remove rownum; recognize that first row should not be "column labels".

Frank Harrell

> 
> for (subj in 1:11) {
> 
>   rownum = subj+1	  # add one to subject as first row is column labels
>   control(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,2:7))
>   test(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,8:13))
> }

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From rksh at soc.soton.ac.uk  Fri Mar 19 16:06:59 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 19 Mar 2004 15:06:59 +0000
Subject: [R] asp=1 and aspect ratio
Message-ID: <a06002003bc80bca7ab38@[139.166.242.29]>


Hi everyone

I want a square scatterplot with abline(0,1) going exactly through the
SW and NE corners.  By "square" I mean that the plotting region is
exactly square, and that the axis limits are identical.

x <- 1:20
y <- x+rep(c(-1,1),10)
lims <- range(c(x,y))

None of the following do this:

plot(x,y) ; abline(0,1)         #not square
plot(x,y,asp=1);abline(0,1)     #diagonal line misses corners

plot(x,y,asp=1,xaxs="i",yaxs="i");abline(0,1)
#diagonal line misses corners

plot(x,y,xaxs="i",yaxs="i");abline(0,1)
#not square and diag misses corners

plot(x,y,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
#not square

plot(x,y,asp=1,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
#not square (plotting region about 8cm by 9.5cm)



If I wrap the above lines in

postscript(file="foo.ps", width=5,height=5)
[...snip...]
dev.off()

then still none of the plots is exactly right [either one or both
corners are missed, or the aspect ratio off].  What am I missing?




-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From ripley at stats.ox.ac.uk  Fri Mar 19 16:07:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 15:07:13 +0000 (GMT)
Subject: [R] Spatial Statistics: surf.gls
In-Reply-To: <C691EA62A179F04E87E33CEE4A1ADCF0B7BD49@MSTM1BMSGM50.ent.core.medtronic.com>
Message-ID: <Pine.LNX.4.44.0403191504180.23498-100000@gannet.stats>

On Fri, 19 Mar 2004, Stegemann, Berthold wrote:

>  In an experimental setup we obtain z-data samples at equidistant grid points.
> 
> The surf.gls (Kriging) algorithm produces an error under this
> circumstance when performing the Choleski decomposition.
> 
> A workaround is to dither the grid coordinates using  
(x <- rnorm(length(x)) ; y<- rnowm(length(y))).

That's not dithering but changing them completely.
Hint: ?jitter.

> Question: Is this an expected behaviour  of the  surf.gls function ?

No, and it has been tested under those circumstances.  Did you adjust the 
accuracy of the approximation via nx?  Is your covariance model actually a 
valid one?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Dan.Kelley at Dal.Ca  Fri Mar 19 16:09:52 2004
From: Dan.Kelley at Dal.Ca (Dan Kelley)
Date: Fri, 19 Mar 2004 11:09:52 -0400
Subject: [R] osx/fink: cannot do "R INSTALL" (library mixup)
Message-ID: <792C2DF5-79B7-11D8-81E4-000A95A7B4EC@Dal.Ca>

Hello friends.  I've just switched from linux to osx, and I'm liking 
it, except now that I'm getting down to brass tacks, and needing to 
install some R packages, I'm at a roadblock.  I'm hoping someone can 
help.

My R is installed from a .dmg, and resides as follows:
	$ which R
	/usr/local/bin/R
The problem is that I cannot build packages.  For example, see the 
below (which also indicates that libfrtbegin.a is in the
	/sw
directory, i.e. that it was installed by the FINK setup).  Does anybody 
know how I can solve this problem, e.g. how to alter the
	-L(directory)
  commands that are employed?

(Aside: I looked for a solution on the R archives.  Also, I've tried to 
make my subject-line clear enough that an answer will benefit other 
osx/fink users.  Thanks!!)

	$ R INSTALL pspline
	* Installing *source* package 'pspline' ...
	** libs
	gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
pspline.so Pspline.o  -L/usr/local/lib 
-L/usr/local/lib/gcc/powerpc-	apple-darwin6.6/3.4 
-L/usr/local/lib/gcc/powerpc-apple-darwin6.6/3.4/../../.. -lfrtbegin 
-lg2c -lSystem -lcc_dynamic
	ld: warning -L: directory name 
(/usr/local/lib/gcc/powerpc-apple-darwin6.6/3.4) does not exist
	ld: warning -L: directory name 
(/usr/local/lib/gcc/powerpc-apple-darwin6.6/3.4/../../..) does not 
exist
	ld: can't locate file for: -lfrtbegin
	make: *** [pspline.so] Error 1
	ERROR: compilation failed for package 'pspline'
	** Removing '/Applications/StartR.app/RAqua.app/Contents/pspline'

	$ locate frtbegin
	/sw/lib/libfrtbegin.a

Dan E. Kelley, Associate Professor                phone:(902)494-1694
Oceanography Department, Dalhousie University       fax:(902)494-2885
Halifax, Nova Scotia                         mailto:Dan.Kelley at Dal.CA
Canada B3H 4J1   http://www.phys.ocean.dal.ca/~kelley/Kelley_Dan.html



From ripley at stats.ox.ac.uk  Fri Mar 19 16:11:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 15:11:13 +0000 (GMT)
Subject: [R] for loop or Hmisc library trap.rule function syntax error
In-Reply-To: <405B1A54@webmail.ualberta.ca>
Message-ID: <Pine.LNX.4.44.0403191508300.23498-100000@gannet.stats>

Try counting parentheses: they don't match.  A good editor (e.g. R mode in 
Emacs) would make this instantly obvious.

I doubt if data.df(subj,2:7) is what you want, either: perhaps 
data.df[subj, 2:7].  There is much more in that vain.

On Fri, 19 Mar 2004, Neil Skjodt wrote:

> Hello:
> 
> I am new R user stumped why the R code after this paragraph generates "Error: 
> syntax error" messages after each of the last 2 lines. I have tried searching 
> the manuals, Hmisc documentation, contributed manuals, help archives, and 
> Internet. I am running R 1.7.1 under Windows 2000 (I will upgrade when my 
> imminent OS upgrade happens). My data was successfully entered and displayed 
> as data.df whose first row is column labels, whose subsequent rows are 
> separate subject results, whose 2nd to 7th columns are numeric control results 
> (for observations at 0, 30, 60, 90, 120, and 180 min), and whose 8th to 13th 
> columns are numeric treatment results. I am trying to enter the control and 
> test area under the curve values from the Hmisc trap.rule function into 
> "control" and "test" for hypothesis testing. I tried posting this last PM, but 
> my message seems to have been lost. I apologize if this appears as a duplicate 
> message. Thanks. Neil
> 
> ...
> control = c();		 
> test = c();
> 
> for (subj in 1:11) {
> 
>   rownum = subj+1	  # add one to subject as first row is column labels
>   control(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,2:7))
>   test(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,8:13))
> }
> ...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bob.ohara at helsinki.fi  Fri Mar 19 16:25:31 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Fri, 19 Mar 2004 17:25:31 +0200
Subject: [R] Odd behaviour of step (and stepAIC)?
Message-ID: <405B10EB.6000201@helsinki.fi>

I can only assume I'm betraying my ignorance here, but this is not what 
I would expect.

I'm getting the following from a stepwise selection (with both step and 
stepAIC):

 > step(lm(sqrt(Grids)~ SE + Edge + NH), scope=~ (Edge + SE + NH)^2)
Start:  AIC= 593.56
  sqrt(Grids) ~ SE + Edge + NH

           Df Sum of Sq    RSS    AIC
<none>                 2147.0  593.6
+ Edge:NH  1       3.0 2143.9  595.1
+ SE:NH    4      23.2 2123.8  598.4
- NH       1      75.8 2222.8  601.6
- Edge     1     448.7 2595.7  646.4
- SE       4    1033.7 3180.6  699.1


My problem is that the SE:Edge term is not added.  Now, I know that I've 
specified the terms in a different order in the start model and the 
scope, but this shouldn't matter, should it?  Aren't these model 
specifications commutative?

Am I missing something important, or is this just an obscure feature?

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From Jan.Verbesselt at agr.kuleuven.ac.be  Fri Mar 19 16:26:48 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Fri, 19 Mar 2004 16:26:48 +0100
Subject: [R] cor.test() -> p-values may be incorrect due to tie
Message-ID: <1079710008.405b1138aa13a@webmail2.kuleuven.be>

Hi R specialists,

When testing the association between two time series the cor.test gives
the following message...-> p-values may be incorrect due to tie

What does it mean? (it is not described in the help)

Thankx,
Jan


>  cor.test(Origi[,1],Origi[,2], alternative = c("two.sided"),method =
c("spearman"), conf.level = 0.95)

        Spearman's rank correlation rho

data:  Origi[, 1] and Origi[, 2] 
S = 101457, p-value = < 2.2e-16
alternative hypothesis: true rho is not equal to 0 
sample estimates:
      rho 
0.8938577 

Warning message: 
p-values may be incorrect due to ties in: cor.test.default(Origi[, 1],
Origi[, 2], alternative = c("two.sided"),



From andy_liaw at merck.com  Fri Mar 19 16:22:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 19 Mar 2004 10:22:36 -0500
Subject: [R] asp=1 and aspect ratio
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A29@usrymx25.merck.com>

Try par(pty="s").

HTH,
Andy

> From: Robin Hankin
> 
> Hi everyone
> 
> I want a square scatterplot with abline(0,1) going exactly through the
> SW and NE corners.  By "square" I mean that the plotting region is
> exactly square, and that the axis limits are identical.
> 
> x <- 1:20
> y <- x+rep(c(-1,1),10)
> lims <- range(c(x,y))
> 
> None of the following do this:
> 
> plot(x,y) ; abline(0,1)         #not square
> plot(x,y,asp=1);abline(0,1)     #diagonal line misses corners
> 
> plot(x,y,asp=1,xaxs="i",yaxs="i");abline(0,1)
> #diagonal line misses corners
> 
> plot(x,y,xaxs="i",yaxs="i");abline(0,1)
> #not square and diag misses corners
> 
> plot(x,y,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square
> 
> plot(x,y,asp=1,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square (plotting region about 8cm by 9.5cm)
> 
> 
> 
> If I wrap the above lines in
> 
> postscript(file="foo.ps", width=5,height=5)
> [...snip...]
> dev.off()
> 
> then still none of the plots is exactly right [either one or both
> corners are missed, or the aspect ratio off].  What am I missing?
> 
> 
> 
> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> SO14 3ZH
> tel +44(0)23-8059-7743
> initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam 
> precaution)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.



From spencer.graves at pdf.com  Fri Mar 19 16:32:39 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Mar 2004 07:32:39 -0800
Subject: [R] asp=1 and aspect ratio
In-Reply-To: <a06002003bc80bca7ab38@[139.166.242.29]>
References: <a06002003bc80bca7ab38@[139.166.242.29]>
Message-ID: <405B1297.6080008@pdf.com>

plot(x,y,asp=1, axes=F, bty="n");abline(0,1) #diagonal line misses corners
axis(2, pos=0)
axis(1, pos=0)

      hope this helps.  spencer graves

Robin Hankin wrote:

>
> Hi everyone
>
> I want a square scatterplot with abline(0,1) going exactly through the
> SW and NE corners.  By "square" I mean that the plotting region is
> exactly square, and that the axis limits are identical.
>
> x <- 1:20
> y <- x+rep(c(-1,1),10)
> lims <- range(c(x,y))
>
> None of the following do this:
>
> plot(x,y) ; abline(0,1)         #not square
> plot(x,y,asp=1);abline(0,1)     #diagonal line misses corners
>
> plot(x,y,asp=1,xaxs="i",yaxs="i");abline(0,1)
> #diagonal line misses corners
>
> plot(x,y,xaxs="i",yaxs="i");abline(0,1)
> #not square and diag misses corners
>
> plot(x,y,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square
>
> plot(x,y,asp=1,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square (plotting region about 8cm by 9.5cm)
>
>
>
> If I wrap the above lines in
>
> postscript(file="foo.ps", width=5,height=5)
> [...snip...]
> dev.off()
>
> then still none of the plots is exactly right [either one or both
> corners are missed, or the aspect ratio off].  What am I missing?
>
>
>
>



From sundar.dorai-raj at pdf.com  Fri Mar 19 16:33:32 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 19 Mar 2004 09:33:32 -0600
Subject: [R] asp=1 and aspect ratio
In-Reply-To: <a06002003bc80bca7ab38@[139.166.242.29]>
References: <a06002003bc80bca7ab38@[139.166.242.29]>
Message-ID: <405B12CC.7030308@pdf.com>



Robin Hankin wrote:
> 
> Hi everyone
> 
> I want a square scatterplot with abline(0,1) going exactly through the
> SW and NE corners.  By "square" I mean that the plotting region is
> exactly square, and that the axis limits are identical.
> 
> x <- 1:20
> y <- x+rep(c(-1,1),10)
> lims <- range(c(x,y))
> 
> None of the following do this:
> 
> plot(x,y) ; abline(0,1)         #not square
> plot(x,y,asp=1);abline(0,1)     #diagonal line misses corners
> 
> plot(x,y,asp=1,xaxs="i",yaxs="i");abline(0,1)
> #diagonal line misses corners
> 
> plot(x,y,xaxs="i",yaxs="i");abline(0,1)
> #not square and diag misses corners
> 
> plot(x,y,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square
> 
> plot(x,y,asp=1,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square (plotting region about 8cm by 9.5cm)
> 
> 
> 
> If I wrap the above lines in
> 
> postscript(file="foo.ps", width=5,height=5)
> [...snip...]
> dev.off()
> 
> then still none of the plots is exactly right [either one or both
> corners are missed, or the aspect ratio off].  What am I missing?
> 
> 

But the margins are all different which is throwing you off. How about:

par(mar = c(4, 4, 4, 4))
plot(x, y, asp = 1)
abline(0, 1)

HTH
-sundar



From ripley at stats.ox.ac.uk  Fri Mar 19 16:38:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Mar 2004 15:38:22 +0000 (GMT)
Subject: [R] asp=1 and aspect ratio
In-Reply-To: <a06002003bc80bca7ab38@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0403191529430.23553-100000@gannet.stats>

par(pty="s") is designed to set square plotting regions.  (That is in `An 
Introduction to R', as well as used frequently in MASS.)

plot(0:21, 0:21, type="n", xlab="x", ylab="x")
points(x,y)
abline(0,1)

is one way to do it thereafter, as well as

plot(x,y,xlim=lims,ylim=lims);abline(0,1)


On Fri, 19 Mar 2004, Robin Hankin wrote:

> 
> Hi everyone
> 
> I want a square scatterplot with abline(0,1) going exactly through the
> SW and NE corners.  By "square" I mean that the plotting region is
> exactly square, and that the axis limits are identical.
> 
> x <- 1:20
> y <- x+rep(c(-1,1),10)
> lims <- range(c(x,y))
> 
> None of the following do this:
> 
> plot(x,y) ; abline(0,1)         #not square
> plot(x,y,asp=1);abline(0,1)     #diagonal line misses corners
> 
> plot(x,y,asp=1,xaxs="i",yaxs="i");abline(0,1)
> #diagonal line misses corners
> 
> plot(x,y,xaxs="i",yaxs="i");abline(0,1)
> #not square and diag misses corners
> 
> plot(x,y,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square
> 
> plot(x,y,asp=1,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> #not square (plotting region about 8cm by 9.5cm)
> 
> 
> 
> If I wrap the above lines in
> 
> postscript(file="foo.ps", width=5,height=5)
> [...snip...]
> dev.off()
> 
> then still none of the plots is exactly right [either one or both
> corners are missed, or the aspect ratio off].  What am I missing?
> 
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Fri Mar 19 16:39:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 19 Mar 2004 10:39:21 -0500
Subject: [R] for loop or Hmisc library trap.rule function syntax error
In-Reply-To: <20040319095019.4f7d8b11.feh3k@spamcop.net>
Message-ID: <20040319153921.UQCJ9779.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Neil,

In addition to Frank's observations, it's not a good idea to "grow" vectors
(here, control and test) in a loop. Doing so causes the vectors to be copied
over and over. While that doesn't make much difference for vectors with 11
elements, it can make a big difference in a large problem. It's better to
allocate the vector initially [e.g., control <- rep(0, 11)] and then replace
elements in the loop.

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Frank 
> E Harrell Jr
> Sent: Friday, March 19, 2004 9:50 AM
> To: Neil Skjodt
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] for loop or Hmisc library trap.rule function 
> syntax error
> 
> On Fri, 19 Mar 2004 07:49:26 -0700
> Neil Skjodt <neil.skjodt at ualberta.ca> wrote:
> 
> > Hello:
> > 
> > I am new R user stumped why the R code after this paragraph 
> generates
> > "Error: syntax error" messages after each of the last 2 
> lines. I have 
> > tried searching the manuals, Hmisc documentation, 
> contributed manuals, 
> > help archives, and Internet. I am running R 1.7.1 under 
> Windows 2000 
> > (I will upgrade when my imminent OS upgrade happens). My data was 
> > successfully entered and displayed as data.df whose first row is 
> > column labels, whose subsequent rows are separate subject results, 
> > whose 2nd to 7th columns are numeric control results (for 
> observations 
> > at 0, 30, 60, 90, 120, and 180 min), and whose 8th to 13th 
> columns are 
> > numeric treatment results. I am trying to enter the control 
> and test 
> > area under the curve values from the Hmisc trap.rule function into 
> > "control" and "test" for hypothesis testing. I tried 
> posting this last 
> > PM, but my message seems to have been lost. I apologize if this 
> > appears as a duplicate message. Thanks. Neil
> > 
> > ...
> > control = c();		 
> > test = c();
> 
> You need to spend time with the manuals.  Remove ; from end 
> of line, set aside vectors of full length, use [ ] for 
> subscripting and subsetting; remove rownum; recognize that 
> first row should not be "column labels".
> 
> Frank Harrell
> 
> > 
> > for (subj in 1:11) {
> > 
> >   rownum = subj+1	  # add one to subject as first row is 
> column labels
> >   control(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,2:7))
> >   test(subj)=trap.rule(c(0,30,60,90,120,180),c(data.df(subj,8:13))
> > }
> 
> ---
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt 
> University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From vaclav.petricek at mff.cuni.cz  Fri Mar 19 16:59:32 2004
From: vaclav.petricek at mff.cuni.cz (Vaclav Petricek)
Date: Fri, 19 Mar 2004 16:59:32 +0100 (CET)
Subject: [R] Draw abbreviated key (lattice, xyplot)
In-Reply-To: <200403181019.29268.deepayan@stat.wisc.edu>
References: <3F5BCBD2-78C3-11D8-B2A8-000A9568DB4C@cih.uib.no>
	<200403180604.15185.andrewr@uidaho.edu>
	<Pine.BSF.4.50.0403181517311.28462-100000@sec.ms.mff.cuni.cz>
	<200403181019.29268.deepayan@stat.wisc.edu>
Message-ID: <Pine.BSF.4.50.0403191632470.82806-100000@sec.ms.mff.cuni.cz>


Deepayan,

> Both auto.key and simpleKey are convenience tools to make key drawing
> easy in 'typical' cases. For more flexibility, define the key as a list
> (the structure is described under 'key' in ?xyplot). You may want to
> look at ?Rows as well, in conjunction with trellis.par.get().
> See ?splom and ?cloud for examples.
>
> You are not very explicit in your description, but perhaps your only
> problem with your second solution is that the first 5 levels are in
> alphabetical order (but you haven't said what order you want them to be
> in). This is an artifact of R's factor() function, where the levels are
> by default
>
>      factor(x, levels = sort(unique.default(x), na.last = TRUE), ...
>
> So your problem may be solved simply by redefining your country variable
> appropriately:
>
> yourdata$country <-
>     factor(as.character(yourdata$country),
>            levels = c('USA', 'Germany', < ... >))
>
> (the as.character() is probably redundant)

Thank you very much - this solves my problem. Putting the countries of
interest first has two benefits
  a) different colors are assigned to them
  b) including just first n of them in the key using simpleKey is easy

As I am more used a little different notation to construct the levels in
the order I want.

t<-transform(t,country=factor(country,
       levels=unique(c(subset(country,year==1997),unique(country)))))

where

  subset(country,year==1997) are the countries I want to be first
  unique(country) after that ensures that I include all the countries and
                  no NAs are introduced

This is how I plot the abbreviated key then

attach(t)
xyplot(papers~year, groups=country, type='l',
       key=simpleKey(levels(country)[1:10],columns=2,lines=T,points=F))

Thanks again,

Vaclav


> Hope that helps,
>
> Deepayan
>
> On Thursday 18 March 2004 09:42, Vaclav Petricek wrote:
> > Hello
> >
> > I have been experimenting with xyplot, reading the help and googling
> > but I am still unable to draw an abbreviated key.
> >
> > I would like to display key for a few particular countries.
> >
> > My dataset looks like this
> >
> >    year papers        country papers.total
> > 1  1988    403            USA          551
> > 2  1988     31 United Kingdom          551
> > 3  1988     24         Canada          551
> > 4  1988     20    Netherlands          551
> > 5  1988     19         Israel          551
> > 6  1988     16        Germany          551
> > 7  1988     13         France          551
> > 8  1988     10          Italy          551
> > 9  1988      8    Switzerland          551
> > 10 1988      5          Japan          551
> > 11 1988      5        Denmark          551
> > 12 1988      3          Spain          551
> > 13 1988      3         Russia          551
> > 14 1988      2         Sweden          551
> > 15 1988      2         Poland          551
> > 16 1988      2        Finland          551
> > 17 1988      2         Brasil          551
> > 18 1988      2      Australia          551
> > 19 1988      1       Thailand          551
> > 20 1988      1         Norway          551
> > 21 1988      1         Mexico          551
> > 22 1988      1        Ireland          551
> > 23 1988      1         Greece          551
> > 24 1989    649            USA          926
> > 25 1989     53 United Kingdom          926
> > 26 1989     43         France          926
> > 27 1989     37         Canada          926
> > 28 1989     36        Germany          926
> > 29 1989     28    Netherlands          926
> > 30 1989     19         Sweden          926
> > [...]
> >
> > I use xyplot to show how many papers
> >
> > > xyplot(papers~year,groups=country,type='l',auto.key=T)
> >
> > produces an extremely long key
> >
> > > xyplot(papers~year,groups=country,type='l',key=simpleKey(levels(as.
> > >factor(country))[1:5]))
> >
> > shows *alphabetically* first five countries in the key
> >
> > > xyplot(papers~year,groups=country,type='l',key=simpleKey(c('USA','G
> > >ermany')))
> >
> > displays correct countries but the colors obviously do not match.
> >
> > Could you please point me in the right direction?
> >
> > Vaclav
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>



From MSchwartz at medanalytics.com  Fri Mar 19 17:01:56 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 19 Mar 2004 10:01:56 -0600
Subject: [R] asp=1 and aspect ratio
In-Reply-To: <1079710151.7030.8.camel@localhost.localdomain>
References: <a06002003bc80bca7ab38@[139.166.242.29]>
	<1079710151.7030.8.camel@localhost.localdomain>
Message-ID: <1079712115.7030.32.camel@localhost.localdomain>

On Fri, 2004-03-19 at 09:29, Marc Schwartz wrote:
> On Fri, 2004-03-19 at 09:06, Robin Hankin wrote:
> > Hi everyone
> > 
> > I want a square scatterplot with abline(0,1) going exactly through the
> > SW and NE corners.  By "square" I mean that the plotting region is
> > exactly square, and that the axis limits are identical.
> > 
> > x <- 1:20
> > y <- x+rep(c(-1,1),10)
> > lims <- range(c(x,y))
> > 
> > None of the following do this:
> > 
> > plot(x,y) ; abline(0,1)         #not square
> > plot(x,y,asp=1);abline(0,1)     #diagonal line misses corners
> > 
> > plot(x,y,asp=1,xaxs="i",yaxs="i");abline(0,1)
> > #diagonal line misses corners
> > 
> > plot(x,y,xaxs="i",yaxs="i");abline(0,1)
> > #not square and diag misses corners
> > 
> > plot(x,y,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> > #not square
> > 
> > plot(x,y,asp=1,xlim=lims,ylim=lims,xaxs="i",yaxs="i");abline(0,1)
> > #not square (plotting region about 8cm by 9.5cm)
> > 
> > 
> > 
> > If I wrap the above lines in
> > 
> > postscript(file="foo.ps", width=5,height=5)
> > [...snip...]
> > dev.off()
> > 
> > then still none of the plots is exactly right [either one or both
> > corners are missed, or the aspect ratio off].  What am I missing?
> 
> This works for me:
> 
> x <- 1:20
> y <- x + rep(c(-1, 1), 10)
> lims <- range(c(x, y))
> plot(x, y, xlim = lims, ylim = lims, xaxs = "i", yaxs = "i")
> abline(0, 1)
> 
> # Plot region is square
> >par("usr")
> [1]  0 21  0 21
> 
> Here is a PS plot generated with:
> 
> postscript(width = 5, height = 5)
> plot(x, y, xlim = lims, ylim = lims, xaxs = "i", yaxs = "i")
> abline(0, 1)
> dev.off()
> 
> http://www.MedAnalytics.com/Rplots.ps
> 
> HTH,
> 
> Marc Schwartz


Robin,

Sorry. It just dawned on me that you were not only referring to the line
meeting the corners, but that the plot region itself by measurement was
square, not just that the coordinates of the plot region were square.

The following adjustment will work and I posted a new PS file at the
same URL:

# Set the plot region to be square
par(pty = "s")

plot(x, y, xlim = lims, ylim = lims, xaxs = "i", yaxs = "i")
abline(0, 1)

HTH,

Marc Schwartz



From Dan.Kelley at Dal.Ca  Fri Mar 19 17:04:37 2004
From: Dan.Kelley at Dal.Ca (Dan Kelley)
Date: Fri, 19 Mar 2004 12:04:37 -0400
Subject: Fwd: [R] osx/fink: cannot do "R INSTALL" (library mixup)
Message-ID: <1F4EF4C7-79BF-11D8-81E4-000A95A7B4EC@Dal.Ca>

Begin forwarded message:

> From: Dan Kelley <Dan.Kelley at Dal.Ca>
> Date: March 19, 2004 12:00:01 PM AST
> To: Don MacQueen <macq at llnl.gov>
> Subject: Re: [R] osx/fink: cannot do "R INSTALL" (library mixup)
>
> That works perfectly!  THanks.  I did
>   524  export PKG_LIBS="-L/usr/local/lib -L/sw/lib"
>   525  R INSTALL pspline
> and all is fine.  Cheers!
>
>
> On Mar 19, 2004, at 11:51 AM, Don MacQueen wrote:
>
>> Did you install the so-called "Developer Tools"? Or perhaps it's more 
>> correctly called "XTools" in 10.3 (Panther).
>>
>> You may have learned about this already, but perhaps not since you 
>> just switched. A default OS X installation does not include all of 
>> the resources needed for code development. I've kind of lost track of 
>> the exact details of what's included in a default installation and 
>> what's not; I find it easier just to install everything.
>>
>> Note that on my system (which is still 10.2.8):
>>    [161]% locate frtbegin
>>    /usr/local/lib/libfrtbegin.a
>> As I understand it, the folks who maintain RAqua binaries for OS X 
>> tend to not want to use fink, so they suggest getting additional 
>> resources from other sources. Evidently I did that.
>>
>>
>> The R documentation is pretty good about installation options. Here 
>> are a few pointers.
>>
>> R INSTALL --help
>>
>> You can set env vars before running R INSTALL, for example:
>>      setenv PKG_CPPFLAGS "-I/usr/local/mysql/include"
>>      setenv PKG_LIBS "-L/usr/local/mysql/lib -lmysqlclient"
>> (csh syntax)
>>
>> -Don
>>
>>> Hello friends.  I've just switched from linux to osx, and I'm liking 
>>> it, except now that I'm getting down to brass tacks, and needing to 
>>> install some R packages, I'm at a roadblock.  I'm hoping someone can 
>>> help.
>>>
>>> My R is installed from a .dmg, and resides as follows:
>>> 	$ which R
>>> 	/usr/local/bin/R
>>> The problem is that I cannot build packages.  For example, see the 
>>> below (which also indicates that libfrtbegin.a is in the
>>> 	/sw
>>> directory, i.e. that it was installed by the FINK setup).  Does 
>>> anybody know how I can solve this problem, e.g. how to alter the
>>> 	-L(directory)
>>>  commands that are employed?
>>>
>>> (Aside: I looked for a solution on the R archives.  Also, I've tried 
>>> to make my subject-line clear enough that an answer will benefit 
>>> other osx/fink users.  Thanks!!)
>>>
>>> 	$ R INSTALL pspline
>>> 	* Installing *source* package 'pspline' ...
>>> 	** libs
>>> 	gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
>>> pspline.so Pspline.o  -L/usr/local/lib 
>>> -L/usr/local/lib/gcc/powerpc-	apple-darwin6.6/3.4 
>>> -L/usr/local/lib/gcc/powerpc-apple-darwin6.6/3.4/../../.. -lfrtbegin 
>>> -lg2c -lSystem -lcc_dynamic
>>> 	ld: warning -L: directory name 
>>> (/usr/local/lib/gcc/powerpc-apple-darwin6.6/3.4) does not exist
>>> 	ld: warning -L: directory name 
>>> (/usr/local/lib/gcc/powerpc-apple-darwin6.6/3.4/../../..) does not 
>>> exist
>>> 	ld: can't locate file for: -lfrtbegin
>>> 	make: *** [pspline.so] Error 1
>>> 	ERROR: compilation failed for package 'pspline'
>>> 	** Removing '/Applications/StartR.app/RAqua.app/Contents/pspline'
>>>
>>> 	$ locate frtbegin
>>> 	/sw/lib/libfrtbegin.a
>>>
>>> Dan E. Kelley, Associate Professor                phone:(902)494-1694
>>> Oceanography Department, Dalhousie University       fax:(902)494-2885
>>> Halifax, Nova Scotia                         mailto:Dan.Kelley at Dal.CA
>>> Canada B3H 4J1   http://www.phys.ocean.dal.ca/~kelley/Kelley_Dan.html
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>> -- 
>> --------------------------------------
>> Don MacQueen
>> Environmental Protection Department
>> Lawrence Livermore National Laboratory
>> Livermore, CA, USA
>> --------------------------------------
>>
> Dan E. Kelley, Associate Professor                phone:(902)494-1694
> Oceanography Department, Dalhousie University       fax:(902)494-2885
> Halifax, Nova Scotia                         mailto:Dan.Kelley at Dal.CA
> Canada B3H 4J1   http://www.phys.ocean.dal.ca/~kelley/Kelley_Dan.html
>
Dan E. Kelley, Associate Professor                phone:(902)494-1694
Oceanography Department, Dalhousie University       fax:(902)494-2885
Halifax, Nova Scotia                         mailto:Dan.Kelley at Dal.CA
Canada B3H 4J1   http://www.phys.ocean.dal.ca/~kelley/Kelley_Dan.html



From p.dalgaard at biostat.ku.dk  Fri Mar 19 17:19:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Mar 2004 17:19:49 +0100
Subject: [R] Odd behaviour of step (and stepAIC)?
In-Reply-To: <405B10EB.6000201@helsinki.fi>
References: <405B10EB.6000201@helsinki.fi>
Message-ID: <x2k71gg66y.fsf@biostat.ku.dk>

"Anon." <bob.ohara at helsinki.fi> writes:

> I can only assume I'm betraying my ignorance here, but this is not
> what I would expect.
> 
> I'm getting the following from a stepwise selection (with both step
> and stepAIC):
> 
>  > step(lm(sqrt(Grids)~ SE + Edge + NH), scope=~ (Edge + SE + NH)^2)
> Start:  AIC= 593.56
>   sqrt(Grids) ~ SE + Edge + NH
> 
>            Df Sum of Sq    RSS    AIC
> <none>                 2147.0  593.6
> + Edge:NH  1       3.0 2143.9  595.1
> + SE:NH    4      23.2 2123.8  598.4
> - NH       1      75.8 2222.8  601.6
> - Edge     1     448.7 2595.7  646.4
> - SE       4    1033.7 3180.6  699.1
> 
> 
> My problem is that the SE:Edge term is not added.  Now, I know that
> I've specified the terms in a different order in the start model and
> the scope, but this shouldn't matter, should it?  Aren't these model
> specifications commutative?
> 
> Am I missing something important, or is this just an obscure feature?

Looks a bit odd, but the first thing I would do is to check whether
the model with that term added actually makes sense:

m1 <- lm(sqrt(Grids)~ SE + Edge + NH)
m2 <- update(m1,~.+SE:Edge)
anova(m1,m2)

My hunch is that you'l find zero DF between the two models, but try it
and see.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Mar 19 17:30:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Mar 2004 17:30:49 +0100
Subject: [R] cor.test() -> p-values may be incorrect due to tie
In-Reply-To: <1079710008.405b1138aa13a@webmail2.kuleuven.be>
References: <1079710008.405b1138aa13a@webmail2.kuleuven.be>
Message-ID: <x2fzc4g5om.fsf@biostat.ku.dk>

Jan Verbesselt <Jan.Verbesselt at agr.kuleuven.ac.be> writes:

> Hi R specialists,
> 
> When testing the association between two time series the cor.test gives
> the following message...-> p-values may be incorrect due to tie
> 
> What does it mean? (it is not described in the help)

It means what it says... The p-values in the test for rho=0 is based
on the assumption that the ranks are 1:n for both variables. In the
presence of ties (multiple x or y having the same value) we calculate
a modified rho, but we still use the same formula for the p-value. 

There are really two issues: there's a nice theory that allows you to
calculate the exact p-value when ties are absent. This becomes much
harder when there are ties. However, there's also an asymptotic
approximation to a normal distribution, and I believe that that would
actually be rather easy to compute in the tied cases, but we don't do
that either. 

I don't think you have anything to worry about with the example you
provide, though.

> >  cor.test(Origi[,1],Origi[,2], alternative = c("two.sided"),method =
> c("spearman"), conf.level = 0.95)
> 
>         Spearman's rank correlation rho
> 
> data:  Origi[, 1] and Origi[, 2] 
> S = 101457, p-value = < 2.2e-16
> alternative hypothesis: true rho is not equal to 0 
> sample estimates:
>       rho 
> 0.8938577 
> 
> Warning message: 
> p-values may be incorrect due to ties in: cor.test.default(Origi[, 1],
> Origi[, 2], alternative = c("two.sided"),

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From giampi at speech.kth.se  Fri Mar 19 17:31:56 2004
From: giampi at speech.kth.se (Giampiero Salvi)
Date: Fri, 19 Mar 2004 17:31:56 +0100 (CET)
Subject: [R] how to fix a factor
Message-ID: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>

Hi all,
I created a data frame with three factors, plus the response that
looks like this:

x1	x2	x3	y
a	1	1	0.3
a	2	1	0.1
b	1	1	0.4
c	4	3	0.1
...

I would like to analise the effect of two of them, keeping the third fixed
(I already know the effect of the last). The reason why I don't create several
data frames for each value of the thirs factor is simply convenience (I'd like
to be able to decide which factor I want to rule out)

for example I'd like to write something like

boxplot(y ~ x1 + x2, x3 == 1)

which of course doesn't work, otherwise I wouldn't write :-)

Is this possible and how?

Thank you
Giampiero



From oliver at imcs.marine.rutgers.edu  Fri Mar 19 17:46:25 2004
From: oliver at imcs.marine.rutgers.edu (Matthew Oliver)
Date: Fri, 19 Mar 2004 11:46:25 -0500 (EST)
Subject: [R] BLAST output parsing
Message-ID: <200403191646.i2JGkPCh017442@imcs.marine.rutgers.edu>

Hi All,

I have use the BLAST program provided by NCBI to compare large databases and have 
generated a lot of output in one long text file. I was looking for a way to parse 
out the results in a nice way so that it could be easily visualized. I was thinking 
of trying to write something in R, but I wanted to check in with the help group to 
make sure I am not re-inventing the wheel. Any help would be great!

Matt Oliver



From robert.kissell at citigroup.com  Fri Mar 19 17:47:10 2004
From: robert.kissell at citigroup.com (Kissell, Robert [EQRE])
Date: Fri, 19 Mar 2004 11:47:10 -0500
Subject: [R] Reading Data
Message-ID: <4115749EFC8862458D6FE4F04F5F7DE701E82F32@EXCHNY37.ny.ssmb.com>

Hi,

Quick question on reading data.

I am running simulations but want to allow the user the option to define the number of simulations. How can I have R read-in user data entered from the keyboard? Is there a difference for reading in numeric and character data?

For example, I am trying to write the following in R:

Enter Number of Iterations?
<<<the user then enters a number say Y >>>

X <- Y       # i then want to read that number into X


Thanks.



From ccleland at optonline.net  Fri Mar 19 17:56:56 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 19 Mar 2004 11:56:56 -0500
Subject: [R] how to fix a factor
In-Reply-To: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>
References: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>
Message-ID: <405B2658.10709@optonline.net>

Giampiero Salvi wrote:
> I created a data frame with three factors, plus the response that
> looks like this:
> 
> x1	x2	x3	y
> a	1	1	0.3
> a	2	1	0.1
> b	1	1	0.4
> c	4	3	0.1
> ...
> 
> I would like to analise the effect of two of them, keeping the third fixed
> (I already know the effect of the last). The reason why I don't create several
> data frames for each value of the thirs factor is simply convenience (I'd like
> to be able to decide which factor I want to rule out)

boxplot(y ~ x1 + x2, data = mydata[mydata$x3==1,])

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From andrewr at uidaho.edu  Fri Mar 19 18:04:21 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 19 Mar 2004 09:04:21 -0800
Subject: [R] how to fix a factor
In-Reply-To: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>
References: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>
Message-ID: <200403190904.21909.andrewr@uidaho.edu>

Giampiero,

It's certainly possible.  

Many commands allow the "subset" option, which permits the analysis of only a 
portion of a dataframe, although boxplot does not.  

Also, you could preface your analysis with a logical condition, like 

my sample <- x3 == 1
boxplot(y[my sample] ~ x1[my sample] + x2[my sample])

You can wrap the preceding code in a for loop to apply it to each level of the 
factor.

Frank Harrell's useful mapply() will allow you to vectorize over more than one 
argument.  Look for it in the package Hmisc on CRAN.

Andrew

On Friday 19 March 2004 08:31, Giampiero Salvi wrote:
> Hi all,
> I created a data frame with three factors, plus the response that
> looks like this:
>
> x1	x2	x3	y
> a	1	1	0.3
> a	2	1	0.1
> b	1	1	0.4
> c	4	3	0.1
> ...
>
> I would like to analise the effect of two of them, keeping the third fixed
> (I already know the effect of the last). The reason why I don't create
> several data frames for each value of the thirs factor is simply
> convenience (I'd like to be able to decide which factor I want to rule out)
>
> for example I'd like to write something like
>
> boxplot(y ~ x1 + x2, x3 == 1)
>
> which of course doesn't work, otherwise I wouldn't write :-)
>
> Is this possible and how?
>
> Thank you
> Giampiero
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From vaclav.petricek at mff.cuni.cz  Fri Mar 19 18:01:41 2004
From: vaclav.petricek at mff.cuni.cz (Vaclav Petricek)
Date: Fri, 19 Mar 2004 18:01:41 +0100 (CET)
Subject: [R] how to fix a factor
In-Reply-To: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>
References: <Pine.LNX.4.58.0403191722340.4545@bayes.speech.kth.se>
Message-ID: <Pine.BSF.4.50.0403191800300.88181-100000@sec.ms.mff.cuni.cz>

On Fri, 19 Mar 2004, Giampiero Salvi wrote:

> Hi all,
> I created a data frame with three factors, plus the response that
> looks like this:
>
> x1	x2	x3	y
> a	1	1	0.3
> a	2	1	0.1
> b	1	1	0.4
> c	4	3	0.1
> ...
>
> I would like to analise the effect of two of them, keeping the third fixed
> (I already know the effect of the last). The reason why I don't create several
> data frames for each value of the thirs factor is simply convenience (I'd like
> to be able to decide which factor I want to rule out)
>
> for example I'd like to write something like
>
> boxplot(y ~ x1 + x2, x3 == 1)
>
> which of course doesn't work, otherwise I wouldn't write :-)
>
> Is this possible and how?

?boxplot

The subset parameter of boxplot might be what you are looking for.

Vaclav



From aiminy at iastate.edu  Fri Mar 19 18:08:41 2004
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 19 Mar 2004 11:08:41 -0600
Subject: [R] (no subject)
Message-ID: <6.0.1.1.2.20040319105944.01d88ea0@aiminy.mail.iastate.edu>

How to use "contour" function?
I type "contour(th1,th2,SumofSquares,levels=c(seq(1000,4000,200)))"

But I didn't see contour in the plot, I guess it is because my data is so 
big that contour is out of range.
Does anybody know how to fix this problem?


here th1 is a 1x101 array(158741.8-298529.6)
th2 is another 1X101 array(-0.0058352269-0.2093140935)

SumofSquares is a 101X101 matrix
here is 1st row of this matrix:
SumofSquares[1,]
   [1] 58276823733 45108959063 36097139203 29640961600 24852628699 
21205691351 18370693380 16132073683 14343415033
  [10] 12902043760 11733956375 10784525542 
10012575486  9386493133  8881608427  8478388277  8161165261  7917225781
  [19]  7736144795  7609292848  7529465556  7490601474  7487564677 
7515975345  7572076406  7652627560  7754820315
  [28]  7876209323  8014656457  8168284935  8335441457  8514664731 
8704659189  8904272893  9112478876  9328359317
  [37]  9551092042  9779938969 10014236185 10253385367 10496846372 
10744130780 10994796282 11248441758 11504702968
  [46] 11763248764 12023777744 12286015297 12549710985 12814636220 
13080582187 13347358003 13614789066 13882715573
  [55] 14150991196 14419481894 14688064834 14956627427 15225066452 
15493287270 15761203103 16028734388 16295808187
  [64] 16562357650 16828321525 17093643719 17358272883 17622162049 
17885268285 18147552386 18408978592 18669514322
  [73] 18929129935 19187798513 19445495656 19702199292 19957889512 
20212548406 20466159917 20718709708 20970185038
  [82] 21220574642 21469868628 21718058379 21965136456 22211096520 
22455933247 22699642261 22942220059 23183663956
  [91] 23423972019 23663143017 23901176371 24138072104 24373830797 
24608453551 24841941947 25074298011 25305524181
[100] 25535623276 25764598466



thanks,
Aimin Yan



From dmurdoch at pair.com  Fri Mar 19 18:20:33 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 19 Mar 2004 12:20:33 -0500
Subject: [R] BLAST output parsing
In-Reply-To: <200403191646.i2JGkPCh017442@imcs.marine.rutgers.edu>
References: <200403191646.i2JGkPCh017442@imcs.marine.rutgers.edu>
Message-ID: <1uam50h74hjdc7tc5pod9fbtc3fdjfsjp6@4ax.com>

On Fri, 19 Mar 2004 11:46:25 -0500 (EST), Matthew Oliver
<oliver at imcs.marine.rutgers.edu> wrote :

>Hi All,
>
>I have use the BLAST program provided by NCBI to compare large databases and have 
>generated a lot of output in one long text file. I was looking for a way to parse 
>out the results in a nice way so that it could be easily visualized. I was thinking 
>of trying to write something in R, but I wanted to check in with the help group to 
>make sure I am not re-inventing the wheel. Any help would be great!

I don't know if this is there or not, but the Bioconductor project
(bioconductor.org) is a likely place to look for something like that.

Duncan



From dmurdoch at pair.com  Fri Mar 19 18:25:10 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 19 Mar 2004 12:25:10 -0500
Subject: [R] Reading Data
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82F32@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F32@EXCHNY37.ny.ssmb.com>
Message-ID: <96bm501t5bqaj4ukslpb7cvbas57qn3cej@4ax.com>

On Fri, 19 Mar 2004 11:47:10 -0500, "Kissell, Robert [EQRE]"
<robert.kissell at citigroup.com> wrote :

>Hi,
>
>Quick question on reading data.
>
>I am running simulations but want to allow the user the option to define the number of simulations. How can I have R read-in user data entered from the keyboard? Is there a difference for reading in numeric and character data?
>
>For example, I am trying to write the following in R:
>
>Enter Number of Iterations?
><<<the user then enters a number say Y >>>
>
>X <- Y       # i then want to read that number into X

You want readline.  It returns a single element character vector;
you'll need to convert it to a number.  For example

> Y <- as.numeric(readline('Enter number of iterations:'))
Enter number of iterations:34
> Y
[1] 34



From MSchwartz at medanalytics.com  Fri Mar 19 18:28:24 2004
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 19 Mar 2004 11:28:24 -0600
Subject: [R] Reading Data
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82F32@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F32@EXCHNY37.ny.ssmb.com>
Message-ID: <1079717303.7030.127.camel@localhost.localdomain>

On Fri, 2004-03-19 at 10:47, Kissell, Robert [EQRE] wrote:
> Hi,
> 
> Quick question on reading data.
> 
> I am running simulations but want to allow the user the option to
> define the number of simulations. How can I have R read-in user data
> entered from the keyboard? Is there a difference for reading in
> numeric and character data?
> 
> For example, I am trying to write the following in R:
> 
> Enter Number of Iterations?
> <<<the user then enters a number say Y >>>
> 
> X <- Y       # i then want to read that number into X
> 
> 
> Thanks.


See ?readline, which returns a character vector by default. Thus you
would need to convert the returned value to numeric:

#This returns "5" as a character
> n <- readline("Enter Number of Iterations? ")
Enter Number of Iterations? 5
> n
[1] "5"

# This returns 5 as a number
> n <- as.numeric(readline("Enter Number of Iterations? "))
Enter Number of Iterations? 5
> n
[1] 5


HTH,

Marc Schwartz



From edd at debian.org  Fri Mar 19 18:39:10 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 19 Mar 2004 11:39:10 -0600
Subject: [R] 1.8.1 on Debian stable
In-Reply-To: <405AD36B.26919.81B501C@localhost>
References: <405AD36B.26919.81B501C@localhost>
Message-ID: <20040319173910.GA17704@sonny.eddelbuettel.com>

On Fri, Mar 19, 2004 at 11:03:07AM -0000, Chris Evans wrote:
> This has probably been addressed here, but I can't find it if it has, 
> and it may be up on the R project homepage or the CRAN pages, but if 
> so ....
> 
> ... so sorry if this has been answered before: question is, can I 
> upgrade to 1.8.1 under Debian stable (a.k.a. "Woody")?  The deb 
> packages in the CRAN archive are clearly for 1.8.0 and those in the 
> "unstable" distro for Debian are 1.8.1.  Therefore I assume there are 

The CRAN backports are contributed by volunteers. If nobody contributed such
a package, and you cannot use testing [1], you are left with compiling it
yourself.

> dependency problems to solve that didn't make it worth making 1.8.1 
> available for Woody and if the package maintainers don't want to try, 

I think you misunderstand what 'stable' means: No new packages. 

Hth, Dirk 

[1] You should really add testing to your /etc/apt/sources.list and give it
a default 'pin' of -1 so that it would never install from it -- unless you
ask for it explicitly.  Then say 'apt-get -t testing install r-base' and it
will install r-base from testing, i.e. 1.8.1, with just its required
libraries, leaving the rest of your system alone. As libraries are properly
versioned, this is generally safe, and a trick that has been used by many,
many users.

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From jonathan.williams at pharmacology.oxford.ac.uk  Fri Mar 19 19:12:53 2004
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Fri, 19 Mar 2004 18:12:53 -0000
Subject: [R] How to collect trees grown by rpart
Message-ID: <NGBBKJEMOMLJFCOIEGCEAEPMJKAA.jonathan.williams@pharm.ox.ac.uk>

I would like to collect the trees grown by rpart fits in an array,
in order to be able to use them later to predict new data. I have 
tried to use parse and eval to do this, without success. I'd be 
very grateful if someone could explain how to do it.

The kind of thing I hope to do is:

resmat=array(NA, 100)
for (run in 1:100) resmat[run]=rpart(y~., data=train[run])

So that, later, I can do:
for (run in 1:100) pred[run]=predict(resmat[run], newdat)

But, resmat[run] does not work, even though it saves the cptables. 

Thanks, in advance,

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From sdavis2 at mail.nih.gov  Fri Mar 19 19:03:08 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 19 Mar 2004 13:03:08 -0500
Subject: [R] How to collect trees grown by rpart
Message-ID: <BC80A00C.5EED%sdavis2@mail.nih.gov>

Jonathan,

Try making a list instead of an array.  See ?list.  Also, did you look into
random forests?  I'm not sure what you want to do, but there might be
methods there to do some of the work for you.

Sean

On 3/19/04 1:12 PM, "Jonathan Williams"
<jonathan.williams at pharmacology.oxford.ac.uk> wrote:

> I would like to collect the trees grown by rpart fits in an array,
> in order to be able to use them later to predict new data. I have
> tried to use parse and eval to do this, without success. I'd be
> very grateful if someone could explain how to do it.
> 
> The kind of thing I hope to do is:
> 
> resmat=array(NA, 100)
> for (run in 1:100) resmat[run]=rpart(y~., data=train[run])
> 
> So that, later, I can do:
> for (run in 1:100) pred[run]=predict(resmat[run], newdat)
> 
> But, resmat[run] does not work, even though it saves the cptables.
> 
> Thanks, in advance,
> 
> Jonathan Williams
> OPTIMA
> Radcliffe Infirmary
> Woodstock Road
> OXFORD OX2 6HE
> Tel +1865 (2)24356
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Mar 19 19:18:10 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Mar 2004 19:18:10 +0100
Subject: [R] How to collect trees grown by rpart
In-Reply-To: <NGBBKJEMOMLJFCOIEGCEAEPMJKAA.jonathan.williams@pharm.ox.ac.uk>
References: <NGBBKJEMOMLJFCOIEGCEAEPMJKAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <405B3962.6060602@statistik.uni-dortmund.de>

Jonathan Williams wrote:
> I would like to collect the trees grown by rpart fits in an array,
> in order to be able to use them later to predict new data. I have 
> tried to use parse and eval to do this, without success. I'd be 
> very grateful if someone could explain how to do it.
> 
> The kind of thing I hope to do is:
> 
> resmat=array(NA, 100)
> for (run in 1:100) resmat[run]=rpart(y~., data=train[run])
> 
> So that, later, I can do:
> for (run in 1:100) pred[run]=predict(resmat[run], newdat)
> 
> But, resmat[run] does not work, even though it saves the cptables. 

Use a list:

resmat <- vector(100, mode="list")
for (run in 1:100)
     resmat[[run]] <- rpart(y~., data=train[run])


Uwe Ligges



> Thanks, in advance,
> 
> Jonathan Williams
> OPTIMA
> Radcliffe Infirmary
> Woodstock Road
> OXFORD OX2 6HE
> Tel +1865 (2)24356
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Mar 19 19:41:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 19 Mar 2004 13:41:38 -0500
Subject: [R] How to collect trees grown by rpart
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A2F@usrymx25.merck.com>

Yes, Jonathan has looked at randomForest, but for rpart trees, ipred might
be more appropriate.

Cheers,
Andy

> From:  Sean Davis
> 
> Jonathan,
> 
> Try making a list instead of an array.  See ?list.  Also, did 
> you look into
> random forests?  I'm not sure what you want to do, but there might be
> methods there to do some of the work for you.
> 
> Sean
> 
> On 3/19/04 1:12 PM, "Jonathan Williams"
> <jonathan.williams at pharmacology.oxford.ac.uk> wrote:
> 
> > I would like to collect the trees grown by rpart fits in an array,
> > in order to be able to use them later to predict new data. I have
> > tried to use parse and eval to do this, without success. I'd be
> > very grateful if someone could explain how to do it.
> > 
> > The kind of thing I hope to do is:
> > 
> > resmat=array(NA, 100)
> > for (run in 1:100) resmat[run]=rpart(y~., data=train[run])
> > 
> > So that, later, I can do:
> > for (run in 1:100) pred[run]=predict(resmat[run], newdat)
> > 
> > But, resmat[run] does not work, even though it saves the cptables.
> > 
> > Thanks, in advance,
> > 
> > Jonathan Williams
> > OPTIMA
> > Radcliffe Infirmary
> > Woodstock Road
> > OXFORD OX2 6HE
> > Tel +1865 (2)24356
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.



From B.Rowlingson at lancaster.ac.uk  Fri Mar 19 19:49:14 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 19 Mar 2004 18:49:14 +0000
Subject: [R] (no subject)
In-Reply-To: <6.0.1.1.2.20040319105944.01d88ea0@aiminy.mail.iastate.edu>
References: <6.0.1.1.2.20040319105944.01d88ea0@aiminy.mail.iastate.edu>
Message-ID: <405B40AA.9010700@lancaster.ac.uk>

Aimin Yan wrote:
> How to use "contour" function?
> I type "contour(th1,th2,SumofSquares,levels=c(seq(1000,4000,200)))"
> 
> But I didn't see contour in the plot, I guess it is because my data is 
> so big that contour is out of range.

  It looks like it, given the data you showed. If you leave out the 
'levels' argument then the contour function tries to pick good values 
for the contour lines. Why dont you try that?

  contour(th1,th2,SumOfSquares)

Barry



From tblackw at umich.edu  Fri Mar 19 19:50:53 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Fri, 19 Mar 2004 13:50:53 -0500 (EST)
Subject: [R] BLAST output parsing
In-Reply-To: <200403191646.i2JGkPCh017442@imcs.marine.rutgers.edu>
References: <200403191646.i2JGkPCh017442@imcs.marine.rutgers.edu>
Message-ID: <Pine.SOL.4.58.0403191345070.20605@zektor.gpcc.itd.umich.edu>

Matt  -

Yes, you would be seriously re-inventing the wheel.
A standard tool is the  bioperl SearchIO  package.
This is described at

www.bioperl.org > Docs > SearchIO HOWTO:

The html version of that link evaluates to:

http://bioperl.org/HOWTOs/html/SearchIO.html

-  best  -  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 19 Mar 2004, Matthew Oliver wrote:

> Hi All,
>
> I have use the BLAST program provided by NCBI to compare large databases and have
> generated a lot of output in one long text file. I was looking for a way to parse
> out the results in a nice way so that it could be easily visualized. I was thinking
> of trying to write something in R, but I wanted to check in with the help group to
> make sure I am not re-inventing the wheel. Any help would be great!
>
> Matt Oliver
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From HankeA at mar.dfo-mpo.gc.ca  Fri Mar 19 19:31:14 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Fri, 19 Mar 2004 14:31:14 -0400
Subject: [R] Reading Data
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12496E@msgmarsta01>

The following response by B.Ripley to a similar request may help.
Alex

On Tue, 21 Oct 2003, Ernie Adorio wrote:

> 
> Am using R on a Linux box and am currently writing an interactive R
script.
> 
> 1. How do I ask a user to press any key to continue ? I used a system call
to 
> read but this only works if the Enter key is pressed:
>   print("Press any key to continue")
>   system("read")

You seem over-fond of the system() command!  Try

cat("Press enter key to continue\n")
foo <- readLines(stdin(), 1)

In general R does not have a character-by-character interface with a 
keyboard.

> 2. How do I get a string input from the user? Would like to see an R
function,
> say askget():
> 
>   delay  =  askget("Enter delay in seconds")
>   system(paste( "sleep ", delay))

cat("Enter delay in seconds:\n")
Delay <- scan("", numeric(1))
Sys.sleep(Delay)

will do it (there are other more elegant ways such as

testit <- function()
{
  cat("Enter delay in seconds: ")
  Delay <- as.numeric(readLines(stdin(), 1))
  if(is.na(Delay)) stop("non-numeric input")
  Sys.sleep(Delay)
}

)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----Original Message-----
From: Kissell, Robert [EQRE] [mailto:robert.kissell at citigroup.com] 
Sent: March 19, 2004 12:47 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Reading Data


Hi,

Quick question on reading data.

I am running simulations but want to allow the user the option to define the
number of simulations. How can I have R read-in user data entered from the
keyboard? Is there a difference for reading in numeric and character data?

For example, I am trying to write the following in R:

Enter Number of Iterations?
<<<the user then enters a number say Y >>>

X <- Y       # i then want to read that number into X


Thanks.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Mar 19 20:05:22 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Mar 2004 20:05:22 +0100
Subject: [R] (no subject)
In-Reply-To: <6.0.1.1.2.20040319105944.01d88ea0@aiminy.mail.iastate.edu>
References: <6.0.1.1.2.20040319105944.01d88ea0@aiminy.mail.iastate.edu>
Message-ID: <405B4472.6070003@statistik.uni-dortmund.de>

Aimin Yan wrote:
> How to use "contour" function?
> I type "contour(th1,th2,SumofSquares,levels=c(seq(1000,4000,200)))"
> 
> But I didn't see contour in the plot, I guess it is because my data is 
> so big that contour is out of range.
> Does anybody know how to fix this problem?


By either not specifying "levels", or by specifying it reasonable, i.e. 
values within the range of your matrix "SumofSquares".
Are there values < 4000 in that matrix? You should see a line in that case.

Uwe Ligges



> 
> here th1 is a 1x101 array(158741.8-298529.6)
> th2 is another 1X101 array(-0.0058352269-0.2093140935)
> 
> SumofSquares is a 101X101 matrix
> here is 1st row of this matrix:
> SumofSquares[1,]
>   [1] 58276823733 45108959063 36097139203 29640961600 24852628699 
> 21205691351 18370693380 16132073683 14343415033
>  [10] 12902043760 11733956375 10784525542 10012575486  9386493133  
> 8881608427  8478388277  8161165261  7917225781
>  [19]  7736144795  7609292848  7529465556  7490601474  7487564677 
> 7515975345  7572076406  7652627560  7754820315
>  [28]  7876209323  8014656457  8168284935  8335441457  8514664731 
> 8704659189  8904272893  9112478876  9328359317
>  [37]  9551092042  9779938969 10014236185 10253385367 10496846372 
> 10744130780 10994796282 11248441758 11504702968
>  [46] 11763248764 12023777744 12286015297 12549710985 12814636220 
> 13080582187 13347358003 13614789066 13882715573
>  [55] 14150991196 14419481894 14688064834 14956627427 15225066452 
> 15493287270 15761203103 16028734388 16295808187
>  [64] 16562357650 16828321525 17093643719 17358272883 17622162049 
> 17885268285 18147552386 18408978592 18669514322
>  [73] 18929129935 19187798513 19445495656 19702199292 19957889512 
> 20212548406 20466159917 20718709708 20970185038
>  [82] 21220574642 21469868628 21718058379 21965136456 22211096520 
> 22455933247 22699642261 22942220059 23183663956
>  [91] 23423972019 23663143017 23901176371 24138072104 24373830797 
> 24608453551 24841941947 25074298011 25305524181
> [100] 25535623276 25764598466
> 
> 
> 
> thanks,
> Aimin Yan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From acuster at nature.berkeley.edu  Fri Mar 19 20:08:24 2004
From: acuster at nature.berkeley.edu (Adrian Custer)
Date: Fri, 19 Mar 2004 11:08:24 -0800
Subject: [R] Incomplete Gamma Functions and GammaDistribution Doc errata.
In-Reply-To: <16474.46288.872673.87181@gargle.gargle.HOWL>
References: <1079664474.19469.82.camel@tsetse.lab-net>
	<16474.46288.872673.87181@gargle.gargle.HOWL>
Message-ID: <1079723304.2688.117.camel@tsetse.lab-net>

My appologies to everyone. 

Prof. Ripley, please realize that I did not intend to suggest that your
mathematics were wrong. My question about mathematical correctness
refered to my presention of your formula as an equation. I have great
admiration for your work and its influences on my field and, therefore,
start with the presumption that all errors must be my own.

Mr. Maechler, please understand that I had no intent to "insult" anyone.
I wanted to communicate three things:

        1) to point out what I understood to be an error in the
        documentation (which you both point out to be my own
        misunderstanding of the terminology).
        
        2) to ask for confirmation that I was interpreting past emails
        correctly.
        
        3) to argue for an R implementation of an incomplete gamma
        mathematical function.

To the R community, please accept my appologies for a message which you
may have taken to be insulting. I am deeply thankful for your work, for
your code and for this email list.

--adrian



From scott.rifkin at yale.edu  Fri Mar 19 20:31:22 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Fri, 19 Mar 2004 14:31:22 -0500 (EST)
Subject: [R] lme: simulate.lme in R
Message-ID: <Pine.LNX.4.44.0403191419560.10844-100000@ajax.its.yale.edu>

The goal:  simulate chi square mixture distributions as a way of 
simulating likelihood ratio test statistics for some mixed models where 
the more specific model has some zero variance components (a la Pinheiro 
and Bates pg. 84-87)

The problem: R doesn't have the function ms which is apparently used by 
simulate.lme

In the current version of nlme for R, is there a way around this?  Is it 
possible to use the simulate function?  Is this going to be rectified in 
the (eagerly anticipated) new release of nlme next month?

Thanks,
Scott.rifkin at yale.edu



From bates at stat.wisc.edu  Fri Mar 19 21:01:10 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 19 Mar 2004 14:01:10 -0600
Subject: [R] lme: simulate.lme in R
In-Reply-To: <Pine.LNX.4.44.0403191419560.10844-100000@ajax.its.yale.edu>
References: <Pine.LNX.4.44.0403191419560.10844-100000@ajax.its.yale.edu>
Message-ID: <6rk71gip2x.fsf@bates4.stat.wisc.edu>

Scott Rifkin <scott.rifkin at yale.edu> writes:

> The goal:  simulate chi square mixture distributions as a way of 
> simulating likelihood ratio test statistics for some mixed models where 
> the more specific model has some zero variance components (a la Pinheiro 
> and Bates pg. 84-87)
> 
> The problem: R doesn't have the function ms which is apparently used by 
> simulate.lme
> 
> In the current version of nlme for R, is there a way around this?  Is it 
> possible to use the simulate function?  Is this going to be rectified in 
> the (eagerly anticipated) new release of nlme next month?

You may be able to replace the call to ms with an appropriate call to
optim or to nlm.  I'd prefer not to take the time right now to work
out exactly how that would be done.  I have mentioned several times on
this list that I'm in the process of developing a new and wonderful
implementation of lme and I would prefer to continue working on that
rather than modifying old-style code.

If you want to try to make the modifications yourself I can respond to
questions.
-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From rossini at blindglobe.net  Fri Mar 19 21:20:03 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 19 Mar 2004 12:20:03 -0800
Subject: [R] BLAST output parsing
In-Reply-To: <Pine.SOL.4.58.0403191345070.20605@zektor.gpcc.itd.umich.edu> (Tom
	Blackwell's message of "Fri, 19 Mar 2004 13:50:53 -0500 (EST)")
References: <200403191646.i2JGkPCh017442@imcs.marine.rutgers.edu>
	<Pine.SOL.4.58.0403191345070.20605@zektor.gpcc.itd.umich.edu>
Message-ID: <85vfl0io7g.fsf@servant.blindglobe.net>


One alternative can be found in my under development (slow, and
probably won't be finished) BioSeq1 package; it can be gotten
painfully via:

          http://www.bioconductor.org/develPage/develPackages.html

There are tools for getting BLAST results back and processing them.

best,
-tony

Tom Blackwell <tblackw at umich.edu> writes:

> Matt  -
>
> Yes, you would be seriously re-inventing the wheel.
> A standard tool is the  bioperl SearchIO  package.
> This is described at
>
> www.bioperl.org > Docs > SearchIO HOWTO:
>
> The html version of that link evaluates to:
>
> http://bioperl.org/HOWTOs/html/SearchIO.html
>
> -  best  -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
> On Fri, 19 Mar 2004, Matthew Oliver wrote:
>
>> Hi All,
>>
>> I have use the BLAST program provided by NCBI to compare large databases and have
>> generated a lot of output in one long text file. I was looking for a way to parse
>> out the results in a nice way so that it could be easily visualized. I was thinking
>> of trying to write something in R, but I wanted to check in with the help group to
>> make sure I am not re-inventing the wheel. Any help would be great!
>>
>> Matt Oliver
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From phddas at yahoo.com  Fri Mar 19 22:29:40 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 19 Mar 2004 13:29:40 -0800 (PST)
Subject: [R] loop through files in a dir
Message-ID: <20040319212940.62493.qmail@web20513.mail.yahoo.com>

Hello
I have data in many files in a directory, how can I
loop through the files in a given dir in-order-to
build a data.frame? 

thanks



From jgentry at jimmy.harvard.edu  Fri Mar 19 22:51:32 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Fri, 19 Mar 2004 16:51:32 -0500 (EST)
Subject: [R] loop through files in a dir
In-Reply-To: <20040319212940.62493.qmail@web20513.mail.yahoo.com>
Message-ID: <Pine.SOL.4.20.0403191650390.4918-100000@santiam.dfci.harvard.edu>

> I have data in many files in a directory, how can I
> loop through the files in a given dir in-order-to
> build a data.frame? 

Just use something like 'x <- dir(YOUR_ARGS_HERE)', and then you can do a
loop like:

for (i in x) {
  do something
  do something else
  etc ...
}

-J



From phddas at yahoo.com  Fri Mar 19 23:17:51 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 19 Mar 2004 14:17:51 -0800 (PST)
Subject: [R] loop through files in a dir
In-Reply-To: <Pine.SOL.4.20.0403191650390.4918-100000@santiam.dfci.harvard.edu>
Message-ID: <20040319221751.43921.qmail@web20507.mail.yahoo.com>

d <- dir("c:/data")
for (i in d){
  dt <- read.csv(c("c:/data/",i),header=FALSE)
}
wouldn't work because c("c:/data",i) puts out "string"
"string" where both strings need to be one string. 
how can I combinde both in one string, c:/data/i like
the good old perl "I wish".
thanks



From kimai at princeton.edu  Fri Mar 19 23:29:36 2004
From: kimai at princeton.edu (Kosuke Imai)
Date: Fri, 19 Mar 2004 17:29:36 -0500
Subject: [R] R_qsort_int_I() error
Message-ID: <E757CF16-79F4-11D8-91CB-000A9594660C@Princeton.Edu>

Hi,
   I want to use R_qsort_int_I() in my C function, but getting the 
following error. It looks like there is a conflict between Rmath.h, 
which I use to generate random numbers, and R_ext/Boolean.h I would 
appreciate any help to fix this problem.

gcc -ansi -g -o Gibbs gibbs.c subroutines.o rand.o vector.o -lm -lRmath 
-llapack -lblas -lfrtbegin -lg2c -lm -shared-libgcc
In file included from /usr/include/R_ext/Utils.h:27,
                  from gibbs.c:7:
/usr/include/R_ext/Boolean.h:29: conflicting types for `FALSE'
/usr/include/Rmath.h:175: previous declaration of `FALSE'
/usr/include/R_ext/Boolean.h:29: conflicting types for `TRUE'
/usr/include/Rmath.h:175: previous declaration of `TRUE'
/usr/include/R_ext/Boolean.h:29: confused by earlier errors, bailing out
make: *** [Gibbs] Error 1

Thanks,
Kosuke


---------------------------------------------------------
Kosuke Imai               Office: Corwin Hall 041
Assistant Professor       Phone: 609-258-6601 (Direct)
Department of Politics    Fax: 609-258-1110 (Department)
Princeton University      Email: kimai at Princeton.Edu
Princeton, NJ 08544-1012  http://www.princeton.edu/~kimai



From bates at stat.wisc.edu  Fri Mar 19 23:27:58 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 19 Mar 2004 16:27:58 -0600
Subject: [R] loop through files in a dir
In-Reply-To: <20040319221751.43921.qmail@web20507.mail.yahoo.com>
References: <20040319221751.43921.qmail@web20507.mail.yahoo.com>
Message-ID: <6rlllwh3pt.fsf@bates4.stat.wisc.edu>

"Fred J." <phddas at yahoo.com> writes:

> d <- dir("c:/data")
> for (i in d){
>   dt <- read.csv(c("c:/data/",i),header=FALSE)
> }

> wouldn't work because c("c:/data",i) puts out "string"
> "string" where both strings need to be one string. 
> how can I combinde both in one string, c:/data/i like
> the good old perl "I wish".

file.path("C:/data", i)



From pkleiber at honlab.nmfs.hawaii.edu  Fri Mar 19 23:38:17 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Fri, 19 Mar 2004 12:38:17 -1000
Subject: [R] loop through files in a dir
In-Reply-To: <20040319221751.43921.qmail@web20507.mail.yahoo.com>
References: <20040319221751.43921.qmail@web20507.mail.yahoo.com>
Message-ID: <405B7659.8010506@honlab.nmfs.hawaii.edu>

?paste

Fred J. wrote:
> d <- dir("c:/data")
> for (i in d){
>   dt <- read.csv(c("c:/data/",i),header=FALSE)
> }
> wouldn't work because c("c:/data",i) puts out "string"
> "string" where both strings need to be one string. 
> how can I combinde both in one string, c:/data/i like
> the good old perl "I wish".
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From jg_liao at yahoo.com  Fri Mar 19 23:53:32 2004
From: jg_liao at yahoo.com (Jason Liao)
Date: Fri, 19 Mar 2004 14:53:32 -0800 (PST)
Subject: [R] huge speed up in R 1.90 alpha 
Message-ID: <20040319225332.71868.qmail@web10507.mail.yahoo.com>

I have a microarray analysis program using Monte Carlo Markov chain. I
downloaded R 1.9 alpha and saw FOUR times increase in speed.
Completelyly amazing. Specifically, one iteration takes 44 seconds
under R 1.8.1 and only 10 seconds under R 1.9.0 alpha. I have averaged
over many iterations so hopefully this is something real. The operating
system is Win 2000 on a Pentium III 1.16 Mhs laptop.



=====
Jason G. Liao, Ph.D.
Division of Biometrics
University of Medicine and Dentistry of New Jersey
335 George Street, Suite 2200
New Brunswick, NJ 08903-2688
phone 732-235-5429, fax (732) 235-5464
http://www.geocities.com/jg_liao



From phddas at yahoo.com  Sat Mar 20 00:05:19 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 19 Mar 2004 15:05:19 -0800 (PST)
Subject: [R] loop through files in a dir
In-Reply-To: <6rlllwh3pt.fsf@bates4.stat.wisc.edu>
Message-ID: <20040319230519.4221.qmail@web20503.mail.yahoo.com>

file.path and paste.
thanks 
as a R starter, how could I have found out without
posting a question. is there a way I could have
searched the help files for a keyword or somthing?

thanks



From phddas at yahoo.com  Sat Mar 20 00:47:32 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 19 Mar 2004 15:47:32 -0800 (PST)
Subject: [R] squashing some numbers
Message-ID: <20040319234732.82547.qmail@web20513.mail.yahoo.com>

Hello
I have a data frame with many col.s and rows
V4 V5 V6 V7
3  4   5  6
1  4   4  1
2  4   4  1
4  0   5  1

since the data has the middle 2 rows with V5 and V6
are equal, I need to produce
V4 V5 V6 V7
3  4   5  6
            this line removed and the value V7 = 1 is 
     
            added to the next row V7 valuve making it
2
2  4   4  2
4  0   5  1

thanks a lot



From pkleiber at honlab.nmfs.hawaii.edu  Sat Mar 20 01:32:58 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Fri, 19 Mar 2004 14:32:58 -1000
Subject: [R] loop through files in a dir
In-Reply-To: <20040319230519.4221.qmail@web20503.mail.yahoo.com>
References: <20040319230519.4221.qmail@web20503.mail.yahoo.com>
Message-ID: <405B913A.1030809@honlab.nmfs.hawaii.edu>

 > help.search("string")
      or
 > help.search("concatenate")

would probably have led you to it.



Fred J. wrote:
> file.path and paste.
> thanks 
> as a R starter, how could I have found out without
> posting a question. is there a way I could have
> searched the help files for a keyword or somthing?
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From tplate at blackmesacapital.com  Sat Mar 20 01:51:27 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 19 Mar 2004 17:51:27 -0700
Subject: [R] substitute question
In-Reply-To: <20040319055822.829AB3999@mprdmxin.myway.com>
References: <20040319055822.829AB3999@mprdmxin.myway.com>
Message-ID: <6.0.3.0.2.20040319165812.09122a68@mailhost.blackmesacapital.com>

I don't think it is correct that "there are two types of expressions and 
calls".  There is only one type of these things.  I believe the relevant 
distinction here is between 'call' objects (which can be informally thought 
of as the parse trees of unevaluated R code, and which formally have mode 
'call' in R) and other things like objects of mode 'function', etc.

However, this is all does pretty confusing when using substitute(), 
because, substitute() does go inside objects that have mode 'call', but 
doesn't go inside of objects that have mode 'function' or 
'expression'.  What makes it more confusing is that sometimes 'call' 
objects can be wrapped up in expression objects.  Note that parse(text=) 
returns a 'call' object wrapped in an 'expression' object, whereas quote() 
returns a 'call' object -- I believe that in general it is true that 
parse(text="XXX")[[1]] === quote(XXX).

Earlier in this discussion, Peter Dalgard stated "you can only do 
substitutions on language objects" and then used the function is.language() 
in an example, which I took at that time to imply that substitute() would 
go inside objects for which is.language() returned true.  However, from 
experimenting, it seems that is.call() rather than is.language() is the 
appropriate test.

Here are some simple examples.

 > esub <- function(expr, sublist) do.call("substitute", list(expr, sublist))
 > e1 <- parse(text="a + 1")
 > e2 <- quote(a + 1)
 > e1
expression(a + 1)
 > e2
a + 1
 > mode(e1)
[1] "expression"
 > mode(e2)
[1] "call"
 > identical(e1[[1]], e2)
[1] TRUE
 >
 > # substitute() doesn't go inside e1, even though is.language(e1) is TRUE
 > c(is.language(e1), is.call(e1))
[1]  TRUE FALSE
 > esub(e1, list(a=as.name('b')))
expression(a + 1)
 >
 > c(is.language(e2), is.call(e2))
[1] TRUE TRUE
 > esub(e2, list(a=as.name('b')))
b + 1
 >
 > c(is.language(e1[[1]]), is.call(e1[[1]]))
[1] TRUE TRUE
 > esub(e1[[1]], list(a=as.name('b')))
b + 1
 > identical(e2, e1[[1]])
[1] TRUE
 >
 > ef <- Quote(function() a + 1)
 > f <- function() a + 1
 > c(is.language(ef), is.call(ef))
[1] TRUE TRUE
 > esub(ef, list(a=as.name('b')))
function() b + 1
 > c(is.language(f), is.call(f))
[1] FALSE FALSE
 > esub(f, list(a=as.name('b')))
function ()
a + 1
 > c(is.language(body(f)), is.call(body(f)))
[1] TRUE TRUE
 > esub(body(f), list(a=as.name('b')))
b + 1
 >
 >

I also see that in S-plus 6.2, substitute() behaves differently -- it does 
go inside objects of mode 'call' and 'expression' and substitutes 'b' for 
'a' in every case above.  To run the above code in S-plus, first do:
 > body <- function(f) f[[1]]
 > quote <- Quote

Although there isn't much to guide one in the documentation ?substitute, 
the "R Language manual" does have some discussion of substitute() and 
'expression' objects.

-- Tony Plate

At Thursday 10:58 PM 3/18/2004, Gabor Grothendieck wrote:
>
>Thanks.  Thus it seems that there are two types of expressions and calls:
>
>1. fully expanded
>2. partially expanded
>
>and that fully expanded ones are a prerequisite for substitution.
>body() and quote() produce such fully expanded expressions.
>
>Using a small utility function we can investigate this:
>
>recurse <- function( x, idx = NULL )
>         if ( length( x ) > 0 ) {
>                 for( i in seq( along = x ) )
>                         if (length(x[[i]])>1)
>                                 Recall( x[[i]], c(idx, i))
>                         else {
>                                 if (length(idx)) cat(idx,"")
>                                 cat( i, class(x[[i]]), ":" )
>                                 cat( rep("\t",length(idx) + 2) )
>                                 print( x[[i]] )
>                         }
>         }
>
>f <- function(){a+1}
>
>eb <- body(f)
>class(eb)
>recurse(eb)
>
>eq <- quote(function(){a+1})
>class(eq)
>recurse(eq)
>
>ep <- parse(text=deparse(f))
>class(ep)
>recurse(ep)
>
>
>The output that the above is shown below. It shows that
>body() and quote() produce fully expanded expression style objects
>although body's is of class { and quote is of class call.
>
>However, parse(text=deparse(f)) also produces a fully expanded
>expression style object of class expression yet substitution
>does not occur with that.  Thus full vs. partial expansion is likely
>a necessary but not a sufficient condition.  There is something
>else but I don't know what it is.
>
>
> > f <- function(){a+1}
> >
> > eb <- body(f)
> > class(eb)
>[1] "{"
> > recurse(eb)
>1 name :                `{`
>2 1 name :                      `+`
>2 2 name :                      a
>2 3 numeric :                   [1] 1
> >
> > eq <- quote(function(){a+1})
> > class(eq)
>[1] "call"
> > recurse(eq)  # lines begin with list indices and class name
>1 name :                `function`
>2 NULL :                NULL
>3 1 name :                      `{`
>3 2 1 name :                            `+`
>3 2 2 name :                            a
>3 2 3 numeric :                         [1] 1
>4 NULL :                NULL
> >
> > ep <- parse(text=deparse(f))
> > class(ep)
>[1] "expression"
> > recurse(ep)
>1 1 name :                      `function`
>1 2 NULL :                      NULL
>1 3 1 name :                            `{`
>1 3 2 1 name :                                  `+`
>1 3 2 2 name :                                  a
>1 3 2 3 numeric :                                       [1] 1
>1 4 NULL :                      NULL
>
>
>Date:   Thu, 18 Mar 2004 17:27:20 -0800 (PST)
>From:   Thomas Lumley <tlumley at u.washington.edu>
>To:   Gabor Grothendieck <ggrothendieck at myway.com>
>Cc:   <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch>
>Subject:   Re: [R] substitute question
>
>
>On Thu, 18 Mar 2004, Gabor Grothendieck wrote:
>
> >
> >
> > I don't think I expressed myself very well on that.
> >
> > Looking at what we get from the example:
> >
> > > z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
> >
> > > z
> > substitute(expression(function ()
> > {
> > a + 1
> > }), list(a = quote(b)))
> >
> > > class(z);mode(z);typeof(z)
> > [1] "call"
> > [1] "call"
> > [1] "language"
> >
> >
> > we see that the function seems to be expanded correctly and
> > the statement does produce a call object. However,
> > applying eval one, two or three times does not give what
> > you would think if you looked at z above.
>
>Maybe we didn't express ourselves well enough.
>
>Looking at z above isn't enough. z is a call to substitute().
>Its first operand is an expression. The expression contains a single term,
>which is a function.
>
>If you typed
>notz<- quote(substitute(expression(function ()
>{
>a + 1
>}), list(a = quote(b))))
>
>you would obtain something that deparsed the same as z, and so looked the
>same, but was actually different. In notz the first operand of substitute
>is an expression containing multiple terms, which if evaluated would
>return a function.
>
>substitute() goes though this expression and checks each term to see if it
>is `a`. In z there is only one term and it isn't `a`. In notz there is
>(after sufficient recursion) an `a` and it gets replaced.
>
>So
>
> > z[[2]][[2]]
>function ()
>{
>a + 1
>}
> > notz[[2]][[2]]
>function() {
>a + 1
>}
>
>are the respective operands, and they still look the same. But
>
> > mode(z[[2]][[2]])
>[1] "function"
> > mode(notz[[2]][[2]])
>[1] "call"
> > length(z[[2]][[2]])
>[1] 1
> > length(notz[[2]][[2]])
>[1] 4
>
>and if we try to find the actual `a` in there
> > notz[[2]][[2]][[3]][[2]][[2]]
>a
> > z[[2]][[2]][[3]][[2]][[2]]
>Error in z[[2]][[2]][[3]] : object is not subsettable
> >
>
>
>      -thomas
>
>
>
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com



From zelickr at pdx.edu  Sat Mar 20 04:58:53 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Fri, 19 Mar 2004 19:58:53 -0800 (PST)
Subject: [R] data frame output almost
In-Reply-To: <16474.46490.151946.699074@gargle.gargle.HOWL>
Message-ID: <Pine.GSO.4.44.0403191948100.9470-100000@freke.odin.pdx.edu>

Hello again,

I got three responses for help on the leading zero problem. Thank you.
Alas I still don't have it working. Here are more specifics:

I read in a data file like this:

participants<-read.table("C:/Work/blah-blah")

The data file consists of the fields last name, first name, social
security number, response score 1, response score 2 and so forth.

If in the console window I type "participants" I get something like:

     Jones  Norman  786123344 98.2 16.3
Flintstone    Fred  111457654 10.1  8.8
      Ugly    Butt   89733456 66.7 32.0

The problem is that the 3rd social security number is really "089733456"
and it needs to look like that.

None of the methods suggested seemed to work. I could make the social
security object alone print with leading zeros, but not as part of the
data frame.

Thanks again,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From ggrothendieck at myway.com  Sat Mar 20 04:09:53 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Mar 2004 22:09:53 -0500 (EST)
Subject: [R] substitute question
Message-ID: <20040320030953.768F33962@mprdmxin.myway.com>



Great detective work, Tony!  Thanks.

Date:   Fri, 19 Mar 2004 17:51:27 -0700 
From:   Tony Plate <tplate at blackmesacapital.com>
To:   <ggrothendieck at myway.com>, <tlumley at u.washington.edu> 
Cc:   <R-help at stat.math.ethz.ch> 
Subject:   Re: [R] substitute question 

 
I don't think it is correct that "there are two types of expressions and 
calls". There is only one type of these things. I believe the relevant 
distinction here is between 'call' objects (which can be informally thought 
of as the parse trees of unevaluated R code, and which formally have mode 
'call' in R) and other things like objects of mode 'function', etc.

However, this is all does pretty confusing when using substitute(), 
because, substitute() does go inside objects that have mode 'call', but 
doesn't go inside of objects that have mode 'function' or 
'expression'. What makes it more confusing is that sometimes 'call' 
objects can be wrapped up in expression objects. Note that parse(text=) 
returns a 'call' object wrapped in an 'expression' object, whereas quote() 
returns a 'call' object -- I believe that in general it is true that 
parse(text="XXX")[[1]] === quote(XXX).

Earlier in this discussion, Peter Dalgard stated "you can only do 
substitutions on language objects" and then used the function is.language() 
in an example, which I took at that time to imply that substitute() would 
go inside objects for which is.language() returned true. However, from 
experimenting, it seems that is.call() rather than is.language() is the 
appropriate test.

Here are some simple examples.

> esub <- function(expr, sublist) do.call("substitute", list(expr, sublist))
> e1 <- parse(text="a + 1")
> e2 <- quote(a + 1)
> e1
expression(a + 1)
> e2
a + 1
> mode(e1)
[1] "expression"
> mode(e2)
[1] "call"
> identical(e1[[1]], e2)
[1] TRUE
>
> # substitute() doesn't go inside e1, even though is.language(e1) is TRUE
> c(is.language(e1), is.call(e1))
[1] TRUE FALSE
> esub(e1, list(a=as.name('b')))
expression(a + 1)
>
> c(is.language(e2), is.call(e2))
[1] TRUE TRUE
> esub(e2, list(a=as.name('b')))
b + 1
>
> c(is.language(e1[[1]]), is.call(e1[[1]]))
[1] TRUE TRUE
> esub(e1[[1]], list(a=as.name('b')))
b + 1
> identical(e2, e1[[1]])
[1] TRUE
>
> ef <- Quote(function() a + 1)
> f <- function() a + 1
> c(is.language(ef), is.call(ef))
[1] TRUE TRUE
> esub(ef, list(a=as.name('b')))
function() b + 1
> c(is.language(f), is.call(f))
[1] FALSE FALSE
> esub(f, list(a=as.name('b')))
function ()
a + 1
> c(is.language(body(f)), is.call(body(f)))
[1] TRUE TRUE
> esub(body(f), list(a=as.name('b')))
b + 1
>
>

I also see that in S-plus 6.2, substitute() behaves differently -- it does 
go inside objects of mode 'call' and 'expression' and substitutes 'b' for 
'a' in every case above. To run the above code in S-plus, first do:
> body <- function(f) f[[1]]
> quote <- Quote

Although there isn't much to guide one in the documentation ?substitute, 
the "R Language manual" does have some discussion of substitute() and 
'expression' objects.

-- Tony Plate

At Thursday 10:58 PM 3/18/2004, Gabor Grothendieck wrote:
>
>Thanks. Thus it seems that there are two types of expressions and calls:
>
>1. fully expanded
>2. partially expanded
>
>and that fully expanded ones are a prerequisite for substitution.
>body() and quote() produce such fully expanded expressions.
>
>Using a small utility function we can investigate this:
>
>recurse <- function( x, idx = NULL )
> if ( length( x ) > 0 ) {
> for( i in seq( along = x ) )
> if (length(x[[i]])>1)
> Recall( x[[i]], c(idx, i))
> else {
> if (length(idx)) cat(idx,"")
> cat( i, class(x[[i]]), ":" )
> cat( rep("\t",length(idx) + 2) )
> print( x[[i]] )
> }
> }
>
>f <- function(){a+1}
>
>eb <- body(f)
>class(eb)
>recurse(eb)
>
>eq <- quote(function(){a+1})
>class(eq)
>recurse(eq)
>
>ep <- parse(text=deparse(f))
>class(ep)
>recurse(ep)
>
>
>The output that the above is shown below. It shows that
>body() and quote() produce fully expanded expression style objects
>although body's is of class { and quote is of class call.
>
>However, parse(text=deparse(f)) also produces a fully expanded
>expression style object of class expression yet substitution
>does not occur with that. Thus full vs. partial expansion is likely
>a necessary but not a sufficient condition. There is something
>else but I don't know what it is.
>
>
> > f <- function(){a+1}
> >
> > eb <- body(f)
> > class(eb)
>[1] "{"
> > recurse(eb)
>1 name : `{`
>2 1 name : `+`
>2 2 name : a
>2 3 numeric : [1] 1
> >
> > eq <- quote(function(){a+1})
> > class(eq)
>[1] "call"
> > recurse(eq) # lines begin with list indices and class name
>1 name : `function`
>2 NULL : NULL
>3 1 name : `{`
>3 2 1 name : `+`
>3 2 2 name : a
>3 2 3 numeric : [1] 1
>4 NULL : NULL
> >
> > ep <- parse(text=deparse(f))
> > class(ep)
>[1] "expression"
> > recurse(ep)
>1 1 name : `function`
>1 2 NULL : NULL
>1 3 1 name : `{`
>1 3 2 1 name : `+`
>1 3 2 2 name : a
>1 3 2 3 numeric : [1] 1
>1 4 NULL : NULL
>
>
>Date: Thu, 18 Mar 2004 17:27:20 -0800 (PST)
>From: Thomas Lumley <tlumley at u.washington.edu>
>To: Gabor Grothendieck <ggrothendieck at myway.com>
>Cc: <tplate at blackmesacapital.com>, <R-help at stat.math.ethz.ch>
>Subject: Re: [R] substitute question
>
>
>On Thu, 18 Mar 2004, Gabor Grothendieck wrote:
>
> >
> >
> > I don't think I expressed myself very well on that.
> >
> > Looking at what we get from the example:
> >
> > > z <- substitute(substitute(expression(f),list(a=quote(b))),list(f=f))
> >
> > > z
> > substitute(expression(function ()
> > {
> > a + 1
> > }), list(a = quote(b)))
> >
> > > class(z);mode(z);typeof(z)
> > [1] "call"
> > [1] "call"
> > [1] "language"
> >
> >
> > we see that the function seems to be expanded correctly and
> > the statement does produce a call object. However,
> > applying eval one, two or three times does not give what
> > you would think if you looked at z above.
>
>Maybe we didn't express ourselves well enough.
>
>Looking at z above isn't enough. z is a call to substitute().
>Its first operand is an expression. The expression contains a single term,
>which is a function.
>
>If you typed
>notz<- quote(substitute(expression(function ()
>{
>a + 1
>}), list(a = quote(b))))
>
>you would obtain something that deparsed the same as z, and so looked the
>same, but was actually different. In notz the first operand of substitute
>is an expression containing multiple terms, which if evaluated would
>return a function.
>
>substitute() goes though this expression and checks each term to see if it
>is `a`. In z there is only one term and it isn't `a`. In notz there is
>(after sufficient recursion) an `a` and it gets replaced.
>
>So
>
> > z[[2]][[2]]
>function ()
>{
>a + 1
>}
> > notz[[2]][[2]]
>function() {
>a + 1
>}
>
>are the respective operands, and they still look the same. But
>
> > mode(z[[2]][[2]])
>[1] "function"
> > mode(notz[[2]][[2]])
>[1] "call"
> > length(z[[2]][[2]])
>[1] 1
> > length(notz[[2]][[2]])
>[1] 4
>
>and if we try to find the actual `a` in there
> > notz[[2]][[2]][[3]][[2]][[2]]
>a
> > z[[2]][[2]][[3]][[2]][[2]]
>Error in z[[2]][[2]][[3]] : object is not subsettable
> >
>
>
> -thomas
>
>
>
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com



From jari.oksanen at oulu.fi  Sat Mar 20 07:49:31 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sat, 20 Mar 2004 08:49:31 +0200
Subject: [R] Odd behaviour of step (and stepAIC)?
In-Reply-To: <405B10EB.6000201@helsinki.fi>
References: <405B10EB.6000201@helsinki.fi>
Message-ID: <BDC47489-7A3A-11D8-98B4-000A95C76CA8@oulu.fi>

There seems to be some obscure features in step() when you have 
interaction terms: their interpretation is order sensitive. Term A:B is 
regarded to be something else than B:A. Sometimes this results in error

Error in factor.scope(ffac, list(add = fadd, drop = fdrop)) :
	upper scope does not include model

Just because R internally decides to order terms differently than in 
the scope (this may happen even when you have produced the scope by 
first fitting the maximal model, and then extracting that using 
scope=formula()). I once was diligent enough to trace this to a certain 
C function where this ordering was done, but I was not clever enough to 
instantly know how to change the behaviour.

cheers, jari oksanen

On 19 Mar 2004, at 17:25, Anon. wrote:

> I can only assume I'm betraying my ignorance here, but this is not 
> what I would expect.
>
> I'm getting the following from a stepwise selection (with both step 
> and stepAIC):
>
> > step(lm(sqrt(Grids)~ SE + Edge + NH), scope=~ (Edge + SE + NH)^2)
> Start:  AIC= 593.56
>  sqrt(Grids) ~ SE + Edge + NH
>
>           Df Sum of Sq    RSS    AIC
> <none>                 2147.0  593.6
> + Edge:NH  1       3.0 2143.9  595.1
> + SE:NH    4      23.2 2123.8  598.4
> - NH       1      75.8 2222.8  601.6
> - Edge     1     448.7 2595.7  646.4
> - SE       4    1033.7 3180.6  699.1
>
>
> My problem is that the SE:Edge term is not added.  Now, I know that 
> I've specified the terms in a different order in the start model and 
> the scope, but this shouldn't matter, should it?  Aren't these model 
> specifications commutative?
>
> Am I missing something important, or is this just an obscure feature?
>
> Bob
>
> -- 
> Bob O'Hara
>
> Dept. of Mathematics and Statistics
> P.O. Box 4 (Yliopistonkatu 5)
> FIN-00014 University of Helsinki
> Finland
> Telephone: +358-9-191 23743
> Mobile: +358 50 599 0540
> Fax:  +358-9-191 22 779
> WWW:  http://www.RNI.Helsinki.FI/~boh/
> Journal of Negative Results - EEB: http://www.jnr-eeb.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Mar 20 08:10:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Mar 2004 07:10:35 +0000 (GMT)
Subject: [R] data frame output almost
In-Reply-To: <Pine.GSO.4.44.0403191948100.9470-100000@freke.odin.pdx.edu>
Message-ID: <Pine.LNX.4.44.0403200659500.29852-100000@gannet.stats>

On Fri, 19 Mar 2004, Randy Zelick wrote:

> I got three responses for help on the leading zero problem. Thank you.

Well, it seems that you didn't tell us what the actual problem was: please
consult the posting guide and its references and learn to ask the right
question.

> Alas I still don't have it working. Here are more specifics:
> 
> I read in a data file like this:
> 
> participants<-read.table("C:/Work/blah-blah")
> 
> The data file consists of the fields last name, first name, social
> security number, response score 1, response score 2 and so forth.
> 
> If in the console window I type "participants" I get something like:
> 
>      Jones  Norman  786123344 98.2 16.3
> Flintstone    Fred  111457654 10.1  8.8
>       Ugly    Butt   89733456 66.7 32.0
> 
> The problem is that the 3rd social security number is really "089733456"
> and it needs to look like that.

Did you read the help page for read.table or the `R Data Import/Export
Manual'?  Set colClasses and avoid the conversion to numeric.  You didn't
tell us you were reading a file with leading zeroes.


> None of the methods suggested seemed to work. I could make the social
> security object alone print with leading zeros, but not as part of the
> data frame.

All the solutions I have seen do work, but mine is very simple.  You need
to convert just that column (which is not what you said you wanted).

library(MASS)
hills$climb <- gsub(" ", "0", format(hills$climb))
hills

You could also use

hills$climb <- formatC(hills$climb, format="f", flag="0", digits=0, width=4)
(Martin didn't give all the details)

hills$climb <- sprintf("%04.0f", as.double(hills$climb))

In every case, printing the data frame does show leading zeroes.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From berwin at maths.uwa.edu.au  Sat Mar 20 09:18:57 2004
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Sat, 20 Mar 2004 16:18:57 +0800
Subject: [R] Odd behaviour of step (and stepAIC)?
In-Reply-To: <BDC47489-7A3A-11D8-98B4-000A95C76CA8@oulu.fi>
References: <405B10EB.6000201@helsinki.fi>
	<BDC47489-7A3A-11D8-98B4-000A95C76CA8@oulu.fi>
Message-ID: <16475.65137.646262.196069@bossiaea.maths.uwa.edu.au>

>>>>> "JO" == Jari Oksanen <jari.oksanen at oulu.fi> writes:

    JO> There seems to be some obscure features in step() when you
    JO> have interaction terms: their interpretation is order
    JO> sensitive. [...]  Just because R internally decides to order
    JO> terms differently than in the scope (this may happen even when
    JO> you have produced the scope by first fitting the maximal
    JO> model, and then extracting that using scope=formula()). I once
    JO> was diligent enough to trace this to a certain C function
    JO> where this ordering was done, but I was not clever enough to
    JO> instantly know how to change the behaviour.
It is well possible that this may be occasionally a problem.  (I
remember that a colleague of mine once showed me a problem were the
behaviour of step depended on how you specified the code.  However, I
believe more recent version of R don't show that behaviour anymore.)

But I don't believe that this is the problem in Bob's case:
    >> I'm getting the following from a stepwise selection (with both step 
    >> and stepAIC):
    >> 
    >> > step(lm(sqrt(Grids)~ SE + Edge + NH), scope=~ (Edge + SE + NH)^2)
    >> Start:  AIC= 593.56
    >> sqrt(Grids) ~ SE + Edge + NH
    >> 
    >> Df Sum of Sq              RSS    AIC
    >> <none>                 2147.0  593.6
    >> + Edge:NH  1       3.0 2143.9  595.1
    >> + SE:NH    4      23.2 2123.8  598.4
    >> - NH       1      75.8 2222.8  601.6
    >> - Edge     1     448.7 2595.7  646.4
    >> - SE       4    1033.7 3180.6  699.1
    >> 
    >> 
    >> My problem is that the SE:Edge term is not added.
If you look at the output, adding the SE:Edge term decreases RSS but
increases AIC.  The increase in parameters is not worth the decrease
in RSS, according to the AIC criterion.  step tries to find the best
AIC model, i.e., the current model is the best.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From p.dalgaard at biostat.ku.dk  Sat Mar 20 11:23:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Mar 2004 11:23:20 +0100
Subject: [R] substitute question
In-Reply-To: <6.0.3.0.2.20040319165812.09122a68@mailhost.blackmesacapital.com>
References: <20040319055822.829AB3999@mprdmxin.myway.com>
	<6.0.3.0.2.20040319165812.09122a68@mailhost.blackmesacapital.com>
Message-ID: <x21xnnzujr.fsf@biostat.ku.dk>

Tony Plate <tplate at blackmesacapital.com> writes:

> Earlier in this discussion, Peter Dalgard stated "you can only do
> substitutions on language objects" and then used the function
> is.language() in an example, which I took at that time to imply that
> substitute() would go inside objects for which is.language() returned
> true.  However, from experimenting, it seems that is.call() rather
> than is.language() is the appropriate test.

Right. That surprised me too, although perhaps it shouldn't have. Of
course we might just have looked at the source, where we see that
PROMSXP, SYMSXP, and LANGSXP are handled, and everything else,
including EXPRSXPs are returned as is. I think this is deliberate and
that expression objects are occasionally used as a quoting mechanism
to prevent substitutions from taking place, but my recollection is a
bit hazy on this point. So at the R level it is is.name() or is.call()
or promises evaluating to either of the two.

Notice, BTW, that expression() is "weird" in the same way as
the function(...)... constructs that tripped Gabor:

> substitute(expression(a+b),list(b=2))
expression(a + 2)
> eval(substitute(substitute(e,list(b=2)), list(e=expression(a+b))))
expression(a + b)
> eval(substitute(substitute(e,list(b=2)), list(e=quote(expression(a+b)))))
expression(a + 2)

and the root cause of this is that  

> expression(a+b)
expression(a + b)
> quote(expression(a+b))
expression(a + b)

There's a group of cases where deparse and parse are not exact
inverses, because some objects are not directly representable in the
language, so you get instead a function call that *evaluates* to a
similar object; the simplest case is vectors of length > 1:

> dput(rnorm(1))
-0.214713202300464
> dput(rnorm(2))
c(0.92727004398074, 0.437854016853309)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Jan.Verbesselt at agr.kuleuven.ac.be  Sat Mar 20 15:35:40 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Sat, 20 Mar 2004 15:35:40 +0100
Subject: [R] labels on axis(4) and adaptation of legend to size of the plot
Message-ID: <1079793340.405c56bcb5283@webmail1.kuleuven.be>

Hi weekend R helpers,

*Is it possible to get a ylabel on the right hand side axis ==> axis(4)?

Thankx,
Jan

 opar <- par(mfrow = c(2,1))
 plot(ts.Origi[,1],ylab='NDII', main=name)
            # Add the legend text in the right order
            legend.txt <- c("NDII", "Inverse KBDI")
            legend(2001.8,0.29, legend.txt,col=c(1,2), lty=1)
            par(new=T)
           plot(ts.Origi[,2], yaxt="n", type="l", col=2, las=1, ylab="")
            axis(4)

  plot(ts.NDVIKB[,1],ylab='NDVI', main=name)
            # Add the legend text in the right order
            legend.txt <- c("NDVI", "Inverse KBDI")
            legend(2001.8,0.42, legend.txt,col=c(1,2), lty=1)
            par(new=T)
          plot(ts.NDVIKB[,2], yaxt="n", type="l", col=2, las=1, ylab="")
            axis(4)



From andrewr at uidaho.edu  Sat Mar 20 15:45:33 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Sat, 20 Mar 2004 06:45:33 -0800
Subject: [R] labels on axis(4) and adaptation of legend to size of the plot
In-Reply-To: <1079793340.405c56bcb5283@webmail1.kuleuven.be>
References: <1079793340.405c56bcb5283@webmail1.kuleuven.be>
Message-ID: <200403200645.33440.andrewr@uidaho.edu>

Jan,

try ?mtext

Cheers

Andrew

On Saturday 20 March 2004 06:35, Jan Verbesselt wrote:
> Hi weekend R helpers,
>
> *Is it possible to get a ylabel on the right hand side axis ==> axis(4)?
>
> Thankx,
> Jan
>
>  opar <- par(mfrow = c(2,1))
>  plot(ts.Origi[,1],ylab='NDII', main=name)
>             # Add the legend text in the right order
>             legend.txt <- c("NDII", "Inverse KBDI")
>             legend(2001.8,0.29, legend.txt,col=c(1,2), lty=1)
>             par(new=T)
>            plot(ts.Origi[,2], yaxt="n", type="l", col=2, las=1, ylab="")
>             axis(4)
>
>   plot(ts.NDVIKB[,1],ylab='NDVI', main=name)
>             # Add the legend text in the right order
>             legend.txt <- c("NDVI", "Inverse KBDI")
>             legend(2001.8,0.42, legend.txt,col=c(1,2), lty=1)
>             par(new=T)
>           plot(ts.NDVIKB[,2], yaxt="n", type="l", col=2, las=1, ylab="")
>             axis(4)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From pburns at pburns.seanet.com  Sat Mar 20 15:55:12 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 20 Mar 2004 14:55:12 +0000
Subject: [R] labels on axis(4) and adaptation of legend to size of the plot
References: <1079793340.405c56bcb5283@webmail1.kuleuven.be>
Message-ID: <405C5B50.6010708@pburns.seanet.com>

Two tricks to know:

You need to have enough room in that margin, so you
probably need to set the "mar" graphics parameter.
For example:

par(mar=c(5,4,4,4) +.1)

You then want to use "mtext", like:

mtext(side=4, line=2, "the other ylab")

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jan Verbesselt wrote:

>Hi weekend R helpers,
>
>*Is it possible to get a ylabel on the right hand side axis ==> axis(4)?
>
>Thankx,
>Jan
>
> opar <- par(mfrow = c(2,1))
> plot(ts.Origi[,1],ylab='NDII', main=name)
>            # Add the legend text in the right order
>            legend.txt <- c("NDII", "Inverse KBDI")
>            legend(2001.8,0.29, legend.txt,col=c(1,2), lty=1)
>            par(new=T)
>           plot(ts.Origi[,2], yaxt="n", type="l", col=2, las=1, ylab="")
>            axis(4)
>
>  plot(ts.NDVIKB[,1],ylab='NDVI', main=name)
>            # Add the legend text in the right order
>            legend.txt <- c("NDVI", "Inverse KBDI")
>            legend(2001.8,0.42, legend.txt,col=c(1,2), lty=1)
>            par(new=T)
>          plot(ts.NDVIKB[,2], yaxt="n", type="l", col=2, las=1, ylab="")
>            axis(4)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From rrsilva at ib.usp.br  Sat Mar 20 13:09:52 2004
From: rrsilva at ib.usp.br (=?iso-8859-1?q?Rog=E9rio=20Rosa=20da=20Silva?=)
Date: Sat, 20 Mar 2004 12:09:52 +0000
Subject: [R] minimum values
Message-ID: <200403201209.53167.rrsilva@ib.usp.br>

Hi,

In my data set I have created a matrix of distances. How can I get a list of 
minimum value in each row?

thanks a lot,

Rog?rio



From jfox at mcmaster.ca  Sat Mar 20 18:02:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 20 Mar 2004 12:02:21 -0500
Subject: [R] minimum values
In-Reply-To: <200403201209.53167.rrsilva@ib.usp.br>
Message-ID: <20040320170220.SOYT11707.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Rogerio,

If the distances is the distance matrix, apply(distances, 1, min) should
give you what you want.

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Rog?rio Rosa da Silva
> Sent: Saturday, March 20, 2004 7:10 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] minimum values
> 
> Hi,
> 
> In my data set I have created a matrix of distances. How can 
> I get a list of minimum value in each row?
> 
> thanks a lot,
> 
> Rog?rio



From MSchwartz at MedAnalytics.com  Sat Mar 20 18:05:57 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 20 Mar 2004 11:05:57 -0600
Subject: [R] minimum values
In-Reply-To: <200403201209.53167.rrsilva@ib.usp.br>
References: <200403201209.53167.rrsilva@ib.usp.br>
Message-ID: <1079802357.7030.233.camel@localhost.localdomain>

On Sat, 2004-03-20 at 06:09, Rog?rio Rosa da Silva wrote:
> Hi,
> 
> In my data set I have created a matrix of distances. How can I get a list of 
> minimum value in each row?
> 
> thanks a lot,
> 
> Rog?rio


If your matrix is 'mat':

apply(mat, 1, min)

Example:

> mat <- matrix(rnorm(30), ncol = 3)

> mat
             [,1]        [,2]       [,3]
 [1,] -0.18136616  0.39529877 -1.4816285
 [2,]  0.05523284 -0.52171745  0.4893621
 [3,] -0.27029731 -0.86745328 -0.1918832
 [4,] -0.73901526 -0.35762159  1.2695002
 [5,] -0.11800851 -1.48057601 -0.8918780
 [6,] -1.52627381  1.30498318  1.0749069
 [7,]  0.08733351  1.17986453 -0.8514364
 [8,]  0.02248721  0.03496345 -1.0305299
 [9,]  0.72882492  1.32881042 -0.6423467
[10,] -0.20463172 -1.33260851  1.2732504

> apply(mat, 1, min)
 [1] -1.4816285 -0.5217175 -0.8674533 -0.7390153 -1.4805760 -1.5262738
 [7] -0.8514364 -1.0305299 -0.6423467 -1.3326085

See ?apply for more information

HTH,

Marc Schwartz



From xgong at brandeis.edu  Sat Mar 20 18:13:48 2004
From: xgong at brandeis.edu (Xin Gong)
Date: Sat, 20 Mar 2004 12:13:48 -0500
Subject: [R] newbie question on clustering performance
Message-ID: <F3AD57BA-7A91-11D8-9029-000393820C14@brandeis.edu>

>
> May I ask you some questions on clustering analysis?
>
> How do I get jacard score from cluster object?
>  or after I call kmanes or pam function how do I know which number of 
> clusters
> sounds the best.
>
> Another question on heatmap, if you do not mind, how do I retrive the 
> name of
> rows when some area is hot.
>
> Thanks in adavance,
>
> Xin Gong
>
>
>
>
>



From phddas at yahoo.com  Sat Mar 20 19:11:03 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 20 Mar 2004 10:11:03 -0800 (PST)
Subject: [R] timing a function
Message-ID: <20040320181103.27029.qmail@web20504.mail.yahoo.com>

Hello
is there a way to time how long it takes to run a code
in R. somthing like tic toc in matlab as such?
help.search("timing") put out nothing.
and while I got you, debugging the code, is there a
step through and the rest of the debugging tools
working with ESS.

thanks



From ccleland at optonline.net  Sat Mar 20 19:20:13 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 20 Mar 2004 13:20:13 -0500
Subject: [R] timing a function
In-Reply-To: <20040320181103.27029.qmail@web20504.mail.yahoo.com>
References: <20040320181103.27029.qmail@web20504.mail.yahoo.com>
Message-ID: <405C8B5D.401@optonline.net>

Fred J. wrote:
> is there a way to time how long it takes to run a code
> in R. somthing like tic toc in matlab as such?
> help.search("timing") put out nothing.

?system.time

> and while I got you, debugging the code, is there a
> step through and the rest of the debugging tools
> working with ESS.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From MSchwartz at MedAnalytics.com  Sat Mar 20 19:36:21 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 20 Mar 2004 12:36:21 -0600
Subject: [R] timing a function
In-Reply-To: <20040320181103.27029.qmail@web20504.mail.yahoo.com>
References: <20040320181103.27029.qmail@web20504.mail.yahoo.com>
Message-ID: <1079807781.7030.264.camel@localhost.localdomain>

On Sat, 2004-03-20 at 12:11, Fred J. wrote:
> Hello
> is there a way to time how long it takes to run a code
> in R. somthing like tic toc in matlab as such?
> help.search("timing") put out nothing.

See ?system.time

> and while I got you, debugging the code, is there a
> step through and the rest of the debugging tools
> working with ESS.

See section 6 (R and Emacs) in the main R FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#R%20and%20Emacs

HTH,

Marc Schwartz



From pauljohn at ku.edu  Sat Mar 20 18:06:45 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Sat, 20 Mar 2004 11:06:45 -0600
Subject: [R] contrast lme and glmmPQL and getting additional results...
Message-ID: <405C7A25.6000303@ku.edu>

I have a longitudinal data analysis project.  There are 10 observations 
on each of 15 units, and I'm estimating this with randomly varying 
intercepts along with an AR1 correction for the error terms within 
units.  There is no correlation across units.  Blundering around in R 
for a long time, I found that for linear/gaussian models, I can use 
either the MASS method glmmPQL (thanks to Venables and Ripley) or the 
lme from nlme (thanks to Pinheiro and Bates).  (I also find that the 
package lme4 has GLMM, but I can't get the correlation structure to work 
with that, so I gave up on that one.)

The glmmPQL and lme results are quite similar, but not identical.

Here are my questions.

1. I believe that both of these offer consistent estimates. Does one 
have preferrable small sample properties?  Is the lme the preferred 
method in this case because it is more narrowly designed to this 
gaussian family model with an identity link?  If there's an argument in 
favor of PQL, I'd like to know it, because a couple of the Hypothesis 
tests based on t-statistics are affected.

2. Is there a pre-made method for calculation of the robust standard errors?

I notice that model.matrix() command does not work for either lme or 
glmmPQL results, and so I start to wonder how people calculate sandwich 
estimators of the standard errors.

3. Are the AIC (or BIC) statistics comparable across models?  Can one 
argue in favor of the glmmPQL results (with, say, a log link) if the AIC 
is more favorable than the AIC from an lme fit?  In JK Lindsey's Models 
for Repeated Measurements, the AIC is sometimes used to make model 
selections, but I don't know where the limits of this application might 
lie.


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From sdavis2 at mail.nih.gov  Sat Mar 20 21:16:38 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 20 Mar 2004 15:16:38 -0500
Subject: [R] Operating on windows of data
Message-ID: <000c01c40eb8$43211770$2f643744@WATSON>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040320/f814303d/attachment.pl

From ggrothendieck at myway.com  Sat Mar 20 21:37:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 20 Mar 2004 15:37:12 -0500 (EST)
Subject: [R] Operating on windows of data
Message-ID: <20040320203712.AE3173961@mprdmxin.myway.com>


See

   ?embed

and possibly

   ?filter


Date:   Sat, 20 Mar 2004 15:16:38 -0500 
From:   Sean Davis <sdavis2 at mail.nih.gov>
To:   R-Help <r-help at stat.math.ethz.ch> 
Subject:   [R] Operating on windows of data 

 
I have a data set that is comprised of, for simplicity, a vector of numbers that I want to march across in overlapping windows of say 10 values each, computing a couple of values for each window. Is there a vectorized way to do this, or do I truly need to resort to looping--I think so? Any other clever thoughts?

Thanks,
Sean

     [[alternative HTML version deleted]]



From MSchwartz at MedAnalytics.com  Sat Mar 20 21:38:06 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 20 Mar 2004 14:38:06 -0600
Subject: [R] Operating on windows of data
In-Reply-To: <000c01c40eb8$43211770$2f643744@WATSON>
References: <000c01c40eb8$43211770$2f643744@WATSON>
Message-ID: <1079815086.7030.322.camel@localhost.localdomain>

On Sat, 2004-03-20 at 14:16, Sean Davis wrote:
> I have a data set that is comprised of, for simplicity, a vector of
> numbers that I want to march across in overlapping windows of say 10
> values each, computing a couple of values for each window.  Is there a
> vectorized way to do this, or do I truly need to resort to looping--I
> think so?  Any other clever thoughts?
> 
> Thanks,
> Sean


See the running() function in the 'gregmisc' package on CRAN or the
filter() function in package 'ts', which is part of the standard R
installation.

HTH,

Marc Schwartz



From kjetil at entelnet.bo  Sat Mar 20 21:38:35 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sat, 20 Mar 2004 16:38:35 -0400
Subject: [R] contrast lme and glmmPQL and getting additional results...
In-Reply-To: <405C7A25.6000303@ku.edu>
Message-ID: <405C738B.32455.314874@localhost>

On 20 Mar 2004 at 11:06, Paul Johnson wrote:

>From what you say, it seems like you have a linear normal (mixed) 
model. This is what lme is made for, and there is no reason to use 
glmmPQL (which calls lme iteratively).

Kjetil Halvorsen

> I have a longitudinal data analysis project.  There are 10
> observations on each of 15 units, and I'm estimating this with
> randomly varying intercepts along with an AR1 correction for the error
> terms within units.  There is no correlation across units.  Blundering
> around in R for a long time, I found that for linear/gaussian models,
> I can use either the MASS method glmmPQL (thanks to Venables and
> Ripley) or the lme from nlme (thanks to Pinheiro and Bates).  (I also
> find that the package lme4 has GLMM, but I can't get the correlation
> structure to work with that, so I gave up on that one.)
> 
> The glmmPQL and lme results are quite similar, but not identical.
> 
> Here are my questions.
> 
> 1. I believe that both of these offer consistent estimates. Does one
> have preferrable small sample properties?  Is the lme the preferred
> method in this case because it is more narrowly designed to this
> gaussian family model with an identity link?  If there's an argument
> in favor of PQL, I'd like to know it, because a couple of the
> Hypothesis tests based on t-statistics are affected.
> 
> 2. Is there a pre-made method for calculation of the robust standard
> errors?
> 
> I notice that model.matrix() command does not work for either lme or
> glmmPQL results, and so I start to wonder how people calculate
> sandwich estimators of the standard errors.
> 
> 3. Are the AIC (or BIC) statistics comparable across models?  Can one
> argue in favor of the glmmPQL results (with, say, a log link) if the
> AIC is more favorable than the AIC from an lme fit?  In JK Lindsey's
> Models for Repeated Measurements, the AIC is sometimes used to make
> model selections, but I don't know where the limits of this
> application might lie.
> 
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504 University of Kansas                  Office:
> (785) 864-9086 Lawrence, Kansas 66044-3177           FAX: (785)
> 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From phddas at yahoo.com  Sat Mar 20 22:54:22 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 20 Mar 2004 13:54:22 -0800 (PST)
Subject: [R] setwd() permenent
Message-ID: <20040320215422.77968.qmail@web20508.mail.yahoo.com>

Hello
when I close and reopen R it gives back a different wd
than what I used in setwd. how can I get it to
permenently use dirname in setwd("dirname") and not
the other name it keeps defaulting to?
I am using W2K and ESS
thanks



From ripley at stats.ox.ac.uk  Sun Mar 21 09:59:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Mar 2004 08:59:20 +0000 (GMT)
Subject: [R] setwd() permenent
In-Reply-To: <20040320215422.77968.qmail@web20508.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403210857001.8472-100000@gannet.stats>

Since you are using ESS, you need to start R in the working directory you 
want.

Since you are using Windows, please do read the rw-FAQ which covers this
for the normal usage calls and asks you to send ESS questions to ess-help.

On Sat, 20 Mar 2004, Fred J. wrote:

> when I close and reopen R it gives back a different wd
> than what I used in setwd. how can I get it to
> permenently use dirname in setwd("dirname") and not
> the other name it keeps defaulting to?
> I am using W2K and ESS

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Mar 21 10:34:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 21 Mar 2004 09:34:50 +0000 (GMT)
Subject: [R] Conflicting headers (was R_qsort_int_I() error)
In-Reply-To: <E757CF16-79F4-11D8-91CB-000A9594660C@Princeton.Edu>
Message-ID: <Pine.LNX.4.44.0403210928410.9020-100000@gannet.stats>

Do look at the code: Rmath.h contains

#ifdef MATHLIB_STANDALONE
 typedef enum { FALSE = 0, TRUE } Rboolean;
#else
# include <R_ext/Boolean.h>

/* for API back-compatibility -- DEPRECATED since R 1.2 -- */
#define LTRUE  TRUE
#define LFALSE FALSE
#endif

Is this code to be linked into R?  If so you should not be defining
MATHLIB_STANDALONE: if not, you should not be using R_ext/Utils.h.


On Fri, 19 Mar 2004, Kosuke Imai wrote:

> Hi,
>    I want to use R_qsort_int_I() in my C function, but getting the 
> following error. It looks like there is a conflict between Rmath.h, 
> which I use to generate random numbers, and R_ext/Boolean.h I would 
> appreciate any help to fix this problem.
> 
> gcc -ansi -g -o Gibbs gibbs.c subroutines.o rand.o vector.o -lm -lRmath 
> -llapack -lblas -lfrtbegin -lg2c -lm -shared-libgcc
> In file included from /usr/include/R_ext/Utils.h:27,
>                   from gibbs.c:7:
> /usr/include/R_ext/Boolean.h:29: conflicting types for `FALSE'
> /usr/include/Rmath.h:175: previous declaration of `FALSE'
> /usr/include/R_ext/Boolean.h:29: conflicting types for `TRUE'
> /usr/include/Rmath.h:175: previous declaration of `TRUE'
> /usr/include/R_ext/Boolean.h:29: confused by earlier errors, bailing out
> make: *** [Gibbs] Error 1

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do, and don't use misleading subject lines.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sam.kemp2 at ntlworld.com  Sun Mar 21 12:07:01 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Sun, 21 Mar 2004 11:07:01 +0000
Subject: [R] writing text on graphics' window
Message-ID: <405D7755.5020307@ntlworld.com>

Hi,

Does anyone know of a method for writing text to the graphics window, 
where there is *no* plot? Basically, I have developed a 'significance 
test' and I would like the output on the graphics window to say 
something about the input parameters and the stats of the significance test.

Any help would be appreciated.

Cheers,

Sam.



From john.maindonald at anu.edu.au  Sun Mar 21 12:19:34 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 21 Mar 2004 22:19:34 +1100
Subject: [R] Solutions to Exercises - Data Analysis & Graphics Using R
Message-ID: <A206C4C2-7B29-11D8-9ADB-000A95CDA0F2@anu.edu.au>

This message is aimed at anyone who may be using
exercises from my book with John Braun for teaching
purposes.  I am using this channel of communication
in the absence of any other obvious effective channel.
I ask the forbearance of list members.

Our intention is to post solutions to selected exercises
(the more challenging exercises) on the web, via a link
from http://wwwmaths.anu.edu.au/~johnm/r-book.html
Anyone who would find this inconvenient should contact
me directly.

I will shortly post a provisional list on the above web site.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From dmurdoch at pair.com  Sun Mar 21 12:58:03 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 21 Mar 2004 06:58:03 -0500
Subject: [R] writing text on graphics' window
In-Reply-To: <405D7755.5020307@ntlworld.com>
References: <405D7755.5020307@ntlworld.com>
Message-ID: <6a0r50h4su6ik1dutjudnims8eas0tcj75@4ax.com>

On Sun, 21 Mar 2004 11:07:01 +0000, you wrote:

>Hi,
>
>Does anyone know of a method for writing text to the graphics window, 
>where there is *no* plot? Basically, I have developed a 'significance 
>test' and I would like the output on the graphics window to say 
>something about the input parameters and the stats of the significance test.
>
>Any help would be appreciated.

You need to make sure a graphics device is active and establish a
coordinate system there.  The easiest way to do that is to make a call
to plot() with everything turned off:

 plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")

You may also want to reduce the "margins" if you want your output to
take up the full frame, e.g.

 oldmargins <- par(mar=c(0,0,0,0))
 plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")

 text(c(0,100,0,100),c(0,0,100,100),c('bottom left',
      'bottom right','top left','top right'))

 par(oldmargins)

Duncan Murdoch



From ggrothendieck at myway.com  Sun Mar 21 13:42:49 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Mar 2004 07:42:49 -0500 (EST)
Subject: [R] writing text on graphics' window
Message-ID: <20040321124249.3E4463984@mprdmxin.myway.com>


Check out

demo(Hershey)
demo(plotmath)

for examples.

Date:   Sun, 21 Mar 2004 11:07:01 +0000 
From:   Samuel Kemp <sam.kemp2 at ntlworld.com>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] writing text on graphics' window 

 
Hi,

Does anyone know of a method for writing text to the graphics window, 
where there is *no* plot? Basically, I have developed a 'significance 
test' and I would like the output on the graphics window to say 
something about the input parameters and the stats of the significance test.

Any help would be appreciated.

Cheers,

Sam.



From phddas at yahoo.com  Sun Mar 21 15:09:47 2004
From: phddas at yahoo.com (Fred J.)
Date: Sun, 21 Mar 2004 06:09:47 -0800 (PST)
Subject: [R] learning to debug
Message-ID: <20040321140947.62861.qmail@web20501.mail.yahoo.com>

Hello
I am trying to follow the instruction in README.debug
on how to debug a code I wrote, so since it only shows
the steps on how to debug a built in function and not
a code in an *.R file, so ok, let me do this and maybe
later I can find out how to code my code in
filename.R.
now
> library(debug)
Loading required package: mvbutils 
MVBUTILS: no "tasks" vector found in ROOT
Loading required package: tcltk 
> x <- 1:30
> mtrace(mean)
> mean(x)
Error in match.fun(FUN) : evaluation is nested too
deeply: infinite recursion?
Error: evaluation is nested too deeply: infinite
recursion?
well, I thought I was doing thing right, what now?

thanks much



From ggrothendieck at myway.com  Sun Mar 21 15:23:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Mar 2004 09:23:32 -0500 (EST)
Subject: [R] timing a function
Message-ID: <20040321142332.7CB5E3992@mprdmxin.myway.com>



Just one tip in regard to the information already provided
by others.

If you time two functions to compare them and it just so happens
that a garbage collection occurs within one but not the other
then your timing will be distorted.  To control for this issue
a gc() command just before each system.time command:

gc(); system.time(x <- rnorm(1000000))

That will cause a garbage collection to occur before the
code is run so that it starts out from a known state.

Another thing you could do is to try running from a fresh
R session.


Date:   Sat, 20 Mar 2004 10:11:03 -0800 (PST) 
From:   Fred J. <phddas at yahoo.com>
To:   r help <r-help at stat.math.ethz.ch> 
Subject:   [R] timing a function 

 
Hello
is there a way to time how long it takes to run a code
in R. somthing like tic toc in matlab as such?
help.search("timing") put out nothing.
and while I got you, debugging the code, is there a
step through and the rest of the debugging tools
working with ESS.

thanks



From phddas at yahoo.com  Sun Mar 21 15:32:26 2004
From: phddas at yahoo.com (Fred J.)
Date: Sun, 21 Mar 2004 06:32:26 -0800 (PST)
Subject: [R] timing a function
In-Reply-To: <20040321142332.7CB5E3992@mprdmxin.myway.com>
Message-ID: <20040321143226.85266.qmail@web20512.mail.yahoo.com>

thanks alot for all the help on this issue.



From dmurdoch at pair.com  Sun Mar 21 15:38:59 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 21 Mar 2004 09:38:59 -0500
Subject: [R] learning to debug
In-Reply-To: <20040321140947.62861.qmail@web20501.mail.yahoo.com>
References: <20040321140947.62861.qmail@web20501.mail.yahoo.com>
Message-ID: <dv9r5016kopd1vnp3nj08d60aspmiepbvn@4ax.com>

On Sun, 21 Mar 2004 06:09:47 -0800 (PST), you wrote:

>Hello
>I am trying to follow the instruction in README.debug
>on how to debug a code I wrote, so since it only shows
>the steps on how to debug a built in function and not
>a code in an *.R file, so ok, let me do this and maybe
>later I can find out how to code my code in
>filename.R.

You were using the contributed debug package; you may need the
maintainer (Mark V. Bravington <mark.bravington at csiro.au>) to help you
out.  

In the meantime, you can probably do what you want with the more
limited built-in debug facilities provided through the debug()
function.  See ?debug for help on it.

One thing to note: R doesn't know anything about filename.R, it
doesn't have a concept of the source of a function.  You will be
debugging an internal copy of the function.  It's not like a source
level debugger in other languages.

Duncan Murdoch



From Atropin75 at t-online.de  Sun Mar 21 16:42:17 2004
From: Atropin75 at t-online.de (Felix Eschenburg)
Date: Sun, 21 Mar 2004 16:42:17 +0100
Subject: [R] Multilevel analysis with package lme
Message-ID: <200403211642.17898.atropin75@t-online.de>

Dear list,
i am a student of psychology and have to do a multilevelanalysis on some data. 

About that i have one general and one specific question.

This is what i have copied from the help-file on lme:
  data(bdf)
     fm <- lme(langPOST ~ IQ.ver.cen + avg.IQ.ver.cen, data = bdf,
               random = ~ IQ.ver.cen | schoolNR)
     summary(fm)

after summary(fm) i get the following error:
Error in verbose || attr(x, "verbose") : invalid `y' type in `x || y'

I assume that i have not installed the package nlme correcty, but reinstalling 
did not fix that error. So, what can i do about that ?


Then i have another question about my data.
I have one response variable and about 20 explanatory variables. These 
variables are nested in a grouping variable of about 100 groups, which is 
nested in another grouping variable of 2 groups .
I have tried this 
lme.model <- lme(respVar~expVar1, data=myData, random = respVar1+...
+respVar20| groupingVariable_level2,na.action=na.omit)

As i understood Snijders and Bosker, with that i have a fixed effect of 
expVar1 on respVar and a random effect of all the others explanatory 
variables. They are also nested in grouping Variable level 2.
Now my question is, if this is the correct term and how do i include the 
groupingVariable 3 into my model ?
As you can see, i just begun to learn about multilevelanalysis, so if you have 
a link, where it is explained, how to do that with R, I would be very 
thankful.
Yours sincerly
Felix Eschenburg



From bates at stat.wisc.edu  Sun Mar 21 17:14:53 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 21 Mar 2004 10:14:53 -0600
Subject: [R] Multilevel analysis with package lme
In-Reply-To: <200403211642.17898.atropin75@t-online.de>
References: <200403211642.17898.atropin75@t-online.de>
Message-ID: <6rwu5efa82.fsf@bates4.stat.wisc.edu>

Atropin75 at t-online.de (Felix Eschenburg) writes:

> Dear list,
> i am a student of psychology and have to do a multilevelanalysis on some data. 
> 
> About that i have one general and one specific question.
> 
> This is what i have copied from the help-file on lme:
>   data(bdf)
>      fm <- lme(langPOST ~ IQ.ver.cen + avg.IQ.ver.cen, data = bdf,
>                random = ~ IQ.ver.cen | schoolNR)
>      summary(fm)
> 
> after summary(fm) i get the following error:
> Error in verbose || attr(x, "verbose") : invalid `y' type in `x || y'

Using
Package: nlme
Version: 3.1-48
Date: 2004/01/14
Priority: recommended
Title: Linear and nonlinear mixed effects models
Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
        Bates <bates at stat.wisc.edu>, Saikat DebRoy
        <saikat at stat.wisc.edu>, and Deepayan Sarkar
        <deepayan at stat.wisc.edu>
Maintainer: Douglas Bates <bates at stat.wisc.edu>

I get

> library(nlme)
> data(bdf)
>   fm <- lme(langPOST ~ IQ.ver.cen + avg.IQ.ver.cen, data = bdf,
               random = ~ IQ.ver.cen | schoolNR)
> summary(fm)
Linear mixed-effects model fit by REML
 Data: bdf 
       AIC      BIC    logLik
  15231.88 15272.01 -7608.938

Random effects:
 Formula: ~IQ.ver.cen | schoolNR
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 2.8400175 (Intr)
IQ.ver.cen  0.4604824 -0.635
Residual    6.4295396       

Fixed effects: langPOST ~ IQ.ver.cen + avg.IQ.ver.cen 
                  Value Std.Error   DF   t-value p-value
(Intercept)    40.75032 0.2879720 2155 141.50793       0
IQ.ver.cen      2.46037 0.0838685 2155  29.33607       0
avg.IQ.ver.cen  1.41174 0.3238119  129   4.35976       0
 Correlation: 
               (Intr) IQ.vr.
IQ.ver.cen     -0.273       
avg.IQ.ver.cen  0.029 -0.212

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-4.17445076 -0.63852405  0.06580449  0.70386427  2.71187810 

Number of Observations: 2287
Number of Groups: 131 

> I assume that i have not installed the package nlme correcty, but
> reinstalling did not fix that error. So, what can i do about that ?

The nlme package is a required package and should be installed with
any binary version of R.  You should not need to install that package
separately.

> Then i have another question about my data.
> I have one response variable and about 20 explanatory variables. These 
> variables are nested in a grouping variable of about 100 groups, which is 
> nested in another grouping variable of 2 groups .
> I have tried this 
> lme.model <- lme(respVar~expVar1, data=myData, random = respVar1+...
> +respVar20| groupingVariable_level2,na.action=na.omit)

I don't think you could have used that syntax.  You would have needed
something like

 random = ~ respVar1+...+respVar20| groupingVariable_level2

and that is almost certainly not what you want to try to fit.  (BTW, I
think you have written respVar where you meant expVar.)  That model
has at least 20*(20+1)/2 = 210 variances and covariances to estimate.
It is unlikely that you could do that or that you would want that.

> As i understood Snijders and Bosker, with that i have a fixed effect of 
> expVar1 on respVar and a random effect of all the others explanatory 
> variables. They are also nested in grouping Variable level 2.
> Now my question is, if this is the correct term and how do i include the 
> groupingVariable 3 into my model ?

I would start with simpler models and take a lot at a lot of
diagnostic plots along the way.

> As you can see, i just begun to learn about multilevelanalysis, so
> if you have a link, where it is explained, how to do that with R, I
> would be very thankful.

Well there is a book by Pinheiro and Bates (Springer, 2000) called
"Mixed-effects models in S and S-PLUS" that you may want to look at.



From ggrothendieck at myway.com  Sun Mar 21 17:46:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Mar 2004 11:46:22 -0500 (EST)
Subject: [R] timing a function
Message-ID: <20040321164622.08C3B39B4@mprdmxin.myway.com>


In rereading my post I noticed one point that may not
be clear.  When referring to using a fresh R session,
that is not an alternative to gc(), its another thing
that can be done in addition.  One should still use gc().
In fact, the garbage collector may be called several
times even during the start up of an R session and
it could still be at a point close to another garbage
collection even at start of a fresh session.


---
Date:   Sun, 21 Mar 2004 09:23:32 -0500 (EST) 
From:   Gabor Grothendieck <ggrothendieck at myway.com>
To:   <phddas at yahoo.com>, <r-help at stat.math.ethz.ch> 
Subject:   RE: [R] timing a function 

 


Just one tip in regard to the information already provided
by others.

If you time two functions to compare them and it just so happens
that a garbage collection occurs within one but not the other
then your timing will be distorted. To control for this issue
a gc() command just before each system.time command:

gc(); system.time(x <- rnorm(1000000))

That will cause a garbage collection to occur before the
code is run so that it starts out from a known state.

Another thing you could do is to try running from a fresh
R session.


Date: Sat, 20 Mar 2004 10:11:03 -0800 (PST) 
From: Fred J. <phddas at yahoo.com>
To: r help <r-help at stat.math.ethz.ch> 
Subject: [R] timing a function 


Hello
is there a way to time how long it takes to run a code
in R. somthing like tic toc in matlab as such?
help.search("timing") put out nothing.
and while I got you, debugging the code, is there a
step through and the rest of the debugging tools
working with ESS.

thanks



From Atropin75 at t-online.de  Sun Mar 21 17:59:07 2004
From: Atropin75 at t-online.de (Felix Eschenburg)
Date: Sun, 21 Mar 2004 17:59:07 +0100
Subject: [R] Multilevel analysis with package lme
In-Reply-To: <6rwu5efa82.fsf@bates4.stat.wisc.edu>
References: <200403211642.17898.atropin75@t-online.de>
	<6rwu5efa82.fsf@bates4.stat.wisc.edu>
Message-ID: <200403211759.07461.atropin75@t-online.de>

First of all a big thanks for the fast answer.
>
> Using
> Package: nlme
> Version: 3.1-48
> Date: 2004/01/14
> Priority: recommended
> Title: Linear and nonlinear mixed effects models
> Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
>         Bates <bates at stat.wisc.edu>, Saikat DebRoy
>         <saikat at stat.wisc.edu>, and Deepayan Sarkar
>         <deepayan at stat.wisc.edu>
> Maintainer: Douglas Bates <bates at stat.wisc.edu>

how do i get this information ?
<version> gives me only information about R itself.

> The nlme package is a required package and should be installed with
> any binary version of R.  You should not need to install that package
> separately.

Well, i just reinstalled the whole r-package, so normally i shouldn't expect 
any errors while trying the examples. I think i will try again to deinstall R 
and reinstall it.

> I don't think you could have used that syntax.  You would have needed
> something like
>
>  random = ~ respVar1+...+respVar20| groupingVariable_level2
> and that is almost certainly not what you want to try to fit.  (BTW, I
> think you have written respVar where you meant expVar.)

Yes, i meant expVar



From machud at inferenzsysteme.informatik.tu-darmstadt.de  Sun Mar 21 19:08:19 2004
From: machud at inferenzsysteme.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sun, 21 Mar 2004 19:08:19 +0100 (CET)
Subject: [R] Trellis plot in multiple display with grid
Message-ID: <Pine.LNX.4.58.0403211900010.3483@kika.intellektik.informatik.tu-darmstadt.de>

Dear all,

I would like to plot a multipanel display. Each plot should have the
curve over time of the solution produced by 10 different algorithms
(groups of data).

I handle this with:


xyplot(quality~time | inst, data=profiles, groups=alg,
       panel=panel.superpose,
       type=c("s"),
       as.table=TRUE,
       scales=list(relation="free",rot=c(0,90),cex=0.8),
       layout=c(3,3),
       main=list(label="Run time profile",cex=1.3),
       xlab=list(label="Normalised time",cex=1.3),
       ylab=list(label="Number of colours",cex=1.3),
       par.strip.text=list(cex=0.9),
       key=list(columns=1,cex=1,space="right",
         text=list(levels(as.factor(profiles$alg))),
         #points=Rows(sps,1:9),
         lines=Rows(spl,1:9)
         )
       )



My problems come when I try to add a grid to each of these plots.
I call this:


xyplot(quality~time | inst, data=profiles, groups=alg,
       #panel=panel.superpose,

       panel=function(x,y)
       {
         panel.grid(h = -1, v = -1, lty = 2)
         panel.xyplot(x,y,type=c("s"))
       },
       as.table=TRUE,
       scales=list(relation="free",rot=c(0,90),cex=0.8),
       layout=c(3,4),
       main=list(label="Run time profile",cex=1.3),
       xlab=list(label="Normalised time",cex=1.3),
       ylab=list(label="Number of colours",cex=1.3),
       par.strip.text=list(cex=0.9),
       key=list(columns=1,cex=1,corner=c(1,0),x=0.9,y=0.07,
         text=list(levels(as.factor(profiles$alg))),
         #points=Rows(sps,1:9),
         lines=Rows(spl,1:9)
         )
       )


but the result is that I am not anymore able to recognize the different
algorithms in each sinlge plot.

Is there anyone who can help me?


Thank you,


Marco Chiarandini


--
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit?t Darmstadt, Alexanderstrasse 10,
D-64283 Darmstadt - Germany, Office: S1/15 Raum 118
Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From deepayan at stat.wisc.edu  Sun Mar 21 20:10:32 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 21 Mar 2004 13:10:32 -0600
Subject: [R] Trellis plot in multiple display with grid
In-Reply-To: <Pine.LNX.4.58.0403211900010.3483@kika.intellektik.informatik.tu-darmstadt.de>
References: <Pine.LNX.4.58.0403211900010.3483@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <200403211310.32848.deepayan@stat.wisc.edu>

On Sunday 21 March 2004 12:08, Marco Chiarandini wrote:
> Dear all,
>
> I would like to plot a multipanel display. Each plot should have the
> curve over time of the solution produced by 10 different algorithms
> (groups of data).
>
> I handle this with:
>
>
> xyplot(quality~time | inst, data=profiles, groups=alg,
>        panel=panel.superpose,
>        type=c("s"),
>        as.table=TRUE,
>        scales=list(relation="free",rot=c(0,90),cex=0.8),
>        layout=c(3,3),
>        main=list(label="Run time profile",cex=1.3),
>        xlab=list(label="Normalised time",cex=1.3),
>        ylab=list(label="Number of colours",cex=1.3),
>        par.strip.text=list(cex=0.9),
>        key=list(columns=1,cex=1,space="right",
>          text=list(levels(as.factor(profiles$alg))),
>          #points=Rows(sps,1:9),
>          lines=Rows(spl,1:9)
>          )
>        )
>
>
>
> My problems come when I try to add a grid to each of these plots.
> I call this:
>
>
> xyplot(quality~time | inst, data=profiles, groups=alg,
>        #panel=panel.superpose,
>
>        panel=function(x,y)
>        {
>          panel.grid(h = -1, v = -1, lty = 2)
>          panel.xyplot(x,y,type=c("s"))
>        },

Instead, try 

        panel=function(x,y,...)
        {
          panel.grid(h = -1, v = -1, lty = 2)
          panel.superpose(x,y,...)
        },
        type = "s",


>        as.table=TRUE,
>        scales=list(relation="free",rot=c(0,90),cex=0.8),
>        layout=c(3,4),
>        main=list(label="Run time profile",cex=1.3),
>        xlab=list(label="Normalised time",cex=1.3),
>        ylab=list(label="Number of colours",cex=1.3),
>        par.strip.text=list(cex=0.9),
>        key=list(columns=1,cex=1,corner=c(1,0),x=0.9,y=0.07,
>          text=list(levels(as.factor(profiles$alg))),
>          #points=Rows(sps,1:9),
>          lines=Rows(spl,1:9)
>          )
>        )
>
>
> but the result is that I am not anymore able to recognize the
> different algorithms in each sinlge plot.
>
> Is there anyone who can help me?
>
>
> Thank you,
>
>
> Marco Chiarandini
>
>
> --
> Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
> Technische Universit?t Darmstadt, Alexanderstrasse 10,
> D-64283 Darmstadt - Germany, Office: S1/15 Raum 118
> Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
> email: machud at intellektik.informatik.tu-darmstadt.de
> web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From machud at inferenzsysteme.informatik.tu-darmstadt.de  Sun Mar 21 20:23:48 2004
From: machud at inferenzsysteme.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sun, 21 Mar 2004 20:23:48 +0100 (CET)
Subject: [R] Trellis plot in multiple display with grid
In-Reply-To: <200403211310.32848.deepayan@stat.wisc.edu>
References: <Pine.LNX.4.58.0403211900010.3483@kika.intellektik.informatik.tu-darmstadt.de>
	<200403211310.32848.deepayan@stat.wisc.edu>
Message-ID: <Pine.LNX.4.58.0403212017580.3483@kika.intellektik.informatik.tu-darmstadt.de>

Thank you a lot! It works exactly as I desired.

I dare to ask you another detail about Trellis multiple display plots.
I would like to plot vertical lines in correspondence of the confidence
intervals with the function below in order to make easier the visual
comparison.

Is this possible? I tried using the same logic proposed in the previous
answer but it does not work.


 Dotplot(as.factor(alg) ~ Cbind(y,lower,upper) | class,
         data=all,subset=ss,
           pch=3,method="bars",pch.bar=2,
         # panel=function(x,y,...) {
         #   panel.abline(v=y)
         #   panel.superpose(x,y,...)
         # },
          scales = list(cex=1,
             x=list(relation="free"),
             y=list(alternating=c(1,1,1,1),
               labels=c(levels(all$alg)),
               at=c(1:nlevels(all$alg)))),
           main=list(cex=1.3,label="Multiple comparisons with Tukey's
                confidence intervals"),
           xlab=list(label="Percentage deviation from best
                   results",cex=1.3,between.columns=5,distance=rep(2,60),size=3),
           ylab="",
           aspect=0.6,as.table=TRUE);




On Sun, 21 Mar 2004, Deepayan Sarkar wrote:

> On Sunday 21 March 2004 12:08, Marco Chiarandini wrote:
> > Dear all,
> >
> > I would like to plot a multipanel display. Each plot should have the
> > curve over time of the solution produced by 10 different algorithms
> > (groups of data).
> >
> > I handle this with:
> >
> >
> > xyplot(quality~time | inst, data=profiles, groups=alg,
> >        panel=panel.superpose,
> >        type=c("s"),
> >        as.table=TRUE,
> >        scales=list(relation="free",rot=c(0,90),cex=0.8),
> >        layout=c(3,3),
> >        main=list(label="Run time profile",cex=1.3),
> >        xlab=list(label="Normalised time",cex=1.3),
> >        ylab=list(label="Number of colours",cex=1.3),
> >        par.strip.text=list(cex=0.9),
> >        key=list(columns=1,cex=1,space="right",
> >          text=list(levels(as.factor(profiles$alg))),
> >          #points=Rows(sps,1:9),
> >          lines=Rows(spl,1:9)
> >          )
> >        )
> >
> >
> >
> > My problems come when I try to add a grid to each of these plots.
> > I call this:
> >
> >
> > xyplot(quality~time | inst, data=profiles, groups=alg,
> >        #panel=panel.superpose,
> >
> >        panel=function(x,y)
> >        {
> >          panel.grid(h = -1, v = -1, lty = 2)
> >          panel.xyplot(x,y,type=c("s"))
> >        },
>
> Instead, try
>
>         panel=function(x,y,...)
>         {
>           panel.grid(h = -1, v = -1, lty = 2)
>           panel.superpose(x,y,...)
>         },
>         type = "s",
>
>
> >        as.table=TRUE,
> >        scales=list(relation="free",rot=c(0,90),cex=0.8),
> >        layout=c(3,4),
> >        main=list(label="Run time profile",cex=1.3),
> >        xlab=list(label="Normalised time",cex=1.3),
> >        ylab=list(label="Number of colours",cex=1.3),
> >        par.strip.text=list(cex=0.9),
> >        key=list(columns=1,cex=1,corner=c(1,0),x=0.9,y=0.07,
> >          text=list(levels(as.factor(profiles$alg))),
> >          #points=Rows(sps,1:9),
> >          lines=Rows(spl,1:9)
> >          )
> >        )
> >
> >
> > but the result is that I am not anymore able to recognize the
> > different algorithms in each sinlge plot.
> >
> > Is there anyone who can help me?
> >
> >
> > Thank you,
> >
> >
> > Marco Chiarandini
> >
> >
> > --
> > Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
> > Technische Universit?t Darmstadt, Alexanderstrasse 10,
> > D-64283 Darmstadt - Germany, Office: S1/15 Raum 118
> > Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
> > email: machud at intellektik.informatik.tu-darmstadt.de
> > web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>

--
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit?t Darmstadt, Alexanderstrasse 10,
D-64283 Darmstadt - Germany, Office: S1/15 Raum 118
Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From rn001 at cebas.csic.es  Sun Mar 21 21:47:33 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Sun, 21 Mar 2004 21:47:33 +0100
Subject: [R] formated output 
Message-ID: <200403212047.i2LKl5009626@natura.cebas.csic.es>

Hi all;
I need to create ASCII files as output from R and I'm using sink(), cat(), 
and paste() for this.
My problem is that the ASCII files hace several columns, and I would like to 
know if intermediate columns (the second one for example) could be alineated 
to the right. My values are integer ranging from 1 to 1000.

Thanks a lot.

Best regards,

Javier



From deepayan at stat.wisc.edu  Sun Mar 21 22:10:50 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 21 Mar 2004 15:10:50 -0600
Subject: [R] Trellis plot in multiple display with grid
In-Reply-To: <Pine.LNX.4.58.0403212017580.3483@kika.intellektik.informatik.tu-darmstadt.de>
References: <Pine.LNX.4.58.0403211900010.3483@kika.intellektik.informatik.tu-darmstadt.de>
	<200403211310.32848.deepayan@stat.wisc.edu>
	<Pine.LNX.4.58.0403212017580.3483@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <200403211510.50211.deepayan@stat.wisc.edu>

On Sunday 21 March 2004 13:23, Marco Chiarandini wrote:
> Thank you a lot! It works exactly as I desired.
>
> I dare to ask you another detail about Trellis multiple display
> plots. I would like to plot vertical lines in correspondence of the
> confidence intervals with the function below in order to make easier
> the visual comparison.
>
> Is this possible? I tried using the same logic proposed in the
> previous answer but it does not work.

This is from Hmisc, right ? I'm not too familiar with it. But I don't 
see the link with the previous example. What exactly is wrong with 
this ? Do you want to use a 'groups' argument ? 

>  Dotplot(as.factor(alg) ~ Cbind(y,lower,upper) | class,
>          data=all,subset=ss,
>            pch=3,method="bars",pch.bar=2,
>          # panel=function(x,y,...) {
>          #   panel.abline(v=y)
>          #   panel.superpose(x,y,...)
>          # },
>           scales = list(cex=1,
>              x=list(relation="free"),
>              y=list(alternating=c(1,1,1,1),
>                labels=c(levels(all$alg)),
>                at=c(1:nlevels(all$alg)))),
>            main=list(cex=1.3,label="Multiple comparisons with Tukey's
>                 confidence intervals"),
>            xlab=list(label="Percentage deviation from best
>                   
> results",cex=1.3,between.columns=5,distance=rep(2,60),size=3),
> ylab="",
>            aspect=0.6,as.table=TRUE);



From ray at mcs.vuw.ac.nz  Sun Mar 21 22:34:35 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 22 Mar 2004 09:34:35 +1200 (NZST)
Subject: [R] writing text on graphics' window
Message-ID: <200403212134.i2LLYZtn002907@tahi.mcs.vuw.ac.nz>

> From: Duncan Murdoch <dmurdoch at pair.com>
> Date: Sun, 21 Mar 2004 06:58:03 -0500
> 
> On Sun, 21 Mar 2004 11:07:01 +0000, you [sam.kemp2 at ntlworld.com] wrote:
> 
> >Does anyone know of a method for writing text to the graphics window, 
> >where there is *no* plot? Basically, I have developed a 'significance 
> >test' and I would like the output on the graphics window to say 
> >something about the input parameters and the stats of the significance test.
> 
> You need to make sure a graphics device is active and establish a
> coordinate system there.  The easiest way to do that is to make a call
> to plot() with everything turned off:
> 
>  plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")
> 
> You may also want to reduce the "margins" if you want your output to
> take up the full frame, e.g.
> 
>  oldmargins <- par(mar=c(0,0,0,0))
>  plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")
> 
An easier way to activate a graphics device and establish a coordinate
system is to call plot.new().


Try:
> plot.new()
> text(0, 0, "ABC")
> par("usr")
[1] -0.04  1.04 -0.04  1.04

Ray Brownrigg



From dmurdoch at pair.com  Sun Mar 21 22:38:25 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 21 Mar 2004 16:38:25 -0500
Subject: [R] formated output 
In-Reply-To: <200403212047.i2LKl5009626@natura.cebas.csic.es>
References: <200403212047.i2LKl5009626@natura.cebas.csic.es>
Message-ID: <9i2s50l67md8d1maatoeip1f98ijnem0pb@4ax.com>

On Sun, 21 Mar 2004 21:47:33 +0100, you wrote:

>Hi all;
>I need to create ASCII files as output from R and I'm using sink(), cat(), 
>and paste() for this.
>My problem is that the ASCII files hace several columns, and I would like to 
>know if intermediate columns (the second one for example) could be alineated 
>to the right. My values are integer ranging from 1 to 1000.

You want to look at the formatC function together with paste to build
up the lines you want.  For example,

> x <- sample(200,5)
> y <- sample(200,5)
> cat(paste(formatC(x, width=5), formatC(y, width=5)), sep='\n')
   84    36
  159    64
   55    78
   59    51
  132   107

Duncan Murdoch



From machud at inferenzsysteme.informatik.tu-darmstadt.de  Sun Mar 21 22:24:13 2004
From: machud at inferenzsysteme.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Sun, 21 Mar 2004 22:24:13 +0100 (CET)
Subject: [R] Trellis plot in multiple display with grid
In-Reply-To: <200403211510.50211.deepayan@stat.wisc.edu>
References: <Pine.LNX.4.58.0403211900010.3483@kika.intellektik.informatik.tu-darmstadt.de>
	<200403211310.32848.deepayan@stat.wisc.edu>
	<Pine.LNX.4.58.0403212017580.3483@kika.intellektik.informatik.tu-darmstadt.de>
	<200403211510.50211.deepayan@stat.wisc.edu>
Message-ID: <Pine.LNX.4.58.0403212217410.3483@kika.intellektik.informatik.tu-darmstadt.de>


> This is from Hmisc, right ? I'm not too familiar with it. But I don't
> see the link with the previous example. What exactly is wrong with
> this ? Do you want to use a 'groups' argument ?


Yes, it is from Hmisc but it requires Lattice and it works with Trellis.

I would like that in each of the displays, which are plotted according to
"class", some vertical lines appear. This vertical lines should be in
correspondence of "y,lower,upper".

I don't know if I need to specifies groups. I tried, but it does not help.

I tried using the part that below is commented but I get an error
message concering missing groups...



> >  Dotplot(as.factor(alg) ~ Cbind(y,lower,upper) | class,
> >          data=all,subset=ss,
> >            pch=3,method="bars",pch.bar=2,
> >          # panel=function(x,y,...) {
> >          #   panel.abline(v=y)
> >          #   panel.superpose(x,y,...)
> >          # },
> >           scales = list(cex=1,
> >              x=list(relation="free"),
> >              y=list(alternating=c(1,1,1,1),
> >                labels=c(levels(all$alg)),
> >                at=c(1:nlevels(all$alg)))),
> >            main=list(cex=1.3,label="Multiple comparisons with Tukey's
> >                 confidence intervals"),
> >            xlab=list(label="Percentage deviation from best
> >
> > results",cex=1.3,between.columns=5,distance=rep(2,60),size=3),
> > ylab="",
> >            aspect=0.6,as.table=TRUE);
>
>
>

--
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit?t Darmstadt, Alexanderstrasse 10,
D-64283 Darmstadt - Germany, Office: S1/15 Raum 118
Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From carlos.ortega at minorplanetspain.com  Mon Mar 22 01:21:15 2004
From: carlos.ortega at minorplanetspain.com (Carlos Ortega)
Date: Mon, 22 Mar 2004 01:21:15 +0100
Subject: [R] formated output 
In-Reply-To: <200403212047.i2LKl5009626@natura.cebas.csic.es>
Message-ID: <004e01c40fa3$995029b0$39ffa6d4@MinorplanetS>

Javier,

You can do that with "write.table".

Regards,
Carlos.


-----Mensaje original-----
De: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] En nombre de javier garcia - CEBAS
Enviado el: domingo, 21 de marzo de 2004 21:48
Para: r-help at stat.math.ethz.ch
Asunto: [R] formated output 


Hi all;
I need to create ASCII files as output from R and I'm using sink(), cat(), 
and paste() for this.
My problem is that the ASCII files hace several columns, and I would like to

know if intermediate columns (the second one for example) could be alineated

to the right. My values are integer ranging from 1 to 1000.

Thanks a lot.

Best regards,

Javier

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From stanimura-ngs at umin.ac.jp  Mon Mar 22 03:59:38 2004
From: stanimura-ngs at umin.ac.jp (Susumu Tanimura)
Date: Mon, 22 Mar 2004 11:59:38 +0900
Subject: [R] where is OOP?
Message-ID: <20040322115938.56e14dee.stanimura-ngs@umin.ac.jp>

Hi there,

Installation error was given when I tried to install pmg (Poor mans
GUI) http://wiener.math.csi.cuny.edu/pmg/index.html.

$ LANG=C sudo R CMD INSTALL pmg_0.6.tar.gz 
* Installing *source* package 'gtkpmg' ...
** R
** save image
Error in library(OOP) : There is no package called 'OOP'
Execution halted
/usr/lib/R/bin/INSTALL: line -116: 20554 Broken pipe             cat "/usr/lib/R/library/gtkpmg/R/gtkpmg"
ERROR: execution of package source for 'gtkpmg' failed
** Removing '/usr/lib/R/library/gtkpmg'

It may be solved if I install OOP in advance, but the OOP library was
not found.  Does anyone tell me where I can download it?



From stanimura-ngs at umin.ac.jp  Mon Mar 22 05:22:31 2004
From: stanimura-ngs at umin.ac.jp (Susumu Tanimura)
Date: Mon, 22 Mar 2004 13:22:31 +0900
Subject: [R] where is OOP?
In-Reply-To: <20040322115938.56e14dee.stanimura-ngs@umin.ac.jp>
References: <20040322115938.56e14dee.stanimura-ngs@umin.ac.jp>
Message-ID: <20040322132231.62477845.stanimura-ngs@umin.ac.jp>

Hi,

This is self-response.  I found OOP on http://www.omegahat.org/OOP/.
However, further error was given.

$ sudo R INSTALL OOP_0.4-2_R.tar.gz 
* Installing *source* package 'OOP' ...
** libs
gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES -mieee-fp  -fPIC  -O2 -m486 -fno-strength-reduce -c RtreeApply.c -o RtreeApply.o
gcc -shared -L/usr/local/lib -o OOP.so RtreeApply.o   
** R
** inst
** save image
[1] TRUE
Initializing OOP objects in database 1
[1] "initialize"
Error in .identC(class(Class), "classRepresentation") : 
	couldn't find function ".traceClassName"
Execution halted
ERROR: execution of package source for 'OOP' failed
** Removing '/usr/lib/R/library/OOP'

Dose anyone succeed to install OOP in R 1.8.1?



From ajayshah at mayin.org  Mon Mar 22 05:58:45 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 22 Mar 2004 10:28:45 +0530
Subject: [R] Operating on windows of data
Message-ID: <20040322045845.GA17018@igidr.ac.in>

> I have a data set that is comprised of, for simplicity, a vector of
> numbers that I want to march across+in overlapping windows of say 10
> values each, computing a couple of values for each window.  Is there
> +a vectorized way to do this, or do I truly need to resort to
> looping--I think so?  Any other clever thoughts?

I'm not sure this is clever, but I use the subset capabilities of
R. See

http://www.mayin.org/ajayshah/KB/R/ols.html

3 of the examples there are related to your question, though it's
still using loops. :-)

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From phddas at yahoo.com  Mon Mar 22 07:27:42 2004
From: phddas at yahoo.com (Fred J.)
Date: Sun, 21 Mar 2004 22:27:42 -0800 (PST)
Subject: [R] regex in R
Message-ID: <20040322062742.81845.qmail@web20501.mail.yahoo.com>

Hello
I could use some help here with trying to use perl
stype regex to extract the first group of letters
before a ( . )
so if I have a sting AACEE.adiid and wanting AACEE 
i <- "AACEE.adiid"
grep(".+\..?+",i,perl=T)
I must be doing somthing wrong but don't know what it
is?

thanks



From ggrothendieck at myway.com  Mon Mar 22 07:39:28 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 01:39:28 -0500 (EST)
Subject: [R] Operating on windows of data
Message-ID: <20040322063928.B11AB39A1@mprdmxin.myway.com>



You can retain the trick of using subset and still get
rid of the loop in:

   http://www.mayin.org/ajayshah/KB/R/EXAMPLES/rollingreg.R

by using sapply like this (untested):

dat <- sapply( seq(T-width), function(i) {
    model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
                i:(i+width-1))
    details <- summary.lm(model)
    tmp <- coefficients(model)
    c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
           R2 = details$r.squared, RMSE = details$sigma )
} )
dat <- as.data.frame(t(dat))
attach(dat)


Date:   Mon, 22 Mar 2004 10:28:45 +0530 
From:   Ajay Shah <ajayshah at mayin.org>
To:   <sdavis2 at mail.nih.gov>, <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Operating on windows of data 

 
> I have a data set that is comprised of, for simplicity, a vector of
> numbers that I want to march across+in overlapping windows of say 10
> values each, computing a couple of values for each window. Is there
> +a vectorized way to do this, or do I truly need to resort to
> looping--I think so? Any other clever thoughts?

I'm not sure this is clever, but I use the subset capabilities of
R. See

http://www.mayin.org/ajayshah/KB/R/ols.html

3 of the examples there are related to your question, though it's
still using loops. :-)

-- 
Ajay Shah Consultant
ajayshah at mayin.org Department of Economic Affairs
http://www.mayin.org/ajayshah Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Mon Mar 22 07:46:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 01:46:22 -0500 (EST)
Subject: [R] regex in R
Message-ID: <20040322064622.9A2FA39AB@mprdmxin.myway.com>



Here are two ways:


sub( "[.].*","" ,i )

unlist( strsplit( i, split="[.]" ) )[1]

---
Date:   Sun, 21 Mar 2004 22:27:42 -0800 (PST) 
From:   Fred J. <phddas at yahoo.com>
To:   r help <r-help at stat.math.ethz.ch> 
Subject:   [R] regex in R 

 
Hello
I could use some help here with trying to use perl
stype regex to extract the first group of letters
before a ( . )
so if I have a sting AACEE.adiid and wanting AACEE 
i <- "AACEE.adiid"
grep(".+\..?+",i,perl=T)
I must be doing somthing wrong but don't know what it
is?

thanks



From ripley at stats.ox.ac.uk  Mon Mar 22 07:47:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 06:47:37 +0000 (GMT)
Subject: [R] regex in R
In-Reply-To: <20040322062742.81845.qmail@web20501.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403220637380.2607-100000@gannet.stats>

On Sun, 21 Mar 2004, Fred J. wrote:

> I could use some help here with trying to use perl
> stype regex to extract the first group of letters
> before a ( . )
> so if I have a sting AACEE.adiid and wanting AACEE 
> i <- "AACEE.adiid"
> grep(".+\..?+",i,perl=T)
> I must be doing somthing wrong but don't know what it
> is?

First, see ?regexp, which says

     Patterns are described here as they would be printed by 'cat': do
     remember that backslashes need to be doubled in entering R
     character strings from the keyboard.

so you need to double \.

Second, your pattern is wrong.  You wanted the first ., so use

".+?\\..*"

in perl style, or just "[^.]+\\..+" in any style.

Second, grep tells you whether or not the pattern occurred.  If you want 
to extract it, you need to use sub and sub-expressions, as in

sub("(.+?)(\\..+)", "\\1", i, perl=TRUE)
sub("([^.]+)(\\..+)", "\\1", i)


Please do read the help pages before posting: they have the information 
and relevant examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hodgess at gator.uhd.edu  Mon Mar 22 08:18:07 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 22 Mar 2004 01:18:07 -0600
Subject: [R] integration in symbols
Message-ID: <200403220718.i2M7I7515426@gator.dt.uh.edu>

Dear R People:

Are there any functions for integration in R, please?
Not integrate....That return a numeric value.
I mean symbolic integration...the moral equivalent
of D and deriv, please.

Thanks in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ggrothendieck at myway.com  Mon Mar 22 08:54:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 02:54:37 -0500 (EST)
Subject: [R] integration in symbols
Message-ID: <20040322075437.D68373971@mprdmxin.myway.com>



The free yacas symbolic math package can output in C form and that is
often the same as R.  For example,

C:\usr\yacas>yacas
[... deleted startup message ...]
In> ToFile("out") WriteString(CForm(Integrate(x)1/x)); True;
Out> True;
In> quit
Quitting...

C:\usr\yacas>type out
log(x)
C:\usr\yacas>

... in R ...
> f <- function(x) NULL
> body(f) <- parse("/usr/yacas/out")
> f
function (x) 
log(x)
> 

With some effort you could probably make it even slicker.  
One method would be via the yacas dll.

---
Date:   Mon, 22 Mar 2004 01:18:07 -0600 
From:   Erin Hodgess <hodgess at gator.uhd.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] integration in symbols 

 
Dear R People:

Are there any functions for integration in R, please?
Not integrate....That return a numeric value.
I mean symbolic integration...the moral equivalent
of D and deriv, please.

Thanks in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From s-plus at wiwi.uni-bielefeld.de  Mon Mar 22 09:58:10 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Mon, 22 Mar 2004 09:58:10 +0100
Subject: [R] squashing some numbers
References: <20040319234732.82547.qmail@web20513.mail.yahoo.com>
Message-ID: <405EAAA2.6040300@wiwi.uni-bielefeld.de>

are you looking for s.th. like this?

# some data
m<-c(3,1,2,4,4,4,4,0,5,4,4,5,6,1,1,1)
m<-as.data.frame(matrix(m,4,4))
print(m)
# sort m -- if necessary
m<-m[order(m[,3]),]
m<-m[order(m[,2]),]
# compute sums
cum<-cumsum(m[,4])
# find rows
ind<-c(T,0!=diff(m[,2] )| 0!=diff(m[,3]))
ind<-c(ind[-1],T)
# combine result
mneu<-cbind(m[ind,-4],V4=diff(c(0,cum[ind])))
print(mneu)

initial matrix:
  V1 V2 V3 V4
1  3  4  5  6
2  1  4  4  1
3  2  4  4  1
4  4  0  5  1

result:
  V1 V2 V3 V4
4  4  0  5  1
3  2  4  4  2
1  3  4  5  6

result without sorting

  V1 V2 V3 V4
1  3  4  5  6
3  2  4  4  2
4  4  0  5  1


Peter Wolf

Fred J. wrote:

>Hello
>I have a data frame with many col.s and rows
>V4 V5 V6 V7
>3  4   5  6
>1  4   4  1
>2  4   4  1
>4  0   5  1
>
>since the data has the middle 2 rows with V5 and V6
>are equal, I need to produce
>V4 V5 V6 V7
>3  4   5  6
>            this line removed and the value V7 = 1 is 
>     
>            added to the next row V7 valuve making it
>2
>2  4   4  2
>4  0   5  1
>
>thanks a lot
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Karen.Richardson at insightful.com  Mon Mar 22 11:20:42 2004
From: Karen.Richardson at insightful.com (Karen Richardson)
Date: Mon, 22 Mar 2004 10:20:42 -0000
Subject: [R] COURSE:  Statistical Modelling - By Dr Bill Venables
Message-ID: <B796B8C05975394DA24E457D1985BDB42D22FD@uk2kexch01.insightful.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040322/71c03007/attachment.pl

From ajayshah at mayin.org  Mon Mar 22 11:48:41 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 22 Mar 2004 16:18:41 +0530
Subject: [R] Operating on windows of data
In-Reply-To: <20040322063928.B11AB39A1@mprdmxin.myway.com>
References: <20040322063928.B11AB39A1@mprdmxin.myway.com>
Message-ID: <20040322104841.GZ7720@igidr.ac.in>

On Mon, Mar 22, 2004 at 01:39:28AM -0500, Gabor Grothendieck wrote:
> You can retain the trick of using subset and still get
> rid of the loop in:
> 
>    http://www.mayin.org/ajayshah/KB/R/EXAMPLES/rollingreg.R
> 
> by using sapply like this (untested):
> 
> dat <- sapply( seq(T-width), function(i) {
>     model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
>                 i:(i+width-1))
>     details <- summary.lm(model)
>     tmp <- coefficients(model)
>     c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
>            R2 = details$r.squared, RMSE = details$sigma )
> } )
> dat <- as.data.frame(t(dat))
> attach(dat)

This brings me to a question I've always had about "the R way" of
avoiding loops. Yes, the sapply() approach above works. My question
is: Why is this much better than writing it using loops?

Loops tap into the intuition of millions of people who have grown up
around procedural languages. Atleast to a person like me, I can read
code involving loops effortlessly.

And I don't see how much faster the sapply() will be. Intuitively, we
may think that the sapply() results in C code getting executed (in the
R sources), while the for loop results in interpretation overhead, and
so the sapply() is surely faster. But when the body of the for loop
involves a weighty thing like a QR decomposition (for the OLS), that
would seem to dominate the cost - as far as I can tell.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From maechler at stat.math.ethz.ch  Mon Mar 22 12:19:41 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 22 Mar 2004 12:19:41 +0100
Subject: [R] Operating on windows of data
In-Reply-To: <20040322104841.GZ7720@igidr.ac.in>
References: <20040322063928.B11AB39A1@mprdmxin.myway.com>
	<20040322104841.GZ7720@igidr.ac.in>
Message-ID: <16478.52173.156236.65810@gargle.gargle.HOWL>

>>>>> "Ajay" == Ajay Shah <ajayshah at mayin.org>
>>>>>     on Mon, 22 Mar 2004 16:18:41 +0530 writes:

    Ajay> On Mon, Mar 22, 2004 at 01:39:28AM -0500, Gabor
    Ajay> Grothendieck wrote:

   >>   You can retain the trick of using subset and still get
   >>   rid of the loop in:
   >>   
   >>      http://www.mayin.org/ajayshah/KB/R/EXAMPLES/rollingreg.R
   >>   
   >>   by using sapply like this (untested):
   >>   
   >>   dat <- sapply( seq(T-width), function(i) {
   >>       model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
   >>                   i:(i+width-1))
   >>       details <- summary.lm(model)
   >>       tmp <- coefficients(model)
   >>       c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
   >>              R2 = details$r.squared, RMSE = details$sigma )
   >>   } )
   >>   dat <- as.data.frame(t(dat))
   >>   attach(dat)

    Ajay> This brings me to a question I've always had about
    Ajay> "the R way" of avoiding loops. Yes, the sapply()
    Ajay> approach above works. My question is: Why is this much
    Ajay> better than writing it using loops?

it's not, not at all.
And you are very much right in all you say below!

The important place for avoiding for() loops is in situation
where you can use truly vectorized operations instead,
e.g., replacing

  n <- length(x) ; r <- numeric(n) 
  for(i in 1:n) r[i] <- sin(x[i])

by  r <- sin(x).

Replacing for() loops with sapply() / lapply() can save some
computing time -- as you remark below -- particular when the
function which they apply is simple -- and saving that time in
"inner" computations can become important.
OTOH, replacing an `outermost' for() loop with a `heavy' body,
as in the example above, is not at all the "R way"
{It may have been the S(-plus) way many years ago, when S was
 particularly unfortunately dealing with for() loops.}

Regards, Martin

    Ajay> Loops tap into the intuition of millions of people who
    Ajay> have grown up around procedural languages. Atleast to
    Ajay> a person like me, I can read code involving loops
    Ajay> effortlessly.

    Ajay> And I don't see how much faster the sapply() will
    Ajay> be. Intuitively, we may think that the sapply()
    Ajay> results in C code getting executed (in the R sources),
    Ajay> while the for loop results in interpretation overhead,
    Ajay> and so the sapply() is surely faster. But when the
    Ajay> body of the for loop involves a weighty thing like a
    Ajay> QR decomposition (for the OLS), that would seem to
    Ajay> dominate the cost - as far as I can tell.



From ripley at stats.ox.ac.uk  Mon Mar 22 12:22:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 11:22:21 +0000 (GMT)
Subject: [R] Operating on windows of data
In-Reply-To: <20040322104841.GZ7720@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0403221114560.5067-100000@gannet.stats>

On Mon, 22 Mar 2004, Ajay Shah wrote:

> On Mon, Mar 22, 2004 at 01:39:28AM -0500, Gabor Grothendieck wrote:
> > You can retain the trick of using subset and still get
> > rid of the loop in:
> > 
> >    http://www.mayin.org/ajayshah/KB/R/EXAMPLES/rollingreg.R
> > 
> > by using sapply like this (untested):
> > 
> > dat <- sapply( seq(T-width), function(i) {
> >     model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
> >                 i:(i+width-1))
> >     details <- summary.lm(model)
> >     tmp <- coefficients(model)
> >     c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
> >            R2 = details$r.squared, RMSE = details$sigma )
> > } )
> > dat <- as.data.frame(t(dat))
> > attach(dat)
> 
> This brings me to a question I've always had about "the R way" of
> avoiding loops. Yes, the sapply() approach above works. My question
> is: Why is this much better than writing it using loops?

Why didn't you test it yourself and find out?  That's part of `doing your 
homework' mentioned in the posting guide.

> Loops tap into the intuition of millions of people who have grown up
> around procedural languages. Atleast to a person like me, I can read
> code involving loops effortlessly.
> 
> And I don't see how much faster the sapply() will be. Intuitively, we
> may think that the sapply() results in C code getting executed (in the
> R sources), while the for loop results in interpretation overhead, and
> so the sapply() is surely faster. But when the body of the for loop
> involves a weighty thing like a QR decomposition (for the OLS), that
> would seem to dominate the cost - as far as I can tell.

See `Writing R Extensions' for ways to time and profile R code, and `S
Programming' for some worked examples (but R has changed a lot since
2000).  It's not `would seem to': if you do your homework you will know.

Loops are often the clearest and sometimes the most efficient way in R.  
You should be worrying about what is inside the loop first: in your case 
calling lmfit not lm, not calling summary.lm just to get sigma and 
r.squared, and so on.  Profile your code, find the bottlenecks and 
eliminate them one by one.  If profiling shows that .Fortran("dqrls" 
dominates, you know that little speed-up is possible without finding 
different compiled code.  (BTW, up/downdating a QR decomposition seems to 
be the most promising approach to windowing a regression.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tplate at acm.org  Sat Mar 20 22:21:34 2004
From: tplate at acm.org (Tony Plate)
Date: Sat, 20 Mar 2004 14:21:34 -0700
Subject: [R] [R-pkgs] new version of abind()
Message-ID: <6.0.3.0.2.20040320142045.090d76d8@wheresmymailserver.com>

There is a new version of the abind package on CRAN (abind_1.1-0).  abind() 
is a multi-dimensional generalization of cbind() and rbind() -- it can bind 
multiple 2-d matrices into a 3-d array, or bind 3-d arrays together, etc.

In this new version the behavior of the function abind() has been enhanced 
slightly (it can now accept a list as the first argument, removing the need 
to use do.call() in most situtations), and changed slightly (to no longer 
by default creates dimension names if none exist, because these are usually 
useless and mostly annoying).  There is also a new function in the package: 
adrop(), which does approximately what drop() does, but allows control over 
which dimensions are dropped.

Note also that the abind() function runs under S-plus as well (I believe 
adrop() does as well, but it is not as well tested).

Details:

2004-03-12

         * R/abind.R man/abind.Rd
         allow first argument of abind() to be a list of objects to be
         bound -- this avoids the need for do.call() when one wants to bind
         a list of objects

         * R/abind.R man/abind.Rd
         changed argument name 'use.anon.names' to the more intuitive
         'make.names' (the argument 'use.anon.names' still works)

         * R/abind.R man/abind.Rd
         changed default value for 'make.names' to FALSE (now more closely
         behaves like rbind() and cbind()).  This means that dimension
           names for dimensions that have no names are only constructed
           when requested, not by default.

         * R/adrop.R man/adrop.Rd
         added new function adrop().  This is a function like drop(), but it
           allows to user to specify which of the dimensions with extent one
           will be dropped.


Please let me know of any problems.

-- Tony Plate

Tony Plate   tplate at acm.org

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From david.meyer at wu-wien.ac.at  Mon Mar 22 13:14:06 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 22 Mar 2004 13:14:06 +0100
Subject: [R] SVM question
In-Reply-To: <200403191134.i2JB5oTH029495@hypatia.math.ethz.ch>
References: <200403191134.i2JB5oTH029495@hypatia.math.ethz.ch>
Message-ID: <20040322131406.347ff4ee.david.meyer@wu-wien.ac.at>

>I have a question concerning the svm in the e1071 package.
>I trained the svm by a set of samples, doing a 10 cross validation.
>The summary function then prints out the total accuracy and single
>accuracies, >works fine.

>My question is then: Is it possible to get classification results per
>cross >validation out the svm? I mean e.g. numbers about the true
>positives ,fp,fn,tf ?

No, because the accuracies are not computed using a confusion matrix.
The computation is internally done in C.

>How do I get a list of the classified examples ? 


-- 
Dr. David Meyer
Department of Information Systems

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/Wer_sind_wir/meyer/



From p.dalgaard at biostat.ku.dk  Mon Mar 22 13:49:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Mar 2004 13:49:04 +0100
Subject: [R] Operating on windows of data
In-Reply-To: <20040322104841.GZ7720@igidr.ac.in>
References: <20040322063928.B11AB39A1@mprdmxin.myway.com>
	<20040322104841.GZ7720@igidr.ac.in>
Message-ID: <x2r7vlgi7z.fsf@biostat.ku.dk>

Ajay Shah <ajayshah at mayin.org> writes:

> This brings me to a question I've always had about "the R way" of
> avoiding loops. Yes, the sapply() approach above works. My question
> is: Why is this much better than writing it using loops?

It's often not so much a matter of avoiding the loops as such. Splus
had a memory-management issue that caused for loops to be extremely
slow, but with R it is often a toss-up. The real power of sapply() and
friends is that they offer a convenient and compact way of looping
*and* collecting results.

Compare

vectorize <- function(FUN)function(x)sapply(x,FUN)

to 

vectorize <- function(FUN) function(x){
        res <- numeric(length(x))
        for (i in seq(along=x))
           res[i] <- FUN(x[i])
        res
    }

and notice that they don't actually do the same thing since sapply is
better at adjusting to the return value of FUN. On the other hand, the
2nd version might actually be faster if FUN is know to return numeric
values only.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fzoellne at TechFak.Uni-Bielefeld.DE  Mon Mar 22 13:53:47 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Mon, 22 Mar 2004 13:53:47 +0100
Subject: [R] SVM question
In-Reply-To: <20040322131406.347ff4ee.david.meyer@wu-wien.ac.at>
References: <200403191134.i2JB5oTH029495@hypatia.math.ethz.ch>
	<20040322131406.347ff4ee.david.meyer@wu-wien.ac.at>
Message-ID: <20040322125347.GA18463@hindemith.TechFak.Uni-Bielefeld.DE>

On Mon, Mar 22, 2004 at 01:14:06PM +0100, David Meyer wrote:
> >I have a question concerning the svm in the e1071 package.
> >I trained the svm by a set of samples, doing a 10 cross validation.
> >The summary function then prints out the total accuracy and single
> >accuracies, >works fine.
> 
> >My question is then: Is it possible to get classification results per
> >cross >validation out the svm? I mean e.g. numbers about the true
> >positives ,fp,fn,tf ?
> 
> No, because the accuracies are not computed using a confusion matrix.
> The computation is internally done in C.

So there is no way to recieve a list of the examples and its assigned class which has been used for testing the svm during the cross validation?  

Yours,

-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From andy_liaw at merck.com  Mon Mar 22 13:59:57 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 22 Mar 2004 07:59:57 -0500
Subject: [R] SVM question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A42@usrymx25.merck.com>

If you really want such features, you have two options:

-  Modify the C and R code yourself.
-  Send request to the author of libsvm, Chih-Jen Lin.

Andy

> From: Frank Gerrit Zoellner
> 
> On Mon, Mar 22, 2004 at 01:14:06PM +0100, David Meyer wrote:
> > >I have a question concerning the svm in the e1071 package.
> > >I trained the svm by a set of samples, doing a 10 cross validation.
> > >The summary function then prints out the total accuracy and single
> > >accuracies, >works fine.
> > 
> > >My question is then: Is it possible to get classification 
> results per
> > >cross >validation out the svm? I mean e.g. numbers about the true
> > >positives ,fp,fn,tf ?
> > 
> > No, because the accuracies are not computed using a 
> confusion matrix.
> > The computation is internally done in C.
> 
> So there is no way to recieve a list of the examples and its 
> assigned class which has been used for testing the svm during 
> the cross validation?  
> 
> Yours,
> 
> -- 
> Frank G. Zoellner
> AG Angewandte Informatik
> Technische Fakult"at
> Universit"at Bielefeld
> phone: +49(0)521-106-2951
> fax:   +49(0)521-106-2992
> email: fzoellne at techfak.uni-bielefeld.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From m.dewey at iop.kcl.ac.uk  Mon Mar 22 14:15:11 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Mon, 22 Mar 2004 13:15:11 +0000
Subject: [R] Lattice, skip= and layout= problem, plotting object from nlme
 output
Message-ID: <5.2.1.1.0.20040322130227.00a2ed60@pop.freeserve.net>

I generate a groupedData object

library(nlme)
obj <- groupedData(mg10 ~ time | gp, data = common, outer = ~pct)

gp has 101 levels, and pct has 3. There are 38, 25, 38 gps in each of the 
levels of pct respectively.

I fit my model

fit.rtg <- lme(mg10 ~ time * group,
    data = obj,
    random = ~time * group | gp)

Now I try to plot the results. I would like to print 40 panels on each page 
and have a new level of pct start a new page. The effect of using outer in 
the call of groupedData is that the values of gp are presented by pct.

plot.rtg <- plot(augPred(fit.rtg),
    skip = c(rep(FALSE, 38), TRUE, TRUE,
             rep(FALSE, 25), rep(TRUE, 15),
             rep(FALSE, 38), TRUE, TRUE),
    layout = c(5, 8, 3),
    strip = FALSE
)

What I get is 38 panels on page 1 with 2 blank panels top right. I had 
hoped to get 25 on the next page with 15 blanks but I get 38 again with 2 
blanks.

If I alter layout to (say) layout = c(12, 10) I get blanks as expected on 
the single page produced so I surmise that skip is forcing the same format 
on each page. There is an example in \cite{pinheiro00} which suggests that 
what I wanted can be done (p113,  p445-447) so I suspect I am doing 
something stupid here.

I am using R 1.8.1 with the versions of lattice and nlme that came with it. 
I am using Windows 98SE if that is relevant.

@BOOK{pinheiro00,
   author = {Pinheiro, J C and Bates, D M},
   year = 2000,
   title = {Mixed-effects models in {S} and {S-PLUS}},
   publisher = {Springer-Verlag}
}


Michael Dewey
m.dewey at iop.kcl.ac.uk



From ggrothendieck at myway.com  Mon Mar 22 14:33:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 08:33:33 -0500 (EST)
Subject: [R] Operating on windows of data
Message-ID: <20040322133333.E710539C1@mprdmxin.myway.com>


> On Mon, Mar 22, 2004 at 01:39:28AM -0500, Gabor Grothendieck wrote:
> > You can retain the trick of using subset and still get
> > rid of the loop in:
> > 
> > http://www.mayin.org/ajayshah/KB/R/EXAMPLES/rollingreg.R
> > 
> > by using sapply like this (untested):
> > 
> > dat <- sapply( seq(T-width), function(i) {
> > model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
> > i:(i+width-1))
> > details <- summary.lm(model)
> > tmp <- coefficients(model)
> > c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
> > R2 = details$r.squared, RMSE = details$sigma )
> > } )
> > dat <- as.data.frame(t(dat))
> > attach(dat)
> 
> This brings me to a question I've always had about "the R way" of
> avoiding loops. Yes, the sapply() approach above works. My question
> is: Why is this much better than writing it using loops?
> 
> Loops tap into the intuition of millions of people who have grown up
> around procedural languages. Atleast to a person like me, I can read
> code involving loops effortlessly.
> 
> And I don't see how much faster the sapply() will be. Intuitively, we
> may think that the sapply() results in C code getting executed (in the
> R sources), while the for loop results in interpretation overhead, and
> so the sapply() is surely faster. But when the body of the for loop
> involves a weighty thing like a QR decomposition (for the OLS), that
> would seem to dominate the cost - as far as I can tell.

Its true that vectorizing loops can make it faster but in my
mind the main advantage is the conceptual one of working with
whole objects at a time and consequently reduction of code size.

Admittedly, the example above does not get you much although
even here it is slightly shorter than the loop version as it 
puts the arrays together for you rather than making you set them 
up yourself.  Also, not shown, there are subsequent 
summary() statements in your file and there would be
a further opportunity for code reduction 
since now your data is in a data frame rather than 
individual vectors so you could combine all the summary statements
into one.

Its really not the best example of the desired approach since 
it chickens out and uses the loop indices to sapply() over but 
I don't think one can expect a complete win for the 
vectorized approach in every single case.



From pauljohn at ku.edu  Mon Mar 22 16:39:57 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Mon, 22 Mar 2004 09:39:57 -0600
Subject: [R] solved mystery of difference between glmmPQL and lme
Message-ID: <405F08CD.8020607@ku.edu>

I asked a few days ago about the difference in results I saw between the 
MASS function glmmPQL (due to Venables and Ripley) and the lme function 
from the package nlme (due to Pinheiro and Bates).  When the two tools 
apply to the same model (gaussian, link=identity, correlation=AR1), I 
was getting different results and wondered if there was an argument in 
favor of one or the other.

Several list readers emailed me to point out that glmmPQL is repeatedly 
calling lme, so if a model really can be estimated by lme, then lme is 
the more appropriate one because it is maximum likelihood, rather than 
quasi-likelihood.

That did not explain the difference in results, so I read the source 
code for  glmmPQL and learned that it sets the method for lme fitting to 
  "ML".  In contrast, lme defaults to "REML".  The estimates from 
glmmPQL and lme (method="ML") are identical in my test case.

pj
-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From nedluk at yahoo.it  Mon Mar 22 16:33:34 2004
From: nedluk at yahoo.it (=?iso-8859-1?q?michele=20lux?=)
Date: Mon, 22 Mar 2004 16:33:34 +0100 (CET)
Subject: [R] problem with seasonal arima
Message-ID: <20040322153334.71910.qmail@web13607.mail.yahoo.com>

hallo to all 
I've to calculate an arima model and I need only the
first and 365 th parameter and also the sar1 and  the
intercept, so I'm traing with:
arima(X,order=c(365,0,0),seasonal=list(order=c(1,0,0),..),fixed=c(NA,rep(0,363),NA,NA,NA),transform.pars=F)

but the error answer is:
Error in polyroot(z) : polynomial degree too high (49
max)

also there are problems in allocating memory (I've 512
mb ram)
may be somebody could help me?
can R do it?
thanks 
michele 

______________________________________________________________________

http://it.yahoo.com/mail_it/foot/?http://it.mail.yahoo.com/



From mario.reis at ucl.ac.uk  Mon Mar 22 17:22:51 2004
From: mario.reis at ucl.ac.uk (Mario dos Reis)
Date: Mon, 22 Mar 2004 16:22:51 +0000
Subject: [R] Setting the 'fig' graphic parameter
In-Reply-To: <200403221109.i2MB3MNo028106@hypatia.math.ethz.ch>
References: <200403221109.i2MB3MNo028106@hypatia.math.ethz.ch>
Message-ID: <1079972571.4538.2.camel@fdosr01.laptop.cryst.bbk.ac.uk>

Hi guys,

# I would like to plot a figure with the following layout:
#
#  ----------------------------
#  |                 |        |
#  |                 |        |
#  |                 |        |
#  |                 |--------|
#  |                 |        |
#  |                 |        |
#  |                 |        |
#  ----------------------------

x <- rnorm(100)
y <- rnorm(100)

par(fig=c(0,0.7,0,1))
plot(x,y)

# (please maximise your plotting device so you get a 'rectangular' area)

# now lets do the upper corner 'little' plot

par(fig=c(0.7, 1, 0.5, 1))
plot(x,y)

# and ...

par(fig=c(0.7, 1, 0, 0.5))
plot(x,y)

Now, my problem is as you might have seen already, that the old figure
gets deleted when the new one is placed. I was trying to reproduce an
exercise a saw in an S-plus book. I would really like to know what is
going on, the documentation about graphic parameters is not very helpful
about fig, and I would really like to set a graph with the above layout.

Thanks,
Mario dos Reis.



From mathieu_chausson at caramail.com  Mon Mar 22 17:26:22 2004
From: mathieu_chausson at caramail.com (mathieu chausson )
Date: Mon, 22 Mar 2004 16:26:22 GMT 
Subject: [R] =?windows-1252?q?_D=E9butant_R?=
Message-ID: <1079972782018190@lycos-europe.com>

Bonjour à tous,
Je débute dans l'utilisation de R, et suis à la recherche de toute piste me permettant l'introduction à ce logiciel. 
J'ai pour le moment la fiche d'Emmanuel Paradis: "R pour les débutants", mais j'aimerais savoir où je peut trouver 
d'autres documentations notamment pour le traitement statistique de données (Anova, modèle linéaire et 
modèle linéaire généralisé), les exemples pas à pas étant les bienvenus.
Merci d'avance.

Marre des Spams ? - http://www.caramailmax.com


From rado.bonk at jrc.it  Mon Mar 22 17:27:21 2004
From: rado.bonk at jrc.it (Rado Bonk)
Date: Mon, 22 Mar 2004 17:27:21 +0100
Subject: [R] R commands formating for LaTeX
Message-ID: <405F13E9.8060700@jrc.it>

Dear R users,

I have to include typical UNIX formated R commands and outputs into the 
article using LaTeX.

What is the easiest way to include R prompt commands and text outputs 
(including messages) into LaTeX documents? Is there any template or library?

So far I'm trying to produce R commands in LaTeX using LaTeX general 
format options.

Thanks,

Rado



From pete at smtl.co.uk  Mon Mar 22 17:30:39 2004
From: pete at smtl.co.uk (Pete Phillips)
Date: Mon, 22 Mar 2004 16:30:39 -0000
Subject: [R] how to loop through names ?
Message-ID: <200403060952.i269qVje011454@lap1.smtl.co.uk>


Hi

I'm sure I'm missing something very straighforward here  :-(

I have a data set 'sales' as follows:
==========================================
# read in the sales data
sales<-read.table("sales.dat",header=TRUE);
#generate a serial field
sales$serial=c(1:24)

sales

   an  l ml  ne ni total serial
1  43 25 35  51 17 69    1
2  38 18 47  94 3  99    2
.......
24 58 13 41  95  4 1     24
===========================================

(extra rows and columns deleted).

I wish to produce a postscript plot file for each column plotted vs the
serial column, using either the 1st to 12th row or the 13th to 24th
rows, where the filename consists of the column name with '.ps' added.

This the code I have so far:

============================================
for (i in 1:(length(names(sales))-1)) {

fname <- paste(names(sales)[[i]],".ps",sep="")

postscript(file=fname)

plot(
sales$serial[13:24], 
sales[names(sales)[[i]]][13:24],
xlab="Month No", ylab="No/month")
 dev.off()

}
==============================================

The filename generation works (yay!), but I think I have missed
something very basic here as that plot line seems too complex (and
doesn't work!).

Any ideas please ?
 
Pete
--
Pete Phillips, Deputy Director,     |   http://www.smtl.co.uk/
Surgical Materials Testing Lab,     |   http://www.worldwidewounds.com/
Princess of Wales Hospital, S Wales |   http://www.dressings.org/
Tel/Fax: +44 1656-752820/30         |   pete at smtl.co.uk



From dray at biomserv.univ-lyon1.fr  Mon Mar 22 17:33:26 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Mon, 22 Mar 2004 11:33:26 -0500
Subject: =?iso-8859-1?Q?Re:_[R]__D=E9butant_R?=
In-Reply-To: <1079972782018190@lycos-europe.com>
Message-ID: <5.2.1.1.0.20040322113033.018cfd10@biomserv.univ-lyon1.fr>

Hello,
if you want to have more answer, speak English, "official language" of this 
list.
But as you are french, you can have a look at 
http://pbil.univ-lyon1.fr/R/enseignement.html where you will find a lot of 
(french) statistical course with R


At 11:26 22/03/2004, mathieu chausson wrote:
>Bonjour ? tous,
>Je d?bute dans l'utilisation de R, et suis ? la recherche de toute piste 
>me permettant l'introduction ? ce logiciel.
>J'ai pour le moment la fiche d'Emmanuel Paradis: "R pour les d?butants", 
>mais j'aimerais savoir o? je peut trouver
>d'autres documentations notamment pour le traitement statistique de 
>donn?es (Anova, mod?le lin?aire et
>mod?le lin?aire g?n?ralis?), les exemples pas ? pas ?tant les bienvenus.
>Merci d'avance.
>
>Marre des Spams ? - http://www.caramailmax.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From ripley at stats.ox.ac.uk  Mon Mar 22 17:32:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 16:32:26 +0000 (GMT)
Subject: [R] Setting the 'fig' graphic parameter
In-Reply-To: <1079972571.4538.2.camel@fdosr01.laptop.cryst.bbk.ac.uk>
Message-ID: <Pine.LNX.4.44.0403221631060.1121-100000@gannet.stats>

R is not S-PLUS, and you need par(new=TRUE) before calling par(fig) etc.

That is probably not an intentional difference.


On Mon, 22 Mar 2004, Mario dos Reis wrote:

> Hi guys,
> 
> # I would like to plot a figure with the following layout:
> #
> #  ----------------------------
> #  |                 |        |
> #  |                 |        |
> #  |                 |        |
> #  |                 |--------|
> #  |                 |        |
> #  |                 |        |
> #  |                 |        |
> #  ----------------------------
> 
> x <- rnorm(100)
> y <- rnorm(100)
> 
> par(fig=c(0,0.7,0,1))
> plot(x,y)
> 
> # (please maximise your plotting device so you get a 'rectangular' area)
> 
> # now lets do the upper corner 'little' plot
> 
> par(fig=c(0.7, 1, 0.5, 1))
> plot(x,y)
> 
> # and ...
> 
> par(fig=c(0.7, 1, 0, 0.5))
> plot(x,y)
> 
> Now, my problem is as you might have seen already, that the old figure
> gets deleted when the new one is placed. I was trying to reproduce an
> exercise a saw in an S-plus book. I would really like to know what is
> going on, the documentation about graphic parameters is not very helpful
> about fig, and I would really like to set a graph with the above layout.
> 
> Thanks,
> Mario dos Reis.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Mar 22 17:37:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 16:37:20 +0000 (GMT)
Subject: [R] problem with seasonal arima
In-Reply-To: <20040322153334.71910.qmail@web13607.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403221634540.1121-100000@gannet.stats>

You won't be able to do it with arima or arima0.  This is not a sensible 
way to fit such a model, which is just 365 uncoupled time series.
Just do a OLS regression on year-lagged values, or write some code to 
combine the likelihoods from the 365 series and maximize the combined 
likelihood.

On Mon, 22 Mar 2004, michele lux wrote:

> hallo to all 
> I've to calculate an arima model and I need only the
> first and 365 th parameter and also the sar1 and  the
> intercept, so I'm traing with:
> arima(X,order=c(365,0,0),seasonal=list(order=c(1,0,0),..),fixed=c(NA,rep(0,363),NA,NA,NA),transform.pars=F)
> 
> but the error answer is:
> Error in polyroot(z) : polynomial degree too high (49
> max)
> 
> also there are problems in allocating memory (I've 512
> mb ram)
> may be somebody could help me?
> can R do it?
> thanks 
> michele 
> 
> ______________________________________________________________________
> 
> http://it.yahoo.com/mail_it/foot/?http://it.mail.yahoo.com/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrewr at uidaho.edu  Mon Mar 22 17:45:03 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 22 Mar 2004 08:45:03 -0800
Subject: [R] R commands formating for LaTeX
In-Reply-To: <405F13E9.8060700@jrc.it>
References: <405F13E9.8060700@jrc.it>
Message-ID: <200403220845.03079.andrewr@uidaho.edu>

Rado,

a simple solution is to use the verbatim environment in LaTeX:

\begin{verbatim}

... R input and output pasted here ...

\end{verbatim}

However, I very strongly recommend that you consider using Sweave instead. 

Good luck,

Andrew

On Monday 22 March 2004 08:27, Rado Bonk wrote:
> Dear R users,
>
> I have to include typical UNIX formated R commands and outputs into the
> article using LaTeX.
>
> What is the easiest way to include R prompt commands and text outputs
> (including messages) into LaTeX documents? Is there any template or
> library?
>
> So far I'm trying to produce R commands in LaTeX using LaTeX general
> format options.
>
> Thanks,
>
> Rado
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From MSchwartz at MedAnalytics.com  Mon Mar 22 17:40:51 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 22 Mar 2004 10:40:51 -0600
Subject: [R] Setting the 'fig' graphic parameter
In-Reply-To: <1079972571.4538.2.camel@fdosr01.laptop.cryst.bbk.ac.uk>
References: <200403221109.i2MB3MNo028106@hypatia.math.ethz.ch>
	<1079972571.4538.2.camel@fdosr01.laptop.cryst.bbk.ac.uk>
Message-ID: <1079973650.7675.125.camel@localhost.localdomain>

On Mon, 2004-03-22 at 10:22, Mario dos Reis wrote:
> Hi guys,
> 
> # I would like to plot a figure with the following layout:
> #
> #  ----------------------------
> #  |                 |        |
> #  |                 |        |
> #  |                 |        |
> #  |                 |--------|
> #  |                 |        |
> #  |                 |        |
> #  |                 |        |
> #  ----------------------------
> 
> x <- rnorm(100)
> y <- rnorm(100)
> 
> par(fig=c(0,0.7,0,1))
> plot(x,y)
> 
> # (please maximise your plotting device so you get a 'rectangular' area)
> 
> # now lets do the upper corner 'little' plot
> 
> par(fig=c(0.7, 1, 0.5, 1))
> plot(x,y)
> 
> # and ...
> 
> par(fig=c(0.7, 1, 0, 0.5))
> plot(x,y)
> 
> Now, my problem is as you might have seen already, that the old figure
> gets deleted when the new one is placed. I was trying to reproduce an
> exercise a saw in an S-plus book. I would really like to know what is
> going on, the documentation about graphic parameters is not very helpful
> about fig, and I would really like to set a graph with the above layout.
> 
> Thanks,
> Mario dos Reis.


Try reading through chapter 12 in An Introduction to R, which covers
plotting basics. There are differences between R and S-PLUS.

You need to use par(new = TRUE) prior to the second and third plots,
otherwise each call to plot() clears the plot device.

Use the following:

x <- rnorm(100)
y <- rnorm(100)

par(fig=c(0,0.7,0,1))
plot(x,y)

par(new = TRUE)
par(fig=c(0.7, 1, 0.5, 1))
plot(x,y)

par(new = TRUE)
par(fig=c(0.7, 1, 0, 0.5))
plot(x,y)

That should get you what you want.

Also, take a look at the layout() function (?layout), which enables you
to define sections within the overall plotting area as do par("mfcol")
and par("mfrow").

HTH,

Marc Schwartz



From hdoran at NASDC.org  Mon Mar 22 17:41:19 2004
From: hdoran at NASDC.org (Harold Doran)
Date: Mon, 22 Mar 2004 11:41:19 -0500
Subject: [R] R commands formating for LaTeX
Message-ID: <66578BFC0BA55348B5907A0F798EE9306635BE@ernesto.NASDC.ORG>

Yes, there is a very good library called Sweave that allows for you to embed all R code, figures, tables into a LaTeX document.

HCD

-----Original Message-----
From: Rado Bonk [mailto:rado.bonk at jrc.it]
Sent: Monday, March 22, 2004 11:27 AM
To: r-help at stat.math.ethz.ch
Subject: [R] R commands formating for LaTeX


Dear R users,

I have to include typical UNIX formated R commands and outputs into the 
article using LaTeX.

What is the easiest way to include R prompt commands and text outputs 
(including messages) into LaTeX documents? Is there any template or library?

So far I'm trying to produce R commands in LaTeX using LaTeX general 
format options.

Thanks,

Rado

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmurdoch at pair.com  Mon Mar 22 17:42:23 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 22 Mar 2004 11:42:23 -0500
Subject: [R] how to loop through names ?
In-Reply-To: <200403060952.i269qVje011454@lap1.smtl.co.uk>
References: <200403060952.i269qVje011454@lap1.smtl.co.uk>
Message-ID: <ig5u505l7o2g0a7taliqfkh4dlgpuo4grj@4ax.com>

On Sat, 06 Mar 2004 09:52:31 +0000, "Pete Phillips" <pete at smtl.co.uk>
wrote :

>sales<-read.table("sales.dat",header=TRUE);
...

>for (i in 1:(length(names(sales))-1)) {
>
>fname <- paste(names(sales)[[i]],".ps",sep="")
>
>postscript(file=fname)
>
>plot(
>sales$serial[13:24], 
>sales[names(sales)[[i]]][13:24],
>xlab="Month No", ylab="No/month")
> dev.off()
>
>}
>==============================================
>
>The filename generation works (yay!), but I think I have missed
>something very basic here as that plot line seems too complex (and
>doesn't work!).

I think your indexing is off a bit.  "sales" is a data frame, so
something like sales[13:24, "an"] would select some rows from the
column named "an".  

Thus sales[13:24, names(sales)[i]] would work, or even more simply
sales[13:24, i].

You don't often need the double bracket indexing "[[i]]".  It is used
with lists, to select the element; names(sales) is a vector, not a
list, so it only needs single brackets.

Duncan Murdoch



From arv at ono.com  Mon Mar 22 17:50:53 2004
From: arv at ono.com (antonio rodriguez)
Date: Mon, 22 Mar 2004 17:50:53 +0100
Subject: =?iso-8859-1?Q?RE:_=5BR=5D__D=E9butant_R?=
In-Reply-To: <5.2.1.1.0.20040322113033.018cfd10@biomserv.univ-lyon1.fr>
Message-ID: <IPEFKICOHOECENGJBAGLMEJNDBAA.arv@ono.com>

And in:

http://zoonek2.free.fr/UNIX/48_R/all.html

an excellent starting point (in french)

Salut

Antonio

> -----Mensaje original-----
> De: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Stephane DRAY
> Enviado el: lunes, 22 de marzo de 2004 17:33
> Para: mathieu chausson ; Aide R
> Asunto: Re: [R] D?butant R
>
>
> Hello,
> if you want to have more answer, speak English, "official
> language" of this
> list.
> But as you are french, you can have a look at
> http://pbil.univ-lyon1.fr/R/enseignement.html where you will find
> a lot of
> (french) statistical course with R
>
>
> At 11:26 22/03/2004, mathieu chausson wrote:
> >Bonjour ? tous,
> >Je d?bute dans l'utilisation de R, et suis ? la recherche de toute piste
> >me permettant l'introduction ? ce logiciel.
> >J'ai pour le moment la fiche d'Emmanuel Paradis: "R pour les d?butants",
> >mais j'aimerais savoir o? je peut trouver
> >d'autres documentations notamment pour le traitement statistique de
> >donn?es (Anova, mod?le lin?aire et
> >mod?le lin?aire g?n?ralis?), les exemples pas ? pas ?tant les bienvenus.
> >Merci d'avance.
> >
> >Marre des Spams ? - http://www.caramailmax.com
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> St?phane DRAY
> ------------------------------------------------------------------
> --------------------------------
>
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
>
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> ------------------------------------------------------------------
> --------------------------------
>
> Web
> http://www.steph280.freesurf.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> ---
> Incoming mail is certified Virus Free.
> Checked by AVG anti-virus system (http://www.grisoft.com).
> Version: 6.0.624 / Virus Database: 401 - Release Date: 15/03/2004
>
---



From rolf at math.unb.ca  Mon Mar 22 17:56:01 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 22 Mar 2004 12:56:01 -0400 (AST)
Subject: [R] detach()
Message-ID: <200403221656.i2MGu1gs021034@erdos.math.unb.ca>


I got bitten recently by the following behaviour of detach();

 > save(file="Junk")
 > attach("Junk")
 > search()
 [1] ".GlobalEnv"      "file:Junk"       "package:methods" "package:ctest"  
 [5] "package:mva"     "package:modreg"  "package:nls"     "package:ts"     
 [9] "package:Misc"    "Autoloads"       "package:base"   
 > detach(2)
 > # No problem; reattach junk.
 > attach("Junk")
 > ind <- 2
 > detach(ind)
Error in detach(ind) : invalid name
 > is.numeric(ind)
[1] TRUE

The help on detach() says:

    name: The object to detach.  Defaults to 'search()[pos]'. This can
          be a name or a character string but _not_ a character vector. 

     pos: Index position in 'search()' of database to detach.  When
          'name' is 'numeric', 'pos = name' is used.           ^^^^ 
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Looking at the code of detach, I see that name gets assigned
substitute(name) before the check as to whether name is numeric.  So
if name is provided as an object --- like ind --- then name becomes
the name ``ind'' and is an object of mode "name" and so the check
will evaluate to FALSE.  (If name is given as an explicit constant
--- like 2 --- then name remains equal to 2 and is indeed numeric so
the check evaluates TRUE.)

Is this the intended behaviour for detach() or is it a bug?

It's obviously no big deal because detach(pos=ind) works perfectly.
But it did trip me up and could conceivably trip others.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ramasamy at cancer.org.uk  Mon Mar 22 18:12:58 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 22 Mar 2004 17:12:58 -0000
Subject: [R] how to loop through names ?
In-Reply-To: <200403060952.i269qVje011454@lap1.smtl.co.uk>
Message-ID: <ODEPICOHNDBJEHIFCIMPKEMBCBAA.ramasamy@cancer.org.uk>

Try plot( sales[ 13:24, "serial" ], sales[ 13:24, i ], xlab="Month No",
ylab="No/month")

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Pete Phillips
> Sent: 06 March 2004 09:53
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to loop through names ?
>
>
>
> Hi
>
> I'm sure I'm missing something very straighforward here  :-(
>
> I have a data set 'sales' as follows:
> ==========================================
> # read in the sales data
> sales<-read.table("sales.dat",header=TRUE);
> #generate a serial field
> sales$serial=c(1:24)
>
> sales
>
>    an  l ml  ne ni total serial
> 1  43 25 35  51 17 69    1
> 2  38 18 47  94 3  99    2
> .......
> 24 58 13 41  95  4 1     24
> ===========================================
>
> (extra rows and columns deleted).
>
> I wish to produce a postscript plot file for each column plotted vs the
> serial column, using either the 1st to 12th row or the 13th to 24th
> rows, where the filename consists of the column name with '.ps' added.
>
> This the code I have so far:
>
> ============================================
> for (i in 1:(length(names(sales))-1)) {
>
> fname <- paste(names(sales)[[i]],".ps",sep="")
>
> postscript(file=fname)
>
> plot(
> sales$serial[13:24],
> sales[names(sales)[[i]]][13:24],
> xlab="Month No", ylab="No/month")
>  dev.off()
>
> }
> ==============================================
>
> The filename generation works (yay!), but I think I have missed
> something very basic here as that plot line seems too complex (and
> doesn't work!).
>
> Any ideas please ?
>
> Pete
> --
> Pete Phillips, Deputy Director,     |   http://www.smtl.co.uk/
> Surgical Materials Testing Lab,     |   http://www.worldwidewounds.com/
> Princess of Wales Hospital, S Wales |   http://www.dressings.org/
> Tel/Fax: +44 1656-752820/30         |   pete at smtl.co.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Mon Mar 22 18:21:49 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 12:21:49 -0500 (EST)
Subject: [R] how to loop through names ?
Message-ID: <20040322172149.1286F39D8@mprdmxin.myway.com>


Try this simplification which loops through the names
rather than their index and improves plot a bit too:

   ix <- 13:24
   for( n in names(sales) ) {
     fname <- paste( n, "ps", sep="." )
     postscript( file = fname )
     plot( sales[ix,"serial"], sales[ix,n], 
         xlab = "Month No", ylab = "No/month", main = n )
     dev.off()
   }

or use the subset= argument to plot.formula:

   plot( sales[,n] ~ sales$serial, subset = ix, 
            xlab = "Month No", ylab = "No/month",  main = n )

Date:   Sat, 06 Mar 2004 09:52:31 +0000 
From:   Pete Phillips <pete at smtl.co.uk>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] how to loop through names ? 

 

Hi

I'm sure I'm missing something very straighforward here :-(

I have a data set 'sales' as follows:
==========================================
# read in the sales data
sales<-read.table("sales.dat",header=TRUE);
#generate a serial field
sales$serial=c(1:24)

sales

an l ml ne ni total serial
1 43 25 35 51 17 69 1
2 38 18 47 94 3 99 2
.......
24 58 13 41 95 4 1 24
===========================================

(extra rows and columns deleted).

I wish to produce a postscript plot file for each column plotted vs the
serial column, using either the 1st to 12th row or the 13th to 24th
rows, where the filename consists of the column name with '.ps' added.

This the code I have so far:

============================================
for (i in 1:(length(names(sales))-1)) {

fname <- paste(names(sales)[[i]],".ps",sep="")

postscript(file=fname)

plot(
sales$serial[13:24], 
sales[names(sales)[[i]]][13:24],
xlab="Month No", ylab="No/month")
dev.off()

}
==============================================

The filename generation works (yay!), but I think I have missed
something very basic here as that plot line seems too complex (and
doesn't work!).

Any ideas please ?

Pete



From michelis at itc.it  Mon Mar 22 18:24:23 2004
From: michelis at itc.it (Francesca Demichelis)
Date: Mon, 22 Mar 2004 18:24:23 +0100
Subject: [R] multivariate analysis
Message-ID: <02DBD906DF7101489473AFB9AAD27E5C011778D1@ntmail.pc.itc.it>

Dear,
I'm using R to make multivariate analysis on bio/pathological data.
In particular I use:
coxph(Surv(time,status)~x+y+z).
How to evaluate the hazard ratios and CI for each covariate?
Could you suggest me a function or a set of function to do it?
 
Thank You,
Francesca
 
 
Francesca Demichelis
Temporanely at CHIP, Children's Hospital, Boston MA
 
Bioinformatics Group - SRA, ITC-irst, Trento I
DIT, University of Trento, Trento I
+39 0461 405312



From jgoebel at diw.de  Mon Mar 22 18:25:57 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Mon, 22 Mar 2004 18:25:57 +0100
Subject: [R] R commands formating for LaTeX
In-Reply-To: <405F13E9.8060700@jrc.it>
References: <405F13E9.8060700@jrc.it>
Message-ID: <20040322172557.GA22640@diw138134.diw-berlin.de>

Hello,

there is a latex package called listings:

---- from the README file -------------------------
Abstract
--------

The `listings' package is a source code printer for LaTeX.
You can typeset stand alone files as well as listings with
an environment similar to  `verbatim' as well as you can
print code snippets using a command similar to \verb'.
Many parameters control the output and if your preferred
programming language isn't already supported, you can make
your own definition.

For more details read the documentation `listings.dvi'.
------------------------------------------------------

R is supported.

best

jan 


On Mon, 22 Mar 2004, Rado Bonk wrote:

> Dear R users,
> 
> I have to include typical UNIX formated R commands and outputs into the 
> article using LaTeX.
> 
> What is the easiest way to include R prompt commands and text outputs 
> (including messages) into LaTeX documents? Is there any template or library?
> 
> So far I'm trying to produce R commands in LaTeX using LaTeX general 
> format options.
> 
> Thanks,
> 
> Rado
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K?nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From joseclaudio.faria at terra.com.br  Mon Mar 22 18:28:04 2004
From: joseclaudio.faria at terra.com.br (joseclaudio.faria)
Date: Mon, 22 Mar 2004 14:28:04 -0300
Subject: [R] Help to compare...
Message-ID: <000801c41033$0cfa18f0$01fea8c0@sapetinga>

Dear list,

I'm needing submit values (V1 = 8,6,4,3,1,2,9) (Id = 2:8) of a data.frame
(DF), like below

Id  V1  V2 ...
1    0    1  ...
2    8    10  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6    1    8  ...
7    2    6  ...
8    9    7  ...
9    6    1  ...
10  5    4  ...

to selection (>=2 and <8) for remanescents like below:

Id  V1  V2 ...
1    0    1  ...
2    .     10  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6    .     8  ...
7    2    6  ...
8    .     7  ...
9    6    1  ...
10  5    4  ...

how to do that betther with R? Is there a command to compare all to same
time?

I would be very thankful.

Yours sincerly

Jos? Cl?udio Faria
UESC/DCET
Brasil
73-634.2779
joseclaudio.faria at terra.com.br
jc_faria at uol.com.br



From tlumley at u.washington.edu  Mon Mar 22 18:29:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Mar 2004 09:29:17 -0800 (PST)
Subject: [R] multivariate analysis
In-Reply-To: <02DBD906DF7101489473AFB9AAD27E5C011778D1@ntmail.pc.itc.it>
References: <02DBD906DF7101489473AFB9AAD27E5C011778D1@ntmail.pc.itc.it>
Message-ID: <Pine.A41.4.58.0403220929030.16022@homer01.u.washington.edu>

On Mon, 22 Mar 2004, Francesca Demichelis wrote:

> Dear,
> I'm using R to make multivariate analysis on bio/pathological data.
> In particular I use:
> coxph(Surv(time,status)~x+y+z).
> How to evaluate the hazard ratios and CI for each covariate?
> Could you suggest me a function or a set of function to do it?

There is an example on the coxph help page.

	-thomas



From HankeA at mar.dfo-mpo.gc.ca  Mon Mar 22 17:49:42 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Mon, 22 Mar 2004 12:49:42 -0400
Subject: [R] Setting the 'fig' graphic parameter
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124971@msgmarsta01>

Try:
x <- rnorm(100)
y <- rnorm(100)

par(fig=c(0,0.7,0,1))
plot(x,y)

# (please maximise your plotting device so you get a 'rectangular' area)

# now lets do the upper corner 'little' plot

par(fig=c(0.7, 1, 0.5, 1),new=T)
plot(x,y)

# and ...

par(fig=c(0.7, 1, 0, 0.5),new=T)
plot(x,y)

-----Original Message-----
From: Mario dos Reis [mailto:mario.reis at ucl.ac.uk] 
Sent: March 22, 2004 12:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Setting the 'fig' graphic parameter


Hi guys,

# I would like to plot a figure with the following layout:
#
#  ----------------------------
#  |                 |        |
#  |                 |        |
#  |                 |        |
#  |                 |--------|
#  |                 |        |
#  |                 |        |
#  |                 |        |
#  ----------------------------

x <- rnorm(100)
y <- rnorm(100)

par(fig=c(0,0.7,0,1))
plot(x,y)

# (please maximise your plotting device so you get a 'rectangular' area)

# now lets do the upper corner 'little' plot

par(fig=c(0.7, 1, 0.5, 1))
plot(x,y)

# and ...

par(fig=c(0.7, 1, 0, 0.5))
plot(x,y)

Now, my problem is as you might have seen already, that the old figure
gets deleted when the new one is placed. I was trying to reproduce an
exercise a saw in an S-plus book. I would really like to know what is
going on, the documentation about graphic parameters is not very helpful
about fig, and I would really like to set a graph with the above layout.

Thanks,
Mario dos Reis.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From karruo at utu.fi  Mon Mar 22 18:34:40 2004
From: karruo at utu.fi (Kari Ruohonen)
Date: Mon, 22 Mar 2004 19:34:40 +0200
Subject: [R] 1.9.0. alpha, namespace and .RData
Message-ID: <17d04ff17d0d69.17d0d6917d04ff@utu.fi>

I am using Debian package of R (Linux) and it changed recently to 1.9.0 alpha. I cannot load my old datasets (.RData) to 1.9.0 because the loading terminates to error messages like "Error in loadNamespace(name): package 'nls' does not have a name space" as an example. I learned that this may be due to the splitting of the old 'base' to new packages 'base', 'stats' and so on. However, preloading 'library(stats)' doesn't help to solve the problem. Is there a way to tell my old data that the libraries and namespaces have changed or any other way to load the old data?

Thanks, Kari Ruohonen



From shyamss at hotmail.com  Mon Mar 22 18:43:03 2004
From: shyamss at hotmail.com (Shyam Sundaram)
Date: Mon, 22 Mar 2004 23:13:03 +0530
Subject: [R] Date operations
Message-ID: <Law10-OE33kB4jYspS1000569d9@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040322/4b3421aa/attachment.pl

From slacey at umich.edu  Mon Mar 22 18:55:22 2004
From: slacey at umich.edu (Steven Lacey)
Date: Mon, 22 Mar 2004 12:55:22 -0500
Subject: [R] calling bwplot within a for loop
Message-ID: <001801c41036$dc490d70$5081d38d@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040322/e7d4e6d5/attachment.pl

From deepayan at stat.wisc.edu  Mon Mar 22 19:21:14 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 22 Mar 2004 12:21:14 -0600
Subject: [R] calling bwplot within a for loop
In-Reply-To: <001801c41036$dc490d70$5081d38d@lsa.adsroot.itcs.umich.edu>
References: <001801c41036$dc490d70$5081d38d@lsa.adsroot.itcs.umich.edu>
Message-ID: <200403221221.14077.deepayan@stat.wisc.edu>

On Monday 22 March 2004 11:55, Steven Lacey wrote:
> Hi,
>
>
>
> I am working with R 1.81. When I call bwplot() it prints the output
> to the windows device as it should. For example,
>
>
>
> d<-data.frame(y=c(2,3,4,5,12,14,16,11),x=c(rep("group1",4),rep("group
>2",4)))
>
> bwplot(y~x,data=d)
>
>
>
> This code results in a parallel boxplot. That is a single plot with 2
> boxplots next to each other; a boxplot for "group1" and a boxplot for
> "group2".
>
>
>
> However, if I call bwplot within a loop, the window device appears
> but nothing is plotted.
>
>
>
> for(i in c(1)){
>
>     print(i)
>
>     bwplot(y~x,data=d)

Replace this by 

      print(bwplot(y~x,data=d))

> }
>
>
>
> Does anyone know why this is happening and how to solve it?
>
>
>
> Thanks,
> Steve
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Mar 22 19:23:01 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 13:23:01 -0500 (EST)
Subject: [R] Date operations
Message-ID: <20040322182301.70E19395B@mprdmxin.myway.com>



Date (R 1.9.0 only), chron (in package chron) and POSIXct
classes all support all those operations. For example,

> dat.char <- c("1-Jan-01", "7-Jan-01", "14-Jan-01", "21-Feb-01")

> # chron
> require(chron)
[1] TRUE
> dat <- chron(dat.char, format="day-month-year")
> min(dat);max(dat);sort(dat);order(dat)
[1] 01-January-2001
[1] 21-February-2001
[1] 01-January-2001  07-January-2001  14-January-2001  21-February-2001
[1] 1 2 3 4

> # POSIXct
> dat <- as.POSIXct(strptime(dat.char,format="%d-%b-%y"))
> dat
[1] "2001-01-01 Eastern Standard Time" "2001-01-07 Eastern Standard Time"
[3] "2001-01-14 Eastern Standard Time" "2001-02-21 Eastern Standard Time"
> min(dat);max(dat);sort(dat);order(dat)
[1] "2001-01-01 Eastern Standard Time"
[1] "2001-02-21 Eastern Standard Time"
[1] "2001-01-01 Eastern Standard Time" "2001-01-07 Eastern Standard Time"
[3] "2001-01-14 Eastern Standard Time" "2001-02-21 Eastern Standard Time"
[1] 1 2 3 4

> # Date
> R.version.string
[1] "R version 1.9.0, 2004-03-05"
> dat <- as.Date(dat.char,format="%d-%b-%y")
> min(dat);max(dat);sort(dat);order(dat)
[1] "2001-01-01"
[1] "2001-02-21"
[1] "2001-01-01" "2001-01-07" "2001-01-14" "2001-02-21"
[1] 1 2 3 4
> 




Date:   Mon, 22 Mar 2004 23:13:03 +0530 
From:   Shyam Sundaram <shyamss at hotmail.com>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Date operations 

 
Hello:
Thanks in advance for your time ?

I am having a data.frame with one of the columns containing the weeks as follows. How can I do the following in the most efficient way ?

1. Find the minimum date ?
2. Find the maximum date ?
3. How do we sort based on ascending order the date ?

An example as follows.

Week
1-Jan-01 (<----- MIN DATE)
7-Jan-01
14-Jan-01
21-Feb-01 (<----- MAX DATE)

What is a good way to do these most effectively in R ?



From ripley at stats.ox.ac.uk  Mon Mar 22 19:31:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 18:31:02 +0000 (GMT)
Subject: [R] calling bwplot within a for loop
In-Reply-To: <001801c41036$dc490d70$5081d38d@lsa.adsroot.itcs.umich.edu>
Message-ID: <Pine.LNX.4.44.0403221828380.1233-100000@gannet.stats>

On Mon, 22 Mar 2004, Steven Lacey wrote:

[...]


> However, if I call bwplot within a loop, the window device appears but
> nothing is plotted. 

[...]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

which asks you to read the FAQ, and this is Q7.24.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Mar 22 19:34:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 18:34:58 +0000 (GMT)
Subject: [R] detach()
In-Reply-To: <200403221656.i2MGu1gs021034@erdos.math.unb.ca>
Message-ID: <Pine.LNX.4.44.0403221701430.1233-100000@gannet.stats>

I think the point is that you supplied name as a name, not a numeric, 
although it is a fine distinction.

    name: The object to detach.  Defaults to 'search()[pos]'. This can
          be a name or a character string but _not_ a character vector.
          ^^^^^^^^^

If that is to be allowed, then it has to take precedence. What would you
expect if I did

data(women)
attach(women)
women <- 4
detach(women)

?  I think almost everyone would expect the current behaviour.

I think the help page should say

When 'name' is a number, 'pos = name' is used.

Brian


On Mon, 22 Mar 2004, Rolf Turner wrote:

> 
> I got bitten recently by the following behaviour of detach();
> 
>  > save(file="Junk")
>  > attach("Junk")
>  > search()
>  [1] ".GlobalEnv"      "file:Junk"       "package:methods" "package:ctest"  
>  [5] "package:mva"     "package:modreg"  "package:nls"     "package:ts"     
>  [9] "package:Misc"    "Autoloads"       "package:base"   
>  > detach(2)
>  > # No problem; reattach junk.
>  > attach("Junk")
>  > ind <- 2
>  > detach(ind)
> Error in detach(ind) : invalid name
>  > is.numeric(ind)
> [1] TRUE
> 
> The help on detach() says:
> 
>     name: The object to detach.  Defaults to 'search()[pos]'. This can
>           be a name or a character string but _not_ a character vector. 
> 
>      pos: Index position in 'search()' of database to detach.  When
>           'name' is 'numeric', 'pos = name' is used.           ^^^^ 
>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> 
> Looking at the code of detach, I see that name gets assigned
> substitute(name) before the check as to whether name is numeric.  So
> if name is provided as an object --- like ind --- then name becomes
> the name ``ind'' and is an object of mode "name" and so the check
> will evaluate to FALSE.  (If name is given as an explicit constant
> --- like 2 --- then name remains equal to 2 and is indeed numeric so
> the check evaluates TRUE.)
> 
> Is this the intended behaviour for detach() or is it a bug?
> 
> It's obviously no big deal because detach(pos=ind) works perfectly.
> But it did trip me up and could conceivably trip others.
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Mar 22 19:40:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Mar 2004 18:40:37 +0000 (GMT)
Subject: [R] Date operations
In-Reply-To: <Law10-OE33kB4jYspS1000569d9@hotmail.com>
Message-ID: <Pine.LNX.4.44.0403221835110.1233-100000@gannet.stats>

All of min, max and sort work for R's dates.  See ?DateTimeClasses.  You 
appear to have character strings, so just need to convert them.

R 1.9.0 will have classes for dates as well as date-times.

For example (1.9.0)

x <- c("1-Jan-01", "7-Jan-01", "14-Jan-01", "21-Feb-01")
y <- as.Date(x, format="%d-%b-%y")
min(y); max(y); sort(y)

On Mon, 22 Mar 2004, Shyam Sundaram wrote:

> Hello:
> Thanks in advance for your time ?
> 
> I am having a data.frame with one of the columns containing the weeks as
> follows. How can I do the following in the most efficient way ?
> 
> 1. Find the minimum date ?
> 2. Find the maximum date ?
> 3. How do we sort based on ascending order the date ?
> 
> An example as follows.
> 
> Week
> 1-Jan-01 (<----- MIN DATE)
> 7-Jan-01
> 14-Jan-01
> 21-Feb-01 (<----- MAX DATE)
> 
> What is a good way to do these most effectively in R ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jbond at arg.org  Mon Mar 22 20:14:53 2004
From: jbond at arg.org (Jason Bond)
Date: Mon, 22 Mar 2004 11:14:53 -0800
Subject: [R] comprehensive documentation list of R probability functions
Message-ID: <5.1.0.14.2.20040322111216.02384230@arg.org>

Hello.  I was wondering if anyone could point me to a comprehensive list of 
all of the probability functions available in R (Random deviates, CDF, 
quantile functions, and PDF functions).  Thanks much,

   Jason



From deepayan at stat.wisc.edu  Mon Mar 22 20:28:13 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 22 Mar 2004 13:28:13 -0600
Subject: [R] Lattice, skip= and layout= problem,
	plotting object from nlme output
In-Reply-To: <5.2.1.1.0.20040322130227.00a2ed60@pop.freeserve.net>
References: <5.2.1.1.0.20040322130227.00a2ed60@pop.freeserve.net>
Message-ID: <200403221328.13969.deepayan@stat.wisc.edu>

On Monday 22 March 2004 07:15, Michael Dewey wrote:
> I generate a groupedData object
>
> library(nlme)
> obj <- groupedData(mg10 ~ time | gp, data = common, outer = ~pct)
>
> gp has 101 levels, and pct has 3. There are 38, 25, 38 gps in each of
> the levels of pct respectively.
>
> I fit my model
>
> fit.rtg <- lme(mg10 ~ time * group,
>     data = obj,
>     random = ~time * group | gp)
>
> Now I try to plot the results. I would like to print 40 panels on
> each page and have a new level of pct start a new page. The effect of
> using outer in the call of groupedData is that the values of gp are
> presented by pct.
>
> plot.rtg <- plot(augPred(fit.rtg),
>     skip = c(rep(FALSE, 38), TRUE, TRUE,
>              rep(FALSE, 25), rep(TRUE, 15),
>              rep(FALSE, 38), TRUE, TRUE),
>     layout = c(5, 8, 3),
>     strip = FALSE
> )
>
> What I get is 38 panels on page 1 with 2 blank panels top right. I
> had hoped to get 25 on the next page with 15 blanks but I get 38
> again with 2 blanks.
>
> If I alter layout to (say) layout = c(12, 10) I get blanks as
> expected on the single page produced so I surmise that skip is
> forcing the same format on each page. There is an example in
> \cite{pinheiro00} which suggests that what I wanted can be done
> (p113,  p445-447) so I suspect I am doing something stupid here.

You are seeing documented lattice behavior, but looks like that's 
inconsistent with what happens in S-PLUS. lattice replicates the skip 
vector to be as long as the number of panels per page and uses that for 
every page, while S-PLUS replicates it to be as long as the total 
number of panels spanning all pages.

I can easily fix this for 1.9.0, but this may break old (but probably 
rare) lattice code. For example, 

layout = c(2,2,2), skip = c(T,F,F)

will now expand to 

page 1: T, F, F, T
page 2: F, F, T, F

as opposed to the current behavior

page 1: T, F, F, T
page 2: T, F, F, T

Any objections to that ? 

Deepayan



From MSchwartz at MedAnalytics.com  Mon Mar 22 20:32:00 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 22 Mar 2004 13:32:00 -0600
Subject: [R] comprehensive documentation list of R probability functions
In-Reply-To: <5.1.0.14.2.20040322111216.02384230@arg.org>
References: <5.1.0.14.2.20040322111216.02384230@arg.org>
Message-ID: <1079983920.7675.166.camel@localhost.localdomain>

On Mon, 2004-03-22 at 13:14, Jason Bond wrote:
> Hello.  I was wondering if anyone could point me to a comprehensive list of 
> all of the probability functions available in R (Random deviates, CDF, 
> quantile functions, and PDF functions).  Thanks much,


A good start would be Chapter 8 (Probability Distributions) in "An
Introduction to R". 

Also help.search("distribution") will provide a keyword listing for your
installed packages (which of course would not include add-on packages
from CRAN, BioC, etc.)

HTH,

Marc Schwartz



From hodgess at gator.uhd.edu  Mon Mar 22 20:40:36 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 22 Mar 2004 13:40:36 -0600
Subject: [R] another date conversion
Message-ID: <200403221940.i2MJeaO16563@gator.dt.uh.edu>

Dear R People
Here is an interesting problem:

> library(pastecs)
> a <- 1:100
> b <- daystoyears(a,datemin="1/1/2003",dateformat="m/d/Y")
> b
  [1] 2002.999 2003.002 2003.005 2003.008 2003.010 2003.013 2003.016 2003.018 2003.021 2003.024 2003.027 2003.029 2003.032
 [14] 2003.035 2003.038 2003.040 2003.043 2003.046 2003.049 2003.051 2003.054 2003.057 2003.060 2003.062 2003.065 2003.068
 [27] 2003.070 2003.073 2003.076 2003.079 2003.081 2003.084 2003.087 2003.090 2003.092 2003.095 2003.098 2003.101 2003.103
 [40] 2003.106 2003.109 2003.112 2003.114 2003.117 2003.120 2003.123 2003.125 2003.128 2003.131 2003.133 2003.136 2003.139
 [53] 2003.142 2003.144 2003.147 2003.150 2003.153 2003.155 2003.158 2003.161 2003.164 2003.166 2003.169 2003.172 2003.175
 [66] 2003.177 2003.180 2003.183 2003.185 2003.188 2003.191 2003.194 2003.196 2003.199 2003.202 2003.205 2003.207 2003.210
 [79] 2003.213 2003.216 2003.218 2003.221 2003.224 2003.227 2003.229 2003.232 2003.235 2003.238 2003.240 2003.243 2003.246
 [92] 2003.248 2003.251 2003.254 2003.257 2003.259 2003.262 2003.265 2003.268 2003.270
> require(chron)
Loading required package: chron 

Attaching package 'chron':


        The following object(s) are masked _by_ .GlobalEnv :

         dates times 

[1] TRUE
> seq.dates(B,by="days",length=length(b))
  [1] (06/26/75 23:59:01) (06/27/75 23:59:01) (06/28/75 23:59:01) (06/29/75 23:59:01) (06/30/75 23:59:01) (07/01/75 23:59:01)
  [7] (07/02/75 23:59:01) (07/03/75 23:59:01) (07/04/75 23:59:01) (07/05/75 23:59:01) (07/06/75 23:59:01) (07/07/75 23:59:01)
 [13] (07/08/75 23:59:01) (07/09/75 23:59:01) (07/10/75 23:59:01) (07/11/75 23:59:01) (07/12/75 23:59:01) (07/13/75 23:59:01)
 [19] (07/14/75 23:59:01) (07/15/75 23:59:01) (07/16/75 23:59:01) (07/17/75 23:59:01) (07/18/75 23:59:01) (07/19/75 23:59:01)
 [25] (07/20/75 23:59:01) (07/21/75 23:59:01) (07/22/75 23:59:01) (07/23/75 23:59:01) (07/24/75 23:59:01) (07/25/75 23:59:01)
 [31] (07/26/75 23:59:01) (07/27/75 23:59:01) (07/28/75 23:59:01) (07/29/75 23:59:01) (07/30/75 23:59:01) (07/31/75 23:59:01)
 [37] (08/01/75 23:59:01) (08/02/75 23:59:01) (08/03/75 23:59:01) (08/04/75 23:59:01) (08/05/75 23:59:01) (08/06/75 23:59:01)
 [43] (08/07/75 23:59:01) (08/08/75 23:59:01) (08/09/75 23:59:01) (08/10/75 23:59:01) (08/11/75 23:59:01) (08/12/75 23:59:01)
 [49] (08/13/75 23:59:01) (08/14/75 23:59:01) (08/15/75 23:59:01) (08/16/75 23:59:01) (08/17/75 23:59:01) (08/18/75 23:59:01)
 [55] (08/19/75 23:59:01) (08/20/75 23:59:01) (08/21/75 23:59:01) (08/22/75 23:59:01) (08/23/75 23:59:01) (08/24/75 23:59:01)
 [61] (08/25/75 23:59:01) (08/26/75 23:59:01) (08/27/75 23:59:01) (08/28/75 23:59:01) (08/29/75 23:59:01) (08/30/75 23:59:01)
 [67] (08/31/75 23:59:01) (09/01/75 23:59:01) (09/02/75 23:59:01) (09/03/75 23:59:01) (09/04/75 23:59:01) (09/05/75 23:59:01)
 [73] (09/06/75 23:59:01) (09/07/75 23:59:01) (09/08/75 23:59:01) (09/09/75 23:59:01) (09/10/75 23:59:01) (09/11/75 23:59:01)
 [79] (09/12/75 23:59:01) (09/13/75 23:59:01) (09/14/75 23:59:01) (09/15/75 23:59:01) (09/16/75 23:59:01) (09/17/75 23:59:01)
 [85] (09/18/75 23:59:01) (09/19/75 23:59:01) (09/20/75 23:59:01) (09/21/75 23:59:01) (09/22/75 23:59:01) (09/23/75 23:59:01)
 [91] (09/24/75 23:59:01) (09/25/75 23:59:01) (09/26/75 23:59:01) (09/27/75 23:59:01) (09/28/75 23:59:01) (09/29/75 23:59:01)
 [97] (09/30/75 23:59:01) (10/01/75 23:59:01) (10/02/75 23:59:01) (10/03/75 23:59:01)
> 




Why is it that the seq.dates values come back as 1975, please?
What am I doing wrong, please?  (I'm sure it's something incredibly simple)

Thanks in advance!

R for Windows 1.8.1

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From andy_liaw at merck.com  Mon Mar 22 20:33:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 22 Mar 2004 14:33:20 -0500
Subject: [R] comprehensive documentation list of R probability functio ns
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A4C@usrymx25.merck.com>

Do read `An Introduction to R', especially section 8.1.  There are other
distributions implemented in contributed packages; e.g., `SuppDists'.

Andy

> From: Jason Bond
> 
> Hello.  I was wondering if anyone could point me to a 
> comprehensive list of 
> all of the probability functions available in R (Random 
> deviates, CDF, 
> quantile functions, and PDF functions).  Thanks much,
> 
>    Jason
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ggrothendieck at myway.com  Mon Mar 22 20:55:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 14:55:23 -0500 (EST)
Subject: [R] another date conversion
Message-ID: <20040322195523.1A08939CC@mprdmxin.myway.com>



Can't say exactly since B has not been defined in your
post but I think you are attempting to mix dates from
different representations/classes as suggested by:

> chron(2003)
[1] 06/27/75

chron represents dates as no. of days since this origin:

> chron(0)
[1] 01/01/70

while the pastecs function cited frepresents dates 
as year.fraction

Thus numbers near 2003 are near the beginning of 2003
to that pastecs function but they represents dates
in 1975 to chron since they are around 2003 days since 
01/01/70.




Date:   Mon, 22 Mar 2004 13:40:36 -0600 
From:   Erin Hodgess <hodgess at gator.uhd.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] another date conversion 

 
Dear R People
Here is an interesting problem:

> library(pastecs)
> a <- 1:100
> b <- daystoyears(a,datemin="1/1/2003",dateformat="m/d/Y")
> b
[1] 2002.999 2003.002 2003.005 2003.008 2003.010 2003.013 2003.016 2003.018 2003.021 2003.024 2003.027 2003.029 2003.032
[14] 2003.035 2003.038 2003.040 2003.043 2003.046 2003.049 2003.051 2003.054 2003.057 2003.060 2003.062 2003.065 2003.068
[27] 2003.070 2003.073 2003.076 2003.079 2003.081 2003.084 2003.087 2003.090 2003.092 2003.095 2003.098 2003.101 2003.103
[40] 2003.106 2003.109 2003.112 2003.114 2003.117 2003.120 2003.123 2003.125 2003.128 2003.131 2003.133 2003.136 2003.139
[53] 2003.142 2003.144 2003.147 2003.150 2003.153 2003.155 2003.158 2003.161 2003.164 2003.166 2003.169 2003.172 2003.175
[66] 2003.177 2003.180 2003.183 2003.185 2003.188 2003.191 2003.194 2003.196 2003.199 2003.202 2003.205 2003.207 2003.210
[79] 2003.213 2003.216 2003.218 2003.221 2003.224 2003.227 2003.229 2003.232 2003.235 2003.238 2003.240 2003.243 2003.246
[92] 2003.248 2003.251 2003.254 2003.257 2003.259 2003.262 2003.265 2003.268 2003.270
> require(chron)
Loading required package: chron 

Attaching package 'chron':


The following object(s) are masked _by_ .GlobalEnv :

dates times 

[1] TRUE
> seq.dates(B,by="days",length=length(b))
[1] (06/26/75 23:59:01) (06/27/75 23:59:01) (06/28/75 23:59:01) (06/29/75 23:59:01) (06/30/75 23:59:01) (07/01/75 23:59:01)
[7] (07/02/75 23:59:01) (07/03/75 23:59:01) (07/04/75 23:59:01) (07/05/75 23:59:01) (07/06/75 23:59:01) (07/07/75 23:59:01)
[13] (07/08/75 23:59:01) (07/09/75 23:59:01) (07/10/75 23:59:01) (07/11/75 23:59:01) (07/12/75 23:59:01) (07/13/75 23:59:01)
[19] (07/14/75 23:59:01) (07/15/75 23:59:01) (07/16/75 23:59:01) (07/17/75 23:59:01) (07/18/75 23:59:01) (07/19/75 23:59:01)
[25] (07/20/75 23:59:01) (07/21/75 23:59:01) (07/22/75 23:59:01) (07/23/75 23:59:01) (07/24/75 23:59:01) (07/25/75 23:59:01)
[31] (07/26/75 23:59:01) (07/27/75 23:59:01) (07/28/75 23:59:01) (07/29/75 23:59:01) (07/30/75 23:59:01) (07/31/75 23:59:01)
[37] (08/01/75 23:59:01) (08/02/75 23:59:01) (08/03/75 23:59:01) (08/04/75 23:59:01) (08/05/75 23:59:01) (08/06/75 23:59:01)
[43] (08/07/75 23:59:01) (08/08/75 23:59:01) (08/09/75 23:59:01) (08/10/75 23:59:01) (08/11/75 23:59:01) (08/12/75 23:59:01)
[49] (08/13/75 23:59:01) (08/14/75 23:59:01) (08/15/75 23:59:01) (08/16/75 23:59:01) (08/17/75 23:59:01) (08/18/75 23:59:01)
[55] (08/19/75 23:59:01) (08/20/75 23:59:01) (08/21/75 23:59:01) (08/22/75 23:59:01) (08/23/75 23:59:01) (08/24/75 23:59:01)
[61] (08/25/75 23:59:01) (08/26/75 23:59:01) (08/27/75 23:59:01) (08/28/75 23:59:01) (08/29/75 23:59:01) (08/30/75 23:59:01)
[67] (08/31/75 23:59:01) (09/01/75 23:59:01) (09/02/75 23:59:01) (09/03/75 23:59:01) (09/04/75 23:59:01) (09/05/75 23:59:01)
[73] (09/06/75 23:59:01) (09/07/75 23:59:01) (09/08/75 23:59:01) (09/09/75 23:59:01) (09/10/75 23:59:01) (09/11/75 23:59:01)
[79] (09/12/75 23:59:01) (09/13/75 23:59:01) (09/14/75 23:59:01) (09/15/75 23:59:01) (09/16/75 23:59:01) (09/17/75 23:59:01)
[85] (09/18/75 23:59:01) (09/19/75 23:59:01) (09/20/75 23:59:01) (09/21/75 23:59:01) (09/22/75 23:59:01) (09/23/75 23:59:01)
[91] (09/24/75 23:59:01) (09/25/75 23:59:01) (09/26/75 23:59:01) (09/27/75 23:59:01) (09/28/75 23:59:01) (09/29/75 23:59:01)
[97] (09/30/75 23:59:01) (10/01/75 23:59:01) (10/02/75 23:59:01) (10/03/75 23:59:01)
> 




Why is it that the seq.dates values come back as 1975, please?
What am I doing wrong, please? (I'm sure it's something incredibly simple)

Thanks in advance!

R for Windows 1.8.1

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From HankeA at mar.dfo-mpo.gc.ca  Mon Mar 22 20:19:31 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Mon, 22 Mar 2004 15:19:31 -0400
Subject: [R] Help to compare...
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124973@msgmarsta01>

DF<-data.frame(V1=c(0,8,6,4,3,1,2,9,6,5),V2=1:10)
DF[((DF$V1>=2 & DF$V1<8)*1:10)[2:8],]

-----Original Message-----
From: joseclaudio.faria [mailto:joseclaudio.faria at terra.com.br] 
Sent: March 22, 2004 1:28 PM
To: R-help at stat.math.ethz.ch
Subject: [R] Help to compare...


Dear list,

I'm needing submit values (V1 = 8,6,4,3,1,2,9) (Id = 2:8) of a data.frame
(DF), like below

Id  V1  V2 ...
1    0    1  ...
2    8    10  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6    1    8  ...
7    2    6  ...
8    9    7  ...
9    6    1  ...
10  5    4  ...

to selection (>=2 and <8) for remanescents like below:

Id  V1  V2 ...
1    0    1  ...
2    .     10  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6    .     8  ...
7    2    6  ...
8    .     7  ...
9    6    1  ...
10  5    4  ...

how to do that betther with R? Is there a command to compare all to same
time?

I would be very thankful.

Yours sincerly

Jos? Cl?udio Faria
UESC/DCET
Brasil
73-634.2779
joseclaudio.faria at terra.com.br
jc_faria at uol.com.br

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Mon Mar 22 21:32:09 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 22 Mar 2004 16:32:09 -0400 (AST)
Subject: [R] detach()
Message-ID: <200403222032.i2MKW9RD002449@erdos.math.unb.ca>

Dear Brian,

You wrote:

> If that is to be allowed, then it has to take precedence. What would
> you expect if I did
> 
> data(women)
> attach(women)
> women <- 4
> detach(women)

I would basically expect either all hell to break loose, mainly
because of the overwriting of ``women'' or the data base in position
4 to be detached.

If you did

data(women)
attach(women)
melvin <- 4
detach(melvin)

I would again expect the data base in position 4 to be detached.

If, more realistically, you did

melvin <- grep("women",search())
detach(melvin)

I would expect the data set ``women'' to be detached.  My
expectations would of course be dashed.  The current structure of
detach() seems to me to be a counterintuitive contortion designed to
protect the user from doing something stupid.  Maybe that's a good
thing.

> ?  I think almost everyone would expect the current behaviour.

	I beg to differ.  Only someone who was in on the design
	process and saw the potential pitfall would expect the
	current behaviour.

	It is a rare and subtle phenomenon that

	> foo(42)

	gives a different result from

	> x <- 42
	> foo(x)

> I think the help page should say
> 
> When 'name' is a number, 'pos = name' is used.

	Even that would be totally misleading to the average user.
	None of us would distinguish between providing a numeric
	argument as an absolute constant and providing it as that
	same constant stored in a (scalar numeric) object.

	To make it clear what is going on an example should be
	given:

      ``When 'name' is an absolute constant, 'pos = name' is used.
	For example:
	> detach(2) # Works.
	> n <- 2
	> detach(n) # Doesn't work.''

					cheers,

						Rolf



From ggrothendieck at myway.com  Mon Mar 22 21:42:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Mar 2004 15:42:07 -0500 (EST)
Subject: [R] detach()
Message-ID: <20040322204207.B97033998@mprdmxin.myway.com>


There is a paper on nonstandard evaluation at:

   http://developer.r-project.org/nonstandard-eval.pdf

that seems relevant here.

Date:   Mon, 22 Mar 2004 16:32:09 -0400 (AST) 
From:   Rolf Turner <rolf at math.unb.ca>
To:   <ripley at stats.ox.ac.uk> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] detach() 

 
Dear Brian,

You wrote:

> If that is to be allowed, then it has to take precedence. What would
> you expect if I did
> 
> data(women)
> attach(women)
> women <- 4
> detach(women)

I would basically expect either all hell to break loose, mainly
because of the overwriting of ``women'' or the data base in position
4 to be detached.

If you did

data(women)
attach(women)
melvin <- 4
detach(melvin)

I would again expect the data base in position 4 to be detached.

If, more realistically, you did

melvin <- grep("women",search())
detach(melvin)

I would expect the data set ``women'' to be detached. My
expectations would of course be dashed. The current structure of
detach() seems to me to be a counterintuitive contortion designed to
protect the user from doing something stupid. Maybe that's a good
thing.

> ? I think almost everyone would expect the current behaviour.

     I beg to differ. Only someone who was in on the design
     process and saw the potential pitfall would expect the
     current behaviour.

     It is a rare and subtle phenomenon that

     > foo(42)

     gives a different result from

     > x <- 42
     > foo(x)

> I think the help page should say
> 
> When 'name' is a number, 'pos = name' is used.

     Even that would be totally misleading to the average user.
     None of us would distinguish between providing a numeric
     argument as an absolute constant and providing it as that
     same constant stored in a (scalar numeric) object.

     To make it clear what is going on an example should be
     given:

``When 'name' is an absolute constant, 'pos = name' is used.
     For example:
     > detach(2) # Works.
     > n <- 2
     > detach(n) # Doesn't work.''

                         cheers,

                              Rolf



From andy_liaw at merck.com  Mon Mar 22 21:44:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 22 Mar 2004 15:44:55 -0500
Subject: [R] detach()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A4F@usrymx25.merck.com>

> From: Rolf Turner
> 
> Dear Brian,
> 
> You wrote:
> 
> > If that is to be allowed, then it has to take precedence. What would
> > you expect if I did
> > 
> > data(women)
> > attach(women)
> > women <- 4
> > detach(women)
> 
> I would basically expect either all hell to break loose, mainly
> because of the overwriting of ``women'' or the data base in position
> 4 to be detached.

Don't think so.  attach(women) places a copy of `women' in search position
2.  women <- 4 creates another object holding the value 4 in the global
environment.  No data have been destroyed or overwritten.  

[The current behavior of detach() apparently detaches `women' from position
2, rather than detaching whatever is in position 4.  This _is_ what I would
expect, although I'd say whoever writes real code like that deserves all the
confusion he/she gets...]

Cheers,
Andy


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From joseclaudio.faria at terra.com.br  Mon Mar 22 21:46:55 2004
From: joseclaudio.faria at terra.com.br (joseclaudio.faria)
Date: Mon, 22 Mar 2004 17:46:55 -0300
Subject: [R] Exclude selected values for columns in frames
Message-ID: <002101c4104e$d3b36760$01fea8c0@sapetinga>

Dear list,

I'm needing submit values (V1 = 8,6,4,3,1,2,9) (Id = 2:8) of a data.frame
(DF), like below

Id  V1  V2 ...
1    0    1  ...
2    8    10  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6    1    8  ...
7    2    6  ...
8    9    7  ...
9    6    1  ...
10  5    4  ...

to selection (>=2 and <8) for remanescents like below:

Id  V1  V2 ...
1     .    1  ...
2     .    10  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6     .     8  ...
7    2    6  ...
8     .     7  ...
9    6    1  ...
10  5    4  ...

or

Id  V1  V2 ...
1     .      .  ...
2     .      .  ...
3    6    2  ...
4    4    4  ...
5    3    7  ...
6     .      .  ...
7    2    6  ...
8     .     7  ...
9    6     .  ...
10  5    4  ...

How to do this betther with R? Is there a command to compare all to same
time?

I would be very thankful.

Yours sincerly

Att.

Jos? Cl?udio Faria
UESC/DCET
Brasil
73-634.2779
joseclaudio.faria at terra.com.br
jc_faria at uol.com.br



From andy_liaw at merck.com  Mon Mar 22 21:49:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 22 Mar 2004 15:49:31 -0500
Subject: [R] detach()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A50@usrymx25.merck.com>

> From: Liaw, Andy
> 
> > From: Rolf Turner
> > 
> > Dear Brian,
> > 
> > You wrote:
> > 
> > > If that is to be allowed, then it has to take precedence. 
> What would
> > > you expect if I did
> > > 
> > > data(women)
> > > attach(women)
> > > women <- 4
> > > detach(women)
> > 
> > I would basically expect either all hell to break loose, mainly
> > because of the overwriting of ``women'' or the data base in position
> > 4 to be detached.
> 
> Don't think so.  attach(women) places a copy of `women' in 
> search position
> 2.  women <- 4 creates another object holding the value 4 in 
> the global
> environment.  No data have been destroyed or overwritten.  

I forgot about the first line, data(women), which loads `women' into the
global environment, and that _is_ overwritten by the line women <- 4.

Andy


------------------------------------------------------------------------------Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.



From m.dewey at iop.kcl.ac.uk  Mon Mar 22 22:06:22 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Mon, 22 Mar 2004 21:06:22 +0000
Subject: [R] Lattice, skip= and layout= problem, plotting object
	from nlme output
In-Reply-To: <200403221328.13969.deepayan@stat.wisc.edu>
References: <5.2.1.1.0.20040322130227.00a2ed60@pop.freeserve.net>
	<5.2.1.1.0.20040322130227.00a2ed60@pop.freeserve.net>
Message-ID: <5.2.1.1.0.20040322210234.00a18d10@mailbox.iop.kcl.ac.uk>

At 13:28 22/03/04 -0600, you wrote:

[snip my original problem]


>You are seeing documented lattice behavior, but looks like that's
>inconsistent with what happens in S-PLUS. lattice replicates the skip
>vector to be as long as the number of panels per page and uses that for
>every page, while S-PLUS replicates it to be as long as the total
>number of panels spanning all pages.
>
>I can easily fix this for 1.9.0, but this may break old (but probably
>rare) lattice code. For example,
>
>layout = c(2,2,2), skip = c(T,F,F)
>
>will now expand to
>
>page 1: T, F, F, T
>page 2: F, F, T, F
>
>as opposed to the current behavior
>
>page 1: T, F, F, T
>page 2: T, F, F, T
>
>Any objections to that ?

 From my point of view the current behaviour is not very useful, and in 
your example either behaviour seems a bit strange so I would not object to 
it changing.

Thanks for the quick response, and thanks for making lattice available. I 
find myself making so many more graphical displays because they are (a) 
easy (b) aesthetically pleasing (c) revealing about my dataset.

>Deepayan

Michael Dewey
m.dewey at iop.kcl.ac.uk



From p.dalgaard at biostat.ku.dk  Mon Mar 22 22:20:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Mar 2004 22:20:17 +0100
Subject: [R] detach()
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A4F@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7A4F@usrymx25.merck.com>
Message-ID: <x2hdwgwpda.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > From: Rolf Turner
> > 
> > Dear Brian,
> > 
> > You wrote:
> > 
> > > If that is to be allowed, then it has to take precedence. What would
> > > you expect if I did
> > > 
> > > data(women)
> > > attach(women)
> > > women <- 4
> > > detach(women)
> > 
> > I would basically expect either all hell to break loose, mainly
> > because of the overwriting of ``women'' or the data base in position
> > 4 to be detached.
> 
> Don't think so.  attach(women) places a copy of `women' in search position
> 2.  women <- 4 creates another object holding the value 4 in the global
> environment.  No data have been destroyed or overwritten.  
> 
> [The current behavior of detach() apparently detaches `women' from position
> 2, rather than detaching whatever is in position 4.  This _is_ what I would
> expect, although I'd say whoever writes real code like that deserves all the
> confusion he/she gets...]

It isn't logical though. It's understandable that people want to see
the symmetry in 

attach(women)
detach(women)

but it really is a false consistency, since it makes good sense to
compute an object in the first case, but in the second what we need
is a name. E.g.

attach(transform(airquality,Month=factor(Month)))
detach(transform(airquality,Month=factor(Month)))

is a bit bizarre, although it works fine.

It's hardly the time to change this though (and if we do, I'd want to
rethink the whole workspace/searchpath structure), especially since
detach(pos=x) will do what you want soon enough.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Mar 22 22:34:06 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Mar 2004 13:34:06 -0800 (PST)
Subject: [R] detach()
In-Reply-To: <200403222032.i2MKW9RD002449@erdos.math.unb.ca>
References: <200403222032.i2MKW9RD002449@erdos.math.unb.ca>
Message-ID: <Pine.A41.4.58.0403221323340.16022@homer01.u.washington.edu>

On Mon, 22 Mar 2004, Rolf Turner wrote:
> If, more realistically, you did
>
> melvin <- grep("women",search())
> detach(melvin)
>
> I would expect the data set ``women'' to be detached.  My
> expectations would of course be dashed.  The current structure of
> detach() seems to me to be a counterintuitive contortion designed to
> protect the user from doing something stupid.  Maybe that's a good
> thing.

I think the motivation may have been that attach() and detach() (like
rm(), data(), and help(), which have the same problem) are rarely used in
programming, and that programmers can use detach(pos=).

It was certainly deliberate, but may well have been a mistake.  One could
even argue that the symmetry between attach(women) and detach(women) is
misleading, and encourages the misconception that attach() puts the object
in the search path, rather than a copy of the object.


	-thomas



From Manuel.A.Morales at williams.edu  Mon Mar 22 22:47:06 2004
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Mon, 22 Mar 2004 16:47:06 -0500
Subject: [R] persp(), axis font size
Message-ID: <405F5EDA.3090608@williams.edu>

Is there a way to adjust the font size for axis labels when using 
persp()? The parameter cex works for adjusting the global font size, but 
  I can't seem to make cex.lab or cex.axis work for adjusting these 
values independently. Or, is there a preferred method for making surface 
plots in R?

I'm using R version 1.8.

Thanks,

Manuel



From merolagio at tiscali.co.uk  Mon Mar 22 23:02:13 2004
From: merolagio at tiscali.co.uk (giovanni merola)
Date: Mon, 22 Mar 2004 22:02:13 +0000
Subject: [R] R-business case
In-Reply-To: <ODEPICOHNDBJEHIFCIMPCEHKCBAA.ramasamy@cancer.org.uk>
References: <ODEPICOHNDBJEHIFCIMPCEHKCBAA.ramasamy@cancer.org.uk>
Message-ID: <405F6265.4080607@tiscali.co.uk>

Hello,
thank you very much for your help. I hope my office will support R.

If not, I'll have to work with SAS!!!

regards, giovanni

Adaikalavan Ramasamy wrote:

>I like the previous suggestion of counting the number of unique e-mails in
>the archive.
>
>Another useful thing would be to count and plot the growth of number of R
>(and Bioconductor) packages over the years. I know not all packages are
>created equal.
>
>Regards, Adai.
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Duncan Murdoch
>>Sent: 17 March 2004 14:03
>>To: Spencer Graves
>>Cc: r-help at stat.math.ethz.ch; giovanni merola
>>Subject: Re: [R] R-business case
>>
>>
>>On Tue, 16 Mar 2004 13:41:38 -0800, Spencer Graves
>><spencer.graves at pdf.com> wrote :
>>
>>    
>>
>>>     Some web sites have "hit counters".  It should be possible to get
>>>a counts of the numbers of times different parts of R are downloaded.
>>>Do the CRAN web sites include any such?
>>>      
>>>
>>The logs aren't currently available for general viewing, but I took a
>>look at what we do have, and it shows rw1081.exe (the Windows binary
>>build of R 1.8.1) being downloaded from the main CRAN site about 28000
>>times.  Not all of those downloads were successful; they average
>>around 10 Meg each, and the file is 22 Meg in size.  On the other
>>hand, traffic to the mirrors isn't included at all.
>>
>>Duncan Murdoch
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>
>  
>



From spencer.graves at pdf.com  Mon Mar 22 23:10:58 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 22 Mar 2004 14:10:58 -0800
Subject: [R] R-business case
In-Reply-To: <405F6265.4080607@tiscali.co.uk>
References: <ODEPICOHNDBJEHIFCIMPCEHKCBAA.ramasamy@cancer.org.uk>
	<405F6265.4080607@tiscali.co.uk>
Message-ID: <405F6472.4040800@pdf.com>

      Even if your office will NOT support R, you can use it for 
anywhere SAS is NOT specifically required.  I work with others who use 
S-Plus, so I use S-Plus for compatibility for things I do that may 
involve sharing with others using SAS.  However, for tasks for which R 
is superior to S-Plus, if I don't need to share with others using 
S-Plus, I use R. 

      Good Luck!
      Spencer Graves

giovanni merola wrote:

> Hello,
> thank you very much for your help. I hope my office will support R.
>
> If not, I'll have to work with SAS!!!
>
> regards, giovanni
>
> Adaikalavan Ramasamy wrote:
>
>> I like the previous suggestion of counting the number of unique 
>> e-mails in
>> the archive.
>>
>> Another useful thing would be to count and plot the growth of number 
>> of R
>> (and Bioconductor) packages over the years. I know not all packages are
>> created equal.
>>
>> Regards, Adai.
>>
>>  
>>
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Duncan Murdoch
>>> Sent: 17 March 2004 14:03
>>> To: Spencer Graves
>>> Cc: r-help at stat.math.ethz.ch; giovanni merola
>>> Subject: Re: [R] R-business case
>>>
>>>
>>> On Tue, 16 Mar 2004 13:41:38 -0800, Spencer Graves
>>> <spencer.graves at pdf.com> wrote :
>>>
>>>   
>>>
>>>>     Some web sites have "hit counters".  It should be possible to get
>>>> a counts of the numbers of times different parts of R are downloaded.
>>>> Do the CRAN web sites include any such?
>>>>     
>>>
>>> The logs aren't currently available for general viewing, but I took a
>>> look at what we do have, and it shows rw1081.exe (the Windows binary
>>> build of R 1.8.1) being downloaded from the main CRAN site about 28000
>>> times.  Not all of those downloads were successful; they average
>>> around 10 Meg each, and the file is 22 Meg in size.  On the other
>>> hand, traffic to the mirrors isn't included at all.
>>>
>>> Duncan Murdoch
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>>   
>>
>>
>>
>>  
>>
>



From christof.bigler at colorado.edu  Mon Mar 22 23:10:59 2004
From: christof.bigler at colorado.edu (Christof Bigler)
Date: Mon, 22 Mar 2004 15:10:59 -0700
Subject: [R] Handling of NAs in functions lrm and robcov
Message-ID: <CCDEEDCC-7C4D-11D8-8F16-000A27D7D440@colorado.edu>

Hi R-helpers

I have a dataframe DF (lets say with the variables, y, x1, x2, x3, ..., 
clust) containing relatively many NAs.
When I fit an ordinal regression model with the function lrm from the 
Design library:
model.lrm <- lrm(y ~ x1 + x2, data=DF, x=TRUE, y=TRUE)
it will by default delete missing values in the variables y, x1, x2.
Based on model.lrm, I want to apply the robust covariance estimator 
using a cluster variable:
model.robcov.lrm <- robcov(model.lrm, cluster=clust)
How can I remove observations in the cluster variable clust that 
contain NAs in y, x1, and x2?

Thanks,
Christof



From donghu at itsa.ucsf.edu  Mon Mar 22 23:29:49 2004
From: donghu at itsa.ucsf.edu (donghu@itsa.ucsf.edu)
Date: Mon, 22 Mar 2004 14:29:49 PST
Subject: [R] lme question
Message-ID: <200403222229.i2MMTnLP010102@itsa.ucsf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040322/4ce050b5/attachment.pl

From andrewr at uidaho.edu  Mon Mar 22 23:39:38 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 22 Mar 2004 14:39:38 -0800
Subject: [R] lme question
In-Reply-To: <200403222229.i2MMTnLP010102@itsa.ucsf.edu>
References: <200403222229.i2MMTnLP010102@itsa.ucsf.edu>
Message-ID: <200403221439.38013.andrewr@uidaho.edu>

Hi Donglei,

what is your goal in fitting this model?  The statement that you've used is 
creating an overparameterized model, as it fits a random intercept for each 
visit within each subject.  I wonder if you might prefer

testresult <- lme(expr~visit, data=testdata, random=~1|subject)

which will fit a random intercept for each subject?

Andrew


On Monday 22 March 2004 14:29, donghu at itsa.ucsf.edu wrote:
> Hi,
>
> I have a dataset like this,
>
> > testdata
>
> Grouped Data: expr ~ visit | subject
>       expr visit subject
> 1 6.502782    V1       A
> 2 6.354506    V1       B
> 3 6.349184    V1       C
> 4 6.386301    V2       A
> 5 6.376405    V2       B
> 6 6.758640    V2       C
> 7 6.414142    V3       A
> 8 6.354521    V3       B
> 9 6.396636    V3       C
>
> I tried the command
>
> > testresult=lme(expr~visit,data=testdata,random=~visit|subject)
>
> I got the following error message.
> Error in MEestimate(lmeSt, grps) : Singularity in backsolve at level 0,
> block 1 In addition: There were 50 or more warnings (use warnings() to see
> the first 50)
>
> Could someone give me some hint on what went wrong?  Thanks.
>
> Donglei Hu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From p.murrell at auckland.ac.nz  Tue Mar 23 01:45:45 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 23 Mar 2004 12:45:45 +1200
Subject: [R] writing text on graphics' window
References: <200403212134.i2LLYZtn002907@tahi.mcs.vuw.ac.nz>
Message-ID: <405F88B9.9060707@stat.auckland.ac.nz>

Hi


Ray Brownrigg wrote:
>>From: Duncan Murdoch <dmurdoch at pair.com>
>>Date: Sun, 21 Mar 2004 06:58:03 -0500
>>
>>On Sun, 21 Mar 2004 11:07:01 +0000, you [sam.kemp2 at ntlworld.com] wrote:
>>
>>
>>>Does anyone know of a method for writing text to the graphics window, 
>>>where there is *no* plot? Basically, I have developed a 'significance 
>>>test' and I would like the output on the graphics window to say 
>>>something about the input parameters and the stats of the significance test.
>>
>>You need to make sure a graphics device is active and establish a
>>coordinate system there.  The easiest way to do that is to make a call
>>to plot() with everything turned off:
>>
>> plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")
>>
>>You may also want to reduce the "margins" if you want your output to
>>take up the full frame, e.g.
>>
>> oldmargins <- par(mar=c(0,0,0,0))
>> plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")
>>
> 
> An easier way to activate a graphics device and establish a coordinate
> system is to call plot.new().
> 
> 
> Try:
> 
>>plot.new()
>>text(0, 0, "ABC")
>>par("usr")
> 
> [1] -0.04  1.04 -0.04  1.04


The grid package gives you the whole page to play with by default and 
gives you more flexibility in how you place the text.  Try ...

library(grid)
grid.text("Here's some\ntext", x=unit(1, "cm"),
           y=unit(1, "npc") - unit(1, "cm"),
           just=c("left", "top"))

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From pxt at ph.adfa.edu.au  Tue Mar 23 01:55:53 2004
From: pxt at ph.adfa.edu.au (Pisut Tempatarachoke)
Date: Tue, 23 Mar 2004 11:55:53 +1100
Subject: [R] R-1.8.1-4: Font family, ticks and mathematical expression??
In-Reply-To: <Pine.LNX.4.44.0403181110490.10371-100000@stat71.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0403181110490.10371-100000@stat71.stat.auckland.ac.nz>
Message-ID: <405F8B19.6070409@ph.adfa.edu.au>

Thank you everyone for helpful hints.  Now I'm able to do what I wanted 
to do.  Thank you all.

Cheers
Tempo



From ernesto at ipimar.pt  Tue Mar 23 01:53:27 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 23 Mar 2004 00:53:27 +0000
Subject: [R] how to change text in lattice plot headers ?
Message-ID: <1080003206.7656.2.camel@mordor.ipimar.pt>

Hi,

I'm using lattice (bwplot) to produce some plots. I want to put the
value of the parameter and use plotmath on the top bar of each plot. Is
there an easy way to do it ?

Thanks

EJ



From ok at cs.otago.ac.nz  Tue Mar 23 02:16:11 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 23 Mar 2004 13:16:11 +1200 (NZST)
Subject: [R] Operating on windows of data
Message-ID: <200403230116.i2N1GBn8304951@atlas.otago.ac.nz>

Martin Maechler <maechler at stat.math.ethz.ch> wrote that
	[using apply functions is] not [better than using loops],
	not at all.

He would be right in all he says if time efficiency were the only
reason to prefer one coding style to another.

Loop-free notatation can reduce the number of variables in scope,
making the code easier to read and rearrange.  By separating "what to
do with the elements" from "how to find the elements", it can lead to
pieces which are separately reusable.  By reducing the volume of
code, it can result in code with fewer mistakes.

(Note the word "can" in each of those sentences.)



From deepayan at stat.wisc.edu  Tue Mar 23 02:23:08 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 22 Mar 2004 19:23:08 -0600
Subject: [R] how to change text in lattice plot headers ?
In-Reply-To: <1080003206.7656.2.camel@mordor.ipimar.pt>
References: <1080003206.7656.2.camel@mordor.ipimar.pt>
Message-ID: <200403221923.08451.deepayan@stat.wisc.edu>

On Monday 22 March 2004 18:53, Ernesto Jardim wrote:
> Hi,
>
> I'm using lattice (bwplot) to produce some plots. I want to put the
> value of the parameter and use plotmath on the top bar of each plot.
> Is there an easy way to do it ?

What do you mean by 'top bar' ? 

The last example in demo(lattice) illustrates usage of plotmath-like 
expressions in almost all conceivable places.

Deepayan



From p.murrell at auckland.ac.nz  Tue Mar 23 02:37:50 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 23 Mar 2004 13:37:50 +1200
Subject: [R] persp(), axis font size
References: <405F5EDA.3090608@williams.edu>
Message-ID: <405F94EE.2040403@stat.auckland.ac.nz>

Hi


Manuel Morales wrote:
> Is there a way to adjust the font size for axis labels when using 
> persp()? The parameter cex works for adjusting the global font size, but 
>  I can't seem to make cex.lab or cex.axis work for adjusting these 
> values independently. Or, is there a preferred method for making surface 
> plots in R?


persp() currently ignores cex.axis, cex.lab (and col.axis and font.axis 
...).

Paul

p.s.  For the record, I don't think it would be hard to get persp() to 
respond to these parameters;  there are a few lines of code in do_axis 
in plot.c (for example, ...

     Rf_gpptr(dd)->cex = Rf_gpptr(dd)->cexbase * Rf_gpptr(dd)->cexaxis;

...) that could be copied into perspAxis in plot3d.c and I think that 
would do the trick.
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From christof.bigler at colorado.edu  Tue Mar 23 04:55:46 2004
From: christof.bigler at colorado.edu (Christof Bigler)
Date: Mon, 22 Mar 2004 20:55:46 -0700
Subject: [R] Handling of NAs in functions lrm and robcov
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A58@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7A58@usrymx25.merck.com>
Message-ID: <F6D01122-7C7D-11D8-8DF9-00039376D216@colorado.edu>

I was pointed by Andy Liaw to use rownames(model.lrm$x) as index to 
clus.

Thanks!
Christof

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christof Bigler
> Sent: Monday, March 22, 2004 5:11 PM
> To: R-help
> Subject: [R] Handling of NAs in functions lrm and robcov
>
>
> Hi R-helpers
>
> I have a dataframe DF (lets say with the variables, y, x1,
> x2, x3, ...,
> clust) containing relatively many NAs.
> When I fit an ordinal regression model with the function lrm from the
> Design library:
> model.lrm <- lrm(y ~ x1 + x2, data=DF, x=TRUE, y=TRUE)
> it will by default delete missing values in the variables y, x1, x2.
> Based on model.lrm, I want to apply the robust covariance estimator
> using a cluster variable:
> model.robcov.lrm <- robcov(model.lrm, cluster=clust)
> How can I remove observations in the cluster variable clust that
> contain NAs in y, x1, and x2?



From rxg218 at psu.edu  Tue Mar 23 05:42:00 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 22 Mar 2004 23:42:00 -0500
Subject: [R] R equivilant to RAND_MAX in C
Message-ID: <1080016919.24094.15.camel@ra.chem.psu.edu>

Hello,
  I have some C code that I'm interfacing to R using the .C calling
interface. Currently the C code uses the rand() function from the GNU C
library to generate random numbers. Since I need the random numbers in a
range from 0 to a (where a is an integer) I use the RAND_MAX macro as

(int)(rand() * (float)(*nobs-1) / (RAND_MAX+1.0))

(taken from the rand() manpage)

However, since I have access to the R RNG's I'd like to use them.

Firstly, would it be an improvement to use the R RNG over that supplied
by the C library?

Secondly, I dont see any mention of an equivilant to RAND_MAX in the
documentation of runif() (as I want to use unif_rand() in my C code).

Is it valid to use the C libraries' RAND_MAX as the maximum value of the
RNG or am I missing something?

As far as I understand using the .C interface I can't call R functions
from my C code (which means I cant access runif()) - is this correct?
As I would rather stay with the .C interface rather than the .Call
interface is there a way to get random numbers within a given range?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All laws are simulations of reality.
-- John C. Lilly



From louize99 at yahoo.co.uk  Tue Mar 23 00:18:37 2004
From: louize99 at yahoo.co.uk (Louize Hill)
Date: Mon, 22 Mar 2004 23:18:37 -0000
Subject: [R] beginners question - kmeans
Message-ID: <001c01c41064$0236db20$18c98c52@Louisept>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040322/0984185a/attachment.pl

From ripley at stats.ox.ac.uk  Tue Mar 23 08:40:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Mar 2004 07:40:04 +0000 (GMT)
Subject: [R] R equivilant to RAND_MAX in C
In-Reply-To: <1080016919.24094.15.camel@ra.chem.psu.edu>
Message-ID: <Pine.LNX.4.44.0403230732310.2297-100000@gannet.stats>

On Mon, 22 Mar 2004, Rajarshi Guha wrote:

> Hello,
>   I have some C code that I'm interfacing to R using the .C calling
> interface. Currently the C code uses the rand() function from the GNU C
> library to generate random numbers. Since I need the random numbers in a
> range from 0 to a (where a is an integer) I use the RAND_MAX macro as
> 
> (int)(rand() * (float)(*nobs-1) / (RAND_MAX+1.0))
> 
> (taken from the rand() manpage)

That isn't a random *number*: it is a random *integer*.  It is a random 
integer on 0, ..., a=*nobs-2: is that what you wanted?

> However, since I have access to the R RNG's I'd like to use them.
> 
> Firstly, would it be an improvement to use the R RNG over that supplied
> by the C library?

Most likely.

> Secondly, I dont see any mention of an equivilant to RAND_MAX in the
> documentation of runif() (as I want to use unif_rand() in my C code).

That's because runif() is documented to return a *double*, and unif_rand() 
a uniform(0,1) *double*. This is in `Writing R Extensions'.

> Is it valid to use the C libraries' RAND_MAX as the maximum value of the
> RNG or am I missing something?

That the C rand() is *integer*.

> As far as I understand using the .C interface I can't call R functions
> from my C code (which means I cant access runif()) - is this correct?
> As I would rather stay with the .C interface rather than the .Call
> interface is there a way to get random numbers within a given range?

a + (b-a) * unif_rand()  for U(a, b)

(int) (a * unif_rand()) for a random integer in 0, ... , a - 1.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar 23 08:47:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Mar 2004 07:47:52 +0000 (GMT)
Subject: [R] beginners question - kmeans
In-Reply-To: <001c01c41064$0236db20$18c98c52@Louisept>
Message-ID: <Pine.LNX.4.44.0403230746150.2297-100000@gannet.stats>

You plotted a data frame, and your points are in 17 dimensions.  Why did 
you expect 17 points, and what would they be?  You clustered 91 points 
....

See examples in MASS (the book in the FAQ) for how you can plot 
clusterings.

On Mon, 22 Mar 2004, Louize Hill wrote:

> I am a complete beginner at R and am using the "kmeans" function for the first time...
> I have a data frame (dat) that is 17 columns *  91 rows (including headers)
> 
> I have entered the following:
> cl <- kmeans(dat, 3, 10)
> plot(dat, col = cl$cluster)
> points(cl$centers, col = 1:2, pch = 8)
> 
> the output is 17*17 graphs ...
> this is not what I was anticipating - I was hoping to get one graph 
> with 17 points in 3 clusters!
> can anyone point me in the right direction?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From olasov at medicine.ucsf.edu  Tue Mar 23 09:27:09 2004
From: olasov at medicine.ucsf.edu (Ben Olasov)
Date: Tue, 23 Mar 2004 00:27:09 -0800
Subject: [R] Common Lisp -> R interface
Message-ID: <web-3257407@medicine.ucsf.edu>

Is there a Common Lisp interface to R that's compatible with either Allegro
Common Lisp (solaris 8/sunblade 2000) or CMUCL (redhat 9)?   I use the emacs
lisp interface to R but would like to pass in data from a CL-based web server
(CL-HTTP).  

Thanks,

Ben



From maechler at stat.math.ethz.ch  Tue Mar 23 09:40:47 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 23 Mar 2004 09:40:47 +0100
Subject: [R] Operating on windows of data
In-Reply-To: <200403230116.i2N1GBn8304951@atlas.otago.ac.nz>
References: <200403230116.i2N1GBn8304951@atlas.otago.ac.nz>
Message-ID: <16479.63503.352478.909231@gargle.gargle.HOWL>

>>>>> "Richard" == Richard A O'Keefe <ok at cs.otago.ac.nz>
>>>>>     on Tue, 23 Mar 2004 13:16:11 +1200 (NZST) writes:

    Richard> Martin Maechler <maechler at stat.math.ethz.ch> wrote
    Richard> that [using apply functions is] not [better than
    Richard> using loops], not at all.

eehm, that's not what I said (nor intended to say)!

Here's part of my post:
      Ajay> approach above works. My question is: Why is this much
      Ajay> better than writing it using loops?

	MM> it's not, not at all.
	MM> And you are very much right in all you say below!

So what I said is that 
 >> 'this' is not much better than writing it using loops

and 'this' was sapply()ing a "heavy" function
{that rather should have been stream lined instead as Brian
 Ripley pointed out clearly}.

I do advocate and use sapply() very heavily myself,
but not for calling a "large" anonymous function
because in that case the style is not really improved.

    Richard> He would be right in all he says if time efficiency
    Richard> were the only reason to prefer one coding style to
    Richard> another.

    Richard> Loop-free notatation can reduce the number of
    Richard> variables in scope, making the code easier to read
    Richard> and rearrange.  By separating "what to do with the
    Richard> elements" from "how to find the elements", it can
    Richard> lead to pieces which are separately reusable.  By
    Richard> reducing the volume of code, it can result in code
    Richard> with fewer mistakes.

    Richard> (Note the word "can" in each of those sentences.)

I completely agree with your principles here,
but *please* note again the code snippet we where talking about :

    > dat <- sapply( seq(T-width), function(i) {
    >     model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
    >                 i:(i+width-1))
    >     details <- summary.lm(model)
    >     tmp <- coefficients(model)
    >     c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
    >            R2 = details$r.squared, RMSE = details$sigma )
    > } )
    > dat <- as.data.frame(t(dat))
    > attach(dat)

which is really an example where sapply() rather obfuscates than
clarifies.
And, BTW, when talking about code style,
attaching data frames is not good R *programming* style nowadays:
in programming, either use with(.) or access the components
directly (x$.. or x[".."] if you hate partial matching).

Martin



From marine_cms at yahoo.com  Tue Mar 23 09:59:09 2004
From: marine_cms at yahoo.com (Ridwan Sala)
Date: Tue, 23 Mar 2004 00:59:09 -0800 (PST)
Subject: [R] Does Chi Square test for R differ from S-Plus?
Message-ID: <20040323085909.72397.qmail@web21407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040323/f9a4c2b7/attachment.pl

From ripley at stats.ox.ac.uk  Tue Mar 23 10:12:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Mar 2004 09:12:38 +0000 (GMT)
Subject: [R] Does Chi Square test for R differ from S-Plus?
In-Reply-To: <20040323085909.72397.qmail@web21407.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403230907270.9037-100000@gannet.stats>

Depends on the `GLM' model and what you mean by that (general linear 
model, generalized linear model?).  And by `Anova' (capitalized), for that 
matter.

S-PLUS (sic) does some inconsistent things with dispersion parameters in 
glm() fits, and it is likely that you are using the wrong test.  But P 
values that near one should arouse your suspicions.

Please read the R posting guide, and then tell us enough of what you did
so we can help you.  

On Tue, 23 Mar 2004, Ridwan Sala wrote:

> I tried to run a GLM model using S-Plus and R. The same model and data were 
> applied to the both packages. I got different results of  P(>|Chi|) for Anova, 
> between S-PLus and R. Other estimates values are the same. 
> 
> P(>|Chi|) for S-Plus:
> 0.99565 0.99682 0.04871 0.84597 1.00000 0.99999 1.00000 1.00000 1.00000 0.95834
> 
> P(>|Chi|) for R:
> 0.00000 0.00000 0.00000  0.00000 0.00000 0.00000 0.00350 0.26400 0.46800 0.0000
> 
> I used S-Plus 6.1 for windows, Profesional Edition release 1, and R 1.8.1.
> 
> Does anyone have any idea how this could happen? 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aghosh at wiwiss.fu-berlin.de  Tue Mar 23 10:24:48 2004
From: aghosh at wiwiss.fu-berlin.de (Amit Ghosh)
Date: Tue, 23 Mar 2004 10:24:48 +0100
Subject: [R] optimal hardware for computations in R?
Message-ID: <005a01c410b8$b0545440$9e8d2da0@wrz03259>

Hi,

I am planning to buy a new PC for computing simulations in R under
Linux. I was searching the web/mailing list-archives for useful hints
about the "optimal" choice of hardware - surprisingly I found no recent
topics.

As far as I know, R doesn't use threads, so I think that there should be
no benefit in choosing a dual-processor machine.

So the remaining affordable choices seems to be Athlon XP, Pentium 4,
Xeon or Athlon64/Opteron. Are there any R-related benchmarks or should
one simply look about the "standard" benchmark-results (SPEC, etc.)? Any
hints or experiences would be appreciated!

Thanks in advance,

Amit



From Achim.Zeileis at wu-wien.ac.at  Mon Mar 22 20:49:42 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 22 Mar 2004 20:49:42 +0100
Subject: [R] useR! 2004 program online
Message-ID: <20040322204942.14bdae25.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

the program for the first R user conference useR! 2004 (taking place in
Vienna, May 20-22) is now available online at the conference web page:

  http://www.ci.tuwien.ac.at/Conferences/useR-2004/program.html

We received many exciting abstracts about a wide spectrum of
applications in which R is used. This is reflected in the program and I
think we can look forward to a very interesting useR! conference.
Note that the early registration deadline is 2004-03-31 so that you can
still register as an early bird if you want to visit this event.

For the organizing committee,
Achim Zeileis, Torsten Hothorn, and David Meyer

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From marine_cms at yahoo.com  Tue Mar 23 11:00:19 2004
From: marine_cms at yahoo.com (Ridwan Sala)
Date: Tue, 23 Mar 2004 02:00:19 -0800 (PST)
Subject: [R] Does Chi Square test for R differ from S-Plus?
In-Reply-To: <Pine.LNX.4.44.0403230907270.9037-100000@gannet.stats>
Message-ID: <20040323100019.79375.qmail@web21402.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040323/9256941b/attachment.pl

From ripley at stats.ox.ac.uk  Tue Mar 23 11:07:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Mar 2004 10:07:47 +0000 (GMT)
Subject: [R] optimal hardware for computations in R?
In-Reply-To: <005a01c410b8$b0545440$9e8d2da0@wrz03259>
Message-ID: <Pine.LNX.4.44.0403230959100.9151-100000@gannet.stats>

On Tue, 23 Mar 2004, Amit Ghosh wrote:

> I am planning to buy a new PC for computing simulations in R under
> Linux. I was searching the web/mailing list-archives for useful hints
> about the "optimal" choice of hardware - surprisingly I found no recent
> topics.

Most of seem to be buying dual Opterons, not least so we can potentially 
access more than 4Gb.

> As far as I know, R doesn't use threads, so I think that there should be
> no benefit in choosing a dual-processor machine.

It certainly can use a threaded BLAS.  You can also do two simulation runs 
simultaneously (and surely you will be doing more than one run?).

> So the remaining affordable choices seems to be Athlon XP, Pentium 4,
> Xeon or Athlon64/Opteron. Are there any R-related benchmarks or should
> one simply look about the "standard" benchmark-results (SPEC, etc.)? Any
> hints or experiences would be appreciated!

It really does depend on what exactly your computations do.  There are R 
`benchmarks', but they are not typical tasks (for me, and probably for no 
one else).

I would buy either a dual Athlon MP or a dual Opteron, and not worry too
much about this -- anything you buy today will look slow next year, and
you are not likely to see differences as large as 2x on one processor.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Mar 23 11:13:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Mar 2004 10:13:46 +0000 (GMT)
Subject: [R] Does Chi Square test for R differ from S-Plus?
In-Reply-To: <20040323100019.79375.qmail@web21402.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403231008010.9151-100000@gannet.stats>

That is a linear model (why use glm when lm will do?), and test="Chisq" is
wrong as the scale has not been specified.  You need an F test.  Had you 
used lm, you would not have been able to select the wrong test ....

BTW, please do not reply to individuals only as the list does not then
know the problem has been closed.

On Tue, 23 Mar 2004, Ridwan Sala wrote:

> I used generalized linear model with the commands as follows:
>  
> For S-Plus:
> >attach(data)
> >options(contrasts=c("contr.treatment","contr.poly"))
> >model1<-glm(logcatchrate~Year+Month+Vessclass+Livebaitday+....+Vessclass:Livebaitday,family=gaussian)
> >anova(model1,test="Chisq")
>  
> For R :
> >data<-read.table("data.dat", header=T)
> >fix(data)
> >attach(data)
> >model1<-glm(logcatchrate~Year+Month+Vessclass+Livebaitday+...+Vessclass:Livebaitday,family=gaussian)>anova(model1,test="Chisq")
>  
>  
>  
> 
> Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Depends on the `GLM' model and what you mean by that (general linear 
> model, generalized linear model?). And by `Anova' (capitalized), for that 
> matter.
> 
> S-PLUS (sic) does some inconsistent things with dispersion parameters in 
> glm() fits, and it is likely that you are using the wrong test. But P 
> values that near one should arouse your suspicions.
> 
> Please read the R posting guide, and then tell us enough of what you did
> so we can help you. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From meinhardploner at gmx.net  Tue Mar 23 12:17:39 2004
From: meinhardploner at gmx.net (Meinhard Ploner)
Date: Tue, 23 Mar 2004 12:17:39 +0100
Subject: [R] how to modify variables of another frame (but not global)
Message-ID: <B1C3A15A-7CBB-11D8-ACAC-0003930EA956@gmx.net>

Hello!

Maybe "frame" is not the right term in this context.
I explain my problem by example code:

fun2 <- function(objName, add) {
	## the object "objName" should be increased by "add",
	## but the evaluation should be done in the calling function (here: 
fun1)
	##    ...... what's the right code??
}

fun1 <- function() {
	x <- 1

	fun2("x", 10)		## should modify "x"

	## now x should be 11, but only here and NOT globally!
	...
}


I would like to appreciate any solution!
Thanks in advance

Meinhard Ploner



From hb at maths.lth.se  Tue Mar 23 12:28:30 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 23 Mar 2004 12:28:30 +0100
Subject: [R] how to modify variables of another frame (but not global)
In-Reply-To: <B1C3A15A-7CBB-11D8-ACAC-0003930EA956@gmx.net>
Message-ID: <000d01c410c9$f8206500$e502eb82@maths.lth.se>

Hi, this has recently been discussed r-help. Please search the archive
for more details. The short summary is that you need to use assign()
or the <<- assignment operator depending on your exact problem.

Cheers

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Meinhard
Ploner
> Sent: den 23 mars 2004 12:18
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to modify variables of another frame (but not
global)
> 
> 
> Hello!
> 
> Maybe "frame" is not the right term in this context.
> I explain my problem by example code:
> 
> fun2 <- function(objName, add) {
> 	## the object "objName" should be increased by "add",
> 	## but the evaluation should be done in the calling 
> function (here: 
> fun1)
> 	##    ...... what's the right code??
> }
> 
> fun1 <- function() {
> 	x <- 1
> 
> 	fun2("x", 10)		## should modify "x"
> 
> 	## now x should be 11, but only here and NOT globally!
> 	...
> }
> 
> 
> I would like to appreciate any solution!
> Thanks in advance
> 
> Meinhard Ploner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From buffa at gci.ac.uk  Tue Mar 23 12:30:35 2004
From: buffa at gci.ac.uk (Francesca Buffa)
Date: Tue, 23 Mar 2004 11:30:35 -0000
Subject: [R] help on data.frames/data.entry
Message-ID: <000001c410ca$4258f1a0$8201000a@pc5571005>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040323/02548001/attachment.pl

From sdavis2 at mail.nih.gov  Tue Mar 23 12:50:27 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 23 Mar 2004 06:50:27 -0500
Subject: [R] help on data.frames/data.entry
References: <000001c410ca$4258f1a0$8201000a@pc5571005>
Message-ID: <000f01c410cd$0c584cb0$2f643744@WATSON>

Francesca,

See ?colnames and ?rownames.  Also, I highly suggest looking at some of the
documentation at www.r-project.org.  There are many, many pages of
well-documented examples of working with data frames and other R data
structures that you may find useful.

Sean
----- Original Message -----
From: "Francesca Buffa" <buffa at gci.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, March 23, 2004 6:30 AM
Subject: [R] help on data.frames/data.entry


> Dear R-help,
>
>
>
> I have a matrix D of dimensions 200 x 26; rows are my cases and columns
are
> my variables. I would like to create a data.frame "DF" with row.names=
> myrownames (this is a vector of characters of length 200) and column names
> "mycolnames" (a vector of characters of length 26). I could find the
option
> in data.frame for naming rows, but not for naming columns. So I did:
>
> > data.entry(D, Names= mycolnames)
>
> > DF <- data.frame(D,row.names=myrownames)
>
> but after the first command (i.e. > data.entry(D, Names= mycolnames)) I've
> received the warning:
>
> Warning message:
>
> the condition has length > 1 and only the first element will be used in:
if
> (dim(x) == dim(args[[i]])) rn <- dimnames(args[[i]])[[1]] else rn <- NULL
>
> what does this warning mean? Is it affecting the values in my data.frame
> table? Or something else? Is there a better way of defining columns names
in
> a data.frame in automated way?
>
>
>
> Thank you very much
>
> Francesca
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Mar 23 12:59:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 23 Mar 2004 12:59:08 +0100
Subject: [R] how to modify variables of another frame (but not global)
In-Reply-To: <B1C3A15A-7CBB-11D8-ACAC-0003930EA956@gmx.net>
References: <B1C3A15A-7CBB-11D8-ACAC-0003930EA956@gmx.net>
Message-ID: <4060268C.9080307@statistik.uni-dortmund.de>

Meinhard Ploner wrote:

> Hello!
> 
> Maybe "frame" is not the right term in this context.
> I explain my problem by example code:
> 
> fun2 <- function(objName, add) {
>     ## the object "objName" should be increased by "add",
>     ## but the evaluation should be done in the calling function (here: 
> fun1)
>     ##    ...... what's the right code??
> }
> 
> fun1 <- function() {
>     x <- 1
> 
>     fun2("x", 10)        ## should modify "x"
> 
>     ## now x should be 11, but only here and NOT globally!
>     ...
> }
> 
> 
> I would like to appreciate any solution!
> Thanks in advance
> 
> Meinhard Ploner
> 


fun2 <- function(objName, add) {
     x <- get(objName, pos = parent.frame()) + add
     assign(objName, x, pos = parent.frame())
}

fun1 <- function() {
     x <- 1
     fun2("x", 10)
     return(x)
}

fun1()
[1] 11


Uwe Ligges



From aroscino at dss.uniba.it  Tue Mar 23 13:03:20 2004
From: aroscino at dss.uniba.it (Annarita Roscino)
Date: Tue, 23 Mar 2004 13:03:20 +0100
Subject: [R] log likelihood maximization with optim 
Message-ID: <000501c410ce$d97a39f0$08befea9@lab1>

Dear all.
I am trying to maximize a complex log likelihood function with respect to 10
parameters using the method "L-BFGS-B" implemented in the optim procedure .
The algorithm that I have written always converges, but I have got different
solutions running the algorithm many times on the same dataset. I suspect
that the log likelihood is flat....
The results usually differ of a quantity that is small (+- 0.04). But, given
that  some of the parameters can vary from -1 to 1, I am not satisfied with
the variability in the solutions and I do not know how to choose a solution
among the results that I get.
I have tried to iterate the optim procedure, using as initial values the
results of the previous step to see if the algorithm does rich the
convergence.But it does not, after 1000 iterations.
I would like to summarise the results of the maximization procedure at the
different iterations as an estimator of the unknown parameters, for
instance, as a kind of MC average.
Does anybody have any expercience in such a theme?

I would really appreciate comments or ideas! It is very important!
Many thanks,
Annarita

Annarita Roscino
Department of Statistical Sciences
University of Bari
tel. 00390805049353
email: aroscino at dss.uniba.it



From aroscino at dss.uniba.it  Tue Mar 23 13:39:07 2004
From: aroscino at dss.uniba.it (Annarita Roscino)
Date: Tue, 23 Mar 2004 13:39:07 +0100
Subject: [R] log likelihood maximization with optim 
Message-ID: <000801c410d3$d850e2e0$08befea9@lab1>





Dear all.
I am trying to maximize a complex log likelihood function with respect to 10
 parameters using the method "L-BFGS-B" implemented in the optim procedure .
The algorithm that I have written always converges, but I have got different
 solutions running the algorithm many times on the same dataset. I suspect
 that the log likelihood is flat....
 The results usually differ of a quantity that is small (+- 0.04). But,
given
 that  some of the parameters can vary from -1 to 1, I am not satisfied with
 the variability in the solutions and I do not know how to choose a solution
 among the results that I get.
 I have tried to iterate the optim procedure, using as initial values the
 results of the previous step to see if the algorithm does rich the
 convergence.But it does not, after 1000 iterations.
 I would like to summarise the results of the maximization procedure at the
 different iterations as an estimator of the unknown parameters, for
 instance, as a kind of MC average.
 Does anybody have any expercience in such a theme?

 I would really appreciate comments or ideas! It is very important!
 Many thanks,
Annarita

 Annarita Roscino
Department of Statistical Sciences
University of Bari
 tel. 00390805049353
email: aroscino at dss.uniba.it



From aroscino at dss.uniba.it  Tue Mar 23 14:14:09 2004
From: aroscino at dss.uniba.it (Annarita Roscino)
Date: Tue, 23 Mar 2004 14:14:09 +0100
Subject: [R] log likelihood maximization with optim 
Message-ID: <000f01c410d8$bdb56aa0$08befea9@lab1>

Apologies for the multiple postings.

Dear all.
I am trying to maximize a complex log likelihood function with respect to 10
 parameters using the method "L-BFGS-B" implemented in the optim procedure .
The algorithm that I have written always converges, but I have got different
 solutions running the algorithm many times on the same dataset. I suspect
 that the log likelihood is flat....
 The results usually differ of a quantity that is small (+- 0.04). But,
given
 that  some of the parameters can vary from -1 to 1, I am not satisfied with
 the variability in the solutions and I do not know how to choose a solution
 among the results that I get.
 I have tried to iterate the optim procedure, using as initial values the
 results of the previous step to see if the algorithm does rich the
 convergence.But it does not, after 1000 iterations.
 I would like to summarise the results of the maximization procedure at the
 different iterations as an estimator of the unknown parameters, for
 instance, as a kind of MC average.
 Does anybody have any expercience in such a theme?

 I would really appreciate comments or ideas! It is very important!
 Many thanks,
Annarita

 Annarita Roscino
Department of Statistical Sciences
University of Bari
 tel. 00390805049353
email: aroscino at dss.uniba.it



From MSchwartz at MedAnalytics.com  Tue Mar 23 14:58:43 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Mar 2004 07:58:43 -0600
Subject: [R] useR! 2004 program online
In-Reply-To: <20040322204942.14bdae25.Achim.Zeileis@wu-wien.ac.at>
References: <20040322204942.14bdae25.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <1080050322.7675.624.camel@localhost.localdomain>

On Mon, 2004-03-22 at 13:49, Achim Zeileis wrote:
> Dear useRs,
> 
> the program for the first R user conference useR! 2004 (taking place in
> Vienna, May 20-22) is now available online at the conference web page:
> 
>   http://www.ci.tuwien.ac.at/Conferences/useR-2004/program.html
> 
> We received many exciting abstracts about a wide spectrum of
> applications in which R is used. This is reflected in the program and I
> think we can look forward to a very interesting useR! conference.
> Note that the early registration deadline is 2004-03-31 so that you can
> still register as an early bird if you want to visit this event.
> 
> For the organizing committee,
> Achim Zeileis, Torsten Hothorn, and David Meyer


Achim, Torsten and David,

Congratulations on putting together a superb program for this first
useR! meeting!

A hearty "Well Done!" to you, the speakers and the program participants!

Best regards,

Marc Schwartz



From ypeng at math.mun.ca  Tue Mar 23 15:08:00 2004
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Tue, 23 Mar 2004 10:38:00 -0330
Subject: [R] optimal hardware for computations in R?
References: <Pine.LNX.4.44.0403230959100.9151-100000@gannet.stats>
Message-ID: <406044C0.2030602@math.mun.ca>

I recently ordered a computer which is intended to run both WindowsXP
and Linux (of course both versions of R as well). Before placing the
order, I discussed it with our system managers. They highly recommanded
a system with one P4 CPU with Intel's so called "hyper-threading"
technology over a system with two CPU's, and they claimed that both OS's
can take benefits from the "hyper-threading" technology. I haven't got
the machine yet and don't know how fast it is. At least this is another
option available.

Paul.


Prof Brian Ripley wrote:
> On Tue, 23 Mar 2004, Amit Ghosh wrote:
> 
> 
>>I am planning to buy a new PC for computing simulations in R under
>>Linux. I was searching the web/mailing list-archives for useful hints
>>about the "optimal" choice of hardware - surprisingly I found no recent
>>topics.
> 
> 
> Most of seem to be buying dual Opterons, not least so we can potentially 
> access more than 4Gb.
> 
> 
>>As far as I know, R doesn't use threads, so I think that there should be
>>no benefit in choosing a dual-processor machine.
> 
> 
> It certainly can use a threaded BLAS.  You can also do two simulation runs 
> simultaneously (and surely you will be doing more than one run?).
> 
> 
>>So the remaining affordable choices seems to be Athlon XP, Pentium 4,
>>Xeon or Athlon64/Opteron. Are there any R-related benchmarks or should
>>one simply look about the "standard" benchmark-results (SPEC, etc.)? Any
>>hints or experiences would be appreciated!
> 
> 
> It really does depend on what exactly your computations do.  There are R 
> `benchmarks', but they are not typical tasks (for me, and probably for no 
> one else).
> 
> I would buy either a dual Athlon MP or a dual Opteron, and not worry too
> much about this -- anything you buy today will look slow next year, and
> you are not likely to see differences as large as 2x on one processor.
>



From dj at research.bell-labs.com  Tue Mar 23 15:09:42 2004
From: dj at research.bell-labs.com (David James)
Date: Tue, 23 Mar 2004 09:09:42 -0500
Subject: [R] useR! 2004 program online
In-Reply-To: <1080050322.7675.624.camel@localhost.localdomain>;
	from MSchwartz@MedAnalytics.com on Tue, Mar 23, 2004 at
	07:58:43AM -0600
References: <20040322204942.14bdae25.Achim.Zeileis@wu-wien.ac.at>
	<1080050322.7675.624.camel@localhost.localdomain>
Message-ID: <20040323090942.B2240@jessie.research.bell-labs.com>

Marc,

I agree with you 100% -- they've done a superb job!  Unfortunately
you sent the email to the wrong David:-)

-- 
David A. James
Statistics Research, Room 2C-253            Phone:  (908) 582-3082       
Bell Labs, Lucent Technologies              Fax:    (908) 582-3340
Murray Hill, NJ 09794-0636

Marc Schwartz wrote:
> On Mon, 2004-03-22 at 13:49, Achim Zeileis wrote:
> > Dear useRs,
> > 
> > the program for the first R user conference useR! 2004 (taking place in
> > Vienna, May 20-22) is now available online at the conference web page:
> > 
> >   http://www.ci.tuwien.ac.at/Conferences/useR-2004/program.html
> > 
> > We received many exciting abstracts about a wide spectrum of
> > applications in which R is used. This is reflected in the program and I
> > think we can look forward to a very interesting useR! conference.
> > Note that the early registration deadline is 2004-03-31 so that you can
> > still register as an early bird if you want to visit this event.
> > 
> > For the organizing committee,
> > Achim Zeileis, Torsten Hothorn, and David Meyer
> 
> 
> Achim, Torsten and David,
> 
> Congratulations on putting together a superb program for this first
> useR! meeting!
> 
> A hearty "Well Done!" to you, the speakers and the program participants!
> 
> Best regards,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Mar 23 15:12:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 23 Mar 2004 09:12:21 -0500 (EST)
Subject: [R] how to modify variables of another frame (but not global)
Message-ID: <20040323141221.0C3E239BD@mprdmxin.myway.com>


There are three ways I know of.  Actually there are some QUESTIONS I
have related to all this which I have interspersed.

The first, f1, uses assign/get and is shown in f1 below.  Note that we had to
copy all of z even though we only changed one element.  You can't do this:
   assign("z[1]",...) # wrong

The second, f2, uses <<- which searches through its parent environments for the
assigned-to variable.  As can be seen in f2a near the end it appears to do the
same thing as assign underneath as the entire z in f seems to have been copied
over.  Note that the right hand side of the assignment is still handled
normally.  QUESTION: Could someone explain what is going on in f2a in detail???

The third, f3 uses eval.parent.  It looks similar to <<- but its actually
different.  eval.parent evaluates the expression in the parent environment of f
so it never looks within f.  Both sides of the assignment are handled in the
parent environment in this one. This can be seen in f3a where the z defined in
f is completely ignored.  I believe that eval.parent does modify individual
cells without copying the entire structure.  QUESTION:  Is that right ???

> # 1. assign and get
> 
> f1 <- function() {
+    f <- function(x,k) { 
+       xc <- as.character(substitute(x))   # character representation of name
+       x <- get( xc, parent.frame() )
+       x[1] <- x[1] + k
+       assign( xc, x, parent.frame() )
+    }
+    z <- 1:5
+    f(z,1)
+    print(z)
+ }
> z <- 7
> f1()
[1] 2 2 3 4 5
> z
[1] 7


> # 2. <<-
> 
> f2 <- function() {
+    f <- function(x,k) eval(eval(substitute( x[1] <<- x[1] + k )))
+    z <- 1:5
+    f(z,1)
+    print(z)
+ }
> z <- 7
> f2()
[1] 2 2 3 4 5
> z
[1] 7
> 
> # 3. eval.parent
> 
> f3 <- function() {
+    f <- function(x,k) eval(eval.parent(substitute(x[1] <- x[1] + k )))
+    z <- 1:5
+    f(z,1)
+    print(z)
+ }
> z <- 7
> f3()
[1] 2 2 3 4 5
> z
[1] 7


> # <<- with a local z
> 
> f2a <- function() {
+    f <- function(x,k) {
+       z <- 10:12
+       eval(eval(substitute( x[1] <<- x[1] + k )))
+    }
+    z <- 1:5
+    f(z,1)
+    print(z)
+ }
> z <- 7
> f2a()
[1] 11 11 12
> z
[1] 7


> # eval parent with a local z
> 
> f3a <- function() {
+    f <- function(x,k) {
+       z <- 10:12
+       eval(eval.parent(substitute(x[1] <- x[1] + k )))
+    }
+    z <- 1:5
+    f(z,1)
+    print(z)
+ }
> z <- 7
> f3a()
[1] 2 2 3 4 5
> z
[1] 7
> 

> R.version.string
[1] "R version 1.8.1, 2003-11-21"

------------------------------------------------------

Here is a copy of just the input 



# 1. assign and get

f1 <- function() {
   f <- function(x,k) { 
      xc <- as.character(substitute(x))   # character representation of name
      x <- get( xc, parent.frame() )
      x[1] <- x[1] + k
      assign( xc, x, parent.frame() )
   }
   z <- 1:5
   f(z,1)
   print(z)
}
z <- 7
f1()
z

# 2. <<-

f2 <- function() {
   f <- function(x,k) eval(eval(substitute( x[1] <<- x[1] + k )))
   z <- 1:5
   f(z,1)
   print(z)
}
z <- 7
f2()
z

# 3. eval.parent

f3 <- function() {
   f <- function(x,k) eval(eval.parent(substitute(x[1] <- x[1] + k )))
   z <- 1:5
   f(z,1)
   print(z)
}
z <- 7
f3()
z

# <<- with a local z

f2a <- function() {
   f <- function(x,k) {
      z <- 10:12
      eval(eval(substitute( x[1] <<- x[1] + k )))
   }
   z <- 1:5
   f(z,1)
   print(z)
}
z <- 7
f2a()
z

# eval parent with a local z

f3a <- function() {
   f <- function(x,k) {
      z <- 10:12
      eval(eval.parent(substitute(x[1] <- x[1] + k )))
   }
   z <- 1:5
   f(z,1)
   print(z)
}
z <- 7
f3a()
z

R.version.string

---

Date:   Tue, 23 Mar 2004 12:17:39 +0100 
From:   Meinhard Ploner <meinhardploner at gmx.net>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] how to modify variables of another frame (but not global) 

 
Hello!

Maybe "frame" is not the right term in this context.
I explain my problem by example code:

fun2 <- function(objName, add) {
     ## the object "objName" should be increased by "add",
     ## but the evaluation should be done in the calling function (here: 
fun1)
     ## ...... what's the right code??
}

fun1 <- function() {
     x <- 1

     fun2("x", 10)          ## should modify "x"

     ## now x should be 11, but only here and NOT globally!
     ...
}


I would like to appreciate any solution!
Thanks in advance

Meinhard Ploner



From flom at ndri.org  Tue Mar 23 15:18:38 2004
From: flom at ndri.org (Peter Flom)
Date: Tue, 23 Mar 2004 09:18:38 -0500
Subject: [R] useR! 2004 program online
Message-ID: <s0600115.071@MAIL.NDRI.ORG>

This looks like a great program!  Unfortunately, my budget won't stretch
to Vienna

Any plans to have any R conferences in the US?

Thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From christopherlmarshall at yahoo.com  Tue Mar 23 15:26:15 2004
From: christopherlmarshall at yahoo.com (Christopher Marshall)
Date: Tue, 23 Mar 2004 06:26:15 -0800 (PST)
Subject: [R] question on R's makefile and configure system
Message-ID: <20040323142615.65792.qmail@web41503.mail.yahoo.com>

I am trying to make a compiled slackware package for R and I am stuck on one point.

Is there a variable I can set on the "make install" step to cause the R files to be copied into a
subdirectory, instead of being copied into the prefix path specified in "./configure
--prefix=/usr"  anchored at root?

With a lot of packages that use autoconf, the variable DESTDIR serves that purpose.  For example,
with gawk, this sequence:

mkdir /tmp/gawk-package
./configure --prefix=/usr
make
make install DESTDIR=/tmp/gawk-package

would result in the compiled and distribution files being copied to /tmp/gawk-package/usr, where
they could be processed by a distribution's package creation tools (makepkg, in the case of
slackware).

Is there a variable in the R configure system that serves this purpose?

Chris Marshall



From MSchwartz at MedAnalytics.com  Tue Mar 23 15:28:54 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Mar 2004 08:28:54 -0600
Subject: [R] optimal hardware for computations in R?
In-Reply-To: <406044C0.2030602@math.mun.ca>
References: <Pine.LNX.4.44.0403230959100.9151-100000@gannet.stats>
	<406044C0.2030602@math.mun.ca>
Message-ID: <1080052134.7675.649.camel@localhost.localdomain>

On Tue, 2004-03-23 at 08:08, Paul Y. Peng wrote:
> I recently ordered a computer which is intended to run both WindowsXP
> and Linux (of course both versions of R as well). Before placing the
> order, I discussed it with our system managers. They highly recommanded
> a system with one P4 CPU with Intel's so called "hyper-threading"
> technology over a system with two CPU's, and they claimed that both OS's
> can take benefits from the "hyper-threading" technology. I haven't got
> the machine yet and don't know how fast it is. At least this is another
> option available.
> 
> Paul.

<snip>

Paul,

Others may chime in here, but you should be aware that there are still
lingering problems with Linux and HT in uni-processor systems, at least
using stock 2.4 (and even 2.6 kernels).

There is a good article at 2cpu.com on HT and the 2.6 kernels here:

http://www.2cpu.com/articles/41_1.html

Under FC1, which I use, there are issues with the 2.4 SMP kernels with
HT enabled. I have a 3.2 Ghz P4 with HT in a Dell i5150 laptop. I had to
disable HT in BIOS and am running the UP kernel, due to a list of known
bugs in the FC1 2.4 kernel series, which include boot lockups, other
boot time errors and even things as subtle as keyboard related problems.

Also, according to Alan Cox at RH, there are still performance tuning
issues relative to process and thread scheduling for HT on the 2.6
kernels that are yet to be included (but will be).

As you will see from the above article, the gains to be had from HT are
likely to be situationally specific and not an "all or nothing" gain. 

HTH,

Marc Schwartz



From andy_liaw at merck.com  Tue Mar 23 15:34:33 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Mar 2004 09:34:33 -0500
Subject: [R] optimal hardware for computations in R?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A62@usrymx25.merck.com>

I thought there are quite a bit of evidence that Hyper-Threading may be more
of a debit than credit under many conditions.  Is that not so?  We have a
dual Xeon box running Linux, and we disabled the HT.

If the computing tasks will ever reach the 3GB limit on memory, the best
choice is still AMD64.  Even though we have 8GB on the dual Xeon, each
process is still limited to 3GB of RAM.  For our application, we'd get as
much RAM as possible, and not necessarily the fastest CPU.  The dual Opteron
244 we have is about 0%-15% faster than the dual 2.4GHz Xeon, depending on
the tasks.

Also, you can easily make use of dual CPUs with Luke's `snow' package.

Andy

> From: Paul Y. Peng
> 
> I recently ordered a computer which is intended to run both WindowsXP
> and Linux (of course both versions of R as well). Before placing the
> order, I discussed it with our system managers. They highly 
> recommanded
> a system with one P4 CPU with Intel's so called "hyper-threading"
> technology over a system with two CPU's, and they claimed 
> that both OS's
> can take benefits from the "hyper-threading" technology. I haven't got
> the machine yet and don't know how fast it is. At least this 
> is another
> option available.
> 
> Paul.
> 
> 
> Prof Brian Ripley wrote:
> > On Tue, 23 Mar 2004, Amit Ghosh wrote:
> > 
> > 
> >>I am planning to buy a new PC for computing simulations in R under
> >>Linux. I was searching the web/mailing list-archives for 
> useful hints
> >>about the "optimal" choice of hardware - surprisingly I 
> found no recent
> >>topics.
> > 
> > 
> > Most of seem to be buying dual Opterons, not least so we 
> can potentially 
> > access more than 4Gb.
> > 
> > 
> >>As far as I know, R doesn't use threads, so I think that 
> there should be
> >>no benefit in choosing a dual-processor machine.
> > 
> > 
> > It certainly can use a threaded BLAS.  You can also do two 
> simulation runs 
> > simultaneously (and surely you will be doing more than one run?).
> > 
> > 
> >>So the remaining affordable choices seems to be Athlon XP, 
> Pentium 4,
> >>Xeon or Athlon64/Opteron. Are there any R-related 
> benchmarks or should
> >>one simply look about the "standard" benchmark-results 
> (SPEC, etc.)? Any
> >>hints or experiences would be appreciated!
> > 
> > 
> > It really does depend on what exactly your computations do. 
>  There are R 
> > `benchmarks', but they are not typical tasks (for me, and 
> probably for no 
> > one else).
> > 
> > I would buy either a dual Athlon MP or a dual Opteron, and 
> not worry too
> > much about this -- anything you buy today will look slow 
> next year, and
> > you are not likely to see differences as large as 2x on one 
> processor.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Tue Mar 23 15:35:30 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Mar 2004 08:35:30 -0600
Subject: [R] useR! 2004 program online
In-Reply-To: <20040323090942.B2240@jessie.research.bell-labs.com>
References: <20040322204942.14bdae25.Achim.Zeileis@wu-wien.ac.at>
	<1080050322.7675.624.camel@localhost.localdomain>
	<20040323090942.B2240@jessie.research.bell-labs.com>
Message-ID: <1080052530.7675.656.camel@localhost.localdomain>

On Tue, 2004-03-23 at 08:09, David James wrote:
> Marc,
> 
> I agree with you 100% -- they've done a superb job!  Unfortunately
> you sent the email to the wrong David:-)


David,

My e-mail went openly to r-help, which is why you got it.

It was a public congratulations.

:-)

Best regards,

Marc



From Timur.Elzhov at jinr.ru  Tue Mar 23 15:53:13 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Tue, 23 Mar 2004 17:53:13 +0300
Subject: [R] par() usage
Message-ID: <20040323145313.GA5389@nf034.jinr.ru>

Dear R experts.

I saw in a lot of examples the following R code:

    x11()
    op <- par(no.readonly = TRUE)
    par(op)
        Warning message: 
        calling par(new=) with no plot 

Why I get a warning? I'm doing something wrong?
Thank you!

--
WBR,
Timur



From andy_liaw at merck.com  Tue Mar 23 16:27:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Mar 2004 10:27:28 -0500
Subject: [R] data.frame preserves names for all columns
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A65@usrymx25.merck.com>

Dear R-help,

I was surprised to find that data.frame() keeps the names of all columns:

> x1 <- structure(1:5, names=1:5)
> x2 <- structure(5:1, names=5:1)
> x3 <- structure(10:6, names=10:6)
> x <- data.frame(x1, x2, x3)
> str(x)
`data.frame':   5 obs. of  3 variables:
 $ x1: Named int  1 2 3 4 5
  ..- attr(*, "names")= chr  "1" "2" "3" "4" ...
 $ x2: Named int  5 4 3 2 1
  ..- attr(*, "names")= chr  "5" "4" "3" "2" ...
 $ x3: Named int  10 9 8 7 6
  ..- attr(*, "names")= chr  "10" "9" "8" "7" ...

I could not find mention of this in ?data.frame.  The problem is that when I
extract a column of `x', I would be expected the rownames of the data frame
to be used as the names (and short of that, no names attribute), rather than
the original names.  Is this intentional?

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820



From olau at fas.harvard.edu  Tue Mar 23 16:59:14 2004
From: olau at fas.harvard.edu (Olivia Lau)
Date: Tue, 23 Mar 2004 10:59:14 -0500 (EST)
Subject: [R] New R package
Message-ID: <Pine.LNX.4.58.0403231057030.27648@ls03.fas.harvard.edu>



               Zelig: Everyone's Statistical Software

                Kosuke Imai, Gary King and Olivia Lau

                             Version 1.0

            (Available at http://gking.harvard.edu/zelig)


A growing proportion of statisticians and methodologists from many
disciplines are converging on R, a powerful statistics package and
programming language.  As an open source project, R is freely
accessible.  With thousands of contributors who have written hundreds
of packaged routines, R can deal with nearly any statistical problem.
Although this high level of participation may be its greatest
strength, the enormous diversity in approaches to statistical
inference covered by R often results in a virtual babel of competing
functions and inconsistent syntax.

To address these problems from a common perspective, we have created
Zelig, a single, easy-to-use program, with a unified framework and
syntax, that can estimate, help interpret, and present the results of
a large range of statistical methods. It literally _is_ "everyone's
statistical software" because Zelig uses R code from many researchers.
We also hope it will _become_ "everyone's statistical software" for
applications, and we have designed it so that anyone can use it or add
their methods to it.  Zelig comes with detailed, self-contained
documentation that minimizes startup costs for Zelig and R, automates
graphics and summaries for all models, and, with only three simple
commands required, generally makes the power of R accessible for all
users.  Zelig also works well for teaching, and is designed so that
scholars can use the same program they use for their research.

Zelig adds considerable infrastructure to improve the use of existing
methods.  Zelig
 - implements and generalizes the program Clarify (for Stata),
   which translates hard-to-interpret coefficients into quantities of
   interest;
 - combines multiply imputed data sets (such as output from Amelia) to
   deal with missing data;
 - automates bootstrap simulation for all models;
 - uses sophisticated nonparametric matching commands which improve
   parametric procedures (via MatchIt);
 - allows one-line commands to run analyses in all designated strata;
 - automates the creation of replication data files so that you (or,
   if you wish, anyone else) can replicate the results of your
   analyses (hence satisfying the replication standard); and
 - makes conditional population and superpopulation inferences.

If you wish to add your models to Zelig, the documentation also
includes instructions for how to do so.  Zelig is easily expandable,
and various contributors are currently working to include their
methods in Zelig.  As Zelig grows, you will have access to an
increasing range of methods and models.



From christopherlmarshall at yahoo.com  Tue Mar 23 17:07:19 2004
From: christopherlmarshall at yahoo.com (Christopher Marshall)
Date: Tue, 23 Mar 2004 08:07:19 -0800 (PST)
Subject: [R] question on R's makefile and configure system
In-Reply-To: <20040323152540.GA498@sonny.eddelbuettel.com>
Message-ID: <20040323160719.42835.qmail@web41504.mail.yahoo.com>

I should have cc'd the list in my first response and forgot.

This exchange is about which path variables to set in the R make process so you can make a
compiled package for a linux distribution.

Dirk gave me the missing piece and I am able to compile proper slackware packages now.

--- Dirk Eddelbuettel <edd at debian.org> wrote:
> On Tue, Mar 23, 2004 at 07:14:27AM -0800, Christopher Marshall wrote:
> > So you mean I should do this:
> > 
> > ./configure --prefix=/usr
> > make
> > mkdir /tmp/r-package
> 
> Not needed, install will create the directory
> 
> > make install prefix=/tmp/r-package/usr
> 
> Yes.
> 
> > I have not used debian and don't know my way around its package management system.
> 
> This has nothing to do with Debian.  Configure, make, install, ... are all
> GNU tools that behave identically irrespective of the system.
> 
> I suggest you try to familiarize yourself with the building process by
> studying a simpler, smaller package for slackware, and then carry over what
> worked for it.  
> 

That's exactly what I did.  I took the gawk.SlackBuild script from
slackware-9.1/sources/a/gawk/gawk/SlackBuild.  I also looked at a lot of other slackware source
packages and the pattern in the build scripts was usually this

./configure --prefix=/usr
make
make install DESTDIR=/tmp/some-package

After you gave me the hint that the variable I needed at the "make install" step was prefix, I got
R to build this way

./configure --prefix=/usr
make
make install prefix=/tmp/some-package/usr

So my problem is solved, and I can now create proper slackware packages.

I realize that the autoconf system and the rest of the build tools are used by all distributions,
but there is a lot of variation in how you set up the variables used in your scripts and it seems
that R is an oddball in this respect.

Thanks for responding so quickly and solving my problem.

Chris Marshall



From tlumley at u.washington.edu  Tue Mar 23 17:09:28 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Mar 2004 08:09:28 -0800 (PST)
Subject: [R] how to modify variables of another frame (but not global)
In-Reply-To: <20040323141221.0C3E239BD@mprdmxin.myway.com>
References: <20040323141221.0C3E239BD@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0403230737350.153930@homer03.u.washington.edu>

On Tue, 23 Mar 2004, Gabor Grothendieck wrote:

> > The second, f2, uses <<- which searches through its parent environments for the
> assigned-to variable.  As can be seen in f2a near the end it appears to
> do the same thing as assign underneath as the entire z in f seems to
> have been copied over.  Note that the right hand side of the assignment
> is still handled normally.  QUESTION: Could someone explain what is
> going on in f2a in detail???

Someone has previously reported this as a bug; it's at least an infelicity.

The idea is that the RHS is evaluated in the local frame and the
assignment is then done in an enclosing frame. Now we get into how
assignment functions work.

a[1]<<-b[1]
like
a[1]<-b[1]
is really
a<-"[<-"(a,1,b)

So in your second example
z[1]<<-z[1]+1 is
z<<-"[<-"(z,1,z)
where the first z is the one in the enclosing environment and the second
is the local one.

Now what appears to happen is that R cleverly optimizes to avoid copying,
assigning the whole vector z rather than just the first element, since the
old z will be overwritten anyway. That is, the code to decide whether the
LHS and RHS are the same vector doesn't seem to be special-cased for <<-.
I'm not completely sure about this -- the code is complicated.

Simpler versions:
> z<-1:3
> local({w<-10:12; z[1]<<-w[1]+1})
> z
[1] 11  2  3
> z<-1:3
> local({z<-10:12; z[1]<<-z[1]+1})
> z
[1] 11 11 12

OTOH, one could reasonably argue that using z<<- when there is a local z
is asking for problems.


> The third, f3 uses eval.parent.  It looks similar to <<- but its actually
> different.  eval.parent evaluates the expression in the parent environment of f
> so it never looks within f.  Both sides of the assignment are handled in the
> parent environment in this one. This can be seen in f3a where the z defined in
> f is completely ignored.  I believe that eval.parent does modify individual
> cells without copying the entire structure.  QUESTION:  Is that right ???

It's potentially right.  That is, in situations where z[1]<-z[1]+1 doesn't
copy, f3 won't copy.

I also note that in general <<- and eval.parent() are not related.  It is
a bad idea to confuse the enclosing and parent environments, even though
they are often the same.

	-thomas



> > # 1. assign and get
> >
> > f1 <- function() {
> +    f <- function(x,k) {
> +       xc <- as.character(substitute(x))   # character representation of name
> +       x <- get( xc, parent.frame() )
> +       x[1] <- x[1] + k
> +       assign( xc, x, parent.frame() )
> +    }
> +    z <- 1:5
> +    f(z,1)
> +    print(z)
> + }
> > z <- 7
> > f1()
> [1] 2 2 3 4 5
> > z
> [1] 7
>
>
> > # 2. <<-
> >
> > f2 <- function() {
> +    f <- function(x,k) eval(eval(substitute( x[1] <<- x[1] + k )))
> +    z <- 1:5
> +    f(z,1)
> +    print(z)
> + }
> > z <- 7
> > f2()
> [1] 2 2 3 4 5
> > z
> [1] 7
> >
> > # 3. eval.parent
> >
> > f3 <- function() {
> +    f <- function(x,k) eval(eval.parent(substitute(x[1] <- x[1] + k )))
> +    z <- 1:5
> +    f(z,1)
> +    print(z)
> + }
> > z <- 7
> > f3()
> [1] 2 2 3 4 5
> > z
> [1] 7
>
>
> > # <<- with a local z
> >
> > f2a <- function() {
> +    f <- function(x,k) {
> +       z <- 10:12
> +       eval(eval(substitute( x[1] <<- x[1] + k )))
> +    }
> +    z <- 1:5
> +    f(z,1)
> +    print(z)
> + }
> > z <- 7
> > f2a()
> [1] 11 11 12
> > z
> [1] 7
>
>
> > # eval parent with a local z
> >
> > f3a <- function() {
> +    f <- function(x,k) {
> +       z <- 10:12
> +       eval(eval.parent(substitute(x[1] <- x[1] + k )))
> +    }
> +    z <- 1:5
> +    f(z,1)
> +    print(z)
> + }
> > z <- 7
> > f3a()
> [1] 2 2 3 4 5
> > z
> [1] 7
> >
>
> > R.version.string
> [1] "R version 1.8.1, 2003-11-21"
>
> ------------------------------------------------------
>
> Here is a copy of just the input
>
>
>
> # 1. assign and get
>
> f1 <- function() {
>    f <- function(x,k) {
>       xc <- as.character(substitute(x))   # character representation of name
>       x <- get( xc, parent.frame() )
>       x[1] <- x[1] + k
>       assign( xc, x, parent.frame() )
>    }
>    z <- 1:5
>    f(z,1)
>    print(z)
> }
> z <- 7
> f1()
> z
>
> # 2. <<-
>
> f2 <- function() {
>    f <- function(x,k) eval(eval(substitute( x[1] <<- x[1] + k )))
>    z <- 1:5
>    f(z,1)
>    print(z)
> }
> z <- 7
> f2()
> z
>
> # 3. eval.parent
>
> f3 <- function() {
>    f <- function(x,k) eval(eval.parent(substitute(x[1] <- x[1] + k )))
>    z <- 1:5
>    f(z,1)
>    print(z)
> }
> z <- 7
> f3()
> z
>
> # <<- with a local z
>
> f2a <- function() {
>    f <- function(x,k) {
>       z <- 10:12
>       eval(eval(substitute( x[1] <<- x[1] + k )))
>    }
>    z <- 1:5
>    f(z,1)
>    print(z)
> }
> z <- 7
> f2a()
> z
>
> # eval parent with a local z
>
> f3a <- function() {
>    f <- function(x,k) {
>       z <- 10:12
>       eval(eval.parent(substitute(x[1] <- x[1] + k )))
>    }
>    z <- 1:5
>    f(z,1)
>    print(z)
> }
> z <- 7
> f3a()
> z
>
> R.version.string
>
> ---
>
> Date:   Tue, 23 Mar 2004 12:17:39 +0100
> From:   Meinhard Ploner <meinhardploner at gmx.net>
> To:   <r-help at stat.math.ethz.ch>
> Subject:   [R] how to modify variables of another frame (but not global)
>
>
> Hello!
>
> Maybe "frame" is not the right term in this context.
> I explain my problem by example code:
>
> fun2 <- function(objName, add) {
>      ## the object "objName" should be increased by "add",
>      ## but the evaluation should be done in the calling function (here:
> fun1)
>      ## ...... what's the right code??
> }
>
> fun1 <- function() {
>      x <- 1
>
>      fun2("x", 10)          ## should modify "x"
>
>      ## now x should be 11, but only here and NOT globally!
>      ...
> }
>
>
> I would like to appreciate any solution!
> Thanks in advance
>
> Meinhard Ploner
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From henric.nilsson at statisticon.se  Tue Mar 23 17:29:12 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Tue, 23 Mar 2004 17:29:12 +0100
Subject: [R] influence.measures, cooks.distance, and glm
Message-ID: <6.0.3.0.0.20040323130326.04ed3348@10.0.10.66>

Dear list,

I've noticed that influence.measures and cooks.distance gives different 
results for non-gaussian GLMs. For example, using R-1.9.0 alpha 
(2003-03-17) under Windows:

 > ## Dobson (1990) Page 93: Randomized Controlled Trial :
 > counts <- c(18,17,15,20,10,20,25,13,12)
 > outcome <- gl(3,1,9)
 > treatment <- gl(3,3)
 > glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
 > influence.measures(glm.D93)$infmat[, 8]
           1           2           3           4           5           6
0.288294276 0.309131723 0.011614584 0.030963844 0.304525117 0.444410274
           7           8           9
0.459190432 0.002802907 0.377028535
 > cooks.distance(glm.D93)
          1          2          3          4          5          6          7
0.35162220 0.43125000 0.01468043 0.03906913 0.35640497 0.62024818 0.62510614
          8          9
0.00356405 0.44408301

After looking at the influence.measure code, it seems to me that this 
function always estimates the dispersion using Deviance/df. On the other 
hand, the cooks.distance function uses the Pearson residuals and extracts 
the dispersion from the fitted model using summary, which to me seems more 
sensible for a GLM.

Can someone please shed some light on this?

//Henric



From srivas at ksu.edu  Tue Mar 23 17:49:07 2004
From: srivas at ksu.edu (Sivakumar Mohandass)
Date: Tue, 23 Mar 2004 10:49:07 -0600
Subject: [R] Chambers Reference
Message-ID: <4060F232@webmail.ksu.edu>

Dear All,

I am looking for the page numbers of Chapter 4, Linear Models in Statistical 
Modeling in S (Chambers and Hastie) (1992) for citation. Could someone please 
help,

Thanks in advance,
Shiva



From Jens_Praestgaard at hgsi.com  Tue Mar 23 17:29:18 2004
From: Jens_Praestgaard at hgsi.com (Jens_Praestgaard@hgsi.com)
Date: Tue, 23 Mar 2004 11:29:18 -0500
Subject: [R] nlme question
Message-ID: <OF90E64C30.8F29F037-ON85256E60.00585573-85256E60.005A7DC4@hgsi.com>

I have a need to call and pass arguments to nlme()  from within another
function. I use R version 1.8.
 I have found an apparent way to make this work, but I would appreciate
some comments  on whether this fix is really appropriate, or there is
another way to do it that does not involve changing the  source code. I
don't have enough experience to start changing the sorurce code of a
library function.

Calling nlme from within a function as done below  will give an error:

testfunc<-
function(dat=v) {
test<-nlsList(result~a+(b-a)/(1+(conc/(c+z*cdiff))^d)
|rep,start=dat$init,data=dat$mixeddat)
return(nlme(test,random=b~1))
}

Here, v is an appropriate data frame in the main workspace.

Inserting two lines in nlme.nlsList will fix it:

Replacing line 21 from bottom "mData<-eval(mData) "  by " mData <-
eval(mData,parent.frame())"

Inserting     thisCall[["data"]]<-mData  on line 4 from bottom before val
<- do.call("nlme.formula", thisCall)

Doing this makes nlme.nlsList recognize the input data of the regression.
But is there a way to do this without changing the source code?

Thank you in advance



From MSchwartz at MedAnalytics.com  Tue Mar 23 18:03:50 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Mar 2004 11:03:50 -0600
Subject: [R] Chambers Reference
In-Reply-To: <4060F232@webmail.ksu.edu>
References: <4060F232@webmail.ksu.edu>
Message-ID: <1080061430.7675.728.camel@localhost.localdomain>

On Tue, 2004-03-23 at 10:49, Sivakumar Mohandass wrote:
> Dear All,
> 
> I am looking for the page numbers of Chapter 4, Linear Models in Statistical 
> Modeling in S (Chambers and Hastie) (1992) for citation. Could someone please 
> help,
> 
> Thanks in advance,
> Shiva

95 - 144

That's in the Chapman & Hall 1993 paperback edition, as opposed to the 
CRC Press hardcover edition.

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Tue Mar 23 18:13:04 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Mar 2004 09:13:04 -0800 (PST)
Subject: [R] how to modify variables of another frame (but not global)
In-Reply-To: <Pine.A41.4.58.0403230737350.153930@homer03.u.washington.edu>
References: <20040323141221.0C3E239BD@mprdmxin.myway.com>
	<Pine.A41.4.58.0403230737350.153930@homer03.u.washington.edu>
Message-ID: <Pine.A41.4.58.0403230910030.66364@homer22.u.washington.edu>

On Tue, 23 Mar 2004, Thomas Lumley wrote:
>
> I also note that in general <<- and eval.parent() are not related.  It is
> a bad idea to confuse the enclosing and parent environments, even though
> they are often the same.
>

I should probably also point out that one source of confusion is that we
have the terminology more-or-less backwards.  It would be more logical to
call the lexical environment "parent" (since it is the one with
inheritance), and the calling frame "enclosing"  (and the function
parent.env() is this way). However, the existence of sys.parent() in S for
the calling frame has forced the current terminology for parent.frame and
eval.parent.

	-thomas



From sluque at mun.ca  Tue Mar 23 18:36:40 2004
From: sluque at mun.ca (Sebastian Luque)
Date: Tue, 23 Mar 2004 11:36:40 -0600
Subject: [R] Coefficients and standard errors in lme
In-Reply-To: <200403221102.i2MB2bT8027973@hypatia.math.ethz.ch>
References: <200403221102.i2MB2bT8027973@hypatia.math.ethz.ch>
Message-ID: <406075A8.2010000@mun.ca>

Hello,

I have been searching for ways to obtain these for combinations of fixed 
factors and levels other than the 'baseline' group (contrasts coded all 
0's) from a mixed-effects model in lme. I've modelled the continuous 
variable y as a function of a continuous covariate x, and fixed factors 
A, B, and C. The fixed factors have two levels each and I'd like to know 
whether the relationship between y and x varies between levels of the 
factors, and whether there are any interactions between these factors. 
I've therefore setup the model as this:

lme.fit <- lme(y ~ x*A*B*C, data=df, random=~x | subjectID)

The contrasts are default ("contr.treatment" and "contr.poly"). As 
usual, the summary provides the coefficients for the 'baseline' group. 
The rest of coefficients correspond to *differences* and their standard 
error with respect to this group. One can calculate the coefficients for 
any combination of factor levels by adding the appropriate coefficients 
in the results. However, I don't understand how to obtain the standard 
errors for these from the summary report. Can someone please let me know 
how to obtain these?

Cheers,
Sebastian



From MSchwartz at MedAnalytics.com  Tue Mar 23 18:47:39 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Mar 2004 11:47:39 -0600
Subject: [R] par() usage
In-Reply-To: <20040323145313.GA5389@nf034.jinr.ru>
References: <20040323145313.GA5389@nf034.jinr.ru>
Message-ID: <1080064058.7675.762.camel@localhost.localdomain>

On Tue, 2004-03-23 at 08:53, Timur Elzhov wrote:
> Dear R experts.
> 
> I saw in a lot of examples the following R code:
> 
>     x11()
>     op <- par(no.readonly = TRUE)
>     par(op)
>         Warning message: 
>         calling par(new=) with no plot 
> 
> Why I get a warning? I'm doing something wrong?
> Thank you!


You have not created any type of plot yet, prior to calling par(op),
which restores the values of the non-readonly pars you saved in your
second line. You simply opened a plotting device with the call to x11()
in the first line.

The error message is indicating that you are trying to set/reset
par("new") without a plot having been created first. In par.c, which is
the relevant source code file for the par() function, there is a check
in the code to inhibit this sequence.

par("new") is used to define whether or not a _new_ plot will overwrite
an _existing_ plot. You have no existing plot.

You might want to look at ?par for examples of how to use the sequence
of saving and restoring the non-readonly parameters. It is typically
done when you will be modifying certain plot parameters and wish to
restore them to a default state later when you are done.

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Tue Mar 23 18:49:47 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Mar 2004 09:49:47 -0800 (PST)
Subject: [R] nlme question
In-Reply-To: <OF90E64C30.8F29F037-ON85256E60.00585573-85256E60.005A7DC4@hgsi.com>
References: <OF90E64C30.8F29F037-ON85256E60.00585573-85256E60.005A7DC4@hgsi.com>
Message-ID: <Pine.A41.4.58.0403230942330.66364@homer22.u.washington.edu>

On Tue, 23 Mar 2004 Jens_Praestgaard at hgsi.com wrote:

> I have a need to call and pass arguments to nlme()  from within another
> function. I use R version 1.8.
>  I have found an apparent way to make this work, but I would appreciate
> some comments  on whether this fix is really appropriate, or there is
> another way to do it that does not involve changing the  source code. I
> don't have enough experience to start changing the sorurce code of a
> library function.
>
> Calling nlme from within a function as done below  will give an error:
>
> testfunc<-
> function(dat=v) {
> test<-nlsList(result~a+(b-a)/(1+(conc/(c+z*cdiff))^d)
> |rep,start=dat$init,data=dat$mixeddat)
> return(nlme(test,random=b~1))
> }
>
> Here, v is an appropriate data frame in the main workspace.
>
>
> Inserting two lines in nlme.nlsList will fix it:
>
> Replacing line 21 from bottom "mData<-eval(mData) "  by " mData <-
> eval(mData,parent.frame())"
>
> Inserting     thisCall[["data"]]<-mData  on line 4 from bottom before val
> <- do.call("nlme.formula", thisCall)
>
> Doing this makes nlme.nlsList recognize the input data of the regression.
> But is there a way to do this without changing the source code?

I'm surprised this works.  I tried
data(Loblolly)
ff<-function(dat){
     fm1 <- nlsList(SSasymp, data = dat)
     fm2 <- nlme(fm1, random = Asym ~ 1)
     fm2
}
ff(Loblolly)

using the example from ?nlme.nlsList, and it failed in the nlsList()
evaluation, because nlsList was passing the name of its data argument and
losing the location.  It didn't get as far as nlme.nlsList.

In this example

ff1<-function(dat){
	fm1<-eval.parent(substitute(nlsList(SSasymp, data = dat)))
	fm2<-nlme(fm1,random=Asym~1)
	fm2
}
ff1(Loblolly)

works, but it isn't pretty (and will fail if `dat' isn't in the global
workspace).


I think it will take more surgery on nlsList and nlme to make this work
generally.


	-thomas



From andrewr at uidaho.edu  Tue Mar 23 18:55:38 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 23 Mar 2004 09:55:38 -0800
Subject: [R] Coefficients and standard errors in lme
In-Reply-To: <406075A8.2010000@mun.ca>
References: <200403221102.i2MB2bT8027973@hypatia.math.ethz.ch>
	<406075A8.2010000@mun.ca>
Message-ID: <200403230955.38573.andrewr@uidaho.edu>

Sebastian,

you might take a look at the function "estimable" in the package gregmisc.  
We've had a lot of luck with that.

Andrew

On Tuesday 23 March 2004 09:36, Sebastian Luque wrote:
> Hello,
>
> I have been searching for ways to obtain these for combinations of fixed
> factors and levels other than the 'baseline' group (contrasts coded all
> 0's) from a mixed-effects model in lme. I've modelled the continuous
> variable y as a function of a continuous covariate x, and fixed factors
> A, B, and C. The fixed factors have two levels each and I'd like to know
> whether the relationship between y and x varies between levels of the
> factors, and whether there are any interactions between these factors.
> I've therefore setup the model as this:
>
> lme.fit <- lme(y ~ x*A*B*C, data=df, random=~x | subjectID)
>
> The contrasts are default ("contr.treatment" and "contr.poly"). As
> usual, the summary provides the coefficients for the 'baseline' group.
> The rest of coefficients correspond to *differences* and their standard
> error with respect to this group. One can calculate the coefficients for
> any combination of factor levels by adding the appropriate coefficients
> in the results. However, I don't understand how to obtain the standard
> errors for these from the summary report. Can someone please let me know
> how to obtain these?
>
> Cheers,
> Sebastian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From rxg218 at psu.edu  Tue Mar 23 19:02:13 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 23 Mar 2004 13:02:13 -0500
Subject: [R] R equivilant to RAND_MAX in C
In-Reply-To: <Pine.LNX.4.44.0403230732310.2297-100000@gannet.stats>
References: <Pine.LNX.4.44.0403230732310.2297-100000@gannet.stats>
Message-ID: <1080064933.32067.2.camel@ra.chem.psu.edu>

On Tue, 2004-03-23 at 02:40, Prof Brian Ripley wrote:
> On Mon, 22 Mar 2004, Rajarshi Guha wrote:
> 
> > Hello,
> >   I have some C code that I'm interfacing to R using the .C calling
> > interface. Currently the C code uses the rand() function from the GNU C
> > library to generate random numbers. Since I need the random numbers in a
> > range from 0 to a (where a is an integer) I use the RAND_MAX macro as
> > 
> > (int)(rand() * (float)(*nobs-1) / (RAND_MAX+1.0))
> > 
> > (taken from the rand() manpage)
> 
> That isn't a random *number*: it is a random *integer*.  It is a random 
> integer on 0, ..., a=*nobs-2: is that what you wanted?

Yes. Thank you for the pointers

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Does Ramanujan know Polish?
-- E.B. Ross



From phddas at yahoo.com  Tue Mar 23 19:12:19 2004
From: phddas at yahoo.com (Fred J.)
Date: Tue, 23 Mar 2004 10:12:19 -0800 (PST)
Subject: [R] building data object on iteration
Message-ID: <20040323181219.89155.qmail@web20501.mail.yahoo.com>

Hello
I need help, few days tying to work this out but
unable to find examples. "alos would appricate
direction on how to find exmples"
getting the file name "without the extension" into a
character vector.
for (i in dir("c:/data/")){
    filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
i))
}
Error: Object "filename" not found
and I understand why the error but cann't find a way
to fix it.

thanks



From ligges at statistik.uni-dortmund.de  Tue Mar 23 19:23:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 23 Mar 2004 19:23:31 +0100
Subject: [R] building data object on iteration
In-Reply-To: <20040323181219.89155.qmail@web20501.mail.yahoo.com>
References: <20040323181219.89155.qmail@web20501.mail.yahoo.com>
Message-ID: <406080A3.9020700@statistik.uni-dortmund.de>

Fred J. wrote:
> Hello
> I need help, few days tying to work this out but
> unable to find examples. "alos would appricate
> direction on how to find exmples"
> getting the file name "without the extension" into a
> character vector.
> for (i in dir("c:/data/")){
>     filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
> i))
> }
> Error: Object "filename" not found
> and I understand why the error but cann't find a way
> to fix it.
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

You don't need a loop:

  filename <- sub("([^.]+)(\\..+)", "\\1", dir("c:/data/"))


Uwe Ligges


PS: Just to get your loop working (you don't want to do it that way!):

  filename <- NULL
  for (i in dir("c:/data/")){
      filename <- c(filename,sub("([^.]+)(\\..+)","\\1", i))



From itayf at fhcrc.org  Tue Mar 23 19:40:35 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Tue, 23 Mar 2004 10:40:35 -0800 (PST)
Subject: [R] building data object on iteration
In-Reply-To: <20040323181219.89155.qmail@web20501.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403231036400.25078-100000@cezanne.fhcrc.org>


On Tue, 23 Mar 2004, Fred J. wrote:

> Hello
> I need help, few days tying to work this out but
> unable to find examples. "alos would appricate
> direction on how to find exmples"
> getting the file name "without the extension" into a
> character vector.
> for (i in dir("c:/data/")){
>     filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
> i))
> }
> Error: Object "filename" not found
> and I understand why the error but cann't find a way
> to fix it.
> 

filename <- c()	#<--- Initialize. Needed for 1st iterataion.
for (i in dir("c:/data/")){
    filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
i))
}

But as noted from another responder -- you don't need a loop.

	Itay



From spencer.graves at pdf.com  Tue Mar 23 20:03:37 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 23 Mar 2004 11:03:37 -0800
Subject: [R] building data object on iteration
In-Reply-To: <406080A3.9020700@statistik.uni-dortmund.de>
References: <20040323181219.89155.qmail@web20501.mail.yahoo.com>
	<406080A3.9020700@statistik.uni-dortmund.de>
Message-ID: <40608A09.7050003@pdf.com>

      What should one use in place of "sub" in S-Plus, where "sub" is a 
subscripting function? 

      Spencer Graves    

Uwe Ligges wrote:

> Fred J. wrote:
>
>> Hello
>> I need help, few days tying to work this out but
>> unable to find examples. "alos would appricate
>> direction on how to find exmples"
>> getting the file name "without the extension" into a
>> character vector.
>> for (i in dir("c:/data/")){
>>     filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
>> i))
>> }
>> Error: Object "filename" not found
>> and I understand why the error but cann't find a way
>> to fix it.
>>
>> thanks
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> You don't need a loop:
>
>  filename <- sub("([^.]+)(\\..+)", "\\1", dir("c:/data/"))
>
>
> Uwe Ligges
>
>
> PS: Just to get your loop working (you don't want to do it that way!):
>
>  filename <- NULL
>  for (i in dir("c:/data/")){
>      filename <- c(filename,sub("([^.]+)(\\..+)","\\1", i))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Mar 23 20:15:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Mar 2004 14:15:27 -0500
Subject: [R] building data object on iteration
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A6A@usrymx25.merck.com>

That's technically an S-PLUS question, more appropriate for S-news than
R-help.

In any case, see page 31 of `S Programming'.

Andy

> From: Spencer Graves
> 
>       What should one use in place of "sub" in S-Plus, where 
> "sub" is a 
> subscripting function? 
> 
>       Spencer Graves    
> 
> Uwe Ligges wrote:
> 
> > Fred J. wrote:
> >
> >> Hello
> >> I need help, few days tying to work this out but
> >> unable to find examples. "alos would appricate
> >> direction on how to find exmples"
> >> getting the file name "without the extension" into a
> >> character vector.
> >> for (i in dir("c:/data/")){
> >>     filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
> >> i))
> >> }
> >> Error: Object "filename" not found
> >> and I understand why the error but cann't find a way
> >> to fix it.
> >>
> >> thanks
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >
> >
> > You don't need a loop:
> >
> >  filename <- sub("([^.]+)(\\..+)", "\\1", dir("c:/data/"))
> >
> >
> > Uwe Ligges
> >
> >
> > PS: Just to get your loop working (you don't want to do it 
> that way!):
> >
> >  filename <- NULL
> >  for (i in dir("c:/data/")){
> >      filename <- c(filename,sub("([^.]+)(\\..+)","\\1", i))
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Tue Mar 23 20:22:40 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Mar 2004 11:22:40 -0800 (PST)
Subject: [R] building data object on iteration
In-Reply-To: <40608A09.7050003@pdf.com>
References: <20040323181219.89155.qmail@web20501.mail.yahoo.com>
	<406080A3.9020700@statistik.uni-dortmund.de> <40608A09.7050003@pdf.com>
Message-ID: <Pine.A41.4.58.0403231113030.66364@homer22.u.washington.edu>

On Tue, 23 Mar 2004, Spencer Graves wrote:

>       What should one use in place of "sub" in S-Plus, where "sub" is a
> subscripting function?

There are probably better places to ask this question....

AFAICS there isn't an equivalent of sub.  In this case you could use
regexpr and substring

regsubstr<-function(re, v){
   matches<-regexpr(re,v)
   ifelse(matches==-1,"", substring(v,matches,matches+attr(matches,"match.length")-1)
}

regsubstr("^[^.]+",filenames)

	-thomas


>       Spencer Graves
>
> Uwe Ligges wrote:
>
> > Fred J. wrote:
> >
> >> Hello
> >> I need help, few days tying to work this out but
> >> unable to find examples. "alos would appricate
> >> direction on how to find exmples"
> >> getting the file name "without the extension" into a
> >> character vector.
> >> for (i in dir("c:/data/")){
> >>     filename <- c(filename,sub("([^.]+)(\\..+)","\\1",
> >> i))
> >> }
> >> Error: Object "filename" not found
> >> and I understand why the error but cann't find a way
> >> to fix it.
> >>
> >> thanks
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >
> >
> > You don't need a loop:
> >
> >  filename <- sub("([^.]+)(\\..+)", "\\1", dir("c:/data/"))
> >
> >
> > Uwe Ligges
> >
> >
> > PS: Just to get your loop working (you don't want to do it that way!):
> >
> >  filename <- NULL
> >  for (i in dir("c:/data/")){
> >      filename <- c(filename,sub("([^.]+)(\\..+)","\\1", i))
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From feh3k at spamcop.net  Tue Mar 23 20:27:32 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Tue, 23 Mar 2004 13:27:32 -0600
Subject: [R] Handling of NAs in functions lrm and robcov
In-Reply-To: <CCDEEDCC-7C4D-11D8-8F16-000A27D7D440@colorado.edu>
References: <CCDEEDCC-7C4D-11D8-8F16-000A27D7D440@colorado.edu>
Message-ID: <20040323132732.7d9befad.feh3k@spamcop.net>

On Mon, 22 Mar 2004 15:10:59 -0700
Christof Bigler <christof.bigler at colorado.edu> wrote:

> Hi R-helpers

Best term for that?

> 
> I have a dataframe DF (lets say with the variables, y, x1, x2, x3, ..., 
> clust) containing relatively many NAs.
> When I fit an ordinal regression model with the function lrm from the 
> Design library:
> model.lrm <- lrm(y ~ x1 + x2, data=DF, x=TRUE, y=TRUE)
> it will by default delete missing values in the variables y, x1, x2.
> Based on model.lrm, I want to apply the robust covariance estimator 
> using a cluster variable:
> model.robcov.lrm <- robcov(model.lrm, cluster=clust)
> How can I remove observations in the cluster variable clust that 
> contain NAs in y, x1, and x2?

robcov is supposed to do that.  If you have an example otherwise, kindly
reproduce it with the most trivial dataset you can simulate, and send the
code and I'll debug.  -Frank

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From feh3k at spamcop.net  Tue Mar 23 20:27:49 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Tue, 23 Mar 2004 13:27:49 -0600
Subject: [R] R commands formating for LaTeX
In-Reply-To: <405F13E9.8060700@jrc.it>
References: <405F13E9.8060700@jrc.it>
Message-ID: <20040323132749.028702ca.feh3k@spamcop.net>

On Mon, 22 Mar 2004 17:27:21 +0100
Rado Bonk <rado.bonk at jrc.it> wrote:

> Dear R users,
> 
> I have to include typical UNIX formated R commands and outputs into the 
> article using LaTeX.
> 
> What is the easiest way to include R prompt commands and text outputs 
> (including messages) into LaTeX documents? Is there any template or
> library?
> 
> So far I'm trying to produce R commands in LaTeX using LaTeX general 
> format options.
> 
> Thanks,
> 
> Rado

See http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/EmacsLaTeXTools
-- the part about the s2latx.pl Perl script for use inside LaTeX.

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From mail at joeconway.com  Tue Mar 23 20:28:27 2004
From: mail at joeconway.com (Joe Conway)
Date: Tue, 23 Mar 2004 11:28:27 -0800
Subject: [R] PL/R article
Message-ID: <40608FDB.80605@joeconway.com>

In case anyone is interested, here is a link to an online article 
written by someone in the Postgres community, covering PL/R:

see (cover):
http://www.varlena.com/varlena/GeneralBits/66.php

and follow to (article):
http://www.varlena.com/varlena/GeneralBits/Tidbits/bernier/art_66/graphingWithR.html

PL/R is a procedural language handler for PostgreSQL, that allows 
Postgres functions to be written in, and executed by, R.

Best regards,

Joe



From ripley at stats.ox.ac.uk  Tue Mar 23 20:27:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Mar 2004 19:27:43 +0000 (GMT)
Subject: [R] optimal hardware for computations in R?
In-Reply-To: <406044C0.2030602@math.mun.ca>
Message-ID: <Pine.LNX.4.44.0403231922420.9708-100000@gannet.stats>

On Tue, 23 Mar 2004, Paul Y. Peng wrote:

> I recently ordered a computer which is intended to run both WindowsXP
> and Linux (of course both versions of R as well). Before placing the
> order, I discussed it with our system managers. They highly recommanded
> a system with one P4 CPU with Intel's so called "hyper-threading"
> technology over a system with two CPU's, and they claimed that both OS's
> can take benefits from the "hyper-threading" technology. I haven't got
> the machine yet and don't know how fast it is. At least this is another
> option available.

I do have such a machine as my home machine, so I already have experience
under Fedora Core 1 and Windows XP. The gain over a single processor is
small compared to a dual processor (at best 1.2x in my experience), and
many magazine tests have recommended turning hyperthreading off.  I know
that a dual Athlon 2600 (my office machine) compiles R about twice as fast
as a Pentium 2.6HT, for example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Tue Mar 23 20:41:25 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 23 Mar 2004 14:41:25 -0500
Subject: [R] influence.measures, cooks.distance, and glm
In-Reply-To: <6.0.3.0.0.20040323130326.04ed3348@10.0.10.66>
Message-ID: <web-24955530@cgpsrv2.cis.mcmaster.ca>

Dear Henric,

Since I'm replying from memory, I'm not sure that I have this quite
right, but if I recall correctly, cooks.distance.glm() originated in
the cookd.glm() function in the car package, and (except for scaling
the result as F) follows the Williams reference given in the help file.
The influence.measures function has a different origin, and I suspect
that it was originally programmed for consistency with S-PLUS.

I hope that this helps.
 John

On Tue, 23 Mar 2004 17:29:12 +0100
 Henric Nilsson <henric.nilsson at statisticon.se> wrote:
> Dear list,
> 
> I've noticed that influence.measures and cooks.distance gives
> different results for non-gaussian GLMs. For example, using R-1.9.0
> alpha (2003-03-17) under Windows:
> 
> > ## Dobson (1990) Page 93: Randomized Controlled Trial :
>  >counts <- c(18,17,15,20,10,20,25,13,12)
>  >outcome <- gl(3,1,9)
>  >treatment <- gl(3,3)
>  >glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
>  >influence.measures(glm.D93)$infmat[, 8]
>           1           2           3           4           5
>           6
> 0.288294276 0.309131723 0.011614584 0.030963844 0.304525117
> 0.444410274
>           7           8           9
> 0.459190432 0.002802907 0.377028535
> > cooks.distance(glm.D93)
>          1          2          3          4          5          6
>          7
> 0.35162220 0.43125000 0.01468043 0.03906913 0.35640497 0.62024818
> 0.62510614
>          8          9
> 0.00356405 0.44408301
> 
> After looking at the influence.measure code, it seems to me that this
> function always estimates the dispersion using Deviance/df. On the
> other hand, the cooks.distance function uses the Pearson residuals
> and extracts the dispersion from the fitted model using summary,
> which to me seems more sensible for a GLM.
> 
> Can someone please shed some light on this?
> 
> //Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From bates at stat.wisc.edu  Tue Mar 23 21:03:26 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 23 Mar 2004 14:03:26 -0600
Subject: [R] PL/R article
In-Reply-To: <40608FDB.80605@joeconway.com>
References: <40608FDB.80605@joeconway.com>
Message-ID: <6rr7vjs54h.fsf@bates4.stat.wisc.edu>

Joe Conway <mail at joeconway.com> writes:

> In case anyone is interested, here is a link to an online article
> written by someone in the Postgres community, covering PL/R:
> 
> 
> see (cover):
> http://www.varlena.com/varlena/GeneralBits/66.php
> 
> and follow to (article):
> http://www.varlena.com/varlena/GeneralBits/Tidbits/bernier/art_66/graphingWithR.html
> 
> PL/R is a procedural language handler for PostgreSQL, that allows
> Postgres functions to be written in, and executed by, R.

Interesting article.  Thanks for bringing it to our attention, Joe.

There are a couple of quotes that I like, such as "It's simply amazing
the things that you can learn when data is presented in a graphical
format." 

It appears that the author is using <<- when he only needs <- in one
function definition.  There isn't anything peculiar about PL/R that
would require <<-, is there?

Also, there are a couple of "Don't try this at home, kids" typos like
using

 apt-get install R

which I think should be

 apt-get install r-base

or even

 apt-get install r-recommended

(The Debian packaging system doesn't allow upper case in names, which
is why I called the original Debian package "r-base".)



From robert.bernier5 at sympatico.ca  Tue Mar 23 21:16:07 2004
From: robert.bernier5 at sympatico.ca (Robert Bernier)
Date: Tue, 23 Mar 2004 15:16:07 -0500
Subject: [R] PL/R article
In-Reply-To: <6rr7vjs54h.fsf@bates4.stat.wisc.edu>
References: <40608FDB.80605@joeconway.com>
	<6rr7vjs54h.fsf@bates4.stat.wisc.edu>
Message-ID: <40609B07.9060901@sympatico.ca>

Thanks for pointing out the mistake, I had forgotten that's what I had 
done to get R installed

Robert Bernier


Douglas Bates wrote:

>Also, there are a couple of "Don't try this at home, kids" typos like
>using
>
> apt-get install R
>
>which I think should be
>
> apt-get install r-base
>
>or even
>
> apt-get install r-recommended
>
>(The Debian packaging system doesn't allow upper case in names, which
>is why I called the original Debian package "r-base".)
>  
>



From gregory_r_warnes at groton.pfizer.com  Tue Mar 23 21:13:53 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Tue, 23 Mar 2004 15:13:53 -0500
Subject: [R] writing text on graphics' window
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B13D@groexmb02.pfizer.com>


Another approach is to use the textplot() function and friends from the
gregmisc library.  From ?textplot:

textplot              package:gregmisc              R Documentation

Display text information in a graphics plot.

Description:

     This function displays text output in a graphics window.  It is
     the equivalent of 'print' except that the output is displayed as a
     plot.

-Greg


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Paul Murrell
> Sent: Monday, March 22, 2004 7:46 PM
> To: sam.kemp2 at ntlworld.com
> Cc: r-help at stat.math.ethz.ch; dmurdoch at pair.com; Ray Brownrigg
> Subject: Re: [R] writing text on graphics' window
> 
> 
> Hi
> 
> 
> Ray Brownrigg wrote:
> >>From: Duncan Murdoch <dmurdoch at pair.com>
> >>Date: Sun, 21 Mar 2004 06:58:03 -0500
> >>
> >>On Sun, 21 Mar 2004 11:07:01 +0000, you 
> [sam.kemp2 at ntlworld.com] wrote:
> >>
> >>
> >>>Does anyone know of a method for writing text to the 
> graphics window, 
> >>>where there is *no* plot? Basically, I have developed a 
> 'significance 
> >>>test' and I would like the output on the graphics window to say 
> >>>something about the input parameters and the stats of the 
> significance test.
> >>
> >>You need to make sure a graphics device is active and establish a
> >>coordinate system there.  The easiest way to do that is to 
> make a call
> >>to plot() with everything turned off:
> >>
> >> plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")
> >>
> >>You may also want to reduce the "margins" if you want your output to
> >>take up the full frame, e.g.
> >>
> >> oldmargins <- par(mar=c(0,0,0,0))
> >> plot(0:100,0:100,type='n',axes=FALSE,xlab="",ylab="")
> >>
> > 
> > An easier way to activate a graphics device and establish a 
> coordinate
> > system is to call plot.new().
> > 
> > 
> > Try:
> > 
> >>plot.new()
> >>text(0, 0, "ABC")
> >>par("usr")
> > 
> > [1] -0.04  1.04 -0.04  1.04
> 
> 
> The grid package gives you the whole page to play with by default and 
> gives you more flexibility in how you place the text.  Try ...
> 
> library(grid)
> grid.text("Here's some\ntext", x=unit(1, "cm"),
>            y=unit(1, "npc") - unit(1, "cm"),
>            just=c("left", "top"))
> 
> Paul
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From martin.renner at stonebow.otago.ac.nz  Tue Mar 23 21:41:28 2004
From: martin.renner at stonebow.otago.ac.nz (Martin Renner)
Date: Tue, 23 Mar 2004 11:41:28 -0900
Subject: [R] optimal hardware for computations in R?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A62@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7A62@usrymx25.merck.com>
Message-ID: <p06020401bc864bc0bc16@[192.168.2.111]>

You might want to consider a G5. It holds 8 GB of RAM, dual 64 bit 
processor, unix-based OS, runs MS Office, and probably gives you the 
most flops for your bug (that's why Virginia Tech went for G5s when 
building their supercomputer). What else would you want?

Martin



From rbaer at atsu.edu  Wed Mar 24 01:07:45 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Tue, 23 Mar 2004 18:07:45 -0600
Subject: [R] dots in function names
References: <000f01c4048e$2498fe50$6401a8c0@meadow>
	<405A4F39.9040109@stat.auckland.ac.nz>
	<001f01c40dc3$41e113b0$2e80010a@BigBaer>
	<405F8A22.7050008@stat.auckland.ac.nz>
Message-ID: <01ef01c41134$0891c900$2e80010a@BigBaer>

In a (off-list) response to a question of frequency polygons, Dr Paul
Murrell writes:
> There's an "ann" arg which you can use to turn off the default labels
> (ann=FALSE).  Have you seen help(plot.default) as well as help(plot)?

Thank you again, Paul.  Interestingly, the ?plot help contains no mention of
the 'ann' parameter.  In fact, I have been guessing about what parameters
are legal in plot() or using examples from various R tutorials because ?plot
help just enumerates them with as ... after x and y.  I did not know that
plot.default existed although I now find it in response to a
help.search("plot").  This search produces many plot.something() functions.
I knew nothing about any of these "other" plot functions.

There has been mention of S3 objects on the list.  Is this relevant here?
Is 'plot.default' a method (or perhaps a type-dependent object
instantiation?) of the "plot function" object, or is 'plot.default' just a
second function object that shares a part of a name and uses a "convenience"
dot for readability?  Does R have standard naming conventions in this
regard?  I'm guessing that 'plot.default' is a method? of 'plot'.  Assuming
that other 'plot.something' functions are methods or instantiations of a
main 'plot' function object, how do I learn more about such a "main"
object's subparts short of studying code?  The ?plot help document does not
have a "see also" for plot.default or (many) other 'plot.something'
functions.  Is there a simple way to troll for such things when trying to
expand your horizons?

Any general hints on  how one should approach the self-education process in
this type of situation?

Thanks,
Rob Baer



From tlumley at u.washington.edu  Wed Mar 24 01:29:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Mar 2004 16:29:07 -0800 (PST)
Subject: [R] dots in function names
In-Reply-To: <01ef01c41134$0891c900$2e80010a@BigBaer>
References: <000f01c4048e$2498fe50$6401a8c0@meadow>
	<405A4F39.9040109@stat.auckland.ac.nz>
	<001f01c40dc3$41e113b0$2e80010a@BigBaer>
	<405F8A22.7050008@stat.auckland.ac.nz>
	<01ef01c41134$0891c900$2e80010a@BigBaer>
Message-ID: <Pine.A41.4.58.0403231614320.66364@homer22.u.washington.edu>

On Tue, 23 Mar 2004, Robert W. Baer, Ph.D. wrote:

>
> There has been mention of S3 objects on the list.  Is this relevant here?
> Is 'plot.default' a method (or perhaps a type-dependent object
> instantiation?) of the "plot function" object, or is 'plot.default' just a
> second function object that shares a part of a name and uses a "convenience"
> dot for readability?  Does R have standard naming conventions in this
> regard?  I'm guessing that 'plot.default' is a method? of 'plot'.

Yes. It is an S3 method and there is a naming convention. plot.something()
is the plot method for objects of class "something".  Typing
  methods("plot")
will give a list of available methods.  plot.default() is the method used
when there is no specific method available for a given object.

Unfortunately the dot is also used for readability, so we have
list.files(), which is not a method for list(), and t.test(), which is not
a method for t().


>  Assuming
> that other 'plot.something' functions are methods or instantiations of a
> main 'plot' function object,

They aren't part of any `plot' object. They are separate functions.

>				 how do I learn more about such a "main"
> object's subparts short of studying code?  The ?plot help document does not
> have a "see also" for plot.default or (many) other 'plot.something'
> functions.

This would be difficult to make comprehensive, since users can freely add
new plot() methods.  Dynamic help of this sort is on the wishlist, but it
may be a while off.

>		  Is there a simple way to troll for such things when trying to
> expand your horizons?

methods("plot") is probably the best way.  You can also look for methods
for a class: eg methods(class="lm") to see all the things you can do to lm
objects.


> Any general hints on  how one should approach the self-education process in
> this type of situation?

There are some excellent books on R and S out there.

	-thomas



From ggrothendieck at myway.com  Wed Mar 24 01:35:13 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 23 Mar 2004 19:35:13 -0500 (EST)
Subject: [R] dots in function names
Message-ID: <20040324003513.9AACE39ED@mprdmxin.myway.com>



Typing 

> plot

shows the source which shows that plot uses UseMethod -- thus
its an S3 generic.  Knowing that, issuing:

> methods(plot)

gives a list of methods.

Date:   Tue, 23 Mar 2004 18:07:45 -0600 
From:   Robert W. Baer, Ph.D. <rbaer at atsu.edu>
To:   <r-help at stat.math.ethz.ch> 
Cc:   Paul Murrell <p.murrell at auckland.ac.nz> 
Subject:   [R] dots in function names 

 
In a (off-list) response to a question of frequency polygons, Dr Paul
Murrell writes:
> There's an "ann" arg which you can use to turn off the default labels
> (ann=FALSE). Have you seen help(plot.default) as well as help(plot)?

Thank you again, Paul. Interestingly, the ?plot help contains no mention of
the 'ann' parameter. In fact, I have been guessing about what parameters
are legal in plot() or using examples from various R tutorials because ?plot
help just enumerates them with as ... after x and y. I did not know that
plot.default existed although I now find it in response to a
help.search("plot"). This search produces many plot.something() functions.
I knew nothing about any of these "other" plot functions.

There has been mention of S3 objects on the list. Is this relevant here?
Is 'plot.default' a method (or perhaps a type-dependent object
instantiation?) of the "plot function" object, or is 'plot.default' just a
second function object that shares a part of a name and uses a "convenience"
dot for readability? Does R have standard naming conventions in this
regard? I'm guessing that 'plot.default' is a method? of 'plot'. Assuming
that other 'plot.something' functions are methods or instantiations of a
main 'plot' function object, how do I learn more about such a "main"
object's subparts short of studying code? The ?plot help document does not
have a "see also" for plot.default or (many) other 'plot.something'
functions. Is there a simple way to troll for such things when trying to
expand your horizons?

Any general hints on how one should approach the self-education process in
this type of situation?

Thanks,
Rob Baer



From mervyn at iastate.edu  Wed Mar 24 02:12:16 2004
From: mervyn at iastate.edu (Mervyn G Marasinghe)
Date: Tue, 23 Mar 2004 19:12:16 -0600
Subject: [R] Question on deriv3()
Message-ID: <6.0.1.1.2.20040323190732.01b4c578@mervyn.mail.iastate.edu>

Hello:

Why is deriv3() functioning differently in R from that in Splus
using library(MASS) ? For example
deriv3(~(t1*log(t2)+lgamma(t1)+(1-t1)*log(y)+y/t2),c("t1","t2"),function(y,t1,t2)NULL)
complains of  lgamma.

Mervyn



From ross at biostat.ucsf.edu  Wed Mar 24 00:33:53 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 23 Mar 2004 15:33:53 -0800
Subject: [R] Status of Rmpi
Message-ID: <1080084832.7020.49.camel@iron.libaux.ucsf.edu>

Rmpi is not currently available on CRAN, and I don't think it has been
for a few months.  It is available at
http://www.stats.uwo.ca/faculty/yu/Rmpi/

Does anyone know its current status?

A few months ago I corresponded with the author, who noted some build
problems (specifically on Debian) were the hang up, and seemed to be
working on it.  I wasn't able to get it to work then (on a Debian
GNU/Linux testing system).  After having tried unsuccessfully to get it
working last week on an oddball system, I tried again on my system.

I was able to build the thing, but not actually get it working.  The two
obvious difficulties were that, first, I had to manually load the
serialize library (which now conflicts with the name of something in the
base package) and, second, that whenever I say makeCluster or
makeMPIcluster the thing just hangs up.  I've tried various permutations
of running or not running lamboot first.

I am able to to lamboot and lamexec, and I also tried setting LAMRSH to
use ssh on the master.  My "network" is just my dual-CPU machine.

Tony Rossini's notes
(http://www.analytics.washington.edu/~rossini/courses/cph-statcomp/cph-4.pdf) refer to SNOW and rpvm as being "currently maintained" (last page), which hints that Rmpi might not be.

I'm kind of interested in getting Rmpi to work (I and others here have
been using Rmpi), though I suppose we could switch to rpvm, so I'll
probably keep fiddling with it.  I'm using LAM 6.5.8-2.

Footnote: Does anyone know if the serialize that comes with R 1.8.1 is
compatible with the serialize package?  Should the latter be
unnecessary?
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From ross at biostat.ucsf.edu  Wed Mar 24 00:10:42 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 23 Mar 2004 15:10:42 -0800
Subject: [R] Rmpi and PBS
In-Reply-To: <Pine.GSO.4.55.0312301129110.20488@student>
References: <Pine.GSO.4.55.0312301129110.20488@student>
Message-ID: <1080083442.7028.26.camel@iron.libaux.ucsf.edu>

On Tue, 2003-12-30 at 08:39, Shengqiao Li wrote:
> Hello:
> 
> Anybody knows how to run Rmpi through PBS (Portable Batch System) on a
> cluster computer. I'm using a supercomputer which require to submit jobs
> to PBS queue for dispatching. I tried use mpirun in my PBS script. But all
> my Rslaves are spawned to the same node. This is not desired.
> 
> Any suggestions are welcome!
> 
> Thanks in advance.
This is a late reply, but perhaps still of interest to some.  It looks
as if this problem has been solved, though only integrated into LAM with
release 7: http://www.lam-mpi.org/papers/hpcs2003/tm-implementation.pdf



From tlumley at u.washington.edu  Tue Mar 23 23:22:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Mar 2004 14:22:29 -0800 (PST)
Subject: [R] how to modify variables of another frame (but not global)
In-Reply-To: <Pine.A41.4.58.0403230737350.153930@homer03.u.washington.edu>
References: <20040323141221.0C3E239BD@mprdmxin.myway.com>
	<Pine.A41.4.58.0403230737350.153930@homer03.u.washington.edu>
Message-ID: <Pine.A41.4.58.0403231412580.66364@homer22.u.washington.edu>

On Tue, 23 Mar 2004, Thomas Lumley wrote:

> On Tue, 23 Mar 2004, Gabor Grothendieck wrote:
>
> > > The second, f2, uses <<- which searches through its parent environments for the
> > assigned-to variable.  As can be seen in f2a near the end it appears to
> > do the same thing as assign underneath as the entire z in f seems to
> > have been copied over.  Note that the right hand side of the assignment
> > is still handled normally.  QUESTION: Could someone explain what is
> > going on in f2a in detail???
>
<snip>
> Now what appears to happen is that R cleverly optimizes to avoid copying,
> assigning the whole vector z rather than just the first element, since the
> old z will be overwritten anyway. That is, the code to decide whether the
> LHS and RHS are the same vector doesn't seem to be special-cased for <<-.
> I'm not completely sure about this -- the code is complicated.

In fact it's simpler and more wrong than that.  The first z to be found is
the local one, so it gets its first element incremented. So the new value
is  c(11,11,12) and this just gets stuck into the z in the enclosing
environment.

The problem is that the code that decides where to put the result
(eval.c:do_set) starts looking in the enclosing environment, but the code
that computes the result (eval.c:evalseq) starts looking in the local
environment.

That is, the code is equivalent to

*tmp*<-z
*tmp*[1]<-z[1]+1
z<<-*tmp*

where the first two z's are the local one and the last one is in the
enclosing environment.


	-thomas



From luke at stat.uiowa.edu  Wed Mar 24 02:35:11 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Tue, 23 Mar 2004 19:35:11 -0600 (CST)
Subject: [R] Status of Rmpi
In-Reply-To: <1080084832.7020.49.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>

The serialize package should no longer be needed since the
functionality is now in R itself.  I haven't run snow with a new
version of Rmpi newer than 0.4-4; with that version things worked on
my systems the last time I tried.  We need to revize the paper you
cite; when we get to that we'll hopefully have a chance to give a
newer version of Rmpi a go.

Best,

luke

On Tue, 23 Mar 2004, Ross Boylan wrote:

> Rmpi is not currently available on CRAN, and I don't think it has been
> for a few months.  It is available at
> http://www.stats.uwo.ca/faculty/yu/Rmpi/
> 
> Does anyone know its current status?
> 
> A few months ago I corresponded with the author, who noted some build
> problems (specifically on Debian) were the hang up, and seemed to be
> working on it.  I wasn't able to get it to work then (on a Debian
> GNU/Linux testing system).  After having tried unsuccessfully to get it
> working last week on an oddball system, I tried again on my system.
> 
> I was able to build the thing, but not actually get it working.  The two
> obvious difficulties were that, first, I had to manually load the
> serialize library (which now conflicts with the name of something in the
> base package) and, second, that whenever I say makeCluster or
> makeMPIcluster the thing just hangs up.  I've tried various permutations
> of running or not running lamboot first.
> 
> I am able to to lamboot and lamexec, and I also tried setting LAMRSH to
> use ssh on the master.  My "network" is just my dual-CPU machine.
> 
> Tony Rossini's notes
> (http://www.analytics.washington.edu/~rossini/courses/cph-statcomp/cph-4.pdf) refer to SNOW and rpvm as being "currently maintained" (last page), which hints that Rmpi might not be.
> 
> I'm kind of interested in getting Rmpi to work (I and others here have
> been using Rmpi), though I suppose we could switch to rpvm, so I'll
> probably keep fiddling with it.  I'm using LAM 6.5.8-2.
> 
> Footnote: Does anyone know if the serialize that comes with R 1.8.1 is
> compatible with the serialize package?  Should the latter be
> unnecessary?
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From alex_s_42 at yahoo.com  Wed Mar 24 00:27:14 2004
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Tue, 23 Mar 2004 15:27:14 -0800 (PST)
Subject: [R] statistical significance test for cluster agreement
Message-ID: <20040323232714.94780.qmail@web60004.mail.yahoo.com>

I was wondering, whether there is a way to have
statistical significance test for cluster agreement.

I know that I can use classAgreement() function to get
Rand index, which will give me some indication whether
the clusters agree or not, but it would be interesting
to have a formal test.

Thanks.



From ross at biostat.ucsf.edu  Wed Mar 24 03:26:52 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 23 Mar 2004 18:26:52 -0800
Subject: [R] Status of Rmpi--Good with tweaks
In-Reply-To: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>
References: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>
Message-ID: <1080095211.7022.77.camel@iron.libaux.ucsf.edu>

On Tue, 2004-03-23 at 17:35, Luke Tierney wrote:
> The serialize package should no longer be needed since the
> functionality is now in R itself.  I haven't run snow with a new
> version of Rmpi newer than 0.4-4; with that version things worked on
> my systems the last time I tried.  We need to revize the paper you
> cite; when we get to that we'll hopefully have a chance to give a
> newer version of Rmpi a go.
> 
> Best,
> 
> luke

I got 0.4-6 to work, with one tweak to eliminate the loading of
serialize and one fix of my own setup.

I changed the .First.Lib code in Rmpi to be
    if (!exists("serialize") && !require(serialize))
	stop("serialize package cannot be loaded. Exit")	

!exists("serialize") && is new.  This is effort to check if the
serialize function exists (R 1.8) and skip loading the library in that
case.  Since there are various ways this might be fooled, perhaps an
explicit check of the R version would be better.

I'm also not sure if the R packaging mechanism makes this easy, or
autogenerates it.  This is a bit of a tricky situation, since serialize
is required for some versions of R but should not be used for the most
recent.

I suspect that without this the cluster would not come up because Rmpi
would not load properly.  I have not tested that.

The other problem was of my own making.  I had Rmpi in a local
directory, loaded with lib.loc.  The children don't know about this, and
automatically try to load Rmpi.  I solved this by setting .Renviron with
the proper R_LIBS path.  (By the way, I wanted to say
R_LIBS=~/lib:${R_LIBS}, but that syntax doesn't work.  This is
consistent with the documentation, just awkward.)

I also ran into a couple of snow things, really documentation issues,
that I'll put in a separate message.



From dmurdoch at pair.com  Wed Mar 24 03:30:47 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 23 Mar 2004 21:30:47 -0500
Subject: [R] statistical significance test for cluster agreement
In-Reply-To: <20040323232714.94780.qmail@web60004.mail.yahoo.com>
References: <20040323232714.94780.qmail@web60004.mail.yahoo.com>
Message-ID: <9is16018cv93oga1b8f9qq4k579t55uscv@4ax.com>

On Tue, 23 Mar 2004 15:27:14 -0800 (PST), you wrote:

>I was wondering, whether there is a way to have
>statistical significance test for cluster agreement.
>
>I know that I can use classAgreement() function to get
>Rand index, which will give me some indication whether
>the clusters agree or not, but it would be interesting
>to have a formal test.

Why not simulate data from your hypothesized null distribution,
cluster it, and see how your dataset's index value compares to the
simulated ones?

Duncan Murdoch



From andy_liaw at merck.com  Wed Mar 24 03:34:17 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Mar 2004 21:34:17 -0500
Subject: [R] statistical significance test for cluster agreement
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A78@usrymx25.merck.com>

But what would such a test do that the rand index does not?  Would you
interpret the p-value from such a test, if exists, to have the meaning that
a real test of hypothesis has?  AFAIK you basically need to have the
hypotheses pinned down even before you see any data, for the inference to be
valid.  Is that possible with clustering?

Just my $0.02...
Andy

> From: Alexander Sirotkin [at Yahoo]
> 
> I was wondering, whether there is a way to have
> statistical significance test for cluster agreement.
> 
> I know that I can use classAgreement() function to get
> Rand index, which will give me some indication whether
> the clusters agree or not, but it would be interesting
> to have a formal test.
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ross at biostat.ucsf.edu  Wed Mar 24 03:53:45 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 23 Mar 2004 18:53:45 -0800
Subject: [R] snow documentation comments
Message-ID: <1080096825.7027.107.camel@iron.libaux.ucsf.edu>

There are a few points I found unclear or unmentioned in the snow
documentation (mostly I looked at the cluster.html web page).  I thought
I'd mention them here.

What is the start up environment for the children?
--------------------------------------------------
My best guess at the answer is in parentheses
Do they inherit shell variables? (no)
Do they inherit variables set in R or other aspects of the R
environment? (no)
What directory does it start in? (the directory you are running in)
What user are you? (same as original)

I realize some of these answers might depend on the parallelization
layer you are using, or whether you select the homogenous option.

There are also some entertaining pathological cases; for example,
lam/mpi lets you start some of the children as another user.

Number of nodes or number of children?
--------------------------------------
I thought all the counts of nodes (e.g., in makeCluster(5)) were of the
total nodes in the cluster, i.e., children + 1.  However, I did a
makeCluster(2) and I got 2 additional R processes running.  Have I
misunderstood the semantics, or is it essentially an implementation
detail that the master node starts a new R process?

Options
-------
Is the full set of cluster options documented anywhere?

-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From ok at cs.otago.ac.nz  Wed Mar 24 03:56:46 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 24 Mar 2004 14:56:46 +1200 (NZST)
Subject: [R] Operating on windows of data
Message-ID: <200403240256.i2O2ukuk329993@atlas.otago.ac.nz>

It appears that I owe Martin Maechler <maechler at stat.math.ethz.ch>
an apology for not realising the importance of the context for what
I quoted.  I apologise.

	but *please* note again the code snippet we where talking about :
	
	    > dat <- sapply( seq(T-width), function(i) {
	    >     model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, A, 
	    >                 i:(i+width-1))
	    >     details <- summary.lm(model)
	    >     tmp <- coefficients(model)
	    >     c( USD = tmp[2], JPY = tmp[3], DEM = tmp[4], 
	    >            R2 = details$r.squared, RMSE = details$sigma )
	    > } )
	    > dat <- as.data.frame(t(dat))
	    > attach(dat)
	
	which is really an example where sapply() rather obfuscates than
	clarifies.

It's not clear to me that the choice of sapply() -vs- 'for' really has
anything to do with it here.  Hmm, maybe it does.  Looking at this code,
I can see at a glance that
    - dat will be a matrix
    - it will have columns 1:T-width
    - it will have rows USD, JPY, DEM, R2, RMSE
    - each column reflects one linear model
and I don't have to decode a lot of indexed assignment statements to
figure this out.

The first way to improve clarity would be to use keyword parameters on
the call to lm, e.g., lm(..., data = A, subset = i:(i+width-1)).

The second way to improve clarity would be to use character indices on
tmp rather than integer indices:
	
	coef <- coefficients(model)
	c(USD = coef["dlusdchf"],
	  JPY = coef["djpychf"],
	  DEM = coef["dldemchf"],
	  R2  = details$r.squared,
	  RMSE= details$sigma)

Hmm.  My "first" and "second" ways are both the same: use names rather
than position.  There is one more clarity improvement to recommend, and
it has nothing to do with using or avoiding sapply(), at least not
directly.

    # dfapply(X, FUN, ...) is like sapply() but
    # expects FUN to return c(x1=...,xn=...) vectors which it
    # turns into rows of the data frame that it returns.

    dfapply <- function (...) as.data.frame(t(sapply(...)))
 
    # Make "dat" a data frame with columns USD, JPY, DEM, R2, RMSE
    # and rows 1:T-width, the ith row extracted from a linear
    # regression on cases i:(i+width-1).

    dat <- dfapply(seq(T-width), function (i) {
        model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf,
		    data = A, subset = i:(i+width-1))
	s <- summary.lm(model)
	v <- coefficients(model)
	c(USD = v["dlusdchf"], JPY = v["djpychf"], DEM = v["dldemchf"],
	  R2 = s$r.squared, RMSE = s$sigma)
    })

Now here's where using sapply() instead of 'for' does pay off, even here.
We ask the question "where is 'i' used?"  Because we're *not* using i in
any visible index calculations, there is only one place that 'i' is used,
and that's in the subset= argument of the lm() call.

That prompts the question "is there any way to exploit the fact that the
rest of the linear model is the same?  Depending on the relative sizes
of A and T-width, there may well be, and Statistical Models in S explains,
if memory serves me, how to do this kind of thing.  But without the fact
that i is only used in one place, it might not be as obvious that it was
worth thinking about.



From mail at joeconway.com  Wed Mar 24 04:02:56 2004
From: mail at joeconway.com (Joe Conway)
Date: Tue, 23 Mar 2004 19:02:56 -0800
Subject: [R] PL/R article
In-Reply-To: <6rr7vjs54h.fsf@bates4.stat.wisc.edu>
References: <40608FDB.80605@joeconway.com>
	<6rr7vjs54h.fsf@bates4.stat.wisc.edu>
Message-ID: <4060FA60.8010304@joeconway.com>

Douglas Bates wrote:
> Interesting article.  Thanks for bringing it to our attention, Joe.
> 
> There are a couple of quotes that I like, such as "It's simply amazing
> the things that you can learn when data is presented in a graphical
> format." 
> 
> It appears that the author is using <<- when he only needs <- in one
> function definition.  There isn't anything peculiar about PL/R that
> would require <<-, is there?

No, nothing at all.

The only time you might need <<- (and I'm admittedly no expert on R, so 
this may be inappropriate use), is when you want to create a variable in 
one PL/R function, and then access it from another PL/R function. For 
instance, when you want to prepare a query, and then execute it multiple 
times. By preparing the query, you save the time of parsing and planning 
each time you execute it. There is an example here (see pg.spi.execp):
http://www.joeconway.com/plr/doc/plr-spi-rsupport-funcs.html

In Robert's example he's calling pg.spi.exec directly, so there is no 
need for <<-.

Thanks for the comments!

Joe



From ross at biostat.ucsf.edu  Wed Mar 24 04:10:14 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 23 Mar 2004 19:10:14 -0800
Subject: [R] Status of Rmpi--Good with tweaks
In-Reply-To: <1080095211.7022.77.camel@iron.libaux.ucsf.edu>
References: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>
	<1080095211.7022.77.camel@iron.libaux.ucsf.edu>
Message-ID: <1080097814.7028.116.camel@iron.libaux.ucsf.edu>

On Tue, 2004-03-23 at 18:26, Ross Boylan wrote:
> On Tue, 2004-03-23 at 17:35, Luke Tierney wrote:
> > The serialize package should no longer be needed since the
> > functionality is now in R itself.  I haven't run snow with a new
> > version of Rmpi newer than 0.4-4; with that version things worked on
> > my systems the last time I tried.  We need to revize the paper you
> > cite; when we get to that we'll hopefully have a chance to give a
> > newer version of Rmpi a go.
> > 
> > Best,
> > 
> > luke
> 
> I got 0.4-6 to work, with one tweak to eliminate the loading of
> serialize and one fix of my own setup.
> 
> I changed the .First.Lib code in Rmpi to be
>     if (!exists("serialize") && !require(serialize))
> 	stop("serialize package cannot be loaded. Exit")	
> 
> !exists("serialize") && is new.  This is effort to check if the
                                          ^an   
> serialize function exists (R 1.8) and skip loading the library in that
> case.  Since there are various ways this might be fooled, perhaps an
> explicit check of the R version would be better.
> 
> I'm also not sure if the R packaging mechanism makes this easy, or
"this" refers to the modification to the .First.Lib that I made.
> autogenerates it.  This is a bit of a tricky situation, since serialize
> is required for some versions of R but should not be used for the most
> recent.
> 
> I suspect that without this the cluster would not come up because Rmpi
> would not load properly.  I have not tested that.



From ggrothendieck at myway.com  Wed Mar 24 06:09:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Mar 2004 00:09:19 -0500 (EST)
Subject: [R] Operating on windows of data
Message-ID: <20040324050919.D53FE399A@mprdmxin.myway.com>



Far from being an example where vectorization
seems to have only minor advantages, if any, over a 
non-vectorized approach, it turns out that this is a shining 
example of why vectorization produces higher quality code.

Consider that the loop approach and all the other
solutions so far have an error!  The last time point is
never included in any window.

The genesis of the error is that index arithmetic for
producing the loop limits and sapply index is sufficiently
complex that no one noticed the error.  Vectorizing
*sufficiently* eliminates the calculation of these limits
entirely and so avoids the possibility of mistake.  The
problem so far is that we just did not take the vectorized
approach far enough (mea culpa).

The following solution uses embed (which can be regarded as
a vectorized sliding window) to get us out of considering such
explicit limits. By eliminating the calculation of these limits
it avoids any potential for this sort of error in the first place.
(It also avoids the transpose by producing data frames at
each step and rbinding them.)


do.call( "rbind", apply( embed(T, width), 1, 
   function(idx) {
      model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf,
		   data = A, subset = idx)
      s <- summary.lm(model)
      v <- coefficients(model)
      data.frame(USD = v["dlusdchf"], JPY = v["djpychf"], 
              DEM = v["dldemchf"], R2 = s$r.squared, RMSE = s$sigma)
   } ) 
)

Note that the above example loops over  vectors of indices
instead of single indexes.  There is an opportunity to do away 
with indices altogether although it is at the considerable
expense of space efficiency so, in practice, one would likely 
be content with the last solution.  Nevertheless, it is of 
interest to display the next solution even if only for sake of completing the thought.

Define embed.data.frame to produce a list of data frames that
represents the sliding windows.  Perform a lapply over those:

embed.data.frame <- function(df,width) 
             apply(embed(1:nrow(df),width),1,function(idx)df[idx,])

do.call( "rbind", lapply(embed.data.frame(df,width), 
  function(df) {
    model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, data = df)
    s <- summary.lm(model)
    v <- coefficients(model)
    data.frame(USD = v["dlusdchf"], JPY = v["djpychf"], 
          DEM = v["dldemchf"], R2 = s$r.squared, RMSE = s$sigma)
  } )
)

This gets rid of all reference to idx and nrow(df).


---

Date:   Wed, 24 Mar 2004 14:56:46 +1200 (NZST) 
From:   Richard A. O'Keefe <ok at cs.otago.ac.nz>
To:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] Operating on windows of data 

[snip]

Hmm. My "first" and "second" ways are both the same: use names rather
than position. There is one more clarity improvement to recommend, and
it has nothing to do with using or avoiding sapply(), at least not
directly.

# dfapply(X, FUN, ...) is like sapply() but
# expects FUN to return c(x1=...,xn=...) vectors which it
# turns into rows of the data frame that it returns.

dfapply <- function (...) as.data.frame(t(sapply(...)))

# Make "dat" a data frame with columns USD, JPY, DEM, R2, RMSE
# and rows 1:T-width, the ith row extracted from a linear
# regression on cases i:(i+width-1).

dat <- dfapply(seq(T-width), function (i) {
model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf,
           data = A, subset = i:(i+width-1))
     s <- summary.lm(model)
     v <- coefficients(model)
     c(USD = v["dlusdchf"], JPY = v["djpychf"], DEM = v["dldemchf"],
      R2 = s$r.squared, RMSE = s$sigma)
})

Now here's where using sapply() instead of 'for' does pay off, even here.
We ask the question "where is 'i' used?" Because we're *not* using i in
any visible index calculations, there is only one place that 'i' is used,
and that's in the subset= argument of the lm() call.

That prompts the question "is there any way to exploit the fact that the
rest of the linear model is the same? Depending on the relative sizes
of A and T-width, there may well be, and Statistical Models in S explains,
if memory serves me, how to do this kind of thing. But without the fact
that i is only used in one place, it might not be as obvious that it was
worth thinking about.



From ggrothendieck at myway.com  Wed Mar 24 06:50:05 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Mar 2004 00:50:05 -0500 (EST)
Subject: [R] Operating on windows of data
Message-ID: <20040324055005.B178E397F@mprdmxin.myway.com>


[I am having some problems with returned email.  Sorry if
you get this twice.]

Well so much for pontificating. My "shining example" has an 
error too although this error is less serious since it will
give an error message. embed(T,width) should be embed(1:T,width).

But perhaps there is more to this than we realize since the
subsequent (space inefficient) solution which I thought might 
be taking it too far does not require calculation of T except
in embed.data.frame which is a reusable function that would
likely be of higher quality through reuse and in any case the
fact is that it did avoid this error.


Date:   Wed, 24 Mar 2004 00:09:19 -0500 (EST) 
From:   Gabor Grothendieck <ggrothendieck at myway.com>
To:   <ok at cs.otago.ac.nz>, <r-help at stat.math.ethz.ch> 
Cc:   <ajayshah at mayin.org> 
Subject:   Re: [R] Operating on windows of data 

 


Far from being an example where vectorization
seems to have only minor advantages, if any, over a 
non-vectorized approach, it turns out that this is a shining 
example of why vectorization produces higher quality code.

Consider that the loop approach and all the other
solutions so far have an error! The last time point is
never included in any window.

The genesis of the error is that index arithmetic for
producing the loop limits and sapply index is sufficiently
complex that no one noticed the error. Vectorizing
*sufficiently* eliminates the calculation of these limits
entirely and so avoids the possibility of mistake. The
problem so far is that we just did not take the vectorized
approach far enough (mea culpa).

The following solution uses embed (which can be regarded as
a vectorized sliding window) to get us out of considering such
explicit limits. By eliminating the calculation of these limits
it avoids any potential for this sort of error in the first place.
(It also avoids the transpose by producing data frames at
each step and rbinding them.)


do.call( "rbind", apply( embed(T, width), 1, 
function(idx) {
model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf,
           data = A, subset = idx)
s <- summary.lm(model)
v <- coefficients(model)
data.frame(USD = v["dlusdchf"], JPY = v["djpychf"], 
DEM = v["dldemchf"], R2 = s$r.squared, RMSE = s$sigma)
} ) 
)

Note that the above example loops over vectors of indices
instead of single indexes. There is an opportunity to do away 
with indices altogether although it is at the considerable
expense of space efficiency so, in practice, one would likely 
be content with the last solution. Nevertheless, it is of 
interest to display the next solution even if only for sake of completing the thought.

Define embed.data.frame to produce a list of data frames that
represents the sliding windows. Perform a lapply over those:

embed.data.frame <- function(df,width) 
apply(embed(1:nrow(df),width),1,function(idx)df[idx,])

do.call( "rbind", lapply(embed.data.frame(df,width), 
function(df) {
model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf, data = df)
s <- summary.lm(model)
v <- coefficients(model)
data.frame(USD = v["dlusdchf"], JPY = v["djpychf"], 
DEM = v["dldemchf"], R2 = s$r.squared, RMSE = s$sigma)
} )
)

This gets rid of all reference to idx and nrow(df).


---

Date: Wed, 24 Mar 2004 14:56:46 +1200 (NZST) 
From: Richard A. O'Keefe <ok at cs.otago.ac.nz>
To: <r-help at stat.math.ethz.ch> 
Subject: Re: [R] Operating on windows of data 

[snip]

Hmm. My "first" and "second" ways are both the same: use names rather
than position. There is one more clarity improvement to recommend, and
it has nothing to do with using or avoiding sapply(), at least not
directly.

# dfapply(X, FUN, ...) is like sapply() but
# expects FUN to return c(x1=...,xn=...) vectors which it
# turns into rows of the data frame that it returns.

dfapply <- function (...) as.data.frame(t(sapply(...)))

# Make "dat" a data frame with columns USD, JPY, DEM, R2, RMSE
# and rows 1:T-width, the ith row extracted from a linear
# regression on cases i:(i+width-1).

dat <- dfapply(seq(T-width), function (i) {
model <- lm(dlinrchf ~ dlusdchf + dljpychf + dldemchf,
data = A, subset = i:(i+width-1))
s <- summary.lm(model)
v <- coefficients(model)
c(USD = v["dlusdchf"], JPY = v["djpychf"], DEM = v["dldemchf"],
R2 = s$r.squared, RMSE = s$sigma)
})

Now here's where using sapply() instead of 'for' does pay off, even here.
We ask the question "where is 'i' used?" Because we're *not* using i in
any visible index calculations, there is only one place that 'i' is used,
and that's in the subset= argument of the lm() call.

That prompts the question "is there any way to exploit the fact that the
rest of the linear model is the same? Depending on the relative sizes
of A and T-width, there may well be, and Statistical Models in S explains,
if memory serves me, how to do this kind of thing. But without the fact
that i is only used in one place, it might not be as obvious that it was
worth thinking about.



From ripley at stats.ox.ac.uk  Wed Mar 24 08:43:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Mar 2004 07:43:14 +0000 (GMT)
Subject: [R] Question on deriv3()
In-Reply-To: <6.0.1.1.2.20040323190732.01b4c578@mervyn.mail.iastate.edu>
Message-ID: <Pine.LNX.4.44.0403240741500.10665-100000@gannet.stats>

Because it is not the same code.  In R, the set of known functions is 
hard-coded.  In S-PLUS, it is extensible, and was extended by MASS (now 
incorporated into the standard distribution).

On Tue, 23 Mar 2004, Mervyn G Marasinghe wrote:

> Why is deriv3() functioning differently in R from that in Splus
> using library(MASS) ? For example
> deriv3(~(t1*log(t2)+lgamma(t1)+(1-t1)*log(y)+y/t2),c("t1","t2"),function(y,t1,t2)NULL)
> complains of  lgamma.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alex_s_42 at yahoo.com  Wed Mar 24 10:12:40 2004
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Wed, 24 Mar 2004 01:12:40 -0800 (PST)
Subject: [R] statistical significance test for cluster agreement
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A78@usrymx25.merck.com>
Message-ID: <20040324091240.24557.qmail@web60003.mail.yahoo.com>

Like you said, such kind of test will not give me
anything that Rand index does not, except for p-value.

The null hypothesis, in my case, is that clustering
results does not match a different clustering, that
someone alse did on the same data.

And I do believe that this hypothesis is valid.
Basicly, it's not that different from chi-squared
goodness of fit test which check whether or not my 
data comes from particular distribution. With an 
exception that I don't know how to do chi-squared test
in this case :)



--- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> But what would such a test do that the rand index
> does not?  Would you
> interpret the p-value from such a test, if exists,
> to have the meaning that
> a real test of hypothesis has?  AFAIK you basically
> need to have the
> hypotheses pinned down even before you see any data,
> for the inference to be
> valid.  Is that possible with clustering?
> 
> Just my $0.02...
> Andy
> 
> > From: Alexander Sirotkin [at Yahoo]
> > 
> > I was wondering, whether there is a way to have
> > statistical significance test for cluster
> agreement.
> > 
> > I know that I can use classAgreement() function to
> get
> > Rand index, which will give me some indication
> whether
> > the clusters agree or not, but it would be
> interesting
> > to have a formal test.
> > 
> > Thanks.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains
> information of Merck & Co., Inc. (One Merck Drive,
> Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme
> or MSD and in Japan as
> Banyu) that may be confidential, proprietary
> copyrighted and/or legally
> privileged. It is intended solely for the use of the
> individual or entity
> named on this message.  If you are not the intended
> recipient, and have
> received this message in error, please notify us
> immediately by reply e-mail
> and then delete it from your system.
>



From pxt at ph.adfa.edu.au  Wed Mar 24 10:34:33 2004
From: pxt at ph.adfa.edu.au (Pisut Tempatarachoke)
Date: Wed, 24 Mar 2004 20:34:33 +1100
Subject: [R] Scaling of font sizes in layout()
Message-ID: <40615629.6070602@ph.adfa.edu.au>

Hi all,

In the following example,

#--------------EXAMPLE------------------
test <- function(subfigure)
{
plot(c(1:10),c(1:10),cex=4)
text(1,9,subfigure,cex=10)
}
m <- matrix(c(1,2,5,5,3,4,5,5),4,2)
layout(m)
test("a")
test("b")
test("c")
test("d")
test("e")
#---------------------------------------

Is it possible to have the font (a,b,...,e) and pch sizes (including the 
axis-label, tick and tick-label sizes) scaled proportionally with the 
size of each plot when I put multiple plots on the same page?

Thanks in advance!!

Regards
Tempo



From JonesW at kssg.com  Wed Mar 24 10:49:30 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Wed, 24 Mar 2004 09:49:30 -0000
Subject: [R] Job Vacancy
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F102C@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040324/40418a68/attachment.pl

From s.chamaille at wanadoo.fr  Wed Mar 24 11:17:09 2004
From: s.chamaille at wanadoo.fr (=?iso-8859-1?Q?Simon_Chamaill=E9?=)
Date: Wed, 24 Mar 2004 11:17:09 +0100
Subject: [R] GLMM
Message-ID: <IEEFKHGFNDGNNFCJALMFCEOCCDAA.s.chamaille@wanadoo.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040324/d3fe335f/attachment.pl

From fzoellne at TechFak.Uni-Bielefeld.DE  Wed Mar 24 11:24:50 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Wed, 24 Mar 2004 11:24:50 +0100
Subject: [R] Problems with postscript output
Message-ID: <20040324102450.GA24046@hindemith.TechFak.Uni-Bielefeld.DE>

Hi all!

I have a little problem with saving plots to file.
I use the command postscript() followed by the plotting command and a dev.off().

When I then look at the resulting image saved to disk, some of the axis labels are missing (see attached image). Is there a way to fix this. 

Yours,
-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de
-------------- next part --------------
A non-text attachment was scrubbed...
Name: distclassreschi2.eps
Type: application/postscript
Size: 4966 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040324/83df039e/distclassreschi2.eps

From joehl at gmx.de  Wed Mar 24 11:41:57 2004
From: joehl at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Wed, 24 Mar 2004 11:41:57 +0100 (MET)
Subject: [R] significance testing under sampling from small finite
	populations
Message-ID: <32647.1080124917@www22.gmx.net>


Dear All,

Are there recommended functions to test for (exact) significance when the
samples are drawn from small finite populations (which sometimes are not much
bigger than the sample)?

I am looking for 

- differences in central tendency
- differences of proportions
- difference in distributions

My current application would be on unpaired comparision of discrete
(6-valued) questionaire data, but I would also appreciate pointers to functions for
continuous data or paired comparisions.


Thanks for any help
Best regards


Jens Oehlschl?gel

--



From f.calboli at ucl.ac.uk  Wed Mar 24 12:46:29 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 24 Mar 2004 11:46:29 +0000
Subject: [R] colour scheme in: plot(survfit.model)
Message-ID: <1080128789.2940.21.camel@monkey>

Dear All,

I would like to ask a question on the colour scheme I can specify for a 

plot(survfit.model).

I have four lines of Drosophila, kept at 3 different temperatures. About
100 individuals per line and temperature were scored, no censoring.

the data looks like:

line	temp	day	status
line1	18	23	1
.....



My model (quite trivial) is like:

surv.model<-survfit(Surv(day, status)~line+temp, mydata)

My problem comes when I:

plot(surv.model,col=1:4)

The plot shows three main groupings, because temperature affects
longevity in drosophila. The problem is that the colour used for "line1"
at temperature "18C" is not the same for "line1" at temperature "28C",
which I find extremely confusing. 

How can I specify to keep the same colour for the same line across the
plot?

regards,

Federico Calboli
-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk



From Joris.DeWolf at cropdesign.com  Wed Mar 24 11:50:22 2004
From: Joris.DeWolf at cropdesign.com (Joris DeWolf)
Date: Wed, 24 Mar 2004 11:50:22 +0100
Subject: [R] Problems with postscript output
In-Reply-To: <20040324102450.GA24046@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040324102450.GA24046@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <406167EE.8040503@cropdesign.com>


This is not a PS problem. The barplot you made has its horizontl axis 
suppressed by default. See argument axis.lty in ?barplot
Joris

Frank Gerrit Zoellner wrote:

>Hi all!
>
>I have a little problem with saving plots to file.
>I use the command postscript() followed by the plotting command and a dev.off().
>
>When I then look at the resulting image saved to disk, some of the axis labels are missing (see attached image). Is there a way to fix this. 
>
>Yours,
>  
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 

====================================================================== 
Joris De Wolf
CropDesign N.V. 
Plant Evaluation Group
Technologiepark 3 
B-9052 Zwijnaarde 
Belgium 
Tel. : +32 9 242 91 55
Fax  : +32 9 241 91 73
====================================================================== 


confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From bjorn.hauksson at sedlabanki.is  Wed Mar 24 12:08:44 2004
From: bjorn.hauksson at sedlabanki.is (=?iso-8859-1?Q?S=CD_Bj=F6rn_Hauksson?=)
Date: Wed, 24 Mar 2004 11:08:44 -0000
Subject: [R] High/low level: Plot 2 time series with different axis (left
	and ri ght) 
Message-ID: <7DCEB30609E0044EB8659C9760219654012AD1B1@iris.centbank.is>

Sun, 14 Mar 2004, Jan Verbesselt wrote:
> Dear R specialists,
> 
> I have two time series in a data.frame and want to plot them in the same
> plot(), with the left axis scaled to time series 1 (-700,0) and the
> right axis scaled to time series 2 (-0.2, 0.4). 
> 
> plot(timeserie1)
> lines(timeserie2, col=c(2)) => this one should be scaled differently
> with a new axis on the right handside.
> 
> How can these be visualised such that the fit is optimal for
> visualisation of the two time series? Which commands can I use?

I have composed a simple R function to do this. See usage example in the function description.

##
## Description: A simple function which plots two time series on one plot where 
##    the series can have different value intervals over the same time interval.
## Usage: ts.plot.2Axis(xleft, xright)
## Arguments: xleft is the time series for the left vertical axis and xright 
##     is for the right axis. xleft and xright are defined as time series with 
##     the 'ts' function in package ts.
##     ts.plot function must be available, do library(ts) to ensure this if 
##     necessary.
##     In addition the usual 'ts.plot' and 'plot' parameters can be set
##     directly (mar, main, xlab, ylab, lwd) or through gpars as in ts.plot. 
##     Also parameter digits is the preferred number of decimal digits on right 
##     axis and ticks is the preferred number of tick marks on right axis.
## Details: The time series for the right vertical axis is scaled with a simple 
##     rule of thumb scaling.
##     The ts.plot function is used to plot the series.
## Value: None. 
## Note: When scaling is not acceptable try switching the series parameters.
##     If a ylabel is to be set it is here only possible for the left axis.
## See also: 'ts.plot', 'ts', 'legend'.
## Author and date: Hauksson, Bjorn Arnar. March 2004.
## Example:
## First paste this function into the R console or use 'source'.
#library(ts)
#data(UKLungDeaths)
#x <- ldeaths
#y <- fdeaths/mdeaths
#ts.plot.2Axis(x, y)
#legTxt <- c("UK lung deaths", "UK female/male deaths (rhs)") 
#legend(1976.5, 3950, legTxt, lty=c(1:2), col=c(1:2), lwd=2, bty="n")
##

ts.plot.2Axis <- function(xleft, xright, digits=1, ticks=5, 
                          mar=(c(4,4,4,4)+0.1), main="", 
                          xlab="", ylab="", lwd=2, gpars=list()) {
	# Settings for other parameters than those in the function parameter list
	par(mar=mar)                # Margins
      k <- ncol(as.matrix(xleft)) # Number of time series on left vertical scale
	lty <- c(1:(k+1))           # Line types
	col <- c(1:(k+1))           # Line colors

	# Scale time series on right vertical axis
	scale <- (max(xleft)-min(xleft))/(max(xright)-min(xright))
      xright2 <- xright*scale
      meanScale <- mean(xleft) - mean(xright2) 
	xright2 <- xright2 + meanScale

	# Plot the series 
	ts.plot(xleft, xright2, lty=lty, col=col, main=main, ylab=ylab, xlab=xlab, 
        lwd=lwd, gpars=gpars)

	# Add the right vertical axis labels
	lab <- seq(round(min(xright), digits), round(max(xright), digits), 
        length=ticks)
	labAt <- seq(min(xright2), max(xright2), length=ticks)	
	axis(side=4, labels=lab, at=labAt)
}

Comments and suggestions for this function would be helpful. A text file with the function is available at my website, http://www.bjornarnar.net/hugbunadur/R/ts.plot.2Axis.R

Best regards,
Bjorn Arnar Hauksson
bjorn.hauksson at sedlabanki.is
http://www.bjornarnar.net/english.php



From p.pagel at gsf.de  Wed Mar 24 12:23:06 2004
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 24 Mar 2004 12:23:06 +0100
Subject: [R] Problems with postscript output
In-Reply-To: <20040324102450.GA24046@hindemith.TechFak.Uni-Bielefeld.DE>
References: <20040324102450.GA24046@hindemith.TechFak.Uni-Bielefeld.DE>
Message-ID: <20040324112306.GA7522@localhost>

	Hi!

> I have a little problem with saving plots to file.  I use the command
> postscript() followed by the plotting command and a dev.off().
> 
> When I then look at the resulting image saved to disk, some of the
> axis labels are missing (see attached image). Is there a way to fix
> this. 

R is trying to be smart about labels getting too close/overlap. Specify the
graph to be a little wider and the labels will appear.

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/~pagel



From Joerg.Schaber at uv.es  Wed Mar 24 12:27:10 2004
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Wed, 24 Mar 2004 12:27:10 +0100
Subject: [R] combined random effects
Message-ID: <4061708E.1040309@uv.es>

Hi,

I have the following linear mixed model:

y(g,i,j,k,l)=u + L(g) + T(i) + D(j) + S(k) + (TS)(i,k) + error(g,i,j,k,l)

where S(k) and the combined effect (TS)(i,k) are random effects whereas 
the rest are fixed effects.

How do I specifiy the random part of the model formula in lme(), 
especially concerning the combined effect (TS)?

Moreover, when I run the model as a fixed effect model I get the 
following error:

 > lm(logMed ~ lenAA + TREAT + DYE + SLIDE + SLIDE:DYE,data=MA)
Error in lm.fit(x, y, offset = offset, ...) :
        NA/NaN/Inf in foreign function call (arg 4)

However, when I run the model with a reduced data set it works.

In the first case I have 16512 data points whereas in the reduced case I 
use only have of it. Do I reach some internal threshold here?

Thanks for help,

joerg



From andy_liaw at merck.com  Wed Mar 24 12:29:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 06:29:06 -0500
Subject: [R] statistical significance test for cluster agreement
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A79@usrymx25.merck.com>

[Apology to the list for the off-topic rant...]

As it turned out, I also have a problem with LOF/GOL/etc. tests:  I'd bet
most of the time when such a test is carried out, it is _not_ the only test
being done, but the p-values in the downstream analysis are almost never
adjusted for this.  How valid would the p-values be?

IMHO, it's bad enough that users of statistical methods do things like this,
but it's quite something else that statisticians do just the same, or even
promote such tests.  It's not a crime to do analysis like that, but to treat
the p-values as if they actually are meaningful probably ought to be
outlawed.

OK, I better run for cover now...

Andy

> From: Alexander Sirotkin [at Yahoo] [mailto:alex_s_42 at yahoo.com] 
> 
> Like you said, such kind of test will not give me
> anything that Rand index does not, except for p-value.
> 
> The null hypothesis, in my case, is that clustering
> results does not match a different clustering, that
> someone alse did on the same data.
> 
> And I do believe that this hypothesis is valid.
> Basicly, it's not that different from chi-squared
> goodness of fit test which check whether or not my 
> data comes from particular distribution. With an 
> exception that I don't know how to do chi-squared test
> in this case :)
> 
> 
> 
> --- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> > But what would such a test do that the rand index
> > does not?  Would you
> > interpret the p-value from such a test, if exists,
> > to have the meaning that
> > a real test of hypothesis has?  AFAIK you basically
> > need to have the
> > hypotheses pinned down even before you see any data,
> > for the inference to be
> > valid.  Is that possible with clustering?
> > 
> > Just my $0.02...
> > Andy
> > 
> > > From: Alexander Sirotkin [at Yahoo]
> > > 
> > > I was wondering, whether there is a way to have
> > > statistical significance test for cluster
> > agreement.
> > > 
> > > I know that I can use classAgreement() function to
> > get
> > > Rand index, which will give me some indication
> > whether
> > > the clusters agree or not, but it would be
> > interesting
> > > to have a formal test.
> > > 
> > > Thanks.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > >
> >
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > 
> > 
> >
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any
> > attachments, contains
> > information of Merck & Co., Inc. (One Merck Drive,
> > Whitehouse Station, New
> > Jersey, USA 08889), and/or its affiliates (which may
> > be known outside the
> > United States as Merck Frosst, Merck Sharp & Dohme
> > or MSD and in Japan as
> > Banyu) that may be confidential, proprietary
> > copyrighted and/or legally
> > privileged. It is intended solely for the use of the
> > individual or entity
> > named on this message.  If you are not the intended
> > recipient, and have
> > received this message in error, please notify us
> > immediately by reply e-mail
> > and then delete it from your system.
> >
> --------------------------------------------------------------
> ----------------
> 
> 
> __________________________________
>



From license at gettyimages.com  Wed Mar 24 12:51:45 2004
From: license at gettyimages.com (Getty Images USA License)
Date: Wed, 24 Mar 2004 22:51:45 +1100
Subject: [R] <AUTO> 'Getty=001-531-403'Notice again
Message-ID: <2004324_@TLZ11460357_@TLZ>

Hello and thank you for your email. We strive to respond to all email inquiries within one business day. If your issue cannot wait that long, please call us at 877-438-8966. Our hours are 6:00 a.m. to 6:00 p.m. Pacific Standard Time, Monday through Friday. 

For reference, your assigned case ID is:1531403



-----Original Message-----
From: r-help at hypatia.math.ethz.ch [r-help at hypatia.math.ethz.ch]
Sent: Wednesday, Mar 24 2004 3:35AM
To: Getty Images USA License [license at photodisc.com]
Subject: Notice again



Do not visit this illegal websites! 

+++ Attachment: No Virus found 
+++ Bitdefender AntiVirus - www.bitdefender.com 



=======================================================
This email and its contents are confidential. If you
are not the intended recipient, please do not disclose
or use the information within this email or its
attachments. If you have received this email in error,
please delete it immediately. Thank you.



From mmarques at inescporto.pt  Wed Mar 24 12:51:34 2004
From: mmarques at inescporto.pt (MMarques Power)
Date: Wed, 24 Mar 2004 11:51:34 +0000
Subject: [R] string problems ( grep and regepxr)
Message-ID: <134506337156.20040324115134@power.inescn.pt>


Recently working with strings and data
I have found a small problem.

Windows XP
R 1.8.1

Reading data from a "txt file" with readLine.
finding a specific line with "grep" command, all OK.
but here comes the problem...
After finding the correct line(s) i need to find a substring
inside each string.
In this case "tabs" I think it represented by "\t" in the grep command
trying to use grep in each string it only returns 1 ...
Afterwards I tried regexpr command it returns the correct position of the
substring that I am looking for but it only reports the first one.
does regexpr only returns the first one ?


Partial example:

d5 = "load0004   node0014        0.05    0.014583333"
     "load0005   node0017        0.05    0.014583333"
     "load0006   node0019        0.05    0.014583333"

     
>grep("\t",d5[1])
[1] 1
>regexpr("\t",d5[1]
[1] 9
attr(,"match.length")
[1] 1

any idea how to make regexpr return the several substrings ?
or the grep and
Am I missing anything obvious ?

THanks in advance
Marco Marques



From Joerg.Schaber at uv.es  Wed Mar 24 13:01:44 2004
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Wed, 24 Mar 2004 13:01:44 +0100
Subject: [R] wolfinger microarray normalization
Message-ID: <406178A8.9090701@uv.es>

Hi,

concerning my earlier mail, maybe someone has noted from the variable names
that I try to analyse mircoarrary experiments.

Does anybody know of a R-implementation of the two-step mixed-model 
normalization procedure proposed by

Wolfinger et al. (2001) J. Comput. Biol. 8:625-637?

That would be great,

best,

joerg



From ripley at stats.ox.ac.uk  Wed Mar 24 13:07:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Mar 2004 12:07:18 +0000 (GMT)
Subject: [R] string problems ( grep and regepxr)
In-Reply-To: <134506337156.20040324115134@power.inescn.pt>
Message-ID: <Pine.LNX.4.44.0403241202360.11172-100000@gannet.stats>

On Wed, 24 Mar 2004, MMarques Power wrote:

> 
> Recently working with strings and data
> I have found a small problem.
> 
> Windows XP
> R 1.8.1
> 
> Reading data from a "txt file" with readLine.
> finding a specific line with "grep" command, all OK.
> but here comes the problem...
> After finding the correct line(s) i need to find a substring
> inside each string.
> In this case "tabs" I think it represented by "\t" in the grep command
> trying to use grep in each string it only returns 1 ...

That says it is present in character element one.  Do read the help page

Value:

     For 'grep' a vector giving either the indices of the elements of
     'x' that yielded a match or, if 'value' is 'TRUE', the matched
     elements.


> Afterwards I tried regexpr command it returns the correct position of the
> substring that I am looking for but it only reports the first one.
> does regexpr only returns the first one ?

Yes.

> Partial example:
> 
> d5 = "load0004   node0014        0.05    0.014583333"
>      "load0005   node0017        0.05    0.014583333"
>      "load0006   node0019        0.05    0.014583333"
> 
>      
> >grep("\t",d5[1])
> [1] 1
> >regexpr("\t",d5[1]
> [1] 9
> attr(,"match.length")
> [1] 1
> 
> any idea how to make regexpr return the several substrings ?
> or the grep and
> Am I missing anything obvious ?

Telling us what you actually want to do!  Would

sapply(strsplit(d5, "\t"), length)

be closer to what you have in mind?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Wed Mar 24 13:38:48 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 24 Mar 2004 07:38:48 -0500
Subject: [R] wolfinger microarray normalization
In-Reply-To: <406178A8.9090701@uv.es>
Message-ID: <BC86EB88.607F%sdavis2@mail.nih.gov>

Joerg,

Have you looked at the BioConductor packages (www.bioconductor.org)?  I'm
not sure if that particular one is there, but there are MANY methods for
normalization as well as objects for dealing with microarray data that may
be of interest.

Sean

On 3/24/04 7:01 AM, "Joerg Schaber" <Joerg.Schaber at uv.es> wrote:

> Hi,
> 
> concerning my earlier mail, maybe someone has noted from the variable names
> that I try to analyse mircoarrary experiments.
> 
> Does anybody know of a R-implementation of the two-step mixed-model
> normalization procedure proposed by
> 
> Wolfinger et al. (2001) J. Comput. Biol. 8:625-637?
> 
> That would be great,
> 
> best,
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tjagoe at liverpool.ac.uk  Wed Mar 24 13:46:04 2004
From: tjagoe at liverpool.ac.uk (Thomas Jagoe)
Date: Wed, 24 Mar 2004 12:46:04 -0000
Subject: [R] loess parameters
Message-ID: <BEEGJHNEILCHNDMIFAIAOEEHCGAA.tjagoe@liv.ac.uk>

Hi,
I have been successfully using the loess function for normalisation of a 2D
array set.
We have recently improved the quality criteria for the data and the numbers
of data points has been reduced to around from around 1000 to 700.
Previously the following would return the loess normalised values for
array$logratio but I am now getting an error:

> array <- read.table("A1.txt", header=T, sep="\t")
> array$logratio<-array$logs555-array$logs647
> array$logav<-(array$logs555+array$logs647)/2
> library(modreg)
> loess2d<-loess(logratio~x+y,data=array)
> array$logratio2DLoeNorm <-array$logratio - predict(loess2d, array)
Error in vector("double", length) : negative length vectors are not allowed

I am assuming that this is due to a problem fitting the data at some
locations and I have tried altering span without much success.  Can anyone
please advise ?

With thanks

Thomas



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Mar 24 14:00:17 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 24 Mar 2004 13:00:17 -0000
Subject: [R] geoR - help for bayesian modelling
Message-ID: <E1B67zf-000Nh8-U3@probity.mcc.ac.uk>

Hi,

I am trying to do a bayesian prediction for soil pollution data above 
a certain threshold, using geoR. 

Everything is working fine until i am doing the krig.bayes. I tried to 
do the prediction on a grid 67 by 113 cells and my computer is 
freezing to death. At larger numbers of cells it tells me after a while 
that it reaches the max. memory of 511 Mb. My computer has only 
512 Mb of RAM. What RAM capacity should i look for to do a 150 
x 250 cell grid???

If i want to do the prediction on my initial data locations (well, 
actually the prediction points are shifted 1 m in X and respectively 
Y direction, so the raw data coordinates don't coincide with the 
prediction coordinates) i am getting the following error using the 
command:

zn.bayes <- krige.bayes(zn.gdata, loc = xy, model = 
model.control(cov.model = "exponential", lambda = 0), prior = 
prior.control(phi.prior ="exponential", phi = 89.1894), 
output=output.control(n.predictive=2, mean.var = TRUE, quantile = 
c(0.025,0.25, 0.5, 0.75, 0.975), threshold = c(300)))

Error in cond.sim(env.loc = base.env, env.iter = iter.env, 
loc.coincide = get("loc.coincide",  : 
        chol: matrix not pos def, diag[13]= -1.279220e-018

I will really appreciate any suggestion you may have.

Thank you so much,

Monica



From ripley at stats.ox.ac.uk  Wed Mar 24 14:03:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Mar 2004 13:03:08 +0000 (GMT)
Subject: [R] loess parameters
In-Reply-To: <BEEGJHNEILCHNDMIFAIAOEEHCGAA.tjagoe@liv.ac.uk>
Message-ID: <Pine.LNX.4.44.0403241256480.11280-100000@gannet.stats>

You are probably running out of memory address space.  Can you 

1) Try this in 1.9.0 beta which gives a more informative error message, 
and

2) Use traceback() and the debugging tools to locate the error more 
exactly.

3) Consider using the options to loess to reduce the load.  Loess is not 
designed for smoothing a 2D grid and you appear only to want the fitted 
values at your grid.  If so, try the fitted() extractor function.
(Or the residual()  extractor function if all you want are residuals.)


On Wed, 24 Mar 2004, Thomas Jagoe wrote:

> Hi,
> I have been successfully using the loess function for normalisation of a 2D
> array set.
> We have recently improved the quality criteria for the data and the numbers
> of data points has been reduced to around from around 1000 to 700.
> Previously the following would return the loess normalised values for
> array$logratio but I am now getting an error:
> 
> > array <- read.table("A1.txt", header=T, sep="\t")
> > array$logratio<-array$logs555-array$logs647
> > array$logav<-(array$logs555+array$logs647)/2
> > library(modreg)
> > loess2d<-loess(logratio~x+y,data=array)
> > array$logratio2DLoeNorm <-array$logratio - predict(loess2d, array)
> Error in vector("double", length) : negative length vectors are not allowed
> 
> I am assuming that this is due to a problem fitting the data at some
> locations and I have tried altering span without much success.  Can anyone
> please advise ?
> 
> With thanks
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Wed Mar 24 14:09:58 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 24 Mar 2004 07:09:58 -0600
Subject: [R] combined random effects
In-Reply-To: <4061708E.1040309@uv.es>
References: <4061708E.1040309@uv.es>
Message-ID: <6rr7vi75nd.fsf@bates4.stat.wisc.edu>

Joerg Schaber <Joerg.Schaber at uv.es> writes:

> Hi,
> 
> I have the following linear mixed model:
> 
> y(g,i,j,k,l)=u + L(g) + T(i) + D(j) + S(k) + (TS)(i,k) + error(g,i,j,k,l)
> 
> where S(k) and the combined effect (TS)(i,k) are random effects
> whereas the rest are fixed effects.


> How do I specifiy the random part of the model formula in lme(),
> especially concerning the combined effect (TS)?

I think you should be able to specify this as
    random = ~ 1 | SLIDE/TREAT

> Moreover, when I run the model as a fixed effect model I get the
> following error:
> 
> 
>  > lm(logMed ~ lenAA + TREAT + DYE + SLIDE + SLIDE:DYE,data=MA)
> Error in lm.fit(x, y, offset = offset, ...) :
>         NA/NaN/Inf in foreign function call (arg 4)
> 
> However, when I run the model with a reduced data set it works.
> 
> In the first case I have 16512 data points whereas in the reduced case
> I use only half of it. Do I reach some internal threshold here?

You should get a different error message if you did reach a threshold.

I'd offer to look at the problem in more detail except that I just
learned that one of the main servers in the r-project.org domain (the
one that serves as cran.us.r-project.org, among other things) has
crashed and I need to get in to the office to check on it.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From Sam.Yang at csiro.au  Wed Mar 24 14:12:02 2004
From: Sam.Yang at csiro.au (Sam.Yang@csiro.au)
Date: Thu, 25 Mar 2004 00:12:02 +1100
Subject: [R] Rmpi and PBS
Message-ID: <1DE7577948CFFF488C33F12CC4385DD50172DEA3@exvic3-mel.vic.csiro.au>

Please remove me from the mailing list

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
To: Shengqiao Li
Cc: r-help
Sent: 24/03/2004 10:10 AM
Subject: Re: [R] Rmpi and PBS

On Tue, 2003-12-30 at 08:39, Shengqiao Li wrote:
> Hello:
> 
> Anybody knows how to run Rmpi through PBS (Portable Batch System) on a
> cluster computer. I'm using a supercomputer which require to submit
jobs
> to PBS queue for dispatching. I tried use mpirun in my PBS script. But
all
> my Rslaves are spawned to the same node. This is not desired.
> 
> Any suggestions are welcome!
> 
> Thanks in advance.
This is a late reply, but perhaps still of interest to some.  It looks
as if this problem has been solved, though only integrated into LAM with
release 7: http://www.lam-mpi.org/papers/hpcs2003/tm-implementation.pdf

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hodgess at gator.uhd.edu  Wed Mar 24 15:13:01 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 24 Mar 2004 08:13:01 -0600
Subject: [R][S] library question
Message-ID: <200403241413.i2OED1r22650@gator.dt.uh.edu>

Dear R and S+ People:

Is it possible to take one of the R libraries and put it
into S+ please?

R Windows XP 1.8.1
S+ Version 6.2

Thanks in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ripley at stats.ox.ac.uk  Wed Mar 24 15:15:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Wed, 24 Mar 2004 14:15:19 +0000 (GMT Standard Time)
Subject: [R][S] library question
In-Reply-To: <200403241413.i2OED1r22650@gator.dt.uh.edu>
References: <200403241413.i2OED1r22650@gator.dt.uh.edu>
Message-ID: <Pine.WNT.4.58.0403241412580.2932@auk>

In some cases it is possible to port an R *package* to S-PLUS: people have
done both that and the reverse.

If you have some specific examples in mind we may be able to help you
further.  Since you are on Windows, it may not be easy unless you have the
requisite tools (e.g. Visuall C++ and Fortran).


On Wed, 24 Mar 2004, Erin Hodgess wrote:

> Is it possible to take one of the R libraries and put it
> into S+ please?
>
> R Windows XP 1.8.1
> S+ Version 6.2


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Mar 24 15:21:31 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 24 Mar 2004 15:21:31 +0100
Subject: [R][S] library question
In-Reply-To: <200403241413.i2OED1r22650@gator.dt.uh.edu>
References: <200403241413.i2OED1r22650@gator.dt.uh.edu>
Message-ID: <16481.39275.402517.24253@gargle.gargle.HOWL>

>>>>> "Erin" == Erin Hodgess <hodgess at gator.uhd.edu>
>>>>>     on Wed, 24 Mar 2004 08:13:01 -0600 writes:

    Erin> Dear R and S+ People: Is it possible to take one of
    Erin> the R libraries and put it into S+ please?

"libraries" (i.e. a collection of compiled code): definitely not
"as is".

However, I presume you mean "packages".
There, it depends if the package only consists of S code (in the
R dialect).  If yes, you often can use most, if not all of the
code in S+. 
But do read the FAQ about S and R differences
and consider the fact that for graphics,
the step from R to S-plus is quite a bit of a downgrade,
and code usually will only work if it was written with back
portability in mind  ("back" : to S+).

{read all of the above with a bit of humour ..}

Note that R is fully open source, so you can always port R
packages that contain compiled code and data sets, and doing so
can be easy -- but may be not at all: 
    The C API of R being quite rich nowadays,
    you may have to work quite a bit on the source before you can
    link the compiled code into S-Plus.

Regards,
Martin

    Erin> R Windows XP 1.8.1 S+ Version 6.2



From henric.nilsson at statisticon.se  Wed Mar 24 15:39:00 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 24 Mar 2004 15:39:00 +0100
Subject: [R] GLMM
In-Reply-To: <IEEFKHGFNDGNNFCJALMFCEOCCDAA.s.chamaille@wanadoo.fr>
References: <IEEFKHGFNDGNNFCJALMFCEOCCDAA.s.chamaille@wanadoo.fr>
Message-ID: <6.0.3.0.0.20040324133416.04c853c8@10.0.10.66>

At 11:17 2004-03-24, you wrote:

>I'm working with count data following over-dispersed poisson distribution
>and have to work with mixed-models on them (like proc GENMOD on SAS sys.).
>I'm still not to sure about what function to use.

This is confusing: Proc GENMOD fits generalized linear models (GLM) and 
handles modelling of overdispersed Poisson data using quasi-likelihood 
(e.g. SCALE=P) or the negative binomial distribution (DIST=NEGBIN), but not 
generalized linear mixed models (GLMM).

In R a GLM is fitted using the glm function, and specifying 
family=quasipoisson is the equivalent of SCALE=P in SAS. The negative 
binomial case is handled either by the negative.binomial family function 
(when the shape parameter is known) or the glm.nb function (if you want to 
estimate the shape by ML). Both negative.binomial and glm.nb are found in 
Venables and Ripley's MASS package.

>It seems to me that a glmmPQL will do the job I want,

If GLMM is what you want, the functions glmmPQL (from MASS) or GLMM (from 
lme4, by Bates and co-workers) will most likely handle your needs.

>but I'll be glad if people who worked on this type of data can share what 
>they learned. Thanks for your time.

If you describe your data and what you'd like to do, someone may be able to 
help you.

The MASS functions, and much more, are described in Venables and Ripley 
"Modern Applied Statistics with S." GLMMs are described in e.g. Brown and 
Prescott "Applied Mixed Models in Medicine", but it's very SAS oriented.

//Henric



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Mar 24 16:05:58 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 24 Mar 2004 15:05:58 -0000
Subject: [R] colors, lines, characters .... documentation
Message-ID: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>

Hi,

Very so often when i am plotting something, doing a histogram, or 
whatever i am struggling to find out which are the numbers for 
different colors, palette names, types of lines, symbols, etc. Is 
there any documentation on line with all these numbers / names 
and the associated symbol / color???

For example if i am using the command image it uses a palette 
from red to yellow, with red the lowest value, and yellow the highest 
value. What if i want a reverse palette, with green the lowest value 
and yellow middle values and red highest value??? Or much more 
simple, just yellow lowest value and red highest value???

Thank you for assistance,

Monica



From Giovanni_Millo at generali.com  Wed Mar 24 16:06:55 2004
From: Giovanni_Millo at generali.com (Millo Giovanni)
Date: Wed, 24 Mar 2004 16:06:55 +0100
Subject: [R] LM omitted variables test
Message-ID: <74F2D4ED68558643B63A6CC21746040D9A072A@BEMAILEXTS1.ad.generali.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040324/564c1785/attachment.pl

From hb at maths.lth.se  Wed Mar 24 16:31:07 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Wed, 24 Mar 2004 16:31:07 +0100
Subject: [R] colors, lines, characters .... documentation
In-Reply-To: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>
Message-ID: <000201c411b5$06ec78d0$e502eb82@maths.lth.se>

Hi, many questions at once there, but here some help regarding
*symbols*. 

I've pasted a function plotSymbols() that shows all symbols available.
Note that the the symbols pch >= 128 are system dependent so you
should not expect them to look the same on Windows, Mac and Unix. Try
also plotSymbols(TRUE). To turn of the click-bell do
'options(locatorBell=FALSE)' (see ?locator).

Cheers

Henrik

----------------------------------------------------
BEGIN code
----------------------------------------------------
plotSymbols <- function(interactive=FALSE) {
  ASCII <- c("\000", sapply(1:255, function(i)
parse(text=paste("\"\\",
                    structure(i,class="octmode"), "\"",
sep=""))[[1]]));

  intToChar <- function(i) {
    ASCII[i %% 256 + 1];
  }

  interactive <- interactive && interactive();

  i <- 0:255;
  ncol <-16;
  
  top <- 3 + 2*interactive;
  opar <- par(cex.axis=0.7, mar=c(3,3,top,3)+0.1)
  on.exit(par(opar))

  plot(i%%ncol,1+i%/%ncol, pch=i, xlim=c(0,ncol-1), xlab="", ylab="", 
 
axes=FALSE);
  axis(1, at=0:15)
  axis(2, at=1:16, labels=0:15*16, las=2)
  axis(3, at=0:15)
  axis(4, at=1:16, labels=0:15*16+15, las=2)
  if (interactive) {
    title(main="Click on a symbol to add it to the data frame. Click
in margin to quit!", cex.main=0.8, line=3.5);
  }

  if (interactive) {
    df <- list();
    usr <- par("usr");
    ready <- FALSE;
    while (!ready) {
      click <- locator(n=1);
      x <- click$x;
      y <- click$y - 1;
      ready <- !(x > 0.5 && x < 15.5 && y > 0.5 && y < 15.5);
      if (!ready) {
        x <- round(x);
        y <- round(y);
        z <- 16*y + x;
        ch  <- intToChar(z);
        dec <- as.character(z); 
        hex <- intToHex(z);
        oct <- intToOct(z);
        spc <- paste(rep("0", 2-nchar(hex)), collapse="");
        hex <- paste(spc, hex, sep="");
        spc <- paste(rep("0", 3-nchar(oct)), collapse="");
        oct <- paste(spc, oct, sep="");
        df$ch  <- c(df$ch , ch );
        df$dec <- c(df$dec, dec);
        df$hex <- c(df$hex, hex);
        df$oct <- c(df$oct, oct);

        if (nchar(ch) == 0) ch <- " ";
        spc <- paste(rep(" ", 3-nchar(dec)), collapse="");
        dec <- paste(spc, dec, sep="");
        cat("Selected ASCII character '", ch, "' ", dec, " 0x", hex, 
                                                 " \\", oct, "\n",
sep="");
      }
    }
    return(df);
  }

  invisible()
} # plotSymbols()
----------------------------------------------------
END code
----------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monica 
> Palaseanu-Lovejoy
> Sent: den 24 mars 2004 16:06
> To: r-help at stat.math.ethz.ch
> Subject: [R] colors, lines, characters .... documentation
> 
> 
> Hi,
> 
> Very so often when i am plotting something, doing a histogram, or 
> whatever i am struggling to find out which are the numbers for 
> different colors, palette names, types of lines, symbols, etc. Is 
> there any documentation on line with all these numbers / names 
> and the associated symbol / color???
> 
> For example if i am using the command image it uses a palette 
> from red to yellow, with red the lowest value, and yellow the
highest 
> value. What if i want a reverse palette, with green the lowest value

> and yellow middle values and red highest value??? Or much more 
> simple, just yellow lowest value and red highest value???
> 
> Thank you for assistance,
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From salfner at informatik.hu-berlin.de  Wed Mar 24 16:49:34 2004
From: salfner at informatik.hu-berlin.de (Felix Salfner)
Date: Wed, 24 Mar 2004 16:49:34 +0100
Subject: [R] hierarchical clustering: stopping rule
Message-ID: <4061AE0E.1010904@informatik.hu-berlin.de>

I'm using 'agnes' from the 'cluster' package to cluster my data 
hierarchically.

I need to find out the 'optimal' number of clusters.

In 'Finding Groups in Data: An Introduction to Cluster Analysis' Kaufman 
and Rousseeuw refer to a strategy proposed by R. Mojena ('Hierarchical 
grouping methods and stopping rules: An evaluation' (The Computer 
Journal, 20(4), 1977).

Mojena describes group weighted average hierarchical clustering methods 
with the following formula:

        n_p          n_q
d_is = ---- d_ps  + ---- d_qs
        n_i          n_i


where i is the index for the new group to be formed out of groups p and q
and s represents a third group
d is the distance measure.

In every clustering step   a_j = min_{i<m} (d_im)


My question now is:

are the values of agnes.object$heights identical to the a_j defined above? 
(Despite of the fact that the heights are permutated for drawing)

I also read the publication of Lance and Williams who originally introduced the above notation but it didn't help ...

Thanks for any hint ...

Felix Salfner



From ivo.welch at yale.edu  Wed Mar 24 16:49:55 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Wed, 24 Mar 2004 10:49:55 -0500
Subject: [R] line number of errors?
Message-ID: <4061AE23.7090002@yale.edu>


Is it possible to instruct R to output a line number when an error or 
warning is encountered in a source() file?

sincerely,  /iaw



From christian.hoffmann at wsl.ch  Wed Mar 24 16:50:54 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Wed, 24 Mar 2004 16:50:54 +0100
Subject: [R] First Variable in lm
Message-ID: <4061AE5E.6050701@wsl.ch>

Hi all,

I just cannot think of how to do it:
I want to take the first variable (column) of a data frame and regress 
it against all other variables.

bla <- function (dat) {
   reg <- lm(whateverthefirstofthevariablenamesis ~., data=dat)
   return(reg)
}

What kind of function do I have to take instead of the 
whateverthefirstofthevariablenamesis,

eval(), substitute(), get(),  ...

to correctly compute this regression?

With   lm(get(names(dat)[1] ~., data=dat) there are no errors, but the 
first variable also shows up among the regressors.

Thanks for help.
Christian Hoffmann

-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From Rau at demogr.mpg.de  Wed Mar 24 16:54:47 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 24 Mar 2004 16:54:47 +0100
Subject: [R] colors, lines, characters .... documentation
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A07C8@hermes.demogr.mpg.de>

Hello,

> -----Original Message-----
> From:	Monica Palaseanu-Lovejoy
> [SMTP:monica.palaseanu-lovejoy at stud.man.ac.uk]
> Sent:	Wednesday, March 24, 2004 4:06 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] colors, lines, characters .... documentation
> 
> Hi,
> 
> Very so often when i am plotting something, doing a histogram, or 
> whatever i am struggling to find out which are the numbers for 
> different colors, palette names, types of lines, symbols, etc. Is 
> there any documentation on line with all these numbers / names 
> and the associated symbol / color???
> 
	what I found to be quite useful is found in the library Hmisc.
(Thanks, Frank Harrell!)

	library(Hmisc)
	show.col()
	show.pch()

	And if you look at the definitions of these functions, it might be
quite easy to modify them for your exact needs.

	Hope this helps,
	Roland



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From luke at stat.uiowa.edu  Wed Mar 24 17:03:35 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 24 Mar 2004 10:03:35 -0600 (CST)
Subject: [R] snow documentation comments
In-Reply-To: <1080096825.7027.107.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0403241002130.3564-100000@itasca.stat.uiowa.edu>

On Tue, 23 Mar 2004, Ross Boylan wrote:

> There are a few points I found unclear or unmentioned in the snow
> documentation (mostly I looked at the cluster.html web page).  I thought
> I'd mention them here.
> 
> What is the start up environment for the children?
> --------------------------------------------------
> My best guess at the answer is in parentheses
> Do they inherit shell variables? (no)
> Do they inherit variables set in R or other aspects of the R
> environment? (no)
> What directory does it start in? (the directory you are running in)
> What user are you? (same as original)
> 
> I realize some of these answers might depend on the parallelization
> layer you are using, or whether you select the homogenous option.

They do

> Number of nodes or number of children?
> --------------------------------------
> I thought all the counts of nodes (e.g., in makeCluster(5)) were of the
> total nodes in the cluster, i.e., children + 1.  However, I did a
> makeCluster(2) and I got 2 additional R processes running.  Have I
> misunderstood the semantics, or is it essentially an implementation
> detail that the master node starts a new R process?

makeCluster(2) creates a cluster of two processes that the master
process uses.  So there are a total of three processes.

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From luke at stat.uiowa.edu  Wed Mar 24 17:04:28 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 24 Mar 2004 10:04:28 -0600 (CST)
Subject: [R] Status of Rmpi--Good with tweaks
In-Reply-To: <4061137D.755F1E28@stats.uwo.ca>
Message-ID: <Pine.LNX.4.44.0403241003590.3564-100000@itasca.stat.uiowa.edu>

Thanks--I'll try this new version when I get back to working on
this--hopefully soon.

luke

On Tue, 23 Mar 2004, Hao Yu wrote:

> Sorry. I have not been able to update Rmpi since the version
> 0.4-4 on R site. However, I have been using and testing Rmpi
> internally since 0.4-4. Now it is version 0.4-7. See the
> attached package. It requires R 1.8.1 (no more serialize
> package requirement) and works with the newest MPI-LAM
> versions 7. It will configure automatically if a rpm package
> from www.lam-mpi.org is used. I tested it on both Redhat 9
> and Debian. At least it passed the package check without any
> warning on Redhat 9. Now the problem is that it may have
> some problems with the default lam coming with Debian
> system. This is the reason why I hesitate to release it to
> R. 
> 
> I am also working on it to see if #ifdef can be used to work
> MPI 1.2 specs other than LAM-MPI. 
> Hopefully in a couple of months, I am able to submit a
> stable version to R.
> 
> Regards,
> 
> Hao 
> 
> PS: Rmpi should still work without serializing as long as
> native MPI calls are used. The serializing is mainly used to
> help moving an arbitrarily R object around.
> 
> Ross Boylan wrote:
> > 
> > On Tue, 2004-03-23 at 18:26, Ross Boylan wrote:
> > > On Tue, 2004-03-23 at 17:35, Luke Tierney wrote:
> > > > The serialize package should no longer be needed since the
> > > > functionality is now in R itself.  I haven't run snow with a new
> > > > version of Rmpi newer than 0.4-4; with that version things worked on
> > > > my systems the last time I tried.  We need to revize the paper you
> > > > cite; when we get to that we'll hopefully have a chance to give a
> > > > newer version of Rmpi a go.
> > > >
> > > > Best,
> > > >
> > > > luke
> > >
> > > I got 0.4-6 to work, with one tweak to eliminate the loading of
> > > serialize and one fix of my own setup.
> > >
> > > I changed the .First.Lib code in Rmpi to be
> > >     if (!exists("serialize") && !require(serialize))
> > >       stop("serialize package cannot be loaded. Exit")
> > >
> > > !exists("serialize") && is new.  This is effort to check if the
> >                                           ^an
> > > serialize function exists (R 1.8) and skip loading the library in that
> > > case.  Since there are various ways this might be fooled, perhaps an
> > > explicit check of the R version would be better.
> > >
> > > I'm also not sure if the R packaging mechanism makes this easy, or
> > "this" refers to the modification to the .First.Lib that I made.
> > > autogenerates it.  This is a bit of a tricky situation, since serialize
> > > is required for some versions of R but should not be used for the most
> > > recent.
> > >
> > > I suspect that without this the cluster would not come up because Rmpi
> > > would not load properly.  I have not tested that.
> 
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From andy_liaw at merck.com  Wed Mar 24 17:04:52 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 11:04:52 -0500
Subject: [R] First Variable in lm
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A7D@usrymx25.merck.com>

You might be trying too hard:

> dat <- data.frame(y=rnorm(10), x1=rnorm(10), x2=rnorm(10))
> fit <- lm(dat)
> summary(fit)

Call:
lm(formula = dat)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.11643 -0.42746 -0.01442  0.55902  1.04890 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   0.7804     0.2700   2.891   0.0233 *
x1            0.7469     0.2925   2.553   0.0379 *
x2           -0.1099     0.2155  -0.510   0.6257  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 0.8234 on 7 degrees of freedom
Multiple R-Squared: 0.5016,     Adjusted R-squared: 0.3592 
F-statistic: 3.522 on 2 and 7 DF,  p-value: 0.08742 

HTH,
Andy

> From: Christian Hoffmann
> 
> Hi all,
> 
> I just cannot think of how to do it:
> I want to take the first variable (column) of a data frame 
> and regress 
> it against all other variables.
> 
> bla <- function (dat) {
>    reg <- lm(whateverthefirstofthevariablenamesis ~., data=dat)
>    return(reg)
> }
> 
> What kind of function do I have to take instead of the 
> whateverthefirstofthevariablenamesis,
> 
> eval(), substitute(), get(),  ...
> 
> to correctly compute this regression?
> 
> With   lm(get(names(dat)[1] ~., data=dat) there are no 
> errors, but the 
> first variable also shows up among the regressors.
> 
> Thanks for help.
> Christian Hoffmann
> 
> -- 
> Dr.sc.math.Christian W. Hoffmann, 
> http://www.wsl.ch/staff/christian.hoffmann
> Mathematics + Statistical Computing   e-mail: 
> christian.hoffmann at wsl.ch
> Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   
> -77  (office)
> CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dray at biomserv.univ-lyon1.fr  Wed Mar 24 17:07:29 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Wed, 24 Mar 2004 11:07:29 -0500
Subject: [R] colors, lines, characters .... documentation
In-Reply-To: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>
Message-ID: <5.2.1.1.0.20040324105643.00b3d998@biomserv.univ-lyon1.fr>

See
?palette
?colors

the colors used when you use number are those in palette:
 > palette()
[1] "black"   "red"     "green3"  "blue"    "cyan"    "magenta" 
"yellow"  "gray"
 > plot(1:20,col=1:20)

You can define your own palette with colors available in colors:
 > palette(colors()[sample(1:657,20)])
 > palette()
  [1] 
"peachpuff"         "purple2"           "gray37"            "paleturquoise4"
  [5] "hotpink"           "mediumspringgreen" 
"gray82"            "red2"
  [9] 
"purple3"           "cadetblue2"        "gray55"            "red"
[13] 
"tomato4"           "gray39"            "rosybrown"         "darkorange1"
[17] 
"gray27"            "lemonchiffon3"     "oldlace"           "honeydew"
 >  plot(1:20,col=1:20)

For the image function, you can enter your own vector of colors (by 
default, col = heat.colors(12)) and one way to define your colors is to use 
'rgb' function with values for Red - Green - Blue.

Sincerely,


At 10:05 24/03/2004, Monica Palaseanu-Lovejoy wrote:
>Hi,
>
>Very so often when i am plotting something, doing a histogram, or
>whatever i am struggling to find out which are the numbers for
>different colors, palette names, types of lines, symbols, etc. Is
>there any documentation on line with all these numbers / names
>and the associated symbol / color???
>
>For example if i am using the command image it uses a palette
>from red to yellow, with red the lowest value, and yellow the highest
>value. What if i want a reverse palette, with green the lowest value
>and yellow middle values and red highest value??? Or much more
>simple, just yellow lowest value and red highest value???
>
>Thank you for assistance,
>
>Monica
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From maechler at stat.math.ethz.ch  Wed Mar 24 17:07:23 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 24 Mar 2004 17:07:23 +0100
Subject: [R] line number of errors?
In-Reply-To: <4061AE23.7090002@yale.edu>
References: <4061AE23.7090002@yale.edu>
Message-ID: <16481.45627.630781.488287@gargle.gargle.HOWL>

>>>>> "ivo" == ivo welch <ivo.welch at yale.edu>
>>>>>     on Wed, 24 Mar 2004 10:49:55 -0500 writes:

    ivo> Is it possible to instruct R to output a line number
    ivo> Is it possible to instruct R to output a line number
    ivo> when an error or warning is encountered in a source()
    ivo> file?

yes, it is possible, since R is a complete programming language,
you can write a new version of source()...  ;-) 
:-)

Note that this feature is for free when using 
ESS (Emacs Speaks Statistics),
and it's C-c C-l (or [Load Source File] entry from the 'iESS'
		  or [Load File] from the 'ESS' menu)

Regards,
Martin



From ggrothendieck at myway.com  Wed Mar 24 17:25:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Mar 2004 16:25:14 +0000 (UTC)
Subject: [R] colors, lines, characters .... documentation
References: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>
	<5.2.1.1.0.20040324105643.00b3d998@biomserv.univ-lyon1.fr>
Message-ID: <loom.20040324T171906-353@post.gmane.org>

Stephane DRAY <dray <at> biomserv.univ-lyon1.fr> writes:
> [...]
> the colors used when you use number are those in palette:
>  > palette()
> [1] "black"   "red"     "green3"  "blue"    "cyan"    "magenta" 
> "yellow"  "gray"
>  > plot(1:20,col=1:20)
> 
> You can define your own palette with colors available in colors:
>  > palette(colors()[sample(1:657,20)])
>  > palette()
> [...]
>  >  plot(1:20,col=1:20)
> 
> For the image function, you can enter your own vector of colors (by 
> default, col = heat.colors(12)) and one way to define your colors is to use 
> 'rgb' function with values for Red - Green - Blue.

In conjunction with these sorts of manipulations you might also be
interested in the color name <--> hex code conversion functions 
previously posted on r-help and found at:

   http://maths.newcastle.edu.au/~rking/R/help/03a/7417.html



From macq at llnl.gov  Wed Mar 24 17:35:43 2004
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 24 Mar 2004 08:35:43 -0800
Subject: [R] colors, lines, characters .... documentation
In-Reply-To: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>
References: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>
Message-ID: <p06002006bc87687c4691@[128.115.153.6]>

Here is a little function that will show available colors in groups 
of 100 at a time.
I've only tested it in an X windows environment.

function (indx = 0:6)
{
     for (ii in unique(indx)) {
         is <- 100 * ii + 1:100
         if (min(is) > length(colors())) {
             cat("Maximum value of arg is", floor(length(colors())/100),
                 "\n")
             return(NULL)
         }
         foo <- matrix(colors()[is], nrow = 10)
         par(mar = c(3, 3, 0.25, 0.25))
         plot(1:10, 1:10, type = "n", yaxt = "n", xlab = "", ylab = "")
         axis(2, at = 1:10, lab = 10:1)
         for (j in 1:10) {
             for (i in 1:10) {
                 points(j, 11 - i, col = foo[i, j], pch = 16,
                   cex = 4)
                 text(j, 11 - i - 0.3, foo[i, j], cex = 0.8)
             }
         }
         if (length(indx) > 1 & ii < max(indx))
             readline(paste("Currently showing group", ii, "  CR to continue "))
     }
     invisible(foo)
}

-Don

At 3:05 PM +0000 3/24/04, Monica Palaseanu-Lovejoy wrote:
>Hi,
>
>Very so often when i am plotting something, doing a histogram, or
>whatever i am struggling to find out which are the numbers for
>different colors, palette names, types of lines, symbols, etc. Is
>there any documentation on line with all these numbers / names
>and the associated symbol / color???
>
>For example if i am using the command image it uses a palette
>from red to yellow, with red the lowest value, and yellow the highest
>value. What if i want a reverse palette, with green the lowest value
>and yellow middle values and red highest value??? Or much more
>simple, just yellow lowest value and red highest value???
>
>Thank you for assistance,
>
>Monica
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From rossini at blindglobe.net  Wed Mar 24 17:47:49 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 24 Mar 2004 08:47:49 -0800
Subject: [R] wolfinger microarray normalization
In-Reply-To: <406178A8.9090701@uv.es> (Joerg Schaber's message of "Wed, 24
	Mar 2004 13:01:44 +0100")
References: <406178A8.9090701@uv.es>
Message-ID: <85y8pq6vka.fsf@servant.blindglobe.net>

Joerg Schaber <Joerg.Schaber at uv.es> writes:

> Hi,
>
> concerning my earlier mail, maybe someone has noted from the variable names
> that I try to analyse mircoarrary experiments.
>
> Does anybody know of a R-implementation of the two-step mixed-model
> normalization procedure proposed by
>
> Wolfinger et al. (2001) J. Comput. Biol. 8:625-637?
>
> That would be great,

I don't believe that there is a package that will do it.   Perhaps the
new version of LME will assist, but if I remember correctly, the
problem is the size of the computations for most modern gene
expression array technologies (if you had a big enough machine...).

There are some good things about SAS, and its handling of large scale
computations still can't be matched well by R.  Whether this feature is
appropriately used in all applications is another issue.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Mar 24 18:03:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Mar 2004 17:03:44 +0000 (GMT)
Subject: [R] First Variable in lm
In-Reply-To: <4061AE5E.6050701@wsl.ch>
Message-ID: <Pine.LNX.4.44.0403241659200.11553-100000@gannet.stats>

On Wed, 24 Mar 2004, Christian Hoffmann wrote:

> Hi all,
> 
> I just cannot think of how to do it:
> I want to take the first variable (column) of a data frame and regress 
> it against all other variables.
> 
> bla <- function (dat) {
>    reg <- lm(whateverthefirstofthevariablenamesis ~., data=dat)
>    return(reg)
> }
> 
> What kind of function do I have to take instead of the 
> whateverthefirstofthevariablenamesis,
> 
> eval(), substitute(), get(),  ...
> 
> to correctly compute this regression?
> 
> With   lm(get(names(dat)[1] ~., data=dat) there are no errors, but the 
> first variable also shows up among the regressors.

Andy Liaw has pointed out that lm(dat) happens to work.  But for a more 
generalizable solution try

bla <- function (dat)
  eval(substitute(lm(foo ~., data=dat), list(foo=as.name(names(dat)[1]))))

which has the advantage of embedding a clean value of $call.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fm3a004 at math.uni-hamburg.de  Wed Mar 24 18:15:55 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Wed, 24 Mar 2004 18:15:55 +0100 (MET)
Subject: [R] statistical significance test for cluster agreement
In-Reply-To: <20040324091240.24557.qmail@web60003.mail.yahoo.com>
Message-ID: <Pine.GSO.3.95q.1040324180554.11217B-100000@sun11.math.uni-hamburg.de>

Dear Alexander,

On Wed, 24 Mar 2004, Alexander Sirotkin [at Yahoo] wrote:

> Like you said, such kind of test will not give me
> anything that Rand index does not, except for p-value.
> 
> The null hypothesis, in my case, is that clustering
> results does not match a different clustering, that
> someone alse did on the same data.

Usually, probability distributions (which you need to formulate null
hypotheses) are over data, not over different
methods applied to the same data. If you see two clusterings on the same
data, they are identical, if they are 100% identical, and if not, then
not. That's not a question of significance.

What you seem to want is the assessment of stability of a clustering on
given data by applying different cluster analyses, but this kind of
problem is not treated in terms of
"significance". Different cluster analyses do different things, and there
is no reason to expect that their results are the same apart from "random
variation" (the only exception is random variation in running the same
algorithm such as k-means from different random starting values - but
that's not a problem to investigate if you *know* the cluster
analysis method that produced your clustering).

Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From ggrothendieck at myway.com  Wed Mar 24 18:51:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Mar 2004 17:51:20 +0000 (UTC)
Subject: [R] First Variable in lm
References: <4061AE5E.6050701@wsl.ch>
Message-ID: <loom.20040324T184546-507@post.gmane.org>

Christian Hoffmann <christian.hoffmann <at> wsl.ch> writes:

> 
> Hi all,
> 
> I just cannot think of how to do it:
> I want to take the first variable (column) of a data frame and regress 
> it against all other variables.
> 
> bla <- function (dat) {
>    reg <- lm(whateverthefirstofthevariablenamesis ~., data=dat)
>    return(reg)
> }

Andy has already given a particularly concise solution but if your
variable is not in first position then you could rearrange the 
order of the variables to allow his solution or use this which works 
for any specified position of the dependent variable:

data(longley)
lm( longley[,7] ~. , data = longley[,-7] )



From vograno at evafunds.com  Wed Mar 24 19:17:33 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Wed, 24 Mar 2004 10:17:33 -0800
Subject: [R] how to customize .First.lib
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3AC8@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040324/0e07e306/attachment.pl

From Valentin.Stanescu at ro.ey.com  Wed Mar 24 19:14:30 2004
From: Valentin.Stanescu at ro.ey.com (Valentin Stanescu)
Date: Wed, 24 Mar 2004 20:14:30 +0200
Subject: [R] Ordered logit/probit
Message-ID: <OF476EAD25.EBDFC9D1-ON42256E61.0054677F-C2256E61.006434AA@eyi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040324/61d2f784/attachment.pl

From RBaskin at ahrq.gov  Wed Mar 24 19:19:55 2004
From: RBaskin at ahrq.gov (Baskin, Robert)
Date: Wed, 24 Mar 2004 13:19:55 -0500
Subject: [R] First Variable in lm
Message-ID: <6BCD3F430455B1418750004BCD27925905C60A@exchange2.ahrq.gov>

First: Thanks to everyone who develops R, maintains r-help, and participates
in the list :)  

This is a silly follow up question.

>From Andy Liaw:
> dat <- data.frame(y=rnorm(10), x1=rnorm(10), x2=rnorm(10))

(Silly question - if the answer is on the lm or formula help page I didn't
get it:)
Why does lm | formula treat dat[,1] slightly differently than dat$y?

I see what it is doing - I am curious as to why:)

> lm(dat$y ~ .,data = dat)

Call:
lm(formula = dat$y ~ ., data = dat)

Coefficients:
(Intercept)           x1           x2  
   -0.08754     -0.04456     -0.16905  

> lm(dat[,1] ~ .,data = dat)

Call:
lm(formula = dat[, 1] ~ ., data = dat)
Coefficients:
(Intercept)            y           x1           x2  
 -5.266e-17    1.000e+00    4.121e-17   -3.274e-17  


As Gabor Grothendieck pointed out:
lm(formula = dat[, 1] ~ ., data = dat[,-1])
works like
lm(formula = dat$y ~ ., data = dat)

Curious Minds Want to Know
Bob


-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at myway.com] 
Sent: Wednesday, March 24, 2004 12:51 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] First Variable in lm

Christian Hoffmann <christian.hoffmann <at> wsl.ch> writes:

> 
> Hi all,
> 
> I just cannot think of how to do it:
> I want to take the first variable (column) of a data frame and regress 
> it against all other variables.
> 
> bla <- function (dat) {
>    reg <- lm(whateverthefirstofthevariablenamesis ~., data=dat)
>    return(reg)
> }

Andy has already given a particularly concise solution but if your
variable is not in first position then you could rearrange the 
order of the variables to allow his solution or use this which works 
for any specified position of the dependent variable:

data(longley)
lm( longley[,7] ~. , data = longley[,-7] )

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Mar 24 19:49:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Mar 2004 18:49:09 +0000 (GMT)
Subject: [R] First Variable in lm
In-Reply-To: <6BCD3F430455B1418750004BCD27925905C60A@exchange2.ahrq.gov>
Message-ID: <Pine.LNX.4.44.0403241839260.12174-100000@gannet.stats>

It isn't lm but terms.formula.  Compare

terms(dat$y ~ .,data = dat)
terms(dat[, 1] ~ ., data = dat)

Now as to why, exactly, see the C code in src/main/model.c.
The short answer is that dat$y matches y, and dat[, 1] does not.
(I am not at all sure the first is intentional.)

On Wed, 24 Mar 2004, Baskin, Robert wrote:

> First: Thanks to everyone who develops R, maintains r-help, and participates
> in the list :)  
> 
> This is a silly follow up question.
> 
> >From Andy Liaw:
> > dat <- data.frame(y=rnorm(10), x1=rnorm(10), x2=rnorm(10))
> 
> (Silly question - if the answer is on the lm or formula help page I didn't
> get it:)
> Why does lm | formula treat dat[,1] slightly differently than dat$y?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From stephane.dray at umontreal.ca  Wed Mar 24 20:41:15 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Wed, 24 Mar 2004 14:41:15 -0500
Subject: [R] binding vectors or matrix using their names
Message-ID: <5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>

Hello list,
I have two vectors x and x2:

x=runif(10)
x2=runif(10)

and one vectors with their names :

my.names=c("x","x2")

I would like to cbind these two vectors using their names contained in the 
vector my.names.
I can create a string with comma
ncomma=paste(my.names,collapse=",")

and now, I just need a function to transform this string into a adequate 
argument for cbind:

cbind(afunction(ncomma))

Is there in R a function that can do the job ? If not, how can I do it ??

Thanks in advance,
Sincerely.


St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From vograno at evafunds.com  Wed Mar 24 20:48:30 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Wed, 24 Mar 2004 11:48:30 -0800
Subject: [R] binding vectors or matrix using their names
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3ACB@phost015.intermedia.net>

?get to convert names into objects

> -----Original Message-----
> From: Stephane DRAY [mailto:stephane.dray at umontreal.ca] 
> Sent: Wednesday, March 24, 2004 11:41 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] binding vectors or matrix using their names
> 
> 
> Hello list,
> I have two vectors x and x2:
> 
> x=runif(10)
> x2=runif(10)
> 
> and one vectors with their names :
> 
> my.names=c("x","x2")
> 
> I would like to cbind these two vectors using their names 
> contained in the 
> vector my.names.
> I can create a string with comma
> ncomma=paste(my.names,collapse=",")
> 
> and now, I just need a function to transform this string into 
> a adequate 
> argument for cbind:
> 
> cbind(afunction(ncomma))
> 
> Is there in R a function that can do the job ? If not, how 
> can I do it ??
> 
> Thanks in advance,
> Sincerely.
> 
> 
> St?phane DRAY
> --------------------------------------------------------------
> ------------------------------------ 
> 
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville 
> Montr?al, Qu?bec H3C 3J7, Canada
> 
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> --------------------------------------------------------------
> ------------------------------------ 
> 
> Web                                          
> http://www.steph280.freesurf.fr/
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tblackw at umich.edu  Wed Mar 24 20:56:59 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Wed, 24 Mar 2004 14:56:59 -0500 (EST)
Subject: [R] binding vectors or matrix using their names
In-Reply-To: <5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>
References: <5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>
Message-ID: <Pine.SOL.4.58.0403241455230.116@zektor.gpcc.itd.umich.edu>


I believe the syntax is

result <- do.call(cbind, as.list(my.names))

Haven't checked this on your example, though.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 24 Mar 2004, Stephane DRAY wrote:

> Hello list,
> I have two vectors x and x2:
>
> x=runif(10)
> x2=runif(10)
>
> and one vectors with their names :
>
> my.names=c("x","x2")
>
> I would like to cbind these two vectors using their names contained in the
> vector my.names.
> I can create a string with comma
> ncomma=paste(my.names,collapse=",")
>
> and now, I just need a function to transform this string into a adequate
> argument for cbind:
>
> cbind(afunction(ncomma))
>
> Is there in R a function that can do the job ? If not, how can I do it ??
>
> Thanks in advance,
> Sincerely.
>
>
> St?phane DRAY
> --------------------------------------------------------------------------------------------------
>
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
>
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> --------------------------------------------------------------------------------------------------
>
> Web                                          http://www.steph280.freesurf.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dray at biomserv.univ-lyon1.fr  Wed Mar 24 21:07:17 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Wed, 24 Mar 2004 15:07:17 -0500
Subject: [R] binding vectors or matrix using their names
In-Reply-To: <Pine.SOL.4.58.0403241455230.116@zektor.gpcc.itd.umich.edu>
References: <5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>
	<5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>
Message-ID: <5.2.1.1.0.20040324150503.036a3178@biomserv.univ-lyon1.fr>

Hi Tom,

Your approach did not work,

 > do.call("cbind", as.list(my.names))
      [,1] [,2]
[1,] "x"  "x2"

but it helps me a lot to find the good one:

do.call("cbind", as.list(parse(text=my.names)))

Thanks,


At 14:56 24/03/2004, Tom Blackwell wrote:

>I believe the syntax is
>
>result <- do.call(cbind, as.list(my.names))
>
>Haven't checked this on your example, though.
>
>-  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
>On Wed, 24 Mar 2004, Stephane DRAY wrote:
>
> > Hello list,
> > I have two vectors x and x2:
> >
> > x=runif(10)
> > x2=runif(10)
> >
> > and one vectors with their names :
> >
> > my.names=c("x","x2")
> >
> > I would like to cbind these two vectors using their names contained in the
> > vector my.names.
> > I can create a string with comma
> > ncomma=paste(my.names,collapse=",")
> >
> > and now, I just need a function to transform this string into a adequate
> > argument for cbind:
> >
> > cbind(afunction(ncomma))
> >
> > Is there in R a function that can do the job ? If not, how can I do it ??
> >
> > Thanks in advance,
> > Sincerely.
> >
> >
> > St?phane DRAY
> > 
> --------------------------------------------------------------------------------------------------
> >
> > D?partement des Sciences Biologiques
> > Universit? de Montr?al, C.P. 6128, succursale centre-ville
> > Montr?al, Qu?bec H3C 3J7, Canada
> >
> > Tel : 514 343 6111 poste 1233
> > E-mail : stephane.dray at umontreal.ca
> > 
> --------------------------------------------------------------------------------------------------
> >
> > 
> Web                                          http://www.steph280.freesurf.fr/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From ross at biostat.ucsf.edu  Wed Mar 24 21:45:33 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 24 Mar 2004 12:45:33 -0800
Subject: [R] snow documentation comments
In-Reply-To: <Pine.LNX.4.44.0403241002130.3564-100000@itasca.stat.uiowa.edu>
References: <Pine.LNX.4.44.0403241002130.3564-100000@itasca.stat.uiowa.edu>
Message-ID: <1080161133.7025.133.camel@iron.libaux.ucsf.edu>

On Wed, 2004-03-24 at 08:03, Luke Tierney wrote:
> On Tue, 23 Mar 2004, Ross Boylan wrote:
> 
> > There are a few points I found unclear or unmentioned in the snow
> > documentation (mostly I looked at the cluster.html web page).  I thought
> > I'd mention them here.
> > 
> > What is the start up environment for the children?
> > --------------------------------------------------
> > My best guess at the answer is in parentheses
> > Do they inherit shell variables? (no)
> > Do they inherit variables set in R or other aspects of the R
> > environment? (no)
> > What directory does it start in? (the directory you are running in)
> > What user are you? (same as original)
> > 
> > I realize some of these answers might depend on the parallelization
> > layer you are using, or whether you select the homogenous option.
> 
> They do
I've since realized they may also depend on the exact version of the
package used (e.g., newer lams apparently have the ability to export
environment variables) and the way it is invoked.

Personally, I'd even find a single sentence saying explicitly that "it
depends" clarifying.  It would probably also be good to discuss the
behavior of the socket-based communication, since that it supplied by
the snow package.  Information on the other interfaces would be nice
too, but probably hazardous to provide accurately.

I'll not for the record that my answers above are using Rmpi with
lam/mpi 6.5.8-2 on Debian, running just on my local computer.  And some
of the answers are speculative!
> 
> > Number of nodes or number of children?
> > --------------------------------------
> > I thought all the counts of nodes (e.g., in makeCluster(5)) were of the
> > total nodes in the cluster, i.e., children + 1.  However, I did a
> > makeCluster(2) and I got 2 additional R processes running.  Have I
> > misunderstood the semantics, or is it essentially an implementation
> > detail that the master node starts a new R process?
> 
> makeCluster(2) creates a cluster of two processes that the master
> process uses.  So there are a total of three processes.
So work that is distributed via snow goes to the two processes only?



From andy_liaw at merck.com  Wed Mar 24 21:52:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 15:52:05 -0500
Subject: [R] binding vectors or matrix using their names
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A80@usrymx25.merck.com>

Perhaps simler:

> x1 <- 1:5
> x2 <- 2:7
> xname <- c("x1", "x2")
> sapply(xname, get)
     x1 x2
[1,]  1  2
[2,]  2  3
[3,]  3  4
[4,]  4  5
[5,]  5  6

HTH,
Andy

> From: Stephane DRAY
> 
> Hi Tom,
> 
> Your approach did not work,
> 
>  > do.call("cbind", as.list(my.names))
>       [,1] [,2]
> [1,] "x"  "x2"
> 
> but it helps me a lot to find the good one:
> 
> do.call("cbind", as.list(parse(text=my.names)))
> 
> Thanks,
> 
> 
> At 14:56 24/03/2004, Tom Blackwell wrote:
> 
> >I believe the syntax is
> >
> >result <- do.call(cbind, as.list(my.names))
> >
> >Haven't checked this on your example, though.
> >
> >-  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> >On Wed, 24 Mar 2004, Stephane DRAY wrote:
> >
> > > Hello list,
> > > I have two vectors x and x2:
> > >
> > > x=runif(10)
> > > x2=runif(10)
> > >
> > > and one vectors with their names :
> > >
> > > my.names=c("x","x2")
> > >
> > > I would like to cbind these two vectors using their names 
> contained in the
> > > vector my.names.
> > > I can create a string with comma
> > > ncomma=paste(my.names,collapse=",")
> > >
> > > and now, I just need a function to transform this string 
> into a adequate
> > > argument for cbind:
> > >
> > > cbind(afunction(ncomma))
> > >
> > > Is there in R a function that can do the job ? If not, 
> how can I do it ??
> > >
> > > Thanks in advance,
> > > Sincerely.
> > >
> > >
> > > St?phane DRAY
> > > 
> > 
> --------------------------------------------------------------
> ------------------------------------
> > >
> > > D?partement des Sciences Biologiques
> > > Universit? de Montr?al, C.P. 6128, succursale centre-ville
> > > Montr?al, Qu?bec H3C 3J7, Canada
> > >
> > > Tel : 514 343 6111 poste 1233
> > > E-mail : stephane.dray at umontreal.ca
> > > 
> > 
> --------------------------------------------------------------
> ------------------------------------
> > >
> > > 
> > Web                                          
> http://www.steph280.freesurf.fr/
> > >
> > > 
> ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > >
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> St?phane DRAY
> 
> --------------------------------------------------------------
> ------------------------------------ 
> 
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
> 
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> --------------------------------------------------------------
> ------------------------------------ 
> 
> Web                                          
> http://www.steph280.freesurf.fr/
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pburns at pburns.seanet.com  Wed Mar 24 22:01:54 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 24 Mar 2004 21:01:54 +0000
Subject: [R] binding vectors or matrix using their names
References: <5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>	<5.2.1.1.0.20040324143511.00bc9d78@magellan.umontreal.ca>
	<5.2.1.1.0.20040324150503.036a3178@biomserv.univ-lyon1.fr>
Message-ID: <4061F742.8090306@pburns.seanet.com>

I think you are looking for the eval-parse-text idiom:

eval(parse(text=paste("cbind(", paste(my.names, collapse=", "), ")")))

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Stephane DRAY wrote:

> Hi Tom,
>
> Your approach did not work,
>
> > do.call("cbind", as.list(my.names))
>      [,1] [,2]
> [1,] "x"  "x2"
>
> but it helps me a lot to find the good one:
>
> do.call("cbind", as.list(parse(text=my.names)))
>
> Thanks,
>
>
> At 14:56 24/03/2004, Tom Blackwell wrote:
>
>> I believe the syntax is
>>
>> result <- do.call(cbind, as.list(my.names))
>>
>> Haven't checked this on your example, though.
>>
>> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>>
>> On Wed, 24 Mar 2004, Stephane DRAY wrote:
>>
>> > Hello list,
>> > I have two vectors x and x2:
>> >
>> > x=runif(10)
>> > x2=runif(10)
>> >
>> > and one vectors with their names :
>> >
>> > my.names=c("x","x2")
>> >
>> > I would like to cbind these two vectors using their names contained 
>> in the
>> > vector my.names.
>> > I can create a string with comma
>> > ncomma=paste(my.names,collapse=",")
>> >
>> > and now, I just need a function to transform this string into a 
>> adequate
>> > argument for cbind:
>> >
>> > cbind(afunction(ncomma))
>> >
>> > Is there in R a function that can do the job ? If not, how can I do 
>> it ??
>> >
>> > Thanks in advance,
>> > Sincerely.
>> >
>> >
>> > St?phane DRAY
>> > 
>> -------------------------------------------------------------------------------------------------- 
>>
>> >
>> > D?partement des Sciences Biologiques
>> > Universit? de Montr?al, C.P. 6128, succursale centre-ville
>> > Montr?al, Qu?bec H3C 3J7, Canada
>> >
>> > Tel : 514 343 6111 poste 1233
>> > E-mail : stephane.dray at umontreal.ca
>> > 
>> -------------------------------------------------------------------------------------------------- 
>>
>> >
>> > Web                                          
>> http://www.steph280.freesurf.fr/
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> St?phane DRAY
> -------------------------------------------------------------------------------------------------- 
>
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
>
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> -------------------------------------------------------------------------------------------------- 
>
> Web                                          
> http://www.steph280.freesurf.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From andy_liaw at merck.com  Wed Mar 24 22:10:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 16:10:43 -0500
Subject: [R] binding vectors or matrix using their names
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A81@usrymx25.merck.com>

Gefore anyone jumps on me, I fibbed:

> From: Liaw, Andy
> 
> Perhaps simler:
> 
> > x1 <- 1:5
> > x2 <- 2:7

That should've been x2 <- 2:6.  (I mistyped the first time, but
cut-and-pasted the wrong line...)

> > xname <- c("x1", "x2")
> > sapply(xname, get)
>      x1 x2
> [1,]  1  2
> [2,]  2  3
> [3,]  3  4
> [4,]  4  5
> [5,]  5  6
> 
> HTH,
> Andy
> 
> > From: Stephane DRAY
> > 
> > Hi Tom,
> > 
> > Your approach did not work,
> > 
> >  > do.call("cbind", as.list(my.names))
> >       [,1] [,2]
> > [1,] "x"  "x2"
> > 
> > but it helps me a lot to find the good one:
> > 
> > do.call("cbind", as.list(parse(text=my.names)))
> > 
> > Thanks,
> > 
> > 
> > At 14:56 24/03/2004, Tom Blackwell wrote:
> > 
> > >I believe the syntax is
> > >
> > >result <- do.call(cbind, as.list(my.names))
> > >
> > >Haven't checked this on your example, though.
> > >
> > >-  tom blackwell  -  u michigan medical school  -  ann arbor  -
> > >
> > >On Wed, 24 Mar 2004, Stephane DRAY wrote:
> > >
> > > > Hello list,
> > > > I have two vectors x and x2:
> > > >
> > > > x=runif(10)
> > > > x2=runif(10)
> > > >
> > > > and one vectors with their names :
> > > >
> > > > my.names=c("x","x2")
> > > >
> > > > I would like to cbind these two vectors using their names 
> > contained in the
> > > > vector my.names.
> > > > I can create a string with comma
> > > > ncomma=paste(my.names,collapse=",")
> > > >
> > > > and now, I just need a function to transform this string 
> > into a adequate
> > > > argument for cbind:
> > > >
> > > > cbind(afunction(ncomma))
> > > >
> > > > Is there in R a function that can do the job ? If not, 
> > how can I do it ??
> > > >
> > > > Thanks in advance,
> > > > Sincerely.
> > > >
> > > >
> > > > St?phane DRAY
> > > > 
> > > 
> > --------------------------------------------------------------
> > ------------------------------------
> > > >
> > > > D?partement des Sciences Biologiques
> > > > Universit? de Montr?al, C.P. 6128, succursale centre-ville
> > > > Montr?al, Qu?bec H3C 3J7, Canada
> > > >
> > > > Tel : 514 343 6111 poste 1233
> > > > E-mail : stephane.dray at umontreal.ca
> > > > 
> > > 
> > --------------------------------------------------------------
> > ------------------------------------
> > > >
> > > > 
> > > Web                                          
> > http://www.steph280.freesurf.fr/
> > > >
> > > > 
> > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > St?phane DRAY
> > 
> > --------------------------------------------------------------
> > ------------------------------------ 
> > 
> > D?partement des Sciences Biologiques
> > Universit? de Montr?al, C.P. 6128, succursale centre-ville
> > Montr?al, Qu?bec H3C 3J7, Canada
> > 
> > Tel : 514 343 6111 poste 1233
> > E-mail : stephane.dray at umontreal.ca
> > --------------------------------------------------------------
> > ------------------------------------ 
> > 
> > Web                                          
> > http://www.steph280.freesurf.fr/
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From p.connolly at hortresearch.co.nz  Wed Mar 24 22:31:59 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 25 Mar 2004 09:31:59 +1200
Subject: [R] Problems with postscript output
In-Reply-To: <20040324112306.GA7522@localhost>;
	from p.pagel@gsf.de on Wed, Mar 24, 2004 at 12:23:06PM +0100
References: <20040324102450.GA24046@hindemith.TechFak.Uni-Bielefeld.DE>
	<20040324112306.GA7522@localhost>
Message-ID: <20040325093159.S2137@hortresearch.co.nz>

On Wed, 24-Mar-2004 at 12:23PM +0100, Philipp Pagel wrote:

|> 	Hi!
|> 
|> > I have a little problem with saving plots to file.  I use the command
|> > postscript() followed by the plotting command and a dev.off().
|> > 
|> > When I then look at the resulting image saved to disk, some of the
|> > axis labels are missing (see attached image). Is there a way to fix
|> > this. 
|> 
|> R is trying to be smart about labels getting too close/overlap. Specify the
|> graph to be a little wider and the labels will appear.

Looks as though that one's already taking up a whole page.  Might be
simpler to reduce the axis.cex a tad (assuming the missing ones are
not more than 3 letters).

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ross at biostat.ucsf.edu  Wed Mar 24 22:48:14 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 24 Mar 2004 13:48:14 -0800
Subject: [R] Status of Rmpi--Good with tweaks
In-Reply-To: <4061137D.755F1E28@stats.uwo.ca>
References: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>
	<1080095211.7022.77.camel@iron.libaux.ucsf.edu>
	<1080097814.7028.116.camel@iron.libaux.ucsf.edu>
	<4061137D.755F1E28@stats.uwo.ca>
Message-ID: <1080164894.7020.154.camel@iron.libaux.ucsf.edu>

I have some more good news and some questions.

On Tue, 2004-03-23 at 20:50, Hao Yu wrote:
> Sorry. I have not been able to update Rmpi since the version
> 0.4-4 on R site. 
I don't think any version of Rmpi is on the R site at the moment.

Minor aside: Also, it would be nice if the packages starting with "R"
were consistent about whether it's "R" or "r."  Rmpi but rpvm is a
little dissonant.  Unfortunately, there seems to be no agreement.

> However, I have been using and testing Rmpi
> internally since 0.4-4. Now it is version 0.4-7. See the
> attached package. It requires R 1.8.1 (no more serialize
> package requirement) and works with the newest MPI-LAM
> versions 7. It will configure automatically if a rpm package
> from www.lam-mpi.org is used. I tested it on both Redhat 9
> and Debian. At least it passed the package check without any
> warning on Redhat 9. Now the problem is that it may have
> some problems with the default lam coming with Debian
> system. This is the reason why I hesitate to release it to
> R. 
I'm happy to report the Rmpi 0.4-7 installs and works without a problem
(or any necessary modification) on my Debian mostly testing system (it
does have some unstable stuff, but in particular it has the older
lam/mpi 6.5.8-2.  version 7 has been held in unstable for 60 days
because of some problems on alpha hardware).

Note this is not quite the lam in the current stable Debian distro,
which is at 6.5.6-6.
> 
> I am also working on it to see if #ifdef can be used to work
> MPI 1.2 specs other than LAM-MPI. 
> Hopefully in a couple of months, I am able to submit a
> stable version to R.
Great.  Thanks.  I think it would be well worth making 0.4-7 available
at http://www.stats.uwo.ca/faculty/yu/Rmpi/.  I'll send you, off list,
the hacks I made to get the thing (starting with 0.4-6) to compile with
the MPI 1.2ish mpich.  It didn't run, but at least it did compile and
load at the end.

By the way, I'm in the dark about (r)sprng.  From some of the snow docs,
I thought that was really snow's business.  But your site notes that the
Rmpi packages are withough SPRNG support.  And Tony Rossini's pages (I
think) had a slightly cryptic remark that SPRNG under MPI was a bit
dicey.
> 
> Regards,
> 
> Hao 
> 
> PS: Rmpi should still work without serializing as long as
> native MPI calls are used. The serializing is mainly used to
> help moving an arbitrarily R object around.
I don't completely follow that, since I don't control whether or not
native MPI calls are used, as far as I know.  Do you mean that Rmpi can
be used alone OK, but if you use snow (or, I guess Rmpi calls oriented
toward snow and the transmission of R objects) it will need the
serialize facility (prior to R 1.8)?



From rossini at blindglobe.net  Wed Mar 24 22:59:56 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 24 Mar 2004 13:59:56 -0800
Subject: [R] Status of Rmpi--Good with tweaks
In-Reply-To: <1080164894.7020.154.camel@iron.libaux.ucsf.edu> (Ross Boylan's
	message of "Wed, 24 Mar 2004 13:48:14 -0800")
References: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>
	<1080095211.7022.77.camel@iron.libaux.ucsf.edu>
	<1080097814.7028.116.camel@iron.libaux.ucsf.edu>
	<4061137D.755F1E28@stats.uwo.ca>
	<1080164894.7020.154.camel@iron.libaux.ucsf.edu>
Message-ID: <85ptb13nz7.fsf@servant.blindglobe.net>

Ross Boylan <ross at biostat.ucsf.edu> writes:

> By the way, I'm in the dark about (r)sprng.  From some of the snow docs,
> I thought that was really snow's business.  But your site notes that the
> Rmpi packages are withough SPRNG support.  And Tony Rossini's pages (I
> think) had a slightly cryptic remark that SPRNG under MPI was a bit
> dicey.

SPRNG doesn't need to be invoked via MPI, but can be configured to use
it as the invocation/transport layer. 

It shouldn't matter to SNOW in the least, or rpvm.  I think it
shouldn't matter to Rmpi, but we won't have a cluster to play with for
a month or so.

Luke -- I've seen the gentle hints.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From christof.bigler at colorado.edu  Wed Mar 24 23:49:06 2004
From: christof.bigler at colorado.edu (Christof Bigler)
Date: Wed, 24 Mar 2004 15:49:06 -0700
Subject: [R] Adapting thresholds for predictions of ordinal logistic
	regression
Message-ID: <746583A6-7DE5-11D8-B7AC-000A27D7D440@colorado.edu>

I'm dealing with a classification problem using ordinal logistic  
regression. In the case of binary logistic regression with unequal  
proportions of 0's and 1's, a threshold in the interval [0,1] has to be  
adapted to transform back the predicted probabilities into 0 and 1.  
This can be done quite straightforward using e.g. the Kappa statistics  
as accuracy criterion.

With ordinal logistic regression this seems to be more cumbersome,  
since several thresholds have to be adapted. Here, the Gamma statistics  
could be used as accuracy criterion.
Below is an example showing individual response probabilities when you  
have equal and unequal proportions of four response categories. In the  
case of equal proportions (upper panel), one would reasonably assign  
the category with the highest probability. However, using the highest  
probability for unequal proportions (lower panel) would result in too  
many observations of class 2 being predicted as class 1.

Is there any objective way to select the thresholds for assigning the  
categories in the case of unequal proportions?
Thanks for your help!

Christof


## R code
library(Design)

# Data set with equal proportions
df1 <-  
cbind.data.frame(y=factor(c(rep(1,50),rep(2,50),rep(3,50),rep(4,50))))
df1$x <-  
c(rnorm(50,50,30),rnorm(50,100,30),rnorm(50,150,30),rnorm(50,200,30))

# Data set with unequal proportions
df2 <-  
cbind.data.frame(y=factor(c(rep(1,200),rep(2,50),rep(3,30),rep(4,20))))
df2$x <-  
c(rnorm(200,50,30),rnorm(50,100,30),rnorm(30,150,30),rnorm(20,200,30))

# Fitting ordinal logistic regression models (proportional odds)
f1 <- lrm(y ~ x, data=df1, x=TRUE, y=TRUE)
f2 <- lrm(y ~ x, data=df2, x=TRUE, y=TRUE)

# Individual response probabilities
f.seq <- seq(-50,300)
f1.pred <- predict.lrm(f1,newdata=f.seq,type="fitted.ind")
f2.pred <- predict.lrm(f2,newdata=f.seq,type="fitted.ind")

par(mfrow=c(2,1))

# First plot (equal proportions)
plot(f.seq,  
f1.pred[,1],ylim=c(0,1),type="l",xlab="x",ylab="Pr(Y=j)",xlim=c( 
-50,300))
lines(f.seq,f1.pred[,2],col="red")
lines(f.seq,f1.pred[,3],col="blue")
lines(f.seq,f1.pred[,4],col="green")
abline(v=c(50,100,150,200),lty=3)
par(new=T)
plot(df1$x,df1$y,xlab="",ylab="",axes=F,bty="n",xlim=c(-50,300))
axis(4,at=pretty(range(as.numeric(df1$y))))

# Second plot (unequal proportions)
plot(f.seq,  
f2.pred[,1],ylim=c(0,1),type="l",xlab="x",ylab="Pr(Y=j)",xlim=c( 
-50,300))
lines(f.seq,f2.pred[,2],col="red")
lines(f.seq,f2.pred[,3],col="blue")
lines(f.seq,f2.pred[,4],col="green")
abline(v=c(50,100,150,200),lty=3)
par(new=T)
plot(df2$x,df2$y,xlab="",ylab="",axes=F,bty="n",xlim=c(-50,300))
axis(4,at=pretty(range(as.numeric(df2$y))))



From luke at stat.uiowa.edu  Thu Mar 25 00:07:19 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 24 Mar 2004 17:07:19 -0600 (CST)
Subject: [R] snow documentation comments
In-Reply-To: <1080161133.7025.133.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0403241703190.3564-100000@itasca.stat.uiowa.edu>

On Wed, 24 Mar 2004, Ross Boylan wrote:

> On Wed, 2004-03-24 at 08:03, Luke Tierney wrote:
> > On Tue, 23 Mar 2004, Ross Boylan wrote:
> > 
> > > There are a few points I found unclear or unmentioned in the snow
> > > documentation (mostly I looked at the cluster.html web page).  I thought
> > > I'd mention them here.
> > > 
> > > What is the start up environment for the children?
> > > --------------------------------------------------
> > > My best guess at the answer is in parentheses
> > > Do they inherit shell variables? (no)
> > > Do they inherit variables set in R or other aspects of the R
> > > environment? (no)
> > > What directory does it start in? (the directory you are running in)
> > > What user are you? (same as original)
> > > 
> > > I realize some of these answers might depend on the parallelization
> > > layer you are using, or whether you select the homogenous option.
> > 
> > They do
> I've since realized they may also depend on the exact version of the
> package used (e.g., newer lams apparently have the ability to export
> environment variables) and the way it is invoked.
> 
> Personally, I'd even find a single sentence saying explicitly that "it
> depends" clarifying.  It would probably also be good to discuss the
> behavior of the socket-based communication, since that it supplied by
> the snow package.  Information on the other interfaces would be nice
> too, but probably hazardous to provide accurately.
> 
> I'll not for the record that my answers above are using Rmpi with
> lam/mpi 6.5.8-2 on Debian, running just on my local computer.  And some
> of the answers are speculative!

I'll make a note to add a sentence.

Initial configuration is definitely the harders part about using snow,
especially as it by nature has to depend on aspects of the local
infrastructure.  Working on both the documentation and on some tools
to handle a richer range of scenarios easily is definitely something
we need to work on.

> > 
> > > Number of nodes or number of children?
> > > --------------------------------------
> > > I thought all the counts of nodes (e.g., in makeCluster(5)) were of the
> > > total nodes in the cluster, i.e., children + 1.  However, I did a
> > > makeCluster(2) and I got 2 additional R processes running.  Have I
> > > misunderstood the semantics, or is it essentially an implementation
> > > detail that the master node starts a new R process?
> > 
> > makeCluster(2) creates a cluster of two processes that the master
> > process uses.  So there are a total of three processes.

> So work that is distributed via snow goes to the two processes only?

That is correct--the master just waits intil somethng comes back from
at least one of the worker processes.

luke


-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From phddas at yahoo.com  Thu Mar 25 00:25:25 2004
From: phddas at yahoo.com (Fred J.)
Date: Wed, 24 Mar 2004 15:25:25 -0800 (PST)
Subject: [R] debugging a code
Message-ID: <20040324232525.83092.qmail@web20507.mail.yahoo.com>

Hello
just learned HowTo but R, reminded me with the way
Perl does it but with much less on-line commands, R
"AFAIK" has n, c, Q and where and cann't debug outside
the {}. 

1) is there a more versatile/flexable debugging method
for R?

I have saved 2 functions in an ASCII file "digfun".
"getdata" function calls "squash" function and both
use loops. In another file.R I have

source("digfun")
debug(getdata)
data <- getdata("c:/data/")

Browse[1]> c
Error in if (d[i, "V3"] == d[i + 1, "V3"] && d[i,
"V4"] == d[i + 1, "V4"] &&  : 
	missing value where TRUE/FALSE needed
> traceback()
2: squash(dt1)
1: getdata("c:/data/")
> 
to dubug this I need to know the value of some
variable at this particular loop/sub-fun loop case,
and since the dubuger terminated by showing ">"
prompt, how then I am going to debug this error?

thanks for helping



From andy_liaw at merck.com  Thu Mar 25 01:27:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 19:27:54 -0500
Subject: [R] debugging a code
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A83@usrymx25.merck.com>

Well, you can insert `browser()' right above the offending line, then run
the code again.  It will stop with the `browse>' prompt right before that
line.  You can then check which value(s) give you the problem.

Have you tried Mark Bravington's `debug' package on CRAN?  There's an
article on it in the most recent issue of R News.

Andy

> From: Fred J.
> 
> Hello
> just learned HowTo but R, reminded me with the way
> Perl does it but with much less on-line commands, R
> "AFAIK" has n, c, Q and where and cann't debug outside
> the {}. 
> 
> 1) is there a more versatile/flexable debugging method
> for R?
> 
> I have saved 2 functions in an ASCII file "digfun".
> "getdata" function calls "squash" function and both
> use loops. In another file.R I have
> 
> source("digfun")
> debug(getdata)
> data <- getdata("c:/data/")
> 
> Browse[1]> c
> Error in if (d[i, "V3"] == d[i + 1, "V3"] && d[i,
> "V4"] == d[i + 1, "V4"] &&  : 
> 	missing value where TRUE/FALSE needed
> > traceback()
> 2: squash(dt1)
> 1: getdata("c:/data/")
> > 
> to dubug this I need to know the value of some
> variable at this particular loop/sub-fun loop case,
> and since the dubuger terminated by showing ">"
> prompt, how then I am going to debug this error?
> 
> thanks for helping
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Thu Mar 25 01:34:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 25 Mar 2004 00:34:32 +0000 (UTC)
Subject: [R] debugging a code
References: <20040324232525.83092.qmail@web20507.mail.yahoo.com>
Message-ID: <loom.20040325T013124-839@post.gmane.org>

You also have carriage return as a debug command.  I think
you are looking for:

  ?recover

Fred J. <phddas <at> yahoo.com> writes:

: 
: Hello
: just learned HowTo but R, reminded me with the way
: Perl does it but with much less on-line commands, R
: "AFAIK" has n, c, Q and where and cann't debug outside
: the {}. 
: 
: 1) is there a more versatile/flexable debugging method
: for R?
: 
: I have saved 2 functions in an ASCII file "digfun".
: "getdata" function calls "squash" function and both
: use loops. In another file.R I have
: 
: source("digfun")
: debug(getdata)
: data <- getdata("c:/data/")
: 
: Browse[1]> c
: Error in if (d[i, "V3"] == d[i + 1, "V3"] && d[i,
: "V4"] == d[i + 1, "V4"] &&  : 
: 	missing value where TRUE/FALSE needed
: : traceback()
: 2: squash(dt1)
: 1: getdata("c:/data/")
: : 
: to dubug this I need to know the value of some
: variable at this particular loop/sub-fun loop case,
: and since the dubuger terminated by showing ">"
: prompt, how then I am going to debug this error?
: 
: thanks for helping
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From p.murrell at auckland.ac.nz  Thu Mar 25 01:59:16 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 25 Mar 2004 12:59:16 +1200
Subject: [R] Scaling of font sizes in layout()
References: <40615629.6070602@ph.adfa.edu.au>
Message-ID: <40622EE4.1060404@stat.auckland.ac.nz>

Hi


Pisut Tempatarachoke wrote:
> Hi all,
> 
> In the following example,
> 
> #--------------EXAMPLE------------------
> test <- function(subfigure)
> {
> plot(c(1:10),c(1:10),cex=4)
> text(1,9,subfigure,cex=10)
> }
> m <- matrix(c(1,2,5,5,3,4,5,5),4,2)
> layout(m)
> test("a")
> test("b")
> test("c")
> test("d")
> test("e")
> #---------------------------------------
> 
> Is it possible to have the font (a,b,...,e) and pch sizes (including the 
> axis-label, tick and tick-label sizes) scaled proportionally with the 
> size of each plot when I put multiple plots on the same page?


When you have multiple figures, R tries to think for you and reduces the 
"base" size of text.  You can explicitly control this base size through 
par().  Does the following slight modification of your example do what 
you want?

test <- function(subfigure)
{
plot(c(1:10),c(1:10),cex=4)
text(1,9,subfigure,cex=10)
}
m <- matrix(c(1,2,5,5,3,4,5,5),4,2)
layout(m)
test("a")
test("b")
test("c")
test("d")
par(cex=1)
test("e")

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ross at biostat.ucsf.edu  Thu Mar 25 02:16:11 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 24 Mar 2004 17:16:11 -0800
Subject: [R] http://cran.us.r-project.org/ inaccessible
Message-ID: <1080177371.7027.198.camel@iron.libaux.ucsf.edu>

For the last couple of days when I go to http://cran.us.r-project.org/ I
see only the left-hand margin logo and table of contents.  If I click on
one of the links there, I get a timeout.  Other sites (e.g.,
http://cran.stat.ucla.edu/) work fine for me.

Lacking complete confidence this is a bug*, as well as any better notion
of where to report this, I'm reporting it here.

*I have had problems accessing one other site,
http://www.oasis-open.org/, that is generally accessible.  It has been
suggested the problem is that oasis does not cope with congestion
notifications (ECN) properly, so it's not clear whether this is my
problem or the sites.



From phddas at yahoo.com  Thu Mar 25 02:23:32 2004
From: phddas at yahoo.com (Fred J.)
Date: Wed, 24 Mar 2004 17:23:32 -0800 (PST)
Subject: [R] if block and brackets
Message-ID: <20040325012332.63865.qmail@web20508.mail.yahoo.com>

Hello
the maunal states "When the if statement is not in a
block the else, if present, must appear on the same
line as statement1. Otherwise the new line at the end
of statement1 yields a syntactically complete
statement that is evaluated."
well, what is wrong with this if structure? I am
getting an error on the line where "else" is

thanks

  if (exists("f")){
    dt <- read.csv(file.path(d,f),header=F)#data frame
    builddl(dt,f)
  else
    for (i in dir(d)){
      dt <-
read.csv(file.path("c:/data",i),header=F)#data frame
      an <- sub("([^.]+)(\\..+)","\\1", i))
    builddl(dt,an)
  }
}



From alex_s_42 at yahoo.com  Wed Mar 24 23:03:47 2004
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Wed, 24 Mar 2004 14:03:47 -0800 (PST)
Subject: [R] statistical significance test for cluster agreement
In-Reply-To: <Pine.GSO.3.95q.1040324180554.11217B-100000@sun11.math.uni-hamburg.de>
Message-ID: <20040324220347.78329.qmail@web60003.mail.yahoo.com>

Christian,

I think I understand your point, but I do not
completely agree with you. I also did not describe 
my problem clear enough.

> If you see two
> clusterings on the same
> data, they are identical, if they are 100%
> identical, and if not, then
> not. 

What you are actually saying is that all values of 
Rand index for cluster agreement other then 1 
inidicate that clusters do not agree. I believe
that many people would disagree with this statement.

Let me explain my problem in a little bit more detail.

I have some classified data set. These classes were 
ontained using non-statistical methods. What I'm
trying
to do is run some clustering algorithm and compare
it's results to this known classification.

I think that this is not very different from
calculating mean and comparing it to some known value.

I think that is should be theoretically possible to
use
Rand index as a test statistic. 

Or maybe I'm missing something...



From bates at stat.wisc.edu  Thu Mar 25 02:28:56 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 24 Mar 2004 19:28:56 -0600
Subject: [R] http://cran.us.r-project.org/ inaccessible
In-Reply-To: <1080177371.7027.198.camel@iron.libaux.ucsf.edu>
References: <1080177371.7027.198.camel@iron.libaux.ucsf.edu>
Message-ID: <6ry8ppzpd3.fsf@bates4.stat.wisc.edu>

Ross Boylan <ross at biostat.ucsf.edu> writes:

> For the last couple of days when I go to http://cran.us.r-project.org/ I
> see only the left-hand margin logo and table of contents.  If I click on
> one of the links there, I get a timeout.  Other sites (e.g.,
> http://cran.stat.ucla.edu/) work fine for me.
> 
> Lacking complete confidence this is a bug*, as well as any better notion
> of where to report this, I'm reporting it here.
> 
> *I have had problems accessing one other site,
> http://www.oasis-open.org/, that is generally accessible.  It has been
> suggested the problem is that oasis does not cope with congestion
> notifications (ECN) properly, so it's not clear whether this is my
> problem or the sites.

cran.us.r-project.org is down temporarily.  We're not sure exactly
what happened but it looks like it was compromised.  Please use
another CRAN mirror for the next few days.

That machine is also rsync.r-project.org and cvs.r-project.org.  The
latter isn't very important to those not on the core development team
but it is very important to us and I have been working all afternoon
to bring the archive back up on another machine.  I think I have that
now.  Rsync and CRAN mirror will follow tomorrow.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From jasont at indigoindustrial.co.nz  Thu Mar 25 02:39:24 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 25 Mar 2004 13:39:24 +1200 (NZST)
Subject: [R] if block and brackets
In-Reply-To: <20040325012332.63865.qmail@web20508.mail.yahoo.com>
References: <20040325012332.63865.qmail@web20508.mail.yahoo.com>
Message-ID: <16907.203.9.176.60.1080178764.squirrel@webmail.maxnet.co.nz>

> what is wrong with this if structure? I am
> getting an error on the line where "else" is
>
> thanks
>
>   if (exists("f")){
>     dt <- read.csv(file.path(d,f),header=F)#data frame
>     builddl(dt,f)
>   else

The block started by "if(...){" isn't complete - it needs a matching "}"
before it is.  As such, the "else" is just hanging by itself.  An "else"
with no corresponding "if" is a syntax error.  This would be correct
(UNTESTED)

if (exists("f")){
  dt <- read.csv(file.path(d,f),header=F)#data frame
  builddl(dt,f)
}
else

Cheers
Jason



From kjetil at entelnet.bo  Thu Mar 25 02:39:54 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 24 Mar 2004 21:39:54 -0400
Subject: [R] colors, lines, characters .... documentation
In-Reply-To: <E1B69xG-000F4l-MW@probity.mcc.ac.uk>
Message-ID: <4062002A.21968.C0E2C8@localhost>

On 24 Mar 2004 at 15:05, Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> Very so often when i am plotting something, doing a histogram, or
> whatever i am struggling to find out which are the numbers for
> different colors, palette names, types of lines, symbols, etc. Is
> there any documentation on line with all these numbers / names and the
> associated symbol / color???
> 

I do usually something like:

plot(1:20, rep(1,20), pch=1:20, col=1:20)
plot(1:20, 1:20, type="n")
for (i in 1:20) {
+    lines( c(1,20), c(i,i), lty=i) }


> For example if i am using the command image it uses a palette 
> from red to yellow, with red the lowest value, and yellow the highest
> value. What if i want a reverse palette, with green the lowest value

:

palette(rev(palette()))
palette()
[1] "gray"    "yellow"  "magenta" "cyan"    "blue"    "green3"  "red"    
[8] "black"  


> and yellow middle values and red highest value??? Or much more simple,
> just yellow lowest value and red highest value???

See also the CRAN package RColorBrewer

Kjetil Halvorsen

> 
> Thank you for assistance,
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From lmassis at yahoo.com.br  Thu Mar 25 02:55:51 2004
From: lmassis at yahoo.com.br (Leonard Assis)
Date: Wed, 24 Mar 2004 22:55:51 -0300
Subject: [R] Ordered logit/probit
In-Reply-To: <OF476EAD25.EBDFC9D1-ON42256E61.0054677F-C2256E61.006434AA@eyi.com>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAuza3AGpmKkqXuv4sp2m60sKAAAAQAAAAHRI9k63crUeP+CWGQNXLzgEAAAAA@yahoo.com.br>

Anyone Knows how Shoud I Fit a Mixed Effects Ordered Logistic Model in
R? (lme?) 


[]s
Leonard Assis
Estat?stico - CONFE 7439

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Valentin
Stanescu
Sent: quarta-feira, 24 de mar?o de 2004 15:15
To: R-help at stat.math.ethz.ch
Subject: [R] Ordered logit/probit

Hello everyone
I am trying to fit an ordered probit/logit model for bank rating
prediction.
Besides polr() in MASS package which is not written especially for
this as far as I know, do you know how else I can do this?
I already found the modified polr () version on the


Valentin STANESCU

Enrst and Young
Tel. 402 4000
----------------------------------------------------------
The information contained in this communication is intended solely for
the use of the individual or entity to whom it is addressed and others
authorized to receive it.   It may contain confidential or legally
privileged information.   If you are not the intended recipient you
are hereby notified that any disclosure, copying, distribution or
taking any action in reliance on the contents of this information is
strictly prohibited and may be unlawful. If you have received this
communication in error, please notify us immediately by responding to
this email and then delete it from your system. Ernst & Young is
neither liable for the proper and complete transmission of the
information contained in this communication nor for any delay in its
receipt.

Note: If you have received a delivery failure report, it may be due to
the change in the Ernst & Young e-mail domain from "eyi.com" to
"ey.com".  Could you please make the necessary amendment, if required,
and resend the message.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kjetil at entelnet.bo  Thu Mar 25 03:00:36 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 24 Mar 2004 22:00:36 -0400
Subject: [R] Ordered logit/probit
In-Reply-To: <OF476EAD25.EBDFC9D1-ON42256E61.0054677F-C2256E61.006434AA@eyi.com>
Message-ID: <40620504.6776.D3D4F1@localhost>

On 24 Mar 2004 at 20:14, Valentin Stanescu wrote:

> Hello everyone
> I am trying to fit an ordered probit/logit model for bank rating

The CRAN package MCMCpack has at least ordered probit. 

Kjetil Halvorsen

> prediction.
> Besides polr() in MASS package which is not written especially for
> this as far as I know, do you know how else I can do this? I already
> found the modified polr () version on the
> 
> 
> Valentin STANESCU
> 
> Enrst and Young
> Tel. 402 4000
> ----------------------------------------------------------
> The information contained in this communication is intended solely for
> the use of the individual or entity to whom it is addressed and others
> authorized to receive it.   It may contain confidential or legally
> privileged information.   If you are not the intended recipient you
> are hereby notified that any disclosure, copying, distribution or
> taking any action in reliance on the contents of this information is
> strictly prohibited and may be unlawful. If you have received this
> communication in error, please notify us immediately by responding to
> this email and then delete it from your system. Ernst & Young is
> neither liable for the proper and complete transmission of the
> information contained in this communication nor for any delay in its
> receipt.
> 
> Note: If you have received a delivery failure report, it may be due to
> the change in the Ernst & Young e-mail domain from "eyi.com" to
> "ey.com".  Could you please make the necessary amendment, if required,
> and resend the message.
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Bill.Venables at csiro.au  Thu Mar 25 03:10:20 2004
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Thu, 25 Mar 2004 13:10:20 +1100
Subject: [R] binding vectors or matrix using their names
Message-ID: <B998A44C8986644EA8029CFE6396A9240183BA@exqld2-bne.qld.csiro.au>

Goodness Patrick, this must surely qualify for the obfuscated R competition finals.  I love it!

There are two solutions I can think of with do.call and here they are:

> x <- 1
> x2 <- runif(10)
> x12 <- c("x", "x2")

> do.call("cbind", lapply(x12, as.name))
      x         x2
 [1,] 1 0.99327265
 [2,] 1 0.63260097
 [3,] 1 0.17411170
 [4,] 1 0.54635634
 [5,] 1 0.75603670
 [6,] 1 0.27739270
 [7,] 1 0.32125068
 [8,] 1 0.01326344
 [9,] 1 0.37519602
[10,] 1 0.11133052

> do.call("cbind", lapply(x12, get))
      [,1]       [,2]
 [1,]    1 0.99327265
 [2,]    1 0.63260097
 [3,]    1 0.17411170
 [4,]    1 0.54635634
 [5,]    1 0.75603670
 [6,]    1 0.27739270
 [7,]    1 0.32125068
 [8,]    1 0.01326344
 [9,]    1 0.37519602
[10,]    1 0.11133052
> 

I suspect the first offers a few advantages over the second, (which some other people have implicitly suggested).  The first preserves the names in the result.  Also, if the vectors are large, the second constructs a large language object and then evaluates it to get a large result.  The first uses the names rather than the objects themselves, so at least the language object is small, even if the result is not.

A more serious, philosophical word on Patrick's solution.  It is rarely necessary (in my limited experience, sure) to have to use parse() like this.  Where it provides a quick (kludgy?) solution I often find it a useful exercise to consider alternatives.  They often come out simpler and you nearly always pick up something useful in the process.

Bill V.



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
Sent: Thursday, 25 March 2004 7:02 AM
To: Stephane DRAY
Cc: r-help at stat.math.ethz.ch; Stephane DRAY; Tom Blackwell
Subject: Re: [R] binding vectors or matrix using their names


I think you are looking for the eval-parse-text idiom:

eval(parse(text=paste("cbind(", paste(my.names, collapse=", "), ")")))

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Stephane DRAY wrote:

> Hi Tom,
>
> Your approach did not work,
>
> > do.call("cbind", as.list(my.names))
>      [,1] [,2]
> [1,] "x"  "x2"
>
> but it helps me a lot to find the good one:
>
> do.call("cbind", as.list(parse(text=my.names)))
>
> Thanks,
>
>
> At 14:56 24/03/2004, Tom Blackwell wrote:
>
>> I believe the syntax is
>>
>> result <- do.call(cbind, as.list(my.names))
>>
>> Haven't checked this on your example, though.
>>
>> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>>
>> On Wed, 24 Mar 2004, Stephane DRAY wrote:
>>
>> > Hello list,
>> > I have two vectors x and x2:
>> >
>> > x=runif(10)
>> > x2=runif(10)
>> >
>> > and one vectors with their names :
>> >
>> > my.names=c("x","x2")
>> >
>> > I would like to cbind these two vectors using their names contained
>> in the
>> > vector my.names.
>> > I can create a string with comma
>> > ncomma=paste(my.names,collapse=",")
>> >
>> > and now, I just need a function to transform this string into a
>> adequate
>> > argument for cbind:
>> >
>> > cbind(afunction(ncomma))
>> >
>> > Is there in R a function that can do the job ? If not, how can I do
>> it ??
>> >
>> > Thanks in advance,
>> > Sincerely.
>> >
>> >
>> > St?phane DRAY
>> > 
>> ---------------------------------------------------------------------
>> -----------------------------
>>
>> >
>> > D?partement des Sciences Biologiques
>> > Universit? de Montr?al, C.P. 6128, succursale centre-ville 
>> > Montr?al, Qu?bec H3C 3J7, Canada
>> >
>> > Tel : 514 343 6111 poste 1233
>> > E-mail : stephane.dray at umontreal.ca
>> > 
>> ---------------------------------------------------------------------
>> -----------------------------
>>
>> >
>> > Web                                          
>> http://www.steph280.freesurf.fr/
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list 
>> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
> St?phane DRAY
> ----------------------------------------------------------------------
> ----------------------------
>
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville Montr?al, 
> Qu?bec H3C 3J7, Canada
>
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> ----------------------------------------------------------------------
> ----------------------------
>
> Web                                          
> http://www.steph280.freesurf.fr/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phddas at yahoo.com  Thu Mar 25 03:36:39 2004
From: phddas at yahoo.com (Fred J.)
Date: Wed, 24 Mar 2004 18:36:39 -0800 (PST)
Subject: [R] if exists with regex return
Message-ID: <20040325023639.24270.qmail@web20510.mail.yahoo.com>

Hello
I am trying for an hour now,  

> p <- "c:/data/"
or 
> p <- "c:/data/abc.hig"
	
d <- sub("(.+/.+?/)(.+)","\\1",p,perl=TRUE)
f <- sub("(.+/.+?/)(.+)?","\\2",p,perl=TRUE)

if (exists("f")){ #why this gives TRUE no mater what?
do this with d and f
} else {
do that with d
}


thanks



From andy_liaw at merck.com  Thu Mar 25 03:51:51 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 21:51:51 -0500
Subject: [R] if exists with regex return
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A89@usrymx25.merck.com>

You assigned `f' whatever the result of
sub("(.+/.+?/)(.+)?","\\2",p,perl=TRUE) is.  The only way for `f' not to
exist after that assignment (that I can think of anyway) is if there was an
error raised in the sub() call.  Otherwise `f' will always exist!  You
should simply check what value `f' has.  My guess is that you really want
something like:

  if (f == "") ...

or maybe:

  if (nchar(f) == 0) ...

Andy

> From: Fred J.
> 
> Hello
> I am trying for an hour now,  
> 
> > p <- "c:/data/"
> or 
> > p <- "c:/data/abc.hig"
> 	
> d <- sub("(.+/.+?/)(.+)","\\1",p,perl=TRUE)
> f <- sub("(.+/.+?/)(.+)?","\\2",p,perl=TRUE)
> 
> if (exists("f")){ #why this gives TRUE no mater what?
> do this with d and f
> } else {
> do that with d
> }
> 
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Thu Mar 25 04:00:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Mar 2004 22:00:45 -0500
Subject: [R] statistical significance test for cluster agreement
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A8A@usrymx25.merck.com>

> From: Alexander Sirotkin [at Yahoo] [mailto:alex_s_42 at yahoo.com] 
> 
> Christian,
> 
> I think I understand your point, but I do not
> completely agree with you. I also did not describe 
> my problem clear enough.
> 
> > If you see two
> > clusterings on the same
> > data, they are identical, if they are 100%
> > identical, and if not, then
> > not. 
> 
> What you are actually saying is that all values of 
> Rand index for cluster agreement other then 1 
> inidicate that clusters do not agree. I believe
> that many people would disagree with this statement.
> 
> Let me explain my problem in a little bit more detail.
> 
> I have some classified data set. These classes were 
> ontained using non-statistical methods. What I'm
> trying
> to do is run some clustering algorithm and compare
> it's results to this known classification.
> 
> I think that this is not very different from
> calculating mean and comparing it to some known value.

AFAICS they are most definitely not the same.  The hypotheses in statistical
tests are about `true', unknown, population mean, not the sample mean
observed in the data.  What exactly would be the hypotheses you intend to
test?  If you are testing whether the clustering algorithm produces
something that disagree with the non-statistical classification, then one
disagreement would have settled it, no?  Before you think about what
statistic to use, do try to figure out how you would write the null and
alternative hypotheses, mathematically.

Andy

 
> I think that is should be theoretically possible to
> use
> Rand index as a test statistic. 
> 
> Or maybe I'm missing something...
> 
> __________________________________
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From nali at umn.edu  Thu Mar 25 07:16:00 2004
From: nali at umn.edu (Na Li)
Date: Thu, 25 Mar 2004 00:16:00 -0600
Subject: [R] Status of Rmpi--Good with tweaks
In-Reply-To: <1080164894.7020.154.camel@iron.libaux.ucsf.edu> (Ross Boylan's
	message of "Wed, 24 Mar 2004 13:48:14 -0800")
References: <Pine.LNX.4.44.0403231925530.11813-100000@itasca2.stat.uiowa.edu>
	<1080095211.7022.77.camel@iron.libaux.ucsf.edu>
	<1080097814.7028.116.camel@iron.libaux.ucsf.edu>
	<4061137D.755F1E28@stats.uwo.ca>
	<1080164894.7020.154.camel@iron.libaux.ucsf.edu>
Message-ID: <a0ad25wixr.fsf@orca.local>

On 24 Mar 2004, Ross Boylan outgrape:

> By the way, I'm in the dark about (r)sprng.  From some of the snow docs,
> I thought that was really snow's business.  But your site notes that the
> Rmpi packages are withough SPRNG support.  And Tony Rossini's pages (I
> think) had a slightly cryptic remark that SPRNG under MPI was a bit
> dicey.

rsprng works like this: each process calls the function init.sprng, with
three pieces of information: 1) the total number of processes; 2) the rank
of current process; 3) a seed that is the same for all processes.

An alternative is that the master generates say ten streams, and send one
stream to each slave (and keep track of who gets what, to ensure
reproducibility).

Only very little (and simple) inter-process communication is required for
SPRNG. The communication layer is orthogonal of SPRNG itself.  rsprng can
be used with Rmpi just fine.  (SPRNG can be used in serial code as well.)

If you look at SPRNG source, it has some sort of MPI support, which is
disabled when I compile SPRNG.  See this note:

http://www.biostat.umn.edu/~nali/RsprngNotes.php

Cheers,

Michael



From ripley at stats.ox.ac.uk  Thu Mar 25 07:25:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Mar 2004 06:25:00 +0000 (GMT)
Subject: [R] debugging a code
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A83@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0403250619340.13289-100000@gannet.stats>

Using debugger() for post-mortem debugging is the best way to chase down 
the immediate cause of an error, which seems to be the issue here.

If you use debug() (which is not a `dubuger') you need to single-step
(with n), and look at values just before the line in which the error
occurs.

There are a lot of good books out there and, `Fred J.', it looks to be 
time you started reading them as you are asking a lot of questions they do 
answer.


On Wed, 24 Mar 2004, Liaw, Andy wrote:

> Well, you can insert `browser()' right above the offending line, then run
> the code again.  It will stop with the `browse>' prompt right before that
> line.  You can then check which value(s) give you the problem.
> 
> Have you tried Mark Bravington's `debug' package on CRAN?  There's an
> article on it in the most recent issue of R News.
> 
> Andy
> 
> > From: Fred J.
> > 
> > Hello
> > just learned HowTo but R, reminded me with the way
> > Perl does it but with much less on-line commands, R
> > "AFAIK" has n, c, Q and where and cann't debug outside
> > the {}. 
> > 
> > 1) is there a more versatile/flexable debugging method
> > for R?
> > 
> > I have saved 2 functions in an ASCII file "digfun".
> > "getdata" function calls "squash" function and both
> > use loops. In another file.R I have
> > 
> > source("digfun")
> > debug(getdata)
> > data <- getdata("c:/data/")
> > 
> > Browse[1]> c
> > Error in if (d[i, "V3"] == d[i + 1, "V3"] && d[i,
> > "V4"] == d[i + 1, "V4"] &&  : 
> > 	missing value where TRUE/FALSE needed
> > > traceback()
> > 2: squash(dt1)
> > 1: getdata("c:/data/")
> > > 
> > to dubug this I need to know the value of some
> > variable at this particular loop/sub-fun loop case,
> > and since the dubuger terminated by showing ">"
> > prompt, how then I am going to debug this error?

You are not even in the `debugger', sic.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Mar 25 07:28:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Mar 2004 06:28:09 +0000 (GMT)
Subject: [R] if block and brackets
In-Reply-To: <20040325012332.63865.qmail@web20508.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403250626560.13289-100000@gannet.stats>

On Wed, 24 Mar 2004, Fred J. wrote:

> Hello
> the maunal states "When the if statement is not in a
> block the else, if present, must appear on the same
> line as statement1. Otherwise the new line at the end
> of statement1 yields a syntactically complete
> statement that is evaluated."
> well, what is wrong with this if structure? I am
> getting an error on the line where "else" is

The else is inside a { } block.

If you use a good editor and proper indentation such errors will be 
obvious to you.

> 
> thanks
> 
>   if (exists("f")){
>     dt <- read.csv(file.path(d,f),header=F)#data frame
>     builddl(dt,f)
>   else
>     for (i in dir(d)){
>       dt <-
> read.csv(file.path("c:/data",i),header=F)#data frame
>       an <- sub("([^.]+)(\\..+)","\\1", i))
>     builddl(dt,an)
>   }
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From olefc at daimi.au.dk  Thu Mar 25 07:33:50 2004
From: olefc at daimi.au.dk (Ole F. Christensen)
Date: Thu, 25 Mar 2004 07:33:50 +0100
Subject: [R] geoR - help for bayesian modelling
Message-ID: <40627D4E.7040801@daimi.au.dk>

Dear Monica

Guess the reason for the problem you are seeing is that you are 
requiring simulations from the predictive distribution. geoR is doing 
this simulation in a joint step, simulating from the joint predictive 
distriubtion [as far as I know some other geostatistical software 
packages are doing such simulation in a sequential way, where a point 
the grid is added at a time]. For a relatively large grid the covariance 
matrix needed for this joint simulation is large [in your example a 
matrix of size 7500 by 7500].

Possible solutions :

* Do you really want simulations from the multivariate predictive 
distribution ? What do you want to do with them ?
Most summaries you would want of the predictive distribution are 
summaries of the univariate distributions at individual locations.

* Do you really need to predict in a 150 by 250 grid ? If possible, then 
reduce the size of your grid.

The error you are seeing is related to the cholesky factorisation of the 
covariance matrix, which is needed to do the joint simulations. If you 
do not require these simulations, the error will disappear.
As you write, the error is probably due to some locations being very 
close to data locations. As I remember there is an internal handling in 
the package of prediction locations close to data locations, but your 
locations are not sufficiently near to the data locations to be handled 
by this. Maybe you should change the prediction coordinates in question 
such that they actually do coincide with the data locations.

Hope this is helpful

Cheers

Ole

***

Hi,

I am trying to do a bayesian prediction for soil pollution data above 
a certain threshold, using geoR. 

Everything is working fine until i am doing the krig.bayes. I tried to 
do the prediction on a grid 67 by 113 cells and my computer is 
freezing to death. At larger numbers of cells it tells me after a while 
that it reaches the max. memory of 511 Mb. My computer has only 
512 Mb of RAM. What RAM capacity should i look for to do a 150 
x 250 cell grid???

If i want to do the prediction on my initial data locations (well, 
actually the prediction points are shifted 1 m in X and respectively 
Y direction, so the raw data coordinates don't coincide with the 
prediction coordinates) i am getting the following error using the 
command:

zn.bayes <- krige.bayes(zn.gdata, loc = xy, model = 
model.control(cov.model = "exponential", lambda = 0), prior = 
prior.control(phi.prior ="exponential", phi = 89.1894), 
output=output.control(n.predictive=2, mean.var = TRUE, quantile = 
c(0.025,0.25, 0.5, 0.75, 0.975), threshold = c(300)))

Error in cond.sim(env.loc = base.env, env.iter = iter.env, 
loc.coincide = get("loc.coincide",  : 
        chol: matrix not pos def, diag[13]= -1.279220e-018

I will really appreciate any suggestion you may have.

Thank you so much,

Monica


-- 
Ole F. Christensen
Center for Bioinformatik
Aarhus Universitet
Ny Munkegade, Bygning 540
8000 Aarhus C



From christian.hoffmann at wsl.ch  Thu Mar 25 08:44:56 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Thu, 25 Mar 2004 08:44:56 +0100
Subject: [R] First Variable in lm
Message-ID: <40628DF8.3090205@wsl.ch>

 > I want to take the first variable (column) of a data frame and regress
 > it against all other variables.
 >
 > bla <- function (dat) {
 >    reg <- lm(whateverthefirstofthevariablenamesis ~., data=dat)
 >    return(reg)
 > }
 >
 >

Thanks to all who answered my question:

Prof. Brian Ripley:
-------------------
bla <- function (dat)
   eval(substitute(lm(foo ~., data=dat), list(foo=as.name(names(dat)[1]))))

which has the advantage of embedding a clean value of $call.

andy_liaw at merck.com:
--------------------
lm(formula = dat)


rolf at math.unb.ca:
-----------------
for(j in 1:ncol(dat)) {
	fff <- as.formula(paste(names(dat)[j],"~",
                           paste(names(dat)[-j],collapse="+")))
	nm <- paste("rslt",j,sep=".")
	assign(nm,lm(fff,data=dat))
}


ggrothendieck at myway.com
-----------------------
data(longley)
lm( longley[,7] ~. , data = longley[,-7] )

You cannot call data() inside a function:
   data(dat)
   reg <- lm(dat[,1] ~ dat[,-1])
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
	invalid variable type
In addition: Warning message:
Data set 'dat' not found in: data(dat)


Regards
Christian
-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From pburns at pburns.seanet.com  Thu Mar 25 09:18:17 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 25 Mar 2004 08:18:17 +0000
Subject: [R] binding vectors or matrix using their names
References: <B998A44C8986644EA8029CFE6396A9240183BA@exqld2-bne.qld.csiro.au>
Message-ID: <406295C9.9070002@pburns.seanet.com>

Not at all -- I can obfuscate much better.  


Bill.Venables at csiro.au wrote:

>Goodness Patrick, this must surely qualify for the obfuscated R competition finals.  I love it!
>
>  
>
[snip]

>
>A more serious, philosophical word on Patrick's solution.  It is rarely necessary (in my limited experience, sure) to have to use parse() like this.  Where it provides a quick (kludgy?) solution I often find it a useful exercise to consider alternatives.  They often come out simpler and you nearly always pick up something useful in the process.
>
>Bill V.
>
>
>  
>
[snip]

>
>I think you are looking for the eval-parse-text idiom:
>
>eval(parse(text=paste("cbind(", paste(my.names, collapse=", "), ")")))
>
>Patrick Burns
>
>  
>
[snip]

>>>      
>>>
>>>>Hello list,
>>>>I have two vectors x and x2:
>>>>
>>>>x=runif(10)
>>>>x2=runif(10)
>>>>
>>>>and one vectors with their names :
>>>>
>>>>my.names=c("x","x2")
>>>>
>>>>I would like to cbind these two vectors using their names contained
>>>>        
>>>>
>>>in the
>>>      
>>>
>>>>vector my.names.
>>>>        
>>>>
[snip]



From p.dalgaard at biostat.ku.dk  Thu Mar 25 09:39:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Mar 2004 09:39:29 +0100
Subject: [R] binding vectors or matrix using their names
In-Reply-To: <B998A44C8986644EA8029CFE6396A9240183BA@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9240183BA@exqld2-bne.qld.csiro.au>
Message-ID: <x2hdwd9v7i.fsf@biostat.ku.dk>

<Bill.Venables at csiro.au> writes:

> A more serious, philosophical word on Patrick's solution. It is
> rarely necessary (in my limited experience, sure) to have to use
> parse() like this. Where it provides a quick (kludgy?) solution I
> often find it a useful exercise to consider alternatives. They often
> come out simpler and you nearly always pick up something useful in
> the process.

Agreed. And the kludges (yes, they are) are prone to unexpected
behaviour if you try to use them generally. R's modelling code has
quite a few (not that I've counted them) constructs of the type

  formula <- parse(paste("~", paste(terms, collapse="+")))

which breaks down horribly if one the terms has a nonstandard name
like `age in years`. Somewhere on my horizon is the task of hunting
down all of these and replace them with something independent of
textual representation.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From phddas at yahoo.com  Thu Mar 25 10:15:52 2004
From: phddas at yahoo.com (Fred J.)
Date: Thu, 25 Mar 2004 01:15:52 -0800 (PST)
Subject: [R] mlocal/mtrace inside a loop
Message-ID: <20040325091552.76458.qmail@web20503.mail.yahoo.com>

Hello
I need some help in figuring Bravingtons debugger
out.
Ok
I have 2 functions, fun1 and fun2 saved in a ASCII
file say filename is funs.
Fun1 has a loop which calls fun2, fun2 has a loop
which fails and I need to find out the value of the
variables of the fun2 and fun1 loops at the specific
iteration that fails. Both fun1 and fun2 loops will
iterate thousands of times so line by line debug is
not practical.
According to what I understood from > ?mlocal and the
Vol. 3/3, December 2003 R-news article "Debugging
Without (Too Many) Tears" by Mark Bravington. P.32
first column last paragraph, I did. "Which needs some
fixing and direction - thanks"

> source("funs")
> library(debug)
> debug(fun1)
data <- fun1(some.arguments")
 
in the file funs 
Was ....
fun1 <- function(arg1){ some looping code with calls
to fun2 }
fun2 <- function(arg2) {some more looping code}

Is now....to confirm to what I read
Fun1 <- function(arg1, nlocal=sys.parent()) 
mlocal(some more looping code)

I think I am implementing this package wrong and need
some help.

Thanks



From phddas at yahoo.com  Thu Mar 25 10:24:47 2004
From: phddas at yahoo.com (Fred J.)
Date: Thu, 25 Mar 2004 01:24:47 -0800 (PST)
Subject: [R] book recommendation
Message-ID: <20040325092447.59954.qmail@web20507.mail.yahoo.com>

Hello
right, I need a good book to help me with R.
background, programming in matlab, perl, visualBasics.
os windows and Linux. some good books to match the
need would be appreciated.

Thanks



From k.wang at auckland.ac.nz  Thu Mar 25 10:31:07 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 25 Mar 2004 21:31:07 +1200
Subject: [R] book recommendation
In-Reply-To: <20040325092447.59954.qmail@web20507.mail.yahoo.com>
Message-ID: <000001c4124b$e69d1d40$6633d882@stat.auckland.ac.nz>

Hi,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fred J.
> Sent: Thursday, March 25, 2004 9:25 PM
> To: r help
> Subject: [R] book recommendation
> 
> 
> Hello
> right, I need a good book to help me with R.
> background, programming in matlab, perl, visualBasics.
> os windows and Linux. some good books to match the
> need would be appreciated.

Perhaps http://www.r-project.org/doc/bib/R-publications.html would be a good
place to start looking?

You also need to be more specific.  It's good that you have quite a few
programming skills but what is it you want with R?  Do you want to write
functions/packages and contribute to the R community (in which case
Chambers's Programming with Data, and VR's S Programming would be good
choices)?  Or do you just want to "use" R as a statistical application?  In
which case there are several, VR's MASS4 for example.

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
Statistics Workshops Co-ordinator
Student Learning Centre
University of Auckland
New Zealand



From ripley at stats.ox.ac.uk  Thu Mar 25 10:31:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Mar 2004 09:31:48 +0000 (GMT)
Subject: [R] book recommendation
In-Reply-To: <20040325092447.59954.qmail@web20507.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403250929460.14066-100000@gannet.stats>

I think you need to learn to use an FAQ: R has one covering books about R.
The rest of your questions should be asked in a more appropriate place: 
the answers really depend on your level of knowledge.

On Thu, 25 Mar 2004, Fred J. wrote:

> right, I need a good book to help me with R.
> background, programming in matlab, perl, visualBasics.
> os windows and Linux. some good books to match the
> need would be appreciated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Mar 25 10:46:31 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 25 Mar 2004 10:46:31 +0100
Subject: [R] R/ESS/Bioc* mailing lists: 2 hour time out -- 6 hours from now
Message-ID: <16482.43639.266058.253129@gargle.gargle.HOWL>

This is (hopefully) a  "once in ten years" event,
happening today, from 17--19 CET
	  (= 16--18 UTC = 11--13 US Eastern == 8--10 US Pacific Time)

The main `vault' (security, air condition,..) where our mail
server hypatia is located will undergo a major `network
re-connection' event that will affect a considerable part of ETH
-- including all the mailing lists @stat.math.ethz.ch.

For you, it should only look as one very long mail delay
(i.e. no message bouncing should occur AFAIK).

Apart from that, I hope many of you have found that the mail
delays, particularly on R-help, have become considerably smaller
thanks to the new hardware (and software to less extent) the
mail server is running on.

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From lmassis at yahoo.com.br  Thu Mar 25 12:55:11 2004
From: lmassis at yahoo.com.br (Leonard Assis)
Date: Thu, 25 Mar 2004 08:55:11 -0300
Subject: [R] Ordered Logistic Mixed Model
In-Reply-To: <6.0.3.0.0.20040324133416.04c853c8@10.0.10.66>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAuza3AGpmKkqXuv4sp2m60sKAAAAQAAAAPfrKn05+/UiBcIXGVeydtAEAAAAA@yahoo.com.br>

 
What is the best solution to fit an Ordered Logistic (Or Probit) Mixed
Model in R?

[]s
Leonard Assis
Estat?stico - CONFE 7439



From ggrothendieck at myway.com  Thu Mar 25 13:12:26 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 25 Mar 2004 12:12:26 +0000 (UTC)
Subject: [R] Calling data within a function (was First Variable in lm)
References: <40628DF8.3090205@wsl.ch>
Message-ID: <loom.20040325T130437-515@post.gmane.org>

Christian Hoffmann <christian.hoffmann <at> wsl.ch> writes:

> ggrothendieck <at> myway.com
> -----------------------
> data(longley)
> lm( longley[,7] ~. , data = longley[,-7] )
> 
> You cannot call data() inside a function:

To call data() within a function:

f <- function() {
       data(longley, envir = environment())
       lm( longley[,7] ~. , data = longley[,-7] )
}



From rolf at math.unb.ca  Thu Mar 25 14:11:38 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 25 Mar 2004 09:11:38 -0400 (AST)
Subject: [R] binding vectors or matrix using their names
Message-ID: <200403251311.i2PDBcPj025202@erdos.math.unb.ca>


Well, I ***liked*** Patrick's approach.  Why?  Because it's basically
straightforward.  You put together a character string by pasting
together the names of the objects you want to do things to, and the
things (operations) you want to do to them.  Then you get R to
``execute'' this string just as if you had typed it in to the
keyboard.  This seems an obvious and sensible approach to the problem
and is completely generalizable.

There is no obfuscation (making things obscure) involved.

The solutions involving do.call() are much more ***elegant*** and
doubtless far less prone to pitfalls.  However they are far less
obvious and thereby much more obscure, so it is the do.call()
solutions that deserve the ``obfuscation'' descriptor if anything
does.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From Rau at demogr.mpg.de  Thu Mar 25 14:17:54 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 25 Mar 2004 14:17:54 +0100
Subject: [R] Error in 'legend' help?
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A07CA@hermes.demogr.mpg.de>

Dear all,

maybe I have misunderstood something but to me it seems like a minor error
in the help for
?legend
for the argument 'bg'. There it says:

      bg: the background color for the legend box.  (Note that this is
          only used if 'bty = "n"'.)

I think, however, that it should be changed to:

      bg: the background color for the legend box.  (Note that this is
          only used if 'bty = "o"'.)

Here is some example code. And only in the upper panel we can see a legend
with a blue background.

par(mfrow=c(2,1))
plot(x=1:10, y=1:10)
legend(x=5,y=5, legend="Legend-Text", bty="o", bg="blue")

plot(x=1:10, y=1:10)
legend(x=5,y=5, legend="Legend-Text", bty="n", bg="blue")

My R version is:
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    8.1            
year     2003           
month    11             
day      21             
language R       

Best,
Roland



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From rdlandes at iastate.edu  Thu Mar 25 14:23:06 2004
From: rdlandes at iastate.edu (rdlandes@iastate.edu)
Date: Thu, 25 Mar 2004 07:23:06 -0600 (CST)
Subject: [R] nlme Crossed Random Effects
Message-ID: <62372521044840@webmail.iastate.edu>

Dear R users:

Is it possible to obtain crossed random effects in the nlme function?  And if so, how?

What follows is an example that pertains.

The data set is from a (hypothetical) calibration experiment.

There were two devices that were simultaneously calibrated using a reference instrument.  
Devices return Ys.
The reference instrument returns Xs.
"eta" is an unseen measurement error in X.  etas that share the same value are the same.

> DATA1
   DEVICE            Y     X eta
1       1 -0.831360195 -0.50   1
2       1 -0.635822615 -0.25   2
3       1 -0.311124100  0.00   3
4       1 -0.200513079  0.25   4
5       1  0.088603957  0.50   5
6       1 -0.843568018 -0.50   6
7       1 -0.547275291 -0.25   7
8       1 -0.201478360  0.00   8
9       1 -0.168931992  0.25   9
10      1 -0.003928236  0.50  10
11      2  0.073571637 -0.50   1
12      2  0.445177509 -0.25   2
13      2  0.661823378  0.00   3
14      2  0.860400764  0.25   4
15      2  0.974066904  0.50   5
16      2  0.114521301 -0.50   6
17      2  0.426822014 -0.25   7
18      2  0.695163094  0.00   8
19      2  0.975051813  0.25   9
20      2  1.187508840  0.50  10


I would like to fit a random coefficients regression of the form:

Y[i,j] = BETA0 + B0[i] + (BETA1 + B1[i])*(X[j] - eta[j]) + e[i,j]

Assume
(B0 B1)[i] ~ iid MVN ( (0,0), (sig.00, sig.01, sig.10, sig.11))
eta[j] ~ iid N(0, v.eta)
epi[i,j] ~ iid N(0, v.epi)
with these being mutual independent.

This is a form of simple linear regression with measurement error in the Xs.

A random coefficients regression of Y on X can be done with lme().
e.g.
fm1.Y.X<-lme( fixed= Y ~ X, data=DATA1, random= ~ X|DEVICE)


I am hoping that nlme() will allow me to perform a random coefficients regression of Y on
(X-
eta). It should be possible if there is a way to get nlme to handle the crossed random
effects.

But I do not know how to specify
(1) (X-eta) as a covariate for the fixed portion of the nlme function
(2) the random effects.

After spending several hours reading the documentation, Pinheiro & Bates, and the R-help 
archives, my best guess is as follows:

DATA2=cbind(rep(1,20),DATA1)
names(DATA2)[1]="Grp"
DATA2=groupedData(Y~1|Grp, data=DATA2)

fm2.Y = nlme(model=Y~B0+B1*X-B1*eta, fixed=B0+B1~1, data=DATA1,
random=list(Grp=pdBlocked(pdSymm
(~ B0+B1),pdIdent(~ eta - 1))), start=c(0,1))

The following error is returned.

Error in pdConstruct.pdBlocked(object, form = form, nam = nam, data = data,  : 
        "form" must be a list

Note:  Here B0 and B1 correspond to BETA0 and BETA1 described above.

Any advice will be very much appreciated.

Reid



From MSchwartz at MedAnalytics.com  Thu Mar 25 14:33:50 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 25 Mar 2004 07:33:50 -0600
Subject: [R] Error in 'legend' help?
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D81030A07CA@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A07CA@hermes.demogr.mpg.de>
Message-ID: <1080221630.6983.63.camel@localhost.localdomain>

On Thu, 2004-03-25 at 07:17, Rau, Roland wrote:
> Dear all,
> 
> maybe I have misunderstood something but to me it seems like a minor error
> in the help for
> ?legend
> for the argument 'bg'. There it says:
> 
>       bg: the background color for the legend box.  (Note that this is
>           only used if 'bty = "n"'.)
> 
> I think, however, that it should be changed to:
> 
>       bg: the background color for the legend box.  (Note that this is
>           only used if 'bty = "o"'.)
> 
> Here is some example code. And only in the upper panel we can see a legend
> with a blue background.
> 
> par(mfrow=c(2,1))
> plot(x=1:10, y=1:10)
> legend(x=5,y=5, legend="Legend-Text", bty="o", bg="blue")
> 
> plot(x=1:10, y=1:10)
> legend(x=5,y=5, legend="Legend-Text", bty="n", bg="blue")
> 
> My R version is:
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R       
> 
> Best,
> Roland


It is a typo. The relevant code in legend() is:

    if (plot && bty != "n") {
        if (trace) 
            catn("  rect2(", left, ",", top, ", w=", w, ", h=", 
                h, ", ...)", sep = "")
        rect2(left, top, dx = w, dy = h, col = bg, density = NULL)
    }


So the help _should_ read:

   bg: the background color for the legend box.  (Note that this is
          only used if 'bty != "n"'.)

HTH,

Marc Schwartz



From dvumani at hotmail.com  Thu Mar 25 14:33:58 2004
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Thu, 25 Mar 2004 13:33:58 +0000
Subject: [R] subsetting based on vector
Message-ID: <BAY16-F66i3QIwemiG900016fc2@hotmail.com>

Dear R users;

I am trying to write a small program which reads in a data set, and selects 
observations from certain years before the analysis. I have a problem 
including the selection criteria in the header of the program.

Here is the problem;

dataFIT<-function(MODEL, MARGINS, yearConsidered){
    library(foreign
    CovPaper<-read.spss("C:/Data/data.sav")
    NewData <- list(CovPaper$"YEAR"[CovPaper$"YEAR" == yearConsidered],
                          CovPaper$"YEAR"[CovPaper$"SEX" == yearConsidered],
                      #### and so on #####
   #fit model to data #
}

When I use one year there is no problem, but I would like some data sets to 
span over years and I am not sure how to do this without having to change 
the body of my fitting program.

I tried searching the R-list but to no avail.

Thanking you in advance.


Vumani

_________________________________________________________________



From theis at statistik.uni-dortmund.de  Thu Mar 25 16:16:58 2004
From: theis at statistik.uni-dortmund.de (Winfried Theis)
Date: Thu, 25 Mar 2004 16:16:58 +0100
Subject: [R] subsetting based on vector
In-Reply-To: <BAY16-F66i3QIwemiG900016fc2@hotmail.com>
References: <BAY16-F66i3QIwemiG900016fc2@hotmail.com>
Message-ID: <1080227818.5726.24.camel@malepartus.statistik.uni-dortmund.de>

Hi!

Well, you could try a subsetting rule like " year %in% yearsconsidered
".

Hope this helps,

Winfried
On Thu, 2004-03-25 at 14:33, Vumani Dlamini wrote:
> Dear R users;
> 
> I am trying to write a small program which reads in a data set, and selects 
> observations from certain years before the analysis. I have a problem 
> including the selection criteria in the header of the program.
> 
> Here is the problem;
> 
> dataFIT<-function(MODEL, MARGINS, yearConsidered){
>     library(foreign
>     CovPaper<-read.spss("C:/Data/data.sav")
>     NewData <- list(CovPaper$"YEAR"[CovPaper$"YEAR" == yearConsidered],
>                           CovPaper$"YEAR"[CovPaper$"SEX" == yearConsidered],
>                       #### and so on #####
>    #fit model to data #
> }
> 
> When I use one year there is no problem, but I would like some data sets to 
> span over years and I am not sure how to do this without having to change 
> the body of my fitting program.
> 
> I tried searching the R-list but to no avail.
> 
> Thanking you in advance.
> 
> 
> Vumani
> 
> _________________________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
-----------------------------------------------------------------
Dr. Dipl.-Math. Winfried Theis, SFB 475, Projekt C5,
Universit?t Dortmund, 44221 Dortmund
e-mail: theis at statistik.uni-dortmund.de
Tel.: +49/231/755-5903 FAX: +49/231/755-4387



From dvumani at hotmail.com  Thu Mar 25 15:10:15 2004
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Thu, 25 Mar 2004 14:10:15 +0000
Subject: [R] subsetting based on vector
Message-ID: <BAY16-F85eEmP9CDBHy0001702d@hotmail.com>

Thanks man. It did the trick.

Vumani


>From: Winfried Theis <theis at statistik.uni-dortmund.de>
>To: Vumani Dlamini <dvumani at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] subsetting based on vector
>Date: Thu, 25 Mar 2004 16:16:58 +0100
>
>Hi!
>
>Well, you could try a subsetting rule like " year %in% yearsconsidered
>".
>
>Hope this helps,
>
>Winfried
>On Thu, 2004-03-25 at 14:33, Vumani Dlamini wrote:
> > Dear R users;
> >
> > I am trying to write a small program which reads in a data set, and 
>selects
> > observations from certain years before the analysis. I have a problem
> > including the selection criteria in the header of the program.
> >
> > Here is the problem;
> >
> > dataFIT<-function(MODEL, MARGINS, yearConsidered){
> >     library(foreign
> >     CovPaper<-read.spss("C:/Data/data.sav")
> >     NewData <- list(CovPaper$"YEAR"[CovPaper$"YEAR" == yearConsidered],
> >                           CovPaper$"YEAR"[CovPaper$"SEX" == 
>yearConsidered],
> >                       #### and so on #####
> >    #fit model to data #
> > }
> >
> > When I use one year there is no problem, but I would like some data sets 
>to
> > span over years and I am not sure how to do this without having to 
>change
> > the body of my fitting program.
> >
> > I tried searching the R-list but to no avail.
> >
> > Thanking you in advance.
> >
> >
> > Vumani
> >
> > _________________________________________________________________
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>--
>-----------------------------------------------------------------
>Dr. Dipl.-Math. Winfried Theis, SFB 475, Projekt C5,
>Universität Dortmund, 44221 Dortmund
>e-mail: theis at statistik.uni-dortmund.de
>Tel.: +49/231/755-5903 FAX: +49/231/755-4387
>



From jbboudenne at cdcixis-cm.com  Thu Mar 25 15:49:18 2004
From: jbboudenne at cdcixis-cm.com (boudenne, jb)
Date: Thu, 25 Mar 2004 15:49:18 +0100
Subject: [R] (no subject)
Message-ID: <4AD617C93696EB47A84E53189B57870315537D@msbac.cm.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040325/8edb1f6f/attachment.pl

From dray at biomserv.univ-lyon1.fr  Thu Mar 25 15:56:56 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Thu, 25 Mar 2004 09:56:56 -0500
Subject: [R] Problems with environments, I think..
Message-ID: <5.2.1.1.0.20040325094427.00b3c9b0@magellan.umontreal.ca>

Hello,
I have a general function which I am used for calculations. I call this 
function through different functions corresponding to different particular 
cases of this method. The particular function create a list, and in this 
list, I have a function which can modify other elements of the list for 
updating purposes.
See the example below:

particular1=function()
{
obj=list()
obj$names=c("obj$c1","obj$c2")
obj$c1=8
obj$update=function(obj) {obj$c1=15; print(obj$c1)}
general(obj)
}

general=function(obj){
obj$c2<-10
print(obj$c1*obj$c2)
upd=eval(parse(text=deparse(obj$update)))
upd(obj)
print(obj$c1*obj$c2)
}

particular1()

 > particular1()
[1] 80
[1] 15
[1] 80

I would like to have: 80, 15, 150. But, the function obj$update change the 
value locally and do not change the "global" value of obj$c1.
I think it is a problem related environment of the function (I have read 
the previous discussion on this list "do.call and environment") but can not 
find how to solve it.

Hope that somebody can help me,
Sincerely.


St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From oleg at sai.msu.su  Thu Mar 25 16:13:53 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 25 Mar 2004 18:13:53 +0300 (MSK)
Subject: [R] downloadable mailing list archive in mbox format
Message-ID: <Pine.GSO.4.58.0403251811080.29270@ra.sai.msu.su>

Hi there,

I'm beginner and would like to have mailing list archive to
read it offline and don't disturb mailing list.
I'd prefer mbox format.

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From macq at llnl.gov  Thu Mar 25 16:53:36 2004
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 25 Mar 2004 07:53:36 -0800
Subject: [R] subsetting based on vector
In-Reply-To: <BAY16-F66i3QIwemiG900016fc2@hotmail.com>
References: <BAY16-F66i3QIwemiG900016fc2@hotmail.com>
Message-ID: <p06002000bc88adfe9268@[128.115.153.6]>

Since you are interested in subsetting, a little search of the online 
help such as

    help.search('subset')

will reveal the subset() function. Which you could use, for example, like this:

dataFIT<-function(MODEL, MARGINS, yearConsidered){
    library(foreign
    CovPaper<-read.spss("C:/Data/data.sav")
    NewData <- subset(CovPaper, YEAR %in% yearConsidered)

   #fit model to data #
}

If you truly need NewData to be a list, use as.list(). Also, within 
the context of your example, you don't need all the quotes; you can 
use CovPaper$YEAR instead of CovPaper$"YEAR".

Also check the documentation of the function(s) you are using to fit 
the model(s) for a subset argument; that might be sufficient for your 
needs.

-Don

At 1:33 PM +0000 3/25/04, Vumani Dlamini wrote:
>Dear R users;
>
>I am trying to write a small program which reads in a data set, and 
>selects observations from certain years before the analysis. I have 
>a problem including the selection criteria in the header of the 
>program.
>
>Here is the problem;
>
>dataFIT<-function(MODEL, MARGINS, yearConsidered){
>    library(foreign
>    CovPaper<-read.spss("C:/Data/data.sav")
>    NewData <- list(CovPaper$"YEAR"[CovPaper$"YEAR" == yearConsidered],
>                          CovPaper$"YEAR"[CovPaper$"SEX" == yearConsidered],
>                      #### and so on #####
>   #fit model to data #
>}
>
>When I use one year there is no problem, but I would like some data 
>sets to span over years and I am not sure how to do this without 
>having to change the body of my fitting program.
>
>I tried searching the R-list but to no avail.
>
>Thanking you in advance.
>
>
>Vumani
>
>_________________________________________________________________
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Isabelle.ZABALZA-MEZGHANI at ifp.fr  Thu Mar 25 17:10:17 2004
From: Isabelle.ZABALZA-MEZGHANI at ifp.fr (ZABALZA-MEZGHANI Isabelle)
Date: Thu, 25 Mar 2004 17:10:17 +0100
Subject: [R] Error : sink stack is full
Message-ID: <488C02265C6AD611BF200002A542182F05B593B8@irnts22.ifp.fr>

Hello,

I have implemented a method which uses sink to follow the progression status
of an iterative process (Below is part of the code)


I have already used such kind of code with no problem. Today, I get a "sink
stack is full" error.
I wonder if it could be linked with the fact that my .RData has a large size
(around 7 Mo) ???

I hope that someone can help me ...
Thanks in advance

Isabelle Zabalza.



From oleg at sai.msu.su  Thu Mar 25 21:44:09 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 25 Mar 2004 23:44:09 +0300 (MSK)
Subject: [R] downloadable mailing list archive in mbox format
In-Reply-To: <40630727.7090509@jhsph.edu>
References: <Pine.GSO.4.58.0403251811080.29270@ra.sai.msu.su>
	<40630727.7090509@jhsph.edu>
Message-ID: <Pine.GSO.4.58.0403252341530.29270@ra.sai.msu.su>

On Thu, 25 Mar 2004, Roger D. Peng wrote:

> You can get archives of the list at the URL listed at the bottom of
> this email.

Thanks !


>
> -roger
>
> Oleg Bartunov wrote:
> > Hi there,
> >
> > I'm beginner and would like to have mailing list archive to
> > read it offline and don't disturb mailing list.
> > I'd prefer mbox format.
> >
> > 	Regards,
> > 		Oleg
> > _____________________________________________________________
> > Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> > Sternberg Astronomical Institute, Moscow University (Russia)
> > Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> > phone: +007(095)939-16-83, +007(095)939-23-83
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From jeaneid at chass.utoronto.ca  Thu Mar 25 19:53:36 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 25 Mar 2004 13:53:36 -0500
Subject: [R] XEMACS, ESS, on Debian
Message-ID: <Pine.SGI.4.40.0403251344430.5581362-100000@origin.chass.utoronto.ca>

Dear all,
This might not be the best place to post this (maybe the ESS help is
better) but I will try anyways.

I am running ESS, Xemacs, R 8.1 on a debian testing dist (i386 arch). when
I envoke  the edit(data) command, a data editor appears and all is fine.
However executing C-c C-c will only blank out the data frame in the data editor
and does not kill it. I have to go and kill it with the mouse in order to
be able to type anything else on the xemacs-ESS session. On a Win XP
machine it works fine the C-c C-c does kill the data editor completely.

Note on the debian machine when issuing C-c C-c, the data frame will not
print in Xemacs even when killed manually. However, If only killed
manually the data frame will print in the ESS session.

Thank you for your help.



From itayf at fhcrc.org  Thu Mar 25 19:24:54 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Thu, 25 Mar 2004 10:24:54 -0800 (PST)
Subject: [R] How to add a top X-axis with a different logarithmic scale?
Message-ID: <Pine.LNX.4.44.0403251021430.5614-100000@cezanne.fhcrc.org>


Hi,

I am trying to put on one plot two different logarithmic
scales, using the bottom and top X-axes.
Below there is an example of what I am trying to achieve,
using axTicks() -- and fails.

I already spent few hours on that, and cannot figure out from
?par and ?axTicks what I am doing wrong.

Example follows:

############################################################

####    Data
x <- c(1.1*1:4, 25*1:5) / 50e+03
y <- c(0.15*1:4, 0.6 + 0.05*1:5)

####    Configure plot
old.par <- par(no.readonly=TRUE)
xlim <- range(xlim)
ylim <- c(0, 1)

####    Plot
## For production I will plot several data sets, so I first
## initialize window, and then use lines().
plot.new()
plot.window(xlim=xlim, ylim=ylim, log="x")
lines(x, y)

####    Plot axes
axis(1)
## Ticks for upper X-axis with a new scale: xlim*4e06
cat("top.ticks:\n", top.ticks <- axTicks(3, usr=xlim*4e06),
    "\n")
## Here I thought to put something like axis(3, at=top.ticks).
## Probably will need to rescale 'top.ticks' according to
## lower X-axis? 
axis(2)

### Restore
par(old.par)

############################################################

Thanks in advance for any comments/suggestions.

	Itay

-----------------------------------------------------------------
itayf at fhcrc.org		   Fred Hutchinson Cancer Research Center



From rvalliant at survey.umd.edu  Thu Mar 25 19:06:55 2004
From: rvalliant at survey.umd.edu (Richard Valliant)
Date: Thu, 25 Mar 2004 13:06:55 -0500
Subject: [R] g-inverse question
Message-ID: <s062d981.013@SURVEYGWIA.UMD.EDU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040325/5514a317/attachment.pl

From hb at maths.lth.se  Thu Mar 25 17:38:17 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 25 Mar 2004 17:38:17 +0100
Subject: [R] colors, lines, characters .... documentation
In-Reply-To: <000201c411b5$06ec78d0$e502eb82@maths.lth.se>
Message-ID: <000101c41287$93453dd0$e502eb82@maths.lth.se>

Oops, I forgot to check that plotSymbols(TRUE) worked with R --vanilla
and it didn't as some of you already noticed; intToHex() and
intToOct() were not defined (they're in my R.classes bundle at
http://www.braju.com/R/). Below is a self-contained version that
should work.

Cheers

Henrik Bengtsson

----------------------------------------------------
BEGIN code
----------------------------------------------------
plotSymbols <- function(interactive=FALSE) {
  ASCII <- c("\000", sapply(1:255, function(i)
parse(text=paste("\"\\",
                    structure(i,class="octmode"), "\"",
sep=""))[[1]]));
  intToChar <- function(i) {
    ASCII[i %% 256 + 1];
  }

  as.character.hexmode <- function(x) {
    hexDigit <- c(0:9, "A", "B", "C", "D", "E", "F")
    isna <- is.na(x)
    y <- x[!isna]
    ans0 <- character(length(y))
    z <- NULL
    while (any(y > 0) | is.null(z)) {
      z <- y%%16
      y <- floor(y/16)
      ans0 <- paste(hexDigit[z + 1], ans0, sep = "")
    }
    ans <- rep(NA, length(x))
    ans[!isna] <- ans0
    ans
  }
  
  intToHex <- function(x) {
    y <- as.integer(x);
    class(y) <- "hexmode";
    y <- as.character(y);
    dim(y) <- dim(x);
    y;
  }

  as.character.octmode <- function(x, ...) {
    isna <- is.na(x)
    y <- x[!isna]
    ans0 <- character(length(y))
    z <- NULL
    while (any(y > 0) | is.null(z)) {
      z <- y%%8
      y <- floor(y/8)
      ans0 <- paste(z, ans0, sep="")
    }
    ans <- rep(as.character(NA), length(x))
    ans[!isna] <- ans0
    ans
  }
  
  intToOct <- function(x) {
    y <- as.integer(x);
    class(y) <- "octmode";
    y <- as.character(y);
    dim(y) <- dim(x);
    y;
  }
    
  interactive <- interactive && interactive();

  i <- 0:255;
  ncol <-16;
  
  top <- 3 + 2*interactive;
  opar <- par(cex.axis=0.7, mar=c(3,3,top,3)+0.1)
  on.exit(par(opar))

  plot(i%%ncol,1+i%/%ncol, pch=i, xlim=c(0,ncol-1), xlab="", ylab="", 
 
axes=FALSE);
  axis(1, at=0:15)
  axis(2, at=1:16, labels=0:15*16, las=2)
  axis(3, at=0:15)
  axis(4, at=1:16, labels=0:15*16+15, las=2)
  if (interactive) {
    title(main="Click on a symbol to add it to the data frame. Click
in margin to quit!", cex.main=0.8, line=3.5);
    df <- list();
    usr <- par("usr");
    ready <- FALSE;
    while (!ready) {
      click <- locator(n=1);
      print(click)
      x <- click$x;
      y <- click$y - 1;
      ready <- !(x > -0.5 && x < 15.5 && y > -0.5 && y < 15.5);
      if (!ready) {
        x <- round(x);
        y <- round(y);
        z <- 16*y + x;
        ch  <- intToChar(z);
        dec <- as.character(z); 
        hex <- intToHex(z);
        oct <- intToOct(z);
        spc <- paste(rep("0", 2-nchar(hex)), collapse="");
        hex <- paste(spc, hex, sep="");
        spc <- paste(rep("0", 3-nchar(oct)), collapse="");
        oct <- paste(spc, oct, sep="");
        df$ch  <- c(df$ch , ch );
        df$dec <- c(df$dec, dec);
        df$hex <- c(df$hex, hex);
        df$oct <- c(df$oct, oct);

        if (nchar(ch) == 0) ch <- " ";
        spc <- paste(rep(" ", 3-nchar(dec)), collapse="");
        dec <- paste(spc, dec, sep="");
        cat("Selected ASCII character '", ch, "' ", dec, " 0x", hex, 
                                                 " \\", oct, "\n",
sep="");
      }
    }
    return(df);
  }

  invisible();
} # plotSymbols()
----------------------------------------------------
END code
----------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Henrik 
> Bengtsson
> Sent: den 24 mars 2004 16:31
> To: 'Monica Palaseanu-Lovejoy'; r-help at stat.math.ethz.ch
> Subject: RE: [R] colors, lines, characters .... documentation
> 
> 
> Hi, many questions at once there, but here some help 
> regarding *symbols*. 
> 
> I've pasted a function plotSymbols() that shows all symbols 
> available. Note that the the symbols pch >= 128 are system 
> dependent so you should not expect them to look the same on 
> Windows, Mac and Unix. Try also plotSymbols(TRUE). To turn of 
> the click-bell do 'options(locatorBell=FALSE)' (see ?locator).
> 
> Cheers
> 
> Henrik
>

[snip old code]

>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monica 
> > Palaseanu-Lovejoy
> > Sent: den 24 mars 2004 16:06
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] colors, lines, characters .... documentation
> > 
> > 
> > Hi,
> > 
> > Very so often when i am plotting something, doing a histogram, or
> > whatever i am struggling to find out which are the numbers for 
> > different colors, palette names, types of lines, symbols, etc. Is 
> > there any documentation on line with all these numbers / names 
> > and the associated symbol / color???
> > 
> > For example if i am using the command image it uses a palette
> > from red to yellow, with red the lowest value, and yellow the
> highest 
> > value. What if i want a reverse palette, with green the lowest
value
> 
> > and yellow middle values and red highest value??? Or much more
> > simple, just yellow lowest value and red highest value???
> > 
> > Thank you for assistance,
> > 
> > Monica
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> > PLEASE 
> > do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rpeng at jhsph.edu  Thu Mar 25 17:21:59 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 25 Mar 2004 11:21:59 -0500
Subject: [R] downloadable mailing list archive in mbox format
In-Reply-To: <Pine.GSO.4.58.0403251811080.29270@ra.sai.msu.su>
References: <Pine.GSO.4.58.0403251811080.29270@ra.sai.msu.su>
Message-ID: <40630727.7090509@jhsph.edu>

You can get archives of the list at the URL listed at the bottom of 
this email.

-roger

Oleg Bartunov wrote:
> Hi there,
> 
> I'm beginner and would like to have mailing list archive to
> read it offline and don't disturb mailing list.
> I'd prefer mbox format.
> 
> 	Regards,
> 		Oleg
> _____________________________________________________________
> Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> Sternberg Astronomical Institute, Moscow University (Russia)
> Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> phone: +007(095)939-16-83, +007(095)939-23-83
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Thu Mar 25 17:04:09 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 25 Mar 2004 10:04:09 -0600
Subject: [R] Error in 'legend' help?
In-Reply-To: <Pine.LNX.4.44.0403251556370.22007-100000@gannet.stats>
References: <Pine.LNX.4.44.0403251556370.22007-100000@gannet.stats>
Message-ID: <1080230649.15996.10.camel@localhost.localdomain>

On Thu, 2004-03-25 at 09:59, Prof Brian Ripley wrote:
> On Thu, 25 Mar 2004, Marc Schwartz wrote:
> 
> > So the help _should_ read:
> > 
> >    bg: the background color for the legend box.  (Note that this is
> >           only used if 'bty != "n"'.)
> 
> and does so read in 1.9.0 beta: I fixed this on Jan 2.
> 
> Can we remind people that R is beta-testing 1.9.0, and please do consider 
> using the betas now.  Certainly, please check it before thinking about 
> errors.


Quite true.  Point taken.

Thanks,

Marc



From ripley at stats.ox.ac.uk  Thu Mar 25 16:59:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Mar 2004 15:59:38 +0000 (GMT)
Subject: [R] Error in 'legend' help?
In-Reply-To: <1080221630.6983.63.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0403251556370.22007-100000@gannet.stats>

On Thu, 25 Mar 2004, Marc Schwartz wrote:

> So the help _should_ read:
> 
>    bg: the background color for the legend box.  (Note that this is
>           only used if 'bty != "n"'.)

and does so read in 1.9.0 beta: I fixed this on Jan 2.

Can we remind people that R is beta-testing 1.9.0, and please do consider 
using the betas now.  Certainly, please check it before thinking about 
errors.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Thu Mar 25 22:29:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 25 Mar 2004 15:29:56 -0600
Subject: [R] How to add a top X-axis with a different logarithmic scale?
In-Reply-To: <Pine.LNX.4.44.0403251021430.5614-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0403251021430.5614-100000@cezanne.fhcrc.org>
Message-ID: <1080250196.7003.79.camel@localhost.localdomain>

On Thu, 2004-03-25 at 12:24, Itay Furman wrote:
> Hi,
> 
> I am trying to put on one plot two different logarithmic
> scales, using the bottom and top X-axes.
> Below there is an example of what I am trying to achieve,
> using axTicks() -- and fails.
> 
> I already spent few hours on that, and cannot figure out from
> ?par and ?axTicks what I am doing wrong.
> 
> Example follows:
> 
> ############################################################

See my interspersed comments and then code below:


> ####    Data
> x <- c(1.1*1:4, 25*1:5) / 50e+03
> y <- c(0.15*1:4, 0.6 + 0.05*1:5)
> 
> ####    Configure plot
> old.par <- par(no.readonly=TRUE)
> xlim <- range(xlim)

I presume the above line should be:

xlim <- range(x)

> ylim <- c(0, 1)
> 
> ####    Plot
> ## For production I will plot several data sets, so I first
> ## initialize window, and then use lines().
> plot.new()
> plot.window(xlim=xlim, ylim=ylim, log="x")
> lines(x, y)

Replace the above 3 lines with:

plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)

> ####    Plot axes
> axis(1)

The above axis(1) is unnecessary. The lower x axis is already created by
plot().

> ## Ticks for upper X-axis with a new scale: xlim*4e06
> cat("top.ticks:\n", top.ticks <- axTicks(3, usr=xlim*4e06),
>     "\n")

I am not sure if this is what you are trying to achieve, but use:

axis(3, labels = axTicks(3) * 4e06)

This will provide for the tick marks to be in their default locations,
just as with the lower 'x' axis, but the labels at each tick mark will
be adjusted based upon the 4e06. You do not need to change the axis
range to change the labels alone.


> ## Here I thought to put something like axis(3, at=top.ticks).
> ## Probably will need to rescale 'top.ticks' according to
> ## lower X-axis? 
> axis(2)

axis(2) is the left hand y axis and is not required here, since it too
is already drawn by plot().


> ### Restore
> par(old.par)
> 
> ############################################################
> 
> Thanks in advance for any comments/suggestions.
> 
> 	Itay


So, to follow up a bit more coherently, if I understand what you are
trying to do, the code should be:

x <- c(1.1 * 1:4, 25 * 1:5) / 50e+03
y <- c(0.15 * 1:4, 0.6 + 0.05 * 1:5)

old.par <- par(no.readonly=TRUE)
xlim <- range(x)
ylim <- c(0, 1)

plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)

axis(3, labels = axTicks(3) * 4e06)

par(old.par)



Does that get you what you want?

HTH,

Marc Schwartz



From papucho at mac.com  Thu Mar 25 22:38:57 2004
From: papucho at mac.com (Ivan Alves)
Date: Thu, 25 Mar 2004 22:38:57 +0100
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <D255C636-7EA4-11D8-A82D-000A959D05F0@mac.com>

Dear all,

S-Plus has the function AggregateSeries() whose name is self 
explanatory.  For instance one can derive monthly series from daily 
ones by specifying end-of-period, averages, sums, etc.  I looked for a 
similar function in the packages "its" and "tseries", but found 
nothing.  I also help.searched() for aggregate to no avail.  Would 
anybody be so kind to point me in the right direction?

Thank you very much in advance.

best regards,

_______________________
Ivan Alves
mailto://papucho at mac.com



From ripley at stats.ox.ac.uk  Thu Mar 25 22:41:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Mar 2004 21:41:33 +0000 (GMT)
Subject: [R] g-inverse question
In-Reply-To: <s062d981.013@SURVEYGWIA.UMD.EDU>
Message-ID: <Pine.LNX.4.44.0403252138530.1149-100000@gannet.stats>

Do read the help file!

ginv has a tolerance, and your matrix is below it: it is computationally 
singular to the tolerance of 1e-8.  Try changing the tolerance ....

On Thu, 25 Mar 2004, Richard Valliant wrote:

> I am using the ginv function from MASS and have run across this problem
> that I do not understand.  If I define the matrix A as below, its
> g-inverse does not satisfy the Moore-Penrose condition 
>  
> A %*% ginv(A) %*% A = A.
>  
> The matrix A is X'WX in a quadratic regression using some very large
> dollar values. 
> The much simpler matrix B does satisfy the MP condition. Am I doing
> something wrong? Is this due to the large values in A?
>  
> I am using v.1.8.1 on Windows XP.
>  
> > A <- matrix(c(15, 20157302,20157302,68854740000000), ncol=2, byrow
> =T)
> > A
>          [,1]         [,2]
> [1,]       15 2.015730e+07
> [2,] 20157302 6.885474e+13
> > ginv(A)
>              [,1]         [,2]
> [1,] 1.244696e-27 4.251721e-21
> [2,] 4.251721e-21 1.452333e-14
> > A %*% ginv(A) %*% A
>              [,1]         [,2]
> [1,] 5.901073e+00 2.015730e+07
> [2,] 2.015730e+07 6.885474e+13
> > B
>      [,1] [,2]
> [1,]    1    2
> [2,]    2    1
> > B %*% ginv(B) %*% B
>      [,1] [,2]
> [1,]    1    2
> [2,]    2    1
> 
>  
> Thanks in advance,
>  
>  
> Richard Valliant, Ph.D.
> University of Maryland
> Joint Program for Survey Methodology
> 1218 Lefrak Hall
> College Park MD 20742
> (301)-405-0932
> FAX: (301) 314-7912
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From papucho at mac.com  Thu Mar 25 22:47:43 2004
From: papucho at mac.com (Ivan Alves)
Date: Thu, 25 Mar 2004 22:47:43 +0100
Subject: [R] S+Finmetrics cointegration functions
Message-ID: <0B82A29F-7EA6-11D8-A82D-000A959D05F0@mac.com>

Dear all,

S+Finmetrics has a number of very specilised functions.  I am 
particularly interested in the estimation of cointegrated VARs (chapter 
12 of Zivot and Wang).  In this context the functions coint() and 
VECM() stand out.  I looked at package "dse1", but found no comparable 
functionality.  Are there any other packages you could point me to?  In 
general, are there efforts for replicating within one package the 
functionality of S+Finmetrics?  Thank you very much in advance for any 
guidance.

Best regards,

_______________________
Ivan Alves
mailto://papucho at mac.com



From ripley at stats.ox.ac.uk  Thu Mar 25 22:51:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Mar 2004 21:51:52 +0000 (GMT)
Subject: [R] Aggregating frequency of irregular time series
In-Reply-To: <D255C636-7EA4-11D8-A82D-000A959D05F0@mac.com>
Message-ID: <Pine.LNX.4.44.0403252149010.1149-100000@gannet.stats>

R itself has no support for irregular time series, but it does have an 
aggregate method for regular ones.  You need to look into whichever 
package is handling irregular time series.  However, it seems to me that 
this is not a time series problem at all: you have a set of observations 
whose indices are data-times, and tapply() will do the job.

On Thu, 25 Mar 2004, Ivan Alves wrote:

> S-Plus has the function AggregateSeries() whose name is self 
> explanatory.  For instance one can derive monthly series from daily 
> ones by specifying end-of-period, averages, sums, etc.  I looked for a 
> similar function in the packages "its" and "tseries", but found 
> nothing.  I also help.searched() for aggregate to no avail.  Would 
> anybody be so kind to point me in the right direction?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Thu Mar 25 22:54:12 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 25 Mar 2004 15:54:12 -0600
Subject: [R] rsync.r-project.org available again
Message-ID: <6rd6707fuj.fsf@bates4.stat.wisc.edu>

The rsync site is back up.  It may take a while for the new location
to percolate through the DNS servers.  The site should be

bates at bates4:~$ host rsync.r-project.org
rsync.r-project.org is an alias for bates4.stat.wisc.edu.
bates4.stat.wisc.edu has address 128.105.174.134

and not the previous site, franz.stat.wisc.edu, which was comprised.



From oleg at sai.msu.su  Thu Mar 25 23:02:13 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Fri, 26 Mar 2004 01:02:13 +0300 (MSK)
Subject: [R] factor based on pattern match ?
Message-ID: <Pine.GSO.4.58.0403260050390.29270@ra.sai.msu.su>

Hello,

is't possible to specify pattern in levels ?

> y=c("ff","f","m","mm","fm","mf","ffm","mmf","mmm","fff");
> factor(y)
 [1] ff  f   m   mm  fm  mf  ffm mmf mmm fff
Levels: f ff fff ffm fm m mf mm mmf mmm

I want to specify levels using regexp ("f.*","m.*") or use some
another method. So, I could have 2 levels, say, F and M, where
F means everything  beginning from 'f' and M - from 'm'.

Something like cut() for numerical data.

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From andy_liaw at merck.com  Thu Mar 25 23:16:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 25 Mar 2004 17:16:34 -0500
Subject: [R] factor based on pattern match ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A9B@usrymx25.merck.com>

Is the following sort of what you want?

> y = factor(c("ff","f","m","mm","fm","mf","ffm","mmf","mmm","fff"))
> levels(y) <- substring(levels(y), 1, 1)
> y
 [1] f f m m f m f m m f
Levels: f m

Andy

> From: Oleg Bartunov
> 
> Hello,
> 
> is't possible to specify pattern in levels ?
> 
> > y=c("ff","f","m","mm","fm","mf","ffm","mmf","mmm","fff");
> > factor(y)
>  [1] ff  f   m   mm  fm  mf  ffm mmf mmm fff
> Levels: f ff fff ffm fm m mf mm mmf mmm
> 
> I want to specify levels using regexp ("f.*","m.*") or use some
> another method. So, I could have 2 levels, say, F and M, where
> F means everything  beginning from 'f' and M - from 'm'.
> 
> Something like cut() for numerical data.
> 
> 	Regards,
> 		Oleg
> _____________________________________________________________
> Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> Sternberg Astronomical Institute, Moscow University (Russia)
> Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> phone: +007(095)939-16-83, +007(095)939-23-83
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From papucho at mac.com  Thu Mar 25 23:26:28 2004
From: papucho at mac.com (Ivan Alves)
Date: Thu, 25 Mar 2004 23:26:28 +0100
Subject: [R] Aggregating frequency of irregular time series
In-Reply-To: <Pine.LNX.4.44.0403252149010.1149-100000@gannet.stats>
References: <Pine.LNX.4.44.0403252149010.1149-100000@gannet.stats>
Message-ID: <7585C31E-7EAB-11D8-A82D-000A959D05F0@mac.com>

Thank you very much Brian.  Indeed, by looking at ?tapply() this would 
do the job for regular time series (whose data-time INDEX renders 
itself naturally for " bundling data" in regular groups). However, with 
irregular time series the story is different, as some careful 
"bundling" is necessary prior to applying tapply(). Whereas this may 
not be a problem for months (creating strings month-year), it would be 
for weeks, for instance.  Also the object returned would need to be 
converted again to a time series object with additional function calls. 
Furthermore, the function AggregateSeries() provides additional 
functionality, such as creating moving averages, rolling variances, 
minima and maxima, all with options to the same function call. I take 
from your response that there is no easy way out (an already existing 
function), and that some programming will be required.

Kind regards,

Ivan
_______________________
Ivan Alves
mailto://papucho at mac.com
On 25 Mar 2004, at 22:51, Prof Brian Ripley wrote:

> R itself has no support for irregular time series, but it does have an
> aggregate method for regular ones.  You need to look into whichever
> package is handling irregular time series.  However, it seems to me 
> that
> this is not a time series problem at all: you have a set of 
> observations
> whose indices are data-times, and tapply() will do the job.
>
> On Thu, 25 Mar 2004, Ivan Alves wrote:
>
>> S-Plus has the function AggregateSeries() whose name is self
>> explanatory.  For instance one can derive monthly series from daily
>> ones by specifying end-of-period, averages, sums, etc.  I looked for a
>> similar function in the packages "its" and "tseries", but found
>> nothing.  I also help.searched() for aggregate to no avail.  Would
>> anybody be so kind to point me in the right direction?
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From andy_liaw at merck.com  Thu Mar 25 23:36:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 25 Mar 2004 17:36:35 -0500
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A9C@usrymx25.merck.com>

> From: Ivan Alves
> 
> Thank you very much Brian.  Indeed, by looking at ?tapply() 
> this would 
> do the job for regular time series (whose data-time INDEX renders 
> itself naturally for " bundling data" in regular groups). 
> However, with 
> irregular time series the story is different, as some careful 
> "bundling" is necessary prior to applying tapply(). Whereas this may 
> not be a problem for months (creating strings month-year), it 
> would be 
> for weeks, for instance.  Also the object returned would need to be 
> converted again to a time series object with additional 
> function calls. 
> Furthermore, the function AggregateSeries() provides additional 
> functionality, such as creating moving averages, rolling variances, 
> minima and maxima, all with options to the same function call. I take 
> from your response that there is no easy way out (an already existing 
> function), and that some programming will be required.

So can we count on you to contribute this (and the cointegration that you
requested in another post)?  You did read the message at the R startup
screen, didn't you?

Best,
Andy
 
> Kind regards,
> 
> Ivan
> _______________________
> Ivan Alves
> mailto://papucho at mac.com
> On 25 Mar 2004, at 22:51, Prof Brian Ripley wrote:
> 
> > R itself has no support for irregular time series, but it 
> does have an
> > aggregate method for regular ones.  You need to look into whichever
> > package is handling irregular time series.  However, it seems to me 
> > that
> > this is not a time series problem at all: you have a set of 
> > observations
> > whose indices are data-times, and tapply() will do the job.
> >
> > On Thu, 25 Mar 2004, Ivan Alves wrote:
> >
> >> S-Plus has the function AggregateSeries() whose name is self
> >> explanatory.  For instance one can derive monthly series from daily
> >> ones by specifying end-of-period, averages, sums, etc.  I 
> looked for a
> >> similar function in the packages "its" and "tseries", but found
> >> nothing.  I also help.searched() for aggregate to no avail.  Would
> >> anybody be so kind to point me in the right direction?
> >
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Toby.Patterson at csiro.au  Thu Mar 25 23:52:14 2004
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Fri, 26 Mar 2004 09:52:14 +1100
Subject: [R] mlocal/mtrace inside a loop
Message-ID: <C4178DC99E08604EA5E2BDB989F0938001E077AF@EXTAS2-HBA.tas.csiro.au>

Look at ?mtrace for a start... 

And probably you need to look at the function go()



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fred J.
Sent: Thursday, March 25, 2004 8:16 PM
To: r help
Subject: [R] mlocal/mtrace inside a loop

Hello
I need some help in figuring Bravington's debugger
out.
Ok
I have 2 functions, fun1 and fun2 saved in a ASCII
file say filename is funs.
Fun1 has a loop which calls fun2, fun2 has a loop
which fails and I need to find out the value of the
variables of the fun2 and fun1 loops at the specific
iteration that fails. Both fun1 and fun2 loops will
iterate thousands of times so line by line debug is
not practical.
According to what I understood from > ?mlocal and the
Vol. 3/3, December 2003 R-news article "Debugging
Without (Too Many) Tears" by Mark Bravington. P.32
first column last paragraph, I did. "Which needs some
fixing and direction - thanks"

> source("funs")
> library(debug)
> debug(fun1)
data <- fun1(some.arguments")
 
in the file funs 
Was ...
fun1 <- function(arg1){ some looping code with calls
to fun2 }
fun2 <- function(arg2) {some more looping code}

Is now...to confirm to what I read
Fun1 <- function(arg1, nlocal=sys.parent()) 
mlocal(some more looping code)

I think I am implementing this package wrong and need
some help.

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From oleg at sai.msu.su  Thu Mar 25 23:51:54 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Fri, 26 Mar 2004 01:51:54 +0300 (MSK)
Subject: [R] factor based on pattern match ?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A9B@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7A9B@usrymx25.merck.com>
Message-ID: <Pine.GSO.4.58.0403260143540.29270@ra.sai.msu.su>

On Thu, 25 Mar 2004, Liaw, Andy wrote:

> Is the following sort of what you want?

thanks for example, but I need something more :)
In real situation, I'd like to use perl like patterns (like in grep),
or just implicitly specify levels, for example: level F is ("f.*", "mmm")


>
> > y = factor(c("ff","f","m","mm","fm","mf","ffm","mmf","mmm","fff"))
> > levels(y) <- substring(levels(y), 1, 1)
> > y
>  [1] f f m m f m f m m f
> Levels: f m
>
> Andy
>
> > From: Oleg Bartunov
> >
> > Hello,
> >
> > is't possible to specify pattern in levels ?
> >
> > > y=c("ff","f","m","mm","fm","mf","ffm","mmf","mmm","fff");
> > > factor(y)
> >  [1] ff  f   m   mm  fm  mf  ffm mmf mmm fff
> > Levels: f ff fff ffm fm m mf mm mmf mmm
> >
> > I want to specify levels using regexp ("f.*","m.*") or use some
> > another method. So, I could have 2 levels, say, F and M, where
> > F means everything  beginning from 'f' and M - from 'm'.
> >
> > Something like cut() for numerical data.
> >
> > 	Regards,
> > 		Oleg
> > _____________________________________________________________
> > Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> > Sternberg Astronomical Institute, Moscow University (Russia)
> > Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> > phone: +007(095)939-16-83, +007(095)939-23-83
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From itayf at fhcrc.org  Fri Mar 26 00:42:33 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Thu, 25 Mar 2004 15:42:33 -0800 (PST)
Subject: [R] How to add a top X-axis with a different logarithmic scale?
In-Reply-To: <1080250196.7003.79.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0403251511360.5926-100000@cezanne.fhcrc.org>


Thanks, Marc, for your reply.
I didn't explain myself well, but thanks to you I think I could 
phrase my question better.

I want to be able to compute new tick-positions for the top 
X-axis.

Why new ticks positions?
See in the example below (that uses your contributed minimal
code :-) what happens to the labels when you
replace 4e06 with 4.1e06.

Also, I assume from your example that using plot.new() + 
plot.window() + lines(), instead of plot() will not make a 
difference in the solution. As I mentioned in _my_ example code:
for a production I will overlay several curves and will
use lines().

	Thanks,
	Itay


#######################################################

x <- c(1.1 * 1:4, 25 * 1:5) / 50e+03
y <- c(0.15 * 1:4, 0.6 + 0.05 * 1:5)

old.par <- par(no.readonly=TRUE)
xlim <- range(x)
ylim <- c(0, 1)

plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)
## Replace the next line with the next-next line
axis(3, labels = axTicks(3) * 4e06)
#### axis(3, labels = axTicks(3) * 4.1e06)#<--- odd labels

par(old.par)

#######################################################


On Thu, 25 Mar 2004, Marc Schwartz wrote:

> On Thu, 2004-03-25 at 12:24, Itay Furman wrote:
> > Hi,
> > 
> > I am trying to put on one plot two different logarithmic
> > scales, using the bottom and top X-axes.
> > Below there is an example of what I am trying to achieve,
> > using axTicks() -- and fails.
> > 
> > I already spent few hours on that, and cannot figure out from
> > ?par and ?axTicks what I am doing wrong.
> > 
> > Example follows:
> > 
> > ############################################################
> 
> See my interspersed comments and then code below:
> 
> 
> > ####    Data
> > x <- c(1.1*1:4, 25*1:5) / 50e+03
> > y <- c(0.15*1:4, 0.6 + 0.05*1:5)
> > 
> > ####    Configure plot
> > old.par <- par(no.readonly=TRUE)
> > xlim <- range(xlim)
> 
> I presume the above line should be:
> 
> xlim <- range(x)
> 
> > ylim <- c(0, 1)
> > 
> > ####    Plot
> > ## For production I will plot several data sets, so I first
> > ## initialize window, and then use lines().
> > plot.new()
> > plot.window(xlim=xlim, ylim=ylim, log="x")
> > lines(x, y)
> 
> Replace the above 3 lines with:
> 
> plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)
> 
> > ####    Plot axes
> > axis(1)
> 
> The above axis(1) is unnecessary. The lower x axis is already created by
> plot().
> 
> > ## Ticks for upper X-axis with a new scale: xlim*4e06
> > cat("top.ticks:\n", top.ticks <- axTicks(3, usr=xlim*4e06),
> >     "\n")
> 
> I am not sure if this is what you are trying to achieve, but use:
> 
> axis(3, labels = axTicks(3) * 4e06)
> 
> This will provide for the tick marks to be in their default locations,
> just as with the lower 'x' axis, but the labels at each tick mark will
> be adjusted based upon the 4e06. You do not need to change the axis
> range to change the labels alone.
> 
> 
> > ## Here I thought to put something like axis(3, at=top.ticks).
> > ## Probably will need to rescale 'top.ticks' according to
> > ## lower X-axis? 
> > axis(2)
> 
> axis(2) is the left hand y axis and is not required here, since it too
> is already drawn by plot().
> 
> 
> > ### Restore
> > par(old.par)
> > 
> > ############################################################
> > 
> > Thanks in advance for any comments/suggestions.
> > 
> > 	Itay
> 
> 
> So, to follow up a bit more coherently, if I understand what you are
> trying to do, the code should be:
> 
> x <- c(1.1 * 1:4, 25 * 1:5) / 50e+03
> y <- c(0.15 * 1:4, 0.6 + 0.05 * 1:5)
> 
> old.par <- par(no.readonly=TRUE)
> xlim <- range(x)
> ylim <- c(0, 1)
> 
> plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)
> 
> axis(3, labels = axTicks(3) * 4e06)
> 
> par(old.par)
> 
> 
> 
> Does that get you what you want?
> 
> HTH,
> 
> Marc Schwartz
> 
> 
>



From vaclav.petricek at mff.cuni.cz  Fri Mar 26 01:13:25 2004
From: vaclav.petricek at mff.cuni.cz (Vaclav Petricek)
Date: Fri, 26 Mar 2004 01:13:25 +0100 (CET)
Subject: [R] xyplot inside a for loop problem
Message-ID: <Pine.BSF.4.50.0403260056430.98397-100000@sec.ms.mff.cuni.cz>


Hello everyone

Could someone please explain me why are xyplot() calls inside a for loop
unsuccessful?

Calling plot() is OK but xyplot() just opens the graphics window and that
is all. No error, no warning :-( The same xyplot() outside for loop works
fine.

---------------------------------------------------------
library(lattice)

# OK:
plot(1,2,type='p',main='standalone plot')
# OK:
for (name in 1) plot(1,2,type='p',main='plot in for')

# OK:
xyplot(1~2,type='p',main='standalone xyplot')
# This does not plot anything:
for (name in 1) xyplot(1~2,type='p',main='xyplot in for')
---------------------------------------------------------

Thank you very much for your help

Vaclav



From andy_liaw at merck.com  Fri Mar 26 01:17:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 25 Mar 2004 19:17:40 -0500
Subject: [R] xyplot inside a for loop problem
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7A9E@usrymx25.merck.com>

That's because you didn't read R FAQ 7.24.

Andy

> From: Vaclav Petricek
> 
> Hello everyone
> 
> Could someone please explain me why are xyplot() calls inside 
> a for loop
> unsuccessful?
> 
> Calling plot() is OK but xyplot() just opens the graphics 
> window and that
> is all. No error, no warning :-( The same xyplot() outside 
> for loop works
> fine.
> 
> ---------------------------------------------------------
> library(lattice)
> 
> # OK:
> plot(1,2,type='p',main='standalone plot')
> # OK:
> for (name in 1) plot(1,2,type='p',main='plot in for')
> 
> # OK:
> xyplot(1~2,type='p',main='standalone xyplot')
> # This does not plot anything:
> for (name in 1) xyplot(1~2,type='p',main='xyplot in for')
> ---------------------------------------------------------
> 
> Thank you very much for your help
> 
> Vaclav
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From sasprog474 at yahoo.com  Fri Mar 26 05:24:47 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Thu, 25 Mar 2004 20:24:47 -0800 (PST)
Subject: [R] Using R's LAPACK & Related files in Visual C++
Message-ID: <20040326042447.72984.qmail@web41409.mail.yahoo.com>

I am a relative newcomer to both the R and C/C++
software worlds -- I'm taking a C Programming class
currently.  I noticed the other day that the

C:\Program Files\R1_8_1\src\include\R_ext

directory on my WinXP box has the header files

BLAS.h
Lapack.h
Linpack.h
RLapack.h

I am interested in (perhaps) using one or more of
these
header files in a straight C program I'm working on in
Visual C++ .NET.  (I need matrix inversion, svd, 
generalized inverse, Cholesky decomposition, and a few
other matrix features for a class project and this
is the first time I've ever worked with header files.)
 My attempts to get the freely available
CLAPACK3-Windows.zip on

http://www.netlib.org/clapack/

to work have failed miserably, hence my interest in
finding other freely available libraries!  Does anyone
know how dependent these R header files are on one
another?  Are they redundant in any way?  Which ones
would be recommended?

Any help / advice would be greatly appreciated,

  Greg



From sasprog474 at yahoo.com  Fri Mar 26 05:40:30 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Thu, 25 Mar 2004 20:40:30 -0800 (PST)
Subject: [R] Using R's LAPACK & Related files in Visual C++  
Message-ID: <20040326044030.78805.qmail@web41408.mail.yahoo.com>

I apologize for double-posting; I believe my first
post was HTML.

I am a relative newcomer to both the R and C/C++
software worlds -- I'm taking a C Programming class
currently.  I noticed the other day that the

C:\Program Files\R1_8_1\src\include\R_ext

directory on my WinXP box has the header files

BLAS.h
Lapack.h
Linpack.h
RLapack.h

I am interested in (perhaps) using one or more of
these header files in a straight C program I'm working
on in Visual C++ .NET.  (I need matrix inversion, svd,
generalized inverse, Cholesky decomposition, and a few
other matrix features for a class project and this is
the first time I've ever worked with header files.) 
My attempts to get the freely available
CLAPACK3-Windows.zip on

http://www.netlib.org/clapack/

to work have failed miserably, hence my interest in
finding other freely available libraries!  Does anyone
know how dependent these R header files are on one
another?  Are they redundant in any way?  Which ones
would be recommended?

Any help / advice would be greatly appreciated.


=====
--------------------

Greg Tarpinian
San Diego, CA



From ggrothendieck at myway.com  Fri Mar 26 05:54:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 26 Mar 2004 04:54:51 +0000 (UTC)
Subject: [R] Aggregating frequency of irregular time series
References: <Pine.LNX.4.44.0403252149010.1149-100000@gannet.stats>
	<7585C31E-7EAB-11D8-A82D-000A959D05F0@mac.com>
Message-ID: <loom.20040326T052935-346@post.gmane.org>



Assume r.its is an its time series and assume that chron has been loaded.

chron represents dates as days since an origin so the week number can be
obtained by dividing by 7.  Then tapply the mean or other function.
Finally, recreate an its.  

week <- 7 * (as.numeric( chron( format( r.its at dates, "%m/%d/%Y" ) ) ) %/% 7 )
z <- tapply( r.its, week, mean )
z <- its( c(z), date = as.POSIXct( chron( as.numeric( names( z ) ) ) ) )

There is a new Dates class in R 1.9.0 that could be used in place of chron.


Ivan Alves <papucho <at> mac.com> writes:

: 
: Thank you very much Brian.  Indeed, by looking at ?tapply() this would 
: do the job for regular time series (whose data-time INDEX renders 
: itself naturally for " bundling data" in regular groups). However, with 
: irregular time series the story is different, as some careful 
: "bundling" is necessary prior to applying tapply(). Whereas this may 
: not be a problem for months (creating strings month-year), it would be 
: for weeks, for instance.  Also the object returned would need to be 
: converted again to a time series object with additional function calls. 
: Furthermore, the function AggregateSeries() provides additional 
: functionality, such as creating moving averages, rolling variances, 
: minima and maxima, all with options to the same function call. I take 
: from your response that there is no easy way out (an already existing 
: function), and that some programming will be required.
: 
: Kind regards,
: 
: Ivan
: _______________________
: Ivan Alves
: mailto://papucho <at> mac.com
: On 25 Mar 2004, at 22:51, Prof Brian Ripley wrote:
: 
: > R itself has no support for irregular time series, but it does have an
: > aggregate method for regular ones.  You need to look into whichever
: > package is handling irregular time series.  However, it seems to me 
: > that
: > this is not a time series problem at all: you have a set of 
: > observations
: > whose indices are data-times, and tapply() will do the job.
: >
: > On Thu, 25 Mar 2004, Ivan Alves wrote:
: >
: >> S-Plus has the function AggregateSeries() whose name is self
: >> explanatory.  For instance one can derive monthly series from daily
: >> ones by specifying end-of-period, averages, sums, etc.  I looked for a
: >> similar function in the packages "its" and "tseries", but found
: >> nothing.  I also help.searched() for aggregate to no avail.  Would
: >> anybody be so kind to point me in the right direction?
: >
: > -- 
: > Brian D. Ripley,                  ripley <at> stats.ox.ac.uk
: > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
: > University of Oxford,             Tel:  +44 1865 272861 (self)
: > 1 South Parks Road,                     +44 1865 272866 (PA)
: > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From jfresen at medunsa.ac.za  Fri Mar 26 07:47:22 2004
From: jfresen at medunsa.ac.za (J Fresen)
Date: Fri, 26 Mar 2004 08:47:22 +0200
Subject: [R] model fitting
Message-ID: <3571656DEC883442BB9A40C741F41EEF014C2410@excmail.medunsa.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040326/60630bc0/attachment.pl

From Bernhard.Pfaff at drkw.com  Fri Mar 26 09:02:22 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 26 Mar 2004 09:02:22 +0100
Subject: [R] S+Finmetrics cointegration functions
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730905@ibfftce505.is.de.dresdnerkb.com>

> 
> Dear all,
> 
> S+Finmetrics has a number of very specilised functions.  I am 
> particularly interested in the estimation of cointegrated 
> VARs (chapter 
> 12 of Zivot and Wang).  In this context the functions coint() and 
> VECM() stand out.  I looked at package "dse1", but found no 
> comparable 
> functionality.  Are there any other packages you could point 
> me to?  In 
> general, are there efforts for replicating within one package the 
> functionality of S+Finmetrics?  Thank you very much in 
> advance for any 
> guidance.


Dear Ivan,

yesterday, I have uploaded an update of package 'urca' (version 0.3-3). In
this update the Johansen procedure as outlined in:

  Johansen, S. (1988), Statistical Analysis of Cointegration Vectors,
  \emph{Journal of Economic Dynamics and Control}, \bold{12}, 231--254.

  Johansen, S. and Juselius, K. (1990), Maximum Likelihood Estimation and
  Inference on Cointegration -- with Applications to the Demand for
  Money, \emph{Oxford Bulletin of Economics and Statistics}, \bold{52,
    2}, 169--210.

  Johansen, S. (1991), Estimation and Hypothesis Testing of
  Cointegration Vectors in Gaussian Vector Autoregressive Models,
  \emph{Econometrica}, \bold{Vol. 59, No. 6}, 1551--1580.

is implemented. Beside containing the Johansen procedure the data sets used
in Johansen, S. and Juselius, K. (1990) are provided, too, such that you can
directly cross check their results and investigate further. 
Various unit root tests encountered in applied econometrics are also
available in this package; among these are:

1) Elliott, G., Rothenberg, T.J. and Stock, J.H. (1996)
2) Kwiatkowski, D., Phillips, P.C.B., Schmidt, P. and Shin, Y., (1992)
3) Phillips, P.C.B. and Perron, P. (1988)
4) Schmidt, P. and Phillips, P.C.B. (1992)
5) Zivot, E. and Andrews, Donald W.K. (1992)
6) Phillips, P.C.B. and Ouliaris, S. (1990)

The package is written in *pure* R and S4 classes are utilised. It is
intended to amend this package by other functionalities in the context of
VECM / cointegration analysis in the near future (e.g. such as impulse
responses, Luetkepohl et al. (2004), "TESTING FOR THE COINTEGRATING RANK OF
A VAR PROCESSWITH LEVEL SHIFT AT UNKNOWN TIME", *Econometrica* Vol. 72 No.
2, 647 -- 662 and the like).

Hopefully this package is of use for you and the econometric oriented R
community as well.

Bernhard


> 
> Best regards,
> 
> _______________________
> Ivan Alves
> mailto://papucho at mac.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express
written permission of the sender. If you are not the intended recipient, please
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From jazevedo at provide.com.br  Fri Mar 26 10:49:22 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Fri, 26 Mar 2004 09:49:22 -0000
Subject: [R] Finite mixture models
In-Reply-To: <3571656DEC883442BB9A40C741F41EEF014C2410@excmail.medunsa.ac.za>
Message-ID: <000601c41317$9da5f090$21a2f080@Lepc204>

Dear R users,
I would like to know if R has any tools to estimate a finite mixture model.
Many thanks,
Joao Pedro



From B.Rowlingson at lancaster.ac.uk  Fri Mar 26 11:11:04 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 26 Mar 2004 10:11:04 +0000
Subject: [R] Error : sink stack is full
In-Reply-To: <488C02265C6AD611BF200002A542182F05B593B8@irnts22.ifp.fr>
References: <488C02265C6AD611BF200002A542182F05B593B8@irnts22.ifp.fr>
Message-ID: <406401B8.2000404@lancaster.ac.uk>

ZABALZA-MEZGHANI Isabelle wrote:
> Hello,
> 
> I have implemented a method which uses sink to follow the progression status
> of an iterative process (Below is part of the code)
> 
> 
> I have already used such kind of code with no problem. Today, I get a "sink
> stack is full" error.
> I wonder if it could be linked with the fact that my .RData has a large size
> (around 7 Mo) ???
> 

  Every time you do a sink('filename') call R keeps track of where the 
output was going before, so when you do sink() it goes back. Example:

  sink('fnord')
   print(1)  # goes to file 'fnord'
     sink('foobar')
       print(2) # goes to file 'foobar'
     sink()
   print(3) # goes to 'fnord'
  sink()
    # now back to normal.

  R has a limit on how deeply nested sink()s can be. A quick test:

 > for(f in 1:1000){
+ sink(paste(f,'-sink'))
+ print(1)
+ }

  got up to 20. This is defined in connections.c in the source code:

src/main/connections.c:#define NSINKS 21

  If you really want to nest sink()s deeper than that then you will have 
to change this parameter and recompile R. More likely is that you are 
doing unneccessary sink()s because of the iterative nature of your code 
(which wasn't attached because I think R-news strips out attachments).

Baz



From ripley at stats.ox.ac.uk  Fri Mar 26 11:16:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Mar 2004 10:16:31 +0000 (GMT)
Subject: [R] Finite mixture models
In-Reply-To: <000601c41317$9da5f090$21a2f080@Lepc204>
Message-ID: <Pine.LNX.4.44.0403261011490.10431-100000@gannet.stats>

On Fri, 26 Mar 2004, Joao Pedro W. de Azevedo wrote:

> I would like to know if R has any tools to estimate a finite mixture model.

It has several.  It is strange you did not want to know what they are, but
for the sake of others who may be intrigued:

See script ch16 in MASS for bivariate mixtures.
See package mclust of multivariate normal mixtures.
See package flexmix for mixtures of regression models.
See package mda for use in discriminant analysis.

Also packages fpc, moc, nor1mix, wle (and maybe more).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Mar 26 11:38:44 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 26 Mar 2004 11:38:44 +0100
Subject: [R] Error : sink stack is full
In-Reply-To: <406401B8.2000404@lancaster.ac.uk>
References: <488C02265C6AD611BF200002A542182F05B593B8@irnts22.ifp.fr>
	<406401B8.2000404@lancaster.ac.uk>
Message-ID: <16484.2100.990982.304762@gargle.gargle.HOWL>

>>>>> "BaRow" == Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
>>>>>     on Fri, 26 Mar 2004 10:11:04 +0000 writes:

 ........
 ((nicely explaining everything))
 .......

    BaRow> ............. your code (which wasn't attached
    BaRow> because I think R-news strips out attachments).

R-help (sic) does strip (most) *binary* attachments
{the "General Instructions" 
 http://www.r-project.org/mail.html#instructions
 mentioning which attachments are allowed.
}
Unfortunately, many (most?) people don't seem to know how to attach
"text/plain" attachments.

Regards,
Martin



From i.visser at uva.nl  Fri Mar 26 12:59:56 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Fri, 26 Mar 2004 12:59:56 +0100
Subject: [R] Console output
Message-ID: <BC89D9CC.C85%i.visser@uva.nl>

Hi all,
I am working on Macos x 10.3 (Panther) to build a package consisting of
C/C++ code that is called from R. In the C-sources I use  several commands
to print info to the console:

I used two different ways:

std::cout << "info\n";
And Rprintf("info\n");

Both work fine when R is run from the command line but neither works when
running Raqau (I did check the preference boxes for console and error output
to be written to the console).

Any suggestions welcome,
ingmar



From Simon.Bond at mrc-bsu.cam.ac.uk  Fri Mar 26 13:12:11 2004
From: Simon.Bond at mrc-bsu.cam.ac.uk (Simon.Bond)
Date: Fri, 26 Mar 2004 12:12:11 +0000 (GMT)
Subject: [R] update/offset
Message-ID: <Pine.GSO.4.58.0403261204330.3473@bach>

I've noticed that the update() function does not seem to work correctly
when offset(..) terms are there:

update(modelwithoffset,  .~.-afixedeffect)

drops the offset term.

I'm using this with a negbin model, but I think it goes wrong with the
update.formula() function.

update.formula
function (old, new, ...)
{
    env <- environment(as.formula(old))
    tmp <- .Internal(update.formula(as.formula(old), as.formula(new)))
    out <- formula(terms.formula(tmp, simplify = TRUE))
    environment(out) <- env
    return(out)
}
<environment: namespace:base>



The .Internal command in the code is where my efforts grind to a halt.


Any suggestions (apart from redefine the new model from scratch) ?

Thanks

Simon Bond.



From adam at ajtee.uklinux.net  Fri Mar 26 14:02:55 2004
From: adam at ajtee.uklinux.net (Adam Tee)
Date: Fri, 26 Mar 2004 13:02:55 +0000
Subject: [R] Fwd: MDS problems [ajtee@ajtee.uklinux.net]
In-Reply-To: <20040326120343.GA1750@ajtee> (from ajtee@ajtee.uklinux.net on
	Fri, Mar 26, 2004 at 12:03:43 +0000)
References: <20040326120343.GA1750@ajtee>
Message-ID: <20040326130255.GA9812@ajtee>

Hi all,


I'm trying to perform an MDS of some data that I have.  When I use
cmdscale everything is fine and I get some interesting results however,
the tends to be low.

What I wnat to do is compare this with the Non-Metric MDS using isoMDS  
or sammon. However, when I try using these I get the following message.

Error in isoMDS(x.dist) : zero or negative distance between objects 2  
and 4

I am using as.dist to convert my full dissimilarity matrix.
Some of the dissimilarities, other than the diagonal, are zero which is  
causing this, is there anything I can do to resolve this.


Below is a sample dissimilarity matrix

0       0       0.0694153       0.187033        0.193339
0       0       0.0146605       0.36829 	0.563406
0.0694153       0.0146605       0       	0.347965
0.187033        0.36829 	0.347965        0


Thanks

Adam



From Bernhard.Pfaff at drkw.com  Fri Mar 26 14:22:48 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 26 Mar 2004 14:22:48 +0100
Subject: [R] Package update: 'urca' version 0.3-3
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730909@ibfftce505.is.de.dresdnerkb.com>

Dear R-list member,

an update of package 'urca' has been uploaded to CRAN (Mirror: Austria). 
In the updated release unit root and cointegration tests encountered in
applied econometric analysis are implemented. The package is written in
'pure' R and utilises S4 classes.

In particular, the Johansen procedure with likelihood ratio tests for the
inclusion of a linear trend, restrictions on beta and/or alpha matrices, as
well as the data used by Johansen, S. and Juselius, K. (1990) in their
seminal paper have been added. 
Where applicable web-links to the original papers/articles and/or data
sources have been included to the help files, too.


Contents (abbreviated list:
***************************

ablrtest                Likelihood ratio test for restrictions on
                        alpha and beta
alrtest                 Likelihood ratio test for restrictions on
                        alpha
blrtest                 Likelihood ratio test for restrictions on beta
ca.jo                   Johansen Procedure for VECM
ca.jo-class             Representation of class 'ca.jo'
ca.po                   Phillips & Ouliaris Cointegration Test
ca.po-class             Representation of class 'ca.po'
cajo.test-class         Representation of class 'cajo.test'
denmark                 Data set for Denmark, Johansen & Juselius
ecb                     Macroeconomic data of the Euro Zone
finland                 Data set for Finland, Johansen & Juseliues
lttest                  Likelihood ratio test for linear trend 
npext                   Nelson & Plosser extended data set
nporg                   Nelson & Plosser original data set
plot-methods            Methods for Function plot in Package 'urca'
plotres                 Graphical inspection of VECM residuals
show-methods            Methods for Function show in Package 'urca'
summary-methods         Methods for Function summary in Package 'urca'
ur.ers                  Elliott, Rothenberg & Stock Unit Root Test
ur.ers-class            Representation of class 'ur.ers'
ur.kpss                 Kwiatkowski et al. Unit Root Test
ur.kpss-class           Representation of class 'ur.kpss'
ur.pp                   Phillips & Perron Unit Root Test
ur.pp-class             Representation of class 'ur.pp'
ur.sp                   Schmidt & Phillips Unit Root Test
ur.sp-class             Representation of class 'ur.sp'
ur.za                   Zivot & Andrews Unit Root Test
ur.za-class             Representation of class 'ur.za' 

Your comments are welcome. Looking forward to hear from,

Best,
Bernhard 

Dr. Bernhard Pfaff
Global Debt Research - Index and Quantitative Strategy
Dresdner Kleinwort Wasserstein
Phone:	+49 (0)69 713 12273 
Mobile: 	na 
Fax:	+49 (0)69 713 19816

<http://www.drkwresearch.com>
Bloomberg: DRKW<GO>



--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express
written permission of the sender. If you are not the intended recipient, please
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From mkondrin at hppi.troitsk.ru  Sat Mar 27 02:00:34 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 26 Mar 2004 17:00:34 -0800
Subject: [R] Omegahat down?
Message-ID: <4064D232.7060702@hppi.troitsk.ru>

Hello!
For 2 days can not connect to www.omegahat.org :(
Something happens?

traceroute to www.omegahat.org (128.105.174.32), 30 hops max, 38 byte 
packets
 ..........
13  144.92.128.196 (144.92.128.196)  161.792 ms  162.260 ms  160.893 ms
14  g1-2.cisco1.cs.wisc.edu (128.105.1.14)  160.996 ms  163.381 ms  
161.632 ms
15  * * *



From mfowle at navicominc.com  Fri Mar 26 15:05:49 2004
From: mfowle at navicominc.com (Mark Fowle)
Date: Fri, 26 Mar 2004 09:05:49 -0500
Subject: [R] Omegahat down?
Message-ID: <00B717603414D21187AD00104B94A2DAB23958@EXCHANGE>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040326/faf92bc6/attachment.pl

From jarioksa at sun3.oulu.fi  Fri Mar 26 15:07:30 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 26 Mar 2004 16:07:30 +0200
Subject: [R] Fwd: MDS problems [ajtee@ajtee.uklinux.net]
In-Reply-To: <20040326130255.GA9812@ajtee>
References: <20040326120343.GA1750@ajtee>  <20040326130255.GA9812@ajtee>
Message-ID: <1080310050.16349.9.camel@biol102145.oulu.fi>

On Fri, 2004-03-26 at 15:02, Adam Tee wrote:
> Hi all,
> 
> 
> I'm trying to perform an MDS of some data that I have.  When I use
> cmdscale everything is fine and I get some interesting results however,
> the tends to be low.
> 
> What I wnat to do is compare this with the Non-Metric MDS using isoMDS  
> or sammon. However, when I try using these I get the following message.
> 
> Error in isoMDS(x.dist) : zero or negative distance between objects 2  
> and 4
> 
The error message is clear: You have some identical sites so that their
distance is zero. I think the canonical solution is to remove the
duplicate cases from the data before calculating the dissimilarities. If
your data frame is called X:

Xuniq <- unique(X)
x.dist <- dist(Xuniq)

(or, as a one-liner: x.dist <- dist(unique(X))

A dirty solution is to lie to isoMDS. The following will do:

isoMDS(x.dist + 1e-5)

But you better not tell anybody that you did this dirty trick.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ligges at statistik.uni-dortmund.de  Fri Mar 26 15:12:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Mar 2004 15:12:37 +0100
Subject: [R] Omegahat down?
In-Reply-To: <00B717603414D21187AD00104B94A2DAB23958@EXCHANGE>
References: <00B717603414D21187AD00104B94A2DAB23958@EXCHANGE>
Message-ID: <40643A55.8010101@statistik.uni-dortmund.de>

Mark Fowle wrote:
> I was wondering the same thing.
> The traceroute seems to end at their border router, which leads me to
> believe that they has a hardware failure.
> Hoping that they get it up in a few days.
> 
> I've checked around and there are a few mirrors but they all just hang, so I
> guess we have to wait...
> 

Some people have had some fun "visiting" other people's machines ...
These machines need to be reinstalled.

Uwe Ligges



From MSchwartz at MedAnalytics.com  Fri Mar 26 15:18:58 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 26 Mar 2004 08:18:58 -0600
Subject: [R] How to add a top X-axis with a different logarithmic scale?
In-Reply-To: <Pine.LNX.4.44.0403251511360.5926-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0403251511360.5926-100000@cezanne.fhcrc.org>
Message-ID: <1080310738.7003.331.camel@localhost.localdomain>

On Thu, 2004-03-25 at 17:42, Itay Furman wrote:
> Thanks, Marc, for your reply.

Happy to help.

> I didn't explain myself well, but thanks to you I think I could 
> phrase my question better.
> 
> I want to be able to compute new tick-positions for the top 
> X-axis.
> 
> Why new ticks positions?
> See in the example below (that uses your contributed minimal
> code :-) what happens to the labels when you
> replace 4e06 with 4.1e06.


OK.  I believe that I have a reasonable way of getting this, while
letting R do the 'heavy lifting'.


x <- c(1.1 * 1:4, 25 * 1:5) / 50e+03
y <- c(0.15 * 1:4, 0.6 + 0.05 * 1:5)

old.par <- par(no.readonly=TRUE)
xlim <- range(x)
ylim <- c(0, 1)

plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)

# Set scaling factor
sf <- 4.1e06

# Now set up new upper x axis range based upon sf
xlim.3 <- xlim * sf

# Now create axis tick mark positions for the new scale
# We will use axTicks() and need to consider that when log scales
# are in use, things get a little hairy. So:

# We need to define a new par("xaxp") based upon xlim.3
# We also need to define a new par("usr")[1:2], which is the
# range of the x axis. When log scales are in use, the actual
# range is 10 ^ par("usr")[1:2], so we take these values and rescale
# using 'sf'. We then convert the result back to log scales using
# log10() to get 'usr':
tm <- axTicks(3, axp = c(range(xlim.3), 3), 
              usr = log10(10 ^ par("usr")[1:2] * sf), 
              log = TRUE)

# Now we can draw axis 3, using the adjusted scaling
# We need to adjust the scaled tick mark positions back
# to the actual range of the x axis, so 'at' is adjusted
# here by the sf, but the labels stay as rescaled:
axis(3, at = tm / sf, labels = tm)


par(old.par)




> Also, I assume from your example that using plot.new() + 
> plot.window() + lines(), instead of plot() will not make a 
> difference in the solution. As I mentioned in _my_ example code:
> for a production I will overlay several curves and will
> use lines().

Correct. The key here with multiple series is that you need to consider
the appropriate ranges for each x and y series when combined, so that
the plot region is properly scaled and all values are within the plot
region. So you need to account for overlapping (but presumably not
exactly the same) x and y axis ranges. 

The R function matplot() is probably the best way of doing this with a
single function. See ?matplot.

In the above code, replace the use of plot() with matplot(). The first
two arguments in matplot() will be your x vectors as a matrix and your y
vectors as a matrix. Let's say you have 3 series. If you only have one
set of y values, skip the cbind() step and just use 'y'. matplot() will
recycle the y values as required.

You would also need to adjust the calculation of 'xlim' as:

xlim <- range(c(x1, x2, x3))

We'll keep 'ylim' as is for now.

Combine your x values and y values into matrices:

x <- cbind(x1, x2, x3)
y <- cbind(y1, y2, y3)

Then use matplot():

matplot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)

Now do the adjustments for the upper x axis:

sf <- 4.1e06
xlim.3 <- xlim * sf
tm <- axTicks(3, axp = c(range(xlim.3), 3), 
              usr = log10(10 ^ par("usr")[1:2] * sf), 
              log = TRUE)
axis(3, at = tm / sf, labels = tm)


I think that should get you there. Be aware that with log scaling,
substantial changes in ranges can have a material impact on the axis
related calculations, so you may need to play around with things if you
use scaling factors that are substantially larger or smaller than what
we have here. Some scaling values may cause problems with this approach.

I hope that helps.

Marc



From sdavis2 at mail.nih.gov  Fri Mar 26 15:31:30 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 26 Mar 2004 09:31:30 -0500
Subject: [R] Gregmisc 'running' question
Message-ID: <BC89A8F2.62AF%sdavis2@mail.nih.gov>

Just a quick and probably simple question:

> dat <- rnorm(500,sd=1+(1:500)/500)
> fun <- function(x) t.test(x)$p.value
> running(dat,width=50,fun=fun,allow=T)
Error in t.test.default(x) : not enough x observations
> running(dat,width=50,fun=fun,allow=F)
Error in t.test.default(x) : not enough x observations
> fun2 <- function(x) mean(x)
> running(dat,width=50,fun=fun2,allow=T)
         1:1          1:2          1:3          1:4          1:5
1:6 
-0.334134613 -0.626595581 -0.368967457 -0.113737178 -0.057448771
0.228643936 
         1:7          1:8          1:9         1:10         1:11
1:12 
-0.058807689  0.021762463 -0.063805657  0.031931121  0.080465708
0.087062800 
....

However, this works fine.

> t.test(dat[1:50])$p.value
[1] 0.1661845

Why doesn't t.test work with running here?

Thanks,
Sean



From Timur.Elzhov at jinr.ru  Fri Mar 26 15:56:30 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Fri, 26 Mar 2004 17:56:30 +0300
Subject: [R] Using R's LAPACK & Related files in Visual C++
In-Reply-To: <20040326044030.78805.qmail@web41408.mail.yahoo.com>
References: <20040326044030.78805.qmail@web41408.mail.yahoo.com>
Message-ID: <20040326145630.GA990@nf034.jinr.ru>

On Thu, Mar 25, 2004 at 08:40:30PM -0800, Greg Tarpinian wrote:

> I am a relative newcomer to both the R and C/C++ software worlds --
> I'm taking a C Programming class currently.  I noticed the other day
> that the
> 
> C:\Program Files\R1_8_1\src\include\R_ext
> 
> directory on my WinXP box has the header files
> 
> BLAS.h
> Lapack.h
> Linpack.h
> RLapack.h
> 
> I am interested in (perhaps) using one or more of these header files
> in a straight C program I'm working on in Visual C++ .NET.  (I need
> matrix inversion, svd, generalized inverse, Cholesky decomposition,
> and a few other matrix features for a class project and this is the
> first time I've ever worked with header files.) My attempts to get the
> freely available CLAPACK3-Windows.zip on
> 
> http://www.netlib.org/clapack/

AFAIK, R uses native FORTRAN l(a,in)pack codes :)  Try it, I guess you'll
be able to call their functions, this way:
  void dposl_(double*, int*, int*, double*); //for `dposl' Fortran function

Note underscore `_' under function name! ;)

--
WBR,
Timur.



From pnick at virgilio.it  Fri Mar 26 15:36:13 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Fri, 26 Mar 2004 15:36:13 +0100
Subject: [R] (no subject)
Message-ID: <405A65490000CD87@ims2b.cp.tin.it>

i need to know how to estimate a linear regression whose coefficients sum
to zero



From malone at alumni.duke.edu  Fri Mar 26 15:59:54 2004
From: malone at alumni.duke.edu (Patrick S. Malone)
Date: Fri, 26 Mar 2004 09:59:54 -0500
Subject: [R] Novice with trouble running PAN
In-Reply-To: <20040326145630.GA990@nf034.jinr.ru>
References: <20040326044030.78805.qmail@web41408.mail.yahoo.com>
	<20040326145630.GA990@nf034.jinr.ru>
Message-ID: <opr5g494eeve5okj@smtp.mindspring.com>

Good morning.

I'm a complete tyro at using R, having downloaded it yesterday for the  
sole purpose of running a problem in PAN, Joe Schafer's program for  
multiple imputation of multilevel data.

I'm starting to get the hang of the command interface, and *think* I have  
all the input matrices set up correctly.  However, on invoking PAN, I'm  
getting a "subscript out of range" error.

Is there a patient soul here with experience with PAN who could help me  
iron the kinks out?

Thanks,
Pat Malone

-- 
Patrick S. Malone, Ph.D., Research Scholar
Duke University Center for Child and Family Policy
Durham, North Carolina, USA
e-mail: malone at alumni.duke.edu
http://www.duke.edu/~malone/



From ivo.welch at yale.edu  Fri Mar 26 16:15:48 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Fri, 26 Mar 2004 10:15:48 -0500
Subject: [R] stop() vs. error() ?
Message-ID: <40644924.5070106@yale.edu>


Why does stop("we are done") print
    "Error in eval.with.vis(expr, envir, enclos) :"  ?
It would seem to me that a plain stop() is not an error, and that it 
would make more sense to have an error() function that is different from 
a stop().  Is there a rationale here that I am missing?

sincerely,  /iaw



From malone at alumni.duke.edu  Fri Mar 26 16:23:33 2004
From: malone at alumni.duke.edu (Patrick S. Malone)
Date: Fri, 26 Mar 2004 10:23:33 -0500
Subject: [R] Novice with trouble running PAN
In-Reply-To: <opr5g494eeve5okj@smtp.mindspring.com>
References: <20040326044030.78805.qmail@web41408.mail.yahoo.com>
	<20040326145630.GA990@nf034.jinr.ru>
	<opr5g494eeve5okj@smtp.mindspring.com>
Message-ID: <opr5g6djiave5okj@smtp.mindspring.com>

I got a suggestion offlist that I go ahead and post my R commands:

lib(pan)
y <- read.table("c:/temp/y.dat", header=TRUE, na.string='.')
subj <- read.table("c:/temp/subj.dat", header=TRUE)
pred <- read.table("c:/temp/pred.dat", header=TRUE, na.strings='.')
xcol <- c(1, 2, 3, 4, 5, 6, 7, 8)
zcol <- c(1, 6, 7, 8)
prior <- list(a=1, Binv=1, c=4, Dinv=4)
result <- pan(y,subj,pred,xcol,zcol,prior,seed=3298,iter=1000)


y.dat is a vector with 504 entries on a continuous variable, about 10% of  
them missing (.).

subj.dat is a vector with 504 entries -- the second-level ID variables  
corresponding to the individual records.

pred.dat has 504 rows and 8 columns -- the first column is a unit vector,  
the remainder are dummy codes.  There are no missings; the na.strings was  
left over from a prior run.

Thanks,
Pat Malone


On Fri, 26 Mar 2004 09:59:54 -0500, Patrick S. Malone  
<malone at alumni.duke.edu> wrote:

> Good morning.
>
> I'm a complete tyro at using R, having downloaded it yesterday for the  
> sole purpose of running a problem in PAN, Joe Schafer's program for  
> multiple imputation of multilevel data.
>
> I'm starting to get the hang of the command interface, and *think* I  
> have all the input matrices set up correctly.  However, on invoking PAN,  
> I'm getting a "subscript out of range" error.
>
> Is there a patient soul here with experience with PAN who could help me  
> iron the kinks out?
>
> Thanks,
> Pat Malone
>



-- 
Patrick S. Malone, Ph.D., Research Scholar
Duke University Center for Child and Family Policy
Durham, North Carolina, USA
e-mail: malone at alumni.duke.edu
http://www.duke.edu/~malone/



From pnick at virgilio.it  Fri Mar 26 16:23:33 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Fri, 26 Mar 2004 16:23:33 +0100
Subject: [R] (senza oggetto)
Message-ID: <405A65490000CF88@ims2b.cp.tin.it>

i need to know how to estimate a linear regression whose coefficients sum
to zero



From andy_liaw at merck.com  Fri Mar 26 16:25:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 26 Mar 2004 10:25:09 -0500
Subject: [R] stop() vs. error() ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AA2@usrymx25.merck.com>

Please do read the documentation of the functions you are trying to use.
The description in ?stop says:

     'stop' stops execution of the current expression and executes an
     error action.

stop() is how error is flagged in R (and S in general).  If that's not what
you want, try something else.  And the `something else' depends on what you
want, which has not been described in detail.

Andy

> From: ivo welch
> 
> Why does stop("we are done") print
>     "Error in eval.with.vis(expr, envir, enclos) :"  ?
> It would seem to me that a plain stop() is not an error, and that it 
> would make more sense to have an error() function that is 
> different from 
> a stop().  Is there a rationale here that I am missing?
> 
> sincerely,  /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From pnick at virgilio.it  Fri Mar 26 16:29:55 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Fri, 26 Mar 2004 16:29:55 +0100
Subject: [R] regression problem
Message-ID: <405A65490000CFD4@ims2b.cp.tin.it>

i need to know how to estimate a linear regression whose coefficients sum
to zero



From stanimura-ngs at umin.ac.jp  Fri Mar 26 16:51:33 2004
From: stanimura-ngs at umin.ac.jp (Susumu =?ISO-2022-JP?B?VGFuaW11cmEvGyRCQytCPBsoQiAbJEI/OBsoQg==?=)
Date: Sat, 27 Mar 2004 00:51:33 +0900
Subject: [R] With which version of XFree86 can R compile?
Message-ID: <20040327005133.1abb199a.stanimura-ngs@umin.ac.jp>

Hi there,

No information was found which version of XFree86 get along with R.  I
could not compile R version 1.8.1 with XFree86 4.3.99.902.

Here is error message.

gcc -I. -I../../../src/include -I../../../src/include -DI18N_MB -I/usr/X11R6/include -I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES -mieee-fp -fPIC  -O2 -pipe -march=i386 -mcpu=i686 -c dataentry_mb.c -o dataentry_mb.lo
In file included from dataentry_mb.c:35:
/usr/X11R6/include/X11/Xlib.h:1389: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1477: error: syntax error before "char"
/usr/X11R6/include/X11/Xlib.h:1505: error: syntax error before "_Xconst"
[snip]
/usr/X11R6/include/X11/Xutil.h:678: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xutil.h:801: error: syntax error before "_Xconst"
In file included from xrm.h:5,
                 from dataentry_mb.c:41:
/usr/X11R6/include/X11/Xresource.h:94: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:98: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:122: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:127: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:194: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xresource.h:203: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xresource.h:208: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xresource.h:214: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xresource.h:227: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xresource.h:269: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:273: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:279: error: syntax error before "char"
/usr/X11R6/include/X11/Xresource.h:284: error: syntax error before "_Xconst"
/usr/X11R6/include/X11/Xresource.h:352: error: syntax error before "_Xconst"
dataentry_mb.c: In function `GetKey':
dataentry_mb.c:1276: warning: passing arg 1 of `XLookupString' from incompatible pointer type
dataentry_mb.c:1276: warning: passing arg 4 of `XLookupString' from incompatible pointer type
dataentry_mb.c: In function `GetCharP':
dataentry_mb.c:1285: warning: passing arg 1 of `XLookupString' from incompatible pointer type
dataentry_mb.c:1285: warning: passing arg 4 of `XLookupString' from incompatible pointer type
dataentry_mb.c: In function `doControl':
dataentry_mb.c:1306: warning: passing arg 1 of `XLookupString' from incompatible pointer type
dataentry_mb.c:1306: warning: passing arg 4 of `XLookupString' from incompatible pointer type
dataentry_mb.c: In function `RefreshKeyboardMapping':
dataentry_mb.c:1335: warning: passing arg 1 of `XRefreshKeyboardMapping' from incompatible pointer type
make[4]: *** [dataentry_mb.lo] Error 1
make[4]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules/X11'
make[3]: *** [R] Error 2
make[3]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules/X11'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src'
make: *** [R] Error 1
error: Bad exit status from /var/tmp/rpm-tmp.85917 (%build)
 
RPM build errors:
    Bad exit status from /var/tmp/rpm-tmp.85917 (%build)



From JonesW at kssg.com  Fri Mar 26 16:44:47 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Fri, 26 Mar 2004 15:44:47 -0000
Subject: [R] model fitting
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F1052@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040326/f23ec671/attachment.pl

From macq at llnl.gov  Fri Mar 26 16:57:48 2004
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 26 Mar 2004 07:57:48 -0800
Subject: [R] Gregmisc 'running' question
In-Reply-To: <BC89A8F2.62AF%sdavis2@mail.nih.gov>
References: <BC89A8F2.62AF%sdavis2@mail.nih.gov>
Message-ID: <p06002000bc8a025b5971@[128.115.153.6]>

It says "not enough x observations". That's pretty clear.
So ask yourself, what is the minimum number of observations one needs 
to do a t test?

Try this to see how many observations t.test was given each time.

    running(dat,width=50,fun=function(x) length(x),allow=T)

or this
   running(dat,width=50,fun=function(x) cat('x:',x,'\n\n'),allow=T)

-Don

At 9:31 AM -0500 3/26/04, Sean Davis wrote:
>Just a quick and probably simple question:
>
>>  dat <- rnorm(500,sd=1+(1:500)/500)
>>  fun <- function(x) t.test(x)$p.value
>>  running(dat,width=50,fun=fun,allow=T)
>Error in t.test.default(x) : not enough x observations
>>  running(dat,width=50,fun=fun,allow=F)
>Error in t.test.default(x) : not enough x observations
>>  fun2 <- function(x) mean(x)
>>  running(dat,width=50,fun=fun2,allow=T)
>          1:1          1:2          1:3          1:4          1:5
>1:6
>-0.334134613 -0.626595581 -0.368967457 -0.113737178 -0.057448771
>0.228643936
>          1:7          1:8          1:9         1:10         1:11
>1:12
>-0.058807689  0.021762463 -0.063805657  0.031931121  0.080465708
>0.087062800
>....
>
>However, this works fine.
>
>>  t.test(dat[1:50])$p.value
>[1] 0.1661845
>
>Why doesn't t.test work with running here?
>
>Thanks,
>Sean
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From zxz1818 at yahoo.com.cn  Fri Mar 26 17:14:09 2004
From: zxz1818 at yahoo.com.cn (=?gb2312?q?Louis?=)
Date: Sat, 27 Mar 2004 00:14:09 +0800 (CST)
Subject: [R] How to do the significant test on Local Moran's I
Message-ID: <20040326161409.7663.qmail@web60901.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040327/5e03a46b/attachment.pl

From apiszcz at solarrain.com  Fri Mar 26 17:18:23 2004
From: apiszcz at solarrain.com (Al Piszcz)
Date: Fri, 26 Mar 2004 11:18:23 -0500 (EST)
Subject: [R] Plot difference between PNG and X11
Message-ID: <Pine.LNX.4.58.0403261117370.31307@l1>


I am rendering a plot using boxplot, mtext, abline, etc.

The X11 view of the plot looks correct.

When I change to the png device to:
  png(filename="bxplot.png",width=640,height=640)

I get half of a character label plotted in the lower left
corner in the resulting .png file.
The character fragment started to appear in png when
I started using mtext.

mtext(labelvector,side=1,at=c(xi,0),line=2)

Any thoughts on items to try? Thx.



From pburns at pburns.seanet.com  Fri Mar 26 17:23:19 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 26 Mar 2004 16:23:19 +0000
Subject: [R] regression problem
References: <405A65490000CFD4@ims2b.cp.tin.it>
Message-ID: <406458F7.4070901@pburns.seanet.com>

Assuming that you want to estimate via least squares, you can
do something like this:

reg.coefsum <- function(x, y, start=coef(lm.fit(x, y)), coefsum=0, 
penalty=1000) {
        subfun.objective <- function(coef){
                sum((y - x %*% coef)^2) + abs(sum(coef)) * penalty
        }
        opt <- optim(start, subfun.objective, method='BFGS')
        ans <- list(coefficients=opt$par)
        ans
}

You can enhance the return value and make other improvements,
like check that the coefficient sum is accurate enough for you.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

pnick at virgilio.it wrote:

>i need to know how to estimate a linear regression whose coefficients sum
>to zero
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From rossini at blindglobe.net  Fri Mar 26 17:30:37 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 26 Mar 2004 08:30:37 -0800
Subject: [R] With which version of XFree86 can R compile?
In-Reply-To: <20040327005133.1abb199a.stanimura-ngs@umin.ac.jp> (Susumu's
	message of "Sat, 27 Mar 2004 00:51:33 +0900")
References: <20040327005133.1abb199a.stanimura-ngs@umin.ac.jp>
Message-ID: <85wu57a7v6.fsf@servant.blindglobe.net>


It isn't clear that R is currently friendly to the new XFree 4.4
license.  I believe that there is an advertising clause for linking --
does anyone know if this has been addressed or clarified?  I've not
been following those issues for a few months.

best,
-tony

Susumu =?ISO-2022-JP?B?VGFuaW11cmEvGyRCQytCPBsoQiAbJEI/OBsoQg==?= <stanimura-ngs at umin.ac.jp> writes:

> Hi there,
>
> No information was found which version of XFree86 get along with R.  I
> could not compile R version 1.8.1 with XFree86 4.3.99.902.
>
> Here is error message.
>
> gcc -I. -I../../../src/include -I../../../src/include -DI18N_MB -I/usr/X11R6/include -I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES -mieee-fp -fPIC  -O2 -pipe -march=i386 -mcpu=i686 -c dataentry_mb.c -o dataentry_mb.lo
> In file included from dataentry_mb.c:35:
> /usr/X11R6/include/X11/Xlib.h:1389: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1477: error: syntax error before "char"
> /usr/X11R6/include/X11/Xlib.h:1505: error: syntax error before "_Xconst"
> [snip]
> /usr/X11R6/include/X11/Xutil.h:678: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xutil.h:801: error: syntax error before "_Xconst"
> In file included from xrm.h:5,
>                  from dataentry_mb.c:41:
> /usr/X11R6/include/X11/Xresource.h:94: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:98: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:122: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:127: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:194: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xresource.h:203: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xresource.h:208: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xresource.h:214: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xresource.h:227: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xresource.h:269: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:273: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:279: error: syntax error before "char"
> /usr/X11R6/include/X11/Xresource.h:284: error: syntax error before "_Xconst"
> /usr/X11R6/include/X11/Xresource.h:352: error: syntax error before "_Xconst"
> dataentry_mb.c: In function `GetKey':
> dataentry_mb.c:1276: warning: passing arg 1 of `XLookupString' from incompatible pointer type
> dataentry_mb.c:1276: warning: passing arg 4 of `XLookupString' from incompatible pointer type
> dataentry_mb.c: In function `GetCharP':
> dataentry_mb.c:1285: warning: passing arg 1 of `XLookupString' from incompatible pointer type
> dataentry_mb.c:1285: warning: passing arg 4 of `XLookupString' from incompatible pointer type
> dataentry_mb.c: In function `doControl':
> dataentry_mb.c:1306: warning: passing arg 1 of `XLookupString' from incompatible pointer type
> dataentry_mb.c:1306: warning: passing arg 4 of `XLookupString' from incompatible pointer type
> dataentry_mb.c: In function `RefreshKeyboardMapping':
> dataentry_mb.c:1335: warning: passing arg 1 of `XRefreshKeyboardMapping' from incompatible pointer type
> make[4]: *** [dataentry_mb.lo] Error 1
> make[4]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules/X11'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules/X11'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src'
> make: *** [R] Error 1
> error: Bad exit status from /var/tmp/rpm-tmp.85917 (%build)
>  
> RPM build errors:
>     Bad exit status from /var/tmp/rpm-tmp.85917 (%build)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jeff.hamann at forestinformatics.com  Fri Mar 26 17:34:36 2004
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Fri, 26 Mar 2004 08:34:36 -0800
Subject: [R] multicolumn sort on dataframe?
Message-ID: <00a601c41350$3d1d7a70$0c00a8c0@rodan>

I couldn't find any reference to this in the FAQ, but is it possible to sort
a dataframe by multiple columns?

I've created some code, similar to the following:

nspr.code <- sp.results$sp.code[order( sp.results$sp.code )]
nspr.tpa <- sp.results$tpa[order( sp.results$sp.code )]

nspr.code <- as.character( levels( nspr.code ) )[nspr.code]
nspr.tpa <- as.numeric( levels( nspr.tpa ) )[nspr.tpa]

hope <- as.data.frame( cbind( nspr.code, as.numeric(nspr.tpa) ) )

and it seems to work, but I have dataframes that I would like to sort on
using multiple columns (numeric and character). Something like :

newframe <- sort( data=frame, list=c(plot,plant,sp) )

Or am I just barking up the wrong tree?

Jeff.


---
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
541-754-1428
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From pburns at pburns.seanet.com  Fri Mar 26 17:31:52 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 26 Mar 2004 16:31:52 +0000
Subject: [R] regression problem
References: <405A65490000CFD4@ims2b.cp.tin.it>
	<406458F7.4070901@pburns.seanet.com>
Message-ID: <40645AF8.9080507@pburns.seanet.com>

The function has a "coefsum" argument but never uses it.
The objective is supposed to be:

               sum((y - x %*% coef)^2) + abs(sum(coef) - coefsum) * penalty

Patrick Burns wrote:

> Assuming that you want to estimate via least squares, you can
> do something like this:
>
> reg.coefsum <- function(x, y, start=coef(lm.fit(x, y)), coefsum=0, 
> penalty=1000) {
>        subfun.objective <- function(coef){
>                sum((y - x %*% coef)^2) + abs(sum(coef)) * penalty
>        }
>        opt <- optim(start, subfun.objective, method='BFGS')
>        ans <- list(coefficients=opt$par)
>        ans
> }
>
> You can enhance the return value and make other improvements,
> like check that the coefficient sum is accurate enough for you.
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> pnick at virgilio.it wrote:
>
>> i need to know how to estimate a linear regression whose coefficients 
>> sum
>> to zero
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>>  
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From JonesW at kssg.com  Fri Mar 26 17:26:04 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Fri, 26 Mar 2004 16:26:04 -0000
Subject: [R] regression problem
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F1054@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040326/3f48ff0b/attachment.pl

From ndesnoyers at att.net  Fri Mar 26 17:41:32 2004
From: ndesnoyers at att.net (ndesnoyers@att.net)
Date: Fri, 26 Mar 2004 16:41:32 +0000
Subject: [R] (marginal) dot diagram
Message-ID: <032620041641.22750.15c5@att.net>

I am attempting to plot something called a (marginal) dot diagram which is a regular scatterplot with an extra set of plots in the margins along both the X and Y axes. I have looked everywhere I can think for information. Can anyone give me any suggestions? Is there a single command that generates it or do I have to generate a regular scatterplot then add the additional plots in the margins?

Thanks in advance for your help.

Neil Desnoyers



From spencer.graves at pdf.com  Fri Mar 26 17:44:55 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 26 Mar 2004 08:44:55 -0800
Subject: [R] multicolumn sort on dataframe?
In-Reply-To: <00a601c41350$3d1d7a70$0c00a8c0@rodan>
References: <00a601c41350$3d1d7a70$0c00a8c0@rodan>
Message-ID: <40645E07.8060105@pdf.com>

?order

Jeff D. Hamann wrote:

>I couldn't find any reference to this in the FAQ, but is it possible to sort
>a dataframe by multiple columns?
>
>I've created some code, similar to the following:
>
>nspr.code <- sp.results$sp.code[order( sp.results$sp.code )]
>nspr.tpa <- sp.results$tpa[order( sp.results$sp.code )]
>
>nspr.code <- as.character( levels( nspr.code ) )[nspr.code]
>nspr.tpa <- as.numeric( levels( nspr.tpa ) )[nspr.tpa]
>
>hope <- as.data.frame( cbind( nspr.code, as.numeric(nspr.tpa) ) )
>
>and it seems to work, but I have dataframes that I would like to sort on
>using multiple columns (numeric and character). Something like :
>
>newframe <- sort( data=frame, list=c(plot,plant,sp) )
>
>Or am I just barking up the wrong tree?
>
>Jeff.
>
>
>---
>Jeff D. Hamann
>Forest Informatics, Inc.
>PO Box 1421
>Corvallis, Oregon USA 97339-1421
>541-754-1428
>jeff.hamann at forestinformatics.com
>www.forestinformatics.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at MedAnalytics.com  Fri Mar 26 17:50:29 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 26 Mar 2004 10:50:29 -0600
Subject: [R] (marginal) dot diagram
In-Reply-To: <032620041641.22750.15c5@att.net>
References: <032620041641.22750.15c5@att.net>
Message-ID: <1080319829.22596.23.camel@localhost.localdomain>

On Fri, 2004-03-26 at 10:41, ndesnoyers at att.net wrote:
> I am attempting to plot something called a (marginal) dot diagram
> which is a regular scatterplot with an extra set of plots in the
> margins along both the X and Y axes. I have looked everywhere I can
> think for information. Can anyone give me any suggestions? Is there a
> single command that generates it or do I have to generate a regular
> scatterplot then add the additional plots in the margins?
> 
> Thanks in advance for your help.
> 
> Neil Desnoyers


See the last example in the help for ?layout, which will provide some
guidance. In that case, it is creating a scatterplot with marginal
histograms.

HTH,

Marc Schwartz



From Simon.Fear at synequanon.com  Fri Mar 26 17:53:31 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 26 Mar 2004 16:53:31 -0000
Subject: [R] (marginal) dot diagram
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE5720118F47B@synequanon01>

Also try ?rug

> On Fri, 2004-03-26 at 10:41, ndesnoyers at att.net wrote:
> > I am attempting to plot something called a (marginal) dot diagram
> > which is a regular scatterplot with an extra set of plots in the
> > margins along both the X and Y axes. I have looked everywhere I can
> > think for information. Can anyone give me any suggestions? 
> Is there a
> > single command that generates it or do I have to generate a regular
> > scatterplot then add the additional plots in the margins?
> > 
> > Thanks in advance for your help.
> > 
> > Neil Desnoyers
> 
> 
> See the last example in the help for ?layout, which will provide some
> guidance. In that case, it is creating a scatterplot with marginal
> histograms.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From ggrothendieck at myway.com  Fri Mar 26 17:54:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 26 Mar 2004 16:54:32 +0000 (UTC)
Subject: [R] regression problem
References: <405A65490000CFD4@ims2b.cp.tin.it>
Message-ID: <loom.20040326T173923-635@post.gmane.org>

 <pnick <at> virgilio.it> writes:

> 
> i need to know how to estimate a linear regression whose coefficients sum
> to zero
> 


Transform the model:

y = Xb + error

to

y = XPb + error 

where P is the projection onto the orthogonal complement of the vector 1.
Solve this regression for b and then Pb will be your desired coefficients.
In R this would be:

n <- ncol(X)
P <- diag(n)-matrix(1,n,n)/n
z <- lm(y ~.-1, data=(X%*%P))
b <- coef(z)
b[is.na(b)] <- 0
P %*% b



From malbani at fas.harvard.edu  Fri Mar 26 18:01:40 2004
From: malbani at fas.harvard.edu (Marco Albani)
Date: Fri, 26 Mar 2004 12:01:40 -0500
Subject: [R] color.ramp in maptools
Message-ID: <406461F4.4060804@fas.harvard.edu>

Dear list members,

I am trying to use the maptools library to display geographical data. At 
the moment I have some trouble understanding how the " auxvar " variable 
is supposed to be used in the plot.Map function.

I am using R Version 1.8.1  (2003-11-21) on Linux

Looking at the plot.Map function itself, I see that it calls a 
color.ramp function (I am reporting only the relevant part here):

...
     if (attr(theMap$Shapes, "shp.type") == "poly") {
         if (!is.null(auxvar) && nclass > 1) {
             col.rmp <- color.ramp(nclass, nvec = auxvar)
             for (i in recs) {
...


I don't seem to be able to get any information on this color.ramp 
function. In fact the function doesn't seem to exist if I search for it 
with ls()

Does anyone have any insight?


-- 
Marco Albani, Ph.D.
Postdoctoral Fellow
Dept. of Organismic and Evolutionary Biology
Harvard University
HUH 22 Divinity Avenue
Cambridge, MA
02138-2094 USA

Tel: +1 617 495 1621
Fax: +1 617 495 9484

http://www.people.fas.harvard.edu/~malbani



From rolf at math.unb.ca  Fri Mar 26 18:04:31 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 26 Mar 2004 13:04:31 -0400 (AST)
Subject: [R] (Off topic) Text on General Linear Model.
Message-ID: <200403261704.i2QH4Vxi014471@erdos.math.unb.ca>


A student (at another university) has contacted me for advice about a
good text dealing with the general linear model (which she needs to
know about for her thesis defense).

[One might say that if she used the GLM in her thesis she ***ought***
to know about the concept already, but ought to and does are very
different, so ne'er mind.]

The lady is an engineering student so I guess she'd prefer something
that wasn't too heavy on the theory, but rather explained the ideas
in a clear and intuitive manner.  Pinheiro and Bates is one book that
springs to mind, but I think she might find it a bit overwhelming.

I don't expect that there are many books totally devoted to the topic
of general linear models as such, so I guess I'm really looking for
books which have a chapter or two devoted to general linear models.
Graybill, ``Theory and Applications of the Linear Model'' has a such
a chapter, but I find it very terse and overly formal.  I imagine
that the student in question would react the same way only more so.

I don't have many other ideas to offer this student, so I thought I
would call on the collective wisdom of the R community for
suggestions.

Since this is off-topic, it would probably be best to email me
directly:

		rolf at math.unb.ca

rather than replying to the list.

Thank you for you indulgence.

					cheers,

						Rolf Turner



From eesteves at ualg.pt  Fri Mar 26 18:07:35 2004
From: eesteves at ualg.pt (eesteves@ualg.pt)
Date: Fri, 26 Mar 2004 17:07:35 +0000
Subject: [R] lme question
Message-ID: <1080320855.40646357a3f6c@wmail.ualg.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040326/d1db5e08/attachment.pl

From MSchwartz at MedAnalytics.com  Fri Mar 26 18:11:39 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 26 Mar 2004 11:11:39 -0600
Subject: [R] With which version of XFree86 can R compile?
In-Reply-To: <85wu57a7v6.fsf@servant.blindglobe.net>
References: <20040327005133.1abb199a.stanimura-ngs@umin.ac.jp>
	<85wu57a7v6.fsf@servant.blindglobe.net>
Message-ID: <1080321099.22596.47.camel@localhost.localdomain>

On Fri, 2004-03-26 at 10:30, A.J. Rossini wrote:
> It isn't clear that R is currently friendly to the new XFree 4.4
> license.  I believe that there is an advertising clause for linking --
> does anyone know if this has been addressed or clarified?  I've not
> been following those issues for a few months.
> 
> best,
> -tony

<snip>

It has not been resolved and most? distros are moving away from XFree
4.4 as a result. There was a thread on ./ on this back in February:

http://yro.slashdot.org/article.pl?sid=04/02/18/131223&tid=104

Fedora Core 2 (which is in beta testing at the moment) has already moved
to X.org's X server, which is based upon XFree 3.99.??, which is the
last update before the license change.

This change has (not surprisingly) resulted in a plethora of package
dependency issues, which are in the process of being resolved and
concerns of course with video drivers and the like.

It is not clear to me from the information that Susumu provided, whether
this is the cause of the problem. I have not seen any indication that,
other than the licensing issues, there are other substantive changes
that would result in the problems posted here.

It may very well be, but it may also simply be a corrupted installation
of the X libraries and/or devel source.

The indications from Mike Harris at RH/FC is that the adoption of the
new X.org server would be "transparent" to the end user. If so, then the
problems posted here "should not be" related to the XFree version
change.

However, that premise would need to be validated.

HTH,

Marc Schwartz



From bates at stat.wisc.edu  Fri Mar 26 18:23:18 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 26 Mar 2004 11:23:18 -0600
Subject: [R] Omegahat down?
In-Reply-To: <40643A55.8010101@statistik.uni-dortmund.de>
References: <00B717603414D21187AD00104B94A2DAB23958@EXCHANGE>
	<40643A55.8010101@statistik.uni-dortmund.de>
Message-ID: <6rfzbvmsjd.fsf@bates4.stat.wisc.edu>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Mark Fowle wrote:
> > I was wondering the same thing.
> > The traceroute seems to end at their border router, which leads me to
> > believe that they has a hardware failure.
> > Hoping that they get it up in a few days.
> > I've checked around and there are a few mirrors but they all just
> > hang, so I
> 
> > guess we have to wait...
> >
> 
> 
> Some people have had some fun "visiting" other people's machines ...
> These machines need to be reinstalled.

The machine that was serving as Omegahat.org was compromised.  We were
going to move that site to another machine anyway so this attack has
resulted in a somewhat accelerated schedule for the move.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From huamanr at mef.gob.pe  Fri Mar 26 18:40:21 2004
From: huamanr at mef.gob.pe (Ricardo Huaman)
Date: Fri, 26 Mar 2004 12:40:21 -0500
Subject: [R] Help import data 
Message-ID: <016e01c41359$69c2fce0$ea09000a@W00RHUAMAN>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040326/09a62656/attachment.pl

From bates at stat.wisc.edu  Fri Mar 26 18:35:59 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 26 Mar 2004 11:35:59 -0600
Subject: [R] Using R's LAPACK & Related files in Visual C++
In-Reply-To: <20040326145630.GA990@nf034.jinr.ru>
References: <20040326044030.78805.qmail@web41408.mail.yahoo.com>
	<20040326145630.GA990@nf034.jinr.ru>
Message-ID: <6rbrmjmry8.fsf@bates4.stat.wisc.edu>

Timur Elzhov <Timur.Elzhov at jinr.ru> writes:

> On Thu, Mar 25, 2004 at 08:40:30PM -0800, Greg Tarpinian wrote:
> 
> > I am a relative newcomer to both the R and C/C++ software worlds --
> > I'm taking a C Programming class currently.  I noticed the other day
> > that the
> > 
> > C:\Program Files\R1_8_1\src\include\R_ext
> > 
> > directory on my WinXP box has the header files
> > 
> > BLAS.h
> > Lapack.h
> > Linpack.h
> > RLapack.h
> > 
> > I am interested in (perhaps) using one or more of these header files
> > in a straight C program I'm working on in Visual C++ .NET.  (I need
> > matrix inversion, svd, generalized inverse, Cholesky decomposition,
> > and a few other matrix features for a class project and this is the
> > first time I've ever worked with header files.) My attempts to get the
> > freely available CLAPACK3-Windows.zip on
> > 
> > http://www.netlib.org/clapack/
> 
> AFAIK, R uses native FORTRAN l(a,in)pack codes :)  Try it, I guess you'll
> be able to call their functions, this way:
>   void dposl_(double*, int*, int*, double*); //for `dposl' Fortran function
> 
> Note underscore `_' under function name! ;)

R's include files BLAS.h, Linpack.h, and Lapack.h all contain

#include <R_ext/RS.h>

which defines portable macros F77_CALL that add the underscore when
necessary.  The reliable way to use these routines in C is as
    F77_CALL(dposl)(A, &n, &n, x);
where A is a pointer to the matrix, stored in Fortran-like column
major order, and x is a pointer to the right hand side.  Note that the
integer n must be passed by address because of Fortran calling
conventions.

The Lapack functions defined in Lapack.h can be called from within C
routines in R packages but you should include a file Makevars that
defines

PKG_LIBS = $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)

Calling Lapack functions from standalone code is more complicated.



From rolf at math.unb.ca  Fri Mar 26 18:37:06 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 26 Mar 2004 13:37:06 -0400 (AST)
Subject: [R] (Off topic) Text on General Linear Model.
Message-ID: <200403261737.i2QHb6KW016440@erdos.math.unb.ca>


Sorry to take up bandwidth, but ....

	Several people have already sent my suggestions
	for books about ***generalized*** linear models.

	My request was for suggestions as to books on
	***GENERAL*** linear models, which is an (entirely?)
	different topic.

			cheers,

				Rolf Turner



From paulojus at est.ufpr.br  Fri Mar 26 18:46:28 2004
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 26 Mar 2004 14:46:28 -0300 (BRT)
Subject: [R] Help import data 
In-Reply-To: <016e01c41359$69c2fce0$ea09000a@W00RHUAMAN>
References: <016e01c41359$69c2fce0$ea09000a@W00RHUAMAN>
Message-ID: <Pine.LNX.4.58L0.0403261445510.2000@est.ufpr.br>

There is a Data Import/Export manual.
Check it!

For 2. see ?read.table

On Fri, 26 Mar 2004, Ricardo Huaman wrote:

> Friends:
>
> 1. Is it possible to import data from excel and/or access to R? How?
>
> 2. Is it possible to import data from a txt file? How?
>
> Thanks in advance,
>
> Ricardo.
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3471
Fax: (+55) 41 361 3141
e-mail: pj at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From Roger.Bivand at nhh.no  Fri Mar 26 18:56:54 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 26 Mar 2004 18:56:54 +0100 (CET)
Subject: [R] color.ramp in maptools
In-Reply-To: <406461F4.4060804@fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0403261831150.9529-100000@reclus.nhh.no>

On Fri, 26 Mar 2004, Marco Albani wrote:

> Dear list members,
> 
> I am trying to use the maptools library to display geographical data. At 
> the moment I have some trouble understanding how the " auxvar " variable 
> is supposed to be used in the plot.Map function.
> 
> I am using R Version 1.8.1  (2003-11-21) on Linux
> 
> Looking at the plot.Map function itself, I see that it calls a 
> color.ramp function (I am reporting only the relevant part here):
> 
> ...
>      if (attr(theMap$Shapes, "shp.type") == "poly") {
>          if (!is.null(auxvar) && nclass > 1) {
>              col.rmp <- color.ramp(nclass, nvec = auxvar)
>              for (i in recs) {
> ...
> 
> 
> I don't seem to be able to get any information on this color.ramp 
> function. In fact the function doesn't seem to exist if I search for it 
> with ls()
> 
> Does anyone have any insight?

The function is not exported in the package namespace (you can read it
using the ::: operator: maptools:::color.ramp will display it). Its usage
is: color.ramp(nclass,color='red',nvec=NULL,type='q'), where: nclass is
the number of classes desired in the ramp; color is the base color to
build the ramp on; nvec is the numeric vector (or factor) from which to 
build the ramp; and type is the type of binning procedure to use (default 
quantiles, if set to "e", it will use equal-sized bins.). 

In general, most users quantize and colour using the fg= argument to
plot.Map() directly, so the color.ramp() arguments to plot.Map() are
seldom used, which is also why the function is not documented or exported
to user space. color.ramp() uses cut() internally to assign the variable 
to intervals, while the prefered function in user space is findInterval(), 
used for tabular lookup in a colour table. I hope this provides the 
information you might need - my general advice would be to create the 
interval breaks before calling plot.Map(), setting:

fg=cols[findInterval(x=nvec, vec=breaks, all.inside=TRUE)]

for numeric variables and a chosen colour palette (see RColorBrewer), and 
using the levels of factors as appropriate for factors (more or less as 
in the example for plot.Map()).

Thanks for choosing a very specific and accurate subject!

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Roger.Bivand at nhh.no  Fri Mar 26 19:05:30 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 26 Mar 2004 19:05:30 +0100 (CET)
Subject: [R] How to do the significant test on Local Moran's I
In-Reply-To: <20040326161409.7663.qmail@web60901.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403261858080.9529-100000@reclus.nhh.no>

On Sat, 27 Mar 2004, [gb2312] Louis wrote:

> Dear Danlin Yu:
>  I read your message at
> https://stat.ethz.ch/pipermail/r-help/2003-April/030754.html. Actually,
> I have a question about significant of Local Moran I too. based on it's
> function:
> 
>                               Ii = zi [??j wijzj],
> 
> I wonder how can I calculate it's z-value.

(this refers to function localmoran() in package spdep - maybe it might 
help to mention this?)

?localmoran

says that the function returns a matrix with four column. Column 4 are the 
z-values (which are (col1 - col2)/sqrt(col3)). How you interpret this will 
depend on how many multiple tests you want to do on the same data.

> 
> Any suggest will be greatly appreciated.
> 
> 
> 
> Louis Zhang
>  
> Dept of Geography and Planning
> The Univ. of Toledo
> 
> 
> 
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From White.Denis at epamail.epa.gov  Fri Mar 26 19:06:09 2004
From: White.Denis at epamail.epa.gov (White.Denis@epamail.epa.gov)
Date: Fri, 26 Mar 2004 10:06:09 -0800
Subject: [R] Accurate area map projections
Message-ID: <OFE7C45A2B.8AD3C5D5-ON88256E63.0061EAB9-88256E63.00636DF4@epamail.epa.gov>





Just catching up on r-help, ignore if you have already have moved on ...

I almost always use the Lambert azimuthal equal-area projection (called
"azequalarea" in the maps package).  Unless the study area is unusually
elongated this projection will give good performance.  In fact, it has
lower average distortion (but not lower extreme distortion) than the
standard projection used for the conterminous US, the Albers conical
equal-area.  See:  Kimerling AJ, Overton WS, White D. 1995. Statistical
comparison of map projection distortions within irregular
areas. Cartography and Geographic Information Systems 22(3):205-221,
(pdf available at http://www.epa.gov/wed/pages/staff/white/pubs.htm)

Denis White
   US EPA, 200 SW 35th St, Corvallis, Oregon, 97333 USA
   voice: 541.754.4476, email: white.denis at epa.gov
   web: www.epa.gov/wed/pages/staff/white/

> Hi,
> Could any one point me to the projection, and parameters if necessary,
that
> would show each country/continent with it's area accurately refelcted
on the
> plot? E.g. aitoff vs. albers vs. bonne vs. cylequalearea vs. guyou -
they
> don't all look the same to mee but some of the documentations suggests
they
> are equal area?  Of course this isn't my field, so I am largely
guessing and
> am prorbably making some naive assumptions :)
> I'm pretty sure this is on the R-help archive but I don't seem to be
able to
> access most of the posts that the search returns - I keep getting a
'file
> not found' page...
> TIA
> Mark



From amurta at ipimar.pt  Fri Mar 26 19:39:23 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Fri, 26 Mar 2004 18:39:23 +0000
Subject: [R] Mahalanobis
Message-ID: <200403261839.23148.amurta@ipimar.pt>

Dear all

Why isn'it possible to calculate Mahalanobis distances with R for a matrix 
with 1 row (observations) more than the number of columns (variables)?

> mydata <- matrix(runif(12,-5,5), 4, 3)
> mahalanobis(x=mydata, center=apply(mydata,2,mean), cov=var(mydata))
[1] 2.25 2.25 2.25 2.25

> mydata <- matrix(runif(420,-5,5), 21, 20)
> mahalanobis(x=mydata, center=apply(mydata,2,mean), cov=var(mydata))
 [1] 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 
19.04762 19.04762 19.04762 19.04762
[13] 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 
19.04762

> mydata <- matrix(runif(132,-5,5), 12, 11)
> mahalanobis(x=mydata, center=apply(mydata,2,mean), cov=var(mydata))
 [1] 10.08333 10.08333 10.08333 10.08333 10.08333 10.08333 10.08333 10.08333 
10.08333 10.08333 10.08333 10.08333

Thanks in advance

Alberto Murta

> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    8.1              
year     2003             
month    11               
day      21               
language R                

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From bates at stat.wisc.edu  Fri Mar 26 19:53:58 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 26 Mar 2004 12:53:58 -0600
Subject: [R] cran.us.r-project.org
Message-ID: <6rn063l9rt.fsf@bates4.stat.wisc.edu>

cran.us.r-project.org is back online again as an alias of
                        cran.mirrors.pair.com

If you do not get a response from http://cran.us.r-project.org you can
try http://cran.mirrors.pair.com instead.  

Our thanks to the good people at pair.com who provide the facilities
for this mirror.
-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From malbani at fas.harvard.edu  Fri Mar 26 19:54:51 2004
From: malbani at fas.harvard.edu (Marco Albani)
Date: Fri, 26 Mar 2004 13:54:51 -0500
Subject: [R] color.ramp in maptools
In-Reply-To: <Pine.LNX.4.44.0403261831150.9529-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0403261831150.9529-100000@reclus.nhh.no>
Message-ID: <40647C7B.3030708@fas.harvard.edu>

Roger Bivand wrote:
> On Fri, 26 Mar 2004, Marco Albani wrote:
> At 
>>the moment I have some trouble understanding how the " auxvar " variable 
>>is supposed to be used in the plot.Map function.
>>
[...]
>>
>>I don't seem to be able to get any information on this color.ramp 
>>function. In fact the function doesn't seem to exist if I search for it 
>>with ls()
>>
>>Does anyone have any insight?
> 
> 
> The function is not exported in the package namespace (you can read it
> using the ::: operator: maptools:::color.ramp will display it). Its usage
> is: color.ramp(nclass,color='red',nvec=NULL,type='q'), where: nclass is
> the number of classes desired in the ramp; color is the base color to
> build the ramp on; nvec is the numeric vector (or factor) from which to 
> build the ramp; and type is the type of binning procedure to use (default 
> quantiles, if set to "e", it will use equal-sized bins.). 
> 

Thank you. So the way I understand it is that auxvar will actually be 
passed as nvec to color.ramp

I tried passing a numeric vector with the following summary stats:

  summary(NE.HWA.pols$att.data$RockDepth)
    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
     0.0   100.0   120.0   112.7   140.0   160.0

And on call

plot.Map(NE.HWA.pols, auxvar = NE.HWA.pols$att.data$RockDepth)

I got this error

Error in cut.default(nvec, brks, labels = FALSE, include.lowest = TRUE) :
	cut: breaks are not unique



From apiszcz at solarrain.com  Fri Mar 26 19:56:49 2004
From: apiszcz at solarrain.com (Al Piszcz)
Date: Fri, 26 Mar 2004 13:56:49 -0500 (EST)
Subject: [R] Plot difference between PNG and X11
In-Reply-To: <406459F8.10109@pburns.seanet.com>
References: <Pine.LNX.4.58.0403261117370.31307@l1>
	<406459F8.10109@pburns.seanet.com>
Message-ID: <Pine.LNX.4.58.0403261356060.24568@l1>


Patrick pointed out an error in my coordinate for the mtext function.
Thank you.


On Fri, 26 Mar 2004, Patrick Burns wrote:

> Date: Fri, 26 Mar 2004 16:27:36 +0000
> From: Patrick Burns <pburns at pburns.seanet.com>
> To: Al Piszcz <apiszcz at solarrain.com>
> Subject: Re: [R] Plot difference between PNG and X11
>
> I can't believe this would be the problem, but the first
> thing to check is that the "mar" graphics parameter is
> the same for both devices.
>
> Then I would play with "cex" to see if that changed the
> behavior.  Good luck,
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Al Piszcz wrote:
>
> >I am rendering a plot using boxplot, mtext, abline, etc.
> >
> >The X11 view of the plot looks correct.
> >
> >When I change to the png device to:
> >  png(filename="bxplot.png",width=640,height=640)
> >
> >I get half of a character label plotted in the lower left
> >corner in the resulting .png file.
> >The character fragment started to appear in png when
> >I started using mtext.
> >
> >mtext(labelvector,side=1,at=c(xi,0),line=2)
> >
> >Any thoughts on items to try? Thx.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> >
> >
>
>
>



From Roger.Bivand at nhh.no  Fri Mar 26 20:16:02 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 26 Mar 2004 20:16:02 +0100 (CET)
Subject: [R] color.ramp in maptools
In-Reply-To: <40647C7B.3030708@fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0403262003060.9529-100000@reclus.nhh.no>

On Fri, 26 Mar 2004, Marco Albani wrote:

> Roger Bivand wrote:
> > On Fri, 26 Mar 2004, Marco Albani wrote:
> > At 
> >>the moment I have some trouble understanding how the " auxvar " variable 
> >>is supposed to be used in the plot.Map function.
> >>
> [...]
> >>
> >>I don't seem to be able to get any information on this color.ramp 
> >>function. In fact the function doesn't seem to exist if I search for it 
> >>with ls()
> >>
> >>Does anyone have any insight?
> > 
> > 
> > The function is not exported in the package namespace (you can read it
> > using the ::: operator: maptools:::color.ramp will display it). Its usage
> > is: color.ramp(nclass,color='red',nvec=NULL,type='q'), where: nclass is
> > the number of classes desired in the ramp; color is the base color to
> > build the ramp on; nvec is the numeric vector (or factor) from which to 
> > build the ramp; and type is the type of binning procedure to use (default 
> > quantiles, if set to "e", it will use equal-sized bins.). 
> > 
> 
> Thank you. So the way I understand it is that auxvar will actually be 
> passed as nvec to color.ramp
> 
> I tried passing a numeric vector with the following summary stats:
> 
>   summary(NE.HWA.pols$att.data$RockDepth)
>     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>      0.0   100.0   120.0   112.7   140.0   160.0
> 
> And on call
> 
> plot.Map(NE.HWA.pols, auxvar = NE.HWA.pols$att.data$RockDepth)
> 
> I got this error
> 
> Error in cut.default(nvec, brks, labels = FALSE, include.lowest = TRUE) :
> 	cut: breaks are not unique
> 

Well, I did say the code isn't often used (AFAIK), and that cut() can bite
- as its name suggests. One possibility is to try a different nclass=, but
I think the code of maptools:::color.ramp needs to be modified (in two
places) to remove duplicate breaks. Put a brks <- unique(brks) before the
two calls to cut(), if you like (done in release to come when 1.9.0 is
out - thanks for drawing attention to the problem). 

Otherwise follow other users' experience, and define your class intervals
before calling plot.Map(). Automating class intervals for choropleth maps
is not easy, and there are very many alternative maps that can be created
for even small n polygons and k classes.

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ivo.welch at yale.edu  Fri Mar 26 20:23:46 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Fri, 26 Mar 2004 14:23:46 -0500
Subject: [R] stop() vs. error() ?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7AA2@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7AA2@usrymx25.merck.com>
Message-ID: <40648342.4040200@yale.edu>


hi andy:  yes, I know what it does.  My suggestion would be to have a 
different command, that is a "pure stop" without error condition (with 
its message).  A stop and an error are really two different things.

regards, /ivo


Liaw, Andy wrote:
> Please do read the documentation of the functions you are trying to use.
> The description in ?stop says:
> 
>      'stop' stops execution of the current expression and executes an
>      error action.
> 
> stop() is how error is flagged in R (and S in general).  If that's not what
> you want, try something else.  And the `something else' depends on what you
> want, which has not been described in detail.
> 
> Andy
> 

<snip>



From p.dalgaard at biostat.ku.dk  Fri Mar 26 20:49:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Mar 2004 20:49:48 +0100
Subject: [R] Omegahat down?
In-Reply-To: <6rfzbvmsjd.fsf@bates4.stat.wisc.edu>
References: <00B717603414D21187AD00104B94A2DAB23958@EXCHANGE>
	<40643A55.8010101@statistik.uni-dortmund.de>
	<6rfzbvmsjd.fsf@bates4.stat.wisc.edu>
Message-ID: <x2u10bl76r.fsf@biostat.ku.dk>

Douglas Bates <bates at stat.wisc.edu> writes:

> The machine that was serving as Omegahat.org was compromised.  We were
> going to move that site to another machine anyway so this attack has
> resulted in a somewhat accelerated schedule for the move.

Before any one get's unduly paranoid: There's nothing strange about
the fact that this happened simultaneously with the troubles on
cvs.r-project.org. It was the same machine.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From pkleiber at honlab.nmfs.hawaii.edu  Fri Mar 26 20:58:38 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Fri, 26 Mar 2004 09:58:38 -1000
Subject: [R] stop() vs. error() ?
In-Reply-To: <40648342.4040200@yale.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7AA2@usrymx25.merck.com>
	<40648342.4040200@yale.edu>
Message-ID: <40648B6E.3060208@honlab.nmfs.hawaii.edu>

The thing is that functions don't really stop... they return.  So what you 
want is return()
  Cheers, Pierre

ivo welch wrote:
> 
> hi andy:  yes, I know what it does.  My suggestion would be to have a 
> different command, that is a "pure stop" without error condition (with 
> its message).  A stop and an error are really two different things.
> 
> regards, /ivo
> 
> 
> Liaw, Andy wrote:
> 
>> Please do read the documentation of the functions you are trying to use.
>> The description in ?stop says:
>>
>>      'stop' stops execution of the current expression and executes an
>>      error action.
>>
>> stop() is how error is flagged in R (and S in general).  If that's not 
>> what
>> you want, try something else.  And the `something else' depends on 
>> what you
>> want, which has not been described in detail.
>>
>> Andy
>>
> 
> <snip>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From ripley at stats.ox.ac.uk  Fri Mar 26 21:16:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Mar 2004 20:16:00 +0000 (GMT)
Subject: [R] update/offset
In-Reply-To: <Pine.GSO.4.58.0403261204330.3473@bach>
Message-ID: <Pine.LNX.4.44.0403262014380.10840-100000@gannet.stats>

Which version of R?  This is supposedly fixed in 1.9.0 beta, so please try 
that.

On Fri, 26 Mar 2004, Simon.Bond wrote:

> I've noticed that the update() function does not seem to work correctly
> when offset(..) terms are there:
> 
> update(modelwithoffset,  .~.-afixedeffect)
> 
> drops the offset term.
> 
> I'm using this with a negbin model, but I think it goes wrong with the
> update.formula() function.
> 
> update.formula
> function (old, new, ...)
> {
>     env <- environment(as.formula(old))
>     tmp <- .Internal(update.formula(as.formula(old), as.formula(new)))
>     out <- formula(terms.formula(tmp, simplify = TRUE))
>     environment(out) <- env
>     return(out)
> }
> <environment: namespace:base>
> 
> 
> 
> The .Internal command in the code is where my efforts grind to a halt.

The prohlem was in terms.formula(tmp, simplify = TRUE).

> Any suggestions (apart from redefine the new model from scratch) ?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Mar 26 21:18:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Mar 2004 20:18:51 +0000 (GMT)
Subject: [R] Fwd: MDS problems [ajtee@ajtee.uklinux.net]
In-Reply-To: <1080310050.16349.9.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.44.0403262017381.10840-100000@gannet.stats>

If you `lie', you may find that the routines fail to converge, especially 
sammon -- it is not a good idea.

On 26 Mar 2004, Jari Oksanen wrote:

> On Fri, 2004-03-26 at 15:02, Adam Tee wrote:
> > Hi all,
> > 
> > 
> > I'm trying to perform an MDS of some data that I have.  When I use
> > cmdscale everything is fine and I get some interesting results however,
> > the tends to be low.
> > 
> > What I wnat to do is compare this with the Non-Metric MDS using isoMDS  
> > or sammon. However, when I try using these I get the following message.
> > 
> > Error in isoMDS(x.dist) : zero or negative distance between objects 2  
> > and 4
> > 
> The error message is clear: You have some identical sites so that their
> distance is zero. I think the canonical solution is to remove the
> duplicate cases from the data before calculating the dissimilarities. If
> your data frame is called X:
> 
> Xuniq <- unique(X)
> x.dist <- dist(Xuniq)
> 
> (or, as a one-liner: x.dist <- dist(unique(X))
> 
> A dirty solution is to lie to isoMDS. The following will do:
> 
> isoMDS(x.dist + 1e-5)
> 
> But you better not tell anybody that you did this dirty trick.
> 
> cheers, jari oksanen
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jmacdon at med.umich.edu  Fri Mar 26 21:20:08 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Fri, 26 Mar 2004 15:20:08 -0500
Subject: [R] stop() vs. error() ?
Message-ID: <s0644a3a.090@med-gwia-01a.med.umich.edu>

And a closer examination of the help page would lead you to this:

options(show.error.messages=FALSE)
stop()

which is what I believe you want....

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> ivo welch <ivo.welch at yale.edu> 03/26/04 02:23PM >>>

hi andy:  yes, I know what it does.  My suggestion would be to have a 
different command, that is a "pure stop" without error condition (with

its message).  A stop and an error are really two different things.

regards, /ivo


Liaw, Andy wrote:
> Please do read the documentation of the functions you are trying to
use.
> The description in ?stop says:
> 
>      'stop' stops execution of the current expression and executes
an
>      error action.
> 
> stop() is how error is flagged in R (and S in general).  If that's
not what
> you want, try something else.  And the `something else' depends on
what you
> want, which has not been described in detail.
> 
> Andy
> 

<snip>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sasprog474 at yahoo.com  Fri Mar 26 21:46:41 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Fri, 26 Mar 2004 12:46:41 -0800 (PST)
Subject: [R] SUMMARY:  Using R's LAPACK & Related files in Visual C++
Message-ID: <20040326204641.65201.qmail@web41410.mail.yahoo.com>

The following were the replies to my question
about using R's LAPACK and other .h files in 
some of my C programs.  From what was
said, it appears that buying a ready-made
library (MKL = $200, for example)
or using CLapack according to the Shumway lecture
notes are the best approaches:

--------------------------------
>From Andy Liaw:

(1) If you just want linear algebra routines in your C
code, you should really look elsewhere.  You could,
for example, check out Intel's Math Kernel
Library.  (It used to be a free download.  Not sure if
that's still the case.)

(2) Googling `calling lapack from C', I also found:
http://physics.asu.edu/phy502-shumway/notes/lec11.html
--------------------------------
>From Timur Elzhov:

AFAIK, R uses native FORTRAN l(a,in)pack codes :)  Try
it, I guess you'll be able to call their functions,
this way:
  void dposl_(double*, int*, int*, double*); //for
`dposl' Fortran 
function

Note underscore `_' under function name! ;)
--------------------------------
>From Douglas Bates:

R's include files BLAS.h, Linpack.h, and Lapack.h all
contain

#include <R_ext/RS.h>

which defines portable macros F77_CALL that add the
underscore when necessary.  The reliable way to use
these routines in C is as

    F77_CALL(dposl)(A, &n, &n, x);

where A is a pointer to the matrix, stored in
Fortran-like column major order, and x is a pointer to
the right hand side.  Note that the integer n must be
passed by address because of Fortran calling
conventions.

The Lapack functions defined in Lapack.h can be called
from within C routines in R packages but you should
include a file Makevars that defines

PKG_LIBS = $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)

Calling Lapack functions from standalone code is more
complicated.



From andy_liaw at merck.com  Fri Mar 26 22:02:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 26 Mar 2004 16:02:40 -0500
Subject: [R] Mahalanobis
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AA5@usrymx25.merck.com>

If I'm not mistaken, the data you generated form a simplex in the
p-dimensional space.  Mahalanobis distance for such data, using sample mean
and covariance, just give the distance to the centroid after normalization.
The normalization step make all the points equidistance from the centroid.

To see this, try generating 3 points in 2D, and plot the principal component
scores:  You'll see the points on the vertices of a regular triangle.

Andy

> From: Alberto Murta
> 
> Dear all
> 
> Why isn'it possible to calculate Mahalanobis distances with R 
> for a matrix 
> with 1 row (observations) more than the number of columns (variables)?
> 
> > mydata <- matrix(runif(12,-5,5), 4, 3)
> > mahalanobis(x=mydata, center=apply(mydata,2,mean), cov=var(mydata))
> [1] 2.25 2.25 2.25 2.25
> 
> > mydata <- matrix(runif(420,-5,5), 21, 20)
> > mahalanobis(x=mydata, center=apply(mydata,2,mean), cov=var(mydata))
>  [1] 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 
> 19.04762 19.04762 
> 19.04762 19.04762 19.04762 19.04762
> [13] 19.04762 19.04762 19.04762 19.04762 19.04762 19.04762 
> 19.04762 19.04762 
> 19.04762
> 
> > mydata <- matrix(runif(132,-5,5), 12, 11)
> > mahalanobis(x=mydata, center=apply(mydata,2,mean), cov=var(mydata))
>  [1] 10.08333 10.08333 10.08333 10.08333 10.08333 10.08333 
> 10.08333 10.08333 
> 10.08333 10.08333 10.08333 10.08333
> 
> Thanks in advance
> 
> Alberto Murta
> 
> > version
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    8.1              
> year     2003             
> month    11               
> day      21               
> language R                
> 
> -- 
>                                          Alberto G. Murta
> Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
> Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
> Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From yshen004 at temple.edu  Fri Mar 26 22:27:49 2004
From: yshen004 at temple.edu (yshen004@temple.edu)
Date: Fri, 26 Mar 2004 16:27:49 -0500
Subject: [R] 3D globe plot
Message-ID: <6f714ced.274ea101.82eed00@po-a.temple.edu>

Hi,

This is Yan, a graduate student from Temple University. My 
current work involves drawing 3D plot in R or S-Plus. I have 
a dataset containing distances between 18 cities in 4 
continents. Using classical multi-deminsional scaling in 2 
dimensions, I could seperate them successfully and plot with 
different colors in a 2D figure, which is attacked. Then I do 
the same in 3 dimensions and use 'brush' command in S. When I 
spin the graph, it shows that all data points are on a globe. 
However, the graph can't be saved. My question is, can I draw 
a globe in R and add all the data points?
Thanks in advance.



Best wishes,




Yan Shen
Temple University
Department of Statistics



From ripley at stats.ox.ac.uk  Fri Mar 26 22:30:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Mar 2004 21:30:09 +0000 (GMT)
Subject: [R] With which version of XFree86 can R compile?
In-Reply-To: <85wu57a7v6.fsf@servant.blindglobe.net>
Message-ID: <Pine.LNX.4.44.0403262127550.10968-100000@gannet.stats>

Note this is not R as distributed by R-core (which does not have
dataentry_mb.c) but I believe some `Japanized' distribution.  People might
want to check if the problem is in the changes (which I suspect) or R
itself.

On Fri, 26 Mar 2004, A.J. Rossini wrote:

> 
> It isn't clear that R is currently friendly to the new XFree 4.4
> license.  I believe that there is an advertising clause for linking --
> does anyone know if this has been addressed or clarified?  I've not
> been following those issues for a few months.
> 
> best,
> -tony
> 
> Susumu =?ISO-2022-JP?B?VGFuaW11cmEvGyRCQytCPBsoQiAbJEI/OBsoQg==?= <stanimura-ngs at umin.ac.jp> writes:
> 
> > Hi there,
> >
> > No information was found which version of XFree86 get along with R.  I
> > could not compile R version 1.8.1 with XFree86 4.3.99.902.
> >
> > Here is error message.
> >
> > gcc -I. -I../../../src/include -I../../../src/include -DI18N_MB -I/usr/X11R6/include -I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES -mieee-fp -fPIC  -O2 -pipe -march=i386 -mcpu=i686 -c dataentry_mb.c -o dataentry_mb.lo
> > In file included from dataentry_mb.c:35:
> > /usr/X11R6/include/X11/Xlib.h:1389: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xlib.h:1477: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xlib.h:1505: error: syntax error before "_Xconst"
> > [snip]
> > /usr/X11R6/include/X11/Xutil.h:678: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xutil.h:801: error: syntax error before "_Xconst"
> > In file included from xrm.h:5,
> >                  from dataentry_mb.c:41:
> > /usr/X11R6/include/X11/Xresource.h:94: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:98: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:122: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:127: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:194: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xresource.h:203: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xresource.h:208: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xresource.h:214: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xresource.h:227: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xresource.h:269: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:273: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:279: error: syntax error before "char"
> > /usr/X11R6/include/X11/Xresource.h:284: error: syntax error before "_Xconst"
> > /usr/X11R6/include/X11/Xresource.h:352: error: syntax error before "_Xconst"
> > dataentry_mb.c: In function `GetKey':
> > dataentry_mb.c:1276: warning: passing arg 1 of `XLookupString' from incompatible pointer type
> > dataentry_mb.c:1276: warning: passing arg 4 of `XLookupString' from incompatible pointer type
> > dataentry_mb.c: In function `GetCharP':
> > dataentry_mb.c:1285: warning: passing arg 1 of `XLookupString' from incompatible pointer type
> > dataentry_mb.c:1285: warning: passing arg 4 of `XLookupString' from incompatible pointer type
> > dataentry_mb.c: In function `doControl':
> > dataentry_mb.c:1306: warning: passing arg 1 of `XLookupString' from incompatible pointer type
> > dataentry_mb.c:1306: warning: passing arg 4 of `XLookupString' from incompatible pointer type
> > dataentry_mb.c: In function `RefreshKeyboardMapping':
> > dataentry_mb.c:1335: warning: passing arg 1 of `XRefreshKeyboardMapping' from incompatible pointer type
> > make[4]: *** [dataentry_mb.lo] Error 1
> > make[4]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules/X11'
> > make[3]: *** [R] Error 2
> > make[3]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules/X11'
> > make[2]: *** [R] Error 1
> > make[2]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src/modules'
> > make[1]: *** [R] Error 1
> > make[1]: Leaving directory `/home/hoge/rpm/BUILD/R-1.8.1/src'
> > make: *** [R] Error 1
> > error: Bad exit status from /var/tmp/rpm-tmp.85917 (%build)
> >  
> > RPM build errors:
> >     Bad exit status from /var/tmp/rpm-tmp.85917 (%build)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pburns at pburns.seanet.com  Fri Mar 26 22:30:37 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 26 Mar 2004 21:30:37 +0000
Subject: [R] R packages for S Poetry
Message-ID: <4064A0FD.9040906@pburns.seanet.com>

Nick Efthymiou has created two R packages out of some
of the functions from S Poetry.  He obviously realized that
I'm too much of a slouch to do it myself.

They are Nick's packages, but they are available through
the S Poetry page on http://www.burns-stat.com

The packages are:

                                   mathgraph
The functions that implement an S3 class for mathematical
graphs plus a few miscellaneous functions.

                                       verify
A family of functions for test suites of functions.


Thanks Nick.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From papucho at mac.com  Fri Mar 26 22:35:27 2004
From: papucho at mac.com (Ivan Alves)
Date: Fri, 26 Mar 2004 22:35:27 +0100
Subject: [R] Aggregating frequency of irregular time series
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7A9C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7A9C@usrymx25.merck.com>
Message-ID: <7F5B4976-7F6D-11D8-8C97-000A959D05F0@mac.com>

To the extent that my daughter, work and sports activities leave me  
some time, for sure I will put together a function like AggregateSeries  
(maybe within the its package)!  As for cointegration, much less likely  
to happen, especially in light of the development of the urca package.  
The days of sleepless nights are long over for me, I am afraid. (just  
do statistics as a hobby, really)

Kind regards,

Ivan
_______________________
Ivan Alves
mailto://papucho at mac.com
On 25 Mar 2004, at 23:36, Liaw, Andy wrote:

>> From: Ivan Alves
>>
>> Thank you very much Brian.  Indeed, by looking at ?tapply()
>> this would
>> do the job for regular time series (whose data-time INDEX renders
>> itself naturally for " bundling data" in regular groups).
>> However, with
>> irregular time series the story is different, as some careful
>> "bundling" is necessary prior to applying tapply(). Whereas this may
>> not be a problem for months (creating strings month-year), it
>> would be
>> for weeks, for instance.  Also the object returned would need to be
>> converted again to a time series object with additional
>> function calls.
>> Furthermore, the function AggregateSeries() provides additional
>> functionality, such as creating moving averages, rolling variances,
>> minima and maxima, all with options to the same function call. I take
>> from your response that there is no easy way out (an already existing
>> function), and that some programming will be required.
>
> So can we count on you to contribute this (and the cointegration that  
> you
> requested in another post)?  You did read the message at the R startup
> screen, didn't you?
>
> Best,
> Andy
>
>> Kind regards,
>>
>> Ivan
>> _______________________
>> Ivan Alves
>> mailto://papucho at mac.com
>> On 25 Mar 2004, at 22:51, Prof Brian Ripley wrote:
>>
>>> R itself has no support for irregular time series, but it
>> does have an
>>> aggregate method for regular ones.  You need to look into whichever
>>> package is handling irregular time series.  However, it seems to me
>>> that
>>> this is not a time series problem at all: you have a set of
>>> observations
>>> whose indices are data-times, and tapply() will do the job.
>>>
>>> On Thu, 25 Mar 2004, Ivan Alves wrote:
>>>
>>>> S-Plus has the function AggregateSeries() whose name is self
>>>> explanatory.  For instance one can derive monthly series from daily
>>>> ones by specifying end-of-period, averages, sums, etc.  I
>> looked for a
>>>> similar function in the packages "its" and "tseries", but found
>>>> nothing.  I also help.searched() for aggregate to no avail.  Would
>>>> anybody be so kind to point me in the right direction?
>>>
>>> --  
>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
>
> ----------------------------------------------------------------------- 
> -------
> Notice:  This e-mail message, together with any attachments, contains  
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station,  
> New Jersey, USA 08889), and/or its affiliates (which may be known  
> outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD  
> and in Japan, as Banyu) that may be confidential, proprietary  
> copyrighted and/or legally privileged. It is intended solely for the  
> use of the individual or entity named on this message.  If you are not  
> the intended recipient, and have received this message in error,  
> please notify us immediately by reply e-mail and then delete it from  
> your system.
> ----------------------------------------------------------------------- 
> -------



From Charles.Annis at StatisticalEngineering.com  Fri Mar 26 22:44:01 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 26 Mar 2004 16:44:01 -0500
Subject: [R] trouble with paste()
Message-ID: <200403262144.i2QLi2o2022333@hypatia.math.ethz.ch>

Greetings:

I'm trying to annotate a plot and find that I cannot correctly paste
elements.  When I do this:

text(1.4, 1., paste("conditional density", "\n", "of y, given ",
expression(x == x[0])), adj=c(0.,0.))

The expression is not properly displayed.

Resolving this is not urgent because the workaround is simple:

text(1.4, 1., paste("conditional density", "\n", "of y, given ", sep=""),
adj=c(0.,0.))
text(2.3, 0.94,expression(x == x[0]), adj=c(0.,0.))

Still, it'd be more convenient if I could learn to do it correctly.

Any help?

Thank you.

Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  503-217-5849
http://www.StatisticalEngineering.com



From papucho at mac.com  Fri Mar 26 22:51:02 2004
From: papucho at mac.com (Ivan Alves)
Date: Fri, 26 Mar 2004 22:51:02 +0100
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <AD1B0575-7F6F-11D8-8C97-000A959D05F0@mac.com>

Thanks to all once again for the very useful suggestions. I will have 
to teach myself some real S before tackling the writting of the 
AggregateSeries-like function! I brace myself.

Kind regards,

Ivan
_______________________
Ivan Alves
mailto://papucho at mac.com



From ivo.welch at yale.edu  Fri Mar 26 22:56:38 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Fri, 26 Mar 2004 16:56:38 -0500
Subject: [R] stop() vs. error() ?
In-Reply-To: <s0644a3a.092@med-gwia-01a.med.umich.edu>
References: <s0644a3a.092@med-gwia-01a.med.umich.edu>
Message-ID: <4064A716.9060406@yale.edu>


Hi jim:  ahhh, but then the error messages are off for "real errors" 
that occur later.

Hi pierre: a return() in a (2nd-level) function can be different from a 
stop().  one could argue about whether it should be possible to stop() 
in a function of course, without it being an error().

regards,

/iaw


James MacDonald wrote:
> And a closer examination of the help page would lead you to this:
> 
> options(show.error.messages=FALSE)
> stop()
> 
> which is what I believe you want....
> 
> Jim


also From Pierre Kleiber:

The thing is that functions don't really stop... they return.  So what 
you want is return()
  Cheers, Pierre



From p.dalgaard at biostat.ku.dk  Fri Mar 26 23:23:00 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Mar 2004 23:23:00 +0100
Subject: [R] With which version of XFree86 can R compile?
In-Reply-To: <Pine.LNX.4.44.0403262127550.10968-100000@gannet.stats>
References: <Pine.LNX.4.44.0403262127550.10968-100000@gannet.stats>
Message-ID: <x2d66zl03f.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> Note this is not R as distributed by R-core (which does not have
> dataentry_mb.c) but I believe some `Japanized' distribution.  People might
> want to check if the problem is in the changes (which I suspect) or R
> itself.

Yes. On the other hand, the original dataentry.c begins with

#include "Defn.h"
#include "Print.h"

/* don't use X11 function prototypes (which tend to ...): */
#define NeedFunctionPrototypes 0
#include <X11/X.h>
#include <X11/Xlib.h>
...

and there is the off chance that the newer X11 is using a symbol in
our Defn.h or Print.h. If so, then we'd like to hear about it ASAP... 

> On Fri, 26 Mar 2004, A.J. Rossini wrote:
> 
> > 
> > It isn't clear that R is currently friendly to the new XFree 4.4
> > license.  I believe that there is an advertising clause for linking --
> > does anyone know if this has been addressed or clarified?  I've not
> > been following those issues for a few months.
> > 

(R as such is not likely to care since we're not distributing X
libraries or headers, v.4.4 or not. Distributors do care, but if it is
already on the system, I don't see what should prevent people from
installing R.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Mar 26 23:28:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Mar 2004 23:28:47 +0100
Subject: [R] trouble with paste()
In-Reply-To: <200403262144.i2QLi2o2022333@hypatia.math.ethz.ch>
References: <200403262144.i2QLi2o2022333@hypatia.math.ethz.ch>
Message-ID: <x28yhnkzts.fsf@biostat.ku.dk>

"Charles Annis, P.E." <Charles.Annis at statisticalengineering.com> writes:

> Greetings:
> 
> I'm trying to annotate a plot and find that I cannot correctly paste
> elements.  When I do this:
> 
> text(1.4, 1., paste("conditional density", "\n", "of y, given ",
> expression(x == x[0])), adj=c(0.,0.))
> 
> The expression is not properly displayed.
> 
> Resolving this is not urgent because the workaround is simple:
> 
> text(1.4, 1., paste("conditional density", "\n", "of y, given ", sep=""),
> adj=c(0.,0.))
> text(2.3, 0.94,expression(x == x[0]), adj=c(0.,0.))
> 
> Still, it'd be more convenient if I could learn to do it correctly.
> 
> Any help?

The whole thing needs to be an expression, or you get the consequences
of

> paste("conditional density", "\n", "of y, given ",
+ expression(x == x[0]))
[1] "conditional density \n of y, given  x == x[0]"

I think you're looking for

expression(atop("conditional density", paste("of y, given ", x==x[0])))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gregory_r_warnes at groton.pfizer.com  Sat Mar 27 00:08:46 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 26 Mar 2004 18:08:46 -0500
Subject: FW: [R] Gregmisc 'running' question
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B186@groexmb02.pfizer.com>


[The new version of gregmisc will be showing up on CRAN shortly.]

-----Original Message-----
From: Warnes, Gregory R 
Sent: Friday, March 26, 2004 6:07 PM
To: 'Sean Davis'
Subject: RE: Quick "running" question

Hi Sean,

Congratulations, you found a bug!

My "running" function took an improper shortcut.  When allow.fewer=FALSE it
was still passing shorter lists of elements to the called function, and then
overwriting the results for the shorter lists with NAs.  I've now corrected
the code to skip evaluation of the function on lists shorter than the
specified length when allow.fewer=FALSE.

I'm attaching the latest version of the gregmisc package with the corrected
code.  I've also added a couple of other features while I was mucking
about...


-Greg


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Don MacQueen
> Sent: Friday, March 26, 2004 10:58 AM
> To: Sean Davis; r-help
> Subject: Re: [R] Gregmisc 'running' question
> 
> 
> It says "not enough x observations". That's pretty clear.
> So ask yourself, what is the minimum number of observations one needs 
> to do a t test?
> 
> Try this to see how many observations t.test was given each time.
> 
>     running(dat,width=50,fun=function(x) length(x),allow=T)
> 
> or this
>    running(dat,width=50,fun=function(x) cat('x:',x,'\n\n'),allow=T)
> 
> -Don
> 
> At 9:31 AM -0500 3/26/04, Sean Davis wrote:
> >Just a quick and probably simple question:
> >
> >>  dat <- rnorm(500,sd=1+(1:500)/500)
> >>  fun <- function(x) t.test(x)$p.value
> >>  running(dat,width=50,fun=fun,allow=T)
> >Error in t.test.default(x) : not enough x observations
> >>  running(dat,width=50,fun=fun,allow=F)
> >Error in t.test.default(x) : not enough x observations
> >>  fun2 <- function(x) mean(x)
> >>  running(dat,width=50,fun=fun2,allow=T)
> >          1:1          1:2          1:3          1:4          1:5
> >1:6
> >-0.334134613 -0.626595581 -0.368967457 -0.113737178 -0.057448771
> >0.228643936
> >          1:7          1:8          1:9         1:10         1:11
> >1:12
> >-0.058807689  0.021762463 -0.063805657  0.031931121  0.080465708
> >0.087062800
> >....
> >
> >However, this works fine.
> >
> >>  t.test(dat[1:50])$p.value
> >[1] 0.1661845
> >
> >Why doesn't t.test work with running here?
> >
> >Thanks,
> >Sean
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> --------------------------------------
> Don MacQueen
> Environmental Protection Department
> Lawrence Livermore National Laboratory
> Livermore, CA, USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From gregory_r_warnes at groton.pfizer.com  Sat Mar 27 00:09:44 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 26 Mar 2004 18:09:44 -0500
Subject: [R] Recall: Quick "running" question
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B189@groexmb02.pfizer.com>

Warnes, Gregory R would like to recall the message, "Quick "running"
question".


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From gregory_r_warnes at groton.pfizer.com  Sat Mar 27 00:10:10 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 26 Mar 2004 18:10:10 -0500
Subject: [R] RE: Quick "running" question
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B18A@groexmb02.pfizer.com>

[The new version of gregmisc will be showing up on CRAN shortly.]

> -----Original Message-----
> From: Warnes, Gregory R 
> Sent: Friday, March 26, 2004 6:07 PM
> To: 'Sean Davis'
> Subject: RE: Quick "running" question
> 
> Hi Sean,
> 
> Congratulations, you found a bug!
> 
> My "running" function took an improper shortcut.  When 
> allow.fewer=FALSE it was still passing shorter lists of 
> elements to the called function, and then overwriting the 
> results for the shorter lists with NAs.  I've now corrected 
> the code to skip evaluation of the function on lists shorter 
> than the specified length when allow.fewer=FALSE.
> 
> I'm attaching the latest version of the gregmisc package with 
> the corrected code.  I've also added a couple of other 
> features while I was mucking about...
> 
> 
> 
> -Greg
> 
> > -----Original Message-----
> > From: Sean Davis [mailto:sdavis2 at mail.nih.gov]
> > Sent: Thursday, March 25, 2004 5:25 PM
> > To: Warnes, Gregory R
> > Subject: Quick "running" question
> > 
> > 
> > Greg,
> > 
> > I am starting to use running and do the following:
> > 
> > > running(1:10,function(h) {t.test(h)},width=5)
> > Error in t.test.default(h) : not enough x observations
> > > running(1:10,function(h) {t.test(h)},width=5,allow.fewer=T)
> > Error in t.test.default(h) : not enough x observations
> > 
> > It works fine for mean, median, etc.  What am I missing?
> > 
> > Thanks,
> > Sean
> > 
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From heron at stanford.edu  Sat Mar 27 01:36:53 2004
From: heron at stanford.edu (Christian Reilly)
Date: Fri, 26 Mar 2004 16:36:53 -0800 (PST)
Subject: [R] gregmisc: install trouble 
Message-ID: <Pine.GSO.4.44.0403261625300.3968-100000@elaine24.Stanford.EDU>


Hello all,

I'm having trouble installing the package 'gregmisc'. I've run the
command:

> R CMD INSTALL -l /usr/lib/R/library /tmp/gregmisc_0.9.0.tar.gz

And I can see it shows up on the list of available packeage when I issue
> library()

But when I do

>library(gregmisc)

I get:

Error in parse(file, n, text, prompt) : syntax error on line 3105


All the other packages I've install load fine. Permissions for the
'gregmisc' library are the same as all the rest.

Any Suggestions?

I'm running on Debian Linux(Woody)

Thanks much,

Christian Reilly


---------------
Hopkins Marine Station
Stanford U
Pacific Grove, CA



From andy_liaw at merck.com  Sat Mar 27 01:41:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 26 Mar 2004 19:41:02 -0500
Subject: [R] stop() vs. error() ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>

So you still have not told us what exactly what you are looking for.  What
do you want some sort of stop() to do inside a function, that is neither an
error nor returning?  Can you show an example in some other language that
has such a feature?

Andy

> From: ivo welch [mailto:ivo.welch at yale.edu] 
> 
> Hi jim:  ahhh, but then the error messages are off for "real errors" 
> that occur later.
> 
> Hi pierre: a return() in a (2nd-level) function can be 
> different from a 
> stop().  one could argue about whether it should be possible 
> to stop() 
> in a function of course, without it being an error().
> 
> regards,
> 
> /iaw
> 
> 
> James MacDonald wrote:
> > And a closer examination of the help page would lead you to this:
> > 
> > options(show.error.messages=FALSE)
> > stop()
> > 
> > which is what I believe you want....
> > 
> > Jim
> 
> 
> also From Pierre Kleiber:
> 
> The thing is that functions don't really stop... they return. 
>  So what 
> you want is return()
>   Cheers, Pierre
> 
>



From dsheuman at rogers.com  Sat Mar 27 01:54:54 2004
From: dsheuman at rogers.com (Danny Heuman)
Date: Fri, 26 Mar 2004 19:54:54 -0500
Subject: [R] Cluster Analysis with minimum cluster size?
Message-ID: <vij9609pfm5si3api9v6v8ed50gcobf934@4ax.com>

Hi all,

Is it possible to run kmeans, pam or clara with a constraint such that
no resulting cluster has fewer than X cases?

These kmeans algorithms often find clusters that are too small for my
use.  There are usually a few clusters with 1-10 cases (generally
substantial outliers).  I then have to manually assign the small ones
to other sizable clusters.

If this doesn't exist, it there such an algorithm that does this?

Thanks,

Danny



From andrewr at uidaho.edu  Sat Mar 27 02:24:47 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 26 Mar 2004 17:24:47 -0800
Subject: [R] stop() vs. error() ?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>
Message-ID: <200403261724.47431.andrewr@uidaho.edu>

Andy,

I'm really not sure that he hasn't told us.  All he wants to do is stop the 
process, without throwing an error.  return() won't work if he's in a nested 
function, it will just send him up to the next level.  For example, let's say 
he's in a loop and calling his function in the loop.  If he detects a 
situation he wants to stop the whole program and report it.  I think that 
return will just terminate the function, and the loop will continue.

e.g.


spend.time <- function(i) {
    if (i == 50) {
        return()
    }
    if (i == 75) {
        stop("Is this an error?")
    }
}

   
for(i in 1:100)
    spend.time(i)

i

Andrew

On Friday 26 March 2004 16:41, Liaw, Andy wrote:
> So you still have not told us what exactly what you are looking for.  What
> do you want some sort of stop() to do inside a function, that is neither an
> error nor returning?  Can you show an example in some other language that
> has such a feature?
>
> Andy
>
> > From: ivo welch [mailto:ivo.welch at yale.edu]
> >
> > Hi jim:  ahhh, but then the error messages are off for "real errors"
> > that occur later.
> >
> > Hi pierre: a return() in a (2nd-level) function can be
> > different from a
> > stop().  one could argue about whether it should be possible
> > to stop()
> > in a function of course, without it being an error().
> >
> > regards,
> >
> > /iaw
> >
> > James MacDonald wrote:
> > > And a closer examination of the help page would lead you to this:
> > >
> > > options(show.error.messages=FALSE)
> > > stop()
> > >
> > > which is what I believe you want....
> > >
> > > Jim
> >
> > also From Pierre Kleiber:
> >
> > The thing is that functions don't really stop... they return.
> >  So what
> > you want is return()
> >   Cheers, Pierre
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From pkleiber at honlab.nmfs.hawaii.edu  Sat Mar 27 03:05:02 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Fri, 26 Mar 2004 16:05:02 -1000
Subject: [R] stop() vs. error() ?
In-Reply-To: <200403261724.47431.andrewr@uidaho.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>
	<200403261724.47431.andrewr@uidaho.edu>
Message-ID: <4064E14E.9080707@honlab.nmfs.hawaii.edu>

OK... how about defining a function exit() as follows

 > exit <- function() {
+  opt <- options(show.error.messages=FALSE)
+  on.exit(options(opt))
+  stop()
+}

then

 > spend.time <- function(i) {
+   print(i)
+   if (i == 5) {
+       exit()
+   }
+   if (i == 75) {
+      stop("Is this an error?")
+   }
+}
 > for(i in 1:100) spend.time(i)
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
 >



Andrew Robinson wrote:
> Andy,
> 
> I'm really not sure that he hasn't told us.  All he wants to do is stop the 
> process, without throwing an error.  return() won't work if he's in a nested 
> function, it will just send him up to the next level.  For example, let's say 
> he's in a loop and calling his function in the loop.  If he detects a 
> situation he wants to stop the whole program and report it.  I think that 
> return will just terminate the function, and the loop will continue.
> 
> e.g.
> 
> 
> spend.time <- function(i) {
>     if (i == 50) {
>         return()
>     }
>     if (i == 75) {
>         stop("Is this an error?")
>     }
> }
> 
>    
> for(i in 1:100)
>     spend.time(i)
> 
> i
> 
> Andrew
> 
> On Friday 26 March 2004 16:41, Liaw, Andy wrote:
> 
>>So you still have not told us what exactly what you are looking for.  What
>>do you want some sort of stop() to do inside a function, that is neither an
>>error nor returning?  Can you show an example in some other language that
>>has such a feature?
>>
>>Andy
>>
>>
>>>From: ivo welch [mailto:ivo.welch at yale.edu]
>>>
>>>Hi jim:  ahhh, but then the error messages are off for "real errors"
>>>that occur later.
>>>
>>>Hi pierre: a return() in a (2nd-level) function can be
>>>different from a
>>>stop().  one could argue about whether it should be possible
>>>to stop()
>>>in a function of course, without it being an error().
>>>
>>>regards,
>>>
>>>/iaw
>>>
>>>James MacDonald wrote:
>>>
>>>>And a closer examination of the help page would lead you to this:
>>>>
>>>>options(show.error.messages=FALSE)
>>>>stop()
>>>>
>>>>which is what I believe you want....
>>>>
>>>>Jim
>>>
>>>also From Pierre Kleiber:
>>>
>>>The thing is that functions don't really stop... they return.
>>> So what
>>>you want is return()
>>>  Cheers, Pierre
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From sbartel at sph.emory.edu  Sat Mar 27 03:25:02 2004
From: sbartel at sph.emory.edu (Scott Bartell)
Date: 26 Mar 2004 21:25:02 -0500
Subject: [R] dev.print
Message-ID: <1080354302.11824.159.camel@sbartel1>

Hi,

Shouldn't the following commands produce a nice jpeg file, or am I
missing something?

hist(rnorm(1000))
dev.print(file="test.jpg",device=jpeg)

I get a very tiny (6x6 pixel) jpeg every time.  Using  

dev.copy(file="test.jpg",device=jpeg) ; dev.off()

instead of dev.print produces a perfectly nice plot, but is more
cumbersome.  

R bug or my misunderstanding?

Scott
Linux user



From rxg218 at psu.edu  Sat Mar 27 03:28:08 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 26 Mar 2004 21:28:08 -0500
Subject: [R] a question about scoping functions
Message-ID: <1080354486.3157.8.camel@localhost.localdomain>

Hi,
 I've written some helper functions which are used by another function
(called func()). When func() is sourced I dont want the helper function
to be seen in the global namespace (as shown by ls()).

Currently what I have done is:

func <- function() {
  helper1 <- function() {
    ...
  }
  helper2 <- function() {
    ...
  }
  # some code
}

Is there anyway to take the functions helper1 and helper2 outside func
(but in the same file) yet keep them from showing up in the global
namespace when the source file for func() is loaded?

A related question is: say I move helper1 and helper2 to a file called
helper.R

When I want to use the helper functions inside func() I can do
source('helper.R') - but that would place them in the global namespace.
Is it possible to source some file within a func() such that the sourced
functions will be local to func()?

Thanks,
-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Did you hear that two rabbits escaped from the zoo and so far they have
only recaptured 116 of them?



From edd at debian.org  Sat Mar 27 03:35:52 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 26 Mar 2004 20:35:52 -0600
Subject: [R] dev.print
In-Reply-To: <1080354302.11824.159.camel@sbartel1>
References: <1080354302.11824.159.camel@sbartel1>
Message-ID: <20040327023552.GA21384@sonny.eddelbuettel.com>

On Fri, Mar 26, 2004 at 09:25:02PM -0500, Scott Bartell wrote:
> Shouldn't the following commands produce a nice jpeg file, or am I
> missing something?
> 
> hist(rnorm(1000))
> dev.print(file="test.jpg",device=jpeg)
[...]
> R bug or my misunderstanding?

The latter. The idiom is

	jpeg(file="/tmp/test.jpeg")     # help help(jpeg) for defaults
	hist(rnorm(1000))
	dev.off()			# important to close device
	
i.e. you open a device (as this, or pdf(), png(), ...), do your plotting and
then finalise matters by dev.off().

A frequently belaboured issue is that several of the graphics format croak
in 'headless' sessions without a $DISPLAY (e.g. for a webserver). They
really do need a display to compute font metrics etc. The canonical way out
is to the virtual display for which XFree86 has a special driver.
	
Hth, Dirk	

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From andy_liaw at merck.com  Sat Mar 27 03:38:50 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 26 Mar 2004 21:38:50 -0500
Subject: [R] a question about scoping functions
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AAB@usrymx25.merck.com>

Here's one possible alternative:

-  Write the helper functions outside of func().
-  source() the file containing all the functions.
-  save(helper1, helper2, file="helperfuns.rda")
-  rm(helper1, helper2)
-  attach("helperfuns.rda")

Eventually you may want to put things into a package...

HTH,
Andy

> From: Rajarshi Guha
> 
> Hi,
>  I've written some helper functions which are used by another function
> (called func()). When func() is sourced I dont want the 
> helper function
> to be seen in the global namespace (as shown by ls()).
> 
> Currently what I have done is:
> 
> func <- function() {
>   helper1 <- function() {
>     ...
>   }
>   helper2 <- function() {
>     ...
>   }
>   # some code
> }
> 
> Is there anyway to take the functions helper1 and helper2 outside func
> (but in the same file) yet keep them from showing up in the global
> namespace when the source file for func() is loaded?
> 
> A related question is: say I move helper1 and helper2 to a file called
> helper.R
> 
> When I want to use the helper functions inside func() I can do
> source('helper.R') - but that would place them in the global 
> namespace.
> Is it possible to source some file within a func() such that 
> the sourced
> functions will be local to func()?
> 
> Thanks,
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Did you hear that two rabbits escaped from the zoo and so far 
> they have
> only recaptured 116 of them?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Sat Mar 27 03:43:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 26 Mar 2004 21:43:05 -0500
Subject: [R] stop() vs. error() ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AAC@usrymx25.merck.com>

OK, I guess I'm dense (again?)...

Stop where, and do what?  I don't know many languages, but I haven't seen
(or heard) any that allows stopping in the middle of a function and do
nothing.  What's the poor user to do at that point?

The only thing I could think of is when the programmer is debugging code,
and browser() allows execution to stop at that point.  Is that what Ivo is
looking for?  It was, as far as I can see, not even implied in his original
message.

Andy

> From: Andrew Robinson [mailto:andrewr at uidaho.edu] 
> Sent: Friday, March 26, 2004 8:25 PM
> To: Liaw, Andy
> Cc: r-help at stat.math.ethz.ch; 'ivo welch'
> Subject: Re: [R] stop() vs. error() ?
> 
> 
> Andy,
> 
> I'm really not sure that he hasn't told us.  All he wants to 
> do is stop the 
> process, without throwing an error.  return() won't work if 
> he's in a nested 
> function, it will just send him up to the next level.  For 
> example, let's say 
> he's in a loop and calling his function in the loop.  If he detects a 
> situation he wants to stop the whole program and report it.  
> I think that 
> return will just terminate the function, and the loop will continue.
> 
> e.g.
> 
> 
> spend.time <- function(i) {
>     if (i == 50) {
>         return()
>     }
>     if (i == 75) {
>         stop("Is this an error?")
>     }
> }
> 
>    
> for(i in 1:100)
>     spend.time(i)
> 
> i
> 
> Andrew
> 
> On Friday 26 March 2004 16:41, Liaw, Andy wrote:
> > So you still have not told us what exactly what you are 
> looking for.  What
> > do you want some sort of stop() to do inside a function, 
> that is neither an
> > error nor returning?  Can you show an example in some other 
> language that
> > has such a feature?
> >
> > Andy
> >
> > > From: ivo welch [mailto:ivo.welch at yale.edu]
> > >
> > > Hi jim:  ahhh, but then the error messages are off for 
> "real errors"
> > > that occur later.
> > >
> > > Hi pierre: a return() in a (2nd-level) function can be
> > > different from a
> > > stop().  one could argue about whether it should be possible
> > > to stop()
> > > in a function of course, without it being an error().
> > >
> > > regards,
> > >
> > > /iaw
> > >
> > > James MacDonald wrote:
> > > > And a closer examination of the help page would lead 
> you to this:
> > > >
> > > > options(show.error.messages=FALSE)
> > > > stop()
> > > >
> > > > which is what I believe you want....
> > > >
> > > > Jim
> > >
> > > also From Pierre Kleiber:
> > >
> > > The thing is that functions don't really stop... they return.
> > >  So what
> > > you want is return()
> > >   Cheers, Pierre
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : 
> http://www.uidaho.edu/~andrewr
> Moscow ID 83843                
>       Or: http://www.biometrics.uidaho.edu
> No statement above necessarily represents my employer's opinion.
> 
> 
>



From phddas at yahoo.com  Sat Mar 27 06:34:21 2004
From: phddas at yahoo.com (Fred J.)
Date: Fri, 26 Mar 2004 21:34:21 -0800 (PST)
Subject: [R] variable scope
Message-ID: <20040327053421.56805.qmail@web20511.mail.yahoo.com>

Hello

getdata <- function(p){
  fname <- NULL; dl <- list(NULL)#build the sturcture
  dt <- read.csv(file.path(d,i),header=F)  #data frame
  ret <- builddl(dt,s) #where "s" is a string
}

how can I get this following function to use fname and
dl from the above function without passing them down
the chain?

builddl <- function(q,s){
  fname <- c(fname,s)
  dl <- list( dl, q)
}

thanks



From ripley at stats.ox.ac.uk  Sat Mar 27 09:31:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 08:31:46 +0000 (GMT)
Subject: [R] dev.print
In-Reply-To: <20040327023552.GA21384@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.44.0403270827400.11650-100000@gannet.stats>

dev.print is designed for *print* devices which compute sizes in inches.
The help says

     'dev.print' is most useful for producing a postscript print (its
     default) when the following applies.  Unless 'file' is specified,
     the plot will be printed.  Unless 'width', 'height' and
     'pointsize' are specified the plot dimensions will be taken from
     the current device, shrunk if necessary to fit on the paper.
     ...

So you asked for a plot of about 7 x 7, and jpeg expected points.  Moral: 
read the help page and specify width and height if it is not what you 
expected.

On Fri, 26 Mar 2004, Dirk Eddelbuettel wrote:

> On Fri, Mar 26, 2004 at 09:25:02PM -0500, Scott Bartell wrote:
> > Shouldn't the following commands produce a nice jpeg file, or am I
> > missing something?
> > 
> > hist(rnorm(1000))
> > dev.print(file="test.jpg",device=jpeg)
> [...]
> > R bug or my misunderstanding?
> 
> The latter. The idiom is
> 
> 	jpeg(file="/tmp/test.jpeg")     # help help(jpeg) for defaults
> 	hist(rnorm(1000))
> 	dev.off()			# important to close device
> 	
> i.e. you open a device (as this, or pdf(), png(), ...), do your plotting and
> then finalise matters by dev.off().
> 
> A frequently belaboured issue is that several of the graphics format croak
> in 'headless' sessions without a $DISPLAY (e.g. for a webserver). They
> really do need a display to compute font metrics etc. The canonical way out
> is to the virtual display for which XFree86 has a special driver.
> 	
> Hth, Dirk	
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Sat Mar 27 11:59:49 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 27 Mar 2004 05:59:49 -0500
Subject: [R] variable scope
In-Reply-To: <20040327053421.56805.qmail@web20511.mail.yahoo.com>
References: <20040327053421.56805.qmail@web20511.mail.yahoo.com>
Message-ID: <iana60tehkd3k84vme33hq5u59galh34jn@4ax.com>

On Fri, 26 Mar 2004 21:34:21 -0800 (PST), you wrote:

>Hello
>
>getdata <- function(p){
>  fname <- NULL; dl <- list(NULL)#build the sturcture
>  dt <- read.csv(file.path(d,i),header=F)  #data frame
>  ret <- builddl(dt,s) #where "s" is a string
>}
>
>how can I get this following function to use fname and
>dl from the above function without passing them down
>the chain?
>
>builddl <- function(q,s){
>  fname <- c(fname,s)
>  dl <- list( dl, q)
>}

The normal way would be to define builddl() within getdata(), i.e.

getdata <- function(p){
  builddl <- function(q,s){
    fname <- c(fname,s)
    dl <- list( dl, q) 
  }
  fname <- NULL; dl <- list(NULL)#build the sturcture
  dt <- read.csv(file.path(d,i),header=F)  #data frame
  ret <- builddl(dt,s) #where "s" is a string
}

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Sat Mar 27 12:29:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 27 Mar 2004 12:29:39 +0100
Subject: [R] gregmisc: install trouble
References: <Pine.GSO.4.44.0403261625300.3968-100000@elaine24.Stanford.EDU>
Message-ID: <406565A3.4352F2EB@statistik.uni-dortmund.de>

Christian Reilly wrote:
> 
> Hello all,
> 
> I'm having trouble installing the package 'gregmisc'. I've run the
> command:
> 
> > R CMD INSTALL -l /usr/lib/R/library /tmp/gregmisc_0.9.0.tar.gz
> 
> And I can see it shows up on the list of available packeage when I issue
> > library()
> 
> But when I do
> 
> >library(gregmisc)
> 
> I get:
> 
> Error in parse(file, n, text, prompt) : syntax error on line 3105
> 
> All the other packages I've install load fine. Permissions for the
> 'gregmisc' library are the same as all the rest.
> 
> Any Suggestions?
> 
> I'm running on Debian Linux(Woody)
> 
> Thanks much,
> 
> Christian Reilly
> 
> ---------------
> Hopkins Marine Station
> Stanford U
> Pacific Grove, CA


Which version of R is this? Works fine with R-1.8.1 and R-1.9.0 beta.
Please download the package again and reinstall.

Uwe Ligges



From apiszcz at solarrain.com  Sat Mar 27 12:54:57 2004
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sat, 27 Mar 2004 06:54:57 -0500 (EST)
Subject: [R] par("yaxp") linear versus log tick mark coordinates
Message-ID: <Pine.LNX.4.58.0403270651210.30227@l1>


As a linear axis I obtain the correct number of intervals
for the tick marks.


> source("test-lin.r")
> par("yaxp")
[1]     0 20000     4


However log returns 1?

> source("test-log.r")
> par("yaxp")
[1]     1 10000     1
>

I'll use log10(par("yaxp")[2]) for now.
I was wondering why '1' is returned instead of 4.

Thx.



From ripley at stats.ox.ac.uk  Sat Mar 27 14:23:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 13:23:22 +0000 (GMT)
Subject: [R] With which version of XFree86 can R compile?
In-Reply-To: <x2d66zl03f.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0403271258410.13391-100000@gannet.stats>

On 26 Mar 2004, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > Note this is not R as distributed by R-core (which does not have
> > dataentry_mb.c) but I believe some `Japanized' distribution.  People might
> > want to check if the problem is in the changes (which I suspect) or R
> > itself.
> 
> Yes. On the other hand, the original dataentry.c begins with
> 
> #include "Defn.h"
> #include "Print.h"
> 
> /* don't use X11 function prototypes (which tend to ...): */
> #define NeedFunctionPrototypes 0
> #include <X11/X.h>
> #include <X11/Xlib.h>
> ...
> 
> and there is the off chance that the newer X11 is using a symbol in
> our Defn.h or Print.h. If so, then we'd like to hear about it ASAP... 

I downloaded the XFree86-4.4.0 sources.  One issue is that 
NeedFunctionPrototypes is no longer supported in constructs like

extern XFontStruct *XLoadQueryFont(
#if NeedFunctionPrototypes
    Display*            /* display */,
    _Xconst char*       /* name */
#endif
);

which explains the warnings on incompatible types.  However, removing 

#define NeedFunctionPrototypes 0

does not cause the other errors on my XFree86 (8.3.3 I think).

More digging needed ....

Brian

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Sat Mar 27 14:31:15 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 27 Mar 2004 08:31:15 -0500
Subject: [R] a question about scoping functions
In-Reply-To: <1080354486.3157.8.camel@localhost.localdomain>
References: <1080354486.3157.8.camel@localhost.localdomain>
Message-ID: <d20b60tornvhr8qgcma1uga29buhajkhdb@4ax.com>

On Fri, 26 Mar 2004 21:28:08 -0500, Rajarshi Guha <rxg218 at psu.edu>
wrote :

>Is there anyway to take the functions helper1 and helper2 outside func
>(but in the same file) yet keep them from showing up in the global
>namespace when the source file for func() is loaded?

A simpler (but slightly less effective way) than what Andy suggested
is to name the helper functions with names starting with a dot.  By
default ls() won't show objects like that (just like ls in Unix).
E.g.

> visible <- 1
> .notvisible <- 2
> ls()
[1] "visible"

>A related question is: say I move helper1 and helper2 to a file called
>helper.R
>
>When I want to use the helper functions inside func() I can do
>source('helper.R') - but that would place them in the global namespace.
>Is it possible to source some file within a func() such that the sourced
>functions will be local to func()?

Putting the line

  source('helper.R', local = TRUE)

in func() will do that.

Duncan



From ripley at stats.ox.ac.uk  Sat Mar 27 14:42:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 13:42:54 +0000 (GMT)
Subject: [R] With which version of XFree86 can R compile?
In-Reply-To: <Pine.LNX.4.44.0403271258410.13391-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0403271339290.16733-100000@gannet.stats>

Almost all these issues are now resolved in the 1.9.0-beta sources, which 
I can now build against the XFree86 4.4.0 headers.  One that remains is

#define KeySym int

which is presumably there for some historical reason but should not be
with current X headers, and so needs a configure test.

On Sat, 27 Mar 2004, Prof Brian Ripley wrote:

> On 26 Mar 2004, Peter Dalgaard wrote:
> 
> > Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> > 
> > > Note this is not R as distributed by R-core (which does not have
> > > dataentry_mb.c) but I believe some `Japanized' distribution.  People might
> > > want to check if the problem is in the changes (which I suspect) or R
> > > itself.
> > 
> > Yes. On the other hand, the original dataentry.c begins with
> > 
> > #include "Defn.h"
> > #include "Print.h"
> > 
> > /* don't use X11 function prototypes (which tend to ...): */
> > #define NeedFunctionPrototypes 0
> > #include <X11/X.h>
> > #include <X11/Xlib.h>
> > ...
> > 
> > and there is the off chance that the newer X11 is using a symbol in
> > our Defn.h or Print.h. If so, then we'd like to hear about it ASAP... 
> 
> I downloaded the XFree86-4.4.0 sources.  One issue is that 
> NeedFunctionPrototypes is no longer supported in constructs like
> 
> extern XFontStruct *XLoadQueryFont(
> #if NeedFunctionPrototypes
>     Display*            /* display */,
>     _Xconst char*       /* name */
> #endif
> );
> 
> which explains the warnings on incompatible types.  However, removing 
> 
> #define NeedFunctionPrototypes 0
> 
> does not cause the other errors on my XFree86 (8.3.3 I think).
> 
> More digging needed ....
> 
> Brian
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Sat Mar 27 15:31:48 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 27 Mar 2004 09:31:48 -0500
Subject: [R] stop() vs. error() ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AAE@usrymx25.merck.com>

[I don't see any reason to keep this off-list.  Apology to those bored by
this thread.]

> From: Andrew Robinson [mailto:andrewr at uidaho.edu] 
> 
> On Friday 26 March 2004 18:43, you wrote:
> > OK, I guess I'm dense (again?)...

I ain't kidding!

> > Stop where, and do what?  
> 
> Stop where the condition is detected, and report which among 
> the plausible 
> conditions has been detected.
>
> > I don't know many languages, but I haven't seen
> > (or heard) any that allows stopping in the middle of a 
> function and do
> > nothing.  
> 
> He doesn't want to do nothing.  He just doesn't want to throw 
> an error.

So the intent is to jump all the way back to the R prompt?  The only good
reason for doing so, that my apparently not very good brain can think of, is
when there's an error.  What I learned in a programming class is that a
program should have one and only one exit point.  I.e., if f() calls g(),
it's not desirable to have g() terminating the execution, but rather to have
g() return a value that f() can check for.  Of course, this can get tedious
if you have f() calling g(), g() calling h(), etc., but it should make the
code easier to maintain down the road.

Having this behavior will also lead to confusion.  Consider this scenario:
Suppose I write a function f() that uses the Stop() and place it in a
package, and upload to CRAN.  Some poor soul downloads the package (actually
believing that it'll do something useful), and try something like:

result <- apply(x, 1, f, ...)

and somehow trigger the Stop() inside f().  There's no error raised.  What
should the user expect?  I can just see the poor R core fuming over the bug
reports for apply() not returning something it promised to in ?apply...

> > What's the poor user to do at that point?
> 
> RTFM, just like at every other point.

If you would be so kind as to as advise which FM to R, I'd be more than
happy to do so.  I'm very serious.

Best,
Andy
 
> Andrew
> -- 
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : 
> http://www.uidaho.edu/~andrewr
> Moscow ID 83843                
>       Or: http://www.biometrics.uidaho.edu
> No statement above necessarily represents my employer's opinion.
> 
> 
>



From adam at ajtee.uklinux.net  Sat Mar 27 13:25:32 2004
From: adam at ajtee.uklinux.net (Adam Tee)
Date: Sat, 27 Mar 2004 12:25:32 +0000
Subject: [R] Fwd: MDS problems [ajtee@ajtee.uklinux.net]
In-Reply-To: <1080310050.16349.9.camel@biol102145.oulu.fi> (from
	jarioksa@sun3.oulu.fi on Fri, Mar 26, 2004 at 14:07:30 +0000)
References: <20040326120343.GA1750@ajtee> <20040326130255.GA9812@ajtee>
	<1080310050.16349.9.camel@biol102145.oulu.fi>
Message-ID: <20040327122532.GA3719@ajtee>

On 03/26/04 14:07:30, Jari Oksanen wrote:
> The error message is clear: You have some identical sites so that
> theirdistance is zero. I think the canonical solution is to remove  
> the duplicate cases from the data before calculating the  
> dissimilarities. If your data frame is called X:
> 
> Xuniq <- unique(X)
> x.dist <- dist(Xuniq)
> 
> (or, as a one-liner: x.dist <- dist(unique(X))
>

One of the difficulties is that it is significant that some of the data  
is identical as I am comparing musical sequences. The above does not  
work as I am not dealing with the raw data. I am using my own  
dissimilarity measure output by a separate program.  The reason for  
using MDS is to visualise the data based on the developed dissimilarity  
measure.

Would it be better to use PCA ??



Adam



From hodgess at gator.uhd.edu  Sat Mar 27 16:03:02 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Sat, 27 Mar 2004 09:03:02 -0600
Subject: [R] availability of version 1.9.0?
Message-ID: <200403271503.i2RF32p05763@gator.dt.uh.edu>

Dear R People:

When will version 1.9 (for Windows) be ready, please?

My reason for asking:  there is an interesting library from
Bioconductor called tkWidgets.  However, it will only
work with version 1.9.0 or higher.

Are there ways around this, or should I just be patient?

Thanks so much in advance!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From phddas at yahoo.com  Sat Mar 27 16:04:48 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 27 Mar 2004 07:04:48 -0800 (PST)
Subject: [R] building a list in loop
Message-ID: <20040327150448.53402.qmail@web20503.mail.yahoo.com>

Hello

getdata <- function(p){
fname <- NULL; dl <- list()#build the sturcture
  builddl <- function(q,s){
   fname <<- c(fname,s) #where "s" is a string
   dl <<- list( dl, dt2)
 }
 list(names = fname, data = dl)
}
data <- getdata("c:\somepath")

> data
$names
[1] "fname"

$data
$data[[1]]  <--- since there is no [[1]] how can I
stop 
list()      <--- this from happening? 

$data[[2]]
... lists all rows and cols here

thanks



From phddas at yahoo.com  Sat Mar 27 16:08:09 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 27 Mar 2004 07:08:09 -0800 (PST)
Subject: [R] variable scope
In-Reply-To: <iana60tehkd3k84vme33hq5u59galh34jn@4ax.com>
Message-ID: <20040327150809.25236.qmail@web20513.mail.yahoo.com>


--- Duncan Murdoch <dmurdoch at pair.com> wrote:
> On Fri, 26 Mar 2004 21:34:21 -0800 (PST), you wrote:
> 
> >Hello
> >
> >getdata <- function(p){
> >  fname <- NULL; dl <- list(NULL)#build the
> sturcture
> >  dt <- read.csv(file.path(d,i),header=F)  #data
> frame
> >  ret <- builddl(dt,s) #where "s" is a string
> >}
> >
> >how can I get this following function to use fname
> and
> >dl from the above function without passing them
> down
> >the chain?
> >
> >builddl <- function(q,s){
> >  fname <- c(fname,s)
> >  dl <- list( dl, q)
> >}
> 
> The normal way would be to define builddl() within
> getdata(), i.e.
> 
> getdata <- function(p){
>   builddl <- function(q,s){
>     fname <- c(fname,s)
>     dl <- list( dl, q) 
>   }
>   fname <- NULL; dl <- list(NULL)#build the
> sturcture
>   dt <- read.csv(file.path(d,i),header=F)  #data

the only problem with this method is that the debug
package$mtrace will not step into the sub.fun builddl
becuase it will treat it as one line statement. hummm
> frame
>   ret <- builddl(dt,s) #where "s" is a string
> }
> 
> Duncan Murdoch



From jgentry at jimmy.harvard.edu  Sat Mar 27 16:09:48 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Sat, 27 Mar 2004 10:09:48 -0500 (EST)
Subject: [R] availability of version 1.9.0?
In-Reply-To: <200403271503.i2RF32p05763@gator.dt.uh.edu>
Message-ID: <Pine.SOL.4.20.0403271008210.10870-100000@santiam.dfci.harvard.edu>

> When will version 1.9 (for Windows) be ready, please?

You can download the beta version at
http://cran.r-project.org/bin/windows/base/rdevel.html

> My reason for asking:  there is an interesting library from
> Bioconductor called tkWidgets.  However, it will only
> work with version 1.9.0 or higher.

Note that the version of tkWidgets that is available as part of the 1.3
release of Bioconductor works with 1.8.x versions of R.  I'm not sure if
you're looking for functionality specifically found in the devel version
of tkWidgets or not though.



From ivo.welch at yale.edu  Sat Mar 27 16:13:06 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Sat, 27 Mar 2004 10:13:06 -0500
Subject: [R] stop() vs. error() ?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>
Message-ID: <40659A02.7070509@yale.edu>


hi andy:  mea culpa.  it is the exit function in most other languages.  
It would be exit(0) invocation in C (C++) and perl, for example.

regards, /iaw

Liaw, Andy wrote:

>So you still have not told us what exactly what you are looking for.  What
>do you want some sort of stop() to do inside a function, that is neither an
>error nor returning?  Can you show an example in some other language that
>has such a feature?
>
>Andy
>  
>
>



From bates at stat.wisc.edu  Sat Mar 27 16:20:53 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 27 Mar 2004 09:20:53 -0600
Subject: [R] availability of version 1.9.0?
In-Reply-To: <200403271503.i2RF32p05763@gator.dt.uh.edu>
References: <200403271503.i2RF32p05763@gator.dt.uh.edu>
Message-ID: <6r7jx6qpt6.fsf@bates4.stat.wisc.edu>

Erin Hodgess <hodgess at gator.uhd.edu> writes:

> When will version 1.9 (for Windows) be ready, please?

The original release date for the R-1.9.0 source code was April 4 but
there will be an as-yet-unspecified delay due to a breakin on the main
archive site.  We had to do a lot of adjustments this week, including
creating a completely new archive site, and we are a bit paranoid right
now about security.  We think we have the generation of the beta
releases set up again but Peter had to leave for ENAR before that
could be completely tested.

Best guess currently is a source code release during the week of April
11 followed a few days later by the Windows binary.

I believe that Duncan Murdoch has beta test versions of the Windows
binary for R-1.9.0 available from his web site.  I have forgotten the
URL.  Perhaps someone can remind us.

> My reason for asking:  there is an interesting library from
> Bioconductor called tkWidgets.  However, it will only
> work with version 1.9.0 or higher.
> 
> Are there ways around this, or should I just be patient?
> 
> Thanks so much in advance!
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From ligges at statistik.uni-dortmund.de  Sat Mar 27 16:46:54 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 27 Mar 2004 16:46:54 +0100
Subject: [R] building a list in loop
In-Reply-To: <20040327150448.53402.qmail@web20503.mail.yahoo.com>
References: <20040327150448.53402.qmail@web20503.mail.yahoo.com>
Message-ID: <4065A1EE.5000707@statistik.uni-dortmund.de>

Fred J. wrote:

> Hello
> 
> getdata <- function(p){
> fname <- NULL; dl <- list()#build the sturcture
>   builddl <- function(q,s){
>    fname <<- c(fname,s) #where "s" is a string
>    dl <<- list( dl, dt2)

"Fred J." alias "phddas", you are really not going to use <<- here, 
please read the manuals.


>  }
>  list(names = fname, data = dl)
> }
> data <- getdata("c:\somepath")

This won't work, please read the R for Windows FAQs.


> 
>>data
> 
> $names
> [1] "fname"
> 
> $data
> $data[[1]]  <--- since there is no [[1]] how can I
> stop 

It's just a matter of printing the list, in one list elements are named, 
in the other elements are unnamed.


> list()      <--- this from happening? 

What are you going to stop???

I guess some kind souls on this list would be happy to help, but you 
need to specify your problem precisely enough! In this case, I don't 
understand your problem, and I even do not understand your question.

Let me cite the last line of all R-help messages:
"PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html".

Uwe Ligges


> $data[[2]]
> ... lists all rows and cols here
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmurdoch at pair.com  Sat Mar 27 16:50:49 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 27 Mar 2004 10:50:49 -0500
Subject: [R] variable scope
In-Reply-To: <20040327150809.25236.qmail@web20513.mail.yahoo.com>
References: <iana60tehkd3k84vme33hq5u59galh34jn@4ax.com>
	<20040327150809.25236.qmail@web20513.mail.yahoo.com>
Message-ID: <th8b601vpkcvv3np21182jv60vqn1nlnsf@4ax.com>

On Sat, 27 Mar 2004 07:08:09 -0800 (PST), "Fred J." <phddas at yahoo.com>
wrote :

>> The normal way would be to define builddl() within
>> getdata(), i.e.
>> 
>> getdata <- function(p){
>>   builddl <- function(q,s){
>>     fname <- c(fname,s)
>>     dl <- list( dl, q) 
>>   }
>>   fname <- NULL; dl <- list(NULL)#build the
>> sturcture
>>   dt <- read.csv(file.path(d,i),header=F)  #data
>
>the only problem with this method is that the debug
>package$mtrace will not step into the sub.fun builddl
>becuase it will treat it as one line statement. 

I haven't used the debug package much, but with the standard "debug"
function you can call "debug(builddl)" within the function, after
creating builddl, to turn on debugging.  

Duncan Murdoch



From ccleland at optonline.net  Sat Mar 27 16:52:07 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 27 Mar 2004 10:52:07 -0500
Subject: [R] availability of version 1.9.0?
In-Reply-To: <6r7jx6qpt6.fsf@bates4.stat.wisc.edu>
References: <200403271503.i2RF32p05763@gator.dt.uh.edu>
	<6r7jx6qpt6.fsf@bates4.stat.wisc.edu>
Message-ID: <4065A327.2010005@optonline.net>

Douglas Bates wrote:
> Erin Hodgess <hodgess at gator.uhd.edu> writes:
> 
>>When will version 1.9 (for Windows) be ready, please?
> 
> The original release date for the R-1.9.0 source code was April 4 but
> there will be an as-yet-unspecified delay due to a breakin on the main
> archive site.  We had to do a lot of adjustments this week, including
> creating a completely new archive site, and we are a bit paranoid right
> now about security.  We think we have the generation of the beta
> releases set up again but Peter had to leave for ENAR before that
> could be completely tested.
> 
> Best guess currently is a source code release during the week of April
> 11 followed a few days later by the Windows binary.
> 
> I believe that Duncan Murdoch has beta test versions of the Windows
> binary for R-1.9.0 available from his web site.  I have forgotten the
> URL.  Perhaps someone can remind us.
> ... 

http://cran.r-project.org/bin/windows/base/rdevel.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From dmurdoch at pair.com  Sat Mar 27 16:53:10 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 27 Mar 2004 10:53:10 -0500
Subject: [R] availability of version 1.9.0?
In-Reply-To: <6r7jx6qpt6.fsf@bates4.stat.wisc.edu>
References: <200403271503.i2RF32p05763@gator.dt.uh.edu>
	<6r7jx6qpt6.fsf@bates4.stat.wisc.edu>
Message-ID: <3o8b60tsd1kop4ds4jsn08rllvnmfbenbr@4ax.com>

On 27 Mar 2004 09:20:53 -0600, Douglas Bates <bates at stat.wisc.edu>
wrote :

>I believe that Duncan Murdoch has beta test versions of the Windows
>binary for R-1.9.0 available from his web site.  I have forgotten the
>URL.  Perhaps someone can remind us.

No, they're on CRAN now.  There's a link from the main download page
for the Windows binaries.

I am doing frequent updates of the binary builds there.  They should
be at most a few days old.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Sat Mar 27 16:59:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 15:59:28 +0000 (GMT)
Subject: [R] stop() vs. error() ?
In-Reply-To: <40659A02.7070509@yale.edu>
Message-ID: <Pine.LNX.4.44.0403271558330.16925-100000@gannet.stats>

That's called q() in R.

On Sat, 27 Mar 2004, ivo welch wrote:

> 
> hi andy:  mea culpa.  it is the exit function in most other languages.  
> It would be exit(0) invocation in C (C++) and perl, for example.
> 
> regards, /iaw
> 
> Liaw, Andy wrote:
> 
> >So you still have not told us what exactly what you are looking for.  What
> >do you want some sort of stop() to do inside a function, that is neither an
> >error nor returning?  Can you show an example in some other language that
> >has such a feature?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phddas at yahoo.com  Sat Mar 27 17:19:20 2004
From: phddas at yahoo.com (Fred J.)
Date: Sat, 27 Mar 2004 08:19:20 -0800 (PST)
Subject: [R] building a list in loop
In-Reply-To: <4065A1EE.5000707@statistik.uni-dortmund.de>
Message-ID: <20040327161920.72507.qmail@web20501.mail.yahoo.com>


--- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote:
> Fred J. wrote:
> 
> > Hello
> > 
> > getdata <- function(p){
> > fname <- NULL; dl <- list()#build the sturcture
> >   builddl <- function(q,s){
> >    fname <<- c(fname,s) #where "s" is a string
> >    dl <<- list( dl, dt2)
> 
> "Fred J." alias "phddas", you are really not going
> to use <<- here, 
> please read the manuals.

thats what I do every time I think of asking a Q. I
sure could use some help in pointing out which manual
since there are few and which title/or sub-title if
you know off hand.

> 
> 
> >  }
> >  list(names = fname, data = dl)
> > }
> > data <- getdata("c:\somepath")
> 
> This won't work, please read the R for Windows FAQs.
> 
> 
> > 
> >>data
> > 
> > $names
> > [1] "fname"
> > 
> > $data
> > $data[[1]]  <--- since there is no [[1]] how can I
> > stop 
> 
> It's just a matter of printing the list, in one list
> elements are named, 
> in the other elements are unnamed.
> 
> 
> > list()      <--- this from happening? 
> 
> What are you going to stop???
> 
> I guess some kind souls on this list would be happy
> to help, but you 
> need to specify your problem precisely enough! In
> this case, I don't 
> understand your problem, and I even do not
> understand your question.
> 
> Let me cite the last line of all R-help messages:
> "PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html".

again, I would surely like to have some guide in HowTo
find the answer for a specific problem rather than get
it handed to me on a plate. that will payoff next for
next times.

many thanks

> 
> Uwe Ligges
> 
> 
> > $data[[2]]
> > ... lists all rows and cols here
> > 
> > thanks
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ivo.welch at yale.edu  Sat Mar 27 17:39:36 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Sat, 27 Mar 2004 11:39:36 -0500
Subject: [R] stop() vs. error() ?
In-Reply-To: <Pine.LNX.4.44.0403271558330.16925-100000@gannet.stats>
References: <Pine.LNX.4.44.0403271558330.16925-100000@gannet.stats>
Message-ID: <4065AE48.9040908@yale.edu>


hi brian:  thanks.  I will put in a suggestion that the docs refer to 
q() in "see also" for "stop".

regards,

/ivo


Prof Brian Ripley wrote:
> That's called q() in R.
> 
<snip>



From ripley at stats.ox.ac.uk  Sat Mar 27 17:47:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 16:47:06 +0000 (GMT)
Subject: [R] stop() vs. error() ?
In-Reply-To: <4065AE48.9040908@yale.edu>
Message-ID: <Pine.LNX.4.44.0403271644490.17029-100000@gannet.stats>

On Sat, 27 Mar 2004, ivo welch wrote:

> hi brian:  thanks.  I will put in a suggestion that the docs refer to 
> q() in "see also" for "stop".

I don't think anyone else is confusing `exit' with `stop', though. I hope 
you don't when driving ....

> 
> regards,
> 
> /ivo
> 
> 
> Prof Brian Ripley wrote:
> > That's called q() in R.
> > 
> <snip>
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jarioksa at sun3.oulu.fi  Sat Mar 27 18:07:39 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Sat, 27 Mar 2004 19:07:39 +0200
Subject: [R] Fwd: MDS problems [ajtee@ajtee.uklinux.net]
In-Reply-To: <20040327122532.GA3719@ajtee>
References: <20040326120343.GA1750@ajtee> <20040326130255.GA9812@ajtee>
	<1080310050.16349.9.camel@biol102145.oulu.fi>
	<20040327122532.GA3719@ajtee>
Message-ID: <1080407259.4065b4db1530c@webmail.oulu.fi>

Quoting Adam Tee <adam at ajtee.uklinux.net>:

> On 03/26/04 14:07:30, Jari Oksanen wrote:
> > The error message is clear: You have some identical sites so that
> > theirdistance is zero. I think the canonical solution is to remove  
> > the duplicate cases from the data before calculating the  
> > dissimilarities. If your data frame is called X:
> > 
> > Xuniq <- unique(X)
> > x.dist <- dist(Xuniq)
> > 
> > (or, as a one-liner: x.dist <- dist(unique(X))
> >
> 
> One of the difficulties is that it is significant that some of the data  
> is identical as I am comparing musical sequences. The above does not  
> work as I am not dealing with the raw data. I am using my own  
> dissimilarity measure output by a separate program.  The reason for  
> using MDS is to visualise the data based on the developed dissimilarity  
> measure.
> 
> Would it be better to use PCA ??

If your own dissimilarity measure really is Euclidean distance, then using
cmdscale (metric scaling) is an inefficient way of doing PCA. Otherwise it is
difficult to see how you could use PCA without access to raw data.

PCA and metric MDS (cmdscale) are adequate if you think you have a linear
mapping form musical similarities to your visual presentation. That's a brave
assumption, but a common and standard one ("canonical", would I say).

Identical observations should have identical locations in *MDS. Removing them is
just removing duplicates. In principle, Kruskal's NMDS (isoMDS) should be able
to deal with them, but Sammon scaling (sammon) would break down with zero
dissimilarities, like B.D. Ripley wrote. In a way, it is just a weighting
problem in isoMDS: if you have identical observations and you really want to
keep them, you should give larger weight to those dissimilarities in assessing
stress. I think you cannot do it now in isoMDS (I think KYST accepted them
silently). In most cases such a weighting would have no observable effect, and
you can just remove the duplicates and have equal weights.

It is not too hard to write a function that prunes identical observations from
dissimilarity matrices. You just  remove columns & rows so that no off-diagonal
zeros are left. I don't have time and patience to write up it just now (you
need as.matrix.distance, then go down the items and delete a row and the
correspoding column if you find an off-diagonal zero).

cheers, jari oksanen



From malbani at fas.harvard.edu  Sat Mar 27 18:25:24 2004
From: malbani at fas.harvard.edu (Marco Albani)
Date: Sat, 27 Mar 2004 12:25:24 -0500
Subject: [R] Estimating SIR model parameters in R
Message-ID: <4065B904.5090404@fas.harvard.edu>

Hello,

does anyone have a reference, pointer, example etc.. on how to estimate 
the parameters for a classical SIR model for spread of disease with R?

I am particlarly interested in the case where only the R term is known 
in time.


Cheers

Marco



From york at zipcon.net  Sat Mar 27 18:39:43 2004
From: york at zipcon.net (Anne York)
Date: Sat, 27 Mar 2004 09:39:43 -0800 (PST)
Subject: [R] multicolumn sort on dataframe?
Message-ID: <Pine.LNX.4.58.0403270931510.10859@localhost.localdomain>

On Fri 26 Mar 2004, Jeff D. Hamann wrote:
                                                                                
>I couldn't find any reference to this in the FAQ, but is it 
>possible to sort a  dataframe by multiple columns?
                                                                                
>I've created some code, similar to the following:
                                                                                
>nspr.code <- sp.results$sp.code[order( sp.results$sp.code )]
>nspr.tpa <- sp.results$tpa[order( sp.results$sp.code )]
                                                                                
>nspr.code <- as.character( levels( nspr.code ) )[nspr.code]
>nspr.tpa <- as.numeric( levels( nspr.tpa ) )[nspr.tpa]
                                                                                
>hope <- as.data.frame( cbind( nspr.code, as.numeric(nspr.tpa) ) )


A simple way to sort multiple columns is to paste them 
together and sort the resulting character vector. THat way 
you only have to do one sort. This is a very old method 
taught to me in the first computer course I ever took (date 
censored); the instructor attributed the method to Von 
Neumann but I have no reference. 

You have to be careful choosing the sep character in paste. 

Here is an example


> set.seed(78) 
> foo = data.frame(x= sample(letters[1:3],5,replace=TRUE),
                   y= sample(1:5,5,replace=TRUE))
> foo
  x y
1 c 3
2 c 2
3 b 2
4 c 1
5 c 3

Sorting on y and then by x: 

> my.order=order(foo.paste=paste(foo[,2],foo[,1],sep="/"))
> my.order
[1] 4 3 2 1 5


> my.order=order(paste(foo[,1],foo[,2],sep="/"))
> foo[my.order,]
  x y
3 b 2
4 c 1
2 c 2
1 c 3
5 c 3
>



From pburns at pburns.seanet.com  Sat Mar 27 20:24:33 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 27 Mar 2004 19:24:33 +0000
Subject: [R] Re: [S] scalability
References: <OF08949065.1D44A7AE-ON86256E63.006E1A84@cr.usgs.gov>
	<4064B9E8.4040503@blackmesacapital.com>
	<4064CA7A.1030100@blackmesacapital.com>
Message-ID: <4065D4F1.5050806@pburns.seanet.com>

I think this is an interesting discussion -- I've learned from both
Steve's and Brian's comments, and I'm broadening it to R-help
since I think others will be interested as well.

The problem up for comment is:

result <- apply(array.3D, 1:2, sum)

Where array.3D is 3000 by 300 by 3.

The original poster already had a perfectly good replacement for
this problem that was virtually instantaneous.  A solution for this
particular problem is not the issue, it is merely the starting point
for cases where there wouldn't be a trivial workaround.

Steve Karmesin wrote:
 
SK> As others have said, what apply has to do in this case is loop over 
the 900,000
SK> cases and do a  'sum' over three elements each time.  In this case 
the overhead
SK> of calling an S+ function totally swamps the numeric operations.
SK>
SK> Doing this on smaller datasets (300x30x3) on my machine (2CPU, 3GHz 
Xeon
SK> running Windows 2000 and S-Plus 6.1) shows an overhead of about 140
SK> microseconds per call to sum, so I would expect it to take 
100*1e-6*9e5=90 seconds.
SK>
SK> The thing is, it is worse than this.  If I do a case with 900x90x3 
it takes 300 usec per 'sum'.
SK>
SK> R is fairly stable at just under 15usec per 'sum' on my machine.
SK>
SK> A little more investigation (together with office mate Tony Plate) 
provides some insight.
SK>
SK> Using mem.tally.reset() and mem.tally.report() shows that for this 
case it is allocating a
SK> whopping 1280 bytes for each call to 'sum'.
SK>
SK> Just touching that much memory is going to be slow.  So why would it 
do that?  Looking
SK> at the definition of the apply function shows that it is allocating 
a general list for the result,
SK> not a vector-based array or matrix.
SK>
SK> Why?  It has a shortcut that lets it use efficient matrices if the 
input is a 2D matrix, but this
SK> one is 3D, so it uses the general code, which is much, much slower 
and uses a lot more memory.
SK>
SK> If you collapse the first two dimensions of the array the times are 
stable at <80usec per
SK> call to sum and it allocates 8 bytes per call, which is just the 
amount of space needed.
SK>
SK> Still, the R code seems to always build a list, and it is about 
15usec per call. Somehow
SK> the underlying function call and perhaps list storage mechanisms are 
more efficient there.

Prof Brian Ripley wrote:

BR> There are almost always pros and cons with these issues.  S's sum() is an 
BR> S4 generic whereas R's is internal *unless* you define an S4 method for 
BR> it (which S-PLUS has already done).  S needs to create several frames for 
BR> what is a nested set of function calls -- 1280b looks modest for that.
BR> 
BR> Also, S has an ability to back out calculations that R does not, and that 
BR> costs memory (and can have benefits).
BR> 
BR> We know there are overheads in making functions generic, especially 
BR> S4-generic, but then there are benefits too.  I am not sure designers who 
BR> add features take enough account of the costs.

Using R 1.8.1 (precompiled) on SuSe Linux with a Xeon 2.4GHz and 1G of 
memory:

set.seed(2)
jja <- array(rnorm(3000*300*3), c(3000, 300, 3))
gc()
system.time(jjsa <- apply(jja, 1:2, sum)) # takes 30 seconds

sumS3 <- function(x, ...) UseMethod("sumS3")
sumS3.default <- function(x, ...) sum(x, ...)
gc()
system.time(jjsa3 <- apply(jja, 1:2, sumS3)) # takes 65 seconds

sumS4 <- function(x, ...) standardGeneric("sumS4")
setMethod("sumS4", signature(x="numeric"), function(x, ...) sum(x, ...))
gc()
system.time(jjsa4 <- apply(jja, 1:2, sumS4)) # takes 58 seconds

Questions:

It looks to me like the penalty for making the functions generic is
similar to one extra function call.  Does the penalty grow as there
are more methods?  Are there other types of penalties for making
a function generic?

Is the test with sumS4 still an unfair comparison with S-PLUS?

Are things better with S-PLUS 6.2?

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From Hillary_Han at brown.edu  Sat Mar 27 21:16:44 2004
From: Hillary_Han at brown.edu (Han, Hillary)
Date: Sat, 27 Mar 2004 15:16:44 -0500
Subject: [R] cbind question
Message-ID: <6F811B8A186F2E46843C900244AD5C008BA5AC@MAIL2.AD.Brown.Edu>

hi, all:

Just wonder if there is any suggestions in how to get around this cbind error. I created two character lists with identical length. First tried combine the lists together with cbind, then convert the lists to matrix, and tried again. Both faied. Any fix to merge the two lists/matrices?


ll<- multiget(ftID, hgu95av2LOCUSID)
> class(ll)
[1] "list"
> sym <- multiget(ftID, hgu95av2SYMBOL)
> class(sym)
[1] "list"
> cbind(sym, ll)
Error in cbind(...) : cannot create a matrix from these types
> sym <- as.matrix(multiget(ftID, hgu95av2SYMBOL))
> ll<- as.matrix(multiget(ftID, hgu95av2LOCUSID))
> dim(sym)
[1] 508   1
> dim(ll)
[1] 508   1
> class(sym)
[1] "matrix"
> class(ll)
[1] "matrix"
> cbind(sym,ll)
Error in cbind(...) : cannot create a matrix from these types

thanks, 
Hillary



From ripley at stats.ox.ac.uk  Sat Mar 27 21:30:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 20:30:32 +0000 (GMT)
Subject: [R] cbind question
In-Reply-To: <6F811B8A186F2E46843C900244AD5C008BA5AC@MAIL2.AD.Brown.Edu>
Message-ID: <Pine.LNX.4.44.0403272023240.17357-100000@gannet.stats>

cbind on vectors/matrices which are not atomic is unsupported: we had a
bug report on that within the last 24 hours (but it seems to be
intentional).

You can just concatenate the lists and add a suitable dimension:

res <- c(ll, sym)
dim(res) <- c(508,2)

if I understand your intentions.

I think.  However, *why* do you want to do this?  A list of lists would 
seem to make more sense, and can be indexed in a similar way.


On Sat, 27 Mar 2004, Han, Hillary wrote:

> Just wonder if there is any suggestions in how to get around this cbind
> error. I created two character lists with identical length. First tried

What is a `character list'?

> combine the lists together with cbind, then convert the lists to matrix,
> and tried again. Both faied. Any fix to merge the two lists/matrices?

Merge?  That's what c() does.

> ll<- multiget(ftID, hgu95av2LOCUSID)
> > class(ll)
> [1] "list"
> > sym <- multiget(ftID, hgu95av2SYMBOL)
> > class(sym)
> [1] "list"
> > cbind(sym, ll)
> Error in cbind(...) : cannot create a matrix from these types
> > sym <- as.matrix(multiget(ftID, hgu95av2SYMBOL))
> > ll<- as.matrix(multiget(ftID, hgu95av2LOCUSID))
> > dim(sym)
> [1] 508   1
> > dim(ll)
> [1] 508   1
> > class(sym)
> [1] "matrix"
> > class(ll)
> [1] "matrix"
> > cbind(sym,ll)
> Error in cbind(...) : cannot create a matrix from these types

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sat Mar 27 22:03:30 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 27 Mar 2004 13:03:30 -0800
Subject: [R] multicolumn sort on dataframe?
In-Reply-To: <Pine.LNX.4.58.0403270931510.10859@localhost.localdomain>
References: <Pine.LNX.4.58.0403270931510.10859@localhost.localdomain>
Message-ID: <4065EC22.8030804@pdf.com>

      "?order" includes the following: 

    order(..., na.last = TRUE, decreasing = FALSE)

    Arguments:     ...: a sequence of vectors, all of the same length.

    Examples:

     (ii <- order(x <- c(1,1,3:1,1:4,3), y <- c(9,9:1), z <-c(2,1:9)))
     ## 6  5  2  1  7  4 10  8  3  9
     rbind(x,y,z)[,ii] # shows the reordering (ties via 2nd & 3rd arg)

  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
x    1    1    1    1    2    2    3    3    3     4
y    5    6    9    9    4    7    1    3    8     2
z    5    4    1    2    6    3    9    7    2     8

      hope this helps.  spencer graves

Anne York wrote:

>On Fri 26 Mar 2004, Jeff D. Hamann wrote:
>                                                                                
>  
>
>>I couldn't find any reference to this in the FAQ, but is it 
>>possible to sort a  dataframe by multiple columns?
>>    
>>
>                                                                                
>  
>
>>I've created some code, similar to the following:
>>    
>>
>                                                                                
>  
>
>>nspr.code <- sp.results$sp.code[order( sp.results$sp.code )]
>>nspr.tpa <- sp.results$tpa[order( sp.results$sp.code )]
>>    
>>
>                                                                                
>  
>
>>nspr.code <- as.character( levels( nspr.code ) )[nspr.code]
>>nspr.tpa <- as.numeric( levels( nspr.tpa ) )[nspr.tpa]
>>    
>>
>                                                                                
>  
>
>>hope <- as.data.frame( cbind( nspr.code, as.numeric(nspr.tpa) ) )
>>    
>>
>
>
>A simple way to sort multiple columns is to paste them 
>together and sort the resulting character vector. THat way 
>you only have to do one sort. This is a very old method 
>taught to me in the first computer course I ever took (date 
>censored); the instructor attributed the method to Von 
>Neumann but I have no reference. 
>
>You have to be careful choosing the sep character in paste. 
>
>Here is an example
>
>
>  
>
>>set.seed(78) 
>>foo = data.frame(x= sample(letters[1:3],5,replace=TRUE),
>>    
>>
>                   y= sample(1:5,5,replace=TRUE))
>  
>
>>foo
>>    
>>
>  x y
>1 c 3
>2 c 2
>3 b 2
>4 c 1
>5 c 3
>
>Sorting on y and then by x: 
>
>  
>
>>my.order=order(foo.paste=paste(foo[,2],foo[,1],sep="/"))
>>my.order
>>    
>>
>[1] 4 3 2 1 5
>
>
>  
>
>>my.order=order(paste(foo[,1],foo[,2],sep="/"))
>>foo[my.order,]
>>    
>>
>  x y
>3 b 2
>4 c 1
>2 c 2
>1 c 3
>5 c 3
>  
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Hillary_Han at brown.edu  Sat Mar 27 22:25:45 2004
From: Hillary_Han at brown.edu (Han, Hillary)
Date: Sat, 27 Mar 2004 16:25:45 -0500
Subject: [R] cbind question
Message-ID: <6F811B8A186F2E46843C900244AD5C008BA5AE@MAIL2.AD.Brown.Edu>

Thanks very much. I used c, and got the two lists merged fine. I would like to write the results into a file. So used

> write.table(try, file = "try.txt")
Error in cbind(...) : cannot create a matrix from these types

Got into the same cbind error... Any suggestions? Thanks in advance.

code:
> > class(ll)
> [1] "list"
> > class(sym)
> [1] "list"
> > dim(sym)
> [1] 508   1
> > dim(ll)
> [1] 508   1
> try <- c(sym[1:5], ll[1:5])
> dim(try)
NULL
> dim(try)<- c(5, 2)
> try
     [,1]      [,2] 
[1,] "Slc40a1" 53945
[2,] "Rassf5"  54354
[3,] "Igfbp4"  16010
[4,] "Hmox1"   15368
[5,] "Cxcr4"   12767
> write.table(try, file = "try.txt")
Error in cbind(...) : cannot create a matrix from these types



-----Original Message-----
From:	Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent:	Sat 3/27/2004 3:30 PM
To:	Han, Hillary
Cc:	r-help at stat.math.ethz.ch
Subject:	Re: [R] cbind question
cbind on vectors/matrices which are not atomic is unsupported: we had a
bug report on that within the last 24 hours (but it seems to be
intentional).

You can just concatenate the lists and add a suitable dimension:

res <- c(ll, sym)
dim(res) <- c(508,2)

if I understand your intentions.

I think.  However, *why* do you want to do this?  A list of lists would 
seem to make more sense, and can be indexed in a similar way.


On Sat, 27 Mar 2004, Han, Hillary wrote:

> Just wonder if there is any suggestions in how to get around this cbind
> error. I created two character lists with identical length. First tried

What is a `character list'?

> combine the lists together with cbind, then convert the lists to matrix,
> and tried again. Both faied. Any fix to merge the two lists/matrices?

Merge?  That's what c() does.

> ll<- multiget(ftID, hgu95av2LOCUSID)
> > class(ll)
> [1] "list"
> > sym <- multiget(ftID, hgu95av2SYMBOL)
> > class(sym)
> [1] "list"
> > cbind(sym, ll)
> Error in cbind(...) : cannot create a matrix from these types
> > sym <- as.matrix(multiget(ftID, hgu95av2SYMBOL))
> > ll<- as.matrix(multiget(ftID, hgu95av2LOCUSID))
> > dim(sym)
> [1] 508   1
> > dim(ll)
> [1] 508   1
> > class(sym)
> [1] "matrix"
> > class(ll)
> [1] "matrix"
> > cbind(sym,ll)
> Error in cbind(...) : cannot create a matrix from these types

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Mar 27 22:34:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 21:34:52 +0000 (GMT)
Subject: [R] cbind question
In-Reply-To: <6F811B8A186F2E46843C900244AD5C008BA5AE@MAIL2.AD.Brown.Edu>
Message-ID: <Pine.LNX.4.44.0403272132470.17548-100000@gannet.stats>

On Sat, 27 Mar 2004, Han, Hillary wrote:

> Thanks very much. I used c, and got the two lists merged fine. I would like to write the results into a file. So used
> 
> > write.table(try, file = "try.txt")
> Error in cbind(...) : cannot create a matrix from these types
> 
> Got into the same cbind error... Any suggestions? Thanks in advance.

You don't have a table!  Use write?  (I don't know what is in your lists, 
but they look like single-element vectors.)

> 
> code:
> > > class(ll)
> > [1] "list"
> > > class(sym)
> > [1] "list"
> > > dim(sym)
> > [1] 508   1
> > > dim(ll)
> > [1] 508   1
> > try <- c(sym[1:5], ll[1:5])
> > dim(try)
> NULL
> > dim(try)<- c(5, 2)
> > try
>      [,1]      [,2] 
> [1,] "Slc40a1" 53945
> [2,] "Rassf5"  54354
> [3,] "Igfbp4"  16010
> [4,] "Hmox1"   15368
> [5,] "Cxcr4"   12767
> > write.table(try, file = "try.txt")
> Error in cbind(...) : cannot create a matrix from these types
> 
> 
> 
> -----Original Message-----
> From:	Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent:	Sat 3/27/2004 3:30 PM
> To:	Han, Hillary
> Cc:	r-help at stat.math.ethz.ch
> Subject:	Re: [R] cbind question
> cbind on vectors/matrices which are not atomic is unsupported: we had a
> bug report on that within the last 24 hours (but it seems to be
> intentional).
> 
> You can just concatenate the lists and add a suitable dimension:
> 
> res <- c(ll, sym)
> dim(res) <- c(508,2)
> 
> if I understand your intentions.
> 
> I think.  However, *why* do you want to do this?  A list of lists would 
> seem to make more sense, and can be indexed in a similar way.
> 
> 
> On Sat, 27 Mar 2004, Han, Hillary wrote:
> 
> > Just wonder if there is any suggestions in how to get around this cbind
> > error. I created two character lists with identical length. First tried
> 
> What is a `character list'?
> 
> > combine the lists together with cbind, then convert the lists to matrix,
> > and tried again. Both faied. Any fix to merge the two lists/matrices?
> 
> Merge?  That's what c() does.
> 
> > ll<- multiget(ftID, hgu95av2LOCUSID)
> > > class(ll)
> > [1] "list"
> > > sym <- multiget(ftID, hgu95av2SYMBOL)
> > > class(sym)
> > [1] "list"
> > > cbind(sym, ll)
> > Error in cbind(...) : cannot create a matrix from these types
> > > sym <- as.matrix(multiget(ftID, hgu95av2SYMBOL))
> > > ll<- as.matrix(multiget(ftID, hgu95av2LOCUSID))
> > > dim(sym)
> > [1] 508   1
> > > dim(ll)
> > [1] 508   1
> > > class(sym)
> > [1] "matrix"
> > > class(ll)
> > [1] "matrix"
> > > cbind(sym,ll)
> > Error in cbind(...) : cannot create a matrix from these types
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Mar 27 22:32:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 27 Mar 2004 21:32:35 +0000 (GMT)
Subject: [R] Re: [S] scalability
In-Reply-To: <4065D4F1.5050806@pburns.seanet.com>
Message-ID: <Pine.LNX.4.44.0403272034410.17357-100000@gannet.stats>

On Sat, 27 Mar 2004, Patrick Burns wrote:

> I think this is an interesting discussion -- I've learned from both
> Steve's and Brian's comments, and I'm broadening it to R-help
> since I think others will be interested as well.
> 
> The problem up for comment is:
> 
> result <- apply(array.3D, 1:2, sum)
> 
> Where array.3D is 3000 by 300 by 3.

...

> Prof Brian Ripley wrote:
> 
> BR> There are almost always pros and cons with these issues.  S's sum() is an 
> BR> S4 generic whereas R's is internal *unless* you define an S4 method for 
> BR> it (which S-PLUS has already done).  S needs to create several frames for 
> BR> what is a nested set of function calls -- 1280b looks modest for that.
> BR> 
> BR> Also, S has an ability to back out calculations that R does not, and that 
> BR> costs memory (and can have benefits).
> BR> 
> BR> We know there are overheads in making functions generic, especially 
> BR> S4-generic, but then there are benefits too.  I am not sure designers who 
> BR> add features take enough account of the costs.
> 
> Using R 1.8.1 (precompiled) on SuSe Linux with a Xeon 2.4GHz and 1G of 
> memory:
> 
> set.seed(2)
> jja <- array(rnorm(3000*300*3), c(3000, 300, 3))
> gc()
> system.time(jjsa <- apply(jja, 1:2, sum)) # takes 30 seconds
> 
> sumS3 <- function(x, ...) UseMethod("sumS3")
> sumS3.default <- function(x, ...) sum(x, ...)
> gc()
> system.time(jjsa3 <- apply(jja, 1:2, sumS3)) # takes 65 seconds

sum is already S3-generic in R, at C level.  So a simple wrapper would be
a better test.  BTW, repeating this speeds things up quite a bit as the gc
limits get tuned.  I get (Athlon 2600)  23-23 secs basic, 23-25 secs for a
simple wrapper and 49 secs for sumS4.

> sumS4 <- function(x, ...) standardGeneric("sumS4")
> setMethod("sumS4", signature(x="numeric"), function(x, ...) sum(x, ...))
> gc()
> system.time(jjsa4 <- apply(jja, 1:2, sumS4)) # takes 58 seconds
> 
> Questions:
> 
> It looks to me like the penalty for making the functions generic is
> similar to one extra function call.  Does the penalty grow as there
> are more methods?  

Yes, probably quite a lot.  AFAIK there is no caching of selected methods 
going on, although it is hard to be sure.

> Are there other types of penalties for making
> a function generic?

Memory usage.  If you put gcinfo(T) you will see the cons cell usage 
growing during the run.

> Is the test with sumS4 still an unfair comparison with S-PLUS?

Yes, somewhat.  You only have one method.

> Are things better with S-PLUS 6.2?

Apparently not.  Even calling the default method directly seems very slow.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From york at zipcon.net  Sat Mar 27 23:41:35 2004
From: york at zipcon.net (Anne York)
Date: Sat, 27 Mar 2004 14:41:35 -0800 (PST)
Subject: [R] multicolumn sort on dataframe?
In-Reply-To: <Pine.LNX.4.58.0403270931510.10859@localhost.localdomain>
References: <Pine.LNX.4.58.0403270931510.10859@localhost.localdomain>
Message-ID: <Pine.LNX.4.58.0403271440270.13331@localhost.localdomain>

Forget about my previous post. That method only works in 
very special cases. Use order with mutiple arguments.

On Sat, 27 Mar 2004, Anne York wrote:

AY > On Fri 26 Mar 2004, Jeff D. Hamann wrote:
AY >                                                                                 
AY > >I couldn't find any reference to this in the FAQ, but is it 
AY > >possible to sort a  dataframe by multiple columns?
AY >                                                                                 
AY > >I've created some code, similar to the following:
AY >                                                                                 
AY > >nspr.code <- sp.results$sp.code[order( sp.results$sp.code )]
AY > >nspr.tpa <- sp.results$tpa[order( sp.results$sp.code )]
AY >                                                                                 
AY > >nspr.code <- as.character( levels( nspr.code ) )[nspr.code]
AY > >nspr.tpa <- as.numeric( levels( nspr.tpa ) )[nspr.tpa]
AY >                                                                                 
AY > >hope <- as.data.frame( cbind( nspr.code, as.numeric(nspr.tpa) ) )
AY > 
AY > 
AY > A simple way to sort multiple columns is to paste them 
AY > together and sort the resulting character vector. THat way 
AY > you only have to do one sort. This is a very old method 
AY > taught to me in the first computer course I ever took (date 
AY > censored); the instructor attributed the method to Von 
AY > Neumann but I have no reference. 
AY > 
AY > You have to be careful choosing the sep character in paste. 
AY > 
AY > Here is an example
AY > 
AY > 
AY > > set.seed(78) 
AY > > foo = data.frame(x= sample(letters[1:3],5,replace=TRUE),
AY >                    y= sample(1:5,5,replace=TRUE))
AY > > foo
AY >   x y
AY > 1 c 3
AY > 2 c 2
AY > 3 b 2
AY > 4 c 1
AY > 5 c 3
AY > 
AY > Sorting on y and then by x: 
AY > 
AY > > my.order=order(foo.paste=paste(foo[,2],foo[,1],sep="/"))
AY > > my.order
AY > [1] 4 3 2 1 5
AY > 
AY > 
AY > > my.order=order(paste(foo[,1],foo[,2],sep="/"))
AY > > foo[my.order,]
AY >   x y
AY > 3 b 2
AY > 4 c 1
AY > 2 c 2
AY > 1 c 3
AY > 5 c 3
AY > >
AY > 
AY >



From xianggui01 at yahoo.com  Sun Mar 28 04:13:40 2004
From: xianggui01 at yahoo.com (Xianggui QU)
Date: Sat, 27 Mar 2004 18:13:40 -0800 (PST)
Subject: [R] Dopt program!
Message-ID: <20040328021340.62695.qmail@web60708.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040327/4d05e650/attachment.pl

From ggrothendieck at myway.com  Sun Mar 28 04:34:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 28 Mar 2004 02:34:57 +0000 (UTC)
Subject: [R] cbind question
References: <6F811B8A186F2E46843C900244AD5C008BA5AE@MAIL2.AD.Brown.Edu>
Message-ID: <loom.20040328T043302-851@post.gmane.org>


Suppose you have these two variables:

   x1 <- matrix(list("a","b","c"),3,1)
   x2 <- matrix(list(1,2,3),3,1)

You can unlist them and then create a data frame out of them:

   data.frame(x1=unlist(x1),x2=unlist(x2))

Aside from the above solution, you might want to question why your 
variables are lists in the first place.


Han, Hillary <Hillary_Han <at> brown.edu> writes:

: 
: Thanks very much. I used c, and got the two lists merged fine. I would like 
to write the results into a file. So used
: 
: > write.table(try, file = "try.txt")
: Error in cbind(...) : cannot create a matrix from these types
: 
: Got into the same cbind error... Any suggestions? Thanks in advance.
: 
: code:
: > > class(ll)
: > [1] "list"
: > > class(sym)
: > [1] "list"
: > > dim(sym)
: > [1] 508   1
: > > dim(ll)
: > [1] 508   1
: > try <- c(sym[1:5], ll[1:5])
: > dim(try)
: NULL
: > dim(try)<- c(5, 2)
: > try
:      [,1]      [,2] 
: [1,] "Slc40a1" 53945
: [2,] "Rassf5"  54354
: [3,] "Igfbp4"  16010
: [4,] "Hmox1"   15368
: [5,] "Cxcr4"   12767
: > write.table(try, file = "try.txt")
: Error in cbind(...) : cannot create a matrix from these types
: 
: 
: -----Original Message-----
: From:	Prof Brian Ripley [mailto:ripley <at> stats.ox.ac.uk]
: Sent:	Sat 3/27/2004 3:30 PM
: To:	Han, Hillary
: Cc:	r-help <at> stat.math.ethz.ch
: Subject:	Re: [R] cbind question
: cbind on vectors/matrices which are not atomic is unsupported: we had a
: bug report on that within the last 24 hours (but it seems to be
: intentional).
: 
: You can just concatenate the lists and add a suitable dimension:
: 
: res <- c(ll, sym)
: dim(res) <- c(508,2)
: 
: if I understand your intentions.
: 
: I think.  However, *why* do you want to do this?  A list of lists would 
: seem to make more sense, and can be indexed in a similar way.
: 
: On Sat, 27 Mar 2004, Han, Hillary wrote:
: 
: > Just wonder if there is any suggestions in how to get around this cbind
: > error. I created two character lists with identical length. First tried
: 
: What is a `character list'?
: 
: > combine the lists together with cbind, then convert the lists to matrix,
: > and tried again. Both faied. Any fix to merge the two lists/matrices?
: 
: Merge?  That's what c() does.
: 
: > ll<- multiget(ftID, hgu95av2LOCUSID)
: > > class(ll)
: > [1] "list"
: > > sym <- multiget(ftID, hgu95av2SYMBOL)
: > > class(sym)
: > [1] "list"
: > > cbind(sym, ll)
: > Error in cbind(...) : cannot create a matrix from these types
: > > sym <- as.matrix(multiget(ftID, hgu95av2SYMBOL))
: > > ll<- as.matrix(multiget(ftID, hgu95av2LOCUSID))
: > > dim(sym)
: > [1] 508   1
: > > dim(ll)
: > [1] 508   1
: > > class(sym)
: > [1] "matrix"
: > > class(ll)
: > [1] "matrix"
: > > cbind(sym,ll)
: > Error in cbind(...) : cannot create a matrix from these types
:



From prad at mail.ahc.umn.edu  Sun Mar 28 09:39:33 2004
From: prad at mail.ahc.umn.edu (prad s u)
Date: Sun, 28 Mar 2004 01:39:33 -0600
Subject: [R] "R" and "S-plus"
Message-ID: <000801c41497$d088f3f0$c32c5ea0@prady>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040328/cf7de531/attachment.pl

From ripley at stats.ox.ac.uk  Sun Mar 28 09:41:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 08:41:02 +0100 (BST)
Subject: [R] Dopt program!
In-Reply-To: <20040328021340.62695.qmail@web60708.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0403280827001.25956-100000@gannet.stats>

On Sat, 27 Mar 2004, Xianggui QU wrote:

> Does anybody have a workable dopt program in R to generate D-optimal
> designs. I downloaded dopt.zip from stat homepage of the university of
> oxford, whenever I load it into R and try to run, the following error
> comes: Error: couldn't find function "Dopt".

Eh?  There is no such file on our homepage!  There is a dopt.zip for 
S-PLUS <= 2000 at the URL www.stats.ox.ac.uk/pub/SWin, but none for R on 
our server.

I don't know how to `load dopt.zip into R': you can only load() R save 
files.

> Any information will be highly appreciated!

Try reading ReadMe files in the directories you chance upon.  You must be
careful not to blame other people for your carelessness.

I guess you are using Windows (since you don't think it necessary to tell 
us).  There is an R port of that S code on CRAN at

http://cran.r-project.org/src/contrib/Devel/

but note it is source code, and you will need to install it: see the 
rw-FAQ if you are indeed using Windows.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Mar 28 11:11:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 10:11:36 +0100 (BST)
Subject: [R] "R" and "S-plus"
In-Reply-To: <000801c41497$d088f3f0$c32c5ea0@prady>
Message-ID: <Pine.LNX.4.44.0403281006330.26186-100000@gannet.stats>

On Sun, 28 Mar 2004, prad s u wrote:

> I apologize in advance if this is the wrong area to post this message. I
> would like to know if there is an "R" equivalent for the "S+finMetrics"
> package? I'd like to be able to use "R" to go through the examples

No.  S+FinMetrics is a *module*, Insightful proprietary code which is an
additional-cost addon for S-PLUS.  It is not part of S-PLUS per se.

> provided in the book "Modeling Financial Time-Series with S-Plus" (E.
> Zivot and J. Wang). I was told that "R" and "S-Plus" were very similar.

Please ask the person who told you that what (s)he meant by it.  Then look
together at the R FAQ for an informed answer.

> and I am relatively new to both languages. I would appreciate any help
> in locating such packages, or alternate suggestions.

There are various facilities for financial time series available for R 
(see the FAQ, again) but you won't be able to work through that book with 
them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From J.Brainard at uea.ac.uk  Sun Mar 28 13:00:31 2004
From: J.Brainard at uea.ac.uk (J.Brainard@uea.ac.uk)
Date: Sun, 28 Mar 2004 12:00:31 +0100
Subject: [R] Could someone email me with the code for glm.nb ?
Message-ID: <E1B7Y1n-000TCl-00@login1.uea.ac.uk>


Hi -- subject says all.  I just want the code for that function,
which I guess was in Venables and Ripley as early as 1994.
Well, and for any of the sub-functions that glm.nb calls.  I
can't install the entire MASS library.

If the code for just glm.nb (again, don't want to touch the MASS 
library, last time I tried to install it was a complete nightmare and
fiasco) is somewhere on a webpage or ftp site where I could copy it, 
pls.  just let me know where?

Many thanks -julii



From erich.neuwirth at univie.ac.at  Sun Mar 28 13:03:06 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sun, 28 Mar 2004 13:03:06 +0200
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
Message-ID: <4066B0EA.9030906@univie.ac.at>

Recently,
I upgraded MiKTeX to the latest versions, and since then there are 
problems when I try to build R and/or R packages.
I tried to build R 1.9.0 beta and R 1.8.1 patched

I am running Windows XP Pro, SP 1
MikTeX version is
This is e-TeX, Version 3.141592-2.1 (MiKTeX 2.4)
(preloaded format=latex 2004.3.27)

fpTeX version is
This is e-TeXk, Version 3.141592-2.1 (Web2c 7.5.2)
(format=latex 2004.3.23)  27 MAR 2004 21:58

With MiKTeX, it is not possible any more
do build documentation  either for R itself or for packages,
I get the following error

! LaTeX Error: Command \middle already defined.
                Or name \end... illegal, see p.192 of the manual.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
  ...

l.45 \newlength{\middle}


With fpTeX, I can build R itself, but when I try to build packages,
I get


(c:/Program Files/TeXLive/texmf/tex/latex/graphics/color.sty
Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)

(c:/Program Files/TeXLive/texmf/tex/latex/texlive/color.cfg
File: color.cfg 2001/08/31 v1.1 color configuration of teTeX/TeXLive
)

! Package color Error: No driver specified.

See the color package documentation for explanation.
Type  H <return>  for immediate help.
  ...

l.123      }

You should make a default driver option in a file
color.cfg
eg: \ExecuteOptions{dvips}

)

Both versions of TeX are installed the standard way,
without changes (except adding a few local LaTeX style files).


Is this happening to anybody else?



-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From ripley at stats.ox.ac.uk  Sun Mar 28 14:10:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 13:10:56 +0100 (BST)
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <4066B0EA.9030906@univie.ac.at>
Message-ID: <Pine.LNX.4.44.0403281301040.26340-100000@gannet.stats>

The fptex problem has been reported as happening on Unix too.  Rd.sty was
changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1 patched
not work?  I have no problem with fptex, and presume Duncan M does not 
either, as he is distributing binaries of 1.9.0 beta.

My color.cfg is 1.0 from TeXLive rather than 1.1 and contains

\ifcase\x
  % default case
  \ExecuteOptions{dvips}%
\or
  % pdfTeX is running in pdf mode
  \ExecuteOptions{pdftex}%
\else
  % VTeX is running
  \ExecuteOptions{vtex}%
\fi

so I guess yours has been modified incorrectly (e.g. for use with MikTeX?)

It looks like your MikTeX is broken.

In any case, you don't need LaTeX to build packages, so what exactly were 
you trying to do?  Check them?


On Sun, 28 Mar 2004, Erich Neuwirth wrote:

> Recently,
> I upgraded MiKTeX to the latest versions, and since then there are 
> problems when I try to build R and/or R packages.
> I tried to build R 1.9.0 beta and R 1.8.1 patched

The date of the beta is crucial.

> I am running Windows XP Pro, SP 1
> MikTeX version is
> This is e-TeX, Version 3.141592-2.1 (MiKTeX 2.4)
> (preloaded format=latex 2004.3.27)
> 
> fpTeX version is
> This is e-TeXk, Version 3.141592-2.1 (Web2c 7.5.2)
> (format=latex 2004.3.23)  27 MAR 2004 21:58
> 
> With MiKTeX, it is not possible any more
> do build documentation  either for R itself or for packages,
> I get the following error
> 
> ! LaTeX Error: Command \middle already defined.
>                 Or name \end... illegal, see p.192 of the manual.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>   ...
> 
> l.45 \newlength{\middle}
> 
> 
> With fpTeX, I can build R itself, but when I try to build packages,
> I get
> 
> 
> (c:/Program Files/TeXLive/texmf/tex/latex/graphics/color.sty
> Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)
> 
> (c:/Program Files/TeXLive/texmf/tex/latex/texlive/color.cfg
> File: color.cfg 2001/08/31 v1.1 color configuration of teTeX/TeXLive
> )
> 
> ! Package color Error: No driver specified.
> 
> See the color package documentation for explanation.
> Type  H <return>  for immediate help.
>   ...
> 
> l.123      }
> 
> You should make a default driver option in a file
> color.cfg
> eg: \ExecuteOptions{dvips}
> 
> )
> 
> Both versions of TeX are installed the standard way,
> without changes (except adding a few local LaTeX style files).
> 
> 
> Is this happening to anybody else?
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Mar 28 14:21:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 13:21:12 +0100 (BST)
Subject: [R] Could someone email me with the code for glm.nb ?
In-Reply-To: <E1B7Y1n-000TCl-00@login1.uea.ac.uk>
Message-ID: <Pine.LNX.4.44.0403281311050.26340-100000@gannet.stats>

On Sun, 28 Mar 2004 J.Brainard at uea.ac.uk wrote:

> Hi -- subject says all.  I just want the code for that function,
> which I guess was in Venables and Ripley as early as 1994.

Yes, for S: that version does not work in R.

> Well, and for any of the sub-functions that glm.nb calls.  I
> can't install the entire MASS library.

If you are using R, you do not need to.  It comes with MASS installed.
If you are using S-PLUS, you have asked in the wrong place for the wrong 
version ....

Actually you need a lot more than the code you asked for, for example the 
print and anova methods.

> If the code for just glm.nb (again, don't want to touch the MASS 
> library, last time I tried to install it was a complete nightmare and
> fiasco) 

Perhaps you need to invest in some basic computer help or training?  
Especially if you somehow managed to install R without it.

> is somewhere on a webpage or ftp site where I could copy it, 
> pls.  just let me know where?

The source code for the MASS *package* for R is on CRAN in the VR bundle, 
and is part of a standard R distribution.


Readers: please do not email him the code, anyone: the package does have a
licence and should not be split up.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yetise at wanadoo.fr  Sun Mar 28 15:11:28 2004
From: yetise at wanadoo.fr (yeti)
Date: Sun, 28 Mar 2004 15:11:28 +0200
Subject: [R] Re: 040328 - Here is it
In-Reply-To: <20040327212849.B49B72400124@mwinf0603.wanadoo.fr>
Message-ID: <BC8C6F20.52B1%yetise@wanadoo.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040328/39dbfea2/attachment.pl

From Roger.Bivand at nhh.no  Sun Mar 28 15:25:49 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 28 Mar 2004 15:25:49 +0200 (CEST)
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
Message-ID: <Pine.LNX.4.44.0403281434180.21313-100000@reclus.nhh.no>

On Sun 28 March 2004, Prof Brian Ripley wrote:

> The fptex problem has been reported as happening on Unix too.  Rd.sty
> was changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1
> patched not work?  I have no problem with fptex, and presume Duncan M
> does not either, as he is distributing binaries of 1.9.0 beta.

On Linux RH9, TeX, Version 3.14159 (Web2C 7.3.1) (format=latex
2003.12.27), using r-devel rsync'ed this morning (1.9.0 beta
(2004-03-28)), checking a package gives:
--------------------------
(/home/rsb/topics/Rtobe190/lib/R/share/texmf/hyperref.cfg
(/usr/share/texmf/tex/latex/graphics/color.sty
Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)

(/usr/share/texmf/tex/latex/config/color.cfg)

! Package color Error: No driver specified.

See the color package documentation for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.123      }
            
You should make a default driver option in a file 
color.cfg
------------------------
but on r-patched 1.8.1 Patched (2004-02-17) everything is as it was:
------------------------
(/home/rsb/topics/Rtobe181/lib/R/share/texmf/hyperref.cfg
Package color Info: Redefining color Blue on input line 2.
Package color Info: Redefining color Red on input line 3.
Package hyperref Info: Option `hyperindex' set `true' on input line 16.
Package hyperref Info: Option `colorlinks' set `true' on input line 16.
Package hyperref Info: Option `linktocpage' set `true' on input line 16.
Package hyperref Info: Option `plainpages' set `false' on input line 16.
)
-----------------------
The issues seem to be around lines 220-240 in share/texmf/Rd.sty, but
could be related to hyperref.cfg in the same directory - the new Rd.sty
includes the same code as is in hyperref.cfg - should this file still be
there? My TeX is too elementary to know, I'm afraid, but if hyperref.cfg
is renamed to something else, the *.dvi checks to completion using devel
without error, with log:
-----------------------
Package hyperref Info: Bookmarks ON on input line 1797.
Package hyperref Info: Hyper figures OFF on input line 1816.
Package hyperref Info: Link nesting OFF on input line 1821.
Package hyperref Info: Hyper index ON on input line 1824.
Package hyperref Info: Plain pages ON on input line 1829.
Package hyperref Info: Backreferencing OFF on input line 1836.
-----------------------
Renaming/removing hyperref.cfg is also suggested by its absence in the 
devel tarball, so this could be a result of rsync not deleting a stale and 
now unneeded file, which then gets read and used. 

My color.cfg says:
-------------------------
\@ifundefined{pdfoutput}%
  {\let\pdfoutput\@undefined
   \ExecuteOptions{dvips}}%
  {\ifcase\pdfoutput
      \ExecuteOptions{dvips}%
   \else
      \ExecuteOptions{pdftex}%
   \fi}%
--------------------------
I looked at this because I wanted to check packages I answer for on the 
beta version, and had deleted the original mails about this before meeting 
the same problem myself.

Roger

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Sun Mar 28 15:36:57 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 28 Mar 2004 15:36:57 +0200
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <Pine.LNX.4.44.0403281301040.26340-100000@gannet.stats>
References: <Pine.LNX.4.44.0403281301040.26340-100000@gannet.stats>
Message-ID: <4066D4F9.8090301@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:

> The fptex problem has been reported as happening on Unix too.  Rd.sty was
> changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1 patched
> not work?  I have no problem with fptex, and presume Duncan M does not 
> either, as he is distributing binaries of 1.9.0 beta.
> 
> My color.cfg is 1.0 from TeXLive rather than 1.1 and contains
> 
> \ifcase\x
>   % default case
>   \ExecuteOptions{dvips}%
> \or
>   % pdfTeX is running in pdf mode
>   \ExecuteOptions{pdftex}%
> \else
>   % VTeX is running
>   \ExecuteOptions{vtex}%
> \fi
> 
> so I guess yours has been modified incorrectly (e.g. for use with MikTeX?)
> 
> It looks like your MikTeX is broken.

The MiKTeX color.cfg is unchanged for "ages", it's still:
[2003/03/08 v1.0 MiKTeX 'color' configuration].

I'll look closer this evening.

Uwe



> In any case, you don't need LaTeX to build packages, so what exactly were 
> you trying to do?  Check them?
> 
> 
> On Sun, 28 Mar 2004, Erich Neuwirth wrote:
> 
> 
>>Recently,
>>I upgraded MiKTeX to the latest versions, and since then there are 
>>problems when I try to build R and/or R packages.
>>I tried to build R 1.9.0 beta and R 1.8.1 patched
> 
> 
> The date of the beta is crucial.
> 
> 
>>I am running Windows XP Pro, SP 1
>>MikTeX version is
>>This is e-TeX, Version 3.141592-2.1 (MiKTeX 2.4)
>>(preloaded format=latex 2004.3.27)
>>
>>fpTeX version is
>>This is e-TeXk, Version 3.141592-2.1 (Web2c 7.5.2)
>>(format=latex 2004.3.23)  27 MAR 2004 21:58
>>
>>With MiKTeX, it is not possible any more
>>do build documentation  either for R itself or for packages,
>>I get the following error
>>
>>! LaTeX Error: Command \middle already defined.
>>                Or name \end... illegal, see p.192 of the manual.
>>
>>See the LaTeX manual or LaTeX Companion for explanation.
>>Type  H <return>  for immediate help.
>>  ...
>>
>>l.45 \newlength{\middle}
>>
>>
>>With fpTeX, I can build R itself, but when I try to build packages,
>>I get
>>
>>
>>(c:/Program Files/TeXLive/texmf/tex/latex/graphics/color.sty
>>Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)
>>
>>(c:/Program Files/TeXLive/texmf/tex/latex/texlive/color.cfg
>>File: color.cfg 2001/08/31 v1.1 color configuration of teTeX/TeXLive
>>)
>>
>>! Package color Error: No driver specified.
>>
>>See the color package documentation for explanation.
>>Type  H <return>  for immediate help.
>>  ...
>>
>>l.123      }
>>
>>You should make a default driver option in a file
>>color.cfg
>>eg: \ExecuteOptions{dvips}
>>
>>)
>>
>>Both versions of TeX are installed the standard way,
>>without changes (except adding a few local LaTeX style files).
>>
>>
>>Is this happening to anybody else?



From ripley at stats.ox.ac.uk  Sun Mar 28 16:10:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 15:10:54 +0100 (BST)
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <Pine.LNX.4.44.0403281434180.21313-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0403281458470.26493-100000@gannet.stats>

hyperref.cfg is not there in the cvs sources ... which I think answers the 
point.  Did you use --delete on rsync?  The 1.9.0 beta FAQ has

     rsync -rC --delete rsync.R-project.org::MODULE R

At present rsync.R-project.org is far too slow for me to test out what it 
picks up (there seems to be a network problem with it).

On Sun, 28 Mar 2004, Roger Bivand wrote:

> On Sun 28 March 2004, Prof Brian Ripley wrote:
> 
> > The fptex problem has been reported as happening on Unix too.  Rd.sty
> > was changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1
> > patched not work?  I have no problem with fptex, and presume Duncan M
> > does not either, as he is distributing binaries of 1.9.0 beta.
> 
> On Linux RH9, TeX, Version 3.14159 (Web2C 7.3.1) (format=latex
> 2003.12.27), using r-devel rsync'ed this morning (1.9.0 beta
> (2004-03-28)), checking a package gives:
> --------------------------
> (/home/rsb/topics/Rtobe190/lib/R/share/texmf/hyperref.cfg
> (/usr/share/texmf/tex/latex/graphics/color.sty
> Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)
> 
> (/usr/share/texmf/tex/latex/config/color.cfg)
> 
> ! Package color Error: No driver specified.
> 
> See the color package documentation for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.123      }
>             
> You should make a default driver option in a file 
> color.cfg
> ------------------------
> but on r-patched 1.8.1 Patched (2004-02-17) everything is as it was:
> ------------------------
> (/home/rsb/topics/Rtobe181/lib/R/share/texmf/hyperref.cfg
> Package color Info: Redefining color Blue on input line 2.
> Package color Info: Redefining color Red on input line 3.
> Package hyperref Info: Option `hyperindex' set `true' on input line 16.
> Package hyperref Info: Option `colorlinks' set `true' on input line 16.
> Package hyperref Info: Option `linktocpage' set `true' on input line 16.
> Package hyperref Info: Option `plainpages' set `false' on input line 16.
> )
> -----------------------
> The issues seem to be around lines 220-240 in share/texmf/Rd.sty, but
> could be related to hyperref.cfg in the same directory - the new Rd.sty
> includes the same code as is in hyperref.cfg - should this file still be
> there? My TeX is too elementary to know, I'm afraid, but if hyperref.cfg
> is renamed to something else, the *.dvi checks to completion using devel
> without error, with log:
> -----------------------
> Package hyperref Info: Bookmarks ON on input line 1797.
> Package hyperref Info: Hyper figures OFF on input line 1816.
> Package hyperref Info: Link nesting OFF on input line 1821.
> Package hyperref Info: Hyper index ON on input line 1824.
> Package hyperref Info: Plain pages ON on input line 1829.
> Package hyperref Info: Backreferencing OFF on input line 1836.
> -----------------------
> Renaming/removing hyperref.cfg is also suggested by its absence in the 
> devel tarball, so this could be a result of rsync not deleting a stale and 
> now unneeded file, which then gets read and used. 
> 
> My color.cfg says:
> -------------------------
> \@ifundefined{pdfoutput}%
>   {\let\pdfoutput\@undefined
>    \ExecuteOptions{dvips}}%
>   {\ifcase\pdfoutput
>       \ExecuteOptions{dvips}%
>    \else
>       \ExecuteOptions{pdftex}%
>    \fi}%
> --------------------------
> I looked at this because I wanted to check packages I answer for on the 
> beta version, and had deleted the original mails about this before meeting 
> the same problem myself.
> 
> Roger
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Sun Mar 28 17:09:40 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 28 Mar 2004 17:09:40 +0200 (CEST)
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <Pine.LNX.4.44.0403281458470.26493-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0403281700570.21313-100000@reclus.nhh.no>

On Sun, 28 Mar 2004, Prof Brian Ripley wrote:

> hyperref.cfg is not there in the cvs sources ... which I think answers the 
> point.  Did you use --delete on rsync?  The 1.9.0 beta FAQ has
> 
>      rsync -rC --delete rsync.R-project.org::MODULE R

$ rsync -rC --delete --verbose rsync.R-project.org::r-devel R-devel

was the command used, and many files were deleted, but not that one. I 
saw something similar when the new structure was introduced, maybe my 
rsync is malfunctioning:

rsync  version 2.5.7  protocol version 26

I checked in the 1.9.0beta for Windows - there is no hyperref.cfg in 
share/texmf there.

> 
> At present rsync.R-project.org is far too slow for me to test out what it 
> picks up (there seems to be a network problem with it).
> 
> On Sun, 28 Mar 2004, Roger Bivand wrote:
> 
> > On Sun 28 March 2004, Prof Brian Ripley wrote:
> > 
> > > The fptex problem has been reported as happening on Unix too.  Rd.sty
> > > was changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1
> > > patched not work?  I have no problem with fptex, and presume Duncan M
> > > does not either, as he is distributing binaries of 1.9.0 beta.
> > 
> > On Linux RH9, TeX, Version 3.14159 (Web2C 7.3.1) (format=latex
> > 2003.12.27), using r-devel rsync'ed this morning (1.9.0 beta
> > (2004-03-28)), checking a package gives:
> > --------------------------
> > (/home/rsb/topics/Rtobe190/lib/R/share/texmf/hyperref.cfg
> > (/usr/share/texmf/tex/latex/graphics/color.sty
> > Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)
> > 
> > (/usr/share/texmf/tex/latex/config/color.cfg)
> > 
> > ! Package color Error: No driver specified.
> > 
> > See the color package documentation for explanation.
> > Type  H <return>  for immediate help.
> >  ...                                              
> >                                                   
> > l.123      }
> >             
> > You should make a default driver option in a file 
> > color.cfg
> > ------------------------
> > but on r-patched 1.8.1 Patched (2004-02-17) everything is as it was:
> > ------------------------
> > (/home/rsb/topics/Rtobe181/lib/R/share/texmf/hyperref.cfg
> > Package color Info: Redefining color Blue on input line 2.
> > Package color Info: Redefining color Red on input line 3.
> > Package hyperref Info: Option `hyperindex' set `true' on input line 16.
> > Package hyperref Info: Option `colorlinks' set `true' on input line 16.
> > Package hyperref Info: Option `linktocpage' set `true' on input line 16.
> > Package hyperref Info: Option `plainpages' set `false' on input line 16.
> > )
> > -----------------------
> > The issues seem to be around lines 220-240 in share/texmf/Rd.sty, but
> > could be related to hyperref.cfg in the same directory - the new Rd.sty
> > includes the same code as is in hyperref.cfg - should this file still be
> > there? My TeX is too elementary to know, I'm afraid, but if hyperref.cfg
> > is renamed to something else, the *.dvi checks to completion using devel
> > without error, with log:
> > -----------------------
> > Package hyperref Info: Bookmarks ON on input line 1797.
> > Package hyperref Info: Hyper figures OFF on input line 1816.
> > Package hyperref Info: Link nesting OFF on input line 1821.
> > Package hyperref Info: Hyper index ON on input line 1824.
> > Package hyperref Info: Plain pages ON on input line 1829.
> > Package hyperref Info: Backreferencing OFF on input line 1836.
> > -----------------------
> > Renaming/removing hyperref.cfg is also suggested by its absence in the 
> > devel tarball, so this could be a result of rsync not deleting a stale and 
> > now unneeded file, which then gets read and used. 
> > 
> > My color.cfg says:
> > -------------------------
> > \@ifundefined{pdfoutput}%
> >   {\let\pdfoutput\@undefined
> >    \ExecuteOptions{dvips}}%
> >   {\ifcase\pdfoutput
> >       \ExecuteOptions{dvips}%
> >    \else
> >       \ExecuteOptions{pdftex}%
> >    \fi}%
> > --------------------------
> > I looked at this because I wanted to check packages I answer for on the 
> > beta version, and had deleted the original mails about this before meeting 
> > the same problem myself.
> > 
> > Roger
> > 
> > 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From kjetil at entelnet.bo  Sun Mar 28 17:13:12 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 28 Mar 2004 11:13:12 -0400
Subject: [R] Dopt program!
In-Reply-To: <Pine.LNX.4.44.0403280827001.25956-100000@gannet.stats>
References: <20040328021340.62695.qmail@web60708.mail.yahoo.com>
Message-ID: <4066B348.21156.24F25F@localhost>

On 28 Mar 2004 at 8:41, Prof Brian Ripley wrote:

> On Sat, 27 Mar 2004, Xianggui QU wrote:
> 
> > Does anybody have a workable dopt program in R to generate D-optimal
> > designs. I downloaded dopt.zip from stat homepage of the university
> > of oxford, whenever I load it into R and try to run, the following
> > error comes: Error: couldn't find function "Dopt".
> 
> Eh?  There is no such file on our homepage!  There is a dopt.zip for
> S-PLUS <= 2000 at the URL www.stats.ox.ac.uk/pub/SWin, but none for R
> on our server.
> 
> I don't know how to `load dopt.zip into R': you can only load() R save
> files.
> 
> > Any information will be highly appreciated!
> 
> Try reading ReadMe files in the directories you chance upon.  You must
> be careful not to blame other people for your carelessness.
> 
> I guess you are using Windows (since you don't think it necessary to
> tell us).  There is an R port of that S code on CRAN at
> 
> http://cran.r-project.org/src/contrib/Devel/
> 
> but note it is source code, and you will need to install it: see the
> rw-FAQ if you are indeed using Windows.
> 

Adding to what Prof. Ripley said: There is a reaon Dopt is in the 
devel directory of CRAN. See the information included in that tar.gz:
The code "bombs" R in some few cases. Still it can be usefull, if 
used with care. The errors are NOT made with the port to R, I have 
tried the dopt.zip Brian Ripley mentions for S-Plus <= 2000 with S-
Plus 4.5, and the same cases which bombs R, bombs S-Plus. 

There is a new package AlgDesign, by Bob Wheeler, on CRAN, which is 
also precompiled for windows. I haven't tried it yet, but it can 
probably do everything Dopt can do, and more. When I get time to 
check it out, and verified this, I will ask for Dopt to be removed 
from the devel directory on CRAN.

Kjetil Halvorsen


> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ivo.welch at yale.edu  Sun Mar 28 18:25:26 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Sun, 28 Mar 2004 11:25:26 -0500
Subject: [R] residuals with missing values
Message-ID: <4066FC76.5020300@yale.edu>


hi:  sorry to bother you all again.  I am running a simple lm(y~x+z) 
regression, in which some of the observations are missing.  
Unfortunately, the residuals vector from the lm object omits all the 
missing values, which means that I cannot simply do residual diagnostics 
(e.g., plot(y,x)).  Would it not make more sense to have the residuals 
propagate the missing values, so that the residuals are guaranteed to 
have the same length as the variables?   Alternatively, maybe the 
residuals() function could do this instead.  But the documentation is 
not clear:

     Methods can make use of 'naresid' methods to compensate for the
     omission of missing values.  The default method does.

How?  I have figured out how to write my own function to do what I need 
(using the names of the residuals object), so this is more a "how to 
properly do this?" question, and/or "suggestion for improved 
documentation" than it is a desparate need of mine.

sincerely,  /iaw



From jfox at mcmaster.ca  Sun Mar 28 18:44:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 28 Mar 2004 11:44:21 -0500
Subject: [R] residuals with missing values
In-Reply-To: <4066FC76.5020300@yale.edu>
Message-ID: <20040328164421.YNOI15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Ivo,

The default na.action is na.omit, which behaves as you describe. Setting
options(na.action="na.exclude"), or specifying the argument
na.action="na.exclude" in the call to lm(), will produce residuals and other
case statistics that have NA for omitted observations. See ?na.exclude and
?lm for details. 

I hope that this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ivo welch
> Sent: Sunday, March 28, 2004 11:25 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] residuals with missing values
> 
> 
> hi:  sorry to bother you all again.  I am running a simple 
> lm(y~x+z) regression, in which some of the observations are missing.  
> Unfortunately, the residuals vector from the lm object omits 
> all the missing values, which means that I cannot simply do 
> residual diagnostics (e.g., plot(y,x)).  Would it not make 
> more sense to have the residuals propagate the missing 
> values, so that the residuals are guaranteed to 
> have the same length as the variables?   Alternatively, maybe the 
> residuals() function could do this instead.  But the 
> documentation is not clear:
> 
>      Methods can make use of 'naresid' methods to compensate for the
>      omission of missing values.  The default method does.
> 
> How?  I have figured out how to write my own function to do 
> what I need (using the names of the residuals object), so 
> this is more a "how to properly do this?" question, and/or 
> "suggestion for improved documentation" than it is a 
> desparate need of mine.
>



From hodgess at gator.uhd.edu  Sun Mar 28 19:02:53 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Sun, 28 Mar 2004 11:02:53 -0600
Subject: [R] splitting a character vector
Message-ID: <200403281702.i2SH2r203912@gator.dt.uh.edu>

Dear R People:

Suppose I have the following;

xa <- c("There are 5 dogs")

I would like to have a new character vector such that
xb[1] is There
xb[2] is are
xb[3] is 5
xb[4] is dogs

Since the original vector has length 1, substring will not work.

Any suggestions would be MOST welcome!

thanks
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu

R 1.8.1 for Windows



From jfox at mcmaster.ca  Sun Mar 28 19:06:53 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 28 Mar 2004 12:06:53 -0500
Subject: [R] splitting a character vector
In-Reply-To: <200403281702.i2SH2r203912@gator.dt.uh.edu>
Message-ID: <20040328170653.XHGO10288.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Erin,

unlist(strsplit(xa, " ")) should give you what you want; note that you don't
need c() to define xa in your example.

I hope this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
> Sent: Sunday, March 28, 2004 12:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] splitting a character vector
> 
> Dear R People:
> 
> Suppose I have the following;
> 
> xa <- c("There are 5 dogs")
> 
> I would like to have a new character vector such that xb[1] 
> is There xb[2] is are xb[3] is 5 xb[4] is dogs
> 
> Since the original vector has length 1, substring will not work.
> 
> Any suggestions would be MOST welcome!
> 
> thanks
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences University 
> of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> R 1.8.1 for Windows
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Mar 28 19:07:45 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 28 Mar 2004 19:07:45 +0200
Subject: [R] splitting a character vector
In-Reply-To: <200403281702.i2SH2r203912@gator.dt.uh.edu>
References: <200403281702.i2SH2r203912@gator.dt.uh.edu>
Message-ID: <40670661.2050908@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Dear R People:
> 
> Suppose I have the following;
> 
> xa <- c("There are 5 dogs")
> 
> I would like to have a new character vector such that
> xb[1] is There
> xb[2] is are
> xb[3] is 5
> xb[4] is dogs

xb <- unlist(strsplit(xa, " "))

Uwe Ligges



> Since the original vector has length 1, substring will not work.
> 
> Any suggestions would be MOST welcome!
> 
> thanks
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> R 1.8.1 for Windows
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Sun Mar 28 19:13:01 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sun, 28 Mar 2004 13:13:01 -0400 (AST)
Subject: [R] Could someone email me with the code for glm.nb ?
Message-ID: <200403281713.i2SHD14Q015171@erdos.math.unb.ca>


> Hi -- subject says all.  I just want the code for that function,
> which I guess was in Venables and Ripley as early as 1994.
> Well, and for any of the sub-functions that glm.nb calls.  I
> can't install the entire MASS library.

	Why on earth not?  What is the matter with your computer/system?

> If the code for just glm.nb (again, don't want to touch the MASS 
> library, last time I tried to install it was a complete nightmare and
> fiasco) is somewhere on a webpage or ftp site where I could copy it, 
> pls.  just let me know where?

	It's called CRAN!  If you really (and let me repeat, this is
	***weird***) don't want to install MASS, then just get the
	package source, untar it, go to the R directory, and copy
	whatever code you want to wherever you want it.

	Life would be much easier for you if you'd get over your
	hang-ups and just install MASS, but.

	If you are using Linux it's completely transparent and
	automatic.  Even if you are using Windoze (God help you!)
	it's ***still*** (mirabile dictu) completely transparent and
	automatic.

	You can do it from within R --- just execute

	> install.packages("MASS",lib=whatever)

	and R does all the rest.

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From ripley at stats.ox.ac.uk  Sun Mar 28 19:17:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 18:17:43 +0100 (BST)
Subject: [R] splitting a character vector
In-Reply-To: <20040328170653.XHGO10288.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.44.0403281816380.26916-100000@gannet.stats>

It would be clearer to use

strsplit(xa, " ")[[1]]

the point being that you do want the result for the first element of the 
first argument, picked out by [[1]].

On Sun, 28 Mar 2004, John Fox wrote:

> Dear Erin,
> 
> unlist(strsplit(xa, " ")) should give you what you want; note that you don't
> need c() to define xa in your example.
> 
> I hope this helps,
>  John 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
> > Sent: Sunday, March 28, 2004 12:03 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] splitting a character vector
> > 
> > Dear R People:
> > 
> > Suppose I have the following;
> > 
> > xa <- c("There are 5 dogs")
> > 
> > I would like to have a new character vector such that xb[1] 
> > is There xb[2] is are xb[3] is 5 xb[4] is dogs
> > 
> > Since the original vector has length 1, substring will not work.
> > 
> > Any suggestions would be MOST welcome!
> > 
> > thanks
> > Erin Hodgess
> > Associate Professor
> > Department of Computer and Mathematical Sciences University 
> > of Houston - Downtown
> > mailto: hodgess at gator.uhd.edu
> > 
> > R 1.8.1 for Windows
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Mar 28 20:39:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 19:39:32 +0100 (BST)
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <Pine.LNX.4.44.0403281700570.21313-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0403281938300.27127-100000@gannet.stats>

I am now able to do a rsync checkout, and share/texmf/hyperref.cfg is not 
there (which is correct).  I suggest you do a clean checkout.

On Sun, 28 Mar 2004, Roger Bivand wrote:

> On Sun, 28 Mar 2004, Prof Brian Ripley wrote:
> 
> > hyperref.cfg is not there in the cvs sources ... which I think answers the 
> > point.  Did you use --delete on rsync?  The 1.9.0 beta FAQ has
> > 
> >      rsync -rC --delete rsync.R-project.org::MODULE R
> 
> $ rsync -rC --delete --verbose rsync.R-project.org::r-devel R-devel
> 
> was the command used, and many files were deleted, but not that one. I 
> saw something similar when the new structure was introduced, maybe my 
> rsync is malfunctioning:
> 
> rsync  version 2.5.7  protocol version 26
> 
> I checked in the 1.9.0beta for Windows - there is no hyperref.cfg in 
> share/texmf there.
> 
> > 
> > At present rsync.R-project.org is far too slow for me to test out what it 
> > picks up (there seems to be a network problem with it).
> > 
> > On Sun, 28 Mar 2004, Roger Bivand wrote:
> > 
> > > On Sun 28 March 2004, Prof Brian Ripley wrote:
> > > 
> > > > The fptex problem has been reported as happening on Unix too.  Rd.sty
> > > > was changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1
> > > > patched not work?  I have no problem with fptex, and presume Duncan M
> > > > does not either, as he is distributing binaries of 1.9.0 beta.
> > > 
> > > On Linux RH9, TeX, Version 3.14159 (Web2C 7.3.1) (format=latex
> > > 2003.12.27), using r-devel rsync'ed this morning (1.9.0 beta
> > > (2004-03-28)), checking a package gives:
> > > --------------------------
> > > (/home/rsb/topics/Rtobe190/lib/R/share/texmf/hyperref.cfg
> > > (/usr/share/texmf/tex/latex/graphics/color.sty
> > > Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)
> > > 
> > > (/usr/share/texmf/tex/latex/config/color.cfg)
> > > 
> > > ! Package color Error: No driver specified.
> > > 
> > > See the color package documentation for explanation.
> > > Type  H <return>  for immediate help.
> > >  ...                                              
> > >                                                   
> > > l.123      }
> > >             
> > > You should make a default driver option in a file 
> > > color.cfg
> > > ------------------------
> > > but on r-patched 1.8.1 Patched (2004-02-17) everything is as it was:
> > > ------------------------
> > > (/home/rsb/topics/Rtobe181/lib/R/share/texmf/hyperref.cfg
> > > Package color Info: Redefining color Blue on input line 2.
> > > Package color Info: Redefining color Red on input line 3.
> > > Package hyperref Info: Option `hyperindex' set `true' on input line 16.
> > > Package hyperref Info: Option `colorlinks' set `true' on input line 16.
> > > Package hyperref Info: Option `linktocpage' set `true' on input line 16.
> > > Package hyperref Info: Option `plainpages' set `false' on input line 16.
> > > )
> > > -----------------------
> > > The issues seem to be around lines 220-240 in share/texmf/Rd.sty, but
> > > could be related to hyperref.cfg in the same directory - the new Rd.sty
> > > includes the same code as is in hyperref.cfg - should this file still be
> > > there? My TeX is too elementary to know, I'm afraid, but if hyperref.cfg
> > > is renamed to something else, the *.dvi checks to completion using devel
> > > without error, with log:
> > > -----------------------
> > > Package hyperref Info: Bookmarks ON on input line 1797.
> > > Package hyperref Info: Hyper figures OFF on input line 1816.
> > > Package hyperref Info: Link nesting OFF on input line 1821.
> > > Package hyperref Info: Hyper index ON on input line 1824.
> > > Package hyperref Info: Plain pages ON on input line 1829.
> > > Package hyperref Info: Backreferencing OFF on input line 1836.
> > > -----------------------
> > > Renaming/removing hyperref.cfg is also suggested by its absence in the 
> > > devel tarball, so this could be a result of rsync not deleting a stale and 
> > > now unneeded file, which then gets read and used. 
> > > 
> > > My color.cfg says:
> > > -------------------------
> > > \@ifundefined{pdfoutput}%
> > >   {\let\pdfoutput\@undefined
> > >    \ExecuteOptions{dvips}}%
> > >   {\ifcase\pdfoutput
> > >       \ExecuteOptions{dvips}%
> > >    \else
> > >       \ExecuteOptions{pdftex}%
> > >    \fi}%
> > > --------------------------
> > > I looked at this because I wanted to check packages I answer for on the 
> > > beta version, and had deleted the original mails about this before meeting 
> > > the same problem myself.
> > > 
> > > Roger
> > > 
> > > 
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From marie-pierre.sylvestre at mail.mcgill.ca  Sun Mar 28 22:00:27 2004
From: marie-pierre.sylvestre at mail.mcgill.ca (Marie-Pierre Sylvestre)
Date: Sun, 28 Mar 2004 15:00:27 -0500
Subject: [R] GLM for logistic regression and WEIGHTS
Message-ID: <008001c414ff$50d997f0$1392a8c0@epimgh.mcgill.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040328/d05ce88a/attachment.pl

From Roger.Bivand at nhh.no  Sun Mar 28 22:13:38 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 28 Mar 2004 22:13:38 +0200 (CEST)
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <Pine.LNX.4.44.0403281938300.27127-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0403282211140.21405-100000@reclus.nhh.no>

On Sun, 28 Mar 2004, Prof Brian Ripley wrote:

> I am now able to do a rsync checkout, and share/texmf/hyperref.cfg is not 
> there (which is correct).  I suggest you do a clean checkout.
> 

Yes, thank you - the error on my part was installing to an existing 
directory tree (which included the now unneeded share/texmf/hyperref.cfg). 
I hope this was Erich's problem too.

> On Sun, 28 Mar 2004, Roger Bivand wrote:
> 
> > On Sun, 28 Mar 2004, Prof Brian Ripley wrote:
> > 
> > > hyperref.cfg is not there in the cvs sources ... which I think answers the 
> > > point.  Did you use --delete on rsync?  The 1.9.0 beta FAQ has
> > > 
> > >      rsync -rC --delete rsync.R-project.org::MODULE R
> > 
> > $ rsync -rC --delete --verbose rsync.R-project.org::r-devel R-devel
> > 
> > was the command used, and many files were deleted, but not that one. I 
> > saw something similar when the new structure was introduced, maybe my 
> > rsync is malfunctioning:
> > 
> > rsync  version 2.5.7  protocol version 26
> > 
> > I checked in the 1.9.0beta for Windows - there is no hyperref.cfg in 
> > share/texmf there.
> > 
> > > 
> > > At present rsync.R-project.org is far too slow for me to test out what it 
> > > picks up (there seems to be a network problem with it).
> > > 
> > > On Sun, 28 Mar 2004, Roger Bivand wrote:
> > > 
> > > > On Sun 28 March 2004, Prof Brian Ripley wrote:
> > > > 
> > > > > The fptex problem has been reported as happening on Unix too.  Rd.sty
> > > > > was changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1
> > > > > patched not work?  I have no problem with fptex, and presume Duncan M
> > > > > does not either, as he is distributing binaries of 1.9.0 beta.
> > > > 
> > > > On Linux RH9, TeX, Version 3.14159 (Web2C 7.3.1) (format=latex
> > > > 2003.12.27), using r-devel rsync'ed this morning (1.9.0 beta
> > > > (2004-03-28)), checking a package gives:
> > > > --------------------------
> > > > (/home/rsb/topics/Rtobe190/lib/R/share/texmf/hyperref.cfg
> > > > (/usr/share/texmf/tex/latex/graphics/color.sty
> > > > Package: color 1999/02/16 v1.0i Standard LaTeX Color (DPC)
> > > > 
> > > > (/usr/share/texmf/tex/latex/config/color.cfg)
> > > > 
> > > > ! Package color Error: No driver specified.
> > > > 
> > > > See the color package documentation for explanation.
> > > > Type  H <return>  for immediate help.
> > > >  ...                                              
> > > >                                                   
> > > > l.123      }
> > > >             
> > > > You should make a default driver option in a file 
> > > > color.cfg
> > > > ------------------------
> > > > but on r-patched 1.8.1 Patched (2004-02-17) everything is as it was:
> > > > ------------------------
> > > > (/home/rsb/topics/Rtobe181/lib/R/share/texmf/hyperref.cfg
> > > > Package color Info: Redefining color Blue on input line 2.
> > > > Package color Info: Redefining color Red on input line 3.
> > > > Package hyperref Info: Option `hyperindex' set `true' on input line 16.
> > > > Package hyperref Info: Option `colorlinks' set `true' on input line 16.
> > > > Package hyperref Info: Option `linktocpage' set `true' on input line 16.
> > > > Package hyperref Info: Option `plainpages' set `false' on input line 16.
> > > > )
> > > > -----------------------
> > > > The issues seem to be around lines 220-240 in share/texmf/Rd.sty, but
> > > > could be related to hyperref.cfg in the same directory - the new Rd.sty
> > > > includes the same code as is in hyperref.cfg - should this file still be
> > > > there? My TeX is too elementary to know, I'm afraid, but if hyperref.cfg
> > > > is renamed to something else, the *.dvi checks to completion using devel
> > > > without error, with log:
> > > > -----------------------
> > > > Package hyperref Info: Bookmarks ON on input line 1797.
> > > > Package hyperref Info: Hyper figures OFF on input line 1816.
> > > > Package hyperref Info: Link nesting OFF on input line 1821.
> > > > Package hyperref Info: Hyper index ON on input line 1824.
> > > > Package hyperref Info: Plain pages ON on input line 1829.
> > > > Package hyperref Info: Backreferencing OFF on input line 1836.
> > > > -----------------------
> > > > Renaming/removing hyperref.cfg is also suggested by its absence in the 
> > > > devel tarball, so this could be a result of rsync not deleting a stale and 
> > > > now unneeded file, which then gets read and used. 
> > > > 
> > > > My color.cfg says:
> > > > -------------------------
> > > > \@ifundefined{pdfoutput}%
> > > >   {\let\pdfoutput\@undefined
> > > >    \ExecuteOptions{dvips}}%
> > > >   {\ifcase\pdfoutput
> > > >       \ExecuteOptions{dvips}%
> > > >    \else
> > > >       \ExecuteOptions{pdftex}%
> > > >    \fi}%
> > > > --------------------------
> > > > I looked at this because I wanted to check packages I answer for on the 
> > > > beta version, and had deleted the original mails about this before meeting 
> > > > the same problem myself.
> > > > 
> > > > Roger
> > > > 
> > > > 
> > > 
> > > 
> > 
> > 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Sun Mar 28 22:58:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 28 Mar 2004 21:58:18 +0100 (BST)
Subject: [R] GLM for logistic regression and WEIGHTS
In-Reply-To: <008001c414ff$50d997f0$1392a8c0@epimgh.mcgill.ca>
Message-ID: <Pine.LNX.4.44.0403282147480.27399-100000@gannet.stats>

There is no restriction to integer weights in R.  Here is a (silly) 
example.

library(MASS)
fit1 <- glm(cbind(pm.y, pm.tot - pm.y) ~ density, binomial, data=rotifer)
wt <- runif(20)
update(fit1, weights=wt)

or

glm(pm.y/pm.tot ~ density, binomial, data=rotifer, weights=pm.tot)
glm(pm.y/pm.tot ~ density, binomial, data=rotifer, weights=pm.tot*wt)

which gives an harmless warning (not an error message).

I use this sort of thing for multiple imputation quite frequently.


On Sun, 28 Mar 2004, Marie-Pierre Sylvestre wrote:

> Hi all,
> 
> I want to use weights for a logistic regression. In SAS, all I have to
> do is to specify my weight vector (they are fractions) and use proc
> logistic on my binary output.

That is all you do in R, too.  See the example above.

> When I tried to do the same in R, I got an error message because my
> weights were not integer. 

Please read the posting guide and supply a reproducible example of how you 
got an *error* message here.

> I understand that the weight option in R is to
> be used when the dependent variable is a proportion so that the weight
> is the total from which this proportion is derived.

You `understand' incorrectly.  Are you familiar with the theory of
generalized linear models -- weights are part of the definition of a glm?

> So what should I do if I want to use logistic regression but want to use
> weight to give more importance to certain observations (e.g.
> weight=0.87) and less to others (e.g. weight=.45) ? Should I
> reparametrize everything in terms of counts or is there an easier way
> out?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Sun Mar 28 23:52:52 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 28 Mar 2004 16:52:52 -0500
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <Pine.LNX.4.44.0403281301040.26340-100000@gannet.stats>
References: <4066B0EA.9030906@univie.ac.at>
	<Pine.LNX.4.44.0403281301040.26340-100000@gannet.stats>
Message-ID: <25ie60p6b0cfoot2k1vn6mldjmvtj2oklp@4ax.com>

On Sun, 28 Mar 2004 13:10:56 +0100 (BST), you wrote:

>The fptex problem has been reported as happening on Unix too.  Rd.sty was
>changed a little while ago, *but only* in 1.9.0 AFAIK.  Does 1.8.1 patched
>not work?  I have no problem with fptex, and presume Duncan M does not 
>either, as he is distributing binaries of 1.9.0 beta.

I've been using MikTeX, but have avoided upgrading to the latest
version since Erich warned me about problems there not too long ago.
After 1.9.0 hits the streets, I'll upgrade, and then try to track down
what needs changing to be compatible with both MikTeX and fptex.

Duncan Murdoch



From carlos.ortega at minorplanetspain.com  Mon Mar 29 00:12:30 2004
From: carlos.ortega at minorplanetspain.com (Carlos Ortega)
Date: Mon, 29 Mar 2004 00:12:30 +0200
Subject: [R] 3D globe plot
In-Reply-To: <6f714ced.274ea101.82eed00@po-a.temple.edu>
Message-ID: <003e01c41511$c5845d30$59ffa6d4@MinorplanetS>

Hello Yan,

Please try it with the library "scatterplot3d".

Regards,
Carlos.


-----Mensaje original-----
De: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] En nombre de yshen004 at temple.edu
Enviado el: viernes, 26 de marzo de 2004 22:28
Para: R-help at lists.R-project.org
Asunto: [R] 3D globe plot


Hi,

This is Yan, a graduate student from Temple University. My 
current work involves drawing 3D plot in R or S-Plus. I have 
a dataset containing distances between 18 cities in 4 
continents. Using classical multi-deminsional scaling in 2 
dimensions, I could seperate them successfully and plot with 
different colors in a 2D figure, which is attacked. Then I do 
the same in 3 dimensions and use 'brush' command in S. When I 
spin the graph, it shows that all data points are on a globe. 
However, the graph can't be saved. My question is, can I draw 
a globe in R and add all the data points?
Thanks in advance.



Best wishes,




Yan Shen
Temple University
Department of Statistics

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From erich.neuwirth at univie.ac.at  Mon Mar 29 00:32:49 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 29 Mar 2004 00:32:49 +0200
Subject: [R] Build problems on Windows with fpTeX and MiKTeX
In-Reply-To: <25ie60p6b0cfoot2k1vn6mldjmvtj2oklp@4ax.com>
References: <4066B0EA.9030906@univie.ac.at>
	<Pine.LNX.4.44.0403281301040.26340-100000@gannet.stats>
	<25ie60p6b0cfoot2k1vn6mldjmvtj2oklp@4ax.com>
Message-ID: <40675291.4050807@univie.ac.at>

Things work again.

I did a clean build for r-devel, getting a fresh source tree,
and now things work again (with fpTeX).
I also build abind and adapt to check if
packages can be build from source on Windows.
Both worked.
I did these two packages because abind is an "R code only" package and
adapt includes C and Fortran code.


-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From christian.schroeder at shredda.de  Mon Mar 29 01:18:16 2004
From: christian.schroeder at shredda.de (=?ISO-8859-1?Q?Christian_Schr=F6der?=)
Date: Mon, 29 Mar 2004 01:18:16 +0200
Subject: [R] German umlauts in PDF
Message-ID: <3142FC6B-810E-11D8-87E1-000A9591B408@shredda.de>

Hi all.

I am using R 1.8.1 on MacOS X (RAqua) to plot some graphs for my
diploma thesis. I had some problems getting the german umlauts
(a-dieresis, for example) into the output file using the pdf device
driver. The umlauts appeared as 'white holes' in the final pdf.

After hours of twiddling I found out that by changing the encoding
to WinAnsi, everything worked like a charm.

Just have a look at this:

pdf("winansi.pdf",encoding="WinAnsi")
plotSymbols()
dev.off()

pdf("isolatin1.pdf",encoding="ISOLatin1")
plotSymbols()
dev.off()

and the generated output in
http://www.shredda.de/winansi.pdf
and
http://www.shredda.de/isolatin1.pdf
respectively.

The former shows the character a-dieresis on position 228.
The latter shows just -ahem- nothing. Since iso8859-1 aka
latin 1 ist supposed to have a-dieresis on position 228 (dec),
I don't get the point.

If this is the preferred way to get umlauts into a pdf, it should
be noted somewhere. Otherwise, what did I miss?

Thanks in advance.

Regards,
Christian



From p.connolly at hortresearch.co.nz  Mon Mar 29 01:31:31 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 29 Mar 2004 11:31:31 +1200
Subject: [R] stop() vs. error() ?
In-Reply-To: <40659A02.7070509@yale.edu>;
	from ivo.welch@yale.edu on Sat, Mar 27, 2004 at 10:13:06AM -0500
References: <3A822319EB35174CA3714066D590DCD504AF7AA9@usrymx25.merck.com>
	<40659A02.7070509@yale.edu>
Message-ID: <20040329113131.A2137@hortresearch.co.nz>

On Sat, 27-Mar-2004 at 10:13AM -0500, ivo welch wrote:

|> 
|> hi andy:  mea culpa.  it is the exit function in most other languages.  
|> It would be exit(0) invocation in C (C++) and perl, for example.

I've not had enough experience in perl to have used exit, but this is
my guess at what is wanted:


spend.time <- function(i) {
  i <- 1
  while(i < 10){
    print(i)
    if (i == 5)
      break
    if (i == 7)
      stop("Is this an error?")
    i <- i + 1
  }
}

> spend.time(7)
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
> 

We don't get as far as 7, so the error message doesn't appear.



However, if the break is put after 7, an error has already occurred, as in:

spend.time2 <- function(i) {
  i <- 1
  while(i < 10){
    print(i)
    if (i == 9)
      break
    if (i == 7)
      stop("Is this an error?")
    i <- i + 1
  }
}

> spend.time2(7)
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
Error in spend.time2(7) : Is this an error?
> 


HTH


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From dunn at usq.edu.au  Mon Mar 29 04:21:30 2004
From: dunn at usq.edu.au (Peter Dunn)
Date: Mon, 29 Mar 2004 12:21:30 +1000
Subject: [R] White background in PS pictures
Message-ID: <4067882A.7090707@usq.edu.au>

Hi all

I am using R to produce postscript pictures via the
postscript  command.  I have never had any problems...
until now, when I want to inlcude my .ps file is a
LaTeX document *without* a white background.  (If
it's important, I'm using the  prosper  class with
the  whitecross option, so the background in blue.)

I would like my .ps file to have a white background.
If I read the help correctly (?postscript), this is
achieved using  bg="white", either in the call to
postscript  or via  ps.options.

However, whenever I do so, and include my .ps
file in my LaTeX document, it appears transparent
(I see the blue background, and the black text on the
figure is hard to read.)

I have read the help (at least what I thought was relevant)
and searched the mail archives but can't find any
reference to this problem.

Help appreciated.

P.

 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R


-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
   Web:    http://www.sci.usq.edu.au/staff/dunn
   Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



From Simon.Wotherspoon at utas.edu.au  Mon Mar 29 04:36:45 2004
From: Simon.Wotherspoon at utas.edu.au (Simon Wotherspoon)
Date: Mon, 29 Mar 2004 12:36:45 +1000
Subject: [R] Error term in aov
Message-ID: <NPEHJKCFDPAENEPOGEBPOEKCCBAA.Simon.Wotherspoon@utas.edu.au>

Hi,
	I'm trying to analyse a hierachical design and am running into some
trouble.  Clearly I don't fully understand "Error" and I was hoping someone
could set me straight.


We measure percentage algal cover in each of 5 quadrats from each of 16
patches where 4 treatments are randomly allocated to a patch.

First suppose patches are coded 1 to 16. then the following gives the
results I would expect.


> Algae <- c(0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0,
+            0, 0, 13, 0, 23, 17, 0, 56, 46, 0, 79, 32, 51, 22, 5, 0, 0, 0,
+            0, 0, 56, 3, 0, 41, 0, 5, 8, 0, 0, 0, 0, 0, 0, 0, 43, 8, 69,
+            29, 39, 40, 63, 0, 71, 5, 46, 44, 41, 29, 11, 65, 55, 61, 74,
+            55, 0, 5, 0, 0, 6, 30, 82, 70, 27, 83)
>
> Treat <- gl(4,20)
> Patch <- gl(16,5)
> Quadrat <- gl(5,1,80)
>
> d <- data.frame(Treat,Patch,Quadrat,Algae)
> fit <- aov(Algae ~ Treat + Error(Patch),data=d)
> summary(fit)

Error: Patch
          Df  Sum Sq Mean Sq F value  Pr(>F)
Treat      3 14429.1  4809.7  2.7171 0.09126 .
Residuals 12 21241.9  1770.2
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Error: Within
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals 64 19110.4   298.6


But if patches are coded  1-4 within treatments, then the following also
gives results I would expect,

> Patch <- gl(4,5,80)
>
> d1 <- data.frame(Treat,Patch,Quadrat,Algae)
> fit <- aov(Algae ~ Treat + Error(Patch %in% Treat),data=d1)
Warning message:
Error model is singular in: aov(Algae ~ Treat + Error(Patch %in% Treat),
data = d1)
> summary(fit)

Error: Patch:Treat
          Df  Sum Sq Mean Sq F value  Pr(>F)
Treat      3 14429.1  4809.7  2.7171 0.09126 .
Residuals 12 21242.0  1770.2
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Error: Within
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals 64 19110.4   298.6


But in R-1.9.0 beta (27/03/2004) I get the error about the model being
singular, which I do not get in 1.8.1.  Why is this?

Secondly, I had the impression that "/" and "%in%" were loosely synonymous,
so why is the following different?
> fit <- aov(Algae ~ Treat + Error(Treat/Patch),data=d1)
> summary(fit)

Error: Treat
      Df  Sum Sq Mean Sq
Treat  3 14429.1  4809.7

Error: Treat:Patch
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals 12 21241.9  1770.2

Error: Within
          Df  Sum Sq Mean Sq F value Pr(>F)
Residuals 64 19110.4   298.6



I had a look at the help for "formula" but that does not mention "/", and
the help for "Error" is not that enlightening either.

Any help is most welcome - sorry for yet again being a drain on everone's
time

Simon.





---



From k.wang at auckland.ac.nz  Mon Mar 29 06:46:34 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 29 Mar 2004 16:46:34 +1200
Subject: [R] Interpreting knn Results
References: <Pine.LNX.4.44.0403281311050.26340-100000@gannet.stats>
Message-ID: <004201c41548$d03db4b0$4c2ed882@cs.auckland.ac.nz>

Hi,

[I'm posting this on behalf of a colleague -- as I don't know knn myself...]

How to interpret the knn() results?

Tried the example codes in the documentation:
     data(iris3)
     train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
     test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
     cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
     knn(train, test, cl, k = 3, prob=TRUE)
     attributes(.Last.value)
and got:
$levels
[1] "c" "s" "v"

$class
[1] "factor"

$prob
 [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
 [7] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[13] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[19] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[25] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
[31] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
[37] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[43] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
[49] 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 1.0000000
[55] 1.0000000 1.0000000 1.0000000 1.0000000 0.5000000 1.0000000
[61] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
[67] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
[73] 1.0000000 1.0000000 0.6666667


What do the prob mean?

Thanks,

Kevin



From ceciliashiraiwa at ig.com.br  Mon Mar 29 07:03:33 2004
From: ceciliashiraiwa at ig.com.br (=?iso-8859-1?Q?Cec=EDlia_Shiraiwa?=)
Date: Mon, 29 Mar 2004 02:03:33 -0300
Subject: [R] logo
Message-ID: <004601c4154b$37a3aae0$88bb64c8@liass>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/40ef1e26/attachment.pl

From kjetil at entelnet.bo  Mon Mar 29 07:39:18 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Mon, 29 Mar 2004 01:39:18 -0400
Subject: [R] logo
In-Reply-To: <004601c4154b$37a3aae0$88bb64c8@liass>
Message-ID: <40677E46.15156.33DE629@localhost>

On 29 Mar 2004 at 2:03, Cec?lia Shiraiwa wrote:

> Dear all,
> 
> I used R in my work and would like to put the logo of these program on
> the background of my presentation. 

Bad idea! Statistics has to do with removal of noise, to keep what 
might be information. Logos in the background of a presentation is 
not information, so it is noise. Noise in statistics presentations
is illogical!

Kjetil Halvorsen

But the logo that cames with the
> program is in low resolution. Does anyone have the R logo in high
> resolution?
> 
> Thanks
> 
> Cec?lia Shiraiwa
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From suryobroto at ipb.ac.id  Mon Mar 29 08:22:05 2004
From: suryobroto at ipb.ac.id (Bambang Suryobroto)
Date: Mon, 29 Mar 2004 13:22:05 +0700
Subject: [R] multicolumn sort on dataframe?
Message-ID: <000701c41556$2b217440$3b00a8c0@MORF2>

Dear lists;

I'm migrating to and slowly learning R. I want to expand this multicolumn
sorting subject to counting the frequencies of mutiplicate rows.

The motivation is to count the frequencies of individuals with same
haplotypes in a population genetic study. A sample of table (ex.dta) is as
follows:

IDNUM DYS19 DYS388 DYS390 DYS393 DYS394 DYS395
TG002   200    129    203    133    251    119
TG053   200    129    203    133    251    119
TG020   200    129    207    133    251    127
TG066    NA     NA     NA     NA     NA     NA
TG104   200    129    203    133    251    119
TG018    NA     NA    199    133     NA    119
TG060   200    129    203    133    251    119
TG058    NA     NA     NA    133     NA     NA
TG009   200    129    203    133    251    119
TG106   200    129    211    137    251    123

I did like this:

> ex <- read.table( "ex.dta" , header=T, row.names=1 )
> one <- rep( 1,10 )
> aggregate( one , by=ex , sum )
  DYS19 DYS388 DYS390 DYS393 DYS394 DYS395 x
1   200    129    203    133    251    119 5
2   200    129    211    137    251    123 1
3   200    129    207    133    251    127 1

and got exactly what I wanted. However, as the table grows larger, the
script takes longer time to complete. For 300x6 table, after about 10
minutes Windows complained low in virtual memory and increased the paging
file while denying request from other applications. Eventually R crashed
leaving Windows crippled.

Did I miss something? Are there any ways other than the two line script
above?

Context:
R 1.8.1 on WinXP Pro
Rgui.exe --max-mem-size=400M
Celeron 1GHz, 256 MB ram, free harddisk space 3.3 GB

All best,

Bambang Suryobroto, D.Sc
Head, Laboratory of Zoology
Department of Biology
Faculty of Mathematics and Natural Sciences
Bogor Agricultural University
Jalan Pajajaran, Bogor 16143
INDONESIA
Tel: +62-251-328391
Fax: +62-251-345011



From J.Brainard at uea.ac.uk  Mon Mar 29 08:46:59 2004
From: J.Brainard at uea.ac.uk (J.Brainard@uea.ac.uk)
Date: Mon, 29 Mar 2004 07:46:59 +0100
Subject: [R] Could someone email me with the code for glm.nb ? 
Message-ID: <E1B7qXz-000yAA-00@login1.uea.ac.uk>


It is a pity that both times I have tried to use R  and the R-mailing
list that I received unpleasant replies, esp. coming from Ripley
himself this time.  Last time I simply asked "Why would I want to
use R over Splus??"  Which provoked some apologies that I wasn't to 
know that this would touch a nerve, in addition to a volume of ranting
and raving.  At least Ripley's reply wasn't vitriolic.  
 
I did not install R.  I wouldn't even try.  It's available centrally 
running on a Unix machine.  Trying to access glm.nb() at the R prompt
leads to these replies from R:

>glm.nb
Error: "glm.nb" Object not found
>


Which is why I though I could just ask for the simple text
function code, type it in, correct typos, make it work, fool-proof, 
low hassle....  Hahahahahahahhaa.

I don't have root priveleges and unashamedly
admit to not having the ability to install either MASS or the
negbin shell archive library (I spent many frustrating failed hours last
time I tried to do that).  I didn't realise R was only for software 
whizzes.  Now I know!


Investment in people is not my employer's strong suit.  Seems to come
with university environments.  Some of us aren't that important -- except
as convenient targets for being belittled, of course.

Maybe the employer will fork out for me to learn Stata and Limdep 
instead, eh?  At least the user-support-groups & might be friendlier.  
I had foolishly thought that the R community, having an ethos of self-
sufficiency, might be friendlier and more responsive than going 
via the Splus list.  Boy am I stupid or what?

-Julii



From ligges at statistik.uni-dortmund.de  Mon Mar 29 08:50:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Mar 2004 08:50:03 +0200
Subject: [R] White background in PS pictures
In-Reply-To: <4067882A.7090707@usq.edu.au>
References: <4067882A.7090707@usq.edu.au>
Message-ID: <4067C71B.3040508@statistik.uni-dortmund.de>

Peter Dunn wrote:
> Hi all
> 
> I am using R to produce postscript pictures via the
> postscript  command.  I have never had any problems...
> until now, when I want to inlcude my .ps file is a
> LaTeX document *without* a white background.  (If
> it's important, I'm using the  prosper  class with
> the  whitecross option, so the background in blue.)
> 
> I would like my .ps file to have a white background.
> If I read the help correctly (?postscript), this is
> achieved using  bg="white", either in the call to
> postscript  or via  ps.options.
> 
> However, whenever I do so, and include my .ps
> file in my LaTeX document, it appears transparent
> (I see the blue background, and the black text on the
> figure is hard to read.)
> 
> I have read the help (at least what I thought was relevant)
> and searched the mail archives but can't find any
> reference to this problem.
> 
> Help appreciated.
> 
> P.

It works for me as follows:

postscript(.....your.settings.....)
par(bg = "white")
plot(.....anything.....)
dev.off()

if it doesn't for you, you might want to try out a more recent version 
of R (R-1.8.1 is recent, the R-1.9.0 beta for people who want to help to 
make R-1.9.0 as stable as possible).


Uwe Ligges

>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    7.1
> year     2003
> month    06
> day      16
> language R
> 
>



From ligges at statistik.uni-dortmund.de  Mon Mar 29 08:54:17 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Mar 2004 08:54:17 +0200
Subject: [R] logo
In-Reply-To: <004601c4154b$37a3aae0$88bb64c8@liass>
References: <004601c4154b$37a3aae0$88bb64c8@liass>
Message-ID: <4067C819.1030807@statistik.uni-dortmund.de>

Cec?lia Shiraiwa wrote:

> Dear all,
> 
> I used R in my work and would like to put the logo of these program on the background of my presentation. But the logo that cames with the program is in low resolution. Does anyone have the R logo in high resolution?
> 
> Thanks
> 
> Cec?lia Shiraiwa

AFAIK, there is not high quality logo available. I think that's why the 
logo contest has been announced a while ago (you might want to look for 
that message by Paul Murrell).

Uwe Ligges



From nusbj at hotmail.com  Mon Mar 29 09:03:18 2004
From: nusbj at hotmail.com (Z P)
Date: Mon, 29 Mar 2004 15:03:18 +0800
Subject: [R] about a series of arrays
Message-ID: <SEA2-F33klim5HJuKfU000341c7@hotmail.com>

Dear all,

I now have a sequence of n<-3:21, for each i, I define an array of dimension
array((1/2)^i,c(rep(2,i))), I must update the (i+1)-th array with a loop 
according to the i-th array, How can I store all the previous arrays (the 
marginal parts of the (i+1)-th array will not equal to the i-th array)? It 
is in an iteration from 3 to 21, I later will need all the arrays. If I 
define 19 different variables, I still can not assign arrays to them in the 
iteration. What can I do? Thanks.

Regards,

Zhen



From ligges at statistik.uni-dortmund.de  Mon Mar 29 09:15:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Mar 2004 09:15:35 +0200
Subject: [R] about a series of arrays
In-Reply-To: <SEA2-F33klim5HJuKfU000341c7@hotmail.com>
References: <SEA2-F33klim5HJuKfU000341c7@hotmail.com>
Message-ID: <4067CD17.2050007@statistik.uni-dortmund.de>

Z P wrote:

> Dear all,
> 
> I now have a sequence of n<-3:21, for each i, I define an array of 
> dimension
> array((1/2)^i,c(rep(2,i))), I must update the (i+1)-th array with a loop 
> according to the i-th array, How can I store all the previous arrays 
> (the marginal parts of the (i+1)-th array will not equal to the i-th 
> array)? It is in an iteration from 3 to 21, I later will need all the 
> arrays. If I define 19 different variables, I still can not assign 
> arrays to them in the iteration. What can I do? Thanks.
> 
> Regards,
> 
> Zhen


Use a list as in:

  Alist <- vector(21, mode="list")
  n <- 3:21
  for(i in n){
     Alist[[i]] <- array(1/2^i, rep(2,i))
  }

with Alist[[3]] as the first non-NULL element, or:

  Alist <- lapply(3:21, function(i) array(1/2^i, rep(2,i)))

with Alist[[1]] as the first non-NULL element


Uwe Ligges

PS: Why do you ask R-help in CC, and other people as the main addresses?



From Saghir.Bashir at UCB-Group.com  Mon Mar 29 09:17:10 2004
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Mon, 29 Mar 2004 09:17:10 +0200
Subject: [R] logo
Message-ID: <3EBA5559F490D61189430002A5F0AE8905632582@ntexcrd.braine.ucb>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/a8b95c85/attachment.pl

From Roger.Bivand at nhh.no  Mon Mar 29 09:18:13 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 29 Mar 2004 09:18:13 +0200 (CEST)
Subject: [R] Could someone email me with the code for glm.nb ? 
In-Reply-To: <E1B7qXz-000yAA-00@login1.uea.ac.uk>
Message-ID: <Pine.LNX.4.44.0403290857500.21900-100000@reclus.nhh.no>

Dear Julii,

I think it would be appropriate for you to:

1) consider how much other users of this list need to know how you are 
feeling this morning;

2) read and follow the recommendation at the foot of list messages, to 
wit: "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html"

By reading R documentation, you would note that a common reason for not 
finding functions may be that you have not said for example library(MASS) 
first. The function you are seeking to use is documented in MASS the book 
by Venables and Ripley, which I expect that you have read and value. 

Although you may not find it easy to use the central Unix resource at your 
university, on which R may not be updated as regularly as it should be, I 
feel that researchers also have some responsibilities for their own tools, 
and would suggest you install a local copy on a machine you yourself 
administer on an operating system of your choice, and keep that copy up to 
date. You will also find that R objects save()ed on the central resource 
can readily be moved to a local system for analysis, so that responsible 
research is still possible even in inclement institutional environments. 
Since collaborative work in open source communities is based on people 
working together, I hope that you will consider contributing when you have 
something to offer.

Stating the R version you have problems with, and giving enough detail to
let the package maintainer (in this case Prof. Ripley) re-create the
setting in which the problem occurred is a minimal offering for a report
to be helpful. 

On Mon, 29 Mar 2004 J.Brainard at uea.ac.uk wrote:

> 
> It is a pity that both times I have tried to use R  and the R-mailing
> list that I received unpleasant replies, esp. coming from Ripley
> himself this time.  Last time I simply asked "Why would I want to
> use R over Splus??"  Which provoked some apologies that I wasn't to 
> know that this would touch a nerve, in addition to a volume of ranting
> and raving.  At least Ripley's reply wasn't vitriolic.  
>  
> I did not install R.  I wouldn't even try.  It's available centrally 
> running on a Unix machine.  Trying to access glm.nb() at the R prompt
> leads to these replies from R:
> 
> >glm.nb
> Error: "glm.nb" Object not found
> >
> 
> 
> Which is why I though I could just ask for the simple text
> function code, type it in, correct typos, make it work, fool-proof, 
> low hassle....  Hahahahahahahhaa.
> 
> I don't have root priveleges and unashamedly
> admit to not having the ability to install either MASS or the
> negbin shell archive library (I spent many frustrating failed hours last
> time I tried to do that).  I didn't realise R was only for software 
> whizzes.  Now I know!
> 
> 
> Investment in people is not my employer's strong suit.  Seems to come
> with university environments.  Some of us aren't that important -- except
> as convenient targets for being belittled, of course.
> 
> Maybe the employer will fork out for me to learn Stata and Limdep 
> instead, eh?  At least the user-support-groups & might be friendlier.  
> I had foolishly thought that the R community, having an ethos of self-
> sufficiency, might be friendlier and more responsive than going 
> via the Splus list.  Boy am I stupid or what?
> 
> -Julii
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From adi at roda.ro  Mon Mar 29 10:54:29 2004
From: adi at roda.ro (Adrian Dusa)
Date: Mon, 29 Mar 2004 11:54:29 +0300
Subject: [R] multicolumn sort on dataframe?
Message-ID: <001301c4156b$7e74bc00$6901a8c0@roda.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/74424076/attachment.pl

From seghieri at ds.unifi.it  Mon Mar 29 11:27:26 2004
From: seghieri at ds.unifi.it (Chiara Seghieri)
Date: Mon, 29 Mar 2004 11:27:26 +0200
Subject: [R] Problems with "optimize"
Message-ID: <5.2.0.9.0.20040329111800.01d9dad0@ds.unifi.it>

Dear All,

I'm trying to maximize a likelihood with respect one parameter using 
"optimize" on simulated data (without error component).
I've iterated the maximization procedure 1000 times and I should always 
obtain the same estimate of the parameter (equal to the simulated one) but, 
instead, i obtain different results (the likelihood function shouldn't be 
flat). Does anybody has experience with the optimize procedure?

Thanks in advance,

Chiara



Chiara Seghieri
Dipartimento di Scienze Statistiche "G.Parenti"
V.le Morgagni, 59
50134 Firenze, Italy
tel.: +39 055 4237230
fax: +39 055 4223560



From christoph.lehmann at gmx.ch  Mon Mar 29 10:58:00 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 29 Mar 2004 10:58:00 +0200
Subject: [R] c() question
Message-ID: <1080550680.1108.23.camel@christophl>

Hi

I need to define the following

	c("one group" = class.weight[2], "other group" = class.weight[1])
#class.weight = c(1,2)

but I don't like the hard-coded way and would like to use

	my.group <- array(c("one group", "other group"))

but now

	c(my.group[1] = class.weight[2], my.group[2] = class.weight[1])

gives an error

how can I solve this small problem?

thanks for a hint

christoph

-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From ligges at statistik.uni-dortmund.de  Mon Mar 29 12:02:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Mar 2004 12:02:39 +0200
Subject: [R] Problems with "optimize"
In-Reply-To: <5.2.0.9.0.20040329111800.01d9dad0@ds.unifi.it>
References: <5.2.0.9.0.20040329111800.01d9dad0@ds.unifi.it>
Message-ID: <4067F43F.3060102@statistik.uni-dortmund.de>

Chiara Seghieri wrote:

> Dear All,
> 
> I'm trying to maximize a likelihood with respect one parameter using 
> "optimize" on simulated data (without error component).
> I've iterated the maximization procedure 1000 times and I should always 
> obtain the same estimate of the parameter (equal to the simulated one) 
> but, instead, i obtain different results (the likelihood function 
> shouldn't be flat). Does anybody has experience with the optimize 
> procedure?
> 
> Thanks in advance,
> 
> Chiara

Maybe you are stuck in local minima?
You have to expect different results, if the data has been simulated 
each time you have optimized, BTW.
We cannot say anything concrete since we don't now how different your 
results are and we do not know anything about the real underlying problem.

Uwe Ligges




> 
> 
> Chiara Seghieri
> Dipartimento di Scienze Statistiche "G.Parenti"
> V.le Morgagni, 59
> 50134 Firenze, Italy
> tel.: +39 055 4237230
> fax: +39 055 4223560
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From JonesW at kssg.com  Mon Mar 29 12:01:00 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 29 Mar 2004 11:01:00 +0100
Subject: [R] Problems with "optimize"
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F1059@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/c6dcd60f/attachment.pl

From JonesW at kssg.com  Mon Mar 29 12:06:58 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 29 Mar 2004 11:06:58 +0100
Subject: [R] stl and NA
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F105A@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/5aeeebf9/attachment.pl

From lecoutre at stat.ucl.ac.be  Mon Mar 29 12:19:14 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 29 Mar 2004 12:19:14 +0200
Subject: [R] c() question
In-Reply-To: <1080550680.1108.23.camel@christophl>
References: <1080550680.1108.23.camel@christophl>
Message-ID: <6.0.1.1.2.20040329121609.02255758@stat4ux.stat.ucl.ac.be>



Hi,

Try to assign the names to your vector after having created it.
It works and it will also simplify your code.

Eric


my.group <- c("one group", "other group")
class.weight <- c(1,2)
my.order <- c(2,1)
my.vector <- class.weight[my.order]
names(my.vector) <- my.group

 > my.vector
   one group other group
           2           1

At 10:58 29/03/2004, Christoph Lehmann wrote:
>Hi
>
>I need to define the following
>
>         c("one group" = class.weight[2], "other group" = class.weight[1])
>#class.weight = c(1,2)
>
>but I don't like the hard-coded way and would like to use
>
>         my.group <- array(c("one group", "other group"))
>
>but now
>
>         c(my.group[1] = class.weight[2], my.group[2] = class.weight[1])
>
>gives an error
>
>how can I solve this small problem?
>
>thanks for a hint
>
>christoph
>
>--
>Christoph Lehmann <christoph.lehmann at gmx.ch>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From SIMKN-JE at pelican.vista.ac.za  Mon Mar 29 22:18:45 2004
From: SIMKN-JE at pelican.vista.ac.za (MR. JUNE ELIJAH SIMAKANI)
Date: Mon, 29 Mar 2004 12:18:45 -0800
Subject: [R] Subsrcribe r-help
Message-ID: <200403291021.i2TAG9fp016104@vugon.vista.ac.za>



**********************************************************
June Elijah Simakani
Department of Statistics
University of Port Elizabeth, Vista Campus
P.O.Box 1600, Port Elizabeth 6000
South Africa
Tel.no: + 27(41) 408 3234(w)
        + 27(41) 582 1573(h)
Fax.no: + 27(41) 464 2859
**********************************************************  
_________________________
Content and Virus scanned 
 by  Inflex  and  Mcafee



From Simon.Fear at synequanon.com  Mon Mar 29 12:26:15 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 29 Mar 2004 11:26:15 +0100
Subject: [R] c() question
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F02233@synequanon01>

Christoph, yes there IS a way, such as the horrible

eval(parse(text=paste("c(", my.group[1],
  " = ", class.weight[2], ",", my.group[2], " = ", class.weight[1], ")", sep="")))

BUT I would suggest you rethink whether there isn't a an altogether
more straightforward way to do what you want. For example, 

tmp <- class.weight
names(tmp) <- my.group

HTH

> -----Original Message-----
> From: Christoph Lehmann [mailto:christoph.lehmann at gmx.ch]
> Sent: 29 March 2004 09:58
> To: r-help at stat.math.ethz.ch
> Subject: [R] c() question
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Hi
> 
> I need to define the following
> 
> 	c("one group" = class.weight[2], "other group" = 
> class.weight[1])
> #class.weight = c(1,2)
> 
> but I don't like the hard-coded way and would like to use
> 
> 	my.group <- array(c("one group", "other group"))
> 
> but now
> 
> 	c(my.group[1] = class.weight[2], my.group[2] = class.weight[1])
> 
> gives an error
> 
> how can I solve this small problem?
> 
> thanks for a hint
> 
> christoph
> 
> -- 
> Christoph Lehmann <christoph.lehmann at gmx.ch>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From stecalza at tiscali.it  Mon Mar 29 11:46:13 2004
From: stecalza at tiscali.it (Stefano Calza)
Date: Mon, 29 Mar 2004 11:46:13 +0200
Subject: [R] problem with update.packages()
In-Reply-To: <s0644a3a.090@med-gwia-01a.med.umich.edu>
References: <s0644a3a.090@med-gwia-01a.med.umich.edu>
Message-ID: <20040329094613.GB13815@med.unibs.it>

Hi.

I'm experiencing a problem with updating packages on R 1.8.1 (2003-11-21) on Debian testing.
I get the following message when updating for example Design:

...
...
/usr/bin/ld: cannot find -lg2c-pic
...

But I sould have it. I never had problem before

$gcc -v

Reading specs from /usr/lib/gcc-lib/i486-linux/3.3.3/specs
Configured with: ../src/configure -v --enable-languages=c,c++,java,f77,pascal,objc,ada,treelang --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-gxx-include-dir=/usr/include/c++/3.3 --enable-shared --with-system-zlib --enable-nls --without-included-gettext --enable-__cxa_atexit --enable-clocale=gnu --enable-debug --enable-java-gc=boehm --enable-java-awt=xlib --enable-objc-gc i486-linux
Thread model: posix
gcc version 3.3.3 (Debian)

Any suggestion?

TIA,
Stefano



From andy_liaw at merck.com  Mon Mar 29 13:19:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Mar 2004 06:19:12 -0500
Subject: [R] Interpreting knn Results
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AB2@usrymx25.merck.com>

Maybe you should show your colleague how to access help pages in R?  Right
in ?knn, it says:

    prob: If this is true, the proportion of the votes for the winning
          class are returned as attribute 'prob'. 

so 1.0 mean all three NNs are of the `winning'; i.e., predicted, class, and
0.66667 means 2 out of the 3 NNs are of the winning class, etc.  

Andy

> From: Ko-Kang Kevin Wang
> 
> Hi,
> 
> [I'm posting this on behalf of a colleague -- as I don't know 
> knn myself...]
> 
> How to interpret the knn() results?
> 
> Tried the example codes in the documentation:
>      data(iris3)
>      train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
>      test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
>      cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
>      knn(train, test, cl, k = 3, prob=TRUE)
>      attributes(.Last.value)
> and got:
> $levels
> [1] "c" "s" "v"
> 
> $class
> [1] "factor"
> 
> $prob
>  [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
>  [7] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
> [13] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
> [19] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
> [25] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
> [31] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
> [37] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
> [43] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
> [49] 1.0000000 1.0000000 1.0000000 0.6666667 0.7500000 1.0000000
> [55] 1.0000000 1.0000000 1.0000000 1.0000000 0.5000000 1.0000000
> [61] 1.0000000 1.0000000 1.0000000 0.6666667 1.0000000 1.0000000
> [67] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.6666667
> [73] 1.0000000 1.0000000 0.6666667
> 
> 
> What do the prob mean?
> 
> Thanks,
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Virgilio.Gomez at uv.es  Mon Mar 29 13:29:27 2004
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Mon, 29 Mar 2004 13:29:27 +0200
Subject: [R] Extrange behaviour of Gcc-3.3
Message-ID: <1080559767.18871.36.camel@chomsky.estadi.uv.es>

Dear R users,

I have found a quite uncommon behaviour when compiling a library I want
to call from R (avce00, in package RArcInfo).

I define CPL_LSB (-DCPL_LSB is passed to the compiler in the command
line) but it's useless, since #ifdef CPL_LSB ... #endif clauses doesn't
notice that CPL_LSB is defined. I also set -O2.

Any clue? I have tested  the library without optimising the code and it
works fine.

I am using gcc-3.3 on a GNU/Linux debian box, but I have the same
problem when I try to compile Windows binaries.

I think it is important to mention that everything started to fail when
R moved from gcc-2.95 to gcc-3.x.



Thanks in advance.

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35



From k.wang at auckland.ac.nz  Mon Mar 29 13:33:46 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 29 Mar 2004 23:33:46 +1200
Subject: [R] Interpreting knn Results
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7AB2@usrymx25.merck.com>
Message-ID: <20040329113356.VRMS9742.mta2-rme.xtra.co.nz@kevinlpt>

Hi,

> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Maybe you should show your colleague how to access help pages
> in R?  Right
> in ?knn, it says:
>
>     prob: If this is true, the proportion of the votes for the
winning
>           class are returned as attribute 'prob'.
>
> so 1.0 mean all three NNs are of the `winning'; i.e.,
> predicted, class, and
> 0.66667 means 2 out of the 3 NNs are of the winning class, etc.

Whoops sorry, I guess I should've known that *_*.  She lost access to
her email due to a technical problem and I was running to a tutorial
so didn't have time to think.  After my tutorial I looked at the
results with her and figured it out....*_*

Thanks,

Kevin



From Ben.Stewart-Koster at student.griffith.edu.au  Mon Mar 29 13:42:47 2004
From: Ben.Stewart-Koster at student.griffith.edu.au (Ben Stewart-Koster)
Date: Mon, 29 Mar 2004 21:42:47 +1000
Subject: [R] rpart or mvpart
Message-ID: <610a84611434.611434610a84@student.griffith.edu.au>



From e.leuven at uva.nl  Mon Mar 29 14:14:42 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Mon, 29 Mar 2004 14:14:42 +0200 (CEST)
Subject: [R] data usage
Message-ID: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>

hello,

for my present project i need to use the data stored in a ca. 100mb 
stata dataset. 

when i import the data in R using:

library("foreign")
x<-read.dta("mydata.dta")

i find that R needs a startling 665mb of memory!

(in stata i can simply allocate, say, 128mb of memory and go ahead)

is there anyway around this, or should i forget R for analysis of 
datasets of this magnitude?

thanks for you help in this, edwin.



From stecalza at tiscali.it  Mon Mar 29 14:42:04 2004
From: stecalza at tiscali.it (Stefano Calza)
Date: Mon, 29 Mar 2004 14:42:04 +0200
Subject: [R] problem with update.packages() - SOLVED
In-Reply-To: <s0644a3a.090@med-gwia-01a.med.umich.edu>
References: <s0644a3a.090@med-gwia-01a.med.umich.edu>
Message-ID: <20040329124204.GE15344@med.unibs.it>

Hi,

problem solved passing to R 1.9.0 (in the Debian unstable branch).

Stefano



From bates at stat.wisc.edu  Mon Mar 29 15:25:34 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 29 Mar 2004 07:25:34 -0600
Subject: [R] data usage
In-Reply-To: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>
References: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>
Message-ID: <6rsmfryecx.fsf@bates4.stat.wisc.edu>

Edwin Leuven <e.leuven at uva.nl> writes:

> for my present project i need to use the data stored in a ca. 100mb 
> stata dataset. 
> 
> when i import the data in R using:
> 
> library("foreign")
> x<-read.dta("mydata.dta")
> 
> i find that R needs a startling 665mb of memory!
> 
> (in stata i can simply allocate, say, 128mb of memory and go ahead)
> 
> is there anyway around this, or should i forget R for analysis of 
> datasets of this magnitude?

What does the 665 MB represent?  Did you try doing a garbage
collection after you had done the import?

I would suggest

library("foreign")
x<-read.dta("mydata.dta")
gc()              # possibly repeat gc() to lower the thresholds
object.size(x)    # the actual storage (in bytes) allocated to this object
save(x, file = "mydata.rda", compress = TRUE)

After that you can start a new session and use

load("mydata.rda")

to obtain a copy of the data set without the storage overhead incurred
by the stata -> R conversion.

P.S. As described in the help page for object.size, the returned value
is more properly described as an estimate of the object size because
sometimes it is difficult to determine the object size accurately.



From andy_liaw at merck.com  Mon Mar 29 15:40:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Mar 2004 08:40:54 -0500
Subject: [R] Could someone email me with the code for glm.nb ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AB4@usrymx25.merck.com>

Dear Dr. Brainard,

In addition to what Roger has pointed out, I would like to point out one
fact that you seem not to be aware of.  Open source software, by and large,
have licences, just as the proprietary ones.  Openly asking others to send
you the code, IMHO, is plain disregard of the licence for which the software
is covered under.  In this case all you received seems to be bruised ego,
but in other situation you might be visited by law enforcement, or a letter
from some attorney, neither of which I doubt you will find `friendlier'.  I
do not know how user-contributed code in Stata are licenced, but that's for
you to investigate.  Please do not confuse open source with public domain.

Also, there is nothing to prevent you from installing add-on packages
without root priviledge.  You can install packages anywhere that you have
read/write access.  The help pages for install.packages() and library() have
all the details, and I am quite sure you do not require assistance in
reading them.

With regard to your previous question: Given the tone that you asked the
question in, the most polite response I can squeeze out of myself is `Why
would I care to tell you?'  Have you tried asking your thesis advisor why
you should read his or her papers?

Best Regards,
Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com        732-594-0820



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> J.Brainard at uea.ac.uk
> Sent: Monday, March 29, 2004 1:47 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Could someone email me with the code for glm.nb ?
> 
> 
> 
> It is a pity that both times I have tried to use R  and the R-mailing
> list that I received unpleasant replies, esp. coming from Ripley
> himself this time.  Last time I simply asked "Why would I want to
> use R over Splus??"  Which provoked some apologies that I wasn't to 
> know that this would touch a nerve, in addition to a volume of ranting
> and raving.  At least Ripley's reply wasn't vitriolic.  
>  
> I did not install R.  I wouldn't even try.  It's available centrally 
> running on a Unix machine.  Trying to access glm.nb() at the R prompt
> leads to these replies from R:
> 
> >glm.nb
> Error: "glm.nb" Object not found
> >
> 
> 
> Which is why I though I could just ask for the simple text
> function code, type it in, correct typos, make it work, fool-proof, 
> low hassle....  Hahahahahahahhaa.
> 
> I don't have root priveleges and unashamedly
> admit to not having the ability to install either MASS or the
> negbin shell archive library (I spent many frustrating failed 
> hours last
> time I tried to do that).  I didn't realise R was only for software 
> whizzes.  Now I know!
> 
> 
> Investment in people is not my employer's strong suit.  Seems to come
> with university environments.  Some of us aren't that 
> important -- except
> as convenient targets for being belittled, of course.
> 
> Maybe the employer will fork out for me to learn Stata and Limdep 
> instead, eh?  At least the user-support-groups & might be 
> friendlier.  
> I had foolishly thought that the R community, having an ethos of self-
> sufficiency, might be friendlier and more responsive than going 
> via the Splus list.  Boy am I stupid or what?
> 
> -Julii
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andreas.plank at web.de  Mon Mar 29 15:54:48 2004
From: andreas.plank at web.de (Andreas Plank)
Date: Mon, 29 Mar 2004 15:54:48 +0200
Subject: [R] calculate length of gradient ?
Message-ID: <200403291354.i2TDsmQ16837@mailgate5.cinetic.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
X-Virus-Scanned: by amavisd-new
X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
X-Spam-Level: ****
X-Spam-Status: No, hits=4.2 required=5.0 tests=MSGID_FROM_MTA_HEADER,RCVD_IN_BL_SPAMCOP_NET autolearn=no version=2.63

Dear r-help list,

my question is about ordination technics:
To decide using a linear model response or a unimodal model response,
in Canoco (software for ordination technics)
you can calculate a so-called "length of gradient".

(from Canoco Help file:)
> The length of gradient is a measure of how unimodal the species
> responses are along an ordination axis. It is the range of the
> sample scores divided by the average within-species standard
> deviation along the axis. The gradient length is expressed in
> standard deviation units of species turnover (SD).

The function  cca in package vegan and other functions give many
outputs, but I didn't find anything about "length of gradient". Is
there a possibility to calculate this in R? If yes how can I
calculate this?

Many thanks in advance, Andreas
_______________________________________________________________________
... and the winner is... WEB.DE FreeMail! - Deutschlands beste E-Mail



From e.leuven at uva.nl  Mon Mar 29 16:01:16 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Mon, 29 Mar 2004 16:01:16 +0200 (CEST)
Subject: [R] data usage
In-Reply-To: <6rsmfryecx.fsf@bates4.stat.wisc.edu>
References: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>
	<6rsmfryecx.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.58.0403291548010.31342@e182104.fee.uva.nl>

> What does the 665 MB represent?  Did you try doing a garbage
> collection after you had done the import?

i didn't (sorry, R beginner)

i followed your example and things look much better now, and
object.size(x) returns:

219,167,604

which is about double the size of the same object in stata where it 
is:

104,882,604

this leaves quite some room for improvement, but at least i can 
now handle the data on my laptop...

thanks for your quick response! edwin


> I would suggest
> 
> library("foreign")
> x<-read.dta("mydata.dta")
> gc()              # possibly repeat gc() to lower the thresholds
> object.size(x)    # the actual storage (in bytes) allocated to this object
> save(x, file = "mydata.rda", compress = TRUE)
> 
> After that you can start a new session and use
> 
> load("mydata.rda")
> 
> to obtain a copy of the data set without the storage overhead incurred
> by the stata -> R conversion.
> 
> P.S. As described in the help page for object.size, the returned value
> is more properly described as an estimate of the object size because
> sometimes it is difficult to determine the object size accurately.
>



From gregory_r_warnes at groton.pfizer.com  Mon Mar 29 15:59:51 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Mon, 29 Mar 2004 08:59:51 -0500
Subject: [R] gregmisc: install trouble
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B18D@groexmb02.pfizer.com>

Further, there is a new version of gregmisc, 0.10.0, on CRAN now for use
with R 1.9.0.

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Uwe Ligges
> Sent: Saturday, March 27, 2004 6:30 AM
> To: Christian Reilly
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] gregmisc: install trouble
> 
> 
> Christian Reilly wrote:
> > 
> > Hello all,
> > 
> > I'm having trouble installing the package 'gregmisc'. I've run the
> > command:
> > 
> > > R CMD INSTALL -l /usr/lib/R/library /tmp/gregmisc_0.9.0.tar.gz
> > 
> > And I can see it shows up on the list of available packeage 
> when I issue
> > > library()
> > 
> > But when I do
> > 
> > >library(gregmisc)
> > 
> > I get:
> > 
> > Error in parse(file, n, text, prompt) : syntax error on line 3105
> > 
> > All the other packages I've install load fine. Permissions for the
> > 'gregmisc' library are the same as all the rest.
> > 
> > Any Suggestions?
> > 
> > I'm running on Debian Linux(Woody)
> > 
> > Thanks much,
> > 
> > Christian Reilly
> > 
> > ---------------
> > Hopkins Marine Station
> > Stanford U
> > Pacific Grove, CA
> 
> 
> Which version of R is this? Works fine with R-1.8.1 and R-1.9.0 beta.
> Please download the package again and reinstall.
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From migreja at med.up.pt  Mon Mar 29 16:01:26 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?J=FAlia?= Rodrigues Igreja)
Date: Mon, 29 Mar 2004 14:01:26 -0000
Subject: [R] UNION
Message-ID: <4.3.2.7.1.19970101004439.00ad0490@mail.med.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/432da7cf/attachment.pl

From pauling at giub.unibe.ch  Mon Mar 29 16:01:40 2004
From: pauling at giub.unibe.ch (Andreas Pauling)
Date: Mon, 29 Mar 2004 16:01:40 +0200
Subject: [R] strange thing with sd
Message-ID: <1080568900.40682c4455c2e@www.cx.unibe.ch>

Dear R people

I came across a strange thing: 


sd(rep(0.01,      15))  #0
sd(rep(0.001,     15))  #4.489023e-19
sd(rep(0.00001,   15))  #0
sd(rep(0.00000001,15))  #1.712427e-24

sd(rep(0.01,      13))  #1.805557e-18
sd(rep(0.001,     13))  #4.513894e-19
sd(rep(0.00001,   13))  #0
sd(rep(0.00000001,13))  #0

sd(rep(5.01,	  15))  #0
sd(rep(5.001,	  15))  #4.489023e-19
sd(rep(5.00001,   15))  #1.838704e-15
sd(rep(5.00000001,15))  #9.19352e-16

sd(rep(5.01,	  13))  #9.244454e-16
sd(rep(5.001,	  13))  #9.244454e-16
sd(rep(5.00001,   13))  #1.848891e-15
sd(rep(5.00000001,13))  #0

Why gives sd sometimes zero and sometimes values close to zero
and why does it depend on the value and on how many times it is
repeated?
Shouldn't it give always zero?
Is there a way to control this?

I use R Version 1.8.1 under UNIX.

Thanks for any suggestions!!

Andreas



From Rau at demogr.mpg.de  Mon Mar 29 16:06:02 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Mon, 29 Mar 2004 16:06:02 +0200
Subject: [R] strange thing with sd
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A07E9@hermes.demogr.mpg.de>

Hello,

> -----Original Message-----
> From:	Andreas Pauling [SMTP:pauling at giub.unibe.ch]
> Sent:	Monday, March 29, 2004 4:02 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] strange thing with sd
> 
> 
> Shouldn't it give always zero?
> 
	I was running your example code, and I always had the result zero
(see below).

	Best,
	Roland 

	> sd(rep(0.01,      15))  #0
	[1] 0
	> sd(rep(0.001,     15))  #4.489023e-19
	[1] 0
	> sd(rep(0.00001,   15))  #0
	[1] 0
	> sd(rep(0.00000001,15))  #1.712427e-24
	[1] 0
	> 
	> sd(rep(0.01,      13))  #1.805557e-18
	[1] 0
	> sd(rep(0.001,     13))  #4.513894e-19
	[1] 0
	> sd(rep(0.00001,   13))  #0
	[1] 0
	> sd(rep(0.00000001,13))  #0
	[1] 0
	> 
	> sd(rep(5.01,  15))  #0
	[1] 0
	> sd(rep(5.001,  15))  #4.489023e-19
	[1] 0
	> sd(rep(5.00001,   15))  #1.838704e-15
	[1] 0
	> sd(rep(5.00000001,15))  #9.19352e-16
	[1] 0
	> 
	> sd(rep(5.01,  13))  #9.244454e-16
	[1] 0
	> sd(rep(5.001,  13))  #9.244454e-16
	[1] 0
	> sd(rep(5.00001,   13))  #1.848891e-15
	[1] 0
	> sd(rep(5.00000001,13))  #0
	[1] 0
	> version
	         _              
	platform i386-pc-mingw32
	arch     i386           
	os       mingw32        
	system   i386, mingw32  
	status                  
	major    1              
	minor    8.1            
	year     2003           
	month    11             
	day      21             
	language R              
	> 


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From migreja at med.up.pt  Mon Mar 29 16:09:07 2004
From: migreja at med.up.pt (Margarida =?iso-8859-1?Q?J=FAlia?= Rodrigues Igreja)
Date: Mon, 29 Mar 2004 14:09:07 -0000
Subject: [R] BETWEEN
Message-ID: <4.3.2.7.1.19970101005426.00ad7c10@mail.med.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/b98a0ebd/attachment.pl

From flom at ndri.org  Mon Mar 29 16:12:49 2004
From: flom at ndri.org (Peter Flom)
Date: Mon, 29 Mar 2004 09:12:49 -0500
Subject: [R] strange thing with sd
Message-ID: <s067e8b3.097@MAIL.NDRI.ORG>

I tried your sample code, and a few other variations, and got 0 for all
of them, running R 1.8.1 on a Windows machine

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



>>> Andreas Pauling <pauling at giub.unibe.ch> 3/29/2004 9:01:40 AM >>>
Dear R people

I came across a strange thing: 


sd(rep(0.01,      15))  #0
sd(rep(0.001,     15))  #4.489023e-19
sd(rep(0.00001,   15))  #0
sd(rep(0.00000001,15))  #1.712427e-24

sd(rep(0.01,      13))  #1.805557e-18
sd(rep(0.001,     13))  #4.513894e-19
sd(rep(0.00001,   13))  #0
sd(rep(0.00000001,13))  #0

sd(rep(5.01,	  15))  #0
sd(rep(5.001,	  15))  #4.489023e-19
sd(rep(5.00001,   15))  #1.838704e-15
sd(rep(5.00000001,15))  #9.19352e-16

sd(rep(5.01,	  13))  #9.244454e-16
sd(rep(5.001,	  13))  #9.244454e-16
sd(rep(5.00001,   13))  #1.848891e-15
sd(rep(5.00000001,13))  #0

Why gives sd sometimes zero and sometimes values close to zero
and why does it depend on the value and on how many times it is
repeated?
Shouldn't it give always zero?
Is there a way to control this?

I use R Version 1.8.1 under UNIX.

Thanks for any suggestions!!

Andreas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Mon Mar 29 16:22:41 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 29 Mar 2004 09:22:41 -0500
Subject: [R] strange thing with sd
In-Reply-To: <s067e8b3.097@MAIL.NDRI.ORG>
References: <s067e8b3.097@MAIL.NDRI.ORG>
Message-ID: <40683131.3030705@optonline.net>

Perhaps this is related to PR#1228

http://r-bugs.biostat.ku.dk/cgi-bin/R/Accuracy?id=1228;user=guest

which seems to have been improved in recent versions.

>>>>Andreas Pauling <pauling at giub.unibe.ch> 3/29/2004 9:01:40 AM >>>
> 
> Dear R people
> 
> I came across a strange thing: 
> 
> 
> sd(rep(0.01,      15))  #0
> sd(rep(0.001,     15))  #4.489023e-19
> sd(rep(0.00001,   15))  #0
> sd(rep(0.00000001,15))  #1.712427e-24
> 
> sd(rep(0.01,      13))  #1.805557e-18
> sd(rep(0.001,     13))  #4.513894e-19
> sd(rep(0.00001,   13))  #0
> sd(rep(0.00000001,13))  #0
> 
> sd(rep(5.01,	  15))  #0
> sd(rep(5.001,	  15))  #4.489023e-19
> sd(rep(5.00001,   15))  #1.838704e-15
> sd(rep(5.00000001,15))  #9.19352e-16
> 
> sd(rep(5.01,	  13))  #9.244454e-16
> sd(rep(5.001,	  13))  #9.244454e-16
> sd(rep(5.00001,   13))  #1.848891e-15
> sd(rep(5.00000001,13))  #0
> 
> Why gives sd sometimes zero and sometimes values close to zero
> and why does it depend on the value and on how many times it is
> repeated?
> Shouldn't it give always zero?
> Is there a way to control this?
> ...

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From B.Rowlingson at lancaster.ac.uk  Mon Mar 29 16:32:19 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 29 Mar 2004 15:32:19 +0100
Subject: [R] strange thing with sd
In-Reply-To: <s067e8b3.097@MAIL.NDRI.ORG>
References: <s067e8b3.097@MAIL.NDRI.ORG>
Message-ID: <40683373.1040600@lancaster.ac.uk>

Peter Flom wrote:
> I tried your sample code, and a few other variations, and got 0 for all
> of them, running R 1.8.1 on a Windows machine

  Not only do I get 0 on R 1.8.1 on Unix (linux, RH8), I get _exactly_ 
0. I thought this was going to be a simple arithmetic precision problem, 
but this test script shows the answers to be exactly zero:

zertest <- function(q){
   if( q != 0 ){
     theCall <- deparse(substitute(q))
     cat(paste("failed",theCall,' : got ',q,'\n',sep=' '))
   }
}

# these fail, as expected:
zertest(sd(c(1,2,3)))
zertest(sd(c(rep(0.01,12),0.00000001)))

# these pass as zero:
zertest(sd(rep(0.01,      13)))  #1.805557e-18
zertest(sd(rep(0.001,     13)))  #4.513894e-19
zertest(sd(rep(0.00001,   13)))  #0
zertest(sd(rep(0.00000001,13)))  #0

  Might be a compiler/config funny. Andreas: what 'UNIX' is this? Did 
you compile R yourself? Did you run 'make check'?

Baz



From andy_liaw at merck.com  Mon Mar 29 16:34:21 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Mar 2004 09:34:21 -0500
Subject: [R] data usage
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AB6@usrymx25.merck.com>

Is the ca. 100MB the size of the .dta file, or the size of the data when
loaded into Stata?  Or is there not a difference?  Have you checked the size
of the .rda file created as Doug had suggested?  I'd be curious to see what
that is...

Andy

> From: Edwin Leuven
> 
> > What does the 665 MB represent?  Did you try doing a garbage
> > collection after you had done the import?
> 
> i didn't (sorry, R beginner)
> 
> i followed your example and things look much better now, and
> object.size(x) returns:
> 
> 219,167,604
> 
> which is about double the size of the same object in stata where it 
> is:
> 
> 104,882,604
> 
> this leaves quite some room for improvement, but at least i can 
> now handle the data on my laptop...
> 
> thanks for your quick response! edwin
> 
> 
> > I would suggest
> > 
> > library("foreign")
> > x<-read.dta("mydata.dta")
> > gc()              # possibly repeat gc() to lower the thresholds
> > object.size(x)    # the actual storage (in bytes) allocated 
> to this object
> > save(x, file = "mydata.rda", compress = TRUE)
> > 
> > After that you can start a new session and use
> > 
> > load("mydata.rda")
> > 
> > to obtain a copy of the data set without the storage 
> overhead incurred
> > by the stata -> R conversion.
> > 
> > P.S. As described in the help page for object.size, the 
> returned value
> > is more properly described as an estimate of the object size because
> > sometimes it is difficult to determine the object size accurately.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From d.orme at imperial.ac.uk  Mon Mar 29 16:44:00 2004
From: d.orme at imperial.ac.uk (David Orme)
Date: Mon, 29 Mar 2004 15:44:00 +0100
Subject: [R] Confidence Intervals for slopes
Message-ID: <83FFF555-818F-11D8-BDA9-000393DC1748@ic.ac.uk>

Hi,

I'm trying to get confidence intervals to slopes from a linear model 
and I can't figure out how to get at them. As a cut 'n' paste example:

#################
# dummy dataset - regression data for 3 treatments, each treatment with 
different (normal) variance
x <- rep(1:10, length=30)
y <- 10 - (rep(c(0.2,0.5,0.8), each=10)*x)+c(rnorm(10, sd=0.1), 
rnorm(10, sd=0.6),rnorm(10, sd=1.1))
z <- gl(3,10)
plot(y~x, pch=unclass(z))

# model as three slopes with common intercept
options(contrasts=c("contr.treatment","contr.poly"))
model <- lm(y~x+x:z)

# coefficient table in summary gives the intercept, first slope and the 
difference in slopes
summary(model)

# confint gives the confidence interval for the intercept and first 
slope,
# and the CIs for the _differences_
confint(model)
#################

What I'd like to report are the actual CI's for the slopes for the 
second and third treatments, in the same way that confint returns the 
parameter estimates for the first treatment. Can anyone point me in the 
right direction?

Thanks,
David



From tlumley at u.washington.edu  Mon Mar 29 16:44:27 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 29 Mar 2004 06:44:27 -0800 (PST)
Subject: [R] data usage
In-Reply-To: <Pine.LNX.4.58.0403291548010.31342@e182104.fee.uva.nl>
References: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>
	<6rsmfryecx.fsf@bates4.stat.wisc.edu>
	<Pine.LNX.4.58.0403291548010.31342@e182104.fee.uva.nl>
Message-ID: <Pine.A41.4.58.0403290642540.66128@homer05.u.washington.edu>

On Mon, 29 Mar 2004, Edwin Leuven wrote:

> > What does the 665 MB represent?  Did you try doing a garbage
> > collection after you had done the import?
>
> i didn't (sorry, R beginner)
>
> i followed your example and things look much better now, and
> object.size(x) returns:
>
> 219,167,604
>
> which is about double the size of the same object in stata

Stata tends to store data as float, integer, or even byte where
appropriate for the precision.  This is one source of space saving. A
factor of 2 is not atypical.

	-thomas



From ligges at statistik.uni-dortmund.de  Mon Mar 29 16:46:46 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Mar 2004 16:46:46 +0200
Subject: [R] strange thing with sd
In-Reply-To: <1080568900.40682c4455c2e@www.cx.unibe.ch>
References: <1080568900.40682c4455c2e@www.cx.unibe.ch>
Message-ID: <406836D6.6000000@statistik.uni-dortmund.de>

Andreas Pauling wrote:

> Dear R people
> 
> I came across a strange thing: 
> 
> 
> sd(rep(0.01,      15))  #0
> sd(rep(0.001,     15))  #4.489023e-19
> sd(rep(0.00001,   15))  #0
> sd(rep(0.00000001,15))  #1.712427e-24
> 
> sd(rep(0.01,      13))  #1.805557e-18
> sd(rep(0.001,     13))  #4.513894e-19
> sd(rep(0.00001,   13))  #0
> sd(rep(0.00000001,13))  #0
> 
> sd(rep(5.01,	  15))  #0
> sd(rep(5.001,	  15))  #4.489023e-19
> sd(rep(5.00001,   15))  #1.838704e-15
> sd(rep(5.00000001,15))  #9.19352e-16
> 
> sd(rep(5.01,	  13))  #9.244454e-16
> sd(rep(5.001,	  13))  #9.244454e-16
> sd(rep(5.00001,   13))  #1.848891e-15
> sd(rep(5.00000001,13))  #0
> 
> Why gives sd sometimes zero and sometimes values close to zero
> and why does it depend on the value and on how many times it is
> repeated?
> Shouldn't it give always zero?
> Is there a way to control this?
> 
> I use R Version 1.8.1 under UNIX.
> 
> Thanks for any suggestions!!
> 
> Andreas
> 

I think this is due to expected inaccuracy of numerical floating point 
representation (and also happens for me under WinNT 4.0, R-1.9.0 beta).

And for the largest value you have listed
   all.equal(1.848891e-15, 0)
is still TRUE.

Uwe Ligges



From e.leuven at uva.nl  Mon Mar 29 16:50:51 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Mon, 29 Mar 2004 16:50:51 +0200 (CEST)
Subject: [R] data usage
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7AB6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7AB6@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.58.0403291639440.31342@e182104.fee.uva.nl>

On Mon, 29 Mar 2004, Liaw, Andy wrote:
> Is the ca. 100MB the size of the .dta file, or the size of the data
> when loaded into Stata?  Or is there not a difference? 

i think it is about the same. 

info on the processes using top to get an idea of memory use of R vs 
Stata using the same data:

COMMAND PRI  NI  SIZE  TRS  RSS SHARE STAT %CPU   TIME CPU 
R.bin   25    0  262M  760 262M  1880 S     0.0   0:28   0 
Stata   39   19  106M  620 106M  1192 S N   0.0   0:00   0 

> Have you
> checked the size of the .rda file created as Doug had suggested?  
> I'd be curious to see what that is...

it is 33,956,125

edwin



From pauling at giub.unibe.ch  Mon Mar 29 16:54:11 2004
From: pauling at giub.unibe.ch (Andreas Pauling)
Date: Mon, 29 Mar 2004 16:54:11 +0200
Subject: [R] strange thing with sd
In-Reply-To: <40683373.1040600@lancaster.ac.uk>
References: <s067e8b3.097@MAIL.NDRI.ORG> <40683373.1040600@lancaster.ac.uk>
Message-ID: <1080572051.40683893d52b0@www.cx.unibe.ch>

Zitat von Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>:

> Peter Flom wrote:
> > I tried your sample code, and a few other variations, and
> got 0 for all
> > of them, running R 1.8.1 on a Windows machine
> 
>   Not only do I get 0 on R 1.8.1 on Unix (linux, RH8), I get
> _exactly_ 
> 0. I thought this was going to be a simple arithmetic
> precision problem, 
> but this test script shows the answers to be exactly zero:
> 
> zertest <- function(q){
>    if( q != 0 ){
>      theCall <- deparse(substitute(q))
>      cat(paste("failed",theCall,' : got ',q,'\n',sep=' '))
>    }
> }
> 
> # these fail, as expected:
> zertest(sd(c(1,2,3)))
> zertest(sd(c(rep(0.01,12),0.00000001)))
> 
> # these pass as zero:
> zertest(sd(rep(0.01,      13)))  #1.805557e-18

This doesnt pass as zero:

> zertest(sd(rep(0.01,      13)))
failed sd(rep(0.01, 13))  : got  1.80555743920831e-18 

I use Unix-Version Sun OS 5.9. I didnt compile it myself. Could
compiler/config problems have caused this?

> zertest(sd(rep(0.001,     13)))  #4.513894e-19

This doesnt pass, either.

> zertest(sd(rep(0.00001,   13)))  #0
> zertest(sd(rep(0.00000001,13)))  #0
> 
>   Might be a compiler/config funny. Andreas: what 'UNIX' is
> this? Did 
> you compile R yourself? Did you run 'make check'?
> 
> Baz
> 
> 
>



From e.leuven at uva.nl  Mon Mar 29 16:59:07 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Mon, 29 Mar 2004 16:59:07 +0200 (CEST)
Subject: [R] data usage
In-Reply-To: <Pine.A41.4.58.0403290642540.66128@homer05.u.washington.edu>
References: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>
	<6rsmfryecx.fsf@bates4.stat.wisc.edu>
	<Pine.LNX.4.58.0403291548010.31342@e182104.fee.uva.nl>
	<Pine.A41.4.58.0403290642540.66128@homer05.u.washington.edu>
Message-ID: <Pine.LNX.4.58.0403291651550.31342@e182104.fee.uva.nl>

> Stata tends to store data as float, integer, or even byte where
> appropriate for the precision.  This is one source of space saving. A
> factor of 2 is not atypical.

i was suspecting something like this. 

what does R do? default to double always (or something like this)? 

is this a deliberate design choice made by the R people or just
convenience while not having to worry about datatypes?

edwin.



From HankeA at mar.dfo-mpo.gc.ca  Mon Mar 29 16:52:19 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Mon, 29 Mar 2004 10:52:19 -0400
Subject: [R] strange thing with sd
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124976@msgmarsta01.bio.dfo.ca>

Just so you don't feel that you are alone....
I get the same response as you use R1.8.0 on an XP operating system.
Alex

-----Original Message-----
From: Andreas Pauling [mailto:pauling at giub.unibe.ch] 
Sent: March 29, 2004 10:02 AM
To: r-help at stat.math.ethz.ch
Subject: [R] strange thing with sd


Dear R people

I came across a strange thing: 


sd(rep(0.01,      15))  #0
sd(rep(0.001,     15))  #4.489023e-19
sd(rep(0.00001,   15))  #0
sd(rep(0.00000001,15))  #1.712427e-24

sd(rep(0.01,      13))  #1.805557e-18
sd(rep(0.001,     13))  #4.513894e-19
sd(rep(0.00001,   13))  #0
sd(rep(0.00000001,13))  #0

sd(rep(5.01,	  15))  #0
sd(rep(5.001,	  15))  #4.489023e-19
sd(rep(5.00001,   15))  #1.838704e-15
sd(rep(5.00000001,15))  #9.19352e-16

sd(rep(5.01,	  13))  #9.244454e-16
sd(rep(5.001,	  13))  #9.244454e-16
sd(rep(5.00001,   13))  #1.848891e-15
sd(rep(5.00000001,13))  #0

Why gives sd sometimes zero and sometimes values close to zero
and why does it depend on the value and on how many times it is
repeated?
Shouldn't it give always zero?
Is there a way to control this?

I use R Version 1.8.1 under UNIX.

Thanks for any suggestions!!

Andreas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andrewr at uidaho.edu  Mon Mar 29 17:36:10 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 29 Mar 2004 07:36:10 -0800
Subject: [R] Confidence Intervals for slopes
In-Reply-To: <83FFF555-818F-11D8-BDA9-000393DC1748@ic.ac.uk>
References: <83FFF555-818F-11D8-BDA9-000393DC1748@ic.ac.uk>
Message-ID: <200403290736.10638.andrewr@uidaho.edu>

David,

try the estimable() function in the gregmisc package.

Andrew

On Monday 29 March 2004 06:44, David Orme wrote:
> Hi,
>
> I'm trying to get confidence intervals to slopes from a linear model
> and I can't figure out how to get at them. As a cut 'n' paste example:
>
> #################
> # dummy dataset - regression data for 3 treatments, each treatment with
> different (normal) variance
> x <- rep(1:10, length=30)
> y <- 10 - (rep(c(0.2,0.5,0.8), each=10)*x)+c(rnorm(10, sd=0.1),
> rnorm(10, sd=0.6),rnorm(10, sd=1.1))
> z <- gl(3,10)
> plot(y~x, pch=unclass(z))
>
> # model as three slopes with common intercept
> options(contrasts=c("contr.treatment","contr.poly"))
> model <- lm(y~x+x:z)
>
> # coefficient table in summary gives the intercept, first slope and the
> difference in slopes
> summary(model)
>
> # confint gives the confidence interval for the intercept and first
> slope,
> # and the CIs for the _differences_
> confint(model)
> #################
>
> What I'd like to report are the actual CI's for the slopes for the
> second and third treatments, in the same way that confint returns the
> parameter estimates for the first treatment. Can anyone point me in the
> right direction?
>
> Thanks,
> David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From kmw at mail.rockefeller.edu  Mon Mar 29 17:37:42 2004
From: kmw at mail.rockefeller.edu (Knut M. Wittkowski)
Date: Mon, 29 Mar 2004 10:37:42 -0500
Subject: [R] logo
In-Reply-To: <3EBA5559F490D61189430002A5F0AE8905632582@ntexcrd.braine.uc
 b>
Message-ID: <5.1.0.14.0.20040329102716.0312f258@imap.rockefeller.edu>

Dear Cecilia,

while Saghir and Kjetil may have a point, guidances are no laws. After all, 
it's your presentation and only you can decide what suits the specific 
purpose best.

An R logo can be found on http://www.r-project.org/foundation/.

Knut

At 09:17 2004-03-29 +0200, Bashir Saghir (Aztek Global) wrote:
>Dear Cecilia,
>
>Be careful not to cause a distraction with the R logo in the background of
>your presentation. You could put it in one of the corners if you feel that
>it should be prominently displayed.
>
>Incidentally you may be interested in some guidelines for presentations that
>I have made available at:
>
>      http://www.sbtc.ltd.uk/freenotes.html
>
>These were written with statisticians in mind but can be generalised to
>any presentation.
>
>I hope that they are useful.
>
>Best regards, Saghir
>
>-----Original Message-----
>From: Cec?lia Shiraiwa [mailto:ceciliashiraiwa at ig.com.br]
>Sent: Monday, March 29, 2004 7:04
>
>Dear all,
>
>I used R in my work and would like to put the logo of these program on the
>background of my presentation. But the logo that cames with the program is
>in low resolution. Does anyone have the R logo in high resolution?
>
>Thanks
>
>Cec?lia Shiraiwa



From fm3a004 at math.uni-hamburg.de  Mon Mar 29 18:07:37 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 29 Mar 2004 18:07:37 +0200 (MET DST)
Subject: [R] StepAIC
Message-ID: <Pine.GSO.3.95q.1040329175548.12165D-100000@sun11.math.uni-hamburg.de>

Dear list,

here is an example of stepAIC that I do not understand.
The data is n=42, Lage is the only factor and there are four other
variables treated as continuous.

First you see the stepAIC-forward solution (fs7). The strange thing here
is that apparently not all interactions are tried for inclusion, but only 
WQ:Lage. In particular, I think that WFL:Lage should be tried
in the last two steps, where WFL and Lage are already in the fit.
After fs7, I give the output of fs6 (backward), where all interactions are
tried as I have expected. (regsubsets works properly forward and
backward.)

Do I misunderstand something or is something strange going on in the
forward fit?

(I don't want to discuss here if the forward fit is a good thing to do
from a data analytic viewpoint. I agree that I should presumably not
choose it. However, I want to understand what the algorithm does.)

Thank you,
Christian

> w6 <- lm(Preis~RW1+WFL+WQ+VD+Lage+Lage*WFL+Lage*WQ+Lage*VD,
+                  data=wohnung)
> w7 <- lm(Preis~1,                 data=wohnung)

> fs7 <-
stepAIC(w7,scope=list(upper=~RW1+WFL+WQ+VD+Lage+Lage*WFL+Lage*WQ+Lage*VD,
+               lower=~1), direction="forward")
Start:  AIC= 623.57 
 Preis ~ 1 

       Df Sum of Sq       RSS       AIC
+ WQ    1  37219390  75101315       609
+ Lage  1  19029749  93290956       618
+ WFL   1  12506022  99814682       621
+ RW1   1   7299347 105021358       623
<none>              112320704       624
+ VD    1   5170556 107150149       624

Step:  AIC= 608.66 
 Preis ~ WQ 

       Df Sum of Sq      RSS      AIC
+ Lage  1   4736613 70364702      608
<none>              75101315      609
+ WFL   1   1863992 73237323      610
+ VD    1    555800 74545515      610
+ RW1   1    462284 74639030      610

Step:  AIC= 607.92 
 Preis ~ WQ + Lage 

          Df Sum of Sq      RSS      AIC
+ WFL      1   4721973 65642729      607
<none>                 70364702      608
+ WQ:Lage  1   2829768 67534934      608
+ RW1      1   2567408 67797294      608
+ VD       1    678458 69686244      610

Step:  AIC= 607.01 
 Preis ~ WQ + Lage + WFL 

          Df Sum of Sq      RSS      AIC
+ WQ:Lage  1   5610596 60032132      605
+ RW1      1   3404796 62237933      607
<none>                 65642729      607
+ VD       1    925528 64717201      608

Step:  AIC= 605.25 
 Preis ~ WQ + Lage + WFL + WQ:Lage 

       Df Sum of Sq      RSS      AIC
+ RW1   1   3492210 56539923      605
<none>              60032132      605
+ VD    1    355353 59676779      607

Step:  AIC= 604.74 
 Preis ~ WQ + Lage + WFL + RW1 + WQ:Lage 

       Df Sum of Sq      RSS      AIC
<none>              56539923      605
+ VD    1     94023 56445900      607


Backward fit:
> stepAIC(w6)
Start:  AIC= 596.53 
 Preis ~ RW1 + WFL + WQ + VD + Lage + Lage * WFL + Lage * WQ +  
    Lage * VD 

           Df Sum of Sq      RSS      AIC
- WQ:Lage   1    190953 40507327      595
- RW1       1    865788 41182162      595
<none>                  40316374      597
- WFL:Lage  1   6491181 46807556      601
- VD:Lage   1  12307855 52624230      606

Step:  AIC= 594.73 
 Preis ~ RW1 + WFL + WQ + VD + Lage + WFL:Lage + VD:Lage 

           Df Sum of Sq      RSS      AIC
- RW1       1    756790 41264117      594
- WQ        1   1910020 42417348      595
<none>                  40507327      595
- WFL:Lage  1  10302360 50809687      602
- VD:Lage   1  13222644 53729971      605

Step:  AIC= 593.51 
 Preis ~ WFL + WQ + VD + Lage + WFL:Lage + VD:Lage 

           Df Sum of Sq      RSS      AIC
- WQ        1   1793962 43058080      593
<none>                  41264117      594
- WFL:Lage  1  12069383 53333500      602
- VD:Lage   1  13657842 54921959      604

Step:  AIC= 593.3 
 Preis ~ WFL + VD + Lage + WFL:Lage + VD:Lage 

           Df Sum of Sq      RSS      AIC
<none>                  43058080      593
- WFL:Lage  1  14241342 57299422      603
- VD:Lage   1  19078878 62136957      607

Call:
lm(formula = Preis ~ WFL + VD + Lage + WFL:Lage + VD:Lage, data = wohnung)

Coefficients:
(Intercept)          WFL           VD        Lage2    WFL:Lage2
VD:Lage2  
  -53269.15        55.92      8025.62     59259.63       -46.71
-8233.36  



***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From jari.oksanen at oulu.fi  Mon Mar 29 17:04:17 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Mon, 29 Mar 2004 18:04:17 +0300
Subject: [R] calculate length of gradient ?
In-Reply-To: <200403291354.i2TDsmQ16837@mailgate5.cinetic.de>
References: <200403291354.i2TDsmQ16837@mailgate5.cinetic.de>
Message-ID: <5995073A-8192-11D8-9E68-000A95C76CA8@oulu.fi>


On 29 Mar 2004, at 16:54, Andreas Plank wrote:on=2.63
> my question is about ordination technics:
> To decide using a linear model response or a unimodal model response,
> in Canoco (software for ordination technics)
> you can calculate a so-called "length of gradient".
>
> (from Canoco Help file:)
>> The length of gradient is a measure of how unimodal the species
>> responses are along an ordination axis. It is the range of the
>> sample scores divided by the average within-species standard
>> deviation along the axis. The gradient length is expressed in
>> standard deviation units of species turnover (SD).
>
> The function  cca in package vegan and other functions give many
> outputs, but I didn't find anything about "length of gradient". Is
> there a possibility to calculate this in R? If yes how can I
> calculate this?

You cannot get this scaling in the cca function. However, function 
decorana() gives you "axis lengths". Cajo ter Braak suggested a linear 
approximation to Mark Hill's "sd scaling" that would be easy to 
implement in cca (actually, in summary.cca(), since no scaling is done 
until your request one), but I haven't bothered to do that. By the way, 
I disagree with the recommendation: CA is in general better than PCA, 
even with "short gradients" so  you don't need axis lengths.

Please read the posting guide (about messages that should be sent to 
the list vs. to the package authors).

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From oleg at sai.msu.su  Mon Mar 29 19:03:41 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Mon, 29 Mar 2004 21:03:41 +0400 (MSD)
Subject: [R] using hist breaks as factor ?
Message-ID: <Pine.GSO.4.58.0403292058220.5513@ra.sai.msu.su>

I have numeric vector z and I want to factorize it using z$breaks
which I got from histograms breaks. Is there an elegant way to do this ?
I, probably, could write a loop and check if z hits into some interval
and replace z with value of z$mids, but I suspect there is more
R-ish way.

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From George_Heine at blm.gov  Mon Mar 29 19:44:02 2004
From: George_Heine at blm.gov (George_Heine@blm.gov)
Date: Mon, 29 Mar 2004 10:44:02 -0700
Subject: [R] How to purchase R on CD
Message-ID: <OF58A5FC51.33F12832-ON87256E66.005EDB5E-87256E66.00616B4B@blm.gov>





My organization has a policy against "installing software downloaded from
the Internet."  There is a waiver procedure, but it is difficult and
lengthy.  It is much easier to get approval to purchase software, even if
it is a CD copy of open-source.

However, a search on r-project.org didn't seem to show any way of obtaining
the distribution in this way.

Any clues would be appreciated!
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
George Heine, PhD
Mathematical Analyst
National IRM Center
U.S. Bureau of Land Management
voice   (303) 236-0099
fax       (303) 236-1974
cell      (303) 905-5382
pager   gheine at my2way.com
<>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>



From ggrothendieck at myway.com  Mon Mar 29 19:49:34 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 29 Mar 2004 17:49:34 +0000 (UTC)
Subject: [R] using hist breaks as factor ?
References: <Pine.GSO.4.58.0403292058220.5513@ra.sai.msu.su>
Message-ID: <loom.20040329T194029-660@post.gmane.org>

Oleg Bartunov <oleg <at> sai.msu.su> writes:
> I have numeric vector z and I want to factorize it using z$breaks

Have a look 
at:

?cut



From andy_liaw at merck.com  Mon Mar 29 19:52:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Mar 2004 12:52:20 -0500
Subject: [R] How to purchase R on CD
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7ABB@usrymx25.merck.com>

If you can use Linux, you can buy a Quantian CD from cheapbyte.com for
$4.99, I believe.  Quantian is a `live' CD:  You pop it in the CD drive,
boot up the computer, and voila, it runs Debian Linux right off the CD,
without touch the hard drive.  Quantian has R and a whole suite of other
quantitative analysis software on it.  You can find info at
http://dirk.eddelbuettel.com/quantian.html.  (Thanks, Dirk!)

Andy

> From: George_Heine at blm.gov
> 
> My organization has a policy against "installing software 
> downloaded from
> the Internet."  There is a waiver procedure, but it is difficult and
> lengthy.  It is much easier to get approval to purchase 
> software, even if
> it is a CD copy of open-source.
> 
> However, a search on r-project.org didn't seem to show any 
> way of obtaining
> the distribution in this way.
> 
> Any clues would be appreciated!
> <>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
> George Heine, PhD
> Mathematical Analyst
> National IRM Center
> U.S. Bureau of Land Management
> voice   (303) 236-0099
> fax       (303) 236-1974
> cell      (303) 905-5382
> pager   gheine at my2way.com
> <>=<>=<>=<>=<>=<>=<>=<>=<>=<>=<>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ccleland at optonline.net  Mon Mar 29 19:55:39 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 29 Mar 2004 12:55:39 -0500
Subject: [R] How to purchase R on CD
In-Reply-To: <OF58A5FC51.33F12832-ON87256E66.005EDB5E-87256E66.00616B4B@blm.gov>
References: <OF58A5FC51.33F12832-ON87256E66.005EDB5E-87256E66.00616B4B@blm.gov>
Message-ID: <4068631B.1050603@optonline.net>

George_Heine at blm.gov wrote:
> My organization has a policy against "installing software downloaded from
> the Internet."  There is a waiver procedure, but it is difficult and
> lengthy.  It is much easier to get approval to purchase software, even if
> it is a CD copy of open-source.
> 
> However, a search on r-project.org didn't seem to show any way of obtaining
> the distribution in this way.
> 
> Any clues would be appreciated!

George:
   I suppose one way is to purchase the Quantian Linux 
distribution from Cheapbytes.

http://cart.cheapbytes.com/cgi-bin/cart/0070011023.html

http://dirk.eddelbuettel.com/quantian.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ggrothendieck at myway.com  Mon Mar 29 19:52:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 29 Mar 2004 17:52:43 +0000 (UTC)
Subject: [R] How to purchase R on CD
References: <OF58A5FC51.33F12832-ON87256E66.005EDB5E-87256E66.00616B4B@blm.gov>
Message-ID: <loom.20040329T195133-293@post.gmane.org>

 <George_Heine <at> blm.gov> writes:
> My organization has a policy against "installing software downloaded from
> the Internet."  

Check out:

http://dirk.eddelbuettel.com/quantian.html



From B.Rowlingson at lancaster.ac.uk  Mon Mar 29 20:08:08 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 29 Mar 2004 19:08:08 +0100
Subject: [R] How to purchase R on CD
In-Reply-To: <OF58A5FC51.33F12832-ON87256E66.005EDB5E-87256E66.00616B4B@blm.gov>
References: <OF58A5FC51.33F12832-ON87256E66.005EDB5E-87256E66.00616B4B@blm.gov>
Message-ID: <40686608.9070209@lancaster.ac.uk>

George_Heine at blm.gov wrote:

> My organization has a policy against "installing software downloaded from
> the Internet."  There is a waiver procedure, but it is difficult and
> lengthy.  It is much easier to get approval to purchase software, even if
> it is a CD copy of open-source.
> 
> However, a search on r-project.org didn't seem to show any way of obtaining
> the distribution in this way.

If you don't want a Linux version, and you dont want a Mac version, but 
you do want a Windows version, GNU-Win sell CDs with masses of goodies 
on, including R (version 1.8.0 at the moment though)

http://gnuwin.epfl.ch/apps/en/bestlist.html

Just dont waste all your time playing Tux Racer.

  A whole $3.95 from osdepot.com.

Baz



From bates at stat.wisc.edu  Mon Mar 29 20:10:53 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 29 Mar 2004 12:10:53 -0600
Subject: [R] strange thing with sd
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE124976@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE124976@msgmarsta01.bio.dfo.ca>
Message-ID: <6r65cnr0b6.fsf@bates4.stat.wisc.edu>

> sd(rep(0.01,      15))  #0
> sd(rep(0.001,     15))  #4.489023e-19
> sd(rep(0.00001,   15))  #0
> sd(rep(0.00000001,15))  #1.712427e-24
> 
> sd(rep(0.01,      13))  #1.805557e-18
> sd(rep(0.001,     13))  #4.513894e-19
> sd(rep(0.00001,   13))  #0
> sd(rep(0.00000001,13))  #0
> 
> sd(rep(5.01,	  15))  #0
> sd(rep(5.001,	  15))  #4.489023e-19
> sd(rep(5.00001,   15))  #1.838704e-15
> sd(rep(5.00000001,15))  #9.19352e-16
> 
> sd(rep(5.01,	  13))  #9.244454e-16
> sd(rep(5.001,	  13))  #9.244454e-16
> sd(rep(5.00001,   13))  #1.848891e-15
> sd(rep(5.00000001,13))  #0

Before we get too carried away with this thread could you all please
consider how the sd function calculates its result?  Doing so makes it
a lot easier to decide why the result will be zero in some cases and a
number very close to zero in other cases.  [Sorry if I sound testy but
one of the big advantages of Open Source software is that you don't
need to speculate on how it arrives at a result, you can actually
check.]

I'll tell you, it takes the square root of the variance.  How is the
variance calculated for a numeric vector?  First you calculate the
mean *using floating point arithmetic* in which it is not necessarily
true that N * k / N == k, or, as it is done in this case,
  [k + k + ... + k]/N == k
where there are N terms in the sum.

There can be round-off error in floating point calculations, which is
why you shouldn't expect exact answers.

Development versions of R are subjected to extensive tests every day,
including tests on the numerical accuracy.  Most of those tests end in
a check using the all.equal function which checks if the relative
difference is less than a threshold.  That's about the best that you
can do with floating point arithmetic.

Here endeth the sermon.



From lmassis at yahoo.com.br  Mon Mar 29 20:13:53 2004
From: lmassis at yahoo.com.br (Leonard Assis)
Date: Mon, 29 Mar 2004 15:13:53 -0300
Subject: [R] How to purchase R on CD
In-Reply-To: <40686608.9070209@lancaster.ac.uk>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA5j8071toSkezRUEHhdMv0cKAAAAQAAAAjzWQmn2Ga06FltjHTBk3bgEAAAAA@yahoo.com.br>

Why Dont U Download and Burn into a CD at your Home?

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Barry Rowlingson
Sent: Monday, March 29, 2004 3:08 PM
To: George_Heine at blm.gov
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to purchase R on CD

George_Heine at blm.gov wrote:

> My organization has a policy against "installing software downloaded
from
> the Internet."  There is a waiver procedure, but it is difficult and
> lengthy.  It is much easier to get approval to purchase software, even
if
> it is a CD copy of open-source.
> 
> However, a search on r-project.org didn't seem to show any way of
obtaining
> the distribution in this way.

If you don't want a Linux version, and you dont want a Mac version, but 
you do want a Windows version, GNU-Win sell CDs with masses of goodies 
on, including R (version 1.8.0 at the moment though)

http://gnuwin.epfl.ch/apps/en/bestlist.html

Just dont waste all your time playing Tux Racer.

  A whole $3.95 from osdepot.com.

Baz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From heron at stanford.edu  Mon Mar 29 20:29:31 2004
From: heron at stanford.edu (Christian Reilly)
Date: Mon, 29 Mar 2004 10:29:31 -0800 (PST)
Subject: [R] gregmisc: install trouble
In-Reply-To: <D7A3CFD7825BD6119B880002A58F06C20680B18D@groexmb02.pfizer.com>
Message-ID: <Pine.GSO.4.44.0403291022450.23816-100000@elaine42.Stanford.EDU>


Thnak you all,

I was running R-base-care 1.5.01 (the debian site default). I
replaced this (at Uwe's suggestion) with 1.8.1 from the CRAN site and
gregmisc installs and runs cleanly.

Thanks again,

Christian





On Mon, 29 Mar 2004, Warnes, Gregory R wrote:

> Date: Mon, 29 Mar 2004 08:59:51 -0500
> From: "Warnes, Gregory R" <gregory_r_warnes at groton.pfizer.com>
> To: 'Uwe Ligges' <ligges at statistik.uni-dortmund.de>,
>      Christian Reilly <heron at stanford.edu>
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] gregmisc: install trouble
>
> Further, there is a new version of gregmisc, 0.10.0, on CRAN now for use
> with R 1.9.0.
>
> -Greg
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Uwe Ligges
> > Sent: Saturday, March 27, 2004 6:30 AM
> > To: Christian Reilly
> > Cc: R-help at stat.math.ethz.ch
> > Subject: Re: [R] gregmisc: install trouble
> >
> >
> > Christian Reilly wrote:
> > >
> > > Hello all,
> > >
> > > I'm having trouble installing the package 'gregmisc'. I've run the
> > > command:
> > >
> > > > R CMD INSTALL -l /usr/lib/R/library /tmp/gregmisc_0.9.0.tar.gz
> > >
> > > And I can see it shows up on the list of available packeage
> > when I issue
> > > > library()
> > >
> > > But when I do
> > >
> > > >library(gregmisc)
> > >
> > > I get:
> > >
> > > Error in parse(file, n, text, prompt) : syntax error on line 3105
> > >
> > > All the other packages I've install load fine. Permissions for the
> > > 'gregmisc' library are the same as all the rest.
> > >
> > > Any Suggestions?
> > >
> > > I'm running on Debian Linux(Woody)
> > >
> > > Thanks much,
> > >
> > > Christian Reilly
> > >
> > > ---------------
> > > Hopkins Marine Station
> > > Stanford U
> > > Pacific Grove, CA
> >
> >
> > Which version of R is this? Works fine with R-1.8.1 and R-1.9.0 beta.
> > Please download the package again and reinstall.
> >
> > Uwe Ligges
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>
> LEGAL NOTICE
> Unless expressly stated otherwise, this message is confidential and may be privileged. It is intended for the addressee(s) only. Access to this E-mail by anyone else is unauthorized. If you are not an addressee, any disclosure or copying of the contents of this E-mail or any action taken (or not taken) in reliance on it is unauthorized and may be unlawful. If you are not an addressee, please inform the sender immediately.
>



From zcs2 at columbia.edu  Mon Mar 29 20:54:08 2004
From: zcs2 at columbia.edu (Zachary C Shirkey)
Date: Mon, 29 Mar 2004 13:54:08 -0500 (EST)
Subject: [R] adjusted R squared
Message-ID: <Pine.GSO.4.58.0403291352340.6012@persimmon.cc.columbia.edu>

	I have run several glm models and would like to obtain the
adjusted R squared for the model.  What is the appropriate command for
obtaining an adjusted R squared?



From itayf at fhcrc.org  Mon Mar 29 20:59:07 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Mon, 29 Mar 2004 10:59:07 -0800 (PST)
Subject: Summary + Thx: [R] How to add a top X-axis with a different
	logarithmic scale?
In-Reply-To: <1080310738.7003.331.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0403291048130.22268-100000@cezanne.fhcrc.org>


Thanks to Marc Schwartz and Jim Lemon for their solutions, and 
the very useful document by JL.

This is a summary in case it is useful to others.

My problem was to provide, for the same data series, two 
different X ordinates ('axes' was probably a misnomer here).
In this case, both ordinates are logarithmic and differ only by 
scale; the solution should work with more complicated 
transformations.


################################################################
####
####    Solution based on Jim Lemon's "Kickstarting R",
####        section "Plotting more than one data series".
####    http://cran.r-project.org
####        (under "Contributed Documentation").

####    Here, the top axis is produced as a side-effect of
####    a plot of the transformed data series, while hiding the
####    actual points.

x <- c(1.1 * 1:4, 25 * 1:5) / 50e+03
y <- c(0.15 * 1:4, 0.6 + 0.05 * 1:5)

old.par <- par(no.readonly=TRUE)
xlim <- range(x)
ylim <- c(0, 1)

plot(x, y, type="l", log="x", xlim=xlim, ylim=ylim,
     xlab="Fraction", ylab="IC")
par(new=T)
## replot the transformed data series (x -> x*4.1e06). We must
## specify 'log="x"' and 'ylim=ylim' to have the correct range;
## 'axes=F' suppresses plotting the axes at bottom and left; 
## 'lty=0' hides the data points.
plot(x*4.1e06, y, type="l", log="x", ylim=ylim, axes=F, lty=0,
     xlab="", ylab="")
## Now, really plot the top X-axis.
## plot() command.
axis(3)
mtext("Total", side=3, line=2)

par(old.par)
################################################################


################################################################
####    Marc Schwartz's solution works 'as-is', so I merely
####    reproduce it here for completeness.

x <- c(1.1 * 1:4, 25 * 1:5) / 50e+03
y <- c(0.15 * 1:4, 0.6 + 0.05 * 1:5)

old.par <- par(no.readonly=TRUE)
xlim <- range(x)
ylim <- c(0, 1)

plot(x, y, type = "l", log = "x", xlim = xlim, ylim = ylim)

# Set scaling factor
sf <- 4.1e06

# Now set up new upper x axis range based upon sf
xlim.3 <- xlim * sf

# Now create axis tick mark positions for the new scale
# We will use axTicks() and need to consider that when log scales
# are in use, things get a little hairy. So:

# We need to define a new par("xaxp") based upon xlim.3
# We also need to define a new par("usr")[1:2], which is the
# range of the x axis. When log scales are in use, the actual
# range is 10 ^ par("usr")[1:2], so we take these values and rescale
# using 'sf'. We then convert the result back to log scales using
# log10() to get 'usr':
tm <- axTicks(3, axp = c(range(xlim.3), 3),
              usr = log10(10 ^ par("usr")[1:2] * sf),
              log = TRUE)

# Now we can draw axis 3, using the adjusted scaling
# We need to adjust the scaled tick mark positions back
# to the actual range of the x axis, so 'at' is adjusted
# here by the sf, but the labels stay as rescaled:
axis(3, at = tm / sf, labels = tm)

par(old.par)
################################################################


	Itay



From bxc at steno.dk  Mon Mar 29 21:04:15 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 29 Mar 2004 21:04:15 +0200
Subject: [R] Confidence Intervals for slopes
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E3AF@exdkba022.novo.dk>

You may want:

lm( y ~ x:z )

This is the same model you fitted, but prametrized differently.
But please check that what you REALLY want is not
  
lm( y ~ z + x:z )

This is the model with different intercepts as well.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------





> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Orme
> Sent: Monday, March 29, 2004 4:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Confidence Intervals for slopes
> 
> 
> Hi,
> 
> I'm trying to get confidence intervals to slopes from a linear model 
> and I can't figure out how to get at them. As a cut 'n' paste example:
> 
> #################
> # dummy dataset - regression data for 3 treatments, each 
> treatment with 
> different (normal) variance
> x <- rep(1:10, length=30)
> y <- 10 - (rep(c(0.2,0.5,0.8), each=10)*x)+c(rnorm(10, sd=0.1), 
> rnorm(10, sd=0.6),rnorm(10, sd=1.1))
> z <- gl(3,10)
> plot(y~x, pch=unclass(z))
> 
> # model as three slopes with common intercept
> options(contrasts=c("contr.treatment","contr.poly"))
> model <- lm(y~x+x:z)
> 
> # coefficient table in summary gives the intercept, first 
> slope and the 
> difference in slopes
> summary(model)
> 
> # confint gives the confidence interval for the intercept and first 
> slope,
> # and the CIs for the _differences_
> confint(model)
> #################
> 
> What I'd like to report are the actual CI's for the slopes for the 
> second and third treatments, in the same way that confint returns the 
> parameter estimates for the first treatment. Can anyone point 
> me in the 
> right direction?
> 
> Thanks,
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sbartel at sph.emory.edu  Mon Mar 29 21:16:26 2004
From: sbartel at sph.emory.edu (Scott Bartell)
Date: 29 Mar 2004 14:16:26 -0500
Subject: [R] dev.print
In-Reply-To: <Pine.LNX.4.44.0403270827400.11650-100000@gannet.stats>
References: <Pine.LNX.4.44.0403270827400.11650-100000@gannet.stats>
Message-ID: <1080587785.11824.207.camel@sbartel1>

Thanks for the explanation.  I actually had read the help page for
dev.print after finding this statement in the Defunct help page (while
searching for savePlot):

     The new function `dev.print()' should now be used for saving plots
     to a file or printing them.

Personally I didn't find the documentation very helpful for using
dev.print to save plots.  "The plot dimensions will be taken from the
current device" suggested to me that the saved image would be the same
size as the screen image.

Scott

On Sat, 2004-03-27 at 03:31, Prof Brian Ripley wrote:
> dev.print is designed for *print* devices which compute sizes in inches.
> The help says
> 
>      'dev.print' is most useful for producing a postscript print (its
>      default) when the following applies.  Unless 'file' is specified,
>      the plot will be printed.  Unless 'width', 'height' and
>      'pointsize' are specified the plot dimensions will be taken from
>      the current device, shrunk if necessary to fit on the paper.
>      ...
> 
> So you asked for a plot of about 7 x 7, and jpeg expected points.  Moral: 
> read the help page and specify width and height if it is not what you 
> expected.



From oleg at sai.msu.su  Mon Mar 29 21:48:55 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Mon, 29 Mar 2004 23:48:55 +0400 (MSD)
Subject: [R] using hist breaks as factor ?
In-Reply-To: <loom.20040329T194029-660@post.gmane.org>
References: <Pine.GSO.4.58.0403292058220.5513@ra.sai.msu.su>
	<loom.20040329T194029-660@post.gmane.org>
Message-ID: <Pine.GSO.4.58.0403292348280.5513@ra.sai.msu.su>

On Mon, 29 Mar 2004, Gabor Grothendieck wrote:

> Oleg Bartunov <oleg <at> sai.msu.su> writes:
> > I have numeric vector z and I want to factorize it using z$breaks
>
> Have a look
> at:
>
> ?cut

Thanks for pointers, this is what I needed.


>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From oleg at sai.msu.su  Mon Mar 29 22:06:39 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Tue, 30 Mar 2004 00:06:39 +0400 (MSD)
Subject: [R] cut and factor
In-Reply-To: <200403291819.i2TIJPx23588@gator.dt.uh.edu>
References: <200403291819.i2TIJPx23588@gator.dt.uh.edu>
Message-ID: <Pine.GSO.4.58.0403292358130.5513@ra.sai.msu.su>

Eric,

thanks for quick reply. at first look I thought it is what I need,
but, unfortunately, it doesn't applied to original data - it
creates new data with loosing original indexes ! I want to keep indexes
of original data, but replace original data with $mids of corresponding
$breaks.

So, if I have z = 1:10, t=hist(z,plot=F)
> z
 [1]  1  2  3  4  5  6  7  8  9 10
> t$breaks
[1]  0  2  4  6  8 10
> t$mids
[1] 1 3 5 7 9

I want z=c(1,1,3,3,5,5,7,7,9,9)


	Oleg

On Mon, 29 Mar 2004, Erin Hodgess wrote:

> Hello Oleg!
>
> Do you mean something like this, please?
> > z <- rnorm(15)
> > z
>  [1] -0.36888946  0.34271755 -0.47761843 -0.58402557  0.05393014  0.69234710
> 2.04861420  1.41823938 -0.57638598  1.51090023
> [11] -0.71616401  0.19227347 -0.19348506 -0.63082326 -0.64346621
> > z1 <- hist(z,plot=F)
> > z1$breaks
> [1] -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5
> > table(cut(z,z1$breaks))
>
> (-1,-0.5]  (-0.5,0]   (0,0.5]   (0.5,1]   (1,1.5]   (1.5,2]   (2,2.5]
>         5         3         3         1         1         1         1
>
> Hope this helps!
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
>
>
> From: Oleg Bartunov <oleg at sai.msu.su>
>
>
> I have numeric vector z and I want to factorize it using z$breaks
> which I got from histograms breaks. Is there an elegant way to do this ?
> I, probably, could write a loop and check if z hits into some interval
> and replace z with value of z$mids, but I suspect there is more
> R-ish way.
>
> 	Regards,
> 		Oleg
> _____________________________________________________________
> Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> Sternberg Astronomical Institute, Moscow University (Russia)
> Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> phone: +007(095)939-16-83, +007(095)939-23-83
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From ggrothendieck at myway.com  Mon Mar 29 22:16:39 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 29 Mar 2004 20:16:39 +0000 (UTC)
Subject: [R] cut and factor
References: <200403291819.i2TIJPx23588@gator.dt.uh.edu>
	<Pine.GSO.4.58.0403292358130.5513@ra.sai.msu.su>
Message-ID: <loom.20040329T221414-775@post.gmane.org>

Oleg Bartunov <oleg <at> sai.msu.su> writes:

> I want to keep indexes
> of original data, but replace original data with $mids of corresponding
> $breaks.


Note the label= arg on cut:

cut(z,t$breaks,lab=t$mids)



From Achim.Zeileis at wu-wien.ac.at  Mon Mar 29 22:18:46 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 29 Mar 2004 22:18:46 +0200
Subject: [R] cut and factor
In-Reply-To: <Pine.GSO.4.58.0403292358130.5513@ra.sai.msu.su>
References: <200403291819.i2TIJPx23588@gator.dt.uh.edu>
	<Pine.GSO.4.58.0403292358130.5513@ra.sai.msu.su>
Message-ID: <20040329221846.0d07ef13.Achim.Zeileis@wu-wien.ac.at>

On Tue, 30 Mar 2004 00:06:39 +0400 (MSD) Oleg Bartunov wrote:

> Eric,
> 
> thanks for quick reply. at first look I thought it is what I need,
> but, unfortunately, it doesn't applied to original data - it
> creates new data with loosing original indexes ! I want to keep
> indexes of original data, but replace original data with $mids of
> corresponding$breaks.

Then you just don't say table() after using cut().
 
> So, if I have z = 1:10, t=hist(z,plot=F)
> > z
>  [1]  1  2  3  4  5  6  7  8  9 10
> > t$breaks
> [1]  0  2  4  6  8 10
> > t$mids
> [1] 1 3 5 7 9
> 
> I want z=c(1,1,3,3,5,5,7,7,9,9)

and you get it via

R> z <- 1:10
R> ht <- hist(z,plot=F)
R> z
 [1]  1  2  3  4  5  6  7  8  9 10
R> ht$breaks
[1]  0  2  4  6  8 10
R> ht$mids
[1] 1 3 5 7 9
R> z2 <- cut(z, ht$breaks, labels = ht$mids)
R> z2
 [1] 1 1 3 3 5 5 7 7 9 9
Levels: 1 3 5 7 9

or if you don't want to have a factor

R> as.numeric(as.character(z2))
 [1] 1 1 3 3 5 5 7 7 9 9

best,
Z

> 
> 	Oleg
> 
> On Mon, 29 Mar 2004, Erin Hodgess wrote:
> 
> > Hello Oleg!
> >
> > Do you mean something like this, please?
> > > z <- rnorm(15)
> > > z
> >  [1] -0.36888946  0.34271755 -0.47761843 -0.58402557  0.05393014 
> >  0.69234710
> > 2.04861420  1.41823938 -0.57638598  1.51090023
> > [11] -0.71616401  0.19227347 -0.19348506 -0.63082326 -0.64346621
> > > z1 <- hist(z,plot=F)
> > > z1$breaks
> > [1] -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5
> > > table(cut(z,z1$breaks))
> >
> > (-1,-0.5]  (-0.5,0]   (0,0.5]   (0.5,1]   (1,1.5]   (1.5,2]  
> > (2,2.5]
> >         5         3         3         1         1         1        
> >         1
> >
> > Hope this helps!
> > Sincerely,
> > Erin Hodgess
> > Associate Professor
> > Department of Computer and Mathematical Sciences
> > University of Houston - Downtown
> > mailto: hodgess at gator.uhd.edu
> >
> >
> >
> > From: Oleg Bartunov <oleg at sai.msu.su>
> >
> >
> > I have numeric vector z and I want to factorize it using z$breaks
> > which I got from histograms breaks. Is there an elegant way to do
> > this ? I, probably, could write a loop and check if z hits into some
> > interval and replace z with value of z$mids, but I suspect there is
> > more R-ish way.
> >
> > 	Regards,
> > 		Oleg
> > _____________________________________________________________
> > Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> > Sternberg Astronomical Institute, Moscow University (Russia)
> > Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> > phone: +007(095)939-16-83, +007(095)939-23-83
> >
> 
> 	Regards,
> 		Oleg
> _____________________________________________________________
> Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> Sternberg Astronomical Institute, Moscow University (Russia)
> Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> phone: +007(095)939-16-83, +007(095)939-23-83
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From solares at unsl.edu.ar  Mon Mar 29 22:22:14 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Mon, 29 Mar 2004 17:22:14 -0300 (ART)
Subject: [R] how transform excell file to text file in R
Message-ID: <59395.170.210.173.216.1080591734.squirrel@inter17.unsl.edu.ar>

Hello, it want to know if there is a library that transform a file in
excell (*. xls) to text file(*.txt, *,prn, *.csv).  Thanks



From oleg at sai.msu.su  Mon Mar 29 22:24:51 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Tue, 30 Mar 2004 00:24:51 +0400 (MSD)
Subject: [R] cut and factor
In-Reply-To: <20040329221846.0d07ef13.Achim.Zeileis@wu-wien.ac.at>
References: <200403291819.i2TIJPx23588@gator.dt.uh.edu>
	<Pine.GSO.4.58.0403292358130.5513@ra.sai.msu.su>
	<20040329221846.0d07ef13.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.GSO.4.58.0403300024290.5513@ra.sai.msu.su>

On Mon, 29 Mar 2004, Achim Zeileis wrote:

> On Tue, 30 Mar 2004 00:06:39 +0400 (MSD) Oleg Bartunov wrote:
>
> > Eric,
> >
> > thanks for quick reply. at first look I thought it is what I need,
> > but, unfortunately, it doesn't applied to original data - it
> > creates new data with loosing original indexes ! I want to keep
> > indexes of original data, but replace original data with $mids of
> > corresponding$breaks.
>
> Then you just don't say table() after using cut().

Aha ! Thanks a lot.



>
> > So, if I have z = 1:10, t=hist(z,plot=F)
> > > z
> >  [1]  1  2  3  4  5  6  7  8  9 10
> > > t$breaks
> > [1]  0  2  4  6  8 10
> > > t$mids
> > [1] 1 3 5 7 9
> >
> > I want z=c(1,1,3,3,5,5,7,7,9,9)
>
> and you get it via
>
> R> z <- 1:10
> R> ht <- hist(z,plot=F)
> R> z
>  [1]  1  2  3  4  5  6  7  8  9 10
> R> ht$breaks
> [1]  0  2  4  6  8 10
> R> ht$mids
> [1] 1 3 5 7 9
> R> z2 <- cut(z, ht$breaks, labels = ht$mids)
> R> z2
>  [1] 1 1 3 3 5 5 7 7 9 9
> Levels: 1 3 5 7 9
>
> or if you don't want to have a factor
>
> R> as.numeric(as.character(z2))
>  [1] 1 1 3 3 5 5 7 7 9 9
>
> best,
> Z
>
> >
> > 	Oleg
> >
> > On Mon, 29 Mar 2004, Erin Hodgess wrote:
> >
> > > Hello Oleg!
> > >
> > > Do you mean something like this, please?
> > > > z <- rnorm(15)
> > > > z
> > >  [1] -0.36888946  0.34271755 -0.47761843 -0.58402557  0.05393014
> > >  0.69234710
> > > 2.04861420  1.41823938 -0.57638598  1.51090023
> > > [11] -0.71616401  0.19227347 -0.19348506 -0.63082326 -0.64346621
> > > > z1 <- hist(z,plot=F)
> > > > z1$breaks
> > > [1] -1.0 -0.5  0.0  0.5  1.0  1.5  2.0  2.5
> > > > table(cut(z,z1$breaks))
> > >
> > > (-1,-0.5]  (-0.5,0]   (0,0.5]   (0.5,1]   (1,1.5]   (1.5,2]
> > > (2,2.5]
> > >         5         3         3         1         1         1
> > >         1
> > >
> > > Hope this helps!
> > > Sincerely,
> > > Erin Hodgess
> > > Associate Professor
> > > Department of Computer and Mathematical Sciences
> > > University of Houston - Downtown
> > > mailto: hodgess at gator.uhd.edu
> > >
> > >
> > >
> > > From: Oleg Bartunov <oleg at sai.msu.su>
> > >
> > >
> > > I have numeric vector z and I want to factorize it using z$breaks
> > > which I got from histograms breaks. Is there an elegant way to do
> > > this ? I, probably, could write a loop and check if z hits into some
> > > interval and replace z with value of z$mids, but I suspect there is
> > > more R-ish way.
> > >
> > > 	Regards,
> > > 		Oleg
> > > _____________________________________________________________
> > > Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> > > Sternberg Astronomical Institute, Moscow University (Russia)
> > > Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> > > phone: +007(095)939-16-83, +007(095)939-23-83
> > >
> >
> > 	Regards,
> > 		Oleg
> > _____________________________________________________________
> > Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> > Sternberg Astronomical Institute, Moscow University (Russia)
> > Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> > phone: +007(095)939-16-83, +007(095)939-23-83
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From mfowle at navicominc.com  Mon Mar 29 22:36:11 2004
From: mfowle at navicominc.com (Mark Fowle)
Date: Mon, 29 Mar 2004 15:36:11 -0500
Subject: [R] how transform excell file to text file in R
Message-ID: <00B717603414D21187AD00104B94A2DAB23968@EXCHANGE>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040329/91fd47d5/attachment.pl

From clint at ecy.wa.gov  Tue Mar 30 00:06:36 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Mon, 29 Mar 2004 14:06:36 -0800 (PST)
Subject: [R] Interesting Behavior in plot()
Message-ID: <Pine.LNX.4.44.0403291308340.19413-100002@aeolus.ecy.wa.gov>

I have a 2 by 226200 table, conveniently read in by read.table(), which
exhibits some strange behavior when plotted by plot(V1,V2).  The general
pattern for the range of windspeeds, [0<V1<50] is as expected -- the wind
gust falls in the interval [V1<V2<65] except for certain values of V2.  
For V2 == c(15,26,37,48,59), the V2 values are positioned at one-tenth of 
the V1 (i.e., as if I had issued the command plot(0.1*V1,V2) for just 
those values of V2 -- see attached plot, test.png).  The only other values 
of V2 in the range, 4 and 70, also seem to be affected but aren't well 
enough represented to show up clearly on the plot.

However, if I plot(ws$V1[ws$V2==48],ws$V2[ws$V2==48]), I see the second 
attachment, test2.png, which confirms that the wind speed (V1) really 
should be positioned where one would expect.  I haven't seen any messages 
covering this behaviour and so am looking for an explanation.

I'm running:

> R.Version()
$platform
[1] "i686-pc-linux-gnu"

$arch
[1] "i686"

$os
[1] "linux-gnu"

$system
[1] "i686, linux-gnu"

$status
[1] ""

$major
[1] "1"

$minor
[1] "8.1"

$year
[1] "2003"

$month
[1] "11"

$day
[1] "21"

$language
[1] "R"

on Red Hat 9.0.

TIA,

Clint

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.png
Type: image/png
Size: 4164 bytes
Desc: 
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040329/bb5d0b42/test.png
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test2.png
Type: image/png
Size: 2231 bytes
Desc: 
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040329/bb5d0b42/test2.png

From dmurdoch at pair.com  Tue Mar 30 00:15:50 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 29 Mar 2004 17:15:50 -0500
Subject: [R] Interesting Behavior in plot()
In-Reply-To: <Pine.LNX.4.44.0403291308340.19413-100002@aeolus.ecy.wa.gov>
References: <Pine.LNX.4.44.0403291308340.19413-100002@aeolus.ecy.wa.gov>
Message-ID: <hq7h605tppppibbkfrc9iilpjq2k5genlp@4ax.com>

On Mon, 29 Mar 2004 14:06:36 -0800 (PST), Clint Bowman
<clint at ecy.wa.gov> wrote :

>I have a 2 by 226200 table, conveniently read in by read.table(), which
>exhibits some strange behavior when plotted by plot(V1,V2).  The general
>pattern for the range of windspeeds, [0<V1<50] is as expected -- the wind
>gust falls in the interval [V1<V2<65] except for certain values of V2.  
>For V2 == c(15,26,37,48,59), the V2 values are positioned at one-tenth of 
>the V1 (i.e., as if I had issued the command plot(0.1*V1,V2) for just 
>those values of V2 -- see attached plot, test.png).  The only other values 
>of V2 in the range, 4 and 70, also seem to be affected but aren't well 
>enough represented to show up clearly on the plot.
>
>However, if I plot(ws$V1[ws$V2==48],ws$V2[ws$V2==48]), I see the second 
>attachment, test2.png, which confirms that the wind speed (V1) really 
>should be positioned where one would expect.  I haven't seen any messages 
>covering this behaviour and so am looking for an explanation.

I can't quite see how it would generate the symptoms you saw, but a
common source of oddities is that data is read as factors rather than
as numeric values.  You can see the difference using the str()
function, e.g.

> str(x)
 num [1:10] -0.4897  0.6804  0.6979 -0.1203 -0.0428 ...
> str(as.factor(x))
 Factor w/ 10 levels "-1.65972758..",..: 4 7 8 5 6 9 2 10 3 1

I'd check your V1 and V2.

Duncan Murdoch



From gwiggner at lix.polytechnique.fr  Tue Mar 30 00:29:44 2004
From: gwiggner at lix.polytechnique.fr (gwiggner@lix.polytechnique.fr)
Date: Tue, 30 Mar 2004 00:29:44 +0200
Subject: [R] Right shift for normality
Message-ID: <1080599384.4068a358ed7eb@www.lix.polytechnique.fr>

Hello,

My data is discrete, taking values between around -5 and +5.
I cannot give bounds for the values. So I consider it as 
numerical data and not categorical data.

The histogram has a 'normal' shape, so
I test for normality via a chisquare statistic (by calculating the expected 
values by hand).

When I use the sample mean and variance, the normality hypothesis has to be
rejected. 
But when I test for sample mean + a small epsilon, I get very high p-values.

I am not sure if this right shift is a good idea.
Any suggestions?

Thanks,
Claus





----------------------------------------------------------------
This message was sent using IMP, the Internet Messaging Program.



From clint at ecy.wa.gov  Tue Mar 30 00:47:57 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Mon, 29 Mar 2004 14:47:57 -0800 (PST)
Subject: [R] Interesting Behavior in plot()
In-Reply-To: <hq7h605tppppibbkfrc9iilpjq2k5genlp@4ax.com>
Message-ID: <Pine.LNX.4.44.0403291445100.19413-100000@aeolus.ecy.wa.gov>

Right on.  The convenience of slurping up data with read.table() and the 
nearly expected plot caused me to overlook that V2 was read in as a 
factor.  I also had quite a few NAs which may have contributed to the 
problem.

Thanks

On Mon, 29 Mar 2004, Duncan Murdoch wrote:

> On Mon, 29 Mar 2004 14:06:36 -0800 (PST), Clint Bowman
> <clint at ecy.wa.gov> wrote :
> 
> >I have a 2 by 226200 table, conveniently read in by read.table(), which
> >exhibits some strange behavior when plotted by plot(V1,V2).  The general
> >pattern for the range of windspeeds, [0<V1<50] is as expected -- the wind
> >gust falls in the interval [V1<V2<65] except for certain values of V2.  
> >For V2 == c(15,26,37,48,59), the V2 values are positioned at one-tenth of 
> >the V1 (i.e., as if I had issued the command plot(0.1*V1,V2) for just 
> >those values of V2 -- see attached plot, test.png).  The only other values 
> >of V2 in the range, 4 and 70, also seem to be affected but aren't well 
> >enough represented to show up clearly on the plot.
> >
> >However, if I plot(ws$V1[ws$V2==48],ws$V2[ws$V2==48]), I see the second 
> >attachment, test2.png, which confirms that the wind speed (V1) really 
> >should be positioned where one would expect.  I haven't seen any messages 
> >covering this behaviour and so am looking for an explanation.
> 
> I can't quite see how it would generate the symptoms you saw, but a
> common source of oddities is that data is read as factors rather than
> as numeric values.  You can see the difference using the str()
> function, e.g.
> 
> > str(x)
>  num [1:10] -0.4897  0.6804  0.6979 -0.1203 -0.0428 ...
> > str(as.factor(x))
>  Factor w/ 10 levels "-1.65972758..",..: 4 7 8 5 6 9 2 10 3 1
> 
> I'd check your V1 and V2.
> 
> Duncan Murdoch
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From tlumley at u.washington.edu  Tue Mar 30 01:16:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 29 Mar 2004 15:16:25 -0800 (PST)
Subject: [R] data usage
In-Reply-To: <Pine.LNX.4.58.0403291651550.31342@e182104.fee.uva.nl>
References: <Pine.LNX.4.58.0403291357040.31342@e182104.fee.uva.nl>
	<6rsmfryecx.fsf@bates4.stat.wisc.edu>
	<Pine.LNX.4.58.0403291548010.31342@e182104.fee.uva.nl>
	<Pine.A41.4.58.0403290642540.66128@homer05.u.washington.edu>
	<Pine.LNX.4.58.0403291651550.31342@e182104.fee.uva.nl>
Message-ID: <Pine.A41.4.58.0403291513500.114482@homer35.u.washington.edu>

On Mon, 29 Mar 2004, Edwin Leuven wrote:

> > Stata tends to store data as float, integer, or even byte where
> > appropriate for the precision.  This is one source of space saving. A
> > factor of 2 is not atypical.
>
> i was suspecting something like this.
>
> what does R do? default to double always (or something like this)?
>
> is this a deliberate design choice made by the R people or just
> convenience while not having to worry about datatypes?

R has only numeric (C double) and integer (C int) types for storing
numeric data. I think the reason for not having single precision is
insufficient accuracy in computation. Shorter integers might be useful
sometimes.

	-thomas



From Ted.Harding at nessie.mcc.ac.uk  Tue Mar 30 02:27:58 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 30 Mar 2004 00:27:58 -0000 (BST)
Subject: [R] Right shift for normality
In-Reply-To: <1080599384.4068a358ed7eb@www.lix.polytechnique.fr>
Message-ID: <XFMail.040330002758.Ted.Harding@nessie.mcc.ac.uk>

On 29-Mar-04 gwiggner at lix.polytechnique.fr wrote:
> Hello,
> 
> My data is discrete, taking values between around -5 and +5.
> I cannot give bounds for the values. So I consider it as 
> numerical data and not categorical data.
> 
> The histogram has a 'normal' shape, so I test for normality
> via a chisquare statistic (by calculating the expected 
> values by hand).
> 
> When I use the sample mean and variance, the normality hypothesis
> has to be rejected. 
> But when I test for sample mean + a small epsilon, I get very high
> p-values.
> 
> I am not sure if this right shift is a good idea.
> Any suggestions?

I suspect that what you are seeing here corresponds to the following.

Because your data are discrete, you are treating them as though
they are "binned" values of an underlying continuous distribution
when you approach the goodness-of-fit using a chisquared measure.

At the same time, because you are using the sample mean and variance
to estimate the parameters of this distribution, you are behaving
as though the discrete values are the exact values of the continuous
variable.

To be consistent, if treating the observed values as "binned" values,
the estimate you should be using for the mean and variance of the
underlying normal distribution should take account of the grouping.

There could be two main approaches to adopt here.

1. Minimum-chisquared: The chisquared value is the sum (O-E)^2/E
where each E is calculated as n*(integral over the range).
Minimise this with respect to mu and sigma^2.

2. Maximum likelihood: The likelihood is the product of P^r where
P is the integral over the range and r is the count in the range.
Maximise this with respect to mu and sigma^2.

Neither of these estimates will give exactly the sample mean
and sample variance as estimates of mean and variance of the
underlying distribution.

Therefore, if the data you have do in fact correspond very closely
to binned values from a normal distribution, the fit you get by
using sample mean and sample variance as estimates will not be
the best fit, and (if you have enough data) the discrepancy may
well be big enough to give a significantly large chisquared.

But it could be (as you appear to have observed) that simply
shifting the sample mean gives you a fit which is closer to
the fit you would get from (1) or (2) (though I would also have
expected it to improve if you slightly reduced sigma^2 as well).

There is a nice paper from quite long ago by Dennis Lindley
which discusses very closely related issues:

  Lindley, D.V. (1950). Grouping corrections and maximum likelihood
    equations. Proceedings of the Cambridge Philosophical Society,
    vol. 46, 106-110.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 30-Mar-04                                       Time: 00:27:58
------------------------------ XFMail ------------------------------



From jari.oksanen at oulu.fi  Tue Mar 30 02:10:06 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Tue, 30 Mar 2004 03:10:06 +0300
Subject: [R] StepAIC
In-Reply-To: <Pine.GSO.3.95q.1040329175548.12165D-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1040329175548.12165D-100000@sun11.math.uni-hamburg.de>
Message-ID: <9969FA74-81DE-11D8-A27F-000A95C76CA8@oulu.fi>


On 29 Mar 2004, at 19:07, Christian Hennig wrote:

Dear list,

First you see the stepAIC-forward solution (fs7). The strange thing here
is that apparently not all interactions are tried for inclusion, but 
only
WQ:Lage. In particular, I think that WFL:Lage should be tried
in the last two steps, where WFL and Lage are already in the fit.
After fs7, I give the output of fs6 (backward), where all interactions 
are
tried as I have expected. (regsubsets works properly forward and
backward.)

Bob O'Hara posted a message about the same issue just one week ago. 
This seems to be a limitation in identifying names of interaction 
terms. In studying scope, A:B and B:A are regarded as different 
animals. R decides internally which way it arranges these names, and if 
the building up model has a candidate B:A, but scope has A:B, then B:A 
is not regarded as being in the scope. The net result is that step (or 
stepAIC) can be used to build interactions only with good luck.

The extreme case is that R decides to include a term (say A:B) from the 
scope, but after inclusion R decides to re-arrange its name as B:A. 
This is no longer in the scope and step ends with an error message. I 
have hoped to work out a reproducible example of this, but haven't had 
time. However, this happens with the latest devel version of vegan if 
you use methods that you shouldn?t use (that is,  you step cca which 
you cannot do). The last step:

Step:  AIC= 125.58
  varespec ~ Al + P + K + Baresoil + P:K + P:Baresoil

               Df    AIC
+ K:Al         1 125.02
+ Zn           1 125.36
<none>           125.58
+ P:Al         1 125.60
+ Al:Baresoil  1 125.82
+ Humdepth     1 125.83
+ Mo           1 125.94
+ Mg           1 125.96
+ Mn           1 126.11
+ S            1 126.36
- P:Baresoil   1 126.43
+ N            1 126.52
+ Fe           1 126.72
+ K:Baresoil   1 126.80
+ pH           1 126.89
+ Ca           1 127.01
- P:K          1 127.14
- Al           1 128.01

Step:  AIC= 125.02
  varespec ~ Al + P + K + Baresoil + P:K + P:Baresoil + Al:K

Error in factor.scope(ffac, list(add = fadd, drop = fdrop)) :
	upper scope does not include model

Interpretation: K:Al was in the scope and it was included. However, 
after inclusion it changed into Al:K which is not in the scope, so that 
R was able to produce a model where "upper scope does not include 
model".

The secret is in the C routines which decide how to order terms in 
formulae.

cheers, jari oksanen (Oulu)



From berwin at maths.uwa.edu.au  Tue Mar 30 05:57:07 2004
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Tue, 30 Mar 2004 11:57:07 +0800
Subject: [R] problem with update.packages()
In-Reply-To: <20040329094613.GB13815@med.unibs.it>
References: <s0644a3a.090@med-gwia-01a.med.umich.edu>
	<20040329094613.GB13815@med.unibs.it>
Message-ID: <16488.61459.552004.686426@bossiaea.maths.uwa.edu.au>

G'day Stefano,

>>>>> "SC" == Stefano Calza <stecalza at tiscali.it> writes:

    SC> Hi.  I'm experiencing a problem with updating packages on R
    SC> 1.8.1 (2003-11-21) on Debian testing.  I get the following
    SC> message when updating for example Design:

    SC> ...  ...  /usr/bin/ld: cannot find -lg2c-pic ...
[...]
    SC> Any suggestion?

I ran into a similar problem some time ago.  I had no problems with my
original g77-3.3 from testing.  But then g77-3.3 was upgraded and when
I wanted to compile a package that contained Fortran code, I ran into
this problem.

A bit of investigation showed that the current g77-3.3 version in
testing has (on my machine) in /usr/lib/gcc-lib/i486-linux/3.3.3
symbolic links for libg2c-pic.a and libg2c-pic.so to libg2c.a and
libg2c.so, respectively, as if the latter libraries are in the same
directory.  But those two libraries are actually installed into
/usr/lib/.  So the symbolic links point into nirvana and ld can't find
the libraries.

My solution was to upgrade to g77-3.3 in unstable (by changing my
preference file in /etc/apt (attached below).  No problems since then.

Hope that helps.

Cheers,

        Berwin

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: preferences
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040330/4c27373e/preferences.pl

From kimai at Princeton.Edu  Tue Mar 30 06:21:58 2004
From: kimai at Princeton.Edu (Kosuke Imai)
Date: Mon, 29 Mar 2004 23:21:58 -0500 (EST)
Subject: [R] RAqua and gcc
Message-ID: <Pine.LNX.4.44.0403292257130.9439-100000@wws-6qcbw21.Princeton.EDU>

Hi,
  I've recently purchased PowerBookG4 (panther) and installed RAqua 1.8.1.  
I also installed gcc version 3.3 through Xcode tools. But, for some
reason, I'm having a hard time getting gcc work with R. For example, I get
the following error while installing Hmisc package. I would appreciate any 
suggestion to fix this problem. 
Thanks
Kosuke

 * Installing *source* package 'Hmisc' ...
** libs
g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
gcc -no-cpp-precomp -I/Applications/StartR.app/RAqua.app/Contents/include  
-I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
dyld: gcc version mismatch for library: /usr/local/lib/libiconv.2.dylib 
(compatibility version of user: 5.0.0 greater than library's version: 
4.0.0)
make: *** [ranksort.o] Trace/BPT trap
ERROR: compilation failed for package 'Hmisc'



From andrewr at uidaho.edu  Tue Mar 30 06:51:01 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 29 Mar 2004 20:51:01 -0800
Subject: [R] RAqua and gcc
In-Reply-To: <Pine.LNX.4.44.0403292257130.9439-100000@wws-6qcbw21.Princeton.EDU>
References: <Pine.LNX.4.44.0403292257130.9439-100000@wws-6qcbw21.Princeton.EDU>
Message-ID: <200403292051.01899.andrewr@uidaho.edu>

Kosuke,

I tried to reproduce your error, running OS X 10.3.3, R 1.8.1. and gcc 3.3 
20030304.  Hmisc installed without any problem.  

Sorry that I couldn't help more.

Andrew

On Monday 29 March 2004 20:21, Kosuke Imai wrote:
> Hi,
>   I've recently purchased PowerBookG4 (panther) and installed RAqua 1.8.1.
> I also installed gcc version 3.3 through Xcode tools. But, for some
> reason, I'm having a hard time getting gcc work with R. For example, I get
> the following error while installing Hmisc package. I would appreciate any
> suggestion to fix this problem.
> Thanks
> Kosuke
>
>  * Installing *source* package 'Hmisc' ...
> ** libs
> g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
> g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
> g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
> g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
> g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
> gcc -no-cpp-precomp -I/Applications/StartR.app/RAqua.app/Contents/include
> -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
> dyld: gcc version mismatch for library: /usr/local/lib/libiconv.2.dylib
> (compatibility version of user: 5.0.0 greater than library's version:
> 4.0.0)
> make: *** [ranksort.o] Trace/BPT trap
> ERROR: compilation failed for package 'Hmisc'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From iwronsky at users.sourceforge.net  Sat Mar 27 12:28:48 2004
From: iwronsky at users.sourceforge.net (Igor Wronsky)
Date: Sat, 27 Mar 2004 13:28:48 +0200 (EET)
Subject: [R] [R-pkgs] Moron v0.6.0b released
Message-ID: <Pine.LNX.4.51.0403271325160.27230@localhost.localdomain>


We are pleased to announce that the development of the
notorious "Method for Object Recognition of Obscure Nature"
(or Moron, for short) has been switched to the R language.

Given a directory of images (i.e. jpegs), Moron attempts
to predict a category distribution for the content of each
image. The default categories modelled are {healthy,pron,
latex/fetish,japanese_cg,manga,b/w photo}. Supposing
good statistical models can be designed for problems
like these, Moron could eventually be usable as a spam filter
for visual data or to sort images, depending on your taste.
Currently it is not accurate enough for production use.

The Moron package contains a data.frame to estimate a
model to recognize the default categories, and source code
to evaluate new images, collect new training sets (this
can be done simply by putting images of different classes
to different directories and running a few provided functions)
and train new classifiers. The included Local Binary Pattern
(LBP) feature extractors and RGB<->HSV conversion routines
might be of aside interest.

Due to its ability to visualize the predictions made
per image region, Moron is a guaranteed hit in both
parties and classrooms, if only given a juicy enough set
of test images. Even layman (male) audience has been
seen to be captivated by the antics of Moron, efficiently
rivalling traditional aquariums and fireplaces.

Moron is released under GPL. Version 0.6.0b for R can be dl'ed from

http://sourceforge.net/project/showfiles.php?group_id=7434&package_id=68958

The package relies on R libraries 'randomForest' and 'pixmap'.
It has been developed on Linux, but is probably usable on
other systems with only a few simple modifications.


Have fun,
Igor

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From Mathieu.Vuilleumier at unine.ch  Tue Mar 30 09:34:28 2004
From: Mathieu.Vuilleumier at unine.ch (VUILLEUMIER Mathieu)
Date: Tue, 30 Mar 2004 09:34:28 +0200
Subject: [R] Recursive regression on a logistic model
Message-ID: <BD1D7341BE3930408509F95C86A451CB442088@mail1.UNINE.CH>

Hello !

I need help ! I would like to make a recursive regression on a logistic model (or linear model) to test the the stability of my data.

Is anybody know if R has a special command or not ?

Thanks for your help.

Best regards,

**************************************************************
Mathieu Vuilleumier - collaborateur scientifique
Institut de recherches ?conomique et r?gionale (IRER)
Universit? de Neuch?tel
Pierre-?-Mazel 7, CH-2000 Neuch?tel
Tel : 032/ 718 14 66
E-mail : mathieu.vuilleumier at unine.ch



From ajayshah at mayin.org  Mon Mar 29 10:01:37 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 29 Mar 2004 13:31:37 +0530
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <D255C636-7EA4-11D8-A82D-000A959D05F0@mac.com>

> S-Plus has the function AggregateSeries() whose name is self
> explanatory.  For instance one can derive monthly series from daily
> ones by specifying end-of-period, averages, sums, etc.  I looked for
> a similar function in the packages "its" and "tseries", but found
> nothing.  I also help.searched() for aggregate to no avail.  Would
> anybody be so kind to point me in the right direction?

I once needed a function which would convert daily stock prices into
weekly returns. I know, the code is pretty bad (it is all loops and
it's very slow), but my knowledge of R is weak and I really needed it,
so I just used brute force. See EOF for the function. Gabor and Dirk
and Brian Ripley and others on the list were very helpful to me in
getting to the point where I could write this, though obviously they
should not be blamed for my bad code! :-)

I would be very happy if listers could give me ideas on how to do this
better.

Daily to monthly is innately easier since months are 'more normal'
than weeks. I have perl code which does this, which supports 2 cases:
Reporting the last traded price (LTP) of the month versus reporting
the average of the month. If this is useful to you, ask me.

What are the specialised finance libraries available with S-Plus? Can
one marry R with commercial S libraries? I don't like having a
dependence on commercial code, but I might be willing to compromise
and buy libraries that work with R.

Thanks,

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi




### ---------------------------------------------------------------------------
### Function levels2WeeklyRet
###
### Purpose  To convert a vector of daily levels (e.g. a stock price, an
###          index, a currency) into a vector of weekly returns.
### How      We focus on friday-to-friday returns. If a friday was
###          not traded, we take the most-recent available price.
###          We compute 100*log(p2/p1) where p1 and p2 are the prices
###          as of 2 consecutive fridays.
### Inputs   lastfriday   A chron object showing the last friday which
###                       is NOT in the dataset.
###          dates        A vector of dates (chron objects)
###          levels       A vector of prices.
###            Daily levels data is the list of points (dates[i], levels[i])
### CAREFUL  This function could break if you feed it really perverse data,
###          e.g. with numerous consecutive days of missing data.
###          It _must_ be the case that there is atleast 1 traded date
###          in the data after lastfriday but before lastfriday+7.
### Depends  library(chron)
levels2WeeklyRet <- function(lastfriday, dates, levels) {
  ## First do a "dates canonicalise" operation; i.e. lay out a big table with
  ## all dates and populate it. Uninitialised values are left at 0.
  T = length(dates);
  all.dates = seq.dates(lastfriday, dates[T]);
  all.values = numeric(length(all.dates));
  j=1;
  for (i in 1:T) {
    while (dates[i] != all.dates[j]) j = j+1;
    all.values[j] = levels[i];
  }

  ## Walk through all.values[] making a vector of LTPs for all fridays.
  f.ltp = numeric(trunc((dates[T]-lastfriday)/7))
  i = 8;                           # The 1st data friday in all.values
  for (j in 1:length(f.ltp)) {     # Walk in friday's vector.
    k=i; while (0 == all.values[k]) k = k-1; # Go as far back as needed
    f.ltp[j] = all.values[k];      # This is the LTP as of this friday.
    i = i+7;                       # Get ready for the next time --
  }
  return(100*diff(log(f.ltp), 1)); # Pretty computation of returns vector! :-)
}

### ---------------------------------------------------------------------------
# Compute the friday before the stated date.
prevFriday <- function(x) {
  repeat {
    x <- x - 1;
    if (5 == with(month.day.year(x), day.of.week(month,day,year))) break;
  }
  return(x);
}
# Other tricks which could have worked also :
#  do.call( "day.of.week", month.day.year(x) )
#  as.numeric(x-3)%%7   # uses fact that chron(3) is Sunday



From jazevedo at provide.com.br  Tue Mar 30 10:23:07 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Tue, 30 Mar 2004 09:23:07 +0100
Subject: [R] Runing S-plus add-on in R (Binary and Smoothed Binary
	Regression Quantiles)
In-Reply-To: <20040327005133.1abb199a.stanimura-ngs@umin.ac.jp>
Message-ID: <000501c41630$3b6802d0$21a2f080@Lepc204>

Hello,
I would like to know if it is possible to run a s-plus package in R. I'm
particularly interested on 
The add-on written by Walter Belluzzo and Gregory Kordas to estimate Binary
and Smoothed Binary Regression Quantiles using the Simulated Annealing
algorithm. 
Many thanks,
Joao Pedro



From tpapp at axelero.hu  Tue Mar 30 10:27:28 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Tue, 30 Mar 2004 10:27:28 +0200
Subject: [R] numerical solution of functional equations (dynamic stochastic
	optimization)
Message-ID: <20040330082728.GB805@localhost>

Hi,

I need some help with solving functional equations (Bellman's or
Euler's) with numerical methods.  I have read the relevant books
(Kenneth L Judd: Numerical methods in economics, and some others), but
have no practical experience.

All the examples in these books are in Matlab, but I would prefer R,
since I have been using that for some time and I think that it is
generally much nicer a language.  So program code or advice from
somebody who has used R for solving dynamic stochastic optimization
problems (preferably in economics, but other fields are OK) would be
welcome.

Some specific questions:

1. The state space is 4 dimensional.  I think that I will use splines,
what is the easy way to generate and handle multidimensional spline
basis in R? There are many spline packages in R, which ones would you
recommend for this?

2. Is there a package in R which helps generate orthogonal an
Chebyshev basis?

3. I would like to use quadrature rules to speed up integration (ie
the calculation of expected values), which package would you
recommend?

Again, R code would be especially appreciated: I know some R, but
people on this list present such elegant solutions to use it in ways I
would not have thought of.

Thanks,

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From christoph.lehmann at gmx.ch  Tue Mar 30 09:50:37 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 30 Mar 2004 09:50:37 +0200
Subject: [R] classification with nnet: handling unequal class sizes
Message-ID: <1080633037.8550.39.camel@christophl>

I hope this question is adequate for this list

I use the nnet code from V&R p. 348: The very nice and general function
CVnn2() to choose the number of hidden units and the amount of weight
decay by an inner cross-validation- with a slight modification to use it
for classification (see below).

My data has 2 classes with unequal size: 45 observations for classI and
116 obs. for classII

With CVnn2 I get the following confusion matrix (%) (average of 10
runs):

	predicted
true	53 47
	16 84

I had a similar biased confusion matrix with randomForest until I used
the sampsize argument (the same holds for svm until I used the
class.weights argument).

How can I handle this problem of unequal class sizes with nnet, in order
to get a less biased confusion matrix?

(with randomForest I finally got 

	78 22
	16 84
)


many thanks for a hint

Christoph


----------------------------------------------------------------------------
#--- neural networks

#classification network is constructed; this has one output and entropy
fit if the number of levels is two, and a number of outputs equal to the
number of classes and a softmax output stage for more levels. ->
therefore two lines of Prof. Ripley's wrapper function are changed below
(original commented out) and an additional function has been introduced
(resmatrix)

con <- function(...)
{
    print(tab <- table(...))
    diag(tab) <- 0
    cat("error rate = ",
        round(100*sum(tab)/length(list(...)[[1]]), 2), "%\n")
    invisible()
}


CVnn2 <- function(formula, data,
                  size = c(0,4,4,10,10), lambda = c(0, rep(c(0.001,
0.01),2)),
                  nreps = 1, nifold = 5, verbose = 99, ...)
{
    resmatrix <- function(predict.matrix,learn, data, ri, i)
    {
       rae.matrix <-   predict.matrix
       rae.matrix[,] <- 0
       rae.vector <- as.numeric(as.factor((predict(learn, data[ri ==
i,], type = "class"))))
       for (k in 1:dim(rae.matrix)[1]) {
         if (rae.vector[k] == 1) rae.matrix[k,1] <- rae.matrix[k,1] + 1
else
         rae.matrix[k,2] <- rae.matrix[k,2] + 1
       }
       rae.matrix
    }


    CVnn1 <- function(formula, data, nreps=1, ri, verbose,  ...)
    {
        totalerror <- 0
        truth <- data[,deparse(formula[[2]])]
        res <-  matrix(0, nrow(data), length(levels(truth)))
        if(verbose > 20) cat("  inner fold")
        for (i in sort(unique(ri))) {
            if(verbose > 20) cat(" ", i,  sep="")
            for(rep in 1:nreps) {
                learn <- nnet(formula, data[ri !=i,], trace = F, ...)
                #res[ri == i,] <- res[ri == i,] + predict(learn, data[ri
== i,])
                res[ri == i,] <- res[ri == i,] + resmatrix(res[ri ==
i,],learn,data, ri, i)
            }
        }
        if(verbose > 20) cat("\n")
        sum(as.numeric(truth) != max.col(res/nreps))
    }
    truth <- data[,deparse(formula[[2]])]
    res <-  matrix(0, nrow(data), length(levels(truth)))
    choice <- numeric(length(lambda))
    for (i in sort(unique(rand))) {
        if(verbose > 0) cat("fold ", i,"\n", sep="")
        ri <- sample(nifold, sum(rand!=i), replace=T)
        for(j in seq(along=lambda)) {
            if(verbose > 10)
                cat("  size =", size[j], "decay =", lambda[j], "\n")
            choice[j] <- CVnn1(formula, data[rand != i,], nreps=nreps,
                               ri=ri, size=size[j], decay=lambda[j],
                               verbose=verbose, ...)
        }
        decay <- lambda[which.is.max(-choice)]
        csize <- size[which.is.max(-choice)]
        if(verbose > 5) cat("  #errors:", choice, "  ") #
        if(verbose > 1) cat("chosen size = ", csize,
                            " decay = ", decay, "\n", sep="")
        for(rep in 1:nreps) {
            learn <- nnet(formula, data[rand != i,], trace=F,
                          size=csize, decay=decay, ...)
            #res[rand == i,] <- res[rand == i,] + predict(learn,
data[rand == i,])
            res[rand == i,] <- res[rand == i,] + resmatrix(res[rand ==
i,],learn,data, rand, i)
        }
    }
    factor(levels(truth)[max.col(res/nreps)], levels = levels(truth))
}



-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From antonello.romani at studenti.unipr.it  Tue Mar 30 11:14:48 2004
From: antonello.romani at studenti.unipr.it (Antonello Romani)
Date: Tue, 30 Mar 2004 11:14:48 +0200
Subject: [R] koq.q ---- Kent O' Quigley R2
Message-ID: <200403301114.48695.antonello.romani@studenti.unipr.it>

Dear R-users,

I apply to your kind attention to know if someone have used the Splus software 
koq.q (Kent & O'Quigley's measure of dependence for censored data) in R and 
kindly can help me.

I have tried several times to contact the authors Andrej Blejec 
(andrej.blejec at uni-lj.si)  or Janez Stare (janez.stare at mf.uni-lj.si) but 
unfortunately no one answered me. 

Following you'll see the function nlminb that i have changed with R-function 
optim() and the error that came out  when i try to run this software.
As attached file there is the file koq.q


Thanking in advance for what you can do about it

Yours Faithfully


Dr. Antonello Romani
Dipartimento di Medicina Sperimentale
Sezione di Patologia Molecolare e Immunologia
via Volturno 39
43100 Parma
Italy


#-----------------------------------------------------------------------------------------------
find.mu.alfa <- function(theta, theta1, x, ell, which,...)
	{
#
#	find theta=c(beta,mu,alfa) which maximize
#	Expected Log-Likelihood given by ell
#
#	Set lower and upper bounds for mu (-Inf,Inf)
#	and alfa (0,Inf)
#
		lower <- c(which, T, 0) * theta
		lower[c(which, T, 0) == T] <-  - Inf
		upper <- c(which, T, T) * theta
		upper[c(which, T, T) == T] <- Inf	#
		optim(par=theta,fn=ell,method="L-BFGS-B",lower = lower, upper = upper, 
 x = x,theta1 =theta1)
		#nlminb(start = theta, objective = ell, x = x,lower = lower, upper = upper,  	
theta1 =	theta1, ...)
	}
#
#----------------------------------------------------------------------------------------------------------

 
fit<-cph(Surv(futime,fustat)~age,data=ovarian,x=T,y=T,surv=T,method="breslow",type="kaplan-meier")
> fit
Cox Proportional Hazards Model

cph(formula = Surv(futime, fustat) ~ age, data = ovarian, method = "breslow",
    x = T, y = T, surv = T, type = "kaplan-meier")

       Obs     Events Model L.R.       d.f.          P      Score    Score P
        26         12      14.29          1      2e-04      12.26      5e-04
        R2
     0.454


     coef se(coef)    z       p
age 0.162   0.0497 3.25 0.00116

>koq(fit)
Beta  =  0
Beta1 =  0.1616199
Error in ELL(theta = theta0, theta1 = theta1, x = x) :
        only 0's may mix with negative subscripts
In addition: Warning message:
Replacement length not a multiple of the elements to replace in matrix(...)
-------------- next part --------------
"koq"<-
function(beta1, x = NULL, p = NULL, verbose = T)
{
#
#--------------------------------------------------------------------
	check.parameters <- function(beta1, x, p)
	{
#	Check parameters and
#	determine variables to be tested
#
		if(data.class(beta1) == "cph") x <- 
				coxph.detail(beta1)$x
		if(is.null(x))
			stop("Independent variables (x) not set")
		if(data.class(beta1) == "coxreg" || data.class(
			beta1) == "cph")
			beta1 <- beta1$coef
		if(!(data.class(beta1) == "numeric"))
			stop("Illegal parameter beta1 \n A vector of numeric coefficients or an object of class \"coxreg\" expected"
				)
		m <- length(beta1)
		if(is.null(p)) p <- 1:m	
	# p is a list of vars to be tested
		if(max(p) > m | (min(p) < 1 & max(p) > 1))
			stop(paste("Illegal variable selected p=",
				list(p)))
		if(max(p) == 1 & (min(p) == 0 | max(p) == 1) & 
			length(p) == length(beta1)) {
			which <- !p	# p is indicator variable
		}
		else {
			which <- rep(T, m)
			which[p] <- F
		}
		beta <- beta1 * which
		list(beta = beta, beta1 = beta1, x = x, p = p, 
			which = which)
	}
#
#----------------------------------------------------------------
	find.mu.alfa <- function(theta, theta1, x, ell, which, 
		...)
	{
#
#	find theta=c(beta,mu,alfa) which maximize 
#	Expected Log-Likelihood given by ell
#	
#	Set lower and upper bounds for mu (-Inf,Inf) 
#	and alfa (0,Inf)
#	
		lower <- c(which, T, 0) * theta
		lower[c(which, T, 0) == T] <-  - Inf
		upper <- c(which, T, T) * theta
		upper[c(which, T, T) == T] <- Inf	#	
		optim(par=theta,ell,x = x,theta1 =theta1)
		#nlminb(start = theta, objective = ell, x = x,
		#	lower = lower, upper = upper, theta1 =
		#	theta1, ...)
	}
#
#----------------------------------------------------------------
	ELL <- function(theta, theta1, x)
	{
#
#	Expected Log-Likelihood function for the Weibull regression model
#	(see reference 1 in help file)
#
#	Note: 	negative Log-likelihood value is returned
#		to facilitate finding of extreme value of ELL in
#		find.mu.alfa
# 
		np <- length(theta) - 2
		alfa <- theta[np + 2]
		mu <- theta[np + 1]
		alfa1 <- theta1[np + 2]
		mu1 <- theta1[np + 1]
		beta <- theta[1:np]
		beta1 <- theta1[1:np]
		a <- alfa/alfa1
		b.beta <- t(as.matrix(beta - a * beta1)) %*% t(x)
		b <- (mu - a * mu1) + b.beta
		ga1 <- gamma(a + 1)	#
#	negative value of ELL is returned !
#
 - (log(alfa) - 0.57721566 * a + mean(b - exp(b) * ga1))
	}
#
#
#-- koq ---------------------------------------------------------
#
	params <- check.parameters(beta1 = beta1, x = x, p = p)	
	# are parameters OK?
	beta1 <- params$beta1
	beta <- params$beta
	which <- params$which	# which - coefficients not tested
	np <- length(beta1)
	x <- as.matrix(params$x)
	if(length(beta1) != ncol(x))
		stop("Number of coefficients in beta1 should be equal to the number of variables (columns) in x"
			)
	if(verbose) {
		cat("Beta  = ", beta, "\n")
		cat("Beta1 = ", beta1, "\n")
	}
	theta1 <- c(beta1, 0, 1)
	theta <- c(beta, 0, 1)
	b0 <- find.mu.alfa(theta = theta, theta1 = theta1, x = x, 
		ell = ELL, which = which)
	theta0 <- b0$parameters	#	
	GAMMA <- 2 * (ELL(theta = theta0, theta1 = theta1, x = x) -
		ELL(theta = theta1, theta1 = theta1, x = x))
	koq <- 1 - exp( - GAMMA)
	koq
}

From i.visser at uva.nl  Tue Mar 30 11:26:32 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 30 Mar 2004 11:26:32 +0200
Subject: [R] RAqua and gcc
In-Reply-To: <Pine.LNX.4.44.0403292257130.9439-100000@wws-6qcbw21.Princeton.EDU>
Message-ID: <BC8F09E8.E18%i.visser@uva.nl>

Hi Kosuke,
The building R from sources FAQ (version from jan 31,2004, can't seem to
find it on the web anymore), says that installing "libwmf+iconv" as part of
the teTex system causes trouble between gcc and R. You may try removing it
and install again.
I succesfully installed from source files from Raqua, which saves the
trouble of going to the terminal and finding the right directory etc.
Bye, ingmar

> From: Kosuke Imai <kimai at Princeton.Edu>
> Date: Mon, 29 Mar 2004 23:21:58 -0500 (EST)
> To: r-help at stat.math.ethz.ch
> Subject: [R] RAqua and gcc
> 
> Hi,
> I've recently purchased PowerBookG4 (panther) and installed RAqua 1.8.1.
> I also installed gcc version 3.3 through Xcode tools. But, for some
> reason, I'm having a hard time getting gcc work with R. For example, I get
> the following error while installing Hmisc package. I would appreciate any
> suggestion to fix this problem.
> Thanks
> Kosuke
> 
> * Installing *source* package 'Hmisc' ...
> ** libs
> g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
> g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
> g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
> g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
> g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
> gcc -no-cpp-precomp -I/Applications/StartR.app/RAqua.app/Contents/include
> -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
> dyld: gcc version mismatch for library: /usr/local/lib/libiconv.2.dylib
> (compatibility version of user: 5.0.0 greater than library's version:
> 4.0.0)
> make: *** [ranksort.o] Trace/BPT trap
> ERROR: compilation failed for package 'Hmisc'
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From f.calboli at ucl.ac.uk  Tue Mar 30 13:24:57 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 30 Mar 2004 12:24:57 +0100
Subject: [R] italian keyboard and ~ symbol
Message-ID: <1080645897.2939.12.camel@monkey>

Dear All,

I would like to ask the following:

a colleague is using R 1.8.1 for Windows XP on his laptop. The keboard
is the standard Italian layout, which is missing the ~ (tilde) key. For
reasons unknown, typing <Ctrl> <Alt> 126, which I understand is the
standard ASCII code, does not produce the desired symbol (any
combination of keys seems to fail...)

Can anyone advice how to produce the ~ symbol, short of a copy/paste
from MS Word?

Regards,

Federico Calboli 


-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk



From m.dewey at iop.kcl.ac.uk  Tue Mar 30 12:43:51 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Tue, 30 Mar 2004 11:43:51 +0100
Subject: [R] Re: How transform excell file to text file in R
In-Reply-To: <200403301000.i2UA08Ta010362@hypatia.math.ethz.ch>
Message-ID: <5.2.1.1.0.20040330114148.00a0fa20@pop.freeserve.net>

At 12:00 30/03/04 +0200, you wrote:
>From: solares at unsl.edu.ar
>Precedence: list
>MIME-Version: 1.0
>Cc:
>To: R-help at stat.math.ethz.ch
>Date: Mon, 29 Mar 2004 17:22:14 -0300 (ART)
>Message-ID: <59395.170.210.173.216.1080591734.squirrel at inter17.unsl.edu.ar>
>Content-Type: text/plain;charset=iso-8859-1
>Subject: [R] how transform excell file to text file in R
>Message: 56
>
>Hello, it want to know if there is a library that transform a file in
>excell (*. xls) to text file(*.txt, *,prn, *.csv).  Thanks

library(RODBC) will read your Excel file into R. You do not need a copy of 
Excel to do this.


Michael Dewey
m.dewey at iop.kcl.ac.uk



From hb at maths.lth.se  Tue Mar 30 12:48:26 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 30 Mar 2004 12:48:26 +0200
Subject: [R] italian keyboard and ~ symbol
In-Reply-To: <1080645897.2939.12.camel@monkey>
Message-ID: <001001c41644$87e49bf0$e502eb82@maths.lth.se>

<Press-Alt> + Num1 + Num2 + Num6 + <Release-Alt> where Num1, Num2 &
Num6 are 1, 2 & 6 on the *numeric keyboard* (not the ones above the
letter keys) will produce ASCII character 126 ("~", tilde) on my WinXP
Pro in both Rterm and Rgui for R v1.8.1. Do you get anything at all?
Does it work in any other Windows applications or dialogs (you
mentioned Word)? What about <Press-Alt> + Num6 + Num5 + <Release-Alt>
(should give "A")?

You could also consider remapping your keyboard, i.e. for some
physical key replace a "useless/never used" character with "~"
(tilde). Some initial links can be found at
http://www.maths.lth.se/help/windows/keyboard/.

Cheers

Henrik

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Federico Calboli
> Sent: den 30 mars 2004 13:25
> To: r-help
> Subject: [R] italian keyboard and ~ symbol
> 
> 
> Dear All,
> 
> I would like to ask the following:
> 
> a colleague is using R 1.8.1 for Windows XP on his laptop. 
> The keboard is the standard Italian layout, which is missing 
> the ~ (tilde) key. For reasons unknown, typing <Ctrl> <Alt> 
> 126, which I understand is the standard ASCII code, does not 
> produce the desired symbol (any combination of keys seems to
fail...)
> 
> Can anyone advice how to produce the ~ symbol, short of a 
> copy/paste from MS Word?
> 
> Regards,
> 
> Federico Calboli 
> 
> 
> -- 
> 
> 
> 
> =================================
> 
> Federico C. F. Calboli
> 
> Dipartimento di Biologia
> Via Selmi 3
> 40126 Bologna
> Italy
> 
> tel (+39) 051 209 4187
> fax (+39) 051 251 4286
> 
> f.calboli at ucl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From f.calboli at ucl.ac.uk  Tue Mar 30 13:58:40 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 30 Mar 2004 12:58:40 +0100
Subject: [R] italian keyboard and ~ symbol
In-Reply-To: <001001c41644$87e49bf0$e502eb82@maths.lth.se>
References: <001001c41644$87e49bf0$e502eb82@maths.lth.se>
Message-ID: <1080647920.2939.15.camel@monkey>

On Tue, 2004-03-30 at 11:48, Henrik Bengtsson wrote:
> <Press-Alt> + Num1 + Num2 + Num6 + <Release-Alt> where Num1, Num2 &
> Num6 are 1, 2 & 6 on the *numeric keyboard* (not the ones above the
> letter keys) will produce ASCII character 126 ("~", tilde) on my WinXP
> Pro in both Rterm and Rgui for R v1.8.1. Do you get anything at all?
> Does it work in any other Windows applications or dialogs (you
> mentioned Word)? What about <Press-Alt> + Num6 + Num5 + <Release-Alt>
> (should give "A")?

Rather annoyingly it does not work.
> 
> You could also consider remapping your keyboard, i.e. for some
> physical key replace a "useless/never used" character with "~"
> (tilde). Some initial links can be found at
> http://www.maths.lth.se/help/windows/keyboard/.
> 

It appears it is the only option.

Regards,

Federico
-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk



From erich.neuwirth at univie.ac.at  Tue Mar 30 13:38:52 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Tue, 30 Mar 2004 13:38:52 +0200
Subject: [R] italian keyboard and ~ symbol
In-Reply-To: <1080647920.2939.15.camel@monkey>
References: <001001c41644$87e49bf0$e502eb82@maths.lth.se>
	<1080647920.2939.15.camel@monkey>
Message-ID: <40695C4C.40303@univie.ac.at>

Before using the sequence
<Press-Alt> + Num1 + Num2 + Num6 + <Release-Alt>
one has to make sure that the numeric keyboard is in NumLock mode,
not in cursor mode.
If it is in cursor mode, this trick will not work
(at least it does not on my machine)



Federico Calboli wrote:

> On Tue, 2004-03-30 at 11:48, Henrik Bengtsson wrote:
> 
>><Press-Alt> + Num1 + Num2 + Num6 + <Release-Alt> where Num1, Num2 &
>>Num6 are 1, 2 & 6 on the *numeric keyboard* (not the ones above the
>>letter keys) will produce ASCII character 126 ("~", tilde) on my WinXP
>>Pro in both Rterm and Rgui for R v1.8.1. Do you get anything at all?
>>Does it work in any other Windows applications or dialogs (you
>>mentioned Word)? What about <Press-Alt> + Num6 + Num5 + <Release-Alt>
>>(should give "A")?
> 
> 
> Rather annoyingly it does not work.
> 
>>You could also consider remapping your keyboard, i.e. for some
>>physical key replace a "useless/never used" character with "~"
>>(tilde). Some initial links can be found at
>>http://www.maths.lth.se/help/windows/keyboard/.
>>
> 
> 
> It appears it is the only option.
> 
> Regards,
> 
> Federico

-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From gcendoya at balcarce.inta.gov.ar  Tue Mar 30 14:07:01 2004
From: gcendoya at balcarce.inta.gov.ar (CENDOYA, Gabriela)
Date: Tue, 30 Mar 2004 09:07:01 -0300
Subject: [R] italian keyboard and ~ symbol
Message-ID: <002501c4164f$82a38b00$b54a6cc8@gcendoya.balcarce.inta.gov.ar>

Hi Federico:
Try Alt Gr (which is the Alt key on the right hand side of the space bar
key)+4 and that works on my XP.
Gabriela



From vfasciani at micron.com  Tue Mar 30 14:17:56 2004
From: vfasciani at micron.com (vfasciani@micron.com)
Date: Tue, 30 Mar 2004 14:17:56 +0200
Subject: [R] italian keyboard and ~ symbol
Message-ID: <2831746CECBA4F4C86F6076F4FC4C3C10E5911@ntxavzmbx02.azit.micron.com>

Usually in a laptop for using a keyboard as a numeric pad you must
activate this function or, in other ones, is only necessary to press
both "Fn" & "Alt" keys.

e.g. <Fn><Alt> pressed and "j"(1) "k"(2) "o"(6).

Saluti,
Vittorio


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico Calboli
Sent: Tuesday, March 30, 2004 1:25 PM
To: r-help
Subject: [R] italian keyboard and ~ symbol


Dear All,

I would like to ask the following:

a colleague is using R 1.8.1 for Windows XP on his laptop. The keboard
is the standard Italian layout, which is missing the ~ (tilde) key. For
reasons unknown, typing <Ctrl> <Alt> 126, which I understand is the
standard ASCII code, does not produce the desired symbol (any
combination of keys seems to fail...)

Can anyone advice how to produce the ~ symbol, short of a copy/paste
from MS Word?

Regards,

Federico Calboli 


-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vfasciani at micron.com  Tue Mar 30 14:26:24 2004
From: vfasciani at micron.com (vfasciani@micron.com)
Date: Tue, 30 Mar 2004 14:26:24 +0200
Subject: [R] italian keyboard and ~ symbol
Message-ID: <2831746CECBA4F4C86F6076F4FC4C3C10E5912@ntxavzmbx02.azit.micron.com>

Another way is to set italian and US keyboards
(Start-->Settings-->Regional and Language Options;
Languages-->Details..) in order to quickly switch with <Alt><Shift>.

Ari-saluti,
Vittorio

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of vfasciani
Sent: Tuesday, March 30, 2004 2:18 PM
To: r-help at stat.math.ethz.ch
Subject: RE: [R] italian keyboard and ~ symbol


Usually in a laptop for using a keyboard as a numeric pad you must
activate this function or, in other ones, is only necessary to press
both "Fn" & "Alt" keys.

e.g. <Fn><Alt> pressed and "j"(1) "k"(2) "o"(6).

Saluti,
Vittorio


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico Calboli
Sent: Tuesday, March 30, 2004 1:25 PM
To: r-help
Subject: [R] italian keyboard and ~ symbol


Dear All,

I would like to ask the following:

a colleague is using R 1.8.1 for Windows XP on his laptop. The keboard
is the standard Italian layout, which is missing the ~ (tilde) key. For
reasons unknown, typing <Ctrl> <Alt> 126, which I understand is the
standard ASCII code, does not produce the desired symbol (any
combination of keys seems to fail...)

Can anyone advice how to produce the ~ symbol, short of a copy/paste
from MS Word?

Regards,

Federico Calboli 


-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From i.visser at uva.nl  Tue Mar 30 14:36:59 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 30 Mar 2004 14:36:59 +0200
Subject: [R] Console/command line output
Message-ID: <BC8F368B.E30%i.visser@uva.nl>

Hi all,
I am working on Macos x 10.3 (Panther) to build a package consisting of
C/C++ code that is called from R. In the C/C++-sources I use  several
commands to print info to the console:

std::cout << "info" << endl;

and: 

Rprintf("info\n");

Both work fine when R is run on the command line but neither works when
running Raqau (I did check the preference boxes for console and error output
to be written to the console).

Can anyone confirm this behavior of Raqua?
Any suggestions welcome,
Ingmar

platform powerpc-apple-darwin6.8
arch     powerpc   
os       darwin6.8 
system   powerpc, darwin6.8
status             
major    1         
minor    8.1       
year     2003      
month    11        
day      21        
language R



From f.calboli at ucl.ac.uk  Tue Mar 30 15:39:57 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 30 Mar 2004 14:39:57 +0100
Subject: [R] italian keyboard and ~ symbol
In-Reply-To: <40695C4C.40303@univie.ac.at>
References: <001001c41644$87e49bf0$e502eb82@maths.lth.se>
	<1080647920.2939.15.camel@monkey>  <40695C4C.40303@univie.ac.at>
Message-ID: <1080653997.2939.25.camel@monkey>

On Tue, 2004-03-30 at 12:38, Erich Neuwirth wrote:
> Before using the sequence
> <Press-Alt> + Num1 + Num2 + Num6 + <Release-Alt>
> one has to make sure that the numeric keyboard is in NumLock mode,
> not in cursor mode.
> If it is in cursor mode, this trick will not work
> (at least it does not on my machine)
> 

It worked! Many thanks!

Regards,

Federico


-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk



From maechler at stat.math.ethz.ch  Tue Mar 30 14:44:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 30 Mar 2004 14:44:12 +0200
Subject: [R] Console/command line output
In-Reply-To: <BC8F368B.E30%i.visser@uva.nl>
References: <BC8F368B.E30%i.visser@uva.nl>
Message-ID: <16489.27548.419518.540329@gargle.gargle.HOWL>

>>>>> "Ingmar" == Ingmar Visser <i.visser at uva.nl>
>>>>>     on Tue, 30 Mar 2004 14:36:59 +0200 writes:

    Ingmar> Hi all, I am working on Macos x 10.3 (Panther) to
    Ingmar> build a package consisting of C/C++ code that is
    Ingmar> called from R. In the C/C++-sources I use several
    Ingmar> commands to print info to the console:

    Ingmar> std::cout << "info" << endl;

    Ingmar> and:

    Ingmar> Rprintf("info\n");

    Ingmar> Both work fine when R is run on the command line but
    Ingmar> neither works when running Raqau (I did check the
    Ingmar> preference boxes for console and error output to be
    Ingmar> written to the console).

Only the Rprintf() one is supposed to work `in all
circumstances';
if it doesn't work in Raqua (sic!  "aqua" is Latin for "water"),
that's a bug in at least one part of your computer environment.

    Ingmar> Can anyone confirm this behavior of Raqua?  Any
    Ingmar> suggestions welcome, Ingmar

    Ingmar> platform powerpc-apple-darwin6.8 arch powerpc os
    Ingmar> darwin6.8 system powerpc, darwin6.8 status major 1
    Ingmar> minor 8.1 year 2003 month 11 day 21 language R



From darkjacknife at hotmail.com  Tue Mar 30 15:02:00 2004
From: darkjacknife at hotmail.com (angel hellraiser)
Date: Tue, 30 Mar 2004 13:02:00 +0000
Subject: [R] about fact.design
Message-ID: <Sea1-F92tg8E7NdCYdr00015e6f@hotmail.com>

Hi R users:

In R for windows. is there any package to work, in experimental design?
With functions like in S-PLUS , fact.design and the other ones.
How to implement in R, all theory about experimental design, I'm talking 
about

ssType3, fact.design and the theory of orthogonal design, combinatory......

Thanks in advance and please excuse me, my "european english".

My  e-mail is darkjacknife at hotmail.com

_________________________________________________________________
Encuentra a tu media naranja entre los perfiles que m?s te gusten. Toda la 
magia del romance en MSN Amor & Amistad. http://match.msn.es/



From jbboudenne at cdcixis-cm.com  Tue Mar 30 16:14:38 2004
From: jbboudenne at cdcixis-cm.com (boudenne, jb)
Date: Tue, 30 Mar 2004 16:14:38 +0200
Subject: [R] Use R In a Cpp Project
Message-ID: <4AD617C93696EB47A84E53189B57870315537F@msbac.cm.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040330/3c6b8a2a/attachment.pl

From darkjacknife at hotmail.com  Tue Mar 30 16:51:00 2004
From: darkjacknife at hotmail.com (angel hellraiser)
Date: Tue, 30 Mar 2004 14:51:00 +0000
Subject: [R] about fact.design
Message-ID: <Sea1-F92kgqW1oevkuj000162c6@hotmail.com>

Hi R-users:

Can I get the S-function fact.desig for experimentl design for like S-PLUS
In general , Is there software for R in windows about experimental design , 
say,
experimental design , ssType3 , combinatory, orthogonal design.......and for 
sampling in finite population, Horwitz-Thompson sampling pps.
Thanks in advance.

_________________________________________________________________



From Jason.L.Higbee at stls.frb.org  Tue Mar 30 16:51:27 2004
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Tue, 30 Mar 2004 08:51:27 -0600
Subject: [R] Where: package licenses
Message-ID: <20040330145128.74FDB85B8C@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040330/80c13564/attachment.pl

From phgrosjean at sciviews.org  Tue Mar 30 16:53:05 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 30 Mar 2004 16:53:05 +0200
Subject: [R] as.environment() does not work for "package:base"?
Message-ID: <MABBLJDICACNFOLGIHJOAEGOEEAA.phgrosjean@sciviews.org>

Hello,

I want to retrieve one environment from the search path. I have:

> search()
 [1] ".GlobalEnv"      "package:R2HTML"  "package:tcltk"   "package:methods"
"package:ctest"
 [6] "package:mva"     "package:modreg"  "package:nls"     "package:ts"
"Autoloads"
[11] "package:base"
> as.environment(1)
<environment: R_GlobalEnv>
> as.environment(2)
<environment: package:R2HTML>
attr(,"name")
[1] "package:R2HTML"
attr(,"path")
[1] "C:/PROGRA~1/R/rw1081/library/R2HTML"
> as.environment(10)
<environment: 01749504>
attr(,"name")
[1] "Autoloads"
> as.environment(11)
NULL
> as.environment("package:base")
NULL

So, everything works fine, except for "package:base", the 11th entry. Is it
by purpose that as.environment() returns NULL? Is it because "package:base"
is "sealed"? Then, how do I get the environment corresponding to
"package:base"?

Thanks,

Philippe Grosjean

.......................................................<?}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From tlumley at u.washington.edu  Tue Mar 30 17:01:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Mar 2004 07:01:17 -0800 (PST)
Subject: [R] RAqua and gcc
In-Reply-To: <BC8F09E8.E18%i.visser@uva.nl>
References: <BC8F09E8.E18%i.visser@uva.nl>
Message-ID: <Pine.A41.4.58.0403300700330.101650@homer35.u.washington.edu>

On Tue, 30 Mar 2004, Ingmar Visser wrote:

> Hi Kosuke,
> The building R from sources FAQ (version from jan 31,2004, can't seem to
> find it on the web anymore), says that installing "libwmf+iconv" as part of
> the teTex system causes trouble between gcc and R. You may try removing it
> and install again.

And this is exactly the error it causes.  If you don't have teTeX it may
be that something else has installed an incompatible version of libiconv.

	-thomas


> I succesfully installed from source files from Raqua, which saves the
> trouble of going to the terminal and finding the right directory etc.
> Bye, ingmar
>
> > From: Kosuke Imai <kimai at Princeton.Edu>
> > Date: Mon, 29 Mar 2004 23:21:58 -0500 (EST)
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] RAqua and gcc
> >
> > Hi,
> > I've recently purchased PowerBookG4 (panther) and installed RAqua 1.8.1.
> > I also installed gcc version 3.3 through Xcode tools. But, for some
> > reason, I'm having a hard time getting gcc work with R. For example, I get
> > the following error while installing Hmisc package. I would appreciate any
> > suggestion to fix this problem.
> > Thanks
> > Kosuke
> >
> > * Installing *source* package 'Hmisc' ...
> > ** libs
> > g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
> > g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
> > g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
> > g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
> > g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
> > gcc -no-cpp-precomp -I/Applications/StartR.app/RAqua.app/Contents/include
> > -I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
> > dyld: gcc version mismatch for library: /usr/local/lib/libiconv.2.dylib
> > (compatibility version of user: 5.0.0 greater than library's version:
> > 4.0.0)
> > make: *** [ranksort.o] Trace/BPT trap
> > ERROR: compilation failed for package 'Hmisc'
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From MSchwartz at MedAnalytics.com  Tue Mar 30 17:02:09 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 30 Mar 2004 09:02:09 -0600
Subject: [R] Where: package licenses
In-Reply-To: <20040330145128.74FDB85B8C@p3fed1.frb.org>
References: <20040330145128.74FDB85B8C@p3fed1.frb.org>
Message-ID: <1080658929.7665.58.camel@localhost.localdomain>

On Tue, 2004-03-30 at 08:51, Jason.L.Higbee at stls.frb.org wrote: 
> R:
> 
> This stems from my curiosity about the previous thread about  a
> request 
> for glm.nb code.  The issue of package licenses was brought up and I
> was 
> hoping for some clarification on that.  Using the function license()
> or 
> licence() gives info on the license for R, but something like 
> license(MASS) does not give info on the license for the MASS package 
> (perhaps it might be good to expand the license function in the way 
> described).  A quick look on CRAN didn't yield any info on package 
> licenses either. 


For the CRAN add on packages (which includes the 'recommended'
packages), each has the license information under the description. For
example, MASS which is part of the VR bundle, is here:

http://cran.r-project.org/src/contrib/PACKAGES.html#VR

The information is as follows:

...
License: GPL (version 2 or later)
...

Some of the packages are released under other licenses or have
restrictions on commercial use or distribution. Read each one carefully.

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Tue Mar 30 17:03:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Mar 2004 07:03:49 -0800 (PST)
Subject: [R] Where: package licenses
In-Reply-To: <20040330145128.74FDB85B8C@p3fed1.frb.org>
References: <20040330145128.74FDB85B8C@p3fed1.frb.org>
Message-ID: <Pine.A41.4.58.0403300702260.101650@homer35.u.washington.edu>

On Tue, 30 Mar 2004 Jason.L.Higbee at stls.frb.org wrote:

> R:
>
> This stems from my curiosity about the previous thread about  a request
> for glm.nb code.  The issue of package licenses was brought up and I was
> hoping for some clarification on that.  Using the function license() or
> licence() gives info on the license for R, but something like
> license(MASS) does not give info on the license for the MASS package
> (perhaps it might be good to expand the license function in the way
> described).  A quick look on CRAN didn't yield any info on package
> licenses either.

The DESCRIPTION file for a package lists the license; this is also given
in the CRAN entry for each package.

	-thomas



From tlumley at u.washington.edu  Tue Mar 30 17:05:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Mar 2004 07:05:25 -0800 (PST)
Subject: [R] Runing S-plus add-on in R (Binary and Smoothed Binary
	Regression Quantiles)
In-Reply-To: <000501c41630$3b6802d0$21a2f080@Lepc204>
References: <000501c41630$3b6802d0$21a2f080@Lepc204>
Message-ID: <Pine.A41.4.58.0403300704100.101650@homer35.u.washington.edu>

On Tue, 30 Mar 2004, Joao Pedro W. de Azevedo wrote:

> Hello,
> I would like to know if it is possible to run a s-plus package in R. I'm
> particularly interested on
> The add-on written by Walter Belluzzo and Gregory Kordas to estimate Binary
> and Smoothed Binary Regression Quantiles using the Simulated Annealing
> algorithm.

In my experience most substantial pieces of S-PLUS code require some
editing to work in R, but often not very much.  It is, of course, easier
if the authors have provided enough tests that you can tell whether the
package is working...

	-thomas



From Jason.L.Higbee at stls.frb.org  Tue Mar 30 17:14:17 2004
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Tue, 30 Mar 2004 09:14:17 -0600
Subject: [R] Where: package licenses
Message-ID: <20040330151418.557CE85DE2@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040330/21e0c539/attachment.pl

From sundar.dorai-raj at PDF.COM  Tue Mar 30 17:51:16 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 30 Mar 2004 09:51:16 -0600
Subject: [R] Where: package licenses
In-Reply-To: <20040330151418.557CE85DE2@p3fed1.frb.org>
References: <20040330151418.557CE85DE2@p3fed1.frb.org>
Message-ID: <40699774.2050208@pdf.com>


You can access this from within R using

package.description("MASS")[["License"]]

package.description attempts to parse the DESCRIPTION file.

-sundar

Jason.L.Higbee at stls.frb.org wrote:
> Thanks Thomas and Marc that is what I was looking for.
> 
> -Jason
> 
> 
> 
> 
> The DESCRIPTION file for a package lists the license; this is also given
> in the CRAN entry for each package.
> 
>                  -thomas
> 
> 
> 
> 
> 
> Marc Schwartz <MSchwartz at MedAnalytics.com>
> 03/30/2004 09:02 AM
> Please respond to MSchwartz
> 
>  
>         To:     Jason.L.Higbee at stls.frb.org
>         cc:     R-Help <r-help at stat.math.ethz.ch>
>         Subject:        Re: [R] Where: package licenses
> 
> On Tue, 2004-03-30 at 08:51, Jason.L.Higbee at stls.frb.org wrote: 
> 
>>R:
>>
>>This stems from my curiosity about the previous thread about  a
>>request 
>>for glm.nb code.  The issue of package licenses was brought up and I
>>was 
>>hoping for some clarification on that.  Using the function license()
>>or 
>>licence() gives info on the license for R, but something like 
>>license(MASS) does not give info on the license for the MASS package 
>>(perhaps it might be good to expand the license function in the way 
>>described).  A quick look on CRAN didn't yield any info on package 
>>licenses either. 
> 
> 
> 
> For the CRAN add on packages (which includes the 'recommended'
> packages), each has the license information under the description. For
> example, MASS which is part of the VR bundle, is here:
> 
> http://cran.r-project.org/src/contrib/PACKAGES.html#VR
> 
> The information is as follows:
> 
> ...
> License: GPL (version 2 or later)
> ...
> 
> Some of the packages are released under other licenses or have
> restrictions on commercial use or distribution. Read each one carefully.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From solares at unsl.edu.ar  Tue Mar 30 17:54:47 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Tue, 30 Mar 2004 12:54:47 -0300 (ART)
Subject: [R] convert *.xls to text
In-Reply-To: <Pine.LNX.4.10.10403271502380.29640-100000@inter10.unsl.edu.ar>
References: <47295.170.210.173.216.1080311844.squirrel@inter17.unsl.edu.ar>
	<Pine.LNX.4.10.10403271502380.29640-100000@inter10.unsl.edu.ar>
Message-ID: <54737.170.210.173.216.1080662087.squirrel@inter17.unsl.edu.ar>

Hi, really i cant save *.xls to *.txt or *.csv because the file coming in
real time, i need a library or programa what to transform in real time
*.xls to tex file. Ruben



From sway at tanox.com  Tue Mar 30 17:58:37 2004
From: sway at tanox.com (Shawn Way)
Date: Tue, 30 Mar 2004 09:58:37 -0600
Subject: [R] convert *.xls to text
Message-ID: <2F3262756375D411B0CC00B0D049775D01A4CE89@westpark.tanox.net>

Your best bet is RODBC then. The file can stay open, but you can run queries
even with the files open.  Just need to learn a little SQL..

Shawn Way, PE
Engineering Manager
sway at tanox.com


-----Original Message-----
From: solares at unsl.edu.ar [mailto:solares at unsl.edu.ar] 
Sent: Tuesday, March 30, 2004 9:55 AM
To: R-help at stat.math.ethz.ch
Subject: [R] convert *.xls to text


Hi, really i cant save *.xls to *.txt or *.csv because the file coming in
real time, i need a library or programa what to transform in real time *.xls
to tex file. Ruben

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Whit.Armstrong at tudor.com  Tue Mar 30 18:02:23 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Tue, 30 Mar 2004 11:02:23 -0500
Subject: [R] convert *.xls to text
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF02517942@tudor.com>

Has anyone looked into using the Jakarta POI to save R data directly to xls
format?

http://jakarta.apache.org/poi/index.html

One of the advantages of POI over the DCOM interface is that one could use
unix/linux platforms to generate excel files / reports.

I would be interested in collaborating with anyone who has a serious
interest in building an R package for this purpose.

Thanks,
Whit

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of solares at unsl.edu.ar
Sent: Tuesday, March 30, 2004 10:55 AM
To: R-help at stat.math.ethz.ch
Subject: [R] convert *.xls to text


Hi, really i cant save *.xls to *.txt or *.csv because the file coming in
real time, i need a library or programa what to transform in real time *.xls
to tex file. Ruben

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jmacdon at med.umich.edu  Tue Mar 30 18:02:38 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 30 Mar 2004 11:02:38 -0500
Subject: [R] Where: package licenses
Message-ID: <s06953da.022@med-gwia-02a.med.umich.edu>

Note that package.descripton() is deprecated in R-1.9.0. You have to use
packageDescription() instead.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Sundar Dorai-Raj <sundar.dorai-raj at PDF.COM> 03/30/04 10:51AM >>>

You can access this from within R using

package.description("MASS")[["License"]]

package.description attempts to parse the DESCRIPTION file.

-sundar

Jason.L.Higbee at stls.frb.org wrote:
> Thanks Thomas and Marc that is what I was looking for.
> 
> -Jason
> 
> 
> 
> 
> The DESCRIPTION file for a package lists the license; this is also
given
> in the CRAN entry for each package.
> 
>                  -thomas
> 
> 
> 
> 
> 
> Marc Schwartz <MSchwartz at MedAnalytics.com>
> 03/30/2004 09:02 AM
> Please respond to MSchwartz
> 
>  
>         To:     Jason.L.Higbee at stls.frb.org 
>         cc:     R-Help <r-help at stat.math.ethz.ch>
>         Subject:        Re: [R] Where: package licenses
> 
> On Tue, 2004-03-30 at 08:51, Jason.L.Higbee at stls.frb.org wrote: 
> 
>>R:
>>
>>This stems from my curiosity about the previous thread about  a
>>request 
>>for glm.nb code.  The issue of package licenses was brought up and I
>>was 
>>hoping for some clarification on that.  Using the function license()
>>or 
>>licence() gives info on the license for R, but something like 
>>license(MASS) does not give info on the license for the MASS package

>>(perhaps it might be good to expand the license function in the way 
>>described).  A quick look on CRAN didn't yield any info on package 
>>licenses either. 
> 
> 
> 
> For the CRAN add on packages (which includes the 'recommended'
> packages), each has the license information under the description.
For
> example, MASS which is part of the VR bundle, is here:
> 
> http://cran.r-project.org/src/contrib/PACKAGES.html#VR 
> 
> The information is as follows:
> 
> ...
> License: GPL (version 2 or later)
> ...
> 
> Some of the packages are released under other licenses or have
> restrictions on commercial use or distribution. Read each one
carefully.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Mar 30 18:05:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 30 Mar 2004 16:05:54 +0000 (UTC)
Subject: [R] convert *.xls to text
References: <47295.170.210.173.216.1080311844.squirrel@inter17.unsl.edu.ar>
	<Pine.LNX.4.10.10403271502380.29640-100000@inter10.unsl.edu.ar>
	<54737.170.210.173.216.1080662087.squirrel@inter17.unsl.edu.ar>
Message-ID: <loom.20040330T180119-463@post.gmane.org>

 <solares <at> unsl.edu.ar> writes:
> Hi, really i cant save *.xls to *.txt or *.csv because the file coming in
> real time, i need a library or programa what to transform in real time
> *.xls to tex file. Ruben


1. There was a post last month on r-help regarding this batch converter:
http://www.analytics.washington.edu/statcomp/downloads/xls2csv

2. Googling for 
    xls to csv 
will likely locate other alternatives too.

3. You can read the .xls directly into R as shown in the R Data Import/Export
manual and then write it out as a text file.



From aarditi at lighthouse.org  Tue Mar 30 18:12:59 2004
From: aarditi at lighthouse.org (Arditi, Aries)
Date: Tue, 30 Mar 2004 11:12:59 -0500
Subject: [R] unexpected behavior in plot
Message-ID: <7300144209720B45B0773007B0E33F1E18C7F1@USJRAEXCH01.Lighthouse.PRI>


I'm having difficulty getting plot to work with type="n", when either the x or y variables is a factor.

For example,

x <- 1:10
y <- 1:10

plot(x, as.factor(y), type="n")
plot(as.factor(x),y, type="n")
plot(y ~ as.factor(x), type="n")

produce plots with data plotted, whereas

plot(x,y, type="n") 
plot(y ~ x, type = "n")

produce the expected dataless plots

I am running R1.8.1 on Windows (xp), with the windows graphics device.

I have a friend who has tried this on a Mac (OS 10.3.3. with quartz() and x11() devices) and gets erratic behavior, i.e. sometimes plots works as expected, and sometimes not.

Does anyone have any clues on this?

Thanks in advance,


Aries Arditi, Ph.D.
Senior Fellow in Vision Science
Arlene R. Gordon Research Institute
Lighthouse International
111 East 59th Street
New York, NY 10022

Tel: +1 212 821 9500 (direct)
Fax: +1 212 751 9667
http://www.lighthouse.org/research_staff_arditi.htm



From edd at debian.org  Tue Mar 30 18:14:37 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 30 Mar 2004 10:14:37 -0600
Subject: [R] convert *.xls to text
In-Reply-To: <7669F018DC9DD711AEC500065B3D5ABF02517942@tudor.com>
References: <7669F018DC9DD711AEC500065B3D5ABF02517942@tudor.com>
Message-ID: <20040330161437.GA6786@sonny.eddelbuettel.com>

On Tue, Mar 30, 2004 at 11:02:23AM -0500, Whit Armstrong wrote:
> Has anyone looked into using the Jakarta POI to save R data directly to xls
> format?
> 
> http://jakarta.apache.org/poi/index.html
> 
> One of the advantages of POI over the DCOM interface is that one could use
> unix/linux platforms to generate excel files / reports.

There are other (non-Java) possibilities:

-- Spreadsheet::WriteExcel is Perl and works whereever Perl works, there is a
   sibbling Spreadsheet::ParseExcel too. Greg Warnes has built another
   xls2csv around this.

-- Gretl (http://gretl.sf.net) has an add-on module for reading .xls that has
   been lifted from other Open Source projects; one could probably build
   something around the libole2 library from the Gnome project.

> I would be interested in collaborating with anyone who has a serious
> interest in building an R package for this purpose.

I would second that, but I really have no spare capacity for more new
projects. I'd try to help, though.

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From bates at stat.wisc.edu  Tue Mar 30 18:16:57 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 30 Mar 2004 10:16:57 -0600
Subject: [R] Where: package licenses
In-Reply-To: <Pine.A41.4.58.0403300702260.101650@homer35.u.washington.edu>
References: <20040330145128.74FDB85B8C@p3fed1.frb.org>
	<Pine.A41.4.58.0403300702260.101650@homer35.u.washington.edu>
Message-ID: <6r7jx2jona.fsf@bates4.stat.wisc.edu>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Tue, 30 Mar 2004 Jason.L.Higbee at stls.frb.org wrote:
> 
> > R:
> >
> > This stems from my curiosity about the previous thread about  a request
> > for glm.nb code.  The issue of package licenses was brought up and I was
> > hoping for some clarification on that.  Using the function license() or
> > licence() gives info on the license for R, but something like
> > license(MASS) does not give info on the license for the MASS package
> > (perhaps it might be good to expand the license function in the way
> > described).  A quick look on CRAN didn't yield any info on package
> > licenses either.
> 
> The DESCRIPTION file for a package lists the license; this is also given
> in the CRAN entry for each package.

and in the result of

help(package = "MASS")



From ligges at statistik.uni-dortmund.de  Tue Mar 30 18:29:45 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 30 Mar 2004 18:29:45 +0200
Subject: [R] unexpected behavior in plot
In-Reply-To: <7300144209720B45B0773007B0E33F1E18C7F1@USJRAEXCH01.Lighthouse.PRI>
References: <7300144209720B45B0773007B0E33F1E18C7F1@USJRAEXCH01.Lighthouse.PRI>
Message-ID: <4069A079.1090406@statistik.uni-dortmund.de>

Arditi, Aries wrote:
> I'm having difficulty getting plot to work with type="n", when either the x or y variables is a factor.
> 
> For example,
> 
> x <- 1:10
> y <- 1:10
> 
> plot(x, as.factor(y), type="n")
> plot(as.factor(x),y, type="n")
> plot(y ~ as.factor(x), type="n")
> 
> produce plots with data plotted, whereas
> 
> plot(x,y, type="n") 
> plot(y ~ x, type = "n")
> 
> produce the expected dataless plots
> 
> I am running R1.8.1 on Windows (xp), with the windows graphics device.
> 
> I have a friend who has tried this on a Mac (OS 10.3.3. with quartz() and x11() devices) and gets erratic behavior, i.e. sometimes plots works as expected, and sometimes not.
> 
> Does anyone have any clues on this?


Yes: If x is a factor, boxplot() is called by plot.factor(), the 
corresponding method for the generic plot().
boxplot() does not care about type="n".

Uwe Ligges



> Thanks in advance,
> 
> 
> Aries Arditi, Ph.D.
> Senior Fellow in Vision Science
> Arlene R. Gordon Research Institute
> Lighthouse International
> 111 East 59th Street
> New York, NY 10022
> 
> Tel: +1 212 821 9500 (direct)
> Fax: +1 212 751 9667
> http://www.lighthouse.org/research_staff_arditi.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rrsilva at ib.usp.br  Tue Mar 30 18:52:56 2004
From: rrsilva at ib.usp.br (=?iso-8859-1?q?Rog=E9rio_Rosa_da_Silva?=)
Date: Tue, 30 Mar 2004 13:52:56 -0300
Subject: [R] add a column to an exported data.frame
Message-ID: <200403301352.57045.rrsilva@ib.usp.br>

Dears R users,

Is there a way to add a column of results to an exported data.frame (exported 
with write) ?

Thanks

Rog?rio



From kjetil at entelnet.bo  Tue Mar 30 19:02:21 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 30 Mar 2004 13:02:21 -0400
Subject: [R] about fact.design
In-Reply-To: <Sea1-F92kgqW1oevkuj000162c6@hotmail.com>
Message-ID: <40696FDD.7132.312AAB@localhost>

On 30 Mar 2004 at 14:51, angel hellraiser wrote:

You can have a look at libraries AlgDesign
and conf.design, both on 
CRAN.

Kjetil Halvorsen

> Hi R-users:
> 
> Can I get the S-function fact.desig for experimentl design for like
> S-PLUS In general , Is there software for R in windows about
> experimental design , say, experimental design , ssType3 ,
> combinatory, orthogonal design.......and for sampling in finite
> population, Horwitz-Thompson sampling pps. Thanks in advance.
> 
> _________________________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Max_Kuhn at bd.com  Tue Mar 30 19:31:36 2004
From: Max_Kuhn at bd.com (Max_Kuhn@bd.com)
Date: Tue, 30 Mar 2004 12:31:36 -0500
Subject: [R] Sweave and graphic output locations
Message-ID: <OF04155701.2B1FC5A5-ON85256E67.005F1D64@bd.com>


Hello,

I'm using R1.8.1 on windows 2000 and version 1.8.1 of the tools package.

I am attempting to have Sweave write files to a different directory via:

testfile <- system.file("Sweave", "Sweave-test-1.Rnw",
                        package = "tools")

## create a LaTeX file
Sweave(
      testfile,
      output = "C:/temp/Sweave-test-1.tex",
      stylepath = FALSE
      )

but it places the graphics files in the working directory.

Does anyone have any ideas on getting all of the files in one place
(automatically)? The include option says:

include: logical (TRUE), indicating whether input statements for text
output and includegraphics statements for figures should be auto-generated.
Use include=FALSE if the output should appear in a different place than the
code chunk (by placing the input line manually).

If this is the solution, the default value should already be resolving it
(correct?). I'd try setting it to FALSE, but it's not clear how to call
this option.

Thanks in advance,

Max




**********************************************************************
This message is intended only for the designated recipient(s...{{dropped}}



From dmurdoch at pair.com  Tue Mar 30 19:45:42 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 30 Mar 2004 12:45:42 -0500
Subject: [R] convert *.xls to text
In-Reply-To: <7669F018DC9DD711AEC500065B3D5ABF02517942@tudor.com>
References: <7669F018DC9DD711AEC500065B3D5ABF02517942@tudor.com>
Message-ID: <hobj60dfgqunc8coknvn41pduqgu3ifh88@4ax.com>

On Tue, 30 Mar 2004 11:02:23 -0500, Whit Armstrong
<Whit.Armstrong at tudor.com> wrote :

>Has anyone looked into using the Jakarta POI to save R data directly to xls
>format?
>
>http://jakarta.apache.org/poi/index.html
>
>One of the advantages of POI over the DCOM interface is that one could use
>unix/linux platforms to generate excel files / reports.
>
>I would be interested in collaborating with anyone who has a serious
>interest in building an R package for this purpose.

It's much easier to write Excel compatible files than it is to read
them, because you only need to be able to handle a small subset of the
possibilities:  you can skip formulas, graphs, etc.

This should be relatively easy to do using the connection code.  You
can probably find sufficient documentation of the format on
wotsit.org, or from OpenOffice or POI.  Those sources are generally
somewhat obsolete (MS likes to keep updating the format), but for
output purposes they should be fine.

I think it would be preferable to do this all in R, rather than mixing
R and Java, if the goals were relatively modest.  If you want full
read/write access, then it would make more sense to rely on someone
else's work (e.g. POI or ODBC).

Duncan Murdoch



From stecalza at tiscali.it  Tue Mar 30 19:52:49 2004
From: stecalza at tiscali.it (Stefano Calza)
Date: Tue, 30 Mar 2004 19:52:49 +0200
Subject: [R] Sweave and graphic output locations
In-Reply-To: <OF04155701.2B1FC5A5-ON85256E67.005F1D64@bd.com>
References: <OF04155701.2B1FC5A5-ON85256E67.005F1D64@bd.com>
Message-ID: <20040330175249.GB4827@med.unibs.it>

Hi,
you can use \SweaveOpts{prefix.string=foo/bar}
So you'll have your graphs put into directory foo and named bar-*

HIH,
Stefano

On Tue, Mar 30, 2004 at 12:31:36PM -0500, Max_Kuhn at bd.com wrote:
> 
> Hello,
> 
> I'm using R1.8.1 on windows 2000 and version 1.8.1 of the tools package.
> 
> I am attempting to have Sweave write files to a different directory via:
> 
> testfile <- system.file("Sweave", "Sweave-test-1.Rnw",
>                         package = "tools")
> 
> ## create a LaTeX file
> Sweave(
>       testfile,
>       output = "C:/temp/Sweave-test-1.tex",
>       stylepath = FALSE
>       )
> 
> but it places the graphics files in the working directory.
> 
> Does anyone have any ideas on getting all of the files in one place
> (automatically)? The include option says:
> 
> include: logical (TRUE), indicating whether input statements for text
> output and includegraphics statements for figures should be auto-generated.
> Use include=FALSE if the output should appear in a different place than the
> code chunk (by placing the input line manually).
> 
> If this is the solution, the default value should already be resolving it
> (correct?). I'd try setting it to FALSE, but it's not clear how to call
> this option.
> 
> Thanks in advance,
> 
> Max
> 
> 
> 
> 
> **********************************************************************
> This message is intended only for the designated recipient(s...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Tue Mar 30 20:22:44 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 30 Mar 2004 13:22:44 -0500
Subject: [R] DataEntryWindow
Message-ID: <Pine.SGI.4.40.0403301257500.6316250-100000@origin.chass.utoronto.ca>

When I originally posted my question under ( C-c C-c does not kill data
editor), I thought that it was a problem with ESS rather than R. However,
I have tried to do the same from a command line R (R 1.8.1 (2003-11-21).
on a debian  testing i386 machine) and I got the same problem.
Specifically I cannot kill the DataEntryWindow by issuing C-c (control c).
However, unless I issue the C-c command the data.frame will be printed on
the buffer when its killed  with the mouse.  moreover, I need to refresh
the screen everytime I  tab away from the dataentrywindow in order to be able to view
its content again ( C-l).

I issued the following commands.

edit(as.data.frame(matrix(rnorm(100), nrow=10)))
C-c

The DataEntryWindow is still present but when I manually (by mouse) kill
it, it does not print in the buffer.

Any help is greatly appreciated.

This is what XFree86 -version gives:

XFree86 Version 4.2.1.1 (Debian 4.2.1-12.1 20031003005825
james at nocrew.org) / X Window System
(protocol Version 11, revision 0, vendor release 6600)
Release Date: 18 October 2002
        If the server is older than 6-12 months, or if your card is
        newer than the above date, look for a newer version before
        reporting problems.  (See http://www.XFree86.Org/)
Build Operating System: Linux 2.4.21-rc1-ac1-cryptoloop i686 [ELF]
Module Loader present



From andy_liaw at merck.com  Tue Mar 30 20:37:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 30 Mar 2004 13:37:32 -0500
Subject: [R] add a column to an exported data.frame
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AD1@usrymx25.merck.com>

If this is on [U,Li]nix, you can write the extra column(s) to another file,
then use the `paste' command (see man paste) to do that at the shell prompt.
If you're on Windows, you can try to find a version of `paste' that works on
Windows.

HTH,
Andy

> From: Rog?rio Rosa da Silva
> 
> Dears R users,
> 
> Is there a way to add a column of results to an exported 
> data.frame (exported 
> with write) ?
> 
> Thanks
> 
> Rog?rio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sigma at consultoresestadisticos.com  Tue Mar 30 20:59:30 2004
From: sigma at consultoresestadisticos.com (Carlos J. Gil Bellosta)
Date: Tue, 30 Mar 2004 20:59:30 +0200
Subject: [R] BoxPlots, 1 Way ANOVA and Non-Statisticians.
In-Reply-To: <Pine.SGI.4.40.0403301257500.6316250-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0403301257500.6316250-100000@origin.chass.utoronto.ca>
Message-ID: <4069C392.3030700@consultoresestadisticos.com>

Dear R-Helpers,

I am working in a project and I have a number of observations belonging 
to several classes. Using a 1 Way ANOVA, I have rejected the equality of 
means hypothesis with a very small p-value. However, the people I have 
to present my results to are not statisticians and they are not very 
likely to be much impressed by a 1.32434e-12 like number/thing.

Therefore I have decided to make to boxplots, one for the actual data 
and the second one for simulated data where the equality of the means 
holds so that the difference in the distributions can be visually 
appreciated.

The problem is that, for the simulated values, being more regular, the 
range of variation is smaller and, therefore, the heigth of the window 
where their boxplot is drawn is also smaller. As a result, the scales of 
the two boxplots are not the same and part of the appeal of the visual 
approach is lost in the way.

My question is, is there a way to make two different boxplots within a 
"common window"? (Or rather, a common size window or, more concretely, 
so that it spans over the same range on the vertical axis).

Sincerely,

Carlos J. Gil Bellosta
Sigma Consultores Estadi'sticos
http://www.consultoresestadisticos.com



From ripley at stats.ox.ac.uk  Tue Mar 30 21:05:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Mar 2004 20:05:00 +0100 (BST)
Subject: [R] Where: package licenses
In-Reply-To: <6r7jx2jona.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0403301957180.31563-100000@gannet.stats>

On 30 Mar 2004, Douglas Bates wrote:

> Thomas Lumley <tlumley at u.washington.edu> writes:
> 
> > On Tue, 30 Mar 2004 Jason.L.Higbee at stls.frb.org wrote:
> > 
> > > R:
> > >
> > > This stems from my curiosity about the previous thread about  a request
> > > for glm.nb code.  The issue of package licenses was brought up and I was
> > > hoping for some clarification on that.  Using the function license() or
> > > licence() gives info on the license for R, but something like
> > > license(MASS) does not give info on the license for the MASS package
> > > (perhaps it might be good to expand the license function in the way
> > > described).  A quick look on CRAN didn't yield any info on package
> > > licenses either.
> > 
> > The DESCRIPTION file for a package lists the license; this is also given
> > in the CRAN entry for each package.
> 
> and in the result of
> 
> help(package = "MASS")

Also, MASS (like many other packages) installs a file called LICENCE (it's 
in English ...) in its top-level directory.  

My point is that you are not allowed to redistribute even parts of it
without the licence, and that is what the original requestor was
explicitly asking for.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ccleland at optonline.net  Tue Mar 30 21:06:49 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 30 Mar 2004 14:06:49 -0500
Subject: [R] BoxPlots, 1 Way ANOVA and Non-Statisticians.
In-Reply-To: <4069C392.3030700@consultoresestadisticos.com>
References: <Pine.SGI.4.40.0403301257500.6316250-100000@origin.chass.utoronto.ca>
	<4069C392.3030700@consultoresestadisticos.com>
Message-ID: <4069C549.9080509@optonline.net>

Carlos:
   You could just give the same values for the ylim argument in each 
boxplot

  par(mfrow=c(1,2), las=1)
  boxplot(rnorm(10), rnorm(10), rnorm(10), ylim=c(-3,3))
  boxplot(rnorm(10, 0.5, 1), rnorm(10, -.5, 1), rnorm(10), ylim=c(-3,3))

Carlos J. Gil Bellosta wrote:
> I am working in a project and I have a number of observations belonging 
> to several classes. Using a 1 Way ANOVA, I have rejected the equality of 
> means hypothesis with a very small p-value. However, the people I have 
> to present my results to are not statisticians and they are not very 
> likely to be much impressed by a 1.32434e-12 like number/thing.
> 
> Therefore I have decided to make to boxplots, one for the actual data 
> and the second one for simulated data where the equality of the means 
> holds so that the difference in the distributions can be visually 
> appreciated.
> 
> The problem is that, for the simulated values, being more regular, the 
> range of variation is smaller and, therefore, the heigth of the window 
> where their boxplot is drawn is also smaller. As a result, the scales of 
> the two boxplots are not the same and part of the appeal of the visual 
> approach is lost in the way.
> 
> My question is, is there a way to make two different boxplots within a 
> "common window"? (Or rather, a common size window or, more concretely, 
> so that it spans over the same range on the vertical axis).

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Achim.Zeileis at wu-wien.ac.at  Tue Mar 30 21:17:36 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 30 Mar 2004 21:17:36 +0200
Subject: [R] Sweave and graphic output locations
In-Reply-To: <OF04155701.2B1FC5A5-ON85256E67.005F1D64@bd.com>
References: <OF04155701.2B1FC5A5-ON85256E67.005F1D64@bd.com>
Message-ID: <20040330211736.42f91bed.Achim.Zeileis@wu-wien.ac.at>

On Tue, 30 Mar 2004 12:31:36 -0500 Max_Kuhn at bd.com wrote:

> 
> Hello,
> 
> I'm using R1.8.1 on windows 2000 and version 1.8.1 of the tools
> package.
> 
> I am attempting to have Sweave write files to a different directory
> via:
> 
> testfile <- system.file("Sweave", "Sweave-test-1.Rnw",
>                         package = "tools")
> 
> ## create a LaTeX file
> Sweave(
>       testfile,
>       output = "C:/temp/Sweave-test-1.tex",
>       stylepath = FALSE
>       )
> 
> but it places the graphics files in the working directory.

I guess the simplest workaround is to change the working directory
before you call Sweave(), i.e.
  setwd("C:/temp/")
  Sweave(testfile, output = "myfile.tex", stylepath = FALSE)
 
> Does anyone have any ideas on getting all of the files in one place
> (automatically)? The include option says:
> 
> include: logical (TRUE), indicating whether input statements for text
> output and includegraphics statements for figures should be
> auto-generated. Use include=FALSE if the output should appear in a
> different place than the code chunk (by placing the input line
> manually).

This is about the 
  \includegraphics{}
statement within the TeX file. Of course you can set each one of these
explicitely giving the full path to the graphics file, but I guess this
is rather cumbersome.

> If this is the solution, the default value should already be resolving
> it(correct?). I'd try setting it to FALSE, but it's not clear how to
> call this option.

You need to do that in the .Rnw file, e.g.

<<fig = TRUE, include = FALSE>>=
plot(1:10)
@
 
hth
Z

> Thanks in advance,
> 
> Max
> 
> 
> 
> 
> **********************************************************************
> This message is intended only for the designated
> recipient(s...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From elvis at xlsolutions-corp.com  Tue Mar 30 21:19:51 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 30 Mar 2004 12:19:51 -0700
Subject: [R] Course***R/Splus Programming Techniques in Boston
Message-ID: <20040330191951.11160.qmail@webmail-2-2.mesa1.secureserver.net>



From robert.kissell at citigroup.com  Tue Mar 30 21:23:49 2004
From: robert.kissell at citigroup.com (Kissell, Robert [EQRE])
Date: Tue, 30 Mar 2004 14:23:49 -0500
Subject: [R] Data Grouping Question
Message-ID: <4115749EFC8862458D6FE4F04F5F7DE701E82F51@EXCHNY37.ny.ssmb.com>

Hi,

I have a quick question regarding grouping data in R. I have the following matrix,

A =	0	1	1	0
	1	0	0	1
	1	0	0	1
	1	1	0	0
	1	0	0	1
	0	1	1	0

I would like to learn how I can group the data on unique rows of A and also count the number of times the row occurred. The command unique(A) provides a matrix with the unique rows, i.e., B

B =	0	1	1	0
	1	0	0	1
	1	1	0	0

but I am also interested in learning how I can count the number of unque rows too. The result can be in either one or two matrices, e.g.,

B =	0	1	1	0
	1	0	0	1
	1	1	0	0

Count =		1
		3
		2

or,

C =	0	1	1	0	1
	1	0	0	1	3
	1	1	0	0	2

Thanks in advance.

Rob Kissell
Robert.Kissell at Citigroup.com



From ccleland at optonline.net  Tue Mar 30 21:32:11 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 30 Mar 2004 14:32:11 -0500
Subject: [R] Data Grouping Question
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82F51@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F51@EXCHNY37.ny.ssmb.com>
Message-ID: <4069CB3B.2030408@optonline.net>

Here is a way, though probably not the best way:

table(apply(A, 1, function(x){paste(x, collapse="")}))

Kissell, Robert [EQRE] wrote:

> Hi,
> 
> I have a quick question regarding grouping data in R. I have the following matrix,
> 
> A =	0	1	1	0
> 	1	0	0	1
> 	1	0	0	1
> 	1	1	0	0
> 	1	0	0	1
> 	0	1	1	0
> 
> I would like to learn how I can group the data on unique rows of A and also count the number of times the row occurred. The command unique(A) provides a matrix with the unique rows, i.e., B
> 
> B =	0	1	1	0
> 	1	0	0	1
> 	1	1	0	0
> 
> but I am also interested in learning how I can count the number of unque rows too. The result can be in either one or two matrices, e.g.,
> 
> B =	0	1	1	0
> 	1	0	0	1
> 	1	1	0	0
> 
> Count =		1
> 		3
> 		2
> 
> or,
> 
> C =	0	1	1	0	1
> 	1	0	0	1	3
> 	1	1	0	0	2

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From pburns at pburns.seanet.com  Tue Mar 30 21:36:53 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 30 Mar 2004 20:36:53 +0100
Subject: [R] Aggregating frequency of irregular time series
References: <D255C636-7EA4-11D8-A82D-000A959D05F0@mac.com>
Message-ID: <4069CC55.7020203@pburns.seanet.com>

Assuming that you are using log returns, one approach (probably
not the best) is to convert the dates to julian days, find all of the
Sundays spanning the dates in the series, then do something along
the lines of:

tapply(the.log.returns, cut(julian.days, sundays), sum)


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:

>>S-Plus has the function AggregateSeries() whose name is self
>>explanatory.  For instance one can derive monthly series from daily
>>ones by specifying end-of-period, averages, sums, etc.  I looked for
>>a similar function in the packages "its" and "tseries", but found
>>nothing.  I also help.searched() for aggregate to no avail.  Would
>>anybody be so kind to point me in the right direction?
>>    
>>
>
>I once needed a function which would convert daily stock prices into
>weekly returns. I know, the code is pretty bad (it is all loops and
>it's very slow), but my knowledge of R is weak and I really needed it,
>so I just used brute force. See EOF for the function. Gabor and Dirk
>and Brian Ripley and others on the list were very helpful to me in
>getting to the point where I could write this, though obviously they
>should not be blamed for my bad code! :-)
>
>I would be very happy if listers could give me ideas on how to do this
>better.
>
>Daily to monthly is innately easier since months are 'more normal'
>than weeks. I have perl code which does this, which supports 2 cases:
>Reporting the last traded price (LTP) of the month versus reporting
>the average of the month. If this is useful to you, ask me.
>
>What are the specialised finance libraries available with S-Plus? Can
>one marry R with commercial S libraries? I don't like having a
>dependence on commercial code, but I might be willing to compromise
>and buy libraries that work with R.
>
>Thanks,
>
>  
>



From Whit.Armstrong at tudor.com  Tue Mar 30 21:55:26 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Tue, 30 Mar 2004 14:55:26 -0500
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF02517959@tudor.com>

This function will probably work for you if your dates are in the format
yyyymmdd.  Use it to convert the series from daily to monthly, then use a 1
period difference to calc the returns of the series.

Feel free to contact me off list with any questions.


Dates <- function(x) rownames(x)

Timebase <- function(x,monthly=T,first.day=T) {
	if(length(dim(x))!=2) stop("can only take 2 dimensional data.")

	seriesDates <- as.integer(Dates(x))
	
	# year and Month
	seriesYM <- as.integer(seriesDates/100)
	seriesYM.unique <- sort(unique(seriesYM))
	ans.nrow <- length(seriesYM.unique)

	# loop through all the unique months and grab the max day of month
	ans <- matrix(NA,nrow=ans.nrow,ncol=ncol(x))
	ans.dates <- vector("numeric",ans.nrow)
	
	for (i in 1:ans.nrow) {
	
		this.month <- seriesYM == seriesYM.unique[i]
		max.day <- max(seriesDates[this.month])		
		ans.dates[i] <- max.day
	}

	
	colnames(ans) <- colnames(x)
	ans[] <- x[as.character(ans.dates),]
	
	if(first.day==T) {
		rownames(ans) <-
paste(substring(as.character(ans.dates),1,6),"01",sep="")
	} else {
		rownames(ans) <- ans.dates
	}
	
	as.tseries(ans)
}

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
Sent: Tuesday, March 30, 2004 2:37 PM
To: Ajay Shah
Cc: r-help at stat.math.ethz.ch; Ivan Alves
Subject: Re: [R] Aggregating frequency of irregular time series


Assuming that you are using log returns, one approach (probably not the
best) is to convert the dates to julian days, find all of the Sundays
spanning the dates in the series, then do something along the lines of:

tapply(the.log.returns, cut(julian.days, sundays), sum)


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ajay Shah wrote:

>>S-Plus has the function AggregateSeries() whose name is self 
>>explanatory.  For instance one can derive monthly series from daily 
>>ones by specifying end-of-period, averages, sums, etc.  I looked for a 
>>similar function in the packages "its" and "tseries", but found 
>>nothing.  I also help.searched() for aggregate to no avail.  Would 
>>anybody be so kind to point me in the right direction?
>>    
>>
>
>I once needed a function which would convert daily stock prices into 
>weekly returns. I know, the code is pretty bad (it is all loops and 
>it's very slow), but my knowledge of R is weak and I really needed it, 
>so I just used brute force. See EOF for the function. Gabor and Dirk 
>and Brian Ripley and others on the list were very helpful to me in 
>getting to the point where I could write this, though obviously they 
>should not be blamed for my bad code! :-)
>
>I would be very happy if listers could give me ideas on how to do this 
>better.
>
>Daily to monthly is innately easier since months are 'more normal' than 
>weeks. I have perl code which does this, which supports 2 cases: 
>Reporting the last traded price (LTP) of the month versus reporting the 
>average of the month. If this is useful to you, ask me.
>
>What are the specialised finance libraries available with S-Plus? Can 
>one marry R with commercial S libraries? I don't like having a 
>dependence on commercial code, but I might be willing to compromise and 
>buy libraries that work with R.
>
>Thanks,
>
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Mar 30 22:27:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Mar 2004 21:27:58 +0100 (BST)
Subject: [R] as.environment() does not work for "package:base"?
In-Reply-To: <MABBLJDICACNFOLGIHJOAEGOEEAA.phgrosjean@sciviews.org>
Message-ID: <Pine.LNX.4.44.0403302125150.31813-100000@gannet.stats>

NULL is the correct answer: as an environment base is denoted by NULL.
Did you think to try

ls(envir=NULL)

?  If not, do so now.

On Tue, 30 Mar 2004, Philippe Grosjean wrote:

> Hello,
> 
> I want to retrieve one environment from the search path. I have:
> 
> > search()
>  [1] ".GlobalEnv"      "package:R2HTML"  "package:tcltk"   "package:methods"
> "package:ctest"
>  [6] "package:mva"     "package:modreg"  "package:nls"     "package:ts"
> "Autoloads"
> [11] "package:base"
> > as.environment(1)
> <environment: R_GlobalEnv>
> > as.environment(2)
> <environment: package:R2HTML>
> attr(,"name")
> [1] "package:R2HTML"
> attr(,"path")
> [1] "C:/PROGRA~1/R/rw1081/library/R2HTML"
> > as.environment(10)
> <environment: 01749504>
> attr(,"name")
> [1] "Autoloads"
> > as.environment(11)
> NULL
> > as.environment("package:base")
> NULL
> 
> So, everything works fine, except for "package:base", the 11th entry. Is it
> by purpose that as.environment() returns NULL? Is it because "package:base"
> is "sealed"? Then, how do I get the environment corresponding to
> "package:base"?
> 
> Thanks,
> 
> Philippe Grosjean
> 
> .......................................................<?}))><....
>  ) ) ) ) )
> ( ( ( ( (   Prof. Philippe Grosjean
> \  ___   )
>  \/ECO\ (   Numerical Ecology of Aquatic Systems
>  /\___/  )  Mons-Hainaut University, Pentagone
> / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
>  /NUM\/  )
>  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
>        \ )  email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
> ( ( ( ( (
> ...................................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From White.Denis at epamail.epa.gov  Tue Mar 30 22:25:05 2004
From: White.Denis at epamail.epa.gov (White.Denis@epamail.epa.gov)
Date: Tue, 30 Mar 2004 12:25:05 -0800
Subject: [R] rank() vs SAS proc rank
Message-ID: <OFB19DF0F3.DF907470-ON88256E67.006DE5AA-88256E67.00702410@epamail.epa.gov>





SAS proc rank has ties options of high and low that would allow
producing ranks of the type found in the sports pages, e.g.,

rank (c(1,1,2,2,2,2,3)) == 1 1 3 3 3 3 7

Could R support these ties.methods?



From ggrothendieck at myway.com  Tue Mar 30 22:53:52 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 30 Mar 2004 20:53:52 +0000 (UTC)
Subject: [R] Data Grouping Question
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F51@EXCHNY37.ny.ssmb.com>
Message-ID: <loom.20040330T224920-380@post.gmane.org>

Kissell, Robert [EQRE] <robert.kissell <at> citigroup.com> writes:

> I would like to learn how I can group the data on unique rows of A and also 
count the number of times the row
> occurred. 

You have already provided the answer, unique(A), to the first part of your
question.  Here are two solutions to the second part:

1. Since each row can be regarded as the representation of a binary
number:

 table(A%*%2^(0:3))

2. Another possibility not dependent on the binary nature of the
data is to define:

 "%+.*%" <- function(a,b)apply(b,2,function(x)apply(t(a) == x,2,all))

This function is the +.x of APL.  It defines an infix function that does a
matrix multiply of matrix a and matrix b except it replaces the usual inner
product of two vectors x and y with all(x==y).

In terms of this function, the answer is:

 colSums( A %+.*% t(unique(A)) )



From ggrothendieck at myway.com  Tue Mar 30 23:02:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 30 Mar 2004 21:02:58 +0000 (UTC)
Subject: [R] rank() vs SAS proc rank
References: <OFB19DF0F3.DF907470-ON88256E67.006DE5AA-88256E67.00702410@epamail.epa.gov>
Message-ID: <loom.20040330T225717-821@post.gmane.org>

 <White.Denis <at> epamail.epa.gov> writes:

> SAS proc rank has ties options of high and low that would allow
> rank (c(1,1,2,2,2,2,3)) == 1 1 3 3 3 3 7
> Could R support these ties.methods?

Don't know how SAS works but for your vector:

> z <- c(1,1,2,2,2,2,3)
> match(z,z)
[1] 1 1 3 3 3 3 7



From rog at stanford.edu  Tue Mar 30 23:16:38 2004
From: rog at stanford.edu (Roger Levy)
Date: 30 Mar 2004 13:16:38 -0800
Subject: [R] exact logistic regression facilities in R?
Message-ID: <258yhiujbd.fsf@joel.Stanford.EDU>

Hi,

Does R have any facilities for exact logistic regression?

Many thanks,

Roger



From ripley at stats.ox.ac.uk  Tue Mar 30 23:18:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Mar 2004 22:18:18 +0100 (BST)
Subject: [R] rank() vs SAS proc rank
In-Reply-To: <OFB19DF0F3.DF907470-ON88256E67.006DE5AA-88256E67.00702410@epamail.epa.gov>
Message-ID: <Pine.LNX.4.44.0403302211480.31813-100000@gannet.stats>

On Tue, 30 Mar 2004 White.Denis at epamail.epa.gov wrote:

> SAS proc rank has ties options of high and low that would allow
> producing ranks of the type found in the sports pages, e.g.,
> 
> rank (c(1,1,2,2,2,2,3)) == 1 1 3 3 3 3 7
> 
> Could R support these ties.methods?

Yes, it is possible to program them in R.  Here is one way:

1 + rowSums(outer(x, x, ">"))

which at least generalizes your single unexplained example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Tue Mar 30 23:18:11 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 30 Mar 2004 17:18:11 -0400 (AST)
Subject: [R] rank() vs SAS proc rank
Message-ID: <200403302118.i2ULIBf5014437@erdos.math.unb.ca>

Gabor Grothendieck writes:

> <White.Denis <at> epamail.epa.gov> writes:
> > SAS proc rank has ties options of high and low that would allow
> > rank (c(1,1,2,2,2,2,3)) == 1 1 3 3 3 3 7
> > Could R support these ties.methods?

> Don't know how SAS works but for your vector:
> 
> > z <- c(1,1,2,2,2,2,3)
> > match(z,z)
> [1] 1 1 3 3 3 3 7

Ah, but if z isn't presorted, then it worketh not:

E.g.

	> w <- c(2,2,2,1,3,1,2)
	> match(w,w)
	[1] 1 1 1 4 5 4 1

Try
	> bar <- function(x) {
	 	o <- order(x)
	 	x <- x[o]
	 	match(x,x)[order(o)]
	 }

Then
	> bar(w)
	[1] 3 3 3 1 7 1 3

which ***is*** what's wanted.  I have not tested this idea any
further. :-)

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From jazevedo at provide.com.br  Wed Mar 31 01:17:46 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Wed, 31 Mar 2004 00:17:46 +0100
Subject: [R] Tricube weighting procedure
In-Reply-To: <200403302118.i2ULIBf5014437@erdos.math.unb.ca>
Message-ID: <000001c416ad$36149b90$21a2f080@Lepc204>

Dear R users,
I would like to know if there is (and how I could obtain it) any R
application that performs tricube weighting regressions.
All the best,
JP



From ggrothendieck at myway.com  Wed Mar 31 01:46:55 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 30 Mar 2004 23:46:55 +0000 (UTC)
Subject: [R] Data Grouping Question
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F51@EXCHNY37.ny.ssmb.com>
	<loom.20040330T224920-380@post.gmane.org>
Message-ID: <loom.20040331T014230-751@post.gmane.org>


In rereading this, the solution works but my comments on the names
of APL operators was off.  I think this would make more
sense in terms of naming:  

"%all.==%" <- function(a,b)apply(b,2,function(x)apply(t(a) == x,2,all))
colSums( A %all.==% t(unique(A)) )



Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: 
: Kissell, Robert [EQRE] <robert.kissell <at> citigroup.com> writes:
: 
: > I would like to learn how I can group the data on unique rows of A and 
also 
: count the number of times the row
: > occurred. 
: 
: You have already provided the answer, unique(A), to the first part of your
: question.  Here are two solutions to the second part:
: 
: 1. Since each row can be regarded as the representation of a binary
: number:
: 
:  table(A%*%2^(0:3))
: 
: 2. Another possibility not dependent on the binary nature of the
: data is to define:
: 
:  "%+.*%" <- function(a,b)apply(b,2,function(x)apply(t(a) == x,2,all))
: 
: This function is the +.x of APL.  It defines an infix function that does a
: matrix multiply of matrix a and matrix b except it replaces the usual inner
: product of two vectors x and y with all(x==y).
: 
: In terms of this function, the answer is:
: 
:  colSums( A %+.*% t(unique(A)) )



From rpeng at jhsph.edu  Wed Mar 31 01:48:48 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 30 Mar 2004 18:48:48 -0500
Subject: [R] a question about scoping functions
In-Reply-To: <1080354486.3157.8.camel@localhost.localdomain>
References: <1080354486.3157.8.camel@localhost.localdomain>
Message-ID: <406A0760.4020201@jhsph.edu>

I wrote a function a while back that does something like "attaching" R 
source files.  The code in the R file is attached in the second position 
on the search list.  I actually don't use it much but you may find it 
useful.


sattach <- function(filename) {
     if(!is.character(filename))
         stop(sQuote("filename"), " must be character string")
     e <- new.env()
     eval(parse(filename), envir = e)
     func.list <- lapply(ls(e), get, envir = e)
     names(func.list) <- ls(e)
     outobj <- make.names(basename(filename))
     assign(outobj, func.list)

     cmd <- paste("attach(", outobj, ")", sep = "")
     eval(parse(text = cmd))
}

-roger

Rajarshi Guha wrote:

> Hi,
>  I've written some helper functions which are used by another function
> (called func()). When func() is sourced I dont want the helper function
> to be seen in the global namespace (as shown by ls()).
> 
> Currently what I have done is:
> 
> func <- function() {
>   helper1 <- function() {
>     ...
>   }
>   helper2 <- function() {
>     ...
>   }
>   # some code
> }
> 
> Is there anyway to take the functions helper1 and helper2 outside func
> (but in the same file) yet keep them from showing up in the global
> namespace when the source file for func() is loaded?
> 
> A related question is: say I move helper1 and helper2 to a file called
> helper.R
> 
> When I want to use the helper functions inside func() I can do
> source('helper.R') - but that would place them in the global namespace.
> Is it possible to source some file within a func() such that the sourced
> functions will be local to func()?
> 
> Thanks,
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Did you hear that two rabbits escaped from the zoo and so far they have
> only recaptured 116 of them?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Wed Mar 31 01:51:46 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 30 Mar 2004 18:51:46 -0500
Subject: [R] Where: package licenses
In-Reply-To: <20040330145128.74FDB85B8C@p3fed1.frb.org>
References: <20040330145128.74FDB85B8C@p3fed1.frb.org>
Message-ID: <406A0812.5030202@jhsph.edu>

Try

file.show(system.file("DESCRIPTION", package = "MASS"))

-roger

Jason.L.Higbee at stls.frb.org wrote:

> R:
> 
> This stems from my curiosity about the previous thread about  a request 
> for glm.nb code.  The issue of package licenses was brought up and I was 
> hoping for some clarification on that.  Using the function license() or 
> licence() gives info on the license for R, but something like 
> license(MASS) does not give info on the license for the MASS package 
> (perhaps it might be good to expand the license function in the way 
> described).  A quick look on CRAN didn't yield any info on package 
> licenses either. 
> 
> 
> Thanks.
> 
> Jason Higbee
> Research Associate
> Federal Reserve Bank of St. Louis
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Mar 31 02:27:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 30 Mar 2004 19:27:25 -0500
Subject: [R] Tricube weighting procedure
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AD8@usrymx25.merck.com>

Could you please explain a bit more what you mean by `tricube weighting
regression'?  The only thing I can think of that might be what you want is
loess/lowess, which uses the tricube kernel for local weighted regression.

Andy

> From: Joao Pedro W. de Azevedo
> Sent: Tuesday, March 30, 2004 6:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Tricube weighting procedure
> 
> 
> Dear R users,
> I would like to know if there is (and how I could obtain it) any R
> application that performs tricube weighting regressions.
> All the best,
> JP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From adaikalavan.ramasamy at cancer.org.uk  Wed Mar 31 02:52:20 2004
From: adaikalavan.ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 31 Mar 2004 01:52:20 +0100
Subject: [R] BoxPlots, 1 Way ANOVA and Non-Statisticians.
In-Reply-To: <4069C549.9080509@optonline.net>
Message-ID: <BNEEKOEDEDFEPEIMALPAEEFCCBAA.adaikalavan.ramasamy@cancer.org.uk>

Also check out bplot() function in the library fields.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Chuck Cleland
> Sent: 30 March 2004 20:07
> To: Carlos J. Gil Bellosta
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] BoxPlots, 1 Way ANOVA and Non-Statisticians.
>
>
> Carlos:
>    You could just give the same values for the ylim argument in each
> boxplot
>
>   par(mfrow=c(1,2), las=1)
>   boxplot(rnorm(10), rnorm(10), rnorm(10), ylim=c(-3,3))
>   boxplot(rnorm(10, 0.5, 1), rnorm(10, -.5, 1), rnorm(10), ylim=c(-3,3))
>
> Carlos J. Gil Bellosta wrote:
> > I am working in a project and I have a number of observations belonging
> > to several classes. Using a 1 Way ANOVA, I have rejected the
> equality of
> > means hypothesis with a very small p-value. However, the people I have
> > to present my results to are not statisticians and they are not very
> > likely to be much impressed by a 1.32434e-12 like number/thing.
> >
> > Therefore I have decided to make to boxplots, one for the actual data
> > and the second one for simulated data where the equality of the means
> > holds so that the difference in the distributions can be visually
> > appreciated.
> >
> > The problem is that, for the simulated values, being more regular, the
> > range of variation is smaller and, therefore, the heigth of the window
> > where their boxplot is drawn is also smaller. As a result, the
> scales of
> > the two boxplots are not the same and part of the appeal of the visual
> > approach is lost in the way.
> >
> > My question is, is there a way to make two different boxplots within a
> > "common window"? (Or rather, a common size window or, more concretely,
> > so that it spans over the same range on the vertical axis).
>
> --
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From arcane at arcanemethods.com  Wed Mar 31 03:00:52 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Tue, 30 Mar 2004 17:00:52 -0800
Subject: [R] Test
Message-ID: <406A1844.2010902@arcanemethods.com>

Sorry.  Newbie.



From ggrothendieck at myway.com  Wed Mar 31 03:33:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 31 Mar 2004 01:33:17 +0000 (UTC)
Subject: [R] rank() vs SAS proc rank
References: <OFB19DF0F3.DF907470-ON88256E67.006DE5AA-88256E67.00702410@epamail.epa.gov>
	<Pine.LNX.4.44.0403302211480.31813-100000@gannet.stats>
Message-ID: <loom.20040331T032312-96@post.gmane.org>


This afternoon (EST) there were solutions to two different problems,
which on reflection have a similarity:

Prof Brian Ripley's rank variation:

  1 + rowSums(outer(x, x, ">"))

and my unique row counts:

  "%all.==%" <- function(a,b)apply(b,2,function(x)apply(t(a) == x,2,all))
  colSums( A %all.==% t(unique(A)) )

It occurred to me that if we define the APL-style
generalized matrix multiply like this:

inner <- function(a,b,f,g){ 
		f <- match.fun(f)
		g <- match.fun(g)
                apply(b,2,function(x)apply(g(t(a),x),2,f))
}

then both problems can be put into a similar form:

   1+inner( t(w), t(w), sum, "<" )

and

   colSums( inner( A, t(unique(A)), all, "==" ) )



From ggrothendieck at myway.com  Wed Mar 31 03:59:06 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 31 Mar 2004 01:59:06 +0000 (UTC)
Subject: [R] Aggregating frequency of irregular time series
References: <7669F018DC9DD711AEC500065B3D5ABF02517959@tudor.com>
Message-ID: <loom.20040331T035348-207@post.gmane.org>



I am not 100% sure I understand what you are trying to accomplish but if
its to calculate the highest date in seriesDates for each value of 
seriesYM then you can do it without a loop like this (untested):

   tapply( seriesDates, seriesYM, max )

Whit Armstrong <Whit.Armstrong <at> tudor.com> writes:

: 
: This function will probably work for you if your dates are in the format
: yyyymmdd.  Use it to convert the series from daily to monthly, then use a 1
: period difference to calc the returns of the series.
: 
: Feel free to contact me off list with any questions.
: 
: Dates <- function(x) rownames(x)
: 
: Timebase <- function(x,monthly=T,first.day=T) {
: 	if(length(dim(x))!=2) stop("can only take 2 dimensional data.")
: 
: 	seriesDates <- as.integer(Dates(x))
: 	
: 	# year and Month
: 	seriesYM <- as.integer(seriesDates/100)
: 	seriesYM.unique <- sort(unique(seriesYM))
: 	ans.nrow <- length(seriesYM.unique)
: 
: 	# loop through all the unique months and grab the max day of month
: 	ans <- matrix(NA,nrow=ans.nrow,ncol=ncol(x))
: 	ans.dates <- vector("numeric",ans.nrow)
: 	
: 	for (i in 1:ans.nrow) {
: 	
: 		this.month <- seriesYM == seriesYM.unique[i]
: 		max.day <- max(seriesDates[this.month])		
: 		ans.dates[i] <- max.day
: 	}
: 
: 	
: 	colnames(ans) <- colnames(x)
: 	ans[] <- x[as.character(ans.dates),]
: 	
: 	if(first.day==T) {
: 		rownames(ans) <-
: paste(substring(as.character(ans.dates),1,6),"01",sep="")
: 	} else {
: 		rownames(ans) <- ans.dates
: 	}
: 	
: 	as.tseries(ans)
: }
: 
: -----Original Message-----
: From: r-help-bounces <at> stat.math.ethz.ch
: [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of Patrick Burns
: Sent: Tuesday, March 30, 2004 2:37 PM
: To: Ajay Shah
: Cc: r-help <at> stat.math.ethz.ch; Ivan Alves
: Subject: Re: [R] Aggregating frequency of irregular time series
: 
: 
: Assuming that you are using log returns, one approach (probably not the
: best) is to convert the dates to julian days, find all of the Sundays
: spanning the dates in the series, then do something along the lines of:
: 
: tapply(the.log.returns, cut(julian.days, sundays), sum)
: 
: Patrick Burns
: 
: Burns Statistics
: patrick <at> burns-stat.com
: +44 (0)20 8525 0696
: http://www.burns-stat.com
: (home of S Poetry and "A Guide for the Unwilling S User")
: 
: Ajay Shah wrote:
: 
: >>S-Plus has the function AggregateSeries() whose name is self 
: >>explanatory.  For instance one can derive monthly series from daily 
: >>ones by specifying end-of-period, averages, sums, etc.  I looked for a 
: >>similar function in the packages "its" and "tseries", but found 
: >>nothing.  I also help.searched() for aggregate to no avail.  Would 
: >>anybody be so kind to point me in the right direction?
: >>    
: >>
: >
: >I once needed a function which would convert daily stock prices into 
: >weekly returns. I know, the code is pretty bad (it is all loops and 
: >it's very slow), but my knowledge of R is weak and I really needed it, 
: >so I just used brute force. See EOF for the function. Gabor and Dirk 
: >and Brian Ripley and others on the list were very helpful to me in 
: >getting to the point where I could write this, though obviously they 
: >should not be blamed for my bad code! 
: >
: >I would be very happy if listers could give me ideas on how to do this 
: >better.
: >
: >Daily to monthly is innately easier since months are 'more normal' than 
: >weeks. I have perl code which does this, which supports 2 cases: 
: >Reporting the last traded price (LTP) of the month versus reporting the 
: >average of the month. If this is useful to you, ask me.
: >
: >What are the specialised finance libraries available with S-Plus? Can 
: >one marry R with commercial S libraries? I don't like having a 
: >dependence on commercial code, but I might be willing to compromise and 
: >buy libraries that work with R.
: >
: >Thanks,
: >
: >  
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide!
: http://www.R-project.org/posting-guide.html
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From owen.wu at sauder.ubc.ca  Wed Mar 31 06:06:53 2004
From: owen.wu at sauder.ubc.ca (Owen Wu)
Date: Tue, 30 Mar 2004 20:06:53 -0800
Subject: [R] White standard errors for lme?
Message-ID: <00a401c416d5$9af662a0$cd8f5289@commerce.ubc.ca>

How can I compute Huber/White standard errors for mixed-effects models?

The function "robcov" in the design library can only provide Huber/White standard errors for models that have a residuals(fit,type=score) function implemented, such as lrm, cph , coxph, and ordinary linear models (ols).  But I hope to do the same for lme (linear mixed-effects models).

Thanks in advance



From etptupaf at bs.ehu.es  Wed Mar 31 09:15:24 2004
From: etptupaf at bs.ehu.es (F. Tusell)
Date: Wed, 31 Mar 2004 09:15:24 +0200
Subject: [R] Failure to compile source packages in Debian
Message-ID: <406A700C.90108@bs.ehu.es>


As of quite recently I experience failures to compile source packages. 
Messages like

  * Installing *source* package 'dse1' ...
** libs
g77 -mieee-fp  -fPIC  -g -O2 -c dsefor.f -o dsefor.o
gcc -shared  -o dse1.so dsefor.o  -L/usr/lib/gcc-lib/i486-linux/3.3.3 
-L/usr/lib/gcc-lib/i486-linux/3.3.3/../../..
-lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR
/usr/bin/ld: no se puede encontrar -lg2c-pic

clearly point to the non-existence of library g2c-pic. Problem is I 
cannot locate  the .deb package
containing such library. Could some Debian user point me to the right 
location? (I am using
Debian "testing"  on a Pentium box and R ver. 1.8.1 installed from the 
.deb package.)

Thank you.

-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr?a y Estad?stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From arcane at arcanemethods.com  Wed Mar 31 10:15:51 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 00:15:51 -0800
Subject: [R] Zero Index Origin?
Message-ID: <406A7E37.5060504@arcanemethods.com>


I'm very new to R and utterly blown away by not only the 
language but the unbelievable set of packages and the 
documentation and the documentation standards and...

I was an early APL user and never lost my love for it and in 
R I find most of the essential things I loved about APL 
except for one thing.  At this early stage of my learning I 
can't yet determine if there is a way to effect what in APL 
was zero index origin, the ordinality of indexes starts with 
0 instead of 1.  Is it possible to effect that in R without 
a lot of difficulty?

I come here today from the world of DSP research and 
development where Matlab has a near hegemony.  I see no 
reason whatsoever that R couldn't replace it with a _far_ 
better and _far_ less idiosyncratic framework.  I'd be 
interested in working on a Matlab equivalent DSP package for 
R (if that isn't being done by someone) and one of the 
things most criticized about Matlab from the standpoint of 
the DSP programmer is its insistence on 1 origin indexing. 
Any feedback greatly appreciated.


Thanks,

Bob
-- 

"Things should be described as simply as possible, but no 
simpler."

                                              A. Einstein



From tobias.verbeke at bivv.be  Wed Mar 31 10:26:56 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Wed, 31 Mar 2004 10:26:56 +0200
Subject: [R] Failure to compile source packages in Debian
In-Reply-To: <1080720392.2345.18.camel@agesi.bs.ehu.es>
Message-ID: <OFF523AEA6.1F0B0153-ONC1256E68.002D90A8-C1256E68.002E5F59@BIVV.BE>





"F. Tusell" <etptupaf at bs.ehu.es> wrote on 31/03/2004 10:06:32:

[...]
>
> and yet libgc2-pic is not resolved. *HOWEVER* following your lead that
> whatever is needed must by in a library of name libg2c*, I did the
> following in /usr/lib:
>
>    ln -s libg2c0.so.0 libg2c-pic.so
>
> and everything works like a charm. Seems it si just a library that
> changed its name.

Hmm.. Doing a search on

http://www.debian.org/distrib/packages#search_contents

using options Keyword g2c-pic
            packages that contain files or directories, whose
                  names contain the keyword
            testing
            Intel x86

points to g77 packages.

Probably installing this one is a better solution ?

HTH,
Tobias



From ripley at stats.ox.ac.uk  Wed Mar 31 10:28:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Mar 2004 09:28:56 +0100 (BST)
Subject: [R] Zero Index Origin?
In-Reply-To: <406A7E37.5060504@arcanemethods.com>
Message-ID: <Pine.LNX.4.44.0403310923170.712-100000@gannet.stats>

Much of R is itself written in R, so you cannot possibly change something 
as fundamental as this.  Further, index 0 has a special meaning that you 
would lose if R have 0-based indexing.

However, the R thinking is to work with whole objects (vectors, arrays, 
lists ...) and you rather rarely need to know what numbers are in an index 
vector.  There are usages such as 1:n, and those are quite often wrong: 
they should be seq(length=n) or seq(along=x) or some such, since n might 
be zero.  If you are writing code that works with single elements, you are 
probably a lot better off writing C code to link into R (and C is 
0-based ...).

On Wed, 31 Mar 2004, Bob Cain wrote:

> 
> I'm very new to R and utterly blown away by not only the 
> language but the unbelievable set of packages and the 
> documentation and the documentation standards and...
> 
> I was an early APL user and never lost my love for it and in 
> R I find most of the essential things I loved about APL 
> except for one thing.  At this early stage of my learning I 
> can't yet determine if there is a way to effect what in APL 
> was zero index origin, the ordinality of indexes starts with 
> 0 instead of 1.  Is it possible to effect that in R without 
> a lot of difficulty?
> 
> I come here today from the world of DSP research and 
> development where Matlab has a near hegemony.  I see no 
> reason whatsoever that R couldn't replace it with a _far_ 
> better and _far_ less idiosyncratic framework.  I'd be 
> interested in working on a Matlab equivalent DSP package for 
> R (if that isn't being done by someone) and one of the 
> things most criticized about Matlab from the standpoint of 
> the DSP programmer is its insistence on 1 origin indexing. 
> Any feedback greatly appreciated.
> 
> 
> Thanks,
> 
> Bob
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s-plus at wiwi.uni-bielefeld.de  Wed Mar 31 11:15:47 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 31 Mar 2004 11:15:47 +0200
Subject: [R] Zero Index Origin?
References: <406A7E37.5060504@arcanemethods.com>
Message-ID: <406A8C43.90004@wiwi.uni-bielefeld.de>

Bob Cain wrote:

>
> I'm very new to R and utterly blown away by not only the language but 
> the unbelievable set of packages and the documentation and the 
> documentation standards and...
>
> I was an early APL user and never lost my love for it and in R I find 
> most of the essential things I loved about APL except for one thing.  
> At this early stage of my learning I can't yet determine if there is a 
> way to effect what in APL was zero index origin, the ordinality of 
> indexes starts with 0 instead of 1.  Is it possible to effect that in 
> R without a lot of difficulty?
>
> I come here today from the world of DSP research and development where 
> Matlab has a near hegemony.  I see no reason whatsoever that R 
> couldn't replace it with a _far_ better and _far_ less idiosyncratic 
> framework.  I'd be interested in working on a Matlab equivalent DSP 
> package for R (if that isn't being done by someone) and one of the 
> things most criticized about Matlab from the standpoint of the DSP 
> programmer is its insistence on 1 origin indexing. Any feedback 
> greatly appreciated.
>
>
> Thanks,
>
> Bob

Hallo Bob,

in APL we control index origin by "QUAD.IO" and  QUAD.IO \in {0,1}.
Suppose within a function index origin is unknown:
a) If we want to work with origin 0 we write   x[ i  + QUAD.IO ]
b) ... with origin 1 ...  x[ i - !QUAD.IO ]  .

So set:   QUAD.IO <- 1 and use a)   ---  apl-like.

Or define an index shift function:

io.0<-function(ind) ind+1

to be able to type  x[io.0(0:5)]

I am shure that your first experiments have been to implement APL functions
like take, drop, rotate, ... and now you are looking for a more elegant way
to manage origin 0 than "+QUAD.IO".

Peter Wolf



From tozzi at cptec.inpe.br  Wed Mar 31 13:32:49 2004
From: tozzi at cptec.inpe.br (Luiz Rodrigo Tozzi)
Date: Wed, 31 Mar 2004 08:32:49 -0300
Subject: [R] Can R be useful to me?
In-Reply-To: <406A1844.2010902@arcanemethods.com>
References: <406A1844.2010902@arcanemethods.com>
Message-ID: <1080732769.406aac61263ea@webmail>

hi

My name is Luiz Tozzi and I have an aplication of R in mind but I dont know if 
it works.

I've got a UNIX DEC machine here in my work and I have to generate statistical 
graphics and tables in GIF. We use to generate them in Excel sheets, importing 
our data to a Windows computer.

My question is: can I generate graphics and tables in gif ou any graphical 
format through shell script?? can I call R, run a package in my ascii data e 
then export the results to a gif, png or whatever?

Depending on your answers I'll learn R to do this kind ok task.

thanks in advaced,

><> ><> ><> ><> ><> ><> ><>

   Luiz Rodrigo L. Tozzi
    METOP -- CPTEC/INPE
 
    tozzi at cptec.inpe.br
    ICQ: 3276578
    TEL: (12)31868419

><> ><> ><> ><> ><> ><> ><>

Deus nunca fica em roaming
Ele esta sempre
 a uma oracao de distancia



From B.Rowlingson at lancaster.ac.uk  Wed Mar 31 13:36:48 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 31 Mar 2004 12:36:48 +0100
Subject: [R] Zero Index Origin?
In-Reply-To: <406A7E37.5060504@arcanemethods.com>
References: <406A7E37.5060504@arcanemethods.com>
Message-ID: <406AAD50.10507@lancaster.ac.uk>

Bob Cain wrote:
>  At
> this early stage of my learning I can't yet determine if there is a way 
> to effect what in APL was zero index origin, the ordinality of indexes 
> starts with 0 instead of 1.  Is it possible to effect that in R without 
> a lot of difficulty?
> 

  Clearly R wasn't written by Dijkstra:

http://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF

  This text was pointed out to me when I started using Python, which has 
zero-based indexing. Python can look so much like R, but there are 
subtle differences.


R:
  > x=c(5,4,3,2,1)
  > x[3]
  [1] 3
  > x[2:4]
  [1] 4 3 2

compare:

Python:
  >>> x=[5,4,3,2,1]
  >>> x[3]
   2
  >>> x
   [5, 4, 3, 2, 1]
  >>> x[2:4]
   [3, 2]

  A single element from a sequence in python is indexed from zero, hence 
x[3] == 2, but a range indexes from the commas between the limits of the 
range. Hence x[2:4] is the elements between comma 2 and comma 4 - hence 
its only 2 elements.

  Did my head in when I first started pythoning. Flipping between R and 
python is not recommended, kudos to all those involved in such R-python 
links...

Baz



From p.pagel at gsf.de  Wed Mar 31 13:55:33 2004
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 31 Mar 2004 13:55:33 +0200
Subject: [R] Can R be useful to me?
In-Reply-To: <1080732769.406aac61263ea@webmail>
References: <406A1844.2010902@arcanemethods.com>
	<1080732769.406aac61263ea@webmail>
Message-ID: <20040331115533.GA11882@localhost>

On Wed, Mar 31, 2004 at 08:32:49AM -0300, Luiz Rodrigo Tozzi wrote:
> 
> My question is: can I generate graphics and tables in gif ou any graphical 
> format through shell script?? can I call R, run a package in my ascii data e 
> then export the results to a gif, png or whatever?

Yes - R is well suited for such a task. Actually there even is a package
called sweave which allows you to use LaTeX to format your results and
create a perfectly fine report on the fly.

If you need to generate such reports on a regular basis R is definitely
a good way to appraoch the task.

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/~pagel



From bitwrit at ozemail.com.au  Wed Mar 31 14:01:34 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 31 Mar 2004 22:01:34 +1000
Subject: [R] Zero Index Origin?
Message-ID: <20040331115935.TRDF5908.smta02.mail.ozemail.net@there>

Hi Bob,

Jonathan Rougier's Oarray package might be what you want.

Jim



From dmurdoch at pair.com  Wed Mar 31 14:02:55 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 31 Mar 2004 07:02:55 -0500
Subject: [R] Can R be useful to me?
In-Reply-To: <1080732769.406aac61263ea@webmail>
References: <406A1844.2010902@arcanemethods.com>
	<1080732769.406aac61263ea@webmail>
Message-ID: <mmcl60t197jk8mc4kvbl70fgrkleg7k8d4@4ax.com>

On Wed, 31 Mar 2004 08:32:49 -0300, you wrote:


>My question is: can I generate graphics and tables in gif ou any graphical 
>format through shell script?? can I call R, run a package in my ascii data e 
>then export the results to a gif, png or whatever?
>
>Depending on your answers I'll learn R to do this kind ok task.

Base R doesn't have support for the GIF format, but it can produce
PNG, EPS or JPEG graphics files.  It's possible there's a
user-contributed package that does GIF, but the license restrictions
make that somewhat unlikely.

So yes, you could run R in batch mode to do what you want, as long as
GIF isn't a requirement.

Duncan Murdoch



From phgrosjean at sciviews.org  Wed Mar 31 14:08:57 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 31 Mar 2004 14:08:57 +0200
Subject: [R] as.environment() does not work for "package:base"?
In-Reply-To: <Pine.LNX.4.44.0403302125150.31813-100000@gannet.stats>
Message-ID: <MABBLJDICACNFOLGIHJOKEHLEEAA.phgrosjean@sciviews.org>

OK. Thank you. I have not seen that particularity in the documentation... is
it written somewhere?
However, I found:
> .BaseNamespaceEnv
<environment: namespace:base>

which is, according to the documentation, still experimental. Should I
better use envir = NULL or .BaseNamespaceEnv to get a full control on
objects in the base environment (this is for an object browser, so, I need
full control on EVERY object in any environment), given that I will be
careful of course with experimental stuff?

Best,

Philippe Grosjean


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Tuesday, 30 March, 2004 22:28
To: Philippe Grosjean
Cc: r-help
Subject: Re: [R] as.environment() does not work for "package:base"?


NULL is the correct answer: as an environment base is denoted by NULL.
Did you think to try

ls(envir=NULL)

?  If not, do so now.

On Tue, 30 Mar 2004, Philippe Grosjean wrote:

> Hello,
>
> I want to retrieve one environment from the search path. I have:
>
> > search()
>  [1] ".GlobalEnv"      "package:R2HTML"  "package:tcltk"
"package:methods"
> "package:ctest"
>  [6] "package:mva"     "package:modreg"  "package:nls"     "package:ts"
> "Autoloads"
> [11] "package:base"
> > as.environment(1)
> <environment: R_GlobalEnv>
> > as.environment(2)
> <environment: package:R2HTML>
> attr(,"name")
> [1] "package:R2HTML"
> attr(,"path")
> [1] "C:/PROGRA~1/R/rw1081/library/R2HTML"
> > as.environment(10)
> <environment: 01749504>
> attr(,"name")
> [1] "Autoloads"
> > as.environment(11)
> NULL
> > as.environment("package:base")
> NULL
>
> So, everything works fine, except for "package:base", the 11th entry. Is
it
> by purpose that as.environment() returns NULL? Is it because
"package:base"
> is "sealed"? Then, how do I get the environment corresponding to
> "package:base"?
>
> Thanks,
>
> Philippe Grosjean
>
> .......................................................<?}))><....
>  ) ) ) ) )
> ( ( ( ( (   Prof. Philippe Grosjean
> \  ___   )
>  \/ECO\ (   Numerical Ecology of Aquatic Systems
>  /\___/  )  Mons-Hainaut University, Pentagone
> / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
>  /NUM\/  )
>  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
>        \ )  email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
> ( ( ( ( (
> ...................................................................
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tozzi at cptec.inpe.br  Wed Mar 31 14:12:24 2004
From: tozzi at cptec.inpe.br (Luiz Rodrigo Tozzi)
Date: Wed, 31 Mar 2004 09:12:24 -0300
Subject: [R] Can R be useful to me?
Message-ID: <1080735144.406ab5a8cce63@webmail>

wow!

this R is real awesome!

thanks all of you.

i'll start the tutorial and manual reading right now

tks!

><> ><> ><> ><> ><> ><> ><>

   Luiz Rodrigo L. Tozzi
    METOP -- CPTEC/INPE
 
    tozzi at cptec.inpe.br
    ICQ: 3276578
    TEL: (12)31868419

><> ><> ><> ><> ><> ><> ><>

Deus nunca fica em roaming
Ele esta sempre
 a uma oracao de distancia



From ripley at stats.ox.ac.uk  Wed Mar 31 14:22:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Mar 2004 13:22:37 +0100 (BST)
Subject: [R] as.environment() does not work for "package:base"?
In-Reply-To: <MABBLJDICACNFOLGIHJOKEHLEEAA.phgrosjean@sciviews.org>
Message-ID: <Pine.LNX.4.44.0403311309560.1788-100000@gannet.stats>

On Wed, 31 Mar 2004, Philippe Grosjean wrote:

> OK. Thank you. I have not seen that particularity in the documentation... is
> it written somewhere?

Yes, the source code if nowhere else.

> However, I found:
> > .BaseNamespaceEnv
> <environment: namespace:base>
> 
> which is, according to the documentation, still experimental. Should I
> better use envir = NULL or .BaseNamespaceEnv to get a full control on
> objects in the base environment (this is for an object browser, so, I need
> full control on EVERY object in any environment), given that I will be
> careful of course with experimental stuff?

There is a difference between the package:foo and namespace:foo.  I don't 
know what you mean by `full control': you should regard namespaces as 
read-only and private.

Base is exceptional in many ways and it is the relationship of its 
namespace to everything else which is subject to change.

> 
> Best,
> 
> Philippe Grosjean
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Tuesday, 30 March, 2004 22:28
> To: Philippe Grosjean
> Cc: r-help
> Subject: Re: [R] as.environment() does not work for "package:base"?
> 
> 
> NULL is the correct answer: as an environment base is denoted by NULL.
> Did you think to try
> 
> ls(envir=NULL)
> 
> ?  If not, do so now.
> 
> On Tue, 30 Mar 2004, Philippe Grosjean wrote:
> 
> > Hello,
> >
> > I want to retrieve one environment from the search path. I have:
> >
> > > search()
> >  [1] ".GlobalEnv"      "package:R2HTML"  "package:tcltk"
> "package:methods"
> > "package:ctest"
> >  [6] "package:mva"     "package:modreg"  "package:nls"     "package:ts"
> > "Autoloads"
> > [11] "package:base"
> > > as.environment(1)
> > <environment: R_GlobalEnv>
> > > as.environment(2)
> > <environment: package:R2HTML>
> > attr(,"name")
> > [1] "package:R2HTML"
> > attr(,"path")
> > [1] "C:/PROGRA~1/R/rw1081/library/R2HTML"
> > > as.environment(10)
> > <environment: 01749504>
> > attr(,"name")
> > [1] "Autoloads"
> > > as.environment(11)
> > NULL
> > > as.environment("package:base")
> > NULL
> >
> > So, everything works fine, except for "package:base", the 11th entry. Is
> it
> > by purpose that as.environment() returns NULL? Is it because
> "package:base"
> > is "sealed"? Then, how do I get the environment corresponding to
> > "package:base"?
> >
> > Thanks,
> >
> > Philippe Grosjean
> >
> > .......................................................<?}))><....
> >  ) ) ) ) )
> > ( ( ( ( (   Prof. Philippe Grosjean
> > \  ___   )
> >  \/ECO\ (   Numerical Ecology of Aquatic Systems
> >  /\___/  )  Mons-Hainaut University, Pentagone
> > / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
> >  /NUM\/  )
> >  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> >        \ )  email: Philippe.Grosjean at umh.ac.be
> >  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
> > ( ( ( ( (
> > ...................................................................
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From felix_eggers at gmx.de  Wed Mar 31 14:50:18 2004
From: felix_eggers at gmx.de (Felix Eggers)
Date: Wed, 31 Mar 2004 14:50:18 +0200
Subject: [R] choice-based conjoint
Message-ID: <BC908B2A.2BF3%felix_eggers@gmx.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040331/d4c5e25d/attachment.pl

From jfox at mcmaster.ca  Wed Mar 31 15:04:13 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 31 Mar 2004 08:04:13 -0500
Subject: [R] Zero Index Origin?
In-Reply-To: <406A7E37.5060504@arcanemethods.com>
Message-ID: <20040331130412.PDTO10288.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Bob,

One approach would be to introduce a class of objects for which zero-based
indexing is implemented. Here's a simple example:

> "[.io0" <- function(x, i) as.vector(x)[i + 1]
> 
> v <- 0:10
> class(v) <- "io0"
> v[0]
[1] 0
> v[0:5]
[1] 0 1 2 3 4 5
> 

Of course, a serious implementation would handle arrays and perhaps other
kinds of objects, and would be more careful about the subscript.

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bob Cain
> Sent: Wednesday, March 31, 2004 3:16 AM
> To: R-help
> Subject: [R] Zero Index Origin?
> 
> 
> I'm very new to R and utterly blown away by not only the 
> language but the unbelievable set of packages and the 
> documentation and the documentation standards and...
> 
> I was an early APL user and never lost my love for it and in 
> R I find most of the essential things I loved about APL 
> except for one thing.  At this early stage of my learning I 
> can't yet determine if there is a way to effect what in APL 
> was zero index origin, the ordinality of indexes starts with 
> 0 instead of 1.  Is it possible to effect that in R without a 
> lot of difficulty?
> 
> I come here today from the world of DSP research and 
> development where Matlab has a near hegemony.  I see no 
> reason whatsoever that R couldn't replace it with a _far_ 
> better and _far_ less idiosyncratic framework.  I'd be 
> interested in working on a Matlab equivalent DSP package for 
> R (if that isn't being done by someone) and one of the things 
> most criticized about Matlab from the standpoint of the DSP 
> programmer is its insistence on 1 origin indexing. 
> Any feedback greatly appreciated.
>



From ggrothendieck at myway.com  Wed Mar 31 15:19:38 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 31 Mar 2004 13:19:38 +0000 (UTC)
Subject: [R] Zero Index Origin?
References: <406A7E37.5060504@arcanemethods.com>
Message-ID: <loom.20040331T150145-904@post.gmane.org>


If you are willing to do it yourself you can define a class 
for which indexing behaves that way.

For example, here is a start for a limited implementation for
vectors.  The first statement defines the constructor, the
second defines [, the third converts an index 0 based vector
back to a regular vector, the next implements lvalues and
the last provides print method.

as.vector0 <- function(x) structure(x, class="vector0")
"[.vector0" <- function(x,i) as.vector0(as.vector.vector0(x)[i+1])
as.vector.vector0 <- function(x) unclass(x)
"[<-.vector0" <- function(x,i,value) { 
              x <- as.vector.vector0(x)
              x[i+1] <- value
              as.vector0(x)
}
print.vector0 <- function(x) print(as.vector.vector0(x))

# Test:
x <- as.vector0(1:10)
x[0:4] <- 100 * x[0:4]
x


Bob Cain <arcane <at> arcanemethods.com> writes:

: 
: I'm very new to R and utterly blown away by not only the 
: language but the unbelievable set of packages and the 
: documentation and the documentation standards and...
: 
: I was an early APL user and never lost my love for it and in 
: R I find most of the essential things I loved about APL 
: except for one thing.  At this early stage of my learning I 
: can't yet determine if there is a way to effect what in APL 
: was zero index origin, the ordinality of indexes starts with 
: 0 instead of 1.  Is it possible to effect that in R without 
: a lot of difficulty?
: 
: I come here today from the world of DSP research and 
: development where Matlab has a near hegemony.  I see no 
: reason whatsoever that R couldn't replace it with a _far_ 
: better and _far_ less idiosyncratic framework.  I'd be 
: interested in working on a Matlab equivalent DSP package for 
: R (if that isn't being done by someone) and one of the 
: things most criticized about Matlab from the standpoint of 
: the DSP programmer is its insistence on 1 origin indexing. 
: Any feedback greatly appreciated.
: 
: Thanks,
: 
: Bob



From baron at psych.upenn.edu  Wed Mar 31 15:25:58 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 31 Mar 2004 08:25:58 -0500
Subject: [R] choice-based conjoint
In-Reply-To: <BC908B2A.2BF3%felix_eggers@gmx.de>
References: <BC908B2A.2BF3%felix_eggers@gmx.de>
Message-ID: <20040331132558.GA25238@psych>

Not quite an answer.  I've been using ace() in the acepack
library to do _ordinary_ conjoint analysis with rating responses.
I never do (nor have any reason to do) choice-based conjoint, so
I haven't thought about how to analyze such data, but I'd be
interested in the answer if anyone has it.  (What would happen if
you just use ace() with a binary dependent variable?  What if the
stimuli are two alternative options rather than one option
vs. a constant default?)

For examples of scripts that I used with an undergrad class, see
http://www.sas.upenn.edu/~baron/353/
particularly
http://www.sas.upenn.edu/~baron/353/ca1.R

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From stephen.kay at adelphigroup.com  Wed Mar 31 15:31:22 2004
From: stephen.kay at adelphigroup.com (stephen.kay@adelphigroup.com)
Date: Wed, 31 Mar 2004 08:31:22 -0500
Subject: [R] choice-based conjoint
Message-ID: <F9C21EA3118FF64BB2A83E914E11BA4878EAC2@ADE-EXCH-004>

The multinomial model can be used- it's the standard method used, although
there are now more sophisticated ones - see Kenneth Train's website at
Berkley for the Mixed Logit. I too am new to R so can't really comment on
how to implement it in this package. If you want to run more sophisticated
models (e.g. Mixed Logit) and you are an academic (non-commercial) you could
download a free version of Ox (a matrix programming language very similar to
GAUSS) and do a search for Mixed Logit - some one wrote a free routine that
converts Ken Train's GAUSS code to Ox. Afraid I don't have time to look up
the website addresses.

Hope this helps,

Stephen
Stephen Kay
Head of Statistics

Adelphi Group Products
www.adelphigroup.com


-----Original Message-----
From: Felix Eggers [mailto:felix_eggers at gmx.de]
Sent: 31 March 2004 13:50
To: r-help at stat.math.ethz.ch
Subject: [R] choice-based conjoint


Hello everyone,

I am new to this list and the R-Project, so I hope my question is not
trivial or has been answered before. I searched the FAQs and the mailing
list archives and I could not find anything about Conjoint Analysis. I am
especially interested in Choice-based Conjoint, resp. discrete choice
models. 

Is there a function / module that handles this issue? Or can the multinomial
logit model be used? I would doubt the latter, for in a Choice-based
Conjoint analysis the choice has to be conditioned to the alternatives in
the choice set which are not equal for the respondents.

Any help will be much appreciated! Thank you!

Felix

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
DISCLAIMER: The information in this message is confidential ...{{dropped}}



From Whit.Armstrong at tudor.com  Wed Mar 31 15:43:53 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed, 31 Mar 2004 08:43:53 -0500
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF0251796F@tudor.com>

You are quite right.

One can see from the original function I wrote that I am not very familiar
with the apply family.

Thanks for pointing out this improvement.

Below is the new version of the function; any other comments are welcome.

Regards,
Whit

Timebase <- function(x,monthly=T,first.day=T) {
	if(length(dim(x))!=2) stop("can only take 2 dimensional data.")

	seriesDates <- as.integer(Dates(x))
	
	# year and Month
	seriesYM <- as.integer(seriesDates/100)
	
	# for each seriesYM grab the max day of month
	ans.dates <- as.numeric(tapply( seriesDates, seriesYM, max ))

	ans <- matrix(NA,nrow=length(ans.dates),ncol=ncol(x))
	
	colnames(ans) <- colnames(x)
	ans[] <- x[as.character(ans.dates),]
	
	if(first.day==T) {
		rownames(ans) <-
paste(substring(as.character(ans.dates),1,6),"01",sep="")
	} else {
		rownames(ans) <- ans.dates
	}
	
	as.tseries(ans)
}

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: Tuesday, March 30, 2004 8:59 PM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Aggregating frequency of irregular time series




I am not 100% sure I understand what you are trying to accomplish but if its
to calculate the highest date in seriesDates for each value of 
seriesYM then you can do it without a loop like this (untested):

   tapply( seriesDates, seriesYM, max )

Whit Armstrong <Whit.Armstrong <at> tudor.com> writes:

: 
: This function will probably work for you if your dates are in the format
: yyyymmdd.  Use it to convert the series from daily to monthly, then use a
1
: period difference to calc the returns of the series.
: 
: Feel free to contact me off list with any questions.
: 
: Dates <- function(x) rownames(x)
: 
: Timebase <- function(x,monthly=T,first.day=T) {
: 	if(length(dim(x))!=2) stop("can only take 2 dimensional data.")
: 
: 	seriesDates <- as.integer(Dates(x))
: 	
: 	# year and Month
: 	seriesYM <- as.integer(seriesDates/100)
: 	seriesYM.unique <- sort(unique(seriesYM))
: 	ans.nrow <- length(seriesYM.unique)
: 
: 	# loop through all the unique months and grab the max day of month
: 	ans <- matrix(NA,nrow=ans.nrow,ncol=ncol(x))
: 	ans.dates <- vector("numeric",ans.nrow)
: 	
: 	for (i in 1:ans.nrow) {
: 	
: 		this.month <- seriesYM == seriesYM.unique[i]
: 		max.day <- max(seriesDates[this.month])		
: 		ans.dates[i] <- max.day
: 	}
: 
: 	
: 	colnames(ans) <- colnames(x)
: 	ans[] <- x[as.character(ans.dates),]
: 	
: 	if(first.day==T) {
: 		rownames(ans) <-
: paste(substring(as.character(ans.dates),1,6),"01",sep="")
: 	} else {
: 		rownames(ans) <- ans.dates
: 	}
: 	
: 	as.tseries(ans)
: }
: 
: -----Original Message-----
: From: r-help-bounces <at> stat.math.ethz.ch
: [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of Patrick Burns
: Sent: Tuesday, March 30, 2004 2:37 PM
: To: Ajay Shah
: Cc: r-help <at> stat.math.ethz.ch; Ivan Alves
: Subject: Re: [R] Aggregating frequency of irregular time series
: 
: 
: Assuming that you are using log returns, one approach (probably not the
: best) is to convert the dates to julian days, find all of the Sundays
: spanning the dates in the series, then do something along the lines of:
: 
: tapply(the.log.returns, cut(julian.days, sundays), sum)
: 
: Patrick Burns
: 
: Burns Statistics
: patrick <at> burns-stat.com
: +44 (0)20 8525 0696
: http://www.burns-stat.com
: (home of S Poetry and "A Guide for the Unwilling S User")
: 
: Ajay Shah wrote:
: 
: >>S-Plus has the function AggregateSeries() whose name is self 
: >>explanatory.  For instance one can derive monthly series from daily 
: >>ones by specifying end-of-period, averages, sums, etc.  I looked for a 
: >>similar function in the packages "its" and "tseries", but found 
: >>nothing.  I also help.searched() for aggregate to no avail.  Would 
: >>anybody be so kind to point me in the right direction?
: >>    
: >>
: >
: >I once needed a function which would convert daily stock prices into 
: >weekly returns. I know, the code is pretty bad (it is all loops and 
: >it's very slow), but my knowledge of R is weak and I really needed it, 
: >so I just used brute force. See EOF for the function. Gabor and Dirk 
: >and Brian Ripley and others on the list were very helpful to me in 
: >getting to the point where I could write this, though obviously they 
: >should not be blamed for my bad code! 
: >
: >I would be very happy if listers could give me ideas on how to do this 
: >better.
: >
: >Daily to monthly is innately easier since months are 'more normal' than 
: >weeks. I have perl code which does this, which supports 2 cases: 
: >Reporting the last traded price (LTP) of the month versus reporting the 
: >average of the month. If this is useful to you, ask me.
: >
: >What are the specialised finance libraries available with S-Plus? Can 
: >one marry R with commercial S libraries? I don't like having a 
: >dependence on commercial code, but I might be willing to compromise and 
: >buy libraries that work with R.
: >
: >Thanks,
: >
: >  
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide!
: http://www.R-project.org/posting-guide.html
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
: 
:

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hhan at cse.psu.edu  Wed Mar 31 16:10:59 2004
From: hhan at cse.psu.edu (Hui Han)
Date: Wed, 31 Mar 2004 09:10:59 -0500
Subject: [R] help with the usage of "randomForest"
Message-ID: <20040331141059.GA28880@andorian.cse.psu.edu>

Dear all,

Can anybody give me some hint on the following error msg I got with using 
randomForest?

I have two-class classification problem. The data file "sample" is:
----------------------------------------------------------
 udomain.edu udomain.hcs hpclass
1 1.0000 1 not
2 NA 2 not
3 NA 0.8 not
4 NA 0.2 hp
5 NA 0.9 hp
------------------------------------------------------------
The steps I called the function are:
(1) Read data
hp <- read.table("sample")
(2) Call randomForest
hp.rf <- randomForest(hpclass ~., yy, data=hp, importance=TRUE, 
proximity=TRUE)

But the error msg I got is:
Error in randomForest.default(m, y, ...) :
        Need at least two classes to do classification.


I learned the usage of randomForest from:
http://www.maths.lth.se/help/R/.R/library/randomForest/html/randomForest.html

Thanks a lot for any of your comments in advance!


Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan



From edd at debian.org  Wed Mar 31 16:21:32 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 31 Mar 2004 08:21:32 -0600
Subject: [R] Failure to compile source packages in Debian
In-Reply-To: <406A700C.90108@bs.ehu.es>
References: <406A700C.90108@bs.ehu.es>
Message-ID: <20040331142132.GA22951@sonny.eddelbuettel.com>

On Wed, Mar 31, 2004 at 09:15:24AM +0200, F. Tusell wrote:
> 
> As of quite recently I experience failures to compile source packages. 
> Messages like
> 
>  * Installing *source* package 'dse1' ...
> ** libs
> g77 -mieee-fp  -fPIC  -g -O2 -c dsefor.f -o dsefor.o
> gcc -shared  -o dse1.so dsefor.o  -L/usr/lib/gcc-lib/i486-linux/3.3.3 
> -L/usr/lib/gcc-lib/i486-linux/3.3.3/../../..
> -lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR
> /usr/bin/ld: no se puede encontrar -lg2c-pic
> 
> clearly point to the non-existence of library g2c-pic. Problem is I 
> cannot locate  the .deb package
> containing such library. Could some Debian user point me to the right 
> location? (I am using
> Debian "testing"  on a Pentium box and R ver. 1.8.1 installed from the 
> .deb package.)

The g77 packages used to provide a g2c-pic library. E.g. when 1.8.1 was
built, they did and thusly the Depends ended up in /etc/R/Makeconf. However,
on your current system, g77 no longer provides the library -- so this fails.

You need to bring R and g77 back in sync. There are several possibilities:

a) install the current r-base-core and r-base-dev packages from unstable
b) recompile the 1.8.1 package locally to reflect to reflect your current
   compiler setup
c) edit /etc/R/Makeconf  

I'm sorry for the bug, but these things are hard to control for.

Dirk


-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From B.Rowlingson at lancaster.ac.uk  Wed Mar 31 15:38:27 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 31 Mar 2004 14:38:27 +0100
Subject: [R] Zero Index Origin?
In-Reply-To: <loom.20040331T150145-904@post.gmane.org>
References: <406A7E37.5060504@arcanemethods.com>
	<loom.20040331T150145-904@post.gmane.org>
Message-ID: <406AC9D3.3000101@lancaster.ac.uk>

Gabor Grothendieck wrote:
> If you are willing to do it yourself you can define a class 
> for which indexing behaves that way.

  I'd like to prefix all these solutions with "Here's how to do it, but 
don't actually do it you crazy fool". It's on a par with redefining pi, 
or redefining '+'. And then redefining '<-'.

  These techniques have their proper place, and that would be in the 
currently non-existent obfuscated R contest.

  No, the R-ish (iRish?) way is to index vectors from 1. That's what the 
R gods intended!


Baz



From tpapp at axelero.hu  Wed Mar 31 16:25:03 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Wed, 31 Mar 2004 16:25:03 +0200
Subject: [R] array addition doesn't recycle!
Message-ID: <20040331142503.GA837@localhost>

Hi,

I have noticed the following:

> a <- array(1:4, c(2, 2))
> A <- array(1:4, c(2,2,2))
> A + a
Error in A + a : non-conformable arrays

It works with a matrix + a vector, why doesn't it work with arrays?
Am I missing something?

How would you do the above operation efficiently (ie I need to add a
matrix to each "plane" of 3-dim array)?  At the moment I am using
something like

A + array(a, c(2,2,2))

but it doesn't seem that efficient.

Thanks

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From dsheuman at rogers.com  Wed Mar 31 16:44:15 2004
From: dsheuman at rogers.com (dsheuman@rogers.com)
Date: Wed, 31 Mar 2004 9:44:15 -0500
Subject: [R] Removing leading and trailing spaces (string manipulation)
Message-ID: <20040331144415.XPRX163224.fep04-mail.bloor.is.net.cable.rogers.com@localhost>

Hi all,

I'm running the following code to generate 40 different jpegs based on the resulting data.  I'd like the file names to be 'Cluster1.jpeg', however the code write filenames like 'Cluster 1 .jpeg'.

How can I get rid of the unwanted spaces?  I've looked at ?format and it doesn't seem to work - at least in this context.


###################
ClusCount <- 40

datain <- as.data.frame( read.csv("c:\\daclus.csv"))

for(i in 1:ClusCount){
	mapit <- subset(datain, cluster == i)
	jpeg(file=paste("c:\\temp\\cluster",format(i),".jpeg"), width = 640, height = 480, pointsize = 12,quality = 300, bg = "white")
	plot( as.matrix(mapit[,2]), as.matrix(mapit[,3]),xlim=c(-141.6,-52.1),ylim=c(41.5,83.7),  type = "p", main = paste("Cluster",i) )
	dev.off()
}
###################


Thanks,

Danny



From HankeA at mar.dfo-mpo.gc.ca  Wed Mar 31 16:16:58 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 31 Mar 2004 10:16:58 -0400
Subject: [R] identify()  and controlling label size
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124979@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040331/7fd985bb/attachment.pl

From sdavis2 at mail.nih.gov  Wed Mar 31 16:55:22 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 31 Mar 2004 09:55:22 -0500
Subject: [R] Removing leading and trailing spaces (string manipulation)
In-Reply-To: <20040331144415.XPRX163224.fep04-mail.bloor.is.net.cable.rogers.com@localhost>
Message-ID: <BC90460A.66C8%sdavis2@mail.nih.gov>

Danny,

Paste uses sep=" " between arguments as a default.  Just include sep="" in
the paste call.

file=paste("c:\\temp\\cluster",format(i),".jpeg",sep="")

Sean

On 3/31/04 9:44 AM, "dsheuman at rogers.com" <dsheuman at rogers.com> wrote:

> Hi all,
> 
> I'm running the following code to generate 40 different jpegs based on the
> resulting data.  I'd like the file names to be 'Cluster1.jpeg', however the
> code write filenames like 'Cluster 1 .jpeg'.
> 
> How can I get rid of the unwanted spaces?  I've looked at ?format and it
> doesn't seem to work - at least in this context.
> 
> 
> ###################
> ClusCount <- 40
> 
> datain <- as.data.frame( read.csv("c:\\daclus.csv"))
> 
> for(i in 1:ClusCount){
> mapit <- subset(datain, cluster == i)
> jpeg(file=paste("c:\\temp\\cluster",format(i),".jpeg"), width = 640, height =
> 480, pointsize = 12,quality = 300, bg = "white")
> plot( as.matrix(mapit[,2]),
> as.matrix(mapit[,3]),xlim=c(-141.6,-52.1),ylim=c(41.5,83.7),  type = "p", main
> = paste("Cluster",i) )
> dev.off()
> }
> ###################
> 
> 
> Thanks,
> 
> Danny
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tpapp at axelero.hu  Wed Mar 31 16:51:56 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Wed, 31 Mar 2004 16:51:56 +0200
Subject: [R] extracting values from a 3d array using a matrix from indices
Message-ID: <20040331145156.GA1008@localhost>

Suppose I have A, an n x m matrix, each element is an integer (an
index).

I also have B, an n x l x m array.  I need C, where

C[n,m] = B[n, A[n, m], m]

I am currently using loops, what would be the "R way" to do this?

Another question: let

A[n, m] <- argmax_l B[n, l, m]

what would be the nicest way of doing this?  Currently I am using
max.col and a single loop, going though the n's.

Background: I solving a discrete-space dynamic programming problem, A
is the optimal policy, C is the value function.  The structure of the
problem allows me to use matrices like above, instead of (n * m) x (n
x m) square matrices.

Thanks

Tamas

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From MSchwartz at MedAnalytics.com  Wed Mar 31 16:53:38 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 31 Mar 2004 08:53:38 -0600
Subject: [R] Removing leading and trailing spaces (string manipulation)
In-Reply-To: <20040331144415.XPRX163224.fep04-mail.bloor.is.net.cable.rogers.com@localhost>
References: <20040331144415.XPRX163224.fep04-mail.bloor.is.net.cable.rogers.com@localhost>
Message-ID: <1080744818.8937.11.camel@localhost.localdomain>

On Wed, 2004-03-31 at 08:44, dsheuman at rogers.com wrote:
> Hi all,
> 
> I'm running the following code to generate 40 different jpegs based on
> the resulting data.  I'd like the file names to be 'Cluster1.jpeg',
> however the code write filenames like 'Cluster 1 .jpeg'.
> 
> How can I get rid of the unwanted spaces?  I've looked at ?format and
> it doesn't seem to work - at least in this context.
> 
> 
> ###################
> ClusCount <- 40
> 
> datain <- as.data.frame( read.csv("c:\\daclus.csv"))
> 
> for(i in 1:ClusCount){
> 	mapit <- subset(datain, cluster == i)
> 	jpeg(file=paste("c:\\temp\\cluster",format(i),".jpeg"), width = 640,
> height = 480, pointsize = 12,quality = 300, bg = "white")
> 	plot( as.matrix(mapit[,2]),
> as.matrix(mapit[,3]),xlim=c(-141.6,-52.1),ylim=c(41.5,83.7),  type =
> "p", main = paste("Cluster",i) )
> 	dev.off()
> }
> ###################
> 
> 
> Thanks,
> 
> Danny


Change the code in your 'jpeg' statement to read:

jpeg(file=paste("c:\\temp\\cluster",format(i),".jpeg", sep = ""), ...
                                                       ^^^^^^^^

You can use the 'sep' argument (which defaults to " ") to eliminate the
spaces between the pasted components.

See ?paste for more information.

HTH,

Marc Schwartz



From B.Rowlingson at lancaster.ac.uk  Wed Mar 31 16:58:38 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 31 Mar 2004 15:58:38 +0100
Subject: [R] identify()  and controlling label size
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE124979@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE124979@msgmarsta01.bio.dfo.ca>
Message-ID: <406ADC9E.5020602@lancaster.ac.uk>

Hanke, Alex wrote:
> I thought this was going to be easy ...
> Can the label size of identify() be controlled by setting par(cex.*) because
> I'm having no luck? My only recourse is to save the index and position of
> the labels from identify() and use text() to replot them.
> 

  Funny, it seems to ignore that, but allows 'col=' and 'font=' to set 
the font colour and type.

  But! It accepts 'ps=' to set the point size! You may be saved...

Try:

plot(1:10)
identify(1:10,ps=24)
identify(1:10,ps=12)

Baz



From ripley at stats.ox.ac.uk  Wed Mar 31 16:59:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Mar 2004 15:59:36 +0100 (BST)
Subject: [R] array addition doesn't recycle!
In-Reply-To: <20040331142503.GA837@localhost>
Message-ID: <Pine.LNX.4.44.0403311556250.2288-100000@gannet.stats>

The recycling rules are documented and this is not amongst them.
Computer packages do have a tendency to follow their rules rather than 
read your mind.

I suspect A + as.vector(a) is what you intended.

On Wed, 31 Mar 2004, Tamas Papp wrote:

> Hi,
> 
> I have noticed the following:
> 
> > a <- array(1:4, c(2, 2))
> > A <- array(1:4, c(2,2,2))
> > A + a
> Error in A + a : non-conformable arrays
> 
> It works with a matrix + a vector, why doesn't it work with arrays?
> Am I missing something?
> 
> How would you do the above operation efficiently (ie I need to add a
> matrix to each "plane" of 3-dim array)?  At the moment I am using
> something like
> 
> A + array(a, c(2,2,2))
> 
> but it doesn't seem that efficient.

Why do you think is `not that efficient'?  Does you have a need to save 
microseconds?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Fear at synequanon.com  Wed Mar 31 16:50:14 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 31 Mar 2004 15:50:14 +0100
Subject: [R] Removing leading and trailing spaces (string manipulation)
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE5720118F49B@synequanon01>

I believe all you need to do here is use plain "i" instead of
"format(i)" in the jpeg call.

Someone is bound to come up with a better generic
answer but this should get you up and running for now.


> -----Original Message-----
> From: dsheuman at rogers.com [mailto:dsheuman at rogers.com]
> Sent: 31 March 2004 15:44
> To: R-help at stat.math.ethz.ch
> Subject: [R] Removing leading and trailing spaces (string 
> manipulation)
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Hi all,
> 
> I'm running the following code to generate 40 different jpegs 
> based on the resulting data.  I'd like the file names to be 
> 'Cluster1.jpeg', however the code write filenames like 
> 'Cluster 1 .jpeg'.
> 
> How can I get rid of the unwanted spaces?  I've looked at 
> ?format and it doesn't seem to work - at least in this context.
> 
> 
> ###################
> ClusCount <- 40
> 
> datain <- as.data.frame( read.csv("c:\\daclus.csv"))
> 
> for(i in 1:ClusCount){
> 	mapit <- subset(datain, cluster == i)
> 	jpeg(file=paste("c:\\temp\\cluster",format(i),".jpeg"), 
> width = 640, height = 480, pointsize = 12,quality = 300, bg = "white")
> 	plot( as.matrix(mapit[,2]), 
> as.matrix(mapit[,3]),xlim=c(-141.6,-52.1),ylim=c(41.5,83.7),  
> type = "p", main = paste("Cluster",i) )
> 	dev.off()
> }
> ###################
> 
> 
> Thanks,
> 
> Danny
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From dsheuman at rogers.com  Wed Mar 31 17:11:56 2004
From: dsheuman at rogers.com (dsheuman@rogers.com)
Date: Wed, 31 Mar 2004 10:11:56 -0500
Subject: [R] Removing leading and trailing spaces (string manipulation) 
Message-ID: <20040331151156.WFYQ138614.fep01-mail.bloor.is.net.cable.rogers.com@localhost>

Thanks to Marc, J.R. and Sean for your help.  I didn't realize that the paste command was where I should be looking.

Danny



From russ at idesiresuccess.com  Wed Mar 31 17:39:07 2004
From: russ at idesiresuccess.com ('I DESIRE' Success)
Date: Wed, 31 Mar 2004 10:39:07 -0500
Subject: [R] Please confirm your subscription.
Message-ID: <1080746443.572971@autocontactor.com>


Hi , 

It's 'I DESIRE' Success writing you to confirm your subscription  
to our mailing list. 

You can confirm your subscription by simply clicking 
the link below: 

https://www.mcssl.com/app/optin.asp?j=0&c=18836264&pg=das&nc=1 

If you received this message in error, or do not wish 
to be included in future mailings, you do not have to do anything
Simply, delete the message and your information will be removed by 
our system automatically. 

You can contact us at russ at idesiresuccess.com, for more information on our
service. 

Regards, 

'I DESIRE' Success



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Mar 31 18:26:36 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 31 Mar 2004 18:26:36 +0200 (CEST)
Subject: [R] help with the usage of "randomForest"
In-Reply-To: <20040331141059.GA28880@andorian.cse.psu.edu>
References: <20040331141059.GA28880@andorian.cse.psu.edu>
Message-ID: <Pine.LNX.4.51.0403311825380.14189@artemis.imbe.med.uni-erlangen.de>

On Wed, 31 Mar 2004, Hui Han wrote:

> Dear all,
>
> Can anybody give me some hint on the following error msg I got with using
> randomForest?
>
> I have two-class classification problem. The data file "sample" is:
> ----------------------------------------------------------
>  udomain.edu udomain.hcs hpclass
> 1 1.0000 1 not
> 2 NA 2 not
> 3 NA 0.8 not
> 4 NA 0.2 hp
> 5 NA 0.9 hp
> ------------------------------------------------------------
> The steps I called the function are:
> (1) Read data
> hp <- read.table("sample")

most probably a problem here. say

R> summary(hp)

and check if the factor `hpclass' has two levels.

Torsten

> (2) Call randomForest
> hp.rf <- randomForest(hpclass ~., yy, data=hp, importance=TRUE,
> proximity=TRUE)
>
> But the error msg I got is:
> Error in randomForest.default(m, y, ...) :
>         Need at least two classes to do classification.
>
>
> I learned the usage of randomForest from:
> http://www.maths.lth.se/help/R/.R/library/randomForest/html/randomForest.html
>
> Thanks a lot for any of your comments in advance!
>
>
> Hui Han
> Department of Computer Science and Engineering,
> The Pennsylvania State University
> University Park, PA,16802
> email: hhan at cse.psu.edu
> homepage: http://www.cse.psu.edu/~hhan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From hhan at cse.psu.edu  Wed Mar 31 18:35:59 2004
From: hhan at cse.psu.edu (Hui Han)
Date: Wed, 31 Mar 2004 11:35:59 -0500
Subject: [R] help with the usage of "randomForest"
In-Reply-To: <Pine.LNX.4.51.0403311825380.14189@artemis.imbe.med.uni-erlangen.de>
References: <20040331141059.GA28880@andorian.cse.psu.edu>
	<Pine.LNX.4.51.0403311825380.14189@artemis.imbe.med.uni-erlangen.de>
Message-ID: <20040331163558.GA29048@andorian.cse.psu.edu>

Thanks for Matt and Torsten for very helpful suggestions!
As Matt pointed out, the problem is that na.action has the default value of na.fail, that
deleted one class samples. I changed all NAs to real values, and the error msg.
dissappeared. 

However my real dataset contains many NAs. I wonder if anybody can point me any documentations on
how to define na.action not be na.fail?

Best regards,
Hui

On Wed, Mar 31, 2004 at 06:26:36PM +0200, Torsten Hothorn wrote:
> On Wed, 31 Mar 2004, Hui Han wrote:
> 
> > Dear all,
> >
> > Can anybody give me some hint on the following error msg I got with using
> > randomForest?
> >
> > I have two-class classification problem. The data file "sample" is:
> > ----------------------------------------------------------
> >  udomain.edu udomain.hcs hpclass
> > 1 1.0000 1 not
> > 2 NA 2 not
> > 3 NA 0.8 not
> > 4 NA 0.2 hp
> > 5 NA 0.9 hp
> > ------------------------------------------------------------
> > The steps I called the function are:
> > (1) Read data
> > hp <- read.table("sample")
> 
> most probably a problem here. say
> 
> R> summary(hp)
> 
> and check if the factor `hpclass' has two levels.
> 
> Torsten
> 
> > (2) Call randomForest
> > hp.rf <- randomForest(hpclass ~., yy, data=hp, importance=TRUE,
> > proximity=TRUE)
> >
> > But the error msg I got is:
> > Error in randomForest.default(m, y, ...) :
> >         Need at least two classes to do classification.
> >
> >
> > I learned the usage of randomForest from:
> > http://www.maths.lth.se/help/R/.R/library/randomForest/html/randomForest.html
> >
> > Thanks a lot for any of your comments in advance!
> >
> >
> > Hui Han
> > Department of Computer Science and Engineering,
> > The Pennsylvania State University
> > University Park, PA,16802
> > email: hhan at cse.psu.edu
> > homepage: http://www.cse.psu.edu/~hhan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >


Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan



From n.soranzo at ucl.ac.uk  Wed Mar 31 18:52:54 2004
From: n.soranzo at ucl.ac.uk (Nicole Soranzo)
Date: Wed, 31 Mar 2004 17:52:54 +0100
Subject: [R] (no subject)
Message-ID: <5.1.0.14.0.20040331174824.00ba2bc0@pop-server.ucl.ac.uk>

Hi, I wonder if you can help me:

I cannot seem to be able to import my data anymore. When I try to import 
the attached file with the string

FunctRes<-read.table("C:/Documents and Settings/FunctRes.txt", header=FALSE)

I obtain:

 > FunctRes
     V1
1  ??C
2   \n
3    C
4    0
5    0
6    B
7   \n
8    C
9    0
10   1
11   B
12  \n
13   C
14   0
15   0
16   B
17  \n
18   C
19   0
20   1
21   B
22  \n
23   C
24   0
25   1
26   B
27  \n
28   C
29   0
30   1
31   B

Can you help me?
Thanks
Nicole



Dr Nicole Soranzo

Centre for Population Genetics and Human Health

University College London
Darwin Building
Gower Street
London WC1E 6BT

Tel: +44 (0)20 7679 4397
Fax: +44 (0)20 7679 2887

E-mail: n.soranzo at ucl.ac.uk
http://popgen.biol.ucl.ac.uk/
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: FunctRes.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040331/3ce0dd9b/FunctRes.txt

From hhan at cse.psu.edu  Wed Mar 31 19:11:12 2004
From: hhan at cse.psu.edu (Hui Han)
Date: Wed, 31 Mar 2004 12:11:12 -0500
Subject: [R] help with the usage of "randomForest"
In-Reply-To: <E7D5AB4811D20B489622AABA9C538591A324FE@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C538591A324FE@teal-exch.amgen.com>
Message-ID: <20040331171112.GB29102@andorian.cse.psu.edu>

Thinking that the following suggestions by Matt may be helpful to others,
I am fowarding his notes to R-list.

Regards,
Hui

On Wed, Mar 31, 2004 at 08:57:13AM -0800, Austin, Matt wrote:
> Use na.action=na.omit in your function call to delete those rows, but this
> can give you problems if you want to use follow-up methods such as the
> partial.plot(). This is what I usually do:
> 
> naRows <- apply(data2, 1, function(x) any(is.na(x)))
> 
> sum(!(naRows))
> 
> data2.noNAs <- data2[!naRows,]
> 
> chg.rf <- randomForest(ch13 ~ .,data=data2.noNAs, importance=TRUE,
> keep.forest=TRUE)
> 
> 
> That way when I call partial.plot() like in the following example I don't
> run into trouble with NAs in the original dataset not matching with what was
> used in the random forest fit.
> 
> 
> postscript("temp.ps", horizontal=TRUE)
> par(mfrow=c(4,4))
> for(i in 1:length(varNames)){
>   partial.plot(chg.f, data2.noNAs, varNames[i], ylim=c(.95, 1.7))
> }
> dev.off()
> 
> 
> -----Original Message-----
> From: Hui Han [mailto:hhan at cse.psu.edu]
> Sent: Wednesday, March 31, 2004 8:12 AM
> To: Austin, Matt
> Subject: Re: [R] help with the usage of "randomForest"
> 
> 
> Matt,
> 
> I appreciate your help so much!! Yes, I changed all NAs to real values, and
> the error msg. disappeared.
> However my real dataset contains many NAs. Can you give me more suggestions
> on how to define na.action not be na.fail?
> 
> Thank you so much again,
> 
> Hui
> 
> On Wed, Mar 31, 2004 at 08:02:47AM -0800, Austin, Matt wrote:
> > What is yy?  Is this your subset index?  If so make sure that you are not
> > removing all of one class.  Note that the default na.action in
> randomForest
> > is na.fail, so even if your subsetting isn't removing all of the rows with
> > an NA the method should still fail.
> > 
> > --Matt
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Hui Han
> > Sent: Wednesday, March 31, 2004 6:11 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] help with the usage of "randomForest"
> > 
> > 
> > Dear all,
> > 
> > Can anybody give me some hint on the following error msg I got with using 
> > randomForest?
> > 
> > I have two-class classification problem. The data file "sample" is:
> > ----------------------------------------------------------
> >  udomain.edu udomain.hcs hpclass
> > 1 1.0000 1 not
> > 2 NA 2 not
> > 3 NA 0.8 not
> > 4 NA 0.2 hp
> > 5 NA 0.9 hp
> > ------------------------------------------------------------
> > The steps I called the function are:
> > (1) Read data
> > hp <- read.table("sample")
> > (2) Call randomForest
> > hp.rf <- randomForest(hpclass ~., yy, data=hp, importance=TRUE, 
> > proximity=TRUE)
> > 
> > But the error msg I got is:
> > Error in randomForest.default(m, y, ...) :
> >         Need at least two classes to do classification.
> > 
> > 
> > I learned the usage of randomForest from:
> >
> http://www.maths.lth.se/help/R/.R/library/randomForest/html/randomForest.htm
> > l
> > 
> > Thanks a lot for any of your comments in advance!
> > 
> > 
> > Hui Han
> > Department of Computer Science and Engineering,
> > The Pennsylvania State University 
> > University Park, PA,16802
> > email: hhan at cse.psu.edu
> > homepage: http://www.cse.psu.edu/~hhan
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> 
> Hui Han
> Department of Computer Science and Engineering,
> The Pennsylvania State University 
> University Park, PA,16802
> email: hhan at cse.psu.edu
> homepage: http://www.cse.psu.edu/~hhan


Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan



From p.dalgaard at biostat.ku.dk  Wed Mar 31 19:22:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Mar 2004 19:22:30 +0200
Subject: [R] (no subject)
In-Reply-To: <5.1.0.14.0.20040331174824.00ba2bc0@pop-server.ucl.ac.uk>
References: <5.1.0.14.0.20040331174824.00ba2bc0@pop-server.ucl.ac.uk>
Message-ID: <x21xn9gcdl.fsf@biostat.ku.dk>

Nicole Soranzo <n.soranzo at ucl.ac.uk> writes:

> I cannot seem to be able to import my data anymore. When I try to
> import the attached file with the string
> 
> FunctRes<-read.table("C:/Documents and Settings/FunctRes.txt", header=FALSE)
> 
> I obtain:
> 
>  > FunctRes
>      V1
> 1  ??C
...
> 
> Can you help me?

Something is wrong with your file (16-bit encoding?) I see almost
every 2nd character coded as ^@ (ASCII NUL). This is the string
terminator and so has antisocial consequences if it finds its way into
the middle of a character string. You need to get rid of those
somehow.

> ??C
> 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stephane.dray at umontreal.ca  Wed Mar 31 19:24:38 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Wed, 31 Mar 2004 12:24:38 -0500
Subject: [R] scan seems to modify the data
Message-ID: <5.2.1.1.0.20040331121836.00bbe850@magellan.umontreal.ca>

Hello list,
I have used scan function to import data into R. I have done some analysis 
and find strange results. I have found my problem : when importing data 
with scan, this can slightly modify the data :

 > write(c(0.251,3.399,-0.481,0.266),"essai.txt")
 > scan("essai.txt")
Read 4 items
[1]  0.251  3.399 -0.481  0.266
 > print(scan("essai.txt"),17)
Read 4 items
[1]  0.25100000000000000  3.39900000000000000 
-0.48099999999999998  0.26600000000000001



Is it normal ? Is it a bug ?
thanks in advance,
Sincerely.


 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R
St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From dmurdoch at pair.com  Wed Mar 31 19:35:18 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 31 Mar 2004 12:35:18 -0500
Subject: [R] (no subject)
In-Reply-To: <x21xn9gcdl.fsf@biostat.ku.dk>
References: <5.1.0.14.0.20040331174824.00ba2bc0@pop-server.ucl.ac.uk>
	<x21xn9gcdl.fsf@biostat.ku.dk>
Message-ID: <r80m60lvbseg421v5pmvkp15g9kd4tjq5g@4ax.com>

On 31 Mar 2004 19:22:30 +0200, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>Nicole Soranzo <n.soranzo at ucl.ac.uk> writes:
>
>> I cannot seem to be able to import my data anymore. When I try to
>> import the attached file with the string
>> 
>> FunctRes<-read.table("C:/Documents and Settings/FunctRes.txt", header=FALSE)
>> 
>> I obtain:
>> 
>>  > FunctRes
>>      V1
>> 1  ??C
>...
>> 
>> Can you help me?
>
>Something is wrong with your file (16-bit encoding?) I see almost
>every 2nd character coded as ^@ (ASCII NUL). 

My text editor identifies it as a Unicode file.  R wants plain ascii.

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Wed Mar 31 19:38:39 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Mar 2004 19:38:39 +0200
Subject: [R] help with the usage of "randomForest"
In-Reply-To: <20040331171112.GB29102@andorian.cse.psu.edu>
References: <E7D5AB4811D20B489622AABA9C538591A324FE@teal-exch.amgen.com>
	<20040331171112.GB29102@andorian.cse.psu.edu>
Message-ID: <x2wu50gbmo.fsf@biostat.ku.dk>

Hui Han <hhan at cse.psu.edu> writes:

> Thinking that the following suggestions by Matt may be helpful to others,
> I am fowarding his notes to R-list.

<Did you ask for permission? You really should.>

> On Wed, Mar 31, 2004 at 08:57:13AM -0800, Austin, Matt wrote:
> > Use na.action=na.omit in your function call to delete those rows, but this
> > can give you problems if you want to use follow-up methods such as the
> > partial.plot(). This is what I usually do:
> > 
> > naRows <- apply(data2, 1, function(x) any(is.na(x)))
> > 
> > sum(!(naRows))
> > 
> > data2.noNAs <- data2[!naRows,]
> > 
> > chg.rf <- randomForest(ch13 ~ .,data=data2.noNAs, importance=TRUE,
> > keep.forest=TRUE)

Actually, data2.noNAs <- na.omit(data2) will do the trick and !naRows
is the same as complete.cases(data2).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Wed Mar 31 19:50:33 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 31 Mar 2004 12:50:33 -0500
Subject: [R] scan seems to modify the data
In-Reply-To: <5.2.1.1.0.20040331121836.00bbe850@magellan.umontreal.ca>
References: <5.2.1.1.0.20040331121836.00bbe850@magellan.umontreal.ca>
Message-ID: <kb0m60p3fp3uplfvttejfm4fkl9gv7obh9@4ax.com>

On Wed, 31 Mar 2004 12:24:38 -0500, Stephane DRAY
<stephane.dray at umontreal.ca> wrote :

>Hello list,
>I have used scan function to import data into R. I have done some analysis 
>and find strange results. I have found my problem : when importing data 
>with scan, this can slightly modify the data :
>
> > write(c(0.251,3.399,-0.481,0.266),"essai.txt")
> > scan("essai.txt")
>Read 4 items
>[1]  0.251  3.399 -0.481  0.266
> > print(scan("essai.txt"),17)
>Read 4 items
>[1]  0.25100000000000000  3.39900000000000000 
>-0.48099999999999998  0.26600000000000001
>
>
>
>Is it normal ? Is it a bug ?

I think it's normal.  Floating point formats aren't exact except for
fractions with only powers of 2 in the denominator.  There is no way
to represent any of your values in the formats that R uses without
slight errors.

I do notice one oddity in the print routines in R:

> x<-scan()
1: 0.266
2: 0.251
3: 
Read 2 items
> print(x,17)
[1] 0.26600000000000001 0.25100000000000000
> x<-scan()
1: 0.266
2: 
Read 1 items
> print(x,17)
[1] 0.266

I don't know why the second print() prints 0.266 differently from the
first one.  (This is in the 1.9.0 beta in Windows).

Duncan Murdoch



From ferri.leberl at gmx.at  Wed Mar 31 20:01:52 2004
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Wed, 31 Mar 2004 20:01:52 +0200
Subject: [R] (kein Betreff)
Message-ID: <200403312001.52461.ferri.leberl@gmx.at>

Dear colleagues!

How can I calculate the mean of every line of "feld" without using the command 
"for"?

Thank You in advance


feld<-array(,c(100,10))
mittel<-array(,c(100,1))
feld[,]<-rnorm(1000)
for(a in 1:100){mittel[a]<-mean(feld[a,])}



From p.dalgaard at biostat.ku.dk  Wed Mar 31 20:02:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Mar 2004 20:02:48 +0200
Subject: [R] scan seems to modify the data
In-Reply-To: <kb0m60p3fp3uplfvttejfm4fkl9gv7obh9@4ax.com>
References: <5.2.1.1.0.20040331121836.00bbe850@magellan.umontreal.ca>
	<kb0m60p3fp3uplfvttejfm4fkl9gv7obh9@4ax.com>
Message-ID: <x2isgkgaif.fsf@biostat.ku.dk>

Duncan Murdoch <dmurdoch at pair.com> writes:

> > print(x,17)
> [1] 0.26600000000000001 0.25100000000000000
...
> > print(x,17)
> [1] 0.266
> 
> I don't know why the second print() prints 0.266 differently from the
> first one.  (This is in the 1.9.0 beta in Windows).

To get the same number of decimals as the other guy. The interesting
question is why 0.251 gets all those trailing zeros:

> print(0.251,17)
[1] 0.25100000000000000
> print(0.253,17)
[1] 0.25300000000000000
> print(0.255,17)
[1] 0.255


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Mar 31 20:05:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Mar 2004 19:05:10 +0100 (BST)
Subject: [R] scan seems to modify the data
In-Reply-To: <kb0m60p3fp3uplfvttejfm4fkl9gv7obh9@4ax.com>
Message-ID: <Pine.LNX.4.44.0403311903460.25212-100000@gannet.stats>

On Wed, 31 Mar 2004, Duncan Murdoch wrote:

> On Wed, 31 Mar 2004 12:24:38 -0500, Stephane DRAY
> <stephane.dray at umontreal.ca> wrote :
> 
> >Hello list,
> >I have used scan function to import data into R. I have done some analysis 
> >and find strange results. I have found my problem : when importing data 
> >with scan, this can slightly modify the data :
> >
> > > write(c(0.251,3.399,-0.481,0.266),"essai.txt")
> > > scan("essai.txt")
> >Read 4 items
> >[1]  0.251  3.399 -0.481  0.266
> > > print(scan("essai.txt"),17)
> >Read 4 items
> >[1]  0.25100000000000000  3.39900000000000000 
> >-0.48099999999999998  0.26600000000000001
> >
> >
> >
> >Is it normal ? Is it a bug ?
> 
> I think it's normal.  Floating point formats aren't exact except for
> fractions with only powers of 2 in the denominator.  There is no way
> to represent any of your values in the formats that R uses without
> slight errors.
> 
> I do notice one oddity in the print routines in R:
> 
> > x<-scan()
> 1: 0.266
> 2: 0.251
> 3: 
> Read 2 items
> > print(x,17)
> [1] 0.26600000000000001 0.25100000000000000
> > x<-scan()
> 1: 0.266
> 2: 
> Read 1 items
> > print(x,17)
> [1] 0.266
> 
> I don't know why the second print() prints 0.266 differently from the
> first one.  (This is in the 1.9.0 beta in Windows).

Because the precision applies to the smallest of the numbers.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at PDF.COM  Wed Mar 31 20:05:21 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 31 Mar 2004 12:05:21 -0600
Subject: [R] (kein Betreff)
In-Reply-To: <200403312001.52461.ferri.leberl@gmx.at>
References: <200403312001.52461.ferri.leberl@gmx.at>
Message-ID: <406B0861.1030205@pdf.com>



Mag. Ferri Leberl wrote:
> Dear colleagues!
> 
> How can I calculate the mean of every line of "feld" without using the command 
> "for"?
> 
> Thank You in advance
> 
> 
> feld<-array(,c(100,10))
> mittel<-array(,c(100,1))
> feld[,]<-rnorm(1000)
> for(a in 1:100){mittel[a]<-mean(feld[a,])}
> 

(please use an informative subject)

See ?rowMeans

mittel <- rowMeans(feld)

-sd



From prodrigues at dcc.fc.up.pt  Wed Mar 31 19:08:46 2004
From: prodrigues at dcc.fc.up.pt (Pedro Rodrigues)
Date: 31 Mar 2004 18:08:46 +0100
Subject: [R] (kein Betreff)
In-Reply-To: <200403312001.52461.ferri.leberl@gmx.at>
References: <200403312001.52461.ferri.leberl@gmx.at>
Message-ID: <1080752926.3596.8.camel@atlantic.ocean>

On Wed, 2004-03-31 at 19:01, Mag. Ferri Leberl wrote:
> Dear colleagues!
> 
> How can I calculate the mean of every line of "feld" without using the command 
> "for"?
> 
> Thank You in advance
> 
> 
> feld<-array(,c(100,10))
> mittel<-array(,c(100,1))
> feld[,]<-rnorm(1000)
> for(a in 1:100){mittel[a]<-mean(feld[a,])}

You could use:

mittel <- apply(feld, 1, mean)

prodrigues

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pnick at virgilio.it  Wed Mar 31 20:07:34 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Wed, 31 Mar 2004 20:07:34 +0200
Subject: [R] ARCH GARCH
Message-ID: <405A654900015188@ims2b.cp.tin.it>

i'm looking for an R library dealing with ARCH and GARCH models



From ripley at stats.ox.ac.uk  Wed Mar 31 20:34:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Mar 2004 19:34:21 +0100 (BST)
Subject: [R] scan seems to modify the data
In-Reply-To: <x2isgkgaif.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0403311915160.25212-100000@gannet.stats>

On 31 Mar 2004, Peter Dalgaard wrote:

> Duncan Murdoch <dmurdoch at pair.com> writes:
> 
> > > print(x,17)
> > [1] 0.26600000000000001 0.25100000000000000
> ...
> > > print(x,17)
> > [1] 0.266
> > 
> > I don't know why the second print() prints 0.266 differently from the
> > first one.  (This is in the 1.9.0 beta in Windows).
> 
> To get the same number of decimals as the other guy. The interesting
> question is why 0.251 gets all those trailing zeros:
> 
> > print(0.251,17)
> [1] 0.25100000000000000
> > print(0.253,17)
> [1] 0.25300000000000000
> > print(0.255,17)
> [1] 0.255

Take a look at formatReal.  scientific thinks 0.251 has 17 digits and
0.255 has 3.  It really doesn't make any sense to ask for more precision 
than you have (.Machine$double.eps) and you do often get spurious 
errors if you attempt to do so.  So 15 digits is normally safe, but no 
more.

Note that there are decimal -> binary -> decimal conversions and you 
can't say which one introduced the small changes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From richard_raubertas at merck.com  Wed Mar 31 20:39:24 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Wed, 31 Mar 2004 13:39:24 -0500
Subject: [R] array addition doesn't recycle!
Message-ID: <38C4C095FC35E5469BED686B42F40A1307802B99@usrymx17.merck.com>

Another alternative is to use the underappreciated function 
'sweep()':

sweep(A, 1:2, a, "+")

Internally this is about the same as your 'A + array(a, c(2,2,2))'.
But it has the advantage that it makes explicit what the 
relationship between the dimensions of 'A' and 'a' is.  I find 
that relying on implicit recycling tends to produce errors that
are hard to trace, and code that is hard to understand six months
later.

Rich Raubertas
Merck & Co.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Wednesday, March 31, 2004 10:00 AM
> To: Tamas Papp
> Cc: R-help mailing list
> Subject: Re: [R] array addition doesn't recycle!
> 
> 
> The recycling rules are documented and this is not amongst them.
> Computer packages do have a tendency to follow their rules 
> rather than 
> read your mind.
> 
> I suspect A + as.vector(a) is what you intended.
> 
> On Wed, 31 Mar 2004, Tamas Papp wrote:
> 
> > Hi,
> > 
> > I have noticed the following:
> > 
> > > a <- array(1:4, c(2, 2))
> > > A <- array(1:4, c(2,2,2))
> > > A + a
> > Error in A + a : non-conformable arrays
> > 
> > It works with a matrix + a vector, why doesn't it work with arrays?
> > Am I missing something?
> > 
> > How would you do the above operation efficiently (ie I need to add a
> > matrix to each "plane" of 3-dim array)?  At the moment I am using
> > something like
> > 
> > A + array(a, c(2,2,2))
> > 
> > but it doesn't seem that efficient.
> 
> Why do you think is `not that efficient'?  Does you have a 
> need to save 
> microseconds?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dray at biomserv.univ-lyon1.fr  Wed Mar 31 20:54:25 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Wed, 31 Mar 2004 13:54:25 -0500
Subject: [R] scan seems to modify the data
In-Reply-To: <Pine.LNX.4.44.0403311915160.25212-100000@gannet.stats>
References: <x2isgkgaif.fsf@biostat.ku.dk>
Message-ID: <5.2.1.1.0.20040331134525.01ba9d18@biomserv.univ-lyon1.fr>

At 13:34 31/03/2004, Prof Brian Ripley wrote:

>Take a look at formatReal.  scientific thinks 0.251 has 17 digits and
>0.255 has 3.  It really doesn't make any sense to ask for more precision
>than you have (.Machine$double.eps) and you do often get spurious
>errors if you attempt to do so.  So 15 digits is normally safe, but no
>more.
>
>Note that there are decimal -> binary -> decimal conversions and you
>can't say which one introduced the small changes.

I completely agree with you. My problem arise when I try to compute a 
correlation. One of the variable seems to have equal values but it does 
not. Hence, it has a very low variance and so when I try to compute the 
correlation with another variable, this correlation is very high. I wonder 
if it would not be good to introduce a tolerance threshold. Is it 
meaningful to produce correlation when a variance is very low ?
See the example below :

 > essai=matrix(c(0.266,.234,.005,.481,.1,.009,.4,.155,.255,.2,.34,.43),4,3)
 > essai2=sweep(essai,2,apply(essai,2,sum),"/")
 > x=coef(lm(essai2~scale(runif(4))))
 > x
                       [,1]      [,2]       [,3]
(Intercept)     0.25000000 0.2500000 0.25000000
scale(runif(4)) 0.05307906 0.1330111 0.06936634
 > cor(x[1,],runif(3))
[1] 0.932772
 > var(x)
            [,1]        [,2]       [,3]
[1,] 0.01938893 0.011518783 0.01778528
[2,] 0.01151878 0.006843202 0.01056607
[3,] 0.01778528 0.010566067 0.01631426
 > var(x[1,])
[1] 1.92593e-33

Obviously, I can introduce this threshold, but I wonder if 15 digits is 
always a good limit to avoid this kind of problems

 > cor(round(x[1,],15),runif(3))
[1] NA
Warning message:
The standard deviation is zero in: cor(x, y, na.method, method == "kendall")


Thanks a lot to all,

St?phane DRAY
-------------------------------------------------------------------------------------------------- 

D?partement des Sciences Biologiques
Universit? de Montr?al, C.P. 6128, succursale centre-ville
Montr?al, Qu?bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From ripley at stats.ox.ac.uk  Wed Mar 31 20:55:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 31 Mar 2004 19:55:59 +0100 (BST)
Subject: [R] scan seems to modify the data
In-Reply-To: <5.2.1.1.0.20040331134525.01ba9d18@biomserv.univ-lyon1.fr>
Message-ID: <Pine.LNX.4.44.0403311953550.25351-100000@gannet.stats>

Note, digits in print() corresponds to signif and not to round.

You need to input some knowledge about your problem to call such issues.

On Wed, 31 Mar 2004, Stephane DRAY wrote:

> At 13:34 31/03/2004, Prof Brian Ripley wrote:
> 
> >Take a look at formatReal.  scientific thinks 0.251 has 17 digits and
> >0.255 has 3.  It really doesn't make any sense to ask for more precision
> >than you have (.Machine$double.eps) and you do often get spurious
> >errors if you attempt to do so.  So 15 digits is normally safe, but no
> >more.
> >
> >Note that there are decimal -> binary -> decimal conversions and you
> >can't say which one introduced the small changes.
> 
> I completely agree with you. My problem arise when I try to compute a 
> correlation. One of the variable seems to have equal values but it does 
> not. Hence, it has a very low variance and so when I try to compute the 
> correlation with another variable, this correlation is very high. I wonder 
> if it would not be good to introduce a tolerance threshold. Is it 
> meaningful to produce correlation when a variance is very low ?
> See the example below :
> 
>  > essai=matrix(c(0.266,.234,.005,.481,.1,.009,.4,.155,.255,.2,.34,.43),4,3)
>  > essai2=sweep(essai,2,apply(essai,2,sum),"/")
>  > x=coef(lm(essai2~scale(runif(4))))
>  > x
>                        [,1]      [,2]       [,3]
> (Intercept)     0.25000000 0.2500000 0.25000000
> scale(runif(4)) 0.05307906 0.1330111 0.06936634
>  > cor(x[1,],runif(3))
> [1] 0.932772
>  > var(x)
>             [,1]        [,2]       [,3]
> [1,] 0.01938893 0.011518783 0.01778528
> [2,] 0.01151878 0.006843202 0.01056607
> [3,] 0.01778528 0.010566067 0.01631426
>  > var(x[1,])
> [1] 1.92593e-33
> 
> Obviously, I can introduce this threshold, but I wonder if 15 digits is 
> always a good limit to avoid this kind of problems
> 
>  > cor(round(x[1,],15),runif(3))
> [1] NA
> Warning message:
> The standard deviation is zero in: cor(x, y, na.method, method == "kendall")
> 
> 
> Thanks a lot to all,
> 
> St?phane DRAY
> -------------------------------------------------------------------------------------------------- 
> 
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
> 
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> -------------------------------------------------------------------------------------------------- 
> 
> Web                                          http://www.steph280.freesurf.fr/
> -------------------------------------------------------------------------------------------------- 
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lehmann at puk.unibe.ch  Wed Mar 31 21:43:17 2004
From: lehmann at puk.unibe.ch (Christoph Lehmann)
Date: 31 Mar 2004 21:43:17 +0200
Subject: [R] classification with nnet: handling unequal class sizes
Message-ID: <1080762197.1053.11.camel@christophl>

Dear Prof. Ripley

Since you are the creator of the MASS library I dare to ask you a short
question, for which I didn't get an answer from the R mailing-list. If
you feel disturbed by my question, please forgive me and just ignore my
mail. 

I use the nnet code from your book, V&R p. 348: The very nice and general function
CVnn2() to choose the number of hidden units and the amount of weight
decay by an inner cross-validation- with a slight modification to use it
for classification (see below).

My data has 2 classes with unequal size: 45 observations for classI and
116 obs. for classII (number of variables: 39)

With CVnn2 I get the following confusion matrix (%) (average of 10
runs):

	predicted
true	53 47
	16 84

I had a similar biased confusion matrix with randomForest until I used
the sampsize argument (the same holds for svm until I used the
class.weights argument).

How can I handle this problem of unequal class sizes with nnet, in order
to get a less biased confusion matrix?

(with randomForest I finally got 

	78 22
	16 84
)


many thanks for a hint. By the way, I just want to say 'thank you' for
your great MASS book. Since your first recommendation, I consult it
quite frequently.

Christoph


----------------------------------------------------------------------------
#--- neural networks

#classification network is constructed; this has one output and entropy
fit if the number of levels is two, and a number of outputs equal to the
number of classes and a softmax output stage for more levels. ->
therefore two lines of Prof. Ripley's wrapper function are changed below
(original commented out) and an additional function has been introduced
(resmatrix)

con <- function(...)
{
    print(tab <- table(...))
    diag(tab) <- 0
    cat("error rate = ",
        round(100*sum(tab)/length(list(...)[[1]]), 2), "%\n")
    invisible()
}


CVnn2 <- function(formula, data,
                  size = c(0,4,4,10,10), lambda = c(0, rep(c(0.001,
0.01),2)),
                  nreps = 1, nifold = 5, verbose = 99, ...)
{
    resmatrix <- function(predict.matrix,learn, data, ri, i)
    {
       rae.matrix <-   predict.matrix
       rae.matrix[,] <- 0
       rae.vector <- as.numeric(as.factor((predict(learn, data[ri ==
i,], type = "class"))))
       for (k in 1:dim(rae.matrix)[1]) {
         if (rae.vector[k] == 1) rae.matrix[k,1] <- rae.matrix[k,1] + 1
else
         rae.matrix[k,2] <- rae.matrix[k,2] + 1
       }
       rae.matrix
    }


    CVnn1 <- function(formula, data, nreps=1, ri, verbose,  ...)
    {
        totalerror <- 0
        truth <- data[,deparse(formula[[2]])]
        res <-  matrix(0, nrow(data), length(levels(truth)))
        if(verbose > 20) cat("  inner fold")
        for (i in sort(unique(ri))) {
            if(verbose > 20) cat(" ", i,  sep="")
            for(rep in 1:nreps) {
                learn <- nnet(formula, data[ri !=i,], trace = F, ...)
                #res[ri == i,] <- res[ri == i,] + predict(learn, data[ri
== i,])
                res[ri == i,] <- res[ri == i,] + resmatrix(res[ri ==
i,],learn,data, ri, i)
            }
        }
        if(verbose > 20) cat("\n")
        sum(as.numeric(truth) != max.col(res/nreps))
    }
    truth <- data[,deparse(formula[[2]])]
    res <-  matrix(0, nrow(data), length(levels(truth)))
    choice <- numeric(length(lambda))
    for (i in sort(unique(rand))) {
        if(verbose > 0) cat("fold ", i,"\n", sep="")
        ri <- sample(nifold, sum(rand!=i), replace=T)
        for(j in seq(along=lambda)) {
            if(verbose > 10)
                cat("  size =", size[j], "decay =", lambda[j], "\n")
            choice[j] <- CVnn1(formula, data[rand != i,], nreps=nreps,
                               ri=ri, size=size[j], decay=lambda[j],
                               verbose=verbose, ...)
        }
        decay <- lambda[which.is.max(-choice)]
        csize <- size[which.is.max(-choice)]
        if(verbose > 5) cat("  #errors:", choice, "  ") #
        if(verbose > 1) cat("chosen size = ", csize,
                            " decay = ", decay, "\n", sep="")
        for(rep in 1:nreps) {
            learn <- nnet(formula, data[rand != i,], trace=F,
                          size=csize, decay=decay, ...)
            #res[rand == i,] <- res[rand == i,] + predict(learn,
data[rand == i,])
            res[rand == i,] <- res[rand == i,] + resmatrix(res[rand ==
i,],learn,data, rand, i)
        }
    }
    factor(levels(truth)[max.col(res/nreps)], levels = levels(truth))
}



-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>
-- 
Christoph Lehmann                            Phone:  ++41 31 930 93 83 
Department of Psychiatric Neurophysiology    Mobile: ++41 76 570 28 00
University Hospital of Clinical Psychiatry   Fax:    ++41 31 930 99 61 
Waldau                                            lehmann at puk.unibe.ch 
CH-3000 Bern 60         http://www.puk.unibe.ch/cl/pn_ni_cv_cl_03.html



From andy_liaw at merck.com  Wed Mar 31 22:11:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 31 Mar 2004 15:11:04 -0500
Subject: [R] help with the usage of "randomForest"
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7ADD@usrymx25.merck.com>

As you've learned, using the formula interface, the NAs are handled by
na.action.  (BTW, the R default is na.omit, so the NAs are silently omitted.
If it were na.fail, you would have gotten an error message.)

There are several options on handling NAs, na.omit being one of them.  If
you have too many NAs, omitting them would leave you too little data, as you
experienced.  One possibility is to use na.roughfix (in the randomForest
package) as na.action, which replaces the NAs with the median of the
variable (or the mode for factor variable).  If you want to, you can use
rfImpute to use randomForest itself to impute NAs (assuming your training
data isn't terribly big).

HTH,
Andy

> From: Hui Han
> 
> Thanks for Matt and Torsten for very helpful suggestions!
> As Matt pointed out, the problem is that na.action has the 
> default value of na.fail, that
> deleted one class samples. I changed all NAs to real values, 
> and the error msg.
> dissappeared. 
> 
> However my real dataset contains many NAs. I wonder if 
> anybody can point me any documentations on
> how to define na.action not be na.fail?
> 
> Best regards,
> Hui
> 
> On Wed, Mar 31, 2004 at 06:26:36PM +0200, Torsten Hothorn wrote:
> > On Wed, 31 Mar 2004, Hui Han wrote:
> > 
> > > Dear all,
> > >
> > > Can anybody give me some hint on the following error msg 
> I got with using
> > > randomForest?
> > >
> > > I have two-class classification problem. The data file 
> "sample" is:
> > > ----------------------------------------------------------
> > >  udomain.edu udomain.hcs hpclass
> > > 1 1.0000 1 not
> > > 2 NA 2 not
> > > 3 NA 0.8 not
> > > 4 NA 0.2 hp
> > > 5 NA 0.9 hp
> > > ------------------------------------------------------------
> > > The steps I called the function are:
> > > (1) Read data
> > > hp <- read.table("sample")
> > 
> > most probably a problem here. say
> > 
> > R> summary(hp)
> > 
> > and check if the factor `hpclass' has two levels.
> > 
> > Torsten
> > 
> > > (2) Call randomForest
> > > hp.rf <- randomForest(hpclass ~., yy, data=hp, importance=TRUE,
> > > proximity=TRUE)
> > >
> > > But the error msg I got is:
> > > Error in randomForest.default(m, y, ...) :
> > >         Need at least two classes to do classification.
> > >
> > >
> > > I learned the usage of randomForest from:
> > > 
> http://www.maths.lth.se/help/R/.R/library/randomForest/html/ra
ndomForest.html
> >
> > Thanks a lot for any of your comments in advance!
> >
> >
> > Hui Han
> > Department of Computer Science and Engineering,
> > The Pennsylvania State University
> > University Park, PA,16802
> > email: hhan at cse.psu.edu
> > homepage: http://www.cse.psu.edu/~hhan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >


Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Mar 31 22:24:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 31 Mar 2004 20:24:42 +0000 (UTC)
Subject: [R] extracting values from a 3d array using a matrix from indices
References: <20040331145156.GA1008@localhost>
Message-ID: <loom.20040331T221421-599@post.gmane.org>


1. Create a matrix A.list each of whose i,j-th entries is a one element list
containing A[i,,j]. Note that A.list and B have the same lengths so
we can use mapply.  With mapply, we can choose the B[k]-th element from the
list represented by A.list[k].  Finally reshape.

# test data
A <- array(1:8,c(2,2,2))
B <- matrix(c(1,2,2,1),2)

# solution
A.list <- apply(A,c(1,3),function(x)list(x))
C <- mapply(A.list,B,FUN=function(x,y)unlist(x)[y])
C <- matrix(C,nrow(A))

# compare to loop solution
C.loop <- matrix(NA,2,2)
for(i in 1:2)for(j in 1:2) C.loop[i,j] <- A[i,B[i,j],j]
identical(C,C.loop) # TRUE

2. The second problem can also be handled with the same A.list in a 
similar way:

C <- sapply(A.list,function(x)which.max(unlist(x)))
C <- matrix(C,nrow(A))

Tamas Papp <tpapp <at> axelero.hu> writes:

: Suppose I have A, an n x m matrix, each element is an integer (an
: index).
: 
: I also have B, an n x l x m array.  I need C, where
: 
: C[n,m] = B[n, A[n, m], m]
: 
: I am currently using loops, what would be the "R way" to do this?
: 
: Another question: let
: 
: A[n, m] <- argmax_l B[n, l, m]
: 
: what would be the nicest way of doing this?  Currently I am using
: max.col and a single loop, going though the n's.
: 
: Background: I solving a discrete-space dynamic programming problem, A
: is the optimal policy, C is the value function.  The structure of the
: problem allows me to use matrices like above, instead of (n * m) x (n
: x m) square matrices.
: 
: Thanks
: 
: Tamas
:



From jfox at mcmaster.ca  Wed Mar 31 22:28:08 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 31 Mar 2004 15:28:08 -0500
Subject: [R] Zero Index Origin?
In-Reply-To: <406AC9D3.3000101@lancaster.ac.uk>
Message-ID: <20040331202806.XJHL18104.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Baz,

I'm inclined to believe that your general advice is correct. As a long-time
APLer who came to R through Lisp-Stat, I think that it's generally a good
idea not to resist the most natural way of programming in R.

On the other hand, introducing a new class and defining methods for it (such
as a method for indexing) doesn't disturb standard methods, as your message
seems to imply. Indeed, the ability to do these kinds of things is in my
view one of the strengths of R (and S more generally). Of course, it is
necessary to implement the new class carefully. Whether it is worth the
trouble to have a class that accommodates zero-origin indexing seems to me
to depend upon the application. I've never myself encountered an application
where this was really desirable.

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Barry 
> Rowlingson
> Sent: Wednesday, March 31, 2004 8:38 AM
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Zero Index Origin?
> 
> Gabor Grothendieck wrote:
> > If you are willing to do it yourself you can define a class 
> for which 
> > indexing behaves that way.
> 
>   I'd like to prefix all these solutions with "Here's how to 
> do it, but don't actually do it you crazy fool". It's on a 
> par with redefining pi, or redefining '+'. And then redefining '<-'.
> 
>   These techniques have their proper place, and that would be 
> in the currently non-existent obfuscated R contest.
> 
>   No, the R-ish (iRish?) way is to index vectors from 1. 
> That's what the R gods intended!
> 
> 
> Baz



From kjetil at entelnet.bo  Wed Mar 31 23:00:00 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 31 Mar 2004 17:00:00 -0400
Subject: [R] ARCH GARCH
In-Reply-To: <405A654900015188@ims2b.cp.tin.it>
Message-ID: <406AF910.3212.5C3D38@localhost>

On 31 Mar 2004 at 20:07, pnick at virgilio.it wrote:

> i'm looking for an R library dealing with ARCH and GARCH models
> 

help.search("ARCH") 
on my system gives among others:

garch-methods(tseries)
                   Methods for Fitted GARCH Models
garch(tseries)     Fit GARCH Models to Time Series
summary.garch(tseries)
                   Summarizing GARCH Model Fits

so you should look into the tseries
package.

Kjetil Halvorsen

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From kjetil at entelnet.bo  Wed Mar 31 23:00:00 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 31 Mar 2004 17:00:00 -0400
Subject: [R] Zero Index Origin?
In-Reply-To: <406AC9D3.3000101@lancaster.ac.uk>
References: <loom.20040331T150145-904@post.gmane.org>
Message-ID: <406AF910.3850.5C3DC5@localhost>

On 31 Mar 2004 at 14:38, Barry Rowlingson wrote:

> Gabor Grothendieck wrote:
> > If you are willing to do it yourself you can define a class 
> > for which indexing behaves that way.
> 
>   I'd like to prefix all these solutions with "Here's how to do it,
>   but 
> don't actually do it you crazy fool". It's on a par with redefining
> pi, or redefining '+'. And then redefining '<-'.

You mean

> y <- 5
> "<-" <- function(x,value) x+value
> y
[1] 5
> y <- 5
[1] 10

(but remember not to save the workspace)

> 
>   These techniques have their proper place, and that would be in the
> currently non-existent obfuscated R contest.
> 
>   No, the R-ish (iRish?) way is to index vectors from 1. That's what
>   the 
> R gods intended! 

But not everybody listens to the gods! On CRAN there is a blasfemish 
package called
Oarray

Kjetil Halvorsen

> 
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Mar 31 23:48:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 31 Mar 2004 23:48:41 +0200
Subject: [R] scan seems to modify the data
References: <5.2.1.1.0.20040331121836.00bbe850@magellan.umontreal.ca>
Message-ID: <406B3CB9.4290BFBD@statistik.uni-dortmund.de>



Stephane DRAY wrote:
> 
> Hello list,
> I have used scan function to import data into R. I have done some analysis
> and find strange results. I have found my problem : when importing data
> with scan, this can slightly modify the data :
> 
>  > write(c(0.251,3.399,-0.481,0.266),"essai.txt")
>  > scan("essai.txt")
> Read 4 items
> [1]  0.251  3.399 -0.481  0.266
>  > print(scan("essai.txt"),17)
> Read 4 items
> [1]  0.25100000000000000  3.39900000000000000
> -0.48099999999999998  0.26600000000000001
> 
> Is it normal ? Is it a bug ?

It is normal, that there are no exact representations for floating point
numbers in a computer (you have only a limited number of bits to
represent them!).
In this case, it is not scan(), but just the representation: Try out and
type 
  print(c(0.251,3.399,-0.481,0.266), 17)

Uwe Ligges



> thanks in advance,
> Sincerely.
> 
>  > version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
> St?phane DRAY
> --------------------------------------------------------------------------------------------------
> 
> D?partement des Sciences Biologiques
> Universit? de Montr?al, C.P. 6128, succursale centre-ville
> Montr?al, Qu?bec H3C 3J7, Canada
> 
> Tel : 514 343 6111 poste 1233
> E-mail : stephane.dray at umontreal.ca
> --------------------------------------------------------------------------------------------------
> 
> Web                                          http://www.steph280.freesurf.fr/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From DJNordlund at aol.com  Wed Mar 31 23:55:57 2004
From: DJNordlund at aol.com (DJNordlund@aol.com)
Date: Wed, 31 Mar 2004 16:55:57 EST
Subject: [R] scan seems to modify the data
Message-ID: <1a5.2157df29.2d9c986d@aol.com>

St?phane,

in the example below which you are concerned about, the large correlation you 
see is not a result of the small variance, but rather the 3 random numbers 
you generated just happened to have the same rank ordering as the magnitudes of 
the three coefficients you were correlating them with.  

Just try your example again but repeat your correlation command

cor(x[1,],runif(3))

several times in succession.  You will probably see correlations ranging from 
large and positive to large (in absolute value) and negative to everywhere in 
between.

Dan Nordlund

------------------Original message----------------------
In a message dated 3/31/2004 10:53:03 AM Pacific Standard Time, 
dray at biomserv.univ-lyon1.fr writes:

>At 13:34 31/03/2004, Prof Brian Ripley wrote:
>
>>Take a look at formatReal.  scientific thinks 0.251 has 17 digits and
>>0.255 has 3.  It really doesn't make any sense to ask for more precision
>>than you have (.Machine$double.eps) and you do often get spurious
>>errors if you attempt to do so.  So 15 digits is normally safe, but no
>>more.
>>
>>Note that there are decimal -> binary -> decimal conversions and you
>>can't say which one introduced the small changes.
>
>I completely agree with you. My problem arise when I try to compute a 
>correlation. One of the variable seems to have equal values but it does 
>not. Hence, it has a very low variance and so when I try to compute the 
>correlation with another variable, this correlation is very high. I wonder 
>if it would not be good to introduce a tolerance threshold. Is it 
>meaningful to produce correlation when a variance is very low ?
>See the example below :
>
>> essai=matrix(c(0.266,.234,.005,.481,.1,.009,.4,.155,.255,.2,.34,.43),4,3)
>> essai2=sweep(essai,2,apply(essai,2,sum),"/")
>> x=coef(lm(essai2~scale(runif(4))))
>> x
>                       [,1]      [,2]       [,3]
>(Intercept)     0.25000000 0.2500000 0.25000000
>scale(runif(4)) 0.05307906 0.1330111 0.06936634
>> cor(x[1,],runif(3))
>[1] 0.932772
>> var(x)
>            [,1]        [,2]       [,3]
>[1,] 0.01938893 0.011518783 0.01778528
>[2,] 0.01151878 0.006843202 0.01056607
>[3,] 0.01778528 0.010566067 0.01631426
>> var(x[1,])
>[1] 1.92593e-33



From ligges at statistik.uni-dortmund.de  Wed Mar 31 23:57:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 31 Mar 2004 23:57:20 +0200
Subject: [R] corrupted ASCII file; was: (no subject)
References: <5.1.0.14.0.20040331174824.00ba2bc0@pop-server.ucl.ac.uk>
Message-ID: <406B3EC0.5BBA1A07@statistik.uni-dortmund.de>



Nicole Soranzo wrote:
> 
> Hi, I wonder if you can help me:
> 
> I cannot seem to be able to import my data anymore. When I try to import
> the attached file with the string
> 
> FunctRes<-read.table("C:/Documents and Settings/FunctRes.txt", header=FALSE)
> 
> I obtain:
> 
>  > FunctRes
>      V1
> 1  ??C
> 2   \n
> 3    C
> 4    0
> 5    0
> 6    B
> 7   \n
> 8    C
> 9    0
> 10   1
> 11   B
> 12  \n
> 13   C
> 14   0
> 15   0
> 16   B
> 17  \n
> 18   C
> 19   0
> 20   1
> 21   B
> 22  \n
> 23   C
> 24   0
> 25   1
> 26   B
> 27  \n
> 28   C
> 29   0
> 30   1
> 31   B
> 
> Can you help me?

Strange. The file got corrupted in some way. Copy and paste the contents
to another (empty) ASCII file solves the problem.
[Please use a sensible subject.]

Uwe Ligges


> Thanks
> Nicole
> 
> Dr Nicole Soranzo
> 
> Centre for Population Genetics and Human Health
> 
> University College London
> Darwin Building
> Gower Street
> London WC1E 6BT
> 
> Tel: +44 (0)20 7679 4397
> Fax: +44 (0)20 7679 2887
> 
> E-mail: n.soranzo at ucl.ac.uk
> http://popgen.biol.ucl.ac.uk/
> 
>   ------------------------------------------------------------------------
>                    Name: FunctRes.txt
>    FunctRes.txt    Type: Plain Text (text/plain)
>                Encoding: base64
> 
>   ------------------------------------------------------------------------
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From HankeA at mar.dfo-mpo.gc.ca  Wed Mar 31 21:29:04 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 31 Mar 2004 15:29:04 -0400
Subject: [R] identify()  and controlling label size
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12497B@msgmarsta01.bio.dfo.ca>

Thank-you it works! I have ignored ps and relied on the cex arguments until
now.
Alex

-----Original Message-----
From: Barry Rowlingson [mailto:B.Rowlingson at lancaster.ac.uk] 
Sent: March 31, 2004 10:59 AM
To: Hanke, Alex
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] identify() and controlling label size


Hanke, Alex wrote:
> I thought this was going to be easy ...
> Can the label size of identify() be controlled by setting par(cex.*)
because
> I'm having no luck? My only recourse is to save the index and position of
> the labels from identify() and use text() to replot them.
> 

  Funny, it seems to ignore that, but allows 'col=' and 'font=' to set 
the font colour and type.

  But! It accepts 'ps=' to set the point size! You may be saved...

Try:

plot(1:10)
identify(1:10,ps=24)
identify(1:10,ps=12)

Baz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gabors at bgnet.bgsu.edu  Wed Mar 31 18:21:10 2004
From: gabors at bgnet.bgsu.edu (Gabor Szekely)
Date: Wed, 31 Mar 2004 11:21:10 -0500
Subject: [R] [R-pkgs] energy 1.0.1
Message-ID: <p04310101bc909e38eb73@[129.1.87.58]>


R Users,

We would like to announce that Version 1.0.1 of the energy package is
now available on CRAN.

The energy package introduces a new class of statistical tests based
on the concept of Newton's potential energy. Included in the package are

    * mvnorm.etest (test) and mvnorm.e (statistic)
      A rotation invariant multivariate goodness-of-fit test,
      implemented for testing multivariate normality with
      estimated parameters

    * eqdist.etest (test) and ksample.e (statistic)
      A k-sample multivariate nonparametric test of equal distributions
      (arbitrary number of samples and arbitrary dimension)
      with optional incomplete statistics to support testing
      arbitrarily large samples

    * energy.hclust and edist
      Hierarchical clustering based on e-distances and
      cluster distance function

    * poisson.mtest (test) and poisson.m (statistic)
      Mean distance test of Poisson distribution, estimated mean

Here is a brief summary of the potential energy background of the
tests in the energy package:

Newton's gravitational force is inversely proportional to squared
Euclidean distances between pairs of objects, and the potential
energy is inversely proportional to their Euclidean distances. The
simplest choice for the absolute value of the force is constant, and
then the potential energy is proportional to the Euclidean distances
between pairs of objects. In statistics, the objects are the observed
random vectors, and thus our energy terms are Euclidean distances
between the pairs of sample elements: ||x_i - x_j||. Tests are based
on the Theorem that a suitable linear combination of these distances
(the potential energy of the 'statistical situation') is always
nonnegative, and equals 0 if and only if the null hypothesis holds.

Comments and suggestions are welcome. The description file is below.

        Gabor Szekely and Maria Rizzo

----------------------------------------------------------------------
Description:
Package: energy
Title: E-statistics (energy statistics) tests of fit, clustering
Version: 1.0.1
Date: March 24, 2004
Author: Maria L. Rizzo <rizzo at math.ohiou.edu> and Gabor J. Szekely
<gabors at bgnet.bgsu.edu>
Description: E-statistics (energy) tests for comparing distributions:
multivariate normality, Poisson test, multivariate k-sample
test for equal distributions, hierarchical clustering by
e-distances. Energy-statistics concept based on a
generalization of Newton's potential energy is due to Gabor J.
Szekely.
Maintainer: Maria Rizzo <rizzo at math.ohiou.edu>
License: GPL 2.0 or later
-- 

****************************************
Gabor J. Szekely
Professor
Department of Mathematics and Statistics
Math Science Building
Bowling Green State University
Bowling Green, OH 43403-0221

E-mail: gabors at bgnet.bgsu.edu
Tel: 	419-372-7474
Fax:	419-372-6092
http://www-math.bgsu.edu/~gabors/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



