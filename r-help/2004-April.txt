From arcane at arcanemethods.com  Thu Apr  1 00:00:50 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 14:00:50 -0800
Subject: [R] Zero Index Origin?
In-Reply-To: <loom.20040331T150145-904@post.gmane.org>
References: <406A7E37.5060504@arcanemethods.com>
	<loom.20040331T150145-904@post.gmane.org>
Message-ID: <406B3F92.8010402@arcanemethods.com>

Thanks for the input.  I'm new to classes and object
orientation as well as to R and was sorta hoping to hear
that a solution might exist within that structure.  Would
existing functions be able to deal with such objects and see
them as the 1 origin objects that they expect?

A solution to backward compatibility that I proposed to the
Matlab group, which was never shot down from a technical
standpoint, but which in the end we could not get them to
consider, was to make the origin a contextual variable which
could be set by the programmer to whatever he wanted, ala
APL.  To deal with existing functions, it was made local in
scope and always initialized to 1 when a function was
called.  I'm not yet sure that this simple approach is even
meaningful much less inclusive in the context of R but I'd
like to invite comment.

I fully understand the potentially large scope of such a
proposed change to the implementation but right now I'm just
curious about the theoretical possibility.

I don't really wish to go into all of the reasoning why this
convenience is important to DSP applications.  Let me just
say that it has been the subject of considerable discussion
WRT Matlab and a rather large number of reasons for its
desirability have been put forth from that community.  I'd
like to promote R in the various usenet and email groups
that are involved in DSP type programming, even to the point
of recruiting an R-DSP working group, and to be able to
point to this capability would be a strong selling point.


Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From jasont at indigoindustrial.co.nz  Thu Apr  1 00:15:50 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 1 Apr 2004 10:15:50 +1200 (NZST)
Subject: [R] Zero Index Origin?
In-Reply-To: <406B3F92.8010402@arcanemethods.com>
References: <406A7E37.5060504@arcanemethods.com><loom.20040331T150145-904@post.gmane.org>
	<406B3F92.8010402@arcanemethods.com>
Message-ID: <46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>

> I don't really wish to go into all of the reasoning why this
> convenience is important to DSP applications.

I do some DSP work, in the context of instrumentation signal filtering,
identification, tracking, and coherence (finding plant signals that have
similar "fingerprints").  I agree that in languages that require you to
work with explicit index values, the zero-offset makes perfect sense.

However, I'd like to second Prof. Ripley's remark that if you're actually
using indicies explicitly, you probably haven't wrapped your head around
how powerful the indexing structure and "whole object" approach is in S
(including R).  It's been a long time since I used counter-based indicies.


> I'd
> like to promote R in the various usenet and email groups
> that are involved in DSP type programming, even to the point
> of recruiting an R-DSP working group

Sounds interesting.  Could you contact me off-list about this?

Cheers

Jason



From robert.kissell at citigroup.com  Thu Apr  1 00:28:19 2004
From: robert.kissell at citigroup.com (Kissell, Robert [EQRE])
Date: Wed, 31 Mar 2004 17:28:19 -0500
Subject: [R] functions & paths
Message-ID: <4115749EFC8862458D6FE4F04F5F7DE701E82F5B@EXCHNY37.ny.ssmb.com>

Hi,
I have a couple of quick questions regarding R.

1) I have a ".First" function that automatically loads the quadprog package into the workspace. The .First function resides in R and is only saved if I save the workspace. The exact function is as follows:

.First<-function() 
{
	library(quadprog)
}

Is there any way that this function can be saved outside of R? That is, is there any procedure that will run specified R commands whenever R is invoked?


2) Is there anyway to specify additional paths for R to search. For example, I have some R script named, myProg.r that is saved in a directory C:\Programs\RLanguage\Rob\. I run the script in the following way:
	source("C:/Programs/RLanguage/Rob/myProg.r")

This works fine, but I was wondering if there is any command that could be entered into the .First function described above that adds the directory C:\Programs\RLanguage\Rob\ to the R path. This way, C:\Programs\RLanguage\Rob\ will always be checked for the code and I could simply run myProg.r as follows:
	source("myProg.r")

3) Is there anyway that I could save functions that I have written outside of R. That is, could I write functions and have them saved in some directory, such as, C:\Programs\RLanguage\RobFunctions\? This way I would be able to copy the functions to other pcs. This is very simple in MatLab. Please let me know.

Thanks for the help.


Rob



From arcane at arcanemethods.com  Thu Apr  1 00:40:19 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 14:40:19 -0800
Subject: [R] Zero Index Origin?
In-Reply-To: <46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>
References: <406A7E37.5060504@arcanemethods.com><loom.20040331T150145-904@post.gmane.org>
	<406B3F92.8010402@arcanemethods.com>
	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>
Message-ID: <406B48D3.9010502@arcanemethods.com>



Jason Turner wrote:


> I do some DSP work, in the context of instrumentation signal filtering,
> identification, tracking, and coherence (finding plant signals that have
> similar "fingerprints").  I agree that in languages that require you to
> work with explicit index values, the zero-offset makes perfect sense.
> 
> However, I'd like to second Prof. Ripley's remark that if you're actually
> using indicies explicitly, you probably haven't wrapped your head around
> how powerful the indexing structure and "whole object" approach is in S
> (including R).  It's been a long time since I used counter-based indicies.

I absolutely agree that the powerful indexing in R 
ameliorates it to a large extent but even the less powerful 
indexing of Matlab should satisfy that condition and yet 
there is still considerable sturm and drang over this issue.


Bob
-- 

"Things should be described as simply as possible, but no 
simpler."

                                              A. Einstein



From Meredith.Briggs at team.telstra.com  Thu Apr  1 00:54:51 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Thu, 1 Apr 2004 08:54:51 +1000
Subject: [R] Is there a package or routine for calculating mimimum spanning
	trees?
Message-ID: <3B5823541A25D311B3B90008C7F9056410E3539D@ntmsg0092.corpmail.telstra.com.au>



From opu1 at columbia.edu  Thu Apr  1 00:58:38 2004
From: opu1 at columbia.edu (Oleg Urminsky)
Date: Wed, 31 Mar 2004 17:58:38 -0500
Subject: [R] Gauss user question
Message-ID: <406B4D1E.4080402@columbia.edu>

I have been using Gauss for doing maximum likelihood estimation of 
somewhat complex models, and I am possibly interested in moving over to 
R.  Does anyone have experience doing both?  My main concern is that my 
models are very slow to estimate in Gauss; wondering if R is likely to 
be faster or slower.

Any feedback would be much appreciated!

Thank you,

Oleg Urminsky
opu1 at columbia.edu



From ggrothendieck at myway.com  Thu Apr  1 01:00:00 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 31 Mar 2004 23:00:00 +0000 (UTC)
Subject: [R] Zero Index Origin?
References: <406A7E37.5060504@arcanemethods.com><loom.20040331T150145-904@post.gmane.org>
	<406B3F92.8010402@arcanemethods.com>
	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>
	<406B48D3.9010502@arcanemethods.com>
Message-ID: <loom.20040401T005002-984@post.gmane.org>


Bob Cain wrote:

: Jason Turner wrote:
: 
: > However, I'd like to second Prof. Ripley's remark that if you're actually
: > using indicies explicitly, you probably haven't wrapped your head around
: > how powerful the indexing structure and "whole object" approach is in S
: > (including R).  It's been a long time since I used counter-based indicies.
: 
: I absolutely agree that the powerful indexing in R 
: ameliorates it to a large extent but even the less powerful 
: indexing of Matlab should satisfy that condition and yet 
: there is still considerable sturm and drang over this issue.

I believe the point was not so much that R has powerful indexing 
(which it does) but that the capabilities of dealing with objects as 
a whole, i.e. without using indexing at all, makes facilities for 
indexing less important.



From edd at debian.org  Thu Apr  1 01:02:35 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 31 Mar 2004 17:02:35 -0600
Subject: [R] functions & paths
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82F5B@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F5B@EXCHNY37.ny.ssmb.com>
Message-ID: <20040331230235.GA29577@sonny.eddelbuettel.com>


Rob,

On Wed, Mar 31, 2004 at 05:28:19PM -0500, Kissell, Robert [EQRE] wrote:
> 1) I have a ".First" function that automatically loads the quadprog package into the workspace. The .First function resides in R and is only saved if I save the workspace. The exact function is as follows:
[...]
> Is there any way that this function can be saved outside of R? That is, is there any procedure that will run specified R commands whenever R is invoked?

Try help(Startup) 

You probably want .Rprofile (on a per-directory basis), and/or the site-wide
$R_HOME/etc/Rprofile.
 
 
> 2) Is there anyway to specify additional paths for R to search. For example, I have some R script named, myProg.r that is saved in a directory C:\Programs\RLanguage\Rob\. I run the script in the following way:
> 	source("C:/Programs/RLanguage/Rob/myProg.r")
>
>  This works fine, but I was wondering if there is any command that could be entered into the .First function described above that adds the directory C:\Programs\RLanguage\Rob\ to the R path. This way, C:\Programs\RLanguage\Rob\ will always be checked for the code and I could simply run myProg.r as follows:
> 	source("myProg.r")

For library(), there is an argumemnt lib.loc. You can also set things from 
Rprofile.

Source is simpler and wants pathname, so maybe you want to write a local function
that does your desired magic with the directory and appends the filename you
supply, before calling source() on the combined string?

> 3) Is there anyway that I could save functions that I have written outside of R. That is, could I write functions and have them saved in some directory, such as, C:\Programs\RLanguage\RobFunctions\? This way I would be able to copy the functions to other pcs. This is very simple in MatLab. Please let me know.

See 2), save() and friends have pathnames too. So
save("N:/my/network/share/foo.R") works just fine.

Hth, Dirk


-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From dray at biomserv.univ-lyon1.fr  Thu Apr  1 01:55:51 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Wed, 31 Mar 2004 18:55:51 -0500
Subject: [R] Is there a package or routine for calculating mimimum
	spanning trees?
In-Reply-To: <3B5823541A25D311B3B90008C7F9056410E3539D@ntmsg0092.corpmai
	l.telstra.com.au>
Message-ID: <5.2.1.1.0.20040331185504.01cce0a0@biomserv.univ-lyon1.fr>

see mst in package ace

At 17:54 31/03/2004, Briggs, Meredith M wrote:

>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From f0z6305 at labs.tamu.edu  Thu Apr  1 02:12:48 2004
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Wed, 31 Mar 2004 18:12:48 -0600
Subject: [R] How to cluster the variables instead of data points?
Message-ID: <035c01c4177e$1159c330$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040331/38a03341/attachment.pl

From macq at llnl.gov  Thu Apr  1 02:06:22 2004
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 31 Mar 2004 16:06:22 -0800
Subject: [R] functions & paths
In-Reply-To: <4115749EFC8862458D6FE4F04F5F7DE701E82F5B@EXCHNY37.ny.ssmb.com>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F5B@EXCHNY37.ny.ssmb.com>
Message-ID: <p06002004bc910b9f64e5@[128.115.153.6]>

At 5:28 PM -0500 3/31/04, Kissell, Robert [EQRE] wrote:
>Hi,
>I have a couple of quick questions regarding R.
>

<- snip ->

>3) Is there anyway that I could save functions that I have written 
>outside of R. That is, could I write functions and have them saved 
>in some directory, such as, C:\Programs\RLanguage\RobFunctions\? 
>This way I would be able to copy the functions to other pcs. This is 
>very simple in MatLab. Please let me know.

Yes. Here are two ways.

  - 1 -
In the directory where you have your function definitions, load the 
functions into R. Quit R, and say "yes" when asked whether or not to 
save the workspace image. Find the file in which the image is saved, 
copy it to the other pcs. On the other pcs, use the "attach" function 
to attach that file. The functions are now available.

-2-
Go get the R Extensions manual, learn how to create a package. Take 
your functions, make them into a package. Copy the package file to 
the other pcs. Use R's "install package" functionality to install the 
package. Then, whenever the functions are needed, use the "library" 
command to load your package.



>Thanks for the help.
>
>Rob
>


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From jasont at indigoindustrial.co.nz  Thu Apr  1 02:34:56 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 1 Apr 2004 12:34:56 +1200 (NZST)
Subject: [R] How to cluster the variables instead of data points?
In-Reply-To: <035c01c4177e$1159c330$a7560d18@f0z6305>
References: <035c01c4177e$1159c330$a7560d18@f0z6305>
Message-ID: <36980.203.9.176.60.1080779696.squirrel@webmail.maxnet.co.nz>

> Do you know if there are some specific functions under R
> to do clustering?
> To be specific, I have a d-dimensional vector x, and wish
> to clustering these d variables of x into some finite groups
> given self-defined distance measure.

Not sure what you want, as the above could be interpreted many ways.

help.search("cluster")

?cut
library(help=cluster)

are good starting points.

Cheers

Jason



From p.dalgaard at biostat.ku.dk  Thu Apr  1 02:39:13 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Apr 2004 02:39:13 +0200
Subject: [R] Zero Index Origin?
In-Reply-To: <406B48D3.9010502@arcanemethods.com>
References: <406A7E37.5060504@arcanemethods.com>
	<loom.20040331T150145-904@post.gmane.org>
	<406B3F92.8010402@arcanemethods.com>
	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>
	<406B48D3.9010502@arcanemethods.com>
Message-ID: <x28yhg4jm6.fsf@biostat.ku.dk>

Bob Cain <arcane at arcanemethods.com> writes:

> Jason Turner wrote:
> 
> 
> > I do some DSP work, in the context of instrumentation signal filtering,
> > identification, tracking, and coherence (finding plant signals that have
> > similar "fingerprints").  I agree that in languages that require you to
> > work with explicit index values, the zero-offset makes perfect sense.
> > However, I'd like to second Prof. Ripley's remark that if you're
> > actually
> > using indicies explicitly, you probably haven't wrapped your head around
> > how powerful the indexing structure and "whole object" approach is in S
> > (including R).  It's been a long time since I used counter-based indicies.
> 
> I absolutely agree that the powerful indexing in R ameliorates it to a
> large extent but even the less powerful indexing of Matlab should
> satisfy that condition and yet there is still considerable sturm and
> drang over this issue.

I think it was said quite early in the thread, but since noone
apparently listened, let me reiterate: One of the powerful indexing
features in R is the negative index ("all, except") and x[-0] would
lead to some complications if 0 was a true index. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Apr  1 02:55:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Apr 2004 02:55:14 +0200
Subject: [R] functions & paths
In-Reply-To: <p06002004bc910b9f64e5@[128.115.153.6]>
References: <4115749EFC8862458D6FE4F04F5F7DE701E82F5B@EXCHNY37.ny.ssmb.com>
	<p06002004bc910b9f64e5@[128.115.153.6]>
Message-ID: <x24qs44ivh.fsf@biostat.ku.dk>

Don MacQueen <macq at llnl.gov> writes:

> At 5:28 PM -0500 3/31/04, Kissell, Robert [EQRE] wrote:
> >Hi,
> >I have a couple of quick questions regarding R.
> >
> 
> <- snip ->
> 
> > 3) Is there anyway that I could save functions that I have written
> > outside of R. That is, could I write functions and have them saved
> > in some directory, such as, C:\Programs\RLanguage\RobFunctions\?
> > This way I would be able to copy the functions to other pcs. This is
> > very simple in MatLab. Please let me know.
> 
> Yes. Here are two ways.
> 
>   - 1 -
> In the directory where you have your function definitions, load the
> functions into R. Quit R, and say "yes" when asked whether or not to
> save the workspace image. Find the file in which the image is saved,
> copy it to the other pcs. On the other pcs, use the "attach" function
> to attach that file. The functions are now available.
> 
> -2-
> Go get the R Extensions manual, learn how to create a package. Take
> your functions, make them into a package. Copy the package file to the
> other pcs. Use R's "install package" functionality to install the
> package. Then, whenever the functions are needed, use the "library"
> command to load your package.

- 3 -

dump(c("fun1","fun2","fun3"),"path/to/myfuns.R")

and 

source("path/to/myfuns.R")

to get them into another session. (Can you even get started with -2-
without knowing this??)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Apr  1 03:02:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Apr 2004 03:02:38 +0200
Subject: [R] Is there a package or routine for calculating mimimum
	spanning trees?
In-Reply-To: <5.2.1.1.0.20040331185504.01cce0a0@biomserv.univ-lyon1.fr>
References: <5.2.1.1.0.20040331185504.01cce0a0@biomserv.univ-lyon1.fr>
Message-ID: <x2zn9w33yp.fsf@biostat.ku.dk>

Stephane DRAY <dray at biomserv.univ-lyon1.fr> writes:

> see mst in package ace
                     ***
                     ape, I believe...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From arcane at arcanemethods.com  Thu Apr  1 04:31:01 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 18:31:01 -0800
Subject: [R] Zero Index Origin?
In-Reply-To: <x28yhg4jm6.fsf@biostat.ku.dk>
References: <406A7E37.5060504@arcanemethods.com>	<loom.20040331T150145-904@post.gmane.org>	<406B3F92.8010402@arcanemethods.com>	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>	<406B48D3.9010502@arcanemethods.com>
	<x28yhg4jm6.fsf@biostat.ku.dk>
Message-ID: <406B7EE5.1010605@arcanemethods.com>



Peter Dalgaard wrote:


> I think it was said quite early in the thread, but since noone
> apparently listened, let me reiterate: One of the powerful indexing
> features in R is the negative index ("all, except") and x[-0] would
> lead to some complications if 0 was a true index. 
> 

Interesting.  The kind of gotcha I hoped wouldn't exist. 
Others?

I've thought of another one.  Legacy functions that accept 
indices as arguments from a 0-origin caller.


Bob
-- 

"Things should be described as simply as possible, but no 
simpler."

                                              A. Einstein



From ok at cs.otago.ac.nz  Thu Apr  1 04:43:04 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 1 Apr 2004 14:43:04 +1200 (NZST)
Subject: [R] Zero Index Origin?
Message-ID: <200404010243.i312h4jA515578@atlas.otago.ac.nz>


I note that (1) "[" is a class-based function in R, so it would be
possible to define a class of zero-origin arrays.  This would mean
that indexing these things would be quite incompatible with all the
other indexing in R, so it's not clear that it would be a good thing.
(2) R lets you define "left-hand functions", so if you define
    sub <- function (a, i) a[i+1]
    "sub<-" <- function (a, i, value) a[i+1] <- value
then you can use sub(a,i) and sub(a,i) <- ... for indexing.  This
still wouldn't work like R indexing in general, but it wouldn't
_look_ as if it should, so that might be better.

I used to love APL myself, and am a great fan of index origin 0, but
I'm *not* a fan of having two different origins in one language
controlled by a variable; it made life quite difficult trying to mix
code from different libraries.  

In R, I find myself wondering whether a["fred"] (string indexing)
should depend on index origin, and if not, why not.  (:-)

Despite my love for origin 0, I've decided that the rest of R is worth it.

Despite my love for origin 0, I've found that for the things that I do
with R, index origin 1 really does seem to work better as a human
interface.

It would be interesting to see some sample code where origin 0 is supposed
to make life easier, and to see what R experts to do make it even easier
than that.



From ecashin at uga.edu  Thu Apr  1 05:22:22 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 31 Mar 2004 22:22:22 -0500
Subject: [R] row selection based on median in data frame
Message-ID: <873c7otma9.fsf@uga.edu>

Hi.  I am having trouble thinking of an easy way to grab rows out of a
data frame.  I want to select the rows with a median value when the
rows are similar.

A simple example is this table, which I could read into a data frame.
I would like to find a new data frame with only the rows with a median
value for the "c" column given a certain "a" value.

For example, the c values for deadlift rows are 13, 8, and 5, so the
row with a c value of 8 should show up in the output.

        a          b          c
     1	deadlift   7          13 
     2	squat      7          24
     3	clean      7          10
     4	deadlift   8           8
     5	squat      8          20
     6	clean      8           2
     7  deadlift   9           5
     8  squat      9          32
     9  clean      9          19

Result:

        a          b          c
     4	deadlift   8           8
     5	squat      8          20
     3	clean      7          10

It's more complicated in my case, because I have not just one "a"
column, but about eight columns that have to be the same.  I can do
this with clumsy loops, but I wonder whether there's a better way.

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From ecashin at uga.edu  Thu Apr  1 05:39:50 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 31 Mar 2004 22:39:50 -0500
Subject: [R] row selection based on median in data frame
References: <873c7otma9.fsf@uga.edu>
Message-ID: <87zn9ws6wp.fsf@uga.edu>

Ed L Cashin <ecashin at uga.edu> writes:

> Hi.  I am having trouble thinking of an easy way to grab rows out of a
> data frame.  I want to select the rows with a median value when the
> rows are similar.

I'm still catching up on my R list reading, and I notice there is a
similar post to mine:

   Federico Calboli    
   data manipulation: getting mean value every 5 rows

I think the responses there answer my question, but I'll have to look
into it.  The responses say to use aggregate and an auxiliary row.

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From ggrothendieck at myway.com  Thu Apr  1 05:45:53 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 1 Apr 2004 03:45:53 +0000 (UTC)
Subject: [R] Zero Index Origin?
References: <406A7E37.5060504@arcanemethods.com>	<loom.20040331T150145-904@post.gmane.org>	<406B3F92.8010402@arcanemethods.com>	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>	<406B48D3.9010502@arcanemethods.com>
	<x28yhg4jm6.fsf@biostat.ku.dk> <406B7EE5.1010605@arcanemethods.com>
Message-ID: <loom.20040401T050941-391@post.gmane.org>

Bob Cain <arcane <at> arcanemethods.com> writes:
... zero origin ...

It occurred to me that perhaps the point about classes could be
framed somewhat differently.

I think you have an implicit assumption that vectors, matrices
and arrays in general should be used to represent the objects
that you are interested in for your DSP work.

This is an understandable assumption coming from an APL background
where everything is an array but in R and other OO languages you
can define your own classes that behave the way you want.  The
objective is not to shoehorn your application into preset structures 
like you do in APL but rather in R it is possible to create a model 
through its class and other facilities that closely models your domain.

Even within the classes offered to you out-of-the-box I wonder
if time series rather than vectors are not the appropriate
class?  For example, even if we accept indexing as the way to
go for the moment, a time series might have an advantage.
Furthermore, are you sure you want to loop over 0:length(whatever)-1.
Maybe that paradigm is just ingrained through lack of access to
anything better.  Might it not really be better to iterate over
time: t0, t0+deltat, ..., t1 in which 0-origin is inadequate so
why stop there when you can have it all?

The point is that rather than shoehorn your problem into vectors,
you could use the facilities of R (either ones that are already there
or ones you create for DSP) to act in precisely the right way and
in the process get rid of the baggage that comes with being forced
to iterate over 1,2,3,... or over 0,1,2,...  Going past that you
can go past iteration altogether and start dealing with whole objects
at once.

Finally, if you really approach this as how to set up one or more
DSP clases, the solution you come up with may not be to create a 0-origin
vector class but rather it should involve really understanding and modeling
your problem domain in a way which was never possible with the 
limited modelling facilities of APL in which everything is an array.

Its been said that you can program FORTRAN in any language. In the 
context of this discussion perhaps it could be said that you can
program in an APL-like style in R but to really get the benefit of
R you have to move to the appropriate program design style.  (I am
not putting down APL which is one of the most brilliant computer
science achievements I have ever seen -- just pointing out where to
go from there.)

Of course the above is motherhood and some specific examples
might put a sharper edge to the discussion.



From ecashin at uga.edu  Thu Apr  1 06:25:23 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 31 Mar 2004 23:25:23 -0500
Subject: [R] row selection based on median in data frame
References: <873c7otma9.fsf@uga.edu> <87zn9ws6wp.fsf@uga.edu>
Message-ID: <87smfos4ss.fsf@uga.edu>

Ed L Cashin <ecashin at uga.edu> writes:

> Ed L Cashin <ecashin at uga.edu> writes:
>
>> Hi.  I am having trouble thinking of an easy way to grab rows out of a
>> data frame.  I want to select the rows with a median value when the
>> rows are similar.
>
> I'm still catching up on my R list reading, and I notice there is a
> similar post to mine:
>
>    Federico Calboli    
>    data manipulation: getting mean value every 5 rows
>
> I think the responses there answer my question, but I'll have to look
> into it.  The responses say to use aggregate and an auxiliary row.

After consulting the docs and Venables and Ripley, I am not sure
aggregate can do what I'm looking for.  Given rows where certain
specified columns have the same values, I'd like to select the row
with the median value in another specified column ("runtime").

That is, after grouping the rows of the data frame based on the
columns in the by parameter, I want to select one whole row "as is",
the row with the median "runtime" value, without doing median on more
than one column.

     'aggregate.data.frame' is the data frame method.  If 'x' is not a
     data frame, it is coerced to one.  Then, each of the variables
     (columns) in 'x' is split into subsets of cases (rows) of
     identical combinations of the components of 'by', and 'FUN' is
     applied to each such subset with further arguments in '...' passed
     to it. (I.e., 'tapply(VAR, by, FUN, ..., simplify = FALSE)' is
     done for each variable 'VAR' in 'x', conveniently wrapped into one
     call to 'lapply()'.) 

Is there a way to tell aggregate just do perform median on column runtime to
select the whole row?  

     Empty subsets are removed, and the result is
     reformatted into a data frame containing the variables in 'by' and
     'x'.  The ones arising from 'by' contain the unique combinations
     of grouping values used for determining the subsets, and the ones
     arising from 'x' the corresponding summary statistics for the
     subset of the respective variables in 'x'.

I'd like to select all the columns, not just the ones I'm using to
group or the one from which I want to find the median value.  I think
I can't use aggregate after all.  But I must admit I'm very tired and
should go to bed.

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From rolf at biostatistics.dk  Thu Apr  1 07:19:49 2004
From: rolf at biostatistics.dk (Rolf Poalis)
Date: Thu, 1 Apr 2004 07:19:49 +0200
Subject: [R] New utility: sas2r
Message-ID: <20040401051949.GA1195@ekstroem.dk>

Dear R users,

Biostatistics Denmark would like to annouce the availability of the
new utility: sas2R --- a SAS to R parser.

For almost 40 years SAS has been the primary tool for statisticians
worldwide and its easy-to-learn syntax, unsurpassed graphical system,
powerful macro language and recent graphical user interfaces have made
SAS the number one statistical software choice for both beginners and
advanced users.

We find it unnecessary for users to learn yet another statistical
software package and have decided to make SAS available to a larger
audience by providing a SAS interface to the free statistical software
package R. It is our hope that the sas2R utility eventually will prove
as powerful as the true SAS system and we believe that this release
marks the beginning of a new era for R.

The sas2R utility consists of a parser (written in perl) that rewrites
the SAS code as R code and a package (sastor) of utility functions
that are needed to run the parsed code. sastor consists mainly of
functions for presenting output in the familiar and canonical SAS
format.

Download the utility at: http://www.biostatistics.dk/sas2r/index.html

Example: The following SAS code:

DATA test;
    INPUT x y f$;
    CARDS;
    1 2 m
    1 3 m
    2 4 m
    2 4 f
    1 5 m
    4 3 f
    3 2 f
;

PROC PRINT DATA=test;
  VAR x;    

PROC REG DATA=test;
  MODEL y = x;

RUN;  

        
is converted to 

library(sastor)
test <- data.frame(x = c(1, 1, 2, 2, 1, 4, 3), y = c(2, 3, 4, 4, 5, 3, 2), f = c("m", "m", "m", "f", "m", "f", "f"))
sas.print.print(test[, c("x"), drop=F])
sas.print.reg(lm(y ~ x, data=test))


which produces the following output:


The SAS2R System

Obs x
1   1
2   1
3   2
4   2
5   1
6   4
7   3

The SAS2R REG Procedure

Analysis of Variance

Dependent Variable: y
                Df Sum Sq Mean Sq F value Pr(>F)
Model            1 0.5000  0.5000  0.3608 0.5742
Error            5 6.9286  1.3857               
Corrected Total  6 7.4286  1.8857               

Root MSE              1.17716    R-Square     0.0673
Dependent Mean        3.28571    Adj R-Sq    -0.1192
Coeff Var            35.82672                       


Parameter Estimates:
            DF  Estimate Std. Error t value Pr(>|t|)  
(Intercept)  1   3.7857     0.9438   4.011   0.0102 *
x            1  -0.2500     0.4162  -0.601   0.5742  




Here is the DESCRIPTION file for the package:

Package: sas2r
Version: 1.04
Title: SAS-to-R parser
Author: Rolf Poalis <rolf at biostatistics.dk>
Maintainer: Rolf Poalis <rolf at biostatistics.dk>
Description: A collection of functions for running SAS code in R.
Depends: R (>= 1.4.0), perl >= 5.8.0
License: GPL
URL: www.biostatistics.dk/sas2r/index.html

The following SAS procedures are parsed succesfully:

PROC CONTENTS
PROC CORR
PROC FREQ
PROC GLM
PROC MEANS
PROC NPAR1WAY
PROC PRINT
PROC REG
PROC SORT
PROC TTEST
PROC UNIVARIATE

The following SAS procedures work to some degree:

PROC GENMOD
PROC GPLOT
PROC MIXED

Please note:

o  All SAS procedure calls must include a DATA=

We look forward to receiving questions, comments and suggestions.

Rolf Poalis, Biostatistics Denmark



From arcane at arcanemethods.com  Thu Apr  1 07:42:50 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 21:42:50 -0800
Subject: [R] New utility: sas2r
In-Reply-To: <20040401051949.GA1195@ekstroem.dk>
References: <20040401051949.GA1195@ekstroem.dk>
Message-ID: <406BABDA.8070303@arcanemethods.com>



Rolf Poalis wrote:

> Dear R users,
> 
> Biostatistics Denmark would like to annouce the availability of the
> new utility: sas2R --- a SAS to R parser.

Has this been done by anyone for Matlab?


Bob
-- 

"Things should be described as simply as possible, but no 
simpler."

                                              A. Einstein



From arcane at arcanemethods.com  Thu Apr  1 07:43:56 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 21:43:56 -0800
Subject: [R] Zero Index Origin?
In-Reply-To: <loom.20040401T050941-391@post.gmane.org>
References: <406A7E37.5060504@arcanemethods.com>	<loom.20040331T150145-904@post.gmane.org>	<406B3F92.8010402@arcanemethods.com>	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>	<406B48D3.9010502@arcanemethods.com>	<x28yhg4jm6.fsf@biostat.ku.dk>
	<406B7EE5.1010605@arcanemethods.com>
	<loom.20040401T050941-391@post.gmane.org>
Message-ID: <406BAC1C.3090805@arcanemethods.com>



Gabor Grothendieck wrote:

[snip good stuff]

> 
> Of course the above is motherhood and some specific examples
> might put a sharper edge to the discussion.
> 

I really appreciate your point of view on this and think you
are probably right.  A question I have from my very limited
understanding yet of OO is whether such objects could be
passed to legacy functions with any expectation of correct
results.


Thanks,

Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From arcane at arcanemethods.com  Thu Apr  1 07:44:16 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 31 Mar 2004 21:44:16 -0800
Subject: [R] Zero Index Origin?
In-Reply-To: <200404010243.i312h4jA515578@atlas.otago.ac.nz>
References: <200404010243.i312h4jA515578@atlas.otago.ac.nz>
Message-ID: <406BAC30.3000003@arcanemethods.com>



Richard A. O'Keefe wrote:

> 
> It would be interesting to see some sample code where origin 0 is supposed
> to make life easier, and to see what R experts to do make it even easier
> than that.

I think I will try and invite Robert Bristow Johnson into
this discussion.  He's one of the most respected voices in
DSP on the net and the most ardent critic of Matlab's 1
origin "problem" in DSP.  Interesting him in a project to
create a Matlike DSP package for R is one of the main
reasons I am inquiring about 0-origin.

Again, the more I get into this language (I just wish I
could suspend everything else I'm doing for the nonce) the
more I simply love it.


Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From ggrothendieck at myway.com  Thu Apr  1 08:51:59 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 1 Apr 2004 06:51:59 +0000 (UTC)
Subject: [R] Zero Index Origin?
References: <406A7E37.5060504@arcanemethods.com>	<loom.20040331T150145-904@post.gmane.org>	<406B3F92.8010402@arcanemethods.com>	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>	<406B48D3.9010502@arcanemethods.com>	<x28yhg4jm6.fsf@biostat.ku.dk>
	<406B7EE5.1010605@arcanemethods.com>
	<loom.20040401T050941-391@post.gmane.org>
	<406BAC1C.3090805@arcanemethods.com>
Message-ID: <loom.20040401T083937-336@post.gmane.org>

Bob Cain <arcane <at> arcanemethods.com> writes:

> A question I have from my very limited
> understanding yet of OO is whether such objects could be
> passed to legacy functions with any expectation of correct
> results.

Typically you need to write a wrapper but sometimes you get it for
free.  For example, in the vector0 class that I displayed previously,
note that no multiplication method was defined for it yet the
example included multiplication and it worked.  

Oarray would get the same benefit.

Note that if a routine f.legacy(x) does not make use of indices at all then
f(unclass(v)) where v is of class vector0 or Oarray would be sufficient
(untested).  Also you can define a generic that dispatches to the
appropriate class:

f <- function(x) UseMethod("f")    # defines a generic
f.Orray <- function(x) f(unclass(x))   
f.default <- f.legacy

Now you can call f(x) and if x is a legacy variable f.legacy gets
used and if x is Oarray then f.Oarray gets used.



From ggrothendieck at myway.com  Thu Apr  1 09:01:18 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 1 Apr 2004 07:01:18 +0000 (UTC)
Subject: [R] New utility: sas2r
References: <20040401051949.GA1195@ekstroem.dk>
	<406BABDA.8070303@arcanemethods.com>
Message-ID: <loom.20040401T085818-334@post.gmane.org>

Bob Cain <arcane <at> arcanemethods.com> writes:

> 
> Rolf Poalis wrote:
> 
> > Dear R users,
> > 
> > Biostatistics Denmark would like to annouce the availability of the
> > new utility: sas2R --- a SAS to R parser.
> 
> Has this been done by anyone for Matlab?
> 
> Bob


Don't know about that but at the bottom of:

   http://cran.r-project.org/other-docs.html

there are some links to R/Octave and R/Matlab lexicons.



From s-plus at wiwi.uni-bielefeld.de  Thu Apr  1 09:58:46 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 01 Apr 2004 09:58:46 +0200
Subject: [R] Zero Index Origin?
References: <200404010243.i312h4jA515578@atlas.otago.ac.nz>
Message-ID: <406BCBB6.8020700@wiwi.uni-bielefeld.de>

Richard A. O'Keefe wrote:

>It would be interesting to see some sample code where origin 0 is supposed
>to make life easier, ...
>
An application is the implementation of algorithms which use origin 0 
and are written in pseudo code.
Write down the statements in R syntax, include some print or browser 
statements and
you are able to demonstrate the working of different approaches. Here is 
an example for sorting
-- I know   sort(x)   is a better solution ...

sort.6<-function(a){
   n<-length(a)
   adapt<-function(i){i+1}  # local function to perform the index correction
   a<-c(0,a)
   for(i in 2:n){
      j<-i-1
      a[adapt(0)]<-a[adapt(i)]
      while(a[adapt(j)]>a[adapt(0)]){
         a[adapt(j+1)]<-a[adapt(j)]
         j<-j-1
      }
      a[adapt(j+1)]<-a[adapt(0)]
   }
   return(a[-1])
}

Peter Wolf



From info at rhkoning.com  Thu Apr  1 09:59:43 2004
From: info at rhkoning.com (Ruud H. Koning)
Date: Thu, 01 Apr 2004 09:59:43 +0200
Subject: [R] Gauss user question
In-Reply-To: <406B4D1E.4080402@columbia.edu>
References: <406B4D1E.4080402@columbia.edu>
Message-ID: <200404010959430984.00421F00@localhost>

I have used gauss for a number of years. It is my experience that R is
slower than gauss, but on the other hand R has much more functionality
available than gauss and that saves programming time (and time to debug).
The trick is of course to program efficiently in R, just as is the case
when you do ml in gauss. Ruud

*********** REPLY SEPARATOR  ***********

On 3/31/2004 at 5:58  Oleg Urminsky wrote:

>I have been using Gauss for doing maximum likelihood estimation of 
>somewhat complex models, and I am possibly interested in moving over to 
>R.  Does anyone have experience doing both?  My main concern is that my 
>models are very slow to estimate in Gauss; wondering if R is likely to 
>be faster or slower.
>
>Any feedback would be much appreciated!
>
>Thank you,
>
>Oleg Urminsky
>opu1 at columbia.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html



From prechelt at pcpool.mi.fu-berlin.de  Thu Apr  1 10:13:37 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Thu, 1 Apr 2004 10:13:37 +0200
Subject: [R] Zero Index Origin?
Message-ID: <85D25331FFB7AE4C900EA467D4ADA39201215D@circle.pcpool.mi.fu-berlin.de>

Bob,

on a remark about Brian Ripley's remark 
  if you're actually using indicies explicitly, 
  you probably haven't wrapped your head around
  how powerful the indexing structure and 
  "whole object" approach is in S and in R,
Gabor Grothendieck wrote:
> I believe the point was not so much that R has powerful indexing 
> (which it does) but that the capabilities of dealing with objects as 
> a whole, i.e. without using indexing at all, makes facilities for 
> indexing less important.

I believe what REALLY makes the question of zero origin versus
1-origin fairly unimportant is the fact that even when you use
indexing, you rarely need to know what the index numbers are.

In typical indexing expressions such as
  x[y==z]
(that is, selecting a subset from one vector, based on agreement
between two other vectors) you just do not care whether the index
numbers that are used implicitly in the process start at
0, 1, or, for that matter, 314.

And, as Brian Ripley has also pointed out: even the expressions 
of the form 1:n should 1:n usually rather be someting like 
seq(length=n) or seq(along=x) instead, which again would even
allow indexing starting at 314 (and progressing in steps of 17 if
you'd like).

  Lutz

Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
Institut f??r Informatik; Freie Universit??t Berlin
Takustr. 9; 14195 Berlin; Germany
+49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/



From ripley at stats.ox.ac.uk  Thu Apr  1 10:16:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Apr 2004 09:16:41 +0100 (BST)
Subject: [R] Zero Index Origin?
In-Reply-To: <406BCBB6.8020700@wiwi.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.44.0404010911570.31705-100000@gannet.stats>

I think that is an excellent illustration of my point:

> If you are writing code that works with single elements, you are
> probably a lot better off writing C code to link into R (and C is
> 0-based ...).

but even in R it is not following

> However, the R thinking is to work with whole objects (vectors, arrays,
> lists ...) and you rather rarely need to know what numbers are in an 
> index vector. 

since you can shift whole blocks at a time rather than use a while loop.

On Thu, 1 Apr 2004, Peter Wolf wrote:

> Richard A. O'Keefe wrote:
> 
> >It would be interesting to see some sample code where origin 0 is supposed
> >to make life easier, ...
> >
> An application is the implementation of algorithms which use origin 0 
> and are written in pseudo code.
> Write down the statements in R syntax, include some print or browser 
> statements and
> you are able to demonstrate the working of different approaches. Here is 
> an example for sorting
> -- I know   sort(x)   is a better solution ...
> 
> sort.6<-function(a){
>    n<-length(a)
>    adapt<-function(i){i+1}  # local function to perform the index correction
>    a<-c(0,a)
>    for(i in 2:n){
>       j<-i-1
>       a[adapt(0)]<-a[adapt(i)]
>       while(a[adapt(j)]>a[adapt(0)]){
>          a[adapt(j+1)]<-a[adapt(j)]
>          j<-j-1
>       }
>       a[adapt(j+1)]<-a[adapt(0)]
>    }
>    return(a[-1])
> }
> 
> Peter Wolf
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rksh at soc.soton.ac.uk  Thu Apr  1 11:20:04 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 1 Apr 2004 09:20:04 +0000
Subject: [R] array addition doesn't recycle!
In-Reply-To: <38C4C095FC35E5469BED686B42F40A1307802B99@usrymx17.merck.com>
References: <38C4C095FC35E5469BED686B42F40A1307802B99@usrymx17.merck.com>
Message-ID: <a0600200abc918cb6c756@[139.166.242.29]>

At 01:39 pm -0500 31/03/04, Raubertas, Richard wrote:
>Another alternative is to use the underappreciated function
>'sweep()':
>
>sweep(A, 1:2, a, "+")
>

I find the following helpful (it's not due to me but I cannot find 
the original poster):


>  "%.+%" <- function(a,x){sweep(a , 2:1 , x ,"+" )}
>  "%+.%" <- function(a,x){sweep(a , 1:2 , x ,"+" )}

>  A <- matrix(1:16,4,4)
>  x <- 10^(0:3)

>  A %+.% x
      [,1] [,2] [,3] [,4]
[1,]    2    6   10   14
[2,]   12   16   20   24
[3,]  103  107  111  115
[4,] 1004 1008 1012 1016
>  A %.+% x
      [,1] [,2] [,3] [,4]
[1,]    2   15  109 1013
[2,]    3   16  110 1014
[3,]    4   17  111 1015
[4,]    5   18  112 1016
>


For 3d arrays this generalizes to

  "%+..%" <- function(a,x){sweep(a , 1 , x ,"+" )}
  "%.+.%" <- function(a,x){sweep(a , 2 , x ,"+" )}
  "%..+%" <- function(a,x){sweep(a , 3 , x ,"+" )}

Then if A <- array(1:8,rep(2,3)) and x <- c(10,100)

A %+..% x et seq give you a consistent method for addition.


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From arcane at arcanemethods.com  Thu Apr  1 10:41:09 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Thu, 01 Apr 2004 00:41:09 -0800
Subject: [R] Zero Index Origin?
In-Reply-To: <loom.20040401T083937-336@post.gmane.org>
References: <406A7E37.5060504@arcanemethods.com>	<loom.20040331T150145-904@post.gmane.org>	<406B3F92.8010402@arcanemethods.com>	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>	<406B48D3.9010502@arcanemethods.com>	<x28yhg4jm6.fsf@biostat.ku.dk>	<406B7EE5.1010605@arcanemethods.com>	<loom.20040401T050941-391@post.gmane.org>	<406BAC1C.3090805@arcanemethods.com>
	<loom.20040401T083937-336@post.gmane.org>
Message-ID: <406BD5A5.5030803@arcanemethods.com>



Gabor Grothendieck wrote:
> Now you can call f(x) and if x is a legacy variable f.legacy gets
> used and if x is Oarray then f.Oarray gets used.
> 

Thanks, Gabor.  While I didn't understand the answer at all,
I expect that I will eventually and I appreciate the thought
that went into the affirmative.


Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From nedluk at yahoo.it  Thu Apr  1 12:10:59 2004
From: nedluk at yahoo.it (=?iso-8859-1?q?michele=20lux?=)
Date: Thu, 1 Apr 2004 12:10:59 +0200 (CEST)
Subject: [R] Search Engine & Keywords
Message-ID: <20040401101059.30636.qmail@web13606.mail.yahoo.com>

Hallo all
I don't understand why the "Search Engine & Keywords"
comand on the html help of R 1.8 doesn't work, is it
active? or the problem is in my machine? 
thanks michele

______________________________________________________________________

http://it.yahoo.com/mail_it/foot/?http://it.mail.yahoo.com/



From ripley at stats.ox.ac.uk  Thu Apr  1 12:49:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Apr 2004 11:49:36 +0100 (BST)
Subject: [R] Search Engine & Keywords
In-Reply-To: <20040401101059.30636.qmail@web13606.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0404011140050.32175-100000@gannet.stats>

The problem is with your machine (and some other people's).  That search 
engine does work on a properly configured machine.

Since haven't given us anything to go on (like which OS, browser, R
version (there is no such thing as R 1.8) ...) I can only suggest you look
at the R 1.9.0 beta which is currently available, as the search page now
has a link to the Installation and Administration manual which now
contains detailed information.

A quick check list:

javascript enabled
java installed and enabled
if Linux/Unix, Sun JRE 1.4.2_02/3/4 are broken.

Oh, and this comes up quite frequently, so you can look in the archives of 
this list.

On Thu, 1 Apr 2004, michele lux wrote:

> I don't understand why the "Search Engine & Keywords"
> comand on the html help of R 1.8 doesn't work, is it
> active? or the problem is in my machine? 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From felix_eggers at gmx.de  Thu Apr  1 13:38:08 2004
From: felix_eggers at gmx.de (Felix Eggers)
Date: Thu, 01 Apr 2004 13:38:08 +0200
Subject: [R] choice-based conjoint
In-Reply-To: <F9C21EA3118FF64BB2A83E914E11BA4878EAC2@ADE-EXCH-004>
Message-ID: <BC91CBC0.2C7E%felix_eggers@gmx.de>

Thank you for your answers and suggestioins!

I just read a tutorial on the use of SAS with Discrete Choice Models by
Warren Kuhfeld ("Marketing Research Methods in SAS", 2004, free for
download). He suggests using the Cox PH model. It necessitates reformatting
the data so that the alternatives chosen have a constant survival time of 1
and the other alternatives in the choice set are marked as censored. Thus,
the varying choice sets can be considered in the estimation.

The translation to R seems straightforward, so I will try it out. I might
come back to this list, if I face problems (as I haven't used R before)...

Felix


Am 31.03.2004 15:31 Uhr schrieb "stephen.kay at adelphigroup.com" unter
<stephen.kay at adelphigroup.com>:

> The multinomial model can be used- it's the standard method used, although
> there are now more sophisticated ones - see Kenneth Train's website at
> Berkley for the Mixed Logit. I too am new to R so can't really comment on
> how to implement it in this package. If you want to run more sophisticated
> models (e.g. Mixed Logit) and you are an academic (non-commercial) you could
> download a free version of Ox (a matrix programming language very similar to
> GAUSS) and do a search for Mixed Logit - some one wrote a free routine that
> converts Ken Train's GAUSS code to Ox. Afraid I don't have time to look up
> the website addresses.
> 
> Hope this helps,
> 
> Stephen
> Stephen Kay
> Head of Statistics
> 
> Adelphi Group Products
> www.adelphigroup.com
> 
> 
> -----Original Message-----
> From: Felix Eggers [mailto:felix_eggers at gmx.de]
> Sent: 31 March 2004 13:50
> To: r-help at stat.math.ethz.ch
> Subject: [R] choice-based conjoint
> 
> 
> Hello everyone,
> 
> I am new to this list and the R-Project, so I hope my question is not
> trivial or has been answered before. I searched the FAQs and the mailing
> list archives and I could not find anything about Conjoint Analysis. I am
> especially interested in Choice-based Conjoint, resp. discrete choice
> models. 
> 
> Is there a function / module that handles this issue? Or can the multinomial
> logit model be used? I would doubt the latter, for in a Choice-based
> Conjoint analysis the choice has to be conditioned to the alternatives in
> the choice set which are not equal for the respondents.
> 
> Any help will be much appreciated! Thank you!
> 
> Felix
> 
>   [[alternative HTML version deleted]]



From oleg at sai.msu.su  Thu Apr  1 13:38:54 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 1 Apr 2004 15:38:54 +0400 (MSD)
Subject: [R] multiple plots problem
Message-ID: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>

Hello,

for testing & learning purposes I create X11 device and specify layout like
layout(c(1,2,3), 3, 1), so I could play with parameters and see
several plots at the same time. That works fine until I try to create 4-th
plot - all other plots erased. Such behaviour isn't desirable for testing
purposes and I'm asking  where to look to disable erasing other plots.

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From d.orme at imperial.ac.uk  Thu Apr  1 13:55:13 2004
From: d.orme at imperial.ac.uk (David Orme)
Date: Thu, 1 Apr 2004 12:55:13 +0100
Subject: [R] Confidence Intervals for slopes
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E3AF@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90179E3AF@exdkba022.novo.dk>
Message-ID: <6F0148BF-83D3-11D8-9BD0-000393DC1748@ic.ac.uk>

Hi,

Thanks for the pointer - the 'lm(y~x:z)'  model does give the slopes 
directly and hence confint gives the confidence intervals.

The thing that puzzles me is that my dummy data explicitly sets the 
three levels of the factor to have different variances and yet the 
standard error is the same for all three parameter estimates in the 
summary.lm output - is this a common standard error of the 'x:z' term 
in the model? If you fit a separate regression to subsets of the data 
for each level in 'z' then the standard errors of the slope reflect 
these differences in variance. What I was trying to get was confidence 
limits from within a single model that also reflect the difference in 
certainty about the three slopes.

I realize that this is a failing of my understanding and more a stats 
question than an R question - if anyone can give me any advice or 
pointers, that would be great.

Thanks,
David

On 29 Mar 2004, at 20:04, BXC (Bendix Carstensen) wrote:

> You may want:
>
> lm( y ~ x:z )
>
> This is the same model you fitted, but prametrized differently.
> But please check that what you REALLY want is not
>
> lm( y ~ z + x:z )
>
> This is the model with different intercepts as well.
>
> Bendix Carstensen
> ----------------------
> Bendix Carstensen
> Senior Statistician
> Steno Diabetes Center
> Niels Steensens Vej 2
> DK-2820 Gentofte
> Denmark
> tel: +45 44 43 87 38
> mob: +45 30 75 87 38
> fax: +45 44 43 07 06
> bxc at steno.dk
> www.biostat.ku.dk/~bxc
> ----------------------
>
>
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David Orme
>> Sent: Monday, March 29, 2004 4:44 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Confidence Intervals for slopes
>>
>>
>> Hi,
>>
>> I'm trying to get confidence intervals to slopes from a linear model
>> and I can't figure out how to get at them. As a cut 'n' paste example:
>>
>> #################
>> # dummy dataset - regression data for 3 treatments, each
>> treatment with
>> different (normal) variance
>> x <- rep(1:10, length=30)
>> y <- 10 - (rep(c(0.2,0.5,0.8), each=10)*x)+c(rnorm(10, sd=0.1),
>> rnorm(10, sd=0.6),rnorm(10, sd=1.1))
>> z <- gl(3,10)
>> plot(y~x, pch=unclass(z))
>>
>> # model as three slopes with common intercept
>> options(contrasts=c("contr.treatment","contr.poly"))
>> model <- lm(y~x+x:z)
>>
>> # coefficient table in summary gives the intercept, first
>> slope and the
>> difference in slopes
>> summary(model)
>>
>> # confint gives the confidence interval for the intercept and first
>> slope,
>> # and the CIs for the _differences_
>> confint(model)
>> #################
>>
>> What I'd like to report are the actual CI's for the slopes for the
>> second and third treatments, in the same way that confint returns the
>> parameter estimates for the first treatment. Can anyone point
>> me in the
>> right direction?
>>
>> Thanks,
>> David
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>> PLEASE
>> do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>



From p.dalgaard at biostat.ku.dk  Thu Apr  1 14:13:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Apr 2004 14:13:18 +0200
Subject: [R] New utility: sas2r
In-Reply-To: <406BABDA.8070303@arcanemethods.com>
References: <20040401051949.GA1195@ekstroem.dk>
	<406BABDA.8070303@arcanemethods.com>
Message-ID: <x2fzbnsxpd.fsf@biostat.ku.dk>

Bob Cain <arcane at arcanemethods.com> writes:

> Rolf Poalis wrote:
> 
> > Dear R users,
> > Biostatistics Denmark would like to annouce the availability of the
> > new utility: sas2R --- a SAS to R parser.
> 
> Has this been done by anyone for Matlab?

I think there was a group at U.of Apraphulia working on it.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jlozano at apoy.upm.edu.ph  Thu Apr  1 14:14:44 2004
From: jlozano at apoy.upm.edu.ph (Jingky P. Lozano)
Date: Thu, 01 Apr 2004 20:14:44 +0800 (PHT)
Subject: [R] Question on Data Simulation 
Message-ID: <1080821684.406c07b41a2ea@web.upm.edu.ph>

Dear mailing list,

Do you know a specific package in R where I can create an artificial survival 
data with controlled censoring condition?  (For example, I want to simulate a 
data set with 5 variables and 20% of the observations are censored.)

Many thanks.
Jei



From feh3k at spamcop.net  Thu Apr  1 14:04:14 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 1 Apr 2004 07:04:14 -0500
Subject: [R] New utility: sas2r
In-Reply-To: <20040401051949.GA1195@ekstroem.dk>
References: <20040401051949.GA1195@ekstroem.dk>
Message-ID: <20040401070414.43a3976d.feh3k@spamcop.net>

On Thu, 1 Apr 2004 07:19:49 +0200
Rolf Poalis <rolf at biostatistics.dk> wrote:

> Dear R users,
> 
> Biostatistics Denmark would like to annouce the availability of the
> new utility: sas2R --- a SAS to R parser.
> 
> For almost 40 years SAS has been the primary tool for statisticians
> worldwide and its easy-to-learn syntax, unsurpassed graphical system,
> powerful macro language and recent graphical user interfaces have made
> SAS the number one statistical software choice for both beginners and
> advanced users.

I think this is an excellent idea although one could argue with
"unsurpassed graphical system" (name a system that is worse at this) and
with "powerful macro language".  What is really needed is a parser for
complex DATA steps, as more analysts have problems recoding and
manipulating data than with figuring out the syntax for lm, glm, etc. 
Does sas2R correctly handle complex DATA steps with MERGE, SET, OUTPUT,
IF, etc.?

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From joehl at gmx.de  Thu Apr  1 15:04:22 2004
From: joehl at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Thu, 1 Apr 2004 15:04:22 +0200 (MEST)
Subject: [R] How to cluster the variables instead of data points?
Message-ID: <16373.1080824662@www45.gmx.net>


Have a look at function varclus in package Hmisc.

Best


Jens Oehlschl??gel

--



From p.pagel at gsf.de  Thu Apr  1 15:14:49 2004
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 1 Apr 2004 15:14:49 +0200
Subject: [R] multiple plots problem
In-Reply-To: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
Message-ID: <20040401131449.GA11297@localhost>

	
> for testing & learning purposes I create X11 device and specify layout like
> layout(c(1,2,3), 3, 1), so I could play with parameters and see
> several plots at the same time. That works fine until I try to create 4-th
> plot - all other plots erased.

That's expected behaviour: you asked for three plots to be present on
your canvas.

> Such behaviour isn't desirable for testing purposes and I'm asking
> where to look to disable erasing other plots.

Two possible solutions - depending on what exactly you'd like to get:

Increase the size of the layout matrix to squeeze more plots on the
canvas or open a new device whenever you want a new window.

cu
	Philipp

-- 
Dr. Philipp Pagel                            Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS          Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany
http://mips.gsf.de/~pagel



From dmurdoch at pair.com  Thu Apr  1 15:37:35 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 01 Apr 2004 08:37:35 -0500
Subject: [R] multiple plots problem
In-Reply-To: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
Message-ID: <s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>

On Thu, 1 Apr 2004 15:38:54 +0400 (MSD), Oleg Bartunov
<oleg at sai.msu.su> wrote :

>Hello,
>
>for testing & learning purposes I create X11 device and specify layout like
>layout(c(1,2,3), 3, 1), so I could play with parameters and see
>several plots at the same time. That works fine until I try to create 4-th
>plot - all other plots erased. Such behaviour isn't desirable for testing
>purposes and I'm asking  where to look to disable erasing other plots.

In Windows you can record all plots by using the record=TRUE option
when you open the graphics display, or from the menu on the display
window.  You cycle back through the history by hitting the PgUp key.  

You're using X11, so you might need to save your plots explicitly
using recordPlot() and then use replayPlot() to see it again (but your
front-end might provide access to them more easily, the way Windows
does).

Duncan Murdoch



From rolf at math.unb.ca  Thu Apr  1 15:41:50 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 1 Apr 2004 09:41:50 -0400 (AST)
Subject: [R] New utility: sas2r
Message-ID: <200404011341.i31DfoMv014390@erdos.math.unb.ca>


Rolf Poalis wrote:

> Biostatistics Denmark would like to annouce the availability of the
> new utility: sas2R --- a SAS to R parser.
> 
> For almost 40 years SAS has been the primary tool for statisticians
> worldwide and its easy-to-learn syntax, unsurpassed graphical system,
> powerful macro language and recent graphical user interfaces have made
> SAS the number one statistical software choice for both beginners and
> advanced users.

	Easy to learn syntax?  Unsurpassed graphical system?

	This ***is*** an April Fool's joke, isn't it?

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From oleg at sai.msu.su  Thu Apr  1 15:43:45 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 1 Apr 2004 17:43:45 +0400 (MSD)
Subject: [R] multiple plots problem
In-Reply-To: <20040401131449.GA11297@localhost>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
	<20040401131449.GA11297@localhost>
Message-ID: <Pine.GSO.4.58.0404011735440.11543@ra.sai.msu.su>

On Thu, 1 Apr 2004, Philipp Pagel wrote:

>
> > for testing & learning purposes I create X11 device and specify layout like
> > layout(c(1,2,3), 3, 1), so I could play with parameters and see
> > several plots at the same time. That works fine until I try to create 4-th
> > plot - all other plots erased.
>
> That's expected behaviour: you asked for three plots to be present on
> your canvas.
>

ok, how to recycle canvas ?

> > Such behaviour isn't desirable for testing purposes and I'm asking
> > where to look to disable erasing other plots.
>
> Two possible solutions - depending on what exactly you'd like to get:
>
> Increase the size of the layout matrix to squeeze more plots on the
> canvas or open a new device whenever you want a new window.

Well, I know them and don't like. It's strange there is no way
to say don't erase the whole canvas (it's ok if hi-level function erase
current frame).


>
> cu
> 	Philipp
>
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From oleg at sai.msu.su  Thu Apr  1 15:48:11 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 1 Apr 2004 17:48:11 +0400 (MSD)
Subject: [R] multiple plots problem
In-Reply-To: <s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
	<s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>
Message-ID: <Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>

On Thu, 1 Apr 2004, Duncan Murdoch wrote:

> On Thu, 1 Apr 2004 15:38:54 +0400 (MSD), Oleg Bartunov
> <oleg at sai.msu.su> wrote :
>
> >Hello,
> >
> >for testing & learning purposes I create X11 device and specify layout like
> >layout(c(1,2,3), 3, 1), so I could play with parameters and see
> >several plots at the same time. That works fine until I try to create 4-th
> >plot - all other plots erased. Such behaviour isn't desirable for testing
> >purposes and I'm asking  where to look to disable erasing other plots.
>
> In Windows you can record all plots by using the record=TRUE option
> when you open the graphics display, or from the menu on the display
> window.  You cycle back through the history by hitting the PgUp key.
>
> You're using X11, so you might need to save your plots explicitly
> using recordPlot() and then use replayPlot() to see it again (but your
> front-end might provide access to them more easily, the way Windows
> does).

Thanks, looks like too much work :( I just don't understand a logic
why I can't recycle canvas.

>
> Duncan Murdoch
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From gb at stat.umu.se  Thu Apr  1 15:48:06 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 1 Apr 2004 15:48:06 +0200
Subject: [R] New utility: sas2r
In-Reply-To: <x2fzbnsxpd.fsf@biostat.ku.dk>
References: <20040401051949.GA1195@ekstroem.dk>
	<406BABDA.8070303@arcanemethods.com> <x2fzbnsxpd.fsf@biostat.ku.dk>
Message-ID: <20040401134806.GA1521@stat.umu.se>

On Thu, Apr 01, 2004 at 02:13:18PM +0200, Peter Dalgaard wrote:
> Bob Cain <arcane at arcanemethods.com> writes:
> 
> > Rolf Poalis wrote:
> > 
> > > Dear R users,
> > > Biostatistics Denmark would like to annouce the availability of the
> > > new utility: sas2R --- a SAS to R parser.
> > 
> > Has this been done by anyone for Matlab?
> 
> I think there was a group at U.of Apraphulia working on it.

Peter, I think it was Aprilfoolia.
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From HankeA at mar.dfo-mpo.gc.ca  Thu Apr  1 15:25:08 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 01 Apr 2004 09:25:08 -0400
Subject: [R] multiple plots problem
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12497C@msgmarsta01.bio.dfo.ca>

The command:
layout(c(1,2,3), 3, 1) specifies 3 plots
Try
layout(1:4,2,2,byrow=T)

Regards,
Alex
-----Original Message-----
From: Oleg Bartunov [mailto:oleg at sai.msu.su] 
Sent: April 1, 2004 7:39 AM
To: R-help
Subject: [R] multiple plots problem


Hello,

for testing & learning purposes I create X11 device and specify layout like
layout(c(1,2,3), 3, 1), so I could play with parameters and see
several plots at the same time. That works fine until I try to create 4-th
plot - all other plots erased. Such behaviour isn't desirable for testing
purposes and I'm asking  where to look to disable erasing other plots.

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From angel_lul at hotmail.com  Thu Apr  1 17:03:55 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Thu, 01 Apr 2004 16:03:55 +0100
Subject: [R] multiple plots problem
In-Reply-To: <Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>	<s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>
	<Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>
Message-ID: <406C2F5B.7030600@hotmail.com>

Have you considered using
?split.screen
and
?erase.screen
with a non-transparent background
instead of layout.
Example:
par(bg = "white")           # default is likely to be transparent
split.screen(c(3,1))        # split display into three screens
screen(1)
plot(1:10)
screen(2)
plot(2:20)
screen(3)
plot(3:30)

screen(1)
erase.screen()
plot(sin(40:400))   

Angel

Oleg Bartunov wrote:

>On Thu, 1 Apr 2004, Duncan Murdoch wrote:
>
>  
>
>>On Thu, 1 Apr 2004 15:38:54 +0400 (MSD), Oleg Bartunov
>><oleg at sai.msu.su> wrote :
>>
>>    
>>
>>>Hello,
>>>
>>>for testing & learning purposes I create X11 device and specify layout like
>>>layout(c(1,2,3), 3, 1), so I could play with parameters and see
>>>several plots at the same time. That works fine until I try to create 4-th
>>>plot - all other plots erased. Such behaviour isn't desirable for testing
>>>purposes and I'm asking  where to look to disable erasing other plots.
>>>      
>>>
>>In Windows you can record all plots by using the record=TRUE option
>>when you open the graphics display, or from the menu on the display
>>window.  You cycle back through the history by hitting the PgUp key.
>>
>>You're using X11, so you might need to save your plots explicitly
>>using recordPlot() and then use replayPlot() to see it again (but your
>>front-end might provide access to them more easily, the way Windows
>>does).
>>    
>>
>
>Thanks, looks like too much work :( I just don't understand a logic
>why I can't recycle canvas.
>
>  
>
>>Duncan Murdoch
>>
>>    
>>
>
>	Regards,
>		Oleg
>_____________________________________________________________
>Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
>Sternberg Astronomical Institute, Moscow University (Russia)
>Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
>phone: +007(095)939-16-83, +007(095)939-23-83
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>.
>
>  
>



From oleg at sai.msu.su  Thu Apr  1 16:23:48 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 1 Apr 2004 18:23:48 +0400 (MSD)
Subject: [R] multiple plots problem
In-Reply-To: <406C2F5B.7030600@hotmail.com>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
	<s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>
	<Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>
	<406C2F5B.7030600@hotmail.com>
Message-ID: <Pine.GSO.4.58.0404011813370.11543@ra.sai.msu.su>

interesting, it works, but still need to explicitly specify screen number.
It'b be nice to have tabbed canvas, but I suspect it's wrong list.

	Oleg
On Thu, 1 Apr 2004, Angel Lopez wrote:

> Have you considered using
> ?split.screen
> and
> ?erase.screen
> with a non-transparent background
> instead of layout.
> Example:
> par(bg = "white")           # default is likely to be transparent
> split.screen(c(3,1))        # split display into three screens
> screen(1)
> plot(1:10)
> screen(2)
> plot(2:20)
> screen(3)
> plot(3:30)
>
> screen(1)
> erase.screen()
> plot(sin(40:400))
>
> Angel
>
> Oleg Bartunov wrote:
>
> >On Thu, 1 Apr 2004, Duncan Murdoch wrote:
> >
> >
> >
> >>On Thu, 1 Apr 2004 15:38:54 +0400 (MSD), Oleg Bartunov
> >><oleg at sai.msu.su> wrote :
> >>
> >>
> >>
> >>>Hello,
> >>>
> >>>for testing & learning purposes I create X11 device and specify layout like
> >>>layout(c(1,2,3), 3, 1), so I could play with parameters and see
> >>>several plots at the same time. That works fine until I try to create 4-th
> >>>plot - all other plots erased. Such behaviour isn't desirable for testing
> >>>purposes and I'm asking  where to look to disable erasing other plots.
> >>>
> >>>
> >>In Windows you can record all plots by using the record=TRUE option
> >>when you open the graphics display, or from the menu on the display
> >>window.  You cycle back through the history by hitting the PgUp key.
> >>
> >>You're using X11, so you might need to save your plots explicitly
> >>using recordPlot() and then use replayPlot() to see it again (but your
> >>front-end might provide access to them more easily, the way Windows
> >>does).
> >>
> >>
> >
> >Thanks, looks like too much work :( I just don't understand a logic
> >why I can't recycle canvas.
> >
> >
> >
> >>Duncan Murdoch
> >>
> >>
> >>
> >
> >	Regards,
> >		Oleg
> >_____________________________________________________________
> >Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> >Sternberg Astronomical Institute, Moscow University (Russia)
> >Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> >phone: +007(095)939-16-83, +007(095)939-23-83
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >.
> >
> >
> >
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From angel_lul at hotmail.com  Thu Apr  1 17:57:01 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Thu, 01 Apr 2004 16:57:01 +0100
Subject: [R] multiple plots problem
In-Reply-To: <Pine.GSO.4.58.0404011813370.11543@ra.sai.msu.su>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>	<s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>	<Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>	<406C2F5B.7030600@hotmail.com>
	<Pine.GSO.4.58.0404011813370.11543@ra.sai.msu.su>
Message-ID: <406C3BCD.3000909@hotmail.com>

Depends on what you understand by _explicitly_ specify screen number:

par(bg = "white")           # default is likely to be transparent
screenlist<-split.screen(c(3,1))        # split display into three
i<-screenlist[1]

# Create a function to simulate the "tabbing"
nextplot<-function(i)
{
i<-i+1
if (i>screenlist[length(screenlist)]) i<-screenlist[1]
screen(i)
erase.screen(i)
return(i)
}

plot(1:10)
i<-nextplot(i)
plot(2:20)
i<-nextplot(i)
plot(3:30)
i<-nextplot(i)
plot(sin(40:400))
i<-nextplot(i)
plot(sin(4:40))
## Continue your plots
close.screen(all = TRUE)

Oleg Bartunov wrote:
> interesting, it works, but still need to explicitly specify screen number.
> It'b be nice to have tabbed canvas, but I suspect it's wrong list.
> 
> 	Oleg
> On Thu, 1 Apr 2004, Angel Lopez wrote:
> 
> 
>>Have you considered using
>>?split.screen
>>and
>>?erase.screen
>>with a non-transparent background
>>instead of layout.
>>Example:
>>par(bg = "white")           # default is likely to be transparent
>>split.screen(c(3,1))        # split display into three screens
>>screen(1)
>>plot(1:10)
>>screen(2)
>>plot(2:20)
>>screen(3)
>>plot(3:30)
>>
>>screen(1)
>>erase.screen()
>>plot(sin(40:400))
>>
>>Angel
>>
>>Oleg Bartunov wrote:
>>
>>
>>>On Thu, 1 Apr 2004, Duncan Murdoch wrote:
>>>
>>>
>>>
>>>
>>>>On Thu, 1 Apr 2004 15:38:54 +0400 (MSD), Oleg Bartunov
>>>><oleg at sai.msu.su> wrote :
>>>>
>>>>
>>>>
>>>>
>>>>>Hello,
>>>>>
>>>>>for testing & learning purposes I create X11 device and specify layout like
>>>>>layout(c(1,2,3), 3, 1), so I could play with parameters and see
>>>>>several plots at the same time. That works fine until I try to create 4-th
>>>>>plot - all other plots erased. Such behaviour isn't desirable for testing
>>>>>purposes and I'm asking  where to look to disable erasing other plots.
>>>>>
>>>>>
>>>>
>>>>In Windows you can record all plots by using the record=TRUE option
>>>>when you open the graphics display, or from the menu on the display
>>>>window.  You cycle back through the history by hitting the PgUp key.
>>>>
>>>>You're using X11, so you might need to save your plots explicitly
>>>>using recordPlot() and then use replayPlot() to see it again (but your
>>>>front-end might provide access to them more easily, the way Windows
>>>>does).
>>>>
>>>>
>>>
>>>Thanks, looks like too much work :( I just don't understand a logic
>>>why I can't recycle canvas.
>>>
>>>
>>>
>>>
>>>>Duncan Murdoch
>>>>
>>>>
>>>>
>>>
>>>	Regards,
>>>		Oleg
>>>_____________________________________________________________
>>>Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
>>>Sternberg Astronomical Institute, Moscow University (Russia)
>>>Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
>>>phone: +007(095)939-16-83, +007(095)939-23-83
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>.
>>>
>>>
>>>
>>
> 
> 	Regards,
> 		Oleg
> _____________________________________________________________
> Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> Sternberg Astronomical Institute, Moscow University (Russia)
> Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> phone: +007(095)939-16-83, +007(095)939-23-83
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> .
>



From Rau at demogr.mpg.de  Thu Apr  1 17:11:31 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 1 Apr 2004 17:11:31 +0200
Subject: [R] Question on Data Simulation
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0801@hermes.demogr.mpg.de>

Hello,

> -----Original Message-----
> From:	"Jingky P. Lozano" [SMTP:jlozano at apoy.upm.edu.ph]
> Sent:	Thursday, April 01, 2004 2:15 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] Question on Data Simulation 
> 
> Dear mailing list,
> 
> Do you know a specific package in R where I can create an artificial
> survival 
> data with controlled censoring condition?  (For example, I want to
> simulate a 
> data set with 5 variables and 20% of the observations are censored.)
> 
	maybe I misunderstood something but why don't you create a variable,
often denoted by 'status' in survival analysis, where the probabilities are
0.2 for censoring and 0.8 for the event and add this to your other data?
	The code should like like this (for a hypothetical size of 1000
individuals):

	status = sample(x=c(0,1), size=1000, replace=TRUE, prob=c(0.2,0.8))


	Best,
	Roland

	P.S. Just a matter of style: if you (=mailing list members) are
asked by someone how to do something in R, do you prefer to give the names
of the arguments in the functions, or do you prefer to keep it short like:
	status = sample(c(0,1), 1000, TRUE, c(0.2,0.8))


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From oleg at sai.msu.su  Thu Apr  1 17:18:52 2004
From: oleg at sai.msu.su (Oleg Bartunov)
Date: Thu, 1 Apr 2004 19:18:52 +0400 (MSD)
Subject: [R] multiple plots problem
In-Reply-To: <406C3BCD.3000909@hotmail.com>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
	<s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>
	<Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>
	<406C2F5B.7030600@hotmail.com>
	<Pine.GSO.4.58.0404011813370.11543@ra.sai.msu.su>
	<406C3BCD.3000909@hotmail.com>
Message-ID: <Pine.GSO.4.58.0404011859250.11543@ra.sai.msu.su>

Angel,

thanks for example, but I think you don't understand me - I was looking
native way to say R, please, don't erase canvas. Any emulations required
additional typing (so many in R! - readline is great, but no support for
prefix search as in bash)) and some attention
(don't forget to type 'i<-nextplot(i)').

Tabbed canvas is totaly different thing - like tabs in modern mozilla/firefox.

I'm not lazy, I'm newbie and trying to adapt R interface to my habit.


	Oleg

On Thu, 1 Apr 2004, Angel Lopez wrote:

> Depends on what you understand by _explicitly_ specify screen number:
>
> par(bg = "white")           # default is likely to be transparent
> screenlist<-split.screen(c(3,1))        # split display into three
> i<-screenlist[1]
>
> # Create a function to simulate the "tabbing"
> nextplot<-function(i)
> {
> i<-i+1
> if (i>screenlist[length(screenlist)]) i<-screenlist[1]
> screen(i)
> erase.screen(i)
> return(i)
> }
>
> plot(1:10)
> i<-nextplot(i)
> plot(2:20)
> i<-nextplot(i)
> plot(3:30)
> i<-nextplot(i)
> plot(sin(40:400))
> i<-nextplot(i)
> plot(sin(4:40))
> ## Continue your plots
> close.screen(all = TRUE)
>
> Oleg Bartunov wrote:
> > interesting, it works, but still need to explicitly specify screen number.
> > It'b be nice to have tabbed canvas, but I suspect it's wrong list.
> >
> > 	Oleg
> > On Thu, 1 Apr 2004, Angel Lopez wrote:
> >
> >
> >>Have you considered using
> >>?split.screen
> >>and
> >>?erase.screen
> >>with a non-transparent background
> >>instead of layout.
> >>Example:
> >>par(bg = "white")           # default is likely to be transparent
> >>split.screen(c(3,1))        # split display into three screens
> >>screen(1)
> >>plot(1:10)
> >>screen(2)
> >>plot(2:20)
> >>screen(3)
> >>plot(3:30)
> >>
> >>screen(1)
> >>erase.screen()
> >>plot(sin(40:400))
> >>
> >>Angel
> >>
> >>Oleg Bartunov wrote:
> >>
> >>
> >>>On Thu, 1 Apr 2004, Duncan Murdoch wrote:
> >>>
> >>>
> >>>
> >>>
> >>>>On Thu, 1 Apr 2004 15:38:54 +0400 (MSD), Oleg Bartunov
> >>>><oleg at sai.msu.su> wrote :
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>>Hello,
> >>>>>
> >>>>>for testing & learning purposes I create X11 device and specify layout like
> >>>>>layout(c(1,2,3), 3, 1), so I could play with parameters and see
> >>>>>several plots at the same time. That works fine until I try to create 4-th
> >>>>>plot - all other plots erased. Such behaviour isn't desirable for testing
> >>>>>purposes and I'm asking  where to look to disable erasing other plots.
> >>>>>
> >>>>>
> >>>>
> >>>>In Windows you can record all plots by using the record=TRUE option
> >>>>when you open the graphics display, or from the menu on the display
> >>>>window.  You cycle back through the history by hitting the PgUp key.
> >>>>
> >>>>You're using X11, so you might need to save your plots explicitly
> >>>>using recordPlot() and then use replayPlot() to see it again (but your
> >>>>front-end might provide access to them more easily, the way Windows
> >>>>does).
> >>>>
> >>>>
> >>>
> >>>Thanks, looks like too much work :( I just don't understand a logic
> >>>why I can't recycle canvas.
> >>>
> >>>
> >>>
> >>>
> >>>>Duncan Murdoch
> >>>>
> >>>>
> >>>>
> >>>
> >>>	Regards,
> >>>		Oleg
> >>>_____________________________________________________________
> >>>Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> >>>Sternberg Astronomical Institute, Moscow University (Russia)
> >>>Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> >>>phone: +007(095)939-16-83, +007(095)939-23-83
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>>.
> >>>
> >>>
> >>>
> >>
> >
> > 	Regards,
> > 		Oleg
> > _____________________________________________________________
> > Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
> > Sternberg Astronomical Institute, Moscow University (Russia)
> > Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
> > phone: +007(095)939-16-83, +007(095)939-23-83
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> > .
> >
>

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83



From brook at biology.nmsu.edu  Thu Apr  1 17:20:53 2004
From: brook at biology.nmsu.edu (brook@biology.nmsu.edu)
Date: Thu, 1 Apr 2004 08:20:53 -0700 (MST)
Subject: [R] general mixed model statement
Message-ID: <200404011520.i31FKrY01956@viola.nmsu.edu>

I am trying to analyze a general mixed model, but am confused about
the model statement to use.  The structure of the problem is the
following:

	y = X b + Z u + e

where X and Z are known incidence matrices for fixed and random
effects respectively, b is a vector of fixed effects to be estimated,
u is a vector of random effects, and e is a vector of error terms.
Additionally, the covariance matrices of the random effects and error
terms are given by

       G = g A, and
       R = r E

where A and E are known matrices and g and r must be estimated.

Presumably there is a way to specify in a model statement the known
covariance structure for the random and error terms, but I cannot find
the documentation I need.  Please either clarify for me how to write
this out or point me to the appropriate documentation that outlines
the full range of model statements that are possible.

Thank you very much for your help.

Cheers,
Brook



From hdoran at NASDC.org  Thu Apr  1 17:27:14 2004
From: hdoran at NASDC.org (Harold Doran)
Date: Thu, 1 Apr 2004 10:27:14 -0500
Subject: [R] general mixed model statement
Message-ID: <66578BFC0BA55348B5907A0F798EE930663600@ernesto.NASDC.ORG>

This would be accomplished via the nlme library for mixed linear models. See Pinhiero and Bates (2000) for all relevant documentation and data applications.

Harold

-----Original Message-----
From: brook at biology.nmsu.edu [mailto:brook at biology.nmsu.edu]
Sent: Thursday, April 01, 2004 10:21 AM
To: r-help at stat.math.ethz.ch
Cc: brook at nmsu.edu
Subject: [R] general mixed model statement


I am trying to analyze a general mixed model, but am confused about
the model statement to use.  The structure of the problem is the
following:

	y = X b + Z u + e

where X and Z are known incidence matrices for fixed and random
effects respectively, b is a vector of fixed effects to be estimated,
u is a vector of random effects, and e is a vector of error terms.
Additionally, the covariance matrices of the random effects and error
terms are given by

       G = g A, and
       R = r E

where A and E are known matrices and g and r must be estimated.

Presumably there is a way to specify in a model statement the known
covariance structure for the random and error terms, but I cannot find
the documentation I need.  Please either clarify for me how to write
this out or point me to the appropriate documentation that outlines
the full range of model statements that are possible.

Thank you very much for your help.

Cheers,
Brook

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at entelnet.bo  Thu Apr  1 17:32:36 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Thu, 01 Apr 2004 11:32:36 -0400
Subject: [R] Zero Index Origin?
In-Reply-To: <406B7EE5.1010605@arcanemethods.com>
References: <x28yhg4jm6.fsf@biostat.ku.dk>
Message-ID: <406BFDD4.10553.6BFF08@localhost>

On 31 Mar 2004 at 18:31, Bob Cain wrote:

> 
> 
> Peter Dalgaard wrote:
> 
> 
> > I think it was said quite early in the thread, but since noone
> > apparently listened, let me reiterate: One of the powerful indexing
> > features in R is the negative index ("all, except") and x[-0] would
> > lead to some complications if 0 was a true index. 

Yes: Look at this: (R1.8.1)

> library(Oarray)

Attaching package 'Oarray':


        The following object(s) are masked from package:base :

         as.array 

> test <- Oarray(1:10, offset=0)
> test
[0,] [1,] [2,] [3,] [4,] [5,] [6,] [7,] [8,] [9,] 
   1    2    3    4    5    6    7    8    9   10 
> str(test)
 int [, 1:10] 2 3 4 5 6 7 8 9 10 NA
 - attr(*, "offset")= num 0
 - attr(*, "drop.negative")= logi TRUE
 - attr(*, "class")= chr "Oarray"
> test[0]
[1] 1
> xx <- 1:10
> test==xx
 [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE
> xx[0]
numeric(0)
> 0==FALSE
[1] TRUE
> test[FALSE]
numeric(0)
> test[0]
[1] 1
> test[-0]
[1] 1


Kjetil Halvorsen


> > 
> 
> Interesting.  The kind of gotcha I hoped wouldn't exist. 
> Others?
> 
> I've thought of another one.  Legacy functions that accept 
> indices as arguments from a 0-origin caller.
> 
> 
> Bob
> -- 
> 
> "Things should be described as simply as possible, but no 
> simpler."
> 
>                                    A. Einstein
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From HankeA at mar.dfo-mpo.gc.ca  Thu Apr  1 16:58:20 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 01 Apr 2004 10:58:20 -0400
Subject: [R] multiple plots problem
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12497D@msgmarsta01.bio.dfo.ca>

Correction:
I should have wrote
layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE))
Sorry
Alex
-----Original Message-----
From: Hanke, Alex [mailto:HankeA at mar.dfo-mpo.gc.ca] 
Sent: April 1, 2004 9:25 AM
To: 'Oleg Bartunov'; 'r-help at stat.math.ethz.ch'
Subject: RE: [R] multiple plots problem


The command:
layout(c(1,2,3), 3, 1) specifies 3 plots
Try
layout(1:4,2,2,byrow=T)

Regards,
Alex
-----Original Message-----
From: Oleg Bartunov [mailto:oleg at sai.msu.su] 
Sent: April 1, 2004 7:39 AM
To: R-help
Subject: [R] multiple plots problem


Hello,

for testing & learning purposes I create X11 device and specify layout like
layout(c(1,2,3), 3, 1), so I could play with parameters and see
several plots at the same time. That works fine until I try to create 4-th
plot - all other plots erased. Such behaviour isn't desirable for testing
purposes and I'm asking  where to look to disable erasing other plots.

	Regards,
		Oleg
_____________________________________________________________
Oleg Bartunov, sci.researcher, hostmaster of AstroNet,
Sternberg Astronomical Institute, Moscow University (Russia)
Internet: oleg at sai.msu.su, http://www.sai.msu.su/~megera/
phone: +007(095)939-16-83, +007(095)939-23-83

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ecashin at uga.edu  Thu Apr  1 17:33:50 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Thu, 01 Apr 2004 10:33:50 -0500
Subject: [R] row selection based on median in data frame
References: <873c7otma9.fsf@uga.edu> <87zn9ws6wp.fsf@uga.edu>
	<87smfos4ss.fsf@uga.edu>
Message-ID: <87isgjsof5.fsf@uga.edu>

Ed L Cashin <ecashin at uga.edu> writes:

...
> Is there a way to tell aggregate just do perform median on column runtime to
> select the whole row?  

Some helpful folks have emailed me requesting more info about what I'm
trying to do.  Here's a simple R function to produce a data frame like
the one I am working on.

demo.frame <- function() {
  n.runs <- 3
  types <- c("red","black","blue")
  foo <- 1:5
  bar <- seq(50,90,by=10)
  d <- data.frame()

  for (i in 1:n.runs) {
    for (t in types) {
      for (f in foo) {
        for (b in bar) {
          row <- data.frame(type=t,
                            foo=f,
                            bar=b,
                            a=rnorm(1),
                            b=rnorm(1),
                            c=rnorm(1))
          d <- rbind(d,row)
        }
      }
    }
  }
  d
}

Every so often, in the resulting rows, you get a row where the type,
the foo, and the bar values are all the same.  I need to look at the
rows with such a matching set of values as a group, selecting the one
row with the median "c" value, and preserving all of that row's other
values.  So median should not be done on the "a" or "b" columns, just
the "c" column.

There are two ways I see to approach this problem.  One would be:

  for each subset of rows with matching type, foo, and bar values, 
    find the row with the median c value and output it

The other, which I've been able to do, takes advantage of knowledge
about the sequence of rows in the data frame:

median.runs <- function(d, n.runs=0) {
  if (missing(n.runs))
    stop("missing n.runs parameter is required")

  len <- length(d$type) / n.runs
  i <- c()

  # build an index that will select similar rows
  for (n in 0:(n.runs - 1)) {
    i[n + 1] <- n * len + 1
  }
  a <- list()
  for (j in 1:len) {
    cat("i:",i,"\n")
    rows <- d[i,]
    md <- median(rows$c)
    cat("md:",md,"\n")
    matches <- rows[rows$c == md,]
    a <- rbind(a, matches[1,])
    i <- i + 1
  }
  a
}


-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From bill.shipley at usherbrooke.ca  Thu Apr  1 17:37:45 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Thu, 1 Apr 2004 10:37:45 -0500
Subject: [R] nls function
Message-ID: <001e01c417ff$474b40f0$801ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040401/f77d49c9/attachment.pl

From tplate at blackmesacapital.com  Thu Apr  1 17:56:05 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 01 Apr 2004 08:56:05 -0700
Subject: [R] Zero Index Origin?
In-Reply-To: <406BAC1C.3090805@arcanemethods.com>
References: <406A7E37.5060504@arcanemethods.com>
	<loom.20040331T150145-904@post.gmane.org>
	<406B3F92.8010402@arcanemethods.com>
	<46646.203.9.176.60.1080771350.squirrel@webmail.maxnet.co.nz>
	<406B48D3.9010502@arcanemethods.com> <x28yhg4jm6.fsf@biostat.ku.dk>
	<406B7EE5.1010605@arcanemethods.com>
	<loom.20040401T050941-391@post.gmane.org>
	<406BAC1C.3090805@arcanemethods.com>
Message-ID: <6.0.3.0.2.20040401081933.0428aeb8@mailhost.blackmesacapital.com>

As far as I can see, the problems with legacy functions kill the 
class-based idea.  As others have noted, if you define a class for which 
the indexing function "[" is zero-based, and then pass an object x of that 
class to a legacy function, anything that function does that assumes 1 is 
the first index and length(x) is the last index and uses "[" will 
fail.  Other problems also arise, e.g., if x is a zero-based array, and y 
is a standard array, then what are the index bases of x+y and y+x?  You 
could make this depend on the order of arguments, but I suspect this would 
lead to horrible programming situations.

Anyway, it seems that what you want to do is change the behavior of the 
indexing operation, not the behavior of the objects themselves.  So, as 
Richard A. O'Keefe suggested, a more feasible approach would be to define a 
new indexing function that was zero-based.  E.g., something like 
zsub().  So, zsub(x, 0, 0) would return the upper left element of the 
matrix x.  This would not be too hard to write (and make it work for arrays 
with any number of dimensions).  If you wanted, you could also define an 
operator to access this function, e.g., so that x^[0,0] returned the upper 
left element of the matrix x, i.e., "^[" does zero-based 
indexing.  However, this is a lot more work and requires modifying the 
source code of R.  As noted by others, this zero-based indexing operator 
would not be able to use negative indices, but you could still use the 
standard indexing operators to get that feature.

I've cursed the one-based indexing in S-derived languages on the rare 
occasions when I've wanted to do index arithmetic (i.e., work out the 
linear locations of array elements based on their indexes) -- it was a pain 
to always have to subtract one from indices before multiplying and then add 
one back in at the end.  However, this really was a minor inconvenience, 
and if I wanted I could have written a function to do my index computations 
for me.  What other more significant computations are so much easier with 
zero-based indexing?

-- Tony Plate

At Wednesday 10:43 PM 3/31/2004, Bob Cain wrote:


>Gabor Grothendieck wrote:
>
>[snip good stuff]
>
>>Of course the above is motherhood and some specific examples
>>might put a sharper edge to the discussion.
>
>I really appreciate your point of view on this and think you
>are probably right.  A question I have from my very limited
>understanding yet of OO is whether such objects could be
>passed to legacy functions with any expectation of correct
>results.
>
>
>Thanks,
>
>Bob
>--
>
>"Things should be described as simply as possible, but no
>simpler."
>
>                                              A. Einstein
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jmuller at mindspring.com  Thu Apr  1 18:05:13 2004
From: jmuller at mindspring.com (John Muller)
Date: Thu, 1 Apr 2004 11:05:13 -0500 (GMT-05:00)
Subject: [R] gray background on jpegs using dev.print()
Message-ID: <31190062.1080835513966.JavaMail.root@wamui10.slb.atl.earthlink.net>

When I try writing the current device to a jpeg file I keep getting a gray background
for the jpeg images using dev.print.

I have tried using the option bg="white" and bg="#FFFFFF"
but neither seem to have any effect.

If instead of writing to a screen device I open a jpeg device I can get any background color
I want.  This works, but I prefer to write to the screen and then 
output to file if I like the results rather than to a file and have to view the jpegs
to see results.

I am using R version 1.8.1 on Windows 2000.

Has anyone else had this problem?
If so please let me know, especially if you found a better workaround or a fix.

- john muller



From andrewr at uidaho.edu  Thu Apr  1 18:12:56 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 1 Apr 2004 08:12:56 -0800
Subject: [R] nls function
In-Reply-To: <001e01c417ff$474b40f0$801ad284@BIO041>
References: <001e01c417ff$474b40f0$801ad284@BIO041>
Message-ID: <200404010812.56801.andrewr@uidaho.edu>

Hi Bill,

I've just spent a few months trying to fit a model to a dataset, and it's not 
easy.  However, in my case, what appears to be recalcitrance on R's part 
actually turns out to be well-founded warnings that the structure of the 
model and the data are not permitting a clean, unambiguous fit.  

I warmly recommend the Bates and Watts book on non-linear regression analysis, 
and also Venables and Ripley MASS, with especial reference to the online 
complements.

My strategy to learn more about my problem was: increase "tol" and decrease 
minFactor until I get a fit, and then examine the intrinsic and parameter 
curvatures using the rms.curv() function in the MASS package.  It showed 
quite clearly that there was a real problem with the parameterization of the 
model.  Of course, using that particular fit was out of the question.

I recommend against trying another tool: instead, learn more about this one 
and why it's behaving in that way.  Good luck. 

Andrew

On Thursday 01 April 2004 07:37, Bill Shipley wrote:
> Hello.  I am trying to fit a non-rectangular hyperbola function to data
> of photosynthetic rate vs. light intensity.  There are 4 parameters that
> have to be estimated.  I find the nls function very difficult to use
> because it often fails to converge and then gives out cryptic error
> messages.  I have tried playing with the control parameters but this
> does not always help.
>
> Is there another non-linear regression function in R that I might try
> (other than regression smoothers, which won?t give the parameter
> estimates of the specified function)?
>
>
>
> Bill Shipley
>
> Subject Matter Editor, Ecology
>
> North American Editor, Annals of Botany
>
> D??partement de biologie, Universit?? de Sherbrooke,
>
> Sherbrooke (Qu??bec) J1K 2R1 CANADA
>
> Bill.Shipley at USherbrooke.ca
>
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From andy_liaw at merck.com  Thu Apr  1 18:14:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 1 Apr 2004 11:14:30 -0500
Subject: [R] gray background on jpegs using dev.print()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AF5@usrymx25.merck.com>

You have not shown us how you call dev.print() or the commands preceding
that.  Did you specify the bg="white" in the call to dev.print()?

Andy

> From: John Muller
> 
> When I try writing the current device to a jpeg file I keep 
> getting a gray background
> for the jpeg images using dev.print.
> 
> I have tried using the option bg="white" and bg="#FFFFFF"
> but neither seem to have any effect.
> 
> If instead of writing to a screen device I open a jpeg device 
> I can get any background color
> I want.  This works, but I prefer to write to the screen and then 
> output to file if I like the results rather than to a file 
> and have to view the jpegs
> to see results.
> 
> I am using R version 1.8.1 on Windows 2000.
> 
> Has anyone else had this problem?
> If so please let me know, especially if you found a better 
> workaround or a fix.
> 
> - john muller
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From mrennie at utm.utoronto.ca  Thu Apr  1 18:14:08 2004
From: mrennie at utm.utoronto.ca (Michael Rennie)
Date: Thu,  1 Apr 2004 11:14:08 -0500
Subject: [R] modelling nested random effects with interactions in R
Message-ID: <1080836048.406c3fd0bfec0@webmail.utm.utoronto.ca>


Hi there

Please excuse this elementary question, but I have been fumbling with this for 
hours and can't seem to get it right.

I have a nested anova, with random factor "lakefac" nested within 
factor "fishfac" (fixed), with an additional fixed factor "Habfac". If I 
consider everything as fixed effects, it's addmittedly not the correct model, 
but I can at least get this to work:

> nested <- anova(lm(ltotinv ~ habfac + fishfac/lakefac + habfac:fishfac + 
habfac:(lakefac %in% fishfac)))
> nested
Analysis of Variance Table

Response: ltotinv
                       Df  Sum Sq Mean Sq F value    Pr(>F)    
habfac                  2 17.3140  8.6570 25.0568 3.534e-08 ***
fishfac                 1  0.9131  0.9131  2.6428   0.11057    
fishfac:lakefac         2  2.3802  1.1901  3.4447   0.04000 *  
habfac:fishfac          2 13.0101  6.5050 18.8281 9.196e-07 ***
habfac:fishfac:lakefac  4  3.0103  0.7526  2.1783   0.08557 .  
Residuals              48 16.5838  0.3455                      
---

So now I try to run it using the lme4 package, treating lakefac as random;

> 
> lakernd <- lme(ltotinv ~ habfac + fishfac/lakefac + habfac:fishfac + habfac:
(lakefac %in% fishfac), random = ~ lakefac)
Error in .class1(object) : Argument "data" is missing, with no default
> lakernd
Error: Object "lakernd" not found

The lme help file suggests that if "data" is not specified, that it defaults to 
whatever object is currently in use in the environment (as was the case in the 
fixed effects model- I am using a matrix called "use" in this example). If I 
(naively) simply try to add data= use to the above formula, this happens:

> lakernd <- lme(ltotinv ~ habfac + fishfac/lakefac + habfac:fishfac + habfac:
(lakefac %in% fishfac), data = use, random = ~ lakefac)
Error in as(data, "data.frame") : No method or default for coercing "matrix" 
to "data.frame"

So how do I get lme to use my data matrix "use" for the model? My guess is that 
my syntax is off, but does anyone have any suggestions on how to fix it? 
Unfortunately, the only resources I have available to me at the moment are 
an "S-plus v. 4. guide to statistics", the archived help files on the R 
website, an the examples and help files in R.  

Your help with this problem is greatly appreciated- I do hope to hear from 
someone as to how I might get this to work.

Sincerely,   

-- 
Michael Rennie
Ph.D. Candidate
University of Toronto at Mississauga
3359 Mississauga Rd. N.
Mississauga ON  L5L 1C6
Ph: 905-828-5452  Fax: 905-828-3792



From jmuller at mindspring.com  Thu Apr  1 18:25:24 2004
From: jmuller at mindspring.com (John Muller)
Date: Thu, 1 Apr 2004 11:25:24 -0500 (GMT-05:00)
Subject: [R] Re: gray background on jpegs using dev.print()
Message-ID: <28010434.1080836727029.JavaMail.root@wamui10.slb.atl.earthlink.net>

I get the gray background when I use plot to write to the screen
and then try to write the screen contents to a file using

     dev.print(jpeg, file="H:/Secondary/cycle_time/test.jpg",width=800,quality=100,bg="white") ;

I have also tried using bg = "#FFFFFF" instead of bg="white"
and have also tried using par to set the background color as in 
    par(bg="#FFFFFF") ;
    dev.print(jpeg, file="H:/Secondary/cycle_time/test.jpg",width=800,quality=100,bg="white") ;

None of these seem to affect the background color of the jpeg image.

- john

-----Original Message-----
From: John Muller <jmuller at mindspring.com>
Sent: Apr 1, 2004 11:05 AM
To: r-help at stat.math.ethz.ch
Subject: gray background on jpegs using dev.print()

When I try writing the current device to a jpeg file I keep getting a gray background
for the jpeg images using dev.print.

I have tried using the option bg="white" and bg="#FFFFFF"
but neither seem to have any effect.

If instead of writing to a screen device I open a jpeg device I can get any background color
I want.  This works, but I prefer to write to the screen and then 
output to file if I like the results rather than to a file and have to view the jpegs
to see results.

I am using R version 1.8.1 on Windows 2000.

Has anyone else had this problem?
If so please let me know, especially if you found a better workaround or a fix.

- john muller



From gosset at vki.ac.be  Thu Apr  1 18:28:51 2004
From: gosset at vki.ac.be (Anne Gosset)
Date: Thu, 1 Apr 2004 18:28:51 +0200
Subject: [R] R fft on 2D matrix
Message-ID: <019901c41806$6ac2acb0$36a3bec1@vkidom.vki.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040401/8df97894/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Apr  1 18:46:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 01 Apr 2004 18:46:55 +0200
Subject: [R] R fft on 2D matrix
In-Reply-To: <019901c41806$6ac2acb0$36a3bec1@vkidom.vki.ac.be>
References: <019901c41806$6ac2acb0$36a3bec1@vkidom.vki.ac.be>
Message-ID: <406C477F.50908@statistik.uni-dortmund.de>

Anne Gosset wrote:

> Hi,
> 
> I am a brand new user of R and I have a really stupid problem: I am having troubles applying fft on 2D matrices.
> I import a data file of 256*256 elements with read.table (it is actually a grey scale image, the corresponding data file being generated with Matlab), and I find that the fft can be applied on one single column of data, but not on one row. 
> (Error message:
> Error in fft(z, inverse) : non-numeric argument
> )

Be careful: read.table() creates a data.frame, but not a matrix.



> Is it the format of the matrix which causes that trouble? 

Well, the trouble is that it is a data.frame.  Try as.matrix(X), if the 
contents was numeric data.


 > (although I checked with the editor that the matrix was well imported)
> Besides this, it is not possible to plot one single row of data (strange plot with the names of variables appearing).

Again, you got a data.frame with 1 row, but 256 variables. Hence the 
plot is a scatterplot matrix, with the one observation plotted.

Uwe Ligges


> Thanks in advance for your help,
> 
> Anne Gosset
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Thu Apr  1 18:47:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 1 Apr 2004 08:47:29 -0800 (PST)
Subject: [R] R fft on 2D matrix
In-Reply-To: <019901c41806$6ac2acb0$36a3bec1@vkidom.vki.ac.be>
References: <019901c41806$6ac2acb0$36a3bec1@vkidom.vki.ac.be>
Message-ID: <Pine.A41.4.58.0404010845580.86044@homer08.u.washington.edu>

On Thu, 1 Apr 2004, Anne Gosset wrote:

> Hi,
>
> I am a brand new user of R and I have a really stupid problem: I am
> having troubles applying fft on 2D matrices. I import a data file of
> 256*256 elements with read.table (it is actually a grey scale image, the
> corresponding data file being generated with Matlab), and I find that
> the fft can be applied on one single column of data, but not on one row.
> (Error message: Error in fft(z, inverse) : non-numeric argument )
>
> Is it the format of the matrix which causes that trouble? (although I
> checked with the editor that the matrix was well imported) Besides this,
> it is not possible to plot one single row of data (strange plot with the
> names of variables appearing).
>

You probably have a data.frame rather than a matrix, as that is what
read.table() produces.  The difference is not obvious to the untrained
eye, but a data.frame is a list of columns of potentially different types,
so a single column reduces to a vector but a single row does not.

Use as.matrix() on your data frame to convert it to a matrix.

	-thomas



From dmurdoch at pair.com  Thu Apr  1 18:48:03 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 01 Apr 2004 11:48:03 -0500
Subject: [R] multiple plots problem
In-Reply-To: <Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>
References: <Pine.GSO.4.58.0404011530360.11543@ra.sai.msu.su>
	<s66o60hn658vf85kouofdfp68rkren8tlg@4ax.com>
	<Pine.GSO.4.58.0404011744060.11543@ra.sai.msu.su>
Message-ID: <pjho609vi030dl2ikhjbkpadgo2nbg4hnb@4ax.com>

On Thu, 1 Apr 2004 17:48:11 +0400 (MSD), Oleg Bartunov
<oleg at sai.msu.su> wrote :

>On Thu, 1 Apr 2004, Duncan Murdoch wrote:

>> In Windows you can record all plots by using the record=TRUE option
>> when you open the graphics display, or from the menu on the display
>> window.  You cycle back through the history by hitting the PgUp key.
>>
>> You're using X11, so you might need to save your plots explicitly
>> using recordPlot() and then use replayPlot() to see it again (but your
>> front-end might provide access to them more easily, the way Windows
>> does).
>
>Thanks, looks like too much work :( I just don't understand a logic
>why I can't recycle canvas.

PgUp isn't a lot of work.  If you choose to use an OS where most users
prefer the command line, then you should expect to do more typing.
(But I bet some other front-ends support something like PgUp, I just
don't know which, because I don't use them.)

Duncan Murdoch



From ripley at stats.ox.ac.uk  Thu Apr  1 18:54:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Apr 2004 17:54:05 +0100 (BST)
Subject: [R] gray background on jpegs using dev.print()
In-Reply-To: <31190062.1080835513966.JavaMail.root@wamui10.slb.atl.earthlink.net>
Message-ID: <Pine.LNX.4.44.0404011744130.1966-100000@gannet.stats>

On Thu, 1 Apr 2004, John Muller wrote:

> When I try writing the current device to a jpeg file I keep getting a
> gray background for the jpeg images using dev.print.

That's because you are not using the correct tool.  dev.print is designed 
for printing, as it says.

> I have tried using the option bg="white" and bg="#FFFFFF"
> but neither seem to have any effect.

Where?  Is what you are copying on a transparent background (the default)?

> If instead of writing to a screen device I open a jpeg device I can get any background color
> I want.  This works, but I prefer to write to the screen and then 
> output to file if I like the results rather than to a file and have to view the jpegs
> to see results.
> 
> I am using R version 1.8.1 on Windows 2000.
> 
> Has anyone else had this problem?

What problem, exactly ...?  Please do read the posting guide and supply 
enough details for us to reproduce this.

And why if you are using Windows *are* you using dev.print when you are 
not printing and there are menus and savePlot to copy a plot to a jpeg 
file?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr  1 19:18:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 1 Apr 2004 18:18:33 +0100 (BST)
Subject: [R] gray background on jpegs using dev.print()
In-Reply-To: <Pine.LNX.4.44.0404011744130.1966-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404011810350.1966-100000@gannet.stats>

> par(bg="white")
> plot(1:10)
> dev.print(jpeg, width=600, height=600)

gives a white background on the jpeg copy.  Without the first line it is a
transparent background, and the default matte for JPEGS is grey74, so the
viewer displays it as transparent over a grey matte.


BTW, John Muller has given an address that does not accept email without
jumping through loops replying to HTML mail.  Can we please add to the
Posting Guide the requirement that a usable return address is supplied?


On Thu, 1 Apr 2004, Prof Brian Ripley wrote:

> On Thu, 1 Apr 2004, John Muller wrote:
> 
> > When I try writing the current device to a jpeg file I keep getting a
> > gray background for the jpeg images using dev.print.
> 
> That's because you are not using the correct tool.  dev.print is designed 
> for printing, as it says.
> 
> > I have tried using the option bg="white" and bg="#FFFFFF"
> > but neither seem to have any effect.
> 
> Where?  Is what you are copying on a transparent background (the default)?
> 
> > If instead of writing to a screen device I open a jpeg device I can get any background color
> > I want.  This works, but I prefer to write to the screen and then 
> > output to file if I like the results rather than to a file and have to view the jpegs
> > to see results.
> > 
> > I am using R version 1.8.1 on Windows 2000.
> > 
> > Has anyone else had this problem?
> 
> What problem, exactly ...?  Please do read the posting guide and supply 
> enough details for us to reproduce this.
> 
> And why if you are using Windows *are* you using dev.print when you are 
> not printing and there are menus and savePlot to copy a plot to a jpeg 
> file?
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nedluk at yahoo.it  Thu Apr  1 20:28:31 2004
From: nedluk at yahoo.it (=?iso-8859-1?q?michele=20lux?=)
Date: Thu, 1 Apr 2004 20:28:31 +0200 (CEST)
Subject: [R] arimax...
Message-ID: <20040401182831.57522.qmail@web13604.mail.yahoo.com>

Hallo all
can someone explain me how the exogenus variables work
in the arimax models is not clear for me...
Thanks Michele



From mok2 at physics.buffalo.edu  Thu Apr  1 20:40:39 2004
From: mok2 at physics.buffalo.edu (Mark O. Kimball)
Date: Thu, 1 Apr 2004 13:40:39 -0500
Subject: [R] Generating file names
Message-ID: <200404011340.39958.mok2@physics.buffalo.edu>

I read a data set into a data.frame. I then operate on the set, add
columns to the frame etc. and now want to output the frame into a file.
Trouble is, I want the file to have a name based upon the original data
set name. 

I cannot figure out how to cat strings together and use that new string
as the output filename...

Example: 	
	original data name: D08a
	desired output name: Analyzed_D08a

Any ideas?

Marko
-- 
Mark O. Kimball
Gasparinilab, University at Buffalo  |  Low temp physics
mok2 at physics.buffalo.edu  |  URL: enthalpy.physics.buffalo.edu
lab phone: 716-645-2017x122  Fax: 716-645-2507



From tlumley at u.washington.edu  Thu Apr  1 20:51:52 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 1 Apr 2004 10:51:52 -0800 (PST)
Subject: [R] Generating file names
In-Reply-To: <200404011340.39958.mok2@physics.buffalo.edu>
References: <200404011340.39958.mok2@physics.buffalo.edu>
Message-ID: <Pine.A41.4.58.0404011050210.86044@homer08.u.washington.edu>

On Thu, 1 Apr 2004, Mark O. Kimball wrote:

> I read a data set into a data.frame. I then operate on the set, add
> columns to the frame etc. and now want to output the frame into a file.
> Trouble is, I want the file to have a name based upon the original data
> set name.
>
> I cannot figure out how to cat strings together and use that new string
> as the output filename...
>
> Example:
> 	original data name: D08a
> 	desired output name: Analyzed_D08a


paste("Analysed",oldname,sep="_")

This is mentioned on the help page for cat(), precisely to help
Unix-influenced people who think that `cat' is a natural abbreviation for
`concatenate'

	-thomas



From ggrothendieck at myway.com  Thu Apr  1 21:17:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 1 Apr 2004 19:17:19 +0000 (UTC)
Subject: [R] Zero Index Origin?
References: <200404010243.i312h4jA515578@atlas.otago.ac.nz>
	<406BCBB6.8020700@wiwi.uni-bielefeld.de>
Message-ID: <loom.20040401T211605-285@post.gmane.org>


Peter Wolf wrote:

sort.6<-function(a){
   n<-length(a)
   adapt<-function(i){i+1}  # local function to perform the index correction
   a<-c(0,a)
   for(i in 2:n){
      j<-i-1
      a[adapt(0)]<-a[adapt(i)]
      while(a[adapt(j)]>a[adapt(0)]){
         a[adapt(j+1)]<-a[adapt(j)]
         j<-j-1
      }
      a[adapt(j+1)]<-a[adapt(0)]
   }
   return(a[-1])
}

and Prof Brian Ripley wrote:

: since you can shift whole blocks at a time rather than use a while loop.

In words the algorithm runs from 2 through length(a) inserting
the current element into the subarray to its left, so to follow up
Prof Riley's suggestion to replace the while loop with code that
copies whole blocks at a time we have the following which does
not seem to suffer from lack of 0-origin and is clearer than
the double loop approach:

sort.6a <- function(a) {
	for(i in 2:length(a)){ # insert a[i] into subvector to left of it
		left <- a[1:(i-1)]
		sel <- left < a[i]
		a[1:i] <- c( left[sel], a[i], left[!sel] )
	}
	a
}



From mok2 at physics.buffalo.edu  Thu Apr  1 21:18:29 2004
From: mok2 at physics.buffalo.edu (Mark O. Kimball)
Date: Thu, 1 Apr 2004 14:18:29 -0500
Subject: [R] Generating file names
In-Reply-To: <200404011340.39958.mok2@physics.buffalo.edu>
References: <200404011340.39958.mok2@physics.buffalo.edu>
Message-ID: <200404011418.29507.mok2@physics.buffalo.edu>

On Thursday 01 April 2004 01:40 pm, Mark O. Kimball wrote:
> I read a data set into a data.frame. I then operate on the set, add
> columns to the frame etc. and now want to output the frame into a file.
> Trouble is, I want the file to have a name based upon the original data
> set name.
>
> I cannot figure out how to cat strings together and use that new string
> as the output filename...
>
> Example:
> 	original data name: D08a
> 	desired output name: Analyzed_D08a
>

Thanks to all who replied. Indeed, the paste() command what exactly what
I needed...

Marko
-- 
Mark O. Kimball
Gasparinilab, University at Buffalo  |  Low temp physics
mok2 at physics.buffalo.edu  |  URL: enthalpy.physics.buffalo.edu
lab phone: 716-645-2017x122  Fax: 716-645-2507



From jmuller at mindspring.com  Thu Apr  1 21:36:36 2004
From: jmuller at mindspring.com (John Muller)
Date: Thu, 1 Apr 2004 14:36:36 -0500 (GMT-05:00)
Subject: [R] gray background on jpegs using dev.print()
Message-ID: <7612244.1080848196504.JavaMail.root@wamui07.slb.atl.earthlink.net>

Sorry about the email issue. I have a spam filter that only let's things on 
my "pass" list into my Inbox, all else gets put into a "suspect" folder.
I have added the R help list to my pass list.

Prof. Ripley asked
>> why if you are using Windows *are* you using dev.print 
I actually started by using the function HTMLplot
from the the R2HTML package. Was getting this gray background problem
so I looked at the code for that and saw that it was calling dev.print
... so that's why I started using it.  Did not know there was a savePlot function.
Will use that now.


Not sure if that is still relevant, but here is a short sequence of commands
that reproduces the issue for me

> palette(rainbow(4)) ;
> plot(1:10) ;
> dev.print(jpeg,file="c:/test1.jpeg",width=500,height=500,bg="white") ;
> savePlot(file="C:/test2",type=c("jpeg"));

For me, this products 2 jpeg files, 
  test1.jpeg   which has a gray background
and 
  test2.jpeg  which has a white background

Seems that the palette is related to the gray background with dev.print.
Not sure if it is possible to reset the palette after the plot to get around
the issue, and not really important to me now since I know about the savePlot 
function.   Just thought some of you might want to know.

Thanks again for all the helpful and encouraging responses, especially Dr. Ripley.

- john muller



-----Original Message-----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Sent: Apr 1, 2004 11:54 AM
To: John Muller <jmuller at mindspring.com>
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] gray background on jpegs using dev.print()

On Thu, 1 Apr 2004, John Muller wrote:

> When I try writing the current device to a jpeg file I keep getting a
> gray background for the jpeg images using dev.print.

That's because you are not using the correct tool.  dev.print is designed 
for printing, as it says.

> I have tried using the option bg="white" and bg="#FFFFFF"
> but neither seem to have any effect.

Where?  Is what you are copying on a transparent background (the default)?

> If instead of writing to a screen device I open a jpeg device I can get any background color
> I want.  This works, but I prefer to write to the screen and then 
> output to file if I like the results rather than to a file and have to view the jpegs
> to see results.
> 
> I am using R version 1.8.1 on Windows 2000.
> 
> Has anyone else had this problem?

What problem, exactly ...?  Please do read the posting guide and supply 
enough details for us to reproduce this.

And why if you are using Windows *are* you using dev.print when you are 
not printing and there are menus and savePlot to copy a plot to a jpeg 
file?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xt_wang at cs.concordia.ca  Fri Apr  2 00:12:03 2004
From: xt_wang at cs.concordia.ca (xt_wang@cs.concordia.ca)
Date: Thu,  1 Apr 2004 17:12:03 -0500
Subject: [R] Use R function in C code
Message-ID: <1080857523.406c93b3c761a@mailhost.cs.concordia.ca>

I want to use R function Matrix inverse in my c code, please tell me how I can.
If there is a sample which can tell me how it works. It will be fantastic.



From JMORRIS6 at OCDUS.JNJ.COM  Fri Apr  2 00:19:55 2004
From: JMORRIS6 at OCDUS.JNJ.COM (Morris, Jeffrey [OCDUS])
Date: Thu, 1 Apr 2004 17:19:55 -0500 
Subject: [R] boot question
Message-ID: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040401/126bb0ba/attachment.pl

From Paul.Sorenson at vision-bio.com  Fri Apr  2 00:30:40 2004
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Fri, 2 Apr 2004 08:30:40 +1000
Subject: [R] Can R be useful to me?
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6273A5D@ewok.vsl.com.au>

On Wed, Mar 31, 2004 at 08:32:49AM -0300, Luiz Rodrigo Tozzi wrote:
> 
> My question is: can I generate graphics and tables in gif ou any graphical 
> format through shell script?? can I call R, run a package in my ascii data e 
> then export the results to a gif, png or whatever?

On Wed, Mar 31, 2004 at 08:32:49AM -0300, Luiz Rodrigo Tozzi wrote:
> 
> My question is: can I generate graphics and tables in gif ou any graphical 
> format through shell script?? can I call R, run a package in my ascii data e 
> then export the results to a gif, png or whatever?

I do this on my windows machine to generate project metrics 
automatically every morning.  For my situation, the following works well:

    o Gather CSV files from various sources (I use python).

    o Run R to generate PNG (or whatever) format images.  I am aware
of packages to link python and R but I just open an R process and
stream commands to it.

    o Copy the CSV files and images to network directory.  This
network directory is also accessible to the intranet via apache.  I
wrote a simple CGI script in python to present all the diagrams back
to the browser.



From Achim.Zeileis at wu-wien.ac.at  Fri Apr  2 00:34:24 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 2 Apr 2004 00:34:24 +0200
Subject: [R] boot question
In-Reply-To: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
References: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
Message-ID: <20040402003424.395c5d23.Achim.Zeileis@wu-wien.ac.at>

On Thu, 1 Apr 2004 17:19:55 -0500  Morris, Jeffrey [OCDUS] wrote:

> What in the world am I missing??

The help page?

As help(boot) tells you:

statistic: <snip>
          In all other cases 'statistic'
          must take at least two arguments.  The first argument passed
          will always be the original data. The second will be a vector
          of indices,
           <snip>

So I would think you want to do:

R> mymean <- function(x, index) mean(x[index])
R> boot(x, mymean, R = 20)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = x, statistic = mymean, R = 20)


Bootstrap Statistics :
    original     bias    std. error
t1*   0.1775 0.03224225   0.1302358

hth,
Z

> > x<-rnorm(20)
> 
> > mean(x)
> [1] -0.2272851
> 
> > results<-boot(x,mean,R=5)
> 
> > results[2]
> $t
>            [,1]
> [1,] -0.2294562
> [2,] -0.2294562
> [3,] -0.2294562
> [4,] -0.2294562
> [5,] -0.2294562
> 
> Jeff Morris
> Ortho-Clinical Diagnostics
> A Johnson & Johnson Co.
> Rochester, NY
> Tel: (585) 453-5794
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From andrewr at uidaho.edu  Fri Apr  2 00:39:55 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 1 Apr 2004 14:39:55 -0800
Subject: [R] boot question
In-Reply-To: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
References: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
Message-ID: <200404011439.55468.andrewr@uidaho.edu>

You have to let boot pass the index for selection to the function as well.  
So, for example, try:

boot.mean <- function(data, index)
	mean(data[index])

results <- boot(x, boot.mean, R=5)


Andrew.

On Thursday 01 April 2004 14:19, Morris, Jeffrey [OCDUS] wrote:
> What in the world am I missing??
>
> > x<-rnorm(20)
> >
> > mean(x)
>
> [1] -0.2272851
>
> > results<-boot(x,mean,R=5)
> >
> > results[2]
>
> $t
>            [,1]
> [1,] -0.2294562
> [2,] -0.2294562
> [3,] -0.2294562
> [4,] -0.2294562
> [5,] -0.2294562
>
> Jeff Morris
> Ortho-Clinical Diagnostics
> A Johnson & Johnson Co.
> Rochester, NY
> Tel: (585) 453-5794
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From canty at math.mcmaster.ca  Fri Apr  2 00:35:17 2004
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 1 Apr 2004 17:35:17 -0500 (EST)
Subject: [R] boot question
In-Reply-To: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
Message-ID: <Pine.SOL.4.44.0404011733310.16791-100000@icarus.math.mcmaster.ca>

What you are missing is reading the helpfile!

The function mean is not a valid statistic to be passed to boot
function(x, i) mean(x[i])
is.

Please do read the documentation.  That's why it is provided.

------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |
------------------------------------------------------------------

On Thu, 1 Apr 2004, Morris, Jeffrey [OCDUS] wrote:

> What in the world am I missing??
>
> > x<-rnorm(20)
>
> > mean(x)
> [1] -0.2272851
>
> > results<-boot(x,mean,R=5)
>
> > results[2]
> $t
>            [,1]
> [1,] -0.2294562
> [2,] -0.2294562
> [3,] -0.2294562
> [4,] -0.2294562
> [5,] -0.2294562
>
> Jeff Morris
> Ortho-Clinical Diagnostics
> A Johnson & Johnson Co.
> Rochester, NY
> Tel: (585) 453-5794
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jasont at indigoindustrial.co.nz  Fri Apr  2 00:37:33 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 2 Apr 2004 10:37:33 +1200 (NZST)
Subject: [R] boot question
In-Reply-To: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
References: <4034A7B865038E40AF358DFF510FAB639BDF20@jcdusroexs2.na.jnj.com>
Message-ID: <48962.203.9.176.60.1080859053.squirrel@webmail.maxnet.co.nz>

> What in the world am I missing??

stype.  As in (from help(boot)

   stype: A character string indicating what the second argument of
          statistic represents. Possible values of stype are '"i"'
          (indices - the default), '"f"' (frequencies), or '"w"'
          (weights).

This works.  n.b. the second argument.

mean.b <- function(x,ind,...) {
  mean(x[ind])
}

zz <- rnorm(20)
mean(zz)
res <- boot(zz,mean.b,R=5)

I'm sure there's a cleaner way to do that, but I don't know it (yet).

Cheers

Jason



From ok at cs.otago.ac.nz  Fri Apr  2 02:13:04 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 2 Apr 2004 12:13:04 +1200 (NZST)
Subject: [R] Zero Index Origin?
Message-ID: <200404020013.i320D4Sg009486@atlas.otago.ac.nz>

I asked where index origin 0 would help.
(I am familiar with Dijkstra's article on why counting should start
at 0 and agree whole-heartedly.  I am also convinced that "kicking
against the goads" is not helpful.)

Peter Wolf <s-plus at wiwi.uni-bielefeld.de>
has provided two examples.

(1) Converting pseudo-code which assumes index origin 0.

    I've done this myself, although not yet in R.  I've also done it
    the other way, converting index origin 1 code to C.

    A good method, I've found, is to to start by converting to
    index-free form.  This applies whatever the source and target
    languages are.

(2) A sorting algorithm.

    sort.6 <- function (a) {
       n <- length(a)
       adapt <- function (i) {i+1}
       a <- c(0,a)
       for (i in 2:n) {
	  j <- i-1
	  a[adapt(0)] <- a[adapt(i)]
	  while (a[adapt(j)] > a[adapt(0)]) {
	     a[adapt(j+1)] <- a[adapt(j)]
	     j <- j-1
	  }
	  a[adapt(j+1)] <- a[adapt(0)]
       }
       a[-1]
    }

    The really interesting thing here is that this basically is an
    index origin 1 algorithm.  The original array and the final result
    start at 1, not 0, and position 0 is used for a "sentinel".

    Let's convert it to 1-origin.  I'll skip the details of how I
    did it because that's not the point I want to make.

    sort.VI <- function (a) {
       a <- c(0, a)
       for (i in 3:length(a)) {
	  j <- i-1
	  a[1] <- a[i]
	  while (a[j] > a[1]) {
	     a[j+1] <- a[j]
	     j <- j-1
	  }
	  a[j+1] <- a[1]
       }
       a[-1]
    }

    What do you get if you move up to index-free form?

    sort.six <- function (a) {
	s <- c()
	for (x in a) {
	    f <- s <= x
	    s <- c(s[f], x, s[!f])    # insert x stably into s
	}
	s
    }

    It's clear that sort.six is shorter, clearer, and easier to get
    right than sort.VI.  But how much do we have to pay for this?
    How much efficiency do we lose?

    > a <- runif(400)
    > system.time(sort.VI(a))
    [1]  3.64  0.02 12.56  0.00  0.00
    > system.time(sort.six(a))
    [1] 0.15 0.01 0.16 0.00 0.00

    We don't lose any efficiency at all.  We gain, considerably.
    (Not as much as we'd gain by using the built-in sort(), of course.)

I expect that this will happen fairly often:  rewriting the code to be
index-free will *usually* make it shorter and clearer, and will *always*
make it easier to adapt to a language with a different index origin.
When the target language is R or S, the result is likely to be faster
than a direct conversion.



From Nick.Ellis at csiro.au  Fri Apr  2 02:57:11 2004
From: Nick.Ellis at csiro.au (Nick.Ellis@csiro.au)
Date: Fri, 2 Apr 2004 10:57:11 +1000
Subject: [R] row selection based on median in data frame
Message-ID: <C8DFDC5896F19C49BC1DD5F2E0F56B380C8471@exqld2-bne.qld.csiro.au>

> tmp
  row.labels        a b  c 
1          1 deadlift 7 13
2          2    squat 7 24
3          3    clean 7 10
4          4 deadlift 8  8
5          5    squat 8 20
6          6    clean 8  2
7          7 deadlift 9  5
8          8    squat 9 32
9          9    clean 9 19
> tapply(tmp$c,tmp$a,median)
 clean deadlift squat 
    10        8    24
> tmp[tapply(1:nrow(tmp),tmp$a,function(i,x) {x <- x[i]; i[x==median(x)]}, x=tmp$c),]
  row.labels        a b  c 
3          3    clean 7 10
4          4 deadlift 8  8
2          2    squat 7 24

If you have multiple grouping variables g1,g2,g3 you simply include those in the 2nd argument:

tmp[tapply(1:nrow(tmp),tmp[c("gp1","gp2","gp3")],function(i,x) {x <- x[i]; i[x==median(x)]}, x=tmp$c),]

Nick Ellis
CSIRO Marine Research	mailto:Nick.Ellis at csiro.au
PO Box 120			ph    +61 (07) 3826 7260
Cleveland QLD 4163    	fax   +61 (07) 3826 7222
Australia			http://www.marine.csiro.au
  
> 
> 
> ------------------------------
> 
> Message: 75
> Date: Wed, 31 Mar 2004 22:22:22 -0500
> From: Ed L Cashin <ecashin at uga.edu>
> Subject: [R] row selection based on median in data frame
> To: r-help at stat.math.ethz.ch
> Message-ID: <873c7otma9.fsf at uga.edu>
> Content-Type: text/plain; charset=us-ascii
> 
> Hi.  I am having trouble thinking of an easy way to grab rows out of a
> data frame.  I want to select the rows with a median value when the
> rows are similar.
> 
> A simple example is this table, which I could read into a data frame.
> I would like to find a new data frame with only the rows with a median
> value for the "c" column given a certain "a" value.
> 
> For example, the c values for deadlift rows are 13, 8, and 5, so the
> row with a c value of 8 should show up in the output.
> 
>         a          b          c
>      1	deadlift   7          13 
>      2	squat      7          24
>      3	clean      7          10
>      4	deadlift   8           8
>      5	squat      8          20
>      6	clean      8           2
>      7  deadlift   9           5
>      8  squat      9          32
>      9  clean      9          19
> 
> Result:
> 
>         a          b          c
>      4	deadlift   8           8
>      5	squat      8          20
>      3	clean      7          10
> 
> It's more complicated in my case, because I have not just one "a"
> column, but about eight columns that have to be the same.  I can do
> this with clumsy loops, but I wonder whether there's a better way.
> 
> -- 
> --Ed L Cashin            |   PGP public key:
>   ecashin at uga.edu        |   http://noserose.net/e/pgp/
>



From feldesmanm at pdx.edu  Fri Apr  2 06:40:22 2004
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Thu, 01 Apr 2004 20:40:22 -0800
Subject: [R] Plot symbols for more than 25 groups
Message-ID: <6.0.3.0.2.20040401203734.02104ec0@pop4.attglobal.net>

Is there any effective way to get distinct geometric plotting symbols and 
colors for plots involving more than 25 groups?

Thanks.




Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesman at attglobal.net
fax:    503-725-3905


"Don't knock on my door if you don't know my Rottweiler's name"  Warren Zevon
"Its midnight and I'm not famous yet"  Jimmy Buffett



From Lorenz.Gygax at fat.admin.ch  Fri Apr  2 07:40:50 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Fri, 2 Apr 2004 07:40:50 +0200 
Subject: [R] modelling nested random effects with interactions in R
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DAE2B@evd-s7014.evd.admin.ch>


Hi Michael,

> I have a nested anova, with random factor "lakefac" nested within 
> factor "fishfac" (fixed), with an additional fixed factor 
> "Habfac". If I consider everything as fixed effects, it's addmittedly
> not the correct model, but I can at least get this to work:
> ...
> So now I try to run it using the lme4 package, treating 
> lakefac as random;

I am not quite sure about lme4 but in limrary ('nlme') you would need to do
something like:

> lakernd <- lme(ltotinv ~ habfac * fishfac,
                 random = ~ 1 | fishfac/lakefac,
                 data= use)

with this model you have the fixed effect of habfac and fishfac and their
interaction, lme should get the df's of the model ok if you specify it that
way. In this way, the random term is only for the intercept (which to my
understanding is the same as saying there is no interaction between random
and fixed effects). If you want to include the interactions you need to do
something like this:

> lakernd.int  <- lme(ltotinv ~ habfac * fishfac,
                      random = ~ habfac * fishfac | fishfac/lakefac,
                      data= use)

Thus, you specify random effects for the habfac, fishfac and their
interactions, which is the same as saying that there are interactions
between random and fixed effects.

Possibly you might need to do something like the following (because fishfac
is in the fixed and the random term, thus you specify interactions
seperately for both levels:

> lakernd.int  <- lme(ltotinv ~ habfac * fishfac,
                      random = list (~ 1 | fishfac,
                                     ~ habfac * fishfac | lakefac)
                      data= use)

To test whether interactions are significant you need to compare models.
Thus the following would test whether all these included interactions
togethter lead to a statistically better model:

anova (lakernd, lakernd.int)

I hope this helps to get you on the right track.

Regards, Lorenz
- 
Lorenz Gygax
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From hodgess at gator.uhd.edu  Fri Apr  2 08:12:12 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Fri, 2 Apr 2004 00:12:12 -0600
Subject: [R] off topic state data
Message-ID: <200404020612.i326CCb05985@gator.dt.uh.edu>

Dear R People:

This is off topic, but this group is amazing:
There is a state object, with all kinds of variables about
US states.  However, the numeric data is from 1977.

Does anyone know of a similar data set with more recent data, please?

Thanks in advance,
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From asemeria at cramont.it  Fri Apr  2 09:50:33 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Fri, 2 Apr 2004 09:50:33 +0200
Subject: [R] Plot symbols for more than 25 groups
Message-ID: <OF92634BC9.179FEF7E-ONC1256E6A.002B1419@tomware.it>





The answer is yes.
Have a look to "pch" and "col" plot parameters on
R-help.

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From spyridoula.tsonaka at med.kuleuven.ac.be  Fri Apr  2 10:16:19 2004
From: spyridoula.tsonaka at med.kuleuven.ac.be (Spyridoula Tsonaka)
Date: Fri, 2 Apr 2004 10:16:19 +0200
Subject: [R] Hessian in constrOptim
Message-ID: <00bb01c4188a$c6e50db0$ae133a86@www.domain>

Dear R-users,

In the function constrOptim there is an option to get an approximation
to the hessian of the surrogate function R at MLE by declaring
hessian=TRUE in the calls to the function optim. I would like to ask
if it is advisable to get an approximate hessian for the funcrion f as
follows:

f''(theta)=R''(theta|theta_k)-B''(theta)

where B''(theta)=mu*sum((g(theta_k)/g(theta)^2)u_i*u_i^T) denotes the
second derivative of the barrier function

(following the notation given by Lange (1999)) where
R''(theta|theta_k) will be replaced by the approximate hessian at MLE
returned by optim.

Thanks in advance,


Spyridoula Tsonaka
Doctoral Student
Biostatistical Centre
Katholieke Universiteit Leuven
Kapucijnenvoer 35
B-3000, Leuven
Belgium

Tel: +32/16/336887
Fax: +32/16/337015



From s-plus at wiwi.uni-bielefeld.de  Fri Apr  2 10:44:30 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 02 Apr 2004 10:44:30 +0200
Subject: [R] Zero Index Origin?
References: <200404020013.i320D4Sg009486@atlas.otago.ac.nz>
Message-ID: <406D27EE.5040200@wiwi.uni-bielefeld.de>

Sorting is a wonderful topic! Especially because you can discuss different
fundamental ideas like brute force, divide and conquer, and questions of
efficiency, tradeoffs of space and time, etc. In this way  sort.6  is 
one of a
sequence of sorting algorithms. Each of them demonstrates a specific point
for teaching purposes. You are right: in sort.6 a[0] plays the role of a 
sentinel.
So the main points are:

  1) What is the new idea found in sort.6 (compared to sort.5 -- a 
solution without sentinel)?
  2) Are there other ideas for improvements concerning space and time 
(this leads to sort.7, etc.)?
  3) What about readability: what is the best notation for human readers?
  4) What should be considered and changed in the light of a specific 
computer language?

Your remarks show that often an example initiates a desire  to start  a 
discussion
of one of the four questions. That's what we want to have in the 
classroom, too.
In case of sort.6 my situation was that I have a lot of nice slides with 
algorithms
in pseudo code and I wanted to get them running for demonstration. For APL
is a little bit out of fashion I used my favorite computer language: R.

Thank you for your comments -- which increase the set of R programming 
pearls
and will enrich the discussion in my lectures

Peter Wolf


Richard A. O'Keefe wrote:

>I asked where index origin 0 would help.
>(I am familiar with Dijkstra's article on why counting should start
>at 0 and agree whole-heartedly.  I am also convinced that "kicking
>against the goads" is not helpful.)
>
>Peter Wolf <s-plus at wiwi.uni-bielefeld.de>
>has provided two examples.
>
>(1) Converting pseudo-code which assumes index origin 0.
>
>    I've done this myself, although not yet in R.  I've also done it
>    the other way, converting index origin 1 code to C.
>
>    A good method, I've found, is to to start by converting to
>    index-free form.  This applies whatever the source and target
>    languages are.
>
>(2) A sorting algorithm.
>
>    sort.6 <- function (a) {
>       n <- length(a)
>       adapt <- function (i) {i+1}
>       a <- c(0,a)
>       for (i in 2:n) {
>	  j <- i-1
>	  a[adapt(0)] <- a[adapt(i)]
>	  while (a[adapt(j)] > a[adapt(0)]) {
>	     a[adapt(j+1)] <- a[adapt(j)]
>	     j <- j-1
>	  }
>	  a[adapt(j+1)] <- a[adapt(0)]
>       }
>       a[-1]
>    }
>
>    The really interesting thing here is that this basically is an
>    index origin 1 algorithm.  The original array and the final result
>    start at 1, not 0, and position 0 is used for a "sentinel".
>
>    Let's convert it to 1-origin.  I'll skip the details of how I
>    did it because that's not the point I want to make.
>
>    sort.VI <- function (a) {
>       a <- c(0, a)
>       for (i in 3:length(a)) {
>	  j <- i-1
>	  a[1] <- a[i]
>	  while (a[j] > a[1]) {
>	     a[j+1] <- a[j]
>	     j <- j-1
>	  }
>	  a[j+1] <- a[1]
>       }
>       a[-1]
>    }
>
>    What do you get if you move up to index-free form?
>
>    sort.six <- function (a) {
>	s <- c()
>	for (x in a) {
>	    f <- s <= x
>	    s <- c(s[f], x, s[!f])    # insert x stably into s
>	}
>	s
>    }
>
>    It's clear that sort.six is shorter, clearer, and easier to get
>    right than sort.VI.  But how much do we have to pay for this?
>    How much efficiency do we lose?
>
>    > a <- runif(400)
>    > system.time(sort.VI(a))
>    [1]  3.64  0.02 12.56  0.00  0.00
>    > system.time(sort.six(a))
>    [1] 0.15 0.01 0.16 0.00 0.00
>
>    We don't lose any efficiency at all.  We gain, considerably.
>    (Not as much as we'd gain by using the built-in sort(), of course.)
>
>I expect that this will happen fairly often:  rewriting the code to be
>index-free will *usually* make it shorter and clearer, and will *always*
>make it easier to adapt to a language with a different index origin.
>When the target language is R or S, the result is likely to be faster
>than a direct conversion.
>  
>
---------------------------------------------------------

Gabor Grothendieck wrote:

......

and Prof Brian Ripley wrote:

: since you can shift whole blocks at a time rather than use a while loop.

In words the algorithm runs from 2 through length(a) inserting
the current element into the subarray to its left, so to follow up
Prof Riley's suggestion to replace the while loop with code that
copies whole blocks at a time we have the following which does
not seem to suffer from lack of 0-origin and is clearer than
the double loop approach:

sort.6a <- function(a) {
	for(i in 2:length(a)){ # insert a[i] into subvector to left of it
		left <- a[1:(i-1)]
		sel <- left < a[i]
		a[1:i] <- c( left[sel], a[i], left[!sel] )
	}
	a
}

-----------------------------------------

Prof Brian Ripley wrote:

I think that is an excellent illustration of my point:


>If you are writing code that works with single elements, you are
>probably a lot better off writing C code to link into R (and C is
>0-based ...).
>  
>

but even in R it is not following


>However, the R thinking is to work with whole objects (vectors, arrays,
>lists ...) and you rather rarely need to know what numbers are in an 
>index vector. 
>  
>

since you can shift whole blocks at a time rather than use a while loop.



From ferraria at ensisun.imag.fr  Fri Apr  2 11:48:58 2004
From: ferraria at ensisun.imag.fr (anthony.ferrari@ensimag.imag.fr)
Date: Fri, 2 Apr 2004 11:48:58 +0200 (MEST)
Subject: [R] marrayNorm package
In-Reply-To: <Pine.GSO.4.40.0403161701100.15543-100000@ensisun>
Message-ID: <Pine.GSO.4.40.0404021135020.11861-100000@ensisun>


Hello,
I'm a french student working on cDNA microarrays.
I decided to use the bioconductor packages and I have a problem with
normalization functions.

As everybody who manipulates microarray data know, there are some spots
on the slide which are to be removed. In the marrayLayout there is a slot
"maSub" (logical vector) where one can define the good spots.
Besides, when you print the features of your slide there is a line :

"Currently working with a subset of 18482 spots"

so it seems to be good.
BUT the problem is that the functions of marrayNorm package don't seem to
take the subset into account even when you fill the field 'subset' with
the 'maSub' vector.

It seems to be the same with for instance the 'maPlot' function. It plots
all the spots of the array without taking the 'maSub' vector into account.


Anyone who can help me ?
I need help !!

thanks

Best regards,
Anthony Ferrari



From asemeria at cramont.it  Fri Apr  2 13:32:24 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Fri, 2 Apr 2004 13:32:24 +0200
Subject: [R] which on array
Message-ID: <OFFDEC8A85.EC09D0B1-ONC1256E6A.003F63E7@tomware.it>





Good morning !
Today I found a strange, for my poor knowledge of R, behaviour of
'which' on a matrix:

HAL9000> str(cluster.matrix)
 num [1:227, 1:6300] 2 2 2 2 2 2 2 2 2 2 ...

HAL9000> class(cluster.matrix)
[1] "matrix"

HAL9000> ase <- cluster.matrix[1:5,1:5]

HAL9000> ase
[,1] [,2] [,3] [,4] [,5]
[1,]    2    2    2    0   -2
[2,]    2    2    2    0   -2
[3,]    2    2    2    0   -2
[4,]    2    2    2    0   -2
[5,]    2    2    2    0   -2

HAL9000> ase2 <- matrix(c(2,2,2,2,2)%*%t(c(1,1,1,0,-1)),5,5)

HAL9000> ase2
     [,1] [,2] [,3] [,4] [,5]
[1,]    2    2    2    0   -2
[2,]    2    2    2    0   -2
[3,]    2    2    2    0   -2
[4,]    2    2    2    0   -2
[5,]    2    2    2    0   -2

HAL9000> str(ase2)
 num [1:5, 1:5] 2 2 2 2 2 2 2 2 2 2 ...

But now:

HAL9000> which(ase2==-2,arr.ind=T)
     row col
[1,]   1   5
[2,]   2   5
[3,]   3   5
[4,]   4   5
[5,]   5   5

HAL9000> which(ase==-2,arr.ind=T)
numeric(0)

which(ase=="-2",arr.ind=T)
     row col
[1,]   1   5
[2,]   2   5
[3,]   3   5
[4,]   4   5
[5,]   5   5


May be the original matrix 'cluster.matrix' is not
properly a numerical matrix!? But 'str' say num[...] !!??
Some suggestion.
Tks!


A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ligges at statistik.uni-dortmund.de  Fri Apr  2 13:39:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 02 Apr 2004 13:39:37 +0200
Subject: [R] which on array
In-Reply-To: <OFFDEC8A85.EC09D0B1-ONC1256E6A.003F63E7@tomware.it>
References: <OFFDEC8A85.EC09D0B1-ONC1256E6A.003F63E7@tomware.it>
Message-ID: <406D50F9.2080002@statistik.uni-dortmund.de>

asemeria at cramont.it wrote:

> 
> 
> 
> Good morning !
> Today I found a strange, for my poor knowledge of R, behaviour of
> 'which' on a matrix:
> 
> HAL9000> str(cluster.matrix)
>  num [1:227, 1:6300] 2 2 2 2 2 2 2 2 2 2 ...
> 
> HAL9000> class(cluster.matrix)
> [1] "matrix"
> 
> HAL9000> ase <- cluster.matrix[1:5,1:5]
> 
> HAL9000> ase
> [,1] [,2] [,3] [,4] [,5]
> [1,]    2    2    2    0   -2
> [2,]    2    2    2    0   -2
> [3,]    2    2    2    0   -2
> [4,]    2    2    2    0   -2
> [5,]    2    2    2    0   -2
> 
> HAL9000> ase2 <- matrix(c(2,2,2,2,2)%*%t(c(1,1,1,0,-1)),5,5)
> 
> HAL9000> ase2
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    2    2    2    0   -2
> [2,]    2    2    2    0   -2
> [3,]    2    2    2    0   -2
> [4,]    2    2    2    0   -2
> [5,]    2    2    2    0   -2
> 
> HAL9000> str(ase2)
>  num [1:5, 1:5] 2 2 2 2 2 2 2 2 2 2 ...
> 
> But now:
> 
> HAL9000> which(ase2==-2,arr.ind=T)
>      row col
> [1,]   1   5
> [2,]   2   5
> [3,]   3   5
> [4,]   4   5
> [5,]   5   5
> 
> HAL9000> which(ase==-2,arr.ind=T)
> numeric(0)
> 
> which(ase=="-2",arr.ind=T)
>      row col
> [1,]   1   5
> [2,]   2   5
> [3,]   3   5
> [4,]   4   5
> [5,]   5   5
> 
> 
> May be the original matrix 'cluster.matrix' is not
> properly a numerical matrix!? But 'str' say num[...] !!??
> Some suggestion.

My guess is that cluster.matrix, hence ase, has been generated in some 
calculation, and its numerical floating point representation is a bit 
off the expected integer. Use identical(), all.equal() and friends to 
analyse it.

Uwe Ligges


> Tks!
> 
> 
> A.S.
> 
> ----------------------------
> 
> Alessandro Semeria
> Models and Simulations Laboratory
> Montecatini Environmental Research Center (Edison Group),
> Via Ciro Menotti 48,
> 48023 Marina di Ravenna (RA), Italy
> Tel. +39 544 536811
> Fax. +39 544 538663
> E-mail: alessandro.semeria at cramont.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From JonesW at kssg.com  Fri Apr  2 13:33:15 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Fri, 2 Apr 2004 12:33:15 +0100 
Subject: [R] GARCH
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02955E50@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040402/0b5777ea/attachment.pl

From tpapp at axelero.hu  Fri Apr  2 14:15:24 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 2 Apr 2004 14:15:24 +0200
Subject: [R] "(de)linearizing" array indices
Message-ID: <20040402121524.GA1665@localhost>

Hi,

I have a dynamic programming problem with many state variables, let's
call them l, n, m, etc.  The transition probabilities are originally
given in an array form, eg

transtition[l,m,n,ll,mm,nn]

give the probability of going from l,m,n to ll,mm,nn.

However, the numerical solution is best handled when I "flatten" the L
x M x N state space into a single one (call it S), ie a linear index,
so I can deal with the problem using simple matrix algebra.  After I
get the solution, I need to get back the original state variables.

At the moment I am using two functions like this:

pack <- function(l, m, n) {
  (((((l - 1) * M) + m - 1) * N) + n
}

unpack <- function(s) {
  s <- s - 1
  n <- s %% N + 1
  s <- s %/% N
  m <- s %% M + 1
  l <- s %/% M + 1
  list(l=l, m=m, n=n)
}

to convert between S and L x N x M.

Sure, it works, but looks ugly as hell.  And I am positive that I am
abusing the R language with the above code.  So could somebody give me
a nicer solution?

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From ripley at stats.ox.ac.uk  Fri Apr  2 14:26:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 13:26:19 +0100 (BST)
Subject: [R] "(de)linearizing" array indices
In-Reply-To: <20040402121524.GA1665@localhost>
Message-ID: <Pine.LNX.4.44.0404021321080.14284-100000@gannet.stats>

If you have a choice, just treat the array as a vector (assign a NULL
dim), and when you are done with it, reassign the dimension.  Now, R
arrays are in column-major (Fortran) order, and your code seems to be in
row-major order.  You can use aperm() to go between the two.

On Fri, 2 Apr 2004, Tamas Papp wrote:

> I have a dynamic programming problem with many state variables, let's
> call them l, n, m, etc.  The transition probabilities are originally
> given in an array form, eg
> 
> transtition[l,m,n,ll,mm,nn]
> 
> give the probability of going from l,m,n to ll,mm,nn.
> 
> However, the numerical solution is best handled when I "flatten" the L
> x M x N state space into a single one (call it S), ie a linear index,
> so I can deal with the problem using simple matrix algebra.  After I
> get the solution, I need to get back the original state variables.
> 
> At the moment I am using two functions like this:
> 
> pack <- function(l, m, n) {
>   (((((l - 1) * M) + m - 1) * N) + n
> }
> 
> unpack <- function(s) {
>   s <- s - 1
>   n <- s %% N + 1
>   s <- s %/% N
>   m <- s %% M + 1
>   l <- s %/% M + 1
>   list(l=l, m=m, n=n)
> }
> 
> to convert between S and L x N x M.
> 
> Sure, it works, but looks ugly as hell.  And I am positive that I am
> abusing the R language with the above code.  So could somebody give me
> a nicer solution?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bernhard.Pfaff at drkw.com  Fri Apr  2 14:52:57 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 2 Apr 2004 14:52:57 +0200
Subject: FW: [R] GARCH
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730933@ibfftce505.is.de.dresdnerkb.com>

> > Hi there fellow R-Users, 
> > 
> > Can anyone recommend a good book on the theory and practice 
> > of applying
> > GARCH models. 
> 
Hello Wayne,
 
* Campbell, John, Lo, Andrew W., MacKinlay, A. Craig, The 
  Econometrics of Financial Markets, 1996, Princeton, NJ: 
  Princeton University Press.
 
 http://pup.princeton.edu/titles/5904.html
 
* Enders, Walter, Applied Econometric Time Series, 2. Edition 
  - August 2003, New York, NY: John Wiley and Sons.
http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471230650,descCd-tableOf
Contents.html

and

* Hamilton, James, Time Series Analysis, 1994, New Jersey: Princeton
University Press. 

http://pup.princeton.edu/TOCs/c5386.html

springs to my mind. Campbell et al. as well as Hamilton cover GARCh models
from a more theoritical/methodological point of view, whereas the book by
Enders is more orientied to GARCh applications. All given sources are not
solely dealing with heteroskedastic models but do expose them in separate
chapters.

> 
> Also, does any one know of any R related subject material  in 
> addition to
> library(tseries).


well, packages/functions for ML-estimation.

HTH,
Bernhard



> 
> Regards
> 
> Wayne
> 
> Dr Wayne R. Jones
> Senior Statistician / Research Analyst
> KSS Limited
> St James's Buildings
> 79 Oxford Street
> Manchester M1 6SS
> Tel: +44(0)161 609 4084
> Mob: +44(0)7810 523 713
> 
> 
> 
> 
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  
> Manchester  M1 6SS  England
> Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
> 
> 
> The information in this Internet email is confidential and 
> m...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From gb at stat.umu.se  Fri Apr  2 15:48:55 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Fri, 2 Apr 2004 15:48:55 +0200
Subject: [R] Question on Data Simulation
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D81030A0801@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0801@hermes.demogr.mpg.de>
Message-ID: <20040402134855.GA8212@stat.umu.se>

On Thu, Apr 01, 2004 at 05:11:31PM +0200, Rau, Roland wrote:
> Hello,
> 
> > -----Original Message-----
> > From:	"Jingky P. Lozano" [SMTP:jlozano at apoy.upm.edu.ph]
> > Sent:	Thursday, April 01, 2004 2:15 PM
> > To:	r-help at stat.math.ethz.ch
> > Subject:	[R] Question on Data Simulation 
> > 
> > Dear mailing list,
> > 
> > Do you know a specific package in R where I can create an artificial
> > survival 
> > data with controlled censoring condition?  (For example, I want to
> > simulate a 
> > data set with 5 variables and 20% of the observations are censored.)
> > 
> 	maybe I misunderstood something but why don't you create a variable,
> often denoted by 'status' in survival analysis, where the probabilities are
> 0.2 for censoring and 0.8 for the event and add this to your other data?
> 	The code should like like this (for a hypothetical size of 1000
> individuals):
> 
> 	status = sample(x=c(0,1), size=1000, replace=TRUE, prob=c(0.2,0.8))

I have an objection to this solution: If you take an uncensored sample 
and regard some of the observations as censored, you actually change the
distribution of the censored observations (You are changing a genuine
observation T = t to T > t). Since I happen to be in need of this kind of
simulated data myself, I wrote the following function, which simulates
exponential censored survival times according to the proportional hazards
model (Type I censoring with common censoring time):
----------------------------------------------------------------------
TypeIsurv <- function(covar, beta, cens.prob = 0, scale = 1){
    ## covar     = n x p matrix of covariates
    ## beta      = p-vector of regression coefficients
    ## cens.prob = expected censoring fraction (0 <= cens.prob < 1).

    ## Output: Censored exponential survival times and censoring
    ## indicators for Type I censoring (common censoring time 'tc'). 
    ## Model: proportional hazards,
    ## h(t; covar, beta) = exp(covar %*% beta)) / scale
    
    ## Here we should have some checks of indata.

    if (scale <= 0) stop("'scale' must be positive")
    if (cens.prob >= 1) return(NULL)
    
    covar <- as.matrix(covar)
    n <- NROW(covar)
    score <- exp(covar %*% beta)

    tt <- rexp(n, score) # Uncensored survival times
    if (cens.prob > 0){
        ## Find the common censoring time 'tc':
        type1 <- function(x) mean(exp(-x * score)) - cens.prob
        lower <- 0
        upper <- -log(cens.prob) / min(score) + 1
        tc <- uniroot(type1, c(lower, upper))$root
        ## Calculate event indicator and observed times:
        event <- tt <= tc
        tt <- pmin(tt, tc)
    }else{
        event <- rep(TRUE, n)
    }
    list(time = scale * tt, event = event, tc = scale * tc)
}
--------------------------------------------------------------------
A word of warning: Not well tested; I only noticed that the censoring
fraction seemed to be correct in some experiments.

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From Ted.Harding at nessie.mcc.ac.uk  Fri Apr  2 15:47:55 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 02 Apr 2004 14:47:55 +0100 (BST)
Subject: [R] tan(mu) link in GLM
Message-ID: <XFMail.040402144755.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I am interested in extending the repertoire of link functions
in glm(Y~X, family=binomial(link=...)) to include a "tan" link:

   eta = (4/pi)*tan(mu)

i.e. this link bears the same relation to the Cauchy distribution
as the probit link bears to the Gaussian. I'm interested in sage
advice about this from people who know their way aroung glm.

>From the surface, it looks as though it might just be a matter
of re-writing 'make.link' in the obvious sort of way so as to
incorporate "tan", but I fear traps ...

What am I missing?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 02-Apr-04                                       Time: 14:47:55
------------------------------ XFMail ------------------------------



From jfox at mcmaster.ca  Fri Apr  2 16:02:02 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 2 Apr 2004 09:02:02 -0500
Subject: [R] Plot symbols for more than 25 groups
In-Reply-To: <6.0.3.0.2.20040401203734.02104ec0@pop4.attglobal.net>
Message-ID: <20040402140201.HHXT15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Marc,

I may misunderstand your question, so perhaps this answer isn't what you're
looking for: 

A short time ago, Henrik Bengtsson posted a function to r-help (search for
plotSymbols in the list archive) that displays all available symbols. I
think that it's fair to say that fewer than 25 of these could be
characterized as distinct geometric symbols. 

On the other hand, the colour space for R is very large; see, e.g.,
help("colours") and the help pages linked to it. 

Moreover, if you take combinations of filled symbols and even basic colours,
you could in principle represent more than 25 groups. I wonder, however,
whether someone viewing a graph with so many different symbols will be able
to parse the information.

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc 
> R. Feldesman
> Sent: Thursday, April 01, 2004 11:40 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plot symbols for more than 25 groups
> 
> Is there any effective way to get distinct geometric plotting 
> symbols and colors for plots involving more than 25 groups?
> 
> Thanks.
> 
> 
> 
> 
> Dr. Marc R. Feldesman
> Professor and Chairman Emeritus
> Anthropology Department - Portland State University
> email:  feldesmanm at pdx.edu
> email:  feldesman at attglobal.net
> fax:    503-725-3905
> 
> 
> "Don't knock on my door if you don't know my Rottweiler's 
> name"  Warren Zevon "Its midnight and I'm not famous yet"  
> Jimmy Buffett
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Apr  2 16:22:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 15:22:45 +0100 (BST)
Subject: [R] tan(mu) link in GLM
In-Reply-To: <XFMail.040402144755.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0404021511560.14677-100000@gannet.stats>

On Fri, 2 Apr 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> I am interested in extending the repertoire of link functions
> in glm(Y~X, family=binomial(link=...)) to include a "tan" link:
> 
>    eta = (4/pi)*tan(mu)
> 
> i.e. this link bears the same relation to the Cauchy distribution
> as the probit link bears to the Gaussian. I'm interested in sage
> advice about this from people who know their way aroung glm.
> 
> From the surface, it looks as though it might just be a matter
> of re-writing 'make.link' in the obvious sort of way so as to
> incorporate "tan", but I fear traps ...

How are you going to do that?  If you edit make.link and have your own 
local copy, the namespace scoping will ensure that the system copy gets 
used, and the code in binomial() will ensure that even that does not get 
called except for the pre-coded list of links.

> What am I missing?

You need a local, modified, copy of binomial, too, AFAICS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tpapp at axelero.hu  Fri Apr  2 16:22:08 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 2 Apr 2004 16:22:08 +0200
Subject: [R] picking out eigenvalues of 1
Message-ID: <20040402142207.GA1288@localhost>

After making

E <- eigen( something )

I would like to extract those eigenvectors which have an eigenvalue of
1.  If I had an isone() function, I would simply say

E$vectors[,which(isone(E))]

but the problem is that I have no such thing.  I found all.equal, so I
could test for all.equal(x, 1), but for complex numbers, I need to use
something like all.equal(x, 1+0i), don't I?

I tried writing one,

isone <- function (x) {
  sapply(E$values - rep(1+0i, 16), function (x) {
    if(identical(all.equal(x, 0+0i), TRUE)) TRUE else FALSE })
}

and it works, but looks like a complicated hack.  Would it be possible
to improve that?  The subtraction of 1+0i is there to make sure that
the result is complex.

Thanks,

Tamas

PS.: I have been asking a lot of "it already works, but would it be
possible to do it better" questions.  I just want to learn the "way of
R", and this list seems to be the perfect place for that.  Sorry if I
am wasting anybody's time.

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From gb at stat.umu.se  Fri Apr  2 16:29:00 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Fri, 2 Apr 2004 16:29:00 +0200
Subject: [R] Underscore and ESS
Message-ID: <20040402142900.GB8212@stat.umu.se>

This is a question that I should have sent to 'ess-help', but I
take my chances...

In 'NEWS' (1.9.0) I read the good news

    o   Underscore '_' is now allowed in syntactically valid names, and
        make.names() no longer changes underscores.  Very old code
...

but when I try it in emacs (ESS 5.1.24), my '_' are changed to ' <-  '!
What can I do? Upgrade to a beta version of ESS? (I want to continue 
using ESS!)

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From chris_ciotti at yahoo.com  Fri Apr  2 16:35:16 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Fri, 02 Apr 2004 09:35:16 -0500
Subject: [R] Single Factor Anova
Message-ID: <406D7A24.8090502@yahoo.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello all -

As I progress in R I am trying to automate functions I would have 
normally farmed out to Excel, SPSS or Statistica.  Single factor anova 
is one of them.  For example, a dataset from NIST StRD 
(http://www.itl.nist.gov/div898/strd/anova/AtmWtAg.html) has two groups:

1             2

107.8681568   107.8681079
107.8681465   107.8681344
107.8681572   107.8681513
107.8681785   107.8681197
107.8681446   107.8681604
107.8681903   107.8681385
107.8681526   107.8681642
107.8681494   107.8681365
107.8681616   107.8681151
107.8681587   107.8681082
107.8681519   107.8681517
107.8681486   107.8681448
107.8681419   107.8681198
107.8681569   107.8681482
107.8681508   107.8681334
107.8681672   107.8681609
107.8681385   107.8681101
107.8681518   107.8681512
107.8681662   107.8681469
107.8681424   107.8681360
107.8681360   107.8681254
107.8681333   107.8681261
107.8681610   107.8681450
107.8681477   107.8681368


The certified values are (I hope the spaces remain intact):

Sums of                    Mean               
Variation          df      Squares              Squares             F 
Statistic

Between Instrument  1 3.63834187500000E-09 3.63834187500000E-09 
1.59467335677930E+01
Within Instrument  46 1.04951729166667E-08 2.28155932971014E-10


Any assistance you can offer is greatly appreciated.  Thanks.


- --
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7


-----BEGIN PGP SIGNATURE-----
Version: PGP 8.0

iQA/AwUBQG16A1kgIqbpS7O3EQI+0wCbBfmexypwUit+JQfHx/ePbq3csyMAoJp+
bvKJsTwB/sjUAf4sXWpWQieb
=w7Bq
-----END PGP SIGNATURE-----



From js229 at yahoo.com  Fri Apr  2 16:36:54 2004
From: js229 at yahoo.com (=?iso-8859-1?q?J=20Swinton?=)
Date: Fri, 2 Apr 2004 15:36:54 +0100 (BST)
Subject: [R] R on Tru64 OSF 5.1
Message-ID: <20040402143654.23358.qmail@web41213.mail.yahoo.com>

Has anyone achieved a current successful build of R
1.8 or 1.9 or earlier for HPUX Tru 64 OSF 5.1?

There is no binary version of R for HPUX Tru 64 OSF
5.1. The R admin manual mentions that the native make
fails on "Alpha/OSF (aka Tru 64)" and gnu make must be
used instead. There are problems reported with
building R versions around 1.4 from 2002 on R-devel,
some of which were fixed with using gmake instead of
make, some of which were fixed by changes to R
sources, but it is unclear if current R compiles on
the current OSF 5.1, which is what I need to know
before trying to get our local IS admins to try again
after their first three failures.

Jonathan



From maechler at stat.math.ethz.ch  Fri Apr  2 16:42:15 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 2 Apr 2004 16:42:15 +0200
Subject: [R] Underscore and ESS
In-Reply-To: <20040402142900.GB8212@stat.umu.se>
References: <20040402142900.GB8212@stat.umu.se>
Message-ID: <16493.31687.462061.906106@gargle.gargle.HOWL>

>>>>> "GB" == G??ran Brostr??m <gb at stat.umu.se>
>>>>>     on Fri, 2 Apr 2004 16:29:00 +0200 writes:

    GB> This is a question that I should have sent to 'ess-help', but I
    GB> take my chances...

(yes, you should have..)

    GB> In 'NEWS' (1.9.0) I read the good news

    GB> o   Underscore '_' is now allowed in syntactically valid names, and
    GB> make.names() no longer changes underscores.  Very old code
    GB> ...

    GB> but when I try it in emacs (ESS 5.1.24), my '_' are changed to ' <-  '!
    GB> What can I do? Upgrade to a beta version of ESS? (I want to continue 
    GB> using ESS!)

o Yes, upgrading to ESS-5.2.0beta would help:
  There you type "_"   *twice* to get a single "_" inserted

o OTOH, in all versions of ESS (and Emacs), you can type
  "C-q _"  which means ``quote _ '', i.e. ``literal '_'''

  (and this applies to all other emacs keys in all emacs modes)

Regards,
Martin



From bjg at network-theory.co.uk  Thu Apr  1 17:12:07 2004
From: bjg at network-theory.co.uk (Brian Gough)
Date: Thu, 01 Apr 2004 16:12:07 +0100
Subject: [R] The R Reference Manual - available as a printed book
Message-ID: <E1B93rT-0000kK-00@localhost>

Hello,

This message is to announce that the "R Reference Manual" is now
available in book form.

There are two volumes, which cover all the commands in the base
package.  They are available for order from all major bookstores.

    "The R Reference Manual - Base Package" (2 volumes)
     Hardback (6"x9"), 1440 pages total
     Retail Price $69.95 per volume
     ISBN: 0-9546120-0 (vol1), 0-9546120-1-9 (vol2)

More information is available from the following webpages:

    http://www.network-theory.co.uk/R/base/   --- Volume 1
    http://www.network-theory.co.uk/R/base2/  --- Volume 2

For each set of manuals sold, $10 will be donated to the R Foundation.

regards,

-- 
Brian Gough

p.s. All of the money raised from sales of our manuals supports the
development of more free software and documentation (see
http://www.network-theory.co.uk/about.html for details).

Network Theory Ltd -- Publishing Free Software Manuals
15 Royal Park
Bristol BS8 3AL
United Kingdom

Tel: +44 (0)117 3179309
Fax: +44 (0)117 9048108
Web: http://www.network-theory.co.uk/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From ripley at stats.ox.ac.uk  Fri Apr  2 16:52:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 15:52:18 +0100 (BST)
Subject: [R] Plot symbols for more than 25 groups
In-Reply-To: <20040402140201.HHXT15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.44.0404021525000.14677-100000@gannet.stats>

On Fri, 2 Apr 2004, John Fox wrote:

> I may misunderstand your question, so perhaps this answer isn't what you're
> looking for: 
> 
> A short time ago, Henrik Bengtsson posted a function to r-help (search for
> plotSymbols in the list archive) that displays all available symbols. I
> think that it's fair to say that fewer than 25 of these could be
> characterized as distinct geometric symbols. 

example(points) shows you there are 18, unless you count shadings.

> On the other hand, the colour space for R is very large; see, e.g.,
> help("colours") and the help pages linked to it. 
> 
> Moreover, if you take combinations of filled symbols and even basic colours,
> you could in principle represent more than 25 groups. I wonder, however,
> whether someone viewing a graph with so many different symbols will be able
> to parse the information.

They may, but long before 18 I would use more recognizable symbols, for
example l/case letters (and u/case too if required), or even numbers.  I
was yesterday looking at a thesis which plotted 40 odd different items on
various multivariate displays, and the need was mainly to identify the
apparent misfits.

You would do well to get people to recognize more than a dozen spot
colours, too -- we are tuned to recognizing quite large blobs of colour.
We see only about a dozen colours in a rainbow, which has infinitely many 
and they are adjacent.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From feldesmanm at pdx.edu  Fri Apr  2 16:56:03 2004
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Fri, 02 Apr 2004 06:56:03 -0800
Subject: [R] Plot symbols for more than 25 groups
In-Reply-To: <406D67C2.7000100@noaa.gov>
References: <6.0.3.0.2.20040401203734.02104ec0@pop4.attglobal.net>
	<406D67C2.7000100@noaa.gov>
Message-ID: <6.0.3.0.2.20040402065251.0214ee28@pop4.attglobal.net>

At 05:16 AM 4/2/2004, Carlisle Thacker watched in amazement as electrons 
turned into magical things called words:
 >Marc,
 >
 >It is very difficult for the eye to distinguish even 25 symbols or 25
 >colors on the same plot.  I find that my brain tends to saturate at 5 of
 >each, and using 5 symbols each taking five colors to get 25 groups is
 >hard to comprehend unless there colors and symbols correspond to quite
 >different characteristics of the data.   Maybe there is a better way to
 >display your data.
 >

This is a straight plot of the bivariate means of 38 different groups of 
primates.  We've already stripped out the data scatter around each mean, 
but we want to highlight where each group lies in the 2-dimensional 
space.  We haven't found a more effective way to illustrate this in a 
single plot than using a simple bivariate scatter.



From andy_liaw at merck.com  Fri Apr  2 17:01:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 2 Apr 2004 10:01:32 -0500
Subject: [R] Single Factor Anova
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AFE@usrymx25.merck.com>

So what is your question?  With R-1.8.1 on WinXPPro, I get:

> y <- scan("clipboard")
Read 48 items
> f <- factor(rep(1:2, 24))
> fit <- aov(y ~ f)
> print(unclass(sum.fit)[[1]], dig=15)
            Df               Sum Sq              Mean Sq  F value     Pr(>F)

f            1 3.63834187585414e-09 3.63834187585414e-09 15.94673 0.00023268
***
Residuals   46 1.04951729171680e-08 2.28155932981910e-10

---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Andy


> From: christopher ciotti
> 
> Hello all -
> 
> As I progress in R I am trying to automate functions I would have 
> normally farmed out to Excel, SPSS or Statistica.  Single 
> factor anova 
> is one of them.  For example, a dataset from NIST StRD 
> (http://www.itl.nist.gov/div898/strd/anova/AtmWtAg.html) has 
> two groups:
> 
> 1             2
> 
> 107.8681568   107.8681079
> 107.8681465   107.8681344
> 107.8681572   107.8681513
> 107.8681785   107.8681197
> 107.8681446   107.8681604
> 107.8681903   107.8681385
> 107.8681526   107.8681642
> 107.8681494   107.8681365
> 107.8681616   107.8681151
> 107.8681587   107.8681082
> 107.8681519   107.8681517
> 107.8681486   107.8681448
> 107.8681419   107.8681198
> 107.8681569   107.8681482
> 107.8681508   107.8681334
> 107.8681672   107.8681609
> 107.8681385   107.8681101
> 107.8681518   107.8681512
> 107.8681662   107.8681469
> 107.8681424   107.8681360
> 107.8681360   107.8681254
> 107.8681333   107.8681261
> 107.8681610   107.8681450
> 107.8681477   107.8681368
> 
> 
> The certified values are (I hope the spaces remain intact):
> 
> Sums of                    Mean               
> Variation          df      Squares              Squares             F 
> Statistic
> 
> Between Instrument  1 3.63834187500000E-09 3.63834187500000E-09 
> 1.59467335677930E+01
> Within Instrument  46 1.04951729166667E-08 2.28155932971014E-10
> 
> 
> Any assistance you can offer is greatly appreciated.  Thanks.
> 
> 
> - --
> chris ciotti (chris_ciotti at yahoo.com)
> PGP ID: 0xE94BB3B7
> 
> 
> -----BEGIN PGP SIGNATURE-----
> Version: PGP 8.0
> 
> iQA/AwUBQG16A1kgIqbpS7O3EQI+0wCbBfmexypwUit+JQfHx/ePbq3csyMAoJp+
> bvKJsTwB/sjUAf4sXWpWQieb
> =w7Bq
> -----END PGP SIGNATURE-----



From ggrothendieck at myway.com  Fri Apr  2 17:03:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 2 Apr 2004 15:03:48 +0000 (UTC)
Subject: [R] Plot symbols for more than 25 groups
References: <6.0.3.0.2.20040401203734.02104ec0@pop4.attglobal.net>
Message-ID: <loom.20040402T163010-156@post.gmane.org>

Marc R. Feldesman <feldesmanm <at> pdx.edu> writes:

> Is there any effective way to get distinct geometric plotting symbols and 
> colors for plots involving more than 25 groups?

Not sure if geometric shapes is essential but if not you could plot them 
with different letters (or LETTERS).  The following plots one point for 
each state of 26 states using a, b, ...,z :

data(state)
df <- data.frame(state.x77)[1:26,]
plot(Life.Exp ~ Income, data=df, pch=letters)

# Alternately you could plot them all with points but label each point
# with a different letter:

plot(Life.Exp ~ Income, data=df, pch=20)
text( df$Income, df$Life.Exp, LETTERS, pos=3)

# or even use the state abbreviations in this case:

plot(Life.Exp ~ Income, data=df, pch=20)
text(df$Income,df$Life.Exp,state.abb[1:26],pos=3)



From andy_liaw at merck.com  Fri Apr  2 17:07:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 2 Apr 2004 10:07:20 -0500
Subject: [R] Single Factor Anova
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7AFF@usrymx25.merck.com>

Sorry, I apparently left out the line

  sum.fit <- summary(fit)

Andy

> From: Liaw, Andy
> 
> So what is your question?  With R-1.8.1 on WinXPPro, I get:
> 
> > y <- scan("clipboard")
> Read 48 items
> > f <- factor(rep(1:2, 24))
> > fit <- aov(y ~ f)
> > print(unclass(sum.fit)[[1]], dig=15)
>             Df               Sum Sq              Mean Sq  F 
> value     Pr(>F)
> 
> f            1 3.63834187585414e-09 3.63834187585414e-09 
> 15.94673 0.00023268
> ***
> Residuals   46 1.04951729171680e-08 2.28155932981910e-10
> 
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Andy
> 
> 
> > From: christopher ciotti
> > 
> > Hello all -
> > 
> > As I progress in R I am trying to automate functions I would have 
> > normally farmed out to Excel, SPSS or Statistica.  Single 
> > factor anova 
> > is one of them.  For example, a dataset from NIST StRD 
> > (http://www.itl.nist.gov/div898/strd/anova/AtmWtAg.html) has 
> > two groups:
> > 
> > 1             2
> > 
> > 107.8681568   107.8681079
> > 107.8681465   107.8681344
> > 107.8681572   107.8681513
> > 107.8681785   107.8681197
> > 107.8681446   107.8681604
> > 107.8681903   107.8681385
> > 107.8681526   107.8681642
> > 107.8681494   107.8681365
> > 107.8681616   107.8681151
> > 107.8681587   107.8681082
> > 107.8681519   107.8681517
> > 107.8681486   107.8681448
> > 107.8681419   107.8681198
> > 107.8681569   107.8681482
> > 107.8681508   107.8681334
> > 107.8681672   107.8681609
> > 107.8681385   107.8681101
> > 107.8681518   107.8681512
> > 107.8681662   107.8681469
> > 107.8681424   107.8681360
> > 107.8681360   107.8681254
> > 107.8681333   107.8681261
> > 107.8681610   107.8681450
> > 107.8681477   107.8681368
> > 
> > 
> > The certified values are (I hope the spaces remain intact):
> > 
> > Sums of                    Mean               
> > Variation          df      Squares              Squares     
>         F 
> > Statistic
> > 
> > Between Instrument  1 3.63834187500000E-09 3.63834187500000E-09 
> > 1.59467335677930E+01
> > Within Instrument  46 1.04951729166667E-08 2.28155932971014E-10
> > 
> > 
> > Any assistance you can offer is greatly appreciated.  Thanks.
> > 
> > 
> > - --
> > chris ciotti (chris_ciotti at yahoo.com)
> > PGP ID: 0xE94BB3B7
> > 
> > 
> > -----BEGIN PGP SIGNATURE-----
> > Version: PGP 8.0
> > 
> > iQA/AwUBQG16A1kgIqbpS7O3EQI+0wCbBfmexypwUit+JQfHx/ePbq3csyMAoJp+
> > bvKJsTwB/sjUAf4sXWpWQieb
> > =w7Bq
> > -----END PGP SIGNATURE-----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse 
> Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known 
> outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and 
> in Japan as
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally
> privileged. It is intended solely for the use of the 
> individual or entity
> named on this message.  If you are not the intended 
> recipient, and have
> received this message in error, please notify us immediately 
> by reply e-mail
> and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From knoblauch at lyon.inserm.fr  Fri Apr  2 17:22:17 2004
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Fri,  2 Apr 2004 17:22:17 +0200
Subject: [R] Plot symbols for more than 25 groups
Message-ID: <1080919337.406d85297b5ef@webmail.lyon.inserm.fr>


A rainbow has a continuous distribution of wavelengths.  These are
not at all the same thing as colors!

Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> 
> You would do well to get people to recognize more than a dozen spot
> colours, too -- we are tuned to recognizing quite large blobs of colour.
> We see only about a dozen colours in a rainbow, which has infinitely many 
> and they are adjacent.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 



____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 92 34 61
portable: 06 84 10 64 10



From chris_ciotti at yahoo.com  Fri Apr  2 17:27:50 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Fri, 02 Apr 2004 10:27:50 -0500
Subject: [R] Single Factor Anova
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7AFE@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7AFE@usrymx25.merck.com>
Message-ID: <406D8676.8020504@yahoo.com>

Liaw, Andy wrote:

>So what is your question?  With R-1.8.1 on WinXPPro, I get:
>  
>

Obviously I was asking about how one would go about running one.  I 
appreciate your answer.  Thanks.


-- 
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7



From ggrothendieck at myway.com  Fri Apr  2 17:39:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 2 Apr 2004 15:39:07 +0000 (UTC)
Subject: [R] "(de)linearizing" array indices
References: <20040402121524.GA1665@localhost>
Message-ID: <loom.20040402T173502-600@post.gmane.org>


R stores its arrays in reverse odometer order where the leftmost
array index varies fastest.  The result of:

matrix(1:4,2) 

shows this.

Your pack and unpack are in odometer
order with the rightmost index varying fastest.  If you were
to move to R's convention then you could move back and
forth by just changing the dimensions (untested):

x <- array(1:64, rep(2,6) )

# array to matrix
xm <- matrix(x, 8) 

or

# other way
x <- array(xm, rep(2,6) )



Tamas Papp <tpapp <at> axelero.hu> writes:

: 
: Hi,
: 
: I have a dynamic programming problem with many state variables, let's
: call them l, n, m, etc.  The transition probabilities are originally
: given in an array form, eg
: 
: transtition[l,m,n,ll,mm,nn]
: 
: give the probability of going from l,m,n to ll,mm,nn.
: 
: However, the numerical solution is best handled when I "flatten" the L
: x M x N state space into a single one (call it S), ie a linear index,
: so I can deal with the problem using simple matrix algebra.  After I
: get the solution, I need to get back the original state variables.
: 
: At the moment I am using two functions like this:
: 
: pack <- function(l, m, n) {
:   (((((l - 1) * M) + m - 1) * N) + n
: }
: 
: unpack <- function(s) {
:   s <- s - 1
:   n <- s %% N + 1
:   s <- s %/% N
:   m <- s %% M + 1
:   l <- s %/% M + 1
:   list(l=l, m=m, n=n)
: }
: 
: to convert between S and L x N x M.
: 
: Sure, it works, but looks ugly as hell.  And I am positive that I am
: abusing the R language with the above code.  So could somebody give me
: a nicer solution?
: 
: Thanks,
: 
: Tamas
:



From ripley at stats.ox.ac.uk  Fri Apr  2 17:47:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 16:47:19 +0100 (BST)
Subject: [R] Plot symbols for more than 25 groups
In-Reply-To: <1080919337.406d85297b5ef@webmail.lyon.inserm.fr>
Message-ID: <Pine.LNX.4.44.0404021642370.6649-100000@gannet.stats>

On Fri, 2 Apr 2004, Ken Knoblauch wrote:

> A rainbow has a continuous distribution of wavelengths.  These are
> not at all the same thing as colors!

I did not say they were.  However, a rainbow has infinitely many colours
(it is show as an arc on a standard colour chart), and your statement
about `not at all the same thing'.

Note that R has a rainbow() function, and that too is nothing to do with 
wavelengths.


> Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> 
> > 
> > You would do well to get people to recognize more than a dozen spot
> > colours, too -- we are tuned to recognizing quite large blobs of colour.
> > We see only about a dozen colours in a rainbow, which has infinitely many 
> > and they are adjacent.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mazaroglu at gmx.de  Fri Apr  2 17:47:28 2004
From: mazaroglu at gmx.de (Martina Azaroglu)
Date: Fri, 2 Apr 2004 17:47:28 +0200 (MEST)
Subject: [R] How canI convert date-time to Julian date?
Message-ID: <9264.1080920848@www61.gmx.net>

Hello!!
I need some help! I tried everything, but nothing worked!
I have a vector c with dates in it, in the format "2004-04-01" and i need to
convert it in the form 
"04/01/2004" or "01/04/2004" !
How can i do that??

Thanks
 Bye Martina

--



From solares at unsl.edu.ar  Fri Apr  2 17:47:48 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Fri, 2 Apr 2004 12:47:48 -0300 (ART)
Subject: [R] convert excell file to text with RODBC package
Message-ID: <50155.209.13.250.66.1080920868.squirrel@inter17.unsl.edu.ar>

Hi, i can convert excell to list in R with package RODBC ()but i don't
understand 2 mistake
1) Don't read the last row of the table excell
2) Don' t take the hours

My excell file call prueba4.xls and have the following rows:
where prueba4.xls was make in excell (office xp) and have one spreadsheet
call "Hoja1", you see each rows of she:
D??a	Hora	col1	col2	col3	col4	col5	col6	col7	col8
15/12/2003	12:14:59	217	2760	8,2	35	79,6	86,4
15/12/2003	12:15:00	217	2764	8,2	35	79,6	86,4
15/12/2003	12:15:01	217	2758	8,3	35	79,6	86,4
15/12/2003	12:15:02	217	2760	8,3	35	79,6	86,4
15/12/2003	12:15:03	217	2755	8,3	35	79,6	86,4
15/12/2003	12:15:04	217	2766	8,3	35	79,6	86,4
15/12/2003	12:15:05	217,1	2766	8,3	35,1	79,6	86,4
15/12/2003	12:15:06	217,1	2758	8,3	35,1	79,6	86,4
15/12/2003	12:15:07	217,1	2768	8,3	35,1	79,6	86,4

My code (i use the R 1.7.1 for windows xp) is the following:

 library(RODBC)
> canal<-odbcConnectExcel("c:/prueba4.xls")
> tablas<-sqlTables(canal)
> tablas
    TABLE_CAT TABLE_SCHEM TABLE_NAME   TABLE_TYPE REMARKS
1 c:\\prueba4        <NA>     Hoja1$ SYSTEM TABLE    <NA>
2 c:\\prueba4        <NA>     Hoja2$ SYSTEM TABLE    <NA>
3 c:\\prueba4        <NA>     Hoja3$ SYSTEM TABLE    <NA>
> tbl<-sqlFetch(canal,substr(tablas[1,3],1,nchar(tablas[1,3])-1))
> tbl[1]
                  D??a
1 2003-12-15 00:00:00
2 2003-12-15 00:00:00
3 2003-12-15 00:00:00
4 2003-12-15 00:00:00
5 2003-12-15 00:00:00
6 2003-12-15 00:00:00
7 2003-12-15 00:00:00
8 2003-12-15 00:00:00
9 2003-12-15 00:00:00

??Which is the mistake?.

And the second question is the command odbcFechRows() don't work,
for example if write
tbl<-odbcFetchRows(canal) in place of sqlFetch result a error
stat = -1 and the list is NULL
odbcGetErrMsg return No results available. Thank Ruben



From ripley at stats.ox.ac.uk  Fri Apr  2 17:55:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 16:55:49 +0100 (BST)
Subject: [R] How canI convert date-time to Julian date?
In-Reply-To: <9264.1080920848@www61.gmx.net>
Message-ID: <Pine.LNX.4.44.0404021649330.6649-100000@gannet.stats>

On Fri, 2 Apr 2004, Martina Azaroglu wrote:

> I need some help! I tried everything, but nothing worked!

I am sure you did not try _everything_, but it would have been helpful to 
give some idea of what you were trying.

> I have a vector c with dates in it, in the format "2004-04-01" and i need to
> convert it in the form 
> "04/01/2004" or "01/04/2004" !

In 1.9.0 beta you can do

format(as.Date("2004-04-01"), "%m/%d/%Y")

if you mean American usage for dates.  But that has nothing to do with 
'Julian date'.  In earlier versions

format(as.POSIXlt("2004-04-01"), "%m/%d/%Y")

will do this too.

> How can i do that??

Try help.search?  For example, help.search("date") in 1.8.1 gave

as.POSIXct(base)        Date-time Conversion Functions
format.POSIXct(base)    Date-time Conversion Functions to and from
                        Character
which are the correct pages for my solution above.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr  2 18:04:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 17:04:37 +0100 (BST)
Subject: [R] convert excell file to text with RODBC package
In-Reply-To: <50155.209.13.250.66.1080920868.squirrel@inter17.unsl.edu.ar>
Message-ID: <Pine.LNX.4.44.0404021658050.6649-100000@gannet.stats>

On Fri, 2 Apr 2004 solares at unsl.edu.ar wrote:

> And the second question is the command odbcFechRows() don't work,

That is not a question, that is an assertion!  There is no such function, 
and I presume you mean odbcFetchRows().

> for example if write
> tbl<-odbcFetchRows(canal) in place of sqlFetch result a error
> stat = -1 and the list is NULL
> odbcGetErrMsg return No results available. Thank Ruben

If you used odbcFetchRows(), it worked exactly as documented.  Please do
read the help pages!  Hint: you had no `pending rowset'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Fri Apr  2 18:23:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 2 Apr 2004 16:23:58 +0000 (UTC)
Subject: [R] How canI convert date-time to Julian date?
References: <9264.1080920848@www61.gmx.net>
Message-ID: <loom.20040402T180301-549@post.gmane.org>



Convert your character dates to one of R's date classes
(POSIXlt, chron or the new Date class in 1.9.0) and then
format the date.  The m/d/y format you want is actually the
default format in chron:

z <- c( "2004-01-20", "2004-02-22" )  # test vector

# load chron and set default year to 4 digits
require(chron)
options( chron.year.abb = FALSE ) 

# convert date to chron and format
format( chron(z, format = "y-m-d", out.format = NULL) )

The options statement makes chron use 4 digit years by
default and the out.format=NULL tells chron to use its
default format for output rather than the format that
the dates were input in.

One caveat.  Ensure that you really do have character dates.
If they were read in using read.table they may be factors.
If that's the case convert them to character first:

z <- as.character(z)


Martina Azaroglu <mazaroglu <at> gmx.de> writes:

: 
: Hello!!
: I need some help! I tried everything, but nothing worked!
: I have a vector c with dates in it, in the format "2004-04-01" and i need to
: convert it in the form 
: "04/01/2004" or "01/04/2004" !
: How can i do that??
: 
: Thanks
:  Bye Martina
: 
: --
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From clint at ecy.wa.gov  Fri Apr  2 18:43:01 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 2 Apr 2004 08:43:01 -0800 (PST)
Subject: [R] Winding Number
Message-ID: <Pine.LNX.4.44.0404020833350.19413-100000@aeolus.ecy.wa.gov>

I have shapefiles for the state climatic divisions for the United States
and read.shape brings them in wonderfully.  Now I wish to run through a
list of several thousand observation sites to find out in which division
each is located.  I figure that I can compute the winding number for each
site and be done.  However a search doesn't find any references and I
can't find a winding number function among the map/tools/stats.  I have
the code for an efficient C++ but was expecting that it would already be
implemented as an R function

Since I haven't used the map/tools/stats collection before, I suspect I'm 
overlooking the function and would be thankful for a pointer.

TIA

Clint

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From Roger.Bivand at nhh.no  Fri Apr  2 19:15:12 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 2 Apr 2004 19:15:12 +0200 (CEST)
Subject: [R] Winding Number
In-Reply-To: <Pine.LNX.4.44.0404020833350.19413-100000@aeolus.ecy.wa.gov>
Message-ID: <Pine.LNX.4.44.0404021854030.28093-100000@reclus.nhh.no>

On Fri, 2 Apr 2004, Clint Bowman wrote:

> I have shapefiles for the state climatic divisions for the United States
> and read.shape brings them in wonderfully.  Now I wish to run through a
> list of several thousand observation sites to find out in which division
> each is located.  I figure that I can compute the winding number for each
> site and be done.  However a search doesn't find any references and I
> can't find a winding number function among the map/tools/stats.  I have
> the code for an efficient C++ but was expecting that it would already be
> implemented as an R function
> 
> Since I haven't used the map/tools/stats collection before, I suspect I'm 
> overlooking the function and would be thankful for a pointer.
> 

This is work in progress - where all good ideas and contributions will be
welcome. If you look on http://sourceforge.net/projects/r-spatial/, you
will see an "alpha" package called "sp", which already has a
point-in-polygon facility, but which may not scale up to the kinds of data
volumes you have, but which invites a spatial query (match polygon?)
function between a SpatialDataFrame with point coordinates and a
SpatialDataPolygons object (sometime). This is only as source packages so 
far, so Windows binaries are not yet available.

I have used the splancs package inout() function before, trying the points
coordinate matrix on each polygon in turn; splancs is available as a
Windows binary. This ought to be less "rough at the edges", and in time
will be. The immediate solution is to use splancs, but this will not work
if the Shapes have multiple polygons. There is a more specialised list for
these kinds of questions in addition to r-help:

https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-geo

and since yesterday (thanks to Jonathan Baron), its archives are also 
searchable from:

http://finzi.psych.upenn.edu/search.html

This question hasn't come up there, but maybe we could move further 
discussion there - posting only for subscribers?

> TIA
> 
> Clint
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From clint at ecy.wa.gov  Fri Apr  2 20:29:34 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 2 Apr 2004 10:29:34 -0800 (PST)
Subject: [R] Winding Number
In-Reply-To: <Pine.LNX.4.44.0404021854030.28093-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0404021026320.19413-100000@aeolus.ecy.wa.gov>

Roger,

Thanks for your reference.  Since I can get the polygon coordinates (and 
have the coordinates of my sites, I can cobble together a function that 
will do the trick.

Again, thanks,

Clint

On Fri, 2 Apr 2004, Roger Bivand wrote:

> On Fri, 2 Apr 2004, Clint Bowman wrote:
> 
> > I have shapefiles for the state climatic divisions for the United States
> > and read.shape brings them in wonderfully.  Now I wish to run through a
> > list of several thousand observation sites to find out in which division
> > each is located.  I figure that I can compute the winding number for each
> > site and be done.  However a search doesn't find any references and I
> > can't find a winding number function among the map/tools/stats.  I have
> > the code for an efficient C++ but was expecting that it would already be
> > implemented as an R function
> > 
> > Since I haven't used the map/tools/stats collection before, I suspect I'm 
> > overlooking the function and would be thankful for a pointer.
> > 
> 
> This is work in progress - where all good ideas and contributions will be
> welcome. If you look on http://sourceforge.net/projects/r-spatial/, you
> will see an "alpha" package called "sp", which already has a
> point-in-polygon facility, but which may not scale up to the kinds of data
> volumes you have, but which invites a spatial query (match polygon?)
> function between a SpatialDataFrame with point coordinates and a
> SpatialDataPolygons object (sometime). This is only as source packages so 
> far, so Windows binaries are not yet available.
> 
> I have used the splancs package inout() function before, trying the points
> coordinate matrix on each polygon in turn; splancs is available as a
> Windows binary. This ought to be less "rough at the edges", and in time
> will be. The immediate solution is to use splancs, but this will not work
> if the Shapes have multiple polygons. There is a more specialised list for
> these kinds of questions in addition to r-help:
> 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-geo
> 
> and since yesterday (thanks to Jonathan Baron), its archives are also 
> searchable from:
> 
> http://finzi.psych.upenn.edu/search.html
> 
> This question hasn't come up there, but maybe we could move further 
> discussion there - posting only for subscribers?
> 
> > TIA
> > 
> > Clint
> > 
> > 
> 
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From tkeitt at mail.utexas.edu  Fri Apr  2 20:36:50 2004
From: tkeitt at mail.utexas.edu (Timothy H. Keitt)
Date: Fri, 02 Apr 2004 12:36:50 -0600
Subject: [R-sig-Geo] Re: [R] Winding Number
In-Reply-To: <Pine.LNX.4.44.0404021026320.19413-100000@aeolus.ecy.wa.gov>
References: <Pine.LNX.4.44.0404021026320.19413-100000@aeolus.ecy.wa.gov>
Message-ID: <1080931009.5547.11.camel@workstation-0-0>

Members of this list may be interested in
http://www.vividsolutions.com/jts/jtshome.htm. There is a C++ port asw
well by the postgis folks.

Tim

On Fri, 2004-04-02 at 12:29, Clint Bowman wrote:
> Roger,
> 
> Thanks for your reference.  Since I can get the polygon coordinates (and 
> have the coordinates of my sites, I can cobble together a function that 
> will do the trick.
> 
> Again, thanks,
> 
> Clint
> 
> On Fri, 2 Apr 2004, Roger Bivand wrote:
> 
> > On Fri, 2 Apr 2004, Clint Bowman wrote:
> > 
> > > I have shapefiles for the state climatic divisions for the United States
> > > and read.shape brings them in wonderfully.  Now I wish to run through a
> > > list of several thousand observation sites to find out in which division
> > > each is located.  I figure that I can compute the winding number for each
> > > site and be done.  However a search doesn't find any references and I
> > > can't find a winding number function among the map/tools/stats.  I have
> > > the code for an efficient C++ but was expecting that it would already be
> > > implemented as an R function
> > > 
> > > Since I haven't used the map/tools/stats collection before, I suspect I'm 
> > > overlooking the function and would be thankful for a pointer.
> > > 
> > 
> > This is work in progress - where all good ideas and contributions will be
> > welcome. If you look on http://sourceforge.net/projects/r-spatial/, you
> > will see an "alpha" package called "sp", which already has a
> > point-in-polygon facility, but which may not scale up to the kinds of data
> > volumes you have, but which invites a spatial query (match polygon?)
> > function between a SpatialDataFrame with point coordinates and a
> > SpatialDataPolygons object (sometime). This is only as source packages so 
> > far, so Windows binaries are not yet available.
> > 
> > I have used the splancs package inout() function before, trying the points
> > coordinate matrix on each polygon in turn; splancs is available as a
> > Windows binary. This ought to be less "rough at the edges", and in time
> > will be. The immediate solution is to use splancs, but this will not work
> > if the Shapes have multiple polygons. There is a more specialised list for
> > these kinds of questions in addition to r-help:
> > 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-geo
> > 
> > and since yesterday (thanks to Jonathan Baron), its archives are also 
> > searchable from:
> > 
> > http://finzi.psych.upenn.edu/search.html
> > 
> > This question hasn't come up there, but maybe we could move further 
> > discussion there - posting only for subscribers?
> > 
> > > TIA
> > > 
> > > Clint
> > > 
> > > 
> > 
> > 
-- 
Timothy H. Keitt
Section of Integrative Biology
University of Texas at Austin
http://www.keittlab.org/



From lmassis at yahoo.com.br  Fri Apr  2 20:05:50 2004
From: lmassis at yahoo.com.br (Leonard Assis)
Date: Fri, 2 Apr 2004 15:05:50 -0300
Subject: [R] Formulae Construction
In-Reply-To: <1080836048.406c3fd0bfec0@webmail.utm.utoronto.ca>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA5j8071toSkezRUEHhdMv0cKAAAAQAAAA1VUZXmiA2kWEq8bhEbzBSAEAAAAA@yahoo.com.br>

Where Can I Have More information about Formulae in R (Nesti8ng,
Crossing Factors, Etc). I??ve tried to obtain this information in R Help
and that source were a kind of inconclusive for my Doubts.

[]s
Leonard Assis
Estat??stico
CONFE 7439



From ripley at stats.ox.ac.uk  Fri Apr  2 21:03:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 2 Apr 2004 20:03:39 +0100 (BST)
Subject: [R] Formulae Construction
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA5j8071toSkezRUEHhdMv0cKAAAAQAAAA1VUZXmiA2kWEq8bhEbzBSAEAAAAA@yahoo.com.br>
Message-ID: <Pine.LNX.4.44.0404022000350.7112-100000@gannet.stats>

Then you need to look elsewhere, like the reference in ?formula and the 
books in the R FAQ.

I am his co-author, but I still think Bill Venables' account in MASS
(Venables & Ripley, 1994, 7, 9, 2002) is unequalled.

On Fri, 2 Apr 2004, Leonard Assis wrote:

> Where Can I Have More information about Formulae in R (Nesti8ng,
> Crossing Factors, Etc). I??ve tried to obtain this information in R Help
> and that source were a kind of inconclusive for my Doubts.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Apr  2 21:00:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 2 Apr 2004 14:00:24 -0500
Subject: [R] Formulae Construction
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B03@usrymx25.merck.com>

I believe the white book has a chapter on the topic.

HTH,
Andy

> From: Leonard Assis
> 
> Where Can I Have More information about Formulae in R (Nesti8ng,
> Crossing Factors, Etc). I??ve tried to obtain this information 
> in R Help
> and that source were a kind of inconclusive for my Doubts.
> 
> []s
> Leonard Assis
> Estat??stico
> CONFE 7439
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gwiggner at lix.polytechnique.fr  Fri Apr  2 21:40:52 2004
From: gwiggner at lix.polytechnique.fr (gwiggner@lix.polytechnique.fr)
Date: Fri,  2 Apr 2004 21:40:52 +0200
Subject: [R] General question on prediction
Message-ID: <1080934852.406dc1c4e89d1@www.lix.polytechnique.fr>

Hi,

Could you give me your opinions on this idea, please ?
Predicting Y from X is similar* to finding the Expectation{Y|X}.
The joint probability Pr(X,Y) 'contains' this expectation.
If this joint probability follows a 'simple' law, it is easier to 
estimate it directly than using regression techniques.

Thanks,
Claus


*when using least sqares error

----------------------------------------------------------------
This message was sent using IMP, the Internet Messaging Program.



From jfri at novozymes.com  Fri Apr  2 23:06:05 2004
From: jfri at novozymes.com (JFRI (Jesper Frickmann))
Date: Fri, 2 Apr 2004 16:06:05 -0500
Subject: [R] Doing SQL GROUP BY in R
Message-ID: <D53147E531BFBC4B8853FD134FAEE44D14FDE9@exusfr014.novo.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040402/5ad5777c/attachment.pl

From mok2 at physics.buffalo.edu  Fri Apr  2 23:27:15 2004
From: mok2 at physics.buffalo.edu (Mark O. Kimball)
Date: Fri, 2 Apr 2004 16:27:15 -0500
Subject: [R] Extending a vector
Message-ID: <200404021627.15131.mok2@physics.buffalo.edu>

I believe this should be an easy thing to do...

I have a function I repeatably call which takes input parameters and
outputs columns to various data frames. I also wish to keep a summary of
certain values as I call the function. I though keeping the values in a
vector then appending the vector by the new amounts would be the way to
do this.

Example: (this is what I want even thought the below syntax is wrong)

a <- 1
print(a) --> 1

a <- c(a,2)
print(a) --> 1,2

a <- c(a,2,4,5)
print(a) --> 1,2,2,4,5

Any help would be greatly appreciated...

Marko
-- 
Mark O. Kimball
Gasparinilab, University of Buffalo  |  Low temp physics
mok2 at physics.buffalo.edu  |  URL: enthalpy.physics.buffalo.edu
lab phone: 716-645-2017x122  Fax: 716-645-2507



From jahernan at umn.edu  Fri Apr  2 23:28:25 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Fri, 02 Apr 2004 15:28:25 -0600
Subject: [R] cumsum() by subgroup
Message-ID: <6.0.0.22.0.20040402152525.0212e5b0@jahernan.email.umn.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040402/49f9f16d/attachment.pl

From vograno at evafunds.com  Fri Apr  2 23:28:41 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 2 Apr 2004 13:28:41 -0800
Subject: [R] Doing SQL GROUP BY in R
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3AEC@phost015.intermedia.net>

You might want to look at ?table and ?xtab. See also ?tapply for use of
general functions (other than just COUNT) with GROUP BY.

HTH,
Vadim

> -----Original Message-----
> From: JFRI (Jesper Frickmann) [mailto:jfri at novozymes.com] 
> Sent: Friday, April 02, 2004 1:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Doing SQL GROUP BY in R
> 
> 
> I want a list of the number of times some factor levels 
> appear together, similar to the following SQL statement:
> 
> SELECT A, B, COUNT(C) FROM TBL GROUP BY A, B
> 
> How do I do that with a data.frame in R?
> 
> Thanks,
> Jesper Frickmann
> Statistician, Quality Control
> Novozymes North America Inc.
> 77 Perry Chapel Church Road
> Franklinton, NC 27525
> USA
> Tel. +1 919 494 3266
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From macq at llnl.gov  Fri Apr  2 23:51:35 2004
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 2 Apr 2004 13:51:35 -0800
Subject: [R] cumsum() by subgroup
In-Reply-To: <6.0.0.22.0.20040402152525.0212e5b0@jahernan.email.umn.edu>
References: <6.0.0.22.0.20040402152525.0212e5b0@jahernan.email.umn.edu>
Message-ID: <p0600200bbc938ffd6d94@[128.115.153.6]>

If you first look at the output of the inner core of your expression:

      split(d.1, d.1$v1)

you might get a hint. Then make this change.

cbind(d.1, v4=c(lapply(split(d.1, d.1$v1), function(x) cumsum(x$v3)), 
recursive=T))

There is no variable named "y" in your data frame, so "x$y" is the 
wrong thing to use in the function definition.

You didn't say which variable you wanted the cumsum of, but v3 seemed 
to make sense...

-Don

At 3:28 PM -0600 4/2/04, Jose A. Hernandez wrote:
>I need to do a simple cumulative sum by group and add the result to the
>data. I found an earlier thread in the help files with a few suggestions.
>
>Somewhat, one of the suggestions does not work with "my data", and I don't
>really understand why ?
>
>The error am getting using the "my data" below is...
>
>Error in data.frame(..., check.names = FALSE) :
>          arguments imply differing number of rows: 10, 0
>
>I'd truly appreciate your input on this matter.
>
>Am still a beginner on R, but am determined to use it for the rest of my
>life...
>
>Thanks in advance.
>
>Sincerely,
>
>Jose
>
>
># The example I found on the help files provided by J. Fox on Wed Jul 24 2002
>f <- c("left","left","left","left","left","left","left","left","left",
>"right","right","right","right","right","right","right","right","right",)
>x <- c(1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8,9)
>y <- c(0,0,9,10,23,45,13,2,6,10,26,9,50,78,20,7,20,19)
>tmp <- data.frame(f,x,y)
>
>cbind(tmp, s=c(lapply(split(tmp, tmp$f), function(x) cumsum(x$y)),
>recursive=T))
>
>
># my data, the same code but it doesn't work...
>v1 <- c(1,1,1,1,1,2,2,2,2,2)
>v2 <- c(1,2,3,4,5,1,2,3,4,5)
>v3 <- c(0,0.1,0.11,0.3,0.5,0,0.4,0.5,2.4,2.6)
>d.1 <- data.frame(v1,v2,v3)
>
>cbind(d.1, v4=c(lapply(split(d.1, d.1$v1), function(x) cumsum(x$y)),
>recursive=T))
>
>
>--
>Jose A. Hernandez
>Ph.D. Candidate
>Precision Agriculture Center
>
>Department of Soil, Water, and Climate
>University of Minnesota
>1991 Upper Buford Circle
>St. Paul, MN 55108
>
>Ph. (612) 625-0445, Fax. (612) 625-2208
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From upton at mitre.org  Fri Apr  2 23:52:21 2004
From: upton at mitre.org (Stephen C. Upton)
Date: Fri, 2 Apr 2004 16:52:21 -0500
Subject: [R] Extending a vector
In-Reply-To: <200404021627.15131.mok2@physics.buffalo.edu>
Message-ID: <000001c418fc$c7251d20$94470843@MITRE.ORG>

Marko,

Looks fine to me. Why do you think the syntax is incorrect? Works for me in
1.8 on Windows.
> a <- 1
> a
[1] 1
> a <- c(a,2)
> a
[1] 1 2
> a <- c(a,2,4,5)
> a
[1] 1 2 2 4 5
> 

steve

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mark O. Kimball
Sent: Friday, April 02, 2004 4:27 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Extending a vector


I believe this should be an easy thing to do...

I have a function I repeatably call which takes input parameters and
outputs columns to various data frames. I also wish to keep a summary of
certain values as I call the function. I though keeping the values in a
vector then appending the vector by the new amounts would be the way to
do this.

Example: (this is what I want even thought the below syntax is wrong)

a <- 1
print(a) --> 1

a <- c(a,2)
print(a) --> 1,2

a <- c(a,2,4,5)
print(a) --> 1,2,2,4,5

Any help would be greatly appreciated...

Marko
-- 
Mark O. Kimball
Gasparinilab, University of Buffalo  |  Low temp physics
mok2 at physics.buffalo.edu  |  URL: enthalpy.physics.buffalo.edu
lab phone: 716-645-2017x122  Fax: 716-645-2507

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pkleiber at honlab.nmfs.hawaii.edu  Sat Apr  3 00:42:07 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Fri, 02 Apr 2004 12:42:07 -1000
Subject: [R] Doing SQL GROUP BY in R
In-Reply-To: <D53147E531BFBC4B8853FD134FAEE44D14FDE9@exusfr014.novo.dk>
References: <D53147E531BFBC4B8853FD134FAEE44D14FDE9@exusfr014.novo.dk>
Message-ID: <406DEC3F.3020206@honlab.nmfs.hawaii.edu>

Assuming you have a data.frame, dat, with coluns A and B, I think what you 
want would be:

 > table(paste(dat$A,dat$B))


JFRI (Jesper Frickmann) wrote:
> I want a list of the number of times some factor levels appear together,
> similar to the following SQL statement:
> 
> SELECT A, B, COUNT(C) FROM TBL GROUP BY A, B
> 
> How do I do that with a data.frame in R?
> 
> Thanks,
> Jesper Frickmann
> Statistician, Quality Control
> Novozymes North America Inc.
> 77 Perry Chapel Church Road
> Franklinton, NC 27525
> USA
> Tel. +1 919 494 3266
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From rbaer at atsu.edu  Sat Apr  3 00:56:32 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Fri, 2 Apr 2004 16:56:32 -0600
Subject: [R] Extending a vector
References: <000001c418fc$c7251d20$94470843@MITRE.ORG>
Message-ID: <001301c41905$bdbd09b0$2e80010a@BigBaer>


>Marko,

>Looks fine to me. Why do you think the syntax is incorrect? Works for me in
>1.8 on Windows.

> I have a function I repeatably call which takes input parameters and
> outputs columns to various data frames.
Looks fine to me although it is not completely clear that your request for
column and "pseudocode"your syntax match.

To make a into a column vector try a transpose: t(a)

This should line up with a dataframe column in whatever logic you use in the
function (which you did not explain to uw)

HTH,
Rob Baer



From mok2 at physics.buffalo.edu  Sat Apr  3 01:03:55 2004
From: mok2 at physics.buffalo.edu (Mark O. Kimball)
Date: Fri, 2 Apr 2004 18:03:55 -0500
Subject: [R] Extending a vector
In-Reply-To: <200404021627.15131.mok2@physics.buffalo.edu>
References: <200404021627.15131.mok2@physics.buffalo.edu>
Message-ID: <200404021803.55694.mok2@physics.buffalo.edu>

On Friday 02 April 2004 04:52 pm, you wrote:

>> I believe this should be an easy thing to do...
>>
>> I have a function I repeatably call which takes input parameters and
>> outputs columns to various data frames. I also wish to keep a summary
>> of
>> certain values as I call the function. I though keeping the values in
>> a
>> vector then appending the vector by the new amounts would be the way
>> to
>> do this.
>>
>> Example: (this is what I want even thought the below syntax is wrong)
>>
>> a <- 1
>> print(a) --> 1
>>
>> a <- c(a,2)
>> print(a) --> 1,2
>>
>> a <- c(a,2,4,5)
>> print(a) --> 1,2,2,4,5
>>
>> Any help would be greatly appreciated...

>
> Looks fine to me. Why do you think the syntax is incorrect? Works for
> me in
> 1.8 on Windows.
>
> > a <- 1
> > a
>
> [1] 1
>
> > a <- c(a,2)
> > a
>
> [1] 1 2
>
> > a <- c(a,2,4,5)
> > a
>
> [1] 1 2 2 4 5
>
>
> steve

Entirely my fault... I forgot about the function scope rules and needed
to use:

        vector <<- stuff to add 

instead of: 

        vector <- stuff to add

Thanks for the help...

Marko
-- 
Mark O. Kimball
Gasparinilab, University of Buffalo  |  Low temp physics
mok2 at physics.buffalo.edu  |  URL: enthalpy.physics.buffalo.edu
lab phone: 716-645-2017x122  Fax: 716-645-2507



From ray at mcs.vuw.ac.nz  Sat Apr  3 01:09:40 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Sat, 3 Apr 2004 11:09:40 +1200 (NZST)
Subject: [R] R on Tru64 OSF 5.1
Message-ID: <200404022309.i32N9e94021242@tahi.mcs.vuw.ac.nz>

> From: =?iso-8859-1?q?J=20Swinton?= <js229 at yahoo.com>
> 
> Has anyone achieved a current successful build of R
> 1.8 or 1.9 or earlier for HPUX Tru 64 OSF 5.1?
> 
> There is no binary version of R for HPUX Tru 64 OSF
> 5.1. The R admin manual mentions that the native make
> fails on "Alpha/OSF (aka Tru 64)" and gnu make must be
> used instead. There are problems reported with
> building R versions around 1.4 from 2002 on R-devel,
> some of which were fixed with using gmake instead of
> make, some of which were fixed by changes to R
> sources, but it is unclear if current R compiles on
> the current OSF 5.1, which is what I need to know
> before trying to get our local IS admins to try again
> after their first three failures.
> 
> Jonathan
> 
It depends what you mean by "successful build".  I succeeded in getting
R-1.8.1 to "./configure" and "make" on a system:
Compaq Tru64 UNIX V5.1A (Rev. 1885); Wed Nov 20 00:37:24 NZDT 2002

However it does not pass  "make check", with the following message:
496306:/vol/R/src/osf1/R-1.8.1/bin/R.bin: /sbin/loader: Fatal Error: call to unresolved symbol from /vol/R/src/osf1/R-1.8.1/modules/lapack.so (pc=0x3ffbfde3308)

during reg-tests-1.

This is possibly a consequence of the following (Info) warnings issued
at make time:

f77 -fpe3   -g -c dlapack0.f -o dlapack0.lo
f90: Info: dlapack0.f, line 1721: This directive is not supported in this platform.
CDEC$          NOVECTOR
---------------^
f77 -fpe3   -g -c dlapack1.f -o dlapack1.lo
f77 -fpe3   -g -c dlapack2.f -o dlapack2.lo
f77 -fpe3   -g -c dlapack3.f -o dlapack3.lo
f90: Info: dlapack3.f, line 21481: This directive is not supported in this platform.
CDEC$          NOVECTOR
---------------^
f90: Info: dlapack3.f, line 21495: This directive is not supported in this platform.
CDEC$             NOVECTOR
------------------^
f90: Info: dlapack3.f, line 21522: This directive is not supported in this platform.
CDEC$          NOVECTOR
---------------^

The weird thing is that the problem occurs during the following code
fragment:

## predict.arima0 needed a matrix newxreg: Roger Koenker, 2001-09-27
u <- rnorm(120)
s <- 1:120
y <- 0.3*s + 5*filter(u, c(.95,-.1), "recursive", init=rnorm(2))
fit0 <- arima0(y,order=c(2,0,0), xreg=s)

which runs perfectly OK if pasted into an interactive session.

I did spend some time trying to isolate/reproduce the problem without
success.  Unfortunately, this is the only Alpha/OSF1 system we have and
it is not used much (particularly not for R), so the installation has
not been well tested for 'real use'.

Let me know if you want the necessary configuration changes.

Hope this helps,
Ray Brownrigg <ray at mcs.vuw.ac.nz>	http://www.mcs.vuw.ac.nz/~ray



From al_helu at yahoo.com  Sat Apr  3 02:32:57 2004
From: al_helu at yahoo.com (Amal Helu)
Date: Fri, 2 Apr 2004 16:32:57 -0800 (PST)
Subject: [R] Sas code
Message-ID: <20040403003257.95533.qmail@web40210.mail.yahoo.com>

Hi

this is my first time and i have no clue how this work
do you replay to my e-mail
or do u post the asnswer some other place 
please let me know
the other question do u answer questions or give help 
for questions written in SAS
for example i need the help to simulate from Bivariate
normal??
so how i am suppose to do that
thanks
amal

__________________________________


From gblevins at mn.rr.com  Sat Apr  3 05:03:24 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Fri, 2 Apr 2004 21:03:24 -0600
Subject: [R] Seeking help for outomating regression (over columns) and
	storing selected output
Message-ID: <01c201c41928$3bdea610$73cc5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040402/e0fa6796/attachment.pl

From tblackw at umich.edu  Sat Apr  3 06:12:30 2004
From: tblackw at umich.edu (Tom Blackwell)
Date: Fri, 2 Apr 2004 23:12:30 -0500 (EST)
Subject: [R] Sas code
In-Reply-To: <20040403003257.95533.qmail@web40210.mail.yahoo.com>
References: <20040403003257.95533.qmail@web40210.mail.yahoo.com>
Message-ID: <Pine.SOL.4.58.0404022310030.25940@rygar.gpcc.itd.umich.edu>

Amal  -

Do:

library("MASS")
help("mvrnorm")

for documentation on the function you should use.

I found this,  even without knowing the function name exactly,
by doing   help.search("mvnorm").   Please read the posting
guide and the FAQ.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 2 Apr 2004, Amal Helu wrote:

> Hi
>
> this is my first time and i have no clue how this work
> do you replay to my e-mail
> or do u post the asnswer some other place
> please let me know
> the other question do u answer questions or give help
> for questions written in SAS
> for example i need the help to simulate from Bivariate
> normal??
> so how i am suppose to do that
> thanks
> amal
>
> __________________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tpapp at axelero.hu  Sat Apr  3 11:07:38 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sat, 3 Apr 2004 11:07:38 +0200
Subject: [R] a fix for rotated PDF graphs
Message-ID: <20040403090738.GA2158@localhost>

Hi,

I have found references for the following problem in the list
archives, but no nice solution.  So I decided to post one I came up
with.

The problem is that graphs output as eps files, for example using

ps.options(onefile=FALSE, paper="special", width=8, height=8,
           horizontal=FALSE, pointsize=12)

get rotated when I convert them to pdf using epstopdf.  Both ghostview
and acroread display them rotated, so I don't think it's an actual
acroread bug.  I am using R 1.8.1.

The solution is to "distill" the files using eps2eps, part of
ghostscript on my Debian box.  Here is an example Makefile I use for
this:

---- cut here ----
EPS2EPS=eps2eps
EPSTOPDF=epstopdf

all: graph1.pdf graph2.pdf

%.pdf: %.raweps
        $(EPS2EPS) $< $*.eps
        $(EPSTOPDF) $*.eps
---- cut here ----

So I have a fix now, and it works.  But I still think that either the
postscript() function in R, or both acroread and gs are broken.  I know
no ps/pdf, so I can't decide which, but in the long run, somebody with
some ps/pdf expertise and a bit of time could look at this problem.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From mike.campana at freesurf.ch  Sat Apr  3 12:31:37 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Sat,  3 Apr 2004 11:31:37 +0100
Subject: [R] plot & easy question
Message-ID: <1080984697.webexpressdV3.1.f@smtp.freesurf.ch>

Dear all

I guess I have a quite simple question
I do not have any problem to plot the data (daily averages)in different 
plots as monthly time series (for example with a for -loop over the 
different months). My problem begin when I want to represent always as a 
time serie 1999 data (from june) and 2000 data (until june) in a single 
plot(always as daily averages). Actually I would like to see the winter 
data in the middle of the plot.

Do you have an advice? Thank you for your answer
Mike

> data

	Datum Bod Loc  Mag Chi  Lug  Mil  Com  day month year Smog
1  19990601  NA  NA 27.6  33 30.8 74.1 53.3      1    6 1999    1
2  19990602  NA  NA 21.8  49 48.9 81.9 70.2      2    6 1999    1
3  19990603  NA  NA 38.9  31 26.9 53.3 49.4      3    6 1999    1
4  19990604  NA  NA 16.8  11 11.0 23.4 19.5      4    6 1999    1
5  19990605  NA  NA  9.0  18 13.6 36.4 32.5      5    6 1999    1
6  19990606  NA  NA 12.9  37 23.2 33.8 39.0      6    6 1999    1
....
579 20000530  NA  16  12.9  16  11.3  48.1  15.6     30   12 2000   2
580 20000531  NA  18  21.0  22  12.1  66.3  39.0     31   12 2000   2 

---



From pburns at pburns.seanet.com  Sat Apr  3 11:33:57 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 03 Apr 2004 10:33:57 +0100
Subject: [R] GARCH
References: <6B5A9304046AD411BD0200508BDFB6CB02955E50@gimli.middleearth.kssg.com>
Message-ID: <406E8505.90403@pburns.seanet.com>

 From a practical point of view, Carol Alexander's "Market Models"
might be about as good as it gets.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Wayne Jones wrote:

>Hi there fellow R-Users, 
>
>Can anyone recommend a good book on the theory and practice of applying
>GARCH models. 
>
>Also, does any one know of any R related subject material  in addition to
>library(tseries).
>
>Regards
>
>Wayne
>
>Dr Wayne R. Jones
>Senior Statistician / Research Analyst
>KSS Limited
>St James's Buildings
>79 Oxford Street
>Manchester M1 6SS
>Tel: +44(0)161 609 4084
>Mob: +44(0)7810 523 713
>
>
>
>
>KSS Ltd
>Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS  England
>Company Registration Number 2800886
>Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
>mailto:kssg at kssg.com		http://www.kssg.com
>
>
>The information in this Internet email is confidential and m...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From kari23000 at yahoo.fr  Sat Apr  3 11:41:47 2004
From: kari23000 at yahoo.fr (=?iso-8859-1?q?karima=20alem?=)
Date: Sat, 3 Apr 2004 11:41:47 +0200 (CEST)
Subject: [R] hellow
Message-ID: <20040403094147.87253.qmail@web40908.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040403/ed96cbb4/attachment.pl

From pburns at pburns.seanet.com  Sat Apr  3 11:58:15 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 03 Apr 2004 10:58:15 +0100
Subject: [R] Extending a vector
References: <200404021627.15131.mok2@physics.buffalo.edu>
Message-ID: <406E8AB7.4010102@pburns.seanet.com>

There is a problem with this scheme that has nothing to do with
syntax.  Extending objects dynamically like this is probably the
surest way to run out of memory.  If you know the final length an
object will be, then it is best to create the object at its full length
and then subscript into it with assignments.  

In this application you apparently don't know what the final length
will be.  In this case you can create an object of some length,
subscript into it with assignments (keeping track of how much has
gone in), then grow the object again by some chunk when more
room is needed.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Mark O. Kimball wrote:

>I believe this should be an easy thing to do...
>
>I have a function I repeatably call which takes input parameters and
>outputs columns to various data frames. I also wish to keep a summary of
>certain values as I call the function. I though keeping the values in a
>vector then appending the vector by the new amounts would be the way to
>do this.
>
>Example: (this is what I want even thought the below syntax is wrong)
>
>a <- 1
>print(a) --> 1
>
>a <- c(a,2)
>print(a) --> 1,2
>
>a <- c(a,2,4,5)
>print(a) --> 1,2,2,4,5
>
>Any help would be greatly appreciated...
>
>Marko
>  
>



From seanpor at acm.org  Sat Apr  3 11:59:33 2004
From: seanpor at acm.org (Sean O'Riordain)
Date: Sat, 03 Apr 2004 10:59:33 +0100
Subject: [R] hellow
In-Reply-To: <20040403094147.87253.qmail@web40908.mail.yahoo.com>
References: <20040403094147.87253.qmail@web40908.mail.yahoo.com>
Message-ID: <406E8B05.3020507@acm.org>

Hi,

I suggest that you read the 'posting guide' - refer the bottom of all 
emails on the list.

http://www.R-project.org/posting-guide.html


(il ne faut que d'aller chez http://www.r-project.org/ vous meme).

Sean


karima alem wrote:

>can i ask you for logiciel R
>
>		
>---------------------------------
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From ripley at stats.ox.ac.uk  Sat Apr  3 12:19:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 3 Apr 2004 11:19:07 +0100 (BST)
Subject: [R] a fix for rotated PDF graphs
In-Reply-To: <20040403090738.GA2158@localhost>
Message-ID: <Pine.LNX.4.44.0404031104350.16993-100000@gannet.stats>

The problem is simply that you are using epstopdf incorrectly.  This is 
not an R issue but the correct solution has been discussed here before
(although my quick searches failed to locate it in the archives).

First, if you want `PDF graphs' you can make them in R with the pdf() 
device.  I am very surprised you did not mention that.

Second, if you insist on using distiller or gs (via epstopdf) you need to
unset the option that rotates plots automatically.  It's documented in 
ps2pdf.htm:

  By default Ghostscript determines viewing page orientation based on the 
  dominant text orientation on the page. Sometimes, when the page has text 
  in several orientations or has no text at all, wrong orientation can be 
  selected.

  Acrobat Distiller parameter AutoRotatePages controls the automatic 
  orientation selection algorithm. On Ghostscript, besides input stream, 
  Distiller parameters can be given as command line arguments. For 
  instance: -dAutoRotatePages=/None or /All or /PageByPage.

You want /None.


On Sat, 3 Apr 2004, Tamas Papp wrote:

> I have found references for the following problem in the list
> archives, but no nice solution.  So I decided to post one I came up
> with.
> 
> The problem is that graphs output as eps files, for example using
> 
> ps.options(onefile=FALSE, paper="special", width=8, height=8,
>            horizontal=FALSE, pointsize=12)
> 
> get rotated when I convert them to pdf using epstopdf.  Both ghostview
> and acroread display them rotated, so I don't think it's an actual
> acroread bug.  I am using R 1.8.1.

No, it's user error.  You asked gs to rotate them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From m.dewey at iop.kcl.ac.uk  Sat Apr  3 13:11:06 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Sat, 03 Apr 2004 12:11:06 +0100
Subject: [R] Re: R-help Digest, Vol 14, Issue 3
In-Reply-To: <200404031001.i33A0DBS004712@hypatia.math.ethz.ch>
Message-ID: <5.2.1.1.0.20040403120228.00a24960@pop.freeserve.net>

At 12:01 03/04/04 +0200, you wrote:
>Content-Transfer-Encoding: 8bit
>From: solares at unsl.edu.ar
>Precedence: list
>MIME-Version: 1.0
>Cc:
>To: R-help at stat.math.ethz.ch
>Date: Fri, 2 Apr 2004 12:47:48 -0300 (ART)
>Message-ID: <50155.209.13.250.66.1080920868.squirrel at inter17.unsl.edu.ar>
>Content-Type: text/plain;charset=iso-8859-1
>Subject: [R] convert excell file to text with RODBC package
>Message: 27
>
>Hi, i can convert excell to list in R with package RODBC ()but i don't
>understand 2 mistake
>1) Don't read the last row of the table excell
>2) Don' t take the hours

See below

>My excell file call prueba4.xls and have the following rows:
>where prueba4.xls was make in excell (office xp) and have one spreadsheet
>call "Hoja1", you see each rows of she:
>D??a     Hora    col1    col2    col3    col4    col5    col6    col7    col8
>15/12/2003      12:14:59        217     2760    8,2     35      79,6    86,4
>15/12/2003      12:15:00        217     2764    8,2     35      79,6    86,4
>15/12/2003      12:15:01        217     2758    8,3     35      79,6    86,4
>15/12/2003      12:15:02        217     2760    8,3     35      79,6    86,4
>15/12/2003      12:15:03        217     2755    8,3     35      79,6    86,4
>15/12/2003      12:15:04        217     2766    8,3     35      79,6    86,4
>15/12/2003      12:15:05        217,1   2766    8,3     35,1    79,6    86,4
>15/12/2003      12:15:06        217,1   2758    8,3     35,1    79,6    86,4
>15/12/2003      12:15:07        217,1   2768    8,3     35,1    79,6    86,4

That seems to have 9 rows of data. There seem to be either 8 columns or 10 
columns of data.

>My code (i use the R 1.7.1 for windows xp) is the following:
>
>  library(RODBC)
> > canal<-odbcConnectExcel("c:/prueba4.xls")
> > tablas<-sqlTables(canal)
> > tablas
>     TABLE_CAT TABLE_SCHEM TABLE_NAME   TABLE_TYPE REMARKS
>1 c:\\prueba4        <NA>     Hoja1$ SYSTEM TABLE    <NA>
>2 c:\\prueba4        <NA>     Hoja2$ SYSTEM TABLE    <NA>
>3 c:\\prueba4        <NA>     Hoja3$ SYSTEM TABLE    <NA>
> > tbl<-sqlFetch(canal,substr(tablas[1,3],1,nchar(tablas[1,3])-1))
> > tbl[1]
>                   D??a
>1 2003-12-15 00:00:00
>2 2003-12-15 00:00:00
>3 2003-12-15 00:00:00
>4 2003-12-15 00:00:00
>5 2003-12-15 00:00:00
>6 2003-12-15 00:00:00
>7 2003-12-15 00:00:00
>8 2003-12-15 00:00:00
>9 2003-12-15 00:00:00
>
>??Which is the mistake?.

Well that has 9 rows the same as Hoja1 in Pruebas4.xls, so I do not 
understand your question 1.

What exactly do you mean by question 2? Do you mean it is not reading the 
hours column (Hora) or do you mean it takes a long time? It looks to me as 
though column one, despite its label of day (D??a) actually contains some 
much more complex Excel format.


Michael Dewey
m.dewey at iop.kcl.ac.uk



From ggrothendieck at myway.com  Sat Apr  3 14:56:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 3 Apr 2004 12:56:19 +0000 (UTC)
Subject: [R] plot & easy question
References: <1080984697.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <loom.20040403T144924-167@post.gmane.org>


Assuming the same Y scale will work for all your data, 
the plot statement plots columns 4 through 8 with no marker at
the points so the axes get set up correctly.  Then matplot adds to it.

dd <- ISOdate( data$year, data$month, data$day )
plot( rep(dd,5), as.matrix(data[,4:8]), type="n" )
matplot( dd, data[,4:8], type="b", add=T )




 <mike.campana <at> freesurf.ch> writes:

: 
: Dear all
: 
: I guess I have a quite simple question
: I do not have any problem to plot the data (daily averages)in different 
: plots as monthly time series (for example with a for ?loop over the 
: different months). My problem begin when I want to represent always as a 
: time serie 1999 data (from june) and 2000 data (until june) in a single 
: plot(always as daily averages). Actually I would like to see the winter 
: data in the middle of the plot.
: 
: Do you have an advice? Thank you for your answer
: Mike
: 
: > data
: 
: 	Datum Bod Loc  Mag Chi  Lug  Mil  Com  day month year Smog
: 1  19990601  NA  NA 27.6  33 30.8 74.1 53.3      1    6 1999    1
: 2  19990602  NA  NA 21.8  49 48.9 81.9 70.2      2    6 1999    1
: 3  19990603  NA  NA 38.9  31 26.9 53.3 49.4      3    6 1999    1
: 4  19990604  NA  NA 16.8  11 11.0 23.4 19.5      4    6 1999    1
: 5  19990605  NA  NA  9.0  18 13.6 36.4 32.5      5    6 1999    1
: 6  19990606  NA  NA 12.9  37 23.2 33.8 39.0      6    6 1999    1
: ....
: 579 20000530  NA  16  12.9  16  11.3  48.1  15.6     30   12 2000   2
: 580 20000531  NA  18  21.0  22  12.1  66.3  39.0     31   12 2000   2 
:



From andy_liaw at merck.com  Sat Apr  3 15:21:48 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 3 Apr 2004 08:21:48 -0500
Subject: [R] Seeking help for outomating regression (over columns)
	and storing selected output
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B0F@usrymx25.merck.com>

I'm quite sure there're better ways, but this works for me:

> dat <- data.frame(y=rnorm(30), x1=runif(30), x2=runif(30), x3=runif(30),
+                   group=factor(rep(1:3, each=10)))
> 
> getCoef <- function(dat) {
+     apply(dat[,c("x1","x2","x3")], 2,
+           function(x) lm.fit(cbind(1, x), dat$y)$coefficients[2])
+ }
> clist <- by(dat[,c("y","x1","x2","x3")], dat$group, getCoef)
> cmat <- do.call("rbind", clist)
> cmat
          x1         x2         x3
1 -1.8646962  0.6182181 -1.7859563
2 -1.5031314 -1.0639626 -0.2982066
3 -0.8302013  0.8111539 -1.0372803

HTH,
Andy

> From: Greg Blevins
> 
> Hello, 
> 
> I have spent considerable time trying to figure out that 
> which I am about to describe.  This included searching Help, 
> consulting my various R books, and trail and (always) error.  
> I have been assuming I would need to use a loop (looping over 
> columns) but perhaps and apply function would do the trick.  
> I have unsuccessfully tried both.
> 
> A scaled down version of my situation is as follows:
> 
> I have a dataframe as follows:
> 
> ID       Y      x1          x2          x3           usergroup.
> 
> Y is a continous criterion, x1-x3 continous predictors, and 
> usergroup is coded a 1, 2 or 3 to indicate user status.
> 
> My end goal is a (dataframe or matrix) with just the 
> regression coef from each of 12 runs (each x regressed 
> separately on Y for the total sample and for each usergroup). 
>  I envision output as follows, a three column by four row 
> dataframe or matrix.
> 
>                                                   
>                          Y and x1;            Y and x2;       
>   Y and x3.
> Total sample:
> usergroup 1:               
> usergroup 2:               (Regression Coefs fill the matrix) 
> usergroup 3:              
> 
> Using 1.8.1
> Windows 2000 and XP
> 
> Help would be most appreciated.
> 
> Greg Blevins, Partner
> The Market Solutions Group
> 	[[alternative HTML version deleted]]
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From rbaer at atsu.edu  Sat Apr  3 15:30:02 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Sat, 3 Apr 2004 07:30:02 -0600
Subject: [R] Seeking help for outomating regression (over columns)
	andstoring selected output
References: <01c201c41928$3bdea610$73cc5e18@glblpyirxqz5lp>
Message-ID: <001501c4197f$c55a31a0$6401a8c0@meadow>

Here's one simplistic solution, perhaps there are better ones:
#  Make some test data and place in dataframe
x1=rnorm(20)
x2=rnorm(20)
x3=rnorm(20)
x4=as.factor(sample(c("G1","G2","G3"),20,replace=T))
y1=2*x1+4*x2+0.5*x3+as.numeric(x4)+rnorm(20)

df=data.frame(y1,x1,x2,x3,x4)

# Now create the ouput dataframe described
out=data.frame(result=c("Intercept",levels(df$x4)))
out$X1=as.numeric(coef(lm(df$y1~df$x1+df$x4)))
out$X2=as.numeric(coef(lm(df$y1~df$x2+df$x4)))
out$X3=as.numeric(coef(lm(df$y1~df$x3+df$x4)))

#look at it
df
out



----- Original Message ----- 
From: "Greg Blevins" <gblevins at mn.rr.com>
To: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Friday, April 02, 2004 9:03 PM
Subject: [R] Seeking help for outomating regression (over columns)
andstoring selected output


> Hello,
>
> I have spent considerable time trying to figure out that which I am about
to describe.  This included searching Help, consulting my various R books,
and trail and (always) error.  I have been assuming I would need to use a
loop (looping over columns) but perhaps and apply function would do the
trick.  I have unsuccessfully tried both.
>
> A scaled down version of my situation is as follows:
>
> I have a dataframe as follows:
>
> ID       Y      x1          x2          x3           usergroup.
>
> Y is a continous criterion, x1-x3 continous predictors, and usergroup is
coded a 1, 2 or 3 to indicate user status.
>
> My end goal is a (dataframe or matrix) with just the regression coef from
each of 12 runs (each x regressed separately on Y for the total sample and
for each usergroup).  I envision output as follows, a three column by four
row dataframe or matrix.
>
>
>                          Y and x1;            Y and x2;         Y and x3.
> Total sample:
> usergroup 1:
> usergroup 2:               (Regression Coefs fill the matrix)
> usergroup 3:
>
> Using 1.8.1
> Windows 2000 and XP
>
> Help would be most appreciated.
>
> Greg Blevins, Partner
> The Market Solutions Group
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Sat Apr  3 18:45:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 3 Apr 2004 16:45:11 +0000 (UTC)
Subject: [R] Seeking help for outomating regression (over columns)
	=?utf-8?b?YW5kCXN0b3Jpbmc=?= selected output
References: <01c201c41928$3bdea610$73cc5e18@glblpyirxqz5lp>
Message-ID: <loom.20040403T183150-812@post.gmane.org>



Note that there is a QUESTION at the end regarding
random effects.

Suppose your data frame is df and has components 
y, x1, x2, x3 and u where u is a factor.  

1. There was a problem posted about doing repeated regressions 
(search for Operating on windows of data) last month that 
has similarities to this one.  

Making use of those ideas, the first sapply below loops 
over the y~xi regressions and the next two loop over 
the usergroup specific regressions.  We just rbind 
them altogether:

xvars <- c("x1", "x2", "x3")
rbind(
   sapply( xvars, function(xi) coef( lm(y ~ df[,xi], data=df))[[2]] ), 
   sapply( xvars, function(xi)
        sapply( levels(df$u), function(ulev)
		coef(lm(y ~ df[,xi], subset=u==ulev, data=df))[[2]]
	)
   )
)


2. Another possibility is to create a giant regression that does 
all the usergroup specific regressions at once and then repeat 
it without the usergroup variable to get the rest.  

df2 is a new data frame that strings out all the x variables into 
a single long column and adds a new factor i that identifies
which x variable it is.  y and u are repeated three times to bring 
them into line with x.  (

xvars <- c("x1", "x2", "x3")
xm <- as.matrix(df[,xvars])
df2 <- data.frame(y=rep(df$y,3), x = c(xm), i=factor(c(col(xm))), u=rep(u,3))

# We could have alternately used reshape like this:
# df2 <-  reshape(df,timevar="i",times=factor(1:3),
#                varying=list(xvars),direction="long",v.name="x")

# The slopes by usergroup and across user group are:

coeff.u <- coef(lm(y ~ i/u/x, data=df2))
coeff.all <- coef(lm(y ~ i/x, data=df2))

# Pick off the slopes (they are at the end of each coef vector) and reform:

z <- matrix( c( matrix( coef.all, nc=2)[,2], matrix( coef.u, nc=2)[,2] ), nc=3)
colnames(z) <- xvars
rownames(z) <- c("All", levels(df$u))

3. Note that the giant regression approach works as long as you are only
interested in the coefficients, however, if you were interested in the
variances then this would not work since each of the two regressions uses a
pooled estimate of variance.

QUESTION:  As a matter of interest, would someone that is familiar with random
effects models show what the corresponding giant model is with separate
variances for each regression.


P.S. I tried the above out on the following which is similar
to the original problem except there are 4 levels in u:

data(state)
x <- state.x77[,1:3]
u <- state.region
y <- state.x77[,4]
df <- data.frame(y=y, x1=x[,1], x2=x[,2], x3=x[,3], u=factor(u))



Greg Blevins <gblevins <at> mn.rr.com> writes:

: 
: Hello, 
: 
: I have spent considerable time trying to figure out that which I am about to 
describe.  This included
: searching Help, consulting my various R books, and trail and (always) 
error.  I have been assuming I would
: need to use a loop (looping over columns) but perhaps and apply function 
would do the trick.  I have
: unsuccessfully tried both.
: 
: A scaled down version of my situation is as follows:
: 
: I have a dataframe as follows:
: 
: ID       Y      x1          x2          x3           usergroup.
: 
: Y is a continous criterion, x1-x3 continous predictors, and usergroup is 
coded a 1, 2 or 3 to indicate user status.
: 
: My end goal is a (dataframe or matrix) with just the regression coef from 
each of 12 runs (each x regressed
: separately on Y for the total sample and for each usergroup).  I envision 
output as follows, a three column
: by four row dataframe or matrix.
: 
:                          Y and x1;            Y and x2;         Y and x3.
: Total sample:
: usergroup 1:               
: usergroup 2:               (Regression Coefs fill the matrix) 
: usergroup 3:              
: 
: Using 1.8.1
: Windows 2000 and XP
: 
: Help would be most appreciated.
: 
: Greg Blevins, Partner
: The Market Solutions Group
: 	[[alternative HTML version deleted]]



From rpeng at jhsph.edu  Sat Apr  3 19:47:32 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 03 Apr 2004 12:47:32 -0500
Subject: [R] a fix for rotated PDF graphs
In-Reply-To: <20040403090738.GA2158@localhost>
References: <20040403090738.GA2158@localhost>
Message-ID: <406EF8B4.7010609@jhsph.edu>

The simplest solution for me is to set

setenv GS_OPTIONS "-dAutoRotatePages=/None"

There was a discussion on this topic a in October, but it was on 
R-devel.  See here

https://www.stat.math.ethz.ch/pipermail/r-devel/2003-October/027759.html

-roger

Tamas Papp wrote:
> Hi,
> 
> I have found references for the following problem in the list
> archives, but no nice solution.  So I decided to post one I came up
> with.
> 
> The problem is that graphs output as eps files, for example using
> 
> ps.options(onefile=FALSE, paper="special", width=8, height=8,
>            horizontal=FALSE, pointsize=12)
> 
> get rotated when I convert them to pdf using epstopdf.  Both ghostview
> and acroread display them rotated, so I don't think it's an actual
> acroread bug.  I am using R 1.8.1.
> 
> The solution is to "distill" the files using eps2eps, part of
> ghostscript on my Debian box.  Here is an example Makefile I use for
> this:
> 
> ---- cut here ----
> EPS2EPS=eps2eps
> EPSTOPDF=epstopdf
> 
> all: graph1.pdf graph2.pdf
> 
> %.pdf: %.raweps
>         $(EPS2EPS) $< $*.eps
>         $(EPSTOPDF) $*.eps
> ---- cut here ----
> 
> So I have a fix now, and it works.  But I still think that either the
> postscript() function in R, or both acroread and gs are broken.  I know
> no ps/pdf, so I can't decide which, but in the long run, somebody with
> some ps/pdf expertise and a bit of time could look at this problem.
> 
> Best,
> 
> Tamas
>



From pallier at lscp.ehess.fr  Sat Apr  3 20:02:05 2004
From: pallier at lscp.ehess.fr (pallier)
Date: Sat, 03 Apr 2004 20:02:05 +0200
Subject: [R] Doing SQL GROUP BY in R
In-Reply-To: <D53147E531BFBC4B8853FD134FAEE44D14FDE9@exusfr014.novo.dk>
References: <D53147E531BFBC4B8853FD134FAEE44D14FDE9@exusfr014.novo.dk>
Message-ID: <406EFC1D.5020209@lscp.ehess.fr>

JFRI (Jesper Frickmann) wrote:

>I want a list of the number of times some factor levels appear together,
>  
>

Say a and b are the names of the factors:

table(a,b)

table(a:b)

aggregate(a,list(a=a,b=b),length)

Christophe Pallier
http://www.pallier.org



From tlumley at u.washington.edu  Sat Apr  3 20:34:30 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 3 Apr 2004 10:34:30 -0800 (PST)
Subject: [R] Seeking help for outomating regression (over columns)
	=?utf-8?b?YW5kCXN0b3Jpbmc=?= selected output
In-Reply-To: <loom.20040403T183150-812@post.gmane.org>
References: <01c201c41928$3bdea610$73cc5e18@glblpyirxqz5lp>
	<loom.20040403T183150-812@post.gmane.org>
Message-ID: <Pine.A41.4.58.0404031013070.162934@homer05.u.washington.edu>

On Sat, 3 Apr 2004, Gabor Grothendieck wrote:
>
> 2. Another possibility is to create a giant regression that does
> all the usergroup specific regressions at once and then repeat
> it without the usergroup variable to get the rest.
>
> df2 is a new data frame that strings out all the x variables into
> a single long column and adds a new factor i that identifies
> which x variable it is.  y and u are repeated three times to bring
> them into line with x.  (
>

<snip>

>
> 3. Note that the giant regression approach works as long as you are only
> interested in the coefficients, however, if you were interested in the
> variances then this would not work since each of the two regressions uses a
> pooled estimate of variance.
>
> QUESTION:  As a matter of interest, would someone that is familiar with random
> effects models show what the corresponding giant model is with separate
> variances for each regression.

There are actually two answers to this.  The first is that if you use the
White/Huber robust/sandwich/model-agnostic variances you get the right
variances automatically.  This is useful when you what to compare
coefficients across models.

On the other hand, I don't think you can get the answer you are looking
for.  The problem is that the giant regression estimates are not MLEs for
anything, and so I think you can't get lme() to simultaneously get the
right coefficients and the right variances.

	-thomas



From yzhou at sdsc.edu  Sat Apr  3 21:31:52 2004
From: yzhou at sdsc.edu (Yi-Xiong Sean Zhou)
Date: Sat, 3 Apr 2004 11:31:52 -0800
Subject: [R] memory limit problem
Message-ID: <000701c419b2$54c0efa0$d60da8c0@D3BC5441>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040403/6826693d/attachment.pl

From ligges at statistik.uni-dortmund.de  Sat Apr  3 22:39:59 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 03 Apr 2004 22:39:59 +0200
Subject: [R] memory limit problem
References: <000701c419b2$54c0efa0$d60da8c0@D3BC5441>
Message-ID: <406F211F.EB8F5CED@statistik.uni-dortmund.de>



Yi-Xiong Sean Zhou wrote:
> 
> Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
> laptop running XP professional with 2G memory?

See ?Memory or the the R for Windows FAQ, which tells you:

"2.7 There seems to be a limit on the memory it uses!

Indeed there is. It is set by the command-line flag --max-mem-size (see
How do I install R for Windows?) and defaults to the smaller of the
amount of physical
RAM in the machine and 1Gb. [...]"


> I have tried
> 
> "C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
> 
> but I only get only 500MB for R actually.
> 
>
> I also tried memory.limit(2^30) in R and got error of:

Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
do you? 


Either use the command line flag --max-mem-size=1500M or within R:
 memory.limit(1500)

 
> 
> Error in memory.size(size) : cannot decrease memory limit

Since your limit was roughly 10^6-times off the right one, you got an
integer overflow internally, I think.

Uwe Ligges


 
> Yi-Xiong
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From yzhou at sdsc.edu  Sat Apr  3 23:49:00 2004
From: yzhou at sdsc.edu (Yi-Xiong Sean Zhou)
Date: Sat, 3 Apr 2004 13:49:00 -0800
Subject: [R] memory limit problem
In-Reply-To: <406F211F.EB8F5CED@statistik.uni-dortmund.de>
Message-ID: <001201c419c5$7cf3f130$d60da8c0@D3BC5441>

After memory.limit(1500), the error message still pop out:

Error: cannot allocate vector of size 11529 Kb

While 

> memory.size()
[1] 307446696
> memory.limit()
[1] 1572864000

And the system is only using 723MB physical memory, while 2G is the total. 

Does anyone have a clue of what is going on? 


Yi-Xiong


-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Saturday, April 03, 2004 12:40 PM
To: Yi-Xiong Sean Zhou
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] memory limit problem



Yi-Xiong Sean Zhou wrote:
> 
> Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
> laptop running XP professional with 2G memory?

See ?Memory or the the R for Windows FAQ, which tells you:

"2.7 There seems to be a limit on the memory it uses!

Indeed there is. It is set by the command-line flag --max-mem-size (see
How do I install R for Windows?) and defaults to the smaller of the
amount of physical
RAM in the machine and 1Gb. [...]"


> I have tried
> 
> "C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
> 
> but I only get only 500MB for R actually.
> 
>
> I also tried memory.limit(2^30) in R and got error of:

Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
do you? 


Either use the command line flag --max-mem-size=1500M or within R:
 memory.limit(1500)

 
> 
> Error in memory.size(size) : cannot decrease memory limit

Since your limit was roughly 10^6-times off the right one, you got an
integer overflow internally, I think.

Uwe Ligges


 
> Yi-Xiong
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jmacdon at med.umich.edu  Sun Apr  4 00:37:33 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Sat, 03 Apr 2004 17:37:33 -0500
Subject: [R] memory limit problem
Message-ID: <s06ef665.005@med-gwia-01a.med.umich.edu>

If you check the max memory used, I bet it is the same as your
memory.limit. Try

memory.size(max=TRUE)

to see how much memory was allocated. You also might try
--max-mem-size=2000M. R will not actually be able to get all 2 Gb of
ram, but I think it will be more than 1.5 Gb you are allowing now.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Yi-Xiong Sean Zhou" <yzhou at sdsc.edu> 04/03/04 04:49PM >>>
After memory.limit(1500), the error message still pop out:

Error: cannot allocate vector of size 11529 Kb

While 

> memory.size()
[1] 307446696
> memory.limit()
[1] 1572864000

And the system is only using 723MB physical memory, while 2G is the
total. 

Does anyone have a clue of what is going on? 


Yi-Xiong


-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Saturday, April 03, 2004 12:40 PM
To: Yi-Xiong Sean Zhou
Cc: r-help at stat.math.ethz.ch 
Subject: Re: [R] memory limit problem



Yi-Xiong Sean Zhou wrote:
> 
> Could anyone advise me how to allocate 1.5Gbyte memory for R on a
Dell
> laptop running XP professional with 2G memory?

See ?Memory or the the R for Windows FAQ, which tells you:

"2.7 There seems to be a limit on the memory it uses!

Indeed there is. It is set by the command-line flag --max-mem-size
(see
How do I install R for Windows?) and defaults to the smaller of the
amount of physical
RAM in the machine and 1Gb. [...]"


> I have tried
> 
> "C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
> 
> but I only get only 500MB for R actually.
> 
>
> I also tried memory.limit(2^30) in R and got error of:

Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
do you? 


Either use the command line flag --max-mem-size=1500M or within R:
 memory.limit(1500)

 
> 
> Error in memory.size(size) : cannot decrease memory limit

Since your limit was roughly 10^6-times off the right one, you got an
integer overflow internally, I think.

Uwe Ligges


 
> Yi-Xiong
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Sun Apr  4 00:51:50 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 03 Apr 2004 17:51:50 -0500
Subject: [R] memory limit problem
In-Reply-To: <001201c419c5$7cf3f130$d60da8c0@D3BC5441>
References: <001201c419c5$7cf3f130$d60da8c0@D3BC5441>
Message-ID: <406F4006.2050901@jhsph.edu>

You may want to try downloading the development version of R at 
http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This 
version deals with Windows' deficiencies in memory management a 
little better.

-roger

Yi-Xiong Sean Zhou wrote:

> After memory.limit(1500), the error message still pop out:
> 
> Error: cannot allocate vector of size 11529 Kb
> 
> While 
> 
> 
>>memory.size()
> 
> [1] 307446696
> 
>>memory.limit()
> 
> [1] 1572864000
> 
> And the system is only using 723MB physical memory, while 2G is the total. 
> 
> Does anyone have a clue of what is going on? 
> 
> 
> Yi-Xiong
> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Saturday, April 03, 2004 12:40 PM
> To: Yi-Xiong Sean Zhou
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] memory limit problem
> 
> 
> 
> Yi-Xiong Sean Zhou wrote:
> 
>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
>>laptop running XP professional with 2G memory?
> 
> 
> See ?Memory or the the R for Windows FAQ, which tells you:
> 
> "2.7 There seems to be a limit on the memory it uses!
> 
> Indeed there is. It is set by the command-line flag --max-mem-size (see
> How do I install R for Windows?) and defaults to the smaller of the
> amount of physical
> RAM in the machine and 1Gb. [...]"
> 
> 
> 
>>I have tried
>>
>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
>>
>>but I only get only 500MB for R actually.
>>
>>
>>I also tried memory.limit(2^30) in R and got error of:
> 
> 
> Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
> do you? 
> 
> 
> Either use the command line flag --max-mem-size=1500M or within R:
>  memory.limit(1500)
> 
>  
> 
>>Error in memory.size(size) : cannot decrease memory limit
> 
> 
> Since your limit was roughly 10^6-times off the right one, you got an
> integer overflow internally, I think.
> 
> Uwe Ligges
> 
> 
>  
> 
>>Yi-Xiong
>>
>>        [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From yzhou at sdsc.edu  Sun Apr  4 04:50:57 2004
From: yzhou at sdsc.edu (Yi-Xiong Sean Zhou)
Date: Sat, 3 Apr 2004 18:50:57 -0800
Subject: [R] memory limit problem
In-Reply-To: <406F4006.2050901@jhsph.edu>
Message-ID: <000001c419ef$acdbf170$2b0ea8c0@D3BC5441>

R1.9.0beta solves the problem for now. The memory foot print of R1.9.0 is
way smaller than R1.8.1, with only 400M. It will be interesting to see how
R1.9.0 handles the memory problem when it needs more than 700M.

Thanks for your helps. 

Yi-Xiong

-----Original Message-----
From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
Sent: Saturday, April 03, 2004 2:52 PM
To: Yi-Xiong Sean Zhou
Cc: 'Uwe Ligges'; r-help at stat.math.ethz.ch
Subject: Re: [R] memory limit problem

You may want to try downloading the development version of R at 
http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This 
version deals with Windows' deficiencies in memory management a 
little better.

-roger

Yi-Xiong Sean Zhou wrote:

> After memory.limit(1500), the error message still pop out:
> 
> Error: cannot allocate vector of size 11529 Kb
> 
> While 
> 
> 
>>memory.size()
> 
> [1] 307446696
> 
>>memory.limit()
> 
> [1] 1572864000
> 
> And the system is only using 723MB physical memory, while 2G is the total.

> 
> Does anyone have a clue of what is going on? 
> 
> 
> Yi-Xiong
> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: Saturday, April 03, 2004 12:40 PM
> To: Yi-Xiong Sean Zhou
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] memory limit problem
> 
> 
> 
> Yi-Xiong Sean Zhou wrote:
> 
>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
>>laptop running XP professional with 2G memory?
> 
> 
> See ?Memory or the the R for Windows FAQ, which tells you:
> 
> "2.7 There seems to be a limit on the memory it uses!
> 
> Indeed there is. It is set by the command-line flag --max-mem-size (see
> How do I install R for Windows?) and defaults to the smaller of the
> amount of physical
> RAM in the machine and 1Gb. [...]"
> 
> 
> 
>>I have tried
>>
>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
>>
>>but I only get only 500MB for R actually.
>>
>>
>>I also tried memory.limit(2^30) in R and got error of:
> 
> 
> Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
> do you? 
> 
> 
> Either use the command line flag --max-mem-size=1500M or within R:
>  memory.limit(1500)
> 
>  
> 
>>Error in memory.size(size) : cannot decrease memory limit
> 
> 
> Since your limit was roughly 10^6-times off the right one, you got an
> integer overflow internally, I think.
> 
> Uwe Ligges
> 
> 
>  
> 
>>Yi-Xiong
>>
>>        [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Sun Apr  4 07:13:46 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 04 Apr 2004 00:13:46 -0500
Subject: [R] memory limit problem
In-Reply-To: <000001c419ef$acdbf170$2b0ea8c0@D3BC5441>
References: <000001c419ef$acdbf170$2b0ea8c0@D3BC5441>
Message-ID: <406F998A.6050905@jhsph.edu>

In general, this is not an R problem, it is a Windows problem.  I find 
that these types of memory problems do not appear on Linux, for example.

-roger

Yi-Xiong Sean Zhou wrote:
> R1.9.0beta solves the problem for now. The memory foot print of R1.9.0 is
> way smaller than R1.8.1, with only 400M. It will be interesting to see how
> R1.9.0 handles the memory problem when it needs more than 700M.
> 
> Thanks for your helps. 
> 
> Yi-Xiong
> 
> -----Original Message-----
> From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
> Sent: Saturday, April 03, 2004 2:52 PM
> To: Yi-Xiong Sean Zhou
> Cc: 'Uwe Ligges'; r-help at stat.math.ethz.ch
> Subject: Re: [R] memory limit problem
> 
> You may want to try downloading the development version of R at 
> http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This 
> version deals with Windows' deficiencies in memory management a 
> little better.
> 
> -roger
> 
> Yi-Xiong Sean Zhou wrote:
> 
> 
>>After memory.limit(1500), the error message still pop out:
>>
>>Error: cannot allocate vector of size 11529 Kb
>>
>>While 
>>
>>
>>
>>>memory.size()
>>
>>[1] 307446696
>>
>>
>>>memory.limit()
>>
>>[1] 1572864000
>>
>>And the system is only using 723MB physical memory, while 2G is the total.
> 
> 
>>Does anyone have a clue of what is going on? 
>>
>>
>>Yi-Xiong
>>
>>
>>-----Original Message-----
>>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
>>Sent: Saturday, April 03, 2004 12:40 PM
>>To: Yi-Xiong Sean Zhou
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] memory limit problem
>>
>>
>>
>>Yi-Xiong Sean Zhou wrote:
>>
>>
>>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
>>>laptop running XP professional with 2G memory?
>>
>>
>>See ?Memory or the the R for Windows FAQ, which tells you:
>>
>>"2.7 There seems to be a limit on the memory it uses!
>>
>>Indeed there is. It is set by the command-line flag --max-mem-size (see
>>How do I install R for Windows?) and defaults to the smaller of the
>>amount of physical
>>RAM in the machine and 1Gb. [...]"
>>
>>
>>
>>
>>>I have tried
>>>
>>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
>>>
>>>but I only get only 500MB for R actually.
>>>
>>>
>>>I also tried memory.limit(2^30) in R and got error of:
>>
>>
>>Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
>>do you? 
>>
>>
>>Either use the command line flag --max-mem-size=1500M or within R:
>> memory.limit(1500)
>>
>> 
>>
>>
>>>Error in memory.size(size) : cannot decrease memory limit
>>
>>
>>Since your limit was roughly 10^6-times off the right one, you got an
>>integer overflow internally, I think.
>>
>>Uwe Ligges
>>
>>
>> 
>>
>>
>>>Yi-Xiong
>>>
>>>       [[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
> 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dsheuman at rogers.com  Sun Apr  4 08:43:22 2004
From: dsheuman at rogers.com (Danny Heuman)
Date: Sun, 04 Apr 2004 01:43:22 -0500
Subject: [R] How to improve this code?
Message-ID: <k3bv60plke2slbevtj6vu36oshivhpv748@4ax.com>

Hi all,

I've got some functioning code that I've literally taken hours to
write.  My 'R' coding is getting better...it used to take days :)

I know I've done a poor job of optimizing the code.  In addition, I'm
missing an important step and don't know where to put it.

So, three questions:

1)  I'd like the resulting output to be sorted on distance (ascending)
and to have the 'rank' column represent the sort order, so that rank 1
is the first customer and rank 10 is the 10th.  Where do I do this?

2)  Can someone suggest ways of 'optimizing' or improving the code?
It's the only way I'm going to learn better ways of approaching R.

3)  If there are no customers in the store's Trade Area, I'd like the
output file have nothing written to it .  How can I do that?

All help is appreciated.

Thanks,

Danny


*********************************************************
library(fields)

#Format of input files:  ID, LONGITUDE, LATITUDE

#Generate Store List
storelist <- cbind(1:100, matrix(rnorm(100, mean = -60,  sd = 3), ncol
= 1),
	     matrix(rnorm(100, mean = 50, sd = 3), ncol = 1))

#Generate Customer List
customerlist <- cbind(1:10000,matrix(rnorm(10000, mean = -60,  sd =
20), ncol = 1),
	     matrix(rnorm(10000, mean = 50, sd = 10), ncol = 1))


#Output file
outfile <- "c:\\output.txt"
outfilecolnames <- c("rank","storeid","custid","distance")
write.table(t(outfilecolnames), file = outfile, append=TRUE,
sep=",",row.names=FALSE, col.names=FALSE)

#Trade Area Size
TAsize <- c(100)

custlatlon <- customerlist[, 2:3]

for(i in 1:length(TAsize)){
	for(j in 1:nrow(storelist)){
		cat("Store: ", storelist[j],"  TA Size = ", TAsize[i],
"\n")
		
		storelatlon <- storelist[j, 2:3]
		
		whichval <-
which(rdist.earth(t(as.matrix(storelatlon)), as.matrix(custlatlon),
miles=F) <= TAsize[i])

		dist <-
as.data.frame(rdist.earth(t(as.matrix(storelatlon)),
as.matrix(custlatlon), miles=F)[whichval])

		storetag <-
as.data.frame(cbind(1:nrow(dist),storelist[j,1]))
		fincalc <-
as.data.frame(cbind(1:nrow(dist),(customerlist[whichval,1]),rdist.earth(t(as.matrix(storelatlon)),
as.matrix(custlatlon), miles=F)[whichval]))

		combinedata <- data.frame(storetag, fincalc)

		combinefinal <- subset(combinedata, select= c(-1,-3))
		
		flush.console()
	
		write.table(combinefinal, file = outfile, append=TRUE,
sep=",", col.names=FALSE)
	}
	
}



From ripley at stats.ox.ac.uk  Sun Apr  4 09:20:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Apr 2004 08:20:25 +0100 (BST)
Subject: [R] memory limit problem
In-Reply-To: <406F998A.6050905@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0404040813010.24326-100000@gannet.stats>

That is true, but I don't see that Yi-Xiong Sean Zhou has actually yet 
followed the instructions for 1.8.1, which is to set --max-mem-size on the 
command line (and this is in the rw-FAQ as people have pointed out).

The issue is that on Windows the memory address space can get fragmented, 
and this is ameliorated by reserving memory in advance -- that is what 
using --max-mem-size (and not memory.limit) does for you.

When used as recommended, both 1.8.1 and 1.9.0beta can handle workspaces
of up to about 1.7Gb.  1.9.0 can go higher on suitable OSes: see its
rw-FAQ.

On Sun, 4 Apr 2004, Roger D. Peng wrote:

> In general, this is not an R problem, it is a Windows problem.  I find 
> that these types of memory problems do not appear on Linux, for example.
> 
> -roger
> 
> Yi-Xiong Sean Zhou wrote:
> > R1.9.0beta solves the problem for now. The memory foot print of R1.9.0 is
> > way smaller than R1.8.1, with only 400M. It will be interesting to see how
> > R1.9.0 handles the memory problem when it needs more than 700M.
> > 
> > Thanks for your helps. 
> > 
> > Yi-Xiong
> > 
> > -----Original Message-----
> > From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
> > Sent: Saturday, April 03, 2004 2:52 PM
> > To: Yi-Xiong Sean Zhou
> > Cc: 'Uwe Ligges'; r-help at stat.math.ethz.ch
> > Subject: Re: [R] memory limit problem
> > 
> > You may want to try downloading the development version of R at 
> > http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This 
> > version deals with Windows' deficiencies in memory management a 
> > little better.
> > 
> > -roger
> > 
> > Yi-Xiong Sean Zhou wrote:
> > 
> > 
> >>After memory.limit(1500), the error message still pop out:
> >>
> >>Error: cannot allocate vector of size 11529 Kb
> >>
> >>While 
> >>
> >>
> >>
> >>>memory.size()
> >>
> >>[1] 307446696
> >>
> >>
> >>>memory.limit()
> >>
> >>[1] 1572864000
> >>
> >>And the system is only using 723MB physical memory, while 2G is the total.
> > 
> > 
> >>Does anyone have a clue of what is going on? 
> >>
> >>
> >>Yi-Xiong
> >>
> >>
> >>-----Original Message-----
> >>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> >>Sent: Saturday, April 03, 2004 12:40 PM
> >>To: Yi-Xiong Sean Zhou
> >>Cc: r-help at stat.math.ethz.ch
> >>Subject: Re: [R] memory limit problem
> >>
> >>
> >>
> >>Yi-Xiong Sean Zhou wrote:
> >>
> >>
> >>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
> >>>laptop running XP professional with 2G memory?
> >>
> >>
> >>See ?Memory or the the R for Windows FAQ, which tells you:
> >>
> >>"2.7 There seems to be a limit on the memory it uses!
> >>
> >>Indeed there is. It is set by the command-line flag --max-mem-size (see
> >>How do I install R for Windows?) and defaults to the smaller of the
> >>amount of physical
> >>RAM in the machine and 1Gb. [...]"
> >>
> >>
> >>
> >>
> >>>I have tried
> >>>
> >>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
> >>>
> >>>but I only get only 500MB for R actually.
> >>>
> >>>
> >>>I also tried memory.limit(2^30) in R and got error of:
> >>
> >>
> >>Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
> >>do you? 
> >>
> >>
> >>Either use the command line flag --max-mem-size=1500M or within R:
> >> memory.limit(1500)
> >>
> >> 
> >>
> >>
> >>>Error in memory.size(size) : cannot decrease memory limit
> >>
> >>
> >>Since your limit was roughly 10^6-times off the right one, you got an
> >>integer overflow internally, I think.
> >>
> >>Uwe Ligges
> >>
> >>
> >> 
> >>
> >>
> >>>Yi-Xiong
> >>>
> >>>       [[alternative HTML version deleted]]
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>
> >>http://www.R-project.org/posting-guide.html
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> > 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pxt at ph.adfa.edu.au  Sun Apr  4 13:51:56 2004
From: pxt at ph.adfa.edu.au (Pisut Tempatarachoke)
Date: Sun, 04 Apr 2004 21:51:56 +1000
Subject: [R] Scaling of font sizes in layout()
In-Reply-To: <40622EE4.1060404@stat.auckland.ac.nz>
References: <40615629.6070602@ph.adfa.edu.au>
	<40622EE4.1060404@stat.auckland.ac.nz>
Message-ID: <406FF6DC.5020302@ph.adfa.edu.au>

Paul Murrell wrote:
> Hi
> 
> 
> Pisut Tempatarachoke wrote:
> 
>> Hi all,
>>
>> In the following example,
>>
>> #--------------EXAMPLE------------------
>> test <- function(subfigure)
>> {
>> plot(c(1:10),c(1:10),cex=4)
>> text(1,9,subfigure,cex=10)
>> }
>> m <- matrix(c(1,2,5,5,3,4,5,5),4,2)
>> layout(m)
>> test("a")
>> test("b")
>> test("c")
>> test("d")
>> test("e")
>> #---------------------------------------
>>
>> Is it possible to have the font (a,b,...,e) and pch sizes (including 
>> the axis-label, tick and tick-label sizes) scaled proportionally with 
>> the size of each plot when I put multiple plots on the same page?
> 
> 
> 
> When you have multiple figures, R tries to think for you and reduces the 
> "base" size of text.  You can explicitly control this base size through 
> par().  Does the following slight modification of your example do what 
> you want?
> 
> test <- function(subfigure)
> {
> plot(c(1:10),c(1:10),cex=4)
> text(1,9,subfigure,cex=10)
> }
> m <- matrix(c(1,2,5,5,3,4,5,5),4,2)
> layout(m)
> test("a")
> test("b")
> test("c")
> test("d")
> par(cex=1)
> test("e")
> 
> Paul

Hi Paul,

Sorry for taking so long to reply.  Your suggestion worked right away 
but I have been busily caught up with other things.  Again, thank you 
very much for your help.

Best regards
Pisut



From yzhou at sdsc.edu  Sun Apr  4 18:16:46 2004
From: yzhou at sdsc.edu (Yi-Xiong Sean Zhou)
Date: Sun, 4 Apr 2004 09:16:46 -0700
Subject: [R] memory limit problem
In-Reply-To: <Pine.LNX.4.44.0404040813010.24326-100000@gannet.stats>
Message-ID: <000201c41a60$3f578760$2b0ea8c0@D3BC5441>

I tried using --max-mem-size=1400M at the command line on 1.8.1 and did not
work. However, 1.9.0beta works. The Os is XP professional on Dell inspiron
8600. 

Yi-Xiong

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Saturday, April 03, 2004 11:20 PM
To: Roger D. Peng
Cc: Yi-Xiong Sean Zhou; r-help at stat.math.ethz.ch
Subject: Re: [R] memory limit problem

That is true, but I don't see that Yi-Xiong Sean Zhou has actually yet 
followed the instructions for 1.8.1, which is to set --max-mem-size on the 
command line (and this is in the rw-FAQ as people have pointed out).

The issue is that on Windows the memory address space can get fragmented, 
and this is ameliorated by reserving memory in advance -- that is what 
using --max-mem-size (and not memory.limit) does for you.

When used as recommended, both 1.8.1 and 1.9.0beta can handle workspaces
of up to about 1.7Gb.  1.9.0 can go higher on suitable OSes: see its
rw-FAQ.

On Sun, 4 Apr 2004, Roger D. Peng wrote:

> In general, this is not an R problem, it is a Windows problem.  I find 
> that these types of memory problems do not appear on Linux, for example.
> 
> -roger
> 
> Yi-Xiong Sean Zhou wrote:
> > R1.9.0beta solves the problem for now. The memory foot print of R1.9.0
is
> > way smaller than R1.8.1, with only 400M. It will be interesting to see
how
> > R1.9.0 handles the memory problem when it needs more than 700M.
> > 
> > Thanks for your helps. 
> > 
> > Yi-Xiong
> > 
> > -----Original Message-----
> > From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
> > Sent: Saturday, April 03, 2004 2:52 PM
> > To: Yi-Xiong Sean Zhou
> > Cc: 'Uwe Ligges'; r-help at stat.math.ethz.ch
> > Subject: Re: [R] memory limit problem
> > 
> > You may want to try downloading the development version of R at 
> > http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This 
> > version deals with Windows' deficiencies in memory management a 
> > little better.
> > 
> > -roger
> > 
> > Yi-Xiong Sean Zhou wrote:
> > 
> > 
> >>After memory.limit(1500), the error message still pop out:
> >>
> >>Error: cannot allocate vector of size 11529 Kb
> >>
> >>While 
> >>
> >>
> >>
> >>>memory.size()
> >>
> >>[1] 307446696
> >>
> >>
> >>>memory.limit()
> >>
> >>[1] 1572864000
> >>
> >>And the system is only using 723MB physical memory, while 2G is the
total.
> > 
> > 
> >>Does anyone have a clue of what is going on? 
> >>
> >>
> >>Yi-Xiong
> >>
> >>
> >>-----Original Message-----
> >>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> >>Sent: Saturday, April 03, 2004 12:40 PM
> >>To: Yi-Xiong Sean Zhou
> >>Cc: r-help at stat.math.ethz.ch
> >>Subject: Re: [R] memory limit problem
> >>
> >>
> >>
> >>Yi-Xiong Sean Zhou wrote:
> >>
> >>
> >>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
> >>>laptop running XP professional with 2G memory?
> >>
> >>
> >>See ?Memory or the the R for Windows FAQ, which tells you:
> >>
> >>"2.7 There seems to be a limit on the memory it uses!
> >>
> >>Indeed there is. It is set by the command-line flag --max-mem-size (see
> >>How do I install R for Windows?) and defaults to the smaller of the
> >>amount of physical
> >>RAM in the machine and 1Gb. [...]"
> >>
> >>
> >>
> >>
> >>>I have tried
> >>>
> >>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
> >>>
> >>>but I only get only 500MB for R actually.
> >>>
> >>>
> >>>I also tried memory.limit(2^30) in R and got error of:
> >>
> >>
> >>Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
> >>do you? 
> >>
> >>
> >>Either use the command line flag --max-mem-size=1500M or within R:
> >> memory.limit(1500)
> >>
> >> 
> >>
> >>
> >>>Error in memory.size(size) : cannot decrease memory limit
> >>
> >>
> >>Since your limit was roughly 10^6-times off the right one, you got an
> >>integer overflow internally, I think.
> >>
> >>Uwe Ligges
> >>
> >>
> >> 
> >>
> >>
> >>>Yi-Xiong
> >>>
> >>>       [[alternative HTML version deleted]]
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>
> >>http://www.R-project.org/posting-guide.html
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> > 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Apr  4 18:42:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 4 Apr 2004 17:42:53 +0100 (BST)
Subject: [R] memory limit problem
In-Reply-To: <000201c41a60$3f578760$2b0ea8c0@D3BC5441>
Message-ID: <Pine.LNX.4.44.0404041736200.25607-100000@gannet.stats>

What do you mean `did not work'?  Did it not start (you may need to reboot 
your machine to clear its memory tables) or did your task run out of 
memory?

Please do read the posting guide and its references, and try to give 
useful information about the problem you encounter.  Saying `did not work' 
without ever saying what is maximally uninformative.


On Sun, 4 Apr 2004, Yi-Xiong Sean Zhou wrote:

> I tried using --max-mem-size=1400M at the command line on 1.8.1 and did not
> work. However, 1.9.0beta works. The Os is XP professional on Dell inspiron
> 8600. 
> 
> Yi-Xiong
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Saturday, April 03, 2004 11:20 PM
> To: Roger D. Peng
> Cc: Yi-Xiong Sean Zhou; r-help at stat.math.ethz.ch
> Subject: Re: [R] memory limit problem
> 
> That is true, but I don't see that Yi-Xiong Sean Zhou has actually yet 
> followed the instructions for 1.8.1, which is to set --max-mem-size on the 
> command line (and this is in the rw-FAQ as people have pointed out).
> 
> The issue is that on Windows the memory address space can get fragmented, 
> and this is ameliorated by reserving memory in advance -- that is what 
> using --max-mem-size (and not memory.limit) does for you.
> 
> When used as recommended, both 1.8.1 and 1.9.0beta can handle workspaces
> of up to about 1.7Gb.  1.9.0 can go higher on suitable OSes: see its
> rw-FAQ.
> 
> On Sun, 4 Apr 2004, Roger D. Peng wrote:
> 
> > In general, this is not an R problem, it is a Windows problem.  I find 
> > that these types of memory problems do not appear on Linux, for example.
> > 
> > -roger
> > 
> > Yi-Xiong Sean Zhou wrote:
> > > R1.9.0beta solves the problem for now. The memory foot print of R1.9.0
> is
> > > way smaller than R1.8.1, with only 400M. It will be interesting to see
> how
> > > R1.9.0 handles the memory problem when it needs more than 700M.
> > > 
> > > Thanks for your helps. 
> > > 
> > > Yi-Xiong
> > > 
> > > -----Original Message-----
> > > From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
> > > Sent: Saturday, April 03, 2004 2:52 PM
> > > To: Yi-Xiong Sean Zhou
> > > Cc: 'Uwe Ligges'; r-help at stat.math.ethz.ch
> > > Subject: Re: [R] memory limit problem
> > > 
> > > You may want to try downloading the development version of R at 
> > > http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This 
> > > version deals with Windows' deficiencies in memory management a 
> > > little better.
> > > 
> > > -roger
> > > 
> > > Yi-Xiong Sean Zhou wrote:
> > > 
> > > 
> > >>After memory.limit(1500), the error message still pop out:
> > >>
> > >>Error: cannot allocate vector of size 11529 Kb
> > >>
> > >>While 
> > >>
> > >>
> > >>
> > >>>memory.size()
> > >>
> > >>[1] 307446696
> > >>
> > >>
> > >>>memory.limit()
> > >>
> > >>[1] 1572864000
> > >>
> > >>And the system is only using 723MB physical memory, while 2G is the
> total.
> > > 
> > > 
> > >>Does anyone have a clue of what is going on? 
> > >>
> > >>
> > >>Yi-Xiong
> > >>
> > >>
> > >>-----Original Message-----
> > >>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> > >>Sent: Saturday, April 03, 2004 12:40 PM
> > >>To: Yi-Xiong Sean Zhou
> > >>Cc: r-help at stat.math.ethz.ch
> > >>Subject: Re: [R] memory limit problem
> > >>
> > >>
> > >>
> > >>Yi-Xiong Sean Zhou wrote:
> > >>
> > >>
> > >>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
> > >>>laptop running XP professional with 2G memory?
> > >>
> > >>
> > >>See ?Memory or the the R for Windows FAQ, which tells you:
> > >>
> > >>"2.7 There seems to be a limit on the memory it uses!
> > >>
> > >>Indeed there is. It is set by the command-line flag --max-mem-size (see
> > >>How do I install R for Windows?) and defaults to the smaller of the
> > >>amount of physical
> > >>RAM in the machine and 1Gb. [...]"
> > >>
> > >>
> > >>
> > >>
> > >>>I have tried
> > >>>
> > >>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
> > >>>
> > >>>but I only get only 500MB for R actually.
> > >>>
> > >>>
> > >>>I also tried memory.limit(2^30) in R and got error of:
> > >>
> > >>
> > >>Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
> > >>do you? 
> > >>
> > >>
> > >>Either use the command line flag --max-mem-size=1500M or within R:
> > >> memory.limit(1500)
> > >>
> > >> 
> > >>
> > >>
> > >>>Error in memory.size(size) : cannot decrease memory limit
> > >>
> > >>
> > >>Since your limit was roughly 10^6-times off the right one, you got an
> > >>integer overflow internally, I think.
> > >>
> > >>Uwe Ligges
> > >>
> > >>
> > >> 
> > >>
> > >>
> > >>>Yi-Xiong
> > >>>
> > >>>       [[alternative HTML version deleted]]
> > >>>
> > >>>______________________________________________
> > >>>R-help at stat.math.ethz.ch mailing list
> > >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>>PLEASE do read the posting guide!
> > >>
> > >>http://www.R-project.org/posting-guide.html
> > >>
> > >>______________________________________________
> > >>R-help at stat.math.ethz.ch mailing list
> > >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>PLEASE do read the posting guide!
> > > 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dvanbrunt at well-wired.com  Mon Apr  5 01:44:02 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Sun, 04 Apr 2004 18:44:02 -0500
Subject: [R] Can't seem to finish a randomForest.... Just goes and goes!
In-Reply-To: <5.2.1.1.0.20040403120228.00a24960@pop.freeserve.net>
Message-ID: <BC9607F2.742D%dvanbrunt@well-wired.com>

Playing with randomForest, samples run fine. But on real data, no go.

Here's the setup: OS X, same behavior whether I'm using R-Aqua 1.8.1 or the
Fink compile-of-my-own with X-11, R version 1.8.1.

This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical RAM.

I have not altered the Startup options of R.

Data set is read in from a text file with "read.table", and has 46 variables
and 1,855 cases. Trying the following:

The DV is categorical, 0 or 1. Most of the IV's are either continuous, or
correctly read in as factors. The largest factor has 30 levels.... Only the
DV seems to need identifying as a factor to force class trees over
regresssion:

>Mydata$V46<-as.factor(Mydata$V46)
>Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximities=FALSE
, importance=FALSE)

5 hours later, R.bin was still taking up 75% of my processor.  When I've
tried this with larger data, I get errors referring to the buffer (sorry,
not in front of me right now).

Any ideas on this? The data don't seem horrifically large. Seems like there
are a few options for setting memory size, but I'm  not sure which of them
to try tweaking, or if that's even the issue.



From andy_liaw at merck.com  Mon Apr  5 02:07:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 4 Apr 2004 20:07:06 -0400
Subject: [R] Can't seem to finish a randomForest.... Just goes and goe s!
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B18@usrymx25.merck.com>

When you have fairly large data, _do not use the formula interface_, as a
couple of copies of the data would be made.  Try simply:

Myforest.rf <- randomForest(Mydata[, -46], Mydata[,46], 
                            ntrees=100, mtry=7)

[Note that you don't need to set proximity (not proximities) or importance
to FALSE, as that's the default already.]

You might also want to use do.trace=1 to see if trees are actually being
grown (assuming there's no output buffering as in Rgui on Windows, otherwise
you'll probably want to turn that off).

I had run randomForest on data set much larger than that, without problem,
so I don't imagine your data would be `difficult'.  (I have not used the
Mac, though.)

Andy

> From: David L. Van Brunt, Ph.D.
> 
> Playing with randomForest, samples run fine. But on real data, no go.
> 
> Here's the setup: OS X, same behavior whether I'm using 
> R-Aqua 1.8.1 or the
> Fink compile-of-my-own with X-11, R version 1.8.1.
> 
> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M 
> physical RAM.
> 
> I have not altered the Startup options of R.
> 
> Data set is read in from a text file with "read.table", and 
> has 46 variables
> and 1,855 cases. Trying the following:
> 
> The DV is categorical, 0 or 1. Most of the IV's are either 
> continuous, or
> correctly read in as factors. The largest factor has 30 
> levels.... Only the
> DV seems to need identifying as a factor to force class trees over
> regresssion:
> 
> >Mydata$V46<-as.factor(Mydata$V46)
> >Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7
,proximities=FALSE
> , importance=FALSE)
> 
> 5 hours later, R.bin was still taking up 75% of my processor. 
>  When I've
> tried this with larger data, I get errors referring to the 
> buffer (sorry,
> not in front of me right now).
> 
> Any ideas on this? The data don't seem horrifically large. 
> Seems like there
> are a few options for setting memory size, but I'm  not sure 
> which of them
> to try tweaking, or if that's even the issue.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dvanbrunt at well-wired.com  Mon Apr  5 02:14:42 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Sun, 04 Apr 2004 19:14:42 -0500
Subject: [R] Can't seem to finish a randomForest.... Just goes and goe s!
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B18@usrymx25.merck.com>
Message-ID: <BC960F22.7432%dvanbrunt@well-wired.com>

Thanks for the pointer!! Can't believe you got back to me so quickly on a
Sunday evening. I'll give that a shot and let you know how it goes.

On 4/4/04 19:07, "Liaw, Andy" <andy_liaw at merck.com> wrote:

> When you have fairly large data, _do not use the formula interface_, as a
> couple of copies of the data would be made.  Try simply:
> 
> Myforest.rf <- randomForest(Mydata[, -46], Mydata[,46],
>                           ntrees=100, mtry=7)
> 
> [Note that you don't need to set proximity (not proximities) or importance
> to FALSE, as that's the default already.]
> 
> You might also want to use do.trace=1 to see if trees are actually being
> grown (assuming there's no output buffering as in Rgui on Windows, otherwise
> you'll probably want to turn that off).
> 
> I had run randomForest on data set much larger than that, without problem,
> so I don't imagine your data would be `difficult'.  (I have not used the
> Mac, though.)
> 
> Andy
> 
>> From: David L. Van Brunt, Ph.D.
>> 
>> Playing with randomForest, samples run fine. But on real data, no go.
>> 
>> Here's the setup: OS X, same behavior whether I'm using
>> R-Aqua 1.8.1 or the
>> Fink compile-of-my-own with X-11, R version 1.8.1.
>> 
>> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M
>> physical RAM.
>> 
>> I have not altered the Startup options of R.
>> 
>> Data set is read in from a text file with "read.table", and
>> has 46 variables
>> and 1,855 cases. Trying the following:
>> 
>> The DV is categorical, 0 or 1. Most of the IV's are either
>> continuous, or
>> correctly read in as factors. The largest factor has 30
>> levels.... Only the
>> DV seems to need identifying as a factor to force class trees over
>> regresssion:
>> 
>>> Mydata$V46<-as.factor(Mydata$V46)
>>> Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7
> ,proximities=FALSE
>> , importance=FALSE)
>> 
>> 5 hours later, R.bin was still taking up 75% of my processor.
>>  When I've
>> tried this with larger data, I get errors referring to the
>> buffer (sorry,
>> not in front of me right now).
>> 
>> Any ideas on this? The data don't seem horrifically large.
>> Seems like there
>> are a few options for setting memory size, but I'm  not sure
>> which of them
>> to try tweaking, or if that's even the issue.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
>> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------

-- 
David L. Van Brunt, Ph.D.
Outlier Consulting & Development
mailto: <ocd at well-wired.com>



From pnick at virgilio.it  Mon Apr  5 02:20:20 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Mon, 5 Apr 2004 02:20:20 +0200
Subject: [R] Cochrane-Orcutt
Message-ID: <405A65490001AAF7@ims2b.cp.tin.it>

hi everybody
i'm looking for a function to estimate a regression model via the Cochrane
Orcutt method
thanks



From nusbj at hotmail.com  Mon Apr  5 02:42:18 2004
From: nusbj at hotmail.com (Z P)
Date: Mon, 05 Apr 2004 08:42:18 +0800
Subject: [R] memory limit problem
Message-ID: <Sea2-F3Mh3lHj0VadoT00005ed4@hotmail.com>


Do you mean in Linux, there is no need to set memory limit? If needed, how 
to set it? Thanks.





>From: "Roger D. Peng" <rpeng at jhsph.edu>
>To: Yi-Xiong Sean Zhou <yzhou at sdsc.edu>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] memory limit problem
>Date: Sun, 04 Apr 2004 00:13:46 -0500
>
>In general, this is not an R problem, it is a Windows problem.  I find that 
>these types of memory problems do not appear on Linux, for example.
>
>-roger
>
>Yi-Xiong Sean Zhou wrote:
>>R1.9.0beta solves the problem for now. The memory foot print of R1.9.0 is
>>way smaller than R1.8.1, with only 400M. It will be interesting to see how
>>R1.9.0 handles the memory problem when it needs more than 700M.
>>
>>Thanks for your helps.
>>
>>Yi-Xiong
>>
>>-----Original Message-----
>>From: Roger D. Peng [mailto:rpeng at jhsph.edu] Sent: Saturday, April 03, 
>>2004 2:52 PM
>>To: Yi-Xiong Sean Zhou
>>Cc: 'Uwe Ligges'; r-help at stat.math.ethz.ch
>>Subject: Re: [R] memory limit problem
>>
>>You may want to try downloading the development version of R at 
>>http://cran.us.r-project.org/bin/windows/base/rdevel.html.  This version 
>>deals with Windows' deficiencies in memory management a little better.
>>
>>-roger
>>
>>Yi-Xiong Sean Zhou wrote:
>>
>>
>>>After memory.limit(1500), the error message still pop out:
>>>
>>>Error: cannot allocate vector of size 11529 Kb
>>>
>>>While
>>>
>>>
>>>
>>>>memory.size()
>>>
>>>[1] 307446696
>>>
>>>
>>>>memory.limit()
>>>
>>>[1] 1572864000
>>>
>>>And the system is only using 723MB physical memory, while 2G is the 
>>>total.
>>
>>
>>>Does anyone have a clue of what is going on?
>>>
>>>
>>>Yi-Xiong
>>>
>>>
>>>-----Original Message-----
>>>From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] Sent: 
>>>Saturday, April 03, 2004 12:40 PM
>>>To: Yi-Xiong Sean Zhou
>>>Cc: r-help at stat.math.ethz.ch
>>>Subject: Re: [R] memory limit problem
>>>
>>>
>>>
>>>Yi-Xiong Sean Zhou wrote:
>>>
>>>
>>>>Could anyone advise me how to allocate 1.5Gbyte memory for R on a Dell
>>>>laptop running XP professional with 2G memory?
>>>
>>>
>>>See ?Memory or the the R for Windows FAQ, which tells you:
>>>
>>>"2.7 There seems to be a limit on the memory it uses!
>>>
>>>Indeed there is. It is set by the command-line flag --max-mem-size (see
>>>How do I install R for Windows?) and defaults to the smaller of the
>>>amount of physical
>>>RAM in the machine and 1Gb. [...]"
>>>
>>>
>>>
>>>
>>>>I have tried
>>>>
>>>>"C:\Program Files\R\rw1081\bin\Rgui.exe" --max-vsize=1400M
>>>>
>>>>but I only get only 500MB for R actually.
>>>>
>>>>
>>>>I also tried memory.limit(2^30) in R and got error of:
>>>
>>>
>>>Well, you don't want to allocate 2^30 *Mega*Bytes (see ?memory.limit),
>>>do you?
>>>
>>>
>>>Either use the command line flag --max-mem-size=1500M or within R:
>>>memory.limit(1500)
>>>
>>>
>>>
>>>
>>>>Error in memory.size(size) : cannot decrease memory limit
>>>
>>>
>>>Since your limit was roughly 10^6-times off the right one, you got an
>>>integer overflow internally, I think.
>>>
>>>Uwe Ligges
>>>
>>>
>>>
>>>
>>>
>>>>Yi-Xiong
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Mon Apr  5 03:12:38 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 4 Apr 2004 21:12:38 -0400
Subject: [R] Cochrane-Orcutt
In-Reply-To: <405A65490001AAF7@ims2b.cp.tin.it>
Message-ID: <20040405011235.XSTP1581.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear pnick,

If you search the r-help archives, you'll see that some time ago I posted a
Cochrane-Orcutt function. It's not clear to me, however, why you'd want to
use this in preference to the gls function in the nlme package.

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> pnick at virgilio.it
> Sent: Sunday, April 04, 2004 7:20 PM
> To: r
> Subject: [R] Cochrane-Orcutt
> 
> hi everybody
> i'm looking for a function to estimate a regression model via 
> the Cochrane Orcutt method thanks
>



From ajayshah at mayin.org  Sat Apr  3 17:16:41 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 3 Apr 2004 20:46:41 +0530
Subject: [R] residuals with missing values
Message-ID: <20040403151641.GA22883@igidr.ac.in>

> hi: sorry to bother you all again.  I am running a simple lm(y~x+z)
> regression, in which some of the observations are missing.
> Unfortunately, the residuals vector from the lm object omits all the
> missing values, which means that I cannot simply do residual
> diagnostics (e.g., plot(y,x)).  Would it not make more sense to have
> the residuals propagate the missing values, so that the residuals
> are guaranteed to have the same length as the variables?
> Alternatively, maybe the residuals() function could do this instead.
> But the documentation is not clear:

I had a similar situation, and Brian Ripley said to me:

  If you have missing data in your data frame and want residuals for
  all observations, you need to use na.action=na.exclude, not the
  default na.omit.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Sat Apr  3 16:57:25 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 3 Apr 2004 20:27:25 +0530
Subject: [R] boot question
Message-ID: <20040403145725.GA22812@igidr.ac.in>

> > x<-rnorm(20)
> > mean(x)
> [1] -0.2272851
> > results<-boot(x,mean,R=5)
> 
> What in the world am I missing??

See http://www.mayin.org/ajayshah/KB/R/statistics.html

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From zelickr at pdx.edu  Mon Apr  5 06:43:40 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Sun, 4 Apr 2004 21:43:40 -0700 (PDT)
Subject: [R] x-only zoom and pan?
In-Reply-To: <Pine.SOL.4.20.0403191650390.4918-100000@santiam.dfci.harvard.edu>
Message-ID: <Pine.GSO.4.44.0404042046350.5033-100000@freke.odin.pdx.edu>

Hello list,

Could the following be done without too much grief...?

Lets say I have two or three time series objects that I want to inspect
visually. Each I would plot with a y-offset so they stack up. They share
the same X scaling. The problem is that each is perhaps 100K values. Due
to the large number of values, features of the data sets cannot be seen
when all values are plotted.

What would be nice is to plot a fraction of the X range (say 10%). This
would be equivalent to zooming in the X direction. Then using a key
(ideally an arrow key), shift the viewing frame right or left to
effectively scroll through the data. So first you view 0-10%, then 10-20%
and so forth.

If necessary I can fabricate a vector with X values in it and plot(x,y)
instead of as time series, if this makes it any easier.

I am using a Windows version of R.

Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From jasont at indigoindustrial.co.nz  Mon Apr  5 07:21:13 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Mon, 5 Apr 2004 17:21:13 +1200 (NZST)
Subject: [R] x-only zoom and pan?
In-Reply-To: <Pine.GSO.4.44.0404042046350.5033-100000@freke.odin.pdx.edu>
References: <Pine.SOL.4.20.0403191650390.4918-100000@santiam.dfci.harvard.edu>
	<Pine.GSO.4.44.0404042046350.5033-100000@freke.odin.pdx.edu>
Message-ID: <18021.203.9.176.60.1081142473.squirrel@webmail.maxnet.co.nz>

> Could the following be done without too much grief...?

It sounds possible, but there are already packages that deal with these
issues.  Some suggestions:

1) Use SVG plots, and Adobe's SVG Viewer plug-in for various web browsers.
 See the RSvgDevice package for details.

2) Use the Java graphics device - though under Windows, getting SJava
working can be problematic.  http://www.omegahat.org/RJavaDevice/

Cheers

Jason



From ggrothendieck at myway.com  Mon Apr  5 07:32:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 5 Apr 2004 05:32:07 +0000 (UTC)
Subject: [R] How to improve this code?
References: <k3bv60plke2slbevtj6vu36oshivhpv748@4ax.com>
Message-ID: <loom.20040405T072643-345@post.gmane.org>


If I understand correctly, storelist and customerlist are two column matrices
of lat and long and you want all combos less than a certain distance apart
sorted by store and distance.

dd is the distance matrix of all pairs.  We form this into a data frame of row
numbers (i.e. store numbers), column numbers (i.e.  customer numbers) and
distances, subset that and then sort it.  Then tapply seq to each group of
data from the same store to get ranks within stores.

Note that this forms some very large matrices if your data is large.

require(fields)
maxd <- 100
dd <- rdist.earth( storelist, customerlist, miles = F )
out <- data.frame( store=c(row(dd)), cust=c(col(dd)), dist=c(dd) )[c(dd)<maxd,]
out <- out[ order( out$store, out$dist ),]
rk <- c( unlist( tapply( out$store,  out$store, function(x)seq(along=x) ) ) )
out <- cbind( rank=rk, out )


Danny Heuman <dsheuman <at> rogers.com> writes:

: 
: Hi all,
: 
: I've got some functioning code that I've literally taken hours to
: write.  My 'R' coding is getting better...it used to take days :)
: 
: I know I've done a poor job of optimizing the code.  In addition, I'm
: missing an important step and don't know where to put it.
: 
: So, three questions:
: 
: 1)  I'd like the resulting output to be sorted on distance (ascending)
: and to have the 'rank' column represent the sort order, so that rank 1
: is the first customer and rank 10 is the 10th.  Where do I do this?
: 
: 2)  Can someone suggest ways of 'optimizing' or improving the code?
: It's the only way I'm going to learn better ways of approaching R.
: 
: 3)  If there are no customers in the store's Trade Area, I'd like the
: output file have nothing written to it .  How can I do that?
: 
: All help is appreciated.
: 
: Thanks,
: 
: Danny
: 
: 
: *********************************************************
: library(fields)
: 
: #Format of input files:  ID, LONGITUDE, LATITUDE
: 
: #Generate Store List
: storelist <- cbind(1:100, matrix(rnorm(100, mean = -60,  sd = 3), ncol
: = 1),
: 	     matrix(rnorm(100, mean = 50, sd = 3), ncol = 1))
: 
: #Generate Customer List
: customerlist <- cbind(1:10000,matrix(rnorm(10000, mean = -60,  sd =
: 20), ncol = 1),
: 	     matrix(rnorm(10000, mean = 50, sd = 10), ncol = 1))
: 
: #Output file
: outfile <- "c:\\output.txt"
: outfilecolnames <- c("rank","storeid","custid","distance")
: write.table(t(outfilecolnames), file = outfile, append=TRUE,
: sep=",",row.names=FALSE, col.names=FALSE)
: 
: #Trade Area Size
: TAsize <- c(100)
: 
: custlatlon <- customerlist[, 2:3]
: 
: for(i in 1:length(TAsize)){
: 	for(j in 1:nrow(storelist)){
: 		cat("Store: ", storelist[j],"  TA Size = ", TAsize[i],
: "\n")
: 		
: 		storelatlon <- storelist[j, 2:3]
: 		
: 		whichval <-
: which(rdist.earth(t(as.matrix(storelatlon)), as.matrix(custlatlon),
: miles=F) <= TAsize[i])
: 
: 		dist <-
: as.data.frame(rdist.earth(t(as.matrix(storelatlon)),
: as.matrix(custlatlon), miles=F)[whichval])
: 
: 		storetag <-
: as.data.frame(cbind(1:nrow(dist),storelist[j,1]))
: 		fincalc <-
: as.data.frame(cbind(1:nrow(dist),(customerlist[whichval,1]),rdist.earth(t
(as.matrix(storelatlon)),
: as.matrix(custlatlon), miles=F)[whichval]))
: 
: 		combinedata <- data.frame(storetag, fincalc)
: 
: 		combinefinal <- subset(combinedata, select= c(-1,-3))
: 		
: 		flush.console()
: 	
: 		write.table(combinefinal, file = outfile, append=TRUE,
: sep=",", col.names=FALSE)
: 	}
: 	
: }
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ripley at stats.ox.ac.uk  Mon Apr  5 08:12:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Apr 2004 07:12:02 +0100 (BST)
Subject: [R] memory limit problem
In-Reply-To: <Sea2-F3Mh3lHj0VadoT00005ed4@hotmail.com>
Message-ID: <Pine.LNX.4.44.0404050704280.27115-100000@gannet.stats>

On Linux, you can set memory limits in the shell you use to run R (via
command limit or ulimit, depending on the shell).

Yes, there can be a need to set memory limits on Linux too.  However, the 
problem was that (apparently, as we have not had a clear report), the 
available memory was not all being used, and that is not seen on Linux (or 
Solaris or ...).

On Mon, 5 Apr 2004, Z P wrote:

> Do you mean in Linux, there is no need to set memory limit? If needed, how 
> to set it? Thanks.

> >From: "Roger D. Peng" <rpeng at jhsph.edu>
> >
> >In general, this is not an R problem, it is a Windows problem.  I find that 
> >these types of memory problems do not appear on Linux, for example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Apr  5 08:27:17 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 5 Apr 2004 08:27:17 +0200 (CEST)
Subject: [R] Can't seem to finish a randomForest.... Just goes and goes!
In-Reply-To: <BC9607F2.742D%dvanbrunt@well-wired.com>
References: <BC9607F2.742D%dvanbrunt@well-wired.com>
Message-ID: <Pine.LNX.4.51.0404050822190.7719@artemis.imbe.med.uni-erlangen.de>

On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:

> Playing with randomForest, samples run fine. But on real data, no go.
>
> Here's the setup: OS X, same behavior whether I'm using R-Aqua 1.8.1 or the
> Fink compile-of-my-own with X-11, R version 1.8.1.
>
> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical RAM.
>
> I have not altered the Startup options of R.
>
> Data set is read in from a text file with "read.table", and has 46 variables
> and 1,855 cases. Trying the following:
>
> The DV is categorical, 0 or 1. Most of the IV's are either continuous, or
> correctly read in as factors. The largest factor has 30 levels.... Only the
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This means: there are 2^(30-1) = 536.870.912 possible splits to be
evaluated everytime this variable is picked up (minus something due to
empty levels). At least the last time I looked at the code, randomForest
used an exhaustive search over all possible splits. Try reducing the
number of levels to something reasonable (or for a first shot: remove this
variable from the learning sample).

Best,

Torsten


> DV seems to need identifying as a factor to force class trees over
> regresssion:
>
> >Mydata$V46<-as.factor(Mydata$V46)
> >Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximities=FALSE
> , importance=FALSE)
>
> 5 hours later, R.bin was still taking up 75% of my processor.  When I've
> tried this with larger data, I get errors referring to the buffer (sorry,
> not in front of me right now).
>
> Any ideas on this? The data don't seem horrifically large. Seems like there
> are a few options for setting memory size, but I'm  not sure which of them
> to try tweaking, or if that's even the issue.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From Matthias.Schmidt at forst.bwl.de  Mon Apr  5 08:41:55 2004
From: Matthias.Schmidt at forst.bwl.de (Schmidt.Matthias (FORST))
Date: Mon, 5 Apr 2004 08:41:55 +0200 
Subject: [R] confidence intervalls for multinom
Message-ID: <855D381F618DE84FA58C035BA803FCE6B9A6DF@FVAFR-SE1.FORST.BWL.DE>

Hi,
I have used the multinom function from the nnet library to estimate
probabilties for different levels of a nominal variable:

predict(object,type="probs")

Is  there any possibility to estimate confidence intervalls for the
probabilties with the multinom function?

best regards

Matthias Schmidt  

***********************************************************
Matthias Schmidt
Forstliche Versuchs- und Forschungsanstalt Baden-W??rttemberg (FVA) 
Abteilung Biometrie und Informatik
Wonnhaldestr. 4
79100 Freiburg i. Br
Tel.: + 49 (0)761 / 4018 -187
Fax: + 49 (0)761 / 4018 - 333



From Bill.Venables at csiro.au  Mon Apr  5 08:40:35 2004
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Mon, 5 Apr 2004 16:40:35 +1000
Subject: [R] Can't seem to finish a randomForest.... Just goes and goes!
Message-ID: <B998A44C8986644EA8029CFE6396A924018444@exqld2-bne.qld.csiro.au>

Alternatively, if you can arrive at a sensible ordering of the levels
you can declare them ordered factors and make the computation feasible
once again.

Bill Venables.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Torsten Hothorn
Sent: Monday, 5 April 2004 4:27 PM
To: David L. Van Brunt, Ph.D.
Cc: R-Help
Subject: Re: [R] Can't seem to finish a randomForest.... Just goes and
goes!


On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:

> Playing with randomForest, samples run fine. But on real data, no go.
>
> Here's the setup: OS X, same behavior whether I'm using R-Aqua 1.8.1 
> or the Fink compile-of-my-own with X-11, R version 1.8.1.
>
> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical 
> RAM.
>
> I have not altered the Startup options of R.
>
> Data set is read in from a text file with "read.table", and has 46 
> variables and 1,855 cases. Trying the following:
>
> The DV is categorical, 0 or 1. Most of the IV's are either continuous,

> or correctly read in as factors. The largest factor has 30 levels.... 
> Only the
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This means: there are 2^(30-1) = 536.870.912 possible splits to be
evaluated everytime this variable is picked up (minus something due to
empty levels). At least the last time I looked at the code, randomForest
used an exhaustive search over all possible splits. Try reducing the
number of levels to something reasonable (or for a first shot: remove
this variable from the learning sample).

Best,

Torsten


> DV seems to need identifying as a factor to force class trees over
> regresssion:
>
> >Mydata$V46<-as.factor(Mydata$V46)
> >Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximi
> >ties=FALSE
> , importance=FALSE)
>
> 5 hours later, R.bin was still taking up 75% of my processor.  When 
> I've tried this with larger data, I get errors referring to the buffer

> (sorry, not in front of me right now).
>
> Any ideas on this? The data don't seem horrifically large. Seems like 
> there are a few options for setting memory size, but I'm  not sure 
> which of them to try tweaking, or if that's even the issue.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Apr  5 09:00:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Apr 2004 08:00:11 +0100 (BST)
Subject: [R] confidence intervalls for multinom
In-Reply-To: <855D381F618DE84FA58C035BA803FCE6B9A6DF@FVAFR-SE1.FORST.BWL.DE>
Message-ID: <Pine.LNX.4.44.0404050754130.27322-100000@gannet.stats>

On Mon, 5 Apr 2004, Schmidt.Matthias (FORST) wrote:

> I have used the multinom function from the nnet library to estimate
> probabilties for different levels of a nominal variable:
> 
> predict(object,type="probs")
> 
> Is  there any possibility to estimate confidence intervalls for the
> probabilties with the multinom function?

No, as confidence intervals (sic) apply to single parameters not
probabilities (sic).  The prediction is a probability distribution, so
the uncertainty would have to be some region in Kd space, not an interval.

Why do you want uncertainty statements about predictions (often called
tolerance intervals/regions)?  In this case you have an event which
happens or not and the meaningful uncertainty is the probability
distribution.  If you really have need of a confidence region, you could
simulate from the uncertainty in the fitted parameters, predict and
summarize somehow the resulting empirical distribution.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sorenh at agrsci.dk  Mon Apr  5 09:36:49 2004
From: sorenh at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Mon, 5 Apr 2004 09:36:49 +0200
Subject: [R] Evaluation of functionals
Message-ID: <000e01c41ae0$c5669f50$d176f9c3@djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040405/0bcbea4d/attachment.pl

From h.wickham at auckland.ac.nz  Mon Apr  5 09:44:29 2004
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Mon, 05 Apr 2004 19:44:29 +1200
Subject: [R] Evaluation of functionals
In-Reply-To: <000e01c41ae0$c5669f50$d176f9c3@djf.agrsci.dk>
References: <000e01c41ae0$c5669f50$d176f9c3@djf.agrsci.dk>
Message-ID: <40710E5D.1040208@auckland.ac.nz>

 > Suppose I have
 >     f1 <- function(x) x
 >     f2 <- function(x) x^2
 >     funlist <- list(f1,f2)
 > Then I would like to evaluate funlist such that when x is 10 I should 
 > get a list with 10 and 100.

How about
sapply(funlist, function(x) x(10))
?

 > A related question is that of anonymous functions: how to evaluate
 > function(x)x^2 on x<-10 without assigning the function to a name?

(function(x) x^2)(10)


Hadley



From ripley at stats.ox.ac.uk  Mon Apr  5 09:49:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 5 Apr 2004 08:49:55 +0100 (BST)
Subject: [R] Evaluation of functionals
In-Reply-To: <000e01c41ae0$c5669f50$d176f9c3@djf.agrsci.dk>
Message-ID: <Pine.LNX.4.44.0404050845320.27382-100000@gannet.stats>

On Mon, 5 Apr 2004, S??ren H??jsgaard wrote:

> Suppose I have
>     f1 <- function(x) x
>     f2 <- function(x) x^2
>     funlist <- list(f1,f2)
> Then I would like to evaluate funlist such that when x is 10 I should get a list with 10 and 100.
> 
> A naive way of doint this is 
>     myf <- funlist[[1]]
>     do.call(paste(quote(myf)), list(x=10))
>     myf <- funlist[[2]]
>     do.call(paste(quote(myf)), list(x=10))

> - but there has to be much more elegant ways of doing this. I just can't
> figure out how..

lapply(funlist, function(f, x)f(x), x=10)

> Put more generally, is there a way of making R "understand"
> automatically that funlist above is really a function of x?

It isn't.  It is a list of functions of , so use lapply.

> A related question is that of anonymous functions: how to evaluate
> function(x)x^2 on x<-10 without assigning the function to a name?

> (function(x)x^2)(10)
[1] 100


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Mon Apr  5 09:43:20 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 05 Apr 2004 09:43:20 +0200
Subject: [R] Evaluation of functionals
In-Reply-To: <000e01c41ae0$c5669f50$d176f9c3@djf.agrsci.dk>
References: <000e01c41ae0$c5669f50$d176f9c3@djf.agrsci.dk>
Message-ID: <6.0.1.1.2.20040405093906.01fcb3c8@stat4ux.stat.ucl.ac.be>


Hi,

Once again, I think apply family is your friend. Dont forget functions are 
objects as any others:

At 09:36 5/04/2004, S??ren H??jsgaard wrote:
>Suppose I have
>     f1 <- function(x) x
>     f2 <- function(x) x^2
>     funlist <- list(f1,f2)
>Then I would like to evaluate funlist such that when x is 10 I should get 
>a list with 10 and 100.

 > lapply(funlist, FUN=function(f) f(10))
[[1]]
[1] 10

[[2]]
[1] 100

Or, better, pass the value as extra-parameter:

 > sapply(funlist, FUN=function(f,x) f(x), x=10)
[1]  10 100

>A related question is that of anonymous functions: how to evaluate
>function(x)x^2 on x<-10 without assigning the function to a name?

Here again, you can embedd the definition of your function in the [sl]apply 
code:

 > sapply(x<-10, FUN=function(x) x^2)
[1] 100


Eric


Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From wolski at molgen.mpg.de  Mon Apr  5 11:16:10 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 05 Apr 2004 11:16:10 +0200
Subject: [R] cluster validation.
Message-ID: <200404051116100685.0061BD9A@harry.molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040405/a053b297/attachment.pl

From s-plus at wiwi.uni-bielefeld.de  Mon Apr  5 10:41:24 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Mon, 05 Apr 2004 10:41:24 +0200
Subject: [R] x-only zoom and pan?
References: <Pine.GSO.4.44.0404042046350.5033-100000@freke.odin.pdx.edu>
Message-ID: <40711BB4.4000303@wiwi.uni-bielefeld.de>

Hallo here is a simple proposal using tcltk-sliders.

Peter Wolf

# step 1:   define general slider function

slider<-function(refresh.code,names,minima,maxima,resolutions,starts,title="control",no=0,
   set.no.value=0){
# pw 03/2004
   if(no!=0) 
return(as.numeric(tclvalue(get(paste("slider",no,sep=""),env=slider.env))))
   if(set.no.value[1]!=0){ 
try(eval(parse(text=paste("tclvalue(slider",set.no.value[1],")<-",
            set.no.value[2],sep="")),env=slider.env)); 
return(set.no.value[2]) }
   if(!exists("slider.env")) slider.env<<-new.env()
   library(tcltk); nt<-tktoplevel(); tkwm.title(nt,title); 
tkwm.geometry(nt,"+0+0")
   for(i in seq(names))
      
eval(parse(text=paste("assign(\"slider",i,"\",tclVar(starts[i]),env=slider.env)",sep="")))
   for(i in seq(names)){
      tkpack(fr<-tkframe(nt));  lab<-tklabel(fr, text=names[i], width="25")
      sc<-tkscale(fr, command=refresh.code, from=minima[i], to=maxima[i],
                     showvalue=T, resolution=resolutions[i], orient="horiz")
      assign("sc",sc,env=slider.env); tkpack(lab,sc,side="right")
      
eval(parse(text=paste("tkconfigure(sc,variable=slider",i,")",sep="")),env=slider.env)
   }
   tkpack(fr<-tkframe(nt),fill="x")
   tkpack(tkbutton(fr, text="Exit",    
command=function()tkdestroy(nt)),side="right")
   tkpack(tkbutton(fr, text="Reset", command=function(){
      for(i in seq(starts)) 
eval(parse(text=paste("tclvalue(slider",i,")<-",starts[i],sep="")),env=slider.env)
      refresh.code()    }  ),side="left")
}

# step 2: define function for zooming

ts.zoom<-function(x,y,z){
# pw 05042004
  library(tcltk)
  n.ts<-1; if(!missing(y)) n.ts<-2; if(!missing(z)) n.ts<-3
  refresh.code<-function(...){
  # initialization
    start<-slider(no=1)*100; delta<-slider(no=2)*100
    if(start+delta>length(x))
      start<-slider(set.no.value=c(1,(length(x)-delta)/100))*100
  # plot
    par(mfrow=c(n.ts,1))
    plot(x,type="l",xlim=c(start,start+delta))
    if(n.ts>=2)  plot(y,type="l",xlim=c(start,start+delta))
    if(n.ts>=3) plot(z,type="l",xlim=c(start,start+delta))
    par(mfrow=c(1,1))
 }
  slider(refresh.code,
  # names of sliders
       c("begin index (unit=100)",  "window width (unit=100)"),
  # min of sliders
       c(0,1),
  # max of sliders
       c(floor(length(x)/100),length(x)/100),
  # step of sliders
       c(1,1),
  # initial values
       c(1,1))
}

# step 3: test it.

ts.zoom(runif(10000), rexp(10000), rnorm(10000))




Randy Zelick wrote:

>Hello list,
>
>Could the following be done without too much grief...?
>
>Lets say I have two or three time series objects that I want to inspect
>visually. Each I would plot with a y-offset so they stack up. They share
>the same X scaling. The problem is that each is perhaps 100K values. Due
>to the large number of values, features of the data sets cannot be seen
>when all values are plotted.
>
>What would be nice is to plot a fraction of the X range (say 10%). This
>would be equivalent to zooming in the X direction. Then using a key
>(ideally an arrow key), shift the viewing frame right or left to
>effectively scroll through the data. So first you view 0-10%, then 10-20%
>and so forth.
>
>If necessary I can fabricate a vector with X values in it and plot(x,y)
>instead of as time series, if this makes it any easier.
>
>I am using a Windows version of R.
>
>Thanks,
>
>=Randy=
>
>R. Zelick				email: zelickr at pdx.edu
>Department of Biology			voice: 503-725-3086
>Portland State University		fax:   503-725-3888
>
>mailing:
>P.O. Box 751
>Portland, OR 97207
>
>shipping:
>1719 SW 10th Ave, Room 246
>Portland, OR 97201
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From nusbj at hotmail.com  Mon Apr  5 13:22:47 2004
From: nusbj at hotmail.com (Z P)
Date: Mon, 05 Apr 2004 19:22:47 +0800
Subject: [R] memory limit problem
Message-ID: <Sea2-F11hzL1f6Ykwn900007d80@hotmail.com>

we have two server, one is unix (1280M) and linux (2319872k), how can I set 
the max?

for the Linux one,

R --max-vsize=2319872k --max-nsize=2319872k

Then I use

>mem.limits()
nsize vsize
   NA    NA

It seems the limits are not set at all

I want to define an array

>a<-array(0,rep(2,28))
Error: cannot allocate vector of size 2097152 Kb

next I type

>a<-array(0,rep(2,27))

It is ok. then

>a<-array(0,rep(2,28))
Error: vector memory exhausted (limit reached?)

Is there any way to help? In fact, I am now using some array(0,rep(2,21)), 
the program run some time then fail, I can not trace what happened since I 
run R in batchs under Linux. Does this happen probably due to the memory 
exhaust? (for some array(0,rep(2,10)), my codes work well).




>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>To: Z P <nusbj at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] memory limit problem
>Date: Mon, 5 Apr 2004 07:12:02 +0100 (BST)
>
>On Linux, you can set memory limits in the shell you use to run R (via
>command limit or ulimit, depending on the shell).
>
>Yes, there can be a need to set memory limits on Linux too.  However, the
>problem was that (apparently, as we have not had a clear report), the
>available memory was not all being used, and that is not seen on Linux (or
>Solaris or ...).
>
>On Mon, 5 Apr 2004, Z P wrote:
>
> > Do you mean in Linux, there is no need to set memory limit? If needed, 
>how
> > to set it? Thanks.
>
> > >From: "Roger D. Peng" <rpeng at jhsph.edu>
> > >
> > >In general, this is not an R problem, it is a Windows problem.  I find 
>that
> > >these types of memory problems do not appear on Linux, for example.
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From viallonv at noos.fr  Mon Apr  5 14:22:04 2004
From: viallonv at noos.fr (Vivian Viallon)
Date: Mon, 5 Apr 2004 14:22:04 +0200
Subject: [R] GAM with Locfit components
Message-ID: <DDEDLHOBICGFLGDOHLEMMEGNCAAA.viallonv@noos.fr>

Hi,
I?m trying to combine the Locfit Package with the Mgcv package (to use
Generalized Additive Models with Locfit components).  I read the book
written by Clive Loader  where it?s said that, for the S language, you just
have to "load" the locfit package using the command :
Library(locfit, first="T")
in order to use locfit components in an additive model.
But I can?t. I guess the C-command differs from the S-command.
Thanks in  advance for your help.
Regards,
Vivian



From KINLEY_ROBERT at Lilly.com  Mon Apr  5 14:22:55 2004
From: KINLEY_ROBERT at Lilly.com (Robert Kinley)
Date: Mon, 05 Apr 2004 13:22:55 +0100
Subject: [R] JOB: Industrial Statistician , Liverpool UK
Message-ID: <OF1B5251D4.23362433-ON80256E6D.0043FA28-80256E6D.0043FF36@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040405/44bba8ec/attachment.pl

From andy_liaw at merck.com  Mon Apr  5 14:32:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Apr 2004 08:32:00 -0400
Subject: [R] GAM with Locfit components
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B1C@usrymx25.merck.com>

Loader's book is referring to the gam() function in S-plus (or S from Bell
Labs), not the one in mgcv.  They are very different things.  I don't know
if it's possible to implement local regression type smoothers (or something
other than splines) in gam() in mgcv, but even if it's possible, the one in
locfit won't work without quite a bit of work, I'd imagine.

Andy

> From: Vivian Viallon
> 
> Hi,
> I'm trying to combine the Locfit Package with the Mgcv package (to use
> Generalized Additive Models with Locfit components).  I read the book
> written by Clive Loader  where it's said that, for the S 
> language, you just
> have to "load" the locfit package using the command :
> Library(locfit, first="T")
> in order to use locfit components in an additive model.
> But I can't. I guess the C-command differs from the S-command.
> Thanks in  advance for your help.
> Regards,
> Vivian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Laurence.Loewe at evolutionary-research.net  Mon Apr  5 15:23:14 2004
From: Laurence.Loewe at evolutionary-research.net (Laurence Loewe)
Date: Mon, 5 Apr 2004 14:23:14 +0100
Subject: [R] Very slow start of R under MacOS X
Message-ID: <a06010201bc970c77c873@[129.215.191.50]>

Hi,

Since R 1.4.1 I am using R for various purposes on my Mac (OS9 and 
OSX10.3) and I am quite happy with it. However, recently I tried to 
update the binaries to 1.7.1 or 1.8, only to find that I have to wait 
for a painfully long time after doublecklicking R to start the 
application until I can type my first command.

Does anybody know, why versions 1.7. and above are so much slower 
than my old 1.4?
Is that only on my Mac or does everybody experience that?
Is there anything that I could do to speedup starting R?

Thanks for any help.
Laurence
-- 

--
Laurence Loewe
Institute of Cell, Animal and Population Biology - University of Edinburgh
Ashworth Laboratories - Kings Buildings - West Mains Road - Edinburgh 
EH9 3JT - Scotland  UK
Tel: +44 (131) 650 - 7330

---
Visit http://www.evolutionary-research.net and be part of evolution at home,
the first public distributed computing system for evolutionary biology. Help
predict genetic causes for the extinction of species while your CPU is idle.



From ligges at statistik.uni-dortmund.de  Mon Apr  5 15:41:09 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 05 Apr 2004 15:41:09 +0200
Subject: [R] memory limit problem
In-Reply-To: <Sea2-F11hzL1f6Ykwn900007d80@hotmail.com>
References: <Sea2-F11hzL1f6Ykwn900007d80@hotmail.com>
Message-ID: <407161F5.50508@statistik.uni-dortmund.de>

Z P wrote:

> we have two server, one is unix (1280M) and linux (2319872k), how can I 
> set the max?
> 
> for the Linux one,
> 
> R --max-vsize=2319872k --max-nsize=2319872k
> 
> Then I use
> 
>> mem.limits()
> 
> nsize vsize
>   NA    NA
> 
> It seems the limits are not set at all
> 
> I want to define an array
> 
>> a<-array(0,rep(2,28))

Well, you need  4 * 2^28 = 1073741824 Byte to store the data for that 
array (if numeric). Internally, the data will probably be copied at 
least once from one object to another, so already consuming ~ 2Gb and 
you have reached the limit in this case. I am not surprised.

Uwe Ligges


> Error: cannot allocate vector of size 2097152 Kb
> 
> next I type
> 
>> a<-array(0,rep(2,27))
> 
> 
> It is ok. then
> 
>> a<-array(0,rep(2,28))
> 
> Error: vector memory exhausted (limit reached?)
> 
> Is there any way to help? In fact, I am now using some 
> array(0,rep(2,21)), the program run some time then fail, I can not trace 
> what happened since I run R in batchs under Linux. Does this happen 
> probably due to the memory exhaust? (for some array(0,rep(2,10)), my 
> codes work well).
> 
> 
> 
> 
>> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> To: Z P <nusbj at hotmail.com>
>> CC: r-help at stat.math.ethz.ch
>> Subject: Re: [R] memory limit problem
>> Date: Mon, 5 Apr 2004 07:12:02 +0100 (BST)
>>
>> On Linux, you can set memory limits in the shell you use to run R (via
>> command limit or ulimit, depending on the shell).
>>
>> Yes, there can be a need to set memory limits on Linux too.  However, the
>> problem was that (apparently, as we have not had a clear report), the
>> available memory was not all being used, and that is not seen on Linux 
>> (or
>> Solaris or ...).
>>
>> On Mon, 5 Apr 2004, Z P wrote:
>>
>> > Do you mean in Linux, there is no need to set memory limit? If 
>> needed, how
>> > to set it? Thanks.
>>
>> > >From: "Roger D. Peng" <rpeng at jhsph.edu>
>> > >
>> > >In general, this is not an R problem, it is a Windows problem.  I 
>> find that
>> > >these types of memory problems do not appear on Linux, for example.
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From wildscop at yahoo.com  Mon Apr  5 16:02:54 2004
From: wildscop at yahoo.com (WilDscOp)
Date: Mon, 05 Apr 2004 20:02:54 +0600
Subject: [R] Selecting Best Regression Equation
Message-ID: <5.1.0.14.2.20040405194140.009f4500@127.0.0.1>

Dear all,

	Does R or S-plus or any of their packages provide any command to form any 
of the following procedures to find Best Regression Equation -

	1. 'All Possible Regressions Procedures' (is there any automated command 
to perform 2^p regressions and ordering according to criteria R2(adj), 
mallows Cp, s2- by not setting all the regression models manually),

	2. 'Backward Elimination Procedure' ,

	3. 'Forward Selection Procedure' ,

	4. 'Stepwise Regression Procedure' (as SAS's PROC REG /METHOD = STEPWISE / 
FORWARD / BACKWARD which methods are also available in SPSS) or

	5. 'Best subset selection (as MINITAB's BREG - BEST k command)'.


Any response / help / comment / suggestion / idea / web-link / replies will 
be greatly appreciated.

Thanks in advance for your time.

_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From tlumley at u.washington.edu  Mon Apr  5 16:40:36 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Apr 2004 07:40:36 -0700 (PDT)
Subject: [R] Very slow start of R under MacOS X
In-Reply-To: <a06010201bc970c77c873@[129.215.191.50]>
References: <a06010201bc970c77c873@[129.215.191.50]>
Message-ID: <Pine.A41.4.58.0404050738540.106912@homer34.u.washington.edu>

On Mon, 5 Apr 2004, Laurence Loewe wrote:

> Hi,
>
> Since R 1.4.1 I am using R for various purposes on my Mac (OS9 and
> OSX10.3) and I am quite happy with it. However, recently I tried to
> update the binaries to 1.7.1 or 1.8, only to find that I have to wait
> for a painfully long time after doublecklicking R to start the
> application until I can type my first command.
>
> Does anybody know, why versions 1.7. and above are so much slower
> than my old 1.4?
> Is that only on my Mac or does everybody experience that?
> Is there anything that I could do to speedup starting R?


You will be pleased to know that 1.9.0 is quite a bit faster (about 5sec
startup on my G4 laptop).

	-thomas



From andy_liaw at merck.com  Mon Apr  5 16:44:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Apr 2004 10:44:00 -0400
Subject: [R] Selecting Best Regression Equation
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B25@usrymx25.merck.com>

See the package `leap' on CRAN, and ?stepAIC in the package `MASS' (as well
as MASS the book).

Andy

> From: WilDscOp
> 
> Dear all,
> 
> 	Does R or S-plus or any of their packages provide any 
> command to form any 
> of the following procedures to find Best Regression Equation -
> 
> 	1. 'All Possible Regressions Procedures' (is there any 
> automated command 
> to perform 2^p regressions and ordering according to criteria 
> R2(adj), 
> mallows Cp, s2- by not setting all the regression models manually),
> 
> 	2. 'Backward Elimination Procedure' ,
> 
> 	3. 'Forward Selection Procedure' ,
> 
> 	4. 'Stepwise Regression Procedure' (as SAS's PROC REG 
> /METHOD = STEPWISE / 
> FORWARD / BACKWARD which methods are also available in SPSS) or
> 
> 	5. 'Best subset selection (as MINITAB's BREG - BEST k command)'.
> 
> 
> Any response / help / comment / suggestion / idea / web-link 
> / replies will 
> be greatly appreciated.
> 
> Thanks in advance for your time.
> 
> _______________________
> 
> Mohammad Ehsanul Karim <wildscop at yahoo.com>
> Institute of Statistical Research and Training
> University of Dhaka, Dhaka- 1000, Bangladesh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From tlumley at u.washington.edu  Mon Apr  5 16:46:06 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Apr 2004 07:46:06 -0700 (PDT)
Subject: [R] Selecting Best Regression Equation
In-Reply-To: <5.1.0.14.2.20040405194140.009f4500@127.0.0.1>
References: <5.1.0.14.2.20040405194140.009f4500@127.0.0.1>
Message-ID: <Pine.A41.4.58.0404050741130.106912@homer34.u.washington.edu>

On Mon, 5 Apr 2004, WilDscOp wrote:

> Dear all,
>
> 	Does R or S-plus or any of their packages provide any command to form any
> of the following procedures to find Best Regression Equation -
>
> 	1. 'All Possible Regressions Procedures' (is there any automated command
> to perform 2^p regressions and ordering according to criteria R2(adj),
> mallows Cp, s2- by not setting all the regression models manually),
>
> 	2. 'Backward Elimination Procedure' ,
>
> 	3. 'Forward Selection Procedure' ,
>
> 	4. 'Stepwise Regression Procedure' (as SAS's PROC REG /METHOD = STEPWISE /
> FORWARD / BACKWARD which methods are also available in SPSS) or
>
> 	5. 'Best subset selection (as MINITAB's BREG - BEST k command)'.
>

4 is available in step() and the MASS package's stepAIC().

2,3,5 are in the leaps package (as is 1 if you really want it, by setting
the number of models large enough).

It's a lot easier to find the functions than to find good reasons for
wanting them.

	-thomas



From ozric at web.de  Mon Apr  5 16:49:21 2004
From: ozric at web.de (Christian Schulz)
Date: Mon, 5 Apr 2004 16:49:21 +0200
Subject: [R] Selecting Best Regression Equation
In-Reply-To: <5.1.0.14.2.20040405194140.009f4500@127.0.0.1>
References: <5.1.0.14.2.20040405194140.009f4500@127.0.0.1>
Message-ID: <200404051649.21167.ozric@web.de>

library(leaps) is imho a starting point.

christian
 

Am Montag, 5. April 2004 16:02 schrieb WilDscOp:
> Dear all,
>
> 	Does R or S-plus or any of their packages provide any command to form any
> of the following procedures to find Best Regression Equation -
>
> 	1. 'All Possible Regressions Procedures' (is there any automated command
> to perform 2^p regressions and ordering according to criteria R2(adj),
> mallows Cp, s2- by not setting all the regression models manually),
>
> 	2. 'Backward Elimination Procedure' ,
>
> 	3. 'Forward Selection Procedure' ,
>
> 	4. 'Stepwise Regression Procedure' (as SAS's PROC REG /METHOD = STEPWISE /
> FORWARD / BACKWARD which methods are also available in SPSS) or
>
> 	5. 'Best subset selection (as MINITAB's BREG - BEST k command)'.
>
>
> Any response / help / comment / suggestion / idea / web-link / replies will
> be greatly appreciated.
>
> Thanks in advance for your time.
>
> _______________________
>
> Mohammad Ehsanul Karim <wildscop at yahoo.com>
> Institute of Statistical Research and Training
> University of Dhaka, Dhaka- 1000, Bangladesh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Mon Apr  5 18:43:53 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 5 Apr 2004 18:43:53 +0200
Subject: [R] x-only zoom and pan?
In-Reply-To: <40711BB4.4000303@wiwi.uni-bielefeld.de>
References: <Pine.GSO.4.44.0404042046350.5033-100000@freke.odin.pdx.edu>
	<40711BB4.4000303@wiwi.uni-bielefeld.de>
Message-ID: <16497.36041.571792.929115@gargle.gargle.HOWL>

>>>>> "Peter" == Peter Wolf <s-plus at wiwi.uni-bielefeld.de>
>>>>>     on Mon, 05 Apr 2004 10:41:24 +0200 writes:

    Peter> Hallo here is a simple proposal using tcltk-sliders.
    Peter> Peter Wolf

Peter, that is interesting (and nice in its generality and programming
interface),
however for a bit more intuitive *user* interface,
I slightly prefer the way I have done it for the  'tkdensity' function
I made (from Peter Dalgaard's original tkdensity demo).

Try
    install.package("sfsmisc")
    example(tkdensity)

Regards,
Martin

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From spencer.graves at pdf.com  Mon Apr  5 18:50:21 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Apr 2004 09:50:21 -0700
Subject: [R] Drawing maps in R? 
Message-ID: <40718E4D.5010804@pdf.com>

      What tools and documentation exist for producing maps and working 
with polygon and polylist objects in R beyond the following: 

      * Packages maps, mapproj, maptools, and mapdata. 

      * Becker and Wilks (1993, 1995), listed as references in the 
documentation on the "map" function. 

      Thanks,
      Spencer Graves



From Roger.Bivand at nhh.no  Mon Apr  5 19:58:05 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 5 Apr 2004 19:58:05 +0200 (CEST)
Subject: [R] Drawing maps in R? 
In-Reply-To: <40718E4D.5010804@pdf.com>
Message-ID: <Pine.LNX.4.44.0404051933360.8114-100000@reclus.nhh.no>

On Mon, 5 Apr 2004, Spencer Graves wrote:

>       What tools and documentation exist for producing maps and working 
> with polygon and polylist objects in R beyond the following: 
> 
>       * Packages maps, mapproj, maptools, and mapdata. 
> 
>       * Becker and Wilks (1993, 1995), listed as references in the 
> documentation on the "map" function. 
> 

Packages maps, mapproj and mapdata relate to the porting to R of the code 
to draw maps, and the geographical data base format described in the 
Becker and Wilks references. To quite a large extent, the user is 
restricted to using the polygons from the provided databases.

Alternative approaches try to allow for users importing foreign formats,
often from geographical information systems - for polygons, these are
vector formats. The thematic area is broad, and the code there is
(packages maptools and shapefiles on CRAN) can only be used with ESRI
(ArcGIS/ArcView) shapefiles, with the exception of RArcInfo (on CRAN),
that supports legacy ArcInfo topological vector formats. There are other
resources mentioned on the R spatial projects website (on Related Projects
on both the main R-project and CRAN navigation sidebars):

http://www.r-project.org/Rgeo

In addition, work is in progress to create S4 classes for spatial data - 
alpha source packages are available on :

http://sourceforge.net/projects/r-spatial/

We are hoping to present something on the spatial "island" in Vienna at 
useR! 2004 in May, so good ideas and suggestions are very welcome.

The key (unanswered) question is whether (or how far) the treatment of 
polygons should be simple or topology-based: shapefiles are simple, and 
all the polygons are seen as independent rings, while "maps" uses an 
underlying topology model, in which polygons are built on the fly from 
lists of directed edges. Doing topology right is probably more a GIS 
operation than a data analysis operation.

These things also get broader discussion on a more specialised mailing 
list:

https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-geo

Hope this helps - the more input and help with user requirements, typical 
input and output formats, (and maybe code too?), the more likely we are to 
move this forward.

Roger Bivand

PS. It would be really helpful if someone/anyone familiar with getting
SJava to work was willing to look at http://www.geotools.org and help
establish/investigate the magic words needed to use a solid and promising
library from within R.


>       Thanks,
>       Spencer Graves
> 
-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From wildscop at yahoo.com  Mon Apr  5 20:16:05 2004
From: wildscop at yahoo.com (WilDscOp)
Date: Tue, 06 Apr 2004 00:16:05 +0600
Subject: [R] Selecting Best Regression Equation : leaps() in R and
 stepwise() in S+
Message-ID: <5.1.0.14.2.20040406000956.00aceec0@127.0.0.1>

Dear all,

	
First of all - thanks to the R-users who replied my previous mail 
"Selecting Best Regression Equation". However, seems i've got some other 
problems now -

My data in c:\leafbrn.txt file is-
--------------------------------------
i	x1	x2	x3	y
1	3.05	1.45	5.67	0.34
2	4.22	1.35	4.86	0.11
3	3.34	0.26	4.19	0.38
4	3.77	0.23	4.42	0.68
5	3.52	1.10	3.17	0.18
6	3.54	0.76	2.76	0.00
7	3.74	1.59	3.81	0.08
8	3.78	0.39	3.23	0.11
9	2.92	0.39	5.44	1.53
10	3.10	0.64	6.16	0.77
11	2.86	0.82	5.48	1.17
12	2.78	0.64	4.62	1.01
13	2.22	0.85	4.49	0.89
14	2.67	0.90	5.59	1.40
15	3.12	0.92	5.86	1.05
16	3.03	0.97	6.60	1.15
17	2.45	0.18	4.51	1.49
18	4.12	0.62	5.31	0.51
19	4.61	0.51	5.16	0.18
20	3.94	0.45	4.45	0.34
21	4.12	1.79	6.17	0.36
22	2.93	0.25	3.38	0.89
23	2.66	0.31	3.51	0.91
24	3.17	0.20	3.08	0.92
25	2.79	0.24	3.98	1.35
26	2.61	0.20	3.64	1.33
27	3.74	2.27	6.50	0.23
28	3.13	1.48	4.28	0.26
29	3.49	0.25	4.71	0.73
30	2.94	2.22	4.58	0.23
--------------------------------------

And my applied R codes are-

--------------------------------------
 > options(prompt="  R >  " )
   R >  leafbrn1<-read.table("c:\\leafbrn.txt",header=T)
   R >  leafbrn.data1<-data.frame(leafbrn1)
   R >  attach(leafbrn.data1)
   R >  x1sq<-x1^2;x2sq<-x2^2;x3sq<-x3^2;x1x2<-x1*x2;x1x3<-x1*x3;x2x3<-x2*x3;
   R >  leafbrn <- cbind(leafbrn1,x1sq,x2sq,x3sq,x1x2,x1x3,x2x3)
   R >  detach(leafbrn.data1)
   R >  leafbrn.data<-data.frame(leafbrn)
   R >  attach(leafbrn.data)
   R >  X<-cbind(x1,x2,x3,x1sq,x2sq,x3sq,x1x2,x1x3,x2x3)
   R >  Y<-y
--------------------------------------
But using package leaps {leaps downloaded from 
http://cran.r-project.org/bin/windows/contrib/1.7/leaps_2.6.zip} with the 
following command -

   R >  leaps(X,Y,int=TRUE, method=c("Cp"), names=NULL, df=NROW(x), 
nbest=choose(9,5), strictly.compatible=T)
# Since there are 9 terms and its combination is higher in 4 or 5

gives the message -

Error in leaps.exhaustive(a) : Exhaustive search will be S L O W, must 
specify really.big=T
--------------------------------------


	1. What to do ? I don't understand where to set really.big=T.

	2. Why do we need strictly.compatible=T ?

	3. Can we simultaneously use Cp, r2, adjr2 in  method?

	4. I still have problem with FORWARD / BACKWARD / STEPWISE methods. Can 
anyone please help me to solve the above problem by using these methods (I 
use R 1.7.1)?

	5. In S-plus 4, is the following command correct for the above problem? - 
it seem to give me too many calculations and i don't understand which one 
is the result!

 > options(prompt="  S+ >  " )
   S+ >  stepwise(X, Y,intercept=T, tolerance=1.e-07,method="exhaustive", 
size.max=ncol(X), nbest=choose(9,5), f.crit=4, plot=T, time=0.02)


Any response / help / comment / suggestion / idea / web-link / replies will 
be greatly appreciated.

Thanks in advance for your time.

_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From jmiyamot at u.washington.edu  Mon Apr  5 21:24:50 2004
From: jmiyamot at u.washington.edu (John Miyamoto)
Date: Mon, 5 Apr 2004 12:24:50 -0700 (PDT)
Subject: [R] studentized deleted residuals and NA's
Message-ID: <Pine.A41.4.58.0404051219110.1052698@mead11.u.washington.edu>

Dear R-Help,
   I am using the studres function from the MASS package to compute
studentized deleted residuals in a oneway anova.  I'm having trouble
interpreting the results in situations where a factor level has only one
observation.  Sometimes studres yields an NaN and sometimes it produces a
numeric value for cases where a factor level has only one observation.  I
would think it should always produce NaN for this case.  The following
example illustrates this point.

library(MASS)
Data <- c(3,6,3,4,1,5,6,2)
Group <- factor(c(1,1,2,3,3,4,5,5))
lm.out <- lm(Data ~ Group)

data.frame(Data, Group,
	hat=lm.influence(lm.out)$hat,
	std.res=stdres(lm.out),
	stu.del.res=studres(lm.out))

OUTPUT:
  Data Group hat        std.res    stu.del.res
1    3     1 0.5 -8.9113279e-01 -8.4852814e-01
2    6     1 0.5  8.9113279e-01  8.4852814e-01
3    3     2 1.0            Inf            NaN
4    4     3 0.5  8.9113279e-01  8.4852814e-01
5    1     3 0.5 -8.9113279e-01 -8.4852814e-01
6    5     4 1.0 -1.5208938e-08 -1.2418046e-08
7    6     5 0.5  1.1881771e+00  1.3333333e+00
8    2     5 0.5 -1.1881771e+00 -1.3333333e+00
Warning messages:
1: NaNs produced in: sqrt((n - p - sr^2)/(n - p - 1))
2: NaNs produced in: sqrt((n - p - sr^2)/(n - p - 1))

Groups 2 and 4 both have only one observation.  Both the standardized
residual (std.res) and studentized deleted residual (stu.del.res) are NaN
for Group = 2 but both have a value for Group = 4.  I would think that
std.res and stu.del.res should be NaN for Group 2 and for Group 4 because
1 - h_ii = 0 for the corresponding cases.  Can someone tell me why the
std.res and stu.del.res are different for Group = 2 and Group = 4?
   I am running R 1.8.1 on a Windows XP machine.

John

--------------------------------------------------------------------
John Miyamoto, Dept. of Psychology, Box 351525
University of Washington, Seattle, WA 98195-1525
Phone 206-543-0805, Fax 206-685-3157, Email jmiyamot at u.washington.edu
Homepage http://faculty.washington.edu/jmiyamot/



From steve.roberts at man.ac.uk  Mon Apr  5 21:32:14 2004
From: steve.roberts at man.ac.uk (Steve Roberts)
Date: Mon, 5 Apr 2004 20:32:14 +0100
Subject: [R] 2 lme questions
Message-ID: <002a01c41b44$ecfa76e0$3e1642c2@D24PSZ0J>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040405/539f3f36/attachment.pl

From ivan2003 at freemail.hu  Mon Apr  5 22:56:26 2004
From: ivan2003 at freemail.hu (ji)
Date: Mon, 5 Apr 2004 22:56:26 +0200 (CEST)
Subject: [R] Hungarian map
Message-ID: <freemail.20040305225626.40700@fm13.freemail.hu>


Hi,
Can anybody help me with a Hungarian map available in R with lakes, 
more rivers / cities and boundaries of the counties. 
If I understood well I could use .shp files (with Maptools).
 
Unfortunately I haven't been able to find any (free) shape file of 
Hungary. 
 
I also download the WDB-files. Unfortunately I can not see the link 
beetween these text files and R. 
 
Thank you
 
Istvan Janosi



From sdhyok at email.unc.edu  Mon Apr  5 22:57:15 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 5 Apr 2004 16:57:15 -0400
Subject: [R] Deep copy in R
Message-ID: <OAEOKPIGCLDDHAEMCAKIGEDJCKAA.sdhyok@email.unc.edu>

I am handling spatial data of huge volumes,
so sensitive to the silent duplication of data in script programs.
In the following R program, exactly when is the vector data deeply copied?
Thanks in advance.

1 v <- 1:10000
2 z <- f(v)

--------- function f ----------
3 f <- function(x) {
4   y = x                               
5   y[10] = 1
6   xf = date.frame(x=x)
7   xf$x[10] = 1
8   return(y)
}

Daehyok Shin
Terrestrial Hydrological Ecosystem Modellers
Geography Department
University of North Carolina-Chapel Hill
sdhyok at email.unc.edu

"We can do no great things, 
only small things with great love."
                         - Mother Teresa



From tlumley at u.washington.edu  Mon Apr  5 23:17:36 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 5 Apr 2004 14:17:36 -0700 (PDT)
Subject: [R] Deep copy in R
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIGEDJCKAA.sdhyok@email.unc.edu>
References: <OAEOKPIGCLDDHAEMCAKIGEDJCKAA.sdhyok@email.unc.edu>
Message-ID: <Pine.A41.4.58.0404051405230.108666@homer03.u.washington.edu>

On Mon, 5 Apr 2004, Shin, Daehyok wrote:

> I am handling spatial data of huge volumes,
> so sensitive to the silent duplication of data in script programs.
> In the following R program, exactly when is the vector data deeply copied?
> Thanks in advance.
>
> 1 v <- 1:10000
> 2 z <- f(v)
>
> --------- function f ----------
> 3 f <- function(x) {
> 4   y = x
> 5   y[10] = 1
> 6   xf = date.frame(x=x)
> 7   xf$x[10] = 1
> 8   return(y)
> }
>

There will be a copy at line 5 and at line 6 or line 7 or both.

Copying occurs when there are (or could be) two references to an object
and one of them is modified.

Passing v to the function f creates a second reference to it (the local
variable x), and then y is a third reference. Modifying y forces a copy so
that x and v don't get modified

The data.frame() call need not copy, but I think it actually does. If it
does, modifying xf$x need not copy.

Returning y does not copy, and xf is discarded for garbage collection when
the function exits.


	-thomas



From HaroldD at ccsso.org  Tue Apr  6 00:09:21 2004
From: HaroldD at ccsso.org (Harold Doran)
Date: Mon, 5 Apr 2004 18:09:21 -0400
Subject: [R] 2 lme questions
Message-ID: <CFF85773D9245040A333571B7E6D651702C49262@ccssosrv1.ccsso.org>

There are two way to accomplish this in nlme. First try using the summary() command, which will produce all variance components and estimates for the fixed effects. Also, try the following to extract the point estimates and approximate CIs for the variance comonents.

> intervals(model.lme, which="var")

Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Steve Roberts
Sent: Monday, April 05, 2004 3:32 PM
To: r-help at stat.math.ethz.ch
Cc: Steve Roberts
Subject: [R] 2 lme questions


Greetings,

1) Is there a nice way of extracting the variance estimates from an lme fit? They don't seem to be part of the lme object.

2) In a series of simulations, I am finding that with ML fitting one of my random effect variances is sometimes being estimated as essentially zero with massive CI instead of the finite value it should have, whilst using REML I get the expected value. I guess it is a numerical/optimisation problem but don't know enough about the lme fitting algorithm to know which tollerance/scale parameter to mess about with. Any suggestions where to start? 

Thanks,

Steve.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From k.wang at auckland.ac.nz  Tue Apr  6 00:19:28 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 6 Apr 2004 10:19:28 +1200
Subject: [R] Nested Loop in R
References: <SEA2-F327Ud9BViWG430007a09c@hotmail.com>
Message-ID: <005201c41b5c$0fda6a50$6633d882@stat.auckland.ac.nz>

Hi,

Apologies for such a trivial question, but it has been over two years since
I last used nested loops and I'm ashamed to say that I forgot how they work
*_*....

Suppose I've the following codes:
#####
x <- round(runif(1000, 1, 6))
samp5 <- vector(mode = "list", length = 5)

for(i in 1:5) {
  samp5[[i]] <- sample(x, 5)
}

plot.new()
plot.window(xlim = c(1, 6), ylim = c(1, 5))
axis(1, at = 1:6)
axis(2, at = 1:5, labels = 5:1, las = 1)
points(jitter(samp5[[1]], 0.1), rep(5, 5))
lines(x = c(mean(samp5[[1]]), mean(samp5[[1]])),
      y = c(5.1, 4.9))
points(jitter(samp5[[2]], 0.1), rep(4, 5))
lines(x = c(mean(samp5[[2]]), mean(samp5[[2]])),
      y = c(4.1, 3.9))
points(jitter(samp5[[3]], 0.1), rep(3, 5))
lines(x = c(mean(samp5[[3]]), mean(samp5[[3]])),
      y = c(3.1, 2.9))
points(jitter(samp5[[4]], 0.1), rep(2, 5))
lines(x = c(mean(samp5[[4]]), mean(samp5[[4]])),
      y = c(2.1, 1.9))
points(jitter(samp5[[5]], 0.1), rep(1, 5))
lines(x = c(mean(samp5[[5]]), mean(samp5[[5]])),
      y = c(1.1, 0.9))
#####

and wanted to convert them into a function like:
#####
clt <- function(x, samp.no = 5, n = 5) {
  samp <- vector(mode = "list", length = samp.no)
  plot.new()
  plot.window(xlim = c(1, 6), ylim = c(1, samp.no))
  axis(1, at = 1:6)
  axis(2, at = 1:samp.no, labels = samp.no:1, las = 1)

  for(i in 1:samp.no) {
    samp[[i]] <- sample(x, n)
    for(j in n:1) {
      points(jitter(samp[[i]], 0.1), rep(j, n))
      lines(x = c(mean(samp[[i]]), mean(samp[[i]])),
            y = c(j - 0.1, j + 0.1))
    }
  }
}

clt(x)
#####

But it didn't quite work and I can see the problem occurs within the nested
loop.  Can anyone spot my mistake?

Thank you very much in advance!

Kevin



From ivo.welch at yale.edu  Tue Apr  6 00:23:00 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Mon, 05 Apr 2004 18:23:00 -0400
Subject: [R] normalized regression output
Message-ID: <4071DC44.5020305@yale.edu>


Hi:  I would like to write a function that takes as its input a formula 
and outputs normalized coefficients ( coef(x)*sdv(x)/sdv(y) ).  now, a 
formula is an object, and I cannot see how to extract the variables for 
obtaining sdv's.  the intent is to write something like

	my.print.lm( formula ) {
	  model <- lm(formula);
	  coefs <- (t(summary.lm(model)))[1,];
	  tvals <- (t(summary.lm(model)))[3,];
	  for (i in 1:length( formula.contents.length ) ) {
	    normcoefs[i] <- coefs[i]*sd( formula.coef[i] )
				/sd( formula.yvar ); }
	  # now I can do nice printing
	}

	my.print.lm ( y~x+z );

or something like it.  If this is not easy, please just tell me.  I just 
do not want to spend a day to reinvent a wheel that is very simple for 
an R statistician.  help appreciated.

regards,

/iaw



From wwsprague at ucdavis.edu  Tue Apr  6 00:52:32 2004
From: wwsprague at ucdavis.edu (Webb Sprague)
Date: Mon, 05 Apr 2004 15:52:32 -0700
Subject: [R] Displaying text
In-Reply-To: <200403311000.i2VA0aVs032371@hypatia.math.ethz.ch>
References: <200403311000.i2VA0aVs032371@hypatia.math.ethz.ch>
Message-ID: <4071E330.6080409@ucdavis.edu>

Hi all,

Is there a way to display text on a graphics page?  Basically, I want to 
plot a bunch of stuff on a single page using mfrow(), but also put a few 
lines describing the stuff you see.  I *do not* want to put the text 
with one of the plots.

I would also like display some tables, alongside the plots, if that is 
possible.

Thanks for everyone's time and attention.

W



From spencer.graves at pdf.com  Tue Apr  6 00:54:38 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 05 Apr 2004 15:54:38 -0700
Subject: [R] 2 lme questions
In-Reply-To: <CFF85773D9245040A333571B7E6D651702C49262@ccssosrv1.ccsso.org>
References: <CFF85773D9245040A333571B7E6D651702C49262@ccssosrv1.ccsso.org>
Message-ID: <4071E3AE.5040003@pdf.com>

      The "print" method and the "summary" command display the STANDARD 
DEVIATIONS (not the variances) on the screen (or in a sink file).  
However, when I do attributes(lme(...)) and 
attributes(summary(lme(...))), I don't see anything I can use.  
Fortunately, the "interval" function produces a list, from which the 
variance estimates can be extracted.  Consider the following example: 

DF <- data.frame(group=c(1,1,2,2), y=c(1, 2, 11, 12))
library(nlme)
fit <- lme(y~1, random=~1|group, DF)

Linear mixed-effects model fit by REML
  Data: DF
  Log-restricted-likelihood: -6.559401
  Fixed: y ~ 1
(Intercept)
        6.5

Random effects:
 Formula: ~1 | group
        (Intercept)  Residual
StdDev:    7.053597 0.7070954

Number of Observations: 4
Number of Groups: 2
 > lme.int <- intervals(fit)
 > lme.int$reStruct^2
Error in lme.int$reStruct^2 : non-numeric argument to binary operator

 > lme.int$reStruct$group^2
                  lower     est.    upper
sd((Intercept)) 3.06859 49.75323 806.6845
 > lme.int$sigma^2
    lower      est.     upper
0.0704255 0.4999839 3.5496217
attr(,"label")
[1] "Within-group standard error:"

      There may be a better way;  if there is, I hope someone will 
enlighten us all.  If not, at least this works in R 1.8.1

      hope this helps.  spencer graves


Harold Doran wrote:

>There are two way to accomplish this in nlme. First try using the summary() command, which will produce all variance components and estimates for the fixed effects. Also, try the following to extract the point estimates and approximate CIs for the variance comonents.
>
>  
>
>>intervals(model.lme, which="var")
>>    
>>
>
>Harold
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Steve Roberts
>Sent: Monday, April 05, 2004 3:32 PM
>To: r-help at stat.math.ethz.ch
>Cc: Steve Roberts
>Subject: [R] 2 lme questions
>
>
>Greetings,
>
>1) Is there a nice way of extracting the variance estimates from an lme fit? They don't seem to be part of the lme object.
>
>2) In a series of simulations, I am finding that with ML fitting one of my random effect variances is sometimes being estimated as essentially zero with massive CI instead of the finite value it should have, whilst using REML I get the expected value. I guess it is a numerical/optimisation problem but don't know enough about the lme fitting algorithm to know which tollerance/scale parameter to mess about with. Any suggestions where to start? 
>
>Thanks,
>
>Steve.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From dvanbrunt at well-wired.com  Tue Apr  6 02:16:44 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Mon, 05 Apr 2004 19:16:44 -0500
Subject: [R] Can't seem to finish a randomForest.... Just goes and
	goes!
In-Reply-To: <B998A44C8986644EA8029CFE6396A924018444@exqld2-bne.qld.csiro.au>
Message-ID: <BC97611C.74CC%dvanbrunt@well-wired.com>

D'OH!

I clearly just needed to Re-RTFM!!!  I had a column still coded as TEXT
(yup, "Monday", etc), and the randomForest manual by Breiman says they need
to be numerically coded. Easy recode. I'll try running it RIGHT this time,
and let you all know how this goes.  Grumble mumble mumble....

On 4/5/04 1:40, "Bill.Venables at csiro.au" <Bill.Venables at csiro.au> wrote:

> Alternatively, if you can arrive at a sensible ordering of the levels
> you can declare them ordered factors and make the computation feasible
> once again.
> 
> Bill Venables.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Torsten Hothorn
> Sent: Monday, 5 April 2004 4:27 PM
> To: David L. Van Brunt, Ph.D.
> Cc: R-Help
> Subject: Re: [R] Can't seem to finish a randomForest.... Just goes and
> goes!
> 
> 
> On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:
> 
>> Playing with randomForest, samples run fine. But on real data, no go.
>> 
>> Here's the setup: OS X, same behavior whether I'm using R-Aqua 1.8.1
>> or the Fink compile-of-my-own with X-11, R version 1.8.1.
>> 
>> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical
>> RAM.
>> 
>> I have not altered the Startup options of R.
>> 
>> Data set is read in from a text file with "read.table", and has 46
>> variables and 1,855 cases. Trying the following:
>> 
>> The DV is categorical, 0 or 1. Most of the IV's are either continuous,
> 
>> or correctly read in as factors. The largest factor has 30 levels....
>> Only the
>                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> 
> This means: there are 2^(30-1) = 536.870.912 possible splits to be
> evaluated everytime this variable is picked up (minus something due to
> empty levels). At least the last time I looked at the code, randomForest
> used an exhaustive search over all possible splits. Try reducing the
> number of levels to something reasonable (or for a first shot: remove
> this variable from the learning sample).
> 
> Best,
> 
> Torsten
> 
> 
>> DV seems to need identifying as a factor to force class trees over
>> regresssion:
>> 
>>> Mydata$V46<-as.factor(Mydata$V46)
>>> Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximi
>>> ties=FALSE
>> , importance=FALSE)
>> 
>> 5 hours later, R.bin was still taking up 75% of my processor.  When
>> I've tried this with larger data, I get errors referring to the buffer
> 
>> (sorry, not in front of me right now).
>> 
>> Any ideas on this? The data don't seem horrifically large. Seems like
>> there are a few options for setting memory size, but I'm  not sure
>> which of them to try tweaking, or if that's even the issue.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
>> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
David L. Van Brunt, Ph.D.
Outlier Consulting & Development
mailto: <ocd at well-wired.com>



From HaroldD at ccsso.org  Tue Apr  6 02:30:08 2004
From: HaroldD at ccsso.org (Harold Doran)
Date: Mon, 5 Apr 2004 20:30:08 -0400
Subject: [R] 2 lme questions
Message-ID: <CFF85773D9245040A333571B7E6D651702C5FC8C@ccssosrv1.ccsso.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040405/df271930/attachment.pl

From andy_liaw at merck.com  Tue Apr  6 02:31:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Apr 2004 20:31:27 -0400
Subject: [R] normalized regression output
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B32@usrymx25.merck.com>

> From: ivo welch
> 
> Hi:  I would like to write a function that takes as its input 
> a formula 
> and outputs normalized coefficients ( coef(x)*sdv(x)/sdv(y) 
> ).  now, a 
> formula is an object, and I cannot see how to extract the 
> variables for 
> obtaining sdv's.  the intent is to write something like
> 
> 	my.print.lm( formula ) {
> 	  model <- lm(formula);
> 	  coefs <- (t(summary.lm(model)))[1,];
> 	  tvals <- (t(summary.lm(model)))[3,];
> 	  for (i in 1:length( formula.contents.length ) ) {
> 	    normcoefs[i] <- coefs[i]*sd( formula.coef[i] )
> 				/sd( formula.yvar ); }
> 	  # now I can do nice printing
> 	}
> 
> 	my.print.lm ( y~x+z );

It's not clear to me what you want to do.  You need data in addition to the
formula, no?  See if the following is approximately what you are looking
for:

myPrint.lm <- function(lm.obj) {
    ## You don't want to include the intercept, do you?
    coefs <- coef(lm.obj)[-1]
    tvals <- summary(lm.obj)$coefficients[-1, 3]
    m <- model.frame(fit)  # get the data used for the fit.
    SD <- apply(m, 2, sd)
    normCoefs <- coefs * SD[-1] / SD[1]
    ## Do whatever nice printing you want.
    ## ...
    invisible(normCoefs)
}

Here's a test:

> set.seed(1)
> dat <- data.frame(y=rnorm(30), x1=runif(30), x2=runif(30), x3=runif(30))
> 
> fit <- lm(y~., dat)
> print(myPrint.lm(fit))
        x1         x2         x3 
0.19635111 0.19688412 0.02435227 
> 
> ## Isn't this easier?
> dat3 <- as.data.frame(scale(dat))
> coef(lm(y ~ 0 + ., dat3))
        x1         x2         x3 
0.19635111 0.19688412 0.02435227 

Andy

 
> or something like it.  If this is not easy, please just tell 
> me.  I just 
> do not want to spend a day to reinvent a wheel that is very 
> simple for 
> an R statistician.  help appreciated.
> 
> regards,
> 
> /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Apr  6 02:35:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Apr 2004 20:35:23 -0400
Subject: [R] Can't seem to finish a randomForest.... Just goes and goe s!
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B33@usrymx25.merck.com>

No, that's not the problem.  You still need to take Bill's and Torsten's
advise.  If the categorical variables (class labels included) are read into
R as factors, then the conversion to integers is automagic:  Factors in R
_are_ integers 1 through the number of levels, with the levels attribute.
randomForest() just takes advantage of that fact.

Andy

> From: David L. Van Brunt, Ph.D.
> 
> D'OH!
> 
> I clearly just needed to Re-RTFM!!!  I had a column still 
> coded as TEXT
> (yup, "Monday", etc), and the randomForest manual by Breiman 
> says they need
> to be numerically coded. Easy recode. I'll try running it 
> RIGHT this time,
> and let you all know how this goes.  Grumble mumble mumble....
> 
> On 4/5/04 1:40, "Bill.Venables at csiro.au" 
> <Bill.Venables at csiro.au> wrote:
> 
> > Alternatively, if you can arrive at a sensible ordering of 
> the levels
> > you can declare them ordered factors and make the 
> computation feasible
> > once again.
> > 
> > Bill Venables.
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Torsten Hothorn
> > Sent: Monday, 5 April 2004 4:27 PM
> > To: David L. Van Brunt, Ph.D.
> > Cc: R-Help
> > Subject: Re: [R] Can't seem to finish a randomForest.... 
> Just goes and
> > goes!
> > 
> > 
> > On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:
> > 
> >> Playing with randomForest, samples run fine. But on real 
> data, no go.
> >> 
> >> Here's the setup: OS X, same behavior whether I'm using 
> R-Aqua 1.8.1
> >> or the Fink compile-of-my-own with X-11, R version 1.8.1.
> >> 
> >> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical
> >> RAM.
> >> 
> >> I have not altered the Startup options of R.
> >> 
> >> Data set is read in from a text file with "read.table", and has 46
> >> variables and 1,855 cases. Trying the following:
> >> 
> >> The DV is categorical, 0 or 1. Most of the IV's are either 
> continuous,
> > 
> >> or correctly read in as factors. The largest factor has 30 
> levels....
> >> Only the
> >                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> > 
> > This means: there are 2^(30-1) = 536.870.912 possible splits to be
> > evaluated everytime this variable is picked up (minus 
> something due to
> > empty levels). At least the last time I looked at the code, 
> randomForest
> > used an exhaustive search over all possible splits. Try reducing the
> > number of levels to something reasonable (or for a first 
> shot: remove
> > this variable from the learning sample).
> > 
> > Best,
> > 
> > Torsten
> > 
> > 
> >> DV seems to need identifying as a factor to force class trees over
> >> regresssion:
> >> 
> >>> Mydata$V46<-as.factor(Mydata$V46)
> >>> 
> Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximi
> >>> ties=FALSE
> >> , importance=FALSE)
> >> 
> >> 5 hours later, R.bin was still taking up 75% of my processor.  When
> >> I've tried this with larger data, I get errors referring 
> to the buffer
> > 
> >> (sorry, not in front of me right now).
> >> 
> >> Any ideas on this? The data don't seem horrifically large. 
> Seems like
> >> there are a few options for setting memory size, but I'm  not sure
> >> which of them to try tweaking, or if that's even the issue.
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >> 
> >> 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> David L. Van Brunt, Ph.D.
> Outlier Consulting & Development
> mailto: <ocd at well-wired.com>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdhyok at catchlab.org  Tue Apr  6 03:32:38 2004
From: sdhyok at catchlab.org (Shin, Daehyok)
Date: Mon, 5 Apr 2004 21:32:38 -0400
Subject: [R] Deep copy in R
In-Reply-To: <Pine.A41.4.58.0404051405230.108666@homer03.u.washington.edu>
Message-ID: <OAEOKPIGCLDDHAEMCAKIAEDMCKAA.sdhyok@catchlab.org>

Hm. Smarter than I expected.
But, any special reason why the 6th line clones another vector?
To me, just reference copy seems to be enough for the purpose.

6 xf = date.frame(x=x)

Daehyok Shin

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Thomas Lumley
> Sent: Monday, April 05, 2004 PM 5:18
> To: Shin, Daehyok
> Cc: R, Help
> Subject: Re: [R] Deep copy in R
>
>
> On Mon, 5 Apr 2004, Shin, Daehyok wrote:
>
> > I am handling spatial data of huge volumes,
> > so sensitive to the silent duplication of data in script programs.
> > In the following R program, exactly when is the vector data
> deeply copied?
> > Thanks in advance.
> >
> > 1 v <- 1:10000
> > 2 z <- f(v)
> >
> > --------- function f ----------
> > 3 f <- function(x) {
> > 4   y = x
> > 5   y[10] = 1
> > 6   xf = date.frame(x=x)
> > 7   xf$x[10] = 1
> > 8   return(y)
> > }
> >
>
> There will be a copy at line 5 and at line 6 or line 7 or both.
>
> Copying occurs when there are (or could be) two references to an object
> and one of them is modified.
>
> Passing v to the function f creates a second reference to it (the local
> variable x), and then y is a third reference. Modifying y forces a copy so
> that x and v don't get modified
>
> The data.frame() call need not copy, but I think it actually does. If it
> does, modifying xf$x need not copy.
>
> Returning y does not copy, and xf is discarded for garbage collection when
> the function exits.
>
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sdhyok at email.unc.edu  Tue Apr  6 03:35:36 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 5 Apr 2004 21:35:36 -0400
Subject: [R] A package to read and write NetCDF?
Message-ID: <OAEOKPIGCLDDHAEMCAKIEEDMCKAA.sdhyok@email.unc.edu>

I am looking for a package to read and write NetCDF files.
NetCDF package says it can only read, not write.
Another package for the standard binary file format?

Daehyok Shin



From dvanbrunt at well-wired.com  Tue Apr  6 03:58:59 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Mon, 05 Apr 2004 20:58:59 -0500
Subject: [R] Can't seem to finish a randomForest.... Just goes and
	goes!
In-Reply-To: <B998A44C8986644EA8029CFE6396A924018444@exqld2-bne.qld.csiro.au>
Message-ID: <BC977913.74E1%dvanbrunt@well-wired.com>

Removing that first 39 level variable, the trees ran just fine. I had also
taken the shorter categoricals (day of week, for example) and read them in
as numerics.

Still working on it. Need that 30 level puppy in there somehow, but it
really is not anything like a rank... It is a nominal variable.

With numeric values, only assigning the outcome (last column) to be a factor
using "as.factor()" it runs fine, and fast.

I may be misusing this analysis. That first column is indeed nominal, and I
was including it because the data within that name are repeated observations
of that subject. But I suppose there's no guarantee that that information
would be selected, so what does that do to the forest?  Sigh. I'm not much
of a lumberjack. Logistic regression is more my style, but this is pretty
interesting stuff.

If interested, here's a link to the data;
http://www.well-wired.com/reflibrary/uploads/1081216314.txt

 

On 4/5/04 1:40, "Bill.Venables at csiro.au" <Bill.Venables at csiro.au> wrote:

> Alternatively, if you can arrive at a sensible ordering of the levels
> you can declare them ordered factors and make the computation feasible
> once again.
> 
> Bill Venables.
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Torsten Hothorn
> Sent: Monday, 5 April 2004 4:27 PM
> To: David L. Van Brunt, Ph.D.
> Cc: R-Help
> Subject: Re: [R] Can't seem to finish a randomForest.... Just goes and
> goes!
> 
> 
> On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:
> 
>> Playing with randomForest, samples run fine. But on real data, no go.
>> 
>> Here's the setup: OS X, same behavior whether I'm using R-Aqua 1.8.1
>> or the Fink compile-of-my-own with X-11, R version 1.8.1.
>> 
>> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical
>> RAM.
>> 
>> I have not altered the Startup options of R.
>> 
>> Data set is read in from a text file with "read.table", and has 46
>> variables and 1,855 cases. Trying the following:
>> 
>> The DV is categorical, 0 or 1. Most of the IV's are either continuous,
> 
>> or correctly read in as factors. The largest factor has 30 levels....
>> Only the
>                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> 
> This means: there are 2^(30-1) = 536.870.912 possible splits to be
> evaluated everytime this variable is picked up (minus something due to
> empty levels). At least the last time I looked at the code, randomForest
> used an exhaustive search over all possible splits. Try reducing the
> number of levels to something reasonable (or for a first shot: remove
> this variable from the learning sample).
> 
> Best,
> 
> Torsten
> 
> 
>> DV seems to need identifying as a factor to force class trees over
>> regresssion:
>> 
>>> Mydata$V46<-as.factor(Mydata$V46)
>>> Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximi
>>> ties=FALSE
>> , importance=FALSE)
>> 
>> 5 hours later, R.bin was still taking up 75% of my processor.  When
>> I've tried this with larger data, I get errors referring to the buffer
> 
>> (sorry, not in front of me right now).
>> 
>> Any ideas on this? The data don't seem horrifically large. Seems like
>> there are a few options for setting memory size, but I'm  not sure
>> which of them to try tweaking, or if that's even the issue.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> 
>> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
David L. Van Brunt, Ph.D.
Outlier Consulting & Development
mailto: <ocd at well-wired.com>



From andy_liaw at merck.com  Tue Apr  6 04:15:21 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 5 Apr 2004 22:15:21 -0400
Subject: [R] Can't seem to finish a randomForest.... Just goes and goe s!
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B37@usrymx25.merck.com>

If that variable is a subject ID, and the data are repeated observations on
the subjects, then you might be treading on thin ice here.  A while back
someone at NCI got a data set with two reps per subject, and he was able to
modify the code so that the bootstrap is done on the subject basis, rather
than observations.  It's a bit of work trying to get a proximity matrix to
make sense, though.

I really have no idea how to take care of repeated measures type data (i.e.,
accounting for intra-subject correlations) in a classification problem.  I
suppose one can formulate it as a GLMM.  I guess it really depends on what
you are looking for; i.e., what's the goal?  I assume you want to predict
something, but is that over all subjects, or subject-specific?  I better
stop here, as this is out of my league...

Andy

> From: David L. Van Brunt, Ph.D. [mailto:dvanbrunt at well-wired.com] 
> 
> Removing that first 39 level variable, the trees ran just 
> fine. I had also
> taken the shorter categoricals (day of week, for example) and 
> read them in
> as numerics.
> 
> Still working on it. Need that 30 level puppy in there somehow, but it
> really is not anything like a rank... It is a nominal variable.
> 
> With numeric values, only assigning the outcome (last column) 
> to be a factor
> using "as.factor()" it runs fine, and fast.
> 
> I may be misusing this analysis. That first column is indeed 
> nominal, and I
> was including it because the data within that name are 
> repeated observations
> of that subject. But I suppose there's no guarantee that that 
> information
> would be selected, so what does that do to the forest?  Sigh. 
> I'm not much
> of a lumberjack. Logistic regression is more my style, but 
> this is pretty
> interesting stuff.
> 
> If interested, here's a link to the data;
> http://www.well-wired.com/reflibrary/uploads/1081216314.txt
> 
>  
> 
> On 4/5/04 1:40, "Bill.Venables at csiro.au" 
> <Bill.Venables at csiro.au> wrote:
> 
> > Alternatively, if you can arrive at a sensible ordering of 
> the levels
> > you can declare them ordered factors and make the 
> computation feasible
> > once again.
> > 
> > Bill Venables.
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Torsten Hothorn
> > Sent: Monday, 5 April 2004 4:27 PM
> > To: David L. Van Brunt, Ph.D.
> > Cc: R-Help
> > Subject: Re: [R] Can't seem to finish a randomForest.... 
> Just goes and
> > goes!
> > 
> > 
> > On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:
> > 
> >> Playing with randomForest, samples run fine. But on real 
> data, no go.
> >> 
> >> Here's the setup: OS X, same behavior whether I'm using 
> R-Aqua 1.8.1
> >> or the Fink compile-of-my-own with X-11, R version 1.8.1.
> >> 
> >> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical
> >> RAM.
> >> 
> >> I have not altered the Startup options of R.
> >> 
> >> Data set is read in from a text file with "read.table", and has 46
> >> variables and 1,855 cases. Trying the following:
> >> 
> >> The DV is categorical, 0 or 1. Most of the IV's are either 
> continuous,
> > 
> >> or correctly read in as factors. The largest factor has 30 
> levels....
> >> Only the
> >                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
> > 
> > This means: there are 2^(30-1) = 536.870.912 possible splits to be
> > evaluated everytime this variable is picked up (minus 
> something due to
> > empty levels). At least the last time I looked at the code, 
> randomForest
> > used an exhaustive search over all possible splits. Try reducing the
> > number of levels to something reasonable (or for a first 
> shot: remove
> > this variable from the learning sample).
> > 
> > Best,
> > 
> > Torsten
> > 
> > 
> >> DV seems to need identifying as a factor to force class trees over
> >> regresssion:
> >> 
> >>> Mydata$V46<-as.factor(Mydata$V46)
> >>> 
> Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximi
> >>> ties=FALSE
> >> , importance=FALSE)
> >> 
> >> 5 hours later, R.bin was still taking up 75% of my processor.  When
> >> I've tried this with larger data, I get errors referring 
> to the buffer
> > 
> >> (sorry, not in front of me right now).
> >> 
> >> Any ideas on this? The data don't seem horrifically large. 
> Seems like
> >> there are a few options for setting memory size, but I'm  not sure
> >> which of them to try tweaking, or if that's even the issue.
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >> 
> >> 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> David L. Van Brunt, Ph.D.
> Outlier Consulting & Development
> mailto: <ocd at well-wired.com>
> 
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ririzarr at jhsph.edu  Tue Apr  6 04:29:13 2004
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Mon, 05 Apr 2004 22:29:13 -0400 (EDT)
Subject: [R] current env
Message-ID: <Pine.GSO.4.10.10404052223550.5968-100000@athena.biostat.jhsph.edu>

i want to do something like this:

f <- function(){
	data(iris)
}

and have f() not leave iris in the global env. i can see there is a env
argument in data() but i dont know what to send it so that it uses the
current environment and not the global env.

any help appriciated.

-r



From dvanbrunt at well-wired.com  Tue Apr  6 04:38:13 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Mon, 05 Apr 2004 21:38:13 -0500
Subject: [R] Can't seem to finish a randomForest.... Just goes and goe s!
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B37@usrymx25.merck.com>
Message-ID: <BC978245.74EC%dvanbrunt@well-wired.com>

Fortunately, I'm not interested in any of the explanatory uses of model
building, just getting a useful prediction. I have a lot of "explaining" on
my day job. For this, I just want to be right, don't care about the why. I
have thought of on alternative approach, not sure of the implications. I
could run each "subject" separately, and predict each separately. Get a
forest for one group, to predict this organization's behavior given its long
history. Repeat for each other. Then just compare the probabilities of
reaching the desired state. In a sense, that's kind of the goal anyway...
But since none of these groups operate in a vacuum, seems like it would be
nice to get the bigger picture in there somehow. Not sure if this
alternative is really workable.

Thanks for all of your hard work, Andy (and for your input Bill and
Torsten). It really is a fascinating approach.

Looks like to modify that bootstrapping, I'd  have to go into the Fortran
code, then? Hmmmm. Talk about being out of one's league!

On 4/5/04 21:15, "Liaw, Andy" <andy_liaw at merck.com> wrote:

> If that variable is a subject ID, and the data are repeated observations on
> the subjects, then you might be treading on thin ice here.  A while back
> someone at NCI got a data set with two reps per subject, and he was able to
> modify the code so that the bootstrap is done on the subject basis, rather
> than observations.  It's a bit of work trying to get a proximity matrix to
> make sense, though.
> 
> I really have no idea how to take care of repeated measures type data (i.e.,
> accounting for intra-subject correlations) in a classification problem.  I
> suppose one can formulate it as a GLMM.  I guess it really depends on what
> you are looking for; i.e., what's the goal?  I assume you want to predict
> something, but is that over all subjects, or subject-specific?  I better
> stop here, as this is out of my league...
> 
> Andy
> 
>> From: David L. Van Brunt, Ph.D. [mailto:dvanbrunt at well-wired.com]
>> 
>> Removing that first 39 level variable, the trees ran just
>> fine. I had also
>> taken the shorter categoricals (day of week, for example) and
>> read them in
>> as numerics.
>> 
>> Still working on it. Need that 30 level puppy in there somehow, but it
>> really is not anything like a rank... It is a nominal variable.
>> 
>> With numeric values, only assigning the outcome (last column)
>> to be a factor
>> using "as.factor()" it runs fine, and fast.
>> 
>> I may be misusing this analysis. That first column is indeed
>> nominal, and I
>> was including it because the data within that name are
>> repeated observations
>> of that subject. But I suppose there's no guarantee that that
>> information
>> would be selected, so what does that do to the forest?  Sigh.
>> I'm not much
>> of a lumberjack. Logistic regression is more my style, but
>> this is pretty
>> interesting stuff.
>> 
>> If interested, here's a link to the data;
>> http://www.well-wired.com/reflibrary/uploads/1081216314.txt
>> 
>>  
>> 
>> On 4/5/04 1:40, "Bill.Venables at csiro.au"
>> <Bill.Venables at csiro.au> wrote:
>> 
>>> Alternatively, if you can arrive at a sensible ordering of
>> the levels
>>> you can declare them ordered factors and make the
>> computation feasible
>>> once again.
>>> 
>>> Bill Venables.
>>> 
>>> -----Original Message-----
>>> From: r-help-bounces at stat.math.ethz.ch
>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Torsten Hothorn
>>> Sent: Monday, 5 April 2004 4:27 PM
>>> To: David L. Van Brunt, Ph.D.
>>> Cc: R-Help
>>> Subject: Re: [R] Can't seem to finish a randomForest....
>> Just goes and
>>> goes!
>>> 
>>> 
>>> On Sun, 4 Apr 2004, David L. Van Brunt, Ph.D. wrote:
>>> 
>>>> Playing with randomForest, samples run fine. But on real
>> data, no go.
>>>> 
>>>> Here's the setup: OS X, same behavior whether I'm using
>> R-Aqua 1.8.1
>>>> or the Fink compile-of-my-own with X-11, R version 1.8.1.
>>>> 
>>>> This is on OS X 10.3 (aka "Panther"), G4 800Mhz with 512M physical
>>>> RAM.
>>>> 
>>>> I have not altered the Startup options of R.
>>>> 
>>>> Data set is read in from a text file with "read.table", and has 46
>>>> variables and 1,855 cases. Trying the following:
>>>> 
>>>> The DV is categorical, 0 or 1. Most of the IV's are either
>> continuous,
>>> 
>>>> or correctly read in as factors. The largest factor has 30
>> levels....
>>>> Only the
>>>                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>>> 
>>> This means: there are 2^(30-1) = 536.870.912 possible splits to be
>>> evaluated everytime this variable is picked up (minus
>> something due to
>>> empty levels). At least the last time I looked at the code,
>> randomForest
>>> used an exhaustive search over all possible splits. Try reducing the
>>> number of levels to something reasonable (or for a first
>> shot: remove
>>> this variable from the learning sample).
>>> 
>>> Best,
>>> 
>>> Torsten
>>> 
>>> 
>>>> DV seems to need identifying as a factor to force class trees over
>>>> regresssion:
>>>> 
>>>>> Mydata$V46<-as.factor(Mydata$V46)
>>>>> 
>> Myforest.rf<-randomForest(V46~.,data=Mydata,ntrees=100,mtry=7,proximi
>>>>> ties=FALSE
>>>> , importance=FALSE)
>>>> 
>>>> 5 hours later, R.bin was still taking up 75% of my processor.  When
>>>> I've tried this with larger data, I get errors referring
>> to the buffer
>>> 
>>>> (sorry, not in front of me right now).
>>>> 
>>>> Any ideas on this? The data don't seem horrifically large.
>> Seems like
>>>> there are a few options for setting memory size, but I'm  not sure
>>>> which of them to try tweaking, or if that's even the issue.
>>>> 
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>> 
>>>> 
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>> 
>> -- 
>> David L. Van Brunt, Ph.D.
>> Outlier Consulting & Development
>> mailto: <ocd at well-wired.com>
>> 
>> 
>> 
>> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
David L. Van Brunt, Ph.D.
Outlier Consulting & Development
mailto: <ocd at well-wired.com>



From rpeng at jhsph.edu  Tue Apr  6 05:15:28 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 05 Apr 2004 23:15:28 -0400
Subject: [R] current env
In-Reply-To: <Pine.GSO.4.10.10404052223550.5968-100000@athena.biostat.jhsph.edu>
References: <Pine.GSO.4.10.10404052223550.5968-100000@athena.biostat.jhsph.edu>
Message-ID: <407220D0.60504@jhsph.edu>

I think you want

f <- function() {
    data(iris, envir = environment())
}

-roger

Rafael A. Irizarry wrote:
> i want to do something like this:
> 
> f <- function(){
> 	data(iris)
> }
> 
> and have f() not leave iris in the global env. i can see there is a env
> argument in data() but i dont know what to send it so that it uses the
> current environment and not the global env.
> 
> any help appriciated.
> 
> -r
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hodgess at gator.uhd.edu  Tue Apr  6 06:09:09 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 5 Apr 2004 23:09:09 -0500
Subject: [R] off topic time series
Message-ID: <200404060409.i36499w27279@gator.dt.uh.edu>

Dear R+ People:

Does anyone know, please:  Would there be any interest in an 
undergraduate time series book (very applied) with R, please?

Or does one alrealy exist (or is in the works), please?

Maybe Kevin Ko-Kang is working on one.......

Thanks for any advice,
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From andrewr at uidaho.edu  Tue Apr  6 06:12:40 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 5 Apr 2004 20:12:40 -0800
Subject: [R] Curious about nomenclature: random deviates
In-Reply-To: <406075A8.2010000@mun.ca>
References: <200403221102.i2MB2bT8027973@hypatia.math.ethz.ch>
	<406075A8.2010000@mun.ca>
Message-ID: <200404052112.40419.andrewr@uidaho.edu>

Hi all,

a student of mine recently stumbled whilst reading the R help files for the 
statistical distributions.  She was confused by their assertion that, for 
example, 'rnorm' generates random deviates.  I have seen this label used 
elsewhere, although it does not seem universal - for example, Ripley (1987) 
doesn't have it in the index. Does anyone know why they're called random 
deviates, as opposed to random numbers?

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.

Cited:

Ripley, B.D. 1987. Stochastic Simulation. Wiley-Interscience;



From Eric.Kort at vai.org  Tue Apr  6 06:15:26 2004
From: Eric.Kort at vai.org (Kort, Eric)
Date: Tue, 6 Apr 2004 00:15:26 -0400
Subject: [R] Problems with Rcmd INSTALL on Win32
Message-ID: <74D0F0AB07F2E647A02D839ED79520F92088C2@VAIEXCH02.vai.org>

Hello.  When I try to use Rcmd INSTALL to install a package (whether my
own or someone else's) for the Windows version of R, I get the following
error:

C:\>Rcmd INSTALL cluster_1.8.1.tar.gz


---------- Making package cluster ------------
  adding build stamp to DESCRIPTION
Fatal error: you must specify `--save', `--no-save' or `--vanilla'

make[1]: *** [frontmatter] Error 2
make: *** [pkg-cluster] Error 2
*** Installation of cluster failed ***

So I tried:

C:\>Rcmd INSTALL --no-save cluster_1.8.1.tar.gz

But got the same error.  I do not experience this with R CMD INSTALL
under FreeBSD.

I am running R 1.8.1, perl 5.8.0, gcc 3.2.3, and have tar, make, etc
installed.

Would someone point out to me what I am doing wrong?

Thanks,
Eric
This email message, including any attachments, is for the so...{{dropped}}



From 0034058 at fudan.edu.cn  Tue Apr  6 06:19:36 2004
From: 0034058 at fudan.edu.cn (vinkwai wong)
Date: Tue, 06 Apr 2004 12:19:36 +0800
Subject: [R] how to learn R quickly?
Message-ID: <0HVQ00BKQEFVTE@mail.fudan.edu.cn>

i am new to R,so i post here to ask you how to learn R quickly?
is there some usefull materials about using R.
ps:though the manual is a good materials,but it is not the best for a newbie,i think.

thank you !



From edd at debian.org  Tue Apr  6 06:21:11 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 5 Apr 2004 23:21:11 -0500
Subject: [R] off topic time series
In-Reply-To: <200404060409.i36499w27279@gator.dt.uh.edu>
References: <200404060409.i36499w27279@gator.dt.uh.edu>
Message-ID: <20040406042111.GA32254@sonny.eddelbuettel.com>

On Mon, Apr 05, 2004 at 11:09:09PM -0500, Erin Hodgess wrote:
> Does anyone know, please:  Would there be any interest in an 
> undergraduate time series book (very applied) with R, please?
> 
> Or does one alrealy exist (or is in the works), please?

http://www.uea.ac.uk/~gj/tsbook.html    

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From aragon at berkeley.edu  Tue Apr  6 06:26:46 2004
From: aragon at berkeley.edu (Tomas Aragon)
Date: Mon, 5 Apr 2004 21:26:46 -0700 (PDT)
Subject: [R] how to learn R quickly?
In-Reply-To: <0HVQ00BKQEFVTE@mail.fudan.edu.cn>
Message-ID: <20040406042646.93004.qmail@web109.biz.mail.yahoo.com>

--- vinkwai wong <0034058 at fudan.edu.cn> wrote:
> i am new to R,so i post here to ask you how to learn R quickly?
> is there some usefull materials about using R.
> ps:though the manual is a good materials,but it is not the best for a
> newbie,i think.
> 
> thank you !
> 
Try http://www.ucbcidp.org/courses/epiwithr.pdf

Early draft of tutorial written for public health epidemiologists.
Tomas

=====



From edd at debian.org  Tue Apr  6 06:27:43 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 5 Apr 2004 23:27:43 -0500
Subject: [R] how to learn R quickly?
In-Reply-To: <0HVQ00BKQEFVTE@mail.fudan.edu.cn>
References: <0HVQ00BKQEFVTE@mail.fudan.edu.cn>
Message-ID: <20040406042743.GB32254@sonny.eddelbuettel.com>

On Tue, Apr 06, 2004 at 12:19:36PM +0800, vinkwai wong wrote:
> i am new to R,so i post here to ask you how to learn R quickly?
> is there some usefull materials about using R.
> ps:though the manual is a good materials,but it is not the best for a newbie,i think.

http://www.burns-stat.com/pages/Tutor/unwilling_S.pdf

R, and the S language, are very rich in features. With that comes a nominal
amount of complexity, and, I'd conjecture, a lack of shortcuts. 

Invest in learning the language, chances are you'll get a lot of payback.

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From ligges at statistik.uni-dortmund.de  Tue Apr  6 08:50:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Apr 2004 08:50:08 +0200
Subject: [R] Problems with Rcmd INSTALL on Win32
In-Reply-To: <74D0F0AB07F2E647A02D839ED79520F92088C2@VAIEXCH02.vai.org>
References: <74D0F0AB07F2E647A02D839ED79520F92088C2@VAIEXCH02.vai.org>
Message-ID: <40725320.9090502@statistik.uni-dortmund.de>

Kort, Eric wrote:

> Hello.  When I try to use Rcmd INSTALL to install a package (whether my
> own or someone else's) for the Windows version of R, I get the following
> error:
> 
> C:\>Rcmd INSTALL cluster_1.8.1.tar.gz
> 
> 
> ---------- Making package cluster ------------
>   adding build stamp to DESCRIPTION
> Fatal error: you must specify `--save', `--no-save' or `--vanilla'
> 
> make[1]: *** [frontmatter] Error 2
> make: *** [pkg-cluster] Error 2
> *** Installation of cluster failed ***
> 
> So I tried:
> 
> C:\>Rcmd INSTALL --no-save cluster_1.8.1.tar.gz
> 
> But got the same error.  I do not experience this with R CMD INSTALL
> under FreeBSD.
> 
> I am running R 1.8.1, perl 5.8.0, gcc 3.2.3, and have tar, make, etc
> installed.
> 
> Would someone point out to me what I am doing wrong?
> 
> Thanks,
> Eric
> This email message, including any attachments, is for the so...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Please read the R for Windows FAQs:
"3.1 Can I install packages (libraries) in this version?"
and the file "readme.packages" in the R sources.

[In your case, I guess you have blanks in your path to R.]

Uwe Ligges



From Saghir.Bashir at UCB-Group.com  Tue Apr  6 08:56:37 2004
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Tue, 6 Apr 2004 08:56:37 +0200 
Subject: [R] how to learn R quickly?
Message-ID: <3EBA5559F490D61189430002A5F0AE89056325B9@ntexcrd.braine.ucb>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040406/8250c6ff/attachment.pl

From crambes at cict.fr  Tue Apr  6 09:03:14 2004
From: crambes at cict.fr (Christophe Crambes)
Date: Tue, 06 Apr 2004 09:03:14 +0200
Subject: [R] search for a library
Message-ID: <40725632.E2FA01C2@cict.fr>

Hello

I am looking for a dynamic linked library called "splines.dll". Where
can I find it ?
Thank you very much.

    Christophe Crambes



From ligges at statistik.uni-dortmund.de  Tue Apr  6 09:14:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Apr 2004 09:14:20 +0200
Subject: [R] search for a library
In-Reply-To: <40725632.E2FA01C2@cict.fr>
References: <40725632.E2FA01C2@cict.fr>
Message-ID: <407258CC.6070907@statistik.uni-dortmund.de>

Christophe Crambes wrote:

> Hello
> 
> I am looking for a dynamic linked library called "splines.dll". Where
> can I find it ?
> Thank you very much.
> 
>     Christophe Crambes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It's part of the Windows binary version of R (package splines). Please 
note the license, if you are going to use that DLL to link other 
software against it ...

Uwe Ligges



From Kersten.Magg at web.de  Tue Apr  6 10:04:01 2004
From: Kersten.Magg at web.de (Kersten Magg)
Date: Tue, 06 Apr 2004 10:04:01 +0200
Subject: [R] warning messages
Message-ID: <1205257932@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
X-Virus-Scanned: by amavisd-new
X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, hits=0.0 required=5.0 tests=none autolearn=no version=2.63

Hello,

I've got some problems with warning messages. In the context of complex statistical simulations I use among other things the function glm(). During large replications in the simulation study I get sometimes different kinds of warning messages. For example: model did not converge or fitted probabilities 0 or 1, ... 
My question is now: how can I handle these messages? I want to transform the special warning message in a binary code and want to save it in a vector of warnings. The problem I've got is that
the command last.warning doesn't work if I start the simulationen with source("..."). The function glm() is part of R-programs in second or third level. I think the top-level function is the cause of 
my problems but I don't know how I can solve the problem.

Could you help me?!
Thanks a lot!

Kersten
_______________________________________________________________________
... and the winner is... WEB.DE FreeMail! - Deutschlands beste E-Mail



From angel_lul at hotmail.com  Tue Apr  6 11:23:38 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Tue, 06 Apr 2004 10:23:38 +0100
Subject: [R] A package to read and write NetCDF?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIEEDMCKAA.sdhyok@email.unc.edu>
References: <OAEOKPIGCLDDHAEMCAKIEEDMCKAA.sdhyok@email.unc.edu>
Message-ID: <4072771A.6050708@hotmail.com>


ncdf: Interface to Unidata netCDF data files

This package provides a high-level R interface to Unidata's netCDF data 
files, which are portable across platforms and include metadata 
information in addition to the data sets. Using this package netCDF 
files can be opened and data sets read in easily. It is also easy to 
create new netCDF dimensions, variables, and files, or manipulate 
existing netCDF files. This interface provides considerably more 
functionality than the "netCDF" package. It is not compatible with the 
"netCDF" package.

Shin, Daehyok wrote:
> I am looking for a package to read and write NetCDF files.
> NetCDF package says it can only read, not write.
> Another package for the standard binary file format?
> 
> Daehyok Shin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> .
>



From angel_lul at hotmail.com  Tue Apr  6 11:30:32 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Tue, 06 Apr 2004 10:30:32 +0100
Subject: [R] k nearest neighbours
Message-ID: <407278B8.6050008@hotmail.com>

I want to
1) Select for each of the n points in a matrix A, those of the m points 
in B that lay within a given radius.
2) Of those points within the radius, select the k nearest ones.

What I now do is
1) Create an n*m matrix C were I put the distances from all the points 
in B to the points in A and make NA those cells were the distance is 
larger than the radius. (The points are geographical locations so I use 
function rdist.earth in package fields) e.g.:
library(fields)
data(ozone)
A<-cbind(ozone$lon.lat[1:10,])
B<-cbind(ozone$lon.lat+2)
C<-rdist.earth(A,B)
radius<-180 # The search radius
C[which(C>radius)]<-NA

2) Then I make NA everything but the k nearest ones
k<-5 # The nearest neighbours
rank<-function(rank){rank<-sort.list(sort.list(rank,))};
C[which(apply(C,2,rank)>k)]<-NA;

My problem is that the code is quite slow and due to the need to create 
a n*m matrix I run out of memory many times. I would also prefer to get 
a C matrix that is n*k instead of n*m were each of the values in C 
indicated the row in B were the corresponding knearest point would be.
But I can not find a way to solve my main problem which is the need to 
create a n*m matrix.
Thanks for any clues,
Angel



From maechler at stat.math.ethz.ch  Tue Apr  6 10:30:08 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Apr 2004 10:30:08 +0200
Subject: [R] x-only zoom and pan?
In-Reply-To: <16497.36041.571792.929115@gargle.gargle.HOWL>
References: <Pine.GSO.4.44.0404042046350.5033-100000@freke.odin.pdx.edu>
	<40711BB4.4000303@wiwi.uni-bielefeld.de>
	<16497.36041.571792.929115@gargle.gargle.HOWL>
Message-ID: <16498.27280.689239.779383@gargle.gargle.HOWL>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 5 Apr 2004 18:43:53 +0200 writes:

>>>>> "Peter" == Peter Wolf <s-plus at wiwi.uni-bielefeld.de>
>>>>>     on Mon, 05 Apr 2004 10:41:24 +0200 writes:

    Peter> Hallo here is a simple proposal using tcltk-sliders.
    Peter> Peter Wolf

    MM> Peter, that is interesting (and nice in its generality and programming
    MM> interface),
    MM> however for a bit more intuitive *user* interface,
    MM> I slightly prefer the way I have done it for the  'tkdensity' function
    MM> I made (from Peter Dalgaard's original tkdensity demo).

    MM> Try

    MM> install.package("sfsmisc")
    MM> example(tkdensity)

and of course, that needs a  
       library(sfsmisc)
between the installation and the example call...

Thank you, Uwe  {and "how embarassing..."}!
Martin



From k.wang at auckland.ac.nz  Tue Apr  6 10:48:48 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 6 Apr 2004 20:48:48 +1200
Subject: [R] Nested Loop in R
References: <OF192DD6B2.92DA00FC-ON85256E6D.0080D5D5@nd.convergys.com>
Message-ID: <002301c41bb3$f9fc8b70$6633d882@stat.auckland.ac.nz>

Thanks for those who replied!

I found out the problem.  I don't need the nested loop at all.  The
following code would've worked (for those who're interested to know):
#####
## Testing CLT.  pp 281 of Chance Encounters
x <- round(runif(1000, 1, 6))
clt <- function(x, samp.no = 5, n = 5) {
  samp <- vector(mode = "list", length = samp.no)
  plot.new()
  plot.window(xlim = c(1, 6), ylim = c(1, samp.no))
  axis(1, at = 1:6)
  axis(2, at = 1:samp.no, labels = samp.no:1, las = 1)

  for(i in 1:samp.no) {
    samp[[i]] <- sample(x, n)
    points(jitter(samp[[i]], 0.3), rep(i, n))
    lines(x = c(mean(samp[[i]]), mean(samp[[i]])),
          y = c(i - 0.1, i + 0.1))
  }
  title(main = paste(samp.no, " samples of size n = ", n, sep = ""),
        xlab = "6-Sided Die",
        ylab = "Sample Number")
}

## 5 Samples
par(mfrow = c(2, 2), bg = "white")
clt(x, samp.no = 5, n = 5)
clt(x, samp.no = 5, n = 10)
clt(x, samp.no = 5, n = 20)
clt(x, samp.no = 5, n = 30)
#####



From ccleland at optonline.net  Tue Apr  6 11:20:22 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 06 Apr 2004 05:20:22 -0400
Subject: [R] 2 lme questions
In-Reply-To: <4071E3AE.5040003@pdf.com>
References: <CFF85773D9245040A333571B7E6D651702C49262@ccssosrv1.ccsso.org>
	<4071E3AE.5040003@pdf.com>
Message-ID: <40727656.1030707@optonline.net>

Spencer Graves wrote:
>      The "print" method and the "summary" command display the STANDARD 
> DEVIATIONS (not the variances) on the screen (or in a sink file).  
> However, when I do attributes(lme(...)) and 
> attributes(summary(lme(...))), I don't see anything I can use.  
> Fortunately, the "interval" function produces a list, from which the 
> variance estimates can be extracted.  Consider the following example:
> DF <- data.frame(group=c(1,1,2,2), y=c(1, 2, 11, 12))
> library(nlme)
> fit <- lme(y~1, random=~1|group, DF)
> 
> Linear mixed-effects model fit by REML
>  Data: DF
>  Log-restricted-likelihood: -6.559401
>  Fixed: y ~ 1
> (Intercept)
>        6.5
> 
> Random effects:
> Formula: ~1 | group
>        (Intercept)  Residual
> StdDev:    7.053597 0.7070954
> 
> Number of Observations: 4
> Number of Groups: 2
>  > lme.int <- intervals(fit)
>  > lme.int$reStruct^2
> Error in lme.int$reStruct^2 : non-numeric argument to binary operator
> 
>  > lme.int$reStruct$group^2
>                  lower     est.    upper
> sd((Intercept)) 3.06859 49.75323 806.6845
>  > lme.int$sigma^2
>    lower      est.     upper
> 0.0704255 0.4999839 3.5496217
> attr(,"label")
> [1] "Within-group standard error:"
> 
>      There may be a better way;  if there is, I hope someone will 
> enlighten us all.  If not, at least this works in R 1.8.1

   How about VarCorr(fit) ?

See ?VarCorr

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From sb at ihe.se  Tue Apr  6 11:35:23 2004
From: sb at ihe.se (Sixten Borg)
Date: Tue, 06 Apr 2004 11:35:23 +0200
Subject: [R] Extracting the survival function estimate from a survreg object.
Message-ID: <s0729613.003@gwmail.ihe.se>


Hello all,

I want to extract the survival function estimate from a model fitted by survreg().
Using predict.survreg(..., type="quantile", p=seq(0,1,0.001)), gives the quantiles, which
I managed to turn around into a survival function estimate (Prob{T > t} as function of t).

Is there a more straightforward way of doing this? I have had difficulties using pweibull() with
the coefficients reported by summary(<survreg object>).

I am enclosing an outline of my code for reference if anyone is interested.

Thanks in advance,

Sixten

---------------------------------------------------------------------
sure <- survreg(formula = Surv(time, dead) ~ age + group + sex, data = modb)

nd <- data.frame(
  age=50, 
  group=factor("A", levels=c("A", "B", "C")),
  sex=factor("M", levels=c("F", "M")))

y <- seq(0, 1, 0.001)

#
# For a range of p-values, predict the quantiles.
#
sufu <- list(
  y=1-y,
  x=predict.survreg(sure, nd, type="quantile", p=y)
)

#
# Find the p value for each t, by locating the last quantile no larger than t.
# The pairs (t,p) forms a step function.
#
sufu2 <- list(
  x=0:14,
  y=unlist(lapply(0:14,function(x){rev(sufu$y[sufu$x<=x])[1]}))
)

# Looks ok?
plot(sufu, type="s", xlim=c(0,20))
points(sufu2,  pch=20, col="blue")

#EOF#



From song.baiyi at udo.edu  Tue Apr  6 12:26:49 2004
From: song.baiyi at udo.edu (Song Baiyi)
Date: Tue, 06 Apr 2004 12:26:49 +0200
Subject: [R] A question about embedded R
Message-ID: <407285E9.8000302@udo.edu>

Hello everyone,

I met a strange problem when I call R expression from C++, here is the 
details:

I edited a function eval_R_command();

double eval_R_command(const char *funcName)
{
    SEXP exp,e;
    ParseStatus status = PARSE_OK;
    int i,n;
    double val;

    PROTECT(exp=NEW_CHARACTER(1));
    SET_STRING_ELT(exp, 0, COPY_TO_USER_STRING(funcName));
   
    PROTECT(e = R_ParseVector(exp, 1, &status));
    n = GET_LENGTH(e);
    for(i = 0; i < n ; i++)
        val = (double)REAL(eval(VECTOR_ELT(e,i), R_GlobalEnv))[0];
    UNPROTECT(2);

    return(val);
}

Then if i call with

eval_R_command("mean(1:5)")
it worked fine and got 3,

but

eval_R_command("sum(1:5)")
it produced a strange number 7.41098e-323! not 15

What is the problem?
By the way which is the better way to call the function in R from C++, 
or they are same?

1. Pass the whole expression as string to R
2. Use Rf_findFun

Thank you very much!

Baiyi Song

CEI
Dortmund University, Germany



From ozric at web.de  Tue Apr  6 12:49:22 2004
From: ozric at web.de (Christian Schulz)
Date: Tue, 6 Apr 2004 12:49:22 +0200
Subject: [R] merge more than two tables
Message-ID: <200404061249.22618.ozric@web.de>

Hi 

have anybody a hint how i could merge  i.e.  10 data.frames togehter.
Ok i could do one merge after another but perhaps with few coding exist a 
better way?

system.time(dm1 <- merge(Base,sub1 ,by.x ="MEMBERNO",by.y="MEMBERNO",all.x=T))
system.time(dm2 <- merge(dm2,sub2,by.x="MEMBERNO",by.y="MEMBERNO",all.x=T))
system.time(dm3 <- merge(dm2,sub3,by.x="MEMBERNO",by.y="MEMBERNO",all.x=T))
....................

many thanks , christian



From p.dalgaard at biostat.ku.dk  Tue Apr  6 12:59:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Apr 2004 12:59:07 +0200
Subject: [R] A question about embedded R
In-Reply-To: <407285E9.8000302@udo.edu>
References: <407285E9.8000302@udo.edu>
Message-ID: <x2n05ptls4.fsf@biostat.ku.dk>

Song Baiyi <song.baiyi at udo.edu> writes:

>     for(i = 0; i < n ; i++)
>         val = (double)REAL(eval(VECTOR_ELT(e,i), R_GlobalEnv))[0];
...
> eval_R_command("mean(1:5)")
> it worked fine and got 3,
> 
> but
> 
> eval_R_command("sum(1:5)")
> it produced a strange number 7.41098e-323! not 15
> 
> What is the problem?

You have a sum of integers, so the result is integer and you access it
as a REAL. You need explicit coercion, or to use
eval_R_command("as.double(sum(1:5))") 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From R.A.Sanderson at newcastle.ac.uk  Tue Apr  6 14:35:39 2004
From: R.A.Sanderson at newcastle.ac.uk (Roy Sanderson)
Date: Tue, 06 Apr 2004 12:35:39 +0000
Subject: [R] Storing p-values from a glm
Message-ID: <3.0.3.32.20040406123539.00a97c90@popin.ncl.ac.uk>

Hello

I need to store the P-statistics from a number of glm objects.  Whilst it's
easy to display these on screen via the summary() function, I'm not clear
on how to extract the P-values and store them in a vector.

Many thanks
Roy

----------------------------------------------------------------------------
Roy Sanderson
Centre for Life Sciences Modelling
Porter Building
University of Newcastle
Newcastle upon Tyne
NE1 7RU
United Kingdom

Tel: +44 191 222 7789

r.a.sanderson at newcastle.ac.uk
http://www.ncl.ac.uk/clsm



From Eric.Kort at vai.org  Tue Apr  6 13:52:16 2004
From: Eric.Kort at vai.org (Kort, Eric)
Date: Tue, 6 Apr 2004 07:52:16 -0400
Subject: [R] Problems with Rcmd INSTALL on Win32
Message-ID: <74D0F0AB07F2E647A02D839ED79520F92088C3@VAIEXCH02.vai.org>

> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: Tuesday, April 06, 2004 2:50 AM
> To: Kort, Eric
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problems with Rcmd INSTALL on Win32
> 
> Kort, Eric wrote:
> 
> > Hello.  When I try to use Rcmd INSTALL to install a package (whether
my
> > own or someone else's) for the Windows version of R, I get the
following
> > error:

8< --SNIP-- >


> 
> Please read the R for Windows FAQs:
> "3.1 Can I install packages (libraries) in this version?"
> and the file "readme.packages" in the R sources.
> 
> [In your case, I guess you have blanks in your path to R.]
> 
> Uwe Ligges
> 

No spaces...but replacing some of my "unix" tools with the set at
http://www.stats.ox.ac.uk/pub/Rtools/tools.zip took care of it.

Thanks for your help, sorry for the noise.

Eric
This email message, including any attachments, is for the so...{{dropped}}



From dmurdoch at pair.com  Tue Apr  6 14:01:38 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 06 Apr 2004 08:01:38 -0400
Subject: [R] Storing p-values from a glm
In-Reply-To: <3.0.3.32.20040406123539.00a97c90@popin.ncl.ac.uk>
References: <3.0.3.32.20040406123539.00a97c90@popin.ncl.ac.uk>
Message-ID: <is657059uqtttcs8oa5rogmp5182i0d427@4ax.com>

On Tue, 06 Apr 2004 12:35:39 +0000, Roy Sanderson
<R.A.Sanderson at newcastle.ac.uk> wrote :

>Hello
>
>I need to store the P-statistics from a number of glm objects.  Whilst it's
>easy to display these on screen via the summary() function, I'm not clear
>on how to extract the P-values and store them in a vector.

Generally when summary() functions display something, they also return
it as part of their value.  

In the case of summary() for glm objects, the table of coefficients
(including estimates, std error, z value, and p value) is returned as
a matrix in summary(glm.object)$coefficients.  You can extract the
p-values as

summary(glm.object)$coefficients[,4]

Duncan Murdoch



From Arne.Muller at aventis.com  Tue Apr  6 14:05:27 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Tue, 6 Apr 2004 14:05:27 +0200
Subject: [R] Storing p-values from a glm
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF18E@crbsmxsusr04.pharma.aventis.com>

Hi,

for example one could do it this way:

v <- summary(fit)$coefficients[,4]

the coefficient attribute is a matrix, and with the "4" you refere to the
pvalue (at least in lm - don't know if summary(glm) produces sligthely
different output).

to skip the intercept (1st row): v <- summary(glmfit)$coefficients[-1,4]

	hope this helps,

	Arne

--
Arne Muller, Ph.D.
Toxicogenomics, Aventis Pharma
arne dot muller domain=aventis com

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Roy Sanderson
> Sent: 06 April 2004 14:36
> To: r-help at stat.math.ethz.ch
> Subject: [R] Storing p-values from a glm
> 
> 
> Hello
> 
> I need to store the P-statistics from a number of glm 
> objects.  Whilst it's
> easy to display these on screen via the summary() function, 
> I'm not clear
> on how to extract the P-values and store them in a vector.
> 
> Many thanks
> Roy
> 
> --------------------------------------------------------------
> --------------
> Roy Sanderson
> Centre for Life Sciences Modelling
> Porter Building
> University of Newcastle
> Newcastle upon Tyne
> NE1 7RU
> United Kingdom
> 
> Tel: +44 191 222 7789
> 
> r.a.sanderson at newcastle.ac.uk
> http://www.ncl.ac.uk/clsm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Tue Apr  6 14:27:30 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 06 Apr 2004 07:27:30 -0500
Subject: [R] Storing p-values from a glm
In-Reply-To: <is657059uqtttcs8oa5rogmp5182i0d427@4ax.com>
References: <3.0.3.32.20040406123539.00a97c90@popin.ncl.ac.uk>
	<is657059uqtttcs8oa5rogmp5182i0d427@4ax.com>
Message-ID: <6ry8p9nvf1.fsf@bates4.stat.wisc.edu>

Duncan Murdoch <dmurdoch at pair.com> writes:

> On Tue, 06 Apr 2004 12:35:39 +0000, Roy Sanderson
> <R.A.Sanderson at newcastle.ac.uk> wrote :
> 
> >Hello
> >
> >I need to store the P-statistics from a number of glm objects.  Whilst it's
> >easy to display these on screen via the summary() function, I'm not clear
> >on how to extract the P-values and store them in a vector.
> 
> Generally when summary() functions display something, they also return
> it as part of their value.  
> 
> In the case of summary() for glm objects, the table of coefficients
> (including estimates, std error, z value, and p value) is returned as
> a matrix in summary(glm.object)$coefficients.  You can extract the
> p-values as
> 
> summary(glm.object)$coefficients[,4]

An alternative and, I think, a preferred form is

 coef(summary(glm.object))

Someone writing a summary method for another class of fitted model
objects may not want to include a component called coefficients but
can certainly implement a coef method.



From rksh at soc.soton.ac.uk  Tue Apr  6 15:36:32 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 6 Apr 2004 13:36:32 +0000
Subject: [R] %+=% and eval.parent()
Message-ID: <a06002011bc9861cabf1d@[139.166.242.29]>


Some time ago, Peter Dalgaard made the wonderful suggestion to define

"%+=%" <- function(a,b) {eval.parent(substitute(a <- a + b)) }


Which I use (R-1.8.1) as follows:

R> a <- matrix(1:9,3,3)
R> a[a%%2==1] %+=% (1000*(1:5))
R> a
      [,1] [,2] [,3]
[1,] 1001    4 4007
[2,]    2 3005    8
[3,] 2003    6 5009
R>

which is what I want.  But the following caught me off-guard:


R> a <- matrix(1:9,3,3)
R> a[a%%2==1] %+=% 1000*(1:5)
[1] 1001 2006 3015 4028 5045
R> a
      [,1] [,2] [,3]
[1,] 1001    4 1007
[2,]    2 1005    8
[3,] 1003    6 1009
R>

Why the difference when I remove the brackets?
And how is the vector that is printed in the unbracketed version determined?
(why is this printed anyway? doing "a[a%%2==1] %+=% 1000" doesnt
print anything).

Is there a better way to code up %+=%  ?

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From p.dalgaard at biostat.ku.dk  Tue Apr  6 14:56:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Apr 2004 14:56:37 +0200
Subject: [R] %+=% and eval.parent()
In-Reply-To: <a06002011bc9861cabf1d@[139.166.242.29]>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
Message-ID: <x28yh9p8my.fsf@biostat.ku.dk>

Robin Hankin <rksh at soc.soton.ac.uk> writes:

> Some time ago, Peter Dalgaard made the wonderful suggestion to define
> 
> "%+=%" <- function(a,b) {eval.parent(substitute(a <- a + b)) }
...
> R> a <- matrix(1:9,3,3)
> R> a[a%%2==1] %+=% 1000*(1:5)
> [1] 1001 2006 3015 4028 5045
> R> a
>       [,1] [,2] [,3]
> [1,] 1001    4 1007
> [2,]    2 1005    8
> [3,] 1003    6 1009
> R>
> 
> Why the difference when I remove the brackets?
> And how is the vector that is printed in the unbracketed version determined?
> (why is this printed anyway? doing "a[a%%2==1] %+=% 1000" doesnt
> print anything).

You're getting caught out by operator precedence:

a[a%%2==1] %+=% 1000*(1:5)

is effectively

(a[a%%2==1] %+=% 1000)*(1:5)

because %foo% operators have higher precedence than * (and much higher
than ordinary assignment)

> Is there a better way to code up %+=%  ?

Not really. You could hack the parser, but from within R, there is no
way to set parser precedence.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Tue Apr  6 14:59:38 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 06 Apr 2004 08:59:38 -0400
Subject: [R] %+=% and eval.parent()
In-Reply-To: <a06002011bc9861cabf1d@[139.166.242.29]>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
Message-ID: <q2a570hiel5ilg76ulg1h4eengqskreqps@4ax.com>

On Tue, 6 Apr 2004 13:36:32 +0000, Robin Hankin <rksh at soc.soton.ac.uk>
wrote :

> the following caught me off-guard:
>
>
>R> a <- matrix(1:9,3,3)
>R> a[a%%2==1] %+=% 1000*(1:5)
>[1] 1001 2006 3015 4028 5045
>R> a
>      [,1] [,2] [,3]
>[1,] 1001    4 1007
>[2,]    2 1005    8
>[3,] 1003    6 1009
>R>
>
>Why the difference when I remove the brackets?

Looks to me like an operator precedence issue.  Your line is
equivalent to 

(a[a%%2==1] %+=% 1000)*(1:5)

This is described in the Language Manual, in Parser | Expressions |
Infix and prefix operators (section 10.4.2 in the PDF version).  The
user defined %xyz% operators have precedence just above * and /.

>And how is the vector that is printed in the unbracketed version determined?
>(why is this printed anyway? doing "a[a%%2==1] %+=% 1000" doesnt
>print anything).
>
>Is there a better way to code up %+=%  ?

Not really.  You can't control operator precedence in R. 

Duncan Murdoch



From asemeria at cramont.it  Tue Apr  6 15:06:02 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 6 Apr 2004 15:06:02 +0200
Subject: [R] %+=% and eval.parent()
Message-ID: <OF1E9469F0.2AAC946C-ONC1256E6E.0047F630@tomware.it>





Look at Syntax on R-help apropos operators precedence

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From datkins at u.washington.edu  Tue Apr  6 15:11:33 2004
From: datkins at u.washington.edu (Dave Atkins)
Date: Tue, 06 Apr 2004 06:11:33 -0700
Subject: [R] 2 lme questions
In-Reply-To: <200404061000.i36A0NaS014813@hypatia.math.ethz.ch>
References: <200404061000.i36A0NaS014813@hypatia.math.ethz.ch>
Message-ID: <4072AC85.3020009@u.washington.edu>


Below are a couple functions written by Jose Pinheiro and posted to the 
Nlme-help listserv (by someone else) a few years back.  They will extract the 
random effects matrix (for a given level) and the level-1 error.

I've used them periodically when I wanted to get at the variance components.

Hope that helps.

Dave

Dave Atkins
Assistant Professor in Clinical Psychology
Travis Research Institute
datkins at fuller.edu

# Message: 1
# Date: Thu, 22 Mar 2001 14:27:18 -0500 (EST)
# From: Kouros Owzar <owzar at email.unc.edu>
# To: Nlme-help at franz.stat.wisc.edu
# Subject: [Nlme-help] Useful Functions
#
# Dear All,
#
# Jose has been nice enough to write two useful functions
# which enable one to extract estimated variance matrices from
# lme and nlme objects.
#
# The details follow:
#
# Consider the model
#
# Y_i  = g(X_i  beta + Z_i  b_i) + e_i
# nix1     nixp px1    nixq qX1  + n_i x1
# where
#
# Y_i   : observations for the ith subject
# X_i   : design matrix for the fixed effects for the ith subject
# beta  : vector of fixed effects
# Z_i   : design matrix for the random effects for the ith subject
# b_i   : random effects vector for ith subject  (~N(0,\Sigma_b)
# e_i   : measurement errors for the ith subject (~N(0,\Sigma_e)
#
# The functions varRan and varWithin (shown below) extract the
# estimated matrices \hat(\Sigma_b) and \hat(\Sigma_e) of
# \Sigma_b and \Sigma_e respectively.
#
# NOTES and REMARKS:
#
# 1. I have NOT carried out extensive testing of these functions.
#    So please use them with care. Some of my tests are shown
#    below.
# 2. varWithin works even if one imposes structures on Sigma_e
# 3. varWithin also supports gls and gnls. The amount of testing
#    done on these objects is considerably less than that done
#    on lme and nlme objects.
# 4. I sincerely thank Jose once again for taking out time out of his busy
#    schedule to write these functions.
#
#
# Sincerely,
#
# Kouros
#
############################### Begin Functions #################################

varRan <- function(object, level = 1)
{
    sigE <- object$sig^2
    sigE*pdMatrix(object$modelStruct$reStruct)[[level]]
}



varWithin <- function(object, wch)
{
   getMat <- function(wch, grps, ugrps, cS, corM, vF, std) {
     wgrps <- grps[grps == ugrps[wch]]
     nG <- length(wgrps)
     if (!is.null(cS)) {
       val <- corM[[wch]]
     } else {
       val <- diag(nG)
     }
     if (!is.null(vF)) {
       std <- diag(std[[wch]])
       val <- std %*% (val %*% std)
     } else {
       val <- std*std*val
     }
     val
   }
   grps <- getGroups(object)
   if (is.null(grps)) {                  # gls/gnls case
     grps <- rep(1, length(resid(object)))
     ugrps <- "1"
   } else {
     ugrps <- unique(as.character(grps))
   }
   if (!is.null(vF <- object$modelStruct$varStruct)) {
     ## weights from variance function, if present
     std <- split(object$sigma/varWeights(vF), grps)[ugrps]
   } else {
     std <- object$sigma
   }
   if (!is.null(cS <- object$modelStruct$corStruct)) {
     corM <- corMatrix(cS)[ugrps]
   } else {
     corM <- NULL
   }
   if (!missing(wch)) {                  # particular group
     return(getMat(wch, grps, ugrps, cS, corM, vF, std))
   } else {
     if (length(ugrps) == 1) {           # single group
       return(getMat(1, grps, ugrps, cS, corM, vF, std))
     }
     val <- vector("list", length(ugrps))
     names(val) <- ugrps
     for(i in 1:length(ugrps)) {
       val[[i]] <- getMat(i, grps, ugrps, cS, corM, vF, std)
     }
     return(val)
   }
}


############################## Some Tests ########################################

mod1<-lme(Orange)
mod2<-update(mod1,corr=corAR1())
OJ<-Orange[-1,]
mod3<-lme(OJ)
mod4<-update(mod3,corr=corARMA(p=1,q=1))

fm1 <- gnls(weight ~ SSlogis(Time, Asym, xmid, scal),data=Soybean)
fm2 <-update(fm1,corr = corAR1())
dim(varWithin(fm1))
sapply(varWithin(fm2),dim)
varWithin(fm2,1)
varWithin(fm2,2)
varWithin(fm2,9)



SB<-Soybean[-c(11,77),]

fm3 <- gnls(weight ~ SSlogis(Time, Asym, xmid, scal),data=SB)
fm4 <-update(fm3,corr = corAR1())
dim(varWithin(fm3))
sapply(varWithin(fm4),dim)
varWithin(fm4,1)
varWithin(fm4,2)
varWithin(fm4,9)


fm11 <- nlme(weight ~ SSlogis(Time, Asym, xmid,
scal),fixed=Asym+xmid+scal~1,sta
rt=c(19,55,8),data=Soybean)
fm22 <- nlme(weight ~ SSlogis(Time, Asym, xmid,
scal),fixed=Asym+xmid+scal~1,sta
rt=c(19,55,8),data=SB)
sapply(varWithin(fm11),dim)
sapply(varWithin(fm22),dim)




--__--__--

_______________________________________________
Nlme-help maillist  -  Nlme-help at nlme.stat.wisc.edu
http://nlme.stat.wisc.edu/mailman/listinfo/nlme-help


End of Nlme-help Digest



From RBaskin at ahrq.gov  Tue Apr  6 15:26:21 2004
From: RBaskin at ahrq.gov (Baskin, Robert)
Date: Tue, 6 Apr 2004 09:26:21 -0400 
Subject: [R] Curious about nomenclature: random deviates
Message-ID: <6BCD3F430455B1418750004BCD27925905C642@exchange2.ahrq.gov>

< Does anyone know why they're called random deviates, as opposed to random
numbers?>
Others will probably give you some technical reason about random numbers can
be considered as random deviates from a mean (I think at least the 1875
Galton paper at http://www.mugu.com/galton/ uses similar terminology (I'm
not claiming this is the earliest use - just the easiest to access at the
moment)).

But everyone knows the real reason for the term is to create the oxymoron
'normal deviates' - it's a great name for a softball team of statisticians
:)

Bob



-----Original Message-----
From: Andrew Robinson [mailto:andrewr at uidaho.edu] 
Sent: Tuesday, April 06, 2004 12:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Curious about nomenclature: random deviates

Hi all,

a student of mine recently stumbled whilst reading the R help files for the 
statistical distributions.  She was confused by their assertion that, for 
example, 'rnorm' generates random deviates.  I have seen this label used 
elsewhere, although it does not seem universal - for example, Ripley (1987) 
doesn't have it in the index. Does anyone know why they're called random 
deviates, as opposed to random numbers?

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.

Cited:

Ripley, B.D. 1987. Stochastic Simulation. Wiley-Interscience;

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From stephan at met.ed.ac.uk  Tue Apr  6 15:54:01 2004
From: stephan at met.ed.ac.uk (Stephan Matthiesen)
Date: Tue, 6 Apr 2004 14:54:01 +0100
Subject: [R] A package to read and write NetCDF?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIEEDMCKAA.sdhyok@email.unc.edu>
References: <OAEOKPIGCLDDHAEMCAKIEEDMCKAA.sdhyok@email.unc.edu>
Message-ID: <200404061454.03263.stephan@met.ed.ac.uk>

Hi,

Am Dienstag, 6. April 2004 02:35 schrieb Shin, Daehyok:
> I am looking for a package to read and write NetCDF files.
> NetCDF package says it can only read, not write.
> Another package for the standard binary file format?

The package clim.pact has the functions r2cdf and retrieve.nc
I haven't tried them, but the documentation says they are for NetCDF writing 
and reading. Hope that helps...

Stephan



From rory at campbell-lange.net  Tue Apr  6 16:07:29 2004
From: rory at campbell-lange.net (Rory Campbell-Lange)
Date: Tue, 6 Apr 2004 15:07:29 +0100
Subject: [R] Ignorant lack of bliss : summarise table by column attribute
Message-ID: <20040406140729.GA3534@campbell-lange.net>

Having read the list posting guidelines I fear my first post is about to
break the rules. Apologies in advance.

We have been asked to produce some graphs of relative performance of 3
groups of people in relation to the trend of their previous performance.
I am neither a mathematician or a statistician, but wondered if R (which
I have been using as a desktop calculator!) and some knowledge from this
list may be able to help.

We have a dataset something like this:

    group | previousavg | lastreading | finalreading
    ------------------------------------------------
    1     | 9.5         | 10          | 12
    1     | 7           | 9           | 11
    1     | 12          | 11          | 12
    2     | 13          | 14          | 16
    2     | 11          | 10          | 9
    3     | 10          | 10          | 10.5
    3     | 8.5         | 10          | 12

I need to produce some graphs typifying the change for each group
between a _projected_ final reading and the final reading given. The
time difference between previousavg and lastreading is 1/2 that between
lastreading and finalreading.

Where I have got to so far:

I have read the result set (less than 200 rows) into a table 'results',
attached it and then rather crudely constructed projected figures :

    results$projected = 
        ((lastreading - previousavg) * 2) + lastreading

then I can see the differentials between projected and finalreading:

    > result$projected - results$finalreading
     [1] -1.4  6.9  1.1  3.4  0.0  3.6 -3.8  0.1 -0.1  0.9  1.2 -3.4 -1.5  0.1  5.6
    [16] -3.3 -1.9  0.9 -3.1  1.5  0.7 -1.6 -0.3  1.1 -0.1 -0.6  1.5  0.2  0.8 -1.0
    [31]  0.8 -0.5  1.9 -4.0 -3.3  3.1  2.8 -0.6  1.2  2.0 -1.9 -1.6 -1.1 -3.9   NA
    ...
     
Aims:

    - Summarise these by groups (I can't work out how to use tapply...)
    - Produce a sensible 'typification' of each group's change in
      relation to the projected figure. I assume this would use a
      statistical algorithm to exclude exceptions.
    - Plot the 3 'typifications' in sensible relation to each other,
      possibly with data points showing the source of these lines.

My sincere apologies if this is completely off-topic for this list. I'm
hoping to learn a little by understanding how certain functions are used
(approaching this like a programmer rather than a statistician.)

If I needed to learn more is the book "Introductory Statistics with R" a
good place to start?

Thanks
Rory

-- 
Rory Campbell-Lange 
<rory at campbell-lange.net>
<www.campbell-lange.net>



From ggrothendieck at myway.com  Tue Apr  6 16:18:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 6 Apr 2004 14:18:42 +0000 (UTC)
Subject: [R] %+=% and eval.parent()
References: <a06002011bc9861cabf1d@[139.166.242.29]>
Message-ID: <loom.20040406T161219-676@post.gmane.org>

Robin Hankin <rksh <at> soc.soton.ac.uk> writes:
> R> a <- matrix(1:9,3,3)
> But the following caught me off-guard:
> R> a <- matrix(1:9,3,3)
> R> a[a%%2==1] %+=% 1000*(1:5)

How about this way:

> a <- matrix(1:9,3,3)
> "plus<-" <- function(a,value) a+value
> plus(a[a%%2==1]) <- 1000*(1:5)
> a
     [,1] [,2] [,3]
[1,] 1001    4 4007
[2,]    2 3005    8
[3,] 2003    6 5009
>



From tlumley at u.washington.edu  Tue Apr  6 16:29:38 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Apr 2004 07:29:38 -0700 (PDT)
Subject: [R] Deep copy in R
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIAEDMCKAA.sdhyok@catchlab.org>
References: <OAEOKPIGCLDDHAEMCAKIAEDMCKAA.sdhyok@catchlab.org>
Message-ID: <Pine.A41.4.58.0404060728000.15994@homer12.u.washington.edu>

On Mon, 5 Apr 2004, Shin, Daehyok wrote:

> Hm. Smarter than I expected.
> But, any special reason why the 6th line clones another vector?
> To me, just reference copy seems to be enough for the purpose.
>
> 6 xf = date.frame(x=x)
>

You would have to look at the code for data.frame to see this, but I think
it does in fact copy.  However, that should stop line 7 from having to
copy so it all works out in the end.

There may also be temporary copies made during data.frame(), which is a
much less straightforward piece of code than you might expect.

	-thomas



From jc at or.psychology.dal.ca  Tue Apr  6 16:45:50 2004
From: jc at or.psychology.dal.ca (John Christie)
Date: Tue, 6 Apr 2004 11:45:50 -0300
Subject: [R] fit.contrast or some other for interactions and stratified
Message-ID: <18EC0118-87D9-11D8-8090-0003933F15A6@or.psychology.dal.ca>


I was trying to do a bunch of planned contrast but ran into a snag.  
fit.contrast won't work with an Error term in aov.  Ok, then I use lme. 
  Only, now it seems there is no way to enter the interaction terms 
(something like "x1:x2").  I couldn't find a way to use the interaction 
term in the help.  Is there an undocumented method or some other 
command for planned contrasts?



From jjava at priscian.com  Wed Apr  7 00:55:21 2004
From: jjava at priscian.com (Jim Java)
Date: Tue,  6 Apr 2004 15:55:21 -0700
Subject: [R] Index of a Loop Variable?
Message-ID: <200404061555.AA5636296@priscian.com>

Hi Everyone:--

Is it possible, within a for loop not explicitly using whole-number
indexing, to find out the index value of the loop variable within the
vector or list that's being looped through? For example, in --

x <- seq(5, 50, by=5)
index.in.x <- 1
for (i in x) {
  cat(paste(" index of i-value ", i, " within x: ", index.in.x, sep=""), fill=T)
  index.in.x <- index.in.x + 1
}

 -- is it in general possible to get values of "index.in.x" without
making it a count variable, as above?

Thank you.

 -- Jim Java



From tlumley at u.washington.edu  Tue Apr  6 16:58:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Apr 2004 07:58:17 -0700 (PDT)
Subject: [R] Index of a Loop Variable?
In-Reply-To: <200404061555.AA5636296@priscian.com>
References: <200404061555.AA5636296@priscian.com>
Message-ID: <Pine.A41.4.58.0404060756400.15994@homer12.u.washington.edu>

On Tue, 6 Apr 2004, Jim Java wrote:

> Hi Everyone:--
>
> Is it possible, within a for loop not explicitly using whole-number
> indexing, to find out the index value of the loop variable within the
> vector or list that's being looped through? For example, in --
>
> x <- seq(5, 50, by=5)
> index.in.x <- 1
> for (i in x) {
>   cat(paste(" index of i-value ", i, " within x: ", index.in.x, sep=""), fill=T)
>   index.in.x <- index.in.x + 1
> }
>
>  -- is it in general possible to get values of "index.in.x" without
> making it a count variable, as above?

No.

Sometimes it is possible to avoid the need to do this by replacing the
loop with sapply().

Otherwise you probably should use  for(i in seq(length=length(x)) )
and get the elements of x as x[i].

	-thomas



From nlwhitehouse at yahoo.com  Tue Apr  6 17:16:02 2004
From: nlwhitehouse at yahoo.com (Nathan Whitehouse)
Date: Tue, 6 Apr 2004 08:16:02 -0700 (PDT)
Subject: [R] Macintosh/SJava
Message-ID: <20040406151602.62153.qmail@web12404.mail.yahoo.com>

Hi,

  Has anyone used the SJava package on Mac
OSX(Panther)?

  When I try to install it I get

  checking for java ../usr/bin/java
  Java VM /usr/bin/java
  checking for javah... no
  Sorry.  We can currently only configure this package
for IBM, Blackdown, or Sun's java implementation
  ERROR: configuration failed ...


  Does anyone have an idea of how to install this or
where to make modifications so it can use the
Macintosh  JVM?

  Thanks,

  Nathan Whitehouse

=====
Nathan Whitehouse
Statistics/Programming
Baylor College of Medicine
Houston, TX, USA
nlwhitehouse at yahoo.com
work: 1-713-798-9029
cell:    1-512-293-5840

http://rho-project.org: rho- open source web services for R.
http://franklin.imgen.bcm.tmc.edu: Shaw laboratory, bcm.



From lecoutre at stat.ucl.ac.be  Tue Apr  6 17:17:53 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 06 Apr 2004 17:17:53 +0200
Subject: [R] Index of a Loop Variable?
In-Reply-To: <200404061555.AA5636296@priscian.com>
References: <200404061555.AA5636296@priscian.com>
Message-ID: <6.0.1.1.2.20040406171517.02004ec0@stat4ux.stat.ucl.ac.be>



Hi Jim,

Use the build "seq(along=vec))" as in:

x <- seq(5, 50, by=5)
for (i in seq(along=x))
cat(paste(" index of i-value ", i, " with x-value: ", x[i], sep=""), fill=T)

Eric


At 00:55 7/04/2004, Jim Java wrote:
>Hi Everyone:--
>
>Is it possible, within a for loop not explicitly using whole-number
>indexing, to find out the index value of the loop variable within the
>vector or list that's being looped through? For example, in --
>
>x <- seq(5, 50, by=5)
>index.in.x <- 1
>for (i in x) {
>   cat(paste(" index of i-value ", i, " within x: ", index.in.x, sep=""), 
> fill=T)
>   index.in.x <- index.in.x + 1
>}
>
>  -- is it in general possible to get values of "index.in.x" without
>making it a count variable, as above?
>
>Thank you.
>
>  -- Jim Java
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From renaud.scheifler at univ-fcomte.fr  Tue Apr  6 17:30:34 2004
From: renaud.scheifler at univ-fcomte.fr (Renaud Scheifler)
Date: Tue, 06 Apr 2004 17:30:34 +0200
Subject: [R] how to update R
Message-ID: <5.0.2.1.2.20040406172554.00acebc0@utinam.univ-fcomte.fr>

Dear R users,
I have R 1.8.0. and, when I try to load the package "multcomp", I have an 
error message saying that this package was developed under R 1.8.1.
So my question is "How to update R" ?
Should I uninstall my version of R to install the new one or is there a 
simpler way to update it ?
Than you for your answer.
Sincerely,
Renaud.



From paulojus at est.ufpr.br  Tue Apr  6 17:33:04 2004
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Tue, 6 Apr 2004 12:33:04 -0300 (BRT)
Subject: [R] how to update R
In-Reply-To: <5.0.2.1.2.20040406172554.00acebc0@utinam.univ-fcomte.fr>
References: <5.0.2.1.2.20040406172554.00acebc0@utinam.univ-fcomte.fr>
Message-ID: <Pine.LNX.4.58L0.0404061231120.10736@est.ufpr.br>

Dear Renaud
There are several alternatives
You need to tell us onwhich platform you run R.
For instance, if on a Debian machine
apt-get update
may work

You can aways compile from the source
Or are you using a windows system?

P.J.

On Tue, 6 Apr 2004, Renaud Scheifler wrote:

> Dear R users,
> I have R 1.8.0. and, when I try to load the package "multcomp", I have an
> error message saying that this package was developed under R 1.8.1.
> So my question is "How to update R" ?
> Should I uninstall my version of R to install the new one or is there a
> simpler way to update it ?
> Than you for your answer.
> Sincerely,
> Renaud.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
Departamento de Estat??stica
Universidade Federal do Paran??
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3471
Fax: (+55) 41 361 3141
e-mail: pj at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From B.Rowlingson at lancaster.ac.uk  Tue Apr  6 17:49:43 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 06 Apr 2004 16:49:43 +0100
Subject: [R] Index of a Loop Variable?
In-Reply-To: <6.0.1.1.2.20040406171517.02004ec0@stat4ux.stat.ucl.ac.be>
References: <200404061555.AA5636296@priscian.com>
	<6.0.1.1.2.20040406171517.02004ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <4072D197.40908@lancaster.ac.uk>

This all reminds me of some looping things I was working on a year or so 
ago.

When looping over, say, 1 to 10,000,000 with:

for(i in 1:10000000)

  R creates a 10,000,000-integer long vector, which seems a bit ugly. 
The solution is obviously to have a counter and add one every time, then 
testing. But that's messy. So I started writing some iterator functions. 
They work like this:

  * first create a new loop object:

myLoop <- loop(N=10000000)

  * then iterate over it in a while loop:

while(iterate(myLoop)){
  doStuff()
}

  * within the while loop, you can use a bunch of methods to get the 
current iteration from the myLoop object, and other clever stuff such as 
predicting when the loop is going to end:

while(iterate(myLoop)){
  doStuff()
  cat("Iteration : ",iteration(myLoop),'\n')
  cat("Predicted finish at: ",predictEnd(myLoop),'\n')
}


  * I then extended this class to make a looping class for MCMC runs:

  myMcLoop <- mcmcLoop(N=10000000, burn.in=10000, thin=1000)

  * then defined some new methods for that, so you can tell if the 
current iteration is in the burn-in period, or if the current iteration 
is one selected from the thinning:

while(iterate(myMcLoop)){
  sample <- doMCMC()
  if(saveResults(myLoop))
    x[[indexResult(myLoop)]] <- sample
  }
}

  So, who wants this? I originally wrote it using S3 methods, then had a 
go at doing it with S4 methods and then had yet another go using the 
R.oo package just for the heck of it. The various bits are scattered 
between various machines so it might take some archaeology to find the 
most recent or best code. Something to keep me busy during over Easter 
break perhaps...

Baz



From zhuw at mail.smu.edu  Tue Apr  6 17:54:27 2004
From: zhuw at mail.smu.edu (zhu wang)
Date: 06 Apr 2004 10:54:27 -0500
Subject: [R] missing values for mda package
In-Reply-To: <200404061000.i36A0NaK014813@hypatia.math.ethz.ch>
References: <200404061000.i36A0NaK014813@hypatia.math.ethz.ch>
Message-ID: <1081266866.2853.6.camel@zwang.stat.smu.edu>

Dear helpers,

I am trying to use the mda package downloaded from the R website, but
the data set has missing values so I got an error message. Should I
manually handle these missing values? I was trying to read the documents
to specify any option related to missing values, but I did not find it.
Please forgive me if I ignore something obvious.

Thanks,

Zhu Wang

Statistical Science Department
Southern Methodist University
Dallas, TX 75275-0332
Phone: (214)768-2453
Fax: (214)768-4035
Email: zhuw at mail.smu.edu



From martin at ist.org  Tue Apr  6 18:35:18 2004
From: martin at ist.org (Martin Keller-Ressel)
Date: Tue, 06 Apr 2004 18:35:18 +0200
Subject: [R] R loosing history
Message-ID: <opr51m04dljigwsf@mail.ist.org>

Hello,

Im running R 1.8.1 on Win2000 and frequently using the function history() 
to keep track of what I did in my R session.
In a recent session I typed history(max=Inf) and to my surprise there were 
only 15 lines of code where there should have been approx. 300 lines and I 
lost a lot of valuable work.
Do you have any idea what caused this behaviour? help(history) didn't make 
me any smarter. Am I misunderstanding the concept of history()? I was 
expecting it to keep track of every command I type during my session.


> R.version
             _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.1
year     2003
month    11
day      21
language R


Any help is appreciated,

Martin Keller-Ressel


-- 
Erstellt mit M2, Operas revolution??rem E-Mail-Modul: 
http://www.opera.com/m2/



From simon at stats.gla.ac.uk  Tue Apr  6 18:38:34 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 6 Apr 2004 17:38:34 +0100 (BST)
Subject: [R] GAM with Locfit components
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B1C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7B1C@usrymx25.merck.com>
Message-ID: <Pine.SOL.4.58.0404061736150.27573@moon.stats.gla.ac.uk>

>From mgcv 1.0 you can add your own smoothers for use with gam, but only if
they can be represented using basis functions and a quadratic penalty: so
P-splines are OK, but loess is not, for example.

Simon

> Loader's book is referring to the gam() function in S-plus (or S from Bell
> Labs), not the one in mgcv.  They are very different things.  I don't know
> if it's possible to implement local regression type smoothers (or something
> other than splines) in gam() in mgcv, but even if it's possible, the one in
> locfit won't work without quite a bit of work, I'd imagine.
>
> Andy
>
> > From: Vivian Viallon
> >
> > Hi,
> > I'm trying to combine the Locfit Package with the Mgcv package (to use
> > Generalized Additive Models with Locfit components).  I read the book
> > written by Clive Loader  where it's said that, for the S
> > language, you just
> > have to "load" the locfit package using the command :
> > Library(locfit, first="T")
> > in order to use locfit components in an additive model.
> > But I can't. I guess the C-command differs from the S-command.
> > Thanks in  advance for your help.
> > Regards,
> > Vivian
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Apr  6 18:54:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Apr 2004 18:54:21 +0200
Subject: [R] missing values for mda package
References: <200404061000.i36A0NaK014813@hypatia.math.ethz.ch>
	<1081266866.2853.6.camel@zwang.stat.smu.edu>
Message-ID: <4072E0BD.609F0C19@statistik.uni-dortmund.de>



zhu wang wrote:
> 
> Dear helpers,
> 
> I am trying to use the mda package downloaded from the R website, but
> the data set has missing values so I got an error message. Should I
> manually handle these missing values? I was trying to read the documents
> to specify any option related to missing values, but I did not find it.
> Please forgive me if I ignore something obvious.

If it is not documented (hence probably not available) and you don't
know how to tell the functions to handle missing values, try to do it
"yourself". ?NA suggests:
"See Also: [...] 'na.action', 'na.omit', 'na.fail' on how methods can be
tuned to deal with missing values."

Uwe Ligges



> Thanks,
> 
> Zhu Wang
> 
> Statistical Science Department
> Southern Methodist University
> Dallas, TX 75275-0332
> Phone: (214)768-2453
> Fax: (214)768-4035
> Email: zhuw at mail.smu.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Apr  6 19:07:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 06 Apr 2004 19:07:27 +0200
Subject: [R] R loosing history
References: <opr51m04dljigwsf@mail.ist.org>
Message-ID: <4072E3CF.992998D9@statistik.uni-dortmund.de>



Martin Keller-Ressel wrote:
> 
> Hello,
> 
> Im running R 1.8.1 on Win2000 and frequently using the function history()
> to keep track of what I did in my R session.
> In a recent session I typed history(max=Inf) and to my surprise there were
> only 15 lines of code where there should have been approx. 300 lines and I
> lost a lot of valuable work.
> Do you have any idea what caused this behaviour? help(history) didn't make
> me any smarter. Am I misunderstanding the concept of history()? I was
> expecting it to keep track of every command I type during my session.

history() *shows* (the last 25 lines of) the current history buffer.
history(max=Inf) *shows* all the current history buffer.
Note: It does not save the history, it does not set any limits. In order
to save the history use savehistory().

A major part of the history buffer will be emptied if the buffer becomes
full.
You can set larger limits to the buffer by clicking "Edit" -> "GUI
preferences" and set "buffer bytes" and "lines" appropriately.

Uwe Ligges

> > R.version
>              _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
> 
> Any help is appreciated,
> 
> Martin Keller-Ressel
> 
> --
> Erstellt mit M2, Operas revolution??rem E-Mail-Modul:
> http://www.opera.com/m2/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Brian.J.GREGOR at odot.state.or.us  Tue Apr  6 19:07:03 2004
From: Brian.J.GREGOR at odot.state.or.us (Brian.J.GREGOR@odot.state.or.us)
Date: Tue, 6 Apr 2004 10:07:03 -0700 
Subject: [R] how to learn R quickly?
Message-ID: <372EFF9FE4E42E419C978E7A305DC5FE0379AD09@exsalem5.odot.state.or.us>

You might check out a short email course that Ben Stabler and I developed
for transportation modelers in Oregon. It's on our web site.
http://www.odot.state.or.us/tddtpau/r/rbook.pdf

Brian Gregor, P.E.
Transportation Planning Analysis Unit
Oregon Department of Transportation
Brian.J.GREGOR at odot.state.or.us
(503) 986-4120


-----Original Message-----
From: vinkwai wong [mailto:0034058 at fudan.edu.cn] 
Sent: Tuesday, April 06, 2004 6:20
To: r-help at stat.math.ethz.ch
Subject: [R] how to learn R quickly?


i am new to R,so i post here to ask you how to learn R quickly?
is there some usefull materials about using R.
ps:though the manual is a good materials,but it is not the best for a
newbie,i think.

thank you !



From andy_liaw at merck.com  Tue Apr  6 19:17:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 6 Apr 2004 13:17:03 -0400
Subject: [R] R v4.0?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B3E@usrymx25.merck.com>

Just out of curiosity, has anyone else seen this?

http://www.fas.umontreal.ca/BIOL/Casgrain/en/labo/R/v4/index.html

Cheers,
Andy
--
Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com        732-594-0820



From kjetil at entelnet.bo  Tue Apr  6 19:25:41 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 06 Apr 2004 13:25:41 -0400
Subject: [R] R loosing history
In-Reply-To: <opr51m04dljigwsf@mail.ist.org>
Message-ID: <4072AFD5.32708.659530@localhost>

On 6 Apr 2004 at 18:35, Martin Keller-Ressel wrote:

To do this I have defined the environment variable 
R_HISTSIZE (with value 10000, usually enough!).

Kjetil Halvorsen

> Hello,
> 
> Im running R 1.8.1 on Win2000 and frequently using the function
> history() to keep track of what I did in my R session. In a recent
> session I typed history(max=Inf) and to my surprise there were only 15
> lines of code where there should have been approx. 300 lines and I
> lost a lot of valuable work. Do you have any idea what caused this
> behaviour? help(history) didn't make me any smarter. Am I
> misunderstanding the concept of history()? I was expecting it to keep
> track of every command I type during my session.
> 
> 
> > R.version
>              _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
> 
> 
> Any help is appreciated,
> 
> Martin Keller-Ressel
> 
> 
> -- 
> Erstellt mit M2, Operas revolution??rem E-Mail-Modul: 
> http://www.opera.com/m2/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Tue Apr  6 19:27:15 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Apr 2004 10:27:15 -0700 (PDT)
Subject: [R] R v4.0?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B3E@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7B3E@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0404061026110.18914@homer32.u.washington.edu>

On Tue, 6 Apr 2004, Liaw, Andy wrote:

> Just out of curiosity, has anyone else seen this?
>
> http://www.fas.umontreal.ca/BIOL/Casgrain/en/labo/R/v4/index.html
>

Yes, it gets mentioned from time to time.

	-thomas



From jeaneid at chass.utoronto.ca  Tue Apr  6 19:33:58 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 6 Apr 2004 13:33:58 -0400
Subject: [R] percentile-percentile plot
Message-ID: <Pine.SGI.4.40.0404061323400.8131409-100000@origin.chass.utoronto.ca>


Hi,

Is there a function that does percentile-percentile plot. I do not mean
the qqplot. I need to plot the percentiles rather than points themselves.
I am hoping for a plot that tells me that the x percentile of one data set
corresponds to the y percentile of the other. for example a point on the
plot of (.5, .2) will tell me that the 50th percentile of the first data
and the 20th percentile of the second data are equal.

Is there any function that does this?
Thank you for the help,
Jean,



From jfox at mcmaster.ca  Tue Apr  6 19:28:56 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 06 Apr 2004 13:28:56 -0400
Subject: [R] how to update R
In-Reply-To: <5.0.2.1.2.20040406172554.00acebc0@utinam.univ-fcomte.fr>
Message-ID: <web-29092641@cgpsrv2.cis.mcmaster.ca>

Dear Renaud,

Unless there's some other problem (such as failure to install the
mvtnorm package, on which multcomp depends), there is no need to panic:
the message is simply a warning.

Since a new version of R (1.9.0) is imminent, you may want to update to
that when it arrives.

I hope this helps,
 John

On Tue, 06 Apr 2004 17:30:34 +0200
 Renaud Scheifler <renaud.scheifler at univ-fcomte.fr> wrote:
> Dear R users,
> I have R 1.8.0. and, when I try to load the package "multcomp", I
> have an error message saying that this package was developed under R
> 1.8.1.
> So my question is "How to update R" ?
> Should I uninstall my version of R to install the new one or is there
> a simpler way to update it ?
> Than you for your answer.
> Sincerely,
> Renaud.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From dzk123 at psu.edu  Tue Apr  6 19:50:39 2004
From: dzk123 at psu.edu (DAEYOUNG KIM)
Date: Tue, 6 Apr 2004 13:50:39 -0400 (EDT)
Subject: [R] Question about error
Message-ID: <200404061750.i36HodQ27452@webmail13.cac.psu.edu>

Dear Sir..

I am a graduate student in Pennstate.
I have some problem in using R.
At first, I used image function under R.(I loaded the packages GeoR, geoRglm,
and coda)
When I performed the following command 
"image(try4,loc=blue.grid,values.to.plot="median",col=gray(seq(1,0.1,l=9)),breaks=seq(9,89,l=10),xlab="X",ylab="Y")"

I got some error message "Error in inout(pts = locations, ploy = borders) :
subscript out of bounds "
But I had no idea about this error.
Would you give me some information or which part should I check again?

Have a nice day.
Sincerely
Daeyoung Kim



From kjetil at entelnet.bo  Tue Apr  6 20:24:43 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 06 Apr 2004 14:24:43 -0400
Subject: [R] percentile-percentile plot
In-Reply-To: <Pine.SGI.4.40.0404061323400.8131409-100000@origin.chass.utoronto.ca>
Message-ID: <4072BDAB.13475.9BA026@localhost>

On 6 Apr 2004 at 13:33, Jean Eid wrote:

qqplot(x,y)

Kjetil Halvorsen

> 
> Hi,
> 
> Is there a function that does percentile-percentile plot. I do not
> mean the qqplot. I need to plot the percentiles rather than points
> themselves. I am hoping for a plot that tells me that the x percentile
> of one data set corresponds to the y percentile of the other. for
> example a point on the plot of (.5, .2) will tell me that the 50th
> percentile of the first data and the 20th percentile of the second
> data are equal.
> 
> Is there any function that does this?
> Thank you for the help,
> Jean,
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From dmurdoch at pair.com  Tue Apr  6 20:41:47 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 06 Apr 2004 14:41:47 -0400
Subject: [R] percentile-percentile plot
In-Reply-To: <Pine.SGI.4.40.0404061323400.8131409-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0404061323400.8131409-100000@origin.chass.utoronto.ca>
Message-ID: <1ut570h6vphl7v9ktbnp67si9fhtrbtgfh@4ax.com>

On Tue, 6 Apr 2004 13:33:58 -0400, Jean Eid
<jeaneid at chass.utoronto.ca> wrote :

>Hi,
>
>Is there a function that does percentile-percentile plot. I do not mean
>the qqplot. I need to plot the percentiles rather than points themselves.
>I am hoping for a plot that tells me that the x percentile of one data set
>corresponds to the y percentile of the other. for example a point on the
>plot of (.5, .2) will tell me that the 50th percentile of the first data
>and the 20th percentile of the second data are equal.
>
>Is there any function that does this?

I think you'll have to write your own.  Here's a rough start:

ppplot <- function(x,y) {
  x <- sort(x)
  y <- sort(y)
  px <- ppoints(x)
  py <- ppoints(y)
  plot(px, approx(y, py, x)$y)
}

This function messes up if the distributions are too different; you'll
need to fix up the approx call somehow.

Duncan Murdoch



From mok2 at physics.buffalo.edu  Tue Apr  6 21:09:51 2004
From: mok2 at physics.buffalo.edu (Mark O. Kimball)
Date: Tue, 6 Apr 2004 14:09:51 -0500
Subject: [R] Minimize a plot window
Message-ID: <200404061509.51753.mok2@physics.buffalo.edu>

Is it possible to start a X11() device in a minimized state? I have many
windows, each with useful data, which clutter the screen. Having each
window minimize upon creation would help.

Mark
-- 
Mark O. Kimball
Gasparinilab, University of Buffalo  |  Low temp physics
mok2 at physics.buffalo.edu  |  URL: enthalpy.physics.buffalo.edu
lab phone: 716-645-2017x122  Fax: 716-645-2507



From ivo.welch at yale.edu  Tue Apr  6 21:21:12 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Tue, 06 Apr 2004 15:21:12 -0400
Subject: [R] assert( condition )
Message-ID: <40730328.1050401@yale.edu>


hi R wizards:  I think it would be a good idea for me to use basic 
argument checking (type, length) in function calls via an assert() like 
function.  How do people usually do this?  Should I define my own 
"assert()" function (or are there some predefined assertions)?  If I 
write my own assert, is it possible to print out the R internal call 
stack?  thanks.

(?assert and google "assert R" and V+D index do not point me the right way.)

regards, /iaw



From rpeng at jhsph.edu  Tue Apr  6 21:27:57 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 06 Apr 2004 15:27:57 -0400
Subject: [-VIRUS-] Re: [R] assert( condition )
In-Reply-To: <40730328.1050401@yale.edu>
References: <40730328.1050401@yale.edu>
Message-ID: <407304BD.7000607@jhsph.edu>

I often use stopifnot().

-roger

ivo welch wrote:
> 
> hi R wizards:  I think it would be a good idea for me to use basic 
> argument checking (type, length) in function calls via an assert() like 
> function.  How do people usually do this?  Should I define my own 
> "assert()" function (or are there some predefined assertions)?  If I 
> write my own assert, is it possible to print out the R internal call 
> stack?  thanks.
> 
> (?assert and google "assert R" and V+D index do not point me the right 
> way.)
> 
> regards, /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dmurdoch at pair.com  Tue Apr  6 21:48:01 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 06 Apr 2004 15:48:01 -0400
Subject: [R] assert( condition )
In-Reply-To: <40730328.1050401@yale.edu>
References: <40730328.1050401@yale.edu>
Message-ID: <2u1670hhfr3l8rc8pcab3019h5o9a1rioq@4ax.com>

On Tue, 06 Apr 2004 15:21:12 -0400, ivo welch <ivo.welch at yale.edu>
wrote :

>hi R wizards:  I think it would be a good idea for me to use basic 
>argument checking (type, length) in function calls via an assert() like 
>function.  How do people usually do this?  Should I define my own 
>"assert()" function (or are there some predefined assertions)?  If I 
>write my own assert, is it possible to print out the R internal call 
>stack?  thanks.
>
>(?assert and google "assert R" and V+D index do not point me the right way.)

The stopifnot() function does pretty much what assert() does in other
languages.

Duncan Murdoch



From tlumley at u.washington.edu  Tue Apr  6 21:51:14 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Apr 2004 12:51:14 -0700 (PDT)
Subject: [R] assert( condition )
In-Reply-To: <40730328.1050401@yale.edu>
References: <40730328.1050401@yale.edu>
Message-ID: <Pine.A41.4.58.0404061249340.18914@homer32.u.washington.edu>

On Tue, 6 Apr 2004, ivo welch wrote:

>
> hi R wizards:  I think it would be a good idea for me to use basic
> argument checking (type, length) in function calls via an assert() like
> function.  How do people usually do this?  Should I define my own
> "assert()" function (or are there some predefined assertions)?  If I
> write my own assert, is it possible to print out the R internal call
> stack?  thanks.
>

In the R testing suite the stopifnot() function does this. After stopping,
traceback() will print out the R stack trace.

A more elegant solution would use the new R condition mechanism and allow
assertion failures to be caught independently of errors.

	-thomas



From rossini at blindglobe.net  Tue Apr  6 22:03:40 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 06 Apr 2004 13:03:40 -0700
Subject: [R] assert( condition )
In-Reply-To: <Pine.A41.4.58.0404061249340.18914@homer32.u.washington.edu>
	(Thomas
	Lumley's message of "Tue, 6 Apr 2004 12:51:14 -0700 (PDT)")
References: <40730328.1050401@yale.edu>
	<Pine.A41.4.58.0404061249340.18914@homer32.u.washington.edu>
Message-ID: <853c7g50wz.fsf@servant.blindglobe.net>

Thomas Lumley <tlumley at u.washington.edu> writes:

> A more elegant solution would use the new R condition mechanism and allow
> assertion failures to be caught independently of errors.

A pointer to this would be appreciated.

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From tlumley at u.washington.edu  Tue Apr  6 22:11:04 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 6 Apr 2004 13:11:04 -0700 (PDT)
Subject: [R] assert( condition )
In-Reply-To: <853c7g50wz.fsf@servant.blindglobe.net>
References: <40730328.1050401@yale.edu>
	<Pine.A41.4.58.0404061249340.18914@homer32.u.washington.edu>
	<853c7g50wz.fsf@servant.blindglobe.net>
Message-ID: <Pine.A41.4.58.0404061310550.145658@homer06.u.washington.edu>

On Tue, 6 Apr 2004, A.J. Rossini wrote:

> Thomas Lumley <tlumley at u.washington.edu> writes:
>
> > A more elegant solution would use the new R condition mechanism and allow
> > assertion failures to be caught independently of errors.
>
> A pointer to this would be appreciated.
>

help(tryCatch)

	-thomas



From jasont at indigoindustrial.co.nz  Tue Apr  6 22:19:22 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 07 Apr 2004 08:19:22 +1200
Subject: [R] Ignorant lack of bliss : summarise table by column attribute
In-Reply-To: <20040406140729.GA3534@campbell-lange.net>
References: <20040406140729.GA3534@campbell-lange.net>
Message-ID: <407310CA.5060008@indigoindustrial.co.nz>

Using the data you supplied, and the work you've done so far, I also did 
this:

## make sure R knows that "group" isn't numeric
## might not be necessary, depending how you imported your data.
results$group <- factor(results$group)

Since you attached the data frame, then altered it, a warning is in 
order:  as the help page for "attach" says, the altered copy and the 
attached copy are *two different objects*.  Beware.  See the help page 
for "with" for a less confusing way to save typing.

results$resid <- results$projected - results$finalreading


Rory Campbell-Lange wrote:
 > Aims:
 >
 >     - Summarise these by groups (I can't work out how to use tapply...)


## some fun stats
tapply(results$resid, results$group, fivenum)
tapply(results$resid, results$group, mean)
tapply(results$resid, results$group, sd)
...

## nice plots:
library(lattice)
dotplot(group ~ resid, data=results)
bwplot(group ~ resid, data=results)

>     - Produce a sensible 'typification' of each group's change in
>       relation to the projected figure. I assume this would use a
>       statistical algorithm to exclude exceptions.

You'll want input from a Real Statistician for that.  This will get you 
both started:

res.mod <- lm(resid ~ group, data=results)
summary(res.mod)
plot(res.mod)

## before proceeding, find someone who understands the
## following help page.
help(p.adjust)

> If I needed to learn more is the book "Introductory Statistics with R" a
> good place to start?

Definately.  Among other things, it'll explain the cryptic remark above 
about the help page.

After you've munched through that book, get "Modern Applied Statistics 
with S", by Venables and Ripley, published by Springer-Verlag, 
preferably the 4th edition (2002).

They're both great texts for their jobs.

Cheers

Jason



From e.rexstad at uaf.edu  Tue Apr  6 22:30:06 2004
From: e.rexstad at uaf.edu (Eric Rexstad)
Date: Tue, 06 Apr 2004 12:30:06 -0800
Subject: [R] Displaying text
Message-ID: <4072A2CE.2329.1FC539E6@localhost>

Webb:

Take a look at the textplot function that lives inside Greg Warnes 
"gregmisc" package.  Note however, that textplot makes a call to 
graph.new() when called, so it will not superimpose text upon an 
existing graph, but will place text (and some tables) into empty 
graphic devices (solved by your use of mfrow).
Eric Rexstad
Institute of Arctic Biology
University of Alaska Fairbanks



From rossini at blindglobe.net  Tue Apr  6 22:30:12 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 06 Apr 2004 13:30:12 -0700
Subject: [R] assert( condition )
In-Reply-To: <Pine.A41.4.58.0404061310550.145658@homer06.u.washington.edu>
	(Thomas
	Lumley's message of "Tue, 6 Apr 2004 13:11:04 -0700 (PDT)")
References: <40730328.1050401@yale.edu>
	<Pine.A41.4.58.0404061249340.18914@homer32.u.washington.edu>
	<853c7g50wz.fsf@servant.blindglobe.net>
	<Pine.A41.4.58.0404061310550.145658@homer06.u.washington.edu>
Message-ID: <85ptak3l4b.fsf@servant.blindglobe.net>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Tue, 6 Apr 2004, A.J. Rossini wrote:
>
>> Thomas Lumley <tlumley at u.washington.edu> writes:
>>
>> > A more elegant solution would use the new R condition mechanism and allow
>> > assertion failures to be caught independently of errors.
>>
>> A pointer to this would be appreciated.
>>
>
> help(tryCatch)

Slick.  very slick if it works as well as I hope it does.  What I was looking for :-).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From bxc at steno.dk  Tue Apr  6 23:11:52 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 6 Apr 2004 23:11:52 +0200
Subject: [R] predict.gl( ..., type="terms" )
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E530@exdkba022.novo.dk>

When I do:

> apc <- glm( D ~ ns( Ax, knots=seq(50,80,10), Bo=c(40,90) ) +
+                 ns( Cx, knots=seq(1880,1940,20), Bo=c(1840,1960) ) +
+                 ns( Px, knots=seq(1960,1980,10), Bo=c(1940,2000) ) +
+                 offset( log( Y ) ),
+             family=poisson )
> pterm <-  predict( apc, type="terms" )
> plink <-  predict( apc, type="link" )
> ( apply( pterm, 1, sum ) + log( Y ) - plink )[1:10]
      1       2       3       4       5       6       7       8       9
10 
6.85047 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047
6.85047 
> coef( apc )[1] 
(Intercept) 
  -13.61998 

>From the help page for predict.glm I would have expected that the
constant 6.85 
was -intercept.

What am I missing from predict.glm? (or from splines?)

Bendix Carstensen

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From clint at ecy.wa.gov  Tue Apr  6 23:28:40 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Tue, 6 Apr 2004 14:28:40 -0700 (PDT)
Subject: [R] Syntax Question
Message-ID: <Pine.LNX.4.44.0404061408470.1426-100000@aeolus.ecy.wa.gov>

I have a large data structure that looks like:

> strsplit(st,",")[14395]
 [1] "KGEG"
 [2] "SA => KGEG"
 [3] "72785"
 [4] "47.62139"
 [5] "-117.52778"
 [6] "723"
 [7] "WA"
 [8] "US"
 [9] "2"
[10] "SPOKANE SPOKANE INTERNATIONAL AIRPORT"
[11] "1"


I'd like to be able to retrieve, for example, the latitude 
as.numeric(strsplit(st,",")[[14395]][4]) and longitude 
as.numeric(strsplit(st,",")[[14395]][5]) for the entry in the structure 
where strsplit(st,",")[[14395]][5])=="KGEG" by specifying various station 
IDs.  That is, if I had a simpler structure I could formulate a 
logical index which would have something along the lines of 
as.numeric(st[st[1]=="KSEA"][4]) and it would return 47.62139.

Somewhere I'm getting all bollixed up with the indexing and keep getting 
sytax errors.  As you can see, the list is quite long (20K+) and I don't 
wish to have to look up each coordinate by hand.

TIA

Clint

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From rpeng at jhsph.edu  Tue Apr  6 23:45:32 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 06 Apr 2004 17:45:32 -0400
Subject: [R] Syntax Question
In-Reply-To: <Pine.LNX.4.44.0404061408470.1426-100000@aeolus.ecy.wa.gov>
References: <Pine.LNX.4.44.0404061408470.1426-100000@aeolus.ecy.wa.gov>
Message-ID: <407324FC.1010803@jhsph.edu>

I'm not sure I fully understand your problem, but maybe something like:

d <- strsplit(st, ",")
index <- sapply(d, function(x) x[[1]]) == "KGEG"
latitude <- sapply(d[index], function(x) x[[4]])

-roger

Clint Bowman wrote:
> I have a large data structure that looks like:
> 
> 
>>strsplit(st,",")[14395]
> 
>  [1] "KGEG"
>  [2] "SA => KGEG"
>  [3] "72785"
>  [4] "47.62139"
>  [5] "-117.52778"
>  [6] "723"
>  [7] "WA"
>  [8] "US"
>  [9] "2"
> [10] "SPOKANE SPOKANE INTERNATIONAL AIRPORT"
> [11] "1"
> 
> 
> I'd like to be able to retrieve, for example, the latitude 
> as.numeric(strsplit(st,",")[[14395]][4]) and longitude 
> as.numeric(strsplit(st,",")[[14395]][5]) for the entry in the structure 
> where strsplit(st,",")[[14395]][5])=="KGEG" by specifying various station 
> IDs.  That is, if I had a simpler structure I could formulate a 
> logical index which would have something along the lines of 
> as.numeric(st[st[1]=="KSEA"][4]) and it would return 47.62139.
> 
> Somewhere I'm getting all bollixed up with the indexing and keep getting 
> sytax errors.  As you can see, the list is quite long (20K+) and I don't 
> wish to have to look up each coordinate by hand.
> 
> TIA
> 
> Clint
>



From p.dalgaard at biostat.ku.dk  Wed Apr  7 00:07:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Apr 2004 00:07:59 +0200
Subject: [R] predict.gl( ..., type="terms" )
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E530@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90179E530@exdkba022.novo.dk>
Message-ID: <x2k70svjy8.fsf@biostat.ku.dk>

"BXC (Bendix Carstensen)" <bxc at steno.dk> writes:

> > apc <- glm( D ~ ns( Ax, knots=seq(50,80,10), Bo=c(40,90) ) +
> +                 ns( Cx, knots=seq(1880,1940,20), Bo=c(1840,1960) ) +
> +                 ns( Px, knots=seq(1960,1980,10), Bo=c(1940,2000) ) +
> +                 offset( log( Y ) ),
> +             family=poisson )
> > pterm <-  predict( apc, type="terms" )
> > plink <-  predict( apc, type="link" )
> > ( apply( pterm, 1, sum ) + log( Y ) - plink )[1:10]
>       1       2       3       4       5       6       7       8       9
> 10 
> 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047 6.85047
> 6.85047 
> > coef( apc )[1] 
> (Intercept) 
>   -13.61998 
> 
> >From the help page for predict.glm I would have expected that the
> constant 6.85 
> was -intercept.
> 
> What am I missing from predict.glm? (or from splines?)

Same thing that you're missing from predict(..., type="terms") in general.
Try

y <- rnorm(10)
x <- runif(10)
apc <- lm(y~x)
predict(apc,type="terms")
predict(apc,type="terms") - predict(apc,type="response")
mean(predict(apc,type="terms"))

and I think enlightenment will follow.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jasont at indigoindustrial.co.nz  Wed Apr  7 00:09:19 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 7 Apr 2004 10:09:19 +1200 (NZST)
Subject: [R] Syntax Question
In-Reply-To: <Pine.LNX.4.44.0404061408470.1426-100000@aeolus.ecy.wa.gov>
References: <Pine.LNX.4.44.0404061408470.1426-100000@aeolus.ecy.wa.gov>
Message-ID: <26142.203.9.176.60.1081289359.squirrel@webmail.maxnet.co.nz>

> I have a large data structure that looks like:
>
>> strsplit(st,",")[14395]
>  [1] "KGEG"
>  [2] "SA => KGEG"
>  [3] "72785"
>  [4] "47.62139"
>  [5] "-117.52778"
>  [6] "723"
>  [7] "WA"
>  [8] "US"
>  [9] "2"
> [10] "SPOKANE SPOKANE INTERNATIONAL AIRPORT"
> [11] "1"
>
>
> I'd like to be able to retrieve, for example, the latitude
> as.numeric(strsplit(st,",")[[14395]][4]) and longitude
> as.numeric(strsplit(st,",")[[14395]][5]) for the entry in the structure
> where strsplit(st,",")[[14395]][5])=="KGEG" by specifying various station
> IDs.  That is, if I had a simpler structure I could formulate a
> logical index which would have something along the lines of
> as.numeric(st[st[1]=="KSEA"][4]) and it would return 47.62139.

Too confusing for me.  I'd just convert it to a data frame.

## UNTESTED!!!
stm <- t(matrix(st,nrow=11))
stdf <- data.frame(stm)
stdf[stdf[,1]=="KSEA",4]

## <OPTIONAL> - just makes life easier later
## do some "as.numeric" calls to various columns.
stdf[,x] <- as.numeric(stdf[,x])
## add some names
names(stdf) <- c("blah",...)
## </OPTIONAL>

Cheers

Jason



From clint at ecy.wa.gov  Wed Apr  7 00:46:42 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Tue, 6 Apr 2004 15:46:42 -0700 (PDT)
Subject: [R] Syntax Question
In-Reply-To: <26142.203.9.176.60.1081289359.squirrel@webmail.maxnet.co.nz>
Message-ID: <Pine.LNX.4.44.0404061544020.1426-100000@aeolus.ecy.wa.gov>

Thanks, I have gotten past the problem with good old grep:

as.numeric(strsplit(st[grep("KGEG",st)],",")[[1]][4])
[1] 47.62139

The difficulty is trying to work with some complicated records that are 
coded up to work with perl.  But grep will work perfectly.

Thanks all,

Clint

On Wed, 7 Apr 2004, Jason Turner wrote:

> > I have a large data structure that looks like:
> >
> >> strsplit(st,",")[14395]
> >  [1] "KGEG"
> >  [2] "SA => KGEG"
> >  [3] "72785"
> >  [4] "47.62139"
> >  [5] "-117.52778"
> >  [6] "723"
> >  [7] "WA"
> >  [8] "US"
> >  [9] "2"
> > [10] "SPOKANE SPOKANE INTERNATIONAL AIRPORT"
> > [11] "1"
> >
> >
> > I'd like to be able to retrieve, for example, the latitude
> > as.numeric(strsplit(st,",")[[14395]][4]) and longitude
> > as.numeric(strsplit(st,",")[[14395]][5]) for the entry in the structure
> > where strsplit(st,",")[[14395]][5])=="KGEG" by specifying various station
> > IDs.  That is, if I had a simpler structure I could formulate a
> > logical index which would have something along the lines of
> > as.numeric(st[st[1]=="KSEA"][4]) and it would return 47.62139.
> 
> Too confusing for me.  I'd just convert it to a data frame.
> 
> ## UNTESTED!!!
> stm <- t(matrix(st,nrow=11))
> stdf <- data.frame(stm)
> stdf[stdf[,1]=="KSEA",4]
> 
> ## <OPTIONAL> - just makes life easier later
> ## do some "as.numeric" calls to various columns.
> stdf[,x] <- as.numeric(stdf[,x])
> ## add some names
> names(stdf) <- c("blah",...)
> ## </OPTIONAL>
> 
> Cheers
> 
> Jason
> 
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From p.connolly at hortresearch.co.nz  Wed Apr  7 01:33:54 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 7 Apr 2004 11:33:54 +1200
Subject: [R] Index of a Loop Variable?
In-Reply-To: <200404061555.AA5636296@priscian.com>;
	from jjava@priscian.com on Tue, Apr 06, 2004 at 03:55:21PM -0700
References: <200404061555.AA5636296@priscian.com>
Message-ID: <20040407113354.L2137@hortresearch.co.nz>

On Tue, 06-Apr-2004 at 03:55PM -0700, Jim Java wrote:

|> Hi Everyone:--
|> 
|> Is it possible, within a for loop not explicitly using whole-number
|> indexing, to find out the index value of the loop variable within the

Depending on what you do with the index in the loop, it's simple
enough to use the names of a list instead of the indexes.  For
example, you can use the part of the list identified by its name
without needing to know what its index is.

for(i in names(xx)){
  do.something(xx[[i]], ...)
  etc
}

But if it's the index you need, the answer would be No.

HTH


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From jeaneid at chass.utoronto.ca  Wed Apr  7 02:12:29 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 6 Apr 2004 20:12:29 -0400
Subject: [R] percentile-percentile plot
In-Reply-To: <1ut570h6vphl7v9ktbnp67si9fhtrbtgfh@4ax.com>
Message-ID: <Pine.SGI.4.40.0404062004350.7739418-100000@origin.chass.utoronto.ca>

Thank you Duncan,



On Tue, 6 Apr 2004, Duncan Murdoch wrote:

> On Tue, 6 Apr 2004 13:33:58 -0400, Jean Eid
> <jeaneid at chass.utoronto.ca> wrote :
>
> >Hi,
> >
> >Is there a function that does percentile-percentile plot. I do not mean
> >the qqplot. I need to plot the percentiles rather than points themselves.
> >I am hoping for a plot that tells me that the x percentile of one data set
> >corresponds to the y percentile of the other. for example a point on the
> >plot of (.5, .2) will tell me that the 50th percentile of the first data
> >and the 20th percentile of the second data are equal.
> >
> >Is there any function that does this?
>
> I think you'll have to write your own.  Here's a rough start:
>
> ppplot <- function(x,y) {
>   x <- sort(x)
>   y <- sort(y)
>   px <- ppoints(x)
>   py <- ppoints(y)
>   plot(px, approx(y, py, x)$y)
> }
>
> This function messes up if the distributions are too different; you'll
> need to fix up the approx call somehow.
>
> Duncan Murdoch
>
>



From ivo.welch at yale.edu  Wed Apr  7 03:30:23 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Tue, 06 Apr 2004 21:30:23 -0400
Subject: [R] assert(condition)
Message-ID: <407359AF.3030408@yale.edu>


thank you all for all the advice.  regards, /iaw



From clongson at bio.mq.edu.au  Wed Apr  7 03:41:11 2004
From: clongson at bio.mq.edu.au (Chris Longson)
Date: Wed, 7 Apr 2004 11:41:11 +1000
Subject: [R] Another solution to eps/latex page rotation problem
Message-ID: <20040407014111.GA21467@smtp.paradise.net.nz>

Hello!

I was going to write to this list asking for help with eps figures
that make the whole page rotate in latex (I'm doing a presentation
with the "prosper" class). I know that this has been discussed before
here, but none of the proposed solutions worked for me. Then I found
that giving ps2pdf (or dvipdf) the option:

-dAutoRotatePages=/None

Solves the problem! I hope this is helpful to somebody else, it's been
driving me nuts...

Regards,
Chris



From ggrothendieck at myway.com  Wed Apr  7 06:29:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 7 Apr 2004 04:29:37 +0000 (UTC)
Subject: [R] Index of a Loop Variable?
References: <200404061555.AA5636296@priscian.com>
Message-ID: <loom.20040407T061732-475@post.gmane.org>

>

You could define the utility function iter:

iter <- function(x) mapply( x, seq(along=x),
                      FUN=function(x,i)list(x=x,i=i), SIMPLIFY=F )

and use it like this to loop over seq(5,50,5) and 1:10 simultaneously:

  > for(idx in iter(seq(5,50,5))) with(idx, cat(x,i,"\n"))
  5 1 
  10 2 
  15 3 
  20 4 
  25 5 
  30 6 
  35 7 
  40 8 
  45 9 
  50 10 



Jim Java writes--

 
 Hi Everyone:--
 
 Is it possible, within a for loop not explicitly using whole-number
 indexing, to find out the index value of the loop variable within the
 vector or list that's being looped through? For example, in --
 
 x <- seq(5, 50, by=5)
 index.in.x <- 1
 for (i in x) {
   cat(paste(" index of i-value ", i, " within x: ", index.in.x, sep=""), 
fill=T)
   index.in.x <- index.in.x + 1
 }
 
  -- is it in general possible to get values of "index.in.x" without
 making it a count variable, as above?
 
 Thank you.
 
  -- Jim Java



From ggrothendieck at myway.com  Wed Apr  7 07:21:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 7 Apr 2004 05:21:40 +0000 (UTC)
Subject: [R] k nearest neighbours
References: <407278B8.6050008@hotmail.com>
Message-ID: <loom.20040407T071340-758@post.gmane.org>

>

- you could loop over the points in A using sapply so you don't store the
huge matrix
- R accepts logical indices -- you can eliminate which
- R has a rank function so you don't need to define your own.  You probably
want to use the ties="first" or ties="random" arg in that.  


Angel Lopez <angel_lul <at> hotmail.com> writes:

: 
: I want to
: 1) Select for each of the n points in a matrix A, those of the m points 
: in B that lay within a given radius.
: 2) Of those points within the radius, select the k nearest ones.
: 
: What I now do is
: 1) Create an n*m matrix C were I put the distances from all the points 
: in B to the points in A and make NA those cells were the distance is 
: larger than the radius. (The points are geographical locations so I use 
: function rdist.earth in package fields) e.g.:
: library(fields)
: data(ozone)
: A<-cbind(ozone$lon.lat[1:10,])
: B<-cbind(ozone$lon.lat+2)
: C<-rdist.earth(A,B)
: radius<-180 # The search radius
: C[which(C>radius)]<-NA
: 
: 2) Then I make NA everything but the k nearest ones
: k<-5 # The nearest neighbours
: rank<-function(rank){rank<-sort.list(sort.list(rank,))};
: C[which(apply(C,2,rank)>k)]<-NA;
: 
: My problem is that the code is quite slow and due to the need to create 
: a n*m matrix I run out of memory many times. I would also prefer to get 
: a C matrix that is n*k instead of n*m were each of the values in C 
: indicated the row in B were the corresponding knearest point would be.
: But I can not find a way to solve my main problem which is the need to 
: create a n*m matrix.
: Thanks for any clues,
: Angel



From rksh at soc.soton.ac.uk  Wed Apr  7 11:09:51 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 7 Apr 2004 09:09:51 +0000
Subject: [R] %+=% and eval.parent()
In-Reply-To: <q2a570hiel5ilg76ulg1h4eengqskreqps@4ax.com>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
	<q2a570hiel5ilg76ulg1h4eengqskreqps@4ax.com>
Message-ID: <a06002014bc9975665894@[139.166.242.29]>

Hi again everybody.

Yesterday I had a problem with c-style "+=" functions.  One suggestion
was to define

R> "plus<-" <- function(x,value){x+value}

Then

R> a <- matrix(1:9,3,3)
R> plus(a[a%%2==1]) <- 1000

works as desired.


QUESTION: why does this behave differently:


R> "plus<-" <- function(x,y){x+y}
R> a <- matrix(1:9,3,3)

R> plus(a[a%%2==1]) <- 1000
Error in "plus<-"(`*tmp*`, value = 1000) :
	unused argument(s) (value ...)
R>

The only change seems to be changing the second argument from "value"
to "y".  Why does this affect anything?  Where do I look for
documentation on things like "plus<-"  ?

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From asemeria at cramont.it  Wed Apr  7 10:29:35 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Wed, 7 Apr 2004 10:29:35 +0200
Subject: [R] %+=% and eval.parent()
Message-ID: <OF23D57F11.750E166C-ONC1256E6F.002EA733@tomware.it>





Value in an internal variable of a command "function",
have a look on R-help apropos "function".

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From anne.piotet at urbanet.ch  Wed Apr  7 10:31:12 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Wed, 7 Apr 2004 10:31:12 +0200
Subject: [R] labels of data in cluster plot
Message-ID: <001b01c41c7a$afceccd0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040407/3e96b032/attachment.pl

From asemeria at cramont.it  Wed Apr  7 10:46:02 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Wed, 7 Apr 2004 10:46:02 +0200
Subject: [R] labels of data in cluster plot
Message-ID: <OF180F9FF3.D5D1D401-ONC1256E6F.0030289A@tomware.it>





"labels" option in plot(pam.object) is what you want,
you can found a description on help("clusplot.default"), this
the method called by plot when act on pam object.

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From p.dalgaard at biostat.ku.dk  Wed Apr  7 10:52:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Apr 2004 10:52:02 +0200
Subject: [R] %+=% and eval.parent()
In-Reply-To: <a06002014bc9975665894@[139.166.242.29]>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
	<q2a570hiel5ilg76ulg1h4eengqskreqps@4ax.com>
	<a06002014bc9975665894@[139.166.242.29]>
Message-ID: <x2hdvwi30t.fsf@biostat.ku.dk>

Robin Hankin <rksh at soc.soton.ac.uk> writes:

> Hi again everybody.
> 
> Yesterday I had a problem with c-style "+=" functions.  One suggestion
> was to define
> 
> R> "plus<-" <- function(x,value){x+value}
> 
> Then
> 
> R> a <- matrix(1:9,3,3)
> R> plus(a[a%%2==1]) <- 1000
> 
> works as desired.
> 
> 
> QUESTION: why does this behave differently:
> 
> 
> R> "plus<-" <- function(x,y){x+y}
> R> a <- matrix(1:9,3,3)
> 
> R> plus(a[a%%2==1]) <- 1000
> Error in "plus<-"(`*tmp*`, value = 1000) :
> 	unused argument(s) (value ...)
> R>
> 
> The only change seems to be changing the second argument from "value"
> to "y".  Why does this affect anything?  Where do I look for
> documentation on things like "plus<-"  ?

These assignment functions work basically by

 plus(x) <- foo 

getting internally transcribed as

 x <- "plus<-"(x, value=foo)

(actually, there's an intermediate alias for the target to avoid
multiple evaluation; this is what shows up as *tmp*, but you're not
supposed to know that...)

The use of the keyword matching form was prompted by some problems
with indexing functions that take a variable number of indices in
addition to x. (Think "[<-"(x,i,value) vs. "[<-"(x,i,j,value) and the
hoops you have to jump through when the value gets passed in the j
argument.) However, keyword matching of course implies that you need
to use the matching keyword in the function definition.

This *should* be somewhere in the R Language Definition, although I'm
not sure it is actually there. Or the blue book, although I suspect
that S v.3 actually used positional matching (and jumped through
hoops).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tpapp at axelero.hu  Wed Apr  7 11:44:20 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Wed, 7 Apr 2004 11:44:20 +0200
Subject: [R] eigenvalues for a sparse matrix
Message-ID: <20040407094420.GA1961@localhost>

Hi,

I have the following problem.  It has two parts.

1. I need to calculate the stationary probabilities of a Markov chain,
eg if the transition matrix is P, I need x such that

xP = x

in other words, the left eigenvectors of P which have an eigenvalue of
one.

Currently I am using eigen(t(P)) and then pick out the vectors I need.
However, this seems to be an overkill (I only need a single vector!)
and takes a lot of time -- P is 1176 x 1176!  Is there a faster way?


2. In fact, P has a structure: it comes from the solution of a
discrete dynamic optimzation problem.  There are exogenous (X) and
endogenous (N) states, and I have a policy function X x N -> N, which
gives the choice of the agent for any (x,n) in (X,N).  X has an
exogenous transition matrix.  I use the following function to build
the "global" transition matrix:

globalTransition <- function(U, modelenv) {
  G <- matrix(0, modelenv$Nn*modelenv$Xn, modelenv$Nn*modelenv$Xn)
                                        # this matrix is very sparse
  for (i in 1:modelenv$Nn) {
    r <- (i - 1)*modelenv$Xn            # start at this row
    for (j in 1:modelenv$Xn) {
      G[r+j, 1:modelenv$Xn+modelenv$Xn*(U[j,i]-1)] <-
        modelenv$Xtrans[j, 1:modelenv$Xn]
    }
  }
  G
}

Nn and Xn are the number of engo- and exogenous states, U is the said
policy function in a matrix form, and Xtrans is the transition matrix
of exogenous states.

The row (and column) indexes of G run like this: 

index Nn Xn
1     1  1
2     1  2
3     1  3
...
Xn    1  Xn
Xn+1  2  1
...
Nn*Xn Nn Xn

As you can see from the above function, each row contains just a few
nonzero elements in columns Xn*(U[j,i]-1)+1, ...; this means that the
agent chose the endogenous state n = U[j,i].

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From angel_lul at hotmail.com  Wed Apr  7 13:45:41 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 07 Apr 2004 12:45:41 +0100
Subject: Update R Import/Export Manual? was: [R] A package to read and write
	NetCDF?
In-Reply-To: <200404061454.03263.stephan@met.ed.ac.uk>
References: <OAEOKPIGCLDDHAEMCAKIEEDMCKAA.sdhyok@email.unc.edu>
	<200404061454.03263.stephan@met.ed.ac.uk>
Message-ID: <4073E9E5.4010005@hotmail.com>

Maybe this package and the one I mentioned (ncdf) should be in the 
manual "R Data Import/Export" that now says:
---
Binary data formats


Packages hdf5 and netCDF on CRAN provide experimental interfaces to 
NASA's HDF5 (Hierarchical Data Format, see 
http://hdf.ncsa.uiuc.edu/HDF5/) and to UCAR's netCDF data files (network 
Common Data Form, see http://www.unidata.ucar.edu/packages/netcdf/), 
respectively.


Both of these are systems to store scientific data in array-oriented 
ways, including descriptions, labels, formats, units, .... HDF5 also 
allows groups of arrays, and the R interface maps lists to HDF5 groups, 
and can write numeric and character vectors and matrices.


The R interface can only read netCDF, not write it (yet).
---

Well, the yet seems no longer valid, package ncdf (and apparently 
clim.pact also) are able to write netCDF.
cheers,
Angel


Stephan Matthiesen wrote:
> Hi,
> 
> Am Dienstag, 6. April 2004 02:35 schrieb Shin, Daehyok:
> 
>>I am looking for a package to read and write NetCDF files.
>>NetCDF package says it can only read, not write.
>>Another package for the standard binary file format?
> 
> 
> The package clim.pact has the functions r2cdf and retrieve.nc
> I haven't tried them, but the documentation says they are for NetCDF writing 
> and reading. Hope that helps...
> 
> Stephan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> .
>



From Joerg.Schaber at uv.es  Wed Apr  7 12:47:02 2004
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Wed, 07 Apr 2004 12:47:02 +0200
Subject: [R] installing RMySQL
Message-ID: <4073DC26.2050208@uv.es>

Hi,

I have problems installing RMySQL under R1.8.1 and RHlinux (kernel2.4.7).
During compilation I get:
** libs
gcc -I/usr/lib/R/include -I/usr/include/mysql -I/usr/local/include 
-D__NO_MATH_INLINES -mieee-fp  -fPIC  -O2 -m486 -fno-strength-reduce -g 
-c RS-DBI.c -o RS-DBI.o
gcc -I/usr/lib/R/include -I/usr/include/mysql -I/usr/local/include 
-D__NO_MATH_INLINES -mieee-fp  -fPIC  -O2 -m486 -fno-strength-reduce -g 
-c RS-MySQL.c -o RS-MySQL.o
gcc -shared -L/usr/local/lib -o RMySQL.so RS-DBI.o RS-MySQL.o 
-lmysqlclient -lz
/usr/bin/ld: cannot find -lz
collect2: ld returned 1 exit status
make: *** [RMySQL.so] Error 1
ERROR: compilation failed for package 'RMySQL'

Do I possibly have to upgrade gcc or ld?
GNU ld version 2.11.90.0.8 (with BFD 2.11.90.0.8)
gcc version 2.96 20000731 (Red Hat Linux 7.1 2.96-98)

Thanks,

joerg



From Arnaud_Amsellem at ssga.com  Wed Apr  7 12:52:36 2004
From: Arnaud_Amsellem at ssga.com (Arnaud_Amsellem@ssga.com)
Date: Wed, 7 Apr 2004 11:52:36 +0100
Subject: [R] Time Varying Coefficients
Message-ID: <OF82C114A6.D20745CD-ON80256E6F.003B34AF-80256E6F.003BBFCE@statestr.com>

I'd like to estimate time varying coefficients in a linear regression using
a Kalman filter.
Even if the Kalman Filter seems to be available in some packages I can't
figure out how to use it to estimate the coefficients.
Is there anyway to do that in R?

Any help appreciated

Thanks



From dmurdoch at pair.com  Wed Apr  7 13:21:08 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 07 Apr 2004 07:21:08 -0400
Subject: [R] %+=% and eval.parent()
In-Reply-To: <loom.20040406T161219-676@post.gmane.org>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
	<loom.20040406T161219-676@post.gmane.org>
Message-ID: <5po7705leb2h9p5phuv20pvn8ef6hnukh3@4ax.com>

On Tue, 6 Apr 2004 14:18:42 +0000 (UTC), you wrote:

>Robin Hankin <rksh <at> soc.soton.ac.uk> writes:
>> R> a <- matrix(1:9,3,3)
>> But the following caught me off-guard:
>> R> a <- matrix(1:9,3,3)
>> R> a[a%%2==1] %+=% 1000*(1:5)
>
>How about this way:
>
>> a <- matrix(1:9,3,3)
>> "plus<-" <- function(a,value) a+value
>> plus(a[a%%2==1]) <- 1000*(1:5)
>> a
>     [,1] [,2] [,3]
>[1,] 1001    4 4007
>[2,]    2 3005    8
>[3,] 2003    6 5009

Wouldn't a clearer syntax to follow be Pascal's?  I.e.

inc <- function(a, b) {
    eval.parent(substitute(a <- a + b))
}

Then in order to increment x[3], you'd use

inc(x[3], 1)

etc.

Duncan Murdoch



From B.Rowlingson at lancaster.ac.uk  Wed Apr  7 13:34:17 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 07 Apr 2004 12:34:17 +0100
Subject: [R] Minimize a plot window
In-Reply-To: <200404061509.51753.mok2@physics.buffalo.edu>
References: <200404061509.51753.mok2@physics.buffalo.edu>
Message-ID: <4073E739.4070709@lancaster.ac.uk>

Mark O. Kimball wrote:
> Is it possible to start a X11() device in a minimized state? I have many
> windows, each with useful data, which clutter the screen. Having each
> window minimize upon creation would help.

  I tried really hard, but couldn't find a way to do this.

  Firstly, X11() doesn't handle arbitrary X arguments, so you can't do: 
X11("-iconify");

  Secondly, X11() doesn't bother with X resources, so you can't add 
"R_X11.iconic: 1" or similar to your Xresources.

  If you are using the twm window manager, you might be able to tell it 
to start X11 windows iconified using the 'StartIconified [{win-list}]' 
option. It says:

         "This  is  useful  for programs that do not support an
         -iconic command line option or resource."

  But these days twm isn't much useful for anything else...

  So, perhaps some coding in devX11.c required....

Baz



From ozric at web.de  Wed Apr  7 13:52:45 2004
From: ozric at web.de (Christian Schulz)
Date: Wed, 7 Apr 2004 13:52:45 +0200
Subject: [R] installing RMySQL
In-Reply-To: <4073DC26.2050208@uv.es>
References: <4073DC26.2050208@uv.es>
Message-ID: <200404071352.45849.ozric@web.de>

Your  gcc cannot find libz, which most 
 likely means that you do not have installed the 
 development version of it.
 Search for libz or zlib  in your rpm manager!?

christian


Am Mittwoch, 7. April 2004 12:47 schrieb Joerg Schaber:
> Hi,
>
> I have problems installing RMySQL under R1.8.1 and RHlinux (kernel2.4.7).
> During compilation I get:
> ** libs
> gcc -I/usr/lib/R/include -I/usr/include/mysql -I/usr/local/include
> -D__NO_MATH_INLINES -mieee-fp  -fPIC  -O2 -m486 -fno-strength-reduce -g
> -c RS-DBI.c -o RS-DBI.o
> gcc -I/usr/lib/R/include -I/usr/include/mysql -I/usr/local/include
> -D__NO_MATH_INLINES -mieee-fp  -fPIC  -O2 -m486 -fno-strength-reduce -g
> -c RS-MySQL.c -o RS-MySQL.o
> gcc -shared -L/usr/local/lib -o RMySQL.so RS-DBI.o RS-MySQL.o
> -lmysqlclient -lz
> /usr/bin/ld: cannot find -lz
> collect2: ld returned 1 exit status
> make: *** [RMySQL.so] Error 1
> ERROR: compilation failed for package 'RMySQL'
>
> Do I possibly have to upgrade gcc or ld?
> GNU ld version 2.11.90.0.8 (with BFD 2.11.90.0.8)
> gcc version 2.96 20000731 (Red Hat Linux 7.1 2.96-98)
>
> Thanks,
>
> joerg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Apr  7 13:51:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Apr 2004 13:51:52 +0200
Subject: [R] %+=% and eval.parent()
In-Reply-To: <5po7705leb2h9p5phuv20pvn8ef6hnukh3@4ax.com>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
	<loom.20040406T161219-676@post.gmane.org>
	<5po7705leb2h9p5phuv20pvn8ef6hnukh3@4ax.com>
Message-ID: <x2oeq4m2ef.fsf@biostat.ku.dk>

Duncan Murdoch <dmurdoch at pair.com> writes:

> On Tue, 6 Apr 2004 14:18:42 +0000 (UTC), you wrote:
> 
> >Robin Hankin <rksh <at> soc.soton.ac.uk> writes:
> >> R> a <- matrix(1:9,3,3)
> >> But the following caught me off-guard:
> >> R> a <- matrix(1:9,3,3)
> >> R> a[a%%2==1] %+=% 1000*(1:5)
> >
> >How about this way:
> >
> >> a <- matrix(1:9,3,3)
> >> "plus<-" <- function(a,value) a+value
> >> plus(a[a%%2==1]) <- 1000*(1:5)
> >> a
> >     [,1] [,2] [,3]
> >[1,] 1001    4 4007
> >[2,]    2 3005    8
> >[3,] 2003    6 5009
> 
> Wouldn't a clearer syntax to follow be Pascal's?  I.e.
> 
> inc <- function(a, b) {
>     eval.parent(substitute(a <- a + b))
> }
> 
> Then in order to increment x[3], you'd use
> 
> inc(x[3], 1)
> 

But that's not kosher in functional programming languages where you
expect function calls not to modify their arguments, is it? (OK, so
assignment is also a function call as is setq in Lisp, but we have
syntactic sugar to cover that. That's what was nice about %<-%.)

BTW, you can also (but I'm not sure you should!) do

"+<-" <- function(x,value) x + value
x     <- c(2,3)
+x[1] <- 3

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From anne.piotet at m-td.com  Wed Apr  7 14:53:29 2004
From: anne.piotet at m-td.com (Anne Piotet)
Date: Wed, 7 Apr 2004 14:53:29 +0200
Subject: [R] labels in cluster  pam plot
Message-ID: <005001c41c9f$5403a7c0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040407/db2a2a2e/attachment.pl

From f.calboli at ucl.ac.uk  Wed Apr  7 16:13:50 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 07 Apr 2004 15:13:50 +0100
Subject: [R] problems loading package "nlme" in Debian Sid
Message-ID: <1081347229.2957.35.camel@monkey>

Dear All,

I apologize if the following has been already asked, but I could not
find anything in the archives.

I am running Debian Sid (on an Athlon XP), with R 1.8.1.cvs.20040307-1
(it describes itself as "R 1.9.0 experimental" when I fire R up) and I
am unable to load the library "nlme" (I apt-getted r-recommended, so the
library is on the system)

>library(nlme)
Error in loadNamespace((i[[1]],c(lib.loc, .libPath()),keep.source):
package "mva" does not have a namespace

In fact, in my /etc/lib/R/library/mva/ does not have a NAMESPACE file.
How can I fix this? 

I hope the fix is not as simple as upgrading to the latest cvs... but
the machine in question cannot be up online until later today, so I
could not test this out. If that's the case, I apologise for the wasted
bandwidth.

Regards,

Federico Calboli
-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 251 4286

f.calboli at ucl.ac.uk



From simon at stats.gla.ac.uk  Wed Apr  7 15:16:37 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Wed, 7 Apr 2004 14:16:37 +0100 (BST)
Subject: [R] eigenvalues for a sparse matrix
In-Reply-To: <20040407094420.GA1961@localhost>
References: <20040407094420.GA1961@localhost>
Message-ID: <Pine.SOL.4.58.0404071408040.8770@moon.stats.gla.ac.uk>

If you know the eigenvalues then you can use inverse iteration to get the
eigenvectors you need: I think that this only requires you to
be able to multiply a vector by your matrix, so that you can take
advantage of the sparse structure, but I'm away from home and can't check
references. However Golub and van Loan (1996) will probably tell you what
to do.

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



>
> I have the following problem.  It has two parts.
>
> 1. I need to calculate the stationary probabilities of a Markov chain,
> eg if the transition matrix is P, I need x such that
>
> xP = x
>
> in other words, the left eigenvectors of P which have an eigenvalue of
> one.
>
> Currently I am using eigen(t(P)) and then pick out the vectors I need.
> However, this seems to be an overkill (I only need a single vector!)
> and takes a lot of time -- P is 1176 x 1176!  Is there a faster way?
>
>
> 2. In fact, P has a structure: it comes from the solution of a
> discrete dynamic optimzation problem.  There are exogenous (X) and
> endogenous (N) states, and I have a policy function X x N -> N, which
> gives the choice of the agent for any (x,n) in (X,N).  X has an
> exogenous transition matrix.  I use the following function to build
> the "global" transition matrix:
>
> globalTransition <- function(U, modelenv) {
>   G <- matrix(0, modelenv$Nn*modelenv$Xn, modelenv$Nn*modelenv$Xn)
>                                         # this matrix is very sparse
>   for (i in 1:modelenv$Nn) {
>     r <- (i - 1)*modelenv$Xn            # start at this row
>     for (j in 1:modelenv$Xn) {
>       G[r+j, 1:modelenv$Xn+modelenv$Xn*(U[j,i]-1)] <-
>         modelenv$Xtrans[j, 1:modelenv$Xn]
>     }
>   }
>   G
> }
>
> Nn and Xn are the number of engo- and exogenous states, U is the said
> policy function in a matrix form, and Xtrans is the transition matrix
> of exogenous states.
>
> The row (and column) indexes of G run like this:
>
> index Nn Xn
> 1     1  1
> 2     1  2
> 3     1  3
> ...
> Xn    1  Xn
> Xn+1  2  1
> ...
> Nn*Xn Nn Xn
>
> As you can see from the above function, each row contains just a few
> nonzero elements in columns Xn*(U[j,i]-1)+1, ...; this means that the
> agent chose the endogenous state n = U[j,i].
>
> Thanks,
>
> Tamas
>
> --
> Tam??s K. Papp
> E-mail: tpapp at axelero.hu
> Please try to send only (latin-2) plain text, not HTML or other garbage.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmurdoch at pair.com  Wed Apr  7 15:16:52 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 07 Apr 2004 09:16:52 -0400
Subject: [R] %+=% and eval.parent()
In-Reply-To: <x2oeq4m2ef.fsf@biostat.ku.dk>
References: <a06002011bc9861cabf1d@[139.166.242.29]>
	<loom.20040406T161219-676@post.gmane.org>
	<5po7705leb2h9p5phuv20pvn8ef6hnukh3@4ax.com>
	<x2oeq4m2ef.fsf@biostat.ku.dk>
Message-ID: <ktu770ll4cqgo12hs7devg7l85q6pere99@4ax.com>

On 07 Apr 2004 13:51:52 +0200, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>Duncan Murdoch <dmurdoch at pair.com> writes:

>> Wouldn't a clearer syntax to follow be Pascal's?  I.e.

>> inc(x[3], 1)
>> 
>
>But that's not kosher in functional programming languages where you
>expect function calls not to modify their arguments, is it? (OK, so
>assignment is also a function call as is setq in Lisp, but we have
>syntactic sugar to cover that. That's what was nice about %<-%.)

I think this whole discussion is aiming for ways around functional
programming.  I think if you're going to do that, you should be clear
about it.

We can't create an operator %+=% (that's what you meant, isn't it?)
that parses like += in C or <- in R, because of limitations in R
parsing.

We can create assignment functions, but they don't read properly.  The
meaning I'd read for 

 plus(x) <- 3

would be "Set the plus feature of x to 3", and then I'd say "What's
the plus of x???"  Your 

 +x <- 3

is just as obscure.

Personally, I'd generally prefer 

 x <- x + 3

(because functional programming is good), but I can understand the
desire to avoid repeating a complicated indexing construction in order
to improve the clarity of code.  I just don't think that

 plus(x[blah1, blah2, blah3]) <- 2

is clearer than

 x[blah1, blah2, blah3] <- x[blah1, blah2, blah3] + 2

(because nothing is getting set to 2!), whereas 

 inc(x[blah1, blah2, blah3], 2)

is reasonably clear.

If this is about efficiency rather than clarity, I think that's a bad
tradeoff.

Duncan Murdoch



From jjava at priscian.com  Wed Apr  7 15:30:49 2004
From: jjava at priscian.com (Jim Java)
Date: Wed, 7 Apr 2004 09:30:49 -0400
Subject: [R] Index of a Loop Variable?
In-Reply-To: <loom.20040407T061732-475@post.gmane.org>
Message-ID: <200404071433224.SM01096@nox>

Thanks to everyone who's responded to my request for info -- the replies
have been helpful and interesting. They'll certainly help me towards
thinking more cleverly in R....

Barry: I'd like to see your looping class, but only if you haven't anything
better to do over your break than trying to put it together again. :)

 -- Jim J.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: Wednesday, April 07, 2004 12:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Index of a Loop Variable?
> 
> You could define the utility function iter:
> 
> iter <- function(x) mapply( x, seq(along=x),
>                       FUN=function(x,i)list(x=x,i=i), SIMPLIFY=F )
> 
> and use it like this to loop over seq(5,50,5) and 1:10 simultaneously:
> 
>   > for(idx in iter(seq(5,50,5))) with(idx, cat(x,i,"\n"))
>   5 1
>   10 2
>   15 3
>   20 4
>   25 5
>   30 6
>   35 7
>   40 8
>   45 9
>   50 10
> 
> Jim Java writes--
> 
>  Hi Everyone:--
> 
>  Is it possible, within a for loop not explicitly using whole-number
>  indexing, to find out the index value of the loop variable within the
>  vector or list that's being looped through?



From bates at stat.wisc.edu  Wed Apr  7 15:39:35 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 07 Apr 2004 08:39:35 -0500
Subject: [R] eigenvalues for a sparse matrix
In-Reply-To: <Pine.SOL.4.58.0404071408040.8770@moon.stats.gla.ac.uk>
References: <20040407094420.GA1961@localhost>
	<Pine.SOL.4.58.0404071408040.8770@moon.stats.gla.ac.uk>
Message-ID: <6rptaj3o14.fsf@bates4.stat.wisc.edu>

Tamas Papp <tpapp at axelero.hu> writes:

> I have the following problem.  It has two parts.
>
> 1. I need to calculate the stationary probabilities of a Markov chain,
> eg if the transition matrix is P, I need x such that
>
> xP = x
>
> in other words, the left eigenvectors of P which have an eigenvalue of
> one.
>
> Currently I am using eigen(t(P)) and then pick out the vectors I need.
> However, this seems to be an overkill (I only need a single vector!)
> and takes a lot of time -- P is 1176 x 1176!  Is there a faster way?
>
>
> 2. In fact, P has a structure: it comes from the solution of a
> discrete dynamic optimzation problem.  There are exogenous (X) and
> endogenous (N) states, and I have a policy function X x N -> N, which
> gives the choice of the agent for any (x,n) in (X,N).  X has an
> exogenous transition matrix.  I use the following function to build
> the "global" transition matrix:
>
> globalTransition <- function(U, modelenv) {
>   G <- matrix(0, modelenv$Nn*modelenv$Xn, modelenv$Nn*modelenv$Xn)
>                                         # this matrix is very sparse
>   for (i in 1:modelenv$Nn) {
>     r <- (i - 1)*modelenv$Xn            # start at this row
>     for (j in 1:modelenv$Xn) {
>       G[r+j, 1:modelenv$Xn+modelenv$Xn*(U[j,i]-1)] <-
>         modelenv$Xtrans[j, 1:modelenv$Xn]
>     }
>   }
>   G
> }
>
> Nn and Xn are the number of engo- and exogenous states, U is the said
> policy function in a matrix form, and Xtrans is the transition matrix
> of exogenous states.
>
> The row (and column) indexes of G run like this:
>
> index Nn Xn
> 1     1  1
> 2     1  2
> 3     1  3
> ...
> Xn    1  Xn
> Xn+1  2  1
> ...
> Nn*Xn Nn Xn
>
> As you can see from the above function, each row contains just a few
> nonzero elements in columns Xn*(U[j,i]-1)+1, ...; this means that the
> agent chose the endogenous state n = U[j,i].

You should be able to generate the triplet form of a sparse matrix
from that representation then generate the compressed sparse
column-oriented form using facilities in the new Matrix package for
R-1.9.0 

I'm not sure about getting eigenvectors for specific eigenvalues from
that representation, although Simon Wood's suggestion of using inverse
iteration seems like it will be quite useful.  The point of using the
cscMatrix class would be that the iteration itself would be much
faster than the same iteration using the dense matrix representation.

Look for the classes "tripletMatrix" and "cscMatrix" in Matrix version
0.8-1 or later.

There is a method for crossprod(mm, x) where mm is a cscMatrix and x
is numeric.  Because mm is column oriented the greatest efficiency is
achieved by multiplying by t(mm) on the left or mm on the right.



From edd at debian.org  Wed Apr  7 15:44:36 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 7 Apr 2004 08:44:36 -0500
Subject: [R] problems loading package "nlme" in Debian Sid
In-Reply-To: <1081347229.2957.35.camel@monkey>
References: <1081347229.2957.35.camel@monkey>
Message-ID: <20040407134436.GA19446@sonny.eddelbuettel.com>

On Wed, Apr 07, 2004 at 03:13:50PM +0100, Federico Calboli wrote:
> Dear All,
> 
> I apologize if the following has been already asked, but I could not
> find anything in the archives.
> 
> I am running Debian Sid (on an Athlon XP), with R 1.8.1.cvs.20040307-1
> (it describes itself as "R 1.9.0 experimental" when I fire R up) and I
> am unable to load the library "nlme" (I apt-getted r-recommended, so the
> library is on the system)
> 
> >library(nlme)
> Error in loadNamespace((i[[1]],c(lib.loc, .libPath()),keep.source):
> package "mva" does not have a namespace
> 
> In fact, in my /etc/lib/R/library/mva/ does not have a NAMESPACE file.
> How can I fix this? 
> 
> I hope the fix is not as simple as upgrading to the latest cvs... but
> the machine in question cannot be up online until later today, so I
> could not test this out. If that's the case, I apologise for the wasted
> bandwidth.

As you may know, I have been building and uploading both pre-1.9.0 version
of R itself (in the 1.8.1.cvs.$DATE packages, currently at
1.8.1.cvs.20040331) and of all (previously packaged) packages in
$CRAN/src/contrib/1.9.0/ which includes nlme.

As for your problem, a simple upgrade to the newest versions should do, see
below.  If you go with Debian 'sid' aka 'unstable', I do recommend a quick
'apt-get update; apt-get dist-upgrade' every few days. The problem described
here has been reported and fixed before. See below for a log, it simply
works if you have the current components.

Hope this helps,  Dirk


edd at homebud:~> R

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0 beta (2004-03-31), ISBN 3-900051-00-3

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

> library(nlme)
> 



> 
> Regards,
> 
> Federico Calboli
> -- 
> 
> 
> 
> =================================
> 
> Federico C. F. Calboli
> 
> Dipartimento di Biologia
> Via Selmi 3
> 40126 Bologna
> Italy
> 
> tel (+39) 051 209 4187
> fax (+39) 051 251 4286
> 
> f.calboli at ucl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From soranzo at psico.univ.trieste.it  Wed Apr  7 15:55:38 2004
From: soranzo at psico.univ.trieste.it (Alessandro Soranzo)
Date: Wed, 7 Apr 2004 15:55:38 +0200
Subject: [R] loading c code in windows ambient
Message-ID: <003601c41ca8$0242c430$b78a698c@psoranzo>

Dear all,
I'm studing how to include .c code in my .r functions . In the R-exts.pdf
manual I have found the following code. At one point the author write
"called from R by"...
How can I load a .c file on R? ( I am using a xp windows as so.)
Thank you
ale

void convolve(double *a, int *na, double *b, int *nb, double *ab)
{
int i, j, nab = *na + *nb - 1;
for(i = 0; i < nab; i++)
ab[i] = 0.0;
for(i = 0; i < *na; i++)
for(j = 0; j < *nb; j++)
ab[i + j] += a[i] * b[j];
}
#called from R by ??????????????????????
conv <- function(a, b)
.C("convolve",
as.double(a),
as.integer(length(a)),
as.double(b),
as.integer(length(b)),
ab = double(length(a) + length(b) - 1))$ab

From b87605205 at ntu.edu.tw  Wed Apr  7 16:13:32 2004
From: b87605205 at ntu.edu.tw (=?big5?B?s6+x0rx3?=)
Date: Wed, 7 Apr 2004 22:13:32 +0800
Subject: [R] a question about levelplot
Message-ID: <001501c41caa$8300eaf0$ca54a8c0@Kanna>

Hello everyone

I met a strange problem when I call "levelplot" in another function

>library(lattice)
>mat = matrix(1:9, nrow=3)
>mylevelplot = function() {
+    png('test.png')
+    levelplot(mat)
+    dev.off()
+}
>mylevelplot()
null device
               1

the test.png file is just an regular empty file.

but in another case
>png('test.png')
>levelplot(mat)
>dev.off()
null device
               1

will get a correct image file.

the first case will work correct if I replace the levelplot function by
image function in the above code.
why the first case doesn't work ?

Which I use R-1.8.1 and lattice 0.8-7

Thanks for any help.



From tpapp at axelero.hu  Wed Apr  7 16:27:12 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Wed, 7 Apr 2004 16:27:12 +0200
Subject: [R] Minimize a plot window
In-Reply-To: <4073E739.4070709@lancaster.ac.uk>
References: <200404061509.51753.mok2@physics.buffalo.edu>
	<4073E739.4070709@lancaster.ac.uk>
Message-ID: <20040407142712.GA862@localhost>

On Wed, Apr 07, 2004 at 12:34:17PM +0100, Barry Rowlingson wrote:

> Mark O. Kimball wrote:
> >Is it possible to start a X11() device in a minimized state? I have many
> >windows, each with useful data, which clutter the screen. Having each
> >window minimize upon creation would help.
> 
>  I tried really hard, but couldn't find a way to do this.
> 
>  Firstly, X11() doesn't handle arbitrary X arguments, so you can't do: 
> X11("-iconify");
> 
>  Secondly, X11() doesn't bother with X resources, so you can't add 
> "R_X11.iconic: 1" or similar to your Xresources.
> 
>  If you are using the twm window manager, you might be able to tell it 
> to start X11 windows iconified using the 'StartIconified [{win-list}]' 
> option. It says:
> 
>         "This  is  useful  for programs that do not support an
>         -iconic command line option or resource."
> 
>  But these days twm isn't much useful for anything else...
> 
>  So, perhaps some coding in devX11.c required....

Some window managers let you do specific things to windows matching
some sort of filter.  It is a long time since I last used the window
manager sawfish, but I think that it has something like this.  Also,
it is customizable in Lisp, so it is very flexible if the GUI doesn't
have what you are looking for.

I am using ratpoison (I admit that it is not for the casual user, but
I find it very productive, also with R).  Using the "rudeness" option,
you can disable raising windows new entirely.  You could even hack up
a call from R (ratpoison can be controlled from the command line, like
sawfish) to set rudeness to 0 when you start plotting those windows,
and restore it to whatever is convenient when you are done.  However,
if you like GUI's, ratpoison will be a bad choice for anything else
--- read the manual before using it, otherwise you won't even be able
to quit.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From rory at campbell-lange.net  Wed Apr  7 16:56:43 2004
From: rory at campbell-lange.net (Rory Campbell-Lange)
Date: Wed, 7 Apr 2004 15:56:43 +0100
Subject: [R] Help with character columns in a table
Message-ID: <20040407145643.GA3542@campbell-lange.net>

I have done a read.table on a data file with the aim of extracting
subsets of the data eg

    table[table$desc = 'result1',]

How can I convert $desc to a character type? At present it is seen by
typeof() as an integer.

How can I do the equivalent of 

    table[table$desc = 'result1' && table$amt > 20, ]

Thanks,
Rory

-- 
Rory Campbell-Lange 
<rory at campbell-lange.net>
<www.campbell-lange.net>



From andy_liaw at merck.com  Wed Apr  7 17:13:13 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 7 Apr 2004 11:13:13 -0400
Subject: [R] Help with character columns in a table
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B4D@usrymx25.merck.com>

By default, read.table & friends reads character columns in as factors.
Other options are documented in ?read.table.

If you _really_ want to convert the column to characters, use something
like:

    table$desc <- as.character(table$desc)

Two more things:

- You probably meant `==' instead of `=', and that should work even without
converting to characters.

- You should avoid using `table' as an object name, because that's also the
name of a built-in function.

HTH,
Andy

> From: Rory Campbell-Lange
> 
> I have done a read.table on a data file with the aim of extracting
> subsets of the data eg
> 
>     table[table$desc = 'result1',]
> 
> How can I convert $desc to a character type? At present it is seen by
> typeof() as an integer.
> 
> How can I do the equivalent of 
> 
>     table[table$desc = 'result1' && table$amt > 20, ]
> 
> Thanks,
> Rory
> 
> -- 
> Rory Campbell-Lange 
> <rory at campbell-lange.net>
> <www.campbell-lange.net>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From GPetris at uark.edu  Wed Apr  7 17:19:25 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Wed, 7 Apr 2004 10:19:25 -0500 (CDT)
Subject: [R] predict( ..., type="terms")
Message-ID: <200404071519.i37FJP4s003278@definetti.uark.edu>


Hello,

After reading the help for predict.lm and predict.glm, it is not clear
to me what are the values returned by predict( ..., type="terms"). 

Anybody willing to enlighten me?

The example provided by Peter Dalgaard in a recent post unfortunately
was enlightening only to the point of making me realize that "terms"
are not what I thought they were.

Thanks in advance,
Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From Giles.Heywood at CommerzbankIB.com  Wed Apr  7 17:19:25 2004
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Wed, 7 Apr 2004 16:19:25 +0100 
Subject: [R] Aggregating frequency of irregular time series
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF74D2@xmx8lonib.lonib.commerzbank.com>

Although this thread might be considered closed by some, I'd like to make a
late contribution, since I contributed the extractIts() function in the
'its' package.

See ?itsSubset, which gives details of:

extractIts(x,weekday=FALSE,find=c("all","last","first"),period=c("week","mon
th"),partials=TRUE,select)

Using this on its own just 'extracts', using it with cumsum() and diff()
will give you 'sum' aggregation albeit with loss of precision.  As for
'average', it would depend how you want to average...

- Giles

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ivan Alves
> Sent: 26 March 2004 21:51
> To: r-help at stat.math.ethz.ch
> Subject: [R] Aggregating frequency of irregular time series
> 
> 
> Thanks to all once again for the very useful suggestions. I will have 
> to teach myself some real S before tackling the writting of the 
> AggregateSeries-like function! I brace myself.
> 
> Kind regards,
> 
> Ivan
> _______________________
> Ivan Alves
> mailto://papucho at mac.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From dyang at NRCan.gc.ca  Wed Apr  7 17:20:38 2004
From: dyang at NRCan.gc.ca (Yang, Richard)
Date: Wed, 7 Apr 2004 11:20:38 -0400 
Subject: [R] NLME augPred Error
Message-ID: <F0E0B899CB43D5118D220002A55113CF04FE5577@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear all;

	This error has been posted to the list a couple of times, but no
answer / walk around solution is available. An example on P. 377 MEMSS

>plot(augPred(fm5CO2.nlme, levels=0:1), layout =c(6,2))

generates an error: "Error in predict.nlme(object, value[1:(nrow(value)/nL),
, drop = FALSE], : Levels Quebec, Mississippi not allowed for Type.

for nlme ver. 3.1-45 in R.1.8.1, but it works fine in Splus6.2 or earlier
versions. 

The source code for augPred() is very cryptica and identical in both S+ and
R:

function (object, primary = NULL, minimum = min(primary), maximum =
max(primary), 
    length.out = 51, ...) 
UseMethod("augPred").

The augPred() creates an augPred class object in S+ but generates the above
error in R. Is there any difference in nlme3 in S+ and nlme versions in R?

	Any ideas?

Richard



From tlumley at u.washington.edu  Wed Apr  7 17:43:53 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 7 Apr 2004 08:43:53 -0700 (PDT)
Subject: [R] predict( ..., type="terms")
In-Reply-To: <200404071519.i37FJP4s003278@definetti.uark.edu>
References: <200404071519.i37FJP4s003278@definetti.uark.edu>
Message-ID: <Pine.A41.4.58.0404070839040.23252@homer03.u.washington.edu>

On Wed, 7 Apr 2004, Giovanni Petris wrote:

>
> Hello,
>
> After reading the help for predict.lm and predict.glm, it is not clear
> to me what are the values returned by predict( ..., type="terms").
>
> Anybody willing to enlighten me?
>

For each term in the formula, extract its coefficients and the
corresponding columns of the design matrix, and multiply the two, and then
center the result.

So, looking at the example in example(termplot) we have a model
y~ns(x,6)+z where z is a factor with 4 levels.

There are 9 coefficients in the model, but only two terms: ns(x,6) (based
on 5 coefficients) and z (based on 3 coefficients).

Adding up the terms and the centering value (also returned by
predict(,type="terms")) gives the linear predictor.

	-thomas



From rpeng at jhsph.edu  Wed Apr  7 17:30:07 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 07 Apr 2004 11:30:07 -0400
Subject: [R] Help with character columns in a table
In-Reply-To: <20040407145643.GA3542@campbell-lange.net>
References: <20040407145643.GA3542@campbell-lange.net>
Message-ID: <40741E7F.1000009@jhsph.edu>

You could set `as.is = TRUE' when running read.table(), as the help 
page says.

-roger

Rory Campbell-Lange wrote:
> I have done a read.table on a data file with the aim of extracting
> subsets of the data eg
> 
>     table[table$desc = 'result1',]
> 
> How can I convert $desc to a character type? At present it is seen by
> typeof() as an integer.
> 
> How can I do the equivalent of 
> 
>     table[table$desc = 'result1' && table$amt > 20, ]
> 
> Thanks,
> Rory
>



From e.hagen at biologie.hu-berlin.de  Wed Apr  7 19:00:29 2004
From: e.hagen at biologie.hu-berlin.de (Ed Hagen)
Date: Wed, 07 Apr 2004 19:00:29 +0200
Subject: [R] Problems with rlm
Message-ID: <407433AD.5050209@biologie.hu-berlin.de>


Dear all,

When calling rlm with the following data, I get an error.  (R v.1.8.1, 
WinXP Pro 2002 with service pack 1.)

> d <- na.omit(data.frame(CPRATIO, HEIGHTZ, FAMILYID))
> c <- tapply(d$CPRATIO, d$FAMILYID, mean)
> h <- tapply(d$HEIGHTZ, d$FAMILYID, mean)
> c
         1         2         3         6         7         9        10
       11
  6.000000  2.500000  3.250000  4.000000  2.333333  2.400000  2.750000
2.000000
        12        13        14        15        16        18        21
       22
  4.000000  5.000000  4.000000  7.000000  4.500000  3.500000  4.000000
5.000000
        23        25        30        31        35        39        40
       44
  6.000000  3.000000  9.000000 10.000000  8.000000 10.000000  4.000000
6.000000
        45        47        50
  4.500000  5.500000  5.000000
> h
          1          2          3          6          7          9
    10
-2.7916667 -1.0246838 -2.0681170 -1.3256766 -3.1122708 -2.1444948
-2.5806538
         11         12         13         14         15         16
    18
-0.6791551 -0.1278583 -3.7737900 -3.3077561 -2.4615233 -3.4016225
-2.2484673
         21         22         23         25         30         31
    35
-0.8938528 -2.2521325 -2.3461999 -2.1405470 -2.7657994 -3.3588822
-2.8144453
         39         40         44         45         47         50
-3.8521439 -2.1882352 -1.7531826 -3.6464482 -2.2158240 -2.7162693
> out <- rlm(h ~ c)
Error in model.matrix.default(mt, mf, contrasts) :
         cannot allocate vector of length 1077237505


If I make the same rlm call a second or third time, R usually crashes.

Any help or suggestions would be greatly appreciated.  These commands
were run after a much larger script had seemingly run successfully. But,
I had no problem calling, e.g., rlm(HEIGHTZ ~ CPRATIO), after the same
script had run.  Although I have made some minor changes in the script, 
I also didn't have this problem when using R 1.7.

Many thanks in advance,

Ed.


-- 
Edward H. Hagen                  Institute for Theoretical Biology
phone: +49/30 2093-8649             Humboldt-Universit??t zu Berlin
fax:   +49/30 2093-8801                         Invalidenstra??e 43
http://itb.biologie.hu-berlin.de/~hagen      10115 Berlin, Germany



From andy_liaw at merck.com  Wed Apr  7 19:14:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 7 Apr 2004 13:14:54 -0400
Subject: [R] Problems with rlm
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B50@usrymx25.merck.com>

You have not given us nearly enough info.  On my WinXP laptop with 512MB
RAM, R-1.8.1, I get:

> system.time(out <- rlm(h ~ c))
[1] 0.03 0.00 0.04   NA   NA
> out
Call:
rlm(formula = h ~ c)
Converged in 8 iterations

Coefficients:
(Intercept)           c 
 -1.4720930  -0.1843265 

Degrees of freedom: 27 total; 25 residual
Scale estimate: 0.541 
> h
         1          2          3          6          7          9         10

-2.7916667 -1.0246838 -2.0681170 -1.3256766 -3.1122708 -2.1444948 -2.5806538

        11         12         13         14         15         16         18

-0.6791551 -0.1278583 -3.7737900 -3.3077561 -2.4615233 -3.4016225 -2.2484673

        21         22         23         25         30         31         35

-0.8938528 -2.2521325 -2.3461999 -2.1405470 -2.7657994 -3.3588822 -2.8144453

        39         40         44         45         47         50 
-3.8521439 -2.1882352 -1.7531826 -3.6464482 -2.2158240 -2.7162693 
> c
        1         2         3         6         7         9        10
11 
 6.000000  2.500000  3.250000  4.000000  2.333333  2.400000  2.750000
2.000000 
       12        13        14        15        16        18        21
22 
 4.000000  5.000000  4.000000  7.000000  4.500000  3.500000  4.000000
5.000000 
       23        25        30        31        35        39        40
44 
 6.000000  3.000000  9.000000 10.000000  8.000000 10.000000  4.000000
6.000000 
       45        47        50 
 4.500000  5.500000  5.000000 

What OS / R version / MASS version (you are using MASS, no?) are you using?
What's your hardware?  Do you have lots of other stuff in the workspace when
you tried to run this?  What does gc() tell you?

Andy

> From: Ed Hagen
> 
> Dear all,
> 
> When calling rlm with the following data, I get an error.  (R 
> v.1.8.1, 
> WinXP Pro 2002 with service pack 1.)
> 
> > d <- na.omit(data.frame(CPRATIO, HEIGHTZ, FAMILYID))
> > c <- tapply(d$CPRATIO, d$FAMILYID, mean)
> > h <- tapply(d$HEIGHTZ, d$FAMILYID, mean)
> > c
>          1         2         3         6         7         9        10
>        11
>   6.000000  2.500000  3.250000  4.000000  2.333333  2.400000  2.750000
> 2.000000
>         12        13        14        15        16        18        21
>        22
>   4.000000  5.000000  4.000000  7.000000  4.500000  3.500000  4.000000
> 5.000000
>         23        25        30        31        35        39        40
>        44
>   6.000000  3.000000  9.000000 10.000000  8.000000 10.000000  4.000000
> 6.000000
>         45        47        50
>   4.500000  5.500000  5.000000
> > h
>           1          2          3          6          7          9
>     10
> -2.7916667 -1.0246838 -2.0681170 -1.3256766 -3.1122708 -2.1444948
> -2.5806538
>          11         12         13         14         15         16
>     18
> -0.6791551 -0.1278583 -3.7737900 -3.3077561 -2.4615233 -3.4016225
> -2.2484673
>          21         22         23         25         30         31
>     35
> -0.8938528 -2.2521325 -2.3461999 -2.1405470 -2.7657994 -3.3588822
> -2.8144453
>          39         40         44         45         47         50
> -3.8521439 -2.1882352 -1.7531826 -3.6464482 -2.2158240 -2.7162693
> > out <- rlm(h ~ c)
> Error in model.matrix.default(mt, mf, contrasts) :
>          cannot allocate vector of length 1077237505
> 
> 
> If I make the same rlm call a second or third time, R usually crashes.
> 
> Any help or suggestions would be greatly appreciated.  These commands
> were run after a much larger script had seemingly run 
> successfully. But,
> I had no problem calling, e.g., rlm(HEIGHTZ ~ CPRATIO), after the same
> script had run.  Although I have made some minor changes in 
> the script, 
> I also didn't have this problem when using R 1.7.
> 
> Many thanks in advance,
> 
> Ed.
> 
> 
> -- 
> Edward H. Hagen                  Institute for Theoretical Biology
> phone: +49/30 2093-8649             Humboldt-Universit??t zu Berlin
> fax:   +49/30 2093-8801                         Invalidenstra??e 43
> http://itb.biologie.hu-berlin.de/~hagen      10115 Berlin, Germany
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dmurdoch at pair.com  Wed Apr  7 19:21:17 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 07 Apr 2004 13:21:17 -0400
Subject: [R] loading c code in windows ambient
In-Reply-To: <003601c41ca8$0242c430$b78a698c@psoranzo>
References: <003601c41ca8$0242c430$b78a698c@psoranzo>
Message-ID: <bsd870hpa23tcki9kh0ehuf4s9ri07mp87@4ax.com>

On Wed, 7 Apr 2004 15:55:38 +0200, "Alessandro Soranzo"
<soranzo at psico.univ.trieste.it> wrote :

>Dear all,
>I'm studing how to include .c code in my .r functions . In the R-exts.pdf
>manual I have found the following code. At one point the author write
>"called from R by"...
>How can I load a .c file on R? ( I am using a xp windows as so.)

You need to compile it into a DLL.  How to do that depends on your
compiler.  Instructions for the recommended compiler (MinGW's gcc) are
in the readme.packages file (which should have been installed in the R
home directory).  Instructions for some other compilers are on my web
page, at
<http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/>.

>}
>#called from R by ??????????????????????
>conv <- function(a, b)
>.C("convolve",
>as.double(a),
>as.integer(length(a)),
>as.double(b),
>as.integer(length(b)),
>ab = double(length(a) + length(b) - 1))$ab

This will look for a function named "convolve", in one of the loaded
DLLs.  It's safer (and R CMD CHECK now warns you about this) to
specify the package where "convolve" lives; then the search will look
in the DLL for that package.  See the help for .C for the syntax.

Duncan Murdoch



From andy_liaw at merck.com  Wed Apr  7 19:23:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 7 Apr 2004 13:23:41 -0400
Subject: [R] Problems with rlm
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B51@usrymx25.merck.com>

My apologies for not reading:

> When calling rlm with the following data, I get an error.  (R v.1.8.1, 
> WinXP Pro 2002 with service pack 1.)

If you get the error toward the end of a long script, you might have too
many things taking up space in the workspace.  Try rm() as many objects as
you can in places within the script, and perhaps add a few calls to gc() as
well and see if that helps.

Andy

> From: Liaw, Andy
> 
> You have not given us nearly enough info.  On my WinXP laptop 
> with 512MB
> RAM, R-1.8.1, I get:
> 
> > system.time(out <- rlm(h ~ c))
> [1] 0.03 0.00 0.04   NA   NA
> > out
> Call:
> rlm(formula = h ~ c)
> Converged in 8 iterations
> 
> Coefficients:
> (Intercept)           c 
>  -1.4720930  -0.1843265 
> 
> Degrees of freedom: 27 total; 25 residual
> Scale estimate: 0.541 
> > h
>          1          2          3          6          7        
>   9         10
> 
> -2.7916667 -1.0246838 -2.0681170 -1.3256766 -3.1122708 
> -2.1444948 -2.5806538
> 
>         11         12         13         14         15        
>  16         18
> 
> -0.6791551 -0.1278583 -3.7737900 -3.3077561 -2.4615233 
> -3.4016225 -2.2484673
> 
>         21         22         23         25         30        
>  31         35
> 
> -0.8938528 -2.2521325 -2.3461999 -2.1405470 -2.7657994 
> -3.3588822 -2.8144453
> 
>         39         40         44         45         47         50 
> -3.8521439 -2.1882352 -1.7531826 -3.6464482 -2.2158240 -2.7162693 
> > c
>         1         2         3         6         7         9        10
> 11 
>  6.000000  2.500000  3.250000  4.000000  2.333333  2.400000  2.750000
> 2.000000 
>        12        13        14        15        16        18        21
> 22 
>  4.000000  5.000000  4.000000  7.000000  4.500000  3.500000  4.000000
> 5.000000 
>        23        25        30        31        35        39        40
> 44 
>  6.000000  3.000000  9.000000 10.000000  8.000000 10.000000  4.000000
> 6.000000 
>        45        47        50 
>  4.500000  5.500000  5.000000 
> 
> What OS / R version / MASS version (you are using MASS, no?) 
> are you using?
> What's your hardware?  Do you have lots of other stuff in the 
> workspace when
> you tried to run this?  What does gc() tell you?
> 
> Andy
> 
> > From: Ed Hagen
> > 
> > Dear all,
> > 
> > When calling rlm with the following data, I get an error.  (R 
> > v.1.8.1, 
> > WinXP Pro 2002 with service pack 1.)
> > 
> > > d <- na.omit(data.frame(CPRATIO, HEIGHTZ, FAMILYID))
> > > c <- tapply(d$CPRATIO, d$FAMILYID, mean)
> > > h <- tapply(d$HEIGHTZ, d$FAMILYID, mean)
> > > c
> >          1         2         3         6         7         
> 9        10
> >        11
> >   6.000000  2.500000  3.250000  4.000000  2.333333  
> 2.400000  2.750000
> > 2.000000
> >         12        13        14        15        16        
> 18        21
> >        22
> >   4.000000  5.000000  4.000000  7.000000  4.500000  
> 3.500000  4.000000
> > 5.000000
> >         23        25        30        31        35        
> 39        40
> >        44
> >   6.000000  3.000000  9.000000 10.000000  8.000000 
> 10.000000  4.000000
> > 6.000000
> >         45        47        50
> >   4.500000  5.500000  5.000000
> > > h
> >           1          2          3          6          7          9
> >     10
> > -2.7916667 -1.0246838 -2.0681170 -1.3256766 -3.1122708 -2.1444948
> > -2.5806538
> >          11         12         13         14         15         16
> >     18
> > -0.6791551 -0.1278583 -3.7737900 -3.3077561 -2.4615233 -3.4016225
> > -2.2484673
> >          21         22         23         25         30         31
> >     35
> > -0.8938528 -2.2521325 -2.3461999 -2.1405470 -2.7657994 -3.3588822
> > -2.8144453
> >          39         40         44         45         47         50
> > -3.8521439 -2.1882352 -1.7531826 -3.6464482 -2.2158240 -2.7162693
> > > out <- rlm(h ~ c)
> > Error in model.matrix.default(mt, mf, contrasts) :
> >          cannot allocate vector of length 1077237505
> > 
> > 
> > If I make the same rlm call a second or third time, R 
> usually crashes.
> > 
> > Any help or suggestions would be greatly appreciated.  
> These commands
> > were run after a much larger script had seemingly run 
> > successfully. But,
> > I had no problem calling, e.g., rlm(HEIGHTZ ~ CPRATIO), 
> after the same
> > script had run.  Although I have made some minor changes in 
> > the script, 
> > I also didn't have this problem when using R 1.7.
> > 
> > Many thanks in advance,
> > 
> > Ed.
> > 
> > 
> > -- 
> > Edward H. Hagen                  Institute for Theoretical Biology
> > phone: +49/30 2093-8649             Humboldt-Universit??t zu Berlin
> > fax:   +49/30 2093-8801                         Invalidenstra??e 43
> > http://itb.biologie.hu-berlin.de/~hagen      10115 Berlin, Germany
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From e.hagen at biologie.hu-berlin.de  Wed Apr  7 19:50:47 2004
From: e.hagen at biologie.hu-berlin.de (Ed Hagen)
Date: Wed, 07 Apr 2004 19:50:47 +0200
Subject: [R] Problems with rlm
In-Reply-To: <200404071713.i37HD7925005@gator.dt.uh.edu>
References: <200404071713.i37HD7925005@gator.dt.uh.edu>
Message-ID: <40743F77.4070808@biologie.hu-berlin.de>


Erin Hodgess wrote:

> Please don't use c as a vector name.  There is a function c and
> that function gets overwritten.

Thanks for that tip.  I was actually using different variable names in 
my script, but simplified them for the posting.  I just tried it using 
the names 'cpavg' and 'havg'.   The first time I called rlm it worked, 
but I got the following errors the second and third time I ran it:

 > out <- rlm(havg ~ cpavg)
Error: cannot allocate vector of size 4178828 Kb
 > out <- rlm(havg ~ cpavg)
Error in model.matrix.default(mt, mf, contrasts) :
         negative length vectors are not allowed

Liaw, Andy wrote:

 > You have not given us nearly enough info.
 > What OS / R version / MASS version (you are using MASS, no?) are you
 > using?

WinXP Pro 2002. Service Pack 1. 512K RAM.  R v. 1.8.1. MASS v. 7.1-11. 
AMD Athlon XP 2600+, 1.91. GHz.

 > Do you have lots of other stuff in the workspace when
 > you tried to run this?

Yes.  A lot of stuff is needed to produce the variables CPRATIO and HEIGHTZ

 > What does gc() tell you?

After running the script, but before running the code in question:

          used (Mb) gc trigger (Mb)
Ncells 720521 19.3    1073225 28.7
Vcells 209563  1.6     786432  6.0

gc() called during and after running the code in question:

 > d2 <- na.omit(data.frame(CPRATIO, HEIGHTZ, FAMILYID))
 > cavg <- tapply(d2$CPRATIO, d2$FAMILYID, mean)
 > havg <- tapply(d2$HEIGHTZ, d2$FAMILYID, mean)
 > gc()
          used (Mb) gc trigger (Mb)
Ncells 720802 19.3    1166886 31.2
Vcells 210286  1.7     786432  6.0
 > out <- rlm(havg ~ cavg)
Error in model.matrix.default(mt, mf, contrasts) :
         cannot allocate vector of length 2146959361
 > gc()
          used (Mb) gc trigger (Mb)
Ncells 720819 19.3    1166886 31.2
Vcells 210334  1.7     786432  6.0

As I noted in my first posting, rlm works fine with CPRATIO and HEIGHTZ, 
which are much larger datasets, if that's any clue (though even CPRATIO 
and HEIGHTZ are still quite small: 85 cases each).  After running the 
main script, I ran

 > out <- rlm(HEIGHTZ ~ CPRATIO)

about 20 consecutive times with no problems.  Yet running rlm with the 
smaller havg and cavg immediately produces errors, and R will usually 
crash after the second or third attempt.

Many thanks for the quick responses,

Ed.

-- 
Edward H. Hagen                  Institute for Theoretical Biology
phone: +49/30 2093-8649             Humboldt-Universit??t zu Berlin
fax:   +49/30 2093-8801                         Invalidenstra??e 43
http://itb.biologie.hu-berlin.de/~hagen      10115 Berlin, Germany



From carolr at fnr.purdue.edu  Wed Apr  7 20:23:07 2004
From: carolr at fnr.purdue.edu (Rizkalla, Carol)
Date: Wed, 7 Apr 2004 13:23:07 -0500
Subject: [R] ZIB models
Message-ID: <AEC7E2C3801FD411B5FE009027C24C2B023BDB47@forest3.fnr.purdue.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040407/90b90d2a/attachment.pl

From jfox at mcmaster.ca  Wed Apr  7 20:32:23 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 7 Apr 2004 14:32:23 -0400
Subject: [R] Problems with rlm
In-Reply-To: <40743F77.4070808@biologie.hu-berlin.de>
Message-ID: <20040407183223.JPWS6153.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Ed and Erin,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ed Hagen
> Sent: Wednesday, April 07, 2004 12:51 PM
> To: Erin Hodgess; r-help at stat.math.ethz.ch; andy_liaw at merck.com
> Subject: Re: [R] Problems with rlm
> 
> 
> Erin Hodgess wrote:
> 
> > Please don't use c as a vector name.  There is a function c 
> and that 
> > function gets overwritten.
> 

Actually, that's not right. A variable named c in the workspace won't shadow
the function c() in the base package (and certainly won't overwrite it):

> c <- 1
> c(c, 2)
[1] 1 2

> Thanks for that tip.  I was actually using different variable 
> names in my script, but simplified them for the posting.  

. . .

Regards,
 John



From bxc at steno.dk  Wed Apr  7 20:44:36 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 7 Apr 2004 20:44:36 +0200
Subject: [R] predict( ..., type="terms")
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E549@exdkba022.novo.dk>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Lumley
> Sent: Wednesday, April 07, 2004 5:44 PM
> To: Giovanni Petris

> On Wed, 7 Apr 2004, Giovanni Petris wrote:
> 
> >
> > Hello,
> >
> > After reading the help for predict.lm and predict.glm, it 
> is not clear 
> > to me what are the values returned by predict( ..., type="terms").
> >
> > Anybody willing to enlighten me?
> >
> 
> For each term in the formula, extract its coefficients and 
> the corresponding columns of the design matrix, and multiply 
> the two, and then center the result.
> 
> So, looking at the example in example(termplot) we have a 
> model y~ns(x,6)+z where z is a factor with 4 levels.
> 
> There are 9 coefficients in the model, but only two terms: 
> ns(x,6) (based on 5 coefficients) and z (based on 3 coefficients).
> 
> Adding up the terms and the centering value (also returned by
> predict(,type="terms")) gives the linear predictor.

1) It would be useful to have this in the help file for predict.(g)lm
   Also that the constant is accessed as attr( pr.obj, "const" )

2) A link to termplot might be useful too.

3) I guess that Giovanni (as well as I) was curious what the rationale
behind
   the centering of effects is. Can anyone tell us?

Bendix Carstensen

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From sundar.dorai-raj at PDF.COM  Wed Apr  7 21:00:11 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 07 Apr 2004 14:00:11 -0500
Subject: [R] ZIB models
In-Reply-To: <AEC7E2C3801FD411B5FE009027C24C2B023BDB47@forest3.fnr.purdue.edu>
References: <AEC7E2C3801FD411B5FE009027C24C2B023BDB47@forest3.fnr.purdue.edu>
Message-ID: <40744FBB.9030006@pdf.com>



Rizkalla, Carol wrote:

 > I attempted to contact Drew Tyre, but the email I have for him is no
 > longer in service.
 >
 > Hopefully someone can help.
 >
 >
 >
 > I'm using obs.error in R to model turtle occupancy in wetlands.
 >

What is "obs.error in R"? I get the following:

 > help.search("obs.error")
No help files found with alias or concept or title matching
'obs.error' using regular expression matching.

 > I have 4 species and 20 possible patch and landscape variables, which
 > I've been testing in smaller groups.
 >
 >
 >
 >
 >>zib.out<-obs.error(y=painted,m=numvis,bp=zvars,pcovar=7)
 >
 >
 >
 >
 > I get the following error message, with all species, all variable
 > groups, standardized and unstandardized data...
 >
 >
 >
 > Error in optim(par = rnorm(pcovar + qcovar), fn = obs.error.LL, method =
 > "L-BFGS-B",  :
 >
 >         L-BFGS-B needs finite values of fn
 >
 >
 >
 > I have a large sample size, so we're not sure that this is related to a
 > convergence problem, but we can't figure out what this error means.
 >
 >

Hi Carol,

(Please see the posting guide)

As the message says, your fn ("obs.error.LL") returned a non-finite 
value which is not allowed. Without seeing your code, it will be 
impossible to say where this happened. Either turn the trace on or step 
through your function to see where infinite values may creep in.

BTW, if you are fitting a ZIB model (zero-inflated binomial) you may be 
interested to know that Jim Lindsey's gnlm package does this through his 
fmr function (I think; I haven't actually done this kind of modelling in 
a while). The package can be downloaded from

http://alpha.luc.ac.be/~jlindsey/rcode.html

Be sure to get the rmutils package while you're there.

--sundar



From sdhyok at email.unc.edu  Wed Apr  7 21:07:49 2004
From: sdhyok at email.unc.edu (Shin)
Date: 07 Apr 2004 15:07:49 -0400
Subject: [R] More user-friendly error message needed.
Message-ID: <1081364866.2239.4.camel@dhcp9931.dhcp.unc.edu>

When I tried the following commands, I got a strange message.


> x<-data.frame(y=c(1:10))
> plot(x$z)
Error in xy.coords(x, y, xlabel, ylabel, log) :
        x and y lengths differ

"The data frame, x, does not have a field named z."
may be better user-friendly message for this kind of common error.


Daehyok Shin



From tplate at blackmesacapital.com  Wed Apr  7 21:35:31 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 07 Apr 2004 13:35:31 -0600
Subject: [R] More user-friendly error message needed.
In-Reply-To: <1081364866.2239.4.camel@dhcp9931.dhcp.unc.edu>
References: <1081364866.2239.4.camel@dhcp9931.dhcp.unc.edu>
Message-ID: <6.1.0.6.2.20040407131705.03be6fd8@mailhost.blackmesacapital.com>

It's actually not that easy to provide the meaningful kind of error message 
you are asking for.  (Though it would be nice.)
Consider what happens in the case you give:

(1) The argument of plot() of evaluated, and has the value NULL
(2) plot() is called with a single argument, which is NULL

All the information available inside plot() is that its argument has the 
value NULL (without getting into meta programming).  However, it would be 
possible for plot(), or xy.coords(), to print a more meaningful error 
message like "called with NULL value for x".  This disadvantage of doing 
this is that if you do it everywhere, a considerable volume of the code 
becomes tests and error messages, which can make the code less readable and 
more prone to errors (because of the possibility that some of the tests are 
buggy themselves).  The weak type checking in the S language exacerbates 
these problems.  As it currently is, many functions are simple and concise, 
but provide very cryptic error messages when something goes wrong.  This is 
the tradeoff.

One might argue that x$z should cause an error, but it is a 
long-established behavior that the "$" operator returns NULL when the list 
element does not exist or is not uniquely identified (this might even be in 
the written definition of the S language).

It might be an interesting CS grad student project to build a system that 
matches cryptic error messages and stack traces against a library of common 
errors in order to provide less cryptic error messages.

-- Tony Plate

At Wednesday 01:07 PM 4/7/2004, Shin wrote:
>When I tried the following commands, I got a strange message.
>
>
> > x<-data.frame(y=c(1:10))
> > plot(x$z)
>Error in xy.coords(x, y, xlabel, ylabel, log) :
>         x and y lengths differ
>
>"The data frame, x, does not have a field named z."
>may be better user-friendly message for this kind of common error.
>
>
>Daehyok Shin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdhyok at email.unc.edu  Wed Apr  7 22:09:47 2004
From: sdhyok at email.unc.edu (Shin)
Date: 07 Apr 2004 16:09:47 -0400
Subject: [R] More user-friendly error message needed.
In-Reply-To: <6.1.0.6.2.20040407131705.03be6fd8@mailhost.blackmesacapital.com>
References: <1081364866.2239.4.camel@dhcp9931.dhcp.unc.edu>
	<6.1.0.6.2.20040407131705.03be6fd8@mailhost.blackmesacapital.com>
Message-ID: <1081368585.2239.22.camel@dhcp9931.dhcp.unc.edu>

Thanks for your kind explanation.
Actually, I just begin to learn R, so not familiar with many behaviors
of R yet. As a user of Python and MATLAB, I understand the problem of
cryptic error messages of loosely-typed languages.

But,if some types of mistakes are often made by users in R interactive
shell, I believe we should try to give more informative messages at
least for the mistakes.
I think the error I reported is one of the mistakes.

Unfortunately, I don't have ability to create a patch for it.
Hopefully, somebody to fix it soon for better R.

Daehyok Shin


On Wed, 2004-04-07 at 15:35, Tony Plate wrote:
> It's actually not that easy to provide the meaningful kind of error message 
> you are asking for.  (Though it would be nice.)
> Consider what happens in the case you give:
> 
> (1) The argument of plot() of evaluated, and has the value NULL
> (2) plot() is called with a single argument, which is NULL
> 
> All the information available inside plot() is that its argument has the 
> value NULL (without getting into meta programming).  However, it would be 
> possible for plot(), or xy.coords(), to print a more meaningful error 
> message like "called with NULL value for x".  This disadvantage of doing 
> this is that if you do it everywhere, a considerable volume of the code 
> becomes tests and error messages, which can make the code less readable and 
> more prone to errors (because of the possibility that some of the tests are 
> buggy themselves).  The weak type checking in the S language exacerbates 
> these problems.  As it currently is, many functions are simple and concise, 
> but provide very cryptic error messages when something goes wrong.  This is 
> the tradeoff.
> 
> One might argue that x$z should cause an error, but it is a 
> long-established behavior that the "$" operator returns NULL when the list 
> element does not exist or is not uniquely identified (this might even be in 
> the written definition of the S language).
> 
> It might be an interesting CS grad student project to build a system that 
> matches cryptic error messages and stack traces against a library of common 
> errors in order to provide less cryptic error messages.
> 
> -- Tony Plate
> 
> At Wednesday 01:07 PM 4/7/2004, Shin wrote:
> >When I tried the following commands, I got a strange message.
> >
> >
> > > x<-data.frame(y=c(1:10))
> > > plot(x$z)
> >Error in xy.coords(x, y, xlabel, ylabel, log) :
> >         x and y lengths differ
> >
> >"The data frame, x, does not have a field named z."
> >may be better user-friendly message for this kind of common error.
> >
> >
> >Daehyok Shin
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From arcane at arcanemethods.com  Wed Apr  7 22:32:00 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 07 Apr 2004 13:32:00 -0700
Subject: [R] More user-friendly error message needed.
In-Reply-To: <1081368585.2239.22.camel@dhcp9931.dhcp.unc.edu>
References: <1081364866.2239.4.camel@dhcp9931.dhcp.unc.edu>	<6.1.0.6.2.20040407131705.03be6fd8@mailhost.blackmesacapital.com>
	<1081368585.2239.22.camel@dhcp9931.dhcp.unc.edu>
Message-ID: <40746540.9010804@arcanemethods.com>



Shin wrote:

> Thanks for your kind explanation.
> Actually, I just begin to learn R, so not familiar with many behaviors
> of R yet. As a user of Python and MATLAB, I understand the problem of
> cryptic error messages of loosely-typed languages.

Besides that, what do you think of R so far?


Bob
-- 

"Things should be described as simply as possible, but no 
simpler."

                                              A. Einstein



From arcane at arcanemethods.com  Wed Apr  7 22:33:17 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Wed, 07 Apr 2004 13:33:17 -0700
Subject: [R] Re: [R-gui] Editor in R
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD972B@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD972B@DJFPOST01.djf.agrsci.dk>
Message-ID: <4074658D.5040104@arcanemethods.com>



S??ren H??jsgaard wrote:

> Dear all,
> I have used R (Windows) for teaching statistics to agronomists and biologists - and they love R!! Yet there is one immediate short coming of R: The lack of an integrated editor. A plain editor (e.g. similar to notepad in windows) in which people can write their code, highlight the piece of code they wish to "submit" and then press a single key to have it pasted into R. 
> 
> For the audience I have in mind, using emacs is way too complicated. WinEdt with the R-extension is probably also too difficult for them (though I have not tried), and at it certainly would move the focus away from statistics to get these technical things "up-and-running". 
> 
> If I knew how to make such a simple editor within R (as an add-on package) then I would do it - but this goes way beyond my capabilities. I would therefore like to encourage anyone in the GUI group with such technical skills to make such an editor. It would be appreciated by many R-beginners...
> 

I'm on a PC and have VIM, a VI clone that I love.  Is it
possible and relatively easy to replace the Emacs editor
with one like that which you already have?


Thanks,

Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From dmurdoch at pair.com  Wed Apr  7 22:46:33 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 07 Apr 2004 16:46:33 -0400
Subject: [R] Re: [R-gui] Editor in R
In-Reply-To: <4074658D.5040104@arcanemethods.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD972B@DJFPOST01.djf.agrsci.dk>
	<4074658D.5040104@arcanemethods.com>
Message-ID: <stp870l026d019vivpub5jf257mp0l9pr9@4ax.com>

On Wed, 07 Apr 2004 13:33:17 -0700, Bob Cain
<arcane at arcanemethods.com> wrote :

>
>S??ren H??jsgaard wrote:
>
>> Dear all,
>> I have used R (Windows) for teaching statistics to agronomists and biologists - and they love R!! Yet there is one immediate short coming of R: The lack of an integrated editor. A plain editor (e.g. similar to notepad in windows) in which people can write their code, highlight the piece of code they wish to "submit" and then press a single key to have it pasted into R. 
>> 
>> For the audience I have in mind, using emacs is way too complicated. WinEdt with the R-extension is probably also too difficult for them (though I have not tried), and at it certainly would move the focus away from statistics to get these technical things "up-and-running". 
>> 
>> If I knew how to make such a simple editor within R (as an add-on package) then I would do it - but this goes way beyond my capabilities. I would therefore like to encourage anyone in the GUI group with such technical skills to make such an editor. It would be appreciated by many R-beginners...
>> 
>
>I'm on a PC and have VIM, a VI clone that I love.  Is it
>possible and relatively easy to replace the Emacs editor
>with one like that which you already have?

Does VIM have a way to run a program that uses stdin, stdout and
stderr, capturing the input and output?  That's how ESS does it in
Emacs.  If so, then it could run Rterm that way, and handle the
console display itself.  You might want to run using the --ess command
line option (but you'll have to check the source code to see exactly
what it does, I forget).

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Wed Apr  7 22:44:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Apr 2004 22:44:02 +0200
Subject: [R] Re: [R-gui] Editor in R
In-Reply-To: <4074658D.5040104@arcanemethods.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD972B@DJFPOST01.djf.agrsci.dk>
	<4074658D.5040104@arcanemethods.com>
Message-ID: <x2isgbecx9.fsf@biostat.ku.dk>

Bob Cain <arcane at arcanemethods.com> writes:

> I'm on a PC and have VIM, a VI clone that I love.  Is it
> possible and relatively easy to replace the Emacs editor
> with one like that which you already have?

Depends on what for. There are R commands that involve calling up an
editor on a file and getting a modified file back (fix(myfun) e.g.).
That editor is completely customizable by options(editor=....). On
Unix-alikes the default is your $EDITOR setting and on Windows I
believe it is Notepad.

However, it is possible to wire up Emacs so that R runs in one of its
text buffers and you can can easily send blocks of code to that
process. This can be difficult to clone with other editors, but AFAIR
WinEdt can do something similar. Wouldn't know about Vim.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From zelickr at pdx.edu  Wed Apr  7 23:15:13 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Wed, 7 Apr 2004 14:15:13 -0700 (PDT)
Subject: [R] Re: [R-gui] Editor in R
In-Reply-To: <x2isgbecx9.fsf@biostat.ku.dk>
Message-ID: <Pine.GSO.4.44.0404071405350.9414-100000@freke.odin.pdx.edu>

Hello there,

I have been using WinEdt as a text editor for a while. I use it in a
simple general purpose way, and take advantage of just a few of the 100's
(1000s?) of features. I would say that used this way it is certainly not
more difficult than R. So it would seem that if you are successful
teaching R to biologists, additionally learning WinEdt should not push
anyone over the edge.

Still, another alternative is Pico, which you can get for free for
Windoze.

By the way, although Notepad works for R scripts, when I tried Wordpad it
did not. Probably puts whacko crapola at the end of each line.

Cheers,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From jasont at indigoindustrial.co.nz  Thu Apr  8 00:30:23 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 08 Apr 2004 10:30:23 +1200
Subject: [R] a question about levelplot
In-Reply-To: <001501c41caa$8300eaf0$ca54a8c0@Kanna>
References: <001501c41caa$8300eaf0$ca54a8c0@Kanna>
Message-ID: <407480FF.7070102@indigoindustrial.co.nz>

See the FAQ

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why%20do%20lattice%2ftrellis%20graphics%20not%20work%3f

????? wrote:

> Hello everyone
> 
> I met a strange problem when I call "levelplot" in another function
> 
> 
>>library(lattice)
>>mat = matrix(1:9, nrow=3)
>>mylevelplot = function() {
> 
> +    png('test.png')
> +    levelplot(mat)
> +    dev.off()
> +}
> 
>>mylevelplot()
> 
> null device
>                1



From ggrothendieck at myway.com  Thu Apr  8 01:26:49 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 7 Apr 2004 23:26:49 +0000 (UTC)
Subject: [R] merge more than two tables
References: <200404061249.22618.ozric@web.de>
Message-ID: <loom.20040408T011258-30@post.gmane.org>


The version 0.1-2 of the zoo package, which should be on CRAN shortly,
supports multiway merges of zoo objects.   

Christian Schulz <ozric <at> web.de> writes:

: 
: Hi 
: 
: have anybody a hint how i could merge  i.e.  10 data.frames togehter.
: Ok i could do one merge after another but perhaps with few coding exist a 
: better way?
: 
: system.time(dm1 <- merge(Base,sub1 ,by.x 
="MEMBERNO",by.y="MEMBERNO",all.x=T))
: system.time(dm2 <- merge(dm2,sub2,by.x="MEMBERNO",by.y="MEMBERNO",all.x=T))
: system.time(dm3 <- merge(dm2,sub3,by.x="MEMBERNO",by.y="MEMBERNO",all.x=T))
: ....................
: 
: many thanks , christian



From ecashin at uga.edu  Thu Apr  8 02:14:12 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 07 Apr 2004 20:14:12 -0400
Subject: [R] getting data frame rows out of a by object
Message-ID: <87isgbba23.fsf@uga.edu>

Hi.  I can quickly create a by object that selects rows from a data
frame.  After that, though, I don't know how to merge the rows back
into a data frame that I can use.

Here is an example where there is a data frame with three columns, a,
b, and c.  I update it so that there are two rows for each combination
of a and b.  I use by to select the subgroups of rows that share the
same a and b values, and then I take only the row with the highest c
value.

I can see little data frames inside the by object, but I can't get a
new data frame containing only the rows with the highest c value.  In
the example below most of the by object is NULL, but it contains data
frames with the rows I'm interested in selecting.

  > d <- data.frame(a=1:4,b=4:1,c=31:34)
  > d
    a b  c
  1 1 4 31
  2 2 3 32
  3 3 2 33
  4 4 1 34
  > b <- by(d, list(d$a,d$b,d$c), function(x) x)
  > d <- data.frame(a=1:4,b=4:1,c=31:34)
  > d <- rbind(d, data.frame(a=1:4,b=4:1,c=41:44))
  > b <- by(d, list(d$a,d$b,d$c), function(x) x[x$c == max(x$c),])
  > b
  : 1
  : 1
  : 31
  NULL
  ------------------------------------------------------------ 
  : 2
  : 1
  : 31
  NULL
  ------------------------------------------------------------ 
...  
  ------------------------------------------------------------ 
  : 3
  : 2
  : 43
     a b  c
  31 3 2 43
...  
  > merge(b)
  Error in as.data.frame.default(x) : can't coerce by into a data.frame
  > 


Any help is most appreciated.  


-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From julian.taylor at adelaide.edu.au  Thu Apr  8 03:02:44 2004
From: julian.taylor at adelaide.edu.au (Julian Taylor)
Date: Thu, 08 Apr 2004 10:32:44 +0930
Subject: [R] getting data frame rows out of a by object
References: <87isgbba23.fsf@uga.edu>
Message-ID: <4074A4B4.47E8D257@adelaide.edu.au>




Ed L Cashin wrote:
> 
> Hi.  I can quickly create a by object that selects rows from a data
> frame.  After that, though, I don't know how to merge the rows back
> into a data frame that I can use.
> 
> Here is an example where there is a data frame with three columns, a,
> b, and c.  I update it so that there are two rows for each combination
> of a and b.  I use by to select the subgroups of rows that share the
> same a and b values, and then I take only the row with the highest c
> value.
> 
> I can see little data frames inside the by object, but I can't get a
> new data frame containing only the rows with the highest c value.  In
> the example below most of the by object is NULL, but it contains data
> frames with the rows I'm interested in selecting.
> 
>   > d <- data.frame(a=1:4,b=4:1,c=31:34)
>   > d
>     a b  c
>   1 1 4 31
>   2 2 3 32
>   3 3 2 33
>   4 4 1 34
>   > b <- by(d, list(d$a,d$b,d$c), function(x) x)
>   > d <- data.frame(a=1:4,b=4:1,c=31:34)
>   > d <- rbind(d, data.frame(a=1:4,b=4:1,c=41:44))
>   > b <- by(d, list(d$a,d$b,d$c), function(x) x[x$c == max(x$c),])

You are better off using other tools to give you the right subsets. Try

d <- do.call("rbind", lapply(split(d, factor(paste(d$a, d$b, sep =
""))), 
                            function(el) el[el$c == max(el$c), ]))

HTH,
Jules

-- 
---
Julian Taylor			phone: +61 8 8303 6751
ARC Research Associate            fax: +61 8 8303 6760
BiometricsSA,                  mobile: +61 4 1638 8180  
University of Adelaide/SARDI    email: julian.taylor at adelaide.edu.au
Private Mail Bag 1                www:
http://www.BiometricsSA.adelaide.edu.au
Glen Osmond SA 5064

"There is no spoon."   -- Orphan boy  
---



From ecashin at uga.edu  Thu Apr  8 03:24:44 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Wed, 07 Apr 2004 21:24:44 -0400
Subject: [R] getting data frame rows out of a by object
References: <87isgbba23.fsf@uga.edu> <4074A4B4.47E8D257@adelaide.edu.au>
Message-ID: <877jwrb6sj.fsf@uga.edu>

Julian Taylor <julian.taylor at adelaide.edu.au> writes:

...
> You are better off using other tools to give you the right subsets. Try
>
> d <- do.call("rbind", lapply(split(d, factor(paste(d$a, d$b, sep =
> ""))), 
>                             function(el) el[el$c == max(el$c), ]))

That does work, thanks.  I am a bit mystified by the use of paste,
factor, and split together like that.  By concatenating the columns as
strings, you are coming up with values that aren't in any one column
of the data frame, but split doesn't care.  

That's surprising.  I thought this method was dangerous, so I tried to
fool split by adding a column-a value that was the same as the
concatenation of other column-a and column-b values, "41".  Split
knows to work with both column a and column b, though, somehow.

I guess paste does more than I thought.

  > d <- rbind(d, data.frame(a=41,b=0,c=0))
  > split(d, factor(paste(d$a, d$b, sep = "")))
  $"14"
     a b  c
  1  1 4 31
  11 1 4 41
  
  $"23"
     a b  c
  2  2 3 32
  21 2 3 42
  
  $"32"
     a b  c
  3  3 2 33
  31 3 2 43
  
  $"41"
     a b  c
  4  4 1 34
  41 4 1 44
  
  $"410"
      a b c
  12 41 0 0
  
  > 

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From ckjmaner at carolina.rr.com  Thu Apr  8 04:24:58 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Wed, 7 Apr 2004 19:24:58 -0700 (PDT)
Subject: [R] RODBC on FreeBSD 4.9
Message-ID: <20040408022458.11459.qmail@web40308.mail.yahoo.com>


Hello.  I have found a number of posts/threads on this
subject and remain unsuccessful to determine the
solution.  In brief, my situation is:

1.  Running FreeBSD 4.9 Stable in x86
2.  Installed, and running, R 1.8.1
3.  Installed, and running, postgreSQL 7.4.2
4.  Installed, and running, unixODBC 2.2.8
5.  Failed, repeatedly, to install RODBC.  The
following is my error log:

tsunami# R CMD INSTALL RODBC
* Installing *source* package 'RODBC' ...
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler...
yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none
needed
checking for library containing SQLTables... no
configure: error: "no ODBC driver manager found"
ERROR: configuration failed for package 'RODBC'
** Removing '/usr/local/lib/R/library/RODBC'

I have set up the DSN/databases using ODBCConfig,
viewed/confirmed through DataManager and have even
used isql to further confirm that all is set up
correctly.  I have read, numerous times, the README
within RODBC.  Unfortunately, I am out of ideas. 
Therefore, I am asking for ideas of others.

Also, I found, and employed, the results from a
previous post,
https://stat.ethz.ch/pipermail/r-help/2001-October/014693.html,
and reset my LIBS and LD_LIBRARY_PATH to those
suggested by the author.  Again, no joy.

So, any thoughts, with successful ones perhaps
resulting in an FAQ/how-to, would be most appreciated.

(Also, please let me know what more info you may need
to help and I will gladly provide it.)


Charles



From ok at cs.otago.ac.nz  Thu Apr  8 06:48:05 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 8 Apr 2004 16:48:05 +1200 (NZST)
Subject: [R] R on MacOS X
Message-ID: <200404080448.i384m5pY126654@atlas.otago.ac.nz>

I just got a G4 running MacOS 10.3 today, so I immediately downloaded
R from CRAN.  To my disappointment, it insists on installing in a fixed
place and requires a root password to do so.  University policy is that
if your machine is on the net, you WON'T get root access nohow, unless
you're a designated sysadmin, which I'm not.  Is there any alternative
to downloading a source distribution and doing the classic
./configure;make;make install waltz?  Will there be?



From mai at ms.uky.edu  Thu Apr  8 07:32:48 2004
From: mai at ms.uky.edu (Mai Zhou)
Date: Thu, 8 Apr 2004 01:32:48 -0400 (EDT)
Subject: [R] R/Splus code for PRESS?
Message-ID: <200404080532.i385WpLw020224@t2.ms.uky.edu>

Dear R-help, 
Does anybody know where can I find R/Splus code for computing
PREdiction Sum of Squares (PRESS)
in a linear regression model?

I have a large regression model (~100 predictors, ~30,000 cases)
and good prediction is the main goal.

I remember there is a faster way to compute it (no need to
repeatedly fit models), but if there is existing code......

Thanks.

Mai Z



From pedro.aphalo at cc.jyu.fi  Thu Apr  8 08:52:24 2004
From: pedro.aphalo at cc.jyu.fi (Pedro J. Aphalo)
Date: Thu, 08 Apr 2004 09:52:24 +0300
Subject: [R] NLME augPred Error
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF04FE5577@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
References: <F0E0B899CB43D5118D220002A55113CF04FE5577@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
Message-ID: <4074F6A8.2060700@cc.jyu.fi>

Dear Richard,

The problem that you report is documented (but no solution given) in the 
file ch08.R in the scripts directory of nlme package.

I have found the following workaround just by chance, but it may
give a clue of what is the problem to those who know how to program
in R.

The solution is to add an explicit call to factor in the nlme call.
In the case of the error reported by Richard the following call to nlme 
in the ch08.R file can be used:

fm4CO2.nlme <- update( fm3CO2.nlme,
   fixed = list(Asym + lrc ~ factor(Type) * factor(Treatment), c0 ~ 1),
   start = c(fm3CO2.fix[1:5], 0, 0, 0, fm3CO2.fix[6]) )

instead of:

fm4CO2.nlme <- update( fm3CO2.nlme,
   fixed = list(Asym + lrc ~ Type * Treatment, c0 ~ 1),
   start = c(fm3CO2.fix[1:5], 0, 0, 0, fm3CO2.fix[6]) )

I hope this helps,

Pedro.

Yang, Richard wrote:
> Dear all;
> 
> 	This error has been posted to the list a couple of times, but no
> answer / walk around solution is available. An example on P. 377 MEMSS
> 
> 
>>plot(augPred(fm5CO2.nlme, levels=0:1), layout =c(6,2))
> 
> 
> generates an error: "Error in predict.nlme(object, value[1:(nrow(value)/nL),
> , drop = FALSE], : Levels Quebec, Mississippi not allowed for Type.
> 
> for nlme ver. 3.1-45 in R.1.8.1, but it works fine in Splus6.2 or earlier
> versions. 
> 
> The source code for augPred() is very cryptica and identical in both S+ and
> R:
> 
> function (object, primary = NULL, minimum = min(primary), maximum =
> max(primary), 
>     length.out = 51, ...) 
> UseMethod("augPred").
> 
> The augPred() creates an augPred class object in S+ but generates the above
> error in R. Is there any difference in nlme3 in S+ and nlme versions in R?
> 
> 	Any ideas?
> 
> Richard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
==================================================================
Pedro J. Aphalo
Department of Biological and Environmental Science
P.O. Box 35
FIN-40014 University of Jyv??skyl??
Finland
Phone  +358 14 260 2339
Mobile +358 50 3721504
Fax    +358 14 260 2321
mailto:pedro.aphalo at cc.jyu.fi
http://www.jyu.fi/~aphalo/                       ,,,^..^,,,



From e.hagen at biologie.hu-berlin.de  Thu Apr  8 10:57:50 2004
From: e.hagen at biologie.hu-berlin.de (Ed Hagen)
Date: Thu, 08 Apr 2004 10:57:50 +0200
Subject: [R] Problems with rlm
In-Reply-To: <38C4C095FC35E5469BED686B42F40A1307803076@usrymx17.merck.com>
References: <38C4C095FC35E5469BED686B42F40A1307803076@usrymx17.merck.com>
Message-ID: <4075140E.7060601@biologie.hu-berlin.de>


> Given what you have tried, I wonder whether the problem is 
> that 'cavg' and 'havg', produced by tapply, are not the 
> simple vectors that 'rlm' expects.  A simple way to check:
> 
> cavg <- as.vector(cavg)
> havg <- as.vector(havg)
> 
> then run rlm.

That worked! Many thanks.  Has there been a change in either MASS or R 
in this regard since R 1.7?  It was only after updating to R 1.8.1 that 
I had this problem.

Thanks again to all for your rapid replies,

Ed.

-- 
Edward H. Hagen                  Institute for Theoretical Biology
phone: +49/30 2093-8649             Humboldt-Universit??t zu Berlin
fax:   +49/30 2093-8801                         Invalidenstra??e 43
http://itb.biologie.hu-berlin.de/~hagen      10115 Berlin, Germany



From rory at campbell-lange.net  Thu Apr  8 12:15:04 2004
From: rory at campbell-lange.net (Rory Campbell-Lange)
Date: Thu, 8 Apr 2004 11:15:04 +0100
Subject: [R] Re: [R-gui] Editor in R
In-Reply-To: <stp870l026d019vivpub5jf257mp0l9pr9@4ax.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD972B@DJFPOST01.djf.agrsci.dk>
	<4074658D.5040104@arcanemethods.com>
	<stp870l026d019vivpub5jf257mp0l9pr9@4ax.com>
Message-ID: <20040408101503.GB5005@campbell-lange.net>

On 07/04/04, Duncan Murdoch (dmurdoch at pair.com) wrote:
> >S?ren H?jsgaard wrote:

> >I'm on a PC and have VIM, a VI clone that I love.  Is it possible and
> >relatively easy to replace the Emacs editor with one like that which
> >you already have?
> 
> Does VIM have a way to run a program that uses stdin, stdout and
> stderr, capturing the input and output?

On my linux system, with my environmental variables pointing to vim, the
command

    edit()

puts me into a vim buffer.

You may want to discover if you can get readline support for R too, as
it is great being able to use vi keybindings to surf through the command
history and do quick and easy cutting and pasting.

eg: <ESC>2k        goes up two commands
    <ESC>/abs      find the first line with 'abs'
    n              find the next line with 'abs'
    W              move to the second word on a line
    d$             delete from second word to end into buffer
    p              paste

Rory
-- 
Rory Campbell-Lange 
<rory at campbell-lange.net>
<www.campbell-lange.net>



From highstat at highstat.com  Thu Apr  8 12:16:56 2004
From: highstat at highstat.com (Highland Statistics Ltd.)
Date: Thu, 8 Apr 2004 05:16:56 -0500
Subject: [R] Time Varying Coefficients
Message-ID: <200404081016.i38AGuI25805@atmapp02.siteprotect.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040408/cb5d7473/attachment.pl

From jps at srres.com  Thu Apr  8 13:37:51 2004
From: jps at srres.com (Jan P. Smit)
Date: Thu, 8 Apr 2004 18:37:51 +0700
Subject: [R] lines and glm
Message-ID: <000001c41d5d$f1050aa0$27b8b7ca@S3IK83SFJDJZ09>

Dear R-helpers,

I'm a beginner using R 1.8.1 on Windows 2000. I'm trying to replicate some
examples in Franses' & Paap's Quantitative Models in Marketing Research. 

> t <- 1:1000
> e1 <- rnorm(1000)
> e2 <- rnorm(1000)
> x <- 0.0001*t+e1
> y2 <- -2+x+e2
> y <- ifelse(y2>0,1,0)
> 
> plot(x, y, pch = 16, col = "darkblue",
+      main = expression(paste("Scatter diagram of ", italic(y[t]), "
against ",
+          italic(x[t]))),
+      xlab = expression(italic(x[t])),
+      ylab = expression(italic(y[t])))
> lines(glm(y ~ x, family = binomial))

Error in xy.coords(x, y) : x and y lengths differ

However, 

> length(x)
[1] 1000
> length(y)
[1] 1000

as it should be.

I'm sure I must be missing something obvious, but it is not clear to me
(after reading the Introduction, FAQ and the relevant functions help pages)
what.

Sincerely,

Jan Smit



From ligges at statistik.uni-dortmund.de  Thu Apr  8 13:48:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 08 Apr 2004 13:48:34 +0200
Subject: [R] lines and glm
In-Reply-To: <000001c41d5d$f1050aa0$27b8b7ca@S3IK83SFJDJZ09>
References: <000001c41d5d$f1050aa0$27b8b7ca@S3IK83SFJDJZ09>
Message-ID: <40753C12.4040403@statistik.uni-dortmund.de>

Jan P. Smit wrote:

> Dear R-helpers,
> 
> I'm a beginner using R 1.8.1 on Windows 2000. I'm trying to replicate some
> examples in Franses' & Paap's Quantitative Models in Marketing Research. 
> 
> 
>>t <- 1:1000
>>e1 <- rnorm(1000)
>>e2 <- rnorm(1000)
>>x <- 0.0001*t+e1
>>y2 <- -2+x+e2
>>y <- ifelse(y2>0,1,0)
>>
>>plot(x, y, pch = 16, col = "darkblue",
> 
> +      main = expression(paste("Scatter diagram of ", italic(y[t]), "
> against ",
> +          italic(x[t]))),
> +      xlab = expression(italic(x[t])),
> +      ylab = expression(italic(y[t])))
> 
>>lines(glm(y ~ x, family = binomial))
> 
> 
> Error in xy.coords(x, y) : x and y lengths differ
 >
> However, 
> 
> 
>>length(x)
> 
> [1] 1000
> 
>>length(y)
> 
> [1] 1000

You don't want plot y against x here, but something related to the glm 
object. I guess you are looking for
    abline(glm.object)

Uwe Ligges



> as it should be.
> 
> I'm sure I must be missing something obvious, but it is not clear to me
> (after reading the Introduction, FAQ and the relevant functions help pages)
> what.
> 
> Sincerely,
> 
> Jan Smit
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at entelnet.bo  Thu Apr  8 13:57:54 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Thu, 08 Apr 2004 07:57:54 -0400
Subject: [R] lines and glm
In-Reply-To: <000001c41d5d$f1050aa0$27b8b7ca@S3IK83SFJDJZ09>
Message-ID: <40750602.28121.211F01@localhost>

On 8 Apr 2004 at 18:37, Jan P. Smit wrote:

> Dear R-helpers,
> 
> I'm a beginner using R 1.8.1 on Windows 2000. I'm trying to replicate
> some examples in Franses' & Paap's Quantitative Models in Marketing
> Research. 
> 
> > t <- 1:1000
> > e1 <- rnorm(1000)
> > e2 <- rnorm(1000)
> > x <- 0.0001*t+e1
> > y2 <- -2+x+e2
> > y <- ifelse(y2>0,1,0)
> > 
> > plot(x, y, pch = 16, col = "darkblue",
> +      main = expression(paste("Scatter diagram of ", italic(y[t]), "
> against ",
> +          italic(x[t]))),
> +      xlab = expression(italic(x[t])),
> +      ylab = expression(italic(y[t])))
> > lines(glm(y ~ x, family = binomial))

I don't think lines know what to do with an glm object! Reolace the 
last line with something like:

 test <- glm(y ~ x, family = binomial)

 o <- order(x)
 lines(x[o], fitted(test)[o])

Kjetil Halvorsen


> 
> Error in xy.coords(x, y) : x and y lengths differ
> 
> However, 
> 
> > length(x)
> [1] 1000
> > length(y)
> [1] 1000
> 
> as it should be.
> 
> I'm sure I must be missing something obvious, but it is not clear to
> me (after reading the Introduction, FAQ and the relevant functions
> help pages) what.
> 
> Sincerely,
> 
> Jan Smit
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Stefano.Guazzetti at ausl.re.it  Thu Apr  8 14:00:23 2004
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Thu, 8 Apr 2004 14:00:23 +0200
Subject: R: [R] lines and glm
Message-ID: <F298786BF61DE64590054AF31EA1B4C8016D1F62@PEPI.ausl.org>

You probably mean something like:

 ti <- 1:1000
 e1 <- rnorm(1000)
 e2 <- rnorm(1000)
 x <- 0.0001*ti+e1
 y2 <- -2+x+e2
 y <- ifelse(y2>0,1,0)
 
 plot(x, y, pch = 16, col = "darkblue",
      main = expression(paste("Scatter diagram of ", 
italic(y[t]), "against ",          italic(x[t]))),
      xlab = expression(italic(x[t])),
      ylab = expression(italic(y[t])))

 model<-glm(y ~ x, family = binomial)
 predProbs<-predict(model,data.frame(x=seq(min(x), max(x), length.out=100)), type="response")
 lines(seq(min(x), max(x), length.out=100), predProbs, col=2, lwd=2)



Note also that it is not a good idea to name "t" a R object, since the name is reserved for a special function.


Stefano


> -----Messaggio originale-----
> Da: Jan P. Smit [mailto:jps at srres.com]
> Inviato: gioved?? 8 aprile 2004 13.38
> A: r-help at stat.math.ethz.ch
> Oggetto: [R] lines and glm
> 
> 
> Dear R-helpers,
> 
> I'm a beginner using R 1.8.1 on Windows 2000. I'm trying to 
> replicate some
> examples in Franses' & Paap's Quantitative Models in 
> Marketing Research. 
> 
> > t <- 1:1000
> > e1 <- rnorm(1000)
> > e2 <- rnorm(1000)
> > x <- 0.0001*t+e1
> > y2 <- -2+x+e2
> > y <- ifelse(y2>0,1,0)
> > 
> > plot(x, y, pch = 16, col = "darkblue",
> +      main = expression(paste("Scatter diagram of ", italic(y[t]), "
> against ",
> +          italic(x[t]))),
> +      xlab = expression(italic(x[t])),
> +      ylab = expression(italic(y[t])))
> > lines(glm(y ~ x, family = binomial))
> 
> Error in xy.coords(x, y) : x and y lengths differ
> 
> However, 
> 
> > length(x)
> [1] 1000
> > length(y)
> [1] 1000
> 
> as it should be.
> 
> I'm sure I must be missing something obvious, but it is not 
> clear to me
> (after reading the Introduction, FAQ and the relevant 
> functions help pages)
> what.
> 
> Sincerely,
> 
> Jan Smit
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From kent at darwin.eeb.uconn.edu  Thu Apr  8 14:03:56 2004
From: kent at darwin.eeb.uconn.edu (Kent Holsinger)
Date: Thu, 08 Apr 2004 08:03:56 -0400
Subject: [R] nlme on Windows 2000 (v1.8.1)
Message-ID: <40753FAC.8060805@darwin.eeb.uconn.edu>

I have a problem with nlme on Windows 2000, and I'm having a devil of a 
time determining whether the problem is with my computer or with 
something in R. I'm running v1.8.1 on a Dell Pentium III with 512MB of 
RAM and all of the recommended Windows 2000 updates applied.

If I use Rterm, I can run analyses with NLME to my heart's content. But 
when I run Rgui, I encounter a floating point exception. To be concrete if I

 > library(nlme)
Loading required package: lattice
 > data(Rail)
 > lme(travel ~ 1, data = Rail, random = ~ 1 | Rail)

(the first example in Pinheiro and Bates), R churns for a while and a 
window pops up informing me of an application error. Specifically, "The 
exception Floating-point division by zero. (0xc000008e) occurred in the 
application at location 0x639b50ff." There error occurs every time. The 
same analysis runs flawlessly in Rterm.

To make it even stranger, I get the same error on a Toshiba Pentium 
laptop (also with 512MB of memory, although I haven't tried the analyses 
in Rterm on that machine). On that machine, I get a blue screen after 
acknowledging the error. The laptop is a dual boot on which I run Linux 
(Fedora Core 1). R works perfectly on it under Linux.

I've downloaded and re-installed the binaries on both machines several 
times, so I don't think I have a corrupted download. Any ideas on how to 
diagnose (and solve) this problem would be greatly appreciated.

Kent

-- 
Kent E. Holsinger                kent at darwin.eeb.uconn.edu
                                  http://darwin.eeb.uconn.edu
-- Department of Ecology & Evolutionary Biology
-- University of Connecticut, U-3043
-- Storrs, CT   06269-3043



From bates at stat.wisc.edu  Thu Apr  8 14:25:26 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 08 Apr 2004 07:25:26 -0500
Subject: [R] getting data frame rows out of a by object
In-Reply-To: <877jwrb6sj.fsf@uga.edu>
References: <87isgbba23.fsf@uga.edu> <4074A4B4.47E8D257@adelaide.edu.au>
	<877jwrb6sj.fsf@uga.edu>
Message-ID: <6rfzbelkqx.fsf@bates4.stat.wisc.edu>

Ed L Cashin <ecashin at uga.edu> writes:

> Julian Taylor <julian.taylor at adelaide.edu.au> writes:
> 
> ...
> > You are better off using other tools to give you the right subsets. Try
> >
> > d <- do.call("rbind", lapply(split(d, factor(paste(d$a, d$b, sep =
> > ""))), 
> >                             function(el) el[el$c == max(el$c), ]))

Just as a precaution I would use sep=":" or sep="/" or some other
character that is unlikely to occur in the levels of the factors a and
b.[1] to avoid possible ambiguities from using sep="".  If, for
example, both factors a and b had levels of "1" to "11" then using
sep="" you cannot tell if the pasted string "111" originated as "1"
pasted to "11" or "11" pasted to "1".

> That does work, thanks.  I am a bit mystified by the use of paste,
> factor, and split together like that.  By concatenating the columns as
> strings, you are coming up with values that aren't in any one column
> of the data frame, but split doesn't care.  

The point is that factor(paste(...)) returns a factor with length of
nrow(d) and with factor levels determined by the combination of levels
of a and b (provided that you don't get ambiguities as described
above).  The split function does not require that the factor
determining the splits be part of the data frame being split.  It can
be given explicitly as it is here.

[1] If you really want to be cautious you could use an octal
representation like sep="\007" to get a character that is very
unlikely to occur in a factor level.



From maechler at stat.math.ethz.ch  Thu Apr  8 14:36:17 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Apr 2004 14:36:17 +0200
Subject: [R] R on MacOS X
In-Reply-To: <200404080448.i384m5pY126654@atlas.otago.ac.nz>
References: <200404080448.i384m5pY126654@atlas.otago.ac.nz>
Message-ID: <16501.18241.640171.281030@gargle.gargle.HOWL>

>>>>> "Richard" == Richard A O'Keefe <ok at cs.otago.ac.nz>
>>>>>     on Thu, 8 Apr 2004 16:48:05 +1200 (NZST) writes:

    Richard> I just got a G4 running MacOS 10.3 today, so I
    Richard> immediately downloaded R from CRAN.  

good move; one of the first thing a new computer needs,
(just after emacs) ;-)

    Richard> To my disappointment, it insists on installing in a
    Richard> fixed place and requires a root password to do so.
    Richard> University policy is that if your machine is on the
    Richard> net, you WON'T get root access nohow, unless you're
    Richard> a designated sysadmin, which I'm not.  Is there any
    Richard> alternative to downloading a source distribution
    Richard> and doing the classic ./configure;make;make install
    Richard> waltz?

yes, there's always been.
You have installed the  pre-compiled aka binary version for Mac;
instead get the source -- and actually make sure you get the
source of "R 1.9.0 beta" since you'll be installing from the
source, and also because there are substantial Mac specific
enhancements there.
For you in NZ, you'd use the Oz-mirror, and look in the src/base
directory  http://mirror.aarnet.edu.au/pub/CRAN/src/base/
(note the above should only work from NZ and AU).

Regards,
Martin Maechler



From p.dalgaard at biostat.ku.dk  Thu Apr  8 14:48:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Apr 2004 14:48:51 +0200
Subject: [R] R on MacOS X
In-Reply-To: <16501.18241.640171.281030@gargle.gargle.HOWL>
References: <200404080448.i384m5pY126654@atlas.otago.ac.nz>
	<16501.18241.640171.281030@gargle.gargle.HOWL>
Message-ID: <x2fzbe3aa4.fsf@biostat.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

>     Richard> a designated sysadmin, which I'm not.  Is there any
>     Richard> alternative to downloading a source distribution
>     Richard> and doing the classic ./configure;make;make install
>     Richard> waltz?
> 
> yes, there's always been.
> You have installed the  pre-compiled aka binary version for Mac;
> instead get the source -- and actually make sure you get the
> source of "R 1.9.0 beta" since you'll be installing from the
> source, and also because there are substantial Mac specific
> enhancements there.

Wasn't that what he was trying to avoid? Excellent answer still... ;-)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From william.g.moore at usa-spaceops.com  Thu Apr  8 15:42:17 2004
From: william.g.moore at usa-spaceops.com (Moore, William G)
Date: Thu, 8 Apr 2004 09:42:17 -0400
Subject: [R] Expected Mean Squares
Message-ID: <03C52175148BC04C974AEA13F429A3F7056A952C@usaflcms03.usa-spaceops.ksc.nasa.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040408/21dd2601/attachment.pl

From pierre.clauss at sinopia.fr  Thu Apr  8 15:50:01 2004
From: pierre.clauss at sinopia.fr (Pierre Clauss)
Date: Thu, 08 Apr 2004 15:50:01 +0200
Subject: [R] plot help
Message-ID: <40755889.A6BE28D4@sinopia.fr>

Hie!
I do not manage to make several plots on a same graph ( for example two
densities on the same graph )
thanks for the answer
Pierre Clauss


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Ce message et toutes les pices jointes (ci - apres le  \"message\" ) 
sont tablis a l'attention exclusive de ses destinataires et sont 
strictement confidentiels. Si vous n'tes pas le destinataire du 
message, il vous est interdit d'en faire la copie, de le faire 
suivre,  d'en divulguer le contenu ou de l'utiliser en tout ou 
partie. Si vous avez reu ce message par erreur, merci d'en avertir 
immdiatement l'expditeur et de le dtruire.

L'intgrit du message n'est pas assure sur Internet, chaque 
information, y compris le nom de l'expditeur pouvant tre intercepte,
modifie, perdue, subir un retard dans sa transmission ou contenir 
des virus. L'expditeur dcline donc toute responsabilit pour toute 
altration, dformation ou falsification subie par le message au 
cours de sa transmission.

Toute opinion contenue dans ce message appartient a son auteur et 
ne peut engager la responsabilit de la socit expditrice du message,
a moins que cela ait t clairement spcifie dans le message et qu'il
soit vrifi que son auteur tait en mesure d'engager ladite socit. 
en particulier, sauf stipulation contraire, ce message ne constitue pas
une offre d'achat ou de vente d'instruments financiers.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

This message and any attachments are confidential to the ord...{{dropped}}



From Matthias.Templ at statistik.gv.at  Thu Apr  8 15:58:50 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Thu, 8 Apr 2004 15:58:50 +0200
Subject: [R] plot help
Message-ID: <83536658864BC243BE3C06D7E936ABD50153686C@xchg1.statistik.gv.at>

Hello!

This works:

hist(rnorm(100, mean = 20, sd =12), xlim=range(0,100), ylim=range(0,50))
par(new = TRUE)
hist(rnorm(100, mean = 88, sd = 2), xlim=range(0,100), ylim=range(0,50))

Matthias Templ

-----Urspr??ngliche Nachricht-----
Von: Pierre Clauss [mailto:pierre.clauss at sinopia.fr] 
Gesendet: Donnerstag, 08. April 2004 15:50
An: R-help at stat.math.ethz.ch
Betreff: [R] plot help


Hie!
I do not manage to make several plots on a same graph ( for example two densities on the same graph ) thanks for the answer Pierre Clauss


xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Ce message et toutes les pi??ces jointes (ci - apres le  \"message\" ) 
sont ??tablis a l'attention exclusive de ses destinataires et sont 
strictement confidentiels. Si vous n'??tes pas le destinataire du 
message, il vous est interdit d'en faire la copie, de le faire 
suivre,  d'en divulguer le contenu ou de l'utiliser en tout ou 
partie. Si vous avez re??u ce message par erreur, merci d'en avertir 
imm??diatement l'exp??diteur et de le d??truire.

L'int??grit?? du message n'est pas assur??e sur Internet, chaque 
information, y compris le nom de l'exp??diteur pouvant ??tre intercept??e, modifi??e, perdue, subir un retard dans sa transmission ou contenir 
des virus. L'exp??diteur d??cline donc toute responsabilit?? pour toute 
alt??ration, d??formation ou falsification subie par le message au 
cours de sa transmission.

Toute opinion contenue dans ce message appartient a son auteur et 
ne peut engager la responsabilit?? de la soci??t?? exp??ditrice du message, a moins que cela ait ??t?? clairement sp??cifie dans le message et qu'il soit v??rifi?? que son auteur ??tait en mesure d'engager ladite soci??t??. 
en particulier, sauf stipulation contraire, ce message ne constitue pas une offre d'achat ou de vente d'instruments financiers.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

This message and any attachments are confidential to the ord...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ecashin at uga.edu  Thu Apr  8 16:22:21 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Thu, 08 Apr 2004 10:22:21 -0400
Subject: [R] getting data frame rows out of a by object
References: <87isgbba23.fsf@uga.edu> <4074A4B4.47E8D257@adelaide.edu.au>
	<877jwrb6sj.fsf@uga.edu> <6rfzbelkqx.fsf@bates4.stat.wisc.edu>
Message-ID: <8765ca8s82.fsf@uga.edu>

Douglas Bates <bates at stat.wisc.edu> writes:

...
> The point is that factor(paste(...)) returns a factor with length of
> nrow(d) and with factor levels determined by the combination of levels
> of a and b (provided that you don't get ambiguities as described
> above).  The split function does not require that the factor
> determining the splits be part of the data frame being split.  It can
> be given explicitly as it is here.

I think I need to look at the source of split when I have more time.

> [1] If you really want to be cautious you could use an octal
> representation like sep="\007" to get a character that is very
> unlikely to occur in a factor level.

I definitely want to be cautious.  Instead of the bell character I
think I'll use the field separator character, "\034", just because
this is the first time I've been able to use it for it's intended
purpose!  ;)

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From bates at stat.wisc.edu  Thu Apr  8 16:58:16 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 08 Apr 2004 09:58:16 -0500
Subject: [R] Expected Mean Squares
In-Reply-To: <03C52175148BC04C974AEA13F429A3F7056A952C@usaflcms03.usa-spaceops.ksc.nasa.gov>
References: <03C52175148BC04C974AEA13F429A3F7056A952C@usaflcms03.usa-spaceops.ksc.nasa.gov>
Message-ID: <6roeq2ikjb.fsf@bates4.stat.wisc.edu>

"Moore, William G" <william.g.moore at usa-spaceops.com> writes:

> I am an applied statistician new to R, and I was wondering if R can
> calculate expected mean squares for unbalanced mixed models.  Any
> help would be appreciated.

Not that I know of but you can obtain REML or ML estimates of the
parameters in an unbalanced mixed model using function lme from the
nlme package.



From bates at stat.wisc.edu  Thu Apr  8 17:02:31 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 08 Apr 2004 10:02:31 -0500
Subject: [R] getting data frame rows out of a by object
In-Reply-To: <8765ca8s82.fsf@uga.edu>
References: <87isgbba23.fsf@uga.edu> <4074A4B4.47E8D257@adelaide.edu.au>
	<877jwrb6sj.fsf@uga.edu> <6rfzbelkqx.fsf@bates4.stat.wisc.edu>
	<8765ca8s82.fsf@uga.edu>
Message-ID: <6rk70qikc8.fsf@bates4.stat.wisc.edu>

Ed L Cashin <ecashin at uga.edu> writes:

> Douglas Bates <bates at stat.wisc.edu> writes:
> 
> ...
> > The point is that factor(paste(...)) returns a factor with length of
> > nrow(d) and with factor levels determined by the combination of levels
> > of a and b (provided that you don't get ambiguities as described
> > above).  The split function does not require that the factor
> > determining the splits be part of the data frame being split.  It can
> > be given explicitly as it is here.
> 
> I think I need to look at the source of split when I have more time.

?split is probably a better place to start.

> > [1] If you really want to be cautious you could use an octal
> > representation like sep="\007" to get a character that is very
> > unlikely to occur in a factor level.
> 
> I definitely want to be cautious.  Instead of the bell character I
> think I'll use the field separator character, "\034", just because
> this is the first time I've been able to use it for it's intended
> purpose!  ;)

Yes, but with "\034" you don't get to make obscure James Bond
references :-).



From ecashin at uga.edu  Thu Apr  8 17:14:25 2004
From: ecashin at uga.edu (Ed L Cashin)
Date: Thu, 08 Apr 2004 11:14:25 -0400
Subject: [R] getting data frame rows out of a by object
References: <87isgbba23.fsf@uga.edu> <4074A4B4.47E8D257@adelaide.edu.au>
	<877jwrb6sj.fsf@uga.edu> <6rfzbelkqx.fsf@bates4.stat.wisc.edu>
	<8765ca8s82.fsf@uga.edu> <6rk70qikc8.fsf@bates4.stat.wisc.edu>
Message-ID: <873c7e8pta.fsf@uga.edu>

Douglas Bates <bates at stat.wisc.edu> writes:

> Ed L Cashin <ecashin at uga.edu> writes:
...
>> I think I need to look at the source of split when I have more time.
>
> ?split is probably a better place to start.

I've read that, so I think I need more nitty gritty.

>> > [1] If you really want to be cautious you could use an octal
>> > representation like sep="\007" to get a character that is very
>> > unlikely to occur in a factor level.
>> 
>> I definitely want to be cautious.  Instead of the bell character I
>> think I'll use the field separator character, "\034", just because
>> this is the first time I've been able to use it for it's intended
>> purpose!  ;)
>
> Yes, but with "\034" you don't get to make obscure James Bond
> references :-).

I hadn't thought of that.

One thing I notice is that I get funny row names using this method.  I
can change them back easily enough, though.

> d
    a b  c
1   1 4 31
2   2 3 32
3   3 2 33
4   4 1 34
11  1 4 41
21  2 3 42
31  3 2 43
41  4 1 44
12 41 0  0
> d2 <- do.call("rbind",
          lapply(split(d,
                       factor(paste(d$a,
                                    d$b,
                                    sep = "\034"))),
                 function(x) x[x$c == min(x$c),]))
> d2
      a b  c
14   1 4 31
23   2 3 32
32   3 2 33
41   4 1 34
410 41 0  0
> row.names(d2) <- 1:nrow(d2)
> d2
   a b  c
1  1 4 31
2  2 3 32
3  3 2 33
4  4 1 34
5 41 0  0
> 

-- 
--Ed L Cashin            |   PGP public key:
  ecashin at uga.edu        |   http://noserose.net/e/pgp/



From mike.campana at freesurf.ch  Thu Apr  8 18:33:08 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Thu,  8 Apr 2004 17:33:08 +0100
Subject: [R] outliers
Message-ID: <1081438388.webexpressdV3.1.f@smtp.freesurf.ch>

Dear all

I would like to represent the outliers in the plot. These few outliers 
are much larger than the limit of 50 in the ylim-argument.

plot(daten$month~daten$no,ylim=c(0,50))

I know that it is possible to introduce the information about the 
presence of outliers without changing the range of the axis. Do you have 
an advice? I hope the question is understandable!!

Thanks a lot for your explantion

Mike

---




From scott.rifkin at yale.edu  Thu Apr  8 17:37:00 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Thu, 8 Apr 2004 11:37:00 -0400 (EDT)
Subject: [R] lme, mixed models, and nuisance parameters
Message-ID: <Pine.LNX.4.44.0404081113170.29848-100000@ajax.its.yale.edu>

I have the following dataset:

96 plots 
12 varieties 
2 time points 

The experiment is arranged as follows: 

A single plot has two varieties tested on it.

With respect to time points, plots come in 3 kinds:
(1) varietyA, timepoint#1 vs. variety B, timepoint#1 
(2) varietyA timepoint #2 vs. varietyB timepoint #2
(3) varietyA timepoint #1 vs. variety A timepoint#2   
- there are 36 of each kind of within timepoint comparison and 24 between
timepoint comparisons.  so it isn't a fully connected design.  

Plots and varieties are random samples from a population of plots and 
varieties, so they are random effects. The timepoints are fixed effects. 

I am particularly interested in the variance components for variety and
timepoint within variety, in the estimate for the fixed timepoint effect
and in the predictions (BLUP) for variety and timepoint within variety.

So the mixed model looks like:

Measurement ~ Timepoint + Plot + Variety + Variety/Timepoint

Here is the question:

I am not interested in Plot, so it would be great if I could avoid 
estimating it.

Since I take two measurements from each plot, I could remove plot by 
taking the difference between the measurements and feeding that into the 
appropriate model.  If I had only one Timepoint, this seems like it would 
be straightforward since in that case the variance of the difference would 
just be twice the variance of Variety, and from the blups of the 
differences I could reconstruct the blups of the individual varieties (or 
would that not be statistically sound?)

However, my differences come in different kinds because of the Timepoints, 
so it's a bit more complicated.

Does anyone have any suggestions for whether it is possible to extract the 
Variety and Variety/Timepont variances and blups and the Timepoint 
estimates if my measurements are differences rather than individual 
measurements?  If so, how would I go about setting the appropriate 
model using lme?


Thanks much for any help,

Scott Rifkin
scott.rifkin at yale.edu



From tlumley at u.washington.edu  Thu Apr  8 17:39:03 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Apr 2004 08:39:03 -0700 (PDT)
Subject: [R] getting data frame rows out of a by object
In-Reply-To: <873c7e8pta.fsf@uga.edu>
References: <87isgbba23.fsf@uga.edu> <4074A4B4.47E8D257@adelaide.edu.au>
	<877jwrb6sj.fsf@uga.edu> <6rfzbelkqx.fsf@bates4.stat.wisc.edu>
	<8765ca8s82.fsf@uga.edu> <6rk70qikc8.fsf@bates4.stat.wisc.edu>
	<873c7e8pta.fsf@uga.edu>
Message-ID: <Pine.A41.4.58.0404080836170.23138@homer33.u.washington.edu>

On Thu, 8 Apr 2004, Ed L Cashin wrote:
>
> One thing I notice is that I get funny row names using this method.  I
> can change them back easily enough, though.
>

I think you can use interaction() rather than the factor(paste()) approach

interaction(d$a, d$b, drop=TRUE) gives a factor with one level for each
existing combination of d$a and d$b.

	-thomas



From tlumley at u.washington.edu  Thu Apr  8 17:49:42 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Apr 2004 08:49:42 -0700 (PDT)
Subject: [R] R on MacOS X
In-Reply-To: <x2fzbe3aa4.fsf@biostat.ku.dk>
References: <200404080448.i384m5pY126654@atlas.otago.ac.nz>
	<16501.18241.640171.281030@gargle.gargle.HOWL>
	<x2fzbe3aa4.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.58.0404080843490.23138@homer33.u.washington.edu>

On Thu, 8 Apr 2004, Peter Dalgaard wrote:

> Martin Maechler <maechler at stat.math.ethz.ch> writes:
>
> >     Richard> a designated sysadmin, which I'm not.  Is there any
> >     Richard> alternative to downloading a source distribution
> >     Richard> and doing the classic ./configure;make;make install
> >     Richard> waltz?
> >
> > yes, there's always been.
> > You have installed the  pre-compiled aka binary version for Mac;
> > instead get the source -- and actually make sure you get the
> > source of "R 1.9.0 beta" since you'll be installing from the
> > source, and also because there are substantial Mac specific
> > enhancements there.
>
> Wasn't that what he was trying to avoid? Excellent answer still... ;-)
>


It's a less helpful answer than usual.  You can't compile R without
downloading and installing g77, and I think the binary distribution of
that also wants to go somewhere that needs administrator permission.

I believe, though I haven't yet tried, that the 1.9.0 setup will be
better.  I think you can put the R Framework in ~/Library/Frameworks and
then put R.app anywhere.

	-thomas



From sdhyok at catchlab.org  Thu Apr  8 18:02:28 2004
From: sdhyok at catchlab.org (Shin, Daehyok)
Date: Thu, 8 Apr 2004 12:02:28 -0400
Subject: [R] More user-friendly error message needed.
In-Reply-To: <40746540.9010804@arcanemethods.com>
Message-ID: <OAEOKPIGCLDDHAEMCAKIGEGICKAA.sdhyok@catchlab.org>

Hi, Bob.
As I said, I just begin to learn R, so I am not in a proper position to
compare the languages.
But, two things of R immediately captured my attention.

Pro. A lot of standard libraries including a powerful visualization, like
MATLAB.
As a scientific programmer, I really missed this kind of standard
GPL-licensed environment
for powerful array processing. You can use Numeric or numarray libraries to
use Python
for scientific computing, but most functions in other libraries don't assume
they can have vectors as arguments.

Con. Poor design for object-oriented programming, compared with Python.
I am quite sure that in the aspect of language design Python is far better
than MATLAB and R.
Simple, elegant, and powerful.
S3 supports only a basic level of OOP, while S4 introduced too complex
mechanisms for it.

Personally, I believe community and libraries are more important than
language itself.
So, even though I was a little disappointed at OOP of R, I will continue to
look at R.
Cheers.

Daehyok Shin (Peter)

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Bob Cain
> Sent: Wednesday, April 07, 2004 PM 4:32
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] More user-friendly error message needed.
>
>
>
>
> Shin wrote:
>
> > Thanks for your kind explanation.
> > Actually, I just begin to learn R, so not familiar with many behaviors
> > of R yet. As a user of Python and MATLAB, I understand the problem of
> > cryptic error messages of loosely-typed languages.
>
> Besides that, what do you think of R so far?
>
>
> Bob
> --
>
> "Things should be described as simply as possible, but no
> simpler."
>
>                                               A. Einstein
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Thu Apr  8 18:02:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 8 Apr 2004 09:02:49 -0700 (PDT)
Subject: [R] R on MacOS X
In-Reply-To: <Pine.A41.4.58.0404080843490.23138@homer33.u.washington.edu>
References: <200404080448.i384m5pY126654@atlas.otago.ac.nz>
	<16501.18241.640171.281030@gargle.gargle.HOWL>
	<x2fzbe3aa4.fsf@biostat.ku.dk>
	<Pine.A41.4.58.0404080843490.23138@homer33.u.washington.edu>
Message-ID: <Pine.A41.4.58.0404080901320.23138@homer33.u.washington.edu>

On Thu, 8 Apr 2004, Thomas Lumley wrote:
>
> I believe, though I haven't yet tried, that the 1.9.0 setup will be
> better.  I think you can put the R Framework in ~/Library/Frameworks and
> then put R.app anywhere.
>

Almost true.  You need to edit R.app/Contents/MacOS/R
and change the first line of code from
R_HOME_DIR=/Library/Frameworks/R.framework/Resources
to
R_HOME_DIR=~/Library/Frameworks/R.framework/Resources


	-thomas



From rafan at infor.org  Thu Apr  8 18:04:59 2004
From: rafan at infor.org (Rong-En Fan)
Date: Fri, 9 Apr 2004 00:04:59 +0800
Subject: [R] socket clusters on snow dies easily
Message-ID: <20040408160459.GA8624@muse.csie.ntu.edu.tw>

hello,

  I'm using R 1.8.1 with the lastest snow package on FreeBSD 4.9.
  However, when I try to using socket clusters, it's very unstable.
  Sometimes it dies half way when I run parSapply(), sometimes
  it dies when cluster connection is idle.

  I create a socket cluster by following cmd

  cl = makeCluster("foo", type = "SOCK", outfile="/tmp/rafanlog");

  Then, I just idle in R, and tail -f outfile, I got following:

  [... some R msgs ...]

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.


>
> #library(serialize)
> library(snow)
>
> slaveLoop(makeSOCKmaster())
Error in unserialize(node$con) : error reading from connection
Execution halted


  after makeCluster() returns, the outfile hangs around the empty lines
  above, about 1 minute later, all msg shows up, also it dies.

  This is reproducible on my machine. Also, one of my friends also 
  got this result on Linux machines w/ R 1.8.x.

  Is there any one also encounter this problem?
  Any comments or suggestion are welcome :)

Regards,
Rong-En Fan



From cmoffet at nwrc.ars.usda.gov  Thu Apr  8 18:14:09 2004
From: cmoffet at nwrc.ars.usda.gov (Corey Moffet)
Date: Thu, 08 Apr 2004 10:14:09 -0600
Subject: [R] Getting R to complete a web form
Message-ID: <3.0.6.32.20040408101409.01062348@pxms.nwrc.ars.usda.gov>

Dear R-Help,

I am testing a web based erosion model and I would like to get R to submit
the http requests.  Now I make the requests manually and use R to extract
the results by scan()ing a url() connection.  It would be nice if I could
fully automate the process.  Any ideas/examples?  The model uses perl
through CGI if that makes a difference.  The web form uses the post request
method.


With best wishes and kind regards I am

Sincerely,

Corey A. Moffet
Rangeland Scientist

##################################################################
                                            ####		     
USDA-ARS                                        #		     
Northwest Watershed Research Center             #		     
800 Park Blvd, Plaza IV, Suite 105          ###########   ####    
Boise, ID 83712-7716                       #    #      # #        
Voice: (208) 422-0718                      #    #  ####   ####    
FAX:   (208) 334-1502                      #    # #           #   
                                            ####   ###########    
##################################################################



From luke at stat.uiowa.edu  Thu Apr  8 18:18:43 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 8 Apr 2004 11:18:43 -0500 (CDT)
Subject: [R] socket clusters on snow dies easily
In-Reply-To: <20040408160459.GA8624@muse.csie.ntu.edu.tw>
Message-ID: <Pine.LNX.4.44.0404081113480.723-100000@itasca.stat.uiowa.edu>

If you absolutely have to use the socket approach you will probably
have to track down the particular reason for this failure in your
environment by looking at what is going on under the hood and checking
the pieces.  The master sets up a server socket, the slave noted
connect to that socket, and for some reason the first read on that
connection seems to be failing.  It could be a timing issues--you may
need to adjust socket timeouts, it could be a permission or firewall
issue; hard to tell without experimenting.

You are probably better off using the PVM version if at all possible.
Overall it more solid than the socket version and likely to have fewer
issues.

Best,

luke

On Fri, 9 Apr 2004, Rong-En Fan wrote:

> hello,
> 
>   I'm using R 1.8.1 with the lastest snow package on FreeBSD 4.9.
>   However, when I try to using socket clusters, it's very unstable.
>   Sometimes it dies half way when I run parSapply(), sometimes
>   it dies when cluster connection is idle.
> 
>   I create a socket cluster by following cmd
> 
>   cl = makeCluster("foo", type = "SOCK", outfile="/tmp/rafanlog");
> 
>   Then, I just idle in R, and tail -f outfile, I got following:
> 
>   [... some R msgs ...]
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> 
> >
> > #library(serialize)
> > library(snow)
> >
> > slaveLoop(makeSOCKmaster())
> Error in unserialize(node$con) : error reading from connection
> Execution halted
> 
> 
>   after makeCluster() returns, the outfile hangs around the empty lines
>   above, about 1 minute later, all msg shows up, also it dies.
> 
>   This is reproducible on my machine. Also, one of my friends also 
>   got this result on Linux machines w/ R 1.8.x.
> 
>   Is there any one also encounter this problem?
>   Any comments or suggestion are welcome :)
> 
> Regards,
> Rong-En Fan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From rossini at blindglobe.net  Thu Apr  8 18:20:07 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 08 Apr 2004 09:20:07 -0700
Subject: [R] socket clusters on snow dies easily
In-Reply-To: <20040408160459.GA8624@muse.csie.ntu.edu.tw> (Rong-En Fan's
	message of "Fri, 9 Apr 2004 00:04:59 +0800")
References: <20040408160459.GA8624@muse.csie.ntu.edu.tw>
Message-ID: <85oeq21lxk.fsf@servant.blindglobe.net>


I don't think the socket stuff is as well tested as PVM and MPI.
We've had a few problems with PVM via rpvm in the past, but I've not
had problems recently (maybe this message will shake them out) and
LAM-MPI via Rmpi seems a bit more stable (though perhaps less
tested). 

best,
-tony


Rong-En Fan <rafan at infor.org> writes:

> hello,
>
>   I'm using R 1.8.1 with the lastest snow package on FreeBSD 4.9.
>   However, when I try to using socket clusters, it's very unstable.
>   Sometimes it dies half way when I run parSapply(), sometimes
>   it dies when cluster connection is idle.
>
>   I create a socket cluster by following cmd
>
>   cl = makeCluster("foo", type = "SOCK", outfile="/tmp/rafanlog");
>
>   Then, I just idle in R, and tail -f outfile, I got following:
>
>   [... some R msgs ...]
>
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
>
>
>>
>> #library(serialize)
>> library(snow)
>>
>> slaveLoop(makeSOCKmaster())
> Error in unserialize(node$con) : error reading from connection
> Execution halted
>
>
>   after makeCluster() returns, the outfile hangs around the empty lines
>   above, about 1 minute later, all msg shows up, also it dies.
>
>   This is reproducible on my machine. Also, one of my friends also 
>   got this result on Linux machines w/ R 1.8.x.
>
>   Is there any one also encounter this problem?
>   Any comments or suggestion are welcome :)
>
> Regards,
> Rong-En Fan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ggrothendieck at myway.com  Thu Apr  8 19:38:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 8 Apr 2004 17:38:40 +0000 (UTC)
Subject: [R] More user-friendly error message needed.
References: <40746540.9010804@arcanemethods.com>
	<OAEOKPIGCLDDHAEMCAKIGEGICKAA.sdhyok@catchlab.org>
Message-ID: <loom.20040408T193719-20@post.gmane.org>


From: Shin, Daehyok <sdhyok <at> catchlab.org>
> Con. Poor design for object-oriented programming, compared with Python.

Have a look at oo.R package if you 
want a more traditional oo approach.



From fm3a004 at math.uni-hamburg.de  Thu Apr  8 19:48:50 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 8 Apr 2004 19:48:50 +0200 (MET DST)
Subject: [R] How to draw a tree?
Message-ID: <Pine.GSO.3.95q.1040408194703.16840P-100000@sun11.math.uni-hamburg.de>

Hi,

I have run rpart to construct a regression tree. Is there any simple
method to draw a nice picture of it, as it is usually done in books and
paper to visualize the tree?

Thank you,
Christian

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From rossini at blindglobe.net  Thu Apr  8 19:53:00 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 08 Apr 2004 10:53:00 -0700
Subject: [R] More user-friendly error message needed.
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIGEGICKAA.sdhyok@catchlab.org> (Daehyok
	Shin's message of "Thu, 8 Apr 2004 12:02:28 -0400")
References: <OAEOKPIGCLDDHAEMCAKIGEGICKAA.sdhyok@catchlab.org>
Message-ID: <851xmy1hmr.fsf@servant.blindglobe.net>

"Shin, Daehyok" <sdhyok at catchlab.org> writes:

> Con. Poor design for object-oriented programming, compared with
> Python.  I am quite sure that in the aspect of language design
> Python is far better than MATLAB and R.  Simple, elegant, and
> powerful.  S3 supports only a basic level of OOP, while S4
> introduced too complex mechanisms for it.

Bzzt...  Different design, not worse design.  It's from a different
OOP heritage than C++/Java/Python-ish objects, and makes more sense if
you come from a different set of computing languages.

That being said, I happen to like Python an awful lot.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From andy_liaw at merck.com  Thu Apr  8 19:55:59 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 8 Apr 2004 13:55:59 -0400
Subject: [R] How to draw a tree?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B5F@usrymx25.merck.com>

See ?post.rpart.  That's probably as good as it'll get.  You might also want
to look at the maptree package on CRAN.

HTH,
Andy

> From: Christian Hennig
> 
> Hi,
> 
> I have run rpart to construct a regression tree. Is there any simple
> method to draw a nice picture of it, as it is usually done in 
> books and
> paper to visualize the tree?
> 
> Thank you,
> Christian
> 
> **************************************************************
> *********
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, 
> http://www.math.uni-hamburg.de/home/hennig/
> 
> ##############################################################
> #########
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Apr  8 19:58:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 08 Apr 2004 19:58:24 +0200
Subject: [R] How to draw a tree?
In-Reply-To: <Pine.GSO.3.95q.1040408194703.16840P-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1040408194703.16840P-100000@sun11.math.uni-hamburg.de>
Message-ID: <407592C0.4020106@statistik.uni-dortmund.de>

Christian Hennig wrote:

> Hi,
> 
> I have run rpart to construct a regression tree. Is there any simple
> method to draw a nice picture of it, as it is usually done in books and
> paper to visualize the tree?
> 
> Thank you,
> Christian
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Christian,

rpobj <- rpart(....)
plot(rpobj)
text(rpobj)

See ?plot.rpart and ?text.rpart for details on how to tune the 
visualization.

Uwe



From ggrothendieck at myway.com  Thu Apr  8 20:47:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 8 Apr 2004 18:47:02 +0000 (UTC)
Subject: [R] More user-friendly error message needed.
References: <40746540.9010804@arcanemethods.com>
	<OAEOKPIGCLDDHAEMCAKIGEGICKAA.sdhyok@catchlab.org>
	<loom.20040408T193719-20@post.gmane.org>
Message-ID: <loom.20040408T204610-117@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

> 
> From: Shin, Daehyok <sdhyok <at> catchlab.org>
> > Con. Poor design for object-oriented programming, compared with Python.
> 
> Have a look at oo.R package if you 
> want a more traditional oo approach.

As several people have pointed out, this package is not on CRAN and to
download it you need to get it from:

http://www.maths.lth.se/help/R/R.classes/

Also, look around on that site for manuals and
other information.



From michelis at itc.it  Thu Apr  8 21:11:35 2004
From: michelis at itc.it (Francesca Demichelis)
Date: Thu, 8 Apr 2004 21:11:35 +0200
Subject: [R] labels on dendograms
Message-ID: <02DBD906DF7101489473AFB9AAD27E5C0176E0C6@ntmail.pc.itc.it>

 
 
Hi,
I like to add class labels on dendograms (as plot(agnesobject)).
Is it possible to be done?
Thank You in advance,
francesca



From charles.rosa at gm.com  Thu Apr  8 22:40:22 2004
From: charles.rosa at gm.com (charles.rosa@gm.com)
Date: Thu, 8 Apr 2004 16:40:22 -0400
Subject: [R] Discrete Choice Modeling modules for R??
Message-ID: <OF6086518D.B46D0EA3-ON85256E70.0071840C-85256E70.00718F33@mail.gm.com>

To all,
Does anyone know whether there are any
Discrete Choice Modeling modules for R??

Thanks, Charlie


-------------------------
Charles H. Rosa, Ph.D.
Staff Research Engineer
General Motors Corporation
R&D and Planning
Mail Code 480-106-359
30500 Mound Rd
Warren, MI 48090-9055
Tel/work/cell: +1 248 670-8389
Fax/work: +1 586 986-0574



From bacolli at uark.edu  Thu Apr  8 22:38:40 2004
From: bacolli at uark.edu (Bret Collier)
Date: Thu, 08 Apr 2004 15:38:40 -0500
Subject: [R] Evaluating AIC
Message-ID: <5.2.1.1.0.20040408153113.02aadfb0@mail.uark.edu>

R Users,
         I was just wondering if anyone has written a program (or if there 
is a package) out there that calculates the different derivations of AIC 
(e.g. AIC, AICc, QAIC, etc.) along with AIC differences (delta's), model 
likelihoods, Akaike weights and evidence ratio's (from Burnham and Anderson 
2002).

Just in a "for instance" if someone had the -2LL, sample sizes, parameter 
counts, and estimates of c_hat output from a program, is there a function 
out there that calculates the above information.  I did not see anything on 
the help pages (or in packages, but I could have missed it) and I didn't 
want to re-invent the wheel.

TIA,

Bret A. Collier
Arkansas Cooperative Fish and Wildlife Research Unit
Department of Biological Sciences  SCEN 513
University of Arkansas
Fayetteville, AR  72701
bacolli at uark.edu



From Brian.J.GREGOR at odot.state.or.us  Thu Apr  8 23:12:39 2004
From: Brian.J.GREGOR at odot.state.or.us (Brian.J.GREGOR@odot.state.or.us)
Date: Thu, 8 Apr 2004 14:12:39 -0700 
Subject: [R] Why are Split and Tapply so slow with named vectors,
	why is a for loop faster than mapply
Message-ID: <372EFF9FE4E42E419C978E7A305DC5FE0379AD14@exsalem5.odot.state.or.us>

First, here's the problem I'm working on so you understand the context. I
have a data frame of travel activity characteristics with 70,000+ records.
These activities are identified by unique chain numbers. (Activities are
part of trip chains.) There are 17,500 chains. 

I use the chain numbers as factors to split various data fields into lists
of chain characteristics with each element of the list representing one
chain. For example:

> betaHomeDist[1:3]
$"400001111"
     1316      2319      2317      1364      1316 
 0.000000 14.930820 24.431210  6.174959  0.000000 

$"400001211"
     1316      2319      2319      1364      1316 
 0.000000 14.930820 14.930820  6.174959  0.000000 

$"400001212"
     1316      1364      2324      1364      1316 
 0.000000  6.174959 14.392375  6.174959  0.000000 

Where each element of the list is a named vector. Each vector element is
named with the zone that the activity occurred within. I use these names in
subsequent computations.

What I've found, however, is that it is not easy (or I have not found the
easy way) to split a named vector into a list that retains the vector names.
For example, splitting an unnamed vector (70,000+) based on the chain
numbers takes very little time:
> system.time(actTimeList <- split(actTime, chainId))
[1] 0.16 0.00 0.15   NA   NA

But if the vector is named, R will work for minutes and still not complete
the job:
> names(actTime) <- zoneNames
> system.time(actTimeList <- split(actTime, chainId))
Timing stopped at: 83.22 0.12 84.49 NA NA

The same thing happens with using tapply with a named vector such as:
tapply(actTime, chainId, function(x) x)

Using the following function with a for loop accomplishes the job in a few
seconds for all 70,000+ records: 
> splitWithNames <- function(dataVector, nameVector, factorVector){
+     dataList <- split(dataVector, factorVector)
+     nameList <- split(nameVector, factorVector)
+     listLength <- length(dataList)
+     namedDataList <- list(NULL)
+     for(i in 1:listLength){
+         x <- dataList[[i]]
+         names(x) <- nameList[[i]]
+         namedDataList[[i]] <- x
+         }
+     namedDataList
+     }
> system.time(actTimeList <- splitWithNames(actTime, zoneNames, chainId))
[1] 8.04 0.00 9.03   NA   NA

However if I rewrite the function to use mapply instead of a for loop, it
again takes a long (undetermined) amount of time to complete. Here are the
results for just 5000  and 10000 records. You can see that there is a
scaling issue:
> testfun <- function(dataVector, nameVector, factorVector){
+     dataList <- split(dataVector, factorVector)
+     nameList <- split(nameVector, factorVector)
+     nameFun <- function(x, xNames){
+         names(x) <- xNames
+         x
+         }
+     mapply(nameFun, dataList, nameList, SIMPLIFY=TRUE)
+     }
> system.time(actTimeList <- testfun(actTime[1:5000], zoneNames[1:5000],
chainId[1:5000]))
[1] 2.99 0.00 2.98   NA   NA
> system.time(actTimeList <- testfun(actTime[1:10000], zoneNames[1:10000],
chainId[1:10000]))
[1] 10.64  0.00 10.64    NA    NA

My problem is solved for now with the home-brew splitWithNames function, but
I'm curious about why named vectors slow down split and tapply so much and
why a function using mapply is so much slower than a function that uses a
for loop?

My computer is a 800+ MHz Pentium III with 512 Mb of memory. The operating
system is Windows NT 4.0. My R version is 1.8.1.

Thank you.

Brian Gregor, P.E.
Transportation Planning Analysis Unit
Oregon Department of Transportation
Brian.J.GREGOR at odot.state.or.us
(503) 986-4120



From p.dalgaard at biostat.ku.dk  Fri Apr  9 00:09:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Apr 2004 00:09:02 +0200
Subject: [R] Why are Split and Tapply so slow with named vectors,
	why is a for loop faster than mapply
In-Reply-To: <372EFF9FE4E42E419C978E7A305DC5FE0379AD14@exsalem5.odot.state.or.us>
References: <372EFF9FE4E42E419C978E7A305DC5FE0379AD14@exsalem5.odot.state.or.us>
Message-ID: <x2wu4q6s1t.fsf@biostat.ku.dk>

Brian.J.GREGOR at odot.state.or.us writes:

> What I've found, however, is that it is not easy (or I have not found the
> easy way) to split a named vector into a list that retains the vector names.
> For example, splitting an unnamed vector (70,000+) based on the chain
> numbers takes very little time:
> > system.time(actTimeList <- split(actTime, chainId))
> [1] 0.16 0.00 0.15   NA   NA
> 
> But if the vector is named, R will work for minutes and still not complete
> the job:
> > names(actTime) <- zoneNames
> > system.time(actTimeList <- split(actTime, chainId))
> Timing stopped at: 83.22 0.12 84.49 NA NA
> 
> The same thing happens with using tapply with a named vector such as:
> tapply(actTime, chainId, function(x) x)
> 
> Using the following function with a for loop accomplishes the job in a few
> seconds for all 70,000+ records: 
> > splitWithNames <- function(dataVector, nameVector, factorVector){
> +     dataList <- split(dataVector, factorVector)
> +     nameList <- split(nameVector, factorVector)
> +     listLength <- length(dataList)
> +     namedDataList <- list(NULL)
> +     for(i in 1:listLength){
> +         x <- dataList[[i]]
> +         names(x) <- nameList[[i]]
> +         namedDataList[[i]] <- x
> +         }
> +     namedDataList
> +     }
> > system.time(actTimeList <- splitWithNames(actTime, zoneNames, chainId))
> [1] 8.04 0.00 9.03   NA   NA
> 
> However if I rewrite the function to use mapply instead of a for loop, it
> again takes a long (undetermined) amount of time to complete. Here are the
> results for just 5000  and 10000 records. You can see that there is a
> scaling issue:
> > testfun <- function(dataVector, nameVector, factorVector){
> +     dataList <- split(dataVector, factorVector)
> +     nameList <- split(nameVector, factorVector)
> +     nameFun <- function(x, xNames){
> +         names(x) <- xNames
> +         x
> +         }
> +     mapply(nameFun, dataList, nameList, SIMPLIFY=TRUE)
> +     }
> > system.time(actTimeList <- testfun(actTime[1:5000], zoneNames[1:5000],
> chainId[1:5000]))
> [1] 2.99 0.00 2.98   NA   NA
> > system.time(actTimeList <- testfun(actTime[1:10000], zoneNames[1:10000],
> chainId[1:10000]))
> [1] 10.64  0.00 10.64    NA    NA
> 
> My problem is solved for now with the home-brew splitWithNames function, but
> I'm curious about why named vectors slow down split and tapply so much and
> why a function using mapply is so much slower than a function that uses a
> for loop?

If you look inside split.default, you'll see that it only uses fast
internal code in simple cases:

    if (is.null(attr(x, "class")) && is.null(names(x)))
        return(.Internal(split(x, f)))

in the other cases, we use

    for (k in lf) y[[k]] <- x[f %in% k]

and if lf is large, we get a large number of calls to %in%. This
wasn't really designed for that case, but I suppose we could be
smarter about it.

Wouldn't know about mapply, but are you sure you want SIMPLIFY=TRUE in
there???

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jasont at indigoindustrial.co.nz  Fri Apr  9 00:25:33 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 09 Apr 2004 10:25:33 +1200
Subject: [R] outliers
In-Reply-To: <1081438388.webexpressdV3.1.f@smtp.freesurf.ch>
References: <1081438388.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <4075D15D.1010409@indigoindustrial.co.nz>

mike.campana at freesurf.ch wrote:

> Dear all
> 
> I would like to represent the outliers in the plot. These few outliers 
> are much larger than the limit of 50 in the ylim-argument.
> 
> plot(daten$month~daten$no,ylim=c(0,50))
> 
> I know that it is possible to introduce the information about the 
> presence of outliers without changing the range of the axis. Do you have 
> an advice? I hope the question is understandable!!

Three things come to mind --

1) Use a logarithmic y-axis.
2) Put text directly on the plot, stating where outliers are.
3) Use layout() and margin and axis settings of par() to stack two plots 
one above the other, with different y-ranges, like this:

## layout of a split-range plot

## fake data

zz <- runif(100,0,35)
zz[sample(seq(along=zz),10)] <- runif(10,90,100)

par(mfcol=c(2,1))
par(mai=c(0, 1, 0.25,0.5))
plot(zz,ylim=c(80,100),xaxt="n",ylab="Outliers")
par(mai=c(1,1,0,0.5))
plot(zz,ylim=c(0,50),ylab="Nice data")

Hope that helps

Jason



From jasont at indigoindustrial.co.nz  Fri Apr  9 00:28:33 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 09 Apr 2004 10:28:33 +1200
Subject: [R] Evaluating AIC
In-Reply-To: <5.2.1.1.0.20040408153113.02aadfb0@mail.uark.edu>
References: <5.2.1.1.0.20040408153113.02aadfb0@mail.uark.edu>
Message-ID: <4075D211.2080001@indigoindustrial.co.nz>

Bret Collier wrote:

> R Users,
>         I was just wondering if anyone has written a program (or if 
> there is a package) out there that calculates the different derivations 
> of AIC 
...
> I did not 
> see anything on the help pages (or in packages, but I could have missed 
> it) 
...
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Where it tells you about help.search()

help.search("AIC")

Cheers

Jason



From jasont at indigoindustrial.co.nz  Fri Apr  9 00:34:13 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 09 Apr 2004 10:34:13 +1200
Subject: [R] Getting R to complete a web form
In-Reply-To: <3.0.6.32.20040408101409.01062348@pxms.nwrc.ars.usda.gov>
References: <3.0.6.32.20040408101409.01062348@pxms.nwrc.ars.usda.gov>
Message-ID: <4075D365.40305@indigoindustrial.co.nz>

Corey Moffet wrote:

> Dear R-Help,
> 
> I am testing a web based erosion model and I would like to get R to submit
> the http requests.  Now I make the requests manually and use R to extract
> the results by scan()ing a url() connection.  It would be nice if I could
> fully automate the process.  Any ideas/examples?  The model uses perl
> through CGI if that makes a difference.  The web form uses the post request
> method.

I usually "cheat" by pre-builing the form parameters into the URL, such as:

http://www.google.co.nz/search?q=R+statistics&ie=UTF-8&oe=UTF-8&hl=en&btnG=Google+Search&meta=

And just stick the parameters I need after the "?".

?connections  or ?url  (same page)

Cheers

Jason



From hodgess at gator.uhd.edu  Fri Apr  9 02:02:27 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 8 Apr 2004 19:02:27 -0500
Subject: [R] AIC
Message-ID: <200404090002.i3902Rf31299@gator.dt.uh.edu>

Hi Bret!

There is a step function and an AIC function in the base package.  Also
there is a logLik function in base as well.

Finally, there is a stepAIC function in MASS

Hope this helps!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From andy_liaw at merck.com  Fri Apr  9 04:53:46 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 8 Apr 2004 22:53:46 -0400
Subject: [R] R/Splus code for PRESS?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B72@usrymx25.merck.com>

[I thought this has been covered before, but I can't find it in the
archive...]

PRESS = sum of squares of deletion residuals

Any good book on regression will show you that deletion residual for
multiple regression fitted by least squares is just the raw residual divided
by one minus the hat values (or `leverage values').  And there is a
hatvalues() function in R.  With that, the computation of PRESS should be
fairly straight forward.

Andy

> From: Mai Zhou
> 
> Dear R-help, 
> Does anybody know where can I find R/Splus code for computing
> PREdiction Sum of Squares (PRESS)
> in a linear regression model?
> 
> I have a large regression model (~100 predictors, ~30,000 cases)
> and good prediction is the main goal.
> 
> I remember there is a faster way to compute it (no need to
> repeatedly fit models), but if there is existing code......
> 
> Thanks.
> 
> Mai Z
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From basisfm at dreamwiz.com  Fri Apr  9 06:33:18 2004
From: basisfm at dreamwiz.com (Ja-Yong Koo)
Date: Fri, 9 Apr 2004 13:33:18 +0900
Subject: [R] question
Message-ID: <200404090433.i394XIK0038323@smtp0.dreamwiz.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040409/917cd3b0/attachment.pl

From tpapp at axelero.hu  Fri Apr  9 07:14:21 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 9 Apr 2004 07:14:21 +0200
Subject: [R] question
In-Reply-To: <200404090433.i394XIK0038323@smtp0.dreamwiz.com>
References: <200404090433.i394XIK0038323@smtp0.dreamwiz.com>
Message-ID: <20040409051421.GA857@localhost>

On Fri, Apr 09, 2004 at 01:33:18PM +0900, Ja-Yong Koo wrote:

> I have found a strange thing. If I copy and paste the R source file, then
> the result is okay with. However, if I use R GUI (File icon and Source
> File),
> then the result is different. Please let me know possible reasons.

Next time you ask a question please provide more information -- I have
to guess what you mean by the result being "different".  And read the
posting guide.

The source("filename.R") command in R simply reads and evaluates the
source file, but won't display it.

Use 

> source("filename.R", echo=TRUE)

to display the commands in the file. See

> ?source

for further information.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From rafan at infor.org  Fri Apr  9 07:59:09 2004
From: rafan at infor.org (Rong-En Fan)
Date: Fri, 9 Apr 2004 13:59:09 +0800
Subject: [R] socket clusters on snow dies easily
In-Reply-To: <Pine.LNX.4.44.0404081113480.723-100000@itasca.stat.uiowa.edu>
References: <20040408160459.GA8624@muse.csie.ntu.edu.tw>
	<Pine.LNX.4.44.0404081113480.723-100000@itasca.stat.uiowa.edu>
Message-ID: <20040409055909.GA19974@muse.csie.ntu.edu.tw>

On Thu, Apr 08, 2004 at 11:18:43AM -0500, Luke Tierney wrote:
> If you absolutely have to use the socket approach you will probably
> have to track down the particular reason for this failure in your
> environment by looking at what is going on under the hood and checking
> the pieces.  The master sets up a server socket, the slave noted
> connect to that socket, and for some reason the first read on that
> connection seems to be failing.  It could be a timing issues--you may
> need to adjust socket timeouts, it could be a permission or firewall
> issue; hard to tell without experimenting.
> 
> You are probably better off using the PVM version if at all possible.
> Overall it more solid than the socket version and likely to have fewer
> issues.

er, due to some historical reason, we want to use socket instead of
MPI or PVM. My friend and I will trace the socket code and see how
to solve this problem. If there is anything new, i'll post here.

Regards,
Rong-En Fan



From =?iso-8859-1?Q?Micha=EBl?=  Fri Apr  9 10:26:54 2004
From: =?iso-8859-1?Q?Micha=EBl?= (=?iso-8859-1?Q?Micha=EBl?=)
Date: Fri, 09 Apr 2004 10:26:54 +0200
Subject: [R] bootstrap function coefficients
Message-ID: <5.0.2.1.2.20040409102435.00b900d8@utinam.univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040409/1d933102/attachment.pl

From nolwenn.lemeur at nantes.inserm.fr  Fri Apr  9 11:26:57 2004
From: nolwenn.lemeur at nantes.inserm.fr (Nolwenn Le Meur)
Date: Fri, 9 Apr 2004 11:26:57 +0200
Subject: [R] generating 00Index.html
Message-ID: <LMEBLNBEKKODLAONNGJMEEDLCDAA.nolwenn.lemeur@nantes.inserm.fr>

Hi,
I've started to create the documentation of my package.
I've generate a pdf and the different html files from .Rd files.
However I don't know how to automatically generate the 00Index.html. Do I
have to write one in latex style and convert it or can I use  the CONTENTS,
INDEX, or Rd files.

Thanks

Nolwenn

********************************************
Nolwenn Le Meur
INSERM U533
Facult?? de m??decine
1, rue Gaston Veil
44035 Nantes Cedex 1
France

Tel: (+33)-2-40-41-29-86 (office)
     (+33)-2-40-41-28-44 (secretary)
Fax: (+33)-2-40-41-29-50
mail: nolwenn.lemeur at nantes.inserm.fr



From ripley at stats.ox.ac.uk  Fri Apr  9 12:16:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 11:16:14 +0100 (BST)
Subject: [R] bootstrap function coefficients
In-Reply-To: <5.0.2.1.2.20040409102435.00b900d8@utinam.univ-fcomte.fr>
Message-ID: <Pine.LNX.4.44.0404091114430.3511-100000@gannet.stats>

Package bootstrap is ORPHANED (that is, unsupported by anyone).  Do try 
package boot instead.

10 is a very small number of bootstrap replications: I do suggest you try 
100 or more.

On Fri, 9 Apr 2004, Micha??l  Coeurdassier wrote:

> Dear R community,
> 
> Please, can you help me with a problem concerning bootstrap. The data table 
> called ??RMika??, contained times (Tps) and corresponding concentration of a 
> chemical in a soil (SolA). I would like to get, by bootstraping, 10 
> estimations of the parameters C0 and k from the function: SolA = 
> C0*exp(-k*Tps).
> 
> # First, I fit the data and all is OK
> 
>  > tabMika<-read.delim("RMika.txt")
>  > library(nls)
>  > attach(tabMika)
>  > Expon<-function(Tps,parm){
> + C0<-parm[1]
> + k<-parm[2]
> + }
>  > DegSA.nls<-nls(SolA~C0*exp(-k*Tps),start=c(C0=35, k=1),tabMika)
>  > summary(DegSA.nls)
> 
> Formula: SolA ~ C0 * exp(-k * Tps)
> 
> Parameters:
>      Estimate Std. Error t value Pr(>|t|)
> C0 25.682104   1.092113   23.52  < 2e-16 ***
> k   0.087356   0.007582   11.52 6.36e-13 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 2.584 on 32 degrees of freedom
> 
> Correlation of Parameter Estimates:
>        C0
> k 0.7101
> 
> 
> # Second, I try to use bootstrap to get the 10 estimates of k and C0
> 
> 
>  > library(bootstrap)
>  > theta<-function(tabMika){coef(eval(DegSA.nls$call))}
>  > bootSolA.nls<-bootstrap(tabMika,10,theta)
> 
> Warning message:
> multi-argument returns are deprecated in: return(thetastar, func.thetastar, 
> jack.boot.val, jack.boot.se,
> 
>  > bootSolA.nls
> $thetastar
>            [,1]        [,2]        [,3]        [,4]        [,5]        [,6]
> C0 25.68210358 25.68210358 25.68210358 25.68210358 25.68210358 25.68210358
> k   0.08735615  0.08735615  0.08735615  0.08735615  0.08735615  0.08735615
>            [,7]        [,8]        [,9]       [,10]
> C0 25.68210358 25.68210358 25.68210358 25.68210358
> k   0.08735615  0.08735615  0.08735615  0.08735615
> 
> # As you can notify, the 10 estimations have the same values for C0 and k. 
> Moreover, this correspond to the values of k and C0 determined by fitting 
> all the data without bootstrap!!!???
> I cannot find what is wrong. Please, if you find a solution, thank you to 
> send it to me.
> 
> Sincerely
> 
> Michael Coeurdassier


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr  9 12:18:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 11:18:04 +0100 (BST)
Subject: [R] generating 00Index.html
In-Reply-To: <LMEBLNBEKKODLAONNGJMEEDLCDAA.nolwenn.lemeur@nantes.inserm.fr>
Message-ID: <Pine.LNX.4.44.0404091116340.3511-100000@gannet.stats>

It happens automatically when you INSTALL the package sources ... sounds 
as if you are not actually doing that.

On Fri, 9 Apr 2004, Nolwenn Le Meur wrote:

> I've started to create the documentation of my package.
> I've generate a pdf and the different html files from .Rd files.
> However I don't know how to automatically generate the 00Index.html. Do I
> have to write one in latex style and convert it or can I use  the CONTENTS,
> INDEX, or Rd files.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr  9 12:23:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 11:23:35 +0100 (BST)
Subject: [R] nlme on Windows 2000 (v1.8.1)
In-Reply-To: <40753FAC.8060805@darwin.eeb.uconn.edu>
Message-ID: <Pine.LNX.4.44.0404091120490.3511-100000@gannet.stats>

See the rw-FAQ, specifically Q2.19.  It is very likely some piece of
rogue software on your machines.

On Thu, 8 Apr 2004, Kent Holsinger wrote:

> I have a problem with nlme on Windows 2000, and I'm having a devil of a 
> time determining whether the problem is with my computer or with 
> something in R. I'm running v1.8.1 on a Dell Pentium III with 512MB of 
> RAM and all of the recommended Windows 2000 updates applied.
> 
> If I use Rterm, I can run analyses with NLME to my heart's content. But 
> when I run Rgui, I encounter a floating point exception. To be concrete if I
> 
>  > library(nlme)
> Loading required package: lattice
>  > data(Rail)
>  > lme(travel ~ 1, data = Rail, random = ~ 1 | Rail)
> 
> (the first example in Pinheiro and Bates), R churns for a while and a 
> window pops up informing me of an application error. Specifically, "The 
> exception Floating-point division by zero. (0xc000008e) occurred in the 
> application at location 0x639b50ff." There error occurs every time. The 
> same analysis runs flawlessly in Rterm.
> 
> To make it even stranger, I get the same error on a Toshiba Pentium 
> laptop (also with 512MB of memory, although I haven't tried the analyses 
> in Rterm on that machine). On that machine, I get a blue screen after 
> acknowledging the error. The laptop is a dual boot on which I run Linux 
> (Fedora Core 1). R works perfectly on it under Linux.
> 
> I've downloaded and re-installed the binaries on both machines several 
> times, so I don't think I have a corrupted download. Any ideas on how to 
> diagnose (and solve) this problem would be greatly appreciated.
> 
> Kent
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Fri Apr  9 13:37:22 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 09 Apr 2004 07:37:22 -0400
Subject: [R] nlme on Windows 2000 (v1.8.1)
In-Reply-To: <40753FAC.8060805@darwin.eeb.uconn.edu>
References: <40753FAC.8060805@darwin.eeb.uconn.edu>
Message-ID: <im2d70hl3pe568qr7g9dn2fobkkfgsiolq@4ax.com>

On Thu, 08 Apr 2004 08:03:56 -0400, you wrote:

>If I use Rterm, I can run analyses with NLME to my heart's content. But 
>when I run Rgui, I encounter a floating point exception.

I agree with Prof Ripley: I'd suspect video (or other) drivers messing
up the floating point control word. 

Identifying the exact cause is quite hard, but you can probably
confirm this by rebooting in "safe mode" (i.e. with minimal drivers
loaded).  You do this by hitting F8 during the boot sequence.  R can
run fine in safe mode; if you don't get the error, then it's pretty
clearly one of the drivers that you usually load that is causing the
trouble.  Figuring out which one won't be easy...

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Apr  9 13:55:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 12:55:13 +0100 (BST)
Subject: [R] %+=% and eval.parent()
In-Reply-To: <x2hdvwi30t.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404091251420.3561-100000@gannet.stats>

On 7 Apr 2004, Peter Dalgaard wrote:

> Robin Hankin <rksh at soc.soton.ac.uk> writes:
> 
> > Hi again everybody.
> > 
> > Yesterday I had a problem with c-style "+=" functions.  One suggestion
> > was to define
> > 
> > R> "plus<-" <- function(x,value){x+value}
> > 
> > Then
> > 
> > R> a <- matrix(1:9,3,3)
> > R> plus(a[a%%2==1]) <- 1000
> > 
> > works as desired.
> > 
> > 
> > QUESTION: why does this behave differently:
> > 
> > 
> > R> "plus<-" <- function(x,y){x+y}
> > R> a <- matrix(1:9,3,3)
> > 
> > R> plus(a[a%%2==1]) <- 1000
> > Error in "plus<-"(`*tmp*`, value = 1000) :
> > 	unused argument(s) (value ...)
> > R>
> > 
> > The only change seems to be changing the second argument from "value"
> > to "y".  Why does this affect anything?  Where do I look for
> > documentation on things like "plus<-"  ?
> 
> These assignment functions work basically by
> 
>  plus(x) <- foo 
> 
> getting internally transcribed as
> 
>  x <- "plus<-"(x, value=foo)
> 
> (actually, there's an intermediate alias for the target to avoid
> multiple evaluation; this is what shows up as *tmp*, but you're not
> supposed to know that...)
> 
> The use of the keyword matching form was prompted by some problems
> with indexing functions that take a variable number of indices in
> addition to x. (Think "[<-"(x,i,value) vs. "[<-"(x,i,j,value) and the
> hoops you have to jump through when the value gets passed in the j
> argument.) However, keyword matching of course implies that you need
> to use the matching keyword in the function definition.
> 
> This *should* be somewhere in the R Language Definition, although I'm
> not sure it is actually there. Or the blue book, although I suspect
> that S v.3 actually used positional matching (and jumped through
> hoops).

It did.  This is in the FAQ, 3.2.3:

   * In R, the argument of a replacement function which corresponds to the
     right hand side must be named `value'.  E.g., `f(a) <- b' is evaluated
     as `a <- "f<-"(a, value = b)'.  S always takes the last argument,
     irrespective of its name.

I seem to recall I discovered the discrepancy in ca 1998.

R-lang is minimalist about what it calls `assignment functions', most of 
the time.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ivan_yegorov at mail.ru  Fri Apr  9 13:55:39 2004
From: ivan_yegorov at mail.ru (=?koi8-r?Q?=22?=Ivan Yegorov=?koi8-r?Q?=22=20?=)
Date: Fri, 09 Apr 2004 15:55:39 +0400
Subject: [R] Question
Message-ID: <E1BBubj-000D4n-00.ivan_yegorov-mail-ru@f19.mail.ru>

Could you please help me. How can I get indexes of matrix elements equal to the specified value. Thanks in advance.



From ccleland at optonline.net  Fri Apr  9 14:06:53 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 09 Apr 2004 08:06:53 -0400
Subject: [R] Question
In-Reply-To: <E1BBubj-000D4n-00.ivan_yegorov-mail-ru@f19.mail.ru>
References: <E1BBubj-000D4n-00.ivan_yegorov-mail-ru@f19.mail.ru>
Message-ID: <407691DD.4070303@optonline.net>

which(matrix(rnorm(30), ncol=3) > 1, arr.ind = TRUE)

See ?which

Ivan Yegorov wrote:
> Could you please help me. How can I get indexes of matrix elements equal to the specified value. Thanks in advance.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From tpapp at axelero.hu  Fri Apr  9 14:15:52 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 9 Apr 2004 14:15:52 +0200
Subject: [R] looking for a tutorial on exception handling
Message-ID: <20040409121552.GA1202@localhost>

Hi,

I would like to learn how to do exception handling in R.  I had a look
at the help page for tryCatch and I am trying to understand it (I am
sure it is well-written, but I have not used exception handling before
in any language, so I have no experience in that area).

Is there a tutorial or an introduction into exception handling in R
with tryCatch & friends?  Preferably with lots of examples.  It would
be nice if there was one online, but references to books would also be
appreciated.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From paolo.radaelli at unimib.it  Fri Apr  9 14:35:04 2004
From: paolo.radaelli at unimib.it (Paolo Radaelli)
Date: Fri, 9 Apr 2004 14:35:04 +0200
Subject: [R] R-estimators
Message-ID: <01b101c41e2f$15386490$90788495@dimequant.unimib.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040409/7dfc9e04/attachment.pl

From ckjmaner at carolina.rr.com  Fri Apr  9 14:39:02 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Fri, 9 Apr 2004 08:39:02 -0400
Subject: [R] RODBC on FreeBSD 4.9
Message-ID: <007501c41e2f$a6ddb3a0$6400a8c0@Athena>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040409/d6d6109d/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Apr  9 14:41:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Apr 2004 14:41:24 +0200
Subject: [R] %+=% and eval.parent()
In-Reply-To: <Pine.LNX.4.44.0404091251420.3561-100000@gannet.stats>
References: <Pine.LNX.4.44.0404091251420.3561-100000@gannet.stats>
Message-ID: <x2d66hs4qz.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > This *should* be somewhere in the R Language Definition, although I'm
> > not sure it is actually there. Or the blue book, although I suspect
> > that S v.3 actually used positional matching (and jumped through
> > hoops).
> 
> It did.  This is in the FAQ, 3.2.3:
> 
>    * In R, the argument of a replacement function which corresponds to the
>      right hand side must be named `value'.  E.g., `f(a) <- b' is evaluated
>      as `a <- "f<-"(a, value = b)'.  S always takes the last argument,
>      irrespective of its name.
> 
> I seem to recall I discovered the discrepancy in ca 1998.

I seem to recall when we introduced it, and that it was deliberate.
What I can't remember is what the occasion was.
 
> R-lang is minimalist about what it calls `assignment functions', most of 
> the time.

`replacement functions' would be correct terminology, right? <- and
<<- are the assignment functions.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kent at darwin.eeb.uconn.edu  Fri Apr  9 14:52:01 2004
From: kent at darwin.eeb.uconn.edu (Kent Holsinger)
Date: Fri, 09 Apr 2004 08:52:01 -0400
Subject: [R] nlme on Windows 2000 (v1.8.1)
In-Reply-To: <im2d70hl3pe568qr7g9dn2fobkkfgsiolq@4ax.com>
References: <40753FAC.8060805@darwin.eeb.uconn.edu>
	<im2d70hl3pe568qr7g9dn2fobkkfgsiolq@4ax.com>
Message-ID: <40769C71.7030302@darwin.eeb.uconn.edu>

Duncan Murdoch wrote:

> On Thu, 08 Apr 2004 08:03:56 -0400, you wrote:
> 
> 
>>If I use Rterm, I can run analyses with NLME to my heart's content. But 
>>when I run Rgui, I encounter a floating point exception.

> You do this by hitting F8 during the boot sequence.  R can
> run fine in safe mode; if you don't get the error, then it's pretty
> clearly one of the drivers that you usually load that is causing the
> trouble.  Figuring out which one won't be easy...

It *is* one of my drivers. NLME works fine in safe mode. If/when I 
manage to find the offending driver I'll report it to the list. Thanks 
to you and Dr. Ripley for the advice. (As I suspected, the fault lies in 
my computers, not R.)

Kent

-- 
Kent E. Holsinger                kent at darwin.eeb.uconn.edu
                                  http://darwin.eeb.uconn.edu
-- Department of Ecology & Evolutionary Biology
-- University of Connecticut, U-3043
-- Storrs, CT   06269-3043



From enpsgp at bath.ac.uk  Fri Apr  9 15:06:50 2004
From: enpsgp at bath.ac.uk (Simon Pickering)
Date: Fri, 9 Apr 2004 14:06:50 +0100
Subject: [R] Cross-compiling packages for (ARM) Zaurus ideas and tips?
Message-ID: <000001c41e33$860cfce0$8449268a@bath.ac.uk>

Hello All,

A bit of a long one (background mainly), this is really just a request
for some ideas.

I ported R 1.7.x for use on the Sharp Zaurus (& iPAQ, simpad, yopy,
etc.) some time ago and have just ported 1.8.1 but had never quite
realised that you actually need to install some packages to make the
system useful (I don't use R, I did it as a favour and of course for the
fun of it).

I use a GCC3.x cross compiler running on an x86 Linux box to do the
compilation. Packaging programs after they have been cross-compiled is a
pain in general (unfortunately you can't just run 'make install') but
normally it only has to be done once. However for the packages it's a
different matter, especially as I don't have enough space to host
compiled packages for everyone to access and in any case there'll always
be one that I've missed, etc. 

I'm thinking about creating a small native GCC for the Z (there are
already some out there, but they need to be GCC 3.x and need g77 neither
of which they currently have). My biggest concern here is that it'll use
quite a lot of space just to be able to create R packages occasionally.

The other method is for people to use a desktop cross compiler to
compile and package the resulting libraries. To this end I've been
looking at the INSTALL script in the hope of modifying it so that it
will compile and then create a .ipk package file (subset of debian .deb
file) which is the standard for the Zaurus. 

I've modified the INSTALL script quite heavily - to try and remove
extraneous env vars and reduce the number of external scripts which are
called, and to create the directory structure in a temp directory so I
can package it up. After battling with sed (steep learning curve there)
I temporarily gave up this morning trying to work out why shlib.mk can't
be found (despite, probably because of, my alterations). What do people
think, is this the best way to go about doing this? Would I be better
starting from scratch? Has anyone tried making a cross-compiler package
handling script before?

I actually thought that the INSTALL script would be fairly simple
(having seen the output when I INSTALLed a package on my desktop box)
but it doesn't seem quite so simple while I'm wading through it. My gut
feeling is that it probably is quite simple to do what I want (which is
only a very small subset of what the INSTALL script can do) and it's
just that there's so much extra in there. I was really just after a
quick (and dirty?) script to get up and running as soon as possible. I
presume the INSTALL script has developed from a more simple one? If so
are there any old (simple) copies lying about anywhere or has the
process changed?

Sorry for the rambling,

Cheers,


Simon

--------------------------------------------
Simon Pickering MEng
Research Officer
Dept. of Engineering and Applied Science
University of Bath
Bath, BA2 7AY, UK

Tel: +44 (0)1225 383314
Fax: +44 (0)1225 386928



From bxc at steno.dk  Fri Apr  9 16:39:22 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 9 Apr 2004 16:39:22 +0200
Subject: [R] Question (on matrix indexes)
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E54F@exdkba022.novo.dk>

I am not sure what you mean but you might be interested in the functions
row() and col().

Bendix Carstensen

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------





> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of "Ivan Yegorov" 
> Sent: Friday, April 09, 2004 1:56 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Question
> 
> 
> Could you please help me. How can I get indexes of matrix 
> elements equal to the specified value. Thanks in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE 
> do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ririzarr at jhsph.edu  Fri Apr  9 17:17:33 2004
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Fri, 9 Apr 2004 11:17:33 -0400 (EDT)
Subject: [R] loess' robustness weights in loess
Message-ID: <Pine.LNX.4.44.0404091112530.1379-100000@blinux08.biostat.jhsph.edu>

hi!

i want to change the "robustness weights" used by loess. these 
are described on page 316 of chambers and hastie's "statistical models in S" 
book as

r_i = B(e_i,6m)

where B is tukey's biweight function, e_i are the residulas, and m is the 
median average distance from 0 of the residuals. i want to 
change 6m to, say, 3m. 

is there a way to do this? i cant figure it out from the help files.

thanks,
rafael



From ripley at stats.ox.ac.uk  Fri Apr  9 17:52:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 16:52:17 +0100 (BST)
Subject: [R] loess' robustness weights in loess
In-Reply-To: <Pine.LNX.4.44.0404091112530.1379-100000@blinux08.biostat.jhsph.edu>
Message-ID: <Pine.LNX.4.44.0404091642370.12866-100000@gannet.stats>

On Fri, 9 Apr 2004, Rafael A. Irizarry wrote:

> hi!
> 
> i want to change the "robustness weights" used by loess. these 
> are described on page 316 of chambers and hastie's "statistical models in S" 
> book as
> 
> r_i = B(e_i,6m)
> 
> where B is tukey's biweight function, e_i are the residulas, and m is the 
> median average distance from 0 of the residuals. i want to 
> change 6m to, say, 3m. 
> 
> is there a way to do this? i cant figure it out from the help files.

Well, they say loess in R is an interface to C/Fortran code, and not the
same code as the S code described in Chambers & Hastie.  I translated the 
C driver routines to R for some added flexibility.

At a quick look, in function simpleLoess() you will find the weights in
object `robust', calculated by Fortran function lowesw.  You could replace
that by a call to an R-level alternative, or play with the Fortran source
code.  You'll have to do what I did way back, and read the source code to
see how it works in detail.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rxg218 at psu.edu  Fri Apr  9 18:03:12 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 09 Apr 2004 12:03:12 -0400
Subject: [R] question regarding vector ops
Message-ID: <1081526592.8110.28.camel@ra.chem.psu.edu>

Hi,
  I was playing with some code and came upon this situation.

> x <- c(1, rep(0,9))
> x
 [1] 1 0 0 0 0 0 0 0 0 0
> idx <- 1:10
> x[idx] <- x[idx] + 1
> x
 [1] 2 1 1 1 1 1 1 1 1 1

This is expected. But if I do:

> x <- c(1, rep(0,9))
> x
 [1] 1 0 0 0 0 0 0 0 0 0
> idx <- rep(0,10)
> idx
 [1] 0 0 0 0 0 0 0 0 0 0
> x[idx] <- x[idx] +1
> x
 [1] 1 0 0 0 0 0 0 0 0 0

I was expoecting that when all the elements of idx are set to 0,
x[0] would become 11.

Could somebody explain why this behavior occurs?

Thanks,


-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
A computer lets you make more mistakes faster than any other invention,
with the possible exceptions of handguns and Tequilla.
-- Mitch Ratcliffe



From ligges at statistik.uni-dortmund.de  Fri Apr  9 18:23:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Apr 2004 18:23:47 +0200
Subject: [R] question regarding vector ops
In-Reply-To: <1081526592.8110.28.camel@ra.chem.psu.edu>
References: <1081526592.8110.28.camel@ra.chem.psu.edu>
Message-ID: <4076CE13.3040609@statistik.uni-dortmund.de>

Rajarshi Guha wrote:

> Hi,
>   I was playing with some code and came upon this situation.
> 
> 
>>x <- c(1, rep(0,9))
>>x
> 
>  [1] 1 0 0 0 0 0 0 0 0 0
> 
>>idx <- 1:10
>>x[idx] <- x[idx] + 1
>>x
> 
>  [1] 2 1 1 1 1 1 1 1 1 1
> 
> This is expected. But if I do:
> 
> 
>>x <- c(1, rep(0,9))
>>x
> 
>  [1] 1 0 0 0 0 0 0 0 0 0
> 
>>idx <- rep(0,10)
>>idx
> 
>  [1] 0 0 0 0 0 0 0 0 0 0
> 
>>x[idx] <- x[idx] +1
>>x
> 
>  [1] 1 0 0 0 0 0 0 0 0 0
> 
> I was expoecting that when all the elements of idx are set to 0,
> x[0] would become 11.
> 
> Could somebody explain why this behavior occurs?


Why do you expect anything to become 11? And what is x[0] in your 
expectation?


x[0] is "empty" (indexing is starting from 1 in R) and returns 
numeric(0), hence
    x[0] <- anything
consequently does nothing ... (alternatively, one might expect an error 
message).


Uwe Ligges




> Thanks,
> 
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> A computer lets you make more mistakes faster than any other invention,
> with the possible exceptions of handguns and Tequilla.
> -- Mitch Ratcliffe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rajarshi at presidency.com  Fri Apr  9 18:33:44 2004
From: rajarshi at presidency.com (Rajarshi Guha)
Date: Fri, 09 Apr 2004 12:33:44 -0400
Subject: [R] question regarding vector ops
In-Reply-To: <1081526592.8110.28.camel@ra.chem.psu.edu>
References: <1081526592.8110.28.camel@ra.chem.psu.edu>
Message-ID: <1081528424.8638.10.camel@ra.chem.psu.edu>

On Fri, 2004-04-09 at 12:03, Rajarshi Guha wrote:
> Hi,
>   I was playing with some code and came upon this situation.
> 
> > x <- c(1, rep(0,9))
> > x
>  [1] 1 0 0 0 0 0 0 0 0 0
> > idx <- 1:10
> > x[idx] <- x[idx] + 1
> > x
>  [1] 2 1 1 1 1 1 1 1 1 1
> 
> This is expected. But if I do:
> 
> > x <- c(1, rep(0,9))
> > x
>  [1] 1 0 0 0 0 0 0 0 0 0
> > idx <- rep(0,10)
> > idx
>  [1] 0 0 0 0 0 0 0 0 0 0
> > x[idx] <- x[idx] +1
> > x
>  [1] 1 0 0 0 0 0 0 0 0 0

Aah, I realize that the second code snippet is not supposed to work.
What I meant was:

> x <- c(1, rep(0,9))
> x
 [1] 1 0 0 0 0 0 0 0 0 0
> idx <- rep(1,10)
>idx
 [1] 1 1 1 1 1 1 1 1 1 1
> x[idx] <- x[idx] + 1
 [1] 2 0 0 0 0 0 0 0 0 0

I expected that x would be

[1] 11 0 0 0 0 0 0 0 0 0

Or am I thinking about it wrong?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Disembowelling takes guts.



From ligges at statistik.uni-dortmund.de  Fri Apr  9 18:58:40 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 09 Apr 2004 18:58:40 +0200
Subject: [R] question regarding vector ops
In-Reply-To: <1081528424.8638.10.camel@ra.chem.psu.edu>
References: <1081526592.8110.28.camel@ra.chem.psu.edu>
	<1081528424.8638.10.camel@ra.chem.psu.edu>
Message-ID: <4076D640.5060201@statistik.uni-dortmund.de>

Rajarshi Guha wrote:
> On Fri, 2004-04-09 at 12:03, Rajarshi Guha wrote:
> 
>>Hi,
>>  I was playing with some code and came upon this situation.
>>
>>
>>>x <- c(1, rep(0,9))
>>>x
>>
>> [1] 1 0 0 0 0 0 0 0 0 0
>>
>>>idx <- 1:10
>>>x[idx] <- x[idx] + 1
>>>x
>>
>> [1] 2 1 1 1 1 1 1 1 1 1
>>
>>This is expected. But if I do:
>>
>>
>>>x <- c(1, rep(0,9))
>>>x
>>
>> [1] 1 0 0 0 0 0 0 0 0 0
>>
>>>idx <- rep(0,10)
>>>idx
>>
>> [1] 0 0 0 0 0 0 0 0 0 0
>>
>>>x[idx] <- x[idx] +1
>>>x
>>
>> [1] 1 0 0 0 0 0 0 0 0 0
> 
> 
> Aah, I realize that the second code snippet is not supposed to work.
> What I meant was:
> 
> 
>>x <- c(1, rep(0,9))
>>x
> 
>  [1] 1 0 0 0 0 0 0 0 0 0
> 
>>idx <- rep(1,10)
>>idx
> 
>  [1] 1 1 1 1 1 1 1 1 1 1
> 
>>x[idx] <- x[idx] + 1
> 
>  [1] 2 0 0 0 0 0 0 0 0 0
> 
> I expected that x would be
> 
> [1] 11 0 0 0 0 0 0 0 0 0
> 
> Or am I thinking about it wrong?

Yes, you are wrong: You are indexing 10 times the first value. Writing 
10 times the same value 10 times into the same location does not sum it up.

Consider
  x[idx] <- 1:10
Then you are writing 1 into x[1], 2 into x[1], 3 into x[1], ... and 
finally 10 into x[1], hence at the end x[1] is 10.

Uwe Ligges





> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Disembowelling takes guts.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From zhuw at mail.smu.edu  Fri Apr  9 19:30:34 2004
From: zhuw at mail.smu.edu (zhu wang)
Date: 09 Apr 2004 12:30:34 -0500
Subject: [R] missing values for mda package
In-Reply-To: <4072E0BD.609F0C19@statistik.uni-dortmund.de>
References: <200404061000.i36A0NaK014813@hypatia.math.ethz.ch>
	<1081266866.2853.6.camel@zwang.stat.smu.edu>
	<4072E0BD.609F0C19@statistik.uni-dortmund.de>
Message-ID: <1081531833.1972.11.camel@zwang.stat.smu.edu>

Thanks. I was able to use na.omit to remove NAs. But it seems to me this
kills one of the advantages of the original algorithm for handling
missing values.

On Tue, 2004-04-06 at 11:54, Uwe Ligges wrote:
> zhu wang wrote:
> > 
> > Dear helpers,
> > 
> > I am trying to use the mda package downloaded from the R website, but
> > the data set has missing values so I got an error message. Should I
> > manually handle these missing values? I was trying to read the documents
> > to specify any option related to missing values, but I did not find it.
> > Please forgive me if I ignore something obvious.
> 
> If it is not documented (hence probably not available) and you don't
> know how to tell the functions to handle missing values, try to do it
> "yourself". ?NA suggests:
> "See Also: [...] 'na.action', 'na.omit', 'na.fail' on how methods can be
> tuned to deal with missing values."
> 
> Uwe Ligges
> 
> 
> 
> > Thanks,
> > 
> > Zhu Wang
> > 
> > Statistical Science Department
> > Southern Methodist University
> > Dallas, TX 75275-0332
> > Phone: (214)768-2453
> > Fax: (214)768-4035
> > Email: zhuw at mail.smu.edu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Dallas, TX 75275-0332
Phone: (214)768-2453
Fax: (214)768-4035
Email: zhuw at mail.smu.edu



From tpapp at axelero.hu  Fri Apr  9 19:42:27 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 9 Apr 2004 19:42:27 +0200
Subject: [R] rectangles without borders
Message-ID: <20040409174227.GA1003@localhost>

Hi,

I want to draw rectangles with "rect" without borders, but with
shading (ie setting density).  ?rect says

     lty: line type for borders; defaults to '"solid"'.

but if I set it to 0, the border disappears, but the shading also
vanishes!  But the above says that lty is the line type for BORDERS,
and does not mention density at all -- how do I get around this?

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From ripley at stats.ox.ac.uk  Fri Apr  9 19:53:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 18:53:29 +0100 (BST)
Subject: [R] missing values for mda package
In-Reply-To: <1081531833.1972.11.camel@zwang.stat.smu.edu>
Message-ID: <Pine.LNX.4.44.0404091849300.12958-100000@gannet.stats>

Package mda covers many things, including bruto, mars, polyreg and mda
itself.  Which `the original algorithm' for which option did you have in
mind?  More concretely, what where you trying to do with the package?

Given that the package is the original authors' own code, it seems
unlikely that they `killed one of the advantages' of their methodology, so 
elucidation is sorely needed.

On 9 Apr 2004, zhu wang wrote:

> Thanks. I was able to use na.omit to remove NAs. But it seems to me this
> kills one of the advantages of the original algorithm for handling
> missing values.
> 
> On Tue, 2004-04-06 at 11:54, Uwe Ligges wrote:
> > zhu wang wrote:
> > > 
> > > Dear helpers,
> > > 
> > > I am trying to use the mda package downloaded from the R website, but
> > > the data set has missing values so I got an error message. Should I
> > > manually handle these missing values? I was trying to read the documents
> > > to specify any option related to missing values, but I did not find it.
> > > Please forgive me if I ignore something obvious.
> > 
> > If it is not documented (hence probably not available) and you don't
> > know how to tell the functions to handle missing values, try to do it
> > "yourself". ?NA suggests:
> > "See Also: [...] 'na.action', 'na.omit', 'na.fail' on how methods can be
> > tuned to deal with missing values."
> > 
> > Uwe Ligges
> > 
> > 
> > 
> > > Thanks,
> > > 
> > > Zhu Wang
> > > 
> > > Statistical Science Department
> > > Southern Methodist University
> > > Dallas, TX 75275-0332
> > > Phone: (214)768-2453
> > > Fax: (214)768-4035
> > > Email: zhuw at mail.smu.edu
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr  9 20:01:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 19:01:31 +0100 (BST)
Subject: [R] rectangles without borders
In-Reply-To: <20040409174227.GA1003@localhost>
Message-ID: <Pine.LNX.4.44.0404091855360.12958-100000@gannet.stats>

The person who added shading did not update the help page for rect ....

Try border=FALSE (and look at ?polygon for more details as to why).


On Fri, 9 Apr 2004, Tamas Papp wrote:

> Hi,
> 
> I want to draw rectangles with "rect" without borders, but with
> shading (ie setting density).  ?rect says
> 
>      lty: line type for borders; defaults to '"solid"'.
> 
> but if I set it to 0, the border disappears, but the shading also
> vanishes!  But the above says that lty is the line type for BORDERS,
> and does not mention density at all -- how do I get around this?
> 
> Thanks,
> 
> Tamas
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zhuw at mail.smu.edu  Fri Apr  9 20:50:26 2004
From: zhuw at mail.smu.edu (Wang, Zhu)
Date: Fri, 9 Apr 2004 13:50:26 -0500
Subject: [R] missing values for mda package
Message-ID: <4FB6E9BD986F194EA932E1F135CDF63630297E@s31xs3>

I basically wanted to use MARS to reproduce results using the dataset "Marketing" in the following book 

http://www-stat-class.stanford.edu/~tibs/ElemStatLearn/

The authors actually provided S-Plus functions for mars, bruto ,etc. I used all default options of mars in R but there was an error due to NAs and I could not find any option to handle missing values. 

Zhu Wang 

-----Original Message-----
From:	Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent:	Fri 4/9/2004 12:53 PM
To:	Wang, Zhu
Cc:	Uwe Ligges; r-help at stat.math.ethz.ch; Kurt.Hornik at R-project.org
Subject:	Re: [R] missing values for mda package
Package mda covers many things, including bruto, mars, polyreg and mda
itself.  Which `the original algorithm' for which option did you have in
mind?  More concretely, what where you trying to do with the package?

Given that the package is the original authors' own code, it seems
unlikely that they `killed one of the advantages' of their methodology, so 
elucidation is sorely needed.

On 9 Apr 2004, zhu wang wrote:

> Thanks. I was able to use na.omit to remove NAs. But it seems to me this
> kills one of the advantages of the original algorithm for handling
> missing values.
> 
> On Tue, 2004-04-06 at 11:54, Uwe Ligges wrote:
> > zhu wang wrote:
> > > 
> > > Dear helpers,
> > > 
> > > I am trying to use the mda package downloaded from the R website, but
> > > the data set has missing values so I got an error message. Should I
> > > manually handle these missing values? I was trying to read the documents
> > > to specify any option related to missing values, but I did not find it.
> > > Please forgive me if I ignore something obvious.
> > 
> > If it is not documented (hence probably not available) and you don't
> > know how to tell the functions to handle missing values, try to do it
> > "yourself". ?NA suggests:
> > "See Also: [...] 'na.action', 'na.omit', 'na.fail' on how methods can be
> > tuned to deal with missing values."
> > 
> > Uwe Ligges
> > 
> > 
> > 
> > > Thanks,
> > > 
> > > Zhu Wang
> > > 
> > > Statistical Science Department
> > > Southern Methodist University
> > > Dallas, TX 75275-0332
> > > Phone: (214)768-2453
> > > Fax: (214)768-4035
> > > Email: zhuw at mail.smu.edu
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr  9 21:16:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 9 Apr 2004 20:16:23 +0100 (BST)
Subject: [R] missing values for MARS (was mda package)
In-Reply-To: <4FB6E9BD986F194EA932E1F135CDF63630297E@s31xs3>
Message-ID: <Pine.LNX.4.44.0404092008280.13176-100000@gannet.stats>

On Fri, 9 Apr 2004, Wang, Zhu wrote:

> I basically wanted to use MARS to reproduce results using the dataset
> "Marketing" in the following book
> 
> http://www-stat-class.stanford.edu/~tibs/ElemStatLearn/
> 
> The authors actually provided S-Plus functions for mars, bruto ,etc. I
> used all default options of mars in R but there was an error due to NAs
> and I could not find any option to handle missing values.

Friedman originated MARS and has code for it.  The code in mda by 
Hastie/Tibshirani is different, and the code on that website is a direct 
ancestor of the mda package for R.  I see no option in the code for mars 
there to handle missing values, so you would do better to ask the authors 
how they did it (if you really believe they have such an option).

And PLEASE read the posting guide and try to learn to ask precise
questions with enough background information!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From peg_j_romansky at groton.pfizer.com  Fri Apr  9 22:20:33 2004
From: peg_j_romansky at groton.pfizer.com (Romansky, Peg J)
Date: Fri, 9 Apr 2004 16:20:33 -0400 
Subject: [R] Pfizer - Open position for Statistician - emphasis
	computational statistics
Message-ID: <56FFAEDA2CFAD411802900805FA7890A0A193A9D@groexmbcr15.pfizer.com>

All:

The Nonclinical Statistics Group at Pfizer Inc., New London, CT, is
currently seeking a Statistician with an interest and experience in 
computational statistics.

Please use the following link for more information and application
information: 
If Interested please apply on-line at www.pfizer.com/are/careers/index.html
<http://www.pfizer.com/are/careers/index.html> and search for Job#
29Mar0428519.




Peg Romansky
PGRD
Staffing - Development
A2148 - NL
860-732-9675





LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From John.Fieberg at dnr.state.mn.us  Fri Apr  9 22:16:34 2004
From: John.Fieberg at dnr.state.mn.us (John Fieberg)
Date: Fri, 09 Apr 2004 15:16:34 -0500
Subject: [R] Regression models w/ splines
Message-ID: <s076bed6.007@co5.dnr.state.mn.us>

Hi - I am fitting various Cox PH models with spline predictors.  After
fitting the model, I would like to use termplot() to examine the
functional form of the fitted model (e.g., to obtain a plot of the
relative risk (or log r.r.) versus the predictors). 

When there is only 1 predictor in the model, termplot returns a "?". 
In this case, I have not been able to figure out how to create the
desired plot.  Otherwise,termplot is easy to use. 

Example:
library(survival)
library(splines)

data(pbc)

fit1<-coxph(Surv(time,status)~ns(bili,4)+ns(age,4), data=pbc)
fit2<-coxph(Surv(time,status)~ns(bili,4), data=pbc)

par(mfrow=c(2,2))
termplot(fit1) # returns two plots (one for bili and one for age)
termplot(fit2) # returns a "?" prompt

Does anyone have any suggestions for obtaining a plot of the log
relative risk as a function of bili in the second example (or for this
type of model in general)?

On a similar note, I would like to be able to obtain predicted values
for "new" subjects.  I tried using:

temp<-data.frame(bili=0.8)
predict(fit2,newdata=temp, safe=T)

This gives the following error message:
Error in qr(t(const)) : NA/NaN/Inf in foreign function call (arg 1)

Is there a simple method available for obtaining predicted values for
new subjects when using these types of models?

I have encountered similar problems when trying to use smoothing
splines w/ the above examples (i.e., 'pspline' in the survival package).
 I am currently using R version 1.8.1 for windows. Any help would be
greatly appreciated!

John

John Fieberg, Ph.D.
Wildlife Biometrician, MN DNR
5463-C W. Broadway
Forest Lake, MN 55434
Phone: (651) 296-2704



From p.dalgaard at biostat.ku.dk  Fri Apr  9 22:55:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Apr 2004 22:55:02 +0200
Subject: [R] Regression models w/ splines
In-Reply-To: <s076bed6.007@co5.dnr.state.mn.us>
References: <s076bed6.007@co5.dnr.state.mn.us>
Message-ID: <x2r7uwrhw9.fsf@biostat.ku.dk>

"John Fieberg" <John.Fieberg at dnr.state.mn.us> writes:

> Hi - I am fitting various Cox PH models with spline predictors.  After
> fitting the model, I would like to use termplot() to examine the
> functional form of the fitted model (e.g., to obtain a plot of the
> relative risk (or log r.r.) versus the predictors). 
> 
> When there is only 1 predictor in the model, termplot returns a "?". 
> In this case, I have not been able to figure out how to create the
> desired plot.  Otherwise,termplot is easy to use. 

Kasper Hansen and Per Jensen from my department reported this
recently, AFAIR with a suggested fix. You might want to go over the
archives for the last month or so.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From statmkg at optonline.net  Sat Apr 10 01:53:10 2004
From: statmkg at optonline.net (MKG)
Date: Fri, 09 Apr 2004 19:53:10 -0400
Subject: [R] von Mises
Message-ID: <40773766.6020208@optonline.net>

Dear All,

I am trying to plot von Mises density on
the circle. One can use dvm function from the
CircStats package, by giving a set of angles,
mu and kappa to plot the circular density
on the line. Does any one have a macro that
does it on the circle? These plots are displayed
in Nick Fisher's book.

Any help will be appreciated.

Thanks,

G. Subramaniam



From feh3k at spamcop.net  Sat Apr 10 00:05:49 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 9 Apr 2004 18:05:49 -0400
Subject: [R] Regression models w/ splines
In-Reply-To: <s076bed6.007@co5.dnr.state.mn.us>
References: <s076bed6.007@co5.dnr.state.mn.us>
Message-ID: <20040409180549.57c92491.feh3k@spamcop.net>

On Fri, 09 Apr 2004 15:16:34 -0500
"John Fieberg" <John.Fieberg at dnr.state.mn.us> wrote:

> Hi - I am fitting various Cox PH models with spline predictors.  After
> fitting the model, I would like to use termplot() to examine the
> functional form of the fitted model (e.g., to obtain a plot of the
> relative risk (or log r.r.) versus the predictors). 

This kind of application is why the Design package was created.  It is
very easy to obtain such plots:

library(Design)
dd <- datadist(mydata); options(datadist='dd')
f <- cph(Surv(y,d) ~ sex*rcs(age,4)+rcs(bp,5))
plot(f, age=NA, sex='female')

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From patrick.giraudoux at univ-fcomte.fr  Sat Apr 10 04:17:04 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 10 Apr 2004 04:17:04 +0200
Subject: [R] adding text in (pair) panels (splom)
Message-ID: <001e01c41ea2$01b6d0f0$6fb0fb51@PC728329681112>

Hi,

I would like to add text in each panel got from the following:

splom(~Cs5bis,
panel=function(x,y,...){
panel.xyplot(x,y,...)
panel.abline(lm(y~x),...)
})

This could be a correlation coefficient or the statistical significance of the correlation or both, or a star if p < 0.05, etc...
The function text() is useless (probably because I cannot pass appropriates coordinates in each panel). Googling the R-archives, I
have found only one hint of Andy Liaw relating a rather complex (to me) script within a home-made panel.myfitline() function.

Is there a simple way to add text in panels created eg with:

panel=function(x,y,...){
panel.xyplot(x,y,...)
panel.abline(lm(y~x),...)
})

?

Any hint appreciated,

Patrick Giraudoux



From p.murrell at auckland.ac.nz  Sat Apr 10 04:39:26 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Sat, 10 Apr 2004 14:39:26 +1200
Subject: [R] adding text in (pair) panels (splom)
References: <001e01c41ea2$01b6d0f0$6fb0fb51@PC728329681112>
Message-ID: <40775E5E.5060509@stat.auckland.ac.nz>

Hi


Patrick Giraudoux wrote:
> Hi,
> 
> I would like to add text in each panel got from the following:
> 
> splom(~Cs5bis,
> panel=function(x,y,...){
> panel.xyplot(x,y,...)
> panel.abline(lm(y~x),...)
> })
> 
> This could be a correlation coefficient or the statistical significance of the correlation or both, or a star if p < 0.05, etc...
> The function text() is useless (probably because I cannot pass appropriates coordinates in each panel). Googling the R-archives, I
> have found only one hint of Andy Liaw relating a rather complex (to me) script within a home-made panel.myfitline() function.
> 
> Is there a simple way to add text in panels created eg with:
> 
> panel=function(x,y,...){
> panel.xyplot(x,y,...)
> panel.abline(lm(y~x),...)
> })


text() is useless because splom is a lattice function and lattice is 
based on grid.  To add text in a panel you could use lattice's ltext(), 
but I think the best way would be to use grid.text() as below.  This 
example draws the correlation coefficient 1mm in from the top-left 
corner of the panel.


	panel=function(x,y,...){
	  panel.xyplot(x,y,...)
	  panel.abline(lm(y~x),...)
           grid.text(round(cor(x, y), 2),
             x=unit(1, "mm"), y=unit(1, "npc") - unit(1, "mm"),
             just=c("left", "top"))
	})

Does that do what you want?

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From patrick.giraudoux at univ-fcomte.fr  Sat Apr 10 05:13:39 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 10 Apr 2004 05:13:39 +0200
Subject: [R] adding text in (pair) panels (splom)
References: <001e01c41ea2$01b6d0f0$6fb0fb51@PC728329681112>
	<40775E5E.5060509@stat.auckland.ac.nz>
Message-ID: <006301c41ea9$d3eae730$6fb0fb51@PC728329681112>

OK Paul and Kjetil

I understand that grid.text is part of the library grid. I have tried with Paul's arguments. It works perfectly well.

Thanks a lot both,

Patrick


----- Original Message ----- 
From: "Paul Murrell" <p.murrell at auckland.ac.nz>
To: "Patrick Giraudoux" <patrick.giraudoux at univ-fcomte.fr>
Cc: "r-help" <r-help at stat.math.ethz.ch>
Sent: Saturday, April 10, 2004 4:39 AM
Subject: Re: [R] adding text in (pair) panels (splom)


> Hi
>
>
> Patrick Giraudoux wrote:
> > Hi,
> >
> > I would like to add text in each panel got from the following:
> >
> > splom(~Cs5bis,
> > panel=function(x,y,...){
> > panel.xyplot(x,y,...)
> > panel.abline(lm(y~x),...)
> > })
> >
> > This could be a correlation coefficient or the statistical significance of the correlation or both, or a star if p < 0.05,
etc...
> > The function text() is useless (probably because I cannot pass appropriates coordinates in each panel). Googling the R-archives,
I
> > have found only one hint of Andy Liaw relating a rather complex (to me) script within a home-made panel.myfitline() function.
> >
> > Is there a simple way to add text in panels created eg with:
> >
> > panel=function(x,y,...){
> > panel.xyplot(x,y,...)
> > panel.abline(lm(y~x),...)
> > })
>
>
> text() is useless because splom is a lattice function and lattice is
> based on grid.  To add text in a panel you could use lattice's ltext(),
> but I think the best way would be to use grid.text() as below.  This
> example draws the correlation coefficient 1mm in from the top-left
> corner of the panel.
>
>
> panel=function(x,y,...){
>   panel.xyplot(x,y,...)
>   panel.abline(lm(y~x),...)
>            grid.text(round(cor(x, y), 2),
>              x=unit(1, "mm"), y=unit(1, "npc") - unit(1, "mm"),
>              just=c("left", "top"))
> })
>
> Does that do what you want?
>
> Paul
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/



From trachidi at carbon.cudenver.edu  Sat Apr 10 05:51:48 2004
From: trachidi at carbon.cudenver.edu (Thami Rachidi)
Date: Fri, 9 Apr 2004 21:51:48 -0600
Subject: [R] Density Estimation
Message-ID: <000a01c41eaf$2d98b870$8a78c284@D26PL721>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040409/12310f97/attachment.pl

From k.wang at auckland.ac.nz  Sat Apr 10 05:54:22 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Sat, 10 Apr 2004 15:54:22 +1200
Subject: [R] Density Estimation
In-Reply-To: <000a01c41eaf$2d98b870$8a78c284@D26PL721>
Message-ID: <20040410035437.NGMU20540.web2-rme.xtra.co.nz@kevinlpt>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
>
> Dear Sir/Madam;
> Would you please tell me what is the command that allows the
> estimation of the Kernel Density for some data.
> Thanks,

?density



From ripley at stats.ox.ac.uk  Sat Apr 10 08:58:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Apr 2004 07:58:24 +0100 (BST)
Subject: [R] Density Estimation
In-Reply-To: <20040410035437.NGMU20540.web2-rme.xtra.co.nz@kevinlpt>
Message-ID: <Pine.LNX.4.44.0404100755050.13952-100000@gannet.stats>

help.search("kernel density") reports

KernSec(GenKern)        Univariate kernel density estimate
KernSur(GenKern)        Bivariate kernel density estimation
bkde(KernSmooth)        Compute a Binned Kernel Density Estimate
bkde2D(KernSmooth)      Compute a 2D Binned Kernel Density Estimate
dpik(KernSmooth)        Select a Bandwidth for Kernel Density
                        Estimation
kde2d(MASS)             Two-Dimensional Kernel Density Estimation

amongst others, and package sm also has a user-friendly selection.

So, apart from point out alternatives I wanted to point out how easy it 
was to find the information originally requested.


On Sat, 10 Apr 2004, Ko-Kang Kevin Wang wrote:

> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> >
> > Dear Sir/Madam;
> > Would you please tell me what is the command that allows the
> > estimation of the Kernel Density for some data.
> > Thanks,
> 
> ?density

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mihastaut at hotmail.com  Sat Apr 10 09:25:43 2004
From: mihastaut at hotmail.com (Miha STAUT)
Date: Sat, 10 Apr 2004 07:25:43 +0000
Subject: [R] Automation of c()
Message-ID: <BAY2-F92Z14eBa1kg3A000492ce@hotmail.com>

Hi,

I have around 300 files with the x, y and z coordinates of a DEM that I 
would like to join in a single data frame object. I know how to automate the 
import of the 300 data frames.

in Bash
ls > names

in R
names<-scan(names...)
With rep() and data.frame() construct a series of read.table() commands, 
which you write to a file and execute them with source().

I do not know however how to automate the combination of the e.g. x vectors 
from all imported data frames in a single vector avoiding the manual writing 
of all the imported data frame names.

Thanks in advance, Miha



From pallier at lscp.ehess.fr  Sat Apr 10 10:19:56 2004
From: pallier at lscp.ehess.fr (pallier)
Date: Sat, 10 Apr 2004 10:19:56 +0200
Subject: [R] Automation of c()
In-Reply-To: <BAY2-F92Z14eBa1kg3A000492ce@hotmail.com>
References: <BAY2-F92Z14eBa1kg3A000492ce@hotmail.com>
Message-ID: <4077AE2C.5000005@lscp.ehess.fr>


 > I have around 300 files with the x, y and z coordinates of a DEM that 
I would like to join in a
 > single data frame object. I know how to automate the import of the 
300 data frames.

 >
 > I do not know however how to automate the combination of the e.g. x 
vectors from all
 > imported data frames in a single vector avoiding the manual writing 
of all the imported
 > data frame names.

Why not concatenate all the data files under Bash?

cat *.dat >alldata.txt

You can then read 'alldata.txt' with a single read.table

Or, if you need to keep the file names in the data frame:

awk '{print FILENAME,$0}' *.dat >alldata.txt

Or am I missing something?

Christophe Pallier
www.pallier.org



From ligges at statistik.uni-dortmund.de  Sat Apr 10 11:39:52 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 10 Apr 2004 11:39:52 +0200
Subject: [R] Automation of c()
References: <BAY2-F92Z14eBa1kg3A000492ce@hotmail.com>
Message-ID: <4077C0E8.A3BD0164@statistik.uni-dortmund.de>



Miha STAUT wrote:
> 
> Hi,
> 
> I have around 300 files with the x, y and z coordinates of a DEM that I
> would like to join in a single data frame object. I know how to automate the
> import of the 300 data frames.
> 
> in Bash
> ls > names
> 
> in R
> names<-scan(names...)
> With rep() and data.frame() construct a series of read.table() commands,
> which you write to a file and execute them with source().

It's simpler, all can be done within R:

names <- list.files()
dat <- read.table(names[1])
for(i in names[-1])
    dat <- rbind(dat, read.table(i))

  
This example is quite ugly and maybe slow (I guess for huge data frames,
the solution presented by Christophe Pallier will be faster), in
particular the rbind() calls. If all data.frames have the same nrow(),
one can improve quite easily...

Uwe Ligges

 
> I do not know however how to automate the combination of the e.g. x vectors
> from all imported data frames in a single vector avoiding the manual writing
> of all the imported data frame names.
> 
> Thanks in advance, Miha



From hodgess at gator.uhd.edu  Sat Apr 10 12:34:52 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Sat, 10 Apr 2004 05:34:52 -0500
Subject: [R] all poss. regression
Message-ID: <200404101034.i3AAYqR23415@gator.dt.uh.edu>

Dear R People:

Is there an "all possible subsets" function in regression, please?

Typically, I use the step function.  A student asked me
about the all possibles.

Thanks in advance!

R 1.8.1 Windows

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ripley at stats.ox.ac.uk  Sat Apr 10 12:55:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Apr 2004 11:55:47 +0100 (BST)
Subject: [R] all poss. regression
In-Reply-To: <200404101034.i3AAYqR23415@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.44.0404101150400.14780-100000@gannet.stats>

Look at package leaps.  Note that it applies to columns of the 
design matrix and not terms, so can be nonsensical with categorical 
predictors (let alone interactions of such).

On Sat, 10 Apr 2004, Erin Hodgess wrote:

> Dear R People:
> 
> Is there an "all possible subsets" function in regression, please?
> 
> Typically, I use the step function.  A student asked me
> about the all possibles.
> 
> Thanks in advance!
> 
> R 1.8.1 Windows

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tpapp at axelero.hu  Sat Apr 10 13:09:18 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sat, 10 Apr 2004 13:09:18 +0200
Subject: [R] (offtopic) I need two sets of 5 different color scales
Message-ID: <20040410110918.GA2148@localhost>

Hi,

I am plotting a policy function (result from a dynamic stochastic
optimization problem, discretized approximation).  The policy function
maps from an 2 x 2 x 2 x 3 x B x F state space to a B x F state space
(B and F are usually between 4-6, and represent domestic and foreign
savings.  The other variables are income (Y), inflation (Pi), domestic
and foreign interest rates (R and Z)).  I actually wrote a plotting
function to represent all this, the result is attached -- please have
a look at it and help me...

I need advice in the following: I need two sets of colors for B and F
which are easy to distinguish (when printed on a color laser printer),
represent cardinality (ie have an intuitive mapping to an interval) or
at least ordinality.

I have experimented with the following:

Bcolors <- hsv(.6, seq(0.2, 1, length=5), 1)
Fcolors <- hsv(seq(.1,0, length=5), seq(0.2, 1, length=5)

this is what you see in the plot.  What colors would you use?  Do you
think that varying both brightness and hue helps to distinguish
colors?  Should I change saturation, too?

Thanks,

Tamas

PS.: The plot is simply gzipped.  If you need a zipped version, or the
source code, contact me.

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.

From tpapp at axelero.hu  Sat Apr 10 13:20:05 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sat, 10 Apr 2004 13:20:05 +0200
Subject: The list engine removed the .eps.gz attachment [Re: [R] (offtopic) I
	need two sets of 5 different color scales]
In-Reply-To: <20040410110918.GA2148@localhost>
References: <20040410110918.GA2148@localhost>
Message-ID: <20040410112005.GA2313@localhost>

On Sat, Apr 10, 2004 at 01:09:18PM +0200, Tamas Papp wrote:

> PS.: The plot is simply gzipped.  If you need a zipped version, or the
> source code, contact me.

It appears to be removed by the list engine.

Is it possible to send eps files to the list?  I gzipped it because it
was 90k originally, but 8k compressed.  What should I do, send the
uncompressed version?

The posting guide says: "No binary attachments except for PS, PDF, and
some image and archive formats", that means no gzipped versions of
these?

Thanks

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From ligges at statistik.uni-dortmund.de  Sat Apr 10 14:55:25 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 10 Apr 2004 14:55:25 +0200
Subject: [R] (offtopic) I need two sets of 5 different color scales
In-Reply-To: <20040410110918.GA2148@localhost>
References: <20040410110918.GA2148@localhost>
Message-ID: <4077EEBD.8040702@statistik.uni-dortmund.de>

Tamas Papp wrote:
> Hi,
> 
> I am plotting a policy function (result from a dynamic stochastic
> optimization problem, discretized approximation).  The policy function
> maps from an 2 x 2 x 2 x 3 x B x F state space to a B x F state space
> (B and F are usually between 4-6, and represent domestic and foreign
> savings.  The other variables are income (Y), inflation (Pi), domestic
> and foreign interest rates (R and Z)).  I actually wrote a plotting
> function to represent all this, the result is attached -- please have
> a look at it and help me...
> 
> I need advice in the following: I need two sets of colors for B and F
> which are easy to distinguish (when printed on a color laser printer),
> represent cardinality (ie have an intuitive mapping to an interval) or
> at least ordinality.
> 
> I have experimented with the following:
> 
> Bcolors <- hsv(.6, seq(0.2, 1, length=5), 1)
> Fcolors <- hsv(seq(.1,0, length=5), seq(0.2, 1, length=5)
> 
> this is what you see in the plot.  What colors would you use?  Do you
> think that varying both brightness and hue helps to distinguish
> colors?  Should I change saturation, too?

Hmm, Ross Ihaka has given a talk at the DSC2003 conference about 
constructing sensible colors. Unfortunately, there is no paper available 
at http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/ ...
Some notes and a color package are available at 
http://www.stat.auckland.ac.nz/~ihaka/color/

Also, you might want to look at the package RColorBrewer available at 
CRAN. The package is supposed to provide palettes of sensible colors.

Uwe Ligges


PS: If you want to send any attachments: Preferably upload to a web site 
and just post the link.



> Thanks,
> 
> Tamas
> 
> PS.: The plot is simply gzipped.  If you need a zipped version, or the
> source code, contact me.
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luke at stat.uiowa.edu  Sat Apr 10 15:34:09 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Sat, 10 Apr 2004 08:34:09 -0500 (CDT)
Subject: [R] looking for a tutorial on exception handling
In-Reply-To: <20040409121552.GA1202@localhost>
Message-ID: <Pine.LNX.4.44.0404100833260.18223-100000@itasca2.stat.uiowa.edu>

Unfortunately there isn't yet.  Writing a paper on this is fairly high
on my list of priorities but hasn't happened yet.

Best,

luke

On Fri, 9 Apr 2004, Tamas Papp wrote:

> Hi,
> 
> I would like to learn how to do exception handling in R.  I had a look
> at the help page for tryCatch and I am trying to understand it (I am
> sure it is well-written, but I have not used exception handling before
> in any language, so I have no experience in that area).
> 
> Is there a tutorial or an introduction into exception handling in R
> with tryCatch & friends?  Preferably with lots of examples.  It would
> be nice if there was one online, but references to books would also be
> appreciated.
> 
> Thanks,
> 
> Tamas
> 
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ggrothendieck at myway.com  Sat Apr 10 16:09:06 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 10 Apr 2004 14:09:06 +0000 (UTC)
Subject: [R] Automation of c()
References: <BAY2-F92Z14eBa1kg3A000492ce@hotmail.com>
Message-ID: <loom.20040410T155956-201@post.gmane.org>

Assuming your files are in /*.txt:

   setwd("/")
   names <- dir( patt="[.]txt$" )
   do.call( "rbind", lapply( names, read.table ) )


Miha STAUT <mihastaut <at> hotmail.com> writes:

: 
: Hi,
: 
: I have around 300 files with the x, y and z coordinates of a DEM that I 
: would like to join in a single data frame object. I know how to automate the 
: import of the 300 data frames.
: 
: in Bash
: ls > names
: 
: in R
: names<-scan(names...)
: With rep() and data.frame() construct a series of read.table() commands, 
: which you write to a file and execute them with source().
: 
: I do not know however how to automate the combination of the e.g. x vectors 
: from all imported data frames in a single vector avoiding the manual writing 
: of all the imported data frame names.
: 
: Thanks in advance, Miha
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From Matthias.Kohl at uni-bayreuth.de  Sat Apr 10 17:24:45 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Sat, 10 Apr 2004 17:24:45 +0200 (MEST)
Subject: [R] (offtopic) I need two sets of 5 different color scales
In-Reply-To: <20040410110918.GA2148@localhost>
References: <20040410110918.GA2148@localhost>
Message-ID: <58194.132.180.7.27.1081610685.squirrel@mail.uni-bayreuth.de>

maybe, the RColorBrewer package does what you want?
see also: ColorBrewer.org

> Hi,
>
> I am plotting a policy function (result from a dynamic stochastic
> optimization problem, discretized approximation).  The policy function
> maps from an 2 x 2 x 2 x 3 x B x F state space to a B x F state space (B
> and F are usually between 4-6, and represent domestic and foreign
> savings.  The other variables are income (Y), inflation (Pi), domestic
> and foreign interest rates (R and Z)).  I actually wrote a plotting
> function to represent all this, the result is attached -- please have a
> look at it and help me...
>
> I need advice in the following: I need two sets of colors for B and F
> which are easy to distinguish (when printed on a color laser printer),
> represent cardinality (ie have an intuitive mapping to an interval) or
> at least ordinality.
>
> I have experimented with the following:
>
> Bcolors <- hsv(.6, seq(0.2, 1, length=5), 1)
> Fcolors <- hsv(seq(.1,0, length=5), seq(0.2, 1, length=5)
>
> this is what you see in the plot.  What colors would you use?  Do you
> think that varying both brightness and hue helps to distinguish
> colors?  Should I change saturation, too?
>
> Thanks,
>
> Tamas
>
> PS.: The plot is simply gzipped.  If you need a zipped version, or the
> source code, contact me.
>
> --
> Tam??s K. Papp
> E-mail: tpapp at axelero.hu
> Please try to send only (latin-2) plain text, not HTML or other garbage.



From hastie at stanford.edu  Sat Apr 10 19:00:03 2004
From: hastie at stanford.edu (Trevor Hastie)
Date: Sat, 10 Apr 2004 10:00:03 -0700
Subject: [R] Re: missing values for mda package 
Message-ID: <001101c41f1d$44a3e1c0$d5bb42ab@yacht>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040410/5e56b655/attachment.pl

From xiaoliu at jhmi.edu  Sat Apr 10 20:21:41 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Sat, 10 Apr 2004 14:21:41 -0400
Subject: [R] confidential interval of correlation coefficient using bootstrap
Message-ID: <9bcce59bf05f.9bf05f9bcce5@jhmimail.jhmi.edu>

I tried 2 methods to estimate C.I. of correlation coefficient of variables x and y:

> x <- c(44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 60.1)
> y <- c( 2.6,  3.1,  2.5,  5.0,  3.6,  4.0,  5.2,  2.8,  3.8)

#METHOD 1: Pearson's
**********************************************************
> cor.test(x, y, method = "pearson", conf.level = 0.95)

        Pearson's product-moment correlation

data:  x and y 
t = 1.8411, df = 7, p-value = 0.1082
alternative hypothesis: true correlation is not equal to 0 
95 percent confidence interval:
 -0.1497426  0.8955795 
sample estimates:
      cor 
0.5711816 
***********************************************************

#METHOD 2: bootstrap
***********************************************************
> boot(cbind(x,y), cor, 200)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = cbind(x, y), statistic = cor, R = 200)


Bootstrap Statistics :
     original     bias    std. error
t1* 0.5429120 -0.5232893   0.3799117
t2* 0.3832856 -0.3779026   0.3876666
************************************************************

'cor.test' gave the value of cor as 0.5711816.  Why did 'boot' give two original cors: t1 and t2.  And why does none of t1 and t2 equal to what 'cor.test' gave.

Thank you very much

Xiao



From ripley at stats.ox.ac.uk  Sat Apr 10 22:35:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 10 Apr 2004 21:35:56 +0100 (BST)
Subject: [R] confidential interval of correlation coefficient using
	bootstrap
In-Reply-To: <9bcce59bf05f.9bf05f9bcce5@jhmimail.jhmi.edu>
Message-ID: <Pine.LNX.4.44.0404102119460.15395-100000@gannet.stats>

You are not using boot() correctly.  Please do try reading the help page, 
which says

statistic: ...

  In all other cases 'statistic' must take at least two arguments.  The
  first argument passed will always be the original data. The second will
  be a vector of indices ...

so you need a function of two args like

> f <- function(data, i) cor(data[i, 1], data[i, 2])
> boot(cbind(x,y), f, 200)

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = cbind(x, y), statistic = f, R = 200)


Bootstrap Statistics :
     original     bias    std. error
t1* 0.5711816 0.05765583   0.2178101


However, once you have done that you need a way to get a confidence 
interval, and you will find ?abc.ci has an example for you, using 
correlations.


On Sat, 10 Apr 2004, XIAO LIU wrote:

> I tried 2 methods to estimate C.I. of correlation coefficient of variables x and y:
> 
> > x <- c(44.4, 45.9, 41.9, 53.3, 44.7, 44.1, 50.7, 45.2, 60.1)
> > y <- c( 2.6,  3.1,  2.5,  5.0,  3.6,  4.0,  5.2,  2.8,  3.8)
> 
> #METHOD 1: Pearson's
> **********************************************************
> > cor.test(x, y, method = "pearson", conf.level = 0.95)
> 
>         Pearson's product-moment correlation
> 
> data:  x and y 
> t = 1.8411, df = 7, p-value = 0.1082
> alternative hypothesis: true correlation is not equal to 0 
> 95 percent confidence interval:
>  -0.1497426  0.8955795 
> sample estimates:
>       cor 
> 0.5711816 
> ***********************************************************
> 
> #METHOD 2: bootstrap
> ***********************************************************
> > boot(cbind(x,y), cor, 200)
> 
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> 
> Call:
> boot(data = cbind(x, y), statistic = cor, R = 200)
> 
> 
> Bootstrap Statistics :
>      original     bias    std. error
> t1* 0.5429120 -0.5232893   0.3799117
> t2* 0.3832856 -0.3779026   0.3876666
> ************************************************************
> 
> 'cor.test' gave the value of cor as 0.5711816.  Why did 'boot' give two original cors: t1 and t2.  And why does none of t1 and t2 equal to what 'cor.test' gave.
> 
> Thank you very much
> 
> Xiao

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From scott.rifkin at yale.edu  Sun Apr 11 05:18:33 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Sat, 10 Apr 2004 23:18:33 -0400 (EDT)
Subject: [R] nlme: creating pdMats
Message-ID: <Pine.LNX.4.44.0404102300410.18611-100000@argos.its.yale.edu>

I have an experiment with an interaction (VT) between Varieties and Time.  
There are 12 Varieties and 2 Times (Early and Late), and Time is a fixed 
affect.

I have been estimating a single variance from this interaction using 
pdIdent(~VT-1).

I would like to test 2 more complicated models.

1) Estimate 2 variances, one for Variety*Time(Early) and one for 
Variety*Time(Late), again with a diagonal matrix.

2) The above with a covariance term between the Times across Varieties.  
To do this, I think I would want a variance covariance matrix like (1) but
with one more parameter wherever a row and a column share a Variety but 
differ in Times.  The rest would be zero.

I would appreciate if someone would explain how I can do this using the 
standard pdMat classes, or, more likely, how to create a new one to do 
this.

Thanks much,
Scott Rifkin



From scott.rifkin at yale.edu  Sun Apr 11 05:29:57 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Sat, 10 Apr 2004 23:29:57 -0400 (EDT)
Subject: [R] fixed and random effects question
Message-ID: <Pine.LNX.4.44.0404102318500.18611-100000@argos.its.yale.edu>

On page 146 of Pinheiro & Bates, they mention models  where every fixed 
effect has an associated random effect. What does it mean for a fixed 
effect to have an associated random effect?

Thanks for any clarification,
Scott Rifkin



From ripley at stats.ox.ac.uk  Sun Apr 11 09:38:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 11 Apr 2004 08:38:02 +0100 (BST)
Subject: [R] fixed and random effects question
In-Reply-To: <Pine.LNX.4.44.0404102318500.18611-100000@argos.its.yale.edu>
Message-ID: <Pine.LNX.4.44.0404110834130.23774-100000@gannet.stats>

Consider the case of a response vs time for multiple measurements on 
subjects.  If one has both fixed and random effect for slope and 
intercept, it means there is a separate linear relationship with time for 
each subject (the random effect) and that the population average may have 
a non-zero slope and intercept (the fixed effect).

On Sat, 10 Apr 2004, Scott Rifkin wrote:

> On page 146 of Pinheiro & Bates, they mention models  where every fixed 
> effect has an associated random effect. What does it mean for a fixed 
> effect to have an associated random effect?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From graham.smith at myotis.co.uk  Sun Apr 11 16:34:00 2004
From: graham.smith at myotis.co.uk (Graham M Smith)
Date: Sun, 11 Apr 2004 15:34 +0100 (BST)
Subject: [R] pasting results into Word/Excel
Message-ID: <memo.20040411153448.57069B@myotis.v21>

Is there some clever way of pasting results from R into Excel or Word, as 
tab limited format so they are easy to turn into a formatted table.

Or is there some other way of doing this to avoid the time spent 
reformatting the output for presentation.

If different, I am also interested in an answer to the same question but 
using S-Plus.

Many thanks,

Graham



From p.dalgaard at biostat.ku.dk  Sun Apr 11 17:03:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Apr 2004 17:03:18 +0200
Subject: [R] pasting results into Word/Excel
In-Reply-To: <memo.20040411153448.57069B@myotis.v21>
References: <memo.20040411153448.57069B@myotis.v21>
Message-ID: <x28yh2tv49.fsf@biostat.ku.dk>

graham.smith at myotis.co.uk (Graham M Smith) writes:

> Is there some clever way of pasting results from R into Excel or Word, as 
> tab limited format so they are easy to turn into a formatted table.

Have a look at write.table().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From anielsen at math.ku.dk  Sun Apr 11 17:08:36 2004
From: anielsen at math.ku.dk (Anders Nielsen)
Date: Sun, 11 Apr 2004 17:08:36 +0200 (CEST)
Subject: [R] pasting results into Word/Excel
In-Reply-To: <memo.20040411153448.57069B@myotis.v21>
Message-ID: <Pine.LNX.4.40.0404111703160.23887-100000@shannon.math.ku.dk>

Hi Graham,

If you want it in excel I suggest you export it as a comma separated
file, which can be done with something like:

> data(airquality)
> write.table(airquality, sep=",", row.names=FALSE, quote=FALSE, file="result.csv")

This file will open nicely in excel.

If you want it as tab separated use the same, but with 'sep="\t"'.

Cheers,

Anders.

On Sun, 11 Apr 2004, Graham M Smith wrote:

> Is there some clever way of pasting results from R into Excel or Word, as
> tab limited format so they are easy to turn into a formatted table.
>
> Or is there some other way of doing this to avoid the time spent
> reformatting the output for presentation.
>
> If different, I am also interested in an answer to the same question but
> using S-Plus.
>
> Many thanks,
>
> Graham
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ligges at statistik.uni-dortmund.de  Sun Apr 11 17:11:29 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 11 Apr 2004 17:11:29 +0200
Subject: [R] pasting results into Word/Excel
In-Reply-To: <memo.20040411153448.57069B@myotis.v21>
References: <memo.20040411153448.57069B@myotis.v21>
Message-ID: <40796021.1090304@statistik.uni-dortmund.de>

Graham M Smith wrote:
> Is there some clever way of pasting results from R into Excel or Word, as 
> tab limited format so they are easy to turn into a formatted table.
> 
> Or is there some other way of doing this to avoid the time spent 
> reformatting the output for presentation.

To copy/paste your data.frame X to Excel, use the following lines to 
write it to the clipboard from R:

    con <- file("clipboard", open="w")
    write.table(x, file=con, sep="\t", row.names=FALSE, col.names=FALSE)
    close(con)

(you might want to define a wrapper function around it ...)




> If different, I am also interested in an answer to the same question but 
> using S-Plus.

What about copying from its spreadsheet?

Uwe Ligges



> Many thanks,
> 
> Graham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From graham.smith at myotis.co.uk  Sun Apr 11 19:52:00 2004
From: graham.smith at myotis.co.uk (Graham M Smith)
Date: Sun, 11 Apr 2004 18:52 +0100 (BST)
Subject: [R] pasting results into Word/Excel
In-Reply-To: <Pine.LNX.4.40.0404111703160.23887-100000@shannon.math.ku.dk>
Message-ID: <memo.20040411185211.56301E@myotis.v21>

Anders,

This seems a good start, and I will try it get it to work, on of the 
issues is that the results output is more complex than simply split by 
comma eg a typical Summary output of my current data set (below)

However, I am a lot further forward than I was a few hours ago.

Thanks

   Section         Cave        Season         Year     BatUse 
 Black  :225   ELE    : 48   Autumn:101   1995   :217   0:233  
 Central:276   BRO    : 47   Spring:169   1996   :149   1:450  
 Iron   : 49   OCA    : 46   Summer:101   1998   : 61          
 West   :133   OGE    : 46   Winter:312   1999   : 56          
               WAT    : 46                1997   : 50          
               OCF1   : 45                1994   : 49          
               (Other):405                (Other):101          
     Number             L.H.        
 Min.   : 0.0000   Min.   : 0.0000  
 1st Qu.: 0.0000   1st Qu.: 0.0000  
 Median : 0.0000   Median : 0.0000  
 Mean   : 0.9634   Mean   : 0.9239  
 3rd Qu.: 1.0000   3rd Qu.: 1.0000  
 Max.   :23.0000   Max.   :23.0000



From graham.smith at myotis.co.uk  Sun Apr 11 19:52:00 2004
From: graham.smith at myotis.co.uk (Graham M Smith)
Date: Sun, 11 Apr 2004 18:52 +0100 (BST)
Subject: [R] pasting results into Word/Excel
In-Reply-To: <40796021.1090304@statistik.uni-dortmund.de>
Message-ID: <memo.20040411185207.56301C@myotis.v21>

Uwe,

> To copy/paste your data.frame X to Excel, use the following lines to 
> write it to the clipboard from R:


Thanks, this is certainly a step in the right direction, it doesn't work 
as it stands but it has given me a start

> > If different, I am also interested in an answer to the same question 
> > but using S-Plus.
> 
> What about copying from its spreadsheet?

Because, unless I am missing something the results don't ned up in a 
spread sheet.

Thanks for your help.

Graham



From ggrothendieck at myway.com  Sun Apr 11 20:04:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 11 Apr 2004 18:04:36 +0000 (UTC)
Subject: [R] pasting results into Word/Excel
References: <memo.20040411153448.57069B@myotis.v21>
Message-ID: <loom.20040411T193846-344@post.gmane.org>

Graham M Smith <graham.smith <at> myotis.co.uk> writes:

: Is there some clever way of pasting results from R into Excel or Word, as 
: tab limited format so they are easy to turn into a formatted table.


Try this:

   data(iris)   # get a test data frame
   require(R2HTML)
   HTML( iris, file("clipboard","w"), append=F )   

Now go to Excel and press ctrl-V to paste it in.
Once its in Excel you can copy and paste from there to Word.



From martin at ist.org  Sun Apr 11 21:35:00 2004
From: martin at ist.org (Martin Keller-Ressel)
Date: Sun, 11 Apr 2004 21:35:00 +0200
Subject: [R] Intercept in lasso models
Message-ID: <opr6a4omv5jigwsf@mail.ist.org>

Hello,

Im using the function l1ce (L1-constrained estimation) from the lasso2 
package.
when I try the example from the help pages

> data(Prostate)
> l1c.P <- l1ce(lpsa ~ ., Prostate, bound=.5)
> coef(l1c.P)
   (Intercept)       lcavol      lweight          age         lbph          
svi
0.7284810757 0.4936540169 0.2681863403 0.0000000000 0.0092825881 
0.4550584943
           lcp      gleason        pgg45
0.0000000000 0.0000000000 0.0001812107


everything looks fine to me. But when I use

> l1c.P <- l1ce(lpsa ~ ., Prostate, bound=(1:20)/20)
> coef(l1c.P)[10,]
    (Intercept)        lcavol       lweight           age          lbph
-4.9496587740  0.4936540169  0.2681863403  0.0000000000  0.0092825881
            svi           lcp       gleason         pgg45
   0.4550584943  0.0000000000  0.0000000000  0.0001812107

the Intercept is different !! Also

> coef(l1c.P)[20,]
    (Intercept)        lcavol       lweight           age          lbph
-23.678040866   0.587022881   0.454460641  -0.019637208   0.107054351
            svi           lcp       gleason         pgg45
    0.766155885  -0.105473570   0.045135964   0.004525324


should be the same as the unconstrained linear model:

> coef(lm(lpsa ~ ., data = Prostate))
   (Intercept)       lcavol      lweight          age         lbph          
svi
   0.669399027  0.587022881  0.454460641 -0.019637208  0.107054351  
0.766155885
           lcp      gleason        pgg45
-0.105473570  0.045135964  0.004525324

which is true except for the Intercept...

What is going on here? Can anybody enlighten me?

Thanks,


Martin Keller-Ressel








--



From ggrothendieck at myway.com  Sun Apr 11 21:28:56 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 11 Apr 2004 19:28:56 +0000 (UTC)
Subject: [R] pasting results into Word/Excel
References: <memo.20040411153448.57069B@myotis.v21>
	<loom.20040411T193846-344@post.gmane.org>
Message-ID: <loom.20040411T211820-204@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

> 
> Graham M Smith <graham.smith <at> myotis.co.uk> writes:
> 
> : Is there some clever way of pasting results from R into Excel or Word, as 
> : tab limited format so they are easy to turn into a formatted table.
> 
> Try this:
> 
>    data(iris)   # get a test data frame
>    require(R2HTML)
>    HTML( iris, file("clipboard","w"), append=F )   
> 
> Now go to Excel and press ctrl-V to paste it in.
> Once its in Excel you can copy and paste from there to Word.

I noticed that your response to other respondents indicate that you
want to display a summary.  Try something like this with the above
approach:

   HTML.matrix( summary(iris), file("clipboard", "w"), append=F )
   # paste into Excel


In Excel, with the table selected, you may wish to 
- set the cell width (Format | Column | Width) wider, say 12, and 
- right justify the matrix (click on right justify icon in toolbar)



From lindaportman at yahoo.com  Sun Apr 11 21:51:42 2004
From: lindaportman at yahoo.com (Linda portman)
Date: Sun, 11 Apr 2004 12:51:42 -0700 (PDT)
Subject: [R] question on plot dates
Message-ID: <20040411195142.70105.qmail@web61305.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040411/b7d6420d/attachment.pl

From xiaoliu at jhmi.edu  Sun Apr 11 22:00:51 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Sun, 11 Apr 2004 16:00:51 -0400
Subject: [R] Killed
Message-ID: <9c2b6e9c1a6d.9c1a6d9c2b6e@jhmimail.jhmi.edu>

I tried bootstrap on a sample of 13,000 observations:

It works fine when R = 200:
>boot(data, cor.i, R = 200)

However, when R = 400, I got:
>boot(data, cor.i, R = 400)
Killed

Any suggestions/ideas? 

Thank you very much

Xiao



From graham.smith at myotis.co.uk  Sun Apr 11 22:18:00 2004
From: graham.smith at myotis.co.uk (Graham M Smith)
Date: Sun, 11 Apr 2004 21:18 +0100 (BST)
Subject: [R] pasting results into Word/Excel
In-Reply-To: <loom.20040411T211820-204@post.gmane.org>
Message-ID: <memo.20040411211846.15847C@myotis.v21>

Gabor,

> > : Is there some clever way of pasting results from R into Excel or 

> > Try this:
> > 
> >    data(iris)   # get a test data frame
> >    require(R2HTML)
> >    HTML( iris, file("clipboard","w"), append=F )   

This worked exactly as I was hoping.

Many thanks,

Graham



From ripley at stats.ox.ac.uk  Sun Apr 11 22:32:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 11 Apr 2004 21:32:59 +0100 (BST)
Subject: [R] question on plot dates
In-Reply-To: <20040411195142.70105.qmail@web61305.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0404112130010.24869-100000@gannet.stats>

On Sun, 11 Apr 2004, Linda portman wrote:

> I am trying to plot "time" variable (the time is recoded for three days at 5 minutes interval in the format of 2001-05-14 13:45:00)  VS. blood pressure. My code is the following: 
>  
>  
> plot(time, bloodpressure,xlab="Time",ylab="bPress",main="Time VS Blood Pressure", type="l", xaxt="n")
>  r <- as.POSIXct(round(range(Time), "days"))
>  axis.POSIXct(1, at=seq(r[1], r[2], by="day"), format="%d-%b")

>  However the plot only shows three values 14-Apr, 15-Apr and 16 Apr, I
> would like to have 24 hours marked on the x axis for each day. How can I
> do it?  Thanks a lot!

You asked for exactly values by day, so is it so surprising that you got 
what you asked for?  (That's the point of at vs x.)  Try by="hour" with a 
suitable format.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Apr 11 22:37:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 11 Apr 2004 21:37:59 +0100 (BST)
Subject: [R] Killed
In-Reply-To: <9c2b6e9c1a6d.9c1a6d9c2b6e@jhmimail.jhmi.edu>
Message-ID: <Pine.LNX.4.44.0404112133120.24869-100000@gannet.stats>

On Sun, 11 Apr 2004, XIAO LIU wrote:

> I tried bootstrap on a sample of 13,000 observations:
> 
> It works fine when R = 200:
> >boot(data, cor.i, R = 200)
> 
> However, when R = 400, I got:
> >boot(data, cor.i, R = 400)
> Killed
> 
> Any suggestions/ideas? 

Read the posting guide and tell us at least what OS this is ....

I don't think R will produce such a message (the string "Killed" does not
appear in the R sources), hence it is probably an operating system message
(possibly caused by running out of some resource?).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Sun Apr 11 23:10:38 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 11 Apr 2004 17:10:38 -0400
Subject: [R] converting lme commands from S-PLUS to R
Message-ID: <4079B44E.1080101@jhsph.edu>

I'm trying to do some smoothing with lme and am having some difficulty 
bringing commands over from S-PLUS to R.  I have the following setup 
(modified from Ngo and Wand, 2004):

set.seed(1)
x <- runif(200)
y <- sin(3*pi*x) + rnorm(200)*.4
## library(splines)
z <- ns(x, 4)

The following runs without error on S-PLUS

f <- lme(y ~ 1, random = pdIdent(~ -1 + z))

But in R I get

library(nlme)
 > f <- lme(y ~ 1, random = pdIdent(~ -1 + z))
Error in getGroups.data.frame(dataMix, groups) :
         Invalid formula for groups

Does the S-PLUS lme have some default setting that R doesn't?  Any help 
would be appreciated.

Thanks,

-roger



From andy_liaw at merck.com  Sun Apr 11 23:15:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 11 Apr 2004 17:15:00 -0400
Subject: [R] Automation of c()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B7D@usrymx25.merck.com>

If the structure of the files are known beforehand, things can be done in
more efficient manner.  Assuming the .txt files all contain 3 columns of
numbers, and without column headings, this is what I would do to read them
into R as one matrix:

[First, generate a test file to be read in 300 times.]

> set.seed(1)
> write(matrix(rnorm(3e3), 3, 1e3), file="xyz.txt", ncol=3)
> ntimes <- 300
> files <- rep("xyz.txt", ntimes)

[Now the test:]

> system.time(dat <- do.call("rbind", lapply(files, function(f)
matrix(scan(f), byrow=T, nc=3))))
[ "Read 3000 items" 300 times omitted... ]
[1] 7.59 0.27 8.29   NA   NA

[Compare this with Gabor's suggestion:]

> system.time(dat2 <- do.call("rbind", lapply(files, read.table, header=F)))
[1] 37.32  0.76 40.61    NA    NA
> all(dat == dat2)
[1] TRUE

Most of the time spent in Gabor's approach is on rbind.data.frame(), because
even if we specify colClasses in read.table(), it doesn't get much faster:

> system.time(dat3 <- do.call("rbind", lapply(files, read.table, header=F,
colClass=rep("numeric", 3))))
[1] 36.48  0.75 38.81    NA    NA

If you don't need the data in a data frame, stick with a matrix.  If you
must have a data frame, it only takes another 4.5 seconds to run
as.data.frame() on the matrix.

Cheers,
Andy


> From: Gabor Grothendieck
> 
> Assuming your files are in /*.txt:
> 
>    setwd("/")
>    names <- dir( patt="[.]txt$" )
>    do.call( "rbind", lapply( names, read.table ) )
> 
> 
> Miha STAUT <mihastaut <at> hotmail.com> writes:
> 
> : 
> : Hi,
> : 
> : I have around 300 files with the x, y and z coordinates of 
> a DEM that I 
> : would like to join in a single data frame object. I know 
> how to automate the 
> : import of the 300 data frames.
> : 
> : in Bash
> : ls > names
> : 
> : in R
> : names<-scan(names...)
> : With rep() and data.frame() construct a series of 
> read.table() commands, 
> : which you write to a file and execute them with source().
> : 
> : I do not know however how to automate the combination of 
> the e.g. x vectors 
> : from all imported data frames in a single vector avoiding 
> the manual writing 
> : of all the imported data frame names.
> : 
> : Thanks in advance, Miha
> : 
> : ______________________________________________
> : R-help <at> stat.math.ethz.ch mailing list
> : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> : 
> :
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lindaportman at yahoo.com  Mon Apr 12 00:52:30 2004
From: lindaportman at yahoo.com (Linda portman)
Date: Sun, 11 Apr 2004 15:52:30 -0700 (PDT)
Subject: [R] plot question
Message-ID: <20040411225230.3630.qmail@web61302.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040411/ce3979b8/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Apr 12 00:53:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Apr 2004 00:53:49 +0200
Subject: [R] converting lme commands from S-PLUS to R
In-Reply-To: <4079B44E.1080101@jhsph.edu>
References: <4079B44E.1080101@jhsph.edu>
Message-ID: <x2zn9igm82.fsf@biostat.ku.dk>

"Roger D. Peng" <rpeng at jhsph.edu> writes:

> f <- lme(y ~ 1, random = pdIdent(~ -1 + z))
> 
> But in R I get
> 
> library(nlme)
>  > f <- lme(y ~ 1, random = pdIdent(~ -1 + z))
> Error in getGroups.data.frame(dataMix, groups) :
>          Invalid formula for groups
> 
> Does the S-PLUS lme have some default setting that R doesn't?  Any
> help would be appreciated.

Which version of S-PLUS? lme has changed and the newer versions insist
on having grouping levels for the random effects, so I think you're
looking for

> g <- rep(1,200)
> f <- lme(y ~ 1, random = list(g=pdIdent(~ z-1)))
> f
Linear mixed-effects model fit by REML
  Data: NULL
  Log-restricted-likelihood: -111.2290
  Fixed: y ~ 1
(Intercept)
  0.3568754

Random effects:
 Formula: ~z - 1 | g
 Structure: Multiple of an Identity
              z1       z2       z3       z4 Residual
StdDev: 1.623315 1.623315 1.623315 1.623315 0.398023

Number of Observations: 200
Number of Groups: 1
 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sdhyok at email.unc.edu  Mon Apr 12 02:14:00 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Sun, 11 Apr 2004 20:14:00 -0400
Subject: [R] Only referenc copy when calling C routine?
Message-ID: <OAEOKPIGCLDDHAEMCAKIKEIJCKAA.sdhyok@email.unc.edu>

What happens when I pass an array to a dynamically linked C routine?
Is only its reference copied when an array is passed and returned?
Or, is its whole content copied?

In R extension manual, I found the following description.
But, I can't know exactly which is true.

"There can be up to 65 further arguments giving R objects to be passed to
compiled code.
Normally these are copied before being passed in, and copied again to an R
list object when
the compiled code returns. If the arguments are given names, these are used
as names for
the components in the returned list object (but not passed to the compiled
code)."

Thanks in advance.

Daehyok Shin



From andy_liaw at merck.com  Mon Apr 12 02:29:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 11 Apr 2004 20:29:43 -0400
Subject: [R] Only referenc copy when calling C routine?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B7E@usrymx25.merck.com>

I thought that description is rather clear.  If `x' is a vector of doubles,
then

result <- .C("myCfunc", x = x)

makes a copy of `x' to be passed to `myfunc'.  The copy in R is not touched.
When `myfunc' returns, another copy is made and placed into result[["x"]].

There's the `DUP' argument in .C/.Fortran that controls whether the copying
is done.  See ?.C for detail.

HTH,
Andy

> Shin, Daehyok
> 
> What happens when I pass an array to a dynamically linked C routine?
> Is only its reference copied when an array is passed and returned?
> Or, is its whole content copied?
> 
> In R extension manual, I found the following description.
> But, I can't know exactly which is true.
> 
> "There can be up to 65 further arguments giving R objects to 
> be passed to
> compiled code.
> Normally these are copied before being passed in, and copied 
> again to an R
> list object when
> the compiled code returns. If the arguments are given names, 
> these are used
> as names for
> the components in the returned list object (but not passed to 
> the compiled
> code)."
> 
> Thanks in advance.
> 
> Daehyok Shin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdhyok at email.unc.edu  Mon Apr 12 02:45:13 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Sun, 11 Apr 2004 20:45:13 -0400
Subject: [R] Only referenc copy when calling C routine?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B7E@usrymx25.merck.com>
Message-ID: <OAEOKPIGCLDDHAEMCAKIAEILCKAA.sdhyok@email.unc.edu>

Thanks, Andy.
Let me ask a little more about the DUP argument.
In the manual, I can see that only reference copy happens when passing an
array,
if DUP=FALSE. Then, what happens when returning it in a list, if DUP=FALSE?
Another reference copy, or deep copy?

I am trying to implement a dynamic modeling system in R,
where some C functions should be called more than 1000 times with arguments
of huge arrays.
So, the issue is important for better performance.
Thanks again,

Daehyok Shin



From andy_liaw at merck.com  Mon Apr 12 03:25:13 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 11 Apr 2004 21:25:13 -0400
Subject: [R] Only referenc copy when calling C routine?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B7F@usrymx25.merck.com>

My understanding is that if DUP=FALSE, no copying is done in either
direction.

If you want to avoid copying, the .Call interface is probably more suitable,
as you pass the actual R objects to the C function.  I'm not familiar with
that, though.

Andy

> From: Shin, Daehyok [mailto:sdhyok at email.unc.edu] 
> 
> Thanks, Andy.
> Let me ask a little more about the DUP argument.
> In the manual, I can see that only reference copy happens 
> when passing an
> array,
> if DUP=FALSE. Then, what happens when returning it in a list, 
> if DUP=FALSE?
> Another reference copy, or deep copy?
> 
> I am trying to implement a dynamic modeling system in R,
> where some C functions should be called more than 1000 times 
> with arguments
> of huge arrays.
> So, the issue is important for better performance.
> Thanks again,
> 
> Daehyok Shin
> 
> 
>



From dmurdoch at pair.com  Mon Apr 12 03:35:25 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 11 Apr 2004 21:35:25 -0400
Subject: [R] Only referenc copy when calling C routine?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B7F@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7B7F@usrymx25.merck.com>
Message-ID: <bdsj70pr5dhantfkump91soaa22793fo7j@4ax.com>

On Sun, 11 Apr 2004 21:25:13 -0400, you wrote:

>My understanding is that if DUP=FALSE, no copying is done in either
>direction.
>
>If you want to avoid copying, the .Call interface is probably more suitable,
>as you pass the actual R objects to the C function.  I'm not familiar with
>that, though.

If you're using C, .Call (or .External) is not really very hard to
use.  You want to take a look at the examples in the Writing
Extensions manual, and probably in one or two simple packages.

The big limitation with .Call is that it is really only practical with
C or C++ (because you definitely need to use the R header files),
whereas .C and .Fortran make calls that just about any language can
handle.

Duncan Murdoch



From andy_liaw at merck.com  Mon Apr 12 03:43:58 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 11 Apr 2004 21:43:58 -0400
Subject: [R] Only referenc copy when calling C routine?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B81@usrymx25.merck.com>

For me it's just a matter of getting my hands dirty, I guess.  For whatever
reason I seem to have a mental block about the PROTECT/UNPROTECT business...

Best,
Andy

> From: Duncan Murdoch [mailto:dmurdoch at pair.com] 
> 
> On Sun, 11 Apr 2004 21:25:13 -0400, you wrote:
> 
> >My understanding is that if DUP=FALSE, no copying is done in either
> >direction.
> >
> >If you want to avoid copying, the .Call interface is 
> probably more suitable,
> >as you pass the actual R objects to the C function.  I'm not 
> familiar with
> >that, though.
> 
> If you're using C, .Call (or .External) is not really very hard to
> use.  You want to take a look at the examples in the Writing
> Extensions manual, and probably in one or two simple packages.
> 
> The big limitation with .Call is that it is really only practical with
> C or C++ (because you definitely need to use the R header files),
> whereas .C and .Fortran make calls that just about any language can
> handle.
> 
> Duncan Murdoch
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From sdhyok at email.unc.edu  Mon Apr 12 03:49:36 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Sun, 11 Apr 2004 21:49:36 -0400
Subject: [R] Only referenc copy when calling C routine?
In-Reply-To: <bdsj70pr5dhantfkump91soaa22793fo7j@4ax.com>
Message-ID: <OAEOKPIGCLDDHAEMCAKIEEIMCKAA.sdhyok@email.unc.edu>

Thanks, Murdoch.
The .Call is what I want.

In R extension manual,
"
Neither .Call nor.External copy their arguments. You should treat arguments
you receive through these
interfaces as read-only.
"

Daehyok Shin

> -----Original Message-----
> From: Duncan Murdoch [mailto:dmurdoch at pair.com]
> Sent: Sunday, April 11, 2004 PM 9:35
> To: Liaw, Andy
> Cc: 'sdhyok at email.unc.edu'; Liaw, Andy; R, Help
> Subject: Re: [R] Only referenc copy when calling C routine?
>
>
> On Sun, 11 Apr 2004 21:25:13 -0400, you wrote:
>
> >My understanding is that if DUP=FALSE, no copying is done in either
> >direction.
> >
> >If you want to avoid copying, the .Call interface is probably
> more suitable,
> >as you pass the actual R objects to the C function.  I'm not
> familiar with
> >that, though.
>
> If you're using C, .Call (or .External) is not really very hard to
> use.  You want to take a look at the examples in the Writing
> Extensions manual, and probably in one or two simple packages.
>
> The big limitation with .Call is that it is really only practical with
> C or C++ (because you definitely need to use the R header files),
> whereas .C and .Fortran make calls that just about any language can
> handle.
>
> Duncan Murdoch
>



From dmurdoch at pair.com  Mon Apr 12 03:54:57 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 11 Apr 2004 21:54:57 -0400
Subject: [R] Only referenc copy when calling C routine?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B81@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7B81@usrymx25.merck.com>
Message-ID: <hhtj70po7rcpoim6qfnti8u0c1eo6bnr1n@4ax.com>

On Sun, 11 Apr 2004 21:43:58 -0400, you wrote:

>For me it's just a matter of getting my hands dirty, I guess.  For whatever
>reason I seem to have a mental block about the PROTECT/UNPROTECT business...

Just protect every SEXP until you don't care if it goes away.  It
might be possible to skip some PROTECT calls, but PROTECT is fairly
cheap, and debugging a missing PROTECT is not!  

Duncan Murdoch



From agm at socrates.Berkeley.EDU  Mon Apr 12 04:15:08 2004
From: agm at socrates.Berkeley.EDU (Antonio Garcia)
Date: Sun, 11 Apr 2004 19:15:08 -0700 (PDT)
Subject: [R] Very large matrices for very large genome
Message-ID: <Pine.SOL.4.56.0404111908420.11624@socrates.Berkeley.EDU>


Hello,

I am using R to look at whole-genome gene expression data. This means
about 27,000 genes, each with a vector of numbers reflecting expression at
different tissues and times. I need to do an all against all co-expression
calculation (basically, just calculate Pearson's r for every gene-gene
pair). I try to store the result of such a thing in a 27000x27000 matrix,
but r seems not to like allocating such a large beast. Any
recommendations?

I also rather want to then manipulate the all-against-all interaction
data, treating it as a graph with edge weighted by the r^2, eliminating
some edges, maybe even finding network motifs like cliques, etc., so
having it as a manipulable object in r presents some advantages.
Alternatively, I could print it out maybe (don't even know how to do that)
and then write script code in another language to manipulate it.

All advice welcome.

A.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Antonio Garcia-Martinez
UC-Berkeley Physics/Joint Genome Institute
http://cryptologia.com

        .=$=.   .=$=.           .=$=.   .=$=.
@       @ | | | @ | | | @       @ | | | @ | | |
| @   @ | | | @   @ | | | @   @ | | | @   @ | |
| | @ | | | @       @ | | | @ | | | @       @ |
~'   `~$~'           `~$~'   `~$~'           `

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From dmurdoch at pair.com  Mon Apr 12 04:47:33 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 11 Apr 2004 22:47:33 -0400
Subject: [R] Very large matrices for very large genome
In-Reply-To: <Pine.SOL.4.56.0404111908420.11624@socrates.Berkeley.EDU>
References: <Pine.SOL.4.56.0404111908420.11624@socrates.Berkeley.EDU>
Message-ID: <ad0k70truse79idfinl3hs2n7a2c2j3gkt@4ax.com>

On Sun, 11 Apr 2004 19:15:08 -0700 (PDT), you wrote:

>Hello,
>
>I am using R to look at whole-genome gene expression data. This means
>about 27,000 genes, each with a vector of numbers reflecting expression at
>different tissues and times.

How long is that vector?  Presumably shorter than 27000.

>I need to do an all against all co-expression
>calculation (basically, just calculate Pearson's r for every gene-gene
>pair). I try to store the result of such a thing in a 27000x27000 matrix,
>but r seems not to like allocating such a large beast. Any
>recommendations?

If you have fewer than 27000 cases, then the correlation matrix is not
full rank, and could be summarized in much less space.  For example,
if you have 100 cases, then a 100x100 matrix will give the correlation
structure, and a 26900x100 matrix would give the weights for the rest
of the genes.

(It's late, so I might wrong about this, but I don't think so.)

To calculate those matrices, just pick the first 100 genes to use for
the correlation matrix (assuming you get a full rank matrix that way),
then regress each of the others onto those.

Duncan Murdoch



From john.ferguson at yale.edu  Mon Apr 12 05:37:22 2004
From: john.ferguson at yale.edu (john.ferguson@yale.edu)
Date: Sun, 11 Apr 2004 23:37:22 -0400
Subject: [R] missing values and survival analysis
Message-ID: <1081741042.407a0ef2d6bf6@www.mail.yale.edu>


Hi everyone,

I'm analysing a survival analysis data set at the moment with missing 
values in the covariate and survival vectors (I have about 60 
variables).  I know there are some functions on the CRAN network to 
deal with missing values in general multivariate data.  Does anybody 
know of any package that deals with missing data specifically in the 
context of survival analysis.  Any help would be greatly appreciated.

Thanks,

john.



From ajayshah at mayin.org  Thu Apr  8 17:54:30 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 8 Apr 2004 21:24:30 +0530
Subject: [R] Clever R syntax for extracting a subset of observations
Message-ID: <20040408155430.GA7720@igidr.ac.in>

I know that if:
   x = seq(1,10)
   d = c(7,3,2)
and if I say
   y = x[d]
then I get the vector y as (7,3,2). Very clever! This idea is used
intensively with the boot library.

Now consider the following code (which works):

  ---------------------------------------------------------------------------
  library(boot)

  sdratio <- function(D, d) {
    return(sd(D$x[d])/sd(D$y[d]))
  }

  x = runif(100)
  y = 2*runif(100)
  D = data.frame(x, y)

  b = boot(D, sdratio, R=1000)
  cat("Standard deviation of sdratio = ", sd(b$t[,1]), "\n")
  ---------------------------------------------------------------------------

Now it would be so elegant to say:

  sdratio <- function(D, d) {
    E = D[d]
    return(sd(E$x)/sd(E$y))
  }

But this doesn't work since if D is a data frame, you can't say
D[d]. Let me show you:

> x = runif(100)
> y = runif(100)
> D = data.frame(x, y)
> d = c(7,3,2)
> E = D[d] 
Error in "[.data.frame"(D, d) : undefined columns selected

Any suggestions on how one can do such pretty things as D[d] where D
is a data frame?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Thu Apr  8 17:56:19 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 8 Apr 2004 21:26:19 +0530
Subject: [R] ARMA models with ARCH errors?
Message-ID: <20040408155619.GB7720@igidr.ac.in>

In R, I see support for ARCH models and for ARMA models (in the
tseries package). How would we estimate the workhorse model where
stock returns are ARMA with ARCH errors?

I am aware of the paper by Andy Weiss. I have used this model quite a
bit using stata and consider it a staple. I couldn't find mention of
it in the tseries library.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From k.wang at auckland.ac.nz  Mon Apr 12 06:23:01 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 12 Apr 2004 16:23:01 +1200
Subject: [R] Clever R syntax for extracting a subset of observations
References: <20040408155430.GA7720@igidr.ac.in>
Message-ID: <008501c42045$d7bc2340$6633d882@stat.auckland.ac.nz>

Hi,
----- Original Message ----- 
From: "Ajay Shah" <ajayshah at mayin.org>
>
> But this doesn't work since if D is a data frame, you can't say
> D[d]. Let me show you:
>
> > x = runif(100)
> > y = runif(100)
> > D = data.frame(x, y)
> > d = c(7,3,2)
> > E = D[d]
> Error in "[.data.frame"(D, d) : undefined columns selected
>
> Any suggestions on how one can do such pretty things as D[d] where D
> is a data frame?

So I take it you want to select the 7th, 3rd, and 2nd observation (i.e.
rows) from D?

A data frame has two dimensions, so you need to specify the row and columns.
Maybe try:
  E = D[d,]

HTH,

Kevin Wang



From ggrothendieck at myway.com  Mon Apr 12 07:10:56 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 12 Apr 2004 05:10:56 +0000 (UTC)
Subject: [R] Clever R syntax for extracting a subset of observations
References: <20040408155430.GA7720@igidr.ac.in>
Message-ID: <loom.20040412T070624-75@post.gmane.org>


sdratio <- function(D, d) with( D[d,], sd(x)/sd(y) )



Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: I know that if:
:    x = seq(1,10)
:    d = c(7,3,2)
: and if I say
:    y = x[d]
: then I get the vector y as (7,3,2). Very clever! This idea is used
: intensively with the boot library.
: 
: Now consider the following code (which works):
: 
:   ---------------------------------------------------------------------------
:   library(boot)
: 
:   sdratio <- function(D, d) {
:     return(sd(D$x[d])/sd(D$y[d]))
:   }
: 
:   x = runif(100)
:   y = 2*runif(100)
:   D = data.frame(x, y)
: 
:   b = boot(D, sdratio, R=1000)
:   cat("Standard deviation of sdratio = ", sd(b$t[,1]), "\n")
:   ---------------------------------------------------------------------------
: 
: Now it would be so elegant to say:
: 
:   sdratio <- function(D, d) {
:     E = D[d]
:     return(sd(E$x)/sd(E$y))
:   }
: 
: But this doesn't work since if D is a data frame, you can't say
: D[d]. Let me show you:
: 
: > x = runif(100)
: > y = runif(100)
: > D = data.frame(x, y)
: > d = c(7,3,2)
: > E = D[d] 
: Error in "[.data.frame"(D, d) : undefined columns selected
: 
: Any suggestions on how one can do such pretty things as D[d] where D
: is a data frame?
:



From ggrothendieck at myway.com  Mon Apr 12 07:46:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 12 Apr 2004 05:46:40 +0000 (UTC)
Subject: [R] plot question
References: <20040411225230.3630.qmail@web61302.mail.yahoo.com>
Message-ID: <loom.20040412T074212-551@post.gmane.org>


plot(1:10)
abline(v=c(3,5,7), col="red")


Linda portman <lindaportman <at> yahoo.com> writes:

: 
: I have a variable named "Medicine" which has seven values in date format, on 
the following plot, how can I use
: a red line to indicate the time when the medicine was taken on x axis?
: 
: The following is my original plot of blood pressure vs. time. 
: plot(time, bloodpressure,xlab="Time",ylab="bPress",main="Time VS Blood 
Pressure", type="l", xaxt="n")
:  r <- as.POSIXct(round(range(Time), "days"))
:  axis.POSIXct(1, at=seq(r[1], r[2], by="day"), format="%d-%b")
: 
: Thanks a lot!



From feh3k at spamcop.net  Mon Apr 12 12:54:11 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Mon, 12 Apr 2004 06:54:11 -0400
Subject: [R] missing values and survival analysis
In-Reply-To: <1081741042.407a0ef2d6bf6@www.mail.yale.edu>
References: <1081741042.407a0ef2d6bf6@www.mail.yale.edu>
Message-ID: <20040412065411.676ad0ae.feh3k@spamcop.net>

On Sun, 11 Apr 2004 23:37:22 -0400
john.ferguson at yale.edu wrote:

> 
> Hi everyone,
> 
> I'm analysing a survival analysis data set at the moment with missing 
> values in the covariate and survival vectors (I have about 60 
> variables).  I know there are some functions on the CRAN network to 
> deal with missing values in general multivariate data.  Does anybody 
> know of any package that deals with missing data specifically in the 
> context of survival analysis.  Any help would be greatly appreciated.
> 
> Thanks,
> 
> john.

Consider using the aregImpute function in the Hmisc package with
right-censored survival times by predicting baseline covariates using the
follow-up time, event indicator/censoring, and the product of the two,
using multiple imputation.  I am not comfortable with imputing follow-up
time and event indicators from covariates though.  If the follow-up time
is completely missing you might consider discarding the observation.

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From erich.neuwirth at univie.ac.at  Mon Apr 12 13:06:43 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 12 Apr 2004 13:06:43 +0200
Subject: [R] Windows startup question
Message-ID: <407A7843.1040506@univie.ac.at>

I would like to be able to double click on an R source file
and by that start R and have the file sourced in R.
I am using WinXP Pro.

So I created an association for file type R and tye following
Run action:
"C:\Program Files\R\rw1090beta\bin\Rgui.exe" R_PROFILE=%1

I have an R file with the following lines at the beginning.

library(vcd)
votes<-array(0,10*7*9*2)
dim(votes)<-c(10,7,9,2)

Double clicking this file (in Windows Explorer) starts R and then 
produces the following output.

Attaching package 'vcd':


         The following object(s) are masked from package:base :

          print.summary.table summary.table

Error in dim(votes) <- c(10, 7, 9, 2) : cannot do complex assignments in 
base namespace

So it seems the first 2 lines are executed,
the vector votes exists, but cannot be redimensioned as an array.

Can i make what I would like fully functional?


-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From p.dalgaard at biostat.ku.dk  Mon Apr 12 14:05:25 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Apr 2004 14:05:25 +0200
Subject: [R] R 1.9.0 is release
Message-ID: <x2oepxl7ui.fsf@biostat.ku.dk>

I've rolled up R-1.9.0.tgz a short while ago. This is a new version
with a number of new features, most notably a substantial
reorganization of the standard packages, a major update of the grid
package, and the fact that underscore can now be used as a regular
character in variable names. See below for further changes.

Because of the disturbances at the machine that hosted the CVS
archives, you will now have to wait for the file to show up on the
CRAN master site. You can then get it from

http://cran.r-project.org/src/base/R-1.9.0.tgz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies.

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

79ae9d20a7bed94d7c92504c8c130310  R-1.9.0.tgz
de40f2e366d3ed9e2b3e8813fb8bba6c  R-1.9.0.tgz-split.aa
a963d3ac009ff6c5f321391e26cb0892  R-1.9.0.tgz-split.ab
0e8f55216051057dfafdabc5f578e68e  R-1.9.0.tgz-split.ac
7add0d9b6dbd8cdbc36608b7166b910b  R-1.9.0.tgz-split.ad
667222d63ce7e19bf869ea20439d0867  R-1.9.0.tgz-split.ae
42dcd15c23b07decb43a65f937942489  R-1.9.0.tgz-split.af
136279c8d59def0a5221e20ba763c997  R-1.9.0.tgz-split.ag

Here is the relevant bit of the NEWS file:


		CHANGES IN R VERSION 1.9.0


USER-VISIBLE CHANGES

    o	Underscore '_' is now allowed in syntactically valid names, and
	make.names() no longer changes underscores.  Very old code
	that makes use of underscore for assignment may now give
	confusing error messages.

    o	Package 'base' has been split into packages 'base', 'graphics',
	'stats' and 'utils'.  All four are loaded in a default
	installation, but the separation allows a 'lean and mean'
	version of R to be used for tasks such as building indices.

	Packages ctest, eda, modreg, mva, nls, stepfun and ts have been
	merged into stats, and lqs has been returned to MASS.  In all
	cases a stub has been left that will issue a warning and ensure
	that the appropriate new home is loaded.  All the time series
	datasets have been moved to package stats.  Sweave has been
	moved to utils.

	Package mle has been moved to stats4 which will become the
	central place for statistical S4 classes and methods
	distributed with base R.  Package mle remains as a stub.

	Users may notice that code in .Rprofile is run with only the
	new base loaded and so functions may now not be found.	For
	example, ps.options(horizontal = TRUE) should be preceded by
	library(graphics) or called as graphics::ps.options or,
	better, set as a hook -- see ?setHook.

    o	There has been a concerted effort to speed up the startup of
	an R session: it now takes about 2/3rds of the time of 1.8.1.

    o	A warning is issued at startup in a UTF-8 locale, as currently R
	only supports single-byte encodings.


NEW FEATURES

    o	$, $<-, [[, [[<- can be applied to environments. Only character
	arguments are allowed and no partial matching is done.	The
	semantics are basically that of get/assign to the environment with
	inherits=FALSE.

    o	There are now print() and [ methods for "acf" objects.

    o	aov() will now handle singular Error() models, with a warning.

    o	arima() allows models with no free parameters to be fitted (to
	find log-likelihood and AIC values, thanks to Rob Hyndman).

    o	array() and matrix() now allow 0-length `data' arguments for
	compatibility with S.

    o	as.data.frame() now has a method for arrays.

    o	as.matrix.data.frame() now coerces an all-logical data frame
	to a logical matrix.

    o	New function assignInNamespace() parallelling fixInNamespace.

    o	There is a new function contourLines() to produce contour
	lines (but not draw anything).	This makes the CRAN package
	clines (with its clines() function) redundant.

    o	D(), deriv(), etc now also differentiate asin(), acos(), atan(),
	(thanks to a contribution of Kasper Kristensen).

    o	The `package' argument to data() is no longer allowed to be a
	(unquoted) name and so can be a variable name or a quoted
	character string.

    o	There is a new class "Date" to represent dates (without times)
	plus many utility functions similar to those for date-times.
	See ?Date.

    o	Deparsing (including using dump() and dput()) an integer
	vector now wraps it in as.integer() so it will be source()d
	correctly.  (Related to PR#4361.)

    o	.Deprecated() has a new argument `package' which is used in
	the warning message for non-base packages.

    o	The print() method for "difftime" objects now handles arrays.

    o	dir.create() is now an internal function (rather than a call to
	mkdir) on Unix as well as on Windows.  There is now an option
	to suppress warnings from mkdir, which may or may not have
	been wanted.

    o	dist() has a new method to calculate Minkowski distances.

    o	expand.grid() returns appropriate array dimensions and dimnames
	in the attribute "out.attrs", and this is used by the
	predict() method for loess to return a suitable array.

    o	factanal(), loess() and princomp() now explicitly check for
	numerical inputs; they might have silently coded factor
	variables in formulae.

    o	New functions factorial(x) defined as gamma(x+1) and for
	S-PLUS compatibility, lfactorial(x) defined as lgamma(x+1).

    o	findInterval(x, v) now allows +/-Inf values, and NAs in x.

    o	formula.default() now looks for a "terms" component before a
	'formula' argument in the saved call: the component will have
	`.' expanded and probably will have the original environment
	set as its environment.	 And what it does is now documented.

    o	glm() arguments `etastart' and `mustart' are now evaluated via
	the model frame in the same way as `subset' and `weights'.

    o	Functions grep(), regexpr(), sub() and gsub() now coerce their
	arguments to character, rather than give an error.

	The perl=TRUE argument now uses character tables prepared for
	the locale currently in use each time it is used, rather than
	those of the C locale.

    o	New functions head() and tail() in package `utils'.
	(Based on a contribution by Patrick Burns.)

    o	legend() has a new argument 'text.col'.

    o	methods(class=) now checks for a matching generic, and so no
	longer returns methods for non-visible generics (and
	eliminates various mismatches).

    o	A new function mget() will retrieve multiple values from an
	environment.

    o	model.frame() methods, for example those for "lm" and "glm",
	pass relevant parts of ... onto the default method.  (This has
	long been documented but not done.)  The default method is now
	able to cope with model classes such as "lqs" and "ppr".

    o	nls() and ppr() have a `model' argument to allow the model frame
	to be returned as part of the fitted object.

    o	"POSIXct" objects can now have a "tzone" attribute that
	determines how they will be converted and printed.  This means
	that date-time objects which have a timezone specified will
	generally be regarded as in their original time zone.

    o	postscript() device output has been modified to work around
	rounding errors in low-precision calculations in gs >= 8.11.
	(PR#5285, which is not a bug in R.)

	It is now documented how to use other Computer Modern fonts,
	for example italic rather than slanted.

    o	ppr() now fully supports categorical explanatory variables,

	ppr() is now interruptible at suitable places in the
	underlying FORTRAN code.

    o	princomp() now warns if both `x' and `covmat' are supplied,
	and returns scores only if the centring used is known.

    o	psigamma(x, deriv=0), a new function generalizes, digamma() etc.
	All these (psigamma, digamma, trigamma,...) now also work for  x < 0.

    o	pchisq(*, ncp > 0) and hence qchisq() now work with much higher
	values of ncp; it has become much more accurate in the left tail.

    o	read.table() now allows embedded newlines in quoted fields. (PR#4555)

    o	rep.default(0-length-vector, length.out=n) now gives a vector
	of length n and not length 0, for compatibility with S.

	If both `each' and `length.out' have been specified, it now
	recycles rather than fills with NAs for S compatibility.

	If both `times' and `length.out' have been specified, `times'
	is now ignored for S compatibility.  (Previously padding with
	NAs was used.)

	The "POSIXct" and "POSIXlt" methods for rep() now pass ... on
	to the default method (as expected by PR#5818).

    o	rgb2hsv() is new, an R interface the C API function with the same name.

    o	User hooks can be set for onLoad, library, detach and
	onUnload of packages/namespaces: see ?setHook.

    o	save() default arguments can now be set using option
	"save.defaults", which is also used by save.image() if option
	"save.image.defaults" is not present.

    o	New function shQuote() to quote strings to be passed to OS shells.

    o	sink() now has a split= argument to direct output to both the
	sink and the current output connection.

    o	split.screen() now works for multiple devices at once.

    o	On some OSes (including Windows and those using glibc)
	strptime() did not validate dates correctly, so we have added
	extra code to do so.  However, this cannot correct scanning
	errors in the OS's strptime (although we have been able to
	work around these on Windows).	Some examples are now tested for
	during configuration.

    o	strsplit() now has `fixed' and `perl' arguments and
	split="" is optimized.

    o	subset() now allows a `drop' argument to remove unused factor
	levels.	 The default is still to keep them.

    o	termplot() has an option to smooth the partial residuals.

    o	varimax() and promax() add class "loadings" to their loadings
	component.


    o	Model fits now add a "dataClasses" attribute to the terms, which
	can be used to check that the variables supplied for
	prediction are of the same type as those used for fitting.
	(It is currently used by predict() methods for classes "lm",
	"mlm", "glm" and "ppr", as well as methods in packages MASS,
	rpart and tree.)

    o	New command-line argument --max-ppsize allows the size of the
	pointer protection stack to be set higher than the previous
	limit of 10000.

    o	The fonts on an X11() device (also jpeg() and png() on Unix)
	can be specified by a new argument `fonts' defaulting to the
	value of a new option "X11fonts".

    o	New functions in the tools package: pkgDepends, getDepList and
	installFoundDepends.  These provide functionality for assessing
	dependencies and the availability of them (either locally or
	from on-line repositories).

    o	The parsed contents of a NAMESPACE file are now stored at
	installation and if available used to speed loading the
	package, so packages with namespaces should be reinstalled.

    o	Argument `asp' although not a graphics parameter is accepted
	in the ... of graphics functions without a warning.  It now
	works as expected in contour().

    o	Package stats4 exports S4 generics for AIC() and BIC().

    o	The Mac OS X version now produces an R framework for easier linking
	of R into other programs.  As a result, R.app is now relocatable.

    o	Added experimental support for conditionals in NAMESPACE files.

    o	Added as.list.environment to coerce environments to lists
	(efficiently).

    o	New function addmargins() in the stats package to add marginal
	summaries to tables, e.g. row and column totals.  (Based on a
	contribution by Bendix Carstensen.)

    o	dendrogam edge and node labels can now be expressions (to be
	plotted via stats:::plotNode called from plot.dendrogram).
	The diamond frames around edge labels are more nicely scaled
	horizontally.

    o	Methods defined in the methods package can now include
	default expressions for arguments.  If these arguments are
	missing in the call, the defaults in the selected method will
	override a default in the generic.  See ?setMethod.

    o	Changes to package 'grid':

	- Renamed push/pop.viewport() to push/popViewport().

	- Added upViewport(), downViewport(), and seekViewport() to
	  allow creation and navigation of viewport tree
	  (rather than just viewport stack).

	- Added id and id.lengths arguments to grid.polygon() to allow
	  multiple polygons within single grid.polygon() call.

	- Added vpList(), vpStack(), vpTree(), and current.vpTree()
	  to allow creation of viewport "bundles" that may be pushed
	  at once (lists are pushed in parallel, stacks in series).

	  current.vpTree() returns the current viewport tree.

	- Added vpPath() to allow specification of viewport path
	  in downViewport() and seekViewport().

	  See ?viewports for an example of its use.

	  NOTE: it is also possible to specify a path directly,
	  e.g., something like "vp1::vp2", but this is only
	  advised for interactive use (in case I decide to change the
	  separator :: in later versions).

	- Added "just" argument to grid.layout() to allow justification
	  of layout relative to parent viewport *IF* the layout is not
	  the same size as the viewport.  There's an example in
	  help(grid.layout).

	- Allowed the "vp" slot in a grob to be a viewport name or a
	  vpPath.  The interpretation of these new alternatives is to
	  call downViewport() with the name or vpPath before drawing the
	  grob and upViewport() the appropriate amount after drawing the
	  grob.	 Here's an example of the possible usage:

	      pushViewport(viewport(w=.5, h=.5, name="A"))
	      grid.rect()
	      pushViewport(viewport(w=.5, h=.5, name="B"))
	      grid.rect(gp=gpar(col="grey"))
	      upViewport(2)
	      grid.rect(vp="A", gp=gpar(fill="red"))
	      grid.rect(vp=vpPath("A", "B"), gp=gpar(fill="blue"))

	- Added engine.display.list() function.	 This allows the user to
	  tell grid NOT to use the graphics engine display list and to handle
	  ALL redraws using its own display list (including redraws after
	  device resizes and copies).

	  This provides a way to avoid some of the problems with resizing
	  a device when you have used grid.convert(), or the gridBase package,
	  or even base functions such as legend().

	  There is a document discussing the use of display lists in grid
	  on the grid web site
	  (http://www.stat.auckland.ac.nz/~paul/grid/grid.html)

	- Changed the implementation of grob objects.  They are no longer
	  implemented as external references.  They are now regular R objects
	  which copy-by-value.	This means that they can be saved/loaded
	  like normal R objects.  In order to retain some existing grob
	  behaviour, the following changes were necessary:

	  + grobs all now have a "name" slot.  The grob name is used to
	    uniquely identify a "drawn" grob (i.e., a grob on the display
	    list).
	  + grid.edit() and grid.pack() now take a grob name as the first
	    argument instead of a grob.	 (Actually, they take a gPath -
	    see below)
	  + the "grobwidth" and "grobheight" units take either a grob
	    OR a grob name (actually a gPath - see below).  Only in the
	    latter case will the unit be updated if the grob "pointed to"
	    is modified.

	  In addition, the following features are now possible with grobs:

	  + grobs now save()/load() like any normal R object.
	  + many grid.*() functions now have a *Grob() counterpart.  The
	    grid.*() version is used for its side-effect of drawing
	    something or modifying something which has been drawn;  the
	    *Grob() version is used for its return value, which is a grob.
	    This makes it more convenient to just work with grob
	    objects without producing any graphical output
	    (by using the *Grob() functions).
	  + there is a gTree object (derived from grob), which is a grob
	    that can have children.  A gTree also has a "childrenvp" slot
	    which is a viewport which is pushed and then "up"ed before the
	    children are drawn;	 this allows the children of a gTree to
	    place themselves somewhere in the viewports specified in the
	    childrenvp by having a vpPath in their vp slot.
	  + there is a gPath object, which is essentially a concatenation
	    of grob names.  This is used to specify the child of
	    (a child of ...) a gTree.
	  + there is a new API for creating/accessing/modifying grob objects:
	    grid.add(), grid.remove(), grid.edit(), grid.get() (and their
	    *Grob() counterparts can be used to add, remove, edit, or extract
	    a grob or the child of a gTree.  NOTE: the new grid.edit() API
	    is incompatible with the previous version.

	- Added stringWidth(), stringHeight(), grobWidth(), and grobHeight()
	  convenience functions (they produce "strwidth", "strheight",
	  "grobwidth", and "grobheight" unit objects, respectively).

	- Allowed viewports to turn off clipping altogether.
	  Possible settings for viewport clip arg are now:

	    "on"      = clip to the viewport (was TRUE)
	    "inherit" = clip to whatever parent says (was FALSE)
	    "off"     = turn off clipping

	  Still accept logical values (and NA maps to "off")


UTILITIES

    o	R CMD check  now runs the (Rd) examples with default RNGkind
	(uniform & normal) and set.seed(1).
	example(*, setRNG = TRUE) does the same.

    o	undoc() in package `tools' has a new default of `use.values =
	NULL' which produces a warning whenever the default values of
	function arguments differ between documentation and code.
	Note that this affects "R CMD check" as well.

    o	Testing examples via massage-examples.pl (as used by R CMD
	check) now restores the search path after every help file.

    o	checkS3methods() in package 'tools' now also looks for generics
	in the loaded namespaces/packages listed in the Depends fields
	of the package's DESCRIPTION file when testing an installed
	package.

    o	The DESCRIPTION file of packages may contain a 'Suggests:'
	field for packages that are only used in examples or
	vignettes.

    o	Added an option to package.dependencies() to handle the
	'Suggests' levels of dependencies.

    o	Vignette dependencies can now be checked and obtained via
	vignetteDepends.

    o	Option 'repositories' to list URLs for package repositories
	added.

    o	package.description() has been replaced by packageDescription().

    o	R CMD INSTALL/build now skip Subversion's .svn directories as
	well as CVS directories.


C-LEVEL FACILITIES

    o	arraySubscript and vectorSubscript take a new argument which
	is a function pointer that provides access to character
	strings (such as the names vector) rather than assuming these
	are passed in.

    o	R_CheckUserInterrupt is now described in `Writing R Extensions'
	and there is a new equivalent subroutine rchkusr for calling
	from FORTRAN code.

    o	hsv2rgb and rgb2hsv are newly in the C API.

    o	Salloc and Srealloc are provided in S.h as wrappers for S_alloc
	and S_realloc, since current S versions use these forms.

    o	The type used for vector lengths is now R_len_t rather than
	int, to allow for a future change.

    o	The internal header nmath/dpq.h has slightly improved macros
	R_DT_val() and R_DT_Cval(), a new R_D_LExp() and improved
	R_DT_log() and R_DT_Clog();  this improves accuracy in several
	[dpq]-functions {for "extreme" arguments}.


DEPRECATED & DEFUNCT

    o	print.coefmat() is defunct, replaced by printCoefmat().

    o	codes() and codes<-() are defunct.

    o	anovalist.lm (replaced in 1.2.0) is now defunct.

    o	glm.fit.null(), lm.fit.null() and lm.wfit.null() are defunct.

    o	print.atomic() is defunct.

    o	The command-line arguments --nsize and --vsize are no longer
	recognized as synonyms for --min-nsize and --min-vsize (which
	replaced them in 1.2.0).

    o	Unnecessary methods coef.{g}lm and fitted.{g}lm have been
	removed: they were each identical to the default method.

    o	La.eigen() is deprecated now eigen() uses LAPACK by default.

    o	tetragamma() and pentagamma() are deprecated, since they are
	equivalent to psigamma(, deriv=2) and psigamma(, deriv=3).

    o	LTRUE/LFALSE in Rmath.h have been removed: they were
	deprecated in 1.2.0.

    o	package.contents() has been deprecated.


INSTALLATION CHANGES

    o	The defaults for configure are now --without-zlib
	--without-bzlib --without-pcre.

	The included PCRE sources have been updated to version 4.5 and
	PCRE >= 4.0 is now required if --with-pcre is used.

	The included zlib sources have been updated to 1.2.1, and this
	is now required if --with-zlib is used.

    o	configure no longer lists bzip2 and PCRE as `additional
	capabilities' as all builds of R have had them since 1.7.0.

    o	--with-blas=goto to use K. Goto's optimized BLAS will now work.


BUG FIXES

    o	When lm.{w}fit() disregarded arguments in ... they reported
	the values and not the names.

    o	lm(singular.ok = FALSE) was looking for 0 rank, not rank < p.

    o	The substitution code for strptime in the sources no longer
	follows glibc in silently `correcting' invalid inputs.

    o	The cor() function did not remove missing values in the
	non-Pearson case.

    o	[l]choose() use a more accurate formula which also slightly
	improves p- and qhyper(); choose(n, k) now returns 0 instead
	of NaN for k < 0 or > n.

    o	find(simple.words=TRUE) (the default) was still using regular
	expressions for e.g. "+" and "*".  Also, it checked the mode
	only of the first object matching a regular expression found
	in a package.

    o	Memory leaks in [dpq]wilcox and [dqr]signrank have been plugged.
	These only occurred when multiple values of m or n > 50 were
	used in a single call. (PR#5314, plus another potential leak.)

    o	Non-finite input values to eigen(), La.eigen(), svd() and
	La.svd() are now errors: they often caused infinite
	looping.  (PR#5406, PR#4366, PR#3723: the fix for 3723/4366
	returned a vector of NAs, not a matrix, for the eigenvectors.)

    o	stepfun(x,y) now gives an error when `x' has length 0 instead
	of an invalid result (that could lead to a segmentation
	fault).

    o	buildVignettes() uses file.remove() instead of unlink() to
	remove temporary files.

    o	methods(class = "lqs") does not produce extraneous entries anymore.

    o	Directly calling a method that uses NextMethod() no longer
	produces the erroneous error message 'function is not a
	closure'.

    o	chisq.test(x, simulate.p.value = TRUE) could hang in an infinite
	loop or segfault, as r2dtable() did, when the entries in x where
	large. (PR#5701)

    o	fisher.test(x) could give a P-value of 'Inf' in similar cases which
	now result in an error (PR#4688).  It silently truncated
	non-integer 'x' instead of rounding.

    o	cutree(a, h=h) silently gave wrong results when 'a' was an
	agnes object; now gives an error and reminds of as.hclust().

    o	postscript() could crash if given a font value outside the
	valid range 1...5.

    o	qchisq(1-e, .., ncp=.) did not terminate for small e.
	(PR#6421 (PR#875))

    o	contrasts() turns a logical variable into a factor.  This now
	always has levels c("FALSE", "TRUE") even if only one (or
	none) of these occur in the variable.

    o	model.frame()'s lm and glm methods had 'data' and 'na.action'
	arguments which they ignored and have been removed.

    o	The defaults data=list() in lm() and glm() could never be
	used and have been removed.  glm had na.action=na.fail, again
	never used.

    o	tools:::.getInternalS3generics() was omitting all the members
	of the S3 group generics, which also accept methods for members.

    o	Some BLASes were returning NA %*% 0 as 0 and some as NA.  Now
	slower but more careful code is used if NAs are present. (PR#4582)

    o	package.skeleton() no longer generates invalid filenames for
	code and help files.  Also, care is taken not to generate
	filenames that differ only by case.

    o	pairs() now respects axis graphical parameters such as
	cex.main, font.main and las.

    o	Saving images of packages with namespaces (such as mle) was
	not compressing the image.

    o	When formula.default() returned a terms object, it returned a
	result of class c("terms", "formula") with different
	subsetting rules from an object of class "formula".

    o	The standalone Rmath library did not build correctly on systems
	with inaccurate log1p.

    o	Specifying asp is now respected in calls like plot(1, 10, asp=1)
	with zero range on both axes.

    o	outer() called rep() with an argument the generic does not
	have, and discarded the class of the answer.

    o	object.size() now returns a real (not integer) answer and so
	can cope with objects occupying more than 2Gb.

    o	Lookups base:: and ::: were not confining their search to the
	named package/namespace.

    o	qbinom() was returning NaN for prob = 0 or 1 or size = 0 even
	though the result is well-defined.  (In part, PR#5900.)

    o	par(mgp)[2] was being interpreted as relative to par(mgp)[3].
	(PR#6045)

    o	Versioned install was broken both with and without namespaces:
	no R code was loaded.

    o	methods(), getS3method() and the registration of S3 methods in
	namespaces were broken if the S3 generic was converted into an
	S4 generic by setting an S4 method.

    o	Title and copyright holder of the reference manual are now in
	sync with the citation() command.

    o	The validation code for POSIXlt dates and hence
	seq(, by="DSTdays") now works for large mday values (not
	just those in -1000...1000).  (PR#6212)

    o	The print() method for data frames now copes with data frames
	containing arrays (other than matrices).

    o	texi2dvi() and buildVignettes() use clean=FALSE as default
	because the option is not supported on some Solaris
	machines. For buildVignettes() this makes no difference as it
	uses an internal cleanup mechanism.

    o	The biplot() method for "prcomp" was not registered nor exported.
	(PR#6425)

    o	Latex conversion of .Rd files was missing newline before
	\end{Section} etc which occasionally gave problems, as fixed for
	some other \end{Foo} in 1.8.1.	(PR#5645)

    o	Work around a glibc bug to make the %Z format usable in strftime().

    o	The glm method for rstandard() was wrongly scaled for cases where
	summary(model)$dispersion != 1.

    o	Calling princomp() with a covariance matrix (rather than a
	list) failed to predict scores rather than predict NA as
	intended.  (PR#6452)

    o	termplot() is more tolerant of variables not in the data= argument.
	(PR#6327)

    o	isoreg() could segfault on monotone input sequences.  (PR#6494)

    o	Rdconv detected \link{\url{}} only very slowly.	 (PR#6496)

    o	aov() with Error() term and no intercept incorrectly assigned
	terms to strata. (PR#6510)

    o	ftable() incorrectly handled arguments named "x".  (PR#6541)

    o	vector(), matrix(), array() and their internal equivalents
	report correctly that the number of elements specified was too
	large (rather than reporting it as negative).

    o	Minor copy-paste error in example(names).  (PR#6594)

    o	length<-() now works correctly on factors (and is now generic
	with a method for factors).

    o	x <- 2^32; x:(x+3) no longer generates an error (but gives a
	result of type "double").

    o	pgamma(30, 100, lower=FALSE, log=TRUE) is not quite 0, now.
	pgamma(x, alph) now only uses a normal approximation for
	alph > 1e5 instead of alph > 1000.  This also improves the accuracy
	of ppois().

    o	qgamma() now does one or more final Newton steps, increasing
	accuracy from around 2e-8 to 3e-16 in some cases.  (PR#2214).
	It allows values p close to 1 not returning Inf, with accuracy for
	'lower=FALSE', and values close to 0 not returning 0 for
	'log=TRUE'.  These also apply to qchisq(), e.g.,
	qchisq(1e-13, 4, lower=FALSE) is now finite and
	qchisq(1e-101, 1) is positive.

    o	gamma(-n) now gives NaN for all negative integers -n.

    o	The Unix version of browseURL() now protects the URL from the
	shell, for example allowing & and $ to occur in the URL.

	It was incorrectly attempting to use -remote "openURL()" for
	unknown browsers.

    o	extractAIC.coxph() works around an inconsistency in the
	$loglik output from coxph.  (PR#6646)

    o	stem() was running into integer overflows with nearly-constant
	inputs, and scaling badly for constant ones.  (Partly PR#6645)

    o	system() under Unix was losing the 8095th char if the output
	was split.  (PR#6624)

    o	plot.lm() gave incorrect results if there were zero weights.
	(PR#6640)

    o	Binary operators warned for inconsistent lengths on vector op
	vector operations, but not on vector op matrix ones.  (PR#6633
	and more.)

	Comparison operators did not warn about inconsistent lengths
	for real vectors, but did for integer, logical and character
	vectors.

    o	spec.pgram(x, ..., pad, fast, ...) computed the periodogram with
	a bias (downward) whenever 'pad > 0' (non-default) or 'fast = TRUE'
	(default) and nextn(n) > n where n = length(x); similarly for
	'df' (approximate degrees of freedom for chisq).

    o	dgamma(0, a) now gives Inf for a < 1 (instead of NaN), and
	so does dchisq(0, 2*a, ncp).

    o	pcauchy() is now correct in the extreme tails.

    o	file.copy() did not check that any existing `from' file had
	been truncated before appending the new contents.

    o	The QC files now check that their file operations succeeded.

    o	replicate() worked by making the supplied expression the
	body of an anonymous function(x), leading to a variable
	capture issue. Now, function(...) is used instead.

    o	chisq.test(simulate.p.value = TRUE) was returning slightly
	incorrect p values, notably p = 0 when the data gave the most
	extreme value.

    o	terms.formula(simplify = TRUE) was losing offset terms.
	Multiple offset terms were not being removed correctly if two
	of them appeared first or last in the formula.	(PR#6656)

    o	Rd conversion to latex did not add a new line before
	\end{Section} in more cases than were corrected in 1.8.1.

    o	split.default() dropped NA levels in its internal code but
	returned them as NA in all components in the interpreted code
	for factors.  (PR#6672)

    o	points.formula() had problems if there was a subset argument
	and no data argument.  (PR#6652)

    o	as.dist() does a bit more checking of its first argument and now
	warns when applied to non-square matrices.

    o	mle() gives a more understandable error message when its 'start'
	argument is not ok.

    o	All uses of dir.create() check the return value.
	download.packages() checks that destdir exists and is a directory.

    o	Methods dispatch corrects an error that failed to find
	methods for classes that extend sealed classes (class unions
	that contain basic classes, e.g.).

    o	Sweave no longer wraps the output of code chunks with
	echo=false and results=tex in Schunk environments.

    o	termplot() handles models with missing data better, especially
	with na.action=na.exclude.

    o	1:2 * 1e-100  now prints with correct number of spaces.

    o	Negative subscripts that were out of range or NA were not handled
	correctly.  Mixing negative and NA subscripts is now caught as
	an error: it was not caught on some platforms and segfaulted
	on others.

    o	gzfile() connections had trouble at EOF when used on uncompressed
	file.

    o	The Unix version of dataentry segfaulted if the `Copy' button
	was used.  (PR#6605)

    o	unlist on lists containing expressions now works (PR#5628)

    o	D(), deriv() and deriv3() now also can deal with gamma and lgamma.

    o	The X11 module can now be built against XFree86 4.4.0 headers (still
	with some warnings).

    o	seq.POSIXt(from, to, by="DSTdays") was shorter than expected
	for rare times in the UK time zone.  (PR#4558)

    o	c/rbind() did not support vectors/matrices of mode "list".  (PR#6702)

    o	summary() methods for POSIX[cl]t and Date classes coerced the
	number of NAs to a date on printing.

    o	KalmanSmooth would sometimes return NA values with NA inputs.
	(PR#6738)

    o	fligner.test() worked correctly only if data were already sorted
	by group levels. (PR#6739)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From hellik at web.de  Mon Apr 12 14:21:10 2004
From: hellik at web.de (Helmut Kudrnovsky)
Date: Mon, 12 Apr 2004 14:21:10 +0200
Subject: [R] question on isoMDS
Message-ID: <5.1.0.14.0.20040412134639.009fdb20@pop3.web.de>

Hello everyone,

I have a question on isoMDS.

My data set (of vegetation) with 210 samples is in this way:

               Rotfoehrenau Lavendelweidenau Silberweidenau ....
067_Breg.7               0                2              0         ....
071_Dona.4              0                2              6          ....
...

I want to do an isoMDS-analysis with the dissimilarity index "bray/curtis" 
as discribed in the help-files of the package vegan:

mds.test <- isoMDS(test.bc, initMDS(test.bc), maxit=200, trace=FALSE, tol=1e-7)

Then I get following error message:

"Error in isoMDS(test.bc, initMDS(test.bc), maxit = 200, trace = FALSE,  :
         zero or negative distance between objects 19 and 20"

The objects 19 and 20 have the same variables in the same way and therefore 
they have a zero distance.

My question is: Is it possible to perform an isoMDS with a data set where 
some samples have the same variables in the same way?

with greetings from Tyrol
Helli Kudrnovsky

R: 1.8.1
OS: Win98



From tlumley at u.washington.edu  Mon Apr 12 16:13:33 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 07:13:33 -0700 (PDT)
Subject: [R] Regression models w/ splines
In-Reply-To: <20040409180549.57c92491.feh3k@spamcop.net>
References: <s076bed6.007@co5.dnr.state.mn.us>
	<20040409180549.57c92491.feh3k@spamcop.net>
Message-ID: <Pine.A41.4.58.0404120711430.52172@homer04.u.washington.edu>

On Fri, 9 Apr 2004, Frank E Harrell Jr wrote:

> On Fri, 09 Apr 2004 15:16:34 -0500
> "John Fieberg" <John.Fieberg at dnr.state.mn.us> wrote:
>
> > Hi - I am fitting various Cox PH models with spline predictors.  After
> > fitting the model, I would like to use termplot() to examine the
> > functional form of the fitted model (e.g., to obtain a plot of the
> > relative risk (or log r.r.) versus the predictors).
>
> This kind of application is why the Design package was created.  It is
> very easy to obtain such plots:

It's actually very easy with termplot(), it's just that there was a bug in
predict.coxph() in handling single-term models.

The version of survival that comes with 1.9.0 should work (and for Frank,
it exports the other internal functions you wanted).

	-thomas



From tlumley at u.washington.edu  Mon Apr 12 16:18:27 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 07:18:27 -0700 (PDT)
Subject: [R] all poss. regression
In-Reply-To: <Pine.LNX.4.44.0404101150400.14780-100000@gannet.stats>
References: <Pine.LNX.4.44.0404101150400.14780-100000@gannet.stats>
Message-ID: <Pine.A41.4.58.0404120715020.52172@homer04.u.washington.edu>

On Sat, 10 Apr 2004, Prof Brian Ripley wrote:

> Look at package leaps.  Note that it applies to columns of the
> design matrix and not terms, so can be nonsensical with categorical
> predictors (let alone interactions of such).
>

A reasonable compromise is to run regsubsets() (in leaps) returning the
best hundred or so models of each size, and then refit each model ensuring
that the terms are treated sensibly.   I use this sometimes for variable
screening.

	-thomas



From tlumley at u.washington.edu  Mon Apr 12 16:21:24 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 07:21:24 -0700 (PDT)
Subject: [R] (offtopic) I need two sets of 5 different color scales
In-Reply-To: <20040410110918.GA2148@localhost>
References: <20040410110918.GA2148@localhost>
Message-ID: <Pine.A41.4.58.0404120719010.52172@homer04.u.washington.edu>

On Sat, 10 Apr 2004, Tamas Papp wrote:
>
> I need advice in the following: I need two sets of colors for B and F
> which are easy to distinguish (when printed on a color laser printer),
> represent cardinality (ie have an intuitive mapping to an interval) or
> at least ordinality.

It's not offtopic.  This is what the RColorBrewer package is for. If you
look at Cindy Brewer's web version (www.colorbrewer.org) there is
additional information about which scales work well on projectors, cheap color
printers, for people with red-green vision anomalies, and so on.

(It would be nice to have these additional data in the R package)

	-thomas



>
> I have experimented with the following:
>
> Bcolors <- hsv(.6, seq(0.2, 1, length=5), 1)
> Fcolors <- hsv(seq(.1,0, length=5), seq(0.2, 1, length=5)
>
> this is what you see in the plot.  What colors would you use?  Do you
> think that varying both brightness and hue helps to distinguish
> colors?  Should I change saturation, too?
>
> Thanks,
>
> Tamas
>
> PS.: The plot is simply gzipped.  If you need a zipped version, or the
> source code, contact me.
>
> --
> Tams K. Papp
> E-mail: tpapp at axelero.hu
> Please try to send only (latin-2) plain text, not HTML or other garbage.
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Mon Apr 12 16:24:18 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 07:24:18 -0700 (PDT)
Subject: [R] pasting results into Word/Excel
In-Reply-To: <memo.20040411185211.56301E@myotis.v21>
References: <memo.20040411185211.56301E@myotis.v21>
Message-ID: <Pine.A41.4.58.0404120723160.52172@homer04.u.washington.edu>

On Sun, 11 Apr 2004, Graham M Smith wrote:

> Anders,
>
> This seems a good start, and I will try it get it to work, on of the
> issues is that the results output is more complex than simply split by
> comma eg a typical Summary output of my current data set (below)

My current approach for model summaries is to use the xtable package and
write things as HTML tables.

	-thomas

> However, I am a lot further forward than I was a few hours ago.
>
> Thanks
>
>    Section         Cave        Season         Year     BatUse
>  Black  :225   ELE    : 48   Autumn:101   1995   :217   0:233
>  Central:276   BRO    : 47   Spring:169   1996   :149   1:450
>  Iron   : 49   OCA    : 46   Summer:101   1998   : 61
>  West   :133   OGE    : 46   Winter:312   1999   : 56
>                WAT    : 46                1997   : 50
>                OCF1   : 45                1994   : 49
>                (Other):405                (Other):101
>      Number             L.H.
>  Min.   : 0.0000   Min.   : 0.0000
>  1st Qu.: 0.0000   1st Qu.: 0.0000
>  Median : 0.0000   Median : 0.0000
>  Mean   : 0.9634   Mean   : 0.9239
>  3rd Qu.: 1.0000   3rd Qu.: 1.0000
>  Max.   :23.0000   Max.   :23.0000
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Mon Apr 12 16:32:11 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 07:32:11 -0700 (PDT)
Subject: [R] Only referenc copy when calling C routine?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIAEILCKAA.sdhyok@email.unc.edu>
References: <OAEOKPIGCLDDHAEMCAKIAEILCKAA.sdhyok@email.unc.edu>
Message-ID: <Pine.A41.4.58.0404120728380.52172@homer04.u.washington.edu>

On Sun, 11 Apr 2004, Shin, Daehyok wrote:

> Thanks, Andy.
> Let me ask a little more about the DUP argument.
> In the manual, I can see that only reference copy happens when passing an
> array,
> if DUP=FALSE. Then, what happens when returning it in a list, if DUP=FALSE?
> Another reference copy, or deep copy?
>

I think that last time I looked at this my conclusion was that you
shouldn't use lists with DUP=FALSE.  Whoever wrote the comments in the
code also seemed a bit worried.

	-thomas



From tlumley at u.washington.edu  Mon Apr 12 16:36:13 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 07:36:13 -0700 (PDT)
Subject: [R] missing values and survival analysis
In-Reply-To: <1081741042.407a0ef2d6bf6@www.mail.yale.edu>
References: <1081741042.407a0ef2d6bf6@www.mail.yale.edu>
Message-ID: <Pine.A41.4.58.0404120734230.52172@homer04.u.washington.edu>

On Sun, 11 Apr 2004 john.ferguson at yale.edu wrote:
> Hi everyone,
>
> I'm analysing a survival analysis data set at the moment with missing
> values in the covariate and survival vectors (I have about 60
> variables).  I know there are some functions on the CRAN network to
> deal with missing values in general multivariate data.  Does anybody
> know of any package that deals with missing data specifically in the
> context of survival analysis.  Any help would be greatly appreciated.

I'd suggest some form of multiple imputation (eg the Design, norm, mix,
pan packages).

	-thomas



From graham.smith at myotis.co.uk  Mon Apr 12 18:32:00 2004
From: graham.smith at myotis.co.uk (Graham M Smith)
Date: Mon, 12 Apr 2004 17:32 +0100 (BST)
Subject: [R] pasting results into Word/Excel
In-Reply-To: <Pine.A41.4.58.0404120723160.52172@homer04.u.washington.edu>
Message-ID: <memo.20040412173251.14279C@myotis.v21>

Thomas,

> My current approach for model summaries is to use the xtable package and
> write things as HTML tables.

It was an HTML approach that Gabor suggested,using R2HTML, and that worked 
well, but I will try xtable as well.

As always with R, there is a solution if you know where to look.

Thanks for your help,

Graham



From m_nica at hotmail.com  Mon Apr 12 18:28:23 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Mon, 12 Apr 2004 11:28:23 -0500
Subject: [R] Panel Data Analysis
Message-ID: <LAW10-OE53szyQzNFJT0001cdc5@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040412/b5c047f3/attachment.pl

From bates at stat.wisc.edu  Mon Apr 12 18:52:33 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 12 Apr 2004 11:52:33 -0500
Subject: [R] Panel Data Analysis
In-Reply-To: <LAW10-OE53szyQzNFJT0001cdc5@hotmail.com>
References: <LAW10-OE53szyQzNFJT0001cdc5@hotmail.com>
Message-ID: <6r8yh1xhny.fsf@bates4.stat.wisc.edu>

"Mihai Nica" <m_nica at hotmail.com> writes:

> I am trying to find a package/solution for panel (longitudinal) data
> analysis. Unfortunately it seems I don't know where to start. Could
> somebody offer a hint?

Analysis of longitudinal data is discussed in Pinheiro and Bates
(2000), "Mixed-effects Models in S and S-PLUS" (Springer), which uses
the nlme package.  You may also want to look at the web site for Jed
Frees upcoming book "Longitudinal and Panel Data: Analysis and
Applications for the Social Sciences" 
        http://research.bus.wisc.edu/jfrees/Book/PDataBook.htm

The Statistical Code section provides examples in R as well as SAS and
Stata.



From bates at stat.wisc.edu  Mon Apr 12 19:04:39 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 12 Apr 2004 12:04:39 -0500
Subject: [R] R-1.9.0 sources available by rsync
Message-ID: <6ru0zpw2jc.fsf@bates4.stat.wisc.edu>

The sources for R-1.9.0 are now available via rsync at
rsync.r-project.org.  You can, for example, use

rsync -avC rsync.r-project.org::r-release ./R-1.9.0

to obtain a copy of the sources.  

Binary distributions for platforms such as Windows and Mac OS X will
be available from the CRAN sites in a few days.

I see that Dirk Eddelbuettel has already uploaded the binary Debian
packages to the main Debian archives.



From m_nica at hotmail.com  Mon Apr 12 19:05:35 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Mon, 12 Apr 2004 12:05:35 -0500
Subject: [R] Panel Data Analysis
References: <LAW10-OE53szyQzNFJT0001cdc5@hotmail.com>
	<6r8yh1xhny.fsf@bates4.stat.wisc.edu>
Message-ID: <LAW10-OE41pWkfb81If0001d142@hotmail.com>

Thanks, this is what I needed!

Sincerely,

Mihai Nica
Jackson State University
155 B Parkhurst Dr.
Jackson, MS 39202

"No good deed will ever remain unpunished"
----- Original Message ----- 
From: "Douglas Bates" <bates at stat.wisc.edu>
To: "Mihai Nica" <m_nica at hotmail.com>
Cc: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Monday, April 12, 2004 11:52 AM
Subject: Re: [R] Panel Data Analysis


> "Mihai Nica" <m_nica at hotmail.com> writes:
> 
> > I am trying to find a package/solution for panel (longitudinal) data
> > analysis. Unfortunately it seems I don't know where to start. Could
> > somebody offer a hint?
> 
> Analysis of longitudinal data is discussed in Pinheiro and Bates
> (2000), "Mixed-effects Models in S and S-PLUS" (Springer), which uses
> the nlme package.  You may also want to look at the web site for Jed
> Frees upcoming book "Longitudinal and Panel Data: Analysis and
> Applications for the Social Sciences" 
>         http://research.bus.wisc.edu/jfrees/Book/PDataBook.htm
> 
> The Statistical Code section provides examples in R as well as SAS and
> Stata.
>



From statmkg at optonline.net  Mon Apr 12 19:36:15 2004
From: statmkg at optonline.net (MKG)
Date: Mon, 12 Apr 2004 13:36:15 -0400
Subject: [R] von Mises Density
Message-ID: <407AD38F.1080604@optonline.net>

Dear All,

I has posted this question on friday.
I am trying to plot von Mises density on
the circle. One can use dvm function from the
CircStats package, by giving a set of angles,
mu and kappa to plot the circular density
on the line. Does any one have a macro that
does it on the circle? These plots are displayed
in Nick Fisher's book.

Any help will be appreciated.

Thanks,

G. Subramaniam



From gregory_r_warnes at groton.pfizer.com  Mon Apr 12 19:58:29 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Mon, 12 Apr 2004 13:58:29 -0400
Subject: [R] [JOB ADV] Computational Statistician
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B26C@groexmb02.pfizer.com>

> We've just opened a position for a computational statistician in the
> Non-Clinical Statistics group here at Pfizer.  
> 
> We're looking for someone with strong computational skills in R and SAS,
> who is interested in creating real applications.  The focus will be on
> statistical genetics, but we handle almost every kind of statistics
> outside of Phase II and later clinical trials, so there will be plenty of
> opportunity to do other things as well.
> 
> For further information or to submit an application, please visit
> http://pfizer.softshoe.com/cgi-bin/job-show?J_PINDEX=J974700KW.
> 
> Thanks,
> 
> -Greg
> 
> Gregory R. Warnes
> Manager, Non-Clinical Statistics
> Pfizer Global Research and Development
> Tel: 860-715-3536
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From mmarques at inescporto.pt  Mon Apr 12 17:59:59 2004
From: mmarques at inescporto.pt (MMarques Power)
Date: Mon, 12 Apr 2004 16:59:59 +0100
Subject: [R] String problems
Message-ID: <96507842062.20040412165959@power.inescn.pt>


after reading some docs I still can not find anything how to escape
the "\" string


I know that "\" in files needs to be escaped in order to be read by R.
But in Windows if I try to call any shell util I keep getting errors.
example:

afile <- "g:\\simulation\\at"
works fine with read.table(afile) ...

but if I try sometthing like this:
aprogram <- paste( "c:\dir", afile, sep=" " )
system(aprogram)
it crashes...

How can I remove the double "\" in the afile ?
I know that I need to reconstruct the afile string without the double
"\"

Ideas ?
or am I missing something ?



From hhan at cse.psu.edu  Mon Apr 12 20:11:07 2004
From: hhan at cse.psu.edu (Hui Han)
Date: Mon, 12 Apr 2004 14:11:07 -0400
Subject: [R] Random Forest:how to do an automatic rerun using only the
	important variables
Message-ID: <20040412181107.GA29339@alphan.cse.psu.edu>

Hi,

I am using the Random Forest Package, and want to do an automatic rerun
using only those variables that were most important in the original run.
Is there anybody who has experience with this and can give me helpful 
suggestions?

Best regards,

Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan



From ligges at statistik.uni-dortmund.de  Mon Apr 12 20:13:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 12 Apr 2004 20:13:42 +0200
Subject: [R] String problems
In-Reply-To: <96507842062.20040412165959@power.inescn.pt>
References: <96507842062.20040412165959@power.inescn.pt>
Message-ID: <407ADC56.3020206@statistik.uni-dortmund.de>

MMarques Power wrote:

> after reading some docs I still can not find anything how to escape
> the "\" string
> 
> 
> I know that "\" in files needs to be escaped in order to be read by R.
> But in Windows if I try to call any shell util I keep getting errors.
> example:
> 
> afile <- "g:\\simulation\\at"
> works fine with read.table(afile) ...
> 
> but if I try sometthing like this:
> aprogram <- paste( "c:\dir", afile, sep=" " )
> system(aprogram)
> it crashes...
> 
> How can I remove the double "\" in the afile ?
> I know that I need to reconstruct the afile string without the double
> "\"
> 
> Ideas ?
> or am I missing something ?

Yes, you missed to double it in "c:\dir". Try:

  afile <- "g:\\simulation\\at"
  aprogram <- paste( "c:\\dir", afile, sep=" " )
  system(aprogram)

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jmacdon at med.umich.edu  Mon Apr 12 20:25:21 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Mon, 12 Apr 2004 14:25:21 -0400
Subject: [R] String problems
Message-ID: <s07aa6dc.088@med-gwia-02a.med.umich.edu>

Plus, do you really want your system call to be

system("c:\\dir g:\\simulation\\at")

not sure what you want, but that is what you will get.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Uwe Ligges <ligges at statistik.uni-dortmund.de> 04/12/04 02:13PM >>>
MMarques Power wrote:

> after reading some docs I still can not find anything how to escape
> the "\" string
> 
> 
> I know that "\" in files needs to be escaped in order to be read by
R.
> But in Windows if I try to call any shell util I keep getting
errors.
> example:
> 
> afile <- "g:\\simulation\\at"
> works fine with read.table(afile) ...
> 
> but if I try sometthing like this:
> aprogram <- paste( "c:\dir", afile, sep=" " )
> system(aprogram)
> it crashes...
> 
> How can I remove the double "\" in the afile ?
> I know that I need to reconstruct the afile string without the
double
> "\"
> 
> Ideas ?
> or am I missing something ?

Yes, you missed to double it in "c:\dir". Try:

  afile <- "g:\\simulation\\at"
  aprogram <- paste( "c:\\dir", afile, sep=" " )
  system(aprogram)

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wwsprague at ucdavis.edu  Mon Apr 12 20:50:30 2004
From: wwsprague at ucdavis.edu (Webb Sprague)
Date: Mon, 12 Apr 2004 11:50:30 -0700
Subject: [R] Line numbers in error messages
Message-ID: <407AE4F6.5080006@ucdavis.edu>

Hi all,

I have searched but to know avail.  Is there a way to get a line number 
when a function crashes?  I am doing an edit->source->run cycle.

Feel free to cc me as I subscribe to the list in digest.

Thanks
W



From andy_liaw at merck.com  Mon Apr 12 22:33:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 12 Apr 2004 16:33:34 -0400
Subject: [R] Random Forest:how to do an automatic rerun using only
	the important variables
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B8E@usrymx25.merck.com>

That's the advantage of having an R interface to RF: you can do such
automation rather easily.  E.g.,

twoStageRF <- function(x, y, nVar=round(0.1*ncol(x)), ...) {
  imp <- randomForest(x, y, importance=TRUE, ...)$importance[,3]
  cutoff <- sort(imp, decreasing=TRUE)[nVar]
  randomForest(x[, imp >= cutoff], y, ...)
}

[Disclaimer: I just wrote the function on the spot, so completely untested.
This is just to demonstrate how simple it would be.  You can embelish it as
much as you'd like.]

I have written a function that uses CV to choose the `optimal' number of
variables to keep (rather than blindly select one up front).  I might toss
it in the next version of the package...

HTH,
Andy

> From: Hui Han
> 
> Hi,
> 
> I am using the Random Forest Package, and want to do an 
> automatic rerun
> using only those variables that were most important in the 
> original run.
> Is there anybody who has experience with this and can give me helpful 
> suggestions?
> 
> Best regards,
> 
> Hui Han
> Department of Computer Science and Engineering,
> The Pennsylvania State University 
> University Park, PA,16802
> email: hhan at cse.psu.edu
> homepage: http://www.cse.psu.edu/~hhan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From frohde_home at yahoo.com  Mon Apr 12 22:49:32 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Mon, 12 Apr 2004 13:49:32 -0700 (PDT)
Subject: [R] Complex sample variances
Message-ID: <20040412204932.29573.qmail@web60105.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040412/9745de7e/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Apr 12 22:58:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 12 Apr 2004 22:58:21 +0200
Subject: [R] Line numbers in error messages
References: <407AE4F6.5080006@ucdavis.edu>
Message-ID: <407B02ED.EABB641F@statistik.uni-dortmund.de>



Webb Sprague wrote:
> 
> Hi all,
> 
> I have searched but to know avail.  Is there a way to get a line number
> when a function crashes?  I am doing an edit->source->run cycle.

I don't think so, but traceback(), debug() and options(error = recover)
are your friends.
Also, you might want to check out the CRAN package "debug".

Uwe Ligges


 
 
> Feel free to cc me as I subscribe to the list in digest.
>
> Thanks
> W
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Mon Apr 12 23:30:34 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 14:30:34 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <20040412204932.29573.qmail@web60105.mail.yahoo.com>
References: <20040412204932.29573.qmail@web60105.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0404121416560.86522@homer10.u.washington.edu>

On Mon, 12 Apr 2004, Fred Rohde wrote:

> Hello,
>  Is there a way to get complex sample variances in the survey package on
> summary statistics other than means?  If not, can they be added to a
> future version?  It would be be great to have them on totals, quantiles,
> ratios, and tables (eg row percent, columns percent, etc).
>

svytotal() and svyratio() will do this for totals and ratios if you have a
new enough version. At the moment the easiest way to get row or column
percentages is to think of them them as ratios of means of binary
variables and use svyratio().

Quantiles are more difficult, since neither Taylor series nor jackknife
approaches work.

	-thomas



From gregory_r_warnes at groton.pfizer.com  Mon Apr 12 23:44:56 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Mon, 12 Apr 2004 17:44:56 -0400
Subject: [R] [R-pkgs] new version of gregmisc package
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B274@groexmb02.pfizer.com>


gregmisc 0.10.1
---------------

gregmisc 0.10.1 is now available at ftp://cran.r-project.org/incoming/ and
will move to the regular contributed packages location
(http://cran.r-project.org/src/contrib/PACKAGES.html#gregmisc) as soon as
the CRAN administrators get caught up.

Recent Changes
---------------

Version 0.10.1

- Fixed bug in textplot() reported by Kevin Wright  <kevin.d.wright at
pioneer
  dot com>.

Version 0.10.0

- Now works with and requires R 1.9.0

- Added ooplot() function that mimics Open-Office style plots.
  Contributed by Lodewijk Bonebakker <bonebakker at comcast.net>

- Fixed bug in running() that arose when the called function
  really needed a minimum number of elements to work on.

- Added several new features to running(), it can now allow sequences
  shorter than the requested width to be present at the front, the back,
  or on both sides of the full lenth sequences.  This allows one to
  align the data so that the window is before, after, or around the
  indexed point.

- Add enhancements to estimable() provided by S??ren H??jsgaard
  <sorenh at agrsci.dk>:

  The estimable function now
  1) also works on geese and gee objects and
  2) can test hypotheses af the forb L * beta = beta0 both as a
     single Wald test and row-wise for each row in L.

- Add colorpanel function, which generates a smoothly varying band of
  colors over a three color range (lo, mid, high).



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From wasserberg at wisc.edu  Mon Apr 12 23:46:37 2004
From: wasserberg at wisc.edu (GIDEON WASSERBERG)
Date: Mon, 12 Apr 2004 16:46:37 -0500
Subject: [R] Matrix decomposition
Message-ID: <dd930fdd8fe8.dd8fe8dd930f@wiscmail.wisc.edu>

I am looking for a manual(s) or any kind of documentation, at the introductory level, regarding matrix algebra (specifically, matrix population models).

Any help will be highly appreciated

Gideon

Gideon Wasserberg (Ph.D.)
Wildlife research unit,
Department of wildlife ecology,
University of Wisconsin
218 Russell labs, 1630 Linden dr.,
Madison, Wisconsin 53706, USA.
Tel.:608 265 2130, Fax: 608 262 6099



From wwsprague at ucdavis.edu  Tue Apr 13 00:20:58 2004
From: wwsprague at ucdavis.edu (Webb Sprague)
Date: Mon, 12 Apr 2004 15:20:58 -0700
Subject: [R] Line numbers in error messages
In-Reply-To: <20040413101212.O2137@hortresearch.co.nz>
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
Message-ID: <407B164A.6040302@ucdavis.edu>

Hi Patrick,

>It's very simple using a browser() line in your function somewhere you
>know your code's OK, then run line by line.  
>
The problem is that sometimes you have code of a few hundred lines, to 
which you have added a strange little line that craps out because of 
some silly mistake that would be apparent if you knew which line to look 
at.  However....  you don't want to start inserting browser statements 
inside the code, hoping to get close, you just want to know what line 
caused the issue.

I already use ESS.  If it would give line numbers, my life would be perfect!

Is there something about debugging in S/R that I am missing?  I don't 
want to have to run a debugger except when there is a real problem, not 
just some silliness.

W



From rossini at blindglobe.net  Tue Apr 13 00:33:57 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 12 Apr 2004 15:33:57 -0700
Subject: [R] Line numbers in error messages
In-Reply-To: <407B02ED.EABB641F@statistik.uni-dortmund.de> (Uwe Ligges's
	message of "Mon, 12 Apr 2004 22:58:21 +0200")
References: <407AE4F6.5080006@ucdavis.edu>
	<407B02ED.EABB641F@statistik.uni-dortmund.de>
Message-ID: <85lll0akru.fsf@servant.blindglobe.net>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Webb Sprague wrote:
>> 
>> Hi all,
>> 
>> I have searched but to know avail.  Is there a way to get a line number
>> when a function crashes?  I am doing an edit->source->run cycle.
>
> I don't think so, but traceback(), debug() and options(error = recover)
> are your friends.
> Also, you might want to check out the CRAN package "debug".
>

There is always ESS's "load-source" command (C-c C-l, if I recall
correctly).


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jasont at indigoindustrial.co.nz  Tue Apr 13 00:36:48 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 13 Apr 2004 10:36:48 +1200
Subject: [R] Line numbers in error messages
In-Reply-To: <407B164A.6040302@ucdavis.edu>
References: <407AE4F6.5080006@ucdavis.edu>	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
Message-ID: <407B1A00.5070608@indigoindustrial.co.nz>

Webb Sprague wrote:
> I already use ESS.  If it would give line numbers, my life would be 
> perfect!

It does, if you don't use source().  Have your R code in one buffer, and 
the R session running, and (from the code buffer), type C-c C-l.

Cheers

Jason



From tlumley at u.washington.edu  Tue Apr 13 01:04:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 16:04:25 -0700 (PDT)
Subject: [R] Line numbers in error messages
In-Reply-To: <407B1A00.5070608@indigoindustrial.co.nz>
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
	<407B1A00.5070608@indigoindustrial.co.nz>
Message-ID: <Pine.A41.4.58.0404121601240.86522@homer10.u.washington.edu>

On Tue, 13 Apr 2004, Jason Turner wrote:

> Webb Sprague wrote:
> > I already use ESS.  If it would give line numbers, my life would be
> > perfect!
>
> It does, if you don't use source().  Have your R code in one buffer, and
> the R session running, and (from the code buffer), type C-c C-l.
>

Yes, yes, very nice. But if the error is inside a function (and isn't a
syntax error) then this will only give you the line where the function was
called.


	-thomas



From andy_liaw at merck.com  Tue Apr 13 01:04:08 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 12 Apr 2004 19:04:08 -0400
Subject: [R] Matrix decomposition
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B90@usrymx25.merck.com>

I don't know anything about matrix population models, but Golub & Van Loan's
`Matrix Computations' is a standard reference for numerical computations
with matrices.  There's also David Harville's fairly recent book, `Matrix
Algebra from Statisticians' Perspective'.  I believe one of James Gentle's
recent volumes also cover numerical linear algebra.

HTH,
Andy

> GIDEON WASSERBERG
> 
> I am looking for a manual(s) or any kind of documentation, at 
> the introductory level, regarding matrix algebra 
> (specifically, matrix population models).
> 
> Any help will be highly appreciated
> 
> Gideon
> 
> Gideon Wasserberg (Ph.D.)
> Wildlife research unit,
> Department of wildlife ecology,
> University of Wisconsin
> 218 Russell labs, 1630 Linden dr.,
> Madison, Wisconsin 53706, USA.
> Tel.:608 265 2130, Fax: 608 262 6099
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From phddas at yahoo.com  Tue Apr 13 01:05:25 2004
From: phddas at yahoo.com (Fred J.)
Date: Mon, 12 Apr 2004 16:05:25 -0700 (PDT)
Subject: [R] fractal calculation using fdim
Message-ID: <20040412230525.20460.qmail@web20515.mail.yahoo.com>

Hello
I am getting this
Error in slopeopt(AllPoints, Alpha) : Object "LineP"
not found
when the dim(<data.frame>) of the data matrix for
which the fdim is being calculated hits a certain
values depending on the data set. e.g
print(dim(mat))
fd <- fdim(mat,q=2)
[1] 2743    2
[1] 2742    3
[1] 2741    4
[1] 2740    5
[1] 2739    6
[1] 2738    7
[1] 2737    8
[1] 2736    9
[1] 2735   10
[1] 2734   11
[1] 2733   12
Error in slopeopt(AllPoints, Alpha) : Object "LineP"
not found
if I change the data set to mat2 with totally
different values I would get the above error at a
dim(mat) of say
2693  52

thanks for any input.



From dmurdoch at pair.com  Tue Apr 13 01:41:05 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 12 Apr 2004 19:41:05 -0400
Subject: [R] Line numbers in error messages
In-Reply-To: <407B164A.6040302@ucdavis.edu>
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
Message-ID: <6v9m70l9psl58cgk6sglgv6rabiiiv2bun@4ax.com>

On Mon, 12 Apr 2004 15:20:58 -0700, you wrote:

>Hi Patrick,
>
>>It's very simple using a browser() line in your function somewhere you
>>know your code's OK, then run line by line.  
>>
>The problem is that sometimes you have code of a few hundred lines, to 
>which you have added a strange little line that craps out because of 
>some silly mistake that would be apparent if you knew which line to look 
>at.  However....  you don't want to start inserting browser statements 
>inside the code, hoping to get close, you just want to know what line 
>caused the issue.

This is something that's on my wish list too, but it would require
fairly low-level changes.  Right now the parser doesn't record source
file information on a line, so there's no way an error message could
report it.

It's not absolutely obvious how to do it, either:  code can come from
files, from saved images, from stuff you typed at the console prompt,
from a connection, as the result of evaluating an expression, etc.
It's a lot more complicated to do this in an interpreted language like
R than in a compiled language.

Duncan Murdoch



From tplate at blackmesacapital.com  Tue Apr 13 01:53:10 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 12 Apr 2004 17:53:10 -0600
Subject: [R] Line numbers in error messages
In-Reply-To: <6v9m70l9psl58cgk6sglgv6rabiiiv2bun@4ax.com>
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
	<6v9m70l9psl58cgk6sglgv6rabiiiv2bun@4ax.com>
Message-ID: <6.1.0.6.2.20040412174751.0353afc0@mailhost.blackmesacapital.com>

Isn't source file information often recorded in the "source" attribute on 
functions (or calls)?  Could either the execution engine or the debugger 
refer to that information?  (Though, in the debugger it might be impossible 
to uniquely identify expressions that appear multiple times in the function 
code.)  If line info was printed out only when source was saved in the 
"source" attribute, this could still be useful.

-- Tony Plate


At Monday 05:41 PM 4/12/2004, Duncan Murdoch wrote:
>On Mon, 12 Apr 2004 15:20:58 -0700, you wrote:
>
> >Hi Patrick,
> >
> >>It's very simple using a browser() line in your function somewhere you
> >>know your code's OK, then run line by line.
> >>
> >The problem is that sometimes you have code of a few hundred lines, to
> >which you have added a strange little line that craps out because of
> >some silly mistake that would be apparent if you knew which line to look
> >at.  However....  you don't want to start inserting browser statements
> >inside the code, hoping to get close, you just want to know what line
> >caused the issue.
>
>This is something that's on my wish list too, but it would require
>fairly low-level changes.  Right now the parser doesn't record source
>file information on a line, so there's no way an error message could
>report it.
>
>It's not absolutely obvious how to do it, either:  code can come from
>files, from saved images, from stuff you typed at the console prompt,
>from a connection, as the result of evaluating an expression, etc.
>It's a lot more complicated to do this in an interpreted language like
>R than in a compiled language.
>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.connolly at hortresearch.co.nz  Tue Apr 13 02:10:46 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 13 Apr 2004 12:10:46 +1200
Subject: [R] Line numbers in error messages
In-Reply-To: <407B164A.6040302@ucdavis.edu>;
	from wwsprague@ucdavis.edu on Mon, Apr 12, 2004 at 03:20:58PM
	-0700
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
Message-ID: <20040413121046.P2137@hortresearch.co.nz>

On Mon, 12-Apr-2004 at 03:20PM -0700, Webb Sprague wrote:

|> Hi Patrick,
|> 
|> >It's very simple using a browser() line in your function somewhere you
|> >know your code's OK, then run line by line.  
|> >
|> The problem is that sometimes you have code of a few hundred lines, to 
|> which you have added a strange little line that craps out because of 
|> some silly mistake that would be apparent if you knew which line to look 
|> at.  However....  you don't want to start inserting browser statements 
|> inside the code, hoping to get close, you just want to know what line 
|> caused the issue.

Many things might be better if they were different, but they aren't.
Just count your blessings you're using something as easy to delve into
as R code.  

I don't find it difficult to work out where the problem is.  The
traceback() stack gives an idea which line of your function had a
problem.  I put a browser just before that, and check if objects are
what I thought they'd be.

Sometimes it might involve adding a browser to a called function, and
sometimes the problem is a long way back.  (I've had lots of
experience doing this sort of thing.)

Even if one does have to go through line by line, unless you have lots
of long calculations, it's very quick to move through scores of
lines of code using C-c C-n.  It's even possible to run a number of
lines in one go to work out which bunch the problem is in, then you
can look further within that bunch if it's the one that produces an
error (it won't have crashed).

I've found it much quicker to find errors in R code than I do in perl code.


|> 
|> I already use ESS.  If it would give line numbers, my life would be perfect!

I think you'd then find something else that wasn't. :-)

|> 
|> Is there something about debugging in S/R that I am missing?  I don't 
|> want to have to run a debugger except when there is a real problem, not 
|> just some silliness.

In my experience, it silliness 99% of the time.

Best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From bacolli at uark.edu  Tue Apr 13 02:06:32 2004
From: bacolli at uark.edu (Bret Collier)
Date: Mon, 12 Apr 2004 19:06:32 -0500
Subject: [R] Matrix decomposition
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B90@usrymx25.merck.co m>
Message-ID: <5.2.1.1.0.20040412190229.00b156f0@mail.uark.edu>

Gideon,

You might look into Caswell, H.  2001.  Matrix population models 2nd 
edition. Sinauer Associates.

Caswell's book covers a majority of matrix population modeling, including 
some good introductory information and has general matrix manipulation as 
an appendix.  I think most of the work in this book was done in Matlab.

HTH,
Bret



> > GIDEON WASSERBERG
> >
> > I am looking for a manual(s) or any kind of documentation, at
> > the introductory level, regarding matrix algebra
> > (specifically, matrix population models).
> >
> > Any help will be highly appreciated
> >
> > Gideon
> >
> > Gideon Wasserberg (Ph.D.)
> > Wildlife research unit,
> > Department of wildlife ecology,
> > University of Wisconsin
> > 218 Russell labs, 1630 Linden dr.,
> > Madison, Wisconsin 53706, USA.
> > Tel.:608 265 2130, Fax: 608 262 6099
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Bret



From tlumley at u.washington.edu  Tue Apr 13 02:32:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 12 Apr 2004 17:32:29 -0700 (PDT)
Subject: [R] Line numbers in error messages
In-Reply-To: <6.1.0.6.2.20040412174751.0353afc0@mailhost.blackmesacapital.com>
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
	<6v9m70l9psl58cgk6sglgv6rabiiiv2bun@4ax.com>
	<6.1.0.6.2.20040412174751.0353afc0@mailhost.blackmesacapital.com>
Message-ID: <Pine.A41.4.58.0404121718260.86522@homer10.u.washington.edu>

On Mon, 12 Apr 2004, Tony Plate wrote:

> Isn't source file information often recorded in the "source" attribute on
> functions (or calls)?  Could either the execution engine or the debugger
> refer to that information?  (Though, in the debugger it might be impossible
> to uniquely identify expressions that appear multiple times in the function
> code.)  If line info was printed out only when source was saved in the
> "source" attribute, this could still be useful.

Yes, but it's still not that straightforward.  The "source" attribute is
just text. The execution engine doesn't know where it is in the text, and
there's no guarantee that, for example, deparse() on an expression will
return a substring of the original text.  And that's before things get
really complicated.

Consider:
> lm(y~x)
Error in eval(expr, envir, enclos) : Object "y" not found
> traceback()
7: eval(expr, envir, enclos)
6: eval(predvars, data, env)
5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)
4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
3: eval(expr, envir, enclos)
2: eval(mf, parent.frame())
1: lm(y ~ x)

The majority of these expressions (lines 3, 4, 5 and 7) do not appear
anywhere in any of the functions being called.

Now, it might well be possible to produce something that would give
source line numbers where they were available, with a few false negatives.
This would be an interesting project, but it isn't trivial.

	-thomas



> -- Tony Plate
>
>
> At Monday 05:41 PM 4/12/2004, Duncan Murdoch wrote:
> >On Mon, 12 Apr 2004 15:20:58 -0700, you wrote:
> >
> > >Hi Patrick,
> > >
> > >>It's very simple using a browser() line in your function somewhere you
> > >>know your code's OK, then run line by line.
> > >>
> > >The problem is that sometimes you have code of a few hundred lines, to
> > >which you have added a strange little line that craps out because of
> > >some silly mistake that would be apparent if you knew which line to look
> > >at.  However....  you don't want to start inserting browser statements
> > >inside the code, hoping to get close, you just want to know what line
> > >caused the issue.
> >
> >This is something that's on my wish list too, but it would require
> >fairly low-level changes.  Right now the parser doesn't record source
> >file information on a line, so there's no way an error message could
> >report it.
> >
> >It's not absolutely obvious how to do it, either:  code can come from
> >files, from saved images, from stuff you typed at the console prompt,
> >from a connection, as the result of evaluating an expression, etc.
> >It's a lot more complicated to do this in an interpreted language like
> >R than in a compiled language.
> >
> >Duncan Murdoch
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From frohde_home at yahoo.com  Tue Apr 13 03:53:26 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Mon, 12 Apr 2004 18:53:26 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <Pine.A41.4.58.0404121416560.86522@homer10.u.washington.edu>
Message-ID: <20040413015326.14864.qmail@web60107.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040412/5822d6e7/attachment.pl

From akhan at cs.otago.ac.nz  Tue Apr 13 04:50:35 2004
From: akhan at cs.otago.ac.nz (Anar Khan)
Date: Tue, 13 Apr 2004 14:50:35 +1200 (NZST)
Subject: [R] Specifying RHOME
Message-ID: <Pine.OSF.4.44.0404131424210.219398-100000@hermes.otago.ac.nz>

Hi,

I am having trouble installing R-1.8.1 on my Fedora 1 machine.  R is
already installed in /usr/local on our network, but I wish to be able to
work on my own copy.  So I installed it locally using:

./configure --prefix=/scratch/akhan/R-1.8.1
make
make install

The installation looks fine according to 'make check'.

But when I run R I get:
Fatal error: unable to open the base package

When I echo RHOME using 'R RHOME' I get:
/usr/local/lib/R

I then tried setting $R_HOME to /scratch/akhan/R-1.8.1, and adding this
dir to PATH in my .tcshrc file.  Echoing RHOME gives the same response as
before:
WARNING: ignoring environment value of R_HOME
/usr/local/lib/R

The 2 copies of R are obviously causing confusion - is there any way of
manually overriding RHOME so that I can run R locally?

Thanks,

Anar



From rbaer at atsu.edu  Tue Apr 13 05:20:55 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Mon, 12 Apr 2004 22:20:55 -0500
Subject: [R] fractal calculation using fdim
References: <20040412230525.20460.qmail@web20515.mail.yahoo.com>
Message-ID: <006001c42106$558a1690$6401a8c0@meadow>

Well I must say that I am still struggling to understand exactly how fdim
works, and what form the first argument is to take.  Presumably the function
produces something like 1-slope for a log log plot as a fractal dimension.
I guess it is really the makeline() function I don't understand from the
documentation.  Is that how you got your data or are you using real data?

That said, I note in the help that the first argument X is to be a dataframe
not a matrix  (mat??).  Could that be giving you problems?  Do you get
better results with as.data.frame(mat)?

Rob
----- Original Message ----- 
From: "Fred J." <phddas at yahoo.com>
To: "r help" <r-help at stat.math.ethz.ch>
Sent: Monday, April 12, 2004 6:05 PM
Subject: [R] fractal calculation using fdim


> Hello
> I am getting this
> Error in slopeopt(AllPoints, Alpha) : Object "LineP"
> not found
> when the dim(<data.frame>) of the data matrix for
> which the fdim is being calculated hits a certain
> values depending on the data set. e.g
> print(dim(mat))
> fd <- fdim(mat,q=2)
> [1] 2743    2
> [1] 2742    3
> [1] 2741    4
> [1] 2740    5
> [1] 2739    6
> [1] 2738    7
> [1] 2737    8
> [1] 2736    9
> [1] 2735   10
> [1] 2734   11
> [1] 2733   12
> Error in slopeopt(AllPoints, Alpha) : Object "LineP"
> not found
> if I change the data set to mat2 with totally
> different values I would get the above error at a
> dim(mat) of say
> 2693  52
>
> thanks for any input.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From jarioksa at sun3.oulu.fi  Tue Apr 13 07:32:30 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 13 Apr 2004 08:32:30 +0300
Subject: [R] question on isoMDS
In-Reply-To: <5.1.0.14.0.20040412134639.009fdb20@pop3.web.de>
References: <5.1.0.14.0.20040412134639.009fdb20@pop3.web.de>
Message-ID: <1081834350.3844.11.camel@biol102145.oulu.fi>

Helmut Kudrnovsky,

This issue was discussed on this list recently:  Search for messages
with Subject: "Fwd: MDS problems" at and after 26 March. Brief answer to
your question is: no, you cannot run isoMDS in MASS if you have zero or
negative dissimilarities. You should remove identical cases. After all,
you know that in isoMDS, identical cases will be identical even after
NMDS and you can locate your removed points over remaining points in the
plot (there is a subtle weighting issue, though).

The canonical solution is to remove the duplicates before calculating
distances:

test.bc <- dist(unique(test)) 

(or you can use vegdist in place of dist, like you seemingly did
previously).

An alternative discussed previously would be to remove the offending
dissimilarities. The following simple-minded function seems to do the
job:

unique.dist <-
function (x, incomparables = FALSE, ...)
{
    if (!is.logical(incomparables) || incomparables)
        .NotYetUsed("incomparables != FALSE")
    is.na(x) <- x <= 0
    x <- as.matrix(x)
    x[upper.tri(x)] <- 0
    take <- complete.cases(x)
    as.dist(x[take,take])
}

If test.bc is your dist structure, you can make it `unique' with:

test.bcuniq <- unique(test.bc)

The name of the function may be confusing: what's `unique' in removing
zeros? However, the following seems to hold:

dist(unique(x)) == unique(dist(x))

which would justify the name.

The previous discussion mentioned some dirty tricks, too.

cheers, jari oksanen

On Mon, 2004-04-12 at 15:21, Helmut Kudrnovsky wrote:

> I have a question on isoMDS.
> 
> My data set (of vegetation) with 210 samples is in this way:
> 
>                Rotfoehrenau Lavendelweidenau Silberweidenau ....
> 067_Breg.7               0                2              0         ....
> 071_Dona.4              0                2              6          ....
> ...
> 
> I want to do an isoMDS-analysis with the dissimilarity index "bray/curtis" 
> as discribed in the help-files of the package vegan:
> 
> mds.test <- isoMDS(test.bc, initMDS(test.bc), maxit=200, trace=FALSE, tol=1e-7)
> 
> Then I get following error message:
> 
> "Error in isoMDS(test.bc, initMDS(test.bc), maxit = 200, trace = FALSE,  :
>          zero or negative distance between objects 19 and 20"
> 
> The objects 19 and 20 have the same variables in the same way and therefore 
> they have a zero distance.
> 
> My question is: Is it possible to perform an isoMDS with a data set where 
> some samples have the same variables in the same way?
> 

-- 
J.Oksanen, Oulu, Finland.
"Object-oriented programming is an exceptionally bad idea which could
only have originated in California." E. Dijkstra



From dmurdoch at pair.com  Mon Apr 12 21:37:40 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 12 Apr 2004 15:37:40 -0400
Subject: [R] R 1.9.0 is release
In-Reply-To: <x2oepxl7ui.fsf@biostat.ku.dk>
References: <x2oepxl7ui.fsf@biostat.ku.dk>
Message-ID: <ikrl705qgqg7eiqo7gvkt8vbp4l248mscr@4ax.com>

On 12 Apr 2004 14:05:25 +0200, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>I've rolled up R-1.9.0.tgz a short while ago. This is a new version
>with a number of new features, most notably a substantial
>reorganization of the standard packages, a major update of the grid
>package, and the fact that underscore can now be used as a regular
>character in variable names.

I've just uploaded the Windows build.  It should appear on CRAN and
the mirrors by tomorrow.

The main Windows-specific changes are the following:

 - A "stay on top" option for windows.
 - Rcmd can now be written R CMD, as on Unix.
 - Tony Plate's "Paste commands only" to paste the commands from a
copied block of output
 
There are many other changes and bug fixes.  Here's an extract from
the CHANGES file:

rw1090
======

Both Rterm and Rgui now give usage information via the --help or -h
command-line flag.

There is now a "Misc|Break to debugger" menu option, enabled 
when a debugger is detected (somewhat fallibly), or infallibly by the
"--debug" command line option.  This will cause a trap to an external
debugger, e.g. for running Rgui under gdb.  If the menu item is
selected when not running under a debugger R is likely to crash.
If the "--debug" option is used, R will break to the debugger during
command line processing, allowing the startup process to be debugged.

Added "stay" argument to bringToTop(), to allow the user to specify
that a window should stay on top of other windows.  Also added "stay
on top" item to the popup menus.  All of these require R to be running
in SDI mode ("Rgui --sdi" or via the settings in file `Rconsole').

Changed windows() so that new windows fit within the MDI client
area.

Added winMenuNames() and winMenuItems() functions to query user menus.

Added menu items for www.r-project.org and CRAN on the help menu. 
(Wishlist PR#6492)

Added "R" command to be similar to Unix invocation of scripts, e.g.
"R CMD INSTALL" is the same as "Rcmd INSTALL".  Rcmd still exists for 
backwards compatibility (and to avoid conflicts over the name `R').
All of R, R CMD and Rcmd now accept --help.

Rcmd Rd2dvi can now be specified as such rather than as Rcmd
Rd2dvi.sh.

Added "Paste commands only" to edit and popup menus in the Rgui
console.
This allows copying of a block of output, but pasting only the
commands
back to the console for re-execution.  (Code contributed by Tony
Plate.)


Installation
------------

Parallel make (make -j2, say) can be used, but only usefully on
dual-processor (or perhaps hyperthreaded) hosts with at least 384Mb of
memory.

Installing now sorts in the C locale to ensure that a consistent sort
order is used.  (Some aspects of sorting used to be done in the locale
of the host machine, but Perl and the cygwin-based tools used the
ASCII collation order.)

The long-untested support for making Windows .hlp files has been
withdrawn.

There is support for using K. Goto's fast BLAS.  On a 2.6Ghz P4 with
1Gb RAM and A a 1000 x 1000 matrix we had the following timings

	R BLAS	ATLAS	Goto
A %*% A	 3.7	0.65	0.56
svd(A)	16.2	7.77	6.83

Note that using a fast BLAS is much less effective for smaller
matrices as are more common in statistical applications.

Faster assembler code for exponentiation is used.

Cross-building of R itself now works again.  (It had been broken since
1.8.0.)


Building/installing packages
----------------------------

R CMD INSTALL/build/check map path names with spaces in to their short
forms.

R CMD INSTALL now supports versioned install via
--with-package-versions.

Installing (binary) package bundles now checks the MD5 sums and
reports success, just as for packages.

Added "* DONE" to the end of INSTALL logs so --install option to CHECK
will work. (This is a repository maintainer option; see 
src/scripts/check.in for docs).


Internal changes
----------------

The fast bmp/png/jpeg code introduced in R 1.8.0 is used even for
256-color displays (as we have now been able to test it on such).

R's internal malloc etc are now remapped to Rm_malloc etc and only
used in allocating memory for R objects, the Wilcoxon tests and a few
other memory-intensive applications.

Improved malloc routines from the current version of Doug Lea's malloc
(as suggested by David Teller) should enable large memory areas to be
used more effectively, in particular those over 2Gb where OS support
has been enabled.  The initially requested memory is no longer
reserved, but as this malloc is able to work with non-contiguous
memory chunks that should not matter.

The installer uses LZMA compression, so Inno Setup >= 4.1.5 is
required.

Version 1.2.5 of libpng is now used in binary builds.


Bug fixes
---------

Fixed list.files() to properly handle paths like "C:", etc.

Fixed unlink() to accept empty file list for Unix consistency.

Fixed handling of whitespace in Rd2dvi.sh processing of DESCRIPTION
files.

Fixed handling of "--max-mem-size" syntax error on command line.

In RGui, ^T would not transpose the first and second characters on a
line.  (PR#5593)

Fixed junk character at start of a pipe(). (PR#5053)

R CMD SHLIB was computing dependencies for all C files, not just those
specified on the command line, and building the DLL from all *.o files
in the directory.

The sizes of metafiles in pixels were often coming out one pixel more
than requested, so the background was not quite all painted.  Now we
over-estimate.

Rproxy.dll would cause a crash when transferring large amounts of
data.

Workaround for Microsoft's esoteric idea that date-times in 1970
before
1970-01-01 00:00:00 GMT are invalid, so as.POSIXct("1970-01-01
00:00:00") failed in timezones ahead of GMT.

Avoid possible segfault with browseURL() on urls of more than 264
chars
(although these are not guaranteed to work).

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From ivan_yegorov at mail.ru  Tue Apr 13 12:51:36 2004
From: ivan_yegorov at mail.ru (=?koi8-r?Q?=22?=Ivan Yegorov=?koi8-r?Q?=22=20?=)
Date: Tue, 13 Apr 2004 14:51:36 +0400
Subject: [R] Question
Message-ID: <E1BDLVw-000Mur-00.ivan_yegorov-mail-ru@f11.mail.ru>

I use R for Windows. I got error "Can not allocate 100 Mb for vector". Does R use only physical memory or it can operate with virtual memory? What should I do if it does. Thanks in advance.



From santosh at igidr.ac.in  Tue Apr 13 13:17:43 2004
From: santosh at igidr.ac.in (Santosh Kumar)
Date: Tue, 13 Apr 2004 16:47:43 +0530 (IST)
Subject: [R] mts
Message-ID: <Pine.OSF.4.30.0404131647140.367082-100000@brahma.igidr.ac.in>

Hi!

I am new to R. I need your help.

I have got the time series of fifteen variables in data file. I would like
to plot it in R in separate ps pages, not in same ps page.

I was reading about mts, but I could not figure out how to do it.

Can anyone help me out?

with regards;
Santosh
--------------------------------------------------------------------------
Santosh Kumar                         URL http://www.igidr.ac.in/~santosh/
PhD Student
Indira Gandhi Institute of Development Research
Gen A. K. Vaidya Marg
Goregaon ( East )
Mumbai pin 400065 India
Phone 28400919



From DANSEN at voeding.tno.nl  Tue Apr 13 13:25:58 2004
From: DANSEN at voeding.tno.nl (Dansen, Ing. M.C.)
Date: Tue, 13 Apr 2004 13:25:58 +0200
Subject: [R] from .csv file  to a pca plot
Message-ID: <3B070848E7C2204F9DEB8BCFD767728003CF83A4@ntexch1.voeding.tno.nl>

Hi,
I'm just a beginner, who has just encountered a problem!

1. -	I wanted to load a csv file with names in the rows (1st column)
   	and and numbers in the 2nd til 10th column. The file contains
names in the headers.

   - 	I used;  a <- as.matrix(read.table("filename", sep=',",
row.names=1, header=TRUE)

Question;	1 -	I would like to select the first four columns
		2 -	and execute a pca(plot) from the mva package on
those four columns
		3 -	how can I set the data type eg(string, integer,
double) separate for each column

Can anyone help me out, Help!

Thanks in advance,

Marinus

 

 
 

This e-mail and its contents are subject to the DISCLAIMER at http://www.tno.nl/disclaimer/email.html



From petr.pikal at precheza.cz  Tue Apr 13 13:39:20 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Apr 2004 13:39:20 +0200
Subject: [R] par() in .Rprofile
Message-ID: <407BED88.16934.13652FE@localhost>

Dear all

I installed new version (from binaries) and I noticed that 

par(bg="white")

which I have in my .Rprofile causes error message on startup
But if I issued this command immediately after startup everything worked as 
expected. I did not see any note in changes file or elsewhere. 

Should I specify white background in .Rprofile differently?
Or is there some other recommended way to set up white background on startup?
Everything was OK in 1.8.1 version.

Using W2000.


Startup example

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3

<snip>

Attaching package 'fun':

        The following object(s) are masked from package:base :

         interaction 

Error: couldn't find function "par"
[Previously saved workspace restored]

> par("bg")
[1] "transparent"
> par(bg="white")
> par("bg")
[1] "white"
>

my .Rprofile

library(fun)
par(bg="white")
RNGkind("Mersenne-Twister", "Inversion")
data(stand)

Thank you

Best regards.

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Tue Apr 13 13:46:55 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Apr 2004 13:46:55 +0200
Subject: [R] Question
In-Reply-To: <E1BDLVw-000Mur-00.ivan_yegorov-mail-ru@f11.mail.ru>
Message-ID: <407BEF4F.7025.13D43E8@localhost>

Hi


On 13 Apr 2004 at 14:51, Ivan Yegorov wrote:

> I use R for Windows. I got error "Can not allocate 100 Mb for vector".
> Does R use only physical memory or it can operate with virtual memory?
> What should I do if it does. Thanks in advance.

Starting R with

--max-mem-size 550M

option can help.

The figure depends on your memory size.

Newer versions are better in using memmory resources.

Cheers
Petr

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Detlef.Steuer at UniBw-Hamburg.DE  Tue Apr 13 13:51:55 2004
From: Detlef.Steuer at UniBw-Hamburg.DE (Detlef Steuer)
Date: Tue, 13 Apr 2004 13:51:55 +0200
Subject: [R] par() in .Rprofile
In-Reply-To: <407BED88.16934.13652FE@localhost>
References: <407BED88.16934.13652FE@localhost>
Message-ID: <20040413135155.43b942ab@gaia.unibw-hamburg.de>


Hi,

it's in the release notes from Peter:

>Users may notice that code in .Rprofile is run with only the
>new base loaded and so functions may now not be found.	For
>example, ps.options(horizontal = TRUE) should be preceded by
>library(graphics) or called as graphics::ps.options or,
>better, set as a hook -- see ?setHook.

detlef


On Tue, 13 Apr 2004 13:39:20 +0200
"Petr Pikal" <petr.pikal at precheza.cz> wrote:

> Dear all
> 
> I installed new version (from binaries) and I noticed that 
> 
> par(bg="white")
> 
> which I have in my .Rprofile causes error message on startup
> But if I issued this command immediately after startup everything worked as 
> expected. I did not see any note in changes file or elsewhere. 

-- 
Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****

"Die herrschenden Ideen sind die Ideen der Herrschenden."
--- K. Marx



From petr.pikal at precheza.cz  Tue Apr 13 13:55:02 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 13 Apr 2004 13:55:02 +0200
Subject: [R] from .csv file  to a pca plot
In-Reply-To: <3B070848E7C2204F9DEB8BCFD767728003CF83A4@ntexch1.voeding.tno.nl>
Message-ID: <407BF136.9742.144B3D7@localhost>

Hallo

On 13 Apr 2004 at 13:25, Dansen, Ing. M.C. wrote:

> Hi,
> I'm just a beginner, who has just encountered a problem!
> 
> 1. -	I wanted to load a csv file with names in the rows (1st column)
>     and and numbers in the 2nd til 10th column. The file contains
> names in the headers.
> 
>    - 	I used;  a <- as.matrix(read.table("filename", sep=',",
> row.names=1, header=TRUE)

maybe read.csv("filename") will do the same without need for other 
specifications.

Why do you convert it to matrix? What is wrong with data frame?

> 
> Question;	1 -	I would like to select the first four columns
a[1:4,]

>   2 -	and execute a pca(plot) from the mva package on
> those four columns

go through examples in mva

>   3 -	how can I set the data type eg(string, integer,
> double) separate for each column

During loading process you can use colClasses argument to read.csv or 
read.table.

Or you can change the column type by as.xxxxxxx statement

> 
> Can anyone help me out, Help!
> 
> Thanks in advance,
> 
> Marinus

Cheers
Petr


> 
> 
> 
> 
> 
> 
> This e-mail and its contents are subject to the DISCLAIMER at
> http://www.tno.nl/disclaimer/email.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ml-r-help at epigenomics.com  Tue Apr 13 14:14:52 2004
From: ml-r-help at epigenomics.com (Matthias Burger)
Date: Tue, 13 Apr 2004 14:14:52 +0200
Subject: class of seq(length=n) values (was Re: [R] Zero Index Origin?)
In-Reply-To: <Pine.LNX.4.44.0403310923170.712-100000@gannet.stats>
References: <406A7E37.5060504@arcanemethods.com>
	<Pine.LNX.4.44.0403310923170.712-100000@gannet.stats>
Message-ID: <407BD9BC.4070700@epigenomics.com>


Dear Brian Ripley,

following your advice (very much appreciated) of using seq(length=n) instead of 
1:n constructs in loop constructions
(usually prepended by the not so elegant if (n > 0))
I was somewhat surprised to find that (in R 1.8.1 & R 1.9.0 beta (2004-03-29) on 
Debian 3.0)
and using the methods package

carrel>a <- seq(1,5)
carrel>class(a)
[1] "integer"
carrel>class(seq(along=a))
[1] "integer"

but
carrel>class(seq(length=length(a)))
[1] "numeric"


and from ?seq
...
Value:

      The result is of 'mode' '"integer"' if 'from' is (numerically
      equal to an) integer and 'by' is not specified.
...
it is not (to me, of course) obvious that if length is specified and from 
omitted a numeric result sequence is returned.

This might only matter in conjunction with S4 class slot assignments where the 
integer class requirement is important.
OTOH a
for (i in as.integer(seq(length=length(a)))) { ... }
is not so elegant either.

Could you please enlighten me as to why this behaviour was chosen and if there 
is any more elegant way than using

as.integer(seq(length=length(a)))
to get the desired result (given that I have to use a loop in the first place).


Regards,

   Matthias



Prof Brian Ripley wrote:
> Much of R is itself written in R, so you cannot possibly change something 
> as fundamental as this.  Further, index 0 has a special meaning that you 
> would lose if R have 0-based indexing.
> 
> However, the R thinking is to work with whole objects (vectors, arrays, 
> lists ...) and you rather rarely need to know what numbers are in an index 
> vector.  There are usages such as 1:n, and those are quite often wrong: 
> they should be seq(length=n) or seq(along=x) or some such, since n might 
> be zero.  If you are writing code that works with single elements, you are 
> probably a lot better off writing C code to link into R (and C is 
> 0-based ...).
> 
> On Wed, 31 Mar 2004, Bob Cain wrote:
> 
> 
>>I'm very new to R and utterly blown away by not only the 
>>language but the unbelievable set of packages and the 
>>documentation and the documentation standards and...
>>
>>I was an early APL user and never lost my love for it and in 
>>R I find most of the essential things I loved about APL 
>>except for one thing.  At this early stage of my learning I 
>>can't yet determine if there is a way to effect what in APL 
>>was zero index origin, the ordinality of indexes starts with 
>>0 instead of 1.  Is it possible to effect that in R without 
>>a lot of difficulty?
>>
>>I come here today from the world of DSP research and 
>>development where Matlab has a near hegemony.  I see no 
>>reason whatsoever that R couldn't replace it with a _far_ 
>>better and _far_ less idiosyncratic framework.  I'd be 
>>interested in working on a Matlab equivalent DSP package for 
>>R (if that isn't being done by someone) and one of the 
>>things most criticized about Matlab from the standpoint of 
>>the DSP programmer is its insistence on 1 origin indexing. 
>>Any feedback greatly appreciated.
>>
>>
>>Thanks,
>>
>>Bob
>>
> 
> 

-- 
Matthias Burger

Bioinformatics R&D
Epigenomics AG                      www.epigenomics.com
Kleine Pr??sidentenstra??e 1          fax:   +49-30-24345-555
10178 Berlin Germany                phone: +49-30-24345-0



From sunnyho at ust.hk  Tue Apr 13 14:25:45 2004
From: sunnyho at ust.hk (Sunny Ho)
Date: Tue, 13 Apr 2004 20:25:45 +0800
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <200404131011.i3DA07u3025004@hypatia.math.ethz.ch>
Message-ID: <200404131225.UAA01688@webmaild.ust.hk>

Hello everyone,

I would like to get some advices on using R with some really large datasets.

I'm using RH9 Linux R 1.8.1 for a research with a lot of numerical data. The datasets total to around 200Mb (shown by memory.size). During my data manipulation, the system memory usage grew to 1.5Gb, and this caused a lot of swapping activities on my 1Gb PC. This is just a small-scale experiment, the full-scale one will be using data 30 times as large (on a 4Gb machine). I can see that I'll need to deal with memory usage problem very soon.

I notice that R keeps all datasets in memory at all times. I wonder whether there is any way to instruct R to push some of the less-frequently-used data tables out of main memory, so as to free up memory for those that are actively in used. It'll be even better if R can keep only part of a table in memory only when that part is needed. Using save & load could help, but I just wonder whether R is intelligent enough to do this by itself, so I don't need to keep track of memory usage at all times.

Another thought is to use a 64-bit machine (AMD64). I find there is a pre-compiled R for Fedora Linux on AMD64. Anyone knows whether this version of R runs as 64-bit? If so, then will R be able to go beyond the 32-bit 4Gb memory limit?

Also, from the manual, I find that the RPgSQL package (for PostgreSQL database) supports a feature "proxy data frame". Does anyone have experience with this? Can "proxy data frame" handle memory efficiently for very large datasets? Say, if I have a 6Gb database table defined as a proxy data frame, will R & RPgSQL be able to handle it with just 4Gb of memory?

Any comments will be useful. Many thanks.

Sunny Ho
(Hong Kong University of Science & Technology)



From talitaperciano at hotmail.com  Tue Apr 13 15:21:50 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Tue, 13 Apr 2004 10:21:50 -0300
Subject: [R] model-based clustering
Message-ID: <BAY14-F65oVEmK3xvp60003df97@hotmail.com>

Hello,

I'm trying to use the model-based clustering functions R provides but i'm 
having some difficulties. Does anybody could help me how to make a good 
analisys of a data set with these functions??



From frohde_home at yahoo.com  Tue Apr 13 15:24:28 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Tue, 13 Apr 2004 06:24:28 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <20040413015326.14864.qmail@web60107.mail.yahoo.com>
Message-ID: <20040413132428.93671.qmail@web60103.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040413/b98aa421/attachment.pl

From talitaperciano at hotmail.com  Tue Apr 13 15:38:22 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Tue, 13 Apr 2004 10:38:22 -0300
Subject: [R] model-based clustering
Message-ID: <BAY14-F56uBhvuc8O7600016628@hotmail.com>

Hello,

I'm trying to use the model-based clustering functions but i'm having some 
difficulties. Does anybody could help me to make a good analisys of a data 
set using these functions??



From santosh at igidr.ac.in  Tue Apr 13 15:46:33 2004
From: santosh at igidr.ac.in (Santosh Kumar)
Date: Tue, 13 Apr 2004 19:16:33 +0530 (IST)
Subject: [R] mts
In-Reply-To: <Pine.OSF.4.30.0404131647140.367082-100000@brahma.igidr.ac.in>
Message-ID: <Pine.OSF.4.30.0404131914380.367263-100000@brahma.igidr.ac.in>

Hi!

 I am new to R. I need your help.

 I have got the time series of fifteen variables in data file. I would like
 to plot it in R in separate ps pages, not in same ps page.

 I was reading about mts, but I could not figure out how to do it.

 Can anyone help me out?

 with regards;
 Santosh
 --------------------------------------------------------------------------
 Santosh Kumar                         URL http://www.igidr.ac.in/~santosh/
 PhD Student
 Indira Gandhi Institute of Development Research
 Gen A. K. Vaidya Marg
 Goregaon ( East )
 Mumbai pin 400065 India
 Phone 28400919
 ------------------------------------------------------------------------



From rpeng at jhsph.edu  Tue Apr 13 15:57:40 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 13 Apr 2004 09:57:40 -0400
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <200404131225.UAA01688@webmaild.ust.hk>
References: <200404131225.UAA01688@webmaild.ust.hk>
Message-ID: <407BF1D4.1080009@jhsph.edu>

As far as I know, R does compile on AMD Opterons and runs as a 
64-bit application.  So it can store objects larger than 4GB. 
However, I don't think R gets tested very often on 64-bit 
machines with such large objects so there may be yet undiscovered 
bugs.

-roger

Sunny Ho wrote:

> Hello everyone,
> 
> I would like to get some advices on using R with some really large datasets.
> 
> I'm using RH9 Linux R 1.8.1 for a research with a lot of numerical data. The datasets total to around 200Mb (shown by memory.size). During my data manipulation, the system memory usage grew to 1.5Gb, and this caused a lot of swapping activities on my 1Gb PC. This is just a small-scale experiment, the full-scale one will be using data 30 times as large (on a 4Gb machine). I can see that I'll need to deal with memory usage problem very soon.
> 
> I notice that R keeps all datasets in memory at all times. I wonder whether there is any way to instruct R to push some of the less-frequently-used data tables out of main memory, so as to free up memory for those that are actively in used. It'll be even better if R can keep only part of a table in memory only when that part is needed. Using save & load could help, but I just wonder whether R is intelligent enough to do this by itself, so I don't need to keep track of memory usage at all times.
> 
> Another thought is to use a 64-bit machine (AMD64). I find there is a pre-compiled R for Fedora Linux on AMD64. Anyone knows whether this version of R runs as 64-bit? If so, then will R be able to go beyond the 32-bit 4Gb memory limit?
> 
> Also, from the manual, I find that the RPgSQL package (for PostgreSQL database) supports a feature "proxy data frame". Does anyone have experience with this? Can "proxy data frame" handle memory efficiently for very large datasets? Say, if I have a 6Gb database table defined as a proxy data frame, will R & RPgSQL be able to handle it with just 4Gb of memory?
> 
> Any comments will be useful. Many thanks.
> 
> Sunny Ho
> (Hong Kong University of Science & Technology)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fm3a004 at math.uni-hamburg.de  Tue Apr 13 16:05:30 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Tue, 13 Apr 2004 16:05:30 +0200 (MET DST)
Subject: [R] model-based clustering
In-Reply-To: <BAY14-F56uBhvuc8O7600016628@hotmail.com>
Message-ID: <Pine.GSO.3.95q.1040413160353.18276F-100000@sun11.math.uni-hamburg.de>

Dear Talita,

no help is possible unless you do not tell us what exactly you want to do
and what exactly your difficulties are.

Best,
Christian

On Tue, 13 Apr 2004, Talita Leite wrote:

> Hello,
> 
> I'm trying to use the model-based clustering functions but i'm having some 
> difficulties. Does anybody could help me to make a good analisys of a data 
> set using these functions??
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From talitaperciano at hotmail.com  Tue Apr 13 16:22:30 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Tue, 13 Apr 2004 11:22:30 -0300
Subject: [R] model-based clustering
Message-ID: <BAY14-F26q5ijb1u2ZN0004644e@hotmail.com>

Hello again,

Let me explain this better. I've been working in clustering methods during 
this year and now i'm starting (or trying to) with model-based clustering. 
I've been searching for help to understand how the functions works and i 
found some. The problem is that i don't know the steps to follow. For 
example: working with the data set IRIS. What steps do i have to follow and 
what functions do i have to use to make a good clustering? To find the three 
groups on that case?

Thanks,

Talita



From erich.neuwirth at univie.ac.at  Tue Apr 13 16:22:37 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Tue, 13 Apr 2004 16:22:37 +0200
Subject: [R] Execute function at startup
Message-ID: <407BF7AD.2050506@univie.ac.at>

It would be convenient to have something like
Rgui runfist="myfunction()"
in Windows.
The reason:
AFAIK Rgui does not accept piped input
(RGui < myfile.R does not seem to work).
A solution could be to put a few fuctions in Rprofile and then
give the name for one of these functions to be executed at startup as
a command line parameter to Rgui.

Can something like this be done?


-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From tlumley at u.washington.edu  Tue Apr 13 16:25:39 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Apr 2004 07:25:39 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <20040413015326.14864.qmail@web60107.mail.yahoo.com>
References: <20040413015326.14864.qmail@web60107.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0404130724040.106200@homer38.u.washington.edu>

On Mon, 12 Apr 2004, Fred Rohde wrote:

> Thanks.  I'll update the survey package.  Sudaan does the standard
> errors on quantiles using Taylor series.  If I can hunt down the formula
> it uses, could you add that to svyquantile?

If I can bring myself to believe it.  Computing standard errors for the
normal approximation to the median is not easy even in simple random
samples.

	-thomas


> Fred
>
> Thomas Lumley <tlumley at u.washington.edu> wrote:
> On Mon, 12 Apr 2004, Fred Rohde wrote:
>
> > Hello,
> > Is there a way to get complex sample variances in the survey package on
> > summary statistics other than means? If not, can they be added to a
> > future version? It would be be great to have them on totals, quantiles,
> > ratios, and tables (eg row percent, columns percent, etc).
> >
>
> svytotal() and svyratio() will do this for totals and ratios if you have a
> new enough version. At the moment the easiest way to get row or column
> percentages is to think of them them as ratios of means of binary
> variables and use svyratio().
>
> Quantiles are more difficult, since neither Taylor series nor jackknife
> approaches work.
>
> -thomas
>
>
> ---------------------------------

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From andy_liaw at merck.com  Tue Apr 13 16:28:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Apr 2004 10:28:38 -0400
Subject: [R] model-based clustering
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B9B@usrymx25.merck.com>

In the time that you posted the numerous uninformative messages, you can do
yourself a great favor by following the posting guide mentioned in the
footer.

Try:

install.packages("mclust")
library(mclust)
?mclust
example(mclust)

Andy

> From: Talita Leite
> 
> Hello again,
> 
> Let me explain this better. I've been working in clustering 
> methods during 
> this year and now i'm starting (or trying to) with 
> model-based clustering. 
> I've been searching for help to understand how the functions 
> works and i 
> found some. The problem is that i don't know the steps to follow. For 
> example: working with the data set IRIS. What steps do i have 
> to follow and 
> what functions do i have to use to make a good clustering? To 
> find the three 
> groups on that case?
> 
> Thanks,
> 
> Talita
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From fm3a004 at math.uni-hamburg.de  Tue Apr 13 16:31:33 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Tue, 13 Apr 2004 16:31:33 +0200 (MET DST)
Subject: [R] Re: model-based clustering
In-Reply-To: <BAY14-F26q5ijb1u2ZN0004644e@hotmail.com>
Message-ID: <Pine.GSO.3.95q.1040413162522.18276G-100000@sun11.math.uni-hamburg.de>

Dear Talita,

you may start with
library(mclust)
?EMclust
example(EMclust)
# The example is with Iris data.

To understand what you're doing, read a paper from the web page
of the developers, cited on the help page.

Best,
Christian

PS: What a "good" or "optimal" clustering is, is by no means well defined.
    Some cluster algorithms will find 2 or more than 3 clusters on Iris,
    and that's not necessarily an argument against these algorithms.
 
On Tue, 13 Apr 2004, Talita Leite wrote:

> Hello again,
> 
> Let me explain this better. I've been working in clustering methods during 
> this year and now i'm starting (or trying to) with model-based clustering. 
> I've been searching for help to understand how the functions works and i 
> found some. The problem is that i don't know the steps to follow. For 
> example: working with the data set IRIS. What steps do i have to follow and 
> what functions do i have to use to make a good clustering? To find the three 
> groups on that case?
> 
> Thanks,
> 
> Talita
> 
> _________________________________________________________________
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From tlumley at u.washington.edu  Tue Apr 13 16:32:27 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Apr 2004 07:32:27 -0700 (PDT)
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <407BF1D4.1080009@jhsph.edu>
References: <200404131225.UAA01688@webmaild.ust.hk>
	<407BF1D4.1080009@jhsph.edu>
Message-ID: <Pine.A41.4.58.0404130728560.106200@homer38.u.washington.edu>

On Tue, 13 Apr 2004, Roger D. Peng wrote:

> As far as I know, R does compile on AMD Opterons and runs as a
> 64-bit application.  So it can store objects larger than 4GB.
> However, I don't think R gets tested very often on 64-bit
> machines with such large objects so there may be yet undiscovered
> bugs.

Using more than 4Gb memory is reasonably tested now.  Single objects of
that size may not be -- I think you still can't have a vector whose
length() is more than 2^31, for example.

	-thomas


>
> -roger
>
> Sunny Ho wrote:
>
> > Hello everyone,
> >
> > I would like to get some advices on using R with some really large datasets.
> >
> > I'm using RH9 Linux R 1.8.1 for a research with a lot of numerical data. The datasets total to around 200Mb (shown by memory.size). During my data manipulation, the system memory usage grew to 1.5Gb, and this caused a lot of swapping activities on my 1Gb PC. This is just a small-scale experiment, the full-scale one will be using data 30 times as large (on a 4Gb machine). I can see that I'll need to deal with memory usage problem very soon.
> >
> > I notice that R keeps all datasets in memory at all times. I wonder whether there is any way to instruct R to push some of the less-frequently-used data tables out of main memory, so as to free up memory for those that are actively in used. It'll be even better if R can keep only part of a table in memory only when that part is needed. Using save & load could help, but I just wonder whether R is intelligent enough to do this by itself, so I don't need to keep track of memory usage at all times.
> >
> > Another thought is to use a 64-bit machine (AMD64). I find there is a pre-compiled R for Fedora Linux on AMD64. Anyone knows whether this version of R runs as 64-bit? If so, then will R be able to go beyond the 32-bit 4Gb memory limit?
> >
> > Also, from the manual, I find that the RPgSQL package (for PostgreSQL database) supports a feature "proxy data frame". Does anyone have experience with this? Can "proxy data frame" handle memory efficiently for very large datasets? Say, if I have a 6Gb database table defined as a proxy data frame, will R & RPgSQL be able to handle it with just 4Gb of memory?
> >
> > Any comments will be useful. Many thanks.
> >
> > Sunny Ho
> > (Hong Kong University of Science & Technology)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From marcelloverona at tiscali.it  Tue Apr 13 18:35:59 2004
From: marcelloverona at tiscali.it (Marcello Verona)
Date: Tue, 13 Apr 2004 16:35:59 +0000
Subject: [R] R , apache and PHP
Message-ID: <407C16EF.2090505@tiscali.it>

I've developed a web application in PHP and R

my script is


<?php

...
exec("R CMD BATCH --silent /home/marcello/R_in/myfile.bat  
/home/marcello/R_out/myfile.out");

...

?>

This script execute in R batch mode and write the myfile.out.

On Win2000 the similar script is ok, but on linux I've a problem.

I suppose is a permession problem because the same script on shell run fine
and on Zend debugger (my IDE for php) is also ok.
In this case the owner is "marcello" , if I run the script by browser 
the owner is "apache".

I've  overwritted all the ownerships of R directory and bin to apache 
user but not work.

If a run
exec("ls > mydir.txt"); is ok (is not a PHP general problem!)

Someone can help me?

Thanks
(and excuse my for my poor english)

Marcello Verona



From marcelloverona at tiscali.it  Tue Apr 13 18:37:14 2004
From: marcelloverona at tiscali.it (Marcello Verona)
Date: Tue, 13 Apr 2004 16:37:14 +0000
Subject: [R] R apache and PHP
Message-ID: <407C173A.9020009@tiscali.it>

I've developed a web application in PHP and R

my script is


<?php

...
exec("R CMD BATCH --silent /home/marcello/R_in/myfile.bat  
/home/marcello/R_out/myfile.out");

...

?>

This script execute in R batch mode and write the myfile.out.

On Win2000 the similar script is ok, but on linux I've a problem.

I suppose is a permession problem because the same script on shell run fine
and on Zend debugger (my IDE for php) is also ok.
In this case the owner is "marcello" , if I run the script by browser 
the owner is "apache".

I've  overwritted all the ownerships of R directory and bin to apache 
user but not work.

If a run
exec("ls > mydir.txt"); is ok (is not a PHP general problem!)

Someone can help me?

Thanks
(and excuse my for my poor english)

Marcello Verona



From andy_liaw at merck.com  Tue Apr 13 16:37:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Apr 2004 10:37:54 -0400
Subject: [R] Need advice on using R with large datasets
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7B9C@usrymx25.merck.com>

I was under the impression that R has been run on 64-bit Solaris (and other
64-bit Unices) for quite a while (as 64-bit app).  We've been running 64-bit
R on amd64 for a few months (and had quite a few oppertunities to get the R
processes using over 8GB of RAM).  Not much problem as far as I can see...

Best,
Andy

> From: Roger D. Peng
> 
> As far as I know, R does compile on AMD Opterons and runs as a 
> 64-bit application.  So it can store objects larger than 4GB. 
> However, I don't think R gets tested very often on 64-bit 
> machines with such large objects so there may be yet undiscovered 
> bugs.
> 
> -roger
> 
> Sunny Ho wrote:
> 
> > Hello everyone,
> > 
> > I would like to get some advices on using R with some 
> really large datasets.
> > 
> > I'm using RH9 Linux R 1.8.1 for a research with a lot of 
> numerical data. The datasets total to around 200Mb (shown by 
> memory.size). During my data manipulation, the system memory 
> usage grew to 1.5Gb, and this caused a lot of swapping 
> activities on my 1Gb PC. This is just a small-scale 
> experiment, the full-scale one will be using data 30 times as 
> large (on a 4Gb machine). I can see that I'll need to deal 
> with memory usage problem very soon.
> > 
> > I notice that R keeps all datasets in memory at all times. 
> I wonder whether there is any way to instruct R to push some 
> of the less-frequently-used data tables out of main memory, 
> so as to free up memory for those that are actively in used. 
> It'll be even better if R can keep only part of a table in 
> memory only when that part is needed. Using save & load could 
> help, but I just wonder whether R is intelligent enough to do 
> this by itself, so I don't need to keep track of memory usage 
> at all times.
> > 
> > Another thought is to use a 64-bit machine (AMD64). I find 
> there is a pre-compiled R for Fedora Linux on AMD64. Anyone 
> knows whether this version of R runs as 64-bit? If so, then 
> will R be able to go beyond the 32-bit 4Gb memory limit?
> > 
> > Also, from the manual, I find that the RPgSQL package (for 
> PostgreSQL database) supports a feature "proxy data frame". 
> Does anyone have experience with this? Can "proxy data frame" 
> handle memory efficiently for very large datasets? Say, if I 
> have a 6Gb database table defined as a proxy data frame, will 
> R & RPgSQL be able to handle it with just 4Gb of memory?
> > 
> > Any comments will be useful. Many thanks.
> > 
> > Sunny Ho
> > (Hong Kong University of Science & Technology)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Apr 13 17:09:26 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2004 17:09:26 +0200
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <407BF1D4.1080009@jhsph.edu>
References: <200404131225.UAA01688@webmaild.ust.hk>
	<407BF1D4.1080009@jhsph.edu>
Message-ID: <x2oepvc3tl.fsf@biostat.ku.dk>

"Roger D. Peng" <rpeng at jhsph.edu> writes:

> As far as I know, R does compile on AMD Opterons and runs as a 64-bit
> application.  So it can store objects larger than 4GB. However, I
> don't think R gets tested very often on 64-bit machines with such
> large objects so there may be yet undiscovered bugs.

There are a few such machines around among R users, and R seems to
work OK on them. One slight gotcha is that the Fortran numeric
libraries (Lapack, ATLAS) tend to use integer indexing, which might
overflow for large objects. Things like data frames which consist of
multiple subobjects might be less sensitive to this. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hhan at cse.psu.edu  Tue Apr 13 17:15:39 2004
From: hhan at cse.psu.edu (Hui Han)
Date: Tue, 13 Apr 2004 11:15:39 -0400
Subject: [R] randomForest: more than one variable needed?
Message-ID: <20040413151539.GA15962@alphan.cse.psu.edu>

Hi,

I am doing feature selection for my dataset. The following is
the extreme case where only one feature is left. But I got
the error below. So my question is that do I have to use
more than one features?

sample.subset
  udomain.edu hpclass
1        -1.0     not
2        -1.0     not
3        -0.2     not
4         1.0      hp
5         1.0      hp
> randomForest(hpclass ~., data=sample.subset, importance=TRUE);
Error in if (n == 0) stop("data (x) has 0 rows") :
        argument is of length zero

Best regards,
Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan



From phddas at yahoo.com  Tue Apr 13 17:24:43 2004
From: phddas at yahoo.com (Fred J.)
Date: Tue, 13 Apr 2004 08:24:43 -0700 (PDT)
Subject: [R] fractal calculation using fdim
In-Reply-To: <006001c42106$558a1690$6401a8c0@meadow>
Message-ID: <20040413152443.57372.qmail@web20511.mail.yahoo.com>

  Is that how you got your data or are
> you using real data?

I am using some synthatic data I happend to have.

> argument X is to be a dataframe
> not a matrix  (mat??).  Could that be giving you
> problems?  Do you get
> better results with as.data.frame(mat)?
no, even with data.frame, it gives the same error



From phgrosjean at sciviews.org  Tue Apr 13 17:29:06 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 13 Apr 2004 17:29:06 +0200
Subject: [R] randomForest: more than one variable needed?
In-Reply-To: <20040413151539.GA15962@alphan.cse.psu.edu>
Message-ID: <MABBLJDICACNFOLGIHJOAEBOEFAA.phgrosjean@sciviews.org>

I don't see much why to use random forest with only one predictive variable!
Recall that random forest grow trees with a random subset of variables "in
competition" for growing each node of the trees in the forest... How do you
make such a random subset with only one predictive variable? there is no
point here!

Philippe Grosjean

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Hui Han
Sent: Tuesday, 13 April, 2004 17:16
To: r-help at stat.math.ethz.ch
Subject: [R] randomForest: more than one variable needed?


Hi,

I am doing feature selection for my dataset. The following is
the extreme case where only one feature is left. But I got
the error below. So my question is that do I have to use
more than one features?

sample.subset
  udomain.edu hpclass
1        -1.0     not
2        -1.0     not
3        -0.2     not
4         1.0      hp
5         1.0      hp
> randomForest(hpclass ~., data=sample.subset, importance=TRUE);
Error in if (n == 0) stop("data (x) has 0 rows") :
        argument is of length zero

Best regards,
Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hhan at cse.psu.edu  Tue Apr 13 17:49:32 2004
From: hhan at cse.psu.edu (Hui Han)
Date: Tue, 13 Apr 2004 11:49:32 -0400
Subject: [R] randomForest: more than one variable needed?
In-Reply-To: <MABBLJDICACNFOLGIHJOAEBOEFAA.phgrosjean@sciviews.org>
References: <20040413151539.GA15962@alphan.cse.psu.edu>
	<MABBLJDICACNFOLGIHJOAEBOEFAA.phgrosjean@sciviews.org>
Message-ID: <20040413154932.GD16043@alphan.cse.psu.edu>

I agree with you about the less practical meaning of this sample of 
the extreme case. I am just curious about the "grammar" syntax of 
randomForest.

Thanks.
Hui

On Tue, Apr 13, 2004 at 05:29:06PM +0200, Philippe Grosjean wrote:
> I don't see much why to use random forest with only one predictive variable!
> Recall that random forest grow trees with a random subset of variables "in
> competition" for growing each node of the trees in the forest... How do you
> make such a random subset with only one predictive variable? there is no
> point here!
> 
> Philippe Grosjean
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Hui Han
> Sent: Tuesday, 13 April, 2004 17:16
> To: r-help at stat.math.ethz.ch
> Subject: [R] randomForest: more than one variable needed?
> 
> 
> Hi,
> 
> I am doing feature selection for my dataset. The following is
> the extreme case where only one feature is left. But I got
> the error below. So my question is that do I have to use
> more than one features?
> 
> sample.subset
>   udomain.edu hpclass
> 1        -1.0     not
> 2        -1.0     not
> 3        -0.2     not
> 4         1.0      hp
> 5         1.0      hp
> > randomForest(hpclass ~., data=sample.subset, importance=TRUE);
> Error in if (n == 0) stop("data (x) has 0 rows") :
>         argument is of length zero
> 
> Best regards,
> Hui Han
> Department of Computer Science and Engineering,
> The Pennsylvania State University
> University Park, PA,16802
> email: hhan at cse.psu.edu
> homepage: http://www.cse.psu.edu/~hhan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 


Hui Han
Department of Computer Science and Engineering,
The Pennsylvania State University 
University Park, PA,16802
email: hhan at cse.psu.edu
homepage: http://www.cse.psu.edu/~hhan



From rpeng at jhsph.edu  Tue Apr 13 17:54:25 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 13 Apr 2004 11:54:25 -0400
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B9C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7B9C@usrymx25.merck.com>
Message-ID: <407C0D31.8000704@jhsph.edu>

I've been running R on 64-bit SuSE Linux on Opterons for a few 
months now and it certainly runs fine in what I would call 
standard situations.  In particular there seems to be no problem 
with workspaces > 4GB.  But I seldom handle single objects (like 
matrices, vectors) that are > 4GB.  The only exception is lists, 
but I think those are okay since they are composed of various 
sub-objects (like Peter mentioned).

-roger

Liaw, Andy wrote:
> I was under the impression that R has been run on 64-bit Solaris (and other
> 64-bit Unices) for quite a while (as 64-bit app).  We've been running 64-bit
> R on amd64 for a few months (and had quite a few oppertunities to get the R
> processes using over 8GB of RAM).  Not much problem as far as I can see...
> 
> Best,
> Andy
> 
> 
>>From: Roger D. Peng
>>
>>As far as I know, R does compile on AMD Opterons and runs as a 
>>64-bit application.  So it can store objects larger than 4GB. 
>>However, I don't think R gets tested very often on 64-bit 
>>machines with such large objects so there may be yet undiscovered 
>>bugs.
>>
>>-roger
>>
>>Sunny Ho wrote:
>>
>>
>>>Hello everyone,
>>>
>>>I would like to get some advices on using R with some 
>>
>>really large datasets.
>>
>>>I'm using RH9 Linux R 1.8.1 for a research with a lot of 
>>
>>numerical data. The datasets total to around 200Mb (shown by 
>>memory.size). During my data manipulation, the system memory 
>>usage grew to 1.5Gb, and this caused a lot of swapping 
>>activities on my 1Gb PC. This is just a small-scale 
>>experiment, the full-scale one will be using data 30 times as 
>>large (on a 4Gb machine). I can see that I'll need to deal 
>>with memory usage problem very soon.
>>
>>>I notice that R keeps all datasets in memory at all times. 
>>
>>I wonder whether there is any way to instruct R to push some 
>>of the less-frequently-used data tables out of main memory, 
>>so as to free up memory for those that are actively in used. 
>>It'll be even better if R can keep only part of a table in 
>>memory only when that part is needed. Using save & load could 
>>help, but I just wonder whether R is intelligent enough to do 
>>this by itself, so I don't need to keep track of memory usage 
>>at all times.
>>
>>>Another thought is to use a 64-bit machine (AMD64). I find 
>>
>>there is a pre-compiled R for Fedora Linux on AMD64. Anyone 
>>knows whether this version of R runs as 64-bit? If so, then 
>>will R be able to go beyond the 32-bit 4Gb memory limit?
>>
>>>Also, from the manual, I find that the RPgSQL package (for 
>>
>>PostgreSQL database) supports a feature "proxy data frame". 
>>Does anyone have experience with this? Can "proxy data frame" 
>>handle memory efficiently for very large datasets? Say, if I 
>>have a 6Gb database table defined as a proxy data frame, will 
>>R & RPgSQL be able to handle it with just 4Gb of memory?
>>
>>>Any comments will be useful. Many thanks.
>>>
>>>Sunny Ho
>>>(Hong Kong University of Science & Technology)
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------
>



From andy_liaw at merck.com  Tue Apr 13 17:57:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Apr 2004 11:57:11 -0400
Subject: [R] randomForest: more than one variable needed?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BA3@usrymx25.merck.com>

With only one `x' variable, RF will be identical to bagging.

This looks like a bug.  I will check it out.

Andy

> From: Hui Han
> 
> I agree with you about the less practical meaning of this sample of 
> the extreme case. I am just curious about the "grammar" syntax of 
> randomForest.
> 
> Thanks.
> Hui
> 
> On Tue, Apr 13, 2004 at 05:29:06PM +0200, Philippe Grosjean wrote:
> > I don't see much why to use random forest with only one 
> predictive variable!
> > Recall that random forest grow trees with a random subset 
> of variables "in
> > competition" for growing each node of the trees in the 
> forest... How do you
> > make such a random subset with only one predictive 
> variable? there is no
> > point here!
> > 
> > Philippe Grosjean
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Hui Han
> > Sent: Tuesday, 13 April, 2004 17:16
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] randomForest: more than one variable needed?
> > 
> > 
> > Hi,
> > 
> > I am doing feature selection for my dataset. The following is
> > the extreme case where only one feature is left. But I got
> > the error below. So my question is that do I have to use
> > more than one features?
> > 
> > sample.subset
> >   udomain.edu hpclass
> > 1        -1.0     not
> > 2        -1.0     not
> > 3        -0.2     not
> > 4         1.0      hp
> > 5         1.0      hp
> > > randomForest(hpclass ~., data=sample.subset, importance=TRUE);
> > Error in if (n == 0) stop("data (x) has 0 rows") :
> >         argument is of length zero
> > 
> > Best regards,
> > Hui Han
> > Department of Computer Science and Engineering,
> > The Pennsylvania State University
> > University Park, PA,16802
> > email: hhan at cse.psu.edu
> > homepage: http://www.cse.psu.edu/~hhan
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> Hui Han
> Department of Computer Science and Engineering,
> The Pennsylvania State University 
> University Park, PA,16802
> email: hhan at cse.psu.edu
> homepage: http://www.cse.psu.edu/~hhan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Apr 13 18:26:57 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 13 Apr 2004 18:26:57 +0200 (CEST)
Subject: [R] randomForest: more than one variable needed?
In-Reply-To: <20040413151539.GA15962@alphan.cse.psu.edu>
References: <20040413151539.GA15962@alphan.cse.psu.edu>
Message-ID: <Pine.LNX.4.51.0404131824260.4926@artemis.imbe.med.uni-erlangen.de>

On Tue, 13 Apr 2004, Hui Han wrote:

> Hi,
>
> I am doing feature selection for my dataset. The following is
> the extreme case where only one feature is left. But I got
> the error below. So my question is that do I have to use
> more than one features?
>
> sample.subset
>   udomain.edu hpclass
> 1        -1.0     not
> 2        -1.0     not
> 3        -0.2     not
> 4         1.0      hp
> 5         1.0      hp
> > randomForest(hpclass ~., data=sample.subset, importance=TRUE);
> Error in if (n == 0) stop("data (x) has 0 rows") :
>         argument is of length zero
>

no idea about the error message, but there is no need for feature
selection before using random forests - give it a try without
preselection of variables.

best

Torsten

> Best regards,
> Hui Han
> Department of Computer Science and Engineering,
> The Pennsylvania State University
> University Park, PA,16802
> email: hhan at cse.psu.edu
> homepage: http://www.cse.psu.edu/~hhan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From p.dalgaard at biostat.ku.dk  Tue Apr 13 18:33:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2004 18:33:07 +0200
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <407C0D31.8000704@jhsph.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7B9C@usrymx25.merck.com>
	<407C0D31.8000704@jhsph.edu>
Message-ID: <x2ekqrbzy4.fsf@biostat.ku.dk>

"Roger D. Peng" <rpeng at jhsph.edu> writes:

> I've been running R on 64-bit SuSE Linux on Opterons for a few months
> now and it certainly runs fine in what I would call standard
> situations.  In particular there seems to be no problem with
> workspaces > 4GB.  But I seldom handle single objects (like matrices,
> vectors) that are > 4GB.  The only exception is lists, but I think
> those are okay since they are composed of various sub-objects (like
> Peter mentioned).

I just tried, and x <- numeric(1e9) (~8GB) doesn't appear to be a
problem, except that it takes "forever" since the machine in question
has only 1GB of memory, and numeric() zero fills the allocated
block...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Tue Apr 13 18:51:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Apr 2004 12:51:30 -0400
Subject: [R] Need advice on using R with large datasets
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BAB@usrymx25.merck.com>

On a dual Opteron 244 with 16GB ram, and 

[andy at leo:cb1]% free
             total       used       free     shared    buffers     cached
Mem:      16278648   14552676    1725972          0     229420    3691824
-/+ buffers/cache:   10631432    5647216
Swap:      2096472      13428    2083044

... using freshly compiled R-1.9.0:

> system.time(x <- numeric(1e9))
[1]  3.60  8.09 15.11  0.00  0.00
> object.size(x)/1024^3
[1] 7.45058

Andy

> From: Peter Dalgaard
> 
> "Roger D. Peng" <rpeng at jhsph.edu> writes:
> 
> > I've been running R on 64-bit SuSE Linux on Opterons for a 
> few months
> > now and it certainly runs fine in what I would call standard
> > situations.  In particular there seems to be no problem with
> > workspaces > 4GB.  But I seldom handle single objects (like 
> matrices,
> > vectors) that are > 4GB.  The only exception is lists, but I think
> > those are okay since they are composed of various sub-objects (like
> > Peter mentioned).
> 
> I just tried, and x <- numeric(1e9) (~8GB) doesn't appear to be a
> problem, except that it takes "forever" since the machine in question
> has only 1GB of memory, and numeric() zero fills the allocated
> block...
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From nedluk at yahoo.it  Tue Apr 13 18:57:59 2004
From: nedluk at yahoo.it (=?iso-8859-1?q?michele=20lux?=)
Date: Tue, 13 Apr 2004 18:57:59 +0200 (CEST)
Subject: [R] (no subject)
Message-ID: <20040413165759.28424.qmail@web13604.mail.yahoo.com>

Hallo all
somebody knows if exist a command who makes the
opposite of what "diff" command do?
I'he to write code?
thanks Michele



From nedluk at yahoo.it  Tue Apr 13 18:58:31 2004
From: nedluk at yahoo.it (=?iso-8859-1?q?michele=20lux?=)
Date: Tue, 13 Apr 2004 18:58:31 +0200 (CEST)
Subject: [R] "diff"^-1
Message-ID: <20040413165831.62777.qmail@web13607.mail.yahoo.com>

Hallo all
somebody knows if exist a command who makes the
opposite of what "diff" command do?
I'he to write code?
thanks Michele



From spencer.graves at pdf.com  Tue Apr 13 19:19:22 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 13 Apr 2004 10:19:22 -0700
Subject: [R] "diff"^-1
In-Reply-To: <20040413165831.62777.qmail@web13607.mail.yahoo.com>
References: <20040413165831.62777.qmail@web13607.mail.yahoo.com>
Message-ID: <407C211A.80103@pdf.com>

?cumsum

michele lux wrote:

>Hallo all
>somebody knows if exist a command who makes the
>opposite of what "diff" command do?
>I'he to write code?
>thanks Michele
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From rpeng at jhsph.edu  Tue Apr 13 19:13:02 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 13 Apr 2004 13:13:02 -0400
Subject: [R] "diff"^-1
In-Reply-To: <20040413165831.62777.qmail@web13607.mail.yahoo.com>
References: <20040413165831.62777.qmail@web13607.mail.yahoo.com>
Message-ID: <407C1F9E.2020505@jhsph.edu>

What do you mean by "opposite"?  Have you looked at patch?

-roger

michele lux wrote:
> Hallo all
> somebody knows if exist a command who makes the
> opposite of what "diff" command do?
> I'he to write code?
> thanks Michele
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From s.chamaille at wanadoo.fr  Tue Apr 13 19:36:23 2004
From: s.chamaille at wanadoo.fr (=?iso-8859-1?Q?Simon_Chamaill=E9?=)
Date: Tue, 13 Apr 2004 19:36:23 +0200
Subject: [R] Non-homogeneity of variance - decreasing variance
Message-ID: <IEEFKHGFNDGNNFCJALMFIEBDCEAA.s.chamaille@wanadoo.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040413/4a85fa9c/attachment.pl

From dmurdoch at pair.com  Tue Apr 13 19:46:20 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 13 Apr 2004 13:46:20 -0400
Subject: [R] (no subject)
In-Reply-To: <20040413165759.28424.qmail@web13604.mail.yahoo.com>
References: <20040413165759.28424.qmail@web13604.mail.yahoo.com>
Message-ID: <pp9o701qcjphsdbnh51if5s6t1f5aa68vf@4ax.com>

On Tue, 13 Apr 2004 18:57:59 +0200 (CEST), michele lux
<nedluk at yahoo.it> wrote :

>Hallo all
>somebody knows if exist a command who makes the
>opposite of what "diff" command do?
>I'he to write code?

Sounds like "patch" is what you want.  

Duncan Murdoch



From sundar.dorai-raj at PDF.COM  Tue Apr 13 19:51:25 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 13 Apr 2004 12:51:25 -0500
Subject: [R] lattice problem in R-1.9.0
Message-ID: <407C289D.5010705@pdf.com>

Hi all,
   I just installed R-1.9.0 on Windows 2000 from binaries. Yesterday, on 
R-1.8.1 I ran a script that looked like:

library(lattice)
tmp <- expand.grid(A = 1:3, B = letters[1:2])
tmp$z <- runif(NROW(tmp))
trellis.device(png, file = "x1081.png", theme = col.whitebg)
xyplot(z ~ A | B, data = tmp,
        panel = function(x, y, i) {
          panel.xyplot(x, y)
          ltext(1, 0.95, paste("i =", i), adj = 0)
        },
        ylim = c(0, 1),
        i = 10)
dev.off()

In R-1.9.0, the same script gives the following error message:

Error in trellis.skeleton(cond = structure(list(B = 
structure(as.integer(c(1,  :
	Invalid value of index.cond

I've tracked it down to including the argument "i" to the panel 
function. If I change the argument to

xyplot(z ~ A | B, data = tmp,
        panel = function(x, y, I) {
          panel.xyplot(x, y)
          ltext(1, 0.95, paste("i =", I), adj = 0)
        },
        ylim = c(0, 1),
        I = 10)

all is copacetic. There is no argument in xyplot that starts with "i" so 
I don't know where the partial matching is occurring.

Thanks,
Sundar



From spencer.graves at pdf.com  Tue Apr 13 19:58:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 13 Apr 2004 10:58:35 -0700
Subject: [R] Opposite of 'diff'?  [was:  (no subject)]
In-Reply-To: <pp9o701qcjphsdbnh51if5s6t1f5aa68vf@4ax.com>
References: <20040413165759.28424.qmail@web13604.mail.yahoo.com>
	<pp9o701qcjphsdbnh51if5s6t1f5aa68vf@4ax.com>
Message-ID: <407C2A4B.20702@pdf.com>

What is "patch"?  I don't find it in R 1.8.1.  However, ?"diff" mentions 
"diffinv";  that and "cumsum" perform as follows: 

 cumsum(diff(1:11))
 [1]  1  2  3  4  5  6  7  8  9 10
 > diffinv(diff(1:11))
 [1]  0  1  2  3  4  5  6  7  8  9 10
 >
spencer graves    

Duncan Murdoch wrote:

>On Tue, 13 Apr 2004 18:57:59 +0200 (CEST), michele lux
><nedluk at yahoo.it> wrote :
>
>  
>
>>Hallo all
>>somebody knows if exist a command who makes the
>>opposite of what "diff" command do?
>>I'he to write code?
>>    
>>
>
>Sounds like "patch" is what you want.  
>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From pgilbert at bank-banque-canada.ca  Tue Apr 13 20:00:02 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 13 Apr 2004 14:00:02 -0400
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B9C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7B9C@usrymx25.merck.com>
Message-ID: <407C2AA2.80107@bank-banque-canada.ca>


Liaw, Andy wrote:

>I was under the impression that R has been run on 64-bit Solaris (and other
>64-bit Unices) for quite a while (as 64-bit app).  
>
Yes, on Solaris it has worked for quite a while. I don't use it a lot, 
but have one problem that I have been running from time to time for a 
few years.  There are two "issues" that I know about.

1/  Some extra capabilities (like png I think)  also need to be compiled 
as 64 bit apps, and in some cases this is a non-trivial effort (on 
Solaris for someone like me that does not do that kind of thing often). 
For this reason I have both a 32-bit version for regular use and a 
64-bit version for special problems.

2/  Some R functions make copies of the data sets used and attach them 
to the result. For small data sets that can be very useful. If the 
result is then used as an argument to another function then very quickly 
there are multiple copies. If the data set is large then one is quickly 
making heavy use of swap, and the processing is very slow. This is not 
just a 64-bit problem, but with a 32-bit architecture it is hard to work 
on a data set big enough that this becomes an issue.  In some cases 
performance  can be improved  a lot by hacking the code and  not 
attaching the  dataset to the result (with some risk that functions 
using the result get broken).

Paul Gilbert

>We've been running 64-bit
>R on amd64 for a few months (and had quite a few oppertunities to get the R
>processes using over 8GB of RAM).  Not much problem as far as I can see...
>
>Best,
>Andy
>
>  
>
>>From: Roger D. Peng
>>
>>As far as I know, R does compile on AMD Opterons and runs as a 
>>64-bit application.  So it can store objects larger than 4GB. 
>>However, I don't think R gets tested very often on 64-bit 
>>machines with such large objects so there may be yet undiscovered 
>>bugs.
>>
>>-roger
>>
>>Sunny Ho wrote:
>>
>>    
>>
>>>Hello everyone,
>>>
>>>I would like to get some advices on using R with some 
>>>      
>>>
>>really large datasets.
>>    
>>
>>>I'm using RH9 Linux R 1.8.1 for a research with a lot of 
>>>      
>>>
>>numerical data. The datasets total to around 200Mb (shown by 
>>memory.size). During my data manipulation, the system memory 
>>usage grew to 1.5Gb, and this caused a lot of swapping 
>>activities on my 1Gb PC. This is just a small-scale 
>>experiment, the full-scale one will be using data 30 times as 
>>large (on a 4Gb machine). I can see that I'll need to deal 
>>with memory usage problem very soon.
>>    
>>
>>>I notice that R keeps all datasets in memory at all times. 
>>>      
>>>
>>I wonder whether there is any way to instruct R to push some 
>>of the less-frequently-used data tables out of main memory, 
>>so as to free up memory for those that are actively in used. 
>>It'll be even better if R can keep only part of a table in 
>>memory only when that part is needed. Using save & load could 
>>help, but I just wonder whether R is intelligent enough to do 
>>this by itself, so I don't need to keep track of memory usage 
>>at all times.
>>    
>>
>>>Another thought is to use a 64-bit machine (AMD64). I find 
>>>      
>>>
>>there is a pre-compiled R for Fedora Linux on AMD64. Anyone 
>>knows whether this version of R runs as 64-bit? If so, then 
>>will R be able to go beyond the 32-bit 4Gb memory limit?
>>    
>>
>>>Also, from the manual, I find that the RPgSQL package (for 
>>>      
>>>
>>PostgreSQL database) supports a feature "proxy data frame". 
>>Does anyone have experience with this? Can "proxy data frame" 
>>handle memory efficiently for very large datasets? Say, if I 
>>have a 6Gb database table defined as a proxy data frame, will 
>>R & RPgSQL be able to handle it with just 4Gb of memory?
>>    
>>
>>>Any comments will be useful. Many thanks.
>>>
>>>Sunny Ho
>>>(Hong Kong University of Science & Technology)
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>      
>>>
>>http://www.R-project.org/posting-guide.html
>>    
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments,...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From deepayan at stat.wisc.edu  Tue Apr 13 20:15:05 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 13 Apr 2004 13:15:05 -0500
Subject: [R] lattice problem in R-1.9.0
In-Reply-To: <407C289D.5010705@pdf.com>
References: <407C289D.5010705@pdf.com>
Message-ID: <200404131315.05397.deepayan@stat.wisc.edu>

On Tuesday 13 April 2004 12:51, Sundar Dorai-Raj wrote:
> Hi all,
>    I just installed R-1.9.0 on Windows 2000 from binaries. Yesterday,
> on R-1.8.1 I ran a script that looked like:
>
> library(lattice)
> tmp <- expand.grid(A = 1:3, B = letters[1:2])
> tmp$z <- runif(NROW(tmp))
> trellis.device(png, file = "x1081.png", theme = col.whitebg)
> xyplot(z ~ A | B, data = tmp,
>         panel = function(x, y, i) {
>           panel.xyplot(x, y)
>           ltext(1, 0.95, paste("i =", i), adj = 0)
>         },
>         ylim = c(0, 1),
>         i = 10)
> dev.off()
>
> In R-1.9.0, the same script gives the following error message:
>
> Error in trellis.skeleton(cond = structure(list(B =
> structure(as.integer(c(1,  :
> 	Invalid value of index.cond
                         ^^^^^^^^^^


> I've tracked it down to including the argument "i" to the panel
> function. If I change the argument to
>
> xyplot(z ~ A | B, data = tmp,
>         panel = function(x, y, I) {
>           panel.xyplot(x, y)
>           ltext(1, 0.95, paste("i =", I), adj = 0)
>         },
>         ylim = c(0, 1),
>         I = 10)
>
> all is copacetic. There is no argument in xyplot that starts with "i"
> so I don't know where the partial matching is occurring.

Actually, in R 1.9.0, xyplot() does have a new argument that starts with 
i, namely 'index.cond' (as indicated by the error message above). This 
(along with many other arguments) doesn't show up in args(xyplot) 
because of the way arguments common to high-level lattice functions are 
handled by common code (they are formally part of ...); but it is 
documented in ?xyplot.

Deepayan



From p.dalgaard at biostat.ku.dk  Tue Apr 13 20:49:09 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2004 20:49:09 +0200
Subject: [R] Opposite of 'diff'?  [was:  (no subject)]
In-Reply-To: <407C2A4B.20702@pdf.com>
References: <20040413165759.28424.qmail@web13604.mail.yahoo.com>
	<pp9o701qcjphsdbnh51if5s6t1f5aa68vf@4ax.com> <407C2A4B.20702@pdf.com>
Message-ID: <x2ad1fbtne.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> What is "patch"?  I don't find it in R 1.8.1.  However, ?"diff"
> mentions "diffinv";  that and "cumsum" perform as follows:

diff is a Unix command for comparing files. Using output from diff to
patch a file is done by a program called ... well you guessed it (an
acronym for "please apply this clever hack" according to legend).

I think that your guess, that it was the R function that was intended,
was a better one, though.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Tue Apr 13 20:59:25 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 13 Apr 2004 14:59:25 -0400
Subject: [R] Opposite of 'diff'?  [was:  (no subject)]
In-Reply-To: <407C2A4B.20702@pdf.com>
References: <20040413165759.28424.qmail@web13604.mail.yahoo.com>
	<pp9o701qcjphsdbnh51if5s6t1f5aa68vf@4ax.com>
	<407C2A4B.20702@pdf.com>
Message-ID: <k1eo70lsp55u9erqbko7museff4kh5h0ho@4ax.com>

On Tue, 13 Apr 2004 10:58:35 -0700, Spencer Graves
<spencer.graves at pdf.com> wrote :

>What is "patch"?  I don't find it in R 1.8.1.  However, ?"diff" mentions 
>"diffinv";  that and "cumsum" perform as follows: 

"diff" is a Unix command to calculate differences between files.
"patch" is a Unix command to apply such differences to a file.

I'm pretty sure I misunderstood the question; "cumsum" is probably the
right answer.

Duncan Murdoch



From kjetil at entelnet.bo  Tue Apr 13 21:09:55 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 13 Apr 2004 15:09:55 -0400
Subject: [R] "diff"^-1
In-Reply-To: <20040413165831.62777.qmail@web13607.mail.yahoo.com>
Message-ID: <407C02C3.28180.422970@localhost>

On 13 Apr 2004 at 18:58, michele lux wrote:

> Hallo all
> somebody knows if exist a command who makes the
> opposite of what "diff" command do?
> I'he to write code?
> thanks Michele
> 

As other responses has shown, your Q could have been clearer!

?diffinv

note that this is not really an inverse, as the following shows:

> diffinv(diff(1:10))
 [1] 0 1 2 3 4 5 6 7 8 9

If you know the first element of your original series, you can do:

> diffinv(diff(3:13), xi=3)
 [1]  3  4  5  6  7  8  9 10 11 12 13

Kjetil Halvorsen

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Apr 13 21:12:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Apr 2004 21:12:04 +0200
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7BAB@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7BAB@usrymx25.merck.com>
Message-ID: <x265c3bsl7.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> On a dual Opteron 244 with 16GB ram, and 
> 
> [andy at leo:cb1]% free
>              total       used       free     shared    buffers     cached
> Mem:      16278648   14552676    1725972          0     229420    3691824
> -/+ buffers/cache:   10631432    5647216
> Swap:      2096472      13428    2083044
> 
> ... using freshly compiled R-1.9.0:
> 
> > system.time(x <- numeric(1e9))
> [1]  3.60  8.09 15.11  0.00  0.00
> > object.size(x)/1024^3
> [1] 7.45058


Well,

> system.time(mean(x))
[1]   15.80   20.94 1323.01    0.00    0.00
> object.size(x)/1024^3
[1] 7.45058

I suppose I just have to look forward to RAM prices dropping...
(Actually, the OS should be able to do better. Should be able to read
the data from disk at about 20s/GB.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Tue Apr 13 22:11:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Apr 2004 13:11:17 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <20040413132428.93671.qmail@web60103.mail.yahoo.com>
References: <20040413132428.93671.qmail@web60103.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0404131256550.26680@homer11.u.washington.edu>

On Tue, 13 Apr 2004, Fred Rohde wrote:

> Looked through the publication, "Statistical Methods and Mathematical
> Algorithms Used in Sudaan" (Shah, et al, 1993) but the only reference to
> variances on quantiles is a 1991 presentation by David Binder.  Googled
> the title and got this link.
>
> http://www.amstat.org/sections/srms/Proceedings/papers/1991_005.pdf
>

Ok.  I see.

I wouldn't have called this a Taylor series method, and I notice that
Binder agrees with me.  They are doing interval estimation by inverting a
score test, which is an interval estimation method I want to add more
generally in R.  It works much better than Wald tests for a number of
quasilikelihood/estimating function estimators in ordinary model-based
analysis, too.

Taylor series methods have trouble with quantiles because the estimating
function isn't differentiable.  Asymptotic normality still applies, but
the asymptotic standard error depends on the density of the variable at
the quantile, and the asymptotic approximation is not as good as usual.
Even the bootstrap needs larger sample sizes for quantiles than for many
statistics.


	-thomas



From rpeng at jhsph.edu  Tue Apr 13 20:40:32 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 13 Apr 2004 14:40:32 -0400
Subject: [R] "diff"^-1
In-Reply-To: <407C1F9E.2020505@jhsph.edu>
References: <20040413165831.62777.qmail@web13607.mail.yahoo.com>
	<407C1F9E.2020505@jhsph.edu>
Message-ID: <407C3420.9080309@jhsph.edu>

Whoops, I was thinking of something totally different.  Apologies.

I think you might want diffinv().

-roger

Roger D. Peng wrote:
> What do you mean by "opposite"?  Have you looked at patch?
> 
> -roger
> 
> michele lux wrote:
> 
>> Hallo all
>> somebody knows if exist a command who makes the
>> opposite of what "diff" command do?
>> I'he to write code?
>> thanks Michele
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From sundar.dorai-raj at PDF.COM  Wed Apr 14 00:53:15 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 13 Apr 2004 17:53:15 -0500
Subject: [R] lattice problem in R-1.9.0
In-Reply-To: <200404131315.05397.deepayan@stat.wisc.edu>
References: <407C289D.5010705@pdf.com>
	<200404131315.05397.deepayan@stat.wisc.edu>
Message-ID: <407C6F5B.3020307@pdf.com>



Deepayan Sarkar wrote:

> On Tuesday 13 April 2004 12:51, Sundar Dorai-Raj wrote:
> 
>>Hi all,
>>   I just installed R-1.9.0 on Windows 2000 from binaries. Yesterday,
>>on R-1.8.1 I ran a script that looked like:
>>
>>library(lattice)
>>tmp <- expand.grid(A = 1:3, B = letters[1:2])
>>tmp$z <- runif(NROW(tmp))
>>trellis.device(png, file = "x1081.png", theme = col.whitebg)
>>xyplot(z ~ A | B, data = tmp,
>>        panel = function(x, y, i) {
>>          panel.xyplot(x, y)
>>          ltext(1, 0.95, paste("i =", i), adj = 0)
>>        },
>>        ylim = c(0, 1),
>>        i = 10)
>>dev.off()
>>
>>In R-1.9.0, the same script gives the following error message:
>>
>>Error in trellis.skeleton(cond = structure(list(B =
>>structure(as.integer(c(1,  :
>>	Invalid value of index.cond
> 
>                          ^^^^^^^^^^
> 
> 
> 
>>I've tracked it down to including the argument "i" to the panel
>>function. If I change the argument to
>>
>>xyplot(z ~ A | B, data = tmp,
>>        panel = function(x, y, I) {
>>          panel.xyplot(x, y)
>>          ltext(1, 0.95, paste("i =", I), adj = 0)
>>        },
>>        ylim = c(0, 1),
>>        I = 10)
>>
>>all is copacetic. There is no argument in xyplot that starts with "i"
>>so I don't know where the partial matching is occurring.
> 
> 
> Actually, in R 1.9.0, xyplot() does have a new argument that starts with 
> i, namely 'index.cond' (as indicated by the error message above). This 
> (along with many other arguments) doesn't show up in args(xyplot) 
> because of the way arguments common to high-level lattice functions are 
> handled by common code (they are formally part of ...); but it is 
> documented in ?xyplot.
> 
> Deepayan
> 

Sorry, should have caught that. As you suspected all I did was 
args(xyplot). I wasn't expecting a new argument.

Thanks for the quick reply.

--sundar



From kjetil at entelnet.bo  Wed Apr 14 00:56:31 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 13 Apr 2004 18:56:31 -0400
Subject: [R] Non-homogeneity of variance - decreasing variance
In-Reply-To: <IEEFKHGFNDGNNFCJALMFIEBDCEAA.s.chamaille@wanadoo.fr>
Message-ID: <407C37DF.16718.1DEBCA@localhost>

On 13 Apr 2004 at 19:36, Simon Chamaill?? wrote:

You could maybe try gls in package nlme, where you can estimate
 parameters in variance functions. If you need a generalized linear 
model, you could have a look at glmmPQL in MASS, but I don't know if 
that accepts models without random effects.

Kjetil Halvorsen

> Hello all,
> I'm running very simple regression but face a problem of
> non-homogeneity of variance, but with a decreasing variance with
> increasing mean...I do not know how to deal with that. this
> relationship doesn't seem to be strong, but it's my first time to see
> something like that, and would like to know what to do if one day it
> becomes stronger. I tested just for fun some transformation but was
> not able to get a better model. I do not know if it can help, but my
> predictor variable is a kind of gamma poisson-shaped-like zero-rich
> distribution (continuous of course), highly overdispersed. If one know
> how to deal with decreasing variance, I would appreciate any advice (I
> tried to modelize negative variance-mean relationship in a new quasi-
> family this was prohibited, only constant, mu, mu^x (and mu(1-mu) for
> binomial) were allowed). I've definitively reached the border of the
> statistical black box for me. thanks simon
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From wasserberg at wisc.edu  Wed Apr 14 01:28:31 2004
From: wasserberg at wisc.edu (GIDEON WASSERBERG)
Date: Tue, 13 Apr 2004 18:28:31 -0500
Subject: [R] Matrix question
Message-ID: <eed846eeedb3.eeedb3eed846@wiscmail.wisc.edu>

Dear Friends

I am doing a simple matrix analysis to calculate the eigenvalue, eigenvector using R for the below matrix, and comparing the result to those obtained from a projection (using excel)

THE MATRIX:
> c
     [,1] [,2] [,3]
[1,]  0.0  2.0    2
[2,]  0.8  0.0    0
[3,]  0.0  0.8    0


The dominant eigenvalue comes out comparable to that calculated numerically, but the eigenvectors do not( see below)!


EIGENVALUES (calculated by R):

> eigen(c)
$values
[1]  1.5564082+0.000000i -0.7782041+0.465623i -0.7782041-0.465623i

EIGENVALUE numerically calculated: 1.556408145


EIGENVECTORS (calculated by R):
$vectors
              [,1]                  [,2]                  [,3]
[1,] -0.8658084+0i  0.6476861+0.0000000i  0.6476861+0.0000000i
[2,] -0.4450290+0i -0.4902997-0.2933611i -0.4902997+0.2933611i
[3,] -0.2287467+0i  0.2382837+0.4441499i  0.2382837-0.4441499i

Stable age distribution (calculated numerically):

0.562365145
0.289057934
0.148576921


My questions are:
1. Both eigenvalue and eigenvectors are associated with some imaginary value (i). How should I relate to that information? 2. More importantly, a. I presume the 1st eigenvector collumn [,1] should correspond to the dominant eigenvalue. How come then that it comes out different from the one calculated numerically? Is there some conversion I should do?

Many thanks

Gideon


Gideon Wasserberg (Ph.D.)
Wildlife research unit,
Department of wildlife ecology,
University of Wisconsin
218 Russell labs, 1630 Linden dr.,
Madison, Wisconsin 53706, USA.
Tel.:608 265 2130, Fax: 608 262 6099



From mwall at diversa.com  Wed Apr 14 01:35:35 2004
From: mwall at diversa.com (Mark Wall)
Date: Tue, 13 Apr 2004 16:35:35 -0700
Subject: [R] Reverse dendrogram plot in R
Message-ID: <81D14648D6BD694CBDB4F45536E81CBC3FCF80@aquarius.diversa.com>

Is there a way to completely reverse a dendrogram plot?

So, for dendrogram D, 
I want to generate a mirror image of plot(D).

This works if one plots an hclust object in reverse order, but I need
this to work as a dendrogram, since the dendrogram is plotted in a more
complicated layout.

Thank you,
Mark Wall



From psirab at inwind.it  Wed Apr 14 01:37:23 2004
From: psirab at inwind.it (Paolo Sirabella)
Date: Wed, 14 Apr 2004 01:37:23 +0200
Subject: [R] R 1.9.0 and cursors
Message-ID: <200404140137.24051.psirab@inwind.it>

Hi all,
I have successfully compiled and installed (OS: Linux Mandrake 9.2) the last 
devel version of R (ver. 1.9.0). All seems to go well, but now I cannot use 
the cursors for exploring back the command history (when I press the cursor 
key the following characters are shown: ^[[A , or ^[[B etc.).

Hints and suggestions are welcomed.

Thanks.

Paolo
-- 
-----------------------------
Paolo Sirabella, PhD
University of Rome "La Sapienza"
Dept. of Human Physiology and Pharmacology
Building of Human Physiology
P.le Aldo Moro, 5 - 00185 - Roma - Italy

Res Non Verba



From andy_liaw at merck.com  Wed Apr 14 01:54:58 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 13 Apr 2004 19:54:58 -0400
Subject: [R] R 1.9.0 and cursors
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BB8@usrymx25.merck.com>

Even if this is not covered in the FAQ, it sure had been asked many times on
R-help that it qualifies to be in the FAQ.  Make sure you have the readline
libraries & headers.  The last output of `configure' should show `readline'
if such capability is found to be working by the configure script.

Andy

> From: Paolo Sirabella
> 
> Hi all,
> I have successfully compiled and installed (OS: Linux 
> Mandrake 9.2) the last 
> devel version of R (ver. 1.9.0). All seems to go well, but 
> now I cannot use 
> the cursors for exploring back the command history (when I 
> press the cursor 
> key the following characters are shown: ^[[A , or ^[[B etc.).
> 
> Hints and suggestions are welcomed.
> 
> Thanks.
> 
> Paolo
> -- 
> -----------------------------
> Paolo Sirabella, PhD
> University of Rome "La Sapienza"
> Dept. of Human Physiology and Pharmacology
> Building of Human Physiology
> P.le Aldo Moro, 5 - 00185 - Roma - Italy
> 
> Res Non Verba
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From nedluk at yahoo.it  Wed Apr 14 02:38:57 2004
From: nedluk at yahoo.it (=?iso-8859-1?q?michele=20lux?=)
Date: Wed, 14 Apr 2004 02:38:57 +0200 (CEST)
Subject: [R] "diff"^-1  
Message-ID: <20040414003857.55495.qmail@web13606.mail.yahoo.com>

Yes kjetil (and other) my question wasn't clear but
what I was looking for was just diffinv!!
thanks michele



From tlumley at u.washington.edu  Wed Apr 14 02:41:11 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 13 Apr 2004 17:41:11 -0700 (PDT)
Subject: [R] Matrix question
In-Reply-To: <eed846eeedb3.eeedb3eed846@wiscmail.wisc.edu>
References: <eed846eeedb3.eeedb3eed846@wiscmail.wisc.edu>
Message-ID: <Pine.A41.4.58.0404131737090.100716@homer35.u.washington.edu>

On Tue, 13 Apr 2004, GIDEON WASSERBERG wrote:

> Dear Friends
>
> I am doing a simple matrix analysis to calculate the eigenvalue,
> eigenvector using R for the below matrix, and comparing the result to
> those obtained from a projection (using excel)
>
> THE MATRIX:
> > c
>      [,1] [,2] [,3]
> [1,]  0.0  2.0    2
> [2,]  0.8  0.0    0
> [3,]  0.0  0.8    0
>
>
> The dominant eigenvalue comes out comparable to that calculated
> numerically, but the eigenvectors do not( see below)!

Yes, they do.

Your dominant eigenvector is -0.6495461 times the R dominant eigenvector,
and eigenvectors are defined only up to direction. You probably want to
rescale the eigenvector so that the sums of entries are 1.

>
> EIGENVALUES (calculated by R):
>
> > eigen(c)
> $values
> [1]  1.5564082+0.000000i -0.7782041+0.465623i -0.7782041-0.465623i
>
> EIGENVALUE numerically calculated: 1.556408145
>
>
> EIGENVECTORS (calculated by R):
> $vectors
>               [,1]                  [,2]                  [,3]
> [1,] -0.8658084+0i  0.6476861+0.0000000i  0.6476861+0.0000000i
> [2,] -0.4450290+0i -0.4902997-0.2933611i -0.4902997+0.2933611i
> [3,] -0.2287467+0i  0.2382837+0.4441499i  0.2382837-0.4441499i
>
> Stable age distribution (calculated numerically):
>
> 0.562365145
> 0.289057934
> 0.148576921
>
>
> My questions are: 1. Both eigenvalue and eigenvectors are associated
> with some imaginary value (i). How should I relate to that information?

The first eigenvalue has zero imaginary component, as does its
eigenvector, so you may not need to relate to it.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From apjaworski at mmm.com  Wed Apr 14 02:50:42 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 13 Apr 2004 19:50:42 -0500
Subject: [R] Matrix question
In-Reply-To: <eed846eeedb3.eeedb3eed846@wiscmail.wisc.edu>
Message-ID: <OFC556FC57.57FA70AB-ON86256E76.000438D0-86256E76.0004A4A0@mmm.com>






Gideon,

Eigenvectors are normalized to unit length.  The first eigenvector
calculated by R is equal (ignoring the signs of course) to your stable
distribution vector divided by its length.

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           GIDEON WASSERBERG    |
|         |           <wasserberg at wisc.edu>|
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           04/13/2004 18:28     |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       "R-help at lists.R-project.org" <R-help at stat.math.ethz.ch>                                                      |
  |      cc:                                                                                                                    |
  |      Subject:  [R] Matrix question                                                                                          |
  >-----------------------------------------------------------------------------------------------------------------------------|




Dear Friends

I am doing a simple matrix analysis to calculate the eigenvalue,
eigenvector using R for the below matrix, and comparing the result to those
obtained from a projection (using excel)

THE MATRIX:
> c
     [,1] [,2] [,3]
[1,]  0.0  2.0    2
[2,]  0.8  0.0    0
[3,]  0.0  0.8    0


The dominant eigenvalue comes out comparable to that calculated
numerically, but the eigenvectors do not( see below)!


EIGENVALUES (calculated by R):

> eigen(c)
$values
[1]  1.5564082+0.000000i -0.7782041+0.465623i -0.7782041-0.465623i

EIGENVALUE numerically calculated: 1.556408145


EIGENVECTORS (calculated by R):
$vectors
              [,1]                  [,2]                  [,3]
[1,] -0.8658084+0i  0.6476861+0.0000000i  0.6476861+0.0000000i
[2,] -0.4450290+0i -0.4902997-0.2933611i -0.4902997+0.2933611i
[3,] -0.2287467+0i  0.2382837+0.4441499i  0.2382837-0.4441499i

Stable age distribution (calculated numerically):

0.562365145
0.289057934
0.148576921


My questions are:
1. Both eigenvalue and eigenvectors are associated with some imaginary
value (i). How should I relate to that information? 2. More importantly, a.
I presume the 1st eigenvector collumn [,1] should correspond to the
dominant eigenvalue. How come then that it comes out different from the one
calculated numerically? Is there some conversion I should do?

Many thanks

Gideon


Gideon Wasserberg (Ph.D.)
Wildlife research unit,
Department of wildlife ecology,
University of Wisconsin
218 Russell labs, 1630 Linden dr.,
Madison, Wisconsin 53706, USA.
Tel.:608 265 2130, Fax: 608 262 6099

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Wed Apr 14 03:15:31 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 13 Apr 2004 21:15:31 -0400
Subject: [R] Non-homogeneity of variance - decreasing variance
In-Reply-To: <IEEFKHGFNDGNNFCJALMFIEBDCEAA.s.chamaille@wanadoo.fr>
Message-ID: <20040414011528.GGIG6153.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Simon,

I'm not sure that I follow this entirely, but if error variance decreases
with the level of the response, you could try raising the response to a
power greater than 1. Of course, the response has to be non-negative. You
might take a look at the spread.level.plot function in the car package,
which will produce a suggested transformation when applied to an lm object.

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Simon Chamaill??
> Sent: Tuesday, April 13, 2004 12:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Non-homogeneity of variance - decreasing variance
> 
> Hello all,
> I'm running very simple regression but face a problem of 
> non-homogeneity of variance, but with a decreasing variance 
> with increasing mean...I do not know how to deal with that.
> this relationship doesn't seem to be strong, but it's my 
> first time to see something like that, and would like to know 
> what to do if one day it becomes stronger. I tested just for 
> fun some transformation but was not able to get a better 
> model. I do not know if it can help, but my predictor 
> variable is a kind of gamma poisson-shaped-like zero-rich 
> distribution (continuous of course), highly overdispersed.
> If one know how to deal with decreasing variance, I would 
> appreciate any advice (I tried to modelize negative 
> variance-mean relationship in a new
> quasi- family this was prohibited, only constant, mu, mu^x 
> (and mu(1-mu) for
> binomial) were allowed). I've definitively reached the border 
> of the statistical black box for me.
> thanks
> simon
>



From phddas at yahoo.com  Wed Apr 14 04:41:50 2004
From: phddas at yahoo.com (Fred J.)
Date: Tue, 13 Apr 2004 19:41:50 -0700 (PDT)
Subject: [R] un-expected return by fdim
Message-ID: <20040414024150.99351.qmail@web20504.mail.yahoo.com>

Browse[1]> Lframe
   v  v  v  v  v  v v v
1  8  7  6  5  4  3 2 1
2  9  8  7  6  5  4 3 2
3 10  9  8  7  6  5 4 3
4 11 10  9  8  7  6 5 4
5 12 11 10  9  8  7 6 5
6 13 12 11 10  9  8 7 6
7 14 13 12 11 10  9 8 7
8 15 14 13 12 11 10 9 8
Browse[1]> fdim(Lframe,q=2)
Error in slopeopt(AllPoints, Alpha) : Object "LineP"
not found

thanks for any feed back



From gregory_r_warnes at groton.pfizer.com  Wed Apr 14 04:42:08 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Tue, 13 Apr 2004 22:42:08 -0400
Subject: [R] Makefile for installing all available packages
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>


Below is a makefile I wrote to download and install all available R packages
from the CRAN and BioConductor package repositories.  

The primary advantage of using this makefile instead of R's built-in
install.package() and update.packages() is the creation of a separate
installation log for every package.   Further, if make is invoked with '-k',
failure to install a single package will not derail the installation of
other packages.

I hope that this script may be useful to other folks.  

-Greg

# Download and install all available R packages from the CRAN and
Bioconductor 
# package repositories
#
RCMD ?= R-1.9.0
WGET ?= wget -N -nd -r -A gz -r -l 1 -nv

PACKAGE_FILES = $(wildcard *.gz ) 
PACKAGE_LOGS  = $(addsuffix .log, $(basename $(basename $(PACKAGE_FILES))))

default: cran bioconductor install

cran: 
	$(WGET) "http://cran.r-project.org/src/contrib/PACKAGES.html"

bioconductor: bioCmain bioCcontrib bioCdata

bioCmain:
	$(WGET)
"http://www.bioconductor.org/repository/release1.3/package/html/index.html"

bioCcontrib:
	$(WGET) "http://www.bioconductor.org/contrib/index.html"

bioCdata:
	$(WGET) "http://www.bioconductor.org/data/metaData.html"

install: $(PACKAGE_LOGS)

%.log: %.tar.gz
	$(RCMD) INSTALL $< > $@.broken 2>&1 
	mv $@.broken $@



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From fred-l at poleto.com  Wed Apr 14 07:17:31 2004
From: fred-l at poleto.com (Frederico Zanqueta Poleto)
Date: Wed, 14 Apr 2004 02:17:31 -0300
Subject: [R] How does nlm work?
Message-ID: <407CC96B.6070302@poleto.com>

Dear R users,

I have looked in the reference
     Schnabel, R. B., Koontz, J. E. and Weiss, B. E. (1985) A modular
     system of algorithms for unconstrained minimization. _ACM Trans.
     Math. Software_, *11*, 419-440.
cited in the nlm help.

This article says that the algorithm permits the use of  step selection 
(line search, dogleg and optimal step), analytic or finite diference 
gradient and analytic, finite diference or BFGS Hessian aproximation.

Looking back in the nlm help, it has the information that:
a) it does just the line search step selecion;
b) it has the option to inform the gradient and the Hessian by 
attributes if the user wants.

My questions are:
1) When I do not supply the Hessian, the function does finite difference 
or BFGS approximation? (Is it possible to select one or other?)

2) I have already used the option to inform the gradient but I don't 
know how to inform the Hessian. Anybody has an example?

3) I have never heard of this step selections (line search, dogleg and 
optimal step). I would like to know something about it. I would 
appreciate if someone could send references for me to learn the subject.

Sincerely,

-- 
Frederico Zanqueta Poleto
fred at poleto.com
--
"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." J. W. Tukey



From jasont at indigoindustrial.co.nz  Wed Apr 14 07:49:29 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 14 Apr 2004 17:49:29 +1200 (NZST)
Subject: [R] How does nlm work?
In-Reply-To: <407CC96B.6070302@poleto.com>
References: <407CC96B.6070302@poleto.com>
Message-ID: <21171.203.9.176.60.1081921769.squirrel@webmail.maxnet.co.nz>

> 3) I have never heard of this step selections (line search, dogleg and
> optimal step). I would like to know something about it. I would
> appreciate if someone could send references for me to learn the subject.

IIRC, you'll find them here:

Nonlinear Regression Analysis and Its Applications
Douglas M. Bates and Donald G. Watts
John Wiley & Sons Inc.,
1988
ISBN: 0471816434

Cheers
Jason



From itayf at fhcrc.org  Wed Apr 14 08:30:28 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Tue, 13 Apr 2004 23:30:28 -0700 (PDT)
Subject: [R] dataframe: visualization as tiles(?)
Message-ID: <Pine.LNX.4.44.0404132313230.13029-100000@cezanne.fhcrc.org>


Dear R users,

I remember seeing somewhere a method of visualizing a set of 
observations on two variables x and y in the following way

	   x=0       x=1

	|-------| |-------|
  y=-1	|	| |	  |
	|-------| |	  |
	 	  |	  |
	|-------| |	  |
	|	| |-------|
  y=0	|	|
	|	| |-------|
	|-------| |	  |
	 	  |-------|
	|-------|
  y=1	|       | |-------|
	|-------| |-------|

where x = 0 or 1; y = -1, 0, 1. The 'tile' area represents 
the count of observations with corresponding x and y values.

Now, I don't remember what is the name of the functions that 
support such plots.

I tried help.search("*tile*"); I skimmed the documentation of 
the 'lattice' package. Both seem not to be what I remembered.

Please send me pointers.

	Thanks in advance
	Itay



From jasont at indigoindustrial.co.nz  Wed Apr 14 08:43:25 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 14 Apr 2004 18:43:25 +1200 (NZST)
Subject: [R] dataframe: visualization as tiles(?)
In-Reply-To: <Pine.LNX.4.44.0404132313230.13029-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0404132313230.13029-100000@cezanne.fhcrc.org>
Message-ID: <32431.203.9.176.60.1081925005.squirrel@webmail.maxnet.co.nz>

>
> Dear R users,
>
> I remember seeing somewhere a method of visualizing a set of
> observations on two variables x and y in the following way

Is this what you want?

> ## fake data
> zz <- data.frame(x=sample(0:1,20,rep=T),y=sample((-1:1),20,rep=T))
> zz

> ## tabulate it
> zz.tab <- data.frame(table(zz))
> zz.tab
> library(lattice)
> barchart(y ~ Freq | x, data=zz.tab)

Cheers

Jason



From jasont at indigoindustrial.co.nz  Wed Apr 14 08:49:21 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 14 Apr 2004 18:49:21 +1200 (NZST)
Subject: [R] dataframe: visualization as tiles(?)
In-Reply-To: <Pine.LNX.4.44.0404132313230.13029-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0404132313230.13029-100000@cezanne.fhcrc.org>
Message-ID: <33737.203.9.176.60.1081925361.squirrel@webmail.maxnet.co.nz>

Whoops - didn't get what you meant

?mosaicplot

is your friend

Cheers
Jason



From gregory_r_warnes at groton.pfizer.com  Wed Apr 14 13:00:30 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Wed, 14 Apr 2004 07:00:30 -0400
Subject: [R] R apache and PHP
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20680B2A4@groexmb02.pfizer.com>

We need to know what kind of error message you are getting before we can be
much help.

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Marcello Verona
> Sent: Tuesday, April 13, 2004 12:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R apache and PHP
> 
> 
> I've developed a web application in PHP and R
> 
> my script is
> 
> 
> <?php
> 
> ...
> exec("R CMD BATCH --silent /home/marcello/R_in/myfile.bat  
> /home/marcello/R_out/myfile.out");
> 
> ...
> 
> ?>
> 
> This script execute in R batch mode and write the myfile.out.
> 
> On Win2000 the similar script is ok, but on linux I've a problem.
> 
> I suppose is a permession problem because the same script on 
> shell run fine
> and on Zend debugger (my IDE for php) is also ok.
> In this case the owner is "marcello" , if I run the script by browser 
> the owner is "apache".
> 
> I've  overwritted all the ownerships of R directory and bin to apache 
> user but not work.
> 
> If a run
> exec("ls > mydir.txt"); is ok (is not a PHP general problem!)
> 
> Someone can help me?
> 
> Thanks
> (and excuse my for my poor english)
> 
> Marcello Verona
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From allan at stats.uct.ac.za  Wed Apr 14 13:31:32 2004
From: allan at stats.uct.ac.za (allan clark)
Date: Wed, 14 Apr 2004 13:31:32 +0200
Subject: [R] r: arma fitting
Message-ID: <407D2114.2452CDAD@stats.uct.ac.za>

hi all

i would like to model an AR model of the following form:

y(t) = a + p*y(t-5) + e(t)

where :

y(t) is the value of y at time t
a is a constant
p is the coefficient of the 5th lagged term
{e} is a normal error series

Any help will be appreciated
Allan



From sunnyho at ust.hk  Wed Apr 14 14:15:21 2004
From: sunnyho at ust.hk (Sunny Ho)
Date: Wed, 14 Apr 2004 20:15:21 +0800
Subject: [R] Re: Need advice on using R with large datasets
In-Reply-To: <200404131225.UAA01688@webmaild.ust.hk>
Message-ID: <200404141215.UAA12979@webmaild.ust.hk>

Thank you guys for sharing your experiences in 64-bit R. Those are very helpful in my planning work.

I wonder anyone has experience in using database interface with R. Are there any "preferred" choice or "hidden catch"? In our setup, we may be using MS SQL Server or Oracle to keep the data. I know that RODBC can talk to them directly. Could there be any 32-bit/64-bit compatibility issues, say, when a 32-bit ORACLE is talking to a 64-bit R ?

Performance wise, when used with R, how does MySQL or PostgreSQL compare to MS SQL Server or Oracle? 

Any comments will be helpful. Thanks

Sunny Ho
(Hong Kong University of Science & Technology)

> Hello everyone,
> 
> I would like to get some advices on using R with some really large datasets.
> 
> I'm using RH9 Linux R 1.8.1 for a research with a lot of numerical data. The datasets total to around 200Mb (shown by memory.size). During my data manipulation, the system memory usage grew to 1.5Gb, and this caused a lot of swapping activities on my 1Gb PC. This is just a small-scale experiment, the full-scale one will be using data 30 times as large (on a 4Gb machine). I can see that I'll need to deal with memory usage problem very soon.
> 
> I notice that R keeps all datasets in memory at all times. I wonder whether there is any way to instruct R to push some of the less-frequently-used data tables out of main memory, so as to free up memory for those that are actively in used. It'll be even better if R can keep only part of a table in memory only when that part is needed. Using save & load could help, but I just wonder whether R is intelligent enough to do this by itself, so I don't need to keep track of memory usage at all times.
> 
> Another thought is to use a 64-bit machine (AMD64). I find there is a pre-compiled R for Fedora Linux on AMD64. Anyone knows whether this version of R runs as 64-bit? If so, then will R be able to go beyond the 32-bit 4Gb memory limit?
> 
> Also, from the manual, I find that the RPgSQL package (for PostgreSQL database) supports a feature "proxy data frame". Does anyone have experience with this? Can "proxy data frame" handle memory efficiently for very large datasets? Say, if I have a 6Gb database table defined as a proxy data frame, will R & RPgSQL be able to handle it with just 4Gb of memory?
> 
> Any comments will be useful. Many thanks.
> 
> Sunny Ho
> (Hong Kong University of Science & Technology)
> 
>



From wildscop at yahoo.com  Wed Apr 14 14:21:04 2004
From: wildscop at yahoo.com (WilDscOp)
Date: Wed, 14 Apr 2004 18:21:04 +0600
Subject: [R] Non-Linear Regression Problem
Message-ID: <5.1.0.14.2.20040414181222.009f40b0@127.0.0.1>

Dear all,

	I was wondering if there is any way i could do a "Grid Search" on a 
parameter space using R (as SAS 6.12 and higher can do it) to start the 
Newton-Gauss Linearization least squares method when i have NO prior 
information about the parameter.
W. N. Venables and B. D. Ripley (2002) "Modern Applied Statistics with S", 
4 th ed., page 216-7  has a topic "Self-starting non-linear regressions" 
using negexp.SSival - but i can not solve my hypothetical problem using 
that - my problem is :

Y = EXP(-(THETA * t)) with data below for estimating THETA:

t    	Y
1	0.80
4 	0.45
16 	0.04

Whatever i could do, is in http://www.angelfire.com/ab5/get5/nonlinear.PDF

Any response / help / comment / suggestion / idea / web-link / replies will 
be greatly appreciated.

Thanks in advance for your time.

_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From jaa53 at cornell.edu  Wed Apr 14 14:41:29 2004
From: jaa53 at cornell.edu (Jose)
Date: Wed, 14 Apr 2004 08:41:29 -0400
Subject: [R] mvtnorm problems
Message-ID: <0D34DB98-8E11-11D8-BDB3-000393BBED36@cornell.edu>

Hi all!

My apologies for posting such a naive question. I've been trying to run 
multiple comparison contrasts on several GLM models. For doing so I've 
tried to use the mulcomp CRAN package. However, before I can make the 
package to work I have to load the mvtnorm package which I cannot find 
in CRAN (or anywhere else). Any inputs?

Thanks a lot,

/J

Jose A. Andres. Post-doc Associate.
Dpt. Ecology and Evolutionary Biology.
Cornell University. 14853 Ithaca,NY



From jfox at mcmaster.ca  Wed Apr 14 14:59:32 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 14 Apr 2004 08:59:32 -0400
Subject: [R] mvtnorm problems
In-Reply-To: <0D34DB98-8E11-11D8-BDB3-000393BBED36@cornell.edu>
Message-ID: <20040414125928.SKDG15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Jose,

The mvtnorm package is indeed on CRAN. I'm not sure why you can't find it.

John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jose
> Sent: Wednesday, April 14, 2004 7:41 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] mvtnorm problems
> 
> Hi all!
> 
> My apologies for posting such a naive question. I've been 
> trying to run multiple comparison contrasts on several GLM 
> models. For doing so I've tried to use the mulcomp CRAN 
> package. However, before I can make the package to work I 
> have to load the mvtnorm package which I cannot find in CRAN 
> (or anywhere else). Any inputs?
> 
> Thanks a lot,
> 
> /J
>



From ligges at statistik.uni-dortmund.de  Wed Apr 14 15:24:11 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 14 Apr 2004 15:24:11 +0200
Subject: [R] mvtnorm problems
In-Reply-To: <20040414125928.SKDG15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <20040414125928.SKDG15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <407D3B7B.2050909@statistik.uni-dortmund.de>

John Fox wrote:

> Dear Jose,
> 
> The mvtnorm package is indeed on CRAN. I'm not sure why you can't find it.
> 
> John 


What about simply trying

  install.packages("mvtnorm")

Uwe Ligges


> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jose
>>Sent: Wednesday, April 14, 2004 7:41 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] mvtnorm problems
>>
>>Hi all!
>>
>>My apologies for posting such a naive question. I've been 
>>trying to run multiple comparison contrasts on several GLM 
>>models. For doing so I've tried to use the mulcomp CRAN 
>>package. However, before I can make the package to work I 
>>have to load the mvtnorm package which I cannot find in CRAN 
>>(or anywhere else). Any inputs?
>>
>>Thanks a lot,
>>
>>/J
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From michael.wolf at upf.edu  Wed Apr 14 15:24:01 2004
From: michael.wolf at upf.edu (michael.wolf@upf.edu)
Date: Wed, 14 Apr 2004 15:24:01 +0200
Subject: [R] how to add legend to time series plot
Message-ID: <1081949041.407d3b7199414@kurla.upf.edu>

Dear all,

I would like to add a legend to a time series plot, but 
cannot get it done. I have searched the archive about this,
but to no avail ...

I have three sets of time series data stored in a matrix
wMat. The following code plots the the data with
a legend, but it does not put the time on the x-axis:


matplot(y = wMat, type = "l",
        ylab = "Allocation", main = "GARCH")
legend(1, -0.3, c("Stocks", "Bonds", "Cash"), col = 1:3, lty = 1:3)


The following code puts the time on the x-axis but now the
legend does not show up:


wMat = ts(wMat, frequency = 12, start = c(1968, 2))
ts.plot(wMat[,1], wMat[,2], wMat[,3], col = 1:3, lty = 1:3,
        ylab = "Allocation", main = "GARCH")
legend(1, -0.3, c("Stocks", "Bonds", "Cash"), col = 1:3, lty = 1:3)


Can anybody help with this? If needed, I can send along postscript files
of the resulting plots.

I do not subscribe to the list, so please (also) reply directly to me.

Thanks very much,
Michael



From jaa53 at cornell.edu  Wed Apr 14 15:40:16 2004
From: jaa53 at cornell.edu (Jose A. Andres)
Date: Wed, 14 Apr 2004 09:40:16 -0400 (EDT)
Subject: [R] mvtnorm problems (II)
Message-ID: <1333.132.236.111.115.1081950016.squirrel@webmail.cornell.edu>

Thanks a lot for your help!

Obviously I've tried to load it before by using the install.packages
function but it didn't work. This is what I got


> install.packages("mvtnorm")
trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 12485 bytes
opened URL
downloaded 12Kb

Warning message:
No package "mvtnorm" on CRAN. in: download.packages(pkgs, destdir = tmpd,
available = available,
>

I do not get what I am doing wrong...



From frohde_home at yahoo.com  Wed Apr 14 15:46:25 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Wed, 14 Apr 2004 06:46:25 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <Pine.A41.4.58.0404130724040.106200@homer38.u.washington.edu>
Message-ID: <20040414134625.69209.qmail@web60107.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040414/150c5a65/attachment.pl

From andy_liaw at merck.com  Wed Apr 14 16:18:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Apr 2004 10:18:04 -0400
Subject: [R] mvtnorm problems (II)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BBF@usrymx25.merck.com>

With R-1.9.0 Windows binary downloaded from CRAN, I get:

> install.packages("mvtnorm")
trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 17545 bytes
opened URL
downloaded 17Kb

trying URL
`http://cran.r-project.org/bin/windows/contrib/1.9/mvtnorm_0.6-6.zip'
Content type `application/zip' length 205955 bytes
opened URL
downloaded 201Kb

package 'mvtnorm' successfully unpacked and MD5 sums checked

Delete downloaded files (y/N)? y

updating HTML package descriptions


One thing to try is to add the --internet2 option to the Rgui.exe command in
the shortcut.  See the R for Windows FAQ.

HTH,
Andy

> From: Jose A. Andres
> 
> Thanks a lot for your help!
> 
> Obviously I've tried to load it before by using the install.packages
> function but it didn't work. This is what I got
> 
> 
> > install.packages("mvtnorm")
> trying URL 
> `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 12485 bytes
> opened URL
> downloaded 12Kb
> 
> Warning message:
> No package "mvtnorm" on CRAN. in: download.packages(pkgs, 
> destdir = tmpd,
> available = available,
> >
> 
> I do not get what I am doing wrong...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From RBaskin at ahrq.gov  Wed Apr 14 16:29:02 2004
From: RBaskin at ahrq.gov (Baskin, Robert)
Date: Wed, 14 Apr 2004 10:29:02 -0400
Subject: [R] Complex sample variances
Message-ID: <6BCD3F430455B1418750004BCD27925905C659@exchange2.ahrq.gov>

<     Construct a new weight within the stratum as the sample weight
multiplied by the frequency>
The correct formula for the new weights can be found in Chapter 6 of Shao
and Tu (1996) "The Jackknife and the Bootstrap", Springer

Also in:
" Keith Rust & Jon Rao have an overview article in Statistical Methods in
Medical Research (1996 vol 5, pp 283-310) which review most of the
literature and methods to that point (also see Shao & Tu's book Chapter 6).
They also give the correct formula for the bootstrap weights.  It is highly
recommended in Rust & Rao (referring to Rao & Wu) that for bootstrap you
select n(h)-1 out of n(h) PSUs in stratum h with replacement."

If you select n(h)-1 out of n(h) PSUS in strata h the new weight should be:

New-weight = Old-weight * frequency PSU is selected * n(h) / (n(h) - 1)

So if you randomly select 1 out 2 PSUs you double the weight because of the
factor n(h) / (n(h) - 1).

This method is basically randomly building BRR replicates (in a 2-per
design) so it is like an inefficient BRR and the number of bootstrap
replicates needed may depend on both the statistic being estimated and the
number of replicates in a fully balanced BRR set.

Bob



-----Original Message-----
From: Fred Rohde [mailto:frohde_home at yahoo.com] 
Sent: Wednesday, April 14, 2004 9:46 AM
To: Thomas Lumley
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Complex sample variances

I think I've figured out a way to do a bootstrap variance estimate of a
quantile.  I need to work out the code, but this is the algorithm (for a
stratified cluster sample):

   Make a list of the stratum values for the sample
   For each stratum value,
      Make a list of the PSU values within that stratum
      Sample n-1 PSU values with replacement
      Get the frequency of PSU values selected 
      Attach the frequency to the sample elements within the stratum by PSU
      Construct a new weight within the stratum as the sample weight
multiplied by the frequency

   Once the new weight is generated in all stratum, get the quantile
estimate(s) from svyquantile using the new weight
   Repeat another 99 times to build 100 bootstrap replicates
   Get the standard deviation of the replicate estimates as the variance

What do you think?  It's kind of general.  For stratified non-clustered
samples, the selections would be done on sample elements, not on PSUs, and
for non-stratified cluster cluster designs, the PSU selections would be done
across the whole sample, not by stratum.

I'm not that up with bootstrapping however.  I'm not sure how to set/save
the seed values so running the procedure again on the same dataset will
produce the same variance.

Fred
Thomas Lumley <tlumley at u.washington.edu> wrote:
On Mon, 12 Apr 2004, Fred Rohde wrote:

> Thanks. I'll update the survey package. Sudaan does the standard
> errors on quantiles using Taylor series. If I can hunt down the formula
> it uses, could you add that to svyquantile?

If I can bring myself to believe it. Computing standard errors for the
normal approximation to the median is not easy even in simple random
samples.

-thomas


> Fred
>
> Thomas Lumley wrote:
> On Mon, 12 Apr 2004, Fred Rohde wrote:
>
> > Hello,
> > Is there a way to get complex sample variances in the survey package on
> > summary statistics other than means? If not, can they be added to a
> > future version? It would be be great to have them on totals, quantiles,
> > ratios, and tables (eg row percent, columns percent, etc).
> >
>
> svytotal() and svyratio() will do this for totals and ratios if you have a
> new enough version. At the moment the easiest way to get row or column
> percentages is to think of them them as ratios of means of binary
> variables and use svyratio().
>
> Quantiles are more difficult, since neither Taylor series nor jackknife
> approaches work.
>
> -thomas
>
>
> ---------------------------------


Thomas Lumley Assoc. Professor, Biostatistics
tlumley at u.washington.edu University of Washington, Seattle

		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andrewr at uidaho.edu  Wed Apr 14 16:29:29 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 14 Apr 2004 07:29:29 -0700
Subject: [R] Aggregate drops empty subsets
Message-ID: <20040414142929.GP27983@uidaho.edu>

Greetings, R community.

I am trying to create a multi-dimensional contingency table suitable
for analysis by glm() using the poisson family.  I have three factors,
each with four levels, with some observed zeros.  I'm trying to use
aggregate to construct my contingency table, but it drops empty
subsets, so the zeros get lost.  I also tried tapply() but it doesn't
carry over the main effects, just the interactions.  I also tried
constructing a new factor from the interactions and merging it with
the contingency table but then I lost the main effects.  

Very small example: from the following dataframe

burn  age

low   young
high  old
low   old
low   young

I would want to distill

burn  age    burn.age     count
low   young  low.young    2
high  young  high.young   0
low   old    low.old      1
high  old    high.old     1

with a solution scaleable to many dimensions.

Is there any easy way to get around this problem?

Thanks for any suggestions,

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From ligges at statistik.uni-dortmund.de  Wed Apr 14 16:29:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 14 Apr 2004 16:29:43 +0200
Subject: [R] mvtnorm problems (II)
References: <1333.132.236.111.115.1081950016.squirrel@webmail.cornell.edu>
Message-ID: <407D4AD7.B5E40C4A@statistik.uni-dortmund.de>



"Jose A. Andres" wrote:
> 
> Thanks a lot for your help!
> 
> Obviously I've tried to load it before by using the install.packages
> function but it didn't work. This is what I got
> 
> > install.packages("mvtnorm")
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 12485 bytes
> opened URL
> downloaded 12Kb
> 
> Warning message:
> No package "mvtnorm" on CRAN. in: download.packages(pkgs, destdir = tmpd,
> available = available,
> >
> 
> I do not get what I am doing wrong...

Your R version is rather outdated (you haven't told us the version
number, nor your OS). 
There is only an outdated version of mvtnorm (0.6-3) that passed the
checks for R-1.7.x. You can get that outdated one from
http://cran.r-project.org/bin/windows/contrib/1.7/last/mvtnorm_0.6-3.zip.

I'd suggest to upgrade to R-1.9.0.

Uwe Ligges



From gb at stat.umu.se  Wed Apr 14 16:25:32 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 14 Apr 2004 16:25:32 +0200
Subject: [R] mvtnorm problems (II)
In-Reply-To: <1333.132.236.111.115.1081950016.squirrel@webmail.cornell.edu>
References: <1333.132.236.111.115.1081950016.squirrel@webmail.cornell.edu>
Message-ID: <20040414142532.GA17345@stat.umu.se>

On Wed, Apr 14, 2004 at 09:40:16AM -0400, Jose A. Andres wrote:
> Thanks a lot for your help!
> 
> Obviously I've tried to load it before by using the install.packages
> function but it didn't work. This is what I got
> 
> 
> > install.packages("mvtnorm")
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
                                                            ^^^
[...]

Try upgrading to R 1.9.0 first (it seems as if 'mvtnorm' is missing in 1.7)

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From ligges at statistik.uni-dortmund.de  Wed Apr 14 16:31:57 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 14 Apr 2004 16:31:57 +0200
Subject: [R] mvtnorm problems (II)
References: <3A822319EB35174CA3714066D590DCD504AF7BBF@usrymx25.merck.com>
Message-ID: <407D4B5D.540DB789@statistik.uni-dortmund.de>



"Liaw, Andy" wrote:
> 
> With R-1.9.0 Windows binary downloaded from CRAN, I get:
> 
> > install.packages("mvtnorm")
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 17545 bytes
> opened URL
> downloaded 17Kb
> 
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/1.9/mvtnorm_0.6-6.zip'
> Content type `application/zip' length 205955 bytes
> opened URL
> downloaded 201Kb
> 
> package 'mvtnorm' successfully unpacked and MD5 sums checked
> 
> Delete downloaded files (y/N)? y
> 
> updating HTML package descriptions
> 
> One thing to try is to add the --internet2 option to the Rgui.exe command in
> the shortcut.  See the R for Windows FAQ.

Won't be helpful here, since getting the packages' list worked. Note the
"1.7" in:
`http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
He managed to confuse us by not telling anything about the R version in
use.
                     
Uwe



> HTH,
> Andy
> 
> > From: Jose A. Andres
> >
> > Thanks a lot for your help!
> >
> > Obviously I've tried to load it before by using the install.packages
> > function but it didn't work. This is what I got
> >
> >
> > > install.packages("mvtnorm")
> > trying URL
> > `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> > Content type `text/plain; charset=iso-8859-1' length 12485 bytes
> > opened URL
> > downloaded 12Kb
> >
> > Warning message:
> > No package "mvtnorm" on CRAN. in: download.packages(pkgs,
> > destdir = tmpd,
> > available = available,
> > >
> >
> > I do not get what I am doing wrong...
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at PDF.COM  Wed Apr 14 16:43:25 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 14 Apr 2004 09:43:25 -0500
Subject: [R] Aggregate drops empty subsets
In-Reply-To: <20040414142929.GP27983@uidaho.edu>
References: <20040414142929.GP27983@uidaho.edu>
Message-ID: <407D4E0D.8060701@pdf.com>



Andrew Robinson wrote:

> Greetings, R community.
> 
> I am trying to create a multi-dimensional contingency table suitable
> for analysis by glm() using the poisson family.  I have three factors,
> each with four levels, with some observed zeros.  I'm trying to use
> aggregate to construct my contingency table, but it drops empty
> subsets, so the zeros get lost.  I also tried tapply() but it doesn't
> carry over the main effects, just the interactions.  I also tried
> constructing a new factor from the interactions and merging it with
> the contingency table but then I lost the main effects.  
> 
> Very small example: from the following dataframe
> 
> burn  age
> 
> low   young
> high  old
> low   old
> low   young
> 
> I would want to distill
> 
> burn  age    burn.age     count
> low   young  low.young    2
> high  young  high.young   0
> low   old    low.old      1
> high  old    high.old     1
> 
> with a solution scaleable to many dimensions.
> 
> Is there any easy way to get around this problem?
> 
> Thanks for any suggestions,
> 
> Andrew

Hi Andrew,

How about:

ui <- data.frame(burn = c("low", "high", "low", "low"),
                  age = c("young", "old", "old", "young"),
                  junk = c("a", "a", "b", "b"))
ui2 <- do.call("table", ui)
ui3 <- expand.grid(dimnames(ui2))
ui3[paste(names(ui), collapse = ".")] <-
   do.call("paste", c(ui3, sep = "."))
ui3$count <- c(ui2)

I'm using R-1.9.0 on win2000.

--sundar



From andrewr at uidaho.edu  Wed Apr 14 16:53:19 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 14 Apr 2004 07:53:19 -0700
Subject: [R] Aggregate drops empty subsets
In-Reply-To: <407D4E0D.8060701@pdf.com>
References: <20040414142929.GP27983@uidaho.edu> <407D4E0D.8060701@pdf.com>
Message-ID: <20040414145319.GR27983@uidaho.edu>

Hi Sundar,

that's excellent - and a very educational answer.  

Thanks very much!

Andrew

> 
> Hi Andrew,
> 
> How about:
> 
> ui <- data.frame(burn = c("low", "high", "low", "low"),
>                  age = c("young", "old", "old", "young"),
>                  junk = c("a", "a", "b", "b"))
> ui2 <- do.call("table", ui)
> ui3 <- expand.grid(dimnames(ui2))
> ui3[paste(names(ui), collapse = ".")] <-
>   do.call("paste", c(ui3, sep = "."))
> ui3$count <- c(ui2)
> 
> I'm using R-1.9.0 on win2000.
> 
> --sundar
> 
> 

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From wettenhall at wehi.edu.au  Wed Apr 14 16:55:46 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Thu, 15 Apr 2004 00:55:46 +1000 (EST)
Subject: [R] Passing a pointer to .C() in Win32
Message-ID: <Pine.LNX.4.58.0404142353120.23757@unix28.alpha.wehi.edu.au>

Hi,

Is there any way to pass an integer from R to C and have 
it cast as a pointer?  

# Win32 Example:

library(tcltk)
tt <- tktoplevel()
hWndString <- tclvalue(tkwm.frame(tt))
# I'll avoid posting code to this function:
source("http://bioinf.wehi.edu.au/folders/james/R/hexStringToDecimalInteger.R")
hWnd <- hexStringToDecimalInteger(hWndString)
system32 <- file.path(Sys.getenv("windir"),"system32")
user32 <- file.path(system32,"user32.dll")
dyn.load(user32)

# WARNING: THIS NEXT LINE WILL PASS AN INVALID WINDOW HANDLE TO 
# USER32.DLL AND PROBABLY CRASH YOUR R SESSION!!!!
.C("SetForegroundWindow",hWnd)

# This above won't work, because .C() will pass a pointer to the 
# integer hWnd to SetForegroundWindow (in user32.dll) whereas 
# I want the integer hWnd to be cast as a pointer.

So for each DLL function I want to call with a pointer argument, 
do I have to define my own C function e.g. my_SetForegroundWindow 
(using the Windows API) which takes an integer argument instead 
and then casts it as a pointer in order to call the real 
SetForegroundWindow function?

Of course environments behave like pointers but they are 
read-only.  You can't do this (below), right?
mode(hWnd) <- "environment"

WHY WOULD I WANT TO DO ANYTHING LIKE THIS ABOVE?

I know that I can use tkfocus for Tk windows, but I have other 
applications in mind.  In Win32, when I run RGui with MDI, the 
bringToTop in tcltk's .onLoad brings the console to the top, but 
doesn't focus it, i.e. after library(tcltk), I can't type into 
the console until I click on it.  Now if I had RConsole->handle 
from the GraphApp code for RGui, maybe I could try something 
like SetForegroundWindow... For more info, go to 
http://msdn.microsoft.com and search for "BringWindowToTop", 
search for "SetForegroundWindow", and compare...

Maybe for this particular application, something like 
show(RConsole) might be better, (see R-devel thread below)
http://www.mail-archive.com/r-devel at stat.math.ethz.ch/msg01829.html

but I'm still wondering if casting to pointers can be done...

Another application is specifying a Tk window to be "Always On 
Top" using user32.dll's SetWindowPos function (again, search 
for it on http://msdn.microsoft.com).

Regards,
James



From marcelloverona at tiscali.it  Wed Apr 14 17:01:33 2004
From: marcelloverona at tiscali.it (marcelloverona@tiscali.it)
Date: Wed, 14 Apr 2004 17:01:33 +0200
Subject: [R] R apache and PHP
In-Reply-To: <D7A3CFD7825BD6119B880002A58F06C20680B2A4@groexmb02.pfizer.com>
Message-ID: <40630A000002F3E5@mail-1.tiscali.it>


I've  solved!
The correct way is the full path.

Not "R CMD...."

But "/usr/bla/bla/R CMD ..."

Thank you

Marcello Verona


>We need to know what kind of error message you are getting before we can
>be
>much help.
>
>-Greg
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Marcello Verona
>> Sent: Tuesday, April 13, 2004 12:37 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] R apache and PHP
>> 
>> 
>> I've developed a web application in PHP and R
>> 
>> my script is
>> 
>> 
>> <?php
>> 
>> ...
>> exec("R CMD BATCH --silent /home/marcello/R_in/myfile.bat  
>> /home/marcello/R_out/myfile.out");
>> 
>> ...
>> 
>> ?>
>> 
>> This script execute in R batch mode and write the myfile.out.
>> 
>> On Win2000 the similar script is ok, but on linux I've a problem.
>> 
>> I suppose is a permession problem because the same script on 
>> shell run fine
>> and on Zend debugger (my IDE for php) is also ok.
>> In this case the owner is "marcello" , if I run the script by browser

>> the owner is "apache".
>> 
>> I've  overwritted all the ownerships of R directory and bin to apache

>> user but not work.
>> 
>> If a run
>> exec("ls > mydir.txt"); is ok (is not a PHP general problem!)
>> 
>> Someone can help me?
>> 
>> Thanks
>> (and excuse my for my poor english)
>> 
>> Marcello Verona
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
>
>LEGAL NOTICE
>Unless expressly stated otherwise, this message is confidential and may
be
>privileged. It is intended for the addressee(s) only. Access to this E-mail
>by anyone else is unauthorized. If you are not an addressee, any disclosure
>or copying of the contents of this E-mail or any action taken (or not taken)
>in reliance on it is unauthorized and may be unlawful. If you are not an
>addressee, please inform the sender immediately.


__________________________________________________________________



From tlumley at u.washington.edu  Wed Apr 14 17:08:35 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 14 Apr 2004 08:08:35 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <6BCD3F430455B1418750004BCD27925905C659@exchange2.ahrq.gov>
References: <6BCD3F430455B1418750004BCD27925905C659@exchange2.ahrq.gov>
Message-ID: <Pine.A41.4.58.0404140806120.26790@homer12.u.washington.edu>

On Wed, 14 Apr 2004, Baskin, Robert wrote:

>
> This method is basically randomly building BRR replicates (in a 2-per
> design) so it is like an inefficient BRR and the number of bootstrap
> replicates needed may depend on both the statistic being estimated and the
> number of replicates in a fully balanced BRR set.
>

And the survey package does already have JK1, JKn, BRR, and Fay's method:
you can convert a survey.design object with design information to a
svrep.design object with replicate weights and proceed a la WesVar.

I've added a svrepquantile function, which will be in a new release soon
-- I need to finish revising it for JSS.

	-thomas



From k.hansen at biostat.ku.dk  Wed Apr 14 17:07:42 2004
From: k.hansen at biostat.ku.dk (Kasper Daniel Hansen)
Date: Wed, 14 Apr 2004 17:07:42 +0200
Subject: [R] Re: [BioC] Makefile for installing all available packages
In-Reply-To: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
References: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
Message-ID: <20040414150742.GB25710@carter.kubism.ku.dk>

On Tue, Apr 13, 2004 at 10:42:08PM -0400, Warnes, Gregory R wrote:
> Below is a makefile I wrote to download and install all available R packages
> from the CRAN and BioConductor package repositories.  

You do not find it a bit overkill to install all of CRAN per default?
-- 
Kasper Daniel Hansen, Research Assistant
Department of Biostatistics, University of Copenhagen



From Bernhard.Pfaff at drkw.com  Wed Apr 14 17:23:47 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Wed, 14 Apr 2004 17:23:47 +0200
Subject: [R] r: arma fitting
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B8BE@ibfftce505.is.de.dresdnerkb.com>

> 
> hi all
> 
> i would like to model an AR model of the following form:
> 
> y(t) = a + p*y(t-5) + e(t)
> 

Hello Allan,

why not simply lm(), such as below?

n <- length(y)
lm(y[-c(1:5)] ~ y[-c((n-4):n)])

or, alternatively you could restrict coefficients ar(1) to ar(4) to zero by
using:

arima(y, order=c(5, 0, 0), fixed=c(0, 0, 0, 0, NA, NA), method="CSS")

HTH,
Bernhard


> where :
> 
> y(t) is the value of y at time t
> a is a constant
> p is the coefficient of the 5th lagged term
> {e} is a normal error series
> 
> Any help will be appreciated
> Allan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From dmurdoch at pair.com  Wed Apr 14 17:42:50 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 14 Apr 2004 11:42:50 -0400
Subject: [R] Re: [BioC] Makefile for installing all available packages
In-Reply-To: <20040414150742.GB25710@carter.kubism.ku.dk>
References: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
	<20040414150742.GB25710@carter.kubism.ku.dk>
Message-ID: <9slq70hneav7toc5b87tl8492sbqdp4cn4@4ax.com>

On Wed, 14 Apr 2004 17:07:42 +0200, Kasper Daniel Hansen
<k.hansen at biostat.ku.dk> wrote :

>On Tue, Apr 13, 2004 at 10:42:08PM -0400, Warnes, Gregory R wrote:
>> Below is a makefile I wrote to download and install all available R packages
>> from the CRAN and BioConductor package repositories.  
>
>You do not find it a bit overkill to install all of CRAN per default?

I can see two situations where this would be desirable:

On a laptop or other machine that is often not connected to the net:
install everything just in case you might want something and can't go
to CRAN for it.

In a public lab where users have limited ability to install packages,
it's good to have them all there.

Duncan Murdoch



From lforzani at stat.umn.edu  Wed Apr 14 17:43:29 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Wed, 14 Apr 2004 10:43:29 -0500 (CDT)
Subject: [R] random effect, double repeated measuses
Message-ID: <Pine.LNX.4.44.0404141041480.18981-100000@muskrat.stat.umn.edu>


Hi, I was trying to use GEE or glmmML to fit a poisson model, but my
response is double repeated meassure. Does anybody now if there is a way
to to this using R?

thanks so much

Liliana Forzani



From lforzani at stat.umn.edu  Wed Apr 14 17:44:14 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Wed, 14 Apr 2004 10:44:14 -0500 (CDT)
Subject: [R] random effect, double repeated measuses 
Message-ID: <Pine.LNX.4.44.0404141044040.18981-100000@muskrat.stat.umn.edu>


Hi, I was trying to use GEE or glmmML to fit a poisson model, but my
response is double repeated meassure. Does anybody now if there is a way
to to this using R?

thanks so much

Liliana Forzani



From laura at env.leeds.ac.uk  Wed Apr 14 17:48:26 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 14 Apr 2004 16:48:26 +0100 (BST)
Subject: [R] prcomp - error code 18
Message-ID: <Pine.LNX.4.44.0404141645510.3935-100000@env-pc-phd13>

I am attempting to perform a pca on a data frame of dimension 5000x19, but
when I execute

pcapres<-prcomp(pres,center=TRUE)

the following error message is returned:

Error in La.svd(x, nu, nv, method) : error code 18 from Lapack routine
dgesdd

Where am I going wrong? I am running R-1.8.0 on Debian.

Regards,
Laura



From solares at unsl.edu.ar  Wed Apr 14 17:58:09 2004
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Wed, 14 Apr 2004 12:58:09 -0300 (ART)
Subject: [R] odbcFetchRows not work?
Message-ID: <49697.170.210.173.216.1081958289.squirrel@inter17.unsl.edu.ar>

Hi, excuse me i am newbee in R, but i dont understand the following code
not work. Libro1.xls have two files and not load in R, under windows. Thanks

 library(RODBC)
> f<-file.choose()
> f
[1] "C:\\Mis documentos\\ruben\\r19\\Libro1.xls"
> help(odbcConnectExcel)
> canal<- odbcConnectExcel(f)
> canal
RODB Connection 0
Details:
  case=nochange
  DBQ=C:\Mis documentos\ruben\r19\Libro1.xls
  DefaultDir=C:\Mis documentos\ruben\r19
  Driver={Microsoft Excel Driver (*.xls)}
  DriverId=790
  MaxBufferSize=2048
  PageTimeout=5
> help(odbcFetchRows)
> tbl<- odbcFetchRows(canal, max = 0)
> tbl
$data
list()

$stat
[1] -1

> tbl<- odbcFetchRows(canal, max = 10)
> tbl
$data
list()

$stat
[1] -1



From frohde_home at yahoo.com  Wed Apr 14 18:00:53 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Wed, 14 Apr 2004 09:00:53 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <Pine.A41.4.58.0404140806120.26790@homer12.u.washington.edu>
Message-ID: <20040414160053.59696.qmail@web60103.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040414/6bcf3e33/attachment.pl

From datkins at u.washington.edu  Wed Apr 14 18:13:39 2004
From: datkins at u.washington.edu (Dave Atkins)
Date: Wed, 14 Apr 2004 09:13:39 -0700
Subject: [R] ltext, plotmath, and substitute
Message-ID: <407D6333.8060801@u.washington.edu>


I am interested to use plotmath functions within a panel function but am having 
some problems getting the code right.  Within each panel I am plotting the data, 
fitting a regression line, and would like to print the regression equation. 
Here is a trivial example of what I'd like to do:

# generate simple data
tmp.df <- data.frame(id = rep(1:4, each=4),
			time = rep(1:4, 4),
			das = rnorm(16))
# plot regression lines and print equations
xyplot(das ~ time | as.factor(id), data = tmp.df, strip = T,
	   panel = function(x,y,...){
		 panel.coef <- round(coef(lm(y~x)), 2)
		 panel.grid()
		 panel.xyplot(x,y)
		 panel.lmline(x,y)
		 ltext(1.2, 1, pos = 4,
		   paste("Y = ",panel.coef[1]," + ",panel.coef[2],"*time",
			sep = ""), cex = 1.2)
		 },
	   layout = c(4,1,1), aspect = 2.5)

The above works fine.  However, I'd like to make the equations more attractive 
by adding a hat over Y and an epsilon at the end, and this is where I have run 
into problems.  I experimented with expression() and paste() for a while without 
luck, and now believe I need to use substitute() (given some of the examples on 
the plotmath() help-page).  But, I can't get the code quite right.

Any suggestions appreciated.

Dave Atkins
datkins at u.washington.edu



From spencer.graves at pdf.com  Wed Apr 14 18:26:51 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 14 Apr 2004 09:26:51 -0700
Subject: [R] Non-Linear Regression Problem
In-Reply-To: <5.1.0.14.2.20040414181222.009f40b0@127.0.0.1>
References: <5.1.0.14.2.20040414181222.009f40b0@127.0.0.1>
Message-ID: <407D664B.1000603@pdf.com>

1.  For the equation you mentioned, have you considered the following: 

DF <- data.frame(t.=c(1, 4, 16), Y=c(.8, .45, .04))
# I do NOT use "t" as a name, as it
# may conflict with the matrix transpose function.
fit0 <- lm(log(Y)~t.-1, DF)
fit0
Call:
lm(formula = log(Y) ~ t. - 1, data = DF)

Coefficients:
     t. 
-0.2012 
################

      If this is the problem you really wanted to solve AND you honestly 
need NONLINEAR least squares, I would expect that (-0.2) should provide 
a reasonable starting value for nls: 

 > fit1 <- nls(Y~exp(-THETA*t.), data=DF, start=c(THETA=-0.2))
 > fit1
Nonlinear regression model
  model:  Y ~ exp(-THETA * t.)
   data:  DF
    THETA
0.2034489
 residual sum-of-squares:  0.0003018337
 >

      2.  Alternatively, you could compute the sum of squares for all 
values of THETA = seq(0, .01, 100) in a loop, then find the minimum by 
eye. 

      3.  If this is just a toy example, and your real problem has 
several parameters, "expand.grid" will produce a grid, and you can 
compute the value of your function and the sum of squares of residuals 
at every point in the grid in a single loop, etc. 

      hope this helps.  spencer graves

WilDscOp wrote:

> Dear all,
>
>     I was wondering if there is any way i could do a "Grid Search" on 
> a parameter space using R (as SAS 6.12 and higher can do it) to start 
> the Newton-Gauss Linearization least squares method when i have NO 
> prior information about the parameter.
> W. N. Venables and B. D. Ripley (2002) "Modern Applied Statistics with 
> S", 4 th ed., page 216-7  has a topic "Self-starting non-linear 
> regressions" using negexp.SSival - but i can not solve my hypothetical 
> problem using that - my problem is :
>
> Y = EXP(-(THETA * t)) with data below for estimating THETA:
>
> t        Y
> 1    0.80
> 4     0.45
> 16     0.04
>
> Whatever i could do, is in 
> http://www.angelfire.com/ab5/get5/nonlinear.PDF
>
> Any response / help / comment / suggestion / idea / web-link / replies 
> will be greatly appreciated.
>
> Thanks in advance for your time.
>
> _______________________
>
> Mohammad Ehsanul Karim <wildscop at yahoo.com>
> Institute of Statistical Research and Training
> University of Dhaka, Dhaka- 1000, Bangladesh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Wed Apr 14 18:28:23 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 14 Apr 2004 12:28:23 -0400
Subject: [R] Re: [BioC] Makefile for installing all available packages
In-Reply-To: <9slq70hneav7toc5b87tl8492sbqdp4cn4@4ax.com>
References: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
	<20040414150742.GB25710@carter.kubism.ku.dk>
	<9slq70hneav7toc5b87tl8492sbqdp4cn4@4ax.com>
Message-ID: <20040414162823.GA4542@psych>

On 04/14/04 11:42, Duncan Murdoch wrote:
>On Wed, 14 Apr 2004 17:07:42 +0200, Kasper Daniel Hansen
><k.hansen at biostat.ku.dk> wrote :
>>You do not find it a bit overkill to install all of CRAN per default?
>
>I can see two situations where this would be desirable:
>
>On a laptop or other machine that is often not connected to the net:
>install everything just in case you might want something and can't go
>to CRAN for it.
>
>In a public lab where users have limited ability to install packages,
>it's good to have them all there.

A third reason:  You run a search site where you want people to
be able to search all the help files, such as the one at the end
of my sig.

May 1, after updating to R-1.9, I plan to try this script to see
if it knows not to update files that haven't changed, and add
some other odds and ends that I have installed.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From bates at stat.wisc.edu  Wed Apr 14 18:29:17 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 14 Apr 2004 11:29:17 -0500
Subject: [R] Non-Linear Regression Problem
In-Reply-To: <5.1.0.14.2.20040414181222.009f40b0@127.0.0.1>
References: <5.1.0.14.2.20040414181222.009f40b0@127.0.0.1>
Message-ID: <6rfzb6o74y.fsf@bates4.stat.wisc.edu>

WilDscOp <wildscop at yahoo.com> writes:

> 	I was wondering if there is any way i could do a "Grid Search"
> on a parameter space using R (as SAS 6.12 and higher can do it) to
> start the Newton-Gauss Linearization least squares method when i have
> NO prior information about the parameter.
> 
> W. N. Venables and B. D. Ripley (2002) "Modern Applied Statistics with
> S", 4 th ed., page 216-7  has a topic "Self-starting non-linear
> regressions" using negexp.SSival - but i can not solve my hypothetical
> problem using that - my problem is :
> 
> 
> Y = EXP(-(THETA * t)) with data below for estimating THETA:
> 
> t    	Y
> 1	0.80
> 4 	0.45
> 16 	0.04
> 
> Whatever i could do, is in http://www.angelfire.com/ab5/get5/nonlinear.PDF
> 
> Any response / help / comment / suggestion / idea / web-link / replies
> will be greatly appreciated.
> 
> 
> Thanks in advance for your time.

> angel = read.table("/tmp/angelfire.dat", header = TRUE)
> angel
   t    Y
1  1 0.80
2  4 0.45
3 16 0.04
> lm(log(Y) ~ t - 1, angel)

Call:
lm(formula = log(Y) ~ t - 1, data = angel)

Coefficients:
      t  
-0.2012  

> fm = nls(Y~ exp(-(theta*t)), angel, c(theta = 0.2012), trace = TRUE)
0.0003229897 :  0.2012 
0.0003018397 :  0.2034108 
0.0003018337 :  0.2034484 
0.0003018337 :  0.2034489 
> summary(fm)

Formula: Y ~ exp(-(theta * t))

Parameters:
      Estimate Std. Error t value Pr(>|t|)    
theta 0.203449   0.006002   33.90  0.00087 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 0.01228 on 2 degrees of freedom



From rossini at blindglobe.net  Wed Apr 14 18:41:24 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 14 Apr 2004 09:41:24 -0700
Subject: [R] Re: [BioC] Makefile for installing all available packages
In-Reply-To: <20040414162823.GA4542@psych> (Jonathan Baron's message of
	"Wed, 14 Apr 2004 12:28:23 -0400")
References: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
	<20040414150742.GB25710@carter.kubism.ku.dk>
	<9slq70hneav7toc5b87tl8492sbqdp4cn4@4ax.com>
	<20040414162823.GA4542@psych>
Message-ID: <85oepusea3.fsf@servant.blindglobe.net>

Jonathan Baron <baron at psych.upenn.edu> writes:

> On 04/14/04 11:42, Duncan Murdoch wrote:
>>On Wed, 14 Apr 2004 17:07:42 +0200, Kasper Daniel Hansen
>><k.hansen at biostat.ku.dk> wrote :
>>>You do not find it a bit overkill to install all of CRAN per default?
>>
>>I can see two situations where this would be desirable:
>>
>>On a laptop or other machine that is often not connected to the net:
>>install everything just in case you might want something and can't go
>>to CRAN for it.
>>
>>In a public lab where users have limited ability to install packages,
>>it's good to have them all there.
>
> A third reason:  You run a search site where you want people to
> be able to search all the help files, such as the one at the end
> of my sig.
>
> May 1, after updating to R-1.9, I plan to try this script to see
> if it knows not to update files that haven't changed, and add
> some other odds and ends that I have installed.

On a single R installation, an alternative R function to accomplish
something similar is: 

installNewCRANPackages <- function() {
  ## (C) A.J. Rossini, 2002--2004                       
  test2 <-  packageStatus()$avail["Status"]
  install.packages(row.names(test2)[which(test2$Status=="not installed")])
}

which gratuitiously installs all new packages whether you want them or
not.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Wed Apr 14 18:46:13 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 14 Apr 2004 09:46:13 -0700
Subject: [R] Re: [BioC] Makefile for installing all available packages
In-Reply-To: <85oepusea3.fsf@servant.blindglobe.net> (A. J. Rossini's
	message of "Wed, 14 Apr 2004 09:41:24 -0700")
References: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
	<20040414150742.GB25710@carter.kubism.ku.dk>
	<9slq70hneav7toc5b87tl8492sbqdp4cn4@4ax.com>
	<20040414162823.GA4542@psych> <85oepusea3.fsf@servant.blindglobe.net>
Message-ID: <85k70ise22.fsf@servant.blindglobe.net>


I should mention that this function is more useful after calling

  update.packages()

best,
-tony


rossini at blindglobe.net (A.J. Rossini) writes:

> Jonathan Baron <baron at psych.upenn.edu> writes:
>
>> On 04/14/04 11:42, Duncan Murdoch wrote:
>>>On Wed, 14 Apr 2004 17:07:42 +0200, Kasper Daniel Hansen
>>><k.hansen at biostat.ku.dk> wrote :
>>>>You do not find it a bit overkill to install all of CRAN per default?
>>>
>>>I can see two situations where this would be desirable:
>>>
>>>On a laptop or other machine that is often not connected to the net:
>>>install everything just in case you might want something and can't go
>>>to CRAN for it.
>>>
>>>In a public lab where users have limited ability to install packages,
>>>it's good to have them all there.
>>
>> A third reason:  You run a search site where you want people to
>> be able to search all the help files, such as the one at the end
>> of my sig.
>>
>> May 1, after updating to R-1.9, I plan to try this script to see
>> if it knows not to update files that haven't changed, and add
>> some other odds and ends that I have installed.
>
> On a single R installation, an alternative R function to accomplish
> something similar is: 
>
> installNewCRANPackages <- function() {
>   ## (C) A.J. Rossini, 2002--2004                       
>   test2 <-  packageStatus()$avail["Status"]
>   install.packages(row.names(test2)[which(test2$Status=="not installed")])
> }
>
> which gratuitiously installs all new packages whether you want them or
> not.
>
> best,
> -tony
>
> -- 
> rossini at u.washington.edu            http://www.analytics.washington.edu/ 
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>
> CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Joerg.Schaber at uv.es  Wed Apr 14 19:05:32 2004
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Wed, 14 Apr 2004 19:05:32 +0200
Subject: [R] trend turning points
Message-ID: <407D6F5C.3060403@uv.es>

Hi,

does anybody know of a nice test to detect trend turning points in time 
series? Possibly with reference?
Thanks,

joerg



From andy_liaw at merck.com  Wed Apr 14 19:13:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Apr 2004 13:13:11 -0400
Subject: [R] trend turning points
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BD0@usrymx25.merck.com>

I don't know about time series data, but if the "errors" are independent
(and preferably constant variance), wouldn't this amounts to estimating
zeroes in the first derivative of the trend?  I believe several packages for
smoothing (e.g., KernSmooth and locfit) can estimate derivatives.  J. S.
Marron's SiZer actually tests for significance of the zeroes, but that has
not been implemented in R, AFAIK.  Marron's web site has Matlab code for it.

Andy

> From: Joerg Schaber
> 
> Hi,
> 
> does anybody know of a nice test to detect trend turning 
> points in time 
> series? Possibly with reference?
> Thanks,
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From xt_wang at cs.concordia.ca  Wed Apr 14 19:20:05 2004
From: xt_wang at cs.concordia.ca (xt_wang@cs.concordia.ca)
Date: Wed, 14 Apr 2004 13:20:05 -0400
Subject: [R] question about /nmath/standalone
Message-ID: <1081963205.407d72c599e9b@mailhost.cs.concordia.ca>

Hello,

I can't link a c code with Mathlib according to introduction of R 
manual "Writing R Extensions", page 60.

It is written :
"It is possible to build Mathlib, the R set of mathematical functions 
documented in
'Rmath.h?, as a standalone library 'libRmath? under Unix and Windows. (This 
includes
the functions documented in Section 5.7 [Numerical analysis subroutines], page 
61 as fromthat header file.)

The library is not built automatically when R is installed, but can be built 
in the
directory 'src/nmath/standalone? in the R sources: see the file 'README? 
there. To use thecode in your own C program include

#define MATHLIB_STANDALONE
#include <Rmath.h>

and link against '-lRmath?. There is an example file 'test.c?."

Have I to do: gcc -lRmath test.c ? In this case I receive this message: 

> /usr/bin/ld: cannot find -lRmath 
> collect2: ld returned 1 exit status "

what should I do? or I do something before I use "gcc -lRmath test.c" to 
compile c code test.c?

I will appreciate if you can help me resolve this problem.

yours,

Maggie Wang



From Achim.Zeileis at wu-wien.ac.at  Wed Apr 14 19:24:23 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 14 Apr 2004 19:24:23 +0200
Subject: [R] trend turning points
In-Reply-To: <407D6F5C.3060403@uv.es>
References: <407D6F5C.3060403@uv.es>
Message-ID: <20040414192423.1c4b0e1d.Achim.Zeileis@wu-wien.ac.at>

On Wed, 14 Apr 2004 19:05:32 +0200 Joerg Schaber wrote:

> Hi,
> 
> does anybody know of a nice test to detect trend turning points in
> time series? Possibly with reference?

You can look at the function breakpoints() in the package strucchange
and the function segmented() in the package segmented which do
segmentation of (generalized) linear regression models. The former tries
to fit fully segmented regression models, the latter broken line trends.
References are given on the respective help pages.

A suitable test for a change in trend in linear regression models is the
OLS-based CUSUM test with a Cramer-von Mises functional of
Kraemer & Ploberger (1996, JoE) which is available via efp() in
strucchange and associated methods.

hth,
Z

> Thanks,
> 
> joerg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at PDF.COM  Wed Apr 14 19:44:21 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 14 Apr 2004 12:44:21 -0500
Subject: [R] question about /nmath/standalone
In-Reply-To: <1081963205.407d72c599e9b@mailhost.cs.concordia.ca>
References: <1081963205.407d72c599e9b@mailhost.cs.concordia.ca>
Message-ID: <407D7875.1080006@pdf.com>


xt_wang at cs.concordia.ca wrote:

> Hello,
> 
> I can't link a c code with Mathlib according to introduction of R 
> manual "Writing R Extensions", page 60.
> 
> It is written :
> "It is possible to build Mathlib, the R set of mathematical functions 
> documented in
> 'Rmath.h?, as a standalone library 'libRmath? under Unix and Windows. (This 
> includes
> the functions documented in Section 5.7 [Numerical analysis subroutines], page 
> 61 as fromthat header file.)
> 
> The library is not built automatically when R is installed, but can be built 
> in the
> directory 'src/nmath/standalone? in the R sources: see the file 'README? 
> there. To use thecode in your own C program include
> 
> #define MATHLIB_STANDALONE
> #include <Rmath.h>
> 
> and link against '-lRmath?. There is an example file 'test.c?."
> 
> Have I to do: gcc -lRmath test.c ? In this case I receive this message: 
> 
> 
>>/usr/bin/ld: cannot find -lRmath 
>>collect2: ld returned 1 exit status "
> 
> 
> what should I do? or I do something before I use "gcc -lRmath test.c" to 
> compile c code test.c?
> 
> I will appreciate if you can help me resolve this problem.
> 
> yours,
> 
> Maggie Wang
> 

Hi Maggie,

(I'm not sure what OS you are using. You never specified. The following 
works in Windows.)

You should use

gcc test.c -o test.exe -L/path/to/Rmath -lRmath

e.g. if Rmath.dll is in C:/R/R-1.9.0/src/nmath/standalone

gcc test.c -o test.exe -LC:/R/R-1.9.0/src/nmath/standalone -lRmath

Then make sure test Rmath.dll is in your path so that test.exe knows 
where the dll is.

--sundar



From bacolli at uark.edu  Wed Apr 14 17:37:19 2004
From: bacolli at uark.edu (Bret Collier)
Date: Wed, 14 Apr 2004 10:37:19 -0500
Subject: [R] Weird Error
Message-ID: <5.2.1.1.0.20040414103150.0286c5a8@mail.uark.edu>

R-Users,

I hope this is not a uniformed question, but I am a little lost.

I ran into a problem this morning and I was wondering if anyone had
seen it before.  I was trying to summarize each column of a data set
(150,000 rows, ~50mb, so it was a relatively big file) imported from a text
file using the below code;

data.summary <- read.csv("c:/summary.txt", sep="")
data.summary <- as.matrix(data.summary)
my.summary <- function(x){
    return(c(min=min(x),max=max(x), mean=mean(x)))}
apply(data.summary, 2, my.summary)


And I got this weird error that I can not find out anything about?


"Process R unknown signal at Wed Apr 14 08:17:22 2004"


Have you seen anything like this before?  Do you think it is the size of
the dataset that is causing the problem, since the same code works for
25000 rows (~17mb) and gives the correct results (I cross-checked in SAS 
and EXCEL).

I was using R 1.8.0 in Xemacs.


TIA,

Bret Collier



From johannesson1 at llnl.gov  Wed Apr 14 20:14:26 2004
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Wed, 14 Apr 2004 11:14:26 -0700
Subject: [R] attaching data.frame/list within a function
Message-ID: <5.0.0.25.2.20040414105500.00abc588@poptop.llnl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040414/6f27da09/attachment.pl

From jfox at mcmaster.ca  Wed Apr 14 20:31:14 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 14 Apr 2004 14:31:14 -0400
Subject: [R] attaching data.frame/list within a function
In-Reply-To: <5.0.0.25.2.20040414105500.00abc588@poptop.llnl.gov>
Message-ID: <20040414183113.NHRV15811.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Gardar,

You could use with(); to modify your example slightly,

> a <- "a"
> b <- 5
> foo <- function(x=0, input=list(a=10)) {
+             with(input, {
+                 print(c(a, b, x))
+                 })
+             }
> foo()
[1] 10  5  0
> 

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gardar 
> Johannesson
> Sent: Wednesday, April 14, 2004 1:14 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] attaching data.frame/list within a function
> 
> I'm trying to find a good way of attaching a list within a 
> function such that the attached variables (the list's 
> members) precede the global environment (.GlobalEnv) in the 
> search list.  Here is a non-working example using attach(), 
> which hopefully explains better what I'm trying to do:
> 
>  > foo <- function(x=0, input=list(a=10)) {
> +   attach(input)
> +   on.exit(detach(input))
> +   print(search())
> +   print(a)
> + }
>  >
>  > a
> Error: Object "a" not found
>  > foo()  ## this prints out a = 10
> [1] ".GlobalEnv"       "input"            "package:methods"  
> "package:stats"
> [5] "package:graphics" "package:utils"    "Autoloads"        
> "package:base"
> [1] 10
>  >
>  > a <- 0
>  > foo()  ## this prints out a = 0, not a = 10
> [1] ".GlobalEnv"       "input"            "package:methods"  
> "package:stats"
> [5] "package:graphics" "package:utils"    "Autoloads"        
> "package:base"
> [1] 0
>  >
> 
> 
> Thanks,
> 
>                  Gardar
>



From andy_liaw at merck.com  Wed Apr 14 20:50:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Apr 2004 14:50:19 -0400
Subject: [R] attaching data.frame/list within a function
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BD9@usrymx25.merck.com>

What you are asking for doesn't even work at the R prompt, let alone inside
a function.  The _really_ ugly kludge I manage to come up with is to
manually assign the components of the list (or variables in the data frame)
into the function's environment:

f <- function(mylist=list(a=1:5)) {
    nm <- names(mylist)
    for (i in seq(along=nm)) {
        assign(nm[i], mget(nm[i], envir=NULL, ifnotfound=mylist[i]))
    }
    sapply(names(mylist), function(x) print(get(x)))
    ls()
}

Here are a couple of tests:

> f()
$a
[1] 1 2 3 4 5

[1] "a"      "i"      "mylist" "nm"    
> f(list(b=5:1, z="ha!"))
$b
[1] 5 4 3 2 1

$z
[1] "ha!"

[1] "b"      "i"      "mylist" "nm"     "z"     

Andy

> From: Gardar Johannesson
> 
> I'm trying to find a good way of attaching a list within a 
> function such 
> that the attached variables (the list's members) precede the global 
> environment (.GlobalEnv) in the search list.  Here is a 
> non-working example 
> using attach(), which hopefully explains better what I'm trying to do:
> 
>  > foo <- function(x=0, input=list(a=10)) {
> +   attach(input)
> +   on.exit(detach(input))
> +   print(search())
> +   print(a)
> + }
>  >
>  > a
> Error: Object "a" not found
>  > foo()  ## this prints out a = 10
> [1] ".GlobalEnv"       "input"            "package:methods"  
> "package:stats"
> [5] "package:graphics" "package:utils"    "Autoloads"        
> "package:base"
> [1] 10
>  >
>  > a <- 0
>  > foo()  ## this prints out a = 0, not a = 10
> [1] ".GlobalEnv"       "input"            "package:methods"  
> "package:stats"
> [5] "package:graphics" "package:utils"    "Autoloads"        
> "package:base"
> [1] 0
>  >
> 
> 
> Thanks,
> 
>                  Gardar
> 
> 
> ___________________________________________________________________
> Gardar Johannesson
> Lawrence Livermore National Laboratory
> 7000 East Avenue, L-229
> Livermore, CA 94550
> 
> johannesson1 at llnl.gov
> Tel: (925) 422-3901,  Fax: (925) 422-4141
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From johannesson1 at llnl.gov  Wed Apr 14 21:34:27 2004
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Wed, 14 Apr 2004 12:34:27 -0700
Subject: [R] attaching data.frame/list within a function
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7BD9@usrymx25.merck.co
 m>
Message-ID: <5.0.0.25.2.20040414115717.02e92668@poptop.llnl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040414/ff84b31f/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Apr 14 21:45:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Apr 2004 21:45:48 +0200
Subject: [R] Weird Error
In-Reply-To: <5.2.1.1.0.20040414103150.0286c5a8@mail.uark.edu>
References: <5.2.1.1.0.20040414103150.0286c5a8@mail.uark.edu>
Message-ID: <x2llkyqr6b.fsf@biostat.ku.dk>

Bret Collier <bacolli at uark.edu> writes:

> R-Users,
> 
> I hope this is not a uniformed question, but I am a little lost.

Don't worry, they all look alike... ;-)
 
> I ran into a problem this morning and I was wondering if anyone had
> seen it before.  I was trying to summarize each column of a data set
> (150,000 rows, ~50mb, so it was a relatively big file) imported from a text
> file using the below code;
> 
> data.summary <- read.csv("c:/summary.txt", sep="")
> data.summary <- as.matrix(data.summary)
> my.summary <- function(x){
>     return(c(min=min(x),max=max(x), mean=mean(x)))}
> apply(data.summary, 2, my.summary)
> 
> 
> And I got this weird error that I can not find out anything about?
> 
> 
> "Process R unknown signal at Wed Apr 14 08:17:22 2004"
> 
> 
> Have you seen anything like this before?  Do you think it is the size of
> the dataset that is causing the problem, since the same code works for
> 25000 rows (~17mb) and gives the correct results (I cross-checked in
> SAS and EXCEL).

Running out of memory and having the OS intervening could give that
kind of message. Or bad RAM. In the first case look up how to set the
memory limits, in the other, change machines to verify.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MZodet at ahrq.gov  Wed Apr 14 21:50:51 2004
From: MZodet at ahrq.gov (Zodet, Marc)
Date: Wed, 14 Apr 2004 15:50:51 -0400
Subject: [R] Variable Descriptors
Message-ID: <6BCD3F430455B1418750004BCD27925918409C@exchange2.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040414/c49d8029/attachment.pl

From feh3k at spamcop.net  Wed Apr 14 22:16:55 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 14 Apr 2004 15:16:55 -0500
Subject: [R] Variable Descriptors
In-Reply-To: <6BCD3F430455B1418750004BCD27925918409C@exchange2.ahrq.gov>
References: <6BCD3F430455B1418750004BCD27925918409C@exchange2.ahrq.gov>
Message-ID: <20040414151655.36d28575.feh3k@spamcop.net>

On Wed, 14 Apr 2004 15:50:51 -0400
"Zodet, Marc" <MZodet at ahrq.gov> wrote:

> Is there a way to associate text descriptions with variables in a
> data.frame?
> 
>  
> 
> For example...
> 
>  
> 
> Let's say that in my data.frame I have a variable named var1.  var1
> represents the responses to the question "When was the last time you saw
> your physician?"  When I tabulate the variable var1 I'd like the output
> to be a bit more descriptive and contain the more descriptive question
> (i.e.,"When was...) rather than just var1.
> 
>  
> 
> What I'm looking for is something analogous to the label statement in
> SAS.

The Hmisc package provides this, and many of its graphics and table-making
functions use the labels.

Frank

> 
>  
> 
> Thanks for your help.
> 
>  
> 
> Marc
> 
>  
> 
> Marc W. Zodet, MS
> 
> Health Statistician
> 
>  
> 
> Agency for Healthcare Research and Quality
> 
> Center for Financing, Access, and Cost Trends
> 
> Division of Statistical Research and Methods
> 
> 540 Gaither Road, Room 5058
> 
> Rockville, Maryland 20850
> 
>  
> 
> Phone:  301-427-1563
> 
> FAX:    301-427-1276
> 
> E-mail:  <mailto:mzodet at ahrq.gov> mzodet at ahrq.gov
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From deepayan at stat.wisc.edu  Wed Apr 14 22:34:18 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 14 Apr 2004 15:34:18 -0500
Subject: [R] ltext, plotmath, and substitute
In-Reply-To: <407D6333.8060801@u.washington.edu>
References: <407D6333.8060801@u.washington.edu>
Message-ID: <200404141534.18116.deepayan@stat.wisc.edu>

On Wednesday 14 April 2004 11:13 am, Dave Atkins wrote:
> I am interested to use plotmath functions within a panel function but am
> having some problems getting the code right.  Within each panel I am
> plotting the data, fitting a regression line, and would like to print the
> regression equation. Here is a trivial example of what I'd like to do:
>
> # generate simple data
> tmp.df <- data.frame(id = rep(1:4, each=4),
> 			time = rep(1:4, 4),
> 			das = rnorm(16))
> # plot regression lines and print equations
> xyplot(das ~ time | as.factor(id), data = tmp.df, strip = T,
> 	   panel = function(x,y,...){
> 		 panel.coef <- round(coef(lm(y~x)), 2)
> 		 panel.grid()
> 		 panel.xyplot(x,y)
> 		 panel.lmline(x,y)
> 		 ltext(1.2, 1, pos = 4,
> 		   paste("Y = ",panel.coef[1]," + ",panel.coef[2],"*time",
> 			sep = ""), cex = 1.2)
> 		 },
> 	   layout = c(4,1,1), aspect = 2.5)
>
> The above works fine.  However, I'd like to make the equations more
> attractive by adding a hat over Y and an epsilon at the end, and this is
> where I have run into problems.  I experimented with expression() and
> paste() for a while without luck, and now believe I need to use
> substitute() (given some of the examples on the plotmath() help-page). 
> But, I can't get the code quite right.

Following example(plotmath), the following seems to work:


panel = function(x, y, ...) {
    panel.coef <- round(coef(lm(y~x)), 2)
    panel.grid()
    panel.xyplot(x,y)
    panel.lmline(x,y)
    ltext(1, 1, pos = 4, 
          as.expression(bquote(hat(Y) == .(panel.coef[1]) + 
          .(panel.coef[2]) * time + epsilon)))
}

The as.expression() is not necessary in text, but is in ltext.



Incidentally, ?bquote says 

Value:

     An expression

but, 

> is.expression(bquote(hat(y)))
[1] FALSE
> is.call(bquote(hat(y)))
[1] TRUE

Something should probably be changed.


Deepayan



From jasont at indigoindustrial.co.nz  Wed Apr 14 22:57:29 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 15 Apr 2004 08:57:29 +1200
Subject: [R] prcomp - error code 18
In-Reply-To: <Pine.LNX.4.44.0404141645510.3935-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0404141645510.3935-100000@env-pc-phd13>
Message-ID: <407DA5B9.2020401@indigoindustrial.co.nz>

Laura Quinn wrote:

> I am attempting to perform a pca on a data frame of dimension 5000x19, but
> when I execute
> 
> pcapres<-prcomp(pres,center=TRUE)
> 
> the following error message is returned:
> 
> Error in La.svd(x, nu, nv, method) : error code 18 from Lapack routine
> dgesdd
> 
> Where am I going wrong? I am running R-1.8.0 on Debian.

Have you tried using the scale=TRUE argument?

You have to check the Lapack documentation or source to find the error 
code's meaning.  This is the INFO argument in Lapack routines.  Positive 
error codes in Lapack indicate some computational failure (singular 
matrix, iterative routine fails to converge, ...).

 From http://www.netlib.org/lapack/double/dgesdd.f

* INFO    (output) INTEGER
*          = 0:  successful exit.
*          < 0:  if INFO = -i, the i-th argument had an illegal value.
*          > 0:  DBDSDC did not converge, updating process failed.

 From the DBDSDC.f source:
*  INFO    (output) INTEGER
*          = 0:  successful exit.
*          < 0:  if INFO = -i, the i-th argument had an illegal value.
*          > 0:  The algorithm failed to compute an singular value.
*                The update process of divide and conquer failed.


So it looks like you've found a problem that's a bit numerically odd. 
Scaling might help this -- it has for me, and it's more (statistically) 
sensible anyway.

Cheers

Jason



From itayf at fhcrc.org  Wed Apr 14 23:13:56 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Wed, 14 Apr 2004 14:13:56 -0700 (PDT)
Subject: [R] dataframe: visualization as tiles(?)
In-Reply-To: <32431.203.9.176.60.1081925005.squirrel@webmail.maxnet.co.nz>
Message-ID: <Pine.LNX.4.44.0404141408320.14582-100000@cezanne.fhcrc.org>


Thanks, but no. In the method that I remember the plot was an 
assortment of oblong(?) tiles, optionally color coded, and 
filling a square region.
According to my recollection it was possilbe to add more 
variables, in which case each tile is broken down further into 
sub-tiles.

	Itay


 On Wed, 14 Apr 2004, Jason Turner wrote:

> >
> > Dear R users,
> >
> > I remember seeing somewhere a method of visualizing a set of
> > observations on two variables x and y in the following way
> 
> Is this what you want?
> 
> > ## fake data
> > zz <- data.frame(x=sample(0:1,20,rep=T),y=sample((-1:1),20,rep=T))
> > zz
> 
> > ## tabulate it
> > zz.tab <- data.frame(table(zz))
> > zz.tab
> > library(lattice)
> > barchart(y ~ Freq | x, data=zz.tab)
> 
> Cheers
> 
> Jason
>



From k.hansen at biostat.ku.dk  Wed Apr 14 23:19:59 2004
From: k.hansen at biostat.ku.dk (Kasper Daniel Hansen)
Date: Wed, 14 Apr 2004 23:19:59 +0200
Subject: [R] Re: [BioC] Makefile for installing all available packages
In-Reply-To: <20040414162823.GA4542@psych>
References: <D7A3CFD7825BD6119B880002A58F06C20680B29E@groexmb02.pfizer.com>
	<20040414150742.GB25710@carter.kubism.ku.dk>
	<9slq70hneav7toc5b87tl8492sbqdp4cn4@4ax.com>
	<20040414162823.GA4542@psych>
Message-ID: <20040414211959.GA27037@carter.kubism.ku.dk>

On Wed, Apr 14, 2004 at 12:28:23PM -0400, Jonathan Baron wrote:
> On 04/14/04 11:42, Duncan Murdoch wrote:
> >On Wed, 14 Apr 2004 17:07:42 +0200, Kasper Daniel Hansen
> ><k.hansen at biostat.ku.dk> wrote :
> >>You do not find it a bit overkill to install all of CRAN per default?
> >
> >I can see two situations where this would be desirable:
> >
> >On a laptop or other machine that is often not connected to the net:
> >install everything just in case you might want something and can't go
> >to CRAN for it.
> >
> >In a public lab where users have limited ability to install packages,
> >it's good to have them all there.
> 
> A third reason:  You run a search site where you want people to
> be able to search all the help files, such as the one at the end
> of my sig.

Well, my (rethorical) question was not really to the usefulness of the 
script, but more to its default behaviour.
-- 
Kasper Daniel Hansen, Research Assistant
Department of Biostatistics, University of Copenhagen



From gferraz at duke.edu  Wed Apr 14 23:35:12 2004
From: gferraz at duke.edu (=?ISO-8859-1?Q?Gon=E7alo_Ferraz?=)
Date: Wed, 14 Apr 2004 17:35:12 -0400
Subject: [R] Uninstalling RAqua (Mac OSX)
Message-ID: <9C716EAE-8E5B-11D8-96DE-000393B64146@duke.edu>

I wrote some R code that runs just fine on a friend's RAqua (MacOSX) 
but will never run on mine. This made me think there could have been a 
problem with my installation or version, so I went to
http://cran.r-project.org/bin/macosx/
and downloaded the RAqua.dmg file (version 1.8.1)
I upgraded RAqua by installing all four packages - but the problem 
persists. Would it help to uninstall completely before installing the 
new version?
How does one uninstall RAqua completely? I see there are files in my 
applications folder and in usr/local/bin... Should it be enough to 
remove those files? I tried that too, and again, the problem persists?
Any help will be greatly appreciated.
Best,
Goncalo



From lforzani at stat.umn.edu  Wed Apr 14 23:38:52 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Wed, 14 Apr 2004 16:38:52 -0500 (CDT)
Subject: [R] poisson repeated meassures
Message-ID: <Pine.LNX.4.44.0404141637340.2426-100000@muskrat.stat.umn.edu>


Dear R users: I try to use GEE or glmm or glmmML for poisson data
but since I have DOUBLE repeated meassure I could not use those.

Is there something in R for double repeated meassure for nonnormal data?

thansk. Liliana Forzani



From xt_wang at cs.concordia.ca  Thu Apr 15 00:03:10 2004
From: xt_wang at cs.concordia.ca (xt_wang@cs.concordia.ca)
Date: Wed, 14 Apr 2004 18:03:10 -0400
Subject: [R] again question about nmath/standalone
Message-ID: <1081980190.407db51edc8e9@mailhost.cs.concordia.ca>

Hello,

I forgot to tell you that I am using Linux OS. And I can?t find 
directory "src/nmath/standalone". I will send you the test code I am using and 
the whole operation process. 

[credsim at confsys ~/src]$ gcc test1.c -o test1 -lRmath
test1.c: In function `main':
test1.c:18: warning: assignment makes pointer from integer without a cast
test1.c:19: warning: assignment makes pointer from integer without a cast
test1.c:41: warning: assignment makes pointer from integer without a cast
test1.c: At top level:
test1.c:55: warning: type mismatch with previous implicit declaration
test1.c:18: warning: previous implicit declaration of `Allocate_Memory_2D'
test1.c:55: warning: `Allocate_Memory_2D' was previously implicitly declared to 
return `int'
/usr/bin/ld: cannot find -lRmath
collect2: ld returned 1 exit status

"test1.c" code is as follow:

#define MATHLIB_STANDALONE
#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include </usr/local/lib/R/include/Rmath.h>

 main()
{

  double **x, **y, **x1, **y1, valin;
  int i,j,I,J;
  

  I=3;
  J=3;

  x=Allocate_Memory_2D( I, J, x1);
  y=Allocate_Memory_2D( I, J, y1);


  FILE *in_file; 


    /* input x value from file data_2Dx.txt */ 
    in_file=fopen("data_2Dx.txt","r");
	if (in_file==NULL)
	{/*Test for error*/
		fprintf(stderr,"Error:Unable to input file 
from 'data_2Dx.txt'\n");
		exit(8);
	}
	for( i=0;i<I; i++)
	  for (j=0;j<J;j++)
	  { fscanf(in_file, "%lf\n", &valin, stdin);/* read a single double 
value in */
	       x[i][j]=valin;
		   valin=0.0;
	  }
	fclose(in_file);


   y=solve(x);

   for (i=0;i<I;i++)
     for (j=0;j<J;j++)
     {
       printf ("y[%d][%d]=%lf\n", i, j, y[i][j]);
     }



}


double **Allocate_Memory_2D( int I, int J, double **W)
{ 
	int i;

	W=(double **)malloc(I*sizeof (double *));
    if(!W)
      printf("It is out of memory. Allocation failed.");
	for (i=0;i<I;i++)
	{
        W[i]=(double *)malloc(J*sizeof(double));
        if(!W[i])
          printf("It is out of memory. Allocation failed.");
	}
	return (W);
}

Looking forward to your early reply! 

Thanks!

Maggie Wang



From bates at stat.wisc.edu  Thu Apr 15 00:12:57 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 14 Apr 2004 17:12:57 -0500
Subject: [R] Weird Error
In-Reply-To: <5.2.1.1.0.20040414103150.0286c5a8@mail.uark.edu>
References: <5.2.1.1.0.20040414103150.0286c5a8@mail.uark.edu>
Message-ID: <6rr7uqmcnq.fsf@bates4.stat.wisc.edu>

Bret Collier <bacolli at uark.edu> writes:

> I hope this is not a uniformed question, but I am a little lost.
> 
> I ran into a problem this morning and I was wondering if anyone had
> seen it before.  I was trying to summarize each column of a data set
> (150,000 rows, ~50mb, so it was a relatively big file) imported from a text
> file using the below code;
> 
> data.summary <- read.csv("c:/summary.txt", sep="")
> data.summary <- as.matrix(data.summary)
> my.summary <- function(x){
>     return(c(min=min(x),max=max(x), mean=mean(x)))}
> apply(data.summary, 2, my.summary)

Peter responded about the error.   You may be able to circumvent the
error by using

apply(data.summary, 2, range)

to get the minimum and maximum and 

colMeans(data.summary)

to get the means.  Those are internal functions and will generate less
overhead (and fewer copies) than calls to your own function.



From jasont at indigoindustrial.co.nz  Thu Apr 15 00:21:19 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 15 Apr 2004 10:21:19 +1200
Subject: [R] poisson repeated meassures
In-Reply-To: <Pine.LNX.4.44.0404141637340.2426-100000@muskrat.stat.umn.edu>
References: <Pine.LNX.4.44.0404141637340.2426-100000@muskrat.stat.umn.edu>
Message-ID: <407DB95F.6030401@indigoindustrial.co.nz>

Liliana Forzani wrote:
> Dear R users: I try to use GEE or glmm or glmmML for poisson data
> but since I have DOUBLE repeated meassure I could not use those.
> 
> Is there something in R for double repeated meassure for nonnormal data?

Jim Lindsey has a few packages for this.

http://alpha.luc.ac.be/~jlindsey/rcode.html

Cheers

Jason



From edd at debian.org  Thu Apr 15 00:35:10 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 14 Apr 2004 17:35:10 -0500
Subject: [R] again question about nmath/standalone
In-Reply-To: <1081980190.407db51edc8e9@mailhost.cs.concordia.ca>
References: <1081980190.407db51edc8e9@mailhost.cs.concordia.ca>
Message-ID: <20040414223510.GA21498@sonny.eddelbuettel.com>


Maggie,

Are you sure you actually _have_ a library libRmath available to your
linker?  

Below is a full log of how this works on any given Debian system for which
we provide the library in a package r-mathlib. Once that is installed, you
should be set.

Hth, Dirk

edd at basebud:/usr/share/doc/r-mathlib/examples> cat test.c 
/*
 *  Mathlib : A C Library of Special Functions
 *  Copyright (C) 2000  The R Development Core Team
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307  USA
 *
 */
  
#define MATHLIB_STANDALONE 1
#include <Rmath.h>
  
int
main()
{
/* something to force the library to be included */
   qnorm(0.7, 0.0, 1.0, 0, 0); 
   return 0;
}
edd at basebud:/usr/share/doc/r-mathlib/examples> gcc -Wall -o /tmp/nmath_test test.c -lRmath -lm
edd at basebud:/usr/share/doc/r-mathlib/examples> ldd /tmp/nmath_test 
        libRmath.so.1 => /usr/lib/libRmath.so.1 (0x40028000)
        libm.so.6 => /lib/libm.so.6 (0x40047000)
        libc.so.6 => /lib/libc.so.6 (0x40069000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)
edd at basebud:/usr/share/doc/r-mathlib/examples> 



-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From andy_liaw at merck.com  Thu Apr 15 01:47:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Apr 2004 19:47:19 -0400
Subject: [R] conditional import in NAMESPACE
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BDE@usrymx25.merck.com>

Dear R-help,

Can some one tell me if it's possible to have conditional importFrom() in
the package NAMESPACE file?  Basically I'd like to know if it's possible to
make the NAMESPACE file compatible with R 1.9.0 and those 1.8.1 and earlier.
The problem is that I want to import cmdscale(), which is in `mva' prior to
1.9.0 but in `stats' post 1.9.0.

Any pointer much appreciated!

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com        732-594-0820



From kjetil at entelnet.bo  Thu Apr 15 01:58:48 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 14 Apr 2004 19:58:48 -0400
Subject: [R] trend turning points
In-Reply-To: <20040414192423.1c4b0e1d.Achim.Zeileis@wu-wien.ac.at>
References: <407D6F5C.3060403@uv.es>
Message-ID: <407D97F8.24329.16D5ED9@localhost>

On 14 Apr 2004 at 19:24, Achim Zeileis wrote:

> On Wed, 14 Apr 2004 19:05:32 +0200 Joerg Schaber wrote:
> 
> > Hi,
> > 
> > does anybody know of a nice test to detect trend turning points in
> > time series? Possibly with reference?
> 
> You can look at the function breakpoints() in the package strucchange

I have found this very usefull. One Q: from the documentation
(vignette) it is not clear if the distribution theory implemented in
strucchange takes account of autocorrelation structure in a 
time series. For instance, to look for trend changes and at the same 
time changes in the form of seasonality I uses 
breakpoints(my.ts ~ 1:n + as.factor(cycle(my.ts)) )

Is this OK?

Kjetil Halvorsen

> and the function segmented() in the package segmented which do
> segmentation of (generalized) linear regression models. The former
> tries to fit fully segmented regression models, the latter broken line
> trends. References are given on the respective help pages.
> 
> A suitable test for a change in trend in linear regression models is
> the OLS-based CUSUM test with a Cramer-von Mises functional of Kraemer
> & Ploberger (1996, JoE) which is available via efp() in strucchange
> and associated methods.
> 
> hth,
> Z
> 
> > Thanks,
> > 
> > joerg
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Thu Apr 15 03:24:25 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 14 Apr 2004 20:24:25 -0500
Subject: [R] conditional import in NAMESPACE
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7BDE@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7BDE@usrymx25.merck.com>
Message-ID: <6rptaaav92.fsf@bates4.stat.wisc.edu>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> Can some one tell me if it's possible to have conditional importFrom() in
> the package NAMESPACE file?  Basically I'd like to know if it's possible to
> make the NAMESPACE file compatible with R 1.9.0 and those 1.8.1 and earlier.
> The problem is that I want to import cmdscale(), which is in `mva' prior to
> 1.9.0 but in `stats' post 1.9.0.

Well, we've got good news and bad news.  The good news is that there
are facilities for conditionals in the NAMESPACE file.  The bad news
is that they were introduced in 1.9.0 so you can't use them for the
purpose you have in mind.

The way many of us have approached this is to freeze a version of the
package that works for R-1.8.1 and add  R(<= 1.8.1) to Depends: in the
DESCRIPTION file.

Then bump the version number, switch to importFrom(stats, ...) in the
NAMESPACE and add R(>= 1.9.0) to the Depends: line.



From andy_liaw at merck.com  Thu Apr 15 03:39:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 14 Apr 2004 21:39:07 -0400
Subject: [R] conditional import in NAMESPACE
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BE8@usrymx25.merck.com>

> From: Douglas Bates [mailto:bates at bates4.stat.wisc.edu] On 
> 
> "Liaw, Andy" <andy_liaw at merck.com> writes:
> 
> > Can some one tell me if it's possible to have conditional 
> importFrom() in
> > the package NAMESPACE file?  Basically I'd like to know if 
> it's possible to
> > make the NAMESPACE file compatible with R 1.9.0 and those 
> 1.8.1 and earlier.
> > The problem is that I want to import cmdscale(), which is 
> in `mva' prior to
> > 1.9.0 but in `stats' post 1.9.0.
> 
> Well, we've got good news and bad news.  The good news is that there
> are facilities for conditionals in the NAMESPACE file.  The bad news
> is that they were introduced in 1.9.0 so you can't use them for the
> purpose you have in mind.
> 
> The way many of us have approached this is to freeze a version of the
> package that works for R-1.8.1 and add  R(<= 1.8.1) to Depends: in the
> DESCRIPTION file.
> 
> Then bump the version number, switch to importFrom(stats, ...) in the
> NAMESPACE and add R(>= 1.9.0) to the Depends: line.

Thanks very much, Doug.  I suppose one ugly way of getting around this is
not to import cmdscale in NAMESPACE, but to check for R version and do the
appropriate require() inside the function that needs cmdscale().  Or is that
really too ugly?

Best,
Andy



From christian_mora at vtr.net  Thu Apr 15 05:33:42 2004
From: christian_mora at vtr.net (Christian Mora)
Date: Wed, 14 Apr 2004 23:33:42 -0400
Subject: [R] Non-Linear Regression Problem
In-Reply-To: <407D664B.1000603@pdf.com>
Message-ID: <000001c4229a$737ee320$b43d68c8@CPQ28661778111>

Working on the same idea, Ive generated a data grid with 4 vars, two of
them with its own sequence and two with fixed values. As Spencer pointed
out one option is to get the values from a simple loop. My question is:
How can jump from one set of starting values to the next (on the data
grid) in case they cause an invalid value in the model Im evaluating? In
my example, Ive 12 possible combinations of the 4 vars and in the 6th
combination I get an error and the loop is terminated so what Im looking
for is to skip this problem and continue from the 7th to 12th
combination of starting points

Ill appreciate any comment

Code:

data<-expand.grid(alpha=100,delta=4,beta=seq(1,2,by=0.5),gamma=seq(.1,.4
,by=.1))
for(i in 1:12){
fit<-nls(y~delta+(alpha-delta)/(1+exp(beta*log(rate/gamma))),data=base,s
tart=c(alpha=data$alpha[i],delta=data$delta[i],beta=data$beta[i],gamma=d
ata$gamma[i]),trace=T)
}

Thanks
CMora



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
Sent: Wednesday, April 14, 2004 12:27 PM
To: WilDscOp
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Non-Linear Regression Problem


1.  For the equation you mentioned, have you considered the following: 

DF <- data.frame(t.=c(1, 4, 16), Y=c(.8, .45, .04))
# I do NOT use "t" as a name, as it
# may conflict with the matrix transpose function.
fit0 <- lm(log(Y)~t.-1, DF)
fit0
Call:
lm(formula = log(Y) ~ t. - 1, data = DF)

Coefficients:
     t. 
-0.2012 
################

      If this is the problem you really wanted to solve AND you honestly

need NONLINEAR least squares, I would expect that (-0.2) should provide 
a reasonable starting value for nls: 

 > fit1 <- nls(Y~exp(-THETA*t.), data=DF, start=c(THETA=-0.2))
 > fit1
Nonlinear regression model
  model:  Y ~ exp(-THETA * t.)
   data:  DF
    THETA
0.2034489
 residual sum-of-squares:  0.0003018337
 >

      2.  Alternatively, you could compute the sum of squares for all 
values of THETA = seq(0, .01, 100) in a loop, then find the minimum by 
eye. 

      3.  If this is just a toy example, and your real problem has 
several parameters, "expand.grid" will produce a grid, and you can 
compute the value of your function and the sum of squares of residuals 
at every point in the grid in a single loop, etc. 

      hope this helps.  spencer graves

WilDscOp wrote:

> Dear all,
>
>     I was wondering if there is any way i could do a "Grid Search" on 
> a parameter space using R (as SAS 6.12 and higher can do it) to start 
> the Newton-Gauss Linearization least squares method when i have NO 
> prior information about the parameter.
> W. N. Venables and B. D. Ripley (2002) "Modern Applied Statistics with

> S", 4 th ed., page 216-7  has a topic "Self-starting non-linear 
> regressions" using negexp.SSival - but i can not solve my hypothetical

> problem using that - my problem is :
>
> Y = EXP(-(THETA * t)) with data below for estimating THETA:
>
> t        Y
> 1    0.80
> 4     0.45
> 16     0.04
>
> Whatever i could do, is in 
> http://www.angelfire.com/ab5/get5/nonlinear.PDF
>
> Any response / help / comment / suggestion / idea / web-link / replies

> will be greatly appreciated.
>
> Thanks in advance for your time.
>
> _______________________
>
> Mohammad Ehsanul Karim <wildscop at yahoo.com>
> Institute of Statistical Research and Training
> University of Dhaka, Dhaka- 1000, Bangladesh
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ajayshah at mayin.org  Tue Apr 13 18:08:15 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 13 Apr 2004 21:38:15 +0530
Subject: [R] In-sample / Out-of-sample using R
Message-ID: <20040413160815.GA21761@igidr.ac.in>

I'm trying to learn how to use R to:
  * Make a random partition of a data frame between in-sample and
         out-of-sample
  * Estimate a model (e.g. lm()) for the in-sample
  * Make predictions for all observations
  * Compare the in-sample error sigma against the out-of-sample error
    sigma.

I came up with the following code. I think it's okay, but I can't help
feeling this is still clunky. Could all ye R wizards please comment on
this, and tell me how I can do it better?

   ---------------------------------------------------------------------------
   # Simulate some data for a linear regression (100 points)
   x = runif(100); y = 2 + 3*x + rnorm(100)
   D = data.frame(x, y)

   # Choose a random subset of 25 points which will be "in sample"
   d = sort(sample(100, 25))               # Sorting just makes d more readable
   cat("Subset of insample points --\n"); print(d)

   # Estimate a linear regression using all points
   m1 = lm(y ~ x, D)
   # Estimate a linear regression using only the subset
   m2 = lm(y ~ x, D, subset=d)

   # Get to predictions --
   yhat1 = predict.lm(m1, D); yhat2 = predict.lm(m2, D)

   # And standard deviations of errors -- 
   full.s = sd(y - yhat1)
   insample.s = sd(y[d] - yhat2[d])
   outsample.s = sd(y[-d] - yhat2[-d])

   cat("Sigmas of prediction errors --\n")
   cat("  All points used in estimation, in sample     : ", full.s, "\n")
   cat("  25 points used in estimation, in sample      : ", insample.s, "\n")
   cat("  25 points used in estimation, out of sample  : ", outsample.s, "\n")
   ---------------------------------------------------------------------------

Here's what I get when I run it:

$ R --slave < insampleoutsample.R 
Subset of insample points --
 [1]  4  6  7 13 20 21 24 25 26 27 29 33 34 36 39 45 47 48 59 60 88 89 91 96 98
Sigmas of prediction errors --
  All points used in estimation, in sample     :  0.9405517 
  25 points used in estimation, in sample      :  1.000709 
  25 points used in estimation, out of sample  :  0.9586921 

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Wed Apr 14 04:47:09 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 14 Apr 2004 08:17:09 +0530
Subject: [R] A bug report?
Message-ID: <20040414024709.GA23509@igidr.ac.in>

Folks,

I have a strange situation, which I may have isolated as a bug
report. Or, it could just be that there's something about R that I
don't know. :-) I have attached the data file and the program file but
don't know whether these attachments will make it into the list. Here
is my bugreport.R program --

---------------------------------------------------------------------------
buoyancy <- function(year, taxbase, tax, description, plotname) {
  cat("Simple full OLS regression with all data:\n")
  logtax = log(tax)
  logtaxbase = log(taxbase)
  m = lm(logtax ~ logtaxbase)
  summary.lm(m)
  details = summary.lm(m)
}

A <- read.table(file="amodi-data.csv", sep=",", col.names=c("year",
                "gdp.ag", "gdp.mining", "gdp.manuf", "gdp.elecgas",
                "gdp.construction", "gdp.industry", "gdp.services",
                "gdp.fc", "indirect.taxes", "subsidies", "j1",
                "gdp.mp", "gdp.mp.93", "gdp.deflator", "gdp.fc.93",
                "gdp.ag.93", "gdp.industry.93", "gdp.services.93",
                "tax.income", "tax.corporation", "tax.direct.others",
                "tax.direct", "tax.customs", "tax.excise",
                "tax.indirect.others", "tax.indirect", "tax.total"))
A = subset(A, !is.na(A$tax.total))
buoyancy(A$year, A$gdp.mp, A$tax.income, "Personal income tax and GDPmp", "p1")
---------------------------------------------------------------------------

This program does not work. The summary.lm(m) statement seems to have
no effect. When I run it, I get:

$ R --slave < bugreport.R 
Simple full OLS regression with all data:

where it is asif the summary.lm(m) statement never occurred. If I put
in a statement print(m) it works, but the summary.lm(m) does not work.

Now here's what's weird: Suppose I remove the statement that comes
AFTER this summary.lm(m) statement. That is, I don't say
    details = summary.lm(m)
as the last line of the function. In this case, the program works fine!

I'm most confused. I can't see how putting in an assignment statement
AFTER a function call can contaminate a PREVIOUS statement. I would be
most happy if you could guide me...

I am running on a nicely-working notebook which runs Debian linux
kernel 2.4.17, and have R 1.8.1 (2003-11-21). I use the `testing'
branch of Debian.

     -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
-------------- next part --------------
buoyancy <- function(year, taxbase, tax, description, plotname) {
  cat("Simple full OLS regression with all data:\n")
  logtax = log(tax)
  logtaxbase = log(taxbase)
  m = lm(logtax ~ logtaxbase)
  summary.lm(m)
  details = summary.lm(m)
}

A <- read.table(file="amodi-data.csv", sep=",", col.names=c("year",
                "gdp.ag", "gdp.mining", "gdp.manuf", "gdp.elecgas",
                "gdp.construction", "gdp.industry", "gdp.services",
                "gdp.fc", "indirect.taxes", "subsidies", "j1",
                "gdp.mp", "gdp.mp.93", "gdp.deflator", "gdp.fc.93",
                "gdp.ag.93", "gdp.industry.93", "gdp.services.93",
                "tax.income", "tax.corporation", "tax.direct.others",
                "tax.direct", "tax.customs", "tax.excise",
                "tax.indirect.others", "tax.indirect", "tax.total"))
A = subset(A, !is.na(A$tax.total))

buoyancy(A$year, A$gdp.mp, A$tax.income, "Personal income tax and GDPmp",
         "p1")

From wettenhall at wehi.edu.au  Thu Apr 15 08:14:28 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Thu, 15 Apr 2004 16:14:28 +1000 (EST)
Subject: [R] Use R function in C code
Message-ID: <Pine.LNX.4.58.0404151550190.7357@unix28.alpha.wehi.edu.au>

Hi,

On Fri Apr 2, xt_wang wrote:
> I want to use R function Matrix inverse in my c code, please 
> tell me how I can.
> If there is a sample which can tell me how it works. It will 
> be fantastic.

A good place to start learning how to interface R with C is the 
"Writing R Extensions" manual installed locally, or:
http://cran.r-project.org/doc/manuals/R-exts.pdf
http://rweb.stat.umn.edu/R/doc/manual/R-exts.html

See also the article "In Search of C/C++ & Fortran Routines" in: 
http://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf

I guess you'd have to decide whether you want to compile R as a 
shared library or not.  I've never done this.

But do you really want to call R from C to get a matrix inverse?  
Can't you just use a CLAPACK routine?
http://www.netlib.org/clapack/

I'm not sure which CLAPACK subroutine is best for your purposes, 
but I have in the past used dgels (double-precision gaussian 
elimination and least-squares) and found it to be good.

I assume you have asked the question: 
"Do I really need an inverse?"

If Ax = b,
x = inv(A) * b 
is computationally about twice as expensive as Gaussian 
Elimination.

In R, to get help on solving linear systems, try:
?solve

> A <- matrix(c(1,2,3,4),nrow=2)
> Ainv <- solve(A)
> A %*% Ainv
     [,1] [,2]
[1,]    1    0
[2,]    0    1

Hope this helps,
James



From jasont at indigoindustrial.co.nz  Thu Apr 15 08:31:15 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 15 Apr 2004 18:31:15 +1200
Subject: [R] Not a bug - Was: [R] A bug report?
In-Reply-To: <20040414024709.GA23509@igidr.ac.in>
References: <20040414024709.GA23509@igidr.ac.in>
Message-ID: <407E2C33.80409@indigoindustrial.co.nz>

Ajay Shah wrote:
> where it is asif the summary.lm(m) statement never occurred. If I put
> in a statement print(m) it works, but the summary.lm(m) does not work.
> 
> Now here's what's weird: Suppose I remove the statement that comes
> AFTER this summary.lm(m) statement. That is, I don't say
>     details = summary.lm(m)
> as the last line of the function. In this case, the program works fine!
> 
> I'm most confused. I can't see how putting in an assignment statement
> AFTER a function call can contaminate a PREVIOUS statement. I would be
> most happy if you could guide me...

See the FAQ about summary() methods, and what they do.

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why%20is%20the%20output%20not%20printed%20when%20I%20source()%20a%20file%3f

Cheers

Jason



From erhansen at math.ku.dk  Thu Apr 15 08:35:56 2004
From: erhansen at math.ku.dk (Ernst Hansen)
Date: Thu, 15 Apr 2004 08:35:56 +0200
Subject: [R] A bug report?
In-Reply-To: <20040414024709.GA23509@igidr.ac.in>
References: <20040414024709.GA23509@igidr.ac.in>
Message-ID: <16510.11596.10282.599728@pc87.math.ku.dk>

Ajay Shah writes:
 > buoyancy <- function(year, taxbase, tax, description, plotname) {
 >   cat("Simple full OLS regression with all data:\n")
 >   logtax = log(tax)
 >   logtaxbase = log(taxbase)
 >   m = lm(logtax ~ logtaxbase)
 >   summary.lm(m)
 >   details = summary.lm(m)
 > }

 > This program does not work. The summary.lm(m) statement seems to have
 > no effect. When I run it, I get:
 > 
 > $ R --slave < bugreport.R 
 > Simple full OLS regression with all data:
 > 
 > where it is asif the summary.lm(m) statement never occurred. If I put
 > in a statement print(m) it works, but the summary.lm(m) does not work.
 > 

Ajay, 

the short answer is: the summary.lm() step is carried out all right,
you just never get to see the result, because it is not written to the
console.  If you want it, just wrap it in an explicit print() call.

This is completely standard behaviour.  If you have

  myf <- function() {2; 10}

the call myf() will return the value 10, but the value 2 will not
be seen anywhere.  While

  myg <- function() {print(2); 10}

will write both the value 2 and the value 10 to the console. 


The reason you get a useful response from your function if you delete
the 'detail =...' line, is because it makes the call to summary.lm()
the last line of the function and hence returns it.  Effectively
making summary.lm() correspond to 10 in the above toy functions,
instead of corresponding to 2.  



There is a more tricky answer to your questions: As you have written
the function, it actually returns the results from the summary.lm()
call -  but it is deliberatly not printed!  

What the function returns, is the value of the last statement, which
happens to be an assignment.  The value of an assignement is the value
that is being assigned.  But the default printing behaviour of an
assignement is to be silent.  If you write

  x <- 1

at the prompt, seemingly nothing is returned...  But actually the
expression has the value 1, it is just not written anywhere.  If you write

  (x <- 1)

or

  print(x <- 1)

you will actually get the number 1 on the console.  


This suggests three alternative ways of curing your function: 

 1) add a final line

      details

 to the function, making it return the value of 'details' rather than
 the value of the assignement. 

 2) Put a set of parenthesis around the assignment, as in

     (details = summary.lm(m))
  

 3) Leave the function as it is, but print it explicitly.  That is, do
 not call it as

     buoyancy(...)
 
 but as

     print(buoyancy(...))



I hope this helps,


Ernst Hansen
Department of Statistics
University of Copenhagen



From bxc at steno.dk  Thu Apr 15 09:54:49 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 15 Apr 2004 09:54:49 +0200
Subject: [R] pretty for a log-axis
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E5FC@exdkba022.novo.dk>

Is there a function that does the same as pretty but on a log-scale?

Suppose you have 

x <- exp( runif( 100, 0, 6 ) )

(which will between 1 and 403), then I would like to have a result like:

log.pretty( x )
[1] 1 5 10 50 100 500

Bendix C.
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From Bernhard.Pfaff at drkw.com  Thu Apr 15 10:01:23 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Thu, 15 Apr 2004 10:01:23 +0200
Subject: [R] trend turning points
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B8C1@ibfftce505.is.de.dresdnerkb.com>

> > 
> > does anybody know of a nice test to detect trend turning points in
> > time series? Possibly with reference?
> 
> You can look at the function breakpoints() in the package strucchange
> and the function segmented() in the package segmented which do
> segmentation of (generalized) linear regression models. The 
> former tries
> to fit fully segmented regression models, the latter broken 
> line trends.
> References are given on the respective help pages.
> 
> A suitable test for a change in trend in linear regression 
> models is the
> OLS-based CUSUM test with a Cramer-von Mises functional of
> Kraemer & Ploberger (1996, JoE) which is available via efp() in
> strucchange and associated methods.


Hello Joerg,

additionally to Achim's comments, there is also the Zivot & Andrews test
available in the contributed package 'urca',

  Zivot, E. and Andrews, Donald W.K. (1992), Further Evidence on the
  Great Crash, the Oil-Price Shock, and the Unit-Root Hypothesis,
  Journal of Business & Economic Statistics, 10(3),
  251-270.

  Download possible at: \url{http://cowles.econ.yale.edu/}, see rubric
  'Discussion Papers (CFDPs)'.


in case you are also interested in inferences on integration of your time
series at hand.

Bernhard

> 
> hth,
> Z
> 
> > Thanks,
> > 
> > joerg
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From david.whiting at ncl.ac.uk  Thu Apr 15 13:42:18 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 15 Apr 2004 11:42:18 +0000
Subject: [R] tapply() and barplot() help files for 1.8.1
Message-ID: <m2smf5bh7p.fsf@ganymede.ammp.or.tz>


Hi,

I've just upgraded to 1.9.0 and one of my Sweave files that produces a
number of barplots in a standard manner now produces them in a
different way.  I have made a couple of small changes to my code to
get the back the output I was getting before upgrading and now (mostly
out of curiosity) would like to understand what has changed.

I *think* I've tracked it down to tapply() and/or barplot() and have
not seen anything in the NEWS file regarding changes to these
functions (as far a I can see).  As part of doing my homework, I would
like to read the version 1.8.1 help files for these two functions, but
now that I've upgraded I'm not sure where I can find them.  Is there a
simple way for me to get copies of these two help files to compare
with the versions in 1.9.0?  As far as I can see, barplot() and
tapply() in 1.9.0 work as described in their 1.9.0 help files (which
does not surprise me).

I've been lurking on this list long enough to know that if there has
been a change it is documented, so it must be that I just haven't
found it yet.  If there hasn't been a change, then I am totally
perplexed, because I have been running this Sweave file several times
a day for the last few weeks and have not changed that part of it
(I've been changing the LaTeX parts).

In the part of the code that has changed I use tapply() to summarise
some data and then plot it with barplot().  I now have to use matrix()
on the output of tapply() before using barplot() because tapply()
produces a list and barplot() wants a vector or matrix.

In the code below, z is a dataframe, yllperdth is a numeric and fld
is the name of a factor, both in the dataframe.

Old version (as used with R 1.8.1):

  ## Calculate the % of YLLs for each group in the cause classification.
  x <- tapply(z$yllperdth, z[, fld], sum)
  totalYLLs <- sum(x)
  x <- x / totalYLLs * 100
  x <- sort(x)
  
  ## Plot the chart. horiz = TRUE makes it a bar instead of 
  ## column chart.  las = 1 prints the labels horizontally.
  xplot <- barplot(x, 
                   horiz = TRUE, 
                   xlab = "Percent of YLLs",
                   las = 1)


New Version (as used with R 1.9.0):

  ## Calculate the % of YLLs for each group in the cause classification.
  x <- tapply(z$yllperdth, z[, fld], sum)
  totalYLLs <- sum(x)
  x <- x / totalYLLs * 100
  x <- sort(x)

  causeNames <- names(x)  ## NEW BIT
  x <- matrix(x)          ## NEW BIT
  

  ## Plot the chart. horiz = TRUE makes it a bar instead of 
  ## column chart.  las = 1 prints the labels horizontally.
  xplot <- barplot(x, 
                   beside = TRUE,           ## NEW BIT
                   names.arg = causeNames,  ## NEW BIT
                   horiz = TRUE, 
                   xlab = "Percent of YLLs",
                   las = 1)




> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    9.0              
year     2004             
month    04               
day      12               
language R                


A little while before upgrading I noted my previous R version (for a
post that I redrafted 7 times and never sent because I found the answer
through refining my draft), and it was:

> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status   Patched          
major    1                
minor    8.1              
year     2004             
month    02               
day      16               
language R                


So, can I get the old help files?  Or it is easy to point me to a
documented change?  Or is it clear from my code what has changed or
what I am or was doing wrong?

Thanks.

Dave

-- 
David Whiting
Dar es Salaam, Tanzania



From sam.kemp2 at ntlworld.com  Wed Apr 14 23:20:24 2004
From: sam.kemp2 at ntlworld.com (Samuel Edward Kemp)
Date: Wed, 14 Apr 2004 22:20:24 +0100
Subject: [R] binary numbers
Message-ID: <407DAB18.4060901@ntlworld.com>

Hi,

Is there a function in R that lets one represent an integer in binary 
format for a given number of bits? So an example would be....

 > binary.function(num=5, num.of.bits=8)
 > "00000101"

Or, is this something I have to write myself?

Any help would be appreciated.

Cheers,

Sam.



From ligges at statistik.uni-dortmund.de  Thu Apr 15 12:09:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Apr 2004 12:09:50 +0200
Subject: [R] odbcFetchRows not work?
In-Reply-To: <49697.170.210.173.216.1081958289.squirrel@inter17.unsl.edu.ar>
References: <49697.170.210.173.216.1081958289.squirrel@inter17.unsl.edu.ar>
Message-ID: <407E5F6E.8010208@statistik.uni-dortmund.de>

solares at unsl.edu.ar wrote:

> Hi, excuse me i am newbee in R, but i dont understand the following code
> not work. Libro1.xls have two files and not load in R, under windows. Thanks
> 
>  library(RODBC)
> 
>>f<-file.choose()
>>f
> 
> [1] "C:\\Mis documentos\\ruben\\r19\\Libro1.xls"
> 
>>help(odbcConnectExcel)
>>canal<- odbcConnectExcel(f)
>>canal
> 
> RODB Connection 0
> Details:
>   case=nochange
>   DBQ=C:\Mis documentos\ruben\r19\Libro1.xls
>   DefaultDir=C:\Mis documentos\ruben\r19
>   Driver={Microsoft Excel Driver (*.xls)}
>   DriverId=790
>   MaxBufferSize=2048
>   PageTimeout=5
> 
>>help(odbcFetchRows)
>>tbl<- odbcFetchRows(canal, max = 0)

I think you are looking for sqlFetch() or sqlQuery() ...

Uwe Ligges



>>tbl
> 
> $data
> list()
> 
> $stat
> [1] -1
> 
> 
>>tbl<- odbcFetchRows(canal, max = 10)
>>tbl
> 
> $data
> list()
> 
> $stat
> [1] -1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Apr 15 12:17:48 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Apr 2004 12:17:48 +0200
Subject: [R] binary numbers
In-Reply-To: <407DAB18.4060901@ntlworld.com>
References: <407DAB18.4060901@ntlworld.com>
Message-ID: <407E614C.7000407@statistik.uni-dortmund.de>

Samuel Edward Kemp wrote:

> Hi,
> 
> Is there a function in R that lets one represent an integer in binary 
> format for a given number of bits? So an example would be....
> 
>  > binary.function(num=5, num.of.bits=8)
>  > "00000101"
> 
> Or, is this something I have to write myself?
> 
> Any help would be appreciated.

binary() in package "wle".

Uwe Ligges




> Cheers,
> 
> Sam.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sam.kemp2 at ntlworld.com  Thu Apr 15 00:26:22 2004
From: sam.kemp2 at ntlworld.com (Samuel Edward Kemp)
Date: Wed, 14 Apr 2004 23:26:22 +0100
Subject: [R] binary numbers
In-Reply-To: <407E614C.7000407@statistik.uni-dortmund.de>
References: <407DAB18.4060901@ntlworld.com>
	<407E614C.7000407@statistik.uni-dortmund.de>
Message-ID: <407DBA8E.4090200@ntlworld.com>

Hi,

I just wrote a little C program (and interfaced it with R) that does it 
for me, but I will take a look at that function.

Cheers,

Sam.



Uwe Ligges wrote:

> Samuel Edward Kemp wrote:
>
>> Hi,
>>
>> Is there a function in R that lets one represent an integer in binary 
>> format for a given number of bits? So an example would be....
>>
>>  > binary.function(num=5, num.of.bits=8)
>>  > "00000101"
>>
>> Or, is this something I have to write myself?
>>
>> Any help would be appreciated.
>
>
> binary() in package "wle".
>
> Uwe Ligges
>
>
>
>
>> Cheers,
>>
>> Sam.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>



From rksh at soc.soton.ac.uk  Thu Apr 15 12:27:39 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 15 Apr 2004 11:27:39 +0100
Subject: [R] binary numbers
In-Reply-To: <407DAB18.4060901@ntlworld.com>
References: <407DAB18.4060901@ntlworld.com>
Message-ID: <a06002005bca4136d0746@[139.166.242.29]>

Hi Sam.  Try:

as.binary <- function(n,base=2 , r=FALSE)
{
   out <- NULL
   while(n > 0) {
     if(r) {
       out <- c(out , n%%base)
     } else {
       out <- c(n%%base , out)
     }   
     n <- n %/% base
   }
   return(out)
}

HTH

robin


>Hi,
>
>Is there a function in R that lets one represent an integer in 
>binary format for a given number of bits? So an example would be....
>
>>  binary.function(num=5, num.of.bits=8)
>>  "00000101"
>
>Or, is this something I have to write myself?
>
>Any help would be appreciated.
>
>Cheers,
>
>Sam.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From wettenhall at wehi.EDU.AU  Thu Apr 15 12:52:03 2004
From: wettenhall at wehi.EDU.AU (wettenhall@wehi.EDU.AU)
Date: Thu, 15 Apr 2004 20:52:03 +1000 (EST)
Subject: [R] Solving Matrices
Message-ID: <3466.138.217.58.200.1082026323.squirrel@homebase.wehi.edu.au>

On April 15th, Elizabeth wrote:
<snip>
>      In execises 39-42, determine if the columns of the matrix span
>      R4:
<snip>
>(or x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5,
>                        4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)
>
>That is the whole of the question <snip>

Have you tried det(x) and/or eigen(x) ?

A zero determinant (within computer precision) means that the matrix does
not have full rank, i.e. it does not span R4.  Count how many eigenvalues
are zero (within computer precision).  What does this tell you?

>    In the Solutions Manual, there is mention of the gauss() and
>bgauss() functions which apparently written by Lay - these are to
>speed up matrix reduction but I have not noticed these functions in R.

Have you encountered speed problems in R?  Do you really need these
functions in R in addition to solve(), backsolve() etc. ?  If you just
want to learn about how they work you could have a look at the Matlab
code for these functions (which I think you have access to).  You could
even try rewriting them in R yourself.

Hope this helps,
James



From highstat at highstat.com  Thu Apr 15 12:51:24 2004
From: highstat at highstat.com (Highland Statistics Ltd.)
Date: Thu, 15 Apr 2004 05:51:24 -0500
Subject: [R] RE: trend turning points
Message-ID: <200404151051.i3FApN608157@atmapp03.siteprotect.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040415/c855c031/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Apr 15 13:06:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Apr 2004 13:06:47 +0200
Subject: [R] how to add legend to time series plot
In-Reply-To: <1081949041.407d3b7199414@kurla.upf.edu>
References: <1081949041.407d3b7199414@kurla.upf.edu>
Message-ID: <407E6CC7.9010409@statistik.uni-dortmund.de>

michael.wolf at upf.edu wrote:

> Dear all,
> 
> I would like to add a legend to a time series plot, but 
> cannot get it done. I have searched the archive about this,
> but to no avail ...
> 
> I have three sets of time series data stored in a matrix
> wMat. The following code plots the the data with
> a legend, but it does not put the time on the x-axis:
> 
> 
> matplot(y = wMat, type = "l",
>         ylab = "Allocation", main = "GARCH")
> legend(1, -0.3, c("Stocks", "Bonds", "Cash"), col = 1:3, lty = 1:3)
> 
> 
> The following code puts the time on the x-axis but now the
> legend does not show up:
> 
> 
> wMat = ts(wMat, frequency = 12, start = c(1968, 2))
> ts.plot(wMat[,1], wMat[,2], wMat[,3], col = 1:3, lty = 1:3,
>         ylab = "Allocation", main = "GARCH")
> legend(1, -0.3, c("Stocks", "Bonds", "Cash"), col = 1:3, lty = 1:3)

I guess it is clipped in this case. Use par("usr") after ts.plot() to 
see what the user coordinates are, and whether (1, -0.3) is a sensible 
location to place the legend.

Uwe Ligges


> Can anybody help with this? If needed, I can send along postscript files
> of the resulting plots.
> 
> I do not subscribe to the list, so please (also) reply directly to me.
> 
> Thanks very much,
> Michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Apr 15 14:03:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 08:03:28 -0400
Subject: [R] A bug report?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BEA@usrymx25.merck.com>

Your question has been answered by others.  I just want to point out that
you should not be calling summary.lm() directly, but should call the generic
summary() instead.

Andy

> From: Ajay Shah
> 
> Folks,
> 
> I have a strange situation, which I may have isolated as a bug
> report. Or, it could just be that there's something about R that I
> don't know. :-) I have attached the data file and the program file but
> don't know whether these attachments will make it into the list. Here
> is my bugreport.R program --
> 
> --------------------------------------------------------------
> -------------
> buoyancy <- function(year, taxbase, tax, description, plotname) {
>   cat("Simple full OLS regression with all data:\n")
>   logtax = log(tax)
>   logtaxbase = log(taxbase)
>   m = lm(logtax ~ logtaxbase)
>   summary.lm(m)
>   details = summary.lm(m)
> }
> 
> A <- read.table(file="amodi-data.csv", sep=",", col.names=c("year",
>                 "gdp.ag", "gdp.mining", "gdp.manuf", "gdp.elecgas",
>                 "gdp.construction", "gdp.industry", "gdp.services",
>                 "gdp.fc", "indirect.taxes", "subsidies", "j1",
>                 "gdp.mp", "gdp.mp.93", "gdp.deflator", "gdp.fc.93",
>                 "gdp.ag.93", "gdp.industry.93", "gdp.services.93",
>                 "tax.income", "tax.corporation", "tax.direct.others",
>                 "tax.direct", "tax.customs", "tax.excise",
>                 "tax.indirect.others", "tax.indirect", "tax.total"))
> A = subset(A, !is.na(A$tax.total))
> buoyancy(A$year, A$gdp.mp, A$tax.income, "Personal income tax 
> and GDPmp", "p1")
> --------------------------------------------------------------
> -------------
> 
> This program does not work. The summary.lm(m) statement seems to have
> no effect. When I run it, I get:
> 
> $ R --slave < bugreport.R 
> Simple full OLS regression with all data:
> 
> where it is asif the summary.lm(m) statement never occurred. If I put
> in a statement print(m) it works, but the summary.lm(m) does not work.
> 
> Now here's what's weird: Suppose I remove the statement that comes
> AFTER this summary.lm(m) statement. That is, I don't say
>     details = summary.lm(m)
> as the last line of the function. In this case, the program 
> works fine!
> 
> I'm most confused. I can't see how putting in an assignment statement
> AFTER a function call can contaminate a PREVIOUS statement. I would be
> most happy if you could guide me...
> 
> I am running on a nicely-working notebook which runs Debian linux
> kernel 2.4.17, and have R 1.8.1 (2003-11-21). I use the `testing'
> branch of Debian.
> 
>      -ans.
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From iwhite at staffmail.ed.ac.uk  Thu Apr 15 14:26:44 2004
From: iwhite at staffmail.ed.ac.uk (I M S White)
Date: Thu, 15 Apr 2004 13:26:44 +0100 (BST)
Subject: [R] summary(difftime(...))
Message-ID: <Pine.GSO.4.58.0404151318470.28978@holyrood.ed.ac.uk>

Is there any way to get a numerical summary of the values of a difftime
object? E.g.

TimeToWean <- difftime(WeanDate, BirthDate, units = "days")

I can repeat

sum(TimeToWean == 20)

with as many other values are needed to build up a frequency table,
but is there a simpler way in R version 1.8.1?

======================================
I.White
ICAPB, University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Fax: 0131 650 6564  Tel: 0131 650 5490
E-mail: iwhite at staffmail.ed.ac.uk



From ligges at statistik.uni-dortmund.de  Thu Apr 15 14:36:00 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Apr 2004 14:36:00 +0200
Subject: [R] summary(difftime(...))
In-Reply-To: <Pine.GSO.4.58.0404151318470.28978@holyrood.ed.ac.uk>
References: <Pine.GSO.4.58.0404151318470.28978@holyrood.ed.ac.uk>
Message-ID: <407E81B0.90908@statistik.uni-dortmund.de>

I M S White wrote:

> Is there any way to get a numerical summary of the values of a difftime
> object? E.g.
> 
> TimeToWean <- difftime(WeanDate, BirthDate, units = "days")
> 
> I can repeat
> 
> sum(TimeToWean == 20)
> 
> with as many other values are needed to build up a frequency table,
> but is there a simpler way in R version 1.8.1?


Are you looking for table() ?

Uwe Ligges



From maechler at stat.math.ethz.ch  Thu Apr 15 14:44:23 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Apr 2004 14:44:23 +0200
Subject: [R] pretty for a log-axis
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E5FC@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90179E5FC@exdkba022.novo.dk>
Message-ID: <16510.33703.597406.179705@gargle.gargle.HOWL>

>>>>> "BXC" == BXC (Bendix Carstensen) <bxc at steno.dk>
>>>>>     on Thu, 15 Apr 2004 09:54:49 +0200 writes:

    BXC> Is there a function that does the same as pretty but on
    BXC> a log-scale?  Suppose you have

    BXC> x <- exp( runif( 100, 0, 6 ) )

    BXC> (which will between 1 and 403), then I would like to
    BXC> have a result like:

    BXC> log.pretty( x )
    BXC> [1] 1 5 10 50 100 500

axTicks()  can be used for this and will use the same algorithm
that R uses internally;  it's not as flexible as pretty() is,
but should fulfill your needs.

As I (author of the function) just see, the documentation of
axTicks is insufficient:   
?axTicks points to ?par ("xaxp") which points
back to ?axTicks without explaining the crucial meaning of  axp[3]
for the log case:

axp[3]	 values
------	 ---------------
  1	   1      * 10^j
  2       (1,5)   * 10^j
  3       (1,2,5) * 10^j

Also, contrary to help(axTicks), the result of axTicks() does
not depend on the graphics state at all --- *when* all arguments
are specified (differently from the default NULL).

Here is an example "script":

## just to "repeat your example" :
> set.seed(1); x <- exp(runif(100,0,6)); rx <- range(x); summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  1.084   6.950  18.680  62.230  99.800 384.300 

> axTicks(1, axp=c(rx, 1), usr=c(1,3), log=TRUE)
[1]   10  100 1000
> axTicks(1, axp=c(rx, 2), usr=c(1,3), log=TRUE)
[1]   10   50  100  500 1000
> axTicks(1, axp=c(rx, 3), usr=c(1,3), log=TRUE)
[1]   10   20   50  100  200  500 1000
> axTicks(1, axp=c(rx, 3), usr=c(0,3), log=TRUE)
 [1]    1    2    5   10   20   50  100  200  500 1000

--- note that "usr" is in the log-transformed scale.

Probably the whole thing is most easily understood by looking at
the source (*including* the comments you usually won't see!)
of which the relevant part is

........
    if(log && axp[3] > 0) { ## special log-scale axp[]
        if(!any((iC <- as.integer(axp[3])) == 1:3))
            stop("invalid positive axp[3]")
        if(is.null(usr)) usr <- par("usr")[if(is.x) 1:2 else 3:4]
        else if(!is.numeric(usr) || length(usr) != 2) stop("invalid `usr'")
        ii <- round(log10(axp[1:2]))
        x10 <- 10^((ii[1] - (iC >= 2)):ii[2])
	r <- switch(iC,				## axp[3]
		    x10,			## 1
		    c(outer(c(1,  5), x10))[-1],## 2
                    c(outer(c(1,2,5), x10))[-1])## 3
        r[usr[1] <= log10(r) & log10(r) <= usr[2]]
    } else { # linear
........

Hoping that helps,
Martin Maechler



From bitwrit at ozemail.com.au  Thu Apr 15 14:50:19 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 15 Apr 2004 22:50:19 +1000
Subject: [R] summary(difftime(...))
In-Reply-To: <407E81B0.90908@statistik.uni-dortmund.de>
References: <Pine.GSO.4.58.0404151318470.28978@holyrood.ed.ac.uk>
	<407E81B0.90908@statistik.uni-dortmund.de>
Message-ID: <20040415124657.RANZ1359.smta01.mail.ozemail.net@there>

I M S White wrote:
> Is there any way to get a numerical summary of the values of a difftime
> object? E.g.
>
> TimeToWean <- difftime(WeanDate, BirthDate, units = "days")
>
> I can repeat
>
> sum(TimeToWean == 20)
>
> with as many other values are needed to build up a frequency table,
> but is there a simpler way in R version 1.8.1?
>
perhaps tabulate() will do what you want. For a somewhat more expansive 
output, you might look at the freq() function in "Kickstarting R" which can 
be browsed online at:

http://cran.r-project/doc/contrib/Lemon-kickstart/index.html

see the section "Frequencies".

Jim



From maechler at stat.math.ethz.ch  Thu Apr 15 14:55:09 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Apr 2004 14:55:09 +0200
Subject: [R] binary numbers
In-Reply-To: <407DAB18.4060901@ntlworld.com>
References: <407DAB18.4060901@ntlworld.com>
Message-ID: <16510.34349.928143.242715@gargle.gargle.HOWL>

>>>>> "Samuel" == Samuel Edward Kemp <sam.kemp2 at ntlworld.com>
>>>>>     on Wed, 14 Apr 2004 22:20:24 +0100 writes:

    Samuel> Hi, Is there a function in R that lets one represent
    Samuel> an integer in binary format for a given number of
    Samuel> bits? So an example would be....

    >> binary.function(num=5, num.of.bits=8) "00000101"

    Samuel> Or, is this something I have to write myself?

no.  In package "sfsmisc", there's also

>>   digitsBase              package:sfsmisc              R Documentation
>> 
>>   Digit/Bit Representation of Integers in any Base
>> 
>>   Description:
>> 
>>        Compute the vector of "digits" A of the 'base' b representation of
>>        a number N, N = sum(k = 0:M ; A[M-k] * b^k).
>> 
>>   Usage:
>> 
>>        digitsBase(x, base = 2, ndigits = 1 + floor(log(max(x), base)))

e.g.,

> library(sfsmisc) # after installing it

> digitsBase(5, base= 2, 10)
      [,1]
 [1,]    0
 [2,]    0
 [3,]    0
 [4,]    0
 [5,]    0
 [6,]    0
 [7,]    0
 [8,]    1
 [9,]    0
[10,]    1

> empty.dimnames(digitsBase(0:33, 2)) # binary
                                                                    
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0
 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0
 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

where you see that it does work vectorized.



From bates at stat.wisc.edu  Thu Apr 15 15:07:52 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 15 Apr 2004 08:07:52 -0500
Subject: [R] Use R function in C code
In-Reply-To: <Pine.LNX.4.58.0404151550190.7357@unix28.alpha.wehi.edu.au>
References: <Pine.LNX.4.58.0404151550190.7357@unix28.alpha.wehi.edu.au>
Message-ID: <6rr7uptmmv.fsf@bates4.stat.wisc.edu>

James Wettenhall <wettenhall at wehi.edu.au> writes:

> Hi,
> 
> On Fri Apr 2, xt_wang wrote:
> > I want to use R function Matrix inverse in my c code, please 
> > tell me how I can.
> > If there is a sample which can tell me how it works. It will 
> > be fantastic.
> 
> A good place to start learning how to interface R with C is the 
> "Writing R Extensions" manual installed locally, or:
> http://cran.r-project.org/doc/manuals/R-exts.pdf
> http://rweb.stat.umn.edu/R/doc/manual/R-exts.html
> 
> See also the article "In Search of C/C++ & Fortran Routines" in: 
> http://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf
> 
> I guess you'd have to decide whether you want to compile R as a 
> shared library or not.  I've never done this.
> 
> But do you really want to call R from C to get a matrix inverse?  
> Can't you just use a CLAPACK routine?
> http://www.netlib.org/clapack/
> 
> I'm not sure which CLAPACK subroutine is best for your purposes, 
> but I have in the past used dgels (double-precision gaussian 
> elimination and least-squares) and found it to be good.
> 
> I assume you have asked the question: 
> "Do I really need an inverse?"

Exactly.  The fact that we write a formula using an inverse of a
matrix does not mean that this is a good way to compute the result.

Versions 0.8-2 and higher of the Matrix package contain a vignette
called Comparisons.pdf that provides comparative timings of several
different approaches to solving a linear least squares problem.

> If Ax = b,
> x = inv(A) * b 
> is computationally about twice as expensive as Gaussian 
> Elimination.

Actually the "ge" in "dgels" stands for "general", not Gaussian
elimination.  For square coefficient matrices the 'solve' function in
R now uses an LU decomposition (dgetrf) followed by dgetrs for
non-missing "b" or dgetri for the inverse ("b" missing).

The general rule is that if you are solving a specific set of linear
equations use dgetrf and dgetrs.  If you decide that you really do
need an inverse, which is surprisingly rare, use dgetrf and dgetri.



From bates at stat.wisc.edu  Thu Apr 15 15:19:49 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 15 Apr 2004 08:19:49 -0500
Subject: [R] Solving Matrices
In-Reply-To: <3466.138.217.58.200.1082026323.squirrel@homebase.wehi.edu.au>
References: <3466.138.217.58.200.1082026323.squirrel@homebase.wehi.edu.au>
Message-ID: <6rn05dtm2y.fsf@bates4.stat.wisc.edu>

wettenhall at wehi.EDU.AU writes:

> On April 15th, Elizabeth wrote:
> <snip>
> >      In execises 39-42, determine if the columns of the matrix span
> >      R4:
> <snip>
> >(or x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5,
> >                        4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)
> >
> >That is the whole of the question <snip>
> 
> Have you tried det(x) and/or eigen(x) ?

An alternative is to determine the condition number (kappa) of the
matrix 

> x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5, 
+                        4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)
> kappa(x)
[1] 5.31557e+17

A very large  condition number like this indicates that the matrix is
computationally singular.



From junwen at astro.ocis.temple.edu  Thu Apr 15 15:50:34 2004
From: junwen at astro.ocis.temple.edu (Junwen wang)
Date: Thu, 15 Apr 2004 09:50:34 -0400 (EDT)
Subject: [R] R apache and PHP
Message-ID: <Pine.OSF.4.53.0404150946260.907105@gs873ps>

Apache may not aware the PATH to R, try 'exec("/pathToR/R CMD ...")'.
In addition, apache and R should be in the same physical driver.

-Junwen

> I've developed a web application in PHP and R
>
> my script is
>
>
> <?php
>
> ...
> exec("R CMD BATCH --silent /home/marcello/R_in/myfile.bat
> /home/marcello/R_out/myfile.out");
>



From ligges at statistik.uni-dortmund.de  Thu Apr 15 15:59:44 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Apr 2004 15:59:44 +0200
Subject: [R] un-expected return by fdim
In-Reply-To: <20040414024150.99351.qmail@web20504.mail.yahoo.com>
References: <20040414024150.99351.qmail@web20504.mail.yahoo.com>
Message-ID: <407E9550.4020705@statistik.uni-dortmund.de>

Fred J. wrote:

> Browse[1]> Lframe
>    v  v  v  v  v  v v v
> 1  8  7  6  5  4  3 2 1
> 2  9  8  7  6  5  4 3 2
> 3 10  9  8  7  6  5 4 3
> 4 11 10  9  8  7  6 5 4
> 5 12 11 10  9  8  7 6 5
> 6 13 12 11 10  9  8 7 6
> 7 14 13 12 11 10  9 8 7
> 8 15 14 13 12 11 10 9 8
> Browse[1]> fdim(Lframe,q=2)
> Error in slopeopt(AllPoints, Alpha) : Object "LineP"
> not found
> 


Dear Fred J.,

what is fdim()?

 > fdim
Error: Object "fdim" not found


Please, consider to follow the posting guide and help the readers of 
this list to easily understand your questions.

Uwe Ligges



From erich.neuwirth at univie.ac.at  Thu Apr 15 16:04:03 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 15 Apr 2004 16:04:03 +0200
Subject: [R] Solving Matrices
In-Reply-To: <6rn05dtm2y.fsf@bates4.stat.wisc.edu>
References: <3466.138.217.58.200.1082026323.squirrel@homebase.wehi.edu.au>
	<6rn05dtm2y.fsf@bates4.stat.wisc.edu>
Message-ID: <407E9653.4050501@univie.ac.at>



>>On April 15th, Elizabeth wrote:
>><snip>
>>
>>>(or x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5,
>>>                       4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)


qr(x)$rank
gives the rank
qr.R(qr(x))
gives the R part (an upper triangular matrix similar to the one
produced by Gauss elimination).
With the given matrix, we get rank 3, but the
last row of qr.R(qr(x)) is not 0 due to rounding errors.

-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From andy_liaw at merck.com  Thu Apr 15 16:40:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 10:40:41 -0400
Subject: [R] all(logical(0)) and any(logical(0))
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>

Dear R-help,

I was bitten by the behavior of all() when given logical(0):  It is TRUE!
(And any(logical(0)) is FALSE.)  Wouldn't it be better to return logical(0)
in both cases?

The problem surfaced because some un-named individual called randomForest(x,
y, xtest, ytest,...), and gave y as a two-level factor, but ytest as just
numeric vector.  I thought I check for that in my code by testing for

if (!all(levels(y) == levels(ytest))) stop(...)

but levels() on a non-factor returns NULL, and the comparison ended up being
logical(0).  Since all(logical(0)) is TRUE, the error is not flagged.

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com        732-594-0820



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From sway at tanox.com  Thu Apr 15 16:41:48 2004
From: sway at tanox.com (Shawn Way)
Date: Thu, 15 Apr 2004 09:41:48 -0500
Subject: [R] residuals
Message-ID: <2F3262756375D411B0CC00B0D049775D01A4CEC2@westpark.tanox.net>


I'm trying to determine the lack of fit for regression on the following:

data <- data.frame(ref=c(0,50,100,0,50,100),
                             actual=c(.01,50.9,100.2,.02,49.9,100.1),
                             level=gl(3,1))
fit <- lm(actual~ref,data)
fit.aov <- aov(actual~ref+Error(level),data)

According to the information I have, the lack of fit for this regression is
the f-ratio between the residuals of the level contribution and the
residuals within.  I'm trying to get the information from the fit to make
this ratio of the residual mean squares.

Any thoughts?



From cmoffet at nwrc.ars.usda.gov  Thu Apr 15 16:52:18 2004
From: cmoffet at nwrc.ars.usda.gov (Corey Moffet)
Date: Thu, 15 Apr 2004 08:52:18 -0600
Subject: [R] vectorizing row selection
Message-ID: <3.0.6.32.20040415085218.010c75b8@pxms.nwrc.ars.usda.gov>

Dear R-help:

I have a data frame (df1) with elements a, b, and c that identify a unique
set of conditions of interest; l and m identify other conditions; and x and
y are responses.

df1 <- data.frame(a = c(1,1,1,2,2,2,3,3,3), b = c(10,10,10,20,20,20,30,30,30),
                  c = c(100,100,100,200,200,200,300,300,300), 
                  l = runif(9), m = runif(9), 
                  x = c(1,2,2,2,2,1,2,2,1), y = c(3,2,1,3,2,1,3,2,1))
df1

I want to select 1 row from df1 for each combination of df1$a, df1$b, and
df1$c that has first the max of df1$x for that combination and than in the
case of a tie the max of df1$y and put in a new data.frame:

  a  b   c         l           m x y
1 1 10 100 0.2222679 0.351739848 2 2
2 2 20 200 0.2219270 0.002530816 2 3
3 3 30 300 0.1260224 0.820658343 2 3

My method is as follows:

max.by.x <- aggregate(list(x = df1$x), 
                      list(a = df1$a, b = df1$b, c = df1$c), 
                      max)

df2 <- df1[1,]
for ( i in 1:length(max.by.x[,1]) ) {
   index <- which(df1$a == max.by.x$a[i] &
                  df1$b == max.by.x$b[i] &
                  df1$c == max.by.x$c[i] &
                  df1$x == max.by.x$x[i])
   index2 <- which.max(df1$y[index])
   df2[i,] <- df1[index[index2],]
}

df2

This seems to work, but for real data with 12000 rows it is really slow.
Does anyone have any ideas for improvement (e.g. vectorizing what is done
in the loop)?

With best wishes and kind regards I am

Sincerely,

Corey A. Moffet
Rangeland Scientist

##################################################################
                                            ####		     
USDA-ARS                                        #		     
Northwest Watershed Research Center             #		     
800 Park Blvd, Plaza IV, Suite 105          ###########   ####    
Boise, ID 83712-7716                       #    #      # #        
Voice: (208) 422-0718                      #    #  ####   ####    
FAX:   (208) 334-1502                      #    # #           #   
                                            ####   ###########    
##################################################################



From tlumley at u.washington.edu  Thu Apr 15 16:56:39 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 15 Apr 2004 07:56:39 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <20040414160053.59696.qmail@web60103.mail.yahoo.com>
References: <20040414160053.59696.qmail@web60103.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0404150755000.34854@homer05.u.washington.edu>

On Wed, 14 Apr 2004, Fred Rohde wrote:

> I wasn't sure if JRR or BRR methods were valid for quantiles?

BRR is valid, as is Fay's method, a smoothed version. The jackknife
methods probably aren't. At some point I should probably try to add
bootstrap methods for designs where BRR doesn't fit well.

	-thomas



From rolf at math.unb.ca  Thu Apr 15 17:01:34 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 15 Apr 2004 12:01:34 -0300 (ADT)
Subject: [R] all(logical(0)) and any(logical(0))
Message-ID: <200404151501.i3FF1Y58000568@erdos.math.unb.ca>


Andy Liaw wrote:

> I was bitten by the behavior of all() when given logical(0):  It is
> TRUE!  (And any(logical(0)) is FALSE.)  Wouldn't it be better to
> return logical(0) in both cases?

It seems to me that what R does is strictly speaking correct.
Anything you say about the members of the empty set is true.
If a set of logical entities is empty, then it is correct to
say that all members of that set are TRUE.  (Because you cannot
find a counter-example --- you cannot find a member of that
set which isn't TRUE.)

Likewise you can't find a member of that set which ***is*** TRUE
so the answer to the question ``Are any of these TRUE?'' is ``No.'',
i.e. ``any(logical(0))'' is FALSE.

So returning logical(0) in these cases is not strictly correct;
whether it would do any harm is not clear to me --- i.e. I can't
think of an example where it would cause harm.  And of course it
would guard against the Trap For Young Players that Andy Liaw
described.  However since it is not strictly correct, great caution
ought to be used.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From andy_liaw at merck.com  Thu Apr 15 17:02:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 11:02:04 -0400
Subject: [R] all(logical(0)) and any(logical(0))
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BF1@usrymx25.merck.com>

I wrote:

> I was bitten by the behavior of all() when given logical(0):  
> It is TRUE!
> (And any(logical(0)) is FALSE.)  Wouldn't it be better to 
> return logical(0)
> in both cases?

I guess the behavior is consistent with:

> prod(numeric(0))
[1] 1
> sum(numeric(0))
[1] 0

but why?

Andy

>



From spencer.graves at pdf.com  Thu Apr 15 17:04:32 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 15 Apr 2004 08:04:32 -0700
Subject: [R] Non-Linear Regression Problem
In-Reply-To: <000001c4229a$737ee320$b43d68c8@CPQ28661778111>
References: <000001c4229a$737ee320$b43d68c8@CPQ28661778111>
Message-ID: <407EA480.9010804@pdf.com>

      Have you considered "try"? 

      When I do this, I prefer to parameterize the problem to avoid 
situations where the function is undefined and provide alternative 
algorithms were a function will have numerical difficulties.  For 
example, suppose I want (sin(x)/x), and I get NA when x = 0.  I might do 
something like the following: 

      sinx.x <- sin(x)/x
      x0 <- (abs(x)<1e-9)
      if(any(x0))
           sinx.x[x0] <- 1

      Sometimes, I'll program a 2- or 3-term Taylor expansion for the 
exceptional case and use it when (abs(x)<1e-4), for example. 

      hope this helps. 
      spencer graves

Christian Mora wrote:

>Working on the same idea, Ive generated a data grid with 4 vars, two of
>them with its own sequence and two with fixed values. As Spencer pointed
>out one option is to get the values from a simple loop. My question is:
>How can jump from one set of starting values to the next (on the data
>grid) in case they cause an invalid value in the model Im evaluating? In
>my example, Ive 12 possible combinations of the 4 vars and in the 6th
>combination I get an error and the loop is terminated so what Im looking
>for is to skip this problem and continue from the 7th to 12th
>combination of starting points
>
>Ill appreciate any comment
>
>Code:
>
>data<-expand.grid(alpha=100,delta=4,beta=seq(1,2,by=0.5),gamma=seq(.1,.4
>,by=.1))
>for(i in 1:12){
>fit<-nls(y~delta+(alpha-delta)/(1+exp(beta*log(rate/gamma))),data=base,s
>tart=c(alpha=data$alpha[i],delta=data$delta[i],beta=data$beta[i],gamma=d
>ata$gamma[i]),trace=T)
>}
>
>Thanks
>CMora
>
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>Sent: Wednesday, April 14, 2004 12:27 PM
>To: WilDscOp
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] Non-Linear Regression Problem
>
>
>1.  For the equation you mentioned, have you considered the following: 
>
>DF <- data.frame(t.=c(1, 4, 16), Y=c(.8, .45, .04))
># I do NOT use "t" as a name, as it
># may conflict with the matrix transpose function.
>fit0 <- lm(log(Y)~t.-1, DF)
>fit0
>Call:
>lm(formula = log(Y) ~ t. - 1, data = DF)
>
>Coefficients:
>     t. 
>-0.2012 
>################
>
>      If this is the problem you really wanted to solve AND you honestly
>
>need NONLINEAR least squares, I would expect that (-0.2) should provide 
>a reasonable starting value for nls: 
>
> > fit1 <- nls(Y~exp(-THETA*t.), data=DF, start=c(THETA=-0.2))
> > fit1
>Nonlinear regression model
>  model:  Y ~ exp(-THETA * t.)
>   data:  DF
>    THETA
>0.2034489
> residual sum-of-squares:  0.0003018337
> >
>
>      2.  Alternatively, you could compute the sum of squares for all 
>values of THETA = seq(0, .01, 100) in a loop, then find the minimum by 
>eye. 
>
>      3.  If this is just a toy example, and your real problem has 
>several parameters, "expand.grid" will produce a grid, and you can 
>compute the value of your function and the sum of squares of residuals 
>at every point in the grid in a single loop, etc. 
>
>      hope this helps.  spencer graves
>
>WilDscOp wrote:
>
>  
>
>>Dear all,
>>
>>    I was wondering if there is any way i could do a "Grid Search" on 
>>a parameter space using R (as SAS 6.12 and higher can do it) to start 
>>the Newton-Gauss Linearization least squares method when i have NO 
>>prior information about the parameter.
>>W. N. Venables and B. D. Ripley (2002) "Modern Applied Statistics with
>>    
>>
>
>  
>
>>S", 4 th ed., page 216-7  has a topic "Self-starting non-linear 
>>regressions" using negexp.SSival - but i can not solve my hypothetical
>>    
>>
>
>  
>
>>problem using that - my problem is :
>>
>>Y = EXP(-(THETA * t)) with data below for estimating THETA:
>>
>>t        Y
>>1    0.80
>>4     0.45
>>16     0.04
>>
>>Whatever i could do, is in 
>>http://www.angelfire.com/ab5/get5/nonlinear.PDF
>>
>>Any response / help / comment / suggestion / idea / web-link / replies
>>    
>>
>
>  
>
>>will be greatly appreciated.
>>
>>Thanks in advance for your time.
>>
>>_______________________
>>
>>Mohammad Ehsanul Karim <wildscop at yahoo.com>
>>Institute of Statistical Research and Training
>>University of Dhaka, Dhaka- 1000, Bangladesh
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From tlumley at u.washington.edu  Thu Apr 15 17:05:35 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 15 Apr 2004 08:05:35 -0700 (PDT)
Subject: [R] all(logical(0)) and any(logical(0))
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0404150801280.34854@homer05.u.washington.edu>

On Thu, 15 Apr 2004, Liaw, Andy wrote:

> Dear R-help,
>
> I was bitten by the behavior of all() when given logical(0):  It is TRUE!
> (And any(logical(0)) is FALSE.)  Wouldn't it be better to return logical(0)
> in both cases?

No, it wouldn't.  The convention that "For all x in A: P(x)" is true
whenever A is empty and that "There exists x in A: P(x)$ is false is very
old.  It is also useful that all(x) && all(y) is necessarily the same as
all(x,y), which wouldn't be true under your suggestion.


The basic principle is that elementwise operations give zero-length
vectors for zero-length operands, but reducing operations that collapse a
vector to a scalar give a suitable scalar.

	-thomas


> The problem surfaced because some un-named individual called randomForest(x,
> y, xtest, ytest,...), and gave y as a two-level factor, but ytest as just
> numeric vector.  I thought I check for that in my code by testing for
>
> if (!all(levels(y) == levels(ytest))) stop(...)
>
> but levels() on a non-factor returns NULL, and the comparison ended up being
> logical(0).  Since all(logical(0)) is TRUE, the error is not flagged.
>
> Best,
> Andy
>
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> mailto:andy_liaw at merck.com        732-594-0820
>
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From mmarques at inescporto.pt  Thu Apr 15 17:06:50 2004
From: mmarques at inescporto.pt (MMarques Power)
Date: Thu, 15 Apr 2004 16:06:50 +0100
Subject: [R] Dates stuff ... beginner question
Message-ID: <1556043406.20040415160650@power.inescn.pt>



Putting it simly I have a date in format YYYYMMDD
Year month day
20030301
and I need simply the day of the week as an integer or even as
string...
according to the documentation in ISOdate or strptime functions
I need to format it ...

If I try something like :

strptime("20030301","%u")
[1] "2004-04-15"

being %u the format value I need
but as result I get the system actual date ??!?...

Using Windows XP and the latest R1.9.0
I need something like 1 for sunday and the sequent values for the rest
of the week.
What am I doing wrong...?
Any help is welcomed...

                      Thanks in advance
                      Marco Marques



From zelickr at pdx.edu  Thu Apr 15 17:08:14 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Thu, 15 Apr 2004 08:08:14 -0700 (PDT)
Subject: [R] basic programming question
In-Reply-To: <Pine.A41.4.58.0404150755000.34854@homer05.u.washington.edu>
Message-ID: <Pine.GSO.4.44.0404150758030.5621-100000@freke.odin.pdx.edu>


Hello list,

I am just starting to write some R functions (R 1.8 and Windows XP) and
got stuck as described below:

We have a bunch of time series data files, each is about 10,000 values.
There is a 12-line header in each one. I can read them and plot them
easily with R, but I want to make an automated system to read all the
files in a folder and have them appear as R objects that I can assemble in
different plots, or analyze together.

Perhaps this would be best done reading into a data table, where each row
is one of the files and each row extends for 10,000 columns? Or perhaps as
a matrix? I could use advice on that. My first notion, though, was just to
have them be individual files.

I started as follows:

#
# build a path from its parts:
#
p1="C:"
p2="Work-PSU"
p3="Lab-People-and-Projects"
p4="Dane"
p5="2004-04-06"
pt=file.path(p1,p2,p3,p4,p5,fsep="/")
#
lf<-list.files(pt)
ff="files"
sayfiles=paste(length(lf),ff,sep=" ")
#
# at this point if I type "sayfiles" I get "30 files" printed at the
console. OK, this is progress.
#
# Now I want to cycle through all the files and read in each one. The
# code here almost works, but I don't know how to do in R the simple task
# of substituting the read file name as the data object name.
#
d=dir(pt)
for (i in d){d[i]=scan(paste(pt,i,sep="/"),skip=12)} # this almost works
#

I know this is wrong, I just want to show my intention -- that is on each
iteration of the loop have a new R object made called d[i], the name of
the current file. This new object is then the result of the scan operation
on the file of that same name.

Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201


R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From dmurdoch at pair.com  Thu Apr 15 17:17:21 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 15 Apr 2004 11:17:21 -0400
Subject: [R] all(logical(0)) and any(logical(0))
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
Message-ID: <ki9t70p2deg9l3l6gtn9uvpu5kl1kudonj@4ax.com>

On Thu, 15 Apr 2004 10:40:41 -0400, "Liaw, Andy" <andy_liaw at merck.com>
wrote :

>Dear R-help,
>
>I was bitten by the behavior of all() when given logical(0):  It is TRUE!
>(And any(logical(0)) is FALSE.)  Wouldn't it be better to return logical(0)
>in both cases?

As Rolf said, this behaviour makes sense.

>The problem surfaced because some un-named individual called randomForest(x,
>y, xtest, ytest,...), and gave y as a two-level factor, but ytest as just
>numeric vector.  I thought I check for that in my code by testing for
>
>if (!all(levels(y) == levels(ytest))) stop(...)

You should fix this by having

y <- as.factor(y)

and 

ytest <- as.factor(ytest)

somewhere earlier in your code, since your code assumes that they are
both factors, but doesn't enforce that in its input checks.  

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Thu Apr 15 17:21:36 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 15 Apr 2004 17:21:36 +0200
Subject: [R] Dates stuff ... beginner question
In-Reply-To: <1556043406.20040415160650@power.inescn.pt>
References: <1556043406.20040415160650@power.inescn.pt>
Message-ID: <407EA880.2060909@statistik.uni-dortmund.de>

MMarques Power wrote:

> 
> Putting it simly I have a date in format YYYYMMDD
> Year month day
> 20030301
> and I need simply the day of the week as an integer or even as
> string...
> according to the documentation in ISOdate or strptime functions
> I need to format it ...
> 
> If I try something like :
> 
> strptime("20030301","%u")
> [1] "2004-04-15"
> 
> being %u the format value I need
> but as result I get the system actual date ??!?...
> 
> Using Windows XP and the latest R1.9.0
> I need something like 1 for sunday and the sequent values for the rest
> of the week.
> What am I doing wrong...?
> Any help is welcomed...

You are looking for, e.g.:

  format(strptime("20030301", "%Y%m%d"), "%A")


Note that the help says for format "%u":

" ... less widely implemented (e.g. not for output on Windows) ..."

Uwe Ligges



>                       Thanks in advance
>                       Marco Marques
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mmarques at inescporto.pt  Thu Apr 15 17:25:21 2004
From: mmarques at inescporto.pt (MMarques Power)
Date: Thu, 15 Apr 2004 16:25:21 +0100
Subject: [R] basic programming question
In-Reply-To: <Pine.GSO.4.44.0404150758030.5621-100000@freke.odin.pdx.edu>
References: <Pine.GSO.4.44.0404150758030.5621-100000@freke.odin.pdx.edu>
Message-ID: <867153828.20040415162521@power.inescn.pt>

Hello Randy,

Thursday, April 15, 2004, 4:08:14 PM, you wrote:

RZ> I am just starting to write some R functions (R 1.8 and Windows XP) and
RZ> got stuck as described below:
RZ> We have a bunch of time series data files, each is about 10,000 values.
RZ> There is a 12-line header in each one. I can read them and plot them
RZ> easily with R, but I want to make an automated system to read all the
RZ> files in a folder and have them appear as R objects that I can assemble in
RZ> different plots, or analyze together.
RZ> Perhaps this would be best done reading into a data table, where each row
RZ> is one of the files and each row extends for 10,000 columns? Or perhaps as
RZ> a matrix? I could use advice on that. My first notion, though, was just to
RZ> have them be individual files.
RZ> I started as follows:
RZ> # build a path from its parts:
RZ> p1="C:"
RZ> p2="Work-PSU"
RZ> p3="Lab-People-and-Projects"
RZ> p4="Dane"
RZ> p5="2004-04-06"
RZ> pt=file.path(p1,p2,p3,p4,p5,fsep="/")
RZ> #
RZ> lf<-list.files(pt)
RZ> ff="files"
RZ> sayfiles=paste(length(lf),ff,sep=" ")
RZ> #
RZ> # at this point if I type "sayfiles" I get "30 files" printed at the
RZ> console. OK, this is progress.
RZ> #
RZ> # Now I want to cycle through all the files and read in each one. The
RZ> # code here almost works, but I don't know how to do in R the simple task
RZ> # of substituting the read file name as the data object name.
RZ> #
RZ> d=dir(pt)
RZ> for (i in d){d[i]=scan(paste(pt,i,sep="/"),skip=12)} # this almost works
RZ> #
RZ> I know this is wrong, I just want to show my intention -- that is on each
RZ> iteration of the loop have a new R object made called d[i], the name of
RZ> the current file. This new object is then the result of the scan operation
RZ> on the file of that same name.


There are several aproaches to solve the problem

On that I use is to put each new file as a new object in the maisn env
of R.
something like this:

        zfil <- list.files(path, pattern = patt )
        for (ii in 1:length(zfil)  ) {
                f1path <- paste(path,"\\", zfil[ii], sep="")
                d1 <- scan(f1path, skip= 1,dec = dec1 , sep=";")
                assign(zfil[ii], d1, env= .GlobalEnv)
        }

so each time a new object is created with the name of the file.

Regarding the idea of putting all the data together a data frame would
be the answer...
Depending on the data it would be possible to make a matrix with some
thing like 100*100 and possibly concatenate the each matrix ...


-- 
Best regards,
 MMarques                            mailto:mmarques at power.inescn.pt



From bates at stat.wisc.edu  Thu Apr 15 17:34:35 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 15 Apr 2004 10:34:35 -0500
Subject: [R] all(logical(0)) and any(logical(0))
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7BF1@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7BF1@usrymx25.merck.com>
Message-ID: <6rsmf5jlv8.fsf@bates4.stat.wisc.edu>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> I wrote:
> 
> > I was bitten by the behavior of all() when given logical(0):  
> > It is TRUE!
> > (And any(logical(0)) is FALSE.)  Wouldn't it be better to 
> > return logical(0)
> > in both cases?
> 
> I guess the behavior is consistent with:
> 
> > prod(numeric(0))
> [1] 1
> > sum(numeric(0))
> [1] 0
> 
> but why?

The operation applied to a zero-length vector returns the identity
element of the operator.  The identity element of * is 1, of + is 0,
of & is TRUE and of | is FALSE.



From yukangtu at hotmail.com  Thu Apr 15 17:41:10 2004
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Thu, 15 Apr 2004 15:41:10 +0000
Subject: [R] how to cite a library
Message-ID: <BAY12-F63PRhzBLBYEp0004a554@hotmail.com>

Dear R users,

I used the multivariate random numbers generation function in MASS for my 
study.  I wonder what is an approriate way to cite the library and its 
authors in my publication to express my gratefulness?

best regards,

Yu-Kang



From andy_liaw at merck.com  Thu Apr 15 17:50:18 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 11:50:18 -0400
Subject: [R] all(logical(0)) and any(logical(0))
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BF2@usrymx25.merck.com>

Thanks to Rolf, Thomas, Duncan & Doug for the explanations!  It's one of
those things that I should have remembered from high school but clearly
didn't...

I've changed my code to:

[If y is factor:]
        if (!is.null(ytest)) {
            if (!is.factor(ytest)) stop("ytest must be a factor")
            if (!all(levels(y) == levels(ytest)))
                stop("y and ytest must have the same levels")
        }

For Duncan: `y' can be either factor or numeric.  That's how randomForest
determines whether it's a classification or regression problem.

Thanks again to all!

Andy

> From: Liaw, Andy
> 
> Dear R-help,
> 
> I was bitten by the behavior of all() when given logical(0):  
> It is TRUE!
> (And any(logical(0)) is FALSE.)  Wouldn't it be better to 
> return logical(0)
> in both cases?
> 
> The problem surfaced because some un-named individual called 
> randomForest(x,
> y, xtest, ytest,...), and gave y as a two-level factor, but 
> ytest as just
> numeric vector.  I thought I check for that in my code by testing for
> 
> if (!all(levels(y) == levels(ytest))) stop(...)
> 
> but levels() on a non-factor returns NULL, and the comparison 
> ended up being
> logical(0).  Since all(logical(0)) is TRUE, the error is not flagged.
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> mailto:andy_liaw at merck.com        732-594-0820
> 
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any 
> attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From andy_liaw at merck.com  Thu Apr 15 18:00:18 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 12:00:18 -0400
Subject: [R] how to cite a library
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BF4@usrymx25.merck.com>

The MASS package (not library) is part of the `VR' bundle, and support
software for MASS the book.  You should cite the book.

Andy

> From: Tu Yu-Kang
> 
> Dear R users,
> 
> I used the multivariate random numbers generation function in 
> MASS for my 
> study.  I wonder what is an approriate way to cite the 
> library and its 
> authors in my publication to express my gratefulness?
> 
> best regards,
> 
> Yu-Kang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Thu Apr 15 18:04:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 15 Apr 2004 16:04:42 +0000 (UTC)
Subject: [R] Solving Matrices
References: <3466.138.217.58.200.1082026323.squirrel@homebase.wehi.edu.au>
Message-ID: <loom.20040415T175921-700@post.gmane.org>

 <wettenhall <at> wehi.EDU.AU> writes:

: 
: On April 15th, Elizabeth wrote:
: <snip>
: >      In execises 39-42, determine if the columns of the matrix span
: >      R4:
: <snip>
: >(or x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5,
: >                        4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)
: >
: >That is the whole of the question <snip>
: 
: Have you tried det(x) and/or eigen(x) ?
: 
: A zero determinant (within computer precision) means that the matrix does
: not have full rank, i.e. it does not span R4.  

In this case we can avoid the computer precision problem by noting
that the determinant of an integer matrix is always integer, so:

   round(det(x))

will be non-zero iff the square integer matrix x is of full rank.



From maechler at stat.math.ethz.ch  Thu Apr 15 18:10:27 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Apr 2004 18:10:27 +0200
Subject: [R] tapply() and barplot() help files for 1.8.1
In-Reply-To: <m2smf5bh7p.fsf@ganymede.ammp.or.tz>
References: <m2smf5bh7p.fsf@ganymede.ammp.or.tz>
Message-ID: <200404151610.i3FGARbS004686@lynne.ethz.ch>

>>>>> "David" == David Whiting <david.whiting at ncl.ac.uk>
>>>>>     on 15 Apr 2004 11:42:18 +0000 writes:

    David> Hi,

    David> I've just upgraded to 1.9.0 and one of my Sweave
    David> files that produces a number of barplots in a
    David> standard manner now produces them in a different way.
    David> I have made a couple of small changes to my code to
    David> get the back the output I was getting before
    David> upgrading and now (mostly out of curiosity) would
    David> like to understand what has changed.

and I like to help you.
As I keep installed `(almost) all released versions of R ever
installed on our machines'
I can easily run 1.8.1 (or 1.4.x or 1.0.x ...) for you.

The only difference
 between the help page help(tapply)
is an extra   "require(stats)" statement at the beginning of the
`Examples' section in 1.9.0.

and the only change to  tapply() is 
    group <- rep.int(one, nx)#- to contain the splitting vector
instead of
    group <- rep    (one, nx)#- to contain the splitting vector

which hardly should have adverse results.

In barplot, there's the new 'offset' option  --- not in NEWS (!!!!)

and another change that may be a problem.

Can you dig harder and if possible provide a reproducible (small..)
example to make progress here...


    David> I *think* I've tracked it down to tapply() and/or
    David> barplot() and have not seen anything in the NEWS file
    David> regarding changes to these functions (as far a I can
    David> see).  As part of doing my homework, I would like to
    David> read the version 1.8.1 help files for these two
    David> functions, but now that I've upgraded I'm not sure
    David> where I can find them.  Is there a simple way for me
    David> to get copies of these two help files to compare with
    David> the versions in 1.9.0?  As far as I can see,
    David> barplot() and tapply() in 1.9.0 work as described in
    David> their 1.9.0 help files (which does not surprise me).

    David> I've been lurking on this list long enough to know
    David> that if there has been a change it is documented, so
    David> it must be that I just haven't found it yet.  If
    David> there hasn't been a change, then I am totally
    David> perplexed, because I have been running this Sweave
    David> file several times a day for the last few weeks and
    David> have not changed that part of it (I've been changing
    David> the LaTeX parts).

    David> In the part of the code that has changed I use
    David> tapply() to summarise some data and then plot it with
    David> barplot().  I now have to use matrix() on the output
    David> of tapply() before using barplot() because tapply()
    David> produces a list and barplot() wants a vector or
    David> matrix.

    David> In the code below, z is a dataframe, yllperdth is a
    David> numeric and fld is the name of a factor, both in the
    David> dataframe.

    David> Old version (as used with R 1.8.1):

    David>   ## Calculate the % of YLLs for each group in the
    David> cause classification.  x <- tapply(z$yllperdth, z[,
    David> fld], sum) totalYLLs <- sum(x) x <- x / totalYLLs *
    David> 100 x <- sort(x)
  
    David>   ## Plot the chart. horiz = TRUE makes it a bar
    David> instead of ## column chart.  las = 1 prints the
    David> labels horizontally.  xplot <- barplot(x, horiz =
    David> TRUE, xlab = "Percent of YLLs", las = 1)


    David> New Version (as used with R 1.9.0):

    David>   ## Calculate the % of YLLs for each group in the
    David> cause classification.  x <- tapply(z$yllperdth, z[,
    David> fld], sum) totalYLLs <- sum(x) x <- x / totalYLLs *
    David> 100 x <- sort(x)

    David>   causeNames <- names(x) ## NEW BIT x <- matrix(x) ##
    David> NEW BIT
  

    David>   ## Plot the chart. horiz = TRUE makes it a bar
    David> instead of ## column chart.  las = 1 prints the
    David> labels horizontally.  xplot <- barplot(x, beside =
    David> TRUE, ## NEW BIT names.arg = causeNames, ## NEW BIT
    David> horiz = TRUE, xlab = "Percent of YLLs", las = 1)




    >> version
    David>          _ platform i686-pc-linux-gnu arch i686 os
    David> linux-gnu system i686, linux-gnu status major 1 minor
    David> 9.0 year 2004 month 04 day 12 language R


    David> A little while before upgrading I noted my previous R
    David> version (for a post that I redrafted 7 times and
    David> never sent because I found the answer through
    David> refining my draft), and it was:

    >> version
    David>          _ platform i686-pc-linux-gnu arch i686 os
    David> linux-gnu system i686, linux-gnu status Patched major
    David> 1 minor 8.1 year 2004 month 02 day 16 language R

    David> So, can I get the old help files?  Or it is easy to
    David> point me to a documented change?  Or is it clear from
    David> my code what has changed or what I am or was doing
    David> wrong?

    David> Thanks.

    David> Dave

    David> -- David Whiting Dar es Salaam, Tanzania



From heberto.ghezzo at mcgill.ca  Thu Apr 15 18:16:03 2004
From: heberto.ghezzo at mcgill.ca (R. Heberto Ghezzo)
Date: Thu, 15 Apr 2004 12:16:03 -0400
Subject: [R] install.packages
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
Message-ID: <407EB542.653DA2C1@mcgill.ca>

Hello,
I just downloaded RW1090. No problems. My thanks to everybody involved in the
project. I work in Win98
I updated my library, some problems with some files that were in the PACKAGES list
but not in 1.9/ site, now all are.
I tried to install "Zelig" from Harvard
install.packages("Zelig",CRAN="http://gking.harvard.edu")
this worked in 1.8.1 but now it appends "bin/windows/contrib/1.9" to the address
and of course it can not find the file and aborts with error 404
Is there a way around besides http'ing directly to harvard and getting the zip and
unzipping it in /library?
Thanks
.
Heberto Ghezzo Ph.D.
McGill University
Montreal - Que - Canada



From phddas at yahoo.com  Thu Apr 15 18:24:11 2004
From: phddas at yahoo.com (Fred J.)
Date: Thu, 15 Apr 2004 09:24:11 -0700 (PDT)
Subject: [R] un-expected return by fdim
In-Reply-To: <407E9550.4020705@statistik.uni-dortmund.de>
Message-ID: <20040415162411.70943.qmail@web20515.mail.yahoo.com>

> what is fdim()?
a package from CRAN which calculate the fractal
dimension of datasets.
once you have it installed do
>library(fdim)
>help(fdim)

thanks

> 
>  > fdim
> Error: Object "fdim" not found
> 
> 
> Please, consider to follow the posting guide and
> help the readers of 
> this list to easily understand your questions.
> 
> Uwe Ligges



From p.dalgaard at biostat.ku.dk  Thu Apr 15 18:24:31 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Apr 2004 18:24:31 +0200
Subject: [R] install.packages
In-Reply-To: <407EB542.653DA2C1@mcgill.ca>
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
	<407EB542.653DA2C1@mcgill.ca>
Message-ID: <x24qrlky4g.fsf@biostat.ku.dk>

"R. Heberto Ghezzo" <heberto.ghezzo at mcgill.ca> writes:

> Hello,
> I just downloaded RW1090. No problems. My thanks to everybody involved in the
> project. I work in Win98
> I updated my library, some problems with some files that were in the PACKAGES list
> but not in 1.9/ site, now all are.
> I tried to install "Zelig" from Harvard
> install.packages("Zelig",CRAN="http://gking.harvard.edu")
> this worked in 1.8.1 but now it appends "bin/windows/contrib/1.9" to the address
> and of course it can not find the file and aborts with error 404
> Is there a way around besides http'ing directly to harvard and getting the zip and
> unzipping it in /library?

Well, you can http to harvard, get it and use "Install from local zip
file", which will get it registered correctly.

And, basically, if Gary King wants to pretend that his site is a CRAN
clone with exactly one package, it's up to him to supply the requisite
1.9 dir. It's not a problem with install.packages() that it barfs if
it gets a CRAN argument that isn't a CRAN mirror.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Thu Apr 15 18:30:08 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 12:30:08 -0400
Subject: [R] un-expected return by fdim
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BF6@usrymx25.merck.com>

> From: Fred J.
> 
> > what is fdim()?
> a package from CRAN which calculate the fractal
> dimension of datasets.
> once you have it installed do
> >library(fdim)
> >help(fdim)

That's not the point.

[Uwe said:]
> > Please, consider to follow the posting guide and
> > help the readers of 
> > this list to easily understand your questions.

That _is_ the point.
 
Andy



From maechler at stat.math.ethz.ch  Thu Apr 15 18:32:17 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 15 Apr 2004 18:32:17 +0200
Subject: [R] install.packages
In-Reply-To: <407EB542.653DA2C1@mcgill.ca>
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
	<407EB542.653DA2C1@mcgill.ca>
Message-ID: <16510.47377.14519.265974@gargle.gargle.HOWL>

>>>>> "R" == R Heberto Ghezzo <heberto.ghezzo at mcgill.ca>
>>>>>     on Thu, 15 Apr 2004 12:16:03 -0400 writes:

    R> Hello, I just downloaded RW1090. No problems. My thanks
    R> to everybody involved in the project. I work in Win98 I
    R> updated my library, some problems with some files that
    R> were in the PACKAGES list but not in 1.9/ site, now all
    R> are.  I tried to install "Zelig" from Harvard
    R> install.packages("Zelig",CRAN="http://gking.harvard.edu")
    R> this worked in 1.8.1 but now it appends
    R> "bin/windows/contrib/1.9" to the address and of course it
    R> can not find the file and aborts with error 404 Is there
    R> a way around besides http'ing directly to harvard and
    R> getting the zip and unzipping it in /library?  

To my astonishment, it actually works for me (in Linux)
with the source package...

OTOH, gking.harvard.edu is not really a CRAN mirror, and "GKing"
has probably not yet created the  bin/windows/contrib/1.9/
directory and its contents.
As a matter of fact, I believe he should rather submit his
package to CRAN proper and you and he wouldn't have to deal with
such things as creating now directories when a the version of R
is bumped up.

As he says on the Zelig web pages, the easy alternative is to
download the zip file and install that from the Package menu on
Windows.

Regards,
Martin



From dmurdoch at pair.com  Thu Apr 15 18:45:24 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 15 Apr 2004 12:45:24 -0400
Subject: [R] tapply() and barplot() help files for 1.8.1
In-Reply-To: <200404151610.i3FGARbS004686@lynne.ethz.ch>
References: <m2smf5bh7p.fsf@ganymede.ammp.or.tz>
	<200404151610.i3FGARbS004686@lynne.ethz.ch>
Message-ID: <jkdt70leiiv1qd3ujcr1mpc5fjpid2uh50@4ax.com>

On Thu, 15 Apr 2004 18:10:27 +0200, Martin Maechler
<maechler at stat.math.ethz.ch> wrote :

>>>>>> "David" == David Whiting <david.whiting at ncl.ac.uk>
>>>>>>     on 15 Apr 2004 11:42:18 +0000 writes:
>
>    David> Hi,
>
>    David> I've just upgraded to 1.9.0 and one of my Sweave
>    David> files that produces a number of barplots in a
>    David> standard manner now produces them in a different way.
>    David> I have made a couple of small changes to my code to
>    David> get the back the output I was getting before
>    David> upgrading and now (mostly out of curiosity) would
>    David> like to understand what has changed.
>
>and I like to help you.
>As I keep installed `(almost) all released versions of R ever
>installed on our machines'
>I can easily run 1.8.1 (or 1.4.x or 1.0.x ...) for you.
>
>The only difference
> between the help page help(tapply)
>is an extra   "require(stats)" statement at the beginning of the
>`Examples' section in 1.9.0.
>
>and the only change to  tapply() is 
>    group <- rep.int(one, nx)#- to contain the splitting vector
>instead of
>    group <- rep    (one, nx)#- to contain the splitting vector
>
>which hardly should have adverse results.
>
>In barplot, there's the new 'offset' option  --- not in NEWS (!!!!)
>
>and another change that may be a problem.

Here's a reproducible bug in barplot in 1.9.0 (based on an email I got
this morning from Richard Rowe):

x <- table(rep(1:5,1:5))
barplot(x)

The problem is that table() produces a one dimensional array, and
barplot() doesn't handle those properly now.  The offending line is
this one:

$ cvs diff -r 1.3 barplot.R
[junk deleted] 
43c43
<       width <- rep(width, length.out = NR * NC)
---
>       width <- rep(width, length.out = NR)

In the example above, x gets turned into a matrix with NR=1 row and
NC=5 columns so only one bar width gets set.

Duncan Murdoch



From lizzy at noradd.org  Thu Apr 15 18:05:21 2004
From: lizzy at noradd.org (etb)
Date: 15 Apr 2004 11:05:21 -0500
Subject: [R] Solving Matrices
In-Reply-To: wettenhall@wehi.EDU.AU's message of "Thu,
	15 Apr 2004 20:52:03 +1000 (EST)"
References: <3466.138.217.58.200.1082026323.squirrel@homebase.wehi.edu.au>
Message-ID: <87pta9jkfy.fsf@liliwhite.renaissance.oasis>

James writes:

> Have you tried det(x) and/or eigen(x) ?
> 
> A zero determinant (within computer precision) means that the matrix
> does not have full rank, i.e. it does not span R4.  Count how many
> eigenvalues are zero (within computer precision).  What does this
> tell you?

I'm still on chapter 1 and we have not yet covered eigenvalues so this
is a bit fuzzy.

> >    In the Solutions Manual, there is mention of the gauss() and
> >bgauss() functions which apparently written by Lay - these are to
> >speed up matrix reduction but I have not noticed these functions in R.
> 
> Have you encountered speed problems in R?  Do you really need these
> functions in R in addition to solve(), backsolve() etc. ?  If you
> just want to learn about how they work you could have a look at the
> Matlab code for these functions (which I think you have access to).
> You could even try rewriting them in R yourself.

No, the speed is fine but I'm in early Linear Algebra and am working
with basic matrices right now. I do have access to the author's
functions and I may try re-writing those in R - that should be a good
project and should help me better understand R as I'm a novice.

Thank you,
Elizabeth



From rpeng at jhsph.edu  Thu Apr 15 19:07:03 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 15 Apr 2004 13:07:03 -0400
Subject: [R] how to cite a library
In-Reply-To: <BAY12-F63PRhzBLBYEp0004a554@hotmail.com>
References: <BAY12-F63PRhzBLBYEp0004a554@hotmail.com>
Message-ID: <407EC137.9060702@jhsph.edu>

Take a look at this thread:

https://www.stat.math.ethz.ch/pipermail/r-help/2004-February/044162.html

In general, it depends, but for MASS you should probably cite the book 
by Venables & Ripley.

-roger

Tu Yu-Kang wrote:
> Dear R users,
> 
> I used the multivariate random numbers generation function in MASS for 
> my study.  I wonder what is an approriate way to cite the library and 
> its authors in my publication to express my gratefulness?
> 
> best regards,
> 
> Yu-Kang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From martinsrui at prof2000.pt  Thu Apr 15 19:26:54 2004
From: martinsrui at prof2000.pt (Rui  Martins )
Date: Thu, 15 Apr 2004 18:26:54 +0100
Subject: [R] I do not achieve to install some packages
Message-ID: <200404151826.AA12059146@mail.prof2000.pt>

I can't load the packages locfit, lokern and quantreg.
When I try to load the 3 packages the R program says that the pakages are not properly installed. He says to see the note in ?library.
I think that the error is because there is no built field in the 3 packages.
What can I do to make the packages run.
My OS is Windows XP.

Thank you

Rui Martins



From pburns at pburns.seanet.com  Thu Apr 15 20:20:09 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 15 Apr 2004 19:20:09 +0100
Subject: [R] all(logical(0)) and any(logical(0))
References: <3A822319EB35174CA3714066D590DCD504AF7BF1@usrymx25.merck.com>
	<6rsmf5jlv8.fsf@bates4.stat.wisc.edu>
Message-ID: <407ED259.1060302@pburns.seanet.com>

I suspect that some people still might not have caught why
the behavior is a good thing.

We want

any(c(A, B))

to give the same answer as

any(A) || any(B)

This should be the behavior even if all of the elements are
in one of the vectors.

This actually is useful in coding, though I can't think of any
specific instances at the moment.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Douglas Bates wrote:

>"Liaw, Andy" <andy_liaw at merck.com> writes:
>
>  
>
>>I wrote:
>>
>>    
>>
>>>I was bitten by the behavior of all() when given logical(0):  
>>>It is TRUE!
>>>(And any(logical(0)) is FALSE.)  Wouldn't it be better to 
>>>return logical(0)
>>>in both cases?
>>>      
>>>
>>I guess the behavior is consistent with:
>>
>>    
>>
>>>prod(numeric(0))
>>>      
>>>
>>[1] 1
>>    
>>
>>>sum(numeric(0))
>>>      
>>>
>>[1] 0
>>
>>but why?
>>    
>>
>
>The operation applied to a zero-length vector returns the identity
>element of the operator.  The identity element of * is 1, of + is 0,
>of & is TRUE and of | is FALSE.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From jzhang10 at uic.edu  Thu Apr 15 21:24:04 2004
From: jzhang10 at uic.edu (jzhang10)
Date: Thu, 15 Apr 2004 14:24:04 -0500
Subject: [R] filled.contour
Message-ID: <4092E9E0@webmail.uic.edu>

Hi,
I want to draw a level plot. The levels are not evenly spaced, so I did 
something like: levels=c(0,2,5,10,30,60). I still want the color bar (key) on 
the right side to be evenly spaced so that the small numbers (0,2,5) are not 
squeezed together.
Does anyone know how to do it?
Thanks!
Jinfeng



From macq at llnl.gov  Thu Apr 15 21:51:48 2004
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 15 Apr 2004 12:51:48 -0700
Subject: [R] basic programming question
In-Reply-To: <Pine.GSO.4.44.0404150758030.5621-100000@freke.odin.pdx.edu>
References: <Pine.GSO.4.44.0404150758030.5621-100000@freke.odin.pdx.edu>
Message-ID: <p06002000bca493696d2d@[128.115.153.6]>

There are indeed a number of ways to do this.

If all have exactly the same number of values, and they have the same 
times, then a matrix or dataframe would be a reasonable place to 
store them. In that case I would have one column per file.

Otherwise, I would tend to use a list.

Here is a toy illustration.

>  d <- list()
>  d[[1]] <- 1:10
>  d[[2]] <- 1:20
>  names(d) <- paste('file',1:2,sep='')
>  d
$file1
  [1]  1  2  3  4  5  6  7  8  9 10

$file2
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20

>  plot(d$file1)   # or whatever makes sense to do with the data from one file

For your situation, the loop would look something like this.

lf<-list.files(pt)
d <- list()
for (i in seq(lf)) {
   d[[i]] <- scan(paste(pt,lf[i],sep="/"),skip=12)
}
names(d) <- lf

Then the i'th element of d is the data from the i'th file in your 
list of files.

If it makes sense, you can do things like
   d[[5]] - d[[2]
to subtract the values in the 2nd file from those in the 5th file, for example.

-Don

At 8:08 AM -0700 4/15/04, Randy Zelick wrote:
>Hello list,
>
>I am just starting to write some R functions (R 1.8 and Windows XP) and
>got stuck as described below:
>
>We have a bunch of time series data files, each is about 10,000 values.
>There is a 12-line header in each one. I can read them and plot them
>easily with R, but I want to make an automated system to read all the
>files in a folder and have them appear as R objects that I can assemble in
>different plots, or analyze together.
>
>Perhaps this would be best done reading into a data table, where each row
>is one of the files and each row extends for 10,000 columns? Or perhaps as
>a matrix? I could use advice on that. My first notion, though, was just to
>have them be individual files.
>
>I started as follows:
>
>#
># build a path from its parts:
>#
>p1="C:"
>p2="Work-PSU"
>p3="Lab-People-and-Projects"
>p4="Dane"
>p5="2004-04-06"
>pt=file.path(p1,p2,p3,p4,p5,fsep="/")
>#
>lf<-list.files(pt)
>ff="files"
>sayfiles=paste(length(lf),ff,sep=" ")
>#
># at this point if I type "sayfiles" I get "30 files" printed at the
>console. OK, this is progress.
>#
># Now I want to cycle through all the files and read in each one. The
># code here almost works, but I don't know how to do in R the simple task
># of substituting the read file name as the data object name.
>#
>d=dir(pt)
>for (i in d){d[i]=scan(paste(pt,i,sep="/"),skip=12)} # this almost works
>#
>
>I know this is wrong, I just want to show my intention -- that is on each
>iteration of the loop have a new R object made called d[i], the name of
>the current file. This new object is then the result of the scan operation
>on the file of that same name.
>
>Thanks,
>
>=Randy=
>
>R. Zelick				email: zelickr at pdx.edu
>Department of Biology			voice: 503-725-3086
>Portland State University		fax:   503-725-3888
>
>mailing:
>P.O. Box 751
>Portland, OR 97207
>
>shipping:
>1719 SW 10th Ave, Room 246
>Portland, OR 97201
>
>
>R. Zelick				email: zelickr at pdx.edu
>Department of Biology			voice: 503-725-3086
>Portland State University		fax:   503-725-3888
>
>mailing:
>P.O. Box 751
>Portland, OR 97207
>
>shipping:
>1719 SW 10th Ave, Room 246
>Portland, OR 97201
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From jasont at indigoindustrial.co.nz  Thu Apr 15 22:03:42 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 16 Apr 2004 08:03:42 +1200
Subject: [R] how to add legend to time series plot
In-Reply-To: <407E6CC7.9010409@statistik.uni-dortmund.de>
References: <1081949041.407d3b7199414@kurla.upf.edu>
	<407E6CC7.9010409@statistik.uni-dortmund.de>
Message-ID: <407EEA9E.30400@indigoindustrial.co.nz>

Uwe Ligges wrote:
> 
> I guess it is clipped in this case. Use par("usr") after ts.plot() to 
> see what the user coordinates are, and whether (1, -0.3) is a sensible 
> location to place the legend.
> 
> Uwe Ligges

Just to add to Uwe's good suggestion -- what I usually do is

opar <- par(no.readonly=TRUE)
par(usr=c(0,1,0,1))

Then pick legend co-ordinates, knowing that x and y on the plot canvas 
now range from 0 to 1.  For somewhere near the top-right corner...

legend(0.8,0.8, ...)
par(opar)  ## when done with the plot, to restore the co-ords.

Another choice is instead of the "par" calls...

legend(locator(1), ...)

and click on the plot where you want the legend.

Thirdly, the Hmisc package has a function largest.empty(), which can 
find the "best" place to put a legend in a plot automagically.

Cheers

Jason



From k.wang at auckland.ac.nz  Thu Apr 15 22:17:29 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 16 Apr 2004 08:17:29 +1200
Subject: [R] I do not achieve to install some packages
In-Reply-To: <200404151826.AA12059146@mail.prof2000.pt>
Message-ID: <20040415201737.BNBL6422.mta3-rme.xtra.co.nz@kevinlpt>

Hi,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> I can't load the packages locfit, lokern and quantreg.
> When I try to load the 3 packages the R program says that the
> pakages are not properly installed. He says to see the note
> in ?library.
> I think that the error is because there is no built field in
> the 3 packages.
> What can I do to make the packages run.

Can you tell us how you installed it?  It would help.  Also which
version of R are you using?

If you installed it within Rgui, by going to Packages -> Install
packages from CRAN..., then it should definitely work.

Cheers,

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
SLC Stats Workshops Co-ordinator
The University of Auckland
New Zealand



From elvis at xlsolutions-corp.com  Thu Apr 15 23:14:37 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Thu, 15 Apr 2004 14:14:37 -0700
Subject: [R] Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 locations near you!
Message-ID: <20040415211437.23054.qmail@webmail-2-5.mesa1.secureserver.net>



From bromberg at foo.fh-furtwangen.de  Thu Apr 15 23:25:29 2004
From: bromberg at foo.fh-furtwangen.de (H. Bromberger)
Date: Thu, 15 Apr 2004 23:25:29 +0200
Subject: [R] sort boxplot to median
Message-ID: <407EFDC9.1000206@foo.fh-furtwangen.de>

Dear guRus,
I'm stuck and really would appreciate some help. I've already crawled the net...
I want to do some Boxplot which are sorted by the median and not alphabetically.
What I did so far:
x <- subset(mydata, Verwalt.Doku==1, select=c(1, 2))
P <- plot(x[,1], x[,2], plot=F)
???sort(P$stats[3,])???
bxp(P, col="yellow", las=1, horizontal=T, xlab="Potential")

Of course it sorts the vector P$stats[3,] but not the rest of the list.
Thanks in advance, and sorry but I am new to R

Hubertus



From frohde_home at yahoo.com  Thu Apr 15 23:36:52 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Thu, 15 Apr 2004 14:36:52 -0700 (PDT)
Subject: [R] Complex sample variances
In-Reply-To: <Pine.A41.4.58.0404150755000.34854@homer05.u.washington.edu>
Message-ID: <20040415213652.51525.qmail@web60101.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040415/638f98c9/attachment.pl

From arcane at arcanemethods.com  Fri Apr 16 00:47:01 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Thu, 15 Apr 2004 15:47:01 -0700
Subject: [R] Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 locations near you!
In-Reply-To: <20040415211437.23054.qmail@webmail-2-5.mesa1.secureserver.net>
References: <20040415211437.23054.qmail@webmail-2-5.mesa1.secureserver.net>
Message-ID: <407F10E5.4020702@arcanemethods.com>

Was there supposed to be any content to this post?  It 
doesn't bode well for the course.  :-)


Bob

elvis at xlsolutions-corp.com wrote:

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 

"Things should be described as simply as possible, but no 
simpler."

                                              A. Einstein



From p.dalgaard at biostat.ku.dk  Fri Apr 16 00:51:56 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Apr 2004 00:51:56 +0200
Subject: [R] Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 locations near you!
In-Reply-To: <407F10E5.4020702@arcanemethods.com>
References: <20040415211437.23054.qmail@webmail-2-5.mesa1.secureserver.net>
	<407F10E5.4020702@arcanemethods.com>
Message-ID: <x2ad1csvlf.fsf@biostat.ku.dk>

Bob Cain <arcane at arcanemethods.com> writes:

> Was there supposed to be any content to this post?  It doesn't bode
> well for the course.  :-)

I was actually reading it with some curiosity as to how they managed
to find 5 locations that were close to everyone on R-help...

> "Things should be described as simply as possible, but no simpler."
> 
>                                               A. Einstein

Quite an appropriate quotation, I must say. :-)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ok at cs.otago.ac.nz  Fri Apr 16 02:08:51 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 16 Apr 2004 12:08:51 +1200 (NZST)
Subject: [R] Solving Matrices
Message-ID: <200404160008.i3G08pYO261268@atlas.otago.ac.nz>

Elizabeth (etb <lizzy at noradd.org>) wrote:
	     In execises 39-42, determine if the columns of the matrix span
	     R4:
                   4	
Presumably that's R, 4-dimensional real space.

	(or x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5, 
	                       4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)
	
	That is the whole of the question and I suppose that the way to answer
	this is by determining if:
	
		1. For each b in Rm, the equation Ax = b has a solution, or
		2. A has a pivot position in every row,
	
	where A is an (m X n) matrix.
	
Another way to look at this is that it's a question about the rank of the
matrix.  The rank of an mxn matrix is at most min(m,n).  Unfortunately,
help("rank") doesn't tell you about matrix rank, but about something else.

Possibly the simplest method is to look at eign(x, only.values=TRUE)
and see how many of the eigenvalues are non-zero.

> eigen(x, only.values=TRUE)
$values
[1]  9.275635e+00+8.169494i  9.275635e+00-8.169494i -1.551270e+00+0.000000i
[4] -1.603246e-14+0.000000i

$vectors
NULL

We see that this matrix has a pair of conjugate eigenvalues
    9.28 +/- 8.17 i
and two real eigenvalues
    -1.55
    -1.6e-14

The smallest eigenvalue is pretty close to 0
Try again, this time asking for the eigenvectors:

    vv <- eigen(x)$vectors

Check what happens when you multiply the eigenvector corresponding to
the smallest eigenvalue by the matrix:

    > print(v1 <- as.real(vv[,4]))
    [1] -0.4172502  0.2377877 -0.8479600 -0.2243281
    > print(v2 <- as.vector(x %*% v1))
    [1]  4.884981e-15 -2.664535e-15  8.437695e-15 -5.329071e-15

So x doesn't _precisely_ map that vector to 0, but it's close enough
for government work.

For another look at this, try the singular-value decomposition.
?svd

    > svd(x)$d
    [1] 2.436185e+01 1.376648e+01 6.163148e+00 9.241655e-16

Again, we see a pretty strong hint that the matrix is close to rank 3.

Finally, look at help(qr).  This is the most direct way of finding the
rank of a matrix.

    > qr(x)$rank
    [1] 3

So here are three different functions all saying much the same thing
about x:  it is numerically close to a matrix which does NOT span the
whole of R**4.



From sundar.dorai-raj at PDF.COM  Fri Apr 16 02:27:42 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 15 Apr 2004 19:27:42 -0500
Subject: [R] sort boxplot to median
In-Reply-To: <407EFDC9.1000206@foo.fh-furtwangen.de>
References: <407EFDC9.1000206@foo.fh-furtwangen.de>
Message-ID: <407F287E.3020802@pdf.com>



H. Bromberger wrote:
> Dear guRus,
> I'm stuck and really would appreciate some help. I've already crawled 
> the net...
> I want to do some Boxplot which are sorted by the median and not 
> alphabetically.
> What I did so far:
> x <- subset(mydata, Verwalt.Doku==1, select=c(1, 2))
> P <- plot(x[,1], x[,2], plot=F)
> ???sort(P$stats[3,])???
> bxp(P, col="yellow", las=1, horizontal=T, xlab="Potential")
> 
> Of course it sorts the vector P$stats[3,] but not the rest of the list.
> Thanks in advance, and sorry but I am new to R
> 
> Hubertus
> 

Hi Hubertus,

To make sure bxp (or boxplot) uses the order you want, then the split 
variable must be ordered (see ?ordered). So order the split variable by 
median. E.g.,

R> set.seed(1)
R> z <- data.frame(x = rep(LETTERS[1:3], each = 6), y = rnorm(18))
R> tapply(z$y, z$x, median)
           A           B           C
-0.22140524  0.53160520 -0.03056194
R> z$x <- with(z, ordered(x, levels(x)[order(tapply(y, x, median))]))
R> tapply(z$y, z$x, median)
           A           C           B
-0.22140524 -0.03056194  0.53160520
R> boxplot(y ~ x, data = z)

If you want the descending order, then see ?rev or put a minus sign in 
front of the tapply call.

HTH,

--sundar



From ok at cs.otago.ac.nz  Fri Apr 16 02:42:08 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 16 Apr 2004 12:42:08 +1200 (NZST)
Subject: [R] all(logical(0)) and any(logical(0))
Message-ID: <200404160042.i3G0g8p9262057@atlas.otago.ac.nz>

"Liaw, Andy" <andy_liaw at merck.com> wrote:
	I was bitten by the behavior of all() when given logical(0): It
	is TRUE!  (And any(logical(0)) is FALSE.)  Wouldn't it be better
	to return logical(0) in both cases?
	
It would be disastrous.  For all integer n >= 0,
	all(integer(n) == integer(n))		=> TRUE
	any(integer(n) != integer(n))		=> FALSE
Your proposal would give wrong answers for n == 0.

For any simple array (who knows what an arbitrary object will do?)
we expect all(x == x) => TRUE, any(x != x) => FALSE.  If this were
changed for empty x, we'd never be able to trust any() or all() again.

Find a book about logic and read how bounded quantification
(\forall x \in set) p(x)
(\exists x \in set) p(x)
is supposed to work when the set is empty.



From jasont at indigoindustrial.co.nz  Fri Apr 16 03:00:50 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 16 Apr 2004 13:00:50 +1200
Subject: [R] Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 locations near you!
In-Reply-To: <x2ad1csvlf.fsf@biostat.ku.dk>
References: <20040415211437.23054.qmail@webmail-2-5.mesa1.secureserver.net>	<407F10E5.4020702@arcanemethods.com>
	<x2ad1csvlf.fsf@biostat.ku.dk>
Message-ID: <407F3042.9030007@indigoindustrial.co.nz>

Peter Dalgaard wrote:
> I was actually reading it with some curiosity as to how they managed
> to find 5 locations that were close to everyone on R-help...

"Close, for sufficiently large values of 'close'..."

:D
Jason



From lindaportman at yahoo.com  Fri Apr 16 03:20:38 2004
From: lindaportman at yahoo.com (Linda portman)
Date: Thu, 15 Apr 2004 18:20:38 -0700 (PDT)
Subject: [R] Distributed lag model
Message-ID: <20040416012038.57553.qmail@web61307.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040415/4f0dc6e5/attachment.pl

From Toby.Patterson at csiro.au  Fri Apr 16 03:32:51 2004
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Fri, 16 Apr 2004 11:32:51 +1000
Subject: [R] Turning windows screen buffering on and off
Message-ID: <C4178DC99E08604EA5E2BDB989F093802420FE@extas2-hba.tas.csiro.au>


All, 

Does anyone know if there is an option I can set to turn screen-buffered
output on and off with the win32 rgui? (Apart from the point and click
method). 

I am running some simulations where it is useful to watch output but it
gets mildly tiresome having to manually switch things on and off via the
gui. 

Thanks 

Toby. 

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    8.1            
year     2003           
month    11             
day      21             
language R



From jasont at indigoindustrial.co.nz  Fri Apr 16 03:32:55 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 16 Apr 2004 13:32:55 +1200 (NZST)
Subject: [R] Distributed lag model
In-Reply-To: <20040416012038.57553.qmail@web61307.mail.yahoo.com>
References: <20040416012038.57553.qmail@web61307.mail.yahoo.com>
Message-ID: <32302.203.9.176.60.1082079175.squirrel@webmail.maxnet.co.nz>

>
> Has anyone writtent an R function for estimating linear
> models with distributed lags(using matrix algebra)?
>
> Y(t) = Bo + B1Xt-1+ B2Xt-2+e
>

The "dse" bundle, with libraries "dse1" and "dse2" have functions for this
in VARX or state space form.

Cheers

Jason



From sakshaug at u.washington.edu  Fri Apr 16 03:51:01 2004
From: sakshaug at u.washington.edu (Joseph Sakshaug)
Date: Thu, 15 Apr 2004 18:51:01 -0700 (PDT)
Subject: [R] Removing values from a vector
Message-ID: <Pine.A41.4.58.0404151805280.34348@dante50.u.washington.edu>


Dear R-help faithful,

I am trying to build a program which will take repeated samples (w/o
replacement) from a population of values. The interesting catch is that I
would like the sample values to be removed from the population, after each
sample is taken.

For example:

pop<-1:10

sample(pop, 2) = lets say, (3, 7)

## This is where I would like values (3, 7) to be removed from the
population vector, giving a new "current" population vector:

"new" pop = [1, 2, 4, 5, 6, 8, 9, 10]
and has length 8 instead of 10.


At first I tried to run a loop and comparison of (sample.pop == pop) and
IF a FALSE was found the loop would store the [ith] value of sample.pop
in a new vector called pop.current. Then, due to my beginning programming
skills, I kind of got stuck linking pop.current to the population which is
to be sampled from...


I know that in PHP there is a function called unset() which removes values
from an array. Does R have an equivalent function?

Thanks in advance.

Joe



From andy_liaw at merck.com  Fri Apr 16 03:55:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 15 Apr 2004 21:55:24 -0400
Subject: [R] Removing values from a vector
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7BFC@usrymx25.merck.com>

Use negative indexing; e.g., pop <- pop[-c(3, 7)] removes the 3rd and 7th
element of pop.

Andy

> From: Joseph Sakshaug
> 
> Dear R-help faithful,
> 
> I am trying to build a program which will take repeated samples (w/o
> replacement) from a population of values. The interesting 
> catch is that I
> would like the sample values to be removed from the 
> population, after each
> sample is taken.
> 
> For example:
> 
> pop<-1:10
> 
> sample(pop, 2) = lets say, (3, 7)
> 
> ## This is where I would like values (3, 7) to be removed from the
> population vector, giving a new "current" population vector:
> 
> "new" pop = [1, 2, 4, 5, 6, 8, 9, 10]
> and has length 8 instead of 10.
> 
> 
> At first I tried to run a loop and comparison of (sample.pop 
> == pop) and
> IF a FALSE was found the loop would store the [ith] value of 
> sample.pop
> in a new vector called pop.current. Then, due to my beginning 
> programming
> skills, I kind of got stuck linking pop.current to the 
> population which is
> to be sampled from...
> 
> 
> I know that in PHP there is a function called unset() which 
> removes values
> from an array. Does R have an equivalent function?
> 
> Thanks in advance.
> 
> Joe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lindaportman at yahoo.com  Fri Apr 16 04:08:48 2004
From: lindaportman at yahoo.com (Linda portman)
Date: Thu, 15 Apr 2004 19:08:48 -0700 (PDT)
Subject: [R] install packages
Message-ID: <20040416020848.59235.qmail@web61301.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040415/0296c533/attachment.pl

From jasont at indigoindustrial.co.nz  Fri Apr 16 04:22:10 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 16 Apr 2004 14:22:10 +1200 (NZST)
Subject: [R] install packages
In-Reply-To: <20040416020848.59235.qmail@web61301.mail.yahoo.com>
References: <20040416020848.59235.qmail@web61301.mail.yahoo.com>
Message-ID: <11810.203.9.176.60.1082082130.squirrel@webmail.maxnet.co.nz>

> I am trying to install packages, WHat i did is:
>
> options(CRAN= "http://cran.us.r-project.org/")
> install.packages("pakgs")

Is that really the package name?  I can't find it on CRAN.

> However I got error message like:
...
> argument `lib' is missing: using C:/PROGRA~1/R/rw1051/library in:
                                                 ^^^^^^
You need to upgrade R.  The latest version is 1.9.0. Version 1.5.1 (yours)
is about two years out of date. Download and install the latest one from
your nearest friendly CRAN mirror.

Cheers

Jason



From sundar.dorai-raj at PDF.COM  Fri Apr 16 04:23:07 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 15 Apr 2004 21:23:07 -0500
Subject: [R] install packages
In-Reply-To: <20040416020848.59235.qmail@web61301.mail.yahoo.com>
References: <20040416020848.59235.qmail@web61301.mail.yahoo.com>
Message-ID: <407F438B.3050006@pdf.com>



Linda portman wrote:
> I am trying to install packages, WHat i did is: 
>  
> options(CRAN= "http://cran.us.r-project.org/")
> install.packages("pakgs")
>  
> However I got error message like:
> trying URL `http://cran.us.r-project.org//bin/windows/contrib/PACKAGES'
> Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
>         cannot open: HTTP status was `404'
> In addition: Warning message: 
> argument `lib' is missing: using C:/PROGRA~1/R/rw1051/library in: install.packages("modreg")
                                                  ^^^^^^
Time to upgrade.

> Can you tell me why?
> 
>  

For 1.9.0, modreg was merged with stats, so is thus a defunct package.

--sundar



From rpeng at jhsph.edu  Fri Apr 16 05:10:43 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 15 Apr 2004 23:10:43 -0400
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <C4178DC99E08604EA5E2BDB989F093802420FE@extas2-hba.tas.csiro.au>
References: <C4178DC99E08604EA5E2BDB989F093802420FE@extas2-hba.tas.csiro.au>
Message-ID: <407F4EB3.4010003@jhsph.edu>

Ctrl-W.

-roger

Toby.Patterson at csiro.au wrote:
> All, 
> 
> Does anyone know if there is an option I can set to turn screen-buffered
> output on and off with the win32 rgui? (Apart from the point and click
> method). 
> 
> I am running some simulations where it is useful to watch output but it
> gets mildly tiresome having to manually switch things on and off via the
> gui. 
> 
> Thanks 
> 
> Toby. 
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lindaportman at yahoo.com  Fri Apr 16 05:18:58 2004
From: lindaportman at yahoo.com (Linda portman)
Date: Thu, 15 Apr 2004 20:18:58 -0700 (PDT)
Subject: [R] estimate distributed lag model using matrix algebra
Message-ID: <20040416031858.3985.qmail@web61309.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040415/39dc84c7/attachment.pl

From Toby.Patterson at csiro.au  Fri Apr 16 06:38:01 2004
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Fri, 16 Apr 2004 14:38:01 +1000
Subject: [R] Turning windows screen buffering on and off
Message-ID: <C4178DC99E08604EA5E2BDB989F0938001E07868@extas2-hba.tas.csiro.au>

I meant via a function or something like:  

options( buffered.output = FALSE)

Sorry, I should have made that clearer. 

Cheers 
Toby 



-----Original Message-----
From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
Sent: Friday, April 16, 2004 1:11 PM
To: Patterson, Toby (Marine, Hobart)
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Turning windows screen buffering on and off

Ctrl-W.

-roger

Toby.Patterson at csiro.au wrote:
> All, 
> 
> Does anyone know if there is an option I can set to turn
screen-buffered
> output on and off with the win32 rgui? (Apart from the point and click
> method). 
> 
> I am running some simulations where it is useful to watch output but
it
> gets mildly tiresome having to manually switch things on and off via
the
> gui. 
> 
> Thanks 
> 
> Toby. 
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From wettenhall at wehi.edu.au  Fri Apr 16 07:18:21 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Fri, 16 Apr 2004 15:18:21 +1000 (EST)
Subject: [R] Passing a pointer to .C() in Win32
In-Reply-To: <Pine.LNX.4.58.0404142353120.23757@unix28.alpha.wehi.edu.au>
References: <Pine.LNX.4.58.0404142353120.23757@unix28.alpha.wehi.edu.au>
Message-ID: <Pine.GSO.4.58.0404161512160.23951@unix33.alpha.wehi.edu.au>

I sent this to R-help recently, but I'm not sure if it was
received correctly.  It it is in:
https://www.stat.math.ethz.ch/pipermail/r-help/2004-April.txt.gz
but not yet in:
http://tolstoy.newcastle.edu.au/R/help/04/04/index.html
Apologies if I've sent it twice.

On Thu, 15 Apr 2004, James Wettenhall wrote:
Hi,

Is there any way to pass an integer from R to C and have
it cast as a pointer?

# Win32 Example:

library(tcltk)
tt <- tktoplevel()
hWndString <- tclvalue(tkwm.frame(tt))
# I'll avoid posting code to this function:
source("http://bioinf.wehi.edu.au/folders/james/R/hexStringToDecimalInteger.R")
hWnd <- hexStringToDecimalInteger(hWndString)
system32 <- file.path(Sys.getenv("windir"),"system32")
user32 <- file.path(system32,"user32.dll")
dyn.load(user32)

# WARNING: THIS NEXT LINE WILL PASS AN INVALID WINDOW HANDLE TO
# USER32.DLL AND PROBABLY CRASH YOUR R SESSION!!!!
.C("SetForegroundWindow",hWnd)

# This above won't work, because .C() will pass a pointer to the
# integer hWnd to SetForegroundWindow (in user32.dll) whereas
# I want the integer hWnd to be cast as a pointer.

So for each DLL function I want to call with a pointer argument,
do I have to define my own C function e.g. my_SetForegroundWindow
(using the Windows API) which takes an integer argument instead
and then casts it as a pointer in order to call the real
SetForegroundWindow function?

Of course environments behave like pointers but they are
read-only.  You can't do this (below), right?
mode(hWnd) <- "environment"

WHY WOULD I WANT TO DO ANYTHING LIKE THIS ABOVE?

I know that I can use tkfocus for Tk windows, but I have other
applications in mind.  In Win32, when I run RGui with MDI, the
bringToTop in tcltk's .onLoad brings the console to the top, but
doesn't focus it, i.e. after library(tcltk), I can't type into
the console until I click on it.  Now if I had RConsole->handle
from the GraphApp code for RGui, maybe I could try something
like SetForegroundWindow... For more info, go to
http://msdn.microsoft.com and search for "BringWindowToTop",
search for "SetForegroundWindow", and compare...

Maybe for this particular application, something like
show(RConsole) might be better, (see R-devel thread below)
http://www.mail-archive.com/r-devel at stat.math.ethz.ch/msg01829.html

but I'm still wondering if casting to pointers can be done...

Another application is specifying a Tk window to be "Always On
Top" using user32.dll's SetWindowPos function (again, search
for it on http://msdn.microsoft.com).

Regards,
James



From david.whiting at ncl.ac.uk  Fri Apr 16 12:26:00 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 16 Apr 2004 10:26:00 +0000
Subject: [R] tapply() and barplot() help files for 1.8.1
In-Reply-To: <200404151610.i3FGARbS004686@lynne.ethz.ch>
References: <m2smf5bh7p.fsf@ganymede.ammp.or.tz>
	<200404151610.i3FGARbS004686@lynne.ethz.ch>
Message-ID: <m2r7uomd6v.fsf@ganymede.ammp.or.tz>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> and I like to help you.
> As I keep installed `(almost) all released versions of R ever
> installed on our machines'
> I can easily run 1.8.1 (or 1.4.x or 1.0.x ...) for you.
> 
> The only difference
>  between the help page help(tapply)
> is an extra   "require(stats)" statement at the beginning of the
> `Examples' section in 1.9.0.
> 
> and the only change to  tapply() is 
>     group <- rep.int(one, nx)#- to contain the splitting vector
> instead of
>     group <- rep    (one, nx)#- to contain the splitting vector
> 
> which hardly should have adverse results.
> 
> In barplot, there's the new 'offset' option  --- not in NEWS (!!!!)
> 
> and another change that may be a problem.
> 
> Can you dig harder and if possible provide a reproducible (small..)
> example to make progress here...
> 

Last night I found I had a backup of the source of 1.8.0, built that
and tested an example and it worked as in 1.9.0.  I then started to
question my sanity (or at least my competence).

The code that follows should be a reproducible example.  It creates a
data frame that has the same structure as the data I am working with
(with a number of other columns dropped) and is followed by the
function that creates the barplot.  The changes I have had to make to
make it work as I thought it was working with 1.8.1 have ## NEW BIT
after them, i.e. those lines were not there in the version I ran with
1.8.1.  The important new lines are:

 x <- matrix(x)          ## NEW BIT

and 

 beside = TRUE,          ## NEW BIT



--- EXAMPLE ---

## Create some fake data.
x <- c(rep("", 926), 
        rep("All Other Perinatal Causes", 46), 
        rep("Anaemia", 3), 
        rep("Congenital Abnormalities", 1), 
        rep("Unsp. Direct Maternal Causes", 24))
y <- runif(length(x))
tempdat <- data.frame(smi=x, yllperdth=y)



## Define the function to make my barplot
bodShare <- function(x, fld, main = "", userpar = 18, xlimMult=1.3 ) {
  ###############################################
  # A horizontal barchart to display BoD shares #
  ###############################################
  z <- subset(x, as.character(x[,fld]) != "")
  z[, fld] <- factor(z[, fld])

  ## We need to change the parameters of the chart.
  ## First save the old settings.
  oldpar <- par("mar")
  newpar <- par("mar")

  ## Increase the size of the margin on the left so there 
  ## is enough space for the long text labels (which will 
  ## be displayed horizontally on the y-axis).
  newpar[2] <- userpar

  
  ## Reduce the top margin because I will use a \caption in LaTeX 
  ## instead.
  newpar[3] <- 1


  ## Now apply the new settings.
  par(mar = newpar)

  ## Calculate the % of YLLs for each group in the cause classification.
  x <- tapply(z$yllperdth, z[, fld], sum)
  totalYLLs <- sum(x)
  x <- x / totalYLLs * 100
  x <- sort(x)

  causeNames <- names(x)  ## NEW BIT
  x <- matrix(x)          ## NEW BIT
  

  ## Plot the chart. horiz = TRUE makes it a bar instead of 
  ## column chart.  las = 1 prints the labels horizontally.
  xplot <- barplot(x, 
##                   main = main,
                   horiz = TRUE, 
                   beside = TRUE,            ## NEW BIT
                   names.arg = causeNames,   ## NEW BIT
                   xlab = "Percent of YLLs",
                   xlim = c(0, max(x) * xlimMult), 
                   las = 1)
  
  text(x + (max(x) * .15), xplot, formatC(x, digits=1, format='f'))

  ## Reset the old margin parameters.
  par(mar = oldpar)
  
  ## Write data to a table for export.
  # First we need to remove newlines from labels.
  names(x) <- sub("\n", "", names(x))
  write.table(as.table(x), file = paste("tables/", fld, ".csv", sep=""), col.names=NA, sep="\t")
  names(x) <- causeNames
  x[length(x)]
}

## Create the barplot.
bodShare(tempdat, "smi")


-- 
David Whiting
Dar es Salaam, Tanzania



From erwan.barret at wanadoo.fr  Fri Apr 16 09:59:23 2004
From: erwan.barret at wanadoo.fr (Erwan BARRET)
Date: Fri, 16 Apr 2004 09:59:23 +0200 (CEST)
Subject: [R] Pb on startup with R1.9.0
Message-ID: <5324679.1082102363049.JavaMail.www@wwinf0402>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040416/7c084add/attachment.pl

From ripley at stats.ox.ac.uk  Fri Apr 16 09:59:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 08:59:43 +0100 (BST)
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <C4178DC99E08604EA5E2BDB989F0938001E07868@extas2-hba.tas.csiro.au>
Message-ID: <Pine.GSO.4.31.0404160852270.16317-100000@toucan.stats>

No, and options() really is part of the R/S language not the interface.

See the rw-FAQ Q6.3 for how to manage the buffering more effectively.
(Hint: you need to put calls to flush.console() in your code.)


On Fri, 16 Apr 2004 Toby.Patterson at csiro.au wrote:

> I meant via a function or something like:
>
> options( buffered.output = FALSE)
>
> Sorry, I should have made that clearer.
>
> Cheers
> Toby
>
>
>
> -----Original Message-----
> From: Roger D. Peng [mailto:rpeng at jhsph.edu]
> Sent: Friday, April 16, 2004 1:11 PM
> To: Patterson, Toby (Marine, Hobart)
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Turning windows screen buffering on and off
>
> Ctrl-W.
>
> -roger
>
> Toby.Patterson at csiro.au wrote:
> > All,
> >
> > Does anyone know if there is an option I can set to turn
> screen-buffered
> > output on and off with the win32 rgui? (Apart from the point and click
> > method).
> >
> > I am running some simulations where it is useful to watch output but
> it
> > gets mildly tiresome having to manually switch things on and off via
> the
> > gui.
> >
> > Thanks
> >
> > Toby.
> >
> >
> >>version
> >
> >          _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    1
> > minor    8.1
> > year     2003
> > month    11
> > day      21
> > language R
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Apr 16 10:07:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Apr 2004 10:07:28 +0200
Subject: [R] Pb on startup with R1.9.0
In-Reply-To: <5324679.1082102363049.JavaMail.www@wwinf0402>
References: <5324679.1082102363049.JavaMail.www@wwinf0402>
Message-ID: <407F9440.4010609@statistik.uni-dortmund.de>

Erwan BARRET wrote:

> I'm using Rprofile file on R1.9.0 startup and inside I'm loading a personal Library which uses winMenuAdd() and I've got this error :
> Error in eval(expr, envir, enclos) : couldn't find function "winMenuAdd"
> I hadn't this problem with others R versions.
> What Can I do now to avoid that?
> I'm using R1.9.0 under Win98.
> 	[[alternative HTML version deleted]]


winMenuAdd() is in package "utils" and that is not loaded when Rprofile 
is executed. You might want to add an explicit call to require("utils").

Uwe Ligges




> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Fri Apr 16 10:15:56 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 16 Apr 2004 10:15:56 +0200
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <C4178DC99E08604EA5E2BDB989F0938001E07868@extas2-hba.tas.csiro.au>
Message-ID: <MABBLJDICACNFOLGIHJOGEEJEFAA.phgrosjean@sciviews.org>

You can use flush.console() in your code wherever you want to print the
output immediatelly on screen in Rgui, even if it is in buffered output
mode. This is the most flexible option, because you decide exactly when to
update the output screen.

Best,

Philippe Grosjean

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Toby.Patterson at csiro.au
Sent: Friday, 16 April, 2004 06:38
To: R-help at stat.math.ethz.ch
Subject: RE: [R] Turning windows screen buffering on and off


I meant via a function or something like:

options( buffered.output = FALSE)

Sorry, I should have made that clearer.

Cheers
Toby



-----Original Message-----
From: Roger D. Peng [mailto:rpeng at jhsph.edu]
Sent: Friday, April 16, 2004 1:11 PM
To: Patterson, Toby (Marine, Hobart)
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Turning windows screen buffering on and off

Ctrl-W.

-roger

Toby.Patterson at csiro.au wrote:
> All,
>
> Does anyone know if there is an option I can set to turn
screen-buffered
> output on and off with the win32 rgui? (Apart from the point and click
> method).
>
> I am running some simulations where it is useful to watch output but
it
> gets mildly tiresome having to manually switch things on and off via
the
> gui.
>
> Thanks
>
> Toby.
>
>
>>version
>
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.1
> year     2003
> month    11
> day      21
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From prechelt at pcpool.mi.fu-berlin.de  Fri Apr 16 10:44:37 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Fri, 16 Apr 2004 10:44:37 +0200
Subject: [R] Dates stuff ... beginner question
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920121AE@circle.pcpool.mi.fu-berlin.de>

You need 
strptime("20030301","%Y%m%d")$wday

See
?strptime
?POSIXlt

  Lutz

> Putting it simly I have a date in format YYYYMMDD
> Year month day
> 20030301
> and I need simply the day of the week as an integer or even as
> string...



From qsdickee at hotmail.com  Fri Apr 16 11:05:07 2004
From: qsdickee at hotmail.com (Qian Kevin)
Date: Fri, 16 Apr 2004 17:05:07 +0800
Subject: [R] R-1.9.0: make error on slackware-current!
Message-ID: <BAY10-F113joO05VnRl0000bd97@hotmail.com>

My box: Slackware-current, Xfree 4.4.
ERROR as follows:

gcc -I. -I../../../src/include -I../../../src/include -I/usr/X11R6/include 
-I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES -mieee-fp -fPIC  
-g -O2 -c dataentry.c -o dataentry.lo
In file included from dataentry.c:31:
/usr/X11R6/include/X11/Xlib.h:1390: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1478: error: parse error before "char"
/usr/X11R6/include/X11/Xlib.h:1506: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1510: error: parse error before "char"
/usr/X11R6/include/X11/Xlib.h:1532: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1567: error: parse error before '*' token
/usr/X11R6/include/X11/Xlib.h:1576: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1601: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1651: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1657: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1704: error: parse error before "char"
/usr/X11R6/include/X11/Xlib.h:1743: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:1984: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2068: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2321: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2331: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2403: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2413: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2571: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2586: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2779: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2846: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2851: error: parse error before "char"
/usr/X11R6/include/X11/Xlib.h:2965: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:2991: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3002: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3027: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3036: error: parse error before "char"
/usr/X11R6/include/X11/Xlib.h:3049: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3192: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3241: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3273: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3364: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3371: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3391: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3397: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3409: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3419: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3429: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3435: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3536: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3553: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3604: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3647: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3653: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3659: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3665: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3673: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3681: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3689: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3701: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3713: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3760: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3771: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3782: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3793: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3804: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xlib.h:3815: error: parse error before "_Xconst"
In file included from dataentry.c:32:
/usr/X11R6/include/X11/Xutil.h:566: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xutil.h:606: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xutil.h:666: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xutil.h:678: error: parse error before "_Xconst"
/usr/X11R6/include/X11/Xutil.h:801: error: parse error before "_Xconst"


Help me!



From ripley at stats.ox.ac.uk  Fri Apr 16 11:06:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 10:06:17 +0100 (BST)
Subject: [R] Pb on startup with R1.9.0
In-Reply-To: <407F9440.4010609@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0404161004410.954-100000@gannet.stats>

On Fri, 16 Apr 2004, Uwe Ligges wrote:

> Erwan BARRET wrote:
> 
> > I'm using Rprofile file on R1.9.0 startup and inside I'm loading a personal Library which uses winMenuAdd() and I've got this error :
> > Error in eval(expr, envir, enclos) : couldn't find function "winMenuAdd"
> > I hadn't this problem with others R versions.
> > What Can I do now to avoid that?
> > I'm using R1.9.0 under Win98.
> 
> 
> winMenuAdd() is in package "utils" and that is not loaded when Rprofile 
> is executed. You might want to add an explicit call to require("utils").

It is better to set a start hook.  This is in the section at the very 
start of the NEWS file, which points you to ?setHook which has a similar 
example.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 16 11:41:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 10:41:27 +0100 (BST)
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <x2oepvc3tl.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404161037320.9334-100000@gannet.stats>

On 13 Apr 2004, Peter Dalgaard wrote:

> "Roger D. Peng" <rpeng at jhsph.edu> writes:
> 
> > As far as I know, R does compile on AMD Opterons and runs as a 64-bit
> > application.  So it can store objects larger than 4GB. However, I
> > don't think R gets tested very often on 64-bit machines with such
> > large objects so there may be yet undiscovered bugs.
> 
> There are a few such machines around among R users, and R seems to
> work OK on them. One slight gotcha is that the Fortran numeric
> libraries (Lapack, ATLAS) tend to use integer indexing, which might
> overflow for large objects. Things like data frames which consist of
> multiple subobjects might be less sensitive to this. 

At present we restrict vectors to 2^31-1 and as from 1.9.0 have many
overflow checks in place.  It's not just Fortran code, BTW: integer
indexing in C is pervasive in the R code, including in many add-on 
packages.

So you can use large workspaces (and as someone said, this has been done 
under Solaris and Compaq Alpha for at least a couple of years), and large 
lists (including data frames), but the size of atomic vectors is limited 
in a rather fundamental way.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fm3a004 at math.uni-hamburg.de  Fri Apr 16 11:42:37 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 16 Apr 2004 11:42:37 +0200 (MET DST)
Subject: [R] Masked help page
Message-ID: <Pine.GSO.3.95q.1040416113429.19223J-100000@sun11.math.uni-hamburg.de>

Hi list,

not really a problem to solve but a strange unwanted behavior (at least I
do not like it):

I'm loading
library(multiv)
to use sammon.
Later I load 
library(MASS)
which tells me that sammon is masked now, because
there is also a MASS version of sammon.

Since I believe that the MASS version is at least as good as the multiv
version, I would also use the MASS version, but
help(sammon)
still gives the help page for multiv's sammon.
The MASS version runs, though (and gives an error if I call it like I
would call the multiv version with dimension called p instead of k).

Is it a general behavior that if a function is masked, the help page still
is the old one?

That would not be too user friendly...

Best,
Christian

> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    8.0              
year     2003             
month    10               
day      08               
language R                


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From ripley at stats.ox.ac.uk  Fri Apr 16 12:18:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 11:18:52 +0100 (BST)
Subject: [R] Masked help page
In-Reply-To: <Pine.GSO.3.95q.1040416113429.19223J-100000@sun11.math.uni-hamburg.de>
Message-ID: <Pine.LNX.4.44.0404161117060.9334-100000@gannet.stats>

I get the sammon help from MASS when I do that in 1.8.1 and 1.9.0, and
that is the design. I suspect your version of MASS is not installed
properly.  If you just do

library(MASS)
help(sammon)

do you get any help for sammon?

On Fri, 16 Apr 2004, Christian Hennig wrote:

> Hi list,
> 
> not really a problem to solve but a strange unwanted behavior (at least I
> do not like it):
> 
> I'm loading
> library(multiv)
> to use sammon.
> Later I load 
> library(MASS)
> which tells me that sammon is masked now, because
> there is also a MASS version of sammon.
> 
> Since I believe that the MASS version is at least as good as the multiv
> version, I would also use the MASS version, but
> help(sammon)
> still gives the help page for multiv's sammon.
> The MASS version runs, though (and gives an error if I call it like I
> would call the multiv version with dimension called p instead of k).
> 
> Is it a general behavior that if a function is masked, the help page still
> is the old one?
> 
> That would not be too user friendly...
> 
> Best,
> Christian
> 
> > version
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    8.0              
> year     2003             
> month    10               
> day      08               
> language R                
> 
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fm3a004 at math.uni-hamburg.de  Fri Apr 16 12:26:07 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 16 Apr 2004 12:26:07 +0200 (MET DST)
Subject: solved: [R] Masked help page
In-Reply-To: <Pine.GSO.3.95q.1040416113429.19223J-100000@sun11.math.uni-hamburg.de>
Message-ID: <Pine.GSO.3.95q.1040416122404.19223M-100000@sun11.math.uni-hamburg.de>

Hi,

Martin Maechler told me that the following is an ESS and not an R-problem,
which can be solved (inside ESS) by

C-u C-h C-v  sammon

Outside ESS, the problem does not occur (which I had not tried out
before).

Thanks,
Christian

On Fri, 16 Apr 2004, Christian Hennig wrote:

> Hi list,
> 
> not really a problem to solve but a strange unwanted behavior (at least I
> do not like it):
> 
> I'm loading
> library(multiv)
> to use sammon.
> Later I load 
> library(MASS)
> which tells me that sammon is masked now, because
> there is also a MASS version of sammon.
> 
> Since I believe that the MASS version is at least as good as the multiv
> version, I would also use the MASS version, but
> help(sammon)
> still gives the help page for multiv's sammon.
> The MASS version runs, though (and gives an error if I call it like I
> would call the multiv version with dimension called p instead of k).
> 
> Is it a general behavior that if a function is masked, the help page still
> is the old one?
> 
> That would not be too user friendly...
> 
> Best,
> Christian
> 
> > version
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    8.0              
> year     2003             
> month    10               
> day      08               
> language R                
> 
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From ripley at stats.ox.ac.uk  Fri Apr 16 12:30:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 11:30:01 +0100 (BST)
Subject: [R] R-1.9.0: make error on slackware-current!
In-Reply-To: <BAY10-F113joO05VnRl0000bd97@hotmail.com>
Message-ID: <Pine.LNX.4.44.0404161120070.9334-100000@gannet.stats>

We have seen this before, and fixed it, at least on my installation.  The 
fix was to ensure that the X11 headers were declared before Defn.h, which 
they are in 1.9.0 but not in 1.8.1.  So your XFree 4.4 installation is not 
liking the way those header files are defined, and we need you to 
disentangle this.

On Fri, 16 Apr 2004, Qian Kevin wrote:

> My box: Slackware-current, Xfree 4.4.
> ERROR as follows:
> 
> gcc -I. -I../../../src/include -I../../../src/include -I/usr/X11R6/include 
> -I/usr/local/include -DHAVE_CONFIG_H -D__NO_MATH_INLINES -mieee-fp -fPIC  
> -g -O2 -c dataentry.c -o dataentry.lo
> In file included from dataentry.c:31:
> /usr/X11R6/include/X11/Xlib.h:1390: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1478: error: parse error before "char"
> /usr/X11R6/include/X11/Xlib.h:1506: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1510: error: parse error before "char"
> /usr/X11R6/include/X11/Xlib.h:1532: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1567: error: parse error before '*' token
> /usr/X11R6/include/X11/Xlib.h:1576: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1601: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1651: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1657: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1704: error: parse error before "char"
> /usr/X11R6/include/X11/Xlib.h:1743: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:1984: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2068: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2321: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2331: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2403: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2413: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2571: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2586: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2779: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2846: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2851: error: parse error before "char"
> /usr/X11R6/include/X11/Xlib.h:2965: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:2991: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3002: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3027: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3036: error: parse error before "char"
> /usr/X11R6/include/X11/Xlib.h:3049: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3192: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3241: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3273: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3364: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3371: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3391: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3397: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3409: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3419: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3429: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3435: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3536: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3553: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3604: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3647: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3653: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3659: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3665: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3673: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3681: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3689: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3701: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3713: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3760: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3771: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3782: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3793: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3804: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xlib.h:3815: error: parse error before "_Xconst"
> In file included from dataentry.c:32:
> /usr/X11R6/include/X11/Xutil.h:566: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xutil.h:606: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xutil.h:666: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xutil.h:678: error: parse error before "_Xconst"
> /usr/X11R6/include/X11/Xutil.h:801: error: parse error before "_Xconst"
> 
> 
> Help me!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bitwrit at ozemail.com.au  Fri Apr 16 00:31:09 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 16 Apr 2004 08:31:09 +1000
Subject: [R] install.packages
In-Reply-To: <16510.47377.14519.265974@gargle.gargle.HOWL>
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>
	<407EB542.653DA2C1@mcgill.ca>
	<16510.47377.14519.265974@gargle.gargle.HOWL>
Message-ID: <20040416103134.GHOU23219.smta09.mail.ozemail.net@there>

I hope I will be pardoned for asking what may be a dumb question on this 
thread. Recently I noticed that when I tried my established method of adding 
a package to R (i.e. download the *.tar.gz file to a directory of similar 
files and INSTALL), I was no longer able to do this. I did eventually find 
the function install.packages() and it worked very nicely, thanks. However, 
I'd like to know if it is still possible to download packages from CRAN or 
other sites. For example, I run R on a little old laptop that has no Internet 
connection whatever, and it will be a bit of a problem keeping it up to date.

Jim



From ligges at statistik.uni-dortmund.de  Fri Apr 16 12:57:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Apr 2004 12:57:06 +0200
Subject: [R] install.packages
In-Reply-To: <20040416103134.GHOU23219.smta09.mail.ozemail.net@there>
References: <3A822319EB35174CA3714066D590DCD504AF7BF0@usrymx25.merck.com>	<407EB542.653DA2C1@mcgill.ca>	<16510.47377.14519.265974@gargle.gargle.HOWL>
	<20040416103134.GHOU23219.smta09.mail.ozemail.net@there>
Message-ID: <407FBC02.7020804@statistik.uni-dortmund.de>

Jim Lemon wrote:
> I hope I will be pardoned for asking what may be a dumb question on this 
> thread. Recently I noticed that when I tried my established method of adding 
> a package to R (i.e. download the *.tar.gz file to a directory of similar 
> files and INSTALL), I was no longer able to do this. I did eventually find 
> the function install.packages() and it worked very nicely, thanks. However, 
> I'd like to know if it is still possible to download packages from CRAN or 
> other sites. For example, I run R on a little old laptop that has no Internet 
> connection whatever, and it will be a bit of a problem keeping it up to date.

Downloading the sources and running
    R INSTALL package_x.y-z.tar.gz
should still work (and does for me) ...
Which R version / OS  are we talking about?

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Fri Apr 16 13:41:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Apr 2004 13:41:19 +0200
Subject: [R] Need advice on using R with large datasets
In-Reply-To: <Pine.LNX.4.44.0404161037320.9334-100000@gannet.stats>
References: <Pine.LNX.4.44.0404161037320.9334-100000@gannet.stats>
Message-ID: <x265c0jgkg.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> At present we restrict vectors to 2^31-1 and as from 1.9.0 have many
> overflow checks in place.  It's not just Fortran code, BTW: integer
> indexing in C is pervasive in the R code, including in many add-on 
> packages.

Of course, to state the obvious, if were talking numeric variables:

> (2^31-1)*8
[1] 17179869176

I.e. 16GB, which is about the limit of what current machines have of
RAM, so we're not really in trouble for the next year or so...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From unung at enciety.com  Fri Apr 16 14:09:50 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: 16 Apr 2004 19:09:50 +0700
Subject: [R] About Read SPSS from SPSS Data Entry
Message-ID: <1082117390.2872.7.camel@IT05>

I've found problem read.spss with library foreign which data are
produced from SPSS data entry.

This error is : 

"File layout code has unexpected value 50331648.  Value should be 2, in
big-endian or little-endian format."

Because SPSS Data Entry has new system-file header. Any one can help it.

Thanks.

Unung Istopo H



From bates at stat.wisc.edu  Fri Apr 16 14:18:40 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 16 Apr 2004 07:18:40 -0500
Subject: [R] Solving Matrices
In-Reply-To: <200404160008.i3G08pYO261268@atlas.otago.ac.nz>
References: <200404160008.i3G08pYO261268@atlas.otago.ac.nz>
Message-ID: <6rekqo3ylb.fsf@bates4.stat.wisc.edu>

"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:

> Elizabeth (etb <lizzy at noradd.org>) wrote:
> 	     In execises 39-42, determine if the columns of the matrix span
> 	     R4:
>                    4	
> Presumably that's R, 4-dimensional real space.
> 
> 	(or x <- matrix(data=c(7, -5, 6, -7, 2, -3, 10, 9, -5, 
> 	                       4, -2, 2, 8, -9, 7, 15), nrow=4, ncol=4)
> 	
> 	That is the whole of the question and I suppose that the way to answer
> 	this is by determining if:
> 	
> 		1. For each b in Rm, the equation Ax = b has a solution, or
> 		2. A has a pivot position in every row,
> 	
> 	where A is an (m X n) matrix.
...
> For another look at this, try the singular-value decomposition.
> ?svd
> 
>     > svd(x)$d
>     [1] 2.436185e+01 1.376648e+01 6.163148e+00 9.241655e-16
> 
> Again, we see a pretty strong hint that the matrix is close to rank 3.

Thanks for the suggestion of looking at the svd.  In fact the svd
gives two linear combinations of the elements of x that are (nearly)
zero - one for the rows and one for the columns.

> sv = svd(x)
> sv
$d
[1] 2.436185e+01 1.376648e+01 6.163148e+00 2.708761e-16

$u
           [,1]       [,2]       [,3]         [,4]
[1,] -0.3797190 -0.5126130  0.4176563 6.469966e-01
[2,]  0.4193577  0.3345945 -0.3773834 7.548294e-01
[3,] -0.4980502 -0.2953438 -0.8153024 1.387779e-16
[4,] -0.6571899  0.7335165  0.1357460 1.078328e-01

$v
           [,1]       [,2]       [,3]       [,4]
[1,] -0.1290047 -0.8838820 -0.1673698 -0.4172502
[2,] -0.5300387  0.1176192 -0.8054078  0.2377877
[3,]  0.1337233  0.4328750 -0.2751384 -0.8479600
[4,] -0.8273662  0.1324295  0.4975987 -0.2243281

> x %*% sv$v[,4]
              [,1]
[1,] -8.881784e-16
[2,]  1.776357e-15
[3,] -4.440892e-16
[4,] -2.220446e-15
> sv$u[,4] %*% x
             [,1]          [,2]         [,3]          [,4]
[1,] 2.220446e-16 -4.440892e-16 5.551115e-16 -3.108624e-15

The left and right singular vectors sv$u[,4] and sv$v[,4] have been
normalized to have unit length and do not have a simple expression as
integers.  However, if we divide by the smallest non-zero element in
the vector we get

> sv$u[,4]/sv$u[4,4]
[1] 6.000000e+00 7.000000e+00 1.286973e-15 1.000000e+00

and now we can verify that

> 6 * x[1,] + 7 * x[2,] + x[4,]
[1] 0 0 0 0

Obtaining the relationship of the columns as integers is more
difficult (probably intentionally) but we can get a hint by looking at
small differences in magnitudes

> sv$v[,4]/(sv$v[2,4] + sv$v[4,4])
[1] -31.00000  17.66667 -63.00000 -16.66667

from which it is easy to see that

> 3*(sv$v[,4]/(sv$v[2,4] + sv$v[4,4]))
[1]  -93   53 -189  -50

> x %*% c(93, -53, 189, 50)
     [,1]
[1,]    0
[2,]    0
[3,]    0
[4,]    0



From andy_liaw at merck.com  Fri Apr 16 14:25:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 16 Apr 2004 08:25:41 -0400
Subject: [R] Turning windows screen buffering on and off
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C00@usrymx25.merck.com>

> From: Prof Brian Ripley
> 
> No, and options() really is part of the R/S language not the 
> interface.
> 
> See the rw-FAQ Q6.3 for how to manage the buffering more effectively.
> (Hint: you need to put calls to flush.console() in your code.)

Doesn't that only work for output at the R level?  I.e., if the output is
coming solely from compiled code, my impression is that flush.console()
won't help.  Am I mistaken?

Is it possible to make it an option in Edit / GUI preferences?

Best,
Andy

> On Fri, 16 Apr 2004 Toby.Patterson at csiro.au wrote:
> 
> > I meant via a function or something like:
> >
> > options( buffered.output = FALSE)
> >
> > Sorry, I should have made that clearer.
> >
> > Cheers
> > Toby
> >
> >
> >
> > -----Original Message-----
> > From: Roger D. Peng [mailto:rpeng at jhsph.edu]
> > Sent: Friday, April 16, 2004 1:11 PM
> > To: Patterson, Toby (Marine, Hobart)
> > Cc: R-help at stat.math.ethz.ch
> > Subject: Re: [R] Turning windows screen buffering on and off
> >
> > Ctrl-W.
> >
> > -roger
> >
> > Toby.Patterson at csiro.au wrote:
> > > All,
> > >
> > > Does anyone know if there is an option I can set to turn
> > screen-buffered
> > > output on and off with the win32 rgui? (Apart from the 
> point and click
> > > method).
> > >
> > > I am running some simulations where it is useful to watch 
> output but
> > it
> > > gets mildly tiresome having to manually switch things on 
> and off via
> > the
> > > gui.
> > >
> > > Thanks
> > >
> > > Toby.
> > >
> > >
> > >>version
> > >
> > >          _
> > > platform i386-pc-mingw32
> > > arch     i386
> > > os       mingw32
> > > system   i386, mingw32
> > > status
> > > major    1
> > > minor    8.1
> > > year     2003
> > > month    11
> > > day      21
> > > language R
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >
> 
> -- 
> Brian D. 
> Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272860 (secr)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From wildscop at yahoo.com  Fri Apr 16 14:24:58 2004
From: wildscop at yahoo.com (WilDscOp)
Date: Fri, 16 Apr 2004 18:24:58 +0600
Subject: [R] Non-Linear Regression Problem
Message-ID: <5.1.0.14.2.20040416182412.009f51d0@127.0.0.1>

Dear all,

First of all i like to thank specially Spencer Graves and Douglas Bates for 
their kind help and suggestions. I estimated the parameter nicely.

But, for my previous posted problem, the rational of the following line
  R> fit0 <- lm(log(Y)~t-1, draper.data)
is not clear to me. How come this gives the initial value?

Any response / help / comment / suggestion / idea / web-link / replies will 
be greatly appreciated.

Thanks in advance for your time.

_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From wildscop at yahoo.com  Fri Apr 16 14:25:42 2004
From: wildscop at yahoo.com (WilDscOp)
Date: Fri, 16 Apr 2004 18:25:42 +0600
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
Message-ID: <5.1.0.14.2.20040416182500.009fa390@127.0.0.1>

Dear all,

For estimating Cobb-Douglad production Function [ Y = ALPHA * (L^(BETA1)) * 
(K^(BETA2))  ], i want to use nls function (without linearizing it). But 
how can i get initial values?

------------------------------------
 > options(prompt=" R> " )
  R> Y <- c(59.6, 63.9, 73.5, 75.6, 77.3, 82.8, 83.6, 84.9, 90.3, 80.5, 
73.5, 60.3, 58.2, 64.4, 75.4, 85, 92.7, 85.4, 92.3, 101.2, 113.3, 107.8, 
105.2, 107.1, 108.8, 131.4, 130.9, 134.7, 129.1, 147.8, 152.1, 154.3, 
159.9) # production
  R> L <- c(39.4, 41.4, 43.9, 43.3, 44.5, 45.8, 45.9, 46.4, 47.6, 45.5, 
42.6, 39.3, 39.6, 42.7, 44.2, 47.1, 48.2, 46.4, 47.8, 49.6, 54.1, 59.1, 
64.9, 66, 64.4, 58.9, 59.3, 60.2, 58.7, 60, 63.8, 64.9, 66) # employment
  R> K <- c(236.2, 240.2, 248.9, 254.5, 264.1, 273.9, 282.6, 290.2, 299.4, 
303.3, 303.4, 297.1, 290.1, 285.4, 287.8, 292.1, 300.3, 301.4, 305.6, 
313.3, 327.4, 339, 347.1, 353.5, 354.1, 359.4, 359.3, 365.2, 363.2, 373.7, 
386, 396.5, 408) # capital
  R> klein <- cbind(Y,L,K)
  R> klein.data<-data.frame(klein)
  R> coef(lm(log(Y)~log(L)+log(K)))
# i used these linearized model's estimated parameters as initial values
(Intercept)      log(L)      log(K)
  -3.6529493   1.0376775   0.7187662
  R> nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), data=klein.data, start = 
c(ALPHA=-3.6529493,BETA1=1.0376775,BETA2=0.7187662), trace = T)
6852786785 :  -3.6529493  1.0376775  0.7187662
1515217 :  -0.02903916  1.04258165  0.71279051
467521.8 :  -0.02987718  1.67381193 -0.05609925
346945.7 :   -0.5570735  10.2050667 -10.2087997
Error in numericDeriv(form[[3]], names(ind), env) :
         Missing value or an Infinity produced when evaluating the model
------------------------------------

1. What went wrong? I think the initial values are not good enough: How can 
i make a grid search?

2. How can i estimate C.E.S Production Function [  Y = GAMA * 
((DELTA*K^(-BETA)) + ((1-DELTA)*L^(-BETA)))^(-PHI/BETA)  ] using the same 
data? How to get the initial value?

N.B.: The data file is available at http://www.angelfire.com/ab5/get5/klein.txt

Any response / help / comment / suggestion / idea / web-link / replies will 
be greatly appreciated.

Thanks in advance for your time.

_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From ripley at stats.ox.ac.uk  Fri Apr 16 14:29:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 13:29:23 +0100 (BST)
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7C00@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0404161327140.23269-100000@gannet.stats>

On Fri, 16 Apr 2004, Liaw, Andy wrote:

> > From: Prof Brian Ripley
> > 
> > No, and options() really is part of the R/S language not the 
> > interface.
> > 
> > See the rw-FAQ Q6.3 for how to manage the buffering more effectively.
> > (Hint: you need to put calls to flush.console() in your code.)
> 
> Doesn't that only work for output at the R level?  I.e., if the output is
> coming solely from compiled code, my impression is that flush.console()
> won't help.  Am I mistaken?

It works for all output, but if you want to trigger it in compiled code
you need C-level calls -- that is also in the rw-FAQ.

> Is it possible to make it an option in Edit / GUI preferences?

`It' being the default state of buffering?  Yes, it is possible.
Patches will be considered ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ReyniesA at ligue-cancer.net  Fri Apr 16 14:50:18 2004
From: ReyniesA at ligue-cancer.net (=?iso-8859-1?Q?Reynies_Aur=E9lien_=28De=29?=)
Date: Fri, 16 Apr 2004 14:50:18 +0200
Subject: [R] how to free memory space ??
Message-ID: <02605465C8869641991A4B9EE9C7DAC627A5BF@SV0013.ligue-cancer.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040416/0244ce50/attachment.pl

From jgentry at jimmy.harvard.edu  Fri Apr 16 14:52:07 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Fri, 16 Apr 2004 08:52:07 -0400 (EDT)
Subject: [R] how to free memory space ??
In-Reply-To: <02605465C8869641991A4B9EE9C7DAC627A5BF@SV0013.ligue-cancer.net>
Message-ID: <Pine.SOL.4.20.0404160851090.26548-100000@santiam.dfci.harvard.edu>

> I thought that 'rm' function would free the memory used for the
> deleted object, but it doesn't seem to be the case, as I don't observe
> any change in the allocated memory for R in the windows task manager.
> I ask this question because sometimes I have encounter a '... ran out
> of memory' error - for a calculation that previously worked fine - I
> did notice that this error often occurs when other objects are using a
> lot of memory. Closing/relaunching R and redoing the calculation helps
> under these circumstances.....

gc() will reclaim memory from objects which are no longer being used.



From michele.alzetta at aliceposta.it  Fri Apr 16 14:55:18 2004
From: michele.alzetta at aliceposta.it (Michele Alzetta)
Date: Fri, 16 Apr 2004 14:55:18 +0200
Subject: [R] R 1.9.0 for Mandrake - and italian translation
Message-ID: <1082120118.6778.6.camel@localhost>

I have built the RPM's for Mandrake 10.0 community, 9.2 and 9.1 which
will propagate through CRAN as usual.

I have also made them available at this address:

http://web.rossoalice.it/michele.alzetta/

On this page the work in progress of translating "An introduction to R"
to italian is also available (about 60 % done).

-- 
Michele Alzetta <michele.alzetta at aliceposta.it>



From wettenhall at wehi.edu.au  Fri Apr 16 15:05:53 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Fri, 16 Apr 2004 23:05:53 +1000 (EST)
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
Message-ID: <Pine.LNX.4.58.0404162257030.22017@unix28.alpha.wehi.edu.au>

Mohammed,

> For estimating Cobb-Douglas production Function [ Y = ALPHA * 
> (L^(BETA1)) * (K^(BETA2))  ], i want to use nls function 
> (without linearizing it). But how can i get initial values? 

This might be a dumb question, but why do you need nonlinear 
regression for that model?  It is linear after taking logs:

log Y = log ALPHA + BETA1 log L + BETA2 log K

> 2. How can i estimate C.E.S Production Function [  Y = GAMA * 
> ((DELTA*K^(-BETA)) + ((1-DELTA)*L^(-BETA)))^(-PHI/BETA)  ] 

Your second model (C.E.S. Prod. Fcn) does indeed look nonlinear, 
and I'm sorry I can't think how to find a good point 
around which to linearize. Can you guess some approximate 
parameter values for some "typical" Y,L,K data?  

If it's too hard to estimate parameters 
for a model, maybe it's time to come up with a new model :)

Regards,
James



From sundar.dorai-raj at PDF.COM  Fri Apr 16 15:16:04 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 16 Apr 2004 08:16:04 -0500
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
In-Reply-To: <5.1.0.14.2.20040416182500.009fa390@127.0.0.1>
References: <5.1.0.14.2.20040416182500.009fa390@127.0.0.1>
Message-ID: <407FDC94.7020301@pdf.com>



WilDscOp wrote:
> Dear all,
> 
> For estimating Cobb-Douglad production Function [ Y = ALPHA * 
> (L^(BETA1)) * (K^(BETA2))  ], i want to use nls function (without 
> linearizing it). But how can i get initial values?
> 
> ------------------------------------
>  > options(prompt=" R> " )
>  R> Y <- c(59.6, 63.9, 73.5, 75.6, 77.3, 82.8, 83.6, 84.9, 90.3, 80.5, 
> 73.5, 60.3, 58.2, 64.4, 75.4, 85, 92.7, 85.4, 92.3, 101.2, 113.3, 107.8, 
> 105.2, 107.1, 108.8, 131.4, 130.9, 134.7, 129.1, 147.8, 152.1, 154.3, 
> 159.9) # production
>  R> L <- c(39.4, 41.4, 43.9, 43.3, 44.5, 45.8, 45.9, 46.4, 47.6, 45.5, 
> 42.6, 39.3, 39.6, 42.7, 44.2, 47.1, 48.2, 46.4, 47.8, 49.6, 54.1, 59.1, 
> 64.9, 66, 64.4, 58.9, 59.3, 60.2, 58.7, 60, 63.8, 64.9, 66) # employment
>  R> K <- c(236.2, 240.2, 248.9, 254.5, 264.1, 273.9, 282.6, 290.2, 
> 299.4, 303.3, 303.4, 297.1, 290.1, 285.4, 287.8, 292.1, 300.3, 301.4, 
> 305.6, 313.3, 327.4, 339, 347.1, 353.5, 354.1, 359.4, 359.3, 365.2, 
> 363.2, 373.7, 386, 396.5, 408) # capital
>  R> klein <- cbind(Y,L,K)
>  R> klein.data<-data.frame(klein)
>  R> coef(lm(log(Y)~log(L)+log(K)))
> # i used these linearized model's estimated parameters as initial values
> (Intercept)      log(L)      log(K)
>  -3.6529493   1.0376775   0.7187662
>  R> nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), data=klein.data, start = 
> c(ALPHA=-3.6529493,BETA1=1.0376775,BETA2=0.7187662), trace = T)
> 6852786785 :  -3.6529493  1.0376775  0.7187662
> 1515217 :  -0.02903916  1.04258165  0.71279051
> 467521.8 :  -0.02987718  1.67381193 -0.05609925
> 346945.7 :   -0.5570735  10.2050667 -10.2087997
> Error in numericDeriv(form[[3]], names(ind), env) :
>         Missing value or an Infinity produced when evaluating the model
> ------------------------------------
> 
> 1. What went wrong? I think the initial values are not good enough: How 
> can i make a grid search?
> 

I think you meant this:

nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), data=klein.data, start = 
c(ALPHA=exp(-3.6529493),BETA1=1.0376775,BETA2=0.7187662), trace = TRUE)

Note that you neglected to exponentiate ALPHA.


> 2. How can i estimate C.E.S Production Function [  Y = GAMA * 
> ((DELTA*K^(-BETA)) + ((1-DELTA)*L^(-BETA)))^(-PHI/BETA)  ] using the 
> same data? How to get the initial value?
> 

This one is tougher. I set initial values DELTA = 0.5 and BETA = -1 and 
solved the log(Y) problem.

# log(Y) = log(GAMMA) + PHI * log(0.5 * K + 0.5 * L)
DELTA <- 0.5
BETA <- -1.0
start <- coef(lm(log(Y) ~ log(I(0.5 * K + 0.5 * L))))
GAMMA <- exp(start[1])
PHI <- -BETA * start[2]

However, I still couldn't get nls to even take one step due to singular 
gradient at the initial values. So I tried optim instead

ces.opt <- function(par, Y, L, K) {
   delta <- par[1]
   beta <- par[2]
   gamma <- par[3]
   phi <- par[4]
   Y0 <- delta * K^(-beta) + (1 - delta) * L^(-beta)
   Yhat <- gamma * Y0^(-phi/beta)
   sum((Y - Yhat)^2)
}

start <- optim(c(DELTA, BETA, GAMMA, PHI), ces.opt,
                Y = Y, L = L, K = K,
                method = "BFGS",
                control = list(maxit = 1000))
Y0hat <- with(start, par[1] * K^(-par[2]) + (1 - par[1]) * L^(-par[2]))
Yhat <- with(start, par[3] * Y0hat^(-par[4]/par[2]))
plot(Yhat ~ Y)

The fit wasn't to shabby. So I used the final values from the output 
from optim to feed to nls.

nls(Y ~ GAMMA * (DELTA * K^(-BETA) + (1 - DELTA) * 
L^(-BETA))^(-PHI/BETA), data=klein.data, start = c(DELTA = start$par[1], 
BETA = start$par[2], GAMMA = start$par[3], PHI = start$par[4]), trace = 
TRUE)

This still didn't work. Perhaps somebody else can explain this, though.


HTH,

--sundar



From ripley at stats.ox.ac.uk  Fri Apr 16 15:17:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 14:17:57 +0100 (BST)
Subject: [R] how to free memory space ??
In-Reply-To: <Pine.SOL.4.20.0404160851090.26548-100000@santiam.dfci.harvard.edu>
Message-ID: <Pine.LNX.4.44.0404161411270.29260-100000@gannet.stats>

On Fri, 16 Apr 2004, Jeff Gentry wrote:

> > I thought that 'rm' function would free the memory used for the
> > deleted object, but it doesn't seem to be the case, as I don't observe
> > any change in the allocated memory for R in the windows task manager.

That is never a good test of how much is actually allocated.  Windows is 
not good at taking back memory, and in any case R for Windows avoids 
offering it back as that is slow if it needs it again.  So memory can be 
reserved for R but not actually allocated.

> > I ask this question because sometimes I have encounter a '... ran out
> > of memory' error - for a calculation that previously worked fine - I
> > did notice that this error often occurs when other objects are using a
> > lot of memory. Closing/relaunching R and redoing the calculation helps
> > under these circumstances.....
> 
> gc() will reclaim memory from objects which are no longer being used.

Yes, but that is done automatically before such error messages are 
produced.

The real issue is that Windows has a single 2Gb (or 3Gb on some versions) 
memory space, and it can get fragmented.  Having free memory does not help 
if it is not in a large enough contiguous chunk, and even if it is, it may 
not be accessible to the memory manager used.

You should find 1.9.0 works better under Windows, so please upgrade if
you have not done so already.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at PDF.COM  Fri Apr 16 15:40:59 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 16 Apr 2004 08:40:59 -0500
Subject: [R] Non-Linear Regression Problem
In-Reply-To: <5.1.0.14.2.20040416182412.009f51d0@127.0.0.1>
References: <5.1.0.14.2.20040416182412.009f51d0@127.0.0.1>
Message-ID: <407FE26B.2090009@pdf.com>



WilDscOp wrote:

> Dear all,
> 
> First of all i like to thank specially Spencer Graves and Douglas Bates 
> for their kind help and suggestions. I estimated the parameter nicely.
> 
> But, for my previous posted problem, the rational of the following line
>  R> fit0 <- lm(log(Y)~t-1, draper.data)
> is not clear to me. How come this gives the initial value?
> 
> Any response / help / comment / suggestion / idea / web-link / replies 
> will be greatly appreciated.
> 
> Thanks in advance for your time.
> 

I don't recall your previous post, but my guess is that you're trying to 
solve

Y = exp(A * t)

Taking the log of both sides gives

log(Y) = A * t

This a linear model with no intercept. To fit this in R, use -1 in the 
formula to drop the intercept.

Ahat <- coef(lm(log(Y) ~ t - 1))

Note, I would not use "t" as a variable name, since it is also a 
function in the base package (transpose of a matrix). Most of the time R 
can make the distinction, but why chance it?

HTH,

--sundar



From kjetil at entelnet.bo  Fri Apr 16 16:14:43 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 16 Apr 2004 10:14:43 -0400
Subject: [R] RE. arimaX
Message-ID: <407FB213.2040.6245E5@localhost>

On 1 Apr 2004 at 20:28, michele lux wrote:

If by arimax you meant arima with the xreg argument, 
where xreg is a vector or matrix of exogeneous variables, 
then it is my understanding (but I did'nt yet understand the 
code completely) that the coefficients of the columns in xreg 
is estimate jointlt with the ARMA parameters, by maximum likelihood
(or conditional maximum likelihood in the case method="CSS"
was used). 

If this is wrong, could somebody correct me?

(If by arimax you meant something else, you should better explain).

Kjetil Halvorsen

> Hallo all
> can someone explain me how the exogenus variables work
> in the arimax models is not clear for me...
> Thanks Michele
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From kjetil at entelnet.bo  Fri Apr 16 04:33:03 2004
From: kjetil at entelnet.bo (Self)
Date: Thu, 15 Apr 2004 22:33:03 -0400
Subject: [R] arimax...
Message-ID: <200404160505.i3G55E5N020268@mailgate.entelnet.bo>

On 1 Apr 2004 at 20:28, michele lux wrote:

If by arimax you meant arima with the xreg argument, 
where xreg is a vector or matrix of exogeneous variables, 
then it is my understanding (but I did'nt yet understand the 
code completely) that the coefficients of the columns in xreg 
is estimate jointlt with the ARMA parameters, by maximum likelihood
(or conditional maximum likelihood in the case method="CSS"
was used). 

If this is wrong, could somebody correct me?

(If by arimax you meant something else, you should better explain).

Kjetil Halvorsen

> Hallo all
> can someone explain me how the exogenus variables work
> in the arimax models is not clear for me...
> Thanks Michele
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From sway at tanox.com  Fri Apr 16 16:49:22 2004
From: sway at tanox.com (Shawn Way)
Date: Fri, 16 Apr 2004 09:49:22 -0500
Subject: [R] residuals
Message-ID: <2F3262756375D411B0CC00B0D049775D01A4CEC5@westpark.tanox.net>

I was going through all kinds of torture to figure this out.  An elegant
solution, indeed.

Thank you very much.

Shawn Way, PE
Engineering Manager
sway at tanox.com


-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Friday, April 16, 2004 9:15 AM
To: 'Shawn Way'
Subject: RE: [R] residuals


Shawn,

This is what I'd do:

lof <- function(x, y) {
    fit1 <- anova(lm(y ~ x))
    fit2 <- summary(aov(y ~ factor(x)))[[1]]
    SSpe <- fit2[["Sum Sq"]][2]
    DFpe <- fit2[["Df"]][2]
    SSresid <- fit1[["Sum Sq"]][2]
    DFresid <- fit1[["Df"]][2]
    SSlof <- SSresid - SSpe
    DFlof <- DFresid - DFpe
    Fstat <- (SSlof / DFlof) / (SSpe / DFpe)
    pval <- pf(Fstat, DFlof, DFpe, lower.tail=FALSE)
    out <- matrix(c(SSlof, SSpe, DFlof, DFpe, Fstat, NA, pval, NA), nrow=2,
                  dimnames=list(c("Lack-of-fit", "Pure Error"),
                                c("Sum Sq", "Df", "F value", "Pr(>F)")))
    class(out) <- c("anova", class(out))
    out
}

On your data, I get:

> lof(data$ref, data$actual)
             Sum Sq Df F value Pr(>F)
Lack-of-fit 0.13441  1  0.7984 0.4374
Pure Error  0.50505  3               

Best,
Andy

> From: Shawn Way
> 
> 
> I'm trying to determine the lack of fit for regression on the
> following:
> 
> data <- data.frame(ref=c(0,50,100,0,50,100),
>                              actual=c(.01,50.9,100.2,.02,49.9,100.1),
>                              level=gl(3,1))
> fit <- lm(actual~ref,data)
> fit.aov <- aov(actual~ref+Error(level),data)
> 
> According to the information I have, the lack of fit for this
> regression is
> the f-ratio between the residuals of the level contribution and the
> residuals within.  I'm trying to get the information from the 
> fit to make
> this ratio of the residual mean squares.
> 
> Any thoughts?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From fm3a004 at math.uni-hamburg.de  Fri Apr 16 17:08:39 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 16 Apr 2004 17:08:39 +0200 (MET DST)
Subject: [R] Specification of model formulae
Message-ID: <Pine.GSO.3.95q.1040416170210.19223P-100000@sun11.math.uni-hamburg.de>

Hi,

is it true that the only way to tell rpart which variables to use
is to specify every single variable in the formula and separate them with
"+"?

I wonder if this error...

> rotree <- rpart(powerdata[,1]~powerdata[,2:3],
+                      method="class")
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
	invalid variable type

...can be prevented by other means than...

> rotree <- rpart(segments~V1.3+V1.9,
+                      method="class",data=powerdata)

This works.
(The variables here are powerdata[,1] and [,2:3].)

But I do not want to write 200 variable names down, as this is the real
size of "powerdata".

Is there any way around that?

Christian



***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From ligges at statistik.uni-dortmund.de  Fri Apr 16 17:18:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 16 Apr 2004 17:18:43 +0200
Subject: [R] Specification of model formulae
In-Reply-To: <Pine.GSO.3.95q.1040416170210.19223P-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1040416170210.19223P-100000@sun11.math.uni-hamburg.de>
Message-ID: <407FF953.6070406@statistik.uni-dortmund.de>

Christian Hennig wrote:
> Hi,
> 
> is it true that the only way to tell rpart which variables to use
> is to specify every single variable in the formula and separate them with
> "+"?
> 
> I wonder if this error...
> 
> 
>>rotree <- rpart(powerdata[,1]~powerdata[,2:3],
> +                      method="class")
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  : 
> 	invalid variable type
> 
> ...can be prevented by other means than...
> 
> 
>>rotree <- rpart(segments~V1.3+V1.9,
> 
> +                      method="class",data=powerdata)
> 
> This works.
> (The variables here are powerdata[,1] and [,2:3].)
> 
> But I do not want to write 200 variable names down, as this is the real
> size of "powerdata".
> 
> Is there any way around that?
>
> Christian

Consider to use

rotree <- rpart(segments ~ ., method = "class", data = powerdata)

The "." defines all variables in powerdata, except for "segments", as 
usual for formulas.

Uwe Ligges





> 
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at entelnet.bo  Fri Apr 16 17:50:32 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 16 Apr 2004 11:50:32 -0400
Subject: [R] Specification of model formulae
In-Reply-To: <Pine.GSO.3.95q.1040416170210.19223P-100000@sun11.math.uni-hamburg.de>
Message-ID: <407FC888.19314.B9FBB1@localhost>

On 16 Apr 2004 at 17:08, Christian Hennig wrote:

You can build your formula along the ways of the following example, 
without typing the 200 names:

> nam <- paste("var", 1:5, sep="", collapse="+")
> nam
[1] "var1+var2+var3+var4+var5"
> nam <- paste("y", nam, sep="~")
> nam
[1] "y~var1+var2+var3+var4+var5"
> as.formula(nam)
y ~ var1 + var2 + var3 + var4 + var5

Kjetil Halvorsen

> Hi,
> 
> is it true that the only way to tell rpart which variables to use is
> to specify every single variable in the formula and separate them with
> "+"?
> 
> I wonder if this error...
> 
> > rotree <- rpart(powerdata[,1]~powerdata[,2:3],
> +                      method="class")
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  : 
>  invalid variable type
> 
> ...can be prevented by other means than...
> 
> > rotree <- rpart(segments~V1.3+V1.9,
> +                      method="class",data=powerdata)
> 
> This works.
> (The variables here are powerdata[,1] and [,2:3].)
> 
> But I do not want to write 200 variable names down, as this is the
> real size of "powerdata".
> 
> Is there any way around that?
> 
> Christian
> 
> 
> 
> **********************************************************************
> * Christian Hennig Fachbereich Mathematik-SPST/ZMS, Universitaet
> Hamburg hennig at math.uni-hamburg.de,
> http://www.math.uni-hamburg.de/home/hennig/
> ######################################################################
> # ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From hodgess at gator.uhd.edu  Fri Apr 16 18:01:10 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Fri, 16 Apr 2004 11:01:10 -0500
Subject: [R] regression and dw
Message-ID: <200404161601.i3GG1Ak12767@gator.dt.uh.edu>

Dear R People:

Suppose we have a regression model that we will call
y.lm

We run the Durbin Watson test for autocorrelation
and we find that there is positive autocorrelation,
and phi = 0.72, say.

What is our next step, please?  

Do we calculate the following
yprime_t = y_t - 0.72y_t-1,
x1prime_t = x1_t - 0.72x1_t-1,

and so on, and re-fit the linear mode?

I haven't done this in a while.  Just wanted to double check.

Thanks so much!
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From Benjamin.STABLER at odot.state.or.us  Fri Apr 16 18:09:29 2004
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Fri, 16 Apr 2004 09:09:29 -0700
Subject: [R] Turning windows screen buffering on and off
Message-ID: <76A000A82289D411952F001083F9DD06047FE5A3@exsalem4-bu.odot.state.or.us>

Well, I too would like to be able to set buffered output to false for Rgui
Windows without user intervention.  Maybe it could be set via the Edit ->
Gui Preferences so that it can be saved and set at startup.  The GUI
Preferences are part of the interface and not the standard language
definition right?...so that seems to be a good spot to set something like
that.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104

---------------------------------

Message: 75
Date: Fri, 16 Apr 2004 08:59:43 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: RE: [R] Turning windows screen buffering on and off
To: <Toby.Patterson at csiro.au>
Cc: R-help at stat.math.ethz.ch
Message-ID: <Pine.GSO.4.31.0404160852270.16317-100000 at toucan.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

No, and options() really is part of the R/S language not the interface.

See the rw-FAQ Q6.3 for how to manage the buffering more effectively.
(Hint: you need to put calls to flush.console() in your code.)


On Fri, 16 Apr 2004 Toby.Patterson at csiro.au wrote:

> I meant via a function or something like:
>
> options( buffered.output = FALSE)
>
> Sorry, I should have made that clearer.
>
> Cheers
> Toby
>
>
>
> -----Original Message-----
> From: Roger D. Peng [mailto:rpeng at jhsph.edu]
> Sent: Friday, April 16, 2004 1:11 PM
> To: Patterson, Toby (Marine, Hobart)
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Turning windows screen buffering on and off
>
> Ctrl-W.
>
> -roger
>
> Toby.Patterson at csiro.au wrote:
> > All,
> >
> > Does anyone know if there is an option I can set to turn
> screen-buffered
> > output on and off with the win32 rgui? (Apart from the point and click
> > method).
> >
> > I am running some simulations where it is useful to watch output but
> it
> > gets mildly tiresome having to manually switch things on and off via
> the
> > gui.
> >
> > Thanks
> >
> > Toby.
> >
> >
> >>version
> >
> >          _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    1
> > minor    8.1
> > year     2003
> > month    11
> > day      21
> > language R
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lizzy at noradd.org  Fri Apr 16 18:09:33 2004
From: lizzy at noradd.org (etb)
Date: 16 Apr 2004 11:09:33 -0500
Subject: [R] Solving Matrices
In-Reply-To: "Richard A. O'Keefe"'s message of "Fri,
	16 Apr 2004 12:08:51 +1200 (NZST)"
References: <200404160008.i3G08pYO261268@atlas.otago.ac.nz>
Message-ID: <87brlr3nwi.fsf@liliwhite.renaissance.oasis>

"Richard" writes:

[ ... ]
>
> So here are three different functions all saying much the same thing
> about x: it is numerically close to a matrix which does NOT span the
> whole of R**4.

Theorem 4 (in my text):

	Let A be an m x n matrix. Then the following statements are
	logically equivalent. That is, for a particular A, either they
	are all true statements or they are all false:

	  a) For each b in R**m, the equation Ax = b has a solution.
	  b) The columns of A span R**m.
	  c) A has a pivot position in every row.

The back of the book is in agreement with you but I can manipulate the
matrix to produce (c) above which should mean that (b) is correct:

> x <- matrix(data=c(7,-5,6,-7,2,-3,10,9,-5,4,-2,2,8,-9,7,15),nrow=4)
> rowEchelonForm(x)
     [,1] [,2] [,3] [,4]
[1,]    1    0    0    0
[2,]    0    1    0    0
[3,]    0    0    1    0
[4,]    0    0    0    1

Likewise, I can produce the above using the swap(), scale(), gauss()
and bgauss() functions that the author of the text originally wrote
for maple, Mathematica, etc. but from all I can tell, the row echelon
form of matrix x meets the criteria of theorem 4 above, yet the columns
of x do not span R**4.

Using the gauss()[1] function:

     [,1]      [,2]       [,3]          [,4]
[1,]    7  2.000000 -5.0000000  8.000000e+00
[2,]    0 -1.571429  0.4285714 -3.285714e+00
[3,]    0  0.000000  4.5454545 -1.718182e+01
[4,]    0  0.000000  0.0000000 -5.035972e-15

Again, this has a pivot column in each row although [4,4] seems a bit
suspicious.

Elizabeth

[1]:

gauss <- function(A, r, v = c()) {
  m <- dim(A)[1]
  n <- dim(A)[2]
  if(length(r) > 1 | r < 1 | r > m)
    stop("The second entry in gauss(...) must be the pivot row index.")
  if(length(v) == 0) {
    if(r+1 > m)
      v <- m
    else
      v <- (r+1):m
  }
  for(j in v) {
    if(j == r)
      stop("Pivot ow cannot change itself.")
  }

  col <- 1
  while(A[r, col] == 0 && col < n)
    col <- col + 1
  if(col == n && A[r, col] == 0)
    stop("Row r has only zeroes and cannot be a pivot row.")
  for(j in v) {
    multiplier <- -A[j, col]/A[r, col]
    A[j,] <- A[j,] + multiplier * A[r,]
  }
  A
}



From ripley at stats.ox.ac.uk  Fri Apr 16 18:22:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 16 Apr 2004 17:22:15 +0100 (BST)
Subject: [R] regression and dw
In-Reply-To: <200404161601.i3GG1Ak12767@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.44.0404161720330.18068-100000@gannet.stats>

On Fri, 16 Apr 2004, Erin Hodgess wrote:

> Dear R People:
> 
> Suppose we have a regression model that we will call
> y.lm
> 
> We run the Durbin Watson test for autocorrelation
> and we find that there is positive autocorrelation,
> and phi = 0.72, say.
> 
> What is our next step, please?  

Look at the residuals more closely, e.g. look at the acf.

> Do we calculate the following
> yprime_t = y_t - 0.72y_t-1,
> x1prime_t = x1_t - 0.72x1_t-1,
> 
> and so on, and re-fit the linear mode?

Better to use arima with AR residuals and an xreg matrix.  

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From olau at fas.harvard.edu  Fri Apr 16 18:48:31 2004
From: olau at fas.harvard.edu (Olivia Lau)
Date: Fri, 16 Apr 2004 12:48:31 -0400 (EDT)
Subject: [R] install.packages
In-Reply-To: <Pine.LNX.4.58.0404161233300.3101@ls02.fas.harvard.edu>
References: <200404161000.i3GA0UON012259@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0404161233300.3101@ls02.fas.harvard.edu>
Message-ID: <Pine.LNX.4.58.0404161240380.7671@ls02.fas.harvard.edu>

install.packages works for Linux because the directory structure is not
version dependent, as it is for the Windows download.  That should be
fixed as soon as my sys admin consents to install R 1.9.0 so we
can do the cross compile.

Of course, you are right, Martin and Peter, that Zelig should be on CRAN
proper.  We just need to write a few .Rd files to be compliant with R
check, and change our documentation to reflect these new installation
instructions.  As you can see, the Zelig documentation currently follows
the model rather than the command, and this needs to be revised to pass
R check in a meaningful way.  In the meanwhile, Windows users can download
the zip file and do a local installation.  I'm sorry about the confusion.

One question about the CRAN submission process:  Once the package is on
CRAN, how frequently can we update the package (release a new version)?
Is it basically continuous?  I read R-ext carefully, but it doesn't seem
to say.

Thanks,

Olivia Lau
Zelig[[3]]

> > >>>>> "R" == R Heberto Ghezzo <heberto.ghezzo at mcgill.ca>
> > >>>>>     on Thu, 15 Apr 2004 12:16:03 -0400 writes:
> >
> >     R> Hello, I just downloaded RW1090. No problems. My thanks
> >     R> to everybody involved in the project. I work in Win98 I
> >     R> updated my library, some problems with some files that
> >     R> were in the PACKAGES list but not in 1.9/ site, now all
> >     R> are.  I tried to install "Zelig" from Harvard
> >     R> install.packages("Zelig",CRAN="http://gking.harvard.edu")
> >     R> this worked in 1.8.1 but now it appends
> >     R> "bin/windows/contrib/1.9" to the address and of course it
> >     R> can not find the file and aborts with error 404 Is there
> >     R> a way around besides http'ing directly to harvard and
> >     R> getting the zip and unzipping it in /library?
> >
> > To my astonishment, it actually works for me (in Linux)
> > with the source package...
> >
> > OTOH, gking.harvard.edu is not really a CRAN mirror, and "GKing"
> > has probably not yet created the  bin/windows/contrib/1.9/
> > directory and its contents.
> > As a matter of fact, I believe he should rather submit his
> > package to CRAN proper and you and he wouldn't have to deal with
> > such things as creating now directories when a the version of R
> > is bumped up.
> >
> > As he says on the Zelig web pages, the easy alternative is to
> > download the zip file and install that from the Package menu on
> > Windows.
> >
> > Regards,
> > Martin



From spencer.graves at pdf.com  Fri Apr 16 18:50:25 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 16 Apr 2004 09:50:25 -0700
Subject: [R] Specification of model formulae
In-Reply-To: <407FC888.19314.B9FBB1@localhost>
References: <407FC888.19314.B9FBB1@localhost>
Message-ID: <40800ED1.9070405@pdf.com>

      I often use the following extension of Christian's suggestion: 

 > nam <- paste("var", 1:5, sep="")
 > nam
[1] "var1" "var2" "var3" "var4" "var5"
 > nam. <- paste(nam, collapse="+")
 > nam.
[1] "var1+var2+var3+var4+var5"
 > nam2 <- paste("I(", nam, "^2)", sep="")
 > nam2
[1] "I(var1^2)" "I(var2^2)" "I(var3^2)" "I(var4^2)" "I(var5^2)"
 > nam2. <- paste(nam2, collapse="+")
 > nam2.
[1] "I(var1^2)+I(var2^2)+I(var3^2)+I(var4^2)+I(var5^2)"
 > mdl2 <- paste("y", "~ (", nam., ")^2 +", nam2.)
 > mdl2
[1] "y ~ ( var1+var2+var3+var4+var5 )^2 + 
I(var1^2)+I(var2^2)+I(var3^2)+I(var4^2)+I(var5^2)"
 > formula(mdl2)
y ~ (var1 + var2 + var3 + var4 + var5)^2 + I(var1^2) + I(var2^2) +
    I(var3^2) + I(var4^2) + I(var5^2)

      This will fit a full quadratic in the 5 variables. 

      If Uwe's solution won't work for you, this can be modified to work 
with any vector of appropriate variable names. 
      hope this helps. 
      spencer graves

kjetil at entelnet.bo wrote:

>On 16 Apr 2004 at 17:08, Christian Hennig wrote:
>
>You can build your formula along the ways of the following example, 
>without typing the 200 names:
>
>  
>
>>nam <- paste("var", 1:5, sep="", collapse="+")
>>nam
>>    
>>
>[1] "var1+var2+var3+var4+var5"
>  
>
>>nam <- paste("y", nam, sep="~")
>>nam
>>    
>>
>[1] "y~var1+var2+var3+var4+var5"
>  
>
>>as.formula(nam)
>>    
>>
>y ~ var1 + var2 + var3 + var4 + var5
>
>Kjetil Halvorsen
>
>  
>
>>Hi,
>>
>>is it true that the only way to tell rpart which variables to use is
>>to specify every single variable in the formula and separate them with
>>"+"?
>>
>>I wonder if this error...
>>
>>    
>>
>>>rotree <- rpart(powerdata[,1]~powerdata[,2:3],
>>>      
>>>
>>+                      method="class")
>>Error in model.frame(formula, rownames, variables, varnames, extras,
>>extranames,  : 
>> invalid variable type
>>
>>...can be prevented by other means than...
>>
>>    
>>
>>>rotree <- rpart(segments~V1.3+V1.9,
>>>      
>>>
>>+                      method="class",data=powerdata)
>>
>>This works.
>>(The variables here are powerdata[,1] and [,2:3].)
>>
>>But I do not want to write 200 variable names down, as this is the
>>real size of "powerdata".
>>
>>Is there any way around that?
>>
>>Christian
>>
>>
>>
>>**********************************************************************
>>* Christian Hennig Fachbereich Mathematik-SPST/ZMS, Universitaet
>>Hamburg hennig at math.uni-hamburg.de,
>>http://www.math.uni-hamburg.de/home/hennig/
>>######################################################################
>># ich empfehle www.boag-online.de
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From maechler at stat.math.ethz.ch  Fri Apr 16 18:52:06 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 16 Apr 2004 18:52:06 +0200
Subject: [R] install.packages
In-Reply-To: <Pine.LNX.4.58.0404161240380.7671@ls02.fas.harvard.edu>
References: <200404161000.i3GA0UON012259@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0404161233300.3101@ls02.fas.harvard.edu>
	<Pine.LNX.4.58.0404161240380.7671@ls02.fas.harvard.edu>
Message-ID: <16512.3894.738727.182087@gargle.gargle.HOWL>

>>>>> "Olivia" == Olivia Lau <olau at fas.harvard.edu>
>>>>>     on Fri, 16 Apr 2004 12:48:31 -0400 (EDT) writes:

	  .........

    Olivia> One question about the CRAN submission process: Once
    Olivia> the package is on CRAN, how frequently can we update
    Olivia> the package (release a new version)?  Is it
    Olivia> basically continuous?  I read R-ext carefully, but
    Olivia> it doesn't seem to say.

Well, the CRAN maintainers (at TU Wien) have to do manual work
for each submission; but since they
``basically work continuously''  :-) :-) 
you can update accordingly ;-)

Regards,
Martin



From kjetil at entelnet.bo  Fri Apr 16 19:21:13 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 16 Apr 2004 13:21:13 -0400
Subject: [R] Problem with breakpoints (strucchange)
Message-ID: <407FDDC9.5432.10D01C8@localhost>

Hola!

I am using package strucchange, and encounters the following:

>  bp <- breakpoints(diesel90  ~ regress -1, h=NULL)
Error in La.chol2inv(x, size) : element (14, 14) is zero, so the 
inverse cannot be computed

The obvious problems have been checked, that is, the model matrix is 
of full rank. What can be causing this?

I can send some data if that can be of help.

Kjetil Halvorsen



From Achim.Zeileis at wu-wien.ac.at  Fri Apr 16 19:15:53 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 16 Apr 2004 19:15:53 +0200
Subject: [R] Problem with breakpoints (strucchange)
In-Reply-To: <407FDDC9.5432.10D01C8@localhost>
References: <407FDDC9.5432.10D01C8@localhost>
Message-ID: <20040416191553.7a60f388.Achim.Zeileis@wu-wien.ac.at>

On Fri, 16 Apr 2004 13:21:13 -0400 kjetil at entelnet.bo wrote:

> Hola!
> 
> I am using package strucchange, and encounters the following:
> 
> >  bp <- breakpoints(diesel90  ~ regress -1, h=NULL)
> Error in La.chol2inv(x, size) : element (14, 14) is zero, so the 
> inverse cannot be computed
> 
> The obvious problems have been checked, that is, the model matrix is 
> of full rank. What can be causing this?

For segmenting the model each sub-model you are fitting has to have a
regressor matrix of full rank. Is `regress' a factor? If so, the minimal
segment size h has to be large enough that the corresponding
coefficients can be estimated in each (potential) segment.

> I can send some data if that can be of help.

If you cannot solve the problem, you can also contact me off-list.

hth,
Z

> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From dfederman at mco.edu  Fri Apr 16 19:32:16 2004
From: dfederman at mco.edu (Douglas Federman)
Date: Fri, 16 Apr 2004 13:32:16 -0400
Subject: [R] changing display of a dataframe
Message-ID: <s07fe06a.034@mail.mco.edu>

I have the following dataframe "s"

   name jul  aug  sep
1  Joe   m1  m2  m3
2  Bill    m2  m3  m2
3  Sue  m3  m1  m1

I am interested in transforming the data into the following:

        jul  aug  sep
m1   Joe Sue Sue
m2  Bill    Joe Bill
m3  Sue  Bill   Joe

Is there a simple way to do this?  

--
"It has often been remarked that an educated man has probably forgotten most of the facts he acquired in school and university. Education is what survives when what has been learned has been forgotten."  -- B.F. Skinner, New Scientist, 31 May 1964, p. 484

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Douglas Federman.vcf
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040416/d892a645/DouglasFederman.pl

From jfox at mcmaster.ca  Fri Apr 16 19:59:59 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 16 Apr 2004 13:59:59 -0400
Subject: [R] Solving Matrices
In-Reply-To: <87brlr3nwi.fsf@liliwhite.renaissance.oasis>
Message-ID: <web-32031664@cgpsrv2.cis.mcmaster.ca>

Dear Elizabeth,

If this is the rowEchelonForm function that I posted to r-help some
time ago, as I mentioned in a subsequent post, the tolerance was set
too low; if you set the tol argument (better, the default) to a larger
number than the machine precision, then the function should work more
reliably.

I hope this helps,
 John

On 16 Apr 2004 11:09:33 -0500
 etb <lizzy at noradd.org> wrote:
> "Richard" writes:
> 
> [ ... ]
> >
> > So here are three different functions all saying much the same
> thing
> > about x: it is numerically close to a matrix which does NOT span
> the
> > whole of R**4.
> 
> Theorem 4 (in my text):
> 
> 	Let A be an m x n matrix. Then the following statements are
> 	logically equivalent. That is, for a particular A, either they
> 	are all true statements or they are all false:
> 
> 	  a) For each b in R**m, the equation Ax = b has a solution.
> 	  b) The columns of A span R**m.
> 	  c) A has a pivot position in every row.
> 
> The back of the book is in agreement with you but I can manipulate
> the
> matrix to produce (c) above which should mean that (b) is correct:
> 
> > x <- matrix(data=c(7,-5,6,-7,2,-3,10,9,-5,4,-2,2,8,-9,7,15),nrow=4)
> > rowEchelonForm(x)
>      [,1] [,2] [,3] [,4]
> [1,]    1    0    0    0
> [2,]    0    1    0    0
> [3,]    0    0    1    0
> [4,]    0    0    0    1
> 
> Likewise, I can produce the above using the swap(), scale(), gauss()
> and bgauss() functions that the author of the text originally wrote
> for maple, Mathematica, etc. but from all I can tell, the row echelon
> form of matrix x meets the criteria of theorem 4 above, yet the
> columns
> of x do not span R**4.
> 
> Using the gauss()[1] function:
> 
>      [,1]      [,2]       [,3]          [,4]
> [1,]    7  2.000000 -5.0000000  8.000000e+00
> [2,]    0 -1.571429  0.4285714 -3.285714e+00
> [3,]    0  0.000000  4.5454545 -1.718182e+01
> [4,]    0  0.000000  0.0000000 -5.035972e-15
> 
> Again, this has a pivot column in each row although [4,4] seems a bit
> suspicious.
> 
> Elizabeth
> 
> [1]:
> 
> gauss <- function(A, r, v = c()) {
>   m <- dim(A)[1]
>   n <- dim(A)[2]
>   if(length(r) > 1 | r < 1 | r > m)
>     stop("The second entry in gauss(...) must be the pivot row
> index.")
>   if(length(v) == 0) {
>     if(r+1 > m)
>       v <- m
>     else
>       v <- (r+1):m
>   }
>   for(j in v) {
>     if(j == r)
>       stop("Pivot ow cannot change itself.")
>   }
> 
>   col <- 1
>   while(A[r, col] == 0 && col < n)
>     col <- col + 1
>   if(col == n && A[r, col] == 0)
>     stop("Row r has only zeroes and cannot be a pivot row.")
>   for(j in v) {
>     multiplier <- -A[j, col]/A[r, col]
>     A[j,] <- A[j,] + multiplier * A[r,]
>   }
>   A
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From ggrothendieck at myway.com  Fri Apr 16 20:40:03 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 16 Apr 2004 18:40:03 +0000 (UTC)
Subject: [R] changing display of a dataframe
References: <s07fe06a.034@mail.mco.edu>
Message-ID: <loom.20040416T202752-433@post.gmane.org>


Read in the data noting that the row names are in column "name" and 
interpreting character data as character type.

The sapply loops over columns creating factors with the same levels (in 
the same order), orders them and uses that to index the row.names.  Finally 
we create row names for the result out of the levels.

d <- read.table("clipboard", header=T, row.names="name", as.is=T)
lev <- sort(d[,1])
d2 <- sapply( d, function(x) row.names(d)[ order( factor(x,levels=lev) ) ] )
row.names(d2) <- lev



Douglas Federman <dfederman <at> mco.edu> writes:

: 
: I have the following dataframe "s"
: 
:    name jul  aug  sep
: 1  Joe   m1  m2  m3
: 2  Bill    m2  m3  m2
: 3  Sue  m3  m1  m1
: 
: I am interested in transforming the data into the following:
: 
:         jul  aug  sep
: m1   Joe Sue Sue
: m2  Bill    Joe Bill
: m3  Sue  Bill   Joe
: 
: Is there a simple way to do this?  
: 
: --
: "It has often been remarked that an educated man has probably forgotten most 
of the facts he acquired in
: school and university. Education is what survives when what has been learned 
has been forgotten."  -- B.F.
: Skinner, New Scientist, 31 May 1964, p. 484
: 
: 
: 
: BEGIN:VCARD
: VERSION:2.1
: X-GWTYPE:USER
: FN:Douglas Federman
: ORG:;1331
: EMAIL;WORK;PREF;NGW:dfederman <at> mco.edu
: N:Federman;Douglas
: END:VCARD
: 
: 
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From HStevens at MUOhio.edu  Fri Apr 16 22:35:15 2004
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Fri, 16 Apr 2004 16:35:15 -0400
Subject: [R] Spatial Voter Model
Message-ID: <91629C5E-8FE5-11D8-A496-000A958F43CC@MUOhio.edu>

Has anyone coded (in R) a spatial voter model with mutation (e.g., 
Kimura and Weiss 1964, Holley and Liggett 1975, Durrett and Levin 
1996)? In principle, it is quite straightforward, but useful 
simulations require many many iterations, making my "straightforward" 
version too time intensive. I am happy to share my version (without 
mutation, below), for what it is worth.

Thank you in advance,
Hank Stevens

# Voter model with no mutation in a square grid of size L^2
L = 50 #Dimension of square matrix (L=10^ or 10^4 would be nice...)
S = 2 # Number of species
loc=c(L,1:L,1) # possible neighbor locations
replacements = L^2 * 10 # iterations (hundreds * L^2  would be nice)
Lat <- matrix(sample(1:S,L^2, replace=TRUE),nrow=L) #Make an arena L X L
Lat0 <- Lat # Save original arena

   # select random rows where number of reps = number of cells in arena
   # and same for columns
ri <- sample(1:L, replacements, replace=TRUE)
rj <- sample(1:L, replacements, replace=TRUE)

#implement a voter model where each cell takes on the value of a 
randomly selected individual around it.

for(i in 1:replacements) Lat[ri[i],rj[i]] <- 
sample(c(Lat[loc[ri[i]],loc[rj[i]+1]],
                                                       
Lat[loc[ri[i]+1],loc[rj[i]+2]],
                                                       
Lat[loc[ri[i]+2],loc[rj[i]+1]],
                                                       
Lat[loc[ri[i]+1],loc[rj[i]]]), 1)

# Calculate Simpon's Diversity (Nei genetic heterozygozity)
1 - sum( (table(Lat0)/sum(table(Lat0)))^2)
1 - sum( (table(Lat)/sum(table(Lat)))^2)
layout(matrix(1:2, nrow=1)) # plot arenas
plot(rep(1:L,each=L),rep(1:L,L),col=c(Lat0),pch=20)
plot(rep(1:L,each=L),rep(1:L,L),col=c(Lat),pch=20)

Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/



From jana.mcpherson at zoology.oxford.ac.uk  Fri Apr 16 22:46:08 2004
From: jana.mcpherson at zoology.oxford.ac.uk (Jana McPherson)
Date: Fri, 16 Apr 2004 16:46:08 -0400
Subject: [R] autologistic regression with Gibbs sampler
Message-ID: <007f01c423f3$d8543980$6800a8c0@janamcpherson>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040416/f33ad3db/attachment.pl

From ritepie at hotmail.com  Fri Apr 16 23:52:50 2004
From: ritepie at hotmail.com (Joe Peterson)
Date: Fri, 16 Apr 2004 21:52:50 +0000
Subject: [R] Rmath
Message-ID: <BAY99-F66d22RQY3Whk0000b97f@hotmail.com>

Dear all,

I have been trying to build standalone Rmath library for irix 6.5, but, 
could not get it due to some reasons, if anyone would like to share already 
built ones, please let me know, I appreciate all of your help.

Sincerely;

Scott



From chris_ciotti at yahoo.com  Sat Apr 17 01:14:29 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Fri, 16 Apr 2004 19:14:29 -0400
Subject: [R] t.test & formatting question
Message-ID: <408068D5.4080901@yahoo.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello all -


I'm trying to format some data where I only need one of the values 
returned from a test, say a t-test in this instance.  I have the following:

 > R.version.string
[1] "R version 1.9.0, 2004-04-12"
 > x <- rnorm(1001)
 > t.test(x)$statistic
        t
0.5555321
 >

Any easy way (other than straight calculation) to get the t-statistic 
w/o the 't' above it?  TIA

- --
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7


-----BEGIN PGP SIGNATURE-----
Version: PGP 8.0

iQA/AwUBQIBotlkgIqbpS7O3EQIt7ACdEhhoM3tqqmDae71Rdo/eHg2TvoQAoLSw
8QjwkHENOjjs5mzCWlXSTQXO
=LvW5
-----END PGP SIGNATURE-----



From tplate at blackmesacapital.com  Sat Apr 17 01:19:44 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 16 Apr 2004 17:19:44 -0600
Subject: [R] t.test & formatting question
In-Reply-To: <408068D5.4080901@yahoo.com>
References: <408068D5.4080901@yahoo.com>
Message-ID: <6.1.0.6.2.20040416171905.03f1fc68@mailhost.blackmesacapital.com>

as.numeric() (and its siblings) strip the names from vectors, e.g.:

 > as.numeric(t.test(rnorm(1001))$statistic)
[1] -0.6320304
 >

hth,

Tony Plate

At Friday 05:14 PM 4/16/2004, christopher ciotti wrote:
>-----BEGIN PGP SIGNED MESSAGE-----
>Hash: SHA1
>
>Hello all -
>
>
>I'm trying to format some data where I only need one of the values 
>returned from a test, say a t-test in this instance.  I have the following:
>
> > R.version.string
>[1] "R version 1.9.0, 2004-04-12"
> > x <- rnorm(1001)
> > t.test(x)$statistic
>        t
>0.5555321
> >
>
>Any easy way (other than straight calculation) to get the t-statistic w/o 
>the 't' above it?  TIA
>
>- --
>chris ciotti (chris_ciotti at yahoo.com)
>PGP ID: 0xE94BB3B7
>
>
>-----BEGIN PGP SIGNATURE-----
>Version: PGP 8.0
>
>iQA/AwUBQIBotlkgIqbpS7O3EQIt7ACdEhhoM3tqqmDae71Rdo/eHg2TvoQAoLSw
>8QjwkHENOjjs5mzCWlXSTQXO
>=LvW5
>-----END PGP SIGNATURE-----
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chris_ciotti at yahoo.com  Sat Apr 17 01:21:31 2004
From: chris_ciotti at yahoo.com (christopher ciotti)
Date: Fri, 16 Apr 2004 19:21:31 -0400
Subject: [R] t.test & formatting question
In-Reply-To: <6.1.0.6.2.20040416171905.03f1fc68@mailhost.blackmesacapital.com>
References: <408068D5.4080901@yahoo.com>
	<6.1.0.6.2.20040416171905.03f1fc68@mailhost.blackmesacapital.com>
Message-ID: <40806A7B.3030808@yahoo.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Tony Plate wrote:

 > as.numeric() (and its siblings) strip the names from vectors, e.g.:
 >
 > > as.numeric(t.test(rnorm(1001))$statistic)
 > [1] -0.6320304
 > >
 >
 > hth,
 >
 > Tony Plate


Thanks for the quick response.

- --
chris ciotti (chris_ciotti at yahoo.com)
PGP ID: 0xE94BB3B7


-----BEGIN PGP SIGNATURE-----
Version: PGP 8.0

iQA/AwUBQIBqXlkgIqbpS7O3EQKNRwCfREBcPiBQDxIKcnUD6uVJyt5YP/wAnjBR
l+C3+RSz2r0juvBZ87DTyyRI
=S8Mz
-----END PGP SIGNATURE-----



From ccleland at optonline.net  Sat Apr 17 01:21:11 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 16 Apr 2004 19:21:11 -0400
Subject: [R] t.test & formatting question
In-Reply-To: <408068D5.4080901@yahoo.com>
References: <408068D5.4080901@yahoo.com>
Message-ID: <40806A67.6070001@optonline.net>

   How about this?

t.test(x)[[1]]

   The result of t.test(x) is a list and "statistic" is the first 
component of that list.

christopher ciotti wrote:
> ...
> I'm trying to format some data where I only need one of the values 
> returned from a test, say a t-test in this instance.  I have the following:
> 
>  > R.version.string
> [1] "R version 1.9.0, 2004-04-12"
>  > x <- rnorm(1001)
>  > t.test(x)$statistic
>        t
> 0.5555321
>  >
> 
> Any easy way (other than straight calculation) to get the t-statistic 
> w/o the 't' above it?  TIA
> ... 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ccleland at optonline.net  Sat Apr 17 01:25:20 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 16 Apr 2004 19:25:20 -0400
Subject: [R] t.test & formatting question
In-Reply-To: <408068D5.4080901@yahoo.com>
References: <408068D5.4080901@yahoo.com>
Message-ID: <40806B60.50108@optonline.net>

   Sorry, I should have checked that more closely.  I see that

t.test(rnorm(12))[[1]]

   retains the "t".

christopher ciotti wrote:
> ...
> I'm trying to format some data where I only need one of the values 
> returned from a test, say a t-test in this instance.  I have the following:
> 
>  > R.version.string
> [1] "R version 1.9.0, 2004-04-12"
>  > x <- rnorm(1001)
>  > t.test(x)$statistic
>        t
> 0.5555321
>  >
> 
> Any easy way (other than straight calculation) to get the t-statistic 
> w/o the 't' above it?  TIA
> ...

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From rbaer at atsu.edu  Sat Apr 17 04:42:42 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Fri, 16 Apr 2004 21:42:42 -0500
Subject: [R] t.test & formatting question
References: <408068D5.4080901@yahoo.com> <40806B60.50108@optonline.net>
Message-ID: <003801c42425$a8adaa40$6401a8c0@meadow>

How about:
as.numeric(t.test(rnorm(12))[[1]])

----- Original Message ----- 
From: "Chuck Cleland" <ccleland at optonline.net>
To: "christopher ciotti" <chris_ciotti at yahoo.com>
Cc: <R-help at stat.math.ethz.ch>
Sent: Friday, April 16, 2004 6:25 PM
Subject: Re: [R] t.test & formatting question


>    Sorry, I should have checked that more closely.  I see that
>
> t.test(rnorm(12))[[1]]
>
>    retains the "t".
>
> christopher ciotti wrote:
> > ...
> > I'm trying to format some data where I only need one of the values
> > returned from a test, say a t-test in this instance.  I have the
following:
> >
> >  > R.version.string
> > [1] "R version 1.9.0, 2004-04-12"
> >  > x <- rnorm(1001)
> >  > t.test(x)$statistic
> >        t
> > 0.5555321
> >  >
> >
> > Any easy way (other than straight calculation) to get the t-statistic
> > w/o the 't' above it?  TIA
> > ...
>
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From anataraj at cs.uoregon.edu  Sat Apr 17 08:10:52 2004
From: anataraj at cs.uoregon.edu (Aroon Nataraj)
Date: Fri, 16 Apr 2004 23:10:52 -0700
Subject: [R] Box-Ljung p-value -> Test for Independence
Message-ID: <4080CA6C.7040100@cs.uoregon.edu>

Hi all
I'm using the Box-Ljung test (from within R) to test if a time-series in 
independently distributed.

2 questions:
1) p-value returned by Box-Ljung:
IF I want to test if the time-series is independant at say 0.05 
sig-level (it means that prob of erroneously accepting that the 
time-series is independent is 0.05 right?)
--> then do I consider time-series as "independant" when
      --> p-value (from Box-Ljung) > 0.05
      OR
       --> p-value < 0.05
Or should i be using (0.95) instead of (0.05) for this case. I'm 
confused about this (this is a goodness-of-fit test?).

2) Box-Ljung takes a lag argument, say lag=k.
     Does it check "all lags upto k"
     OR
     Does it check "only AT k" (i.e acf val is small at only k?)

thank you in advance. I apologize if the questions are very basic.

Aroon



From tpapp at axelero.hu  Sat Apr 17 09:58:03 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sat, 17 Apr 2004 09:58:03 +0200
Subject: [R] Spatial Voter Model
In-Reply-To: <91629C5E-8FE5-11D8-A496-000A958F43CC@MUOhio.edu>
References: <91629C5E-8FE5-11D8-A496-000A958F43CC@MUOhio.edu>
Message-ID: <20040417075802.GA1039@localhost>

On Fri, Apr 16, 2004 at 04:35:15PM -0400, Martin Henry H. Stevens wrote:

>   # select random rows where number of reps = number of cells in arena
>   # and same for columns
> ri <- sample(1:L, replacements, replace=TRUE)
> rj <- sample(1:L, replacements, replace=TRUE)
> 
> #implement a voter model where each cell takes on the value of a 
> randomly selected individual around it.
> 
> for(i in 1:replacements) Lat[ri[i],rj[i]] <- 
> sample(c(Lat[loc[ri[i]],loc[rj[i]+1]],
>                                                       
> Lat[loc[ri[i]+1],loc[rj[i]+2]],
>                                                       
> Lat[loc[ri[i]+2],loc[rj[i]+1]],
>                                                       
> Lat[loc[ri[i]+1],loc[rj[i]]]), 1)

I don't think that you need a loop for this.  Look at the help page of
the [ operatot (type ?"[" at the prompt).  You need something like this:

## this gives you a row & column index matrix (elements to replace)
r <- matrix(sample(1:L, 2*replacements, replact=TRUE), replacements, 2)

## offset matrix - you only need to create this once
roff <- matrix(c(0,1,
		 1,2,
		 2,1,
		 1,1), 4, 2, byrow=TRUE)

## make random neighbor offset selection
n <- sample(1:nrow(roff), replacements, replace=TRUE)

## row & column offsets
roffij <- t(sapply(n, function (i) { roff[i,1:2] }))

## replace elements
Lat[r] <- Lat[r + roffij]

I haven't tested this, please check everything, and look at the help
pages for further information.

> # Calculate Simpon's Diversity (Nei genetic heterozygozity)
> 1 - sum( (table(Lat0)/sum(table(Lat0)))^2)
> 1 - sum( (table(Lat)/sum(table(Lat)))^2)

You might be able to speed this up by writing some C code, and calling
it in R.  It is only worth it if you have large matrices, and use this
kind of thing really often (and you can program C).  Maybe somebody
has a better suggestion.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From darkjacknife at hotmail.com  Sat Apr 17 13:20:57 2004
From: darkjacknife at hotmail.com (angel hellraiser)
Date: Sat, 17 Apr 2004 11:20:57 +0000
Subject: [R] about lme
Message-ID: <Sea1-F77vbQaPCgGweH000055a8@hotmail.com>

Dear R users:

I've a problem with lme function, when I want
to model an unbalanced two-way anova, with 2 random factors say t and b.

My two models are:

model1- y(ijk) = beta+b(i)+t(j)+epsilon(ijk)
model2- y(ijk)= beta+b(i)+t(j)+b:t(ij)+epsilon(ijk)

beta overall mean effect

The data.frame is X

t   b   med   celda

1  1    10      1
1  1    12      1
1  1    11      1
1  2    13      2
1  2    15      2
1  3    21      3
1  3    19      3
2  1    16      4
2  1    18      4
2  2    13      5
2  2    19      5
2  2    14      5
2  3    11      6
2  3    13      6

I try with lme to obtain the variance estimates like with varcomp, for 
model1
model-2, sum and interaction effects.
varcomp gives me:

variance estimates:
t                 0
b                0
t:b              7.407
residuals    3.8429

I try with lme:

x <- lme(med~ 1, data = X;random = ~ 1 | t+b
or   random = ~t+b | celda
      random = ~t*b | celda
      random = ~ 1 | t*b  ,  method = "ML")

I get "bad groupping" or "singular convergence".

Please, Can anyone tell me how to model, model-1 and model-2 in lme.

Do you know any library for S-PLUS, R  in order to get like in SPSS expected 
mean
squares if no library, how obtain in S-PLUS, R?

My e-mail is darkjacknife at hotmail.com

_________________________________________________________________
Protege tu correo del spam y los virus con MSN 8. Prueba gratis dos meses 
MSN 8. http://join.msn.com/?pgmarket=es-es&XAPID=199&DI=1055



From ajayshah at mayin.org  Sat Apr 17 13:43:25 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 17 Apr 2004 17:13:25 +0530
Subject: [R] Box-Ljung p-value -> Test for Independence
Message-ID: <20040417114325.GA8338@igidr.ac.in>

> I'm using the Box-Ljung test (from within R) to test if a
> time-series in independently distributed.

I have window$r.nifty, which is a time-series of returns on the Nifty
market index.

  # Box-Ljung test on Nifty --
  test = Box.test(window$r.nifty, lag=10, type="Ljung");
  cat("Box Ljung prob value: ", test$p.value, "\n")

  # Runs test on Nifty
  test = runs.test(factor(sign(window$r.nifty)));
  cat("Runs test prob value: ", test$p.value, "\n")

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From pburns at pburns.seanet.com  Sat Apr 17 15:33:15 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 17 Apr 2004 14:33:15 +0100
Subject: [R] Box-Ljung p-value -> Test for Independence
References: <4080CA6C.7040100@cs.uoregon.edu>
Message-ID: <4081321B.5090208@pburns.seanet.com>

1) A small p-value is evidence that there is dependence.
So you want to see large p-values.  But a large p-value
is not really evidence of independence -- merely a lack
of evidence of dependence.

You might be able to get a hint of the power of your test
(which is what you really care about) from the working
paper about Ljung-Box on the Burns Statistics website.

2) The statistic is really an average of the lags up to the
stated lag.  So if the dependence is all at lag 5, tests with
lags below 5 have no power, the lag 5 test has maximum
power, and the power decreases as the lag of the test
increases above 5.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Aroon Nataraj wrote:

> Hi all
> I'm using the Box-Ljung test (from within R) to test if a time-series 
> in independently distributed.
>
> 2 questions:
> 1) p-value returned by Box-Ljung:
> IF I want to test if the time-series is independant at say 0.05 
> sig-level (it means that prob of erroneously accepting that the 
> time-series is independent is 0.05 right?)
> --> then do I consider time-series as "independant" when
>      --> p-value (from Box-Ljung) > 0.05
>      OR
>       --> p-value < 0.05
> Or should i be using (0.95) instead of (0.05) for this case. I'm 
> confused about this (this is a goodness-of-fit test?).
>
> 2) Box-Ljung takes a lag argument, say lag=k.
>     Does it check "all lags upto k"
>     OR
>     Does it check "only AT k" (i.e acf val is small at only k?)
>
> thank you in advance. I apologize if the questions are very basic.
>
> Aroon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From apv at capital.net  Sat Apr 17 13:12:20 2004
From: apv at capital.net (Arend P. van der Veen)
Date: Sat, 17 Apr 2004 07:12:20 -0400
Subject: [R] GNOME 2.6 and R 1.8.1
Message-ID: <1082200340.17597.13.camel@localhost>

Hi,

I use R 1.8.1 (R PROJECT) on FreeBSD 4.9 P5 with GNOME 2.6.0.  When I 
install R it cannot find GNOME.  It does successfully find X11 and
tcltk.  It complains that it can not find:

gnome-config
gnomeConf.sh

When I run R from GNOME the graphic  windows do not have any title bars
and R terminal output gets corrupted.

When I run R from sawfish everything works great.

Has any body been able to successfully run R from gnome 2.6 under
FreeBSD ?

Thanks in advance
Arend van der Veen



From patrick.giraudoux at univ-fcomte.fr  Sat Apr 17 21:38:01 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 17 Apr 2004 21:38:01 +0200
Subject: [R] nlme - sum of squares - permutation test
Message-ID: <001101c424b3$8bd40390$be2ff951@PC728329681112>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040417/bf944044/attachment.pl

From cheney at soc.umass.edu  Sun Apr 18 00:40:33 2004
From: cheney at soc.umass.edu (Eric Cheney)
Date: Sat, 17 Apr 2004 18:40:33 -0400
Subject: [R] accessing log likelihood of poison model
Message-ID: <20040417224033.GA25643@sociostat.org>

Could someone tell me how to access the log likelihood 
of a poisson model?  I've done the following....

<BEGIN R STUFF>

freq.mod <- glm(formula = nfix ~ gls.gls + pol.gls + pol.rel + rac.gls +
rac.pol + rac.rac + rac.rel + white + gls.gls.w + pol.gls.w + pol.rel.w
+ rac.gls.w + rac.pol.w + rac.rac.w + rac.rac.w + rac.rel.w, family =
poisson, data = Complex2.freq, offset = lnoffset)

summary(freq.mod)
anova(freq.mod)

<END R STUFF>

And that's great; but I need the log likelihood.

Anyone know?

Thanks in advance?
-- 
<Eric Cheney>   cheney at soc.umass.edu        http://sociostat.org/eric/



From p.dalgaard at biostat.ku.dk  Sun Apr 18 00:52:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2004 00:52:19 +0200
Subject: [R] accessing log likelihood of poison model
In-Reply-To: <20040417224033.GA25643@sociostat.org>
References: <20040417224033.GA25643@sociostat.org>
Message-ID: <x2isfyi5ek.fsf@biostat.ku.dk>

Eric Cheney <cheney at soc.umass.edu> writes:

> Could someone tell me how to access the log likelihood 
> of a poisson model?  I've done the following....
> 
> <BEGIN R STUFF>
> 
> freq.mod <- glm(formula = nfix ~ gls.gls + pol.gls + pol.rel + rac.gls +
> rac.pol + rac.rac + rac.rel + white + gls.gls.w + pol.gls.w + pol.rel.w
> + rac.gls.w + rac.pol.w + rac.rac.w + rac.rac.w + rac.rel.w, family =
> poisson, data = Complex2.freq, offset = lnoffset)
> 
> summary(freq.mod)
> anova(freq.mod)
> 
> <END R STUFF>
> 
> And that's great; but I need the log likelihood.
> 
> Anyone know?

The deviance will not suffice? 

  sum(dpois(nfix, fitted(freq.mod), log.p=T))

should do the trick otherwise.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From cheney at soc.umass.edu  Sun Apr 18 02:31:10 2004
From: cheney at soc.umass.edu (Eric Cheney)
Date: Sat, 17 Apr 2004 20:31:10 -0400
Subject: [R] accessing log likelihood of poison model
In-Reply-To: <x2isfyi5ek.fsf@biostat.ku.dk>
References: <20040417224033.GA25643@sociostat.org>
	<x2isfyi5ek.fsf@biostat.ku.dk>
Message-ID: <20040418003110.GA26102@sociostat.org>

On Sunday, 18 April 2004 at  0:52:19 +0200, Peter Dalgaard wrote:
> Eric Cheney <cheney at soc.umass.edu> writes:
> 
> > Could someone tell me how to access the log likelihood 
> > of a poisson model?  I've done the following....
> > 
> > <BEGIN R STUFF>
> > 
> > freq.mod <- glm(formula = nfix ~ gls.gls + pol.gls + pol.rel + rac.gls +
> > rac.pol + rac.rac + rac.rel + white + gls.gls.w + pol.gls.w + pol.rel.w
> > + rac.gls.w + rac.pol.w + rac.rac.w + rac.rac.w + rac.rel.w, family =
> > poisson, data = Complex2.freq, offset = lnoffset)
> > 
> > summary(freq.mod)
> > anova(freq.mod)
> > 
> > <END R STUFF>
> > 
> > And that's great; but I need the log likelihood.
> > 
> > Anyone know?
> 
> The deviance will not suffice? 
> 
>   sum(dpois(nfix, fitted(freq.mod), log.p=T))
> 
> should do the trick otherwise.

Thank you, that did the trick.  I should note that the method doesn't 
work when I have a noninteger in Y.  I'm doing a "log linear" analysis
of a cross table, and one of the cells of the cross table has a zero
value.  I put a .5 in that cell.  With the ".5 analysis" the above
method doesn't work.  So I had to revert to an analysis with a zero
cell to make it work. Which has some problems.  So I'm still left
looking for the log likelihood of the ".5 analysis".  I've had success
with Stata using this method, but not R.

Thanks again.
-- 
<Eric Cheney>   cheney at soc.umass.edu        http://sociostat.org/eric/



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 18 03:27:50 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 18 Apr 2004 02:27:50 +0100 (BST)
Subject: [R] lm with data=(means,sds,ns)
Message-ID: <XFMail.040418022750.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I am dealing with data which have been presented as

  at each x_i, mean m_i of the y-values at x_i,
               sd s_i of the y-values at x_i
               number n_i of the y-values at x_i

and I want to linearly regress y on x.

There does not seem to be an option to 'lm' which can
deal with such data directly, though the regression
problem could be algebraically expressed in these terms.

One way of fudging it would be to replace each m_i by
a set  of n_i numbers Y_i constructed as

  u_i <- rnorm(ni)

  Y_i <- m_i + s_i*(u_i - mean(u_i))/sd(u_i)

and associate these with X_i <- rep(x_i,n_i), thereby
constructing a regression-equivalent set of pseudo "raw data"
which could be fed to lm(Y~X). However, this strikes me as
cumbersome, at least, and even ugly!

Is there a direct way to go from {(n_i,m_i,s_i)} to the
fitted regression, with summaries and all (and use of 'predict')?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Apr-04                                       Time: 02:27:50
------------------------------ XFMail ------------------------------



From renaud.lancelot at cirad.fr  Sun Apr 18 08:16:19 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 18 Apr 2004 09:16:19 +0300
Subject: [R] accessing log likelihood of poison model
In-Reply-To: <20040418003110.GA26102@sociostat.org>
References: <20040417224033.GA25643@sociostat.org>	<x2isfyi5ek.fsf@biostat.ku.dk>
	<20040418003110.GA26102@sociostat.org>
Message-ID: <40821D33.1070600@cirad.fr>

Eric Cheney wrote:

> On Sunday, 18 April 2004 at  0:52:19 +0200, Peter Dalgaard wrote:
> 
>>Eric Cheney <cheney at soc.umass.edu> writes:
>>
>>
>>>Could someone tell me how to access the log likelihood 
>>>of a poisson model?  I've done the following....
>>>
>>><BEGIN R STUFF>
>>>
>>>freq.mod <- glm(formula = nfix ~ gls.gls + pol.gls + pol.rel + rac.gls +
>>>rac.pol + rac.rac + rac.rel + white + gls.gls.w + pol.gls.w + pol.rel.w
>>>+ rac.gls.w + rac.pol.w + rac.rac.w + rac.rac.w + rac.rel.w, family =
>>>poisson, data = Complex2.freq, offset = lnoffset)
>>>
>>>summary(freq.mod)
>>>anova(freq.mod)
>>>
>>><END R STUFF>
>>>
>>>And that's great; but I need the log likelihood.
>>>
>>>Anyone know?
>>
>>The deviance will not suffice? 
>>
>>  sum(dpois(nfix, fitted(freq.mod), log.p=T))
>>
>>should do the trick otherwise.
> 
> 
> Thank you, that did the trick.  I should note that the method doesn't 
> work when I have a noninteger in Y.  I'm doing a "log linear" analysis
> of a cross table, and one of the cells of the cross table has a zero
> value.  I put a .5 in that cell.  With the ".5 analysis" the above
> method doesn't work.  So I had to revert to an analysis with a zero
> cell to make it work. Which has some problems.  So I'm still left
> looking for the log likelihood of the ".5 analysis".  I've had success
> with Stata using this method, but not R.
> 
> Thanks again.

?logLik

example from ?glm:

 > counts <- c(18,17,15,20,10,20,25,13,12)
 > outcome <- gl(3,1,9)
 > treatment <- gl(3,3)
 > print(d.AD <- data.frame(treatment, outcome, counts))
   treatment outcome counts
1         1       1     18
2         1       2     17
3         1       3     15
4         2       1     20
5         2       2     10
6         2       3     20
7         3       1     25
8         3       2     13
9         3       3     12
 > glm.D93 <- glm(counts ~ outcome + treatment, family=poisson())
 >
 > logLik(glm.D93)
`log Lik.' -23.38066 (df=5)

Best,

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 04 824 55 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From wildscop at yahoo.com  Sun Apr 18 09:28:41 2004
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Sun, 18 Apr 2004 13:28:41 +0600
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
Message-ID: <5.1.0.14.2.20040418132149.02452a30@mail.dhaka.net>

Dear James Wettenhall,

Your question - why do i need nonlinear regression for that model when it 
is linear after taking logs - is not a dumb question: rather it is a 
rational one. Actually C.E.S Production Function [ Y = GAMA * 
((DELTA*K^(-BETA)) + ((1-DELTA)*L^(-BETA)))^(-PHI/BETA) ] is my main 
concern (In this case there is no way to linearize it), the Cobb-Douglas 
being just a 'Toy problem' to see how non-linear process works. And i'm 
sorry that i cannot guess some approximate parameter values for that CES 
using some "typical" Y,L,K data : that why it is a problem (doing a grid 
search over infinite parameter space is indeed time consuming).


Thanks in advance for your time and effort - and sorry for my late reply.
_______________________

Mohammad Ehsanul Karim <appstat at HotPOP.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From wildscop at yahoo.com  Sun Apr 18 09:40:21 2004
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Sun, 18 Apr 2004 13:40:21 +0600
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
Message-ID: <5.1.0.14.2.20040418133141.024500f0@mail.dhaka.net>

Dear Sundar Dorai-Raj,

Thank you very much for mentioning to exponentiate ALPHA.

However, so far i understand that the parameters in the non-linear equation
Y = ALPHA * (L^(BETA1)) * (K^(BETA2))
and the coefficients of log(L) and log(K) of the following equation (after 
linearizing)
log(Y) = log(ALPHA) +(BETA1)*log(L) + (BETA2)*log(K)
should be the same when estimated from either equation. Is it true? If it 
is, then why the estimates of the two procedure (see below) are different? 
Can you please explain it?
-----------------------------
 > coef(lm(log(Y)~log(L)+log(K), data=klein.data))

(Intercept)      log(L)      log(K)
  -3.6529493   1.0376775   0.7187662
-----------------------------
 > nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), data=klein.data, start = 
c(ALPHA=exp(-3.6529493),BETA1=1.0376775,BETA2 = 0.7187662), trace = TRUE)

Nonlinear regression model
   model:  Y ~ ALPHA * (L^(BETA1)) * (K^(BETA2))
    data:  klein.data
       ALPHA       BETA1       BETA2
0.003120991 0.414100040 1.513546235
  residual sum-of-squares:  3128.245
-----------------------------

Thanks in advance for your time and effort - and sorry for my late reply.
_______________________

Mohammad Ehsanul Karim <appstat at HotPOP.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh



From tg.tatar at verizon.net  Sun Apr 18 09:47:21 2004
From: tg.tatar at verizon.net (Timothy Tatar)
Date: Sun, 18 Apr 2004 00:47:21 -0700
Subject: [R] R-1.9.0: make error on slackware-current!
Message-ID: <000001c42519$61ed5390$6401a8c0@eagle>

It appears that the #If NeedFunctionPrototypes compiler directive has been
removed from Xlib.h and Xutil.h in Xfree86 4.4. All the prototypes
containing the offending _Xconst are now being processed. R 1.8.1, which
built successfully under XFree86 4.3, fails under XFree86 4.4 with the same
error messages.



From ripley at stats.ox.ac.uk  Sun Apr 18 10:17:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Apr 2004 09:17:44 +0100 (BST)
Subject: [R] R-1.9.0: make error on slackware-current!
In-Reply-To: <000001c42519$61ed5390$6401a8c0@eagle>
Message-ID: <Pine.LNX.4.44.0404180903580.13354-100000@gannet.stats>

On Sun, 18 Apr 2004, Timothy Tatar wrote:

> It appears that the #If NeedFunctionPrototypes compiler directive has been
> removed from Xlib.h and Xutil.h in Xfree86 4.4. All the prototypes
> containing the offending _Xconst are now being processed. R 1.8.1, which
> built successfully under XFree86 4.3, fails under XFree86 4.4 with the same
> error messages.

Yes, we do know.  However, 1.9.0 _has_ been built against XFree 4.4, and
also XFree 8.3 with NeedFunctionPrototypes defined to be 1.  _Xconst 
should be defined, so the problem seems to be with `slackware-current' 
(whatever that is) rather than XFree 4.4.  Does adding

#ifndef _Xconst
#define _Xconst const
#endif /* _Xconst */

help?  (That's inside #if NeedFunctionPrototypes in the headers I have.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Apr 18 10:38:36 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2004 10:38:36 +0200
Subject: [R] accessing log likelihood of poison model
In-Reply-To: <40821D33.1070600@cirad.fr>
References: <20040417224033.GA25643@sociostat.org>
	<x2isfyi5ek.fsf@biostat.ku.dk> <20040418003110.GA26102@sociostat.org>
	<40821D33.1070600@cirad.fr>
Message-ID: <x2hdvhr88j.fsf@biostat.ku.dk>

Renaud Lancelot <renaud.lancelot at cirad.fr> writes:

> >>>And that's great; but I need the log likelihood.
> >>>
> >>>Anyone know?
> >>
> >> The deviance will not suffice?  sum(dpois(nfix, fitted(freq.mod),
> >> log.p=T))
> >>
> >>should do the trick otherwise.

> > Thank you, that did the trick.  I should note that the method
> > doesn't work when I have a noninteger in Y.  I'm doing a "log
> > linear" analysis
> > of a cross table, and one of the cells of the cross table has a zero
> > value.  I put a .5 in that cell.  With the ".5 analysis" the above
> > method doesn't work.  So I had to revert to an analysis with a zero
> > cell to make it work. Which has some problems.  So I'm still left
> > looking for the log likelihood of the ".5 analysis".  I've had success
> > with Stata using this method, but not R.
> > Thanks again.
> 
> ?logLik

Right. Forgot about that. It's the same sum(dpois(....,log=T))
calculation, though (after a detour around the AIC), so that too will
generate -Inf if noninteger values are plugged in. Fair enough: The
likelihood is by definition the probability of obtaining the data, and
the Poisson distribution has probability zero of generating fractional
values. 

However, the deviance is well-defined in such cases. I'd suspect that
what Stata outputs is not the true log-likelihood, but a number
obtained by "analytic continuation" (wrong term, I know) of the
Poisson density to nonintegers, e.g.

> p <- function(x,lambda) lambda^x*exp(-lambda)/gamma(x+1)
> p(1,1)
[1] 0.3678794
> dpois(1,1)
[1] 0.3678794
> dpois(.5,1)
[1] 0
Warning message:
non-integer x = 0.500000
> p(.5,1)
[1] 0.4151075

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sun Apr 18 10:53:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Apr 2004 09:53:01 +0100 (BST)
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <XFMail.040418022750.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0404180947370.11764-100000@gannet.stats>

The short answer is no, as there is no way to recover the fitted values 
and residuals so you can't get a proper fit object of class "lm" (and 
hence get `summaries and all').

Your pseudo-data method needs to fix the u_i to be mean zero, variance 
one in the sample.  That is probably the quickest method.  The elegant one 
is to create a new class "groupedlm" and write a constructor etc for it 
....

On Sun, 18 Apr 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
> 
> I am dealing with data which have been presented as
> 
>   at each x_i, mean m_i of the y-values at x_i,
>                sd s_i of the y-values at x_i
>                number n_i of the y-values at x_i
> 
> and I want to linearly regress y on x.
> 
> There does not seem to be an option to 'lm' which can
> deal with such data directly, though the regression
> problem could be algebraically expressed in these terms.
> 
> One way of fudging it would be to replace each m_i by
> a set  of n_i numbers Y_i constructed as
> 
>   u_i <- rnorm(ni)
> 
>   Y_i <- m_i + s_i*(u_i - mean(u_i))/sd(u_i)
> 
> and associate these with X_i <- rep(x_i,n_i), thereby
> constructing a regression-equivalent set of pseudo "raw data"
> which could be fed to lm(Y~X). However, this strikes me as
> cumbersome, at least, and even ugly!
> 
> Is there a direct way to go from {(n_i,m_i,s_i)} to the
> fitted regression, with summaries and all (and use of 'predict')?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Apr 18 13:26:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2004 13:26:07 +0200
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <Pine.LNX.4.44.0404180947370.11764-100000@gannet.stats>
References: <Pine.LNX.4.44.0404180947370.11764-100000@gannet.stats>
Message-ID: <x2d665plww.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> The short answer is no, as there is no way to recover the fitted values 
> and residuals so you can't get a proper fit object of class "lm" (and 
> hence get `summaries and all').
> 
> Your pseudo-data method needs to fix the u_i to be mean zero, variance 
> one in the sample.  

That's what he did, no? A slightly more readable version is

 m + s*scale(rnorm(n))

> That is probably the quickest method.  The elegant one 
> is to create a new class "groupedlm" and write a constructor etc for it 
> ....

However, it begs the question whether it wouldn't have been better to
design the RSS into the lm class rather than computing it from
residuals in summary.lm and anova.lm and predict.lm and...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 18 13:26:51 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 18 Apr 2004 12:26:51 +0100 (BST)
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <Pine.LNX.4.44.0404180947370.11764-100000@gannet.stats>
Message-ID: <XFMail.040418122651.Ted.Harding@nessie.mcc.ac.uk>

Thanks, Brian!

On 18-Apr-04 Prof Brian Ripley wrote:
> The short answer is no, as there is no way to recover the fitted values

Well, the fitted values (a + b*x_i) would be available, as would be
the estimates and SEs of coefficients, sums of squares, and relevant
F ratios and P values.

> and residuals so you can't get a proper fit object of class "lm" (and 
> hence get `summaries and all').

Residuals granted. However, much of what is useful in 'summary.lm'
would be supported. So also (which is what I really wanted a lazy
route to) would be the requisite summary statistics to generate
confidence and prediction bands as in 'predict.lm'.

> Your pseudo-data method needs to fix the u_i to be mean zero,
> variance one in the sample.  That is probably the quickest method. 
> The elegant one is to create a new class "groupedlm" and write a
> constructor etc for it

That's the sort of thing I feared! No time for that at the moment,
though one day I may find it to be an absorbing exercise in extending
my R skills and understanding.

Anyway, I'm grateful to know what the position is. At least I can now
feel happy about having to roll up my sleeves and get stuck in the
hard way.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Apr-04                                       Time: 12:26:51
------------------------------ XFMail ------------------------------



From pallier at lscp.ehess.fr  Sun Apr 18 13:47:02 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Sun, 18 Apr 2004 13:47:02 +0200
Subject: [R] multistratum glm?
Message-ID: <40826AB6.4030605@lscp.ehess.fr>

Hello,

I routinely use aov and and the Error term to perform analyses of 
variance of experiments with 'within-subject' factors. I wonder whether 
a notion like 'multistratum models' exists for glm models when 
performing a logit analysis (without being 100%  sure whether this would 
make sense).

I have data of an experiment where the outcome is a categorical variable:

20 individuals listened to 80 synthetic utterances (distributed in 4 
types) and were ask classify them into four categories. (The variables 
in the data.frame are 'subject', 'sentence', 'type', and 'response')

Here is the table of counts table(type,response):
 
       response
type  a   b  c   d
  a 181 166 42  11
  b  69 170 72  89
  c  90 174 75  61
  d  14 125 53 208


There are several questions of interest, such as, for example:

- are responses distibuted in the same way for the different types?

- are the numbers of 'a' responses for the 'b' and 'c' types 
significantly different?

- is the proportion of 'd' over 'a' responses different for the 'b' and 
'c'  categories?

...  

(I want to make inferences for the population of potential subjects on 
the one hand, and on the population of potential sentences on the other 
hand).

If the responses were continuous, I would just run two one-way anovas: 
one with the factor type over the means by subject*type,
and the other with the factor type over the means by sentences (in 
type). And use t.test to compare between different pairs of types.

Now, as the answers are categorical, I am not sure about the correct 
approach and how to use R to perform such an analysis.

I could treat response as a factor, and use percentages of responses per 
subject in each cell of response*type,
and run an anova on that...[ 
aov(percentage~response*type+Error(subject/(response*type))] But it 
seems incorrect to me to use the response of the subject as an 
independent variable (though I do not have a forceful argument).
 
Simple Chi-square tests are not the answer either, as a given subject 
contributed several times (80) to the counts in the table above.

My reading of MASS and of several other books suggest the use of 
logit/multinomial models when the response is categorical. But in all 
the examples provided, the units of analysis contribute only one 
measurement. Should I include the subject and sentences factors in the 
formula? But then they would be treated as fixed-factors in the 
analysis, would they not?


Any suggestion is welcome.

Christophe Pallier
www.pallier.org



From sundar.dorai-raj at PDF.COM  Sun Apr 18 13:55:22 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Sun, 18 Apr 2004 06:55:22 -0500
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
In-Reply-To: <5.1.0.14.2.20040418133141.024500f0@mail.dhaka.net>
References: <5.1.0.14.2.20040418133141.024500f0@mail.dhaka.net>
Message-ID: <40826CAA.7070106@pdf.com>



Mohammad Ehsanul Karim wrote:
> Dear Sundar Dorai-Raj,
> 
> Thank you very much for mentioning to exponentiate ALPHA.
> 
> However, so far i understand that the parameters in the non-linear equation
> Y = ALPHA * (L^(BETA1)) * (K^(BETA2))
> and the coefficients of log(L) and log(K) of the following equation 
> (after linearizing)
> log(Y) = log(ALPHA) +(BETA1)*log(L) + (BETA2)*log(K)
> should be the same when estimated from either equation. Is it true? If 
> it is, then why the estimates of the two procedure (see below) are 
> different? Can you please explain it?
> -----------------------------
>  > coef(lm(log(Y)~log(L)+log(K), data=klein.data))
> 
> (Intercept)      log(L)      log(K)
>  -3.6529493   1.0376775   0.7187662
> -----------------------------
>  > nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), data=klein.data, start = 
> c(ALPHA=exp(-3.6529493),BETA1=1.0376775,BETA2 = 0.7187662), trace = TRUE)
> 
> Nonlinear regression model
>   model:  Y ~ ALPHA * (L^(BETA1)) * (K^(BETA2))
>    data:  klein.data
>       ALPHA       BETA1       BETA2
> 0.003120991 0.414100040 1.513546235
>  residual sum-of-squares:  3128.245
> -----------------------------
> 

Not necessarily. In the first model, you're minimizing:

sum((log(Y) - log(Yhat))^2)

because the nonlinear model you're fitting is:

Y = ALPHA * L^BETA1 * K^BETA2 * ERROR
log(Y) = log(ALPHA) + BETA1 * log(L) + BETA2 * log(K) + log(ERROR)

Note the multiplicative error structure. In the second model you're 
mininmizing

sum((Y - Yhat)^2)

because the nonlinear model you're fitting is

Y = ALPHA * L^BETA1 * K^BETA2 + ERROR

Note the additive error structure. Different error structures, different 
parameter estimates.

Also, the residual sums of squares for the nls fit is smaller, although 
I'm not sure whether this comparison is really fair:

klein.lm <- lm(log(Y) ~ log(L) + log(K))
# `start' is not shown here but can be copied from above
klein.nls <- nls(Y ~ ALPHA * L^BETA1 * K^BETA2, data = klein.data,
                  start = start, trace = TRUE)
rss.lm <- sum((Y - exp(fitted(klein.lm)))^2) # 3861.147
rss.nls <- sum((Y - fitted(klein.nls))^2) # 3128.245

Now, which one do you use? Depends on whether you believe you have 
multiplicative errors (use lm) or additive errors (use nls).

--sundar



From ripley at stats.ox.ac.uk  Sun Apr 18 14:05:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Apr 2004 13:05:20 +0100 (BST)
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <XFMail.040418122651.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0404181301190.12887-100000@gannet.stats>

On Sun, 18 Apr 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> Thanks, Brian!
> 
> On 18-Apr-04 Prof Brian Ripley wrote:
> > The short answer is no, as there is no way to recover the fitted values
> 
> Well, the fitted values (a + b*x_i) would be available, as would be
> the estimates and SEs of coefficients, sums of squares, and relevant
> F ratios and P values.

Many of those are calculated from the residuals ....

> > and residuals so you can't get a proper fit object of class "lm" (and 
> > hence get `summaries and all').
> 
> Residuals granted. However, much of what is useful in 'summary.lm'
> would be supported. So also (which is what I really wanted a lazy
> route to) would be the requisite summary statistics to generate
> confidence and prediction bands as in 'predict.lm'.

But the residual sum of squares is calculated from the residuals, so you 
are missing the estimate of sigma^2.

> > Your pseudo-data method needs to fix the u_i to be mean zero,
> > variance one in the sample.  That is probably the quickest method. 
> > The elegant one is to create a new class "groupedlm" and write a
> > constructor etc for it
> 
> That's the sort of thing I feared! No time for that at the moment,
> though one day I may find it to be an absorbing exercise in extending
> my R skills and understanding.
> 
> Anyway, I'm grateful to know what the position is. At least I can now
> feel happy about having to roll up my sleeves and get stuck in the
> hard way.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 18-Apr-04                                       Time: 12:26:51
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Apr 18 14:04:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2004 14:04:45 +0200
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <XFMail.040418122651.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040418122651.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x28ygtpk4i.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> Thanks, Brian!
> 
> On 18-Apr-04 Prof Brian Ripley wrote:
> > The short answer is no, as there is no way to recover the fitted values
> 
> Well, the fitted values (a + b*x_i) would be available, as would be
> the estimates and SEs of coefficients, sums of squares, and relevant
> F ratios and P values.

Not quite. You can get the estimates and per-group fitted values
alright from a weighted regression, but the SEs require that you have
the residual sum of squares and the within-group part of the SS is not
obtainable, although trivially computable as sum(s^2*(n-1))

(I did go through some of this for the trypsin example in Ch 10.4 in
ISwR)  

> > and residuals so you can't get a proper fit object of class "lm" (and 
> > hence get `summaries and all').
> 
> Residuals granted. However, much of what is useful in 'summary.lm'
> would be supported. So also (which is what I really wanted a lazy
> route to) would be the requisite summary statistics to generate
> confidence and prediction bands as in 'predict.lm'.
> 
> > Your pseudo-data method needs to fix the u_i to be mean zero,
> > variance one in the sample.  That is probably the quickest method. 
> > The elegant one is to create a new class "groupedlm" and write a
> > constructor etc for it
> 
> That's the sort of thing I feared! No time for that at the moment,
> though one day I may find it to be an absorbing exercise in extending
> my R skills and understanding.
> 
> Anyway, I'm grateful to know what the position is. At least I can now
> feel happy about having to roll up my sleeves and get stuck in the
> hard way.

Actually, I susppect that it's only about half an hour's work:

1) The constructor returns (with class lmG or so)
   list(wlm=lm(y~formula,weights=n), withinSS=sum(s^2*(n-1)),
        withinDF=sum(n-1))  

2) summary.lmG is like summary.lm except that resvar adds in the
   withinSS and withinDF. Likewise anova and predict methods.

3) done

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 18 14:06:48 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 18 Apr 2004 13:06:48 +0100 (BST)
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <x2d665plww.fsf@biostat.ku.dk>
Message-ID: <XFMail.040418130648.Ted.Harding@nessie.mcc.ac.uk>

On 18-Apr-04 Peter Dalgaard wrote:
> However, it begs the question whether it wouldn't have been better
> to design the RSS into the lm class rather than computing it from
> residuals in summary.lm and anova.lm and predict.lm and...

Well, something like this though lay under my original query.
The 'lm' fit, as opposed to 'glm' where for some models it
may be necessary to work from individual observations, is
basically least-squares, so it could be done once and for
all as Peter suggests. Hence I wondered if this had somehow
already been implemented (not necessarily in the 'lm' function
itself).

It does seem more logical to "design the RSS into the lm class".

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Apr-04                                       Time: 13:06:48
------------------------------ XFMail ------------------------------



From kjetil at entelnet.bo  Sun Apr 18 14:55:01 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 18 Apr 2004 08:55:01 -0400
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <XFMail.040418022750.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <40824265.25876.12625C@localhost>

On 18 Apr 2004 at 2:27, Ted Harding wrote:

> Hi Folks,
> 
> I am dealing with data which have been presented as
> 
>   at each x_i, mean m_i of the y-values at x_i,
>                sd s_i of the y-values at x_i
>                number n_i of the y-values at x_i
> 
> and I want to linearly regress y on x.

You need weighted regresseion, so lm with the argument weights. 
Assuning constant variance of the errors, you can calculate
w[i] <-  n_i

and put mean m_i of the y-values at x_i into y,
x_i values into x, n)- values into n. Then

lm(y ~ x, weight=n)

Kjetil Halvorsen

> 
> There does not seem to be an option to 'lm' which can
> deal with such data directly, though the regression
> problem could be algebraically expressed in these terms.
> 
> One way of fudging it would be to replace each m_i by
> a set  of n_i numbers Y_i constructed as
> 
>   u_i <- rnorm(ni)
> 
>   Y_i <- m_i + s_i*(u_i - mean(u_i))/sd(u_i)
> 
> and associate these with X_i <- rep(x_i,n_i), thereby
> constructing a regression-equivalent set of pseudo "raw data"
> which could be fed to lm(Y~X). However, this strikes me as
> cumbersome, at least, and even ugly!
> 
> Is there a direct way to go from {(n_i,m_i,s_i)} to the
> fitted regression, with summaries and all (and use of 'predict')?
> 
> With thanks,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> Fax-to-email: +44
> (0)870 167 1972 Date: 18-Apr-04                                  
> Time: 02:27:50 ------------------------------ XFMail
> ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 18 14:52:02 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 18 Apr 2004 13:52:02 +0100 (BST)
Subject: [R] lm with data=(means,sds,ns)
In-Reply-To: <x28ygtpk4i.fsf@biostat.ku.dk>
Message-ID: <XFMail.040418135202.Ted.Harding@nessie.mcc.ac.uk>

On 18-Apr-04 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
>> On 18-Apr-04 Prof Brian Ripley wrote:
>> > The short answer is no, as there is no way to recover the fitted
>> > values
>> 
>> Well, the fitted values (a + b*x_i) would be available, as would be
>> the estimates and SEs of coefficients, sums of squares, and relevant
>> F ratios and P values.
> 
> Not quite. You can get the estimates and per-group fitted values
> alright from a weighted regression, but the SEs require that you have
> the residual sum of squares and the within-group part of the SS is not
> obtainable, although trivially computable as sum(s^2*(n-1))

By "available" I did mean "computable" even if not explicitly present
in the implementation of 'lm'. Sorry if not clear!

> (I did go through some of this for the trypsin example in Ch 10.4 in
> ISwR)  

This looks more and more like a book I should get hold of ...

>> Anyway, I'm grateful to know what the position is. At least I can now
>> feel happy about having to roll up my sleeves and get stuck in the
>> hard way.
> 
> Actually, I susppect that it's only about half an hour's work:
> 
> 1) The constructor returns (with class lmG or so)
>    list(wlm=lm(y~formula,weights=n), withinSS=sum(s^2*(n-1)),
>         withinDF=sum(n-1))  
> 
> 2) summary.lmG is like summary.lm except that resvar adds in the
>    withinSS and withinDF. Likewise anova and predict methods.
> 
> 3) done

Hmm ... makes it look sublimely simple!

Thanks a lot for the comments and pointers.
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Apr-04                                       Time: 13:52:02
------------------------------ XFMail ------------------------------



From mateusz_CUT_IT_ at loskot.net  Sun Apr 18 15:08:09 2004
From: mateusz_CUT_IT_ at loskot.net (=?ISO-8859-2?Q?Mateusz_=A3oskot?=)
Date: Sun, 18 Apr 2004 15:08:09 +0200
Subject: [R] Histogram ploting
Message-ID: <40827DB9.1080508@loskot.net>

Hi,

It's my first post on this group.
I've just started learning & using R and I like it ;-)
I have I think simple question. I'm trying to plot
a histogram for my data set.
My data set is defined as follows:

Class	N
12.5	3
17.5	10
22.5	12
27.5	8
32.5	7
37.5	3
42.5	4
47.5	2

Class means middle of set of my ranges I define.
N column stores number of measurements counted to
particular class.
And now I would like to plot a simple histogram presenting
numbers of measurements in each class.
As I read in manual, hist function takes x (my N) as the first param
but I can not identify how should I pass my
class ranged into hist function.
I believe you can understand my problem ;-)))
Could anyone help me ?

Kind regards

-- 

Mateusz ??oskot
mateusz at loskot dot net



From edd at debian.org  Sun Apr 18 15:10:54 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 18 Apr 2004 08:10:54 -0500
Subject: [R] 1.9.0 for Debian 'stable' - volunteer needed
Message-ID: <20040418131054.GA7382@sonny.eddelbuettel.com>


As Doug had reported, Debian packages for R 1.9.0 were uploaded last Monday.
These are currently part of 'unstable', but can already be used on 'testing'
into which they should migrate in a few days.

Debian 'stable' is another matter. Neither Doug nor I has a stable system
left that would be suitable for compiling R.  So we would need a volunteer
willing to work with us in order to build R 1.9.0 for that architecture.
This would probably entail rewriting/simplifying debian/rules etc somewhat,
and I would of course assist with that.

Thanks,   Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From pallier at lscp.ehess.fr  Sun Apr 18 15:17:02 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Sun, 18 Apr 2004 15:17:02 +0200
Subject: [R] Histogram ploting
In-Reply-To: <40827DB9.1080508@loskot.net>
References: <40827DB9.1080508@loskot.net>
Message-ID: <40827FCE.3010002@lscp.ehess.fr>

Hello Mateusz,

The 'hist' function works on the raw data.
In your data set example, you have already computed the number of data 
points in each bin.
What you really want is probably a barplot of N

You could display your data:

plot(Class,N,'h')

Or

names(N)<-Class
barplot(N)

Christophe Pallier


 ??oskot wrote:

> Hi,
>
> It's my first post on this group.
> I've just started learning & using R and I like it ;-)
> I have I think simple question. I'm trying to plot
> a histogram for my data set.
> My data set is defined as follows:
>
> Class    N
> 12.5    3
> 17.5    10
> 22.5    12
> 27.5    8
> 32.5    7
> 37.5    3
> 42.5    4
> 47.5    2
>
> Class means middle of set of my ranges I define.
> N column stores number of measurements counted to
> particular class.
> And now I would like to plot a simple histogram presenting
> numbers of measurements in each class.
> As I read in manual, hist function takes x (my N) as the first param
> but I can not identify how should I pass my
> class ranged into hist function.
> I believe you can understand my problem ;-)))
> Could anyone help me ?
>
> Kind regards
>



From edgar at cs.uprm.edu  Sun Apr 18 16:55:01 2004
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Sun, 18 Apr 2004 10:55:01 -0400 (EDT)
Subject: [R] outliers using Random Forest
Message-ID: <Pine.GSO.4.33.0404181041500.17892-100000@cs.uprm.edu>

Hello,
Does anybody know if the outscale option of randomForest yields the
standarized version of the outlier measure for each case? or the results
are only the raw values. Also I have notice that this measure presents
very high variability. I mean if I repeat the experiment I am getting very
different values for this measure and it is hard to flag the outliers.
This does not happen with two other criteria than I am using: LOF and
Bay's Orca. I am getting several cases that can be considered as outliers
with both approaches.
 I run my experiments  using Bupa and Diabetes available at
UCI Machine database repository.

Thanks in advance for any response.



From mateusz_CUT_IT_ at loskot.net  Sun Apr 18 17:13:34 2004
From: mateusz_CUT_IT_ at loskot.net (=?ISO-8859-2?Q?Mateusz_=A3oskot?=)
Date: Sun, 18 Apr 2004 17:13:34 +0200
Subject: [R] Histogram ploting
In-Reply-To: <40827FCE.3010002@lscp.ehess.fr>
References: <40827DB9.1080508@loskot.net> <40827FCE.3010002@lscp.ehess.fr>
Message-ID: <40829B1E.3080403@loskot.net>

Hi Christophe,

On 4/18/2004 3:17 PM, Christophe Pallier wrote:
> The 'hist' function works on the raw data.
> In your data set example, you have already computed the number of data 
> points in each bin.

Yes, you are right. I evidently misunderstood the hist function
usage described in manuals.

> What you really want is probably a barplot of N
> You could display your data:
> 
> plot(Class,N,'h')

Yes, that's right.
Thank you very much.

Kind regards

-- 

Mateusz ??oskot
mateusz at loskot dot net



From andy_liaw at merck.com  Sun Apr 18 22:24:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 18 Apr 2004 16:24:53 -0400
Subject: [R] outliers using Random Forest
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C14@usrymx25.merck.com>

The thing to do is probably:

1. Use fairly large number of trees (e.g., 1000).
2. Run a few times and average the results.

The reason for the instability is sort of two fold:

1. The random forest algorithm itself is based on randomization.  That's why
it's probably a good idea to have 500-1000 trees to get more stable
proximity measures (of which the outlying measures are based on).

2. If you are running randomForest in unsupervised mode (i.e., not giving it
the class labels), then the program treats the data as "class 1", creates a
synthetic "class 2", and run the classification algorithm to get the
proximity measures.  You probably need to run the algorithm a few times so
that the result will be based on several simulated data, instead of just
one.

HTH,
Andy

> From: Edgar Acuna
> 
> Hello,
> Does anybody know if the outscale option of randomForest yields the
> standarized version of the outlier measure for each case? or 
> the results
> are only the raw values. Also I have notice that this measure presents
> very high variability. I mean if I repeat the experiment I am 
> getting very
> different values for this measure and it is hard to flag the outliers.
> This does not happen with two other criteria than I am using: LOF and
> Bay's Orca. I am getting several cases that can be considered 
> as outliers
> with both approaches.
>  I run my experiments  using Bupa and Diabetes available at
> UCI Machine database repository.
> 
> Thanks in advance for any response.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From h.wickham at auckland.ac.nz  Sun Apr 18 22:57:38 2004
From: h.wickham at auckland.ac.nz (Hadley Wickham)
Date: Mon, 19 Apr 2004 08:57:38 +1200
Subject: [R] Error in gam?
Message-ID: <4082EBC2.1090805@auckland.ac.nz>

I'm having some problems using variable names containing spaces (using 
backticks) with gam (mgcv 0.9-6, R 1.8.1).  Some toy code to reproduce 
my problem is below.  Am I doing something wrong, or should I pass this 
bug on to Simon Wood? (Or do I need to rename my variables to get rid of 
the spaces?)

Thanks,

Hadley


library(mgcv)

test <- data.frame(a = c(1:10), `a b` = c(1:10)^2, c=2:11, check.names=F)
test$`a b`

lm(a ~ `a b`, data=test) #works
gam(a ~ s(c), data=test) #works
gam(a ~ s(`a b`), data=test) #error below

Error in parse(file, n, text, prompt) : parse error

traceback()
  9: parse(text = x)
  8: eval(parse(text = x)[[1]])
  7: formula(eval(parse(text = x)[[1]]))
  6: switch(mode(x), "NULL" = structure(NULL, class = "formula"),
        character = formula(eval(parse(text = x)[[1]])), call = eval(x),
        stop("invalid formula"))
5: formula.default(object, env = NULL)
4: formula(object, env = NULL)
3: as.formula(rf)
2: gam.parser(formula)
1: gam(a ~ s(`a b`), data = test)



From p.dalgaard at biostat.ku.dk  Sun Apr 18 23:18:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Apr 2004 23:18:28 +0200
Subject: [R] Error in gam?
In-Reply-To: <4082EBC2.1090805@auckland.ac.nz>
References: <4082EBC2.1090805@auckland.ac.nz>
Message-ID: <x265bxdly3.fsf@biostat.ku.dk>

Hadley Wickham <h.wickham at auckland.ac.nz> writes:

> I'm having some problems using variable names containing spaces (using
> backticks) with gam (mgcv 0.9-6, R 1.8.1).  Some toy code to reproduce
> my problem is below.  Am I doing something wrong, or should I pass
> this bug on to Simon Wood? (Or do I need to rename my variables to get
> rid of the spaces?)

It's a problem of a sort that we've seen with several kinds of
modeling code.

Basically, the code appears to be using a deparse-paste-parse
technique and gets the backtick bit wrong. Probably, we should start
an effort to get things done right within base R first and only then
pester Simon and others.  In particular, I suspect we should supply a
replacement for the parse(paste("~", paste(terms, collapse="+")))
construction that doesn't have to go via the textual representation.

For now, I think you might want to rename variables and begone with
it...  


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sun Apr 18 23:23:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 18 Apr 2004 22:23:13 +0100 (BST)
Subject: [R] Error in gam?
In-Reply-To: <4082EBC2.1090805@auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0404182208130.16122-100000@gannet.stats>

On Mon, 19 Apr 2004, Hadley Wickham wrote:

> I'm having some problems using variable names containing spaces (using 
> backticks) with gam (mgcv 0.9-6, R 1.8.1).  Some toy code to reproduce 
> my problem is below.  Am I doing something wrong, or should I pass this 
> bug on to Simon Wood? (Or do I need to rename my variables to get rid of 
> the spaces?)

Well, that's not the current mgcv but the latter gives a similar error.
Yes, it needs Simon's attention.  The problem is in interpret.gam, where
lines like

ff1 <- paste(smooth.spec[[i]]$term[1:nt], collapse = "+")
fake.formula <- paste(fake.formula, "+", ff1)

expect syntactically valid names, and so the deparse calls in s() (and
probably elsewhere) need backtick=TRUE.  With that change, your example 
works.

It's probably optimistic to expect most of the code available for R to 
work with backtick-quoted names.

> 
> Thanks,
> 
> Hadley
> 
> 
> library(mgcv)
> 
> test <- data.frame(a = c(1:10), `a b` = c(1:10)^2, c=2:11, check.names=F)
> test$`a b`
> 
> lm(a ~ `a b`, data=test) #works
> gam(a ~ s(c), data=test) #works
> gam(a ~ s(`a b`), data=test) #error below
> 
> Error in parse(file, n, text, prompt) : parse error
> 
> traceback()
>   9: parse(text = x)
>   8: eval(parse(text = x)[[1]])
>   7: formula(eval(parse(text = x)[[1]]))
>   6: switch(mode(x), "NULL" = structure(NULL, class = "formula"),
>         character = formula(eval(parse(text = x)[[1]])), call = eval(x),
>         stop("invalid formula"))
> 5: formula.default(object, env = NULL)
> 4: formula(object, env = NULL)
> 3: as.formula(rf)
> 2: gam.parser(formula)
> 1: gam(a ~ s(`a b`), data = test)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Mon Apr 19 00:41:52 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 19 Apr 2004 10:41:52 +1200
Subject: [R] filled.contour
References: <4092E9E0@webmail.uic.edu>
Message-ID: <40830430.2080502@stat.auckland.ac.nz>

Hi


jzhang10 wrote:
> Hi,
> I want to draw a level plot. The levels are not evenly spaced, so I did 
> something like: levels=c(0,2,5,10,30,60). I still want the color bar (key) on 
> the right side to be evenly spaced so that the small numbers (0,2,5) are not 
> squeezed together.
> Does anyone know how to do it?


One way is to use the key.axes argument to draw the key yourself.
Below is a simple example;  this makes use of the fact that the key is 
just another plot with a scale of c(0, 1) on the x-axis and a scale of 
c(min(levels), max(levels)) on the y-axis.


x <- y <- seq(-4*pi, 4*pi, len = 27)
r <- sqrt(outer(x^2, y^2, "+"))
# Range of r is 0 to 17.77153
levels <- c(0, 2, 4, 8, 18)
cols <- cm.colors(length(levels) - 1)

# Default key has levels on linear axis
# Plot looks awful, but that's not the point ...
filled.contour(cos(r^2)*exp(-r/(2*pi)), levels=levels, col=cols)

# Custom axis producing even spacing of levels
customAxis <- function() {
   n <- length(levels)
   # Generate even spacing over y-scale range
   y <- seq(min(levels), max(levels), length.out=n)
   # Draw main key with rectangles
   # (using same colours as in main plot)
   rect(0, y[1:(n-1)], 1, y[2:n], col=cols)
   # Draw key axis
   # NOTE: labels do not correspond to "real" y-locations
   axis(4, at=y, labels=levels)
}
filled.contour(cos(r^2)*exp(-r/(2*pi)), levels=levels, col=cols,
                key.axes=customAxis())


Hope that helps.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From marcelloverona at tiscali.it  Mon Apr 19 02:45:54 2004
From: marcelloverona at tiscali.it (Marcello Verona)
Date: Mon, 19 Apr 2004 00:45:54 +0000
Subject: [R] R on virtual server: problem with f2c
Message-ID: <40832142.5040705@tiscali.it>

Hi
I've a problem:
I wont to install R on a virtual server without root privileges.

I've tried to install in /usr/local/
but the fortran is not present.
I've tried to install libf2c and in the ./configure now is ok, but the 
make crash.

I don't know if my libf2c is right...
Have a good link for f2c?

Thank you.
Marcello



From spencer.graves at pdf.com  Mon Apr 19 00:59:34 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 18 Apr 2004 15:59:34 -0700
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
In-Reply-To: <40826CAA.7070106@pdf.com>
References: <5.1.0.14.2.20040418133141.024500f0@mail.dhaka.net>
	<40826CAA.7070106@pdf.com>
Message-ID: <40830856.6080502@pdf.com>

      Given enough data, the choice between the two models can be made 
in part by plotting the residuals vs. the predicted:  or vs. 
log(predicted):  Suppose the "true" model was

      log(Y) = log(ALPHA) +(BETA1)*log(L) + (BETA2)*log(K) + err,

where err is independent, normal with constant variance s.e^2.  This is 
is equivalent to

      Y = ALPHA * (L^(BETA1)) * (K^(BETA2))*exp(err)

      Suppose you fit with nls

      Y = ALPHA * (L^(BETA1)) * (K^(BETA2)) + Err,

assuming Err is independent, normal with constant variance s.E^2.  If Y 
ranges over less than a factor of 2 or 5, it probably doesn't matter 
much which model you use.  However, if Y ranges over 2 or more orders of 
magnitude, then a plot of residuals or abs(residuals) vs. predicted (or 
vs. log(predicted)) should show how the residuals get larger as the 
predictions increase. 

      For example, suppose we know that BETA1 = BETA2 = 1, K = 1, and L 
ranges from 0.001 to 1000, and we fit

      Y = ALPHA*K + Err,

when the "truth" is

      log(Y) = log(ALPHA) + log(L) + err,

with err ~ N(0, 1).  Then the following simulation shows what we want: 

log10.L <- seq(-3, 3, .2)
DF <- data.frame(L=10^log10.L, 
Y=exp(log(10)*log10.L+rnorm(length(log10.L))))

fitLin <- lm(Y ~ L-1, DF)
fitLog <- lm(log(Y/L) ~ 1, DF)

plot(DF$L, abs(resid(fitLin)), log="xy")
plot(DF$L, abs(resid(fitLog)), log="xy")
qqnorm(resid(fitLin), datax=TRUE)
qqnorm(resid(fitLog), datax=TRUE)

      The difference between resid(fitLin) and resid(fitLog) is dramatic. 

      Now suppose we modify the simulation by replacing the first 
lot10.L and DF as follows: 

log10.L <- seq(100, 100.1, length=31)
DF <- data.frame(L=10^log10.L, 
Y=exp(log(10)*log10.L+0.01*rnorm(length(log10.L))))

      Then the difference between resid(fitLin) and resid(fitLog) is 
hardly noticeable. 

      Beyond this, extensive experience with economic data indicates 
that many economic quantities are lognormally distrributed, provided the 
logic of the situation says the numbers should always be positive.  To 
understand this, consider for example the money spent each week by a 
small child with an allowance of $3 per week.  Suppose she spends 
roughly $3 per week, a little less some weeks and a little more in 
others.  Her expendatures may vary over a 3 month period from $2.37 to 
$3.69, just over 10%, 63 cents low one week, 69 cents high another.  Now 
compare this with the expendatures of a $3,000,000 per year business.  
If their business is stable over a decade, their expendatures may vary 
from $2,370,000 in their slowest year in that decade to $3,690,000 at 
the max, just over 10%.  However, that 10% is NOT 63 cents low in one 
case but $630,000. 
    
      Hope this helps.  spencer graves

Sundar Dorai-Raj wrote:

>
>
> Mohammad Ehsanul Karim wrote:
>
>> Dear Sundar Dorai-Raj,
>>
>> Thank you very much for mentioning to exponentiate ALPHA.
>>
>> However, so far i understand that the parameters in the non-linear 
>> equation
>> Y = ALPHA * (L^(BETA1)) * (K^(BETA2))
>> and the coefficients of log(L) and log(K) of the following equation 
>> (after linearizing)
>> log(Y) = log(ALPHA) +(BETA1)*log(L) + (BETA2)*log(K)
>> should be the same when estimated from either equation. Is it true? 
>> If it is, then why the estimates of the two procedure (see below) are 
>> different? Can you please explain it?
>> -----------------------------
>>  > coef(lm(log(Y)~log(L)+log(K), data=klein.data))
>>
>> (Intercept)      log(L)      log(K)
>>  -3.6529493   1.0376775   0.7187662
>> -----------------------------
>>  > nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), data=klein.data, start = 
>> c(ALPHA=exp(-3.6529493),BETA1=1.0376775,BETA2 = 0.7187662), trace = 
>> TRUE)
>>
>> Nonlinear regression model
>>   model:  Y ~ ALPHA * (L^(BETA1)) * (K^(BETA2))
>>    data:  klein.data
>>       ALPHA       BETA1       BETA2
>> 0.003120991 0.414100040 1.513546235
>>  residual sum-of-squares:  3128.245
>> -----------------------------
>>
>
> Not necessarily. In the first model, you're minimizing:
>
> sum((log(Y) - log(Yhat))^2)
>
> because the nonlinear model you're fitting is:
>
> Y = ALPHA * L^BETA1 * K^BETA2 * ERROR
> log(Y) = log(ALPHA) + BETA1 * log(L) + BETA2 * log(K) + log(ERROR)
>
> Note the multiplicative error structure. In the second model you're 
> mininmizing
>
> sum((Y - Yhat)^2)
>
> because the nonlinear model you're fitting is
>
> Y = ALPHA * L^BETA1 * K^BETA2 + ERROR
>
> Note the additive error structure. Different error structures, 
> different parameter estimates.
>
> Also, the residual sums of squares for the nls fit is smaller, 
> although I'm not sure whether this comparison is really fair:
>
> klein.lm <- lm(log(Y) ~ log(L) + log(K))
> # `start' is not shown here but can be copied from above
> klein.nls <- nls(Y ~ ALPHA * L^BETA1 * K^BETA2, data = klein.data,
>                  start = start, trace = TRUE)
> rss.lm <- sum((Y - exp(fitted(klein.lm)))^2) # 3861.147
> rss.nls <- sum((Y - fitted(klein.nls))^2) # 3128.245
>
> Now, which one do you use? Depends on whether you believe you have 
> multiplicative errors (use lm) or additive errors (use nls).
>
> --sundar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From gblevins at mn.rr.com  Mon Apr 19 04:54:21 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Sun, 18 Apr 2004 21:54:21 -0500
Subject: [R] Seeking help with multcomp
Message-ID: <000e01c425b9$9e2f1640$73cc5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040418/4fa8dd61/attachment.pl

From wettenhall at wehi.edu.au  Mon Apr 19 05:14:17 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Mon, 19 Apr 2004 13:14:17 +1000 (EST)
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
In-Reply-To: <5.1.0.14.2.20040418132149.02452a30@mail.dhaka.net>
References: <5.1.0.14.2.20040418132149.02452a30@mail.dhaka.net>
Message-ID: <Pine.LNX.4.58.0404191230030.5771@unix28.alpha.wehi.edu.au>

On Sun, 18 Apr 2004, Mohammad Ehsanul Karim wrote:
> concern (In this case there is no way to linearize it), the Cobb-Douglas 
> being just a 'Toy problem' to see how non-linear process works. And i'm 
> sorry that i cannot guess some approximate parameter values for that CES 
> using some "typical" Y,L,K data : that why it is a problem (doing a grid 
> search over infinite parameter space is indeed time consuming).

Mohammed,

OK, so you really do want to try nonlinear regression.  That's 
fine as long as you know that there are a lot more things that 
can go wrong than with linear regression.  Have you read the 
references at the end of the help for nls?  (I have to admit I 
haven't yet.)

Do you know under what conditions your cost function will be 
convex with respect to the parameters you are estimating?  The 
sum of convex functions is convex.  So if every one of your 
squared-error terms is convex then the sum will be.

Let's say you are minimizing this cost function: 
  Sum_i (Y_i - f(delta,beta,phi)_i)^2

where f()_i is your C.E.S. function evaluated at 
each data point (L_i,K_i).

Can you calculate the Hessian matrix (second derivative matrix) 
of the cost function with respect to the parameters, and see 
under what conditions it is positive definite?  (i.e. under 
what conditions is your cost function convex?)

A non-convex cost function is one possible reason why a 
nonlinear optimization routine may have trouble converging.  
There are some fiddles you can apply if you don't have convexity 
but they don't always work.  For example, in Newton descent, you 
use a Hessian matrix to calculate a descent step and in BFGS you 
use an approximate inverse Hessian to calculate a descent step.  
If the Hessian is not positive definite, you can cheat by making 
it positive-definite by using a modified cholesky factorization, 
e.g. Schnabel and Eskow.  This should guarantee a descent 
direction.

Sometimes you don't need a Hessian to be positive-definite in 
all directions, only within the subspace dictated by the 
constraints.  For example the negative(*) Cobb-Douglas utility 
function (for two commodities) is not convex over all R2 but it 
is convex in the subspace in which the budget constraint is 
satisfied at equality.  I'm not talking about regression here, 
just "max utility subject to budget constraint".

(*) negative, because instead of maximizing utility and having 
to talk about concavity I want to talk about minimizing 
negative utility so I can talk about convexity.

Consider a second-order Taylor approximation about a potential 
minimizing solution x*.  x is the vector of parameters.  g is 
the cost function (because I already used 'f' above for the 
C.E.S. function).  x* is the optimal solution.  If there's 
any chance of standard nonlinear optimization working, we 
hope that g'(x*) is zero.

g(x) = g(x*) + g'(x*)(x-x*) + (1/2)(x-x*)T g''(x*) (x-x*)

(where T means transpose)

So with g'(x*) = 0, we have

g(x)-g(x*) = (1/2)(x-x*)T g''(x*)(x-x*)

So if we move in a direction x-x* away from the optimal point 
x*, we want this to always be positive (strictly convex, 
positive definte hessian), or at least never be negative 
(positive semi-definite hessian).  g''(x*) is the Hessian 
evaluated at the optimal solution.

There are many ways to test positive-definiteness.  If all the 
eigenvalues are positive the matrix is positive definte.  There 
are several ways to test positive definiteness over the subspace 
dictated by your constraints, e.g. a bordered hessian.

Good luck,
James



From ayalahec at msu.edu  Mon Apr 19 05:28:09 2004
From: ayalahec at msu.edu (Hector L. Ayala-del-Rio)
Date: Sun, 18 Apr 2004 23:28:09 -0400
Subject: [R] barplot fill patterns
Message-ID: <944378F8-91B1-11D8-A09C-000393DB5846@msu.edu>

Dear R-helpers,

   I will like to know if  there is a way to generate a stacked column  
graph using both patterns and colors to fill the bars.  I have many  
categories for the number of color available in R, so I will like R to  
start with solid colors and then use patterns an colors.  I have been  
using barplot and some code that I found in the archives, but I am  
still having bars with the exact same color and patterns even though  
they belong to different categories.  How many different colors are  
available in R??  I know it is a silly question but I have looked  
everywhere and I can not find it.  I have also tried to add the legend,  
with the difficulty that overlaps with the plot.  Does anybody know how  
to adjust the size of the graph and the legend so they can fit in the  
same page???  Below is how part of the data looks and the code I have  
been using.  Any suggestions are welcome

thanks

Hector

   
 >barplot(t(Control.cfo.norm.mat),beside=F,space=.3,legend.text=colnames( 
Control.cfo.norm.mat),col=(0:100),las=3,  
density=rep(c(85,55,-1),5),angle=rep(c(45,90,180),5))


 >Control.cfo.norm.mat
                X36         X40         X58         X60         X62       
   X66
26Y     0.000000000 0.000000000 0.000000000 0.005409866 0.084265542  
0.03218412
C-50    0.000000000 0.000000000 0.000000000 0.019747309 0.096583996  
0.15919513
C-90    0.005584642 0.000000000 0.005671902 0.022687609 0.067102967  
0.07643979
C-127-1 0.000000000 0.000000000 0.011776706 0.013816766 0.033753709  
0.36999258
C-164   0.000000000 0.000000000 0.000000000 0.009163186 0.024120738  
0.31976823
C-198   0.000000000 0.000000000 0.000000000 0.009708738 0.035671430  
0.15904876
C-226   0.000000000 0.000000000 0.000000000 0.000000000 0.019187779  
0.07991803
C-268   0.000000000 0.000000000 0.000000000 0.008017960 0.028864657  
0.06724396
C-294-C 0.004623288 0.000000000 0.000000000 0.009589041 0.032448630  
0.06446918
C-310   0.000000000 0.000000000 0.000000000 0.000000000 0.010871603  
0.02858482
C-357-2 0.008525755 0.000000000 0.000000000 0.039520426 0.017850799  
0.03596803
C-375   0.007081596 0.000000000 0.004406326 0.018176096 0.030293493  
0.06428515
C-399   0.008050940 0.000000000 0.003659518 0.041206177 0.077581790  
0.09104882
C-414   0.008423269 0.000000000 0.000000000 0.025927876 0.053698342  
0.10858121
C-428   0.004066090 0.000000000 0.000000000 0.005163289 0.006454111  
0.04485607
C-434   0.000000000 0.000000000 0.000000000 0.014529545 0.014097440  
0.09122826
C-454   0.006096595 0.000000000 0.000000000 0.020348377 0.013460016  
0.07624703
C-465   0.000000000 0.000000000 0.000000000 0.029669076 0.051477114  
0.10904019
C-479   0.007429222 0.000000000 0.000000000 0.023358544 0.019878188  
0.10293822
C-525   0.005839724 0.000000000 0.000000000 0.010835151 0.009990853  
0.05832688
C-532   0.000000000 0.000000000 0.000000000 0.018876507 0.018876507  
0.05390039
C-541   0.006378889 0.000000000 0.000000000 0.010660608 0.000000000  
0.03416637
C-567   0.000000000 0.000000000 0.000000000 0.018852788 0.022763738  
0.12996390
C-605   0.000000000 0.000000000 0.000000000 0.015993146 0.120234185  
0.14065400
C-616   0.009669768 0.000000000 0.000000000 0.014960774 0.035668674  
0.05893085
C-630-1 0.011308562 0.000000000 0.000000000 0.008885299 0.162493269  
0.04671513
C-630-2 0.008951265 0.000000000 0.000000000 0.007514643 0.133384904  
0.04099900
C-667   0.005172106 0.000000000 0.000000000 0.016229713 0.087836633  
0.14071696
C-687   0.010054618 0.000000000 0.009309831 0.027681231 0.241559086  
0.16335650
C-694   0.011289799 0.004304743 0.012264457 0.032163743 0.235298895  
0.16902209
C-722   0.007263246 0.000000000 0.000000000 0.010429276 0.085203464  
0.06453115
C-729   0.012428930 0.000000000 0.000000000 0.012693376 0.218960730  
0.09956366
C-750   0.011836777 0.000000000 0.000000000 0.020558613 0.121378881  
0.11535666
C-811   0.000000000 0.000000000 0.000000000 0.013054830 0.019938286  
0.15475908
                 X77         X92         X95         X96        X107      
    X109
26Y     0.011736659 0.000000000 0.009536035 0.019622226 0.000000000  
0.000000000
C-50    0.004679457 0.000000000 0.005053814 0.005240992 0.000000000  
0.000000000
C-90    0.000000000 0.000000000 0.007591623 0.000000000 0.000000000  
0.007940663


H??ctor L. Ayala-del-R??o, Ph.D.
Center for Microbial Ecology &
Center for Genomic and Evolutionary Studies
on Microbial Life at Low Temperatures
Michigan State University
545 Plant & Soil Sciences Building
East Lansing, MI 48824-1325
Phone: 517-353-9021
Fax: 517-353-2917



From tpapp at axelero.hu  Mon Apr 19 06:43:17 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Mon, 19 Apr 2004 06:43:17 +0200
Subject: [R] barplot fill patterns
In-Reply-To: <944378F8-91B1-11D8-A09C-000393DB5846@msu.edu>
References: <944378F8-91B1-11D8-A09C-000393DB5846@msu.edu>
Message-ID: <20040419044317.GA897@localhost>

On Sun, Apr 18, 2004 at 11:28:09PM -0400, Hector L. Ayala-del-Rio wrote:

> Dear R-helpers,
> 
>   I will like to know if  there is a way to generate a stacked column  
> graph using both patterns and colors to fill the bars.  I have many  
> categories for the number of color available in R, so I will like R to  
> start with solid colors and then use patterns an colors.  I have been  

1. Density patterns: see the help of barplot (?barplot), density gives
the number of lines per inch, angle gives the shading angle.  Both can
be vectors, and will be recycled.

2. Look at the package RColorBrewer, a package with predefined
palettes of three types, each with colors that are easy to distinguish
visually.

Combining the above two will give you a lot of variation.  Experiment.

> they belong to different categories.  How many different colors are  
> available in R??  I know it is a silly question but I have looked  

R has a continous palette, so I would say it has 2^24 ~ 16 million
colors on most devices.  The question doesn't make sense, though: your
eyes will only be able to distinguish a few shades in the spectrum.

> everywhere and I can not find it.  I have also tried to add the legend,  
> with the difficulty that overlaps with the plot.  Does anybody know how  
> to adjust the size of the graph and the legend so they can fit in the  
> same page???  Below is how part of the data looks and the code I have  

experiment with commands like

par(xpd=NA)
par(mar=c(3.1,4.1,1.1,2.1))

which disable clipping and extend the margin.  Use the legend()
function after barplot, it lets you specify the coordinates.  

> >barplot(t(Control.cfo.norm.mat),beside=F,space=.3,legend.text=colnames( 
> Control.cfo.norm.mat),col=(0:100),las=3,  
> density=rep(c(85,55,-1),5),angle=rep(c(45,90,180),5))

Everything makes sense except col=(0:100). Use either color names (eg
"green") or hexadecimal colors (eg "#00FF00"), many functions can be
used to generate the latter, eg heat.colors() and its friends,
RColorBrewer, etc.  Using rep(..., each=..., times=...) you can
generate useful combinations of patterns/colors/angles.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From maechler at stat.math.ethz.ch  Mon Apr 19 09:37:33 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 19 Apr 2004 09:37:33 +0200
Subject: [R] Histogram ploting
In-Reply-To: <40829B1E.3080403@loskot.net>
References: <40827DB9.1080508@loskot.net> <40827FCE.3010002@lscp.ehess.fr>
	<40829B1E.3080403@loskot.net>
Message-ID: <16515.33213.672258.329644@gargle.gargle.HOWL>

>>>>> "Mateusz" == Mateusz ??oskot <mateusz_CUT_IT_ at loskot.net>
>>>>>     on Sun, 18 Apr 2004 17:13:34 +0200 writes:

    Mateusz> Hi Christophe, On 4/18/2004 3:17 PM, Christophe
    Mateusz> Pallier wrote:
    >> The 'hist' function works on the raw data.  In your data
    >> set example, you have already computed the number of data
    >> points in each bin.

    Mateusz> Yes, you are right. I evidently misunderstood the
    Mateusz> hist function usage described in manuals.

    >> What you really want is probably a barplot of N You could
    >> display your data:
    >> 
    >> plot(Class,N,'h')

    Mateusz> Yes, that's right.  Thank you very much.

well, I think you did have real histogram data,
and in teaching about graphics I do emphasize the difference
between a barplot {in R: plot of table(); space between bars} 
and a histogram {continuous x; no space between bars}.

In this case, I'd rather construct an object of class
'histogram' and plot() it, i.e., call the plot.histogram method:

(mids <- seq(12.5, 47.5, by = 5))
N <- c(3,10, 12,8, 7,3, 4,2)
## Construct breaks from  mids  "in general"
## (here, simply br <- seq(10,50,by=5) is easier)
dx <- mean(diff(mids))
br <- (mids[-1] + mids[-length(mids)])/2
(br <- c(br[1] - dx, br, br[length(br)] + dx))

his <- list(breaks=br, counts=N, mids = mids)
class(his) <- "histogram"
plot(his, main = "Histogram of <my stuff>")


Regards,
Martin



From edgar at cs.uprm.edu  Mon Apr 19 10:58:38 2004
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Mon, 19 Apr 2004 04:58:38 -0400 (EDT)
Subject: [R] outliers using Random Forest
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7C14@usrymx25.merck.com>
Message-ID: <Pine.GSO.4.33.0404190447290.19640-100000@cs.uprm.edu>

Dear Andy,
Thanks for your quick answer. I increased the number of trees and the
outlyingness measure got more stable. But still I do not know if I am
working with the raw measure or with the normalized measure mentioned
in the Breiman's Wald lecture. The normalized measure nout is

nout=(nout-med)/mean(abs(nout-med))
where med is the median of the class containing the case correponding
to nout.

Best regards
Edgar Acuna

On Sun, 18 Apr 2004, Liaw, Andy wrote:

> The thing to do is probably:
>
> 1. Use fairly large number of trees (e.g., 1000).
> 2. Run a few times and average the results.
>
> The reason for the instability is sort of two fold:
>
> 1. The random forest algorithm itself is based on randomization.  That's why
> it's probably a good idea to have 500-1000 trees to get more stable
> proximity measures (of which the outlying measures are based on).
>
> 2. If you are running randomForest in unsupervised mode (i.e., not giving it
> the class labels), then the program treats the data as "class 1", creates a
> synthetic "class 2", and run the classification algorithm to get the
> proximity measures.  You probably need to run the algorithm a few times so
> that the result will be based on several simulated data, instead of just
> one.
>
> HTH,
> Andy
>
> > From: Edgar Acuna
> >
> > Hello,
> > Does anybody know if the outscale option of randomForest yields the
> > standarized version of the outlier measure for each case? or
> > the results
> > are only the raw values. Also I have notice that this measure presents
> > very high variability. I mean if I repeat the experiment I am
> > getting very
> > different values for this measure and it is hard to flag the outliers.
> > This does not happen with two other criteria than I am using: LOF and
> > Bay's Orca. I am getting several cases that can be considered
> > as outliers
> > with both approaches.
> >  I run my experiments  using Bupa and Diabetes available at
> > UCI Machine database repository.
> >
> > Thanks in advance for any response.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------
>



From erich.neuwirth at univie.ac.at  Mon Apr 19 10:00:33 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 19 Apr 2004 10:00:33 +0200
Subject: [R] New unique name
Message-ID: <40838721.7080404@univie.ac.at>

In some languages there is a function
gensym()
which returns a new unique name (in the current environment).
This is quite helpful when one has to do temporary assignments.
I could not find such a function in R.
Is there one?


-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From Bernhard.Pfaff at drkw.com  Mon Apr 19 10:02:34 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 19 Apr 2004 10:02:34 +0200
Subject: [R] regression and dw
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B8CD@ibfftce505.is.de.dresdnerkb.com>

> > Dear R People:
> > 
> > Suppose we have a regression model that we will call
> > y.lm
> > 
> > We run the Durbin Watson test for autocorrelation
> > and we find that there is positive autocorrelation,
> > and phi = 0.72, say.
> > 
> > What is our next step, please?  
> 
> Look at the residuals more closely, e.g. look at the acf.
> 
> > Do we calculate the following
> > yprime_t = y_t - 0.72y_t-1,
> > x1prime_t = x1_t - 0.72x1_t-1,
> > 
> > and so on, and re-fit the linear mode?

Hello Erin,

beside the points mentioned by Prof. Ripley, you might also want to consider
test for order of integration of y and x and if cointegration exists between
these variables. A high R2 and a low DW is often a hindsight for a spurious
relationship, which needs to be investigated further. There is the
contributed package 'urca' available. Incidentally, a package update will be
put on CRAN shortly. If you want to receive it now, pls. contact me offline
and name your OS (i.e. zip or tar.gz).

HTH,
Bernhard  



> 
> Better to use arima with AR residuals and an xreg matrix.  
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From phgrosjean at sciviews.org  Mon Apr 19 10:05:48 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 19 Apr 2004 10:05:48 +0200
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE5A3@exsalem4-bu.odot.state.or.us>
Message-ID: <MABBLJDICACNFOLGIHJOOEGIEFAA.phgrosjean@sciviews.org>

The buffered output is a nice thing, and if the user want to use it, then
fine! However, it should be nice to know if it is set ON or OFF, and to
temporarily change it for some outputs in R scripts. I think this is the
primary request. Then, your request appears to me as a secondary one: to set
buffered output ON or OFF at statup of Rgui (by defining it in the
preference panel). Personnally, I would really like to have both.
Best,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Benjamin.STABLER at odot.state.or.us
Sent: Friday, 16 April, 2004 18:09
To: R-help at stat.math.ethz.ch
Subject: RE: [R] Turning windows screen buffering on and off


Well, I too would like to be able to set buffered output to false for Rgui
Windows without user intervention.  Maybe it could be set via the Edit ->
Gui Preferences so that it can be saved and set at startup.  The GUI
Preferences are part of the interface and not the standard language
definition right?...so that seems to be a good spot to set something like
that.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104

---------------------------------

Message: 75
Date: Fri, 16 Apr 2004 08:59:43 +0100 (BST)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: RE: [R] Turning windows screen buffering on and off
To: <Toby.Patterson at csiro.au>
Cc: R-help at stat.math.ethz.ch
Message-ID: <Pine.GSO.4.31.0404160852270.16317-100000 at toucan.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

No, and options() really is part of the R/S language not the interface.

See the rw-FAQ Q6.3 for how to manage the buffering more effectively.
(Hint: you need to put calls to flush.console() in your code.)


On Fri, 16 Apr 2004 Toby.Patterson at csiro.au wrote:

> I meant via a function or something like:
>
> options( buffered.output = FALSE)
>
> Sorry, I should have made that clearer.
>
> Cheers
> Toby
>
>
>
> -----Original Message-----
> From: Roger D. Peng [mailto:rpeng at jhsph.edu]
> Sent: Friday, April 16, 2004 1:11 PM
> To: Patterson, Toby (Marine, Hobart)
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Turning windows screen buffering on and off
>
> Ctrl-W.
>
> -roger
>
> Toby.Patterson at csiro.au wrote:
> > All,
> >
> > Does anyone know if there is an option I can set to turn
> screen-buffered
> > output on and off with the win32 rgui? (Apart from the point and click
> > method).
> >
> > I am running some simulations where it is useful to watch output but
> it
> > gets mildly tiresome having to manually switch things on and off via
> the
> > gui.
> >
> > Thanks
> >
> > Toby.
> >
> >
> >>version
> >
> >          _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    1
> > minor    8.1
> > year     2003
> > month    11
> > day      21
> > language R
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ajsmit at science.uct.ac.za  Mon Apr 19 10:15:56 2004
From: ajsmit at science.uct.ac.za (AJ Smit)
Date: Mon, 19 Apr 2004 10:15:56 +0200
Subject: [R] SE for combined data
Message-ID: <40838ABC.5060100@science.uct.ac.za>

Dear all

I have just had the question from a colleague. I know that it is not directly related to R (I will probably use R to do the analysis), but I hope someone can give us some insight:

Thanks,
AJ Smit



I sampled populations of a seaweed in the intertidal in order to estimate
the standing biomass of that seaweed at that site.

Due to clumped distribution patterns, I chose a stratified sampling system,
as follows. In each of three subjectively defined biomass classes (low,
medium and high biomass density), four quadrats (usually) were haphazardly
placed, and the biomass in those quadrats harvested. This provided an
estimate of the biomass density present in that biomass density class. The
area of ground covered by that biomass density class was also estimated,
and, by combining the estimated biomass density and the area covered by that
biomass density class, the total biomass in that biomass density class was
estimated. When the estimated biomass in the three biomass density classes
was combined, I had a figure for the standing biomass for that site..

So, for each biomass density class, I have a number of biomass density
estimates (usually, but not always, four), and an estimate of the area
covered by that class.

I repeated this at a number of sites. Biomass density classes were not
necessarily the same between sites.

Given that I can calculate measures of variation for each biomass density
class, is there a way to combine these data, presumably weighted by the area
covered by each biomass density class, and calculate the standard error for
the final biomass estimate at each site?
------------------------------------
Thanks
Neil

-- 
~~~~~~~~~~~~~~~~~~~~~~~
Dr Albertus J. Smit
Department of Botany
University of Cape Town
Private Bag Rondebosch
7700
Cape Town
SOUTH AFRICA
~~~~~~~~~~~~~~~~~~~~~~~~
Tel. +27 21 689 3032
Fax  +27 21 650 4041



From Bernhard.Pfaff at drkw.com  Mon Apr 19 10:18:24 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 19 Apr 2004 10:18:24 +0200
Subject: [R] Box-Ljung p-value -> Test for Independence
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B8CE@ibfftce505.is.de.dresdnerkb.com>

> 
> 1) A small p-value is evidence that there is dependence.
> So you want to see large p-values.  But a large p-value
> is not really evidence of independence -- merely a lack
> of evidence of dependence.

Hello Aroon,

additionally to the points voiced by Patrick, only if your series at hand is
normally distributed you can infer from uncorrelatedness (Ljung-Box test) to
independence. These two are equivalent in the case of normally distributed
random variables. Uncorrelated rv are not necessarily independent, whereas
the opposite is true. Hence, you want to utilise "Jarque-Bera-test", or some
test for normality, too.

HTH,
Bernhard 


> 
> You might be able to get a hint of the power of your test
> (which is what you really care about) from the working
> paper about Ljung-Box on the Burns Statistics website.
> 
> 2) The statistic is really an average of the lags up to the
> stated lag.  So if the dependence is all at lag 5, tests with
> lags below 5 have no power, the lag 5 test has maximum
> power, and the power decreases as the lag of the test
> increases above 5.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Aroon Nataraj wrote:
> 
> > Hi all
> > I'm using the Box-Ljung test (from within R) to test if a 
> time-series 
> > in independently distributed.
> >
> > 2 questions:
> > 1) p-value returned by Box-Ljung:
> > IF I want to test if the time-series is independant at say 0.05 
> > sig-level (it means that prob of erroneously accepting that the 
> > time-series is independent is 0.05 right?)
> > --> then do I consider time-series as "independant" when
> >      --> p-value (from Box-Ljung) > 0.05
> >      OR
> >       --> p-value < 0.05
> > Or should i be using (0.95) instead of (0.05) for this case. I'm 
> > confused about this (this is a goodness-of-fit test?).
> >
> > 2) Box-Ljung takes a lag argument, say lag=k.
> >     Does it check "all lags upto k"
> >     OR
> >     Does it check "only AT k" (i.e acf val is small at only k?)
> >
> > thank you in advance. I apologize if the questions are very basic.
> >
> > Aroon
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From ramzi.temanni at laposte.net  Mon Apr 19 10:45:32 2004
From: ramzi.temanni at laposte.net (Ramzi TEMANNI)
Date: Mon, 19 Apr 2004 10:45:32 +0200
Subject: [R] solve 2 var problem
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/SScWgRDKkWYRSKuOxiKY8KAAAAQAAAAbWJA1MYFLEiyuJ4IP3IIkQEAAAAA@laposte.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/36141154/attachment.pl

From ajayshah at mayin.org  Mon Apr 19 10:57:00 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 19 Apr 2004 14:27:00 +0530
Subject: [R] trend turning points
Message-ID: <20040419085700.GA17914@igidr.ac.in>

  >> does anybody know of a nice test to detect trend turning points in
  >> time series? Possibly with reference?
  >
  >You can look at the function breakpoints() in the package strucchange
  >and the function segmented() in the package segmented which do
  >segmentation of (generalized) linear regression models. The former
  >tries to fit fully segmented regression models, the latter broken line
  >trends.  References are given on the respective help pages.
  >
  >A suitable test for a change in trend in linear regression models is
  >the OLS-based CUSUM test with a Cramer-von Mises functional of Kraemer
  >& Ploberger (1996, JoE) which is available via efp() in strucchange
  >and associated methods.

I have seen strucchange with great interest but somehow the
combination of the R and the statistics knowledge that is required to
access it feels like a bottleneck :-) Could someone perhaps write a
simple R program which illustrates the facilities and usage?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From Bernhard.Pfaff at drkw.com  Mon Apr 19 11:01:33 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 19 Apr 2004 11:01:33 +0200
Subject: [R] Non-Linear Regression (Cobb-Douglas and C.E.S)
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B8CF@ibfftce505.is.de.dresdnerkb.com>

> Dear all,
> 
> For estimating Cobb-Douglad production Function [ Y = ALPHA * 
> (L^(BETA1)) * 
> (K^(BETA2))  ], i want to use nls function (without 
> linearizing it). But 
> how can i get initial values?
> 
> ------------------------------------
>  > options(prompt=" R> " )
>   R> Y <- c(59.6, 63.9, 73.5, 75.6, 77.3, 82.8, 83.6, 84.9, 
> 90.3, 80.5, 
> 73.5, 60.3, 58.2, 64.4, 75.4, 85, 92.7, 85.4, 92.3, 101.2, 
> 113.3, 107.8, 
> 105.2, 107.1, 108.8, 131.4, 130.9, 134.7, 129.1, 147.8, 152.1, 154.3, 
> 159.9) # production
>   R> L <- c(39.4, 41.4, 43.9, 43.3, 44.5, 45.8, 45.9, 46.4, 
> 47.6, 45.5, 
> 42.6, 39.3, 39.6, 42.7, 44.2, 47.1, 48.2, 46.4, 47.8, 49.6, 
> 54.1, 59.1, 
> 64.9, 66, 64.4, 58.9, 59.3, 60.2, 58.7, 60, 63.8, 64.9, 66) # 
> employment
>   R> K <- c(236.2, 240.2, 248.9, 254.5, 264.1, 273.9, 282.6, 
> 290.2, 299.4, 
> 303.3, 303.4, 297.1, 290.1, 285.4, 287.8, 292.1, 300.3, 301.4, 305.6, 
> 313.3, 327.4, 339, 347.1, 353.5, 354.1, 359.4, 359.3, 365.2, 
> 363.2, 373.7, 
> 386, 396.5, 408) # capital
>   R> klein <- cbind(Y,L,K)
>   R> klein.data<-data.frame(klein)
>   R> coef(lm(log(Y)~log(L)+log(K)))
> # i used these linearized model's estimated parameters as 
> initial values
> (Intercept)      log(L)      log(K)
>   -3.6529493   1.0376775   0.7187662
>   R> nls(Y~ALPHA * (L^(BETA1)) * (K^(BETA2)), 
> data=klein.data, start = 
> c(ALPHA=-3.6529493,BETA1=1.0376775,BETA2=0.7187662), trace = T)
> 6852786785 :  -3.6529493  1.0376775  0.7187662
> 1515217 :  -0.02903916  1.04258165  0.71279051
> 467521.8 :  -0.02987718  1.67381193 -0.05609925
> 346945.7 :   -0.5570735  10.2050667 -10.2087997
> Error in numericDeriv(form[[3]], names(ind), env) :
>          Missing value or an Infinity produced when 
> evaluating the model
> ------------------------------------
> 
> 1. What went wrong? I think the initial values are not good 
> enough: How can 
> i make a grid search?
> 
> 2. How can i estimate C.E.S Production Function [  Y = GAMA * 
> ((DELTA*K^(-BETA)) + ((1-DELTA)*L^(-BETA)))^(-PHI/BETA)  ] 
> using the same 
> data? How to get the initial value?
> 

Dear James, Wettenhall,

as far as the CES production function is concerned, you might want to
utilise the Kmenta approximation. The following link elucidates this
approach and other feasible estimation techniques.

http://www.cu.lu/crea/projets/mod-L/prod.pdf


HTH,
Bernhard

> N.B.: The data file is available at 
http://www.angelfire.com/ab5/get5/klein.txt

Any response / help / comment / suggestion / idea / web-link / replies will 
be greatly appreciated.

Thanks in advance for your time.

_______________________

Mohammad Ehsanul Karim <wildscop at yahoo.com>
Institute of Statistical Research and Training
University of Dhaka, Dhaka- 1000, Bangladesh

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From Achim.Zeileis at wu-wien.ac.at  Mon Apr 19 11:10:47 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 19 Apr 2004 11:10:47 +0200
Subject: [R] trend turning points
In-Reply-To: <20040419085700.GA17914@igidr.ac.in>
References: <20040419085700.GA17914@igidr.ac.in>
Message-ID: <20040419111047.7d5ac57a.Achim.Zeileis@wu-wien.ac.at>

On Mon, 19 Apr 2004 14:27:00 +0530 Ajay Shah wrote:

>   >> does anybody know of a nice test to detect trend turning points
>   >in> time series? Possibly with reference?
>   >
>   >You can look at the function breakpoints() in the package
>   >strucchange and the function segmented() in the package segmented
>   >which do segmentation of (generalized) linear regression models.
>   >The former tries to fit fully segmented regression models, the
>   >latter broken line trends.  References are given on the respective
>   >help pages.
>   >
>   >A suitable test for a change in trend in linear regression models
>   >is the OLS-based CUSUM test with a Cramer-von Mises functional of
>   >Kraemer& Ploberger (1996, JoE) which is available via efp() in
>   >strucchange and associated methods.
> 
> I have seen strucchange with great interest but somehow the
> combination of the R and the statistics knowledge that is required to
> access it feels like a bottleneck :-) Could someone perhaps write a
> simple R program which illustrates the facilities and usage?

The package has an accompanying vignette which explains the ideas behind
it and how to use its inference functions. The breakpoints() facilities
are also documented in a paper by Zeileis, Kleiber, Kraemer, Hornik
(2003, CSDA).

A mini-example would be:

## load package and data
R> data(Nile)
R> library(strucchange)

## fit, visualize and test OLS-based CUSUM test
## with a Cramer-von Mises functional
R> ocus <- efp(Nile ~ 1, type = "OLS-CUSUM")
R> plot(ocus, functional = "meanL2")
R> sctest(ocus, functional = "meanL2")

## estimate breakpoints
R> bp <- breakpoints(Nile ~ 1)
R> plot(bp)
R> summary(bp)

hth,
Z

> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>



From ripley at stats.ox.ac.uk  Mon Apr 19 11:46:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Apr 2004 10:46:05 +0100 (BST)
Subject: [R] New unique name
In-Reply-To: <40838721.7080404@univie.ac.at>
Message-ID: <Pine.LNX.4.44.0404191034160.29088-100000@gannet.stats>

On Mon, 19 Apr 2004, Erich Neuwirth wrote:

> In some languages there is a function
> gensym()
> which returns a new unique name (in the current environment).
> This is quite helpful when one has to do temporary assignments.
> I could not find such a function in R.
> Is there one?

Not that I know of.  `In the current environment' is problematic, since R
has many environments and lexical scoping so different functions can have
different environments.  It's better to pick your own:  
ErichNeuwirth190404a looks unlikely to be used by anyone else!
basename(tempfile("ENeuwirth")) also looks like a good start.

Normally this is done within the body of a function, and all one needs to 
do is to choose a symbol name not used in that function (although we do 
see occasional problem with the use of `x' causing masking). 
An alternative is to use a nested function for the computations, or to set 
up a temporary environment (most conveniently via local()).  (If perchance 
you meant the local frame, then using local() is the R equivalent.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From song.baiyi at udo.edu  Mon Apr 19 12:23:44 2004
From: song.baiyi at udo.edu (Song Baiyi)
Date: Mon, 19 Apr 2004 12:23:44 +0200
Subject: [R] How can I get the length of returning array in c++ calling R
	code?
Message-ID: <4083A8B0.4070002@udo.edu>

Hello,

Now I would like to call R code from c++ and R code return a dynamic 
length of integer array.

I used

length(eval(VECTOR_ELT(expression,1),R_GlobalEnv)) 


to return the length of result array in c++, but it did not work.

What can I do to get the length?

Thank you very much!

Baiyi Song

CEI
Dortmund Univercity



From stefanhrafnjonsson at hotmail.com  Mon Apr 19 12:34:55 2004
From: stefanhrafnjonsson at hotmail.com (Stefann Jonsso)
Date: Mon, 19 Apr 2004 10:34:55 +0000
Subject: [R] looking up value from a pair of vectors
Message-ID: <BAY10-F102nUa2v3Wy80001d2ce@hotmail.com>

Hello  R community.

Can anyone inform me how to solve this short problem? I have a dataset that 
I want to recode according to a  pair of short numerical vectors. Here is a 
short example:


# Start R code

map1 <- matrix(ncol=3, byrow=T,c( 197796,"label0",1,
197797,"label1",2,
197798,"label2",3,
197799,"label3",4,
197800,"label4",5))

xx <- as.integer(map1[,1])
lab <- map1[,2]
yy <- as.integer(map1[,3]


data1a <- c(197797, 197798, 197799, 197800,
           197797, 197797, 197797, 197797)



data1b <- some.function.or.combinaion.of.few(data1a,xx,yy)

# desired result for data1b

data1b
2 3 4 5 2 2 2 2

# End R code


data1a is always longer than xx.
all numerical entries in data1a are members of xx. (in the more general 
problem, the the number 197796 may or may not be included in data1a)
value in data1a that are not members of xx would produce a "Na"



Thanks in advance for all replies,
Stefan Jonsson



From ripley at stats.ox.ac.uk  Mon Apr 19 12:48:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Apr 2004 11:48:18 +0100 (BST)
Subject: [R] looking up value from a pair of vectors
In-Reply-To: <BAY10-F102nUa2v3Wy80001d2ce@hotmail.com>
Message-ID: <Pine.LNX.4.44.0404191147510.29472-100000@gannet.stats>

> match(data1a, xx)
[1] 2 3 4 5 2 2 2 2

On Mon, 19 Apr 2004, Stefann Jonsso wrote:

> Hello  R community.
> 
> Can anyone inform me how to solve this short problem? I have a dataset that 
> I want to recode according to a  pair of short numerical vectors. Here is a 
> short example:
> 
> 
> # Start R code
> 
> map1 <- matrix(ncol=3, byrow=T,c( 197796,"label0",1,
> 197797,"label1",2,
> 197798,"label2",3,
> 197799,"label3",4,
> 197800,"label4",5))
> 
> xx <- as.integer(map1[,1])
> lab <- map1[,2]
> yy <- as.integer(map1[,3]
> 
> 
> data1a <- c(197797, 197798, 197799, 197800,
>            197797, 197797, 197797, 197797)
> 
> 
> 
> data1b <- some.function.or.combinaion.of.few(data1a,xx,yy)
> 
> # desired result for data1b
> 
> data1b
> 2 3 4 5 2 2 2 2
> 
> # End R code
> 
> 
> data1a is always longer than xx.
> all numerical entries in data1a are members of xx. (in the more general 
> problem, the the number 197796 may or may not be included in data1a)
> value in data1a that are not members of xx would produce a "Na"

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adaikalavan.ramasamy at cancer.org.uk  Mon Apr 19 12:50:03 2004
From: adaikalavan.ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 19 Apr 2004 11:50:03 +0100
Subject: [R] looking up value from a pair of vectors
In-Reply-To: <BAY10-F102nUa2v3Wy80001d2ce@hotmail.com>
References: <BAY10-F102nUa2v3Wy80001d2ce@hotmail.com>
Message-ID: <1082371803.4352.12.camel@vpn202001.lif.icnet.uk>

> (m <- match(data1a, map1[ ,1])) 
[1] 2 3 4 5 2 2 2 2

> map1[m ,2] # to return labels                     
[1] "label1" "label2" "label3" "label4" "label1" "label1" "label1"
"label1"


On Mon, 2004-04-19 at 11:34, Stefann Jonsso wrote:
> Hello  R community.
> 
> Can anyone inform me how to solve this short problem? I have a dataset that 
> I want to recode according to a  pair of short numerical vectors. Here is a 
> short example:
> 
> 
> # Start R code
> 
> map1 <- matrix(ncol=3, byrow=T,c( 197796,"label0",1,
> 197797,"label1",2,
> 197798,"label2",3,
> 197799,"label3",4,
> 197800,"label4",5))
> 
> xx <- as.integer(map1[,1])
> lab <- map1[,2]
> yy <- as.integer(map1[,3]
> 
> 
> data1a <- c(197797, 197798, 197799, 197800,
>            197797, 197797, 197797, 197797)
> 
> 
> 
> data1b <- some.function.or.combinaion.of.few(data1a,xx,yy)
> 
> # desired result for data1b
> 
> data1b
> 2 3 4 5 2 2 2 2
> 
> # End R code
> 
> 
> data1a is always longer than xx.
> all numerical entries in data1a are members of xx. (in the more general 
> problem, the the number 197796 may or may not be included in data1a)
> value in data1a that are not members of xx would produce a Na
> 
> 
> 
> Thanks in advance for all replies,
> Stefan Jonsson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Mon Apr 19 13:00:35 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Apr 2004 12:00:35 +0100 (BST)
Subject: [R] New unique name
In-Reply-To: <40838721.7080404@univie.ac.at>
Message-ID: <XFMail.040419120035.Ted.Harding@nessie.mcc.ac.uk>

On 19-Apr-04 Erich Neuwirth wrote:
> In some languages there is a function
> gensym()
> which returns a new unique name (in the current environment).
> This is quite helpful when one has to do temporary assignments.
> I could not find such a function in R.
> Is there one?

If you are running R on a Unix-like system (such as Linux) then
you could use the "mktemp" function (see 'man mktemp').

In R,

  system("mktemp -u tmpXXXXXX")

will return a unique identifier (and will not, because of the
"-u" flag, create the file).

The identifier will be of the form (e.g.) "tmpdPT6Nw", i.e. with
the 6 X's replaced by random characters.

A bit sledgehammer for nut, but as least it meets your needs!

(Strictly speaking, uniqueness in repeated use is not absolutely
guaranteed with the "-u" option, since strict uniqueness depends
on 'mktemp' checking for an existing file with the same name.
However, since there are some (26+26+10)^6 > 10^10 combinations
to randomly choose from, you should be pretty safe.)

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 19-Apr-04                                       Time: 12:00:35
------------------------------ XFMail ------------------------------



From balajis at stanford.edu  Mon Apr 19 13:15:11 2004
From: balajis at stanford.edu (Balaji Srinivasan)
Date: Mon, 19 Apr 2004 04:15:11 -0700
Subject: [R] R analog of Matlab "eigs" function
Message-ID: <1082373311.4083b4bf31e40@webmail.stanford.edu>

Hi, 

I was wondering if anyone knew of an implementation of a function similar to
"eigs" in Matlab (full description here:
http://www.mathworks.com/access/helpdesk/help/techdoc/ref/eigs.html). 

This function differs from the standard "eigen" in that it computes a *few*
eigenvectors for cases in which your matrix is very large and/or you don't
need all the eigenvectors. (It uses the Arnoldi-Lanczos iterative method, as
implemented in C in ARPACK). For example, this is the case for classical
multidimensional scaling when you only need the first 2 eigenvectors. 

I feel almost certain that something like this is probably somewhere in R,
possibly even as a (hidden) subroutine within a function like cmdscale.
However, I didn't turn anything up after quite a bit of googling and
help.search(). Would appreciate any help with this. 

Thanks in advance, 

Balaji Srinivasan



From bweir at pcez.com  Mon Apr 19 13:27:49 2004
From: bweir at pcez.com (Brian)
Date: Mon, 19 Apr 2004 04:27:49 -0700
Subject: [R] One inflated Poisson or Negative Binomal regression
Message-ID: <000601c42601$6b4b8270$0200a8c0@deepbeige>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/9529dbd9/attachment.pl

From B.Rowlingson at lancaster.ac.uk  Mon Apr 19 13:31:44 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 19 Apr 2004 12:31:44 +0100
Subject: [R] New unique name
In-Reply-To: <XFMail.040419120035.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040419120035.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <4083B8A0.1070302@lancaster.ac.uk>

(Ted Harding) wrote:

> A bit sledgehammer for nut, but as least it meets your needs!

  You could even use uuidgen to create a universally unique ID, and then 
use make.names to R-ify it:

 > make.names(system("uuidgen",intern=T))
[1] "aa33d26c.88a5.4eab.94ba.5073c4718ffe"
 > make.names(system("uuidgen",intern=T))
[1] "d5aea7a0.81e9.4690.8d48.d74fd2b50a83"
 > make.names(system("uuidgen",intern=T))
[1] "X2570d3e0.6b07.42be.a9c6.3701ac82b4f0"

  Of course the requirement was to make an object with a name that 
didn't already exist, and there's no guarantee that you haven't already 
created an object called "X2570d3e0.6b07.42be.a9c6.3701ac82b4f0" for 
some reason.

  So that's no good.

  How about getting an alphabetically-sorted list of all the current 
objects and then using the last one augmented with something:

makeUnique <- function(){
  allObjects=sort(ls(pos=seq(1:length(search()))))
  paste(allObjects[length(allObjects)],"-temp",sep='')
}

  I'm sure someone can come up with a bigger sledgehammer, or a reason 
why this wont always be unique. What's the longest name of an object in 
R? That'll break it... Pah.

Baz



From ripley at stats.ox.ac.uk  Mon Apr 19 13:52:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 19 Apr 2004 12:52:13 +0100 (GMT Daylight Time)
Subject: [R] New unique name
In-Reply-To: <4083B8A0.1070302@lancaster.ac.uk>
Message-ID: <Pine.WNT.4.44.0404191247200.2328-100000@gannet.stats.ox.ac.uk>

On Mon, 19 Apr 2004, Barry Rowlingson wrote:

> (Ted Harding) wrote:
>
> > A bit sledgehammer for nut, but as least it meets your needs!
>
>   You could even use uuidgen to create a universally unique ID, and then
> use make.names to R-ify it:
>
>  > make.names(system("uuidgen",intern=T))
> [1] "aa33d26c.88a5.4eab.94ba.5073c4718ffe"
>  > make.names(system("uuidgen",intern=T))
> [1] "d5aea7a0.81e9.4690.8d48.d74fd2b50a83"
>  > make.names(system("uuidgen",intern=T))
> [1] "X2570d3e0.6b07.42be.a9c6.3701ac82b4f0"
>
>   Of course the requirement was to make an object with a name that
> didn't already exist, and there's no guarantee that you haven't already
> created an object called "X2570d3e0.6b07.42be.a9c6.3701ac82b4f0" for
> some reason.
>
>   So that's no good.
>
>   How about getting an alphabetically-sorted list of all the current
> objects and then using the last one augmented with something:
>
> makeUnique <- function(){
>   allObjects=sort(ls(pos=seq(1:length(search()))))
>   paste(allObjects[length(allObjects)],"-temp",sep='')
> }
>
>   I'm sure someone can come up with a bigger sledgehammer, or a reason
> why this wont always be unique. What's the longest name of an object in
> R? That'll break it... Pah.

There is more to the environment than the search path, which is what makes
this tricky.  I suspect Erich really only wanted to avoid objects in the
current frame (which is not on the search path except for interactive use
of R), but he didn't tell us too precisely.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From monica.palaseanu-lovejoy at stud.man.ac.uk  Mon Apr 19 14:03:11 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Mon, 19 Apr 2004 13:03:11 +0100
Subject: [R] geoR - help for bayesian modelling
Message-ID: <E1BFXUk-000B7B-26@probity.mcc.ac.uk>

Hi,

I am trying to do a bayesian prediction for soil pollution data above 
a certain threshold, using geoR. 

Everything is working fine until i am doing the krig.bayes ( I am 
using as a guide the geoR tutorial from the web page 
http://www.est.ufpr.br/geoR/geoRdoc/geoRintro.html#starting). I 
tried to do the prediction on a grid 67 by 113 cells and my 
computer is freezing to death. At larger numbers of cells it tells me 
after a while that it reaches the max. memory of 511 Mb. My 
computer has only 512 Mb of RAM. What RAM capacity should i 
look for to do a 150 x 250 cell grid??? (I tried the modelling on a 1 
Gb RAM computer and it didn't work either). I am interested to do a 
modelling where my resolution is 5 m x 5 m (150 x 250 grid cell).

If i want to do the prediction on my initial data locations (well, 
actually the prediction points are shifted 1 m in X and respectively 
Y direction, so the raw data coordinates don't coincide with the 
prediction coordinates) i am getting the following error using the 
command:

zn.bayes <- krige.bayes(zn.gdata, loc = xy, model = 
model.control(cov.model = "exponential", lambda = 0), prior = 
prior.control(phi.prior ="exponential", phi = 89.1894), 
output=output.control(n.predictive=2, mean.var = TRUE, quantile = 
c(0.025,0.25, 0.5, 0.75, 0.975), threshold = c(300)))

Error in cond.sim(env.loc = base.env, env.iter = iter.env, 
loc.coincide = get("loc.coincide",  : 
        chol: matrix not pos def, diag[13]= -1.279220e-018

I will really appreciate any suggestion you may have.

Thank you so much,

Monica



From Jonathan.Swinton at astrazeneca.com  Mon Apr 19 14:16:09 2004
From: Jonathan.Swinton at astrazeneca.com (Swinton, Jonathan)
Date: Mon, 19 Apr 2004 13:16:09 +0100
Subject: [R] How to write an S4 method for sum or a Summary generic
Message-ID: <FCA5F290CE7FFD42A6F1515497B0F0A20436218E@ukapphresmsx02.ukapd.astrazeneca.net>

If I have a class Foo, then i can write an S3 method for sum for it:

>setClass("Foo",representation(a="integer"));aFoo=new("Foo",a=c(1:3,NA))
>sum.Foo <- function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
>sum(aFoo)

But how do I write an S4 method for this? All my attempts to do so have
foundered. For example
>setMethod("sum",signature("Foo","logical"),
function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
creates a method which seems to despatch on na.rm="Foo":
> getMethods("sum")
na.rm = "ANY":
function (..., na.rm = FALSE) 
.Internal(sum(..., na.rm = na.rm))

na.rm = "Foo":
function (..., na.rm = FALSE) 
{
    .local <- function (x, na.rm) 
    {
        print(x)
        print(na.rm)
        sum(x at a, na.rm = na.rm)
    }
    .local(..., na.rm = na.rm)
}

na.rm = "missing":
function (..., na.rm = FALSE) 
.Internal(sum(..., na.rm = na.rm))
##:    (inherited from na.rm = "ANY")

Pages 350-352 of the Green book discuss at some length how to write a
generic function for Summary group generics which uses tail recursion to
allow the correct method to be called on each member of a ... argument list.
But it gives no examples of what individual method functions need to look
like. Any ideas or a place to look for working code?

Jonathan Swinton, Statistical Scientist, Computational Biology, Pathway
Analysis, Global Sciences and Information, AstraZeneca.



From joehl at gmx.de  Mon Apr 19 14:23:17 2004
From: joehl at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Mon, 19 Apr 2004 14:23:17 +0200 (MEST)
Subject: [R] New unique name and fixing getAnywhere()
Message-ID: <32269.1082377397@www1.gmx.net>


# what about

gensym <- function(root="GeneratedSymbolname", pool=c(letters, LETTERS,
0:1), n=16, sep="_")
{
  todo <- TRUE
  while (todo){
    symbolname <- paste(root, paste(sample(pool, n, TRUE), collapse=""),
sep=sep)
    todo <- length(getAnywhere(symbolname)$objs)
  }
  symbolname
}

# but this requires a slightly changed version of getAnywhere()
# which currently finds: getAnywhere("find")
# but does not find: symbolname <- "find"; getAnywhere(symbolname)
# (BTW current getAnywhere() has returnvalue$objs whereas the documentation
says returnvalue$funs)
# the following patch avoids this problem and is more aligned with get()

getAnywhere <- function(x)
{
    stopifnot(is.character(x))
    objs <- list()
    where <- character(0)
    visible <- logical(0)
    if (length(pos <- find(x, numeric = TRUE))) {
        objs <- lapply(pos, function(pos, x) get(x, pos = pos),
            x = x)
        where <- names(pos)
        visible <- rep.int(TRUE, length(pos))
    }
    if (length(grep(".", x, fixed = TRUE))) {
        np <- length(parts <- strsplit(x, ".", fixed = TRUE)[[1]])
        for (i in 2:np) {
            gen <- paste(parts[1:(i - 1)], collapse = ".")
            cl <- paste(parts[i:np], collapse = ".")
            if (!is.null(f <- getS3method(gen, cl, TRUE))) {
                ev <- topenv(environment(f), NULL)
                nmev <- if (isNamespace(ev))
                  getNamespaceName(ev)
                else NULL
                objs <- c(objs, f)
                msg <- paste("registered S3 method for", gen)
                if (!is.null(nmev))
                  msg <- paste(msg, "from namespace", nmev)
                where <- c(where, msg)
                visible <- c(visible, FALSE)
            }
        }
    }
    for (i in loadedNamespaces()) {
        ns <- asNamespace(i)
        if (exists(x, envir = ns, inherits = FALSE)) {
            f <- get(x, envir = ns, inherits = FALSE)
            objs <- c(objs, f)
            where <- c(where, paste("namespace", i, sep = ":"))
            visible <- c(visible, FALSE)
        }
    }
    ln <- length(objs)
    dups <- rep.int(FALSE, ln)
    objs2 <- lapply(objs, function(x) {
        if (is.function(x))
            environment(x) <- NULL
        x
    })
    if (ln > 1)
        for (i in 2:ln) for (j in 1:(i - 1)) if (identical(objs2[[i]],
            objs2[[j]])) {
            dups[i] <- TRUE
            break
        }
    res <- list(name = x, objs = objs, where = where, visible = visible,
        dups = dups)
    class(res) <- "getAnywhere"
    res
}



-- 

Ab sofort DSL-Tarif ohne Grundgeb??hr: http://www.gmx.net/info



From andy_liaw at merck.com  Mon Apr 19 14:30:42 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Apr 2004 08:30:42 -0400
Subject: [R] outliers using Random Forest
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C15@usrymx25.merck.com>

> From: Edgar Acuna [mailto:edgar at cs.uprm.edu] 
> 
> Dear Andy,
> Thanks for your quick answer. I increased the number of trees and the
> outlyingness measure got more stable. But still I do not know if I am
> working with the raw measure or with the normalized measure mentioned
> in the Breiman's Wald lecture. The normalized measure nout is
> 
> nout=(nout-med)/mean(abs(nout-med))
> where med is the median of the class containing the case correponding
> to nout.

Looking at the Fortran subroutine `locateout' in rfsub.f, yes, they are
normalized.  (That part of the code is not changed from Breiman & Cutler's
original.)

Andy

 
> Best regards
> Edgar Acuna
> 
> On Sun, 18 Apr 2004, Liaw, Andy wrote:
> 
> > The thing to do is probably:
> >
> > 1. Use fairly large number of trees (e.g., 1000).
> > 2. Run a few times and average the results.
> >
> > The reason for the instability is sort of two fold:
> >
> > 1. The random forest algorithm itself is based on 
> randomization.  That's why
> > it's probably a good idea to have 500-1000 trees to get more stable
> > proximity measures (of which the outlying measures are based on).
> >
> > 2. If you are running randomForest in unsupervised mode 
> (i.e., not giving it
> > the class labels), then the program treats the data as 
> "class 1", creates a
> > synthetic "class 2", and run the classification algorithm to get the
> > proximity measures.  You probably need to run the algorithm 
> a few times so
> > that the result will be based on several simulated data, 
> instead of just
> > one.
> >
> > HTH,
> > Andy
> >
> > > From: Edgar Acuna
> > >
> > > Hello,
> > > Does anybody know if the outscale option of randomForest 
> yields the
> > > standarized version of the outlier measure for each case? or
> > > the results
> > > are only the raw values. Also I have notice that this 
> measure presents
> > > very high variability. I mean if I repeat the experiment I am
> > > getting very
> > > different values for this measure and it is hard to flag 
> the outliers.
> > > This does not happen with two other criteria than I am 
> using: LOF and
> > > Bay's Orca. I am getting several cases that can be considered
> > > as outliers
> > > with both approaches.
> > >  I run my experiments  using Bupa and Diabetes available at
> > > UCI Machine database repository.
> > >
> > > Thanks in advance for any response.
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments, contains
> > information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New
> > Jersey, USA 08889), and/or its affiliates (which may be 
> known outside the
> > United States as Merck Frosst, Merck Sharp & Dohme or MSD 
> and in Japan as
> > Banyu) that may be confidential, proprietary copyrighted 
> and/or legally
> > privileged. It is intended solely for the use of the 
> individual or entity
> > named on this message.  If you are not the intended 
> recipient, and have
> > received this message in error, please notify us 
> immediately by reply e-mail
> > and then delete it from your system.
> > 
> --------------------------------------------------------------
> ----------------
> >
> 
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From erich.neuwirth at univie.ac.at  Mon Apr 19 14:42:16 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 19 Apr 2004 14:42:16 +0200
Subject: [R] New unique name
In-Reply-To: <Pine.WNT.4.44.0404191247200.2328-100000@gannet.stats.ox.ac.uk>
References: <Pine.WNT.4.44.0404191247200.2328-100000@gannet.stats.ox.ac.uk>
Message-ID: <4083C928.80003@univie.ac.at>

Since I have not been precise enough, let me explain in more detail.
I am executing some R code in the COM server.
For making function calls with parameters existing as ranges in Excel,
I need to assign values for the arguments of the function call to
temporary R variables.
This is the only way of transferring large matrices quickly from Excel
to R.
Then, I construct a string which is the function call to be executed.
The string contains the names of the newly created R variables
for the function arguments.
This string then is sent to R and executed as a command.
Therefore, I need names which do not get in conflict with anything
defined at the moment, because these names have to hold my transferred
matrices and should not change anything that exists at that moment.
I do some funny stuff with a strange word and the date and the clock
at the moment, but I would like a cleaner solution.

What is an easy way of doing this?

All the Linux based solutions will not work, since this for the
RExcel project.

Erich Neuwirth



From jc at or.psychology.dal.ca  Mon Apr 19 14:41:07 2004
From: jc at or.psychology.dal.ca (John Christie)
Date: Mon, 19 Apr 2004 09:41:07 -0300
Subject: [R] looking for something like "ave" I can pass non numeric to
Message-ID: <D41BB6F7-91FE-11D8-8F67-000A9576D04E@or.psychology.dal.ca>

Hi,
	I have been trying to calculate summary error and coding statistics on 
a by subject basis and seem to be writing a lot of code to do a simple 
thing.  I won't go into my messy version.  What I am asking is if 
anyone knows of a single command that could take the following data and 
get error rates over a given vector.

Given data from ss with,,,

subj	a	rt 	code
1	1	200	good
1	1	321	good
1	2	457	good
1	2	384	bad
2	1	228	good
2	1	343	bad
....

I would like to do a calculation something like ave returns.  An 
artificial example that does not work where I have passed the code 
field to ave is below.

ss$FAcount<- ave(ss$code, ss$subj, function(x) 
length(x[x=="good"])/length(x))



From ripley at stats.ox.ac.uk  Mon Apr 19 15:02:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Apr 2004 14:02:24 +0100 (BST)
Subject: [R] R-1.9.0: make error on slackware-current!
In-Reply-To: <Pine.LNX.4.44.0404180903580.13354-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404191400120.4405-100000@gannet.stats>

I have been able to reproduce this.  It appears that _some_ XFree 4.4.0 
Linux installations need 

#include <stdio.h>

so please add that before the X11/X.h call in src/modules/X11/dataentry.c

It seems nothing to do with _Xconst.


On Sun, 18 Apr 2004, Prof Brian Ripley wrote:

> On Sun, 18 Apr 2004, Timothy Tatar wrote:
> 
> > It appears that the #If NeedFunctionPrototypes compiler directive has been
> > removed from Xlib.h and Xutil.h in Xfree86 4.4. All the prototypes
> > containing the offending _Xconst are now being processed. R 1.8.1, which
> > built successfully under XFree86 4.3, fails under XFree86 4.4 with the same
> > error messages.
> 
> Yes, we do know.  However, 1.9.0 _has_ been built against XFree 4.4, and
> also XFree 8.3 with NeedFunctionPrototypes defined to be 1.  _Xconst 
> should be defined, so the problem seems to be with `slackware-current' 
> (whatever that is) rather than XFree 4.4.  Does adding
> 
> #ifndef _Xconst
> #define _Xconst const
> #endif /* _Xconst */
> 
> help?  (That's inside #if NeedFunctionPrototypes in the headers I have.)
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Apr 19 15:09:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Apr 2004 14:09:34 +0100 (BST)
Subject: [R] R-1.9.0: make error on slackware-current!
In-Reply-To: <Pine.LNX.4.44.0404191400120.4405-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404191408240.5627-100000@gannet.stats>

Sorry, a time skew caused a problem here:

removing the line 

#define NeedFunctionPrototypes 0

is also needed.

On Mon, 19 Apr 2004, Prof Brian Ripley wrote:

> I have been able to reproduce this.  It appears that _some_ XFree 4.4.0 
> Linux installations need 
> 
> #include <stdio.h>
> 
> so please add that before the X11/X.h call in src/modules/X11/dataentry.c
> 
> It seems nothing to do with _Xconst.
> 
> 
> On Sun, 18 Apr 2004, Prof Brian Ripley wrote:
> 
> > On Sun, 18 Apr 2004, Timothy Tatar wrote:
> > 
> > > It appears that the #If NeedFunctionPrototypes compiler directive has been
> > > removed from Xlib.h and Xutil.h in Xfree86 4.4. All the prototypes
> > > containing the offending _Xconst are now being processed. R 1.8.1, which
> > > built successfully under XFree86 4.3, fails under XFree86 4.4 with the same
> > > error messages.
> > 
> > Yes, we do know.  However, 1.9.0 _has_ been built against XFree 4.4, and
> > also XFree 8.3 with NeedFunctionPrototypes defined to be 1.  _Xconst 
> > should be defined, so the problem seems to be with `slackware-current' 
> > (whatever that is) rather than XFree 4.4.  Does adding
> > 
> > #ifndef _Xconst
> > #define _Xconst const
> > #endif /* _Xconst */
> > 
> > help?  (That's inside #if NeedFunctionPrototypes in the headers I have.)
> > 
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Mon Apr 19 15:58:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Apr 2004 09:58:25 -0400
Subject: [R] solve 2 var problem
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C18@usrymx25.merck.com>

trace(K) is just sum(diag(K)).  Assuming by `.' you mean multiplication
(I'll use `*' below),

trace(x*A + y*B) = x*trace(A) + y*trace(B)

Let a = trace(A) = sum(diag(A)) and b = trace(B) = sum(diag(B)).  You are
then looking for (x, y) that satisfy

a*x + b*y < C

which ought to be trivial.  I don't think you need R for that.

Andy

> From: Ramzi TEMANNI
> 
> Hi
> 
> I'm getting started with R and i have difficulties finding 
> how to solve this
> problem in R :
> 
>  
> 
> Find x,y satisfying 
> 
> Trace(K) < C, 
> 
> K positive where ( K=x.A+y.B), 
> 
> [A,B,K square Matrix in  R(n x n), x,y in R]
> 
>  
> 
> Thanks in advance,
> 
> Ramzi
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From zelickr at pdx.edu  Mon Apr 19 16:00:48 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Mon, 19 Apr 2004 07:00:48 -0700 (PDT)
Subject: [R] histogram y-scaling
In-Reply-To: <Pine.LNX.4.44.0404191408240.5627-100000@gannet.stats>
Message-ID: <Pine.GSO.4.44.0404190653030.10073-100000@gere.odin.pdx.edu>

Hello all,

Relative to WinXP & R1.8....

I have two histograms to plot, and for comparison purposes I want them to
have the same Y-scaling. I tried to find the size of the bin with the
maximum count before generating the histogram, but this did not work (see
below). What is a better way?

par(mfrow=c(2,1)) # set up for plotting in 2 rows and 1 column

x1<-seq(-0.5,58.5,1) # make a range of x values for histogram

I thought the following lines would allow me to capture the results of the
hist function and determine the max bin count for scaling *before* making
the plot, but R cleverly saw around my method and plots it anyway. With
this code I get two plots.

q=hist(mt1,x1)       # stick results in a variable... alas also plots
cts=q$counts         # get the bin counts
mct1=max(cts)        # how many values in the bin with the most values
hist(mt1,x1)         # generate histogram plot

# go on with histogram #2...


Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From ripley at stats.ox.ac.uk  Mon Apr 19 16:15:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Apr 2004 15:15:30 +0100 (BST)
Subject: [R] histogram y-scaling
In-Reply-To: <Pine.GSO.4.44.0404190653030.10073-100000@gere.odin.pdx.edu>
Message-ID: <Pine.LNX.4.44.0404191510590.18641-100000@gannet.stats>

?hist reveals argument plot=TRUE, so try plot=FALSE.

On Mon, 19 Apr 2004, Randy Zelick wrote:

> Hello all,
> 
> Relative to WinXP & R1.8....

No such thing.  There is R 1.8.0 and R 1.8.1 but not R 1.8.

> I have two histograms to plot, and for comparison purposes I want them to
> have the same Y-scaling. I tried to find the size of the bin with the
> maximum count before generating the histogram, but this did not work (see
> below). What is a better way?
> 
> par(mfrow=c(2,1)) # set up for plotting in 2 rows and 1 column
> 
> x1<-seq(-0.5,58.5,1) # make a range of x values for histogram
> 
> I thought the following lines would allow me to capture the results of the
> hist function and determine the max bin count for scaling *before* making
> the plot, but R cleverly saw around my method and plots it anyway. With
> this code I get two plots.
> 
> q=hist(mt1,x1)       # stick results in a variable... alas also plots
> cts=q$counts         # get the bin counts
> mct1=max(cts)        # how many values in the bin with the most values
> hist(mt1,x1)         # generate histogram plot
> 
> # go on with histogram #2...

Something like
q1 <- hist(mt1, x1, plot = FALSE)
q2 <- hist(mt2, x2, plot = FALSE)
mctl <- max(q1$counts, q2$counts)
plot(q1, ylim=c(0, mctl))
plot(q2,  ylim=c(0, mctl))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Carlisle.Thacker at noaa.gov  Mon Apr 19 16:20:43 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Mon, 19 Apr 2004 10:20:43 -0400
Subject: [R] Problem with bwplot and panel.abline
Message-ID: <4083E03B.48E261A9@noaa.gov>

When plotting box & whisker plots to show the distributions of
residuals of several models, it would be useful to add a vertical line
to indicate zero residuals.  By adding an explicit panel function that
includes panel.abline and panel.bwplot, it is possible to get the
vertical line, but the median is no longer indicated.

The problem seems to be with panel.bwplot ---  omitting panel.abline
does not restore the median indicator.  Adding pch=16 explicitly as an
argument to panel.bwplot does restore the dot, but in a strange color
(yellow).  Adding col="black" explicitly as an argument to
panel.bwplot makes the background black.

Any suggestions would be highly appreciated.

platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    8.0                 
year     2003                
month    10                  
day      08                  
language R                   

> rlm1.residuals[1,]
   p      rlm1
1 25 0.1878673

postscript(file="bw.residual.verification.eps",horizontal=F,paper="letter")
trellispar <- trellis.par.get("plot.symbol")
trellispar$pch <- "."
trellis.par.set("plot.symbol",trellispar)
trellispar <- trellis.par.get("box.dot")
trellispar$cex <- 0.4
trellispar$col <- "black"
trellis.par.set("box.dot",trellispar)
trellispar <- trellis.par.get("box.umbrella")
trellispar$lty <- 1
trellis.par.set("box.umbrella",trellispar)
bwplot(-p~rlm1,
    data=rlm1.residuals,
as.table=TRUE,
panel = function(x,y,subscripts,...){
        panel.abline(v=0,lty=3,lwd=1,col="red")
        panel.bwplot(x,y,subscripts,pch=16,...)},
scales=list(x=list(cex=0.7),
            y=list(tick.number=length(where),
                            at=length(where):1,
                         labels=format(where),
                            cex=0.7)),
 xlab = "measured salinity - estimated salinity (psu)",
 ylab = "pressure (dbar)",
  sub = list("lm(s~t+I(t^2))",font=1),
main=list(paste(format(length(unique(P0.verification$id))),
"verification profiles"),font=1))
dev.off()

Thanks,

Carlisle

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From tlumley at u.washington.edu  Mon Apr 19 16:22:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Apr 2004 07:22:25 -0700 (PDT)
Subject: [R] New unique name
In-Reply-To: <4083C928.80003@univie.ac.at>
References: <Pine.WNT.4.44.0404191247200.2328-100000@gannet.stats.ox.ac.uk>
	<4083C928.80003@univie.ac.at>
Message-ID: <Pine.A41.4.58.0404190718180.135316@homer10.u.washington.edu>

On Mon, 19 Apr 2004, Erich Neuwirth wrote:

> Since I have not been precise enough, let me explain in more detail.
> I am executing some R code in the COM server.
> For making function calls with parameters existing as ranges in Excel,
> I need to assign values for the arguments of the function call to
> temporary R variables.
> This is the only way of transferring large matrices quickly from Excel
> to R.
> Then, I construct a string which is the function call to be executed.
> The string contains the names of the newly created R variables
> for the function arguments.
> This string then is sent to R and executed as a command.

I used the following for local variables in a macro

gensym<-function(base=".v.",envir=parent.frame()){
   repeat{
        nm<-paste(base,paste(sample(letters,7,replace=TRUE),
                collapse=""),sep=".")
        if (!exists(nm,envir=envir))
                break
    }
    as.name(nm)
}


You wouldn't need the as.name() conversion, since you want a string.

A sufficiently clever person could come up with code that this could
conflict with (eg, if your code relied on the non-existence of
.v..agdefge), but in normal circumstances it should be fine.

	-thomas



From dmurdoch at pair.com  Mon Apr 19 16:33:30 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 19 Apr 2004 10:33:30 -0400
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <MABBLJDICACNFOLGIHJOOEGIEFAA.phgrosjean@sciviews.org>
References: <76A000A82289D411952F001083F9DD06047FE5A3@exsalem4-bu.odot.state.or.us>
	<MABBLJDICACNFOLGIHJOOEGIEFAA.phgrosjean@sciviews.org>
Message-ID: <vbo780pf5h2faimos632up3qirsqr6oa5e@4ax.com>

On Mon, 19 Apr 2004 10:05:48 +0200, "Philippe Grosjean"
<phgrosjean at sciviews.org> wrote :

>The buffered output is a nice thing, and if the user want to use it, then
>fine! However, it should be nice to know if it is set ON or OFF, and to
>temporarily change it for some outputs in R scripts. I think this is the
>primary request. Then, your request appears to me as a secondary one: to set
>buffered output ON or OFF at statup of Rgui (by defining it in the
>preference panel). Personnally, I would really like to have both.

In the long term, I'd like it if all of Rgui could be moved to a
package.  From most users' points of view things wouldn't change much,
but it would mean it was more practical for someone like you to write
your own front-end, either by modifying the standard one or by
starting from scratch.

Duncan Murdoch



From paolo.radaelli at unimib.it  Mon Apr 19 16:40:17 2004
From: paolo.radaelli at unimib.it (Paolo Radaelli)
Date: Mon, 19 Apr 2004 16:40:17 +0200
Subject: [R] R-estimators
Message-ID: <024e01c4261c$3bd60cc0$90788495@dimequant.unimib.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/980e9b26/attachment.pl

From deepayan at stat.wisc.edu  Mon Apr 19 16:40:52 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 19 Apr 2004 09:40:52 -0500
Subject: [R] Problem with bwplot and panel.abline
In-Reply-To: <4083E03B.48E261A9@noaa.gov>
References: <4083E03B.48E261A9@noaa.gov>
Message-ID: <200404190940.52824.deepayan@stat.wisc.edu>


Your example is not reproducible. The following gives what I would 
expect (even with your modified settings):

data(singer)
bwplot(voice.part ~ height, data=singer, 
       panel=function(x,y,...) {
           panel.abline(v=65)
           panel.bwplot(x,y,...)
       })

My suspicion is that what you are seeing has to do with your modified 
settings and nothing to do with "bwplot and panel.abline" as your 
subject suggests.

Deepayan

On Monday 19 April 2004 09:20, W. C. Thacker wrote:
> When plotting box & whisker plots to show the distributions of
> residuals of several models, it would be useful to add a vertical
> line to indicate zero residuals.  By adding an explicit panel
> function that includes panel.abline and panel.bwplot, it is possible
> to get the vertical line, but the median is no longer indicated.
>
> The problem seems to be with panel.bwplot ---  omitting panel.abline
> does not restore the median indicator.  Adding pch=16 explicitly as
> an argument to panel.bwplot does restore the dot, but in a strange
> color (yellow).  Adding col="black" explicitly as an argument to
> panel.bwplot makes the background black.
>
> Any suggestions would be highly appreciated.
>
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    1
> minor    8.0
> year     2003
> month    10
> day      08
> language R
>
> > rlm1.residuals[1,]
>
>    p      rlm1
> 1 25 0.1878673
>
> postscript(file="bw.residual.verification.eps",horizontal=F,paper="le
>tter") trellispar <- trellis.par.get("plot.symbol")
> trellispar$pch <- "."
> trellis.par.set("plot.symbol",trellispar)
> trellispar <- trellis.par.get("box.dot")
> trellispar$cex <- 0.4
> trellispar$col <- "black"
> trellis.par.set("box.dot",trellispar)
> trellispar <- trellis.par.get("box.umbrella")
> trellispar$lty <- 1
> trellis.par.set("box.umbrella",trellispar)
> bwplot(-p~rlm1,
>     data=rlm1.residuals,
> as.table=TRUE,
> panel = function(x,y,subscripts,...){
>         panel.abline(v=0,lty=3,lwd=1,col="red")
>         panel.bwplot(x,y,subscripts,pch=16,...)},
> scales=list(x=list(cex=0.7),
>             y=list(tick.number=length(where),
>                             at=length(where):1,
>                          labels=format(where),
>                             cex=0.7)),
>  xlab = "measured salinity - estimated salinity (psu)",
>  ylab = "pressure (dbar)",
>   sub = list("lm(s~t+I(t^2))",font=1),
> main=list(paste(format(length(unique(P0.verification$id))),
> "verification profiles"),font=1))
> dev.off()
>
> Thanks,
>
> Carlisle
>
> William Carlisle Thacker
>
> Atlantic Oceanographic and Meteorological Laboratory
> 4301 Rickenbacker Causeway, Miami, Florida 33149 USA
> Office: (305) 361-4323           Fax: (305) 361-4392
>
> "Too many have dispensed with generosity
>      in order to practice charity."     Albert Camus
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From dmurdoch at pair.com  Mon Apr 19 16:50:43 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 19 Apr 2004 10:50:43 -0400
Subject: [R] New unique name
In-Reply-To: <4083C928.80003@univie.ac.at>
References: <Pine.WNT.4.44.0404191247200.2328-100000@gannet.stats.ox.ac.uk>
	<4083C928.80003@univie.ac.at>
Message-ID: <6mp7809d3qe951vhu4i2oonnn2af40vlve@4ax.com>

I think the function below will tell you all the names that are in
scope at the point where it is called.  You can then make up something
that's not in the list.

Duncan Murdoch

all.names <- function() {
    env <- new.env(parent=parent.frame())
    result <- ls(env=env)
    while (!is.null(env)) {
        env <- parent.env(env)
        result <- c(result, ls(env=env))
    }
    sort(unique(result))
}



From Jonathan.Swinton at astrazeneca.com  Mon Apr 19 17:20:09 2004
From: Jonathan.Swinton at astrazeneca.com (Swinton, Jonathan)
Date: Mon, 19 Apr 2004 16:20:09 +0100
Subject: [R] Unexpected behaviour of identical
Message-ID: <FCA5F290CE7FFD42A6F1515497B0F0A204362194@ukapphresmsx02.ukapd.astrazeneca.net>


 # works as expected
> ac <- c('A','B');
> identical(ac,ac[1:2])
[1] TRUE
 
 #but
> af <- factor(ac)
> identical(af,af[1:2])
[1] FALSE

Any opinions?

Jonathan Swinton, Statistical Scientist, Computational Biology, Pathway
Analysis, Global Sciences and Information, AstraZeneca



From charles.edwin.white at us.army.mil  Mon Apr 19 17:15:39 2004
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Mon, 19 Apr 2004 11:15:39 -0400
Subject: [R] Menu addditions to Rcmdr v0.9-6
Message-ID: <12D0D00E1404D511A4820090274CA09C03FBA630@dasmtyjqf010.amedd.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/1debefd0/attachment.pl

From rossini at blindglobe.net  Mon Apr 19 17:31:20 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 19 Apr 2004 08:31:20 -0700
Subject: [R] New unique name
In-Reply-To: <40838721.7080404@univie.ac.at> (Erich Neuwirth's message of
	"Mon, 19 Apr 2004 10:00:33 +0200")
References: <40838721.7080404@univie.ac.at>
Message-ID: <85fzb0dlx3.fsf@servant.blindglobe.net>


One possibility would be the Ruuid library in BioConductor.  I'm not
sure if the uuid library  is available (or compilable) under MS
Windows, though.   I'll be checking in a bit (weeks) if no one else
does.


Erich Neuwirth <erich.neuwirth at univie.ac.at> writes:

> In some languages there is a function
> gensym()
> which returns a new unique name (in the current environment).
> This is quite helpful when one has to do temporary assignments.
> I could not find such a function in R.
> Is there one?
>
>
> -- 
> Erich Neuwirth, Computer Supported Didactics Working Group
> Visit our SunSITE at http://sunsite.univie.ac.at
> Phone: +43-1-4277-38624 Fax: +43-1-4277-9386
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From phgrosjean at sciviews.org  Mon Apr 19 18:03:04 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 19 Apr 2004 18:03:04 +0200
Subject: [R] Turning windows screen buffering on and off
In-Reply-To: <vbo780pf5h2faimos632up3qirsqr6oa5e@4ax.com>
Message-ID: <MABBLJDICACNFOLGIHJOAEHDEFAA.phgrosjean@sciviews.org>

Duncan Murdoch wrote:
>In the long term, I'd like it if all of Rgui could be moved to a
>package.  From most users' points of view things wouldn't change much,
>but it would mean it was more practical for someone like you to write
>your own front-end, either by modifying the standard one or by
>starting from scratch.

I totally agree. Recent developments in SciViews (which will be publicly
released in May) focus on GUI extentions *for* Rgui, not *in replacement* of
it as it was the case for SciViews Insider. I have difficulties with
specific aspects like buffered output, whose state is hard to know from R
code... This would be a lot more easier if all Rgui extensions could be
accessed through R code. A dedicated package is an excellent idea.

OK, that said, I go back to my work, because I still have a lot to finish
for UseR! conference...
Best regards,

Philippe Grosjean

.......................................................<?}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From rafan at infor.org  Mon Apr 19 18:12:25 2004
From: rafan at infor.org (Rong-En Fan)
Date: Tue, 20 Apr 2004 00:12:25 +0800
Subject: [R] classification and association rules in R
Message-ID: <20040419161225.GA68519@muse.csie.ntu.edu.tw>

hello,

  I am looking for a classification or/and association rules in R.
However, after searching in CRAN, nothing found. Is anyone know
if they are available in R?

By the way, I heard that there are some people developing a better
search interface for R (or CRAN?). Where are the related information
I can get?

Thanks.

Regards,
Rong-En Fan



From rxg218 at psu.edu  Mon Apr 19 18:34:23 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 19 Apr 2004 12:34:23 -0400
Subject: [R] using subscripts in a plot title with 2 lines
Message-ID: <1082392463.3162.10.camel@ra.chem.psu.edu>

Hi,
  I'm making a plot in which the title takes up two lines. The title
contains a subscript but when I look at the plot it does'nt seem tocome
out properly. The code I'm using is:

   xtxt = expression(paste('Observed -log( ', IC[50], ' )'))
   ytxt = expression(paste('Predicted -log( ', IC[50], ' )'))
   mtxt = expression(paste('Plot of Observed vs. Predicted -log(',
IC[50], ') Values for the PDGFR\nDataset'))

   plot(tset$V2, tset$V3, xlim = c(-2,2), ylim = c(-2,2),
    pch = 19, col = "blue", 
    main=mtxt,
    xlab = xtxt,
    ylab = ytxt)

Removing the IC[50] term makes it come out OK. 
Is there any way to get around this?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Does Ramanujan know Polish?
-- E.B. Ross



From bates at stat.wisc.edu  Mon Apr 19 18:37:43 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 19 Apr 2004 11:37:43 -0500
Subject: [R] How to write an S4 method for sum or a Summary generic
In-Reply-To: <FCA5F290CE7FFD42A6F1515497B0F0A20436218E@ukapphresmsx02.ukapd.astrazeneca.net>
References: <FCA5F290CE7FFD42A6F1515497B0F0A20436218E@ukapphresmsx02.ukapd.astrazeneca.net>
Message-ID: <6r7jwbzzxk.fsf@bates4.stat.wisc.edu>

"Swinton, Jonathan" <Jonathan.Swinton at astrazeneca.com> writes:

> If I have a class Foo, then i can write an S3 method for sum for it:
> 
> >setClass("Foo",representation(a="integer"));aFoo=new("Foo",a=c(1:3,NA))
> >sum.Foo <- function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
> >sum(aFoo)
> 
> But how do I write an S4 method for this? All my attempts to do so have
> foundered. For example
> >setMethod("sum",signature("Foo","logical"),

> function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
> creates a method which seems to despatch on na.rm="Foo":

There is no x argument in the generic so you can't dispatch on it.

> sum
function (..., na.rm = FALSE) 
.Internal(sum(..., na.rm = na.rm))



From Phguardiol at aol.com  Mon Apr 19 19:05:51 2004
From: Phguardiol at aol.com (Phguardiol@aol.com)
Date: Mon, 19 Apr 2004 13:05:51 EDT
Subject: [R] survival analysis question
Message-ID: <13.2ae39f4b.2db560ef@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/48cd2a71/attachment.pl

From pauljohn at ku.edu  Mon Apr 19 19:15:53 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Mon, 19 Apr 2004 12:15:53 -0500
Subject: [R] One inflated Poisson or Negative Binomal regression
In-Reply-To: <sf9f9292.024@MAIL.NDRI.ORG>
References: <sf9f9292.024@MAIL.NDRI.ORG>
Message-ID: <40840949.5010804@ku.edu>

Dear Peter:

I notice there is a R code for a Zero-inflated Poisson/NB process on the 
Stanford Political Science Computational Lab (Prof. Simon Jackman) web 
page.  If I were wanting to do  a one-inflated model, I would start with 
that because, at least to my eye, it is very easy to  follow.  Mind you, 
I did not try this myself, but I bet you could make it go.  In the file 
zeroinfl.r, look at the function:

 zeroinflNegBin <- function(parms){

it is pretty clear you'd have to supply a probability model for the 
outcomes valued 1 and then fit them into the overall likelihood.

pj


http://pscl.stanford.edu/content.html
Peter Flom wrote:

>Hello
>
>I am interested in Poisson or (ideally) Negative Binomial regression
>with an inflated number of 1  responses
>
>I have seen JK Lindsey's fmr function in the gnlm library, which fits
>zero inflated Poisson (ZIP) or zero inflated negative binomial
>regression, but the help file states that for ' Poisson or related
>distributions  the mixture involves the zero category'.
>
>I had thought of perhaps subtracting 1 from all the counts and then
>fitting the ZIP or ZINB models, and then adding 1, but am not sure if
>this is legitimate, or if there is some better method.
>
>Contextual details:
>The dependent variable is number of primary sexual partners in the last
>year.  The independent variables include a) Being married or in a
>committed relationship  b) using hard drugs  c) sex  d) age
>
>N is c. 500
>
>Not surprisingly, there are a large number of 1 responses, especially
>for those who are married or in a relationship.  More surprisingly, the
>mean number of partners is the same (1.05 vs. 1.02) for people in and
>not in relationships, but the variances are very different, mostly
>because those in a relationhsip are much more likely to say exactly 1.
>
>Thanks in advance
>
>Peter
>
>Peter L. Flom, PhD
>Assistant Director, Statistics and Data Analysis Core
>Center for Drug Use and HIV Research
>  
>

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504                              
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From p.dalgaard at biostat.ku.dk  Mon Apr 19 19:28:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Apr 2004 19:28:07 +0200
Subject: [R] Unexpected behaviour of identical
In-Reply-To: <FCA5F290CE7FFD42A6F1515497B0F0A204362194@ukapphresmsx02.ukapd.astrazeneca.net>
References: <FCA5F290CE7FFD42A6F1515497B0F0A204362194@ukapphresmsx02.ukapd.astrazeneca.net>
Message-ID: <x2brlnandk.fsf@biostat.ku.dk>

"Swinton, Jonathan" <Jonathan.Swinton at astrazeneca.com> writes:

>  # works as expected
> > ac <- c('A','B');
> > identical(ac,ac[1:2])
> [1] TRUE
>  
>  #but
> > af <- factor(ac)
> > identical(af,af[1:2])
> [1] FALSE
> 
> Any opinions?

Hmm, surprising indeed. The proximate cause would seem to be that

> names(attributes(af))
[1] "levels" "class"
> names(attributes(af[1:2]))
[1] "class"  "levels"

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ivan.borozan at utoronto.ca  Mon Apr 19 19:30:55 2004
From: ivan.borozan at utoronto.ca (ivan.borozan@utoronto.ca)
Date: Mon, 19 Apr 2004 13:30:55 -0400
Subject: [R] question about linear models.
Message-ID: <1082395855.40840ccf300a8@webmail.utoronto.ca>

hi there,

i have the following table with two factors A, B each respectively with 3 and 4
levels (unbalanced design)   

>S1
     samples A B
1  1.3398553 0 0
2  0.8455924 0 0
3  1.0290893 0 0
4  1.2720512 0 0
5  1.2071754 0 0
6  1.1859539 0 0
7  2.7399659 2 3
8  1.2476911 2 3
9  2.6389479 2 2
10 1.6914068 1 2
11 2.2260561 2 1
12 1.2955187 1 1
13 1.6526140 1 3
14 2.3159151 2 3
15 2.3905009 1 2
16 2.9520105 2 2
17 1.9478868 1 1
18 1.9936118 1 1
19 1.3775338 1 3
20 1.9638190 2 2
21 1.4697860 1 2
22 2.2028858 2 3
23 2.4024771 2 1
24 1.9935864 1 1


i fit two different models

fit1<-aov(samples~A + B,data=S1,contrasts = list(A = contr.treatment, B =
contr.treatment))
fit2<-aov(samples~A,data=S1,contrasts = list(A = contr.treatment))
fit3<-aov(samples~B,data=S1,contrasts = list(B = contr.treatment))


and using 

>anova(fit1,fit2)
Analysis of Variance Table

Model 1: samples ~ A + B
Model 2: samples ~ A
  Res.Df      RSS Df Sum of Sq      F Pr(>F)
1     19  2.74820                           
2     21  3.14667 -2  -0.39847 1.3774 0.2763

i get B as not significant and


>anova(fit1,fit3)

Analysis of Variance Table

Model 1: samples ~ A + B
Model 2: samples ~ B
  Res.Df     RSS Df Sum of Sq      F   Pr(>F)   
1     19  2.7482                                
2     20  4.2391 -1   -1.4909 10.308 0.004604 **

A as significant.



however if i do

>anova(fit3)

Analysis of Variance Table

Response: samples
          Df Sum Sq Mean Sq F value   Pr(>F)   
B          3 3.7241  1.2414  5.8567 0.004854 **
Residuals 20 4.2391  0.2120                    


i get B as significant and

>anova(fit2)

Analysis of Variance Table

Response: samples
          Df Sum Sq Mean Sq F value    Pr(>F)    
A          2 4.8165  2.4083  16.072 5.835e-05 ***
Residuals 21 3.1467  0.1498 

A as significant.




Should i conclude that A is significant and B is not or rather that both factors
are significant ?


all the best



From andy_liaw at merck.com  Mon Apr 19 19:33:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Apr 2004 13:33:39 -0400
Subject: [R] using subscripts in a plot title with 2 lines
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C24@usrymx25.merck.com>

If I understood your problem correctly, one ugly way of getting around this
is to use call mtext() twice to put the two lines in the title area, using
the line argument to mtext to control the location of the lines.  E.g.,
something like:

plot(1, main="")
mtext(expression(IC[50]), line=3, side=3, cex=1.2)
mtext("of My Data", line=1.5, side=3, cex=1.2)

HTH,
Andy

> From: Rajarshi Guha
> 
> Hi,
>   I'm making a plot in which the title takes up two lines. The title
> contains a subscript but when I look at the plot it does'nt 
> seem tocome
> out properly. The code I'm using is:
> 
>    xtxt = expression(paste('Observed -log( ', IC[50], ' )'))
>    ytxt = expression(paste('Predicted -log( ', IC[50], ' )'))
>    mtxt = expression(paste('Plot of Observed vs. Predicted -log(',
> IC[50], ') Values for the PDGFR\nDataset'))
> 
>    plot(tset$V2, tset$V3, xlim = c(-2,2), ylim = c(-2,2),
>     pch = 19, col = "blue", 
>     main=mtxt,
>     xlab = xtxt,
>     ylab = ytxt)
> 
> Removing the IC[50] term makes it come out OK. 
> Is there any way to get around this?
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Does Ramanujan know Polish?
> -- E.B. Ross
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From gregory_r_warnes at groton.pfizer.com  Mon Apr 19 19:47:51 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Mon, 19 Apr 2004 13:47:51 -0400
Subject: [R] [R-pkgs] New package: mcgibbsit, an MCMC run length diagnostic 
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C521278@groexmb02.pfizer.com>


Package: mcgibbsit
Title: Warnes and Raftery's MCGibbsit MCMC diagnostic 
Version: 1.0
Author: Gregory R. Warnes <gregory_r_warnes at groton.pfizer.com>
Description: 
  mcgibbsit provides an implementation of Warnes & Raftery's MCGibbsit
  run-length diagnostic for a set of (not-necessarily independent) MCMC
  sampers.  It combines the estimate error-bounding approach of Raftery
  and Lewis with evaulate between verses within chain approach
  of Gelman and Rubin.
Maintainer: Gregory R. Warnes <gregory_r_warnes at groton.pfizer.com>
License: GPL
Depends: coda

References:

Warnes GR. The Normal Kernel Coupler: An adaptive Markov Chain Monte Carlo
method for efficiently sampling from multi-modal distributions
<http://www.analytics.washington.edu/statcomp/projects/mcmc/nkc/>, Ph.D.
thesis, Department of Biostatistics, University of Washington,
<http://www.biostat.washington.edu/> October 2000.   (See Chapter 3, "Using
the Normal Kernel Coupler")


Gregory R. Warnes
Manager, Non-Clinical Statistics
Pfizer Global Research and Development
Tel: 860-715-3536



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From arrayprofile at yahoo.com  Mon Apr 19 19:53:37 2004
From: arrayprofile at yahoo.com (array chip)
Date: Mon, 19 Apr 2004 10:53:37 -0700 (PDT)
Subject: [R] axis label character bolder
Message-ID: <20040419175337.88074.qmail@web41210.mail.yahoo.com>

Hi, how can I make the character label of the axes of
a plot darker (bolder), but not in a larger size?



From ccleland at optonline.net  Mon Apr 19 20:00:40 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 19 Apr 2004 14:00:40 -0400
Subject: [R] axis label character bolder
In-Reply-To: <20040419175337.88074.qmail@web41210.mail.yahoo.com>
References: <20040419175337.88074.qmail@web41210.mail.yahoo.com>
Message-ID: <408413C8.8040208@optonline.net>

   If you simply want a bold font, have you tried setting 
font.axis and/or font.lab in par()?  For example,

par(font.axis=2, font.lab=2)
plot(rnorm(10))

   That gives me bold fonts for both components.

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status   Patched
major    1
minor    8.1
year     2003
month    12
day      04
language R

array chip wrote:
> Hi, how can I make the character label of the axes of
> a plot darker (bolder), but not in a larger size?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From kvanhorn at ksvanhorn.com  Mon Apr 19 20:12:47 2004
From: kvanhorn at ksvanhorn.com (Kevin S. Van Horn)
Date: Mon, 19 Apr 2004 12:12:47 -0600
Subject: [R] Size of R user base
Message-ID: <4084169F.8010401@ksvanhorn.com>

I have been trying to determine the size of the R user base, and was 
asked to share my findings with this mailing list.  Although I still 
don't have any definite estimate of this number, I do have some 
interesting and indicative information:


1. It appears that there are about 100,000 S-PLUS users.

Rationale: According to Insightful's 2002 Annual Report, over 100,000 
people use Insightful software; since license revenues from S-PLUS and 
add-on modules accounted for nearly all of their license revenues in 
2002, and their other products are much more costly than S-PLUS, it 
seems that the great majority of users of Insightful software are S-PLUS 
users.

Conclusion: S-PLUS costs $3500 (Windows) or $4500 (Linux/Unix) for an 
individual copy; R is free.  This suggests that there may be more R 
users than S-PLUS users, which suggests > 100,000 R users.

Does anyone has any other information that would give some notion as to 
the RELATIVE numbers of R and S-PLUS users?


2. At least one R book has achieved sales of just over 5,000 copies.  (I 
could not find sales figures for other R books, as it appears that 
publishers are closed-mouthed about such figures.  And no, I can't 
reveal which particular book this was, so don't ask.)

Conclusion:  Very few books sell to more than 12% of the population of 
potential buyers, and most books have a far lower penetration -- 1% or 
less is not uncommon.  A 12% penetration for the book in question 
implies 42,000 R users; a more reasonable 5% penetration implies 100,000 
users.  A low 1% penetration implies 500,000 users.


3. There are a total of 3225 unique subscribers to the three R mailing 
lists.



From ivo.welch at yale.edu  Mon Apr 19 20:16:26 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Mon, 19 Apr 2004 14:16:26 -0400
Subject: [R] fill between lines
Message-ID: <4084177A.8060308@yale.edu>


hi:  is it possible to color areas between two functions?  for example,

    x<- 1:100;
    plot(x, x^2, type="l");
    lines(x,0.5*x^2, type="l");
    # better
    plotwithfill(x, x^2, 0.5*x^2, color=c("yellow", "red");

where the first color is used if f(x)=x^2 > g(x)=0.5*x^2, and the second 
for the reverse.  Help appreciated.

Regards, / ivo



From andy_liaw at merck.com  Mon Apr 19 20:35:49 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Apr 2004 14:35:49 -0400
Subject: [R] fill between lines
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C29@usrymx25.merck.com>

Nothing easily that I can think of.  You need to plot each `piece'
separately using polygon().  I.e.,

1. Find the `pieces'.
2. Determine the color the piece should be in.
3. Draw the piece with polygon().

Step 1 above is probably the trickiest part, but quite doable, I think.

Best,
Andy

> From: ivo welch
> 
> hi:  is it possible to color areas between two functions?  
> for example,
> 
>     x<- 1:100;
>     plot(x, x^2, type="l");
>     lines(x,0.5*x^2, type="l");
>     # better
>     plotwithfill(x, x^2, 0.5*x^2, color=c("yellow", "red");
> 
> where the first color is used if f(x)=x^2 > g(x)=0.5*x^2, and 
> the second 
> for the reverse.  Help appreciated.
> 
> Regards, / ivo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From rpeng at jhsph.edu  Mon Apr 19 20:40:20 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 19 Apr 2004 14:40:20 -0400
Subject: [R] fill between lines
In-Reply-To: <4084177A.8060308@yale.edu>
References: <4084177A.8060308@yale.edu>
Message-ID: <40841D14.7090006@jhsph.edu>

If one function dominates the other, you can use polygon(),

x <- 1:100
p <- rbind(cbind(x, 0.5*x^2), cbind(rev(x), rev(x^2)))
matplot(x, cbind(x^2, 0.5*x^2), type = "l", col = 1, lty = 1)
polygon(p, col = 2)

or some variant of that.

-roger

ivo welch wrote:
> 
> hi:  is it possible to color areas between two functions?  for example,
> 
>    x<- 1:100;
>    plot(x, x^2, type="l");
>    lines(x,0.5*x^2, type="l");
>    # better
>    plotwithfill(x, x^2, 0.5*x^2, color=c("yellow", "red");
> 
> where the first color is used if f(x)=x^2 > g(x)=0.5*x^2, and the second 
> for the reverse.  Help appreciated.
> 
> Regards, / ivo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ivo.welch at yale.edu  Mon Apr 19 21:04:22 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Mon, 19 Apr 2004 15:04:22 -0400
Subject: [R] thanks for fill
Message-ID: <408422B6.30805@yale.edu>


Hi:  thanks for the polygon recommendations.  these work like a charm 
for me.

minor documentation bug:  ?plot.default states lwd is not yet supported 
for postscript, but it does seem to work.

regards,

/iaw



From bill.shipley at usherbrooke.ca  Mon Apr 19 22:19:34 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 19 Apr 2004 16:19:34 -0400
Subject: [R] error message in mle function
Message-ID: <003b01c4264b$a1a93480$801ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/29fbd534/attachment.pl

From n.bouget at laposte.net  Mon Apr 19 22:24:27 2004
From: n.bouget at laposte.net (n.bouget)
Date: Mon, 19 Apr 2004 22:24:27 +0200
Subject: [R] Informations about merge function
Message-ID: <HWFQ0R$4FAB86663498FC609FA5444F61DBDF95@laposte.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/54aeed43/attachment.pl

From frohde_home at yahoo.com  Mon Apr 19 22:28:53 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Mon, 19 Apr 2004 13:28:53 -0700 (PDT)
Subject: [R] specifying as.svrepdesign with odd number PSUs
Message-ID: <20040419202853.62219.qmail@web60103.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/0f5f9297/attachment.pl

From frohde_home at yahoo.com  Mon Apr 19 22:29:05 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Mon, 19 Apr 2004 13:29:05 -0700 (PDT)
Subject: [R] specifying as.svrepdesign with odd number PSUs
Message-ID: <20040419202905.2928.qmail@web60104.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040419/a5347c06/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Mon Apr 19 22:22:51 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Apr 2004 21:22:51 +0100 (BST)
Subject: [R] Size of R user base
In-Reply-To: <4084169F.8010401@ksvanhorn.com>
Message-ID: <XFMail.040419212251.Ted.Harding@nessie.mcc.ac.uk>

A very intriguing commentary!
Some comments to "modulate" these estimates.

On 19-Apr-04 Kevin S. Van Horn wrote:
> 1. It appears that there are about 100,000 S-PLUS users.
> 
> Rationale: According to Insightful's 2002 Annual Report, over 100,000 
> people use Insightful software; since license revenues from S-PLUS and 
> add-on modules accounted for nearly all of their license revenues in 
> 2002, and their other products are much more costly than S-PLUS, it 
> seems that the great majority of users of Insightful software are
> S-PLUS 
> users.
> 
> Conclusion: S-PLUS costs $3500 (Windows) or $4500 (Linux/Unix) for an 
> individual copy; R is free.  This suggests that there may be more R 
> users than S-PLUS users, which suggests > 100,000 R users.
> 
> Does anyone has any other information that would give some notion as to
> the RELATIVE numbers of R and S-PLUS users?

There is one major factor in here. The number of Windows users
in the world is much higher than the number of Unix/Linux users,
especially in the corporate sector. Organisations whose work
needs R/S-PLUS and whose IT is Windows based will (I believe)
mostly go for S-PLUS (I could expand in my reasons for believing
this). Therefore I suspect that in the 2-way table

          Windows  Unix/Linux
S-PLUS      N11       N12

R           N21       N22

you are likely to find that N11/N21 >> N12/N22.
Certainly N11+N21 > N12+N22. This tends to imply N11+N12 > N12+N22.
The relative cost of S-PLUS vs R is not likely to be a factor in
the choice, for most corporate users. Therefore I would lower your
estimate, here, of R usage quite a bit (though I can't guess by
how much).

> 2. At least one R book has achieved sales of just over 5,000 copies. 
> (I could not find sales figures for other R books, as it appears that 
> publishers are closed-mouthed about such figures.  And no, I can't 
> reveal which particular book this was, so don't ask.)
> 
> Conclusion:  Very few books sell to more than 12% of the population of 
> potential buyers, and most books have a far lower penetration -- 1% or 
> less is not uncommon.  A 12% penetration for the book in question 
> implies 42,000 R users; a more reasonable 5% penetration implies
> 100,000 
> users.  A low 1% penetration implies 500,000 users.

Comment: More R users are likely to buy a book on R than S-PLUS
users are likely to buy a book on S-PLUS. S-PLUS users who do
buy a book may in fact buy a book on R rather than S-PLUS, if
that book is well known to be good. (I'm assuming that the
"R book" you refer to is R-specific rather than written for
both R and S-PLUS or for "S-PLUS with R variations"; otherwise
you have to take off the S-PLUS-only purchasers)

> 3. There are a total of 3225 unique subscribers to the three R mailing 
> lists.

I think this may be the most directly informative piece of data
(though still on the soft side). People who use R are likely to
become aware of the mailing lists, and to subscribe. So I suspect
that this number exceeds say 20-40% of R users (you can't be precise
with this sort of intuitive guess). This would suggest 7000-16000 R
users.
You might perhaps double or triple this to allow for groups where
one member of the group subscribes as the "spokesman" for the rest.
Maybe also inflate a bit to allow for R users who don't think
they need to consult mailing lists (who are they??).

Hmmm!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 19-Apr-04                                       Time: 21:22:51
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Apr 19 21:28:31 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Apr 2004 20:28:31 +0100 (BST)
Subject: [R] question about linear models.
In-Reply-To: <1082395855.40840ccf300a8@webmail.utoronto.ca>
Message-ID: <XFMail.040419202831.Ted.Harding@nessie.mcc.ac.uk>

This would make a good exam question!

First, look at the distribution of levels:

      B=0   B=1   B=2   B=3
A=0    6    --    --    --
A=1   --     4     3     2
A=2   --     2     3     4


And then look at the mean values within combinations of levels:

      B=0   B=1   B=2   B=3
A=0  1.15   --    --    --  | 1.15
A=1   --   1.81  1.85  1.52 | 1.76
A=2   --   2.31  2.52  2.13 | 2.30
----------------------------+------
     1.15  1.98  2.18  1.93 | 1.81
(Residual SE after fitting A+B = 0.38)

First, it is clear that (A=0) vs (A>0) is exactly associated
with (B=0) vs (B>0). Therefore any difference between means
for (A=0) vs (A>0) is fully confounded with (B=0) vs (B>0).
Clearly (from table of means) there *is* a difference here
(significant as it turns out), so fitting A alone will give
a significant result as will fitting B alone.

Further (table of means) the response increases almost linearly
with A (about 0.6/level), while it does not change much for
(B=1/2/3). So almost all if the variation with respect to B
is accounted for by the difference between (B=0) and (B>0)
which is totally confounded with A. Therefore, once you have
fitted A, fitting B as an additional variate will not change
the fit significantly.

However, if you fit B first followed by adding A, you first
(B fit) take out the difference between (B=0) vs (B>0),
equivalent to (A=0) vs (A>0). However, from inspection of
table of means, while there is little differfence between
(B=1)/(B=2)/(B=3) nevertheless there is a systematic difference
at each level of B between (A=1) and (A=2) -- 0.5, 0.67
and 0.61 respectively. This shows up as an effect of A after
fitting B.

So, in summary, there is a significant effect of A alone (due
to the constant increase per increment in level); there is a
significant effect of B alone (due to the contrast between
(B=0) and (B>0) equivalent to the contrast between (A=0)
and (A>0)); however, once the effect of A has been allowed
for you only have the contrast between levels (B=1)/(B=2)/(B=3)
of B which do not differ enough to be significant. On the other
hand, fitting B first still leaves a constant effect of A
at each of the levels of B which shows up as significant for
A after fitting B. You do not have enough data to detect as
significant the sort of differences between levels of B=1/2/3.

Best wishes,
Ted.

==================================================================

On 19-Apr-04 ivan.borozan at utoronto.ca wrote:
> i have the following table with two factors A, B each respectively
> with 3 and 4 levels (unbalanced design)   
> 
>>S1
>      samples A B
> 1  1.3398553 0 0
> 2  0.8455924 0 0
> 3  1.0290893 0 0
> 4  1.2720512 0 0
> 5  1.2071754 0 0
> 6  1.1859539 0 0
> 7  2.7399659 2 3
> 8  1.2476911 2 3
> 9  2.6389479 2 2
> 10 1.6914068 1 2
> 11 2.2260561 2 1
> 12 1.2955187 1 1
> 13 1.6526140 1 3
> 14 2.3159151 2 3
> 15 2.3905009 1 2
> 16 2.9520105 2 2
> 17 1.9478868 1 1
> 18 1.9936118 1 1
> 19 1.3775338 1 3
> 20 1.9638190 2 2
> 21 1.4697860 1 2
> 22 2.2028858 2 3
> 23 2.4024771 2 1
> 24 1.9935864 1 1
> 
> 
> i fit two different models
> 
> fit1<-aov(samples~A + B,data=S1,contrasts = list(A = contr.treatment, B
> =
> contr.treatment))
> fit2<-aov(samples~A,data=S1,contrasts = list(A = contr.treatment))
> fit3<-aov(samples~B,data=S1,contrasts = list(B = contr.treatment))
> 
> 
> and using 
> 
>>anova(fit1,fit2)
> Analysis of Variance Table
> 
> Model 1: samples ~ A + B
> Model 2: samples ~ A
>   Res.Df      RSS Df Sum of Sq      F Pr(>F)
> 1     19  2.74820                           
> 2     21  3.14667 -2  -0.39847 1.3774 0.2763
> 
> i get B as not significant and
> 
> 
>>anova(fit1,fit3)
> 
> Analysis of Variance Table
> 
> Model 1: samples ~ A + B
> Model 2: samples ~ B
>   Res.Df     RSS Df Sum of Sq      F   Pr(>F)   
> 1     19  2.7482                                
> 2     20  4.2391 -1   -1.4909 10.308 0.004604 **
> 
> A as significant.
> 
> 
> 
> however if i do
> 
>>anova(fit3)
> 
> Analysis of Variance Table
> 
> Response: samples
>           Df Sum Sq Mean Sq F value   Pr(>F)   
> B          3 3.7241  1.2414  5.8567 0.004854 **
> Residuals 20 4.2391  0.2120                    
> 
> 
> i get B as significant and
> 
>>anova(fit2)
> 
> Analysis of Variance Table
> 
> Response: samples
>           Df Sum Sq Mean Sq F value    Pr(>F)    
> A          2 4.8165  2.4083  16.072 5.835e-05 ***
> Residuals 21 3.1467  0.1498 
> 
> A as significant.
> 
> 
> 
> 
> Should i conclude that A is significant and B is not or rather that
> both factors
> are significant ?
> 
> 
> all the best
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 19-Apr-04                                       Time: 20:28:31
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Mon Apr 19 22:40:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Apr 2004 16:40:25 -0400
Subject: [R] Size of R user base
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C2C@usrymx25.merck.com>

> From: Ted.Harding at nessie.mcc.ac.uk
> 
> A very intriguing commentary!
> Some comments to "modulate" these estimates.
> 
> On 19-Apr-04 Kevin S. Van Horn wrote:
> > 1. It appears that there are about 100,000 S-PLUS users.
> > 
> > Rationale: According to Insightful's 2002 Annual Report, 
> over 100,000 
> > people use Insightful software; since license revenues from 
> S-PLUS and 
> > add-on modules accounted for nearly all of their license 
> revenues in 
> > 2002, and their other products are much more costly than S-PLUS, it 
> > seems that the great majority of users of Insightful software are
> > S-PLUS 
> > users.
> > 
> > Conclusion: S-PLUS costs $3500 (Windows) or $4500 
> (Linux/Unix) for an 
> > individual copy; R is free.  This suggests that there may be more R 
> > users than S-PLUS users, which suggests > 100,000 R users.
> > 
> > Does anyone has any other information that would give some 
> notion as to
> > the RELATIVE numbers of R and S-PLUS users?
> 
> There is one major factor in here. The number of Windows users
> in the world is much higher than the number of Unix/Linux users,
> especially in the corporate sector. Organisations whose work
> needs R/S-PLUS and whose IT is Windows based will (I believe)
> mostly go for S-PLUS (I could expand in my reasons for believing
> this). Therefore I suspect that in the 2-way table
> 
>           Windows  Unix/Linux
> S-PLUS      N11       N12
> 
> R           N21       N22
> 
> you are likely to find that N11/N21 >> N12/N22.
> Certainly N11+N21 > N12+N22. This tends to imply N11+N12 > N12+N22.
> The relative cost of S-PLUS vs R is not likely to be a factor in
> the choice, for most corporate users. Therefore I would lower your
> estimate, here, of R usage quite a bit (though I can't guess by
> how much).
> 
> > 2. At least one R book has achieved sales of just over 
> 5,000 copies. 
> > (I could not find sales figures for other R books, as it 
> appears that 
> > publishers are closed-mouthed about such figures.  And no, I can't 
> > reveal which particular book this was, so don't ask.)
> > 
> > Conclusion:  Very few books sell to more than 12% of the 
> population of 
> > potential buyers, and most books have a far lower 
> penetration -- 1% or 
> > less is not uncommon.  A 12% penetration for the book in question 
> > implies 42,000 R users; a more reasonable 5% penetration implies
> > 100,000 
> > users.  A low 1% penetration implies 500,000 users.
> 
> Comment: More R users are likely to buy a book on R than S-PLUS
> users are likely to buy a book on S-PLUS. S-PLUS users who do
> buy a book may in fact buy a book on R rather than S-PLUS, if
> that book is well known to be good. (I'm assuming that the
> "R book" you refer to is R-specific rather than written for
> both R and S-PLUS or for "S-PLUS with R variations"; otherwise
> you have to take off the S-PLUS-only purchasers)
> 
> > 3. There are a total of 3225 unique subscribers to the 
> three R mailing 
> > lists.
> 
> I think this may be the most directly informative piece of data
> (though still on the soft side). People who use R are likely to
> become aware of the mailing lists, and to subscribe. So I suspect
> that this number exceeds say 20-40% of R users (you can't be precise
> with this sort of intuitive guess). This would suggest 7000-16000 R
> users.
> You might perhaps double or triple this to allow for groups where
> one member of the group subscribes as the "spokesman" for the rest.
> Maybe also inflate a bit to allow for R users who don't think
> they need to consult mailing lists (who are they??).

How about those poor students who don't know how lucky they are to have
instructors forcing R upon them for a course?  I'd bet they are very
unlikely to subscribe to the list(s).  Although I don't know if one would
want to include them as `R users'...

Best,
Andy

 
> Hmmm!
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 19-Apr-04                                       Time: 21:22:51
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kvanhorn at ksvanhorn.com  Mon Apr 19 22:50:00 2004
From: kvanhorn at ksvanhorn.com (Kevin S. Van Horn)
Date: Mon, 19 Apr 2004 14:50:00 -0600
Subject: [R] Size of R user base
In-Reply-To: <XFMail.040419212251.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040419212251.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <40843B78.2000804@ksvanhorn.com>

(Ted Harding) wrote:

>>1. It appears that there are about 100,000 S-PLUS users.
>>    
>>
[...]

>>Does anyone has any other information that would give some notion as to
>>the RELATIVE numbers of R and S-PLUS users?
>>    
>>
>
>There is one major factor in here. The number of Windows users
>in the world is much higher than the number of Unix/Linux users,
>especially in the corporate sector. Organisations whose work
>needs R/S-PLUS and whose IT is Windows based will (I believe)
>mostly go for S-PLUS (I could expand in my reasons for believing
>this).
>

But R is available for Windows, too.  I've downloaded and installed both 
the Linux and Windows versions; neither task was difficult, and the 
Windows version had a rather nicer interface.



From macq at llnl.gov  Mon Apr 19 22:48:49 2004
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 19 Apr 2004 13:48:49 -0700
Subject: [R] fill between lines
In-Reply-To: <4084177A.8060308@yale.edu>
References: <4084177A.8060308@yale.edu>
Message-ID: <p0600200cbca9eaa22866@[128.115.153.6]>

It could be approximated with line segments, using the segments function.

Something like two calls similar to
    segments(x,x^2, x, 0.5*x^2)

one call where f > g, the other call f < g, using a different value 
for the 'col' argument

Make x very dense, say seq(1,100,len=1000) instead of 1:100, and it 
will look like it's filled.

-Don

At 2:16 PM -0400 4/19/04, ivo welch wrote:
>hi:  is it possible to color areas between two functions?  for example,
>
>    x<- 1:100;
>    plot(x, x^2, type="l");
>    lines(x,0.5*x^2, type="l");
>    # better
>    plotwithfill(x, x^2, 0.5*x^2, color=c("yellow", "red");
>
>where the first color is used if f(x)=x^2 > g(x)=0.5*x^2, and the 
>second for the reverse.  Help appreciated.
>
>Regards, / ivo
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ripley at stats.ox.ac.uk  Mon Apr 19 23:27:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 19 Apr 2004 22:27:25 +0100 (BST)
Subject: [R] Size of R user base
In-Reply-To: <4084169F.8010401@ksvanhorn.com>
Message-ID: <Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>

On Mon, 19 Apr 2004, Kevin S. Van Horn wrote:

> 2. At least one R book has achieved sales of just over 5,000 copies.  (I 
> could not find sales figures for other R books, as it appears that 
> publishers are closed-mouthed about such figures.  And no, I can't 
> reveal which particular book this was, so don't ask.)

Some of us know quite accurately, though.

> Conclusion:  Very few books sell to more than 12% of the population of 
> potential buyers, and most books have a far lower penetration -- 1% or 

Where did you get that 12% from?

> less is not uncommon.  A 12% penetration for the book in question 
> implies 42,000 R users; a more reasonable 5% penetration implies 100,000 
> users.  A low 1% penetration implies 500,000 users.

One S book has sold half your number of S-PLUS users, although some sales
are known to be to R users.

I have big problems with the definition.  What is an `R user'?  Someone 
who has ever used R, even for a one-hour practical class?  Someone who has 
used R in the last 3 months?  Even given a definition, I would not be able 
to give you an accurate answer for our site, for either S-PLUS or R.  
(There are machines with each installed that I strongly suspect are 
unused.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From edd at debian.org  Mon Apr 19 23:47:30 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 19 Apr 2004 16:47:30 -0500
Subject: [R] Size of R user base
In-Reply-To: <Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
References: <4084169F.8010401@ksvanhorn.com>
	<Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
Message-ID: <20040419214729.GA26717@sonny.eddelbuettel.com>

On Mon, Apr 19, 2004 at 10:27:25PM +0100, Prof Brian Ripley wrote:
> > less is not uncommon.  A 12% penetration for the book in question 
> > implies 42,000 R users; a more reasonable 5% penetration implies 100,000 
> > users.  A low 1% penetration implies 500,000 users.
> 
> One S book has sold half your number of S-PLUS users, although some sales
> are known to be to R users.

But then you also need to control for different editions and serial buyers.
I happen to have purchased three different editions of a certain S-Plus / R
book now in its 4th edition. 

My preference goes with the numbering scheme attributed to a tribe on some
island in the Pacific which consists of a 'factor' with four levels: 'one',
'two', 'three', and 'lots'.  Hence, I'd go with 'lots of R users'.

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From p.connolly at hortresearch.co.nz  Mon Apr 19 23:55:23 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 20 Apr 2004 09:55:23 +1200
Subject: [R] Size of R user base
In-Reply-To: <20040419214729.GA26717@sonny.eddelbuettel.com>;
	from edd@debian.org on Mon, Apr 19, 2004 at 04:47:30PM -0500
References: <4084169F.8010401@ksvanhorn.com>
	<Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
	<20040419214729.GA26717@sonny.eddelbuettel.com>
Message-ID: <20040420095523.D2137@hortresearch.co.nz>

On Mon, 19-Apr-2004 at 04:47PM -0500, Dirk Eddelbuettel wrote:

|> On Mon, Apr 19, 2004 at 10:27:25PM +0100, Prof Brian Ripley wrote:
|> > > less is not uncommon.  A 12% penetration for the book in question 
|> > > implies 42,000 R users; a more reasonable 5% penetration implies 100,000 
|> > > users.  A low 1% penetration implies 500,000 users.
|> > 
|> > One S book has sold half your number of S-PLUS users, although some sales
|> > are known to be to R users.
|> 
|> But then you also need to control for different editions and serial buyers.
|> I happen to have purchased three different editions of a certain S-Plus / R
|> book now in its 4th edition. 
|> 
|> My preference goes with the numbering scheme attributed to a tribe on some
|> island in the Pacific which consists of a 'factor' with four levels: 'one',
|> 'two', 'three', and 'lots'.  Hence, I'd go with 'lots of R users'.

So how will you distinguish your 4th edition of a certain S-Plus / R
book from what comes next?

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From rossini at blindglobe.net  Tue Apr 20 00:06:18 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 19 Apr 2004 15:06:18 -0700
Subject: [R] Size of R user base
In-Reply-To: <20040420095523.D2137@hortresearch.co.nz> (Patrick Connolly's
	message of "Tue, 20 Apr 2004 09:55:23 +1200")
References: <4084169F.8010401@ksvanhorn.com>
	<Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
	<20040419214729.GA26717@sonny.eddelbuettel.com>
	<20040420095523.D2137@hortresearch.co.nz>
Message-ID: <851xmjwrl1.fsf@servant.blindglobe.net>

Patrick Connolly <p.connolly at hortresearch.co.nz> writes:

> On Mon, 19-Apr-2004 at 04:47PM -0500, Dirk Eddelbuettel wrote:
>
> |> On Mon, Apr 19, 2004 at 10:27:25PM +0100, Prof Brian Ripley wrote:
> |> > > less is not uncommon.  A 12% penetration for the book in question 
> |> > > implies 42,000 R users; a more reasonable 5% penetration implies 100,000 
> |> > > users.  A low 1% penetration implies 500,000 users.
> |> > 
> |> > One S book has sold half your number of S-PLUS users, although some sales
> |> > are known to be to R users.
> |> 
> |> But then you also need to control for different editions and serial buyers.
> |> I happen to have purchased three different editions of a certain S-Plus / R
> |> book now in its 4th edition. 
> |> 
> |> My preference goes with the numbering scheme attributed to a tribe on some
> |> island in the Pacific which consists of a 'factor' with four levels: 'one',
> |> 'two', 'three', and 'lots'.  Hence, I'd go with 'lots of R users'.
>
> So how will you distinguish your 4th edition of a certain S-Plus / R
> book from what comes next?

It's all "lots", and I hope it stays (i.e. continues) that way.  

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Mon Apr 19 23:47:59 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 19 Apr 2004 22:47:59 +0100 (BST)
Subject: [R] Size of R user base
In-Reply-To: <40843B78.2000804@ksvanhorn.com>
Message-ID: <XFMail.040419224759.Ted.Harding@nessie.mcc.ac.uk>

On 19-Apr-04 Kevin S. Van Horn wrote:
> (Ted Harding) wrote:
>>There is one major factor in here. The number of Windows users
>>in the world is much higher than the number of Unix/Linux users,
>>especially in the corporate sector. Organisations whose work
>>needs R/S-PLUS and whose IT is Windows based will (I believe)
>>mostly go for S-PLUS (I could expand in my reasons for believing
>>this).
>>
> 
> But R is available for Windows, too.  I've downloaded and installed
> both the Linux and Windows versions; neither task was difficult, and
> the Windows version had a rather nicer interface.

Thanks. I know that there is R for Windows, and I'm not disputing
your comparison between R for Linux and R for Windows, but my argument
was directed at the choices people would make between S-PLUS
for Windows and R for Windows, coupled with the fact that Windows
is much more prevalent than Unix/Linux.

Though they have the choice, I argue that many (especially corporate
but by no means only these) Windows-based organisations would go for
S-PLUS rather than R (for all sorts of reasons, ranging from
"To install S-PLUS just plug in the CD and click" to the manuals for
S-PLUS and add-ons which get the user, albeit potentially brainlessly,
from installation to data-analysis much more readily than the R
documentation which does demand considerable study, thought, and
development of understanding).

Not to mention that (in theory) S-PLUS as a commercial product could
be presumed to come with guarantees and support and, quite possibly
erroneously, expected to be a better-tested, more reliable, quality
product ... it's what they pay those $$000s for, isn't it?

So the gross preponderance of Windows, and the motives of many
Windows-based organisations, will (I argue) lead to a preponderance
of S-PLUS over R.

This preponderance would, I suspect, be less marked (could just
possibly swing the other way) for Unix users. But the net effect
overall would, I believe, be that S-PLUS would outnumber R.

Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 19-Apr-04                                       Time: 22:47:59
------------------------------ XFMail ------------------------------



From jasont at indigoindustrial.co.nz  Tue Apr 20 00:14:15 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 20 Apr 2004 10:14:15 +1200
Subject: [R] classification and association rules in R
In-Reply-To: <20040419161225.GA68519@muse.csie.ntu.edu.tw>
References: <20040419161225.GA68519@muse.csie.ntu.edu.tw>
Message-ID: <40844F37.9030205@indigoindustrial.co.nz>

Rong-En Fan wrote:

> By the way, I heard that there are some people developing a better
> search interface for R (or CRAN?). Where are the related information
> I can get?

Strangely enough, by following the "Search" link on CRAN.

Jason



From kvanhorn at ksvanhorn.com  Tue Apr 20 00:20:14 2004
From: kvanhorn at ksvanhorn.com (Kevin S. Van Horn)
Date: Mon, 19 Apr 2004 16:20:14 -0600
Subject: [R] Size of R user base
In-Reply-To: <Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
References: <Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
Message-ID: <4084509E.5060702@ksvanhorn.com>

Prof Brian Ripley wrote:

>>Conclusion:  Very few books sell to more than 12% of the population of 
>>potential buyers, and most books have a far lower penetration -- 1% or 
>>    
>>
>
>Where did you get that 12% from?
>

A booklet on assessing financial feasibility in nonfiction book 
publishing.  That's a general figure, so perhaps it doesn't apply if the 
book in question is a must-have, definitive reference for the group in 
question... like the book you mention (if it's the one I think it is).

> I have big problems with the definition. What is an `R user'? Someone  
> who has ever used R, even for a one-hour practical class? Someone who 
> has used R in the last 3 months?


Good question.  I guess I'd lean more towards your second definition, 
with the added caveat of "and expects to use it again in the next 3 months".



From jasont at indigoindustrial.co.nz  Tue Apr 20 00:19:06 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 20 Apr 2004 10:19:06 +1200
Subject: [R] Size of R user base
In-Reply-To: <20040419214729.GA26717@sonny.eddelbuettel.com>
References: <4084169F.8010401@ksvanhorn.com>	<Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
	<20040419214729.GA26717@sonny.eddelbuettel.com>
Message-ID: <4084505A.3030209@indigoindustrial.co.nz>

Dirk Eddelbuettel wrote:
> My preference goes with the numbering scheme attributed to a tribe on some
> island in the Pacific which consists of a 'factor' with four levels: 'one',
> 'two', 'three', and 'lots'.

Australia.  I've been there.  Nice place.  ;)

Jason
(who is an Australian)



From spencer.graves at pdf.com  Tue Apr 20 00:36:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 19 Apr 2004 15:36:54 -0700
Subject: [R] error message in mle function
In-Reply-To: <003b01c4264b$a1a93480$801ad284@BIO041>
References: <003b01c4264b$a1a93480$801ad284@BIO041>
Message-ID: <40845486.7080109@pdf.com>

      Is one of your variance components essentially estimating zero?  
If you check the numbers, delete the smallest one, and then do "anova" 
comparing the two fits, you might find that the one you deleted was not 
statistically significant. 

      If I'm not mistaken, "lme" estimates the logarithms of the 
variance components.  When one of them is zero, the logarithm wants to 
go to (-Inf).  In that case, "lme" will still return an answer.  
However, "intervals" complains, because the observed information matrix 
for the parameter estimates is singular in the direction of log(variance 
component) that wants to go to (-Inf).  Doug Bates, the developer of 
"lme" has taught many people about how to fix that problem using profile 
likelihood.  I have not seen "lme4" yet, so the problem may already have 
been fixed.  If any if this is inaccurate, I hope Doug will correct me. 

      hope this helps. 
      Spencer Graves

Bill Shipley wrote:

>I am getting an error message concerning the estimation of confidence
>intervals when fitting a mixed model and don't know what the problem is,
>or its solution.
>
> 
>
>Just to provide context: the model is describing the effects of age,
>exp(age), harvest age, and climate variables on bighorn horn annular
>length.
>
> 
>
>The data structure is repeated measures (between individuals, within
>individuals over time).
>
> 
>
>Id is a random effect (there are between 3-11 horn measurements per ram,
>one horn measurement per age, over the 25 year period in the dataset).
>
> 
>
>The mixed effect results is unable to provide confidence intervals for
>the fixed and random effects because: of an error in the
>variance-covariance structure. The error says that the intervals are
>non-positive definitive. 
>
> 
>
> 
>
>Bill Shipley
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From tlumley at u.washington.edu  Tue Apr 20 02:13:59 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 19 Apr 2004 17:13:59 -0700 (PDT)
Subject: [R] specifying as.svrepdesign with odd number PSUs
In-Reply-To: <20040419202905.2928.qmail@web60104.mail.yahoo.com>
References: <20040419202905.2928.qmail@web60104.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0404191658421.55948@homer38.u.washington.edu>

On Mon, 19 Apr 2004, Fred Rohde wrote:

> Is there a way to create a BRR svrepdesign from a survey design when the
> number of PSUs is odd in one or more stratum?  Creating a JKn
> svrepdesign with that condition works okay, but when I tried to create a
> svrepdesign with type="BRR" I get an error and this message:
>
>      "Can't split with odd numbers of PSUs in a stratum"

Oh dear, how very frustrating, (or words to that effect).
The parameters aren't being passed on to brrweights().

You aren't supposed to be able to split a stratum with an odd number of
clusters, because the result won't be balanced.    You are supposed to
be able to merge two strata with an odd number of clusters, but it isn't
working.  Note that it should only work when the number of odd strata is
even.  BRR is, fundamentally, intended for strata of size 2 and I think
there's something slightly dodgy about attempts to get around this.

	-thomas



From jfox at mcmaster.ca  Tue Apr 20 02:59:35 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 19 Apr 2004 20:59:35 -0400
Subject: [R] Menu addditions to Rcmdr v0.9-6
In-Reply-To: <12D0D00E1404D511A4820090274CA09C03FBA630@dasmtyjqf010.amedd.army.mil>
Message-ID: <20040420005932.JXYJ7116.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Charles, 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of White, 
> Charles E WRAIR-Wash DC
> Sent: Monday, April 19, 2004 10:16 AM
> To: R Help (E-mail)
> Subject: [R] Menu addditions to Rcmdr v0.9-6
> 
> 1)     In general, I would appreciate help in adding 
> functions to the Rcmdr
> menu system. 

I realize that the documentation for adding capabilities to the Rcmdr is
woefully inadequate. When I have time this summer, and before I bump the
version number to 1.0-0, I plan to provide better documentation. 

> I've been able to modify the menus themselves 
> and source test code but I can't get R functions to execute 
> from the menu. My latest proof of concept code follows:
>  
> Three lines added to "Rcmdr-menus.txt":
>     menu    junkMenu        topMenu         ""                ""
>     item    junkMenu        command         "Print stuff..."  
> "function()
> stuff ()"
>     item    topMenu         cascade         "Junk"            junkMenu
> 
>  
> "stuff.R":
> stuff <-function(){
> c("This is stuff to print...")
> }
>  

For this kind of function call, you have to add an explicit print():

stuff <-function(){
  print(c("This is stuff to print..."))
  }

> 2)     A file called "compareModels.demo" is mentioned in the 
> documentation
> but I haven't been able to find it in the Windows library 
> downloaded from CRAN. Where should I look?
>  

Unfortunately, as also happened with a previous example, I became too
enamoured of the compare-models dialog, which is now called by the "Models
-> Hypothesis tests -> Compare models" menu. I'll try to come up with a
sufficiently uninteresting example so that I don't do this again. You can
still examine the compareModels function, however, by looking at the R code
for the package.

I'm sorry that you encountered this problem.

John

> Thanks!
>  
> Chuck
>  
> Charles E. White, Senior Biostatistician, MS Walter Reed Army 
> Institute of Research
> 503 Robert Grant Ave., Room 1w102
> Silver Spring, MD 20910-1557
> 301 319-9781
> Personal/Professional Site:
> http://users.starpower.net/cwhite571/professional/ 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From sdhyok at email.unc.edu  Tue Apr 20 04:05:57 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 19 Apr 2004 22:05:57 -0400
Subject: [R] Can't find memory.size()
Message-ID: <OAEOKPIGCLDDHAEMCAKIGEAHCLAA.sdhyok@email.unc.edu>

I try memory.size function to find out available memory size,
but surprisingly R complains it can't find the function.
?memory.size also failed.

Is it not in the base library? If so, why can't my R find it?
I am using the binary 1.9.0 version for Mandrake 9.1.
Thanks in advance.

Daehyok Shin
Terrestrial Hydrological Ecosystem Modellers
Geography Department
University of North Carolina-Chapel Hill
sdhyok at email.unc.edu

"We can do no great things, 
only small things with great love."
                         - Mother Teresa



From p.murrell at auckland.ac.nz  Tue Apr 20 04:24:03 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 20 Apr 2004 14:24:03 +1200
Subject: [R] using subscripts in a plot title with 2 lines
References: <3A822319EB35174CA3714066D590DCD504AF7C24@usrymx25.merck.com>
Message-ID: <408489C3.9060003@stat.auckland.ac.nz>

Hi


Liaw, Andy wrote:
> If I understood your problem correctly, one ugly way of getting around this
> is to use call mtext() twice to put the two lines in the title area, using
> the line argument to mtext to control the location of the lines.  E.g.,
> something like:
> 
> plot(1, main="")
> mtext(expression(IC[50]), line=3, side=3, cex=1.2)
> mtext("of My Data", line=1.5, side=3, cex=1.2)


An alternative that keeps everything as one expression is to use atop() 
within the expression rather than the '\n'.  For example:

plot(1, main="")
mtext(expression(paste(atop(paste('Plot of Observed vs. Predicted 
-log(', IC[50], ') Values for the PDGFR'),'Dataset'))), side=3, line=1.5)

But this may still look too ugly :)

In general, '\n' will not be handled very well within mathematical 
expressions.

Paul


>>From: Rajarshi Guha
>>
>>Hi,
>>  I'm making a plot in which the title takes up two lines. The title
>>contains a subscript but when I look at the plot it does'nt 
>>seem tocome
>>out properly. The code I'm using is:
>>
>>   xtxt = expression(paste('Observed -log( ', IC[50], ' )'))
>>   ytxt = expression(paste('Predicted -log( ', IC[50], ' )'))
>>   mtxt = expression(paste('Plot of Observed vs. Predicted -log(',
>>IC[50], ') Values for the PDGFR\nDataset'))
>>
>>   plot(tset$V2, tset$V3, xlim = c(-2,2), ylim = c(-2,2),
>>    pch = 19, col = "blue", 
>>    main=mtxt,
>>    xlab = xtxt,
>>    ylab = ytxt)
>>
>>Removing the IC[50] term makes it come out OK. 
>>Is there any way to get around this?
>>
>>Thanks,
>>
>>-------------------------------------------------------------------
>>Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
>>GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
>>-------------------------------------------------------------------
>>Does Ramanujan know Polish?
>>-- E.B. Ross
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From andy_liaw at merck.com  Tue Apr 20 05:11:17 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 19 Apr 2004 23:11:17 -0400
Subject: [R] Can't find memory.size()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C2D@usrymx25.merck.com>

> From: Shin, Daehyok
> 
> I try memory.size function to find out available memory size,
> but surprisingly R complains it can't find the function.
> ?memory.size also failed.
> 
> Is it not in the base library?

Not really.  help.search("memory.size") in R-1.9.0 says:

memory.size(utils)       Report on Memory Allocation

so it's in `utils', which I believe is loaded by default.

> If so, why can't my R find it?
> I am using the binary 1.9.0 version for Mandrake 9.1.

Because Linux is not Windows...

Andy


> Thanks in advance.
> 
> Daehyok Shin
> Terrestrial Hydrological Ecosystem Modellers
> Geography Department
> University of North Carolina-Chapel Hill
> sdhyok at email.unc.edu
> 
> "We can do no great things, 
> only small things with great love."
>                          - Mother Teresa
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Alexander.Herr at csiro.au  Tue Apr 20 06:42:20 2004
From: Alexander.Herr at csiro.au (Alexander.Herr@csiro.au)
Date: Tue, 20 Apr 2004 14:42:20 +1000
Subject: [R] Error while loading R2HTML in Rprofile
Message-ID: <062AE320EF971E40ACD0F6C93391D7690A5354@exqld1-tsv.nexus.csiro.au>

Hi List,

When loading library(R2HTML) in Rprofile I get the following error:

Error in firstlib(which.lib.loc, package) : 
        couldn't find function "ps.options"
Error in library(R2HTML) : .First.lib failed
[Previously saved workspace restored]

% ps.options is there
% Loading the library from the command prompt works fine, other libraries load fine through Rprofile. 
% This only happens since R1.9.0.

Any ideas?

Thanks Herry

System: Windows XP on dual processor dell, 1gig ram.



--------------------------------------------
Alexander Herr - Herry
Northern Futures
Davies Laboratory, CSIRO
PMB, Aitkenvale, QLD 4814
Phone (07) 4753 8510
Fax   (07) 4753 8650
Home: http://herry.ausbats.org.au
Webadmin ABS: http://ausbats.org.au
Sustainable Ecosystems: http://www.cse.csiro.au/
--------------------------------------------
 



From Wanzare at HCJP.com  Tue Apr 20 06:53:48 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Tue, 20 Apr 2004 13:53:48 +0900
Subject: [R] Rank - Descending order
Message-ID: <1CBA12F2D414914989C723D196B287DC0555EF@jp-svr-ex1.HCJP.COM>

Dear All,
	Is there any simple way to way to produce "rank", for a given
list, but in a descending order?

	E.G: 	
		x	= list(a=c(1,5,2,4));
		rank(x$a);	produces 1,4,2,3 

	However I am looking for a way to generate (4,1,3,2). 

	It would be particularly nice if the proposed solution has all
the niceties of rank function (like NA handling and ties.method
functionality) 


TIA

Manoj



From ggrothendieck at myway.com  Tue Apr 20 07:08:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 05:08:40 +0000 (UTC)
Subject: [R] Rank - Descending order
References: <1CBA12F2D414914989C723D196B287DC0555EF@jp-svr-ex1.HCJP.COM>
Message-ID: <loom.20040420T070529-373@post.gmane.org>


Here are a couple to try:

z <- c(1,5,2,4)
rank(-rank(z))

# If z is numeric this can be simplified to:
rank(-z)

I haven't checked what happens to NAs and ties.method.


Manoj - Hachibushu Capital <Wanzare <at> HCJP.com> writes:

: 
: Dear All,
: 	Is there any simple way to way to produce "rank", for a given
: list, but in a descending order?
: 
: 	E.G: 	
: 		x	= list(a=c(1,5,2,4));
: 		rank(x$a);	produces 1,4,2,3 
: 
: 	However I am looking for a way to generate (4,1,3,2). 
: 
: 	It would be particularly nice if the proposed solution has all
: the niceties of rank function (like NA handling and ties.method
: functionality) 
: 
: TIA
: 
: Manoj



From david.whiting at ncl.ac.uk  Tue Apr 20 10:12:28 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 20 Apr 2004 08:12:28 +0000
Subject: [Way OT] [R] Size of R user base
In-Reply-To: <4084505A.3030209@indigoindustrial.co.nz>
References: <4084169F.8010401@ksvanhorn.com>
	<Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
	<20040419214729.GA26717@sonny.eddelbuettel.com>
	<4084505A.3030209@indigoindustrial.co.nz>
Message-ID: <m2llkrkqz7.fsf_-_@ganymede.ammp.or.tz>

Jason Turner <jasont at indigoindustrial.co.nz> writes:

And the Huli of Papua New Guinea use '15' to mean a very large number
and '15 times 15 samting (something)' to mean something close to
infinity.


> Dirk Eddelbuettel wrote:
> > My preference goes with the numbering scheme attributed to a tribe on some
> > island in the Pacific which consists of a 'factor' with four levels: 'one',
> > 'two', 'three', and 'lots'.
> 
> Australia.  I've been there.  Nice place.  ;)
> 
> Jason
> (who is an Australian)

-- 
David Whiting
Dar es Salaam, Tanzania



From Detlef.Steuer at UniBw-Hamburg.DE  Tue Apr 20 08:18:02 2004
From: Detlef.Steuer at UniBw-Hamburg.DE (Detlef Steuer)
Date: Tue, 20 Apr 2004 08:18:02 +0200
Subject: [R] Size of R user base
In-Reply-To: <4084169F.8010401@ksvanhorn.com>
References: <4084169F.8010401@ksvanhorn.com>
Message-ID: <20040420081802.2a1f7ae7@gaia.unibw-hamburg.de>


On Mon, 19 Apr 2004 12:12:47 -0600
"Kevin S. Van Horn" <kvanhorn at ksvanhorn.com> wrote:

> I have been trying to determine the size of the R user base

Hey, that one is easy: We are legion ....
:-)

detlef

-- 
Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From ripley at stats.ox.ac.uk  Tue Apr 20 08:30:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 07:30:50 +0100 (BST)
Subject: [R] Error while loading R2HTML in Rprofile
In-Reply-To: <062AE320EF971E40ACD0F6C93391D7690A5354@exqld1-tsv.nexus.csiro.au>
Message-ID: <Pine.LNX.4.44.0404200722430.31359-100000@gannet.stats>

On Tue, 20 Apr 2004 Alexander.Herr at csiro.au wrote:

> When loading library(R2HTML) in Rprofile I get the following error:
> 
> Error in firstlib(which.lib.loc, package) : 
>         couldn't find function "ps.options"
> Error in library(R2HTML) : .First.lib failed
> [Previously saved workspace restored]
> 
> % ps.options is there
> % Loading the library from the command prompt works fine, other libraries load fine through Rprofile. 
> % This only happens since R1.9.0.
> 
> Any ideas?

Please read the USER-VISIBLE CHANGES in the NEWS file.  Until R2HTML is 
updated to require(graphics), you need to add that to your .Rprofile, 
before library(R2HTML).

I do wonder what a package is doing re-setting a global option like this:  
if there is a good reason I think it should at least issue a message to
say it has done so.  To my mind it makes having library(R2HTML) in
.Rprofile rather undesirable.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Apr 20 08:34:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 07:34:22 +0100 (BST)
Subject: [R] Rank - Descending order
In-Reply-To: <1CBA12F2D414914989C723D196B287DC0555EF@jp-svr-ex1.HCJP.COM>
Message-ID: <Pine.LNX.4.44.0404200732180.31359-100000@gannet.stats>

rank((-x$a))

rank() does not work on lists but on numeric vectors, and the above works 
generally for numeric vectors.

On Tue, 20 Apr 2004, Manoj - Hachibushu Capital wrote:

> Dear All,
> 	Is there any simple way to way to produce "rank", for a given
> list, but in a descending order?
> 
> 	E.G: 	
> 		x	= list(a=c(1,5,2,4));
> 		rank(x$a);	produces 1,4,2,3 
> 
> 	However I am looking for a way to generate (4,1,3,2). 
> 
> 	It would be particularly nice if the proposed solution has all
> the niceties of rank function (like NA handling and ties.method
> functionality) 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Apr 20 08:59:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Apr 2004 08:59:39 +0200
Subject: [R] Size of R user base
In-Reply-To: <4084509E.5060702@ksvanhorn.com>
References: <Pine.LNX.4.44.0404192218550.30669-100000@gannet.stats>
	<4084509E.5060702@ksvanhorn.com>
Message-ID: <4084CA5B.7030002@statistik.uni-dortmund.de>

Kevin S. Van Horn wrote:
> Prof Brian Ripley wrote:
> 
>>> Conclusion:  Very few books sell to more than 12% of the population 
>>> of potential buyers, and most books have a far lower penetration -- 
>>> 1% or   
>>
>>
>> Where did you get that 12% from?
>>
> 
> A booklet on assessing financial feasibility in nonfiction book 
> publishing.  That's a general figure, so perhaps it doesn't apply if the 
> book in question is a must-have, definitive reference for the group in 
> question... like the book you mention (if it's the one I think it is).
> 
>> I have big problems with the definition. What is an `R user'? Someone  
>> who has ever used R, even for a one-hour practical class? Someone who 
>> has used R in the last 3 months?
> 
> 
> 
> Good question.  I guess I'd lean more towards your second definition, 
> with the added caveat of "and expects to use it again in the next 3 
> months".


Folks, for sure I am biased, but on the floor my office is located, 
there are at least 12 people working quite frequently with R. I'm the 
only one subscribed on R-help. Let's underestimate the rest of the 
department to include 10 other R users (among them, there is another one 
subscribed to R-help). Many of our students are not only using R when 
they have to, but also when they are allowed to use a software product 
of their choice. I don't think any of these students I'd call R users is 
subscribed to the list.
BTW: There are more users who own the book that was mentioned several 
times in this thread than users who are subscribed to the list. 
Furthermore, some people have got the book, but do not use R or S-PLUS.

I think it is impossible to estimate the real number of R users 
(whatever that is) if we require a moderate confidence interval.

Uwe



From bxc at steno.dk  Tue Apr 20 09:10:42 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 20 Apr 2004 09:10:42 +0200
Subject: [R] Size of R user base
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90179E68C@exdkba022.novo.dk>

I wonder if there is a count of the number of downloads of 
each version of the R installations (for example rw1090.exe)?

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From mike.campana at freesurf.ch  Tue Apr 20 10:22:02 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Tue, 20 Apr 2004 09:22:02 +0100
Subject: [R] polygon
Message-ID: <1082445722.webexpressdV3.1.f@smtp.freesurf.ch>

Dear all

In order to clearly mark values wich are larger than a treshold value, I 
would like to color the surface below the line given by plot (yy~xx). To 
color is only the surface between abline (treshold) and yy if they are 
larger than the specific limit. I guess I can use the function polygon, 
but I can not find any valuable solution.
I'm grateful to you for an advice or an example.
Mike  

xx <- c(1:100)
yy <- rnorm(100)
plot (yy~xx,type="l")
abline (h=0.5,col="red")
#?? how can I use polygon()
#in order to color surface below yy value and > abline??
polygon(xx,yy,col="gray")

---



From ripley at stats.ox.ac.uk  Tue Apr 20 09:43:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 08:43:05 +0100 (BST)
Subject: [R] Size of R user base
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E68C@exdkba022.novo.dk>
Message-ID: <Pine.LNX.4.44.0404200828250.4195-100000@gannet.stats>

Not really.  We have looked at that in the past, but 

- unsuccessful partial downloads can account for a high proportion of 
download attempts, and it is hard to tell from the httpd logs exactly 
which were successful.

- one would have to do it on all CRAN mirrors.

- the number will be a serious underestimate due to the prevalence of
caches.  (Once anyone in .ac.uk or on my ISP .ntl.com downloads it, it is
likely to be grabbed from the cache for the next week at least.)


Of course, once you have #downloads, it tells you little about
#installations, and that tells you little about #users.


It seems to me against the spirit of Open Source software to attempt to 
monitor distribution.  We could ask R to `call home' on first use (in the 
way e.g. pine does) but I suspect many users would find that 
objectionable.


On Tue, 20 Apr 2004, BXC (Bendix Carstensen) wrote:

> I wonder if there is a count of the number of downloads of 
> each version of the R installations (for example rw1090.exe)?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue Apr 20 09:56:50 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Apr 2004 09:56:50 +0200
Subject: [R] Size of R user base
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90179E68C@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90179E68C@exdkba022.novo.dk>
Message-ID: <16516.55234.859527.878062@gargle.gargle.HOWL>

>>>>> "BXC" == BXC (Bendix Carstensen) <bxc at steno.dk>
>>>>>     on Tue, 20 Apr 2004 09:10:42 +0200 writes:

    BXC> I wonder if there is a count of the number of downloads
    BXC> of each version of the R installations (for example
    BXC> rw1090.exe)?

doesn't work either for several reasons,
some being

- students get R (and ESS/WinEdt and ...) on CDs
  or download from local repositories

- many Unix users in the same organization use R installed only
  once in a mounted (aka "network") file system. 
  Windows users as well, e.g. here at ETH, and probably in many
  other places.

----

In general, I quite agree with Uwe's comments --- including those
about the ratio of R users and R mailing list subscribers; I'd
guess that to be around 10:1, though rather higher than lower.

About a `confidence interval': If we log transform it becomes
feasible to give an interval I'm ``quite confident'' about:

  log10(N) \in [3.5, 5.3]

but then that's probably not informative enough for management
decisions.

Martin



From phgrosjean at sciviews.org  Tue Apr 20 10:47:06 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 20 Apr 2004 10:47:06 +0200
Subject: [R] Size of R user base
Message-ID: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>

This question of the R-users is of course very interesting, but not from the
point of view of the absolute number of users (which has only a limited
interest). Here is what I see interesting:
1) What is the fraction of stat software users that use R (in particular R
versus S-PLUS, as it was the initial question)?
2) How this fraction fluctuates in time?
3) How this fraction changes according to the platform (Windows, Linux/Unix,
MacOS)?

Ad even interesting, but even more difficult to assess:
4) Does R have an impact on the number of stat software users (i.e., do more
people use "serious" stat systems than Excel, for instance)? An example: I
teach biostats in a Belgium University. Before me, student had to use
Excel... a big mistake, of course. Now, they learn R... and some of them
become true R users (whatever the definition you give to it).
5) Does R have an impact on the quality of statistical analyses done (better
use of methods, and use of less common methods but appropriate for a study)?

All these questions need an estimate of the number of R users, of course.
Plus (4) and (5) are subjective, and difficult to evaluate at a large scale.
However, it is perhaps possible to do at the scale of a company, or of an
university. If someone has some experience in such kind of evaluation or can
point me to the right (not specialized, please!) documentation, I am
interested.


A last comment/question: would it be possible to add some code in R that
does the following:
1) it is triggered only if the software was used at least, let's say 10, or
20 times on the computer where it was installed,
2) then it checks if an update of R is available (just by looking if a given
link in a centralized web site -CRAN?- exists),
3) when it finds that link, it just warns the user of an update in a not
annoying way, for instance like that:

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

An update is available at http://cran.r-project.org.

>

and,
4) it deactivate itself once the link is found.

The main role of this code would be, of course, to warn users that an update
is available. One side effect would be that it should be possible to monitor
access to the link in the centralized web site and to know:
1) How many people installed and used R at least 10 (or 20) times on their
computer?
2) On which platform?
3) Perhaps some more infos, like location of the machines?

Of course, this will only work with computers connected to the internet,...
but at least, it could be one way to evaluate the number of R users. Would
that be an infringment of Open Source, or any other rule of freedom? I don't
know, but it does seem to be quite widespread (at least for commercial
software). so, why an Open Source software would not be able to monitor the
number of users?

Best,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From wb at arb-phys.uni-dortmund.de  Tue Apr 20 10:49:02 2004
From: wb at arb-phys.uni-dortmund.de (Wilhelm B. Kloke)
Date: Tue, 20 Apr 2004 10:49:02 +0200 (CEST)
Subject: [R] Indexing by factor misfeature
Message-ID: <200404200849.i3K8n24Y050469@yorikke.arb-phys.uni-dortmund.de>

Yesterday I was biten by a feature, which I find too dangerous.

I wanted to use a factor `Subject?? as index into a data frame, whose row
names were the levels of this factor. So there a 2 different possible
interpretations of this: Either Subject is coerced to numeric or to
character. The intended interpretation was, of course, `as.character(Subject)'.
R did `as.numeric(Subject)??. This will not cause a semantic error, when
the levels of Subject happen to be in numeric order; so it will be
unnoticed quite often.

I propose to issue a warning whenever a factor is implicitly coerced,
at least in an ambiguous context.



From ripley at stats.ox.ac.uk  Tue Apr 20 11:00:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 10:00:41 +0100 (BST)
Subject: [R] Indexing by factor misfeature
In-Reply-To: <200404200849.i3K8n24Y050469@yorikke.arb-phys.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0404200956360.4320-100000@gannet.stats>

This *is* a documented feature: see ?Extract.

I am afraid your `of course' is completely non-obvious to me.  I would 
object strongly to code written to use a documented feature issuing a 
warning.

On Tue, 20 Apr 2004, Wilhelm B. Kloke wrote:

> Yesterday I was biten by a feature, which I find too dangerous.
> 
> I wanted to use a factor `Subject?? as index into a data frame, whose row
> names were the levels of this factor. So there a 2 different possible
> interpretations of this: Either Subject is coerced to numeric or to
> character. The intended interpretation was, of course, `as.character(Subject)'.
> R did `as.numeric(Subject)??. This will not cause a semantic error, when
> the levels of Subject happen to be in numeric order; so it will be
> unnoticed quite often.
> 
> I propose to issue a warning whenever a factor is implicitly coerced,
> at least in an ambiguous context.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jasont at indigoindustrial.co.nz  Tue Apr 20 11:04:57 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 20 Apr 2004 21:04:57 +1200 (NZST)
Subject: [R] Indexing by factor misfeature
In-Reply-To: <200404200849.i3K8n24Y050469@yorikke.arb-phys.uni-dortmund.de>
References: <200404200849.i3K8n24Y050469@yorikke.arb-phys.uni-dortmund.de>
Message-ID: <32680.203.9.176.60.1082451897.squirrel@webmail.maxnet.co.nz>

> Yesterday I was biten by a feature, which I find too dangerous.

>From your description, it sounds like you confused row lables and vector
values -- two very different things.  It also sounds like you mixed up
factor values and levels.  Let me know if I'm wrong.

> I wanted to use a factor `Subject?? as index into a data frame, whose row
> names were the levels of this factor.
> So there a 2 different possible
> interpretations of this: Either Subject is coerced to numeric or to
> character. The intended interpretation was, of course,
> `as.character(Subject)'.
> R did `as.numeric(Subject)??.

What did you do to make R do this coersion?  I can't tell from here.

Try this, with the same "Subject" vector:

foo <- factor(Subject)
foo
levels(foo) <- names(Subject)
foo

Does that help?

Cheers

Jason



From Rau at demogr.mpg.de  Tue Apr 20 11:37:38 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Tue, 20 Apr 2004 11:37:38 +0200
Subject: [R] Size of R user base
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0869@hermes.demogr.mpg.de>

Hello,

> -----Original Message-----
> From:	Philippe Grosjean [SMTP:phgrosjean at sciviews.org]
> Sent:	Tuesday, April 20, 2004 10:47 AM
> To:	r-help at stat.math.ethz.ch
> Subject:	RE: [R] Size of R user base
> 
> Of course, this will only work with computers connected to the
> internet,...
> but at least, it could be one way to evaluate the number of R users. Would
> that be an infringment of Open Source, or any other rule of freedom? I
> don't
> know, but it does seem to be quite widespread (at least for commercial
> software). so, why an Open Source software would not be able to monitor
> the
> number of users?
> 
	I don't know if it would violate any part of the GPL under which R
is licensed. But I think it is against the spirit of free software (free as
in "free speech"not "free beer"...) to try to control the users (in German:
legitim vs. legal).
	But more importantly: the GPL reserves the right to change the
source code. So what happens if someone removes this part of the code before
compiling? Then there would not be any chance of tracing those R users,
right? And, as far as I understood, it is also allowed to distribute those
modified source-code versions. I assume that there would be widespread
interest in such a derivative work of R where this feedback-code would have
been left out. So again, there is a problem of getting to know the size of
the R User Base. 

	Just some thoughts,
	Roland



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From david.whiting at ncl.ac.uk  Tue Apr 20 14:29:59 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 20 Apr 2004 12:29:59 +0000
Subject: [R] Size of R user base
In-Reply-To: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>
References: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>
Message-ID: <m265bultmg.fsf@ganymede.ammp.or.tz>

"Philippe Grosjean" <phgrosjean at sciviews.org> writes:


[...]

> A last comment/question: would it be possible to add some code in R that
> does the following:
> 1) it is triggered only if the software was used at least, let's say 10, or
> 20 times on the computer where it was installed,
> 2) then it checks if an update of R is available (just by looking if a given
> link in a centralized web site -CRAN?- exists),
> 3) when it finds that link, it just warns the user of an update in a not
> annoying way, for instance like that:
> 

[...]

> 
> An update is available at http://cran.r-project.org.
> 

> and,
> 4) it deactivate itself once the link is found.
> 

[...]

> Of course, this will only work with computers connected to the internet,...
> but at least, it could be one way to evaluate the number of R users. Would
> that be an infringment of Open Source, or any other rule of freedom? I don't
> know, but it does seem to be quite widespread (at least for commercial
> software). so, why an Open Source software would not be able to monitor the
> number of users?

Smoothwall (www.smoothwall.org) does (did?) that, so there is an Open
Source precedent.  They did it both to keep an eye on the number of
installations and to inform users of updates.  I have no idea how
accurate they think their statistics are though.  Also, Smoothwall is
a firewall so it going to be connected to the internet frequently and
judging by the discussions so far the definition of a "Smoothwall
installation" seems to be easier to pin-down than that of an "R-user."

One other concern: would it cause some kind of lag on startup if it
has to check, especially when a machine is not connected or has a slow
connection?

-- 
David Whiting
Dar es Salaam, Tanzania



From uli at biochem.dshs-koeln.de  Tue Apr 20 12:46:17 2004
From: uli at biochem.dshs-koeln.de (Ulrich Flenker)
Date: Tue, 20 Apr 2004 12:46:17 +0200 (CEST)
Subject: [R] R-1.9.0: configure/install problem
Message-ID: <Pine.LNX.4.44.0404201049370.28830-100000@FILESERV.biochem.dshs-koeln.de>

Dear R-helpers,

for the first time since R-0.6x I have to face installation problem. I
switched to a new Linux-box (SuSE 9.0) and installed as many libraries as
possible.

R-1.9.0.tgz went to /usr/local/lib and was extracted without problems.

After running properly for a while, 'configure' (called without any
switches) gave
"configure: creating ./config.status" which was followed by
"/bin/sh: relocation error: /bin/sh: undefined symbol:  rl_completion_suppress_append".

The last line obviously wasn't produced by 'configure' as it came via
stderr.

No makefile was created. I ran './config.status' followed by 'make'. After
several minutes of behaving nice, the compilation process aborted with
the following output:


make[2]: Entering directory `/usr/local/lib/R-1.9.0/src/library'
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
make[2]: *** [Rdfiles] Fehler 127
make[2]: Leaving directory `/usr/local/lib/R-1.9.0/src/library'
make[1]: *** [docs] Fehler 2
make[1]: Leaving directory `/usr/local/lib/R-1.9.0/src/library'
make: [docs] Fehler 2 (ignoriert)
make[1]: Entering directory 
`/usr/local/lib/R-1.9.0/src/library/Recommended'
make[2]: Entering directory 
`/usr/local/lib/R-1.9.0/src/library/Recommended'
make[2]: Leaving directory 
`/usr/local/lib/R-1.9.0/src/library/Recommended'
make[2]: Entering directory 
`/usr/local/lib/R-1.9.0/src/library/Recommended'
MAKE="make" R_LIBS= ../../../bin/R CMD INSTALL -l ../../../library 
survival.tgz
sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
make[2]: *** [survival.ts] Fehler 127
make[2]: Leaving directory 
`/usr/local/lib/R-1.9.0/src/library/Recommended'
make[1]: *** [recommended-packages] Fehler 2
make[1]: Leaving directory 
`/usr/local/lib/R-1.9.0/src/library/Recommended'
make: *** [stamp-recommended] Fehler 2

The same occured when trying to build R-1.8.0 on my new system. In
contrast it configured and compiled nicely on my 4 year old Linux
system. Can somebody help?

Many, many thanks in advance!



--
	Uli Flenker

	Institute of Biochemistry
	German Sport University Cologne
	Carl-Diem-Weg 6
	50933 Cologne

	+49(0)221/4982-5060



From ripley at stats.ox.ac.uk  Tue Apr 20 12:53:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 11:53:45 +0100 (BST)
Subject: [R] R-1.9.0: configure/install problem
In-Reply-To: <Pine.LNX.4.44.0404201049370.28830-100000@FILESERV.biochem.dshs-koeln.de>
Message-ID: <Pine.LNX.4.44.0404201150100.5785-100000@gannet.stats>

This looks like a readline version problem for the shell /bin/sh
(presumably actually bash) you have installed.  Do you have an older
version of readline somewhere on your system?


On Tue, 20 Apr 2004, Ulrich Flenker wrote:

> Dear R-helpers,
> 
> for the first time since R-0.6x I have to face installation problem. I
> switched to a new Linux-box (SuSE 9.0) and installed as many libraries as
> possible.
> 
> R-1.9.0.tgz went to /usr/local/lib and was extracted without problems.
> 
> After running properly for a while, 'configure' (called without any
> switches) gave
> "configure: creating ./config.status" which was followed by
> "/bin/sh: relocation error: /bin/sh: undefined symbol:  rl_completion_suppress_append".
> 
> The last line obviously wasn't produced by 'configure' as it came via
> stderr.
> 
> No makefile was created. I ran './config.status' followed by 'make'. After
> several minutes of behaving nice, the compilation process aborted with
> the following output:
> 
> 
> make[2]: Entering directory `/usr/local/lib/R-1.9.0/src/library'
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> make[2]: *** [Rdfiles] Fehler 127
> make[2]: Leaving directory `/usr/local/lib/R-1.9.0/src/library'
> make[1]: *** [docs] Fehler 2
> make[1]: Leaving directory `/usr/local/lib/R-1.9.0/src/library'
> make: [docs] Fehler 2 (ignoriert)
> make[1]: Entering directory 
> `/usr/local/lib/R-1.9.0/src/library/Recommended'
> make[2]: Entering directory 
> `/usr/local/lib/R-1.9.0/src/library/Recommended'
> make[2]: Leaving directory 
> `/usr/local/lib/R-1.9.0/src/library/Recommended'
> make[2]: Entering directory 
> `/usr/local/lib/R-1.9.0/src/library/Recommended'
> MAKE="make" R_LIBS= ../../../bin/R CMD INSTALL -l ../../../library 
> survival.tgz
> sh: relocation error: sh: undefined symbol: rl_completion_suppress_append
> make[2]: *** [survival.ts] Fehler 127
> make[2]: Leaving directory 
> `/usr/local/lib/R-1.9.0/src/library/Recommended'
> make[1]: *** [recommended-packages] Fehler 2
> make[1]: Leaving directory 
> `/usr/local/lib/R-1.9.0/src/library/Recommended'
> make: *** [stamp-recommended] Fehler 2
> 
> The same occured when trying to build R-1.8.0 on my new system. In
> contrast it configured and compiled nicely on my 4 year old Linux
> system. Can somebody help?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uli at biochem.dshs-koeln.de  Tue Apr 20 13:34:39 2004
From: uli at biochem.dshs-koeln.de (Ulrich Flenker)
Date: Tue, 20 Apr 2004 13:34:39 +0200 (CEST)
Subject: [R] R-1.9.0: configure/install problem
In-Reply-To: <Pine.LNX.4.44.0404201150100.5785-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404201315570.28830-100000@FILESERV.biochem.dshs-koeln.de>

On Tue, 20 Apr 2004, Prof Brian Ripley wrote:

> This looks like a readline version problem for the shell /bin/sh
> (presumably actually bash) you have installed.  Do you have an older
> version of readline somewhere on your system?
> 

Thanks for the quick reply!

/bin/sh is a symbolic link to /bin bash. 'ldd /bin/bash' gives:

        libreadline.so.4 => /lib/libreadline.so.4 (0x4002f000)
        libhistory.so.4 => /lib/libhistory.so.4 (0x4005c000)
        libncurses.so.5 => /lib/libncurses.so.5 (0x40063000)
        libdl.so.2 => /lib/libdl.so.2 (0x400a9000)
        libc.so.6 => /lib/i686/libc.so.6 (0x400ad000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)


In directory /lib there is libreadline.so.4, which is a symbolic link to
libreadline.so.4.3. I didn't find any other candidates for
readline-libraries on my system. How to proceed? Should I transfer old
readline-files from my old box?

Which readline-versions is R known to work with? Writing these lines, I 
think I remember this was an issue on this list some time ago.

Best ...

	Uli



From kjetil at entelnet.bo  Tue Apr 20 13:40:21 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 20 Apr 2004 07:40:21 -0400
Subject: [R] Size of R user base
In-Reply-To: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>
Message-ID: <4084D3E5.17583.1C5C02@localhost>

On 20 Apr 2004 at 10:47, Philippe Grosjean wrote:

.
.
.
.
> Of course, this will only work with computers connected to the
> internet,... but at least, it could be one way to evaluate the number
> of R users. Would that be an infringment of Open Source, or any other
> rule of freedom? I don't know, but it does seem to be quite widespread
> (at least for commercial software). so, why an Open Source software
> would not be able to monitor the number of users?

That would make R into spyware, and there exist software to monitore 
and warn aganst/automatically remove spyware, and some users have 
such installed (and it will grow).

Kjetil Halvorsen

> 
> Best,
> 
> Philippe Grosjean
> 
> .......................................................<??}))><....
>  ) ) ) ) )
> ( ( ( ( (   Prof. Philippe Grosjean
> \  ___   )
>  \/ECO\ (   Numerical Ecology of Aquatic Systems
>  /\___/  )  Mons-Hainaut University, Pentagone
> / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
>  /NUM\/  )
>  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
>        \ )  email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org) ( (
> ( ( (
> ...................................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Apr 20 13:47:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 12:47:31 +0100 (BST)
Subject: [R] R-1.9.0: configure/install problem
In-Reply-To: <Pine.LNX.4.44.0404201315570.28830-100000@FILESERV.biochem.dshs-koeln.de>
Message-ID: <Pine.LNX.4.44.0404201244540.11664-100000@gannet.stats>

It is not an R issue but a bash issue: it is sh not R that is reporting 
the error.  It might be that the paths R sets are part of the problem: are 
you sure there is no libreadline in /usr/local/lib, for example?

Earlier versions of readline are static libraries, precisely to avoid this 
kind of problem.

On Tue, 20 Apr 2004, Ulrich Flenker wrote:

> On Tue, 20 Apr 2004, Prof Brian Ripley wrote:
> 
> > This looks like a readline version problem for the shell /bin/sh
> > (presumably actually bash) you have installed.  Do you have an older
> > version of readline somewhere on your system?
> > 
> 
> Thanks for the quick reply!
> 
> /bin/sh is a symbolic link to /bin bash. 'ldd /bin/bash' gives:
> 
>         libreadline.so.4 => /lib/libreadline.so.4 (0x4002f000)
>         libhistory.so.4 => /lib/libhistory.so.4 (0x4005c000)
>         libncurses.so.5 => /lib/libncurses.so.5 (0x40063000)
>         libdl.so.2 => /lib/libdl.so.2 (0x400a9000)
>         libc.so.6 => /lib/i686/libc.so.6 (0x400ad000)
>         /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x40000000)
> 
> 
> In directory /lib there is libreadline.so.4, which is a symbolic link to
> libreadline.so.4.3. I didn't find any other candidates for
> readline-libraries on my system. How to proceed? Should I transfer old
> readline-files from my old box?

No!

> Which readline-versions is R known to work with? Writing these lines, I 
> think I remember this was an issue on this list some time ago.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tmulholl at bigpond.net.au  Tue Apr 20 13:50:09 2004
From: tmulholl at bigpond.net.au (Tom Mulholland)
Date: Tue, 20 Apr 2004 19:50:09 +0800
Subject: [R] Size of R user base. It's not what you've got it's what you
	do with it
In-Reply-To: <4084169F.8010401@ksvanhorn.com>
Message-ID: <001501c426cd$a292b010$2302a8c0@ACER>

While reading all of these comments I started thinking about how you might
collect the data. As pointed out, in many ways, there are immediate problems
with the validity of what's collected. That is what is the concept that we
are trying to measure.

So, that was always going to make it hard, but assuming that you got past
that point and you could ask a question who would you ask it of. Here in
Western Australia there's a small and mostly unconnected fraternity of
people who use R. In a place where even strangers seem somehow kind of
familiar (you've been sitting on the same bus for the last 20 years and
still don't know who they are), so my gut feeling is that it's not a major
penetration. Trying to do a random sample would be problematic to say the
least. If you started to try and stratify the sample where would you start.
If you go to places where you know R is being used you have problems with
bias. That might rule out universities as a stratified sample because we're
not really clear about what we are asking.

So what other sources are there. Well there's been some comment about the
mailing list and all the problems that might be involved in that and
counting downloads. Guess that's out. Then I thought about the vibrancy of
the R email list. For a topic such as a statistical programming language
it's unusual to see such activity (at least in my experience.)

So my mind turned to trying to measure some smaller but representative
process. In much the same way that criminologists use the homicide rate as
the pointed end of the violence spectrum (let's not go there, find another
list to have that debate)

So I started thinking about what questions you might ask the typical SPSS or
SAS or ... user that would help. That's when it struck me. Lot's of us, R
Users that is, also have the luck (some might say misfortune) to also work
with other languages. Why do we pick R? The one thing that I have noticed
that separates R Users and S Plus users (and probably some of the other
products that I don't use and know about) with the mainstream use of SPSS
and SAS (at least here in WA) is that the mainstream users are not pushing
the envelope.

So the question is not "How many users are there ?", it's "What are people
doing with it ?" I put the formal citation in each publication I produce and
while they are mainly in-house productions (a problem with applied research)
maybe they'll eventually start to get into the citation charts. I hear some
academics love browsing these. (That is aimed at no-one who's on this list)

Tom Mulholland
Tom Mulholland Associates

Footnote: When 5 out of 6 paragraphs start with the word "So" it's time to
get a life. So I'm off to get a life.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kevin S. Van Horn
Sent: Tuesday, April 20, 2004 2:13 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Size of R user base


I have been trying to determine the size of the R user base, and was
asked to share my findings with this mailing list.  Although I still
don't have any definite estimate of this number, I do have some
interesting and indicative information:


1. It appears that there are about 100,000 S-PLUS users.

Rationale: According to Insightful's 2002 Annual Report, over 100,000
people use Insightful software; since license revenues from S-PLUS and
add-on modules accounted for nearly all of their license revenues in
2002, and their other products are much more costly than S-PLUS, it
seems that the great majority of users of Insightful software are S-PLUS
users.

Conclusion: S-PLUS costs $3500 (Windows) or $4500 (Linux/Unix) for an
individual copy; R is free.  This suggests that there may be more R
users than S-PLUS users, which suggests > 100,000 R users.

Does anyone has any other information that would give some notion as to
the RELATIVE numbers of R and S-PLUS users?


2. At least one R book has achieved sales of just over 5,000 copies.  (I
could not find sales figures for other R books, as it appears that
publishers are closed-mouthed about such figures.  And no, I can't
reveal which particular book this was, so don't ask.)

Conclusion:  Very few books sell to more than 12% of the population of
potential buyers, and most books have a far lower penetration -- 1% or
less is not uncommon.  A 12% penetration for the book in question
implies 42,000 R users; a more reasonable 5% penetration implies 100,000
users.  A low 1% penetration implies 500,000 users.


3. There are a total of 3225 unique subscribers to the three R mailing
lists.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

---
Incoming mail is certified Virus Free.



---



From p.dalgaard at biostat.ku.dk  Tue Apr 20 13:55:50 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2004 13:55:50 +0200
Subject: [R] Size of R user base
In-Reply-To: <4084D3E5.17583.1C5C02@localhost>
References: <4084D3E5.17583.1C5C02@localhost>
Message-ID: <x2d6626eyh.fsf@biostat.ku.dk>

kjetil at entelnet.bo writes:

> On 20 Apr 2004 at 10:47, Philippe Grosjean wrote:

> > Of course, this will only work with computers connected to the
> > internet,... but at least, it could be one way to evaluate the number
> > of R users. Would that be an infringment of Open Source, or any other
> > rule of freedom? I don't know, but it does seem to be quite widespread
> > (at least for commercial software). so, why an Open Source software
> > would not be able to monitor the number of users?
> 
> That would make R into spyware, and there exist software to monitore 
> and warn aganst/automatically remove spyware, and some users have 
> such installed (and it will grow).

Not quite spyware. Spyware generally works in more covert ways and
tries to hide itself from the user.

However, quite a few people think that having programs connecting to
places on the internet without being asked is annoying and a potential
invasion of privacy and it may even cost people money if they're on a
dialup non-flat-fee connection.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From baron at psych.upenn.edu  Tue Apr 20 14:17:33 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 20 Apr 2004 08:17:33 -0400
Subject: [R] Size of R user base
In-Reply-To: <x2d6626eyh.fsf@biostat.ku.dk>
References: <4084D3E5.17583.1C5C02@localhost> <x2d6626eyh.fsf@biostat.ku.dk>
Message-ID: <20040420121733.GA16439@psych>

I looked at the number of unique IP addresses who come to my R
search site (in sig below).  The result was surpising, and I fear
I did it wrong.

I have been doing it each week, and typical numbers are around
700.  (Sometimes as high as 900 when I do something like this,
which reminds people of its existence.)  For the last 4.5 weeks,
the total of the weekly totals comes to 3148.  But the number of
unique addresses in the same period, combining all weeks, is
2586.  That means that a (to me) surprisingly high proportion of
users is new each week.  Extend this over years, and it adds up.

The command I used was (folded here, but originally on one line):

grep -e htsearch -e " /R/" access_log* | cut -d" " -f1 | cut -d:
-f2 | sort | uniq | wc -l

Of course, IP addresses are crude, because some people use more
than one, and some addresses are used by more than one person,
but I suspect that they are within one power of 10 of the right
answer.  (In my field, that is called "precise.")

FWIW, I have several students who use R.  They have downloaded
it, but they do not use my R page or subscribe to the help list.
They ask me for help instead.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From ripley at stats.ox.ac.uk  Tue Apr 20 14:52:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 20 Apr 2004 13:52:54 +0100 (GMT Daylight Time)
Subject: [R] Size of R user base
In-Reply-To: <x2d6626eyh.fsf@biostat.ku.dk>
Message-ID: <Pine.WNT.4.44.0404201350370.4072-100000@gannet.stats.ox.ac.uk>

On 20 Apr 2004, Peter Dalgaard wrote:

> kjetil at entelnet.bo writes:
>
> > On 20 Apr 2004 at 10:47, Philippe Grosjean wrote:
>
> > > Of course, this will only work with computers connected to the
> > > internet,... but at least, it could be one way to evaluate the number
> > > of R users. Would that be an infringment of Open Source, or any other
> > > rule of freedom? I don't know, but it does seem to be quite widespread
> > > (at least for commercial software). so, why an Open Source software
> > > would not be able to monitor the number of users?
> >
> > That would make R into spyware, and there exist software to monitore
> > and warn aganst/automatically remove spyware, and some users have
> > such installed (and it will grow).
>
> Not quite spyware. Spyware generally works in more covert ways and
> tries to hide itself from the user.
>
> However, quite a few people think that having programs connecting to
> places on the internet without being asked is annoying and a potential
> invasion of privacy and it may even cost people money if they're on a
> dialup non-flat-fee connection.

It is possible to ask: I mentioned pine, which does, and so do some perl
installers.

I don't think people would mind if the rw1091.exe installer had an option
(ticked by default) to `call home', but I don't think we would learn enough
for this to be worth setting up.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From darkjacknife at hotmail.com  Tue Apr 20 15:31:59 2004
From: darkjacknife at hotmail.com (angel hellraiser)
Date: Tue, 20 Apr 2004 13:31:59 +0000
Subject: [R] experimental design
Message-ID: <Sea1-F49t2KoTstO66l00046ec2@hotmail.com>

Hi R users:

Anyone knows like to obtain software for experimental design?

I need to get Aliasing structure like SAS does.
Anything for windows.

Thanks in advance.

My e-mail is darkjacknife at hotmail.com

_________________________________________________________________
??Vas a comprar algo a trav??s de Internet? Ord??nalo por el mejor precio en 
MSN Compras. http://www.msn.es/compras/



From m_nica at hotmail.com  Tue Apr 20 15:37:44 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Tue, 20 Apr 2004 08:37:44 -0500
Subject: [R] Size of R user base
References: <Pine.WNT.4.44.0404201350370.4072-100000@gannet.stats.ox.ac.uk>
Message-ID: <LAW10-OE50vPOJD0xIk00004323@hotmail.com>

Why not ask those downloading rw1091 to complete (at their choice) a small
survey (including questions such as 'will it be used by a group or an
individual'). It would provide some data that could lead to an estimate as
good as anything else.

Mihai

----- Original Message ----- 
From: "Prof Brian D Ripley" <ripley at stats.ox.ac.uk>
To: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
Cc: <r-help at stat.math.ethz.ch>; <kjetil at entelnet.bo>; "Philippe Grosjean"
<phgrosjean at sciviews.org>
Sent: Tuesday, April 20, 2004 7:52 AM
Subject: Re: [R] Size of R user base


> On 20 Apr 2004, Peter Dalgaard wrote:
>
> > kjetil at entelnet.bo writes:
> >
> > > On 20 Apr 2004 at 10:47, Philippe Grosjean wrote:
> >
> > > > Of course, this will only work with computers connected to the
> > > > internet,... but at least, it could be one way to evaluate the
number
> > > > of R users. Would that be an infringment of Open Source, or any
other
> > > > rule of freedom? I don't know, but it does seem to be quite
widespread
> > > > (at least for commercial software). so, why an Open Source software
> > > > would not be able to monitor the number of users?
> > >
> > > That would make R into spyware, and there exist software to monitore
> > > and warn aganst/automatically remove spyware, and some users have
> > > such installed (and it will grow).
> >
> > Not quite spyware. Spyware generally works in more covert ways and
> > tries to hide itself from the user.
> >
> > However, quite a few people think that having programs connecting to
> > places on the internet without being asked is annoying and a potential
> > invasion of privacy and it may even cost people money if they're on a
> > dialup non-flat-fee connection.
>
> It is possible to ask: I mentioned pine, which does, and so do some perl
> installers.
>
> I don't think people would mind if the rw1091.exe installer had an option
> (ticked by default) to `call home', but I don't think we would learn
enough
> for this to be worth setting up.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272860 (secr)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From rory at campbell-lange.net  Tue Apr 20 15:36:29 2004
From: rory at campbell-lange.net (Rory Campbell-Lange)
Date: Tue, 20 Apr 2004 14:36:29 +0100
Subject: [R] Size of R user base
In-Reply-To: <Pine.LNX.4.44.0404200828250.4195-100000@gannet.stats>
References: <0ABD88905D18E347874E0FB71C0B29E90179E68C@exdkba022.novo.dk>
	<Pine.LNX.4.44.0404200828250.4195-100000@gannet.stats>
Message-ID: <20040420133629.GD32250@campbell-lange.net>

On 20/04/04, Prof Brian Ripley (ripley at stats.ox.ac.uk) wrote:
> - the number will be a serious underestimate due to the prevalence of
> caches.  (Once anyone in .ac.uk or on my ISP .ntl.com downloads it, it
> is likely to be grabbed from the cache for the next week at least.)

And a lot of people must use it as part of a distribution, eg Debian.

-- 
Rory Campbell-Lange 
<rory at campbell-lange.net>
<www.campbell-lange.net>



From tpapp at axelero.hu  Tue Apr 20 15:33:20 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Tue, 20 Apr 2004 15:33:20 +0200
Subject: [R] Size of R user base
In-Reply-To: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>
References: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>
Message-ID: <20040420133320.GA916@localhost>

On Tue, Apr 20, 2004 at 10:47:06AM +0200, Philippe Grosjean wrote:

> The main role of this code would be, of course, to warn users that an update
> is available. One side effect would be that it should be possible to monitor
> access to the link in the centralized web site and to know:

I appreciate that it would be really nice to know the size of the user
base: the fact that lots of people are using R would give everybody a
warm, fuzzy feeling.

On the other hand, some people are getting carried away with this
discussion.  I would consider the above suggestion a serious violation
of my privacy: if I want to make this type of personal data (what
software you use is personal data, in the legal sense) public, I can
do it, but I don't want any software on my computer which "kindly"
does that for me if I did not ask for it.  I don't care whether the
data would be anonymized -- it is a matter of principle.  Note that I
am using Debian's popularity-contest package, but that was my choice,
not somebody else's.  BTW, this package would be able to give some
statistics on how widespread R is among Debian users.

So before people are coming up with ingenious (and possibly invasive)
methods to estimate the user base, I would like somebody to explain
why this is relevant.  R already has the "critical mass" it needs,
given that it is a specialized piece of software.  It is always good
to have a broader user base, but R is doing fine as it is.  In some
markets, it is safer to pick a product with a wider user base (think
VHS vs Betamax), and sometimes this argument applies to software (eg
you should not make your bet on a commercial, closed-source software
company with a dwindling user base, because if they go bankrupt, you
can expect no updates etc).  But free software is entirely different.

Knowing that we have more than a couple thousand users and dozens of
active developers is good enough for me.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From Arne.Muller at aventis.com  Tue Apr 20 15:44:41 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Tue, 20 Apr 2004 15:44:41 +0200
Subject: [R] strange result with contrasts
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF1A0@crbsmxsusr04.pharma.aventis.com>

Hello,

I'm trying to reproduce some SAS result wit R (after I got suspicious with the result in R). I struggle with the contrasts in a linear model.

I've got three factors

> d$dose <- as.factor(d$dose)   # 5 levels
> d$time <- as.factor(d$time)   # 2 levels
> d$batch <- as.factor(d$batch) # 3 levels

the data frame d contains 82 rows. There are 2 to 4 replicates of each dose within each time point and each batch. There's one dose completely missing from one batch.

I then generate Dunnett contrasts using the multicomp library:

> contrasts(d$dose) <- contr.Dunnett(levels(d$dose), 1)
> contrasts(d$time) <- contr.Dunnett(levels(d$time), 1)
> contrasts(d$batch) <- contr.Dunnett(levels(d$batch), 1)

For the moment I'm just looking at the dose effects of the complete model:

> summary(lm(value ~ dose * time * batch, data = d))$coefficients[1:5,]
                   Estimate Std. Error     t value      Pr(>|t|)
(Intercept)      6.80211741 0.01505426 451.8399839 1.962247e-101
dose010mM-000mM -0.03454211 0.04113846  -0.8396549  4.046723e-01
dose025mM-000mM -0.01972550 0.04288981  -0.4599111  6.473607e-01
dose050mM-000mM -0.12015983 0.05356935  -2.2430704  2.886726e-02 <- significant
dose100mM-000mM  0.01252061 0.04113846   0.3043529  7.619872e-01

A collegue of mine has run the same data through a SAS program (listed below)

proc glm data = dftest;
  class dose time batch;
  model value = dose|time|batch;
  means dose / dunnett ('000mM');
  lsmeans dose /pdiff singular=1; 
run;

Giving the following p-values:
                           Pr(>|t|) 
          dose010mM-000mM  0.4047
          dose025mM-000mM  0.6474
          dose050mM-000mM  0.5745 <---
          dose100mM-000mM  0.7620

The p-values are the same expect for the one indicated.

A stripchart for the data in R shows that "dose050mM-000mM" should not be significant (it doesn't look different from e.g. "dose025mM-000mM").

Do you've any suggestions what I'm doing wrong here (assuming that I believe the SAS result)? Any hints what I can do to further analyse this problem?

	Many thanks for your help,
	+regards,

	Arne

--
Arne Muller, Ph.D.
Toxicogenomics, Aventis Pharma
arne dot muller domain=aventis com



From tpapp at axelero.hu  Tue Apr 20 15:39:07 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Tue, 20 Apr 2004 15:39:07 +0200
Subject: [R] loading into a list
Message-ID: <20040420133907.GA970@localhost>

I have the following problem.

Use the same algorithm (with different parameters) to generate
simulation results.  I store these in variables, eg A, B, C, which I
save into a file with

save(A, B, C, file="solution001.Rdata")

I do this many times.  Then I would like to load the above, but in
such a manner that A, B, C woule be part of a list, eg

sol001 <- loadIntoList("solution001.Rdata")

so that sol001 is a list with elements A, B, C.

I am looking for a way to implement the above function.  The variables
are very large and I need a lot of time to compute them, so saving &
loading the results is the only viable alternative.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From frohde_home at yahoo.com  Tue Apr 20 15:58:11 2004
From: frohde_home at yahoo.com (Fred Rohde)
Date: Tue, 20 Apr 2004 06:58:11 -0700 (PDT)
Subject: [R] specifying as.svrepdesign with odd number PSUs
In-Reply-To: <Pine.A41.4.58.0404191658421.55948@homer38.u.washington.edu>
Message-ID: <20040420135811.45260.qmail@web60108.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040420/8661e02d/attachment.pl

From ripley at stats.ox.ac.uk  Tue Apr 20 16:06:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 20 Apr 2004 15:06:25 +0100 (GMT Daylight Time)
Subject: [R] loading into a list
In-Reply-To: <20040420133907.GA970@localhost>
Message-ID: <Pine.WNT.4.44.0404201505030.2012-100000@gannet.stats.ox.ac.uk>

Try .saveRDS/.readRDS which enable you to save and load unnamed objects (in
your case list(A, B, C)).

On Tue, 20 Apr 2004, Tamas Papp wrote:

> I have the following problem.
>
> Use the same algorithm (with different parameters) to generate
> simulation results.  I store these in variables, eg A, B, C, which I
> save into a file with
>
> save(A, B, C, file="solution001.Rdata")
>
> I do this many times.  Then I would like to load the above, but in
> such a manner that A, B, C woule be part of a list, eg
>
> sol001 <- loadIntoList("solution001.Rdata")
>
> so that sol001 is a list with elements A, B, C.
>
> I am looking for a way to implement the above function.  The variables
> are very large and I need a lot of time to compute them, so saving &
> loading the results is the only viable alternative.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vito.muggeo at giustizia.it  Tue Apr 20 16:09:01 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Tue, 20 Apr 2004 16:09:01 +0200
Subject: [R] random effects structure in lme
Message-ID: <010a01c426e1$0ae392c0$5c13070a@PROCGEN>

Dear all,
In using the lme() function from the nlme package, I would like to specify a
particular correlation structure for the random effects.
For instance, for a 3 by 3 matrix, say, I am interested in assuming a
`quasi-diagonal'
matrix having zero entries in all but the first column (and the main
diagonal, of course)

Hence, given the three random effects b1 b2 and b3, I want to assume
cov(b2,b3)=0;
given four random effects b1,b2,b3,b4 it should be
cov(b2,b3)=cov(b4,b2)=cov(b4,b3)=0.

For a simple diagonal matrix I specify `random=list(id=pdDiag(~1+x1+x2))',
(id is the grouping variable), but I am not able to go on....

Please, could anyone give me any advice?

many thanks,
vito



From edd at debian.org  Tue Apr 20 16:08:56 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 20 Apr 2004 09:08:56 -0500
Subject: [R] Size of R user base
In-Reply-To: <20040420133320.GA916@localhost>
References: <MABBLJDICACNFOLGIHJOOEIAEFAA.phgrosjean@sciviews.org>
	<20040420133320.GA916@localhost>
Message-ID: <20040420140856.GA4113@sonny.eddelbuettel.com>

On Tue, Apr 20, 2004 at 03:33:20PM +0200, Tamas Papp wrote:
[...]
> data would be anonymized -- it is a matter of principle.  Note that I
> am using Debian's popularity-contest package, but that was my choice,
 
Background: popularity-contest sends (IIRC) weekly listings of which of the
over ten thousand Debian packages the reporting machine has installed. This
mechanism is anonymous, and 'double opt-in' as you need to a) install
popularity-contest and b) opt-in during its setup.  There is presumably some
self-selection at work as Debian developers will be overrepresented among
the couple of thousand reporting machines (though I suspect this to be
better now than it once was). That said, it gives some information, and our
cdrom team uses that to distribute all these packages over the binary cdroms
(more popular ones come first). 

The URL is http://popcon.debian.org (but the machine appears to be off-line
right now). The results are ordered by 'distribution' and 'section'. Most of
our many R packages are in 'math', and the last time I checked the results
were pretty good.

There is a overzealous bias-correction in popcon: it tries to count only
packages that are 'used', i.e. where stat() reports that the file has been
touched 'recently'. 

> not somebody else's.  BTW, this package would be able to give some
> statistics on how widespread R is among Debian users.
[...]

Ok, given that the machine popcon.debian.org is off-line, the result below
comes from Google's cache. The cache of the front-page indicated some 4300
or so submissions, so we're roughly at 10% of the installations for
r-base-core. Not shabby. If we were to extrapolate that to all Linux users
(I'm kidding here) we'd get truly spectacular numbers. 

Dirk


#Format
#   
#<name> is the package name;
#<inst> is the number of people who installed this package;
#<vote> is the number of people who use this package regularly;
#<old> is the number of people who installed, but don't use this package
#        regularly;
#<recent> is the number of people who upgraded this package recently;
#<no-files> is the number of people whose entry didn't contain enough
#        information (atime and ctime were 0).
#rank name                            inst  vote   old recent no-files  (maintainer)
1     bc                              4320  1646  1054  1619     1 (Dirk Eddelbuettel)             
2     dc                              3180   608  1095  1477     0 (Dirk Eddelbuettel)             
3     gnumeric                        1571   444   445   682     0 (J.h.m. Dassen)                 
4     gnuplot                         1663   388   594   680     1 (Thimo Neubauer)                
5     gcalctool                        840   242   326   272     0 (Sebastien Bacher)              
6     libgsl0                         1130   195   156   144   635 (Dirk Eddelbuettel)             
7     pdl                              830   130   624    76     0 (Henning Glawe)                 
8     grace                            466   111   276    79     0 (Torsten Werner)                
9     kmplot                           613   100   174   339     0 (Ben Burton)                    
10    octave2.1                        381    96    86   199     0 (Dirk Eddelbuettel)             
11    plotutils                        552    92   425    35     0 (Stephen Zander)                
12    kgeo                             481    90   327    64     0 (Ben Burton)                    
13    kpercentage                      588    87   166   335     0 (Ben Burton)                    
14    r-base-core                      435    87   199   149     0 (Dirk Eddelbuettel)             
15    octave2.0                        434    85   307    42     0 (Dirk Eddelbuettel)             
16    apcalc                           144    61    40    43     0 (Martin Buck)                   
17    maxima                           230    60   154    16     0 (Camm Maguire)                  
18    kig                              353    58    36   259     0 (Ben Burton)                    
19    kbruch                           342    51    28   263     0 (Ben Burton)                    
20    yorick                           317    48   245    24     0 (David H. Munro)                
21    calctool                         765    40   103    22   600 (Sebastien Bacher)              
22    wzip                             179    36   133     9     1 (Andreas Franzen)               
23    rpncalc                          111    31    70    10     0 (David Frey)                    
24    galculator                       179    30    41   108     0 (Sebastien Bacher)              
25    pari-gp                          110    26    75     9     0 (Bill Allombert)                
26    maxima-doc                       123    23    71    10    19 (Camm Maguire)                  
27    maxima-test                      108    23    71    10     4 (Camm Maguire)                  
28    xmaxima                          104    23    71    10     0 (Camm Maguire)                  
29    maxima-share                     117    22    70    10    15 (Camm Maguire)                  
30    maxima-src                       112    22    70    10    10 (Camm Maguire)                  
31    pi                                85    22    47    16     0 (Richard Kreckel)               
32    octave-forge                     102    21    49    32     0 (Dirk Eddelbuettel)             
33    saml                              95    21    72     2     0 (Not in sid)                    
34    sc                               118    21    69    28     0 (Chad Miller)                   
35    apcalc-common                     57    19     0    37     1 (Martin Buck)                   
36    geomview                          80    17    58     5     0 (Steve M. Robbins)              
37    grpn                              62    17    41     4     0 (Wartan Hachaturow)             
38    gsl-bin                           96    17    67    12     0 (Dirk Eddelbuettel)             
39    gtkgraph                          81    17    62     2     0 (Not in sid)                    
40    octave2.1-emacsen                 79    14    23    42     0 (Dirk Eddelbuettel)             
41    scigraphica                       35    14    18     3     0 (David Schleef)                 
42    jgraph                            75    13    60     1     1 (Pedro Zorzenon Neto)           
43    yacas                             87    13    65     9     0 (Gopal Narayanan)               
44    octave2.1-headers                 71    12    24    35     0 (Dirk Eddelbuettel)             
45    geg                               82    11    68     3     0 (Frederic Peters)               
46    kmatplot                          56    11    24    21     0 (Hugo Van Der Merwe)            
47    autoclass                         86    10    70     5     1 (James R. Van Zandt)            
48    xsiag                             45    10    33     2     0 (Not in sid)                    
49    axiom                             47     9    32     6     0 (Camm Maguire)                  
50    drgeo                             39     9    26     4     0 (Hilaire Fernandes)             
51    r-mathlib                         49     9    25    15     0 (Dirk Eddelbuettel)             
52    siag-common                       48     9    34     3     2 (Not in sid)                    
53    glpk                              30     8    19     3     0 (Falk Hueffner)                 
54    lp-solve                          46     8    34     4     0 (James R. Van Zandt)            
55    oleo                              44     8    31     5     0 (Mario Lang)                    
56    plotmtv                           61     8    50     3     0 (Mikael Hedin)                  
57    pspp                              39     8    26     5     0 (James R. Van Zandt)            
58    tilp                              35     8    24     3     0 (Julien Blache)                 
59    xmgr                              44     8    32     4     0 (Torsten Werner)                
60    xgraph                            69     7    55     7     0 (Barak A. Pearlmutter)          
61    xlispstat                         34     7    21     6     0 (Douglas Bates)                 
62    eukleides                         53     6    44     3     0 (Streph Treadway)               
63    gbase                             38     6    28     4     0 (Josip Rodin)                   
64    slsc                              47     6    38     3     0 (Jim Mintha)                    
65    yacas-proteus                     41     6    31     4     0 (Gopal Narayanan)               
66    gap-core                          37     5    29     3     0 (Bill Allombert)                
67    gap-libs                          37     5    29     3     0 (Bill Allombert)                
68    kseg                              50     5    42     3     0 (Steve M. Robbins)              
69    octave2.0-emacsen                 24     5    16     3     0 (Dirk Eddelbuettel)             
70    octave2.0-headers                 14     5     9     0     0 (Dirk Eddelbuettel)             
71    plplot-tcl                        38     5    17    16     0 (Rafael Laboissiere)            
72    xeukleides                        41     5    32     4     0 (Streph Treadway)               
73    acl2                              11     4     7     0     0 (Camm Maguire)                  
74    euler                             43     4    26    13     0 (Marco Presi)                   
75    freefem                           18     4    10     4     0 (Christophe Prud'homme)         
76    gretl                             17     4     6     7     0 (Dirk Eddelbuettel)             
77    libfreefem0                       19     4    10     3     2 (Christophe Prud'homme)         
78    plplot                            24     4    18     2     0 (Not in sid)                    
79    scigraphica-common                49     4    45     0     0 (Not in sid)                    
80    tsiag                             25     4    19     2     0 (Not in sid)                    
81    x48                               11     4     6     1     0 (Not in sid)                    
82    bugsx                             24     3    16     5     0 (Marcin Owsiany)                
83    dome                              26     3    20     3     0 (Streph Treadway)               
84    evolver                           16     3    10     3     0 (Adam C. Powell, Iv)            
85    freefem3d                          8     3     4     1     0 (Christophe Prud'homme)         
86    genius                             6     3     3     0     0 (Not in sid)                    
87    gerris                            12     3     8     1     0 (Marcelo E. Magallon)           
88    ginac-tools                       17     3     9     5     0 (Richard Kreckel)               
89    gmp-ecm                           23     3    20     0     0 (Laurent Fousse)                
90    kali                              30     3    13    14     0 (Colin Watson)                  
91    mcl                               14     3     9     2     0 (Joost Van Baal)                
92    multimix                          12     3     8     1     0 (James R. Van Zandt)            
93    pgapack                           24     3    19     2     0 (Andreas Franzen)               
94    python-mpi                        17     3     2    12     0 (Matthias Klose)                
95    qhull-bin                         17     3     7     7     0 (Rafael Laboissiere)            
96    quickplot                         35     3    29     3     0 (Gopal Narayanan)               
97    scigraphica-gnome                 12     3     8     1     0 (Not in sid)                    
98    gambit                            20     2    17     1     0 (Peter Van Rossum)              
99    magnus                            15     2    10     3     0 (Ben Burton)                    
100   maria                             14     2    12     0     0 (Ralf Treinen)                  
101   matwrap                           17     2    13     2     0 (Dirk Eddelbuettel)             
102   meschach                          26     2    13     2     9 (Drew Parsons)                  
103   nco                               10     2     8     0     0 (Brian Mays)                    
104   op-calculator-fb                   6     2     4     0     0 (Ivan E. Moore Ii)              
105   op-sheet-fb                        9     2     7     0     0 (Ivan E. Moore Ii)              
106   regina-normal                     17     2     4    11     0 (Ben Burton)                    
107   spline                            27     2    23     2     0 (David Frey)                    
108   xspread                           16     2    13     1     0 (Not in sid)                    
109   admesh                            11     1     9     1     0 (Kevin M. Rosenberg)            
110   aribas                            11     1     9     1     0 (Ralf Treinen)                  
111   gap-small-groups                  24     1    22     1     0 (Bill Allombert)                
112   gap-small-groups-extra            14     1    13     0     0 (Bill Allombert)                
113   lbt                               13     1    11     1     0 (Ralf Treinen)                  
114   octave2.0-staticlibs               5     1     4     0     0 (Dirk Eddelbuettel)             
115   plplot-bin                         4     1     1     2     0 (Rafael Laboissiere)            
116   r-base                           362     1     8     1   352 (Dirk Eddelbuettel)             
117   tela                              14     1     3    10     0 (Mikael Hedin)                  
118   xeuklides                          5     1     4     0     0 (Not in sid)                    
119   abacus                             5     0     5     0     0 (Not in sid)                    
120   calc                             149     0     0     0   149 (Manoj Srivastava)              
121   circlepack                        12     0    10     2     0 (Colin Watson)                  
122   cln                                3     0     2     0     1 (Not in sid)                    
123   dstool                             2     0     2     0     0 (Not in sid)                    
124   ess                               87     0     0     0    87 (Camm Maguire)                  
125   euler-doc                          6     0     0     0     6 (Marco Presi)                   
126   freefem-examples                  15     0     0     0    15 (Christophe Prud'homme)         
127   g2                                 1     0     1     0     0 (Not in sid)                    
128   gambit-doc                        20     0     0     0    20 (Peter Van Rossum)              
129   gap                               40     0     6     1    33 (Bill Allombert)                
130   gap-character-tables              14     0     0     0    14 (Bill Allombert)                
131   gap-online-help                   37     0     0     0    37 (Bill Allombert)                
132   gap-prim-groups                   23     0     0     0    23 (Bill Allombert)                
133   gap-table-of-marks                 8     0     0     0     8 (Bill Allombert)                
134   gap-trans-groups                  24     0     0     0    24 (Bill Allombert)                
135   genesis-data                      15     0     0    15     0 (Sam Hocevar)                   
136   genius-dev                         3     0     3     0     0 (Not in sid)                    
137   gmsh                               6     0     0     6     0 (Christophe Prud'homme)         
138   gnumeric-common                  773     0     0     0   773 (J.h.m. Dassen)                 
139   gnumeric-gda                      24     0     0     0    24 (Not in sid)                    
140   gnumeric-plugins-extra           215     0     0     0   215 (J.h.m. Dassen)                 
141   gnumeric-python                   25     0     0     0    25 (Not in sid)                    
142   gnuplot-mode                     100     0     0     0   100 (Ryuichi Arafune)               
143   gnuplot-x11                      142     0     0     0   142 (Thimo Neubauer)                
144   grafix                             3     0     2     1     0 (Not in sid)                    
145   gretl-common                      12     0     0     0    12 (Dirk Eddelbuettel)             
146   gretl-data                         4     0     0     0     4 (Dirk Eddelbuettel)             
147   gretl-doc                          3     0     0     0     3 (Dirk Eddelbuettel)             
148   gsl-ref-html                      41     0     0     0    41 (Dirk Eddelbuettel)             
149   gsl-ref-pdf                        4     0     0     0     4 (Not in sid)                    
150   gsl-ref-psdoc                    104     0     0     0   104 (Dirk Eddelbuettel)             
151   libfreefem                         2     0     0     0     2 (Not in sid)                    
152   maxima-emacs                      58     0     0     0    58 (Camm Maguire)                  
153   octave                           264     0     1     0   263 (Dirk Eddelbuettel)             
154   octave-ci                        181     0     0     0   181 (Dirk Eddelbuettel)             
155   octave-doc                        38     0     0     0    38 (Not in sid)                    
156   octave-epstk                      86     0     0     0    86 (Dirk Eddelbuettel)             
157   octave-htmldoc                    35     0     0     0    35 (Not in sid)                    
158   octave-matcompat                  28     0     0     0    28 (Dirk Eddelbuettel)             
159   octave-plplot                     33     0     0     0    33 (Rafael Laboissiere)            
160   octave-sp                         51     0     0     0    51 (Dirk Eddelbuettel)             
161   octave-statdataml                 16     0     0     0    16 (Rafael Laboissiere)            
162   octave2.0-htmldoc                 24     0     0     0    24 (Dirk Eddelbuettel)             
163   octave2.0-info                    31     0     0     0    31 (Dirk Eddelbuettel)             
164   octave2.1-htmldoc                151     0     0     0   151 (Dirk Eddelbuettel)             
165   octave2.1-info                   166     0     0     0   166 (Dirk Eddelbuettel)             
166   pari-extra                        47     0     0     0    47 (Bill Allombert)                
167   plplot-tcl-dev                     3     0     1     2     0 (Rafael Laboissiere)            
168   r-base-html                      337     0     0     0   337 (Dirk Eddelbuettel)             
169   r-base-latex                     336     0     0     0   336 (Dirk Eddelbuettel)             
170   r-cran-abind                      18     0     0     0    18 (Dirk Eddelbuettel)             
171   r-cran-boot                       80     0     0     0    80 (Dirk Eddelbuettel)             
172   r-cran-car                        24     0     0     0    24 (Dirk Eddelbuettel)             
173   r-cran-cluster                    79     0     0     0    79 (Dirk Eddelbuettel)             
174   r-cran-coda                       24     0     0     0    24 (Chris Lawrence)                
175   r-cran-dbi                         8     0     0     0     8 (Steffen Moeller)               
176   r-cran-design                     21     0     0     0    21 (Dirk Eddelbuettel)             
177   r-cran-effects                    19     0     0     0    19 (Dirk Eddelbuettel)             
178   r-cran-foreign                    81     0     0     0    81 (Dirk Eddelbuettel)             
179   r-cran-gtkdevice                  35     0     0     0    35 (Dirk Eddelbuettel)             
180   r-cran-hmisc                      26     0     0     0    26 (Dirk Eddelbuettel)             
181   r-cran-its                         8     0     0     0     8 (Dirk Eddelbuettel)             
182   r-cran-kernsmooth                 80     0     0     0    80 (Dirk Eddelbuettel)             
183   r-cran-lattice                    80     0     0     0    80 (Dirk Eddelbuettel)             
184   r-cran-lmtest                     21     0     0     0    21 (Dirk Eddelbuettel)             
185   r-cran-mapdata                    12     0     0     0    12 (Chris Lawrence)                
186   r-cran-maps                       13     0     0     0    13 (Chris Lawrence)                
187   r-cran-mcmcpack                   22     0     0     0    22 (Chris Lawrence)                
188   r-cran-mgcv                       79     0     0     0    79 (Dirk Eddelbuettel)             
189   r-cran-multcomp                   15     0     0     0    15 (Dirk Eddelbuettel)             
190   r-cran-mvtnorm                    15     0     0     0    15 (Dirk Eddelbuettel)             
191   r-cran-nlme                       80     0     0     0    80 (Dirk Eddelbuettel)             
192   r-cran-qtl                        13     0     0     0    13 (Steffen Moeller)               
193   r-cran-rcmdr                      19     0     0     0    19 (Dirk Eddelbuettel)             
194   r-cran-relimp                     15     0     0     0    15 (Dirk Eddelbuettel)             
195   r-cran-rgl                         4     0     0     0     4 (Dirk Eddelbuettel)             
196   r-cran-rmysql                      6     0     5     1     0 (Steffen Moeller)               
197   r-cran-rodbc                      10     0     0     0    10 (Dirk Eddelbuettel)             
198   r-cran-rpart                      81     0     0     0    81 (Dirk Eddelbuettel)             
199   r-cran-rquantlib                   8     0     0     0     8 (Dirk Eddelbuettel)             
200   r-cran-statdataml                 13     0     0     0    13 (Rafael Laboissiere)            
201   r-cran-survival                   83     0     0     0    83 (Dirk Eddelbuettel)             
202   r-cran-tkrplot                    15     0     0     0    15 (Dirk Eddelbuettel)             
203   r-cran-tseries                    16     0     0     0    16 (Dirk Eddelbuettel)             
204   r-cran-vr                         81     0     0     0    81 (Dirk Eddelbuettel)             
205   r-cran-xml                        14     0     0     0    14 (Rafael Laboissiere)            
206   r-noncran-lindsey                 25     0     0     0    25 (Chris Lawrence)                
207   r-omegahat-rgtk                    9     0     8     1     0 (Dirk Eddelbuettel)             
208   r-other-gking-zelig                3     0     0     0     3 (Chris Lawrence)                
209   r-recommended                    241     0     0     0   241 (Dirk Eddelbuettel)             
210   snappea                           11     0     9     2     0 (Ben Burton)                    
211   sppc                               9     0     4     5     0 (Mikael Hedin)                  
212   task-science                      10     0     0     0    10 (Not in sid)                    
213   umfpack-doc                        2     0     0     0     2 (Not in sid)                    
214   umfpack3                           2     0     1     0     1 (Not in sid)                    
215   umfpack3-dev                       1     0     1     0     0 (Not in sid)                    
216   yorick-gist                        2     0     1     1     0 (Not in sid)                    
------------------------------------------------------------------
216   Total                          29665  5516  9708  7546  6895                             




-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From Achim.Zeileis at wu-wien.ac.at  Tue Apr 20 15:57:13 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 20 Apr 2004 15:57:13 +0200
Subject: [R] loading into a list
In-Reply-To: <20040420133907.GA970@localhost>
References: <20040420133907.GA970@localhost>
Message-ID: <20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>

On Tue, 20 Apr 2004 15:39:07 +0200 Tamas Papp wrote:

> I have the following problem.
> 
> Use the same algorithm (with different parameters) to generate
> simulation results.  I store these in variables, eg A, B, C, which I
> save into a file with
> 
> save(A, B, C, file="solution001.Rdata")
> 
> I do this many times.  Then I would like to load the above, but in
> such a manner that A, B, C woule be part of a list, eg
> 
> sol001 <- loadIntoList("solution001.Rdata")
> 
> so that sol001 is a list with elements A, B, C.
> 
> I am looking for a way to implement the above function. 

If your objects are always called A, B, and C, the following should
work:

loadIntoList <- function(file) {
  load(file)
  return(list(A = A, B = B, C = C))
}

hth,
Z

> The variables
> are very large and I need a lot of time to compute them, so saving &
> loading the results is the only viable alternative.
> 
> Thanks,
> 
> Tamas
> 
> -- 
> Tam??s K. Papp
> E-mail: tpapp at axelero.hu
> Please try to send only (latin-2) plain text, not HTML or other
> garbage.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Tue Apr 20 16:15:55 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 20 Apr 2004 10:15:55 -0400
Subject: [R] loading into a list
In-Reply-To: <20040420133907.GA970@localhost>
References: <20040420133907.GA970@localhost>
Message-ID: <4085309B.3080106@jhsph.edu>

I can think of two possible solutions.  One is to load the .Rdata file 
into an environment and then use the $ and [[ operators to access 
elements.

 > x <- 1; y <- 2
 > save(x, y, file = "xy.RData")
 > e <- new.env()
 > load("xy.RData", envir = e)
 > rm(x, y)
 > e$x
[1] 1
 > e$y
[1] 2

Another way, if you really want a list, is to load into an environment 
(or the workspace) and use mget,

 > e <- new.env()
 > load("xy.RData", envir = e)
 > mget(c("x", "y"), e)
$x
[1] 1

$y
[1] 2

Both of these only work in R >= 1.9.0.

-roger

Tamas Papp wrote:
> I have the following problem.
> 
> Use the same algorithm (with different parameters) to generate
> simulation results.  I store these in variables, eg A, B, C, which I
> save into a file with
> 
> save(A, B, C, file="solution001.Rdata")
> 
> I do this many times.  Then I would like to load the above, but in
> such a manner that A, B, C woule be part of a list, eg
> 
> sol001 <- loadIntoList("solution001.Rdata")
> 
> so that sol001 is a list with elements A, B, C.
> 
> I am looking for a way to implement the above function.  The variables
> are very large and I need a lot of time to compute them, so saving &
> loading the results is the only viable alternative.
> 
> Thanks,
> 
> Tamas
>



From p.dalgaard at biostat.ku.dk  Tue Apr 20 16:14:00 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2004 16:14:00 +0200
Subject: [R] loading into a list
In-Reply-To: <Pine.WNT.4.44.0404201505030.2012-100000@gannet.stats.ox.ac.uk>
References: <Pine.WNT.4.44.0404201505030.2012-100000@gannet.stats.ox.ac.uk>
Message-ID: <x2u0ze4tzr.fsf@biostat.ku.dk>

Prof Brian D Ripley <ripley at stats.ox.ac.uk> writes:

> Try .saveRDS/.readRDS which enable you to save and load unnamed objects (in
> your case list(A, B, C)).

Or, consider whether an environment wouldn't suit your purposes just as
well as a list:

e <- new.env()
load("foo.Rdata",e)
ls(e)
e$A 

..etc...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Stephan.Matthiesen at ed.ac.uk  Tue Apr 20 16:18:20 2004
From: Stephan.Matthiesen at ed.ac.uk (Stephan Matthiesen)
Date: Tue, 20 Apr 2004 15:18:20 +0100
Subject: [R] Size of R user base
In-Reply-To: <4084169F.8010401@ksvanhorn.com>
References: <4084169F.8010401@ksvanhorn.com>
Message-ID: <200404201518.20562.Stephan.Matthiesen@ed.ac.uk>

Hello,

Am Montag, 19. April 2004 19:12 schrieb Kevin S. Van Horn:
> 1. It appears that there are about 100,000 S-PLUS users.

Let me add one thought that I haven't seen in the discussion yet:

To estimate the user base, one should perhaps not only look at S-Plus for 
comparison. R can be used for many other things than statistics. The users I 
know (physicists and geoscientists) use it for quite different purpose (e.g. 
numerically solving ODEs) for which they might have used Matlab instead, or 
they use it for plotting (as a replacement of IDL), whereas they may only 
need the most basic statistical features.

I could imagine that uses outside "generic" statistics will become more common 
as more packages are added and R becomes more widely known. 

Perhaps these users are also less likely to buy books on R (which are probably 
more geared towards statistics), and perhaps also less likely to subscribe to 
the mailing list.

However, I have no idea how to estimate the number of these users.... It seems 
to me that the number of downloads is most direct. What about the number of 
package updates? A package update (from within R) requires a working 
installation, and also a user who actually starts the update. And I suspect 
that even users who get it on CD would do an internet package update every 
now and again. Might be better than counting the number of downloads of the 
program itself.

Cheers
Stephan



From rpeng at jhsph.edu  Tue Apr 20 16:18:21 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 20 Apr 2004 10:18:21 -0400
Subject: [R] loading into a list
In-Reply-To: <20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>
References: <20040420133907.GA970@localhost>
	<20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <4085312D.3060504@jhsph.edu>

Or maybe

loadIntoList <- function(filename) {
	e <- new.env()
	load(filename, envir = e)
	mget(ls(e), envir = e)
}

-roger

Achim Zeileis wrote:
> On Tue, 20 Apr 2004 15:39:07 +0200 Tamas Papp wrote:
> 
> 
>>I have the following problem.
>>
>>Use the same algorithm (with different parameters) to generate
>>simulation results.  I store these in variables, eg A, B, C, which I
>>save into a file with
>>
>>save(A, B, C, file="solution001.Rdata")
>>
>>I do this many times.  Then I would like to load the above, but in
>>such a manner that A, B, C woule be part of a list, eg
>>
>>sol001 <- loadIntoList("solution001.Rdata")
>>
>>so that sol001 is a list with elements A, B, C.
>>
>>I am looking for a way to implement the above function. 
> 
> 
> If your objects are always called A, B, and C, the following should
> work:
> 
> loadIntoList <- function(file) {
>   load(file)
>   return(list(A = A, B = B, C = C))
> }
> 
> hth,
> Z
> 
> 
>>The variables
>>are very large and I need a lot of time to compute them, so saving &
>>loading the results is the only viable alternative.
>>
>>Thanks,
>>
>>Tamas
>>
>>-- 
>>Tam??s K. Papp
>>E-mail: tpapp at axelero.hu
>>Please try to send only (latin-2) plain text, not HTML or other
>>garbage.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tpapp at axelero.hu  Tue Apr 20 16:25:27 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Tue, 20 Apr 2004 16:25:27 +0200
Subject: [R] loading into a list
In-Reply-To: <20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>
References: <20040420133907.GA970@localhost>
	<20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <20040420142527.GA1320@localhost>

On Tue, Apr 20, 2004 at 03:57:13PM +0200, Achim Zeileis wrote:

> On Tue, 20 Apr 2004 15:39:07 +0200 Tamas Papp wrote:
> 
> > I have the following problem.
> > 
> > Use the same algorithm (with different parameters) to generate
> > simulation results.  I store these in variables, eg A, B, C, which I
> > save into a file with
> > 
> > save(A, B, C, file="solution001.Rdata")
> > 
> > I do this many times.  Then I would like to load the above, but in
> > such a manner that A, B, C woule be part of a list, eg
> > 
> > sol001 <- loadIntoList("solution001.Rdata")
> > 
> > so that sol001 is a list with elements A, B, C.
> > 
> > I am looking for a way to implement the above function. 
> 
> If your objects are always called A, B, and C, the following should
> work:
> 
> loadIntoList <- function(file) {
>   load(file)
>   return(list(A = A, B = B, C = C))
> }

Sorry for not mentioning that, but variables sometimes have different
names.  I am looking for something like this:

loadIntoList <- function(file) {
  load(file)
  v <- list()
  ## for each variable name 'i' in ls(), assign 
  ## v$i <- i
  ## but I don't know how to write this...  I need help in that.
}

Professor Ripley mentioned save/readRDS, I have looked into that but I
need the variable names too.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From Emilie.Haon-Lasportes at beaulieu.rennes.inra.fr  Tue Apr 20 16:29:06 2004
From: Emilie.Haon-Lasportes at beaulieu.rennes.inra.fr (Emilie Haon.Lasportes)
Date: Tue, 20 Apr 2004 16:29:06 +0200
Subject: [R] import Excel file ?
Message-ID: <5.0.2.1.2.20040420162719.00b00620@beaulieu.rennes.inra.fr>

Good afternoon,

I would like to know how can I import Excel files in R ? Do I have to 
transform them in text files (.txt) ?
Thank you

Emilie Haon-Lasportes



From sway at tanox.com  Tue Apr 20 16:44:27 2004
From: sway at tanox.com (Shawn Way)
Date: Tue, 20 Apr 2004 09:44:27 -0500
Subject: [R] import Excel file ?
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF308805DEAE@houston.tanox.net>

If you want to just import the data and go, saving as a .csv file and
using read.csv (something like this) is the way to go.

If you are using a real-time file (open) or using a large excel file,
you may want to look at RODBC.  While this is a little harder to figure
out, it allows one to use SQL queries to retrieve the data needed from
excel and prevent overburdening R with data.

Shawn Way, PE
Engineering Manager
sway at tanox.com


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Emilie
Haon.Lasportes
Sent: Tuesday, April 20, 2004 9:29 AM
To: R-help at stat.math.ethz.ch
Subject: [R] import Excel file ?


Good afternoon,

I would like to know how can I import Excel files in R ? Do I have to 
transform them in text files (.txt) ?
Thank you

Emilie Haon-Lasportes

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Bernhard.Pfaff at drkw.com  Tue Apr 20 16:48:32 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 20 Apr 2004 16:48:32 +0200
Subject: [R] import Excel file ?
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29B8DD@ibfftce505.is.de.dresdnerkb.com>

> 
> Good afternoon,
> 
> I would like to know how can I import Excel files in R ? Do I have to 
> transform them in text files (.txt) ?
> Thank you

Hello Emilie,

two possibilities come to my mind:

1) Save your execl file as csv and use read.csv()

2) Use the contributed package "RODBC"
library(RODBC)
chan <- odbcConnectExcel("Path to your Excel file")
your.df <- sqlFetch(chan, "name of your data sheet")
close(chan)

and of course, last but not least, consult the manual: R Data Import/Export

HTH,
Bernhard
> 
> Emilie Haon-Lasportes
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From phgrosjean at sciviews.org  Tue Apr 20 17:02:44 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 20 Apr 2004 17:02:44 +0200
Subject: [R] Size of R user base
In-Reply-To: <20040420133320.GA916@localhost>
Message-ID: <MABBLJDICACNFOLGIHJOAEIKEFAA.phgrosjean@sciviews.org>

>On Tue, Apr 20, 2004 at 10:47:06AM +0200, Philippe Grosjean wrote:

>> The main role of this code would be, of course, to warn users that an
update
>> is available. One side effect would be that it should be possible to
monitor
>> access to the link in the centralized web site and to know:

Tamas K. Papp answered:

>I appreciate that it would be really nice to know the size of the user
>base: the fact that lots of people are using R would give everybody a
>warm, fuzzy feeling.

OK, as I said in my mail, the goal is different, but anyway...

>On the other hand, some people are getting carried away with this
>discussion.  I would consider the above suggestion a serious violation
>of my privacy: if I want to make this type of personal data (what
>software you use is personal data, in the legal sense) public, I can
>do it, but I don't want any software on my computer which "kindly"
>does that for me if I did not ask for it.  I don't care whether the
>data would be anonymized -- it is a matter of principle.

You have strong principles, excellent position probably for saving Freedom.
However, do you really apply them? If yes, it means you never use your web
browser except with https:// addresses (or even not at all), because you
could say the same thing about the web: everybody can track how you use it.
The fact is that you use internet (because you posted this mail). So what?
You accept the architecture of the web and the way it operates with no
confidentiality at all, except in secure sites, and you violently protest
against doing the same thing for software usage?

That said, and after thinking a while, yes, what's wrong in what I propose
is that the piece of code does not ask to the user if it can track its
use... a subtle, but important difference! However, think at the following
story. Do you know that voting in Belgium is not a right, but an
*obligation*. Why do you *have to* vote in Belgium (you would said, this is
my freedom: if I do not want to vote, I don't... then you can be jailed
according to Belgian's law)? The reason is that if vote is not obligatory,
only a fraction of people do vote, and there is a bias in the results (for
instance, because extremists tend to vote more than others). So, what's the
matter to ask people that use R to accept the lost of a microscopic fraction
of their Freedom in sending such a basic information as "I use it".

>So before people are coming up with ingenious (and possibly invasive)
>methods to estimate the user base, I would like somebody to explain
>why this is relevant.  R already has the "critical mass" it needs,
>given that it is a specialized piece of software.  It is always good
>to have a broader user base, but R is doing fine as it is.  In some
>markets, it is safer to pick a product with a wider user base (think
>VHS vs Betamax), and sometimes this argument applies to software (eg
>you should not make your bet on a commercial, closed-source software
>company with a dwindling user base, because if they go bankrupt, you
>can expect no updates etc).  But free software is entirely different.

No software is eternel. I think a good picture of the change in time of R
users would be an interesting tool to have an idea on how well it fits or
don't fit users' needs. Moreover, if you develop for both R and S-PLUS, and
think to develop only for one software only in the future, it would be nice
to know what fraction of the "S language and environment" users you miss
this way.

Best,

Philippe Grosjean



From dmurdoch at pair.com  Tue Apr 20 17:06:52 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 20 Apr 2004 11:06:52 -0400
Subject: [R] Size of R user base
In-Reply-To: <LAW10-OE50vPOJD0xIk00004323@hotmail.com>
References: <Pine.WNT.4.44.0404201350370.4072-100000@gannet.stats.ox.ac.uk>
	<LAW10-OE50vPOJD0xIk00004323@hotmail.com>
Message-ID: <avea80p2imlr8lacvn5e7op23l524ktb49@4ax.com>

On Tue, 20 Apr 2004 08:37:44 -0500, "Mihai Nica" <m_nica at hotmail.com>
wrote:

>Why not ask those downloading rw1091 to complete (at their choice) a small
>survey (including questions such as 'will it be used by a group or an
>individual'). It would provide some data that could lead to an estimate as
>good as anything else.

Yes, that sounds like a good idea.  That's more or less how People
Magazine's 1998 survey to find the Most Beautiful People in the World
was designed.  You might recall that "Hank, the ugly drunken dwarf"
was the winner.

Duncan Murdoch



From d.stasinopoulos at londonmet.ac.uk  Tue Apr 20 17:12:07 2004
From: d.stasinopoulos at londonmet.ac.uk (Dimitrios Stasinopoulos)
Date: Tue, 20 Apr 2004 16:12:07 +0100
Subject: [R] Creating a package in R 1.9.0
Message-ID: <40853DC7.553D626D@londonmet.ac.uk>

Dear all

I am trying to create a package in R 1.9.0  and I a getting an
error message which I do not understand. (I am using R in Windows
XP and 2000)

For example the following works well in 1.8.1

 C:\Program Files\R\rw1081\src\gnuwin32>make pkg-gamlss

---------- Making package gamlss ------------
  adding build stamp to DESCRIPTION
  installing inst files
  installing indices
  not zipping data
  installing help
 >>> Building/Updating help pages for package 'gamlss'
     Formats: text html latex example
  adding MD5 sums

but not in 1.9.0

 C:\Program Files\R\rw1090\src\gnuwin32>make pkg-gamlss

---------- Making package gamlss ------------
  adding build stamp to DESCRIPTION
  installing inst files
FIND: Parameter format not correct make[2]: ***
[C:/PROGRA~1/R/rw1090/library/gamlss/inst] Error 2 make[1]: ***
[all] Error 2 make: *** [pkg-gamlss] Error 2

Can anyone help?

Thanks

Mikis



From ggrothendieck at myway.com  Tue Apr 20 17:13:13 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 15:13:13 +0000 (UTC)
Subject: [R] import Excel file ?
References: <5.0.2.1.2.20040420162719.00b00620@beaulieu.rennes.inra.fr>
Message-ID: <loom.20040420T170936-376@post.gmane.org>

Emilie Haon.Lasportes <Emilie.Haon-Lasportes <at> beaulieu.rennes.inra.fr> 
writes:
> I would like to know how can I import Excel files in R ? Do I have to 
> transform them in text files (.txt) ?

The easiest way for one time use is to select the data in Excel, 
press ctrl-c to copy it to the clipboard and 
then use the following R command:

   read.table("clipboard", header=T)

(assuming the first selected line is the header).



From james at staarfunds.com  Tue Apr 20 17:16:21 2004
From: james at staarfunds.com (Jim Thomas)
Date: Tue, 20 Apr 2004 11:16:21 -0400
Subject: [R] openMosix and R:  File I/O issues?
In-Reply-To: <200404201002.i3KA0NZ0023948@hypatia.math.ethz.ch>
References: <200404201002.i3KA0NZ0023948@hypatia.math.ethz.ch>
Message-ID: <40853EC5.6030605@staarfunds.com>

Hi there,

We're attempting to run an LVQ analysis over a cluster of machines via R 
and openMosix.  R spawns several child processes simply by writing 
commands to several files and using system() to start a slave process. 
 The processes migrate perfectly, and often finish with no reported 
errors, writing their results into respective files for the parent 
process to piece together.  However, occasionally we have had the 
problem that the results from a child process never make it into a file. 
 The process finishes, and exits, with no errors - but the file never 
turns up.  Repeated tests with the same data have shown that the 
specific process that dies is random, and stress tests of R I/O have 
shown that there are no issues there.  Does anyone know of I/O issues 
with openMosix, either specifically related to R or not?

using:
openMosix kernel 2.4.21
R 1.8.1
RedHat Enterprise Edition

Thanks,
Jim



From ripley at stats.ox.ac.uk  Tue Apr 20 17:18:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 20 Apr 2004 16:18:28 +0100 (BST)
Subject: [R] Creating a package in R 1.9.0
In-Reply-To: <40853DC7.553D626D@londonmet.ac.uk>
Message-ID: <Pine.LNX.4.44.0404201615340.14900-100000@gannet.stats>

Note the capitalized FIND, which is a Windows command.  See file
readme.packages, which says

  To avoid frustration, please
  use our tool set, and make sure it is at the front of your path
  (including beofre the Windows system directories).

I believe you haven't done so.


On Tue, 20 Apr 2004, Dimitrios Stasinopoulos wrote:

> Dear all
> 
> I am trying to create a package in R 1.9.0  and I a getting an
> error message which I do not understand. (I am using R in Windows
> XP and 2000)
> 
> For example the following works well in 1.8.1
> 
>  C:\Program Files\R\rw1081\src\gnuwin32>make pkg-gamlss
> 
> ---------- Making package gamlss ------------
>   adding build stamp to DESCRIPTION
>   installing inst files
>   installing indices
>   not zipping data
>   installing help
>  >>> Building/Updating help pages for package 'gamlss'
>      Formats: text html latex example
>   adding MD5 sums
> 
> but not in 1.9.0
> 
>  C:\Program Files\R\rw1090\src\gnuwin32>make pkg-gamlss
> 
> ---------- Making package gamlss ------------
>   adding build stamp to DESCRIPTION
>   installing inst files
> FIND: Parameter format not correct make[2]: ***
> [C:/PROGRA~1/R/rw1090/library/gamlss/inst] Error 2 make[1]: ***
> [all] Error 2 make: *** [pkg-gamlss] Error 2
> 
> Can anyone help?
> 
> Thanks
> 
> Mikis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Luisr at frs.fo  Tue Apr 20 17:20:27 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Tue, 20 Apr 2004 16:20:27 +0100
Subject: [R] R1.9.0
Message-ID: <s0854dd6.053@ffdata.setur.fo>

I have just installed R 1.9.0 on my computer(Windows XP) and uninstalled 1.8.1

When Itry to execute .RData I'm asked to specify the program to open it.But whe I do it I get a message ".RData is not a valid Win32 program"

Can anyone help please?

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From p.dalgaard at biostat.ku.dk  Tue Apr 20 17:21:12 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Apr 2004 17:21:12 +0200
Subject: [R] import Excel file ?
In-Reply-To: <loom.20040420T170936-376@post.gmane.org>
References: <5.0.2.1.2.20040420162719.00b00620@beaulieu.rennes.inra.fr>
	<loom.20040420T170936-376@post.gmane.org>
Message-ID: <x2llkq4qvr.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> Emilie Haon.Lasportes <Emilie.Haon-Lasportes <at> beaulieu.rennes.inra.fr> 
> writes:
> > I would like to know how can I import Excel files in R ? Do I have to 
> > transform them in text files (.txt) ?
> 
> The easiest way for one time use is to select the data in Excel, 
> press ctrl-c to copy it to the clipboard and 
> then use the following R command:
> 
>    read.table("clipboard", header=T)
> 
> (assuming the first selected line is the header).

If you have empty cells, I rather strongly suspect that you want
read.delim() in there (or read.delim2() for the locale-challenged).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Tue Apr 20 17:27:43 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 20 Apr 2004 11:27:43 -0400
Subject: [R] Creating a package in R 1.9.0
In-Reply-To: <40853DC7.553D626D@londonmet.ac.uk>
References: <40853DC7.553D626D@londonmet.ac.uk>
Message-ID: <41ga80trqjeotcijrsn6tg5spa2rahjfdg@4ax.com>

On Tue, 20 Apr 2004 16:12:07 +0100, Dimitrios Stasinopoulos
<d.stasinopoulos at londonmet.ac.uk> wrote:

>Dear all
>
>I am trying to create a package in R 1.9.0  and I a getting an
>error message which I do not understand. (I am using R in Windows
>XP and 2000)
>
>For example the following works well in 1.8.1
>
> C:\Program Files\R\rw1081\src\gnuwin32>make pkg-gamlss

That's not the normal way to build a user's package, that's the way to
build one of the packages in the R distribution.  It should probably
work, only if you have the full setup to build R itself.

Can you try

R CMD INSTALL gamlss

while you are in the parent directory of the package?

>FIND: Parameter format not correct make[2]: ***
>[C:/PROGRA~1/R/rw1090/library/gamlss/inst] Error 2 make[1]: ***
>[all] Error 2 make: *** [pkg-gamlss] Error 2

My guess would be that you're not putting the R tools first in your
path, so it's finding some incorrect version of one of them.  See
"readme.packages" for full directions.

Duncan Murdoch



From JoannW at usca.edu  Tue Apr 20 17:41:26 2004
From: JoannW at usca.edu (Joann Williamson)
Date: Tue, 20 Apr 2004 11:41:26 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
Message-ID: <F9D084AF922BC24AA87F674DE1E6850E3BAA03@exchange.usca.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040420/b8ab8058/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Apr 20 17:43:58 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 20 Apr 2004 17:43:58 +0200
Subject: [R] R1.9.0
In-Reply-To: <s0854dd6.053@ffdata.setur.fo>
References: <s0854dd6.053@ffdata.setur.fo>
Message-ID: <4085453E.4040900@statistik.uni-dortmund.de>

Luis Rideau Cruz wrote:

> I have just installed R 1.9.0 on my computer(Windows XP) and uninstalled 1.8.1
> 
> When Itry to execute .RData I'm asked to specify the program to open it.But whe I do it I get a message ".RData is not a valid Win32 program"
> 
> Can anyone help please?



You just need to fix your registry as follows:


[HKEY_CLASSES_ROOT\.RData]
@="RData_auto_file"

[HKEY_CLASSES_ROOT\RData_auto_file]
@=""

[HKEY_CLASSES_ROOT\RData_auto_file\shell]

[HKEY_CLASSES_ROOT\RData_auto_file\shell\open]
@=""

[HKEY_CLASSES_ROOT\RData_auto_file\shell\open\command]
@="\"c:\\Program Files\\R\\rw1090\\bin\\RGui.exe\" \"%1\""


Of course, you need to specify the correct path here ...


The Setup Wizard does that job for you, but having uninstalled the old 
version *after* installing a new one has deleted the keys.

Uwe Ligges








> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From zwu at jhsph.edu  Tue Apr 20 17:47:17 2004
From: zwu at jhsph.edu (Zhijin Wu)
Date: Tue, 20 Apr 2004 11:47:17 -0400 (EDT)
Subject: [R] compile Fortran code which calls C subroutine
Message-ID: <Pine.GSO.4.10.10404201147050.18019-100000@athena.biostat.jhsph.edu>


      I used "R CMD SHLIB" to compile the fortran filename.f file and the
    filename.so is generated. But since in filename.f it calls
    another subroutine written in C, i had problem in "dyn.load" because it
    could not find the C subroutine. 
     I have the .c file but don't know how to tell R about it.
      How should I compile when I want to call the fortran function in R 
which  calls (FORTRAN-callable) C-code?
      Thanks a lot!
    best regards,
    Zhijin



From mike.campana at freesurf.ch  Tue Apr 20 18:50:48 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Tue, 20 Apr 2004 17:50:48 +0100
Subject: [R] import Excel file ?
Message-ID: <1082476248.webexpressdV3.1.f@smtp.freesurf.ch>

You can also find different examples in the manual R for Beginners by 
Emmanuel Paradis (that you can easily download from the internet)

under http://zoonek2.free.fr/UNIX/48_R/all.html you find a lot of 
information about R in french, which is also very useful

Mike

---


---


---


---



From gregory_r_warnes at groton.pfizer.com  Tue Apr 20 17:54:36 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Tue, 20 Apr 2004 11:54:36 -0400
Subject: [R] [JOB] Toxicogenomics Statistician
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C52128D@groexmb02.pfizer.com>


> Toxicogenomics Statistician
> 
> The successful candidate will provide statistical support to nonclinical
> areas of R&D, especially in the areas of investigative toxicology, safety
> biomarkers, and toxicogenomics.  The candidate will collaborate with
> scientists to plan meaningful experimental studies, statistically analyze
> the results of those and other studies, and communicate results. Further,
> the candidate will work with programmers to create tools that facilitate
> use of advanced statistical methodology by Pfizer scientists. 
> 
> A minimum of an M.S. - Statistics is required. A PhD - Statistics is
> desired. Two years statistical consulting experience, preferably in
> pharmaceutical environment using gene expression microarrays,  is
> required. Experience with other -omics technologies, such as RTPCR,
> Protein Mass-Spec, etc, is a strong plus. A Strong science background,
> with good working knowledge of biology, chemistry or pharmacology is also
> desired. Further, good computational skills in SAS, S-plus or R and good
> communication skills (written, oral presentation) are necessary.
> 
> Other desirable attributes include: 
> 
> Several years statistical experience in a scientific environment, with
> some pharmaceutical experience 
> Outstanding communication skills 
> Ability to balance many projects and clients simultaneously 
> Works independently, with minimal supervision 
> Learns new skills quickly 
> Good team player 
> Strong statistical knowledge in general linear models, experimental
> design, probability, categorical data analysis 
> Ability to identify and understand relevant scientific literature and
> apply knowledge gained from these sources 
> Ability to identify/develop and apply new quantitative methods as needed
> to solve problems in pharmaceutical research 
> Level will be determined by candidate's background and qualifications. 
> 
> 
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From arcane at arcanemethods.com  Tue Apr 20 17:58:24 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Tue, 20 Apr 2004 08:58:24 -0700
Subject: [R] Size of R user base
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7C2C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7C2C@usrymx25.merck.com>
Message-ID: <408548A0.3020505@arcanemethods.com>



Liaw, Andy wrote:

> How about those poor students who don't know how lucky they are to have
> instructors forcing R upon them for a course?  I'd bet they are very
> unlikely to subscribe to the list(s).  Although I don't know if one would
> want to include them as `R users'...
> 

I certainly would, and as the most important users of the
bunch in terms of their propensity and ability to propegate it.

Maybe it's time for a comp.lang.r usenet group (and 
application subtree).


Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From rossini at blindglobe.net  Tue Apr 20 18:02:03 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 20 Apr 2004 09:02:03 -0700
Subject: [R] Re: [openMosix-general] openMosix and R:  File I/O issues?
In-Reply-To: <40853EC5.6030605@staarfunds.com> (Jim Thomas's message of
	"Tue, 20 Apr 2004 11:16:21 -0400")
References: <200404201002.i3KA0NZ0023948@hypatia.math.ethz.ch>
	<40853EC5.6030605@staarfunds.com>
Message-ID: <85smeyvds4.fsf@servant.blindglobe.net>


Other than OpenMOSIX disliking I/O, I havn't seen any problems.  I
would suspect network hardware and drivers -- while generally robust, cluster
computations stress it much more than daily use, with less tolerance
for problems.

best,
-tony


Jim Thomas <james at staarfunds.com> writes:

> Hi there,
>
> We're attempting to run an LVQ analysis over a cluster of machines via
> R and openMosix.  R spawns several child processes simply by writing
> commands to several files and using system() to start a slave
> process. The processes migrate perfectly, and often finish with no
> reported errors, writing their results into respective files for the
> parent process to piece together.  However, occasionally we have had
> the problem that the results from a child process never make it into a
> file. The process finishes, and exits, with no errors - but the file
> never turns up.  Repeated tests with the same data have shown that the
> specific process that dies is random, and stress tests of R I/O have
> shown that there are no issues there.  Does anyone know of I/O issues
> with openMosix, either specifically related to R or not?
>
> using:
> openMosix kernel 2.4.21
> R 1.8.1
> RedHat Enterprise Edition
>
> Thanks,
> Jim
>
>
>
> -------------------------------------------------------
> This SF.Net email is sponsored by: IBM Linux Tutorials
> Free Linux tutorial presented by Daniel Robbins, President and CEO of
> GenToo technologies. Learn everything from fundamentals to system
> administration.http://ads.osdn.com/?ad_id=1470&alloc_id=3638&op=click
> _______________________________________________
> openMosix-general mailing list
> openMosix-general at lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/openmosix-general
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From sfalcon at fhcrc.org  Tue Apr 20 18:17:15 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 20 Apr 2004 09:17:15 -0700
Subject: [R] loading into a list
In-Reply-To: <20040420142527.GA1320@localhost>
References: <20040420133907.GA970@localhost>
	<20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>
	<20040420142527.GA1320@localhost>
Message-ID: <20040420161714.GA22820@queenbee.fhcrc.org>

On Tue, Apr 20, 2004 at 04:25:27PM +0200, Tamas Papp wrote:
> Sorry for not mentioning that, but variables sometimes have different
> names.  I am looking for something like this:
> 
> loadIntoList <- function(file) {
>   load(file)
>   v <- list()
>   ## for each variable name 'i' in ls(), assign 
>   ## v$i <- i
>   ## but I don't know how to write this...  I need help in that.
> }

This might be helpful...

loadIntoList <- function(file) {
  load(file)
  v <- list()
  for (i in ls()) {
      v[[i]] <- eval(as.name(i))
  }
}

The ls() may give you too much, however...

+ seth



From dmurdoch at pair.com  Tue Apr 20 18:35:04 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 20 Apr 2004 12:35:04 -0400
Subject: [R] compile Fortran code which calls C subroutine
In-Reply-To: <Pine.GSO.4.10.10404201147050.18019-100000@athena.biostat.jhsph.edu>
References: <Pine.GSO.4.10.10404201147050.18019-100000@athena.biostat.jhsph.edu>
Message-ID: <t6ka80t1bdnplv60d8felh09ju2the9ple@4ax.com>

On Tue, 20 Apr 2004 11:47:17 -0400 (EDT), Zhijin Wu <zwu at jhsph.edu>
wrote:

>      I used "R CMD SHLIB" to compile the fortran filename.f file and the
>    filename.so is generated. But since in filename.f it calls
>    another subroutine written in C, i had problem in "dyn.load" because it
>    could not find the C subroutine. 
>     I have the .c file but don't know how to tell R about it.
>      How should I compile when I want to call the fortran function in R 
>which  calls (FORTRAN-callable) C-code?

According to the help for R CMD SHLIB, this should work:

R CMD SHLIB filename.f other.c

Duncan Murdoch



From amurta at ipimar.pt  Tue Apr 20 19:58:53 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Tue, 20 Apr 2004 17:58:53 +0000
Subject: [R] Size of R user base
In-Reply-To: <MABBLJDICACNFOLGIHJOAEIKEFAA.phgrosjean@sciviews.org>
References: <MABBLJDICACNFOLGIHJOAEIKEFAA.phgrosjean@sciviews.org>
Message-ID: <200404201758.55396.amurta@ipimar.pt>

On Tuesday 20 April 2004 15:02, Philippe Grosjean wrote:

> No software is eternel. I think a good picture of the change in time of R
> users would be an interesting tool to have an idea on how well it fits or
> don't fit users' needs. Moreover, if you develop for both R and S-PLUS, and
> think to develop only for one software only in the future, it would be nice
> to know what fraction of the "S language and environment" users you miss
> this way.

I think this kind of marketing concerns don't have much to do with a 
cooperative software project like R. There is a direct way of knowing user's 
needs -- and that is subscribing to this list. 
If R is going to become obsolete one day, it will probably more because of 
lack of developers than because of lack of users, and there isn't always a 
linear relation between these two.


-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From ernesto at ipimar.pt  Tue Apr 20 19:05:49 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 20 Apr 2004 18:05:49 +0100
Subject: [R] How to debug S4 method
Message-ID: <1082480749.4397.37.camel@gandalf.local>

Hi,

I'm starting with S4 ...

I wrote a simple generic function and one method "catch". Now I want to
debug the method using "debug" and when I call debug(catch) it tries to
debug the generic function not the method (as expected).

How can I call debug on the method ?

Thanks

EJ



From rpeng at jhsph.edu  Tue Apr 20 19:01:08 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 20 Apr 2004 13:01:08 -0400
Subject: [R] compile Fortran code which calls C subroutine
In-Reply-To: <Pine.GSO.4.10.10404201147050.18019-100000@athena.biostat.jhsph.edu>
References: <Pine.GSO.4.10.10404201147050.18019-100000@athena.biostat.jhsph.edu>
Message-ID: <40855754.9080804@jhsph.edu>

What happens if you do

R CMD SHLIB filename1.f filename2.c

Does that work?

-roger

Zhijin Wu wrote:
>       I used "R CMD SHLIB" to compile the fortran filename.f file and the
>     filename.so is generated. But since in filename.f it calls
>     another subroutine written in C, i had problem in "dyn.load" because it
>     could not find the C subroutine. 
>      I have the .c file but don't know how to tell R about it.
>       How should I compile when I want to call the fortran function in R 
> which  calls (FORTRAN-callable) C-code?
>       Thanks a lot!
>     best regards,
>     Zhijin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From m_nica at hotmail.com  Tue Apr 20 19:11:01 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Tue, 20 Apr 2004 12:11:01 -0500
Subject: [R] Size of R user base
References: <Pine.WNT.4.44.0404201350370.4072-100000@gannet.stats.ox.ac.uk>
	<LAW10-OE50vPOJD0xIk00004323@hotmail.com>
	<avea80p2imlr8lacvn5e7op23l524ktb49@4ax.com>
Message-ID: <LAW10-OE51LTekBw7Hz00024708@hotmail.com>

Well, it's a matter of taste. However, both the topic and the target group
would be totally different. Hopefully :-).

----- Original Message ----- 
From: "Duncan Murdoch" <dmurdoch at pair.com>
To: "Mihai Nica" <m_nica at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 20, 2004 10:06 AM
Subject: Re: [R] Size of R user base


> On Tue, 20 Apr 2004 08:37:44 -0500, "Mihai Nica" <m_nica at hotmail.com>
> wrote:
>
> >Why not ask those downloading rw1091 to complete (at their choice) a
small
> >survey (including questions such as 'will it be used by a group or an
> >individual'). It would provide some data that could lead to an estimate
as
> >good as anything else.
>
> Yes, that sounds like a good idea.  That's more or less how People
> Magazine's 1998 survey to find the Most Beautiful People in the World
> was designed.  You might recall that "Hank, the ugly drunken dwarf"
> was the winner.
>
> Duncan Murdoch
>



From u6001513 at tknet.tku.edu.tw  Tue Apr 20 19:18:57 2004
From: u6001513 at tknet.tku.edu.tw (=?big5?B?pP26zadn?=)
Date: Wed, 21 Apr 2004 01:18:57 +0800
Subject: [R] I have a question!!!!  Help me!!
Message-ID: <000801c426fb$914547e0$6e01fea9@hlc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040421/0e65ea38/attachment.pl

From ggrothendieck at myway.com  Tue Apr 20 19:14:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 17:14:54 +0000 (UTC)
Subject: [R] Size of R user base
References: <3A822319EB35174CA3714066D590DCD504AF7C2C@usrymx25.merck.com>
	<408548A0.3020505@arcanemethods.com>
Message-ID: <loom.20040420T185905-990@post.gmane.org>

Bob Cain <arcane <at> arcanemethods.com> writes:

> Maybe it's time for a comp.lang.r usenet group (and 
> application subtree).

Note that you can already access r-help via http or nntp using gmane.  The
http and nntp interfacs are here, respectively:

   http://news.gmane.org/gmane.comp.lang.r.general
   nntp://news.gmane.org/gmane.comp.lang.r.general



From Carlisle.Thacker at noaa.gov  Tue Apr 20 19:16:55 2004
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Tue, 20 Apr 2004 13:16:55 -0400
Subject: [R] panel.grid
References: <MABBLJDICACNFOLGIHJOAEIKEFAA.phgrosjean@sciviews.org>
	<200404201758.55396.amurta@ipimar.pt>
Message-ID: <40855B07.8F348CDD@noaa.gov>

Is there a way to get panel.grid to put the grid lines at specific
locations?  It would be nice to have them correspond to selected tick
marks.

Thanks,

Carlisle
-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus



From deepayan at stat.wisc.edu  Tue Apr 20 19:27:48 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 20 Apr 2004 12:27:48 -0500
Subject: [R] panel.grid
In-Reply-To: <40855B07.8F348CDD@noaa.gov>
References: <MABBLJDICACNFOLGIHJOAEIKEFAA.phgrosjean@sciviews.org>
	<200404201758.55396.amurta@ipimar.pt> <40855B07.8F348CDD@noaa.gov>
Message-ID: <200404201227.48911.deepayan@stat.wisc.edu>

On Tuesday 20 April 2004 12:16, W. C. Thacker wrote:
> Is there a way to get panel.grid to put the grid lines at specific
> locations?  It would be nice to have them correspond to selected tick
> marks.

?panel.grid says:

Arguments:

     h,v: For panel.abline, numerical vectors giving y and x locations
          respectively of horizontal and vertical lines to be added to
          the plot. For panel.grid, number of horizontal and vertical
          reference lines to be added to the plot; h=-1 and v=-1 make
          the grids aligned with the axis labels (this doesn't always
          work). 

So, doesn't look like it. But it seems you are looking for the 
functionality of panel.abline() instead, so why not use that ?

Deepayan



From ggrothendieck at myway.com  Tue Apr 20 19:48:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 13:48:43 -0400 (EDT)
Subject: [R] Error message during debug
Message-ID: <20040420174843.B110012D51@mprdmxin.myway.com>



In R 1.9.0 on Windows XP Pro I get an error if I try to
debug the identity function f shown:

 > f <- function(x)x
 > debug(f)
 > f(1)
 debugging in: f(1)
 Error in f(1) : Unimplemented feature in eval
 > R.version.string
 [1] "R version 1.9.0, 2004-04-12"

Without debuggging its ok.



_______________________________________________

Make My Way your home on the Web - http://www.myway.com



From sundar.dorai-raj at PDF.COM  Tue Apr 20 19:52:06 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 20 Apr 2004 10:52:06 -0700
Subject: [R] Error message during debug
In-Reply-To: <20040420174843.B110012D51@mprdmxin.myway.com>
References: <20040420174843.B110012D51@mprdmxin.myway.com>
Message-ID: <40856346.9000302@pdf.com>



Gabor Grothendieck wrote:
> 
> In R 1.9.0 on Windows XP Pro I get an error if I try to
> debug the identity function f shown:
> 
>  > f <- function(x)x
>  > debug(f)
>  > f(1)
>  debugging in: f(1)
>  Error in f(1) : Unimplemented feature in eval
>  > R.version.string
>  [1] "R version 1.9.0, 2004-04-12"
> 
> Without debuggging its ok.
> 
> 
> 

Try

f <- function(x) { x }

--sundar



From ggrothendieck at myway.com  Tue Apr 20 19:53:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 17:53:19 +0000 (UTC)
Subject: [R] I have a question!!!!  Help me!!
References: <000801c426fb$914547e0$6e01fea9@hlc>
Message-ID: <loom.20040420T194955-200@post.gmane.org>

Check out ?solve:

m <- matrix(c(2,1,3,1),2) # create 2x2 matrix
b <- c(5,2)
solve(m,b)

.... <at> tknet.tku.edu.tw> writes:

: 
: Dearing author:
: I am a student.
: I am a beginner in learning R-project.
: I have a queation!!
: If we want to solve following equation, how can i do first??
: We want to solve X's and Y's value!!
: When we have two equation: 
: For example:
: 
: 2X+3Y=5
: X+Y=2
: 
: I just take a simple example!!
: MY form is more difficult than above example!!
: Please help me!! And give me a reference to solve it!!
: THANK YOU!!
:                             HUANG



From ggrothendieck at myway.com  Tue Apr 20 19:57:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 17:57:07 +0000 (UTC)
Subject: [R] Error message during debug
References: <20040420174843.B110012D51@mprdmxin.myway.com>
	<40856346.9000302@pdf.com>
Message-ID: <loom.20040420T195440-326@post.gmane.org>

Sundar Dorai-Raj <sundar.dorai-raj <at> PDF.COM> writes:

: 
: Gabor Grothendieck wrote:
: > 
: > In R 1.9.0 on Windows XP Pro I get an error if I try to
: > debug the identity function f shown:
: > 
: >  > f <- function(x)x
: >  > debug(f)
: >  > f(1)
: >  debugging in: f(1)
: >  Error in f(1) : Unimplemented feature in eval
: >  > R.version.string
: >  [1] "R version 1.9.0, 2004-04-12"
: > 
: > Without debuggging its ok.
: > 
: > 
: > 
: 
: Try
: 
: f <- function(x) { x }
: 
: --sundar

That made the error go away.  Note that 

  f <- function(x)sin(x)

does NOT produce the error under debugging.  Don't know why 
x and sin(x) would act differently in the above respect.



From dmurdoch at pair.com  Tue Apr 20 20:02:42 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 20 Apr 2004 14:02:42 -0400
Subject: [R] New unique name
In-Reply-To: <85fzb0dlx3.fsf@servant.blindglobe.net>
References: <40838721.7080404@univie.ac.at>
	<85fzb0dlx3.fsf@servant.blindglobe.net>
Message-ID: <gcpa80hon74hk4gu4d3ri6ipbbuh45nct2@4ax.com>

On Mon, 19 Apr 2004 08:31:20 -0700, rossini at blindglobe.net (A.J.
Rossini) wrote :

>One possibility would be the Ruuid library in BioConductor.  I'm not
>sure if the uuid library  is available (or compilable) under MS
>Windows, though.   I'll be checking in a bit (weeks) if no one else
>does.

It is one of the Windows binary packages downloadable from
BioConductor.

Duncan Murdoch



From mike.campana at freesurf.ch  Tue Apr 20 21:04:18 2004
From: mike.campana at freesurf.ch (mike.campana@freesurf.ch)
Date: Tue, 20 Apr 2004 20:04:18 +0100
Subject: [R] polygon
Message-ID: <1082484258.webexpressdV3.1.f@smtp.freesurf.ch>

Dear all
I try once again.
In order to clearly mark values wich are larger than a treshold value, I 

would like to color the surface below the line given by plot (yy~xx). To 
color is only the surface between abline (treshold) and yy if they are 
larger than the specific limit. I guess I can use the function polygon, 
but I can not find any valuable solution.
I'm grateful to you for an advice, an example or a link.
Mike  

xx <- c(1:100)
yy <- rnorm(100)
plot (yy~xx,type="l")
abline (h=0.5,col="red")
#?? how can I use polygon()
#in order to color surface below yy value and > abline??
polygon(xx,yy,col="gray")

---


---


---


---



From ozric at web.de  Tue Apr 20 20:18:52 2004
From: ozric at web.de (Christian Schulz)
Date: Tue, 20 Apr 2004 20:18:52 +0200
Subject: [R] benchmark dual amd opteron
Message-ID: <200404202018.12414.ozric@web.de>

Hi,

is it possible to get the benchmark-results  from
anybody is using a common amd dual opteron with windows
or linux, R.1.8.1 or 1.9.0?

http://www.sciviews.org/other/benchmark/R2.R


many thanks
& regards, christian



From andy_liaw at merck.com  Tue Apr 20 20:21:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 20 Apr 2004 14:21:23 -0400
Subject: [R] benchmark dual amd opteron
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C3C@usrymx25.merck.com>

It doesn't run as is in R-1.9.0.  R-1.9.0 doesn't like `eigen.default()',
and after changing that to `eigen()', it can't find Matrix.class().

Andy

> From: Christian Schulz
> 
> Hi,
> 
> is it possible to get the benchmark-results  from
> anybody is using a common amd dual opteron with windows
> or linux, R.1.8.1 or 1.9.0?
> 
http://www.sciviews.org/other/benchmark/R2.R


many thanks
& regards, christian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From cstrato at aon.at  Tue Apr 20 21:04:42 2004
From: cstrato at aon.at (cstrato)
Date: Tue, 20 Apr 2004 21:04:42 +0200
Subject: [R] Size of R user base 
In-Reply-To: <x2d6626eyh.fsf@biostat.ku.dk>
References: <4084D3E5.17583.1C5C02@localhost> <x2d6626eyh.fsf@biostat.ku.dk>
Message-ID: <4085744A.2020700@aon.at>

At first I want to make clear that I have always paid for the
software that I use, I am even paying for the shareware that I use.

After this statement let me come to the current discussion:
It is really shocking that some people even think about integrating
a spyware system into an open source software project.
The internet has started as a chaotic and democratic system, but
is in danger to become the worst "Big Brother is watching you"
assistant to certain governments and companies, which become a
danger for the democracy and for peace.

I really hope that the R core developer will never consider such
a dangerous option!

Having said this, I am willing to ACTIVELY send an e-mail to
CRAN or R to tell that I am using R.

R is such a great project, developed by a great community in a
democratic manner, showing how well democracy can work. Adding
a monitoring system would destroy the whole project.

P.S.: Personally, I have even disabled cookies from my browser,
both at home and at the company, and I must say, that very seldom
would I need to enable cookies. If a company web-site forces me
to enable cookies, than I ignore this company: companies want to
sell products to customers, and with such a behavior they are
hopefully loosing customers.

Best regards
Christian
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
C.h.r.i.s.t.i.a.n. .S.t.r.a.t.o.w.a
V.i.e.n.n.a.         .A.u.s.t.r.i.a
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-



Peter Dalgaard wrote:
> kjetil at entelnet.bo writes:
> 
> 
>>On 20 Apr 2004 at 10:47, Philippe Grosjean wrote:
> 
> 
>>>Of course, this will only work with computers connected to the
>>>internet,... but at least, it could be one way to evaluate the number
>>>of R users. Would that be an infringment of Open Source, or any other
>>>rule of freedom? I don't know, but it does seem to be quite widespread
>>>(at least for commercial software). so, why an Open Source software
>>>would not be able to monitor the number of users?
>>
>>That would make R into spyware, and there exist software to monitore 
>>and warn aganst/automatically remove spyware, and some users have 
>>such installed (and it will grow).
> 
> 
> Not quite spyware. Spyware generally works in more covert ways and
> tries to hide itself from the user.
> 
> However, quite a few people think that having programs connecting to
> places on the internet without being asked is annoying and a potential
> invasion of privacy and it may even cost people money if they're on a
> dialup non-flat-fee connection.
>



From elvis at xlsolutions-corp.com  Tue Apr 20 21:12:28 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 20 Apr 2004 12:12:28 -0700
Subject: [R] Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 USA  locations near you!
Message-ID: <20040420191228.5794.qmail@webmail02.mesa1.secureserver.net>



From bates at stat.wisc.edu  Tue Apr 20 21:26:09 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 20 Apr 2004 14:26:09 -0500
Subject: [R] benchmark dual amd opteron
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7C3C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7C3C@usrymx25.merck.com>
Message-ID: <6rhdve78oe.fsf@bates4.stat.wisc.edu>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> It doesn't run as is in R-1.9.0.  R-1.9.0 doesn't like `eigen.default()',
> and after changing that to `eigen()', it can't find Matrix.class().

I've changed the Matrix package since that benchmark was written.
I'll rewrite the benchmark and submit it to Phillippe Grosjean.



From matthew_wiener at merck.com  Tue Apr 20 21:31:06 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 20 Apr 2004 15:31:06 -0400
Subject: [R] FW: Aligning different trellis plots
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E01A341@uswsmx03.merck.com>

All --

I am trying to combine trellis plots and having a couple of small problems.

I'm trying to combine two trellis plots that display data of different
kinds.  Each has a single row of plots, and I'd like to display them over
one another.  So I use 

print(plot1, split = c(1,1,2,1), more = TRUE)
print(plot2, split = c(1,2,1,2))

I end up with two minor issues.  First, I'd like to have the top of the
bottom plot touch the bottom of the top plot, so it all looks like one plot
(I'd turn off the strip in the bottom one).  I've turned off the x labels
and bottom axes for the top plot, but there still ends up being some space
between them.  The effect I'm looking for is like setting various components
of "mar" to 0 in base graphics, but I can't figure out how to do it.

The second issue has to do with axes.  My two plots have the same number of
panels across, and I would like them to line up vertically.  However, my
y-axis labels, on the left of each plot, are slightly different size, and
this means that the panels are of slightly different sizes in the two plots.

Here's a toy example that shows my two problems.

    df.1 <- data.frame(x = rep(1:10, 5),
                       y = runif(50, 0, 10),
                       group = rep(1:5, each = 10))
    df.2 <- data.frame(x = rep(1:10, 5),
                       y = runif(50, -1, 1),
                       group = rep(1:5, each = 10))
    plot.1 <- xyplot(y ~ x | group, data = df.1,
                     scales = list(x = list(alternating = 0),
                       y = list(alternating= 1)),
                     xlab = "",                      ## same results with
xlab = NULL
                     layout = c(5,1,1))
    plot.2 <- xyplot(y ~ x | group, data = df.2,
                     scales = list(alternating = 1),
                     layout = c(5,1,1))

    print(plot.1, split = c(1,1,1,2), more = TRUE)
    print(plot.2, split = c(1,2,1,2))

I realize that it could be the xlab = "" that is giving me trouble -- it may
still be reserving space.  But xlab = NULL does the same thing, and I
haven't been able to find anything else to try.

One moderately ugly way to solve the problem is to use position instead of
split in the print statement for the trellis plots, and use overlapping
ranges to force the bottom of the top plot to line up with the top of the
bottom plot.  But it would be nice to have something a little more
automatic.

Or is there some better way to do this altogether -- perhaps that would
force a single plot to contain two kinds of panels?  That seems to really go
against the principle of lattice graphics, though.

Thanks for any help.

Regards,

Matt Wiener


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From pallier at lscp.ehess.fr  Tue Apr 20 21:45:25 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Tue, 20 Apr 2004 21:45:25 +0200
Subject: [R] I have a question!!!!  Help me!!
In-Reply-To: <000801c426fb$914547e0$6e01fea9@hlc>
References: <000801c426fb$914547e0$6e01fea9@hlc>
Message-ID: <40857DD5.60503@lscp.ehess.fr>


>We want to solve X's and Y's value!!
>When we have two equation: 
>For example:
>
>2X+3Y=5
>X+Y=2
>
>  
>

solve(matrix(c(2,1,3,1),2,2),c(5,2)) yields the solution.

see '?solve' (you have to know about matrix algebra to understand this 
function)

Christophe Pallier



From jcraig at vbi.vt.edu  Tue Apr 20 22:13:09 2004
From: jcraig at vbi.vt.edu (Hanna Craig)
Date: Tue, 20 Apr 2004 16:13:09 -0400
Subject: [R] contributed packages and downloads questions
Message-ID: <019a01c42713$e6e51050$b962ad80@bioinformatics.vt.edu>

Hello,

I am trying to download two library files: multiv and e1071 to do principal
components analysis. I was able to unzip the folders to my R directory, but
when I type library(multiv) in the console I get an error message: Error in
testRversion(descfields) : This package has not been installed properly
 See the Note in ?library
So, it seems that I am not downloading these two library files properly.
Where can I find the library files (I cannot find my way back to them for
some reason) and how do I download them? Thanks!

Hanna



From dmurdoch at pair.com  Tue Apr 20 22:20:26 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 20 Apr 2004 16:20:26 -0400
Subject: [R] contributed packages and downloads questions
In-Reply-To: <019a01c42713$e6e51050$b962ad80@bioinformatics.vt.edu>
References: <019a01c42713$e6e51050$b962ad80@bioinformatics.vt.edu>
Message-ID: <6c1b80ta1gmthmj0ev3gfg59p6vrr4vppo@4ax.com>

On Tue, 20 Apr 2004 16:13:09 -0400, "Hanna Craig" <jcraig at vbi.vt.edu>
wrote :

>Hello,
>
>I am trying to download two library files: multiv and e1071 to do principal
>components analysis. I was able to unzip the folders to my R directory, but
>when I type library(multiv) in the console I get an error message: Error in
>testRversion(descfields) : This package has not been installed properly
> See the Note in ?library
>So, it seems that I am not downloading these two library files properly.
>Where can I find the library files (I cannot find my way back to them for
>some reason) and how do I download them? Thanks!

The easiest way to install a package in Windows (which I assume you're
using, since you're talking about zip files) is to use the "Packages |
Install packages from CRAN" menu entry.  That'll work for both of
those packages.

For future reference, if you want to install a package from a zip
file, you should use the "Packages | Install package from local zip
file" menu entry.

Duncan Murdoch



From bamelbourne at ucdavis.edu  Tue Apr 20 22:35:16 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Tue, 20 Apr 2004 13:35:16 -0700
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
References: <F9D084AF922BC24AA87F674DE1E6850E3BAA03@exchange.usca.edu>
Message-ID: <00c301c42716$fe456120$1313eda9@des.ucdavis.edu>

That exact same thing is happening to me too. It seems to be intermittent.
After a reboot, it might (but might not) fix itself. My older versions of R
also no longer work. R1.8.1 fails with the windows message "R for Windows
GUI front-end has encountered a problem and needs to close.  We are sorry
for the inconvenience."

This appears to have occured to me after installing the latest Microsoft
Windows critical updates. I have reinstalled R1.8.1 and R1.9.0 to no avail.
None of the other programs on my computer are affected, only R.

Is anybody else experiencing this?

Brett
Brett Melbourne, Postdoctoral Fellow
Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
Center for Population Biology, Storer Hall
University of California Davis CA 95616


----- Original Message ----- 
From: "Joann Williamson" <JoannW at usca.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 20, 2004 8:41 AM
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE


> I installed R project 1.9.0 on Windows XP.  The installation went well.
> Then, I rebooted.  Next, I clicked the icon under Programs in the Start
> menu and received this error, "Fatal Error: Invalid HOMEDRIVE".  I
> clicked OK to the error message.  The program does not start.  I went to
> a DOS prompt and did "echo %HOMEDRIVE%" and it returned "C:".  This
> letter is a valid drive letter.  Can you tell me how to fix this?
>
> Thanks you,
>
> Joann Williamson
>
> Network Administrator
>
> University of South Carolina Aiken
>
> 471 University Parkway
>
> Aiken, SC 29801
>
> 803-641-3473
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f0z6305 at labs.tamu.edu  Tue Apr 20 23:20:51 2004
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Tue, 20 Apr 2004 16:20:51 -0500
Subject: [R] How to get the threshold value for hierarchical clustering
	algorithms?
Message-ID: <002f01c4271d$5bc52c80$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040420/800aaf23/attachment.pl

From p.murrell at auckland.ac.nz  Tue Apr 20 23:32:29 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 21 Apr 2004 09:32:29 +1200
Subject: [R] polygon
References: <1082445722.webexpressdV3.1.f@smtp.freesurf.ch>
Message-ID: <408596ED.8050900@stat.auckland.ac.nz>

Hi


mike.campana at freesurf.ch wrote:
> Dear all
> 
> In order to clearly mark values wich are larger than a treshold value, I 
> would like to color the surface below the line given by plot (yy~xx). To 
> color is only the surface between abline (treshold) and yy if they are 
> larger than the specific limit. I guess I can use the function polygon, 
> but I can not find any valuable solution.
> I'm grateful to you for an advice or an example.
> Mike  
> 
> xx <- c(1:100)
> yy <- rnorm(100)
> plot (yy~xx,type="l")
> abline (h=0.5,col="red")
> #?? how can I use polygon()
> #in order to color surface below yy value and > abline??
> polygon(xx,yy,col="gray")


You can do it by "brute force" as follows:

xx <- c(1:100)
yy <- rnorm(100)
# Set up plot but draw nothing
plot (yy ~ xx, type="n", axes=FALSE)
# Some constants
n <- 100
hline <- 0.5
# Fill in grey beneath yy
polygon(c(xx[1], xx, xx[n]), c(min(yy), yy, min(yy)), col="grey")
# Fill in white beneath abline
usr <- par("usr")
rect(usr[1], usr[3], usr[2], hline, col="white", border=NA)
# Redraw all of yy line
lines(xx, yy)
# Draw red abline
abline (h=0.5,col="red")
# Draw bounding box and axes
box()
axis(1)
axis(2)

Or you could calculate all intercepts of yy line with the abline, but 
that's probably not worth the effort.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From d.scott at auckland.ac.nz  Tue Apr 20 23:45:31 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Wed, 21 Apr 2004 09:45:31 +1200 (NZST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <00c301c42716$fe456120$1313eda9@des.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0404210942350.25386-100000@stat71.stat.auckland.ac.nz>

On Tue, 20 Apr 2004, Brett Melbourne wrote:

> That exact same thing is happening to me too. It seems to be intermittent.
> After a reboot, it might (but might not) fix itself. My older versions of R
> also no longer work. R1.8.1 fails with the windows message "R for Windows
> GUI front-end has encountered a problem and needs to close.  We are sorry
> for the inconvenience."
> 
> This appears to have occured to me after installing the latest Microsoft
> Windows critical updates. I have reinstalled R1.8.1 and R1.9.0 to no avail.
> None of the other programs on my computer are affected, only R.
> 
> Is anybody else experiencing this?
>

Yes. With 1.8.1 if the updates are installed R fails as you indicate. If 
the updates are taken off it works again. Bit of a problem here because 
the University IT Security runs checks on the updates and complains to 
users who don't have them installed.

 
> Brett
> Brett Melbourne, Postdoctoral Fellow
> Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
> Center for Population Biology, Storer Hall
> University of California Davis CA 95616
> 
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From genehammel at comcast.net  Tue Apr 20 23:52:17 2004
From: genehammel at comcast.net (Gene Hammel)
Date: Tue, 20 Apr 2004 14:52:17 -0700
Subject: [R] Creating variable names
Message-ID: <40859B91.1050105@comcast.net>

My apologies for asking what is doubtless a dumb question, but I have 
scant experience in R.
It would be very convenient in doing lots of plots to be able to do them 
in a loop that stepped through a vector of variable names. For example 
one could say

x<-("mydates")
y<-c("foo1","foo2","foo3") #where "foon" were vectors
plot(x,y[1],type="n")
points(x,y[1])
points(x,y[2],pch=2)
points(x,y[3],pch=3)

This is pretty easy in Perl, but of course Perl is not for plotting. Of 
course one could construct a data structure in R that would hold what 
was wanted and maybe that is the way to go. But I thought I would ask, 
if you have the patience to get this far with what is, I suspect, a 
silly question from a novice.

Thanks.



From binabina at bellsouth.net  Wed Apr 21 00:09:20 2004
From: binabina at bellsouth.net (zubin)
Date: Tue, 20 Apr 2004 18:09:20 -0400
Subject: [R] multi-user engine
Message-ID: <MBBBIIHJANJBMHLGMACKKEAMCGAA.binabina@bellsouth.net>

hello, i just got introduced to R - WOW its beautiful..  

I am presently a SAS user and wanted to configure R to work in a multi-user
enteprise environment.  Client - Server.  Where we have a strong LINUX
server supporting about 10 statisticians with R.  Anyone have any backround
or information they can share to help me get jump-started on setting up R in
this environment?  Does each user have their own worksession with the
serving executing the submitted R code? Has it been done, any feedback?  

thx in advance..

-zubin





From pburns at pburns.seanet.com  Wed Apr 21 00:20:44 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 20 Apr 2004 23:20:44 +0100
Subject: [R] Size of R user base
References: <0ABD88905D18E347874E0FB71C0B29E90179E68C@exdkba022.novo.dk>
	<16516.55234.859527.878062@gargle.gargle.HOWL>
Message-ID: <4085A23C.4090604@pburns.seanet.com>

I disagree with Martin.  I think his confidence interval is sufficient for
management decisions.  There are three basic questions that should
be answered:

1)  Does it do what we want?

2) Will it disappear?

3) Is the quality sufficient?

Question 1 can only be answered within the organization.

Question 2 is partly answered by Martin's and other's estimates of
the user base.  The volume of messages on R-help is another clue --
this is probably more correlated with the number of new users than
the total number of users.  

I think there is sufficient evidence that R will survive at least as long
as the expected duration of a manager's tenure, and that there will
be a reasonable supply of potential employees with knowledge of
it.

As for Question 3, I -- who am a magnet for bugs-- have been using
R intensely for 2 years now, and have only found a couple of esoteric
bugs (one of which is clearly not down to R) plus two or three minor
bugs which have been fixed.  This is a level of excellence that I
never would have imagined.

In contrast, for 5 weeks I've been using a now obsolete version of
another language not unlike R on an operating system that ought to be
obsolete. In that time I've found 2 serious bugs (i.e., system 
terminating --
only one of which is esoteric), and a couple other moderately annoying
bugs.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Martin Maechler wrote:

>
>About a `confidence interval': If we log transform it becomes
>feasible to give an interval I'm ``quite confident'' about:
>
>  log10(N) \in [3.5, 5.3]
>
>but then that's probably not informative enough for management
>decisions.
>
>Martin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From pkleiber at honlab.nmfs.hawaii.edu  Wed Apr 21 00:25:34 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Tue, 20 Apr 2004 12:25:34 -1000
Subject: [R] Creating variable names
In-Reply-To: <40859B91.1050105@comcast.net>
References: <40859B91.1050105@comcast.net>
Message-ID: <4085A35E.3000207@honlab.nmfs.hawaii.edu>

If I understand your problem, it's easy in R too....

 > x<-("mydates")
 > y<-c("foo1","foo2","foo3") #where "foon" were vectors
 > plot(x,y[1],type="n")
 > for(i in seq(length(y))) points(x, get(y[1]), pch=i)
 >

Cheers, Pierre


Gene Hammel wrote:
> My apologies for asking what is doubtless a dumb question, but I have 
> scant experience in R.
> It would be very convenient in doing lots of plots to be able to do them 
> in a loop that stepped through a vector of variable names. For example 
> one could say
> 
> x<-("mydates")
> y<-c("foo1","foo2","foo3") #where "foon" were vectors
> plot(x,y[1],type="n")
> points(x,y[1])
> points(x,y[2],pch=2)
> points(x,y[3],pch=3)
> 
> This is pretty easy in Perl, but of course Perl is not for plotting. Of 
> course one could construct a data structure in R that would hold what 
> was wanted and maybe that is the way to go. But I thought I would ask, 
> if you have the patience to get this far with what is, I suspect, a 
> silly question from a novice.
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396
-----------------------------------------------------------------
  "God could have told Moses about galaxies and mitochondria and
   all.  But behold... It was good enough for government work."



From elvis at xlsolutions-corp.com  Wed Apr 21 00:30:25 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 20 Apr 2004 15:30:25 -0700
Subject: [R] Re: Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 USA  locations near you!]
Message-ID: <20040420223025.5311.qmail@webmail03.mesa1.secureserver.net>



From bates at stat.wisc.edu  Wed Apr 21 00:31:04 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 20 Apr 2004 17:31:04 -0500
Subject: [R] Creating variable names
In-Reply-To: <40859B91.1050105@comcast.net>
References: <40859B91.1050105@comcast.net>
Message-ID: <6ry8oq46zb.fsf@bates4.stat.wisc.edu>

Gene Hammel <genehammel at comcast.net> writes:

> My apologies for asking what is doubtless a dumb question, but I have
> scant experience in R.
> 
> It would be very convenient in doing lots of plots to be able to do
> them in a loop that stepped through a vector of variable names. For
> example one could say
> 
> 
> x<-("mydates")

   You misquoted that line.  There is no function before the (

> y<-c("foo1","foo2","foo3") #where "foon" were vectors
> plot(x,y[1],type="n")
> points(x,y[1])
> points(x,y[2],pch=2)
> points(x,y[3],pch=3)
> 

The general approach in R is often called "whole object".  That is,
you try to put your data into a structure that will facilitate the
operations you wish to perform.  In this case bind the responses into
a matrix and use matplot

matplot(x, cbind(foo1, foo2, foo3), type = 'p')

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From ggrothendieck at myway.com  Wed Apr 21 00:58:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 20 Apr 2004 18:58:50 -0400 (EDT)
Subject: [R] Debug problem
Message-ID: <20040420225850.82FD73988@mprdmxin.myway.com>




Consider the following R session under Windows XP Pro.  Note that 
if we set debugging for function
f then even if we reset its environment it still has debugging on.
Also if we copy f to g then g has debugging; however, if we change
g's environment then g no longer has debugging.    Why did f retain
debugging when its environment was changed but g did not?  

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
[...snip...]
Type 'q()' to quit R.

> f <- function(x) { x }
> f(1)
[1] 1
> debug(f)
> f(1)
debugging in: f(1)
debug: {
    x
}
Browse[1]> Q
> environment(f) <- new.env()
> f(1)
debugging in: f(1)
debug: {
    x
}
Browse[1]> Q
> g <- f
> g(1)
debugging in: g(1)
debug: {
    x
}
Browse[1]> Q
> environment(g) <- new.env()
> g(1)
[1] 1
> 
> 



P.S.  Here is just the input if you want to try it yourself by copying
and pasting into an R session:

f <- function(x) { x }
f(1)
debug(f)
f(1)
Q
environment(f) <- new.env()
f(1)
Q
g <- f
g(1)
Q
environment(g) <- new.env()
g(1)





_______________________________________________

Make My Way your home on the Web - http://www.myway.com



From rossini at blindglobe.net  Wed Apr 21 01:06:34 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 20 Apr 2004 16:06:34 -0700
Subject: [R] multi-user engine
In-Reply-To: <MBBBIIHJANJBMHLGMACKKEAMCGAA.binabina@bellsouth.net> (zubin's
	message of "Tue, 20 Apr 2004 18:09:20 -0400")
References: <MBBBIIHJANJBMHLGMACKKEAMCGAA.binabina@bellsouth.net>
Message-ID: <854qre5jwl.fsf@servant.blindglobe.net>

"zubin" <binabina at bellsouth.net> writes:

> I am presently a SAS user and wanted to configure R to work in a multi-user
> enteprise environment.  Client - Server.  Where we have a strong LINUX
> server supporting about 10 statisticians with R.  Anyone have any backround
> or information they can share to help me get jump-started on setting up R in
> this environment?  Does each user have their own worksession with the
> serving executing the submitted R code? Has it been done, any feedback?  

Might be hard to do in a robust framework.

Greg Warnes' RSOAP and related manager framework is one start, as is
Simon Urbanek's Rserve package.

We've (Greg and I, in that order) have gotten part way on such a
beast, using a web objects framework for consulting/collaboration
services.  Hopefully, we'll be releasing a live CD this summer (we are
pretty close to doing it today, but you know the old saw about the
last 5% of the work...).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rbaer at atsu.edu  Wed Apr 21 01:56:03 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Tue, 20 Apr 2004 18:56:03 -0500
Subject: [R] Debug problem
References: <20040420225850.82FD73988@mprdmxin.myway.com>
Message-ID: <002f01c42733$0a021320$2e80010a@BigBaer>

> P.S.  Here is just the input if you want to try it yourself by copying
> and pasting into an R session:

If you are using Windows RGUI and you haven't noticed the new "Paste
commands only" in the Edit menu of R1.9.0, check it out.  It is a fantastic
time saver.

Rob



From tlumley at u.washington.edu  Wed Apr 21 01:57:01 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 20 Apr 2004 16:57:01 -0700 (PDT)
Subject: [R] Debug problem
In-Reply-To: <20040420225850.82FD73988@mprdmxin.myway.com>
References: <20040420225850.82FD73988@mprdmxin.myway.com>
Message-ID: <Pine.A41.4.58.0404201647380.131234@homer04.u.washington.edu>

On Tue, 20 Apr 2004, Gabor Grothendieck wrote:

>
>
>
> Consider the following R session under Windows XP Pro.  Note that
> if we set debugging for function
> f then even if we reset its environment it still has debugging on.
> Also if we copy f to g then g has debugging; however, if we change
> g's environment then g no longer has debugging.    Why did f retain
> debugging when its environment was changed but g did not?
>


My guess, which could be verified by careful reading of the code, is as
follows:

- There is a debugging flag on each function object
- Changing the environment does not change this flag
- Assigning f to g makes g a reference to the same copy of f
- Now there are two references to the same function, modifying the
environment of either one causes copying, and the modified function will
not have the debugging flag set.


This correctly predicts that after

f<-function(x) {x}
g<-f
debug(f)
environment(f)<-new.env()

the debugging flag will be set for g but not f, since f was modified.

	-thomas



From deepayan at stat.wisc.edu  Wed Apr 21 02:06:46 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 20 Apr 2004 19:06:46 -0500
Subject: [R] FW: Aligning different trellis plots
In-Reply-To: <45AAE6FD142DCB43A38C00A11FF5DF3E01A341@uswsmx03.merck.com>
References: <45AAE6FD142DCB43A38C00A11FF5DF3E01A341@uswsmx03.merck.com>
Message-ID: <200404201906.46067.deepayan@stat.wisc.edu>

On Tuesday 20 April 2004 14:31, Wiener, Matthew wrote:
> All --
>
> I am trying to combine trellis plots and having a couple of small
> problems.
>
> I'm trying to combine two trellis plots that display data of
> different kinds.  Each has a single row of plots, and I'd like to
> display them over one another.  So I use
>
> print(plot1, split = c(1,1,2,1), more = TRUE)
> print(plot2, split = c(1,2,1,2))
>
> I end up with two minor issues.  First, I'd like to have the top of
> the bottom plot touch the bottom of the top plot, so it all looks
> like one plot (I'd turn off the strip in the bottom one).  I've
> turned off the x labels and bottom axes for the top plot, but there
> still ends up being some space between them.  The effect I'm looking
> for is like setting various components of "mar" to 0 in base
> graphics, but I can't figure out how to do it.

This is doable with a bit of hacking the code. There is a hard-coded 1/2 
"lines" gap on all sides of a trellis plot. I guess this should be a 
user-settable parameter, but it isn't currently. You can change this in 
the calculateGridLayout() function in lattice/R/print.trellis.R 
(assuming you are using R 1.9.0).

> The second issue has to do with axes.  My two plots have the same
> number of panels across, and I would like them to line up vertically.
>  However, my y-axis labels, on the left of each plot, are slightly
> different size, and this means that the panels are of slightly
> different sizes in the two plots.

Unfortunately there's not much that can be done about this easily. The 
space allocated for tick labels is exactly as much is needed; I 
normally consider this an advantage, but in this case it's somewhat 
irritating.

You may be able to take advantage a rarely used feature of print.trellis 
which controls the panel widths. For your example, this might look 
like:

    print(plot.1, split = c(1,1,1,2), more = TRUE,
          panel.width = list(1, "inches"))
    print(plot.2, split = c(1,2,1,2),
          panel.width = list(1, "inches"))

although even this is not really perfect.

> Here's a toy example that shows my two problems.
>
>     df.1 <- data.frame(x = rep(1:10, 5),
>                        y = runif(50, 0, 10),
>                        group = rep(1:5, each = 10))
>     df.2 <- data.frame(x = rep(1:10, 5),
>                        y = runif(50, -1, 1),
>                        group = rep(1:5, each = 10))
>     plot.1 <- xyplot(y ~ x | group, data = df.1,
>                      scales = list(x = list(alternating = 0),
>                        y = list(alternating= 1)),
>                      xlab = "", ## same results with xlab = NULL
>                      layout = c(5,1,1))
>     plot.2 <- xyplot(y ~ x | group, data = df.2,
>                      scales = list(alternating = 1),
>                      layout = c(5,1,1))
>
>     print(plot.1, split = c(1,1,1,2), more = TRUE)
>     print(plot.2, split = c(1,2,1,2))
>
> I realize that it could be the xlab = "" that is giving me trouble --
> it may still be reserving space.  But xlab = NULL does the same
> thing, and I haven't been able to find anything else to try.
>
> One moderately ugly way to solve the problem is to use position
> instead of split in the print statement for the trellis plots, and
> use overlapping ranges to force the bottom of the top plot to line up
> with the top of the bottom plot.  But it would be nice to have
> something a little more automatic.
>
> Or is there some better way to do this altogether -- perhaps that
> would force a single plot to contain two kinds of panels?  That seems
> to really go against the principle of lattice graphics, though.

Probably not, depends on whether the conditioning variable ('group' in 
your example) has the same interpretation for both sets. If they do, 
your example might be modified as follows (doesn't look particularly 
nice though):


df.3 <- rbind(df.1, df.2)
df.3$group2 <- rep(1:2, each = 50)

xyplot(y ~ x | group * group2, data = df.3,
       scales = list(y = "free"))


Deepayan



From ggrothendieck at myway.com  Wed Apr 21 02:47:55 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 21 Apr 2004 00:47:55 +0000 (UTC)
Subject: [R] Debug problem
References: <20040420225850.82FD73988@mprdmxin.myway.com>
	<Pine.A41.4.58.0404201647380.131234@homer04.u.washington.edu>
Message-ID: <loom.20040421T023106-178@post.gmane.org>

Thomas Lumley <tlumley <at> u.washington.edu> writes:
: On Tue, 20 Apr 2004, Gabor Grothendieck wrote:
: > Consider the following R session under Windows XP Pro.  Note that
: > if we set debugging for function
: > f then even if we reset its environment it still has debugging on.
: > Also if we copy f to g then g has debugging; however, if we change
: > g's environment then g no longer has debugging.    Why did f retain
: > debugging when its environment was changed but g did not?
: 
: My guess, which could be verified by careful reading of the code, is as
: follows:
: 
: - There is a debugging flag on each function object
: - Changing the environment does not change this flag
: - Assigning f to g makes g a reference to the same copy of f
: - Now there are two references to the same function, modifying the
: environment of either one causes copying, and the modified function will
: not have the debugging flag set.


Thanks.  Your hypothesis does seem likely.  In

g <- f; environment(g) <- new.env()

is there any way that I can automatically set g to have debugging 
iff f has it?



From ok at cs.otago.ac.nz  Wed Apr 21 03:55:40 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 21 Apr 2004 13:55:40 +1200 (NZST)
Subject: [R] Size of R user base
Message-ID: <200404210155.i3L1te9M362709@atlas.otago.ac.nz>

"Philippe Grosjean" <phgrosjean at sciviews.org> wrote:
	A last comment/question:  would it be possible to add some code
	in R that does the following:
	["calls home" to say that it is being used/asks for updates/&c]

There are all sorts of things the R developers might like to know about how
it is used.  There are also all sorts of reasons why they shouldn't do
anything like that.  Any habitual reader of comp.risks can think of more
reasons than I care to spend typing up.  I'll mention just one:  a number
of Microsoft users got hit with unexpectedly large phone bills a while back.
Their software was "calling home" *without* asking the user's permission or
even telling the user, and Microsoft's normal lines were out of service, so
normal full cost calls were made.

As far as informing the user that there is an update,
	
	An update is available at http://cran.r-project.org.
	
the only *really* useful information here is the URL, and that can be
displayed without calling home.  If one's R installation is more than a
couple of months old there is almost certainly an update.  It would suffice
to say

	You can check for updates by visiting http://cran.r-project.org
	or by using the check.CRAN.for.updates function.

Another reason for not calling home, of course, is that R already takes
quite long enough to start up, thank you very much.  (And that doesn't
count opening a graphics window, just time to first prompt.)

	Of course, this will only work with computers connected to the
	internet,... but at least, it could be one way to evaluate the
	number of R users.  Would that be an infringment of Open Source,
	or any other rule of freedom?  I don't know, but it does seem to
	be quite widespread (at least for commercial software).

Yes, and it's an unwarrnated invasion of privacy there.  The fact that
some be****ed program is sending who knows what information about me to
who knows where without my say-so is one big reason why I avoid commercial
software (read: Windows software; none of the commercial software I use on
my Solaris box does this).

	so, why an Open Source software would not be able to monitor the
	number of users?
	
Because even if R *did* do the unwise and unforgivable, we STILL could not
know the number of users!  You would, to start with, only know about copies
of R on machines that were connected to the internet and allowed this kind
of traffic through their firewall.  Now I have R on two old Macs at home,
and you'd never hear about those.  Worse, here at work I have accounts on
a G3 Mac, a G4 Mac, three different UltraSPARCs, three Alphas, and a couple
of Linux boxes.  That's about 10 different accounts.  (How do I keep track
of 10 different passwords?  Easy:  every so often I ask our sysadmin to give
me new passwords on the machines I use less often because I've forgotten
them.)  How is your monitoring site to know that these 10 users are really
the same person?  And when I fire up R on a student's Linux box to demonstrate
a point (to a student who _isn't_ an R user), how is the monitoring site to
know that it's really me, not the student, so that the number of "users"
should not be incremented?

In fact, the more I think about it, the more it seems to me that "the
number of users" is not a well defined concept.  For a commercial system,
you can count the number of licences sold, and that means something
pretty clear, because each licence is money in your pocket.  For a system
like R, the amount of traffic on the mailing list is reasonably well
defined and of interest because it's stuff that the maintainers have to at
least glance at, so it directly affects their lives.  If you are thinking
about popularity contests, bear in mind that a Microsoft staffer wrote an
article "Evangelism is WAR" in which he explicitly stated that other
software producers are the "enemy" and users are "pawns"; do you really
want to get into that kind of contest?  If you're concerned about mind-
share rather than market-share, I have talked a data-mining student into
at least looking at R.  She has tried it.  She's doing a literature survey
first.  Is she an "R user" yet?  If she uses it for a month, and drops it
for a year, is she still an "R user"?  I use R in bursts myself; intensely
for a couple of days, then stop and think about things and do other work for
a week or so, then come back.  When, precisely, am I an "R user", and when
would I stop being one?

The first rule of measurement is "Don't bother with a measurement if you
don't know what you're going to do with the answer".  If you knew the
number of "R users", however defined, how would that actually help you?

Why do bad things to make a measurement that's ill-defined, arguably
impossible to measure meaningfully, and not that much use when you have it?



From dmurdoch at pair.com  Wed Apr 21 04:04:57 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 20 Apr 2004 22:04:57 -0400
Subject: [R] Debug problem
In-Reply-To: <loom.20040421T023106-178@post.gmane.org>
References: <20040420225850.82FD73988@mprdmxin.myway.com>
	<Pine.A41.4.58.0404201647380.131234@homer04.u.washington.edu>
	<loom.20040421T023106-178@post.gmane.org>
Message-ID: <e2kb80tcbgqgte05j3osoph9fprdq8tcul@4ax.com>

On Wed, 21 Apr 2004 00:47:55 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:


>Thanks.  Your hypothesis does seem likely.  In
>
>g <- f; environment(g) <- new.env()
>
>is there any way that I can automatically set g to have debugging 
>iff f has it?

undebug() gives a warning if the function doesn't have the debug flag
set, so it's possible to do it.  I'm new to the new signals in R, so
this might be a really ugly implementation, but here goes:

isbeingdebugged <- function(f) {
    if (is.function(f)) {
        result <- tryCatch(undebug(f), warning = function(w) 1)
	  result <- is.null(result)
	  if (result) debug(f)
	  result
    } else stop('Not a function')
}

copydebugstatus <- function(f, g) {
   if (isbeingdebugged(f)) debug(g)
   else if (isbeingdebugged(g)) undebug(g)
}

Duncan Murdoch



From arcane at arcanemethods.com  Wed Apr 21 04:22:59 2004
From: arcane at arcanemethods.com (Bob Cain)
Date: Tue, 20 Apr 2004 19:22:59 -0700
Subject: [R] Size of R user base
In-Reply-To: <loom.20040420T185905-990@post.gmane.org>
References: <3A822319EB35174CA3714066D590DCD504AF7C2C@usrymx25.merck.com>	<408548A0.3020505@arcanemethods.com>
	<loom.20040420T185905-990@post.gmane.org>
Message-ID: <4085DB03.3060507@arcanemethods.com>



Gabor Grothendieck wrote:

> 
> Note that you can already access r-help via http or nntp using gmane.  The
> http and nntp interfacs are here, respectively:
> 
>    http://news.gmane.org/gmane.comp.lang.r.general
>    nntp://news.gmane.org/gmane.comp.lang.r.general

Not quite as good as having it part of the standard usenet
tree but sure better for me than a mailing list.  Many  thanks!

It would be nice to get our other lists up there as well.


Bob
-- 

"Things should be described as simply as possible, but no
simpler."

                                              A. Einstein



From eugenedalt at yahoo.com  Wed Apr 21 06:26:50 2004
From: eugenedalt at yahoo.com (eugene dalt)
Date: Tue, 20 Apr 2004 21:26:50 -0700 (PDT)
Subject: [R] Re: Course***May-June 2004***R/Splus Programming Techniques,
	@ 5 USA  locations near you!]
In-Reply-To: <20040420223025.5311.qmail@webmail03.mesa1.secureserver.net>
Message-ID: <20040421042650.91412.qmail@web10909.mail.yahoo.com>

It seems like they are having trouble posting this...


XLSolutions Corporation (www.xlsolutions-corp.com) is
proud
to announce May-June 2004 2-day "R/S-plus Fundamentals
and Programming
Techniques".
 
****Boston  ---------------------> May, 27-28; June,
24-25
****Chicago -------------------> May, 27-28; June,
24-25

****Atlanta -------------------> May, 24-25; June,
28-29
****San Francisco  --------------> May, 27-28; June,
10-11

****Washington DC -----------------> May, 24-25; June
10-11

**** Raleigh ---------------------> May, June: TBD
 
Interested in our Summer R/Splus Advanced Programming
course? Please email us.
 
Reserve your seat now at the early bird rates! Payment
due AFTER the
class.
 
Course Description:
This two-day beginner to intermediate R/S-plus course
focuses
 on a broad spectrum of topics,
from reading raw data to a comparison of R and S. We
will learn
the essentials of data manipulation, graphical
visualization
and R/S-plus programming. We will explore statistical
data analysis
tools,including graphics with data sets. How to
enhance your plots.
We will perform basic statistics and fit linear
regression models.
Participants are encouraged to bring data for
interactive sessions
 
With the following outline:
- An Overview of R and Splus
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
 
Email us for group discounts.

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578

Visit us: www.xlsolutions-corp.com/training.htm

Please let us know if you and your colleagues are
interested in this
classto take advantage of group discount. Register now
to secure your seat!
Interested in R/Splus Advanced course? email us.
 
Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From ggrothendieck at myway.com  Wed Apr 21 06:35:24 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 21 Apr 2004 04:35:24 +0000 (UTC)
Subject: [R] Debug problem
References: <20040420225850.82FD73988@mprdmxin.myway.com>
	<Pine.A41.4.58.0404201647380.131234@homer04.u.washington.edu>
	<loom.20040421T023106-178@post.gmane.org>
	<e2kb80tcbgqgte05j3osoph9fprdq8tcul@4ax.com>
Message-ID: <loom.20040421T063458-263@post.gmane.org>


Excellent!  Thanks.

Duncan Murdoch <dmurdoch <at> pair.com> writes:

: 
: On Wed, 21 Apr 2004 00:47:55 +0000 (UTC), Gabor Grothendieck
: <ggrothendieck <at> myway.com> wrote:
: 
: >Thanks.  Your hypothesis does seem likely.  In
: >
: >g <- f; environment(g) <- new.env()
: >
: >is there any way that I can automatically set g to have debugging 
: >iff f has it?
: 
: undebug() gives a warning if the function doesn't have the debug flag
: set, so it's possible to do it.  I'm new to the new signals in R, so
: this might be a really ugly implementation, but here goes:
: 
: isbeingdebugged <- function(f) {
:     if (is.function(f)) {
:         result <- tryCatch(undebug(f), warning = function(w) 1)
: 	  result <- is.null(result)
: 	  if (result) debug(f)
: 	  result
:     } else stop('Not a function')
: }
: 
: copydebugstatus <- function(f, g) {
:    if (isbeingdebugged(f)) debug(g)
:    else if (isbeingdebugged(g)) undebug(g)
: }
: 
: Duncan Murdoch
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From mont4260 at uidaho.edu  Wed Apr 21 07:01:36 2004
From: mont4260 at uidaho.edu (Deb Montgomery)
Date: Tue, 20 Apr 2004 22:01:36 -0700
Subject: [R] difference between coxph and cph
Message-ID: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>

Hi. I am using Windows version of R 1.8.1. Being somewhat new to survival
analysis, I am trying to compare cph (Design) with coxph (survival) for use
with a survival data set.

I was wondering why cph and coxph provide me with different confidence
intervals
for the hazard ratios for one of the variables. I was wondering if I am
doing something wrong? Or if the two functions are calculating hazard ratios
and the 95% confidence intervals differently? I have listed part of the code
and part of the results from the 2 functions. Sorry if this question is a
repeat, I didn't find it when I searched the archives.

###########################################################
# s= Surv(Time1, Time2, censor)
#f= coxph(s~  Siblings + Weight.at.age.4)
#summary(f)
#Call:
#coxph(formula = s ~ Siblings + Weight.at.age.4)
# n= 132
#                exp(coef) exp(-coef) lower .95 upper .95
#Siblings            1.52      0.657     0.815      2.84
#Weight.at.age.4      0.91      1.099     0.772      1.07
##############################################################
#s= Surv(Time1, Time2, censor)
#f= cph(s~  Siblings + Weight.at.age.4,surv=TRUE ,  x=T, y = T)
# summary(f)
#             Effects              Response : s
# Factor          Low   High   Diff.  Effect S.E. Lower 0.95 Upper 0.95
# Siblings       0.000  1.000 1.0000  0.42  0.32 -0.20      1.04
#  Hazard Ratio   0.000  1.000 1.0000  1.52    NA  0.82      2.84
# Weight.at.age.4 8.613 10.602 1.9885 -0.19  0.17 -0.51      0.14
# Hazard Ratio   8.613 10.602 1.9885  0.83    NA  0.60      1.15


Sincerely

Deb Montgomery

Department of Fisheries and Wildlife Resources
University of Idaho
Moscow, Idaho 83843
208-885-4008
mont4260 at uidaho.edu



From scott.waichler at verizon.net  Wed Apr 21 07:01:44 2004
From: scott.waichler at verizon.net (Scott Waichler)
Date: Tue, 20 Apr 2004 22:01:44 -0700
Subject: [R] Extracting information from webpages
Message-ID: <NEBBIKBOJBDOECLOBANEMEMICMAA.scott.waichler@verizon.net>

> Hello R Community,
> 
> I would like to be able to download recent (yesterday's close is fine)
> stock and 
> mutual fund prices from somewhere and use them for a personal finance
> project. 
> Ideally, I would do this at a website of my choosing (e.g.
> morningstar.com) and
> have the possibility of getting a wide range of other information about
> the security 
> as well.  I see there is a package httpRequest that should allow me to 
> retrieve the webpage with an posted security symbol.  I looked at package 
> XML for processing the returned webpage, but it didn't work on an 
> example *.asp file that I tried.  I also checked out the nascent package
> fBasics, 
> but I didn't find what I'm looking for.  I know there is a Perl module
> Finance::Quote; 
> should I go that route and avoid trying to parse webpages?  I was hoping
> to learn 
> how to extract information from webpages in general so I could apply the
> techniques 
> for other purposes too.
> 
> Thanks,
> Scott Waichler
> scott at lifetime.oregonstate.edu
> ***********************************************************************
> 

From ggrothendieck at myway.com  Wed Apr 21 07:19:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 21 Apr 2004 05:19:23 +0000 (UTC)
Subject: [R] Extracting information from webpages
References: <NEBBIKBOJBDOECLOBANEMEMICMAA.scott.waichler@verizon.net>
Message-ID: <loom.20040421T071809-109@post.gmane.org>


Check out get.hist.quote in package tseries.

Scott Waichler <scott.waichler <at> verizon.net> writes:

: 
: > Hello R Community,
: > 
: > I would like to be able to download recent (yesterday's close is fine)
: > stock and 
: > mutual fund prices from somewhere and use them for a personal finance
: > project. 
: > Ideally, I would do this at a website of my choosing (e.g.
: > morningstar.com) and
: > have the possibility of getting a wide range of other information about
: > the security 
: > as well.  I see there is a package httpRequest that should allow me to 
: > retrieve the webpage with an posted security symbol.  I looked at package 
: > XML for processing the returned webpage, but it didn't work on an 
: > example *.asp file that I tried.  I also checked out the nascent package
: > fBasics, 
: > but I didn't find what I'm looking for.  I know there is a Perl module
: > Finance::Quote; 
: > should I go that route and avoid trying to parse webpages?  I was hoping
: > to learn 
: > how to extract information from webpages in general so I could apply the
: > techniques 
: > for other purposes too.
: > 
: > Thanks,
: > Scott Waichler
: > scott <at> lifetime.oregonstate.edu
: > ***********************************************************************
: > 
: 
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From robert.king at newcastle.edu.au  Wed Apr 21 08:21:17 2004
From: robert.king at newcastle.edu.au (Robert King)
Date: Wed, 21 Apr 2004 16:21:17 +1000 (EST)
Subject: [R] Assigning functions in Rprofile
Message-ID: <Pine.LNX.4.58.0404211613320.19909@tolstoy.newcastle.edu.au>

I use the ugly hack shown below as my .Rprofile (mainly because I couldn't
find any other way to spawn a new mozilla window on starting R or calling
help.start(), but then send all the help html documents to that new
window).

With a recent change from a cvs checkout to the released 1.9.0 I find that
the new hs() appears if there is nothing of that name in .RData, but if
there is the old hs() there, it just stays.

What is the cause of this?

Thanks,
Robert.

I'm using a debian i386 system.

.Rprofile
=========
library(gld)
options(pager="less",CRAN="ftp://mirror.aarnet.edu.au/pub/cran")
hs <- function(){
if (nchar(system("echo $DISPLAY",intern=T))==0) # no X
        {options(htmlhelp=FALSE)
        ret <- "no X"}
else    {system("mozilla") # we have X - I'm starting this to get a new
        # window for the R session, without starting a new window every
time
        options(pdfviewer="/usr/local/bin/acroread",browser="mozilla")
        help.start()
        ret <- "X"}
ret
}


----
Robert King, Statistics, School of Mathematical & Physical Sciences,
University of Newcastle, Australia
Room V133  ph +61 2 4921 5548
Robert.King at newcastle.edu.au   http://maths.newcastle.edu.au/~rking/

He defended the cause of the poor and needy, and so all went well. Is that
not what it means to know me?" declares the LORD.
        -- Jeremiah 22:16



From Mathieu.Vuilleumier at unine.ch  Wed Apr 21 08:34:00 2004
From: Mathieu.Vuilleumier at unine.ch (VUILLEUMIER Mathieu)
Date: Wed, 21 Apr 2004 08:34:00 +0200
Subject: [R] two stage level least square in R 
Message-ID: <BD1D7341BE3930408509F95C86A451CBC94DA7@mail1.UNINE.CH>

I m in front of a problem of simultaneity bias between two equations and would like to apply the two stage level least square....Is there a special command in R ? I didn't found something about that in the R help.

Thanks for your help !!!!



**************************************************************
Mathieu Vuilleumier - collaborateur scientifique
Institut de recherches ??conomique et r??gionale (IRER)
Universit?? de Neuch??tel
Pierre-??-Mazel 7, CH-2000 Neuch??tel
Tel : 032/ 718 14 66
E-mail : mathieu.vuilleumier at unine.ch



From ligges at statistik.uni-dortmund.de  Wed Apr 21 08:35:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Apr 2004 08:35:12 +0200
Subject: [OT] Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404210942350.25386-100000@stat71.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0404210942350.25386-100000@stat71.stat.auckland.ac.nz>
Message-ID: <40861620.6080502@statistik.uni-dortmund.de>

David Scott wrote:

> On Tue, 20 Apr 2004, Brett Melbourne wrote:
> 
> 
>>That exact same thing is happening to me too. It seems to be intermittent.
>>After a reboot, it might (but might not) fix itself. My older versions of R
>>also no longer work. R1.8.1 fails with the windows message "R for Windows
>>GUI front-end has encountered a problem and needs to close.  We are sorry
>>for the inconvenience."
>>
>>This appears to have occured to me after installing the latest Microsoft
>>Windows critical updates. I have reinstalled R1.8.1 and R1.9.0 to no avail.
>>None of the other programs on my computer are affected, only R.
>>
>>Is anybody else experiencing this?
>>
> 
> 
> Yes. With 1.8.1 if the updates are installed R fails as you indicate. If 
> the updates are taken off it works again. Bit of a problem here because 
> the University IT Security runs checks on the updates and complains to 
> users who don't have them installed.
> 

Yep. Those updates are a problem. We have seen bluescreens (!) on 
machines running a directCD service on startup. So not only R is 
affected. I guess Microsoft will provide updates within the next few months.

Uwe Ligges



> 
>>Brett
>>Brett Melbourne, Postdoctoral Fellow
>>Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
>>Center for Population Biology, Storer Hall
>>University of California Davis CA 95616
>>
> 
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
> 		The University of Auckland, PB 92019
> 		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz 
> 
> 
> Graduate Officer, Department of Statistics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Apr 21 08:44:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 07:44:50 +0100 (BST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404210942350.25386-100000@stat71.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0404210743290.18446-100000@gannet.stats>

Has this anything to do with `Invalid HOMEDRIVE', as given in the subject 
line?  R can only fail with one error message, so which is it, please?

On Wed, 21 Apr 2004, David Scott wrote:

> On Tue, 20 Apr 2004, Brett Melbourne wrote:
> 
> > That exact same thing is happening to me too. It seems to be intermittent.
> > After a reboot, it might (but might not) fix itself. My older versions of R
> > also no longer work. R1.8.1 fails with the windows message "R for Windows
> > GUI front-end has encountered a problem and needs to close.  We are sorry
> > for the inconvenience."
> > 
> > This appears to have occured to me after installing the latest Microsoft
> > Windows critical updates. I have reinstalled R1.8.1 and R1.9.0 to no avail.
> > None of the other programs on my computer are affected, only R.
> > 
> > Is anybody else experiencing this?
> >
> 
> Yes. With 1.8.1 if the updates are installed R fails as you indicate. If 
> the updates are taken off it works again. Bit of a problem here because 
> the University IT Security runs checks on the updates and complains to 
> users who don't have them installed.
> 
>  
> > Brett
> > Brett Melbourne, Postdoctoral Fellow
> > Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
> > Center for Population Biology, Storer Hall
> > University of California Davis CA 95616
> > 
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
> 		The University of Auckland, PB 92019
> 		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz 
> 
> 
> Graduate Officer, Department of Statistics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Apr 21 08:49:47 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 21 Apr 2004 08:49:47 +0200
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404210942350.25386-100000@stat71.stat.auckland.ac.nz>
References: <00c301c42716$fe456120$1313eda9@des.ucdavis.edu>
	<Pine.LNX.4.44.0404210942350.25386-100000@stat71.stat.auckland.ac.nz>
Message-ID: <16518.6539.413640.921830@gargle.gargle.HOWL>

>>>>> "DScottNZ" == David Scott <d.scott at auckland.ac.nz>
>>>>>     on Wed, 21 Apr 2004 09:45:31 +1200 (NZST) writes:

    DScottNZ> On Tue, 20 Apr 2004, Brett Melbourne wrote:

    >> That exact same thing is happening to me too. It seems to
    >> be intermittent.  After a reboot, it might (but might
    >> not) fix itself. My older versions of R also no longer
    >> work. R1.8.1 fails with the windows message "R for
    >> Windows GUI front-end has encountered a problem and needs
    >> to close.  We are sorry for the inconvenience."
    >> 
    >> This appears to have occured to me after installing the
    >> latest Microsoft Windows critical updates. I have
    >> reinstalled R1.8.1 and R1.9.0 to no avail.  None of the
    >> other programs on my computer are affected, only R.
    >> 
    >> Is anybody else experiencing this?

    DScottNZ> Yes. With 1.8.1 if the updates are installed R
    DScottNZ> fails as you indicate. If the updates are taken
    DScottNZ> off it works again. Bit of a problem here because
    DScottNZ> the University IT Security runs checks on the
    DScottNZ> updates and complains to users who don't have them
    DScottNZ> installed.

One logical conclusion seems that Microsoft has found that
R must be a virus since it's spreading rapidly around the globe.

So with the newest security patches they prevent the virus from
doing damage...

Or, maybe it's not just R, but Free Software in general which is
such a threat to MS?  ;-)



From ligges at statistik.uni-dortmund.de  Wed Apr 21 08:52:02 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Apr 2004 08:52:02 +0200
Subject: [R] Assigning functions in Rprofile
In-Reply-To: <Pine.LNX.4.58.0404211613320.19909@tolstoy.newcastle.edu.au>
References: <Pine.LNX.4.58.0404211613320.19909@tolstoy.newcastle.edu.au>
Message-ID: <40861A12.1060306@statistik.uni-dortmund.de>

Robert King wrote:

> I use the ugly hack shown below as my .Rprofile (mainly because I couldn't
> find any other way to spawn a new mozilla window on starting R or calling
> help.start(), but then send all the help html documents to that new
> window).
> 
> With a recent change from a cvs checkout to the released 1.9.0 I find that
> the new hs() appears if there is nothing of that name in .RData, but if
> there is the old hs() there, it just stays.
> 
> What is the cause of this?

Because .Rprofile is loaded before .RData. Hence tho "old" version 
overwrites the new one ...

Uwe Ligges


> Thanks,
> Robert.
> 
> I'm using a debian i386 system.
> 
> .Rprofile
> =========
> library(gld)
> options(pager="less",CRAN="ftp://mirror.aarnet.edu.au/pub/cran")
> hs <- function(){
> if (nchar(system("echo $DISPLAY",intern=T))==0) # no X
>         {options(htmlhelp=FALSE)
>         ret <- "no X"}
> else    {system("mozilla") # we have X - I'm starting this to get a new
>         # window for the R session, without starting a new window every
> time
>         options(pdfviewer="/usr/local/bin/acroread",browser="mozilla")
>         help.start()
>         ret <- "X"}
> ret
> }
> 
> 
> ----
> Robert King, Statistics, School of Mathematical & Physical Sciences,
> University of Newcastle, Australia
> Room V133  ph +61 2 4921 5548
> Robert.King at newcastle.edu.au   http://maths.newcastle.edu.au/~rking/
> 
> He defended the cause of the poor and needy, and so all went well. Is that
> not what it means to know me?" declares the LORD.
>         -- Jeremiah 22:16
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gb at stat.umu.se  Wed Apr 21 08:49:08 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 21 Apr 2004 08:49:08 +0200
Subject: [R] install.packages and warnings
Message-ID: <20040421064908.GA25078@stat.umu.se>

When I install packages from CRAN I sometimes get a hint about warnings:

---------------------------------------
> install.packages(c("Hmisc", "Design"))
......
  which.influence                   text    html    latex   example
There were 14 warnings (use warnings() to see them)
* DONE (Design)

Delete downloaded files (y/N)? y

> warnings()
NULL
---------------------------------------
Where did the warnings go? (R-1.9.0, Debian testing/unstable)

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From ripley at stats.ox.ac.uk  Wed Apr 21 09:09:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 08:09:20 +0100 (BST)
Subject: [R] Assigning functions in Rprofile
In-Reply-To: <Pine.LNX.4.58.0404211613320.19909@tolstoy.newcastle.edu.au>
Message-ID: <Pine.LNX.4.44.0404210805100.18446-100000@gannet.stats>

On Wed, 21 Apr 2004, Robert King wrote:

> I use the ugly hack shown below as my .Rprofile (mainly because I couldn't
> find any other way to spawn a new mozilla window on starting R or calling
> help.start(), but then send all the help html documents to that new
> window).
> 
> With a recent change from a cvs checkout to the released 1.9.0 I find that
> the new hs() appears if there is nothing of that name in .RData, but if
> there is the old hs() there, it just stays.
> 
> What is the cause of this?

?Startup tells you that .RData is loaded after .Rprofile is executed, but 
it has been that way for quite a while.  So this is behaving as 
documented, AFAICS.

What is new is that help.start() is now in package utils so you should be 
using utils::help.start() to be safe.

> 
> Thanks,
> Robert.
> 
> I'm using a debian i386 system.
> 
> .Rprofile
> =========
> library(gld)
> options(pager="less",CRAN="ftp://mirror.aarnet.edu.au/pub/cran")
> hs <- function(){
> if (nchar(system("echo $DISPLAY",intern=T))==0) # no X
>         {options(htmlhelp=FALSE)
>         ret <- "no X"}
> else    {system("mozilla") # we have X - I'm starting this to get a new
>         # window for the R session, without starting a new window every
> time
>         options(pdfviewer="/usr/local/bin/acroread",browser="mozilla")
>         help.start()
>         ret <- "X"}
> ret
> }

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gb at stat.umu.se  Wed Apr 21 09:07:07 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 21 Apr 2004 09:07:07 +0200
Subject: [R] difference between coxph and cph
In-Reply-To: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>
References: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>
Message-ID: <20040421070707.GB25078@stat.umu.se>

On Tue, Apr 20, 2004 at 10:01:36PM -0700, Deb Montgomery wrote:
> Hi. I am using Windows version of R 1.8.1. Being somewhat new to survival
> analysis, I am trying to compare cph (Design) with coxph (survival) for use
> with a survival data set.
> 
> I was wondering why cph and coxph provide me with different confidence
> intervals
> for the hazard ratios for one of the variables. I was wondering if I am
> doing something wrong? Or if the two functions are calculating hazard ratios
> and the 95% confidence intervals differently? 

Yes, for 'Weight.at.age.4' you get differing parameter estimates (0.91
versus 0.83). Want to know the correct answer? Try 'coxreg' in  package
'eha'! :-) 

More seriously, the difference may well be of numerical character,
different convergence criteria, "unbalanced" data, etc. It is really
impossible to say without knowing what your data are (and without looking
into the code of coxph and cph).

> I have listed part of the code
> and part of the results from the 2 functions. Sorry if this question is a
> repeat, I didn't find it when I searched the archives.
> 
> ###########################################################
> # s= Surv(Time1, Time2, censor)
> #f= coxph(s~  Siblings + Weight.at.age.4)
> #summary(f)
> #Call:
> #coxph(formula = s ~ Siblings + Weight.at.age.4)
> # n= 132
> #                exp(coef) exp(-coef) lower .95 upper .95
> #Siblings            1.52      0.657     0.815      2.84
> #Weight.at.age.4      0.91      1.099     0.772      1.07
> ##############################################################
> #s= Surv(Time1, Time2, censor)
> #f= cph(s~  Siblings + Weight.at.age.4,surv=TRUE ,  x=T, y = T)
> # summary(f)
> #             Effects              Response : s
> # Factor          Low   High   Diff.  Effect S.E. Lower 0.95 Upper 0.95
> # Siblings       0.000  1.000 1.0000  0.42  0.32 -0.20      1.04
> #  Hazard Ratio   0.000  1.000 1.0000  1.52    NA  0.82      2.84
> # Weight.at.age.4 8.613 10.602 1.9885 -0.19  0.17 -0.51      0.14
> # Hazard Ratio   8.613 10.602 1.9885  0.83    NA  0.60      1.15
> 
> 
> Sincerely
> 
> Deb Montgomery
> 
> Department of Fisheries and Wildlife Resources
> University of Idaho
> Moscow, Idaho 83843
> 208-885-4008
> mont4260 at uidaho.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From ripley at stats.ox.ac.uk  Wed Apr 21 09:17:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 08:17:34 +0100 (BST)
Subject: [R] install.packages and warnings
In-Reply-To: <20040421064908.GA25078@stat.umu.se>
Message-ID: <Pine.LNX.4.44.0404210814040.18446-100000@gannet.stats>

These are probably indicating many files lacking EOL on the last line. It 
comes from

  echo ".installPackageIndices(\".\", \"${R_PACKAGE_DIR}\")" | \
    R_DEFAULT_PACKAGES=tools ${R_EXE} --vanilla >/dev/null
  if test ${?} -ne 0; then
    error "installing package indices failed"
    do_exit_on_error
  fi

AFAICS.

On Wed, 21 Apr 2004, G??ran Brostr??m wrote:

> When I install packages from CRAN I sometimes get a hint about warnings:
> 
> ---------------------------------------
> > install.packages(c("Hmisc", "Design"))
> ......
>   which.influence                   text    html    latex   example
> There were 14 warnings (use warnings() to see them)
> * DONE (Design)
> 
> Delete downloaded files (y/N)? y
> 
> > warnings()
> NULL
> ---------------------------------------
> Where did the warnings go? (R-1.9.0, Debian testing/unstable)
> 
> G??ran
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Wed Apr 21 10:29:08 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 21 Apr 2004 10:29:08 +0200
Subject: [R] Size of R user base
In-Reply-To: <200404210155.i3L1te9M362709@atlas.otago.ac.nz>
Message-ID: <MABBLJDICACNFOLGIHJOIEJLEFAA.phgrosjean@sciviews.org>

Richard A. O'Keefe wrote:

>[...]
>The first rule of measurement is "Don't bother with a measurement if you
>don't know what you're going to do with the answer".  If you knew the
>number of "R users", however defined, how would that actually help you?

Obviously, there are lots of statisticians and people used to work with
measurements... and who know how it can be difficult to use, and interpret
them (is it surprising in R-Help mailing list?). However, I think that the
*change in time* of this number (as soon as it is somehow representative to
the reality, that is, an unbiased measurement of effective use of the
software) is the interesting phenomenon. I would be very interested to have
this figure: change in time of R users.

Now, it appears that:
1) For ethical reasons, it is not conceivable to place a "spyware", as some
call it, in R and
2) for obvious reasons, we would never have unbiased estimation of the
evaluation of the number of users if the process ask each user if he is
willing to send this information, or not.

So, what do we have? Not much, except the number of books about R sold (this
fluctuation in time is not representative, because there are also more books
about R now than in the past,... and for other reasons).

We have also the activity in the R-help mailing list, which could be
representative of the most active users, certainly. Does anyone have of
figure of the number of messages in R-Help with time since its creation? (it
is probably available somewhere, but I don't know where).

Still, this is not representative at all of occasional users that probably
represent the largest fraction. I am particularly interested by them.
Especially because one of my works focus on GUI, that is, something supposed
to ease access and help occasional users to get into the software!
Best,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From maj at stats.waikato.ac.nz  Wed Apr 21 10:39:26 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Wed, 21 Apr 2004 20:39:26 +1200
Subject: [R] Rgui front-end has encountered a problem and needs to close
Message-ID: <4086333E.5080509@stats.waikato.ac.nz>

Well I don't know if anyone can help with this but it will be 
interesting to know if others have had the same problem.

I can't start R at home on my laptop [ I'm using 1.8.1 under Windows 
XP]. When I click on the shortcut I get the usual Windows box for when 
an application needs to close. A couple of clicks down it displays the 
following:

Error signature
AppName: rgui.exe	 AppVer: 1.81.31121.0	 ModName: msvcrt.dll
ModVer: 7.0.2600.1106	 Offset: 0003213b

as well as another uncopyable window containing a large amount of binary.

I was going to re-install R at work today but the thing worked OK. But 
back home tonight I get the same problem! The only difference between 
home and work is that at work I'm connected to a LAN.

Cheers,

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From nusbj at hotmail.com  Wed Apr 21 10:41:40 2004
From: nusbj at hotmail.com (Z P)
Date: Wed, 21 Apr 2004 16:41:40 +0800
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
Message-ID: <Sea2-F23ez9Ursy9Lw700013b3a@hotmail.com>

it is for windows. R can not start, one information windows says fatal 
error: invalid homedrive.




>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>To: David Scott <d.scott at auckland.ac.nz>
>CC: Joann Williamson <JoannW at usca.edu>, r-help at stat.math.ethz.ch
>Subject: Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE
>Date: Wed, 21 Apr 2004 07:44:50 +0100 (BST)
>
>Has this anything to do with `Invalid HOMEDRIVE', as given in the subject
>line?  R can only fail with one error message, so which is it, please?
>
>On Wed, 21 Apr 2004, David Scott wrote:
>
> > On Tue, 20 Apr 2004, Brett Melbourne wrote:
> >
> > > That exact same thing is happening to me too. It seems to be 
>intermittent.
> > > After a reboot, it might (but might not) fix itself. My older versions 
>of R
> > > also no longer work. R1.8.1 fails with the windows message "R for 
>Windows
> > > GUI front-end has encountered a problem and needs to close.  We are 
>sorry
> > > for the inconvenience."
> > >
> > > This appears to have occured to me after installing the latest 
>Microsoft
> > > Windows critical updates. I have reinstalled R1.8.1 and R1.9.0 to no 
>avail.
> > > None of the other programs on my computer are affected, only R.
> > >
> > > Is anybody else experiencing this?
> > >
> >
> > Yes. With 1.8.1 if the updates are installed R fails as you indicate. If
> > the updates are taken off it works again. Bit of a problem here because
> > the University IT Security runs checks on the updates and complains to
> > users who don't have them installed.
> >
> >
> > > Brett
> > > Brett Melbourne, Postdoctoral Fellow
> > > Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
> > > Center for Population Biology, Storer Hall
> > > University of California Davis CA 95616
> > >
> > _________________________________________________________________
> > David Scott	Department of Statistics, Tamaki Campus
> > 		The University of Auckland, PB 92019
> > 		Auckland	NEW ZEALAND
> > Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> > Email:	d.scott at auckland.ac.nz
> >
> >
> > Graduate Officer, Department of Statistics
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
> >
> >
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ernesto at ipimar.pt  Wed Apr 21 10:55:08 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 21 Apr 2004 09:55:08 +0100
Subject: [R] Size of R user base
In-Reply-To: <4085744A.2020700@aon.at>
References: <4084D3E5.17583.1C5C02@localhost> <x2d6626eyh.fsf@biostat.ku.dk>
	<4085744A.2020700@aon.at>
Message-ID: <1082537708.8800.8.camel@gandalf.local>

Hi,

I agree with Christian. The monitoring must be done in a ACTIVE way by
the user.

I usually don't use software that has that kind of features and it
annoys me to know that my computer is sending information to someone
whithout my knowledge or permission.

Best regards

EJ 

On Tue, 2004-04-20 at 20:04, cstrato wrote:
> At first I want to make clear that I have always paid for the
> software that I use, I am even paying for the shareware that I use.
> 
> After this statement let me come to the current discussion:
> It is really shocking that some people even think about integrating
> a spyware system into an open source software project.
> The internet has started as a chaotic and democratic system, but
> is in danger to become the worst "Big Brother is watching you"
> assistant to certain governments and companies, which become a
> danger for the democracy and for peace.
> 
> I really hope that the R core developer will never consider such
> a dangerous option!
> 
> Having said this, I am willing to ACTIVELY send an e-mail to
> CRAN or R to tell that I am using R.
> 
> R is such a great project, developed by a great community in a
> democratic manner, showing how well democracy can work. Adding
> a monitoring system would destroy the whole project.
> 
> P.S.: Personally, I have even disabled cookies from my browser,
> both at home and at the company, and I must say, that very seldom
> would I need to enable cookies. If a company web-site forces me
> to enable cookies, than I ignore this company: companies want to
> sell products to customers, and with such a behavior they are
> hopefully loosing customers.
> 
> Best regards
> Christian
> -.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
> C.h.r.i.s.t.i.a.n. .S.t.r.a.t.o.w.a
> V.i.e.n.n.a.         .A.u.s.t.r.i.a
> -.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
> 
> 
> 
> Peter Dalgaard wrote:
> > kjetil at entelnet.bo writes:
> > 
> > 
> >>On 20 Apr 2004 at 10:47, Philippe Grosjean wrote:
> > 
> > 
> >>>Of course, this will only work with computers connected to the
> >>>internet,... but at least, it could be one way to evaluate the number
> >>>of R users. Would that be an infringment of Open Source, or any other
> >>>rule of freedom? I don't know, but it does seem to be quite widespread
> >>>(at least for commercial software). so, why an Open Source software
> >>>would not be able to monitor the number of users?
> >>
> >>That would make R into spyware, and there exist software to monitore 
> >>and warn aganst/automatically remove spyware, and some users have 
> >>such installed (and it will grow).
> > 
> > 
> > Not quite spyware. Spyware generally works in more covert ways and
> > tries to hide itself from the user.
> > 
> > However, quite a few people think that having programs connecting to
> > places on the internet without being asked is annoying and a potential
> > invasion of privacy and it may even cost people money if they're on a
> > dialup non-flat-fee connection.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From nusbj at hotmail.com  Wed Apr 21 10:53:37 2004
From: nusbj at hotmail.com (Z P)
Date: Wed, 21 Apr 2004 16:53:37 +0800
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
Message-ID: <Sea2-F349mK6PYzDabi00030971@hotmail.com>

I find more interesting situation. My laptop (toshiba tecra9100) can run R 
1.9.0 and the earlier version well when I connect to internet. The same 
wrong error appears when I disconnect the internet.

One IBM laptop can not run R in any situation and one FUJITSE laptop never 
met any error message.

I wonder whether the R development group can contact Microsoft to fix this 
problem. R is really powerful as compared with S+.


>From: Martin Maechler <maechler at stat.math.ethz.ch>
>Reply-To: Martin Maechler <maechler at stat.math.ethz.ch>
>To: r-help at stat.math.ethz.ch
>Subject: Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE
>Date: Wed, 21 Apr 2004 08:49:47 +0200
>
> >>>>> "DScottNZ" == David Scott <d.scott at auckland.ac.nz>
> >>>>>     on Wed, 21 Apr 2004 09:45:31 +1200 (NZST) writes:
>
>     DScottNZ> On Tue, 20 Apr 2004, Brett Melbourne wrote:
>
>     >> That exact same thing is happening to me too. It seems to
>     >> be intermittent.  After a reboot, it might (but might
>     >> not) fix itself. My older versions of R also no longer
>     >> work. R1.8.1 fails with the windows message "R for
>     >> Windows GUI front-end has encountered a problem and needs
>     >> to close.  We are sorry for the inconvenience."
>     >>
>     >> This appears to have occured to me after installing the
>     >> latest Microsoft Windows critical updates. I have
>     >> reinstalled R1.8.1 and R1.9.0 to no avail.  None of the
>     >> other programs on my computer are affected, only R.
>     >>
>     >> Is anybody else experiencing this?
>
>     DScottNZ> Yes. With 1.8.1 if the updates are installed R
>     DScottNZ> fails as you indicate. If the updates are taken
>     DScottNZ> off it works again. Bit of a problem here because
>     DScottNZ> the University IT Security runs checks on the
>     DScottNZ> updates and complains to users who don't have them
>     DScottNZ> installed.
>
>One logical conclusion seems that Microsoft has found that
>R must be a virus since it's spreading rapidly around the globe.
>
>So with the newest security patches they prevent the virus from
>doing damage...
>
>Or, maybe it's not just R, but Free Software in general which is
>such a threat to MS?  ;-)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From bamelbourne at ucdavis.edu  Wed Apr 21 10:56:58 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Wed, 21 Apr 2004 01:56:58 -0700
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
References: <Pine.LNX.4.44.0404210743290.18446-100000@gannet.stats>
Message-ID: <003901c4277e$9b42def0$0213eda9@des.ucdavis.edu>

Hi Brian,

R1.9.0 fails with the message  "Fatal error: Invalid HOMEDRIVE" and no
prompt to send an error report to Microsoft.

R1.8.1 fails with the message "R for Windows GUI front-end has encountered a
problem and needs to close.  We are sorry for the inconvenience", included
in a prompt to send an error report to Microsoft.

For both versions, R fails to start at all. I'm running Windows XP Pro, and
like David Scott I'm supposed to install the XP critical updates as soon as
they come out, which I did last week.

I suppose, given the other reports of this behaviour, the subject line
should be "R fails after installing Windows XP updates".

Cheers
Brett


----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "David Scott" <d.scott at auckland.ac.nz>
Cc: "Brett Melbourne" <bamelbourne at ucdavis.edu>; "Joann Williamson"
<JoannW at usca.edu>; <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 20, 2004 11:44 PM
Subject: Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE


> Has this anything to do with `Invalid HOMEDRIVE', as given in the subject
> line?  R can only fail with one error message, so which is it, please?
>
> On Wed, 21 Apr 2004, David Scott wrote:
>
> > On Tue, 20 Apr 2004, Brett Melbourne wrote:
> >
> > > That exact same thing is happening to me too. It seems to be
intermittent.
> > > After a reboot, it might (but might not) fix itself. My older versions
of R
> > > also no longer work. R1.8.1 fails with the windows message "R for
Windows
> > > GUI front-end has encountered a problem and needs to close.  We are
sorry
> > > for the inconvenience."
> > >
> > > This appears to have occured to me after installing the latest
Microsoft
> > > Windows critical updates. I have reinstalled R1.8.1 and R1.9.0 to no
avail.
> > > None of the other programs on my computer are affected, only R.
> > >
> > > Is anybody else experiencing this?
> > >
> >
> > Yes. With 1.8.1 if the updates are installed R fails as you indicate. If
> > the updates are taken off it works again. Bit of a problem here because
> > the University IT Security runs checks on the updates and complains to
> > users who don't have them installed.
> >
> >
> > > Brett
> > > Brett Melbourne, Postdoctoral Fellow
> > > Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
> > > Center for Population Biology, Storer Hall
> > > University of California Davis CA 95616
> > >
> > _________________________________________________________________
> > David Scott Department of Statistics, Tamaki Campus
> > The University of Auckland, PB 92019
> > Auckland NEW ZEALAND
> > Phone: +64 9 373 7599 ext 86830 Fax: +64 9 373 7000
> > Email: d.scott at auckland.ac.nz
> >
> >
> > Graduate Officer, Department of Statistics
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bamelbourne at ucdavis.edu  Wed Apr 21 10:59:26 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Wed, 21 Apr 2004 01:59:26 -0700
Subject: [R] Rgui front-end has encountered a problem and needs to close
References: <4086333E.5080509@stats.waikato.ac.nz>
Message-ID: <003f01c4277e$f34f7590$0213eda9@des.ucdavis.edu>

Hi Murray,
This looks like the same problem I'm having. Did you recently install
Windows XP critical updates? That appears to have caused the problem on my
machine.
Brett

Brett Melbourne, Postdoctoral Fellow
Biological Invasions IGERT www.cpb.ucdavis.edu/bioinv
Center for Population Biology, Storer Hall
University of California Davis CA 95616


----- Original Message ----- 
From: "Murray Jorgensen" <maj at stats.waikato.ac.nz>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 21, 2004 1:39 AM
Subject: [R] Rgui front-end has encountered a problem and needs to close


> Well I don't know if anyone can help with this but it will be
> interesting to know if others have had the same problem.
>
> I can't start R at home on my laptop [ I'm using 1.8.1 under Windows
> XP]. When I click on the shortcut I get the usual Windows box for when
> an application needs to close. A couple of clicks down it displays the
> following:
>
> Error signature
> AppName: rgui.exe AppVer: 1.81.31121.0 ModName: msvcrt.dll
> ModVer: 7.0.2600.1106 Offset: 0003213b
>
> as well as another uncopyable window containing a large amount of binary.
>
> I was going to re-install R at work today but the thing worked OK. But
> back home tonight I get the same problem! The only difference between
> home and work is that at work I'm connected to a LAN.
>
> Cheers,
>
> Murray
>
> -- 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tpapp at axelero.hu  Wed Apr 21 11:10:00 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Wed, 21 Apr 2004 11:10:00 +0200
Subject: [R] resetting the interpreter
Message-ID: <20040421091000.GA1665@localhost>

Hi,

Sometimes I want to "reset" the R interpreter (as if I quit and
started it again, with everything starting from scratch).  The main
reason for this is when I expreriment with calculations, and later
form them into general functions, sometimes I forget to include
variables or reset some settings which I don't notice because they are
already set.  This causes problems during batch calculations, and a
lot of wasted time.

Currently I am using rm(list=ls()), but it only cleans up the
namespace.  Quitting and restarting is slow.  What I am looking for is
a function that resets the R interpreter without restarting it.

I did search the archives, but maybe I am using the wrong keywords:
reset gives me a lot of results.  I would appreciate any advice on
this, ie if there is no function to do this, how people cope with this
problem.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From B.Rowlingson at lancaster.ac.uk  Wed Apr 21 11:14:16 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 21 Apr 2004 10:14:16 +0100
Subject: [R] multi-user engine
In-Reply-To: <854qre5jwl.fsf@servant.blindglobe.net>
References: <MBBBIIHJANJBMHLGMACKKEAMCGAA.binabina@bellsouth.net>
	<854qre5jwl.fsf@servant.blindglobe.net>
Message-ID: <40863B68.1070909@lancaster.ac.uk>

A.J. Rossini wrote:
> "zubin" <binabina at bellsouth.net> writes:
> 
> 
>>I am presently a SAS user and wanted to configure R to work in a multi-user
>>enteprise environment.  Client - Server.  Where we have a strong LINUX
>>server supporting about 10 statisticians with R.  Anyone have any backround
>>or information they can share to help me get jump-started on setting up R in
>>this environment? 

  Why can't you install R in the usual way on the server and give the 
statisticians (there's only 10 of them, so its not much admin) accounts 
on the server? They then login (using an SSH client) and get graphics 
back via X-windows. Free SSH clients and X-windows servers are available 
for all the usual platforms. Their filestore would be on the server. I 
just don't see the advantage (and see plenty of disadvantages) of 
complicating things with a client-server framework where it might not be 
necessary.

  Or does your 'enterprise environment' not permit such things?

  Barry



From bamelbourne at ucdavis.edu  Wed Apr 21 11:13:16 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Wed, 21 Apr 2004 02:13:16 -0700
Subject: [R] Rgui front-end has encountered a problem and needs to close
References: <4086333E.5080509@stats.waikato.ac.nz>
Message-ID: <006f01c42780$e2654000$0213eda9@des.ucdavis.edu>

I get the same message and error signature on failing to start 1.8.1. If I
continue through to debug in VC++6, I get the message
"Unhandled exception in Rgui.exe (MSVRCT.DLL): 0xC0000005: Access
Violation."
cheers
Brett

----- Original Message ----- 
From: "Murray Jorgensen" <maj at stats.waikato.ac.nz>
To: "R-help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, April 21, 2004 1:39 AM
Subject: [R] Rgui front-end has encountered a problem and needs to close


> Well I don't know if anyone can help with this but it will be
> interesting to know if others have had the same problem.
>
> I can't start R at home on my laptop [ I'm using 1.8.1 under Windows
> XP]. When I click on the shortcut I get the usual Windows box for when
> an application needs to close. A couple of clicks down it displays the
> following:
>
> Error signature
> AppName: rgui.exe AppVer: 1.81.31121.0 ModName: msvcrt.dll
> ModVer: 7.0.2600.1106 Offset: 0003213b
>
> as well as another uncopyable window containing a large amount of binary.
>
> I was going to re-install R at work today but the thing worked OK. But
> back home tonight I get the same problem! The only difference between
> home and work is that at work I'm connected to a LAN.
>
> Cheers,
>
> Murray
>
> -- 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Apr 21 11:21:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 10:21:36 +0100 (BST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <003901c4277e$9b42def0$0213eda9@des.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0404211005170.9702-100000@gannet.stats>

Thanks for the clarification.  We have several machines which have all the
critical updates on (and one XP machine with *all* the updates) and both
1.8.1 and 1.9.0 work properly on all the machines I tested this morning.  
So there is some additional factor.

If anyone who has the problem knows which critical update caused it (David
Scott), it might help others with the problem.

It might be worth trying setting the environment variable R_USER or HOME
(which I have set, but I have also tested after unsetting it).


For the record, it looks like the C entry point getenv() has been broken,
and since 1.9.0 introduced a length check on the result of getenv it 
terminates `cleanly' whereas 1.8.1 segfaults.  Or possibly the setting of 
environment variables (HOMEDRIVE and HOMEPATH) has been broken, which 
would amount to the same thing.


On Wed, 21 Apr 2004, Brett Melbourne wrote:

> Hi Brian,
> 
> R1.9.0 fails with the message  "Fatal error: Invalid HOMEDRIVE" and no
> prompt to send an error report to Microsoft.
> 
> R1.8.1 fails with the message "R for Windows GUI front-end has encountered a
> problem and needs to close.  We are sorry for the inconvenience", included
> in a prompt to send an error report to Microsoft.
> 
> For both versions, R fails to start at all. I'm running Windows XP Pro, and
> like David Scott I'm supposed to install the XP critical updates as soon as
> they come out, which I did last week.
> 
> I suppose, given the other reports of this behaviour, the subject line
> should be "R fails after installing Windows XP updates".
> 
> Cheers
> Brett

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Wed Apr 21 12:02:53 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 21 Apr 2004 12:02:53 +0200
Subject: [R] benchmark dual amd opteron
In-Reply-To: <6rzn965rfc.fsf@bates4.stat.wisc.edu>
Message-ID: <MABBLJDICACNFOLGIHJOAEJOEFAA.phgrosjean@sciviews.org>

>Douglas Bates <bates at stat.wisc.edu> writes:

>> "Liaw, Andy" <andy_liaw at merck.com> writes:
>>
>> > It doesn't run as is in R-1.9.0.  R-1.9.0 doesn't like
`eigen.default()',
>> > and after changing that to `eigen()', it can't find Matrix.class().
>>
>> I've changed the Matrix package since that benchmark was written.
>> I'll rewrite the benchmark and submit it to Phillippe Grosjean.

>I have added calls to invisible(gc()) before the timings that involve
>large objects so the calculation doesn't get penalized for repeated
>garbage collections.  I have changed the linear least squares code to
>use a Cholesky decomposition and several other places to use the new
>Matrix package.  Eventually I will clean up the new Matrix package so
>things work as before but I am still rewriting all the Matrix package
>and all the lme4 package.  (There are about 1500 lines of pretty dense
>C code, ssclme.c, in the Matrix package that provide functions that
>are called in the lme4 package but, for reasons related to loaders
>have to sit in the Matrix package.)

>Anyway, here's a new version of R2.R and the results on this system
>(2.0 GHz Pentium-4, Debian GNU Linux, R-2.0.0 (20040420 snapshot),
>Goto's BLAS)

== The short answer: ==
Thank you Doug. This benchmark was done for R 1.6.2... So, it certainly
requires some reworking. Also, many changes would be welcome (see
hereunder). This is a huge work if the benchmark should run in R, S-PLUS,
Matlab, Octave, Scilab, Ox, O-Matrix, etc... Yet, it is more conceivable to
improve it for R only, or perhaps for R and S-PLUS. I don't have time for
that now, but can plan to do, at least some, of this work for, let's say,
the time next R version (2.0) will be available, and to put this in a R
package distributed on CRAN. Of course, I appreciate help!

== The long answer: ==
The second version of the benchmark is now as old as R 1.6.2! It certainly
needs some reworking. However, there is a major problem in my benchmark
script: it does not check the results! At the time I made it, I did all the
required check manually to make sure that all software did complete and
returned meaningful results. Now, with all these changes, I am not so sure
it is still true. For instance, if a software just fails in the middle of
the calculation, but returns nicely (let's say with NAs), then the timing
will be done exactly as if it succeed... However, it is certainly not the
same!

I got lot of critics about these benchmarks, but obviously, it is still
useful for some applications (is it worth to invest in a dual processor
machine, or so?). Also, it helped to track slower functions in R: sort() up
to version 1.5 and, more recently, slow operation of the exponentiation (^)
in version 1.8.X where tracked thanks to this benchmark and subsequently
solved by the R developers...

It takes lot of time to set up such a benchmark. However, with time, I think
the following would be worth considering:

1) To add some code for checking results returned by the tests,

2) To test on small, medium and large data sets, instead of just one data
set size... but what is 'small', 'medium', 'large' exactly?

3) To test more functions and get a more complete coverage for a better
overall estimate of speed,

4) To benchmark graph,

5) To use real world examples instead,

6) To pack it in a R package distributed on CRAN...

Best,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From Jonathan.Swinton at astrazeneca.com  Wed Apr 21 12:10:38 2004
From: Jonathan.Swinton at astrazeneca.com (Swinton, Jonathan)
Date: Wed, 21 Apr 2004 11:10:38 +0100
Subject: [R] RE: How to write an S4 method for sum or a Summary generic
Message-ID: <FCA5F290CE7FFD42A6F1515497B0F0A20436219F@ukapphresmsx02.ukapd.astrazeneca.net>

I asked last week if anyone knew how to define an S4 method for "sum" or any
member of the "Summary" group generics. Doug Bates pointed out that

	>There is no x argument in the generic so you can't dispatch on it.

	> sum
	function (..., na.rm = FALSE) 
	.Internal(sum(..., na.rm = na.rm))

but I don't know if he meant to imply that therefore there was no way of
doing this. I had no other replies. I have now grepped all the R sources,
and all the  CRAN or Bioconductor packages, and found no example of any
setMethod("sum or setMethod("Summary. (Moreover it seems to me  that the
implementation of setGeneric("max" proposed on p351 of the Green Book, and
designed explicitly to allow S4 methods to be written for "Summary" group
generics, is not in fact implemented in R, but I don't understand the code
well enough.)

Is it the opinion of r-help or the R core that it is not possible to do
this? To be explicit, given

	
>setGeneric("sumLike",function(...,na.rm=FALSE)standardGeneric("sumLike"))
	>setClass("Foo",representation(a="numeric"));
	>aFoo <- new("Foo",a=c(1,NA))
	>.sum.Foo <- function(x,na.rm) {sum(x at a,na.rm)}

Then is there a setMethod call for sumLike which will despatch
sumLike(aFoo,na.rm) to  .sum.Foo ? If not, might this be considered a bug?




>  -----Original Message-----
> From: 	Swinton, Jonathan  
> Sent:	19 April 2004 13:16
> To:	'r-help at stat.math.ethz.ch'
> Subject:	How to write an S4 method for sum or a Summary generic
> 
> If I have a class Foo, then i can write an S3 method for sum for it:
> 
> >setClass("Foo",representation(a="integer"));aFoo=new("Foo",a=
> c(1:3,NA))
> >sum.Foo <- 
> function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
> >sum(aFoo)
> 
> But how do I write an S4 method for this? All my attempts to 
> do so have foundered. For example
> >setMethod("sum",signature("Foo","logical"), 
> function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
> creates a method which seems to despatch on na.rm="Foo":
> > getMethods("sum")
> na.rm = "ANY":
> function (..., na.rm = FALSE) 
> .Internal(sum(..., na.rm = na.rm))
> 
> na.rm = "Foo":
> function (..., na.rm = FALSE) 
> {
>     .local <- function (x, na.rm) 
>     {
>         print(x)
>         print(na.rm)
>         sum(x at a, na.rm = na.rm)
>     }
>     .local(..., na.rm = na.rm)
> }
> 
> na.rm = "missing":
> function (..., na.rm = FALSE) 
> .Internal(sum(..., na.rm = na.rm))
> ##:    (inherited from na.rm = "ANY")
> 
> Pages 350-352 of the Green book discuss at some length how to 
> write a generic function for Summary group generics which 
> uses tail recursion to allow the correct method to be called 
> on each member of a ... argument list.  But it gives no 
> examples of what individual method functions need to look 
> like. Any ideas or a place to look for working code?
> 
> Jonathan Swinton, Statistical Scientist, Computational 
> Biology, Pathway Analysis, Global Sciences and Information, 
> AstraZeneca.
>



From ripley at stats.ox.ac.uk  Wed Apr 21 12:53:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 11:53:52 +0100 (BST)
Subject: [R] RE: How to write an S4 method for sum or a Summary generic
In-Reply-To: <FCA5F290CE7FFD42A6F1515497B0F0A20436219F@ukapphresmsx02.ukapd.astrazeneca.net>
Message-ID: <Pine.LNX.4.44.0404211142590.10075-100000@gannet.stats>

As Doug pointed out, the problem is the generic you are trying to create.
I don't see how to add to his reply: for an S4 generic to be useful, you 
need named arguments to dispatch on.

Note than in S4 max and sum have arglist

function (x, ..., na.rm = FALSE)

and that is different from S3 and R.  If you start with your own generic 
with that arglist, you should be able to add S4 methods.

On Wed, 21 Apr 2004, Swinton, Jonathan wrote:

> I asked last week if anyone knew how to define an S4 method for "sum" or any
> member of the "Summary" group generics. Doug Bates pointed out that
> 
> 	>There is no x argument in the generic so you can't dispatch on it.
> 
> 	> sum
> 	function (..., na.rm = FALSE) 
> 	.Internal(sum(..., na.rm = na.rm))
> 
> but I don't know if he meant to imply that therefore there was no way of
> doing this. 

That is what he said, as I read it.

> I had no other replies. I have now grepped all the R sources,
> and all the  CRAN or Bioconductor packages, and found no example of any
> setMethod("sum or setMethod("Summary. (Moreover it seems to me  that the
> implementation of setGeneric("max" proposed on p351 of the Green Book, and
> designed explicitly to allow S4 methods to be written for "Summary" group
> generics, is not in fact implemented in R, but I don't understand the code
> well enough.)
> 
> Is it the opinion of r-help or the R core that it is not possible to do
> this? To be explicit, given
> 	
> >setGeneric("sumLike",function(...,na.rm=FALSE)standardGeneric("sumLike"))
> 	>setClass("Foo",representation(a="numeric"));
> 	>aFoo <- new("Foo",a=c(1,NA))
> 	>.sum.Foo <- function(x,na.rm) {sum(x at a,na.rm)}
> 
> Then is there a setMethod call for sumLike which will despatch
> sumLike(aFoo,na.rm) to  .sum.Foo ? If not, might this be considered a bug?

Not a bug in R.  With no argument to dispatch on, how can dispatch be 
done?

> 
> 
> 
> 
> >  -----Original Message-----
> > From: 	Swinton, Jonathan  
> > Sent:	19 April 2004 13:16
> > To:	'r-help at stat.math.ethz.ch'
> > Subject:	How to write an S4 method for sum or a Summary generic
> > 
> > If I have a class Foo, then i can write an S3 method for sum for it:
> > 
> > >setClass("Foo",representation(a="integer"));aFoo=new("Foo",a=
> > c(1:3,NA))
> > >sum.Foo <- 
> > function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
> > >sum(aFoo)
> > 
> > But how do I write an S4 method for this? All my attempts to 
> > do so have foundered. For example
> > >setMethod("sum",signature("Foo","logical"), 
> > function(x,na.rm){print(x);print(na.rm);sum(x at a,na.rm=na.rm)}
> > creates a method which seems to despatch on na.rm="Foo":
> > > getMethods("sum")
> > na.rm = "ANY":
> > function (..., na.rm = FALSE) 
> > .Internal(sum(..., na.rm = na.rm))
> > 
> > na.rm = "Foo":
> > function (..., na.rm = FALSE) 
> > {
> >     .local <- function (x, na.rm) 
> >     {
> >         print(x)
> >         print(na.rm)
> >         sum(x at a, na.rm = na.rm)
> >     }
> >     .local(..., na.rm = na.rm)
> > }
> > 
> > na.rm = "missing":
> > function (..., na.rm = FALSE) 
> > .Internal(sum(..., na.rm = na.rm))
> > ##:    (inherited from na.rm = "ANY")
> > 
> > Pages 350-352 of the Green book discuss at some length how to 
> > write a generic function for Summary group generics which 
> > uses tail recursion to allow the correct method to be called 
> > on each member of a ... argument list.  But it gives no 
> > examples of what individual method functions need to look 
> > like. Any ideas or a place to look for working code?
> > 
> > Jonathan Swinton, Statistical Scientist, Computational 
> > Biology, Pathway Analysis, Global Sciences and Information, 
> > AstraZeneca.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jlozano at apoy.upm.edu.ph  Wed Apr 21 13:13:14 2004
From: jlozano at apoy.upm.edu.ph (Jingky Lozano)
Date: Wed, 21 Apr 2004 19:13:14 +0800
Subject: [R] Boot package
Message-ID: <1082545994.4086574aa13bd@mail.upm.edu.ph>

Dear mailing list,

I tried to run the example for the conditional bootstap written in the help file
of censboot.  I got the following result:

STRATIFIED CONDITIONAL BOOTSTRAP FOR CENSORED DATA


Call:
censboot(data = aml, statistic = aml.fun, R = 499, F.surv = aml.s1, 
    G.surv = aml.s2, strata = aml$group, sim = "cond")


Bootstrap Statistics :
    original    bias    std. error
t1*       31       Inf         NaN
t2*       23 -2.058116    8.670602


*****
I don't know if there is something in the given code that results in "Inf" and
"NaN" result.  I tried a different data, analyzing it without stratification. 
I got a different error:

> data.s1 <- survfit(Surv(time,cens), data=dataset)
> data.s2 <- survfit(Surv(time-0.001*cens,1-cens), data=dataset)
> msurv.cond  <- censboot(data=dataset,statistic=data.fun,R=r,
F.surv=data.s1,G.surv=data.s2,sim="cond")
Error in sample(length(x), size, replace, prob) : 
        invalid first argument

Can the boot package also output standard error and bias of the regression
coefficients if I do a cox proportional hazard analysis applying all the
different bootstrap methods for censored data??? I did a trial and error with
my codes but it all got muddled in the end giving me nothing.  

I hope you can help me with some of my questions. Thank you for your time.

Jei



----------------------------------------------------------------

University of the Philippines Manila (http://mail.upm.edu.ph)



From ticas at psb.ugent.be  Wed Apr 21 13:15:16 2004
From: ticas at psb.ugent.be (Tineke Casneuf)
Date: Wed, 21 Apr 2004 13:15:16 +0200
Subject: [R] 3D histogram
Message-ID: <408657C4.80393B6B@psb.ugent.be>

Hi,

I have been trying to look for documentation on making a 3D histogram in
R (so a histogram on which there are several distributions are plotted),
but I cannot find anything on it. Is it possible to construct such
histograms and should use a special package (f.e. R.Basic)?

Many thanks in advance,

Tine

--



From ripley at stats.ox.ac.uk  Wed Apr 21 13:30:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 12:30:22 +0100 (BST)
Subject: [R] Boot package
In-Reply-To: <1082545994.4086574aa13bd@mail.upm.edu.ph>
Message-ID: <Pine.LNX.4.44.0404211219370.10186-100000@gannet.stats>

Some bootstrap resamples are giving infinite results for the first 
component of the statistic. That can certainly happen, given the censoring 
pattern.  Here is one such bootstrap resample

   time cens group
20    5    1     2
17    8    1     2
19    8    1     2
23   12    1     2
6    13    1     1
3    13    0     1
13   13    0     2
12   16    0     2
5    18    1     1
10   23    1     1
14   23    1     2
18   30    1     2
21   30    1     2
22   30    1     2
1    31    1     1
7    34    1     1
15   43    1     2
16   45    1     2
2    45    0     1
4    45    0     1
9    45    0     1
8   161    0     1
11  161    0     1

Note that 6/11 of the group 1 samples were censored, so the median
survival time is estimated to be infinite.


On Wed, 21 Apr 2004, Jingky Lozano wrote:

> Dear mailing list,
> 
> I tried to run the example for the conditional bootstap written in the help file
> of censboot.  I got the following result:
> 
> STRATIFIED CONDITIONAL BOOTSTRAP FOR CENSORED DATA
> 
> 
> Call:
> censboot(data = aml, statistic = aml.fun, R = 499, F.surv = aml.s1, 
>     G.surv = aml.s2, strata = aml$group, sim = "cond")
> 
> 
> Bootstrap Statistics :
>     original    bias    std. error
> t1*       31       Inf         NaN
> t2*       23 -2.058116    8.670602
> 
> 
> *****
> I don't know if there is something in the given code that results in "Inf" and
> "NaN" result. 

So you could do what I did, and get aml.fun to print out it's input and 
output.

> I tried a different data, analyzing it without stratification. 
> I got a different error:
> 
> > data.s1 <- survfit(Surv(time,cens), data=dataset)
> > data.s2 <- survfit(Surv(time-0.001*cens,1-cens), data=dataset)
> > msurv.cond  <- censboot(data=dataset,statistic=data.fun,R=r,
> F.surv=data.s1,G.surv=data.s2,sim="cond")
> Error in sample(length(x), size, replace, prob) : 
>         invalid first argument

You need to recompute the whole examples, and probably you have not done 
so.

> Can the boot package also output standard error and bias of the regression
> coefficients if I do a cox proportional hazard analysis applying all the
> different bootstrap methods for censored data??? I did a trial and error with
> my codes but it all got muddled in the end giving me nothing.  

You can, but you do need to understand what you are doing here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Lorenz.Gygax at fat.admin.ch  Wed Apr 21 13:36:09 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Wed, 21 Apr 2004 13:36:09 +0200
Subject: [R] 3D histogram
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DAE91@evd-s7014.evd.admin.ch>


Hi Tine,

> I have been trying to look for documentation on making a 3D 
> histogram in R (so a histogram on which there are several distributions 
> are plotted), but I cannot find anything on it. Is it possible to
> construct such histograms and should use a special package (f.e. R.Basic)?

In my opinion you should never use 3D graphs. Due to the perspective on
these graphs it is always difficult to compare different values of the
graphed data and usually it is also difficult to get a good estimate of the
actual values as the axes are not always aligned with the data of interst.

You might want to ask yourself whether your data is really as simple that it
can be shown in bars (or could you e.g. rather use boxlplots). If you want
to graph your data as in barplots, you could plot the values of your bars
using points in a 2D diagram and e.g. connect the points that would have
been in one row/column in the 3D diagram.

If you are interested in the graphical presentation of quantitative data, I
can only recommend:

@Book{Tuf:99,
  author = 	 {Tufte, Edward R},
  title = 	 {The Visual Display of Quantitative Information},
  publisher = 	 {Graphic Press},
  year = 	 {1999},
  address = 	 {Cheshire, Connecticut},
  edition = 	 {17th printing}
}

Hope this helps. Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Tag der offenen T??r, 11./12. Juni 2004: http://www.fat.ch/2004

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From jashonder at yahoo.com  Wed Apr 21 13:42:20 2004
From: jashonder at yahoo.com (John Shonder)
Date: Wed, 21 Apr 2004 04:42:20 -0700 (PDT)
Subject: [R] Can't install new packages
Message-ID: <20040421114220.28729.qmail@web14427.mail.yahoo.com>

I am using R 1.7.0 under unix, installed on Mac OS X
via fink. I wanted to install the date package, and
used the following command:

install.packages(date)

It sure looked like it was going to work:

trying URL
`http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length
180100 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .....
downloaded 175Kb

Error in unique.default(pkgs) : unique() applies only
to vectors

What is this "Error in unique.default(pkgs)? By the
way I ran R under the "sudo" command, so things could
certainly be written to other directories if
necessary.

I appreciate any help. Thanks.

John Shonder



From ligges at statistik.uni-dortmund.de  Wed Apr 21 13:47:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Apr 2004 13:47:47 +0200
Subject: [R] Can't install new packages
In-Reply-To: <20040421114220.28729.qmail@web14427.mail.yahoo.com>
References: <20040421114220.28729.qmail@web14427.mail.yahoo.com>
Message-ID: <40865F63.9040809@statistik.uni-dortmund.de>

John Shonder wrote:
> I am using R 1.7.0 under unix, installed on Mac OS X
> via fink. I wanted to install the date package, and
> used the following command:
> 
> install.packages(date)


  install.packages("date")

(note the quotes!)

Uwe Ligges


> It sure looked like it was going to work:
> 
> trying URL
> `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length
> 180100 bytes
> opened URL
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... .......... .....
> downloaded 175Kb
> 
> Error in unique.default(pkgs) : unique() applies only
> to vectors
> 
> What is this "Error in unique.default(pkgs)? By the
> way I ran R under the "sudo" command, so things could
> certainly be written to other directories if
> necessary.
> 
> I appreciate any help. Thanks.
> 
> John Shonder
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From enrique.bengoechea at credit-suisse.com  Wed Apr 21 13:37:25 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Wed, 21 Apr 2004 13:37:25 +0200
Subject: [R] RODBC and SQlServer Float type
Message-ID: <OF0D8EFFD6.B17E6F58-ONC1256E7D.003C26C7@csintra.net>

Hi,

I'm trying to get data from a MS SqlServer database into R, using the RODBC package. The problem I've found is that numeric data from my sql queries is returned with only two decimal digits.

After searching the docs, I think the problem comes from my sql numeric data having "float" type instead of "double", as I found the following:

"Where possible sqlGetResults transfers data directly: this happens for double, real, integer and smallint columns in the table. All other SQL data types are converted to character strings (of length up to 256), and then converted by type.convert as
controlled by the as.is argument."

What I did not found was a way to get back more than two decimal digits in this character-numeric transformation. I would acknowledge any clue as to whether this is possible.

Thanks in advance!

Enrique
___________________________________________________________________________

Enrique Bengoechea
Investment Consulting - CREDIT SUISSE Spain



From enrique.bengoechea at credit-suisse.com  Wed Apr 21 13:51:18 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Wed, 21 Apr 2004 13:51:18 +0200
Subject: [R] Re: RODBC and SQlServer Float type
Message-ID: <OF55ECBADC.7E218EC2-ONC1256E7D.0040DA7B@csintra.net>

Ooops, I answer myself: just modify the Sql query to make a cast:   "SELECT cast(price AS real) ..." and it works, although I would still like to know whether there's some R-based solution (e.g. if you are reading full tables instead of launching Sql
queries, which I think would have the two-digits problem).

Enrique
___________________________________________________________________________

Enrique Bengoechea
Investment Consulting - CREDIT SUISSE Spain



From p.dalgaard at biostat.ku.dk  Wed Apr 21 11:33:43 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Apr 2004 11:33:43 +0200
Subject: [R] difference between coxph and cph
In-Reply-To: <20040421070707.GB25078@stat.umu.se>
References: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>
	<20040421070707.GB25078@stat.umu.se>
Message-ID: <x2ad15smiw.fsf@biostat.ku.dk>

G??ran Brostr??m <gb at stat.umu.se> writes:

> On Tue, Apr 20, 2004 at 10:01:36PM -0700, Deb Montgomery wrote:
> > Hi. I am using Windows version of R 1.8.1. Being somewhat new to survival
> > analysis, I am trying to compare cph (Design) with coxph (survival) for use
> > with a survival data set.
> > 
> > I was wondering why cph and coxph provide me with different confidence
> > intervals
> > for the hazard ratios for one of the variables. I was wondering if I am
> > doing something wrong? Or if the two functions are calculating hazard ratios
> > and the 95% confidence intervals differently? 
> 
> Yes, for 'Weight.at.age.4' you get differing parameter estimates (0.91
> versus 0.83). Want to know the correct answer? Try 'coxreg' in  package
> 'eha'! :-) 
> 
> More seriously, the difference may well be of numerical character,
> different convergence criteria, "unbalanced" data, etc. It is really
> impossible to say without knowing what your data are (and without looking
> into the code of coxph and cph).

I'd sooner expect that there is a definition issue: The results given
for Weight.at.age.4 are almost exactly the square root of those from
cph. Perhaps the latter have been raised to the 1.9885th power since
that appears to be the range of the variable.


> > I have listed part of the code
> > and part of the results from the 2 functions. Sorry if this question is a
> > repeat, I didn't find it when I searched the archives.
> > 
> > ###########################################################
> > # s= Surv(Time1, Time2, censor)
> > #f= coxph(s~  Siblings + Weight.at.age.4)
> > #summary(f)
> > #Call:
> > #coxph(formula = s ~ Siblings + Weight.at.age.4)
> > # n= 132
> > #                exp(coef) exp(-coef) lower .95 upper .95
> > #Siblings            1.52      0.657     0.815      2.84
> > #Weight.at.age.4      0.91      1.099     0.772      1.07
> > ##############################################################
> > #s= Surv(Time1, Time2, censor)
> > #f= cph(s~  Siblings + Weight.at.age.4,surv=TRUE ,  x=T, y = T)
> > # summary(f)
> > #             Effects              Response : s
> > # Factor          Low   High   Diff.  Effect S.E. Lower 0.95 Upper 0.95
> > # Siblings       0.000  1.000 1.0000  0.42  0.32 -0.20      1.04
> > #  Hazard Ratio   0.000  1.000 1.0000  1.52    NA  0.82      2.84
> > # Weight.at.age.4 8.613 10.602 1.9885 -0.19  0.17 -0.51      0.14
> > # Hazard Ratio   8.613 10.602 1.9885  0.83    NA  0.60      1.15
> > 
> > 
> > Sincerely
> > 
> > Deb Montgomery
> > 
> > Department of Fisheries and Wildlife Resources
> > University of Idaho
> > Moscow, Idaho 83843
> > 208-885-4008
> > mont4260 at uidaho.edu
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> -- 
>  G??ran Brostr??m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume?? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From HankeA at mar.dfo-mpo.gc.ca  Wed Apr 21 14:57:14 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 21 Apr 2004 09:57:14 -0300
Subject: [R] (no subject)
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE12498E@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040421/3728a229/attachment.pl

From andy_liaw at merck.com  Wed Apr 21 15:04:16 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Apr 2004 09:04:16 -0400
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C45@usrymx25.merck.com>

I guess you're referring to
http://www.fas.umontreal.ca/biol/casgrain/en/labo/R/index.html.  It's not a
package for R, but another stand-alone package also named `R'.

Andy

> From: Hanke, Alex
> 
> Dear R-Help
> Does "The R Package for Multivariate and Spatial Analysis Version 4.0
> (Casgrain
> and Legendre, 2001)" exist on CRAN and under what name?  It 
> supposedly has a
> chronological clustering program ,CHRONO, that I would like to use. 
> Alternatively, I would ask if there is a R based program that performs
> chronological clustering?
> 
> Thanks
> Alex
> 
> Alex Hanke
> Department of Fisheries and Oceans
> St. Andrews Biological Station
> 531 Brandy Cove Road
> St. Andrews, NB
> Canada
> E5B 2L9
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Wed Apr 21 15:04:38 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 21 Apr 2004 09:04:38 -0400
Subject: [R] two stage level least square in R 
In-Reply-To: <BD1D7341BE3930408509F95C86A451CBC94DA7@mail1.UNINE.CH>
Message-ID: <20040421130440.NMQS15096.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Mathieu,

If "two stage level least square" is the same as two-stage least squares,
then you might take a look at the tsls function in the sem package.

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> VUILLEUMIER Mathieu
> Sent: Wednesday, April 21, 2004 1:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] two stage level least square in R 
> 
> I m in front of a problem of simultaneity bias between two 
> equations and would like to apply the two stage level least 
> square....Is there a special command in R ? I didn't found 
> something about that in the R help.
> 
> Thanks for your help !!!!
> 
> 
> 
> **************************************************************
> Mathieu Vuilleumier - collaborateur scientifique Institut de 
> recherches ??conomique et r??gionale (IRER) Universit?? de 
> Neuch??tel Pierre-??-Mazel 7, CH-2000 Neuch??tel Tel : 032/ 718 
> 14 66 E-mail : mathieu.vuilleumier at unine.ch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Apr 21 15:14:05 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 21 Apr 2004 13:14:05 +0000 (UTC)
Subject: [R] Size of R user base
References: <200404210155.i3L1te9M362709@atlas.otago.ac.nz>
	<MABBLJDICACNFOLGIHJOIEJLEFAA.phgrosjean@sciviews.org>
Message-ID: <loom.20040421T150926-75@post.gmane.org>

Philippe Grosjean <phgrosjean <at> sciviews.org> writes:
> We have also the activity in the R-help mailing list, which could be
> representative of the most active users, certainly. Does anyone have of
> figure of the number of messages in R-Help with time since its creation? (it
> is probably available somewhere, but I don't know where).



If you check the r-help archive for last month

https://www.stat.math.ethz.ch/pipermail/r-help/2004-March/date.html

at CRAN it says at the top there were 1949 messages for March.

Looking at

	https://www.stat.math.ethz.ch/pipermail/r-help/ 

it shows the Gzip's size of each month's archives and from that 
March had 886 KB of Gzip's text from which we can estimate 1949/886
= 2.2 messages per KB.  Over the last number of months there were
the following number of G'zipped KB for successive months over the
last 84 months:

55  19  19  18  19  17  35  27  47  55  32  50  55  41  49  50  28  53  42
81  54  99  60  84  80  76  75  78  61  83  97 141 122  96 144 173 153 226
202 131 165 183 175 168 187 240 272 262 195 236 244 285 249 326 345 392 268
455 320 418 453 468 422 447 400 323 516 478 327 450 487 535 658 573 606 659
543 655 722 677 567 519 703 886

where the last point 886KB is March 2004.  Summing those number and
using 2.2 messages per KB gives an estimate of about 50,000 messages 
over that period of time.

Fitting a log linear model to those numbers gives:

	log(KB) = 3.2 + .043 i

where i is the month number which indicates that the archive size
(and hence the number of messages and possibly the user base) is
growing at 4% per month!

P.S.  The following web page gives the number of messages per day over
the last few days as a graph:

	http://gmane.org/info.php?group=gmane.comp.lang.r.general



From ggrothendieck at myway.com  Wed Apr 21 15:15:01 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 21 Apr 2004 13:15:01 +0000 (UTC)
Subject: [R] resetting the interpreter
References: <20040421091000.GA1665@localhost>
Message-ID: <loom.20040421T151435-293@post.gmane.org>


If you haven't tried 1.9.0 yet it does start up faster than 1.8.1.


Tamas Papp <tpapp <at> axelero.hu> writes:

: 
: Hi,
: 
: Sometimes I want to "reset" the R interpreter (as if I quit and
: started it again, with everything starting from scratch).  The main
: reason for this is when I expreriment with calculations, and later
: form them into general functions, sometimes I forget to include
: variables or reset some settings which I don't notice because they are
: already set.  This causes problems during batch calculations, and a
: lot of wasted time.
: 
: Currently I am using rm(list=ls()), but it only cleans up the
: namespace.  Quitting and restarting is slow.  What I am looking for is
: a function that resets the R interpreter without restarting it.
: 
: I did search the archives, but maybe I am using the wrong keywords:
: reset gives me a lot of results.  I would appreciate any advice on
: this, ie if there is no function to do this, how people cope with this
: problem.
: 
: Thanks,
: 
: Tamas
:



From btom at passagen.se  Wed Apr 21 15:22:42 2004
From: btom at passagen.se (btom@passagen.se)
Date: Wed, 21 Apr 2004 15:22:42 +0200
Subject: [R] Kendall, cor.test, ties, why?
Message-ID: <4071F2B900007B42@webmail-se2.sol.no1.asap-asp.net>

Hi,
I can't figure out why it is not possible to compute an exact p-value in
cor.test if there are ties between values in one of the arrays like below:
cor.test(c(1,2,2), c(5,6,7), method="k", alternative="two.sided").

Perhaps this is due to my lack of understanding of what is ment by p-value
in this case. To me it seems reasonable that the p-value above should be
the number of permutations, normalized by the total number of permutations,
of one of the arrays that together with the other (unpermuted) array produce
a higer (or equal) absolute tau than than that of the original permutation.
I would be most greatful if someone cold help me understand the p-value
better.

cheers
Tom



_______________________________________________________
Skicka gratis SMS!
http://www.passagen.se



From jgoebel at diw.de  Wed Apr 21 15:23:29 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Wed, 21 Apr 2004 15:23:29 +0200
Subject: [R] Size of R user base
In-Reply-To: <1082537708.8800.8.camel@gandalf.local>
References: <4084D3E5.17583.1C5C02@localhost> <x2d6626eyh.fsf@biostat.ku.dk>
	<4085744A.2020700@aon.at> <1082537708.8800.8.camel@gandalf.local>
Message-ID: <20040421132329.GA5610@diw138134.diw-berlin.de>

Hi,

i also strongly recommend an active way for transmitting
such informations. 

One possibility could be, to give the users "incentives" 
for using a network connection within R.
Stata has a -- i guess -- widely used command, called
"net search", which is something like "help.search()", 
but connecting to stata.com and searches also all the 
user distributed program files (.ado).

If R had something similar, which searches all packages
available on CRAN, and not only the installed packages
one  
a) could count the access to this service and
b) had a could starting point for questions like
   "is a function 'doing something' available for R". 

just my 2 cents ...

jan 

On Wed, 21 Apr 2004, Ernesto Jardim wrote:

> Hi,
> 
> I agree with Christian. The monitoring must be done in a ACTIVE way by
> the user.
> 
> I usually don't use software that has that kind of features and it
> annoys me to know that my computer is sending information to someone
> whithout my knowledge or permission.
> 
> Best regards
> 
> EJ 
> 
> On Tue, 2004-04-20 at 20:04, cstrato wrote:
> > At first I want to make clear that I have always paid for the
> > software that I use, I am even paying for the shareware that I use.
> > 
> > After this statement let me come to the current discussion:
> > It is really shocking that some people even think about integrating
> > a spyware system into an open source software project.
> > The internet has started as a chaotic and democratic system, but
> > is in danger to become the worst "Big Brother is watching you"
> > assistant to certain governments and companies, which become a
> > danger for the democracy and for peace.
> > 
> > I really hope that the R core developer will never consider such
> > a dangerous option!
> > 
> > Having said this, I am willing to ACTIVELY send an e-mail to
> > CRAN or R to tell that I am using R.
> > 
> > R is such a great project, developed by a great community in a
> > democratic manner, showing how well democracy can work. Adding
> > a monitoring system would destroy the whole project.
> > 
> > P.S.: Personally, I have even disabled cookies from my browser,
> > both at home and at the company, and I must say, that very seldom
> > would I need to enable cookies. If a company web-site forces me
> > to enable cookies, than I ignore this company: companies want to
> > sell products to customers, and with such a behavior they are
> > hopefully loosing customers.
> > 
> > Best regards
> > Christian

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K??nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From dmurdoch at pair.com  Wed Apr 21 15:40:33 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 21 Apr 2004 09:40:33 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404211005170.9702-100000@gannet.stats>
References: <003901c4277e$9b42def0$0213eda9@des.ucdavis.edu>
	<Pine.LNX.4.44.0404211005170.9702-100000@gannet.stats>
Message-ID: <q8uc80hsdsss9fnndjm5t6bffentc4s4pd@4ax.com>

On Wed, 21 Apr 2004 10:21:36 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:

>For the record, it looks like the C entry point getenv() has been broken,
>and since 1.9.0 introduced a length check on the result of getenv it 
>terminates `cleanly' whereas 1.8.1 segfaults.  Or possibly the setting of 
>environment variables (HOMEDRIVE and HOMEPATH) has been broken, which 
>would amount to the same thing.

I think I'm current with MS updates, but I don't see any problems like
the ones described, or with the R function Sys.getenv.  What symptoms
make you think getenv() is broken?

My suspicion about the reported problems would be the usual suspects:
anti-virus software, keyboard accelerators, virus infection, etc.

To check on these, affected users could try rebooting in "safe mode"
(hit F8 during the reboot process, then follow instructions).  This
boots Windows with minimal services loaded.  If some other program is
interfering with R, this will remove it, and R should be fine.  If
it's an R problem, it should still show up here.

Duncan Murdoch



From dmurdoch at pair.com  Wed Apr 21 15:41:18 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 21 Apr 2004 09:41:18 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Sea2-F349mK6PYzDabi00030971@hotmail.com>
References: <Sea2-F349mK6PYzDabi00030971@hotmail.com>
Message-ID: <tduc80hj314std6f5vn9i2pfbrn7gme0o7@4ax.com>

On Wed, 21 Apr 2004 16:53:37 +0800, "Z P" <nusbj at hotmail.com> wrote:

>I wonder whether the R development group can contact Microsoft to fix this 
>problem. R is really powerful as compared with S+.

If we could identify what's going wrong, we could probably work around
it.  My experience is that it is a complete waste of time to contact
MS about bugs in their software.

I'd suggest rebooting in "safe mode", to identify if this is an R
problem or something caused by one of your other programs.  Hit F8
during rebooting to get this option.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Apr 21 15:44:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 14:44:13 +0100 (BST)
Subject: [R] Kendall, cor.test, ties, why?
In-Reply-To: <4071F2B900007B42@webmail-se2.sol.no1.asap-asp.net>
Message-ID: <Pine.LNX.4.44.0404211437430.20927-100000@gannet.stats>

How many permutations do you think there are?  Direct enumeration is only 
efficient for single-digit sample sizes, and those are too small to be 
interesting in practice.

The issue is to count them reasonably efficiently: if there are no ties 
this can be done inductively on the sample size, but otherwise it is a lot 
more complicated.

On Wed, 21 Apr 2004 btom at passagen.se wrote:

> I can't figure out why it is not possible to compute an exact p-value in
> cor.test if there are ties between values in one of the arrays like below:
> cor.test(c(1,2,2), c(5,6,7), method="k", alternative="two.sided").
> 
> Perhaps this is due to my lack of understanding of what is ment by p-value
> in this case. To me it seems reasonable that the p-value above should be
> the number of permutations, normalized by the total number of permutations,
> of one of the arrays that together with the other (unpermuted) array produce
> a higer (or equal) absolute tau than than that of the original permutation.
> I would be most greatful if someone cold help me understand the p-value
> better.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Apr 21 15:49:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 14:49:51 +0100 (BST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <q8uc80hsdsss9fnndjm5t6bffentc4s4pd@4ax.com>
Message-ID: <Pine.LNX.4.44.0404211444420.20927-100000@gannet.stats>

On Wed, 21 Apr 2004, Duncan Murdoch wrote:

> On Wed, 21 Apr 2004 10:21:36 +0100 (BST), Prof Brian Ripley
> <ripley at stats.ox.ac.uk> wrote:
> 
> >For the record, it looks like the C entry point getenv() has been broken,
> >and since 1.9.0 introduced a length check on the result of getenv it 
> >terminates `cleanly' whereas 1.8.1 segfaults.  Or possibly the setting of 
> >environment variables (HOMEDRIVE and HOMEPATH) has been broken, which 
> >would amount to the same thing.
> 
> I think I'm current with MS updates, but I don't see any problems like
> the ones described, or with the R function Sys.getenv.  

Nor do I, as I said in the part of the message you have clipped.

> What symptoms
> make you think getenv() is broken?

Because it is the result from getenv that is tested in 1.9.0 and not in
1.8.1.  The error message from 1.9.0 is generated iff getenv returns an
overlong string for HOMEDRIVE or HOMEPATH, or a null string for HOMEPATH.  
We can discard the last being true in the environment, as it works if the
patch is backed out (as I understand it).  So getenv must suddenly be 
returning an incorrect result (or something very catastrophic happens to 
end up jumping to that error message, but let's look for simple 
explanations first).

> My suspicion about the reported problems would be the usual suspects:
> anti-virus software, keyboard accelerators, virus infection, etc.

Or in this case we have been told by David Scott and less unequivocably 
by others, Microsoft critical updates.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From feh3k at spamcop.net  Wed Apr 21 13:02:39 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 21 Apr 2004 07:02:39 -0400
Subject: [R] Size of R user base
In-Reply-To: <4085DB03.3060507@arcanemethods.com>
References: <3A822319EB35174CA3714066D590DCD504AF7C2C@usrymx25.merck.com>
	<408548A0.3020505@arcanemethods.com>
	<loom.20040420T185905-990@post.gmane.org>
	<4085DB03.3060507@arcanemethods.com>
Message-ID: <20040421070239.17016f16.feh3k@spamcop.net>

One statistic that might be worth collecting is the number of unique IP
addresses that request update.packages( ) or install.packages( ) on CRAN
or one of its mirrors.

Frank Harrell
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From feh3k at spamcop.net  Wed Apr 21 13:09:03 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 21 Apr 2004 07:09:03 -0400
Subject: [R] difference between coxph and cph
In-Reply-To: <20040421070707.GB25078@stat.umu.se>
References: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>
	<20040421070707.GB25078@stat.umu.se>
Message-ID: <20040421070903.6cf77b46.feh3k@spamcop.net>

On Wed, 21 Apr 2004 09:07:07 +0200
G??ran Brostr??m <gb at stat.umu.se> wrote:

> On Tue, Apr 20, 2004 at 10:01:36PM -0700, Deb Montgomery wrote:
> > Hi. I am using Windows version of R 1.8.1. Being somewhat new to
> > survival analysis, I am trying to compare cph (Design) with coxph
> > (survival) for use with a survival data set.
> > 
> > I was wondering why cph and coxph provide me with different confidence
> > intervals
> > for the hazard ratios for one of the variables. I was wondering if I
> > am doing something wrong? Or if the two functions are calculating
> > hazard ratios and the 95% confidence intervals differently? 
> 
> Yes, for 'Weight.at.age.4' you get differing parameter estimates (0.91
> versus 0.83). Want to know the correct answer? Try 'coxreg' in  package
> 'eha'! :-) 
> 
> More seriously, the difference may well be of numerical character,
> different convergence criteria, "unbalanced" data, etc. It is really
> impossible to say without knowing what your data are (and without
> looking into the code of coxph and cph).

No, cph is essentially a wrapper for coxph and uses the same computations.
 The problem is that Deb did not read the documentation to summary.Design
nor the Overview of the Design package.  The output below makes it pretty
clear that the range over which Weight.at.age.4 is being evaluated is not
an interval of length 1.0 as is assumed by coxph's summary function.  The
Design package by default computes inter-quartile-range effects.  The
quartiles in this case are apparently 8.6 and 10.6.

Frank Harrell

> 
> > I have listed part of the code
> > and part of the results from the 2 functions. Sorry if this question
> > is a repeat, I didn't find it when I searched the archives.
> > 
> > ###########################################################
> > # s= Surv(Time1, Time2, censor)
> > #f= coxph(s~  Siblings + Weight.at.age.4)
> > #summary(f)
> > #Call:
> > #coxph(formula = s ~ Siblings + Weight.at.age.4)
> > # n= 132
> > #                exp(coef) exp(-coef) lower .95 upper .95
> > #Siblings            1.52      0.657     0.815      2.84
> > #Weight.at.age.4      0.91      1.099     0.772      1.07
> > ##############################################################
> > #s= Surv(Time1, Time2, censor)
> > #f= cph(s~  Siblings + Weight.at.age.4,surv=TRUE ,  x=T, y = T)
> > # summary(f)
> > #             Effects              Response : s
> > # Factor          Low   High   Diff.  Effect S.E. Lower 0.95 Upper
> > # 0.95 Siblings       0.000  1.000 1.0000  0.42  0.32 -0.20      1.04
> > #  Hazard Ratio   0.000  1.000 1.0000  1.52    NA  0.82      2.84
> > # Weight.at.age.4 8.613 10.602 1.9885 -0.19  0.17 -0.51      0.14
> > # Hazard Ratio   8.613 10.602 1.9885  0.83    NA  0.60      1.15
> > 
> > 
> > Sincerely
> > 
> > Deb Montgomery
> > 
> > Department of Fisheries and Wildlife Resources
> > University of Idaho
> > Moscow, Idaho 83843
> > 208-885-4008
> > mont4260 at uidaho.edu
> 
> -- 
>  G??ran Brostr??m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume?? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From ernesto at ipimar.pt  Wed Apr 21 16:23:33 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 21 Apr 2004 15:23:33 +0100
Subject: [R] is.infinite usage
Message-ID: <1082557412.8800.45.camel@gandalf.local>

Hi,

Is there a way of using is.infinite as is.na ? I'd like to avoid looping
the data.frame.

> mat
  x   y  z
1 1 Inf NA
2 2 Inf NA
3 3 Inf NA
> is.na(mat)
      x     y    z
1 FALSE FALSE TRUE
2 FALSE FALSE TRUE
3 FALSE FALSE TRUE

What I get at the moment is

> is.infinite(mat)
    x     y     z
FALSE FALSE FALSE

Thanks

EJ



From mmarques at inescporto.pt  Wed Apr 21 16:14:57 2004
From: mmarques at inescporto.pt (MMarques Power)
Date: Wed, 21 Apr 2004 15:14:57 +0100
Subject: [R] Multidimensional numeric structure
Message-ID: <6385880484.20040421151457@power.inescn.pt>


I know  this is going to be a dumb question but...
I need a cube like struture , I have 3 dimensions well defined which
I need to define a value in the cube something like
Cube [][][] in C+ ou Java ...
I have thought in making a matrix and then an array...
and even the other way around.
The problem is that the I have the indexs and wanted to access direcly
the element both reading or writing without to much effort....

But how can "access" a specific number only by their index ?
of course It is possible to make a specific calc in order to reach the
place I needed ....
But what for ?  if I have the correct indexs directly ?

Is "R" capable of building such a struture besides adding up
matrixes and or arrays ?
Did I miss anything ?

Thanks in advance
Marco Marques



From andy_liaw at merck.com  Wed Apr 21 16:35:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Apr 2004 10:35:05 -0400
Subject: [R] Multidimensional numeric structure
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C47@usrymx25.merck.com>

Have you gone through `An Introduction to R', or even ?array?

Try:

mycube = array(1:27, c(3, 3, 3))
mycube[1, 2, 3]
mycube[2, , ]
mycube[, 2, ]
etc.

Andy

> From: MMarques Power
> 
> 
> I know  this is going to be a dumb question but...
> I need a cube like struture , I have 3 dimensions well defined which
> I need to define a value in the cube something like
> Cube [][][] in C+ ou Java ...
> I have thought in making a matrix and then an array...
> and even the other way around.
> The problem is that the I have the indexs and wanted to access direcly
> the element both reading or writing without to much effort....
> 
> But how can "access" a specific number only by their index ?
> of course It is possible to make a specific calc in order to reach the
> place I needed ....
> But what for ?  if I have the correct indexs directly ?
> 
> Is "R" capable of building such a struture besides adding up
> matrixes and or arrays ?
> Did I miss anything ?
> 
> Thanks in advance
> Marco Marques
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From rossini at blindglobe.net  Wed Apr 21 16:36:31 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 21 Apr 2004 07:36:31 -0700
Subject: [R] multi-user engine
In-Reply-To: <40863B68.1070909@lancaster.ac.uk> (Barry Rowlingson's message
	of "Wed, 21 Apr 2004 10:14:16 +0100")
References: <MBBBIIHJANJBMHLGMACKKEAMCGAA.binabina@bellsouth.net>
	<854qre5jwl.fsf@servant.blindglobe.net>
	<40863B68.1070909@lancaster.ac.uk>
Message-ID: <85ad15xus0.fsf@servant.blindglobe.net>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> A.J. Rossini wrote:
>> "zubin" <binabina at bellsouth.net> writes:
>>
>>>I am presently a SAS user and wanted to configure R to work in a multi-user
>>>enteprise environment.  Client - Server.  Where we have a strong LINUX
>>>server supporting about 10 statisticians with R.  Anyone have any backround
>>>or information they can share to help me get jump-started on setting up R in
>>> this environment?
>
>   Why can't you install R in the usual way on the server and give the
>   statisticians (there's only 10 of them, so its not much admin)
>   accounts on the server? They then login (using an SSH client) and
>   get graphics back via X-windows. Free SSH clients and X-windows
>   servers are available for all the usual platforms. Their filestore
>   would be on the server. I just don't see the advantage (and see
>   plenty of disadvantages) of complicating things with a client-server
>   framework where it might not be necessary.

Depends on the environment.  I can think of many good reasons for
wanting a client-server situation, where the solution you point out
would be a complete non-starter.  However, since client-server isn't
ready at this point without a great deal of work and pain, so it
(client-server) is pretty much a non-starter, and the solution you
point out is the only "get it done this month" solution that I know
of.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jari.oksanen at oulu.fi  Wed Apr 21 16:21:38 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Wed, 21 Apr 2004 17:21:38 +0300
Subject: [R] (no subject)
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE12498E@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE12498E@msgmarsta01.bio.dfo.ca>
Message-ID: <33944F62-939F-11D8-A111-000A95C76CA8@oulu.fi>

The "R Package" of Casgrain & Legendre is not an R package, but it is 
an independent, compiled software package. Its name, R, comes from 
golden VAX/VMS (or perhaps even PDP!) days, and pre-dates the R we know 
as R. It seems that Pierre Legendre now uses our R in teaching, at 
least in some parts, so it may be that some of the methods will be 
ported to our R. Otherwise you must use his (or their) R, which is 
freely available (and I think, even GPL'ed now).

For the Casgrain-Legendre R, see 
http://www.fas.umontreal.ca/BIOL/legendre/. Pierre Legendre explained 
somewhere the naming history, but I can't find it in his (or 
Casgrain's) web pages. However, his package was called R long before 
Ihaka & Gentleman introduced their R.

cheers,  jari oksanen

On 21 Apr 2004, at 15:57, Hanke, Alex wrote:

> Dear R-Help
> Does "The R Package for Multivariate and Spatial Analysis Version 4.0
> (Casgrain
> and Legendre, 2001)" exist on CRAN and under what name?  It supposedly 
> has a
> chronological clustering program ,CHRONO, that I would like to use.
> Alternatively, I would ask if there is a R based program that performs
> chronological clustering?
>
> Thanks
> Alex
>
> Alex Hanke
> Department of Fisheries and Oceans
> St. Andrews Biological Station
> 531 Brandy Cove Road
> St. Andrews, NB
> Canada
> E5B 2L9
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
--
Jari Oksanen, Oulu, Finland



From lecoutre at stat.ucl.ac.be  Wed Apr 21 16:31:19 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 21 Apr 2004 16:31:19 +0200
Subject: [R] [R-pkgs] R2HTML update
Message-ID: <6.0.1.1.2.20040421161018.0212e378@stat4ux.stat.ucl.ac.be>


Hi to all R users,

I have just uploaded in CRAN R2HTML 1.3, latest update of the R2HTML 
package, so that it should soon be available for your favorite platform.


CHANGELOG:
- Fixed some user-contributed bugs.
- More CSS styles are defined, so that it's possible to change the whole 
look-and-feel of outputs / Add some CSS sample files.
- Added HTMLstem to produce a stem-and-leaf plot (as stem does directly 
print to console).
- Added HTML.cormat, which is not a real S3 method but allows to write 
correlation matrices with "traffic highlighting" (colours).
- Added a Sweave driver (for Friedrich Leisch wonderfull Sweave tool) which 
allows to include R code within complex HTML files and to replace this code 
by it's outputs (objects in HTML and graphs in png). Comes with sample files.
- Dynamic use with HTMLStart() has a new option 'autobrowse' which 
auto-refresh (callls) the browser each time a command is processed.

DESCRIPTION:
Package: R2HTML
Version: 1.3
Title: HTML exportation for R objects
Author: Eric Lecoutre <lecoutre at stat.ucl.ac.be>
Maintainer: Eric Lecoutre <lecoutre at stat.ucl.ac.be>
Depends: R (>= 1.8.1)
Description: Includes HTML function and methods to write in an HTML file. 
Thus, making HTML reports is easy. Includes a fonction that allows 
redirection on the fly, which appears to be very usefull for teaching 
purpose, as the student can keep a copy of the produced output to keep all 
that he did during the course. Package comes with a vignette describing how 
to write HTML reports for statistical analysis. Finally, a driver for 
Sweave allows to parse HTML flat files containing R code and to 
automatically write the corresponding outputs (tables and graphs).
License: GPL version 2 or newer
URL: http://www.r-project.org, http://www.stat.ucl.ac.be/R2HTML/

Eric
Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From tlumley at u.washington.edu  Wed Apr 21 16:42:11 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 21 Apr 2004 07:42:11 -0700 (PDT)
Subject: [R] difference between coxph and cph
In-Reply-To: <20040421070707.GB25078@stat.umu.se>
References: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>
	<20040421070707.GB25078@stat.umu.se>
Message-ID: <Pine.A41.4.58.0404210736040.121004@homer09.u.washington.edu>

On Wed, 21 Apr 2004, [iso-8859-1] G??ran Brostr??m wrote:
> More seriously, the difference may well be of numerical character,
> different convergence criteria, "unbalanced" data, etc. It is really
> impossible to say without knowing what your data are (and without looking
> into the code of coxph and cph).

The Cox partial loglikelihood is concave, so it really doesn't take much
numerical care to get the right maximum (unlike some of the parameteric
survival models) except on really ugly data sets.

This one requires reading either the output or the documentation, rather
than the code.  Reading the documentation is especially important for the
Design package -- the whole point of the package is to provide a different
interface to regression modelling.

	-thomas



From ligges at statistik.uni-dortmund.de  Wed Apr 21 16:53:15 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Apr 2004 16:53:15 +0200
Subject: [R] is.infinite usage
In-Reply-To: <1082557412.8800.45.camel@gandalf.local>
References: <1082557412.8800.45.camel@gandalf.local>
Message-ID: <40868ADB.7040309@statistik.uni-dortmund.de>

Ernesto Jardim wrote:

> Hi,
> 
> Is there a way of using is.infinite as is.na ? I'd like to avoid looping
> the data.frame.
> 
> 
>>mat
> 
>   x   y  z
> 1 1 Inf NA
> 2 2 Inf NA
> 3 3 Inf NA
> 
>>is.na(mat)
> 
>       x     y    z
> 1 FALSE FALSE TRUE
> 2 FALSE FALSE TRUE
> 3 FALSE FALSE TRUE
> 
> What I get at the moment is
> 
> 
>>is.infinite(mat)
> 
>     x     y     z
> FALSE FALSE FALSE


No, but what's the problem using

   sapply(mat, is.infinite)

instead?

Uwe Ligges



> Thanks
> 
> EJ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ernesto at ipimar.pt  Wed Apr 21 17:10:11 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 21 Apr 2004 16:10:11 +0100
Subject: [R] is.infinite usage
In-Reply-To: <40868ADB.7040309@statistik.uni-dortmund.de>
References: <1082557412.8800.45.camel@gandalf.local>
	<40868ADB.7040309@statistik.uni-dortmund.de>
Message-ID: <1082560211.8800.51.camel@gandalf.local>

On Wed, 2004-04-21 at 15:53, Uwe Ligges wrote:
> Ernesto Jardim wrote:
> 
> > Hi,
> > 
> > Is there a way of using is.infinite as is.na ? I'd like to avoid looping
> > the data.frame.
> > 
> > 
> >>mat
> > 
> >   x   y  z
> > 1 1 Inf NA
> > 2 2 Inf NA
> > 3 3 Inf NA
> > 
> >>is.na(mat)
> > 
> >       x     y    z
> > 1 FALSE FALSE TRUE
> > 2 FALSE FALSE TRUE
> > 3 FALSE FALSE TRUE
> > 
> > What I get at the moment is
> > 
> > 
> >>is.infinite(mat)
> > 
> >     x     y     z
> > FALSE FALSE FALSE
> 
> 
> No, but what's the problem using
> 
>    sapply(mat, is.infinite)
> 
> instead?
> 
> Uwe Ligges
> 
> 
> 
> > Thanks
> > 
> > EJ
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

None,

It works !

Thanks

EJ



From zelickr at pdx.edu  Wed Apr 21 17:10:53 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Wed, 21 Apr 2004 08:10:53 -0700 (PDT)
Subject: [R] formatted table to disk
In-Reply-To: <Pine.A41.4.58.0404210736040.121004@homer09.u.washington.edu>
Message-ID: <Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>

Hello there,

Using R 1.8.1 and WinXP...

I would like to write a disk file (ascii text) of an R data frame. When I
type the name of the object in the console window, it is sensibly and
nicely formatted. When I use write.table, it is *not*, because the
separation character is just a space.

I looked through the introductory material (my level) on the R website,
but did not find a reference to this issue.

At a minimum I suppose there is a way to insert a tab character in the
sep=" " field, but I don't know how to do that. Alternatively, I could
deal with a variable specifying fixed-field widths for each of the
variables in the frame. Didn't find that either.

Even so, you would loose the right-justification and the fact that R seems
to take into account the width of the largest value in a column.

Of course the really slick deal would be to have paginated output where
the column titles repeat at the top of the page (having entered a
parameter for the number of lines on your page after margins and so
forth).

A somewhat brutal solution would be to create all the text strings for
making a LaTex document/table environment and write all that stuff out to
the text file. Please don't tell me I have to do that!

So is there a function for producing a file on disk with the same
formatting as the screen?

Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From gb at stat.umu.se  Wed Apr 21 17:07:57 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 21 Apr 2004 17:07:57 +0200
Subject: [R] difference between coxph and cph
In-Reply-To: <20040421070903.6cf77b46.feh3k@spamcop.net>
References: <ANECLCAJEBKHCHNELKPLCENMCCAA.mont4260@uidaho.edu>
	<20040421070707.GB25078@stat.umu.se>
	<20040421070903.6cf77b46.feh3k@spamcop.net>
Message-ID: <20040421150756.GA30928@stat.umu.se>

On Wed, Apr 21, 2004 at 07:09:03AM -0400, Frank E Harrell Jr wrote:
[...]

> No, cph is essentially a wrapper for coxph and uses the same computations.
>  The problem is that Deb did not read the documentation to summary.Design
> nor the Overview of the Design package.  

And obviously that I didn't :-( 

[...]
> The Design package by default computes inter-quartile-range effects.
[...]

Interesting idea... is this a common procedure in biostatistics? 
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From stecalza at tiscali.it  Wed Apr 21 17:13:43 2004
From: stecalza at tiscali.it (Stefano Calza)
Date: Wed, 21 Apr 2004 17:13:43 +0200
Subject: [R] extractAIC.survreg problem with df
In-Reply-To: <Pine.LNX.4.44.0404021649330.6649-100000@gannet.stats>
References: <9264.1080920848@www61.gmx.net>
	<Pine.LNX.4.44.0404021649330.6649-100000@gannet.stats>
Message-ID: <20040421151343.GB4706@med.unibs.it>

Hi everybody.

I'm having problems with the extractAIC.survreg function and the edf

I get weird results which I think are due to the fact that the function defines edf as (from the stats package in 1.9.0 source code)

...
...
n <- length(fit$residuals)
edf <- n-fit$df.residual
...

But in a survreg object there's no attribute residuals!!??

if in the function I use instead

...
n <- length(residuals(fit)
...

everything's fine, or better

...
edf <- fit$df
...

Am I missing something?

TIA,
Stefano



From dmurdoch at pair.com  Wed Apr 21 17:14:19 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 21 Apr 2004 11:14:19 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404211444420.20927-100000@gannet.stats>
References: <q8uc80hsdsss9fnndjm5t6bffentc4s4pd@4ax.com>
	<Pine.LNX.4.44.0404211444420.20927-100000@gannet.stats>
Message-ID: <gs2d8050ldc85454pvc3q4m9nlb20ug075@4ax.com>

On Wed, 21 Apr 2004 14:49:51 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:

>
>> What symptoms
>> make you think getenv() is broken?
>
>Because it is the result from getenv that is tested in 1.9.0 and not in
>1.8.1.  The error message from 1.9.0 is generated iff getenv returns an
>overlong string for HOMEDRIVE or HOMEPATH, or a null string for HOMEPATH.  
>We can discard the last being true in the environment, as it works if the
>patch is backed out (as I understand it).  So getenv must suddenly be 
>returning an incorrect result (or something very catastrophic happens to 
>end up jumping to that error message, but let's look for simple 
>explanations first).
>
>> My suspicion about the reported problems would be the usual suspects:
>> anti-virus software, keyboard accelerators, virus infection, etc.
>
>Or in this case we have been told by David Scott and less unequivocably 
>by others, Microsoft critical updates.

What I meant is that there seems to be some interaction between those
updates and something else, and the usual suspects would be the ones I
listed.  (And another suspect I forgot:  getenv is in the MSVCRT dll,
and the version of that probably varies from system to system.)  Since
you and I don't get the errors, it's not *only* the updates that are
causing problems.  

I think it would be helpful if the affected users tried your
suggestion of setting the environment variables explicitly.  Here's
more detail on how:

Change the command line that invokes R to explicitly set the
environment variables, e.g. change the shortcut to have target

[Rhome]\Rgui.exe HOMEDRIVE=C:

If this works, then it would indicate to me that the environment
Windows is passing to R is messed up somehow.  If someone with the
error gets in touch with me offline, I could send them a little
program to examine the environment block and see if there's something
wrong with it.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Wed Apr 21 17:28:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 21 Apr 2004 17:28:05 +0200
Subject: [R] Multidimensional numeric structure
In-Reply-To: <6385880484.20040421151457@power.inescn.pt>
References: <6385880484.20040421151457@power.inescn.pt>
Message-ID: <40869305.4070303@statistik.uni-dortmund.de>

MMarques Power wrote:

> I know  this is going to be a dumb question but...
> I need a cube like struture , I have 3 dimensions well defined which
> I need to define a value in the cube something like
> Cube [][][] in C+ ou Java ...
> I have thought in making a matrix and then an array...
> and even the other way around.
> The problem is that the I have the indexs and wanted to access direcly
> the element both reading or writing without to much effort....
> 
> But how can "access" a specific number only by their index ?
> of course It is possible to make a specific calc in order to reach the
> place I needed ....
> But what for ?  if I have the correct indexs directly ?
> 
> Is "R" capable of building such a struture besides adding up
> matrixes and or arrays ?
> Did I miss anything ?
> 
> Thanks in advance
> Marco Marques
> 

You can index a 3d array A with

  A[x,y,z]

Uwe Ligges



From phgrosjean at sciviews.org  Wed Apr 21 17:27:59 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 21 Apr 2004 17:27:59 +0200
Subject: [R] Size of R user base
In-Reply-To: <loom.20040421T150926-75@post.gmane.org>
Message-ID: <MABBLJDICACNFOLGIHJOKEKGEFAA.phgrosjean@sciviews.org>

Thank you, Gabor, for these stats. Here is what I did with it.

Philippe Grosjean

===============
# This is monthly R-help usage as given by the size of gzipped archives over
the last 7 years
RhelpUsage <- ts(c(55, 19, 19, 18, 19, 17, 35, 27, 47, 55, 32, 50, 55, 41,
49, 50, 28, 53, 42,
                81, 54, 99, 60, 84, 80, 76, 75, 78, 61, 83, 97, 141, 122,
96, 144, 173, 153, 226,
                202, 131, 165, 183, 175, 168, 187, 240, 272, 262, 195, 236,
244, 285, 249, 326, 345, 392, 268,
                455, 320, 418, 453, 468, 422, 447, 400, 323, 516, 478, 327,
450, 487, 535, 658, 573, 606, 659,
                543, 655, 722, 677, 567, 519, 703, 886), start = 1997.25,
deltat = 1/12)
time(RhelpUsage)
plot(RhelpUsage)

# OK, log() is probably a good transformation, given heteroscedasticity
(looks like multiplicative error)
LogRhU <- log(RhelpUsage)
plot(LogRhU)

# Humm... may be monthly sample is not the best interval (i.e., the one that
optimize signal/noise ratio?
# I can check that using the information theory, thanks to the turnogram()
function in pastecs:
library(pastecs)
RhU.turno <- turnogram(RhelpUsage, FUN = sum) # Info according to turning
points with different intervals
plot(RhU.turno)
summary(RhU.turno)
# Clearly, the signal/noise ratio is optimal for semester archives, let's
extract such a series...
RhU6 <- extract(RhU.turno)
plot(RhU6)
# ... and
LogRhU6 <- log(RhU6)
plot(LogRhU6)

# Well, we are obviously still in the ascending phase!
LogRhU6.lm <- lm(LogRhU6 ~ time(LogRhU6))
summary(LogRhU6.lm)
abline(coef = coef(LogRhU6.lm), col = 2)

plot(LogRhU6)	# Not linear...

# ... I don't like much polynomial regression
# I prefer to fit a simple asymptotic growth model
df1 <- data.frame(time=as.vector(time(LogRhU6)), load=as.vector(LogRhU6))
LogRhU6.nls <- nls(load ~ SSasympOff(time, Asym, lrc, c0), data = df1)
summary(LogRhU6.nls)

plot(df1$time, df1$load)
lines(df1$time, predict(LogRhU6.nls), col = 2)
# Not too bad! (well, nls ignores autocorrelation, but let's pretend it is
correct)

# A graph in estimated number of messages (according to ratio messages /
gzip size in March 2004):
plot(df1$time, exp(df1$load) * 1949 / 886, xlab = "time (years)", ylab =
"nbr. of messages / semester", main = "R-help mailing list usage
(estimation)")
lines(df1$time, exp(predict(LogRhU6.nls)) * 1949 / 886, col = 2)
abline(v = 1998:2003, col= "gray", lty = 2)
abline(h = (1:9)*1000, col= "gray", lty = 2)
# I like this graph and it makes me feel how fast the activity
# of R-help mailing list increases. In the near future, I should
# remember to switch to a "digest" mode of this list!



# Now... let's be silly and let's do some stupid extrapolations:
# 1) according to the model, the R-help mailing list started in:
coef(LogRhU6.nls)[3] # April 1992 (this model is notorious for bad estimate
of initial date for growth)

# 2) Asymptotic maximum monthly number of messages in R-help list is
#    given an indication of 1949/886 = 2.2 messages per KB (March 2004)
exp(coef(LogRhU6.nls)[1]) / 6 * 1949 / 886
# that is 25,000 monthly messages (OK, really, really stupid... that's just
for fun!)

# For the rest, I am not good at all,...
# and I am not an "extrapolator", so, do not expect I would predict the
number of
# messages in R-help mailing list for a next year or so!
=================


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gabor Grothendieck
Sent: Wednesday, 21 April, 2004 15:14
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Size of R user base


Philippe Grosjean <phgrosjean <at> sciviews.org> writes:
> We have also the activity in the R-help mailing list, which could be
> representative of the most active users, certainly. Does anyone have of
> figure of the number of messages in R-Help with time since its creation?
(it
> is probably available somewhere, but I don't know where).



If you check the r-help archive for last month

https://www.stat.math.ethz.ch/pipermail/r-help/2004-March/date.html

at CRAN it says at the top there were 1949 messages for March.

Looking at

	https://www.stat.math.ethz.ch/pipermail/r-help/

it shows the Gzip's size of each month's archives and from that
March had 886 KB of Gzip's text from which we can estimate 1949/886
= 2.2 messages per KB.  Over the last number of months there were
the following number of G'zipped KB for successive months over the
last 84 months:

55  19  19  18  19  17  35  27  47  55  32  50  55  41  49  50  28  53  42
81  54  99  60  84  80  76  75  78  61  83  97 141 122  96 144 173 153 226
202 131 165 183 175 168 187 240 272 262 195 236 244 285 249 326 345 392 268
455 320 418 453 468 422 447 400 323 516 478 327 450 487 535 658 573 606 659
543 655 722 677 567 519 703 886

where the last point 886KB is March 2004.  Summing those number and
using 2.2 messages per KB gives an estimate of about 50,000 messages
over that period of time.

Fitting a log linear model to those numbers gives:

	log(KB) = 3.2 + .043 i

where i is the month number which indicates that the archive size
(and hence the number of messages and possibly the user base) is
growing at 4% per month!

P.S.  The following web page gives the number of messages per day over
the last few days as a graph:

	http://gmane.org/info.php?group=gmane.comp.lang.r.general

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Apr 21 17:28:51 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 21 Apr 2004 16:28:51 +0100
Subject: [R] Does R runs on IRIX 6.5 platforms?
Message-ID: <E1BGJeo-0002Gf-AQ@serenity.mcc.ac.uk>

Hi,

I am trying to do a bayesian modelling and my Windows PC 
seems not to have enough memory to do it. So now i would like to 
run GeoR on an IRIX 6.5 platform (UNIX related as i understood). 
Do you know if R is compatible with this type of platform? I am not 
allowed to try it until i have an answer to that.

Thank you so much,

Monica



From p.dalgaard at biostat.ku.dk  Wed Apr 21 17:33:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Apr 2004 17:33:40 +0200
Subject: [R] formatted table to disk
In-Reply-To: <Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>
References: <Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>
Message-ID: <x23c6xqraj.fsf@biostat.ku.dk>

Randy Zelick <zelickr at pdx.edu> writes:

> Hello there,
> 
> Using R 1.8.1 and WinXP...
> 
> I would like to write a disk file (ascii text) of an R data frame. When I
> type the name of the object in the console window, it is sensibly and
> nicely formatted. When I use write.table, it is *not*, because the
> separation character is just a space.
> 
> I looked through the introductory material (my level) on the R website,
> but did not find a reference to this issue.
> 
> At a minimum I suppose there is a way to insert a tab character in the
> sep=" " field, but I don't know how to do that. Alternatively, I could
> deal with a variable specifying fixed-field widths for each of the
> variables in the frame. Didn't find that either.

"\t" for TAB
 
> Even so, you would loose the right-justification and the fact that R seems
> to take into account the width of the largest value in a column.
> 
> Of course the really slick deal would be to have paginated output where
> the column titles repeat at the top of the page (having entered a
> parameter for the number of lines on your page after margins and so
> forth).
> 
> A somewhat brutal solution would be to create all the text strings for
> making a LaTex document/table environment and write all that stuff out to
> the text file. Please don't tell me I have to do that!

Someone did that for you. Check out the xtable package.

> So is there a function for producing a file on disk with the same
> formatting as the screen?

sink() might well do it.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Wed Apr 21 17:41:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 21 Apr 2004 11:41:28 -0400
Subject: [R] Does R runs on IRIX 6.5 platforms?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C4B@usrymx25.merck.com>

If you had searched the list archive (as the posting guide suggests that you
do), you should have seen that some had success, even compiling it as 64-bit
app.

Andy

> From: Monica Palaseanu-Lovejoy
> Sent: Wednesday, April 21, 2004 11:29 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Does R runs on IRIX 6.5 platforms?
> 
> 
> Hi,
> 
> I am trying to do a bayesian modelling and my Windows PC 
> seems not to have enough memory to do it. So now i would like to 
> run GeoR on an IRIX 6.5 platform (UNIX related as i understood). 
> Do you know if R is compatible with this type of platform? I am not 
> allowed to try it until i have an answer to that.
> 
> Thank you so much,
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Wed Apr 21 17:46:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 16:46:40 +0100 (BST)
Subject: [R] extractAIC.survreg problem with df
In-Reply-To: <20040421151343.GB4706@med.unibs.it>
Message-ID: <Pine.LNX.4.44.0404211645460.24248-100000@gannet.stats>

There has been a change in the survreg function since that was written 
....

On Wed, 21 Apr 2004, Stefano Calza wrote:

> Hi everybody.
> 
> I'm having problems with the extractAIC.survreg function and the edf
> 
> I get weird results which I think are due to the fact that the function defines edf as (from the stats package in 1.9.0 source code)
> 
> ...
> ...
> n <- length(fit$residuals)
> edf <- n-fit$df.residual
> ...
> 
> But in a survreg object there's no attribute residuals!!??
> 
> if in the function I use instead
> 
> ...
> n <- length(residuals(fit)
> ...
> 
> everything's fine, or better
> 
> ...
> edf <- fit$df
> ...
> 
> Am I missing something?
> 
> TIA,
> Stefano
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From itayf at fhcrc.org  Wed Apr 21 17:57:57 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Wed, 21 Apr 2004 08:57:57 -0700 (PDT)
Subject: [R] Extracting information from webpages
In-Reply-To: <NEBBIKBOJBDOECLOBANEMEMICMAA.scott.waichler@verizon.net>
Message-ID: <Pine.LNX.4.44.0404210841110.13121-100000@cezanne.fhcrc.org>


On Tue, 20 Apr 2004, Scott Waichler wrote:

[snip]

> > should I go that route and avoid trying to parse webpages?  I was hoping
> > to learn 
> > how to extract information from webpages in general so I could apply the
> > techniques 
> > for other purposes too.
> > 

>From the Introduction to R Data Import/Export (1.8.1):
	....
	In general, statistical systems like R are not 
	particularly well suited to manipulations of large-scale  
	data. ...

And
	... it can be rewarding to use tools such as `awk' and 
	`perl' to manipulate data before import or after export.
	...

You imply that you wish to learn a general technique, therefore 
I'd vote for a general purpose language. Perl is a good choice.

	HTH
	Itay

--------------------------------------------------------------
itayf at fhcrc.org		Fred Hutchinson Cancer Research Center



From ferraria at ensisun.imag.fr  Wed Apr 21 18:07:57 2004
From: ferraria at ensisun.imag.fr (anthony.ferrari@ensimag.imag.fr)
Date: Wed, 21 Apr 2004 18:07:57 +0200 (MEST)
Subject: [R] calling R from java
Message-ID: <Pine.GSO.4.40.0404211755330.6469-100000@ensisun>


Hello,

I need to call R from a java(swing) application. I manage to do it with
something like :

Process p = Runtime.getRuntime().exec("R --slave")
OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
o.wrote("...")
... etc

but at the end no .Rdata file has been created and there are some data I
don't want to reload each time (for time execution reasons).
So,
Is it possible when calling R from java to create a .Rdata file to save
data and as a consequence when recalling in the same directory not to have
to reload those data ?
 (Maybe the option --slave need to be changed ? )


many thanks for helping me

best regards,
af
anthony.ferrari at ensimag.imag.fr



From jgoebel at diw.de  Wed Apr 21 18:36:00 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Wed, 21 Apr 2004 18:36:00 +0200
Subject: [R] formatted table to disk
In-Reply-To: <Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>
References: <Pine.A41.4.58.0404210736040.121004@homer09.u.washington.edu>
	<Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>
Message-ID: <20040421163600.GA6586@diw138134.diw-berlin.de>

What about sink()?
see help(sink)

greetings

jan

On Wed, 21 Apr 2004, Randy Zelick wrote:

> Hello there,
> 
> Using R 1.8.1 and WinXP...
> 
> I would like to write a disk file (ascii text) of an R data frame. When I
> type the name of the object in the console window, it is sensibly and
> nicely formatted. When I use write.table, it is *not*, because the
> separation character is just a space.
> 
> I looked through the introductory material (my level) on the R website,
> but did not find a reference to this issue.
> 
> At a minimum I suppose there is a way to insert a tab character in the
> sep=" " field, but I don't know how to do that. Alternatively, I could
> deal with a variable specifying fixed-field widths for each of the
> variables in the frame. Didn't find that either.
> 
> Even so, you would loose the right-justification and the fact that R seems
> to take into account the width of the largest value in a column.
> 
> Of course the really slick deal would be to have paginated output where
> the column titles repeat at the top of the page (having entered a
> parameter for the number of lines on your page after margins and so
> forth).
> 
> A somewhat brutal solution would be to create all the text strings for
> making a LaTex document/table environment and write all that stuff out to
> the text file. Please don't tell me I have to do that!
> 
> So is there a function for producing a file on disk with the same
> formatting as the screen?
> 
> Thanks,
> 
> =Randy=
> 
> R. Zelick				email: zelickr at pdx.edu
> Department of Biology			voice: 503-725-3086
> Portland State University		fax:   503-725-3888
> 
> mailing:
> P.O. Box 751
> Portland, OR 97207
> 
> shipping:
> 1719 SW 10th Ave, Room 246
> Portland, OR 97201
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K??nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From mmiller3 at iupui.edu  Wed Apr 21 18:38:35 2004
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Wed, 21 Apr 2004 11:38:35 -0500
Subject: [R] formatted table to disk
In-Reply-To: <Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>
	(Randy
	Zelick's message of "Wed, 21 Apr 2004 08:10:53 -0700 (PDT)")
References: <Pine.GSO.4.44.0404210751300.19730-100000@freke.odin.pdx.edu>
Message-ID: <8765bte16c.fsf@lumen.indyrad.iupui.edu>

>>>>> "Randy" == Randy Zelick <zelickr at pdx.edu> writes:

    > Hello there, Using R 1.8.1 and WinXP...

    > I would like to write a disk file (ascii text) of an R data
    > frame. When I type the name of the object in the console
    > window, it is sensibly and nicely formatted. When I use
    > write.table, it is *not*, because the separation character
    > is just a space.

Try sink.

    > A somewhat brutal solution would be to create all the text
    > strings for making a LaTex document/table environment and
    > write all that stuff out to the text file. Please don't
    > tell me I have to do that!

xtable will already do it for you :-)

Mike



From bates at stat.wisc.edu  Wed Apr 21 18:52:39 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 21 Apr 2004 11:52:39 -0500
Subject: Posting rates [was Re: [R] Size of R user base]
In-Reply-To: <loom.20040421T150926-75@post.gmane.org>
References: <200404210155.i3L1te9M362709@atlas.otago.ac.nz>
	<MABBLJDICACNFOLGIHJOIEJLEFAA.phgrosjean@sciviews.org>
	<loom.20040421T150926-75@post.gmane.org>
Message-ID: <6rhdvdmfxk.fsf_-_@bates4.stat.wisc.edu>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> P.S.  The following web page gives the number of messages per day over
> the last few days as a graph:
> 
> 	http://gmane.org/info.php?group=gmane.comp.lang.r.general

Interesting graphs.  Thanks for pointing out this URL.

Notice the very high posting/spam rates, which are a testament to the
effectiveness of the filters that Martin Maechler has installed on the
mailing list.



From rossini at blindglobe.net  Wed Apr 21 19:04:00 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 21 Apr 2004 10:04:00 -0700
Subject: [R] calling R from java
In-Reply-To: <Pine.GSO.4.40.0404211755330.6469-100000@ensisun> (anthony's
	message of "Wed, 21 Apr 2004 18:07:57 +0200 (MEST)")
References: <Pine.GSO.4.40.0404211755330.6469-100000@ensisun>
Message-ID: <85u0zddzzz.fsf@servant.blindglobe.net>


Look at the SJava package on Omegahat.org

Good luck.

best,
-tony


"anthony.ferrari at ensimag.imag.fr" <ferraria at ensisun.imag.fr> writes:

> Hello,
>
> I need to call R from a java(swing) application. I manage to do it with
> something like :
>
> Process p = Runtime.getRuntime().exec("R --slave")
> OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
> o.wrote("...")
> ... etc
>
> but at the end no .Rdata file has been created and there are some data I
> don't want to reload each time (for time execution reasons).
> So,
> Is it possible when calling R from java to create a .Rdata file to save
> data and as a consequence when recalling in the same directory not to have
> to reload those data ?
>  (Maybe the option --slave need to be changed ? )
>
>
> many thanks for helping me
>
> best regards,
> af
> anthony.ferrari at ensimag.imag.fr
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Apr 21 19:12:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 18:12:33 +0100 (BST)
Subject: [R] extractAIC.survreg problem with df
In-Reply-To: <Pine.LNX.4.44.0404211645460.24248-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404211806050.24568-100000@gannet.stats>

A few more comments:

1) fit$residuals is a *component* of a list, not an attribute.

2) Since n is evaluated as zero, this only affects the constant in AIC
and model selection was unaffected.

3) fit$df is a vector, and you want sum(fit$df).

I've updated the code in R-patched.


On Wed, 21 Apr 2004, Prof Brian Ripley wrote:

> There has been a change in the survreg function since that was written 
> ....
> 
> On Wed, 21 Apr 2004, Stefano Calza wrote:
> 
> > Hi everybody.
> > 
> > I'm having problems with the extractAIC.survreg function and the edf
> > 
> > I get weird results which I think are due to the fact that the function defines edf as (from the stats package in 1.9.0 source code)
> > 
> > ...
> > ...
> > n <- length(fit$residuals)
> > edf <- n-fit$df.residual
> > ...
> > 
> > But in a survreg object there's no attribute residuals!!??
> > 
> > if in the function I use instead
> > 
> > ...
> > n <- length(residuals(fit)
> > ...
> > 
> > everything's fine, or better
> > 
> > ...
> > edf <- fit$df
> > ...
> > 
> > Am I missing something?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Apr 21 19:36:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Apr 2004 19:36:38 +0200
Subject: Posting rates [was Re: [R] Size of R user base]
In-Reply-To: <6rhdvdmfxk.fsf_-_@bates4.stat.wisc.edu>
References: <200404210155.i3L1te9M362709@atlas.otago.ac.nz>
	<MABBLJDICACNFOLGIHJOIEJLEFAA.phgrosjean@sciviews.org>
	<loom.20040421T150926-75@post.gmane.org>
	<6rhdvdmfxk.fsf_-_@bates4.stat.wisc.edu>
Message-ID: <x2isft9qs9.fsf@biostat.ku.dk>

Douglas Bates <bates at stat.wisc.edu> writes:

> Gabor Grothendieck <ggrothendieck at myway.com> writes:
> 
> > P.S.  The following web page gives the number of messages per day over
> > the last few days as a graph:
> > 
> > 	http://gmane.org/info.php?group=gmane.comp.lang.r.general
> 
> Interesting graphs.  Thanks for pointing out this URL.
> 
> Notice the very high posting/spam rates, which are a testament to the
> effectiveness of the filters that Martin Maechler has installed on the
> mailing list.

...except that there seems to be something wrong with their graphs:
The posting/spam curve looks identical to the postings curve, and
according to the spam curve, the former should give a division by zero
for most of the range.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Wed Apr 21 20:01:50 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 21 Apr 2004 13:01:50 -0500
Subject: Posting rates [was Re: [R] Size of R user base]
In-Reply-To: <x2isft9qs9.fsf@biostat.ku.dk>
References: <200404210155.i3L1te9M362709@atlas.otago.ac.nz>
	<MABBLJDICACNFOLGIHJOIEJLEFAA.phgrosjean@sciviews.org>
	<loom.20040421T150926-75@post.gmane.org>
	<6rhdvdmfxk.fsf_-_@bates4.stat.wisc.edu>
	<x2isft9qs9.fsf@biostat.ku.dk>
Message-ID: <6r7jw9mcq9.fsf@bates4.stat.wisc.edu>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Douglas Bates <bates at stat.wisc.edu> writes:
> 
> > Gabor Grothendieck <ggrothendieck at myway.com> writes:
> > 
> > > P.S.  The following web page gives the number of messages per day over
> > > the last few days as a graph:
> > > 
> > > 	http://gmane.org/info.php?group=gmane.comp.lang.r.general
> > 
> > Interesting graphs.  Thanks for pointing out this URL.
> > 
> > Notice the very high posting/spam rates, which are a testament to the
> > effectiveness of the filters that Martin Maechler has installed on the
> > mailing list.
> 
> ...except that there seems to be something wrong with their graphs:
> The posting/spam curve looks identical to the postings curve, and
> according to the spam curve, the former should give a division by zero
> for most of the range.

Look carefully at the bottom plot.  It is two time-series plots
superimposed, not a plot of a ratio as the title indicates.



From ggrothendieck at myway.com  Wed Apr 21 20:03:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 21 Apr 2004 18:03:09 +0000 (UTC)
Subject: [R] Size of R user base
References: <loom.20040421T150926-75@post.gmane.org>
	<MABBLJDICACNFOLGIHJOKEKGEFAA.phgrosjean@sciviews.org>
Message-ID: <loom.20040421T195106-630@post.gmane.org>

Phillippe, Intriguing analysis.  

Lets pursue this but assume that instead of the archive size reaching
a monthly plateau that the cumulative archive size reaches a limit
at which point it ceases to grow and the lifetime of R is effectively
over.   We can fit a logistic growth curve to the cumulative KB size
as shown below.  From it we reach a number of interesting conclusions.
The archive will grow to 40MB which represents a cumulative number of
messages of about 88,000.  Note that this just about double what we
have seen to date which means that half the messages in the archive
that will ever be there are there now and we are just about at the
point of inflection in R's lifespan.  Furthermore, the amount of
time in months for R to grow from 10% of its plateau archive size
to 90% is log(81)/coef(res)[3] which is 73 months and is the time
that R can be expected to be the most vibrant, i.e. from the
time it began with some momentum to the point where it starts
saturating out.

   tt <- seq(84)
   res <- nls(cumsum(RhelpUsage) ~ a/(1+exp(b-c*tt)), start=list
(a=10000,b=1,c=.04))
   summary(res)

which gives:

Formula: cumsum(RhelpUsage) ~ a/(1 + exp(b - c * tt))

Parameters:
   Estimate Std. Error t value Pr(>|t|)    
a 3.996e+04  1.087e+03   36.77   <2e-16 ***
b 4.960e+00  1.831e-02  270.84   <2e-16 ***
c 6.038e-02  6.766e-04   89.24   <2e-16 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 155.4 on 81 degrees of freedom

Correlation of Parameter Estimates:
         a      b
b -0.04244       
c -0.92249 0.4191


Philippe Grosjean <phgrosjean <at> sciviews.org> writes:

: 
: Thank you, Gabor, for these stats. Here is what I did with it.
: 
: Philippe Grosjean
: 
: ===============
: # This is monthly R-help usage as given by the size of gzipped archives over
: the last 7 years
: RhelpUsage <- ts(c(55, 19, 19, 18, 19, 17, 35, 27, 47, 55, 32, 50, 55, 41,
: 49, 50, 28, 53, 42,
:                 81, 54, 99, 60, 84, 80, 76, 75, 78, 61, 83, 97, 141, 122,
: 96, 144, 173, 153, 226,
:                 202, 131, 165, 183, 175, 168, 187, 240, 272, 262, 195, 236,
: 244, 285, 249, 326, 345, 392, 268,
:                 455, 320, 418, 453, 468, 422, 447, 400, 323, 516, 478, 327,
: 450, 487, 535, 658, 573, 606, 659,
:                 543, 655, 722, 677, 567, 519, 703, 886), start = 1997.25,
: deltat = 1/12)
: time(RhelpUsage)
: plot(RhelpUsage)
: 
: # OK, log() is probably a good transformation, given heteroscedasticity
: (looks like multiplicative error)
: LogRhU <- log(RhelpUsage)
: plot(LogRhU)
: 
: # Humm... may be monthly sample is not the best interval (i.e., the one that
: optimize signal/noise ratio?
: # I can check that using the information theory, thanks to the turnogram()
: function in pastecs:
: library(pastecs)
: RhU.turno <- turnogram(RhelpUsage, FUN = sum) # Info according to turning
: points with different intervals
: plot(RhU.turno)
: summary(RhU.turno)
: # Clearly, the signal/noise ratio is optimal for semester archives, let's
: extract such a series...
: RhU6 <- extract(RhU.turno)
: plot(RhU6)
: # ... and
: LogRhU6 <- log(RhU6)
: plot(LogRhU6)
: 
: # Well, we are obviously still in the ascending phase!
: LogRhU6.lm <- lm(LogRhU6 ~ time(LogRhU6))
: summary(LogRhU6.lm)
: abline(coef = coef(LogRhU6.lm), col = 2)
: 
: plot(LogRhU6)	# Not linear...
: 
: # ... I don't like much polynomial regression
: # I prefer to fit a simple asymptotic growth model
: df1 <- data.frame(time=as.vector(time(LogRhU6)), load=as.vector(LogRhU6))
: LogRhU6.nls <- nls(load ~ SSasympOff(time, Asym, lrc, c0), data = df1)
: summary(LogRhU6.nls)
: 
: plot(df1$time, df1$load)
: lines(df1$time, predict(LogRhU6.nls), col = 2)
: # Not too bad! (well, nls ignores autocorrelation, but let's pretend it is
: correct)
: 
: # A graph in estimated number of messages (according to ratio messages /
: gzip size in March 2004):
: plot(df1$time, exp(df1$load) * 1949 / 886, xlab = "time (years)", ylab =
: "nbr. of messages / semester", main = "R-help mailing list usage
: (estimation)")
: lines(df1$time, exp(predict(LogRhU6.nls)) * 1949 / 886, col = 2)
: abline(v = 1998:2003, col= "gray", lty = 2)
: abline(h = (1:9)*1000, col= "gray", lty = 2)
: # I like this graph and it makes me feel how fast the activity
: # of R-help mailing list increases. In the near future, I should
: # remember to switch to a "digest" mode of this list!
: 
: # Now... let's be silly and let's do some stupid extrapolations:
: # 1) according to the model, the R-help mailing list started in:
: coef(LogRhU6.nls)[3] # April 1992 (this model is notorious for bad estimate
: of initial date for growth)
: 
: # 2) Asymptotic maximum monthly number of messages in R-help list is
: #    given an indication of 1949/886 = 2.2 messages per KB (March 2004)
: exp(coef(LogRhU6.nls)[1]) / 6 * 1949 / 886
: # that is 25,000 monthly messages (OK, really, really stupid... that's just
: for fun!)
: 
: # For the rest, I am not good at all,...
: # and I am not an "extrapolator", so, do not expect I would predict the
: number of
: # messages in R-help mailing list for a next year or so!
: =================
: 
: -----Original Message-----
: From: r-help-bounces <at> stat.math.ethz.ch
: [mailto:r-help-bounces <at> stat.math.ethz.ch]On Behalf Of Gabor Grothendieck
: Sent: Wednesday, 21 April, 2004 15:14
: To: r-help <at> stat.math.ethz.ch
: Subject: Re: [R] Size of R user base
: 
: 
: Philippe Grosjean <phgrosjean <at> sciviews.org> writes:
: > We have also the activity in the R-help mailing list, which could be
: > representative of the most active users, certainly. Does anyone have of
: > figure of the number of messages in R-Help with time since its creation?
: (it
: > is probably available somewhere, but I don't know where).
: 
: If you check the r-help archive for last month
: 
: https://www.stat.math.ethz.ch/pipermail/r-help/2004-March/date.html
: 
: at CRAN it says at the top there were 1949 messages for March.
: 
: Looking at
: 
: 	https://www.stat.math.ethz.ch/pipermail/r-help/
: 
: it shows the Gzip's size of each month's archives and from that
: March had 886 KB of Gzip's text from which we can estimate 1949/886
: = 2.2 messages per KB.  Over the last number of months there were
: the following number of G'zipped KB for successive months over the
: last 84 months:
: 
: 55  19  19  18  19  17  35  27  47  55  32  50  55  41  49  50  28  53  42
: 81  54  99  60  84  80  76  75  78  61  83  97 141 122  96 144 173 153 226
: 202 131 165 183 175 168 187 240 272 262 195 236 244 285 249 326 345 392 268
: 455 320 418 453 468 422 447 400 323 516 478 327 450 487 535 658 573 606 659
: 543 655 722 677 567 519 703 886
: 
: where the last point 886KB is March 2004.  Summing those number and
: using 2.2 messages per KB gives an estimate of about 50,000 messages
: over that period of time.
: 
: Fitting a log linear model to those numbers gives:
: 
: 	log(KB) = 3.2 + .043 i
: 
: where i is the month number which indicates that the archive size
: (and hence the number of messages and possibly the user base) is
: growing at 4% per month!
: 
: P.S.  The following web page gives the number of messages per day over
: the last few days as a graph:
: 
: 	http://gmane.org/info.php?group=gmane.comp.lang.r.general



From NIANQING.XIAO at saic.com  Wed Apr 21 20:30:36 2004
From: NIANQING.XIAO at saic.com (Xiao, Nick [RA])
Date: Wed, 21 Apr 2004 14:30:36 -0400
Subject: [R] calling R from java
Message-ID: <E7A58A017A8B7B4BA9DC772ED12E423B030B0931@US-Germantown.mail.saic.com>

Hi, 

There are two ways to do what you want to do: 

1. Explicitly use save() in R. Something like: o.write("save(obj1, obj2,
file=\"filename\")") to save a select set of R objects to a file you specify
(can be retrieved with load later), or

2. When you quit R: o.write("quit(\"yes\")\n") instead of
o.write("quit(\"no\")\n") to save everything to the .Rdata

HTH, 

Nick

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
anthony.ferrari at ensimag.imag.fr
Sent: Wednesday, April 21, 2004 12:08 PM
To: r-help at stat.math.ethz.ch
Subject: [R] calling R from java



Hello,

I need to call R from a java(swing) application. I manage to do it with
something like :

Process p = Runtime.getRuntime().exec("R --slave")
OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
o.wrote("...")
... etc

but at the end no .Rdata file has been created and there are some data I
don't want to reload each time (for time execution reasons).
So,
Is it possible when calling R from java to create a .Rdata file to save
data and as a consequence when recalling in the same directory not to have
to reload those data ?
 (Maybe the option --slave need to be changed ? )


many thanks for helping me

best regards,
af
anthony.ferrari at ensimag.imag.fr

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Apr 21 20:24:07 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 21 Apr 2004 11:24:07 -0700
Subject: [R] 3D histogram
In-Reply-To: <BF74FADD4B44554CA7E53D0B5242CD6A018DAE91@evd-s7014.evd.admin.ch>
References: <BF74FADD4B44554CA7E53D0B5242CD6A018DAE91@evd-s7014.evd.admin.ch>
Message-ID: <4086BC47.2010808@pdf.com>

Hi, Lorenz: 

      I mostly agree with what you say, including the reference to 
Tufte.  However, I find that perspective plots can help build intuition 
about relationships that are not obtainable from other graphics.  It's 
similar to the difference between perspective plots and floor plans, 
etc., in architecture:  Professionals use both for different purposes. 

      I agree that perspective graphics, etc., often obfuscate more than 
they display.  However, I think they can have some utility. 

      I just did a search of "www.r-project.org" -> search -> "R site 
search".  The term "3-d histogram" produced too many hits.  "Bivariate 
histogram" led me to hist2d {gregmisc}.  I haven't tried it, but it 
looks like it might do what Tine wants, possibly in conjunction with 
persp and contour, as described in the examples. 

      spencer graves

Lorenz.Gygax at fat.admin.ch wrote:

>Hi Tine,
>
>  
>
>>I have been trying to look for documentation on making a 3D 
>>histogram in R (so a histogram on which there are several distributions 
>>are plotted), but I cannot find anything on it. Is it possible to
>>construct such histograms and should use a special package (f.e. R.Basic)?
>>    
>>
>
>In my opinion you should never use 3D graphs. Due to the perspective on
>these graphs it is always difficult to compare different values of the
>graphed data and usually it is also difficult to get a good estimate of the
>actual values as the axes are not always aligned with the data of interst.
>
>You might want to ask yourself whether your data is really as simple that it
>can be shown in bars (or could you e.g. rather use boxlplots). If you want
>to graph your data as in barplots, you could plot the values of your bars
>using points in a 2D diagram and e.g. connect the points that would have
>been in one row/column in the 3D diagram.
>
>If you are interested in the graphical presentation of quantitative data, I
>can only recommend:
>
>@Book{Tuf:99,
>  author = 	 {Tufte, Edward R},
>  title = 	 {The Visual Display of Quantitative Information},
>  publisher = 	 {Graphic Press},
>  year = 	 {1999},
>  address = 	 {Cheshire, Connecticut},
>  edition = 	 {17th printing}
>}
>
>Hope this helps. Lorenz
>- 
>Lorenz Gygax, Dr. sc. nat.
>Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      
>
>Tag der offenen T??r, 11./12. Juni 2004: http://www.fat.ch/2004
>
>Center for proper housing of ruminants and pigs
>Swiss Veterinary Office
>agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
>Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ripley at stats.ox.ac.uk  Wed Apr 21 20:35:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 21 Apr 2004 19:35:58 +0100 (BST)
Subject: [R] calling R from java
In-Reply-To: <E7A58A017A8B7B4BA9DC772ED12E423B030B0931@US-Germantown.mail.saic.com>
Message-ID: <Pine.LNX.4.44.0404211933030.24782-100000@gannet.stats>

A third way is to start R with --slave --save (in that order).

On Wed, 21 Apr 2004, Xiao, Nick [RA] wrote:

> There are two ways to do what you want to do: 
> 
> 1. Explicitly use save() in R. Something like: o.write("save(obj1, obj2,
> file=\"filename\")") to save a select set of R objects to a file you specify
> (can be retrieved with load later), or

Or use save.image to save all the objects.

> 2. When you quit R: o.write("quit(\"yes\")\n") instead of
> o.write("quit(\"no\")\n") to save everything to the .Rdata
> 
> HTH, 
> 
> Nick
> 
> -----Original Message-----
> Hello,
> 
> I need to call R from a java(swing) application. I manage to do it with
> something like :
> 
> Process p = Runtime.getRuntime().exec("R --slave")
> OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
> o.wrote("...")
> ... etc
> 
> but at the end no .Rdata file has been created and there are some data I
> don't want to reload each time (for time execution reasons).
> So,
> Is it possible when calling R from java to create a .Rdata file to save
> data and as a consequence when recalling in the same directory not to have
> to reload those data ?
>  (Maybe the option --slave need to be changed ? )

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From james at staarfunds.com  Wed Apr 21 20:40:11 2004
From: james at staarfunds.com (James Thomas)
Date: Wed, 21 Apr 2004 14:40:11 -0400 (BST)
Subject: [R] Re: [openMosix-general] openMosix and R:  File I/O issues?
In-Reply-To: <200404201002.i3KA0NZ0023948@hypatia.math.ethz.ch>
	<40853EC5.6030605@staarfunds.com>
Message-ID: <200404211840.OAA01494@hank.bcentralhost.com>

Thanks for the response.  Are there any documented reasons as to why oM "dislikes" I/O?  Our network hardware works fine with SNOW and rMPI, so our problem must lie somewhere within the mosix framework.

Jim

---- "(A.J. Rossini)" <rossini at blindglobe.net> wrote:
>
> 
> Other than OpenMOSIX disliking I/O, I havn't seen any problems.  I
> would suspect network hardware and drivers -- while generally robust, cluster
> computations stress it much more than daily use, with less tolerance
> for problems.
> 
> best,
> -tony
> 
> 
> Jim Thomas <james at staarfunds.com> writes:
> 
> > Hi there,
> >
> > We're attempting to run an LVQ analysis over a cluster of machines via
> > R and openMosix.  R spawns several child processes simply by writing
> > commands to several files and using system() to start a slave
> > process. The processes migrate perfectly, and often finish with no
> > reported errors, writing their results into respective files for the
> > parent process to piece together.  However, occasionally we have had
> > the problem that the results from a child process never make it into a
> > file. The process finishes, and exits, with no errors - but the file
> > never turns up.  Repeated tests with the same data have shown that the
> > specific process that dies is random, and stress tests of R I/O have
> > shown that there are no issues there.  Does anyone know of I/O issues
> > with openMosix, either specifically related to R or not?
> >
> > using:
> > openMosix kernel 2.4.21
> > R 1.8.1
> > RedHat Enterprise Edition
> >
> > Thanks,
> > Jim
> >
> >
> >
> > -------------------------------------------------------
> > This SF.Net email is sponsored by: IBM Linux Tutorials
> > Free Linux tutorial presented by Daniel Robbins, President and CEO of
> > GenToo technologies. Learn everything from fundamentals to system
> > administration.http://ads.osdn.com/?ad_id=1470&alloc_id=3638&op=click
> > _______________________________________________
> > openMosix-general mailing list
> > openMosix-general at lists.sourceforge.net
> > https://lists.sourceforge.net/lists/listinfo/openmosix-general
> >
> 
> -- 
> rossini at u.washington.edu            http://www.analytics.washington.edu/ 
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
> 
> CONFIDENTIALITY NOTICE: This e-mail message and any attachments may be
> confidential and privileged. If you received this message in error,
> please destroy it and notify the sender. Thank you.
> 
>



From flom at ndri.org  Wed Apr 21 20:44:19 2004
From: flom at ndri.org (Peter Flom)
Date: Wed, 21 Apr 2004 14:44:19 -0400
Subject: [R] 3D histogram
Message-ID: <s08688cd.055@MAIL.NDRI.ORG>

I agree with Lorenz, but would add a couple other recommended books:

Cleveland, WS (1994) The Elements of Graphing Data 

and

Cleveland WS (1993) Visualizing Data.

both are pub. by Hobart Press, in Summit NJ, USA

I think the problem with 3 D graphics (at least for me) is that not only can they obscure patterns that exist, they can sometimes show patterns that do NOT exist.  Not a good deal  But others may be better at using them than I

HTH

Peter

Lorenz.Gygax at fat.admin.ch wrote:

>Hi Tine,
>
>  
>
>>I have been trying to look for documentation on making a 3D 
>>histogram in R (so a histogram on which there are several distributions 
>>are plotted), but I cannot find anything on it. Is it possible to
>>construct such histograms and should use a special package (f.e. R.Basic)?
>>    
>>
>
>In my opinion you should never use 3D graphs. Due to the perspective on
>these graphs it is always difficult to compare different values of the
>graphed data and usually it is also difficult to get a good estimate of the
>actual values as the axes are not always aligned with the data of interst.
>
>You might want to ask yourself whether your data is really as simple that it
>can be shown in bars (or could you e.g. rather use boxlplots). If you want
>to graph your data as in barplots, you could plot the values of your bars
>using points in a 2D diagram and e.g. connect the points that would have
>been in one row/column in the 3D diagram.
>
>If you are interested in the graphical presentation of quantitative data, I
>can only recommend:
>
>@Book{Tuf:99,
>  author = 	 {Tufte, Edward R},
>  title = 	 {The Visual Display of Quantitative Information},
>  publisher = 	 {Graphic Press},
>  year = 	 {1999},
>  address = 	 {Cheshire, Connecticut},
>  edition = 	 {17th printing}
>}
>
>Hope this helps. Lorenz
>- 
>Lorenz Gygax, Dr. sc. nat.
>Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      
>
>Tag der offenen T??r, 11./12. Juni 2004: http://www.fat.ch/2004 
>
>Center for proper housing of ruminants and pigs
>Swiss Veterinary Office
>agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
>Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From rossini at blindglobe.net  Wed Apr 21 20:48:08 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 21 Apr 2004 11:48:08 -0700
Subject: [R] Re: [openMosix-general] openMosix and R:  File I/O issues?
In-Reply-To: <200404211840.OAA01494@hank.bcentralhost.com> (James Thomas's
	message of "Wed, 21 Apr 2004 14:40:11 -0400 (BST)")
References: <200404211840.OAA01494@hank.bcentralhost.com>
Message-ID: <8565btcglz.fsf@servant.blindglobe.net>


Yes.  Read the docs -- it's not so much disliking I/O, as the need to
route I/O back to the originating node (depends on the type of I/O,
whether oMFS is enabled, etc).

James Thomas <james at staarfunds.com> writes:

> Thanks for the response.  Are there any documented reasons as to why
> oM "dislikes" I/O?  Our network hardware works fine with SNOW and
> rMPI, so our problem must lie somewhere within the mosix framework.
>
> Jim
>
> ---- "(A.J. Rossini)" <rossini at blindglobe.net> wrote:
>>
>> 
>> Other than OpenMOSIX disliking I/O, I havn't seen any problems.  I
>> would suspect network hardware and drivers -- while generally
>> robust, cluster computations stress it much more than daily use,
>> with less tolerance for problems.
>> 
>> best, -tony
>> 
>> 
>> Jim Thomas <james at staarfunds.com> writes:
>> 
>> > Hi there,
>> >
>> > We're attempting to run an LVQ analysis over a cluster of machines
>> > via R and openMosix.  R spawns several child processes simply by
>> > writing commands to several files and using system() to start a
>> > slave process. The processes migrate perfectly, and often finish
>> > with no reported errors, writing their results into respective
>> > files for the parent process to piece together.  However,
>> > occasionally we have had the problem that the results from a child
>> > process never make it into a file. The process finishes, and
>> > exits, with no errors - but the file never turns up.  Repeated
>> > tests with the same data have shown that the specific process that
>> > dies is random, and stress tests of R I/O have shown that there
>> > are no issues there.  Does anyone know of I/O issues with
>> > openMosix, either specifically related to R or not?
>> >
>> > using: openMosix kernel 2.4.21 R 1.8.1 RedHat Enterprise Edition
>> >
>> > Thanks, Jim
>> >
>> >
>> >
>> > ------------------------------------------------------- This
>> > SF.Net email is sponsored by: IBM Linux Tutorials Free Linux
>> > tutorial presented by Daniel Robbins, President and CEO of GenToo
>> > technologies. Learn everything from fundamentals to system
>> > administration.http://ads.osdn.com/?ad_id=1470&alloc_id=3638&op=click
>> > _______________________________________________ openMosix-general
>> > mailing list openMosix-general at lists.sourceforge.net
>> > https://lists.sourceforge.net/lists/listinfo/openmosix-general
>> >
>>  -- rossini at u.washington.edu http://www.analytics.washington.edu/
>> Biomedical and Health Informatics University of Washington
>> Biostatistics, SCHARP/HVTN Fred Hutchinson Cancer Research Center UW
>> (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
>> FHCRC (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>> 
>> CONFIDENTIALITY NOTICE: This e-mail message and any attachments may
>> be confidential and privileged. If you received this message in
>> error, please destroy it and notify the sender. Thank you.
>> 
>> 
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From dmurdoch at pair.com  Wed Apr 21 20:50:13 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 21 Apr 2004 14:50:13 -0400
Subject: [R] 3D histogram
In-Reply-To: <s08688cd.055@MAIL.NDRI.ORG>
References: <s08688cd.055@MAIL.NDRI.ORG>
Message-ID: <ddgd809rakdv4ds25i6ejcovh3qttbg12n@4ax.com>

On Wed, 21 Apr 2004 14:44:19 -0400, "Peter Flom" <flom at ndri.org> wrote
:

>I agree with Lorenz, but would add a couple other recommended books:
>
>Cleveland, WS (1994) The Elements of Graphing Data 
>
>and
>
>Cleveland WS (1993) Visualizing Data.
>
>both are pub. by Hobart Press, in Summit NJ, USA
>
>I think the problem with 3 D graphics (at least for me) is that not only can they obscure patterns that exist, they can sometimes show patterns that do NOT exist.  Not a good deal  But others may be better at using them than I

3-D graphics can reveal a lot, but they generally need to be dynamic,
not static.  The only static ones that I find useful are the stereo
pairs, but those are too much trouble for most purposes.

In R, Daniel Adler's rgl package does dynamic graphics.  (I wrote a
package a few years ago that was also called rgl; I've renamed mine to
djmrgl, and have agreed with Daniel that we're better off with his.)  

Duncan Murdoch



From rossini at blindglobe.net  Wed Apr 21 21:03:10 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 21 Apr 2004 12:03:10 -0700
Subject: [R] 3D histogram
In-Reply-To: <ddgd809rakdv4ds25i6ejcovh3qttbg12n@4ax.com> (Duncan Murdoch's
	message of "Wed, 21 Apr 2004 14:50:13 -0400")
References: <s08688cd.055@MAIL.NDRI.ORG>
	<ddgd809rakdv4ds25i6ejcovh3qttbg12n@4ax.com>
Message-ID: <85llkpb1ch.fsf@servant.blindglobe.net>

Duncan Murdoch <dmurdoch at pair.com> writes:

> On Wed, 21 Apr 2004 14:44:19 -0400, "Peter Flom" <flom at ndri.org> wrote
> :
>
>>I agree with Lorenz, but would add a couple other recommended books:
>>
>>Cleveland, WS (1994) The Elements of Graphing Data 
>>
>>and
>>
>>Cleveland WS (1993) Visualizing Data.
>>
>>both are pub. by Hobart Press, in Summit NJ, USA
>>
>>I think the problem with 3 D graphics (at least for me) is that not only can they obscure patterns that exist, they can sometimes show patterns that do NOT exist.  Not a good deal  But others may be better at using them than I
>
> 3-D graphics can reveal a lot, but they generally need to be dynamic,
> not static.  The only static ones that I find useful are the stereo
> pairs, but those are too much trouble for most purposes.
>
> In R, Daniel Adler's rgl package does dynamic graphics.  (I wrote a
> package a few years ago that was also called rgl; I've renamed mine to
> djmrgl, and have agreed with Daniel that we're better off with his.)  

I agree with Duncan -- rgl works quite well, and we are building on it
for a VR data analysis environment.

Another option is Orca/rorca, which provides a higher level of
abstraction along with linked multiviews, but you need to have SJava
installed and working for that to work for you:

        http://www.analytics.washington.edu/orca

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From james at staarfunds.com  Wed Apr 21 21:13:57 2004
From: james at staarfunds.com (Jim Thomas)
Date: Wed, 21 Apr 2004 15:13:57 -0400
Subject: [R] Re: [openMosix-general] openMosix and R:  File I/O issues?
In-Reply-To: <D9A95B4B7B20354992E165EEADA319990233D0F2@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA319990233D0F2@uswpmx00.merck.com>
Message-ID: <4086C7F5.4070103@staarfunds.com>

Memory could be an issue. The three nodes in the cluster we are using 
only have 512 MB. The machines are otherwise identical in terms of 
hardware. They are dual PIIIs (purchased from Penguin Computing in 
2001). (If you need specs on motherboard etc, we can get that info 
together.)

The installed operating system is RHEL-3, standard workstation with some 
additional libraries (e.g., all of the development ones) and packages 
(e.g., tarball-based installs of R & Octave).

Swap was set to twice the RAM.

Not sure about how big the R processes are. Under Matlab and using a 
network version of LVQ, the processes were under 256 MB. It could be a 
lot larger in R, given that KNN is explicitly used in certain stages of 
LVQ and that should consume a lot of memory.

oMFS is not currently being used, nor any other network filesystem... do 
you think this would be helpful by allowing the large processes to 
remain on the nodes to which they migrated when writing files.  The 
actual file I/O is very small.... only approximately 5 - 50 results are 
being written into a file that remains under 2k in size.  However, I 
believe that these results are not derived until LVQ finishes so there's 
no way to write a few resluts periodically.

Thanks for your help!

Jim


Huntsinger, Reid wrote:

>Can you have the R processes open the file, write, and close periodically?
>Just to see where they die? Is it that the file never gets flushed to disk
>before the process dies for some reason? Maybe processes migrate back for
>file i/o and the total required memory is just too large? R processes can
>easily have large memory requirements.
>
>More details on your setup would probably help: are you using omfs? other
>network file systems? How is the cluster set up? How much RAM and swap? How
>big are the R processes?
>
>Reid Huntsinger
>
>-----Original Message-----
>From: openmosix-general-admin at lists.sourceforge.net
>[mailto:openmosix-general-admin at lists.sourceforge.net] On Behalf Of Jim
>Thomas
>Sent: Tuesday, April 20, 2004 11:16 AM
>To: r-help at stat.math.ethz.ch; openmosix-general at lists.sourceforge.net
>Subject: [openMosix-general] openMosix and R: File I/O issues?
>
>
>Hi there,
>
>We're attempting to run an LVQ analysis over a cluster of machines via R 
>and openMosix.  R spawns several child processes simply by writing 
>commands to several files and using system() to start a slave process. 
> The processes migrate perfectly, and often finish with no reported 
>errors, writing their results into respective files for the parent 
>process to piece together.  However, occasionally we have had the 
>problem that the results from a child process never make it into a file. 
> The process finishes, and exits, with no errors - but the file never 
>turns up.  Repeated tests with the same data have shown that the 
>specific process that dies is random, and stress tests of R I/O have 
>shown that there are no issues there.  Does anyone know of I/O issues 
>with openMosix, either specifically related to R or not?
>
>using:
>openMosix kernel 2.4.21
>R 1.8.1
>RedHat Enterprise Edition
>
>Thanks,
>Jim
>
>
>
>-------------------------------------------------------
>This SF.Net email is sponsored by: IBM Linux Tutorials
>Free Linux tutorial presented by Daniel Robbins, President and CEO of
>GenToo technologies. Learn everything from fundamentals to system
>administration.http://ads.osdn.com/?ad_id=1470&alloc_id=3638&op=click
>_______________________________________________
>openMosix-general mailing list
>openMosix-general at lists.sourceforge.net
>https://lists.sourceforge.net/lists/listinfo/openmosix-general
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>
>
>  
>



From rossini at blindglobe.net  Wed Apr 21 21:15:18 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 21 Apr 2004 12:15:18 -0700
Subject: [R] 3D histogram
In-Reply-To: <85llkpb1ch.fsf@servant.blindglobe.net> (A. J. Rossini's
	message of "Wed, 21 Apr 2004 12:03:10 -0700")
References: <s08688cd.055@MAIL.NDRI.ORG>
	<ddgd809rakdv4ds25i6ejcovh3qttbg12n@4ax.com>
	<85llkpb1ch.fsf@servant.blindglobe.net>
Message-ID: <85ekqhb0s9.fsf@servant.blindglobe.net>

rossini at blindglobe.net (A.J. Rossini) writes:

  <-- stuff -->

And for completeness, I should mention ggobi/Rggobi, which works well
if you can get it installed (it does come preinstalled on Quantian and
is available from Debian).


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From reid_huntsinger at merck.com  Wed Apr 21 21:25:24 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 21 Apr 2004 15:25:24 -0400
Subject: [R] RE: [openMosix-general] openMosix and R:  File I/O issues?
Message-ID: <D9A95B4B7B20354992E165EEADA319990233D0FD@uswpmx00.merck.com>

It's generally said that OM needs lots more swap than plain Linux. There has
to be someplace to juggle processes around. We use 4 GB nodes with something
like 12 GB swap; that plus round-robin logins to distribute home nodes
seemed to solve a lot of our problems. 

To use oMFS to allow processes to write from the node they've migrated to
you also need DFSA enabled. That sounds like a great idea but it seems to
cause problems in our setup--that is, with DFSA off we go a lot longer
without oMFS misbehavior or other odd phenomena than when DFSA is on. 

How many processes do you run at a time?

Reid Huntsinger

-----Original Message-----
From: Jim Thomas [mailto:james at staarfunds.com] 
Sent: Wednesday, April 21, 2004 3:14 PM
To: Huntsinger, Reid
Cc: r-help at stat.math.ethz.ch; openmosix-general at lists.sourceforge.net
Subject: Re: [openMosix-general] openMosix and R: File I/O issues?


Memory could be an issue. The three nodes in the cluster we are using 
only have 512 MB. The machines are otherwise identical in terms of 
hardware. They are dual PIIIs (purchased from Penguin Computing in 
2001). (If you need specs on motherboard etc, we can get that info 
together.)

The installed operating system is RHEL-3, standard workstation with some 
additional libraries (e.g., all of the development ones) and packages 
(e.g., tarball-based installs of R & Octave).

Swap was set to twice the RAM.

Not sure about how big the R processes are. Under Matlab and using a 
network version of LVQ, the processes were under 256 MB. It could be a 
lot larger in R, given that KNN is explicitly used in certain stages of 
LVQ and that should consume a lot of memory.

oMFS is not currently being used, nor any other network filesystem... do 
you think this would be helpful by allowing the large processes to 
remain on the nodes to which they migrated when writing files.  The 
actual file I/O is very small.... only approximately 5 - 50 results are 
being written into a file that remains under 2k in size.  However, I 
believe that these results are not derived until LVQ finishes so there's 
no way to write a few resluts periodically.

Thanks for your help!

Jim


Huntsinger, Reid wrote:

>Can you have the R processes open the file, write, and close periodically?
>Just to see where they die? Is it that the file never gets flushed to disk
>before the process dies for some reason? Maybe processes migrate back for
>file i/o and the total required memory is just too large? R processes can
>easily have large memory requirements.
>
>More details on your setup would probably help: are you using omfs? other
>network file systems? How is the cluster set up? How much RAM and swap? How
>big are the R processes?
>
>Reid Huntsinger
>
>-----Original Message-----
>From: openmosix-general-admin at lists.sourceforge.net
>[mailto:openmosix-general-admin at lists.sourceforge.net] On Behalf Of Jim
>Thomas
>Sent: Tuesday, April 20, 2004 11:16 AM
>To: r-help at stat.math.ethz.ch; openmosix-general at lists.sourceforge.net
>Subject: [openMosix-general] openMosix and R: File I/O issues?
>
>
>Hi there,
>
>We're attempting to run an LVQ analysis over a cluster of machines via R 
>and openMosix.  R spawns several child processes simply by writing 
>commands to several files and using system() to start a slave process. 
> The processes migrate perfectly, and often finish with no reported 
>errors, writing their results into respective files for the parent 
>process to piece together.  However, occasionally we have had the 
>problem that the results from a child process never make it into a file. 
> The process finishes, and exits, with no errors - but the file never 
>turns up.  Repeated tests with the same data have shown that the 
>specific process that dies is random, and stress tests of R I/O have 
>shown that there are no issues there.  Does anyone know of I/O issues 
>with openMosix, either specifically related to R or not?
>
>using:
>openMosix kernel 2.4.21
>R 1.8.1
>RedHat Enterprise Edition
>
>Thanks,
>Jim
>
>
>
>-------------------------------------------------------
>This SF.Net email is sponsored by: IBM Linux Tutorials
>Free Linux tutorial presented by Daniel Robbins, President and CEO of
>GenToo technologies. Learn everything from fundamentals to system
>administration.http://ads.osdn.com/?ad_id=1470&alloc_id=3638&op=click
>_______________________________________________
>openMosix-general mailing list
>openMosix-general at lists.sourceforge.net
>https://lists.sourceforge.net/lists/listinfo/openmosix-general
>
>
>
>---------------------------------------------------------------------------
---
>Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
Jersey, USA 08889), and/or its affiliates (which may be known outside the
United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
Banyu) that may be confidential, proprietary copyrighted and/or legally
privileged. It is intended solely for the use of the individual or entity
named on this message.  If you are not the intended recipient, and have
received this message in error, please notify us immediately by reply e-mail
and then delete it from your system.
>---------------------------------------------------------------------------
---
>
>
>  
>




------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From dmurdoch at pair.com  Wed Apr 21 22:33:05 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 21 Apr 2004 16:33:05 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>
References: <n2cd80tc8a6ontje1ovo7hsnv5jsf2ecr5@4ax.com>
	<Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>
Message-ID: <1cmd80pvpmhlrkhll66lm6qmegr0c7h6or@4ax.com>

On Wed, 21 Apr 2004 18:40:05 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote :

>On Wed, 21 Apr 2004, Duncan Murdoch wrote:
>

>> I think it would be more useful for debugging purposes to do them one
>> at a time.  Do we still get an error message about HOMEDRIVE if we do
>> the above?  What if we set only HOMEPATH?
>
>Unfortunately you get the same message unless both are set correctly, so 
>setting just one on the command line will tell you nothing.  I've changed 
>the message for R-patched so we can tell them apart.

I've just uploaded builds of r-patched and r-devel to CRAN that
incorporate this change.  They should be visible tomorrow at 

<http://cran.r-project.org/bin/windows/base>

Follow the links near the top of the page; the ones in the main part
are for the unpatched 1.9.0 release.

Duncan Murdoch



From kjetil at entelnet.bo  Wed Apr 21 23:49:03 2004
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 21 Apr 2004 17:49:03 -0400
Subject: [R] multistratum glm?
Message-ID: <4086B40F.14240.8BC25@localhost>

On 18 Apr 2004 at 13:47, Christophe Pallier wrote:

You should probably look into glmmPQL (package MASS)
or GLMM (package lme4).

Kjetil Halvorsen

> Hello,
> 
> I routinely use aov and and the Error term to perform analyses of
> variance of experiments with 'within-subject' factors. I wonder
> whether a notion like 'multistratum models' exists for glm models 
when
> performing a logit analysis (without being 100%  sure whether this
> would make sense).
> 
> I have data of an experiment where the outcome is a categorical
> variable:
> 
> 20 individuals listened to 80 synthetic utterances (distributed in 
4
> types) and were ask classify them into four categories. (The 
variables
> in the data.frame are 'subject', 'sentence', 'type', and 
'response')
> 
> Here is the table of counts table(type,response):
> 
>        response
> type  a   b  c   d
>   a 181 166 42  11
>   b  69 170 72  89
>   c  90 174 75  61
>   d  14 125 53 208
> 
> 
> There are several questions of interest, such as, for example:
> 
> - are responses distibuted in the same way for the different types?
> 
> - are the numbers of 'a' responses for the 'b' and 'c' types 
> significantly different?
> 
> - is the proportion of 'd' over 'a' responses different for the 'b'
> and 'c'  categories?
> 
> ...  
> 
> (I want to make inferences for the population of potential subjects 
on
> the one hand, and on the population of potential sentences on the
> other hand).
> 
> If the responses were continuous, I would just run two one-way 
anovas:
> one with the factor type over the means by subject*type, and the 
other
> with the factor type over the means by sentences (in type). And use
> t.test to compare between different pairs of types.
> 
> Now, as the answers are categorical, I am not sure about the 
correct
> approach and how to use R to perform such an analysis.
> 
> I could treat response as a factor, and use percentages of 
responses
> per subject in each cell of response*type, and run an anova on
> that...[ 
aov(percentage~response*type+Error(subject/(response*type))]
> But it seems incorrect to me to use the response of the subject as 
an
> independent variable (though I do not have a forceful argument).
> 
> Simple Chi-square tests are not the answer either, as a given 
subject
> contributed several times (80) to the counts in the table above.
> 
> My reading of MASS and of several other books suggest the use of
> logit/multinomial models when the response is categorical. But in 
all
> the examples provided, the units of analysis contribute only one
> measurement. Should I include the subject and sentences factors in 
the
> formula? But then they would be treated as fixed-factors in the
> analysis, would they not?
> 
> 
> Any suggestion is welcome.
> 
> Christophe Pallier
> www.pallier.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From christof.bigler at colorado.edu  Thu Apr 22 00:01:09 2004
From: christof.bigler at colorado.edu (Christof Bigler)
Date: Wed, 21 Apr 2004 16:01:09 -0600
Subject: [R] Chi-square test with spatial data
Message-ID: <657008C8-93DF-11D8-A393-000A27D7D440@colorado.edu>

Hi

I analyze grid based, spatial data, and for each grid cell I have 
several
variables (fire severity, elevation, vegetation etc.). I would like to
know if there is e.g. lower fire severity above 3000 m a.s.l. as
compared to below 3000 m.
Since the data are highly correlated, I wondered if it's possible to
test the independence of two variables using chi-square tests. Is there
any R function that could be used to correct the p values of a 
chi-square test?

Thanks for any advice,
Christof



From xiao.gang.fan1 at libertysurf.fr  Thu Apr 22 00:06:03 2004
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Thu, 22 Apr 2004 00:06:03 +0200
Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
References: <n2cd80tc8a6ontje1ovo7hsnv5jsf2ecr5@4ax.com>	<Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>
	<1cmd80pvpmhlrkhll66lm6qmegr0c7h6or@4ax.com>
Message-ID: <4086F04B.2010708@libertysurf.fr>

With the R 1.9.0 version (Windows patched binary), I've got a strange
behaviour: the function winMenuAdd is no longer usable in the start up
file .Rprofile, the following error message was given:

Error in try(winMenuAdd("test")) : couldn't find function "winMenuAdd"

but the function works fine in interactive, once R loaded.

Any idea ?
Thanks
--
Fan

Duncan Murdoch wrote:
> On Wed, 21 Apr 2004 18:40:05 +0100 (BST), Prof Brian Ripley
> <ripley at stats.ox.ac.uk> wrote :
> 
> 
>>On Wed, 21 Apr 2004, Duncan Murdoch wrote:
>>
> 
> 
>>>I think it would be more useful for debugging purposes to do them one
>>>at a time.  Do we still get an error message about HOMEDRIVE if we do
>>>the above?  What if we set only HOMEPATH?
>>
>>Unfortunately you get the same message unless both are set correctly, so 
>>setting just one on the command line will tell you nothing.  I've changed 
>>the message for R-patched so we can tell them apart.
> 
> 
> I've just uploaded builds of r-patched and r-devel to CRAN that
> incorporate this change.  They should be visible tomorrow at 
> 
> <http://cran.r-project.org/bin/windows/base>
> 
> Follow the links near the top of the page; the ones in the main part
> are for the unpatched 1.9.0 release.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Brian.J.GREGOR at odot.state.or.us  Thu Apr 22 00:18:41 2004
From: Brian.J.GREGOR at odot.state.or.us (Brian.J.GREGOR@odot.state.or.us)
Date: Wed, 21 Apr 2004 15:18:41 -0700
Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
Message-ID: <372EFF9FE4E42E419C978E7A305DC5FE0379AD45@exsalem5.odot.state.or.us>

I had this problem too. The documentation for the winMenus says that these
functions are part of the utils package. R must be loading this library
after it sources in Rprofile. If you add library(utils) to the beginning of
your .First function, it should take care of the problem.

Brian Gregor, P.E.
Transportation Planning Analysis Unit
Oregon Department of Transportation
Brian.J.GREGOR at odot.state.or.us
(503) 986-4120
 

> -----Original Message-----
> From: Fan [mailto:xiao.gang.fan1 at libertysurf.fr]
> Sent: Wednesday, April 21, 2004 3:06 PM
> To: Duncan Murdoch
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
> 
> 
> With the R 1.9.0 version (Windows patched binary), I've got a strange
> behaviour: the function winMenuAdd is no longer usable in the start up
> file .Rprofile, the following error message was given:
> 
> Error in try(winMenuAdd("test")) : couldn't find function "winMenuAdd"
> 
> but the function works fine in interactive, once R loaded.
> 
> Any idea ?
> Thanks
> --
> Fan
> 
> Duncan Murdoch wrote:
> > On Wed, 21 Apr 2004 18:40:05 +0100 (BST), Prof Brian Ripley
> > <ripley at stats.ox.ac.uk> wrote :
> > 
> > 
> >>On Wed, 21 Apr 2004, Duncan Murdoch wrote:
> >>
> > 
> > 
> >>>I think it would be more useful for debugging purposes to 
> do them one
> >>>at a time.  Do we still get an error message about 
> HOMEDRIVE if we do
> >>>the above?  What if we set only HOMEPATH?
> >>
> >>Unfortunately you get the same message unless both are set 
> correctly, so 
> >>setting just one on the command line will tell you nothing. 
>  I've changed 
> >>the message for R-patched so we can tell them apart.
> > 
> > 
> > I've just uploaded builds of r-patched and r-devel to CRAN that
> > incorporate this change.  They should be visible tomorrow at 
> > 
> > <http://cran.r-project.org/bin/windows/base>
> > 
> > Follow the links near the top of the page; the ones in the main part
> > are for the unpatched 1.9.0 release.
> > 
> > Duncan Murdoch
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From xiao.gang.fan1 at libertysurf.fr  Thu Apr 22 00:34:03 2004
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Thu, 22 Apr 2004 00:34:03 +0200
Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
References: <372EFF9FE4E42E419C978E7A305DC5FE0379AD45@exsalem5.odot.state.or.us>
Message-ID: <4086F6DB.6070804@libertysurf.fr>

Brian, thanks for the hint, it works.

The old 1.8 version works very well without the need of such
"manual" loading, I guess, there'd be some internal changes
in the order of libraries loading at the startup
in the 1.9.0 version ?

--
Fan

Brian.J.GREGOR at odot.state.or.us wrote:
> I had this problem too. The documentation for the winMenus says that these
> functions are part of the utils package. R must be loading this library
> after it sources in Rprofile. If you add library(utils) to the beginning of
> your .First function, it should take care of the problem.
> 
> Brian Gregor, P.E.
> Transportation Planning Analysis Unit
> Oregon Department of Transportation
> Brian.J.GREGOR at odot.state.or.us
> (503) 986-4120
>  
> 
> 
>>-----Original Message-----
>>From: Fan [mailto:xiao.gang.fan1 at libertysurf.fr]
>>Sent: Wednesday, April 21, 2004 3:06 PM
>>To: Duncan Murdoch
>>Cc: r-help at stat.math.ethz.ch
>>Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
>>
>>
>>With the R 1.9.0 version (Windows patched binary), I've got a strange
>>behaviour: the function winMenuAdd is no longer usable in the start up
>>file .Rprofile, the following error message was given:
>>
>>Error in try(winMenuAdd("test")) : couldn't find function "winMenuAdd"
>>
>>but the function works fine in interactive, once R loaded.
>>
>>Any idea ?
>>Thanks
>>--
>>Fan
>>
>>Duncan Murdoch wrote:
>>
>>>On Wed, 21 Apr 2004 18:40:05 +0100 (BST), Prof Brian Ripley
>>><ripley at stats.ox.ac.uk> wrote :
>>>
>>>
>>>
>>>>On Wed, 21 Apr 2004, Duncan Murdoch wrote:
>>>>
>>>
>>>
>>>>>I think it would be more useful for debugging purposes to 
>>>>
>>do them one
>>
>>>>>at a time.  Do we still get an error message about 
>>>>
>>HOMEDRIVE if we do
>>
>>>>>the above?  What if we set only HOMEPATH?
>>>>
>>>>Unfortunately you get the same message unless both are set 
>>>
>>correctly, so 
>>
>>>>setting just one on the command line will tell you nothing. 
>>>
>> I've changed 
>>
>>>>the message for R-patched so we can tell them apart.
>>>
>>>
>>>I've just uploaded builds of r-patched and r-devel to CRAN that
>>>incorporate this change.  They should be visible tomorrow at 
>>>
>>><http://cran.r-project.org/bin/windows/base>
>>>
>>>Follow the links near the top of the page; the ones in the main part
>>>are for the unpatched 1.9.0 release.
>>>
>>>Duncan Murdoch
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>
>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From binabina at bellsouth.net  Thu Apr 22 00:34:43 2004
From: binabina at bellsouth.net (zubin)
Date: Wed, 21 Apr 2004 18:34:43 -0400
Subject: [R] multi-user engine
In-Reply-To: <85ad15xus0.fsf@servant.blindglobe.net>
Message-ID: <MBBBIIHJANJBMHLGMACKIEBFCGAA.binabina@bellsouth.net>

Thanks for the feedback, looks like RSERV is a viable solution?

http://stats.math.uni-augsburg.de/Rserve/p_index.shtml

any info on folks who have experience with Rserve?

thx again..

-----Original Message-----
From: A.J. Rossini [mailto:rossini at blindglobe.net]
Sent: Wednesday, April 21, 2004 10:37 AM
To: Barry Rowlingson
Cc: zubin; r-help at stat.math.ethz.ch
Subject: Re: [R] multi-user engine


Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> A.J. Rossini wrote:
>> "zubin" <binabina at bellsouth.net> writes:
>>
>>>I am presently a SAS user and wanted to configure R to work in a
multi-user
>>>enteprise environment.  Client - Server.  Where we have a strong LINUX
>>>server supporting about 10 statisticians with R.  Anyone have any
backround
>>>or information they can share to help me get jump-started on setting up R
in
>>> this environment?
>
>   Why can't you install R in the usual way on the server and give the
>   statisticians (there's only 10 of them, so its not much admin)
>   accounts on the server? They then login (using an SSH client) and
>   get graphics back via X-windows. Free SSH clients and X-windows
>   servers are available for all the usual platforms. Their filestore
>   would be on the server. I just don't see the advantage (and see
>   plenty of disadvantages) of complicating things with a client-server
>   framework where it might not be necessary.

Depends on the environment.  I can think of many good reasons for
wanting a client-server situation, where the solution you point out
would be a complete non-starter.  However, since client-server isn't
ready at this point without a great deal of work and pain, so it
(client-server) is pretty much a non-starter, and the solution you
point out is the only "get it done this month" solution that I know
of.

best,
-tony

--
rossini at u.washington.edu            http://www.analytics.washington.edu/
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From zelickr at pdx.edu  Thu Apr 22 01:49:33 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Wed, 21 Apr 2004 16:49:33 -0700 (PDT)
Subject: [R] formatted table to disk
In-Reply-To: <8765bte16c.fsf@lumen.indyrad.iupui.edu>
Message-ID: <Pine.GSO.4.44.0404211646580.28889-100000@gere.odin.pdx.edu>

Hello,

Thanks for the help, all who made suggestions.

Sink worked -- I don't know why I can't find these things.

I'll look at the xtable package as suggested also.

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From jasont at indigoindustrial.co.nz  Thu Apr 22 02:15:20 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 22 Apr 2004 12:15:20 +1200
Subject: [R] formatted table to disk
In-Reply-To: <Pine.GSO.4.44.0404211646580.28889-100000@gere.odin.pdx.edu>
References: <Pine.GSO.4.44.0404211646580.28889-100000@gere.odin.pdx.edu>
Message-ID: <40870E98.4000202@indigoindustrial.co.nz>

Randy Zelick wrote:

> Hello,
> 
> Thanks for the help, all who made suggestions.
> 
> Sink worked -- I don't know why I can't find these things.
> 
> I'll look at the xtable package as suggested also.

The Hmisc package also has a latex() function, for different take on the 
same thing.  Also, R2HTML is good if you want HTML output instead (but I 
just use LaTeX, then latex2html).

Cheers

Jason



From d.scott at auckland.ac.nz  Thu Apr 22 02:33:51 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 22 Apr 2004 12:33:51 +1200 (NZST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <003901c4277e$9b42def0$0213eda9@des.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0404221227450.28618-100000@stat71.stat.auckland.ac.nz>

On Wed, 21 Apr 2004, Brett Melbourne wrote:

> Hi Brian,
> 
> R1.9.0 fails with the message  "Fatal error: Invalid HOMEDRIVE" and no
> prompt to send an error report to Microsoft.
> 
> R1.8.1 fails with the message "R for Windows GUI front-end has encountered a
> problem and needs to close.  We are sorry for the inconvenience", included
> in a prompt to send an error report to Microsoft.
> 
> For both versions, R fails to start at all. I'm running Windows XP Pro, and
> like David Scott I'm supposed to install the XP critical updates as soon as
> they come out, which I did last week.
> 
> I suppose, given the other reports of this behaviour, the subject line
> should be "R fails after installing Windows XP updates".
> 
This has been the experience of some users here. The additional 
information I can offer is that the update which appears to cause the 
problem is

KB835732

There is at least one user here who had the problem with 1.8.1, but can 
now run it. He does have the critical update on. He may be working without 
network interaction which seems to be another piece of the puzzle from 
what I am reading. 

David Scott



_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From ajayshah at mayin.org  Wed Apr 21 18:12:51 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 21 Apr 2004 21:42:51 +0530
Subject: [R] Question on CAR appendix on NLS
Message-ID: <20040421161251.GA25189@igidr.ac.in>

The PDF file on the web, which is an appendix on nonlinear regression
associated with the CAR book, is very nice.

When I ran through the code presented there, I found something
odd. The code does a certain model in 3 ways: Vanilla NLS (using
numerical differentation), Analytical derivatives (where the user
supplies the derivatives) and analytical derivatives (using automatic
differentiation). The three results agree, except for the correlation
of parameter estimates :

          beta1   beta2
  beta2 -0.1662                      Numerical derivatives
  beta3  0.9145 -0.5407

  beta2 -0.7950                      Analytical derivatives
  beta3  0.9145 -0.9661

  beta2 -0.1662                      Automatic differentiation
  beta3  0.9145 -0.5407

Is this just a glitch of a small sample, or should I worry? My source
file (which should be the same as John Fox's file; I typed it in while
reading the PDF file, and made minor changes) is attached.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
-------------- next part --------------
# John Fox has a book "An R and S+ companion to applied regression"
# (abbreviated CAR).
#
# An appendix associated with this book, titled
#   "Nonlinear regression and NLS"
# is up on the web, and I strongly recommend that you go read it.
#
# All the data and code of this program is from there.

# First take some data - from the CAR book --
library(car)
data(US.pop)
attach(US.pop)
plot(year, population, type="l", col="blue")

# So you see, we have a time-series of the US population. We want to
# fit a nonlinear model to it.

library(nls)                            # The nonlinear regression library.
time = 0:20
pop.mod = nls(population ~ beta1/(1 + exp(beta2 + beta3*time)),
  start = list(beta1=350, beta2=4.5, beta3=-0.3), trace=T)
# Note that you just write in the formula that you want to fit,
# and supply starting values. "trace=T" makes him show iterations go by.

summary(pop.mod)
# Add in predicted values into the plot
lines(year, fitted.values(pop.mod), lwd=3, col="red")
z <- locator(1)

# Look at residuals
plot(year, residuals(pop.mod), type="b")
abline(h=0, lty=2)
z <- locator(1)

# Using analytical derivatives:
model = function(beta1, beta2, beta3, time) {
   m = beta1/(1+exp(beta2+beta3*time))
   term = exp(beta2 + beta3*time)
   gradient = cbind((1+term)^-2,
     -beta1*(1+term)^-2 * term,
     -beta1*(1+term)^-2 * term * time)
   attr(m, 'gradient') <- gradient
   m
 }

summary(nls(population ~ model(beta1, beta2, beta3, time),
            start=list(beta1=350, beta2=4.5, beta3=-0.3)))

# Using analytical derivatives, using automatic differentiation (!!!):
model = deriv(~ beta1/(1 + exp(beta2+beta3*time)), # rhs of model
  c('beta1', 'beta2', 'beta3'), # parameter names
  function(beta1, beta2, beta3, time){} # arguments for result
  )
summary(nls(population ~ model(beta1, beta2, beta3, time),
            start=list(beta1=350, beta2=4.5, beta3=-0.3)))

From u6001513 at tknet.tku.edu.tw  Thu Apr 22 07:04:47 2004
From: u6001513 at tknet.tku.edu.tw (=?big5?B?pP26zadn?=)
Date: Thu, 22 Apr 2004 13:04:47 +0800
Subject: [R] (no subject)
Message-ID: <000c01c42827$56126c20$6e01fea9@hlc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040422/48dfb562/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 22 08:19:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 07:19:47 +0100 (BST)
Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
In-Reply-To: <4086F6DB.6070804@libertysurf.fr>
Message-ID: <Pine.LNX.4.44.0404220718300.26762-100000@gannet.stats>

On Thu, 22 Apr 2004, Fan wrote:

> Brian, thanks for the hint, it works.
> 
> The old 1.8 version works very well without the need of such
> "manual" loading, I guess, there'd be some internal changes
> in the order of libraries loading at the startup
> in the 1.9.0 version ?

Yes, and that is right at the top of the NEWS file, including the fix.

BTW, there has been a `1.8 version' of R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahmlatif at yahoo.com  Thu Apr 22 10:23:50 2004
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Thu, 22 Apr 2004 01:23:50 -0700 (PDT)
Subject: [R] RODBC installation in debian
Message-ID: <20040422082350.81867.qmail@web41214.mail.yahoo.com>

Hello List,

I am trying to install RODBC package in a debian linux
box but getting the following message. Can anyone help
me to find what I am doing wrong here:

$ R CMD INSTALL RODBC_1.0-4.tar.gz
###
* Installing *source* package 'RODBC' ...
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler...
yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none
needed
checking for library containing SQLTables... no
configure: error: "no ODBC driver manager found"
ERROR: configuration failed for package 'RODBC'
** Removing '/usr/local/lib/R/site-library/RODBC'


R > version
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    9.0
year     2004
month    04
day      12
language R

Thanks,

Mahbub.



From phgrosjean at sciviews.org  Thu Apr 22 10:34:17 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 22 Apr 2004 10:34:17 +0200
Subject: [R] Lyapunov exponent?
Message-ID: <MABBLJDICACNFOLGIHJOAELBEFAA.phgrosjean@sciviews.org>

Hello,

Does anybody know if there is somewhere in R a function to calculate the
Lyapunov exponent in a time series?
Thanks,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From jasont at indigoindustrial.co.nz  Thu Apr 22 10:34:23 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 22 Apr 2004 20:34:23 +1200 (NZST)
Subject: [R] RODBC installation in debian
In-Reply-To: <20040422082350.81867.qmail@web41214.mail.yahoo.com>
References: <20040422082350.81867.qmail@web41214.mail.yahoo.com>
Message-ID: <11457.203.9.176.60.1082622863.squirrel@webmail.maxnet.co.nz>

> I am trying to install RODBC package in a debian linux
> box but getting the following message. Can anyone help
> me to find what I am doing wrong here:
>

This sort of problem usually stems from issues covered in the README file
in the RODBC tarball.  Have you untarred it and read this file?

Cheers

Jason



From Camarda at demogr.mpg.de  Thu Apr 22 11:37:44 2004
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Thu, 22 Apr 2004 11:37:44 +0200
Subject: [R] histbackback for population pyramid
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8106B40EDC@hermes.demogr.mpg.de>

Hi R-people,
I hope someone could help me. I have to draw a population pyramid, but using
histbackback (in Hmisc library) function I've got just histograms of density
or frequency of my distribution. How can I get a population pyramid without
playing on barplot function? I give a simple example that, solved, would
help me.

# dataset
x1 <- c(100, 80, 60, 40, 20, 10)
x2 <- c(95, 85, 65, 45, 15, 5)
x3 <- -x2

# a too simple pyramid with barplots (I could use other options for possible
improvements)
par(mfrow=c(1,2))
barplot(x3, horiz=TRUE)
barplot(x1, horiz=TRUE)

# what I could get from histbackback function
library(Hmisc)
par(mfrow=c(1,1))
histbackback(x1, x2, xlab=c("males", "females"))

Thanks in advance, Giancarlo




+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From JonesW at kssg.com  Thu Apr 22 11:33:16 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Thu, 22 Apr 2004 10:33:16 +0100
Subject: [R] lme correlation structure error
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02955E9E@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040422/76d85a7c/attachment.pl

From angel_lul at hotmail.com  Thu Apr 22 12:53:20 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Thu, 22 Apr 2004 11:53:20 +0100
Subject: [R] Lyapunov exponent?
In-Reply-To: <MABBLJDICACNFOLGIHJOAELBEFAA.phgrosjean@sciviews.org>
References: <MABBLJDICACNFOLGIHJOAELBEFAA.phgrosjean@sciviews.org>
Message-ID: <4087A420.7010405@hotmail.com>

Philippe,
I think, it was discussed last month on the list: 
http://tolstoy.newcastle.edu.au/R/help/04/03/0318.html

In summary:


From: Stephen Ellner (spe2 at cornell.edu)
  Date: Sat 06 Mar 2004 - 01:57:09 EST

  The lyapunov exponent part of Funfits is semi-available,
  but only for Windows. Doug Nychka and I, who wrote that part
  of Funfits, started trying to make it a CRANworthy package but
  never got it done. I've just linked it to my web page
  (www.eeb.cornell.edu/Ellner); follow the Software link on
  that page and get LENNS.zip.



Steve

Angel Lopez wrote:
  >"Frozen" package Funfits was able to. Although its succesor package is
  >Fields, it has not lyapunov exponent calculations.
  >You can get funfits from :
  >http://www.cgd.ucar.edu/stats/Software/Funfits/index.shtml
  >I haven't got the statistical or programming knowledge to revise that
  >code, if you are able to I would be keen to know of any updates.
  >Cheers,
  >
Best,
Angel

Philippe Grosjean wrote:
> Hello,
> 
> Does anybody know if there is somewhere in R a function to calculate the
> Lyapunov exponent in a time series?
> Thanks,
> 
> Philippe Grosjean
> 
> .......................................................<??}))><....
>  ) ) ) ) )
> ( ( ( ( (   Prof. Philippe Grosjean
> \  ___   )
>  \/ECO\ (   Numerical Ecology of Aquatic Systems
>  /\___/  )  Mons-Hainaut University, Pentagone
> / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
>  /NUM\/  )
>  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
>        \ )  email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
> ( ( ( ( (
> ...................................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> .
>



From fzoellne at TechFak.Uni-Bielefeld.DE  Thu Apr 22 12:50:44 2004
From: fzoellne at TechFak.Uni-Bielefeld.DE (Frank Gerrit Zoellner)
Date: Thu, 22 Apr 2004 12:50:44 +0200
Subject: [R] SVM question
In-Reply-To: <20040322131406.347ff4ee.david.meyer@wu-wien.ac.at>
References: <200403191134.i2JB5oTH029495@hypatia.math.ethz.ch>
	<20040322131406.347ff4ee.david.meyer@wu-wien.ac.at>
Message-ID: <20040422105044.GA3800@hindemith.TechFak.Uni-Bielefeld.DE>

Hi!

I have another SVM question.
I run and train a C-Classification using the SVM, works fine.
Now I want to receive the output (the classified examples):

pred <- fitted(model)

where pred is then a list of the classification result of each input elememt i supposed. But if I compare the number of entries in pred with the original numer of input values there is a difference. How can this happen and how can I get back all the classified examples?

Yours,
Frank 

-- 
Frank G. Zoellner
AG Angewandte Informatik
Technische Fakult"at
Universit"at Bielefeld
phone: +49(0)521-106-2951
fax:   +49(0)521-106-2992
email: fzoellne at techfak.uni-bielefeld.de



From jfox at mcmaster.ca  Thu Apr 22 13:00:57 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 22 Apr 2004 07:00:57 -0400
Subject: [R] Question on CAR appendix on NLS
In-Reply-To: <20040421161251.GA25189@igidr.ac.in>
Message-ID: <20040422110058.MGNC14198.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Alay,

I'm leaving town this morning for several days, so I don't have time to
check through your code, but I did rerun the examples from the appendix (see
below), and all three approaches produce identical parameter correlations
(using R 1.9.0 under Win XP). Perhaps you made an error in entering a
derivative.

I hope this helps,
 John

> library(car)
> data(US.pop)
> attach(US.pop)
> time <- 0:20
> pop.mod <- nls(population ~ beta1/(1 + exp(beta2 + beta3*time)), 
+     start=list(beta1=350, beta2=4.5, beta3=-0.3),
+     trace=T)
13007.48 :  350.0   4.5  -0.3 
609.5727 :  351.8074862   3.8405002  -0.2270578 
365.4396 :  383.7045367   3.9911148  -0.2276690 
356.4056 :  389.1350277   3.9897242  -0.2265769 
356.4001 :  389.1462893   3.9903758  -0.2266276 
356.4001 :  389.1665304   3.9903412  -0.2266193 
356.4001 :  389.1655126   3.9903457  -0.2266199 
> summary(pop.mod)

Formula: population ~ beta1/(1 + exp(beta2 + beta3 * time))

Parameters:
       Estimate Std. Error t value Pr(>|t|)    
beta1 389.16551   30.81197   12.63 2.20e-10 ***
beta2   3.99035    0.07032   56.74  < 2e-16 ***
beta3  -0.22662    0.01086  -20.87 4.60e-14 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 4.45 on 18 degrees of freedom

Correlation of Parameter Estimates:
        beta1   beta2
beta2 -0.1662        
beta3  0.9145 -0.5407

> 
> 
>  model <- function(beta1, beta2, beta3, time){
+     model <- beta1/(1 + exp(beta2 + beta3*time))
+     term <- exp(beta2 + beta3*time)
+     gradient <- cbind((1 + term)^-1, # in proper order
+         -beta1*(1 + term)^-2 * term,
+         -beta1*(1 + term)^-2 * term * time)
+     attr(model, "gradient") <- gradient
+     model
+     }
>     
>  summary(nls(population ~ model(beta1, beta2, beta3, time),
+     start=list(beta1=350, beta2=4.5, beta3=-0.3)))

Formula: population ~ model(beta1, beta2, beta3, time)

Parameters:
       Estimate Std. Error t value Pr(>|t|)    
beta1 389.16551   30.81196   12.63 2.20e-10 ***
beta2   3.99035    0.07032   56.74  < 2e-16 ***
beta3  -0.22662    0.01086  -20.87 4.60e-14 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 4.45 on 18 degrees of freedom

Correlation of Parameter Estimates:
        beta1   beta2
beta2 -0.1662        
beta3  0.9145 -0.5407

> 
>  model <- deriv(~ beta1/(1 + exp(beta2 + beta3*time)), # rhs of model
+     c("beta1", "beta2", "beta3"), # parameter names
+     function(beta1, beta2, beta3, time){} # arguments for result
+     )
>  summary(nls(population ~ model(beta1, beta2, beta3, time),
+  start=list(beta1=350, beta2=4.5, beta3=-0.3)))

Formula: population ~ model(beta1, beta2, beta3, time)

Parameters:
       Estimate Std. Error t value Pr(>|t|)    
beta1 389.16551   30.81196   12.63 2.20e-10 ***
beta2   3.99035    0.07032   56.74  < 2e-16 ***
beta3  -0.22662    0.01086  -20.87 4.60e-14 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 4.45 on 18 degrees of freedom

Correlation of Parameter Estimates:
        beta1   beta2
beta2 -0.1662        
beta3  0.9145 -0.5407

> 
>  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ajay Shah
> Sent: Wednesday, April 21, 2004 11:13 AM
> To: r-help
> Subject: [R] Question on CAR appendix on NLS
> 
> The PDF file on the web, which is an appendix on nonlinear 
> regression associated with the CAR book, is very nice.
> 
> When I ran through the code presented there, I found 
> something odd. The code does a certain model in 3 ways: 
> Vanilla NLS (using numerical differentation), Analytical 
> derivatives (where the user supplies the derivatives) and 
> analytical derivatives (using automatic differentiation). The 
> three results agree, except for the correlation of parameter 
> estimates :
> 
>           beta1   beta2
>   beta2 -0.1662                      Numerical derivatives
>   beta3  0.9145 -0.5407
> 
>   beta2 -0.7950                      Analytical derivatives
>   beta3  0.9145 -0.9661
> 
>   beta2 -0.1662                      Automatic differentiation
>   beta3  0.9145 -0.5407
> 
> Is this just a glitch of a small sample, or should I worry? 
> My source file (which should be the same as John Fox's file; 
> I typed it in while reading the PDF file, and made minor 
> changes) is attached.
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>



From Luisr at frs.fo  Thu Apr 22 14:00:04 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Thu, 22 Apr 2004 13:00:04 +0100
Subject: [R] fill up a matrix
Message-ID: <s087c1e8.097@ffdata.setur.fo>

Hi all,

I want to fill in this matrix vectors (by column) without overwriting the first elements in column 1995.

Is there any other way than concatenate the first element with the vector and then assign this new vector 
to the column in matrix?   

 matrix[,"3"]<-c(1591,"vector")
 matrix[,"4"]<-c(405,"vector")
...
...

                               matrix
             2     3     4      5     6   7    8    9   10
1995  278 1591 405 482 285 99 220 48  4
1996 1220   NA  NA  NA  NA NA  NA NA NA
1997 3106   NA  NA  NA  NA NA  NA NA NA
1998 1895   NA  NA  NA  NA NA  NA NA NA
1999 1376   NA  NA  NA  NA NA  NA NA NA
2000  565   NA  NA  NA  NA NA  NA NA NA
2001  491   NA  NA  NA  NA NA  NA NA NA
2002 1169   NA  NA  NA  NA NA  NA NA NA
2003 2310   NA  NA  NA  NA NA  NA NA NA


Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From jeaneid at chass.utoronto.ca  Thu Apr 22 14:29:40 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 22 Apr 2004 08:29:40 -0400
Subject: [R] RODBC installation in debian
In-Reply-To: <20040422082350.81867.qmail@web41214.mail.yahoo.com>
Message-ID: <Pine.SGI.4.40.0404220822120.11571122-100000@origin.chass.utoronto.ca>

I just did this yesterday. you do not have an ODBC driver ( as it says in
the log). you need to (as root) use apt-get install unixodbc and apt-get
install unixodbc-dev.
The unixodbc package alone will not work, you need to install unixodbc-dev
(development),

Hope this helps.

I have a question for you since you are running debian. does control-c
cotrol-c kill the data editor when it is envoked. Please let me know, it
is so frustrating to kill it by mouse everytime I need to look at a
dataframe. What debian are you using, I am using debian testing.
pleae run this experiment for me ans let me knwo what happens.

edit(as.data.frame(matrix(0, ncol=7, nrow=10)))
 and then try to kill it by issuing control-c control-c . In mine you have
to finish the job by clicking on the kill symbol in the data editor
itself.

Thanks,

On Thu, 22 Apr 2004, Mahbub Latif wrote:

> Hello List,
>
> I am trying to install RODBC package in a debian linux
> box but getting the following message. Can anyone help
> me to find what I am doing wrong here:
>
> $ R CMD INSTALL RODBC_1.0-4.tar.gz
> ###
> * Installing *source* package 'RODBC' ...
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler...
> yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none
> needed
> checking for library containing SQLTables... no
> configure: error: "no ODBC driver manager found"
> ERROR: configuration failed for package 'RODBC'
> ** Removing '/usr/local/lib/R/site-library/RODBC'
>
>
> R > version
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
>
> Thanks,
>
> Mahbub.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Apr 22 14:21:10 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2004 14:21:10 +0200
Subject: [R] fill up a matrix
In-Reply-To: <s087c1e8.097@ffdata.setur.fo>
References: <s087c1e8.097@ffdata.setur.fo>
Message-ID: <x2fzaw9pah.fsf@biostat.ku.dk>

"Luis Rideau Cruz" <Luisr at frs.fo> writes:

> Hi all,
> 
> I want to fill in this matrix vectors (by column) without overwriting the first elements in column 1995.
> 
> Is there any other way than concatenate the first element with the vector and then assign this new vector 
> to the column in matrix?   
> 
>  matrix[,"3"]<-c(1591,"vector")
>  matrix[,"4"]<-c(405,"vector")
> ...
> ...
> 
>                                matrix
>              2     3     4      5     6   7    8    9   10
> 1995  278 1591 405 482 285 99 220 48  4
> 1996 1220   NA  NA  NA  NA NA  NA NA NA
> 1997 3106   NA  NA  NA  NA NA  NA NA NA
> 1998 1895   NA  NA  NA  NA NA  NA NA NA
> 1999 1376   NA  NA  NA  NA NA  NA NA NA
> 2000  565   NA  NA  NA  NA NA  NA NA NA
> 2001  491   NA  NA  NA  NA NA  NA NA NA
> 2002 1169   NA  NA  NA  NA NA  NA NA NA
> 2003 2310   NA  NA  NA  NA NA  NA NA NA


There's a bit of formatting damage there, is this a matrix with
rownames 1995:2003 and colnames 2:10?

Have a look at this:

 m <- matrix(NA,9,9)
 m[,1] <- round(1e4*runif(9))
 m[1,] <- round(1e4*runif(9))
 colnames(m) <- 2:10
 rownames(m) <- 1995:2003
 m # should look like yours now
 m[-1,-1] <- 1:64
 m

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jeaneid at chass.utoronto.ca  Thu Apr 22 14:45:43 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 22 Apr 2004 08:45:43 -0400
Subject: [R] fill up a matrix
In-Reply-To: <s087c1e8.097@ffdata.setur.fo>
Message-ID: <Pine.SGI.4.40.0404220844260.11571122-100000@origin.chass.utoronto.ca>


you can do this,
tmpmatrix <- matrix(unlist((lapply(as.data.frame(x[, 3:ncol(x)]),
function(t)
return(rep(t[!is.na(t)], 9))) )), byrow=F, ncol=8)

matrix <- cbind(matrix[, 1:2], tmpmatrix)

On Thu, 22 Apr 2004, Luis Rideau Cruz wrote:

> Hi all,
>
> I want to fill in this matrix vectors (by column) without overwriting the first elements in column 1995.
>
> Is there any other way than concatenate the first element with the vector and then assign this new vector
> to the column in matrix?
>
>  matrix[,"3"]<-c(1591,"vector")
>  matrix[,"4"]<-c(405,"vector")
> ...
> ...
>
>                                matrix
>              2     3     4      5     6   7    8    9   10
> 1995  278 1591 405 482 285 99 220 48  4
> 1996 1220   NA  NA  NA  NA NA  NA NA NA
> 1997 3106   NA  NA  NA  NA NA  NA NA NA
> 1998 1895   NA  NA  NA  NA NA  NA NA NA
> 1999 1376   NA  NA  NA  NA NA  NA NA NA
> 2000  565   NA  NA  NA  NA NA  NA NA NA
> 2001  491   NA  NA  NA  NA NA  NA NA NA
> 2002 1169   NA  NA  NA  NA NA  NA NA NA
> 2003 2310   NA  NA  NA  NA NA  NA NA NA
>
>
> Luis Ridao Cruz
> Fiskirannsknarstovan
> Natn 1
> P.O. Box 3051
> FR-110 Trshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Thu Apr 22 14:45:05 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 22 Apr 2004 12:45:05 +0000 (UTC)
Subject: [R] fill up a matrix
References: <s087c1e8.097@ffdata.setur.fo>
Message-ID: <loom.20040422T144329-372@post.gmane.org>


Try this:

m <- matrix(1:9, 3)
m[-1,] <- matrix(11:16,2)


Luis Rideau Cruz <Luisr <at> frs.fo> writes:

: 
: Hi all,
: 
: I want to fill in this matrix vectors (by column) without overwriting the 
first elements in column 1995.
: 
: Is there any other way than concatenate the first element with the vector 
and then assign this new vector 
: to the column in matrix?   
: 
:  matrix[,"3"]<-c(1591,"vector")
:  matrix[,"4"]<-c(405,"vector")
: ...
: ...
: 
:                                matrix
:              2     3     4      5     6   7    8    9   10
: 1995  278 1591 405 482 285 99 220 48  4
: 1996 1220   NA  NA  NA  NA NA  NA NA NA
: 1997 3106   NA  NA  NA  NA NA  NA NA NA
: 1998 1895   NA  NA  NA  NA NA  NA NA NA
: 1999 1376   NA  NA  NA  NA NA  NA NA NA
: 2000  565   NA  NA  NA  NA NA  NA NA NA
: 2001  491   NA  NA  NA  NA NA  NA NA NA
: 2002 1169   NA  NA  NA  NA NA  NA NA NA
: 2003 2310   NA  NA  NA  NA NA  NA NA NA
: 
: 
: Luis Ridao Cruz
: Fiskirannsknarstovan
: Natn 1
: P.O. Box 3051
: FR-110 Trshavn
: Faroe Islands
: Phone:             +298 353900
: Phone(direct): +298 353912
: Mobile:             +298 580800
: Fax:                 +298 353901
: E-mail:              luisr <at> frs.fo
: Web:                www.frs.fo
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From bates at stat.wisc.edu  Thu Apr 22 14:45:32 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 22 Apr 2004 07:45:32 -0500
Subject: [R] lme correlation structure error
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02955E9E@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02955E9E@gimli.middleearth.kssg.com>
Message-ID: <6rfzaw9o5v.fsf@bates4.stat.wisc.edu>

Wayne Jones <JonesW at kssg.com> writes:

> I am trying to follow an example of modelling a serial correlation structure
> in the textbook "Mixed Effects Model in S and Splus". 
> However, I am getting some very odd results. Here is what I am trying to
> run: 
> 
> 
> library(nlme)
> data(Ovary)
> fm1<-lme(follicles~sin(2*pi*Time)+cos(2*pi*Time),data=Ovary,random=pdDiag(~s
> in(2*pi*Time)))
> 
> ### The example is fine up to here with all parameter estimates being
> identical to that in the book. 
> 
> 
> fm2<-update(fm1,correlation=corAR1())
> 
> #### The parameters of fm2 are different to that in the book and 
> 
> plot(ACF(fm2))
> 
> ##### signifies that serial correlation still exists in the residuals.
> 
> 
> 
> 
> ###### When I try and run this (which runs fine in the book)
> 
> fm5<-update(fm1,corr=corARMA(p=1,q=1))
> 
> #### I get the following error message 
> 
> #Error in "coef<-.corARMA"(*tmp*, value = c(62.3428455941166,
> 62.3428517930051 : 
> #       Coefficient matrix not invertible
> 
> 
> I have tried running the example on R for windows versions 1.7.1 and 1.8.1
> with the same results. 
> 
> Can anyone shed light on this?? Perhaps another R-user could kindly run this
> example and see if they get the same results??

I believe it is the optimizer being used by lme that causes the
problem.  In the initial iterations the nlm function will sometimes
take very large steps in the parameter space and end up stuck in
places where the likelihood is very flat.  Use 

control = list(msVerbose = TRUE)

in the call to lme to see where the parameter vector is being
directed.

The examples in our book were done in S-PLUS (version 3.4, I think)
which uses different optimizer code, and that optimizer code does not
appear to be freely available.  (It's from the PORT library from Bell
Labs.  It can be downloaded but you have to click through an agreement
panel and give your name, email address, etc.)



From ferraria at ensisun.imag.fr  Thu Apr 22 15:14:58 2004
From: ferraria at ensisun.imag.fr (anthony.ferrari@ensimag.imag.fr)
Date: Thu, 22 Apr 2004 15:14:58 +0200 (MEST)
Subject: [R] calling R from java
In-Reply-To: <85u0zddzzz.fsf@servant.blindglobe.net>
Message-ID: <Pine.GSO.4.40.0404221509540.9708-100000@ensisun>


I downloaded it and installed it with all the recommended options. I
created the LD_LIBRARY_PATH
environment variable but my java compiler (latest version) can't find
packages like org.omegahat.R.java and cannot resolve symbol (class) like
REvaluator, RForeignReference...  any idea ?

many thanks for answering my last question I promise !
best regards,
Anthony


On Wed, 21 Apr 2004, A.J. Rossini wrote:

>
> Look at the SJava package on Omegahat.org
>
> Good luck.
>
> best,
> -tony
>
>
> "anthony.ferrari at ensimag.imag.fr" <ferraria at ensisun.imag.fr> writes:
>
> > Hello,
> >
> > I need to call R from a java(swing) application. I manage to do it with
> > something like :
> >
> > Process p = Runtime.getRuntime().exec("R --slave")
> > OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
> > o.wrote("...")
> > ... etc
> >
> > but at the end no .Rdata file has been created and there are some data I
> > don't want to reload each time (for time execution reasons).
> > So,
> > Is it possible when calling R from java to create a .Rdata file to save
> > data and as a consequence when recalling in the same directory not to have
> > to reload those data ?
> >  (Maybe the option --slave need to be changed ? )
> >
> >
> > many thanks for helping me
> >
> > best regards,
> > af
> > anthony.ferrari at ensimag.imag.fr
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
> rossini at u.washington.edu            http://www.analytics.washington.edu/
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>
> CONFIDENTIALITY NOTICE: This e-mail message and any attachments may be
> confidential and privileged. If you received this message in error,
> please destroy it and notify the sender. Thank you.
>



From B.SARRANT at locindus.fr  Thu Apr 22 15:25:32 2004
From: B.SARRANT at locindus.fr (SARRANT Bruno)
Date: Thu, 22 Apr 2004 15:25:32 +0200
Subject: [R] calling R from java[Scanned]
Message-ID: <64D4E16DEFDC514287CDD31577FF0E1510B437@SERVEUR.locindus.fr>

look at http://www.omegahat.org/download/index.html

for my part, i'm working on Window 2000 and got lots of problem in SJava building due to C code compilation...

Cheers

	Bruno

-----Message d'origine-----
De : anthony.ferrari at ensimag.imag.fr [mailto:ferraria at ensisun.imag.fr]
Envoy?? : jeudi 22 avril 2004 15:15
?? : rossini at u.washington.edu
Cc : r-help at stat.math.ethz.ch; anthony.ferrari at ensimag.imag.fr
Objet : Re: [R] calling R from java[Scanned]



I downloaded it and installed it with all the recommended options. I
created the LD_LIBRARY_PATH
environment variable but my java compiler (latest version) can't find
packages like org.omegahat.R.java and cannot resolve symbol (class) like
REvaluator, RForeignReference...  any idea ?

many thanks for answering my last question I promise !
best regards,
Anthony


On Wed, 21 Apr 2004, A.J. Rossini wrote:

>
> Look at the SJava package on Omegahat.org
>
> Good luck.
>
> best,
> -tony
>
>
> "anthony.ferrari at ensimag.imag.fr" <ferraria at ensisun.imag.fr> writes:
>
> > Hello,
> >
> > I need to call R from a java(swing) application. I manage to do it with
> > something like :
> >
> > Process p = Runtime.getRuntime().exec("R --slave")
> > OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
> > o.wrote("...")
> > ... etc
> >
> > but at the end no .Rdata file has been created and there are some data I
> > don't want to reload each time (for time execution reasons).
> > So,
> > Is it possible when calling R from java to create a .Rdata file to save
> > data and as a consequence when recalling in the same directory not to have
> > to reload those data ?
> >  (Maybe the option --slave need to be changed ? )
> >
> >
> > many thanks for helping me
> >
> > best regards,
> > af
> > anthony.ferrari at ensimag.imag.fr
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
> rossini at u.washington.edu            http://www.analytics.washington.edu/
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>
> CONFIDENTIALITY NOTICE: This e-mail message and any attachments may be
> confidential and privileged. If you received this message in error,
> please destroy it and notify the sender. Thank you.
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito.muggeo at giustizia.it  Thu Apr 22 15:27:27 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 22 Apr 2004 15:27:27 +0200
Subject: [R] random effects structure in lme
References: <6B5A9304046AD411BD0200508BDFB6CB02955E9E@gimli.middleearth.kssg.com>
	<6rfzaw9o5v.fsf@bates4.stat.wisc.edu>
Message-ID: <014401c4286d$91a9ff00$5c13070a@PROCGEN>

Dear all,
Some days ago I posted the same message below, but unfortunately with no
reply.

So, apologizes for this my re-sending, but I hope there is now someone
on-line that can help me....


In using the lme() function from the nlme package, I would like to specify a
particular correlation structure for the random effects.
For instance, for a 3 by 3 matrix, say, I am interested in assuming a
`quasi-diagonal' matrix having zero entries in all but the first column (and
the main
diagonal, of course)

Hence, given the three random effects b1 b2 and b3, I want to assume
cov(b2,b3)=0; given four random effects b1,b2,b3,b4 it should be
cov(b2,b3)=cov(b4,b2)=cov(b4,b3)=0.

For a simple diagonal matrix I specify `random=list(id=pdDiag(~1+x1+x2))',
(id is the grouping variable), but I am not able to go on....

Please, could anyone give me any advice?

many thanks,
vito



From phgrosjean at sciviews.org  Thu Apr 22 15:31:53 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 22 Apr 2004 15:31:53 +0200
Subject: [R] New version of benchmark comparing R with other software
Message-ID: <MABBLJDICACNFOLGIHJOOELFEFAA.phgrosjean@sciviews.org>

Hello,

Thanks to Douglas Bates, there is now a new benchmark suite (version 2.3)
which is compatible with R 1.9.0 and the recent Matrix library (0.8-1 or
above). You find it at http://www.sciviews.org/other/benchmark.htm. It
compares R 1.9.0 under Windows with:
S-PLUS 6.5, Matlab 6.0, O-Matrix 5.6, Octave 2.1.42, Scilab 2.7 and Ox 3.30.

In short, R in its version 1.9.0 and with the new Matrix library, is now one
of the fastest matrix calculation package among those tested on the computer
and system used (P IV, 1 Gb RAM and Windows XP pro).

Keeping its limitations in mind (only a few functions tested, and relatively
artificial situations that may or may not compare with real-world cases),
this benchmark suite could be useful for deciding which software to chose
for computing intensive matrix calculations, and also to compare the same
software (R) on different platforms, with one or several processors, and/or
with various optimized BLAS libraries.

Best,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From sdavis2 at mail.nih.gov  Thu Apr 22 15:37:16 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 22 Apr 2004 09:37:16 -0400
Subject: [R] <no subject>
Message-ID: <BCAD42CC.757C%sdavis2@mail.nih.gov>

Dear all,

I want to compute pairwise correlations between all rows of two matrices
where the pairs are formed by taking all rows from each matrix with a common
factor.  In other words, I want to take all pairwise correlations between
rows after stratifying by a factor (the same factor) in each matrix.  More
concretely, I have factor for matrix 1 like:

A
A
B
B
C
E
G

And for matrix 2,

A
B
C
D
E
F
G

And two matrices with the same number of columns.  I want to get
correlations between row 1 of matrix 1 and row 1 of matrix 2, row 2 of
matrix 2 and row 1 of matrix 2, etc.

I just can't seem to get the correct combination of "apply" functions to do
this.  Any ideas?

Thanks,
Sean



From xt_wang at cs.concordia.ca  Thu Apr 22 15:37:22 2004
From: xt_wang at cs.concordia.ca (xt_wang@cs.concordia.ca)
Date: Thu, 22 Apr 2004 09:37:22 -0400
Subject: [R] Urgent:again question about nmath/standalone
Message-ID: <1082641042.4087ca9279d2a@mailhost.cs.concordia.ca>

Hello, all,

I have the same question as the last mail I sent. I have installed libRmath in 
my system. But I still can not link Rmath through -lRmath. The whole process 
is as follow.Is there anybody who is so kind to help me find out the problem? 
I will appreciate very much.

[credsim at confsys ~]$ cd /usr/lib
[credsim at confsys lib]$ ls -l libR*
-rw-------    1 bcdesai  bcdesai    237828 Apr 15 18:10 libRmath.a
lrwxrwxrwx    1 root     root           17 Apr 15 18:12 libRmath.so -> 
libRmath.so.1.0.0
lrwxrwxrwx    1 root     root           17 Apr 15 18:12 libRmath.so.1 -> 
libRmath.so.1.0.0
-rw-------    1 bcdesai  bcdesai    114084 Apr 15 18:10 libRmath.so.1.0.0
[credsim at confsys lib]$ locate test1.c
/b2/home/credsim/src/test1.c
/b2/downloads/a-mp-ms-my-o-p-u/mysql-3.23.55/heap/hp_test1.c
/b2/downloads/a-mp-ms-my-o-p-u/mysql-3.23.55/myisam/ft_test1.c
/b2/downloads/a-mp-ms-my-o-p-u/mysql-3.23.55/myisam/mi_test1.c
/b2/home-apr-18-04-1145Am/credsim/src/test1.c
[credsim at confsys lib]$ cd  /b2/home/credsim/src
[credsim at confsys ~/src]$ gcc -Wall -o test1 test1.c -lRmath -lm
test1.c:12: warning: return type defaults to `int'
test1.c: In function `main':
test1.c:21: warning: implicit declaration of function `Allocate_Memory_2D'
test1.c:21: warning: assignment makes pointer from integer without a cast
test1.c:22: warning: assignment makes pointer from integer without a cast
test1.c:37: warning: too many arguments for format
test1.c:44: warning: implicit declaration of function `solve'
test1.c:44: warning: assignment makes pointer from integer without a cast
test1.c:54: warning: control reaches end of non-void function
test1.c: At top level:
test1.c:58: warning: type mismatch with previous implicit declaration
test1.c:21: warning: previous implicit declaration of `Allocate_Memory_2D'
test1.c:58: warning: `Allocate_Memory_2D' was previously implicitly declared 
to return `int'
/usr/bin/ld: cannot find -lRmath
collect2: ld returned 1 exit status
[credsim at confsys ~/src]$ gcc test1.c -o test1 -lRmath
test1.c: In function `main':
test1.c:21: warning: assignment makes pointer from integer without a cast
test1.c:22: warning: assignment makes pointer from integer without a cast
test1.c:44: warning: assignment makes pointer from integer without a cast
test1.c: At top level:
test1.c:58: warning: type mismatch with previous implicit declaration
test1.c:21: warning: previous implicit declaration of `Allocate_Memory_2D'
test1.c:58: warning: `Allocate_Memory_2D' was previously implicitly declared 
to return `int'
/usr/bin/ld: cannot find -lRmath
collect2: ld returned 1 exit status

"test1.c"
#define MATHLIB_STANDALONE
#include <math.h>
#include <stdlib.h>
#include <stdio.h>
#include </usr/local/lib/R/include/Rmath.h>

 main()
{

  double **x, **y, **x1, **y1, valin;
  int i,j,I,J;
  

  I=3;
  J=3;

  x=Allocate_Memory_2D( I, J, x1);
  y=Allocate_Memory_2D( I, J, y1);


  FILE *in_file; 


    /* input x value from file data_2Dx.txt */ 
    in_file=fopen("data_2Dx.txt","r");
        if (in_file==NULL)
        {/*Test for error*/
                fprintf(stderr,"Error:Unable to input file 
from 'data_2Dx.txt'\n");
                exit(8);
        }
        for( i=0;i<I; i++)
          for (j=0;j<J;j++)
          { fscanf(in_file, "%lf\n", &valin, stdin);/* read a single double 
value in */
               x[i][j]=valin;
                   valin=0.0;
          }
        fclose(in_file);


   y=solve(x);

   for (i=0;i<I;i++)
     for (j=0;j<J;j++)
     {
       printf ("y[%d][%d]=%lf\n", i, j, y[i][j]);
     }

}


double **Allocate_Memory_2D( int I, int J, double **W)
{ 
        int i;

        W=(double **)malloc(I*sizeof (double *));
    if(!W)
      printf("It is out of memory. Allocation failed.");
        for (i=0;i<I;i++)
        {
        W[i]=(double *)malloc(J*sizeof(double));
        if(!W[i])
          printf("It is out of memory. Allocation failed.");
        }
        return (W);
}

Thanks a lot!

Maggie Wang



From djw1005 at cam.ac.uk  Thu Apr 22 15:43:12 2004
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Thu, 22 Apr 2004 14:43:12 +0100 (BST)
Subject: [R] Trouble with HTML search engine
Message-ID: <Pine.SOL.3.96.1040422140058.15487A-100000@draco.cus.cam.ac.uk>


There have been a number of posts to this list by people having trouble
with the HTML search engine. Often these troubles are caused by incorrect
setups (user hasn't installed Java properly, or Java is disabled, or
Javascript is disabled). Sometimes the trouble persists even when Java is
installed properly.

I have written an alternative HTML search engine, which is based on
Javascript rather than Java. (Hopefully, this means that there is less
that can go wrong.) I haven't wrapped it up in a package yet, because I
don't know how well it works or even if there is any interest. If you
would like to try it, you can:

source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")
helpHTML()

(Or you can download the code and run it locally. You will also need to
download searchtemplate.html from the same location. You can then run
helpHTML(searchTemplate=localfilename) to tell it to use your local copy
of searchtemplate.html.)

ISSUES WITH MY CODE
-------------------
I've tested it on Windows XP cross {R1.8.0,R1.9.0} cross {IE,Firebird}.
I've also tested it on Debian 3.0 with R1.8.0 cross {Mozilla,Firebird}.
I'd be grateful to learn whether it works elsewhere.

Searching is a bit slow. On my newish computer it takes three seconds or
so. On an older departmental machine it takes ten seconds. Such is
(Javascript) life.

My searching algorithm is not the same as the current searching algorithm.
I wrote this for my own use, and so I've used a scoring mechanism which
reflects the way I like to search. The text at the top of the search page
explains some of the options.

There are obviously things I don't understand about the current help
setup. If anyone is sufficiently interested in this to explain them to me,
I would be grateful. (1) On my Windows XP setup, R writes an index in the
directory it was installed. What if it doesn't have write permission?  (2)
On my Debian setup, R copies all of the HTML help into a temporary
directory. Why not just refer to the files where they are, rather than
copying them all across? Because I don't understand these two points, I've
written my indexing routines to (a) create a search index in a temporary
directory, and (b) refer to the files in their install directories. My
indexing routines run the same under both Windows and Linux.


ISSUES WITH R ------------- 
The R "Installation and administration"  document tells us that "Sun's
Java Run-time Environment j2re 1.4.2_02 does not work under Linux". Prof
Ripley said on 1 April 2004 that "if Linux/Unix, Sun JRE 1.4.2_02/3/4 are
broken"  This is news to those like myself who run Linux with Sun's JRE
1.4.2_03 and find that all their other applets work fine. (Though I'm sure
there are bugs in the JRE, as in most complex projects.)

On my computer, the trouble boiled down to this: the Javascript which
displays search results was unable to interface with the Java applet which
performed the search. As far as I am aware, there are no published
standards which govern this interface. Therefore it is necessary to rely
on vendor documentation (insofar as we can say that organizations which
distribute free software are vendors). In the case of Mozilla, this
interface is called LiveConnect; some documentation is available at
mozilla.org. Generally speaking, an object on a web page may export
certain methods, making them available to Javascript. For example, an
object which contains a Java applet typically makes available the static
methods of the classes in that applet. Again, I am not aware of any
published standards on which methods are exported, so again we have to
rely on vendor documentation. In the case of Sun's Java, the documentation
explains how to use these exported methods
http://java.sun.com/j2se/1.4.2/docs/guide/plugin/developer_guide/js_java.html
However, the R HTML search page does not follow this documentation. I
found that if I alter the R HTML search page to conform to this
documentation, it works.

It is always going to be difficult to write portable code when there are
no published standards, only vendor-specific documentation. I have
therefore attempted, in my Javascript search, to stick to pure ECMAscript
(though undoubtedly I have failed in places).

Damon.



From edd at debian.org  Thu Apr 22 15:46:37 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 22 Apr 2004 08:46:37 -0500
Subject: [R] RODBC installation in debian
In-Reply-To: <20040422082350.81867.qmail@web41214.mail.yahoo.com>
References: <20040422082350.81867.qmail@web41214.mail.yahoo.com>
Message-ID: <20040422134637.GA5044@sonny.eddelbuettel.com>


On Thu, Apr 22, 2004 at 01:23:50AM -0700, Mahbub Latif wrote:
> Hello List,
> 
> I am trying to install RODBC package in a debian linux
> box but getting the following message. Can anyone help
> me to find what I am doing wrong here:
> 
> $ R CMD INSTALL RODBC_1.0-4.tar.gz

Well, you could use

	$ apt-get install r-cran-rodbc

instead. 

Other than that, the error below seems to indicate that you did not install
the development package for the odbc headers, i.e. unixodbc-dev.

Hth, Dirk



> ###
> * Installing *source* package 'RODBC' ...
> checking for gcc... gcc
> checking for C compiler default output... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler...
> yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none
> needed
> checking for library containing SQLTables... no
> configure: error: "no ODBC driver manager found"
> ERROR: configuration failed for package 'RODBC'
> ** Removing '/usr/local/lib/R/site-library/RODBC'
> 
> 
> R > version
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
> 
> Thanks,
> 
> Mahbub.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From edd at debian.org  Thu Apr 22 15:57:57 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 22 Apr 2004 08:57:57 -0500
Subject: [R] Urgent:again question about nmath/standalone
In-Reply-To: <1082641042.4087ca9279d2a@mailhost.cs.concordia.ca>
References: <1082641042.4087ca9279d2a@mailhost.cs.concordia.ca>
Message-ID: <20040422135757.GC5044@sonny.eddelbuettel.com>

On Thu, Apr 22, 2004 at 09:37:22AM -0400, xt_wang at cs.concordia.ca wrote:
> Hello, all,
> 
> I have the same question as the last mail I sent. I have installed libRmath in 

I'd still recommend the same I sent you last time, e.g. in.

	http://tolstoy.newcastle.edu.au/R/help/04/04/0719.html

Try with a smaller example that does not add your code as e.g. the test file
I sent. I find debugging 'upwards' starting from the simple-most possible,
and verified, test case, and the adding (and verifiying) incrementally, to
be the most bullet-proof method.

Dirk

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From Brian.Beckage at uvm.edu  Thu Apr 22 16:00:14 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Thu, 22 Apr 2004 10:00:14 -0400
Subject: [R] Trouble with HTML search engine
In-Reply-To: <Pine.SOL.3.96.1040422140058.15487A-100000@draco.cus.cam.ac.uk>
References: <Pine.SOL.3.96.1040422140058.15487A-100000@draco.cus.cam.ac.uk>
Message-ID: <p06020400bcad7f37e318@[10.0.1.2]>

I tried under Mac OS 10.3.3 using


>source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")
helpHTML()

from within xemacs using Safari and the "R search engine" page comes 
up.  I tried two searches on 'lm' and 'matrix' and it seemed to work 
fine.  This is very nice as I have not been able to use the R Search 
Engine on this system despite having Java enabled, etc.

Brian



At 2:43 PM +0100 4/22/04, Damon Wischik wrote:
>There have been a number of posts to this list by people having trouble
>with the HTML search engine. Often these troubles are caused by incorrect
>setups (user hasn't installed Java properly, or Java is disabled, or
>Javascript is disabled). Sometimes the trouble persists even when Java is
>installed properly.
>
>I have written an alternative HTML search engine, which is based on
>Javascript rather than Java. (Hopefully, this means that there is less
>that can go wrong.) I haven't wrapped it up in a package yet, because I
>don't know how well it works or even if there is any interest. If you
>would like to try it, you can:
>
>source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")
>helpHTML()
>
>(Or you can download the code and run it locally. You will also need to
>download searchtemplate.html from the same location. You can then run
>helpHTML(searchTemplate=localfilename) to tell it to use your local copy
>of searchtemplate.html.)
>
>ISSUES WITH MY CODE
>-------------------
>I've tested it on Windows XP cross {R1.8.0,R1.9.0} cross {IE,Firebird}.
>I've also tested it on Debian 3.0 with R1.8.0 cross {Mozilla,Firebird}.
>I'd be grateful to learn whether it works elsewhere.
>
>Searching is a bit slow. On my newish computer it takes three seconds or
>so. On an older departmental machine it takes ten seconds. Such is
>(Javascript) life.
>
>My searching algorithm is not the same as the current searching algorithm.
>I wrote this for my own use, and so I've used a scoring mechanism which
>reflects the way I like to search. The text at the top of the search page
>explains some of the options.
>
>There are obviously things I don't understand about the current help
>setup. If anyone is sufficiently interested in this to explain them to me,
>I would be grateful. (1) On my Windows XP setup, R writes an index in the
>directory it was installed. What if it doesn't have write permission?  (2)
>On my Debian setup, R copies all of the HTML help into a temporary
>directory. Why not just refer to the files where they are, rather than
>copying them all across? Because I don't understand these two points, I've
>written my indexing routines to (a) create a search index in a temporary
>directory, and (b) refer to the files in their install directories. My
>indexing routines run the same under both Windows and Linux.
>
>
>ISSUES WITH R -------------
>The R "Installation and administration"  document tells us that "Sun's
>Java Run-time Environment j2re 1.4.2_02 does not work under Linux". Prof
>Ripley said on 1 April 2004 that "if Linux/Unix, Sun JRE 1.4.2_02/3/4 are
>broken"  This is news to those like myself who run Linux with Sun's JRE
>1.4.2_03 and find that all their other applets work fine. (Though I'm sure
>there are bugs in the JRE, as in most complex projects.)
>
>On my computer, the trouble boiled down to this: the Javascript which
>displays search results was unable to interface with the Java applet which
>performed the search. As far as I am aware, there are no published
>standards which govern this interface. Therefore it is necessary to rely
>on vendor documentation (insofar as we can say that organizations which
>distribute free software are vendors). In the case of Mozilla, this
>interface is called LiveConnect; some documentation is available at
>mozilla.org. Generally speaking, an object on a web page may export
>certain methods, making them available to Javascript. For example, an
>object which contains a Java applet typically makes available the static
>methods of the classes in that applet. Again, I am not aware of any
>published standards on which methods are exported, so again we have to
>rely on vendor documentation. In the case of Sun's Java, the documentation
>explains how to use these exported methods
>http://java.sun.com/j2se/1.4.2/docs/guide/plugin/developer_guide/js_java.html
>However, the R HTML search page does not follow this documentation. I
>found that if I alter the R HTML search page to conform to this
>documentation, it works.
>
>It is always going to be difficult to write portable code when there are
>no published standards, only vendor-specific documentation. I have
>therefore attempted, in my Javascript search, to stick to pure ECMAscript
>(though undoubtedly I have failed in places).
>
>Damon.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From ripley at stats.ox.ac.uk  Thu Apr 22 16:01:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 15:01:39 +0100 (BST)
Subject: [R] Trouble with HTML search engine
In-Reply-To: <Pine.SOL.3.96.1040422140058.15487A-100000@draco.cus.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0404221448340.30177-100000@gannet.stats>

On Thu, 22 Apr 2004, Damon Wischik wrote:

> 
> There have been a number of posts to this list by people having trouble
> with the HTML search engine. Often these troubles are caused by incorrect
> setups (user hasn't installed Java properly, or Java is disabled, or
> Javascript is disabled). Sometimes the trouble persists even when Java is
> installed properly.
> 
> I have written an alternative HTML search engine, which is based on
> Javascript rather than Java. (Hopefully, this means that there is less
> that can go wrong.) I haven't wrapped it up in a package yet, because I
> don't know how well it works or even if there is any interest. If you
> would like to try it, you can:
> 
> source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")
> helpHTML()
> 
> (Or you can download the code and run it locally. You will also need to
> download searchtemplate.html from the same location. You can then run
> helpHTML(searchTemplate=localfilename) to tell it to use your local copy
> of searchtemplate.html.)
> 
> ISSUES WITH MY CODE
> -------------------
> I've tested it on Windows XP cross {R1.8.0,R1.9.0} cross {IE,Firebird}.
> I've also tested it on Debian 3.0 with R1.8.0 cross {Mozilla,Firebird}.
> I'd be grateful to learn whether it works elsewhere.
> 
> Searching is a bit slow. On my newish computer it takes three seconds or
> so. On an older departmental machine it takes ten seconds. Such is
> (Javascript) life.
> 
> My searching algorithm is not the same as the current searching algorithm.
> I wrote this for my own use, and so I've used a scoring mechanism which
> reflects the way I like to search. The text at the top of the search page
> explains some of the options.
> 
> There are obviously things I don't understand about the current help
> setup. If anyone is sufficiently interested in this to explain them to me,
> I would be grateful. (1) On my Windows XP setup, R writes an index in the
> directory it was installed. What if it doesn't have write permission?  

That *is* described in the rw-FAQ.

> (2) On my Debian setup, R copies all of the HTML help into a temporary
> directory. Why not just refer to the files where they are, rather than
> copying them all across? Because I don't understand these two points,
> I've

It doesn't, it copies some files and links others.  The reason it copies
some is that fairly recent security checks mean the code fails if those
are links.  The reason that it copies/links is that the HTML files have
links of the form ../../pkgname/html/funcname.html which will not work
across library trees.

> written my indexing routines to (a) create a search index in a temporary
> directory, and (b) refer to the files in their install directories. My
> indexing routines run the same under both Windows and Linux.
> 
> 
> ISSUES WITH R ------------- 
> The R "Installation and administration"  document tells us that "Sun's
> Java Run-time Environment j2re 1.4.2_02 does not work under Linux". Prof
> Ripley said on 1 April 2004 that "if Linux/Unix, Sun JRE 1.4.2_02/3/4 are
> broken"  This is news to those like myself who run Linux with Sun's JRE
> 1.4.2_03 and find that all their other applets work fine. (Though I'm sure
> there are bugs in the JRE, as in most complex projects.)

Note that a patch-level release stopped existing code working, on some but 
not all platforms.  If we did that in R without any mention in the relase 
notes, I am sure you would complain.

> On my computer, the trouble boiled down to this: the Javascript which
> displays search results was unable to interface with the Java applet which
> performed the search. As far as I am aware, there are no published
> standards which govern this interface. Therefore it is necessary to rely
> on vendor documentation (insofar as we can say that organizations which
> distribute free software are vendors). In the case of Mozilla, this
> interface is called LiveConnect; some documentation is available at
> mozilla.org. Generally speaking, an object on a web page may export
> certain methods, making them available to Javascript. For example, an
> object which contains a Java applet typically makes available the static
> methods of the classes in that applet. Again, I am not aware of any
> published standards on which methods are exported, so again we have to
> rely on vendor documentation. In the case of Sun's Java, the documentation
> explains how to use these exported methods
> http://java.sun.com/j2se/1.4.2/docs/guide/plugin/developer_guide/js_java.html
> However, the R HTML search page does not follow this documentation. I
> found that if I alter the R HTML search page to conform to this
> documentation, it works.

Can you tell us exactly what changes you made, please?

> It is always going to be difficult to write portable code when there are
> no published standards, only vendor-specific documentation. I have
> therefore attempted, in my Javascript search, to stick to pure ECMAscript
> (though undoubtedly I have failed in places).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 22 16:09:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 15:09:12 +0100 (BST)
Subject: [R] Urgent:again question about nmath/standalone
In-Reply-To: <1082641042.4087ca9279d2a@mailhost.cs.concordia.ca>
Message-ID: <Pine.LNX.4.44.0404221502290.30177-100000@gannet.stats>

WHY is this urgent?

The error message is '/usr/bin/ld: cannot find -lRmath' so you have not
put libRmath somewhere ld can find it.  That is not an R question, and we
can't help you solve it, especially as you have not told us your OS.  It
does look as if you may not have permission to read the copies in
/usr/lib, but you haven't told us whether your user account is the same as
bcdesai, not the search path ld is using.  Please ask your local OS 
support for help.


On Thu, 22 Apr 2004 xt_wang at cs.concordia.ca wrote:

> Hello, all,
> 
> I have the same question as the last mail I sent. I have installed libRmath in 
> my system. But I still can not link Rmath through -lRmath. The whole process 
> is as follow.Is there anybody who is so kind to help me find out the problem? 
> I will appreciate very much.
> 
> [credsim at confsys ~]$ cd /usr/lib
> [credsim at confsys lib]$ ls -l libR*
> -rw-------    1 bcdesai  bcdesai    237828 Apr 15 18:10 libRmath.a
> lrwxrwxrwx    1 root     root           17 Apr 15 18:12 libRmath.so -> 
> libRmath.so.1.0.0
> lrwxrwxrwx    1 root     root           17 Apr 15 18:12 libRmath.so.1 -> 
> libRmath.so.1.0.0
> -rw-------    1 bcdesai  bcdesai    114084 Apr 15 18:10 libRmath.so.1.0.0
> [credsim at confsys lib]$ locate test1.c
> /b2/home/credsim/src/test1.c
> /b2/downloads/a-mp-ms-my-o-p-u/mysql-3.23.55/heap/hp_test1.c
> /b2/downloads/a-mp-ms-my-o-p-u/mysql-3.23.55/myisam/ft_test1.c
> /b2/downloads/a-mp-ms-my-o-p-u/mysql-3.23.55/myisam/mi_test1.c
> /b2/home-apr-18-04-1145Am/credsim/src/test1.c
> [credsim at confsys lib]$ cd  /b2/home/credsim/src
> [credsim at confsys ~/src]$ gcc -Wall -o test1 test1.c -lRmath -lm
> test1.c:12: warning: return type defaults to `int'
> test1.c: In function `main':
> test1.c:21: warning: implicit declaration of function `Allocate_Memory_2D'
> test1.c:21: warning: assignment makes pointer from integer without a cast
> test1.c:22: warning: assignment makes pointer from integer without a cast
> test1.c:37: warning: too many arguments for format
> test1.c:44: warning: implicit declaration of function `solve'
> test1.c:44: warning: assignment makes pointer from integer without a cast
> test1.c:54: warning: control reaches end of non-void function
> test1.c: At top level:
> test1.c:58: warning: type mismatch with previous implicit declaration
> test1.c:21: warning: previous implicit declaration of `Allocate_Memory_2D'
> test1.c:58: warning: `Allocate_Memory_2D' was previously implicitly declared 
> to return `int'
> /usr/bin/ld: cannot find -lRmath
> collect2: ld returned 1 exit status
> [credsim at confsys ~/src]$ gcc test1.c -o test1 -lRmath
> test1.c: In function `main':
> test1.c:21: warning: assignment makes pointer from integer without a cast
> test1.c:22: warning: assignment makes pointer from integer without a cast
> test1.c:44: warning: assignment makes pointer from integer without a cast
> test1.c: At top level:
> test1.c:58: warning: type mismatch with previous implicit declaration
> test1.c:21: warning: previous implicit declaration of `Allocate_Memory_2D'
> test1.c:58: warning: `Allocate_Memory_2D' was previously implicitly declared 
> to return `int'
> /usr/bin/ld: cannot find -lRmath
> collect2: ld returned 1 exit status
> 
> "test1.c"
> #define MATHLIB_STANDALONE
> #include <math.h>
> #include <stdlib.h>
> #include <stdio.h>
> #include </usr/local/lib/R/include/Rmath.h>
> 
>  main()
> {
> 
>   double **x, **y, **x1, **y1, valin;
>   int i,j,I,J;
>   
> 
>   I=3;
>   J=3;
> 
>   x=Allocate_Memory_2D( I, J, x1);
>   y=Allocate_Memory_2D( I, J, y1);
> 
> 
>   FILE *in_file; 
> 
> 
>     /* input x value from file data_2Dx.txt */ 
>     in_file=fopen("data_2Dx.txt","r");
>         if (in_file==NULL)
>         {/*Test for error*/
>                 fprintf(stderr,"Error:Unable to input file 
> from 'data_2Dx.txt'\n");
>                 exit(8);
>         }
>         for( i=0;i<I; i++)
>           for (j=0;j<J;j++)
>           { fscanf(in_file, "%lf\n", &valin, stdin);/* read a single double 
> value in */
>                x[i][j]=valin;
>                    valin=0.0;
>           }
>         fclose(in_file);
> 
> 
>    y=solve(x);
> 
>    for (i=0;i<I;i++)
>      for (j=0;j<J;j++)
>      {
>        printf ("y[%d][%d]=%lf\n", i, j, y[i][j]);
>      }
> 
> }
> 
> 
> double **Allocate_Memory_2D( int I, int J, double **W)
> { 
>         int i;
> 
>         W=(double **)malloc(I*sizeof (double *));
>     if(!W)
>       printf("It is out of memory. Allocation failed.");
>         for (i=0;i<I;i++)
>         {
>         W[i]=(double *)malloc(J*sizeof(double));
>         if(!W[i])
>           printf("It is out of memory. Allocation failed.");
>         }
>         return (W);
> }
> 
> Thanks a lot!
> 
> Maggie Wang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jason.L.Higbee at stls.frb.org  Thu Apr 22 15:52:23 2004
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Thu, 22 Apr 2004 08:52:23 -0500
Subject: [R] Re: pausing a program
Message-ID: <20040422142511.9438986A80@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040422/e16837fa/attachment.pl

From ajayshah at mayin.org  Thu Apr 22 07:49:06 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 22 Apr 2004 11:19:06 +0530
Subject: [R] Improving on edit()?
Message-ID: <20040422054906.GA28355@igidr.ac.in>

I learned about edit() recently and it's nice. For those on the list
who haven't found it yet, try:

> X = data.frame(100:0,200:100,300:200)
> Y=edit(X)

It gives you a (low grade) GUI where you can look at the data frame,
and make changes.

I was curious about a few things:

  1. tk has full featured support for "spreadsheet type" tables
     where the user can move around efficiently, make changes, etc.
     Perhaps it would be possible to make edit() use this?

  2. How might one write a function "ooedit" which would do the
     following:
        Write out the data as a .csv file
        Run openoffice on it
        Pick up the resulting .csv file upon exit and return it.

     :-)
     My R knowledge doesn't yet support writing this..

Thanks,

        -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Thu Apr 22 07:34:40 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 22 Apr 2004 11:04:40 +0530
Subject: Evidence from Debian's package tracking (Was Re: [R] Size of R user
	base.)
Message-ID: <20040422053440.GA27799@igidr.ac.in>

I have watched the discussions about the size of the R user base with
much interest. One more source of data that might help is the
voluntary data capture in Debian. If you are a Debian user, you should
volunteer information. It's very easy: as root, say:

      # apt-get install popularity-contest

The results are found at:

  http://popcon.debian.org/main/math/by_inst

This shows that of the 4800 people who volunteered information, 1631
had installed gnuplot -- which suggests that perhaps one third of
Debian installs are by numerate people. R-base was installed by
roughly one-tenth of the sample.

So that's one useful fact: Roughly one in ten of Debian users is an R
user. Roughly one in three of the numerate users is an R user.

I would take this one-in-ten fact quite seriously, except for the
extent to which which R users are perhaps more likely (as compared
with the population) to volunteer information about what packages they
use.

Now let's engage in some wild guesswork.

* It is believed that there are roughly 2e7 desktops in the world
  today, running a freeware Unix system.

* Debian is undoubtedly a biased source of data, in having the more
  geeky users. Let's knock off a factor of 10 in order to correct for
  this.

* If we think that 1% of all freeware Unix users are R users, then we
  get to an estimate of 200,000 users of R in the freeware Unix
  world. There would be more using Mac OS X, Solaris, etc.

Google data shows that 1% of google hits are from Linux while 4% are
from Mac users. So for each Linux user, there are 4 Mac OS X
users. But then, a lot of them are Aunt Tillie, and are unlikely to
need anything more than a calculator.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From feh3k at spamcop.net  Thu Apr 22 14:20:08 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 22 Apr 2004 08:20:08 -0400
Subject: [R] POSIXct vs Dates
Message-ID: <20040422082008.1b872388.feh3k@spamcop.net>

I noticed the addition of the Dates class for dates without times, in R
1.9.  I am making extensive use of POSIXct at present and would like to
know whether it is worth changing to Dates.  What are a few of the
trade-offs?

Thanks,

Frank
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From tpapp at axelero.hu  Thu Apr 22 16:48:57 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Thu, 22 Apr 2004 16:48:57 +0200
Subject: [R] Re: pausing a program
In-Reply-To: <20040422142511.9438986A80@p3fed1.frb.org>
References: <20040422142511.9438986A80@p3fed1.frb.org>
Message-ID: <20040422144857.GA3299@localhost>

On Thu, Apr 22, 2004 at 08:52:23AM -0500, Jason.L.Higbee at stls.frb.org wrote:

> R:
> 
> I have a program that runs a For loop for days and I need to (if possible) 
> pause the program.  Any ideas on how to do that?  If I use stop, how 
> certain can I be that all the statements executed in the previous 
> iteration of the for loop?

What I usually do is start the program from the shell prompt (you need
Unix for that) command line (R <problem.r >problem.log) and use Ctrl-Z
to pause it, and fg (shell command to restart it).

I think that you might be able to achieve the same effect using the
unix command kill, look at the man page (you need kill -STOP pid and
kill -CONT pid).

If you are referring to kill -STOP in your message (the stop() in R
does something completely different), I don't see what you want.  Have
the program print something [eg print(paste("completed iteration #", i))]
at the end of each loop, and use

$ tail problem.log

to check that.  BTW, how could any of the statements in the loop body
NOT execute? I don't get it.  A loop executes all of its body in each
iteration, so all the statements in the previous iteration were
executed by definition...

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From apjaworski at mmm.com  Thu Apr 22 17:00:39 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 22 Apr 2004 10:00:39 -0500
Subject: [R] Question on CAR appendix on NLS
In-Reply-To: <20040421161251.GA25189@igidr.ac.in>
Message-ID: <OFB9DF4CD7.DE0A4E40-ON86256E7E.00524894-86256E7E.00527538@mmm.com>






Ajay,

I beleve there is an error in your hand-calculated derivatives.

model = function(beta1, beta2, beta3, time) {
   m = beta1/(1+exp(beta2+beta3*time))
   term = exp(beta2 + beta3*time)
   gradient = cbind((1+term)^-2,   <--------  the exponent should be -1
     -beta1*(1+term)^-2 * term,
     -beta1*(1+term)^-2 * term * time)
   attr(m, 'gradient') <- gradient
   m
 }

Cheers,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Ajay Shah                                                     
             <ajayshah at mayin.o                                             
             rg>                                                        To 
             Sent by:                  r-help <r-help at stat.math.ethz.ch>   
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] Question on CAR appendix on NLS 
             04/21/2004 11:12                                              
             AM                                                            
                                                                           
                                                                           
                                                                           
                                                                           




The PDF file on the web, which is an appendix on nonlinear regression
associated with the CAR book, is very nice.

When I ran through the code presented there, I found something
odd. The code does a certain model in 3 ways: Vanilla NLS (using
numerical differentation), Analytical derivatives (where the user
supplies the derivatives) and analytical derivatives (using automatic
differentiation). The three results agree, except for the correlation
of parameter estimates :

          beta1   beta2
  beta2 -0.1662                      Numerical derivatives
  beta3  0.9145 -0.5407

  beta2 -0.7950                      Analytical derivatives
  beta3  0.9145 -0.9661

  beta2 -0.1662                      Automatic differentiation
  beta3  0.9145 -0.5407

Is this just a glitch of a small sample, or should I worry? My source
file (which should be the same as John Fox's file; I typed it in while
reading the PDF file, and made minor changes) is attached.

--
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
[attachment "nls.R" deleted by Andrzej P. Jaworski/US-Corporate/3M/US]
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Apr 22 17:19:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2004 17:19:14 +0200
Subject: [R] Urgent:again question about nmath/standalone
In-Reply-To: <20040422135757.GC5044@sonny.eddelbuettel.com>
References: <1082641042.4087ca9279d2a@mailhost.cs.concordia.ca>
	<20040422135757.GC5044@sonny.eddelbuettel.com>
Message-ID: <x2brlk9h1p.fsf@biostat.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> On Thu, Apr 22, 2004 at 09:37:22AM -0400, xt_wang at cs.concordia.ca wrote:
> > Hello, all,
> > 
> > I have the same question as the last mail I sent. I have installed libRmath in 
> 
> I'd still recommend the same I sent you last time, e.g. in.
> 
> 	http://tolstoy.newcastle.edu.au/R/help/04/04/0719.html
> 
> Try with a smaller example that does not add your code as e.g. the test file
> I sent. I find debugging 'upwards' starting from the simple-most possible,
> and verified, test case, and the adding (and verifiying) incrementally, to
> be the most bullet-proof method.

Well, yes, but in the present case there are a couple of fairly
obvious things to try fixing. The only Rmath related thing is that the
linking command is missing a -L /usr/lib/R/..../standalone

The rest are problems with the general C programming, e.g. the need to
define or at least declare functions before they are used.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rossini at blindglobe.net  Thu Apr 22 17:27:36 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 22 Apr 2004 08:27:36 -0700
Subject: [R] Re: Evidence from Debian's package tracking
In-Reply-To: <20040422053440.GA27799@igidr.ac.in> (Ajay Shah's message of
	"Thu, 22 Apr 2004 11:04:40 +0530")
References: <20040422053440.GA27799@igidr.ac.in>
Message-ID: <85smewghhz.fsf@servant.blindglobe.net>

Ajay Shah <ajayshah at mayin.org> writes:

>
> This shows that of the 4800 people who volunteered information, 1631
> had installed gnuplot -- which suggests that perhaps one third of
> Debian installs are by numerate people. R-base was installed by
> roughly one-tenth of the sample.
>
> So that's one useful fact: Roughly one in ten of Debian users is an R
> user. Roughly one in three of the numerate users is an R user.

The dangers with generalization... I don't have gnuplot on most of my
machines. 

So either I'm not numerate, or that assumption is wrong.

(however, given my counting skills yesterday, the former might be
true).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Thu Apr 22 17:28:44 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 22 Apr 2004 08:28:44 -0700
Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
In-Reply-To: <Pine.LNX.4.44.0404220718300.26762-100000@gannet.stats> (Brian
	Ripley's message of "Thu, 22 Apr 2004 07:19:47 +0100 (BST)")
References: <Pine.LNX.4.44.0404220718300.26762-100000@gannet.stats>
Message-ID: <85oepkghg3.fsf@servant.blindglobe.net>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Thu, 22 Apr 2004, Fan wrote:
>
>> Brian, thanks for the hint, it works.
>> 
>> The old 1.8 version works very well without the need of such
>> "manual" loading, I guess, there'd be some internal changes
>> in the order of libraries loading at the startup
>> in the 1.9.0 version ?
>
> Yes, and that is right at the top of the NEWS file, including the fix.
>
> BTW, there has been a `1.8 version' of R.

You perhaps mean "there has not"?

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Thu Apr 22 17:29:27 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 22 Apr 2004 08:29:27 -0700
Subject: [R] calling R from java
In-Reply-To: <Pine.GSO.4.40.0404221509540.9708-100000@ensisun> (anthony's
	message of "Thu, 22 Apr 2004 15:14:58 +0200 (MEST)")
References: <Pine.GSO.4.40.0404221509540.9708-100000@ensisun>
Message-ID: <85k708ghew.fsf@servant.blindglobe.net>


I'll repeat:

"Good Luck".

See the omegahat list archives for help.

best,
-tony


"anthony.ferrari at ensimag.imag.fr" <ferraria at ensisun.imag.fr> writes:

> I downloaded it and installed it with all the recommended options. I
> created the LD_LIBRARY_PATH
> environment variable but my java compiler (latest version) can't find
> packages like org.omegahat.R.java and cannot resolve symbol (class) like
> REvaluator, RForeignReference...  any idea ?
>
> many thanks for answering my last question I promise !
> best regards,
> Anthony
>
>
> On Wed, 21 Apr 2004, A.J. Rossini wrote:
>
>>
>> Look at the SJava package on Omegahat.org
>>
>> Good luck.
>>
>> best,
>> -tony
>>
>>
>> "anthony.ferrari at ensimag.imag.fr" <ferraria at ensisun.imag.fr> writes:
>>
>> > Hello,
>> >
>> > I need to call R from a java(swing) application. I manage to do it with
>> > something like :
>> >
>> > Process p = Runtime.getRuntime().exec("R --slave")
>> > OutputStreanWriter o = new OutputStreamWriter(p.getOutputStream())
>> > o.wrote("...")
>> > ... etc
>> >
>> > but at the end no .Rdata file has been created and there are some data I
>> > don't want to reload each time (for time execution reasons).
>> > So,
>> > Is it possible when calling R from java to create a .Rdata file to save
>> > data and as a consequence when recalling in the same directory not to have
>> > to reload those data ?
>> >  (Maybe the option --slave need to be changed ? )
>> >
>> >
>> > many thanks for helping me
>> >
>> > best regards,
>> > af
>> > anthony.ferrari at ensimag.imag.fr
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> >
>>
>> --
>> rossini at u.washington.edu            http://www.analytics.washington.edu/
>> Biomedical and Health Informatics   University of Washington
>> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
>> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
>> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>>
>> CONFIDENTIALITY NOTICE: This e-mail message and any attachments may be
>> confidential and privileged. If you received this message in error,
>> please destroy it and notify the sender. Thank you.
>>
>
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From B.Rowlingson at lancaster.ac.uk  Thu Apr 22 17:38:49 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 22 Apr 2004 16:38:49 +0100
Subject: [R] Urgent:again question about nmath/standalone
In-Reply-To: <Pine.LNX.4.44.0404221502290.30177-100000@gannet.stats>
References: <Pine.LNX.4.44.0404221502290.30177-100000@gannet.stats>
Message-ID: <4087E709.3040405@lancaster.ac.uk>


>>[credsim at confsys ~]$ cd /usr/lib
>>[credsim at confsys lib]$ ls -l libR*
>>-rw-------    1 bcdesai  bcdesai    237828 Apr 15 18:10 libRmath.a
>>lrwxrwxrwx    1 root     root           17 Apr 15 18:12 libRmath.so -> 
>>libRmath.so.1.0.0
>>lrwxrwxrwx    1 root     root           17 Apr 15 18:12 libRmath.so.1 -> 
>>libRmath.so.1.0.0
>>-rw-------    1 bcdesai  bcdesai    114084 Apr 15 18:10 libRmath.so.1.0.0



Permissions?

  Are you logged in as user 'bcdesai'? Because only that username can 
read the libRmath files in /usr/lib.

  If you are logged in as bcdesai and its still not working, then it 
wouldn't surprise me if linux was paranoid enough to not allow you to 
link to libraries not owned by 'root' in /usr/lib.

  So, change the owner to root and the permissions to 755 and try again. 
You may need your friendly local sysadmin to do this. Or 'bcdesai'.

Baz



From VinyardWC at logcom.usmc.mil  Thu Apr 22 17:43:47 2004
From: VinyardWC at logcom.usmc.mil (Vinyard Maj William C)
Date: Thu, 22 Apr 2004 11:43:47 -0400
Subject: [R] Fatal Error: Invalid HOMEDRIVE
Message-ID: <E85DCCA6ED13F749A27DEF593B2297FB03669798@matcomexch02.matcom.usmc.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040422/55a2262d/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 22 17:49:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 16:49:54 +0100 (BST)
Subject: [R] Error with 1.9.0 - winMenuAdd not usable in .Rprofile
In-Reply-To: <85oepkghg3.fsf@servant.blindglobe.net>
Message-ID: <Pine.LNX.4.44.0404221649090.31934-100000@gannet.stats>

On Thu, 22 Apr 2004, A.J. Rossini wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > On Thu, 22 Apr 2004, Fan wrote:
> >
> >> Brian, thanks for the hint, it works.
> >> 
> >> The old 1.8 version works very well without the need of such
> >> "manual" loading, I guess, there'd be some internal changes
> >> in the order of libraries loading at the startup
> >> in the 1.9.0 version ?
> >
> > Yes, and that is right at the top of the NEWS file, including the fix.
> >
> > BTW, there has been a `1.8 version' of R.
> 
> You perhaps mean "there has not"?

I meant `there has never been', thank you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 22 17:55:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 16:55:03 +0100 (BST)
Subject: [R] HTML help pages
In-Reply-To: <Pine.SOL.3.96.1040127141430.7203A-100000@virgo.cus.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0404221637190.31934-100000@gannet.stats>

Unfortunately your suggested change to SearchEngine.html is not valid
JavaScript under Internet Explorer 6: at least APPLET appears to be
recognised under all commonly used browsers.

I found your Javascript search a lot slower under IE6 (and IE6 was a lot
slower than FireFox).  Not that I use IE6 if I can help it, but I suspect
more than 50% of users of R HTML help do.

On Tue, 27 Jan 2004, Damon Wischik wrote:

> 
> On Tue, 27 Jan 2004, Frank E Harrell Jr wrote:
> > I've always wondered whether there is a
> > way to implement this without java since the java approach has caused so
> > many problems for users and it seems to entail some overhead.
> 
> On my own pages, I've used Javascript for search rather than Java. You can
> see how it works at
>   http://www.wischik.com/damon/Recipe/index/search.html
> The idea is to embed all of the index into the html page in XML-like
> markup, and to have Javascript trawl through this list. Page download time
> should be much the same (with the current R solution, the index file has
> to be downloaded; with the Javascript, the index is downloaded as part of
> the search page.) Searching will be a bit slower; whether that is
> acceptable depends on the size of the index.
> 
> I'm glad to say I've finally got the searching to work in Mozilla Firebird
> 0.7. I think the problem is to do with this:
> * Mozilla Firebird 0.7 requires Java 1.4 or later
> * Java 1.4 from Sun does not properly support the Applet tag.
> 
> The solution (really a dirty non-standard hack), according to the Sun
> documentation, is to use code like the following: 
> 
> <embed type="application/x-java-applet"
>        code="SearchEngine.class"
>        width="0" height="0"
>        id="SearchEngine" 
>        scriptable="true"
>        INDEXFILE="index.txt">
> </embed>
> 
> instead of the current
> 
> <applet
>     code=SearchEngine.class
>     name=SearchEngine
>     width=0
>     height=0 >
>     <param name="INDEXFILE" value="index.txt">
> </applet>
> 
> The official W3C position is that APPLET is deprecated in favour of
> OBJECT, and EMBED is not even mentioned.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 22 18:04:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 17:04:08 +0100 (BST)
Subject: [R] POSIXct vs Dates
In-Reply-To: <20040422082008.1b872388.feh3k@spamcop.net>
Message-ID: <Pine.LNX.4.44.0404221655120.32019-100000@gannet.stats>

On Thu, 22 Apr 2004, Frank E Harrell Jr wrote:

> I noticed the addition of the Dates class for dates without times, in R
> 1.9.  I am making extensive use of POSIXct at present and would like to
> know whether it is worth changing to Dates.  What are a few of the
> trade-offs?

You lose the ability to handle times and timezones.

Those who do not understand timezones have no opportunity to make errors.

There is probably a little less support for Dates at present, but you can 
coerce to POSIXct if you need to.

(It's really the second point that motivated us.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From slyden at us.nomura.com  Thu Apr 22 18:10:35 2004
From: slyden at us.nomura.com (Lyden, Scott)
Date: Thu, 22 Apr 2004 12:10:35 -0400
Subject: [R] as.Date
Message-ID: <3BAEEB78D8B88440A86A933B9BC706BCAFE636@NYEX022.corp.us.nomura.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040422/85f4f4ae/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 22 18:16:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 17:16:44 +0100 (BST)
Subject: [R] as.Date
In-Reply-To: <3BAEEB78D8B88440A86A933B9BC706BCAFE636@NYEX022.corp.us.nomura.com>
Message-ID: <Pine.LNX.4.44.0404221712180.32019-100000@gannet.stats>

You do need R 1.9.0 or later.  On the other hand, `the manual' (which 
manual) should only refer to as.Date in 1.9.0 or later.  If you are 
using R 1.9.0, something is wrong (as that code is run as part of the 
installation checks).

(`or later' here means one of the r-patched or r-devel snapshots.)

On Thu, 22 Apr 2004, Lyden, Scott wrote:

> 
> Hi.  I'm sure this is a complete green-horn question.  I apologize.
> 
> I'm trying to use as.Date *exactly* as shown on p. 194 of the manual (code
> fragment and error message pasted below).  Is there some kind of "include"
> or "import" statement that I need to issue?  Thank you very much for saving
> what remains of my hair.
> 
> 
> > x <- c("1jan1960", "2jan1960", "31mar1960", "30jul1960")
> > z <- as.Date(x, "%d%b%Y")
> Error: couldn't find function "as.Date"
> > 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cepl at surfbest.net  Thu Apr 22 16:37:37 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Thu, 22 Apr 2004 10:37:37 -0400
Subject: [R] RODBC installation in debian
In-Reply-To: <Pine.SGI.4.40.0404220822120.11571122-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0404220822120.11571122-100000@origin.chass.utoronto.ca>
Message-ID: <200404221037.39933.cepl@surfbest.net>

On Thursday 22 of April 2004 08:29, Jean Eid wrote:
> I just did this yesterday. you do not have an ODBC driver ( as
> it says in the log). you need to (as root) use apt-get install
> unixodbc and apt-get install unixodbc-dev.
> The unixodbc package alone will not work, you need to install
> unixodbc-dev (development),

There is actually package r-cran-rodbc, which has all these 
things resolved. The package is for testing, but I have made a 
backport for stable which is available on http://www.ceplovi.cz/
matej/progs/debian.

	Have a nice day,

		Matej

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
We must all hang together, or assuredly we shall all hang
separately.
   -- Benjamin Franklin



From cepl at surfbest.net  Thu Apr 22 06:56:46 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Thu, 22 Apr 2004 00:56:46 -0400
Subject: [R] Selection of cities sample
Message-ID: <200404220056.46149.cepl@surfbest.net>

 Hi,

I have a question, how to most properly select set of cities 
which would be as similar as possible in some particular 
variables with the City of Boston (which I use as my base line). 
I thought about ordering cities by sum of ((differences between 
value of that particular variable for that particular city and 
the value of same variable for Boston) divided by the standard 
deviation of the variable and multiplied by the weight 
(expressing how much that particular variable is important for 
me, or how much I want to avoid cities with this 
characteristic)). Is it sound method?

Or I am creating something which is already available in R as a 
standard function (which I suspect)?

	Thanks,

		Matej

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
Give your heartache to him. (1Pt 5,7; Mt 11:28-30)



From stephen at anc.ed.ac.uk  Thu Apr 22 18:32:31 2004
From: stephen at anc.ed.ac.uk (Stephen Eglen)
Date: Thu, 22 Apr 2004 17:32:31 +0100
Subject: [R] Release candidate (5.2.0rc3) of Emacs Speaks Statistics
Message-ID: <16519.62367.975298.884144@bushmills.inf.ed.ac.uk>

We are almost ready to release Emacs Speaks Statistics (ESS) version
5.2.  A release candidate, 5.2.0rc3, is available at:

http://www.analytics.washington.edu/downloads/ess/ess-5.2.0rc3.tar.gz
or
http://www.analytics.washington.edu/downloads/ess/ess-5.2.0rc3.zip

If you know of any release-critical bugs, please report them with
Emacs using "M-x ess-submit-bug-report".  If no bugs are forthcoming,
we hope to release 5.2 next Wednesday.

Changes since 5.1.24 are listed below.

Thanks, 
The ESS Core Team.

Changes/New Features in 5.2.0:
   * ESS[BUGS]:  new info documentation!  now supports interactive
     processing thanks to Aki Vehtari (mailto:Aki.Vehtari at hut.fi); new
     architecture-independent unix support as well as support for BUGS
     v. 0.5

   * ESS[SAS]:  convert .log to .sas with ess-sas-transcript; info
     documentation improved; Local Variable bug fixes; SAS/IML
     statements/functions now highlighted; files edited remotely by
     ange-ftp/EFS/tramp are recognized and pressing SUBMIT opens a
     buffer on the remote host via the local variable
     ess-sas-shell-buffer-remote-init which defaults to "ssh"; changed
     the definition of the variable ess-sas-edit-keys-toggle to boolean
     rather than 0/1; added the function ess-electric-run-semicolon
     which automatically reverse indents lines containing only "run;";
     C-F1 creates MS RTF portrait from the current buffer; C-F2 creates
     MS RTF landscape from the current buffer; C-F9 opens a SAS DATASET
     with PROC INSIGHT rather than PROC FSVIEW; C-F10 kills all buffers
     associated with .sas program; "inferior" aliases for SAS batch:
     C-c C-r for submit region, C-c C-b for submit buffer, C-c C-x for
     goto .log; C-c C-y for goto .lst

   * ESS[S]: Pressing underscore ("_") once inserts " <- " (as before);
     pressing underscore twice inserts a literal underscore;
     ess-dump-filename-template-proto (new name!) now can be customized
     successfully (for S language dialects); Support for Imenu has been
     improved; set ess-imenu-use-S to non-nil to get an "Imenu-S" item
     on your menubar; ess-help: Now using nice underlines (instead of
     `nuke-* ^H_')

   * ESS[R]:  After (require 'essa-r), M-x ess-r-var allows to load
     numbers from any Emacs buffer into an existing *R* process; M-x
     ess-rdired gives a "directory editor" of R objects; fixed
     ess-retr-lastvalue-command, i.e. .Last.value bug (thanks to David
     Brahm)

   * ESS: Support for creating new window frames has been added to ESS.
     Inferior ESS processes can be created in dedicated frames by
     setting inferior-ess-own-frame to t.  ESS help buffers can also
     open in new frames; see the documentation for ess-help-own-frame
     for details.  (Thanks to Kevin Rodgers for contributing code.)



From p.dalgaard at biostat.ku.dk  Thu Apr 22 18:46:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Apr 2004 18:46:37 +0200
Subject: [R] Fatal Error: Invalid HOMEDRIVE
In-Reply-To: <E85DCCA6ED13F749A27DEF593B2297FB03669798@matcomexch02.matcom.usmc.mil>
References: <E85DCCA6ED13F749A27DEF593B2297FB03669798@matcomexch02.matcom.usmc.mil>
Message-ID: <x24qrcx8nm.fsf@biostat.ku.dk>

Vinyard Maj William C <VinyardWC at logcom.usmc.mil> writes:

> All,
> 
> I've encountered the same problem as others who have posted under the same
> subject.
> 
> I've had R-1.8.1 installed and running since it was released.  Yesterday
> morning when I tried to start Rgui.exe I got the subject error message.
> Since I live at the whim of the network administrators I can only assume it
> was a recent MS critical update.
> 
> I tried installing R-1.9.0 but no joy.
> 
> I created ".Renviron" with HOMEDRIVE and HOMEPATH values set appropriately
> (see bat file below) but this did not solve the problem either.
> 
> The following did work:
> 
> Create batch file MyRBat.bat
> 
> ---start---
> SET HOMEDRIVE=D:  # for some reason "echo %HOMEDRIVE%" returns "C:" even
> when I set the install path to D:\{HOME}\

I don't think that is anomalous in itself. Install paths and home
directories are distinct concepts. But could you perhaps prepend
something like

SET > c:/TEMP/envir.txt

(I'm very rusty on Windows, but I think that should work) and tell us
what the variables are set to? 

> SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> 
> Rgui.exe
> ---end---

Btw, there's a user-friendlier way of setting environment variables on
XP isn't there?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Thu Apr 22 19:02:18 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 22 Apr 2004 10:02:18 -0700 (PDT)
Subject: [R] Trouble with HTML search engine
In-Reply-To: <Pine.SOL.3.96.1040422140058.15487A-100000@draco.cus.cam.ac.uk>
References: <Pine.SOL.3.96.1040422140058.15487A-100000@draco.cus.cam.ac.uk>
Message-ID: <Pine.A41.4.58.0404220939530.151766@homer12.u.washington.edu>

On Thu, 22 Apr 2004, Damon Wischik wrote:
> There are obviously things I don't understand about the current help
> setup. If anyone is sufficiently interested in this to explain them to me,
> I would be grateful. (1) On my Windows XP setup, R writes an index in the
> directory it was installed. What if it doesn't have write permission?

Then the index doesn't get updated.  It is very difficult to come up with
a directory that is known to be writable under Windows.

>  (2)
> On my Debian setup, R copies all of the HTML help into a temporary
> directory. Why not just refer to the files where they are, rather than
> copying them all across?

It actually doesn't copy.  It makes symbolic links for most of it, so the
operating system is just referring to the files where they are.  It is
necessary to modify the index to allow for HTML help for installed
packages (the limitation you noted for Windows), and it is simplest to
link the HTML files to the same directory to allow relative paths to work.


	-thomas



From Tomo.Eguchi at noaa.gov  Thu Apr 22 19:03:09 2004
From: Tomo.Eguchi at noaa.gov (Tomo Eguchi)
Date: Thu, 22 Apr 2004 10:03:09 -0700
Subject: [R] error message in help.search
Message-ID: <000001c4288b$b39925a0$9a3637a1@laptop1>

Hello, 

I looked around but couldn't find solutions for my problem.  I downloaded
v.1.9 and the patched version for windows.  I use Windows XP professional
with AMD Athlon XP.  When I use help.search() function, I get the following
error message:

Error in .readRDS(contentsFile) : can't read workspace version 167772160
written by R 512.1.7; need R 256.1.4 or newer

What does this mean?  More importantly, how can I fix it?  

Thanks,

Tomo

*********************************
Tomoharu Eguchi
Protected Resources Division
Southwest Fisheries Science Center
8604 La Jolla Shores Dr.
La Jolla, CA 92037
858.546.5615 Voice
858.546.5657 FAX



From ripley at stats.ox.ac.uk  Thu Apr 22 20:25:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 22 Apr 2004 19:25:22 +0100 (BST)
Subject: [R] Trouble with HTML search engine
In-Reply-To: <Pine.A41.4.58.0404220939530.151766@homer12.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0404221911190.32448-100000@gannet.stats>

On Thu, 22 Apr 2004, Thomas Lumley wrote:

> On Thu, 22 Apr 2004, Damon Wischik wrote:
> > There are obviously things I don't understand about the current help
> > setup. If anyone is sufficiently interested in this to explain them to me,
> > I would be grateful. (1) On my Windows XP setup, R writes an index in the
> > directory it was installed. What if it doesn't have write permission?
> 
> Then the index doesn't get updated.  It is very difficult to come up with
> a directory that is known to be writable under Windows.

Actually, no, as R guarantees you one via tempdir() (not that it is easy,
but we do work on it).  The problem is that users can get at HTML search
without running R, for example directly from an RGui menu or from Windows
Explorer.  If we gave that up we could use a temporary location, provided
we can find a way to refer to it that Java/JavaScript understands in all
browsers and JVMs (and I don't know how to refer to Windows drives
completely portably in file: URLs - file:///C:/R/rw1090 seems the most
commonly accepted form, but one browser used to require : to be replaced
by |).

I don't think this is much of an issue.  Most people using R on Windows 
machines with read-only access don't install any packages.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hodgess at gator.uhd.edu  Thu Apr 22 21:06:24 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 22 Apr 2004 14:06:24 -0500
Subject: [R] downloading prob
Message-ID: <200404221906.i3MJ6Op20627@gator.dt.uh.edu>

Dear R People:

Are there problems with downloading the new R-1.9.0, please?

I've tried several mirrors but I keep getting thrown out.

Is anyone else having this problem, please?

TIA,
Hoping to become R-1.9.0 for Windows.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From myao at ou.edu  Thu Apr 22 21:00:18 2004
From: myao at ou.edu (Yao, Minghua)
Date: Thu, 22 Apr 2004 14:00:18 -0500
Subject: [R] Question on outer(x, y, FUN,...)
Message-ID: <78B50CF247E5D04B8A5E02D001CC8E9A595C4E@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040422/68fa18c3/attachment.pl

From ggrothendieck at myway.com  Thu Apr 22 21:53:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 22 Apr 2004 19:53:02 +0000 (UTC)
Subject: [R] POSIXct vs Dates
References: <20040422082008.1b872388.feh3k@spamcop.net>
Message-ID: <loom.20040422T194738-955@post.gmane.org>

Frank E Harrell Jr <feh3k <at> spamcop.net> writes:
> I noticed the addition of the Dates class for dates without times, in R
> 1.9.  I am making extensive use of POSIXct at present and would like to
> know whether it is worth changing to Dates.  What are a few of the
> trade-offs?

Before Date became available I moved all my POSIXct code to chron.
It would probably be easier to move to Date than chron
since Date seems very close in terms of its interface to POSIXct so
you would likely have an easier time of it than I did.

The trouble I was having using POSIXct was that its easy to introduce
subtle problems into your code without proper attention to time zones.

If you look through the archives of r-help you will find numerous 
examples of the sort of subtle problem that you can inadvertently
introduce.  The day of the week, day of the month, month of the
year, etc. are all time zone dependent since the same date time
might represent Monday in one time zone but Tuesday in another.
You MUST take great care that you know which timezone each routine
that your wrote or are making use of is assuming.

Time zones are not part of the problem
yet you have to track them.  That conflicts with good design 
since good design means your programs don't depend on extraneous
elements.

With chron and Date there are no time zones so this type of problem
will not occur in the first place.



From ade4 at biomserv.univ-lyon1.fr  Thu Apr 22 17:43:08 2004
From: ade4 at biomserv.univ-lyon1.fr (ade4@biomserv.univ-lyon1.fr)
Date: Thu, 22 Apr 2004 17:43:08 +0200
Subject: [R] [R-pkgs] ade4 package update
Message-ID: <p06002000bca43a623e05@[134.214.34.24]>

The ade4 package (v. 1.2-1) has been updated on CRAN.

New features include:

- functions based on Rao's axiomatization of diversity measures : Rao's
diversity coefficient and dissimilarity coefficient (divc and disc)

- functions based on Excoffier et al. analysis of molecular variance
with tests of the difference among the factors (amova).

- functions introducing double principal coordinate analysis (dpcoa).

- genet class and related functions to analyse genetic data (tables with
populations in rows and alleles/loci in columns).

- kdist class and related functions to analyse series of distance matrices
measured on the same individuals.

- rlq analysis, a three-tables ordination method, with permutation test (rlq)

- hill-smith analysis, a row-weighted version of dudi.mix (dudi.hillsmith)

- function ktab.match2ktabs, to prepare a STATICO analysis

- functions s.class and s.chull allow the use of color.

-- 
--
Jean Thioulouse - Labo. BBE, UMR CNRS 5558, Equipe "Ecologie Statistique"
Universite Lyon 1,  Batiment G. Mendel, 69622 Villeurbanne Cedex,  France
Tel: (33) 4 72 43 27 56                           Fax: (33) 4 72 43 27 56
                 http://pbil.univ-lyon1.fr/JTHome.html

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From dmurdoch at pair.com  Thu Apr 22 22:34:49 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 22 Apr 2004 16:34:49 -0400
Subject: [R] downloading prob
In-Reply-To: <200404221906.i3MJ6Op20627@gator.dt.uh.edu>
References: <200404221906.i3MJ6Op20627@gator.dt.uh.edu>
Message-ID: <m1bg8016p1a6rk8flfv7pk0utll52q1ciq@4ax.com>

On Thu, 22 Apr 2004 14:06:24 -0500, Erin Hodgess
<hodgess at gator.uhd.edu> wrote :

>Dear R People:
>
>Are there problems with downloading the new R-1.9.0, please?
>
>I've tried several mirrors but I keep getting thrown out.
>
>Is anyone else having this problem, please?
>
>TIA,
>Hoping to become R-1.9.0 for Windows.

I'd guess it's a problem local to you, because I haven't heard of it
from anyone else, and it's unlikely the mirrors would all have the
same problem.

Can you try downloading from a different location?

Duncan Murdoch



From xiao.gang.fan1 at libertysurf.fr  Thu Apr 22 23:02:02 2004
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Thu, 22 Apr 2004 23:02:02 +0200
Subject: [R] New version of benchmark comparing R with other software
References: <MABBLJDICACNFOLGIHJOOELFEFAA.phgrosjean@sciviews.org>
Message-ID: <408832CA.90702@libertysurf.fr>

Merci, Philippe.

Just a complement regarding Matlab 6.5 (R13).

R 1.9.0 is globally faster than Matlab 6.5 (tested on an AthlonXP):
                                   Matlab       R
Total time for all 15 tests (sec)  14.29   10.81
Overall mean (sec)                  0.86    0.45

Matlab 6.5 has optimized the loops performance (sth called "Just In Time"),
with spectacular results, codes with loops could be faster then vectorized
codes. Here's the result for the "loops" test:
                                               Matlab     R
Creation of a 220x220 Toeplitz matrix (loops)   0.01  0.50

Bravo to the R dev team, nice job !
Cheers
--
Fan

Philippe Grosjean wrote:
> Hello,
> 
> Thanks to Douglas Bates, there is now a new benchmark suite (version 2.3)
> which is compatible with R 1.9.0 and the recent Matrix library (0.8-1 or
> above). You find it at http://www.sciviews.org/other/benchmark.htm. It
> compares R 1.9.0 under Windows with:
> S-PLUS 6.5, Matlab 6.0, O-Matrix 5.6, Octave 2.1.42, Scilab 2.7 and Ox 3.30.
> 
> In short, R in its version 1.9.0 and with the new Matrix library, is now one
> of the fastest matrix calculation package among those tested on the computer
> and system used (P IV, 1 Gb RAM and Windows XP pro).
> 
> Keeping its limitations in mind (only a few functions tested, and relatively
> artificial situations that may or may not compare with real-world cases),
> this benchmark suite could be useful for deciding which software to chose
> for computing intensive matrix calculations, and also to compare the same
> software (R) on different platforms, with one or several processors, and/or
> with various optimized BLAS libraries.
> 
> Best,
> 
> Philippe Grosjean
> 
> .......................................................<??}))><....
>  ) ) ) ) )
> ( ( ( ( (   Prof. Philippe Grosjean
> \  ___   )
>  \/ECO\ (   Numerical Ecology of Aquatic Systems
>  /\___/  )  Mons-Hainaut University, Pentagone
> / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
>  /NUM\/  )
>  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
>        \ )  email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
> ( ( ( ( (
> ...................................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.connolly at hortresearch.co.nz  Thu Apr 22 23:43:19 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 23 Apr 2004 09:43:19 +1200
Subject: [R] POSIXct vs Dates
In-Reply-To: <loom.20040422T194738-955@post.gmane.org>;
	from ggrothendieck@myway.com on Thu, Apr 22, 2004 at 07:53:02PM
	+0000
References: <20040422082008.1b872388.feh3k@spamcop.net>
	<loom.20040422T194738-955@post.gmane.org>
Message-ID: <20040423094319.M2137@hortresearch.co.nz>

On Thu, 22-Apr-2004 at 07:53PM +0000, Gabor Grothendieck wrote:

[....]
 
|> Time zones are not part of the problem
|> yet you have to track them.  That conflicts with good design 
|> since good design means your programs don't depend on extraneous
|> elements.
|> 
|> With chron and Date there are no time zones so this type of problem
|> will not occur in the first place.

I, for one, think it's a great idea.  I'm sure it must be common to
deal only in whole days in a lot of fields.  When calculating the
difference between times, one is interested only in a precision of
days (with plus or minus 4 or 5 hours considered fair enough), yet the
difference between POSIXct times will be given as days if they're both
the same side of daylight saving changes and as seconds otherwise.
That's no big deal when used interactively, but not trivial otherwise.

I devised small functions that coerced everything to UT, but that had
to be done nearly every step of the way, otherwise the propensity to
convert back to local time kept jumping in.  It's kind of like that
software (there's plenty of it) that keeps defaulting to 'letter'
paper irrespective of how often you've set it to the proper
international standard.

Shortly after I got my GMT coercing functions working and almost ready
to make available to the list, I became aware of the forthcoming Date
class in 1.9.0, so I decided that was a better idea.

In another place and another era, it might have been termed a "Great
Leap Forward".

Great work.

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From pigood at verizon.net  Thu Apr 22 23:52:44 2004
From: pigood at verizon.net (Phillip Good)
Date: Thu, 22 Apr 2004 14:52:44 -0700
Subject: [R] Help with debugging
References: <Pine.LNX.4.44.0404221911190.32448-100000@gannet.stats>
Message-ID: <002701c428b4$2507a9a0$b6e10804@dslverizon.net>

Once the IF triggers in the following program, it just won't turn off:

#Get p-value from permutation distribution
N = 64
cnt = 0
for (i in 1:N){
 pdata = sample (data)
 statp=F1(size,pdata,gmean,samps)
 if (stat0 <= statp ){
          cnt=cnt+1
          print(i)
          print (statb)
           }
 }
cnt/N   
#print statements are soley for debugging purposes


#Here are the function code and test data needed to run the program:
F1=function(size,data, gmean, samps){
stat=0
start=0
end=0
for (i in 1:samps){
     end=end+size[i]
     term=mean(data[start:end])
     stat= stat +abs(term-gmean)
     start=end+1
     }
list(stat=stat)
}

samps = 4 
size = c(4,4,3,5)
data = rnorm(16, 2*rbinom(16,1, .4))
data = data +c(2,2,2,2, 0,0,0,0, 0,0,0, 0,0,0,0,0)
gmean = mean(data)
stat0=F1(size,data,gmean,samps)



From sundar.dorai-raj at PDF.COM  Fri Apr 23 00:06:53 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 22 Apr 2004 15:06:53 -0700
Subject: [R] Help with debugging
In-Reply-To: <002701c428b4$2507a9a0$b6e10804@dslverizon.net>
References: <Pine.LNX.4.44.0404221911190.32448-100000@gannet.stats>
	<002701c428b4$2507a9a0$b6e10804@dslverizon.net>
Message-ID: <408841FD.3000406@pdf.com>



Phillip Good wrote:

> Once the IF triggers in the following program, it just won't turn off:
> 
> #Get p-value from permutation distribution
> N = 64
> cnt = 0
> for (i in 1:N){
>  pdata = sample (data)
>  statp=F1(size,pdata,gmean,samps)
>  if (stat0 <= statp ){
>           cnt=cnt+1
>           print(i)
>           print (statb)
>            }
>  }
> cnt/N   
> #print statements are soley for debugging purposes
> 
> 
> #Here are the function code and test data needed to run the program:
> F1=function(size,data, gmean, samps){
> stat=0
> start=0
> end=0
> for (i in 1:samps){
>      end=end+size[i]
>      term=mean(data[start:end])
>      stat= stat +abs(term-gmean)
>      start=end+1
>      }
> list(stat=stat)
> }
> 
> samps = 4 
> size = c(4,4,3,5)
> data = rnorm(16, 2*rbinom(16,1, .4))
> data = data +c(2,2,2,2, 0,0,0,0, 0,0,0, 0,0,0,0,0)
> gmean = mean(data)
> stat0=F1(size,data,gmean,samps)
> 

stat0 is a list, so the comparison in the "if" statement should be

if(stat0$stat <= statp$stat)

Also, you should be more clear about the statement, "it just won't turn 
off...". I have no idea what that means.

--sundar

N = 64
cnt = 0
for (i in 1:N){
  pdata = sample (data)
  statp=F1(size,pdata,gmean,samps)
  if (stat0$stat <= statp$stat ){
           cnt=cnt+1
           print(i)
           print (statp) # you had "statb", which I'm assuming was a typo
            }
  }
cnt/N



From p.dalgaard at biostat.ku.dk  Fri Apr 23 00:20:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Apr 2004 00:20:17 +0200
Subject: [R] Help with debugging
In-Reply-To: <002701c428b4$2507a9a0$b6e10804@dslverizon.net>
References: <Pine.LNX.4.44.0404221911190.32448-100000@gannet.stats>
	<002701c428b4$2507a9a0$b6e10804@dslverizon.net>
Message-ID: <x2zn93wt7i.fsf@biostat.ku.dk>

"Phillip Good" <pigood at verizon.net> writes:

> Once the IF triggers in the following program, it just won't turn off:
>

>  statp=F1(size,pdata,gmean,samps)
>  if (stat0 <= statp ){

There are 2 bugs here: One in your code and one in R.

F1 returns a list, so the logical thing to do would be to use

  stat0$stat <= statp$stat

The bug in R is that it appears to be random what happens if you do
compare lists:

> replicate(50,list(1) <= list(2))
 [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
[13] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
[25] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[49] FALSE FALSE

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sbartel at sph.emory.edu  Fri Apr 23 00:57:26 2004
From: sbartel at sph.emory.edu (Scott Bartell)
Date: Thu, 22 Apr 2004 18:57:26 -0400
Subject: [R] slower execution in R 1.9.0
Message-ID: <1082674645.8558.1006.camel@sbartel1>

I have an R function (about 1000 lines long) that takes more than 20
times as long to run under R Windows 1.9.0 and 1.8.1 than it does under
1.7.1.  Profile results indicate that the $<-.data.frame operation is
the culprit, but I don't understand exactly what that is (assignment of
data frame elements to another variable?), or why it's only a problem
under 1.8.1 and 1.9.0.  Any advice?   


top 5 operations under 1.9.0
               self.time self.pct total.time total.pct
$<-.data.frame    110.64     93.3     110.68      93.3
FMD                 5.38      4.5     118.60     100.0
DIRinf              0.54      0.5      11.82      10.0
INDinf              0.46      0.4       9.78       8.2
pweibull            0.38      0.3       0.38       0.3
  
top 5 operations under 1.7.1
                       self.time self.pct total.time total.pct
pweibull                    2.62     55.7       2.62      55.7
^                           0.76     16.2       0.76      16.2
CalcDistance                0.16      3.4       1.26      26.8
FMD                         0.16      3.4       4.70     100.0
matrix                      0.14      3.0       0.14       3.0


- Scott Bartell



From rpeng at jhsph.edu  Fri Apr 23 01:04:16 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 22 Apr 2004 19:04:16 -0400
Subject: [R] Question on outer(x, y, FUN,...)
In-Reply-To: <78B50CF247E5D04B8A5E02D001CC8E9A595C4E@XMAIL.sooner.net.ou.edu>
References: <78B50CF247E5D04B8A5E02D001CC8E9A595C4E@XMAIL.sooner.net.ou.edu>
Message-ID: <40884F70.2020307@jhsph.edu>

Your function is probably not vectorized.  See the R FAQ question 
  7.19.

-roger

Yao, Minghua wrote:
> Dear R gurus,
>  
> When I used outer(x, y, FUN, ...), I got error message when there was matrix multiplications with x or y in FUN. FUN worked fine when x and y were applied to it in FUN(x, y, ...). Is this a bug for outer( )?
>  
> -MY
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From d.scott at auckland.ac.nz  Fri Apr 23 01:41:45 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 23 Apr 2004 11:41:45 +1200 (NZST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <Pine.LNX.4.44.0404211005170.9702-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404231100390.726-100000@hydra.stat.auckland.ac.nz>


A quick question: has anybody encountered this error working on a desktop?

Users here who have had problems are working on laptops, and laptops have 
been mentioned in other posts. Brian said he was unable to reproduce the 
problem.

David Scott


-- 
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From k.wang at auckland.ac.nz  Fri Apr 23 01:49:15 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 23 Apr 2004 11:49:15 +1200
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
References: <Pine.LNX.4.44.0404231100390.726-100000@hydra.stat.auckland.ac.nz>
Message-ID: <00a701c428c4$6b897c90$6633d882@stat.auckland.ac.nz>

I heard of this problem mentioned by one staff member here, but he suspected
it was to do with Windows XP's patch.  When the technician told us about
getting the patches for Windows, Rgui started to crash.  But then a few days
later a new patch by Microsoft was released and after installing the new
patch, the problem was solved.  As a result, the staff member suspected it's
the problem with Microsoft but now it may have been solved.

Kevin

----- Original Message ----- 
From: "David Scott" <d.scott at auckland.ac.nz>
To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, April 23, 2004 11:41 AM
Subject: Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE


>
> A quick question: has anybody encountered this error working on a desktop?
>
> Users here who have had problems are working on laptops, and laptops have
> been mentioned in other posts. Brian said he was unable to reproduce the
> problem.
>
> David Scott
>
>
> -- 
> _________________________________________________________________
> David Scott Department of Statistics, Tamaki Campus
> The University of Auckland, PB 92019
> Auckland NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830 Fax: +64 9 373 7000
> Email: d.scott at auckland.ac.nz
>
>
> Graduate Officer, Department of Statistics
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From dvanbrunt at well-wired.com  Fri Apr 23 03:28:21 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Thu, 22 Apr 2004 20:28:21 -0500
Subject: [R] Extracting the MSE and % Variance from RandomForest
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7B5F@usrymx25.merck.com>
Message-ID: <BCADDB65.810A%dvanbrunt@well-wired.com>

I'm almost embarrassed to ask... Almost!

I've typed "names(myforest.rf)" to see that lots of interesting info can be
called out bit by bit...

But I can't seem to figure out how to just grab one or two the summary stats
so I can save it out into a table alongside the prediction for the new data.

Specifically, though I see "rsq" for all the trees, and ditto for "mse",
(here's the embarrassing part) how do I just save the mse or % Var for the
whole forest? I really can't find it, not for lack of looking. Just clearly
not in the right place!

Or do I need to calculate it somehow?



From andy_liaw at merck.com  Fri Apr 23 03:46:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 22 Apr 2004 21:46:05 -0400
Subject: [R] Extracting the MSE and % Variance from RandomForest
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C60@usrymx25.merck.com>

Several ways:

1. Read ?randomForest, especially the `Value' section.
2. Look at str(myforest.rf).
3. Look at print.randomForest.

If the forest has 100 trees, then the mse and rsq are vectors with 100
elements each, the i-th element being the mse (or rsq) of the forest
consisting of the first i trees.  So the last element is the mse (or rsq) of
the whole forest.

HTH,
Andy

> From: David L. Van Brunt, Ph.D.
> 
> I'm almost embarrassed to ask... Almost!
> 
> I've typed "names(myforest.rf)" to see that lots of 
> interesting info can be
> called out bit by bit...
> 
> But I can't seem to figure out how to just grab one or two 
> the summary stats
> so I can save it out into a table alongside the prediction 
> for the new data.
> 
> Specifically, though I see "rsq" for all the trees, and ditto 
> for "mse",
> (here's the embarrassing part) how do I just save the mse or 
> % Var for the
> whole forest? I really can't find it, not for lack of 
> looking. Just clearly
> not in the right place!
> 
> Or do I need to calculate it somehow?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Michael_Bibo at health.qld.gov.au  Fri Apr 23 03:53:00 2004
From: Michael_Bibo at health.qld.gov.au (Michael Bibo)
Date: Fri, 23 Apr 2004 11:53:00 +1000
Subject: [R] Re:Importing SPSS Data Entry data files
Message-ID: <s089039d.093@health-es2.health.qld.gov.au>

On 16 Apr, Unung Istopo Hartanto asked about using R library 'foreign' to import SPSS Data Entry data files.  SPSS Data Entry is an application that allows for the creation of formatted data entry forms (as does EpiInfo, Epidata and CSPro (all free downloads)).  As such it creates the usual SPSS data files, but adds forms information, which the 'foreign' package does not know how to handle.
 
The solution is relatively simple: SPSS Data Entry Builder itself allows for the exporting of the data file without the forms information (File -> Export -> Data).  This gives you a 'standard' SPSS data file, which you can import into R using the 'foreign' library, or through R Commander.

Michael Bibo
Research Officer
Alcohol, Tobacco and Other Drugs Service
Resource Unit
West Moreton Community Health Services
West Moreton Health Service District
Queensland Health

Michael_Bibo at health.qld.gov.au

Ph. +61 7 3817 2474

P.O. Box 878
Ipswich,  4305
Australia



***********************************************************************************
This email, including any attachments sent with it, is confidential and for the sole use of the intended recipient(s).  This confidentiality is not waived or lost, if you receive it and you are not the intended recipient(s), or if it is transmitted/received in error.

Any unauthorised use, alteration, disclosure, distribution or review of this email is prohibited.  It may be subject to a statutory duty of confidentiality if it relates to health service matters.

If you are not the intended recipient(s), or if you have received this email in error, you are asked to immediately notify the sender by telephone or by return email.  You should also delete this email and destroy any hard copies produced.



From ok at cs.otago.ac.nz  Fri Apr 23 04:24:12 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 23 Apr 2004 14:24:12 +1200 (NZST)
Subject: [R] resetting the interpreter
Message-ID: <200404230224.i3N2OCIF398237@atlas.otago.ac.nz>

Gabor Grothendieck <ggrothendieck at myway.com> wrote:
	If you haven't tried 1.9.0 yet it does start up faster than 1.8.1.
	
For what it's worth, on my SunBlade 100,
R 1.7.1 started in 4 seconds of CPU time, 25 seconds of wall-clock time
R 1.9.0 starts  in 3 seconds of CPU time, 20 seconds of wall-clock time.

I measured the time by doing
	% time R
	> quit("no")

It's really good to see *any* improvement in startup time, and I must
thank the R developers for even *trying* to do anything about it.  There's
some way to go yet; octave starts in under a second.



From dvanbrunt at well-wired.com  Fri Apr 23 04:30:25 2004
From: dvanbrunt at well-wired.com (David L. Van Brunt, Ph.D.)
Date: Thu, 22 Apr 2004 21:30:25 -0500
Subject: [R] Extracting the MSE and % Variance from RandomForest
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7C60@usrymx25.merck.com>
Message-ID: <BCADE9F1.8117%dvanbrunt@well-wired.com>

AH!

That's the piece. I didn't realize it was a sort of running total. That
makes sense. 

On 4/22/04 20:46, "Liaw, Andy" <andy_liaw at merck.com> wrote:

> consisting of the first i trees.  So the last element is the mse (or rsq) of
> the whole forest.

-- 
David L. Van Brunt, Ph.D.
Outlier Consulting & Development
mailto: <ocd at well-wired.com>



From Michael_Bibo at health.qld.gov.au  Fri Apr 23 04:32:43 2004
From: Michael_Bibo at health.qld.gov.au (Michael Bibo)
Date: Fri, 23 Apr 2004 12:32:43 +1000
Subject: [R] Barplot and R 1.9.0
Message-ID: <s0890cff.026@health-es2.health.qld.gov.au>

I would like to preface this by saying that I have not been using R for long, and consider myself a beginner.  But the more I use and learn R, the more impressed I am with both the 'product' itself, and the efforts of those developing it.

It has been noted (eg Duncan Murdoch, 15 April), that the barplot function in R 1.9.0, when used on a tabulated single factor, produces a plot only one bar wide, and seems to superimpose the other bars.

It may be useful for users to note that the barplot2 function from the 'gregmisc' package still works as barplot did in R 1.8.1.

Consider:

library(gregmisc)
x<-rep(1:3,20)
barplot(table(x))
barplot2(table(x))

for an example.


Michael Bibo
Queensland Health
michael_bibo at health.qld.gov.au



***********************************************************************************
This email, including any attachments sent with it, is confidential and for the sole use of the intended recipient(s).  This confidentiality is not waived or lost, if you receive it and you are not the intended recipient(s), or if it is transmitted/received in error.

Any unauthorised use, alteration, disclosure, distribution or review of this email is prohibited.  It may be subject to a statutory duty of confidentiality if it relates to health service matters.

If you are not the intended recipient(s), or if you have received this email in error, you are asked to immediately notify the sender by telephone or by return email.  You should also delete this email and destroy any hard copies produced.



From ajayshah at mayin.org  Fri Apr 23 07:21:11 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Fri, 23 Apr 2004 10:51:11 +0530
Subject: [R] Re: Evidence from Debian's package tracking
In-Reply-To: <85smewghhz.fsf@servant.blindglobe.net>
References: <20040422053440.GA27799@igidr.ac.in>
	<85smewghhz.fsf@servant.blindglobe.net>
Message-ID: <20040423052111.GN732@igidr.ac.in>

> > This shows that of the 4800 people who volunteered information, 1631
> > had installed gnuplot -- which suggests that perhaps one third of
> > Debian installs are by numerate people. R-base was installed by
> > roughly one-tenth of the sample.
> >
> > So that's one useful fact: Roughly one in ten of Debian users is an R
> > user. Roughly one in three of the numerate users is an R user.
> 
> The dangers with generalization... I don't have gnuplot on most of my
> machines. 

:-) Okay, okay, how about "atmost 33% of the numerate users are R users".

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From bamelbourne at ucdavis.edu  Fri Apr 23 08:19:06 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Thu, 22 Apr 2004 23:19:06 -0700
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
References: <n2cd80tc8a6ontje1ovo7hsnv5jsf2ecr5@4ax.com><Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>
	<1cmd80pvpmhlrkhll66lm6qmegr0c7h6or@4ax.com>
Message-ID: <008f01c428fa$e76ad5d0$2313eda9@des.ucdavis.edu>

Some more diagnostic information on this problem related to Windows
critical updates.

I am running R 1.8.1 and 1.9.0 under Windows XP Pro, on a laptop, which is
networked at work but frequently used off the network. R starts properly
sometimes, when attached to the network, but never when disconnected and
logged on to the same (networked) user profile. R does however, always start
properly on a non-network profile local to my machine.

I have all the critical Windows XP updates installed, so later updates did
not fix the problem.

On failing to start 1.8.1, entering debug gives the message "Unhandled
exception in Rgui.exe (MSVCRT.DLL):0xC0000005:Access Violation"

The following observations apply to both 1.8.1 and 1.9.0:

Setting HOMEDRIVE and HOMEPATH does not fix the problem (see SET dump
appended to end of message).
Rebooting in safe mode (with networking) does not fix the problem.
Rebooting in safe mode (without networking) does fix the problem but in that
case I am running on a non-network profile local to the machine, which
always works anyway. So the problem appears to be related to networking.

I can confirm that the offending update is KB835732. Incidentally, this
uninstalls with the message "Setup detected programs on your computer: R for
Windows 1.8.1. If KB835732 is removed, these programs might not run
properly". So, the update appears to be aware of an interaction with R
1.8.1.

Both R 1.9.0 and R 1.8.1 run properly after uninstalling the offending
update.

I will install r-patched tomorrow at work and let you know what happens.

cheers
Brett

Brett Melbourne, Postdoctoral Fellow
Center for Population Biology
University of California Davis CA 95616


SET dump: environment when R fails: HOMEDRIVE and HOMEPATH set explicitly.

C:\WINDOWS\system32>SET
ALLUSERSPROFILE=C:\Documents and Settings\All Users
APPDATA=C:\Documents and Settings\melbourne\Application Data
CommonProgramFiles=C:\Program Files\Common Files
COMPUTERNAME=CAVIAR
ComSpec=C:\WINDOWS\system32\cmd.exe
HOMEDRIVE=C:
HOMEPATH = C:
HOMESHARE=\\desnet\melbourne
include=C:\Program Files\Microsoft Visual Studio\VC98\atl\include;C:\Program
Fil
es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
Visual St
udio\VC98\include
lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
Files\Micro
soft Visual Studio\VC98\lib
LOGONSERVER=\\CAYENNE
MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
NUMBER_OF_PROCESSORS=1
OS=Windows_NT
Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
FILES\TH
INKPAD\UTILITIES;C:\Program
Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
Program Files\ATI Technologies\ATI Control Panel;C:\Program Files\Microsoft
Visu
al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
Studio\Common\MSD
ev98\Bin;C:\Program Files\Microsoft Visual Studio\Common\Tools;C:\Program
Files\
Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
PROCESSOR_ARCHITECTURE=x86
PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
PROCESSOR_LEVEL=15
PROCESSOR_REVISION=0204
ProgramFiles=C:\Program Files
PROMPT=$P$G
SESSIONNAME=Console
SystemDrive=C:
SystemRoot=C:\WINDOWS
TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
USERDNSDOMAIN=DES.UCDAVIS.EDU
USERDOMAIN=DES
USERNAME=melbourne
USERPROFILE=C:\Documents and Settings\melbourne
windir=C:\WINDOWS



From ligges at statistik.uni-dortmund.de  Fri Apr 23 08:44:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Apr 2004 08:44:34 +0200
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <008f01c428fa$e76ad5d0$2313eda9@des.ucdavis.edu>
References: <n2cd80tc8a6ontje1ovo7hsnv5jsf2ecr5@4ax.com><Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>	<1cmd80pvpmhlrkhll66lm6qmegr0c7h6or@4ax.com>
	<008f01c428fa$e76ad5d0$2313eda9@des.ucdavis.edu>
Message-ID: <4088BB52.6000107@statistik.uni-dortmund.de>

Brett Melbourne wrote:
> Some more diagnostic information on this problem related to Windows
> critical updates.
> 
> I am running R 1.8.1 and 1.9.0 under Windows XP Pro, on a laptop, which is
> networked at work but frequently used off the network. R starts properly
> sometimes, when attached to the network, but never when disconnected and
> logged on to the same (networked) user profile. R does however, always start
> properly on a non-network profile local to my machine.
> 
> I have all the critical Windows XP updates installed, so later updates did
> not fix the problem.
> 
> On failing to start 1.8.1, entering debug gives the message "Unhandled
> exception in Rgui.exe (MSVCRT.DLL):0xC0000005:Access Violation"
> 
> The following observations apply to both 1.8.1 and 1.9.0:
> 
> Setting HOMEDRIVE and HOMEPATH does not fix the problem (see SET dump
> appended to end of message).
> Rebooting in safe mode (with networking) does not fix the problem.
> Rebooting in safe mode (without networking) does fix the problem but in that
> case I am running on a non-network profile local to the machine, which
> always works anyway. So the problem appears to be related to networking.
> 
> I can confirm that the offending update is KB835732. Incidentally, this
> uninstalls with the message "Setup detected programs on your computer: R for
> Windows 1.8.1. If KB835732 is removed, these programs might not run
> properly". So, the update appears to be aware of an interaction with R
> 1.8.1.
> 
> Both R 1.9.0 and R 1.8.1 run properly after uninstalling the offending
> update.
> 
> I will install r-patched tomorrow at work and let you know what happens.
> 
> cheers
> Brett
> 
> Brett Melbourne, Postdoctoral Fellow
> Center for Population Biology
> University of California Davis CA 95616
> 
> 
> SET dump: environment when R fails: HOMEDRIVE and HOMEPATH set explicitly.
> 
> C:\WINDOWS\system32>SET
> ALLUSERSPROFILE=C:\Documents and Settings\All Users
> APPDATA=C:\Documents and Settings\melbourne\Application Data
> CommonProgramFiles=C:\Program Files\Common Files
> COMPUTERNAME=CAVIAR
> ComSpec=C:\WINDOWS\system32\cmd.exe
> HOMEDRIVE=C:
> HOMEPATH = C:


That's wrong. Please set it to a sensible HOMEPATH, e.g. simply
SET HOMEPATH=\
or
SET HOMEPATH=\Documents and Settings\melbourne

More interesting: What does the log say when you have not set HOMEPATH 
before???

Uwe Ligges




> HOMESHARE=\\desnet\melbourne
> include=C:\Program Files\Microsoft Visual Studio\VC98\atl\include;C:\Program
> Fil
> es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
> Visual St
> udio\VC98\include
> lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
> Files\Micro
> soft Visual Studio\VC98\lib
> LOGONSERVER=\\CAYENNE
> MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
> NUMBER_OF_PROCESSORS=1
> OS=Windows_NT
> Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
> FILES\TH
> INKPAD\UTILITIES;C:\Program
> Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
> Program Files\ATI Technologies\ATI Control Panel;C:\Program Files\Microsoft
> Visu
> al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
> Studio\Common\MSD
> ev98\Bin;C:\Program Files\Microsoft Visual Studio\Common\Tools;C:\Program
> Files\
> Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
> PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> PROCESSOR_ARCHITECTURE=x86
> PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
> PROCESSOR_LEVEL=15
> PROCESSOR_REVISION=0204
> ProgramFiles=C:\Program Files
> PROMPT=$P$G
> SESSIONNAME=Console
> SystemDrive=C:
> SystemRoot=C:\WINDOWS
> TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> USERDNSDOMAIN=DES.UCDAVIS.EDU
> USERDOMAIN=DES
> USERNAME=melbourne
> USERPROFILE=C:\Documents and Settings\melbourne
> windir=C:\WINDOWS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bamelbourne at ucdavis.edu  Fri Apr 23 09:13:00 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Fri, 23 Apr 2004 00:13:00 -0700
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
References: <n2cd80tc8a6ontje1ovo7hsnv5jsf2ecr5@4ax.com><Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>	<1cmd80pvpmhlrkhll66lm6qmegr0c7h6or@4ax.com>
	<008f01c428fa$e76ad5d0$2313eda9@des.ucdavis.edu>
	<4088BB52.6000107@statistik.uni-dortmund.de>
Message-ID: <000701c42902$6aee6be0$0a13eda9@des.ucdavis.edu>

Hi Uwe,

This is what the log says when neither homedrive or homepath are set
manually. This is after R has failed to start (I don't know if that's
important). Hope that helps. Let me know if there's any other info I can
gather.

Brett


C:\WINDOWS\system32>set
ALLUSERSPROFILE=C:\Documents and Settings\All Users
APPDATA=C:\Documents and Settings\melbourne\Application Data
CommonProgramFiles=C:\Program Files\Common Files
COMPUTERNAME=CAVIAR
ComSpec=C:\WINDOWS\system32\cmd.exe
HOMEDRIVE=C:
HOMESHARE=\\desnet\melbourne
include=C:\Program Files\Microsoft Visual Studio\VC98\atl\include;C:\Program
Fil
es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
Visual St
udio\VC98\include
lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
Files\Micro
soft Visual Studio\VC98\lib
LOGONSERVER=\\CAYENNE
MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
NUMBER_OF_PROCESSORS=1
OS=Windows_NT
Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
FILES\TH
INKPAD\UTILITIES;C:\Program
Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
Program Files\ATI Technologies\ATI Control Panel;C:\Program Files\Microsoft
Visu
al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
Studio\Common\MSD
ev98\Bin;C:\Program Files\Microsoft Visual Studio\Common\Tools;C:\Program
Files\
Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
PROCESSOR_ARCHITECTURE=x86
PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
PROCESSOR_LEVEL=15
PROCESSOR_REVISION=0204
ProgramFiles=C:\Program Files
PROMPT=$P$G
SESSIONNAME=Console
SystemDrive=C:
SystemRoot=C:\WINDOWS
TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
USERDNSDOMAIN=DES.UCDAVIS.EDU
USERDOMAIN=DES
USERNAME=melbourne
USERPROFILE=C:\Documents and Settings\melbourne
windir=C:\WINDOWS




----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Brett Melbourne" <bamelbourne at ucdavis.edu>
Cc: "Duncan Murdoch" <dmurdoch at pair.com>; "Prof Brian Ripley"
<ripley at stats.ox.ac.uk>; "David Scott" <d.scott at auckland.ac.nz>;
<r-help at stat.math.ethz.ch>
Sent: Thursday, April 22, 2004 11:44 PM
Subject: Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE


> Brett Melbourne wrote:
> > Some more diagnostic information on this problem related to Windows
> > critical updates.
> >
> > I am running R 1.8.1 and 1.9.0 under Windows XP Pro, on a laptop, which
is
> > networked at work but frequently used off the network. R starts properly
> > sometimes, when attached to the network, but never when disconnected and
> > logged on to the same (networked) user profile. R does however, always
start
> > properly on a non-network profile local to my machine.
> >
> > I have all the critical Windows XP updates installed, so later updates
did
> > not fix the problem.
> >
> > On failing to start 1.8.1, entering debug gives the message "Unhandled
> > exception in Rgui.exe (MSVCRT.DLL):0xC0000005:Access Violation"
> >
> > The following observations apply to both 1.8.1 and 1.9.0:
> >
> > Setting HOMEDRIVE and HOMEPATH does not fix the problem (see SET dump
> > appended to end of message).
> > Rebooting in safe mode (with networking) does not fix the problem.
> > Rebooting in safe mode (without networking) does fix the problem but in
that
> > case I am running on a non-network profile local to the machine, which
> > always works anyway. So the problem appears to be related to networking.
> >
> > I can confirm that the offending update is KB835732. Incidentally, this
> > uninstalls with the message "Setup detected programs on your computer: R
for
> > Windows 1.8.1. If KB835732 is removed, these programs might not run
> > properly". So, the update appears to be aware of an interaction with R
> > 1.8.1.
> >
> > Both R 1.9.0 and R 1.8.1 run properly after uninstalling the offending
> > update.
> >
> > I will install r-patched tomorrow at work and let you know what happens.
> >
> > cheers
> > Brett
> >
> > Brett Melbourne, Postdoctoral Fellow
> > Center for Population Biology
> > University of California Davis CA 95616
> >
> >
> > SET dump: environment when R fails: HOMEDRIVE and HOMEPATH set
explicitly.
> >
> > C:\WINDOWS\system32>SET
> > ALLUSERSPROFILE=C:\Documents and Settings\All Users
> > APPDATA=C:\Documents and Settings\melbourne\Application Data
> > CommonProgramFiles=C:\Program Files\Common Files
> > COMPUTERNAME=CAVIAR
> > ComSpec=C:\WINDOWS\system32\cmd.exe
> > HOMEDRIVE=C:
> > HOMEPATH = C:
>
>
> That's wrong. Please set it to a sensible HOMEPATH, e.g. simply
> SET HOMEPATH=\
> or
> SET HOMEPATH=\Documents and Settings\melbourne
>
> More interesting: What does the log say when you have not set HOMEPATH
> before???
>
> Uwe Ligges
>
>
>
>
> > HOMESHARE=\\desnet\melbourne
> > include=C:\Program Files\Microsoft Visual
Studio\VC98\atl\include;C:\Program
> > Fil
> > es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
> > Visual St
> > udio\VC98\include
> > lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
> > Files\Micro
> > soft Visual Studio\VC98\lib
> > LOGONSERVER=\\CAYENNE
> > MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
> > NUMBER_OF_PROCESSORS=1
> > OS=Windows_NT
> > Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
> > FILES\TH
> > INKPAD\UTILITIES;C:\Program
> > Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
> > Program Files\ATI Technologies\ATI Control Panel;C:\Program
Files\Microsoft
> > Visu
> > al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
> > Studio\Common\MSD
> > ev98\Bin;C:\Program Files\Microsoft Visual
Studio\Common\Tools;C:\Program
> > Files\
> > Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
> > PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> > PROCESSOR_ARCHITECTURE=x86
> > PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
> > PROCESSOR_LEVEL=15
> > PROCESSOR_REVISION=0204
> > ProgramFiles=C:\Program Files
> > PROMPT=$P$G
> > SESSIONNAME=Console
> > SystemDrive=C:
> > SystemRoot=C:\WINDOWS
> > TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> > TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> > USERDNSDOMAIN=DES.UCDAVIS.EDU
> > USERDOMAIN=DES
> > USERNAME=melbourne
> > USERPROFILE=C:\Documents and Settings\melbourne
> > windir=C:\WINDOWS
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Apr 23 09:37:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 08:37:59 +0100 (BST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <000701c42902$6aee6be0$0a13eda9@des.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0404230823340.1166-100000@gannet.stats>

That's as expected: HOMEPATH is not set and so HOMEDRIVE is invalid (it 
needs to be paired with a valid HOMEPATH).  The message in R-patched is

Fatal error: Invalid HOMEDRIVE+HOMEPATH

With HOMEDRIVE set but HOMEPATH unset, 1.8.1 does segfault:  
Windows NT/2000/XP is supposed to have both HOMEDRIVE and HOMEPATH set.
Perhaps some recent `hotfix' has managed to unset it on some machines?
(That code had been unchanged for several years until I decided to add 
protection against invalidly long settings for 1.9.0.)

What happens if you set HOME (to a valid path)?  (My original suggestion.)


On Fri, 23 Apr 2004, Brett Melbourne wrote:

> Hi Uwe,
> 
> This is what the log says when neither homedrive or homepath are set
> manually. This is after R has failed to start (I don't know if that's
> important). Hope that helps. Let me know if there's any other info I can
> gather.
> 
> Brett
> 
> 
> C:\WINDOWS\system32>set
> ALLUSERSPROFILE=C:\Documents and Settings\All Users
> APPDATA=C:\Documents and Settings\melbourne\Application Data
> CommonProgramFiles=C:\Program Files\Common Files
> COMPUTERNAME=CAVIAR
> ComSpec=C:\WINDOWS\system32\cmd.exe
> HOMEDRIVE=C:
> HOMESHARE=\\desnet\melbourne
> include=C:\Program Files\Microsoft Visual Studio\VC98\atl\include;C:\Program
> Fil
> es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
> Visual St
> udio\VC98\include
> lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
> Files\Micro
> soft Visual Studio\VC98\lib
> LOGONSERVER=\\CAYENNE
> MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
> NUMBER_OF_PROCESSORS=1
> OS=Windows_NT
> Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
> FILES\TH
> INKPAD\UTILITIES;C:\Program
> Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
> Program Files\ATI Technologies\ATI Control Panel;C:\Program Files\Microsoft
> Visu
> al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
> Studio\Common\MSD
> ev98\Bin;C:\Program Files\Microsoft Visual Studio\Common\Tools;C:\Program
> Files\
> Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
> PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> PROCESSOR_ARCHITECTURE=x86
> PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
> PROCESSOR_LEVEL=15
> PROCESSOR_REVISION=0204
> ProgramFiles=C:\Program Files
> PROMPT=$P$G
> SESSIONNAME=Console
> SystemDrive=C:
> SystemRoot=C:\WINDOWS
> TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> USERDNSDOMAIN=DES.UCDAVIS.EDU
> USERDOMAIN=DES
> USERNAME=melbourne
> USERPROFILE=C:\Documents and Settings\melbourne
> windir=C:\WINDOWS
> 
> 
> 
> 
> ----- Original Message ----- 
> From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> To: "Brett Melbourne" <bamelbourne at ucdavis.edu>
> Cc: "Duncan Murdoch" <dmurdoch at pair.com>; "Prof Brian Ripley"
> <ripley at stats.ox.ac.uk>; "David Scott" <d.scott at auckland.ac.nz>;
> <r-help at stat.math.ethz.ch>
> Sent: Thursday, April 22, 2004 11:44 PM
> Subject: Re: [R] Error with 1.9.0 - Invalid HOMEDRIVE
> 
> 
> > Brett Melbourne wrote:
> > > Some more diagnostic information on this problem related to Windows
> > > critical updates.
> > >
> > > I am running R 1.8.1 and 1.9.0 under Windows XP Pro, on a laptop, which
> is
> > > networked at work but frequently used off the network. R starts properly
> > > sometimes, when attached to the network, but never when disconnected and
> > > logged on to the same (networked) user profile. R does however, always
> start
> > > properly on a non-network profile local to my machine.
> > >
> > > I have all the critical Windows XP updates installed, so later updates
> did
> > > not fix the problem.
> > >
> > > On failing to start 1.8.1, entering debug gives the message "Unhandled
> > > exception in Rgui.exe (MSVCRT.DLL):0xC0000005:Access Violation"
> > >
> > > The following observations apply to both 1.8.1 and 1.9.0:
> > >
> > > Setting HOMEDRIVE and HOMEPATH does not fix the problem (see SET dump
> > > appended to end of message).
> > > Rebooting in safe mode (with networking) does not fix the problem.
> > > Rebooting in safe mode (without networking) does fix the problem but in
> that
> > > case I am running on a non-network profile local to the machine, which
> > > always works anyway. So the problem appears to be related to networking.
> > >
> > > I can confirm that the offending update is KB835732. Incidentally, this
> > > uninstalls with the message "Setup detected programs on your computer: R
> for
> > > Windows 1.8.1. If KB835732 is removed, these programs might not run
> > > properly". So, the update appears to be aware of an interaction with R
> > > 1.8.1.
> > >
> > > Both R 1.9.0 and R 1.8.1 run properly after uninstalling the offending
> > > update.
> > >
> > > I will install r-patched tomorrow at work and let you know what happens.
> > >
> > > cheers
> > > Brett
> > >
> > > Brett Melbourne, Postdoctoral Fellow
> > > Center for Population Biology
> > > University of California Davis CA 95616
> > >
> > >
> > > SET dump: environment when R fails: HOMEDRIVE and HOMEPATH set
> explicitly.
> > >
> > > C:\WINDOWS\system32>SET
> > > ALLUSERSPROFILE=C:\Documents and Settings\All Users
> > > APPDATA=C:\Documents and Settings\melbourne\Application Data
> > > CommonProgramFiles=C:\Program Files\Common Files
> > > COMPUTERNAME=CAVIAR
> > > ComSpec=C:\WINDOWS\system32\cmd.exe
> > > HOMEDRIVE=C:
> > > HOMEPATH = C:
> >
> >
> > That's wrong. Please set it to a sensible HOMEPATH, e.g. simply
> > SET HOMEPATH=\
> > or
> > SET HOMEPATH=\Documents and Settings\melbourne
> >
> > More interesting: What does the log say when you have not set HOMEPATH
> > before???
> >
> > Uwe Ligges
> >
> >
> >
> >
> > > HOMESHARE=\\desnet\melbourne
> > > include=C:\Program Files\Microsoft Visual
> Studio\VC98\atl\include;C:\Program
> > > Fil
> > > es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
> > > Visual St
> > > udio\VC98\include
> > > lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
> > > Files\Micro
> > > soft Visual Studio\VC98\lib
> > > LOGONSERVER=\\CAYENNE
> > > MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
> > > NUMBER_OF_PROCESSORS=1
> > > OS=Windows_NT
> > > Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
> > > FILES\TH
> > > INKPAD\UTILITIES;C:\Program
> > > Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
> > > Program Files\ATI Technologies\ATI Control Panel;C:\Program
> Files\Microsoft
> > > Visu
> > > al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
> > > Studio\Common\MSD
> > > ev98\Bin;C:\Program Files\Microsoft Visual
> Studio\Common\Tools;C:\Program
> > > Files\
> > > Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
> > > PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> > > PROCESSOR_ARCHITECTURE=x86
> > > PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
> > > PROCESSOR_LEVEL=15
> > > PROCESSOR_REVISION=0204
> > > ProgramFiles=C:\Program Files
> > > PROMPT=$P$G
> > > SESSIONNAME=Console
> > > SystemDrive=C:
> > > SystemRoot=C:\WINDOWS
> > > TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> > > TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> > > USERDNSDOMAIN=DES.UCDAVIS.EDU
> > > USERDOMAIN=DES
> > > USERNAME=melbourne
> > > USERPROFILE=C:\Documents and Settings\melbourne
> > > windir=C:\WINDOWS
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 23 09:44:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 08:44:41 +0100 (BST)
Subject: [R] slower execution in R 1.9.0
In-Reply-To: <1082674645.8558.1006.camel@sbartel1>
Message-ID: <Pine.LNX.4.44.0404230840090.1166-100000@gannet.stats>

Rather a misleading subject, if it was slower under 1.8.1.

On Thu, 22 Apr 2004, Scott Bartell wrote:

> I have an R function (about 1000 lines long) that takes more than 20
> times as long to run under R Windows 1.9.0 and 1.8.1 than it does under
> 1.7.1.  Profile results indicate that the $<-.data.frame operation is
> the culprit, but I don't understand exactly what that is (assignment of
> data frame elements to another variable?), or why it's only a problem
> under 1.8.1 and 1.9.0.  Any advice?   

Don't use $<- on data frames, that is code like

foo$bar <- bah

for foo a data frame, if you don't want to pay the penalty of error
checking.  Versions prior to R 1.8.0 treated data frames as lists for that
construct, and often ended up with invalid data frames.

Sound like you could use a lists for your working data.

> 
> top 5 operations under 1.9.0
>                self.time self.pct total.time total.pct
> $<-.data.frame    110.64     93.3     110.68      93.3
> FMD                 5.38      4.5     118.60     100.0
> DIRinf              0.54      0.5      11.82      10.0
> INDinf              0.46      0.4       9.78       8.2
> pweibull            0.38      0.3       0.38       0.3
>   
> top 5 operations under 1.7.1
>                        self.time self.pct total.time total.pct
> pweibull                    2.62     55.7       2.62      55.7
> ^                           0.76     16.2       0.76      16.2
> CalcDistance                0.16      3.4       1.26      26.8
> FMD                         0.16      3.4       4.70     100.0
> matrix                      0.14      3.0       0.14       3.0

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 23 10:01:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 09:01:09 +0100 (BST)
Subject: [R] resetting the interpreter
In-Reply-To: <200404230224.i3N2OCIF398237@atlas.otago.ac.nz>
Message-ID: <Pine.LNX.4.44.0404230845340.1166-100000@gannet.stats>

On Fri, 23 Apr 2004, Richard A. O'Keefe wrote:

> Gabor Grothendieck <ggrothendieck at myway.com> wrote:
> 	If you haven't tried 1.9.0 yet it does start up faster than 1.8.1.
> 	
> For what it's worth, on my SunBlade 100,
> R 1.7.1 started in 4 seconds of CPU time, 25 seconds of wall-clock time
> R 1.9.0 starts  in 3 seconds of CPU time, 20 seconds of wall-clock time.
> 
> I measured the time by doing
> 	% time R
> 	> quit("no")

Better to do

	% time R --vanilla < /dev/null

That does seem a rather slow machine: our Sun server manages it in 1.0.

> It's really good to see *any* improvement in startup time, and I must
> thank the R developers for even *trying* to do anything about it.  There's
> some way to go yet; octave starts in under a second.

It is documented in the NEWS file:

    o	There has been a concerted effort to speed up the startup of
	an R session: it now takes about 2/3rds of the time of 1.8.1.

and BTW without those efforts it would have been 1.5x the time of 1.8.1.

If you want a much greater increase, remove `methods' from the list of 
default packages as it is responsible for about half the current startup 
time.  And if you want a really fast startup with minimal facilities, use 
no default packages.

On my office desktop (RH8.0, Athlon MP2600):

time R --vanilla < /dev/null
0.873u 0.035s 0:00.91 98.9%     0+0k 0+0io 1098pf+0w

setenv R_DEFAULT_PACKAGES "utils,graphics,stats"
time R --vanilla < /dev/null
0.505u 0.041s 0:00.55 98.1%     0+0k 0+0io 1090pf+0w

setenv R_DEFAULT_PACKAGES NULL
time R --vanilla < /dev/null
0.099u 0.017s 0:00.11 90.9%     0+0k 0+0io 1033pf+0w

There are planned improvements (hopefully for 2.0.0 next October) which 
will load objects on first use and so about halve the second of these 
times.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bamelbourne at ucdavis.edu  Fri Apr 23 10:17:48 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Fri, 23 Apr 2004 01:17:48 -0700
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
References: <Pine.LNX.4.44.0404230901190.1166-100000@gannet.stats>
Message-ID: <000f01c4290b$7778a1b0$0613eda9@des.ucdavis.edu>

I believe I have the right settings now (SET dump follows), and both 1.9.0
and 1.8.1 still fail to start as before. I have tried also setting
HOMEPATH=\ and HOMEPATH=\Documents and Settings\melbourne.

Let me know if I can gather any other useful info.
Brett


C:\WINDOWS\system32>set
ALLUSERSPROFILE=C:\Documents and Settings\All Users
APPDATA=C:\Documents and Settings\melbourne\Application Data
CommonProgramFiles=C:\Program Files\Common Files
COMPUTERNAME=CAVIAR
ComSpec=C:\WINDOWS\system32\cmd.exe
HOME=C:\
HOMEDRIVE=C:
HOMESHARE=\\desnet\melbourne
include=C:\Program Files\Microsoft Visual Studio\VC98\atl\include;C:\Program
Fil
es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
Visual St
udio\VC98\include
lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
Files\Micro
soft Visual Studio\VC98\lib
LOGONSERVER=\\CAYENNE
MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
NUMBER_OF_PROCESSORS=1
OS=Windows_NT
Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
FILES\TH
INKPAD\UTILITIES;C:\Program
Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
Program Files\ATI Technologies\ATI Control Panel;C:\Program Files\Microsoft
Visu
al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
Studio\Common\MSD
ev98\Bin;C:\Program Files\Microsoft Visual Studio\Common\Tools;C:\Program
Files\
Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
PROCESSOR_ARCHITECTURE=x86
PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
PROCESSOR_LEVEL=15
PROCESSOR_REVISION=0204
ProgramFiles=C:\Program Files
PROMPT=$P$G
SESSIONNAME=Console
SystemDrive=C:
SystemRoot=C:\WINDOWS
TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
USERDNSDOMAIN=DES.UCDAVIS.EDU
USERDOMAIN=DES
USERNAME=melbourne
USERPROFILE=C:\Documents and Settings\melbourne
windir=C:\WINDOWS



From wolfram at fischer-zim.ch  Fri Apr 23 10:15:17 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 23 Apr 2004 10:15:17 +0200
Subject: [R] mosaicplot: color recycling and defaults of xlab/ylab
Message-ID: <20040423081517.GA3243@s1x.local>

________________________________________________________________________
PROBLEMS

(a) colors are not recycled over all fields of a mosaicplot.
(b) default xlab/ylab annotation does not always correspond to ``dir''.

________________________________________________________________________
EXAMPLE

par( mfcol = c( 2, 3 ) )
haircolors <- c( 'black', 'brown', 'red', 'sandybrown' )
eyecolors <- c( 'saddlebrown', 'skyblue', 'sandybrown', 'seagreen2' )

mosaicplot( main='(1)', ~Eye + Hair, color=haircolors, data=HairEyeColor)
mosaicplot( main='(2)', ~Hair + Eye, color=eyecolors, data=HairEyeColor)
mosaicplot( main='(3)', ~Eye + Hair, color=rep( each=4, eyecolors), data=HairEyeColor)
mosaicplot( main='(4)', ~Hair + Eye, color=rep( each=4, haircolors ), data=HairEyeColor )
mosaicplot( main='(5)', ~Eye + Hair, color=haircolors, dir=c('h','v'), data=HairEyeColor )
mosaicplot( main='(6)', ~Eye + Hair, color=haircolors, dir=c('h','v'), data=HairEyeColor, xlab='Hair', ylab='Eye' )

________________________________________________________________________
COMMENTS AND QUESTIONS

(1) and (2) are normal mosaicplots with colors of fields
set in correspondance to the colors of hairs resp. of eyes.

(3) is like (1) but I tried to get ``eyecolors''.
Problem (a); what could I do to get ``eyecolors'' in the fields?

(4) is like (2) but the colors should be ``haircolors''.
Same problem as in (3).

(5) is a mirrored version of (1):
I saw: Mirroring does not change the attribution of colors.
If all fields of each line would have the same color
(e.g. the last line should be "green")
I could change ``colors=eyecolors''
and I would receive the graphic I searched.

Here I encountered Problem (b):
In this case, default xlab should be 'Hair' and ylab should be 'Eye'
(6) I solved that problem provisionally by using the xlab and ylab options.

________________________________________________________________________

Thanks - Wolfram



From ripley at stats.ox.ac.uk  Fri Apr 23 10:23:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 09:23:20 +0100 (BST)
Subject: [R] Help with debugging
In-Reply-To: <x2zn93wt7i.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404230918430.1485-100000@gannet.stats>

On 23 Apr 2004, Peter Dalgaard wrote:

> "Phillip Good" <pigood at verizon.net> writes:
> 
> > Once the IF triggers in the following program, it just won't turn off:
> >
> 
> >  statp=F1(size,pdata,gmean,samps)
> >  if (stat0 <= statp ){
> 
> There are 2 bugs here: One in your code and one in R.
> 
> F1 returns a list, so the logical thing to do would be to use
> 
>   stat0$stat <= statp$stat
> 
> The bug in R is that it appears to be random what happens if you do
> compare lists:
> 
> > replicate(50,list(1) <= list(2))
>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
> [13] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
> [25] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> [49] FALSE FALSE

Yes.  relop.c has

    /* FIXME (?): S does
    if (!isVectorAtomic(x) || !isVectorAtomic(y)) { */
    if (!isVector(x) || !isVector(y)) {
	if (isNull(x) || isNull(y)) {
	    UNPROTECT(2);
	    return allocVector(LGLSXP,0);
	}
	errorcall(call,
		  "comparison (%d) is possible only for atomic types",
		  PRIMVAL(op));
    }

    /* ELSE :  x and y are both atomic */

but that the latter is not true, and integer_relop gets called (with 
nonsense results).  The FIXME is long overdue ....

Not that the (4) in

> list(1) <= list(2)
Error in list(1) <= list(2) : comparison (4) is possible only for atomic 
types

is very revealing.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 23 10:27:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 09:27:35 +0100 (BST)
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <000f01c4290b$7778a1b0$0613eda9@des.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0404230924410.1485-100000@gannet.stats>

To confirm, in 1.9.0 you get a message about HOMEDRIVE when HOME is set?
(That can only happen if HOME is being ignored by getenv.)

On Fri, 23 Apr 2004, Brett Melbourne wrote:

> I believe I have the right settings now (SET dump follows), and both 1.9.0
> and 1.8.1 still fail to start as before. I have tried also setting
> HOMEPATH=\ and HOMEPATH=\Documents and Settings\melbourne.
> 
> Let me know if I can gather any other useful info.
> Brett
> 
> 
> C:\WINDOWS\system32>set
> ALLUSERSPROFILE=C:\Documents and Settings\All Users
> APPDATA=C:\Documents and Settings\melbourne\Application Data
> CommonProgramFiles=C:\Program Files\Common Files
> COMPUTERNAME=CAVIAR
> ComSpec=C:\WINDOWS\system32\cmd.exe
> HOME=C:\
> HOMEDRIVE=C:
> HOMESHARE=\\desnet\melbourne
> include=C:\Program Files\Microsoft Visual Studio\VC98\atl\include;C:\Program
> Fil
> es\Microsoft Visual Studio\VC98\mfc\include;C:\Program Files\Microsoft
> Visual St
> udio\VC98\include
> lib=C:\Program Files\Microsoft Visual Studio\VC98\mfc\lib;C:\Program
> Files\Micro
> soft Visual Studio\VC98\lib
> LOGONSERVER=\\CAYENNE
> MSDevDir=C:\Program Files\Microsoft Visual Studio\Common\MSDev98
> NUMBER_OF_PROCESSORS=1
> OS=Windows_NT
> Path=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\PROGRAM
> FILES\TH
> INKPAD\UTILITIES;C:\Program
> Files\Hummingbird\Connectivity\7.10\Accessories\;C:\
> Program Files\ATI Technologies\ATI Control Panel;C:\Program Files\Microsoft
> Visu
> al Studio\Common\Tools\WinNT;C:\Program Files\Microsoft Visual
> Studio\Common\MSD
> ev98\Bin;C:\Program Files\Microsoft Visual Studio\Common\Tools;C:\Program
> Files\
> Microsoft Visual Studio\VC98\bin;C:\PROGRA~1\INSIGH~1\splus61\cmd
> PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> PROCESSOR_ARCHITECTURE=x86
> PROCESSOR_IDENTIFIER=x86 Family 15 Model 2 Stepping 4, GenuineIntel
> PROCESSOR_LEVEL=15
> PROCESSOR_REVISION=0204
> ProgramFiles=C:\Program Files
> PROMPT=$P$G
> SESSIONNAME=Console
> SystemDrive=C:
> SystemRoot=C:\WINDOWS
> TEMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> TMP=C:\DOCUME~1\MELBOU~1\LOCALS~1\Temp
> USERDNSDOMAIN=DES.UCDAVIS.EDU
> USERDOMAIN=DES
> USERNAME=melbourne
> USERPROFILE=C:\Documents and Settings\melbourne
> windir=C:\WINDOWS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From monica.palaseanu-lovejoy at stud.man.ac.uk  Fri Apr 23 11:45:41 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Fri, 23 Apr 2004 10:45:41 +0100
Subject: [R] GepR - bayesian modelling - please help!
Message-ID: <E1BGxFp-000L7i-Nr@serenity.mcc.ac.uk>

Hi,

I am coming back with a question about GeoR hopping somebody 
can help.

I am doing a bayesian kriging and the prediction coordinates are 
shifted 0.1 m from the data coordinates.

I am using the following command and i am getting the following 
error:

zn.bayes <- krige.bayes(zn.gdata, loc = loc1, model = 
model.control(cov.model = "exponential", lambda = 0), 
prior = prior.control(phi.prior ="exponential", phi = 
89.1948), output=output.control(n.predictive=2, 
mean.var = TRUE, quantile = c(0.025, 0.25, 0.5, 0.75, 
0.975), threshold = c(300)))

Error: Error in cond.sim(env.loc = base.env, env.iter = iter.env, 
loc.coincide = get("loc.coincide",  : 
        chol: matrix not pos def, diag[13]= -3.487549e-019

What i am doing wrong? The location loc1 is a data frame with the 
shifted coord. values. Any suggestion is very well appreciated.

Thanks,

Monica



From KINLEY_ROBERT at Lilly.com  Fri Apr 23 11:52:30 2004
From: KINLEY_ROBERT at Lilly.com (Robert Kinley)
Date: Fri, 23 Apr 2004 10:52:30 +0100
Subject: [R] JOB: Industrial Statistician ,
	Liverpool UK - CLOSING DATE 7th MAY
Message-ID: <OFF7555150.E48621BE-ON80256E7F.00360220-80256E7F.00363CDF@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/d905c82d/attachment.pl

From martinol at ensam.inra.fr  Fri Apr 23 12:12:31 2004
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Fri, 23 Apr 2004 12:12:31 +0200
Subject: [R] tutorial for lme
Message-ID: <4088EC0F.5090906@ensam.inra.fr>

Hi all,

I find an introduction for program lme()  for fitting mixed effects models.
I do not have the book wirtten by Pinheiro and Bates, and a tutorial 
would be very useful.
In particular, I have some difficulties to understand how specifications 
for random effects and variance structures
could be specified.

somebody knows an introduction for that ?
Thanks,
Olivier



From Luisr at frs.fo  Fri Apr 23 12:31:28 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 23 Apr 2004 11:31:28 +0100
Subject: [R] nls
Message-ID: <s088fea1.033@ffdata.setur.fo>

Hi all,
I have define a function 

Statistical_Catch_At_Age <- function(Cya,Mya,Fy,Sa)
{
...... here there are the objects Cya,Mya,Fy,Sa
and the most important of all is the next:

fitting<-nls( formula=Cya ~ (Sa*Fy) / (Sa*Fy + Mya) * Nya * (1-exp( -( Sa*Fy + Mya ))),
start = list( Fy = c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5) , Sa = c(0.2,0.3,0.4,0.5,0.5,0.6,0.6,0.6,0.5) ),
data = list(Cya,Mya,Fy,Sa))
fitting
summary(fitting)
}

The subscripts "y" represent rows and "a" columns
But I get an error message: 

"Error in qr.qty(QR, resid) : qr and y must have the same number of rows"


Help






Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From tobias.verbeke at bivv.be  Fri Apr 23 12:34:11 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Fri, 23 Apr 2004 12:34:11 +0200
Subject: [R] tutorial for lme
In-Reply-To: <4088EC0F.5090906@ensam.inra.fr>
Message-ID: <OF9BD139B7.D6ABCFB4-ONC1256E7F.0039D5A9-C1256E7F.003A1001@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 23/04/2004 12:12:31:

> Hi all,
>
> I find an introduction for program lme()  for fitting mixed effects
models.
> I do not have the book wirtten by Pinheiro and Bates, and a tutorial
> would be very useful.
> In particular, I have some difficulties to understand how specifications
> for random effects and variance structures
> could be specified.
>
> somebody knows an introduction for that ?

http://socserv.mcmaster.ca/jfox/Books/Companion/appendix-mixed-models.pdf

This is part of the web appendix to
"An R and S-PLUS Companion to Applied Regression"
by John Fox, Sage Publications, 2002.


HTH,
Tobias



From VinyardWC at logcom.usmc.mil  Fri Apr 23 13:54:11 2004
From: VinyardWC at logcom.usmc.mil (Vinyard Maj William C)
Date: Fri, 23 Apr 2004 07:54:11 -0400
Subject: [R] Fatal Error: INVALID HOMEDRIVE
Message-ID: <E85DCCA6ED13F749A27DEF593B2297FB0366979F@matcomexch02.matcom.usmc.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/38efc133/attachment.pl

From P.Lemmens at nici.kun.nl  Fri Apr 23 15:06:41 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Fri, 23 Apr 2004 15:06:41 +0200
Subject: [R] Sum Sq of SPSS and R different for repeated measures Anova
Message-ID: <24703390.1082732801@lemmens.socsci.kun.nl>

Dear all,

I'm still learning and transitioning from SPSS to R (1.9.0, winXP) and 
today I have data from two repeated measures experiments. For each of the 
subjects I've averaged for two within-SS factors (2 x 2, 4 means per 
subjects). One experiment had 16 subjects, the other one 25 (between-SS 
factor exp). So I have something like:

avg.cond <- read.table('data.txt') # data set attached as text.
avg.cond[1:5,]
#  pp pictcat cond       rt    exp
#1  1  animal  con 517.8125 exp11b
#2  2  animal  con 425.9375 exp11b
#3  3  animal  con 379.6563 exp11b
#4  4  animal  con 410.6563 exp11b
#5  5  animal  con 420.3125 exp11b

Then I do the anova:
summary(taov <- aov(rt~exp*pictcat*cond+Error(pp/(pictcat*cond)), 
data=subset(avg.cond, cond=='con'|cond=='incon')))
#
#Error: pp
#          Df Sum Sq Mean Sq F value Pr(>F)
#exp        1    178     178  0.0102 0.9202
#Residuals 39 683329   17521
#
#Error: pp:pictcat
#            Df Sum Sq Mean Sq F value   Pr(>F)
#pictcat      1  62382   62382  87.334 1.68e-11 ***
#exp:pictcat  1    653     653   0.914   0.3449
#Residuals   39  27858     714
#---
#Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
#
#Error: pp:cond
#          Df  Sum Sq Mean Sq F value   Pr(>F)
#cond       1  6550.2  6550.2  9.3057 0.004095 **
#exp:cond   1   206.3   206.3  0.2931 0.591313
#Residuals 39 27451.9   703.9
#---
#Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
#
#Error: pp:pictcat:cond
#                 Df  Sum Sq Mean Sq F value Pr(>F)
#pictcat:cond      1   517.4   517.4  1.8747 0.1788
#exp:pictcat:cond  1    67.3    67.3  0.2439 0.6241
#Residuals        39 10763.2   276.0


Now I want to verify these results with how I used to do it in SPSS (from 
it's menu): Analyze > General Linear Model > Repeated Measures (with the 
data set suitably rearranged to conform to SPSS' format). Comparing the 
"Tests of Within-Subject Effects" (I table a cannot reproduce in this 
medium) with the above results, I find identical results but for the main 
effects of pictcat and cond and their interaction. The Sum of squares of R 
are slightly larger (in itself not a bad thing because it results in larger 
F-ratio's) than those of SPSS (resp., 62141, 5747, 416 for pictcat, cond, 
pictcat:cond). The funny thing is that the SumSq are spot on for the 
interactions with the between-SS factor exp. The data for both analyses are 
exactly the same.

I've read about R using type I SS, but if I change the type III default of 
SPSS into type I, the results get more different, instead more more 
comparable. I've read about drop1, but

drop1(taov, test="F")
#Error in extractAIC(object, scale, k = k, ...) :
#        no applicable method for "extractAIC"

Also

library(car)
Anova(taov, type="III")
#Error in Anova(taov, type = "III") : no applicable method for "Anova"

which I found in the archives doesn't work. As the data for R and SPSS are 
identical and because the interactions with exp are identical between R and 
SPSS, there is something else going on (if it were the type I/III 
difference then the exp interactions would have been different as well, 
right?). I cannot deduce the reason of the difference. Can anybody help 
with this?


thank you for your time,
regards,
Paul Lemmens

-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066

-------------- next part --------------
"pp" "pictcat" "cond" "rt" "exp"
"1" "1" "animal" "con" 517.8125 "exp11b"
"2" "2" "animal" "con" 425.9375 "exp11b"
"3" "3" "animal" "con" 379.6563 "exp11b"
"4" "4" "animal" "con" 410.6563 "exp11b"
"5" "5" "animal" "con" 420.3125 "exp11b"
"6" "6" "animal" "con" 381.1613 "exp11b"
"7" "7" "animal" "con" 423.1875 "exp11b"
"8" "8" "animal" "con" 397.8387 "exp11b"
"9" "9" "animal" "con" 404.5714 "exp11b"
"10" "10" "animal" "con" 481.5000 "exp11b"
"11" "11" "animal" "con" 364.6875 "exp11b"
"12" "12" "animal" "con" 397.0938 "exp11b"
"13" "13" "animal" "con" 375.4375 "exp11b"
"14" "14" "animal" "con" 345.5313 "exp11b"
"15" "15" "animal" "con" 378.3548 "exp11b"
"16" "16" "animal" "con" 476.5161 "exp11b"
"17" "17" "animal" "con" 455.3750 "exp11b"
"18" "18" "animal" "con" 374.3125 "exp11b"
"19" "19" "animal" "con" 387.6250 "exp11b"
"20" "20" "animal" "con" 355.8750 "exp11b"
"21" "21" "animal" "con" 321.7097 "exp11b"
"22" "22" "animal" "con" 452.7813 "exp11b"
"23" "23" "animal" "con" 373.1290 "exp11b"
"24" "24" "animal" "con" 438.4063 "exp11b"
"25" "25" "animal" "con" 456.8125 "exp11b"
"26" "1" "other" "con" 470.4375 "exp11b"
"27" "2" "other" "con" 463.0938 "exp11b"
"28" "3" "other" "con" 463.1563 "exp11b"
"29" "4" "other" "con" 490.9063 "exp11b"
"30" "5" "other" "con" 467.4375 "exp11b"
"31" "6" "other" "con" 428.8125 "exp11b"
"32" "7" "other" "con" 459.2813 "exp11b"
"33" "8" "other" "con" 441.9375 "exp11b"
"34" "9" "other" "con" 397.2903 "exp11b"
"35" "10" "other" "con" 467.2500 "exp11b"
"36" "11" "other" "con" 431.3548 "exp11b"
"37" "12" "other" "con" 460.5625 "exp11b"
"38" "13" "other" "con" 401.8438 "exp11b"
"39" "14" "other" "con" 417.8438 "exp11b"
"40" "15" "other" "con" 414.6875 "exp11b"
"41" "16" "other" "con" 489.9375 "exp11b"
"42" "17" "other" "con" 485.1563 "exp11b"
"43" "18" "other" "con" 367.6250 "exp11b"
"44" "19" "other" "con" 422.1875 "exp11b"
"45" "20" "other" "con" 381.2500 "exp11b"
"46" "21" "other" "con" 346.1875 "exp11b"
"47" "22" "other" "con" 442.3438 "exp11b"
"48" "23" "other" "con" 412.7097 "exp11b"
"49" "24" "other" "con" 444.5313 "exp11b"
"50" "25" "other" "con" 509.3750 "exp11b"
"51" "1" "animal" "incon" 519.0313 "exp11b"
"52" "2" "animal" "incon" 465.9355 "exp11b"
"53" "3" "animal" "incon" 362.6875 "exp11b"
"54" "4" "animal" "incon" 417.1875 "exp11b"
"55" "5" "animal" "incon" 404.5625 "exp11b"
"56" "6" "animal" "incon" 394.9063 "exp11b"
"57" "7" "animal" "incon" 431.2500 "exp11b"
"58" "8" "animal" "incon" 421.0938 "exp11b"
"59" "9" "animal" "incon" 416.2903 "exp11b"
"60" "10" "animal" "incon" 507.4688 "exp11b"
"61" "11" "animal" "incon" 334.3871 "exp11b"
"62" "12" "animal" "incon" 402.4667 "exp11b"
"63" "13" "animal" "incon" 377.9063 "exp11b"
"64" "14" "animal" "incon" 362.6452 "exp11b"
"65" "15" "animal" "incon" 342.9355 "exp11b"
"66" "16" "animal" "incon" 475.1250 "exp11b"
"67" "17" "animal" "incon" 557.7500 "exp11b"
"68" "18" "animal" "incon" 361.0968 "exp11b"
"69" "19" "animal" "incon" 421.2813 "exp11b"
"70" "20" "animal" "incon" 345.1563 "exp11b"
"71" "21" "animal" "incon" 358.2258 "exp11b"
"72" "22" "animal" "incon" 484.9677 "exp11b"
"73" "23" "animal" "incon" 374.5000 "exp11b"
"74" "24" "animal" "incon" 445.3438 "exp11b"
"75" "25" "animal" "incon" 458.5000 "exp11b"
"76" "1" "other" "incon" 526.5625 "exp11b"
"77" "2" "other" "incon" 504.3871 "exp11b"
"78" "3" "other" "incon" 434.9375 "exp11b"
"79" "4" "other" "incon" 431.6563 "exp11b"
"80" "5" "other" "incon" 486.1563 "exp11b"
"81" "6" "other" "incon" 424.8065 "exp11b"
"82" "7" "other" "incon" 456.5161 "exp11b"
"83" "8" "other" "incon" 481.0625 "exp11b"
"84" "9" "other" "incon" 397.0345 "exp11b"
"85" "10" "other" "incon" 506.9375 "exp11b"
"86" "11" "other" "incon" 423.6000 "exp11b"
"87" "12" "other" "incon" 525.1935 "exp11b"
"88" "13" "other" "incon" 417.3125 "exp11b"
"89" "14" "other" "incon" 420.4688 "exp11b"
"90" "15" "other" "incon" 422.1250 "exp11b"
"91" "16" "other" "incon" 481.0000 "exp11b"
"92" "17" "other" "incon" 517.5625 "exp11b"
"93" "18" "other" "incon" 367.1250 "exp11b"
"94" "19" "other" "incon" 460.4688 "exp11b"
"95" "20" "other" "incon" 406.4688 "exp11b"
"96" "21" "other" "incon" 383.0625 "exp11b"
"97" "22" "other" "incon" 570.7097 "exp11b"
"98" "23" "other" "incon" 429.8438 "exp11b"
"99" "24" "other" "incon" 465.6250 "exp11b"
"100" "25" "other" "incon" 511.8710 "exp11b"
"101" "1" "animal" "neut" 500.8750 "exp11b"
"102" "2" "animal" "neut" 431.7813 "exp11b"
"103" "3" "animal" "neut" 362.3125 "exp11b"
"104" "4" "animal" "neut" 385.3438 "exp11b"
"105" "5" "animal" "neut" 390.2500 "exp11b"
"106" "6" "animal" "neut" 415.4375 "exp11b"
"107" "7" "animal" "neut" 508.7813 "exp11b"
"108" "8" "animal" "neut" 395.6563 "exp11b"
"109" "9" "animal" "neut" 419.9000 "exp11b"
"110" "10" "animal" "neut" 469.1250 "exp11b"
"111" "11" "animal" "neut" 356.8125 "exp11b"
"112" "12" "animal" "neut" 394.5000 "exp11b"
"113" "13" "animal" "neut" 378.3125 "exp11b"
"114" "14" "animal" "neut" 363.9063 "exp11b"
"115" "15" "animal" "neut" 377.5484 "exp11b"
"116" "16" "animal" "neut" 489.9677 "exp11b"
"117" "17" "animal" "neut" 443.8750 "exp11b"
"118" "18" "animal" "neut" 385.9688 "exp11b"
"119" "19" "animal" "neut" 429.6774 "exp11b"
"120" "20" "animal" "neut" 344.2258 "exp11b"
"121" "21" "animal" "neut" 332.6207 "exp11b"
"122" "22" "animal" "neut" 438.7813 "exp11b"
"123" "23" "animal" "neut" 384.5161 "exp11b"
"124" "24" "animal" "neut" 425.8438 "exp11b"
"125" "25" "animal" "neut" 455.7188 "exp11b"
"126" "1" "other" "neut" 475.3750 "exp11b"
"127" "2" "other" "neut" 462.2903 "exp11b"
"128" "3" "other" "neut" 420.7188 "exp11b"
"129" "4" "other" "neut" 420.6875 "exp11b"
"130" "5" "other" "neut" 465.3750 "exp11b"
"131" "6" "other" "neut" 410.6250 "exp11b"
"132" "7" "other" "neut" 518.1250 "exp11b"
"133" "8" "other" "neut" 452.4194 "exp11b"
"134" "9" "other" "neut" 420.8667 "exp11b"
"135" "10" "other" "neut" 448.3125 "exp11b"
"136" "11" "other" "neut" 460.4333 "exp11b"
"137" "12" "other" "neut" 436.8387 "exp11b"
"138" "13" "other" "neut" 425.4688 "exp11b"
"139" "14" "other" "neut" 409.1563 "exp11b"
"140" "15" "other" "neut" 401.7500 "exp11b"
"141" "16" "other" "neut" 494.1563 "exp11b"
"142" "17" "other" "neut" 454.3438 "exp11b"
"143" "18" "other" "neut" 421.1250 "exp11b"
"144" "19" "other" "neut" 426.9688 "exp11b"
"145" "20" "other" "neut" 393.2813 "exp11b"
"146" "21" "other" "neut" 366.5625 "exp11b"
"147" "22" "other" "neut" 484.7813 "exp11b"
"148" "23" "other" "neut" 411.0313 "exp11b"
"149" "24" "other" "neut" 448.6250 "exp11b"
"150" "25" "other" "neut" 474.6875 "exp11b"
"151" "1" "animal" "silent" 496.1667 "exp11b"
"152" "2" "animal" "silent" 447.7312 "exp11b"
"153" "3" "animal" "silent" 393.0104 "exp11b"
"154" "4" "animal" "silent" 390.1354 "exp11b"
"155" "5" "animal" "silent" 424.0729 "exp11b"
"156" "6" "animal" "silent" 399.3298 "exp11b"
"157" "7" "animal" "silent" 450.5208 "exp11b"
"158" "8" "animal" "silent" 409.2500 "exp11b"
"159" "9" "animal" "silent" 419.7419 "exp11b"
"160" "10" "animal" "silent" 439.7396 "exp11b"
"161" "11" "animal" "silent" 366.6042 "exp11b"
"162" "12" "animal" "silent" 439.2111 "exp11b"
"163" "13" "animal" "silent" 391.3333 "exp11b"
"164" "14" "animal" "silent" 367.5638 "exp11b"
"165" "15" "animal" "silent" 352.5745 "exp11b"
"166" "16" "animal" "silent" 518.7204 "exp11b"
"167" "17" "animal" "silent" 462.0104 "exp11b"
"168" "18" "animal" "silent" 389.6882 "exp11b"
"169" "19" "animal" "silent" 440.1579 "exp11b"
"170" "20" "animal" "silent" 354.1684 "exp11b"
"171" "21" "animal" "silent" 334.9560 "exp11b"
"172" "22" "animal" "silent" 459.4842 "exp11b"
"173" "23" "animal" "silent" 403.8542 "exp11b"
"174" "24" "animal" "silent" 429.6458 "exp11b"
"175" "25" "animal" "silent" 441.8438 "exp11b"
"176" "1" "other" "silent" 499.5000 "exp11b"
"177" "2" "other" "silent" 460.4792 "exp11b"
"178" "3" "other" "silent" 434.3125 "exp11b"
"179" "4" "other" "silent" 428.8750 "exp11b"
"180" "5" "other" "silent" 479.2708 "exp11b"
"181" "6" "other" "silent" 418.8617 "exp11b"
"182" "7" "other" "silent" 460.8958 "exp11b"
"183" "8" "other" "silent" 462.7396 "exp11b"
"184" "9" "other" "silent" 411.1354 "exp11b"
"185" "10" "other" "silent" 437.8438 "exp11b"
"186" "11" "other" "silent" 430.8152 "exp11b"
"187" "12" "other" "silent" 439.4149 "exp11b"
"188" "13" "other" "silent" 407.5579 "exp11b"
"189" "14" "other" "silent" 405.6316 "exp11b"
"190" "15" "other" "silent" 398.0426 "exp11b"
"191" "16" "other" "silent" 484.2766 "exp11b"
"192" "17" "other" "silent" 465.6250 "exp11b"
"193" "18" "other" "silent" 377.5208 "exp11b"
"194" "19" "other" "silent" 429.3542 "exp11b"
"195" "20" "other" "silent" 390.1915 "exp11b"
"196" "21" "other" "silent" 355.4375 "exp11b"
"197" "22" "other" "silent" 461.4479 "exp11b"
"198" "23" "other" "silent" 432.6000 "exp11b"
"199" "24" "other" "silent" 446.1895 "exp11b"
"200" "25" "other" "silent" 523.1042 "exp11b"
"1100" "31" "animal" "con" 379.7188 "exp11f"
"210" "32" "animal" "con" 429.0000 "exp11f"
"310" "33" "animal" "con" 347.4688 "exp11f"
"410" "34" "animal" "con" 375.1333 "exp11f"
"510" "35" "animal" "con" 390.9375 "exp11f"
"610" "40" "animal" "con" 314.8387 "exp11f"
"710" "41" "animal" "con" 480.8125 "exp11f"
"810" "42" "animal" "con" 401.8125 "exp11f"
"910" "43" "animal" "con" 731.3750 "exp11f"
"1010" "44" "animal" "con" 399.9688 "exp11f"
"1110" "45" "animal" "con" 398.4483 "exp11f"
"1210" "46" "animal" "con" 406.1290 "exp11f"
"1310" "47" "animal" "con" 428.0968 "exp11f"
"1410" "48" "animal" "con" 390.6333 "exp11f"
"1510" "49" "animal" "con" 297.7419 "exp11f"
"1610" "50" "animal" "con" 338.0000 "exp11f"
"1710" "31" "other" "con" 434.2188 "exp11f"
"1810" "32" "other" "con" 488.6207 "exp11f"
"1910" "33" "other" "con" 422.9355 "exp11f"
"201" "34" "other" "con" 416.0625 "exp11f"
"211" "35" "other" "con" 409.1875 "exp11f"
"221" "40" "other" "con" 360.1563 "exp11f"
"231" "41" "other" "con" 574.5625 "exp11f"
"241" "42" "other" "con" 477.4516 "exp11f"
"251" "43" "other" "con" 744.6563 "exp11f"
"261" "44" "other" "con" 415.6452 "exp11f"
"271" "45" "other" "con" 418.7857 "exp11f"
"281" "46" "other" "con" 453.5938 "exp11f"
"291" "47" "other" "con" 440.2500 "exp11f"
"301" "48" "other" "con" 428.1250 "exp11f"
"311" "49" "other" "con" 350.1333 "exp11f"
"321" "50" "other" "con" 348.4375 "exp11f"
"331" "31" "animal" "incon" 399.0938 "exp11f"
"341" "32" "animal" "incon" 406.0333 "exp11f"
"351" "33" "animal" "incon" 388.7419 "exp11f"
"361" "34" "animal" "incon" 384.1935 "exp11f"
"371" "35" "animal" "incon" 429.3750 "exp11f"
"381" "40" "animal" "incon" 320.5000 "exp11f"
"391" "41" "animal" "incon" 511.9375 "exp11f"
"401" "42" "animal" "incon" 452.1563 "exp11f"
"411" "43" "animal" "incon" 666.0313 "exp11f"
"421" "44" "animal" "incon" 394.1613 "exp11f"
"431" "45" "animal" "incon" 399.3571 "exp11f"
"441" "46" "animal" "incon" 434.2903 "exp11f"
"451" "47" "animal" "incon" 435.1875 "exp11f"
"461" "48" "animal" "incon" 379.6250 "exp11f"
"471" "49" "animal" "incon" 302.9667 "exp11f"
"481" "50" "animal" "incon" 332.6250 "exp11f"
"491" "31" "other" "incon" 464.6875 "exp11f"
"501" "32" "other" "incon" 479.9688 "exp11f"
"511" "33" "other" "incon" 473.8750 "exp11f"
"521" "34" "other" "incon" 407.5000 "exp11f"
"531" "35" "other" "incon" 453.5484 "exp11f"
"541" "40" "other" "incon" 357.9355 "exp11f"
"551" "41" "other" "incon" 549.5625 "exp11f"
"561" "42" "other" "incon" 497.7742 "exp11f"
"571" "43" "other" "incon" 700.1563 "exp11f"
"581" "44" "other" "incon" 424.5172 "exp11f"
"591" "45" "other" "incon" 424.9630 "exp11f"
"601" "46" "other" "incon" 486.6563 "exp11f"
"611" "47" "other" "incon" 531.3438 "exp11f"
"621" "48" "other" "incon" 439.3438 "exp11f"
"631" "49" "other" "incon" 341.2143 "exp11f"
"641" "50" "other" "incon" 338.3548 "exp11f"
"651" "31" "animal" "neut" 401.1250 "exp11f"
"661" "32" "animal" "neut" 489.5484 "exp11f"
"671" "33" "animal" "neut" 342.4688 "exp11f"
"681" "34" "animal" "neut" 379.7586 "exp11f"
"691" "35" "animal" "neut" 395.9688 "exp11f"
"701" "40" "animal" "neut" 308.5806 "exp11f"
"711" "41" "animal" "neut" 507.3125 "exp11f"
"721" "42" "animal" "neut" 425.2813 "exp11f"
"731" "43" "animal" "neut" 773.1875 "exp11f"
"741" "44" "animal" "neut" 377.4194 "exp11f"
"751" "45" "animal" "neut" 406.0714 "exp11f"
"761" "46" "animal" "neut" 401.5938 "exp11f"
"771" "47" "animal" "neut" 418.8710 "exp11f"
"781" "48" "animal" "neut" 380.4516 "exp11f"
"791" "49" "animal" "neut" 300.7188 "exp11f"
"801" "50" "animal" "neut" 325.2258 "exp11f"
"811" "31" "other" "neut" 449.9063 "exp11f"
"821" "32" "other" "neut" 558.5000 "exp11f"
"831" "33" "other" "neut" 414.8065 "exp11f"
"841" "34" "other" "neut" 392.5161 "exp11f"
"851" "35" "other" "neut" 413.3125 "exp11f"
"861" "40" "other" "neut" 345.2581 "exp11f"
"871" "41" "other" "neut" 498.5313 "exp11f"
"881" "42" "other" "neut" 460.8750 "exp11f"
"891" "43" "other" "neut" 849.8125 "exp11f"
"901" "44" "other" "neut" 396.8333 "exp11f"
"911" "45" "other" "neut" 459.3000 "exp11f"
"921" "46" "other" "neut" 449.0000 "exp11f"
"931" "47" "other" "neut" 470.5313 "exp11f"
"941" "48" "other" "neut" 399.9677 "exp11f"
"951" "49" "other" "neut" 364.9688 "exp11f"
"961" "50" "other" "neut" 302.6129 "exp11f"
"971" "31" "animal" "silent" 399.0421 "exp11f"
"981" "32" "animal" "silent" 469.4066 "exp11f"
"991" "33" "animal" "silent" 375.7935 "exp11f"
"1001" "34" "animal" "silent" 388.0323 "exp11f"
"1011" "35" "animal" "silent" 404.2917 "exp11f"
"1021" "40" "animal" "silent" 319.1489 "exp11f"
"1031" "41" "animal" "silent" 469.3263 "exp11f"
"1041" "42" "animal" "silent" 448.0538 "exp11f"
"1051" "43" "animal" "silent" 679.1684 "exp11f"
"1061" "44" "animal" "silent" 398.3778 "exp11f"
"1071" "45" "animal" "silent" 393.2308 "exp11f"
"1081" "46" "animal" "silent" 436.5054 "exp11f"
"1091" "47" "animal" "silent" 402.4894 "exp11f"
"1101" "48" "animal" "silent" 398.0000 "exp11f"
"1111" "49" "animal" "silent" 305.5914 "exp11f"
"1121" "50" "animal" "silent" 332.3804 "exp11f"
"1131" "31" "other" "silent" 441.2947 "exp11f"
"1141" "32" "other" "silent" 518.7957 "exp11f"
"1151" "33" "other" "silent" 425.0213 "exp11f"
"1161" "34" "other" "silent" 401.6809 "exp11f"
"1171" "35" "other" "silent" 417.8105 "exp11f"
"1181" "40" "other" "silent" 351.2234 "exp11f"
"1191" "41" "other" "silent" 493.2708 "exp11f"
"1201" "42" "other" "silent" 477.5851 "exp11f"
"1211" "43" "other" "silent" 739.9063 "exp11f"
"1221" "44" "other" "silent" 397.1170 "exp11f"
"1231" "45" "other" "silent" 424.6316 "exp11f"
"1241" "46" "other" "silent" 461.3118 "exp11f"
"1251" "47" "other" "silent" 437.8065 "exp11f"
"1261" "48" "other" "silent" 427.5326 "exp11f"
"1271" "49" "other" "silent" 351.5699 "exp11f"
"1281" "50" "other" "silent" 328.5104 "exp11f"

From tmchoi at ris.chonnam.ac.kr  Fri Apr 23 15:25:06 2004
From: tmchoi at ris.chonnam.ac.kr (Taemyong Choi)
Date: Fri, 23 Apr 2004 22:25:06 +0900
Subject: [R] matrix <- matrix ??
Message-ID: <200404231324.i3NDOaVt015511@ris.chonnam.ac.kr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/2561e5cb/attachment.pl

From pigood at verizon.net  Fri Apr 23 15:39:48 2004
From: pigood at verizon.net (Phillip Good)
Date: Fri, 23 Apr 2004 06:39:48 -0700
Subject: [R] Help with debugging
References: <Pine.LNX.4.44.0404230918430.1485-100000@gannet.stats>
Message-ID: <003f01c42938$72f6c9a0$e1e10804@dslverizon.net>

When one wishes to return only a single value from a user-developed R
function, "return" is preferable to "list."

Phillip Good



From thsudler at swissonline.ch  Fri Apr 23 15:47:46 2004
From: thsudler at swissonline.ch (thsudler@swissonline.ch)
Date: Fri, 23 Apr 2004 15:47:46 +0200
Subject: [R] Tcl Tk table
Message-ID: <200404231347.i3NDlkRv002693@smtp.hispeed.ch>

Hi 

I've a problem with the following example:

library(tcltk)
.Tcl("array unset tclArray")

myRarray <- matrix(1:1000, ncol=20)

for (i in (0:49))
  for (j in (0:19))
     .Tcl(paste("set tclArray(",i,",",j,") ",myRarray[i+1,j+1],sep=""))
     
tt<-tktoplevel()

table1 <- tkwidget(tt,"table",variable="tclArray", rows="50", cols="50")

tkpack(table1)

#Old version which worked in R 1.6 but it doesn't work with R 1.9 (and also not with 1.8), why??
tkcmd(.Tk.ID(table1),"tag","celltag","gruen",list(c("3,3", "4,4", "5,5", "6,6", "7,7", "8,8")))
tkcmd(.Tk.ID(table1),"tag","configure","gruen",bg="green",fg="green")
#Error message: Error in switch(storage.mode(x), character = .External("RTcl_ObjFromCharVector",  :  Cannot handle object of mode  list

#But this works also with R 1.9
tkcmd(.Tk.ID(table1),"tag","celltag","gruen","3,3","4,4","5,5","6,6","7,7","8,8")
tkcmd(.Tk.ID(table1),"tag","configure","gruen",bg="green",fg="green")


Under R version 1.6 I had no problem. Now I installed R 1.9 (with ActiveTcl) and my program doesn't work. Maybe you ask why I?m not using the solution which works. It's because I've a list with thousands of coordiantes of cells which I want to have green, a list with thousands of coordinates of cells which I want to have blue, and so on.

If I do this in a "for loop", it needs about 5 minutes until all the cells have the color I want (because it's a really big table). Also with "lapply" it needs about 3 minutes. In R version 1.6 (where it worked with a "list") the table was generated in less than 1 second!

I would be very grateful if somebody could help me. Thanks a lot in advance!

Thomas



From macq at llnl.gov  Fri Apr 23 16:06:45 2004
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 23 Apr 2004 07:06:45 -0700
Subject: [R] POSIXct vs Dates
In-Reply-To: <20040422082008.1b872388.feh3k@spamcop.net>
References: <20040422082008.1b872388.feh3k@spamcop.net>
Message-ID: <p06002000bcaecf88d2dd@[128.115.153.6]>

I can agree with the other responses, and mention that I'm on the 
other side of the coin, so to speak.

The *only* reason I tried R to begin with was to find out if it could 
handle data whose time scale was in minutes and seconds better than 
the other software I was using. With its POSIXt class, which had only 
recently been introduced, it was better.

In particular, I needed software that would successfully plot the 
data as it crossed the boundary between daylight savings time and 
standard time *and* label the time axis with daylight savings time or 
standard time as appropriate. R did; the other software did not.

-Don

At 8:20 AM -0400 4/22/04, Frank E Harrell Jr wrote:
>I noticed the addition of the Dates class for dates without times, in R
>1.9.  I am making extensive use of POSIXct at present and would like to
>know whether it is worth changing to Dates.  What are a few of the
>trade-offs?
>
>Thanks,
>
>Frank
>---
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Luisr at frs.fo  Fri Apr 23 16:33:55 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 23 Apr 2004 15:33:55 +0100
Subject: [R] Naming list elements
Message-ID: <s0893772.045@ffdata.setur.fo>

How to name the following list GH.MEINKV.byyear ( a list consisting of 9 data frames)

GH.MEINKV.byyear<-as.list(numeric(length(levels(GH.MEINKV$"Year"))))	




Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From nlwhitehouse at yahoo.com  Fri Apr 23 17:03:07 2004
From: nlwhitehouse at yahoo.com (Nathan Whitehouse)
Date: Fri, 23 Apr 2004 08:03:07 -0700 (PDT)
Subject: [R] SJava embedded in R
Message-ID: <20040423150307.72828.qmail@web12407.mail.yahoo.com>

Hi,

  Here's some suggestions for troubleshooting the
install.  

  The CLASSPATH needs to be 
$SJAVA_HOME/org/omegahat/Jars/Environment.jar,
$SJAVA_HOME/org/omegahat/Jars/antlr.jar
$SJAVA_HOME/org/omegahat/Jars/jas.jar
$SJAVA_HOME.

This allows the java compiler/interpreter to find
  1)the ROmegahatInterpreter and REvaluator classes(in
a org/omegahat/R/Java tree under SJava
  2)& the base omegahat interpreter libraries from
which the ROmegahatInterpreter & REvaluator classes
descend.(Found in those jar files)

   on Unix-like systems,
  the LD_LIBRARY_PATH has to point to the shared
objects(so)/dynamic linking libraries, usually in 
   
  $R_HOME/libs.

  These .so/.dll files have to be symbolically linked
to other names.  This is because the Java/JNI loader
needs a specific naming convention to load symbol
libraries/shared objects(something to do with platform
compatibility.)

  R INSTALL -c SJava-version.tar.gz
  (-c executes a cleanup script which performs the
linking)

  I would doublecheck to make sure you have R compiled
as a shared library.  Easily- look in R_HOME/bin for
libR.so. (or, I assume libR.dll)

  Hope some of this will help.
  
  happy pinball,
  n
  
I downloaded it and installed it with all the
recommended options. I
created the LD_LIBRARY_PATH
environment variable but my java compiler (latest
version) can't find
packages like org.omegahat.R.java and cannot resolve
symbol (class) 
like
REvaluator, RForeignReference...  any idea ?

many thanks for answering my last question I promise !
best regards,
Anthony

=====
Nathan Whitehouse
Statistics/Programming
Baylor College of Medicine
Houston, TX, USA
nlwhitehouse at yahoo.com
work: 1-713-798-9029
cell:    1-512-293-5840

http://rho-project.org: rho- open source web services for R.
http://franklin.imgen.bcm.tmc.edu: Shaw laboratory, bcm.



From cepl at surfbest.net  Fri Apr 23 10:37:26 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Fri, 23 Apr 2004 04:37:26 -0400
Subject: [R] Selection of cities sample
In-Reply-To: <200404220056.46149.cepl@surfbest.net>
References: <200404220056.46149.cepl@surfbest.net>
Message-ID: <200404230437.28129.cepl@surfbest.net>

On Thursday 22 of April 2004 00:56, Matej Cepl wrote:
> I have a question, how to most properly select set of cities 
> which would be as similar as possible in some particular 
> variables with the City of Boston (which I use as my base
> line).  
> I thought about ordering cities by sum of ((differences between 
> value of that particular variable for that particular city and 
> the value of same variable for Boston) divided by the standard 
> deviation of the variable and multiplied by the weight 
> (expressing how much that particular variable is important for 
> me, or how much I want to avoid cities with this 
> characteristic)). Is it sound method?
> 
> Or I am creating something which is already available in R as a 
> standard function (which I suspect)?

So, OK, I did it again -- asking without knowing what I am asking 
for. I'm sorry for that. After whole day spending with Gordon's 
"Classification" and MASS on cluster analysis, I made this 
function:

# $Id: compute.R,v 1.1 2004/04/23 08:19:06 matej Exp matej $
# Make selection of rows from data frame which are most similar
# to the row identified by (at least part of) row.name.
# variables:
#   dframe -- data frame to select from
#   variables -- vector of names of variables to be used for
#      computation of similarity
#   weights -- vector of weights measuring how much each variable
#      is important for the selection
#   basename -- row.name of the row which the similarity should
#      be measured with
#   howmany -- how many elements should be selected

makeSelection <- function (dframe, variables = names(dframe),
      weights = 1, basename, howmany = length(row.names(dframe))) 
{
   normal <- dframe[variables]/sd(dframe,na.rm=TRUE)
   normal$coef <- apply(normal,1, function (x) 
{ sum(x*weights) })
   base.coef <- normal[grep(basename,row.names(normal)),]$coef
   normal$distance <- abs(normal$coef - base.coef)
   return(row.names(normal[order(normal$distance),])[2:howmany
+1])
}

Can anybody comment on this please, whether it does roughly what 
I described above?

	Thanks a lot,

		Matej

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
The law, in its majestic equality, forbids the rich as well as
the poor to sleep under bridges, to beg in the streets, and to
steal bread.
    -- Anatole France



From paulojus at est.ufpr.br  Fri Apr 23 17:49:52 2004
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 23 Apr 2004 12:49:52 -0300 (BRT)
Subject: [R] GepR - bayesian modelling - please help!
In-Reply-To: <E1BGxFp-000L7i-Nr@serenity.mcc.ac.uk>
References: <E1BGxFp-000L7i-Nr@serenity.mcc.ac.uk>
Message-ID: <Pine.LNX.4.58L0.0404231249080.23157@est.ufpr.br>

Monica

Could you please send me the
zn.gdata and loc1 objects and I will investigate this

P.J.

On Fri, 23 Apr 2004, Monica Palaseanu-Lovejoy wrote:

> Hi,
>
> I am coming back with a question about GeoR hopping somebody
> can help.
>
> I am doing a bayesian kriging and the prediction coordinates are
> shifted 0.1 m from the data coordinates.
>
> I am using the following command and i am getting the following
> error:
>
> zn.bayes <- krige.bayes(zn.gdata, loc = loc1, model =
> model.control(cov.model = "exponential", lambda = 0),
> prior = prior.control(phi.prior ="exponential", phi =
> 89.1948), output=output.control(n.predictive=2,
> mean.var = TRUE, quantile = c(0.025, 0.25, 0.5, 0.75,
> 0.975), threshold = c(300)))
>
> Error: Error in cond.sim(env.loc = base.env, env.iter = iter.env,
> loc.coincide = get("loc.coincide",  :
>         chol: matrix not pos def, diag[13]= -3.487549e-019
>
> What i am doing wrong? The location loc1 is a data frame with the
> shifted coord. values. Any suggestion is very well appreciated.
>
> Thanks,
>
> Monica
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

Paulo Justiniano Ribeiro Jr
Departamento de Estat??stica
Universidade Federal do Paran??
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3471
Fax: (+55) 41 361 3141
e-mail: pj at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From s-plus at wiwi.uni-bielefeld.de  Fri Apr 23 17:50:20 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 23 Apr 2004 17:50:20 +0200
Subject: [R] Tcl Tk table
References: <200404231347.i3NDlkRv002693@smtp.hispeed.ch>
Message-ID: <40893B3C.8080002@wiwi.uni-bielefeld.de>

thsudler at swissonline.ch wrote:

>Hi 
>
>I've a problem with the following example:
>
>library(tcltk)
>.Tcl("array unset tclArray")
>
>myRarray <- matrix(1:1000, ncol=20)
>
>for (i in (0:49))
>  for (j in (0:19))
>     .Tcl(paste("set tclArray(",i,",",j,") ",myRarray[i+1,j+1],sep=""))
>     
>tt<-tktoplevel()
>
>table1 <- tkwidget(tt,"table",variable="tclArray", rows="50", cols="50")
>
>tkpack(table1)
>
>#Old version which worked in R 1.6 but it doesn't work with R 1.9 (and also not with 1.8), why??
>.....
>Under R version 1.6 I had no problem. Now I installed R 1.9 (with ActiveTcl) and my program doesn't work. ...
>Thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
Are you sure?

 > library(tcltk)
 > .Tcl("array unset tclArray")
<Tcl>
 >
 > myRarray <- matrix(1:1000, ncol=20)
 >
 > for (i in (0:49))
+   for (j in (0:19))
+      .Tcl(paste("set tclArray(",i,",",j,") ",myRarray[i+1,j+1],sep=""))
 >
 > tt<-tktoplevel()
 >
 > table1 <- tkwidget(tt,"table",variable="tclArray", rows="50", cols="50")
Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class = 
"tclObj") :
        [tcl] invalid command name "table".
 >
 > version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    6.2
year     2003
month    01
day      10
language R

the error occurs with version 1.6.2, too.

Some debugging shows: the error message comes from tcl/tk and
you need some additional features to be allowed to use the table-widget!
you have to include:

tclRequire("Tktable")

... and all is ok for 1.8.1 and I also hope for 1.9.0.

see: http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/tktable.html

Peter Wolf



From thsudler at swissonline.ch  Fri Apr 23 18:30:18 2004
From: thsudler at swissonline.ch (thsudler@swissonline.ch)
Date: Fri, 23 Apr 2004 18:30:18 +0200
Subject: [R] Tcl Tk table
Message-ID: <200404231630.i3NGUIUd007371@smtp.hispeed.ch>

Hi

Yes, I'm sure. I allready installed this additional package. The command

tclRequire("Tktable")

doesn't cause an error. So I've the Tktable package. But the command with "list" doesn't works:

tkcmd(.Tk.ID(table1),"tag","celltag","gruen",list(c("3,3", "4,4", "5,5", "6,6", "7,7", "8,8")))
tkcmd(.Tk.ID(table1),"tag","configure","gruen",bg="green",fg="green")

Error message:
Error in switch(storage.mode(x), character = .External("RTcl_ObjFromCharVector",  :  Cannot handle object of mode  list

Do you know why this could be?

Thomas

----- Original Message ----- 
From: "Peter Wolf" <s-plus at wiwi.uni-bielefeld.de>
To: <thsudler at swissonline.ch>; <r-help at stat.math.ethz.ch>
Sent: Friday, April 23, 2004 5:50 PM
Subject: Re: [R] Tcl Tk table


> thsudler at swissonline.ch wrote:
> 
> >Hi 
> >
> >I've a problem with the following example:
> >
> >library(tcltk)
> >.Tcl("array unset tclArray")
> >
> >myRarray <- matrix(1:1000, ncol=20)
> >
> >for (i in (0:49))
> >  for (j in (0:19))
> >     .Tcl(paste("set tclArray(",i,",",j,") ",myRarray[i+1,j+1],sep=""))
> >     
> >tt<-tktoplevel()
> >
> >table1 <- tkwidget(tt,"table",variable="tclArray", rows="50", cols="50")
> >
> >tkpack(table1)
> >
> >#Old version which worked in R 1.6 but it doesn't work with R 1.9 (and also not with 1.8), why??
> >.....
> >Under R version 1.6 I had no problem. Now I installed R 1.9 (with ActiveTcl) and my program doesn't work. ...
> >Thomas
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >  
> >
> Are you sure?
> 
>  > library(tcltk)
>  > .Tcl("array unset tclArray")
> <Tcl>
>  >
>  > myRarray <- matrix(1:1000, ncol=20)
>  >
>  > for (i in (0:49))
> +   for (j in (0:19))
> +      .Tcl(paste("set tclArray(",i,",",j,") ",myRarray[i+1,j+1],sep=""))
>  >
>  > tt<-tktoplevel()
>  >
>  > table1 <- tkwidget(tt,"table",variable="tclArray", rows="50", cols="50")
> Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class = 
> "tclObj") :
>         [tcl] invalid command name "table".
>  >
>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    6.2
> year     2003
> month    01
> day      10
> language R
> 
> the error occurs with version 1.6.2, too.
> 
> Some debugging shows: the error message comes from tcl/tk and
> you need some additional features to be allowed to use the table-widget!
> you have to include:
> 
> tclRequire("Tktable")
> 
> ... and all is ok for 1.8.1 and I also hope for 1.9.0.
> 
> see: http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/tktable.html
> 
> Peter Wolf
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From monica.palaseanu-lovejoy at stud.man.ac.uk  Fri Apr 23 18:49:36 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Fri, 23 Apr 2004 17:49:36 +0100
Subject: [R] GepR - bayesian modelling - please help! Corrections ....
In-Reply-To: <Pine.LNX.4.58L0.0404231249080.23157@est.ufpr.br>
References: <E1BGxFp-000L7i-Nr@serenity.mcc.ac.uk>
Message-ID: <E1BH3s7-000LAK-GE@serenity.mcc.ac.uk>

Hi again,

I was checking again everything after i sent you my data. It seems 
that the problem is the following. My prediction locations are 
different in number than my data locations ..... i forgot that when i 
built the geodata object few points had more than one value so i 
used the "first" option and my data number decreased. When i 
used the right number of locations shifted by 0.1 m the krige. 
bayes did work this time.

Now i have to figure out how much memory i do need to run the 
same function on a grid with about 15000 grid cells. If you have any 
info about that i will be really very grateful. 

Also i was searching for the "Ribeiro, P.J. Jr. and Diggle, P.J. 
(1999) _Bayesian inference in Gaussian model-based 
geostatistics_. Tech. Report ST-99-08, Dept Maths and Stats, 
Lancaster University.Available at: 
 <URL: http://www.maths.lancs.ac.uk/~ribeiro/publications.html>" 

The link is invalidated now.

Thank you so much for all your help and time,

Monica



From ripley at stats.ox.ac.uk  Fri Apr 23 18:52:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 17:52:02 +0100 (BST)
Subject: [R] HTML help pages
In-Reply-To: <Pine.LNX.4.44.0404221637190.31934-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404231749050.2483-100000@gannet.stats>

On Thu, 22 Apr 2004, Prof Brian Ripley wrote:

> Unfortunately your suggested change to SearchEngine.html is not valid
> JavaScript under Internet Explorer 6: at least APPLET appears to be
> recognised under all commonly used browsers.

Also unfortunately, the embed version also fails for me on two Linux boxes
(one RH8.0, one FC1) with j2re_1.4.2_04.  Both the original and the embed
version work with j2re_1.4.2_01 on both boxes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Apr 23 19:01:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Apr 2004 19:01:49 +0200
Subject: [R] Tcl Tk table
In-Reply-To: <200404231347.i3NDlkRv002693@smtp.hispeed.ch>
References: <200404231347.i3NDlkRv002693@smtp.hispeed.ch>
Message-ID: <x2y8omljb6.fsf@biostat.ku.dk>

thsudler at swissonline.ch writes:

> tkcmd(.Tk.ID(table1),"tag","celltag","gruen",list(c("3,3", "4,4", "5,5", "6,6", "7,7", "8,8")))

Did that ever work? Prior to 1.8.0, a character vector would get
expanded to multiple arguments, but not lists, AFAIR. 

The character vector expansion was sort of a nice feature in some
cases, but it was was inconsistent with the rest of the design where
one argument to tkcmd is one Tcl object (or two, for named arguments)
and there were other cases where you'd want a character vector to be
passed as a Tcl list. Also, the whole mechanism for converting from R
to Tcl commands was horribly inefficient in some cases, and needed
replacement.

If you need to create a Tcl command with a variable number of
arguments, you now need to create an R function call with a variable number
of arguments, i.e. use do.call() as in

do.call("tkcmd", c(list(table1,"tag","cell","gruen"),
                 as.list(paste(3:8,3:8),sep=","))

(note btw that .Tk.ID() is wrong, you can just pass the widget itself) 

In a *really* tight spot, you can still get the old behaviour using
.Tcl(.Tcl.args(.....))

> #But this works also with R 1.9
> tkcmd(.Tk.ID(table1),"tag","celltag","gruen","3,3","4,4","5,5","6,6","7,7","8,8")
> tkcmd(.Tk.ID(table1),"tag","configure","gruen",bg="green",fg="green")

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hj4cho at msn.com  Fri Apr 23 19:09:36 2004
From: hj4cho at msn.com (Hyung Cho)
Date: Fri, 23 Apr 2004 17:09:36 +0000
Subject: [R] (no subject)
Message-ID: <BAY5-F10WbnJ7ybxvxE0006428c@hotmail.com>



Dear all:

  I am developing a package in R. While I am running R CMD check, I found 
the following warning message:


Found the following C sources/headers with CRLF line endings:
  src/hem.c
  src/random4f.h
ISO C requires LF line endings.



It seems that it comes from a line ending problem in C. What are CRLF/LF 
line endings?
How can I fix it? Thank you for your help in advance.

Best,
HJ

_________________________________________________________________
Test your 'Travel Quotient and get the chance to win your dream trip! 
http://travel.msn.com



From hj4cho at msn.com  Fri Apr 23 19:11:34 2004
From: hj4cho at msn.com (Hyung Cho)
Date: Fri, 23 Apr 2004 17:11:34 +0000
Subject: [R] CRLF/LF line endings
Message-ID: <BAY5-F338o14xqP360Y0002374e@hotmail.com>




Dear all:

  I am developing a package in R. While I am running R CMD check, I
found the following warning message:


Found the following C sources/headers with CRLF line endings:
  src/hem.c
  src/random4f.h
ISO C requires LF line endings.



It seems that it comes from a line ending problem in C. What are
CRLF/LF line endings?
How can I fix it? Thank you for your help in advance.

Best,
HJ

_________________________________________________________________
Stop worrying about overloading your inbox - get MSN Hotmail Extra Storage! 
http://join.msn.com/?pgmarket=en-us&page=hotmail/es2&ST=1/go/onm00200362ave/direct/01/



From bamelbourne at ucdavis.edu  Fri Apr 23 19:12:45 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Fri, 23 Apr 2004 10:12:45 -0700
Subject: [R] Fatal Error: INVALID HOMEDRIVE
References: <E85DCCA6ED13F749A27DEF593B2297FB0366979F@matcomexch02.matcom.usmc.mil>
Message-ID: <007a01c42956$32bf4a60$160feda9@des.ucdavis.edu>

I can also start 1.8.1 and 1.9.0 properly in a similar way to that described
here by Vinyard, by typing the following at the command prompt or from a
batch file.

SET HOMEPATH=\
"C:\Program Files\R\rw1090\bin\Rgui.exe"

However, both 1.8.1 and 1.9.0 still fail in the usual way when started from
the program icon (I have confirmed, by SET, before and after failing that
HOMEPATH is properly set). I observe the same behaviour when HOME=C:\ is
set.

Brett



----- Original Message ----- 
From: "Vinyard Maj William C" <VinyardWC at logcom.usmc.mil>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, April 23, 2004 4:54 AM
Subject: [R] Fatal Error: INVALID HOMEDRIVE


> As requested:
>
> set > eviron.txt
>
> ALLUSERSPROFILE=D:\Documents and Settings\All Users
> APPDATA=D:\Documents and Settings\vinyardwc\Application Data
> CMP_HOME=C:\PROGRA~1\DIICOE~1
> CommonProgramFiles=C:\Program Files\Common Files
> COMPUTERNAME=MATCOM08WK072
> ComSpec=C:\WINNT\system32\cmd.exe
> HOMEDRIVE=C:
> HOMESHARE=\\matcomapps05\users
> LOGONSERVER=\\MATCOMDC03
> NUMBER_OF_PROCESSORS=1
> OS=Windows_NT
> Os2LibPath=C:\WINNT\system32\os2\dll;
> Path=d:\docume~1\vinyardwc\gnuwin32\bin;d:\docume~1\vinyardwc\gnu\bin;
> PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> PBXHOME=C:\PBrokerX
> PROCESSOR_ARCHITECTURE=x86
> PROCESSOR_IDENTIFIER=x86 Family 15 Model 1 Stepping 3, GenuineIntel
> PROCESSOR_LEVEL=15
> PROCESSOR_REVISION=0103
> ProgramFiles=C:\Program Files
> PROMPT=$P$G
> RESOURCENAME=DIICOE
> SMS_LOCAL_DIR=C:\WINNT
> SMS_LOCAL_DIR_USER=C:\WINNT
> SystemDrive=C:
> SystemRoot=C:\WINNT
> TEMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> TMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> UCP_HOME=..\data\UCP
> USERDOMAIN=MATCOM
> USERNAME=VinyardWC
> USERPROFILE=D:\Documents and Settings\vinyardwc
> windir=C:\WINNT
> WORKDIRECTORY=C:\Program Files\DII COE Message Processor\bin
>
> I use a DELL desktop with Win 2000 Pro.  I have R-1.9.0 installed.  A
> co-worker has R-1.8.1 installed on a Dell laptop also with Win 2000 Pro.
He
> has not experienced any problems. Our network admin push updates over the
> net at night.  I assume the netadmin pushes the same updates to both
> machines.
>
> I changed my batch file from:
>
> SET HOMEDRIVE=D:
> SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> Rgui.exe
>
> to:
>
> @SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> @ Start Rgui.exe
>
> The second version works.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Apr 23 19:19:26 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Apr 2004 19:19:26 +0200
Subject: [R] (no subject)
In-Reply-To: <BAY5-F10WbnJ7ybxvxE0006428c@hotmail.com>
References: <BAY5-F10WbnJ7ybxvxE0006428c@hotmail.com>
Message-ID: <x2u0zaliht.fsf@biostat.ku.dk>

"Hyung Cho" <hj4cho at msn.com> writes:

> Found the following C sources/headers with CRLF line endings:
>   src/hem.c
>   src/random4f.h
> ISO C requires LF line endings.

> It seems that it comes from a line ending problem in C. What are
> CRLF/LF line endings?

CR is "carriage return" (ASCII char #13, "\r")
LF is "line feed" (ASCII #10, "\n")

DOS and *some* parts of Windows wants text lines to end with both
characters. Unix, Linux, and ISO C want just the LF. (And older(?)
MacOS uses CR just to be perverse...)  

> How can I fix it? Thank you for your help in advance.

Windows, right? (Notepad is known to do that to you) There's a
dos2unix command in the tools package which you can just run over your
various text files.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From VinyardWC at logcom.usmc.mil  Fri Apr 23 19:36:31 2004
From: VinyardWC at logcom.usmc.mil (Vinyard Maj William C)
Date: Fri, 23 Apr 2004 13:36:31 -0400
Subject: [R] Fatal Error: INVALID HOMEDRIVE
Message-ID: <E85DCCA6ED13F749A27DEF593B2297FB036697AB@matcomexch02.matcom.usmc.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/f5808f88/attachment.pl

From jcmartinez at banxico.org.mx  Fri Apr 23 19:47:44 2004
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Fri, 23 Apr 2004 12:47:44 -0500
Subject: [R] installation problem
Message-ID: <ED7E0E44EAADFB46A6ABA12A647C1306108810F3@CORREOINT.banxico.org.mx>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/97d608d0/attachment.pl

From clint at ecy.wa.gov  Fri Apr 23 19:56:03 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 23 Apr 2004 10:56:03 -0700 (PDT)
Subject: [R] CRLF/LF line endings
In-Reply-To: <BAY5-F338o14xqP360Y0002374e@hotmail.com>
Message-ID: <Pine.LNX.4.44.0404231055120.7096-100000@aeolus.ecy.wa.gov>

A quick run through dos2unix should make the files right, e.g., 

dos2unix hem.c
dos2unix random4f.h

On Fri, 23 Apr 2004, Hyung Cho wrote:

> 
> 
> 
> Dear all:
> 
>   I am developing a package in R. While I am running R CMD check, I
> found the following warning message:
> 
> 
> Found the following C sources/headers with CRLF line endings:
>   src/hem.c
>   src/random4f.h
> ISO C requires LF line endings.
> 
> 
> 
> It seems that it comes from a line ending problem in C. What are
> CRLF/LF line endings?
> How can I fix it? Thank you for your help in advance.
> 
> Best,
> HJ
> 
> _________________________________________________________________
> Stop worrying about overloading your inbox - get MSN Hotmail Extra Storage! 
> http://join.msn.com/?pgmarket=en-us&page=hotmail/es2&ST=1/go/onm00200362ave/direct/01/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From ripley at stats.ox.ac.uk  Fri Apr 23 20:01:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 23 Apr 2004 19:01:24 +0100 (BST)
Subject: [R] installation problem
In-Reply-To: <ED7E0E44EAADFB46A6ABA12A647C1306108810F3@CORREOINT.banxico.org.mx>
Message-ID: <Pine.LNX.4.44.0404231859160.12918-100000@gannet.stats>

If you look in the rw-FAQ, you will see that R itself does not use the 
registry.  You voild (and should as you do not have permission) have 
deselected this option on the last installer page.

It is nothing to do with which drive, rather than you did the install with 
too limited privileges.

On Fri, 23 Apr 2004, Mart??nez Ovando Juan Carlos wrote:

> Hello all,
> 
>  
> 
> I??m trying to install R (1.09 beta version) in my computer, but my path
> root to install is 'D:\...\Software' and not the usual 'C:\...'; in the
> registration key step the install software mark the follow error
> message:
> 
>  
> 
> Error creating the registry key:
> 
> HKEY_LOCAL_MACHINE\software\R-core
> 
>             
> 
> RegCerateKey failed; code 5.
> 
> Access denied.
> 
>  
> 
> After ignoring this message few times, it seems tha R was instaled
> successfully. Do any of you know if this could cause a problem in the
> software performance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bamelbourne at ucdavis.edu  Fri Apr 23 20:06:05 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Fri, 23 Apr 2004 11:06:05 -0700
Subject: [R] Fatal Error: INVALID HOMEDRIVE
References: <E85DCCA6ED13F749A27DEF593B2297FB036697AB@matcomexch02.matcom.usmc.mil>
Message-ID: <00c101c4295d$a708b6c0$1613eda9@des.ucdavis.edu>

> have you tried changing the "Target" line in the properties dialog for the
> icon to this:
>
> "C:\Program Files\R\rw1090\bin\Rgui.exe"
HOMEPATH=\path\to\normal\R\workdir

This works for me too, in the the "Target" line in the properties dialog for
the icon:
"C:\Program Files\R\rw1090\bin\Rgui.exe" HOMEPATH=\

Brett


>
>
> -----Original Message-----
> From: Brett Melbourne [mailto:bamelbourne at ucdavis.edu]
> Sent: Friday, April 23, 2004 13:13
> To: Vinyard Maj William C; r-help at stat.math.ethz.ch
> Subject: Re: [R] Fatal Error: INVALID HOMEDRIVE
>
>
> I can also start 1.8.1 and 1.9.0 properly in a similar way to that
described
> here by Vinyard, by typing the following at the command prompt or from a
> batch file.
>
> SET HOMEPATH=\
> "C:\Program Files\R\rw1090\bin\Rgui.exe"
>
> However, both 1.8.1 and 1.9.0 still fail in the usual way when started
from
> the program icon (I have confirmed, by SET, before and after failing that
> HOMEPATH is properly set). I observe the same behaviour when HOME=C:\ is
> set.
>
> Brett
>
>
>
> ----- Original Message ----- 
> From: "Vinyard Maj William C" <VinyardWC at logcom.usmc.mil>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, April 23, 2004 4:54 AM
> Subject: [R] Fatal Error: INVALID HOMEDRIVE
>
>
> > As requested:
> >
> > set > eviron.txt
> >
> > ALLUSERSPROFILE=D:\Documents and Settings\All Users
> > APPDATA=D:\Documents and Settings\vinyardwc\Application Data
> > CMP_HOME=C:\PROGRA~1\DIICOE~1
> > CommonProgramFiles=C:\Program Files\Common Files
> > COMPUTERNAME=MATCOM08WK072
> > ComSpec=C:\WINNT\system32\cmd.exe
> > HOMEDRIVE=C:
> > HOMESHARE=\\matcomapps05\users
> > LOGONSERVER=\\MATCOMDC03
> > NUMBER_OF_PROCESSORS=1
> > OS=Windows_NT
> > Os2LibPath=C:\WINNT\system32\os2\dll;
> > Path=d:\docume~1\vinyardwc\gnuwin32\bin;d:\docume~1\vinyardwc\gnu\bin;
> > PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> > PBXHOME=C:\PBrokerX
> > PROCESSOR_ARCHITECTURE=x86
> > PROCESSOR_IDENTIFIER=x86 Family 15 Model 1 Stepping 3, GenuineIntel
> > PROCESSOR_LEVEL=15
> > PROCESSOR_REVISION=0103
> > ProgramFiles=C:\Program Files
> > PROMPT=$P$G
> > RESOURCENAME=DIICOE
> > SMS_LOCAL_DIR=C:\WINNT
> > SMS_LOCAL_DIR_USER=C:\WINNT
> > SystemDrive=C:
> > SystemRoot=C:\WINNT
> > TEMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> > TMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> > UCP_HOME=..\data\UCP
> > USERDOMAIN=MATCOM
> > USERNAME=VinyardWC
> > USERPROFILE=D:\Documents and Settings\vinyardwc
> > windir=C:\WINNT
> > WORKDIRECTORY=C:\Program Files\DII COE Message Processor\bin
> >
> > I use a DELL desktop with Win 2000 Pro.  I have R-1.9.0 installed.  A
> > co-worker has R-1.8.1 installed on a Dell laptop also with Win 2000 Pro.
> He
> > has not experienced any problems. Our network admin push updates over
the
> > net at night.  I assume the netadmin pushes the same updates to both
> > machines.
> >
> > I changed my batch file from:
> >
> > SET HOMEDRIVE=D:
> > SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> > Rgui.exe
> >
> > to:
> >
> > @SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> > @ Start Rgui.exe
> >
> > The second version works.
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From VinyardWC at logcom.usmc.mil  Fri Apr 23 20:13:52 2004
From: VinyardWC at logcom.usmc.mil (Vinyard Maj William C)
Date: Fri, 23 Apr 2004 14:13:52 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
Message-ID: <E85DCCA6ED13F749A27DEF593B2297FB036697AC@matcomexch02.matcom.usmc.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/17e0f138/attachment.pl

From vograno at evafunds.com  Fri Apr 23 20:30:19 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 23 Apr 2004 11:30:19 -0700
Subject: [R] time zones in POSIXt
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3B24@phost015.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040423/d6271edb/attachment.pl

From tpopenfoose at earthlink.net  Fri Apr 23 21:17:48 2004
From: tpopenfoose at earthlink.net (Toby Popenfoose)
Date: Fri, 23 Apr 2004 14:17:48 -0500
Subject: [R] Statistical Quality Control
Message-ID: <001501c42967$ab89ed90$d5b2b241@poweramd>

blake holton wrote

>I've been familiarizing myself with the features of R over the past few
>days. I'm impressed with the quality and quantity of the features and
>packages. One feature that I would be interested in would be a package for
>statistical quality control. Does a package for statistical quality control
>exist that I've been unable to locate?
>

>If not, is anyone aware of efforts to develop a statistical quality control
>package? It has been awhile since I've coded in C, but I would be willing
to
>contribute.


There is the qcc package in CRAN

and there is also the Qtoolbox package at

http://www.cmis.csiro.au/S-PLUS/qtoolbox/R.html



HTH

Toby Popenfoose



From ligges at statistik.uni-dortmund.de  Fri Apr 23 21:42:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 23 Apr 2004 21:42:28 +0200
Subject: [R] Fatal Error: INVALID HOMEDRIVE
References: <E85DCCA6ED13F749A27DEF593B2297FB0366979F@matcomexch02.matcom.usmc.mil>
	<007a01c42956$32bf4a60$160feda9@des.ucdavis.edu>
Message-ID: <408971A4.C5E1FA0F@statistik.uni-dortmund.de>



Brett Melbourne wrote:
> 
> I can also start 1.8.1 and 1.9.0 properly in a similar way to that described
> here by Vinyard, by typing the following at the command prompt or from a
> batch file.
> 
> SET HOMEPATH=\
> "C:\Program Files\R\rw1090\bin\Rgui.exe"
> 
> However, both 1.8.1 and 1.9.0 still fail in the usual way when started from
> the program icon


Well, using "set" in the shell only sets the variable in this current
instance of the shell.
It does not set the variable for all instances. And you are running in
another instance when clicking on an icon ...

You might want to set an appropriate HOMEPATH in your Control Panel (I
think in System).

Uwe Ligges


> (I have confirmed, by SET, before and after failing that
> HOMEPATH is properly set). I observe the same behaviour when HOME=C:\ is
> set.
> 
> Brett
> 
> ----- Original Message -----
> From: "Vinyard Maj William C" <VinyardWC at logcom.usmc.mil>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, April 23, 2004 4:54 AM
> Subject: [R] Fatal Error: INVALID HOMEDRIVE
> 
> > As requested:
> >
> > set > eviron.txt
> >
> > ALLUSERSPROFILE=D:\Documents and Settings\All Users
> > APPDATA=D:\Documents and Settings\vinyardwc\Application Data
> > CMP_HOME=C:\PROGRA~1\DIICOE~1
> > CommonProgramFiles=C:\Program Files\Common Files
> > COMPUTERNAME=MATCOM08WK072
> > ComSpec=C:\WINNT\system32\cmd.exe
> > HOMEDRIVE=C:
> > HOMESHARE=\\matcomapps05\users
> > LOGONSERVER=\\MATCOMDC03
> > NUMBER_OF_PROCESSORS=1
> > OS=Windows_NT
> > Os2LibPath=C:\WINNT\system32\os2\dll;
> > Path=d:\docume~1\vinyardwc\gnuwin32\bin;d:\docume~1\vinyardwc\gnu\bin;
> > PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> > PBXHOME=C:\PBrokerX
> > PROCESSOR_ARCHITECTURE=x86
> > PROCESSOR_IDENTIFIER=x86 Family 15 Model 1 Stepping 3, GenuineIntel
> > PROCESSOR_LEVEL=15
> > PROCESSOR_REVISION=0103
> > ProgramFiles=C:\Program Files
> > PROMPT=$P$G
> > RESOURCENAME=DIICOE
> > SMS_LOCAL_DIR=C:\WINNT
> > SMS_LOCAL_DIR_USER=C:\WINNT
> > SystemDrive=C:
> > SystemRoot=C:\WINNT
> > TEMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> > TMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> > UCP_HOME=..\data\UCP
> > USERDOMAIN=MATCOM
> > USERNAME=VinyardWC
> > USERPROFILE=D:\Documents and Settings\vinyardwc
> > windir=C:\WINNT
> > WORKDIRECTORY=C:\Program Files\DII COE Message Processor\bin
> >
> > I use a DELL desktop with Win 2000 Pro.  I have R-1.9.0 installed.  A
> > co-worker has R-1.8.1 installed on a Dell laptop also with Win 2000 Pro.
> He
> > has not experienced any problems. Our network admin push updates over the
> > net at night.  I assume the netadmin pushes the same updates to both
> > machines.
> >
> > I changed my batch file from:
> >
> > SET HOMEDRIVE=D:
> > SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> > Rgui.exe
> >
> > to:
> >
> > @SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> > @ Start Rgui.exe
> >
> > The second version works.
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From edd at debian.org  Fri Apr 23 22:52:27 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 23 Apr 2004 15:52:27 -0500
Subject: [R] time zones in POSIXt
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3B24@phost015.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A50C3B24@phost015.intermedia.net>
Message-ID: <20040423205227.GA26095@sonny.eddelbuettel.com>


On Fri, Apr 23, 2004 at 11:30:19AM -0700, Vadim Ogranovich wrote:
> Hi,
>  
> I have two data sources. One records time in PST time zone, the other in
> GMT. I want to compute the difference between the two, but don't see
> how. Here is an example where I compute time difference between
> identical times each (meant to be) relative to its time zone.
>  
> > as.POSIXlt("2000-05-10 10:15:00",  "PST") -  as.POSIXlt("2000-05-10
> 10:15:00",  "GMT")
> Time difference of 0 secs
> 
> I was expecting to see 8hrs (which is the time difference between London
> and San-Francisco). Why is it so and what is the correct way of doing
> it?

Seems to work with POSIXct in 1.8.1 and 1.9.0:

> as.POSIXct("2000-05-10 10:15:00",  "PST8PDT") -  as.POSIXct("2000-05-10
10:15:00", tz="UTC")
Time difference of 7 hours


Dirk

>  
>  
> I use R-1.8.1 on RH-7.3.
>  
> Thanks,
> Vadim
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
The relationship between the computed price and reality is as yet unknown.  
                                             -- From the pac(8) manual page



From wcvinyard at earthlink.net  Fri Apr 23 23:09:06 2004
From: wcvinyard at earthlink.net (Bill Vinyard)
Date: Fri, 23 Apr 2004 17:09:06 -0400
Subject: [R] Fatal Error: INVALID HOMEDRIVE
In-Reply-To: <408971A4.C5E1FA0F@statistik.uni-dortmund.de>
Message-ID: <MJENLJEPCHEMCAGNPDMGKEOCCBAA.wcvinyard@earthlink.net>

I set HOMEPATH in "System" as you suggested...the same error occurs when I
try to start Rgui without appending "HOMEPATH=\"

BTW, I have a Dell Win2000 Pro laptop at home with all critical upgrades
installed and don't have the same problem.

The easiest solution appears to be appending the HOMEPATH statement after
the Rgui statement in "Target" of the icon properties dialog.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Uwe Ligges
Sent: Friday, April 23, 2004 15:42
To: Brett Melbourne
Cc: Vinyard Maj William C; r-help at stat.math.ethz.ch
Subject: Re: [R] Fatal Error: INVALID HOMEDRIVE




Brett Melbourne wrote:
>
> I can also start 1.8.1 and 1.9.0 properly in a similar way to that
described
> here by Vinyard, by typing the following at the command prompt or from a
> batch file.
>
> SET HOMEPATH=\
> "C:\Program Files\R\rw1090\bin\Rgui.exe"
>
> However, both 1.8.1 and 1.9.0 still fail in the usual way when started
from
> the program icon


Well, using "set" in the shell only sets the variable in this current
instance of the shell.
It does not set the variable for all instances. And you are running in
another instance when clicking on an icon ...

You might want to set an appropriate HOMEPATH in your Control Panel (I
think in System).

Uwe Ligges


> (I have confirmed, by SET, before and after failing that
> HOMEPATH is properly set). I observe the same behaviour when HOME=C:\ is
> set.
>
> Brett
>
> ----- Original Message -----
> From: "Vinyard Maj William C" <VinyardWC at logcom.usmc.mil>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, April 23, 2004 4:54 AM
> Subject: [R] Fatal Error: INVALID HOMEDRIVE
>
> > As requested:
> >
> > set > eviron.txt
> >
> > ALLUSERSPROFILE=D:\Documents and Settings\All Users
> > APPDATA=D:\Documents and Settings\vinyardwc\Application Data
> > CMP_HOME=C:\PROGRA~1\DIICOE~1
> > CommonProgramFiles=C:\Program Files\Common Files
> > COMPUTERNAME=MATCOM08WK072
> > ComSpec=C:\WINNT\system32\cmd.exe
> > HOMEDRIVE=C:
> > HOMESHARE=\\matcomapps05\users
> > LOGONSERVER=\\MATCOMDC03
> > NUMBER_OF_PROCESSORS=1
> > OS=Windows_NT
> > Os2LibPath=C:\WINNT\system32\os2\dll;
> > Path=d:\docume~1\vinyardwc\gnuwin32\bin;d:\docume~1\vinyardwc\gnu\bin;
> > PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH
> > PBXHOME=C:\PBrokerX
> > PROCESSOR_ARCHITECTURE=x86
> > PROCESSOR_IDENTIFIER=x86 Family 15 Model 1 Stepping 3, GenuineIntel
> > PROCESSOR_LEVEL=15
> > PROCESSOR_REVISION=0103
> > ProgramFiles=C:\Program Files
> > PROMPT=$P$G
> > RESOURCENAME=DIICOE
> > SMS_LOCAL_DIR=C:\WINNT
> > SMS_LOCAL_DIR_USER=C:\WINNT
> > SystemDrive=C:
> > SystemRoot=C:\WINNT
> > TEMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> > TMP=D:\DOCUME~1\VINYAR~1\LOCALS~1\Temp
> > UCP_HOME=..\data\UCP
> > USERDOMAIN=MATCOM
> > USERNAME=VinyardWC
> > USERPROFILE=D:\Documents and Settings\vinyardwc
> > windir=C:\WINNT
> > WORKDIRECTORY=C:\Program Files\DII COE Message Processor\bin
> >
> > I use a DELL desktop with Win 2000 Pro.  I have R-1.9.0 installed.  A
> > co-worker has R-1.8.1 installed on a Dell laptop also with Win 2000 Pro.
> He
> > has not experienced any problems. Our network admin push updates over
the
> > net at night.  I assume the netadmin pushes the same updates to both
> > machines.
> >
> > I changed my batch file from:
> >
> > SET HOMEDRIVE=D:
> > SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> > Rgui.exe
> >
> > to:
> >
> > @SET HOMEPATH=\Documents and Settings\vinyardwc\My Documents\Rwork\work
> > @ Start Rgui.exe
> >
> > The second version works.
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vograno at evafunds.com  Sat Apr 24 02:35:38 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 23 Apr 2004 17:35:38 -0700
Subject: [R] time zones in POSIXt
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A50C3B29@phost015.intermedia.net>

Thank you for the lead, Dirk! Indeed this works on my machine too:

> as.POSIXct("2000-05-10 10:15:00",  tz="PST8PDT") -
as.POSIXct("2000-05-10 10:15:00",  tz="GMT")
Time difference of 7 hours


However when I replace POSIXct by POSIXlt it breaks (this looks like a
bug to me):

> as.POSIXlt("2000-05-10 10:15:00",  tz="PST8PDT") -
as.POSIXlt("2000-05-10 10:15:00",  tz="GMT")
Time difference of 0 secs



Now a couple of new questions:
* how could I learn about appropriate names for time zones? For example
I was using "PST" whereas it seems I had to use either "PST8" or
"PST8PDT". Why "PST" was not good? Is it documented anywhere?

* there seems to be no difference betweeen GMT and BST on my machine
though PST8 and PST8PDT are treated properly:

# PST8 is not identical to PST8PDT
> ISOdatetime(2003, seq(12), 1, 10, 0, 0, tz="PST8") - ISOdatetime(2003,
seq(12), 1, 10, 0, 0, tz="PST8PDT")
Time differences of    0,    0,    0,    0, 3600, 3600, 3600, 3600,
3600, 3600,    0,    0 secs

# GMT0 is identical to BST
> ISOdatetime(2003, seq(12), 1, 10, 0, 0, tz="GMT0") - ISOdatetime(2003,
seq(12), 1, 10, 0, 0, tz="BST")
Time differences of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 secs

Why is such dichotomy?

Thanks,
Vadim






> -----Original Message-----
> From: Dirk Eddelbuettel [mailto:edd at debian.org] 
> Sent: Friday, April 23, 2004 1:52 PM
> To: Vadim Ogranovich
> Cc: R-Help
> Subject: Re: [R] time zones in POSIXt
> 
> 
> 
> On Fri, Apr 23, 2004 at 11:30:19AM -0700, Vadim Ogranovich wrote:
> > Hi,
> >  
> > I have two data sources. One records time in PST time zone, 
> the other 
> > in GMT. I want to compute the difference between the two, but don't 
> > see how. Here is an example where I compute time difference between 
> > identical times each (meant to be) relative to its time zone.
> >  
> > > as.POSIXlt("2000-05-10 10:15:00",  "PST") -  
> as.POSIXlt("2000-05-10
> > 10:15:00",  "GMT")
> > Time difference of 0 secs
> > 
> > I was expecting to see 8hrs (which is the time difference between 
> > London and San-Francisco). Why is it so and what is the 
> correct way of 
> > doing it?
> 
> Seems to work with POSIXct in 1.8.1 and 1.9.0:
> 
> > as.POSIXct("2000-05-10 10:15:00",  "PST8PDT") -  
> > as.POSIXct("2000-05-10
> 10:15:00", tz="UTC")
> Time difference of 7 hours
> 
> 
> Dirk
> 
> >  
> >  
> > I use R-1.8.1 on RH-7.3.
> >  
> > Thanks,
> > Vadim
> >  
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> The relationship between the computed price and reality is as 
> yet unknown.  
>                                              -- From the 
> pac(8) manual page
>



From bamelbourne at ucdavis.edu  Sat Apr 24 03:07:52 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Fri, 23 Apr 2004 18:07:52 -0700
Subject: [R] Fatal Error: INVALID HOMEDRIVE
References: <E85DCCA6ED13F749A27DEF593B2297FB0366979F@matcomexch02.matcom.usmc.mil>
	<007a01c42956$32bf4a60$160feda9@des.ucdavis.edu>
	<408971A4.C5E1FA0F@statistik.uni-dortmund.de>
Message-ID: <002601c42998$92a59730$4e0feda9@des.ucdavis.edu>

I also set HOMEPATH in my System control panel as you suggested, then
rebooted ... the same error occurs. Windows XP appears to be over-riding the
setting because no HOMEPATH appears in the environment even when it's set in
the control panel thus.

But R starts fine with "C:\Program Files\R\rw1090\bin\Rgui.exe" HOMEPATH=\
in the Target field of the icon.

cheers (and thanks for the help)
Brett


>
> You might want to set an appropriate HOMEPATH in your Control Panel (I
> think in System).
>
> Uwe Ligges
>



From maj at stats.waikato.ac.nz  Sat Apr 24 07:16:08 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sat, 24 Apr 2004 17:16:08 +1200
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <008f01c428fa$e76ad5d0$2313eda9@des.ucdavis.edu>
References: <n2cd80tc8a6ontje1ovo7hsnv5jsf2ecr5@4ax.com><Pine.LNX.4.44.0404211838340.24680-100000@gannet.stats>	<1cmd80pvpmhlrkhll66lm6qmegr0c7h6or@4ax.com>
	<008f01c428fa$e76ad5d0$2313eda9@des.ucdavis.edu>
Message-ID: <4089F818.9000800@stats.waikato.ac.nz>

I just thought I would add that everything Brett Melbourne says in the 
next paragraph applies exactly to me!

Brett Melbourne wrote:
> Some more diagnostic information on this problem related to Windows
> critical updates.
> 
> I am running R 1.8.1 and 1.9.0 under Windows XP Pro, on a laptop, which is
> networked at work but frequently used off the network. R starts properly
> sometimes, when attached to the network, but never when disconnected and
> logged on to the same (networked) user profile. R does however, always start
> properly on a non-network profile local to my machine.
> 

Now I will test if I can solve it in the same way as he has.

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From ajayshah at mayin.org  Sat Apr 24 07:09:21 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 24 Apr 2004 10:39:21 +0530
Subject: [R] Moving window regressions - how can I improve this code?
Message-ID: <20040424050921.GI732@igidr.ac.in>

I wrote a function which does "moving window" regressions. E.g. if
there are 100 observations and the window width is 50, then I first
run the regression for observations 1..50, then for 2..51, and so on.

I am extremely pleased with R in my experience with writing this,
since I was able to pass the model as an argument into the function
:-) Forgive me if I sound naive, but that's rocket science to me!!

For a regression with K explanatory variables, I make a matrix with
2*K+2 columns, where I capture:
    K coefficients and K standard errors
    the residual sigma
    R^2

My code is:

   # ------------------------------------------------------------ 
   movingWindowRegression <- function(data, T, width, model, K) {
     results = matrix(nrow=T, ncol=2*K+2)
     for (i in width:T) {
       details <- summary.lm(lm(as.formula(model), data[(i-width+1):i,]))
       n=1;
       for (j in 1:K) {
         results[i, n]   = details$coefficients[j, 1]
         results[i, n+1] = details$coefficients[j, 2]
         n = n + 2
       }
       results[i, n] = details$sigma
       results[i, n+1] = details$r.squared
     }
     return(results)
   }
   
   # Simulate some data for a linear regression
   T = 20
   x = runif(T); y = 2 + 3*x + rnorm(T);
   D = data.frame(x, y)
   
   r = movingWindowRegression(D, T=T, width=10, model="y ~ x", K=2)
   print(r)
   # ------------------------------------------------------------ 

I would be very happy if you could look at this and tell me how to do
things better.

I have two specific questions:

  1. I find it silly that I have to manually pass K and T into the
     function. It would be so much nicer to have:

        r = movingWindowRegression(D,      width=10, model="y ~ x")
     instead of the existing
        r = movingWindowRegression(D, T=T, width=10, model="y ~ x", K=2)

     How can the function inspect the data frame D and learn the
     number of rows?

     How can the function inspect the model specification string and
     learn K, the number of explanatory variables?

  2. "The R way" consists of avoiding loops when the code is
     vectorisable. I am using a loop to copy out from
     details$coefficients into the columns of results[i,]. Is there a
     better way?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From maj at stats.waikato.ac.nz  Sat Apr 24 07:46:37 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sat, 24 Apr 2004 17:46:37 +1200
Subject: [R] Changing Gui preferences
Message-ID: <4089FF3D.6070308@stats.waikato.ac.nz>

I want to modify my Gui preferences to get rid of the MDI toolbar and 
status bar. [ Under Windows XP and R 1.8.1 and 1.9.0.]

When I uncheck the boxes and click "apply" I am told to save the 
preferences and the changes will take effect when restarting R. I click 
"save" and a box comes up to ask me to save Rconsole (I'm not sure where 
this should be saved but I navigate my way to where the old Rconsole was 
and save on top of it. Strangely I am not asked if I want to replace the 
old Rconsole.

Anyway when I re-start there are no changes and I am still cluttered up 
by the MDI bars that I want to get rid of!

Murray


-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From ggrothendieck at myway.com  Sat Apr 24 08:40:47 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 24 Apr 2004 06:40:47 +0000 (UTC)
Subject: [R] time zones in POSIXt
References: <C698D707214E6F4AB39AB7096C3DE5A50C3B24@phost015.intermedia.net>
Message-ID: <loom.20040424T082613-917@post.gmane.org>


I think this is a problem with difftime (which gets called when the
subtraction in your example is invoked).  The first few lines of 
difftime are:

function (time1, time2, tz = "", units = c("auto", "secs", "mins", 
    "hours", "days", "weeks")) 
{
    time1 <- as.POSIXct(time1, tz = tz)
    time2 <- as.POSIXct(time2, tz = tz)

so any POSIXlt timezone gets clobbered with difftime's tz.   



Vadim Ogranovich <vograno <at> evafunds.com> writes:

: 
: Hi,
: 
: I have two data sources. One records time in PST time zone, the other in
: GMT. I want to compute the difference between the two, but don't see
: how. Here is an example where I compute time difference between
: identical times each (meant to be) relative to its time zone.
: 
: > as.POSIXlt("2000-05-10 10:15:00",  "PST") -  as.POSIXlt("2000-05-10
: 10:15:00",  "GMT")
: Time difference of 0 secs
: 
: I was expecting to see 8hrs (which is the time difference between London
: and San-Francisco). Why is it so and what is the correct way of doing
: it?
: 
: I use R-1.8.1 on RH-7.3.
: 
: Thanks,
: Vadim
:  
: 
: 	[[alternative HTML version deleted]]
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ripley at stats.ox.ac.uk  Sat Apr 24 08:47:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Apr 2004 07:47:55 +0100 (BST)
Subject: [R] time zones in POSIXt
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A50C3B29@phost015.intermedia.net>
Message-ID: <Pine.LNX.4.44.0404240725510.17225-100000@gannet.stats>

On Fri, 23 Apr 2004, Vadim Ogranovich wrote:

> Thank you for the lead, Dirk! Indeed this works on my machine too:
> 
> > as.POSIXct("2000-05-10 10:15:00",  tz="PST8PDT") -
> as.POSIXct("2000-05-10 10:15:00",  tz="GMT")
> Time difference of 7 hours
> 
> 
> However when I replace POSIXct by POSIXlt it breaks (this looks like a
> bug to me):
> 
> > as.POSIXlt("2000-05-10 10:15:00",  tz="PST8PDT") -
> as.POSIXlt("2000-05-10 10:15:00",  tz="GMT")
> Time difference of 0 secs

No, it's not a bug.  POSIXlt times are just numeric representations of
broken-down times, and the tzone attribute (which is normally not present)
is just a reminder to you.  (It has not been checked, so this is avoid
nasty surprises if it is wrong.)  Maybe it was a bad idea to allow - for
POSIXlt times, but then we do expect users to read the documentation
(including the code if they are puzzled).

> Now a couple of new questions:
> * how could I learn about appropriate names for time zones? For example
> I was using "PST" whereas it seems I had to use either "PST8" or
> "PST8PDT". Why "PST" was not good? Is it documented anywhere?

Specific to your OS, so I expect it documents it somewhere.

> * there seems to be no difference betweeen GMT and BST on my machine
> though PST8 and PST8PDT are treated properly:
> 
> # PST8 is not identical to PST8PDT
> > ISOdatetime(2003, seq(12), 1, 10, 0, 0, tz="PST8") - ISOdatetime(2003,
> seq(12), 1, 10, 0, 0, tz="PST8PDT")
> Time differences of    0,    0,    0,    0, 3600, 3600, 3600, 3600,
> 3600, 3600,    0,    0 secs
> 
> # GMT0 is identical to BST
> > ISOdatetime(2003, seq(12), 1, 10, 0, 0, tz="GMT0") - ISOdatetime(2003,
> seq(12), 1, 10, 0, 0, tz="BST")
> Time differences of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 secs
> 
> Why is such dichotomy?

Ask your OS designer - timezones are handled by your OS.  I would not 
consider "BST" to be a timezone as it is only defined for half the year.
Windiows thinks I am on `GMT Daylight Time' although that usage is 
otherwise unknown in this country.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Apr 24 08:52:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Apr 2004 07:52:14 +0100 (BST)
Subject: [R] Fatal Error: INVALID HOMEDRIVE
In-Reply-To: <002601c42998$92a59730$4e0feda9@des.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0404240749340.17225-100000@gannet.stats>

On Fri, 23 Apr 2004, Brett Melbourne wrote:

> I also set HOMEPATH in my System control panel as you suggested, then
> rebooted ... the same error occurs. Windows XP appears to be over-riding the
> setting because no HOMEPATH appears in the environment even when it's set in
> the control panel thus.

That was my original hypothesis.

> But R starts fine with "C:\Program Files\R\rw1090\bin\Rgui.exe" HOMEPATH=\
> in the Target field of the icon.

Better to set HOME there

"C:\Program Files\R\rw1090\bin\Rgui.exe" HOME=C:\

in case HOMEDRIVE disappears too.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sat Apr 24 08:53:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 24 Apr 2004 06:53:21 +0000 (UTC)
Subject: [R] Moving window regressions - how can I improve this code?
References: <20040424050921.GI732@igidr.ac.in>
Message-ID: <loom.20040424T085001-151@post.gmane.org>

1. ?nrow
2. ?all.vars

Getting rid of loop was discussed on Mar 22 and there was some debate
on whether or not it was a good idea although it turned out there was
a bug in the code that only the loop free version brought out.  
See archives.

Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: I wrote a function which does "moving window" regressions. E.g. if
: there are 100 observations and the window width is 50, then I first
: run the regression for observations 1..50, then for 2..51, and so on.
: 
: I am extremely pleased with R in my experience with writing this,
: since I was able to pass the model as an argument into the function
:  Forgive me if I sound naive, but that's rocket science to me!!
: 
: For a regression with K explanatory variables, I make a matrix with
: 2*K+2 columns, where I capture:
:     K coefficients and K standard errors
:     the residual sigma
:     R^2
: 
: My code is:
: 
:    # ------------------------------------------------------------ 
:    movingWindowRegression <- function(data, T, width, model, K) {
:      results = matrix(nrow=T, ncol=2*K+2)
:      for (i in width:T) {
:        details <- summary.lm(lm(as.formula(model), data[(i-width+1):i,]))
:        n=1;
:        for (j in 1:K) {
:          results[i, n]   = details$coefficients[j, 1]
:          results[i, n+1] = details$coefficients[j, 2]
:          n = n + 2
:        }
:        results[i, n] = details$sigma
:        results[i, n+1] = details$r.squared
:      }
:      return(results)
:    }
: 
:    # Simulate some data for a linear regression
:    T = 20
:    x = runif(T); y = 2 + 3*x + rnorm(T);
:    D = data.frame(x, y)
: 
:    r = movingWindowRegression(D, T=T, width=10, model="y ~ x", K=2)
:    print(r)
:    # ------------------------------------------------------------ 
: 
: I would be very happy if you could look at this and tell me how to do
: things better.
: 
: I have two specific questions:
: 
:   1. I find it silly that I have to manually pass K and T into the
:      function. It would be so much nicer to have:
: 
:         r = movingWindowRegression(D,      width=10, model="y ~ x")
:      instead of the existing
:         r = movingWindowRegression(D, T=T, width=10, model="y ~ x", K=2)
: 
:      How can the function inspect the data frame D and learn the
:      number of rows?
: 
:      How can the function inspect the model specification string and
:      learn K, the number of explanatory variables?
: 
:   2. "The R way" consists of avoiding loops when the code is
:      vectorisable. I am using a loop to copy out from
:      details$coefficients into the columns of results[i,]. Is there a
:      better way?
:



From ripley at stats.ox.ac.uk  Sat Apr 24 09:06:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Apr 2004 08:06:26 +0100 (BST)
Subject: [R] Changing Gui preferences
In-Reply-To: <4089FF3D.6070308@stats.waikato.ac.nz>
Message-ID: <Pine.LNX.4.44.0404240754030.17225-100000@gannet.stats>

On Sat, 24 Apr 2004, Murray Jorgensen wrote:

> I want to modify my Gui preferences to get rid of the MDI toolbar and 
> status bar. [ Under Windows XP and R 1.8.1 and 1.9.0.]
> 
> When I uncheck the boxes and click "apply" I am told to save the 
> preferences and the changes will take effect when restarting R. I click 
> "save" and a box comes up to ask me to save Rconsole (I'm not sure where 
> this should be saved but I navigate my way to where the old Rconsole was 
> and save on top of it.

?Rconsole tells you where the copy in use is saved.

>  Strangely I am not asked if I want to replace the old Rconsole.

That's programmable, but from this dialog box one would normally want to 
overwrite, no?

> Anyway when I re-start there are no changes and I am still cluttered up 
> by the MDI bars that I want to get rid of!

I think this might be another manifestation of that Microsoft bug, since 
the normal place to look is in the user's home directory.  You need to 
make sure the copy you saved is being used.  (I have just tested this, and 
it does still work.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sat Apr 24 09:08:01 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 24 Apr 2004 07:08:01 +0000 (UTC)
Subject: [R] time zones in POSIXt
References: <C698D707214E6F4AB39AB7096C3DE5A50C3B29@phost015.intermedia.net>
	<Pine.LNX.4.44.0404240725510.17225-100000@gannet.stats>
Message-ID: <loom.20040424T090054-829@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

: > However when I replace POSIXct by POSIXlt it breaks (this looks like a
: > bug to me):
: > 
: > > as.POSIXlt("2000-05-10 10:15:00",  tz="PST8PDT") -
: > as.POSIXlt("2000-05-10 10:15:00",  tz="GMT")
: > Time difference of 0 secs
: 
: No, it's not a bug.  POSIXlt times are just numeric representations of
: broken-down times, and the tzone attribute (which is normally not present)
: is just a reminder to you.  (It has not been checked, so this is avoid
: nasty surprises if it is wrong.)  Maybe it was a bad idea to allow - for
: POSIXlt times, but then we do expect users to read the documentation
: (including the code if they are puzzled).

This this is by design then its a bug by virtue of being a design error.



From maj at stats.waikato.ac.nz  Sat Apr 24 11:05:07 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sat, 24 Apr 2004 21:05:07 +1200
Subject: [R] Changing Gui preferences
In-Reply-To: <Pine.LNX.4.44.0404240754030.17225-100000@gannet.stats>
References: <Pine.LNX.4.44.0404240754030.17225-100000@gannet.stats>
Message-ID: <408A2DC3.1020904@stats.waikato.ac.nz>

Prof Brian Ripley wrote:

> On Sat, 24 Apr 2004, Murray Jorgensen wrote:
> 
> 
>>I want to modify my Gui preferences to get rid of the MDI toolbar and 
>>status bar. [ Under Windows XP and R 1.8.1 and 1.9.0.]
>>
>>When I uncheck the boxes and click "apply" I am told to save the 
>>preferences and the changes will take effect when restarting R. I click 
>>"save" and a box comes up to ask me to save Rconsole (I'm not sure where 
>>this should be saved but I navigate my way to where the old Rconsole was 
>>and save on top of it.
> 
> 
> ?Rconsole tells you where the copy in use is saved.

What it says is:

      There are system copies of these files in 'R_HOME\etc'.  Users can
      have personal copies of the files: these are looked for in the
      location given by the environment variable 'R_USER'. The system
      files are read only if a corresponding personal file is not found.

      If the environment variable 'R_USER' is not set, the R system sets
      it to 'HOME' if that is set (stripping any trailing slash),
      otherwise to '{HOMEDRIVE}{HOMEPATH}' if 'HOMEDRIVE' is set
      otherwise to the working directory.

Now under Control Panel>System>Advanced I find a box listing 
environmental variables. I find no R_HOME or R_USER listed there so 
presumably they are not set.

I'm not sure how to find 'HOME' but I presume that it is the path that 
appears in the 'Change directory' box when you open this through the 
gui. In which case for me 'HOME' is

C:\apps\R\rw1090

Now I have 3 versions of Rconsole near here. They are in

C:\apps\R\rw1090\etc
C:\apps\R\rw1090           and
C:\apps\R

I still don't know how to change my Gui preferences!

 >[...]

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From f.calboli at ucl.ac.uk  Sat Apr 24 13:33:48 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 24 Apr 2004 12:33:48 +0100
Subject: [R] tutorial for lme
Message-ID: <1082806428.2943.11.camel@monkey>

> In particular, I have some difficulties to understand how
specifications
> for random effects and variance structures
> could be specified.
>

I had similar problems myself... until a reply from Douglas Bates to one
of my posts, wich clarified me how to specify the random effects. You
should be able to find it at:

http://finzi.psych.upenn.edu/R/Rhelp02/archive/14199.html

(or search the R site using the keywords Calboli and Crawley, should be
the first hit). I am not an expert, but you are welcome to contact me
off list for any further clarifications I can offer (not many, do not
hold your breath).

regards,

Federico Calboli


-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 209 4286

f.calboli at ucl.ac.uk



From f.calboli at ucl.ac.uk  Sat Apr 24 13:44:17 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 24 Apr 2004 12:44:17 +0100
Subject: [R] anova(my.lme.model, type="marginal") in Pinheiro and Bates
Message-ID: <1082807057.2943.22.camel@monkey>

Dear All,

I was recently discussing the use of 

anova(my.lme.model, type="marginal")

with a colleague. I would like to find where the subject is discussed in
Pinheiro and Bates 2000. I like the book because it is clear and
accessible even for people with limited formal training in Math and
Stats like me, but I failed to find the subject at hand in the index. 

I realize that reading the book cover to cover would do me a lot of
good, but I would find it a bit impractical at the moment... can anyone
please point out where in the book the use of marginal anova is
discussed (if at all)? 

Regards,

Federico Calboli


-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 209 4286

f.calboli at ucl.ac.uk



From cg.pettersson at evp.slu.se  Sat Apr 24 12:47:24 2004
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Sat, 24 Apr 2004 12:47:24 +0200
Subject: [R] Data import, going from 8 to 550 columns
Message-ID: <200404241047.i3OAlOoB025132@mail1.slu.se>

Hello all!

I need to import a NIR dataset into R. It should be quite trivial, but
I 
can??t make it work. (No problems with the text in the beginning, as #
is 
recognised by read.table as the comment sign.)

The thing I can??t get around is the <CR> that ends every line after
column eight as the line in R should be 550 columns wide (including
the JF-number). 
Every new line in R should begin with the "JF2455" and so on.
Naturally it is possible to re-shape the tables in Excel before
import, but it is quite tedious and doesn??t feel right...!

How do I make read.table to just go on reading on the next line when
it comes to <CR>, and how do I make it use the double <CR> followed by
a blank to begin the next line?

The data-file(s) looks like this:


#ID=Samples from soil scanning
#SAMPLE_NUMBERS_PRESENT=Y
#NX_VARIABLES=550
#NY_VARIABLES=0
#FIRST_WAVELENGTH=1300.000000
#LAST_WAVELENGTH=2398.000000
#WAVELENGTH_INCREMENT=2.000000
JF2455  0.4367495 0.4365539 0.4363573 0.4361560 0.4359702 0.4357788 
0.4355963 0.4354126 0.4352311 0.4350726 0.4349101 0.4347557 0.4346097
0.4344587 
0.4343193 0.4341759 0.4340320 0.4338984 0.4337671 0.4336369 0.4335097
0.4333864 
  the original table is 8 columns wide, ended with a <CR>
  sixty four lines removed here....

0.5015950 0.5020472 0.5026294 0.5033303 0.5041344 0.5049909 0.5059010
0.5067372 
0.5075415 0.5082389 0.5089509 0.5095288 0.5101137 0.5106306 0.5111954
0.5116805 
 
 JF2456  0.3604568 0.3600681 0.3596676 0.3592694 0.3588919 0.3585098 
0.3581379 0.3577725 0.3573992 0.3570563 0.3566975 0.3563588 0.3560365
0.3556931 
0.3553730 0.3550543 0.3547286 0.3544230 0.3541073 0.3537982 0.3535004
0.3531921 
0.3529077 0.3526271 0.3523493 0.3520919 0.3518271 0.3515673 0.3513192
0.3510693 
0.3508208 0.3505693 .......

and so on

Thanks!
/CG


CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From hb at maths.lth.se  Sat Apr 24 13:13:41 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sat, 24 Apr 2004 13:13:41 +0200
Subject: [R] Changing Gui preferences
In-Reply-To: <408A2DC3.1020904@stats.waikato.ac.nz>
Message-ID: <007a01c429ed$347a6140$1e0040d5@maths.lth.se>

Hi, a quick reply to help troubleshoot. The below (is "minimal" and)
works on both R v1.8.1 and R v1.9.0 on WinXP Pro:

>From http://www.maths.lth.se/help/R/Rgui/:

"By default all plot windows (The "Figure 2" etc windows) opened in
Rgui are placed within the Rgui frame. Some people prefer this others
don't. An option is to let the main window and all plot windows live
separately. See screenshots to the right. 

This is done by selecting "GUI preferences..." in the "Edit" menu.
Then select "SDI" (instead of "MDI") next to "Single or multiple
windows". The click "Save" and accept the suggested pathname
("Rconsole") by clicking "Save" in the opened file dialog. The click
"Cancel" (yes, it is indeed ok). Restart R to get effectuate the
settings."

My R_USER and HOME as seen by R are:

> Sys.getenv("R_USER")
                          R_USER 
"C:\\Documents and Settings\\hb" 
> Sys.getenv("HOME")
HOME 
  "" 

Maybe it helps you on the way...

Henrik Bengtsson




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray 
> Jorgensen
> Sent: den 24 april 2004 11:05
> To: Prof Brian Ripley; R-help
> Subject: Re: [R] Changing Gui preferences
> 
> 
> Prof Brian Ripley wrote:
> 
> > On Sat, 24 Apr 2004, Murray Jorgensen wrote:
> > 
> > 
> >>I want to modify my Gui preferences to get rid of the MDI 
> toolbar and
> >>status bar. [ Under Windows XP and R 1.8.1 and 1.9.0.]
> >>
> >>When I uncheck the boxes and click "apply" I am told to save the
> >>preferences and the changes will take effect when 
> restarting R. I click 
> >>"save" and a box comes up to ask me to save Rconsole (I'm 
> not sure where 
> >>this should be saved but I navigate my way to where the old 
> Rconsole was 
> >>and save on top of it.
> > 
> > 
> > ?Rconsole tells you where the copy in use is saved.
> 
> What it says is:
> 
>       There are system copies of these files in 'R_HOME\etc'. 
>  Users can
>       have personal copies of the files: these are looked for in the
>       location given by the environment variable 'R_USER'. The
system
>       files are read only if a corresponding personal file is 
> not found.
> 
>       If the environment variable 'R_USER' is not set, the R 
> system sets
>       it to 'HOME' if that is set (stripping any trailing slash),
>       otherwise to '{HOMEDRIVE}{HOMEPATH}' if 'HOMEDRIVE' is set
>       otherwise to the working directory.
> 
> Now under Control Panel>System>Advanced I find a box listing 
> environmental variables. I find no R_HOME or R_USER listed there so 
> presumably they are not set.
> 
> I'm not sure how to find 'HOME' but I presume that it is the 
> path that 
> appears in the 'Change directory' box when you open this through the

> gui. In which case for me 'HOME' is
> 
> C:\apps\R\rw1090
> 
> Now I have 3 versions of Rconsole near here. They are in
> 
> C:\apps\R\rw1090\etc
> C:\apps\R\rw1090           and
> C:\apps\R
> 
> I still don't know how to change my Gui preferences!
> 
>  >[...]
> 
> Murray
> 
> -- 
> Dr Murray Jorgensen
http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New
Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838
4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395
862
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Sat Apr 24 13:15:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Apr 2004 12:15:52 +0100 (BST)
Subject: [R] Data import, going from 8 to 550 columns
In-Reply-To: <200404241047.i3OAlOoB025132@mail1.slu.se>
Message-ID: <Pine.LNX.4.44.0404241214280.24062-100000@gannet.stats>

You cannot use read.table() to read multi-line records as here.
Why?  It is not a table?

You can use scan() to do this.

On Sat, 24 Apr 2004, CG Pettersson wrote:

> Hello all!
> 
> I need to import a NIR dataset into R. It should be quite trivial, but
> I 
> can??t make it work. (No problems with the text in the beginning, as #
> is 
> recognised by read.table as the comment sign.)
> 
> The thing I can??t get around is the <CR> that ends every line after
> column eight as the line in R should be 550 columns wide (including
> the JF-number). 
> Every new line in R should begin with the "JF2455" and so on.
> Naturally it is possible to re-shape the tables in Excel before
> import, but it is quite tedious and doesn??t feel right...!
> 
> How do I make read.table to just go on reading on the next line when
> it comes to <CR>, and how do I make it use the double <CR> followed by
> a blank to begin the next line?
> 
> The data-file(s) looks like this:
> 
> 
> #ID=Samples from soil scanning
> #SAMPLE_NUMBERS_PRESENT=Y
> #NX_VARIABLES=550
> #NY_VARIABLES=0
> #FIRST_WAVELENGTH=1300.000000
> #LAST_WAVELENGTH=2398.000000
> #WAVELENGTH_INCREMENT=2.000000
> JF2455  0.4367495 0.4365539 0.4363573 0.4361560 0.4359702 0.4357788 
> 0.4355963 0.4354126 0.4352311 0.4350726 0.4349101 0.4347557 0.4346097
> 0.4344587 
> 0.4343193 0.4341759 0.4340320 0.4338984 0.4337671 0.4336369 0.4335097
> 0.4333864 
>   the original table is 8 columns wide, ended with a <CR>
>   sixty four lines removed here....
> 
> 0.5015950 0.5020472 0.5026294 0.5033303 0.5041344 0.5049909 0.5059010
> 0.5067372 
> 0.5075415 0.5082389 0.5089509 0.5095288 0.5101137 0.5106306 0.5111954
> 0.5116805 
>  
>  JF2456  0.3604568 0.3600681 0.3596676 0.3592694 0.3588919 0.3585098 
> 0.3581379 0.3577725 0.3573992 0.3570563 0.3566975 0.3563588 0.3560365
> 0.3556931 
> 0.3553730 0.3550543 0.3547286 0.3544230 0.3541073 0.3537982 0.3535004
> 0.3531921 
> 0.3529077 0.3526271 0.3523493 0.3520919 0.3518271 0.3515673 0.3513192
> 0.3510693 
> 0.3508208 0.3505693 .......
> 
> and so on
> 
> Thanks!
> /CG
> 
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences
> Dep. of Ecology and Crop Production. Box 7043
> SE-750 07 Uppsala
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tura at centroin.com.br  Sat Apr 24 14:42:36 2004
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sat, 24 Apr 2004 09:42:36 -0300
Subject: [R] Changing Gui preferences
Message-ID: <6.0.0.22.2.20040424094228.037b7980@pop.centroin.com.br>

At 06:05 24/04/2004, you wrote:

>I'm not sure how to find 'HOME' but I presume that it is the path that appears in the 'Change directory' box when you open this through the gui. In which case for me 'HOME' is
>
>C:\apps\R\rw1090
>
>Now I have 3 versions of Rconsole near here. They are in
>
>C:\apps\R\rw1090\etc
>C:\apps\R\rw1090           and
>C:\apps\R
>
>I still don't know how to change my Gui preferences!

Murray,

I don?t use win XP, I use win 98 SE.
In my system I know de 'HOME' with a simple procedure:

1- Look R shortcut
2- Right Click in R shortcut
3- Choose properties
4- Look satrt in box
5- de directorie in box is the 'HOME'



Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil  

From ripley at stats.ox.ac.uk  Sat Apr 24 15:44:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 24 Apr 2004 14:44:59 +0100 (BST)
Subject: [R] Changing Gui preferences
In-Reply-To: <6.0.0.22.2.20040424094228.037b7980@pop.centroin.com.br>
Message-ID: <Pine.LNX.4.44.0404241444130.24233-100000@gannet.stats>

This *is* in the rw-FAQ.  This answer is correct on Win 98, but not on 
WinXP (when the latter is not broken by a hotfix patch).

On Sat, 24 Apr 2004, Bernardo Rangel Tura wrote:

> At 06:05 24/04/2004, you wrote:
> 
> >I'm not sure how to find 'HOME' but I presume that it is the path that appears in the 'Change directory' box when you open this through the gui. In which case for me 'HOME' is
> >
> >C:\apps\R\rw1090
> >
> >Now I have 3 versions of Rconsole near here. They are in
> >
> >C:\apps\R\rw1090\etc
> >C:\apps\R\rw1090           and
> >C:\apps\R
> >
> >I still don't know how to change my Gui preferences!
> 
> Murray,
> 
> I don??t use win XP, I use win 98 SE.
> In my system I know de 'HOME' with a simple procedure:
> 
> 1- Look R shortcut
> 2- Right Click in R shortcut
> 3- Choose properties
> 4- Look satrt in box
> 5- de directorie in box is the 'HOME'
> 
> 
> 
> Bernardo Rangel Tura, MD, MSc
> National Institute of Cardiology Laranjeiras
> Rio de Janeiro Brazil  
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Sat Apr 24 15:58:21 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 24 Apr 2004 08:58:21 -0500
Subject: [R] Moving window regressions - how can I improve this code?
In-Reply-To: <20040424050921.GI732@igidr.ac.in>
References: <20040424050921.GI732@igidr.ac.in>
Message-ID: <6rvfjpbhqa.fsf@bates4.stat.wisc.edu>

Ajay Shah <ajayshah at mayin.org> writes:

> I wrote a function which does "moving window" regressions. E.g. if
> there are 100 observations and the window width is 50, then I first
> run the regression for observations 1..50, then for 2..51, and so on.
> 
> I am extremely pleased with R in my experience with writing this,
> since I was able to pass the model as an argument into the function
> :-) Forgive me if I sound naive, but that's rocket science to me!!
> 
> For a regression with K explanatory variables, I make a matrix with
> 2*K+2 columns, where I capture:
>     K coefficients and K standard errors
>     the residual sigma
>     R^2
> 
> My code is:
> 
>    # ------------------------------------------------------------ 
>    movingWindowRegression <- function(data, T, width, model, K) {
>      results = matrix(nrow=T, ncol=2*K+2)
>      for (i in width:T) {
>        details <- summary.lm(lm(as.formula(model), data[(i-width+1):i,]))
>        n=1;
>        for (j in 1:K) {
>          results[i, n]   = details$coefficients[j, 1]
>          results[i, n+1] = details$coefficients[j, 2]
>          n = n + 2
>        }
>        results[i, n] = details$sigma
>        results[i, n+1] = details$r.squared
>      }
>      return(results)
>    }
>    
>    # Simulate some data for a linear regression
>    T = 20
>    x = runif(T); y = 2 + 3*x + rnorm(T);
>    D = data.frame(x, y)
>    
>    r = movingWindowRegression(D, T=T, width=10, model="y ~ x", K=2)
>    print(r)
>    # ------------------------------------------------------------ 
> 
> I would be very happy if you could look at this and tell me how to do
> things better.

First, thanks for posting the code.  It takes courage to send your
code to the list like this for public commentary.  However, questions
like this provide instructive examples.

Some comments:

 As Gabor pointed out, there is a generic function nrow that can be
 applied to data frames.

 As a matter of style, it is better to use
    details <- summary(lm(model, data[<row range>,]))
 That is, call the generic function "summary", not the specific name
 of the method "summary.lm".  It is redundant to use summary.lm(lm(...))
 and this construction is not guaranteed to continue to be valid.

 With regard to the looping, I would suggest using a list of summary
 objects as the basic data structure within your function, then
 extracting the pieces that you want from that list using lapply or sapply.

 That is, I would start with 

movingWindow <- function(formula, data, width, ...) {
    nr = nrow(data)
    width = as.integer(width)[1]
    if (width <= 0 || width >= nr)
        stop(paste("width must be in the range 1,...,", nr, sep=""))
    nreg = nr - width
    base = 0:(width - 1)
    sumrys <- lapply(1:nreg,
                     function(st) {
                         summary(lm(formula, data[base + st,]))
                     })
    sumrys
}

  I changed the argument name "model" to "formula" and changed the
  order of the arguments to the standard order used in R modeling
  functions.
 
  You may not want to use this form for very large data sets because
  the raw summaries could be very large.  However this is a place to
  start.
  
  The reason for creating a list is so you can use sapply and lapply to
  extract results from the list.  To get the coefficients and standard
  errors from a summary, use the coef generic and the select columns from
  the result.  For example,

> data(women)
> sumrys = movingWindow(weight ~ height, women, 9)
> unlist(lapply(sumrys, function(sm) sm$sigma))  # sigma values
[1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228
> sapply(sumrys, "[[", "sigma")                  # same thing
[1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228
> sapply(sumrys, function(sm) coef(sm)[,1:2])
             [,1]         [,2]         [,3]         [,4]        [,5]
[1,] -59.77777778 -67.12777778 -73.42222222 -81.97222222 -91.7777778
[2,]   3.00000000   3.11666667   3.21666667   3.35000000   3.5000000
[3,]   3.77647036   2.64466036   3.70537766   4.71400546   5.1522157
[4,]   0.06085806   0.04194352   0.05784947   0.07246601   0.0780042
              [,6]
[1,] -106.12777778
[2,]    3.71666667
[3,]    6.60219186
[4,]    0.09846709
> 

  The last part shows how to get the coefficients estimates and their
  standard errors as columns of a matrix.  I think I would return the
  coefficients and standard errors separately, as in

movingWindow <- function(formula, data, width, ...) {
    nr = nrow(data)
    width = as.integer(width)[1]
    if (width <= 0 || width >= nr)
        stop(paste("width must be in the range 1,...,", nr, sep=""))
    nreg = nr - width
    base = 0:(width - 1)
    sumrys <- lapply(1:nreg,
                     function(st) {
                         summary(lm(formula, data[base + st,]))
                     })
    list(coefficients = sapply(sumrys, function(sm) coef(sm)[,"Estimate"]),
         Std.Error = sapply(sumrys, function(sm) coef(sm)[,"Std. Error"]),
         sigma = sapply(sumrys, "[[", "sigma"),
         r.squared = sapply(sumrys, "[[", "r.squared"))
}

> movingWindow(weight ~ height, women, width = 9)
$coefficients
                 [,1]       [,2]       [,3]      [,4]      [,5]        [,6]
(Intercept) -59.77778 -67.127778 -73.422222 -81.97222 -91.77778 -106.127778
height        3.00000   3.116667   3.216667   3.35000   3.50000    3.716667

$Std.Error
                  [,1]       [,2]       [,3]       [,4]      [,5]       [,6]
(Intercept) 3.77647036 2.64466036 3.70537766 4.71400546 5.1522157 6.60219186
height      0.06085806 0.04194352 0.05784947 0.07246601 0.0780042 0.09846709

$sigma
[1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228

$r.squared
[1] 0.9971276 0.9987338 0.9977411 0.9967352 0.9965351 0.9951107

  The next enhancements come from realizing that you do not need
  to call lm repeatedly.  The lm function involves many steps working
  with the formula and the data frame and optional arguments to
  produce a model matrix and response vector.   You only need to do
  that once.  After that you can call lm.fit on subsets of the rows.

  If you arrange that the call to lm is based on the "matched call" to
  your function then you can get the ability to work with standard
  modeling arguments such as subset and na.action for free.  This may
  seem like an obscure point but there are big gains in having your
  modeling function behave like all the other R modeling functions.
  Thomas Lumley's article on "Standard non-standard evaluation in R"
  (which I would say was available at developer.r-project.org except
  that the developer.r-project.org site is still down) explains this
  in more detail.

  Furthermore, by doing the initial lm fit you find out how many
  coefficients there are in the model and can do a better job of
  checking for a sensible "width" argument.

  There are subtleties in this version of movingWindow2 involving
  manipulation of the arguments in the original call to lm but these
  are explained in the manual page for lm.  You do need to set the
  class and the terms component in the result of lm.fit before you can
  apply summary to it.

movingWindow2 <- function(formula, data, width, ...) {
    mCall = match.call()
    mCall$width = NULL
    mCall[[1]] = as.name("lm")
    mCall$x = mCall$y = TRUE
    bigfit = eval(mCall, parent.frame())
    ncoef = length(coef(bigfit))
    nr = nrow(data)
    width = as.integer(width)[1]
    if (width <= ncoef || width >= nr)
        stop(paste("width must be in the range ", ncoef + 1,
                   ",...,", nr - 1, sep=""))
    y = bigfit$y
    x = bigfit$x
    terms = bigfit$terms
    base = 0:(width - 1)
    sumrys <- lapply(1:(nr - width),
                     function(st) {
                         inds = base + st
                         fit = lm.fit(x[inds,], y[inds])
                         fit$terms = terms
                         class(fit) = "lm"
                         summary(fit)
                     })
    list(coefficients = sapply(sumrys, function(sm) coef(sm)[,"Estimate"]),
         Std.Error = sapply(sumrys, function(sm) coef(sm)[,"Std. Error"]),
         sigma = sapply(sumrys, "[[", "sigma"),
         r.squared = sapply(sumrys, "[[", "r.squared"))
}

> movingWindow2(weight ~ height, women, 9)
$coefficients
                 [,1]       [,2]       [,3]      [,4]      [,5]        [,6]
(Intercept) -59.77778 -67.127778 -73.422222 -81.97222 -91.77778 -106.127778
height        3.00000   3.116667   3.216667   3.35000   3.50000    3.716667

$Std.Error
                  [,1]       [,2]       [,3]       [,4]      [,5]       [,6]
(Intercept) 3.77647036 2.64466036 3.70537766 4.71400546 5.1522157 6.60219186
height      0.06085806 0.04194352 0.05784947 0.07246601 0.0780042 0.09846709

$sigma
[1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228

$r.squared
[1] 0.9971276 0.9987338 0.9977411 0.9967352 0.9965351 0.9951107

  The use of lapply and sapply on lists is a style of programming
  called "functional programming".  The functional programming
  facilities in the S language are not as widely recognized and
  appreciated as they should be.  Phil Spector's book "An Introduction
  to S and S-PLUS" is one place where these are discussed and
  illustrated.

P.S. If building a list of summaries is taking too much memory then
replace summary(fit) by summary(fit)[c("coefficients", "sigma", "r.squared")]



From ggrothendieck at myway.com  Sat Apr 24 17:52:28 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 24 Apr 2004 15:52:28 +0000 (UTC)
Subject: [R] Moving window regressions - how can I improve this code?
References: <20040424050921.GI732@igidr.ac.in>
	<6rvfjpbhqa.fsf@bates4.stat.wisc.edu>
Message-ID: <loom.20040424T172140-508@post.gmane.org>


Douglas Bates produced an extremely useful post so I hesitate to 
add more but here goes anyways. 

1. The bug I referred to in my previous post is still in the code.  
The last data point in women never gets used.  I think this
goes beyond just making a simple error but gets to the point of
how to avoid index arithmetic in the first place due to its
error prone nature and excess complexity.    Using embed()
or running() (the latter is in package gregmisc) would do that although
at the expense of more memory.

2. I find using stopifnot convenient for testing assertions.  There
is also a bug in the assertion code shown (width should be allowed 
to equal nr) and again I think its due to the introduction of complexity.  
The problem is that its harder to get it right when you have to invert 
the assertion condition.  I have recently discovered the R 
stopifnot function which allows one to state assertions without 
inverting them and now use it all the time.   Once you use stopifnot
the assertion becomes somewhat clearer since its not muddied by the
inversion and in my mind it starts to bring out a wider range of
considerations such as whether a width
with zero or fewer degrees of freedom should be allowed.  Perhaps
the condition should be width > length(all.vars(formula)) or if
formulae such as y~x-1 are allowed then a more complex specification.

With these changes (except I've kept width > 0), movingWindow becomes

movingWindow <- function(formula, data, width, ...) {
    nr <- nrow(data)
    width <- as.integer(width)[1]
    stopifnot( width > 0, width <= nr )
    indices <- as.data.frame( t( embed( 1:nr, width ) ) )
    lapply(indices, function(st) summary(lm(formula, data[st,])) )
}

Similar comments could be applied to movingWindow2.


Douglas Bates <bates <at> stat.wisc.edu> writes:

: 
: Ajay Shah <ajayshah <at> mayin.org> writes:
: 
: > I wrote a function which does "moving window" regressions. E.g. if
: > there are 100 observations and the window width is 50, then I first
: > run the regression for observations 1..50, then for 2..51, and so on.
: > 
: > I am extremely pleased with R in my experience with writing this,
: > since I was able to pass the model as an argument into the function
: >  Forgive me if I sound naive, but that's rocket science to me!!
: > 
: > For a regression with K explanatory variables, I make a matrix with
: > 2*K+2 columns, where I capture:
: >     K coefficients and K standard errors
: >     the residual sigma
: >     R^2
: > 
: > My code is:
: > 
: >    # ------------------------------------------------------------ 
: >    movingWindowRegression <- function(data, T, width, model, K) {
: >      results = matrix(nrow=T, ncol=2*K+2)
: >      for (i in width:T) {
: >        details <- summary.lm(lm(as.formula(model), data[(i-width+1):i,]))
: >        n=1;
: >        for (j in 1:K) {
: >          results[i, n]   = details$coefficients[j, 1]
: >          results[i, n+1] = details$coefficients[j, 2]
: >          n = n + 2
: >        }
: >        results[i, n] = details$sigma
: >        results[i, n+1] = details$r.squared
: >      }
: >      return(results)
: >    }
: >    
: >    # Simulate some data for a linear regression
: >    T = 20
: >    x = runif(T); y = 2 + 3*x + rnorm(T);
: >    D = data.frame(x, y)
: >    
: >    r = movingWindowRegression(D, T=T, width=10, model="y ~ x", K=2)
: >    print(r)
: >    # ------------------------------------------------------------ 
: > 
: > I would be very happy if you could look at this and tell me how to do
: > things better.
: 
: First, thanks for posting the code.  It takes courage to send your
: code to the list like this for public commentary.  However, questions
: like this provide instructive examples.
: 
: Some comments:
: 
:  As Gabor pointed out, there is a generic function nrow that can be
:  applied to data frames.
: 
:  As a matter of style, it is better to use
:     details <- summary(lm(model, data[<row range>,]))
:  That is, call the generic function "summary", not the specific name
:  of the method "summary.lm".  It is redundant to use summary.lm(lm(...))
:  and this construction is not guaranteed to continue to be valid.
: 
:  With regard to the looping, I would suggest using a list of summary
:  objects as the basic data structure within your function, then
:  extracting the pieces that you want from that list using lapply or sapply.
: 
:  That is, I would start with 
: 
: movingWindow <- function(formula, data, width, ...) {
:     nr = nrow(data)
:     width = as.integer(width)[1]
:     if (width <= 0 || width >= nr)
:         stop(paste("width must be in the range 1,...,", nr, sep=""))
:     nreg = nr - width
:     base = 0:(width - 1)
:     sumrys <- lapply(1:nreg,
:                      function(st) {
:                          summary(lm(formula, data[base + st,]))
:                      })
:     sumrys
: }
: 
:   I changed the argument name "model" to "formula" and changed the
:   order of the arguments to the standard order used in R modeling
:   functions.
: 
:   You may not want to use this form for very large data sets because
:   the raw summaries could be very large.  However this is a place to
:   start.
: 
:   The reason for creating a list is so you can use sapply and lapply to
:   extract results from the list.  To get the coefficients and standard
:   errors from a summary, use the coef generic and the select columns from
:   the result.  For example,
: 
: > data(women)
: > sumrys = movingWindow(weight ~ height, women, 9)
: > unlist(lapply(sumrys, function(sm) sm$sigma))  # sigma values
: [1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228
: > sapply(sumrys, "[[", "sigma")                  # same thing
: [1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228
: > sapply(sumrys, function(sm) coef(sm)[,1:2])
:              [,1]         [,2]         [,3]         [,4]        [,5]
: [1,] -59.77777778 -67.12777778 -73.42222222 -81.97222222 -91.7777778
: [2,]   3.00000000   3.11666667   3.21666667   3.35000000   3.5000000
: [3,]   3.77647036   2.64466036   3.70537766   4.71400546   5.1522157
: [4,]   0.06085806   0.04194352   0.05784947   0.07246601   0.0780042
:               [,6]
: [1,] -106.12777778
: [2,]    3.71666667
: [3,]    6.60219186
: [4,]    0.09846709
: > 
: 
:   The last part shows how to get the coefficients estimates and their
:   standard errors as columns of a matrix.  I think I would return the
:   coefficients and standard errors separately, as in
: 
: movingWindow <- function(formula, data, width, ...) {
:     nr = nrow(data)
:     width = as.integer(width)[1]
:     if (width <= 0 || width >= nr)
:         stop(paste("width must be in the range 1,...,", nr, sep=""))
:     nreg = nr - width
:     base = 0:(width - 1)
:     sumrys <- lapply(1:nreg,
:                      function(st) {
:                          summary(lm(formula, data[base + st,]))
:                      })
:     list(coefficients = sapply(sumrys, function(sm) coef(sm)[,"Estimate"]),
:          Std.Error = sapply(sumrys, function(sm) coef(sm)[,"Std. Error"]),
:          sigma = sapply(sumrys, "[[", "sigma"),
:          r.squared = sapply(sumrys, "[[", "r.squared"))
: }
: 
: > movingWindow(weight ~ height, women, width = 9)
: $coefficients
:                  [,1]       [,2]       [,3]      [,4]      [,5]        [,6]
: (Intercept) -59.77778 -67.127778 -73.422222 -81.97222 -91.77778 -106.127778
: height        3.00000   3.116667   3.216667   3.35000   3.50000    3.716667
: 
: $Std.Error
:                   [,1]       [,2]       [,3]       [,4]      [,5]       [,6]
: (Intercept) 3.77647036 2.64466036 3.70537766 4.71400546 5.1522157 6.60219186
: height      0.06085806 0.04194352 0.05784947 0.07246601 0.0780042 0.09846709
: 
: $sigma
: [1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228
: 
: $r.squared
: [1] 0.9971276 0.9987338 0.9977411 0.9967352 0.9965351 0.9951107
: 
:   The next enhancements come from realizing that you do not need
:   to call lm repeatedly.  The lm function involves many steps working
:   with the formula and the data frame and optional arguments to
:   produce a model matrix and response vector.   You only need to do
:   that once.  After that you can call lm.fit on subsets of the rows.
: 
:   If you arrange that the call to lm is based on the "matched call" to
:   your function then you can get the ability to work with standard
:   modeling arguments such as subset and na.action for free.  This may
:   seem like an obscure point but there are big gains in having your
:   modeling function behave like all the other R modeling functions.
:   Thomas Lumley's article on "Standard non-standard evaluation in R"
:   (which I would say was available at developer.r-project.org except
:   that the developer.r-project.org site is still down) explains this
:   in more detail.
: 
:   Furthermore, by doing the initial lm fit you find out how many
:   coefficients there are in the model and can do a better job of
:   checking for a sensible "width" argument.
: 
:   There are subtleties in this version of movingWindow2 involving
:   manipulation of the arguments in the original call to lm but these
:   are explained in the manual page for lm.  You do need to set the
:   class and the terms component in the result of lm.fit before you can
:   apply summary to it.
: 
: movingWindow2 <- function(formula, data, width, ...) {
:     mCall = match.call()
:     mCall$width = NULL
:     mCall[[1]] = as.name("lm")
:     mCall$x = mCall$y = TRUE
:     bigfit = eval(mCall, parent.frame())
:     ncoef = length(coef(bigfit))
:     nr = nrow(data)
:     width = as.integer(width)[1]
:     if (width <= ncoef || width >= nr)
:         stop(paste("width must be in the range ", ncoef + 1,
:                    ",...,", nr - 1, sep=""))
:     y = bigfit$y
:     x = bigfit$x
:     terms = bigfit$terms
:     base = 0:(width - 1)
:     sumrys <- lapply(1:(nr - width),
:                      function(st) {
:                          inds = base + st
:                          fit = lm.fit(x[inds,], y[inds])
:                          fit$terms = terms
:                          class(fit) = "lm"
:                          summary(fit)
:                      })
:     list(coefficients = sapply(sumrys, function(sm) coef(sm)[,"Estimate"]),
:          Std.Error = sapply(sumrys, function(sm) coef(sm)[,"Std. Error"]),
:          sigma = sapply(sumrys, "[[", "sigma"),
:          r.squared = sapply(sumrys, "[[", "r.squared"))
: }
: 
: > movingWindow2(weight ~ height, women, 9)
: $coefficients
:                  [,1]       [,2]       [,3]      [,4]      [,5]        [,6]
: (Intercept) -59.77778 -67.127778 -73.422222 -81.97222 -91.77778 -106.127778
: height        3.00000   3.116667   3.216667   3.35000   3.50000    3.716667
: 
: $Std.Error
:                   [,1]       [,2]       [,3]       [,4]      [,5]       [,6]
: (Intercept) 3.77647036 2.64466036 3.70537766 4.71400546 5.1522157 6.60219186
: height      0.06085806 0.04194352 0.05784947 0.07246601 0.0780042 0.09846709
: 
: $sigma
: [1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228
: 
: $r.squared
: [1] 0.9971276 0.9987338 0.9977411 0.9967352 0.9965351 0.9951107
: 
:   The use of lapply and sapply on lists is a style of programming
:   called "functional programming".  The functional programming
:   facilities in the S language are not as widely recognized and
:   appreciated as they should be.  Phil Spector's book "An Introduction
:   to S and S-PLUS" is one place where these are discussed and
:   illustrated.
: 
: P.S. If building a list of summaries is taking too much memory then
: replace summary(fit) by summary(fit)[c("coefficients", "sigma", "r.squared")]
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From son.jaa at gmx.de  Sat Apr 24 19:03:56 2004
From: son.jaa at gmx.de (Sonja Dornieden)
Date: Sat, 24 Apr 2004 19:03:56 +0200
Subject: [R] Modalwert
Message-ID: <000501c42a1e$21eb49f0$fe79a8c0@amd26>

Hai -
kann mir jemand sagen, wie ich den Modalwert in R berechne?! IRgendwie finde
ich den Befehl nicht....
greetz und herzlichen Dank
Sonja



From baron at psych.upenn.edu  Sat Apr 24 19:12:40 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 24 Apr 2004 13:12:40 -0400
Subject: [R] Modalwert
In-Reply-To: <000501c42a1e$21eb49f0$fe79a8c0@amd26>
References: <000501c42a1e$21eb49f0$fe79a8c0@amd26>
Message-ID: <20040424171240.GA28524@psych>

On 04/24/04 19:03, Sonja Dornieden wrote:
>Hai -
>kann mir jemand sagen, wie ich den Modalwert in R berechne?! IRgendwie finde
>ich den Befehl nicht....

Veilleicht
which.max()

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From bates at stat.wisc.edu  Sat Apr 24 19:24:39 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 24 Apr 2004 12:24:39 -0500
Subject: [R] Moving window regressions - how can I improve this code?
In-Reply-To: <loom.20040424T172140-508@post.gmane.org>
References: <20040424050921.GI732@igidr.ac.in>
	<6rvfjpbhqa.fsf@bates4.stat.wisc.edu>
	<loom.20040424T172140-508@post.gmane.org>
Message-ID: <6rk705b86g.fsf@bates4.stat.wisc.edu>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> Douglas Bates produced an extremely useful post so I hesitate to 
> add more but here goes anyways. 
> 
> 1. The bug I referred to in my previous post is still in the code.  
> The last data point in women never gets used.  I think this
> goes beyond just making a simple error but gets to the point of
> how to avoid index arithmetic in the first place due to its
> error prone nature and excess complexity.    Using embed()
> or running() (the latter is in package gregmisc) would do that although
> at the expense of more memory.

You're right.  I miscounted and it is easy to do that when
manipulating the indices explicitly.  I wasn't aware of the embed
function but, now that you have pointed it out, I agree that it should
be used here.

> 2. I find using stopifnot convenient for testing assertions.  There
> is also a bug in the assertion code shown (width should be allowed 
> to equal nr) and again I think its due to the introduction of complexity.  
> The problem is that its harder to get it right when you have to invert 
> the assertion condition.  I have recently discovered the R 
> stopifnot function which allows one to state assertions without 
> inverting them and now use it all the time.   Once you use stopifnot
> the assertion becomes somewhat clearer since its not muddied by the
> inversion and in my mind it starts to bring out a wider range of
> considerations such as whether a width
> with zero or fewer degrees of freedom should be allowed.  Perhaps
> the condition should be width > length(all.vars(formula)) or if
> formulae such as y~x-1 are allowed then a more complex specification.

I agree that stopifnot is the best construction for assertions.
Thanks for pointing that out.  It is one of the very useful utilities
added by Martin Maechler, who also wrote the str function which gets
my vote for the best debugging tool in R.

If you don't know the total number of coefficients then you would need
to assert only  width > 0.  If you do know the number of coefficients
then you can assert width >= ncoef.

> With these changes (except I've kept width > 0), movingWindow becomes
> 
> movingWindow <- function(formula, data, width, ...) {
>     nr <- nrow(data)
>     width <- as.integer(width)[1]
>     stopifnot( width > 0, width <= nr )
>     indices <- as.data.frame( t( embed( 1:nr, width ) ) )
>     lapply(indices, function(st) summary(lm(formula, data[st,])) )
> }
> 
> Similar comments could be applied to movingWindow2.

Adopting your suggestions I would change the movingWindow2 code to

movingWindow2 <- function(formula, data, width, ...) {
    mCall = match.call()
    mCall$width = NULL
    mCall[[1]] = as.name("lm")
    mCall$x = mCall$y = TRUE
    bigfit = eval(mCall, parent.frame())
    ncoef = length(coef(bigfit))
    nr = nrow(data)
    width = as.integer(width)[1]
    stopifnot(width >= ncoef, width <= nr)
    y = bigfit$y
    x = bigfit$x
    terms = bigfit$terms
    inds = embed(seq(nr), width)[, rev(seq(width))]
    sumrys <- lapply(seq(nrow(inds)),
                     function(st) {
                         ind = inds[st,]
                         fit = lm.fit(x[ind,], y[ind])
                         fit$terms = terms
                         class(fit) = "lm"
                         summary(fit)
                     })
    list(coefficients = sapply(sumrys, function(sm) coef(sm)[,"Estimate"]),
         Std.Error = sapply(sumrys, function(sm) coef(sm)[,"Std. Error"]),
         sigma = sapply(sumrys, "[[", "sigma"),
         r.squared = sapply(sumrys, "[[", "r.squared"))
}

for which the test gives

> data(women)
> movingWindow2(weight ~ height, women, 9)
$coefficients
                 [,1]       [,2]       [,3]      [,4]      [,5]        [,6]
(Intercept) -59.77778 -67.127778 -73.422222 -81.97222 -91.77778 -106.127778
height        3.00000   3.116667   3.216667   3.35000   3.50000    3.716667
                   [,7]
(Intercept) -122.955556
height         3.966667

$Std.Error
                  [,1]       [,2]       [,3]       [,4]      [,5]       [,6]
(Intercept) 3.77647036 2.64466036 3.70537766 4.71400546 5.1522157 6.60219186
height      0.06085806 0.04194352 0.05784947 0.07246601 0.0780042 0.09846709
                 [,7]
(Intercept) 7.7792788
height      0.1143188

$sigma
[1] 0.4714045 0.3248931 0.4481000 0.5613193 0.6042180 0.7627228 0.8855094

$r.squared
[1] 0.9971276 0.9987338 0.9977411 0.9967352 0.9965351 0.9951107 0.9942195



From m_nica at hotmail.com  Sat Apr 24 21:33:10 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Sat, 24 Apr 2004 14:33:10 -0500
Subject: [R] Colour coding and point types in a plot
Message-ID: <BAY18-DAV21R7vA17hq0000286d@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040424/d4ba9799/attachment.pl

From kimai at Princeton.Edu  Sat Apr 24 21:36:46 2004
From: kimai at Princeton.Edu (Kosuke Imai)
Date: Sat, 24 Apr 2004 15:36:46 -0400 (EDT)
Subject: [R] determinant via Lapack
Message-ID: <Pine.LNX.4.44.0404241534030.3233-100000@wws-6qcbw21.Princeton.EDU>

Hi,
  What Lapack routine(s) should I use to calculate the determinant of a
symmetric matrix of real numbers? I would appreciate any guidance on this
issue.
Thanks,
Kosuke

---------------------------------------------------------
Kosuke Imai               Office: Corwin Hall 041
Assistant Professor       Phone: 609-258-6601 (Direct)
Department of Politics    Fax: 609-258-1110 (Department)
Princeton University      Email: kimai at Princeton.Edu
Princeton, NJ 08544-1012  http://www.princeton.edu/~kimai



From maj at stats.waikato.ac.nz  Sat Apr 24 23:13:11 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Sun, 25 Apr 2004 09:13:11 +1200
Subject: [R] Changing Gui preferences
In-Reply-To: <007a01c429ed$347a6140$1e0040d5@maths.lth.se>
References: <007a01c429ed$347a6140$1e0040d5@maths.lth.se>
Message-ID: <408AD867.20907@stats.waikato.ac.nz>

Hi Henrik,

thanks for this. My error had been in not accepting the location "C:\"
suggested by R for Rconsole.

It seems that my R_USER was indeed C: so I was rejecting the correct 
place for Rconsole:

 > Sys.getenv("R_USER")
R_USER
   "C:"

Cheers,

Murray\
Henrik Bengtsson wrote:

> Hi, a quick reply to help troubleshoot. The below (is "minimal" and)
> works on both R v1.8.1 and R v1.9.0 on WinXP Pro:
> 
>>From http://www.maths.lth.se/help/R/Rgui/:
> 
> "By default all plot windows (The "Figure 2" etc windows) opened in
> Rgui are placed within the Rgui frame. Some people prefer this others
> don't. An option is to let the main window and all plot windows live
> separately. See screenshots to the right. 
> 
> This is done by selecting "GUI preferences..." in the "Edit" menu.
> Then select "SDI" (instead of "MDI") next to "Single or multiple
> windows". The click "Save" and accept the suggested pathname
> ("Rconsole") by clicking "Save" in the opened file dialog. The click
> "Cancel" (yes, it is indeed ok). Restart R to get effectuate the
> settings."
> 
> My R_USER and HOME as seen by R are:
> 
> 
>>Sys.getenv("R_USER")
> 
>                           R_USER 
> "C:\\Documents and Settings\\hb" 
> 
>>Sys.getenv("HOME")
> 
> HOME 
>   "" 
> 
> Maybe it helps you on the way...
> 
> Henrik Bengtsson
> 
> 
> 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Murray 
>>Jorgensen
>>Sent: den 24 april 2004 11:05
>>To: Prof Brian Ripley; R-help
>>Subject: Re: [R] Changing Gui preferences
>>
>>
>>Prof Brian Ripley wrote:
>>
>>
>>>On Sat, 24 Apr 2004, Murray Jorgensen wrote:
>>>
>>>
>>>
>>>>I want to modify my Gui preferences to get rid of the MDI 
>>
>>toolbar and
>>
>>>>status bar. [ Under Windows XP and R 1.8.1 and 1.9.0.]
>>>>
>>>>When I uncheck the boxes and click "apply" I am told to save the
>>>>preferences and the changes will take effect when 
>>
>>restarting R. I click 
>>
>>>>"save" and a box comes up to ask me to save Rconsole (I'm 
>>
>>not sure where 
>>
>>>>this should be saved but I navigate my way to where the old 
>>
>>Rconsole was 
>>
>>>>and save on top of it.
>>>
>>>
>>>?Rconsole tells you where the copy in use is saved.
>>
>>What it says is:
>>
>>      There are system copies of these files in 'R_HOME\etc'. 
>> Users can
>>      have personal copies of the files: these are looked for in the
>>      location given by the environment variable 'R_USER'. The
> 
> system
> 
>>      files are read only if a corresponding personal file is 
>>not found.
>>
>>      If the environment variable 'R_USER' is not set, the R 
>>system sets
>>      it to 'HOME' if that is set (stripping any trailing slash),
>>      otherwise to '{HOMEDRIVE}{HOMEPATH}' if 'HOMEDRIVE' is set
>>      otherwise to the working directory.
>>
>>Now under Control Panel>System>Advanced I find a box listing 
>>environmental variables. I find no R_HOME or R_USER listed there so 
>>presumably they are not set.
>>
>>I'm not sure how to find 'HOME' but I presume that it is the 
>>path that 
>>appears in the 'Change directory' box when you open this through the
> 
> 
>>gui. In which case for me 'HOME' is
>>
>>C:\apps\R\rw1090
>>
>>Now I have 3 versions of Rconsole near here. They are in
>>
>>C:\apps\R\rw1090\etc
>>C:\apps\R\rw1090           and
>>C:\apps\R
>>
>>I still don't know how to change my Gui preferences!
>>
>> >[...]
>>
>>Murray
>>
>>-- 
>>Dr Murray Jorgensen
> 
> http://www.stats.waikato.ac.nz/Staff/maj.html
> 
>>Department of Statistics, University of Waikato, Hamilton, New
> 
> Zealand
> 
>>Email: maj at waikato.ac.nz                                Fax 7 838
> 
> 4155
> 
>>Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395
> 
> 862
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
>>PLEASE 
>>do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From dmurdoch at pair.com  Sun Apr 25 01:53:34 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 24 Apr 2004 19:53:34 -0400
Subject: [R] Error with 1.9.0 - Invalid HOMEDRIVE
In-Reply-To: <F9D084AF922BC24AA87F674DE1E6850E3BAA03@exchange.usca.edu>
References: <F9D084AF922BC24AA87F674DE1E6850E3BAA03@exchange.usca.edu>
Message-ID: <asul809bf79rl7tdtt2ohvqh9tgmatu781@4ax.com>

On Tue, 20 Apr 2004 11:41:26 -0400, "Joann Williamson"
<JoannW at usca.edu> wrote:
> "Fatal Error: Invalid HOMEDRIVE". 

I've now put two fixes in place for this.

In r-patched (that will probably become 1.9.1), I've just put in a
workaround so that R will start even when Windows isn't supplying
HOMEDRIVE and HOMEPATH as it should.  Those users who have this
problem may find that it is inconsistent about which directory it
chooses for R_USER:  if HOMEDRIVE and HOMEPATH are present, it will
use those, if not, it will use the current directory.

In r-devel (that will become 2.0.0 in the fall), I've changed the
logic slightly, to use the Windows "personal" directory (normally 
"C:\Documents and Settings\username\My Documents" in XP) if possible.
This should be consistent, and if the documentation is correct, should
allow "roaming" users to keep their settings as they move from machine
to machine.

I'd appreciate it if anyone who has had these problems could try one
or both of the new versions.  They should be available for download
from CRAN tomorrow.  Go to
<http://cran.r-project.org/bin/windows/base> and follow the links near
the top of the page to r-patched or r-devel.

If you don't want to use the test versions, then the workaround with
1.9.0 is to put "R_USER=some valid path" in the command line that
starts R.

Thanks!

Duncan Murdoch



From cepl at surfbest.net  Sun Apr 25 04:43:11 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Sat, 24 Apr 2004 22:43:11 -0400
Subject: dd [Was: Re: [R] Selection of cities sample]
In-Reply-To: <200404230437.28129.cepl@surfbest.net>
References: <200404220056.46149.cepl@surfbest.net>
	<200404230437.28129.cepl@surfbest.net>
Message-ID: <200404242243.11423.cepl@surfbest.net>

On Friday 23 of April 2004 04:37, Matej Cepl wrote:
> I have a question, how to most properly select set of cities 
> which would be as similar as possible in some particular 
> variables with the City of Boston (which I use as my base
> line).  

Hi,

how to weigh variables used in daisy function? After week spent 
with MASS, Crawley (2002), and Gordon (1999), I finished with 
this function (which is actually not a real function but just 
convenient packaging of one complex expression):

function(x) {
   require(cluster)
   return(hclust(daisy(
      as.matrix(x),
      metric="euclidean",
      stand=TRUE),
      method="average")
   )
}

When plotting this I got a huge tree (available in PDF on http://
www.ceplovi.cz/matej/tmp/mctree.pdf), which seems to be very 
helpful, because by selecting particular cluster I get my group 
of cities to use as a sample. Would anybody be so kind and 
comment on this code, please?

Now, I would love to weigh some variables in a dataframe used for 
calculation (because I am more concerned with some variables 
more than with others, which should be included with lower 
weigh). In help("daisy") I found this:

	If 'nok' is the number of nonzero weights, the
	dissimilarity is multiplied by the factor '1/nok' and thus
	ranges between 0 and 1.

Do I understand correctly that this allows weighing of 
non-interval (non-continuous) variables? If yes, how can weigh 
variables, which are interval (whole my table is from counts and 
two percent variables)?

	Thanks for any reply,

		Matej Cepl

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
Just remember, brothers and sisters--their skins may be white,
but their souls are just as black as ours!
   -- a black preacher



From shitao at hotmail.com  Sun Apr 25 05:16:48 2004
From: shitao at hotmail.com (Tao Shi)
Date: Sun, 25 Apr 2004 03:16:48 +0000
Subject: [R] a simple suggestion for the next version of R windows
Message-ID: <Sea2-F34IVegqpkL6yk0004641f@hotmail.com>

Is it possible to replace the word "R Console" on the title bar (is it what 
it's called? It's the blue area above menu bar) with the name of the work 
space file you're using or loaded, so people who are runing multple R 
sessions at same time can identify them immediately.  I'm using 1.9.0 in SDI 
mode.

Thanks,

...Tao

_________________________________________________________________




From shitao at hotmail.com  Sun Apr 25 05:34:47 2004
From: shitao at hotmail.com (Tao Shi)
Date: Sun, 25 Apr 2004 03:34:47 +0000
Subject: [R] RE: [Rd] a simple suggestion for the next version of R windows
Message-ID: <Sea2-F34GykvlK6ZuAw000464b1@hotmail.com>

Hi, Andy:

Thanks!  I think it also depends on people's working hobby.

What you suggested is a good way around it.  But I'm still thinking since 
the R window already has the big blue R logo to identify itself, the word "R 
console" is really redundant and could be replaced by something more 
informative.  Not nesessarly everytime you change to a new diretory, you 
need to change to a new identifier, but at least the every first .RData file 
you loaded.  I don't know.......  Something along that line.  It will be 
very helpful when you use Alt+Tab to move between windows, b/c all you see 
are "R console"s.......

...Tao


----Original Message Follows----
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "'Tao Shi'" <shitao at hotmail.com>,r-help-request at stat.math.ethz.ch
CC: r-devel at stat.math.ethz.ch
Subject: RE: [Rd] a simple suggestion for the next version of R windows
Date: Sat, 24 Apr 2004 23:18:06 -0400

I don't see how this can work.  I frequently run only one R session (also in
SDI), but change working directories several times during the session, with
or without loading the workspace files in those directories, depending on
what I need to do.  I don't think the Window title can change every time I
do setwd("somewhereElse"); load(".RData").

One possibility that could probably help you is to change the R command
prompt from "> " to the current working directory plus "> ".  I believe this
can be done with the taskCallbackManager().

Andy

 > From: Tao Shi
 >
 > Is it possible to replace the word "R Console" on the title
 > bar (is it what
 > it's called? It's the blue area above menu bar) with the name
 > of the work
 > space file you're using or loaded, so people who are runing multple R
 > sessions at same time can identify them immediately.  I'm
 > using 1.9.0 in SDI
 > mode.
 >
 > Thanks,
 >
 > ...Tao
 >
 > _________________________________________________________________
 > Is your PC infected? Get a FREE online computer virus scan
 > from McAfee(r)
 > Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963
 >
 > ______________________________________________
 > R-devel at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-devel
 >
 >


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Sun Apr 25 05:35:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 24 Apr 2004 23:35:30 -0400
Subject: [R] determinant via Lapack
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C7B@usrymx25.merck.com>

This is not an R question, is it?  You'd do better by consulting the Lapack
manual.

Googling around turns up
http://www.ma.utexas.edu/documentation/lapack/node134.html, which I think is
enough for you.

Andy

> From: Kosuke Imai
> 
> Hi,
>   What Lapack routine(s) should I use to calculate the 
> determinant of a
> symmetric matrix of real numbers? I would appreciate any 
> guidance on this
> issue.
> Thanks,
> Kosuke
> 
> ---------------------------------------------------------
> Kosuke Imai               Office: Corwin Hall 041
> Assistant Professor       Phone: 609-258-6601 (Direct)
> Department of Politics    Fax: 609-258-1110 (Department)
> Princeton University      Email: kimai at Princeton.Edu
> Princeton, NJ 08544-1012  http://www.princeton.edu/~kimai
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From yanw at stat.berkeley.edu  Sun Apr 25 10:51:24 2004
From: yanw at stat.berkeley.edu (Yan Wang)
Date: Sun, 25 Apr 2004 01:51:24 -0700
Subject: [R] multipanel display of levelplots? 
Message-ID: <408B7C0C.7060503@stat.berkeley.edu>

I cannot achieve a multipanel display of several levelplots. Here is 
part of my code.

At first, create the layout:
push.viewport(viewport(layout=grid.layout(1, 2)))

Then for the left panel:
push.viewport(viewport(layout.pos.col=1,layout.pos.row=1))
push.viewport(viewport(width=0.6, height=0.6))
levelplot(z~x*y, grid)
pop.viewport()

Similar code for the right panel.

However, there is always error message for "pop.viewport()" as "Error in 
pop.vp(i == n, recording) : Illegal to pop top-level viewport". If I 
omit "pop.viewport()", the two levelplots were printed on seperate pages 
without a layout of (1,2).

Any suggestion is greatly appreciated.



From tpapp at axelero.hu  Sun Apr 25 09:49:25 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sun, 25 Apr 2004 09:49:25 +0200
Subject: [R] Colour coding and point types in a plot
In-Reply-To: <BAY18-DAV21R7vA17hq0000286d@hotmail.com>
References: <BAY18-DAV21R7vA17hq0000286d@hotmail.com>
Message-ID: <20040425074925.GA1397@localhost>

On Sat, Apr 24, 2004 at 02:33:10PM -0500, Mihai Nica wrote:

> R 1.8.1, Windows 2000.

You might want to upgrade to 1.9.0.

> I am trying to find the "legend" for color coding and point types in
> a plot, which probably are standard for everybody but myself. Thanks
> for the tip!

> help.search("legend")

Help files with alias or concept or title matching 'legend' using
fuzzy matching:



gauss.quad(gss)         Generating Gauss-Legendre Quadrature
legend(graphics)        Add Legends to Plots
grid.legend(grid)       Internal Grid Functions
grid.plot.and.legend(grid)
                        A Simple Plot and Legend Demo
draw.key(lattice-0.9.9)
                        Produce a Legend or Key



Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.

> ?legend

is what you need, if I understand your question correctly.

Please read the posting guide.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From tpapp at axelero.hu  Sun Apr 25 09:49:26 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sun, 25 Apr 2004 09:49:26 +0200
Subject: [R] Colour coding and point types in a plot
In-Reply-To: <BAY18-DAV21R7vA17hq0000286d@hotmail.com>
References: <BAY18-DAV21R7vA17hq0000286d@hotmail.com>
Message-ID: <20040425074925.GA1397@localhost>

On Sat, Apr 24, 2004 at 02:33:10PM -0500, Mihai Nica wrote:

> R 1.8.1, Windows 2000.

You might want to upgrade to 1.9.0.

> I am trying to find the "legend" for color coding and point types in
> a plot, which probably are standard for everybody but myself. Thanks
> for the tip!

> help.search("legend")

Help files with alias or concept or title matching 'legend' using
fuzzy matching:



gauss.quad(gss)         Generating Gauss-Legendre Quadrature
legend(graphics)        Add Legends to Plots
grid.legend(grid)       Internal Grid Functions
grid.plot.and.legend(grid)
                        A Simple Plot and Legend Demo
draw.key(lattice-0.9.9)
                        Produce a Legend or Key



Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.

> ?legend

is what you need, if I understand your question correctly.

Please read the posting guide.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From tpapp at axelero.hu  Sun Apr 25 11:08:09 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sun, 25 Apr 2004 11:08:09 +0200
Subject: [R] R vs Matlab: which is more "programmer friendly"?
Message-ID: <20040425090809.GA704@localhost>

Hi,

The department of economics at our university (Budapest) is planning a
course on numerical methods in economics.  They are trying to decide
which software to use for that, and I would like to advocate R.  The
other alternative is Matlab.

I have found comparisons in terms of computational time for matrix
algebra, but I don't think that is relevant: the bottleneck for
economists is usually the programmer's time: if it takes a couple of
hours to write something that is run only a few times, one should not
care whether it runs in 2 or 2.1 minutes...

I am an economist, and I have used Octave, but only until I found R.
So I am not in a position to evaluate Matlab vs R.  I would be
grateful if somebody could compare R to Matlab, especially regarding
the following:

1. How "smart" the language is.  R appears to be a nice functional
programming language, is Matlab comparable?  Last time I used Octave,
it seemed to be little more than syntactic sugar on some C/Fortran
libraries.  It appears to me that using R gradually pushes people
towards better programming habits, but I may be biased (I am a Scheme
lover).

2. Learning curve.  If somebody could share his/her experience on
using R or Matlab or both in the classrom, how students take to it.

3. Which language do you think is better for students' further
development?  We would like to equip them with something they can use
later on in their career even if they don't become theoretical
economists (very few undergraduate students do that).

4. How flexible are these languages when developing new
applications/functions?  Very few of the problems I encounter have a
ready-made solution in a toolbox/library.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From sebastian.goeres at student.unisg.ch  Sun Apr 25 12:27:27 2004
From: sebastian.goeres at student.unisg.ch (sebastian.goeres@student.unisg.ch)
Date: Sun, 25 Apr 2004 12:27:27 +0200
Subject: [R] Stichprobe bilden
Message-ID: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040425/7b9c2c37/attachment.pl

From baron at psych.upenn.edu  Sun Apr 25 13:23:20 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 25 Apr 2004 07:23:20 -0400
Subject: [R] Stichprobe bilden
In-Reply-To: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>
References: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>
Message-ID: <20040425112320.GA16244@psych>

On 04/25/04 12:27, sebastian.goeres at student.unisg.ch wrote:
>Liebe Alle,
>
>ich habe ien Problemm. ich habe einen Panel-Datensatz ueber die 185
>Staaten (nach Code von IMF) und von 1948-1999. Ich habe auch 12 Variablen
>in diesem Datensatz, die ich fuer die Regression benoetige. ich muss ein
>sub-sample bilden fuer die 65 Staaten und fuer den 1970-1999 Jahren. Wie
>kann ich aus dieser Stichprobe eine kleienere Stichprobe mit R bilden,
>damit auch meine 12 Variablen in Bezug auf die Jahren und Laendern richtig
>selektiert wurden.

My German is good enough to understand what you wrote, but not
good enough to write back.

You do not say enough.  The simple answer is to use the subset
operator [], or subset(), and something else like %in% to match
the countries.

For example, if your data are in a data frame called Gross, which
has a variable for Staat and another for Jahr,

Klein <- Gross[(Gross$Jahr > 1969) & (Gross$Staat %in% Staat65),]

where Staat65 is a vector with the list of countries you want,
like

Staat65 <- c("Germany","China", .....)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From ggrothendieck at myway.com  Sun Apr 25 14:32:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 25 Apr 2004 12:32:48 +0000 (UTC)
Subject: [R] R vs Matlab: which is more "programmer friendly"?
References: <20040425090809.GA704@localhost>
Message-ID: <loom.20040425T142147-830@post.gmane.org>

Tamas Papp <tpapp <at> axelero.hu> writes:
> 4. How flexible are these languages when developing new
> applications/functions?  Very few of the problems I encounter have a
> ready-made solution in a toolbox/library.

You might want to check your assumption that what you need is not available.
I believe that there is more not in CRAN than in CRAN.  Henrik Bengtsson's
site lists 370 packages:
http://www.maths.lth.se/help/R/.R/doc/html/packages.html
You could try posting if googling and looking at the usual places, 
cran.r-project.org, www.bioconductor.org and www.omegahat.org, does 
not turn up what you are looking for.

Also, there is a document on the R site that
provides a translation between Octave and R that might give you some
insight into your questions:
http://cran.r-project.org/doc/contrib/R-and-octave-2.txt



From ggrothendieck at myway.com  Sun Apr 25 16:06:52 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 25 Apr 2004 14:06:52 +0000 (UTC)
Subject: [R] Moving window regressions - how can I improve this code?
References: <20040424050921.GI732@igidr.ac.in>
	<6rvfjpbhqa.fsf@bates4.stat.wisc.edu>
	<loom.20040424T172140-508@post.gmane.org>
Message-ID: <loom.20040425T155754-1@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
 
> movingWindow <- function(formula, data, width, ...) {
>     nr <- nrow(data)
>     width <- as.integer(width)[1]
>     stopifnot( width > 0, width <= nr )
>     indices <- as.data.frame( t( embed( 1:nr, width ) ) )
>     lapply(indices, function(st) summary(lm(formula, data[st,])) )
> }
> 

Just one further simplification using apply instead of lapply to
eliminate having to transform embed:

movingWindow <- function(formula, data, width, ...) {
    nr <- nrow(data)
    width <- as.integer(width)[1]
    stopifnot( width > 0, width <= nr )
    apply( embed(1:nr, width), 1, # rows are indices of successive windows
		function(st) summary(lm(formula, data[st,])) )
}

This could also be used in movingWindow2, as well.



From kjetil at acelerate.com  Sun Apr 25 17:29:50 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Sun, 25 Apr 2004 11:29:50 -0400
Subject: [R] ts's in lm()
Message-ID: <408BA12E.9254.5C13396@localhost>

Is this a bug?

seasonal.dummies <-
function(x, contr=NULL) {
# takes a time series and returns a matrix of seasonal dummies for 
# x. This is almost cycle(x), we only have to make it into a factor
# and add suitable level names.
# return a matrix which includes a constant!
# level names here assumes frequency is 12!
cyc <- factor( cycle(x), labels=c("ene","feb", "mar", "abr", "may",
               "jun","jul", "ago", "sep",  "oct","nov", "dic") )
if(!is.null(contr)) cyc <- C(cyc, contr)
model.matrix( ~  cyc )
}


> modelo1 <- function(xx, yy) {
 # xx es serie explicativa (economia)
 # yy es serie para predecir --- gasolina.eq
 name.x <- deparse(substitute(xx))
 name.y <- deparse(substitute(yy))
 xx <- ts(rep(xx,rep(3, length(xx))), frequency=12, start=
            c(start(xx)[1], (start(xx)[2]-1)*3+1) )
 common <- ts.intersect(xx, yy)
 colnames(common) <- c(name.x, name.y)
 xx <- common[,1]
 yy <- common[,2]
 sea <- seasonal.dummies(xx)
 return(lm(yy  ~ 0+sea+xx))
 }
> mod1 <- modelo1(PIBtrimestral, gasolina.eq)
Error in "storage.mode<-"(`*tmp*`, value = "double") : 
        invalid time series parameters specified

lm() works with time series objects made directly with ts(), and the 
arguments to modelo1() is of this type.

Kjetil Halvorsen



From danbebber at forestecology.co.uk  Sun Apr 25 17:29:35 2004
From: danbebber at forestecology.co.uk (Daniel Bebber)
Date: Sun, 25 Apr 2004 16:29:35 +0100
Subject: [R] R vs Matlab: which is more "programmer friendly"?
In-Reply-To: <200404251005.i3PA2SqP013135@hypatia.math.ethz.ch>
Message-ID: <000e01c42ada$1d6657a0$da41fea9@dops7026>

I have been using R for about 2 years and recently took a 6 week
introductory course to Matlab.
I can give entirely personal answers to your questions.

1. 'How smart?'. Don't know exactly what you mean, but both languages are
extremely functional. Both emphasize writing of functions to call rather
than repeatedly typing in the same code, so they encourage good programming
practice. I started with R, so prefer its syntax.

2. 'Learning curve'. Similar. R has a simpler interface. Matlab has various
enhancements that may help in the learning process, for example the path
browser and workspace browser in which you can interactively keep track of
all the objects in the workspace. Both have comprehensive help packages.

3. 'Further development'. Has to be R- its FREE! I don't know many students
who would fork out the money for a Matlab license.

4. 'Flexibility'. Both are perhaps infinitely flexible. You can write any
code you like, to do any statistical or mathematical processing. I would say
R is better for statistical analysis (though Matlab has a stats package you
can purchase), while Matlab is designed for mathematics. With Matlab you can
compile GUIs to run analyses, which are probably useful for sharing with
people who can't program.

Some other points.
1. I have heard that Matlab is faster than S-PLUS (and hence R?) at
performing calculations.
2. I found Matlab to be quite frustrating in its handling of data- for
example it is extremely difficult to save a data frame in which variables
are labelled with there names.

Hope this helps.

Dan Bebber

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK

------------------------------

Message: 24
Date: Sun, 25 Apr 2004 11:08:09 +0200
From: Tamas Papp <tpapp at axelero.hu>
Subject: [R] R vs Matlab: which is more "programmer friendly"?
To: R-help mailing list <r-help at stat.math.ethz.ch>
Message-ID: <20040425090809.GA704 at localhost>
Content-Type: text/plain; charset=iso-8859-1

Hi,

The department of economics at our university (Budapest) is planning a
course on numerical methods in economics.  They are trying to decide
which software to use for that, and I would like to advocate R.  The
other alternative is Matlab.

I have found comparisons in terms of computational time for matrix
algebra, but I don't think that is relevant: the bottleneck for
economists is usually the programmer's time: if it takes a couple of
hours to write something that is run only a few times, one should not
care whether it runs in 2 or 2.1 minutes...

I am an economist, and I have used Octave, but only until I found R.
So I am not in a position to evaluate Matlab vs R.  I would be
grateful if somebody could compare R to Matlab, especially regarding
the following:

1. How "smart" the language is.  R appears to be a nice functional
programming language, is Matlab comparable?  Last time I used Octave,
it seemed to be little more than syntactic sugar on some C/Fortran
libraries.  It appears to me that using R gradually pushes people
towards better programming habits, but I may be biased (I am a Scheme
lover).

2. Learning curve.  If somebody could share his/her experience on
using R or Matlab or both in the classrom, how students take to it.

3. Which language do you think is better for students' further
development?  We would like to equip them with something they can use
later on in their career even if they don't become theoretical
economists (very few undergraduate students do that).

4. How flexible are these languages when developing new
applications/functions?  Very few of the problems I encounter have a
ready-made solution in a toolbox/library.

Thanks,

Tamas

--
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



------------------------------

_______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE read the posting guide! http://www.R-project.org/posting-guide.html


End of R-help Digest, Vol 14, Issue 25



From alain.yamakana at rogers.com  Sun Apr 25 17:45:45 2004
From: alain.yamakana at rogers.com (Alain Yamakana)
Date: Sun, 25 Apr 2004 11:45:45 -0400
Subject: [R] Bug?
Message-ID: <000501c42adc$627dcd80$1d503141@bloor.phub.net.cable.rogers.com>

Either something is wrong with my machine or there may be a bugg. I had run
R-1.8.1 two weeks ago. Since yesterday, I am getting this message:<<R for
Windows GUI front-end has encountered a problem and needs to close. We are
sorry for the inconvenience.>> No new software has been installed since I
installed R-1.8.1. Tired to fail getting R-1.8.1 work, I decided to install
R-1.9.0 and got the following message <<Fatal Error. HOMEDRIVE>>. How to
solve either one of these two problems?

Best regards,
Alain



From wcvinyard at earthlink.net  Sun Apr 25 17:44:51 2004
From: wcvinyard at earthlink.net (Bill Vinyard)
Date: Sun, 25 Apr 2004 11:44:51 -0400
Subject: [R] R vs Matlab: which is more "programmer friendly"?
In-Reply-To: <20040425090809.GA704@localhost>
Message-ID: <MJENLJEPCHEMCAGNPDMGCEOLCBAA.wcvinyard@earthlink.net>


I was exposed to and have used Matlab and R both in and out of the
classroom, both as a student and as an instructor.

1.  Both Matlab and R are "smart."  In terms of flexibility, I think they
are both about the same.  This includes their FSF or commercial counterparts
Octave and SPlus respectively.  With appropriate guidance, all of these
applications can be used to develop better programming habits.  As a general
rule the commercial versions have a smoother appearance and better
documentation...but this is not true of R.  R has some of the best
documentation around as well as superb on-line support via the R mailing
lists.  From a student's perspective access to well written documentation
and on-line support are essential.

2.  Both require the user to have some programming experience; otherwise the
learning curve is steep in the beginning.  Both can be used to introduce
programming provided you make room in your syllabus for the basics in the
beginning.

3.  In my opinion, the main difference -- besides the obvious one...cost --
is that R is easily accessible anywhere in the world, costs nothing (which
makes it affordable for students), has excellent documentation, a robust,
friendly and helpful user base, superior on-line support ...

4.  The power of R and Matlab is that they are both easily extensible.  If
you can't find a ready-made function to do what you want, you can easily, on
the fly, write your own and then make it a permanent function for your use
at any time.

Bill
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tamas Papp
Sent: Sunday, April 25, 2004 05:08
To: R-help mailing list
Subject: [R] R vs Matlab: which is more "programmer friendly"?


Hi,

The department of economics at our university (Budapest) is planning a
course on numerical methods in economics.  They are trying to decide
which software to use for that, and I would like to advocate R.  The
other alternative is Matlab.

I have found comparisons in terms of computational time for matrix
algebra, but I don't think that is relevant: the bottleneck for
economists is usually the programmer's time: if it takes a couple of
hours to write something that is run only a few times, one should not
care whether it runs in 2 or 2.1 minutes...

I am an economist, and I have used Octave, but only until I found R.
So I am not in a position to evaluate Matlab vs R.  I would be
grateful if somebody could compare R to Matlab, especially regarding
the following:

1. How "smart" the language is.  R appears to be a nice functional
programming language, is Matlab comparable?  Last time I used Octave,
it seemed to be little more than syntactic sugar on some C/Fortran
libraries.  It appears to me that using R gradually pushes people
towards better programming habits, but I may be biased (I am a Scheme
lover).

2. Learning curve.  If somebody could share his/her experience on
using R or Matlab or both in the classrom, how students take to it.

3. Which language do you think is better for students' further
development?  We would like to equip them with something they can use
later on in their career even if they don't become theoretical
economists (very few undergraduate students do that).

4. How flexible are these languages when developing new
applications/functions?  Very few of the problems I encounter have a
ready-made solution in a toolbox/library.

Thanks,

Tamas

--
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From aolinto_r at bignet.com.br  Sun Apr 25 17:52:36 2004
From: aolinto_r at bignet.com.br (aolinto_r@bignet.com.br)
Date: Sun, 25 Apr 2004 12:52:36 -0300
Subject: [R] nonparametric multiple sample comparison
Message-ID: <1082908356.408bdec4676a5@webmail.bignet.com.br>

Hello all,

Here goes one of my first functions.

I want to make a nonparametric multiple sample comparison with unequal sample 
sizes (see Zar?s Biostatistical Analysis, 3rd. Ed., pg. 201 Example 10.11, pg. 
288 Example 11.10). In the real world, I want to compare samples of fish 
length captured with different fishing gears.

After using the Kruskal-Wallis test I want to check the differences between 
each two samples. 

I wrote the function multcomp (see below) and it?s working OK but I still have 
some doubts. To use it one must have a two-column dataframe with groups and 
measurements.

1) In line 20 (Results <- data.frame....) I create a dataframe to store the final 
results. It has a fixed row number but it would be better to have a variable 
number, i.e., the number of combination of the groups taken 2 by 2 (4 groups = 
5 combinations, 6 groups = 15 combinations). Which function in R returns the 
number of combinations? I couldn?t find it!

2) In lines 35 to 39 I print the result tables. How can I make a "clean" 
print, without row numbers and quotation marks?

3) Also I?d like to calculate the critical values "q" and p-values but I 
couldn?t find the distribution indicated in Zar?s Table B.15 
(App107) "Critical Values of Q for Nonparametric Multiple Comparison Testing". 
Is it possible to calculate these numbers in R?

Thanks in advance for any help or comments to improve this function.

Best regards,

Antonio Olinto
Sao Paulo Fisheries Institute
Brazil

multcomp <- function(DataSet) {
dat.multcomp <- DataSet
names(dat.multcomp) <- c("VarCat","VarNum")
attach(dat.multcomp)
dat.multcomp$Rank <- rank(VarNum)
attach(dat.multcomp)
RankList <- aggregate(Rank,list(Rank=Rank),FUN=length)
t <- length(RankList$Rank)
st <- 0
for (i in 1:t) if (RankList[i,2]>1) st<-st+(RankList[i,2]^3-RankList[i,2])
LevCat <- levels(dat.multcomp$VarCat)
NLevCat <- aggregate(VarCat,list(LevCat=VarCat),FUN=length)
RLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=sum)
MLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=mean)
SampleSummary <- data.frame(LevCat,RLevCat[,2],NLevCat[,2],MLevCat[,2])
names(SampleSummary)<-c("Samples","RSum","N","RMean")
SampleSummary <- SampleSummary[order(SampleSummary$RMean,decreasing=T),]
NCat <- length(LevCat)
N <- length(dat.multcomp$VarCat)
Results <- data.frame(rep(NA,6),rep(NA,6),rep(NA,6),rep(NA,6))
names(Results) <- c("Comparison","Difference","SE","Q")
l <- 1
for (i in 1:(NCat-1)) {
   for (j in NCat:(i+1)) {
   SE <- sqrt(((N*(N+1)/12)-(st/(12*(N-1))))*((1/SampleSummary[i,3])+
(1/SampleSummary[j,3])))
   Dif <- SampleSummary[i,4]-SampleSummary[j,4]
   Q=Dif/SE
Results[l,1] <- paste(SampleSummary[i,1],"vs",SampleSummary[j,1])
Results[l,2] <- round(Dif,4)
Results[l,3] <- round(SE,4)
Results[l,4] <- round(Q,4)
l <-l+1
   }
}
print("Sample summary ranked by mean ranks")
print(SampleSummary)
print("")
print("Table of multiple comparisons")
print(Results)
}

===========

Data example taken from Zar (Biostatiscical Analysis):

dat.limno <- data.frame(scan(what = list (Pound=" ", pH=0), sep=" "))
1 7.68
1 7.69
1 7.7
1 7.7
1 7.72
1 7.73
1 7.73
1 7.76
2 7.71
2 7.73
2 7.74
2 7.74
2 7.78
2 7.78
2 7.8
2 7.81
3 7.74
3 7.75
3 7.77
3 7.78
3 7.8
3 7.81
3 7.84
4 7.71
4 7.71
4 7.74
4 7.79
4 7.81
4 7.85
4 7.87
4 7.91

kruskal.test(pH~Pound)

        Kruskal-Wallis rank sum test

data
Kruskal-Wallis chi-squared = 11.9435, df = 3, p-value = 0.007579

multcomp(dat.limno)

[1] "Sample summary ranked by mean ranks"
  Samples  RSum N    RMean
3       3 145.0 7 20.71429
4       4 163.5 8 20.43750
2       2 132.5 8 16.56250
1       1  55.0 8  6.87500
[1] ""
[1] "Table of multiple comparisons"
  Comparison Difference     SE      Q
1     3 vs 1    13.8393 4.6923 2.9493
2     3 vs 2     4.1518 4.6923 0.8848
3     3 vs 4     0.2768 4.6923 0.0590
4     4 vs 1    13.5625 4.5332 2.9918
5     4 vs 2     3.8750 4.5332 0.8548
6     2 vs 1     9.6875 4.5332 2.1370


-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From ripley at stats.ox.ac.uk  Sun Apr 25 18:15:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Apr 2004 17:15:43 +0100 (BST)
Subject: [R] ts's in lm()
In-Reply-To: <408BA12E.9254.5C13396@localhost>
Message-ID: <Pine.LNX.4.44.0404251713320.4536-100000@gannet.stats>

On Sun, 25 Apr 2004 kjetil at acelerate.com wrote:

> Is this a bug?

Without even a traceback() nor reproducible code it is hard to say.
But for lm, you don't want time series and it would be better to drop the 
ts class and tsp attributes.

> seasonal.dummies <-
> function(x, contr=NULL) {
> # takes a time series and returns a matrix of seasonal dummies for 
> # x. This is almost cycle(x), we only have to make it into a factor
> # and add suitable level names.
> # return a matrix which includes a constant!
> # level names here assumes frequency is 12!
> cyc <- factor( cycle(x), labels=c("ene","feb", "mar", "abr", "may",
>                "jun","jul", "ago", "sep",  "oct","nov", "dic") )
> if(!is.null(contr)) cyc <- C(cyc, contr)
> model.matrix( ~  cyc )
> }
> 
> 
> > modelo1 <- function(xx, yy) {
>  # xx es serie explicativa (economia)
>  # yy es serie para predecir --- gasolina.eq
>  name.x <- deparse(substitute(xx))
>  name.y <- deparse(substitute(yy))
>  xx <- ts(rep(xx,rep(3, length(xx))), frequency=12, start=
>             c(start(xx)[1], (start(xx)[2]-1)*3+1) )
>  common <- ts.intersect(xx, yy)
>  colnames(common) <- c(name.x, name.y)
>  xx <- common[,1]
>  yy <- common[,2]
>  sea <- seasonal.dummies(xx)
>  return(lm(yy  ~ 0+sea+xx))
>  }
> > mod1 <- modelo1(PIBtrimestral, gasolina.eq)
> Error in "storage.mode<-"(`*tmp*`, value = "double") : 
>         invalid time series parameters specified
> 
> lm() works with time series objects made directly with ts(), and the 
> arguments to modelo1() is of this type.
> 
> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Apr 25 18:23:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Apr 2004 17:23:03 +0100 (BST)
Subject: [R] Bug?
In-Reply-To: <000501c42adc$627dcd80$1d503141@bloor.phub.net.cable.rogers.com>
Message-ID: <Pine.LNX.4.44.0404251715540.4536-100000@gannet.stats>

Yes, it is a bug, but not in R.  It is a bug in a Windows XP critical 
update: are you *sure* no new software has been installed behind your 
back?

The fix (which you can find several times in the list archives) is to add

R_USER=c:/

(or better, the correct path to your home aka personal directory) to the
shortcut you use to start R.

There is a workaround in R-patched, usually available from

http://cran.r-project.org/bin/windows/base/rpatched.html

but I think that has the wrong version at present since it is labelled
R-2.0.0 patch build for Windows and contains rw2000dev.exe, not 
rw1090pat.exe.

On Sun, 25 Apr 2004, Alain Yamakana wrote:

> Either something is wrong with my machine or there may be a bugg. I had run
> R-1.8.1 two weeks ago. Since yesterday, I am getting this message:<<R for
> Windows GUI front-end has encountered a problem and needs to close. We are
> sorry for the inconvenience.>> No new software has been installed since I
> installed R-1.8.1. Tired to fail getting R-1.8.1 work, I decided to install
> R-1.9.0 and got the following message <<Fatal Error. HOMEDRIVE>>. How to
> solve either one of these two problems?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Sun Apr 25 19:22:35 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 25 Apr 2004 13:22:35 -0400
Subject: [R] Bug?
In-Reply-To: <Pine.LNX.4.44.0404251715540.4536-100000@gannet.stats>
References: <000501c42adc$627dcd80$1d503141@bloor.phub.net.cable.rogers.com>
	<Pine.LNX.4.44.0404251715540.4536-100000@gannet.stats>
Message-ID: <vmsn801rhohtb5d7q5pt297o4qemi9trt9@4ax.com>

On Sun, 25 Apr 2004 17:23:03 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:

>There is a workaround in R-patched, usually available from
>
>http://cran.r-project.org/bin/windows/base/rpatched.html
>
>but I think that has the wrong version at present since it is labelled
>R-2.0.0 patch build for Windows and contains rw2000dev.exe, not 
>rw1090pat.exe.

The right file is there, but I uploaded the wrong link to it.  You can
download http://cran.r-project.org/bin/windows/base/rw1090pat.exe.
I've replaced the rpatched.html web page (but it won't be visible
until tomorrow).

Duncan Murdoch



From ggrothendieck at myway.com  Sun Apr 25 19:36:38 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 25 Apr 2004 17:36:38 +0000 (UTC)
Subject: [R] nonparametric multiple sample comparison
References: <1082908356.408bdec4676a5@webmail.bignet.com.br>
Message-ID: <loom.20040425T192451-168@post.gmane.org>


I have not attempted to follow this in detail but a few comments 
are interspersed within your discussion and the code:

 <aolinto_r <at> bignet.com.br> writes:

: 
: Hello all,
: 
: Here goes one of my first functions.
: 
: I want to make a nonparametric multiple sample comparison with unequal 
sample 
: sizes (see Zars Biostatistical Analysis, 3rd. Ed., pg. 201 Example 10.11, 
pg. 
: 288 Example 11.10). In the real world, I want to compare samples of fish 
: length captured with different fishing gears.
: 
: After using the Kruskal-Wallis test I want to check the differences between 
: each two samples. 
: 
: I wrote the function multcomp (see below) and its working OK but I still 
have 
: some doubts. To use it one must have a two-column dataframe with groups and 
: measurements.
: 
: 1) In line 20 (Results <- data.frame....) I create a dataframe to store the 
final 
: results. It has a fixed row number but it would be better to have a variable 
: number, i.e., the number of combination of the groups taken 2 by 2 (4 groups 
= 
: 5 combinations, 6 groups = 15 combinations). Which function in R returns the 
: number of combinations? I couldnt find it!


?choose


: 
: 2) In lines 35 to 39 I print the result tables. How can I make a "clean" 
: print, without row numbers and quotation marks?


?cat

: 
: 3) Also Id like to calculate the critical values "q" and p-values but I 
: couldnt find the distribution indicated in Zars Table B.15 
: (App107) "Critical Values of Q for Nonparametric Multiple Comparison 
Testing". 
: Is it possible to calculate these numbers in R?
: 
: Thanks in advance for any help or comments to improve this function.
: 
: Best regards,
: 
: Antonio Olinto
: Sao Paulo Fisheries Institute
: Brazil
: 
: multcomp <- function(DataSet) {
: dat.multcomp <- DataSet

You don't need to copy the DataSet.  If you try to modify any object
within a function then R makes a copy of it automatically anyways.

: names(dat.multcomp) <- c("VarCat","VarNum")
: attach(dat.multcomp)

attach is generally frowned upon.  Just call it a short name such as d so that
its easy to refer to.  (Alternately wrap references to data frame in a with.)
I have assumed the data frame is called d in what follows.

: dat.multcomp$Rank <- rank(VarNum)
: attach(dat.multcomp)
: RankList <- aggregate(Rank,list(Rank=Rank),FUN=length)
: t <- length(RankList$Rank)
: st <- 0
: for (i in 1:t) if (RankList[i,2]>1) st<-st+(RankList[i,2]^3-RankList[i,2])

The last two statements could be reduced to:

st <- sum(ifelse( RankList[,2]>1, RankList[,2]^3-RankList[,2], 0 )

: LevCat <- levels(dat.multcomp$VarCat)
: NLevCat <- aggregate(VarCat,list(LevCat=VarCat),FUN=length)
: RLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=sum)
: MLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=mean)
: SampleSummary <- data.frame(LevCat,RLevCat[,2],NLevCat[,2],MLevCat[,2])
: names(SampleSummary)<-c("Samples","RSum","N","RMean")

If I have followed this correctly then the above 6 lines can be reduced to:

SampleSummary <- do.call( "rbind", by(d$Rank, d$VarCat, 
	function(x) c(Rsum=sum(x),N=length(x),RMean=mean(x))))

although that produces a matrix rather than a data frame and puts the 
levels as the row rather than a column so you will have to adjust 
what follows.

: SampleSummary <- SampleSummary[order(SampleSummary$RMean,decreasing=T),]
: NCat <- length(LevCat)
: N <- length(dat.multcomp$VarCat)
: Results <- data.frame(rep(NA,6),rep(NA,6),rep(NA,6),rep(NA,6))
: names(Results) <- c("Comparison","Difference","SE","Q")
: l <- 1
: for (i in 1:(NCat-1)) {
:    for (j in NCat:(i+1)) {
:    SE <- sqrt(((N*(N+1)/12)-(st/(12*(N-1))))*((1/SampleSummary[i,3])+
: (1/SampleSummary[j,3])))
:    Dif <- SampleSummary[i,4]-SampleSummary[j,4]
:    Q=Dif/SE
: Results[l,1] <- paste(SampleSummary[i,1],"vs",SampleSummary[j,1])
: Results[l,2] <- round(Dif,4)
: Results[l,3] <- round(SE,4)
: Results[l,4] <- round(Q,4)
: l <-l+1
:    }
: }
: print("Sample summary ranked by mean ranks")
: print(SampleSummary)
: print("")
: print("Table of multiple comparisons")
: print(Results)
: }
: 
: ===========
: 
: Data example taken from Zar (Biostatiscical Analysis):
: 
: dat.limno <- data.frame(scan(what = list (Pound=" ", pH=0), sep=" "))
: 1 7.68
: 1 7.69
: 1 7.7
: 1 7.7
: 1 7.72
: 1 7.73
: 1 7.73
: 1 7.76
: 2 7.71
: 2 7.73
: 2 7.74
: 2 7.74
: 2 7.78
: 2 7.78
: 2 7.8
: 2 7.81
: 3 7.74
: 3 7.75
: 3 7.77
: 3 7.78
: 3 7.8
: 3 7.81
: 3 7.84
: 4 7.71
: 4 7.71
: 4 7.74
: 4 7.79
: 4 7.81
: 4 7.85
: 4 7.87
: 4 7.91
: 
: kruskal.test(pH~Pound)
: 
:         Kruskal-Wallis rank sum test
: 
: data
: Kruskal-Wallis chi-squared = 11.9435, df = 3, p-value = 0.007579
: 
: multcomp(dat.limno)
: 
: [1] "Sample summary ranked by mean ranks"
:   Samples  RSum N    RMean
: 3       3 145.0 7 20.71429
: 4       4 163.5 8 20.43750
: 2       2 132.5 8 16.56250
: 1       1  55.0 8  6.87500
: [1] ""
: [1] "Table of multiple comparisons"
:   Comparison Difference     SE      Q
: 1     3 vs 1    13.8393 4.6923 2.9493
: 2     3 vs 2     4.1518 4.6923 0.8848
: 3     3 vs 4     0.2768 4.6923 0.0590
: 4     4 vs 1    13.5625 4.5332 2.9918
: 5     4 vs 2     3.8750 4.5332 0.8548
: 6     2 vs 1     9.6875 4.5332 2.1370
: 
: 
: -------------------------------------------------
: WebMail Bignet - O seu provedor do litoral
: www.bignet.com.br
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From tpapp at axelero.hu  Sun Apr 25 20:13:14 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Sun, 25 Apr 2004 20:13:14 +0200
Subject: [R] expression with running index
Message-ID: <20040425181314.GA2108@localhost>

Hi,

I need a list of expression of the form expression(b[i]), where i is a
running index.  So for example, if i <- -1:2, I would like to have a
list equivalent to

list(expression(b[-1]), expression(b[0]), expression(b[1]), expression(b[2]))

"i" might be a character vector (like c("f", "g", "h"))

Could somebody help me out by writing a function that produces the
list above for a given string in place of "b" and a vector of subscripts?

Sorry if this has been discussed before, I tried searching the
archives but "expression" as a keyword gives too many results on
different subjects.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From akopps at CSUA.Berkeley.EDU  Sun Apr 25 20:31:36 2004
From: akopps at CSUA.Berkeley.EDU (Akop Pogosian)
Date: Sun, 25 Apr 2004 11:31:36 -0700
Subject: [R] .Rprofile error: ps.options
Message-ID: <20040425183136.GA13565@csua.berkeley.edu>

After upgrading to R 1.9.0, I started getting the following error on
startup:

Error: couldn't find function "ps.options"

My .Rprofile file contains this line:

ps.options(paper="letter",horizontal=F)

If I type "source ~/.Rprofile" after R starts, it works just fine. Did
something related to startup change in R 1.9.0?


-akop



From ligges at statistik.uni-dortmund.de  Sun Apr 25 20:34:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 25 Apr 2004 20:34:31 +0200
Subject: [R] expression with running index
In-Reply-To: <20040425181314.GA2108@localhost>
References: <20040425181314.GA2108@localhost>
Message-ID: <408C04B7.8020807@statistik.uni-dortmund.de>

Tamas Papp wrote:

> Hi,
> 
> I need a list of expression of the form expression(b[i]), where i is a
> running index.  So for example, if i <- -1:2, I would like to have a
> list equivalent to
> 
> list(expression(b[-1]), expression(b[0]), expression(b[1]), expression(b[2]))
> 
> "i" might be a character vector (like c("f", "g", "h"))
> 
> Could somebody help me out by writing a function that produces the
> list above for a given string in place of "b" and a vector of subscripts?
> 
> Sorry if this has been discussed before, I tried searching the
> archives but "expression" as a keyword gives too many results on
> different subjects.
> 
> Thanks,
> 
> Tamas
> 

For example:

  lapply(as.double(-1:2), function(x) substitute(b[i], i = list(i=x)))

Uwe Ligges



From rpeng at jhsph.edu  Sun Apr 25 20:40:33 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 25 Apr 2004 14:40:33 -0400
Subject: [R] .Rprofile error: ps.options
In-Reply-To: <20040425183136.GA13565@csua.berkeley.edu>
References: <20040425183136.GA13565@csua.berkeley.edu>
Message-ID: <408C0621.2090703@jhsph.edu>

Yes, the startup is different in 1.9.0.  See the NEWS file in the 
installation directory.

-roger

Akop Pogosian wrote:

> After upgrading to R 1.9.0, I started getting the following error on
> startup:
> 
> Error: couldn't find function "ps.options"
> 
> My .Rprofile file contains this line:
> 
> ps.options(paper="letter",horizontal=F)
> 
> If I type "source ~/.Rprofile" after R starts, it works just fine. Did
> something related to startup change in R 1.9.0?
> 
> 
> -akop
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Sun Apr 25 20:40:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Apr 2004 19:40:44 +0100 (BST)
Subject: [R] expression with running index
In-Reply-To: <20040425181314.GA2108@localhost>
Message-ID: <Pine.LNX.4.44.0404251932070.4805-100000@gannet.stats>

lapply(-1:2, function(i) substitute(expression(b[i]), list(i=i)))

would be a good start. (Note that what it gives is

[[1]]
expression(b[as.integer(-1)])

which is not what you asked for but is what I think you intended.

Then we can elaborate this to 

f <- function(ind, vec)
    lapply(ind, 
           function(i, vec) substitute(expression(vec[i]), 
                                       list(i=i, vec=vec)), 
           vec=as.name(vec))

f(-1:2, "b") and f(c("f", "g", "h"), "b") both work

Also, I am not sure you need the expression() in there, as without it you 
have a language call which will almost certainly do.


On Sun, 25 Apr 2004, Tamas Papp wrote:

> Hi,
> 
> I need a list of expression of the form expression(b[i]), where i is a
> running index.  So for example, if i <- -1:2, I would like to have a
> list equivalent to
> 
> list(expression(b[-1]), expression(b[0]), expression(b[1]), expression(b[2]))
> 
> "i" might be a character vector (like c("f", "g", "h"))
> 
> Could somebody help me out by writing a function that produces the
> list above for a given string in place of "b" and a vector of subscripts?
> 
> Sorry if this has been discussed before, I tried searching the
> archives but "expression" as a keyword gives too many results on
> different subjects.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Apr 25 20:45:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 25 Apr 2004 19:45:09 +0100 (BST)
Subject: [R] .Rprofile error: ps.options
In-Reply-To: <20040425183136.GA13565@csua.berkeley.edu>
Message-ID: <Pine.LNX.4.44.0404251940540.4805-100000@gannet.stats>

Do look at the NEWS file.  This is covered in the top 40 lines, and points
you to an example (?setHook) of how to do almost exactly this.

BTW, please use FALSE not F for FALSE in programming (as in .Rprofile).

On Sun, 25 Apr 2004, Akop Pogosian wrote:

> After upgrading to R 1.9.0, I started getting the following error on
> startup:
> 
> Error: couldn't find function "ps.options"
> 
> My .Rprofile file contains this line:
> 
> ps.options(paper="letter",horizontal=F)
> 
> If I type "source ~/.Rprofile" after R starts, it works just fine. Did
> something related to startup change in R 1.9.0?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phddas at yahoo.com  Sun Apr 25 23:44:55 2004
From: phddas at yahoo.com (Fred J.)
Date: Sun, 25 Apr 2004 14:44:55 -0700 (PDT)
Subject: [R] stepping into a .C call
Message-ID: <20040425214455.12149.qmail@web20503.mail.yahoo.com>

Hello
The Debug and mvbutils packages by Mark Bravington
have been a great help for me in learning R, until I
come to a line in the R code like ".C("fun name",...).
Is there a way to step into this call and see what the
source code does exactly to the supplied arrgs? 
One example in the "fdim" package, a line like 
DK <- .C("pointdif", as.integer(NumRow),
as.integer(NumCol), as.integer(X), NumpDif =
as.integer(NumpDif))
I would like to know what the code does inorder to
come up with the value to assign to DK.

Thanks



From Toby.Patterson at csiro.au  Mon Apr 26 01:15:32 2004
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Mon, 26 Apr 2004 09:15:32 +1000
Subject: [R] stepping into a .C call
Message-ID: <C4178DC99E08604EA5E2BDB989F0938024210C@extas2-hba.tas.csiro.au>

No - not from debug directly. You need to read "writing R extensions" in
the help. 
Cheers
T. 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fred J.
Sent: Monday, April 26, 2004 7:45 AM
To: r help
Subject: [R] stepping into a .C call

Hello
The Debug and mvbutils packages by Mark Bravington
have been a great help for me in learning R, until I
come to a line in the R code like ".C("fun name",...).
Is there a way to step into this call and see what the
source code does exactly to the supplied arrgs? 
One example in the "fdim" package, a line like 
DK <- .C("pointdif", as.integer(NumRow),
as.integer(NumCol), as.integer(X), NumpDif =
as.integer(NumpDif))
I would like to know what the code does inorder to
come up with the value to assign to DK.

Thanks

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Apr 26 02:06:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 25 Apr 2004 20:06:29 -0400
Subject: [R] nonparametric multiple sample comparison
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C7E@usrymx25.merck.com>

Have you looked at the npmc package on CRAN?

HTH,
Andy

> From: aolinto_r at bignet.com.br
> 
> Hello all,
> 
> Here goes one of my first functions.
> 
> I want to make a nonparametric multiple sample comparison 
> with unequal sample 
> sizes (see Zar's Biostatistical Analysis, 3rd. Ed., pg. 201 
> Example 10.11, pg. 
> 288 Example 11.10). In the real world, I want to compare 
> samples of fish 
> length captured with different fishing gears.
> 
> After using the Kruskal-Wallis test I want to check the 
> differences between 
> each two samples. 
> 
> I wrote the function multcomp (see below) and it's working OK 
> but I still have 
> some doubts. To use it one must have a two-column dataframe 
> with groups and 
> measurements.
> 
> 1) In line 20 (Results <- data.frame...) I create a dataframe 
> to store the final 
> results. It has a fixed row number but it would be better to 
> have a variable 
> number, i.e., the number of combination of the groups taken 2 
> by 2 (4 groups = 
> 5 combinations, 6 groups = 15 combinations). Which function 
> in R returns the 
> number of combinations? I couldn't find it!
> 
> 2) In lines 35 to 39 I print the result tables. How can I 
> make a "clean" 
> print, without row numbers and quotation marks?
> 
> 3) Also I'd like to calculate the critical values "q" and 
> p-values but I 
> couldn't find the distribution indicated in Zar's Table B.15 
> (App107) "Critical Values of Q for Nonparametric Multiple 
> Comparison Testing". 
> Is it possible to calculate these numbers in R?
> 
> Thanks in advance for any help or comments to improve this function.
> 
> Best regards,
> 
> Antonio Olinto
> Sao Paulo Fisheries Institute
> Brazil
> 
> multcomp <- function(DataSet) {
> dat.multcomp <- DataSet
> names(dat.multcomp) <- c("VarCat","VarNum")
> attach(dat.multcomp)
> dat.multcomp$Rank <- rank(VarNum)
> attach(dat.multcomp)
> RankList <- aggregate(Rank,list(Rank=Rank),FUN=length)
> t <- length(RankList$Rank)
> st <- 0
> for (i in 1:t) if (RankList[i,2]>1) 
> st<-st+(RankList[i,2]^3-RankList[i,2])
> LevCat <- levels(dat.multcomp$VarCat)
> NLevCat <- aggregate(VarCat,list(LevCat=VarCat),FUN=length)
> RLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=sum)
> MLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=mean)
> SampleSummary <- 
> data.frame(LevCat,RLevCat[,2],NLevCat[,2],MLevCat[,2])
> names(SampleSummary)<-c("Samples","RSum","N","RMean")
> SampleSummary <- 
> SampleSummary[order(SampleSummary$RMean,decreasing=T),]
> NCat <- length(LevCat)
> N <- length(dat.multcomp$VarCat)
> Results <- data.frame(rep(NA,6),rep(NA,6),rep(NA,6),rep(NA,6))
> names(Results) <- c("Comparison","Difference","SE","Q")
> l <- 1
> for (i in 1:(NCat-1)) {
>    for (j in NCat:(i+1)) {
>    SE <- sqrt(((N*(N+1)/12)-(st/(12*(N-1))))*((1/SampleSummary[i,3])+
> (1/SampleSummary[j,3])))
>    Dif <- SampleSummary[i,4]-SampleSummary[j,4]
>    Q=Dif/SE
> Results[l,1] <- paste(SampleSummary[i,1],"vs",SampleSummary[j,1])
> Results[l,2] <- round(Dif,4)
> Results[l,3] <- round(SE,4)
> Results[l,4] <- round(Q,4)
> l <-l+1
>    }
> }
> print("Sample summary ranked by mean ranks")
> print(SampleSummary)
> print("")
> print("Table of multiple comparisons")
> print(Results)
> }
> 
> ===========
> 
> Data example taken from Zar (Biostatiscical Analysis):
> 
> dat.limno <- data.frame(scan(what = list (Pound=" ", pH=0), sep=" "))
> 1 7.68
> 1 7.69
> 1 7.7
> 1 7.7
> 1 7.72
> 1 7.73
> 1 7.73
> 1 7.76
> 2 7.71
> 2 7.73
> 2 7.74
> 2 7.74
> 2 7.78
> 2 7.78
> 2 7.8
> 2 7.81
> 3 7.74
> 3 7.75
> 3 7.77
> 3 7.78
> 3 7.8
> 3 7.81
> 3 7.84
> 4 7.71
> 4 7.71
> 4 7.74
> 4 7.79
> 4 7.81
> 4 7.85
> 4 7.87
> 4 7.91
> 
> kruskal.test(pH~Pound)
> 
>         Kruskal-Wallis rank sum test
> 
> data
> Kruskal-Wallis chi-squared = 11.9435, df = 3, p-value = 0.007579
> 
> multcomp(dat.limno)
> 
> [1] "Sample summary ranked by mean ranks"
>   Samples  RSum N    RMean
> 3       3 145.0 7 20.71429
> 4       4 163.5 8 20.43750
> 2       2 132.5 8 16.56250
> 1       1  55.0 8  6.87500
> [1] ""
> [1] "Table of multiple comparisons"
>   Comparison Difference     SE      Q
> 1     3 vs 1    13.8393 4.6923 2.9493
> 2     3 vs 2     4.1518 4.6923 0.8848
> 3     3 vs 4     0.2768 4.6923 0.0590
> 4     4 vs 1    13.5625 4.5332 2.9918
> 5     4 vs 2     3.8750 4.5332 0.8548
> 6     2 vs 1     9.6875 4.5332 2.1370
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral
> www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Sun Apr 25 22:14:51 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 25 Apr 2004 21:14:51 +0100 (BST)
Subject: [R] R vs Matlab: which is more "programmer friendly"?
In-Reply-To: <20040425090809.GA704@localhost>
Message-ID: <XFMail.040425211451.Ted.Harding@nessie.mcc.ac.uk>

Hi Tamas,

I used Matlab starting in about 1990 then later (since about 1993)
octave which I then stuck with without reverting to Matlab. Like you,
since starting to get used to R I now rarely use octave (at any rate
for staistics).

Some comments below to add to those of others. These mainly refer to
octave, since I have not been in contact with Matlab for a good few
years.

On 25-Apr-04 Tamas Papp wrote:
> The department of economics at our university (Budapest) is planning a
> course on numerical methods in economics.  They are trying to decide
> which software to use for that, and I would like to advocate R.  The
> other alternative is Matlab.
> [...]
> 1. How "smart" the language is.  R appears to be a nice functional
> programming language, is Matlab comparable?  Last time I used Octave,
> it seemed to be little more than syntactic sugar on some C/Fortran
> libraries.  It appears to me that using R gradually pushes people
> towards better programming habits, but I may be biased (I am a Scheme
> lover).

I think you are being unfair on Matlab/octave, which has a full
programming language with a very healthy supply of builtin routines 
for numerical operations on matrices and vectors. However, their main
programming resources are of the kind assignment, looping,
conditionals, etc. rather like C itself. Primitve types are also
somewhat limited -- manily integers, reals, complex, and vectors
and 2-D arrays of these, and "structures" which are basically lists
of items each of which can be any of these types (access using
C-like "." notation). Nevertheless these are enough to develop
a great variety of applications.

I don't know what the situation now is with Matlab, but octave has
long suffered from not supporting arrays with more than 2 dimensions,
which somewhat impedes its use for many statistical purposes, though I
think the issue has been recently addressed in a more constructive
fashion. You can emulate multidimensional arrays as structures,
computing an index into a vector (linear array of elements):
http://www.octave.org/octave-lists/archive/help-octave.2001/msg00338.html

There has for quite some time been an offshoot of octave called
"TeLa" ("Tensor Language") of which details can be found at

  http://www.geo.fmi.fi/prog/tela.html

(According to the changelog on this site the latest version is
2.0 of August 2002). This was specifically written to support
multidimensional arrays because of fmi.fi (Finnish Meteorological
Institute)'s interest in complex multidimensional dynamics.

It is quite easy to write "scripts" which define functions and
implement complex programs using builtin and user-defined functions.
Anything in Statistics which can be expressed in terms of vectors
and matrices can easily be implemented in Matlab/octave. For a good
few years I did all my statistical computing on Matlab, then octave.

However, R beats either in terms of the flexibility and depth
that the language offers. This however makes it a considerably
more formidable learning task to get full benefit from it.

> 2. Learning curve.  If somebody could share his/her experience on
> using R or Matlab or both in the classrom, how students take to it.

No comment here for classrom use (except in the background for
demonstration -- I haven't taught students how to use either for
their own work, though I have tuaght PhD students how to use
octave/Matlab and it worked well. I think at  some of these might
not have got on too easily with R.

However, one point to consider is the graphical capabilities of
Matlab/octave vs R. Probably matlab has come a long way on this
front since I used to use it, but even in its early days it
offered a wider range of graphics than octave did. Octave has
always done graphics by creating files of commands which it
submits to gnuplot, so it is limited by what the capabilities
of gnuplot are at the time. On the other hand, you can suss out
how octave does this and write your own scripts for implementing
graphics in other commandfile-driven graphics programs (I once did
this for Kenny Toh's now apparently dormant[*] PlotMTV, and it has
also been done for PLPlot -- quite effectively in my view; you could
even write an octave interface to R graphics if you wanted!). 
[*] Not further developed since about 1977, I think, though a
    very useful program.

> 3. Which language do you think is better for students' further
> development?  We would like to equip them with something they can use
> later on in their career even if they don't become theoretical
> economists (very few undergraduate students do that).

This has to depend on your judgement as to which language better
provides the tools they will need in that field (or may be further
developed to provide them). 

Matlab/octave are particularly well equipped to handle "signal
processing" types of computation, especially with their special
"toolboxes" -- assemblages of supplementary functions written
for these purposes. This includes all kinds of filters, spectral
analyses, etc. You can also readily supplement these with routines
for additional analyses. As pointed out above, a lot of statistical
analyses can be programmed by someone who knows what they are doing.

They are also very well adapted for modelling and simulation, both
deterministic and stochatic, and come equipped with solvers for systems
of differential equations (non-linear as well as linear) and numerical
quadrature routines.

On the other hand, R is remarkable for its very broad coverage of
different types of statistical analysis and model-fitting.

Probably either is equally straightforward to use for basic work.
For more advanced or complex work it may be better to use both,
each for the kind of task it is better equipped for.

> 4. How flexible are these languages when developing new
> applications/functions?  Very few of the problems I encounter have a
> ready-made solution in a toolbox/library.

In my experience, Matlab/octave -- because of the essential simplicity
of both the programming language and the primtiive data types,
is easiest and simplest for developing new applications or functions,
provided you know well what it is you want to implement. But you
may have to do a lot of work on the detail.

On the other hand, R's complexity of language structure and richness
of objects and types offers great flexibility with little effort
provided you are familiar enough with its resources. I personally
find that a lot of the time and effort I spend when developing
applications goes on developing this familiarity.

On a sort of analogy, working with Matlab/octave could be likened to
working on an older (non-electronic) car: so long as you have good
mechanical sense, are handy with a few tools, and are prepared
to undo a lot of nuts and bolts, it is straightfoward though
possibly tedious.

Working with R is sometimes more like neurosurgery: you have to become
aware of the existence of the particular structure you need to deal
with, where to locate it, and what its function is in relation to
other structures; and, when you have finally reached it, you may need
to handle it with delicacy and finesse!

Hoping this is useful!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 25-Apr-04                                       Time: 21:14:51
------------------------------ XFMail ------------------------------



From alain.yamakana at rogers.com  Mon Apr 26 03:37:49 2004
From: alain.yamakana at rogers.com (Alain Yamakana)
Date: Sun, 25 Apr 2004 21:37:49 -0400
Subject: [R] Bug?
References: <Pine.LNX.4.44.0404251715540.4536-100000@gannet.stats>
Message-ID: <004301c42b2f$1658eb80$1d503141@bloor.phub.net.cable.rogers.com>

Thanks Professor Ripley and Professor Murdoch. I successfully downloaded and
installed the suggested R-patched. Prof Ripley's question makes me realize
that there may be conflict between R and SPSS 12. I had SPSS on my machine
(Pentium IV from Gateway running under Windows XP) before I installed R1081.
Three weeks I noticed that my SPSS 12 was not working. I have been told it's
because I manually changed the location from Program files to C:\Stats. I
used to do that under Windows 95 without any problem. The technician then
reinstalled the SPSS 12 in C:\Stats. He tried it and it seemed to work; this
was about April 2, 04. Since I didn't use SPSS 12, but I did run R1081.
Today, the SPSS 12 is not working; it fails to start. Hopefully this
information will be of any interest. In the meantime my problem is solved
and I am very appreciative of your very kind and prompt reply.

Best regards,
Alain

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Alain Yamakana" <alain.yamakana at rogers.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, April 25, 2004 12:23 PM
Subject: Re: [R] Bug?


> Yes, it is a bug, but not in R.  It is a bug in a Windows XP critical
> update: are you *sure* no new software has been installed behind your
> back?
>
> The fix (which you can find several times in the list archives) is to add
>
> R_USER=c:/
>
> (or better, the correct path to your home aka personal directory) to the
> shortcut you use to start R.
>
> There is a workaround in R-patched, usually available from
>
> http://cran.r-project.org/bin/windows/base/rpatched.html
>
> but I think that has the wrong version at present since it is labelled
> R-2.0.0 patch build for Windows and contains rw2000dev.exe, not
> rw1090pat.exe.
>
> On Sun, 25 Apr 2004, Alain Yamakana wrote:
>
> > Either something is wrong with my machine or there may be a bugg. I had
run
> > R-1.8.1 two weeks ago. Since yesterday, I am getting this message:<<R
for
> > Windows GUI front-end has encountered a problem and needs to close. We
are
> > sorry for the inconvenience.>> No new software has been installed since
I
> > installed R-1.8.1. Tired to fail getting R-1.8.1 work, I decided to
install
> > R-1.9.0 and got the following message <<Fatal Error. HOMEDRIVE>>. How to
> > solve either one of these two problems?
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Mon Apr 26 08:05:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Apr 2004 07:05:06 +0100 (BST)
Subject: [R] Bug?
In-Reply-To: <004301c42b2f$1658eb80$1d503141@bloor.phub.net.cable.rogers.com>
Message-ID: <Pine.LNX.4.44.0404260702530.22842-100000@gannet.stats>

There is no known conflict between R and SPSS 12: we run both on Windows 
XP machines.

You seem very keen to blame R for the problems on your machine!

On Sun, 25 Apr 2004, Alain Yamakana wrote:

> Thanks Professor Ripley and Professor Murdoch. I successfully downloaded and
> installed the suggested R-patched. Prof Ripley's question makes me realize
> that there may be conflict between R and SPSS 12. I had SPSS on my machine
> (Pentium IV from Gateway running under Windows XP) before I installed R1081.
> Three weeks I noticed that my SPSS 12 was not working. I have been told it's
> because I manually changed the location from Program files to C:\Stats. I
> used to do that under Windows 95 without any problem. The technician then
> reinstalled the SPSS 12 in C:\Stats. He tried it and it seemed to work; this
> was about April 2, 04. Since I didn't use SPSS 12, but I did run R1081.
> Today, the SPSS 12 is not working; it fails to start. Hopefully this
> information will be of any interest. In the meantime my problem is solved
> and I am very appreciative of your very kind and prompt reply.
> 
> Best regards,
> Alain
> 
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Alain Yamakana" <alain.yamakana at rogers.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Sunday, April 25, 2004 12:23 PM
> Subject: Re: [R] Bug?
> 
> 
> > Yes, it is a bug, but not in R.  It is a bug in a Windows XP critical
> > update: are you *sure* no new software has been installed behind your
> > back?
> >
> > The fix (which you can find several times in the list archives) is to add
> >
> > R_USER=c:/
> >
> > (or better, the correct path to your home aka personal directory) to the
> > shortcut you use to start R.
> >
> > There is a workaround in R-patched, usually available from
> >
> > http://cran.r-project.org/bin/windows/base/rpatched.html
> >
> > but I think that has the wrong version at present since it is labelled
> > R-2.0.0 patch build for Windows and contains rw2000dev.exe, not
> > rw1090pat.exe.
> >
> > On Sun, 25 Apr 2004, Alain Yamakana wrote:
> >
> > > Either something is wrong with my machine or there may be a bugg. I had
> run
> > > R-1.8.1 two weeks ago. Since yesterday, I am getting this message:<<R
> for
> > > Windows GUI front-end has encountered a problem and needs to close. We
> are
> > > sorry for the inconvenience.>> No new software has been installed since
> I
> > > installed R-1.8.1. Tired to fail getting R-1.8.1 work, I decided to
> install
> > > R-1.9.0 and got the following message <<Fatal Error. HOMEDRIVE>>. How to
> > > solve either one of these two problems?
> >
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wettenhall at wehi.edu.au  Mon Apr 26 08:24:12 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Mon, 26 Apr 2004 16:24:12 +1000 (EST)
Subject: [R] Tcl Tk table
Message-ID: <Pine.LNX.4.58.0404261621120.15034@unix28.alpha.wehi.edu.au>

Peter Dalgaard wrote:
> do.call("tkcmd", c(list(table1,"tag","cell","gruen"),
>                 as.list(paste(3:8,3:8),sep=","))
>
> (note btw that .Tk.ID() is wrong, you can just pass the widget
> itself)

I think the sep="," should be inside the paste(3:8,3:8)
brackets i.e. paste(3:8,3:8,sep=","), and there is a closing
bracket missing at the end.

The .Tk.ID() was probably copied from one of my examples.  Oops.
I must have been confusing tkcmd(...) with .Tcl(paste(...))

Just to expand on earlier answers, if the table widget has a Tk
ID of .1.1, and you effectively want this Tcl command:
.Tcl(".1.1 tag celltag gruen 3,3 4,4 5,5 6,6 7,7 8,8")

then neither passing "3,3 4,4 5,5 6,6 7,7 8,8" to tkcmd nor
passing c("3,3","4,4","5,5","6,6","7,7","8,8") to tkcmd will
work because in both case the collection of
cell-coordinate-pairs will be passed as a Tcl list which is one
argument of the command whereas you want multiple arguments,
i.e. it will be passed as this Tcl list:
{3,3 4,4 5,5 6,6 7,7 8,8}
or equivalently:
"3,3 4,4 5,5 6,6 7,7 8,8"

To get each cell-coordinate-pair passed as a separate argument,
use do.call or eval(as.call(list(...))), for example:

eval(as.call(list(tkcmd,.Tk.ID(table1),"tag","celltag",
  "gruen","3,3","4,4", "5,5","6,6", "7,7", "8,8")))

As noted previously, in R >= 1.8.0 tclArray() is a better way
to handle Tcl arrays. 

Regards,
James



From ripley at stats.ox.ac.uk  Mon Apr 26 08:35:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 26 Apr 2004 07:35:09 +0100 (BST)
Subject: [R] stepping into a .C call
In-Reply-To: <C4178DC99E08604EA5E2BDB989F0938024210C@extas2-hba.tas.csiro.au>
Message-ID: <Pine.LNX.4.44.0404260726440.22842-100000@gannet.stats>

On Mon, 26 Apr 2004 Toby.Patterson at csiro.au wrote:

> No - not from debug directly. You need to read "writing R extensions" in
> the help. 

You can step into the call with a C-level debugger like gdb (which Writing
R Extensions introduces in section `Finding entry points in dynamically
loaded code'), but only if the package was compiled (with debug options)  
on your machine and the sources are still installed.  However, it is
almost certainly easier to read the C source code, which is in the package
sources.

The distributed Windows binaries (and `Fred J.' quoted Windows in PR#6812)  
are not compiled with debug options, so to do this under Windows you need 
to start by compiling R and the package yourself.


> -----Original Message-----
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Fred J.
> Sent: Monday, April 26, 2004 7:45 AM
> 
> The Debug and mvbutils packages by Mark Bravington
> have been a great help for me in learning R, until I
> come to a line in the R code like ".C("fun name",...).
> Is there a way to step into this call and see what the
> source code does exactly to the supplied arrgs? 
> One example in the "fdim" package, a line like 
> DK <- .C("pointdif", as.integer(NumRow),
> as.integer(NumCol), as.integer(X), NumpDif =
> as.integer(NumpDif))
> I would like to know what the code does inorder to
> come up with the value to assign to DK.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tpapp at axelero.hu  Mon Apr 26 08:33:57 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Mon, 26 Apr 2004 08:33:57 +0200
Subject: [R] need settings for the listings package
Message-ID: <20040426063357.GA1538@localhost>

Hi,

I am typesetting R code in TeX using the listings package.  I have
experimented with various settings for the text, but all look a bit
ugly.  This might be because I have no typographic experience ;-)

I would really appreciate if people sent me the settings they use for
the listings package (eg in the \usepackage line, or \lstset, I am
thinking about choices for basicstyle, ... etc).  I could experiment
with lost of different settings, but I guess that some people who use
listings have already come up with a style they like.  I am printing
in black and white (though color is possible, I just don't think it
would add much, but color settings are welcome too).

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From arinbasu at softhome.net  Mon Apr 26 09:28:58 2004
From: arinbasu at softhome.net (arinbasu@softhome.net)
Date: Mon, 26 Apr 2004 01:28:58 -0600
Subject: [R] Looking for help in calculating percentiles
In-Reply-To: <200404251003.i3PA2Sq1013135@hypatia.math.ethz.ch> 
References: <200404251003.i3PA2Sq1013135@hypatia.math.ethz.ch>
Message-ID: <courier.408CBA3A.000024E2@softhome.net>

Hi All: 

I am working with a dataset on Arsenic toxicity, and I am trying to 
calculate the 20th, 40th, 60th, 80th, and highest percentiles for a 
variable, dietary Moisture (variable name dMoist). 

The inbuilt function quantile(dMoist) would print 0, 25th, 50th, 75th, and 
100th percentile. Does there exist a function that can calculate xth 
percentile (where x = 10th, 20th, ... etc) values? 

I looked for such functions in the documentation, but couldn't find one. Has 
anyone written a similar function that can be used? Is there a module or a 
function that I have missed? 

I use R in both Windows XP as well as Linux (Fedora Core 1 (Yarrow)) on a 
Duron I Gz machine with 128 MB RAM. 

Thanks in Advance,
Arin Basu



From Matthias.Templ at statistik.gv.at  Mon Apr 26 09:36:09 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 26 Apr 2004 09:36:09 +0200
Subject: AW: [R] Looking for help in calculating percentiles
Message-ID: <83536658864BC243BE3C06D7E936ABD5015368A0@xchg1.statistik.local>

Type e.g.:  
quantile(x,0.1)
or
Quantile(x,0.8)

Which calculates the 10th and 80th quantile

Matthias Templ

-----Urspr??ngliche Nachricht-----
Von: arinbasu at softhome.net [mailto:arinbasu at softhome.net] 
Gesendet: Montag, 26. April 2004 09:29
An: r-help at stat.math.ethz.ch
Betreff: [R] Looking for help in calculating percentiles


Hi All: 

I am working with a dataset on Arsenic toxicity, and I am trying to 
calculate the 20th, 40th, 60th, 80th, and highest percentiles for a 
variable, dietary Moisture (variable name dMoist). 

The inbuilt function quantile(dMoist) would print 0, 25th, 50th, 75th, and 
100th percentile. Does there exist a function that can calculate xth 
percentile (where x = 10th, 20th, ... etc) values? 

I looked for such functions in the documentation, but couldn't find one. Has 
anyone written a similar function that can be used? Is there a module or a 
function that I have missed? 

I use R in both Windows XP as well as Linux (Fedora Core 1 (Yarrow)) on a 
Duron I Gz machine with 128 MB RAM. 

Thanks in Advance,
Arin Basu

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ekato at ees.hokudai.ac.jp  Mon Apr 26 10:14:00 2004
From: ekato at ees.hokudai.ac.jp (Etsushi Kato)
Date: Mon, 26 Apr 2004 17:14:00 +0900
Subject: [R] npmc problem?
Message-ID: <AC36F4BE-9759-11D8-9F16-00039394E428@ees.hokudai.ac.jp>

Hi,

I recently tried to conduct non-parametric multiple comparison with 
npmc package, and encountered some problem; sometimes it stopped with 
following error.

Error in uniroot(f = function(arg) p - z.dist(arg, corr = corr, df = 
df,  :
         f() values at end points not of opposite sign


Although I'm not quite sure about calculation of z.quaintile in npmc.R, 
it seems increment of upper (or decrement of lower) is not sufficient 
since the z.dist() value is not repeatable (randomized values) resulted 
from mvtnorm, and sometimes failed to do uniroot().

For my case, I can avoid the error with changing upper and lower with 
adding or subtracting 2 for uniroot.    Is it safe to use such modified 
verson of npmc?

--- npmc.orig   2004-04-26 14:13:00.000000000 +0900
+++ npmc        2004-04-26 16:50:59.000000000 +0900
@@ -131,7 +131,7 @@
          lower <- lower-1;
      }
      ur <- uniroot(f=function(arg) 
p-z.dist(arg,corr=corr,df=df,sides=sides),
-                  upper=upper, lower=lower
+                  upper=upper+2, lower=lower-2
                    );
      ur$root;
    }

Here is the my test dataframe.

"class" "var"
"1"  1197.90816
"1"  3725.82418
"1"  1159.66667
"1"  2835.05556
"1"  2093.20000
"2"  3012.50000
"2"  1895.52381
"2"  3033.07692
"2"  1790.00000
"2"  4103.60000
"2"  1870.13636
"2"  3331.15385
"2"  2066.00000
"2"  2021.92308
"3"  2440.05000
"3"  4615.78893
"3"  2063.50000
"3"  1193.65790
"4"  1569.62500
"4"  1912.72222
"4"  4708.72727
"4"  1722.95833
"4"  2809.14286
"5"  1077.43478
"5"   193.77500
"5"  1230.23529
"5"  8180.73171
"5"  3879.00000
"5"  1050.55556
"6"  1406.72000
"6"   208.94737
"6"  1276.68421
"6"   639.85714
"6"   793.73684
"6"  1043.83871
"8"   647.55000
"8"    97.81000
"8"   208.67308
"8"   765.85000
"8"  2803.00000
"8"   543.71429
"8"   120.16000
"8"  5757.25000
"8"  5211.62500
"9"  3675.00000
"9"   479.09508
"9"  2620.70707
"9"  3095.75000
"9"   633.16250
"10"  2397.03390
"10"   524.37625
"10"  2693.09524
"10"   796.36250
"10"  2261.87500
"10"  1122.01250
"11"  1543.10000
"11"  2127.04545
"11"  1692.87500
"12"  2752.75000
"12"   385.43750
"12"  4576.50000
"12"  7006.12500
"12"  2779.87500
"12"  2704.25000
"13"  3715.25000
"13"  3447.87500
"13"   509.60000
"13"  3238.62500
"13"   674.06250
"13"  4329.62500
"13"  5593.50000
"14"  2666.87500
"14"  1636.37500
"14"   195.76250
"14"   687.06250
"14"  2175.50000
"14"  1385.87500
"15"   571.41250
"15"  2457.89812
"15"  4110.75935
"16"  3279.87500
"16"  2788.50000
"16"   641.95000
"16"   431.38280
"16"  1343.25000
"16"  1952.62500
"17"  2917.00000
"17" 14622.54902
"17"  5030.37500
"19"  1118.98750
"19"   609.13750
"19"  1580.37500
"19"   770.58750
"19"    53.66231
"19"   253.28750
-- 
Etsushi Kato
ekato at ees.hokudai.ac.jp



From p.dalgaard at biostat.ku.dk  Mon Apr 26 10:11:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Apr 2004 10:11:45 +0200
Subject: [R] Tcl Tk table
In-Reply-To: <Pine.LNX.4.58.0404261621120.15034@unix28.alpha.wehi.edu.au>
References: <Pine.LNX.4.58.0404261621120.15034@unix28.alpha.wehi.edu.au>
Message-ID: <x2k703i2f2.fsf@biostat.ku.dk>

James Wettenhall <wettenhall at wehi.edu.au> writes:

> Peter Dalgaard wrote:
> > do.call("tkcmd", c(list(table1,"tag","cell","gruen"),
> >                 as.list(paste(3:8,3:8),sep=","))
> >
> > (note btw that .Tk.ID() is wrong, you can just pass the widget
> > itself)
> 
> I think the sep="," should be inside the paste(3:8,3:8)
> brackets i.e. paste(3:8,3:8,sep=","), and there is a closing
> bracket missing at the end.

Right, thanks. I didn't have tktable to hand so I couldn't check...
 
> To get each cell-coordinate-pair passed as a separate argument,
> use do.call or eval(as.call(list(...))), for example:
> 
> eval(as.call(list(tkcmd,.Tk.ID(table1),"tag","celltag",
>   "gruen","3,3","4,4", "5,5","6,6", "7,7", "8,8")))
> 
> As noted previously, in R >= 1.8.0 tclArray() is a better way
> to handle Tcl arrays. 

Doesn't really help with tagging though (or does it?)

BTW, .Tk.ID sneaked in again... I wonder what is the cleanest way of
doing this stuff. This works (inserting "list" to compensate for
absence of tktable):

> part1 <- .Tcl.args.objv("list",tt,"tag","celltag","gruen")
> part2 <- as.list(paste(3:8,3:8,sep=","))
> do.call("tkcmd",c(part1,part2))
<Tcl> .1 tag celltag gruen 3,3 4,4 5,5 6,6 7,7 8,8

as does this:

> part1 <- .Tcl.args.objv("list",tt,"tag","celltag","gruen")
> part2 <- lapply(paste(3:8,3:8,sep=","),as.tclObj)
> .Tcl.objv(c(part1,part2))
<Tcl> .1 tag celltag gruen 3,3 4,4 5,5 6,6 7,7 8,8

However, having any dot-function called by the user signals a design
problem to me (especially since .Tcl.objv does not check its arguments
at all, so BadThings will happen if you pass it something that is not
a list of tclObj's).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jasont at indigoindustrial.co.nz  Mon Apr 26 10:25:12 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Mon, 26 Apr 2004 20:25:12 +1200 (NZST)
Subject: [R] Looking for help in calculating percentiles
In-Reply-To: <courier.408CBA3A.000024E2@softhome.net>
References: <200404251003.i3PA2Sq1013135@hypatia.math.ethz.ch>
	<courier.408CBA3A.000024E2@softhome.net>
Message-ID: <12857.203.9.176.60.1082967912.squirrel@webmail.maxnet.co.nz>

> Hi All:
>
> I am working with a dataset on Arsenic toxicity, and I am trying to
> calculate the 20th, 40th, 60th, 80th, and highest percentiles for a
> variable, dietary Moisture (variable name dMoist).

?quantile
> quantile(rnorm(100),c(0.2,0.4,0.6,0.8,1))
       20%        40%        60%        80%       100%
-1.0023956 -0.2448597  0.2846434  0.8926857  3.0120353



From s-plus at wiwi.uni-bielefeld.de  Mon Apr 26 10:54:04 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Mon, 26 Apr 2004 10:54:04 +0200
Subject: [R] Tcl Tk table
References: <200404231630.i3NGUIUd007371@smtp.hispeed.ch>
Message-ID: <408CCE2C.2060703@wiwi.uni-bielefeld.de>

Hello Thomas,

sorry, I missed the point. James proposals work very well . I prefer the 
second one (b)

(a) 
eval(as.call(list(tkcmd,.Tk.ID(table1),"tag","celltag","gruen","5,5","6,6", 
"7,7", "8,8")))

(b) .Tcl(paste(.Tk.ID(table1),"tag celltag  gruen 3,3 4,4"))

Now we can define functions like

set.color<-function(widget,colorname,coords,rows,cols){
  # coords: string of type  "2,2 4,5 9,6" or
  # (rows,cols): rows and column number of the cells to be marked
  if(missing(coords)) coords<-paste(paste(rows,cols,sep=","),collapse=" ")
  .Tcl(paste(.Tk.ID(widget),"tag celltag ",colorname, coords))
  tkcmd(.Tk.ID(widget),"tag","configure",colorname,bg=colorname,fg="red")
}
set.color(table1,"blue", "2,1 3,4 5,2")
set.color(table1,"red", rows=3:1, cols=1:3)

Peter

James Wettenhall wrote:

> Just to expand on earlier answers, if the table widget has a Tk
> ID of .1.1, and you effectively want this Tcl command:
> .Tcl(".1.1 tag celltag gruen 3,3 4,4 5,5 6,6 7,7 8,8")

> then neither passing "3,3 4,4 5,5 6,6 7,7 8,8" to tkcmd nor
> passing c("3,3","4,4","5,5","6,6","7,7","8,8") to tkcmd will
> work because in both case the collection of
> cell-coordinate-pairs will be passed as a Tcl list which is one
> argument of the command whereas you want multiple arguments,
> i.e. it will be passed as this Tcl list:
> {3,3 4,4 5,5 6,6 7,7 8,8}
> or equivalently:
> "3,3 4,4 5,5 6,6 7,7 8,8"
>
> To get each cell-coordinate-pair passed as a separate argument,
> use do.call or eval(as.call(list(...))), for example:
>
> eval(as.call(list(tkcmd,.Tk.ID(table1),"tag","celltag",
    "gruen","3,3","4,4", "5,5","6,6", "7,7", "8,8")))

thsudler at swissonline.ch wrote:

>Hi
>
>Yes, I'm sure. I allready installed this additional package. The command
>
>tclRequire("Tktable")
>
>doesn't cause an error. So I've the Tktable package. But the command with "list" doesn't works:
>
>tkcmd(.Tk.ID(table1),"tag","celltag","gruen",list(c("3,3", "4,4", "5,5", "6,6", "7,7", "8,8")))
>tkcmd(.Tk.ID(table1),"tag","configure","gruen",bg="green",fg="green")
>
>Error message:
>Error in switch(storage.mode(x), character = .External("RTcl_ObjFromCharVector",  :  Cannot handle object of mode  list
>
>Do you know why this could be?
>
>Thomas
>
>----- Original Message ----- 
>From: "Peter Wolf" <s-plus at wiwi.uni-bielefeld.de>
>To: <thsudler at swissonline.ch>; <r-help at stat.math.ethz.ch>
>Sent: Friday, April 23, 2004 5:50 PM
>Subject: Re: [R] Tcl Tk table
>
>
>  
>
>>thsudler at swissonline.ch wrote:
>>
>>    
>>
>>>Hi 
>>>
>>>I've a problem with the following example:
>>>
>>>library(tcltk)
>>>.Tcl("array unset tclArray")
>>>
>>>myRarray <- matrix(1:1000, ncol=20)
>>>
>>>for (i in (0:49))
>>> for (j in (0:19))
>>>    .Tcl(paste("set tclArray(",i,",",j,") ",myRarray[i+1,j+1],sep=""))
>>>    
>>>tt<-tktoplevel()
>>>
>>>table1 <- tkwidget(tt,"table",variable="tclArray", rows="50", cols="50")
>>>
>>>tkpack(table1)
>>>
>>>#Old version which worked in R 1.6 but it doesn't work with R 1.9 (and also not with 1.8), why??
>>>.....
>>>Under R version 1.6 I had no problem. Now I installed R 1.9 (with ActiveTcl) and my program doesn't work. ...
>>>Thomas
>>>



From Detlef.Steuer at UniBw-Hamburg.DE  Mon Apr 26 11:01:04 2004
From: Detlef.Steuer at UniBw-Hamburg.DE (Detlef Steuer)
Date: Mon, 26 Apr 2004 11:01:04 +0200
Subject: [R] Looking for help in calculating percentiles
In-Reply-To: <courier.408CBA3A.000024E2@softhome.net>
References: <200404251003.i3PA2Sq1013135@hypatia.math.ethz.ch>
	<courier.408CBA3A.000024E2@softhome.net>
Message-ID: <20040426110104.3510414b@gaia.unibw-hamburg.de>

On Mon, 26 Apr 2004 01:28:58 -0600
arinbasu at softhome.net wrote:

> Hi All: 
> 
> I am working with a dataset on Arsenic toxicity, and I am trying to 
> calculate the 20th, 40th, 60th, 80th, and highest percentiles for a 
> variable, dietary Moisture (variable name dMoist). 

Try 
?quantile




-- 
Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****

"Die herrschenden Ideen sind die Ideen der Herrschenden."
--- K. Marx



From Jan.Verbesselt at agr.kuleuven.ac.be  Mon Apr 26 10:43:57 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Mon, 26 Apr 2004 10:43:57 +0200
Subject: [R] Spatial Autocorrelation for point data
Message-ID: <000401c42b6a$9d3a54f0$1145210a@agr.ad10.intern.kuleuven.ac.be>

Hi R helpers,

Is there a function (package?) in R available which tests "spatial
autocorrelation" between points (e.g. vector layer of weather stations)?
(e.g. Moran's I...)

Via the archives we found out that there is a package 'spdep' which uses
grid data for testing spatial autocorrelation.

Thanks a lot,
Jan


_______________________________________________________________________
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From phgrosjean at sciviews.org  Mon Apr 26 11:56:07 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 26 Apr 2004 11:56:07 +0200
Subject: [R] New version of benchmark comparing R with other software
In-Reply-To: <408832CA.90702@libertysurf.fr>
Message-ID: <MABBLJDICACNFOLGIHJOKEOJEFAA.phgrosjean@sciviews.org>

Huummm, interesting! When a matrix calculation is faster with loops,... hope
this is not the end of these nice and easier to read vectorized operations.
Best,

Philippe

P.S.: I'll will update my benchmark with Matlab R13 as soon as I got the
funding to update my license.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Fan
Sent: Thursday, 22 April, 2004 23:02
To: Philippe Grosjean
Cc: r-help at stat.math.ethz.ch; bates at stat.wisc.edu
Subject: Re: [R] New version of benchmark comparing R with other
software


Merci, Philippe.

Just a complement regarding Matlab 6.5 (R13).

R 1.9.0 is globally faster than Matlab 6.5 (tested on an AthlonXP):
                                   Matlab       R
Total time for all 15 tests (sec)  14.29   10.81
Overall mean (sec)                  0.86    0.45

Matlab 6.5 has optimized the loops performance (sth called "Just In Time"),
with spectacular results, codes with loops could be faster then vectorized
codes. Here's the result for the "loops" test:
                                               Matlab     R
Creation of a 220x220 Toeplitz matrix (loops)   0.01  0.50

Bravo to the R dev team, nice job !
Cheers
--
Fan

Philippe Grosjean wrote:
> Hello,
>
> Thanks to Douglas Bates, there is now a new benchmark suite (version 2.3)
> which is compatible with R 1.9.0 and the recent Matrix library (0.8-1 or
> above). You find it at http://www.sciviews.org/other/benchmark.htm. It
> compares R 1.9.0 under Windows with:
> S-PLUS 6.5, Matlab 6.0, O-Matrix 5.6, Octave 2.1.42, Scilab 2.7 and Ox
3.30.
>
> In short, R in its version 1.9.0 and with the new Matrix library, is now
one
> of the fastest matrix calculation package among those tested on the
computer
> and system used (P IV, 1 Gb RAM and Windows XP pro).
>
> Keeping its limitations in mind (only a few functions tested, and
relatively
> artificial situations that may or may not compare with real-world cases),
> this benchmark suite could be useful for deciding which software to chose
> for computing intensive matrix calculations, and also to compare the same
> software (R) on different platforms, with one or several processors,
and/or
> with various optimized BLAS libraries.
>
> Best,
>
> Philippe Grosjean
>
> .......................................................<??}))><....
>  ) ) ) ) )
> ( ( ( ( (   Prof. Philippe Grosjean
> \  ___   )
>  \/ECO\ (   Numerical Ecology of Aquatic Systems
>  /\___/  )  Mons-Hainaut University, Pentagone
> / ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
>  /NUM\/  )
>  \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
>        \ )  email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
> ( ( ( ( (
> ...................................................................
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Mon Apr 26 12:42:32 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 26 Apr 2004 12:42:32 +0200 (CEST)
Subject: [R] Spatial Autocorrelation for point data
In-Reply-To: <000401c42b6a$9d3a54f0$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.44.0404261230180.31826-100000@reclus.nhh.no>

On Mon, 26 Apr 2004, Jan Verbesselt wrote:

> Hi R helpers,
> 
> Is there a function (package?) in R available which tests "spatial
> autocorrelation" between points (e.g. vector layer of weather stations)?
> (e.g. Moran's I...)
> 
> Via the archives we found out that there is a package 'spdep' which uses
> grid data for testing spatial autocorrelation.
> 

I think you will find that spdep contains the tools you need to build 
neighbour lists from points (for example by triangulation, tri2nb()), in 
order to calculate Moran's I. Look at the help pages for knearneigh(), 
tri2nb(), dnearneigh(), and/or graph2nb() to get an idea of the 
possibilities. Other functions can be used to modify triangulation 
neighbours, bu note that if you generate points with no neighbours for 
your definition of neighbours, you will need to specify this in arguments 
to analysis functions. 

Roger Bivand

> Thanks a lot,
> Jan
> 
> 
> _______________________________________________________________________
> Jan Verbesselt 
> Research Associate 
> Lab of Geomatics and Forest Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium 
> Tel:+32-16-329750   Fax: +32-16-329760
> http://gloveg.kuleuven.ac.be/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From talitaperciano at hotmail.com  Mon Apr 26 14:26:14 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Mon, 26 Apr 2004 09:26:14 -0300
Subject: [R] AIC and BIC
Message-ID: <BAY14-F23YqFlro9ZAq00017d43@hotmail.com>

Hello

I'm with a doubt using BIC and AIC. I want to know if both of then are a way 
to steem the best model to use. How i know which of then to choose?



Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
Constru????o de Conhecimento por Agrupamento de Dados - CoCADa



From andy_liaw at merck.com  Mon Apr 26 15:10:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 26 Apr 2004 09:10:22 -0400
Subject: [R] need settings for the listings package
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7C83@usrymx25.merck.com>

My suggestion would be to look at what Sweave does.  It typesets code in
italics and output in upright.  I believe you just need to use the style
file R_HOME/share/texmf/Sweave.sty.

HTH,
Andy

> From: Tamas Papp
> 
> Hi,
> 
> I am typesetting R code in TeX using the listings package.  I have
> experimented with various settings for the text, but all look a bit
> ugly.  This might be because I have no typographic experience ;-)
> 
> I would really appreciate if people sent me the settings they use for
> the listings package (eg in the \usepackage line, or \lstset, I am
> thinking about choices for basicstyle, ... etc).  I could experiment
> with lost of different settings, but I guess that some people who use
> listings have already come up with a style they like.  I am printing
> in black and white (though color is possible, I just don't think it
> would add much, but color settings are welcome too).
> 
> Thanks,
> 
> Tamas
> 
> -- 
> Tam??s K. Papp
> E-mail: tpapp at axelero.hu
> Please try to send only (latin-2) plain text, not HTML or 
> other garbage.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From anne.piotet at urbanet.ch  Mon Apr 26 15:14:40 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Mon, 26 Apr 2004 15:14:40 +0200
Subject: [R] Adding regression surface to cloud plot 
Message-ID: <000c01c42b90$6f80bf10$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040426/dd4f7f29/attachment.pl

From wettenhall at wehi.edu.au  Mon Apr 26 15:17:21 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Mon, 26 Apr 2004 23:17:21 +1000 (EST)
Subject: [R] Tcl Tk table
In-Reply-To: <x2k703i2f2.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404262213190.5340-100000@unix24.alpha.wehi.edu.au>

On 26 Apr 2004, Peter Dalgaard wrote:
> > As noted previously, in R >= 1.8.0 tclArray() is a better way
> > to handle Tcl arrays. 
> 
> Doesn't really help with tagging though (or does it?)

Not that I know of.  I just thought I'd point out to anyone 
still using .Tcl("set tclarray...") or whatever, that while they 
may have copied that from one of my examples previously, now I 
would recommend using the official tclArray() interface in the 
tcltk package which does some nice stuff that the user shouldn't 
have to worry about (see tclArray() and tclVar() functions).
 
> BTW, .Tk.ID sneaked in again... I wonder what is the cleanest way of
> doing this stuff. This works (inserting "list" to compensate for
> absence of tktable):

Oops.  Sorry about the unnecessary .Tk.ID()  

It is very nice having all of the wrapper functions to take 
care of quoting-hell etc. etc. but at the moment there are a 
lot more Tcl/Tk examples and documentation in pure Tcl/Tk than 
R-Tcl/Tk, so often, I firstly ask "How would I do this in 
pure Tcl/Tk?" and then I translate, so that's why using the
.Tk.ID() function feels quite natural to me.  

If I teach myself to avoid .Tk.ID() as much as possible in 
order to improve my R-Tcl/Tk coding style, should I also try to 
avoid tclvalue() as much as possible?  (I'm pretty sure the 
answer is "No".)  They seem similar in that they both convert 
a Tcl/Tk object into a string, but while .Tk.ID() is 
unnecessary/wrong for the example in this thread, tclvalue() 
is still necessary for this:
foo <- tclVar("one two")
tkmessageBox(message=foo)           # Doesn't work :(
tkmessageBox(message=tclvalue(foo)) # Does work :)

So this analogy is my current excuse for mistakenly 
using .Tk.ID() when it isn't needed.

Sometimes I get an error from tcl which I don't really 
understand, and the first question I ask myself is "What 
would the Tcl/Tk command look like if I had written it in pure 
Tcl/Tk?" or more precisely, "What is the Tcl/Tk command being 
sent to Tcl/Tk from R?" 

It seems that this second question was slightly easier to answer 
with the old R-Tcl/Tk interface, where you could use .Tcl.args() 
etc.  .Tcl.args.objv() is very similar but it doesn't look as 
much like a Tcl command as it did with .Tcl.args(), so if you 
wanted to ask a question to someone who only speaks pure Tcl/Tk 
and just gave them the output of .Tcl.args.objv(), they might be 
a little confused (see below).  And if you just looked at the 
tkcmd() function's R code to try to work out what the 
corresponding Tcl/Tk command would be for an R-Tcl/Tk command, 
you may at first think that there's no way to tell within 
R, because .Tcl.objv() calls an external C function. But in 
fact .Tcl.args.objv() actually tells you quite a lot:

> .Tcl.args.objv("wm","title",tt,"The Title")
[[1]]
<Tcl> wm 

[[2]]
<Tcl> title 

[[3]]
<Tcl> .3 

[[4]]
<Tcl> The Title 

Of course, a nicer solution than making it easy to find the 
corresponding Tcl/Tk command so you can go running to the Tcl/Tk 
community for help, is to have the R-Tcl/Tk user base contribute 
to its documentation so eventually it is better than the pure 
Tcl/Tk documentation or the Perl/Tk documentation!!!

Where should we start?  What can I do?

Regards,
James



From ggrothendieck at myway.com  Mon Apr 26 15:20:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 26 Apr 2004 09:20:54 -0400 (EDT)
Subject: [R] Converting Date or chron to POSIXt
Message-ID: <20040426132054.1DD4112D1A@mprdmxin.myway.com>



Everything below was done on Windows XP Pro using R 1.9.0:

Problem 1. 

Converting objects of class Date to POSIXlt appears not to set 
the isdst component as expected:

> # converting a Date to a POSIXlt, at first, appears to work as expected
> as.POSIXlt(as.Date("2004-10-16"))
[1] "2004-10-16"

> # but upon conversion to POSIXct its off by one hour
> as.POSIXct(as.POSIXlt(as.Date("2004-10-16")))
[1] "2004-10-16 01:00:00 Eastern Daylight Time"

> # the isdst component from the first line is 0
> unclass(as.POSIXlt(as.Date("2004-10-16")))$isdst
[1] 0

> # the isdst component if converted directly from character is 1
> as.POSIXlt("2004-10-16")
[1] "2004-10-16"
> unclass(as.POSIXlt("2004-10-16"))$isdst
[1] 1


> # Problem 2.

> # in converting Date directly to POSIXct it appears that the
> # timezone is assumed to be GMT and specifying the tz= arg does
> # not appear to have any effect:

> as.POSIXct(as.Date("2004-10-16"))
[1] "2004-10-15 20:00:00 Eastern Daylight Time"

> # specifying tz="" gives same result
> as.POSIXct(as.Date("2004-10-16"),tz="")
[1] "2004-10-15 20:00:00 Eastern Daylight Time"

> # specifying tz="GMT" gives same result
> as.POSIXct(as.Date("2004-10-16"),tz="GMT")
[1] "2004-10-15 20:00:00 Eastern Daylight Time"

> # The same thing happens when attempting to convert from chron

> # convert chron date to POSIXct
> as.POSIXct(chron("10/16/2004"))
[1] "2004-10-15 20:00:00 Eastern Daylight Time"

> # specify GMT
> as.POSIXct(chron("10/16/2004"),tz="GMT")
[1] "2004-10-15 20:00:00 Eastern Daylight Time"

> # specify current timezone
> as.POSIXct(chron("10/16/2004"),tz="")
[1] "2004-10-15 20:00:00 Eastern Daylight Time"

My current workaround is to convert Date or chron
objects to character and then convert the character
representation:

> # format chron date and then convert.  Now its ok.
> options(chron.year.abb=F)
> as.POSIXct( format( chron("10/15/2004"), "y-m-d") )
[1] "2004-10-15 Eastern Daylight Time"

> # format Date date and then convert.  Now its ok.
> as.POSIXct( format( as.Date("2004-10-15")) )
[1] "2004-10-15 Eastern Daylight Time"



From tpapp at axelero.hu  Mon Apr 26 15:08:33 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Mon, 26 Apr 2004 15:08:33 +0200
Subject: [R] text() does not handle lists of expressions
Message-ID: <20040426130833.GA761@localhost>

Hi,

text() does not work for lists of expressions.  To demonstrate:

ee <- list(expression(b[1]), expression(b[2]))
plot(1:2, 1:2)
text(1.2,1.2,ee[[1]])                   # fine
text(1.2,1.8,ee[[2]])                   # fine
text(1.8,c(1.2,1.8),ee)                 # wrong

So I have to use a loop for that.  I cannot coerce lists of
expressions to vectors, so in this case, I cannot exploit the feature
of text() accepting x and y coordinates (and other things) as vectors.

I do not mean to say that this is a bug, it may simply be "unintended
behaviour", but it would be really nice if text() could handle lists.
Or is is there a way to have a vector of expressions that I am not
aware of?

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From booth_n at hotmail.com  Mon Apr 26 15:33:03 2004
From: booth_n at hotmail.com (booth n)
Date: Mon, 26 Apr 2004 13:33:03 +0000
Subject: [R] getting R 1.8 or 1.9 to work!?
Message-ID: <Sea1-F78M7hxWlMX5u700050f96@hotmail.com>

I've had a interesting day with R; can anyone offer any advice?

First with R 1.8.1. on XP Pro:

"R for Windows GUI front-end has encountered a problem and needs to close.
We are sorry for the inconvenience."

I then tried installing 1.9.0. and got the "information" dialogue:

"Fatal error: INVALID HOMEDRIVE"

instead of a functioning R!

I attempted to set HOMEPATH via XP user accounts and advanced...-  but all I
still have is the following:

Microsoft Windows XP [Version 5.1.2600]
(C) Copyright 1985-2001 Microsoft Corp.

C:\>set home
HOMEDRIVE=C:
HOMESHARE=\\concol\concol$

, and the startup directory is the standard "C:\Program Files\R\rw1090"?

Help would be most welcome!



From deepayan at stat.wisc.edu  Mon Apr 26 15:34:50 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 26 Apr 2004 08:34:50 -0500
Subject: [R] text() does not handle lists of expressions
In-Reply-To: <20040426130833.GA761@localhost>
References: <20040426130833.GA761@localhost>
Message-ID: <200404260834.50103.deepayan@stat.wisc.edu>

On Monday 26 April 2004 08:08, Tamas Papp wrote:
> Hi,
>
> text() does not work for lists of expressions.  To demonstrate:
>
> ee <- list(expression(b[1]), expression(b[2]))

Have you tried 

ee <- expression(b[1], b[2])

instead ?

Deepayan



From pgrandeau at wanadoo.fr  Mon Apr 26 15:56:52 2004
From: pgrandeau at wanadoo.fr (Pascal)
Date: Mon, 26 Apr 2004 15:56:52 +0200
Subject: [R] Multidim
Message-ID: <408D1524.7080408@wanadoo.fr>

Does it exist a Windows version of  the package MULTIDIM ?
Thank you.

P. Grandeau



From p.dalgaard at biostat.ku.dk  Mon Apr 26 15:53:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Apr 2004 15:53:40 +0200
Subject: [R] Tcl Tk table
In-Reply-To: <Pine.LNX.4.44.0404262213190.5340-100000@unix24.alpha.wehi.edu.au>
References: <Pine.LNX.4.44.0404262213190.5340-100000@unix24.alpha.wehi.edu.au>
Message-ID: <x21xmaluaj.fsf@biostat.ku.dk>

James Wettenhall <wettenhall at wehi.edu.au> writes:

> If I teach myself to avoid .Tk.ID() as much as possible in 
> order to improve my R-Tcl/Tk coding style, should I also try to 
> avoid tclvalue() as much as possible?  (I'm pretty sure the 
> answer is "No".)  They seem similar in that they both convert 
> a Tcl/Tk object into a string, but while .Tk.ID() is 
> unnecessary/wrong for the example in this thread, tclvalue() 
> is still necessary for this:
> foo <- tclVar("one two")
> tkmessageBox(message=foo)           # Doesn't work :(
> tkmessageBox(message=tclvalue(foo)) # Does work :)
> 
> So this analogy is my current excuse for mistakenly 
> using .Tk.ID() when it isn't needed.

OK, let's remove your excuse then... ;-)

Notice that tclVar objects are like pointers: They represent the
variable rather than its content. I.e. tclvalue on the R side
corresponds to a "$" on the Tcl side:

> library(tcltk)
> foo <- tclVar("one two")
> foo
$env
<environment: 0x907109c>

attr(,"class")
[1] "tclVar"
> ls(foo$env)
[1] "::RTcl1"
> tkcmd("list",fee=foo)
<Tcl> -fee ::RTcl1
> tkcmd("list",fee=tclvalue(foo))
<Tcl> -fee {one two}

Notice that you really do need to pass the variable sometimes, e.g. 

b <- tkcheckbutton(tt,variable=foo)

in which case you do not want tclvalue(foo) unless you managed to
create a variable called "one two". 

So you can't do without tclvalue, but you might want to replace it
with tclObj, which removes the detour via the string rep.:

> tkcmd("list",fee=tclvalue(foo))
<Tcl> -fee {one two}
> tkcmd("list",fee=tclObj(foo))
<Tcl> -fee {one two}


Somewhat more confusing is the relation between tclvalue and
as.character, e.g.

> tclvalue(tclObj(foo))
[1] "one two"
> as.character(tclObj(foo))
[1] "one" "two"

This reflects Tcl's distinction between lists and their string
representation. (With hindsight, I suspect that this use of tclvalue
might better have been avoided in favour of as.string.tclObj() or
similar.)
 
(I'll get back to you on the doc issues). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Mon Apr 26 15:57:51 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 26 Apr 2004 09:57:51 -0400
Subject: [R] Adding regression surface to cloud plot 
In-Reply-To: <000c01c42b90$6f80bf10$6c00a8c0@mtd4>
Message-ID: <20040426135751.QYWM6153.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Anne,

You can make such plots with the rgl package. You'll find an example at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/S-course/index.html>.

I hope this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> Sent: Monday, April 26, 2004 8:15 AM
> To: R list
> Subject: [R] Adding regression surface to cloud plot 
> 
> Hello!
> I would like to add a plot of the regression surface to my 
> cloud plot . Is it possible? 
> Thanks
> 
> Anne



From tplate at blackmesacapital.com  Tue Apr 13 21:47:55 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 13 Apr 2004 13:47:55 -0600
Subject: [R] Line numbers in error messages
In-Reply-To: <Pine.A41.4.58.0404121718260.86522@homer10.u.washington.edu
 >
References: <407AE4F6.5080006@ucdavis.edu>
	<20040413101212.O2137@hortresearch.co.nz>
	<407B164A.6040302@ucdavis.edu>
	<6v9m70l9psl58cgk6sglgv6rabiiiv2bun@4ax.com>
	<6.1.0.6.2.20040412174751.0353afc0@mailhost.blackmesacapital.com>
	<Pine.A41.4.58.0404121718260.86522@homer10.u.washington.edu>
Message-ID: <6.1.0.6.2.20040413133733.035310e8@mailhost.blackmesacapital.com>

I agree it's not completely straightforward, but given the current 
function, the current expression, and a list of which expressions have 
already been executed (presumably from the execution engine), one should be 
able to deduce a line number in many cases.

-- Tony Plate

At Monday 06:32 PM 4/12/2004, Thomas Lumley wrote:
>On Mon, 12 Apr 2004, Tony Plate wrote:
>
> > Isn't source file information often recorded in the "source" attribute on
> > functions (or calls)?  Could either the execution engine or the debugger
> > refer to that information?  (Though, in the debugger it might be impossible
> > to uniquely identify expressions that appear multiple times in the function
> > code.)  If line info was printed out only when source was saved in the
> > "source" attribute, this could still be useful.
>
>Yes, but it's still not that straightforward.  The "source" attribute is
>just text. The execution engine doesn't know where it is in the text, and
>there's no guarantee that, for example, deparse() on an expression will
>return a substring of the original text.  And that's before things get
>really complicated.
>
>Consider:
> > lm(y~x)
>Error in eval(expr, envir, enclos) : Object "y" not found
> > traceback()
>7: eval(expr, envir, enclos)
>6: eval(predvars, data, env)
>5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)
>4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
>3: eval(expr, envir, enclos)
>2: eval(mf, parent.frame())
>1: lm(y ~ x)
>
>The majority of these expressions (lines 3, 4, 5 and 7) do not appear
>anywhere in any of the functions being called.
>
>Now, it might well be possible to produce something that would give
>source line numbers where they were available, with a few false negatives.
>This would be an interesting project, but it isn't trivial.
>
>         -thomas
>
>
>
> > -- Tony Plate
> >
> >
> > At Monday 05:41 PM 4/12/2004, Duncan Murdoch wrote:
> > >On Mon, 12 Apr 2004 15:20:58 -0700, you wrote:
> > >
> > > >Hi Patrick,
> > > >
> > > >>It's very simple using a browser() line in your function somewhere you
> > > >>know your code's OK, then run line by line.
> > > >>
> > > >The problem is that sometimes you have code of a few hundred lines, to
> > > >which you have added a strange little line that craps out because of
> > > >some silly mistake that would be apparent if you knew which line to look
> > > >at.  However....  you don't want to start inserting browser statements
> > > >inside the code, hoping to get close, you just want to know what line
> > > >caused the issue.
> > >
> > >This is something that's on my wish list too, but it would require
> > >fairly low-level changes.  Right now the parser doesn't record source
> > >file information on a line, so there's no way an error message could
> > >report it.
> > >
> > >It's not absolutely obvious how to do it, either:  code can come from
> > >files, from saved images, from stuff you typed at the console prompt,
> > >from a connection, as the result of evaluating an expression, etc.
> > >It's a lot more complicated to do this in an interpreted language like
> > >R than in a compiled language.
> > >
> > >Duncan Murdoch
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
>
>Thomas Lumley                   Assoc. Professor, Biostatistics
>tlumley at u.washington.edu        University of Washington, Seattle



From tplate at blackmesacapital.com  Mon Apr 26 16:48:26 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 26 Apr 2004 08:48:26 -0600
Subject: [R] Line numbers in error messages
Message-ID: <6.1.0.6.2.20040426084459.03b45348@mailhost.blackmesacapital.com>

Please ignore this message -- it was an old one that I meant to delete, not 
send, because I found that in R, information about the expression and 
function of each frame didn't seemed to be stored in dumps, which makes 
this of thing difficult to do (at least I couldn't find this information in 
a sample dump.)

-- Tony Plate

>I agree it's not completely straightforward, but given the current 
>function, the current expression, and a list of which expressions have 
>already been executed (presumably from the execution engine), one should 
>be able to deduce a line number in many cases.
>
>-- Tony Plate
>
>At Monday 06:32 PM 4/12/2004, Thomas Lumley wrote:
>>On Mon, 12 Apr 2004, Tony Plate wrote:
>>
>> > Isn't source file information often recorded in the "source" attribute on
>> > functions (or calls)?  Could either the execution engine or the debugger
>> > refer to that information?  (Though, in the debugger it might be 
>> impossible
>> > to uniquely identify expressions that appear multiple times in the 
>> function
>> > code.)  If line info was printed out only when source was saved in the
>> > "source" attribute, this could still be useful.
>>
>>Yes, but it's still not that straightforward.  The "source" attribute is
>>just text. The execution engine doesn't know where it is in the text, and
>>there's no guarantee that, for example, deparse() on an expression will
>>return a substring of the original text.  And that's before things get
>>really complicated.
>>
>>Consider:
>> > lm(y~x)
>>Error in eval(expr, envir, enclos) : Object "y" not found
>> > traceback()
>>7: eval(expr, envir, enclos)
>>6: eval(predvars, data, env)
>>5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)
>>4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
>>3: eval(expr, envir, enclos)
>>2: eval(mf, parent.frame())
>>1: lm(y ~ x)
>>
>>The majority of these expressions (lines 3, 4, 5 and 7) do not appear
>>anywhere in any of the functions being called.
>>
>>Now, it might well be possible to produce something that would give
>>source line numbers where they were available, with a few false negatives.
>>This would be an interesting project, but it isn't trivial.
>>
>>         -thomas
>>
>>
>>
>> > -- Tony Plate
>> >
>> >
>> > At Monday 05:41 PM 4/12/2004, Duncan Murdoch wrote:
>> > >On Mon, 12 Apr 2004 15:20:58 -0700, you wrote:
>> > >
>> > > >Hi Patrick,
>> > > >
>> > > >>It's very simple using a browser() line in your function somewhere you
>> > > >>know your code's OK, then run line by line.
>> > > >>
>> > > >The problem is that sometimes you have code of a few hundred lines, to
>> > > >which you have added a strange little line that craps out because of
>> > > >some silly mistake that would be apparent if you knew which line to 
>> look
>> > > >at.  However....  you don't want to start inserting browser statements
>> > > >inside the code, hoping to get close, you just want to know what line
>> > > >caused the issue.
>> > >
>> > >This is something that's on my wish list too, but it would require
>> > >fairly low-level changes.  Right now the parser doesn't record source
>> > >file information on a line, so there's no way an error message could
>> > >report it.
>> > >
>> > >It's not absolutely obvious how to do it, either:  code can come from
>> > >files, from saved images, from stuff you typed at the console prompt,
>> > >from a connection, as the result of evaluating an expression, etc.
>> > >It's a lot more complicated to do this in an interpreted language like
>> > >R than in a compiled language.
>> > >
>> > >Duncan Murdoch
>> > >
>> > >______________________________________________
>> > >R-help at stat.math.ethz.ch mailing list
>> > >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > >PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> >
>>
>>Thomas Lumley                   Assoc. Professor, Biostatistics
>>tlumley at u.washington.edu        University of Washington, Seattle



From tlumley at u.washington.edu  Mon Apr 26 16:50:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 26 Apr 2004 07:50:07 -0700 (PDT)
Subject: [R] text() does not handle lists of expressions
In-Reply-To: <20040426130833.GA761@localhost>
References: <20040426130833.GA761@localhost>
Message-ID: <Pine.A41.4.58.0404260747581.110134@homer35.u.washington.edu>

On Mon, 26 Apr 2004, Tamas Papp wrote:

> So I have to use a loop for that.  I cannot coerce lists of
> expressions to vectors, so in this case, I cannot exploit the feature
> of text() accepting x and y coordinates (and other things) as vectors.

An expression is already a vector in this sense
eg
  plot(1:10)
  text(c(4,2),c(2,4),labels=expression(a[1],b[2]))


	-thomas



From ligges at statistik.uni-dortmund.de  Mon Apr 26 16:51:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 26 Apr 2004 16:51:30 +0200
Subject: [R] Multidim
In-Reply-To: <408D1524.7080408@wanadoo.fr>
References: <408D1524.7080408@wanadoo.fr>
Message-ID: <408D21F2.1070104@statistik.uni-dortmund.de>

Pascal wrote:

> Does it exist a Windows version of  the package MULTIDIM ?
> Thank you.

http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html tells 
you that the package does not pass the checks under Windows, hence there 
is no binary on CRAN. You might want to compile from source yourself and 
fix the bug(s).

Uwe Ligges


> P. Grandeau
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Mon Apr 26 16:51:30 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 26 Apr 2004 09:51:30 -0500
Subject: [R] multipanel display of levelplots?
In-Reply-To: <408B7C0C.7060503@stat.berkeley.edu>
References: <408B7C0C.7060503@stat.berkeley.edu>
Message-ID: <200404260951.30767.deepayan@stat.wisc.edu>

On Sunday 25 April 2004 03:51, Yan Wang wrote:
> I cannot achieve a multipanel display of several levelplots. Here is
> part of my code.
>
> At first, create the layout:
> push.viewport(viewport(layout=grid.layout(1, 2)))
>
> Then for the left panel:
> push.viewport(viewport(layout.pos.col=1,layout.pos.row=1))
> push.viewport(viewport(width=0.6, height=0.6))
> levelplot(z~x*y, grid)
> pop.viewport()
>
> Similar code for the right panel.
>
> However, there is always error message for "pop.viewport()" as "Error
> in pop.vp(i == n, recording) : Illegal to pop top-level viewport". If
> I omit "pop.viewport()", the two levelplots were printed on seperate
> pages without a layout of (1,2).
>
> Any suggestion is greatly appreciated.

I'm not sure what you are trying to do here. If you want multipanel 
levelplot displays in the Trellis sense, this is not the way to do it, 
see example(levelplot). If for some reason you want trellis plots in 
grid viewports, then you will have to tell the print method not to 
start a new page, by replacing 

levelplot(z~x*y, grid)

with 

print(levelplot(z~x*y, grid), newpage = FALSE)

push.viewport etc are deprecated, BTW.

Deepayan



From fm3a004 at math.uni-hamburg.de  Mon Apr 26 16:48:34 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 26 Apr 2004 16:48:34 +0200 (MET DST)
Subject: [R] [R-pkgs] Cluster validation statistics in fpc
Message-ID: <Pine.GSO.3.95q.1040426164217.24006B-100000@sun11.math.uni-hamburg.de>

Hi,

this is to announce a new version (1.1-2) of my package fpc. Apart from
the stuff already present in the older version (methods for fixed point
clustering and clusterwise regression, somewhat bug-cleaned and with
faster examples) there is now a function cluster.stats, which computes
some distance-based statistics often used for cluster validation,
description and decision about the number of clusters, including the
so-called corrected rand index and some further methods mentioned in the
Gordon 1999 book on Classification.

Best,
Christian 
 
***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From tpapp at axelero.hu  Mon Apr 26 18:26:25 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Mon, 26 Apr 2004 18:26:25 +0200
Subject: function vectors (was Re: [R] text() does not handle lists of
	expressions)
In-Reply-To: <Pine.A41.4.58.0404260747581.110134@homer35.u.washington.edu>
References: <20040426130833.GA761@localhost>
	<Pine.A41.4.58.0404260747581.110134@homer35.u.washington.edu>
Message-ID: <20040426162625.GA1562@localhost>

On Mon, Apr 26, 2004 at 07:50:07AM -0700, Thomas Lumley wrote:

> An expression is already a vector in this sense
> eg
>   plot(1:10)
>   text(c(4,2),c(2,4),labels=expression(a[1],b[2]))

Thanks (also to Deepayan Sarkar), now I see.  I have found
?character-class, and now I see that vectors can have a mode of
"expression", too.

Another (related) question: why aren't there vectors of
mode="function" in R?  Are they difficult to implement, not needed,
etc., what is the reason for their abscence?  I found that I can make
a vector of functions by using expression and then eval, so there is a
workaround, but I am just curious about this.

Thanks,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From gerifalte28 at hotmail.com  Mon Apr 26 18:37:24 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Mon, 26 Apr 2004 16:37:24 +0000
Subject: [R] Spatial Autocorrelation for point data
Message-ID: <BAY99-F6X0XVpg8ohJS000678ac@hotmail.com>

Dear Jan

There is an excelent spatial library wich includes Moran's I and other 
tests, developed by Richard Davis and Robin Reich from Colorado State 
University but I am not sure that it is currently available in the R site 
since it is still on development.  You may want to email them to see if you 
can have access to their library. You can search for their contact 
information at 
http://search.colostate.edu/index.asp?page=home&module=directory

I hope that this helps

Francisco


>From: "Jan Verbesselt" <Jan.Verbesselt at agr.kuleuven.ac.be>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Spatial Autocorrelation for point data
>Date: Mon, 26 Apr 2004 10:43:57 +0200
>
>Hi R helpers,
>
>Is there a function (package?) in R available which tests "spatial
>autocorrelation" between points (e.g. vector layer of weather stations)?
>(e.g. Moran's I...)
>
>Via the archives we found out that there is a package 'spdep' which uses
>grid data for testing spatial autocorrelation.
>
>Thanks a lot,
>Jan
>
>
>_______________________________________________________________________
>Jan Verbesselt
>Research Associate
>Lab of Geomatics and Forest Engineering K.U. Leuven
>Vital Decosterstraat 102. B-3000 Leuven Belgium
>Tel:+32-16-329750   Fax: +32-16-329760
>http://gloveg.kuleuven.ac.be/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Mon Apr 26 18:47:51 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 26 Apr 2004 18:47:51 +0200
Subject: function vectors (was [R]..... lists of expressions)
In-Reply-To: <20040426162625.GA1562@localhost>
References: <20040426130833.GA761@localhost>
	<Pine.A41.4.58.0404260747581.110134@homer35.u.washington.edu>
	<20040426162625.GA1562@localhost>
Message-ID: <16525.15671.894922.100612@gargle.gargle.HOWL>

>>>>> "Tamas" == Tamas Papp <tpapp at axelero.hu>
>>>>>     on Mon, 26 Apr 2004 18:26:25 +0200 writes:

    Tamas> On Mon, Apr 26, 2004 at 07:50:07AM -0700, Thomas Lumley wrote:
    >> An expression is already a vector in this sense
    >> eg
    >> plot(1:10)
    >> text(c(4,2),c(2,4),labels=expression(a[1],b[2]))

    Tamas> Thanks (also to Deepayan Sarkar), now I see.  I have found
    Tamas> ?character-class, and now I see that vectors can have a mode of
    Tamas> "expression", too.

    Tamas> Another (related) question: why aren't there vectors of
    Tamas> mode="function" in R?  Are they difficult to implement, not needed,
    Tamas> etc., what is the reason for their abscence?  I found that I can make
    Tamas> a vector of functions by using expression and then eval, so there is a
    Tamas> workaround, but I am just curious about this.

since  lists of functions are easy enough to handle,
(and they *are* ``vectors'' (though not atomic ones) on the
 C-level of R).


    Tamas> Thanks,

You're welcome.
Martin



From Erik.B.Johnson at colorado.edu  Mon Apr 26 19:01:12 2004
From: Erik.B.Johnson at colorado.edu (Erik Johnson)
Date: Mon, 26 Apr 2004 11:01:12 -0600
Subject: [R] nnet question
In-Reply-To: <Pine.LNX.4.44.0404260702530.22842-100000@gannet.stats>
References: <Pine.LNX.4.44.0404260702530.22842-100000@gannet.stats>
Message-ID: <408D4058.1080007@colorado.edu>

I am using R 1.8.0, and am attempting to fit a Neural Network model of a 
time series (here called Metrics.data).  It consists of one time series 
variable run on its lag (AR(1)).  Basically, in an OLS model it would 
look like
Metrics.data$ewindx ~ Metrics.data$ewindx.lag1
However, I am trying to run this through a neural network estimation.  
So far, I have been getting convergence very quickly, and do not believe 
it too be true.
Here is the code and output.  Please note that I am using all of the 
values for training and testing in one matrix, as I do not care about 
the testing results right now, I only want to capture weights.  Here is 
the code and output

 > nnet(metrics.data$ewindxlag1,metrics.data$ewindx,size=2, entropy=FALSE)
# weights:  7
initial  value 78858370643.085342
final  value 78841786515.212158
converged
a 1-2-1 network with 7 weights
options were -

When I run the iris3 example, the convergence looks much nicer 
(consisting of more than one iteration).  Am I missing some fundamental 
understanding of this example?  Thanks for any input.



From bamelbourne at ucdavis.edu  Mon Apr 26 18:56:29 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Mon, 26 Apr 2004 09:56:29 -0700
Subject: [R] Bug?
References: <Pine.LNX.4.44.0404251715540.4536-100000@gannet.stats>
	<004301c42b2f$1658eb80$1d503141@bloor.phub.net.cable.rogers.com>
Message-ID: <008601c42baf$6b8cb070$8e93eda9@des.ucdavis.edu>

Alain,
I'm sure you'll find that the Windows critical update KB835732 has been
installed on your machine. This is most likely the problem, not SPSS.
Brett

Brett Melbourne, Postdoctoral Fellow
Center for Population Biology, Storer Hall
University of California Davis CA 95616


----- Original Message ----- 
From: "Alain Yamakana" <alain.yamakana at rogers.com>
To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
Cc: <r-help at stat.math.ethz.ch>; "Duncan Murdoch" <dmurdoch at pair.com>
Sent: Sunday, April 25, 2004 6:37 PM
Subject: Re: [R] Bug?


> Thanks Professor Ripley and Professor Murdoch. I successfully downloaded
and
> installed the suggested R-patched. Prof Ripley's question makes me realize
> that there may be conflict between R and SPSS 12. I had SPSS on my machine
> (Pentium IV from Gateway running under Windows XP) before I installed
R1081.
> Three weeks I noticed that my SPSS 12 was not working. I have been told
it's
> because I manually changed the location from Program files to C:\Stats. I
> used to do that under Windows 95 without any problem. The technician then
> reinstalled the SPSS 12 in C:\Stats. He tried it and it seemed to work;
this
> was about April 2, 04. Since I didn't use SPSS 12, but I did run R1081.
> Today, the SPSS 12 is not working; it fails to start. Hopefully this
> information will be of any interest. In the meantime my problem is solved
> and I am very appreciative of your very kind and prompt reply.
>
> Best regards,
> Alain
>
> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Alain Yamakana" <alain.yamakana at rogers.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Sunday, April 25, 2004 12:23 PM
> Subject: Re: [R] Bug?
>
>
> > Yes, it is a bug, but not in R.  It is a bug in a Windows XP critical
> > update: are you *sure* no new software has been installed behind your
> > back?
> >
> > The fix (which you can find several times in the list archives) is to
add
> >
> > R_USER=c:/
> >
> > (or better, the correct path to your home aka personal directory) to the
> > shortcut you use to start R.
> >
> > There is a workaround in R-patched, usually available from
> >
> > http://cran.r-project.org/bin/windows/base/rpatched.html
> >
> > but I think that has the wrong version at present since it is labelled
> > R-2.0.0 patch build for Windows and contains rw2000dev.exe, not
> > rw1090pat.exe.
> >
> > On Sun, 25 Apr 2004, Alain Yamakana wrote:
> >
> > > Either something is wrong with my machine or there may be a bugg. I
had
> run
> > > R-1.8.1 two weeks ago. Since yesterday, I am getting this message:<<R
> for
> > > Windows GUI front-end has encountered a problem and needs to close. We
> are
> > > sorry for the inconvenience.>> No new software has been installed
since
> I
> > > installed R-1.8.1. Tired to fail getting R-1.8.1 work, I decided to
> install
> > > R-1.9.0 and got the following message <<Fatal Error. HOMEDRIVE>>. How
to
> > > solve either one of these two problems?
> >
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bill.shipley at usherbrooke.ca  Mon Apr 26 18:59:57 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 26 Apr 2004 12:59:57 -0400
Subject: [R] mixed model with binomial link?
Message-ID: <006101c42baf$e72a0b60$801ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040426/a408b2ee/attachment.pl

From HaroldD at ccsso.org  Mon Apr 26 19:06:04 2004
From: HaroldD at ccsso.org (Harold Doran)
Date: Mon, 26 Apr 2004 13:06:04 -0400
Subject: [R] mixed model with binomial link?
Message-ID: <CFF85773D9245040A333571B7E6D651702C4930D@ccssosrv1.ccsso.org>

nlme() should be able to do this.

Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Bill Shipley
Sent: Monday, April 26, 2004 1:00 PM
To: R help list
Subject: [R] mixed model with binomial link?


Hello.  I have to fit a mixed model from a repeated measures split-plot
experiment in which the response variable is binary.  This requires a
generalised linear mixed model in which I can specify a binomial
distribution.  I can't find the appropriate package in R.  I have looked
at glmmML, but it doesn't seem to allow any mixed structure beyond a
simple 2-level one.  Can anyone point me to the appropriate package, so
that I don't have to go to SAS?

Thanks.

Bill Shipley

Subject Matter Editor, Ecology

North American Editor, Annals of Botany

D??partement de biologie, Universit?? de Sherbrooke,

Sherbrooke (Qu??bec) J1K 2R1 CANADA

Bill.Shipley at USherbrooke.ca

 <http://callisto.si.usherb.ca:8080/bshipley/>
http://callisto.si.usherb.ca:8080/bshipley/

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Mon Apr 26 19:17:17 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 26 Apr 2004 12:17:17 -0500
Subject: [R] mixed model with binomial link?
In-Reply-To: <CFF85773D9245040A333571B7E6D651702C4930D@ccssosrv1.ccsso.org>
References: <CFF85773D9245040A333571B7E6D651702C4930D@ccssosrv1.ccsso.org>
Message-ID: <6rvfjm8xr6.fsf@bates4.stat.wisc.edu>

"Harold Doran" <HaroldD at ccsso.org> writes:

> nlme() should be able to do this.

I think it would be better to use GLMM from the lme4 package for this.



From dmurdoch at pair.com  Mon Apr 26 19:20:42 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 26 Apr 2004 13:20:42 -0400
Subject: [R] Bug? (Fatal error:  INVALID HOMEDRIVE)
In-Reply-To: <008601c42baf$6b8cb070$8e93eda9@des.ucdavis.edu>
References: <Pine.LNX.4.44.0404251715540.4536-100000@gannet.stats>
	<004301c42b2f$1658eb80$1d503141@bloor.phub.net.cable.rogers.com>
	<008601c42baf$6b8cb070$8e93eda9@des.ucdavis.edu>
Message-ID: <nsgq801769baoq3tl4usflbk3mgmqbgl0k@4ax.com>

On Mon, 26 Apr 2004 09:56:29 -0700, "Brett Melbourne"
<bamelbourne at ucdavis.edu> wrote :

>Alain,
>I'm sure you'll find that the Windows critical update KB835732 has been
>installed on your machine. This is most likely the problem, not SPSS.

I put up a little web page

http://www.murdoch-sutherland.com/HOMEPATH.html

describing this bug in KB835732.  Can you let me know if your
experience with it matches my description?  

For newcomers reading this, the symptoms are that R fails to start.
In version 1.9.0 the error message is usually "Fatal Error: INVALID
HOMEDRIVE".  The patched version of 1.9.0 can deal with the bug; the
workaround in earlier ones is to put something like "R_USER=c:/" on
the command line that starts R.

Duncan Murdoch



From rossini at blindglobe.net  Mon Apr 26 19:22:24 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 26 Apr 2004 10:22:24 -0700
Subject: [R] mixed model with binomial link?
In-Reply-To: <CFF85773D9245040A333571B7E6D651702C4930D@ccssosrv1.ccsso.org>
	(Harold Doran's message of "Mon, 26 Apr 2004 13:06:04 -0400")
References: <CFF85773D9245040A333571B7E6D651702C4930D@ccssosrv1.ccsso.org>
Message-ID: <858ygi7iy7.fsf@servant.blindglobe.net>


Nope.  nlme != glmm's.  Though it makes for a first (or half)-order
approximation, sometimes. 

best,
-tony


"Harold Doran" <HaroldD at ccsso.org> writes:

> nlme() should be able to do this.
>
> Harold
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Bill Shipley
> Sent: Monday, April 26, 2004 1:00 PM
> To: R help list
> Subject: [R] mixed model with binomial link?
>
>
> Hello.  I have to fit a mixed model from a repeated measures split-plot
> experiment in which the response variable is binary.  This requires a
> generalised linear mixed model in which I can specify a binomial
> distribution.  I can't find the appropriate package in R.  I have looked
> at glmmML, but it doesn't seem to allow any mixed structure beyond a
> simple 2-level one.  Can anyone point me to the appropriate package, so
> that I don't have to go to SAS?
>
> Thanks.
>
> Bill Shipley
>
> Subject Matter Editor, Ecology
>
> North American Editor, Annals of Botany
>
> D??partement de biologie, Universit?? de Sherbrooke,
>
> Sherbrooke (Qu??bec) J1K 2R1 CANADA
>
> Bill.Shipley at USherbrooke.ca
>
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
>
>  
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Erik.B.Johnson at colorado.edu  Mon Apr 26 19:26:20 2004
From: Erik.B.Johnson at colorado.edu (Erik Johnson)
Date: Mon, 26 Apr 2004 11:26:20 -0600
Subject: [R] nnet question
Message-ID: <408D463C.5040708@colorado.edu>

I am using R 1.8.0, and am attempting to fit a Neural Network model of a 
time series (here called Metrics.data).  It consists of one time series 
variable run on its lag (AR(1)).  Basically, in an OLS model it would 
look like
Metrics.data$ewindx ~ Metrics.data$ewindx.lag1
However, I am trying to run this through a neural network estimation.  
So far, I have been getting convergence very quickly, and do not believe 
it too be true.
Here is the code and output.  Please note that I am using all of the 
values for training and testing in one matrix, as I do not care about 
the testing results right now, I only want to capture weights.  Here is 
the code and output

 > nnet(metrics.data$ewindxlag1,metrics.data$ewindx,size=2, entropy=FALSE)
# weights:  7
initial  value 78858370643.085342
final  value 78841786515.212158
converged
a 1-2-1 network with 7 weights
options were -

When I run the iris3 example, the convergence looks much nicer 
(consisting of more than one iteration).  Am I missing some fundamental 
understanding of this example?  Thanks for any input.
-------------- next part --------------
An embedded message was scrubbed...
From: Erik Johnson <ebjohnso at colorado.edu>
Subject: [R] nnet question
Date: Mon, 26 Apr 2004 11:01:12 -0600
Size: 1622
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040426/b2c41f9c/Rnnetquestion.mht

From bonenfan at biomserv.univ-lyon1.fr  Mon Apr 26 19:53:36 2004
From: bonenfan at biomserv.univ-lyon1.fr (Bonenfant Christophe)
Date: Mon, 26 Apr 2004 19:53:36 +0200
Subject: [R] GEE models
Message-ID: <200404261753.i3QHrbt11574@biomserv.univ-lyon1.fr>

Hi there,

I'm trying to fit GEE models on binomial data. My problem is how to "select" a model if ever it is possible and how I decide a factor has to be drop out or not. More specifically:

1) Are QIC (Pan 2001) reliable and can be used in a model selection framework (Burhnam and Anderson 1998)? If yes, how can the QIC be estimated from available information in a gee object?

2) If not, can R compute an ANOVA table like for GLM (seems that SAS does it)? The problem is to know whether a factor with more than 2 levels is significant or not. Can I consider such a factor as significant if at least one modality (using a Wald-test)  is significant?

3) Some models can be fitted with a gml but not with a gee? Is there any explanation for that?

Any answer would be greatly appreciated,

Christophe



------------------------------------------------------------------------
Bonenfant Christophe

Laboratoire de Biom??trie et Biologie Evolutive
UMR CNRS 5558 - Universit?? Claude Bernard Lyon 1
43, Boulevard du 11 Novembre 1918
F-69622 Villeurbanne Cedex

T??l: 04-72-44-81-11
Mobile: 06-62-11-20-00
Courriel: bonenfan at biomserv.univ-lyon1.fr
In Tartiflette we trust...



From bonenfan at biomserv.univ-lyon1.fr  Mon Apr 26 20:04:17 2004
From: bonenfan at biomserv.univ-lyon1.fr (Bonenfant Christophe)
Date: Mon, 26 Apr 2004 20:04:17 +0200
Subject: [R] GEE
Message-ID: <200404261804.i3QI4Jt11839@biomserv.univ-lyon1.fr>

Hi there,

I'm trying to fit GEE models on binomial data. My problem is how to "select" a model if ever it is possible and how I decide a factor has to be drop out or not. More specifically:

1) Are QIC (Pan 2001) reliable and can be used in a model selection framework (Burhnam and Anderson 1998)? If yes, how can the QIC be estimated from available information in a gee object?

2) If not, can R compute an ANOVA table like for GLM (seems that SAS does it)? The problem is to know whether a factor with more than 2 levels is significant or not. Can I consider such a factor as significant if at least one modality (using a Wald-test)  is significant?

3) Some models can be fitted with a gml but not with a gee? Is there any explanation for that?

Any answer would be greatly appreciated,

Christophe



------------------------------------------------------------------------
Bonenfant Christophe

Laboratoire de Biom??trie et Biologie Evolutive
UMR CNRS 5558 - Universit?? Claude Bernard Lyon 1
43, Boulevard du 11 Novembre 1918
F-69622 Villeurbanne Cedex

T??l: 04-72-44-81-11
Mobile: 06-62-11-20-00
Courriel: bonenfan at biomserv.univ-lyon1.fr
In Tartiflette we trust...
------------------------------------------------------------------------

------------------------------------------------------------------------
Bonenfant Christophe

Laboratoire de Biom??trie et Biologie Evolutive
UMR CNRS 5558 - Universit?? Claude Bernard Lyon 1
43, Boulevard du 11 Novembre 1918
F-69622 Villeurbanne Cedex

T??l: 04-72-44-81-11
Mobile: 06-62-11-20-00
Courriel: bonenfan at biomserv.univ-lyon1.fr
In Tartiflette we trust...



From bamelbourne at ucdavis.edu  Mon Apr 26 20:02:25 2004
From: bamelbourne at ucdavis.edu (Brett Melbourne)
Date: Mon, 26 Apr 2004 11:02:25 -0700
Subject: [R] getting R 1.8 or 1.9 to work!?
Message-ID: <003401c42bb8$a135f020$8e93eda9@des.ucdavis.edu>

It's a bug in windows critical update KB835732.
cheers
Brett


----- Original Message ----- 
From: "Duncan Murdoch" <dmurdoch at pair.com>
To: "Brett Melbourne" <bamelbourne at ucdavis.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, April 26, 2004 10:20 AM
Subject: Re: [R] Bug? (Fatal error: INVALID HOMEDRIVE)


> 
> I put up a little web page
> 
> http://www.murdoch-sutherland.com/HOMEPATH.html
> 
> describing this bug in KB835732.  Can you let me know if your
> experience with it matches my description?  
> 
> For newcomers reading this, the symptoms are that R fails to start.
> In version 1.9.0 the error message is usually "Fatal Error: INVALID
> HOMEDRIVE".  The patched version of 1.9.0 can deal with the bug; the
> workaround in earlier ones is to put something like "R_USER=c:/" on
> the command line that starts R.
> 
> Duncan Murdoch



From bonenfan at biomserv.univ-lyon1.fr  Mon Apr 26 20:03:31 2004
From: bonenfan at biomserv.univ-lyon1.fr (Bonenfant Christophe)
Date: Mon, 26 Apr 2004 20:03:31 +0200
Subject: [R] (no subject)
Message-ID: <200404261803.i3QI3Xt11813@biomserv.univ-lyon1.fr>

Hi there,

I'm trying to fit GEE models on binomial data. My problem is how to "select" a model if ever it is possible and how I decide a factor has to be drop out or not. More specifically:

1) Are QIC (Pan 2001) reliable and can be used in a model selection framework (Burhnam and Anderson 1998)? If yes, how can the QIC be estimated from available information in a gee object?

2) If not, can R compute an ANOVA table like for GLM (seems that SAS does it)? The problem is to know whether a factor with more than 2 levels is significant or not. Can I consider such a factor as significant if at least one modality (using a Wald-test)  is significant?

3) Some models can be fitted with a gml but not with a gee? Is there any explanation for that?

Any answer would be greatly appreciated,

Christophe



------------------------------------------------------------------------
Bonenfant Christophe

Laboratoire de Biom??trie et Biologie Evolutive
UMR CNRS 5558 - Universit?? Claude Bernard Lyon 1
43, Boulevard du 11 Novembre 1918
F-69622 Villeurbanne Cedex

T??l: 04-72-44-81-11
Mobile: 06-62-11-20-00
Courriel: bonenfan at biomserv.univ-lyon1.fr
In Tartiflette we trust...
------------------------------------------------------------------------

------------------------------------------------------------------------
Bonenfant Christophe

Laboratoire de Biom??trie et Biologie Evolutive
UMR CNRS 5558 - Universit?? Claude Bernard Lyon 1
43, Boulevard du 11 Novembre 1918
F-69622 Villeurbanne Cedex

T??l: 04-72-44-81-11
Mobile: 06-62-11-20-00
Courriel: bonenfan at biomserv.univ-lyon1.fr
In Tartiflette we trust...



From bill.shipley at usherbrooke.ca  Mon Apr 26 20:36:13 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 26 Apr 2004 14:36:13 -0400
Subject: [R] mixed model with multinomial link?
Message-ID: <006c01c42bbd$5a2e17c0$801ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040426/6a4a9729/attachment.pl

From canty at math.mcmaster.ca  Mon Apr 26 20:38:10 2004
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Mon, 26 Apr 2004 14:38:10 -0400 (EDT)
Subject: [R] names attribute of data.frames after rbind
Message-ID: <Pine.LNX.4.44.0404261420160.31311-100000@mathserv>

Hi, 

If columns of a data.frame have a names attribute and we rbind two
similar data.frames together, the names of the column of the resulting
data.frame only has the correct values for the first component and
has "" in all other positions.  Is this a documented "feature" or
a bug?  If it is intentional, why?  Here is a small example to
show what I mean.  The same behaviour appears on R1.9.0 for Windows
and R1.8.1 for Solaris (I haven't got around to upgrading there yet).

> x1 <- 1:5; names(x1)=1:5
> x2 <- 6:10; names(x2)=6:10
> x1 <- data.frame(x=x1)
> x2 <- data.frame(x=x2)
> x12 <- rbind(x1, x2)
> attributes(x1$x)
$names
[1] "1" "2" "3" "4" "5"

> attributes(x2$x)
$names
[1] "6"  "7"  "8"  "9"  "10"

> attributes(x12$x)
$names
 [1] "1" "2" "3" "4" "5" ""  ""  ""  ""  "" 

> x21 <- rbind(x2, x1)
> attributes(x21$x)
$names
 [1] "6"  "7"  "8"  "9"  "10" ""   ""   ""   ""   ""  


-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From Peter.Ruckdeschel at uni-bayreuth.de  Mon Apr 26 20:39:01 2004
From: Peter.Ruckdeschel at uni-bayreuth.de (Peter Ruckdeschel)
Date: Mon, 26 Apr 2004 20:39:01 +0200
Subject: [R] [R-pkgs] New package: distr
Message-ID: <408D5745.7000203@uni-bayreuth.de>

We would like to announce the availability on CRAN of a new package distr .
It is to provide a conceptual treatment of random variables (r.v.'s) by 
means of
S4--classes. A virtual mother class "Distribution" is introduced.
All distributions of the "base" package are implemented as subclasses of
either "AbscontDistribution" or "DiscreteDistribution".

Using these classes, we also provide (default) methods to automatically
generate the image distributions under unary mathematical operations as
well as a general convolution algorithm.

Additionally, we also provide classes for a standardized treatment of
simulations (also under contaminations) and evaluations of statistical
procedures on such simulations.


DESCRIPTION:

Package: distr
Version: 1.3
Date: 2004/04/23
Title: distr
Authors: Peter Ruckdeschel <peter.ruckdeschel at uni-bayreuth.de>,
Matthias Kohl <matthias.kohl at uni-bayreuth.de>,
Thomas Stabla <statho3 at web.de>,
Florian Camphausen <fcampi at gmx.de>
Maintainer: Peter Ruckdeschel <peter.ruckdeschel at uni-bayreuth.de>
Description: S4 Classes for Distributions
Depends: R (>= 1.9.0),  (version for 1.8.x on URL cited below),
     setRNG (>= 2004.3-1)
License: GPL version 2 or later
URL: http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/

Reference:   
http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/distr.pdf

We look forward to receiving questions, comments and suggestions

Peter Ruckdeschel
Matthias Kohl
Thomas Stabla
Florian Camphausen

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From v_bill_pikounis at merck.com  Mon Apr 26 20:47:37 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Mon, 26 Apr 2004 14:47:37 -0400
Subject: [R] Lost graph contents using Copy as metafile
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F041562B1@usrymx18.merck.com>

Dear colleagues:
I use R 1.9.0 on Windows XP. One of my common tasks is to get R graphs into
Word documents.  A open windows() device, almost always trellis.device() for
me, provides great convenience with the right-click shortcut menu item "Copy
as metafile".  I typically have the History > Recording feature turned on as
well.

Since upgrading to 1.9.0, I have experienced an intermittent problem that I
unfortunately cannot reproduce reliably.  Sometime well within a session,
when I use the "Copy as metafile" technique, the resulting paste into a
MS-Word document results in a blank "picture" graph object as it appears in
Word.  The graph's object outline (to resize, move, etc.) can be seen, but
everything inside is lost.  When I try shutting down the current graphics
device and restart a fresh one, the problem does not go away...the graph
looks fine on the R trellis device window, but its contents cannot be copied
and pasted successfully into a Word document. Again, the embedded object is
there, is identified by Word as a picture, but is blank.  If I re-start R,
all is OK again with "Copy as metafile", i.e.. pasting from the on-screen
device into Word is as desired.

Has anyone else encountered this? I am loath to report it as a bug at this
point, since I cannot reliably reproduce it.

Thanks,
Bill

> version

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.0            
year     2004           
month    04             
day      12             
language R    

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From xiao.gang.fan1 at libertysurf.fr  Mon Apr 26 21:10:26 2004
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Mon, 26 Apr 2004 21:10:26 +0200
Subject: [R] R vs Matlab: which is more "programmer friendly"?
References: <20040425090809.GA704@localhost>
Message-ID: <408D5EA2.9040907@libertysurf.fr>

To get more opinions, you might be also interested to take a
look at MATLAB newsgroup, there were some interesting threads
on the topic, for example:

http://newsreader.mathworks.com/WebX?50 at 88.3Z0AaB5okKu.1@.eec5e6d

--
Fan

Tamas Papp wrote:
> Hi,
> 
> The department of economics at our university (Budapest) is planning a
> course on numerical methods in economics.  They are trying to decide
> which software to use for that, and I would like to advocate R.  The
> other alternative is Matlab.
> 
> I have found comparisons in terms of computational time for matrix
> algebra, but I don't think that is relevant: the bottleneck for
> economists is usually the programmer's time: if it takes a couple of
> hours to write something that is run only a few times, one should not
> care whether it runs in 2 or 2.1 minutes...
> 
> I am an economist, and I have used Octave, but only until I found R.
> So I am not in a position to evaluate Matlab vs R.  I would be
> grateful if somebody could compare R to Matlab, especially regarding
> the following:
> 
> 1. How "smart" the language is.  R appears to be a nice functional
> programming language, is Matlab comparable?  Last time I used Octave,
> it seemed to be little more than syntactic sugar on some C/Fortran
> libraries.  It appears to me that using R gradually pushes people
> towards better programming habits, but I may be biased (I am a Scheme
> lover).
> 
> 2. Learning curve.  If somebody could share his/her experience on
> using R or Matlab or both in the classrom, how students take to it.
> 
> 3. Which language do you think is better for students' further
> development?  We would like to equip them with something they can use
> later on in their career even if they don't become theoretical
> economists (very few undergraduate students do that).
> 
> 4. How flexible are these languages when developing new
> applications/functions?  Very few of the problems I encounter have a
> ready-made solution in a toolbox/library.
> 
> Thanks,
> 
> Tamas
>



From rajarshi at presidency.com  Mon Apr 26 21:16:19 2004
From: rajarshi at presidency.com (Rajarshi Guha)
Date: Mon, 26 Apr 2004 15:16:19 -0400
Subject: [R] R vs Matlab: which is more "programmer friendly"?
In-Reply-To: <408D5EA2.9040907@libertysurf.fr>
References: <20040425090809.GA704@localhost> <408D5EA2.9040907@libertysurf.fr>
Message-ID: <1083006979.27902.4.camel@ra.chem.psu.edu>

On Mon, 2004-04-26 at 15:10, Fan wrote:
> To get more opinions, you might be also interested to take a
> look at MATLAB newsgroup, there were some interesting threads
> on the topic, for example:
> 
> http://newsreader.mathworks.com/WebX?50 at 88.3Z0AaB5okKu.1@.eec5e6d

That was an interesting link.
One point popped up at me. At one one point somebody mentions:

2) easy prototyping 
   - I don't even think that S+ and R are 
     real competitors here.

I have'nt used S+ but I have used R to 'prototype' code and then once I
know the algorithm works, rework it in C for efficiency if required.

Personally, I find R a very nice framework for prototyping numerical
code (my other favourite is Python) since in many cases the 'prototype'
is fast enough for my needs.

Do other people use R in this fashion?

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All great discoveries are made by mistake.
-- Young



From rossini at blindglobe.net  Mon Apr 26 21:57:59 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 26 Apr 2004 12:57:59 -0700
Subject: [R] R vs Matlab: which is more "programmer friendly"?
In-Reply-To: <1083006979.27902.4.camel@ra.chem.psu.edu> (Rajarshi Guha's
	message of "Mon, 26 Apr 2004 15:16:19 -0400")
References: <20040425090809.GA704@localhost> <408D5EA2.9040907@libertysurf.fr>
	<1083006979.27902.4.camel@ra.chem.psu.edu>
Message-ID: <85isfm5x6g.fsf@servant.blindglobe.net>

Rajarshi Guha <rajarshi at presidency.com> writes:

> Personally, I find R a very nice framework for prototyping numerical
> code (my other favourite is Python) since in many cases the 'prototype'
> is fast enough for my needs.
>
> Do other people use R in this fashion?

All the time.  The fun comes when you mix the two prototyping
languages (i.e. via Rpy).

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From k.wang at auckland.ac.nz  Mon Apr 26 22:52:38 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 27 Apr 2004 08:52:38 +1200
Subject: [R] AIC and BIC
References: <BAY14-F23YqFlro9ZAq00017d43@hotmail.com>
Message-ID: <001f01c42bd0$69196380$4333d882@stat.auckland.ac.nz>

Hi,

----- Original Message ----- 
From: "Talita Leite" <talitaperciano at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 27, 2004 12:26 AM
Subject: [R] AIC and BIC


> Hello
>
> I'm with a doubt using BIC and AIC. I want to know if both of then are a
way
> to steem the best model to use. How i know which of then to choose?

This is something that can be easily from by asking the dear old google...

But in general, there are various different model selection techniques and
using some sort of Information Criterion is just one of them.  Personally I
tend to go for BIC, as AIC does not take the number of observations into
account.  Having said that when I do model selection, I would do a few
different *IC's just to be sure...

Cheers,

Kevin



From zelickr at pdx.edu  Mon Apr 26 23:10:30 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Mon, 26 Apr 2004 14:10:30 -0700 (PDT)
Subject: [R] standard library?
Message-ID: <Pine.GSO.4.44.0404261359270.2776-100000@freke.odin.pdx.edu>

Hello all,

I found a package I want to try listed at

      http://www.maths.lth.se/help/R/.R/doc/html/packages.html

I am using R 1.9.0 on winXP.

The package is called R.audio

When I tried the example, it did not work and produced the error message:

      Error: Object "Sound" not found

Normally I would assume that I have to download a package and I would go
looking for it... except I found R.audio in a documentation directory
called "Packages in the standard library"

It *seems* like what I want ought to be available by default, since to me
the idea behind a "standard library" is that it is the stuff you get
*without* shopping around.

Nevertheless, I looked on CRAN for a "standard library" and did not find a
reference to it. I also did not find a reference to R.audio there.

Of course I am missing something obvious! What would that be?

Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From tplate at blackmesacapital.com  Mon Apr 26 23:28:53 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 26 Apr 2004 15:28:53 -0600
Subject: [R] names attribute of data.frames after rbind
In-Reply-To: <Pine.LNX.4.44.0404261420160.31311-100000@mathserv>
References: <Pine.LNX.4.44.0404261420160.31311-100000@mathserv>
Message-ID: <6.1.0.6.2.20040426144613.03882218@mailhost.blackmesacapital.com>

AFAIK, data frames don't usually have names attributes on the 
components.  I don't see this explicitly documented, but Venables & Ripley 
MASS seems to imply this in the section on data frames in Chapter 2 (3rd 
ed) (they say "there is a set of names, the row.names, common to all 
variables").  The documentation for "labels" in Hmisc also seems to imply 
that columns of data frames do not have their own names.

So my question would be: is data.frame() correct in not stripping names off 
components?

A couple of simple examples are:
 > data.frame(x=c(a=2,b=2))[[1]]
a b
2 2
 > as.data.frame(list(x=c(a=2,b=2)))[[1]]
a b
2 2
 >

-- Tony Plate

At Monday 12:38 PM 4/26/2004, Angelo Canty wrote:
>Hi,
>
>If columns of a data.frame have a names attribute and we rbind two
>similar data.frames together, the names of the column of the resulting
>data.frame only has the correct values for the first component and
>has "" in all other positions.  Is this a documented "feature" or
>a bug?  If it is intentional, why?  Here is a small example to
>show what I mean.  The same behaviour appears on R1.9.0 for Windows
>and R1.8.1 for Solaris (I haven't got around to upgrading there yet).
>
> > x1 <- 1:5; names(x1)=1:5
> > x2 <- 6:10; names(x2)=6:10
> > x1 <- data.frame(x=x1)
> > x2 <- data.frame(x=x2)
> > x12 <- rbind(x1, x2)
> > attributes(x1$x)
>$names
>[1] "1" "2" "3" "4" "5"
>
> > attributes(x2$x)
>$names
>[1] "6"  "7"  "8"  "9"  "10"
>
> > attributes(x12$x)
>$names
>  [1] "1" "2" "3" "4" "5" ""  ""  ""  ""  ""
>
> > x21 <- rbind(x2, x1)
> > attributes(x21$x)
>$names
>  [1] "6"  "7"  "8"  "9"  "10" ""   ""   ""   ""   ""
>
>
>--
>------------------------------------------------------------------
>|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
>|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
>|   McMaster University            Fax  : (905) 522-0935         |
>|   1280 Main St. W.                                             |
>|   Hamilton ON L8S 4K1                                          |
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Apr 26 23:34:43 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Apr 2004 23:34:43 +0200
Subject: [R] standard library?
In-Reply-To: <Pine.GSO.4.44.0404261359270.2776-100000@freke.odin.pdx.edu>
References: <Pine.GSO.4.44.0404261359270.2776-100000@freke.odin.pdx.edu>
Message-ID: <x2smeqjudo.fsf@biostat.ku.dk>

Randy Zelick <zelickr at pdx.edu> writes:

> Hello all,
> 
> I found a package I want to try listed at
> 
>       http://www.maths.lth.se/help/R/.R/doc/html/packages.html
> 
> I am using R 1.9.0 on winXP.
> 
> The package is called R.audio
> 
> When I tried the example, it did not work and produced the error message:
> 
>       Error: Object "Sound" not found
> 
> Normally I would assume that I have to download a package and I would go
> looking for it... except I found R.audio in a documentation directory
> called "Packages in the standard library"
> 
> It *seems* like what I want ought to be available by default, since to me
> the idea behind a "standard library" is that it is the stuff you get
> *without* shopping around.
> 
> Nevertheless, I looked on CRAN for a "standard library" and did not find a
> reference to it. I also did not find a reference to R.audio there.
> 
> Of course I am missing something obvious! What would that be?

Every *installation* of R has a "standard library". If you
install packages that's where they go by default. Some multi-user
installations make their help pages available on the net. The
installation at the Technical University at Lund contains something
called R.audio. Since this appears not to be something from CRAN, it
might be a good idea to follow the link in the Author field (to
Henrik's R.classes bundle).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Mon Apr 26 23:40:10 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 26 Apr 2004 17:40:10 -0400
Subject: [R] Lost graph contents using Copy as metafile
In-Reply-To: <CFBD404F5E0C9547B4E10B7BDC3DFA2F041562B1@usrymx18.merck.com>
Message-ID: <20040426214011.LKVX14198.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Bill,

I don't think that you've had a reply yet, so: I reported this problem a
little while ago and understand that it will be fixed.

John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Pikounis, Bill
> Sent: Monday, April 26, 2004 1:48 PM
> To: 'r-help at lists.R-project.org'
> Subject: [R] Lost graph contents using Copy as metafile
> 
> Dear colleagues:
> I use R 1.9.0 on Windows XP. One of my common tasks is to get 
> R graphs into Word documents.  A open windows() device, 
> almost always trellis.device() for me, provides great 
> convenience with the right-click shortcut menu item "Copy as 
> metafile".  I typically have the History > Recording feature 
> turned on as well.
> 
> Since upgrading to 1.9.0, I have experienced an intermittent 
> problem that I unfortunately cannot reproduce reliably.  
> Sometime well within a session, when I use the "Copy as 
> metafile" technique, the resulting paste into a MS-Word 
> document results in a blank "picture" graph object as it 
> appears in Word.  The graph's object outline (to resize, 
> move, etc.) can be seen, but everything inside is lost.  When 
> I try shutting down the current graphics device and restart a 
> fresh one, the problem does not go away...the graph looks 
> fine on the R trellis device window, but its contents cannot 
> be copied and pasted successfully into a Word document. 
> Again, the embedded object is there, is identified by Word as 
> a picture, but is blank.  If I re-start R, all is OK again 
> with "Copy as metafile", i.e.. pasting from the on-screen 
> device into Word is as desired.
> 
> Has anyone else encountered this? I am loath to report it as 
> a bug at this point, since I cannot reliably reproduce it.
> 
> Thanks,
> Bill



From ggrothendieck at myway.com  Mon Apr 26 23:54:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 26 Apr 2004 21:54:25 +0000 (UTC)
Subject: [R] standard library?
References: <Pine.GSO.4.44.0404261359270.2776-100000@freke.odin.pdx.edu>
Message-ID: <loom.20040426T235323-678@post.gmane.org>

Randy Zelick <zelickr <at> pdx.edu> writes:
> I found a package I want to try listed at
> 
>       http://www.maths.lth.se/help/R/.R/doc/html/packages.html
> 
> I am using R 1.9.0 on winXP.
> 
> The package is called R.audio


Check out the installation instructions at:

http://www.maths.lth.se/help/R/R.classes/



From pnick at virgilio.it  Tue Apr 27 00:41:14 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Tue, 27 Apr 2004 00:41:14 +0200
Subject: [R] fractional differenced models
Message-ID: <4086F5AC00007BEB@ims2b.cp.tin.it>

hi everybody
i would like to know how to compute residuals of a fractional differenced
ARIMA model
(i'm using the function fracdiff in the fracdiff package)



From jim.lemon at uts.edu.au  Tue Apr 27 00:53:26 2004
From: jim.lemon at uts.edu.au (Jim Lemon)
Date: Tue, 27 Apr 2004 08:53:26 +1000
Subject: [R] Change in downloading packages
Message-ID: <0HWS00M4GVGTKP@mail.uts.edu.au>

Hi,

I sent an earlier message concerning this, but perhaps I wasn't clear enough. 
Up until about a month ago, I downloaded packages by clicking on the filename 
of the package. Now this action leads me to a directory listing, e.g.:

drwxr-xr-x hornik/users      0 2002-03-20 18:00:54 npmc/
-rw-r--r-- hornik/users    314 2002-03-20 11:33:42 npmc/DESCRIPTION
-rw-r--r-- hornik/users    202 2002-03-20 11:49:40 npmc/INDEX
-rwxr--r-- hornik/users     99 2002-03-20 12:18:34 npmc/build
drwxr-xr-x hornik/users      0 2002-03-20 12:18:15 npmc/man/
...

from which I cannot download anything. It now appears necessary to FTP to:

cran.r-project.org

(or a suitable mirror) change to the:

pub/R/src/contrib

directory and download the package. While I appreciate the functionality of 
install.packages(), one has to be connected to the Internet to use it, and I 
have one old laptop that I use for remote work that has no such connection. 
Downloading the source package is much more convenient than rebuilding a 
package myself from the source code to upgrade this PC. I think that this 
change should at least be announced with a suggestion for a suitable 
workaround for those who need the actual source package.

R-1.8.0 on
RedHat Linux 7.2

Jim

Feel free to ignore any garbage beneath this line.

-- 

DISCLAIMER: This email message and any accompanying attachme...{{dropped}}



From rossini at blindglobe.net  Tue Apr 27 01:00:31 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 26 Apr 2004 16:00:31 -0700
Subject: [R] Change in downloading packages
In-Reply-To: <0HWS00M4GVGTKP@mail.uts.edu.au> (Jim Lemon's message of "Tue,
	27 Apr 2004 08:53:26 +1000")
References: <0HWS00M4GVGTKP@mail.uts.edu.au>
Message-ID: <85r7ua1h0w.fsf@servant.blindglobe.net>


This makes no sense to me.  "Clicking" where?  Sounds like you've
installed one of the unix winzip act-a-likes, i.e. guitar or karchiver
or similar...



Jim Lemon <jim.lemon at uts.edu.au> writes:

> Hi,
>
> I sent an earlier message concerning this, but perhaps I wasn't clear enough. 
> Up until about a month ago, I downloaded packages by clicking on the filename 
> of the package. Now this action leads me to a directory listing, e.g.:
>
> drwxr-xr-x hornik/users      0 2002-03-20 18:00:54 npmc/
> -rw-r--r-- hornik/users    314 2002-03-20 11:33:42 npmc/DESCRIPTION
> -rw-r--r-- hornik/users    202 2002-03-20 11:49:40 npmc/INDEX
> -rwxr--r-- hornik/users     99 2002-03-20 12:18:34 npmc/build
> drwxr-xr-x hornik/users      0 2002-03-20 12:18:15 npmc/man/
> ...
>
> from which I cannot download anything. It now appears necessary to FTP to:
>
> cran.r-project.org
>
> (or a suitable mirror) change to the:
>
> pub/R/src/contrib
>
> directory and download the package. While I appreciate the functionality of 
> install.packages(), one has to be connected to the Internet to use it, and I 
> have one old laptop that I use for remote work that has no such connection. 
> Downloading the source package is much more convenient than rebuilding a 
> package myself from the source code to upgrade this PC. I think that this 
> change should at least be announced with a suggestion for a suitable 
> workaround for those who need the actual source package.
>
> R-1.8.0 on
> RedHat Linux 7.2
>
> Jim
>
> Feel free to ignore any garbage beneath this line.
>
> -- 
>
> DISCLAIMER: This email message and any accompanying attachme...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From gregory_r_warnes at groton.pfizer.com  Tue Apr 27 01:03:08 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Mon, 26 Apr 2004 19:03:08 -0400
Subject: [R] Moving window regressions - how can I improve this code?
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C521318@groexmb02.pfizer.com>


movingWindow can be made even simpler when using the running() with recent
versions of the gregmisc library:


movingWindow <- function(formula, data, width, ...)
  {
    index <- 1:nrow(data)
    running(index, fun=function(st) summary(lm(formula, data[st,])),
width=width)
  }

this returns a *matrix* with one row per element of summary.  To get the
coefficients, do

movingWindow( weight ~ height, women, 5)["coefficients",]

-Greg


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gabor 
> Grothendieck
> Sent: Sunday, April 25, 2004 10:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Moving window regressions - how can I improve this
> code?
> 
> 
> Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
>  
> > movingWindow <- function(formula, data, width, ...) {
> >     nr <- nrow(data)
> >     width <- as.integer(width)[1]
> >     stopifnot( width > 0, width <= nr )
> >     indices <- as.data.frame( t( embed( 1:nr, width ) ) )
> >     lapply(indices, function(st) summary(lm(formula, data[st,])) )
> > }
> > 
> 
> Just one further simplification using apply instead of lapply to
> eliminate having to transform embed:
> 
> movingWindow <- function(formula, data, width, ...) {
>     nr <- nrow(data)
>     width <- as.integer(width)[1]
>     stopifnot( width > 0, width <= nr )
>     apply( embed(1:nr, width), 1, # rows are indices of 
> successive windows
> 		function(st) summary(lm(formula, data[st,])) )
> }
> 
> This could also be used in movingWindow2, as well.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Apr 27 01:03:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2004 01:03:35 +0200
Subject: [R] Change in downloading packages
In-Reply-To: <0HWS00M4GVGTKP@mail.uts.edu.au>
References: <0HWS00M4GVGTKP@mail.uts.edu.au>
Message-ID: <x2k702jq9k.fsf@biostat.ku.dk>

Jim Lemon <jim.lemon at uts.edu.au> writes:

> Hi,
> 
> I sent an earlier message concerning this, but perhaps I wasn't clear enough. 
> Up until about a month ago, I downloaded packages by clicking on the filename 
> of the package. Now this action leads me to a directory listing, e.g.:
> 
> drwxr-xr-x hornik/users      0 2002-03-20 18:00:54 npmc/
> -rw-r--r-- hornik/users    314 2002-03-20 11:33:42 npmc/DESCRIPTION
> -rw-r--r-- hornik/users    202 2002-03-20 11:49:40 npmc/INDEX
> -rwxr--r-- hornik/users     99 2002-03-20 12:18:34 npmc/build
> drwxr-xr-x hornik/users      0 2002-03-20 12:18:15 npmc/man/
> ...
> 
> from which I cannot download anything. It now appears necessary to FTP to:
> 
> cran.r-project.org
> 
> (or a suitable mirror) change to the:
> 
> pub/R/src/contrib
> 
> directory and download the package. While I appreciate the functionality of 
> install.packages(), one has to be connected to the Internet to use it, and I 
> have one old laptop that I use for remote work that has no such connection. 
> Downloading the source package is much more convenient than rebuilding a 
> package myself from the source code to upgrade this PC. I think that this 
> change should at least be announced with a suggestion for a suitable 
> workaround for those who need the actual source package.
> 
> R-1.8.0 on
> RedHat Linux 7.2

The files on CRAN are still .tar.gz files, so I suspect that your
browser is just trying to be smart (have you switched/upgraded it
lately?). 

Normally (dep. upon browser, of course), you can just right-click on
the link and select "Save link target as..." or shift/left-click.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fungile at yahoo.com  Tue Apr 27 01:54:50 2004
From: fungile at yahoo.com (K Fung)
Date: Mon, 26 Apr 2004 16:54:50 -0700 (PDT)
Subject: [R] grep and strings
Message-ID: <20040426235450.23446.qmail@web50402.mail.yahoo.com>

I have the following command:

length(
  grep("\.a\.", names(temp)))

where temp is a vector with names

I want to count the number of entries that contain the
sequence of characters ".a.".  However, R seems to
return all entries that contain "a" (every entry
contains "." in this example).

How should I fix this?  Thanks
		
__________________________________


From spencer.graves at pdf.com  Tue Apr 27 02:06:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 26 Apr 2004 17:06:12 -0700
Subject: [R] grep and strings
In-Reply-To: <20040426235450.23446.qmail@web50402.mail.yahoo.com>
References: <20040426235450.23446.qmail@web50402.mail.yahoo.com>
Message-ID: <408DA3F4.10204@pdf.com>

How about the following: 

 > tst <- c(".a.", "asdf")
 > sum(regexpr("\\.a\\.", tst)>0)
[1] 1

      hope this helps.  spencer graves

K Fung wrote:

>I have the following command:
>
>length(
>  grep("\.a\.", names(temp)))
>
>where temp is a vector with names
>
>I want to count the number of entries that contain the
>sequence of characters ".a.".  However, R seems to
>return all entries that contain "a" (every entry
>contains "." in this example).
>
>How should I fix this?  Thanks
>
>
>__________________________________
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From dmurdoch at pair.com  Tue Apr 27 02:07:26 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 26 Apr 2004 20:07:26 -0400
Subject: [R] Change in downloading packages
In-Reply-To: <0HWS00M4GVGTKP@mail.uts.edu.au>
References: <0HWS00M4GVGTKP@mail.uts.edu.au>
Message-ID: <us8r80d2m98rud11lmbd0nu59chrqlagjp@4ax.com>

On Tue, 27 Apr 2004 08:53:26 +1000, Jim Lemon <jim.lemon at uts.edu.au>
wrote:

>Hi,
>
>I sent an earlier message concerning this, but perhaps I wasn't clear enough. 
>Up until about a month ago, I downloaded packages by clicking on the filename 
>of the package. Now this action leads me to a directory listing, e.g.:
>
>drwxr-xr-x hornik/users      0 2002-03-20 18:00:54 npmc/
>-rw-r--r-- hornik/users    314 2002-03-20 11:33:42 npmc/DESCRIPTION
>-rw-r--r-- hornik/users    202 2002-03-20 11:49:40 npmc/INDEX
>-rwxr--r-- hornik/users     99 2002-03-20 12:18:34 npmc/build
>drwxr-xr-x hornik/users      0 2002-03-20 12:18:15 npmc/man/
>...
>
>from which I cannot download anything. It now appears necessary to FTP to:

I just tried this, and I don't see what you describe.  On the page

http://cran.r-project.org/src/contrib/PACKAGES.html

when I click on npmc, I go to a page describing the package (i.e.
http://cran.r-project.org/src/contrib/Descriptions/npmc.html), and can
click on the package source link to download it.

Maybe you're looking somewhere else?

Duncan Murdoch



From James.Callahan at CityofOrlando.net  Tue Apr 27 02:25:27 2004
From: James.Callahan at CityofOrlando.net (James.Callahan@CityofOrlando.net)
Date: Mon, 26 Apr 2004 20:25:27 -0400
Subject: [R] Re: R vs Matlab: which is more "programmer friendly"?
In-Reply-To: <200404251003.i3PA2Sq2013135@hypatia.math.ethz.ch>
Message-ID: <OFCDDFD4ED.F0BB66F2-ON85256E82.0078686E-85256E83.00025052@ci.orlando.fl.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040426/75578379/attachment.pl

From fungile at yahoo.com  Tue Apr 27 03:12:29 2004
From: fungile at yahoo.com (K Fung)
Date: Mon, 26 Apr 2004 18:12:29 -0700 (PDT)
Subject: [R] grep and strings
In-Reply-To: <408DA3F4.10204@pdf.com>
Message-ID: <20040427011229.75434.qmail@web50409.mail.yahoo.com>

Thanks

Your suggestion works.  The fix is to use double \\
rather than single \.  Changing this makes grep work
as well.


--- Spencer Graves <spencer.graves at pdf.com> wrote:
> How about the following: 
> 
>  > tst <- c(".a.", "asdf")
>  > sum(regexpr("\\.a\\.", tst)>0)
> [1] 1
> 
>       hope this helps.  spencer graves
> 
> K Fung wrote:
> 
> >I have the following command:
> >
> >length(
> >  grep("\.a\.", names(temp)))
> >
> >where temp is a vector with names
> >
> >I want to count the number of entries that contain
> the
> >sequence of characters ".a.".  However, R seems to
> >return all entries that contain "a" (every entry
> >contains "." in this example).
> >
> >How should I fix this?  Thanks
> >
> >
> >__________________________________
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
>
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
		
__________________________________


From ggrothendieck at myway.com  Tue Apr 27 03:41:44 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 27 Apr 2004 01:41:44 +0000 (UTC)
Subject: [R] Re: R vs Matlab: which is more "programmer friendly"?
References: <200404251003.i3PA2Sq2013135@hypatia.math.ethz.ch>
	<OFCDDFD4ED.F0BB66F2-ON85256E82.0078686E-85256E83.00025052@ci.orlando.fl.us>
Message-ID: <loom.20040427T030048-9@post.gmane.org>

 <James.Callahan <at> CityofOrlando.net> writes:

> R dates are a pain 

Note that R 1.9.0 has introduced a new Date class that has no time zones 
making Date easier to use and less error prone than POSIX dates.



From wettenhall at wehi.edu.au  Tue Apr 27 04:27:57 2004
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Tue, 27 Apr 2004 12:27:57 +1000 (EST)
Subject: [R] Tcl Tk table
In-Reply-To: <x21xmaluaj.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404271219550.7739-100000@unix24.alpha.wehi.edu.au>

Maybe time to change the subject line? ;-)

> Notice that tclVar objects are like pointers: They represent the
> variable rather than its content. I.e. tclvalue on the R side
> corresponds to a "$" on the Tcl side:

OK, that does make sense now that I think about it.  When using 
the message= argument of a message box, you want the VALUE of a 
text variable, ($ in Tcl or tclvalue in R-Tcl/Tk), but when you 
want to use the textvariable= argument of an entry box, then you 
need to pass the variable by reference (or a pointer to it), so 
you don't use tclvalue(), you just use the variable name.  So we 
should never try to get rid of tclvalue() but .Tk.ID() is indeed 
unnecessary.

OK, so forget my analogy between tclvalue() and .Tk.ID()  ;-)

Happy Tcling,
James



From wiedemann at fmp-berlin.de  Tue Apr 27 08:43:53 2004
From: wiedemann at fmp-berlin.de (Urs Wiedemann)
Date: Tue, 27 Apr 2004 08:43:53 +0200
Subject: [R] lme - Confidence interval for predicted values
Message-ID: <006301c42c23$016ffb10$0b815fc2@fmpberlin.de>

After having fit (using lme) a mixed effects model with a single
random effect, I like to estimate the confidence interval for the
predicted mean expectations.
To my knowledge this is usually done (for fixed effects models) by
calculating:

cibandwidth <- sqrt(diag(Xnew %*% solve(t(X) %*% X) t(Xnew))) *
qt((1-level)/2, DF)

The CI is then the predicted value +/- cibandwidth. This is what
predict.lm provides with ci.fit if I am not wrong.

So for lme there is a very nice post on the S-news list:
http://www.biostat.wustl.edu/archives/html/s-news/2003-09/msg00021.html

Hopefully I quote the message correctly:
>var(y.hat) = X2 %*% inverse(t(X)%*%inverse(Sig)%*%X) %*% t(X2)
>
>The hard part for lme is deciding what goes in X and what is Sig:
>If you want a confidence interval on y for a particular subject
>included in X, then that subject is part of X; otherwise,
>it must be included in Sig.

My question is now where to obtain "Sig" from lme. Probably this is
obvious and I simply overlooked it.
Thus, I would be very grateful if anyone could help me in this matter.

Thanks Urs
______________________________________________________________________

Urs Wiedemann
Forschungsinstitut fuer Molekulare Pharmakologie (FMP)
Abteilung NMR-unterstuetzte Strukturforschung
Campus Berlin-Buch
Robert-Roessle-Str. 10
D-13125 Berlin

Tel.  +49 (30) 94 793-278
Fax   +49 (30) 94 793-169
email wiedemann at fmp-berlin.de
www   www.fmp-berlin.de



From vietnguyen at fastmail.fm  Tue Apr 27 08:44:30 2004
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Tue, 27 Apr 2004 16:44:30 +1000
Subject: [R] legend(1,1,expression(a<-"10^-6"))
Message-ID: <408E014E.6070206@fastmail.fm>

hi all,

I have a problem with showing properly formated mathematical expressions 
in a plot.

The following works:

plot(1:10)
legend(1,10, expression(10^-6))

Howver, if my expression is kept in a char variable, I don't know how to 
show it in the legend.

# I have
st_"10^-6"
# I try to do this
plot(1:10)
legend(1,10, expression(st))
# which doesn't turn out as expected


Is it possible to convert a string into expression? Or is there a 
different way of doing what I want to do?

Thanks in advance for your help.
viet



From ligges at statistik.uni-dortmund.de  Tue Apr 27 09:23:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Apr 2004 09:23:24 +0200
Subject: [R] standard library?
In-Reply-To: <x2smeqjudo.fsf@biostat.ku.dk>
References: <Pine.GSO.4.44.0404261359270.2776-100000@freke.odin.pdx.edu>
	<x2smeqjudo.fsf@biostat.ku.dk>
Message-ID: <408E0A6C.9000409@statistik.uni-dortmund.de>

Peter Dalgaard wrote:

> Randy Zelick <zelickr at pdx.edu> writes:
> 
> 
>>Hello all,
>>
>>I found a package I want to try listed at
>>
>>      http://www.maths.lth.se/help/R/.R/doc/html/packages.html
>>
>>I am using R 1.9.0 on winXP.
>>
>>The package is called R.audio
>>
>>When I tried the example, it did not work and produced the error message:
>>
>>      Error: Object "Sound" not found
>>
>>Normally I would assume that I have to download a package and I would go
>>looking for it... except I found R.audio in a documentation directory
>>called "Packages in the standard library"
>>
>>It *seems* like what I want ought to be available by default, since to me
>>the idea behind a "standard library" is that it is the stuff you get
>>*without* shopping around.
>>
>>Nevertheless, I looked on CRAN for a "standard library" and did not find a
>>reference to it. I also did not find a reference to R.audio there.
>>
>>Of course I am missing something obvious! What would that be?
> 
> 
> Every *installation* of R has a "standard library". If you
> install packages that's where they go by default. Some multi-user
> installations make their help pages available on the net. The
> installation at the Technical University at Lund contains something
> called R.audio. Since this appears not to be something from CRAN, it
> might be a good idea to follow the link in the Author field (to
> Henrik's R.classes bundle).
> 

Note that there is also a package "sound" available at CRAN.
Another package called "tuneR" dealing with wave files and containing 
some stuff to analyze musical sound will be presented at the useR! 
conference (a first version will hopefully appear on CRAN within a few 
months).

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Tue Apr 27 09:28:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Apr 2004 09:28:14 +0200
Subject: [R] legend(1,1,expression(a<-"10^-6"))
In-Reply-To: <408E014E.6070206@fastmail.fm>
References: <408E014E.6070206@fastmail.fm>
Message-ID: <408E0B8E.7080701@statistik.uni-dortmund.de>

Viet Nguyen wrote:

> hi all,
> 
> I have a problem with showing properly formated mathematical expressions 
> in a plot.
> 
> The following works:
> 
> plot(1:10)
> legend(1,10, expression(10^-6))
> 
> Howver, if my expression is kept in a char variable, I don't know how to 
> show it in the legend.
> 
> # I have
> st_"10^-6"

You have got a completely outdated version of R, if the code given above 
really works. Note that _ has long been depeceated and does no longer 
work for assignments.


> # I try to do this
> plot(1:10)
> legend(1,10, expression(st))
> # which doesn't turn out as expected
> 

?parse: "parse returns the parsed but unevaluated expressions in a list. 
Each element of the list is of mode expression."

  st <- "10^{-6}"
  plot(1:10)
  legend(1,10, parse(text = st))

Uwe Ligges



> Is it possible to convert a string into expression? Or is there a 
> different way of doing what I want to do?
> 
> Thanks in advance for your help.
> viet
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rksh at soc.soton.ac.uk  Tue Apr 27 09:48:59 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 27 Apr 2004 08:48:59 +0100
Subject: [R] comment()  a function
Message-ID: <a0600200cbcb3c037617f@[139.166.242.29]>


Hi everyone.

I want to comment() a function f(), and I know it's there
because comment(f) returns the comment as expected...but dput() does not
print the the comment.   dput() prints
comments for other types of objects. What's wrong with my understanding here?


f <- function(x){x^2}
comment(f) <- "works fine"
comment(f)
[1] "works fine"
dput(f)
function(x)
{
          x^2
}


why hasn't dput() printed the comment?
How can I save a function onto disk with a comment()?

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From schubert at prodigal.murdoch.edu.au  Tue Apr 27 10:55:18 2004
From: schubert at prodigal.murdoch.edu.au (daniel schubert)
Date: Tue, 27 Apr 2004 16:55:18 +0800
Subject: [R] lmRobMM vs rlm
Message-ID: <6.0.0.22.0.20040427123214.025cc6a0@prodigal>

I am needing some expertise with regard
to the S-Plus  command lmRobMM and its R counterpart 
rlm(formula,data,method="MM")

I have used lmRobMM(formula,data) in S-Plus on the Stackloss data and 
obtained for my residuals

6.217777 1.150717 6.427946 8.174019 -0.6713005 -1.248641 -0.4236203 
0.5763797 -1.057899 0.3593823

         11        12        13        14       15        16         17 
     18        19       20
  0.9629239 0.4732042 -2.506497 -1.346176 1.344408 0.1432279 -0.3729551 
0.09646618 0.5861859 1.934193

         21
  -8.629863


  (Intercept)        V1        V2          V3
    -37.65246 0.7976856 0.5773405 -0.06706018


but when I use rlm(formula,data,method="MM") in R my residuals are

           1            2            3            4            5            6
   2.82002948  -2.29288550   3.78605126   7.23168396  -1.60941126  -2.18886365
            7            8            9           10           11           12
  -1.09082616  -0.09082616  -1.43350203  -0.32664493   0.68958988   0.15612729
           13           14           15           16           17           18
  -3.10081497  -1.43820259   2.20031311   0.86156818  -0.29869392   0.49171093
           19           20           21
   1.02517352   1.61796106 -10.50956979

(Intercept)    VAR00001    VAR00002    VAR00003
-41.5230433   0.9388404   0.5794524  -0.1129150

I am very worried by this discrepancy what have I done wrong?

I read in R:Robust Fitting of Linear Models: MM estimation is M-Estimation 
with Tukey's biweight initialized by a specific S-estimator
and so I alter my S-Plus code to

ct<-lmRobMM.robust.control(weight=c("Optimal","Bisquare"))
fit<-lmRobMM(formula=V4~V1+V2+V3,data=data, robust.control=ct)

and obtain for my residuals

         1        2        3        4          5         6          7 
   8         9        10        11        12       13        14
  6.132346 1.061665 6.342018 8.212728 -0.7022119 -1.244742 -0.3631877 
0.6368123 -1.001597 0.2162886 0.8524152 0.3242646 -2.64235 -1.407392

        15        16         17         18        19       20        21
  1.338706 0.1266635 -0.4053969 0.08936822 0.6175189 1.894162 -8.820719

(Intercept)        V1        V2          V3
    -37.35298 0.8107863 0.5425301 -0.07068073


then I try

ct<-lmRobMM.robust.control(weight=c("Bisquare","Bisquare"))
fit<-lmRobMM(formula=V4~V1+V2+V3,data=data, robust.control=ct)

and obtain

   1        2        3       4          5         6          7         8 
      9        10        11        12        13        14
  5.954287 0.880977 6.206231 8.16344 -0.7473658 -1.291963 -0.3966992 
0.6033008 -1.020411 0.1894031 0.8491936 0.3204805 -2.663977 -1.402163

        15        16         17        18        19       20        21
  1.392297 0.1723669 -0.3985709 0.1145995 0.6433127 1.882605 -8.908035

(Intercept)        V1        V2          V3
    -37.56484 0.8178879 0.5445969 -0.07331006


no matter what I try I cannot reproduce my R-fit?


thank you for the time you spend with this query
cheers
Daniel



From tpapp at axelero.hu  Tue Apr 27 11:00:16 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Tue, 27 Apr 2004 11:00:16 +0200
Subject: [R] parsing a data file
Message-ID: <20040427090016.GA728@localhost>

Hi,

I need to parse a data file (output of a measuring device) of the
following format:

BEGIN RECORD [first record data] RECORD [second
record data] RECORD
[third record data]
END

Line breaks can (and do ;-() occur anywhere.  White space behaves very
much like TeX, eg it is not important whether there are one or more
spaces or linebreaks as long as there is one of them.  It is a text
file, not binary.

I need to extract the record data I marked with []'s, eg a vector such
as c("[first record data]", "[second]", ...) would be nice as a
result.

What functions should I use for this?

Thanks,

Tamas


-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From Arne.Muller at aventis.com  Tue Apr 27 11:26:58 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Tue, 27 Apr 2004 11:26:58 +0200
Subject: [R] paste dimnames problem
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDD0CC@crbsmxsusr04.pharma.aventis.com>

Hello,

I've the following list n:
> n
[[1]]
[1] "NEW" "OLD" "PRG"

[[2]]
[1] "04h" "24h"

[[3]]
[1] "000mM" "010mM" "025mM" "050mM" "100mM"

where

n <- dimnames(some.multidim.array)

I'm trying to define a generic function that generates meaningful names from this list, e.g. NEW.04h.000mM would be the first name, then NEW.04h.010mM, ... . Overall this would generate 30 names (3*2*5). Here, this would be 3 cascaded loops, but the dimensions of the array should be flexible.

What would suggest?

	I'm happy for your comments,
	+regards,

	Arne



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Apr 27 11:50:59 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 27 Apr 2004 11:50:59 +0200
Subject: [R] paste dimnames problem
References: <C80ECAFA2ACC1B45BE45D133ED660ADECDD0CC@crbsmxsusr04.pharma.aventis.com>
Message-ID: <000701c42c3d$259167d0$ad133a86@www.domain>

Dear Arne,

you could use something like:

n <- list(c("NEW", "OLD", "PRG"), c("04h", "24h"), c("000mM", "010mM",
"025mM", "050mM", "100mM"))
dnames <- apply(expand.grid(n), 1, function(x) paste(x, collapse=""))
dnames

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <Arne.Muller at aventis.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 27, 2004 11:26 AM
Subject: [R] paste dimnames problem


> Hello,
>
> I've the following list n:
> > n
> [[1]]
> [1] "NEW" "OLD" "PRG"
>
> [[2]]
> [1] "04h" "24h"
>
> [[3]]
> [1] "000mM" "010mM" "025mM" "050mM" "100mM"
>
> where
>
> n <- dimnames(some.multidim.array)
>
> I'm trying to define a generic function that generates meaningful
names from this list, e.g. NEW.04h.000mM would be the first name, then
NEW.04h.010mM, ... . Overall this would generate 30 names (3*2*5).
Here, this would be 3 cascaded loops, but the dimensions of the array
should be flexible.
>
> What would suggest?
>
> I'm happy for your comments,
> +regards,
>
> Arne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From s-plus at wiwi.uni-bielefeld.de  Tue Apr 27 12:06:15 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 27 Apr 2004 12:06:15 +0200
Subject: [R] paste dimnames problem
References: <C80ECAFA2ACC1B45BE45D133ED660ADECDD0CC@crbsmxsusr04.pharma.aventis.com>
Message-ID: <408E3097.2050305@wiwi.uni-bielefeld.de>

Try:
<<*>>=
n<-list(c("new","old","prg"),
        c("04h","24h", "48h"),
        c("000mM","010mM","025mM", "050mM", "100mM"))
unlist(lapply(1:3,function(x) paste(n[[1]][x],n[[2]][x],n[[3]][x],sep=".")))

@
output-start
Tue Apr 27 12:03:57 2004
[1] "new.04h.000mM" "old.24h.010mM" "prg.48h.025mM"
output-end

Peter Wolf


Arne.Muller at aventis.com wrote:

>Hello,
>
>I've the following list n:
>  
>
>>n
>>    
>>
>[[1]]
>[1] "NEW" "OLD" "PRG"
>
>[[2]]
>[1] "04h" "24h"
>
>[[3]]
>[1] "000mM" "010mM" "025mM" "050mM" "100mM"
>
>where
>
>n <- dimnames(some.multidim.array)
>
>I'm trying to define a generic function that generates meaningful names from this list, e.g. NEW.04h.000mM would be the first name, then NEW.04h.010mM, ... . Overall this would generate 30 names (3*2*5). Here, this would be 3 cascaded loops, but the dimensions of the array should be flexible.
>
>What would suggest?
>
>	I'm happy for your comments,
>	+regards,
>
>	Arne
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From s-plus at wiwi.uni-bielefeld.de  Tue Apr 27 12:14:59 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 27 Apr 2004 12:14:59 +0200
Subject: [R] parsing a data file
References: <20040427090016.GA728@localhost>
Message-ID: <408E32A3.8070707@wiwi.uni-bielefeld.de>

Hello Tamas,
here is a starting point for your file parsing problem:

@
<<*>>=
x1<-scan("t",what="")
print(x1)
x2<-paste(x1,collapse=" ")
print(x2)
x3<-strsplit(x2, "RECORD")[[1]]
print(x3)

@
output-start
 [1] "BEGIN"   "RECORD"  "[first"  "record"  "data]"   "RECORD"  "[second"
 [8] "record"  "data]"   "RECORD"  "[third"  "record"  "data]"   "END"   

[1] "BEGIN RECORD [first record data] RECORD [second record data] RECORD 
[third record data] END"

[1] "BEGIN "                   " [first record data] "  
[3] " [second record data] "   " [third record data] END"
output-end

Peter Wolf

Tamas Papp wrote:

>Hi,
>
>I need to parse a data file (output of a measuring device) of the
>following format:
>
>BEGIN RECORD [first record data] RECORD [second
>record data] RECORD
>[third record data]
>END
>
>Line breaks can (and do ;-() occur anywhere.  White space behaves very
>much like TeX, eg it is not important whether there are one or more
>spaces or linebreaks as long as there is one of them.  It is a text
>file, not binary.
>
>I need to extract the record data I marked with []'s, eg a vector such
>as c("[first record data]", "[second]", ...) would be nice as a
>result.
>
>What functions should I use for this?
>
>Thanks,
>
>Tamas
>
>
>  
>



From lecoutre at stat.ucl.ac.be  Tue Apr 27 12:09:42 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 27 Apr 2004 12:09:42 +0200
Subject: [R] parsing a data file
In-Reply-To: <20040427090016.GA728@localhost>
References: <20040427090016.GA728@localhost>
Message-ID: <6.0.1.1.2.20040427120507.0231aec0@stat4ux.stat.ucl.ac.be>



Hi,

I dont think there is any built-in function to do that...
Your friend is readLines and some "manual" post-processing.
Here is what I did (not sure it is the best...)

tmptxt = readLines("g:/record.txt")
tmptxt = paste(tmptxt,collapse=" ") # All as a single string
tmptxt = strsplit(tmptxt,"RECORD")[[1]]
tmptxt = tmptxt[-c(1,length(tmptxt))]
num = as.numeric(tmptxt)

which you could transform into a function

readRecords = function(file){
        tmptxt=readLines(file)
         tmptxt = readLines(file)
         tmptxt = paste(tmptxt,collapse=" ") # All as a single string
         tmptxt = strsplit(tmptxt,"RECORD")[[1]]
         tmptxt = tmptxt[-c(1,length(tmptxt))]
         num = as.numeric(tmptxt)
         return(num)
}


Eric

At 11:00 27/04/2004, Tamas Papp wrote:
>Hi,
>
>I need to parse a data file (output of a measuring device) of the
>following format:
>
>BEGIN RECORD [first record data] RECORD [second
>record data] RECORD
>[third record data]
>END
>
>Line breaks can (and do ;-() occur anywhere.  White space behaves very
>much like TeX, eg it is not important whether there are one or more
>spaces or linebreaks as long as there is one of them.  It is a text
>file, not binary.
>
>I need to extract the record data I marked with []'s, eg a vector such
>as c("[first record data]", "[second]", ...) would be nice as a
>result.
>
>What functions should I use for this?
>
>Thanks,
>
>Tamas
>
>
>--
>Tam??s K. Papp
>E-mail: tpapp at axelero.hu

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From lecoutre at stat.ucl.ac.be  Tue Apr 27 12:15:10 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 27 Apr 2004 12:15:10 +0200
Subject: [R] parsing a data file
Message-ID: <6.0.1.1.2.20040427121127.02276ea0@stat4ux.stat.ucl.ac.be>


Oups,

I realize i do remove the last value with my code...
Better remove then from text directly:


readRecords = function(file){

	tmptxt = readLines(file)
	tmptxt = paste(tmptxt,collapse=" ") # All as a single string
	tmptxt=substr(tmptxt, 6,nchar(tmptxt)-3)# remove BEGIN and END
	tmptxt = strsplit(tmptxt,"RECORD")[[1]]
	
	num = as.numeric(tmptxt)
	return(num)
}

Eric
---
Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 27 12:10:55 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 27 Apr 2004 11:10:55 +0100 (BST)
Subject: [R] parsing a data file
In-Reply-To: <20040427090016.GA728@localhost>
Message-ID: <XFMail.040427111055.Ted.Harding@nessie.mcc.ac.uk>

On 27-Apr-04 Tamas Papp wrote:
> I need to parse a data file (output of a measuring device) of the
> following format:
> 
> BEGIN RECORD [first record data] RECORD [second
> record data] RECORD
> [third record data]
> END
> 
> Line breaks can (and do ;-() occur anywhere.  White space behaves very
> much like TeX, eg it is not important whether there are one or more
> spaces or linebreaks as long as there is one of them.  It is a text
> file, not binary.
> 
> I need to extract the record data I marked with []'s, eg a vector such
> as c("[first record data]", "[second]", ...) would be nice as a
> result.
> 
> What functions should I use for this?

I don't know whether there is any R function capable of handling
a format as anarchic as this one, but if you are willing to do the
job outside R (i.e. produce a derived data file which is cleanly
structured which can then be read by R) then it looks like an awk
job (some might say perl job). You can use sed to strip cruft.

For example:

cat temp
BEGIN RECORD [first record data] RECORD [second
record data] RECORD
[third record data]
END

cat temp | sed 's/BEGIN//' | sed 's/END//' | tr '\n' ' ' |
    awk 'BEGIN{RS="RECORD"}{print $0}'

 [first record data] 
 [second record data] 
 [third record data]

Does this help?
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 27-Apr-04                                       Time: 11:10:55
------------------------------ XFMail ------------------------------



From monica.palaseanu-lovejoy at stud.man.ac.uk  Tue Apr 27 12:31:36 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Tue, 27 Apr 2004 11:31:36 +0100
Subject: [R] Legend - revisited
Message-ID: <E1BIPsO-000OR4-Pd@serenity.mcc.ac.uk>

Hi,

I am trying to put a legend to a graph of quintiles. The points for 
quintiles are colored differently and the points themselves have 
different sizes. I was able to match everything but the points sizes. 
If i am using cex parameter it changes both the points and the text 
dimensions .... 

I am not sore if i was clear, but what i want is different sizes for the 
points but same size for the text in the legend. Here it is the 
command i am using:

legend(438000, 367000, c("1st quintile", "2nd quintile", "3rd 
quintile", "4th quintile", "5th quintile"), pch = c(21,21,21,21,21), 
pt.bg = c("blue", "green", "yellow", "brown", "red"), cex=1.2)

Any suggestion will be greatly appreciated.

Monica



From B.Rowlingson at lancaster.ac.uk  Tue Apr 27 12:35:59 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 27 Apr 2004 11:35:59 +0100
Subject: [R] parsing a data file
In-Reply-To: <20040427090016.GA728@localhost>
References: <20040427090016.GA728@localhost>
Message-ID: <408E378F.8020906@lancaster.ac.uk>

Tamas Papp wrote:

> I need to parse a data file (output of a measuring device) of the
> following format:
> 
> BEGIN RECORD [first record data] RECORD [second
> record data] RECORD
> [third record data]
> END

  Is it just the one 'BEGIN/END' pair per file? Or are there several? 
What's the format of the [first record data] entries? Numbers, strings? 
Are there literally square brackets in there?

> I need to extract the record data I marked with []'s, eg a vector such
> as c("[first record data]", "[second]", ...) would be nice as a
> result.
> 
> What functions should I use for this?

  I'd consider writing a Perl script that converted this into an XML 
file, then you could probably use the RXML package to read it, and it 
would be in a format readable by any XML-reading thing, or at least in a 
more easily-convertable form. But that might be a bit heavyweight, and 
the Ted Harding approach of sed, tr, and awk is always appealing, 
assuming you have a Unix box or a Unix box-of-tricks on Windows (cygwin).


Baz



From rvencio at ime.usp.br  Tue Apr 27 13:59:22 2004
From: rvencio at ime.usp.br (Ricardo Zorzetto Nicoliello Vencio)
Date: Tue, 27 Apr 2004 08:59:22 -0300 (BRT)
Subject: [R] constrOptim does ineq, not eq, but who do ?
Message-ID: <Pine.LNX.4.44.0404270818510.14927-100000@kevlar.ime.usp.br>

Hi everybody,

please, could you give me help ? I scanned the help archives and didn't
found hints...

I want to solve a large sparse linear system subjected to an inequality
constrains (all solutions positive) and an equality constrain (all
solutions sum to 1), thus I tried to fool constrOptim using:

 x[1] +   0  + ... +  0   >= 0
...
   0  +   0  + ... + x[n] >= 0
 x[1] + x[2] + ... + x[n] >= 1
-x[1] - x[2] - ... - x[n] >= -1

in minimization: min( dist(A*x - b) )

But, since the initial guess x0 must be in the fasiable region and NOT in
the boundaries (due to trick I only have boundary), this trick doens't
work out.

If constrOptim doesn't make subject to equalities, what function could do
this optimization ? any ideia ?

I look for something similar to MATLAB's lsqlin function:
http://web.media.mit.edu/~bill/workspace/matlab/toolbox/optim/lsqlin.html



From jfox at mcmaster.ca  Tue Apr 27 14:14:38 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 27 Apr 2004 08:14:38 -0400
Subject: [R] Adding regression surface to cloud plot 
In-Reply-To: <000801c42c23$34f2ee70$6c00a8c0@mtd4>
Message-ID: <20040427121439.QCVL7304.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Anne,

I should have mentioned that the Rcmdr package contains a function --
scatter3D -- to make these kinds of graphs. (I'm copying this response to
the r-help list -- I hope that's OK.)

John 

> -----Original Message-----
> From: Anne [mailto:anne.piotet at urbanet.ch] 
> Sent: Tuesday, April 27, 2004 1:45 AM
> To: John Fox
> Subject: Re: [R] Adding regression surface to cloud plot 
> 
> Thanks I 'll try
> Anne
> ----- Original Message -----
> From: "John Fox" <jfox at mcmaster.ca>
> To: "'Anne'" <anne.piotet at urbanet.ch>
> Cc: "'R list'" <r-help at stat.math.ethz.ch>
> Sent: Monday, April 26, 2004 3:57 PM
> Subject: RE: [R] Adding regression surface to cloud plot 
> 
> 
> > Dear Anne,
> > 
> > You can make such plots with the rgl package. You'll find 
> an example 
> > at 
> <http://socserv.socsci.mcmaster.ca/jfox/Courses/S-course/index.html>.
> > 
> > I hope this helps,
> >  John
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch 
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anne
> > > Sent: Monday, April 26, 2004 8:15 AM
> > > To: R list
> > > Subject: [R] Adding regression surface to cloud plot
> > > 
> > > Hello!
> > > I would like to add a plot of the regression surface to my cloud 
> > > plot . Is it possible?
> > > Thanks
> > > 
> > > Anne
> > 
>



From spyridoula.tsonaka at med.kuleuven.ac.be  Tue Apr 27 14:23:12 2004
From: spyridoula.tsonaka at med.kuleuven.ac.be (Spyridoula Tsonaka)
Date: Tue, 27 Apr 2004 14:23:12 +0200
Subject: [R] constrOptim does ineq, not eq, but who do ?
References: <Pine.LNX.4.44.0404270818510.14927-100000@kevlar.ime.usp.br>
Message-ID: <000c01c42c52$685be5d0$ae133a86@www.domain>

Dear Ricardo,

Optimization of the surrogate function that is used in the constrOptim
under equality constraints can be implemented by applying the
lagrangian rule to the surrogate function (instead of using 'optim').

The iterative formula can be found in Lange, K. (1999). Numerical
analysis for statisticians. Springer-Verlag, New York. pp: 184.

I hope this helps.

Roula


----- Original Message ----- 
From: "Ricardo Zorzetto Nicoliello Vencio" <rvencio at ime.usp.br>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, April 27, 2004 1:59 PM
Subject: [R] constrOptim does ineq, not eq, but who do ?


> Hi everybody,
>
> please, could you give me help ? I scanned the help archives and
didn't
> found hints...
>
> I want to solve a large sparse linear system subjected to an
inequality
> constrains (all solutions positive) and an equality constrain (all
> solutions sum to 1), thus I tried to fool constrOptim using:
>
>  x[1] +   0  + ... +  0   >= 0
> ...
>    0  +   0  + ... + x[n] >= 0
>  x[1] + x[2] + ... + x[n] >= 1
> -x[1] - x[2] - ... - x[n] >= -1
>
> in minimization: min( dist(A*x - b) )
>
> But, since the initial guess x0 must be in the fasiable region and
NOT in
> the boundaries (due to trick I only have boundary), this trick
doens't
> work out.
>
> If constrOptim doesn't make subject to equalities, what function
could do
> this optimization ? any ideia ?
>
> I look for something similar to MATLAB's lsqlin function:
>
http://web.media.mit.edu/~bill/workspace/matlab/toolbox/optim/lsqlin.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Tue Apr 27 15:32:13 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 27 Apr 2004 08:32:13 -0500
Subject: [R] lme - Confidence interval for predicted values
In-Reply-To: <006301c42c23$016ffb10$0b815fc2@fmpberlin.de>
References: <006301c42c23$016ffb10$0b815fc2@fmpberlin.de>
Message-ID: <6rd65teeci.fsf@bates4.stat.wisc.edu>

"Urs Wiedemann" <wiedemann at fmp-berlin.de> writes:

> After having fit (using lme) a mixed effects model with a single
> random effect, I like to estimate the confidence interval for the
> predicted mean expectations.
> To my knowledge this is usually done (for fixed effects models) by
> calculating:
> 
> cibandwidth <- sqrt(diag(Xnew %*% solve(t(X) %*% X) t(Xnew))) *
> qt((1-level)/2, DF)
> 
> The CI is then the predicted value +/- cibandwidth. This is what
> predict.lm provides with ci.fit if I am not wrong.
> 
> So for lme there is a very nice post on the S-news list:
> http://www.biostat.wustl.edu/archives/html/s-news/2003-09/msg00021.html
> 
> Hopefully I quote the message correctly:
> >var(y.hat) = X2 %*% inverse(t(X)%*%inverse(Sig)%*%X) %*% t(X2)
> >
> >The hard part for lme is deciding what goes in X and what is Sig:
> >If you want a confidence interval on y for a particular subject
> >included in X, then that subject is part of X; otherwise,
> >it must be included in Sig.

I think the answer to this question will soon become easier.  Later
today (if I can escape administrative duties, otherwise tomorrow) I
will make available an alpha-level release candidate of a completely
rewritten lme4 package.  And I do mean completely rewritten.  This is
the fourth and, I swear, the last time I have designed and implemented
from scratch methods for parameter estimation in linear mixed models.

The new version of lme (that is, from the not-yet-released lme4_0.6-1
or later) allows the usual optional arguments for model-fitting
functions in R, including "x=TRUE" which will cause the slot x in the
fitted model object to contain a list of model matrices.  The last
element of this list is the X that you want.

> My question is now where to obtain "Sig" from lme. Probably this is
> obvious and I simply overlooked it.

The new version of the lme4 package has a method for the "vcov"
generic.  This method returns this Sig.



From FWS4 at CDRH.FDA.GOV  Tue Apr 27 16:10:22 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Tue, 27 Apr 2004 10:10:22 -0400
Subject: [R] Mixed Effects Models in S and S-Plus book
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB7DA@drm556>

Anybody know where I can get the Pinheiro/Bates book?
I can't find a bookstore w/ stock and the publisher
says they don't know when they'll have it again.

Thanks.

-Frank



From j.nocera at unb.ca  Tue Apr 27 16:20:16 2004
From: j.nocera at unb.ca (Joe Nocera)
Date: Tue, 27 Apr 2004 11:20:16 -0300
Subject: [R] Mixed Effects Models in S and S-Plus book
In-Reply-To: <644D9337A02FC24689647BF9E48EC39E08ABB7DA@drm556>
Message-ID: <NGBBIMKLALDJHADGCBBBAEOKDAAA.j.nocera@unb.ca>

Frank,

Try www.abebooks.com

I found three booksellers there that have the book.

Cheers,
Joe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Samuelson, Frank*
Sent: April 27, 2004 11:10 AM
To: 'r-help at stat.math.ethz.ch '
Subject: [R] Mixed Effects Models in S and S-Plus book


Anybody know where I can get the Pinheiro/Bates book?
I can't find a bookstore w/ stock and the publisher
says they don't know when they'll have it again.

Thanks.

-Frank

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bob.ohara at helsinki.fi  Tue Apr 27 16:24:42 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Tue, 27 Apr 2004 17:24:42 +0300
Subject: [R] Mixed Effects Models in S and S-Plus book
References: <644D9337A02FC24689647BF9E48EC39E08ABB7DA@drm556>
Message-ID: <408E6D2A.1010003@helsinki.fi>

Samuelson, Frank* wrote:
> Anybody know where I can get the Pinheiro/Bates book?
> I can't find a bookstore w/ stock and the publisher
> says they don't know when they'll have it again.
> 
Just by chance, it's on offer at the moment at NHBS:
<http://www.nhbs.com/xbscripts/bkfsrch?search=107853>
I found this out just after I had bought mine at full price.  :-(

Of course, you should check postage costs too!

Bob

-- 
Bob O'Hara

Dept. of Mathematics and Statistics
P.O. Box 4 (Yliopistonkatu 5)
FIN-00014 University of Helsinki
Finland
Telephone: +358-9-191 23743
Mobile: +358 50 599 0540
Fax:  +358-9-191 22 779
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: http://www.jnr-eeb.org



From chrysopa at insecta.ufv.br  Tue Apr 27 16:34:57 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 27 Apr 2004 11:34:57 -0300
Subject: [R] minimal requirement
Message-ID: <200404271134.57230.chrysopa@insecta.ufv.br>

Hi,

what is the minimal hardware requirement for run R on windows 9x? and for run 
it in a linux with X and a light windowmanager? Not for hard use, only for 
learning.

Thanks
Ronaldo
-- 
Genius may have its limitations, but stupidity is not thus handicapped.
		-- Elbert Hubbard
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From deepayan at stat.wisc.edu  Tue Apr 27 17:00:01 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 27 Apr 2004 10:00:01 -0500
Subject: [R] Adding regression surface to cloud plot
In-Reply-To: <000c01c42b90$6f80bf10$6c00a8c0@mtd4>
References: <000c01c42b90$6f80bf10$6c00a8c0@mtd4>
Message-ID: <200404271000.02135.deepayan@stat.wisc.edu>

On Monday 26 April 2004 08:14, Anne wrote:
> Hello!
> I would like to add a plot of the regression surface to my cloud plot
> . Is it possible? Thanks

Possible (since 1.9.0), but not easy. Technically, the biggest problem 
is deciding which points are 'behind' the surface and which 'in 
front' (that would decide the order of plotting). Otherwise, all the 
pieces are in place.

The following should work for regression surfaces, but may need an 
inequality to be changed for specific viewpoints:


panel.3dreg <-
    function(x, y, z, rot.mat, distance,
             xlim.scaled, ylim.scaled, 
             nmesh = 21, ...)
{
    fm <- lm(z ~ x + y)
    id <- predict(fm) < z ## may need to be reversed
    panel.3dscatter(x[id], y[id], z[id], rot.mat, distance,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    ...)
    x.locs <- seq(xlim.scaled[1], xlim.scaled[2], length = nmesh)
    y.locs <- seq(ylim.scaled[1], ylim.scaled[2], length = nmesh)
    grid <-
        expand.grid(y = y.locs, x = x.locs)
    grid$z <- predict(fm, newdata = grid)
    panel.3dwire(x = x.locs, y = y.locs, z = grid$z,
                 rot.mat, distance,
                 xlim.scaled = xlim.scaled,
                 ylim.scaled = ylim.scaled,
                 ..., col.at = .5, col.regions = "transparent")
    panel.3dscatter(x[!id], y[!id], z[!id], rot.mat, distance,
                    xlim.scaled = xlim.scaled,
                    ylim.scaled = ylim.scaled,
                    ...)
}

## example
data(rock)
cloud(area ~ peri * shape, rock,
      panel.3d.cloud = panel.3dreg,
      nmesh = 40, pch = 16)


- Deepayan



From baron at psych.upenn.edu  Tue Apr 27 17:03:18 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 27 Apr 2004 11:03:18 -0400
Subject: [R] installing R on Fedora Core 2 test 2
Message-ID: <20040427150318.GA19763@psych>

Just a report for those who want to try this.  (Executive
summary: It eventually worked.)

The RPM for Fedora Core 1 did not work because it wanted
libtcl8.3.so and libtk8.3.so, while this distribution had 8.4
instead of 8.3.  I didn't try making soft links, thus pretending
that it had 8.3.  (I'm planning to install the final version of
Core 2 when it comes out soon.  This is a computer I set up
yesterday for grad students to learn about Linux and R.)

I installed from source and that went pretty smoothly, once I
realized my mistake in NOT doing what Martyn Plummer did with
Core 1, which was to install the "workstation" version of Fedora
rather than the "personal" version (or whatever it is called).
As a result, I had to install gcc and lots of other stuff.
Fortunately, Fedora comes with yum, which makes such
installations incredibly easy.  You just say, for example, 

yum install gcc

and its done pretty fast (assuming you have a fast connection).

The one thing I cannot figure out is that readline does not
work.  It was installed, but apparently not detected.  Grepping
config.site for readline gets stuff like this:

configure:21256: checking for rl_callback_read_char in -lreadline
configure:21286: gcc -o conftest -g -O2 -I/usr/local/include
-L/usr/local/lib conftest.c -lreadline  -ldl -lncurses -lm  >&5
/usr/bin/ld: cannot find -lreadline
configure:22002: checking readline/history.h usability
conftest.c:83:30: readline/history.h: No such file or directory
| #include <readline/history.h>
configure:22046: checking readline/history.h presence
conftest.c:49:30: readline/history.h: No such file or directory
[more stuff like this]
ac_cv_header_readline_history_h=no
ac_cv_header_readline_readline_h=no
ac_cv_lib_readline_rl_callback_read_char=no

This is not a huge problem in practice, since ESS under Xemacs
does work fine, and ESS has the main functions of readline.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From JIM at mshri.on.ca  Tue Apr 27 16:56:00 2004
From: JIM at mshri.on.ca (Jim Wei)
Date: Tue, 27 Apr 2004 10:56:00 -0400
Subject: [R] Error on installation of R-1.8 and R-1.9
Message-ID: <490D0AFAF3D2D3119F6C00508B6FDF150264423D@ex.mshri.on.ca>

Hi; 

We have R-1.7 installed on our Sun box with Solaris 8, and try to install
R-1.8.1 or R-1.9,  after I run ./configure,  both installation give me the
error as following 
---------------------------------------------------------------------------
checking for int... yes
checking size of int... configure: error: cannot compute sizeof (int), 77
See `config.log' for more details.

----------------------------------------------------------------------------

Any suggestions  will be greatly appreciated. 

Thanks


Jim Wei



From rpeng at jhsph.edu  Tue Apr 27 17:07:25 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 27 Apr 2004 11:07:25 -0400
Subject: [R] installing R on Fedora Core 2 test 2
In-Reply-To: <20040427150318.GA19763@psych>
References: <20040427150318.GA19763@psych>
Message-ID: <408E772D.3090301@jhsph.edu>

For readline you are probably missing the development version of 
readline which usually contains the header files.  I think it's still 
called readline-devel but it's been a while for me.

-roger

Jonathan Baron wrote:
> Just a report for those who want to try this.  (Executive
> summary: It eventually worked.)
> 
> The RPM for Fedora Core 1 did not work because it wanted
> libtcl8.3.so and libtk8.3.so, while this distribution had 8.4
> instead of 8.3.  I didn't try making soft links, thus pretending
> that it had 8.3.  (I'm planning to install the final version of
> Core 2 when it comes out soon.  This is a computer I set up
> yesterday for grad students to learn about Linux and R.)
> 
> I installed from source and that went pretty smoothly, once I
> realized my mistake in NOT doing what Martyn Plummer did with
> Core 1, which was to install the "workstation" version of Fedora
> rather than the "personal" version (or whatever it is called).
> As a result, I had to install gcc and lots of other stuff.
> Fortunately, Fedora comes with yum, which makes such
> installations incredibly easy.  You just say, for example, 
> 
> yum install gcc
> 
> and its done pretty fast (assuming you have a fast connection).
> 
> The one thing I cannot figure out is that readline does not
> work.  It was installed, but apparently not detected.  Grepping
> config.site for readline gets stuff like this:
> 
> configure:21256: checking for rl_callback_read_char in -lreadline
> configure:21286: gcc -o conftest -g -O2 -I/usr/local/include
> -L/usr/local/lib conftest.c -lreadline  -ldl -lncurses -lm  >&5
> /usr/bin/ld: cannot find -lreadline
> configure:22002: checking readline/history.h usability
> conftest.c:83:30: readline/history.h: No such file or directory
> | #include <readline/history.h>
> configure:22046: checking readline/history.h presence
> conftest.c:49:30: readline/history.h: No such file or directory
> [more stuff like this]
> ac_cv_header_readline_history_h=no
> ac_cv_header_readline_readline_h=no
> ac_cv_lib_readline_rl_callback_read_char=no
> 
> This is not a huge problem in practice, since ESS under Xemacs
> does work fine, and ESS has the main functions of readline.
> 
> Jon



From MSchwartz at MedAnalytics.com  Tue Apr 27 17:12:04 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 27 Apr 2004 10:12:04 -0500
Subject: [R] Mixed Effects Models in S and S-Plus book
In-Reply-To: <644D9337A02FC24689647BF9E48EC39E08ABB7DA@drm556>
References: <644D9337A02FC24689647BF9E48EC39E08ABB7DA@drm556>
Message-ID: <1083078723.8397.68.camel@localhost>

On Tue, 2004-04-27 at 09:10, Samuelson, Frank* wrote:
> Anybody know where I can get the Pinheiro/Bates book?
> I can't find a bookstore w/ stock and the publisher
> says they don't know when they'll have it again.
> 
> Thanks.
> 
> -Frank


You might want to look here:

http://www.bookfinder.us/booksearch/ISBN?query=0387989579&src=bates+pinheiro

There is an indication of the book being in stock at one place
(ECampus.com). You might want to call the indicated suppliers before
purchasing to confirm the status.

An alternative would be to check a local library (ie. University).
Presumably a colleague at the FDA could lend you a copy or is there an
internal library?

Is this situation indicative of a new printing/edition being
forthcoming?

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Tue Apr 27 17:21:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2004 17:21:48 +0200
Subject: [R] installing R on Fedora Core 2 test 2
In-Reply-To: <20040427150318.GA19763@psych>
References: <20040427150318.GA19763@psych>
Message-ID: <x21xm978fn.fsf@biostat.ku.dk>

Jonathan Baron <baron at psych.upenn.edu> writes:

> The one thing I cannot figure out is that readline does not
> work.  It was installed, but apparently not detected.  Grepping
> config.site for readline gets stuff like this:
> 
> configure:21256: checking for rl_callback_read_char in -lreadline
> configure:21286: gcc -o conftest -g -O2 -I/usr/local/include
> -L/usr/local/lib conftest.c -lreadline  -ldl -lncurses -lm  >&5
> /usr/bin/ld: cannot find -lreadline

Hm? The usual problem is that people forget to install readline-devel,
but is that the expected error message then? Could you do an rpm -ql
readline and see where the libreadline stuff went to? If you did
forget readline-devel, try that first of course.

BTW, it might be a good idea to see if you can rpm --rebuild from
Martyn's src.rpm. That gives you your very own .rpm, which you can
install and eventually upgrade (which is the point) using the standard
tools.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From luisr at frs.fo  Tue Apr 27 17:38:17 2004
From: luisr at frs.fo (Luis Rideau Cruz)
Date: Tue, 27 Apr 2004 16:38:17 +0100
Subject: [R] adding a second axis to a plot
Message-ID: <s08e8c8b.081@ffdata.setur.fo>

How to plot something and then
add a new axis for a new vector to be plotted?

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From Max_Kuhn at bd.com  Tue Apr 27 17:42:39 2004
From: Max_Kuhn at bd.com (Max_Kuhn@bd.com)
Date: Tue, 27 Apr 2004 11:42:39 -0400
Subject: [R] SWeave, R and backslash
Message-ID: <OF94B18CB0.77950D51-ON85256E83.00556FD8@bd.com>

All,

I've been using SWeave to generate output based on reports (R 1.8.1). We
developed a front end for the non-statistical users and, in this interface,
they can select one or more analyses.

There are multiple rnw files that make up the final documents for
typesetting. There is the overall rnw file that may call a number of tex
files (also produced via SWeave) using \include. Let's say that an anova
analysis is requested, then SWeave runs on anaova.rnw and an string is set
that will be used by SWeave so that it includes the file.

My first shot at this string was DoAnova <- "\include{anova}" of they want
that analysis and DoAnova <- "" if not. Of course, this didn't work since
SWeave gets the text include{anova}. So, I've tried DoAnova <-
"\\include{anova}" and DoAnova <- "\!\include{anova}" to no avail.

Does anyone have any ideas about getting the backslash?

Thanks,

Max


**********************************************************************
This message is intended only for the designated recipient(s...{{dropped}}



From MBock at arcadis-us.com  Tue Apr 27 17:43:52 2004
From: MBock at arcadis-us.com (Bock, Michael)
Date: Tue, 27 Apr 2004 09:43:52 -0600
Subject: [R] Problems raised to 1/3 power and NaN
Message-ID: <0016F5677B1F1D4281EEBC034993595155B875@CORPEXBE1.arcadis-us.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040427/76d6d68b/attachment.pl

From s.johri at imperial.ac.uk  Tue Apr 27 17:54:42 2004
From: s.johri at imperial.ac.uk (Johri, Saurabh)
Date: Tue, 27 Apr 2004 16:54:42 +0100
Subject: [R] beginners  k means clustering question
Message-ID: <281BA46F2CF1054691D5E0F27B981C482DF863@icex33.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040427/b4592d3b/attachment.pl

From maechler at stat.math.ethz.ch  Tue Apr 27 18:10:32 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 27 Apr 2004 18:10:32 +0200
Subject: [R] Problems raised to 1/3 power and NaN
In-Reply-To: <0016F5677B1F1D4281EEBC034993595155B875@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC034993595155B875@CORPEXBE1.arcadis-us.com>
Message-ID: <16526.34296.287751.267137@gargle.gargle.HOWL>

Well,

your example can be simplified to

> -1^(1/3)
[1] -1

> (-1)^(1/3)
[1] NaN

Do you see the light?





[think "precedence rules"]

Regards,
Martin



From Bruno.Giordano at ircam.fr  Tue Apr 27 18:11:33 2004
From: Bruno.Giordano at ircam.fr (Bruno Giordano)
Date: Tue, 27 Apr 2004 18:11:33 +0200
Subject: [R] bcanon bug???
References: <002701c3df7c$a6f33000$745188c1@mat.ua.pt>
	<007d01c3df81$969888e0$2f643744@WATSON>
Message-ID: <007301c42c72$5111ee40$52416681@brungio>

Hallo,
I'm using bcanon to get bootstrap confidence intervals.
I need to modify the default confidence levels
(alpha=c(0.025, 0.05, 0.1, 0.16, 0.84, 0.9, 0.95, 0.975)),

adding two more extreme values:

alpha=c((0.05/x),0.025, 0.05, 0.1, 0.16, 0.84, 0.9, 0.95, 0.975,1-(0.05/x)))

trying to adapt for x/2 comparisons with the bootstraped statistic.

The function seems to work ok, but in some cases the bootstrap estimates for
the
highest confidence level are obviously wrong:


0.05/x    -1.33318711781158
0.025    -1.09386961626344
0.05    -0.868768454819694
0.1    -0.652649120212949
0.16    0.0453058935697044
0.84    0.104645679223495
0.9    0.213385388775905
0.95    0.368838243723381
0.975    0.615868703042345
1-0.05/x    -1.33318711781158

where the estimate for the highest confidence level is simply the copy of
that for the lowest.

Did someone experience a similar problem?
I guess I should do some debugging on the bcanon code.

Thanks
    Bruno

~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Bruno L. Giordano - Ph. D. student
Dipartimento di Psicologia Generale
Via Venezia 8 - 35131 Padova, Italy

currently hosted by:

Equipe Perception et Cognition Musicales
Ircam-CNRS (UMR 9912)
1 place Igor-Stravinsky
F-75004 Paris, France



From petr.pikal at precheza.cz  Tue Apr 27 18:13:16 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Apr 2004 18:13:16 +0200
Subject: [R] Legend - revisited
In-Reply-To: <E1BIPsO-000OR4-Pd@serenity.mcc.ac.uk>
Message-ID: <408EA2BC.27434.23EB53E@localhost>

Hallo Monica

I tried to do the same task recently and the only solution I was 
able to come with was to hack legend() function. You can find it 
in graphics library (R1.9.0).

I just added

cex.pt = cex,

in the definition

and 

cex=cex.pt, 

instead cex=cex,

into draw points part of the function.

Than I can when calling legend to leave default cex or to use 
different setting for legend text ("cex") and points ("cex.pt" ).

BTW cex is vectorised so cex=c(1,1,2,3) gives you different cex 
setting for 4 different legend items.

I copied the modified function into my library and now I wait if 
some other function starts to complain about my modifications :-).

HTH.
Cheers
Petr





On 27 Apr 2004 at 11:31, Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> I am trying to put a legend to a graph of quintiles. The points for
> quintiles are colored differently and the points themselves have
> different sizes. I was able to match everything but the points sizes.
> If i am using cex parameter it changes both the points and the text
> dimensions .... 
> 
> I am not sore if i was clear, but what i want is different sizes for
> the points but same size for the text in the legend. Here it is the
> command i am using:
> 
> legend(438000, 367000, c("1st quintile", "2nd quintile", "3rd 
> quintile",  4th quintile ,  5th quintile"), pch = c(21,21,21,21,21),
> pt.bg = c( blue ,  green ,  yellow ,  brown ,  red ), cex=1.2)
> 
> Any suggestion will be greatly appreciated.
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From prechelt at pcpool.mi.fu-berlin.de  Tue Apr 27 18:20:41 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Tue, 27 Apr 2004 18:20:41 +0200
Subject: [R] minimal requirement
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920458EB@circle.pcpool.mi.fu-berlin.de>

Ronaldo,

> what is the minimal hardware requirement for run R on windows 9x?

My R 1.8.0 installation (on Windows XP) requires 42 MB of disk space.
The R process, after starting, requires about 20 MB of virtual memory.

So I guess if you have about 60 MB free disk space
and about 40-60 MB available virtual memory
and enough patience,
you are ready to go for learning purposes.

What will probably happen is this:
a.) you will quickly like R
b.) you will find you want more RAM

  Lutz



From p.dalgaard at biostat.ku.dk  Tue Apr 27 18:17:00 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2004 18:17:00 +0200
Subject: [R] Problems raised to 1/3 power and NaN
In-Reply-To: <0016F5677B1F1D4281EEBC034993595155B875@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC034993595155B875@CORPEXBE1.arcadis-us.com>
Message-ID: <x2smep5rb7.fsf@biostat.ku.dk>

"Bock, Michael" <MBock at arcadis-us.com> writes:

> > Wq
> [1] -10.72508
> > Wq <- Wq^(1/3)
> > Wq
> [1] NaN
> > z <- -10.72508^(1/3)
> > z
> [1] -2.205296
> 
> 
> as you can see if Wq = -10.72508,  Wq^(1/3) is NaN but
> z<- -10.72508^(1/3) returns a number.

Hint:
> -2^(1/2)
[1] -1.414214
> sqrt(-2)
[1] NaN
Warning message:
NaNs produced in: sqrt(-2)
> sqrt(-2+0i)
[1] 0+1.414214i

Powers of negative numbers are not defined for non-integral exponents.
You need something like

cuberoot <- function(x)sign(x)*abs(x)^(1/3)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From petr.pikal at precheza.cz  Tue Apr 27 18:21:25 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 27 Apr 2004 18:21:25 +0200
Subject: [R] adding a second axis to a plot
In-Reply-To: <s08e8c8b.081@ffdata.setur.fo>
Message-ID: <408EA4A5.16161.24629B1@localhost>

Hi

You have quite many options

plot()
lines()
points()

with appropriate scaling

or
par(new=TRUE)
before calling second plot

or

matplot()

see documentation to functions

Depends on what you want to plot.

I use following function - plot.yy(x,y1,y2)

plot.yy<-function(x,yright,yleft, yleftlim=NULL, yrightlim = 
NULL, xlab = NULL ,yylab=c("",""),pch=c(1,2),col=c(1,2), 
linky=F, smooth=0, lwds=1, length=10, format="%d-%H:%M", 
...)

{

par(mar=c(5,4,4,2),oma=c(0,0,0,3))
plot(x, yright, ylim=yrightlim, axes=F,ylab="", xlab=xlab, 
pch=pch[1],col=col[1], ...)
axis(4,pretty(range(yright,na.rm=T),10),col=col[1])

if (linky) lines(x,yright,col=col[1], ...)

if (smooth!=0) lines(supsmu(x,yright,span=smooth),col=col[1], 
lwd=lwds, ...)

if(yylab[1]=="") 
mtext(deparse(substitute(yright)),side=4,outer=T,line=1, 
col=col[1], ...)
else 
mtext(yylab[1],side=4,outer=T,line=1, col=col[1], ...)

par(new=T)
plot(x,yleft, ylim=yleftlim, ylab="", axes=F ,xlab=xlab, 
pch=pch[2],col=col[2], ...)
box()
axis(2,pretty(range(yleft,na.rm=T),10),col=col[2], col.axis=col[2])

#if (is.null(class(x))) axis(1,pretty(range(x,na.rm=T),10)) else 
axis.POSIXct(1, x)

if (is.null(class(x))) axis(1,pretty(range(x,na.rm=T),10)) else 
{
l<-length(x)
axis(1,at=x[seq(1,l,length=length)],labels=format(as.POSIXct(x[s
eq(1,l,length=length)]),format=format))
}


#if (xDatum) 
axis(1,dates(pretty(range(datum,na.rm=T),10)),labels=as.characte
r(chron(pretty(range(datum,na.rm=T),10)),format=c("d/m/y")))
#else axis(1,pretty(range(x,na.rm=T),10))

if(yylab[2]=="")
mtext(deparse(substitute(yleft)),side=2,line=2, col=col[2], ...)
else
mtext(yylab[2],side=2,line=2, col=col[2], ...)


if (linky) lines(x,yleft,col=col[2], lty=2, ...)
if (smooth!=0) lines(supsmu(x,yleft,span=smooth),col=col[2], 
lty=2, lwd=lwds, ...)

}


Cheers
Petr



On 27 Apr 2004 at 16:38, Luis Rideau Cruz wrote:

> How to plot something and then
> add a new axis for a new vector to be plotted?
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From MBock at arcadis-us.com  Tue Apr 27 18:27:18 2004
From: MBock at arcadis-us.com (Bock, Michael)
Date: Tue, 27 Apr 2004 10:27:18 -0600
Subject: [R] Problems raised to 1/3 power and NaN
Message-ID: <0016F5677B1F1D4281EEBC034993595155B876@CORPEXBE1.arcadis-us.com>

I see the light! I have gotten response re precedence :
i.e (-2)^2 = 4 and -2^2 = -4 (^ evaluated before -) 

I understood precedence and still could not see the problem but as Peter states:

Powers of negative numbers are not defined for non-integral exponents.
You need something like

cuberoot <- function(x)sign(x)*abs(x)^(1/3)

I though you could take the cubed root of negative numbers (basic algebra) but perhaps this does not hold true for computer math due to rounding of the exponents. 

I am on my way! . Thanks 10^6
Mike



-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
Sent: Tuesday, April 27, 2004 12:17 PM
To: Bock, Michael
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Problems raised to 1/3 power and NaN


"Bock, Michael" <MBock at arcadis-us.com> writes:

> > Wq
> [1] -10.72508
> > Wq <- Wq^(1/3)
> > Wq
> [1] NaN
> > z <- -10.72508^(1/3)
> > z
> [1] -2.205296
> 
> 
> as you can see if Wq = -10.72508,  Wq^(1/3) is NaN but
> z<- -10.72508^(1/3) returns a number.

Hint:
> -2^(1/2)
[1] -1.414214
> sqrt(-2)
[1] NaN
Warning message:
NaNs produced in: sqrt(-2)
> sqrt(-2+0i)
[1] 0+1.414214i

Powers of negative numbers are not defined for non-integral exponents.
You need something like

cuberoot <- function(x)sign(x)*abs(x)^(1/3)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From prechelt at pcpool.mi.fu-berlin.de  Tue Apr 27 18:30:12 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Tue, 27 Apr 2004 18:30:12 +0200
Subject: [R] bcanon bug???
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920458EC@circle.pcpool.mi.fu-berlin.de>


> in some cases the bootstrap estimates for the
> highest confidence level are obviously wrong:
> 0.05/x    -1.33318711781158
> 0.025    -1.09386961626344
> 0.05    -0.868768454819694
> 0.1    -0.652649120212949
> 0.16    0.0453058935697044
> 0.84    0.104645679223495
> 0.9    0.213385388775905
> 0.95    0.368838243723381
> 0.975    0.615868703042345
> 1-0.05/x    -1.33318711781158
> where the estimate for the highest confidence level is simply 
> the copy of that for the lowest.

Which is not quite so obviously wrong:
it is perfectly right if x = 0.1

You did not tell us the value of x.

This would mean an entry

0.5   -1.333

which does not quite fit into the rest of your series,
but maybe what you are bootstrapping is not quite as 
continuous as it seems?

  Lutz



From Bruno.Giordano at ircam.fr  Tue Apr 27 18:42:06 2004
From: Bruno.Giordano at ircam.fr (Bruno Giordano)
Date: Tue, 27 Apr 2004 18:42:06 +0200
Subject: [R] bcanon bug???
References: <85D25331FFB7AE4C900EA467D4ADA3920458EC@circle.pcpool.mi.fu-berlin.de>
Message-ID: <008e01c42c76$945afad0$52416681@brungio>



> You did not tell us the value of x.
:-)

x is an even integer (6,8,10). With x=6, you get

alpha=0.008333333,[default values],0.9916667

This is the reason for why the result I get sometimes is so odd.

>but maybe what you are bootstrapping is not quite as
>continuous as it seems?

Sorry for not being specific. I guess the statistic I'm testing is quite
continuous: I'm actually bootstraping the difference in the loglikelihood of
two nonnested model, where the dependent is always the same one, but the
independent is transformed in different ways (raw "linear" value, log value,
or power transformed value). As I'm comparing more than two transforms it
comes the need to adjust the confidence levels (I might also include other
transforms with a perceptually relevant meaning).

    Bruno



From jonathan.williams at pharmacology.oxford.ac.uk  Tue Apr 27 19:06:03 2004
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Tue, 27 Apr 2004 18:06:03 +0100
Subject: [R] Extracting labels for residuals from lme
Message-ID: <NGBBKJEMOMLJFCOIEGCEIEEPJLAA.jonathan.williams@pharm.ox.ac.uk>

Dear R-helpers,
I want to try to extract residuals from a multi-level 
linear mixed effects model, to correlate with another
variable. I need to know which residuals relate to
which experimental units in the lme. I can show the
labels that relate to the experimental units via the
command
ranef(fit0)$resid
which gives:
604/1/0 -1.276971e-05
604/1/1 -1.078644e-03
606/1/0 -7.391706e-03
606/1/1  8.371521e-03
610/1/0 -6.361077e-03
610/1/1 -1.090040e-03
646/1/0 -1.696881e-03
646/1/1 -6.396153e-03

But, I cannot figure out how to access the labels for 
each row in this table. names(unlist(ranef(fit0)[3])) 
and labels(unlist(ranef(fit0)[3])) both give the result
"side.(Intercept)xxx" where xxx is simply the row number
of the data frame in the lme. I want to be able to access
the unit identities (604, 606, 610 and 646 in the above
table). Could someone tell me how to get them?

Many thanks, in advance,

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356

PS, I am using R 1.9.0 on a Windows NT platform
(though I don't suppose it makes any difference).



From jonathan.williams at pharmacology.oxford.ac.uk  Tue Apr 27 19:17:13 2004
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Tue, 27 Apr 2004 18:17:13 +0100
Subject: [R] R hang-up using lm
Message-ID: <NGBBKJEMOMLJFCOIEGCEAEFAJLAA.jonathan.williams@pharm.ox.ac.uk>

Dear R-helpers,
I have found a slightly annoying problem when trying to
plot lines on graphs. I first created my data using
tapply, thus:-
y1=as.vector(fit1$coef$random$id)
x1=tapply(o1,id,median,na.rm=T)
x2=tapply(o2,id,median,na.rm=T)

#then I plot the data, thus:-
plot(x1[x2==0],y[x2==0])
#if I now try to fit the linear regression, R 'hangs up'
abline(lm(y[x2==0]~x1[x2==0]),lty='solid')
#but, curiously, the problem does not occur if I plot the lowess line
lines(lowess(y[x2==0]~x1[x2==0]))

I can also avoid the problem by coercing x1 and x2 to be vectors.
So, I don't need help to solve the problem. But, it seems an odd
problem since it affects one plotting method, but not the other, 
and it actually causes R to crash.

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From Prabhakar at Metreo.com  Tue Apr 27 20:40:26 2004
From: Prabhakar at Metreo.com (Prabhakar Krishnamurthy)
Date: Tue, 27 Apr 2004 11:40:26 -0700
Subject: [R] coding of categories in rpart
Message-ID: <6ED0981F1D12F14AA1ABF568854ECD274CCDB4@exchange>


Hello,

I am using rpart to derive classification rules for customer segments.
I have a few categorical variables in the set of independent variables.
For instance,

Account Size can be (Very-Small, Small, Medium, Large, V-Large)

Rpart seems to encode these categories into: a,b,c,d,e

The results are expressed in terms of the encoded values.

How do I find out what encoding was used by rpart.  i.e.
what categories in my input set do a, b, c,... correspond to?

thanks,
Prabhakar



From cepl at surfbest.net  Tue Apr 27 21:26:49 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Tue, 27 Apr 2004 15:26:49 -0400
Subject: [R] minimal requirement
In-Reply-To: <200404271134.57230.chrysopa@insecta.ufv.br>
References: <200404271134.57230.chrysopa@insecta.ufv.br>
Message-ID: <200404271526.49094.cepl@surfbest.net>

On Tuesday 27 of April 2004 10:34, Ronaldo Reis Jr. wrote:
> what is the minimal hardware requirement for run R on windows
> 9x? and for run it in a linux with X and a light windowmanager?
> Not for hard use, only for learning.

I have hear AMD K6 300MhZ / 160 MB RAM and it runs like a charm 
even with KDE (which is certainly not a light window manager). R 
itself does not take much space but everything depends on your 
data -- R takes everything into memory, so with 60MB crime data 
I had to put everything into PostgreSQL and even then it is not 
much fast.

Matej

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
Before you criticize someone, walk a mile in his shoes. That way,
if he gets angry, he'll be a mile away -- and barefoot.
      -- J.D. Salinger
         The Catcher in the Rye



From sasprog474 at yahoo.com  Tue Apr 27 22:32:06 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Tue, 27 Apr 2004 13:32:06 -0700 (PDT)
Subject: [R] p-values
Message-ID: <20040427203206.45556.qmail@web41409.mail.yahoo.com>

I apologize if this question is not completely 
appropriate for this list.

I have been using SAS for a while and am now in the 
process of learning some C and R as a part of my 
graduate studies.  All of the statistical packages I
have used generally yield p-values as a default output
to standard procedures.

This week I have been reading "Testing Precise
Hypotheses" by J.O. Berger & Mohan Delampady,
Statistical Science, Vol. 2, No. 3, 317-355 and
"Bayesian Analysis: A Look at Today and Thoughts of
Tomorrow" by J.O. Berger, JASA, Vol. 95, No. 452, p.
1269 - 1276, both as supplements to my Math Stat.
course.

It appears, based on these articles, that p-values are
more or less useless.  If this is indeed the case,
then why is a p-value typically given as a default
output?  For example, I know that PROC MIXED and 
lme( ) both yield p-values for fixed effects terms.

The theory I am learning does not seem to match what
is commonly available in the software, and I am just
wondering why.

Thanks,
    Greg



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 27 22:08:07 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 27 Apr 2004 21:08:07 +0100 (BST)
Subject: [R] se.fit in predict.glm
Message-ID: <XFMail.040427210807.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I'm seeking confirmation of something which is probably true
but which I have not managed to find in the documentation.

I have a binary response y={0.1} and a variable x and have
fitted a probit response to the data with

  f <- glm( y~x, family=binomial(link=probit) )

and then, with a specified set of x-value X I have used the
predict.glm function as

  p <- predict( f, X, type="response", se.fit=TRUE )

obtaining, as described in ?predict.glm, a list p with components

  p$fit  the fitted values (of P[y=1]) at the value of X

  p$se.fit

The documentation does not say definitely what p$se.fit is,
only calling it "Estimated standard errors". I *believe*
this means, at each value of X, the SE in the estimation
of P[y=1] taking account of the joint uncertainty in the
estimation of 'a' and 'b' in the relation

  probit(P) = a + b*X

Can someone confirm that this really is so?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 27-Apr-04                                       Time: 21:08:07
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Tue Apr 27 22:44:33 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 27 Apr 2004 22:44:33 +0200
Subject: [R] beginners  k means clustering question
In-Reply-To: <281BA46F2CF1054691D5E0F27B981C482DF863@icex33.ic.ac.uk>
References: <281BA46F2CF1054691D5E0F27B981C482DF863@icex33.ic.ac.uk>
Message-ID: <408EC631.9050000@statistik.uni-dortmund.de>

Johri, Saurabh wrote:

> Hi all,
>  
> I am wandering.. is it possible to cluster data which is in a single
> column ?
> for example.. I have some data as follows:
>  
> 4013
> 7362
> 7585
> 9304
> 11879
> 14785
> 21795
> 30500
> 30669
> 30924
> 33988
> 36975
> 40422
> 42911
> 50501
> 51593
> 53729
> 54338
> 55497
> 57337
> 61993
> 62601
> 66229
> 69815
> 69933
> 70760
> 71340
> 75921
> 83972
> 90134
> 91061
> .
> .
> .
>  
> is it possible to cluster this data since it is in a single column ?
>  
> I have used the following R commands:
>  
> data <- read.table("cluster.txt")
> dataMatrix <- t(data)
>  
> I then tried to cluster using the following:
>  
> xcl <-cclust(dataMatrix,2,20,verbose=TRUE,method="kmeans")
>  
> when I run this i receive the following error message:
>  
> Error in x[rank(runif(xrows))[1:ncenters], ] : 
>         incorrect number of dimensions
> 
> I would be grateful for any pointers in the right direction
>  
> Thanks for your help
>  
> Saurabh
>  
>  
>  


cclust() does not work for one-dim. data. I'd use kmeans() in package 
"stats" ...

Uwe Ligges



> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From richard_raubertas at merck.com  Tue Apr 27 22:46:52 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Tue, 27 Apr 2004 16:46:52 -0400
Subject: [R] R hang-up using lm
Message-ID: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>

Within the last few weeks, someone else reported a similar
problem when using the results of tapply in a call to rlm().
Note that the result of tapply is a 1D array, and it appears
there is a general problem with using such a thing on the
RHS in formula-based modeling functions:

set.seed(3)
yy <- rnorm(20)
gg <- rep(1:10, 2)
y <- tapply(yy, gg, median)
x <- 1:10
z <- lm(y ~ x)  # OK
z <- lm(x ~ y)  # crashes R

(R 1.8.1 on Windows XP Pro)

Rich Raubertas
Merck & Co.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Jonathan Williams
> Sent: Tuesday, April 27, 2004 1:17 PM
> To: Ethz. Ch
> Subject: [R] R hang-up using lm
> 
> 
> Dear R-helpers,
> I have found a slightly annoying problem when trying to
> plot lines on graphs. I first created my data using
> tapply, thus:-
> y1=as.vector(fit1$coef$random$id)
> x1=tapply(o1,id,median,na.rm=T)
> x2=tapply(o2,id,median,na.rm=T)
> 
> #then I plot the data, thus:-
> plot(x1[x2==0],y[x2==0])
> #if I now try to fit the linear regression, R 'hangs up'
> abline(lm(y[x2==0]~x1[x2==0]),lty='solid')
> #but, curiously, the problem does not occur if I plot the lowess line
> lines(lowess(y[x2==0]~x1[x2==0]))
> 
> I can also avoid the problem by coercing x1 and x2 to be vectors.
> So, I don't need help to solve the problem. But, it seems an odd
> problem since it affects one plotting method, but not the other, 
> and it actually causes R to crash.
> 
> Jonathan Williams
> OPTIMA
> Radcliffe Infirmary
> Woodstock Road
> OXFORD OX2 6HE
> Tel +1865 (2)24356
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From rolf at math.unb.ca  Tue Apr 27 22:53:53 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 27 Apr 2004 17:53:53 -0300 (ADT)
Subject: [OT] Re: [R] p-values
Message-ID: <200404272053.i3RKrrgS019401@erdos.math.unb.ca>

Greg Tarpinian wrote:

> I apologize if this question is not completely appropriate for this
> list.
> 
> I have been using SAS for a while and am now in the process of
> learning some C and R as a part of my graduate studies.  All of the
> statistical packages I have used generally yield p-values as a
> default output to standard procedures.
> 
> This week I have been reading "Testing Precise Hypotheses" by J.O.
> Berger & Mohan Delampady, Statistical Science, Vol. 2, No. 3, 317-355
> and "Bayesian Analysis: A Look at Today and Thoughts of Tomorrow" by
> J.O. Berger, JASA, Vol. 95, No. 452, p.  1269 - 1276, both as
> supplements to my Math Stat.  course.
> 
> It appears, based on these articles, that p-values are more or less
> useless.  If this is indeed the case, then why is a p-value typically
> given as a default output?  For example, I know that PROC MIXED and
> lme( ) both yield p-values for fixed effects terms.
> 
> The theory I am learning does not seem to match what is commonly
> available in the software, and I am just wondering why.

You shouldn't pay too much attention to the religeous ranting of
blinkered Bayesians.

Of course p-values should be taken with a grain of salt.  But
then so should everything else.  Including Bayesian methods.

Remember that all models are just that --- models.  They are not
reality.  George Box said something like ``All models are wrong.
Some models are useful.''

Experience has shown that p-values, ***interpreted with proper
caution and good common sense***, are very useful indeed.
And that's why they're there.

Bayesian methods and models are good too; they're just not the
be-all-and-end-all that the Zealots proclaim them to be.  And you
have to avoid the utterly ridiculous concept of ``personal
probability'' that Bayesians seem to find central.

A propos of the notion of ``personal probability'', the physicist
Carl Sagan (in his wonderful book ``The Demon Haunted World'')
referred to the ``dangerous doctrine'' that strength of belief has
anything to do with the truth of a proposition.  He was not writing
in any statistical context.  Rather, he was discussing the daffy
notion of ``alien abductions'' which apparently has been taken
seriously by some Ivy League psychologist on the basis that many of
those who claim to have been abducted so fervently ***believe***
their own tales. (So they must be true!)  However, the point still
stands.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From amackey at pcbi.upenn.edu  Tue Apr 27 23:10:09 2004
From: amackey at pcbi.upenn.edu (Aaron J. Mackey)
Date: Tue, 27 Apr 2004 17:10:09 -0400
Subject: [R] reading a "sparse" matrix into R
Message-ID: <43D15D80-988F-11D8-A84D-000A958C5008@pcbi.upenn.edu>


I have a 47k x 47k adjacency matrix that is very sparse (at most 30 
entries per row); my textual representation therefore is simply an 
adjacency list of connections between nodes for each row, e.g.

node	connections
A		B	C	D	E
B		A	C	D
C		A	E
D		A
E		A	F
F		E
G
H

I'd like to import this into a dataframe of node/connection 
(character/vector-of-characters) pairs.  I've experimented with scan, 
but haven't been able to coax it to work.  I can also "hack" it with 
strsplit() myself, but I thought there might be a more elegant way.

Thanks,

-Aaron



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 27 23:25:22 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 27 Apr 2004 22:25:22 +0100 (BST)
Subject: [R] p-values
In-Reply-To: <20040427203206.45556.qmail@web41409.mail.yahoo.com>
Message-ID: <XFMail.040427222522.Ted.Harding@nessie.mcc.ac.uk>

On 27-Apr-04 Greg Tarpinian wrote:
> I apologize if this question is not completely 
> appropriate for this list.

Never mind! (I'm only hoping that my response is ... )

> [...]
> This week I have been reading "Testing Precise
> Hypotheses" by J.O. Berger & Mohan Delampady,
> Statistical Science, Vol. 2, No. 3, 317-355 and
> "Bayesian Analysis: A Look at Today and Thoughts of
> Tomorrow" by J.O. Berger, JASA, Vol. 95, No. 452, p.
> 1269 - 1276, both as supplements to my Math Stat.
> course.
> 
> It appears, based on these articles, that p-values are
> more or less useless.

I don't have these articles available, but I'm guessing
that they stress the Bayesian approach to inference.
Saying "p-values are more or less useless" is controversial.
Bayesians consider p-values to be approximately irrelevant
to the real question, which is what you can say about
the probability that a hypothesis is true/false, or
what is the probability that a parameter lies in a
particular range (sometimes the same question); and the
"probability" they refer to is a posterior probability
distribution on hypotheses, or over parameter values.
The "P-value" which is emitted at the end of standard
analysis is not such a probability, but instead is that part
of a distribution over the sample space which is defined
by a "cut-off" value of a test statistic calculated from the
data. So they are different entities. Numerically they may
coincide; indeed, for statistical problems with a certain
structure the P-value is equal to the Bayesian posterior
probability when a particular prior distribution is
adopted.

> If this is indeed the case,
> then why is a p-value typically given as a default
> output?  For example, I know that PROC MIXED and 
> lme( ) both yield p-values for fixed effects terms.

P-values are not as useless as sometimes claimed. They
at least offer a measure of discrepancy between data and
hypothesis (the smaller the P-value, the more discrepant
the data), and they offer this measure on a standard scale,
the "probabiltiy scale" -- the chance of getting something
at least as discrepant, if the hypothesis being tested is
true. What "discrepant" objectively means is defined by
the test statistic used in calculating the P-value: larger
values of the test statistic correspond to more discrepant
data.

Confidence intervals are essentially aggregates of hypotheses
which have not been rejected at a significance level equal
to 1 minus the P-value.

The P-value/confidence-interval approach (often called the
"frequentist approach") gives results which do not depend
on assuming any prior distribution on the parameters/hypotheses,
and therefore could be called "objective" in that they
avoid being accused of importing "subjective" information
into the inference in the form of a Bayesion prior distribution.
This can have the consequence that your confidence interval
may include values in a range which, a priori, you do not
acept as plausible; or exclude a range of values in which
you are a priori confident that the real value lies.
The Bayesian comment on this situation is that the frequentist
approach is "incoherent", to which the frequentist might
respond "well, I just got an unlucky experiment this time"
(which is bound to occur with due frequency).

> The theory I am learning does not seem to match what
> is commonly available in the software, and I am just
> wondering why.

The standard ritual for evaluating statistical estimates
and hypothesis tests is frequentist (as above). Rightly
interpreted, it is by no means useless. For complex
historical reasons, it has become the norm in "research
methodology", and this is essentially why it is provided
by the standard software packages (otherwise pharmaceutical
companies would never buy the software, since they need
this in order to get past the FDA or other regulatory
authority). However, because this is the "norm", such
results often have more meaning attributed to them than
they can support, by people disinclined to delve into
what "rightly interpreted" might mean.

This is not a really clean answer to your question; but
then your question touches on complex and conflicting
issues!

Hoping this helps (and hoping that I am not poking a
hornets' nest here)!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 27-Apr-04                                       Time: 22:25:22
------------------------------ XFMail ------------------------------



From iwallace at eforceglobal.com  Tue Apr 27 23:33:40 2004
From: iwallace at eforceglobal.com (Ian Wallace)
Date: Tue, 27 Apr 2004 15:33:40 -0600
Subject: [R] Fedora 1 RPM Packages
Message-ID: <1083101619.3011.5.camel@localhost.localdomain>

Don't know if this is the correct place to post this question however I
thought I would start here.  The Fedora 1 packages are built without the
option '--enable-R-shlib' turned on in the R.spec file.  Other software,
like, plr (the postgresql library that calls R) needs the shared lib. 
Any chance that we can change the R.spec file to include:

[iwallace at localhost SPECS]$ diff -bu R.spec R.spec.orig
--- R.spec      2004-04-27 15:31:51.000000000 -0600
+++ R.spec.orig 2004-04-27 15:31:39.000000000 -0600
@@ -36,7 +36,6 @@
 %build
 export R_BROWSER="/usr/bin/mozilla"
 ( %configure \
-    --enable-R-shlib \
     --with-tcl-config=%{_libdir}/tclConfig.sh \
     --with-tk-config=%{_libdir}/tkConfig.sh ) \
  | egrep '^R is now|^ |^$' - > CAPABILITIES

I'm not sure if there are other issues with using the shared library,
I'm very new to R and just joined the mailing list.

Thanks!
ian

-- 
Ian Wallace <iwallace at eforceglobal.com>



From p.dalgaard at biostat.ku.dk  Tue Apr 27 23:32:56 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Apr 2004 23:32:56 +0200
Subject: [R] se.fit in predict.glm
In-Reply-To: <XFMail.040427210807.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040427210807.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2oepd5con.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> The documentation does not say definitely what p$se.fit is,
> only calling it "Estimated standard errors". I *believe*
> this means, at each value of X, the SE in the estimation
> of P[y=1] taking account of the joint uncertainty in the
> estimation of 'a' and 'b' in the relation
> 
>   probit(P) = a + b*X
> 
> Can someone confirm that this really is so?

Pretty accurate, I'd say. 

Basically, the fitted value is a function of the estimated parameters.
Asymptotically, the latter are approximately normally distributed with
a small dispersion so that the function is effectively linear and you
can approximate the distribution of the fitted value with a normal
distribution.

Just be aware that the fitted values can be on different scales
(P vs. logit(P)) and that the se.fit similarly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From yanw at stat.berkeley.edu  Tue Apr 27 23:43:37 2004
From: yanw at stat.berkeley.edu (Yan Wang)
Date: Tue, 27 Apr 2004 14:43:37 -0700
Subject: [R] helps on levelplot
Message-ID: <408ED409.9090901@stat.berkeley.edu>

I'm a new user of levelplot, and are not familiar with the terminology 
very well. Is the bar alongside the levelplot indicating color or shade 
call "colorkey"? I have to adjust the size of its label, but couldn't 
make it so far. This is how I did:

levelplot(z~x*y, grid, at=seq(0,1,by=0.1), 
colorkey=list(labels=list(cex=2)))

But I got error message "Error in draw.colorkey(x$colorkey) : Object 
"at" not found". I tried to specify "at" in colorkey too, but it didn't 
help.

A second task I have to do is to add horizontal and vertical lines on 
the levelplots, just like the way that ablines() works for image(). 
ablines doesn't work for levelplot. Any help will be much appreciated.



From Ted.Harding at nessie.mcc.ac.uk  Tue Apr 27 23:50:30 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 27 Apr 2004 22:50:30 +0100 (BST)
Subject: [R] se.fit in predict.glm
In-Reply-To: <x2oepd5con.fsf@biostat.ku.dk>
Message-ID: <XFMail.040427225030.Ted.Harding@nessie.mcc.ac.uk>

On 27-Apr-04 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
>> The documentation does not say definitely what p$se.fit is,
>> only calling it "Estimated standard errors". I *believe*
>> this means, at each value of X, the SE in the estimation
>> of P[y=1] taking account of the joint uncertainty in the
>> estimation of 'a' and 'b' in the relation
>> 
>>   probit(P) = a + b*X
>> 
>> Can someone confirm that this really is so?
> 
> Pretty accurate, I'd say. 
> 
> Basically, the fitted value is a function of the estimated parameters.
> Asymptotically, the latter are approximately normally distributed with
> a small dispersion so that the function is effectively linear and you
> can approximate the distribution of the fitted value with a normal
> distribution.

Thanks, Peter, that will do nicely! (And spot-on for the
particular application I have in hand).

> Just be aware that the fitted values can be on different scales
> (P vs. logit(P)) and that the se.fit similarly.

I take it your comment refers to the difference, in predict.glm,
between type = "link" (default) and type = "response"?

Thanks, and best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 27-Apr-04                                       Time: 22:50:30
------------------------------ XFMail ------------------------------



From dericks at northwestern.edu  Wed Apr 28 00:25:20 2004
From: dericks at northwestern.edu (Daniel Erickson)
Date: Tue, 27 Apr 2004 17:25:20 -0500
Subject: [R] r on AlphaVMS
Message-ID: <6.1.0.6.2.20040427172300.039a86d8@casbah.it.northwestern.edu>

Is there a way to compile and run R on AlphaVMS?


Daniel Erickson (dericks at northwestern.edu)



From p.dalgaard at biostat.ku.dk  Wed Apr 28 00:23:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2004 00:23:28 +0200
Subject: [R] se.fit in predict.glm
In-Reply-To: <XFMail.040427225030.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040427225030.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2ekq95acf.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> > Just be aware that the fitted values can be on different scales
> > (P vs. logit(P)) and that the se.fit similarly.
> 
> I take it your comment refers to the difference, in predict.glm,
> between type = "link" (default) and type = "response"?

Yep (in the opposite order, though).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From deepayan at stat.wisc.edu  Wed Apr 28 00:38:55 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 27 Apr 2004 17:38:55 -0500
Subject: [R] helps on levelplot
In-Reply-To: <408ED409.9090901@stat.berkeley.edu>
References: <408ED409.9090901@stat.berkeley.edu>
Message-ID: <200404271738.55733.deepayan@stat.wisc.edu>

On Tuesday 27 April 2004 16:43, Yan Wang wrote:
> I'm a new user of levelplot, and are not familiar with the
> terminology very well. Is the bar alongside the levelplot indicating
> color or shade call "colorkey"? I have to adjust the size of its
> label, but couldn't make it so far. This is how I did:
>
> levelplot(z~x*y, grid, at=seq(0,1,by=0.1),
> colorkey=list(labels=list(cex=2)))
>
> But I got error message "Error in draw.colorkey(x$colorkey) : Object
> "at" not found". I tried to specify "at" in colorkey too, but it
> didn't help.

Looks like a bug. For a workaround, you need to specify at in the labels 
component, not in colorkey (both are allowed), e.g.

colorkey=list(labels=list(cex=2, at=<whatever>))

> A second task I have to do is to add horizontal and vertical lines on
> the levelplots, just like the way that ablines() works for image().
> ablines doesn't work for levelplot. Any help will be much
> appreciated.

Use panel.abline, in the same way panel functions work in all lattice 
functions. e.g.,

levelplot(matrix(1:20, 4, 5), 
          panel = function(...) {
              panel.levelplot(...)
              panel.abline(h = 3.2)
          })

Deepayan



From rtomek at hotmail.com  Wed Apr 28 01:02:37 2004
From: rtomek at hotmail.com (Tomek R)
Date: Tue, 27 Apr 2004 23:02:37 +0000
Subject: [R] Solving linear equations
Message-ID: <BAY18-F69xmgyrniJpu00018ae1@hotmail.com>

Is there a simple method under R to solve an overdetermined system of linear 
equations Ax=b with A being a matrix m*n? I know that solve() seems to work 
for m*m matrices, but I had no luck with overdetermined systems.

Mlod



From dmurdoch at pair.com  Wed Apr 28 01:12:04 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 27 Apr 2004 19:12:04 -0400
Subject: [R] Solving linear equations
In-Reply-To: <BAY18-F69xmgyrniJpu00018ae1@hotmail.com>
References: <BAY18-F69xmgyrniJpu00018ae1@hotmail.com>
Message-ID: <appt80lm6j2r65geio9afqgppvd68t4sqe@4ax.com>

On Tue, 27 Apr 2004 23:02:37 +0000, "Tomek R" <rtomek at hotmail.com>
wrote:

>Is there a simple method under R to solve an overdetermined system of linear 
>equations Ax=b with A being a matrix m*n? I know that solve() seems to work 
>for m*m matrices, but I had no luck with overdetermined systems.

Isn't that just a linear regression problem?  You can get x as

lm.fit(A, b)$coefficients

or go lower level and use 

qr.coef(qr(A), b)

Duncan Murdoch



From spencer.graves at pdf.com  Wed Apr 28 01:18:21 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 27 Apr 2004 16:18:21 -0700
Subject: [R] Solving linear equations
In-Reply-To: <BAY18-F69xmgyrniJpu00018ae1@hotmail.com>
References: <BAY18-F69xmgyrniJpu00018ae1@hotmail.com>
Message-ID: <408EEA3D.7080907@pdf.com>

      How do you wish to decide how to resolve the likely 
inconsistencies in the overdetermined system?  If you assume the 
vertical deviations from a linear fit are normally distributed with 
constant variance, then "lm" should do what you want. 

      hope this helps.  spencer graves

Tomek R wrote:

> Is there a simple method under R to solve an overdetermined system of 
> linear equations Ax=b with A being a matrix m*n? I know that solve() 
> seems to work for m*m matrices, but I had no luck with overdetermined 
> systems.
>
> Mlod
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Wed Apr 28 01:19:59 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Tue, 27 Apr 2004 19:19:59 -0400
Subject: [R] R hang-up using lm
In-Reply-To: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
Message-ID: <408EB25F.18784.1A36B08@localhost>

On 27 Apr 2004 at 16:46, Raubertas, Richard wrote:

> Within the last few weeks, someone else reported a similar
> problem when using the results of tapply in a call to rlm().
> Note that the result of tapply is a 1D array, and it appears
> there is a general problem with using such a thing on the
> RHS in formula-based modeling functions:
> 
> set.seed(3)
> yy <- rnorm(20)
> gg <- rep(1:10, 2)
> y <- tapply(yy, gg, median)
> x <- 1:10
> z <- lm(y ~ x)  # OK
> z <- lm(x ~ y)  # crashes R
> 
> (R 1.8.1 on Windows XP Pro)
> 

What exactly do you mean by "crashes R"

Doing this in R1.9.0, windows XP pro, there is no indication of 
problems.

Kjetil Halvorsen

> Rich Raubertas
> Merck & Co.
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > Jonathan Williams
> > Sent: Tuesday, April 27, 2004 1:17 PM
> > To: Ethz. Ch
> > Subject: [R] R hang-up using lm
> > 
> > 
> > Dear R-helpers,
> > I have found a slightly annoying problem when trying to
> > plot lines on graphs. I first created my data using
> > tapply, thus:-
> > y1=as.vector(fit1$coef$random$id)
> > x1=tapply(o1,id,median,na.rm=T)
> > x2=tapply(o2,id,median,na.rm=T)
> > 
> > #then I plot the data, thus:-
> > plot(x1[x2==0],y[x2==0])
> > #if I now try to fit the linear regression, R 'hangs up'
> > abline(lm(y[x2==0]~x1[x2==0]),lty='solid')
> > #but, curiously, the problem does not occur if I plot the lowess
> > #line
> > lines(lowess(y[x2==0]~x1[x2==0]))
> > 
> > I can also avoid the problem by coercing x1 and x2 to be vectors.
> > So, I don't need help to solve the problem. But, it seems an odd
> > problem since it affects one plotting method, but not the other, and
> > it actually causes R to crash.
> > 
> > Jonathan Williams
> > OPTIMA
> > Radcliffe Infirmary
> > Woodstock Road
> > OXFORD OX2 6HE
> > Tel +1865 (2)24356
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> ----------------------------------------------------------------------
> -------- Notice:  This e-mail message, together with any
> attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Wed Apr 28 01:47:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 27 Apr 2004 16:47:17 -0700 (PDT)
Subject: [R] Problems raised to 1/3 power and NaN
In-Reply-To: <0016F5677B1F1D4281EEBC034993595155B876@CORPEXBE1.arcadis-us.com>
References: <0016F5677B1F1D4281EEBC034993595155B876@CORPEXBE1.arcadis-us.com>
Message-ID: <Pine.A41.4.58.0404271643130.139832@homer09.u.washington.edu>

On Tue, 27 Apr 2004, Bock, Michael wrote:
>
> I though you could take the cubed root of negative numbers (basic
> algebra) but perhaps this does not hold true for computer math due to
> rounding of the exponents.
>

Yes. All the representable floating point numbers are fractions whose
denominator is a power of two, and so none of them give exactly
real-valued roots of negative numbers.

You could use complex numbers, but then you have the problem of getting
the correct cube root.

> (-1+0i)^(1/3)
[1] 0.5+0.8660254i


	-thomas



From twiens at interbaun.com  Wed Apr 28 01:49:01 2004
From: twiens at interbaun.com (Trevor Wiens)
Date: Tue, 27 Apr 2004 17:49:01 -0600
Subject: [R] Spatial autocorrelation in linear regression models
Message-ID: <408EF16D.1030103@interbaun.com>

The scope of my problem relates to bird count data.

Point counts (standing and listening at one point) were conducted along 
transects, every 500 m.

In literature, I've found reference to calculating an autocorrelate for 
each observation where if a species was found in an adjacent site, it is 
assigned a value of 1, otherwise it has a value of 0. This is simple. 
What I don't understand is how this can be included in a model without 
it having the opposite effect and becoming a predictor?

I'm using stepAIC for my model selection.

T
-- 
Trevor Wiens
twiens at interbaun.com



From kjetil at acelerate.com  Wed Apr 28 02:02:32 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Tue, 27 Apr 2004 20:02:32 -0400
Subject: [R] Rtemp directories accumulating over time
Message-ID: <408EBC58.3302.1CA60BA@localhost>

Hola!

There is a nuisance that the number of directories with name starting 
Rtmp (and always empty) in my temp directory is increasing over time. 

I put the following in my  .Rprofile:

tmp0001 <- tempdir()
setHook( packageEvent("base","onUnload"), 
      function(...) unlink( tmp0001, recursive=TRUE) )


which solves part of the problem, but not all. So there are also 
other tmpdirs made by R. Why, where, and why are they not removed at 
exit (when their content are removed)?

Kjetil Halvorsen



From kjetil at acelerate.com  Wed Apr 28 02:02:32 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Tue, 27 Apr 2004 20:02:32 -0400
Subject: [R] plot.ts
Message-ID: <408EBC58.4806.1CA603D@localhost>

I have problems getting sensible series name plotted
with the ts.plot function. It doesn't seem to 
use either ylab= or xy.labels= arguments. 
I ended up using

plot({arg <- ts.union(gasolina.eq, PIBmensPred, PIBgrowthmens) ;
       colnames(arg) <- c("Gaso" ,"PIB", "PIBgrowth");arg },
   main="Gasolina eq. con crecimiento Economico", 
   xlab="Tiempo")

There must be a more natural way?

Kjetil Halvorsen



From hodgess at gator.uhd.edu  Wed Apr 28 02:18:04 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Tue, 27 Apr 2004 19:18:04 -0500
Subject: [R] second y axis
Message-ID: <200404280018.i3S0I4g22391@gator.dt.uh.edu>

Dear R People:

Someone had asked about plotting 2 time series with the
same time component with different scales(y axes).

I had forgotten to save the question, but here is the answer:

> data(USeconomic)
> help(USeconomic)
> plot(M1)
> par(new=TRUE)
> plot(GNP,axes=F,xlab="",ylab="")
> axis(4)
> 

Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From ggrothendieck at myway.com  Wed Apr 28 02:46:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 28 Apr 2004 00:46:37 +0000 (UTC)
Subject: [R] plot.ts
References: <408EBC58.4806.1CA603D@localhost>
Message-ID: <loom.20040428T024402-344@post.gmane.org>

 <kjetil <at> acelerate.com> writes:
> I have problems getting sensible series name plotted
> with the ts.plot function. It doesn't seem to 
> use either ylab= or xy.labels= arguments. 
> I ended up using
> 
> plot({arg <- ts.union(gasolina.eq, PIBmensPred, PIBgrowthmens) ;
>        colnames(arg) <- c("Gaso" ,"PIB", "PIBgrowth");arg },
>    main="Gasolina eq. con crecimiento Economico", 
>    xlab="Tiempo")

plot( 
  ts.union(Gaso = gasolina.eq, PIB = PIBmensPred, PIBgrowth = PIBgrowthmens),
  main = "Gasolina eq. con crecimiento Economico",
  xlab = "Tiempo" )



From lforzani at stat.umn.edu  Wed Apr 28 05:24:30 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Tue, 27 Apr 2004 22:24:30 -0500 (CDT)
Subject: [R] sas vs r
In-Reply-To: <x2ekq95acf.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404272213160.21225-100000@muskrat.stat.umn.edu>

I have a code in sas (NLMIXED) and I have a hard time converting to r

1)it is poisson, with random intercept, but
it have an offset.  Means, I do not want one of the coefficient to be
estimate. Means, may model is

g(mean) = beta X + Z,

Z fixed, X fixed and beta to be estimate

I am using glmmML.

2) the same but I have random slope (and I think with glmmML I can use
only random intercept)

3) I try to use nlme, is this "equivalent" to NLMIXED? thanks in advance

Liliana Forzani



From pauljohn at ku.edu  Wed Apr 28 08:16:03 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Wed, 28 Apr 2004 01:16:03 -0500
Subject: [R] Possible bug in foreign library import of Stata datasets
Message-ID: <408F4C23.4010600@ku.edu>

Concerning this article, Christopher Zorn, "Generalized Estimating 
Equation Models for Correlated Data: A Review with Applications." 2001. 
American Journal of Political Science 45(April):470-90.

The author very kindly provides data for replication on his web page: 
http://www.emory.edu/POLS/zorn/Data/GEE.zip.

  I've been comparing the Professor Zorn's results obtained with Stata 
and R.  I ran into some trouble with the results in Table 2.  I traced 
the problem back to the R foreign library's data import.  Observe the 
variable "deml" in the Stata output:


table deml

----------------------
Lower of  |
two       |
POLITY    |
democracy |
s         |      Freq.
----------+-----------
    -10.00 |        826
     -9.00 |      3,829
     -8.00 |      2,161
     -7.00 |      6,847
     -6.00 |        541
     -5.00 |        451
     -4.00 |        152
     -3.00 |        306
     -2.00 |        145
     -1.00 |        252
      0.00 |         94
      1.00 |        103
      2.00 |        169
      3.00 |        108
      4.00 |        404
      5.00 |        634
      6.00 |        154
      7.00 |        281
      8.00 |        923
      9.00 |        258
     10.00 |      2,352
----------------------


The negative valued observations get mixed up in R:

 > library(foreign)
 > dat2 <- read.dta("table2.dta")
 > table(deml)
deml
    0    1    2    3    4    5    6    7    8    9   10  246  247
   94  103  169  108  404  634  154  281  923  258 2352  826 3829
  248  249  250 251  252  253  254  255
  2161 6847  541 451  152  306  145  252

The read.dta has translated the negative values as (256-deml).

Is this the kind of thing that is a bug, or have I missed something in 
the documentation about the handling of negative numbers?  Should a 
formal bug report be filed?


-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From yanw at stat.berkeley.edu  Wed Apr 28 08:35:26 2004
From: yanw at stat.berkeley.edu (Yan Wang)
Date: Tue, 27 Apr 2004 23:35:26 -0700
Subject: [R] helps on levelplot
In-Reply-To: <200404271738.55733.deepayan@stat.wisc.edu>
References: <408ED409.9090901@stat.berkeley.edu>
	<200404271738.55733.deepayan@stat.wisc.edu>
Message-ID: <408F50AE.7000604@stat.berkeley.edu>

Thank you for the hints! I have some followup questions.

About the panel function, here is the code I copied from the example:
      xyplot(NOx ~ C | EE, data = ethanol,
             prepanel = function(x, y) prepanel.loess(x, y, span = 1),
             xlab = "Compression Ratio", ylab = "NOx (micrograms/J)",
             panel = function(x, y) {
                 panel.grid(h=-1, v= 2)
                 panel.xyplot(x, y)
                 panel.loess(x,y, span=1)
             },
             aspect = "xy")

My question is what is the relation between the xyplot function and 
panel.xyplot(x,y). In my case, I found that in the panel function, if I 
only have panel.abline(), the levelplot wouldn't be drawn at all. But I 
don't know how to specify the arguments in panel.levelplot(), which is 
exactly the part that you used "..." in your last reply. For all sorts 
of possible ways I tried, I got various of errors.

About the label size for the colorkey, when I coded as you suggested:
levelplot(z~x*y, grid,at=seq(0,1,by=0.1),scales=list(cex=2),
	colorkey=list(labels=list(cex=2, at=seq(0,1,by=0.1))))

I got error "Error in draw.colorkey(x$colorkey) : invalid type/length 
(3/1) in vector allocation". In fact, for whatever "at" I specified in 
labels, I got the same error message.

Further help is greatly appreciated.


Deepayan Sarkar wrote:

> On Tuesday 27 April 2004 16:43, Yan Wang wrote:
> 
>>I'm a new user of levelplot, and are not familiar with the
>>terminology very well. Is the bar alongside the levelplot indicating
>>color or shade call "colorkey"? I have to adjust the size of its
>>label, but couldn't make it so far. This is how I did:
>>
>>levelplot(z~x*y, grid, at=seq(0,1,by=0.1),
>>colorkey=list(labels=list(cex=2)))
>>
>>But I got error message "Error in draw.colorkey(x$colorkey) : Object
>>"at" not found". I tried to specify "at" in colorkey too, but it
>>didn't help.
> 
> 
> Looks like a bug. For a workaround, you need to specify at in the labels 
> component, not in colorkey (both are allowed), e.g.
> 
> colorkey=list(labels=list(cex=2, at=<whatever>))
> 
> 
>>A second task I have to do is to add horizontal and vertical lines on
>>the levelplots, just like the way that ablines() works for image().
>>ablines doesn't work for levelplot. Any help will be much
>>appreciated.
> 
> 
> Use panel.abline, in the same way panel functions work in all lattice 
> functions. e.g.,
> 
> levelplot(matrix(1:20, 4, 5), 
>           panel = function(...) {
>               panel.levelplot(...)
>               panel.abline(h = 3.2)
>           })
> 
> Deepayan



From maechler at stat.math.ethz.ch  Wed Apr 28 08:57:25 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Apr 2004 08:57:25 +0200
Subject: [R] Legend - revisited
In-Reply-To: <408EA2BC.27434.23EB53E@localhost>
References: <E1BIPsO-000OR4-Pd@serenity.mcc.ac.uk>
	<408EA2BC.27434.23EB53E@localhost>
Message-ID: <16527.21973.352245.569224@gargle.gargle.HOWL>

Thank you, Petr,

>>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
>>>>>     on Tue, 27 Apr 2004 18:13:16 +0200 writes:

    Petr> Hallo Monica I tried to do the same task recently and
    Petr> the only solution I was able to come with was to hack
    Petr> legend() function. You can find it in graphics library
    Petr> (R1.9.0).

    Petr> I just added

    Petr> cex.pt = cex,

    Petr> in the definition

    Petr> and

    Petr> cex=cex.pt,

    Petr> instead cex=cex,

    Petr> into draw points part of the function.

Since this seems really something you'd want (cex of text differing
from cex of points), I'd propose to use

  pt.cex  instead of  cex.pt,

for naming consistency, because we already have 'pt.bg'
(together with 'bg') as argument name.

    Petr> Than I can when calling legend to leave default cex or
    Petr> to use different setting for legend text ("cex") and
    Petr> points ("cex.pt" ).

    Petr> BTW cex is vectorised so cex=c(1,1,2,3) gives you
    Petr> different cex setting for 4 different legend items.

    Petr> I copied the modified function into my library and now

you certainly mean "package", not "library" ... ;-|

    Petr> I wait if some other function starts to complain about
    Petr> my modifications :-).

Hardly possible -- since this is a feature that cannot break
back-compatibly.
For this same reason, I'm considering adding this mini-extension
already for R-patched (probably to be 1.9.1).

Martin



From maechler at stat.math.ethz.ch  Wed Apr 28 09:37:03 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Apr 2004 09:37:03 +0200
Subject: [R] Error on installation of R-1.8 and R-1.9
In-Reply-To: <490D0AFAF3D2D3119F6C00508B6FDF150264423D@ex.mshri.on.ca>
References: <490D0AFAF3D2D3119F6C00508B6FDF150264423D@ex.mshri.on.ca>
Message-ID: <16527.24351.506292.53271@gargle.gargle.HOWL>

>>>>> "Jim" == Jim Wei <JIM at mshri.on.ca>
>>>>>     on Tue, 27 Apr 2004 10:56:00 -0400 writes:

    Jim> Hi; 

    Jim> We have R-1.7 installed on our Sun box with Solaris 8,
    Jim> and try to install R-1.8.1 or R-1.9, after I run
    Jim> ./configure, both installation give me the error as
    Jim> following

    Jim> ---------------------------------------------------------------------
    Jim> checking for int... yes
    Jim> checking size of int... configure: error: cannot compute sizeof (int), 77
    Jim> See `config.log' for more details.

So, why didn't you tell us what you found -- doing what the
above line tells you to do?

    Jim> ---------------------------------------------------------------------

    Jim> Any suggestions  will be greatly appreciated. 

(see above) +

I'd guess you're using a C-compiler C-library (+ Fortran...)
combination that is not correct for your version of Solaris.

This often happens when you upgrade Solaris and keep older gcc
installations (partly) around.

But we cannot help you really when you don't do your part.... 

Martin Maechler



From p.dalgaard at biostat.ku.dk  Wed Apr 28 10:04:11 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2004 10:04:11 +0200
Subject: [R] Possible bug in foreign library import of Stata datasets
In-Reply-To: <408F4C23.4010600@ku.edu>
References: <408F4C23.4010600@ku.edu>
Message-ID: <x2hdv4y1dw.fsf@biostat.ku.dk>

Paul Johnson <pauljohn at ku.edu> writes:

> Concerning this article, Christopher Zorn, "Generalized Estimating
> Equation Models for Correlated Data: A Review with Applications."
> 2001. American Journal of Political Science 45(April):470-90.
> 
> The author very kindly provides data for replication on his web page:
> http://www.emory.edu/POLS/zorn/Data/GEE.zip.
> 
>   I've been comparing the Professor Zorn's results obtained with Stata
> and R.  I ran into some trouble with the results in Table 2.  I traced
> the problem back to the R foreign library's data import.  Observe the
> variable "deml" in the Stata output:
> 
> 
> table deml
> 
> ----------------------
> Lower of  |
> two       |
> POLITY    |
> democracy |
> s         |      Freq.
> ----------+-----------
>     -10.00 |        826
>      -9.00 |      3,829
>      -8.00 |      2,161
>      -7.00 |      6,847
>      -6.00 |        541
>      -5.00 |        451
>      -4.00 |        152
>      -3.00 |        306
>      -2.00 |        145
>      -1.00 |        252
>       0.00 |         94
>       1.00 |        103
>       2.00 |        169
>       3.00 |        108
>       4.00 |        404
>       5.00 |        634
>       6.00 |        154
>       7.00 |        281
>       8.00 |        923
>       9.00 |        258
>      10.00 |      2,352
> ----------------------
> 
> 
> The negative valued observations get mixed up in R:
> 
>  > library(foreign)
>  > dat2 <- read.dta("table2.dta")
>  > table(deml)
> deml
>     0    1    2    3    4    5    6    7    8    9   10  246  247
>    94  103  169  108  404  634  154  281  923  258 2352  826 3829
>   248  249  250 251  252  253  254  255
>   2161 6847  541 451  152  306  145  252
> 
> The read.dta has translated the negative values as (256-deml).
> 
> Is this the kind of thing that is a bug, or have I missed something in
> the documentation about the handling of negative numbers?  Should a
> formal bug report be filed?

Looks like a classic signed/unsigned confusion. Negative numbers
stored in ones-complement format in single bytes, but getting
interpreted as unsigned. A bug report could be a good idea if the
resident Stata expert (Thomas, I believe) is unavailable just now.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jazevedo at provide.com.br  Wed Apr 28 10:08:26 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Wed, 28 Apr 2004 09:08:26 +0100
Subject: [R] Error on installation of R-1.8 and R-1.9
In-Reply-To: <16527.24351.506292.53271@gargle.gargle.HOWL>
Message-ID: <000001c42cf7$fbc4f5f0$a200a8c0@Lepc204>

Dear R users,
I would like to estimate a random and fixed effect panel data with spatial
correlation. 
I would honestly appreciate if anyone could point me toward any reference on
how I could estimate such models using R.
All the best,
Joao Pedro



From jazevedo at provide.com.br  Wed Apr 28 10:21:11 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Wed, 28 Apr 2004 09:21:11 +0100
Subject: [R] panel data with spatial correlation
In-Reply-To: <16527.24351.506292.53271@gargle.gargle.HOWL>
Message-ID: <000301c42cf9$c41d7440$a200a8c0@Lepc204>

Dear R users,
I would like to estimate a random and fixed effect panel data with spatial
correlation. 
I would honestly appreciate if anyone could point me toward any reference on
how I could estimate such models using R. All the best, Joao Pedro

PS: I'm sorry for the sending the previous message with the wrong subject.
My mistake.



From maechler at stat.math.ethz.ch  Wed Apr 28 10:23:53 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Apr 2004 10:23:53 +0200
Subject: [R] R hang-up using lm
In-Reply-To: <408EB25F.18784.1A36B08@localhost>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
Message-ID: <16527.27161.906345.349644@gargle.gargle.HOWL>

>>>>> "kjetil" == kjetil  <kjetil at acelerate.com>
>>>>>     on Tue, 27 Apr 2004 19:19:59 -0400 writes:

    kjetil> On 27 Apr 2004 at 16:46, Raubertas, Richard wrote:
    >> Within the last few weeks, someone else reported a similar
    >> problem when using the results of tapply in a call to rlm().
    >> Note that the result of tapply is a 1D array, and it appears
    >> there is a general problem with using such a thing on the
    >> RHS in formula-based modeling functions:
    >> 
    >> set.seed(3)
    >> yy <- rnorm(20)
    >> gg <- rep(1:10, 2)
    >> y <- tapply(yy, gg, median)
    >> x <- 1:10
    >> z <- lm(y ~ x)  # OK
    >> z <- lm(x ~ y)  # crashes R
    >> 
    >> (R 1.8.1 on Windows XP Pro)
    >> 

    kjetil> What exactly do you mean by "crashes R"

    kjetil> Doing this in R1.9.0, windows XP pro, there is no indication of 
    kjetil> problems.

nor is there with 1.9.0 or R-patched on Linux,
nor with R 1.8.1 on Linux.

no warning, no error, no problem at all.
Is it really the above (reproducible, thank you!) example
that crashes your R 1.8.1 ?

Martin



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 28 10:14:43 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 28 Apr 2004 09:14:43 +0100 (BST)
Subject: [R] Possible bug in foreign library import of Stata datasets
In-Reply-To: <408F4C23.4010600@ku.edu>
Message-ID: <XFMail.040428084439.Ted.Harding@nessie.mcc.ac.uk>

On 28-Apr-04 Paul Johnson wrote:
> The negative valued observations get mixed up in R:
> 
>  > library(foreign)
>  > dat2 <- read.dta("table2.dta")
>  > table(deml)
> deml
>     0    1    2    3    4    5    6    7    8    9   10  246  247
>    94  103  169  108  404  634  154  281  923  258 2352  826 3829
>   248  249  250 251  252  253  254  255
>   2161 6847  541 451  152  306  145  252
> 
> The read.dta has translated the negative values as (256-deml).
> 
> Is this the kind of thing that is a bug, or have I missed something in 
> the documentation about the handling of negative numbers?  Should a 
> formal bug report be filed?

This observation suggests a fairly clear diagnostic: the original
negative numbers (tabulated as "-10.00" etc) are coming through
as what C would call "signed char" -- positive for N=0 to 127,
negative (N-256) for N=128 to 255, but are being interpreted as
positive integers in (0,255). An unusual though feasible type.

The question is where this is occurring. The Stata tabulation
represents them as apparent reals; but the storage in the .dta file
may be 1-byte for economy of space. If so, then whether or not this
is a bug in read.dta may depend on whether the .dta file includes a
"flag" for such 1-byte data that they really are intended to represent
signed values (and possibly on whether there is a further flag for
real versus integer types). If not, then 1-byte data will not be
distinguishable from unsigned short integers, and read.dta can
hardly be blamed for getting the wrong impression.

Since I'm not familiar with Stata data file formats, I can't
comment further!

Ted.



From renaud.lancelot at pasteur.mg  Wed Apr 28 10:46:51 2004
From: renaud.lancelot at pasteur.mg (Renaud Lancelot)
Date: Wed, 28 Apr 2004 11:46:51 +0300
Subject: [R] R hang-up using lm
In-Reply-To: <16527.27161.906345.349644@gargle.gargle.HOWL>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
Message-ID: <408F6F7B.9060208@pasteur.mg>

Martin Maechler a ??crit :
>>>>>>"kjetil" == kjetil  <kjetil at acelerate.com>
>>>>>>    on Tue, 27 Apr 2004 19:19:59 -0400 writes:
> 
> 
>     kjetil> On 27 Apr 2004 at 16:46, Raubertas, Richard wrote:
>     >> Within the last few weeks, someone else reported a similar
>     >> problem when using the results of tapply in a call to rlm().
>     >> Note that the result of tapply is a 1D array, and it appears
>     >> there is a general problem with using such a thing on the
>     >> RHS in formula-based modeling functions:
>     >> 
>     >> set.seed(3)
>     >> yy <- rnorm(20)
>     >> gg <- rep(1:10, 2)
>     >> y <- tapply(yy, gg, median)
>     >> x <- 1:10
>     >> z <- lm(y ~ x)  # OK
>     >> z <- lm(x ~ y)  # crashes R
>     >> 
>     >> (R 1.8.1 on Windows XP Pro)
>     >> 
> 
>     kjetil> What exactly do you mean by "crashes R"
> 
>     kjetil> Doing this in R1.9.0, windows XP pro, there is no indication of 
>     kjetil> problems.
> 
> nor is there with 1.9.0 or R-patched on Linux,
> nor with R 1.8.1 on Linux.
> 
> no warning, no error, no problem at all.
> Is it really the above (reproducible, thank you!) example
> that crashes your R 1.8.1 ?

It does it for me: Windows XP Pro, R 1.9.0 (P IV, 2.4 GHz, 256 Mo RAM). 
It freezes RGui and a few seconds later, a Windows message appears 
saying that Rgui front-end met a problem and must be closed.

Best,

Renaud

-- 
Dr Renaud Lancelot
v??t??rinaire ??pid??miologiste
Ambassade de France - SCAC
BP 834 Antananarivo 101
Madagascar

t??l. +261 (0)32 04 824 55 (cell)
      +261 (0)20 22 494 37 (home)



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Apr 28 11:13:36 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 28 Apr 2004 10:13:36 +0100
Subject: [R] Legend - revisited, Thanks!
Message-ID: <E1BIl8W-0000CD-Jv@serenity.mcc.ac.uk>

Hi,

Thanks for the replies. I am not good enough in R to really hack the 
legend function but i can grasp what is all about. Actually i tried 
different cex.pt, pt.cex, and any other number of combinations 
between cex and pt and pch hopping one might be a parameter in 
the function .... Oh well ....

But pt.cex would be a nice addition - i think. Meanwhile Peter's 
solution seems more close to my abilities and understanding R so 
i will try that one.

But i am looking forward for the improved legend.

Thanks again,

Monica



From henric.nilsson at statisticon.se  Wed Apr 28 11:13:30 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 28 Apr 2004 11:13:30 +0200
Subject: [R] sas vs r
In-Reply-To: <Pine.LNX.4.44.0404272213160.21225-100000@muskrat.stat.umn.edu>
References: <x2ekq95acf.fsf@biostat.ku.dk>
	<Pine.LNX.4.44.0404272213160.21225-100000@muskrat.stat.umn.edu>
Message-ID: <6.0.3.0.0.20040428110234.0600d598@10.0.10.66>

At 05:24 2004-04-28, Liliana Forzani wrote:

>I have a code in sas (NLMIXED) and I have a hard time converting to r
>1)it is poisson, with random intercept, but
>it have an offset.  Means, I do not want one of the coefficient to be
>estimate. Means, may model is
>g(mean) = beta X + Z,
>Z fixed, X fixed and beta to be estimate
>I am using glmmML.

If I recall correctly, neither glmmML nor glmmPQL (from MASS) handles 
offset terms. But GLMM in the lme4 package does.

>2) the same but I have random slope (and I think with glmmML I can use
>only random intercept)

GLMM in lme4 can do this.

>3) I try to use nlme, is this "equivalent" to NLMIXED?

No, nlme fits non-linear and linear mixed-effect models with Gaussian error 
terms.

I hope this helps,
Henric



From gb at stat.umu.se  Wed Apr 28 11:18:38 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 28 Apr 2004 11:18:38 +0200
Subject: [R] R hang-up using lm
In-Reply-To: <408F6F7B.9060208@pasteur.mg>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
	<408F6F7B.9060208@pasteur.mg>
Message-ID: <20040428091838.GA8253@stat.umu.se>

On Wed, Apr 28, 2004 at 11:46:51AM +0300, Renaud Lancelot wrote:
> Martin Maechler a ??crit :
> >>>>>>"kjetil" == kjetil  <kjetil at acelerate.com>
> >>>>>>   on Tue, 27 Apr 2004 19:19:59 -0400 writes:
> >
> >
> >    kjetil> On 27 Apr 2004 at 16:46, Raubertas, Richard wrote:
> >    >> Within the last few weeks, someone else reported a similar
> >    >> problem when using the results of tapply in a call to rlm().
> >    >> Note that the result of tapply is a 1D array, and it appears
> >    >> there is a general problem with using such a thing on the
> >    >> RHS in formula-based modeling functions:
> >    >> 
> >    >> set.seed(3)
> >    >> yy <- rnorm(20)
> >    >> gg <- rep(1:10, 2)
> >    >> y <- tapply(yy, gg, median)
> >    >> x <- 1:10
> >    >> z <- lm(y ~ x)  # OK
> >    >> z <- lm(x ~ y)  # crashes R
> >    >> 
> >    >> (R 1.8.1 on Windows XP Pro)
> >    >> 
> >
> >    kjetil> What exactly do you mean by "crashes R"
> >
> >    kjetil> Doing this in R1.9.0, windows XP pro, there is no indication 
> >    of kjetil> problems.
> >
> >nor is there with 1.9.0 or R-patched on Linux,
> >nor with R 1.8.1 on Linux.
> >
> >no warning, no error, no problem at all.
> >Is it really the above (reproducible, thank you!) example
> >that crashes your R 1.8.1 ?
> 
> It does it for me: Windows XP Pro, R 1.9.0 (P IV, 2.4 GHz, 256 Mo RAM). 
> It freezes RGui and a few seconds later, a Windows message appears 
> saying that Rgui front-end met a problem and must be closed.

I had to try it too: No crashes on Win2000 pro (1.8.1) or Linux (1.9.0),
but (in both cases):


>  lm(y ~ x)

Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
    -0.8783       0.1293  


>  lm(x ~ y)

Call:
lm(formula = x ~ y)

Coefficients:
(Intercept)  
        5.5  

i.e., only an intercept estimate in the second case! Surely something is
wrong!? 

G??ran


> 
> Best,
> 
> Renaud
> 
> -- 
> Dr Renaud Lancelot
> v??t??rinaire ??pid??miologiste
> Ambassade de France - SCAC
> BP 834 Antananarivo 101
> Madagascar
> 
> t??l. +261 (0)32 04 824 55 (cell)
>      +261 (0)20 22 494 37 (home)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From gb at stat.umu.se  Wed Apr 28 11:39:34 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 28 Apr 2004 11:39:34 +0200
Subject: [R] sas vs r
In-Reply-To: <6.0.3.0.0.20040428110234.0600d598@10.0.10.66>
References: <x2ekq95acf.fsf@biostat.ku.dk>
	<Pine.LNX.4.44.0404272213160.21225-100000@muskrat.stat.umn.edu>
	<6.0.3.0.0.20040428110234.0600d598@10.0.10.66>
Message-ID: <20040428093934.GB8253@stat.umu.se>

On Wed, Apr 28, 2004 at 11:13:30AM +0200, Henric Nilsson wrote:
> At 05:24 2004-04-28, Liliana Forzani wrote:
> 
> >I have a code in sas (NLMIXED) and I have a hard time converting to r
> >1)it is poisson, with random intercept, but
> >it have an offset.  Means, I do not want one of the coefficient to be
> >estimate. Means, may model is
> >g(mean) = beta X + Z,
> >Z fixed, X fixed and beta to be estimate
> >I am using glmmML.
> 
> If I recall correctly, neither glmmML nor glmmPQL (from MASS) handles 
> offset terms. But GLMM in the lme4 package does.

glmmML handles offset terms. I am pretty sure that glmmPQL does too.

> 
> >2) the same but I have random slope (and I think with glmmML I can use
> >only random intercept)
> 
> GLMM in lme4 can do this.
> 
> >3) I try to use nlme, is this "equivalent" to NLMIXED?
> 
> No, nlme fits non-linear and linear mixed-effect models with Gaussian error 
> terms.
> 
> I hope this helps,
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From p.dalgaard at biostat.ku.dk  Wed Apr 28 11:41:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2004 11:41:03 +0200
Subject: [R] R hang-up using lm
In-Reply-To: <20040428091838.GA8253@stat.umu.se>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
	<408F6F7B.9060208@pasteur.mg> <20040428091838.GA8253@stat.umu.se>
Message-ID: <x28yggxwwg.fsf@biostat.ku.dk>

G??ran Brostr??m <gb at stat.umu.se> writes:

> > >    >> set.seed(3)
> > >    >> yy <- rnorm(20)
> > >    >> gg <- rep(1:10, 2)
> > >    >> y <- tapply(yy, gg, median)
> > >    >> x <- 1:10
> > >    >> z <- lm(y ~ x)  # OK
> > >    >> z <- lm(x ~ y)  # crashes R
....
> I had to try it too: No crashes on Win2000 pro (1.8.1) or Linux (1.9.0),
> but (in both cases):
> 
> 
> >  lm(y ~ x)
> 
> Call:
> lm(formula = y ~ x)
> 
> Coefficients:
> (Intercept)            x  
>     -0.8783       0.1293  
> 
> 
> >  lm(x ~ y)
> 
> Call:
> lm(formula = x ~ y)
> 
> Coefficients:
> (Intercept)  
>         5.5  
> 
> i.e., only an intercept estimate in the second case! Surely something is
> wrong!? 

I just tried it on the 64-bit system and got G??ran's result. However,
repeating the lm(x~y) bit seems to have gotten itself stuck after the
3rd time (looks like a memory runaway problem, currently at 3.6GB and
counting...). So perhaps those who couldn't reproduce should try it with
replicate(100, lm(x~y)) or so.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From messaoud at statistik.uni-dortmund.de  Wed Apr 28 11:29:27 2004
From: messaoud at statistik.uni-dortmund.de (Amor Messaoud)
Date: Wed, 28 Apr 2004 11:29:27 +0200
Subject: [R] Hsieh
Message-ID: <408F7977.8060709@statistik.uni-dortmund.de>



From Stephane.Dupas at pge.cnrs-gif.fr  Wed Apr 28 11:51:31 2004
From: Stephane.Dupas at pge.cnrs-gif.fr (=?ISO-8859-1?Q?Dupas_St=E9phane?=)
Date: Wed, 28 Apr 2004 11:51:31 +0200
Subject: [R] nlm
Message-ID: <408F7EA3.5020308@pge.cnrs-gif.fr>

Hi, I am performing deviance minimization on a biological model of 
reproductive incompatibility due to symbiotic bacteria, I have up to 15 
parameter estimates, but even with 8 parameters.
Using simulated data, I observe my nlm procedure do not converge to 
minimum value, how can I improve and make sure it converges?
thanks,

St??phane


-- 
St??phane Dupas
IRD c/o CNRS
Laboratoire Populations  G??n??tique et Evolution
1 av de la Terrasse
91198 Gif sur Yvette
+ 33 1 69 82 37 04 
http://www.cnrs-gif.fr/pge/index.html
http://www.cnrs-gif.fr/pge/index.php?lang=en



From prechelt at pcpool.mi.fu-berlin.de  Wed Apr 28 11:58:06 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Wed, 28 Apr 2004 11:58:06 +0200
Subject: [R] Extracting numbers from somewhere within strings
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920458EE@circle.pcpool.mi.fu-berlin.de>

Hello everybody,

I have a bunch of strings like this:
"IBM POWER4+ 1.9GHz"                  
"IBM RS64-III 500MHz"              
"IBM RS64-IV 600 MHz"                 
"IBM RS64 IV 750MHz"               
"Intel Itanium 2 Processor 6M 1.5GHz" 
"Intel Itanium2 1 Ghz"             
"Intel Itanium2 1.5GHz"               
"Intel MP 1.6GHz"                   

I want to extract the processor speed.

I am using
  grep("MHz", tpc$cpu, ignore.case=T)
  grep("GHz", tpc$cpu, ignore.case=T)
to extract the unit, because there are only these two.

But how to extract the number before it?
(I am using R 1.8.0)

In Perl one would match a regexp such as
  /([0-9.]+) ?[MG][Hh][Zz]/
and then obtain the number as $1.
But the capability of returning $1 is apparently not
implemented in grep() or any other function I could find.

How is it best done?

Thanks in advance,

  Lutz


Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
Institut fuer Informatik; Freie Universitaet Berlin
Takustr. 9; 14195 Berlin; Germany
+49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/



From Jean-Pierre.Mueller at dssp.unil.ch  Wed Apr 28 12:01:50 2004
From: Jean-Pierre.Mueller at dssp.unil.ch (=?ISO-8859-1?Q?Jean-Pierre_M=FCller?=)
Date: Wed, 28 Apr 2004 12:01:50 +0200
Subject: [R] R hang-up using lm
In-Reply-To: <20040428091838.GA8253@stat.umu.se>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
	<408F6F7B.9060208@pasteur.mg> <20040428091838.GA8253@stat.umu.se>
Message-ID: <11AF0D83-98FB-11D8-82CD-0003939DFC18@dssp.unil.ch>


Le 28 avr. 04, ?? 11:18, G??ran Brostr??m a ??crit :

>
>>  lm(y ~ x)
>
> Call:
> lm(formula = y ~ x)
>
> Coefficients:
> (Intercept)            x
>     -0.8783       0.1293
>
>
>>  lm(x ~ y)
>
> Call:
> lm(formula = x ~ y)
>
> Coefficients:
> (Intercept)
>         5.5
>

Same on RAqua 181,

and the 2 show:
z[]
...
$model
Error in as.data.frame.default(x[[i]], optional = TRUE) :
	can't coerce array into a data.frame

And calling first
z <- lm(x ~ y)
gives
Error: cannot allocate vector of size 3043328 Kb

HTH.
-- 
Jean-Pierre M??ller
SSP / BFSH2 / UNIL / CH - 1015 Lausanne
Voice:+41 21 692 3116 / Fax:+41 21 692 3115



From haleh.yasrebi at unn.ac.uk  Wed Apr 28 12:08:00 2004
From: haleh.yasrebi at unn.ac.uk (haleh.yasrebi)
Date: Wed, 28 Apr 2004 11:08:00 +0100
Subject: [R] connection to libraries problem
Message-ID: <518AAA861379D411B4E900508BCF7B7404077048@atlanta.unn.ac.uk>

Hello All,
Although I have downloaded some libraries such as multivariate data analysis
library (multiv) and ade4, their functions such as pca or reconst are not
recognised. Should I install any thing else or use any instruction so that R
could find the location of libraries?

Thanks for your quick response,

Haleh Yasrebi



From Roger.Bivand at nhh.no  Wed Apr 28 12:07:37 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 28 Apr 2004 12:07:37 +0200 (CEST)
Subject: [R] R hang-up using lm
In-Reply-To: <x28yggxwwg.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0404281204340.3664-100000@reclus.nhh.no>

On 28 Apr 2004, Peter Dalgaard wrote:

> G??ran Brostr??m <gb at stat.umu.se> writes:
> 
> > > >    >> set.seed(3)
> > > >    >> yy <- rnorm(20)
> > > >    >> gg <- rep(1:10, 2)
> > > >    >> y <- tapply(yy, gg, median)
> > > >    >> x <- 1:10
> > > >    >> z <- lm(y ~ x)  # OK
> > > >    >> z <- lm(x ~ y)  # crashes R
> ....
> > I had to try it too: No crashes on Win2000 pro (1.8.1) or Linux (1.9.0),
> > but (in both cases):
> > 
> > 
> > >  lm(y ~ x)
> > 
> > Call:
> > lm(formula = y ~ x)
> > 
> > Coefficients:
> > (Intercept)            x  
> >     -0.8783       0.1293  
> > 
> > 
> > >  lm(x ~ y)
> > 
> > Call:
> > lm(formula = x ~ y)
> > 
> > Coefficients:
> > (Intercept)  
> >         5.5  
> > 
> > i.e., only an intercept estimate in the second case! Surely something is
> > wrong!? 
> 
> I just tried it on the 64-bit system and got G??ran's result. However,
> repeating the lm(x~y) bit seems to have gotten itself stuck after the
> 3rd time (looks like a memory runaway problem, currently at 3.6GB and
> counting...). So perhaps those who couldn't reproduce should try it with
> replicate(100, lm(x~y)) or so.
> 

RHEL 3, R 1.9.0 seg.faults:

> set.seed(3)
> yy <- rnorm(20)
> gg <- rep(1:10, 2)
> y <- tapply(yy, gg, median)
> x <- 1:10
> z <- lm(y ~ x)  # OK
> str(y)
 num [, 1:10] -0.853 -0.712 -0.229 -0.450  0.174 ...
 - attr(*, "dimnames")=List of 1
  ..$ : chr [1:10] "1" "2" "3" "4" ...
> dim(y)
[1] 10
> z <- lm(x ~ y)
> replicate(100, lm(x~y))
Error in model.matrix.default(mt, mf, contrasts) : 
        invalid type for dimname (must be a vector)
> lm(x~y)

Program received signal SIGSEGV, Segmentation fault.
Rf_getAttrib (vec=0x8f71200, name=0x81f47d8) at attrib.c:99
99              if (TAG(s) == name) {
(gdb) bt
#0  Rf_getAttrib (vec=0x8f71200, name=0x81f47d8) at attrib.c:99
#1  0x08122363 in Rf_isArray (s=0x8f71200) at util.c:413
#2  0x08069aef in Rf_dimnamesgets (vec=0x8f71200, val=0x8f627c8)
    at attrib.c:581
#3  0x080cb845 in do_modelmatrix (call=0x8674200, op=0x8212f00, args=0x0, 
    rho=0x8f5eaf8) at model.c:1904
#4  0x080cc6dc in do_internal (call=0x4320, op=0x8204a78, args=0x8f710e8, 
    env=0x8f5eaf8) at names.c:1057
#5  0x080a74d2 in Rf_eval (e=0x86740b0, rho=0x8f5eaf8) at eval.c:375
#6  0x080a905b in do_set (call=0x8674040, op=0x82038c4, args=0x867405c, 
    rho=0x8f5eaf8) at eval.c:1271
#7  0x080a74d2 in Rf_eval (e=0x8674040, rho=0x8f5eaf8) at eval.c:375
#8  0x080a88ce in do_begin (call=0x867a068, op=0x82037c8, args=0x8674008, 
    rho=0x8f5eaf8) at eval.c:1046
#9  0x080a74d2 in Rf_eval (e=0x867a068, rho=0x8f5eaf8) at eval.c:375
#10 0x080a7779 in Rf_applyClosure (call=0x8f5ec80, op=0x867aaf4, 
    arglist=0x8f5ee5c, rho=0x8f5ed60, suppliedenv=0x8f5ecb8) at eval.c:566
#11 0x080cc9af in applyMethod (call=0x8f5ec80, op=0x867aaf4, 
    args=0x8f5ee5c, 
    rho=0x8f5ed60, newrho=0x8f5ecb8) at objects.c:119
#12 0x080cd01c in Rf_usemethod (generic=0x87aaf88 "model.matrix", 
    obj=0x8f5a18c, call=0x867aa14, args=0x81f4998, rho=0x8f5ed60, 
    callrho=0x8f3ab40, defrho=0x8b2a974, ans=0xbfffd288) at objects.c:326
#13 0x080cd4b5 in do_usemethod (call=0x867aa14, op=0x8212308, 
    args=0x867aa30, env=0x8f5ed60) at objects.c:389
#14 0x080a74d2 in Rf_eval (e=0x867aa14, rho=0x8f5ed60) at eval.c:375
#15 0x080a7779 in Rf_applyClosure (call=0x875c02c, op=0x867a838, 
    arglist=0x8f5ee5c, rho=0x8f3ab40, suppliedenv=0x81f4998) at eval.c:566
#16 0x080a72f5 in Rf_eval (e=0x875c02c, rho=0x8f3ab40) at eval.c:410
#17 0x080a905b in do_set (call=0x875be6c, op=0x82038c4, args=0x875bfd8, 
    rho=0x8f3ab40) at eval.c:1271
#18 0x080a74d2 in Rf_eval (e=0x875be6c, rho=0x8f3ab40) at eval.c:375
#19 0x080a88ce in do_begin (call=0x875d598, op=0x82037c8, args=0x875be50, 
    rho=0x8f3ab40) at eval.c:1046
#20 0x080a74d2 in Rf_eval (e=0x875d598, rho=0x8f3ab40) at eval.c:375
#21 0x080a74d2 in Rf_eval (e=0x875da20, rho=0x8f3ab40) at eval.c:375
#22 0x080a88ce in do_begin (call=0x8760a88, op=0x82037c8, args=0x875da04, 
    rho=0x8f3ab40) at eval.c:1046
#23 0x080a74d2 in Rf_eval (e=0x8760a88, rho=0x8f3ab40) at eval.c:375
#24 0x080a7779 in Rf_applyClosure (call=0x8f3ad38, op=0x8760804, 
    arglist=0x8f3ace4, rho=0x821546c, suppliedenv=0x81f4998) at eval.c:566
#25 0x080a72f5 in Rf_eval (e=0x8f3ad38, rho=0x821546c) at eval.c:410
#26 0x080c0eda in Rf_ReplIteration (rho=0x821546c, savestack=136268184, 
    browselevel=0, state=0xbfffdeb0) at main.c:250
#27 0x080c1083 in R_ReplConsole (rho=0x821546c, savestack=0, 
    browselevel=0) at main.c:298
#28 0x080c190f in run_Rmainloop () at main.c:653
#29 0x0812480c in main (ac=1, av=0xbfffe3c4) at system.c:99
(gdb) 



> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From lforzani at stat.umn.edu  Wed Apr 28 12:16:47 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Wed, 28 Apr 2004 05:16:47 -0500 (CDT)
Subject: [R] glmmPQL
In-Reply-To: <408F7EA3.5020308@pge.cnrs-gif.fr>
Message-ID: <Pine.LNX.4.44.0404280516070.1814-100000@muskrat.stat.umn.edu>

when I tried the example in glmmPQL I got an error

library(nlme)
   summary(glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
                      family = binomial, data = bacteria))

iteration 1
iteration 2
iteration 3
iteration 4
iteration 5
iteration 6
Error: No slot of name "reStruct" for this object of class "lme"
Error in logLik(object at reStruct) : Unable to find the argument "object" in
selecting a method for function "logLik"
>



From p.dalgaard at biostat.ku.dk  Wed Apr 28 12:17:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2004 12:17:18 +0200
Subject: [R] Extracting numbers from somewhere within strings
In-Reply-To: <85D25331FFB7AE4C900EA467D4ADA3920458EE@circle.pcpool.mi.fu-berlin.de>
References: <85D25331FFB7AE4C900EA467D4ADA3920458EE@circle.pcpool.mi.fu-berlin.de>
Message-ID: <x24qr4xv81.fsf@biostat.ku.dk>

"Lutz Prechelt" <prechelt at pcpool.mi.fu-berlin.de> writes:

> Hello everybody,
> 
> I have a bunch of strings like this:
> "IBM POWER4+ 1.9GHz"                  
> "IBM RS64-III 500MHz"              
> "IBM RS64-IV 600 MHz"                 
> "IBM RS64 IV 750MHz"               
> "Intel Itanium 2 Processor 6M 1.5GHz" 
> "Intel Itanium2 1 Ghz"             
> "Intel Itanium2 1.5GHz"               
> "Intel MP 1.6GHz"                   
> 
> I want to extract the processor speed.
> 
> I am using
>   grep("MHz", tpc$cpu, ignore.case=T)
>   grep("GHz", tpc$cpu, ignore.case=T)
> to extract the unit, because there are only these two.
> 
> But how to extract the number before it?
> (I am using R 1.8.0)
> 
> In Perl one would match a regexp such as
>   /([0-9.]+) ?[MG][Hh][Zz]/
> and then obtain the number as $1.
> But the capability of returning $1 is apparently not
> implemented in grep() or any other function I could find.
> 
> How is it best done?
> 
> Thanks in advance,

gsub() has \1 etc. For instance

> gsub("^.* ([0-9\\.]+) *[MG][Hh]z$","\\1",x)
[1] "1.9" "500" "600" "750" "1.5" "1"   "1.5" "1.6"

(Not exactly trivial to get that right, but neither is it in Perl...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lforzani at stat.umn.edu  Wed Apr 28 12:34:22 2004
From: lforzani at stat.umn.edu (Liliana Forzani)
Date: Wed, 28 Apr 2004 05:34:22 -0500 (CDT)
Subject: [R] glmmPQL
In-Reply-To: <Pine.LNX.4.44.0404280516070.1814-100000@muskrat.stat.umn.edu>
Message-ID: <Pine.LNX.4.44.0404280533410.1814-100000@muskrat.stat.umn.edu>

I thinks I understand the porblem, you can not use glmmPQL if you have
open lme4



On Wed, 28 Apr 2004, Liliana Forzani wrote:

> when I tried the example in glmmPQL I got an error
>
> library(nlme)
>    summary(glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
>                       family = binomial, data = bacteria))
>
> iteration 1
> iteration 2
> iteration 3
> iteration 4
> iteration 5
> iteration 6
> Error: No slot of name "reStruct" for this object of class "lme"
> Error in logLik(object at reStruct) : Unable to find the argument "object" in
> selecting a method for function "logLik"
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From stephen at anc.ed.ac.uk  Wed Apr 28 12:35:21 2004
From: stephen at anc.ed.ac.uk (Stephen Eglen)
Date: Wed, 28 Apr 2004 11:35:21 +0100
Subject: [R] Emacs Speaks Statistics version 5.2.0 has been released
Message-ID: <16527.35049.586431.147267@bushmills.inf.ed.ac.uk>

Emacs Speaks Statistics (ESS) version 5.2 is available for download
at:

http://www.analytics.washington.edu/downloads/ess/ess-5.2.0.tar.gz
or
http://www.analytics.washington.edu/downloads/ess/ess-5.2.0.zip

Changes since 5.1.24 are listed below.

Thanks, 
The ESS Core Team.

Changes/New Features in 5.2.0:
   * ESS[BUGS]:  new info documentation!  now supports interactive
     processing thanks to Aki Vehtari (mailto:Aki.Vehtari at hut.fi); new
     architecture-independent unix support as well as support for BUGS
     v. 0.5

   * ESS[SAS]:  convert .log to .sas with ess-sas-transcript; info
     documentation improved; Local Variable bug fixes; SAS/IML
     statements/functions now highlighted; files edited remotely by
     ange-ftp/EFS/tramp are recognized and pressing SUBMIT opens a
     buffer on the remote host via the local variable
     ess-sas-shell-buffer-remote-init which defaults to "ssh"; changed
     the definition of the variable ess-sas-edit-keys-toggle to boolean
     rather than 0/1; added the function ess-electric-run-semicolon
     which automatically reverse indents lines containing only "run;";
     C-F1 creates MS RTF portrait from the current buffer; C-F2 creates
     MS RTF landscape from the current buffer; C-F9 opens a SAS DATASET
     with PROC INSIGHT rather than PROC FSVIEW; C-F10 kills all buffers
     associated with .sas program; "inferior" aliases for SAS batch:
     C-c C-r for submit region, C-c C-b for submit buffer, C-c C-x for
     goto .log; C-c C-y for goto .lst

   * ESS[S]: Pressing underscore ("_") once inserts " <- " (as before);
     pressing underscore twice inserts a literal underscore.  To stop
     this smart behaviour, add "(ess-smart-underscore nil)" to your
     .emacs after ess-site has been loaded;
     ess-dump-filename-template-proto (new name!) now can be customized
     successfully (for S language dialects); Support for Imenu has been
     improved; set ess-imenu-use-S to non-nil to get an "Imenu-S" item
     on your menubar; ess-help: Now using nice underlines (instead of
     `nuke-* ^H_')

   * ESS[R]:  After (require 'essa-r), M-x ess-r-var allows to load
     numbers from any Emacs buffer into an existing *R* process; M-x
     ess-rdired gives a "directory editor" of R objects; fixed
     ess-retr-lastvalue-command, i.e. .Last.value bug (thanks to David
     Brahm)

   * ESS: Support for creating new window frames has been added to ESS.
     Inferior ESS processes can be created in dedicated frames by
     setting inferior-ess-own-frame to t.  ESS help buffers can also
     open in new frames; see the documentation for ess-help-own-frame
     for details.  (Thanks to Kevin Rodgers for contributing code.)



From gb at stat.umu.se  Wed Apr 28 12:59:54 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 28 Apr 2004 12:59:54 +0200
Subject: [R] R hang-up using lm
In-Reply-To: <20040428091838.GA8253@stat.umu.se>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
	<408F6F7B.9060208@pasteur.mg> <20040428091838.GA8253@stat.umu.se>
Message-ID: <20040428105954.GA8999@stat.umu.se>

On Wed, Apr 28, 2004 at 11:18:38AM +0200, G??ran Brostr??m wrote:
> On Wed, Apr 28, 2004 at 11:46:51AM +0300, Renaud Lancelot wrote:
> > Martin Maechler a ??crit :
> > >>>>>>"kjetil" == kjetil  <kjetil at acelerate.com>
> > >>>>>>   on Tue, 27 Apr 2004 19:19:59 -0400 writes:
> > >
> > >
> > >    kjetil> On 27 Apr 2004 at 16:46, Raubertas, Richard wrote:
> > >    >> Within the last few weeks, someone else reported a similar
> > >    >> problem when using the results of tapply in a call to rlm().
> > >    >> Note that the result of tapply is a 1D array, and it appears
> > >    >> there is a general problem with using such a thing on the
> > >    >> RHS in formula-based modeling functions:
> > >    >> 
> > >    >> set.seed(3)
> > >    >> yy <- rnorm(20)
> > >    >> gg <- rep(1:10, 2)
> > >    >> y <- tapply(yy, gg, median)
> > >    >> x <- 1:10
> > >    >> z <- lm(y ~ x)  # OK
> > >    >> z <- lm(x ~ y)  # crashes R
> > >    >> 
> > >    >> (R 1.8.1 on Windows XP Pro)
> > >    >> 
> > >
> > >    kjetil> What exactly do you mean by "crashes R"
> > >
> > >    kjetil> Doing this in R1.9.0, windows XP pro, there is no indication 
> > >    of kjetil> problems.
> > >
> > >nor is there with 1.9.0 or R-patched on Linux,
> > >nor with R 1.8.1 on Linux.
> > >
> > >no warning, no error, no problem at all.
> > >Is it really the above (reproducible, thank you!) example
> > >that crashes your R 1.8.1 ?
> > 
> > It does it for me: Windows XP Pro, R 1.9.0 (P IV, 2.4 GHz, 256 Mo RAM). 
> > It freezes RGui and a few seconds later, a Windows message appears 
> > saying that Rgui front-end met a problem and must be closed.
> 
> I had to try it too: No crashes on Win2000 pro (1.8.1) or Linux (1.9.0),
> but (in both cases):
> 
> 
> >  lm(y ~ x)
> 
> Call:
> lm(formula = y ~ x)
> 
> Coefficients:
> (Intercept)            x  
>     -0.8783       0.1293  
> 
> 
> >  lm(x ~ y)
> 
> Call:
> lm(formula = x ~ y)
> 
> Coefficients:
> (Intercept)  
>         5.5  
> 
> i.e., only an intercept estimate in the second case! Surely something is
> wrong!? 

Obviousy, y, generated as above, has an attribute that confuses 'lm',
because 

> lm(x ~ as.vector(y))

works as expected. To add further to confusion, R-1.8.1 (Windows):

> glm(x ~ y)

Error in model.matrix.default(mt, mf, contrasts) :
        invalid type for dimname (must be a vector)

while 1.9.0 (Linux):


> glm(x ~ y)

Call:  glm(formula = x ~ y) 

Coefficients:
(Intercept)  
        5.5  

Degrees of Freedom: 9 Total (i.e. Null);  9 Residual
Null Deviance:      82.5 
Residual Deviance: 82.5         AIC: 53.48 

> 
> G??ran
> 
> 
> > 
> > Best,
> > 
> > Renaud
> > 
> > -- 
> > Dr Renaud Lancelot
> > v??t??rinaire ??pid??miologiste
> > Ambassade de France - SCAC
> > BP 834 Antananarivo 101
> > Madagascar
> > 
> > t??l. +261 (0)32 04 824 55 (cell)
> >      +261 (0)20 22 494 37 (home)
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> -- 
>  G??ran Brostr??m                    tel: +46 90 786 5223
>  Department of Statistics          fax: +46 90 786 6614
>  Ume?? University                   http://www.stat.umu.se/egna/gb/
>  SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From petr.pikal at precheza.cz  Wed Apr 28 13:30:59 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 28 Apr 2004 13:30:59 +0200
Subject: [R] connection to libraries problem
In-Reply-To: <518AAA861379D411B4E900508BCF7B7404077048@atlanta.unn.ac.uk>
Message-ID: <408FB213.14027.1370FB6@localhost>



On 28 Apr 2004 at 11:08, haleh.yasrebi wrote:

> Hello All,
> Although I have downloaded some libraries such as multivariate data
> analysis library (multiv) and ade4, their functions such as pca or

Hi

library(packagename)

after you start R session

see

?library

Cheers
Petr

> reconst are not recognised. Should I install any thing else or use any
> instruction so that R could find the location of libraries?
> 
> Thanks for your quick response,
> 
> Haleh Yasrebi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From rtomek at hotmail.com  Wed Apr 28 14:11:10 2004
From: rtomek at hotmail.com (Tomek R)
Date: Wed, 28 Apr 2004 12:11:10 +0000
Subject: [R] Solving linear equations
Message-ID: <BAY18-F99EiouWrJBcB0001a596@hotmail.com>

lm.fit(A, b)$coefficients seems to do what I want, but, as you have rightly 
pointed out, the deviations most probably will not have constant variance 
(but should be normally distributed).  Basically, I make measurements from 
which I obtain the vector b with variance in each of the values v.  How 
would I fit my data then? (A is known and fixed). What's the best book to 
look at for solving those problems?

Thanks,

Tomek


>      How do you wish to decide how to resolve the likely inconsistencies 
>in the overdetermined system?  If you assume the vertical deviations from a 
>linear fit are normally distributed with constant variance, then 
>&quot;lm&quot; should do what you want.

_________________________________________________________________



From ripley at stats.ox.ac.uk  Wed Apr 28 14:11:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Apr 2004 13:11:09 +0100 (BST)
Subject: [R] nnet question
In-Reply-To: <408D4058.1080007@colorado.edu>
Message-ID: <Pine.LNX.4.44.0404281309230.25577-100000@gannet.stats>

Please read the documentation: why is a logistic output equation 
appropriate for example.  The iris3 example does not use the same 
arguments as you have, so you are claiming to know what you are doiung 
here.

You *have* worked though the examples in the book this software supports, 
haven't you?

On Mon, 26 Apr 2004, Erik Johnson wrote:

> I am using R 1.8.0, and am attempting to fit a Neural Network model of a 
> time series (here called Metrics.data).  It consists of one time series 
> variable run on its lag (AR(1)).  Basically, in an OLS model it would 
> look like
> Metrics.data$ewindx ~ Metrics.data$ewindx.lag1
> However, I am trying to run this through a neural network estimation.  
> So far, I have been getting convergence very quickly, and do not believe 
> it too be true.
> Here is the code and output.  Please note that I am using all of the 
> values for training and testing in one matrix, as I do not care about 
> the testing results right now, I only want to capture weights.  Here is 
> the code and output
> 
>  > nnet(metrics.data$ewindxlag1,metrics.data$ewindx,size=2, entropy=FALSE)
> # weights:  7
> initial  value 78858370643.085342
> final  value 78841786515.212158
> converged
> a 1-2-1 network with 7 weights
> options were -
> 
> When I run the iris3 example, the convergence looks much nicer 
> (consisting of more than one iteration).  Am I missing some fundamental 
> understanding of this example?  Thanks for any input.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Apr 28 14:20:15 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Apr 2004 14:20:15 +0200
Subject: [R] connection to libraries problem
In-Reply-To: <518AAA861379D411B4E900508BCF7B7404077048@atlanta.unn.ac.uk>
References: <518AAA861379D411B4E900508BCF7B7404077048@atlanta.unn.ac.uk>
Message-ID: <16527.41343.610822.720710@gargle.gargle.HOWL>

>>>>> "haleh" == haleh yasrebi <haleh.yasrebi at unn.ac.uk>
>>>>>     on Wed, 28 Apr 2004 11:08:00 +0100 writes:

    haleh> Hello All, Although I have downloaded some libraries
    haleh> such as multivariate data analysis library (multiv)
    haleh> and ade4, their functions such as pca or reconst are
    haleh> not recognised. Should I install any thing else or
    haleh> use any instruction so that R could find the location
    haleh> of libraries?

Petr has already helped you, so, ....,

                                              =========== 
please, Please, PLEASE, PLEASE, P_L_E_A_S_E , P L E A S E 
					      =========== 

       --------------------------------------------
       These are  NOT 'libraries'  but PACKAGES !!!
       --------------------------------------------

The packages are put into one or more libraries full of
packages,  which you see when typing  library().

The first argument of the function library() is called
 >>>> package <<<

phew..., thanks!
Martin



From john.maindonald at anu.edu.au  Wed Apr 28 14:31:03 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 28 Apr 2004 22:31:03 +1000
Subject: [R] p-values
In-Reply-To: <200404281007.i3SA6eWm029623@hypatia.math.ethz.ch>
References: <200404281007.i3SA6eWm029623@hypatia.math.ethz.ch>
Message-ID: <E9B6AC8C-990F-11D8-BD27-000A95CDA0F2@anu.edu.au>

The Bayesian framework is surely a good framework
for thinking about inference, and for exploring common
misinterpretations of p-values.  P-values are surely
unhelpful and to be avoided in cases where there is
`strong' prior evidence.  I will couch the discussion that
follows in terms of confidence intervals, which makes
the discussion simpler, rather than in terms of p-values.

The prior evidence is in my sense strong if it leads to a
Bayesian credible interval that is very substantially
different from the frequentist confidence interval
(though I prefer the term `coverage interval').
Typically the intervals will be similar if a "diffuse" prior
is used, i.e., all values over a wide enough range are,
on some suitable scale, a-priori equally likely.  This is,
in my view, the message that you should take from your
reading.

Examples of non-diffuse priors are what Berger focuses on.
Consider for example his discussion of one of Jeffreys'
analyses, where Jeffreys puts 50% of the probability on
on a point value of a a continuous parameter, i.e., there is
a large spike in the prior at that point.

Berger commonly has scant commentary on the specific
features of his priors that make the Bayesian results seem
very different (at least to the extent of having a different "feel")
from the frequentist results. His paper in vol 18, no 1 of
Statistical Science (pp.1-32; pp.12-27 are comment from
other) seems more judicious in this respect than some of
his earlier papers.

It is interesting to speculate how R's model fitting routines
might be tuned to allow a Bayesian interpretation.  What
family or families of priors would be on offer, and/or used by
default? What default mechanisms would be suitable &
useful for indicating the sensitivity of results to the choice
of prior?

John Maindonald.


> From: Greg Tarpinian <sasprog474 at yahoo.com>
> Date: 28 April 2004 6:32:06 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] p-values
>
>
> I apologize if this question is not completely
> appropriate for this list.
>
> I have been using SAS for a while and am now in the
> process of learning some C and R as a part of my
> graduate studies.  All of the statistical packages I
> have used generally yield p-values as a default output
> to standard procedures.
>
> This week I have been reading "Testing Precise
> Hypotheses" by J.O. Berger & Mohan Delampady,
> Statistical Science, Vol. 2, No. 3, 317-355 and
> "Bayesian Analysis: A Look at Today and Thoughts of
> Tomorrow" by J.O. Berger, JASA, Vol. 95, No. 452, p.
> 1269 - 1276, both as supplements to my Math Stat.
> course.
>
> It appears, based on these articles, that p-values are
> more or less useless.  If this is indeed the case,
> then why is a p-value typically given as a default
> output?  For example, I know that PROC MIXED and
> lme( ) both yield p-values for fixed effects terms.
>
> The theory I am learning does not seem to match what
> is commonly available in the software, and I am just
> wondering why.
>
> Thanks,
>     Greg

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From david.meyer at wu-wien.ac.at  Wed Apr 28 14:46:56 2004
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Wed, 28 Apr 2004 14:46:56 +0200
Subject: [R] reading a "sparse" matrix into R
In-Reply-To: <200404281013.i3SA796V029904@hypatia.math.ethz.ch>
References: <200404281013.i3SA796V029904@hypatia.math.ethz.ch>
Message-ID: <20040428144656.4cd4e34f.david.meyer@wu-wien.ac.at>

Have you considered the read.matrix.csr() function in pkg. e1071? It
uses another sparse input format, but perhaps you can easily transform
your data in the supported one. Also, in my experience, data frames are
not the best basis for a sparse format since they might turn out to be
very memory consuming and slow... The sparse formats provided by the
SparseM package are better suited for this.

-d

Date: Tue, 27 Apr 2004 17:10:09 -0400
From: "Aaron J. Mackey" <amackey at pcbi.upenn.edu>
Subject: [R] reading a "sparse" matrix into R
To: r-help at stat.math.ethz.ch
Message-ID: <43D15D80-988F-11D8-A84D-000A958C5008 at pcbi.upenn.edu>
Content-Type: text/plain; charset=US-ASCII; format=flowed


I have a 47k x 47k adjacency matrix that is very sparse (at most 30 
entries per row); my textual representation therefore is simply an 
adjacency list of connections between nodes for each row, e.g.

node	connections
A		B	C	D	E
B		A	C	D
C		A	E
D		A
E		A	F
F		E
G
H

I'd like to import this into a dataframe of node/connection 
(character/vector-of-characters) pairs.  I've experimented with scan, 
but haven't been able to coax it to work.  I can also "hack" it with 
strsplit() myself, but I thought there might be a more elegant way.

Thanks,

-Aaron



From sofie_pony at hotmail.com  Wed Apr 28 14:59:07 2004
From: sofie_pony at hotmail.com (sofie)
Date: Wed, 28 Apr 2004 14:59:07 +0200
Subject: [R] (no subject)
Message-ID: <BAY9-DAV26n97jyJhIL0003b336@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040428/86c9c72a/attachment.pl

From ripley at stats.ox.ac.uk  Wed Apr 28 14:53:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Apr 2004 13:53:42 +0100 (BST)
Subject: [R] Rtemp directories accumulating over time
In-Reply-To: <408EBC58.3302.1CA60BA@localhost>
Message-ID: <Pine.LNX.4.44.0404281347060.25610-100000@gannet.stats>

On Tue, 27 Apr 2004 kjetil at acelerate.com wrote:

> There is a nuisance that the number of directories with name starting 
> Rtmp (and always empty) in my temp directory is increasing over time. 
> 
> I put the following in my  .Rprofile:
> 
> tmp0001 <- tempdir()
> setHook( packageEvent("base","onUnload"), 
>       function(...) unlink( tmp0001, recursive=TRUE) )
> 
> 
> which solves part of the problem, but not all. 

I don't see why: package base is never unloaded so that hook function is 
never run.  (Indeed, no package/namespace is unloaded except by 
explicit user action, in particular not when R is terminated.)

> So there are also other tmpdirs made by R. Why, where, and why are they
> not removed at exit (when their content are removed)?

They are removed by R.  This is a Windows-only bug, as Windows sometimes
does not act on commands to remove empty directories (but only sometimes).
Session temporary directories should only be left around when a session 
crashes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Wed Apr 28 14:59:37 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 28 Apr 2004 08:59:37 -0400
Subject: [R] loading into a list
In-Reply-To: <4085312D.3060504@jhsph.edu>
References: <20040420133907.GA970@localhost>
	<20040420155713.3fda58ec.Achim.Zeileis@wu-wien.ac.at>
	<4085312D.3060504@jhsph.edu>
Message-ID: <408FAAB9.7070003@jhsph.edu>

Delayed response (!)

But perhaps more transparent than

mget(ls(e), envir = e)

is simply

as.list(e)

-roger

Roger D. Peng wrote:

> Or maybe
> 
> loadIntoList <- function(filename) {
>     e <- new.env()
>     load(filename, envir = e)
>     mget(ls(e), envir = e)
> }
> 
> -roger
> 
> Achim Zeileis wrote:
> 
>> On Tue, 20 Apr 2004 15:39:07 +0200 Tamas Papp wrote:
>>
>>
>>> I have the following problem.
>>>
>>> Use the same algorithm (with different parameters) to generate
>>> simulation results.  I store these in variables, eg A, B, C, which I
>>> save into a file with
>>>
>>> save(A, B, C, file="solution001.Rdata")
>>>
>>> I do this many times.  Then I would like to load the above, but in
>>> such a manner that A, B, C woule be part of a list, eg
>>>
>>> sol001 <- loadIntoList("solution001.Rdata")
>>>
>>> so that sol001 is a list with elements A, B, C.
>>>
>>> I am looking for a way to implement the above function. 
>>
>>
>>
>> If your objects are always called A, B, and C, the following should
>> work:
>>
>> loadIntoList <- function(file) {
>>   load(file)
>>   return(list(A = A, B = B, C = C))
>> }
>>
>> hth,
>> Z
>>
>>
>>> The variables
>>> are very large and I need a lot of time to compute them, so saving &
>>> loading the results is the only viable alternative.
>>>
>>> Thanks,
>>>
>>> Tamas
>>>
>>> -- 
>>> Tam??s K. Papp
>>> E-mail: tpapp at axelero.hu
>>> Please try to send only (latin-2) plain text, not HTML or other
>>> garbage.
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From deepayan at stat.wisc.edu  Wed Apr 28 15:01:24 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 28 Apr 2004 08:01:24 -0500
Subject: [R] helps on levelplot
In-Reply-To: <408F50AE.7000604@stat.berkeley.edu>
References: <408ED409.9090901@stat.berkeley.edu>
	<200404271738.55733.deepayan@stat.wisc.edu>
	<408F50AE.7000604@stat.berkeley.edu>
Message-ID: <200404280801.24074.deepayan@stat.wisc.edu>

On Wednesday 28 April 2004 01:35, Yan Wang wrote:
> Thank you for the hints! I have some followup questions.
>
> About the panel function, here is the code I copied from the example:
>       xyplot(NOx ~ C | EE, data = ethanol,
>              prepanel = function(x, y) prepanel.loess(x, y, span =
> 1), xlab = "Compression Ratio", ylab = "NOx (micrograms/J)", panel =
> function(x, y) {
>                  panel.grid(h=-1, v= 2)
>                  panel.xyplot(x, y)
>                  panel.loess(x,y, span=1)
>              },
>              aspect = "xy")
>
> My question is what is the relation between the xyplot function and
> panel.xyplot(x,y). In my case, I found that in the panel function, if
> I only have panel.abline(), the levelplot wouldn't be drawn at all.
> But I don't know how to specify the arguments in panel.levelplot(),
> which is exactly the part that you used "..." in your last reply. For
> all sorts of possible ways I tried, I got various of errors.

I believe this is described in some detail in the documentation. Have 
you read the entry for 'panel' in help(xyplot) ?

Generally speaking, each lattice function has its own panel function -- 
e.g. panel.xyplot, panel.levelplot, etc. The arguments they accept may 
be different, and are described in their respective help pages. 
Naturally, any high level function (like xyplot and levelplot) will 
pass to its panel function enough information for the default panel 
functions to work. Another way to find out what arguments are passed 
would be to use 

panel = function(...) print(names(list(...)))

BTW, I would recommend always using a ... argument in your panel 
function definition.

I don't know if this answers your questions. If it doesn't please be 
more specific in describing where your confusion lies.

> About the label size for the colorkey, when I coded as you suggested:
> levelplot(z~x*y, grid,at=seq(0,1,by=0.1),scales=list(cex=2),
> 	colorkey=list(labels=list(cex=2, at=seq(0,1,by=0.1))))
>
> I got error "Error in draw.colorkey(x$colorkey) : invalid type/length
> (3/1) in vector allocation". In fact, for whatever "at" I specified
> in labels, I got the same error message.

Works for me as expected. Perhaps you are not using the latest R 
(1.9.0).

Deepayan



From cristian at biometria.univr.it  Wed Apr 28 15:07:41 2004
From: cristian at biometria.univr.it (Cristian Pattaro)
Date: Wed, 28 Apr 2004 15:07:41 +0200
Subject: [R] Problem with W2000
Message-ID: <408FAC9D.7060805@biometria.univr.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040428/fcc3aa8f/attachment.pl

From ripley at stats.ox.ac.uk  Wed Apr 28 15:12:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Apr 2004 14:12:17 +0100 (BST)
Subject: [R] Fedora 1 RPM Packages
In-Reply-To: <1083101619.3011.5.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0404281407240.25610-100000@gannet.stats>

I think this is appropriate for most users: if you want to use
--enable-R-shlib you can and probably should compile R yourself.  (After
all, you are going to need the development tools for almost all uses of
libR.so.)  It is not a zero-cost option: it does for example cause shared
objects in packages to be linked against libR.so not R.bin and so
increases considerably the run-time footprint.

On Tue, 27 Apr 2004, Ian Wallace wrote:

> Don't know if this is the correct place to post this question however I
> thought I would start here.  The Fedora 1 packages are built without the
> option '--enable-R-shlib' turned on in the R.spec file.  Other software,
> like, plr (the postgresql library that calls R) needs the shared lib. 
> Any chance that we can change the R.spec file to include:
> 
> [iwallace at localhost SPECS]$ diff -bu R.spec R.spec.orig
> --- R.spec      2004-04-27 15:31:51.000000000 -0600
> +++ R.spec.orig 2004-04-27 15:31:39.000000000 -0600
> @@ -36,7 +36,6 @@
>  %build
>  export R_BROWSER="/usr/bin/mozilla"
>  ( %configure \
> -    --enable-R-shlib \
>      --with-tcl-config=%{_libdir}/tclConfig.sh \
>      --with-tk-config=%{_libdir}/tkConfig.sh ) \
>   | egrep '^R is now|^ |^$' - > CAPABILITIES
> 
> I'm not sure if there are other issues with using the shared library,
> I'm very new to R and just joined the mailing list.
> 
> Thanks!
> ian
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeaneid at chass.utoronto.ca  Wed Apr 28 15:33:01 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 28 Apr 2004 09:33:01 -0400
Subject: [R] numericDeriv
Message-ID: <Pine.SGI.4.40.0404280918430.12869226-100000@origin.chass.utoronto.ca>

Dear All,
I am trying to solve a Generalized Method of Moments problem which
necessitate the gradient of moments computation to get the
standard  errors of estimates.
I know optim does not output the gradient, but I can use numericDeriv to
get that. My question is: is this the best function to do this?

Thank you
Jean,



From bates at stat.wisc.edu  Wed Apr 28 15:43:32 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 28 Apr 2004 08:43:32 -0500
Subject: [R] reading a "sparse" matrix into R
In-Reply-To: <20040428144656.4cd4e34f.david.meyer@wu-wien.ac.at>
References: <200404281013.i3SA796V029904@hypatia.math.ethz.ch>
	<20040428144656.4cd4e34f.david.meyer@wu-wien.ac.at>
Message-ID: <6rllkgrzej.fsf@bates4.stat.wisc.edu>

David Meyer <david.meyer at wu-wien.ac.at> writes:

> Have you considered the read.matrix.csr() function in pkg. e1071? It
> uses another sparse input format, but perhaps you can easily transform
> your data in the supported one. Also, in my experience, data frames are
> not the best basis for a sparse format since they might turn out to be
> very memory consuming and slow... The sparse formats provided by the
> SparseM package are better suited for this.

The Matrix package (versions 0.8-1 and later) has a C function that does
the opposite operation, converting a symmetric sparse matrix to a
graph.

I would look at the way that the graph is stored there (the
formulation is from the Metis package of C code) and try to convert
your adjacency graph to that formulation first.



From gb at stat.umu.se  Wed Apr 28 16:00:17 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 28 Apr 2004 16:00:17 +0200
Subject: model.matrix (Re: [R] R hang-up using lm)
In-Reply-To: <20040428105954.GA8999@stat.umu.se>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
	<408F6F7B.9060208@pasteur.mg> <20040428091838.GA8253@stat.umu.se>
	<20040428105954.GA8999@stat.umu.se>
Message-ID: <20040428140017.GA1200@stat.umu.se>

On Wed, Apr 28, 2004 at 12:59:54PM +0200, G??ran Brostr??m wrote:
> On Wed, Apr 28, 2004 at 11:18:38AM +0200, G??ran Brostr??m wrote:
> > On Wed, Apr 28, 2004 at 11:46:51AM +0300, Renaud Lancelot wrote:
> > > Martin Maechler a ??crit :
> > > >>>>>>"kjetil" == kjetil  <kjetil at acelerate.com>
> > > >>>>>>   on Tue, 27 Apr 2004 19:19:59 -0400 writes:
> > > >
> > > >
> > > >    kjetil> On 27 Apr 2004 at 16:46, Raubertas, Richard wrote:
> > > >    >> Within the last few weeks, someone else reported a similar
> > > >    >> problem when using the results of tapply in a call to rlm().
> > > >    >> Note that the result of tapply is a 1D array, and it appears
> > > >    >> there is a general problem with using such a thing on the
> > > >    >> RHS in formula-based modeling functions:
> > > >    >> 
> > > >    >> set.seed(3)
> > > >    >> yy <- rnorm(20)
> > > >    >> gg <- rep(1:10, 2)
> > > >    >> y <- tapply(yy, gg, median)
> > > >    >> x <- 1:10
> > > >    >> z <- lm(y ~ x)  # OK
> > > >    >> z <- lm(x ~ y)  # crashes R
> > > >    >> 
> > > >    >> (R 1.8.1 on Windows XP Pro)
> > > >    >> 
> > > >
> > > >    kjetil> What exactly do you mean by "crashes R"
> > > >
> > > >    kjetil> Doing this in R1.9.0, windows XP pro, there is no indication 
> > > >    of kjetil> problems.
> > > >
> > > >nor is there with 1.9.0 or R-patched on Linux,
> > > >nor with R 1.8.1 on Linux.
> > > >
> > > >no warning, no error, no problem at all.
> > > >Is it really the above (reproducible, thank you!) example
> > > >that crashes your R 1.8.1 ?
> > > 
> > > It does it for me: Windows XP Pro, R 1.9.0 (P IV, 2.4 GHz, 256 Mo RAM). 
> > > It freezes RGui and a few seconds later, a Windows message appears 
> > > saying that Rgui front-end met a problem and must be closed.
> > 
> > I had to try it too: No crashes on Win2000 pro (1.8.1) or Linux (1.9.0),
> > but (in both cases):
> > 
> > 
> > >  lm(y ~ x)
> > 
> > Call:
> > lm(formula = y ~ x)
> > 
> > Coefficients:
> > (Intercept)            x  
> >     -0.8783       0.1293  
> > 
> > 
> > >  lm(x ~ y)
> > 
> > Call:
> > lm(formula = x ~ y)
> > 
> > Coefficients:
> > (Intercept)  
> >         5.5  
> > 
> > i.e., only an intercept estimate in the second case! Surely something is
> > wrong!? 
> 
> Obviousy, y, generated as above, has an attribute that confuses 'lm',
> because 
> 
> > lm(x ~ as.vector(y))
> 
> works as expected. To add further to confusion, R-1.8.1 (Windows):
> 
> > glm(x ~ y)
> 
> Error in model.matrix.default(mt, mf, contrasts) :
>         invalid type for dimname (must be a vector)

so, 

> model.matrix(x ~ y)
   (Intercept)
1            1
2            1
3            1
4            1
5            1
6            1
7            1
8            1
9            1
10           1
attr(,"assign")
[1] 0

but


> model.matrix(x ~ as.vector(y))
   (Intercept) as.vector(y)
1            1 -0.853357506
2            1 -0.711872147
3            1 -0.228785137
4            1 -0.449739758
5            1  0.173914266
6            1 -0.138766243
7            1 -0.433799800
8            1  0.234183701
9            1  0.002728104
10           1  0.733590165
attr(,"assign")
[1] 0 1

AND

> rr <- model.matrix.default(x ~ y)
Segmenteringsfel
gb at tal:~$ 

After a restart and repeating 

> model.matrix.default(x ~ y)

several times, I FINALLY got

>  model.matrix.default(x ~ y)
   (Intercept)
1            1
2            1
3            1
4            1
5            1
6            1
7            1
8            1
9            1
10           1
attr(,"assign")
[1] 0
>  model.matrix.default(x ~ y)
   (Intercept)            y
1            1 -0.853357506
2            1 -0.711872147
3            1 -0.228785137
4            1 -0.449739758
5            1  0.173914266
6            1 -0.138766243
7            1 -0.433799800
8            1  0.234183701
9            1  0.002728104
10           1  0.733590165
attr(,"assign")
[1] 0 1

>  model.matrix.default(x ~ y)
Segmenteringsfel
gb at tal:~$ 

(Don't ask me what's going on: but 'Segmenteringsfel' means 'Segmentation
fault':) 

BTW, this was on Debian testing/unstable; R-1.9.0
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From reid_huntsinger at merck.com  Wed Apr 28 16:32:34 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 28 Apr 2004 10:32:34 -0400
Subject: [R] reading a "sparse" matrix into R
Message-ID: <D9A95B4B7B20354992E165EEADA319990233D110@uswpmx00.merck.com>

I've used lists (generic vectors) for this, with integer storage mode. Then
I can easily manipulate them in R, they don't take too much room, and C code
to traverse them (e.g. find connected components) is fast.

Did you have a need for a data frame? That seems like it might be painful to
manipulate.

I read the text file with readLines.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Aaron J. Mackey
Sent: Tuesday, April 27, 2004 5:10 PM
To: r-help at stat.math.ethz.ch
Subject: [R] reading a "sparse" matrix into R



I have a 47k x 47k adjacency matrix that is very sparse (at most 30 
entries per row); my textual representation therefore is simply an 
adjacency list of connections between nodes for each row, e.g.

node	connections
A		B	C	D	E
B		A	C	D
C		A	E
D		A
E		A	F
F		E
G
H

I'd like to import this into a dataframe of node/connection 
(character/vector-of-characters) pairs.  I've experimented with scan, 
but haven't been able to coax it to work.  I can also "hack" it with 
strsplit() myself, but I thought there might be a more elegant way.

Thanks,

-Aaron

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Wed Apr 28 16:40:00 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 28 Apr 2004 16:40:00 +0200
Subject: model.matrix (Re: [R] R hang-up using lm)
In-Reply-To: <20040428140017.GA1200@stat.umu.se>
References: <38C4C095FC35E5469BED686B42F40A130808EB1C@usrymx17.merck.com>
	<408EB25F.18784.1A36B08@localhost>
	<16527.27161.906345.349644@gargle.gargle.HOWL>
	<408F6F7B.9060208@pasteur.mg> <20040428091838.GA8253@stat.umu.se>
	<20040428105954.GA8999@stat.umu.se>
	<20040428140017.GA1200@stat.umu.se>
Message-ID: <16527.49728.776094.390997@gargle.gargle.HOWL>

>>>>> "GB" == G??ran Brostr??m <gb at stat.umu.se>
>>>>>     on Wed, 28 Apr 2004 16:00:17 +0200 writes:


  <.........>

    GB> so,

    >> model.matrix(x ~ y)
    GB>    (Intercept) 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1
    GB> attr(,"assign") [1] 0

    GB> but


    >> model.matrix(x ~ as.vector(y))
    GB>    (Intercept) as.vector(y) 1 1 -0.853357506 2 1
    GB> -0.711872147 3 1 -0.228785137 4 1 -0.449739758 5 1
    GB> 0.173914266 6 1 -0.138766243 7 1 -0.433799800 8 1
    GB> 0.234183701 9 1 0.002728104 10 1 0.733590165
    GB> attr(,"assign") [1] 0 1

    GB> AND

    >> rr <- model.matrix.default(x ~ y)
    GB> Segmenteringsfel gb at tal:~$

 <..............>

    GB> (Don't ask me what's going on: but 'Segmenteringsfel'
    GB> means 'Segmentation fault':)

Thank you, G??ran.

Yes, the bug is in model.matrix.default();  
one part of the problems happens in the line
    ans <- .Internal(model.matrix(t, data))
(which only returns the intercept part).
How to fix this is not yet clear to me, since we have to decide
if the internal C code should do more checking
or the R code in model.matrix.default.

BTW, a very simple reproducible example is

x <- 1:7; y. <- x ; y <- array(y, 7)

 model.matrix(x ~ y) # which behaves badly when called repeatedly;
		     # which for me means memory allocation problems
## as opposed to 

 model.matrix(x ~ y.) # which is all fine

----

I'll also file this as bug report.
Martin



From myao at ou.edu  Wed Apr 28 16:42:19 2004
From: myao at ou.edu (Yao, Minghua)
Date: Wed, 28 Apr 2004 09:42:19 -0500
Subject: [R] Problem in Installing Package from CRAN...
Message-ID: <78B50CF247E5D04B8A5E02D001CC8E9A595C50@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040428/da84befd/attachment.pl

From asemeria at cramont.it  Wed Apr 28 16:56:59 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Wed, 28 Apr 2004 16:56:59 +0200
Subject: [R] Problem in Installing Package from CRAN...
Message-ID: <OF208EDFA8.0A656DB4-ONC1256E84.00521ED0@tomware.it>





Your user is not within administrators group.
Try with the user administrator.

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Wed Apr 28 16:55:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Apr 2004 15:55:01 +0100 (BST)
Subject: [R] Problem in Installing Package from CRAN...
In-Reply-To: <78B50CF247E5D04B8A5E02D001CC8E9A595C50@XMAIL.sooner.net.ou.edu>
Message-ID: <Pine.LNX.4.44.0404281554070.25719-100000@gannet.stats>

Do you have write permission in C:/PROGRA~1/R/rw1090/library?  If not (and 
it seems not), then you need to install elsewhere.

This *is* covered in the rw-FAQ: please consult the posting guide.


On Wed, 28 Apr 2004, Yao, Minghua wrote:

> Hi,
>  
> I have installed R 1.9.0 under Windows XP. When I used 
>  
> "Packages->Install Package(s) from CRAN..." to install the package 'gregmisc', I got this message:
>  
> > local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 17113 bytes
> opened URL
> downloaded 16Kb
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/gregmisc_0.10.2.zip'
> Content type `application/zip' length 594089 bytes
> opened URL
> downloaded 580Kb
> Error in unpackPkg(foundpkgs[okp, 2], pkgnames[okp], lib, installWithVers) : 
>         Unable to create temp directory C:/PROGRA~1/R/rw1090/library\file2869
> > 
>  
> Thanks in advance for any help.
>  
> Minghua
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From feh3k at spamcop.net  Wed Apr 28 17:13:50 2004
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 28 Apr 2004 10:13:50 -0500
Subject: [R] p-values
In-Reply-To: <XFMail.040427222522.Ted.Harding@nessie.mcc.ac.uk>
References: <20040427203206.45556.qmail@web41409.mail.yahoo.com>
	<XFMail.040427222522.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20040428101350.1af491f7.feh3k@spamcop.net>

On Tue, 27 Apr 2004 22:25:22 +0100 (BST)
(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> wrote:

> On 27-Apr-04 Greg Tarpinian wrote:
> > I apologize if this question is not completely 
> > appropriate for this list.
> 
> Never mind! (I'm only hoping that my response is ... )
> 
> > [...]
> > This week I have been reading "Testing Precise
> > Hypotheses" by J.O. Berger & Mohan Delampady,
> > Statistical Science, Vol. 2, No. 3, 317-355 and
> > "Bayesian Analysis: A Look at Today and Thoughts of
> > Tomorrow" by J.O. Berger, JASA, Vol. 95, No. 452, p.
> > 1269 - 1276, both as supplements to my Math Stat.
> > course.
> > 
> > It appears, based on these articles, that p-values are
> > more or less useless.
> 
. . .
> I don't have these articles available, but I'm guessing
> that they stress the Bayesian approach to inference.
> Saying "p-values are more or less useless" is controversial.
> Bayesians consider p-values to be approximately irrelevant
> to the real question, which is what you can say about
> the probability that a hypothesis is true/false, or
> what is the probability that a parameter lies in a
> particular range (sometimes the same question); and the
> "probability" they refer to is a posterior probability
> distribution on hypotheses, or over parameter values.
> The "P-value" which is emitted at the end of standard
> analysis is not such a probability, but instead is that part
> of a distribution over the sample space which is defined
> by a "cut-off" value of a test statistic calculated from the
> data. So they are different entities. Numerically they may
> coincide; indeed, for statistical problems with a certain
> structure the P-value is equal to the Bayesian posterior
> probability when a particular prior distribution is
> adopted.
> 
> > If this is indeed the case,
> > then why is a p-value typically given as a default
> > output?  For example, I know that PROC MIXED and 
> > lme( ) both yield p-values for fixed effects terms.
> 
> P-values are not as useless as sometimes claimed. They
> at least offer a measure of discrepancy between data and
> hypothesis (the smaller the P-value, the more discrepant
> the data), and they offer this measure on a standard scale,
> the "probabiltiy scale" -- the chance of getting something
> at least as discrepant, if the hypothesis being tested is
> true. What "discrepant" objectively means is defined by
> the test statistic used in calculating the P-value: larger
> values of the test statistic correspond to more discrepant
> data.

Ted, this opens up a can of worms, depending on what you mean by
"discrepant" and even "data" (something conditioned upon or a stochastic
quantity that we happen to only be looking at one copy of?).  I think your
statement plays into some of the severe difficulties with P-values,
especially large P-values.
  
> 
> Confidence intervals are essentially aggregates of hypotheses
> which have not been rejected at a significance level equal
> to 1 minus the P-value.
> 
> The P-value/confidence-interval approach (often called the
> "frequentist approach") gives results which do not depend
> on assuming any prior distribution on the parameters/hypotheses,
> and therefore could be called "objective" in that they
> avoid being accused of importing "subjective" information
> into the inference in the form of a Bayesion prior distribution.

They are objective only in the sense that subjectivity is deferred in a
difficult to document way when P-values are translated into decisions.

> This can have the consequence that your confidence interval
> may include values in a range which, a priori, you do not
> acept as plausible; or exclude a range of values in which
> you are a priori confident that the real value lies.
> The Bayesian comment on this situation is that the frequentist
> approach is "incoherent", to which the frequentist might
> respond "well, I just got an unlucky experiment this time"
> (which is bound to occur with due frequency).
> 
> > The theory I am learning does not seem to match what
> > is commonly available in the software, and I am just
> > wondering why.
> 
> The standard ritual for evaluating statistical estimates
> and hypothesis tests is frequentist (as above). Rightly
> interpreted, it is by no means useless. For complex
> historical reasons, it has become the norm in "research
> methodology", and this is essentially why it is provided
> by the standard software packages (otherwise pharmaceutical
> companies would never buy the software, since they need
> this in order to get past the FDA or other regulatory
> authority). However, because this is the "norm", such
> results often have more meaning attributed to them than
> they can support, by people disinclined to delve into
> what "rightly interpreted" might mean.

The statement that frequentist methods are the norm, which I'm afraid is
usually true, is a sad comment on the state of much of "scientific"
inquiry.  IMHO P-values are so defective that the imperfect Bayesian
approach should be seriously entertained.

> 
> This is not a really clean answer to your question; but
> then your question touches on complex and conflicting
> issues!
> 
> Hoping this helps (and hoping that I am not poking a
> hornets' nest here)!
> Ted.
> 

---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University
---
Frank E Harrell Jr   Professor and Chair           School of Medicine
                     Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Wed Apr 28 17:16:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 28 Apr 2004 16:16:11 +0100 (BST)
Subject: [R] coding of categories in rpart
In-Reply-To: <6ED0981F1D12F14AA1ABF568854ECD274CCDB4@exchange>
Message-ID: <Pine.LNX.4.44.0404281612560.25719-100000@gannet.stats>

On Tue, 27 Apr 2004, Prabhakar Krishnamurthy wrote:

> I am using rpart to derive classification rules for customer segments.
> I have a few categorical variables in the set of independent variables.
> For instance,
> 
> Account Size can be (Very-Small, Small, Medium, Large, V-Large)
> 
> Rpart seems to encode these categories into: a,b,c,d,e

It doesn't.  That is one output representation (of several), of the factor
levels.

> The results are expressed in terms of the encoded values.
> 
> How do I find out what encoding was used by rpart.  i.e.
> what categories in my input set do a, b, c,... correspond to?

By reading the documentation!  E.g. ?text.rpart.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Wed Apr 28 17:24:53 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 28 Apr 2004 08:24:53 -0700 (PDT)
Subject: [R] Possible bug in foreign library import of Stata datasets
In-Reply-To: <x2hdv4y1dw.fsf@biostat.ku.dk>
References: <408F4C23.4010600@ku.edu> <x2hdv4y1dw.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.58.0404280810360.18626@homer04.u.washington.edu>

On Wed, 28 Apr 2004, Peter Dalgaard wrote:
>
> Looks like a classic signed/unsigned confusion. Negative numbers
> stored in ones-complement format in single bytes, but getting
> interpreted as unsigned. A bug report could be a good idea if the
> resident Stata expert (Thomas, I believe) is unavailable just now.
>

Yes, it's a simple signed/unsigned bug. We carefully read in the 2-byte
and 4-byte types as unsigned int so we can use << and >> to do byte
swapping, but of course this doesn't work for the 1-byte type.  There's
the additional problem that sometimes we want to read an unsigned byte of
meta-data.

	-thomas



From tlumley at u.washington.edu  Wed Apr 28 17:38:57 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 28 Apr 2004 08:38:57 -0700 (PDT)
Subject: [R] Possible bug in foreign library import of Stata datasets
In-Reply-To: <408F4C23.4010600@ku.edu>
References: <408F4C23.4010600@ku.edu>
Message-ID: <Pine.A41.4.58.0404280837420.18626@homer04.u.washington.edu>

On Wed, 28 Apr 2004, Paul Johnson wrote:
>
> The read.dta has translated the negative values as (256-deml).
>
> Is this the kind of thing that is a bug, or have I missed something in
> the documentation about the handling of negative numbers?  Should a
> formal bug report be filed?

A fixed version of the foreign package has been sent to CRAN.

	-thomas



From iwallace at eforceglobal.com  Wed Apr 28 17:39:58 2004
From: iwallace at eforceglobal.com (Ian Wallace)
Date: Wed, 28 Apr 2004 09:39:58 -0600
Subject: [R] Fedora 1 RPM Packages
In-Reply-To: <Pine.LNX.4.44.0404281407240.25610-100000@gannet.stats>
References: <Pine.LNX.4.44.0404281407240.25610-100000@gannet.stats>
Message-ID: <1083166797.2171.2.camel@localhost.localdomain>

On Wed, 2004-04-28 at 07:12, Prof Brian Ripley wrote:
> I think this is appropriate for most users: if you want to use
> --enable-R-shlib you can and probably should compile R yourself.  (After
> all, you are going to need the development tools for almost all uses of
> libR.so.)  It is not a zero-cost option: it does for example cause shared
> objects in packages to be linked against libR.so not R.bin and so
> increases considerably the run-time footprint.
> 


>From the response that I have had from the list it appears that not many
people use the shared lib from R, and that it wouldn't make sense to
build it that way by default.

I'll drop trying to get this added.  If it's only a handful of us who
are using PL/R (PostgresQL procedural language) we'll have to just
recompile the source RPM.  Which is no big deal.

Thanks for the responses.  Now off to the real work of getting some data
in PostgresQL and taking a look at it!

cheers
ian



> On Tue, 27 Apr 2004, Ian Wallace wrote:
> 
> > Don't know if this is the correct place to post this question however I
> > thought I would start here.  The Fedora 1 packages are built without the
> > option '--enable-R-shlib' turned on in the R.spec file.  Other software,
> > like, plr (the postgresql library that calls R) needs the shared lib. 
> > Any chance that we can change the R.spec file to include:
> > 
> > [iwallace at localhost SPECS]$ diff -bu R.spec R.spec.orig
> > --- R.spec      2004-04-27 15:31:51.000000000 -0600
> > +++ R.spec.orig 2004-04-27 15:31:39.000000000 -0600
> > @@ -36,7 +36,6 @@
> >  %build
> >  export R_BROWSER="/usr/bin/mozilla"
> >  ( %configure \
> > -    --enable-R-shlib \
> >      --with-tcl-config=%{_libdir}/tclConfig.sh \
> >      --with-tk-config=%{_libdir}/tkConfig.sh ) \
> >   | egrep '^R is now|^ |^$' - > CAPABILITIES
> > 
> > I'm not sure if there are other issues with using the shared library,
> > I'm very new to R and just joined the mailing list.
> > 
> > Thanks!
> > ian
> > 
> > 
-- 
Ian Wallace <iwallace at eforceglobal.com>



From plummer at iarc.fr  Wed Apr 28 17:49:51 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 28 Apr 2004 17:49:51 +0200
Subject: [R] installing R on Fedora Core 2 test 2
In-Reply-To: <x21xm978fn.fsf@biostat.ku.dk>
References: <20040427150318.GA19763@psych>  <x21xm978fn.fsf@biostat.ku.dk>
Message-ID: <1083167391.6078.153.camel@xena.iarc.fr>

On Tue, 2004-04-27 at 17:21, Peter Dalgaard wrote:
> Jonathan Baron <baron at psych.upenn.edu> writes:
> 
> > The one thing I cannot figure out is that readline does not
> > work.  It was installed, but apparently not detected.  Grepping
> > config.site for readline gets stuff like this:
> > 
> > configure:21256: checking for rl_callback_read_char in -lreadline
> > configure:21286: gcc -o conftest -g -O2 -I/usr/local/include
> > -L/usr/local/lib conftest.c -lreadline  -ldl -lncurses -lm  >&5
> > /usr/bin/ld: cannot find -lreadline
> 
> Hm? The usual problem is that people forget to install readline-devel,
> but is that the expected error message then? Could you do an rpm -ql
> readline and see where the libreadline stuff went to? If you did
> forget readline-devel, try that first of course.
> 
> BTW, it might be a good idea to see if you can rpm --rebuild from
> Martyn's src.rpm. That gives you your very own .rpm, which you can
> install and eventually upgrade (which is the point) using the standard
> tools.

The RPM also has a patched shell script that sets LANG=C so you don't
get the warning about utf-8 locales not being supported (This may not
worry you, but try checking an R package and you will see it can be a
problem).

You can check the CAPABILITIES file against my rpm for Fedora 1 (which
you can download from CRAN) to make sure you got everything.

I'll be moving to Fedora 2 when it is released.  But the rapid release
schedule of Fedora, combined with lack of binary compatibility between
versions, does pose problems for me as a binary package maintainer. I
think I will only maintain the current and previous releases at any
given time. 

Martyn



From henric.nilsson at statisticon.se  Wed Apr 28 18:06:25 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 28 Apr 2004 18:06:25 +0200
Subject: [R] sas vs r
In-Reply-To: <20040428093934.GB8253@stat.umu.se>
References: <x2ekq95acf.fsf@biostat.ku.dk>
	<Pine.LNX.4.44.0404272213160.21225-100000@muskrat.stat.umn.edu>
	<6.0.3.0.0.20040428110234.0600d598@10.0.10.66>
	<20040428093934.GB8253@stat.umu.se>
Message-ID: <6.0.3.0.0.20040428175923.0602fda0@10.0.10.66>

At 11:39 2004-04-28, G??ran Brostr??m wrote:

> > If I recall correctly, neither glmmML nor glmmPQL (from MASS) handles
> > offset terms. But GLMM in the lme4 package does.
>glmmML handles offset terms.

You're right. I confused it with glmmML's not being able to fit models with 
family = gaussian. Sorry.

>  I am pretty sure that glmmPQL does too.

glmmPQL doesn't complain about the offset term but ignores it. I guess that 
glmmPQL ignores offsets since lme, upon which glmmPQL depends, ignores offsets.

//Henric



From plummer at iarc.fr  Wed Apr 28 18:11:03 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 28 Apr 2004 18:11:03 +0200
Subject: [R] Fedora 1 RPM Packages
In-Reply-To: <1083166797.2171.2.camel@localhost.localdomain>
References: <Pine.LNX.4.44.0404281407240.25610-100000@gannet.stats>
	<1083166797.2171.2.camel@localhost.localdomain>
Message-ID: <1083168662.6078.170.camel@xena.iarc.fr>

>From the response that I have had from the list it appears that not
many
> people use the shared lib from R, and that it wouldn't make sense to
> build it that way by default.

The generic answer to this kind of request is that if a feature isn't
enabled by default, then it doesn't go in the RPM.

Martyn



From v.demartino2 at virgilio.it  Wed Apr 28 18:32:11 2004
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Wed, 28 Apr 2004 18:32:11 +0200
Subject: [R] Slicing an area....
Message-ID: <4087DF450001A1D2@ims3e.cp.tin.it>

I have interpolated a large number of data and obtained a regular x-y curve
(and its interpolated function).
My problem is that I need to slice the entire area delimited by the curve,
the x-axis and the two finite extremes of the curve  (starting from the
very bottom of the entire area) into smaller stripes of given (and different)
areas.

How can I do it?

Vittorio



From wwsprague at ucdavis.edu  Wed Apr 28 18:41:22 2004
From: wwsprague at ucdavis.edu (Webb Sprague)
Date: Wed, 28 Apr 2004 09:41:22 -0700
Subject: [R] label separators  in boxplots with conditioning
Message-ID: <408FDEB2.6010609@ucdavis.edu>

Hi R-helpers,

I have a data.frame with three columns (lots more reps though in each), 
like so:

'FOO'    'BAR'    'RESULT'
1           .01                75
1           .05                12
1.1        .01                100
1.1        .05                50
1.2        .01                75
1.2        .05                12

I am calling boxplot(RESULT ~ FOO:BAR, ...)  This gives me the box plots 
I want, but on the x-axis my labels are "1.01", "1.05", "1.1.01", 
"1.1.05", "1.2.01", "1.2.01".  I would like to separate the factors by 
something other than a dot for obvious reasons.  I would also like to 
*avoid* using the 'names' parameter to boxplot (because I am lazy and 
want a general solution).

Please cc me directly as I read the list on digest

Thanks again to such a helpful list!
W



From spencer.graves at pdf.com  Wed Apr 28 18:56:33 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 28 Apr 2004 09:56:33 -0700
Subject: [R] numericDeriv
In-Reply-To: <Pine.SGI.4.40.0404280918430.12869226-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0404280918430.12869226-100000@origin.chass.utoronto.ca>
Message-ID: <408FE241.9070207@pdf.com>

      optim(..., hessian=TRUE, ...) outputs a list with a component 
hessian, which is the second derivative of the log(likelihood) at the 
minimum.  If your objective function is (-log(likelihood)), then 
optim(..., hessian=TRUE)$hessian is the observed information matrix.  If 
eigen(...$hessian)$values are all positive with at most a few orders of 
magnitude between the largest and smallest, then it is invertable, and 
the square roots of the diagonal elements of the inverse give standard 
errors for the normal approximation to the distribution of parameter 
estimates.  With objective functions that may not always be well 
behaved, I find that optim sometimes stops short of the optimum.  I run 
it with method = "Nelder-Mead", "BFGS", and "CG", then restart the 
algorithm giving the best answer to one of the other algorithms.  Doug 
Bates and Brian Ripley could probably suggest something better, but this 
has produced acceptable answers for me in several cases, and I did not 
push it beyond that. 

      hope this helps. 

Jean Eid wrote:

>Dear All,
>I am trying to solve a Generalized Method of Moments problem which
>necessitate the gradient of moments computation to get the
>standard  errors of estimates.
>I know optim does not output the gradient, but I can use numericDeriv to
>get that. My question is: is this the best function to do this?
>
>Thank you
>Jean,
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From jeaneid at chass.utoronto.ca  Wed Apr 28 19:36:54 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 28 Apr 2004 13:36:54 -0400
Subject: [R] numericDeriv
In-Reply-To: <408FE241.9070207@pdf.com>
Message-ID: <Pine.SGI.4.40.0404281322430.12814104-100000@origin.chass.utoronto.ca>

True, True, However I am not estimating via MLE. The objective function is
bunch of moment conditions weighted according to the uncertainty of the
moment ( i.e. an estimate of the asymptotic Var-Cov matrix of the moments
(not the estimates)) Technically it looks more like a weighted nonlinear
least square problem. I have a bunch of momnets that look like this
                         E(e_{ik} z_i)=0
where e_{ik} is the error term and is a nonlinear function
of  the paramaters at observation i. . z_i is an instrument ( the model
have endogenous covariates). k above indicates that there is more than one
functional form for the residuals (simultaneous equation system that is
nonlinear). one of them look like
 e_{ik}=\ln(p-{1\over \alpha} \Delta^{-1})-W\theta
There are two more.
I am interseted in estimating  \alpha, \theta, (\theta \in R^{k}) in
addition to other paramaters in the other equations.
I only want to use these moment conditions rather than assuming knowledge
of the distribution oof the error term.

At the end of the day, I need to use the delta method to get at an
estimate for the standard errors.

Hope this clarifies some bit more


On Wed, 28 Apr 2004, Spencer Graves wrote:

>       optim(..., hessian=TRUE, ...) outputs a list with a component
> hessian, which is the second derivative of the log(likelihood) at the
> minimum.  If your objective function is (-log(likelihood)), then
> optim(..., hessian=TRUE)$hessian is the observed information matrix.  If
> eigen(...$hessian)$values are all positive with at most a few orders of
> magnitude between the largest and smallest, then it is invertable, and
> the square roots of the diagonal elements of the inverse give standard
> errors for the normal approximation to the distribution of parameter
> estimates.  With objective functions that may not always be well
> behaved, I find that optim sometimes stops short of the optimum.  I run
> it with method = "Nelder-Mead", "BFGS", and "CG", then restart the
> algorithm giving the best answer to one of the other algorithms.  Doug
> Bates and Brian Ripley could probably suggest something better, but this
> has produced acceptable answers for me in several cases, and I did not
> push it beyond that.
>
>       hope this helps.
>
> Jean Eid wrote:
>
> >Dear All,
> >I am trying to solve a Generalized Method of Moments problem which
> >necessitate the gradient of moments computation to get the
> >standard  errors of estimates.
> >I know optim does not output the gradient, but I can use numericDeriv to
> >get that. My question is: is this the best function to do this?
> >
> >Thank you
> >Jean,
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
>
>



From bates at stat.wisc.edu  Wed Apr 28 20:16:04 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 28 Apr 2004 13:16:04 -0500
Subject: [R] [R-pkgs] Release candidate 1 of lme4_0.6-1
Message-ID: <6roepcq87v.fsf@bates4.stat.wisc.edu>

Deepayan Sarkar and I have a source package of release candidate 1 of
the 0.6 series of the lme4 package available at
         http://www.stat.wisc.edu/~bates/lme4_0.6-0-1.tar.gz
This package requires Matrix_0.8-6 which has been uploaded to CRAN and
should be available in a few days.  A copy of the source package is
available as
         http://www.stat.wisc.edu/~bates/Matrix_0.8-6.tar.gz

Although this version of lme4 passes "R CMD check" on our GNU/Linux
systems we have not uploaded it to CRAN because it still lacks
capabilities that are available in lme4_0.5-2, which is currently on
CRAN.  As soon as we have all the capabilities of the 0.5 series
available in the 0.6 series we will release lme4_0.6-1.tar.gz to CRAN.

This version of lme4 is a complete rewrite of the data structures and
algorithms for fitting linear mixed models.  An incomplete draft
version of a paper describing the methods is available as a vignette.
Subsequent releases will contain a more polished version of this
paper.

The big change relative to earlier versions is that you can fit models
with crossed random effects quickly and easily.  For example, using
the data on Scottish secondary school students achievement scores
(from http://multilevel.ioe.ac.uk/softrev/) we can fit a model with
random effects for both the secondary and the primary school attended
as

> library(lme4)
 This package is in development.  For production work use
 lme from package nlme or glmmPQL from package MASS.
> data(ScotsSec)
> fm1 = lme(attain ~ verbal*sex, ScotsSec, random=list(primary=~1,second=~1))
> gc();system.time(lme(attain ~ verbal*sex, ScotsSec, random=list(primary=~1,second=~1)))
         used (Mb) gc trigger (Mb)
Ncells 701438 18.8    1166886 31.2
Vcells 267929  2.1     786432  6.0
[1] 0.1 0.0 0.1 0.0 0.0
> summary(fm1)
Linear mixed-effects model fit by REML
Fixed: formula 
      AIC      BIC    logLik
 14882.32 14925.32 -7434.162

Random effects:
 Groups   Name        Variance Std.Dev.
 primary  (Intercept) 0.275458 0.52484 
 second   (Intercept) 0.014748 0.12144 
 Residual             4.2531   2.0623  

Fixed effects:
              Estimate Std. Error   DF t value Pr(>|t|)    
(Intercept) 5.9147e+00 7.6795e-02 3431 77.0197  < 2e-16 ***
verbal      1.5836e-01 3.7872e-03 3431 41.8136  < 2e-16 ***
sexF        1.2155e-01 7.2413e-02 3431  1.6786  0.09332 .  
verbal:sexF 2.5929e-03 5.3885e-03 3431  0.4812  0.63041    
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Correlation of Fixed Effects:
            (Intr) verbal sexF  
verbal       0.177              
sexF        -0.482 -0.178       
verbal:sexF -0.122 -0.680  0.161

Number of Observations: 3435
Number of Groups: 
primary  second 
    148      19 

There are other examples in the tests subdirectory.  

The lme function behaves as previously *with one exception*.  In the
model specification there is no longer any distinction between crossed
or nested or partially crossed random effects.  This means that for
nested random effects you must ensure that every inner grouping
corresponds to a unique level of the inner grouping factor.  For
example, in the Pixel data there are two grouping factors, Dog and
Side with Side nested within Dog.  You must create a new grouping
factor, say DS, with unique levels for each Dog/Side combination to be
able to specify a model of "Side within Dog".

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From Governor at gov.state.ks.us  Wed Apr 28 21:38:09 2004
From: Governor at gov.state.ks.us (Governor [GO])
Date: Wed, 28 Apr 2004 14:38:09 -0500
Subject: [R] Thank you for your message.
Message-ID: <88874062F82A9540BA47F6CE75A810C2FE365C@daemail01.damail.gov.state.ks.us>

Thank you for your e-mail message to my office. Your views are very important to me and I appreciate you taking the time to share you comments and concerns.

I have asked my staff to respond to only mail that is properly identified. If you live in Kansas and have included your mailing address, you may expect a written reply from my office. Unfortunately, because of a variety of constraints, I cannot respond to you by e-mail at this time. My staff will make every effort to respond as quickly as possible. I do have a website at www.ksgovernor.org which links you to a wealth of information about services available in Kansas. I have posted new releases, position statements, and other information that will keep you advised about my administration. Students wanting to know about Kansas will find helpful facts on that page, too.

To schedule an event/appointment, please submit your request by U.S. mail to: Governor's Office, The Statehouse, 300 SW 10th, Topeka, KS 66612.

If you have a matter that needs immediate attention, I would encourage you to call my office at 1-800-748-4408. Thank you for your interest in my administration.

Sincerely,
Kathleen Sebelius
Governor of the State of Kansas



From stats at psyctc.org  Wed Apr 28 22:29:09 2004
From: stats at psyctc.org (Chris Evans)
Date: Wed, 28 Apr 2004 21:29:09 +0100
Subject: [R] simple repeated measures model: dumb user baffled!
Message-ID: <1695522873.20040428212909@psyctc.org>

I am in the process of transferring from an old version of S+ to using
R having used a variety of other packages in the past.  I'm hugely
impressed with R but it has an excellent but depressing habit of exposing
that I'm not a professional statistician and has done so again.

Someone has run a nice little repeated measures design on my advice,
students randomised to four orders of a condition that can be N or E:
four orders used: NEEN, NENE, ENEN, ENNE, ten students in each block.  I've inherited
the data (in SPSS but I can deal with that!) with variables like:
ID GENDER ORDER C1 C2 C3 C4 RESP1 RESP2 RESP3 RESP4 ...

ORDER is the order as a/b/c/d; C1:C4 has the N or E for each occasion, and
RESP1:RESP4 the response variables.  (There are a number of these but
looking at each separately is justifiable theoretically).

I've had a look around the R help and some S+ books I've got and I
realise I'm seriously out of my depth and my repeated measures ANOVA
knowledge is rusty and very different from the way that more modern
statistics handles such designs.

Can anyone point me to an idiot's guide to the syntax that would help
me test:
a) that there is a change (probably a fall in RESPn) over the four
repeats (probable through a practice effect)
b) whether that shows any sign of higher than linear change
c) whether on top of that, there are N/E differences.

I realise that this is probably trivially easy but I'm staring at all
sorts of wonderful things in Venables & Ripley (S+ 2nd ed.) and in Chambers & Hastie
(S, 1st ed.) but nothing is quite near enough to what I need to help
me overcome my limitations!

TIA,

Chris



From Ted.Harding at nessie.mcc.ac.uk  Wed Apr 28 22:32:11 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 28 Apr 2004 21:32:11 +0100 (BST)
Subject: [R] se.fit in predict.glm
In-Reply-To: <x2oepd5con.fsf@biostat.ku.dk>
Message-ID: <XFMail.040428213211.Ted.Harding@nessie.mcc.ac.uk>

On 27-Apr-04 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
>> The documentation does not say definitely what p$se.fit is,
>> only calling it "Estimated standard errors". I *believe*
>> this means, at each value of X, the SE in the estimation
>> of P[y=1] taking account of the joint uncertainty in the
>> estimation of 'a' and 'b' in the relation
>> 
>>   probit(P) = a + b*X
>> 
>> Can someone confirm that this really is so?
> 
> Pretty accurate, I'd say. 
> 
> Basically, the fitted value is a function of the estimated parameters.
> Asymptotically, the latter are approximately normally distributed with
> a small dispersion so that the function is effectively linear and you
> can approximate the distribution of the fitted value with a normal
> distribution.

Hmm!!
This is a bit of a minefield! I've been fitting a binomial regression
  g <- glm(y~x, family=binomial(link=probit))
followed by a predicition
  p <- predict.glm(g, newx, type="response", se.fit=TRUE)

Plotting the fit p$fit and the inplied "confidence bands"
  p$fit +- 2*p$se.fit
gave rather narrow (I thought) bands for the prediction, so I did
a simulation of 20 mvrnorm draws from the joint distribution of
a and b (using their SDs and correlation from summary.glm).

I then plotted the corresponding curves pnorm(a + b*x) and got
a set of 20 curves about half of which lay well outside the
above "condfidence band", some quite a long way off. (This, I must
say, is what I had intuitively been expecting to find in the first
place!)

I then turned to the V&R book, and happened on the section "Problems
with binomial GLMs" in 7.2; and discovered "profile".

So this led me to "confint" in MASS via ?profile and ?profile.glm.

The results of
  confint(g)
gave me a and b for the lower (2.5%) and upper (97.5%) curve.
When plotted, these curves lay well outside the "confidence
bands" obtained from "predict.glm" and were much more realistically
related to my simulations (19 of my simulated curves nicely packed
the space between tthe two "confint" curves, and one lay just
outside -- couldn't have hoped for a result more close to expectation!).

Nevertheless, I don't think my data were all that few or nasty:

23 x-values, roughly equally spaced, with about 12 0/1 results
at each, and numbers of responses going up as
  0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 2 4 5 8 8 9 4 10

So I tend to conclude that the "predict.glm(...,se.fit=TRUE)"
method should perhaps be avoided in favour of using "confint",
though I see no indication that "confint" respects the covariance
of the parameter estimates (intercept and slope) whereas the
"predict" method in theory does.

Maybe I'll have another go, after centering the x-values at their
mean ...

Anyway, comments would be appreciated!

Best wishe to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 28-Apr-04                                       Time: 21:32:11
------------------------------ XFMail ------------------------------



From spe2 at cornell.edu  Wed Apr 28 23:19:38 2004
From: spe2 at cornell.edu (Stephen Ellner)
Date: Wed, 28 Apr 2004 17:19:38 -0400
Subject: [R] Matrix efficiency in 1.9.0 vs 1.8.1 
Message-ID: <5.2.1.1.2.20040428171741.0256c688@postoffice6.mail.cornell.edu>

I'm seeking some advice on effectively using the new Matrix
library in R1.9.0 for operations with large dense matrices. I'm working on
integral operator models (implemented numerically via matrix operations)
and except for the way entries are generated, the examples below really are 
representative of my problem sizes. 

My main concern is speed of large dense matrix multiplication.
In R 1.8.1 (Windows2000 Professional, dual AthlonMP 2800)  
> a=matrix(rnorm(2500*2500),2500,2500); v=rnorm(2500); 
> system.time(a%*%v);
[1] 0.11 0.00 0.12   NA   NA

In R 1.9.0, same platform:
> a=matrix(rnorm(2500*2500),2500,2500); v=rnorm(2500);
> system.time(a%*%v);
[1] 0.24 0.00 0.25   NA   NA

These differences are consistent. But using the Matrix library 
in 1.9.0, the discrepancy disappears  
> library(Matrix);
> a=Matrix(rnorm(2500*2500),2500,2500); v=Matrix(rnorm(2500),2500,1);
> system.time(a%*%v);
[1] 0.11 0.00 0.11   NA   NA

The problem is 
> b=a/3
Error in a/3 : non-numeric argument to binary operator

which seems to mean that I can't just rewrite code to use Matrix 
instead of matrix objects -- I would have to do lots and lots of
conversions between Matrix and matrix. Am I missing a trick
here somewhere, that would let me use only Matrix objects and do
with them the things one can do with matrix objects? Or some other
way to avoid the twofold speed hit in moving to 1.9? 

I've tried using the Rblas.dll for AthlonXP on CRAN, and 
it doesn't help. 

Thanks in advance, 

Steve 


Stephen P. Ellner (spe2 at cornell.edu)
Department of Ecology and Evolutionary Biology
Corson Hall, Cornell University, Ithaca NY 14853-2701
Phone (607) 254-4221    FAX (607) 255-8088



From p.dalgaard at biostat.ku.dk  Wed Apr 28 23:57:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Apr 2004 23:57:20 +0200
Subject: [R] se.fit in predict.glm
In-Reply-To: <XFMail.040428213211.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040428213211.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2ad0vbwan.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> The results of
>   confint(g)
> gave me a and b for the lower (2.5%) and upper (97.5%) curve.
> When plotted, these curves lay well outside the "confidence
> bands" obtained from "predict.glm" and were much more realistically
> related to my simulations (19 of my simulated curves nicely packed
> the space between tthe two "confint" curves, and one lay just
> outside -- couldn't have hoped for a result more close to expectation!).

Hmm, that doesn't actually hold up mathematically... You cannot just
take upper/lower limits of both parameters and combine them.

 
> Nevertheless, I don't think my data were all that few or nasty:
> 
> 23 x-values, roughly equally spaced, with about 12 0/1 results
> at each, and numbers of responses going up as
>   0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 2 4 5 8 8 9 4 10
> 
> So I tend to conclude that the "predict.glm(...,se.fit=TRUE)"
> method should perhaps be avoided in favour of using "confint",
> though I see no indication that "confint" respects the covariance
> of the parameter estimates (intercept and slope) whereas the
> "predict" method in theory does.
> 
> Maybe I'll have another go, after centering the x-values at their
> mean ...

Shouldn't change anything (except maybe demonstrate the fallacy of
your calculation above -- lower b's give higher p's when x is negative).

> Anyway, comments would be appreciated!

I don't seem to get anything that drastic. Things look somewhat better
if you use the link-scale estimates, but the response-scale curves are
not hopeless. You do have to notice that these are pointwise CIs so
having multiple curves straying outside is not necessarily a problem.

Just as a sanity check, did your plots look anything like the below:

y <- c(0,0,0,0,0,0,0,0,0,0,2,0,0,1,0,2,4,5,8,8,9,4,10)
x <- 1:23
d <- data.frame(x,y,n=12)
m1 <- glm(cbind(y,n-y) ~ x,data=d,family=binomial(probit))
confint(m1) # +-2SE approximation
library(MASS)
confint(m1) # profiling
x1 <- with(predict(m1,se.fit=TRUE,type="response"),
          fit+outer(se.fit,c(l=-2,u=2)))
x2 <- with(predict(m1,se.fit=TRUE,type="link"),
          fit+outer(se.fit,c(l=-2,u=2)))
x2 <- pnorm(x2)

ab <- mvrnorm(20,coef(m1),vcov(m1))
matplot(x2,type="l",col="black",lwd=3,lty=1)
matlines(x1,type="l",col="red",lwd=3,lty=1)
matlines(pnorm(t(ab%*%rbind(1,x))))



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Thu Apr 29 00:21:41 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 28 Apr 2004 17:21:41 -0500
Subject: [R] Matrix efficiency in 1.9.0 vs 1.8.1
In-Reply-To: <5.2.1.1.2.20040428171741.0256c688@postoffice6.mail.cornell.edu>
References: <5.2.1.1.2.20040428171741.0256c688@postoffice6.mail.cornell.edu>
Message-ID: <6rllkfpwui.fsf@bates4.stat.wisc.edu>

Stephen Ellner <spe2 at cornell.edu> writes:

> I'm seeking some advice on effectively using the new Matrix
> library in R1.9.0 for operations with large dense matrices. I'm working on
> integral operator models (implemented numerically via matrix operations)
> and except for the way entries are generated, the examples below really are 
> representative of my problem sizes. 
> 
> My main concern is speed of large dense matrix multiplication.
> In R 1.8.1 (Windows2000 Professional, dual AthlonMP 2800)  
> > a=matrix(rnorm(2500*2500),2500,2500); v=rnorm(2500); 
> > system.time(a%*%v);
> [1] 0.11 0.00 0.12   NA   NA
> 
> In R 1.9.0, same platform:
> > a=matrix(rnorm(2500*2500),2500,2500); v=rnorm(2500);
> > system.time(a%*%v);
> [1] 0.24 0.00 0.25   NA   NA
> 
> These differences are consistent. But using the Matrix library 
> in 1.9.0, the discrepancy disappears  
> > library(Matrix);
> > a=Matrix(rnorm(2500*2500),2500,2500); v=Matrix(rnorm(2500),2500,1);
> > system.time(a%*%v);
> [1] 0.11 0.00 0.11   NA   NA
> 
> The problem is 
> > b=a/3
> Error in a/3 : non-numeric argument to binary operator
> 
> which seems to mean that I can't just rewrite code to use Matrix 
> instead of matrix objects -- I would have to do lots and lots of
> conversions between Matrix and matrix. Am I missing a trick
> here somewhere, that would let me use only Matrix objects and do
> with them the things one can do with matrix objects? Or some other
> way to avoid the twofold speed hit in moving to 1.9? 

The trick is waiting for the author of the Matrix package to write the
methods for arithmetic operations or contributing said methods
yourself. :-)

Actually I want to at least e-discuss the implementation with John
Chambers and other members of the R Development Core Team before doing
much more implementation.  There are some subtle issues about how to
arrange the classes and this is usually the point where John can
provide invaluable guidance.



From iwallace at eforceglobal.com  Thu Apr 29 00:39:37 2004
From: iwallace at eforceglobal.com (Ian Wallace)
Date: Wed, 28 Apr 2004 16:39:37 -0600
Subject: [R] Fedora 1 RPM Packages
In-Reply-To: <1083168662.6078.170.camel@xena.iarc.fr>
References: <Pine.LNX.4.44.0404281407240.25610-100000@gannet.stats>
	<1083166797.2171.2.camel@localhost.localdomain>
	<1083168662.6078.170.camel@xena.iarc.fr>
Message-ID: <1083191976.10696.1.camel@localhost.localdomain>

That makes sense since most people will build with the defaults and
that's what we'd like packages for the main stream.

I'm already working around it ... but thought I would mention that some
of us do like the shared library.  I hadn't even given any thought
though to the memory foot print and assumed it would be roughly the
same, I never checked though.

cheers
ian


On Wed, 2004-04-28 at 10:11, Martyn Plummer wrote:
> >From the response that I have had from the list it appears that not
> many
> > people use the shared lib from R, and that it wouldn't make sense to
> > build it that way by default.
> 
> The generic answer to this kind of request is that if a feature isn't
> enabled by default, then it doesn't go in the RPM.
> 
> Martyn
-- 
Ian Wallace <iwallace at eforceglobal.com>



From kjetil at acelerate.com  Thu Apr 29 00:44:55 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Wed, 28 Apr 2004 18:44:55 -0400
Subject: [R] plot.ts
In-Reply-To: <loom.20040428T024402-344@post.gmane.org>
Message-ID: <408FFBA7.25032.5FB99D@localhost>

On 28 Apr 2004 at 0:46, Gabor Grothendieck wrote:

>  <kjetil <at> acelerate.com> writes:
> > I have problems getting sensible series name plotted
> > with the ts.plot function. It doesn't seem to 
> > use either ylab= or xy.labels= arguments. 
> > I ended up using
> > 
> > plot({arg <- ts.union(gasolina.eq, PIBmensPred, PIBgrowthmens) ;
> >        colnames(arg) <- c("Gaso" ,"PIB", "PIBgrowth");arg },
> >    main="Gasolina eq. con crecimiento Economico", 
> >    xlab="Tiempo")
> 
> plot( 
>   ts.union(Gaso = gasolina.eq, PIB = PIBmensPred, PIBgrowth =
>   PIBgrowthmens), main = "Gasolina eq. con crecimiento Economico",
>   xlab = "Tiempo" )
> 

Thanks! Then I can avoid the variable labels at all, if I want, by 
using backticks:

 plot( 
   ts.union(` ` = gasolina.eq, `  ` = PIBmensPred, `   ` =
   PIBgrowthmens), main = "Gasolina eq. con crecimiento Economico",
   xlab = "Tiempo" )


However, with plot.type="s", whatever I do, I get the complete x 
argument as y-label, no way to avoid it. Any solution (apart from 
hacking the code, which I am about to do)?

Kjetil Halvorsen



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Thu Apr 29 00:44:55 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Wed, 28 Apr 2004 18:44:55 -0400
Subject: [R] Rtemp directories accumulating over time
In-Reply-To: <Pine.LNX.4.44.0404281347060.25610-100000@gannet.stats>
References: <408EBC58.3302.1CA60BA@localhost>
Message-ID: <408FFBA7.18035.5FB901@localhost>

On 28 Apr 2004 at 13:53, Prof Brian Ripley wrote:

> On Tue, 27 Apr 2004 kjetil at acelerate.com wrote:
> 

.
.
> I don't see why: package base is never unloaded so that hook function
> is never run.  (Indeed, no package/namespace is unloaded except by
> explicit user action, in particular not when R is terminated.)
> 
> > So there are also other tmpdirs made by R. Why, where, and why are
> > they not removed at exit (when their content are removed)?
> 
> They are removed by R.  This is a Windows-only bug, as Windows
> sometimes does not act on commands to remove empty directories (but
> only sometimes).

So is there anything I can do to remedy this nuisance (apart from 
reinstalling a newer XP (other messages indicates that can give 
surprises!) or changing OS)?

Kjetil Halvorsen

 Session temporary directories should only be left
> around when a session crashes.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From hodgess at gator.uhd.edu  Thu Apr 29 01:05:56 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 28 Apr 2004 18:05:56 -0500
Subject: [R] [R ESS]
Message-ID: <200404282305.i3SN5uV04652@gator.dt.uh.edu>

Dear ESS/R People:

I installed ESS as per the directions on the ESS page from
the R-Gui Page.

When I start Xemacs/ESS, the scratch window comes up but
no R.  Also, the special function button do not come up.

Any clues as to what I'm doing wrong, please?
R for Windows


Thanks in advance,
Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 29 00:54:20 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 28 Apr 2004 23:54:20 +0100 (BST)
Subject: [R] se.fit in predict.glm
In-Reply-To: <x2ad0vbwan.fsf@biostat.ku.dk>
Message-ID: <XFMail.040428235420.Ted.Harding@nessie.mcc.ac.uk>

On 28-Apr-04 Peter Dalgaard wrote:
> Hmm, that doesn't actually hold up mathematically... You cannot just
> take upper/lower limits of both parameters and combine them.

Yes, excuse me. I had misunderstood what 'confint' does -- I misled
myself into thinking that it gave be the 'a' and 'b' for the
lower and the upper curve!

> I don't seem to get anything that drastic. Things look somewhat better
> if you use the link-scale estimates, but the response-scale curves are
> not hopeless. You do have to notice that these are pointwise CIs so
> having multiple curves straying outside is not necessarily a problem.
> 
> Just as a sanity check, did your plots look anything like the below:

Yes, that's useful -- and it also brought to the surface the other
error I'd made whereby a slip of eyesight caused me to transcribe
a correlation of -0.91 as 0.91, so I was simulating from the
wrong distribution ...

> y <- c(0,0,0,0,0,0,0,0,0,0,2,0,0,1,0,2,4,5,8,8,9,4,10)
> x <- 1:23
> d <- data.frame(x,y,n=12)
> m1 <- glm(cbind(y,n-y) ~ x,data=d,family=binomial(probit))
> confint(m1) # +-2SE approximation
> library(MASS)
> confint(m1) # profiling
> x1 <- with(predict(m1,se.fit=TRUE,type="response"),
>           fit+outer(se.fit,c(l=-2,u=2)))
> x2 <- with(predict(m1,se.fit=TRUE,type="link"),
>           fit+outer(se.fit,c(l=-2,u=2)))
> x2 <- pnorm(x2)
> 
> ab <- mvrnorm(20,coef(m1),vcov(m1))
> matplot(x2,type="l",col="black",lwd=3,lty=1)
> matlines(x1,type="l",col="red",lwd=3,lty=1)
> matlines(pnorm(t(ab%*%rbind(1,x))))

Yes, fairly similar when I do it right. Thanks for the above
code. I'd also overlooked 'vcov' etc and was copying SEs and
correlations from the output of 'summary.glm'.

The main lesson: one shouldn't stay up too late at these things.
Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 28-Apr-04                                       Time: 23:54:20
------------------------------ XFMail ------------------------------



From hodgess at gator.uhd.edu  Thu Apr 29 01:15:50 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 28 Apr 2004 18:15:50 -0500
Subject: [R] [R ESS] please disregard prev. message
Message-ID: <200404282315.i3SNFoC05786@gator.dt.uh.edu>

Wrong name in init.el 

Sorry,
Erin



From ggrothendieck at myway.com  Thu Apr 29 01:09:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 28 Apr 2004 23:09:20 +0000 (UTC)
Subject: [R] plot.ts
References: <loom.20040428T024402-344@post.gmane.org>
	<408FFBA7.25032.5FB99D@localhost>
Message-ID: <loom.20040429T010750-163@post.gmane.org>

 <kjetil <at> acelerate.com> writes:
:  plot( 
:    ts.union(` ` = gasolina.eq, `  ` = PIBmensPred, `   ` =
:    PIBgrowthmens), main = "Gasolina eq. con crecimiento Economico",
:    xlab = "Tiempo" )
: 
: However, with plot.type="s", whatever I do, I get the complete x 
: argument as y-label, no way to avoid it. Any solution (apart from 
: hacking the code, which I am about to do)?

par(ann=F)



From jasont at indigoindustrial.co.nz  Thu Apr 29 01:15:18 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 29 Apr 2004 11:15:18 +1200
Subject: [R] [R ESS]
In-Reply-To: <200404282305.i3SN5uV04652@gator.dt.uh.edu>
References: <200404282305.i3SN5uV04652@gator.dt.uh.edu>
Message-ID: <40903B06.9040403@indigoindustrial.co.nz>

Erin Hodgess wrote:

> Dear ESS/R People:
> 
> I installed ESS as per the directions on the ESS page from
> the R-Gui Page.
> 
> When I start Xemacs/ESS, the scratch window comes up but
> no R.  Also, the special function button do not come up.

If the R bin folder is on your path (something like c:\Program 
Files\R\rw1091\bin), and ess is loaded, then it should be as simple as 
M-x R (I typically type ESC x R <enter>, because "Alt" is in an awkward 
place on my laptop).  If that doesn't work, then ess probably isn't 
being loaded.  Check the docs for recommended settings on the 
.xemacs\init.el file.

Jason



From hodgess at gator.uhd.edu  Thu Apr 29 01:40:10 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Wed, 28 Apr 2004 18:40:10 -0500
Subject: [R] [R ESS] one more thing.....
Message-ID: <200404282340.i3SNeAA09722@gator.dt.uh.edu>

Ok.
R loads up.
But when I try to use 
Ctrl-x-o or any control/escape key
in the minibuffer, 
I get
XEmacs does not own the primary selection

Any ideas, please?
Thanks yet again,
Erin
mailto: hodgess at gator.uhd.edu



From klealambrou at hotmail.com  Thu Apr 29 03:55:46 2004
From: klealambrou at hotmail.com (klea lambrou)
Date: Thu, 29 Apr 2004 01:55:46 +0000
Subject: [R] (no subject)
Message-ID: <BAY16-F19ZjfNLnlyg30000ecd5@hotmail.com>



From Alexander.Herr at csiro.au  Thu Apr 29 04:20:44 2004
From: Alexander.Herr at csiro.au (Alexander.Herr@csiro.au)
Date: Thu, 29 Apr 2004 12:20:44 +1000
Subject: [R] R-crash using read.shape (maptools)
Message-ID: <062AE320EF971E40ACD0F6C93391D7690A5361@exqld1-tsv.nexus.csiro.au>

Hi List,

I am trying to read a large shapefile (~37,000 polys) using read.shape [winxp, 1gig ram, dellbox). I receive the following error:

AppName: rgui.exe	 AppVer: 1.90.30412.0	 ModName: maptools.dll
ModVer: 1.90.30412.0	 Offset: 0000309d

The getinfo.shape returns info, and the shapefile is readable in arcmap. 

Any ideas on how to overcome this?

Thanks Herry

-------------------------------------------
Alexander Herr - Herry
--------------------------------------------
 



From dsheuman at rogers.com  Thu Apr 29 04:49:56 2004
From: dsheuman at rogers.com (Danny Heuman)
Date: Wed, 28 Apr 2004 22:49:56 -0400
Subject: [R] R-crash using read.shape (maptools)
In-Reply-To: <062AE320EF971E40ACD0F6C93391D7690A5361@exqld1-tsv.nexus.csiro.au>
References: <062AE320EF971E40ACD0F6C93391D7690A5361@exqld1-tsv.nexus.csiro.au>
Message-ID: <r8r090hplh7hgm2rut6hgs1cpb1c78m947@4ax.com>

Hi Herry,

On Thu, 29 Apr 2004 12:20:44 +1000, you wrote:

>Hi List,
>
>I am trying to read a large shapefile (~37,000 polys) using read.shape [winxp, 1gig ram, dellbox). I receive the following error:
>
>AppName: rgui.exe	 AppVer: 1.90.30412.0	 ModName: maptools.dll
>ModVer: 1.90.30412.0	 Offset: 0000309d
>
>The getinfo.shape returns info, and the shapefile is readable in arcmap. 
>
>Any ideas on how to overcome this?
>
>Thanks Herry
>
>-------------------------------------------
>Alexander Herr - Herry
>--------------------------------------------
> 
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


I've had difficulty if there is too much detail in the polygon
definition (i.e. too many nodes).  Try thinning the polygons and try
again.

Danny



From fungile at yahoo.com  Thu Apr 29 06:41:21 2004
From: fungile at yahoo.com (K Fung)
Date: Wed, 28 Apr 2004 21:41:21 -0700 (PDT)
Subject: [R] outer
Message-ID: <20040429044121.34857.qmail@web50410.mail.yahoo.com>

Hello,

Can anyone help explain why the following are not
equivalent?

I have written a function called cord.mag(x,y) which
takes two numbers and outputs a number.

Further I defined m=1:5, n=1:26

>  for(i in m) { for (j in n) print(cord.mag(i,j))}

this prints the m*n values, one on each line properly

> outer(m,n,cord.mag)

this gives me a matrix of zeroes

> outer(1,2,cord.mag)

this gives the right value on the other hand

Thanks



From john.maindonald at anu.edu.au  Thu Apr 29 06:49:26 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 29 Apr 2004 14:49:26 +1000
Subject: [R] p-values
In-Reply-To: <200404281007.i3SA6eWm029623@hypatia.math.ethz.ch>
References: <200404281007.i3SA6eWm029623@hypatia.math.ethz.ch>
Message-ID: <975B1B9A-9998-11D8-BD27-000A95CDA0F2@anu.edu.au>

This is, of course, not strictly about R.  But if there should be
a decision to pursue such matters on this list, then we'd need
another list to which such discussion might be diverted.

I've pulled Frank's "Regression Modeling Stratregies" down
from my shelf and looked to see what he says about
inferential issues.  There is a suggestion, in the introduction,
that modeling provides the groundwork that can be used a
point of departure for a variety of inferential interpretations.
As far as I can see Bayesian interpretations are never
really explicitly discussed, though the word Bayesian does
appear in a couple of places in the text.  Frank, do you now
have ideas on how you would (perhaps, in a future edition,
will) push the discussion in a more overtly Bayesian direction?
What might be the style of a modeling book, aimed at practical
data analysts who of necessity must (mostly, at least) use
off-the-shelf software, that "seriously entertains" the Bayesian
approach?

R provides a lot of help for those who want a frequentist
interpretation, even to including by default the *, **, ***
labeling that some of us deplore.  There is no similar help
for those who want at least the opportunity to place the
output from a modeling exercise in a Bayesian context of
some description.  There is surely a strong argument for
the use of a more neutral form of default output, even to
the excluding of p-values, on the argument that they also
push too strongly in the direction of a frequentist
interpretative framework.

There seems, unfortunately, to be a dearth of good ideas
on how the assist the placing of output from modeling
functions such as R provides in an explicitly Bayesian
framework.  Or is it, at least in part, that I am unaware of
what is out there? That, I guess, is the point of my
question to Frank.  Is it just too technically demanding
to go much beyond trying to get users to understand
that a Bayesian credible interval can, if there is an
informative prior, be very different from a frequentist CI,
that they really do need to pause if there is an
informative prior lurking somewhere in the undergrowth?

John Maindonald.

Frank Harrell wrote:

> They [p-values] are objective only in the sense that
> subjectivity is deferred in a difficult to document way
> when P-values are translated into decisions.


> The statement that frequentist methods are the norm, which I'm
> afraid is usually true, is a sad comment on the state of much
> of "scientific" inquiry.  IMHO P-values are so defective that
> the imperfect Bayesian approach should be seriously entertained.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From jadhavpr at vcu.edu  Thu Apr 29 03:33:24 2004
From: jadhavpr at vcu.edu (Pravin)
Date: Wed, 28 Apr 2004 21:33:24 -0400
Subject: [R] randomization test
Message-ID: <000001c42d89$f6ee7a50$0800a8c0@Pravin>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040428/9689571b/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 29 08:21:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 07:21:37 +0100 (BST)
Subject: [R] Rtemp directories accumulating over time
In-Reply-To: <408FFBA7.18035.5FB901@localhost>
Message-ID: <Pine.LNX.4.44.0404290720540.11928-100000@gannet.stats>

On Wed, 28 Apr 2004 kjetil at acelerate.com wrote:

> On 28 Apr 2004 at 13:53, Prof Brian Ripley wrote:
> 
> > On Tue, 27 Apr 2004 kjetil at acelerate.com wrote:
> > 
> 
> .
> .
> > I don't see why: package base is never unloaded so that hook function
> > is never run.  (Indeed, no package/namespace is unloaded except by
> > explicit user action, in particular not when R is terminated.)
> > 
> > > So there are also other tmpdirs made by R. Why, where, and why are
> > > they not removed at exit (when their content are removed)?
> > 
> > They are removed by R.  This is a Windows-only bug, as Windows
> > sometimes does not act on commands to remove empty directories (but
> > only sometimes).
> 
> So is there anything I can do to remedy this nuisance (apart from 
> reinstalling a newer XP (other messages indicates that can give 
> surprises!) or changing OS)?

Well, there is nothing I know how to do.  Someone else may know how to 
workaround the Windows bug.

> 
> Kjetil Halvorsen
> 
>  Session temporary directories should only be left
> > around when a session crashes.
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> > Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> > UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Apr 29 08:35:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 07:35:47 +0100 (BST)
Subject: [R] Fedora 1 RPM Packages
In-Reply-To: <1083191976.10696.1.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0404290729270.11928-100000@gannet.stats>

On Wed, 28 Apr 2004, Ian Wallace wrote:

> That makes sense since most people will build with the defaults and
> that's what we'd like packages for the main stream.
> 
> I'm already working around it ... but thought I would mention that some
> of us do like the shared library.  I hadn't even given any thought
> though to the memory foot print and assumed it would be roughly the
> same, I never checked though.

It would be nice if R used its shared library itself, as it does on 
Windows.  However,

- on some platforms we cannot build the shared library at all (since some 
of the crucial system routines, usually the Fortran run time, are not 
PIC, or BLAS and other libraries are not shared).

- on some platforms there is an appreciable performance hit (I have seen 
reports of 5-10%).

We don't have the resources to build and check different versions of R for 
different platforms.

> 
> cheers
> ian
> 
> 
> On Wed, 2004-04-28 at 10:11, Martyn Plummer wrote:
> > >From the response that I have had from the list it appears that not
> > many
> > > people use the shared lib from R, and that it wouldn't make sense to
> > > build it that way by default.
> > 
> > The generic answer to this kind of request is that if a feature isn't
> > enabled by default, then it doesn't go in the RPM.
> > 
> > Martyn
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Meredith.Briggs at team.telstra.com  Thu Apr 29 08:53:53 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Thu, 29 Apr 2004 16:53:53 +1000
Subject: [R] I'm trying to use package ts (decompose). How do you set up the
	data/ See attached. thanks
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35483@ntmsg0092.corpmail.telstra.com.au>


InDATA <-read.table("C:/Data/May 2004/season.txt",header=T)

X <- decompose(InDATA)
print(X)

Period	Connections	
Q1	67519	
Q2	69713	
Q3	68920	
Q4	69452	
Q1	70015	
Q2	59273	
Q3	57063	
Q4	65596	
Q1	73527	
Q2	58586	
Q3	69522	
Q4	60091	
Q1	51686	
Q2	63490	
Q3	55702	
Q4	53200	
Q1	51033	
Q2	48175	
Q3	52709	
Q4	50106	
Q1	50855	
Q2	43466	
Q3	48190	
Q4	41702	
Q1	48747	
Q2	51441	
Q3	42537	
Q4	49145	
Q1	41457	
Q2	39306	
Q3	43121	
Q4	42777	
Q1	40631	
Q2	41764	
Q3	44712	
Q4	38669	
Q1	41367	
Q2	37515	
Q3	36201	
Q4	37650	
Q1	35963	
Q2	38097	
Q3	39854	
Q4	38307	
Q1	39383	
Q2	37015	
Q3	46416	
Q4	40359	
Q1	38098	
Q2	31419	
Q3	31457	
Q4	32817	
Q1	29141	
Q2	31210	
Q3	29555	
Q4	31102	
Q1	32147	
Q2	29351	
Q3	33982	
Q4	29781	
Q1	32134	
Q2	30048	
Q3	27811	
Q4	30126	
Q1	26934	
Q2	27758	
Q3	28372	
Q4	28631	
Q1	29102	
Q2	28372	
Q3	33295	
Q4	29182	
Q1	30343	
Q2	26944	
Q3	26115	
Q4	27551	
Q1	25303	
Q2	26854	
Q3	26911	
Q4	27225	
Q1	28141	
Q2	26710	
Q3	32192	
Q4	28281	
Q1	28958	
Q2	25624	
Q3	24861	
Q4	26459	
Q1	23941	
Q2	25578	
Q3	25281	
Q4	26070	
Q1	26881	
Q2	25496	
Q3	30128	
Q4	26471



From Roger.Bivand at nhh.no  Thu Apr 29 09:04:18 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 29 Apr 2004 09:04:18 +0200 (CEST)
Subject: [R] R-crash using read.shape (maptools)
In-Reply-To: <r8r090hplh7hgm2rut6hgs1cpb1c78m947@4ax.com>
Message-ID: <Pine.LNX.4.44.0404290857070.5980-100000@reclus.nhh.no>

On Wed, 28 Apr 2004, Danny Heuman wrote:

> Hi Herry,
> 
> On Thu, 29 Apr 2004 12:20:44 +1000, you wrote:
> 
> >Hi List,
> >
> >I am trying to read a large shapefile (~37,000 polys) using read.shape [winxp, 1gig ram, dellbox). I receive the following error:
> >
> >AppName: rgui.exe	 AppVer: 1.90.30412.0	 ModName: maptools.dll
> >ModVer: 1.90.30412.0	 Offset: 0000309d
> >
> >The getinfo.shape returns info, and the shapefile is readable in arcmap. 
> >
> >Any ideas on how to overcome this?
> >
> >Thanks Herry
> >
> >-------------------------------------------
> >Alexander Herr - Herry
> >--------------------------------------------
> > 
> 
> I've had difficulty if there is too much detail in the polygon
> definition (i.e. too many nodes).  Try thinning the polygons and try
> again.
> 
> Danny
> 

I think that this is fair advice. The underlying shapelib code may not 
scale well to very many, very intricate polygons, and may not handle 
malformed polygons gracefully. Arcgis typically both reads and writes its 
own format quite uncritically. If this really needs to be solved, please 
contact me off-list with a smaller reproducible sample and information on 
the platform you are using (looks like windows (XP?), R 1.9.0, but which 
maptools?

Roger Bivand

Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From salmon.jacques at free.fr  Thu Apr 29 09:09:27 2004
From: salmon.jacques at free.fr (Famille SALMON)
Date: Thu, 29 Apr 2004 09:09:27 +0200
Subject: [R] problem with pca
Message-ID: <20040429070933.35A77C4366@postfix3-1.free.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/1119ab47/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Apr 29 09:10:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Apr 2004 09:10:40 +0200
Subject: [R] outer
In-Reply-To: <20040429044121.34857.qmail@web50410.mail.yahoo.com>
References: <20040429044121.34857.qmail@web50410.mail.yahoo.com>
Message-ID: <x2fzan5ken.fsf@biostat.ku.dk>

K Fung <fungile at yahoo.com> writes:

> > outer(m,n,cord.mag)
> 
> this gives me a matrix of zeroes
> 
> > outer(1,2,cord.mag)
> 
> this gives the right value on the other hand

Your cord.mag function probably doesn't vectorize. Try

Cord.mag <- function(m,n) mapply(cord.mag, m, n)
outer(m, n, Cord.mag)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From STEL at glostruphosp.kbhamt.dk  Thu Apr 29 09:22:52 2004
From: STEL at glostruphosp.kbhamt.dk (Ladelund, Steen)
Date: Thu, 29 Apr 2004 09:22:52 +0200
Subject: [R] Sweave in makefile under Win XP
Message-ID: <F9E47473E3BCD1118C0500204808C390061A0840@GLO_003>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/9efafc70/attachment.pl

From Murray.Keir at macquarie.com  Thu Apr 29 09:24:33 2004
From: Murray.Keir at macquarie.com (Murray Keir)
Date: Thu, 29 Apr 2004 17:24:33 +1000
Subject: [R] accessing information in lists
Message-ID: <0638B6AA309DA441AA45CC45DB77954602E9992B@nt_syd_exm02.pc.internal.macquarie.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/ec93adee/attachment.pl

From ripley at stats.ox.ac.uk  Thu Apr 29 09:48:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 08:48:07 +0100 (BST)
Subject: [R] Sweave in makefile under Win XP
In-Reply-To: <F9E47473E3BCD1118C0500204808C390061A0840@GLO_003>
Message-ID: <Pine.LNX.4.44.0404290844380.12126-100000@gannet.stats>

On Thu, 29 Apr 2004, Ladelund, Steen wrote:

> Hi all.
>  
> To automate the production of a document I want to have something like:
>  
> test.tex : test.Rnw
>     "Sweave("test.Rnw") "| R  --no-restore --no-save
>  
> in a makefile.
>  
> This however gives an error as R states: Sweave("test.Rnw") : not found.
>  
> I use R-1.9.0pat (040424 I think) on Win XP.
>  
> Any suggestions, hints or plain help appreciated.

Look at the examples in the makefiles in the R sources, and beware of 
shell quoting.  So

echo "Sweave('test.Rnw')" | Rterm.exe  --vanilla --slave

may be the simplest.  (I would avoid using `R' in programming as it may 
well find the wrong thing one day.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Apr 29 09:45:27 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Apr 2004 09:45:27 +0200
Subject: [R] Sweave in makefile under Win XP
In-Reply-To: <F9E47473E3BCD1118C0500204808C390061A0840@GLO_003>
References: <F9E47473E3BCD1118C0500204808C390061A0840@GLO_003>
Message-ID: <x2brlb5iso.fsf@biostat.ku.dk>

"Ladelund, Steen" <STEL at glostruphosp.kbhamt.dk> writes:

> Hi all.
>  
> To automate the production of a document I want to have something like:
>  
> test.tex : test.Rnw
>     "Sweave("test.Rnw") "| R  --no-restore --no-save
>  
> in a makefile.
>  
> This however gives an error as R states: Sweave("test.Rnw") : not found.

If that's what you did, I don't think R ever saw it. The shell would
think that you were trying run a program called "Sweave(test.Rnw) ".
If you want to pipe the string to R, you need at least an "echo" in
front of it. Also beware that the quotes as written don't match up the
way I think you think they do.

echo "Sweave(\"test.Rnw\")"| R  --no-restore --no-save
  
should be more like it. Notice that you need to start the line with a
TAB inside a makefile, not 4 spaces (or did your email program just
mangle it?).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From s-plus at wiwi.uni-bielefeld.de  Thu Apr 29 10:04:29 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 29 Apr 2004 10:04:29 +0200
Subject: [R] accessing information in lists
References: <0638B6AA309DA441AA45CC45DB77954602E9992B@nt_syd_exm02.pc.internal.macquarie.com>
Message-ID: <4090B70D.40706@wiwi.uni-bielefeld.de>

Try:

<<*>>=
rev.matrix<-matrix(rnorm(40),10,4)
rev.acf<-apply(rev.matrix, 2, acf, na.action=na.contiguous, lag.max=12, 
plot=FALSE)
x.acf<-lapply(rev.acf,function(x) x[["acf"]][,,1])
matrix(unlist(x.acf),ncol=4)

output-start
Thu Apr 29 09:56:22 2004
             [,1]         [,2]        [,3]        [,4]
 [1,]  1.00000000  1.000000000  1.00000000  1.00000000
 [2,] -0.18009242 -0.066131687 -0.19268167 -0.25340270
 [3,] -0.15442666 -0.342732543 -0.30959136 -0.11987720
 [4,]  0.14785268  0.074751553  0.16748753 -0.16495692
 [5,] -0.34024505  0.210414077 -0.03338385  0.25205651
 [6,] -0.05562243  0.044047730  0.02971677 -0.05640866
 [7,]  0.09955468 -0.358754179 -0.02301302 -0.11177397
 [8,]  0.01609530  0.072731988 -0.16732203 -0.25562259
 [9,] -0.04621386 -0.005510738 -0.13049985  0.23890998
[10,]  0.01309776 -0.128816203  0.15928749 -0.02892446
output-end

BTW -- do not use  the name  t  for a variable, parameter, etc. because
there is a function  t().

Peter Wolf

Murray Keir wrote:

>I've created a dataframe containing multiple ACF lists through the command
>
>rev.acf<-apply(rev.matrix, 2, acf, na.action=na.contiguous, lag.max=12, plot=FALSE)
>
>where rev.matrix is an n by t matrix containing n time series in columns.  I'd now like to pull out only the ACF information and store it in a seperate n by 12 matrix.  So far the only way I can work out to access this is 
>
>rev.acf[["series1"]][["acf"]]
>rev.acf[["series2"]][["acf"]]
>etc
>
>Is there some way I can do this automatically?
>
>Thanks
>Murray
>
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From maechler at stat.math.ethz.ch  Thu Apr 29 10:07:09 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Apr 2004 10:07:09 +0200
Subject: [R] Matrix efficiency in 1.9.0 vs 1.8.1 
In-Reply-To: <5.2.1.1.2.20040428171741.0256c688@postoffice6.mail.cornell.edu>
References: <5.2.1.1.2.20040428171741.0256c688@postoffice6.mail.cornell.edu>
Message-ID: <16528.47021.237691.186@gargle.gargle.HOWL>

>>>>> "Stephen" == Stephen Ellner <spe2 at cornell.edu>
>>>>>     on Wed, 28 Apr 2004 17:19:38 -0400 writes:

    Stephen> I'm seeking some advice on effectively using the
    Stephen> new Matrix library in R1.9.0 for operations with
    Stephen> large dense matrices. I'm working on integral
    Stephen> operator models (implemented numerically via matrix
    Stephen> operations) and except for the way entries are
    Stephen> generated, the examples below really are
    Stephen> representative of my problem sizes.

    Stephen> My main concern is speed of large dense matrix
    Stephen> multiplication.  In R 1.8.1 (Windows2000
    Stephen> Professional, dual AthlonMP 2800)
    >> a=matrix(rnorm(2500*2500),2500,2500); v=rnorm(2500);
    >> system.time(a%*%v);
    Stephen> [1] 0.11 0.00 0.12 NA NA

    Stephen> In R 1.9.0, same platform:
    >> a=matrix(rnorm(2500*2500),2500,2500); v=rnorm(2500);
    >> system.time(a%*%v);
    Stephen> [1] 0.24 0.00 0.25 NA NA

    <... and then you talk about the  
	 Matrix  **package** (not `library')
     ...>

Unfortunately, the 1.9.0 vs. 1.8.1 performance comparison
is not just on your computer/OS/R compilation/... version,
but I see the same phenomenon on my Linux and Solaris clients,
e.g.,

  > n <- 2500; set.seed(1); a <- matrix(rnorm(n * n), n); v <- rnorm(n)
  > gc()
	    used (Mb) gc trigger  (Mb)
  Ncells  435805 11.7     741108  19.8
  Vcells 6378701 48.7   19150535 146.2

For R 1.8.1 on an AMD Athlon

 > tmp <- gc(); system.time(for(i in 1:4) y <- a %*% v)
 [1] 0.36 0.00 0.39 0.00 0.00

For R 1.9.0 (patched):

 > tmp <- gc(); system.time(for(i in 1:4) y <- a %*% v)
 [1] 0.81 0.00 0.87 0.00 0.00
 
----

On a fast (hyper threaded) pentium 4 with 2 GB RAM, the
efficiency loss factor is even about 3 {with several runs,
showing only one here}:

R 1.8.1:

>  tmp <- gc(); system.time(for(i in 1:10) y <- a %*% v)
[1] 0.23 0.00 0.23 0.00 0.00

R 1.9.0 (patched):

> tmp <- gc(); system.time(for(i in 1:10) y <- a %*% v)
[1] 0.75 0.00 0.74 0.00 0.00

---------

So, there's definitely something we (R core) should look into.

Martin Maechler



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 29 10:08:42 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 29 Apr 2004 09:08:42 +0100 (BST)
Subject: [R] p-values
In-Reply-To: <975B1B9A-9998-11D8-BD27-000A95CDA0F2@anu.edu.au>
Message-ID: <XFMail.040429090842.Ted.Harding@nessie.mcc.ac.uk>

On 29-Apr-04 John Maindonald wrote:
> This is, of course, not strictly about R.  But if there should be
> a decision to pursue such matters on this list, then we'd need
> another list to which such discussion might be diverted.

A few of us have taken further discussion on this topic off-list,
between ourselves.

> [...]
> R provides a lot of help for those who want a frequentist
> interpretation, even to including by default the *, **, ***
> labeling that some of us deplore.  There is no similar help
> for those who want at least the opportunity to place the
> output from a modeling exercise in a Bayesian context of
> some description.  There is surely a strong argument for
> the use of a more neutral form of default output, even to
> the excluding of p-values, on the argument that they also
> push too strongly in the direction of a frequentist
> interpretative framework.

I can do without the stars, but the p-values are handy
(saves separately computing them if one wants to know what
they are).

> There seems, unfortunately, to be a dearth of good ideas
> on how the assist the placing of output from modeling
> functions such as R provides in an explicitly Bayesian
> framework.  Or is it, at least in part, that I am unaware of
> what is out there? That, I guess, is the point of my
> question to Frank.  Is it just too technically demanding
> to go much beyond trying to get users to understand
> that a Bayesian credible interval can, if there is an
> informative prior, be very different from a frequentist CI,
> that they really do need to pause if there is an
> informative prior lurking somewhere in the undergrowth?

I think a good starting point would be the ability to extract
the likelihood function from a model, perhaps by providing
an "interrogation" method whereby the user can generate values
of it for paramter-values submitted as arguments. This already
exists in a few packages that I know about, e.g. Schafer's
multiple imputation packages 'cat', 'norm, and 'mix' where,
not only tucked away in FORTRAN code are there MCMC engines
which sample the likelihood function, but there are also
functions like 'loglik.mix' which return log-likelihood values
directly.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 29-Apr-04                                       Time: 09:08:42
------------------------------ XFMail ------------------------------



From villirillo at inwind.it  Thu Apr 29 12:53:08 2004
From: villirillo at inwind.it (Villirillo)
Date: Thu, 29 Apr 2004 12:53:08 +0200
Subject: [R] line transect method
Message-ID: <000c01c42dd8$28f50a70$53a6623e@xnf50cg03dx2pj>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/46191769/attachment.pl

From simon at stats.gla.ac.uk  Thu Apr 29 13:05:29 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Thu, 29 Apr 2004 12:05:29 +0100 (BST)
Subject: [R] line transect method
In-Reply-To: <000c01c42dd8$28f50a70$53a6623e@xnf50cg03dx2pj>
References: <000c01c42dd8$28f50a70$53a6623e@xnf50cg03dx2pj>
Message-ID: <Pine.SOL.4.58.0404291204470.4508@moon.stats.gla.ac.uk>

It might be worth looking at:

http://www.ruwpa.st-and.ac.uk/estimating.abundance/

best,
Simon

On Thu, 29 Apr 2004, Villirillo wrote:

> Hello, I'm a final-year student in statistics and I deal with as subject of thesis "rare and elusive populations". I want make a simulation, in which I want generate a population and estimate its density with "line transect method" using R language. Is there someone that can send me the program that generate a population and estimate its density with "line transect method" in R language, please?
>
> I'm sure of your courtesy I'd like immediately to thank for whoever wants to answer me and send the program.
>
> My e-mail: villirillo at inwind.it
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Apr 29 13:13:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 12:13:55 +0100 (BST)
Subject: [R] I'm trying to use package ts (decompose). How do you set up
	the data/ See attached. thanks
In-Reply-To: <3B5823541A25D311B3B90008C7F9056410E35483@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <Pine.LNX.4.44.0404291210000.20845-100000@gannet.stats>

Package ts in R contains no functionality, it having been moved to package 
stats.  The function ts() was not in package ts in older versions either.

I have no idea what your data refer to (quarters, probably, but of which 
years?).  Assumming you know the time base, see the help page ?ts for how 
to create a time series.

BTW, read.table is intended for reading a table, not a single variable, 
and scan() might be easier.

On Thu, 29 Apr 2004, Briggs, Meredith M wrote:

> 
> InDATA <-read.table("C:/Data/May 2004/season.txt",header=T)
> 
> X <- decompose(InDATA)
> print(X)

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Thu Apr 29 13:16:56 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 29 Apr 2004 11:16:56 +0000 (UTC)
Subject: [R] label separators  in boxplots with conditioning
References: <408FDEB2.6010609@ucdavis.edu>
Message-ID: <loom.20040429T130232-459@post.gmane.org>

Webb Sprague <wwsprague <at> ucdavis.edu> writes:

: 
: Hi R-helpers,
: 
: I have a data.frame with three columns (lots more reps though in each), 
: like so:
: 
: 'FOO'    'BAR'    'RESULT'
: 1           .01                75
: 1           .05                12
: 1.1        .01                100
: 1.1        .05                50
: 1.2        .01                75
: 1.2        .05                12
: 
: I am calling boxplot(RESULT ~ FOO:BAR, ...)  This gives me the box plots 
: I want, but on the x-axis my labels are "1.01", "1.05", "1.1.01", 
: "1.1.05", "1.2.01", "1.2.01".  I would like to separate the factors by 
: something other than a dot for obvious reasons.  I would also like to 
: *avoid* using the 'names' parameter to boxplot (because I am lazy and 
: want a general solution).

1. Assuming BAR is numeric and always positive and the data frame is z, 
this will insert a minus sign giving you some separation:

   boxplot(RESULT~FOO:I(-BAR),data=z)

2. You could consider replacing BAR with something along the lines of:

   z$BAR <- factor(paste("(",z$BAR,")",sep=""))



From susanabird at yahoo.com.au  Thu Apr 29 13:49:34 2004
From: susanabird at yahoo.com.au (=?iso-8859-1?q?Susana=20Bird?=)
Date: Thu, 29 Apr 2004 21:49:34 +1000 (EST)
Subject: [R] Dummies in R
Message-ID: <20040429114934.99361.qmail@web60407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/ed2d4eb3/attachment.pl

From julien.glenat at imag.fr  Thu Apr 29 14:47:41 2004
From: julien.glenat at imag.fr (Julien Glenat)
Date: Thu, 29 Apr 2004 14:47:41 +0200
Subject: [R] GUI checkbox linked to entry
Message-ID: <200404291247.i3TClfMR011493@kama.imag.fr>


Hi , i am using R 1.8 and tcltk library in order to make a GUI and i would 
like to know if it is possible to link a check box to one or more entry 
dynamically (and eventually other widgets) .
( in concrete terms : if the check box is ticked the entry is avaible for 
input and unavaible when not ticked)



From partha_bagchi at hgsi.com  Thu Apr 29 14:47:23 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Thu, 29 Apr 2004 08:47:23 -0400
Subject: [R] p-values
Message-ID: <OF36C606F3.0C3E0CC2-ON85256E85.0045FD50-85256E85.004641F2@hgsi.com>

I am sure you are aware of this, but for the record I wanted to mention 
that the book "Bayesian Data Analysis", 2nd Edition, by Gelman, Carlin, 
Stern, and Rubin, published by Chapman and Hall/CRC contains an appendix 
(appendix C) on computations with R and BUGS.

Hopefully Frank will have a section in his book in the future? 






John Maindonald <john.maindonald at anu.edu.au>
Sent by: r-help-bounces at stat.math.ethz.ch
04/29/2004 12:49 AM

 
        To:     r-help at stat.math.ethz.ch
        cc:     ryan.elmore at anu.edu.au, alan.welsh at anu.edu.au
        Subject:        Re:[R] p-values


This is, of course, not strictly about R.  But if there should be
a decision to pursue such matters on this list, then we'd need
another list to which such discussion might be diverted.

I've pulled Frank's "Regression Modeling Stratregies" down
from my shelf and looked to see what he says about
inferential issues.  There is a suggestion, in the introduction,
that modeling provides the groundwork that can be used a
point of departure for a variety of inferential interpretations.
As far as I can see Bayesian interpretations are never
really explicitly discussed, though the word Bayesian does
appear in a couple of places in the text.  Frank, do you now
have ideas on how you would (perhaps, in a future edition,
will) push the discussion in a more overtly Bayesian direction?
What might be the style of a modeling book, aimed at practical
data analysts who of necessity must (mostly, at least) use
off-the-shelf software, that "seriously entertains" the Bayesian
approach?

R provides a lot of help for those who want a frequentist
interpretation, even to including by default the *, **, ***
labeling that some of us deplore.  There is no similar help
for those who want at least the opportunity to place the
output from a modeling exercise in a Bayesian context of
some description.  There is surely a strong argument for
the use of a more neutral form of default output, even to
the excluding of p-values, on the argument that they also
push too strongly in the direction of a frequentist
interpretative framework.

There seems, unfortunately, to be a dearth of good ideas
on how the assist the placing of output from modeling
functions such as R provides in an explicitly Bayesian
framework.  Or is it, at least in part, that I am unaware of
what is out there? That, I guess, is the point of my
question to Frank.  Is it just too technically demanding
to go much beyond trying to get users to understand
that a Bayesian credible interval can, if there is an
informative prior, be very different from a frequentist CI,
that they really do need to pause if there is an
informative prior lurking somewhere in the undergrowth?

John Maindonald.

Frank Harrell wrote:

> They [p-values] are objective only in the sense that
> subjectivity is deferred in a difficult to document way
> when P-values are translated into decisions.


> The statement that frequentist methods are the norm, which I'm
> afraid is usually true, is a sad comment on the state of much
> of "scientific" inquiry.  IMHO P-values are so defective that
> the imperfect Bayesian approach should be seriously entertained.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chris1 at psyctc.org  Thu Apr 29 14:51:54 2004
From: chris1 at psyctc.org (chris1)
Date: Thu, 29 Apr 2004 13:51:54 +0100
Subject: [R] simple repeated measures model: dumb user baffled!
In-Reply-To: <4090C83A.7010403@lscp.ehess.fr>
References: <1695522873.20040428212909@psyctc.org>
	<4090C83A.7010403@lscp.ehess.fr>
Message-ID: <1054948386.20040429135154@psyctc.org>

On Thursday, April 29, 2004, 10:17:46 AM, Christophe Pallier wrote:

CP> Hi,

CP> Here is how I would go:

CP> suj <- gl(40,4)
CP> order <- gl(4,40)
CP> time <- gl(4,1,160)
CP> cond <- factor(c(rep(c('N','E','E','N'),10),
CP>          rep(c('N','E','N','E'),10),
CP>          rep(c('E','N','E','N'),10),
CP>          rep(c('E','N','N','E'),10)))

CP> resp <-rnorm(40*4) # you should use your actual data, just contatening
CP> the 4 columns RESP1 to RESP4:

CP> interaction.plot(time,cond,resp)
CP> summary(aov(resp~+time*cond+Error(suj/(time*cond))))

CP> There is a warning message that bothers me a bit (Warning message: Error
CP> model is singular),
CP> but the degrees of freedom on the table using the Error:suj:time seem fine.

CP> If you try it with your actual data and compare this output with your
CP> statistical package, I would like you know if
You didn't say what you'd want to know but it certainly worked:

summary(aov(nrs~+OCC*LANG+Error(ID/(OCC*LANG))))

Error: ID
          Df  Sum Sq Mean Sq F value Pr(>F)
OCC:LANG   2  0.6794  0.3397  0.5776 0.5662
Residuals 37 21.7594  0.5881               

Error: ID:OCC
           Df Sum Sq Mean Sq F value  Pr(>F)   
OCC         3 0.7196  0.2399  4.3846 0.00586 **
LANG        1 0.3702  0.3702  6.7665 0.01053 * 
OCC:LANG    3 0.0850  0.0283  0.5180 0.67073   
Residuals 113 6.1815  0.0547                   

CP> Concerning the test of higher than linear change, I am not sure how to
CP> handle it. Probaly using a contrast,
CP> but this is an area where I am still not at ease with R.
I thought I could just put "ordered(OCC)" but that didn't do it.
Anyone else help us here?

CP> Hope this helps,
Wonderful.  I probably should have been able to see this but sometimes
I can't unless someone kindly leads the way.  Much appreciated.

Chris



From andy_liaw at merck.com  Thu Apr 29 15:25:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 29 Apr 2004 09:25:35 -0400
Subject: [R] Dummies in R
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7CB0@usrymx25.merck.com>

> From: Susana Bird
> 
> Dear all, 
>  
> my problem is following. I know Stata, but currently I have 
> to use R.

I have the opposite of your problem:  I don't know Stata, but I don't have
to use Stata.  8-)

> Could You please help in finding the analogy to R.
>  
> (1) creating of City-Dummy.
>  
> I know in Stata:
>  
> g byte city=0
> replace city=1 if city==12&year==2000
>  
> and 
>  
> (2) Create a Time-Dummy-Variable
>  
> g byte T2000=0
> replace T2000=1 if year==2000

Because I don't know Stata, I'm not sure what those commands do, but I'll
take a crack at guessing anyway.  If `year' is a vector of numbers, you can
do something like:

  T2000 = ifelse(year == 2000, 1, 0)

[Your #1 above doesn't quite make sense to me:  If you initialize `city' to
0, how do you condition on city == 12?  Anyway, the same idea should apply:
use ifelse().]
 
> (3) I need the City DUmmy for the following combination: I 
> have the financial flows between two cities in the state and 
> I need the paired-Dummy for the exporter (state1) and 
> importer (state2):
>  
> g byte city11=0
> replace city11=1 if state1==12|state2==12

Seems to me that ifelse() will work here, too.
  
> (4) I am interesting in residuals for particular city==12. I 
> have the Dummy for City==12 and regress it on Y-Variable. How 
> could I extract from the output in R the residulas for 
> city==12 (1) to plot this residuals and residuals from other 
> regions in boxplot and (2) to etsimate the confidence interval. 

Something like:

  resid(regModel)[city==12]

HTH,
Andy
  
> Thanks very much in advice,
>  
> Susan
>  
>  
>  
> 
> 
> 
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From nele.b at gmx.de  Thu Apr 29 15:34:48 2004
From: nele.b at gmx.de (Nele Brick)
Date: Thu, 29 Apr 2004 15:34:48 +0200
Subject: [R] Maen squared deviation
Message-ID: <000901c42dee$be5b3010$fe79a8c0@amd26>

Dear list,

i'm looking for how to get the mean squared deviation in R. I tried a lot of
(complicated) instructions, sometimes the worked, but now i'm looking for an
easier one....Hope you can help - greetings.



From dray at biomserv.univ-lyon1.fr  Thu Apr 29 15:46:24 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Thu, 29 Apr 2004 09:46:24 -0400
Subject: [R] problem with pca
In-Reply-To: <20040429070933.35A77C4366@postfix3-1.free.fr>
Message-ID: <5.2.1.1.0.20040429092952.00b3ebb8@biomserv.univ-lyon1.fr>

Hello,
if you want some help, you have to be more precise ! What is the error 
message, could you give an example ?
read the posting guide at http://www.R-project.org/posting-guide.html (I am 
not sure that this question is devoted to R-help, it would be perhaps 
better to ask your question to the maintainer of the package)
I suspect that some of your variables are factor and not numeric. Did you 
code presence-absence as numeric (binary) variables or as factor ?
If you consider both quantitative and qualitative variables, have a look at 
dudi.mix and dudi.hillsmith.

But as you are not precise, you can not receive precise answers.

Sincerely,

If you have
At 03:09 29/04/2004, Famille SALMON wrote:
>Hello
>I have some kind of problem with R.
>I want to do a pca (I work on R and ade4 library) but R doesn't wan to
>realize my pca because of my data:
>I have some binary variable (presence/absence of birds, hole, vegetals...)
>and metric variable (cliff 's size...) and each time I have an error message
>because of the mix of the 2 type of variable.
>So, what can I do?
>
>Famille SALMON
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From tlumley at u.washington.edu  Thu Apr 29 15:59:47 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Apr 2004 06:59:47 -0700 (PDT)
Subject: [R] p-values
In-Reply-To: <975B1B9A-9998-11D8-BD27-000A95CDA0F2@anu.edu.au>
References: <200404281007.i3SA6eWm029623@hypatia.math.ethz.ch>
	<975B1B9A-9998-11D8-BD27-000A95CDA0F2@anu.edu.au>
Message-ID: <Pine.A41.4.58.0404290658570.26066@homer04.u.washington.edu>

On Thu, 29 Apr 2004, John Maindonald wrote:

> This is, of course, not strictly about R.  But if there should be
> a decision to pursue such matters on this list, then we'd need
> another list to which such discussion might be diverted.
>

Ted Harding started such a list (stats-discuss) quite some time ago. IIRC
it was to divert discussions like this from allstat.

	-thomas



From b2kspiller at hotmail.com  Thu Apr 29 15:56:19 2004
From: b2kspiller at hotmail.com (Rob Kamstra)
Date: Thu, 29 Apr 2004 15:56:19 +0200
Subject: [R] RPART drawing the tree
Message-ID: <BAY17-F202pebcffrXU000db99f@hotmail.com>

Hello,

I am using the RPART library to find patterns in HIV mutations regarding 
drug-resistancy.
My data consists of aminoacid at certain locations and two classes resistant 
and susceptible.

The classification and pruning work fine with Rpart. however there is a 
problem with displaying the data as a tree in the display window.

in the display window the data contain only levels at the splits example: 
(abcde) left (fg) right. but i would like to have the aminoacids displayed. 
how can this be achieved ?

Rob Kamstra

_________________________________________________________________
MSN Search, for accurate results! http://search.msn.nl



From tlumley at u.washington.edu  Thu Apr 29 16:08:43 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Apr 2004 07:08:43 -0700 (PDT)
Subject: [R] Dummies in R
In-Reply-To: <20040429114934.99361.qmail@web60407.mail.yahoo.com>
References: <20040429114934.99361.qmail@web60407.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0404290704210.26066@homer04.u.washington.edu>

On Thu, 29 Apr 2004, [iso-8859-1] Susana Bird wrote:

> Dear all,
>  my problem is following. I know Stata, but currently I have to use R.
> Could You please help in finding the analogy to R.
>
> (1) creating of City-Dummy.
> (2) Create a Time-Dummy-Variable
>

Andy Liaw has described how to do this, but you probably don't need to.

In regression models in R, factor variables are automatically expanded to
a suitable design matrix, by default a set of indicator (`dummy')
variables for each category except the first.

regModel <- lm( outcome~factor(city)*factor(time))

will give you indicator variables for each city and time and for the
interactions.

otherRegModel <- lm(outcome~factor(city)+factor(time))

is the model without interactions.

	-thomas



From ripley at stats.ox.ac.uk  Thu Apr 29 16:10:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 15:10:27 +0100 (BST)
Subject: [R] RPART drawing the tree
In-Reply-To: <BAY17-F202pebcffrXU000db99f@hotmail.com>
Message-ID: <Pine.LNX.4.44.0404291509050.30590-100000@gannet.stats>

On Thu, 29 Apr 2004, Rob Kamstra wrote:

> I am using the RPART library to find patterns in HIV mutations regarding 
> drug-resistancy.
> My data consists of aminoacid at certain locations and two classes resistant 
> and susceptible.
> 
> The classification and pruning work fine with Rpart. however there is a 
> problem with displaying the data as a tree in the display window.
> 
> in the display window the data contain only levels at the splits example: 
> (abcde) left (fg) right. but i would like to have the aminoacids displayed. 
> how can this be achieved ?

By reading the documentation, as suggested in 

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

specifically by ?text.rpart, since that has an argument `pretty' to 
control this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cepl at surfbest.net  Thu Apr 29 16:12:04 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Thu, 29 Apr 2004 10:12:04 -0400
Subject: [R] Dummies in R
In-Reply-To: <20040429114934.99361.qmail@web60407.mail.yahoo.com>
References: <20040429114934.99361.qmail@web60407.mail.yahoo.com>
Message-ID: <200404291012.04317.cepl@surfbest.net>

On Thursday 29 of April 2004 07:49, Susana Bird wrote:
> my problem is following. I know Stata, but currently I have to 
use R. Could You please help in finding the analogy to R.

I am too clueless about Stata, but I have another hint (check 
"Introduction to R" whether I have guessed correctly for what 
you want to do):

> g byte city=0

I have no clue what does it is supposed to mean (initialize city 
with zeroes? But how can you test for its value then?).

> replace city=1 if city==12&year==2000

What about this?

	city <- city[city == "12" & year == "2000"]

Take a look at this example:

	> city <- cbind(rnorm(100,50,10),0)
	> city
	> city[,2] <- as.logical(city[,1]<"50")
	> city

Does it make any sense to you?

	Mat??j

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
A modest little person, with much to be modest about.
      -- Winston Churchill



From myao at ou.edu  Thu Apr 29 16:19:39 2004
From: myao at ou.edu (Yao, Minghua)
Date: Thu, 29 Apr 2004 09:19:39 -0500
Subject: [R] Problems in plot
Message-ID: <78B50CF247E5D04B8A5E02D001CC8E9A595C53@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/5d593343/attachment.pl

From apb14 at duke.edu  Thu Apr 29 16:23:14 2004
From: apb14 at duke.edu (ashley ballantyne)
Date: Thu, 29 Apr 2004 10:23:14 -0400
Subject: [R] geoR y-scale problem
Message-ID: <40910FD2.5030807@duke.edu>

I am attempting to use the Bayesian kriging function in geoR by Ribeiro 
and I am encountering a problem with the "image" command.
The program is not responsive to the "ylim" command, such that:

image(temp.bayes,loc=pred.grid,col=rainbow(25, start=.6, 
end=.1),xlab="Longitude",ylab="Latitude", ylim=c(-30,30),borders=NULL)
legend.krige(y.leg=c(-150,-140), x.leg=c(-150,150), 
values=temp.bayes$predictive$mean.simulations,col=rainbow(25, start=.6, 
end=.1))
map('worldHires', ylim=c(-30,30), add=TRUE,interior=FALSE)
title(expression(paste("a.)",Delta, "T", degree,C)))

fails to generate a figure with the y-axis from 30 degrees south to 30 
degrees north.  Interestingly, the image command is responsive to the "xlim"
command

image(temp.bayes,loc=pred.grid,col=rainbow(25, start=.6, 
end=.1),xlab="Longitude",ylab="Latitude", xlim=c(-30,30),borders=NULL)

but this changes the scale of both the y and x axes.  I have tried 
exporting the gridded data to the base R package to use the standard  
"image" command; however the data are in a very cumbersome format.  
Please help if you have encountered such a problem.

Thank you,
Ashley



From adrian_d at eskimo.com  Thu Apr 29 16:25:08 2004
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Thu, 29 Apr 2004 07:25:08 -0700 (PDT)
Subject: [R] memory problems with lm 
Message-ID: <Pine.SUN.4.58.0404290724280.4407@eskimo.com>


Hello list,

I've seen the recent discussions documenting problems with lm.

I have encountered the following problem.  I use WinXP Pro with
service pack 1, and R 1.9.0, on a XEON 2GHz, with 1GB of RAM.

> eff.fro
              std.dev         mean
NSTRDSP  7.403749e-01 1.215686e-01
CPFGEP   9.056763e+00 1.815686e+00
WSWOLF   4.703588e+05 1.112832e+05
NPILGRIM 1.017640e+06 2.134335e+05
WSNMILE  1.367312e+07 1.892021e+06
WSHIDESL 1.830811e+07 1.892021e+06
> reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
Error in model.matrix.default(mt, mf, contrasts) :
        cannot allocate vector of length 1074790452
> log(eff.fro$mean)
[1] -2.1072763  0.5964635 11.6198339 12.2710808 14.4531561
[6] 14.4531561
> reg <- lm(log(eff.fro$mean) ~ log(eff.fro$std.dev))
Error: cannot allocate vector of size 3360077 Kb
> lef <- log(eff.fro)
> lef
            std.dev       mean
NSTRDSP  -0.3005986 -2.1072763
CPFGEP    2.2035117  0.5964635
WSWOLF   13.0612512 11.6198339
NPILGRIM 13.8329973 12.2710808
WSNMILE  16.4309427 14.4531561
WSHIDESL 16.7228546 14.4531561
> lef <- log(eff.fro)
> reg <- lm(lef$mean ~ lef$std.dev)

Here the my computer completely crashed.  A window poped-up and said
memory problem at address ..., and if I want to debug.

I ran the same code one more time, and it worked but it did not work
how I wanted (where is the slope?):
> reg <- lm(lef$mean ~ lef$std.dev)
> reg

Call:
lm(formula = lef$mean ~ lef$std.dev)

Coefficients:
(Intercept)
      8.548

>
> summary(reg)

Call:
lm(formula = lef$mean ~ lef$std.dev)

Residuals:
      1       2       3       4       5       6
-10.655  -7.951   3.072   3.723   5.905   5.905

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)    8.548      2.999    2.85   0.0358 *
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 7.346 on 5 degrees of freedom

I ran again:

  reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)

and I get the pop-up:
The instruction at "0x6b4c45a5" referenced memory at "0x0032374a".
The memory could not be "read".  Click OK to terminate the program.


Any ideas?  Thank you,
Adrian



From ian.ke at sympatico.ca  Thu Apr 29 16:28:20 2004
From: ian.ke at sympatico.ca (Ian Kennedy)
Date: Thu, 29 Apr 2004 10:28:20 -0400
Subject: [R] Entering times around the start of daylight savings time
Message-ID: <200404291028.21019.ian.ke@sympatico.ca>

I'm having problems entering dates and times around when daylight savings time 
starts. If I type (on R 1.8.1 on Gentoo Linux)

>  ISOdatetime(2004,4,4,0:4,0,0,"GMT")
[1] "2004-04-03 19:00:00 EST" "2004-04-03 20:00:00 EST"
[3] "2004-04-03 22:00:00 EST" "2004-04-03 22:00:00 EST"
[5] "2004-04-03 23:00:00 EST"

Giving the times between 2:00 and 3:00 GMT on 4 April which are all off by one 
hour. I tried setting TZ (to "Canada/Eastern") but didn't see any change.

For comparison I tried the same thing in R 1.8.1 for Windows and got a similar 
error, but the one hour that is wrong is one hour early, rather than one hour 
late:
> ISOdatetime(2004,4,4,0:4,0,0,"GMT")
[1] "2004-04-03 19:00:00 Eastern Standard Time"
[2] "2004-04-03 20:00:00 Eastern Standard Time"
[3] "2004-04-03 20:00:00 Eastern Standard Time"
[4] "2004-04-03 22:00:00 Eastern Standard Time"
[5] "2004-04-03 23:00:00 Eastern Standard Time"

If I try the same thing on R 1.9 for OS X, I get the correct result, that is 
one hour intervals.

So far I've been able to enter times correctly for this period by using chron, 
multiplying by the number of seconds in a day and forcing the resulting 
number to be a POSIXct, but this seems too involved and probably unreliable.

Thanks for any suggestions,

Ian Kennedy



From andy_liaw at merck.com  Thu Apr 29 16:30:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 29 Apr 2004 10:30:35 -0400
Subject: [R] memory problems with lm
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7CB6@usrymx25.merck.com>

Can you show us the output of str(eff.fro)?  Do you have other things in the
global environment or the search path that's taking up memory?  What does
gc() say?

Andy

> From: Adrian Dragulescu
> 
> Hello list,
> 
> I've seen the recent discussions documenting problems with lm.
> 
> I have encountered the following problem.  I use WinXP Pro with
> service pack 1, and R 1.9.0, on a XEON 2GHz, with 1GB of RAM.
> 
> > eff.fro
>               std.dev         mean
> NSTRDSP  7.403749e-01 1.215686e-01
> CPFGEP   9.056763e+00 1.815686e+00
> WSWOLF   4.703588e+05 1.112832e+05
> NPILGRIM 1.017640e+06 2.134335e+05
> WSNMILE  1.367312e+07 1.892021e+06
> WSHIDESL 1.830811e+07 1.892021e+06
> > reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> Error in model.matrix.default(mt, mf, contrasts) :
>         cannot allocate vector of length 1074790452
> > log(eff.fro$mean)
> [1] -2.1072763  0.5964635 11.6198339 12.2710808 14.4531561
> [6] 14.4531561
> > reg <- lm(log(eff.fro$mean) ~ log(eff.fro$std.dev))
> Error: cannot allocate vector of size 3360077 Kb
> > lef <- log(eff.fro)
> > lef
>             std.dev       mean
> NSTRDSP  -0.3005986 -2.1072763
> CPFGEP    2.2035117  0.5964635
> WSWOLF   13.0612512 11.6198339
> NPILGRIM 13.8329973 12.2710808
> WSNMILE  16.4309427 14.4531561
> WSHIDESL 16.7228546 14.4531561
> > lef <- log(eff.fro)
> > reg <- lm(lef$mean ~ lef$std.dev)
> 
> Here the my computer completely crashed.  A window poped-up and said
> memory problem at address ..., and if I want to debug.
> 
> I ran the same code one more time, and it worked but it did not work
> how I wanted (where is the slope?):
> > reg <- lm(lef$mean ~ lef$std.dev)
> > reg
> 
> Call:
> lm(formula = lef$mean ~ lef$std.dev)
> 
> Coefficients:
> (Intercept)
>       8.548
> 
> >
> > summary(reg)
> 
> Call:
> lm(formula = lef$mean ~ lef$std.dev)
> 
> Residuals:
>       1       2       3       4       5       6
> -10.655  -7.951   3.072   3.723   5.905   5.905
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)    8.548      2.999    2.85   0.0358 *
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 7.346 on 5 degrees of freedom
> 
> I ran again:
> 
>   reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> 
> and I get the pop-up:
> The instruction at "0x6b4c45a5" referenced memory at "0x0032374a".
> The memory could not be "read".  Click OK to terminate the program.
> 
> 
> Any ideas?  Thank you,
> Adrian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ripley at stats.ox.ac.uk  Thu Apr 29 16:31:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 15:31:10 +0100 (BST)
Subject: [R] Problems in plot
In-Reply-To: <78B50CF247E5D04B8A5E02D001CC8E9A595C53@XMAIL.sooner.net.ou.edu>
Message-ID: <Pine.LNX.4.44.0404291528440.30590-100000@gannet.stats>

The primary graphics device under Windows is called *windows* not *x11*.

Something in your Windows setup is sometimes failing to choose a
reasonable window size.  I have never seen that, and suspect it is nothing
to do with it, but please use windows() and see if the problem vanishes.

On Thu, 29 Apr 2004, Yao, Minghua wrote:

> Hello,
>  
>  
>  
> I have R1.9.0 under Windows XP. My program plots several plots using
>  
> x11()
> par(cex = 0.75)
> ......
> x11()
> par(cex = 0.75)
> ......
> x11()
> par(cex = 0.75)
> ......
> x11()
> par(cex = 0.75)
> ......
>  
> Sometimes, one of them generates  a small frame only with title area "R graphics: Device X (ACTIVE)". The message in the console window is 
>  
> Error in plot.new() : Figure margins too large
>  
> This program ran well under R1.6.X under Windows NT.
>  
> It seems to me that it is not a specific x11() that generates that small graphics frame.
>  
> Thank you for you help in advance.
>  
> Minghua
> 
>  
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Andre.Skusa at cebitec.uni-bielefeld.de  Thu Apr 29 16:31:25 2004
From: Andre.Skusa at cebitec.uni-bielefeld.de (Andre Skusa)
Date: Thu, 29 Apr 2004 16:31:25 +0200
Subject: [R] graph algorithms in R
Message-ID: <409111BD.40603@cebitec.uni-bielefeld.de>

Dear R users,

just a quick question: Is there a reliable and good graph library for R, 
eg. with shortest path algorithms on adjacency matrixes? I already found 
a graph package as part of "Bioconductor". Are there more?

If anyone knows and would tell me, I appreciate very much!

Cheers,

Andre

-- 
Dipl.-Inform. Andre Skusa (PhD student)

NRW Graduate School in Bioinformatics and Genome Research
Center of Biotechnology (CeBiTec)
University of Bielefeld
Postfach 10 01 31
D-33501 Bielefeld
Germany

home: http://www.cebitec.uni-bielefeld.de/~askusa

email: andre.skusa at cebitec.uni-bielefeld.de



From ripley at stats.ox.ac.uk  Thu Apr 29 16:34:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 29 Apr 2004 15:34:37 +0100 (BST)
Subject: [R] memory problems with lm 
In-Reply-To: <Pine.SUN.4.58.0404290724280.4407@eskimo.com>
Message-ID: <Pine.LNX.4.44.0404291531400.30590-100000@gannet.stats>

This may or may not be the same problem (which is already solved). But 
please read the section on BUGS in the R FAQ and set up a reproducible 
example.  Then try out the current version of r-patched (one dated 
tomorrow or later, to be safe) and see if the problem recurs.  If it does, 
please file a bug report.

My guess is that eff.fro$std.dev is a 1D array (use dim or str to find 
out), and you did not intend that.

On Thu, 29 Apr 2004, Adrian Dragulescu wrote:

> 
> Hello list,
> 
> I've seen the recent discussions documenting problems with lm.
> 
> I have encountered the following problem.  I use WinXP Pro with
> service pack 1, and R 1.9.0, on a XEON 2GHz, with 1GB of RAM.
> 
> > eff.fro
>               std.dev         mean
> NSTRDSP  7.403749e-01 1.215686e-01
> CPFGEP   9.056763e+00 1.815686e+00
> WSWOLF   4.703588e+05 1.112832e+05
> NPILGRIM 1.017640e+06 2.134335e+05
> WSNMILE  1.367312e+07 1.892021e+06
> WSHIDESL 1.830811e+07 1.892021e+06
> > reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> Error in model.matrix.default(mt, mf, contrasts) :
>         cannot allocate vector of length 1074790452
> > log(eff.fro$mean)
> [1] -2.1072763  0.5964635 11.6198339 12.2710808 14.4531561
> [6] 14.4531561
> > reg <- lm(log(eff.fro$mean) ~ log(eff.fro$std.dev))
> Error: cannot allocate vector of size 3360077 Kb
> > lef <- log(eff.fro)
> > lef
>             std.dev       mean
> NSTRDSP  -0.3005986 -2.1072763
> CPFGEP    2.2035117  0.5964635
> WSWOLF   13.0612512 11.6198339
> NPILGRIM 13.8329973 12.2710808
> WSNMILE  16.4309427 14.4531561
> WSHIDESL 16.7228546 14.4531561
> > lef <- log(eff.fro)
> > reg <- lm(lef$mean ~ lef$std.dev)
> 
> Here the my computer completely crashed.  A window poped-up and said
> memory problem at address ..., and if I want to debug.
> 
> I ran the same code one more time, and it worked but it did not work
> how I wanted (where is the slope?):
> > reg <- lm(lef$mean ~ lef$std.dev)
> > reg
> 
> Call:
> lm(formula = lef$mean ~ lef$std.dev)
> 
> Coefficients:
> (Intercept)
>       8.548
> 
> >
> > summary(reg)
> 
> Call:
> lm(formula = lef$mean ~ lef$std.dev)
> 
> Residuals:
>       1       2       3       4       5       6
> -10.655  -7.951   3.072   3.723   5.905   5.905
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept)    8.548      2.999    2.85   0.0358 *
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 7.346 on 5 degrees of freedom
> 
> I ran again:
> 
>   reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> 
> and I get the pop-up:
> The instruction at "0x6b4c45a5" referenced memory at "0x0032374a".
> The memory could not be "read".  Click OK to terminate the program.
> 
> 
> Any ideas?  Thank you,
> Adrian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adrian_d at eskimo.com  Thu Apr 29 16:46:19 2004
From: adrian_d at eskimo.com (Adrian Dragulescu)
Date: Thu, 29 Apr 2004 07:46:19 -0700 (PDT)
Subject: [R] memory problems with lm
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7CB6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7CB6@usrymx25.merck.com>
Message-ID: <Pine.SUN.4.58.0404290745410.4407@eskimo.com>


If I enforce the variables to be numeric it works fine.

> str(eff.fro)
`data.frame':   6 obs. of  2 variables:
 $ std.dev: num [, 1:6] 7.40e-01 9.06e+00 4.70e+05 1.02e+06 1.37e+07 ...
  ..- attr(*, "dimnames")=List of 1
  .. ..$ : chr  "NSTRDSP" "CPFGEP" "WSWOLF" "NPILGRIM" ...
 $ mean   : num  1.22e-01 1.82e+00 1.11e+05 2.13e+05 1.89e+06 ...
> gc()
         used (Mb) gc trigger (Mb)
Ncells 578941 15.5    1166886 31.2
Vcells 589444  4.5    2377385 18.2
> eff.fro
              std.dev         mean
NSTRDSP  7.403749e-01 1.215686e-01
CPFGEP   9.056763e+00 1.815686e+00
WSWOLF   4.703588e+05 1.112832e+05
NPILGRIM 1.017640e+06 2.134335e+05
WSNMILE  1.367312e+07 1.892021e+06
WSHIDESL 1.830811e+07 1.892021e+06
> reg <- lm(log(as.numeric(mean)) ~ log(as.numeric(std.dev)),
data=eff.fro)
> reg

Call:
lm(formula = log(as.numeric(mean)) ~ log(as.numeric(std.dev)),     data =
eff.fro)

Coefficients:
             (Intercept)  log(as.numeric(std.dev))
                 -1.6368                    0.9864


Adrian


On Thu, 29 Apr 2004, Liaw, Andy wrote:

> Can you show us the output of str(eff.fro)?  Do you have other things in the
> global environment or the search path that's taking up memory?  What does
> gc() say?
>
> Andy
>
> > From: Adrian Dragulescu
> >
> > Hello list,
> >
> > I've seen the recent discussions documenting problems with lm.
> >
> > I have encountered the following problem.  I use WinXP Pro with
> > service pack 1, and R 1.9.0, on a XEON 2GHz, with 1GB of RAM.
> >
> > > eff.fro
> >               std.dev         mean
> > NSTRDSP  7.403749e-01 1.215686e-01
> > CPFGEP   9.056763e+00 1.815686e+00
> > WSWOLF   4.703588e+05 1.112832e+05
> > NPILGRIM 1.017640e+06 2.134335e+05
> > WSNMILE  1.367312e+07 1.892021e+06
> > WSHIDESL 1.830811e+07 1.892021e+06
> > > reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> > Error in model.matrix.default(mt, mf, contrasts) :
> >         cannot allocate vector of length 1074790452
> > > log(eff.fro$mean)
> > [1] -2.1072763  0.5964635 11.6198339 12.2710808 14.4531561
> > [6] 14.4531561
> > > reg <- lm(log(eff.fro$mean) ~ log(eff.fro$std.dev))
> > Error: cannot allocate vector of size 3360077 Kb
> > > lef <- log(eff.fro)
> > > lef
> >             std.dev       mean
> > NSTRDSP  -0.3005986 -2.1072763
> > CPFGEP    2.2035117  0.5964635
> > WSWOLF   13.0612512 11.6198339
> > NPILGRIM 13.8329973 12.2710808
> > WSNMILE  16.4309427 14.4531561
> > WSHIDESL 16.7228546 14.4531561
> > > lef <- log(eff.fro)
> > > reg <- lm(lef$mean ~ lef$std.dev)
> >
> > Here the my computer completely crashed.  A window poped-up and said
> > memory problem at address ..., and if I want to debug.
> >
> > I ran the same code one more time, and it worked but it did not work
> > how I wanted (where is the slope?):
> > > reg <- lm(lef$mean ~ lef$std.dev)
> > > reg
> >
> > Call:
> > lm(formula = lef$mean ~ lef$std.dev)
> >
> > Coefficients:
> > (Intercept)
> >       8.548
> >
> > >
> > > summary(reg)
> >
> > Call:
> > lm(formula = lef$mean ~ lef$std.dev)
> >
> > Residuals:
> >       1       2       3       4       5       6
> > -10.655  -7.951   3.072   3.723   5.905   5.905
> >
> > Coefficients:
> >             Estimate Std. Error t value Pr(>|t|)
> > (Intercept)    8.548      2.999    2.85   0.0358 *
> > ---
> > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> > Residual standard error: 7.346 on 5 degrees of freedom
> >
> > I ran again:
> >
> >   reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> >
> > and I get the pop-up:
> > The instruction at "0x6b4c45a5" referenced memory at "0x0032374a".
> > The memory could not be "read".  Click OK to terminate the program.
> >
> >
> > Any ideas?  Thank you,
> > Adrian
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------
>



From andy_liaw at merck.com  Thu Apr 29 16:56:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 29 Apr 2004 10:56:12 -0400
Subject: [R] memory problems with lm
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7CB9@usrymx25.merck.com>

I believe Prof. Ripley is right.  The problem is

>  $ std.dev: num [, 1:6] 7.40e-01 9.06e+00 4.70e+05 1.02e+06 
> 1.37e+07 ...
>   ..- attr(*, "dimnames")=List of 1

which looks like an array, rather than a vector.

Andy

> From: Adrian Dragulescu [mailto:adrian_d at eskimo.com] 
> 
> If I enforce the variables to be numeric it works fine.
> 
> > str(eff.fro)
> `data.frame':   6 obs. of  2 variables:
>  $ std.dev: num [, 1:6] 7.40e-01 9.06e+00 4.70e+05 1.02e+06 
> 1.37e+07 ...
>   ..- attr(*, "dimnames")=List of 1
>   .. ..$ : chr  "NSTRDSP" "CPFGEP" "WSWOLF" "NPILGRIM" ...
>  $ mean   : num  1.22e-01 1.82e+00 1.11e+05 2.13e+05 1.89e+06 ...
> > gc()
>          used (Mb) gc trigger (Mb)
> Ncells 578941 15.5    1166886 31.2
> Vcells 589444  4.5    2377385 18.2
> > eff.fro
>               std.dev         mean
> NSTRDSP  7.403749e-01 1.215686e-01
> CPFGEP   9.056763e+00 1.815686e+00
> WSWOLF   4.703588e+05 1.112832e+05
> NPILGRIM 1.017640e+06 2.134335e+05
> WSNMILE  1.367312e+07 1.892021e+06
> WSHIDESL 1.830811e+07 1.892021e+06
> > reg <- lm(log(as.numeric(mean)) ~ log(as.numeric(std.dev)),
> data=eff.fro)
> > reg
> 
> Call:
> lm(formula = log(as.numeric(mean)) ~ 
> log(as.numeric(std.dev)),     data =
> eff.fro)
> 
> Coefficients:
>              (Intercept)  log(as.numeric(std.dev))
>                  -1.6368                    0.9864
> 
> 
> Adrian
> 
> 
> On Thu, 29 Apr 2004, Liaw, Andy wrote:
> 
> > Can you show us the output of str(eff.fro)?  Do you have 
> other things in the
> > global environment or the search path that's taking up 
> memory?  What does
> > gc() say?
> >
> > Andy
> >
> > > From: Adrian Dragulescu
> > >
> > > Hello list,
> > >
> > > I've seen the recent discussions documenting problems with lm.
> > >
> > > I have encountered the following problem.  I use WinXP Pro with
> > > service pack 1, and R 1.9.0, on a XEON 2GHz, with 1GB of RAM.
> > >
> > > > eff.fro
> > >               std.dev         mean
> > > NSTRDSP  7.403749e-01 1.215686e-01
> > > CPFGEP   9.056763e+00 1.815686e+00
> > > WSWOLF   4.703588e+05 1.112832e+05
> > > NPILGRIM 1.017640e+06 2.134335e+05
> > > WSNMILE  1.367312e+07 1.892021e+06
> > > WSHIDESL 1.830811e+07 1.892021e+06
> > > > reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> > > Error in model.matrix.default(mt, mf, contrasts) :
> > >         cannot allocate vector of length 1074790452
> > > > log(eff.fro$mean)
> > > [1] -2.1072763  0.5964635 11.6198339 12.2710808 14.4531561
> > > [6] 14.4531561
> > > > reg <- lm(log(eff.fro$mean) ~ log(eff.fro$std.dev))
> > > Error: cannot allocate vector of size 3360077 Kb
> > > > lef <- log(eff.fro)
> > > > lef
> > >             std.dev       mean
> > > NSTRDSP  -0.3005986 -2.1072763
> > > CPFGEP    2.2035117  0.5964635
> > > WSWOLF   13.0612512 11.6198339
> > > NPILGRIM 13.8329973 12.2710808
> > > WSNMILE  16.4309427 14.4531561
> > > WSHIDESL 16.7228546 14.4531561
> > > > lef <- log(eff.fro)
> > > > reg <- lm(lef$mean ~ lef$std.dev)
> > >
> > > Here the my computer completely crashed.  A window 
> poped-up and said
> > > memory problem at address ..., and if I want to debug.
> > >
> > > I ran the same code one more time, and it worked but it 
> did not work
> > > how I wanted (where is the slope?):
> > > > reg <- lm(lef$mean ~ lef$std.dev)
> > > > reg
> > >
> > > Call:
> > > lm(formula = lef$mean ~ lef$std.dev)
> > >
> > > Coefficients:
> > > (Intercept)
> > >       8.548
> > >
> > > >
> > > > summary(reg)
> > >
> > > Call:
> > > lm(formula = lef$mean ~ lef$std.dev)
> > >
> > > Residuals:
> > >       1       2       3       4       5       6
> > > -10.655  -7.951   3.072   3.723   5.905   5.905
> > >
> > > Coefficients:
> > >             Estimate Std. Error t value Pr(>|t|)
> > > (Intercept)    8.548      2.999    2.85   0.0358 *
> > > ---
> > > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> > >
> > > Residual standard error: 7.346 on 5 degrees of freedom
> > >
> > > I ran again:
> > >
> > >   reg <- lm(log(mean) ~ log(std.dev), data=eff.fro)
> > >
> > > and I get the pop-up:
> > > The instruction at "0x6b4c45a5" referenced memory at "0x0032374a".
> > > The memory could not be "read".  Click OK to terminate 
> the program.
> > >
> > >
> > > Any ideas?  Thank you,
> > > Adrian
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments, contains
> > information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New
> > Jersey, USA 08889), and/or its affiliates (which may be 
> known outside the
> > United States as Merck Frosst, Merck Sharp & Dohme or MSD 
> and in Japan as
> > Banyu) that may be confidential, proprietary copyrighted 
> and/or legally
> > privileged. It is intended solely for the use of the 
> individual or entity
> > named on this message.  If you are not the intended 
> recipient, and have
> > received this message in error, please notify us 
> immediately by reply e-mail
> > and then delete it from your system.
> > 
> --------------------------------------------------------------
> ----------------
> >
> 
>



From r.alberts at cs.rug.nl  Fri Apr 30 02:17:18 2004
From: r.alberts at cs.rug.nl (Rudi Alberts)
Date: 29 Apr 2004 17:17:18 -0700
Subject: [R] R plot to HTML image map
Message-ID: <1083284238.1530.54.camel@gbic04>

Hi,

I automatically create some R matplots and now I would like to 
automatically create HMTL image maps and make the plots clickable.
Do you have any suggestions how to do this?

regards, R. Alberts



From macq at llnl.gov  Thu Apr 29 17:30:31 2004
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 29 Apr 2004 08:30:31 -0700
Subject: [R] Entering times around the start of daylight savings time
In-Reply-To: <200404291028.21019.ian.ke@sympatico.ca>
References: <200404291028.21019.ian.ke@sympatico.ca>
Message-ID: <p0600200abcb6cb04cba6@[128.115.153.6]>

You might do better using seq.POSIXt. Your basic problem is that your 
sequence includes a time that doesn't exist, 02:00 on the transition 
day.

seq.POSIXt(ISOdatetime(2004,4,4,0,0,0),by='hour',len=5)
[1] "2004-04-04 00:00:00 PST" "2004-04-04 01:00:00 PST" "2004-04-04 
03:00:00 PDT" "2004-04-04 04:00:00 PDT" "2004-04-04 05:00:00 PDT"

seq.POSIXt(ISOdatetime(2004,4,4,0,0,0,'GMT'),by='hour',len=5)
[1] "2004-04-03 16:00:00 PST" "2004-04-03 17:00:00 PST" "2004-04-03 
18:00:00 PST" "2004-04-03 19:00:00 PST" "2004-04-03 20:00:00 PST"

Both of the above examples give answers that are correct for my 
timezone, and are the same on both an OS X system and a Solaris 
system. R 1.8.1. Note that they are in fact one hour apart:

>  diff(seq.POSIXt(ISOdatetime(2004,4,4,0,0,0,'GMT'),by='hour',len=5))
Time differences of 1, 1, 1, 1 hours
>  diff(seq.POSIXt(ISOdatetime(2004,4,4,0,0,0),by='hour',len=5))
Time differences of 1, 1, 1, 1 hours

>  diff(as.numeric(seq.POSIXt(ISOdatetime(2004,4,4,0,0,0),by='hour',len=5)))
[1] 3600 3600 3600 3600
> 
>diff(as.numeric(seq.POSIXt(ISOdatetime(2004,4,4,0,0,0,'GMT'),by='hour',len=5)))
[1] 3600 3600 3600 3600


Also, since I'm in US pacific and you're in US eastern, perhaps this 
example will come closer to yours:
>  seq.POSIXt(ISOdatetime(2004,4,4,8,0,0,'GMT'),by='hour',len=5)
[1] "2004-04-04 00:00:00 PST" "2004-04-04 01:00:00 PST" "2004-04-04 
03:00:00 PDT" "2004-04-04 04:00:00 PDT" "2004-04-04 05:00:00 PDT"

Again, note that it correctly makes the PST to PDT transition, giving 
times that are one hour apart.
> 
>diff(as.numeric(seq.POSIXt(ISOdatetime(2004,4,4,8,0,0,'GMT'),by='hour',len=5)))
[1] 3600 3600 3600 3600


Whatever the timezone, if the change from standard time to daylight 
savings time takes place at 02:00, then there are no "times between 
2:00 and 3:00". That is, the correct sequence of times is 01:58, 
01:59, 03:00, 03:01, etc., because at 2 AM we jump directly to 3 AM 
(that's what daylight savings time is, after all).

  seq.POSIXt(ISOdatetime(2004,4,4,1,58,0),by='min',len=5)
[1] "2004-04-04 01:58:00 PST" "2004-04-04 01:59:00 PST" "2004-04-04 
03:00:00 PDT" "2004-04-04 03:01:00 PDT" "2004-04-04 03:02:00 PDT"

When I first encountered this issue a while back, the advice was that 
if you specify times that don't exist (such as 02:00 on the 
transition day), what happens depends on your OS; and one should not 
expect it to make sense because in fact the time doesn't exist.

A final note, in my OS X system, I get

>  ISOdatetime(2004,4,4,0:4,0,0)
[1] "2004-04-04 00:00:00 PST" "2004-04-04 01:00:00 PST" NA 
"2004-04-04 03:00:00 PDT" "2004-04-04 04:00:00 PDT"

I would consider this correct, since it gives NA for 
ISOdatetime(2004,4,4,2,0,0), a time that doesn't exist. Excluding the 
NA, it results in one hour intervals.

-Don

At 10:28 AM -0400 4/29/04, Ian Kennedy wrote:
>I'm having problems entering dates and times around when daylight savings time
>starts. If I type (on R 1.8.1 on Gentoo Linux)
>
>>   ISOdatetime(2004,4,4,0:4,0,0,"GMT")
>[1] "2004-04-03 19:00:00 EST" "2004-04-03 20:00:00 EST"
>[3] "2004-04-03 22:00:00 EST" "2004-04-03 22:00:00 EST"
>[5] "2004-04-03 23:00:00 EST"
>
>Giving the times between 2:00 and 3:00 GMT on 4 April which are all off by one
>hour. I tried setting TZ (to "Canada/Eastern") but didn't see any change.
>
>For comparison I tried the same thing in R 1.8.1 for Windows and got a similar
>error, but the one hour that is wrong is one hour early, rather than one hour
>late:
>  > ISOdatetime(2004,4,4,0:4,0,0,"GMT")
>[1] "2004-04-03 19:00:00 Eastern Standard Time"
>[2] "2004-04-03 20:00:00 Eastern Standard Time"
>[3] "2004-04-03 20:00:00 Eastern Standard Time"
>[4] "2004-04-03 22:00:00 Eastern Standard Time"
>[5] "2004-04-03 23:00:00 Eastern Standard Time"
>
>If I try the same thing on R 1.9 for OS X, I get the correct result, that is
>one hour intervals.
>
>So far I've been able to enter times correctly for this period by using chron,
>multiplying by the number of seconds in a day and forcing the resulting
>number to be a POSIXct, but this seems too involved and probably unreliable.
>
>Thanks for any suggestions,
>
>Ian Kennedy
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From B.Rowlingson at lancaster.ac.uk  Thu Apr 29 17:36:58 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 29 Apr 2004 16:36:58 +0100
Subject: [R] R plot to HTML image map
In-Reply-To: <1083284238.1530.54.camel@gbic04>
References: <1083284238.1530.54.camel@gbic04>
Message-ID: <4091211A.4050505@lancaster.ac.uk>

Rudi Alberts wrote:
> Hi,
> 
> I automatically create some R matplots and now I would like to 
> automatically create HMTL image maps and make the plots clickable.
> Do you have any suggestions how to do this?

What part of the plots do you want clickable? Lines are normally a bit 
thin for clickable imagemaps...

  I wrote a library to help me produce imagemaps from R code, its here:

http://www.maths.lancs.ac.uk/Software/Imagemap/

  along with an example.

  I wrote the code a while ago, and I can't remember how it works, and 
you need the png() graphics device - which needs an X11 display, so its 
tricky to do this from a console or server application. You need to let 
it talk to an X virtual frame buffer in this case.

  The tricky bit, I recall vaguely, was mapping device coords to pixel 
coords on the PNG device.

  Good luck...

Baz



From Ted.Harding at nessie.mcc.ac.uk  Thu Apr 29 17:39:04 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 29 Apr 2004 16:39:04 +0100 (BST)
Subject: [R] p-values
In-Reply-To: <Pine.A41.4.58.0404290658570.26066@homer04.u.washington.edu>
Message-ID: <XFMail.040429163904.Ted.Harding@nessie.mcc.ac.uk>

On 29-Apr-04 Thomas Lumley wrote:
> On Thu, 29 Apr 2004, John Maindonald wrote:
> 
>> This is, of course, not strictly about R.  But if there should be
>> a decision to pursue such matters on this list, then we'd need
>> another list to which such discussion might be diverted.
>>
> 
> Ted Harding started such a list (stats-discuss) quite some time ago.
> IIRC it was to divert discussions like this from allstat.

I did indeed! But it hardly ever received any postings -- my suspicion,
which was reinforced by private comments from a number of people, was
that because it ws *not* allstat (and therefore would not catch the eye
of UK people that posters might hope to reach) it could not be expected
to. As one person put it: "One list to find them all ... ".

The R list is special, in many ways, and you can get views and information
on practically anything from some of the best in the world, so long as
it is R-related (even sometimes remotely). The present thread was
started by Greg Tarpinian asking a question in a place where he thought
he might get a response, even though not in an R context (though it
seems oone may develop -- linking R to Bayesian inference).

After a few public postings, interested parties have retired to another
room
(where others are welcome to join us) for a while; we now number 6.

Agreed it could be continued on another list (even stats-discuss,
which still exists though totally dormant), but this may not suit
everyone. Things are doing fine at the moment. But it might prove
to be a useful overspill area from teh R list -- someone starts
a ball rolling which doesn;t relly belong here, and other could
chase it on stats-discuss. So I'm keeping options open.

There's also a list stat-l at lists.mcgill.ca which is active, though
comfortably low-traffic.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 29-Apr-04                                       Time: 16:39:04
------------------------------ XFMail ------------------------------



From Han-Lin.lai at noaa.gov  Thu Apr 29 18:18:34 2004
From: Han-Lin.lai at noaa.gov (Han-Lin Lai)
Date: Thu, 29 Apr 2004 09:18:34 -0700
Subject: [R] Mixed-effects model for nested design data
Message-ID: <40912ADA.25408DD1@noaa.gov>

Hi,

I am using nlme for data from nested design.  That is, "tows" are nested
within "trip",  "trips" nested within "vessel", and "vessels" nested
within "season".  I also have several covariates, say "tow_time",
"latitude" and "depth"
My model is
   y = season + tow_time + latitude + depth + vessel(season) +
trip(season, vessel) + e

In SAS, the program would be

proc mixed NOCLPRINT NOITPRINT data=obtwl.x;
  class vessel trip tow season depth;
  model y = season depth latitude /solution;  <----------fixed effects
  random vessel(season) trip(season vessel);
run;

My question is:  How this nested mixed-effects model can be fitted in R-
"nlme"?

Thanks in advance for the helps.

Cheers!
Han

From LUCKE at uthscsa.edu  Thu Apr 29 18:36:10 2004
From: LUCKE at uthscsa.edu (Lucke, Joseph F)
Date: Thu, 29 Apr 2004 11:36:10 -0500
Subject: [R] p-values
Message-ID: <C4A57662D47C7B44B781D39E4C8F06940E31B6@SAIGA.win.uthscsa.edu>

One might begin by considering _conditional_ p-values as elaborated by
Hubbard and Bayarri and especially Sellke, Bayarri, and Berger.

Record Number: 1545
@article{
Hubbard2003,
   Author = {Hubbard, R. and Bayarri, M. J.},
   Title = {Confusion over measures of evidence ($p$)'s versus errors
($\alpha$'s) in classical statistical testing},
   Journal = {The American Statistician},
   Volume = {57},
   Number = {3},
   Pages = {171--182},
   Abstract = {Confusion surrounding the reporting and interpretation of
results of classical statistical tests is widespread among applied
researchers, most of who erroneously believe that such tests are prescribed
by a single coherent theory of statistical evidence.},
 Keywords = {p-values; Bayesian analysis; Fisher; hypothesis test;
conditional error probabilities; conditional alpha; Bayes factor; posterior
probability; significance probability; significance test; Neyman-Pearson
Theory;},
 Year = {2003} }

Record Number: 1546
@article{
Sellke2001,
   Author = {Sellke, T. and Bayarri, M. J. and Berger, J. O.},
   Title = {Calibration of $p$ values for testing precise null hypotheses.},
   Journal = {The American Statistician},
   Volume = {55},
   Number = {1},
   Pages = {62--71},
   Abstract = {$P$ values are the most commonly used toll to measure
evidence against a hypothesis or hypothesized model.  Unfortunately, they
are often incorrectly viewed as an error probability for rejection of the
hypothesis or, even worse, as the posterior probability that the hypothesis
is true.},
 Keywords = {Bayes factor; Bayesian robustness; conditional alpha;
conditional error probabilities; odds;},
 Year = {2001} }


Joe

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ted Harding
Sent: Thursday, April 29, 2004 10:39 AM
To: Thomas Lumley
Cc: r-help at stat.math.ethz.ch; John Maindonald
Subject: Re:[R] p-values

On 29-Apr-04 Thomas Lumley wrote:
> On Thu, 29 Apr 2004, John Maindonald wrote:
> 
>> This is, of course, not strictly about R.  But if there should be
>> a decision to pursue such matters on this list, then we'd need
>> another list to which such discussion might be diverted.
>>
> 
> Ted Harding started such a list (stats-discuss) quite some time ago.
> IIRC it was to divert discussions like this from allstat.

I did indeed! But it hardly ever received any postings -- my suspicion,
which was reinforced by private comments from a number of people, was
that because it ws *not* allstat (and therefore would not catch the eye
of UK people that posters might hope to reach) it could not be expected
to. As one person put it: "One list to find them all ... ".

The R list is special, in many ways, and you can get views and information
on practically anything from some of the best in the world, so long as
it is R-related (even sometimes remotely). The present thread was
started by Greg Tarpinian asking a question in a place where he thought
he might get a response, even though not in an R context (though it
seems oone may develop -- linking R to Bayesian inference).

After a few public postings, interested parties have retired to another
room
(where others are welcome to join us) for a while; we now number 6.

Agreed it could be continued on another list (even stats-discuss,
which still exists though totally dormant), but this may not suit
everyone. Things are doing fine at the moment. But it might prove
to be a useful overspill area from teh R list -- someone starts
a ball rolling which doesn;t relly belong here, and other could
chase it on stats-discuss. So I'm keeping options open.

There's also a list stat-l at lists.mcgill.ca which is active, though
comfortably low-traffic.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 29-Apr-04                                       Time: 16:39:04
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From s-plus at wiwi.uni-bielefeld.de  Thu Apr 29 18:52:56 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Thu, 29 Apr 2004 18:52:56 +0200
Subject: [R] GUI checkbox linked to entry
References: <200404291247.i3TClfMR011493@kama.imag.fr>
Message-ID: <409132E8.3050703@wiwi.uni-bielefeld.de>

Julien Glenat wrote:

>Hi , i am using R 1.8 and tcltk library in order to make a GUI and i would 
>like to know if it is possible to link a check box to one or more entry 
>dynamically (and eventually other widgets) .
>( in concrete terms : if the check box is ticked the entry is avaible for 
>input and unavaible when not ticked)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
Are you looking for something like this?

require(tcltk)
cbValue <- tclVar("0"); info<-tclVar("initial text")
top <- tktoplevel()
cb  <- tkcheckbutton(top,variable=cbValue,text="write or not")
en  <- tkentry      (top,textvariable=info,state="disabled")
exit<- tkbutton     (top, text="exit", command=function() tkdestroy(top))
tkpack(cb,en,exit)

set.entry.state<-function(){
    on.off<-tclvalue(cbValue)
    if(on.off=="1") tkconfigure(en, state="normal")
    if(on.off=="0") tkconfigure(en, state="disabled")
    print(on.off)
}
tkbind(top,"<1>",set.entry.state)

Do you know http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples?

Peter Wolf



From godbless_maria at hotmail.com  Thu Apr 29 22:27:56 2004
From: godbless_maria at hotmail.com (Maria Gu)
Date: Thu, 29 Apr 2004 20:27:56 +0000
Subject: [R] Probability(Markov chain transition matrix)
Message-ID: <BAY2-F163Hv032jUTK3000341b1@hotmail.com>

Hello, My name is Maria, MBA student in Sanfransisco, USA.
In my credit scoring class, I have hard time making "transition matrix", 
which explains probability especially in relation to "Markov chain model". 
It is regarding people's monthly credit payment behavior. Does R have 
function to caculate it? I am actually a novice in using 'R'. Please help 
me!!!

Maria Gu



From dyang at NRCan.gc.ca  Thu Apr 29 22:45:20 2004
From: dyang at NRCan.gc.ca (Yang, Richard)
Date: Thu, 29 Apr 2004 16:45:20 -0400
Subject: [R] Plot.lme error
Message-ID: <F0E0B899CB43D5118D220002A55113CF04FE559B@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear All;

	Attempting to reproduce Figure 4.15 of MEMSS (p. 171) in R using

             plot(Wafer, outer = ~ Wafer)
yields an error:

> plot(Wafer, outer = ~ Wafer)
Error in order(na.last, decreasing, ...) : 
        Argument lengths differ

	The plot() produces the figure without problems for all versions of
Splus (4.5 to 6.2) on  Windows. Noticing that the plot() differs slightly
between R and Splus, I copied the function from Splus, renamed it myplot()
in R and ran but without luck;

> myplot(Wafer, outer = ~ Wafer)
Error in myplot(Wafer, outer = ~Wafer) : Covariate must be numeric.

Coercing the covariate into numeric, but...

> myplot(Wafer, outer = ~ Wafer)
Error in !outer : invalid argument type

	Any suggestions and work around for the problem? I found the problem
dated back to R.1.4.1

	TIA,

Richard



From deepayan at stat.wisc.edu  Thu Apr 29 23:22:42 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 29 Apr 2004 16:22:42 -0500
Subject: [R] Plot.lme error
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF04FE559B@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
References: <F0E0B899CB43D5118D220002A55113CF04FE559B@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
Message-ID: <200404291622.42995.deepayan@stat.wisc.edu>

On Thursday 29 April 2004 15:45, Yang, Richard wrote:
> Dear All;
>
> 	Attempting to reproduce Figure 4.15 of MEMSS (p. 171) in R using
>
>              plot(Wafer, outer = ~ Wafer)
>
> yields an error:
> > plot(Wafer, outer = ~ Wafer)
>
> Error in order(na.last, decreasing, ...) :
>         Argument lengths differ

Something's going wrong, but I don't know what. 

plot(Wafer, displayLevel = 1, key = FALSE)

seems to reproduce the figure correctly. I don't think figuring out the 
problem is worth the effort at this point.

Deepayan



From kjetil at acelerate.com  Thu Apr 29 23:32:46 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Thu, 29 Apr 2004 17:32:46 -0400
Subject: [R] Probability(Markov chain transition matrix)
In-Reply-To: <BAY2-F163Hv032jUTK3000341b1@hotmail.com>
Message-ID: <40913C3E.32399.166C9DA@localhost>

On 29 Apr 2004 at 20:27, Maria Gu wrote:

> Hello, My name is Maria, MBA student in Sanfransisco, USA.
> In my credit scoring class, I have hard time making "transition
> matrix", which explains probability especially in relation to "Markov
> chain model". It is regarding people's monthly credit payment
> behavior. Does R have function to caculate it? I am actually a novice
> in using 'R'. Please help me!!!
> 
> Maria Gu
> 

Hola Maria, 

you need to be more specific in what you want. Do you want to 
estimate a markov transistion matrix from data? Do you want to 
simulate from a Markoc chain model? Whatever you want, it is 
likely possible in R, but we need more information.

If you want to simulate from a model, maybe something like:

> P <- matrix(c(0.1, 0.4, 0.5, 
+               0.9, 0.05, 0.05, 
+               0.3, 0.6, 0.1),3,3, byrow=TRUE)
> simMarkov <- function(P, init, n) {
+    p <- dim(P)[1]
+    chain <- numeric(length=n)
+    chain[1] <- init
+    for (i in 2:n) {
+      chain[i] <- sample(1:p, 1, prob=P[chain[i-1],])
+    }
+    return(chain)
+ }
> simMarkov(P,1,20)
 [1] 1 2 1 3 2 2 2 1 2 1 2 1 3 1 2 1 3 2 1 1
> 

Kjetil Halvorsen

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From petzoldt at rcs.urz.tu-dresden.de  Thu Apr 29 23:36:36 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 29 Apr 2004 23:36:36 +0200
Subject: [R] Stichprobe bilden
In-Reply-To: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>
References: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>
Message-ID: <40917564.5040609@rcs.urz.tu-dresden.de>

sebastian.goeres at student.unisg.ch wrote:

> Liebe Alle,
> 
> ich habe ien Problemm. ich habe einen Panel-Datensatz ueber die 185 
> Staaten (nach Code von IMF) und von 1948-1999. Ich habe auch 12 Variablen 
> in diesem Datensatz, die ich fuer die Regression benoetige. ich muss ein 
> sub-sample bilden fuer die 65 Staaten und fuer den 1970-1999 Jahren. Wie 
> kann ich aus dieser Stichprobe eine kleienere Stichprobe mit R bilden, 
> damit auch meine 12 Variablen in Bezug auf die Jahren und Laendern richtig 
> selektiert wurden. 

If you want to make a subsample, you can select cases (rows) using 
sub(), see ?sub for details, e.g. somethink like:

sumsamp <- sub(dataset, year %in% 1970:1999
                       & state %in% c("UK", "USA", "CH", "DE", "IT"))


Hope it helps!

Thomas P.



From tlumley at u.washington.edu  Thu Apr 29 23:51:46 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 29 Apr 2004 14:51:46 -0700 (PDT)
Subject: [R] Stichprobe bilden
In-Reply-To: <40917564.5040609@rcs.urz.tu-dresden.de>
References: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>
	<40917564.5040609@rcs.urz.tu-dresden.de>
Message-ID: <Pine.A41.4.58.0404291451220.201644@homer05.u.washington.edu>

On Thu, 29 Apr 2004, Thomas Petzoldt wrote:
>
> If you want to make a subsample, you can select cases (rows) using
> sub(), see ?sub for details, e.g. somethink like:
>

I think you mean subset().  sub() is a regular expression substitution
function.

	-thomas



From james at staarfunds.com  Fri Apr 30 00:12:53 2004
From: james at staarfunds.com (Jim Thomas)
Date: Thu, 29 Apr 2004 18:12:53 -0400
Subject: [R] openMosix vs SNOW: redhat kernel causing slowdown?
Message-ID: <40917DE5.4080101@staarfunds.com>

Hi there,

We're currently attempting to explain a slowdown of an LVQ-type parallel 
analysis we're working on.  We are benchmarking our analysis running 
over openMosix against the same running via SNOW for R.  Both perform 
similarly on small datasets, but on large datasets SNOW drastically 
outperforms openMosix.  However, these results are achieve running SNOW 
on the default RedHat kernel (Enterprise Edition, RHEL-3) and mosix on 
the same kernel patched with the rpm for RedHat 9 from sourceforge. 
 Even though the rpm seems to be compatible and everything runs fine, 
when the same SNOW analysis is run under the patched kernel there is an 
enormous slowdown in runtime, to the point that openMosix outperforms SNOW.

Is there some sort of incompatibility issue with the RedHat kernels 
(either general or specific to this version) that would cause a slowdown 
of this kind?  I know that Enterprise Edition was marketed as having 
tools to increase java clustering speeds, would this interfere with the 
type of socket communication SNOW uses?

Thanks,
Jim



From pnick at virgilio.it  Fri Apr 30 01:42:06 2004
From: pnick at virgilio.it (pnick@virgilio.it)
Date: Fri, 30 Apr 2004 01:42:06 +0200
Subject: [R] ARIMAX-ARCH
Message-ID: <4086F5AC0000E26F@ims2b.cp.tin.it>

hi everybody
i have to fit an ARIMAX-ARCH model, if anybody knows about  the existence
of a function that can do it or has any suggestion please mail me.
thanks a lot 
        Niccolo



From hodgess at gator.uhd.edu  Fri Apr 30 02:24:33 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 29 Apr 2004 19:24:33 -0500
Subject: [R] ARIMAX-ARCH
Message-ID: <200404300024.i3U0OXu25738@gator.dt.uh.edu>

Please try the 
library(tseries)

There are functions for garch in there.



From jqxin at iastate.edu  Fri Apr 30 03:21:50 2004
From: jqxin at iastate.edu (xin)
Date: Thu, 29 Apr 2004 20:21:50 -0500
Subject: [R] how to use biplot to show PC1 and PC3 or else?
References: <200404300024.i3U0OXu25738@gator.dt.uh.edu>
Message-ID: <005401c42e51$838b7540$2c02a8c0@holysmoke>

hello:
I am trying to use biplot show relationship between PC1 and PC3 of princomp.
There is example showing how to do it with PC1 and PC2. But I am totally out
of idea for PC1 and PC3. Thanks very much for your kind help.

Best Wishes,

yours,
xin



From wcvinyard at earthlink.net  Fri Apr 30 04:32:13 2004
From: wcvinyard at earthlink.net (Bill Vinyard)
Date: Thu, 29 Apr 2004 22:32:13 -0400
Subject: [R] how to use biplot to show PC1 and PC3 or else?
In-Reply-To: <005401c42e51$838b7540$2c02a8c0@holysmoke>
Message-ID: <MJENLJEPCHEMCAGNPDMGMEBKCCAA.wcvinyard@earthlink.net>

 xin,

 data(USArrests)
 biplot(princomp(USArrests)) # This plots the default 1st and 2nd components

 biplot(princomp(USArrests),choices=c(1,3)) # use "choices" to plot other
components...in this case 1st and 3rd

Bill

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of xin
Sent: Thursday, April 29, 2004 21:22
To: r-help at stat.math.ethz.ch
Subject: [R] how to use biplot to show PC1 and PC3 or else?


hello:
I am trying to use biplot show relationship between PC1 and PC3 of princomp.
There is example showing how to do it with PC1 and PC2. But I am totally out
of idea for PC1 and PC3. Thanks very much for your kind help.

Best Wishes,

yours,
xin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rossini at blindglobe.net  Fri Apr 30 05:04:54 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 29 Apr 2004 20:04:54 -0700
Subject: [R] graph algorithms in R
In-Reply-To: <409111BD.40603@cebitec.uni-bielefeld.de> (Andre Skusa's
	message of "Thu, 29 Apr 2004 16:31:25 +0200")
References: <409111BD.40603@cebitec.uni-bielefeld.de>
Message-ID: <85y8oei2sp.fsf@servant.blindglobe.net>


You found graph, but there is also an R API for the Boost.Graph
libraries, RBGL, there.

best,
-tony

Andre Skusa <Andre.Skusa at cebitec.uni-bielefeld.de> writes:

> Dear R users,
>
> just a quick question: Is there a reliable and good graph library for
> R, eg. with shortest path algorithms on adjacency matrixes? I already
> found a graph package as part of "Bioconductor". Are there more?
>
> If anyone knows and would tell me, I appreciate very much!
>
> Cheers,
>
> Andre
>
> -- 
> Dipl.-Inform. Andre Skusa (PhD student)
>
> NRW Graduate School in Bioinformatics and Genome Research
> Center of Biotechnology (CeBiTec)
> University of Bielefeld
> Postfach 10 01 31
> D-33501 Bielefeld
> Germany
>
> home: http://www.cebitec.uni-bielefeld.de/~askusa
>
> email: andre.skusa at cebitec.uni-bielefeld.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From phddas at yahoo.com  Fri Apr 30 06:52:05 2004
From: phddas at yahoo.com (Fred J.)
Date: Thu, 29 Apr 2004 21:52:05 -0700 (PDT)
Subject: [R] Hankel Matrix
Message-ID: <20040430045205.99248.qmail@web20506.mail.yahoo.com>

Hello
is there a function in R to generate Hankel Matrix?

thanks



From Lorenz.Gygax at fat.admin.ch  Fri Apr 30 07:24:11 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Fri, 30 Apr 2004 07:24:11 +0200
Subject: [R] Mixed-effects model for nested design data
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DAEE5@evd-s7014.evd.admin.ch>


Dear Han,

> I am using nlme for data from nested design.  That is, "tows" are nested
> within "trip",  "trips" nested within "vessel", and "vessels" nested
> within "season".  I also have several covariates, say "tow_time",
> "latitude" and "depth"
> My model is
>    y = season + tow_time + latitude + depth + vessel(season) +
> trip(season, vessel) + e
> In SAS, the program would be
> proc mixed NOCLPRINT NOITPRINT data=obtwl.x;
>   class vessel trip tow season depth;
>   model y = season depth latitude /solution;  <----------fixed effects
>   random vessel(season) trip(season vessel);
> run;
> My question is:  How this nested mixed-effects model can be 
> fitted in R-> "nlme"?

I do not know about SAS but I would guess that your model should be fitted
as something like:

lme (fixed= y ~ season + tow_time + latitude + depth,
     random= ~ 1 | season/vessel/trip)

Maybe you should do some reading in the book by Pinheiro & Bates?
They explain well how to set up models.

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Tag der offenen T??r, 11./12. Juni 2004: http://www.fat.ch/2004

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From thpe at hhbio.wasser.tu-dresden.de  Fri Apr 30 07:51:13 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 30 Apr 2004 07:51:13 +0200
Subject: [R] Stichprobe bilden
In-Reply-To: <Pine.A41.4.58.0404291451220.201644@homer05.u.washington.edu>
References: <OF3FDE3E46.8A4271E8-ONC1256E81.0038BC39-C1256E81.00397295@unisg.ch>	<40917564.5040609@rcs.urz.tu-dresden.de>
	<Pine.A41.4.58.0404291451220.201644@homer05.u.washington.edu>
Message-ID: <4091E951.9050902@hhbio.wasser.tu-dresden.de>

Thomas Lumley wrote:
> On Thu, 29 Apr 2004, Thomas Petzoldt wrote:
> 
>>If you want to make a subsample, you can select cases (rows) using
>>sub(), see ?sub for details, e.g. somethink like:
>>
> 
> 
> I think you mean subset().  sub() is a regular expression substitution
> function.

Absolutely sure! I should not try to answer questions late at night.

Thomas P.



From phddas at yahoo.com  Fri Apr 30 07:56:59 2004
From: phddas at yahoo.com (Fred J.)
Date: Thu, 29 Apr 2004 22:56:59 -0700 (PDT)
Subject: [R] absolute value
Message-ID: <20040430055659.14252.qmail@web20505.mail.yahoo.com>

Hello

could you please tell me what is the function to get
the absolute value of the real or complex number.
most of other languages it is abs(x) , what is it in
r?

I did few searhces in the help docs for no avail.

thanks
F.J



From alan.arnholt at unavarra.es  Fri Apr 30 08:18:07 2004
From: alan.arnholt at unavarra.es (alan.arnholt@unavarra.es)
Date: Fri, 30 Apr 2004 08:18:07 MET
Subject: [R] Exact Binomial test feature or bug?
Message-ID: <200404300618.IAA19106@unavarra.es>

Dear R Users,

Is the p-value reported in a two-tailed binomial exact 
test in error or is it a feature?  
If it is a feature, could someone provide a reference 
for its two-tailed p-value computations?
Using Blaker's (2000 - Canad. J. Statist 28: 783-798) 
approach,the p-value is the minimum of the two-tailed 
probabilities $P \left(Y\geq y_{obs}\right)$ and 
$P\left(Y\leq y_{obs}\right)$ plus an attainable 
probability in the other tail that is as close as 
possible to, but not greater than that one-tailed 
probability.  Consider the following examples 
performed in R version 1.9
under windows 2000.

> binom.test(110,500,.2)

        Exact binomial test

data:  110 and 500 
number of successes = 110, number of trials = 500, p-
value = 0.2636
alternative hypothesis: true probability of success is 
not equal to 0.2 
95 percent confidence interval:
 0.1844367 0.2589117 
sample estimates:
probability of success 
                  0.22 

(P-value should be according to the Blaker definition 
0.2881)

> binom.test(90,500,.2)

        Exact binomial test

data:  90 and 500 
number of successes = 90, number of trials = 500, p-
value = 0.2881
alternative hypothesis: true probability of success is 
not equal to 0.2 
95 percent confidence interval:
 0.1473006 0.2165364 
sample estimates:
probability of success 
                  0.18 

(P-value should be according to the Blaker definition 
0.2647)


The following code (only checked on windows) reports 
the Blaker definition of the p-value. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

bino.test <- 
function(x, n, p = 0.5, alternative = c
("two.sided", "less", "greater"), conf.level = 0.95)
{
	eps <- sqrt(.Machine$double.eps)
	if(any(is.na(x) || (x < 0) || (x != round(x))))
		stop("x must be nonnegative and 
integer")
	if(length(x) == 2) {
		n <- sum(x)
		x <- x[1]
	}
	else if(length(x) == 1) {
		if((length(n) > 1) || is.na(n) || (n < 
1) || (n != round(n)) || (x > n))
			stop("n must be a positive 
integer >= x")
	}
	else stop("incorrect length of x")
	if(!missing(p) && (length(p) > 1 || is.na(p) 
|| p < 0 || p > 1))
		stop("p must be a single number 
between 0 and 1")
	alternative <- match.arg(alternative)
	if(!((length(conf.level) == 1) && is.finite
(conf.level) && (conf.level > 0) && 

(conf.level < 1)))
		stop("conf.level must be a single 
number between 0 and 1")
	DNAME <- paste(deparse(substitute(x)), "and", 
deparse(substitute(n)))
	PVAL <- switch(alternative,
		less = pbinom(x, n, p),
		greater = 1 - pbinom(x - 1, n, p),
		{
			pvec <- pbinom(0:n, n, p)
			if(x/n < p) {
				pb <- pbinom(x, n, p)
				p2 <- max((1 - pvec)
[pb - (1 - pvec) >=  - eps * pb], 0)
				min(1, pb + p2)
			}
			else {
				pb <- (1 - pbinom(x - 
1, n, p))
				p2 <- max(pvec[pb - 
pvec >=  - eps * pb], 0)
				min(1, pb + p2)
			}
		}
		)
	p.L <- function(x, n, alpha)
	{
		if(x == 0)
			0
		else qbeta(alpha, x, n - x + 1)
	}
	p.U <- function(x, n, alpha)
	{
		if(x == n)
			1
		else qbeta(1 - alpha, x + 1, n - x)
	}
	CINT <- switch(alternative,
		less = c(0, p.U(x, n, 1 - conf.level)),
		greater = c(p.L(x, n, 1 - conf.level), 
1),
		two.sided = {
			alpha <- (1 - conf.level)/2
			c(p.L(x, n, alpha), p.U(x, n, 
alpha))
		}
		)
	attr(CINT, "conf.level") <- conf.level
	ESTIMATE <- x/n
	names(x) <- "number of successes"
	names(n) <- "number of trials"
	names(ESTIMATE) <- names(p) <- "probability of 
success"
	structure(list(statistic = x, parameter = n, 
p.value = PVAL, conf.int = CINT, estimate = 

ESTIMATE,
		null.value = p, alternative = 
alternative, method = "Exact binomial test", 

data.name = DNAME),
		class = "htest")
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Thanks in advance for the responses.  I am not 
currently on the R-mailing list. So, if you could 

cc your responses to alan.arnholt at unavarra.es I would 
be most thankful.

Alan Arnholt



---------------------------------------------
This message was sent using Endymion MailMan.
http://www.endymion.com/products/mailman/



From petr.pikal at precheza.cz  Fri Apr 30 08:19:00 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 30 Apr 2004 08:19:00 +0200
Subject: [R] absolute value
In-Reply-To: <20040430055659.14252.qmail@web20505.mail.yahoo.com>
Message-ID: <40920BF4.27757.1CBF40@localhost>

Hi
Did you try 
help.search("abs")

gives me 

abs(base)                               Miscellaneous Mathematical Functions
meanabsdev(cluster)                     Internal cluster functions
absolute.size(grid)                     Absolute Size of a Grob
abs.error.pred(hmisc)                   Indexes of Absolute Prediction Error for Linear 
Models
corresp(MASS)                           Simple Correspondence Analysis
crabs(MASS)                             Morphological Measurements on Leptograpsus 
Crabs
loglm1(MASS)                            Fit Log-Linear Models by Iterative Proportional 
Scaling -- Internal function
quine(MASS)                             Absenteeism from School in Rural New South 
Wales
HTML(R2HTML)                            Outputs an object to a HTML file
mad(stats)                              Median Absolute Deviation
xtabs(stats)                            Cross Tabulation
filePathAsAbsolute(tools)               File Utilities

or
?abs

You should enhance your search utility
Cheers
Petr


On 29 Apr 2004 at 22:56, Fred J. wrote:

> Hello
> 
> could you please tell me what is the function to get
> the absolute value of the real or complex number.
> most of other languages it is abs(x) , what is it in
> r?
> 
> I did few searhces in the help docs for no avail.
> 
> thanks
> F.J
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From vito.muggeo at giustizia.it  Fri Apr 30 08:25:44 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Fri, 30 Apr 2004 08:25:44 +0200
Subject: R: [R] absolute value
References: <20040430055659.14252.qmail@web20505.mail.yahoo.com>
Message-ID: <010901c42e7b$fa92db40$5c13070a@PROCGEN>

Are you looking for Re() and friends?

Toy examples:

> abs(Re(3+4i))
[1] 3
> abs(Re(-3+4i))
[1] 3
> abs(Re(3))
[1] 3

see ?complex  for further details on complex numbers in R

vito

----- Original Message -----
From: Fred J. <phddas at yahoo.com>
To: r help <r-help at stat.math.ethz.ch>
Sent: Friday, April 30, 2004 7:56 AM
Subject: [R] absolute value


> Hello
>
> could you please tell me what is the function to get
> the absolute value of the real or complex number.
> most of other languages it is abs(x) , what is it in
> r?
>
> I did few searhces in the help docs for no avail.
>
> thanks
> F.J
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Apr 30 09:56:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Apr 2004 09:56:40 +0200
Subject: [R] absolute value
In-Reply-To: <20040430055659.14252.qmail@web20505.mail.yahoo.com>
References: <20040430055659.14252.qmail@web20505.mail.yahoo.com>
Message-ID: <x2u0z1yk3r.fsf@biostat.ku.dk>

"Fred J." <phddas at yahoo.com> writes:

> Hello
> 
> could you please tell me what is the function to get
> the absolute value of the real or complex number.
> most of other languages it is abs(x) , what is it in
> r?
> 
> I did few searhces in the help docs for no avail.

Will you believe it?

> abs(1+1i)
[1] 1.414214

In the complex case some may prefer Mod(x) which does the same thing.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Apr 30 10:16:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Apr 2004 10:16:16 +0200
Subject: [R] Exact Binomial test feature or bug?
In-Reply-To: <200404300618.IAA19106@unavarra.es>
References: <200404300618.IAA19106@unavarra.es>
Message-ID: <x2pt9pyj73.fsf@biostat.ku.dk>

alan.arnholt at unavarra.es writes:

> Dear R Users,
> 
> Is the p-value reported in a two-tailed binomial exact 
> test in error or is it a feature?

A feature. Two sided exact tests are really a contradiction in terms
since there are multiple ways of doing it and confidence intervals are
even worse. We do at times consider making the different methods
selectable. There's a similar issue with fisher.test.
  
> If it is a feature, could someone provide a reference 
> for its two-tailed p-value computations?

It's just the sum of probabilities of outcomes with a smaller
likelihood. (In fisher.test for more than 2x2 tables that is basically
the only thing you can do, so it does give some consistency.)

> Using Blaker's (2000 - Canad. J. Statist 28: 783-798) 
> approach,the p-value is the minimum of the two-tailed 
> probabilities $P \left(Y\geq y_{obs}\right)$ and 
> $P\left(Y\leq y_{obs}\right)$ plus an attainable 
> probability in the other tail that is as close as 
> possible to, but not greater than that one-tailed 
> probability. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From r-stats at arcriswell.com  Fri Apr 30 10:36:45 2004
From: r-stats at arcriswell.com (Andrew Criswell)
Date: Fri, 30 Apr 2004 15:36:45 +0700
Subject: [R] Arrow movements in R
Message-ID: <4092101D.3020901@arcriswell.com>

Hello:

I apologize for asking this question again.

I recently upgraded to R-1.9.1 but now the arrow keys do not scroll up 
and down the command lines in R. Instead I get  ^[[A

My SuSe 9 platform is missing some program. What is it?

Thanks,
ANDREW



From jasont at indigoindustrial.co.nz  Fri Apr 30 10:59:47 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 30 Apr 2004 20:59:47 +1200 (NZST)
Subject: [R] Arrow movements in R
In-Reply-To: <4092101D.3020901@arcriswell.com>
References: <4092101D.3020901@arcriswell.com>
Message-ID: <31698.203.9.176.60.1083315587.squirrel@webmail.maxnet.co.nz>

> I recently upgraded to R-1.9.1 but now the arrow keys do not scroll up
> and down the command lines in R. Instead I get  ^[[A
>
> My SuSe 9 platform is missing some program. What is it?

The readline-devel rpm.

Is this FA'd enough to be a FAQ?

Jason



From p.dalgaard at biostat.ku.dk  Fri Apr 30 11:10:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Apr 2004 11:10:53 +0200
Subject: [R] Arrow movements in R
In-Reply-To: <4092101D.3020901@arcriswell.com>
References: <4092101D.3020901@arcriswell.com>
Message-ID: <x2fzalygo2.fsf@biostat.ku.dk>

Andrew Criswell <r-stats at arcriswell.com> writes:

> Hello:
> 
> I apologize for asking this question again.
> 
> I recently upgraded to R-1.9.1 but now the arrow keys do not scroll up
> and down the command lines in R. Instead I get  ^[[A

The time traveller strikes again? 1.9.1 is tentatively slated for mid-June...
 
> My SuSe 9 platform is missing some program. What is it?

readline-devel, as others have said. But why not use Detlefs RPM?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Apr 30 11:13:15 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Apr 2004 11:13:15 +0200
Subject: [R] Arrow movements in R
In-Reply-To: <31698.203.9.176.60.1083315587.squirrel@webmail.maxnet.co.nz>
References: <4092101D.3020901@arcriswell.com>
	<31698.203.9.176.60.1083315587.squirrel@webmail.maxnet.co.nz>
Message-ID: <x2brl9ygk4.fsf@biostat.ku.dk>

"Jason Turner" <jasont at indigoindustrial.co.nz> writes:

> > I recently upgraded to R-1.9.1 but now the arrow keys do not scroll up
> > and down the command lines in R. Instead I get  ^[[A
> >
> > My SuSe 9 platform is missing some program. What is it?
> 
> The readline-devel rpm.
> 
> Is this FA'd enough to be a FAQ?

Yes... (7.22 to be specific. Wording could be better, I suppose.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ben.Flood at ul.ie  Fri Apr 30 11:15:25 2004
From: Ben.Flood at ul.ie (Ben.Flood)
Date: Fri, 30 Apr 2004 10:15:25 +0100
Subject: [R] dlls
Message-ID: <0B2F845E9599D611A6FA00B0D0D1DE4A01FB9BE6@exch-staff4.ul.ie>

Hi,
	I am trying to run an algorithm in a large dataset and want to speed
this process up by using C/C++ functions.  I have been attempting to create
a dll library using Microsoft Visual C++ in Windows and call that using
dyn.load("file.dll") but when I use is.loaded("function") I invariably get
FALSE.  
	If anybody has done this and could give me more detailed
instructions it would be very much appreciated,
	best regards.



From jasonparcon at yahoo.com  Fri Apr 30 04:37:40 2004
From: jasonparcon at yahoo.com (Jason Parcon)
Date: Thu, 29 Apr 2004 19:37:40 -0700 (PDT)
Subject: [R] Help
Message-ID: <20040430023740.9852.qmail@web60302.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040429/0755a100/attachment.pl

From ripley at stats.ox.ac.uk  Fri Apr 30 11:33:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 10:33:22 +0100 (BST)
Subject: [R] dlls
In-Reply-To: <0B2F845E9599D611A6FA00B0D0D1DE4A01FB9BE6@exch-staff4.ul.ie>
Message-ID: <Pine.LNX.4.44.0404301028190.3510-100000@gannet.stats>

On Fri, 30 Apr 2004, Ben.Flood wrote:

> 	I am trying to run an algorithm in a large dataset and want to speed
> this process up by using C/C++ functions.  I have been attempting to create
> a dll library using Microsoft Visual C++ in Windows and call that using
> dyn.load("file.dll") but when I use is.loaded("function") I invariably get
> FALSE.  
> 	If anybody has done this and could give me more detailed
> instructions it would be very much appreciated,

It's a `Microsoft Visual C++ in Windows' issue, not an R issue.  I expect
you are not exporting any entry points (the default under that `system',
amazingly): readme.packages tells you how to use VC++ in detail, and there
are worked examples in the on-line complements to `S Programming'.

Do you have a very good reason to make things difficult for yourself by 
not using the recommended compilers?  Just a thought ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Apr 30 11:37:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 10:37:36 +0100 (BST)
Subject: [R] rank of matrix (was Help)
In-Reply-To: <20040430023740.9852.qmail@web60302.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0404301033410.3510-100000@gannet.stats>

On Thu, 29 Apr 2004, Jason Parcon wrote:

> To whom it may concern:

>  Is there a function that we can use to easily obtain the rank of a
> matrix in R?

Yes.  See ?qr.  Note that the `rank' is not really well-defined for a 
numerical matrix due to imprecise representation, and it may well be 
better to look at the SVD (?svd) and consider the sizes of the singular 
values.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do!  In particular, use a sensible subject line.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From erich.neuwirth at univie.ac.at  Fri Apr 30 12:04:09 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 30 Apr 2004 12:04:09 +0200
Subject: [R] Hankel Matrix
In-Reply-To: <20040430045205.99248.qmail@web20506.mail.yahoo.com>
References: <20040430045205.99248.qmail@web20506.mail.yahoo.com>
Message-ID: <40922499.7010006@univie.ac.at>

hankel<-function(seq){
outer(1:((length(seq)+1)/2),1:((length(seq)+1)/2),
     function(x,y) seq[x+y-1])
}


will take a sequence of odd length and ouput the corresponding
square Hankel matrix.


Fred J. wrote:

 > Hello
 > is there a function in R to generate Hankel Matrix?



-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From v.demartino2 at virgilio.it  Fri Apr 30 15:14:27 2004
From: v.demartino2 at virgilio.it (Vittorio)
Date: Fri, 30 Apr 2004 14:14:27 +0100
Subject: [R] Slicing an area....
In-Reply-To: <4087DF450001A1D2@ims3e.cp.tin.it>
References: <4087DF450001A1D2@ims3e.cp.tin.it>
Message-ID: <200404301414.27289.v.demartino2@virgilio.it>

On Wednesday 28 April 2004 17:32, v.demartino2 at virgilio.it wrote:
> I have interpolated a large number of data and obtained a regular x-y curve
> (and its interpolated function).
> My problem is that I need to slice the entire area delimited by the curve,
> the x-axis and the two finite extremes of the curve  (starting from the
> very bottom of the entire area) into smaller stripes of given (and
> different) areas.
>
> How can I do it?
>
> Vittorio
>
Sorry friends.

THIS MESSAGE WASN'T INTENDED FOR THIS MAILING-LIST BUT WAS A PRIVATE 
MESSAGE TO A FRIEND OF MINE...  Just the wrong To:!!!!!!

SORRY AGAIN

Vittorio



From klaus.thul at infineon.com  Fri Apr 30 13:47:35 2004
From: klaus.thul at infineon.com (klaus.thul@infineon.com)
Date: Fri, 30 Apr 2004 13:47:35 +0200
Subject: [R] rbind with missing columns
Message-ID: <7509DD89A305F34E9EF16F1EEDFB80AF08D6D3@drsse401.eu.infineon.com>

Hello,

I have several data sets, which I want to combine column-by-column using
"rbind" (the data sets have ~100 columns). The problem is, that in some
data sets some of the columns are missing.

Simply using rbind gives me:
"Error in match.names(clabs, names(xi)) : names don't match previous
names"

Is there a simple way to make R do the rbind despite the missing columns
and to fill out the missing data with NA's? (I could program this
somehow, but probably there is one very simple solution available)

To make it clear here a simplified example. "unknown.command" is what I
am looking for.

A <- data.frame(a = c(1,2,3), b = c(4,5,6))
B <- data.frame(a = c(1,2,3))
unknown.command(A, B) - should give

A B
1 4
2 5
3 6
4 NA
5 NA
6 NA

Thank you for your help
Klaus



From ian.ke at sympatico.ca  Fri Apr 30 13:51:43 2004
From: ian.ke at sympatico.ca (Ian Kennedy)
Date: Fri, 30 Apr 2004 07:51:43 -0400
Subject: [R] Entering times around the start of daylight savings time
In-Reply-To: <p0600200abcb6cb04cba6@[128.115.153.6]>
References: <200404291028.21019.ian.ke@sympatico.ca>
	<p0600200abcb6cb04cba6@[128.115.153.6]>
Message-ID: <200404300751.43434.ian.ke@sympatico.ca>

> You might do better using seq.POSIXt. Your basic problem is that your
> sequence includes a time that doesn't exist, 02:00 on the transition
> day.

Thanks. I guess I didn't state my question clearly enough. I didn't use 
seq.POSIXt because I was interested in reading times not necessarily in a 
regular sequence. seq.POSIXt does give the even time series, as you showed, 
and it includes the 02:00 GMT time on 4 April. I chose GMT because I thought 
02:00 should exist in that time zone.

My question could be rephrased to be why do seq.POSIXt and ISOdatetime differ 
between 02:00 and 02:59:59, or at least why do they differ in Linux and 
Windows, since they don't in OS X.

> diff(ISOdatetime(2004,4,4,0:5,0,0,"GMT"))
Time differences of 3600, 7200,    0, 3600, 3600 secs
> diff(seq.POSIXt(ISOdatetime(2004,4,4,0,0,0,"GMT"),length.out=6,by="hour"))
Time differences of 1, 1, 1, 1, 1 hours

Shouldn't these be the same? They are the same on R in OS X

> Whatever the timezone, if the change from standard time to daylight
> savings time takes place at 02:00, then there are no "times between
> 2:00 and 3:00". That is, the correct sequence of times is 01:58,
> 01:59, 03:00, 03:01, etc., because at 2 AM we jump directly to 3 AM
> (that's what daylight savings time is, after all).
>
>   seq.POSIXt(ISOdatetime(2004,4,4,1,58,0),by='min',len=5)
> [1] "2004-04-04 01:58:00 PST" "2004-04-04 01:59:00 PST" "2004-04-04
> 03:00:00 PDT" "2004-04-04 03:01:00 PDT" "2004-04-04 03:02:00 PDT"
>
> When I first encountered this issue a while back, the advice was that
> if you specify times that don't exist (such as 02:00 on the
> transition day), what happens depends on your OS; and one should not
> expect it to make sense because in fact the time doesn't exist.

That is why I chose to use GMT in my example, because in addition to being the 
only time zone string I noticed in the R documentation, it shouldn't switch 
to daylight savings time and should have the hour between 2:00 and 3:00.

> A final note, in my OS X system, I get
>
> >  ISOdatetime(2004,4,4,0:4,0,0)
> [1] "2004-04-04 00:00:00 PST" "2004-04-04 01:00:00 PST" NA
> "2004-04-04 03:00:00 PDT" "2004-04-04 04:00:00 PDT"
>
> I would consider this correct, since it gives NA for
> ISOdatetime(2004,4,4,2,0,0), a time that doesn't exist. Excluding the
> NA, it results in one hour intervals.

I'd consider that correct too because it assumes you are entering times from 
your local time zone which varies between PST and PDT and has no 02:00 time 
when switching to DST. I played a bit more with OS X I found I can get three 
distinct results depending on the time zone I enter. Besides your example:

for "GMT" (I get the same result with "PST8" or "EST5"):
>  ISOdatetime(2004,4,4,0:4,0,0,"GMT")
[1] "2004-04-04 00:00:00 GMT" "2004-04-04 01:00:00 GMT"
 "2004-04-04 02:00:00 GMT"  "2004-04-04 03:00:00 GMT"
 "2004-04-04 04:00:00 GMT"

for "PST5", or apparently any timezone string the OS does not recognize: 

>  ISOdatetime(2004,4,4,0:4,0,0,"PST5")
[1] "2004-04-04 00:00:00 EST" "2004-04-04 01:00:00 EST" 
"2004-04-04 02:00:00 EST"    NA                                            NA

All of these make some sort of sense. If I enter a specific time zone such as 
"GMT" it should assume all the times are from that time zone, even if the 
place those times were recorded changes from standard time to daylight 
savings time. For the last example above, if I enter an unknown time zone, 
then it doesn't know what to do when daylight savings time would start. (As 
an aside, 

So, to add to my question above, how can I get R in Linux (or Windows for that 
matter) to behave like OS X in this matter?

Thanks again

Ian Kennedy



From dmurdoch at pair.com  Fri Apr 30 14:15:18 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 30 Apr 2004 08:15:18 -0400
Subject: [R] rbind with missing columns
In-Reply-To: <7509DD89A305F34E9EF16F1EEDFB80AF08D6D3@drsse401.eu.infineon.com>
References: <7509DD89A305F34E9EF16F1EEDFB80AF08D6D3@drsse401.eu.infineon.com>
Message-ID: <2lg4909rnn388gd4e3s93tiq0fiplbmor2@4ax.com>

On Fri, 30 Apr 2004 13:47:35 +0200, <klaus.thul at infineon.com> wrote :

>Hello,
>
>I have several data sets, which I want to combine column-by-column using
>"rbind" (the data sets have ~100 columns). The problem is, that in some
>data sets some of the columns are missing.
>
>Simply using rbind gives me:
>"Error in match.names(clabs, names(xi)) : names don't match previous
>names"
>
>Is there a simple way to make R do the rbind despite the missing columns
>and to fill out the missing data with NA's? (I could program this
>somehow, but probably there is one very simple solution available)

It's not there already as far as I know, but it's not too hard to
write a new rbind that does this:

newrbind <- function(A,B) {
  anames <- names(A)
  bnames <- names(B)
  notinb <- anames[!(anames %in% bnames)]
  if (length(notinb)) B[[notinb]] <- rep(NA, nrow(B))
  notina <- bnames[!(bnames %in% anames)]
  if (length(notina)) A[[notina]] <- rep(NA, nrow(A))
  rbind(A, B)
}

This only works on data.frames; if you want it to work as generally as
the real rbind, you'll need to add some extra conversions at the
start.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Apr 30 14:20:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 13:20:56 +0100 (BST)
Subject: [R] Rtemp directories accumulating over time
In-Reply-To: <Pine.LNX.4.44.0404290720540.11928-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0404301316150.546-100000@gannet.stats>

I've found a couple of things.  The issue is a `sharing violation', which 
although it is documented that another process is accessing the dir, it 
can be the same process.

Guido's code in R_ShowFiles opened a FindFirst handle and failed to close 
it, so sessions using file.show() on temporary files had a sharing 
violation.  Leaving a file connection open can also do this, and I found 
one example in reg-tests-1.R.

The issue remains that on most OSes you can unlink files which are in use, 
and on Windows you cannot, *and* `in use' can be in non-obvious ways and 
differ for plain files and for directories.


On Thu, 29 Apr 2004, Prof Brian Ripley wrote:

> On Wed, 28 Apr 2004 kjetil at acelerate.com wrote:
> 
> > On 28 Apr 2004 at 13:53, Prof Brian Ripley wrote:
> > 
> > > On Tue, 27 Apr 2004 kjetil at acelerate.com wrote:
> > > 
> > 
> > .
> > .
> > > I don't see why: package base is never unloaded so that hook function
> > > is never run.  (Indeed, no package/namespace is unloaded except by
> > > explicit user action, in particular not when R is terminated.)
> > > 
> > > > So there are also other tmpdirs made by R. Why, where, and why are
> > > > they not removed at exit (when their content are removed)?
> > > 
> > > They are removed by R.  This is a Windows-only bug, as Windows
> > > sometimes does not act on commands to remove empty directories (but
> > > only sometimes).
> > 
> > So is there anything I can do to remedy this nuisance (apart from 
> > reinstalling a newer XP (other messages indicates that can give 
> > surprises!) or changing OS)?
> 
> Well, there is nothing I know how to do.  Someone else may know how to 
> workaround the Windows bug.
> 
> > 
> > Kjetil Halvorsen
> > 
> >  Session temporary directories should only be left
> > > around when a session crashes.
> > > 
> > > -- 
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> > > Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> > > UK                Fax:  +44 1865 272595
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lmassis at yahoo.com.br  Fri Apr 30 13:33:22 2004
From: lmassis at yahoo.com.br (Leonard assis)
Date: Fri, 30 Apr 2004 08:33:22 -0300
Subject: [R] dlls
In-Reply-To: <Pine.LNX.4.44.0404301028190.3510-100000@gannet.stats>
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAHlebCTgy30WZ5WwsVbJtq8KAAAAQAAAArIHzSHKDW0yAzaqGLgNoEAEAAAAA@yahoo.com.br>

Is there any way to compile R 1.9 Sources in VC++ ? 


[]s
Leonard Assis
Estat??stico /  CONFE 7439
UIN : 41-764-523 /  Skype : lmassis /  msn : lmassis at msn.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof Brian Ripley
Sent: Friday, April 30, 2004 6:33 AM
To: Ben.Flood
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] dlls

On Fri, 30 Apr 2004, Ben.Flood wrote:

> 	I am trying to run an algorithm in a large dataset and want to speed

> this process up by using C/C++ functions.  I have been attempting to 
> create a dll library using Microsoft Visual C++ in Windows and call 
> that using
> dyn.load("file.dll") but when I use is.loaded("function") I invariably 
> get FALSE.
> 	If anybody has done this and could give me more detailed
instructions 
> it would be very much appreciated,

It's a `Microsoft Visual C++ in Windows' issue, not an R issue.  I expect
you are not exporting any entry points (the default under that `system',
amazingly): readme.packages tells you how to use VC++ in detail, and there
are worked examples in the on-line complements to `S Programming'.

Do you have a very good reason to make things difficult for yourself by not
using the recommended compilers?  Just a thought ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Apr 30 14:34:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 13:34:46 +0100 (BST)
Subject: [R] dlls
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAHlebCTgy30WZ5WwsVbJtq8KAAAAQAAAArIHzSHKDW0yAzaqGLgNoEAEAAAAA@yahoo.com.br>
Message-ID: <Pine.LNX.4.44.0404301326290.3849-100000@gannet.stats>

On Fri, 30 Apr 2004, Leonard assis wrote:

> Is there any way to compile R 1.9 Sources in VC++ ? 

There is no R 1.9, period!

If you mean 1.9.0, we think not.  Previous attempts have failed to produce 
a correctly working binary.  What would be the advantage even if that 
compiler worked correctly?

[Irrelevant part of another thread deleted.  The posting guide 
specifically asks you not to do what you just did, and the FAQ asks you 
not to send questions to individuals.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Fri Apr 30 14:45:15 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 30 Apr 2004 08:45:15 -0400
Subject: [R] dlls
In-Reply-To: <Pine.LNX.4.44.0404301028190.3510-100000@gannet.stats>
References: <0B2F845E9599D611A6FA00B0D0D1DE4A01FB9BE6@exch-staff4.ul.ie>
	<Pine.LNX.4.44.0404301028190.3510-100000@gannet.stats>
Message-ID: <qph490t8h9av95gpho5hupejp8od6mt1k9@4ax.com>

On Fri, 30 Apr 2004 10:33:22 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote :

>On Fri, 30 Apr 2004, Ben.Flood wrote:
>
>> 	I am trying to run an algorithm in a large dataset and want to speed
>> this process up by using C/C++ functions.  I have been attempting to create
>> a dll library using Microsoft Visual C++ in Windows and call that using
>> dyn.load("file.dll") but when I use is.loaded("function") I invariably get
>> FALSE.  
>> 	If anybody has done this and could give me more detailed
>> instructions it would be very much appreciated,
>
>It's a `Microsoft Visual C++ in Windows' issue, not an R issue.  I expect
>you are not exporting any entry points (the default under that `system',
>amazingly): readme.packages tells you how to use VC++ in detail, and there
>are worked examples in the on-line complements to `S Programming'.
>
>Do you have a very good reason to make things difficult for yourself by 
>not using the recommended compilers?  Just a thought ....

I think there are good reasons to support VC++, but not good enough
that I actually want to do it :-).  The two main reasons are:

 - it produces faster code than gcc
 - the debugger is much nicer than gdb

I think the choice not to export entry points is a good choice:
programmers should think about what parts of the interface to their
modules are visible to users.  It may feel unnatural in C, but it
really is a good idea.

If anyone who does use VC++ finds that any of the details in
readme.packages are out of date, please send me corrections.

Duncan Murdoch



From dmurdoch at pair.com  Fri Apr 30 14:51:11 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 30 Apr 2004 08:51:11 -0400
Subject: [R] dlls
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAHlebCTgy30WZ5WwsVbJtq8KAAAAQAAAArIHzSHKDW0yAzaqGLgNoEAEAAAAA@yahoo.com.br>
References: <Pine.LNX.4.44.0404301028190.3510-100000@gannet.stats>
	<!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAHlebCTgy30WZ5WwsVbJtq8KAAAAQAAAArIHzSHKDW0yAzaqGLgNoEAEAAAAA@yahoo.com.br>
Message-ID: <vii490tv77rqu5lnihr36s24kacq14enst@4ax.com>

On Fri, 30 Apr 2004 08:33:22 -0300, "Leonard assis"
<lmassis at yahoo.com.br> wrote :

>Is there any way to compile R 1.9 Sources in VC++ ? 

There probably is, but it would be so much trouble that it's probably
not worth doing.  It's certainly not something that we want to
support.  Maybe you should contact Microsoft, and get them to modify
their tools so they work on the R sources?

Duncan Murdoch



From f.calboli at ucl.ac.uk  Fri Apr 30 16:18:21 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: 30 Apr 2004 15:18:21 +0100
Subject: [R] Mixed-effects model for nested design data
In-Reply-To: <200404301005.i3UA1Jhg009439@hypatia.math.ethz.ch>
References: <200404301005.i3UA1Jhg009439@hypatia.math.ethz.ch>
Message-ID: <1083334701.3693.9.camel@monkey>

quote:

> I am using nlme for data from nested design.  That is, "tows" are
nested
> within "trip",  "trips" nested within "vessel", and "vessels" nested
> within "season".  I also have several covariates, say "tow_time",
> "latitude" and "depth"
> My model is
>    y = season + tow_time + latitude + depth + vessel(season) +
> trip(season, vessel) + e
> In SAS, the program would be
> proc mixed NOCLPRINT NOITPRINT data=obtwl.x;
>   class vessel trip tow season depth;
>   model y = season depth latitude /solution;  <----------fixed effects
>   random vessel(season) trip(season vessel);
> run;
> My question is:  How this nested mixed-effects model can be 
> fitted in R-> "nlme"?


> I do not know about SAS but I would guess that your model should be
> fitted
> as something like:
> 
> lme (fixed= y ~ season + tow_time + latitude + depth,
>      random= ~ 1 | season/vessel/trip)
> 
> Maybe you should do some reading in the book by Pinheiro & Bates?
> They explain well how to set up models.



I would create a grouped data variable, to avoid having season a both a
random and fixed effect:

your.data$SV<-getGroups(your.data, form=~1|season/vessel, level=2)

the effect is to create a variable that groups vessels %in% season. BTW,
according to your coding of the data, this stem is not always necessary.

HTH

Federico Calboli
-- 



=================================

Federico C. F. Calboli

Dipartimento di Biologia
Via Selmi 3
40126 Bologna
Italy

tel (+39) 051 209 4187
fax (+39) 051 209 4286

f.calboli at ucl.ac.uk
fcalboli at alma.unibo.it



From skip at pobox.com  Fri Apr 30 15:24:27 2004
From: skip at pobox.com (Skip Montanaro)
Date: Fri, 30 Apr 2004 08:24:27 -0500
Subject: [R] configure problem - mixed fortran/c
Message-ID: <16530.21387.428120.477777@montanaro.dyndns.org>


I'm trying to build R 1.9.0 to get the opportunity to build rpy for some
folks at my office.  (I'm a Python guy, not an R guy, so I'm completely
unfamiliar with the machinations of building R.)  I'm having trouble getting
past the configure step.  A plain old configure generates this output at the
end:

    checking whether we can compute C Make dependencies... yes, using gcc -MM
    checking whether gcc supports -c -o FILE.lo... yes
    checking how to get verbose linking output from g77... -v
    checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib -L/usr/local/lib -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2 -L/usr/ccs/bin -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s
    checking for dummy main to link with Fortran libraries... none
    checking for Fortran name-mangling scheme... lower case, underscore, extra underscore
    checking whether g77 appends underscores to external names... yes
    checking whether mixed C/Fortran code can be run... configure: WARNING: cannot run mixed C/Fortan code
    configure: error: Maybe check LDFLAGS for paths to Fortran libraries?

I'm having trouble deciphering what exactly the configure script is testing
at that point.  Based upon the message it emitted last it looks like it's
trying to link conftestf.o and conftest.o (around line 25747), but in the
config.log file I don't see it getting that far (around line 25500).  I saw
no obvious configure options which would allow me to worm around this
problem.

My environment is:

    Solaris 9 on Intel
    gcc and g77 3.3.2

Any suggestions to help get this beast built would be greatly appreciated.

Thanks,

-- 
Skip Montanaro
Got gigs? http://www.musi-cal.com/submit.html
Got spam? http://www.spambayes.org/
skip at pobox.com



From s-plus at wiwi.uni-bielefeld.de  Fri Apr 30 15:33:35 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 30 Apr 2004 15:33:35 +0200
Subject: [R] rbind with missing columns
References: <7509DD89A305F34E9EF16F1EEDFB80AF08D6D3@drsse401.eu.infineon.com>
	<2lg4909rnn388gd4e3s93tiq0fiplbmor2@4ax.com>
Message-ID: <409255AF.50207@wiwi.uni-bielefeld.de>

here is another solution:

A <- data.frame(a = c(1,2,3), b = c(4,5,6))
B <- data.frame(a = c(1,2,3))
C <- data.frame(b=4:6)

rbind.data.frame.NA<-function(...){
  N<-unique(unlist(lapply(list(...),names)))
  result<-NULL
  for(DF in list(...)){
    x<-as.data.frame(lapply(N,function(x)if(x %in% names(DF)) DF[,x] 
else NA))
    names(x)<-N
    result<-rbind(result,x)
  }
  result
}

rbind.data.frame.NA(B,A,C)

@
output-start
Fri Apr 30 15:21:21 2004
    a  b
1   1 NA
2   2 NA
3   3 NA
11  1  4
21  2  5
31  3  6
12 NA  4
22 NA  5
32 NA  6
output-end

Peter Wolf

Duncan Murdoch wrote:

>On Fri, 30 Apr 2004 13:47:35 +0200, <klaus.thul at infineon.com> wrote :
>
>  
>
>>Hello,
>>
>>I have several data sets, which I want to combine column-by-column using
>>"rbind" (the data sets have ~100 columns). The problem is, that in some
>>data sets some of the columns are missing.
>>
>>Simply using rbind gives me:
>>"Error in match.names(clabs, names(xi)) : names don't match previous
>>names"
>>
>>Is there a simple way to make R do the rbind despite the missing columns
>>and to fill out the missing data with NA's? (I could program this
>>somehow, but probably there is one very simple solution available)
>>    
>>
>
>It's not there already as far as I know, but it's not too hard to
>write a new rbind that does this:
>
>newrbind <- function(A,B) {
>  anames <- names(A)
>  bnames <- names(B)
>  notinb <- anames[!(anames %in% bnames)]
>  if (length(notinb)) B[[notinb]] <- rep(NA, nrow(B))
>  notina <- bnames[!(bnames %in% anames)]
>  if (length(notina)) A[[notina]] <- rep(NA, nrow(A))
>  rbind(A, B)
>}
>
>This only works on data.frames; if you want it to work as generally as
>the real rbind, you'll need to add some extra conversions at the
>start.
>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From apv at capital.net  Fri Apr 30 15:33:48 2004
From: apv at capital.net (Arend P. van der Veen)
Date: Fri, 30 Apr 2004 09:33:48 -0400
Subject: [R] searching a vector
Message-ID: <1083332027.54725.4.camel@localhost>

Hi,

I have a integer vector x that contains a unique set of numbers: 

x <- c(1,2,4,6,8,10,12)

Is there a simple test I can use to determine if an integer such as 6 is
contained in x ?  

Thanks in advance for any help,
Arend



From ripley at stats.ox.ac.uk  Fri Apr 30 15:37:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 14:37:28 +0100 (BST)
Subject: [R] configure problem - mixed fortran/c
In-Reply-To: <16530.21387.428120.477777@montanaro.dyndns.org>
Message-ID: <Pine.LNX.4.44.0404301430340.7690-100000@gannet.stats>

On Fri, 30 Apr 2004, Skip Montanaro wrote:

> I'm trying to build R 1.9.0 to get the opportunity to build rpy for some
> folks at my office.  (I'm a Python guy, not an R guy, so I'm completely
> unfamiliar with the machinations of building R.)  I'm having trouble getting
> past the configure step.  A plain old configure generates this output at the
> end:
> 
>     checking whether we can compute C Make dependencies... yes, using gcc -MM
>     checking whether gcc supports -c -o FILE.lo... yes
>     checking how to get verbose linking output from g77... -v
>     checking for Fortran libraries of g77...  -L/usr/ccs/lib -L/usr/lib -L/usr/local/lib -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2 -L/usr/ccs/bin -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2/../../.. -lfrtbegin -lg2c -lm -lgcc_s
>     checking for dummy main to link with Fortran libraries... none
>     checking for Fortran name-mangling scheme... lower case, underscore, extra underscore
>     checking whether g77 appends underscores to external names... yes
>     checking whether mixed C/Fortran code can be run... configure: WARNING: cannot run mixed C/Fortan code
>     configure: error: Maybe check LDFLAGS for paths to Fortran libraries?
> 
> I'm having trouble deciphering what exactly the configure script is testing
> at that point. 

Building an executable from a C and a Fortran file.

>  Based upon the message it emitted last it looks like it's
> trying to link conftestf.o and conftest.o (around line 25747), but in the
> config.log file I don't see it getting that far (around line 25500).  I saw

Without an extract from the log it is hard for us to comment.

> no obvious configure options which would allow me to worm around this
> problem.

There is one, and it told you, `check LDFLAGS for paths to Fortran
libraries'. I don't see that you have told us the result of your check, so
surmise you haven't checked, and suspect that is the problem.

> My environment is:
> 
>     Solaris 9 on Intel
>     gcc and g77 3.3.2

You need wherever libg2c.so is installed in your LD_LIBRARY_PATH.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tobias.verbeke at bivv.be  Fri Apr 30 15:41:47 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Fri, 30 Apr 2004 15:41:47 +0200
Subject: [R] searching a vector
In-Reply-To: <1083332027.54725.4.camel@localhost>
Message-ID: <OF42B73EE8.3924F880-ONC1256E86.004B0581-C1256E86.004B5A9C@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 30/04/2004 15:33:48:

> Hi,
>
> I have a integer vector x that contains a unique set of numbers:
>
> x <- c(1,2,4,6,8,10,12)
>
> Is there a simple test I can use to determine if an integer such as 6 is
> contained in x ?

> x <- c(1,2,4,6,8,10,12)
> 6 %in% x
[1] TRUE

See ?"%in%" and its 'See also:' section.


HTH,
Tobias



From haleh.yasrebi at unn.ac.uk  Fri Apr 30 15:42:45 2004
From: haleh.yasrebi at unn.ac.uk (haleh.yasrebi)
Date: Fri, 30 Apr 2004 14:42:45 +0100
Subject: [R] calculation of U and V matrix of SVD decomposition (according
 to LINPACK, X = UDV')
Message-ID: <518AAA861379D411B4E900508BCF7B740407704E@atlanta.unn.ac.uk>

Hello,
Like QR decomposition, I am looking for decomposition to get U and V matrix
of SVD decomposition (according to LINPACK, X = UDV'). Do you know if there
is a function which could calculate this decomposition?

Look forward to your reply,

Haleh



From skip at pobox.com  Fri Apr 30 15:52:48 2004
From: skip at pobox.com (Skip Montanaro)
Date: Fri, 30 Apr 2004 08:52:48 -0500
Subject: [R] configure problem - mixed fortran/c
In-Reply-To: <Pine.LNX.4.44.0404301430340.7690-100000@gannet.stats>
References: <16530.21387.428120.477777@montanaro.dyndns.org>
	<Pine.LNX.4.44.0404301430340.7690-100000@gannet.stats>
Message-ID: <16530.23088.26598.377348@montanaro.dyndns.org>


    Brian> There is one, and it told you, `check LDFLAGS for paths to
    Brian> Fortran libraries'. I don't see that you have told us the result
    Brian> of your check, so surmise you haven't checked, and suspect that
    Brian> is the problem.

    >> My environment is:
    >> 
    >> Solaris 9 on Intel
    >> gcc and g77 3.3.2

    Brian> You need wherever libg2c.so is installed in your LD_LIBRARY_PATH.

Thanks, that was the ticket.  Now that I know what's going on, I understand
what I was looking at in config.log.  I see link lines like this:

    gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c \
    cfortran_test.o -ldl -ltermcap -lm   -L/usr/ccs/lib -L/usr/lib \
    -L/usr/local/lib \
    -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2 \
    -L/usr/ccs/bin \
    -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2/../../.. \
    -lfrtbegin -lg2c -lm -lgcc_s 

libg2c.so resides in /opt/lang/gcc-g77-3.3.2/lib.  It would never have
occurred to me that a library which is part of the g77 distribution couldn't
be found by configure when other directories in that tree.  Given that
that's within the g77 tree and presumably that tree is structured in a
consistent manner, wouldn't it make sense for configure to also add
"-L/opt/lang/gcc-g77-3.3.2/lib" to the link line?  (Maybe that's an autoconf
issue.  I don't see mention of "mixed C/Fortran" in configure.ac, just in
configure.)

Thanks again,

-- 
Skip Montanaro
Got gigs? http://www.musi-cal.com/submit.html
Got spam? http://www.spambayes.org/
skip at pobox.com



From ripley at stats.ox.ac.uk  Fri Apr 30 16:10:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 15:10:52 +0100 (BST)
Subject: [R] configure problem - mixed fortran/c
In-Reply-To: <16530.23088.26598.377348@montanaro.dyndns.org>
Message-ID: <Pine.LNX.4.44.0404301506410.7764-100000@gannet.stats>

On Fri, 30 Apr 2004, Skip Montanaro wrote:

> 
>     Brian> There is one, and it told you, `check LDFLAGS for paths to
>     Brian> Fortran libraries'. I don't see that you have told us the result
>     Brian> of your check, so surmise you haven't checked, and suspect that
>     Brian> is the problem.
> 
>     >> My environment is:
>     >> 
>     >> Solaris 9 on Intel
>     >> gcc and g77 3.3.2
> 
>     Brian> You need wherever libg2c.so is installed in your LD_LIBRARY_PATH.
> 
> Thanks, that was the ticket.  Now that I know what's going on, I understand
> what I was looking at in config.log.  I see link lines like this:
> 
>     gcc -o conftest -g -O2 -I/usr/local/include -L/usr/local/lib conftest.c \
>     cfortran_test.o -ldl -ltermcap -lm   -L/usr/ccs/lib -L/usr/lib \
>     -L/usr/local/lib \
>     -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2 \
>     -L/usr/ccs/bin \
>     -L/opt/lang/gcc-g77-3.3.2/lib/gcc-lib/i386-pc-solaris2.8/3.3.2/../../.. \
>     -lfrtbegin -lg2c -lm -lgcc_s 
> 
> libg2c.so resides in /opt/lang/gcc-g77-3.3.2/lib.  It would never have
> occurred to me that a library which is part of the g77 distribution couldn't
> be found by configure when other directories in that tree.  Given that
> that's within the g77 tree and presumably that tree is structured in a
> consistent manner, wouldn't it make sense for configure to also add
> "-L/opt/lang/gcc-g77-3.3.2/lib" to the link line?  

No.  You need it in the run library path, not just the link library path.  
So you could put

-L/opt/lang/gcc-g77-3.3.2/lib -R/opt/lang/gcc-g77-3.3.2/lib

in LDFLAGS.

> (Maybe that's an autoconf
> issue.  I don't see mention of "mixed C/Fortran" in configure.ac, just in
> configure.)

Look in m4/R.m4 or aclocal.m4.

We have no idea where you installed g77, but that is not a standard place 
on (Sparc-)Solaris, BTW, so I think it's a `feature' of your system.
But not even /usr/local/lib (the usual place for an install of gcc from 
sources) is in the standard Solaris path AFAIR.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pigood at verizon.net  Fri Apr 30 16:29:02 2004
From: pigood at verizon.net (Phillip Good)
Date: Fri, 30 Apr 2004 07:29:02 -0700
Subject: [R] R code for LAD regression and Deming Regression
Message-ID: <00f901c42ebf$7c381d20$a9e00804@dslverizon.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040430/ddf70e39/attachment.pl

From jmuller at mindspring.com  Fri Apr 30 17:07:30 2004
From: jmuller at mindspring.com (John Muller)
Date: Fri, 30 Apr 2004 11:07:30 -0400 (GMT-04:00)
Subject: [R] daily time series in R
Message-ID: <11389284.1083337650826.JavaMail.root@wamui02.slb.atl.earthlink.net>

I have some daily data and would like to apply some of the time series functions to it.

I have read various notes in the help archive on this, the latest I found suggested
that I need to use the irts class (Irregularly spaced time series) for daily data
since a year does not divide into an integer number of days.

I see why I would have to do that if I have gaps (e.g. only data on weekdays)
but do I still have to do that even if I have no gaps, that is if I have every 
data for every day of the year?  In the latter case, the data is evenly spaced,
it;s just that a year is not an integer multiple of this spacing.

I have the same question for weekly data
(which I might create from daily data sampled only on work or weekdays)

Here again, the data is sampled at regular intervals, but a year is not 
an integer multiple of the distance between samples.

Can you give some suggestions for dealing with this?
Should I first create an irts object and then create a ts from that?

If that is the suggestion, then when I get a ts object I have noticed
that in the plot the time axis has year and a decimal fraction of year.
What's a good way to get more standard time increments on the X axis,
such as months or week start days?

Thanks greatly,

- john muller



From bates at stat.wisc.edu  Fri Apr 30 18:20:21 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 30 Apr 2004 11:20:21 -0500
Subject: [R] Deepayan Sarkar wins the 2004 Chambers Award
Message-ID: <6rekq5saii.fsf@bates4.stat.wisc.edu>

It was announced today that Deepayan Sarkar has won the 2004 Chambers
Award sponsored by the Statistical Computing section of the American
Statistical Association.  This award, made possible by a generous
donation by Dr. John M. Chambers, is given annually in recognition
of outstanding contributions to statistical software by a student.
Deepayan received the award for his lattice package for R.

The award consists of a cash prize, a plaque, and paid registration to
attend the Joint Statistical Meetings.  Deepayan will formally receive
the award during the Statistical Computing/Statistical Graphics
sections' business meeting and mixer at the 2004 Joint Statistical
Meetings in Toronto.

Our congratulations to Deepayan.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From maechler at stat.math.ethz.ch  Fri Apr 30 18:39:15 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 30 Apr 2004 18:39:15 +0200
Subject: [R] calculation of U and V matrix of SVD decomposition (according
	to LINPACK, X = UDV')
In-Reply-To: <518AAA861379D411B4E900508BCF7B740407704E@atlanta.unn.ac.uk>
References: <518AAA861379D411B4E900508BCF7B740407704E@atlanta.unn.ac.uk>
Message-ID: <16530.33075.833133.384871@gargle.gargle.HOWL>

>>>>> "haleh" == haleh yasrebi <haleh.yasrebi at unn.ac.uk>
>>>>>     on Fri, 30 Apr 2004 14:42:45 +0100 writes:

    haleh> Hello, Like QR decomposition, I am looking for
    haleh> decomposition to get U and V matrix of SVD
    haleh> decomposition (according to LINPACK, X = UDV'). Do
    haleh> you know if there is a function which could calculate
    haleh> this decomposition?

yes, I know that there is...  ;-)  
but probably you wanted to ask another question  :-)

    haleh> Look forward to your reply,

I'm pretty sure you haven't tried  ?svd  aka  help(svd),
nor other things that are much recommended *before* asking the
2000+ readers of R-help.

It may really make much sense for you to learn how to get help
from R's builtin documentation systems.
Probably you should also take time to read the
"Introduction to R" manual that is almost surely part of your R
installation, and is additionally available from the R project
web pages.

 >>>> PLEASE do read the posting guide!
 >>>> http://www.R-project.org/posting-guide.html

Regards,
Martin Maechler



From ripley at stats.ox.ac.uk  Fri Apr 30 18:50:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 30 Apr 2004 17:50:03 +0100 (BST)
Subject: [R] daily time series in R
In-Reply-To: <11389284.1083337650826.JavaMail.root@wamui02.slb.atl.earthlink.net>
Message-ID: <Pine.LNX.4.44.0404301746500.8111-100000@gannet.stats>

On Fri, 30 Apr 2004, John Muller wrote:

> I have some daily data and would like to apply some of the time series
> functions to it.
> 
> I have read various notes in the help archive on this, the latest I found suggested
> that I need to use the irts class (Irregularly spaced time series) for daily data
> since a year does not divide into an integer number of days.
> 
> I see why I would have to do that if I have gaps (e.g. only data on weekdays)
> but do I still have to do that even if I have no gaps, that is if I have every 
> data for every day of the year?  In the latter case, the data is evenly spaced,
> it;s just that a year is not an integer multiple of this spacing.

You can have daily or weekly data as a "ts" object, provided there are no 
gaps.  You can set the frequency to one if it suits you: the frequency 
only affects the labelling (including of frequencies in spectral 
densities).

The `time series functions' only apply to objects of class "ts", so you do 
need to use that.

> I have the same question for weekly data
> (which I might create from daily data sampled only on work or weekdays)
> 
> Here again, the data is sampled at regular intervals, but a year is not 
> an integer multiple of the distance between samples.
> 
> Can you give some suggestions for dealing with this?
> Should I first create an irts object and then create a ts from that?
> 
> If that is the suggestion, then when I get a ts object I have noticed
> that in the plot the time axis has year and a decimal fraction of year.
> What's a good way to get more standard time increments on the X axis,
> such as months or week start days?

Add an axis yourself: see ?axis.POSIXct.  R is programmable, and not 
everyone's wishes are already programmed in.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mdowle at concordiafunds.com  Fri Apr 30 19:22:07 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Fri, 30 Apr 2004 18:22:07 +0100
Subject: [R] RODBC & MS SQL Server: repeated calls to sqlGetResults() problem
Message-ID: <78166BFC5165D811AA0400065BF0324B1B9105@wisconsin.concordia>


Dear list,

RODBC is mostly working very well on Windows XP talking to MS SQL Server.
However, when trying to retrieve a result set in repeated batches the first
batch returns results ok, but then subsequent calls return no data (see code
below). I tried setting believeNRows=FALSE both in odbcConnect() and in
sqlGetResults() but this doesn't appear to make any difference. Also
odbcFetchRows() suffers the same problem.  The documentation states
"sqlGetResults is a mid-level function. It should be called after a call to
odbcQuery and used to retrieve waiting results into a data frame. Its main
use is with max set to non zero it will retrieve the result set in batches
with repeated calls."

Have I mis-understood? Is this a driver problem? Something else?  Any
help/advice much appreciated.

Many thanks in advance,

Matthew


> channel = odbcConnect("MY DSN")
> odbcGetInfo(channel)
[1] "Microsoft SQL Server version 08.00.0760. Driver ODBC version 03.52"
> odbcQuery(channel, "select * from TEST")    # Table TEST contains a single
column by 100 rows
[1] 1
> sqlGetResults(channel, max=10)
  last_update
1  2004-02-06
2  2004-02-06
3  2004-02-06
4  2004-02-06
5  2004-02-06
6  2004-02-06
7  2004-02-06
8  2004-02-06
9  2004-02-06
10 2004-02-06
> sqlGetResults(channel, max=10)
[1] last_update
<0 rows> (or 0-length row.names)    # why is this empty?
> 
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.0            
year     2004           
month    04             
day      12             
language R              
>



From gordon_fredericks at hotmail.com  Fri Apr 30 20:01:13 2004
From: gordon_fredericks at hotmail.com (Gordon Fredericks)
Date: Fri, 30 Apr 2004 14:01:13 -0400
Subject: [R] Code for quasi-likelihood binomial estimation
Message-ID: <BAY14-F12A460byKB9Y000437fe@hotmail.com>

Hello,

Has anyone written up code to estimate for example a simple logit using 
quasi-likelihood?  I know that glm() already does this, but I'd like to do 
some tinkering with the variance function beyond what glm() allows.  I've 
scanned online sources and everyone seems to use glm().  Will take a crack 
at it if necessary but have zero experience w/q-likelihood and not that much 
with coding; thought I'd check here first so as not to reinvent the wheel, 
perhaps learn a bit from what others have already done.

Thanks,
GF

_________________________________________________________________
Stop worrying about overloading your inbox - get MSN Hotmail Extra Storage!



From sundar.dorai-raj at PDF.COM  Fri Apr 30 20:12:07 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 30 Apr 2004 13:12:07 -0500
Subject: [R] Code for quasi-likelihood binomial estimation
In-Reply-To: <BAY14-F12A460byKB9Y000437fe@hotmail.com>
References: <BAY14-F12A460byKB9Y000437fe@hotmail.com>
Message-ID: <409296F7.5080404@pdf.com>


Gordon Fredericks wrote:

> Hello,
> 
> Has anyone written up code to estimate for example a simple logit using 
> quasi-likelihood?  I know that glm() already does this, but I'd like to 
> do some tinkering with the variance function beyond what glm() allows.  
> I've scanned online sources and everyone seems to use glm().  Will take 
> a crack at it if necessary but have zero experience w/q-likelihood and 
> not that much with coding; thought I'd check here first so as not to 
> reinvent the wheel, perhaps learn a bit from what others have already done.
> 
> Thanks,
> GF
> 

See ?quasibinomial.

?glm would have pointed you to ?family which would have shown you the 
quasi family.

--sundar



From tlumley at u.washington.edu  Fri Apr 30 20:18:19 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Apr 2004 11:18:19 -0700 (PDT)
Subject: [R] Code for quasi-likelihood binomial estimation
In-Reply-To: <BAY14-F12A460byKB9Y000437fe@hotmail.com>
References: <BAY14-F12A460byKB9Y000437fe@hotmail.com>
Message-ID: <Pine.A41.4.58.0404301117280.15050@homer39.u.washington.edu>

On Fri, 30 Apr 2004, Gordon Fredericks wrote:

> Hello,
>
> Has anyone written up code to estimate for example a simple logit using
> quasi-likelihood?  I know that glm() already does this, but I'd like to do
> some tinkering with the variance function beyond what glm() allows.

This should just be a matter of writing a family() object for glm.  I'd
recommend looking at the negative binomial example in MASS (the book as
well as the package).

	-thomas



From pgilbert at bank-banque-canada.ca  Fri Apr 30 20:19:29 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 30 Apr 2004 14:19:29 -0400
Subject: [R] numericDeriv
In-Reply-To: <Pine.SGI.4.40.0404280918430.12869226-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0404280918430.12869226-100000@origin.chass.utoronto.ca>
Message-ID: <409298B1.3000101@bank-banque-canada.ca>

Jean Eid wrote:

>Dear All,
>I am trying to solve a Generalized Method of Moments problem which
>necessitate the gradient of moments computation to get the
>standard  errors of estimates.
>I know optim does not output the gradient, but I can use numericDeriv to
>get that. My question is: is this the best function to do this?
>  
>
'Best' depends on what you want. If you want an accurate numerical 
estimate of the gradient then you might look at the program 
gradRichardson in the curve package of the dseplus bundle in the devel 
area of CRAN. It uses Richarson extrapolation to improve the accuracy. 
However, this is not the program to use if you want a quick numerical 
estimate of the gradient. Most anything else you might think of using 
will be quicker.

There are some other programs around too. A year or two ago there was 
some discussion of putting various gradient calculation techniques 
together in one place. I don't  think anything has happen yet.

Paul Gilbert



From tlumley at u.washington.edu  Fri Apr 30 20:44:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 30 Apr 2004 11:44:25 -0700 (PDT)
Subject: [R] R code for LAD regression and Deming Regression
In-Reply-To: <00f901c42ebf$7c381d20$a9e00804@dslverizon.net>
References: <00f901c42ebf$7c381d20$a9e00804@dslverizon.net>
Message-ID: <Pine.A41.4.58.0404301143360.15050@homer39.u.washington.edu>

On Fri, 30 Apr 2004, Phillip Good wrote:

> Before I duplicate effort are there existing programs for computing the coefficients for
> a) LAD regression

LAD regression is a special case of quantile regression, which is handled
by the quantreg package.

	-thomas



From wolski at molgen.mpg.de  Fri Apr 30 21:01:32 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 30 Apr 2004 21:01:32 +0200
Subject: [R] ./configure --prefix and R_LIBS
Message-ID: <200404302101320233.0262302B@mail.math.fu-berlin.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040430/90c4f9f3/attachment.pl

From Detlef.Steuer at unibw-hamburg.de  Fri Apr 30 22:29:56 2004
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Fri, 30 Apr 2004 22:29:56 +0200
Subject: [R] ./configure --prefix and R_LIBS
In-Reply-To: <200404302101320233.0262302B@mail.math.fu-berlin.de>
References: <200404302101320233.0262302B@mail.math.fu-berlin.de>
Message-ID: <20040430222956.0f8893bb@rechenknecht.local>

On Fri, 30 Apr 2004 21:01:32 +0200
"Wolski" <wolski at molgen.mpg.de> wrote:

> Hi!
> I am installing R in non standard directory.
> ./configure --prefix=/non/standard/directory
> 
> The installation works fine.
> But after starting R i get
> .libPaths()
> /usr/lib/R/library
> 
> but they cant be there of course!

Looks like the prefix doesn't get promoted to exec-prefix

try
./configure --prefix=/no/st/di --exec-prefix=/no/st/di 

May or may not solve it.

hth
detlef


> 
> Have I to set some additional switches during config?
> Eryk
> 
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic
> Ihnestrasse 63-73 14195 Berlin       'v'
> tel: 0049-30-84131469               /   \
> mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From godbless_maria at hotmail.com  Fri Apr 30 22:25:19 2004
From: godbless_maria at hotmail.com (Maria Gu)
Date: Fri, 30 Apr 2004 20:25:19 +0000
Subject: [R] Probability(Markov chain transition matrix)
Message-ID: <BAY2-F8LdElsBt8sBju000149cf@hotmail.com>

Hi all, thank you for the response of my inquiry.
Let me explain more specifically about what I want from R.
It is about estimating bad debt related to people's credit card payment 
behavior.
Here we go!


Status of a credit account {NC, 0,1,2,.....M} where NC is no-credit 
status(account has no balance), 0 is where the account has a credit balance 
but the payment are up to date, 1 is where the account is one payment 
overdue, and M payment overdue is classified as default.

So we come up with Transition matrix stating probabilities of each state

FromTo NC   0       1     2     3
NC      0.79  0.21  0      0    0
0          0.09  0.73 0.18 0    0
1          0.09  0.51  0   0.40  0
2          0.09  0.38  0      0   0.55
3          0.06  0.32  0      0   0.62


Thus if one starts with all the accounts having no credit ?0 =(1,0,0,0,0), 
after one period the distribution of account is ?1=(0.79, 0.21, 0, 0, 0)  
after subsequent periods, it becomes

2=(0.64, 0.32, 0.04, 0, 0),
3= (0.540, 0.387, 0.058, 0.015, 0)
4=(0.468,0.431, 0.070, 0.023, 0.008)
5=(0.417, 0.460, 0.077, 0.028, 0.018)
and ++++
10=(0.315, 0.512, 0.091, 0.036, 0.046)

This proves a way for estimating the amount of bad debt will appear the 
future periods. After 10 periods, it estimates that 4.6%(0.046) of the 
accounts will be bad.

I am sure R can solve this, please help me!

Maria Gu
510-418-1240

_________________________________________________________________




From myao at ou.edu  Fri Apr 30 23:12:35 2004
From: myao at ou.edu (Yao, Minghua)
Date: Fri, 30 Apr 2004 16:12:35 -0500
Subject: [R] Problems in plot
Message-ID: <78B50CF247E5D04B8A5E02D001CC8E9A595C54@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040430/2aafb8ff/attachment.pl

From kjetil at acelerate.com  Fri Apr 30 23:23:16 2004
From: kjetil at acelerate.com (kjetil@acelerate.com)
Date: Fri, 30 Apr 2004 21:23:16 -0000
Subject: [R] searching a vector
In-Reply-To: <1083332027.54725.4.camel@localhost>
Message-ID: <40BA1846.18174.683834D@localhost>

On 30 Apr 2004 at 9:33, Arend P. van der Veen wrote:


> x <- c(1,2,4,6,8,10,12)
> 6 %in% x
[1] TRUE
> 


Kjetil Halvorsen

> Hi,
> 
> I have a integer vector x that contains a unique set of numbers: 
> 
> x <- c(1,2,4,6,8,10,12)
> 
> Is there a simple test I can use to determine if an integer such as 6
> is contained in x ?  
> 
> Thanks in advance for any help,
> Arend
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From wwsprague at ucdavis.edu  Fri Apr 30 23:51:51 2004
From: wwsprague at ucdavis.edu (foobar)
Date: Fri, 30 Apr 2004 14:51:51 -0700
Subject: [R] searching a vector
In-Reply-To: <1083332027.54725.4.camel@localhost>
References: <1083332027.54725.4.camel@localhost>
Message-ID: <c6uhlu$8ln$1@sea.gmane.org>

Arend P. van der Veen wrote:

> Hi,
> 
> I have a integer vector x that contains a unique set of numbers: 
> 
> x <- c(1,2,4,6,8,10,12)
> 
> Is there a simple test I can use to determine if an integer such as 6 is
> contained in x ?  

Try the following and experiment:

x %in% c(2,4)
which(x %in% c(2,4))
x[which(x %in% c(2,4)) ]

1.  logical vector w/ TRUE's where one of the c(2,4) are located, F's 
otherwise
2.  indexes of the TRUE's generated above
3.  the vector subset from those indexes

Sorry to be so terse, but that might help
> 
> Thanks in advance for any help,
> Arend
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



