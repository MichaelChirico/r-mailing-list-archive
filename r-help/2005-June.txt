From graumann at caltech.edu  Wed Jun  1 00:08:19 2005
From: graumann at caltech.edu (Johannes Graumann)
Date: Tue, 31 May 2005 15:08:19 -0700
Subject: [R] subset-analogue removing fed in indexes?
In-Reply-To: <971536df05053114381997562e@mail.gmail.com>
References: <1117575094.3174.67.camel@localhost>
	<971536df05053114381997562e@mail.gmail.com>
Message-ID: <1117577299.3174.69.camel@localhost>

Jebus, you guys are fast!

Thanks so much,

Joh

On Tue, 2005-05-31 at 17:38 -0400, Gabor Grothendieck wrote:
> On 5/31/05, Johannes Graumann <graumann at caltech.edu> wrote:
> > Hello,
> > 
> > Here's my issue:
> > 
> > I want to plot the following vectors:
> > > x <- c(0.0, 2.0, 15.0, 100.0, 105.0, 105.1, 110.0, 120.0, 120.1,
> > 130.0)
> > > data <- c(8.75, 8.75, 16.25, 38.75, 61.25, 8.75, NA, 8.75, NA, NA)
> > 
> > and avoid the line discontinuations caused by 'NA'.
> > > plot_data <- na.omit(data)
> > will clean up 'data' for me, but now I need to get a 'plot_x' which
> > omits the values indexed with what's spit out by 'na.exclude(data)'.
> > 
> > Can anybody let me know a smooth way of how to delete entries with
> > certain indexes from a vector?
> 
> plot(approx(x,data))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From efg at stowers-institute.org  Wed Jun  1 00:14:13 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 31 May 2005 17:14:13 -0500
Subject: [R] Why does "summary" show number of NAs as non-integer?
Message-ID: <d7ineb$nbt$1@sea.gmane.org>

Example:

> set.seed(19)
> summary( c(NA, runif(10,1,100), NaN) )
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
  7.771  24.850  43.040  43.940  63.540  83.830   2.000

Why isn't the number of NA's just "2" instead of the "2.000" shown above?

efg



From luan_sheng at yahoo.com.cn  Wed Jun  1 00:22:06 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Wed, 1 Jun 2005 06:22:06 +0800
Subject: [R] FW: why is it numeric(0)?
Message-ID: <200505312222.j4VMMG3D027976@hypatia.math.ethz.ch>

hello,everyone. I have one question:

example 1
> x=numeric(0)
> y=5
> print(x+y)
numeric(0)
 
example 2
> x=numeric(1)
> y=5
> print(x+y)
[1] 5

why the print(x+y)  is numeric(0) at the first example, but the result is 0
at the second example? 


__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰



From luan_sheng at yahoo.com.cn  Wed Jun  1 00:40:32 2005
From: luan_sheng at yahoo.com.cn (luan_sheng)
Date: Wed, 1 Jun 2005 06:40:32 +0800
Subject: [R] FW: why is it numeric(0)?
In-Reply-To: <Pine.LNX.4.58.0505311524190.21819@echidna.fhcrc.org>
Message-ID: <200505312240.j4VMeccU031252@hypatia.math.ethz.ch>

 yea, I have read the help. But some one tell me that i f you want use a
vector that you don't know it's length, you should use xx=numric(0) , is it
not right? If it isn't right, how can I do?
thanks

-----Original Message-----
From: Douglas Grove [mailto:dgrove at fhcrc.org] 
Sent: Wednesday, June 01, 2005 6:27 AM
To: luan_sheng
Subject: Re: [R] FW: why is it numeric(0)?

Have you read the help page for numeric (?numeric) to understand what it
does?  You should really look at help pages prior to posting.

numeric(x) returns a numeric vector of length x, with all entries
initialized to zero

so numeric(1) returns 0,
numeric(2) returns c(0,0)
etc.

numeric(0) returns a numeric vector of *length 0*, so when you add anything
to it you get the same result (it's basically a numeric NULL)



On Wed, 1 Jun 2005, luan_sheng wrote:

> hello,everyone. I have one question:
> 
> example 1
> > x=numeric(0)
> > y=5
> > print(x+y)
> numeric(0)
>  
> example 2
> > x=numeric(1)
> > y=5
> > print(x+y)
> [1] 5
> 
> why the print(x+y)  is numeric(0) at the first example, but the result 
> is 0 at the second example?
> 
> 
> __________________________________________________
> 
> Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰



From gunter.berton at gene.com  Wed Jun  1 00:40:51 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 31 May 2005 15:40:51 -0700
Subject: [R] Why does "summary" show number of NAs as non-integer?
In-Reply-To: <d7ineb$nbt$1@sea.gmane.org>
Message-ID: <200505312240.j4VMepGX000203@hertz.gene.com>

summary() is an S3 generic that for your vector dispatches
summary.default(). The output of summary default has class "table" and so
calls print.table (print is another S3 generic). Look at the code of
print.table() to see how it formats the output.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Earl F. Glynn
> Sent: Tuesday, May 31, 2005 3:14 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Why does "summary" show number of NAs as non-integer?
> 
> Example:
> 
> > set.seed(19)
> > summary( c(NA, runif(10,1,100), NaN) )
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
>   7.771  24.850  43.040  43.940  63.540  83.830   2.000
> 
> Why isn't the number of NA's just "2" instead of the "2.000" 
> shown above?
> 
> efg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Wed Jun  1 00:43:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 31 May 2005 15:43:58 -0700
Subject: [R] FW: why is it numeric(0)?
In-Reply-To: <200505312222.j4VMMG3D027976@hypatia.math.ethz.ch>
Message-ID: <200505312244.j4VMhwcX022091@meitner.gene.com>

?numeric

Hint: Type numeric(0) and numeric(1) at the prompt and see what you get.
What is the sum of anything and a zero length vector?

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of luan_sheng
> Sent: Tuesday, May 31, 2005 3:22 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] FW: why is it numeric(0)?
> 
> hello,everyone. I have one question:
> 
> example 1
> > x=numeric(0)
> > y=5
> > print(x+y)
> numeric(0)
>  
> example 2
> > x=numeric(1)
> > y=5
> > print(x+y)
> [1] 5
> 
> why the print(x+y)  is numeric(0) at the first example, but 
> the result is 0
> at the second example? 
> 
> 
> __________________________________________________
> 
> QE;"Cb7QGSJOd#-VP9z5ZR;>xN^@,;xSJ<~I'HE3,4sSJOd
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gavin.simpson at ucl.ac.uk  Wed Jun  1 00:47:05 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 31 May 2005 23:47:05 +0100
Subject: [R] Increasing Console "Paste Buffer"
In-Reply-To: <1117572426.11503.12.camel@localhost.localdomain>
References: <1117572426.11503.12.camel@localhost.localdomain>
Message-ID: <429CE969.8070505@ucl.ac.uk>

Manuel Morales wrote:
> Hello list.
> 
> I'm using R from the gnome-terminal in Fedora. My preference is to write
> programs in VIM, and then source the file from R, or copy and paste the
> lines into the console. I'm wondering if there is a way to increase the
> "paste buffer" as an alternative to "sourcing" large analyses. As was
> mentioned in a recent thread on Linux GUI's, I find that if I paste in a
> large amount of text, the lines end up getting cut off at some point. I
> wonder if this is an R restriction, because it seems like I am able to
> paste substantially more text in other console-based programs. Is there
> any way to increase the amount of text that I can paste into an R
> session?
> 
> Thanks!
> 
> Manuel
> 

Manuel,

Maybe I misunderstand what you mean by "lines end up getting cut off at 
some point" so correct me if I got it wrong, but I assume you mean that 
after a certain number of lines entered you can no longer scroll back up 
and view the earlier lines?

If this is the case, then, and again I assume you are using Gnome 
(default in FC) as the desktop system, then open a terminal. The  Edit > 
Current Profile in the menu bar for the terminal. Select the Scrolling 
Tab. Alter Scrollback lines and kilobytes accordingly to suit. You only 
need to do one, not both - I find it easier to think  in numbers of 
lines so I changed that. You'll have to modify this if you use KDE or 
another window manager with FC3.

HTH

Gavin



From p.murrell at auckland.ac.nz  Wed Jun  1 00:51:05 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 01 Jun 2005 10:51:05 +1200
Subject: [R] Problem going back to a viewport with gridBase
References: <971536df05053020191ebe8f44@mail.gmail.com>
Message-ID: <429CEA59.7020206@stat.auckland.ac.nz>

Hi


Gabor Grothendieck wrote:
> I am setting up base plots -- one in viewport A and and one in B.  This part
> works fine.  But if I go back to A after having done B and add
> horizontal lines it seems
> to not use the correct coordinates.  How do I tell it to resume using A's
> coordinates?  I am already using par(fig = gridFIG()) but it seems that that's
> not enough to reestablish them.  What happens is that when I go back to 
> A it draws the horizontal lines as if its relative to B's coordinates
> rather than
> restablishing A's coordinates.  As a result the horizontal lines are
> drawn near the
> bottom of the graph instead of at the correct heights.  Try running the code
> below to see what I mean.
> 
> I have also tried to use baseViewports with this but did not have any
> success.
> 
> How do I modify this example so that the horizontal red lines come out
> at the appropriate levels?    Note that this is just an example and in 
> the future I will want to have multiple viewports each with a base plot and
> add arbitrary additional line or point plots to them so the solution needs
> to be sufficiently general that I can so generalize it.
> 
> Thanks.
> 
> 
> library(gridBase)
> 
> opar <- par(no.readonly = TRUE)
> grid.newpage()
> 
> # two columns, one row
> unit. <- unit(c(1,1), c("null","null"))
> pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
> 
> # draw green graph in first column (viewport A)
> pushViewport(viewport(layout.pos.col = 1, name = "A"))
> par(fig = gridFIG()); par(new = TRUE)
> plot(1:10, col = "green", pch = 20)
> upViewport(1)
> 
> # draw purple graph in second column (viewport B)
> pushViewport(viewport(layout.pos.col = 2, name = "B"))
> par(fig = gridFIG()); par(new = TRUE)
> plot(1:100, col = "purple", pch = 18)
> upViewport()
> 
> # go back to A and add horizontal grid lines
> seekViewport("A")
> par(fig = gridFIG())
> abline(h=1:10, col = "red")  #### THESE DO NOT GET DRAWN AS EXPECTED
> popViewport()
> 
> # go back to B and add vertical grid lines
> seekViewport("B")
> par(fig = gridFIG())
> abline(v=1:10, col = "red")
> popViewport()
> par(opar)


The base, or "traditional", graphics system only records the *current* 
plotting coordinates;  it does not retain a memory of previous plotting 
coordinates.  What your example does is *reposition* the plotting 
region, but to do what you want you would have to recreate the plotting 
coordinates of the first plot.  This is possible (at least in simple 
cases like the above), as shown below.  However, perhaps a better 
approach would be to use a combination of grid and lattice plots, where 
the coordinate systems are retained and don't need to be recreated.  An 
example of this approach is given at the end.

#######
# Modified example using gridBase
#######
library(gridBase)

opar <- par(no.readonly = TRUE)
grid.newpage()

# two columns, one row
unit. <- unit(c(1,1), c("null","null"))
pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))

# draw green graph in first column (viewport A)
pushViewport(viewport(layout.pos.col = 1, name = "A"))
par(fig = gridFIG()); par(new = TRUE)
plot(1:10, col = "green", pch = 20)
upViewport(1)

# draw purple graph in second column (viewport B)
pushViewport(viewport(layout.pos.col = 2, name = "B"))
par(fig = gridFIG()); par(new = TRUE)
plot(1:100, col = "purple", pch = 18)
upViewport()

# go back to A and add horizontal grid lines
seekViewport("A")
par(fig = gridFIG()); par(new=TRUE)  #### extra par(new=TRUE)
plot(1:10, type="n", axes=FALSE, ann=FALSE)  #### RESET PLOT A AXES
abline(h=1:10, col = "red")
popViewport()

# go back to B and add vertical grid lines
seekViewport("B")
par(fig = gridFIG()); par(new=TRUE)  #### extra par(new=TRUE)
plot(1:100, type="n", axes=FALSE, ann=FALSE)  #### RESET PLOT B AXES
abline(v=1:10, col = "red")
popViewport()
par(opar)


#######
# Similar result but using grid and lattice
#######
library(grid)
library(lattice)

grid.newpage()

# two columns, one row
unit. <- unit(c(1,1), c("null","null"))
pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))

# draw green graph in first column (viewport A)
pushViewport(viewport(layout.pos.col = 1, name = "A"))
# lattice plot instead of base plot
p1 <- xyplot(1:10 ~ 1:10, col="green", pch=20)
# prefix important so I can refer to it later
print(p1, newpage=FALSE, prefix="plotA")
upViewport(1)

# draw purple graph in second column (viewport B)
pushViewport(viewport(layout.pos.col = 2, name = "B"))
p2 <- xyplot(1:100 ~ 1:100, col="purple", pch=18)
print(p2, newpage=FALSE, prefix="plotB")
upViewport()

# go back to A and add horizontal grid lines
seekViewport(trellis.vpname("panel", 1, 1, prefix="plotA"))
# I'm working on a grid.abline() ...
grid.segments(x0=0, x1=1,
               y0=unit(1:10, "native"),
               y1=unit(1:10, "native"),
               gp=gpar(col="red"))

# go back to B and add vertical grid lines
seekViewport(trellis.vpname("panel", 1, 1, prefix="plotB"))
grid.segments(y0=0, y1=1,
               x0=unit(1:10, "native"),
               x1=unit(1:10, "native"),
               gp=gpar(col="red"))

upViewport(0)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From MSchwartz at mn.rr.com  Wed Jun  1 01:22:58 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 31 May 2005 18:22:58 -0500
Subject: [R] Formatting numbers with a limited amount
	of	digits	consistently
In-Reply-To: <429C7EBB.6060705@stats.uwo.ca>
References: <d7f8op$fa3$1@sea.gmane.org> <429B4B72.9000109@stats.uwo.ca>
	<971536df05053010571ad00617@mail.gmail.com>
	<429B65A5.3000503@stats.uwo.ca>
	<971536df05053020533592e59c@mail.gmail.com>
	<1117549806.22595.144.camel@horizons.localdomain>
	<429C7EBB.6060705@stats.uwo.ca>
Message-ID: <1117581778.22595.170.camel@horizons.localdomain>

On Tue, 2005-05-31 at 11:11 -0400, Duncan Murdoch wrote:
> Marc Schwartz wrote:
> 
> > Final note to Henrik: Note that the IEEE 754 rounding standard as
> > implemented in R results in:
> > 
> > 
> >>round(18.15, 1)
> > 
> > [1] 18.1
> > 
> >>formatC(18.15, format = "f", digits = 1)
> > 
> > [1] "18.1"
> > 
> >>sprintf("%5.1f", 18.15)
> > 
> > [1] " 18.1"
> > 
> > This is because the rounding method implemented is the "go to the even
> > digit" approach. Thus, you don't get 18.2. 
> > 
> > See ?round for more information.
> 
> I don't think "go to the even digit" is being applied here:  ".1" is not 
>   an even digit.
> 
> I suspect what's going on in this example is that 18.15 is not being 
> represented exactly; it's stored internally as something slightly less 
> than that value, so it rounds down.
> 
> You'd see the "go to the even digit" rule applied when rounding 17.5 or 
> 18.5, which can be represented exactly, being fractions with a power of 
> 2 in the denominator:
> 
>  > round(18.5, 0)
> [1] 18
>  > round(17.5, 0)
> [1] 18
> 
> (This is very gratifying.  Usually when I try to predict the exact 
> behaviour of round() or signif() I end up having to rewrite my 
> prediction afterwards.  But this time I got it right. Honest!)
> 
> Duncan Murdoch

Duncan,

Just got back from a day long meeting.

You are indeed correct on the rounding here. If you look at how 18.15
appears when printed with more significant digits:

> print(18.15, 20)
[1] 18.149999999999998579

That's what I get for trying to deal with floating point representation
issues first thing after a three day weekend...  ;-)

Thanks for the correction.

Marc



From MSchwartz at MedAnalytics.com  Wed Jun  1 01:32:05 2005
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 31 May 2005 18:32:05 -0500
Subject: [R] Why does "summary" show number of NAs as non-integer?
In-Reply-To: <d7ineb$nbt$1@sea.gmane.org>
References: <d7ineb$nbt$1@sea.gmane.org>
Message-ID: <1117582325.22595.175.camel@horizons.localdomain>

On Tue, 2005-05-31 at 17:14 -0500, Earl F. Glynn wrote:
> Example:
> 
> > set.seed(19)
> > summary( c(NA, runif(10,1,100), NaN) )
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
>   7.771  24.850  43.040  43.940  63.540  83.830   2.000
> 
> Why isn't the number of NA's just "2" instead of the "2.000" shown above?
> 
> efg

This is actually related to the thread on formatting numbers.

In reviewing the Detail section of ?print.default:

"The same number of decimal places is used throughout a vector, This
means that digits specifies the minimum number of significant digits to
be used, and that at least one entry will be printed with that minimum
number."

'digits' in the above is the digits argument to print.default(). In this
case, it defaults to options("digits"), which is 7.

In the above output from summary, you will note that all of the output
has three digits after the decimal place.

Thus:

> c(2)
[1] 2

> c(2, 3)
[1] 2 3

> c(2, 3.5)
[1] 2.0 3.5

> c(2, 3.57)
[1] 2.00 3.57

> c(2, 3.579)
[1] 2.000 3.579


Note how the output format of "2" varies depending upon how many decimal
places I use in the second element. 

This goes to the need to use other functions where there is a need to
exert greater control over how numeric output can be formatted and
aligned using formatC() and/or sprintf().

For example:

> sprintf("0 decimal places: %d    3 decimal places: %4.3f", 2, 3.57911)
[1] "0 decimal places: 2    3 decimal places: 3.579"


See ?sprintf and ?formatC for more information.

HTH,

Marc Schwartz



From ggrothendieck at gmail.com  Wed Jun  1 01:33:43 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 19:33:43 -0400
Subject: [R] Problem going back to a viewport with gridBase
In-Reply-To: <429CEA59.7020206@stat.auckland.ac.nz>
References: <971536df05053020191ebe8f44@mail.gmail.com>
	<429CEA59.7020206@stat.auckland.ac.nz>
Message-ID: <971536df050531163353031d97@mail.gmail.com>

On 5/31/05, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> Hi
> 
> 
> Gabor Grothendieck wrote:
> > I am setting up base plots -- one in viewport A and and one in B.  This part
> > works fine.  But if I go back to A after having done B and add
> > horizontal lines it seems
> > to not use the correct coordinates.  How do I tell it to resume using A's
> > coordinates?  I am already using par(fig = gridFIG()) but it seems that that's
> > not enough to reestablish them.  What happens is that when I go back to
> > A it draws the horizontal lines as if its relative to B's coordinates
> > rather than
> > restablishing A's coordinates.  As a result the horizontal lines are
> > drawn near the
> > bottom of the graph instead of at the correct heights.  Try running the code
> > below to see what I mean.
> >
> > I have also tried to use baseViewports with this but did not have any
> > success.
> >
> > How do I modify this example so that the horizontal red lines come out
> > at the appropriate levels?    Note that this is just an example and in
> > the future I will want to have multiple viewports each with a base plot and
> > add arbitrary additional line or point plots to them so the solution needs
> > to be sufficiently general that I can so generalize it.
> >
> > Thanks.
> >
> >
> > library(gridBase)
> >
> > opar <- par(no.readonly = TRUE)
> > grid.newpage()
> >
> > # two columns, one row
> > unit. <- unit(c(1,1), c("null","null"))
> > pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
> >
> > # draw green graph in first column (viewport A)
> > pushViewport(viewport(layout.pos.col = 1, name = "A"))
> > par(fig = gridFIG()); par(new = TRUE)
> > plot(1:10, col = "green", pch = 20)
> > upViewport(1)
> >
> > # draw purple graph in second column (viewport B)
> > pushViewport(viewport(layout.pos.col = 2, name = "B"))
> > par(fig = gridFIG()); par(new = TRUE)
> > plot(1:100, col = "purple", pch = 18)
> > upViewport()
> >
> > # go back to A and add horizontal grid lines
> > seekViewport("A")
> > par(fig = gridFIG())
> > abline(h=1:10, col = "red")  #### THESE DO NOT GET DRAWN AS EXPECTED
> > popViewport()
> >
> > # go back to B and add vertical grid lines
> > seekViewport("B")
> > par(fig = gridFIG())
> > abline(v=1:10, col = "red")
> > popViewport()
> > par(opar)
> 
> 
> The base, or "traditional", graphics system only records the *current*
> plotting coordinates;  it does not retain a memory of previous plotting
> coordinates.  What your example does is *reposition* the plotting
> region, but to do what you want you would have to recreate the plotting
> coordinates of the first plot.  This is possible (at least in simple
> cases like the above), as shown below.  However, perhaps a better
> approach would be to use a combination of grid and lattice plots, where
> the coordinate systems are retained and don't need to be recreated.  An
> example of this approach is given at the end.
> 
> #######
> # Modified example using gridBase
> #######
> library(gridBase)
> 
> opar <- par(no.readonly = TRUE)
> grid.newpage()
> 
> # two columns, one row
> unit. <- unit(c(1,1), c("null","null"))
> pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
> 
> # draw green graph in first column (viewport A)
> pushViewport(viewport(layout.pos.col = 1, name = "A"))
> par(fig = gridFIG()); par(new = TRUE)
> plot(1:10, col = "green", pch = 20)
> upViewport(1)
> 
> # draw purple graph in second column (viewport B)
> pushViewport(viewport(layout.pos.col = 2, name = "B"))
> par(fig = gridFIG()); par(new = TRUE)
> plot(1:100, col = "purple", pch = 18)
> upViewport()
> 
> # go back to A and add horizontal grid lines
> seekViewport("A")
> par(fig = gridFIG()); par(new=TRUE)  #### extra par(new=TRUE)
> plot(1:10, type="n", axes=FALSE, ann=FALSE)  #### RESET PLOT A AXES
> abline(h=1:10, col = "red")
> popViewport()
> 
> # go back to B and add vertical grid lines
> seekViewport("B")
> par(fig = gridFIG()); par(new=TRUE)  #### extra par(new=TRUE)
> plot(1:100, type="n", axes=FALSE, ann=FALSE)  #### RESET PLOT B AXES
> abline(v=1:10, col = "red")
> popViewport()
> par(opar)
> 
> 
> #######
> # Similar result but using grid and lattice
> #######
> library(grid)
> library(lattice)
> 
> grid.newpage()
> 
> # two columns, one row
> unit. <- unit(c(1,1), c("null","null"))
> pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
> 
> # draw green graph in first column (viewport A)
> pushViewport(viewport(layout.pos.col = 1, name = "A"))
> # lattice plot instead of base plot
> p1 <- xyplot(1:10 ~ 1:10, col="green", pch=20)
> # prefix important so I can refer to it later
> print(p1, newpage=FALSE, prefix="plotA")
> upViewport(1)
> 
> # draw purple graph in second column (viewport B)
> pushViewport(viewport(layout.pos.col = 2, name = "B"))
> p2 <- xyplot(1:100 ~ 1:100, col="purple", pch=18)
> print(p2, newpage=FALSE, prefix="plotB")
> upViewport()
> 
> # go back to A and add horizontal grid lines
> seekViewport(trellis.vpname("panel", 1, 1, prefix="plotA"))
> # I'm working on a grid.abline() ...
> grid.segments(x0=0, x1=1,
>               y0=unit(1:10, "native"),
>               y1=unit(1:10, "native"),
>               gp=gpar(col="red"))
> 
> # go back to B and add vertical grid lines
> seekViewport(trellis.vpname("panel", 1, 1, prefix="plotB"))
> grid.segments(y0=0, y1=1,
>               x0=unit(1:10, "native"),
>               x1=unit(1:10, "native"),
>               gp=gpar(col="red"))
> 
> upViewport(0)
> 
> Paul
> --
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> 
> 

Thanks.  I will study that further.  One other question:

Using layout or mfcol/mfrow (both from base graphics) one can 
set it up so each new plot goes into a successive cell.  That is one can
do a traversal of the cells in a layout by just issuing successive
calls to plot.  Is there something analogous to that in grid?   What I
am doing right now is to calculate the row and column of the next cell
and then move to it like this:

   # mm.row[j] gives the row in the layout of the jth cell
   # mm.col[j] gives the col in the layout of the jth cell
   mm <- matrix(seq(nr*nc), nr, nc)
   mm.row <- c(row(mm))
   mm.col <- c(col(mm))

  # go to next cell in the array
   j <- j + 1 # increment position
  pushViewport(viewport(layout.pos.row = mm.row[j], layout.pos.col = mm.col[j]))

Is that how to do it or is there some layout/mfcol-like way?



Thanks.



From davidoff at haas.berkeley.edu  Wed Jun  1 01:37:46 2005
From: davidoff at haas.berkeley.edu (Thomas Davidoff)
Date: Tue, 31 May 2005 16:37:46 -0700
Subject: [R] Tiger problems
Message-ID: <A6AB7A6E-97AC-485B-95D4-E5FFC6E65C34@haas.berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050531/b0faf629/attachment.pl

From tlumley at u.washington.edu  Wed Jun  1 01:40:23 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 May 2005 16:40:23 -0700 (PDT)
Subject: [R] FW: why is it numeric(0)?
In-Reply-To: <200505312222.j4VMMG3D027976@hypatia.math.ethz.ch>
References: <200505312222.j4VMMG3D027976@hypatia.math.ethz.ch>
Message-ID: <Pine.A41.4.61b.0505311558070.190182@homer05.u.washington.edu>

On Wed, 1 Jun 2005, luan_sheng wrote:

> hello,everyone. I have one question:
>
> example 1
>> x=numeric(0)
>> y=5
>> print(x+y)
> numeric(0)
>
> example 2
>> x=numeric(1)
>> y=5
>> print(x+y)
> [1] 5
>
> why the print(x+y)  is numeric(0) at the first example, but the result is 0
> at the second example?
>

numeric(0) is a zero-length vector of floating point numbers, so your 
first example takes no floating point numbers and adds 5 to each one. The 
result is still no floating point numbers.

numeric(1) is a vector containing a single 0, so the second example takes 
0 and adds 5, to give a vector containing a single 5.


 	-thomas



From p.murrell at auckland.ac.nz  Wed Jun  1 01:58:58 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 01 Jun 2005 11:58:58 +1200
Subject: [R] Problem going back to a viewport with gridBase
References: <971536df05053020191ebe8f44@mail.gmail.com>	
	<429CEA59.7020206@stat.auckland.ac.nz>
	<971536df050531163353031d97@mail.gmail.com>
Message-ID: <429CFA42.4080106@stat.auckland.ac.nz>

Hi


Gabor Grothendieck wrote:
> On 5/31/05, Paul Murrell <p.murrell at auckland.ac.nz> wrote:
> 
>>Hi
>>
>>
>>Gabor Grothendieck wrote:
>>
>>>I am setting up base plots -- one in viewport A and and one in B.  This part
>>>works fine.  But if I go back to A after having done B and add
>>>horizontal lines it seems
>>>to not use the correct coordinates.  How do I tell it to resume using A's
>>>coordinates?  I am already using par(fig = gridFIG()) but it seems that that's
>>>not enough to reestablish them.  What happens is that when I go back to
>>>A it draws the horizontal lines as if its relative to B's coordinates
>>>rather than
>>>restablishing A's coordinates.  As a result the horizontal lines are
>>>drawn near the
>>>bottom of the graph instead of at the correct heights.  Try running the code
>>>below to see what I mean.
>>>
>>>I have also tried to use baseViewports with this but did not have any
>>>success.
>>>
>>>How do I modify this example so that the horizontal red lines come out
>>>at the appropriate levels?    Note that this is just an example and in
>>>the future I will want to have multiple viewports each with a base plot and
>>>add arbitrary additional line or point plots to them so the solution needs
>>>to be sufficiently general that I can so generalize it.
>>>
>>>Thanks.
>>>
>>>
>>>library(gridBase)
>>>
>>>opar <- par(no.readonly = TRUE)
>>>grid.newpage()
>>>
>>># two columns, one row
>>>unit. <- unit(c(1,1), c("null","null"))
>>>pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
>>>
>>># draw green graph in first column (viewport A)
>>>pushViewport(viewport(layout.pos.col = 1, name = "A"))
>>>par(fig = gridFIG()); par(new = TRUE)
>>>plot(1:10, col = "green", pch = 20)
>>>upViewport(1)
>>>
>>># draw purple graph in second column (viewport B)
>>>pushViewport(viewport(layout.pos.col = 2, name = "B"))
>>>par(fig = gridFIG()); par(new = TRUE)
>>>plot(1:100, col = "purple", pch = 18)
>>>upViewport()
>>>
>>># go back to A and add horizontal grid lines
>>>seekViewport("A")
>>>par(fig = gridFIG())
>>>abline(h=1:10, col = "red")  #### THESE DO NOT GET DRAWN AS EXPECTED
>>>popViewport()
>>>
>>># go back to B and add vertical grid lines
>>>seekViewport("B")
>>>par(fig = gridFIG())
>>>abline(v=1:10, col = "red")
>>>popViewport()
>>>par(opar)
>>
>>
>>The base, or "traditional", graphics system only records the *current*
>>plotting coordinates;  it does not retain a memory of previous plotting
>>coordinates.  What your example does is *reposition* the plotting
>>region, but to do what you want you would have to recreate the plotting
>>coordinates of the first plot.  This is possible (at least in simple
>>cases like the above), as shown below.  However, perhaps a better
>>approach would be to use a combination of grid and lattice plots, where
>>the coordinate systems are retained and don't need to be recreated.  An
>>example of this approach is given at the end.
>>
>>#######
>># Modified example using gridBase
>>#######
>>library(gridBase)
>>
>>opar <- par(no.readonly = TRUE)
>>grid.newpage()
>>
>># two columns, one row
>>unit. <- unit(c(1,1), c("null","null"))
>>pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
>>
>># draw green graph in first column (viewport A)
>>pushViewport(viewport(layout.pos.col = 1, name = "A"))
>>par(fig = gridFIG()); par(new = TRUE)
>>plot(1:10, col = "green", pch = 20)
>>upViewport(1)
>>
>># draw purple graph in second column (viewport B)
>>pushViewport(viewport(layout.pos.col = 2, name = "B"))
>>par(fig = gridFIG()); par(new = TRUE)
>>plot(1:100, col = "purple", pch = 18)
>>upViewport()
>>
>># go back to A and add horizontal grid lines
>>seekViewport("A")
>>par(fig = gridFIG()); par(new=TRUE)  #### extra par(new=TRUE)
>>plot(1:10, type="n", axes=FALSE, ann=FALSE)  #### RESET PLOT A AXES
>>abline(h=1:10, col = "red")
>>popViewport()
>>
>># go back to B and add vertical grid lines
>>seekViewport("B")
>>par(fig = gridFIG()); par(new=TRUE)  #### extra par(new=TRUE)
>>plot(1:100, type="n", axes=FALSE, ann=FALSE)  #### RESET PLOT B AXES
>>abline(v=1:10, col = "red")
>>popViewport()
>>par(opar)
>>
>>
>>#######
>># Similar result but using grid and lattice
>>#######
>>library(grid)
>>library(lattice)
>>
>>grid.newpage()
>>
>># two columns, one row
>>unit. <- unit(c(1,1), c("null","null"))
>>pushViewport(viewport(layout = grid.layout(1, 2, widths = unit.)))
>>
>># draw green graph in first column (viewport A)
>>pushViewport(viewport(layout.pos.col = 1, name = "A"))
>># lattice plot instead of base plot
>>p1 <- xyplot(1:10 ~ 1:10, col="green", pch=20)
>># prefix important so I can refer to it later
>>print(p1, newpage=FALSE, prefix="plotA")
>>upViewport(1)
>>
>># draw purple graph in second column (viewport B)
>>pushViewport(viewport(layout.pos.col = 2, name = "B"))
>>p2 <- xyplot(1:100 ~ 1:100, col="purple", pch=18)
>>print(p2, newpage=FALSE, prefix="plotB")
>>upViewport()
>>
>># go back to A and add horizontal grid lines
>>seekViewport(trellis.vpname("panel", 1, 1, prefix="plotA"))
>># I'm working on a grid.abline() ...
>>grid.segments(x0=0, x1=1,
>>              y0=unit(1:10, "native"),
>>              y1=unit(1:10, "native"),
>>              gp=gpar(col="red"))
>>
>># go back to B and add vertical grid lines
>>seekViewport(trellis.vpname("panel", 1, 1, prefix="plotB"))
>>grid.segments(y0=0, y1=1,
>>              x0=unit(1:10, "native"),
>>              x1=unit(1:10, "native"),
>>              gp=gpar(col="red"))
>>
>>upViewport(0)
>>
>>Paul
>>--
>>Dr Paul Murrell
>>Department of Statistics
>>The University of Auckland
>>Private Bag 92019
>>Auckland
>>New Zealand
>>64 9 3737599 x85392
>>paul at stat.auckland.ac.nz
>>http://www.stat.auckland.ac.nz/~paul/
>>
>>
> 
> 
> Thanks.  I will study that further.  One other question:
> 
> Using layout or mfcol/mfrow (both from base graphics) one can 
> set it up so each new plot goes into a successive cell.  That is one can
> do a traversal of the cells in a layout by just issuing successive
> calls to plot.  Is there something analogous to that in grid?   What I
> am doing right now is to calculate the row and column of the next cell
> and then move to it like this:
> 
>    # mm.row[j] gives the row in the layout of the jth cell
>    # mm.col[j] gives the col in the layout of the jth cell
>    mm <- matrix(seq(nr*nc), nr, nc)
>    mm.row <- c(row(mm))
>    mm.col <- c(col(mm))
> 
>   # go to next cell in the array
>    j <- j + 1 # increment position
>   pushViewport(viewport(layout.pos.row = mm.row[j], layout.pos.col = mm.col[j]))
> 
> Is that how to do it or is there some layout/mfcol-like way?


That is how to do it.

As far as grid is concerned, all viewports are equal and grid has no 
idea whether a viewport corresponds to a "plot region" or a "margin" or 
whatever, so grid has no concept of which viewport is the "next" one to use.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From murdoch at stats.uwo.ca  Wed Jun  1 01:59:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 May 2005 19:59:22 -0400
Subject: [R] FW: why is it numeric(0)?
In-Reply-To: <200505312240.j4VMeccU031252@hypatia.math.ethz.ch>
References: <200505312240.j4VMeccU031252@hypatia.math.ethz.ch>
Message-ID: <429CFA5A.4040006@stats.uwo.ca>

luan_sheng wrote:
>  yea, I have read the help. But some one tell me that i f you want use a
> vector that you don't know it's length, you should use xx=numric(0) ,
is it
> not right? If it isn't right, how can I do?

It depends on the context.  Why do you need a vector, but don't know its
length?  Normally in a situation like that you'd just delay creating it
until you knew the length.

Duncan Murdoch



From kzhao at usc.edu  Wed Jun  1 02:18:41 2005
From: kzhao at usc.edu (Keyan Zhao)
Date: Tue, 31 May 2005 17:18:41 -0700
Subject: [R] specifying values in correlation matrix in nlme
In-Reply-To: <Pine.GSO.4.58.0505301022150.16596@holyrood.ed.ac.uk>
References: <1117149404.22493.37.camel@cycad.usc.edu>
	<Pine.GSO.4.58.0505301022150.16596@holyrood.ed.ac.uk>
Message-ID: <1117585122.9411.6.camel@cycad.usc.edu>

Thanks a lot for your suggestion.
I will try both way and see.
Keyan

On Mon, 2005-05-30 at 10:32 +0100, I M S White wrote:
> The only way I know of in nlme is to transform Zu to Z1 u1 so that
> u1 ~ N(0, cI), which nlme can cope with. E.g. if A = PDP' is the
> spectral decomposition of A, take Z1=ZPD^{1/2}, u1 = D^{-1/2}P'u.
> Unfortunately Z1 is much less sparse than Z and in my experience
> this only works with small problems.
> 
> The kinship package has a function lmekin which will do what you want. It
> uses ML but I reckon it could be easily modified to use REML. It does not
> make use of nlme, it just evaluates the log likelihood and passes it to a
> general purpose optimiser.
> 
> 
> On Thu, 26 May 2005, Keyan Zhao wrote:
> 
> > Could anyone help with a linear mixed model fitting problem ?
> >
> > The model is :
> >
> > Y= Xp + Zu + e
> > where X, Z are known design matrix, p is fixed effect factor, u is
> > random effect,  u~ (0, G) , e~(0,R)
> >
> > The main problem is , I want to fix the covariance matrix G to be a
> > constant times a known covariance matrix A,   G = c*A (c is positive
> > constant, A is a predefined matrix with values manually set by me.
> >
> > I know the correlation option in lme function can specify some kind of
> > correlation. but only with the Construct function defined, not whatever
> > ever form I want.
> >
> > Any good ideas of how to do this in R ?
> >
> > Thanks a lot in advance,
> >
> > Keyan Zhao
> > Computational Biology and Bioinformatics program
> > Univ of Southern California
> > Email: kzhao at usc.edu
> >
> ======================================
> I.White
> University of Edinburgh
> Ashworth Laboratories, West Mains Road
> Edinburgh EH9 3JT
> Tel: 0131 650 5490  Fax: 0131 650 6564
> E-mail: iwhite at staffmail.ed.ac.uk
> ======================================



From Tom.Mulholland at dpi.wa.gov.au  Wed Jun  1 02:42:22 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 1 Jun 2005 08:42:22 +0800
Subject: [R] POSIX problem
Message-ID: <4702645135092E4497088F71D9C8F51A128B84@afhex01.dpi.wa.gov.au>

Well I skipped to the end so pardon me if I've missed something. My first reaction was to go and look at the excellent article by Gabor in RNews 2004-1 on dates (p.32 in particular)

> as.POSIXct(strptime("7/12/2001 10:32",format = "%d/%m/%Y %H:%M"))
[1] "2001-12-07 10:32:00 W. Australia Standard Time"

as.POSIXct handles a limited number of formats and does not have a "format" parameter.

> args(as.POSIXct)
function (x, tz = "") 

So you need to use strptime.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of David Scott
> Sent: Tuesday, 31 May 2005 8:27 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] POSIX problem
> 
> 
> 
> I am having trouble with creating a POSIXct object. If I 
> create a variable 
> of class Date first out of the date part of my data, I am ok, 
> but if I 
> just paste the date and time parts together and try and 
> create the POSIXct 
> object, I have problems.
> 
> Here is a toy example created from the actual data which caused the 
> problem. I am using R 2.0.1 on Windows XP.
> 
> > # Data frame with dates and times, as character
> > PeopleData.df
>     StartDate StartTime
> 1 29/10/2001     15:26
> 2  7/12/2001     10:32
> 3 16/11/2001     13:58
> 4 28/11/2001     14:00
> 5  2/11/2001     15:22
> 6 26/11/2001     11:15
> > str(PeopleData.df)
> `data.frame':   6 obs. of  2 variables:
>   $ StartDate: chr  "29/10/2001" "7/12/2001" "16/11/2001" 
> "28/11/2001" ...
>   $ StartTime: chr  "15:26" "10:32" "13:58" "14:00" ...
> > dput(PeopleData.df)
> structure(list(StartDate = c("29/10/2001", "7/12/2001", "16/11/2001",
> "28/11/2001", "2/11/2001", "26/11/2001"), StartTime = c("15:26",
> "10:32", "13:58", "14:00", "15:22", "11:15")), .Names = c("StartDate",
> "StartTime"), row.names = c("1", "2", "3", "4", "5", "6"), class = 
> "data.frame")
> > BeginDate <- as.Date(PeopleData.df$StartDate,format="%d/%m/%Y")
> > BeginDate
> [1] "2001-10-29" "2001-12-07" "2001-11-16" "2001-11-28" "2001-11-02"
> [6] "2001-11-26"
> > # Create POSIXct date-time object without difficulty
> > BeginTime <- 
> as.POSIXct(format(paste(BeginDate,PeopleData.df$StartTime),
> +                                 format="%Y/%m/%d %H:%M"))
> > BeginTime
> [1] "2001-10-29 15:26:00 New Zealand Standard Time"
> [2] "2001-12-07 10:32:00 New Zealand Standard Time"
> [3] "2001-11-16 13:58:00 New Zealand Standard Time"
> [4] "2001-11-28 14:00:00 New Zealand Standard Time"
> [5] "2001-11-02 15:22:00 New Zealand Standard Time"
> [6] "2001-11-26 11:15:00 New Zealand Standard Time"
> > # But not directly from the dates and times
> > BeginTime <- 
> as.POSIXct(format(paste(PeopleData.df$StartDate,PeopleData.df$
> StartTime),
> +                                 format="%d/%m/%Y %H:%M"))
> > BeginTime
> [1] "0029-10-20 New Zealand Standard Time"
> [2] "0007-12-20 New Zealand Standard Time"
> [3] "0016-11-20 New Zealand Standard Time"
> [4] "0028-11-20 New Zealand Standard Time"
> [5] "0002-11-20 New Zealand Standard Time"
> [6] "0026-11-20 New Zealand Standard Time"
> > # Format looks correct to me
> > paste(PeopleData.df$StartDate,PeopleData.df$StartTime)
> [1] "29/10/2001 15:26" "7/12/2001 10:32"  "16/11/2001 13:58" 
> "28/11/2001 
> 14:00"
> [5] "2/11/2001 15:22"  "26/11/2001 11:15"
> 
> What I think might be causing the problem is the lack of a 
> leading zero 
> for some of the days (as in 7/12/2001). This doesn't phase 
> as.Date though.
> 
> David Scott
> 
> 
> 
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
>  		The University of Auckland, PB 92019
>  		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz
> 
> 
> Graduate Officer, Department of Statistics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Wed Jun  1 02:52:44 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 1 Jun 2005 08:52:44 +0800
Subject: [R] Pairs plot 
Message-ID: <4702645135092E4497088F71D9C8F51A128B85@afhex01.dpi.wa.gov.au>

It would have been helpful if you had written the code that actually showed the issue you have rather than leaving it to us to try and reproduce.

That leaves generic options. If the screen is too small for you then try plotting it to postscript or PDF device and setting the paper size to A3 or whatever size suits you best A4 may be sufficient. You can then use a viewer to scan over the plot or print it out in which case you get the benefit of the higher resolutions available to the printer (typically 300dpi or greater compared to the 96dpi of the screen).

see ?pdf and ?postscript for details on using these devices.

Tom



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Fr??d??ric Ooms
> Sent: Tuesday, 31 May 2005 7:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Pairs plot 
> 
> 
> Hello,
> I would like to draw a pair plot with a lot of descriptors 
> (see the attached file) and due to the number of descriptors 
> contained in the file I am not able to view the plot I only 
> end up with really small square in the graphic device. In am 
> working with the windows version of R.
> Thanks for helping me to get a better view;
> Fred 
>  <<DB_molprop.txt>> 
>



From goedman at mac.com  Wed Jun  1 03:06:21 2005
From: goedman at mac.com (Rob J Goedman)
Date: Tue, 31 May 2005 18:06:21 -0700
Subject: [R] Tiger problems
In-Reply-To: <A6AB7A6E-97AC-485B-95D4-E5FFC6E65C34@haas.berkeley.edu>
References: <A6AB7A6E-97AC-485B-95D4-E5FFC6E65C34@haas.berkeley.edu>
Message-ID: <A3A25AFB-DCD6-474D-9675-E4055588FE85@mac.com>

Thomas,

This is explained on Simon's R Wiki page ( http://wiki.urbanek.info,  
look under TigeR
and troubleshooting tips).

It'll occur in both R.app and when running R from the terminal.

Rob

On May 31, 2005, at 4:37 PM, Thomas Davidoff wrote:

> I get the following when I try to run R from the terminal (I think ok
> from the gui, but not what I want to do):
>
> dyld: Symbol not found: __cg_jpeg_resync_to_restart
>    Referenced from: /System/Library/Frameworks/
> ApplicationServices.framework/Versions/A/Frameworks/ImageIO.framework/
> Versions/A/ImageIO
>    Expected in: /sw/lib/libJPEG.dylib
>
> Trace/BPT trap
>
> Any suggestions?  I have installed the latest both from CRAN mirrors
> and from Mac.
>
> Thomas Davidoff
> Assistant Professor
> Haas School of Business
> UC Berkeley
> Berkeley, CA 94720
> phone:     (510) 643-1425
> fax:        (510) 643-7357
> davidoff at haas.berkeley.edu
> http://faculty.haas.berkeley.edu/davidoff
>
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From ggrothendieck at gmail.com  Wed Jun  1 03:59:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 31 May 2005 21:59:26 -0400
Subject: [R] Loading matrices and other things
In-Reply-To: <429CD97C.2030502@bc.edu>
References: <429CD97C.2030502@bc.edu>
Message-ID: <971536df05053118591facfc37@mail.gmail.com>

On 5/31/05, Mike Schuler <schulerm at bc.edu> wrote:
> Hi all,
> 
> I'm new to R, so needless to say I have a couple questions (which I hope
> I haven't missed through the documentation).
> I have several files in lower triangular matrix form. For each of these
> matrices, I want to perform some form of hierarchical clustering on each
> matrix and capture the output of the clustering.
> 
> The first problem I run into is actually loading the matrix file into R.
> I've attempted using the read.table function but to no avail. What is
> the best way to read in a matrix?
>    Note: matrices are in a form like so, a space between each value,
> then a newline There is also a diagonal of 0's stripped out. (Matrices
> are the output of RNAdistance if that's helpful)
>    Let's say its stored in a file called 'rtest'
>                21
>                34 55
>                55 34 21
>                27 10 61 44
>                59 42 25 8 40
>                61 44 27 10 34 6
>                73 64 57 48 66 44 50
>                78 69 62 53 71 49 55 5
>                77 68 103 94 70 94 96 88 89
>                77 68 103 94 70 90 96 84 85 10
>                31 24 53 46 30 50 52 72 73 74 74
> 
> Second, I've searched through the web and it seems hclust
> <http://www.maths.lth.se/help/R/.R/library/mva/html/hclust.html> is the
> appropriate function From what I can tell from here
> <http://stat.ethz.ch/R-manual/R-devel/library/stats/html/dist.html> the
> above matrix should be a valid format (even without the 0s), but
> confirmation would be nice. And with hclust, does this produce a tree
> with the output, or would that be the plclust function? I haven't been
> able to experiment with this because of my inability to do accomplish
> the previous question.

Here is something to try:

# get number of entries and read in
n <- max(count.fields("myfile.dat")) + 1
x <- scan("myfile.dat")

# create matrix from x
x.mat <- matrix(0,n,n)
x.mat[upper.tri(x.mat)] <- x
x.mat <- x.mat + t(x.mat)

# convert to distance matrix
x.dist <- as.dist(x.mat)

# run hclust
x.hclust <- hclust(x.dist)

# plot
plot(x.hclust, cex = 0.6)
rect.hclust(x.hclust,k=5,border="red")

> And last, I want to be able to run R on many different files of the same
> matrix type. Is it possible to write a (Python) script run through the
> appropriate tasks and save the visual output as a postscript file?

You don't need another language.  It can all be done from R.  Suppose
we want to read in each .dat file in the current directory, plot it and
save the plot:

for (f in dir(patt = "[.]dat$")) {  x <- read.table(f); plot(x);
savePlot(f, "ps") }

savePlot, used above, is specific to Windows. See ?dev.print
if you are not on Windows.



From charles.edwin.white at us.army.mil  Wed Jun  1 04:08:50 2005
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Tue, 31 May 2005 22:08:50 -0400
Subject: RESOLVED: [R] R GUI for Linux?
Message-ID: <8BAEC5E546879B4FAA536200A292C6140DD115@AMEDMLNARMC135.amed.ds.army.mil>

1) As long as the developer version of tcltk is installed, I now have no problem compiling R with tcltk support on Fedora Core 3. After extracting the gunzipped directory, I switch to the new directory and execute the following two commands:
./configure ----enable-R-shlib
make

2) After compiling JGR as specified on their web site, the JGR program is initialized with the 'run' file. That's one of the first things said in the instructions but I forgot by the time I got things compiled.

Thanks for your help.

Chuck

-----Original Message-----
From: Gavin Simpson [mailto:gavin.simpson at ucl.ac.uk]
Sent: Tue 5/31/2005 8:57 AM
To: White, Charles E WRAIR-Wash DC
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R GUI for Linux?
 
White, Charles E WRAIR-Wash DC wrote:
> Thanks for the tip. I thought that 'Base' files were installed already
> but I know better now. Unfortunately, I'm still not up and running yet.
> Since I don't have access to the subject machine at the moment, I'll be
> even more vague than usual.

Base refers to the repository in which the packages reside. Unless you 
did a Full/Complete installation of FC3 from the CD's/DVD then you will 
*not* have all of Base installed.

> I installed the development versions of tcltk and specified the
> locations for the *.sh files, the *.h files, and both of the individual
> directories in /usr/lib. Based on my reading of one of the *.sh files,
> it looks like I can specify multiple directories for a configuration
> variable by putting them in quotes and leaving a space in between like
> 'path1 path2'. My reward was that ./configure didn't complain about
> searching for any tcltk files and I got the clear message that R
> couldn't compile/link to tcltk. Hmph. <grin>
> 
> Next steps when I get home tonight:
> (a) Replace library locations in /usr/lib with those in /usr/share
> (b) Include all four library locations
> (c) Wait until Fedora Core 4 comes out in a couple of weeks, wipe my
> drive, and start all over again.
> 
> Any other suggestions? Thanks again.
> 
> Chuck

How are you specifying the locations of the *.sh and *.h files?

I've done very little to my FC boxes and as long as tcl tk, tcl-devel 
and tk-devel packages are installed, ./configure picks up the locations 
just fine. You shouldn't have to do anything. Perhaps undo you what did 
after installing the tcl/tk and devel packages and try it again as this 
most definitely works for me (almost) out of the box.

If that doesn't work, wait till you get back to your 'subject' machine 
and post back exactly what messages you are receiving that indicate R 
couldn't compile/link to tcl/tk

G

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From charles-r-nospam at plessy.org  Wed Jun  1 05:54:08 2005
From: charles-r-nospam at plessy.org (charles-r-nospam@plessy.org)
Date: Wed, 1 Jun 2005 12:54:08 +0900
Subject: [R] Different versions, different results ?
Message-ID: <20050601035408.GA22620@kunpuu.plessy.org>

Dear all,

I wrote the following batch script on a iMac, and ran it on a linux
mosix cluster.

tu <- read.table("cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshape.table")
tu_reshaped <- t(reshape(tu[1:50,], direction="wide", timevar="tu", idvar=c("rna","lib")))
write.table(tu_reshaped, "cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table")
q(sav="no")

(I will remove the [1:50,] later, the table has 153,646 rows)

The original tables are identical on both machines :

$ head cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshape.table 
"rna" "lib" "tu" "x"
"1" "CAA" "114BA" "1" 2
"2" "CAB" "114BA" "1" 5
"3" "CAC" "114BA" "1" 1
"4" "CAD" "114BA" "1" 5
"5" "CAG" "114BA" "1" 4
"6" "CAH" "114BA" "1" 2
"7" "CAI" "114BA" "1" 6
"8" "CAJ" "114BA" "1" 2
"9" "CAA" "114BB" "1" 1

The written table, however, is different :

GSLC8|Reproducibility|$ head cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table 
"1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "27" "35" "38" "44" "47" "50"
"rna" "CAA" "CAB" "CAC" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAE" "CAF" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAE" "CAE" "CAE" "CAJ" "CAF" "CAD"
"lib" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BA" "061AA" "061AA" "114BA" "062AA"
"x.1" " 2" " 5" " 1" " 5" " 4" " 2" " 6" " 2" " 1" " 5" " 3" " 4" " 2" " 2" " 1" " 2" " 1" " 1" " 2" " 1" " 1" " 1" " 1" " 2" NA NA NA NA NA NA
"x.2" NA " 1" NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA
"x.4" NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA NA NA NA NA NA
"x.5" NA NA NA NA NA NA " 2" " 1" NA " 2" " 1" NA NA NA NA NA " 1" NA NA NA NA NA " 1" NA NA NA NA NA NA NA
"x.7" NA NA NA NA NA NA " 1" NA " 1" NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA
"x.24" " 1" " 2" NA NA NA NA NA NA NA NA " 3" NA " 1" NA NA NA NA NA NA NA NA NA NA NA " 1" NA " 1" NA NA NA
"x.114" NA NA " 1" NA NA NA " 1" NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA NA " 1" NA " 1" " 1" NA

charles at tofu:~$ head cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table
"X1" "X2" "X3" "X4" "X5" "X6" "X7" "X8" "X9" "X10" "X11" "X12" "X13" "X14" "X15" "X16" "X17" "X18" "X19" "X20" "X21" "X22" "X23" "X24" "X27" "X35" "X38" "X44" "X47" "X50"
"rna" "CAA" "CAB" "CAC" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAE" "CAF" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAE" "CAE" "CAE" "CAJ" "CAF" "CAD"
"lib" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BA" "061AA" "061AA" "114BA" "062AA"
"x.1" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5"
"x.2" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.4" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.5" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.7" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.24" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA"
"x.114" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA"

There is something obviously wrong in the second one. The versions are:

GSLC8|Reproducibility|$ R --version
R 2.1.0 Patched (2005-05-12).

charles at tofu:~$ R --version
R 1.5.1 (2002-06-17).

I am wondering wether there is a fix or I have to ask the admin of the mosix cluster to upgrade R...

Best regards,


-- 
Charles Plessy, Ph.D. - Genome Science Laboratory
The Institute for Physical and Chemical Research (RIKEN)
2-1 Hirosawa, Wako, Saitama 351-0198, Japan
plessy at riken.jp --  Fax: 048-462-4686  --  Tel: 048-467-9515



From wang at galton.uchicago.edu  Wed Jun  1 07:04:29 2005
From: wang at galton.uchicago.edu (Yong Wang)
Date: Wed, 1 Jun 2005 00:04:29 -0500 (CDT)
Subject: [R] A problem on sink() and format,suggestions appreciated
In-Reply-To: <200505311005.j4VA5lOK016925@hypatia.math.ethz.ch>
References: <200505311005.j4VA5lOK016925@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0505312343120.29650@aitken.uchicago.edu>

Dear R users
I get a weired problem when use sink:
since the data set pretty big, I sink intermediate result for further 
use,following
lines are consistently used when write data

###########################
sink("dataname.txt")
data
sink()
##########################

at first couples of run, all 10 variables are wrote to a file in following 
format:


 	V1  	   V2 	  .......  	V10
1	10457   132356.7  .......      4356.8
2       75690.2 66697     .......      98777
.	..................................
10000   654786  3412.54   ......       98712.567



then I did modifcations somewhere else, the format change to:

 	V1  	   V2 	  .......  	V9
1	10457   132356.7  .......      7823.569
2       75690.2 66697     .......      77024
.	..................................
10000   654786  3412.54   ......       336721

 	V10
1	4356.8
2       98777
.	.......
10000   98712.567

this format is not convenient to read.
can you hint me where the problem is and how can I make it back to the 
first format?
I worked on this hours but still can not figure out.

any suggestions highly appreciated!

thank you

yong



From lilyfang21 at hotmail.com  Wed Jun  1 07:37:45 2005
From: lilyfang21 at hotmail.com (Fang Lily)
Date: Wed, 01 Jun 2005 05:37:45 +0000
Subject: [R] How to do the coding for simulation in R?
Message-ID: <BAY104-F12EC837FA40CADA337B818D2050@phx.gbl>

Dear list,

     I am working on a project of simulating normal distributed data in R. 
Would you please show me some coding especially regarding how to do the 
circling?

Thanks,

Lily



From 0034058 at fudan.edu.cn  Wed Jun  1 07:59:39 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 01 Jun 2005 13:59:39 +0800
Subject: [R] font size in the trellis plot
Message-ID: <0IHE000OE5HYIJ@mail.fudan.edu.cn>

>library(lattice)
>dotplot(variety ~ yield | site, data = barley, groups = year,
             key = simpleKey(levels(barley$year), space = "right"),
             xlab = "Barley Yield (bushels/acre) ",
             aspect=0.5, layout = c(1,6), ylab=NULL)

and i get the plot whose font overlaps .what parematers should i change.(i do not want to change the size of the plot).



From luan_sheng at yahoo.com  Wed Jun  1 08:07:26 2005
From: luan_sheng at yahoo.com (luan_sheng)
Date: Wed, 01 Jun 2005 14:07:26 +0800
Subject: [R] A problem on sink() and format,suggestions appreciated
In-Reply-To: <Pine.LNX.4.61.0505312343120.29650@aitken.uchicago.edu>
References: <200505311005.j4VA5lOK016925@hypatia.math.ethz.ch>
	<Pine.LNX.4.61.0505312343120.29650@aitken.uchicago.edu>
Message-ID: <429D509E.50900@yahoo.com>

Yong Wang ÅÂÜôÅÈÅì:

> Dear R users
> I get a weired problem when use sink:
> since the data set pretty big, I sink intermediate result for further 
> use,following
> lines are consistently used when write data
>
> ###########################
> sink("dataname.txt")
> data
> sink()
> ##########################
>
> at first couples of run, all 10 variables are wrote to a file in 
> following format:
>
>
> V1 V2 ....... V10
> 1 10457 132356.7 ....... 4356.8
> 2 75690.2 66697 ....... 98777
> . ..................................
> 10000 654786 3412.54 ...... 98712.567
>
>
>
> then I did modifcations somewhere else, the format change to:
>
> V1 V2 ....... V9
> 1 10457 132356.7 ....... 7823.569
> 2 75690.2 66697 ....... 77024
> . ..................................
> 10000 654786 3412.54 ...... 336721
>
> V10
> 1 4356.8
> 2 98777
> . .......
> 10000 98712.567
>
> this format is not convenient to read.
> can you hint me where the problem is and how can I make it back to the 
> first format?
> I worked on this hours but still can not figure out.
>
> any suggestions highly appreciated!
>
> thank you
>
> yong
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
I dont' know the method you write data into your file.But if you use 
"write" function ,then you can specify the colums by the
"ncolumns". The following is the usage "write"

write(x, file = "data", ncolumns = if(is.character(x)) 1 else 5, append 
= FALSE)



__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰



From ligges at statistik.uni-dortmund.de  Wed Jun  1 08:15:24 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jun 2005 08:15:24 +0200
Subject: [R] question regarding new class
In-Reply-To: <BAY10-F2182A2D1A11FFCF3CB63E9D6040@phx.gbl>
References: <BAY10-F2182A2D1A11FFCF3CB63E9D6040@phx.gbl>
Message-ID: <429D527C.2050204@statistik.uni-dortmund.de>

Laura Holt wrote:

> Hi R people:
> 
> I have created a new class for a project that I am working on.  It works 
> fine.
> 
> Just one question, please:  When I access the slots, is there any way 
> that I could use
> x$Name instead on x at Name, please?  Or is it that way by design, please?

This is by design, x is not a list but of class ....

Insetad, you can use slot(), of course or define special exttractor 
functions fo your particular class.

Uwe Ligges

> Thanks.
> 
> Laura Holt
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jun  1 08:21:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jun 2005 08:21:31 +0200
Subject: [R] How to do the coding for simulation in R?
In-Reply-To: <BAY104-F12EC837FA40CADA337B818D2050@phx.gbl>
References: <BAY104-F12EC837FA40CADA337B818D2050@phx.gbl>
Message-ID: <429D53EB.3090901@statistik.uni-dortmund.de>

Fang Lily wrote:

> Dear list,
> 
>     I am working on a project of simulating normal distributed data in 
> R. Would you please show me some coding especially regarding how to do 
> the circling?
> 
> Thanks,
> 
> Lily
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



Dear Lily,

PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

Please read the manual "An Introduction to R".

After you have read *both* of them, and followed the advices given
therein, you are welcome to ask some questions.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Jun  1 08:25:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jun 2005 08:25:12 +0200
Subject: [R] Different versions, different results ?
In-Reply-To: <20050601035408.GA22620@kunpuu.plessy.org>
References: <20050601035408.GA22620@kunpuu.plessy.org>
Message-ID: <429D54C8.7050205@statistik.uni-dortmund.de>

charles-r-nospam at plessy.org wrote:

> Dear all,
> 
> I wrote the following batch script on a iMac, and ran it on a linux
> mosix cluster.
> 
> tu <- read.table("cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshape.table")
> tu_reshaped <- t(reshape(tu[1:50,], direction="wide", timevar="tu", idvar=c("rna","lib")))
> write.table(tu_reshaped, "cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table")
> q(sav="no")
> 
> (I will remove the [1:50,] later, the table has 153,646 rows)
> 
> The original tables are identical on both machines :
> 
> $ head cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshape.table 
> "rna" "lib" "tu" "x"
> "1" "CAA" "114BA" "1" 2
> "2" "CAB" "114BA" "1" 5
> "3" "CAC" "114BA" "1" 1
> "4" "CAD" "114BA" "1" 5
> "5" "CAG" "114BA" "1" 4
> "6" "CAH" "114BA" "1" 2
> "7" "CAI" "114BA" "1" 6
> "8" "CAJ" "114BA" "1" 2
> "9" "CAA" "114BB" "1" 1
> 
> The written table, however, is different :
> 
> GSLC8|Reproducibility|$ head cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table 
> "1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "27" "35" "38" "44" "47" "50"
> "rna" "CAA" "CAB" "CAC" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAE" "CAF" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAE" "CAE" "CAE" "CAJ" "CAF" "CAD"
> "lib" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BA" "061AA" "061AA" "114BA" "062AA"
> "x.1" " 2" " 5" " 1" " 5" " 4" " 2" " 6" " 2" " 1" " 5" " 3" " 4" " 2" " 2" " 1" " 2" " 1" " 1" " 2" " 1" " 1" " 1" " 1" " 2" NA NA NA NA NA NA
> "x.2" NA " 1" NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA
> "x.4" NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA NA NA NA NA NA
> "x.5" NA NA NA NA NA NA " 2" " 1" NA " 2" " 1" NA NA NA NA NA " 1" NA NA NA NA NA " 1" NA NA NA NA NA NA NA
> "x.7" NA NA NA NA NA NA " 1" NA " 1" NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA
> "x.24" " 1" " 2" NA NA NA NA NA NA NA NA " 3" NA " 1" NA NA NA NA NA NA NA NA NA NA NA " 1" NA " 1" NA NA NA
> "x.114" NA NA " 1" NA NA NA " 1" NA NA NA NA NA NA NA NA NA NA NA " 1" NA NA NA NA NA NA " 1" NA " 1" " 1" NA
> 
> charles at tofu:~$ head cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table
> "X1" "X2" "X3" "X4" "X5" "X6" "X7" "X8" "X9" "X10" "X11" "X12" "X13" "X14" "X15" "X16" "X17" "X18" "X19" "X20" "X21" "X22" "X23" "X24" "X27" "X35" "X38" "X44" "X47" "X50"
> "rna" "CAA" "CAB" "CAC" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAE" "CAF" "CAG" "CAH" "CAI" "CAJ" "CAA" "CAB" "CAD" "CAG" "CAH" "CAI" "CAJ" "CAE" "CAE" "CAE" "CAJ" "CAF" "CAD"
> "lib" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BA" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BB" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BC" "114BA" "061AA" "061AA" "114BA" "062AA"
> "x.1" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5" "2" "5"
> "x.2" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.4" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.5" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.7" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA""x.24" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA"
> "x.114" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA" "NA"
> 
> There is something obviously wrong in the second one. The versions are:
> 
> GSLC8|Reproducibility|$ R --version
> R 2.1.0 Patched (2005-05-12).
> 
> charles at tofu:~$ R --version
> R 1.5.1 (2002-06-17).
> 
> I am wondering wether there is a fix or I have to ask the admin of the mosix cluster to upgrade R...


Yes, yes:
Yes there is a fix: R has been fixed more than once in the last 3 year. 
More precisely, there have been 12 *releases* in the meantime.

So please upgrade!

Uwe Ligges


> Best regards,
> 
>



From ligges at statistik.uni-dortmund.de  Wed Jun  1 08:38:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jun 2005 08:38:14 +0200
Subject: [R] font size in the trellis plot
In-Reply-To: <0IHE000OE5HYIJ@mail.fudan.edu.cn>
References: <0IHE000OE5HYIJ@mail.fudan.edu.cn>
Message-ID: <429D57D6.2060901@statistik.uni-dortmund.de>

ronggui wrote:

>>library(lattice)
>>dotplot(variety ~ yield | site, data = barley, groups = year,
> 
>              key = simpleKey(levels(barley$year), space = "right"),
>              xlab = "Barley Yield (bushels/acre) ",
>              aspect=0.5, layout = c(1,6), ylab=NULL)
> 
> and i get the plot whose font overlaps .what parematers should i change.(i do not want to change the size of the plot).


Please read ?dotplot. It's huge, but also very informative and suggests
to specify the argument scales=list(cex=...) as in:



 library(lattice)
 dotplot(variety ~ yield | site, data = barley, groups = year,
              key = simpleKey(levels(barley$year), space = "right"),
              xlab = "Barley Yield (bushels/acre) ",
  aspect=0.5, layout = c(1,6), ylab=NULL, scales=list(cex=0.5))

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Jussi.Makinen at valtiokonttori.fi  Wed Jun  1 08:47:43 2005
From: Jussi.Makinen at valtiokonttori.fi (=?iso-8859-1?Q?M=E4kinen_Jussi?=)
Date: Wed, 1 Jun 2005 09:47:43 +0300
Subject: [R] A suggestion to improve ifelse behaviour with vector
	yes/noarguments
Message-ID: <0BDE2460F08BF0429F933A40431A61E801E82452@vk2kmail01.valtiokonttori.local>


> Thomas Lumley wrote:
> > On Tue, 31 May 2005, Duncan Murdoch wrote:
> > 
> > 
> >>M??kinen Jussi wrote:
> >>
> >>>Dear All,
> >>>
> >>>I luckily found the following feature (or problem) when tried to 
> >>>apply
> >>>ifelse-function to an ordered data.
> >>>
> >>>
> >>>
> >>>>test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE) ifelse(test, 
> >>>>0, 1:4)
> >>>
> >>>[1] 0 0 0 4 1 2 3
> >>>
> > 
> > <snippage>
> > 
> >>As Dimitris said, this is just recycling.  I think getting rid of 
> >>recycling
> >>on vectors with length greater than 1 would have been a good decision in S 
> >>about 15 years ago, but it's too late now.
> > 
> > 
> > It wouldn't help the original poster, though.  I agree that 
> > 0,0,0,4,1,2,3
> > is a slightly weird result, but I can't think of any reasonable model for 
> > the behaviour of ifelse() that would give any other result except an error 
> > message. [or  0,NA,NA,4,NA,NA,NA, I suppose].
> 
> I would vote for the error message.  I can't think of a single example 
> where a vector of length 7 is needed, and a vector of length 4 is 
> recycled to give it, that *doesn't* give a slightly weird result.
> 
> Maybe this is something that should have been changed in R 2.0.0; we 
> squandered that change from 1.x.x to 2.x.x.
> 
> Duncan Murdoch

Hello,

I'm happy with the modified ifelse:

ifelse.o <- function (test, yes, no) 
{
    storage.mode(test) <- "logical"
    ans <- test
    nas <- is.na(test)
    if (any(test[!nas])) 
        ans[test & !nas] <- rep(yes, length.out = length(ans))[test & 
            !nas]
    if (any(!test[!nas])) 
		### Changed
        ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))
    ans[nas] <- NA
    ans
}

giving me:

> test <- c(TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE)
> ifelse.o(test, 0, 1:4)
 [1] 0 0 0 1 2 3 4

and

> test <- c(FALSE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)
> ifelse.o(test, 0, 1:4)
 [1] 1 2 0 0 0 3 4 1 2 3 4

comparing to:

> ifelse(test, 0, 1:4)
 [1] 1 2 0 0 0 2 3 4 1 2 3

So in 'ifelse.o' the recycling starts from the first element of the 'no' -argument and continues the cycling in a logical way (IMHO). That is something I expected 'ifelse' would do.

Thank you for your comments,

Jussi M??kinen



From ligges at statistik.uni-dortmund.de  Wed Jun  1 08:50:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jun 2005 08:50:03 +0200
Subject: [R] help
In-Reply-To: <b7a5c128bb4c4db6afaab75c54d89580@oninet.pt>
References: <b7a5c128bb4c4db6afaab75c54d89580@oninet.pt>
Message-ID: <429D5A9B.6050803@statistik.uni-dortmund.de>

It


is


really


hard


to


read


your


message.


Anyway,


I


guess


you


are


looking


for


?reshape


Uwe Ligges







jose silva wrote:

> Dear all:
> 
> 
>  
> 
>  
> 
> 
>  
> 
> I have this:
> 
> 
>  
> 
>  A1 B1 C1 D1 E1
> 
> 
>  
> 
> A2 B2 C2 D2 E2
> 
> 
>  
> 
> A3 B3 C3 D3 E3
> 
> 
>  
> 
>  
> 
> 
>  
> 
> And I want this
> 
> 
>  
> 
> A1 E1
> 
> 
>  
> 
> B1 E1
> 
> 
>  
> 
> C1 E1
> 
> 
>  
> 
> D1 E1
> 
> 
>  
> 
> A2 E2
> 
> 
>  
> 
> B2 E2
> 
> 
>  
> 
> C2 E2
> 
> 
>  
> 
> D2 E2
> 
> 
>  
> 
> A3 E3
> 
> 
>  
> 
> B3 E3
> 
> 
>  
> 
> C3 E3
> 
> 
>  
> 
> D3 E3
> 
> 
>  
> 
>  
> 
> 
>  
> 
> Example:
> 
> 
>  
> 
>  
> 
> 
>  
> 
> m<- matrix(1:15,nrow=3,byrow=T)
> 
> 
>  
> 
> m
> 
> 
>  
> 
> v<- unlist(list(t(m[,1:4])))
> 
> 
>  
> 
> u<- rep(c(5,10,15),c(4,4,4))
> 
> 
>  
> 
> data.frame(v,u)
> 
> 
>  
> 
>  
> 
> 
>  
> 
> This is the result I want but I would like to learn a simpler way to do it. Any clue?
> 
> 
>  
> 
>  
> 
> 
>  
> 
> Thanks
> 
> 
>  
> 
>  
> 
> 
>  
> 
> j. silva
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jlvw at na.rau.ac.za  Wed Jun  1 08:48:42 2005
From: jlvw at na.rau.ac.za (Jacob van Wyk)
Date: Wed, 01 Jun 2005 08:48:42 +0200
Subject: [R] Re: Errors in variables
Message-ID: <s29d767f.015@rauzen.rau.ac.za>

Just a note to thank everybody who responded to my question.
Much appreciated!
Jacob


Jacob L van Wyk
Department of Mathematics and Statistics
University of Johannesburg APK
P O Box 524
Auckland Park 2006
South Africa
Tel: +27-11-489-3080
Fax: +27-11-489-2832



From j.van_den_hoff at fz-rossendorf.de  Wed Jun  1 10:18:55 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 01 Jun 2005 10:18:55 +0200
Subject: [R] x11 and pseudo-color
Message-ID: <429D6F6F.9010000@fz-rossendorf.de>

for some reason the following message seems not to have reached the list 
  in the first try, at least I can't find it. my apologies if this is my 
fault:

we are running R under Solaris with SunRay Terminals, which are set to 8
bit color to comply with some other software. In this configuration,
X11() opens with colortype=true, i.e., it is not recognized that
actually the display is only 8 bit. This leads to error messages
(advising to use 'pseudo.cube').

question 1: is X11() supposed to recognize  the actual color
capabilities? i.e. is this a bug?

question 2: is there a possibility to query the color capabilities from
within R in order to being able to open the X11() displays always (for
true color as well as for 8 bit) with the correct colortype setting from
within a function?

regards,
joerg



From luk111111 at yahoo.com  Wed Jun  1 11:15:08 2005
From: luk111111 at yahoo.com (luk)
Date: Wed, 1 Jun 2005 02:15:08 -0700 (PDT)
Subject: [R] determine the shrinkage threshold in PAMR?
Message-ID: <20050601091508.89844.qmail@web30904.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050601/faf3d07d/attachment.pl

From jtk at cmp.uea.ac.uk  Wed Jun  1 12:20:30 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 1 Jun 2005 11:20:30 +0100
Subject: [R] Increasing Console "Paste Buffer"
In-Reply-To: <429CE969.8070505@ucl.ac.uk>
References: <1117572426.11503.12.camel@localhost.localdomain>
	<429CE969.8070505@ucl.ac.uk>
Message-ID: <20050601102030.GD21551@jtkpc.cmp.uea.ac.uk>

On Tue, May 31, 2005 at 11:47:05PM +0100, Gavin Simpson wrote:
> Manuel Morales wrote:
> >Hello list.
> >
> >I'm using R from the gnome-terminal in Fedora. My preference is to write
> >programs in VIM, and then source the file from R, or copy and paste the
> >lines into the console. I'm wondering if there is a way to increase the
> >"paste buffer" as an alternative to "sourcing" large analyses. As was
> >mentioned in a recent thread on Linux GUI's, I find that if I paste in a
> >large amount of text, the lines end up getting cut off at some point. I
> >wonder if this is an R restriction, because it seems like I am able to
> >paste substantially more text in other console-based programs. Is there
> >any way to increase the amount of text that I can paste into an R
> >session?
> >
> >Thanks!
> >
> >Manuel
> >
> 
> Manuel,
> 
> Maybe I misunderstand what you mean by "lines end up getting cut off at 
> some point" so correct me if I got it wrong, but I assume you mean that 
> after a certain number of lines entered you can no longer scroll back up 
> and view the earlier lines?

I think that this is not an issue of the scroll buffer, but of buffers
internal to the terminal program or the shell, which are designed to hold
keyboard input and which can be overwhelmed by the rate of input when
large text selections are pasted in, as this appears as though thousands
of keys had been typed almost instantaneously from their view, so to speak.

The point at which the buffer overruns is quite unpredictable and
irreproducible, but generally, the slower a program is to interpret its
input, the faster the overrun occurs. Editors like vim are likely processing
their input much faster than R, and they may therefore be much less prone
to this effect.

I've seen this phenomenon with rxvt and the fancy terminals that come with
Gnome and KDE. The only terminal program with which I've never seen that
is xterm -- but that doesn't mean that xterm is entirely proof against such
loss of input either.

Pasting in larger amounts of code frequently results in a screen which is
rather difficult to interpret. More than once, I've been called to help
people who didn't get the desired result from pasting code they presumed
correct int some terminal, only to find that they were overlooking that
error message triggered by line 7 out of 53 lines because that was hidden
in the swamp resulting from all the subsequent lines of input and any
output triggered by these. One fundamental problem with pasting lines is
that the pasted matter will continue to be entered into the interpreter
regardless of any errors caused along the way.

For these reasons, I generally strongly recommend against pasting into
terminals.

In R, use the source() instead...  ;-)

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From m_osm at gmx.net  Wed Jun  1 11:26:22 2005
From: m_osm at gmx.net (Mahdi Osman)
Date: Wed, 1 Jun 2005 11:26:22 +0200 (MEST)
Subject: [R] glm
Message-ID: <12601.1117617982@www74.gmx.net>

Hi, list,

I have got a dataset on soil and plant. I would like to fit a "glm" to my
data. My response variable is percentage data. That is percent of plant root
length colonized by Arbuscular micorrhiza fungi. Because of the nature of my
data, I am not quite sure whether gamma or gaussian distribution is suitable
for this type of data. If I use gamma distribution, which link function is
appropriate?

I appreciate your suggestions very much.

Thanks in advance!


Mahdi


-- 
-----------------------------------
Mahdi Osman (PhD)
E-mail: m_osm at gmx.net
-----------------------------------

Geschenkt: 3 Monate GMX ProMail gratis + 3 Ausgaben stern gratis
++ Jetzt anmelden & testen ++ http://www.gmx.net/de/go/promail ++



From flatman at swing.be  Wed Jun  1 11:46:20 2005
From: flatman at swing.be (Flatman)
Date: Wed, 01 Jun 2005 11:46:20 +0200
Subject: [R] emacs
Message-ID: <m2y89uif4j.fsf@flatsoft.no-ip.info>

hi !

is there an emacs mode for .r code ?

thanks
erik



From sdavis2 at mail.nih.gov  Wed Jun  1 11:52:44 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 1 Jun 2005 05:52:44 -0400
Subject: [R] emacs
In-Reply-To: <m2y89uif4j.fsf@flatsoft.no-ip.info>
References: <m2y89uif4j.fsf@flatsoft.no-ip.info>
Message-ID: <97269435966ebeadc25dae04e1ec892d@mail.nih.gov>

http://ess.r-project.org/Manual/ess.html

On Jun 1, 2005, at 5:46 AM, Flatman wrote:

> hi !
>
> is there an emacs mode for .r code ?
>
> thanks
> erik
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Kevin.Wang at maths.anu.edu.au  Wed Jun  1 11:52:53 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Wed, 01 Jun 2005 19:52:53 +1000
Subject: [R] emacs
In-Reply-To: <m2y89uif4j.fsf@flatsoft.no-ip.info>
References: <m2y89uif4j.fsf@flatsoft.no-ip.info>
Message-ID: <429D8575.9040705@maths.anu.edu.au>

Hi,

Check out the ESS site, http://stat.ethz.ch/ESS/

Cheers,

Kev


Flatman wrote:
> hi !
> 
> is there an emacs mode for .r code ?
> 
> thanks
> erik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From j.van_den_hoff at fz-rossendorf.de  Wed Jun  1 11:53:51 2005
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 01 Jun 2005 11:53:51 +0200
Subject: [R] nls(() and trace
Message-ID: <429D85AF.8090503@fz-rossendorf.de>

hi everybody,

is there a canonical way to get hold of the "trace=TRUE" output from 
nls, i.e. to copy it to a R variable (or at least to an external log file)?

I have only found the possibility to "fix(nlsModel)" (and than the 
correct copy of that: namespace function ...) within the R-session by 
modifying the trace() definition within nlsModel. not really good for 
everyday use ...


regards,
joerg


ps: if this comes to douglas bates' attention: would'nt it be helpful to 
add the trace output as a further component(possibly optitional, 
depending on the trace flag)  to the nls-object returned by nls()?



From ozric at web.de  Wed Jun  1 12:01:19 2005
From: ozric at web.de (christian schulz)
Date: Wed, 01 Jun 2005 12:01:19 +0200
Subject: [R] many chr2factors ?
Message-ID: <429D876F.1070106@web.de>

Hi,

i would like transfrom 
characters from a data.frame to factors automatic.

 > tofac <- function(df){
+ i=0
+ repeat{
+ i <- i+1
+ if(!is.character(df[,i]))
+ next
+ df[,i] <- as.factor(df[,i])
+ print(i)
+ if(i == length(df))
+ break }
+ }
 >
 > tofac(abrdat)
[1] 7
[1] 8
[1] 9
[1] 11
[1] 13
[1] 15
Error in "[.data.frame"(df, , i) : undefined columns selected

This are the correct columns and i get the idea put into the loop
a empty matrix with dimension like df and return it!?

Another check?
abrdat2 <- apply(abrdat,2,function(x) 
ifelse(is.character(x),as.factor(x),x))


many thanks & regards,
christian



From gavin.simpson at ucl.ac.uk  Wed Jun  1 12:29:59 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 01 Jun 2005 11:29:59 +0100
Subject: [R] Increasing Console "Paste Buffer"
In-Reply-To: <20050601102030.GD21551@jtkpc.cmp.uea.ac.uk>
References: <1117572426.11503.12.camel@localhost.localdomain>
	<429CE969.8070505@ucl.ac.uk>
	<20050601102030.GD21551@jtkpc.cmp.uea.ac.uk>
Message-ID: <429D8E27.9060808@ucl.ac.uk>

Jan T. Kim wrote:
> On Tue, May 31, 2005 at 11:47:05PM +0100, Gavin Simpson wrote:
> 
>>Manuel Morales wrote:
>>
>>>Hello list.
>>>
>>>I'm using R from the gnome-terminal in Fedora. My preference is to write
>>>programs in VIM, and then source the file from R, or copy and paste the
>>>lines into the console. I'm wondering if there is a way to increase the
>>>"paste buffer" as an alternative to "sourcing" large analyses. As was
>>>mentioned in a recent thread on Linux GUI's, I find that if I paste in a
>>>large amount of text, the lines end up getting cut off at some point. I
>>>wonder if this is an R restriction, because it seems like I am able to
>>>paste substantially more text in other console-based programs. Is there
>>>any way to increase the amount of text that I can paste into an R
>>>session?
>>>
>>>Thanks!
>>>
>>>Manuel
>>>
>>
>>Manuel,
>>
>>Maybe I misunderstand what you mean by "lines end up getting cut off at 
>>some point" so correct me if I got it wrong, but I assume you mean that 
>>after a certain number of lines entered you can no longer scroll back up 
>>and view the earlier lines?
> 
> 
> I think that this is not an issue of the scroll buffer, but of buffers
> internal to the terminal program or the shell, which are designed to hold
> keyboard input and which can be overwhelmed by the rate of input when
> large text selections are pasted in, as this appears as though thousands
> of keys had been typed almost instantaneously from their view, so to speak.

I did say I was guessing :-)

> 
> For these reasons, I generally strongly recommend against pasting into
> terminals.

Thanks for this Jan. I haven't noticed this myself but then again I hate 
copy/paste and rarely use R outside emacs/ess these days.

> In R, use the source() instead...  ;-)

Agreed. source("filename", echo = TRUE) will sort of replicate the 
behaviour the original poster would get if they like to see the commands 
printed among the results. But if he is pasting in that much data, 
Manuel will still have to increase the buffer on the terminal, 
especially if he is using one of the defaults in FC3 as the output will 
quickly get lost.

> Best regards, Jan

All the best,

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From BPikouni at CNTUS.JNJ.COM  Wed Jun  1 12:43:31 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Wed, 1 Jun 2005 06:43:31 -0400 
Subject: [R] panel.axis() & grid/lattice settings
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB01990113@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050601/27d28951/attachment.pl

From slist at oomvanlieshout.net  Wed Jun  1 12:49:52 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Wed, 01 Jun 2005 12:49:52 +0200
Subject: [R] Increasing Console "Paste Buffer"
In-Reply-To: <429D8E27.9060808@ucl.ac.uk>
References: <1117572426.11503.12.camel@localhost.localdomain>	<429CE969.8070505@ucl.ac.uk>	<20050601102030.GD21551@jtkpc.cmp.uea.ac.uk>
	<429D8E27.9060808@ucl.ac.uk>
Message-ID: <429D92D0.7030403@oomvanlieshout.net>

An interesting thought just came to me when reading this discussion! I 
use both R and Latex and have never had the trouble of overlooking error 
messages when debugging long Latex code!

Of course this is because when compiling a latex document, a summary of 
the compilation process is provided at the end! If any errors occurred, 
they will be mentioned in the summary.

Maybe R could provide the same summary as an optional part of the 
source() command!?

Cheers,

Sander.



Gavin Simpson wrote:
> Jan T. Kim wrote:
>> On Tue, May 31, 2005 at 11:47:05PM +0100, Gavin Simpson wrote:
>>
>>> Manuel Morales wrote:
>>>
>>>> Hello list.
>>>>
>>>> I'm using R from the gnome-terminal in Fedora. My preference is to 
>>>> write
>>>> programs in VIM, and then source the file from R, or copy and paste the
>>>> lines into the console. I'm wondering if there is a way to increase the
>>>> "paste buffer" as an alternative to "sourcing" large analyses. As was
>>>> mentioned in a recent thread on Linux GUI's, I find that if I paste 
>>>> in a
>>>> large amount of text, the lines end up getting cut off at some point. I
>>>> wonder if this is an R restriction, because it seems like I am able to
>>>> paste substantially more text in other console-based programs. Is there
>>>> any way to increase the amount of text that I can paste into an R
>>>> session?
>>>>
>>>> Thanks!
>>>>
>>>> Manuel
>>>>
>>>
>>> Manuel,
>>>
>>> Maybe I misunderstand what you mean by "lines end up getting cut off 
>>> at some point" so correct me if I got it wrong, but I assume you mean 
>>> that after a certain number of lines entered you can no longer scroll 
>>> back up and view the earlier lines?
>>
>>
>> I think that this is not an issue of the scroll buffer, but of buffers
>> internal to the terminal program or the shell, which are designed to hold
>> keyboard input and which can be overwhelmed by the rate of input when
>> large text selections are pasted in, as this appears as though thousands
>> of keys had been typed almost instantaneously from their view, so to 
>> speak.
> 
> I did say I was guessing :-)
> 
>>
>> For these reasons, I generally strongly recommend against pasting into
>> terminals.
> 
> Thanks for this Jan. I haven't noticed this myself but then again I hate 
> copy/paste and rarely use R outside emacs/ess these days.
> 
>> In R, use the source() instead...  ;-)
> 
> Agreed. source("filename", echo = TRUE) will sort of replicate the 
> behaviour the original poster would get if they like to see the commands 
> printed among the results. But if he is pasting in that much data, 
> Manuel will still have to increase the buffer on the terminal, 
> especially if he is using one of the defaults in FC3 as the output will 
> quickly get lost.
> 
>> Best regards, Jan
> 
> All the best,
> 
> Gav
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From martin.klaffenboeck at gmx.at  Wed Jun  1 12:55:59 2005
From: martin.klaffenboeck at gmx.at (Martin Klaffenboeck)
Date: Wed, 01 Jun 2005 12:55:59 +0200
Subject: [R] histogramm?
Message-ID: <1117623359.13574.3.camel@localhost>

Hello there!

When I do freq=F on hist, I get on the left a small number, what exactly
does that mean?

Thanks,
Martin



From gavin.simpson at ucl.ac.uk  Wed Jun  1 13:02:00 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 01 Jun 2005 12:02:00 +0100
Subject: [R] histogramm?
In-Reply-To: <1117623359.13574.3.camel@localhost>
References: <1117623359.13574.3.camel@localhost>
Message-ID: <429D95A8.4000807@ucl.ac.uk>

Martin Klaffenboeck wrote:
> Hello there!
> 
> When I do freq=F on hist, I get on the left a small number, what exactly
> does that mean?
> 
> Thanks,
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

Did you look at ?hist, which states:

     freq: logical; if 'TRUE', the histogram graphic is a representation
           of frequencies, the 'counts' component of the result; if
           'FALSE', _relative_ frequencies ("probabilities"), component
           'density', are plotted.   Defaults to 'TRUE' _iff_ 'breaks'
           are equidistant (and 'probability' is not specified).

So freq = FALSE plots relative frequencies (probabilities).

Please read the posting guide and look at the help for the function you 
are using and trying to understand.

HTH

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From murdoch at stats.uwo.ca  Wed Jun  1 13:03:00 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 01 Jun 2005 07:03:00 -0400
Subject: [R] Increasing Console "Paste Buffer"
In-Reply-To: <429D92D0.7030403@oomvanlieshout.net>
References: <1117572426.11503.12.camel@localhost.localdomain>	<429CE969.8070505@ucl.ac.uk>	<20050601102030.GD21551@jtkpc.cmp.uea.ac.uk>	<429D8E27.9060808@ucl.ac.uk>
	<429D92D0.7030403@oomvanlieshout.net>
Message-ID: <429D95E4.4010906@stats.uwo.ca>

Sander Oom wrote:
> An interesting thought just came to me when reading this discussion! I 
> use both R and Latex and have never had the trouble of overlooking error 
> messages when debugging long Latex code!
> 
> Of course this is because when compiling a latex document, a summary of 
> the compilation process is provided at the end! If any errors occurred, 
> they will be mentioned in the summary.
> 
> Maybe R could provide the same summary as an optional part of the 
> source() command!?
> 

I think it does, doesn't it?  R will stop at the first error and print 
it, e.g.

 > source('c:/temp/test.R')
Error in parse(file, n = -1, NULL, "?") : syntax error on line 4

  If there were only warnings, it will show them at the end:

 > source('c:/temp/test.R')
Warning messages:
1: longer object length
         is not a multiple of shorter object length in: 1:3 + 1:4
2: longer object length
         is not a multiple of shorter object length in: 1:3 + 1:4

Even if you use echo=TRUE, these summaries show up at the end.  It's 
only if you use cut and paste that you might miss these.

Duncan Murdoch



From murdoch at stats.uwo.ca  Wed Jun  1 13:12:47 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 01 Jun 2005 07:12:47 -0400
Subject: [R] A suggestion to improve ifelse behaviour with
	vector	yes/noarguments
In-Reply-To: <0BDE2460F08BF0429F933A40431A61E801E82452@vk2kmail01.valtiokonttori.local>
References: <0BDE2460F08BF0429F933A40431A61E801E82452@vk2kmail01.valtiokonttori.local>
Message-ID: <429D982F.3070900@stats.uwo.ca>

M??kinen Jussi wrote:

> Hello,
> 
> I'm happy with the modified ifelse:
> 
> ifelse.o <- function (test, yes, no) 
> {
>     storage.mode(test) <- "logical"
>     ans <- test
>     nas <- is.na(test)
>     if (any(test[!nas])) 
>         ans[test & !nas] <- rep(yes, length.out = length(ans))[test & 
>             !nas]
>     if (any(!test[!nas])) 
> 		### Changed
>         ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))
>     ans[nas] <- NA
>     ans
> }

I wouldn't be:

 > x
  [1] -0.4539550 -1.3023478  0.9034912  1.7485065  0.6910265 -0.7712547
  [7] -0.6345585  1.8296632  2.1207810  0.7643834
 > ifelse.o(x > 0, 1:10, 0)
  [1]  0  0  3  4  5  0  0  8  9 10
 > ifelse.o(x > 0, 0, 1:10)
  [1] 1 2 0 0 0 3 4 0 0 0

I'd call these results fairly perverse.

Duncan Murdoch



From joseclaudio.faria at terra.com.br  Wed Jun  1 13:22:08 2005
From: joseclaudio.faria at terra.com.br (Jose Claudio Faria)
Date: Wed, 01 Jun 2005 08:22:08 -0300
Subject: [R] Re: R GUI for Linux?
Message-ID: <429D9A60.4000805@terra.com.br>

Dears,

I know a little about Object Pascal language and I've been working (in the last 
two years) with the Tinn-R development (www.sciview.org/Tinn-R).

This work started adapting Tinn (a good frame, but with limitations) as an R 
script editor.

Tinn is a small ASCII file editor primarily intended as a better replacement
of the default Notepad.exe under Windows. Tinn is the recursive acronym
'Tinn is not Notepad'. Tinn-R is an extension of Tinn that provides additional
tools to control R (Rgui in MDI or SDI mode, see http://cran.r-project.org,
SciViews R console, see http://www.sciviews.org/SciViews-R) or S-Plus. As such, 
Tinn-R is a feature-rich replacement of the basic script editor provided with 
Rgui. It provides advanced syntax-highlighting, submission of code in whole, or 
line-by-line, and many other useful tools to ease writing and debugging of R 
code. Both Tinn and Tinn-R are distributed under the GPL 2 or above license.

So, I think (but I'm very suspicious), it's a nice R script editor running only 
under Windows.

Otherwise, I don't know nothing about C/C++ language.
My question is, why we not provide (starting from an open source editor like 
Bluefish, Kate, or another good editor under Linux) the resources of Tinn-R. I 
think is not very hard (or impossible) translate the Tinn-R functions/procedures 
from Object Pascal to C/C++.

I think (as coordinator of Tinn-R team) that we can help with this work. All we 
need is start it and that one person (C/C++ programmer) coordinate the works.

Emacs + ESS, in my opinion, is not adequate for beginning.

Best regards,
-- 
Jose Claudio Faria
Brasil/Bahia/UESC/DCET
Estatistica Experimental/Prof. Adjunto
mails:
  joseclaudio.faria at terra.com.br
  jc_faria at uesc.br
  jc_faria at uol.com.br
tel: 73-3634.2779



From rolf at math.unb.ca  Wed Jun  1 13:26:59 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 1 Jun 2005 08:26:59 -0300 (ADT)
Subject: [R] histogramm?
Message-ID: <200506011126.j51BQxRA019579@erdos.math.unb.ca>

Gavin Simpson wrote:

	<snip>

>      freq: logical; if 'TRUE', the histogram graphic is a representation
>            of frequencies, the 'counts' component of the result; if
>            'FALSE', _relative_ frequencies ("probabilities"), component
>            'density', are plotted.   Defaults to 'TRUE' _iff_ 'breaks'
>            are equidistant (and 'probability' is not specified).
> 
> So freq = FALSE plots relative frequencies (probabilities).

	Careful! You left out the quote marks on ``probabilities''.
	These are ***CRUCIAL*** inasmuch as the values are NOT
	probabilities, but rather probability ***densities***.
	Which, as I recall Bill Venables once pointed out to me, is
	the only thing that makes sense.  If you are doing a
	histogram, you are looking at a continuous random variable.
	If the variate in question is discrete, so that you
	***really*** want probabilities, you should be doing a
	barplot.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From slist at oomvanlieshout.net  Wed Jun  1 13:39:53 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Wed, 01 Jun 2005 13:39:53 +0200
Subject: [R] Increasing Console "Paste Buffer"
In-Reply-To: <429D95E4.4010906@stats.uwo.ca>
References: <1117572426.11503.12.camel@localhost.localdomain>	<429CE969.8070505@ucl.ac.uk>	<20050601102030.GD21551@jtkpc.cmp.uea.ac.uk>	<429D8E27.9060808@ucl.ac.uk>	<429D92D0.7030403@oomvanlieshout.net>
	<429D95E4.4010906@stats.uwo.ca>
Message-ID: <429D9E89.6010708@oomvanlieshout.net>

Indeed it does! Sorry for the impulsive response!

Sander.


Duncan Murdoch wrote:
> Sander Oom wrote:
>> An interesting thought just came to me when reading this discussion! I 
>> use both R and Latex and have never had the trouble of overlooking 
>> error messages when debugging long Latex code!
>>
>> Of course this is because when compiling a latex document, a summary 
>> of the compilation process is provided at the end! If any errors 
>> occurred, they will be mentioned in the summary.
>>
>> Maybe R could provide the same summary as an optional part of the 
>> source() command!?
>>
> 
> I think it does, doesn't it?  R will stop at the first error and print 
> it, e.g.
> 
>  > source('c:/temp/test.R')
> Error in parse(file, n = -1, NULL, "?") : syntax error on line 4
> 
>  If there were only warnings, it will show them at the end:
> 
>  > source('c:/temp/test.R')
> Warning messages:
> 1: longer object length
>         is not a multiple of shorter object length in: 1:3 + 1:4
> 2: longer object length
>         is not a multiple of shorter object length in: 1:3 + 1:4
> 
> Even if you use echo=TRUE, these summaries show up at the end.  It's 
> only if you use cut and paste that you might miss these.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From mkondrin at hppi.troitsk.ru  Wed Jun  1 15:08:49 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Wed, 01 Jun 2005 17:08:49 +0400
Subject: [R] Fitting ARMA model with known inputs.
Message-ID: <429DB361.50800@hppi.troitsk.ru>

Hello!
Is it possible to use R time series to identificate a process which is 
subjected to known input? I.e. I have 2 sequences - one is measurements 
of black box's state and the second is the "force" by which this black 
box is driven (which is known too) and I want to fit thist two series 
with AR-process. The "ar" procedure from stats package expects that the 
force is always random. Is it possible to feed it known vector as input 
parameter?
Thank you in advance.



From efg at stowers-institute.org  Wed Jun  1 15:41:05 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Wed, 1 Jun 2005 08:41:05 -0500
Subject: [R] Why does "summary" show number of NAs as non-integer?
References: <d7ineb$nbt$1@sea.gmane.org>
	<1117582325.22595.175.camel@horizons.localdomain>
Message-ID: <d7kdo2$8kt$1@sea.gmane.org>

"Berton Gunter" <gunter.berton at gene.com> wrote in message
news:200505312240.j4VMepGX000203 at hertz.gene.com...
> summary() is an S3 generic that for your vector dispatches
> summary.default(). The output of summary default has class "table" and so
> calls print.table (print is another S3 generic). Look at the code of
> print.table() to see how it formats the output.

"Marc Schwartz" <MSchwartz at MedAnalytics.com> wrote in message
news:1117582325.22595.175.camel at horizons.localdomain...
> On Tue, 2005-05-31 at 17:14 -0500, Earl F. Glynn wrote:

> > Why isn't the number of NA's just "2" instead of the "2.000" shown
above?

> "The same number of decimal places is used throughout a vector

I'm talking about how this should be designed.  The current impementation
may be to print a vector using generic logic, but why use generic logic to
produce a wrong solution? Shouldn't correctness be more important than using
a generic solution?

There is special logic to suppress NA's when they don't exist (see below),
so why isn't there special logic to print the count of NAs, which MUST be an
integer, correctly when they do exist?

An integer should NOT be displayed with meaningless decimal places. Why
would this ever be desirable?  The generic solution should be dropped in
favor of a correct solution.

# Why not use special logic to show the number of NA's correctly as an
integer?
> set.seed(19)
> summary( c(NA, runif(10,1,100), NaN) )
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
  7.771  24.850  43.040  43.940  63.540  83.830   2.000

# There is already special logic to suppress NA's
> set.seed(19)
> summary( runif(10,1,100) )
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  7.771  24.850  43.040  43.940  63.540  83.830

"2.000" and "2" do not have equivalent meaning.

efg



From ggrothendieck at gmail.com  Wed Jun  1 15:48:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 1 Jun 2005 09:48:13 -0400
Subject: [R] Why does "summary" show number of NAs as non-integer?
In-Reply-To: <d7kdo2$8kt$1@sea.gmane.org>
References: <d7ineb$nbt$1@sea.gmane.org>
	<1117582325.22595.175.camel@horizons.localdomain>
	<d7kdo2$8kt$1@sea.gmane.org>
Message-ID: <971536df050601064833ca1249@mail.gmail.com>

On 6/1/05, Earl F. Glynn <efg at stowers-institute.org> wrote:
> "Berton Gunter" <gunter.berton at gene.com> wrote in message
> news:200505312240.j4VMepGX000203 at hertz.gene.com...
> > summary() is an S3 generic that for your vector dispatches
> > summary.default(). The output of summary default has class "table" and so
> > calls print.table (print is another S3 generic). Look at the code of
> > print.table() to see how it formats the output.
> 
> "Marc Schwartz" <MSchwartz at MedAnalytics.com> wrote in message
> news:1117582325.22595.175.camel at horizons.localdomain...
> > On Tue, 2005-05-31 at 17:14 -0500, Earl F. Glynn wrote:
> 
> > > Why isn't the number of NA's just "2" instead of the "2.000" shown
> above?
> 
> > "The same number of decimal places is used throughout a vector
> 
> I'm talking about how this should be designed.  The current impementation
> may be to print a vector using generic logic, but why use generic logic to
> produce a wrong solution? Shouldn't correctness be more important than using
> a generic solution?
> 
> There is special logic to suppress NA's when they don't exist (see below),
> so why isn't there special logic to print the count of NAs, which MUST be an
> integer, correctly when they do exist?
> 
> An integer should NOT be displayed with meaningless decimal places. Why
> would this ever be desirable?  The generic solution should be dropped in
> favor of a correct solution.
> 
> # Why not use special logic to show the number of NA's correctly as an
> integer?
> > set.seed(19)
> > summary( c(NA, runif(10,1,100), NaN) )
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
>  7.771  24.850  43.040  43.940  63.540  83.830   2.000
> 
> # There is already special logic to suppress NA's
> > set.seed(19)
> > summary( runif(10,1,100) )
>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>  7.771  24.850  43.040  43.940  63.540  83.830
> 
> "2.000" and "2" do not have equivalent meaning.

Try:

R> library(Hmisc)
R> describe( c(NA, runif(10,1,100), NaN) )
c(NA, runif(10, 1, 100), NaN) 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
     10       2      10   50.99   15.24   16.82   21.14   52.70   76.35   83.52 
    .95 
  90.79 

          13.65 17.17 18.12 30.18 46.21 59.19 65.36 80.01 81.90 98.06
Frequency     1     1     1     1     1     1     1     1     1     1
%            10    10    10    10    10    10    10    10    10    10



From dvumani at hotmail.com  Wed Jun  1 15:53:01 2005
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Wed, 01 Jun 2005 13:53:01 +0000
Subject: [R] "\n" in legend using substitute and as.expression
Message-ID: <BAY16-F5C692F1F28078958CFE69A3050@phx.gbl>

Dear R users,
Is it possible to force a hard return in an expression. I tried including 
one for a legend using an expression and it didn't work. Here is my code,
legend(12, 0.10, c("Fitted density",
    as.expression(substitute(paste("Weibull (", alpha==shapeU," , ", 
beta==scaleU,") Mean = ",meanU," days"),
list(shapeU = round(mle.r$estimate[1],3),
         scaleU = round(mle.r$estimate[2],3),
         meanU = round(mle.r$estimate[3),4)))) ),
    lty=c(1,1),col=c(1,2),lwd=c(1,1),merge = TRUE)

(Windows XP + R2.10 user)

Thanks,
Vumani



From Jussi.Makinen at valtiokonttori.fi  Wed Jun  1 16:13:11 2005
From: Jussi.Makinen at valtiokonttori.fi (=?iso-8859-1?Q?M=E4kinen_Jussi?=)
Date: Wed, 1 Jun 2005 17:13:11 +0300
Subject: [R] A suggestion to improve ifelse behaviour with
	vector	yes/noarguments
Message-ID: <0BDE2460F08BF0429F933A40431A61E801E82459@vk2kmail01.valtiokonttori.local>

Thanks Duncan,

For pointing this out. You maybe saved my day. I agree that these results are neither something I would like to see. I changed the code to be:

ifelse.o <- function (test, yes, no) 
{
    storage.mode(test) <- "logical"
    ans <- test
    nas <- is.na(test)
    if (any(test[!nas])) 
        ### Own change
	  ans[test & !nas] <- rep(yes, length.out = length(ans[test & !nas]))
    if (any(!test[!nas]))
	### Own change
        ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))
	ans[nas] <- NA
    ans
}

which yields now

> x <- rnorm(10)
> ifelse.o(x > 0, 1:10, 0)
 [1] 1 2 0 3 0 4 0 5 6 0
> ifelse.o(x > 0, 0, 1:10)
 [1] 0 0 1 0 2 0 3 0 0 4

I'm even more happy now :-) I will anyway need to use this modificate function for just one specific problem. Hopefully there is no other hooks,

Jussi



M??kinen Jussi wrote:

> Hello,
> 
> I'm happy with the modified ifelse:
> 
> ifelse.o <- function (test, yes, no)
> {
>     storage.mode(test) <- "logical"
>     ans <- test
>     nas <- is.na(test)
>     if (any(test[!nas])) 
>         ans[test & !nas] <- rep(yes, length.out = length(ans))[test & 
>             !nas]
>     if (any(!test[!nas])) 
> 		### Changed
>         ans[!test & !nas] <- rep(no, length.out = length(ans[!test & !nas]))
>     ans[nas] <- NA
>     ans
> }

I wouldn't be:

 > x
  [1] -0.4539550 -1.3023478  0.9034912  1.7485065  0.6910265 -0.7712547
  [7] -0.6345585  1.8296632  2.1207810  0.7643834
 > ifelse.o(x > 0, 1:10, 0)
  [1]  0  0  3  4  5  0  0  8  9 10
 > ifelse.o(x > 0, 0, 1:10)
  [1] 1 2 0 0 0 3 4 0 0 0

I'd call these results fairly perverse.

Duncan Murdoch



From gavin.simpson at ucl.ac.uk  Wed Jun  1 16:14:45 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 01 Jun 2005 15:14:45 +0100
Subject: [R] StructTS and arima and missing values
Message-ID: <429DC2D5.7090807@ucl.ac.uk>

Dear List,

I am thinking about ways in which I might analyse some stratigraphic 
data. The nature of the data series I have generates a number of issues:

1) The data I have in mind come from a sediment core sequence taken from 
the bottom of a lake. The sequence is sliced into a priori defined 
slices, in this case 0.2cm per slice. in this way a sequence of 0.2cm 
slices is produced for the entire core.
2) Each slice is assigned a date (plus some error) using radiometric 
dating techniques and a derived age/depth model (we age some of the 
samples and then interpolate/extrapolate for the other samples). This 
can be done in a variety of ways but effectively the end result is that 
each 0.2cm sediment slice has a date (year) attached to it (with some 
error). Changes in the lake system tend to result in changes in the 
accumulation rate of the sediment sequence, so what we end up with is 
say a 200 year core sequence that is irregularly sampled in time, but 
regularly in depth down core.

So for example in one core I end up with the following sequence of years 
sampled:

 > dat
  [1] 2001 2000 1999 1998 1997 1996 1994 1993 1992 1990 1988 1986
[13] 1984 1982 1980 1977 1974 1972 1969 1966 1963 1960 1957 1953
[25] 1950 1946 1943 1940 1936 1931 1927 1922 1918 1914 1908 1902
[37] 1896 1890 1884 1878 1872

I am prepared to accept, for the sake of modelling, that these dates are 
known and ignore the errors in the dating if that helps.

Having read Brian Ripley's article on Time series in R News Vol 2(2) 
June 2002, I know that arima and StructTS can now handle missing values, 
and there is some discussion about the specifics of how these functions 
can handle missing values, but it is still not clear, in my mind at 
least, if it would be appropriate to use arima or StructTS on data of 
this nature -- I'm more interested in fitting a structured time series 
to this data.

Can StructTS cope with missing values in the sense that I have described 
them above? If anyone has any suggestions as to how I might approach 
these data using R they would be gratefully received.

Many thanks for your time,

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From chrysopa at gmail.com  Wed Jun  1 16:18:09 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 1 Jun 2005 11:18:09 -0300
Subject: [R] How to automatic obtain the p value from a nested aov?
Message-ID: <200506011118.10024.chrysopa@gmail.com>

Hi,

I try this nested anova:

> m <- aov(Glycogen~Treatment+Error(Treatment/Rat/Liver))
> summary(m)

Error: Treatment
          Df  Sum Sq Mean Sq
Treatment  2 1557.56  778.78

Error: Treatment:Rat
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  3 797.67  265.89               

Error: Treatment:Rat:Liver
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals 12  594.0    49.5               

Error: Within
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals 18 381.00   21.17               
> 

My objective is the effect of treatment.

I know how to calculate the F for this:

778.78/265.89

But, exist any package that make this operation automatic? Is for a class 
work.

Thanks
Ronaldo
-- 
Vou rezar 1/3, para encontrar 1/2 de te levar para 1/4
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From SuzieBlatt at netscape.net  Wed Jun  1 16:28:20 2005
From: SuzieBlatt at netscape.net (SuzieBlatt@netscape.net)
Date: Wed, 01 Jun 2005 10:28:20 -0400
Subject: [R] cor.test on spatial data
Message-ID: <26402548.26F5F992.0D1322AF@netscape.net>


I have spatial data on various species of trees within plots.  What I'd like to know is if their patterns are positively associated or not.  Could I use cor.test on the values generated by Kest or Gest functions? or would it be more powerful to do a G i to j and then G j to i analysis instead?
Thanks,
Suzie

__________________________________________________________________
Switch to Netscape Internet Service.




New! Netscape Toolbar for Internet Explorer
Search from anywhere on the Web and block those annoying pop-ups.
Download now at http://channels.netscape.com/ns/search/install.jsp



From bates at stat.wisc.edu  Wed Jun  1 16:33:23 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 01 Jun 2005 09:33:23 -0500
Subject: [R] A problem on sink() and format,suggestions appreciated
In-Reply-To: <429D509E.50900@yahoo.com>
References: <200505311005.j4VA5lOK016925@hypatia.math.ethz.ch>	<Pine.LNX.4.61.0505312343120.29650@aitken.uchicago.edu>
	<429D509E.50900@yahoo.com>
Message-ID: <429DC733.6030407@stat.wisc.edu>

luan_sheng wrote:
> Yong Wang ÅÂÜôÅÈÅì:
> 
>> Dear R users
>> I get a weired problem when use sink:
>> since the data set pretty big, I sink intermediate result for further
>> use,following
>> lines are consistently used when write data
>>
>> ###########################
>> sink("dataname.txt")
>> data
>> sink()
>> ##########################
>>
>> at first couples of run, all 10 variables are wrote to a file in
>> following format:
>>
>>
>> V1 V2 ....... V10
>> 1 10457 132356.7 ....... 4356.8
>> 2 75690.2 66697 ....... 98777
>> . ..................................
>> 10000 654786 3412.54 ...... 98712.567
>>
>>
>>
>> then I did modifcations somewhere else, the format change to:
>>
>> V1 V2 ....... V9
>> 1 10457 132356.7 ....... 7823.569
>> 2 75690.2 66697 ....... 77024
>> . ..................................
>> 10000 654786 3412.54 ...... 336721
>>
>> V10
>> 1 4356.8
>> 2 98777
>> . .......
>> 10000 98712.567
>>
>> this format is not convenient to read.
>> can you hint me where the problem is and how can I make it back to the
>> first format?
>> I worked on this hours but still can not figure out.
>>
>> any suggestions highly appreciated!
>>
>> thank you
>>
>> yong
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> I dont' know the method you write data into your file.But if you use
> "write" function ,then you can specify the colums by the
> "ncolumns". The following is the usage "write"
> 
> write(x, file = "data", ncolumns = if(is.character(x)) 1 else 5, append
> = FALSE)

Alternatively you could save the original R object to a file, remove it
from your worksheet, and load it later when you need it.  See

?save



From bates at stat.wisc.edu  Wed Jun  1 16:42:24 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 01 Jun 2005 09:42:24 -0500
Subject: [R] nls(() and trace
In-Reply-To: <429D85AF.8090503@fz-rossendorf.de>
References: <429D85AF.8090503@fz-rossendorf.de>
Message-ID: <429DC950.6020006@stat.wisc.edu>

joerg van den hoff wrote:
> hi everybody,
> 
> is there a canonical way to get hold of the "trace=TRUE" output from
> nls, i.e. to copy it to a R variable (or at least to an external log file)?
> 
> I have only found the possibility to "fix(nlsModel)" (and than the
> correct copy of that: namespace function ...) within the R-session by
> modifying the trace() definition within nlsModel. not really good for
> everyday use ...
> 
> 
> regards,
> joerg
> 
> 
> ps: if this comes to douglas bates' attention: would'nt it be helpful to
> add the trace output as a further component(possibly optitional,
> depending on the trace flag)  to the nls-object returned by nls()?

My immediate response was to say that it already was but then I checked
the code and found that indeed it is not.  Yes, this is a good idea and
I will put it on the ToDo list but I should warn you that the list is
rather long.  If anyone else feels motivated to do this it would be most
welcome.



From Luisr at frs.fo  Wed Jun  1 16:57:29 2005
From: Luisr at frs.fo (Luis Ridao Cruz)
Date: Wed, 01 Jun 2005 15:57:29 +0100
Subject: [R] Different y-axis limits in lattice panels
Message-ID: <s29ddaef.034@ffdata.setur.fo>

R-help,

I want to keep the original y-axis limits in panels when drawing
lattice plots.
My data contain some few large values making dificult to see any
patterns/trends in different plots.

What I want to do is to keep the original scale (y-axis) in every
single panel.

I have searched in the archives and in the help pages but found no
answer.

Thanks in advance.

Windows XP

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R              
>



From sofyan.iyan at gmail.com  Wed Jun  1 17:05:52 2005
From: sofyan.iyan at gmail.com (Sofyan Iyan)
Date: Wed, 1 Jun 2005 17:05:52 +0200
Subject: [R] Which variable exist after random
Message-ID: <92e186a0050601080518988055@mail.gmail.com>

Dear R-helper,
How could I count only some variable was exist after running sample
(random) function.
For example,

> testx <- factor(c("Game","Paper","Internet","Time","Money"))
>    for(i in 1:2) {
+        x <- sample(testx,replace=TRUE)
+        print(x)
+    }
[1] Money    Money    Time     Internet Time    
Levels: Game Internet Money Paper Time
[1] Time  Money Game  Money Money
Levels: Game Internet Money Paper Time
> 

The result above Game, Internet, Money, and Time only exist and
"Paper" was missing.
Best, Sofyan



From kevin.thorpe at utoronto.ca  Wed Jun  1 17:08:54 2005
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Wed, 01 Jun 2005 11:08:54 -0400
Subject: [R] Different y-axis limits in lattice panels
In-Reply-To: <s29ddaef.034@ffdata.setur.fo>
References: <s29ddaef.034@ffdata.setur.fo>
Message-ID: <429DCF86.9060205@utoronto.ca>

See ?xyplot and read about the scales argument.

Luis Ridao Cruz wrote:
> R-help,
> 
> I want to keep the original y-axis limits in panels when drawing
> lattice plots.
> My data contain some few large values making dificult to see any
> patterns/trends in different plots.
> 
> What I want to do is to keep the original scale (y-axis) in every
> single panel.
> 
> I have searched in the archives and in the help pages but found no
> answer.
> 
> Thanks in advance.
> 
> Windows XP
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    1.0            
> year     2005           
> month    04             
> day      18             
> language R              
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Kevin E. Thorpe
Biostatistician/Trialist, Knowledge Translation Program
Assistant Professor, Department of Public Health Sciences
Faculty of Medicine, University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.946.8081  Fax: 416.971.2462



From Matthias.Templ at statistik.gv.at  Wed Jun  1 17:14:33 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 1 Jun 2005 17:14:33 +0200
Subject: [R] Which variable exist after random
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAAB5@xchg1.statistik.local>

> Dear R-helper,
> How could I count only some variable was exist after running sample
> (random) function.

?

> For example,
> 
> > testx <- factor(c("Game","Paper","Internet","Time","Money"))
> >    for(i in 1:2) {
> +        x <- sample(testx,replace=TRUE)
> +        print(x)
> +    }
> [1] Money    Money    Time     Internet Time    
> Levels: Game Internet Money Paper Time
> [1] Time  Money Game  Money Money
> Levels: Game Internet Money Paper Time
> > 
> 
> The result above Game, Internet, Money, and Time only exist 
> and "Paper" was missing. Best, Sofyan
> 

sample(testx, replace=FALSE)

Is that what you want??

Best,
Matthias



From Matthias.Templ at statistik.gv.at  Wed Jun  1 17:22:26 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 1 Jun 2005 17:22:26 +0200
Subject: [R] Which variable exist after random
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAAB6@xchg1.statistik.local>

> Dear R-helper,
> How could I count only some variable was exist after running sample
> (random) function.

Or is this what you want?

Tab <- table(sample(testx, replace=TRUE))
Tab[ Tab > 0 ]

> For example,
> 
> > testx <- factor(c("Game","Paper","Internet","Time","Money"))
> >    for(i in 1:2) {
> +        x <- sample(testx,replace=TRUE)
> +        print(x)
> +    }
> [1] Money    Money    Time     Internet Time    
> Levels: Game Internet Money Paper Time
> [1] Time  Money Game  Money Money
> Levels: Game Internet Money Paper Time
> > 
> 
> The result above Game, Internet, Money, and Time only exist 
> and "Paper" was missing. Best, Sofyan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From buser at stat.math.ethz.ch  Wed Jun  1 17:23:03 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 1 Jun 2005 17:23:03 +0200
Subject: [R] many chr2factors ?
In-Reply-To: <429D876F.1070106@web.de>
References: <429D876F.1070106@web.de>
Message-ID: <17053.53975.755861.961421@stat.math.ethz.ch>

Dear Christian

If you create your data frame by using data.frame all characters
are automatically transformed into factors unless you force them
to stay a character. Maybe that can solve your problem easily.

dat <- data.frame(a=1:10, b=letters[1:10])
str(dat)
  `data.frame':	10 obs. of  2 variables:
  $ a: Factor w/ 10 levels "a","b","c","d",..: 1 2 3 4 5 6 7 8 9 10
  $ b: int  1 2 3 4 5 6 7 8 9 10
 
Assuming that doesn't solve your problem due to the way your
data frame are created you can do it afterwards.

There are two problems with your code. 

First: (and that causes the error) you use in your repeat 

if(!is.character(df[,i]))
  next

Imagine that the last column of you data frame is not a
character you jump to the next cycle and then you are outside of
the range of your data frame. Your break condition is ignored.

Second: You change your data frame inside of a
function. Variables that are created or changed within a
function are local. Their life ends with the end of the
function. Therefore all changes you do will have no effect on
the global data frame you want to change. See the example:

dat1 <- structure(list(a = 1:10, b = letters[1:10]), .Names = c("a", "b"),
                  row.names = as.character(1:10), class = "data.frame")
str(data.frame(dat1))
  `data.frame':	10 obs. of  2 variables:
  $ a: int  1 2 3 4 5 6 7 8 9 10
  $ b: chr  "a" "b" "c" "d" ...
tofac(dat1)
  [1] 2
str(data.frame(dat1))
  `data.frame':	10 obs. of  2 variables:
  $ a: int  1 2 3 4 5 6 7 8 9 10
  $ b: chr  "a" "b" "c" "d" ...

You can use the following code instead

tofac <- function(x){
  for(i in 1:length(x)) {
    if(is.character(x[,i]))
      x[,i] <- factor(x[,i])
  }
  x
}

dat1 <- tofac(dat1)
  [1] 2
str(dat1)
  `data.frame':	10 obs. of  2 variables:
  $ a: int  1 2 3 4 5 6 7 8 9 10
  $ b: Factor w/ 10 levels "a","b","c","d",..: 1 2 3 4 5 6 7 8 9 10

The for loop avoids the problem with the index. Therefore it
works in example that have a non character variable in the last
column, too and by returning x at the end you are sure that you
object keeps existing.

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

christian schulz writes:
 > Hi,
 > 
 > i would like transfrom 
 > characters from a data.frame to factors automatic.
 > 
 >  > tofac <- function(df){
 > + i=0
 > + repeat{
 > + i <- i+1
 > + if(!is.character(df[,i]))
 > + next
 > + df[,i] <- as.factor(df[,i])
 > + print(i)
 > + if(i == length(df))
 > + break }
 > + }
 >  >
 >  > tofac(abrdat)
 > [1] 7
 > [1] 8
 > [1] 9
 > [1] 11
 > [1] 13
 > [1] 15
 > Error in "[.data.frame"(df, , i) : undefined columns selected
 > 
 > This are the correct columns and i get the idea put into the loop
 > a empty matrix with dimension like df and return it!?
 > 
 > Another check?
 > abrdat2 <- apply(abrdat,2,function(x) 
 > ifelse(is.character(x),as.factor(x),x))
 > 
 > 
 > many thanks & regards,
 > christian
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Wed Jun  1 17:23:30 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 1 Jun 2005 08:23:30 -0700 (PDT)
Subject: [R] Different versions, different results ?
In-Reply-To: <20050601035408.GA22620@kunpuu.plessy.org>
References: <20050601035408.GA22620@kunpuu.plessy.org>
Message-ID: <Pine.A41.4.61b.0506010821500.46036@homer07.u.washington.edu>

On Wed, 1 Jun 2005 charles-r-nospam at plessy.org wrote:

> Dear all,
>
> I wrote the following batch script on a iMac, and ran it on a linux
> mosix cluster.
>
> tu <- read.table("cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshape.table")
> tu_reshaped <- t(reshape(tu[1:50,], direction="wide", timevar="tu", idvar=c("rna","lib")))
> write.table(tu_reshaped, "cage.mm5.tags.rna_lib.CAA-CAJ.tu-reshaped.table")
> q(sav="no")
>
<snip>
> There is something obviously wrong in the second one. The versions are:
>
> GSLC8|Reproducibility|$ R --version
> R 2.1.0 Patched (2005-05-12).
>
> charles at tofu:~$ R --version
> R 1.5.1 (2002-06-17).
>
> I am wondering wether there is a fix or I have to ask the admin of the 
> mosix cluster to upgrade R...

Having multiple `id' variables in reshape() was a feature added in version 
2.0.0, so you have to upgrade.

 	-thomas



From ligges at statistik.uni-dortmund.de  Wed Jun  1 17:25:14 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jun 2005 17:25:14 +0200
Subject: [R] "\n" in legend using substitute and as.expression
In-Reply-To: <BAY16-F5C692F1F28078958CFE69A3050@phx.gbl>
References: <BAY16-F5C692F1F28078958CFE69A3050@phx.gbl>
Message-ID: <429DD35A.7020107@statistik.uni-dortmund.de>

Vumani Dlamini wrote:

> Dear R users,
> Is it possible to force a hard return in an expression. I tried 
> including one for a legend using an expression and it didn't work. Here 
> is my code,
> legend(12, 0.10, c("Fitted density",
>    as.expression(substitute(paste("Weibull (", alpha==shapeU," , ", 
> beta==scaleU,") Mean = ",meanU," days"),
> list(shapeU = round(mle.r$estimate[1],3),
>         scaleU = round(mle.r$estimate[2],3),
>         meanU = round(mle.r$estimate[3),4)))) ),
>    lty=c(1,1),col=c(1,2),lwd=c(1,1),merge = TRUE)



Please try to specify *reproducible* examples. For the above example, we 
do not have the plot and the data. Additionally, the stuff above is not 
even syntactically correct as e.g. in:
     meanU = round(mle.r$estimate[3),4)))) ),
                                 ^ ^
It has been described in former messages on this list (hence in the 
mailing list archives) that newlines are not really supported in the 
"mathematical annotation environment".

Uwe Ligges





> (Windows XP + R2.10 user)
> 
> Thanks,
> Vumani
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Wed Jun  1 17:24:55 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 1 Jun 2005 12:24:55 -0300 (ADT)
Subject: [R] Fitting ARMA model with known inputs.
Message-ID: <200506011524.j51FOtiC004856@erdos.math.unb.ca>

It is not at all clear what you want to do.  One conjecture
(attempt at reading your mind):

	X_t = ``black box's state'' at time t
	f_t = ``force'' at time t

	Proposed model e.g. AR(3):

	X_t = phi_1 * X_{t-1} + phi_2 * X_{t-2}
                              + phi_3 * X_{t-3} + f_t

	You wish to identify/estimate the coefficients phi_1, phi_2,
	phi_3.

Remarks:

	(a) This model probably doesn't make a lot of sense, with
	known/observed f_t.  It will almost surely not hold exactly,
	for ***any*** values of the phi_i.

	(b) A model which makes a bit more sense, in the abstract, is

	X_t = phi_1 * X_{t-1} + phi_2 * X_{t-2}
                              + phi_3 * X_{t-3} + f_t + E_t

	where E_t is (unobserved) i.i.d. random ``error''.

	(c) This last model is just a simple regression model and
	may be fitted using lm().

				cheers,

					Rolf Turner
					rolf at math.unb.ca

Original message:

> Hello!
> Is it possible to use R time series to identificate a process which is 
> subjected to known input? I.e. I have 2 sequences - one is measurements 
> of black box's state and the second is the "force" by which this black 
> box is driven (which is known too) and I want to fit thist two series 
> with AR-process. The "ar" procedure from stats package expects that the 
> force is always random. Is it possible to feed it known vector as input 
> parameter?
> Thank you in advance.



From Achim.Zeileis at wu-wien.ac.at  Wed Jun  1 17:04:45 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 1 Jun 2005 17:04:45 +0200
Subject: [R] Which variable exist after random
In-Reply-To: <92e186a0050601080518988055@mail.gmail.com>
References: <92e186a0050601080518988055@mail.gmail.com>
Message-ID: <20050601170445.11dade4e.Achim.Zeileis@wu-wien.ac.at>

On Wed, 1 Jun 2005 17:05:52 +0200 Sofyan Iyan wrote:

> Dear R-helper,
> How could I count only some variable was exist after running sample
> (random) function.

I'm not completely sure what you want, but maybe you want to look at
table() and try print(table(x)) instead of print(x). Those levels with 0
occurences don't occur in the random sample.
Z

> For example,
> 
> > testx <- factor(c("Game","Paper","Internet","Time","Money"))
> >    for(i in 1:2) {
> +        x <- sample(testx,replace=TRUE)
> +        print(x)
> +    }
> [1] Money    Money    Time     Internet Time    
> Levels: Game Internet Money Paper Time
> [1] Time  Money Game  Money Money
> Levels: Game Internet Money Paper Time
> > 
> 
> The result above Game, Internet, Money, and Time only exist and
> "Paper" was missing.
> Best, Sofyan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Jun  1 17:46:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 1 Jun 2005 11:46:24 -0400
Subject: [R] Which variable exist after random
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E8F7@usctmx1106.merck.com>

Or perhaps

> f <- factor(letters[1:2], levels=letters[1:3])
> f
[1] a b
Levels: a b c
> levels(f[, drop=TRUE])
[1] "a" "b"

Andy

> From: TEMPL Matthias
> 
> > Dear R-helper,
> > How could I count only some variable was exist after running sample
> > (random) function.
> 
> Or is this what you want?
> 
> Tab <- table(sample(testx, replace=TRUE))
> Tab[ Tab > 0 ]
> 
> > For example,
> > 
> > > testx <- factor(c("Game","Paper","Internet","Time","Money"))
> > >    for(i in 1:2) {
> > +        x <- sample(testx,replace=TRUE)
> > +        print(x)
> > +    }
> > [1] Money    Money    Time     Internet Time    
> > Levels: Game Internet Money Paper Time
> > [1] Time  Money Game  Money Money
> > Levels: Game Internet Money Paper Time
> > > 
> > 
> > The result above Game, Internet, Money, and Time only exist 
> > and "Paper" was missing. Best, Sofyan
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read 
> > the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jrstear at sandia.gov  Wed Jun  1 17:49:14 2005
From: jrstear at sandia.gov (Jon Stearley)
Date: Wed, 1 Jun 2005 09:49:14 -0600
Subject: [R] sink() within a function?
Message-ID: <6059c2676c3747db6936c95ed6405ea2@sandia.gov>

sink() isn't behaving as i expect, when used inside a function, eg:
   x<-data.frame(F=c("O","O"))
   f<-"foo.txt"
   sink(f); format(x); sink();   # foo.txt looks great!
   foo<-function(x,f) { sink(f); format(x); sink(); }
   foo(x,f=f) # foo.txt is empty!
why is this, and how can i successfully sink() within a function?

my real function does some rearrangement and formatting, but the above 
illustrates the problem i'm encountering.  thx in advance for any help! 
    [MacOS 10.3, R 2.0.1]

-- 
+--------------------------------------------------------------+
| Jon Stearley                  (505) 845-7571  (FAX 844-9297) |
| Sandia National Laboratories  Scalable Systems Integration   |
+--------------------------------------------------------------+

one more clue:
   library(debug); mtrace(foo); foo(x,f=f); ...  # foo.txt is not empty!



From helprhelp at gmail.com  Wed Jun  1 17:50:53 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 1 Jun 2005 10:50:53 -0500
Subject: [R] help: reference & book
In-Reply-To: <cdf8178305052715024413af54@mail.gmail.com>
References: <cdf8178305052715024413af54@mail.gmail.com>
Message-ID: <cdf8178305060108505c815a59@mail.gmail.com>

Hi, listers:
I am really in need for some good books on financial market analysis,
better with R. Can anyone help?
Thanks.

weiwei



From amsa36060 at yahoo.com  Wed Jun  1 17:53:00 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Wed, 1 Jun 2005 08:53:00 -0700 (PDT)
Subject: [R] How  to name variables in a single plot
Message-ID: <20050601155300.85800.qmail@web60421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050601/003eb75f/attachment.pl

From mmccall at mail.nih.gov  Wed Jun  1 17:59:29 2005
From: mmccall at mail.nih.gov (Matt McCall)
Date: Wed, 1 Jun 2005 11:59:29 -0400
Subject: [R] pdf error msg
In-Reply-To: <Pine.A41.4.61b.0505310900420.190182@homer05.u.washington.edu>
References: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>
	<Pine.A41.4.61b.0505310900420.190182@homer05.u.washington.edu>
Message-ID: <74ec607856f3f001f1934281d3be64ab@mail.nih.gov>

I'm sorry, but is there anything I can do to remedy this? I reinstalled 
R and BioConductor, and the error still occurs.
Thanks you any advice.
Matt


On May 31, 2005, at 12:03 PM, Thomas Lumley wrote:

> On Tue, 31 May 2005, Matt McCall wrote:
>
>> *** malloc[477]: Deallocation of a pointer not malloced: 0x143cf5c0; 
>> This could be a double free(), or free() called with the middle of an 
>> allocated block; Try setting environment variable MallocHelp to see 
>> tools to help debug
>>
>
> And this is a double free(). It looks as though the function for 
> creating a PDF device used not to free the passed-in structure in case 
> of error, but it now does.
>
> Thanks for reporting this.
>
> 	-thomas
>



From sofyan.iyan at gmail.com  Wed Jun  1 18:06:27 2005
From: sofyan.iyan at gmail.com (Sofyan Iyan)
Date: Wed, 1 Jun 2005 18:06:27 +0200
Subject: [R] Which variable exist after random
In-Reply-To: <83536658864BC243BE3C06D7E936ABD5027BAAB6@xchg1.statistik.local>
References: <83536658864BC243BE3C06D7E936ABD5027BAAB6@xchg1.statistik.local>
Message-ID: <92e186a0050601090652dcc0e1@mail.gmail.com>

Thanks for your help.
How about if I count all. This below I have fixed my code which used
table. I need the result:

    Game Internet    Money    Paper     Time 
       4        1              1            4        0 

Regards, Sofyan

> testx <- factor(c("Game","Paper","Internet","Time","Money"))
>     for(i in 1:2) {
+         x <- sample(testx,replace=TRUE)
+         print(x)
+         print(table(x))
+     }
[1] Game  Paper Game  Paper Paper
Levels: Game Internet Money Paper Time
x
    Game Internet    Money    Paper     Time 
       2        0        0        3        0 
[1] Game     Game     Internet Paper    Money   
Levels: Game Internet Money Paper Time
x
    Game Internet    Money    Paper     Time 
       2        1        1        1        0 
> 


On 6/1/05, TEMPL Matthias <Matthias.Templ at statistik.gv.at> wrote:
> > Dear R-helper,
> > How could I count only some variable was exist after running sample
> > (random) function.
> 
> Or is this what you want?
> 
> Tab <- table(sample(testx, replace=TRUE))
> Tab[ Tab > 0 ]
>



From buser at stat.math.ethz.ch  Wed Jun  1 18:07:13 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Wed, 1 Jun 2005 18:07:13 +0200
Subject: [R] How  to name variables in a single plot
In-Reply-To: <20050601155300.85800.qmail@web60421.mail.yahoo.com>
References: <20050601155300.85800.qmail@web60421.mail.yahoo.com>
Message-ID: <17053.56625.518031.921145@stat.math.ethz.ch>

Dear Amari

Why not use legend?

y<- c(1:100)
x1<-seq(0.1,10, by=0.1) 
x2<-seq(0.5,50,by=0.5)
mydata<- data.frame(  y=y,  x1=x1, x2=x2)
matplot(mydata, type = "l" ,xlab="Time",ylab="MSE ", col = 1:3, lty = 1:3)
legend(10,90,c("line1", "line2", "line3"), col = 1:3, lty = 1:3)

You can work with text to place any text within the plot
See ?legend    ?text

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------

Amir Safari writes:
 > 
 >  
 > 
 > Dear R Friends ,
 > 
 > I want to name my variables( more than 2 variables in a single plot) within a plot to distinct them from each other, but I cann't. How it is possible? I don't mean x and y axis using xlab or ylab. At the below , it follows some lines, only as an example that you could try please, if it is possible. I really thanks for your attention.
 > 
 > Amir
 > 
 >  
 > 
 >  
 > 
 > library(graphics)
 > 
 > y<- c(1:100)
 > 
 > x1<-seq(0.1,10, by=0.1) 
 > 
 > x2<-seq(0.5,50,by=0.5)
 > 
 > mydata<- data.frame(  y=y,  x1=x1, x2=x2)
 > 
 > matplot(mydata, type = "l" )
 > 
 > matplot(mydata, type = "l" ,xlab="Time")
 > 
 > matplot(mydata, type = "l" ,xlab="Time",ylab="MSE ")
 > 
 > 
 > 
 > __________________________________________________
 > 
 > 
 > 
 > 	[[alternative HTML version deleted]]
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Wed Jun  1 18:19:36 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 1 Jun 2005 12:19:36 -0400
Subject: [R] How to name variables in a single plot
In-Reply-To: <20050601155300.85800.qmail@web60421.mail.yahoo.com>
References: <20050601155300.85800.qmail@web60421.mail.yahoo.com>
Message-ID: <971536df05060109196e500b97@mail.gmail.com>

On 6/1/05, Amir Safari <amsa36060 at yahoo.com> wrote:
> 
> 
> 
> Dear R Friends ,
> 
> I want to name my variables( more than 2 variables in a single plot) within a plot to distinct them from each other, but I cann't. How it is possible? I don't mean x and y axis using xlab or ylab. At the below , it follows some lines, only as an example that you could try please, if it is possible. I really thanks for your attention.
> 
> Amir
> 
> 
> 
> 
> 
> library(graphics)
> 
> y<- c(1:100)
> 
> x1<-seq(0.1,10, by=0.1)
> 
> x2<-seq(0.5,50,by=0.5)
> 
> mydata<- data.frame(  y=y,  x1=x1, x2=x2)
> 
> matplot(mydata, type = "l" )
> 
> matplot(mydata, type = "l" ,xlab="Time")
> 
> matplot(mydata, type = "l" ,xlab="Time",ylab="MSE ")
> 
> 

example(matplot) gives some alternative ways of doing this.
Another way, not shown there, is via text:

text(rep(100,3), mydata[100,], colnames(mydata), pos = 2)



From spencer.graves at pdf.com  Wed Jun  1 18:26:18 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 01 Jun 2005 09:26:18 -0700
Subject: [R] glm
In-Reply-To: <12601.1117617982@www74.gmx.net>
References: <12601.1117617982@www74.gmx.net>
Message-ID: <429DE1AA.2010301@pdf.com>

	  I would not use gamma or gaussian for a proportion.  If the 
percentages were success or failure rate out of a known number of 
trials, then I'd try function "glm" with family = binomial or 
quasibinomial.  However, "percent of plant root length colonized" does 
not sound to me like "glm" would be appropriate.  The beta distribution 
seems to me to be the most plausibly distribution to use.

	  I therefore did a search for "beta regression using 
"www.r-project.org" -> search -> "R site search".  This revealed a 
package "betareg" available from CRAN.  Have you considered that?  If 
no, I think I'd try it.

	  spencer graves

Mahdi Osman wrote:
> Hi, list,
> 
> I have got a dataset on soil and plant. I would like to fit a "glm" to my
> data. My response variable is percentage data. That is percent of plant root
> length colonized by Arbuscular micorrhiza fungi. Because of the nature of my
> data, I am not quite sure whether gamma or gaussian distribution is suitable
> for this type of data. If I use gamma distribution, which link function is
> appropriate?
> 
> I appreciate your suggestions very much.
> 
> Thanks in advance!
> 
> 
> Mahdi
> 
>



From mkondrin at hppi.troitsk.ru  Wed Jun  1 18:31:35 2005
From: mkondrin at hppi.troitsk.ru (mkondrin)
Date: Wed, 01 Jun 2005 20:31:35 +0400
Subject: [R] Fitting ARMA model with known inputs.
In-Reply-To: <200506011524.j51FOtiC004856@erdos.math.unb.ca>
References: <200506011524.j51FOtiC004856@erdos.math.unb.ca>
Message-ID: <429DE2E7.1080701@hppi.troitsk.ru>

Thank you very much for clearing my question (for me too ;))
The model I would like to fit is :

X_t = phi_1 * X_{t-1} + phi_2 * X_{t-2}
                              + phi_3 * X_{t-3} + 
A_1*f_{t-1}+A_2*f_{t-2}...+A_k*f_{t-k} + E_t (*)

(X_t and f_t time series are both known, k - fixed and more than 1).
lm is a good answer (I surely try it), but I thought may be somethere in 
R exists a front-end to lm for this particular case. For example if I 
have a model
X_t = phi_1 * X_{t-1} + phi_2 * X_{t-2}
                              + phi_3 * X_{t-3} + E_t (**), I would use 
an "ar" command from "stats" package. My problem is how to make my model 
(*) suit "ar" command (may model (*) be rewritten in (**) form)?


Rolf Turner wrote:

>It is not at all clear what you want to do.  One conjecture
>(attempt at reading your mind):
>
>	X_t = ``black box's state'' at time t
>	f_t = ``force'' at time t
>
>	Proposed model e.g. AR(3):
>
>	X_t = phi_1 * X_{t-1} + phi_2 * X_{t-2}
>                              + phi_3 * X_{t-3} + f_t
>
>	You wish to identify/estimate the coefficients phi_1, phi_2,
>	phi_3.
>
>Remarks:
>
>	(a) This model probably doesn't make a lot of sense, with
>	known/observed f_t.  It will almost surely not hold exactly,
>	for ***any*** values of the phi_i.
>
>	(b) A model which makes a bit more sense, in the abstract, is
>
>	X_t = phi_1 * X_{t-1} + phi_2 * X_{t-2}
>                              + phi_3 * X_{t-3} + f_t + E_t
>
>	where E_t is (unobserved) i.i.d. random ``error''.
>
>	(c) This last model is just a simple regression model and
>	may be fitted using lm().
>
>				cheers,
>
>					Rolf Turner
>					rolf at math.unb.ca
>
>Original message:
>
>  
>
>>Hello!
>>Is it possible to use R time series to identificate a process which is 
>>subjected to known input? I.e. I have 2 sequences - one is measurements 
>>of black box's state and the second is the "force" by which this black 
>>box is driven (which is known too) and I want to fit thist two series 
>>with AR-process. The "ar" procedure from stats package expects that the 
>>force is always random. Is it possible to feed it known vector as input 
>>parameter?
>>Thank you in advance.
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From helprhelp at gmail.com  Wed Jun  1 18:46:08 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 1 Jun 2005 11:46:08 -0500
Subject: [R] Re: help: reference & book
In-Reply-To: <cdf8178305060108505c815a59@mail.gmail.com>
References: <cdf8178305052715024413af54@mail.gmail.com>
	<cdf8178305060108505c815a59@mail.gmail.com>
Message-ID: <cdf8178305060109465297a10a@mail.gmail.com>

I am interested in stock market. My model is built on s/p500.

Sorry for the uncleaness.

weiwei

On 6/1/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Hi, listers:
> I am really in need for some good books on financial market analysis,
> better with R. Can anyone help?
> Thanks.
> 
> weiwei
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From thoralf.mildenberger at uni-dortmund.de  Wed Jun  1 18:46:20 2005
From: thoralf.mildenberger at uni-dortmund.de (Thoralf Mildenberger)
Date: Wed, 1 Jun 2005 18:46:20 +0200 (CEST)
Subject: [R] using user-supplied derivatives in rgenoud
Message-ID: <33016.129.217.181.180.1117644380.squirrel@unimail.uni-dortmund.de>

I have been using the rgenoud package for a nonlinear least-squares
problem with lots of local minima, and it works very well but takes lots
of time. According to the article refrenced in the documentation, the
original GENOUD-software by the same authors seems to allow for
user-supplied analytical derivatives instead of numerical approximations,
which would probably save some time. Does anybody know whether this
feature is also available in rgenoud? The documentation says nothing about
this, but rgenoud seems to call optim() in the stats-package, which does
allow for passing a function evaluating the derivatives.

Thank you,
Thoralf Mildenberger



From jrstear at sandia.gov  Wed Jun  1 19:00:25 2005
From: jrstear at sandia.gov (Jon Stearley)
Date: Wed, 1 Jun 2005 11:00:25 -0600
Subject: [R] sink() within a function?
In-Reply-To: <200506011623.j51GNPv9007461@erdos.math.unb.ca>
References: <200506011623.j51GNPv9007461@erdos.math.unb.ca>
Message-ID: <a2fb2c317d530f08762e83f792dbb1bf@sandia.gov>


On Jun 1, 2005, at 10:23 AM, Rolf Turner wrote:
>
> foo<-function(x,f) { sink(f); print(format(x)); sink(); }

many thanks!  doh ;)

?format is unclear on return value imho.

-- 
-jon



From roger.bos at gmail.com  Wed Jun  1 19:06:02 2005
From: roger.bos at gmail.com (roger bos)
Date: Wed, 1 Jun 2005 13:06:02 -0400
Subject: [R] Re: help: reference & book
In-Reply-To: <cdf8178305060109465297a10a@mail.gmail.com>
References: <cdf8178305052715024413af54@mail.gmail.com>
	<cdf8178305060108505c815a59@mail.gmail.com>
	<cdf8178305060109465297a10a@mail.gmail.com>
Message-ID: <1db7268005060110061e7c1e9@mail.gmail.com>

Your question is kind of broad, but two books I like are "Statistical
Analysis of Financial Data in S-Plus" by Rene A. Carmona (Springer)
and "Modeling Financial Time Series With S-Plus" by Eric Zivot and
Jiahue Wang.  The last one is an Insightful publication and is
basically the user's guide for their S+Finmetrics product, but even
w/o Finmetrics its a worthwhile read.

HTH,

Roger


On 6/1/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> I am interested in stock market. My model is built on s/p500.
> 
> Sorry for the uncleaness.
> 
> weiwei
> 
> On 6/1/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > Hi, listers:
> > I am really in need for some good books on financial market analysis,
> > better with R. Can anyone help?
> > Thanks.
> >
> > weiwei
> >
> 
> 
> --
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Scott.Waichler at pnl.gov  Wed Jun  1 19:12:35 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Wed, 01 Jun 2005 10:12:35 -0700
Subject: [R] problem with chron scales in lattice
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01FD7103@pnlmse35.pnl.gov>


I can't get the scales parameter in xyplot of lattice to work as I
expected.  I'm using R-2.1.0 and lattice 0.11-8.  There should be year
labels from 1992 to 2004 for the x axis in the plot below, but instead
only a few of them appear, and in the wrong spots, as if the coordinate
system has changed after finishing with the panel function.

library(chron)
library(lattice)

# vertical grid lines at start of each year
startdate <- "1/1/1992"
enddate   <- "1/1/2005"
startdatetime <- chron(dates=startdate, times="00:00:00")
enddatetime <- chron(dates=enddate, times="00:00:00")
x.lines <- as.chron(seq.dates(startdate, enddate, by="years"))
x.limits <- c(as.chron(startdate), as.chron(enddate))

# year labels on x axis (centered between grid lines)
x.at <- as.chron(seq.dates("7/1/1992", "7/1/2004", by="years"))
x.labels <- format(as.Date(dates(x.at)), "%Y")

# dummy data
x <- x.at[1:2]
y <- c(105, 105)
id <- c(1,2)

print(xyplot(y ~ x | id,
   panel = function(x, y, subscripts, ...) {
     panel.abline(v = x.lines, col = "lightgray", lty = 1, lwd= 1.0)
     # function to plot data would go here
   },  # end of panel
   as.table=T, layout=c(1,2), main="",
   xlim=x.limits, 
   scales = list(x = list(alternating = T, at = x.at, labels = x.labels,
tck=0)),
   strip = F
 ) # end xyplot()
) # end print()

Thanks for any help,

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From murdoch at stats.uwo.ca  Wed Jun  1 19:22:42 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 01 Jun 2005 13:22:42 -0400
Subject: [R] sink() within a function?
In-Reply-To: <a2fb2c317d530f08762e83f792dbb1bf@sandia.gov>
References: <200506011623.j51GNPv9007461@erdos.math.unb.ca>
	<a2fb2c317d530f08762e83f792dbb1bf@sandia.gov>
Message-ID: <429DEEE2.5080400@stats.uwo.ca>

Jon Stearley wrote:
> On Jun 1, 2005, at 10:23 AM, Rolf Turner wrote:
> 
>>foo<-function(x,f) { sink(f); print(format(x)); sink(); }
> 
> 
> many thanks!  doh ;)
> 
> ?format is unclear on return value imho.

?format does describe the return value, though in the Details section 
rather than in Value:

      These functions convert their first argument to a vector (or
      array) of character strings which have a common format (as is done
      by 'print'), fulfilling 'length(format*(x, *)) == length(x)'.  The
      trimming with 'trim = TRUE' is useful when the strings are to be
      used for plot 'axis' annotation.

I'll try to make it clearer.

Duncan Murdoch



From tlumley at u.washington.edu  Wed Jun  1 19:28:35 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 1 Jun 2005 10:28:35 -0700 (PDT)
Subject: [R] pdf error msg
In-Reply-To: <74ec607856f3f001f1934281d3be64ab@mail.nih.gov>
References: <7095ebaa8d4659b5a06405351cb22339@mail.nih.gov>
	<Pine.A41.4.61b.0505310900420.190182@homer05.u.washington.edu>
	<74ec607856f3f001f1934281d3be64ab@mail.nih.gov>
Message-ID: <Pine.A41.4.61b.0506011026440.46036@homer07.u.washington.edu>

On Wed, 1 Jun 2005, Matt McCall wrote:

> I'm sorry, but is there anything I can do to remedy this? I reinstalled R and 
> BioConductor, and the error still occurs.

The error appears to be caused by trying to write to a file that you don't 
have write permission for.  The work-around is not to do that.  Now, if 
you do have write permission for that file and R is just confused we have 
a different problem, and one that needs more details.

Does the problem happen with other filenames or other directories, for 
example?

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From mi2kelgrum at yahoo.com  Wed Jun  1 20:08:46 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Wed, 1 Jun 2005 11:08:46 -0700 (PDT)
Subject: [R] x[x$a=="q",,drop=TRUE]
Message-ID: <20050601180846.17078.qmail@web60211.mail.yahoo.com>

I'm trying to select a subset of a dataframe while
dropping some factors. While the dataset gets smaller
all Factor levels remain and I need to get rid of
them.  Strangely enough, I am almost certain that the
same code on the same data worked OK earlier today -
and it is not the first time that I'm not able to
replicate earlier results with this command (I know, I
might just be going crazy). What am I doing wrong?

I'm working on Windows Server 2003, R 2.1.0
(2005-04-18).

> str(spray)
`data.frame':   370 obs. of  7 variables:
 $ PD        : Factor w/ 8 levels
"Botrytis","Downy",..: 2 2 2 2 4 2 2 5 5 5 ...
 $ postSpmtsQ: num  1309 1309  384  384 1044 ...
 $ ante62Q   : num  284 284 218 218 366 ...
 $ ante08Q   : num  331 331 228 228 492 ...
 $ ante29Q   : num   297  297 1067 1067 1034 ...
 $ ante16Q   : num  0 0 0.2 0.2 0 0 0 6.7 0 31.5 ...
 $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
27 5 27 5 36 27 5 24 24 24 ...
> sprayS <- spray[spray$PD == "Spidermites", , drop =
TRUE]
> str(sprayS)
`data.frame':   111 obs. of  7 variables:
 $ PD        : Factor w/ 8 levels
"Botrytis","Downy",..: 5 5 5 5 5 5 5 5 5 5 ...
 $ postSpmtsQ: num  13395 31588 84254   136   619 ...
 $ ante62Q   : num   1357 21187 21819   218   237 ...
 $ ante08Q   : num    973 21740 25112   228   134 ...
 $ ante29Q   : num    2103 106970  66676   1067    119
...
 $ ante16Q   : num  6.7 0 31.5 0.2 0 0 0 0 14.3 0 ...
 $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
24 24 24 24 24 24 24 24 24 24 ...
> table(sprayS$Trt)

    Acrobat MZ WP           Agrifos      Apollo 50 SC 
          CALMAG 
                0                 0                13 
               0 
            DM-31    Dynamec 1.8 EC   Equation Pro DF 
       Evisect S 
                0                13                 0 
               0 
            Flint         Floramite           Impulse 
          Karate 
                0                15                 0 
               0 
      Karate zeon            Melody    Meltatox 40 EC 
             MKP 
                0                 0                 0 
               0 
         Molasses       Nembicidine     Nimrod 250 EC 
  Nissorun 10 EC 
                0                 0                 0 
              12 
           Oberon     Orthene 75 WP       Oscar 20 SC 
         Pegasus 
               15                 0                 9 
              26 
     Polar 50 WSG            Potfos          Proplant 
           Pyrus 
                0                 0                 0 
               0 
Ridomil MZ 63 5WP   Rovral aqua flo      Score 250 EC 
    Secure 36 SC 
                0                 0                 0 
               8 
      Sequestrone          Shavit f         Sporekill 
    Stroby 50 WG 
                0                 0                 0 
               0 
           Switch            Tracer          Trafos K 
        Vandozeb 
                0                 0                 0 
               0 
          Vitomex 
                0 

cheers,
Mikkel



From mi2kelgrum at yahoo.com  Wed Jun  1 20:15:36 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Wed, 1 Jun 2005 11:15:36 -0700 (PDT)
Subject: [R] Zelig: starting values for negbin
Message-ID: <20050601181536.57461.qmail@web60212.mail.yahoo.com>

I'm running a negative binomial model in zelig and
it's asking me to supply starting values for the
coefficients. How do I supply these? Could I use the
coefficients from the poisson model (which runs
smoothly)? 

> z.out <- zelig(postSpmtsQ ~ (ante62Q + ante08Q +
ante29Q + ante16Q) * Trt, model = "negbin", data =
sprayS)
Error: no valid set of coefficients has been found:
please supply starting values

I don't find anything on this in the documentation and
would appreciate any hints.

Mikkel


		
__________________________________ 

Find restaurants, movies, travel and more fun for the weekend. Check it out!



From thchung at tgen.org  Wed Jun  1 20:20:02 2005
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Wed, 01 Jun 2005 11:20:02 -0700
Subject: [R] x[x$a=="q",,drop=TRUE]
In-Reply-To: <20050601180846.17078.qmail@web60211.mail.yahoo.com>
Message-ID: <BEC34A62.5689%thchung@tgen.org>

Hi, Mikkel;

The problem here, I think, is that spray$PD is NOT recognized as character
in R. Actually, R understands it as factors according to your printout. This
happens quite a lot if you use read.table and come across characters within
the data file. One remedy I use often use is doing something like

spray$PD <- as.character(spray$PD)

to make it sure that spray$PD is character.

Tae-Hoon Chung
--------------------------------------------------
Tae-Hoon Chung
Post-Doctoral Researcher
Translational Genomics Research Institute (TGen)
445 N. 5th Street (Suite 530)
Phoenix, AZ 85004
1-602-343-8724 (Direct)
1-480-323-9820 (Mobile)
1-602-343-8840 (Fax)
--------------------------------------------------



On 6/1/05 11:08 AM, "Mikkel Grum" <mi2kelgrum at yahoo.com> wrote:

> I'm trying to select a subset of a dataframe while
> dropping some factors. While the dataset gets smaller
> all Factor levels remain and I need to get rid of
> them.  Strangely enough, I am almost certain that the
> same code on the same data worked OK earlier today -
> and it is not the first time that I'm not able to
> replicate earlier results with this command (I know, I
> might just be going crazy). What am I doing wrong?
> 
> I'm working on Windows Server 2003, R 2.1.0
> (2005-04-18).
> 
>> str(spray)
> `data.frame':   370 obs. of  7 variables:
>  $ PD        : Factor w/ 8 levels
> "Botrytis","Downy",..: 2 2 2 2 4 2 2 5 5 5 ...
>  $ postSpmtsQ: num  1309 1309  384  384 1044 ...
>  $ ante62Q   : num  284 284 218 218 366 ...
>  $ ante08Q   : num  331 331 228 228 492 ...
>  $ ante29Q   : num   297  297 1067 1067 1034 ...
>  $ ante16Q   : num  0 0 0.2 0.2 0 0 0 6.7 0 31.5 ...
>  $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
> 27 5 27 5 36 27 5 24 24 24 ...
>> sprayS <- spray[spray$PD == "Spidermites", , drop =
> TRUE]
>> str(sprayS)
> `data.frame':   111 obs. of  7 variables:
>  $ PD        : Factor w/ 8 levels
> "Botrytis","Downy",..: 5 5 5 5 5 5 5 5 5 5 ...
>  $ postSpmtsQ: num  13395 31588 84254   136   619 ...
>  $ ante62Q   : num   1357 21187 21819   218   237 ...
>  $ ante08Q   : num    973 21740 25112   228   134 ...
>  $ ante29Q   : num    2103 106970  66676   1067    119
> ...
>  $ ante16Q   : num  6.7 0 31.5 0.2 0 0 0 0 14.3 0 ...
>  $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
> 24 24 24 24 24 24 24 24 24 24 ...
>> table(sprayS$Trt)
> 
>     Acrobat MZ WP           Agrifos      Apollo 50 SC
>           CALMAG 
>                 0                 0                13
>                0 
>             DM-31    Dynamec 1.8 EC   Equation Pro DF
>        Evisect S 
>                 0                13                 0
>                0 
>             Flint         Floramite           Impulse
>           Karate 
>                 0                15                 0
>                0 
>       Karate zeon            Melody    Meltatox 40 EC
>              MKP 
>                 0                 0                 0
>                0 
>          Molasses       Nembicidine     Nimrod 250 EC
>   Nissorun 10 EC 
>                 0                 0                 0
>               12 
>            Oberon     Orthene 75 WP       Oscar 20 SC
>          Pegasus 
>                15                 0                 9
>               26 
>      Polar 50 WSG            Potfos          Proplant
>            Pyrus 
>                 0                 0                 0
>                0 
> Ridomil MZ 63 5WP   Rovral aqua flo      Score 250 EC
>     Secure 36 SC 
>                 0                 0                 0
>                8 
>       Sequestrone          Shavit f         Sporekill
>     Stroby 50 WG 
>                 0                 0                 0
>                0 
>            Switch            Tracer          Trafos K
>         Vandozeb 
>                 0                 0                 0
>                0 
>           Vitomex
>                 0
> 
> cheers,
> Mikkel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spluque at gmail.com  Wed Jun  1 20:04:08 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Wed, 01 Jun 2005 13:04:08 -0500
Subject: [R] tick marks in opposite axis xyplot
Message-ID: <87sm02vtrb.fsf@gmail.com>

Hello,

I cannot find how to avoid drawing tick marks on the opposite side of axes
in xyplot (lattice package). Say you have a simple plot:

x <- y <- rnorm(10)
xyplot(y ~ x)

There are tick marks on the top and right axes. Is it possible to turn off
drawing of those tick marks, or even turn off drawing of those axes
completely? Thanks in advance.


Cheers,
Sebastian
-- 
Sebastian P. Luque



From gerifalte28 at hotmail.com  Wed Jun  1 20:40:36 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 01 Jun 2005 18:40:36 +0000
Subject: [R] x[x$a=="q",,drop=TRUE]
In-Reply-To: <20050601180846.17078.qmail@web60211.mail.yahoo.com>
Message-ID: <BAY103-F7EB199C47C4A7EDFB26B7A6050@phx.gbl>

the argument drop =TRUE is not meant to do that (see several responses from 
Peter Dalgaard about this issue i.e. 
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/37333.html)

If you want to drop unused factor levels try:

sprayS$PD<-factor(sprayS$PD)


Cheers

Francisco

>From: Mikkel Grum <mi2kelgrum at yahoo.com>
>To: RHelp <r-help at stat.math.ethz.ch>
>Subject: [R] x[x$a=="q",,drop=TRUE]
>Date: Wed, 1 Jun 2005 11:08:46 -0700 (PDT)
>
>I'm trying to select a subset of a dataframe while
>dropping some factors. While the dataset gets smaller
>all Factor levels remain and I need to get rid of
>them.  Strangely enough, I am almost certain that the
>same code on the same data worked OK earlier today -
>and it is not the first time that I'm not able to
>replicate earlier results with this command (I know, I
>might just be going crazy). What am I doing wrong?
>
>I'm working on Windows Server 2003, R 2.1.0
>(2005-04-18).
>
> > str(spray)
>`data.frame':   370 obs. of  7 variables:
>  $ PD        : Factor w/ 8 levels
>"Botrytis","Downy",..: 2 2 2 2 4 2 2 5 5 5 ...
>  $ postSpmtsQ: num  1309 1309  384  384 1044 ...
>  $ ante62Q   : num  284 284 218 218 366 ...
>  $ ante08Q   : num  331 331 228 228 492 ...
>  $ ante29Q   : num   297  297 1067 1067 1034 ...
>  $ ante16Q   : num  0 0 0.2 0.2 0 0 0 6.7 0 31.5 ...
>  $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
>27 5 27 5 36 27 5 24 24 24 ...
> > sprayS <- spray[spray$PD == "Spidermites", , drop =
>TRUE]
> > str(sprayS)
>`data.frame':   111 obs. of  7 variables:
>  $ PD        : Factor w/ 8 levels
>"Botrytis","Downy",..: 5 5 5 5 5 5 5 5 5 5 ...
>  $ postSpmtsQ: num  13395 31588 84254   136   619 ...
>  $ ante62Q   : num   1357 21187 21819   218   237 ...
>  $ ante08Q   : num    973 21740 25112   228   134 ...
>  $ ante29Q   : num    2103 106970  66676   1067    119
>...
>  $ ante16Q   : num  6.7 0 31.5 0.2 0 0 0 0 14.3 0 ...
>  $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
>24 24 24 24 24 24 24 24 24 24 ...
> > table(sprayS$Trt)
>
>     Acrobat MZ WP           Agrifos      Apollo 50 SC
>           CALMAG
>                 0                 0                13
>                0
>             DM-31    Dynamec 1.8 EC   Equation Pro DF
>        Evisect S
>                 0                13                 0
>                0
>             Flint         Floramite           Impulse
>           Karate
>                 0                15                 0
>                0
>       Karate zeon            Melody    Meltatox 40 EC
>              MKP
>                 0                 0                 0
>                0
>          Molasses       Nembicidine     Nimrod 250 EC
>   Nissorun 10 EC
>                 0                 0                 0
>               12
>            Oberon     Orthene 75 WP       Oscar 20 SC
>          Pegasus
>                15                 0                 9
>               26
>      Polar 50 WSG            Potfos          Proplant
>            Pyrus
>                 0                 0                 0
>                0
>Ridomil MZ 63 5WP   Rovral aqua flo      Score 250 EC
>     Secure 36 SC
>                 0                 0                 0
>                8
>       Sequestrone          Shavit f         Sporekill
>     Stroby 50 WG
>                 0                 0                 0
>                0
>            Switch            Tracer          Trafos K
>         Vandozeb
>                 0                 0                 0
>                0
>           Vitomex
>                 0
>
>cheers,
>Mikkel
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Wed Jun  1 20:40:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 1 Jun 2005 14:40:55 -0400
Subject: [R] x[x$a=="q",,drop=TRUE]
In-Reply-To: <20050601180846.17078.qmail@web60211.mail.yahoo.com>
References: <20050601180846.17078.qmail@web60211.mail.yahoo.com>
Message-ID: <971536df0506011140ea2400@mail.gmail.com>

On 6/1/05, Mikkel Grum <mi2kelgrum at yahoo.com> wrote:
> I'm trying to select a subset of a dataframe while
> dropping some factors. While the dataset gets smaller
> all Factor levels remain and I need to get rid of
> them.  Strangely enough, I am almost certain that the
> same code on the same data worked OK earlier today -
> and it is not the first time that I'm not able to
> replicate earlier results with this command (I know, I
> might just be going crazy). What am I doing wrong?
> 
> I'm working on Windows Server 2003, R 2.1.0
> (2005-04-18).
> 
> > str(spray)
> `data.frame':   370 obs. of  7 variables:
>  $ PD        : Factor w/ 8 levels
> "Botrytis","Downy",..: 2 2 2 2 4 2 2 5 5 5 ...
>  $ postSpmtsQ: num  1309 1309  384  384 1044 ...
>  $ ante62Q   : num  284 284 218 218 366 ...
>  $ ante08Q   : num  331 331 228 228 492 ...
>  $ ante29Q   : num   297  297 1067 1067 1034 ...
>  $ ante16Q   : num  0 0 0.2 0.2 0 0 0 6.7 0 31.5 ...
>  $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
> 27 5 27 5 36 27 5 24 24 24 ...
> > sprayS <- spray[spray$PD == "Spidermites", , drop =
> TRUE]
> > str(sprayS)
> `data.frame':   111 obs. of  7 variables:
>  $ PD        : Factor w/ 8 levels
> "Botrytis","Downy",..: 5 5 5 5 5 5 5 5 5 5 ...
>  $ postSpmtsQ: num  13395 31588 84254   136   619 ...
>  $ ante62Q   : num   1357 21187 21819   218   237 ...
>  $ ante08Q   : num    973 21740 25112   228   134 ...
>  $ ante29Q   : num    2103 106970  66676   1067    119
> ...
>  $ ante16Q   : num  6.7 0 31.5 0.2 0 0 0 0 14.3 0 ...
>  $ Trt       : Factor w/ 41 levels "Acrobat MZ WP",..:
> 24 24 24 24 24 24 24 24 24 24 ...
> > table(sprayS$Trt)
> 
>    Acrobat MZ WP           Agrifos      Apollo 50 SC
>          CALMAG
>                0                 0                13
>               0
>            DM-31    Dynamec 1.8 EC   Equation Pro DF
>       Evisect S
>                0                13                 0
>               0
>            Flint         Floramite           Impulse
>          Karate
>                0                15                 0
>               0
>      Karate zeon            Melody    Meltatox 40 EC
>             MKP
>                0                 0                 0
>               0
>         Molasses       Nembicidine     Nimrod 250 EC
>  Nissorun 10 EC
>                0                 0                 0
>              12
>           Oberon     Orthene 75 WP       Oscar 20 SC
>         Pegasus
>               15                 0                 9
>              26
>     Polar 50 WSG            Potfos          Proplant
>           Pyrus
>                0                 0                 0
>               0
> Ridomil MZ 63 5WP   Rovral aqua flo      Score 250 EC
>    Secure 36 SC
>                0                 0                 0
>               8
>      Sequestrone          Shavit f         Sporekill
>    Stroby 50 WG
>                0                 0                 0
>               0
>           Switch            Tracer          Trafos K
>        Vandozeb
>                0                 0                 0
>               0
>          Vitomex
>                0
> 
> cheers,
> Mikkel
> 

Your code says to drop dimensions whereas you want to drop factor 
levels (I think).

For example, using the iris data set from R:

ii <- subset(iris, Species == "setosa")   # subset out setosa only
ii$Species <- ii$Species[drop = TRUE]  # drop unused factors
levels(ii$Species) # check that unused factors are gone


iris1 <- subset(iris0, Species == "setosa")
iris1$Species <- iris1$Species[drop = TRUE]



From deepayan at stat.wisc.edu  Wed Jun  1 20:57:43 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 1 Jun 2005 13:57:43 -0500
Subject: [R] tick marks in opposite axis xyplot
In-Reply-To: <87sm02vtrb.fsf@gmail.com>
References: <87sm02vtrb.fsf@gmail.com>
Message-ID: <200506011357.43119.deepayan@stat.wisc.edu>

On Wednesday 01 June 2005 13:04, Sebastian Luque wrote:
> Hello,
>
> I cannot find how to avoid drawing tick marks on the opposite side of axes
> in xyplot (lattice package). Say you have a simple plot:
>
> x <- y <- rnorm(10)
> xyplot(y ~ x)
>
> There are tick marks on the top and right axes. Is it possible to turn off
> drawing of those tick marks, or even turn off drawing of those axes
> completely? Thanks in advance.

xyplot(y ~ x, scales = list(tck = c(1, 0)))

Deepayan



From ozric at web.de  Wed Jun  1 20:59:26 2005
From: ozric at web.de (christian schulz)
Date: Wed, 01 Jun 2005 20:59:26 +0200
Subject: [R] many chr2factors ?
In-Reply-To: <17053.53975.755861.961421@stat.math.ethz.ch>
References: <429D876F.1070106@web.de>
	<17053.53975.755861.961421@stat.math.ethz.ch>
Message-ID: <429E058E.90609@web.de>

...many thanks to clarify for me some things!
christian

>Dear Christian
>
>If you create your data frame by using data.frame all characters
>are automatically transformed into factors unless you force them
>to stay a character. Maybe that can solve your problem easily.
>
>dat <- data.frame(a=1:10, b=letters[1:10])
>str(dat)
>  `data.frame':	10 obs. of  2 variables:
>  $ a: Factor w/ 10 levels "a","b","c","d",..: 1 2 3 4 5 6 7 8 9 10
>  $ b: int  1 2 3 4 5 6 7 8 9 10
> 
>Assuming that doesn't solve your problem due to the way your
>data frame are created you can do it afterwards.
>
>There are two problems with your code. 
>
>First: (and that causes the error) you use in your repeat 
>
>if(!is.character(df[,i]))
>  next
>
>Imagine that the last column of you data frame is not a
>character you jump to the next cycle and then you are outside of
>the range of your data frame. Your break condition is ignored.
>
>Second: You change your data frame inside of a
>function. Variables that are created or changed within a
>function are local. Their life ends with the end of the
>function. Therefore all changes you do will have no effect on
>the global data frame you want to change. See the example:
>
>dat1 <- structure(list(a = 1:10, b = letters[1:10]), .Names = c("a", "b"),
>                  row.names = as.character(1:10), class = "data.frame")
>str(data.frame(dat1))
>  `data.frame':	10 obs. of  2 variables:
>  $ a: int  1 2 3 4 5 6 7 8 9 10
>  $ b: chr  "a" "b" "c" "d" ...
>tofac(dat1)
>  [1] 2
>str(data.frame(dat1))
>  `data.frame':	10 obs. of  2 variables:
>  $ a: int  1 2 3 4 5 6 7 8 9 10
>  $ b: chr  "a" "b" "c" "d" ...
>
>You can use the following code instead
>
>tofac <- function(x){
>  for(i in 1:length(x)) {
>    if(is.character(x[,i]))
>      x[,i] <- factor(x[,i])
>  }
>  x
>}
>
>dat1 <- tofac(dat1)
>  [1] 2
>str(dat1)
>  `data.frame':	10 obs. of  2 variables:
>  $ a: int  1 2 3 4 5 6 7 8 9 10
>  $ b: Factor w/ 10 levels "a","b","c","d",..: 1 2 3 4 5 6 7 8 9 10
>
>The for loop avoids the problem with the index. Therefore it
>works in example that have a non character variable in the last
>column, too and by returning x at the end you are sure that you
>object keeps existing.
>
>Regards,
>
>Christoph
>
>--------------------------------------------------------------
>Christoph Buser <buser at stat.math.ethz.ch>
>Seminar fuer Statistik, LEO C13
>ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
>phone: x-41-44-632-4673		fax: 632-1228
>http://stat.ethz.ch/~buser/
>--------------------------------------------------------------
>
>christian schulz writes:
> > Hi,
> > 
> > i would like transfrom 
> > characters from a data.frame to factors automatic.
> > 
> >  > tofac <- function(df){
> > + i=0
> > + repeat{
> > + i <- i+1
> > + if(!is.character(df[,i]))
> > + next
> > + df[,i] <- as.factor(df[,i])
> > + print(i)
> > + if(i == length(df))
> > + break }
> > + }
> >  >
> >  > tofac(abrdat)
> > [1] 7
> > [1] 8
> > [1] 9
> > [1] 11
> > [1] 13
> > [1] 15
> > Error in "[.data.frame"(df, , i) : undefined columns selected
> > 
> > This are the correct columns and i get the idea put into the loop
> > a empty matrix with dimension like df and return it!?
> > 
> > Another check?
> > abrdat2 <- apply(abrdat,2,function(x) 
> > ifelse(is.character(x),as.factor(x),x))
> > 
> > 
> > many thanks & regards,
> > christian
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From SuzieBlatt at netscape.net  Wed Jun  1 21:04:34 2005
From: SuzieBlatt at netscape.net (SuzieBlatt@netscape.net)
Date: Wed, 01 Jun 2005 15:04:34 -0400
Subject: [R] Gmulti problem
Message-ID: <7CF747BB.17E6F689.0D1322AF@netscape.net>


I'm playing around with Gmulti and have created the appropriate marked point planar pattern (or it tells me that it is anyway), but when I go to use Gmulti, I get a "I and J must be logical vectors" error.  Can anyone explain to me why this is?

My code is something like:
test <- getBigPPP(1994, includeG, c("elm.american", "ash.american", "pine.white"), number=TRUE)
# the getBigPPP is a code that creates the planar point pattern and the #polygonal boundary without me going through all the individual steps each #time.  It does work and I have used it with Kest, Gest and other things #where a planar point pattern is required.  When I type 'test' I get the #following output:
# marked planar point pattern: 197 points
# multitype, with levels = 1,2,3
# window: polygonal boundary
# enclosing rectangle: [ 4953, 5023 ] x [ 4965, 5060]  

Gmulti(test, I=1, J=2)

#gives me:
# Error in Gmulti(test, I=1, J=2),
# I and J must be logical vectors 


Cheers,
Suzie



From deepayan at stat.wisc.edu  Wed Jun  1 21:31:00 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 1 Jun 2005 14:31:00 -0500
Subject: [R] problem with chron scales in lattice
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01FD7103@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01FD7103@pnlmse35.pnl.gov>
Message-ID: <200506011431.00725.deepayan@stat.wisc.edu>

On Wednesday 01 June 2005 12:12, Waichler, Scott R wrote:
> I can't get the scales parameter in xyplot of lattice to work as I
> expected.  I'm using R-2.1.0 and lattice 0.11-8.  There should be year
> labels from 1992 to 2004 for the x axis in the plot below, but instead
> only a few of them appear, and in the wrong spots, as if the coordinate
> system has changed after finishing with the panel function.

This is rather odd. The coordinate system seems fine, since the following 
works as it should:


trellis.focus("panel", 1, 2)
panel.axis(side = "top", at = as.numeric(x.at), 
          outside = F, half = F, labels = x.labels, 
          rot = 0)

I'll try to figure out what the problem is. A temporary workaround is to 
explicitly coerce x.limits and x.at to numeric:

    ...
    xlim = as.numeric(x.limits),
    scales = list(x = list(alternating = T, 
                  at = as.numeric(x.at), 
                  labels = x.labels, tck=0)),
    ...


> library(chron)
> library(lattice)
>
> # vertical grid lines at start of each year
> startdate <- "1/1/1992"
> enddate   <- "1/1/2005"
> startdatetime <- chron(dates=startdate, times="00:00:00")
> enddatetime <- chron(dates=enddate, times="00:00:00")
> x.lines <- as.chron(seq.dates(startdate, enddate, by="years"))
> x.limits <- c(as.chron(startdate), as.chron(enddate))
>
> # year labels on x axis (centered between grid lines)
> x.at <- as.chron(seq.dates("7/1/1992", "7/1/2004", by="years"))
> x.labels <- format(as.Date(dates(x.at)), "%Y")
>
> # dummy data
> x <- x.at[1:2]
> y <- c(105, 105)
> id <- c(1,2)
>
> print(xyplot(y ~ x | id,
>    panel = function(x, y, subscripts, ...) {
>      panel.abline(v = x.lines, col = "lightgray", lty = 1, lwd= 1.0)
>      # function to plot data would go here
>    },  # end of panel
>    as.table=T, layout=c(1,2), main="",
>    xlim=x.limits,
>    scales = list(x = list(alternating = T, at = x.at, labels = x.labels,
> tck=0)),
>    strip = F
>  ) # end xyplot()
> ) # end print()
>
> Thanks for any help,
>
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler at pnl.gov



From Scott.Waichler at pnl.gov  Wed Jun  1 21:29:18 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Wed, 01 Jun 2005 12:29:18 -0700
Subject: [R] problem with chron scales in lattice
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A01FD71BC@pnlmse35.pnl.gov>

> > I can't get the scales parameter in xyplot of lattice to work as I 
> > expected.  I'm using R-2.1.0 and lattice 0.11-8.  There should be
year 
> > labels from 1992 to 2004 for the x axis in the plot below, but
instead 
> > only a few of them appear, and in the wrong spots, as if the 
> > coordinate system has changed after finishing with the panel
function.

> I'll try to figure out what the problem is. A temporary 
> workaround is to 
> explicitly coerce x.limits and x.at to numeric:
> 
>     ...
>     xlim = as.numeric(x.limits),
>     scales = list(x = list(alternating = T, 
>                   at = as.numeric(x.at), 
>                   labels = x.labels, tck=0)),
>     ...

Thanks, Deepayan.  I thought I had already thoroughly tried as.numeric
around the chron objects, but apparently not.  

Scott Waichler



From fd2119 at columbia.edu  Wed Jun  1 21:38:50 2005
From: fd2119 at columbia.edu (Fabrice De Clerck)
Date: Wed, 1 Jun 2005 15:38:50 -0400
Subject: [R] saving in Tiger (Mac users)
Message-ID: <38EF900F-D7A3-4FF5-927A-AD84E53884E2@columbia.edu>

Hello-

I am relatively new to the R community and have been playing with the  
program for about a month. Last week I switched my operating system  
to Tiger (Mac OS 10.4). Since then (I think) I have been unable to  
save scripts and get a message as follows in the R console:

2005-06-01 15:27:10.152 R[203] *** -[NSBigMutableString  
writeToFile:options:error:]: selector not recognized [self = 0x6843d30]
2005-06-01 15:27:10.174 R[203] *** NSTimer discarding exception '*** - 
[NSBigMutableString writeToFile:options:error:]: selector not  
recognized [self = 0x6843d30]' that raised during firing of timer  
with target 11cc230 and selector 'runRELP:'

Does anyone know what I might be doing wrong? or how to fix this error?

Thanks!
Fabrice



From djanes at oeb.harvard.edu  Wed Jun  1 21:42:32 2005
From: djanes at oeb.harvard.edu (Dan Janes)
Date: Wed, 01 Jun 2005 15:42:32 -0400
Subject: [R] Bootstrap direction
Message-ID: <6.1.0.6.0.20050601154038.01fd2c40@mlr.oeb.harvard.edu>

Hi all,
I am trying to bootstrap a small data set into 1000 "pseudodatasets" and 
then run an ANOVA on each one.  Can anyone provide guidance on how I could 
do this?

Thank you.

-Dan Janes


************************************************
Dan Janes, Ph.D.
Harvard University/OEB
26 Oxford St.
Cambridge, MA 02138
Office: 617-496-2375
Fax: 617-495-5667
Email: djanes at oeb.harvard.edu



From deepayan at stat.wisc.edu  Wed Jun  1 22:07:13 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 1 Jun 2005 15:07:13 -0500
Subject: [R] panel.axis() & grid/lattice settings
In-Reply-To: <A89517C7FD248040BB71CA3C04C1ACBB01990113@CNTUSMAEXS4.na.jnj.com>
References: <A89517C7FD248040BB71CA3C04C1ACBB01990113@CNTUSMAEXS4.na.jnj.com>
Message-ID: <200506011507.13817.deepayan@stat.wisc.edu>

On Wednesday 01 June 2005 05:43, Pikounis, Bill [CNTUS] wrote:
> Hello,
> I wish to customize the tick marks and labels of axes in panels produced by
> high-level lattice functions, namely xyplot. I know I can use the scales
> argument to specify values for rot, cex, etc. in the top-level call.
>
> However, I am interested in determining values for cex and rot based on the
> current panel / viewport and device. More specifically, I would like to
> make adjustments when tick labels overlap on the x-axis, such as labels of
> a factor. If I use base graphics, par("cin") or par("cxy") or strwidth(),
> etc. can be used to develop an algorithm to adjust cex or/and rot if
> needed.
>
> I am trying to determine the parameters/settings in grid analogous to
> par("cin"), etc. mentioned above, knowing that par() has no effect in
> lattice / grid. I have dug around the sources for grid and lattice but
> cannot seem to come up with such parameters -- most notably something like
> strwidth(). I see that panel.axis() has a check.overlap argument for
> labels, but I could not trace down the actual code to see how that works.
> What have I overlooked, or where should I be looking?

Paul may be able to give a more insightful answer, but grid allows a string to 
determine it's width in terms of itself, e.g.:

unit(1, "strwidth", data = "foo")

If you want to convert that into, say, inches, you could use 

> convertX(unit(1, "strwidth", data = "foo"), "inches", TRUE)
[1] 0.2344092

I think this would depend on the gpars() in effect, in particular fontsize.

> Indirectly related, setting outside=TRUE in a panel.axis() call does not
> produce visible labels, perhaps due to "issues of clipping" as mentioned in
> its help page. How might one disable clipping for the current panel /
> viewport?

At the grid level, there's a 'clip' argument to 'viewport()'. In lattice, 
these are chosen from 

> str(trellis.par.get("clip"))
List of 2
 $ panel: chr "on"
 $ strip: chr "on"

(In case you are using 'trellis.focus', that can set clipping off.)

Deepayan



From goedman at mac.com  Wed Jun  1 22:02:12 2005
From: goedman at mac.com (Rob J Goedman)
Date: Wed, 1 Jun 2005 13:02:12 -0700
Subject: [R] saving in Tiger (Mac users)
In-Reply-To: <38EF900F-D7A3-4FF5-927A-AD84E53884E2@columbia.edu>
References: <38EF900F-D7A3-4FF5-927A-AD84E53884E2@columbia.edu>
Message-ID: <553A66BC-80E9-4468-BFAC-3B03D96287E1@mac.com>

Fabrice,

Which version of R/R.app are you using? If you're not using the  
latest version (R-2.1.0a.dmg),
can you upgrade and try it again?

Rob

On Jun 1, 2005, at 12:38 PM, Fabrice De Clerck wrote:

> Hello-
>
> I am relatively new to the R community and have been playing with  
> the program for about a month. Last week I switched my operating  
> system to Tiger (Mac OS 10.4). Since then (I think) I have been  
> unable to save scripts and get a message as follows in the R console:
>
> 2005-06-01 15:27:10.152 R[203] *** -[NSBigMutableString  
> writeToFile:options:error:]: selector not recognized [self =  
> 0x6843d30]
> 2005-06-01 15:27:10.174 R[203] *** NSTimer discarding exception  
> '*** -[NSBigMutableString writeToFile:options:error:]: selector not  
> recognized [self = 0x6843d30]' that raised during firing of timer  
> with target 11cc230 and selector 'runRELP:'
>
> Does anyone know what I might be doing wrong? or how to fix this  
> error?
>
> Thanks!
> Fabrice
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From nxt7 at psu.edu  Wed Jun  1 22:20:22 2005
From: nxt7 at psu.edu (NATALIA F TCHETCHERINA)
Date: Wed, 1 Jun 2005 16:20:22 -0400 (EDT)
Subject: [R] mplot :how to deal with missing data
Message-ID: <200506012020.QAA01240@webmail10.cac.psu.edu>

Hello all,
I have  data:

           Genes   time rep vart dye         y trt
130911   sa1-d07 030min   1  col   g  9.636244   o
145771   sa1-d07 030min   1  col   r  8.107577   c
93335    sa1-d07 030min   1  ler   g  7.409566   o
94821    sa1-d07 030min   1  ler   r  5.107160   c
10119101 sa1-d07 030min   2  col   g  8.336862   o
11605101 sa1-d07 030min   2  col   r  7.824530   c
725313   sa1-d07 030min   2  ler   g  8.249347   o
740171   sa1-d07 030min   2  ler   r  7.565084   c
1160522  sa1-d07 030min   3  col   g        NA   c
1011922  sa1-d07 030min   3  col   r        NA   o
562232   sa1-d07 030min   3  ler   g  9.974227   c
547362   sa1-d07 030min   3  ler   r 10.341149   o
..................................................
..................................................
..................................................

I would like to get graphs means for two-way factor combinations
I used Rlab package:
> mplot(data$y[which(data$Genes==sa1-d07)],
data$time[which(data$Genes==sa1-d07)], data$trt[which(data$Genes==sa1-d07)])

However, I have the following error message:

plot window will lay out plots in a 3 by 1 matrix 
Error in plot.window(xlim, ylim, log, asp, ...) : 
        need finite 'ylim' values
In addition: Warning messages:
1: no finite arguments to min; returning Inf 
2: no finite arguments to max; returning -Inf 
> 
I think this is because of some y='NA'.
My question is: how I can deal with this problem?

Sincerely, Natalia.


From bolker at zoo.ufl.edu  Wed Jun  1 22:29:00 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 1 Jun 2005 20:29:00 +0000 (UTC)
Subject: [R] Bootstrap direction
References: <6.1.0.6.0.20050601154038.01fd2c40@mlr.oeb.harvard.edu>
Message-ID: <loom.20050601T220735-680@post.gmane.org>


  
Dan Janes <djanes <at> oeb.harvard.edu> writes:

> 
> Hi all,
> I am trying to bootstrap a small data set into 1000 "pseudodatasets" and 
> then run an ANOVA on each one.  Can anyone provide guidance on how I could 
> do this?
> 
> Thank you.
> 
> -Dan Janes
> 
> ************************************************
> Dan Janes, Ph.D.
> Harvard University/OEB
> 26 Oxford St.
> Cambridge, MA 02138
> Office: 617-496-2375
> Fax: 617-495-5667
> Email: djanes <at> oeb.harvard.edu
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

There are several bootstrap libraries (boot, bootstrap,
simpleboot) available on CRAN, but I think the following
will also do what you want:

## make up some data
X <- data.frame(z=rnorm(20),f=factor(rep(1:5,each=4)))

## linear fit with true values
lm0 <- lm(z~f,data=X)
Fvals <- numeric(1000)
for (i in 1:1000) {
   X.boot <- X[sample(1:nrow(X),replace=TRUE),]
   Fvals[i] <- anova(lm(z~f,data=X.boot))$"F value"[1]
}
hist(Fvals)

  the only part that's at all tricky is figuring out how
to extract the F value (which is what I'm guessing you want)
from the anova() of each bootstrapped data set.

  One complicating issue is that this doesn't
stratify (so it doesn't preserve the experimental design --
in this case that there are 4 samples per factor level).

  If you replace the stuff in the for loop with this:

   z.boot <- unlist(lapply(split(X$z,X$f),sample,replace=TRUE))
   Fvals[i] <- anova(lm(z.boot~X$f))$"F value"[1]

I think it will stratify, at the cost of a bit more black magic.
If you're going to do more of this you should probably look into
the packages (and possibly the associated books).
  Of course, you should check all of this carefully before you believe it.

  cheers, Ben



From j_brindle at hotmail.com  Wed Jun  1 23:02:56 2005
From: j_brindle at hotmail.com (Jim BRINDLE)
Date: Wed, 01 Jun 2005 17:02:56 -0400
Subject: [R] "mvr" function
Message-ID: <BAY20-F8DD342CFF3CF7E07B378C80050@phx.gbl>

Hello,

I am trying to understand how to utilize the "mvr" function in the pls 
Package of R.  I am utilizing the R "pls Package" document dated 18 May 2005 
as guidance.  My data set consists of a 12 x 12 data frame created from 
reading in a table of values.  I have read the data in via the command:

volumes <- read.table("THA_vol.txt", header = TRUE)

and then created a data.frame called "vol".  My response variable is in the 
last column of the "vol" data frame and my dependent variables are in 
columns 1 through 11.

To familiarize myself with this approach I have utilized the NIR data set 
(included in the pls Package).  I get the following command to work with the 
NIR data set:

NIR.pcr <- pcr(y ~ X,6,data=NIR,validation="CV")

However, when I run the following script which effectively substitutes my 
data set (& modify variable names accordingly) into the above equation:

y <- vol[,12]
X <- vol[,1:11]
ans.pcr <- pcr(y ~ X,6,data=vol,validation="CV")


I get the following error:

Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
       invalid variable type

I have looked at the NIR data set in the pls Package and tried to see how it 
"structurally" differs from my data-set "structure" (other than in its 
size).

Does anyone have any insight they might be willing to share?

Thank you kindly.

- Jim



From jsorkin at grecc.umaryland.edu  Wed Jun  1 23:14:01 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 01 Jun 2005 17:14:01 -0400
Subject: [R] R square from lm
Message-ID: <s29decf8.013@grecc.umaryland.edu>

I would like to get R2 (and and adjusted R2)  from an lm. I know I can
compute R2 (SSreg/SStotal), but it would be nice to know if I could get
if automatically, e.g.
fit1<-lm(y~x)
Rsq<-fit1$rSquare.

R2.1.0 Patched under Windows 2000

Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From andy_liaw at merck.com  Wed Jun  1 23:34:10 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 1 Jun 2005 17:34:10 -0400
Subject: [R] R square from lm
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E8FF@usctmx1106.merck.com>

Something like:

> x = runif(10)
> y = rnorm(x)
> fm = lm(y~x)
> summary(fm)[c("r.squared", "adj.r.squared")]
$r.squared
[1] 0.03385419

$adj.r.squared
[1] -0.08691404

[They are computed by summary.lm(), not lm().]

HTH,
Andy

> From: John Sorkin
> 
> I would like to get R2 (and and adjusted R2)  from an lm. I know I can
> compute R2 (SSreg/SStotal), but it would be nice to know if I 
> could get
> if automatically, e.g.
> fit1<-lm(y~x)
> Rsq<-fit1$rSquare.
> 
> R2.1.0 Patched under Windows 2000
> 
> Thanks,
> John
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ccleland at optonline.net  Wed Jun  1 23:36:32 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 01 Jun 2005 17:36:32 -0400
Subject: [R] R square from lm
In-Reply-To: <s29decf8.013@grecc.umaryland.edu>
References: <s29decf8.013@grecc.umaryland.edu>
Message-ID: <429E2A60.9040303@optonline.net>

John Sorkin wrote:
> I would like to get R2 (and and adjusted R2)  from an lm. I know I can
> compute R2 (SSreg/SStotal), but it would be nice to know if I could get
> if automatically, e.g.
> fit1<-lm(y~x)
> Rsq<-fit1$rSquare.
> 
> R2.1.0 Patched under Windows 2000

summary(fit1)$r.squared
summary(fit1)$adj.r.squared

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Robert.McGehee at geodecapital.com  Thu Jun  2 01:14:03 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 1 Jun 2005 19:14:03 -0400
Subject: [R] "mvr" function
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C9465E8@MSGBOSCLB2WIN.DMN1.FMR.COM>

Jim,
I had some of the same difficulties. The NIR data frame consists of a
column of y variables and a matrix of X variables (and until looking at
this dataset, I had not realized that data frames could hold matrices).
So, after consulting the R-help sages, I turned by data into an
identical structure using something like this:

dataSet <- data.frame(y = vol[, 12])
dataSet$X <- data.matrix(vol[, 1:11])

ans.pcr <- pcr(y ~ X, 6, data = dataSet, validation = "CV")

If there's a more elegant way of doing this without using data frames of
matrices, I'd be interested as well.

HTH,
Robert

-----Original Message-----
From: Jim BRINDLE [mailto:j_brindle at hotmail.com] 
Sent: Wednesday, June 01, 2005 5:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] "mvr" function


Hello,

I am trying to understand how to utilize the "mvr" function in the pls 
Package of R.  I am utilizing the R "pls Package" document dated 18 May
2005 
as guidance.  My data set consists of a 12 x 12 data frame created from 
reading in a table of values.  I have read the data in via the command:

volumes <- read.table("THA_vol.txt", header = TRUE)

and then created a data.frame called "vol".  My response variable is in
the 
last column of the "vol" data frame and my dependent variables are in 
columns 1 through 11.

To familiarize myself with this approach I have utilized the NIR data
set 
(included in the pls Package).  I get the following command to work with
the 
NIR data set:

NIR.pcr <- pcr(y ~ X,6,data=NIR,validation="CV")

However, when I run the following script which effectively substitutes
my 
data set (& modify variable names accordingly) into the above equation:

y <- vol[,12]
X <- vol[,1:11]
ans.pcr <- pcr(y ~ X,6,data=vol,validation="CV")


I get the following error:

Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
       invalid variable type

I have looked at the NIR data set in the pls Package and tried to see
how it 
"structurally" differs from my data-set "structure" (other than in its 
size).

Does anyone have any insight they might be willing to share?

Thank you kindly.

- Jim

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From luan_sheng at yahoo.com  Thu Jun  2 02:08:06 2005
From: luan_sheng at yahoo.com (luan_sheng)
Date: Thu, 02 Jun 2005 08:08:06 +0800
Subject: [R] Who can tell me some packages for population genetic structure's
 simulation?
Message-ID: <429E4DE6.6020704@yahoo.com>

hi,everyone,I am beginner for R. I want to use R for simulating
population genetic structrue of different generations. This thesis
include alleles of 1-10 loci. Does a package like this exist?

thanks for you.

__________________________________________________

Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰



From p.murrell at auckland.ac.nz  Thu Jun  2 02:37:17 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 02 Jun 2005 12:37:17 +1200
Subject: [R] x11 and pseudo-color
References: <429D6F6F.9010000@fz-rossendorf.de>
Message-ID: <429E54BD.3000204@stat.auckland.ac.nz>

Hi


joerg van den hoff wrote:
> for some reason the following message seems not to have reached the list 
>  in the first try, at least I can't find it. my apologies if this is my 
> fault:
> 
> we are running R under Solaris with SunRay Terminals, which are set to 8
> bit color to comply with some other software. In this configuration,
> X11() opens with colortype=true, i.e., it is not recognized that
> actually the display is only 8 bit. This leads to error messages
> (advising to use 'pseudo.cube').
> 
> question 1: is X11() supposed to recognize  the actual color
> capabilities? i.e. is this a bug?


R does try to determine the capabilities of the X11 server.  I think 
what may be happening is that your display is being correctly identified 
as pseudocolor, but R has two strategies for handling pseudocolor, 
"pseudo" and "pseudo.cube" and it uses "pseudo" by default (see ?x11). 
If you use more than 256 colours under this scheme you will run out of 
colours.


> question 2: is there a possibility to query the color capabilities from
> within R in order to being able to open the X11() displays always (for
> true color as well as for 8 bit) with the correct colortype setting from
> within a function?


Not that I know of.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From jsorkin at grecc.umaryland.edu  Thu Jun  2 04:13:10 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 01 Jun 2005 22:13:10 -0400
Subject: [R] confusion with boot
Message-ID: <s29e3314.055@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050601/e5fbc28a/attachment.pl

From andy_liaw at merck.com  Thu Jun  2 04:25:01 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 1 Jun 2005 22:25:01 -0400
Subject: [R] confusion with boot
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E902@usctmx1106.merck.com>

Your function does not meet the requirement for boot().  Here's an example:

> x <- runif(100)
> y <- x + rnorm(100, sd=0.1)
> dat <- data.frame(x, y)
> rm(x,y)
> rsq <- function(data, idx) summary(lm(y~x, data=dat[idx,]))$r.squared
> rsq.boot <- boot(dat, rsq, R=200)
> rsq.boot

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = dat, statistic = rsq, R = 200)


Bootstrap Statistics :
     original       bias    std. error
t1* 0.8964966 0.0002857293  0.01841738

HTH,
Andy

> From: John Sorkin
> 
> I think I am doing something wrong when I try to bootstrap R square
> obtained from lm. My code is included below. No matter how 
> many times I
> run the simulation, I always get exactly the same result, the bias and
> std.error are always zero. I would think that these values should be
> non-zero. I would appreciate any suggestions as to what I am doing
> wrong, or perhaps what I fail to understand.
> R 2.1.0 Patched Win 2k.
> Thanks,
> John
>  
>  
> >detefun3<-function (d,w)
> summary(lm(d$sg120~d$fg120adj,data=d))$r.square
> > boot(delete,deletefun3,R=200)
>  
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> Call:
> boot(data = delete, statistic = deletefun3, R = 200)
> 
> Bootstrap Statistics :
>      original  bias    std. error
> t1* 0.3028048       0           0
> > 
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From epakpahan at gmail.com  Thu Jun  2 07:51:38 2005
From: epakpahan at gmail.com (Eduwin Pakpahan)
Date: Thu, 2 Jun 2005 12:51:38 +0700
Subject: [R] Who can tell me some packages for population genetic
	structure's simulation?
In-Reply-To: <429E4DE6.6020704@yahoo.com>
References: <429E4DE6.6020704@yahoo.com>
Message-ID: <646e054005060122512dc92abc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050602/a636417a/attachment.pl

From yzhang4 at turing.une.edu.au  Thu Jun  2 08:15:39 2005
From: yzhang4 at turing.une.edu.au (Yuandan Zhang)
Date: Thu, 02 Jun 2005 16:15:39 +1000
Subject: [R] Who can tell me some packages for population
	genetic	structure's simulation?
In-Reply-To: <646e054005060122512dc92abc@mail.gmail.com>
References: <429E4DE6.6020704@yahoo.com>
	<646e054005060122512dc92abc@mail.gmail.com>
Message-ID: <429EA40B.7000600@turing.une.edu.au>

There are others too,

such as
genetics
qtl
qtlsim
bqtl




Eduwin Pakpahan wrote:

>download this package "gap" - genetic analysis package. 
>
>E
>
>
>On 6/2/05, luan_sheng <luan_sheng at yahoo.com> wrote:
>  
>
>>hi,everyone,I am beginner for R. I want to use R for simulating
>>population genetic structrue of different generations. This thesis
>>include alleles of 1-10 loci. Does a package like this exist?
>>
>>thanks for you.
>>
>>__________________________________________________
>>
>>Å—Å≈ÅªÅ¢Å√Å‚Å∑Å—GÅ”Å ÅœÅ‰Å£Å≠Å÷Å–ÅπÅ˙ÅµÅ⁄Å“ÅªÅæÅ¯ÅŒÅﬁÅ¿Å¨ÅªÅ¯Å”Å ÅºÅ˛Å…ÅßÅ»Å≈Å≥Å¨Å¥ÅÛÅ”Å ÅœÅ‰
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From schween at snafu.de  Thu Jun  2 10:41:49 2005
From: schween at snafu.de (Sven C. Koehler)
Date: Thu, 2 Jun 2005 10:41:49 +0200
Subject: [R] Dynamic Dictionary Data Type?
Message-ID: <20050602084149.GC4564@boing.buug.de>

Hello!

I have an algorithm which performs lengthy operations and I would like to
cache results.  Other languages usually offer a dictionary data type
which I can use as an efficient way to dynamically cache already
calculated results - what's the best way to do this in R?

Best wishes,

Sven C. Koehler



From mhenze at phytomed.uni-kiel.de  Thu Jun  2 10:54:16 2005
From: mhenze at phytomed.uni-kiel.de (Matthias Henze)
Date: Thu, 02 Jun 2005 10:54:16 +0200
Subject: [R] Selecting input and output variables in clara (cluster-package)
Message-ID: <1117702456.1720.10.camel@localhost.localdomain>

Moin,

is there any possibility to choose the input variables which are
involved in the analysis,
 " ..., and each column correspondends to a variable. ..."
(R-Reference-Manual)
or do I have to delete them from the data.frame?

An then I'd like to get the original dataset with the classification of
the cluster, at the moment i do it this way

clusterFrame <- merge(claraResult$clustering,claraResult$data,by = 0,
all.x = TRUE);


Thanks Matthias

-- 

Matthias Henze
Institut f??r Phytopathologie
Hermann-Rodewaldstr. 9
24118 Kiel

Telefon :0049(0) 431-880 7433
Fax     :0049(0) 431-880 1583
email   :mhenze at phytomed.uni-kiel.de



From LarsVegas at nikocity.de  Thu Jun  2 10:56:13 2005
From: LarsVegas at nikocity.de (Lars Claussen)
Date: Thu, 2 Jun 2005 10:56:13 +0200
Subject: [R] DEM calculation
Message-ID: <001601c56750$efedc970$0300a8c0@LARSRENNMAUS>

Hello R-World,

i am trying to calculate data for a DEM (Digital Elavation Model) which i
also want to plot in R. i have the coordinates for the lower left corner
which look something like this:

x<-42,2
y<-50,5

besides i have the cellsize of the grid, which is:

z<-1,1

what i do is to calculate the corrdinates of the cells to the right and top,
what i can do by specifying the number of rows and columns. these are
j<-1:805 (columns)
i<-1:616 (rows)

well, to calculate them seperatly is easy:

r<-x+(i-1)*z
h<-y+(j-1)*z

but i can't get it straight how to create a data frame with two rows, one
for the x- the other for the y-coordinate. The problem is, that lots of
values have to be repeated as every  first row x-coordinate has the same
y-coordinate and so on.

finally it should look like this:

x            y
42,2   50,5
43,3   50,5
44,4   50,5
45,5   50,5
 .         .
 .         .
42,2   51,5
43,3   51,5
44,3   51,5

any suggestion? thanks in advance, Lars Claussen



From lecoutre at stat.ucl.ac.be  Thu Jun  2 11:28:11 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 2 Jun 2005 11:28:11 +0200
Subject: [R] DEM calculation
In-Reply-To: <001601c56750$efedc970$0300a8c0@LARSRENNMAUS>
Message-ID: <00dd01c56755$6512f730$6e8b6882@didacdom.stat.ucl.ac.be>



You can use ??expand.grid'

?? expand.grid(x=seq(42.2,45.2,by=1),y=seq(50.5,51.5,by=1))
     x    y
1 42.2 50.5
2 43.2 50.5
3 44.2 50.5
4 45.2 50.5
5 42.2 51.5
6 43.2 51.5
7 44.2 51.5
8 45.2 51.5


Eric

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lars Claussen
> Sent: jeudi 2 juin 2005 10:56
> To: R-help at stat.math.ethz.ch
> Subject: [R] DEM calculation
> 
> 
> Hello R-World,
> 
> i am trying to calculate data for a DEM (Digital Elavation 
> Model) which i also want to plot in R. i have the coordinates 
> for the lower left corner which look something like this:
> 
> x<-42,2
> y<-50,5
> 
> besides i have the cellsize of the grid, which is:
> 
> z<-1,1
> 
> what i do is to calculate the corrdinates of the cells to the 
> right and top, what i can do by specifying the number of rows 
> and columns. these are j<-1:805 (columns) i<-1:616 (rows)
> 
> well, to calculate them seperatly is easy:
> 
> r<-x+(i-1)*z
> h<-y+(j-1)*z
> 
> but i can't get it straight how to create a data frame with 
> two rows, one for the x- the other for the y-coordinate. The 
> problem is, that lots of values have to be repeated as every  
> first row x-coordinate has the same y-coordinate and so on.
> 
> finally it should look like this:
> 
> x            y
> 42,2   50,5
> 43,3   50,5
> 44,4   50,5
> 45,5   50,5
>  .         .
>  .         .
> 42,2   51,5
> 43,3   51,5
> 44,3   51,5
> 
> any suggestion? thanks in advance, Lars Claussen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fernando.tusell at ehu.es  Thu Jun  2 11:36:50 2005
From: fernando.tusell at ehu.es (F.Tusell)
Date: Thu, 02 Jun 2005 11:36:50 +0200
Subject: [R] Fitting ARMA model with known inputs.
Message-ID: <1117705010.9862.18.camel@agesi.bs.ehu.es>

Functions like arima() (in stats) may be what you want. Also you may
have a look at package dse1.

HTH, ft.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica       fernando.tusell at ehu.es
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From ligges at statistik.uni-dortmund.de  Thu Jun  2 11:39:04 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Jun 2005 11:39:04 +0200
Subject: [R] using user-supplied derivatives in rgenoud
In-Reply-To: <33016.129.217.181.180.1117644380.squirrel@unimail.uni-dortmund.de>
References: <33016.129.217.181.180.1117644380.squirrel@unimail.uni-dortmund.de>
Message-ID: <429ED3B8.1060601@statistik.uni-dortmund.de>

Thoralf Mildenberger wrote:
> I have been using the rgenoud package for a nonlinear least-squares
> problem with lots of local minima, and it works very well but takes lots
> of time. According to the article refrenced in the documentation, the
> original GENOUD-software by the same authors seems to allow for
> user-supplied analytical derivatives instead of numerical approximations,
> which would probably save some time. Does anybody know whether this
> feature is also available in rgenoud? The documentation says nothing about
> this, but rgenoud seems to call optim() in the stats-package, which does
> allow for passing a function evaluating the derivatives.
> 
> Thank you,
> Thoralf Mildenberger


Looking at the code suggests that you cannot supply derivates.
Anyway, this question is very special and should perhaps be addressed to 
the maintainer of the package rather than R-help.

Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sofyan.iyan at gmail.com  Thu Jun  2 11:50:03 2005
From: sofyan.iyan at gmail.com (Sofyan Iyan)
Date: Thu, 2 Jun 2005 11:50:03 +0200
Subject: [R] Re: Which variable exist after random
In-Reply-To: <92e186a0050601080518988055@mail.gmail.com>
References: <92e186a0050601080518988055@mail.gmail.com>
Message-ID: <92e186a005060202507670d83d@mail.gmail.com>

Dear R-help again,
I have a result something like this,
[[1]]
[1] "Game"     "Internet"

[[2]]
[1] "Game"     "Internet"

[[3]]
[1] "Game"    "Time"

How could I make the result above like,

[1] "Game","Internet","Time"

Regards, Sofyan

On 6/1/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> Dear R-helper,
> How could I count only some variable was exist after running sample
> (random) function.
> For example,
> 
> > testx <- factor(c("Game","Paper","Internet","Time","Money"))
> >    for(i in 1:2) {
> +        x <- sample(testx,replace=TRUE)
> +        print(x)
> +    }
> [1] Money    Money    Time     Internet Time
> Levels: Game Internet Money Paper Time
> [1] Time  Money Game  Money Money
> Levels: Game Internet Money Paper Time
> >
> 
> The result above Game, Internet, Money, and Time only exist and
> "Paper" was missing.
> Best, Sofyan
>



From ernesto at ipimar.pt  Thu Jun  2 12:06:55 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 02 Jun 2005 11:06:55 +0100
Subject: [R] showMethods doubt
Message-ID: <429EDA3F.5040402@ipimar.pt>

Hi,

I'm developing in S4 and I wanted to see the methods for a specific 
class using showMethods but I didn't succed. Can someone help ? See the 
example below.

setClass("myclass",
	representation(
		name	="character"
	)
)

setGeneric("mymeth", function(obj, ...){
	standardGeneric("mymeth")
	}
)

setMethod("mymeth", signature("myclass"), function(obj){
	print(paste("Hi,", obj at name, "and I don't know what to do with 
showMethods !"))
	}
)

Now if you do:

 > myobj <- new("myclass", name="my name is joe")
 > mymeth(myobj)
[1] "Hi, my name is joe and I don't know what to do with showMethods !"

So the method is working, let's use showMethods:

 > showMethods(classes="myclass")

Function "addNextMethod":
<Empty Methods List>

Function "body<-":
<Empty Methods List>

Function "coerce":
<Empty Methods List>

Function "initialize":
.Object = "myclass"
     (inherited from .Object = "ANY")

Function "loadMethod":
<Empty Methods List>

Function "show":
object = "myclass"
     (inherited from object = "ANY")
NULL

mymeth does not show on that list, why ?

Regards

EJ



From andy_liaw at merck.com  Thu Jun  2 12:31:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Jun 2005 06:31:53 -0400
Subject: [R] Re: Which variable exist after random
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E903@usctmx1106.merck.com>

Try unique(unlist(myList)).

Andy

> From: Sofyan Iyan
> 
> Dear R-help again,
> I have a result something like this,
> [[1]]
> [1] "Game"     "Internet"
> 
> [[2]]
> [1] "Game"     "Internet"
> 
> [[3]]
> [1] "Game"    "Time"
> 
> How could I make the result above like,
> 
> [1] "Game","Internet","Time"
> 
> Regards, Sofyan
> 
> On 6/1/05, Sofyan Iyan <sofyan.iyan at gmail.com> wrote:
> > Dear R-helper,
> > How could I count only some variable was exist after running sample
> > (random) function.
> > For example,
> > 
> > > testx <- factor(c("Game","Paper","Internet","Time","Money"))
> > >    for(i in 1:2) {
> > +        x <- sample(testx,replace=TRUE)
> > +        print(x)
> > +    }
> > [1] Money    Money    Time     Internet Time
> > Levels: Game Internet Money Paper Time
> > [1] Time  Money Game  Money Money
> > Levels: Game Internet Money Paper Time
> > >
> > 
> > The result above Game, Internet, Money, and Time only exist and
> > "Paper" was missing.
> > Best, Sofyan
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From subianto at gmail.com  Thu Jun  2 13:04:28 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Thu, 02 Jun 2005 13:04:28 +0200
Subject: [R] How to change all name of variables
Message-ID: <429EE7BC.4050706@gmail.com>

Dear R-helpers,
First I apologize if my question is quite simple
I have a large datasets which more 100 variables.
For a research I need to change all name of variables with add one or
more letters on each variables.
For example,
 > data(Pima.tr)
 > Pima.tr[1:5,]
   npreg glu bp skin  bmi   ped age type
1     5  86 68   28 30.2 0.364  24   No
2     7 195 70   33 25.1 0.163  55  Yes
3     5  77 82   41 35.8 0.156  35   No
4     0 165 76   43 47.9 0.259  26   No
5     0 107 60   25 26.4 0.133  23   No
 >
 > dimnames(Pima.tr)[[2]]
[1] "npreg" "glu"   "bp"    "skin"  "bmi"   "ped"   "age"   "type"
 >

I need to change the variables name ,
"npreg" "glu" "bp" "skin" "bmi" "ped" "age" "type"
with
"xyz.npreg" "xyz.glu" "xyz.bp" "xyz.skin" "xyz.bmi" "xyz.ped" "xyz.age" 
"xyz.type"

How can I make this (automatically). I don't want to make manual with 
more 100 variables.
I  would be very happy if anyone could help me.
Thank you for your time.
Kindly regards, Muhammad Subianto



From sdavis2 at mail.nih.gov  Thu Jun  2 13:20:09 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 2 Jun 2005 07:20:09 -0400
Subject: [R] How to change all name of variables
In-Reply-To: <429EE7BC.4050706@gmail.com>
References: <429EE7BC.4050706@gmail.com>
Message-ID: <04ec56529aabadcc4bdc9163fbc57a29@mail.nih.gov>

See ?paste.

Something like below if you have 100 column names:

dimnames(pima.tr)[[2]] <- 
paste(rep('xyz',100),dimnames(pima.tr)[[2]],sep=".")

You probably want to test the paste statement before setting the 
dimnames, or operate on a copy of the data until you get the hang of 
using paste.

Sean

On Jun 2, 2005, at 7:04 AM, Muhammad Subianto wrote:

> Dear R-helpers,
> First I apologize if my question is quite simple
> I have a large datasets which more 100 variables.
> For a research I need to change all name of variables with add one or
> more letters on each variables.
> For example,
> > data(Pima.tr)
> > Pima.tr[1:5,]
>   npreg glu bp skin  bmi   ped age type
> 1     5  86 68   28 30.2 0.364  24   No
> 2     7 195 70   33 25.1 0.163  55  Yes
> 3     5  77 82   41 35.8 0.156  35   No
> 4     0 165 76   43 47.9 0.259  26   No
> 5     0 107 60   25 26.4 0.133  23   No
> >
> > dimnames(Pima.tr)[[2]]
> [1] "npreg" "glu"   "bp"    "skin"  "bmi"   "ped"   "age"   "type"
> >
>
> I need to change the variables name ,
> "npreg" "glu" "bp" "skin" "bmi" "ped" "age" "type"
> with
> "xyz.npreg" "xyz.glu" "xyz.bp" "xyz.skin" "xyz.bmi" "xyz.ped" 
> "xyz.age" "xyz.type"
>
> How can I make this (automatically). I don't want to make manual with 
> more 100 variables.
> I  would be very happy if anyone could help me.
> Thank you for your time.
> Kindly regards, Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From fezzi at stat.unibo.it  Thu Jun  2 13:43:11 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Thu, 2 Jun 2005 13:43:11 +0200 (CEST)
Subject: [R] maximum likelihood standard deviation
Message-ID: <2673357.1117712591764.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear R-helpers,
Anybody knows which function can I use to comupute maximum likelihood
standard errors?

Using the function "nlm" I can get the estimate of the parameters of any
likelihood that I want (for example now I am working on a jump diffusion
process) but what about the standard error?

Is there a function that I can use to calculate the second derivate of
the likelihood function respect to the vector of parameters?

Thank you so much for your help,

Carlo Fezzi



From chrysopa at gmail.com  Thu Jun  2 13:43:25 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Thu, 2 Jun 2005 08:43:25 -0300
Subject: [R] How to calculate the correct SE in a nested or spliplot anova?
Message-ID: <200506020843.25852.chrysopa@gmail.com>

Hi!

How to calculate the correct SE of mean in a nested or spliplot anova?

Nested example:
---------------------
m <- aov(Glycogen~Treatment+Error(Treatment/Rat/Liver))
> m

Call:
aov(formula = Glycogen ~ Treatment + Error(Treatment/Rat/Liver))

Grand Mean: 142.2222 

Stratum 1: Treatment

Terms:
                Treatment
Sum of Squares   1557.556
Deg. of Freedom         2

Estimated effects may be unbalanced

Stratum 2: Treatment:Rat

Terms:
                Residuals
Sum of Squares   797.6667
Deg. of Freedom         3

Residual standard error: 16.3061 

Stratum 3: Treatment:Rat:Liver

Terms:
                Residuals
Sum of Squares        594
Deg. of Freedom        12

Residual standard error: 7.035624 

Stratum 4: Within

Terms:
                Residuals
Sum of Squares        381
Deg. of Freedom        18

Residual standard error: 4.600725 
> 
--------------------
I need to make a barplot with error bar for treatment. How to calculate the 
correct SE?

SplitPLot example:
--------------------
> m <- 
aov(yield~irrigation*density*fertilizer+Error(block/irrigation/density/fertilizer))
> m

Call:
aov(formula = yield ~ irrigation * density * fertilizer + 
Error(block/irrigation/density/fertilizer))

Grand Mean: 99.72222 

Stratum 1: block

Terms:
                Residuals
Sum of Squares   194.4444
Deg. of Freedom         3

Residual standard error: 8.050765 

Stratum 2: block:irrigation

Terms:
                irrigation Residuals
Sum of Squares    8277.556  1411.778
Deg. of Freedom          1         3

Residual standard error: 21.69315 
8 out of 9 effects not estimable
Estimated effects are balanced

Stratum 3: block:irrigation:density

Terms:
                 density irrigation:density Residuals
Sum of Squares  1758.361           2747.028  2787.944
Deg. of Freedom        2                  2        12

Residual standard error: 15.24233 
8 out of 12 effects not estimable
Estimated effects may be unbalanced

Stratum 4: block:irrigation:density:fertilizer

Terms:
                fertilizer irrigation:fertilizer density:fertilizer
Sum of Squares   1977.4444              953.4444           304.8889
Deg. of Freedom          2                     2                  4
                irrigation:density:fertilizer Residuals
Sum of Squares                       234.7222 3108.8333
Deg. of Freedom                             4        36

Residual standard error: 9.292819 
Estimated effects may be unbalanced
-----------------------

I need the SE for each combination. How to calculate this?

Thanks for all.
Ronaldo
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From luan_sheng at yahoo.com  Thu Jun  2 13:46:41 2005
From: luan_sheng at yahoo.com (luan_sheng)
Date: Thu, 02 Jun 2005 19:46:41 +0800
Subject: [R] Do some one give me several whole examples or papers for
 rmetasim package' using?
Message-ID: <429EF1A1.8080005@yahoo.com>

Now I want to some simulation Impact of shrimp's releasing to genetic
structure of wild population,i think rmetasim can help me finish this
task. But on hand I have not the resource about it. Can anyone help me ?
Thanks!


luan_sheng



From dimitris.rizopoulos at med.kuleuven.be  Thu Jun  2 13:50:06 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 2 Jun 2005 13:50:06 +0200
Subject: [R] maximum likelihood standard deviation
References: <2673357.1117712591764.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
Message-ID: <001d01c56769$38488170$0540210a@www.domain>

look at mle() (package "stats4") and optim() (i.e., "optim(..., 
hessian = TRUE)").

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Carlo Fezzi" <fezzi at stat.unibo.it>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 02, 2005 1:43 PM
Subject: [R] maximum likelihood standard deviation


> Dear R-helpers,
> Anybody knows which function can I use to comupute maximum 
> likelihood
> standard errors?
>
> Using the function "nlm" I can get the estimate of the parameters of 
> any
> likelihood that I want (for example now I am working on a jump 
> diffusion
> process) but what about the standard error?
>
> Is there a function that I can use to calculate the second derivate 
> of
> the likelihood function respect to the vector of parameters?
>
> Thank you so much for your help,
>
> Carlo Fezzi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bolker at zoo.ufl.edu  Thu Jun  2 13:50:38 2005
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 2 Jun 2005 11:50:38 +0000 (UTC)
Subject: [R] maximum likelihood standard deviation
References: <2673357.1117712591764.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>
Message-ID: <loom.20050602T134817-270@post.gmane.org>


Carlo Fezzi <fezzi <at> stat.unibo.it> writes:

> 
> Dear R-helpers,
> Anybody knows which function can I use to comupute maximum likelihood
> standard errors?
> 
> Using the function "nlm" I can get the estimate of the parameters of any
> likelihood that I want (for example now I am working on a jump diffusion
> process) but what about the standard error?
> 
> Is there a function that I can use to calculate the second derivate of
> the likelihood function respect to the vector of parameters?
> 
> Thank you so much for your help,
> 
> Carlo Fezzi
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


  if you set hessian=TRUE when calling nlm it will give you
exactly what you want -- second derivative [sic] of the
likelihood w.r.t. parameters (provided of course that your
objective function in nlm() is the negative log-likelihood).
solve() on the hessian should invert the Hessian and give
you the variance-covariance matrix (then sqrt(diag()) to
get the s.d.s)



From Andreas.Friedrich at dit.de  Thu Jun  2 14:16:54 2005
From: Andreas.Friedrich at dit.de (Friedrich, Andreas (dit))
Date: Thu, 2 Jun 2005 14:16:54 +0200 
Subject: [R] repeated vector in matrix
Message-ID: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050602/8adfbf4f/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jun  2 14:21:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Jun 2005 14:21:36 +0200
Subject: [R] repeated vector in matrix
In-Reply-To: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>
References: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>
Message-ID: <429EF9D0.1000703@statistik.uni-dortmund.de>

Friedrich, Andreas (dit) wrote:

> Hallo,
> 
> I'll need a matrix with n rows of the an identical vector.
> 
> 
> 
> 
>>h
> 
> [1] 3 3 3 3 2 2 2
> 
> 
> The nmatrix should look like this:
> 
> 
>>x<-rbind(h,h,h)
>>x
> 
>   [,1] [,2] [,3] [,4] [,5] [,6] [,7]
> h    3    3    3    3    2    2    2
> h    3    3    3    3    2    2    2
> h    3    3    3    3    2    2    2
> 
> 
> 
> but I need n rows which must be variable. Can anyone help me?


   x <- matrix(h, nrow = n, ncol = length(h), byrow = TRUE)

Uwe Ligges

> thanks Andreas
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Jun  2 14:23:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jun 2005 14:23:31 +0200
Subject: [R] repeated vector in matrix
In-Reply-To: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>
References: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>
Message-ID: <x2psv5lzgc.fsf@turmalin.kubism.ku.dk>

"Friedrich, Andreas (dit)" <Andreas.Friedrich at dit.de> writes:

> Hallo,
> 
> I'll need a matrix with n rows of the an identical vector.
> 
> 
> 
> > h
> [1] 3 3 3 3 2 2 2
> 
> 
> The nmatrix should look like this:
> 
> > x<-rbind(h,h,h)
> > x
>   [,1] [,2] [,3] [,4] [,5] [,6] [,7]
> h    3    3    3    3    2    2    2
> h    3    3    3    3    2    2    2
> h    3    3    3    3    2    2    2
> 
> 
> 
> but I need n rows which must be variable. Can anyone help me?

Like this, for instance:

> h<-c(3, 3, 3, 3, 2, 2, 2)
> n<-5
> matrix(h, nrow=n, ncol=length(h), byrow=TRUE)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    3    3    3    3    2    2    2
[2,]    3    3    3    3    2    2    2
[3,]    3    3    3    3    2    2    2
[4,]    3    3    3    3    2    2    2
[5,]    3    3    3    3    2    2    2


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.be  Thu Jun  2 14:23:30 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 2 Jun 2005 14:23:30 +0200
Subject: [R] repeated vector in matrix
References: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>
Message-ID: <006901c5676d$e2a15fd0$0540210a@www.domain>

try this:

h <- rep(3:2, c(4, 3))
############
n  <- 10
matrix(rep(h, n),  nrow = n, byrow = TRUE)

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Friedrich, Andreas (dit)" <Andreas.Friedrich at dit.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 02, 2005 2:16 PM
Subject: [R] repeated vector in matrix


> Hallo,
>
> I'll need a matrix with n rows of the an identical vector.
>
>
>
>> h
> [1] 3 3 3 3 2 2 2
>
>
> The nmatrix should look like this:
>
>> x<-rbind(h,h,h)
>> x
>  [,1] [,2] [,3] [,4] [,5] [,6] [,7]
> h    3    3    3    3    2    2    2
> h    3    3    3    3    2    2    2
> h    3    3    3    3    2    2    2
>
>
>
> but I need n rows which must be variable. Can anyone help me?
>
> thanks Andreas
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bitwrit at ozemail.com.au  Fri Jun  3 00:42:11 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 02 Jun 2005 22:42:11 +0000
Subject: [R] mplot :how to deal with missing data
In-Reply-To: <200506012020.QAA01240@webmail10.cac.psu.edu>
References: <200506012020.QAA01240@webmail10.cac.psu.edu>
Message-ID: <429F8B43.5060407@ozemail.com.au>

NATALIA F TCHETCHERINA wrote:
> Hello all,
> I have  data:
> 
>            Genes   time rep vart dye         y trt
> 130911   sa1-d07 030min   1  col   g  9.636244   o
> 145771   sa1-d07 030min   1  col   r  8.107577   c
> 93335    sa1-d07 030min   1  ler   g  7.409566   o
> 94821    sa1-d07 030min   1  ler   r  5.107160   c
> 10119101 sa1-d07 030min   2  col   g  8.336862   o
> 11605101 sa1-d07 030min   2  col   r  7.824530   c
> 725313   sa1-d07 030min   2  ler   g  8.249347   o
> 740171   sa1-d07 030min   2  ler   r  7.565084   c
> 1160522  sa1-d07 030min   3  col   g        NA   c
> 1011922  sa1-d07 030min   3  col   r        NA   o
> 562232   sa1-d07 030min   3  ler   g  9.974227   c
> 547362   sa1-d07 030min   3  ler   r 10.341149   o
> ..................................................
> ..................................................
> ..................................................
> 
> I would like to get graphs means for two-way factor combinations
> I used Rlab package:
> 
>>mplot(data$y[which(data$Genes==sa1-d07)],
> 
> data$time[which(data$Genes==sa1-d07)], data$trt[which(data$Genes==sa1-d07)])
>
> However, I have the following error message:
> 
> plot window will lay out plots in a 3 by 1 matrix 
> Error in plot.window(xlim, ylim, log, asp, ...) : 
>         need finite 'ylim' values
> In addition: Warning messages:
> 1: no finite arguments to min; returning Inf 
> 2: no finite arguments to max; returning -Inf 
> 
> I think this is because of some y='NA'.
> My question is: how I can deal with this problem?

Hi Natalia,

Your problem is that the character variable "genes" should be enclosed 
in quotes:

mplot(data$y[which(data$Genes=="sa1-d07")],
   data$time[which(data$Genes=="sa1-d07")],
   data$trt[which(data$Genes=="sa1-d07")])

or a bit more neatly:

which.genes<-data$Genes=="sa1-d07"
mplot(data$y[which.genes],data$time[which.genes],data$trt[which.genes])

Without the quotes, the interpreter reads the value as sa1 - d07, and 
since it cannot find an object named sa1, returns no values. Also, 
"data" is not an ideal name for your data frame.

Jim



From r.hankin at noc.soton.ac.uk  Thu Jun  2 14:44:44 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Thu, 2 Jun 2005 13:44:44 +0100
Subject: [R] repeated vector in matrix
In-Reply-To: <006901c5676d$e2a15fd0$0540210a@www.domain>
References: <D86FDEC0CB88E54391764DDF2169CC9E03360D87@afwpm005.intradit.net>
	<006901c5676d$e2a15fd0$0540210a@www.domain>
Message-ID: <0809fac5fccda1354c2f9498d9b47e6b@soc.soton.ac.uk>


On Jun 2, 2005, at 01:23 pm, Dimitris Rizopoulos wrote:

> try this:

> h <- rep(3:2, c(4, 3))
> ############
> n  <- 10
> matrix(rep(h, n),  nrow = n, byrow = TRUE)
>


Hi

outer() also works:


 > h <-rep(c(3,2),c(4,3))
 > outer(rep(1,8),h)
      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    3    3    3    3    2    2    2
[2,]    3    3    3    3    2    2    2
[3,]    3    3    3    3    2    2    2
[4,]    3    3    3    3    2    2    2
[5,]    3    3    3    3    2    2    2
[6,]    3    3    3    3    2    2    2
[7,]    3    3    3    3    2    2    2
[8,]    3    3    3    3    2    2    2
 >

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From amberfer at ull.es  Thu Jun  2 14:45:19 2005
From: amberfer at ull.es (amberfer@ull.es)
Date: Thu,  2 Jun 2005 13:45:19 +0100
Subject: [R] post hoc kruskal wallis
Message-ID: <1117716319.429eff5f497b8@correoweb.ccti.ull.es>



Hola, 
Tengo un problema con el post hoc kruskal wallis. No encuentro en el SSPS 
ning??n test estad??stico para el post hoc. En el libro de "Zar" proponen el 
Turkey-type nonparametric multicomparison. Mi problema es que tengo muchos 
datos y hacerlo a mano es complicado. El R es un programa que conozco menos, no 
se si habr?? alguna prueba que pueda servir. Los test del paquete npmc no se si 
sirven para el post hoc ni tampoco entoy seguro que el test NDWD del paquete 
coin ser??a adecuado.
Un saludo y gracias por su ayuda
Alfredo Berm??dez



From mathias.furevik at gmail.com  Thu Jun  2 15:19:12 2005
From: mathias.furevik at gmail.com (=?ISO-8859-1?Q?Mathias_Hunsk=E5r_Furevik?=)
Date: Thu, 2 Jun 2005 15:19:12 +0200
Subject: [R] plot/lm/abline
Message-ID: <1762952c05060206196df5dc7d@mail.gmail.com>

Hi,

when I run

> plot.default(z1, z2, xlab = "x", ylab = "y", main = "xxxx", pch = "+")
> abline(lm(z1 ~ z2))

then the plot is plotted perfectly (scatterplot), however, the lm()
function doesnt appear on the plot. What could be wrong?

(Yesterday it worked perfectly, with the lm() line.)

Running R 2 on OS X.

Mathias Hunsk??r Furevik
Norway



From ggrothendieck at gmail.com  Thu Jun  2 15:21:35 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 2 Jun 2005 09:21:35 -0400
Subject: [R] How to change all name of variables
In-Reply-To: <429EE7BC.4050706@gmail.com>
References: <429EE7BC.4050706@gmail.com>
Message-ID: <971536df050602062113f40b61@mail.gmail.com>

On 6/2/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Dear R-helpers,
> First I apologize if my question is quite simple
> I have a large datasets which more 100 variables.
> For a research I need to change all name of variables with add one or
> more letters on each variables.
> For example,
>  > data(Pima.tr)
>  > Pima.tr[1:5,]
>   npreg glu bp skin  bmi   ped age type
> 1     5  86 68   28 30.2 0.364  24   No
> 2     7 195 70   33 25.1 0.163  55  Yes
> 3     5  77 82   41 35.8 0.156  35   No
> 4     0 165 76   43 47.9 0.259  26   No
> 5     0 107 60   25 26.4 0.133  23   No
>  >
>  > dimnames(Pima.tr)[[2]]
> [1] "npreg" "glu"   "bp"    "skin"  "bmi"   "ped"   "age"   "type"
>  >
> 
> I need to change the variables name ,
> "npreg" "glu" "bp" "skin" "bmi" "ped" "age" "type"
> with
> "xyz.npreg" "xyz.glu" "xyz.bp" "xyz.skin" "xyz.bmi" "xyz.ped" "xyz.age"
> "xyz.type"
> 
> How can I make this (automatically). I don't want to make manual with
> more 100 variables.

Try this:

names(prima) <- paste("xyz", names(prima), sep = ".")



From sarah.goslee at gmail.com  Thu Jun  2 15:31:32 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 2 Jun 2005 09:31:32 -0400
Subject: [R] plot/lm/abline
In-Reply-To: <1762952c05060206196df5dc7d@mail.gmail.com>
References: <1762952c05060206196df5dc7d@mail.gmail.com>
Message-ID: <efb536d505060206311709a7aa@mail.gmail.com>

On 6/2/05, Mathias Hunsk??r Furevik <mathias.furevik at gmail.com> wrote:
> Hi,
> 
> when I run
> 
> > plot.default(z1, z2, xlab = "x", ylab = "y", main = "xxxx", pch = "+")
> > abline(lm(z1 ~ z2))

I think you have the independent and dependent variables switched:

abline(lm(z2 ~ z1))

Sarah
-- 
Sarah Goslee
http://www.stringpage.com



From canty at math.mcmaster.ca  Thu Jun  2 15:34:44 2005
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 2 Jun 2005 09:34:44 -0400 (EDT)
Subject: [R] confusion with boot
In-Reply-To: <s29e3314.055@grecc.umaryland.edu>
Message-ID: <Pine.LNX.4.44.0506020930440.3521-100000@mathserv.math.mcmaster.ca>

Your function never uses the argument w.  This argument is used to define 
the bootstrap sample as described in the help for boot.  You have choices 
about how to do this for linear models.  If you want to do case resampling
then you function should be

detefun3<-function (d,i) {
  d <- d[i,]                                                     
  summary(lm(d$sg120~d$fg120adj,data=d))$r.square   
}

Why you would want to bootstrap R-squared or whether that is a good idea, 
I am not so sure.

Angelo

On Wed, 1 Jun 2005, John Sorkin wrote:

> I think I am doing something wrong when I try to bootstrap R square
> obtained from lm. My code is included below. No matter how many times I
> run the simulation, I always get exactly the same result, the bias and
> std.error are always zero. I would think that these values should be
> non-zero. I would appreciate any suggestions as to what I am doing
> wrong, or perhaps what I fail to understand.
> R 2.1.0 Patched Win 2k.
> Thanks,
> John
>  
>  
> >detefun3<-function (d,w)
> summary(lm(d$sg120~d$fg120adj,data=d))$r.square
> > boot(delete,deletefun3,R=200)
>  
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> Call:
> boot(data = delete, statistic = deletefun3, R = 200)
> 
> Bootstrap Statistics :
>      original  bias    std. error
> t1* 0.3028048       0           0
> > 
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From murdoch at stats.uwo.ca  Thu Jun  2 15:34:04 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 09:34:04 -0400
Subject: [R] plot/lm/abline
In-Reply-To: <1762952c05060206196df5dc7d@mail.gmail.com>
References: <1762952c05060206196df5dc7d@mail.gmail.com>
Message-ID: <429F0ACC.9030300@stats.uwo.ca>

Mathias Hunsk??r Furevik wrote:
> Hi,
> 
> when I run
> 
> 
>>plot.default(z1, z2, xlab = "x", ylab = "y", main = "xxxx", pch = "+")
>>abline(lm(z1 ~ z2))
> 
> 
> then the plot is plotted perfectly (scatterplot), however, the lm()
> function doesnt appear on the plot. What could be wrong?
> 
> (Yesterday it worked perfectly, with the lm() line.)

You've reversed the order of variables.  Try

abline(lm(z2 ~ z1))

Plot takes variables in the order x, y.

lm takes variables in the order y ~ x.

Duncan Murdoch



From mi2kelgrum at yahoo.com  Thu Jun  2 15:40:09 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Thu, 2 Jun 2005 06:40:09 -0700 (PDT)
Subject: [R] plot/lm/abline
Message-ID: <20050602134009.78166.qmail@web60223.mail.yahoo.com>

Hi Mathias,

Try

abline(lm(z2 ~ z1))

Mikkel

......................................

Hi,

when I run

> plot.default(z1, z2, xlab = "x", ylab = "y", main =
"xxxx", pch = "+")
> abline(lm(z1 ~ z2))

then the plot is plotted perfectly (scatterplot),
however, the lm()
function doesnt appear on the plot. What could be
wrong?

(Yesterday it worked perfectly, with the lm() line.)

Running R 2 on OS X.

Mathias Hunsk??r Furevik
Norway



From abunn at whrc.org  Thu Jun  2 15:43:05 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 2 Jun 2005 09:43:05 -0400
Subject: [R] Preprocessing troublesome files in R - looking for some perl
	like functionality
In-Reply-To: <1762952c05060206196df5dc7d@mail.gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIGEOODGAA.abunn@whrc.org>

Hi all:

I have acquired a 100s of data files that I need to preprocess to get them
usable in R. The files are fixed width (to a point) and contain 1 to 3 lines
of header, followed by a variable number of fixed width data lines (that I
can read with read.fwf). I want to read through the files and remove every
_line_ where characters column 83-86 do not equal "STD". If I can do that
and store it in a text file, then I can get the data I need using read.fwf.
I can't figure out how to do this because of the irregularity of the header
info buried in the file. It seems like the kind of thing perl or emacs would
be good at but I'd like to do it all in R if possible. Any pointers
appreciated.

-Andy

R > version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This is a snippet of one of the data files:

929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
RAW
RUSS061830 568  11122  1 806  1 843  2 862  3 902  31244  3 986  31210
31074  3  RAW
RUSS0618401369  4 937  41154  4 869  4 702  4 716  4 972  4 682  5 878  5
582  5  RAW
929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
STD
RUSS061830 568  11122  1 806  1 843  2 862  3 902  31244  3 986  31210
31074  3  STD
RUSS0618401369  4 937  41154  4 869  4 702  4 716  4 972  4 682  5 878  5
582  5  STD
RUSS0619701158 26 906 26 954 26 746 26 629 26 858 261268 261345 261102
261298 26  STD
RUSS061980 483 26 780 26 995 261273 261391 26 996 261621 26 878 261418 26
514 26  STD
RUSS0619901071 269990  09990  09990  09990  09990  09990  09990  09990
09990  0  STD
929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
RES
RUSS061830 604  11215  1 889  1 828  2 909  3 982  31294  3 947  31091
31030  3  RES
RUSS0618401290  4 858  41057  4 917  4 712  4 824  41077  4 709  5 911  5
747  5  RES
RUSS061850 873  5 994  51179  71040  71028  7 923  71120  7 846 101146 11
854 13  RES
RUSS0618601609 141209 16 780 16 758 171238 171191 17 858 17 903 17 930 18
334 18  RES
929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
ARS
RUSS061850 873  5 994  51179  71040  71028  7 923  71120  7 846 101146 11
854 13  ARS
RUSS0618601609 141209 16 780 16 758 171238 171191 17 858 17 903 17 930 18
334 18  ARS



From rolf at math.unb.ca  Thu Jun  2 16:14:38 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 2 Jun 2005 11:14:38 -0300 (ADT)
Subject: [R] Caution on the use of model.matrix.
Message-ID: <200506021414.j52EEck7006242@erdos.math.unb.ca>

I have just been bitten by a quirk in the behaviour of model.matrix.
I used model.matrix inside a function, and passed to it a formula
that was built elsewhere.

The formula was of the form ``y ~ x + w + z''.  Now, model.matrix
cheerfully accepts formulae of this form, although it only
***needs*** the right hand side, i.e. ``~ x + w + z'' --- the ``y''
can be dropped (but in general needn't be).

The quirk by which I was bitten was that if the y column of the data
frame being used contains missing values, then the corresponding rows
are dropped (silently) and the resulting design matrix has rows
corresponding only to the non-missing values of y.  This was not the
desired behaviour in my application.

Might I respectfully suggest to R Core that a WARNING be added to the
help for model.matrix to the effect that

		model.matrix(y~x + w + z,XXX)
and
		model.matrix(~x + w + z,XXX)

give DIFFERENT results if the column ``y'' of the data frame XXX
contains missing values?

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From bates at stat.wisc.edu  Thu Jun  2 16:20:33 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 02 Jun 2005 09:20:33 -0500
Subject: [R] showMethods doubt
In-Reply-To: <429EDA3F.5040402@ipimar.pt>
References: <429EDA3F.5040402@ipimar.pt>
Message-ID: <429F15B1.3090305@stat.wisc.edu>

Ernesto Jardim wrote:
> Hi,
> 
> I'm developing in S4 and I wanted to see the methods for a specific
> class using showMethods but I didn't succed. Can someone help ? See the
> example below.
> 
> setClass("myclass",
>     representation(
>         name    ="character"
>     )
> )
> 
> setGeneric("mymeth", function(obj, ...){
>     standardGeneric("mymeth")
>     }
> )
> 
> setMethod("mymeth", signature("myclass"), function(obj){
>     print(paste("Hi,", obj at name, "and I don't know what to do with
> showMethods !"))
>     }
> )
> 
> Now if you do:
> 
>> myobj <- new("myclass", name="my name is joe")
>> mymeth(myobj)
> [1] "Hi, my name is joe and I don't know what to do with showMethods !"
> 
> So the method is working, let's use showMethods:
> 
>> showMethods(classes="myclass")
> 
> Function "addNextMethod":
> <Empty Methods List>
> 
> Function "body<-":
> <Empty Methods List>
> 
> Function "coerce":
> <Empty Methods List>
> 
> Function "initialize":
> .Object = "myclass"
>     (inherited from .Object = "ANY")
> 
> Function "loadMethod":
> <Empty Methods List>
> 
> Function "show":
> object = "myclass"
>     (inherited from object = "ANY")
> NULL
> 
> mymeth does not show on that list, why ?

Try

showMethods(classes = "myclass", where = .GlobalEnv)

I believe the convention is that unless you give showMethods a generic
function as argument `f' it will look for the generics in "package:methods".



From p.dalgaard at biostat.ku.dk  Thu Jun  2 16:22:31 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jun 2005 16:22:31 +0200
Subject: [R] Preprocessing troublesome files in R - looking for some perl
	like functionality
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIGEOODGAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIGEOODGAA.abunn@whrc.org>
Message-ID: <x2ll5sn8ig.fsf@turmalin.kubism.ku.dk>

"Andy Bunn" <abunn at whrc.org> writes:

> Hi all:
> 
> I have acquired a 100s of data files that I need to preprocess to get them
> usable in R. The files are fixed width (to a point) and contain 1 to 3 lines
> of header, followed by a variable number of fixed width data lines (that I
> can read with read.fwf). I want to read through the files and remove every
> _line_ where characters column 83-86 do not equal "STD". If I can do that
> and store it in a text file, then I can get the data I need using read.fwf.
> I can't figure out how to do this because of the irregularity of the header
> info buried in the file. It seems like the kind of thing perl or emacs would
> be good at but I'd like to do it all in R if possible. Any pointers
> appreciated.

How large are the files? With today's RAM sizes, it could be feasible
to do something along the lines of

1) x <- readLines(....), i <- read.fwf(...col83-86...)
2) read.fwf(textConnection(x[I %in% "STD"]),......)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Thu Jun  2 16:28:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 10:28:14 -0400
Subject: [R] Preprocessing troublesome files in R - looking for some perl
	like functionality
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIGEOODGAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIGEOODGAA.abunn@whrc.org>
Message-ID: <429F177E.4060602@stats.uwo.ca>

Andy Bunn wrote:
> Hi all:
> 
> I have acquired a 100s of data files that I need to preprocess to get them
> usable in R. The files are fixed width (to a point) and contain 1 to 3 lines
> of header, followed by a variable number of fixed width data lines (that I
> can read with read.fwf). I want to read through the files and remove every
> _line_ where characters column 83-86 do not equal "STD". If I can do that
> and store it in a text file, then I can get the data I need using read.fwf.
> I can't figure out how to do this because of the irregularity of the header
> info buried in the file. It seems like the kind of thing perl or emacs would
> be good at but I'd like to do it all in R if possible. Any pointers
> appreciated.

Seems to me a couple of passes through read.fwf might work.  On the 
first pass, define one column running from columns 1 to 82, another from 
83 to 86, another from 87 to the longest possible line width.  All 
columns to be class "character".  Read using this format, select based 
on the 2nd column, and write out the selected lines -- or use the result 
as input to a textConnection.

Duncan Murdoch
> 
> -Andy
> 
> R > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> This is a snippet of one of the data files:
> 
> 929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
> RAW
> RUSS061830 568  11122  1 806  1 843  2 862  3 902  31244  3 986  31210
> 31074  3  RAW
> RUSS0618401369  4 937  41154  4 869  4 702  4 716  4 972  4 682  5 878  5
> 582  5  RAW
> 929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
> STD
> RUSS061830 568  11122  1 806  1 843  2 862  3 902  31244  3 986  31210
> 31074  3  STD
> RUSS0618401369  4 937  41154  4 869  4 702  4 716  4 972  4 682  5 878  5
> 582  5  STD
> RUSS0619701158 26 906 26 954 26 746 26 629 26 858 261268 261345 261102
> 261298 26  STD
> RUSS061980 483 26 780 26 995 261273 261391 26 996 261621 26 878 261418 26
> 514 26  STD
> RUSS0619901071 269990  09990  09990  09990  09990  09990  09990  09990
> 09990  0  STD
> 929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
> RES
> RUSS061830 604  11215  1 889  1 828  2 909  3 982  31294  3 947  31091
> 31030  3  RES
> RUSS0618401290  4 858  41057  4 917  4 712  4 824  41077  4 709  5 911  5
> 747  5  RES
> RUSS061850 873  5 994  51179  71040  71028  7 923  71120  7 846 101146 11
> 854 13  RES
> RUSS0618601609 141209 16 780 16 758 171238 171191 17 858 17 903 17 930 18
> 334 18  RES
> 929    2 Russia   Dahurian larch 150  6946-11249 1830 1990 -
> ARS
> RUSS061850 873  5 994  51179  71040  71028  7 923  71120  7 846 101146 11
> 854 13  ARS
> RUSS0618601609 141209 16 780 16 758 171238 171191 17 858 17 903 17 930 18
> 334 18  ARS
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From h.wickham at gmail.com  Thu Jun  2 16:33:08 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 2 Jun 2005 09:33:08 -0500
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <20050602084149.GC4564@boing.buug.de>
References: <20050602084149.GC4564@boing.buug.de>
Message-ID: <f8e6ff0505060207336c7a50de@mail.gmail.com>

> cache results.  Other languages usually offer a dictionary data type
> which I can use as an efficient way to dynamically cache already
> calculated results - what's the best way to do this in R?

It really depends on what sort of data you want to cache - if it is
fundamentally 1D, use a vector, 2D use a matrix etc.  The closest
equivalent to dictionary/hashmap is a list - however R's pass a copy
semantics might slow things down if you are caching a lot of data.

Hadley Wickham
http://had.co.nz



From dimitris.rizopoulos at med.kuleuven.be  Thu Jun  2 16:36:54 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 2 Jun 2005 16:36:54 +0200
Subject: [R] Caution on the use of model.matrix.
References: <200506021414.j52EEck7006242@erdos.math.unb.ca>
Message-ID: <00a901c56780$85bb0bf0$0540210a@www.domain>

I think you could bit that using (inside your function) something like 
this

old.o <- options(na.action = na.fail)
# old.o <- options(na.action = na.pass)
on.exit(old.o)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Rolf Turner" <rolf at math.unb.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 02, 2005 4:14 PM
Subject: [R] Caution on the use of model.matrix.


>I have just been bitten by a quirk in the behaviour of model.matrix.
> I used model.matrix inside a function, and passed to it a formula
> that was built elsewhere.
>
> The formula was of the form ``y ~ x + w + z''.  Now, model.matrix
> cheerfully accepts formulae of this form, although it only
> ***needs*** the right hand side, i.e. ``~ x + w + z'' --- the ``y''
> can be dropped (but in general needn't be).
>
> The quirk by which I was bitten was that if the y column of the data
> frame being used contains missing values, then the corresponding 
> rows
> are dropped (silently) and the resulting design matrix has rows
> corresponding only to the non-missing values of y.  This was not the
> desired behaviour in my application.
>
> Might I respectfully suggest to R Core that a WARNING be added to 
> the
> help for model.matrix to the effect that
>
> model.matrix(y~x + w + z,XXX)
> and
> model.matrix(~x + w + z,XXX)
>
> give DIFFERENT results if the column ``y'' of the data frame XXX
> contains missing values?
>
> cheers,
>
> Rolf Turner
> rolf at math.unb.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Jun  2 16:38:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Jun 2005 15:38:43 +0100 (BST)
Subject: [R] Preprocessing troublesome files in R - looking for some perl
	like functionality
In-Reply-To: <x2ll5sn8ig.fsf@turmalin.kubism.ku.dk>
References: <NEBBIPHDAMMOKDKPOFFIGEOODGAA.abunn@whrc.org>
	<x2ll5sn8ig.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0506021529410.15737@gannet.stats>

On Thu, 2 Jun 2005, Peter Dalgaard wrote:

> "Andy Bunn" <abunn at whrc.org> writes:
>
>> Hi all:
>>
>> I have acquired a 100s of data files that I need to preprocess to get them
>> usable in R. The files are fixed width (to a point) and contain 1 to 3 lines
>> of header, followed by a variable number of fixed width data lines (that I
>> can read with read.fwf). I want to read through the files and remove every
>> _line_ where characters column 83-86 do not equal "STD". If I can do that
>> and store it in a text file, then I can get the data I need using read.fwf.
>> I can't figure out how to do this because of the irregularity of the header
>> info buried in the file. It seems like the kind of thing perl or emacs would
>> be good at but I'd like to do it all in R if possible. Any pointers
>> appreciated.
>
> How large are the files? With today's RAM sizes, it could be feasible
> to do something along the lines of
>
> 1) x <- readLines(....), i <- read.fwf(...col83-86...)
> 2) read.fwf(textConnection(x[I %in% "STD"]),......)

or use a file() (no file= argument) connection, which will be faster for 
large files (and read.fwf should probably use internally).

I would have used

x <- readLines(...)
tmp <- file()
writeLines(x[substr(x, 83, 86) == "STD"], tmp)
read.fwf(tmp, ...)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Jun  2 16:39:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Jun 2005 10:39:43 -0400
Subject: [R] Dynamic Dictionary Data Type?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E908@usctmx1106.merck.com>

AFAIK R does copy-on-modify, so if you call f(aList, ...) and inside f()
aList is not modified, then a copy is not really made.

Andy

> From: hadley wickham
> 
> > cache results.  Other languages usually offer a dictionary data type
> > which I can use as an efficient way to dynamically cache already
> > calculated results - what's the best way to do this in R?
> 
> It really depends on what sort of data you want to cache - if it is
> fundamentally 1D, use a vector, 2D use a matrix etc.  The closest
> equivalent to dictionary/hashmap is a list - however R's pass a copy
> semantics might slow things down if you are caching a lot of data.
> 
> Hadley Wickham
> http://had.co.nz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From tlumley at u.washington.edu  Thu Jun  2 16:44:21 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 2 Jun 2005 07:44:21 -0700 (PDT)
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <f8e6ff0505060207336c7a50de@mail.gmail.com>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>

On Thu, 2 Jun 2005, hadley wickham wrote:

>> cache results.  Other languages usually offer a dictionary data type
>> which I can use as an efficient way to dynamically cache already
>> calculated results - what's the best way to do this in R?
>
> It really depends on what sort of data you want to cache - if it is
> fundamentally 1D, use a vector, 2D use a matrix etc.  The closest
> equivalent to dictionary/hashmap is a list - however R's pass a copy
> semantics might slow things down if you are caching a lot of data.
>

An environment is a hash table, and copying occurs only on modification, 
when any language would have to copy in this context.

 	-thomas



From ripley at stats.ox.ac.uk  Thu Jun  2 16:55:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Jun 2005 15:55:46 +0100 (BST)
Subject: [R] Errors in Variables
In-Reply-To: <20050529215610.JINS25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <20050529215610.JINS25800.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.61.0506021522001.15500@gannet.stats>

On Sun, 29 May 2005, John Fox wrote:

> Dear Spencer,
>
>> -----Original Message-----
>> From: Spencer Graves [mailto:spencer.graves at pdf.com]
>> Sent: Sunday, May 29, 2005 4:13 PM
>> To: John Fox
>> Cc: r-help at stat.math.ethz.ch; 'Jacob van Wyk'; 'Eric-Olivier Le Bigot'
>> Subject: Re: [R] Errors in Variables
>>
>> Hi, John:
>>
>> 	  Thanks for the clarification.  I know that the
>> "errors in X problem"
>> requires additional information, most commonly one of the
>> variances or the correlation.  The question I saw (below)
>> indicated he had tried "model of the form y ~ x (with a given
>> covariance matrix ...)", which made me think of "sem".
>>
>> 	  If he wants "the least (orthogonal) distance", could
>> he could get it indirectly from "sem" by calling "sem"
>> repeatedly giving, say, a variance for "x", then averaging
>> the variances of "x" and "y" and trying that in "sem"?
>>
>
> I'm not sure how that would work, but seems similar to averaging the
> regressions of y on x and x on y.
>
>> 	  Also, what do you know about "ODRpack"?  It looks
>> like that might solve "the least (orthogonal) distance".
>>
>
> I'm not familiar with ODRpack, but it seems to me that one could fairly
> simply minimize the sum of squared least distances using, e.g., optim.

Exactly.  In fact this is easily reduced to a function of one variable 
(the slope, known to lie between the y or x and x om y regressions) and so 
optimize() would be more appropriate.  I did do that in S once upon a long 
time, but it seemed too esoteric to package up (and it would take me 
longer to find the code than to do it again).

My paper quoted originally deals with the case of known variances for each 
of x and y (and heteroscedasticity in both).  It was written for chemists, 
and contains all the formulae one needs.  In their applications knowing 
(at least approximately) the variances is a reasonable assumption.

Brian


>> 	  Thanks again for your note, John.
>> 	  Best Wishes,
>> 	  Spencer Graves
>>
>> John Fox wrote:
>>
>>> Dear Spencer,
>>>
>>> The reason that I didn't respond to the original posting (I'm the
>>> author of the sem package), that that without additional
>> information
>>> (such as the error variance of x), a model with error in
>> both x and y
>>> will be underidentified (unless there are multiple indicators of x,
>>> which didn't seem to be the case here). I figured that what
>> Jacob had
>>> in mind was something like minimizing the least
>> (orthogonal) distance
>>> of the points to the regression line (implying by the way
>> that x and y
>>> are on the same scale or somehow standardized), which isn't
>> doable with sem as far as I'm aware.
>>>
>>> Regards,
>>>  John
>>>
>>> --------------------------------
>>> John Fox
>>> Department of Sociology
>>> McMaster University
>>> Hamilton, Ontario
>>> Canada L8S 4M4
>>> 905-525-9140x23604
>>> http://socserv.mcmaster.ca/jfox
>>> --------------------------------
>>>
>>>
>>>> -----Original Message-----
>>>> From: r-help-bounces at stat.math.ethz.ch
>>>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Spencer Graves
>>>> Sent: Saturday, May 28, 2005 4:47 PM
>>>> To: Eric-Olivier Le Bigot
>>>> Cc: r-help at stat.math.ethz.ch; Jacob van Wyk
>>>> Subject: Re: [R] Errors in Variables
>>>>
>>>> 	  I'm sorry, I have not followed this thread, but I
>> wonder if you
>>>> have considered library(sem), "structural equations modeling"?
>>>> "Errors in variables" problems are the canonical special case.
>>>>
>>>> 	  Also, have you done a search of "www.r-project.org"
>>>> -> search -> "R site search" for terms like "errors in
>>>> variables regression"?  This just led me to "ODRpack",
>> which is NOT a
>>>> CRAN package but is apparently available after a Google
>> search.  If it
>>>> were my problem, I'd first try to figure out "sem";  if that seemed
>>>> too difficult, I might then look at "ODRpack".
>>>>
>>>> 	  Also, have you read the posting guide!
>>>> http://www.R-project.org/posting-guide.html?  This suggests, among
>>>> other things, that you provide a toy example that a potential
>>>> respondant could easily copy from your email, test a few
>>>> modifications, and prase a reply in a minute or so.
>>>> This also helps clarify your question so any respondants are more
>>>> likely to suggest something that is actually useful to you.
>>  Moreover,
>>>> many people have reported that they were able to answer their own
>>>> question in the course of preparing a question for this
>> list using the
>>>> posting guide.
>>>>
>>>> 	  hope this helps.  spencer graves
>>>>
>>>> Eric-Olivier Le Bigot wrote:
>>>>
>>>>
>>>>> I'm interested in this "2D line fitting" too!  I've been looking,
>>>>> without success, in the list of R packages.
>>>>>
>>>>> It might be possible to implement quite easily some of the
>>>>
>>>> formalism
>>>>
>>>>> that you can find in Numerical Recipes (Fortran 77, 2nd ed.),
>>>>> paragraph 15.3.  As a matter of fact, I did this in R but
>>>>
>>>> only for a
>>>>
>>>>> model of the form y ~ x (with a given covariance matrix
>>>>
>>>> between x and
>>>>
>>>>> y).  I can send you the R code (preliminary version: I
>>>>
>>>> wrote it yesterday), if you want.
>>>>
>>>>> Another interesting reference might be Am. J. Phys. 60, p.
>>>>
>>>> 66 (1992).
>>>>
>>>>> But, again, you would have to implement things by yourself.
>>>>>
>>>>> All the best,
>>>>>
>>>>> EOL
>>>>>
>>>>> --
>>>>> Dr. Eric-Olivier LE BIGOT (EOL)                CNRS
>>>>
>>>> Associate Researcher
>>>>
>>>> ~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>>> ~~~~o~o~~~
>>>>
>>>>> Kastler Brossel Laboratory (LKB)
>>>>
>>>> http://www.lkb.ens.fr
>>>>
>>>>> Universit? P. & M. Curie and Ecole Normale Sup?rieure, Case 74
>>>>> 4 place Jussieu              75252 Paris CEDEX 05
>>>>
>>>>      France
>>>>
>>>> ~~~o~o~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>>> ~~~~o~o~~~
>>>>
>>>>> office  : 01 44 27 73 67                             fax:
>>>>
>>>> 01 44 27 38 45
>>>>
>>>>> ECR room: 01 44 27 47 12                      x-ray room:
>>>>
>>>> 01 44 27 63 00
>>>>
>>>>> home: 01 73 74 61 87      For int'l calls: 33 + number
>>>>
>>>> without leading 0
>>>>
>>>>>
>>>>> On Wed, 25 May 2005, Jacob van Wyk wrote:
>>>>>
>>>>>
>>>>>> I hope somebody can help.
>>>>>> A student of mine is doing a study on Measurement Error models
>>>>>> (errors-in-variables, total least squares, etc.). I have an old
>>>>>> reference to a "multi archive"  that contains
>>>>>> leiv3: Programs for best line fitting with errors in both
>>>>
>>>> coordinates.
>>>>
>>>>>> (The date is October 1989, by B.D. Ripley et al.) I have done a
>>>>>> search for something similar in R withour success. Has this been
>>>>>> implemented in a R-package, possibly under some sort of
>>>>
>>>> assumptions
>>>>
>>>>>> about variances. I would lke my student to apply some regression
>>>>>> techniques to data that fit this profile.
>>>>>> Any help is much appreciated.
>>>>>> (If I have not done my search more carefully - my
>>>>
>>>> apologies.) Thanks
>>>>
>>>>>> Jacob
>>>>>>
>>>>>>
>>>>>> Jacob L van Wyk
>>>>>> Department of Mathematics and Statistics University of
>>>>
>>>> Johannesburg
>>>>
>>>>>> APK P O Box 524 Auckland Park 2006 South Africa
>>>>>> Tel: +27-11-489-3080
>>>>>> Fax: +27-11-489-2832
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at stat.math.ethz.ch mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide!
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>
>>>>>
>>>>>
>>>> ------------------------------------------------------------
>> ----------
>>>>
>>>>> --
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!
>>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Thu Jun  2 17:07:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Jun 2005 16:07:07 +0100 (BST)
Subject: [R] Caution on the use of model.matrix.
In-Reply-To: <00a901c56780$85bb0bf0$0540210a@www.domain>
References: <200506021414.j52EEck7006242@erdos.math.unb.ca>
	<00a901c56780$85bb0bf0$0540210a@www.domain>
Message-ID: <Pine.LNX.4.61.0506021602580.16022@gannet.stats>

On Thu, 2 Jun 2005, Dimitris Rizopoulos wrote:

> I think you could bit that using (inside your function) something like this
>
> old.o <- options(na.action = na.fail)
> # old.o <- options(na.action = na.pass)
> on.exit(old.o)

You can.  But the real problem is more likely that Rolf has not passed 
model.matrix a model frame, so it calls model.frame() internally.  The 
help page is a bit confused in that it says

     data: a data frame created with 'model.frame'.

which the default for the argument is not.  So a better solution would 
then be to call model.frame and pass a model frame to model.matrix.

delete.response() might also be useful.

The suggested warning only applies if `data' is not supplied.

> ----- Original Message ----- From: "Rolf Turner" <rolf at math.unb.ca>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, June 02, 2005 4:14 PM
> Subject: [R] Caution on the use of model.matrix.
>
>
>> I have just been bitten by a quirk in the behaviour of model.matrix.
>> I used model.matrix inside a function, and passed to it a formula
>> that was built elsewhere.
>> 
>> The formula was of the form ``y ~ x + w + z''.  Now, model.matrix
>> cheerfully accepts formulae of this form, although it only
>> ***needs*** the right hand side, i.e. ``~ x + w + z'' --- the ``y''
>> can be dropped (but in general needn't be).
>> 
>> The quirk by which I was bitten was that if the y column of the data
>> frame being used contains missing values, then the corresponding rows
>> are dropped (silently) and the resulting design matrix has rows
>> corresponding only to the non-missing values of y.  This was not the
>> desired behaviour in my application.
>> 
>> Might I respectfully suggest to R Core that a WARNING be added to the
>> help for model.matrix to the effect that
>> 
>> model.matrix(y~x + w + z,XXX)
>> and
>> model.matrix(~x + w + z,XXX)
>> 
>> give DIFFERENT results if the column ``y'' of the data frame XXX
>> contains missing values?
>> 
>> cheers,
>> 
>> Rolf Turner
>> rolf at math.unb.ca
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From h.wickham at gmail.com  Thu Jun  2 17:09:07 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 2 Jun 2005 10:09:07 -0500
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
Message-ID: <f8e6ff05050602080953b85dd1@mail.gmail.com>

> An environment is a hash table, and copying occurs only on modification,
> when any language would have to copy in this context.

Yes, I'm aware that copying only occurs on modification.  However, it
was my understanding that

a <- list(a =1)
a$b <- 4

would create a new copy of a, whereas in Java say

HashMap a = new HashMap();
a.put("a", 1);
a.put("b", 2);

wouldn't create a copy of a. (please bear in mind my java syntax is very rusty!)

Caching data implies updating at each step, thus possibly creating n
copies of the list.  Is that wrong?

Hadley



From subianto at gmail.com  Thu Jun  2 17:22:32 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Thu, 02 Jun 2005 17:22:32 +0200
Subject: [R] How to change all name of variables
In-Reply-To: <429EE7BC.4050706@gmail.com>
References: <429EE7BC.4050706@gmail.com>
Message-ID: <429F2438.7060607@gmail.com>

Dear all R-helpers,
Thanks you very much for your help. I would like to thanks Sean Davis 
and Gabor Grothendieck for their help.
Best wishes, Muhammad Subianto


On this day 6/2/2005 3:21 PM, Gabor Grothendieck wrote:
 >
 > Try this:
 >
 > names(prima) <- paste("xyz", names(prima), sep = ".")
 >

On this day 6/2/2005 1:20 PM, Sean Davis wrote:
 > See ?paste.
 >
 > Something like below if you have 100 column names:
 >
 > dimnames(pima.tr)[[2]] <-
 > paste(rep('xyz',100),dimnames(pima.tr)[[2]],sep=".")
 >
 > You probably want to test the paste statement before setting the
 > dimnames, or operate on a copy of the data until you get the hang of
 > using paste.
 >
 > Sean


On this day 6/2/2005 1:04 PM, Muhammad Subianto wrote:
> Dear R-helpers,
> First I apologize if my question is quite simple
> I have a large datasets which more 100 variables.
> For a research I need to change all name of variables with add one or
> more letters on each variables.
> For example,
>  > data(Pima.tr)
>  > Pima.tr[1:5,]
>   npreg glu bp skin  bmi   ped age type
> 1     5  86 68   28 30.2 0.364  24   No
> 2     7 195 70   33 25.1 0.163  55  Yes
> 3     5  77 82   41 35.8 0.156  35   No
> 4     0 165 76   43 47.9 0.259  26   No
> 5     0 107 60   25 26.4 0.133  23   No
>  >
>  > dimnames(Pima.tr)[[2]]
> [1] "npreg" "glu"   "bp"    "skin"  "bmi"   "ped"   "age"   "type"
>  >
> 
> I need to change the variables name ,
> "npreg" "glu" "bp" "skin" "bmi" "ped" "age" "type"
> with
> "xyz.npreg" "xyz.glu" "xyz.bp" "xyz.skin" "xyz.bmi" "xyz.ped" "xyz.age" 
> "xyz.type"
> 
> How can I make this (automatically). I don't want to make manual with 
> more 100 variables.
> I  would be very happy if anyone could help me.
> Thank you for your time.
> Kindly regards, Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Thu Jun  2 17:53:11 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 2 Jun 2005 08:53:11 -0700 (PDT)
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <f8e6ff05050602080953b85dd1@mail.gmail.com>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0506020849470.251626@homer07.u.washington.edu>

On Thu, 2 Jun 2005, hadley wickham wrote:

>> An environment is a hash table, and copying occurs only on modification,
>> when any language would have to copy in this context.
>
> Yes, I'm aware that copying only occurs on modification.  However, it
> was my understanding that
>
> a <- list(a =1)
> a$b <- 4
>
> would create a new copy of a, whereas in Java say
>

It depends, but I was talking about *environments*, not lists, and

a<-new.env()
assign("b",4,a)
assign("c", 42, a)
etc

wouldn't copy anything.  Bill Venables had an article about environments 
for caching previously computed results in an early issue of R News.


 	-thomas



From ripley at stats.ox.ac.uk  Thu Jun  2 18:12:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Jun 2005 17:12:24 +0100 (BST)
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <f8e6ff05050602080953b85dd1@mail.gmail.com>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506021632510.17584@gannet.stats>

On Thu, 2 Jun 2005, hadley wickham wrote:

>> An environment is a hash table, and copying occurs only on modification,
>> when any language would have to copy in this context.

Caveat: the default is new.env(hash=FALSE), so an environment is a hash 
table in one sense but not necessarily so in the strict sense.

> Yes, I'm aware that copying only occurs on modification.  However, it
> was my understanding that
>
> a <- list(a =1)
> a$b <- 4
>
> would create a new copy of a,

Thomas was talking about environments, not lists (and $ works differently 
for them).

> whereas in Java say
>
> HashMap a = new HashMap();
> a.put("a", 1);
> a.put("b", 2);
>
> wouldn't create a copy of a. (please bear in mind my java syntax is very rusty!)
>
> Caching data implies updating at each step, thus possibly creating n
> copies of the list.  Is that wrong?

It depends what you mean by `copy'.  If you expand a hash table you at 
some point need to re-arrange the hashing of the entries.  That's what 
will happen with an R environment or an R list too.  The possible 
difference is that you might expect the table to have some room for 
expansion, and in your list example you did not give any.

R's operations on lists make more copies than are strictly necessary, but 
it is not clear that this is more costly than the housekeeping that would 
otherwise be necessary.  In a$b <- 4, the wrapper VECSXP is recreated and 
the pointers copied across, but the list elements are not copied.  For
a$a <- 4 it is probable that no copying is done (although if a2 <- a had 
been done previously, the pending recursive copy would then be done).
(It is also possible that I have overlooked something in the rather 
complex code used to do these subassignments.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Thu Jun  2 18:17:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 2 Jun 2005 12:17:19 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <Pine.LNX.4.61.0506021632510.17584@gannet.stats>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
Message-ID: <971536df0506020917730d77d4@mail.gmail.com>

On 6/2/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 2 Jun 2005, hadley wickham wrote:
> 
> >> An environment is a hash table, and copying occurs only on modification,
> >> when any language would have to copy in this context.
> 
> Caveat: the default is new.env(hash=FALSE), so an environment is a hash
> table in one sense but not necessarily so in the strict sense.

Can you expand on this?  When would one use hash = TRUE vs.
the default?



From murdoch at stats.uwo.ca  Thu Jun  2 19:24:05 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 13:24:05 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <971536df0506020917730d77d4@mail.gmail.com>
References: <20050602084149.GC4564@boing.buug.de>	<f8e6ff0505060207336c7a50de@mail.gmail.com>	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>	<f8e6ff05050602080953b85dd1@mail.gmail.com>	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<971536df0506020917730d77d4@mail.gmail.com>
Message-ID: <429F40B5.9030302@stats.uwo.ca>

Gabor Grothendieck wrote:
> On 6/2/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> 
>>On Thu, 2 Jun 2005, hadley wickham wrote:
>>
>>
>>>>An environment is a hash table, and copying occurs only on modification,
>>>>when any language would have to copy in this context.
>>
>>Caveat: the default is new.env(hash=FALSE), so an environment is a hash
>>table in one sense but not necessarily so in the strict sense.
> 
> 
> Can you expand on this?  When would one use hash = TRUE vs.
> the default?

It's an optimization question.  hash = TRUE uses more memory and takes 
longer to set up, but will give faster searches in big environments. 
The current default assumes that environments are small so the effort of 
building the hash table is not worthwhile, and it does linear searches.

I suspect that we might have the default wrong (or perhaps should make 
the transition from linear to hash search automatically at a certain 
threshold size), but I haven't done the testing necessary to show this.

Duncan Murdoch



From emb7 at st-andrews.ac.uk  Thu Jun  2 21:05:28 2005
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Thu, 02 Jun 2005 19:05:28 +0000
Subject: [R] nls.control: increasing number of iterations
Message-ID: <429F5878.4090103@st-and.ac.uk>

Hello,
I'm using the nls function and would like to increase the number of 
iterations. According to the documentation as well as other postings on 
R-help, I've tried to do this using the "control" argument:

nls(y ~ SSfpl(x, A, B, xmid, scal), data=my.data, 
control=nls.control(maxiter=200))

but no matter how much I increase "maxiter", I get the following error 
message:

Error in nls(y ~ cbind(1, 1/(1 + exp((xmid - x)/exp(lscal)))), data = 
xy,  :
        number of iterations exceeded maximum of 50

The second line here suggests that the maximum number of iterations is 
still 50, despite changing the "control" argument. Also, increasing 
"maxiter" to something silly, like 50000, doesn't seem to increase the 
process time, which made me a bit suspicious that I'm not giving the 
right statement to the "control" argument. Or is the function indeed 
using the increased number of "maxiter", while the error message remains 
the same rather than reflecting the requested change to nls.control?

I should say that nls converges without increasing maxiter when I use 
the full dataset, and it is only when I use a smaller subset that the 
error occurs, possibly due to noisy data. But I would very much 
appreciate if someone could clarify whether the error message is wrong 
or if my statement to the "control" argument is wrong.

Thanks very much in advance,
Martin

-- 
Dr Martin Biuw
NERC Postdoctoral research fellow
Sea Mammal Research Unit
Gatty Marine Laboratory
University of St Andrews
St Andrews, Fife KY16 8LB
Scotland
Ph: +44-(0)1334-462677
Fax: +44-(0)1334-462632
www: http://www.smru.st-and.ac.uk



From abunn at whrc.org  Thu Jun  2 20:12:24 2005
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 2 Jun 2005 14:12:24 -0400
Subject: [R] Preprocessing troublesome files in R - looking for some perl
	like functionality
In-Reply-To: <Pine.LNX.4.61.0506021529410.15737@gannet.stats>
Message-ID: <NEBBIPHDAMMOKDKPOFFIGEPGDGAA.abunn@whrc.org>

> x <- readLines(...)
> tmp <- file()
> writeLines(x[substr(x, 83, 86) == "STD"], tmp)
> read.fwf(tmp, ...)

I wrapped this approach into function and it worked swimmingly. Thanks
all. -Andy



From bates at stat.wisc.edu  Thu Jun  2 20:18:59 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 02 Jun 2005 13:18:59 -0500
Subject: [R] nls.control: increasing number of iterations
In-Reply-To: <429F5878.4090103@st-and.ac.uk>
References: <429F5878.4090103@st-and.ac.uk>
Message-ID: <429F4D93.6030400@stat.wisc.edu>

Martin Biuw wrote:
> Hello,
> I'm using the nls function and would like to increase the number of
> iterations. According to the documentation as well as other postings on
> R-help, I've tried to do this using the "control" argument:
> 
> nls(y ~ SSfpl(x, A, B, xmid, scal), data=my.data,
> control=nls.control(maxiter=200))
> 
> but no matter how much I increase "maxiter", I get the following error
> message:
> 
> Error in nls(y ~ cbind(1, 1/(1 + exp((xmid - x)/exp(lscal)))), data =
> xy,  :
>        number of iterations exceeded maximum of 50
> 
> The second line here suggests that the maximum number of iterations is
> still 50, despite changing the "control" argument. Also, increasing
> "maxiter" to something silly, like 50000, doesn't seem to increase the
> process time, which made me a bit suspicious that I'm not giving the
> right statement to the "control" argument. Or is the function indeed
> using the increased number of "maxiter", while the error message remains
> the same rather than reflecting the requested change to nls.control?
> 
> I should say that nls converges without increasing maxiter when I use
> the full dataset, and it is only when I use a smaller subset that the
> error occurs, possibly due to noisy data. But I would very much
> appreciate if someone could clarify whether the error message is wrong
> or if my statement to the "control" argument is wrong.
> 
> Thanks very much in advance,
> Martin
> 

If you look closely you will see that the error is not occurring in your
call to nls; it is occurring in a call generated as part of the
determination of the starting estimates.  If you use explicit starting
values you will get the correct behavior for maxiter.



From kjetil at acelerate.com  Thu Jun  2 20:14:59 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 02 Jun 2005 14:14:59 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <Pine.LNX.4.61.0506021632510.17584@gannet.stats>
References: <20050602084149.GC4564@boing.buug.de>	<f8e6ff0505060207336c7a50de@mail.gmail.com>	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
Message-ID: <429F4CA3.9020501@acelerate.com>

Prof Brian Ripley wrote:

> On Thu, 2 Jun 2005, hadley wickham wrote:
>
>>> An environment is a hash table, and copying occurs only on 
>>> modification,
>>> when any language would have to copy in this context.
>>
>
> Caveat: the default is new.env(hash=FALSE), so an environment is a 
> hash table in one sense but not necessarily so in the strict sense.
>
>> Yes, I'm aware that copying only occurs on modification.  However, it
>> was my understanding that
>>
>> a <- list(a =1)
>> a$b <- 4
>>
>> would create a new copy of a,
>
>
> Thomas was talking about environments, not lists (and $ works 
> differently for them).
>
>> whereas in Java say
>>
>> HashMap a = new HashMap();
>> a.put("a", 1);
>> a.put("b", 2);
>>
>> wouldn't create a copy of a. (please bear in mind my java syntax is 
>> very rusty!)
>>
>> Caching data implies updating at each step, thus possibly creating n
>> copies of the list.  Is that wrong?
>
>
> It depends what you mean by `copy'.  If you expand a hash table you at 
> some point need to re-arrange the hashing of the entries.  That's what 
> will happen with an R environment or an R list too.  The possible 
> difference is that you might expect the table to have some room for 
> expansion, and in your list example you did not give any.
>
> R's operations on lists make more copies than are strictly necessary, 
> but it is not clear that this is more costly than the housekeeping 
> that would otherwise be necessary.  In a$b <- 4, the wrapper VECSXP is 
> recreated and the pointers copied across, but the list elements are 
> not copied.  For
> a$a <- 4 it is probable that no copying is done (although if a2 <- a 
> had been done previously, the pending recursive copy would then be done).
> (It is also possible that I have overlooked something in the rather 
> complex code used to do these subassignments.)
>
The original poster asked for caching of results. here is an example 
using new.env():

memo <- function(fun) {
   mem <- new.env() 
   function(x) {
       if (exists(as.character(x), envir=mem)) get(as.character(x), 
envir=mem, inherits=FALSE) else {
       val <- fun(x)
       assign(as.character(x), value=val, envir=mem)
       val }
   }
}

 > fib <- memo(function(n) if(n<=1) 1 else fib(n-1)+fib(n-2))
 > system.time( fib(300) )
[1] 0.01 0.00 0.02   NA   NA

ls(get("mem", env=environment(fib)))
   *output supressed*

To compare:
system.time( {fib2 <- function(n)if(n<=1)1 else 
fib2(n-1)+fib2(n-2);fib2(30)})
[1]  8.07  0.08 12.75    NA    NA


(there is (at least) one problem with this solution: if the global 
workspace contains a
 variable `6`, it gives error. Why?)

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From spencer.graves at pdf.com  Thu Jun  2 20:19:09 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 02 Jun 2005 11:19:09 -0700
Subject: [R] glm with variance = mu+theta*mu^2?  
Message-ID: <429F4D9D.9000308@pdf.com>

	  How might you fit a generalized linear model (glm) with variance = 
mu+theta*mu^2 (where mu = mean of the exponential family random variable 
and theta is a parameter to be estimated)?

	  This appears in Table 2.7 of Fahrmeir and Tutz (2001) Multivariate 
Statisticial Modeling Based on Generalized Linear Models, 2nd ed. 
(Springer, p. 60), where they compare "log-linear model fits to cellular 
differentiation data based on quasi-likelihoods" between variance = 
phi*mu (quasi-Poisson), variance = phi*mu^2 (quasi-exponential), and 
variance = mu+theta*mu^2.  The "quasi" function accepted for the family 
argument in "glm" generates functions "variance", "validmu", and 
"dev.resids".  I can probably write functions  to mimic the "quasi" 
function.  However, I have two questions in regard to this:

	  (1) I don't know what to use for "dev.resids".  This may not matter 
for fitting.  I can try a couple of different things to see if it matters.

	  (2) Might someone else suggest something different, e.g., using 
something like optim to solve an appropriate quasi-score function?

	  Thanks,
	  spencer graves



From ggrothendieck at gmail.com  Thu Jun  2 20:36:45 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 2 Jun 2005 14:36:45 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <429F4CA3.9020501@acelerate.com>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<429F4CA3.9020501@acelerate.com>
Message-ID: <971536df05060211365d989144@mail.gmail.com>

On 6/2/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> Prof Brian Ripley wrote:
> 
> > On Thu, 2 Jun 2005, hadley wickham wrote:
> >
> >>> An environment is a hash table, and copying occurs only on
> >>> modification,
> >>> when any language would have to copy in this context.
> >>
> >
> > Caveat: the default is new.env(hash=FALSE), so an environment is a
> > hash table in one sense but not necessarily so in the strict sense.
> >
> >> Yes, I'm aware that copying only occurs on modification.  However, it
> >> was my understanding that
> >>
> >> a <- list(a =1)
> >> a$b <- 4
> >>
> >> would create a new copy of a,
> >
> >
> > Thomas was talking about environments, not lists (and $ works
> > differently for them).
> >
> >> whereas in Java say
> >>
> >> HashMap a = new HashMap();
> >> a.put("a", 1);
> >> a.put("b", 2);
> >>
> >> wouldn't create a copy of a. (please bear in mind my java syntax is
> >> very rusty!)
> >>
> >> Caching data implies updating at each step, thus possibly creating n
> >> copies of the list.  Is that wrong?
> >
> >
> > It depends what you mean by `copy'.  If you expand a hash table you at
> > some point need to re-arrange the hashing of the entries.  That's what
> > will happen with an R environment or an R list too.  The possible
> > difference is that you might expect the table to have some room for
> > expansion, and in your list example you did not give any.
> >
> > R's operations on lists make more copies than are strictly necessary,
> > but it is not clear that this is more costly than the housekeeping
> > that would otherwise be necessary.  In a$b <- 4, the wrapper VECSXP is
> > recreated and the pointers copied across, but the list elements are
> > not copied.  For
> > a$a <- 4 it is probable that no copying is done (although if a2 <- a
> > had been done previously, the pending recursive copy would then be done).
> > (It is also possible that I have overlooked something in the rather
> > complex code used to do these subassignments.)
> >
> The original poster asked for caching of results. here is an example
> using new.env():
> 
> memo <- function(fun) {
>   mem <- new.env()
>   function(x) {
>       if (exists(as.character(x), envir=mem)) get(as.character(x),
> envir=mem, inherits=FALSE) else {
>       val <- fun(x)
>       assign(as.character(x), value=val, envir=mem)
>       val }
>   }
> }
> 
>  > fib <- memo(function(n) if(n<=1) 1 else fib(n-1)+fib(n-2))
>  > system.time( fib(300) )
> [1] 0.01 0.00 0.02   NA   NA
> 
> ls(get("mem", env=environment(fib)))
>   *output supressed*
> 
> To compare:
> system.time( {fib2 <- function(n)if(n<=1)1 else
> fib2(n-1)+fib2(n-2);fib2(30)})
> [1]  8.07  0.08 12.75    NA    NA
> 
> 
> (there is (at least) one problem with this solution: if the global
> workspace contains a
>  variable `6`, it gives error. Why?)

I think you need to add the argument inherits = FALSE to the call
to exists.



From ripley at stats.ox.ac.uk  Thu Jun  2 20:40:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 2 Jun 2005 19:40:35 +0100 (BST)
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <429F40B5.9030302@stats.uwo.ca>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<971536df0506020917730d77d4@mail.gmail.com>
	<429F40B5.9030302@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0506021916230.20355@gannet.stats>

On Thu, 2 Jun 2005, Duncan Murdoch wrote:

> Gabor Grothendieck wrote:
>> On 6/2/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> 
>>> On Thu, 2 Jun 2005, hadley wickham wrote:
>>> 
>>> 
>>>>> An environment is a hash table, and copying occurs only on modification,
>>>>> when any language would have to copy in this context.
>>> 
>>> Caveat: the default is new.env(hash=FALSE), so an environment is a hash
>>> table in one sense but not necessarily so in the strict sense.
>> 
>> 
>> Can you expand on this?  When would one use hash = TRUE vs.
>> the default?
>
> It's an optimization question.  hash = TRUE uses more memory and takes longer 
> to set up, but will give faster searches in big environments. The current 
> default assumes that environments are small so the effort of building the 
> hash table is not worthwhile, and it does linear searches.

It's not really size: building small hash tables is quick.  The issue is 
more to do with whether there are many lookups done compared to entries.

We met the same issues for a named vector a while back.  The relevant NEWS 
item was

     o	Indexing a vector by a character vector was slow if both the
 	vector and index were long (say 10,000).  Now hashing is used
 	and the time should be linear in the longer of the lengths
 	(but more memory is used).


> I suspect that we might have the default wrong (or perhaps should make the 
> transition from linear to hash search automatically at a certain threshold 
> size), but I haven't done the testing necessary to show this.

Here's an example

tr <- as.character(trunc(runif(1e5, max=100)))
system.time({
   env <- new.env(hash=F)
   for(i in 1:1e5) assign(tr[i], i, envir=env)
}, gcFirst=TRUE)

which takes about 5% less with hashing.  Now change to max=1e4: the hashed 
version takes about 50% longer, the unhashed one 120x.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Thu Jun  2 20:41:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 2 Jun 2005 14:41:32 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <971536df05060211365d989144@mail.gmail.com>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<429F4CA3.9020501@acelerate.com>
	<971536df05060211365d989144@mail.gmail.com>
Message-ID: <971536df050602114127a785f@mail.gmail.com>

On 6/2/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/2/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> > Prof Brian Ripley wrote:
> >
> > > On Thu, 2 Jun 2005, hadley wickham wrote:
> > >
> > >>> An environment is a hash table, and copying occurs only on
> > >>> modification,
> > >>> when any language would have to copy in this context.
> > >>
> > >
> > > Caveat: the default is new.env(hash=FALSE), so an environment is a
> > > hash table in one sense but not necessarily so in the strict sense.
> > >
> > >> Yes, I'm aware that copying only occurs on modification.  However, it
> > >> was my understanding that
> > >>
> > >> a <- list(a =1)
> > >> a$b <- 4
> > >>
> > >> would create a new copy of a,
> > >
> > >
> > > Thomas was talking about environments, not lists (and $ works
> > > differently for them).
> > >
> > >> whereas in Java say
> > >>
> > >> HashMap a = new HashMap();
> > >> a.put("a", 1);
> > >> a.put("b", 2);
> > >>
> > >> wouldn't create a copy of a. (please bear in mind my java syntax is
> > >> very rusty!)
> > >>
> > >> Caching data implies updating at each step, thus possibly creating n
> > >> copies of the list.  Is that wrong?
> > >
> > >
> > > It depends what you mean by `copy'.  If you expand a hash table you at
> > > some point need to re-arrange the hashing of the entries.  That's what
> > > will happen with an R environment or an R list too.  The possible
> > > difference is that you might expect the table to have some room for
> > > expansion, and in your list example you did not give any.
> > >
> > > R's operations on lists make more copies than are strictly necessary,
> > > but it is not clear that this is more costly than the housekeeping
> > > that would otherwise be necessary.  In a$b <- 4, the wrapper VECSXP is
> > > recreated and the pointers copied across, but the list elements are
> > > not copied.  For
> > > a$a <- 4 it is probable that no copying is done (although if a2 <- a
> > > had been done previously, the pending recursive copy would then be done).
> > > (It is also possible that I have overlooked something in the rather
> > > complex code used to do these subassignments.)
> > >
> > The original poster asked for caching of results. here is an example
> > using new.env():
> >
> > memo <- function(fun) {
> >   mem <- new.env()
> >   function(x) {
> >       if (exists(as.character(x), envir=mem)) get(as.character(x),
> > envir=mem, inherits=FALSE) else {
> >       val <- fun(x)
> >       assign(as.character(x), value=val, envir=mem)
> >       val }
> >   }
> > }
> >
> >  > fib <- memo(function(n) if(n<=1) 1 else fib(n-1)+fib(n-2))
> >  > system.time( fib(300) )
> > [1] 0.01 0.00 0.02   NA   NA
> >
> > ls(get("mem", env=environment(fib)))
> >   *output supressed*
> >
> > To compare:
> > system.time( {fib2 <- function(n)if(n<=1)1 else
> > fib2(n-1)+fib2(n-2);fib2(30)})
> > [1]  8.07  0.08 12.75    NA    NA
> >
> >
> > (there is (at least) one problem with this solution: if the global
> > workspace contains a
> >  variable `6`, it gives error. Why?)
> 
> I think you need to add the argument inherits = FALSE to the call
> to exists.
> 

or alternately use new.env(parent = NULL) in which case you don't
need the inherits = FALSE on the get either.



From murdoch at stats.uwo.ca  Thu Jun  2 20:40:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 14:40:38 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <429F4CA3.9020501@acelerate.com>
References: <20050602084149.GC4564@boing.buug.de>	<f8e6ff0505060207336c7a50de@mail.gmail.com>	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>	<f8e6ff05050602080953b85dd1@mail.gmail.com>	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<429F4CA3.9020501@acelerate.com>
Message-ID: <429F52A6.9020308@stats.uwo.ca>

On 6/2/2005 2:14 PM, Kjetil Brinchmann Halvorsen wrote:
> Prof Brian Ripley wrote:
> 
>> On Thu, 2 Jun 2005, hadley wickham wrote:
>>
>>>> An environment is a hash table, and copying occurs only on 
>>>> modification,
>>>> when any language would have to copy in this context.
>>>
>>
>> Caveat: the default is new.env(hash=FALSE), so an environment is a 
>> hash table in one sense but not necessarily so in the strict sense.
>>
>>> Yes, I'm aware that copying only occurs on modification.  However, it
>>> was my understanding that
>>>
>>> a <- list(a =1)
>>> a$b <- 4
>>>
>>> would create a new copy of a,
>>
>>
>> Thomas was talking about environments, not lists (and $ works 
>> differently for them).
>>
>>> whereas in Java say
>>>
>>> HashMap a = new HashMap();
>>> a.put("a", 1);
>>> a.put("b", 2);
>>>
>>> wouldn't create a copy of a. (please bear in mind my java syntax is 
>>> very rusty!)
>>>
>>> Caching data implies updating at each step, thus possibly creating n
>>> copies of the list.  Is that wrong?
>>
>>
>> It depends what you mean by `copy'.  If you expand a hash table you at 
>> some point need to re-arrange the hashing of the entries.  That's what 
>> will happen with an R environment or an R list too.  The possible 
>> difference is that you might expect the table to have some room for 
>> expansion, and in your list example you did not give any.
>>
>> R's operations on lists make more copies than are strictly necessary, 
>> but it is not clear that this is more costly than the housekeeping 
>> that would otherwise be necessary.  In a$b <- 4, the wrapper VECSXP is 
>> recreated and the pointers copied across, but the list elements are 
>> not copied.  For
>> a$a <- 4 it is probable that no copying is done (although if a2 <- a 
>> had been done previously, the pending recursive copy would then be done).
>> (It is also possible that I have overlooked something in the rather 
>> complex code used to do these subassignments.)
>>
> The original poster asked for caching of results. here is an example 
> using new.env():
> 
> memo <- function(fun) {
>    mem <- new.env() 
>    function(x) {
>        if (exists(as.character(x), envir=mem)) get(as.character(x), 
> envir=mem, inherits=FALSE) else {
>        val <- fun(x)
>        assign(as.character(x), value=val, envir=mem)
>        val }
>    }
> }
> 
>  > fib <- memo(function(n) if(n<=1) 1 else fib(n-1)+fib(n-2))
>  > system.time( fib(300) )
> [1] 0.01 0.00 0.02   NA   NA
> 
> ls(get("mem", env=environment(fib)))
>    *output supressed*
> 
> To compare:
> system.time( {fib2 <- function(n)if(n<=1)1 else 
> fib2(n-1)+fib2(n-2);fib2(30)})
> [1]  8.07  0.08 12.75    NA    NA
> 
> 
> (there is (at least) one problem with this solution: if the global 
> workspace contains a
>  variable `6`, it gives error. Why?)

Your environment mem has parent being the memo function environment, 
which has parent the global environment, so your `6` is being found there.

You can avoid the search by having "inherits=FALSE" in your call to 
exists().  Then it won't be necessary in your call to get().

In R up to 2.1.x, you can't avoid mem having a parent environment; 
hopefully in 2.2.x you can tell R to create mem with an empty parent.

Duncan Murdoch



From Jennifer.Morin at colorado.edu  Thu Jun  2 20:56:35 2005
From: Jennifer.Morin at colorado.edu (Jennifer.Morin@colorado.edu)
Date: Thu, 02 Jun 2005 12:56:35 -0600
Subject: [R] Adding a legend to a symbol plot
Message-ID: <1117738595.429f566332f87@webmail.colorado.edu>

I have created a symbol plot with circles that represent the mean temperature
at lat/lon locations over the United States. The radius of the circle
corresponds to the mean temperature. I would like to add a legend that
identifies a range of temperatures (e.g. 0-10, 10-20, etc) with circles of the
appropriate radii next to them. I've read the manual on how to add a legend,
and I'm fine with adding one to the plot. However, I cannot get circles of
different sizes to come up next to the ranges in my legend. All that is coming
up are circles of all the same radius. In looking at the R manual, it appears
that in order to get the right radius for the circles, I have to use the "pch"
parameter in the "legend" command, but I'm unclear on how I should define this
parameter. Can anyone help?

thanks,
Jennifer Morin



From sfalcon at fhcrc.org  Thu Jun  2 20:59:47 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 02 Jun 2005 11:59:47 -0700
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <429F4CA3.9020501@acelerate.com> (Kjetil Brinchmann Halvorsen's
	message of "Thu, 02 Jun 2005 14:14:59 -0400")
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<429F4CA3.9020501@acelerate.com>
Message-ID: <m2br6o60v0.fsf@macaroni.local>

On  2 Jun 2005, kjetil at acelerate.com wrote:
> The original poster asked for caching of results. here is an example
> using new.env():
>
> memo <- function(fun) {
> mem <- new.env() 
> function(x) {
> if (exists(as.character(x), envir=mem)) get(as.character(x), 
> envir=mem, inherits=FALSE) else {
> val <- fun(x)
> assign(as.character(x), value=val, envir=mem)
> val }
> }
> }
>
>> fib <- memo(function(n) if(n<=1) 1 else fib(n-1)+fib(n-2))
>> system.time( fib(300) )
> [1] 0.01 0.00 0.02   NA   NA
>
> ls(get("mem", env=environment(fib)))
> *output supressed*
>
> To compare:
> system.time( {fib2 <- function(n)if(n<=1)1 else 
> fib2(n-1)+fib2(n-2);fib2(30)})
> [1]  8.07  0.08 12.75    NA    NA
>
>
> (there is (at least) one problem with this solution: if the global 
> workspace contains a
> variable `6`, it gives error. Why?)

Because by default, new.env creates an environment with parent set to
parent.frame().  

So when you call exists, although "6" is not found in the mem
environment, it is found in the global environment.  But then the get
fails.

In terms of using R's environments as dictionary data structures I
think what one wants most of the time is:

myEnv <- new.env(hash=TRUE, parent=NULL)

+ seth



From kjetil at acelerate.com  Thu Jun  2 21:02:09 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 02 Jun 2005 15:02:09 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <429F40B5.9030302@stats.uwo.ca>
References: <20050602084149.GC4564@boing.buug.de>	<f8e6ff0505060207336c7a50de@mail.gmail.com>	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>	<f8e6ff05050602080953b85dd1@mail.gmail.com>	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>	<971536df0506020917730d77d4@mail.gmail.com>
	<429F40B5.9030302@stats.uwo.ca>
Message-ID: <429F57B1.1050504@acelerate.com>

Duncan Murdoch wrote:

> Gabor Grothendieck wrote:
>
>> On 6/2/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>
>>> On Thu, 2 Jun 2005, hadley wickham wrote:
>>>
>>>
>>>>> An environment is a hash table, and copying occurs only on 
>>>>> modification,
>>>>> when any language would have to copy in this context.
>>>>
>>>
>>> Caveat: the default is new.env(hash=FALSE), so an environment is a hash
>>> table in one sense but not necessarily so in the strict sense.
>>
>>
>>
>> Can you expand on this?  When would one use hash = TRUE vs.
>> the default?
>
>
> It's an optimization question.  hash = TRUE uses more memory and takes 
> longer to set up, but will give faster searches in big environments. 
> The current default assumes that environments are small so the effort 
> of building the hash table is not worthwhile, and it does linear 
> searches.
>
> I suspect that we might have the default wrong (or perhaps should make 
> the transition from linear to hash search automatically at a certain 
> threshold size), but I haven't done the testing necessary to show this.
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
>
An example, with my example in a post some m inutes ago:

memo <- function(fun, hash) {
   mem <- new.env(hash=hash) 
   function(x) {
       if (exists(as.character(x), envir=mem)) get(as.character(x), 
envir=mem, inherits=FALSE) else {
       val <- fun(x)
       assign(as.character(x), value=val, envir=mem)
       val }
   }
}

 > fib1 <- memo( function(n) if(n<=1)1 else fib1(n-1)+fib1(n-2), TRUE)
 > fib2 <- memo( function(n) if(n<=1)1 else fib2(n-1)+fib2(n-2), FALSE)
 > system.time( fib1(400) )
[1] 0.06 0.00 1.06   NA   NA
 > system.time( fib2(400) )
[1] 0.00 0.00 1.44   NA   NA

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From gunter.berton at gene.com  Thu Jun  2 21:07:04 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 2 Jun 2005 12:07:04 -0700
Subject: [R] Adding a legend to a symbol plot
In-Reply-To: <1117738595.429f566332f87@webmail.colorado.edu>
Message-ID: <200506021907.j52J74SC027732@volta.gene.com>

Not an answer, but a note. Encoding the mean temperature as the circle
radius is a bad idea: the eye perceives the area and so the radius^2, thus
the perceived effect is the square of the actual temperature effect (Howard
Wainer once referrred to this as "goosing up the effect by squaring the
eyeball.") . You should encode the sqrt of the temperature as the radius so
that the area is proportional to temperature. Details matter in graphical
perception.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Jennifer.Morin at colorado.edu
> Sent: Thursday, June 02, 2005 11:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Adding a legend to a symbol plot
> 
> I have created a symbol plot with circles that represent the 
> mean temperature
> at lat/lon locations over the United States. The radius of the circle
> corresponds to the mean temperature. I would like to add a legend that
> identifies a range of temperatures (e.g. 0-10, 10-20, etc) 
> with circles of the
> appropriate radii next to them. I've read the manual on how 
> to add a legend,
> and I'm fine with adding one to the plot. However, I cannot 
> get circles of
> different sizes to come up next to the ranges in my legend. 
> All that is coming
> up are circles of all the same radius. In looking at the R 
> manual, it appears
> that in order to get the right radius for the circles, I have 
> to use the "pch"
> parameter in the "legend" command, but I'm unclear on how I 
> should define this
> parameter. Can anyone help?
> 
> thanks,
> Jennifer Morin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Thu Jun  2 21:14:34 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 2 Jun 2005 16:14:34 -0300 (ADT)
Subject: [R] Caution on the use of model.matrix.
Message-ID: <200506021914.j52JEYUG019506@erdos.math.unb.ca>

Brian Ripley wrote:

> <snip> But the real problem is more likely that Rolf has not passed 
> model.matrix a model frame, so it calls model.frame() internally.  The 
> help page is a bit confused in that it says
> 
>      data: a data frame created with 'model.frame'.
> 
> which the default for the argument is not.  So a better solution would 
> then be to call model.frame and pass a model frame to model.matrix.
> 
> delete.response() might also be useful.
> 
> The suggested warning only applies if `data' is not supplied.

	I don't grok this.  I ***did*** supply data (in the form
	of a data frame, not a model frame).  My call was of the form

		X <- model.matrix(fmla,XXX)

	where (originally) ``fmla'' was a formula with the structure
	``y ~ x + w + z'', and XXX was a data frame with columns
	``y'', ``x'', ``w'', and ``z''.  (The response variable ``y''
	had NAs in it, which caused the problem.) The data frame XXX was
	``input data''; it was not created with model.frame, but it
	was data nonetheless.

	I replaced the forgoing call with

		X <- model.matrix(fmla[-2],XXX)

	(the ``-2'' causing the ``y'' part of the formula to
	be discarded) and got the results I wanted.

	There may be a better way of achieving my goal, but
	I'm happy with my method --- unless someone points out
	lurking hazzards that have so far not been apparent to me.

	I merely wanted to point out to others the somewhat
	unintuitive behaviour of model.matrix.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From jsorkin at grecc.umaryland.edu  Thu Jun  2 21:21:01 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Thu, 02 Jun 2005 15:21:01 -0400
Subject: [R] bias and std. error from boot
Message-ID: <s29f23f2.047@grecc.umaryland.edu>

I am running a bootstrap 
bootResult<-boot(.......)
and would like to obtain the bias and std. error produced by the
bootstrap. 

I can get the values to print to the screen, I can get the original
(non-bootstrap) estimates, OrgEst<-bootResult$t0

but I can not find any way to assign the bias and std. error values to
variables, e.g. 
bias<-bootResult$bias does not work. 

Any help would be appreciated.

Thank you, 
John 


John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From murdoch at stats.uwo.ca  Thu Jun  2 21:37:29 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 15:37:29 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <Pine.LNX.4.61.0506021916230.20355@gannet.stats>
References: <20050602084149.GC4564@boing.buug.de>
	<f8e6ff0505060207336c7a50de@mail.gmail.com>
	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>
	<f8e6ff05050602080953b85dd1@mail.gmail.com>
	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>
	<971536df0506020917730d77d4@mail.gmail.com>
	<429F40B5.9030302@stats.uwo.ca>
	<Pine.LNX.4.61.0506021916230.20355@gannet.stats>
Message-ID: <429F5FF9.2060407@stats.uwo.ca>

On 6/2/2005 2:40 PM, Prof Brian Ripley wrote:
> On Thu, 2 Jun 2005, Duncan Murdoch wrote:
> 
>> Gabor Grothendieck wrote:
>>> On 6/2/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>>> 
>>>> On Thu, 2 Jun 2005, hadley wickham wrote:
>>>> 
>>>> 
>>>>>> An environment is a hash table, and copying occurs only on modification,
>>>>>> when any language would have to copy in this context.
>>>> 
>>>> Caveat: the default is new.env(hash=FALSE), so an environment is a hash
>>>> table in one sense but not necessarily so in the strict sense.
>>> 
>>> 
>>> Can you expand on this?  When would one use hash = TRUE vs.
>>> the default?
>>
>> It's an optimization question.  hash = TRUE uses more memory and takes longer 
>> to set up, but will give faster searches in big environments. The current 
>> default assumes that environments are small so the effort of building the 
>> hash table is not worthwhile, and it does linear searches.
> 
> It's not really size: building small hash tables is quick.  The issue is 
> more to do with whether there are many lookups done compared to entries.
> 
> We met the same issues for a named vector a while back.  The relevant NEWS 
> item was
> 
>      o	Indexing a vector by a character vector was slow if both the
>  	vector and index were long (say 10,000).  Now hashing is used
>  	and the time should be linear in the longer of the lengths
>  	(but more memory is used).
> 
> 
>> I suspect that we might have the default wrong (or perhaps should make the 
>> transition from linear to hash search automatically at a certain threshold 
>> size), but I haven't done the testing necessary to show this.
> 
> Here's an example
> 
> tr <- as.character(trunc(runif(1e5, max=100)))
> system.time({
>    env <- new.env(hash=F)
>    for(i in 1:1e5) assign(tr[i], i, envir=env)
> }, gcFirst=TRUE)
> 
> which takes about 5% less with hashing.  Now change to max=1e4: the hashed 
> version takes about 50% longer, the unhashed one 120x.
> 

That tests lots of lookups with fairly big tables.  I just ran the 
following code to check on relatively few lookups in small tables:

result <- matrix(NA, 10,10)

timeit <- function(lookups, entries, reps=1000) {
     tr <- as.character(trunc(runif(lookups, max=entries)))
     hash <- system.time({
     	for (n in 1:reps) {
     	    env <- new.env(hash=T)
     	    for(i in 1:lookups) assign(tr[i], i, envir=env)
       }
     }, gcFirst=TRUE)[1]
     nohash <- system.time({
     	for (n in 1:reps) {
     	    env <- new.env(hash=F)
     	    for(i in 1:lookups) assign(tr[i], i, envir=env)
     	}
     }, gcFirst=TRUE)[1]
     hash/nohash
}

for (i in 1:10) for (j in 1:10) result[i,j] <- timeit(2^i,2^j)

This gives the ratio of times from hashed versus non-hashed for 2^i 
lookups from 2^j possible names.  The results were as follows:

 > round(result, 3)
        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]
  [1,] 1.000 0.333 1.000 1.000 1.000 1.000 0.500 3.000 1.000 0.500
  [2,] 1.000 1.000 0.750 1.000 1.000 1.333 1.000 1.000 1.000 1.000
  [3,] 0.800 0.600 1.000 1.000 0.800 1.000 0.800 1.000 1.000 1.000
  [4,] 1.000 0.800 1.143 1.125 0.889 1.000 1.000 1.000 1.000 0.700
  [5,] 1.067 0.938 1.143 1.000 0.941 1.000 1.063 1.067 0.938 1.143
  [6,] 1.000 1.034 0.966 0.838 1.000 1.031 1.000 1.000 1.091 1.065
  [7,] 1.000 1.000 0.891 0.952 0.984 0.970 0.957 0.960 1.041 0.986
  [8,] 1.009 0.992 0.992 0.984 0.904 0.928 0.906 0.905 0.900 0.898
  [9,] 1.040 1.008 0.967 0.972 0.957 0.922 0.884 0.778 0.810 0.789
[10,] 1.022 0.998 1.002 0.976 0.930 0.886 0.799 0.759 0.723 0.657

So hashing in the small tables doesn't look much slower (maybe 5%?  I 
need more reps...) than linear lookup, even taking into account the hash 
table creation.  I think we should switch to hash=TRUE as our default.

Duncan Murdoch



From murdoch at stats.uwo.ca  Thu Jun  2 21:42:08 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 15:42:08 -0400
Subject: [R] Dynamic Dictionary Data Type?
In-Reply-To: <971536df050602114127a785f@mail.gmail.com>
References: <20050602084149.GC4564@boing.buug.de>	<f8e6ff0505060207336c7a50de@mail.gmail.com>	<Pine.A41.4.61b.0506020742270.220594@homer11.u.washington.edu>	<f8e6ff05050602080953b85dd1@mail.gmail.com>	<Pine.LNX.4.61.0506021632510.17584@gannet.stats>	<429F4CA3.9020501@acelerate.com>	<971536df05060211365d989144@mail.gmail.com>
	<971536df050602114127a785f@mail.gmail.com>
Message-ID: <429F6110.1010109@stats.uwo.ca>

On 6/2/2005 2:41 PM, Gabor Grothendieck wrote:

> or alternately use new.env(parent = NULL) in which case you don't
> need the inherits = FALSE on the get either.

On 6/2/2005 2:59 PM, Seth Falcon wrote:

 >
 > In terms of using R's environments as dictionary data structures I
 > think what one wants most of the time is:
 >
 > myEnv <- new.env(hash=TRUE, parent=NULL)

One would think that, but it's not true.  "parent = NULL" sets the 
parent to the base environment.  There is currently no way to set the 
parent to be an empty environment.  I tried to fix this in 2.1.0, but 
ran out of time; hopefully I'll be more successful for 2.2.0.

Duncan Murdoch



From MikeJones at westat.com  Thu Jun  2 21:53:27 2005
From: MikeJones at westat.com (Mike Jones)
Date: Thu, 2 Jun 2005 15:53:27 -0400
Subject: [R] overlaying plots
Message-ID: <403593359CA56C4CAE1F8F4F00DCFE7D847C0C@MAILBE2.westat.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050602/f783831c/attachment.pl

From gunter.berton at gene.com  Thu Jun  2 22:09:41 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 2 Jun 2005 13:09:41 -0700
Subject: [R] Adding a legend to a symbol plot
In-Reply-To: <1117738595.429f566332f87@webmail.colorado.edu>
Message-ID: <200506022009.j52K9fU9001972@ohm.gene.com>

The symbol sizes, themselves, already encode the mean temperatures, so I'm
not exactly sure what you want to put in a legend. Perhaps the circles and
values for a few selected quantiles of the distribution to anchor the eye?
If so, legend() with pch and pt.cex does not appear capable of doing what
you want. However, it is simple to do by hand using symbols() and text()to
place the circles and values in the plot. If you want them outside the plot
region, set xpd  to TRUE (in both symbols() and text()). 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Jennifer.Morin at colorado.edu
> Sent: Thursday, June 02, 2005 11:57 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Adding a legend to a symbol plot
> 
> I have created a symbol plot with circles that represent the 
> mean temperature
> at lat/lon locations over the United States. The radius of the circle
> corresponds to the mean temperature. I would like to add a legend that
> identifies a range of temperatures (e.g. 0-10, 10-20, etc) 
> with circles of the
> appropriate radii next to them. I've read the manual on how 
> to add a legend,
> and I'm fine with adding one to the plot. However, I cannot 
> get circles of
> different sizes to come up next to the ranges in my legend. 
> All that is coming
> up are circles of all the same radius. In looking at the R 
> manual, it appears
> that in order to get the right radius for the circles, I have 
> to use the "pch"
> parameter in the "legend" command, but I'm unclear on how I 
> should define this
> parameter. Can anyone help?
> 
> thanks,
> Jennifer Morin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mischke at sozpsy.unizh.ch  Thu Jun  2 22:19:40 2005
From: mischke at sozpsy.unizh.ch (Stefan Mischke)
Date: Thu, 2 Jun 2005 22:19:40 +0200
Subject: [R] merge large matrices
Message-ID: <4a7c5f0d82924c8f8fded9d6395945fd@sozpsy.unizh.ch>

Dear List

I have two large matrices A and B. Both have the same dimensions, let's 
say 20k x 30k. About half the cells of B are missing. Now I'm looking 
for an efficient way to merge them, so that the missing values in B are 
replaced by the corresponding values of A.

Matrix A
	[,1]	[,2]	[,3]
[1,]	1	2	3
[2,]	4	5	6

merged with Matrix B

	[,1]	[,2]	[,3]
[1,]	10	NA	NA
[2,]	NA	50	60

equals

	[,1]	[,2]	[,3]
[1,]	10	2	3
[2,]	4	50	60

One way to do this, is  of course looping through all the cells, 
checking for NAs and then replacing them with the corresponding values. 
But this is way too slow for my application. There must be a more 
efficient way.
Does R provide any functions for this?

Cheers
Stefan



From sarah.goslee at gmail.com  Thu Jun  2 22:28:46 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 2 Jun 2005 16:28:46 -0400
Subject: [R] merge large matrices
In-Reply-To: <4a7c5f0d82924c8f8fded9d6395945fd@sozpsy.unizh.ch>
References: <4a7c5f0d82924c8f8fded9d6395945fd@sozpsy.unizh.ch>
Message-ID: <efb536d5050602132873ca2617@mail.gmail.com>

On 6/2/05, Stefan Mischke <mischke at sozpsy.unizh.ch> wrote:
> Dear List
> 
> I have two large matrices A and B. Both have the same dimensions, let's
> say 20k x 30k. About half the cells of B are missing. Now I'm looking
> for an efficient way to merge them, so that the missing values in B are
> replaced by the corresponding values of A.

How about:
B[is.na(B)] <- A[is.na(B)]


Sarah

-- 
Sarah Goslee
http://www.stringpage.com



From murdoch at stats.uwo.ca  Thu Jun  2 22:29:14 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 02 Jun 2005 16:29:14 -0400
Subject: [R] merge large matrices
In-Reply-To: <4a7c5f0d82924c8f8fded9d6395945fd@sozpsy.unizh.ch>
References: <4a7c5f0d82924c8f8fded9d6395945fd@sozpsy.unizh.ch>
Message-ID: <429F6C1A.5090202@stats.uwo.ca>

On 6/2/2005 4:19 PM, Stefan Mischke wrote:
> Dear List
> 
> I have two large matrices A and B. Both have the same dimensions, let's 
> say 20k x 30k. About half the cells of B are missing. Now I'm looking 
> for an efficient way to merge them, so that the missing values in B are 
> replaced by the corresponding values of A.
> 
> Matrix A
> 	[,1]	[,2]	[,3]
> [1,]	1	2	3
> [2,]	4	5	6
> 
> merged with Matrix B
> 
> 	[,1]	[,2]	[,3]
> [1,]	10	NA	NA
> [2,]	NA	50	60
> 
> equals
> 
> 	[,1]	[,2]	[,3]
> [1,]	10	2	3
> [2,]	4	50	60
> 
> One way to do this, is  of course looping through all the cells, 
> checking for NAs and then replacing them with the corresponding values. 
> But this is way too slow for my application. There must be a more 
> efficient way.
> Does R provide any functions for this?

As long as the dimensions are the same:

replace <- is.na(B)
B[replace] <- A[replace]

should work.

Duncan Murdoch



From amberfer at ull.es  Thu Jun  2 22:34:27 2005
From: amberfer at ull.es (amberfer@ull.es)
Date: Thu,  2 Jun 2005 21:34:27 +0100
Subject: [R] post hoc kruskal wallis
Message-ID: <1117744467.429f6d53e6c9c@correoweb.ccti.ull.es>

I??m looking for a program with a post hoc kruskall Wallis test like the Tukey-
type non parametric test of ZAR(Biostatistical Analysis 2??Edition). SSPS 
hasn??t got any non parametric post host test. I don??t know if R has an 
appropiate post hoc non parametric test. I??m not sure if NDWD test of the 
package ?coin? is an appropiate test or if the package ?npmc? is a best option.
Thank you


Alfredo Berm??dez







-------------------------------------------------


----- Fin del mensaje reenviado -----



From gunter.berton at gene.com  Thu Jun  2 22:36:21 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 2 Jun 2005 13:36:21 -0700
Subject: [R] overlaying plots
In-Reply-To: <403593359CA56C4CAE1F8F4F00DCFE7D847C0C@MAILBE2.westat.com>
Message-ID: <200506022036.j52KaLt4001455@faraday.gene.com>

Please read the chapter on plotting in "An Introduction to R" where this is
explained.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike Jones
> Sent: Thursday, June 02, 2005 12:53 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] overlaying plots
> 
> surely this not hard, but i can't figure it out.  i'd like to overlay
> the results of two ecdf's.  
>  
> e.g. xx<--ecdf(x) and yy<--ecdf(y).  i'd like to plot xx and yy on the
> same graph.   
>  
> thanks...mj
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.murrell at auckland.ac.nz  Thu Jun  2 22:57:20 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 03 Jun 2005 08:57:20 +1200
Subject: [R] panel.axis() & grid/lattice settings
References: <A89517C7FD248040BB71CA3C04C1ACBB01990113@CNTUSMAEXS4.na.jnj.com>
	<200506011507.13817.deepayan@stat.wisc.edu>
Message-ID: <429F72B0.9040505@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Wednesday 01 June 2005 05:43, Pikounis, Bill [CNTUS] wrote:
> 
>>Hello,
>>I wish to customize the tick marks and labels of axes in panels produced by
>>high-level lattice functions, namely xyplot. I know I can use the scales
>>argument to specify values for rot, cex, etc. in the top-level call.
>>
>>However, I am interested in determining values for cex and rot based on the
>>current panel / viewport and device. More specifically, I would like to
>>make adjustments when tick labels overlap on the x-axis, such as labels of
>>a factor. If I use base graphics, par("cin") or par("cxy") or strwidth(),
>>etc. can be used to develop an algorithm to adjust cex or/and rot if
>>needed.
>>
>>I am trying to determine the parameters/settings in grid analogous to
>>par("cin"), etc. mentioned above, knowing that par() has no effect in
>>lattice / grid. I have dug around the sources for grid and lattice but
>>cannot seem to come up with such parameters -- most notably something like
>>strwidth(). I see that panel.axis() has a check.overlap argument for
>>labels, but I could not trace down the actual code to see how that works.
>>What have I overlooked, or where should I be looking?
> 
> 
> Paul may be able to give a more insightful answer, but grid allows a string to 
> determine it's width in terms of itself, e.g.:
> 
> unit(1, "strwidth", data = "foo")


Or just stringWidth("foo") (and of course stringHeight("foo")).


> If you want to convert that into, say, inches, you could use 
> 
> 
>>convertX(unit(1, "strwidth", data = "foo"), "inches", TRUE)
> 
> [1] 0.2344092
> 
> I think this would depend on the gpars() in effect, in particular fontsize.


Yep.  There's also grobWidth(textGrob("foo")), which gives you not just 
the size of a string, but the size of a text graphical object.  The 
difference is that a text graphical object can include information about 
where it will be drawn, in what font, and so on.  For example, 
grobWidth(textGrob("foo", gp=gpar(fontfamily="mono", cex=2))) is very 
different from stringWidth("foo").   If you want to determine the size 
of some text that you want to draw as part of the panel axis AND you 
want it to look like the text that lattice would have drawn, then 
something like this might give you what you want:

grobWidth(textGrob("your label",
                    gp=do.call("gpar",  trellis.par.get("axis.text"))))

It is also important that you do these calculations within a panel 
function so that they get evaluated within the appropriate context 
(i.e., higher-level lattice graphical parameter settings have been 
enforced).  The result of the above call to grobWidth() could be very 
different if evaluated in some other context.

Finally, if you use convertX() (or any of its ilk), be aware that 
changes in the device size (e.g., resize a window) could make the 
calculations invalid.  Maybe we could follow this up on the r-devel 
mailing list if you think this will be an issue.

Paul


>>Indirectly related, setting outside=TRUE in a panel.axis() call does not
>>produce visible labels, perhaps due to "issues of clipping" as mentioned in
>>its help page. How might one disable clipping for the current panel /
>>viewport?
> 
> 
> At the grid level, there's a 'clip' argument to 'viewport()'. In lattice, 
> these are chosen from 
> 
> 
>>str(trellis.par.get("clip"))
> 
> List of 2
>  $ panel: chr "on"
>  $ strip: chr "on"
> 
> (In case you are using 'trellis.focus', that can set clipping off.)
> 
> Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From choudary.jagar at swosu.edu  Thu Jun  2 22:59:42 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Thu, 2 Jun 2005 15:59:42 -0500
Subject: [R] how to rectify t.test( ) error
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C22D@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050602/f21768a1/attachment.pl

From ross at biostat.ucsf.edu  Thu Jun  2 23:44:09 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 02 Jun 2005 14:44:09 -0700
Subject: [R] Too generic with S4 methods?
Message-ID: <1117748649.2413.38.camel@iron.libaux.ucsf.edu>

I tried the following (relevant excerpt only)
setMethod("likelihood",
           signature(spec="Specification", covs="vector",
                     states="vector"),
          function(spec, covs, states) {
####
setMethod("likelihood",
           signature(model="Model", path="matrix"),
           function(model, path) {

This fails with the message
arguments in definition changed from (spec) to (object)
Error in match.call(fun, fcall) : unused argument(s) (model ...)

I'll note in passing that it would be helpful to have a line number for
the error, and  that the error didn't mean much to me.

After study, my guess is that generics are not supposed to work this
way.  All the definitions need to have the same named arguments
(exceptions: some can be "missing", and ... is allowed).  So the error
above is a complaint that my second signature uses different names (or
is it different classes?).  And the "unused argument" is triggered by
the fact that the original signature had 3 args named spec, covs, and
states, and the new signature has none of them.  Is that about right?

It looks as if I might get away by not naming the arguments, so the
first signature could be signature("Specification", "vector", "vector")
and the 2nd signature("Model", "matrix", "missing").

Among other problems, the lack of identifiers makes the semantics of the
signature obscure in this case.

Basically, would it be advisable to use different generic names for the
two functions listed above?
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From BPikouni at CNTUS.JNJ.COM  Thu Jun  2 23:50:05 2005
From: BPikouni at CNTUS.JNJ.COM (Pikounis, Bill [CNTUS])
Date: Thu, 2 Jun 2005 17:50:05 -0400 
Subject: Thanks! (was  [R] panel.axis() & grid/lattice settings)
Message-ID: <A89517C7FD248040BB71CA3C04C1ACBB01990124@CNTUSMAEXS4.na.jnj.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050602/8781cf95/attachment.pl

From mischke at sozpsy.unizh.ch  Thu Jun  2 23:52:15 2005
From: mischke at sozpsy.unizh.ch (Stefan Mischke)
Date: Thu, 2 Jun 2005 23:52:15 +0200
Subject: [R] merge large matrices
In-Reply-To: <efb536d5050602132873ca2617@mail.gmail.com>
References: <4a7c5f0d82924c8f8fded9d6395945fd@sozpsy.unizh.ch>
	<efb536d5050602132873ca2617@mail.gmail.com>
Message-ID: <92be7ba2b75976a472ebbe878f641a40@sozpsy.unizh.ch>

Thanks a lot!
Both versions seem to work equally efficient.
N8,
Stefan



> replace <- is.na(B)
> B[replace] <- A[replace]
>
> Duncan Murdoch


> B[is.na(B)] <- A[is.na(B)]
>
> Sarah



From gunter.berton at gene.com  Fri Jun  3 00:02:16 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 2 Jun 2005 15:02:16 -0700
Subject: [R] Too generic with S4 methods?
In-Reply-To: <1117748649.2413.38.camel@iron.libaux.ucsf.edu>
Message-ID: <200506022202.j52M2GJ8016581@ohm.gene.com>

Yes, you're correct. From the Green book ("Programming with Data"):

(p.323) "Strictly speaking, the [specific] method supplied [for a generic]
must have the exact same formal argument list as the generic ... However, if
you do supply a function with different arguments, setMethod will construct
a valid method that calls the function you supplied, passing down all the
arguments of the generic."

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ross Boylan
> Sent: Thursday, June 02, 2005 2:44 PM
> To: r-help
> Subject: [R] Too generic with S4 methods?
> 
> I tried the following (relevant excerpt only)
> setMethod("likelihood",
>            signature(spec="Specification", covs="vector",
>                      states="vector"),
>           function(spec, covs, states) {
> ####
> setMethod("likelihood",
>            signature(model="Model", path="matrix"),
>            function(model, path) {
> 
> This fails with the message
> arguments in definition changed from (spec) to (object)
> Error in match.call(fun, fcall) : unused argument(s) (model ...)
> 
> I'll note in passing that it would be helpful to have a line 
> number for
> the error, and  that the error didn't mean much to me.
> 
> After study, my guess is that generics are not supposed to work this
> way.  All the definitions need to have the same named arguments
> (exceptions: some can be "missing", and ... is allowed).  So the error
> above is a complaint that my second signature uses different names (or
> is it different classes?).  And the "unused argument" is triggered by
> the fact that the original signature had 3 args named spec, covs, and
> states, and the new signature has none of them.  Is that about right?
> 
> It looks as if I might get away by not naming the arguments, so the
> first signature could be signature("Specification", "vector", 
> "vector")
> and the 2nd signature("Model", "matrix", "missing").
> 
> Among other problems, the lack of identifiers makes the 
> semantics of the
> signature obscure in this case.
> 
> Basically, would it be advisable to use different generic 
> names for the
> two functions listed above?
> -- 
> Ross Boylan                                      wk:  (415) 502-4031
> 530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
> Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
> University of California, San Francisco
> San Francisco, CA 94143-0840                     hm:  (415) 550-1062
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Fri Jun  3 00:28:21 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Jun 2005 18:28:21 -0400
Subject: [R] bias and std. error from boot
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E911@usctmx1106.merck.com>

They are calculated in the print() method for boot objects, but not
returned.  boot:::print.boot has:

[...]
            op <- cbind(t0, apply(t, 2, mean, na.rm = TRUE) - 
                t0, sqrt(apply(t, 2, function(t.st)
var(t.st[!is.na(t.st)]))))
            dimnames(op) <- list(rn, c("original", " bias  ", 
                " std. error"))
 
[...]

HTH,
Andy

> From: John Sorkin
> 
> I am running a bootstrap 
> bootResult<-boot(.......)
> and would like to obtain the bias and std. error produced by the
> bootstrap. 
> 
> I can get the values to print to the screen, I can get the original
> (non-bootstrap) estimates, OrgEst<-bootResult$t0
> 
> but I can not find any way to assign the bias and std. error values to
> variables, e.g. 
> bias<-bootResult$bias does not work. 
> 
> Any help would be appreciated.
> 
> Thank you, 
> John 
> 
> 
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
> 
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> 
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Fri Jun  3 00:55:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jun 2005 00:55:30 +0200
Subject: [R] how to rectify t.test( ) error
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C22D@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C22D@swosu-mbx01.admin.swosu.edu>
Message-ID: <x2y89sgyhp.fsf@turmalin.kubism.ku.dk>

"Jagarlamudi, Choudary" <choudary.jagar at swosu.edu> writes:

> Hi All,
>  
>  I get the following error when i perform a t.test.
>  
> studentt<-apply(tlr, 1, function(x) t.test(x[2:6], x[7:11],var.equal=TRUE)$p.value)
>  
> # tlr is a table of 11 columns and 22500 rows. I am not able to post tlr due to its size.
> 
> Error in if (stderr < 10 * .Machine$double.eps * max(abs(mx), abs(my))) stop("data are essentially constant") : 
>         missing value where TRUE/FALSE needed
> In addition: Warning messages: 
> 1: argument is not numeric or logical: returning NA in: mean.default(x) 
> 2: argument is not numeric or logical: returning NA in: mean.default(y) 
>  
> Could someone kindly give me pointers regarding the error. This worked fine with 2 other data sets.

I would conjecture that your table is nonnumeric. What is mode(tlr)?
(-- or mode(as.matrix(tlr)) if tlr is a data frame).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Fri Jun  3 02:05:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 2 Jun 2005 20:05:42 -0400
Subject: [R] post hoc kruskal wallis
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E912@usctmx1106.merck.com>

By post hoc I guess you meant pairwise comparisons.  You might want to check
out the npmc package on CRAN.

Andy

> From: amberfer at ull.es
> 
> I??m looking for a program with a post hoc kruskall Wallis 
> test like the Tukey-
> type non parametric test of ZAR(Biostatistical Analysis 
> 2??Edition). SSPS 
> hasn??t got any non parametric post host test. I don??t know if 
> R has an 
> appropiate post hoc non parametric test. I??m not sure if NDWD 
> test of the 
> package "coin" is an appropiate test or if the package "npmc" 
> is a best option.
> Thank you
> 
> 
> Alfredo Berm??dez
> 
> 
> 
> 
> 
> 
> 
> -------------------------------------------------
> 
> 
> ----- Fin del mensaje reenviado -----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From chuai_juok at hotmail.com  Fri Jun  3 06:25:55 2005
From: chuai_juok at hotmail.com (Ukech U. Kidi)
Date: Fri, 03 Jun 2005 04:25:55 +0000
Subject: [R] GARCH (1 , 1), Hill estimator of alpha, Pareto estimator
Message-ID: <BAY19-F216F41CE90FD039B431F47FD070@phx.gbl>

Dear R users,

Could you please help me out. I am in trouble as I am unable to model graphs 
to explain the GARCH (1 , 1) model, the Hill estimator (of alpha), and the 
Pareto estimator.

I just got introduce to R. I am working on a paper which must be worked from 
R.


You look at the difficulty I had from the text below.


[1] "DAX"       "DAX_CAC"   "DAX_CAC40" "Nikkei"    "NSCP"      "T"

[7] "Time"      "x"         "X"         "Y1"        "Y2"        "Y3"

[13] "Y4"

>save.image("R:/My documents/.RData")

>library(stats)

>data(DAX_CAC)

Warning message:

Data set 'DAX_CAC' not found in: data(DAX_CAC)

>data("DAX_CAC")

Warning message:

Data set 'DAX_CAC' not found in: data("DAX_CAC")

>dax<- diff(log(DAX_CAC$DAX[1:1865]))

>m1<- garch(dax)

Error: couldn't find function "garch"

>m1<- garch(dax[1:1865])

Error: couldn't find function "garch"

>m1<- garch(dax[1:1865])


Thank you in advance,

Ukech U. kidi



From adrian at maths.uwa.edu.au  Fri Jun  3 06:54:03 2005
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Fri, 3 Jun 2005 12:54:03 +0800
Subject: [R] dot in formula
Message-ID: <17055.57963.594896.830503@maths.uwa.edu.au>

gReetings,

I want to manipulate a formula object, containing the name "."
so that "." is replaced by a desired (arbitrary) expression.
What is a safe way to do this?

Adrian Baddeley



From ales.ziberna at guest.arnes.si  Fri Jun  3 08:21:47 2005
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Fri, 3 Jun 2005 08:21:47 +0200
Subject: [R] overlaying plots
References: <403593359CA56C4CAE1F8F4F00DCFE7D847C0C@MAILBE2.westat.com>
Message-ID: <008101c56804$863c3de0$598debd4@ales>

You can simply do
plot(xx)
plot(yy,add=T)

Ale? ?iberna

----- Original Message ----- 
From: "Mike Jones" <MikeJones at westat.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 02, 2005 9:53 PM
Subject: [R] overlaying plots


> surely this not hard, but i can't figure it out.  i'd like to overlay
> the results of two ecdf's.
>
> e.g. xx<--ecdf(x) and yy<--ecdf(y).  i'd like to plot xx and yy on the
> same graph.
>
> thanks...mj
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From buser at stat.math.ethz.ch  Fri Jun  3 08:26:06 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 3 Jun 2005 08:26:06 +0200
Subject: [R] mplot :how to deal with missing data
In-Reply-To: <200506012020.QAA01240@webmail10.cac.psu.edu>
References: <200506012020.QAA01240@webmail10.cac.psu.edu>
Message-ID: <17055.63486.931236.122474@stat.math.ethz.ch>

If you have to deal with missing values, you might be interested
in na.omit().
?na.omit

Regards,

Christoph

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


NATALIA F TCHETCHERINA writes:
 > Hello all,
 > I have  data:
 > 
 >            Genes   time rep vart dye         y trt
 > 130911   sa1-d07 030min   1  col   g  9.636244   o
 > 145771   sa1-d07 030min   1  col   r  8.107577   c
 > 93335    sa1-d07 030min   1  ler   g  7.409566   o
 > 94821    sa1-d07 030min   1  ler   r  5.107160   c
 > 10119101 sa1-d07 030min   2  col   g  8.336862   o
 > 11605101 sa1-d07 030min   2  col   r  7.824530   c
 > 725313   sa1-d07 030min   2  ler   g  8.249347   o
 > 740171   sa1-d07 030min   2  ler   r  7.565084   c
 > 1160522  sa1-d07 030min   3  col   g        NA   c
 > 1011922  sa1-d07 030min   3  col   r        NA   o
 > 562232   sa1-d07 030min   3  ler   g  9.974227   c
 > 547362   sa1-d07 030min   3  ler   r 10.341149   o
 > ..................................................
 > ..................................................
 > ..................................................
 > 
 > I would like to get graphs means for two-way factor combinations
 > I used Rlab package:
 > > mplot(data$y[which(data$Genes==sa1-d07)],
 > data$time[which(data$Genes==sa1-d07)], data$trt[which(data$Genes==sa1-d07)])
 > 
 > However, I have the following error message:
 > 
 > plot window will lay out plots in a 3 by 1 matrix 
 > Error in plot.window(xlim, ylim, log, asp, ...) : 
 >         need finite 'ylim' values
 > In addition: Warning messages:
 > 1: no finite arguments to min; returning Inf 
 > 2: no finite arguments to max; returning -Inf 
 > > 
 > I think this is because of some y='NA'.
 > My question is: how I can deal with this problem?
 > 
 > Sincerely, Natalia.
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Jun  3 08:59:12 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 3 Jun 2005 08:59:12 +0200 (CEST)
Subject: [R] [R-pkgs] New CRAN package `coin'
Message-ID: <Pine.LNX.4.51.0506030855240.564@artemis.imbe.med.uni-erlangen.de>


Conditional Inference Procedures in a Permutation Test Framework

The `coin' package implements a general framework for conditional
inference procedures, commonly known as permutation tests,
theoretically derived by Strasser & Weber (1999). The conditional
expectation and covariance for a broad class of multivariate linear
statistics as well as the corresponding multivariate limiting distribution
was derived by Strasser & Weber (1999). These results are
utilized to construct tests for independence between two sets of
variables.

Beside a general implementation of the abstract framework the package
offers a rather huge set of convenience functions implementing well
known classical as well as less prominent classical and non-classical test
procedures in a conditional inference framework. Examples are linear rank
statistics for the two- and K-sample location and scale problem against
ordered and unordered alternatives including post-hoc tests for arbitrary
contrasts, tests of independence for contingency tables, two- and K-sample
tests for censored data, tests for independence of two continuous
variables as well as tests for marginal homogeneity and symmetry.
Conditional counterparts of most of the classical procedures given in
famous text books like Hollander & Wolfe (1999) or Agresti (2002) can be
implemented as part of the general framework without much effort.
Approximations of the exact null distribution via the limiting
distribution and conditional Monte-Carlo procedures are available for
every test while the exact null distribution
is currently available for two-sample problems only.

Inference problems can be specified by a traditional formula based
interface. Support for data available as `exprSet' objects in a BioC
environment is implemented as well. The theoretical framework is sketched
in the vignette and we refer to `vignette("coin")' for further details.

Comments, suggestions, bug-reports etc. are more than welcome!

Best,

Torsten, Kurt, Mark and Achim

_____________________________________________________________________________

Package: coin
Title: Conditional Inference Procedures in a Permutation Test Framework
Date: $Date: 2005/06/02 14:55:45 $
Version: 0.2-11
Author: Torsten Hothorn and Kurt Hornik, with contributions by
  Mark van de Wiel and Achim Zeileis
Maintainer: Torsten Hothorn <Torsten.Hothorn at R-project.org>
Description: Conditional inference procedures for the general independence
  problem including two-sample, K-sample, correlation, censored, ordered
  and multivariate problems.
Depends: R (>= 2.0.0), methods, survival, mvtnorm
Suggests: multcomp
SaveImage: yes
LazyData: yes
License: GPL

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From p.dalgaard at biostat.ku.dk  Fri Jun  3 09:52:29 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jun 2005 09:52:29 +0200
Subject: [R] dot in formula
In-Reply-To: <17055.57963.594896.830503@maths.uwa.edu.au>
References: <17055.57963.594896.830503@maths.uwa.edu.au>
Message-ID: <x2br6n7u82.fsf@turmalin.kubism.ku.dk>

Adrian Baddeley <adrian at maths.uwa.edu.au> writes:

> gReetings,
> 
> I want to manipulate a formula object, containing the name "."
> so that "." is replaced by a desired (arbitrary) expression.
> What is a safe way to do this?

I'm not vouching for the safety, but something along the lines of

mycall <- quote(a+b) # NOT mode "expression"
substitute(~., list(.=mycall))

or (this stuff is such a pain)

eval(substitute(substitute(formula, list(.=mycall)), list=myformula))

Notice, BTW, that formulas can have "." on both sides, so you may want
to do it only to either the LHS or RHS as appropriate. Also note that
the substitute operations loses the the class and environment of the
original, so you may need to put that back in place with as.formula
and environment<-

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bhx2 at mevik.net  Fri Jun  3 09:55:13 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 03 Jun 2005 09:55:13 +0200
Subject: [R] "mvr" function
In-Reply-To: <BAY20-F8DD342CFF3CF7E07B378C80050@phx.gbl> (Jim BRINDLE's
	message of "Wed, 01 Jun 2005 17:02:56 -0400")
References: <BAY20-F8DD342CFF3CF7E07B378C80050@phx.gbl>
Message-ID: <m064wvnace.fsf@bar.nemo-project.org>

Jim BRINDLE writes:

> volumes <- read.table("THA_vol.txt", header = TRUE)
>
> and then created a data.frame called "vol".  My response variable is
> in the last column of the "vol" data frame and my dependent variables
> are in columns 1 through 11.

[...]

> y <- vol[,12]
> X <- vol[,1:11]
> ans.pcr <- pcr(y ~ X,6,data=vol,validation="CV")

There are two problems here:

1) X is a data frame, not a matrix.  This is what causes the error message.

2) You specify in the call that pcr should look in the data frame
   `vol' for variables called 'y' and 'X'.  (Presumably) they don't
   exist there, but in the global environment (because of the
   assignments `y <- vol[,12]', etc).  (This will not lead to an
   error, because pcr will find the variables anyway, but might lead
   to confusion or errors if you later modify those variables.)

The first problem can be overcome by doing

 X <- as.matrix(vol[,1:11])

and the second one by

 ans.pcr <- pcr(y ~ X, 6, validation = "CV")

However, there are (as always in R :) several ways of accomplishing
the same thing.  One solution is simply

 ans.pcr <- pcr(V12 ~ ., 6, data = vol, validation = "CV")

(where V12 must be substituted with the name of the 12th variable of
vol; see names(vol)).  This formula tells pcr to use V12 as the
response, and the remaining variable (in vol) as predictors.

A more general solution is to say

 vol2 <- data.frame(y = vol[,12], X = I(as.matrix(vol[,1:11])))
 ans.pcr <- pcr(y ~ X, 6, data = vol2, validation = "CV")

The I() makes R store X as a matrix in vol2, instead of as 11 separate
variables.  This is handy for cases where you have several matrices.

The manual page for `lm' and the R manual `An Introduction to R'
(chapter 11) are good references for the formula handling in R.


-- 
HTH,
Bj??rn-Helge Mevik



From bhx2 at mevik.net  Fri Jun  3 10:04:31 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 03 Jun 2005 10:04:31 +0200
Subject: [R] "mvr" function
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C9465E8@MSGBOSCLB2WIN.DMN1.FMR.COM>
	(Robert McGehee's message of "Wed, 1 Jun 2005 19:14:03 -0400")
References: <67DCA285A2D7754280D3B8E88EB548020C9465E8@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <m01x7jn9ww.fsf@bar.nemo-project.org>

McGehee, Robert writes:

> dataSet <- data.frame(y = vol[, 12])
> dataSet$X <- data.matrix(vol[, 1:11])
>
> ans.pcr <- pcr(y ~ X, 6, data = dataSet, validation = "CV")
>
> If there's a more elegant way of doing this without using data frames of
> matrices, I'd be interested as well.

I actually find using data frames with matrices the most elegant
way. :-)  Especially if you have several matrices.

Alternatively, to regress one variable of a data frame on the rest of
the variables, one can use

 ans.pcr <- pcr(y ~ ., 6, data = vol, validation = "CV")

(assuming the response variable is called `y' in the data frame; see
names(vol).)

One does not _have_ to store the data in a data frame (although I
would recommend it, because it is then easier to specify test data
sets and alternative data sets).  One can simply store the variables
in the global environment, and skip the `data' argument of `pcr',

-- 
HTH,
Bj??rn-Helge Mevik



From bhx2 at mevik.net  Fri Jun  3 10:09:08 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 03 Jun 2005 10:09:08 +0200
Subject: [R] dot in formula
In-Reply-To: <17055.57963.594896.830503@maths.uwa.edu.au> (Adrian Baddeley's
	message of "Fri, 3 Jun 2005 12:54:03 +0800")
References: <17055.57963.594896.830503@maths.uwa.edu.au>
Message-ID: <m0wtpblv4r.fsf@bar.nemo-project.org>

Adrian Baddeley writes:

> I want to manipulate a formula object, containing the name "."
> so that "." is replaced by a desired (arbitrary) expression.

How about

myf <- y ~ .
update(myf, . ~ -. + X)

-- 
HTH,
Bj??rn-Helge Mevik



From ripley at stats.ox.ac.uk  Fri Jun  3 10:36:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 3 Jun 2005 09:36:42 +0100 (BST)
Subject: [R] dot in formula
In-Reply-To: <m0wtpblv4r.fsf@bar.nemo-project.org>
References: <17055.57963.594896.830503@maths.uwa.edu.au>
	<m0wtpblv4r.fsf@bar.nemo-project.org>
Message-ID: <Pine.LNX.4.61.0506030924580.1331@gannet.stats>

On Fri, 3 Jun 2005, Bj?rn-Helge Mevik wrote:

> Adrian Baddeley writes:
>
>> I want to manipulate a formula object, containing the name "."
>> so that "." is replaced by a desired (arbitrary) expression.
>
> How about
>
> myf <- y ~ .
> update(myf, . ~ -. + X)

or just update(myf, . ~ X).  Here "." is not being treated literally (nor 
can it be in the internal formula manipulation code).

If you just want to replace a rhs `.' then there are easier ways, e.g.

> myf <- y ~ .
> myf[[3]] <- quote(X)
> myf
y ~ X

However, I think Adrian might want something like

> myf <- y ~ a + . + x
> update(myf, . ~ -. + X)
y ~ X

I think the best way to do that is by substitute(), e.g.

> substitute(y ~ a + . + x, list("."=quote(X)))
y ~ a + X + x

If that is not enough ideas, perhaps Adrian could give us an example?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Jun  3 11:14:00 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 3 Jun 2005 11:14:00 +0200 (CEST)
Subject: [R] post hoc kruskal wallis
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E912@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E912@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.51.0506031108320.2301@artemis.imbe.med.uni-erlangen.de>


On Thu, 2 Jun 2005, Liaw, Andy wrote:

> By post hoc I guess you meant pairwise comparisons.  You might want to check
> out the npmc package on CRAN.
>

?oneway_test in `coin' has an example, essentially

### Length of YOY Gizzard Shad from Kokosing Lake, Ohio,
### sampled in Summer 1984, Hollander & Wolfe (1999), Table 6.3, page 200
YOY <- data.frame(length = c(46, 28, 46, 37, 32, 41, 42, 45, 38, 44,
                             42, 60, 32, 42, 45, 58, 27, 51, 42, 52,
                             38, 33, 26, 25, 28, 28, 26, 27, 27, 27,
                             31, 30, 27, 29, 30, 25, 25, 24, 27, 30),
                  site = factor(c(rep("I", 10), rep("II", 10),
                                  rep("III", 10), rep("IV", 10))))

### Kruskal-Wallis test, approximate exact p-value
kw <- kruskal_test(length ~ site, data = YOY,
             distribution = approximate(B = 9999))
kw
pvalue(kw)

### Nemenyi-Damico-Wolfe-Dunn test (joint ranking)
### Hollander & Wolfe (1999), page 244
### (where Steel-Dwass results are given)
if (require(multcomp)) {

    NDWD <- oneway_test(length ~ site, data = YOY,
        ytrafo = function(data) trafo(data, numeric_trafo = rank),
        xtrafo = function(data) trafo(data, factor_trafo = function(x)
            model.matrix(~x - 1) %*% t(contrMat(table(x), "Tukey"))),
        teststat = "maxtype", distribution = approximate(B = 90000))

    ### global p-value
    print(pvalue(NDWD))

    ### sites (I = II) != (III = IV) at alpha = 0.01 (page 244)
    print(pvalue(NDWD, adjusted = TRUE))
}


the key step is

xtrafo = function(data) trafo(data, factor_trafo = function(x)
            model.matrix(~x - 1) %*% t(contrMat(table(x), "Tukey")))

where you may specify _any_ contrast matrix, for example just a subset of
all pairwise comparisons you are willing to pay for.

Torsten

> Andy
>
> > From: amberfer at ull.es
> >
> > I??m looking for a program with a post hoc kruskall Wallis
> > test like the Tukey-
> > type non parametric test of ZAR(Biostatistical Analysis
> > 2??Edition). SSPS
> > hasn??t got any non parametric post host test. I don??t know if
> > R has an
> > appropiate post hoc non parametric test. I??m not sure if NDWD
> > test of the
> > package "coin" is an appropiate test or if the package "npmc"
> > is a best option.
> > Thank you
> >
> >
> > Alfredo Berm??dez
> >
> >
> >
> >
> >
> >
> >
> > -------------------------------------------------
> >
> >
> > ----- Fin del mensaje reenviado -----
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From poizot at cnam.fr  Fri Jun  3 11:17:57 2005
From: poizot at cnam.fr (Poizot Emmanuel)
Date: Fri, 03 Jun 2005 11:17:57 +0200
Subject: [R] geometric mean regression
Message-ID: <42A02045.5060407@cnam.fr>

Hi,

is it possible to perform a geometric mean regression with R ?
Thanks.

------------------------------------------------
Emmanuel Poizot
Cnam/Intechmer
B.P. 324
50103 Cherbourg Cedex

Phone (Direct) : (00 33)(0)233887342
Fax : (00 33)(0)233887339
------------------------------------------------


From stefaan.lhermitte at biw.kuleuven.be  Fri Jun  3 12:16:28 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Fri, 03 Jun 2005 12:16:28 +0200
Subject: [R] Simplify formula for iterative programming
Message-ID: <42A02DFC.6050902@biw.kuleuven.be>

Dear R-ians,

I am looking for the simplification of a formula to improve the 
calculation speed of my program. Therefore I want to simplify the 
following formula:

H = Si (Sj ( sqrt [ (Ai - Aj)?? + (Bi - Bj)??  ] ) )

where:
A, B = two vectors (with numerical data) of length n
sqrt = square root
Si = summation over i  (= 0 to n)
Sj = summation over j (= 0 to n)
Ai = element of A with index i
Aj = element of A with index j
Bi = element of B with index i
Bj = element of B with index j

n is not fixed, but it changes with every run for my program. Therefore 
for I am looking for a simplication of h in order to calculate it when 
my A and B get extendend by 1 element (n = n + 1).

I know a computional simplified formula exists for the standard 
deviation (sd) that is much easier in iterative programming. Therefore I 
wondered I anybody knew about analog simplifications to simplify H:

sd = sqrt [ ( Si (Xi - mean(X) )?? ) /n  ]  -> simplified computation -> 
sqrt [ (n * Si( X?? ) - ( Si( X ) )?? )/ n?? ]

This simplied formula is much easier in iterative programming, since I 
don't have to keep every element of X.
E.g.: I have a vector X[1:10]  and I already have caculated Si( X[1:10]?? 
) (I will call this A) and Si( X ) (I will call this B).
When X gets extendend by 1 element (eg. X[11]) it easy fairly simple to 
calculate sd(X[1:11]) without having to reuse the elements of X[1:10].
I just have to calculate:

sd = sqrt [ (n * (A + X[11]??) - (A + X[11]??)?? ) / n?? ]

This is fairly easy in an iterative process, since before we continue 
with the next step we set:
A = (A + X[11]??)
B = (B + X[11])

Can anybody help me to do something comparable for H? Any other help to 
calculate H easily in an iterative process is also welcome!

Kind regards,
Stef



From Dubravko.Dolic at komdat.com  Fri Jun  3 12:54:52 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Fri, 3 Jun 2005 12:54:52 +0200
Subject: [R] Segmentation fault using gentoo linux and mysql
Message-ID: <52D1AC81378E9342947189B041760147254FE2@agentsmith.komdat.intern>

Dear Listmembers

We have a data server with some amounts of data which we try to analyze using R. Now having some MySQL databases with MySQL Ver 12.22 Distrib 4.0.24 and R 2.1.0 compiled from sources on this machine:

Fujitsu Siemens CELSIUS V810
4 GB RAM
Dual AMD Opteron Processor 246
Harddisk via SAN.

(see user details:)
sysname                         "Linux" <- gentoo linux
release		"2.6.11-hardened-r13"                               version "#1 SMP Fri May 27 15:25:55 CEST 2005"                               machine 				"x86_64" 

Editor: Emacs with ESS 5.2.3


While I can connect R to the database (RMySQL/DBI) and retrieve basic information (e.g. SHOW TABLE, DESCRIBE) it is not possible to send SQL Queries. The moment I try to do: res <- dbSendQuery(con, "select * from table") or any other SELECT statement R terminates and I receive a Segmentation Fault.

Is anyone out there having similar problems in this environment. Any hints welcome.

Regards


Dubravko Dolic
Statistical Analyst
Tel:?? ?????? +49 (0)89-55 27 44 - 4630
Fax: ?????? +49 (0)89-55 27 44 - 2463
Email: dubravko.dolic at komdat.com
Komdat GmbH
Nymphenburger Stra??e 86
80636 M??nchen
---------------------------------------------
ONLINE MARKETING THAT WORKS
---------------------------------------------
This electronic message contains information from Komdat Gmb...{{dropped}}



From Nivesh.Pawar at math.uni-leipzig.de  Fri Jun  3 13:59:56 2005
From: Nivesh.Pawar at math.uni-leipzig.de (Nivesh Pawar)
Date: Fri, 3 Jun 2005 13:59:56 +0200 (CEST)
Subject: [R] Need Help - Urgent
Message-ID: <Pine.GSO.4.58.0506031352240.28057@misun113>


Hello,
I am a student and some really urgent help.I am using R software and while
creating a grid with eight elements its showing the error and while if i
do it with seven it says that the memory is not sufficient...
here is the line ...with the error....

> Z<-as.matrix(expand.grid(x,y,x,y,t,s,t,s))

Error in rep.int(rep.int(seq(length = nx), rep.int(rep.fac, nx)), orep) :
        cannot allocate vector of length 1128960000

Is it possible to somehow increase the default size of the matrix?
please help..

THanking you
nivesh pawar



From amberfer at ull.es  Fri Jun  3 14:22:08 2005
From: amberfer at ull.es (amberfer@ull.es)
Date: Fri,  3 Jun 2005 13:22:08 +0100
Subject: [R] post hoc kruskal wallis
Message-ID: <1117801328.42a04b70ee748@correoweb.ccti.ull.es>



I??m looking for a program with a post hoc kruskall Wallis test like the Tukey-
type non parametric test of ZAR(Biostatistical Analysis 2??Edition). SSPS 
hasn??t got any non parametric post host test. I don??t know if R has an 
appropiate post hoc non parametric test. I??m not sure if NDWD test of the 
package ?coin? is an appropiate test or if the package ?npmc? is a best option.
Thank you


Alfredo Berm??dez



From buser at stat.math.ethz.ch  Fri Jun  3 14:35:02 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 3 Jun 2005 14:35:02 +0200
Subject: [R] Simplify formula for iterative programming
In-Reply-To: <42A02DFC.6050902@biw.kuleuven.be>
References: <42A02DFC.6050902@biw.kuleuven.be>
Message-ID: <17056.20086.695346.832894@stat.math.ethz.ch>

Dear Stef

Please check my code below carefully, because it is Friday
afternoon and there might be mistakes in it:

## initialize two random vectors (length n = 100)
a <- rnorm(100)
b <- rnorm(100)
## computation of your formula (see ?outer which is very
## useful here) 
(c <- sum(sqrt(outer(a,a,"-")^2 + outer(b,b,"-")^2)))
## expand a and b to length n+1
a1 <- c(a,rnorm(1))
b1 <- c(b,rnorm(1))
## computation calculating the whole sum
(c1 <- sum(sqrt(outer(a1,a1,"-")^2 + outer(b1,b1,"-")^2)))
## computation using the result of the smaller vectors
c + 2*sum(sqrt((a1 - a1[length(a1)])^2 + (b1 - b1[length(b1)])^2))

Regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


Stefaan Lhermitte writes:
 > Dear R-ians,
 > 
 > I am looking for the simplification of a formula to improve the 
 > calculation speed of my program. Therefore I want to simplify the 
 > following formula:
 > 
 > H = Si (Sj ( sqrt [ (Ai - Aj)?? + (Bi - Bj)??  ] ) )
 > 
 > where:
 > A, B = two vectors (with numerical data) of length n
 > sqrt = square root
 > Si = summation over i  (= 0 to n)
 > Sj = summation over j (= 0 to n)
 > Ai = element of A with index i
 > Aj = element of A with index j
 > Bi = element of B with index i
 > Bj = element of B with index j
 > 
 > n is not fixed, but it changes with every run for my program. Therefore 
 > for I am looking for a simplication of h in order to calculate it when 
 > my A and B get extendend by 1 element (n = n + 1).
 > 
 > I know a computional simplified formula exists for the standard 
 > deviation (sd) that is much easier in iterative programming. Therefore I 
 > wondered I anybody knew about analog simplifications to simplify H:
 > 
 > sd = sqrt [ ( Si (Xi - mean(X) )?? ) /n  ]  -> simplified computation -> 
 > sqrt [ (n * Si( X?? ) - ( Si( X ) )?? )/ n?? ]
 > 
 > This simplied formula is much easier in iterative programming, since I 
 > don't have to keep every element of X.
 > E.g.: I have a vector X[1:10]  and I already have caculated Si( X[1:10]?? 
 > ) (I will call this A) and Si( X ) (I will call this B).
 > When X gets extendend by 1 element (eg. X[11]) it easy fairly simple to 
 > calculate sd(X[1:11]) without having to reuse the elements of X[1:10].
 > I just have to calculate:
 > 
 > sd = sqrt [ (n * (A + X[11]??) - (A + X[11]??)?? ) / n?? ]
 > 
 > This is fairly easy in an iterative process, since before we continue 
 > with the next step we set:
 > A = (A + X[11]??)
 > B = (B + X[11])
 > 
 > Can anybody help me to do something comparable for H? Any other help to 
 > calculate H easily in an iterative process is also welcome!
 > 
 > Kind regards,
 > Stef
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Mugdha.Wagle at STJUDE.ORG  Fri Jun  3 14:40:38 2005
From: Mugdha.Wagle at STJUDE.ORG (Wagle, Mugdha)
Date: Fri, 3 Jun 2005 07:40:38 -0500
Subject: [R] reading tables into R
Message-ID: <F2235647AC878D438F09255C39842FBC0D342B9C@SJMEMXMB03.stjude.sjcrh.local>

I have been using R and Perl. When  I read in a text file using the read.table option, and I try to mathematically manipulate the individual elements in the table, I keep getting an "object is not subsettable" error. If I try to use a different method, it works, but takes too much time(basically, I then need to read in values individually into R instead of as a 2D array, so the number of function calls from Perl to R is very large). Could you suggest another method whereby I could read an entire matrix or a file using an R function call? 
 
Thanks!
 
Sincerely,
Mugdha Wagle,
Hartwell center for Bioinformatics and Biotechnology,
St.Jude Children's Research Hospital



From bitwrit at ozemail.com.au  Sat Jun  4 00:46:37 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 03 Jun 2005 22:46:37 +0000
Subject: [R] Creating datasets
Message-ID: <42A0DDCD.40406@ozemail.com.au>

Hi folks,

Having lost count of the times I have read the relevant section in 
R-exts.html (and by the way, where has that useful file gone? I had to 
look at it on CRAN.), searched for "creating datasets" (this is the 
first time that Jon Baron's excellent search site has let me down) and 
read the idiot's guide for creating packages, I am willing to admit that 
I have failed the test.

First:
Linux (Fedora Core 2)
R-2.1.0

I am attempting to integrate three functions that have been kindly 
donated to the plotrix package by Sander Oom dealing with plotting soil 
texture. Everything works okay, but I want to add a dataset to the 
package. R-exts says that three types of data files are okay:

R code, rectangular tables (e.g. CSV) or files created by save (.rda)

I note that all the "real" packages (oops, bundles) like MASS have three 
files in their data directory that look like an index of the datasets, 
the datasets themselves and something I haven't worked out yet. I 
haven't found a description of how to do that trick.

Thus I created the dataset by reading a CSV data file into a data frame

oksoil<-read.table("oksoil.csv",sep=",")

That data frame works fine in all the functions. Then I saved it:

save(oksoil,file="/home/jim/R/plotrix/data/oksoil.rda")

But when I check the package, the examples don't work:

R CMD check /home/jim/R/plotrix
...
 >  data(oksoil)
Warning in data(oksoil) : data set 'oksoil' not found
 >  # first just show the soil triangle
 >  soil.texture()
 >  # now plot the observations
 >  show.soil.texture(oksoil)
Error in show.soil.texture(oksoil) : Object "oksoil" not found
Execution halted

I promise to try writing an "Idiot's Guide to Creating Datasets" if 
someone can provide a method.

Jim



From maechler at stat.math.ethz.ch  Fri Jun  3 14:57:06 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 3 Jun 2005 14:57:06 +0200
Subject: [R] Creating datasets *IN PACKAGES*
In-Reply-To: <42A0DDCD.40406@ozemail.com.au>
References: <42A0DDCD.40406@ozemail.com.au>
Message-ID: <17056.21410.516985.696416@stat.math.ethz.ch>

Hi Jim,

are you sure you haven't confused  
      *source* packages
with  *installed* (aka binary) packages ??

Source packages' ./data/ subdirectories have no "funny files" in
them.

Martin



From bates at stat.wisc.edu  Fri Jun  3 15:06:24 2005
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 03 Jun 2005 08:06:24 -0500
Subject: [R] Creating datasets
In-Reply-To: <42A0DDCD.40406@ozemail.com.au>
References: <42A0DDCD.40406@ozemail.com.au>
Message-ID: <42A055D0.8000008@stat.wisc.edu>

Jim Lemon wrote:
> Hi folks,
> 
> Having lost count of the times I have read the relevant section in
> R-exts.html (and by the way, where has that useful file gone? I had to
> look at it on CRAN.), searched for "creating datasets" (this is the
> first time that Jon Baron's excellent search site has let me down) and
> read the idiot's guide for creating packages, I am willing to admit that
> I have failed the test.
> 
> First:
> Linux (Fedora Core 2)
> R-2.1.0
> 
> I am attempting to integrate three functions that have been kindly
> donated to the plotrix package by Sander Oom dealing with plotting soil
> texture. Everything works okay, but I want to add a dataset to the
> package. R-exts says that three types of data files are okay:
> 
> R code, rectangular tables (e.g. CSV) or files created by save (.rda)
> 
> I note that all the "real" packages (oops, bundles) like MASS have three
> files in their data directory that look like an index of the datasets,
> the datasets themselves and something I haven't worked out yet. I
> haven't found a description of how to do that trick.

These are created from the original data sets when the package is
installed, provided that the LazyData switch is turned on in the
DESCRIPTION file or the call to R CMD INSTALL.

> Thus I created the dataset by reading a CSV data file into a data frame
> 
> oksoil<-read.table("oksoil.csv",sep=",")
> 
> That data frame works fine in all the functions. Then I saved it:
> 
> save(oksoil,file="/home/jim/R/plotrix/data/oksoil.rda")
> 
> But when I check the package, the examples don't work:
> 
> R CMD check /home/jim/R/plotrix
> ...
>>  data(oksoil)
> Warning in data(oksoil) : data set 'oksoil' not found
>>  # first just show the soil triangle
>>  soil.texture()
>>  # now plot the observations
>>  show.soil.texture(oksoil)
> Error in show.soil.texture(oksoil) : Object "oksoil" not found
> Execution halted

You must put the data set into a source package directory then install
the package using Rcmd INSTALL or R CMD INSTALL.  Some indices are
created during the installation process and it is these that allow R to
find the data sets by name.

> I promise to try writing an "Idiot's Guide to Creating Datasets" if
> someone can provide a method.



From murdoch at stats.uwo.ca  Fri Jun  3 15:01:27 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 03 Jun 2005 09:01:27 -0400
Subject: [R] Creating datasets
In-Reply-To: <42A0DDCD.40406@ozemail.com.au>
References: <42A0DDCD.40406@ozemail.com.au>
Message-ID: <42A054A7.3090405@stats.uwo.ca>

On 6/3/2005 6:46 PM, Jim Lemon wrote:
> Hi folks,
> 
> Having lost count of the times I have read the relevant section in 
> R-exts.html (and by the way, where has that useful file gone? I had to 
> look at it on CRAN.),

Should be in R_HOME/doc/manual.

  searched for "creating datasets" (this is the
> first time that Jon Baron's excellent search site has let me down) and 
> read the idiot's guide for creating packages, I am willing to admit that 
> I have failed the test.
> 
> First:
> Linux (Fedora Core 2)
> R-2.1.0
> 
> I am attempting to integrate three functions that have been kindly 
> donated to the plotrix package by Sander Oom dealing with plotting soil 
> texture. Everything works okay, but I want to add a dataset to the 
> package. R-exts says that three types of data files are okay:
> 
> R code, rectangular tables (e.g. CSV) or files created by save (.rda)
> 
> I note that all the "real" packages (oops, bundles) like MASS have three 
> files in their data directory that look like an index of the datasets, 
> the datasets themselves and something I haven't worked out yet. I 
> haven't found a description of how to do that trick.
> 
> Thus I created the dataset by reading a CSV data file into a data frame
> 
> oksoil<-read.table("oksoil.csv",sep=",")
> 
> That data frame works fine in all the functions. Then I saved it:
> 
> save(oksoil,file="/home/jim/R/plotrix/data/oksoil.rda")
> 
> But when I check the package, the examples don't work:
> 
> R CMD check /home/jim/R/plotrix
> ...
>  >  data(oksoil)
> Warning in data(oksoil) : data set 'oksoil' not found

This looks like it should have worked.  I don't know what went wrong for 
you.  Can you look in plotrix.Rcheck/plotrix/data and confirm that 
oksoil.rda got installed there?

Duncan Murdoch



From sdavis2 at mail.nih.gov  Fri Jun  3 15:03:49 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 3 Jun 2005 09:03:49 -0400
Subject: [R] reading tables into R
In-Reply-To: <F2235647AC878D438F09255C39842FBC0D342B9C@SJMEMXMB03.stjude.sjcrh.local>
References: <F2235647AC878D438F09255C39842FBC0D342B9C@SJMEMXMB03.stjude.sjcrh.local>
Message-ID: <21bb2f3022705a8ed024cb4b214b34e8@mail.nih.gov>


On Jun 3, 2005, at 8:40 AM, Wagle, Mugdha wrote:

> I have been using R and Perl. When  I read in a text file using the 
> read.table option, and I try to mathematically manipulate the 
> individual elements in the table, I keep getting an "object is not 
> subsettable" error. If I try to use a different method, it works, but 
> takes too much time(basically, I then need to read in values 
> individually into R instead of as a 2D array, so the number of 
> function calls from Perl to R is very large). Could you suggest 
> another method whereby I could read an entire matrix or a file using 
> an R function call?
>

Mugdha,

I think you will probably have to be more specific.   Could you give an 
example of the data format, the commands you used to load it that 
didn't work, and those that did and explain how perl comes into this?  
Also, knowing what OS and version of R you are using is quite helpful.

Sean



From andy_liaw at merck.com  Fri Jun  3 15:04:20 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Jun 2005 09:04:20 -0400
Subject: [R] Creating datasets
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E914@usctmx1106.merck.com>

RSiteSearch("creating datasets", restrict="doc") gives me R-exts.html as the
first hit...

I did pretty much the same thing you did and had no problem.  In the data/
directory of the source package I only have the one .rda file and nothing
else.  Did you try installing the package and test it before you run check?

Andy

> From: Jim Lemon
> 
> Hi folks,
> 
> Having lost count of the times I have read the relevant section in 
> R-exts.html (and by the way, where has that useful file gone? 
> I had to 
> look at it on CRAN.), searched for "creating datasets" (this is the 
> first time that Jon Baron's excellent search site has let me 
> down) and 
> read the idiot's guide for creating packages, I am willing to 
> admit that 
> I have failed the test.
> 
> First:
> Linux (Fedora Core 2)
> R-2.1.0
> 
> I am attempting to integrate three functions that have been kindly 
> donated to the plotrix package by Sander Oom dealing with 
> plotting soil 
> texture. Everything works okay, but I want to add a dataset to the 
> package. R-exts says that three types of data files are okay:
> 
> R code, rectangular tables (e.g. CSV) or files created by save (.rda)
> 
> I note that all the "real" packages (oops, bundles) like MASS 
> have three 
> files in their data directory that look like an index of the 
> datasets, 
> the datasets themselves and something I haven't worked out yet. I 
> haven't found a description of how to do that trick.
> 
> Thus I created the dataset by reading a CSV data file into a 
> data frame
> 
> oksoil<-read.table("oksoil.csv",sep=",")
> 
> That data frame works fine in all the functions. Then I saved it:
> 
> save(oksoil,file="/home/jim/R/plotrix/data/oksoil.rda")
> 
> But when I check the package, the examples don't work:
> 
> R CMD check /home/jim/R/plotrix
> ...
>  >  data(oksoil)
> Warning in data(oksoil) : data set 'oksoil' not found
>  >  # first just show the soil triangle
>  >  soil.texture()
>  >  # now plot the observations
>  >  show.soil.texture(oksoil)
> Error in show.soil.texture(oksoil) : Object "oksoil" not found
> Execution halted
> 
> I promise to try writing an "Idiot's Guide to Creating Datasets" if 
> someone can provide a method.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Mugdha.Wagle at STJUDE.ORG  Fri Jun  3 15:18:30 2005
From: Mugdha.Wagle at STJUDE.ORG (Wagle, Mugdha)
Date: Fri, 3 Jun 2005 08:18:30 -0500
Subject: [R] reading tables into R.    .
Message-ID: <F2235647AC878D438F09255C39842FBC0D342B9D@SJMEMXMB03.stjude.sjcrh.local>

Hi,
 
The file I am reading is a text file, whose contents are a matrix that has 15 rows and 58 columns. The first row has column names, and the first column has row names, so the format is correct as far as using read.table is concerned. The other values in the table are all float values (numeric). So when I read in the file using data1 <- read.table("HAL001_HAL0015_Signals.txt"), it gets read in as a table, but when I try to manipulate an individual value as follows: data1[2 ,2] <- log(data1[2 ,2]+20) , I get the "object is not subsettable" message. This error occurs when I use R only...Perl is not being used at this point. My script needs to take input from an HTML page (using a CGI interface) which will be the name of the file to be passed onto R as a table. Since read.table isn't working within R itself, I haven't used it for the function call from Perl to R yet. Instead, I have been making repeated function calls using a loop in Perl, to access other R and Bioconductor functions that I need such as t.test and ANOVA. This is very time-consuming, however. I am working in a Linux environment, and using
 R 2.1.0
 
Thanks for any help and suggestions!
 
Mugdha Wagle
 
________________________________

From: Sean Davis [mailto:sdavis2 at mail.nih.gov]
Sent: Fri 6/3/2005 8:03 AM
To: Wagle, Mugdha
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] reading tables into R. .




On Jun 3, 2005, at 8:40 AM, Wagle, Mugdha wrote:

> I have been using R and Perl. When  I read in a text file using the
> read.table option, and I try to mathematically manipulate the
> individual elements in the table, I keep getting an "object is not
> subsettable" error. If I try to use a different method, it works, but
> takes too much time(basically, I then need to read in values
> individually into R instead of as a 2D array, so the number of
> function calls from Perl to R is very large). Could you suggest
> another method whereby I could read an entire matrix or a file using
> an R function call?
>

Mugdha,

I think you will probably have to be more specific.   Could you give an
example of the data format, the commands you used to load it that
didn't work, and those that did and explain how perl comes into this? 
Also, knowing what OS and version of R you are using is quite helpful.

Sean



From reid_huntsinger at merck.com  Fri Jun  3 15:31:38 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 3 Jun 2005 09:31:38 -0400
Subject: [R] Need Help - Urgent
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9473@uswpmx00.merck.com>

You're asking for a matrix with
length(x)^2*length(y)^2*length(s)^2*length(t)^2 rows. What are these
lengths? Is that what you're expecting? Perhaps you want the distinct factor
levels in x,y,s,t rather than x,y,s,t themselves?

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nivesh Pawar
Sent: Friday, June 03, 2005 8:00 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Need Help - Urgent



Hello,
I am a student and some really urgent help.I am using R software and while
creating a grid with eight elements its showing the error and while if i
do it with seven it says that the memory is not sufficient...
here is the line ...with the error....

> Z<-as.matrix(expand.grid(x,y,x,y,t,s,t,s))

Error in rep.int(rep.int(seq(length = nx), rep.int(rep.fac, nx)), orep) :
        cannot allocate vector of length 1128960000

Is it possible to somehow increase the default size of the matrix?
please help..

THanking you
nivesh pawar

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From chuai_juok at hotmail.com  Fri Jun  3 15:37:37 2005
From: chuai_juok at hotmail.com (Ukech U. Kidi)
Date: Fri, 03 Jun 2005 13:37:37 +0000
Subject: [R] RE: GARCH (1 , 1), Hill estimator of alpha, Pareto estimator
In-Reply-To: <BAY19-F216F41CE90FD039B431F47FD070@phx.gbl>
Message-ID: <BAY19-F322C29D18D64C62C4DF6DBFD070@phx.gbl>

Dear R users,

Could you please help me out. I am unable to formulate a model in R to 
graphs a GARCH (1 , 1) model, the Hill estimator (of alpha), and the Pareto 
estimator.

I have a financial time series data from the DAX and CAC40.  I just got 
introduce to R. I am working on a paper which must be worked from R.

I successfully imported the data into R. I also succeeded to produce te 
qq-plots  and histograms from the data.

My difficulty at the moment lies where I could not go further to plot the 
GARCH plots, the Hill estimator of alpha and the Pareto estimator.

Reading from the FAQs of this list, I attempted the following and wasn't 
succeesful. Could anyone show me the steps that I might have omited.

Check my errors from the following.

library(stats)

data(DAX_CAC)

Warning message:

Data set 'DAX_CAC' not found in: data(DAX_CAC)

data("DAX_CAC")


Warning message:

Data set 'DAX_CAC' not found in: data("DAX_CAC")

    dax<- diff(log(DAX_CAC$DAX[1:1865]))


    m1<- garch(dax)


Error: couldn't find function "garch"

    m1<- garch(dax[1:1865])


Error: couldn't find function "garch"

    m1<- garch(dax[1:1865])



Thank you in advance,

Ukech U. kidi



From hb at maths.lth.se  Fri Jun  3 15:57:56 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 03 Jun 2005 15:57:56 +0200
Subject: [R] reading tables into R.    .
In-Reply-To: <F2235647AC878D438F09255C39842FBC0D342B9D@SJMEMXMB03.stjude.sjcrh.local>
References: <F2235647AC878D438F09255C39842FBC0D342B9D@SJMEMXMB03.stjude.sjcrh.local>
Message-ID: <42A061E4.9030006@maths.lth.se>

Hi, a good start is to verify that what you have read meets your 
expectations. Try

  str(data1)
  dim(data1)
  print(data1)

and make sure you read ?read.table. There are plenty of *arguments* that 
affects the way R reads your data text file.

/Henrik

Wagle, Mugdha wrote:
> Hi,
>  
> The file I am reading is a text file, whose contents are a matrix that has 15 rows and 58 columns. The first row has column names, and the first column has row names, so the format is correct as far as using read.table is concerned. The other values in the table are all float values (numeric). So when I read in the file using data1 <- read.table("HAL001_HAL0015_Signals.txt"), it gets read in as a table, but when I try to manipulate an individual value as follows: data1[2 ,2] <- log(data1[2 ,2]+20) , I get the "object is not subsettable" message. This error occurs when I use R only...Perl is not being used at this point. My script needs to take input from an HTML page (using a CGI interface) which will be the name of the file to be passed onto R as a table. Since read.table isn't working within R itself, I haven't used it for the function call from Perl to R yet. Instead, I have been making repeated function calls using a loop in Perl, to access other R and Bioconductor fun
ct!
>  ions that I need such as t.test and ANOVA. This is very time-consuming, however. I am working in a Linux environment, and using
>  R 2.1.0
>  
> Thanks for any help and suggestions!
>  
> Mugdha Wagle
>  
> ________________________________
> 
> From: Sean Davis [mailto:sdavis2 at mail.nih.gov]
> Sent: Fri 6/3/2005 8:03 AM
> To: Wagle, Mugdha
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] reading tables into R. .
> 
> 
> 
> 
> On Jun 3, 2005, at 8:40 AM, Wagle, Mugdha wrote:
> 
> 
>>I have been using R and Perl. When  I read in a text file using the
>>read.table option, and I try to mathematically manipulate the
>>individual elements in the table, I keep getting an "object is not
>>subsettable" error. If I try to use a different method, it works, but
>>takes too much time(basically, I then need to read in values
>>individually into R instead of as a 2D array, so the number of
>>function calls from Perl to R is very large). Could you suggest
>>another method whereby I could read an entire matrix or a file using
>>an R function call?
>>
> 
> 
> Mugdha,
> 
> I think you will probably have to be more specific.   Could you give an
> example of the data format, the commands you used to load it that
> didn't work, and those that did and explain how perl comes into this? 
> Also, knowing what OS and version of R you are using is quite helpful.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From macq at llnl.gov  Fri Jun  3 16:05:25 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 3 Jun 2005 07:05:25 -0700
Subject: [R] reading tables into R
In-Reply-To: <F2235647AC878D438F09255C39842FBC0D342B9C@SJMEMXMB03.stjude.sjcrh.local>
References: <F2235647AC878D438F09255C39842FBC0D342B9C@SJMEMXMB03.stjude.sjcrh.local>
Message-ID: <p0621020bbec612e01d97@[128.115.153.6]>

You will have to be more specific, if you want specific help.

However, when I get the "object is not subsettable" error, it is 
because I have accidentally referred to the wrong object.

For example,

>  cespl[1,4]
Error in cespl[1, 4] : object is not subsettable
>  mode(cespl)
[1] "function"

It is not possible to take a subset of a function; this is an action 
that makes no sense.

-Don


At 7:40 AM -0500 6/3/05, Wagle, Mugdha wrote:
>I have been using R and Perl. When  I read in a text file using the 
>read.table option, and I try to mathematically manipulate the 
>individual elements in the table, I keep getting an "object is not 
>subsettable" error. If I try to use a different method, it works, 
>but takes too much time(basically, I then need to read in values 
>individually into R instead of as a 2D array, so the number of 
>function calls from Perl to R is very large). Could you suggest 
>another method whereby I could read an entire matrix or a file using 
>an R function call?
>
>Thanks!
>
>Sincerely,
>Mugdha Wagle,
>Hartwell center for Bioinformatics and Biotechnology,
>St.Jude Children's Research Hospital
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From gchappi at gmail.com  Fri Jun  3 16:28:29 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Fri, 3 Jun 2005 16:28:29 +0200
Subject: [R] plots from batchfile on windows
Message-ID: <47fce06505060307281e15b026@mail.gmail.com>

Hi,

On windows I'd like to run a batchfile that leaves me a plot. As a
test I have the scriptfile "test.r" which only contains:

    x <- 1:10;
    y <- sample( 10 )
    plot( x, y )

Now I tried the following (but nothing worked):

a) "R --vanilla < test.r" in the cmd window, => that doesn't give me a plot
b) the same, but I added "bitmap(file="outplot.png")" in the
scriptfile => Here I get an error that gswin32c.exe was not found.
(Maybe I need to install ghostscript???)
c) some attempts with Rgui.exe => I didn't arrive to pass a command argument.

It would be nice, to get some hints, how this could be done. I did
search the newsgroup archive, but unfortunately didn't find something.

Thanks and best regards,
Hans-Peter



From ggrothendieck at gmail.com  Fri Jun  3 16:32:52 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Jun 2005 10:32:52 -0400
Subject: [R] plots from batchfile on windows
In-Reply-To: <47fce06505060307281e15b026@mail.gmail.com>
References: <47fce06505060307281e15b026@mail.gmail.com>
Message-ID: <971536df05060307325eb41c4f@mail.gmail.com>

On 6/3/05, Hans-Peter <gchappi at gmail.com> wrote:
> Hi,
> 
> On windows I'd like to run a batchfile that leaves me a plot. As a
> test I have the scriptfile "test.r" which only contains:
> 
>    x <- 1:10;
>    y <- sample( 10 )
>    plot( x, y )
> 
> Now I tried the following (but nothing worked):
> 
> a) "R --vanilla < test.r" in the cmd window, => that doesn't give me a plot
> b) the same, but I added "bitmap(file="outplot.png")" in the
> scriptfile => Here I get an error that gswin32c.exe was not found.
> (Maybe I need to install ghostscript???)
> c) some attempts with Rgui.exe => I didn't arrive to pass a command argument.
> 
> It would be nice, to get some hints, how this could be done. I did
> search the newsgroup archive, but unfortunately didn't find something.
> 

This was discussed previously this week.  Look right at the end of:

https://www.stat.math.ethz.ch/pipermail/r-help/2005-June/071147.html



From slist at oomvanlieshout.net  Fri Jun  3 16:33:44 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 03 Jun 2005 16:33:44 +0200
Subject: [R] How to 'de-cross' a table?
Message-ID: <42A06A48.5060609@oomvanlieshout.net>

Dear R users,

I have received a table in the following format:

id  a   b  c1  c2  d1  d2
1   1   1  65  97  78  98
2   1   2  65  97  42  97
3   2   1  65  68  97  98
4   2   2  65  97  97  98

Factors of the design are: a, b, and e, where e has levels c and d. The
levels c and d then have 2 replicates (r) each.

Now I would like to get:

id  a   b   e   r  value
1   1   1   c   1  65
2   1   1   c   2  97
3   1   1   d   1  78
4   1   1   d   2  98

Any suggestions on how to tackle this? I'm not sure what the 'task' is
called, so struggle to effectively search the web or R help.

Thanks,

Sander.


--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From h.andersson at nioo.knaw.nl  Fri Jun  3 16:38:24 2005
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Fri, 03 Jun 2005 16:38:24 +0200
Subject: [R] usign system() both quietly and quickly
Message-ID: <d7pplk$sqo$1@sea.gmane.org>

I try to run an external application (A diffusive transport model 
programmed in Fortran 95) in a loop using the approach

for(i in 1:100) {
1. write parameters[i] to inputfile for application
2. Run application using system("application.exe")
3. Read output written to a file by application to a list
}


The thing that annoys me is I can not figure out how to suppress the 
output from this application to the screen. If I use 
system(...,intern=T) it takes much longer to complete and is obviously 
not what I want.

Any ideas?

Have a nice weekend!

R running under ESS/Emacs
--------------------------
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From hvillalo at ipn.mx  Fri Jun  3 16:39:41 2005
From: hvillalo at ipn.mx (Hector Villalobos Ortiz)
Date: Fri, 03 Jun 2005 08:39:41 -0600
Subject: [R] Reading biplot function's source code
Message-ID: <web-23659546@ipn.mx>

Hi everybody,

Excuse me for this silly question, but after searching the 
help archives I'm still unable to find my way to read the 
source code of the "biplot" function in R.

I have installed the mvbutils package, and tried:
> fixr(biplot)

which only gives me:

function (x, ...)
UseMethod("biplot")

with no further details....

I'd like to read the code to see if it is possible to 
display arrows only for some "significant" variables to 
avoid cluttering. Thank you in advance.

-- 
H??ctor Villalobos <hvillalo at ipn.mx>
  IPN-CICIMAR. A.P. 592. Col. Centro
  La Paz, Baja California Sur, M??XICO. 23000



From gchappi at gmail.com  Fri Jun  3 16:50:49 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Fri, 3 Jun 2005 16:50:49 +0200
Subject: [R] plots from batchfile on windows
In-Reply-To: <971536df05060307325eb41c4f@mail.gmail.com>
References: <47fce06505060307281e15b026@mail.gmail.com>
	<971536df05060307325eb41c4f@mail.gmail.com>
Message-ID: <47fce0650506030750e22e506@mail.gmail.com>

On 6/3/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> This was discussed previously this week.  Look right at the end of:
> https://www.stat.math.ethz.ch/pipermail/r-help/2005-June/071147.html

I had to add a "win.graph..." command, now it works:

    x <- 1:10;
    y <- sample( 10 )
    win.graph(width = 7, height = 7, pointsize = 12)
    plot( x, y )
    savePlot( filename = "test", type = "png" )

Great, thanks a lot for the tipp!

Hans-Peter



-- 
Regards,
Hans-Peter



From bxc at steno.dk  Fri Jun  3 16:50:51 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Fri, 3 Jun 2005 16:50:51 +0200
Subject: [R] plots from batchfile on windows
Message-ID: <40D3930AC1C8EA469E39536E5BC8083545AF2A@EXDKBA021.corp.novocorp.net>

Just open a device before you plot:

pdf( "plotfile.pdf" )
plot( x, y )
dev.off()

also have a look at:

?Devices

Best
Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Friday, June 03, 2005 4:33 PM
> To: Hans-Peter
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] plots from batchfile on windows
> 
> 
> On 6/3/05, Hans-Peter <gchappi at gmail.com> wrote:
> > Hi,
> > 
> > On windows I'd like to run a batchfile that leaves me a plot. As a 
> > test I have the scriptfile "test.r" which only contains:
> > 
> >    x <- 1:10;
> >    y <- sample( 10 )
> >    plot( x, y )
> > 
> > Now I tried the following (but nothing worked):
> > 
> > a) "R --vanilla < test.r" in the cmd window, => that 
> doesn't give me a 
> > plot
> > b) the same, but I added "bitmap(file="outplot.png")" in the
> > scriptfile => Here I get an error that gswin32c.exe was not found.
> > (Maybe I need to install ghostscript???)
> > c) some attempts with Rgui.exe => I didn't arrive to pass a 
> command argument.
> > 
> > It would be nice, to get some hints, how this could be done. I did 
> > search the newsgroup archive, but unfortunately didn't find 
> something.
> > 
> 
> This was discussed previously this week.  Look right at the end of:
> 
https://www.stat.math.ethz.ch/pipermail/r-help/2005-June/071147.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From stefaan.lhermitte at biw.kuleuven.be  Fri Jun  3 16:54:52 2005
From: stefaan.lhermitte at biw.kuleuven.be (Stefaan Lhermitte)
Date: Fri, 03 Jun 2005 16:54:52 +0200
Subject: [R] Simplify formula for iterative programming
In-Reply-To: <17056.20086.695346.832894@stat.math.ethz.ch>
References: <42A02DFC.6050902@biw.kuleuven.be>
	<17056.20086.695346.832894@stat.math.ethz.ch>
Message-ID: <42A06F3C.8090805@biw.kuleuven.be>

Dear Christoph,

Thanks for your help!
I checked it in R and it works if we extend a and b with one element for 
each run. Unfortunately, I actually want to merge two vectors and then 
calculate the H for the merge. It is consequently no addition of 1 
element but an addition of x elements. I did not mention it in my first 
post in order not to make it too complicated.

The second problem that still remains is that I have to keep the 
original values of my original vectors. In the example I gave with sd  
only a calculated value and the new value are needed. The iterative 
process does not the need previously processed values of A anymore. I 
hoped something comaprable was possible for H.

Thanks anyway for your help!

Kind regards,
Stef



From gavin.simpson at ucl.ac.uk  Fri Jun  3 17:10:35 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 03 Jun 2005 16:10:35 +0100
Subject: [R] Reading biplot function's source code
In-Reply-To: <web-23659546@ipn.mx>
References: <web-23659546@ipn.mx>
Message-ID: <42A072EB.7030006@ucl.ac.uk>

Hector Villalobos Ortiz wrote:
> Hi everybody,
> 
> Excuse me for this silly question, but after searching the help archives 
> I'm still unable to find my way to read the source code of the "biplot" 
> function in R.
> 
> I have installed the mvbutils package, and tried:
> 
>> fixr(biplot)
> 
> 
> which only gives me:
> 
> function (x, ...)
> UseMethod("biplot")
> 
> with no further details....
> 
> I'd like to read the code to see if it is possible to display arrows 
> only for some "significant" variables to avoid cluttering. Thank you in 
> advance.
> 

That is the code for biplot()! biplot() is a generic function, and 
biplot methods can be written to draw biplots of results produced by 
different functions.

 > methods(biplot)
[1] biplot.default*  biplot.prcomp*   biplot.princomp*

Shows us that currently there are 3 methods for biplot in my 
environment. The * indicates that the code for those methods are not 
visible to users directly.

getAnywhere(biplot.prcomp) is one way of viewing the code.

HTH

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpsonATNOSPAMucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From roger.bos at gmail.com  Fri Jun  3 17:24:43 2005
From: roger.bos at gmail.com (roger bos)
Date: Fri, 3 Jun 2005 11:24:43 -0400
Subject: [R] Need Help - Urgent
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A9473@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A9473@uswpmx00.merck.com>
Message-ID: <1db72680050603082465a64818@mail.gmail.com>

Those sizes are smaller than I thought they would be, but you repeat
the variables more than once in expand.grid, so its still large
overall.  Maybe you can try a simpler example first and then build up
to what you really want once you get the simpler one working.  The
underlying problem is that the expand.grid is so large you don't have
enough memory to store it.  Its easy to do this since because of the
nature of expand.grid.

Your other option is to post a new message explaining the end goal and
asking for suggestions on how to get there.

Thanks,

Roger


On 6/3/05, Huntsinger, Reid <reid_huntsinger at merck.com> wrote:
> You're asking for a matrix with
> length(x)^2*length(y)^2*length(s)^2*length(t)^2 rows. What are these
> lengths? Is that what you're expecting? Perhaps you want the distinct factor
> levels in x,y,s,t rather than x,y,s,t themselves?
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Nivesh Pawar
> Sent: Friday, June 03, 2005 8:00 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Need Help - Urgent
> 
> 
> 
> Hello,
> I am a student and some really urgent help.I am using R software and while
> creating a grid with eight elements its showing the error and while if i
> do it with seven it says that the memory is not sufficient...
> here is the line ...with the error....
> 
> > Z<-as.matrix(expand.grid(x,y,x,y,t,s,t,s))
> 
> Error in rep.int(rep.int(seq(length = nx), rep.int(rep.fac, nx)), orep) :
>        cannot allocate vector of length 1128960000
> 
> Is it possible to somehow increase the default size of the matrix?
> please help..
> 
> THanking you
> nivesh pawar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From charles-r-nospam at plessy.org  Fri Jun  3 17:31:23 2005
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Sat, 4 Jun 2005 00:31:23 +0900
Subject: [R] How to 'de-cross' a table?
In-Reply-To: <42A06A48.5060609@oomvanlieshout.net>
References: <42A06A48.5060609@oomvanlieshout.net>
Message-ID: <20050603153123.GA19566@kunpuu.plessy.org>

On Fri, Jun 03, 2005 at 04:33:44PM +0200, Sander Oom wrote :
> Dear R users,
> 
> I have received a table in the following format:
> 
> id  a   b  c1  c2  d1  d2
> 1   1   1  65  97  78  98
> 2   1   2  65  97  42  97
> 3   2   1  65  68  97  98
> 4   2   2  65  97  97  98
> 
> Factors of the design are: a, b, and e, where e has levels c and d. The
> levels c and d then have 2 replicates (r) each.
> 
> Now I would like to get:
> 
> id  a   b   e   r  value
> 1   1   1   c   1  65
> 2   1   1   c   2  97
> 3   1   1   d   1  78
> 4   1   1   d   2  98

I think that "reshape" is the tool you are looking for.

-- 
Charles



From jsorkin at grecc.umaryland.edu  Fri Jun  3 18:00:27 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Fri, 03 Jun 2005 12:00:27 -0400
Subject: [R] Load package using R code rather than GUI Package menu
Message-ID: <s2a04682.083@grecc.umaryland.edu>

Is there any way to load a package using R code rather than the Load
Package command in the Package menu? I would like to load the boot
package. 
R.2.1.0 Patched
Win 2k.

Thanks,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119 
- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From v.demartino2 at virgilio.it  Fri Jun  3 18:07:13 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Fri, 3 Jun 2005 18:07:13 +0200
Subject: [R] Problems of performance between linux and freebsd
Message-ID: <429C8F5C00005A46@ims3e.cp.tin.it>

Dear All,
I posted the following message to the Freebsd-questions mailing 
=================================================================
<snip>
On my laptop I've being using linux for some years now and "landed" at last
to 
the gentoo distribution which I tuned for working with the statistical 
software R and the bunch of TeX programs such as latex, pdflatex, context
and 
the likes (this absorb 95% of my activity on the linux box).
The main action I took with gentoo was to compile everything from scratch
for 
my pc hardware, limiting the use of X & kde using all the above mentioned

software prevailingly in a simple console.

Now in a partition of the same laptop I installed freebsd 5.4, set 
softupdates, dma=1, and the same pieces of software as those I had installed

under linux using the ports, therefore, compiling everything for my pc 
hardware, but, owing to my poor knowledge of Freebsd I'm experiencing some

problems.

In a nutshell I prepared the following test files:

1) a ConTeXt file full of long (useless) chapters and pictures to run with

texexec;
2) An R program which reads a long text file, puts the data in dataframes,

then loops to accomplish many repetitive mathematical calculations.

Now running the test  files in a console linux turns out to be roughly 20-25%

fast
er than freebsd. Particulary slow results the loop phase of the R source

code that while running slows down and seems to clog in the end. The slowing

down is also perceptible under linux but not at the same level.

Admiring the "ordered and unitary layout" of freebsd, the logical way of

setting it up, the richness of applications,  I would like to line the 
freebsd installation up to the linux one, improving its performance at most,

not necessarily making it faster than linux.

How could I obtain those improvements? What could I do to speed those programs

under freebsd?

</snip>

Now I can be more precise about R.
i) I downloaded the tarball of R 2.0.1 from CRAN;
ii) I compiled the **same source *** source by means of ./configure, make,
make install both on gentoo and on freebsd which share the same make.conf
to optimize for a pentium4 and -O2 flag.
iii) I created 2 R test programs one (simula.r) as described above at point
2) and the other (test.R) using the previous and adding some moreR code
usin the library nnet and looping desperately with it (see attached files).

simula.r takes 29 sec to be executed under win xp, 25 sec under linux gentoo
and 26 under freebsd 5.4, all comparable. On the contrary the program test.R
takes 93 sec under win xp, 85 sec under linux gentoo and an unbelievable
165 sec under freebsd. 
What I noticed is that test.R produces a lot of output while looping the
nnet section of the program which under win xp you can only see at end of
the run, under a gentoo text-console flows happily & briskly on the screen
whilst under a freebsd text-console it starts happily, slows down in the
middle of the looping process and clogs in the end. In a nutshell freebsd
takes  twice as much the time of the other 2 OSs to execute the program.
Sorry for my being so detailed.
What shall I do?
Vittorio
   

 

 


From maarranz at tol-project.org  Fri Jun  3 19:11:02 2005
From: maarranz at tol-project.org (Miguel A. Arranz)
Date: Fri, 3 Jun 2005 19:11:02 +0200
Subject: [R] Load package using R code rather than GUI Package menu
In-Reply-To: <s2a04682.083@grecc.umaryland.edu>
References: <s2a04682.083@grecc.umaryland.edu>
Message-ID: <200506031911.02560.maarranz@tol-project.org>

library(boot)


On Friday 03 June 2005 18:00, John Sorkin wrote:
> Is there any way to load a package using R code rather than the Load
> Package command in the Package menu? I would like to load the boot
> package.
> R.2.1.0 Patched
> Win 2k.
>
> Thanks,
> John
>
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>
> 410-605-7119
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
*************************
Miguel A. Arranz
Tol-Project
maarranz at tol-project.org



From depire at inrets.fr  Fri Jun  3 18:14:23 2005
From: depire at inrets.fr (Depire Alexandre)
Date: Fri, 3 Jun 2005 18:14:23 +0200
Subject: [R] Histogram of multiple series on one histogram
In-Reply-To: <1117623359.13574.3.camel@localhost>
References: <1117623359.13574.3.camel@localhost>
Message-ID: <200506031814.23789.depire@inrets.fr>

Hello,
I have three sample, for example
	a<-c(10,20,10,20,30)
	b<-c(10,20,20,30,30)
	c<-c(20,20,10)

I would like to have only one histogram with these series,
I try the following code:
	hist(a)
	hist(b,add=TRUE,col="red")
	hist(c,add=TRUE,col="green")
but it's not that I want 

I would like an histogram with, from left to right, count of "10" for "a", 
count of "10" for "b", count of "10" for "c", count of "20" for "a", count of 
"20" for "b", ... not overlapped

Thanks


-- 
----------------
Alexandre DEPIRE
INRETS / GARIG
Tel.: (+33) 01 47 40 71 66
Fax: (+33) 01 45 47 56 06
2 av. du g??n??ral Malleret-Joinville
94114 Arcueil - France



From marco.zucchelli at biosci.ki.se  Fri Jun  3 18:17:18 2005
From: marco.zucchelli at biosci.ki.se (Marco Zucchelli)
Date: Fri, 3 Jun 2005 18:17:18 +0200
Subject: [R] help with  guiDlgOpen
Message-ID: <001101c56857$b8f40350$ec6eed82@pizero>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050603/8877578b/attachment.pl

From schulerm at bc.edu  Fri Jun  3 18:22:52 2005
From: schulerm at bc.edu (Mike Schuler)
Date: Fri, 03 Jun 2005 12:22:52 -0400
Subject: [R] Hclust question
Message-ID: <42A083DC.5020700@bc.edu>

Hey,

I am running hclust on several different distance matrices and I have a 
question thats more about labeling. I've been looking for a way to label 
the edge values on the graph with their distances between them. I've 
been looking through the documentation and I haven't found anything yet. 
Anyone know if there is a way to plot 'hclust' graphs with such edge 
values? Or convert it to another type that can have its edge values labeled?

And to clarify what I mean by edge values, I have two pictures 
describing what I mean. First, 
http://clavius.bc.edu/~schulerm/images/test.gif [3.8k] is what I 
currently have. What I'm looking for is something similar to this: 
http://clavius.bc.edu/~schulerm/images/test2.gif [5.1k]. I arbitrarily 
put in values on the edges with the Gimp but I'm looking for some way to 
do that on my plots of 'hclust'.

Thanks in advance
Mike Schuler



From tlumley at u.washington.edu  Fri Jun  3 18:32:31 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 3 Jun 2005 09:32:31 -0700 (PDT)
Subject: [R] How to 'de-cross' a table?
In-Reply-To: <42A06A48.5060609@oomvanlieshout.net>
References: <42A06A48.5060609@oomvanlieshout.net>
Message-ID: <Pine.A41.4.61b.0506030929490.263790@homer12.u.washington.edu>

On Fri, 3 Jun 2005, Sander Oom wrote:

> Dear R users,
>
> I have received a table in the following format:
>
> id  a   b  c1  c2  d1  d2
> 1   1   1  65  97  78  98
> 2   1   2  65  97  42  97
> 3   2   1  65  68  97  98
> 4   2   2  65  97  97  98
>
> Factors of the design are: a, b, and e, where e has levels c and d. The
> levels c and d then have 2 replicates (r) each.
>
> Now I would like to get:
>
> id  a   b   e   r  value
> 1   1   1   c   1  65
> 2   1   1   c   2  97
> 3   1   1   d   1  78
> 4   1   1   d   2  98
>
> Any suggestions on how to tackle this? I'm not sure what the 'task' is
> called, so struggle to effectively search the web or R help.
>

reshape() is the probably function.  It seems you need to use it twice, 
for the two levels of uncrossing
> d1<-reshape(d, direction="long", 
varying=list(c("c1","d1"),c("c2","d2")), 
v.names=c("rep1","rep2"),timevar="r", times=c("c","d"))
> d1
     id a b r rep1 rep2
1.c  1 1 1 c   65   97
2.c  2 1 2 c   65   97
3.c  3 2 1 c   65   68
4.c  4 2 2 c   65   97
1.d  1 1 1 d   78   98
2.d  2 1 2 d   42   97
3.d  3 2 1 d   97   98
4.d  4 2 2 d   97   98
> reshape(d1, direction="long", varying=list(c("rep1","rep2")), 
v.names="value",idvar="tmp", timevar="r")
     id a b r value tmp
1.1  1 1 1 1    65   1
2.1  2 1 2 1    65   2
3.1  3 2 1 1    65   3
4.1  4 2 2 1    65   4
5.1  1 1 1 1    78   5
6.1  2 1 2 1    42   6
7.1  3 2 1 1    97   7
8.1  4 2 2 1    97   8
1.2  1 1 1 2    97   1
2.2  2 1 2 2    97   2
3.2  3 2 1 2    68   3
4.2  4 2 2 2    97   4
5.2  1 1 1 2    98   5
6.2  2 1 2 2    97   6
7.2  3 2 1 2    98   7
8.2  4 2 2 2    98   8

You can then delete the `tmp` variable if you don't need it.

An easier way to uncross would be with stack()
> stack(d[,4:7])
    values ind
1      65  c1
2      65  c1
3      65  c1
4      65  c1
5      97  c2
6      97  c2
7      68  c2
8      97  c2
9      78  d1
10     42  d1
11     97  d1
12     97  d1
13     98  d2
14     97  d2
15     98  d2
16     98  d2

but you lose the other variables.


 	-thomas



From Friedrich.Leisch at tuwien.ac.at  Fri Jun  3 18:41:59 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 3 Jun 2005 18:41:59 +0200
Subject: [R] Hclust question
In-Reply-To: <42A083DC.5020700@bc.edu>
References: <42A083DC.5020700@bc.edu>
Message-ID: <17056.34903.985219.95697@celebrian.ci.tuwien.ac.at>

>>>>> On Fri, 03 Jun 2005 12:22:52 -0400,
>>>>> Mike Schuler (MS) wrote:

  > Hey,
  > I am running hclust on several different distance matrices and I have a 
  > question thats more about labeling. I've been looking for a way to label 
  > the edge values on the graph with their distances between them. I've 
  > been looking through the documentation and I haven't found anything yet. 
  > Anyone know if there is a way to plot 'hclust' graphs with such edge 
  > values? Or convert it to another type that can have its edge values labeled?

  > And to clarify what I mean by edge values, I have two pictures 
  > describing what I mean. First, 
  > http://clavius.bc.edu/~schulerm/images/test.gif [3.8k] is what I 
  > currently have. What I'm looking for is something similar to this: 
  > http://clavius.bc.edu/~schulerm/images/test2.gif [5.1k]. I arbitrarily 
  > put in values on the edges with the Gimp but I'm looking for some way to 
  > do that on my plots of 'hclust'.

help(dendrogram)
example(dendrogram)

should give you a start.

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From spencer.graves at pdf.com  Fri Jun  3 18:43:49 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 03 Jun 2005 09:43:49 -0700
Subject: [R] RE: GARCH (1 , 1), Hill estimator of alpha, Pareto estimator
In-Reply-To: <BAY19-F322C29D18D64C62C4DF6DBFD070@phx.gbl>
References: <BAY19-F322C29D18D64C62C4DF6DBFD070@phx.gbl>
Message-ID: <42A088C5.1070205@pdf.com>

	  First, I doubt if you need "library(stats)", because it is normally 
automatically attached when R starts.  To confirm, request "search()". 
When I just did this with R 2.1.0 patched, "package:stats"   was the 
third item.

	  Second, if you've already produced qq-plots and histograms, then I 
doubt if you need the "data(DAX_CAC)".  If you have not exited R since 
you successfully imported the data, and you stored it as "DAX_CAC", it 
should still be available.

	  Third, the message 'Error: couldn't find function "garch"' indicates 
that no function by that name is in the current "search()" path.  The 
command 'RSiteSearch("garch")' revealed that it was part of package 
"tseries".  To be able to access that, you need to 
'install.packages("tseries")', 'update.packages()', and then 
'library(tseries)'.

	  This should get you past the error messages I see.

	  spencer graves	

Ukech U. Kidi wrote:

> Dear R users,
> 
> Could you please help me out. I am unable to formulate a model in R to 
> graphs a GARCH (1 , 1) model, the Hill estimator (of alpha), and the 
> Pareto estimator.
> 
> I have a financial time series data from the DAX and CAC40.  I just got 
> introduce to R. I am working on a paper which must be worked from R.
> 
> I successfully imported the data into R. I also succeeded to produce te 
> qq-plots  and histograms from the data.
> 
> My difficulty at the moment lies where I could not go further to plot 
> the GARCH plots, the Hill estimator of alpha and the Pareto estimator.
> 
> Reading from the FAQs of this list, I attempted the following and wasn't 
> succeesful. Could anyone show me the steps that I might have omited.
> 
> Check my errors from the following.
> 
> library(stats)
> 
> data(DAX_CAC)
> 
> Warning message:
> 
> Data set 'DAX_CAC' not found in: data(DAX_CAC)
> 
> data("DAX_CAC")
> 
> 
> Warning message:
> 
> Data set 'DAX_CAC' not found in: data("DAX_CAC")
> 
>    dax<- diff(log(DAX_CAC$DAX[1:1865]))
> 
> 
>    m1<- garch(dax)
> 
> 
> Error: couldn't find function "garch"
> 
>    m1<- garch(dax[1:1865])
> 
> 
> Error: couldn't find function "garch"
> 
>    m1<- garch(dax[1:1865])
> 
> 
> 
> Thank you in advance,
> 
> Ukech U. kidi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Fri Jun  3 18:48:16 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 03 Jun 2005 12:48:16 -0400
Subject: [R] help with  guiDlgOpen
In-Reply-To: <001101c56857$b8f40350$ec6eed82@pizero>
References: <001101c56857$b8f40350$ec6eed82@pizero>
Message-ID: <42A089D0.50702@stats.uwo.ca>

On 6/3/2005 12:17 PM, Marco Zucchelli wrote:
> Hi,
> 
>  I am trying to open a file with guidlgOpen. on windows. When the file is in a directory whose name contains a space, the name is splitted into 2, i.e. 
> 
>>Afile <- guiDlgOpen(title= " Open file",defaultFile="",defaultDir="", multi=FALSE, filters = c("All files (*.*)", "*.*"))
> 
>> file
> [1] "C:/Program"                    "Files/R/Work/dcdc2_levels.txt"

I think that function is in the contributed package svDialogs.  You 
should probably address your question to its author, Philippe Grosjean 
(phgrosjean at sciviews.org).

Duncan Murdoch



From asr at ufl.edu  Fri Jun  3 18:51:02 2005
From: asr at ufl.edu (Allen S. Rout)
Date: Fri, 03 Jun 2005 12:51:02 -0400
Subject: [R] Dirty Rotten Hack. (reversing tickmarks on axes?)
Message-ID: <200506031651.j53Gp2lc386384@nersp.nerdc.ufl.edu>


I feel dirty.


I have some graphs I'm building to communicate chargeback rates and service
usage for our backup system here at the University of Florida.  These come
down to daily data points on a graph of number-of-bytes transferred and
stored. 

Since we chargeback on the same basis (price per MB this, price per KB that)
the same chart with a different scale can be used to communicate bytes and
dollars. I set about trying to accomplish this like so:

http://nersp.nerdc.ufl.edu/~asr/media/r-foo/try1.png

Those axes are a little messy. I tried nudging them around

http://nersp.nerdc.ufl.edu/~asr/media/r-foo/try2.png

which is better but not good.  What I really want to do is tell my axis()
function to reverse the tick direction:  put your ticks and labels "inside"
the graph.  Something like

http://nersp.nerdc.ufl.edu/~asr/media/r-foo/dirty-hack.png

which I accomplished by telling axis() 'line=-47.7'. 

Eugh.

Note that the distance between the left side and the right is different
between the right side to the left. :)   I don't particularly object to this,
when you abuse a tool in this manner you need to expect oddities.

I've wandered through the mailing list logs, and haven't seen reference to
this particular desire.  Am I alone? :) 



- Allen S. Rout



From gunter.berton at gene.com  Fri Jun  3 19:05:04 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 3 Jun 2005 10:05:04 -0700
Subject: [R] Dirty Rotten Hack. (reversing tickmarks on axes?)
In-Reply-To: <200506031651.j53Gp2lc386384@nersp.nerdc.ufl.edu>
Message-ID: <200506031705.j53H55xX019063@faraday.gene.com>

?par tck

Make tck a positive fraction <.5 (e.g. .02) in your call.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Allen S. Rout
> Sent: Friday, June 03, 2005 9:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Dirty Rotten Hack. (reversing tickmarks on axes?)
> 
> 
> I feel dirty.
> 
> 
> I have some graphs I'm building to communicate chargeback 
> rates and service
> usage for our backup system here at the University of 
> Florida.  These come
> down to daily data points on a graph of number-of-bytes 
> transferred and
> stored. 
> 
> Since we chargeback on the same basis (price per MB this, 
> price per KB that)
> the same chart with a different scale can be used to 
> communicate bytes and
> dollars. I set about trying to accomplish this like so:
> 
> http://nersp.nerdc.ufl.edu/~asr/media/r-foo/try1.png
> 
> Those axes are a little messy. I tried nudging them around
> 
> http://nersp.nerdc.ufl.edu/~asr/media/r-foo/try2.png
> 
> which is better but not good.  What I really want to do is 
> tell my axis()
> function to reverse the tick direction:  put your ticks and 
> labels "inside"
> the graph.  Something like
> 
> http://nersp.nerdc.ufl.edu/~asr/media/r-foo/dirty-hack.png
> 
> which I accomplished by telling axis() 'line=-47.7'. 
> 
> Eugh.
> 
> Note that the distance between the left side and the right is 
> different
> between the right side to the left. :)   I don't particularly 
> object to this,
> when you abuse a tool in this manner you need to expect oddities.
> 
> I've wandered through the mailing list logs, and haven't seen 
> reference to
> this particular desire.  Am I alone? :) 
> 
> 
> 
> - Allen S. Rout
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Jun  3 19:11:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jun 2005 19:11:39 +0200
Subject: [R] Dirty Rotten Hack. (reversing tickmarks on axes?)
In-Reply-To: <200506031651.j53Gp2lc386384@nersp.nerdc.ufl.edu>
References: <200506031651.j53Gp2lc386384@nersp.nerdc.ufl.edu>
Message-ID: <x2k6lbbc1g.fsf@turmalin.kubism.ku.dk>

"Allen S. Rout" <asr at ufl.edu> writes:

> I feel dirty.
> 
> 
> I have some graphs I'm building to communicate chargeback rates and service
> usage for our backup system here at the University of Florida.  These come
> down to daily data points on a graph of number-of-bytes transferred and
> stored. 
> 
> Since we chargeback on the same basis (price per MB this, price per KB that)
> the same chart with a different scale can be used to communicate bytes and
> dollars. I set about trying to accomplish this like so:
> 
> http://nersp.nerdc.ufl.edu/~asr/media/r-foo/try1.png
> 
> Those axes are a little messy. I tried nudging them around
> 
> http://nersp.nerdc.ufl.edu/~asr/media/r-foo/try2.png
> 
> which is better but not good.  What I really want to do is tell my axis()
> function to reverse the tick direction:  put your ticks and labels "inside"
> the graph.  Something like
> 
> http://nersp.nerdc.ufl.edu/~asr/media/r-foo/dirty-hack.png
> 
> which I accomplished by telling axis() 'line=-47.7'. 
> 
> Eugh.
> 
> Note that the distance between the left side and the right is different
> between the right side to the left. :)   I don't particularly object to this,
> when you abuse a tool in this manner you need to expect oddities.
> 
> I've wandered through the mailing list logs, and haven't seen reference to
> this particular desire.  Am I alone? :) 

Possibly in space, but not in time and space: Someone seems to have
been there before. Check out

plot(0,tcl=.5,mgp=c(-3,-2,0))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Fri Jun  3 19:14:37 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Jun 2005 13:14:37 -0400
Subject: [R] Problems of performance between linux and freebsd
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E917@usctmx1106.merck.com>

> From: v.demartino2 at virgilio.it
> 
> Dear All,
> I posted the following message to the Freebsd-questions mailing 
> =================================================================
> <snip>
> On my laptop I've being using linux for some years now and 
> "landed" at last
> to 
> the gentoo distribution which I tuned for working with the 
> statistical 
> software R and the bunch of TeX programs such as latex, 
> pdflatex, context
> and 
> the likes (this absorb 95% of my activity on the linux box).
> The main action I took with gentoo was to compile everything 
> from scratch
> for 
> my pc hardware, limiting the use of X & kde using all the 
> above mentioned
> 
> software prevailingly in a simple console.
> 
> Now in a partition of the same laptop I installed freebsd 5.4, set 
> softupdates, dma=1, and the same pieces of software as those 
> I had installed
> 
> under linux using the ports, therefore, compiling everything 
> for my pc 
> hardware, but, owing to my poor knowledge of Freebsd I'm 
> experiencing some
> 
> problems.
> 
> In a nutshell I prepared the following test files:
> 
> 1) a ConTeXt file full of long (useless) chapters and 
> pictures to run with
> 
> texexec;
> 2) An R program which reads a long text file, puts the data 
> in dataframes,
> 
> then loops to accomplish many repetitive mathematical calculations.
> 
> Now running the test  files in a console linux turns out to 
> be roughly 20-25%
> 
> fast
> er than freebsd. Particulary slow results the loop phase of 
> the R source
> 
> code that while running slows down and seems to clog in the 
> end. The slowing
> 
> down is also perceptible under linux but not at the same level.
> 
> Admiring the "ordered and unitary layout" of freebsd, the 
> logical way of
> 
> setting it up, the richness of applications,  I would like to 
> line the 
> freebsd installation up to the linux one, improving its 
> performance at most,
> 
> not necessarily making it faster than linux.
> 
> How could I obtain those improvements? What could I do to 
> speed those programs
> 
> under freebsd?
> 
> </snip>
> 
> Now I can be more precise about R.
> i) I downloaded the tarball of R 2.0.1 from CRAN;
> ii) I compiled the **same source *** source by means of 
> ./configure, make,
> make install both on gentoo and on freebsd which share the 
> same make.conf
> to optimize for a pentium4 and -O2 flag.
> iii) I created 2 R test programs one (simula.r) as described 
> above at point
> 2) and the other (test.R) using the previous and adding some 
> moreR code
> usin the library nnet and looping desperately with it (see 
> attached files).
> 
> simula.r takes 29 sec to be executed under win xp, 25 sec 
> under linux gentoo
> and 26 under freebsd 5.4, all comparable. On the contrary the 
> program test.R
> takes 93 sec under win xp, 85 sec under linux gentoo and an 
> unbelievable
> 165 sec under freebsd. 
> What I noticed is that test.R produces a lot of output while 
> looping the
> nnet section of the program which under win xp you can only 
> see at end of
> the run, under a gentoo text-console flows happily & briskly 
> on the screen
> whilst under a freebsd text-console it starts happily, slows 
> down in the
> middle of the looping process and clogs in the end. In a 
> nutshell freebsd
> takes  twice as much the time of the other 2 OSs to execute 
> the program.
> Sorry for my being so detailed.
> What shall I do?
> Vittorio

I'm no expert (not even amateur) in these matters...  Anyway, under Windows
R(gui) buffers its console output by default, so you don't see the output
until the execution is finished (or the output exceeds the buffer size).  I
have no idea how konsole (I assume that's what you're using in Linux) or
whatever you're using in freeBSD does.  What would the numbers be if you run
the R script via R CMD BATCH?  Supposedly that comparison would eliminate
the differences in how console outputs are buffered (or not).

Andy
    
> 
>  
> 
>  
> 
>



From kjetil at acelerate.com  Fri Jun  3 19:20:12 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 03 Jun 2005 13:20:12 -0400
Subject: [R] geometric mean regression
In-Reply-To: <42A02045.5060407@cnam.fr>
References: <42A02045.5060407@cnam.fr>
Message-ID: <42A0914C.1050809@acelerate.com>

Poizot Emmanuel wrote:

> Hi,
>
> is it possible to perform a geometric mean regression with R ?
> Thanks.
>
As has been said on this list before, "This is R, there is no if, only 
how",

but if you actually wanted to ask how it is possible, it would help if
you explained what is "geometric mean regression".

Kjetil

> ------------------------------------------------
> Emmanuel Poizot
> Cnam/Intechmer
> B.P. 324
> 50103 Cherbourg Cedex
>
> Phone (Direct) : (00 33)(0)233887342
> Fax : (00 33)(0)233887339
> ------------------------------------------------
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>------------------------------------------------------------------------
>
>No virus found in this incoming message.
>Checked by AVG Anti-Virus.
>Version: 7.0.322 / Virus Database: 267.4.0 - Release Date: 01/06/2005
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From rwcitek at alum.calberkeley.org  Fri Jun  3 19:20:48 2005
From: rwcitek at alum.calberkeley.org (Robert Citek)
Date: Fri, 3 Jun 2005 12:20:48 -0500
Subject: [R] R IRC channel?
Message-ID: <5D828055-FEEF-461A-9442-B59869476D8F@alum.calberkeley.org>


Is there an IRC channel for R?

I have a bunch of relatively simple questions that I can't find  
answers to.  So it'd be nice to send them to an IRC channel rather  
than clutter up the mailing list.  I'm sure after a few examples and  
pointers to the right docs I'll be able to find my way.

Regards,
- Robert



From murdoch at stats.uwo.ca  Fri Jun  3 19:43:59 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 03 Jun 2005 13:43:59 -0400
Subject: [R] R IRC channel?
In-Reply-To: <5D828055-FEEF-461A-9442-B59869476D8F@alum.calberkeley.org>
References: <5D828055-FEEF-461A-9442-B59869476D8F@alum.calberkeley.org>
Message-ID: <42A096DF.2080409@stats.uwo.ca>

On 6/3/2005 1:20 PM, Robert Citek wrote:
> Is there an IRC channel for R?
> 
> I have a bunch of relatively simple questions that I can't find  
> answers to.  So it'd be nice to send them to an IRC channel rather  
> than clutter up the mailing list.  I'm sure after a few examples and  
> pointers to the right docs I'll be able to find my way.

I don't know if there's an IRC list, but you shouldn't feel bad about 
asking simple questions in R-help.  You should "do your homework" first 
(see the posting guide), but if you've done that and are still at a 
loss, feel free to post here.  (Mention what sort of searches you did 
that turned up empty; besides getting the answer you want, you might 
help to improve the documentation).

Duncan Murdoch



From jfbrennan at rogers.com  Fri Jun  3 19:54:41 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 3 Jun 2005 13:54:41 -0400
Subject: [R] Histogram of multiple series on one histogram
In-Reply-To: <200506031814.23789.depire@inrets.fr>
Message-ID: <200506031754.j53HseGO028647@hypatia.math.ethz.ch>

I think you can use barplot for what you want.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Depire Alexandre
Sent: June 3, 2005 12:14 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Histogram of multiple series on one histogram

Hello,
I have three sample, for example
	a<-c(10,20,10,20,30)
	b<-c(10,20,20,30,30)
	c<-c(20,20,10)

I would like to have only one histogram with these series,
I try the following code:
	hist(a)
	hist(b,add=TRUE,col="red")
	hist(c,add=TRUE,col="green")
but it's not that I want 

I would like an histogram with, from left to right, count of "10" for "a", 
count of "10" for "b", count of "10" for "c", count of "20" for "a", count
of 
"20" for "b", ... not overlapped

Thanks


-- 
----------------
Alexandre DEPIRE
INRETS / GARIG
Tel.: (+33) 01 47 40 71 66
Fax: (+33) 01 45 47 56 06
2 av. du g??n??ral Malleret-Joinville
94114 Arcueil - France

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From fisik at ncsu.edu  Fri Jun  3 23:10:46 2005
From: fisik at ncsu.edu (Fikret Isik)
Date: Fri, 03 Jun 2005 14:10:46 -0700
Subject: [R] Errors on bar plots
Message-ID: <42A0C756.6040709@ncsu.edu>

Hi R Gurus,

I am a new R user. I did my home work by reading the manuals and 
searching the internet for examples but failed to find an example before 
writing this message for help.

I would like to create bar plots with the standard error of the mean (or 
95% confidence intervals) on the bar.

Any suggestions will be appreciated.

Operating system: Windows XP pro
R version 2.1.0

Data look like this:

Site Family  Height
1     A         12.3
1     B         13.5
1     C         17.4
2     A         14.0
2     B         11.1
2     C         15.9
.       .          .

Fikret Isik


-- 
Fikret Isik, 
Research Assistant Prof.
 Department of Forestry and Env. Resources
 3010 Biltmore Hall
NC State University,  
 Campus Box 8002, Raleigh, NC 27695
PHONE: 919-515-5029,  
FAX: 919-515-3169
WEB: http://www4.ncsu.edu/~fisik



From dimpatridge at yahoo.co.uk  Fri Jun  3 20:16:48 2005
From: dimpatridge at yahoo.co.uk (DIM PATRIDGE)
Date: Fri, 3 Jun 2005 19:16:48 +0100 (BST)
Subject: [R] noise poser spectral density
Message-ID: <20050603181648.60739.qmail@web25710.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050603/7a377020/attachment.pl

From ggrothendieck at gmail.com  Fri Jun  3 20:20:36 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Jun 2005 14:20:36 -0400
Subject: [R] Errors on bar plots
In-Reply-To: <42A0C756.6040709@ncsu.edu>
References: <42A0C756.6040709@ncsu.edu>
Message-ID: <971536df05060311204a123179@mail.gmail.com>

On 6/3/05, Fikret Isik <fisik at ncsu.edu> wrote:
> Hi R Gurus,
> 
> I am a new R user. I did my home work by reading the manuals and
> searching the internet for examples but failed to find an example before
> writing this message for help.
> 
> I would like to create bar plots with the standard error of the mean (or
> 95% confidence intervals) on the bar.
> 
> Any suggestions will be appreciated.
> 
> Operating system: Windows XP pro
> R version 2.1.0
> 
> Data look like this:
> 
> Site Family  Height
> 1     A         12.3
> 1     B         13.5
> 1     C         17.4
> 2     A         14.0
> 2     B         11.1
> 2     C         15.9
> .       .          .
> 

Is this what you are looking for?

http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=54

If so look at barplot2 in package gplots which is in the gregmisc bundle.

You could also look at the notch argument to boxplot.



From jfbrennan at rogers.com  Fri Jun  3 20:31:13 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 3 Jun 2005 14:31:13 -0400
Subject: [R] Histogram of multiple series on one histogram
In-Reply-To: <200506031814.23789.depire@inrets.fr>
Message-ID: <200506031831.j53IVBfU005771@hypatia.math.ethz.ch>

Something like this!?!?
R>new
       a  
 [1,] 10 1
 [2,] 20 1
 [3,] 10 1
 [4,] 20 1
 [5,] 30 1
 [6,] 10 2
 [7,] 20 2
 [8,] 20 2
 [9,] 30 2
[10,] 30 2
[11,] 20 3
[12,] 20 3
[13,] 10 3
R>barplot(table(new[,2],new[,1]),beside=T,legend.text=c("a","b","c"))

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Depire Alexandre
Sent: June 3, 2005 12:14 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Histogram of multiple series on one histogram

Hello,
I have three sample, for example
	a<-c(10,20,10,20,30)
	b<-c(10,20,20,30,30)
	c<-c(20,20,10)

I would like to have only one histogram with these series,
I try the following code:
	hist(a)
	hist(b,add=TRUE,col="red")
	hist(c,add=TRUE,col="green")
but it's not that I want 

I would like an histogram with, from left to right, count of "10" for "a", 
count of "10" for "b", count of "10" for "c", count of "20" for "a", count
of 
"20" for "b", ... not overlapped

Thanks


-- 
----------------
Alexandre DEPIRE
INRETS / GARIG
Tel.: (+33) 01 47 40 71 66
Fax: (+33) 01 45 47 56 06
2 av. du g??n??ral Malleret-Joinville
94114 Arcueil - France

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Fri Jun  3 20:37:51 2005
From: francoisromain at free.fr (Romain Francois)
Date: Fri, 03 Jun 2005 20:37:51 +0200
Subject: [R] Histogram of multiple series on one histogram
In-Reply-To: <200506031814.23789.depire@inrets.fr>
References: <1117623359.13574.3.camel@localhost>
	<200506031814.23789.depire@inrets.fr>
Message-ID: <42A0A37F.3000007@free.fr>

Hello alexandre,

what you are trying to do is *not* an histogram (as a density 
estimator), if you divide each bar in 3, the surfaces of a won't sum to 1.
However a barplot or a barplot2 (in package gplots, bundle gregmisc) 
would do the trick.
See graph 54 on the graph gallery :
http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=54

BTW, don't call your vector 'c' unless you want funny things to happen

Romain


Le 03.06.2005 18:14, Depire Alexandre a ??crit :

>Hello,
>I have three sample, for example
>	a<-c(10,20,10,20,30)
>	b<-c(10,20,20,30,30)
>	c<-c(20,20,10)
>
>I would like to have only one histogram with these series,
>I try the following code:
>	hist(a)
>	hist(b,add=TRUE,col="red")
>	hist(c,add=TRUE,col="green")
>but it's not that I want 
>
>I would like an histogram with, from left to right, count of "10" for "a", 
>count of "10" for "b", count of "10" for "c", count of "20" for "a", count of 
>"20" for "b", ... not overlapped
>
>Thanks
>
>
>  
>

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From helprhelp at gmail.com  Fri Jun  3 21:07:57 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 3 Jun 2005 14:07:57 -0500
Subject: [R] factor vector manipulation
Message-ID: <cdf817830506031207778bfa3a@mail.gmail.com>

Hi,
I have one question on factor vector.
I have 3 factor vectors:

a<-factor(c("1", "2", "3"))
b<-factor(c("a", "b", "c"))
c<-factor(c("b", "a", "c"))

what I want is like:
  c x
1 b 2
2 a 1
3 c 3

which means, I use b as keys and vector a as values and I find values for c.

I used the following codes:
x<-c()
d<-data.frame(b,a)
for (each in 1:length(c)){   # i don't know why each in c did not work
  tmp<-d$a[d$b==c[each]]                                  # question 
  x<-append(x, levels(tmp)[as.integer(tmp)]) 
}

data.frame(c,x)

If someone has a better way to do it, please help!!

Also, I don't understand why I have to use levels() since w/o it, i
only added the index for levels in to x.

BTW, when b is like 60,000, the process is very slow. Should I use hash here?

Thanks,

Weiwei


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From yzhou at sdsc.edu  Fri Jun  3 21:44:49 2005
From: yzhou at sdsc.edu (Yi-Xiong Zhou)
Date: Fri, 3 Jun 2005 12:44:49 -0700
Subject: [R] p-value > 1 in fisher.test()
Message-ID: <452F37AE49199D49B1702D7D45038C4D0D9C6A@et.ad.sdsc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050603/073d9684/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Jun  3 22:01:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Jun 2005 22:01:02 +0200
Subject: [R] p-value > 1 in fisher.test()
In-Reply-To: <452F37AE49199D49B1702D7D45038C4D0D9C6A@et.ad.sdsc.edu>
References: <452F37AE49199D49B1702D7D45038C4D0D9C6A@et.ad.sdsc.edu>
Message-ID: <x2fyvz5hxd.fsf@turmalin.kubism.ku.dk>

"Yi-Xiong Zhou" <yzhou at sdsc.edu> writes:

> The following contingency table generates p-value > 1 from fisher.test()
> 
>  
> 
> ff = c(0,10,250,5000); dim(ff) = c(2,2); fhisher.test(ff)$p.value

On this system (PIII, Fedora Core 3):

> ff = c(0,10,250,5000); dim(ff) = c(2,2); fisher.test(ff)$p.value -1
[1] 1.384892e-12

Is that interesting?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From macq at llnl.gov  Fri Jun  3 22:37:27 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 3 Jun 2005 13:37:27 -0700
Subject: [R] p-value > 1 in fisher.test()
In-Reply-To: <x2fyvz5hxd.fsf@turmalin.kubism.ku.dk>
References: <452F37AE49199D49B1702D7D45038C4D0D9C6A@et.ad.sdsc.edu>
	<x2fyvz5hxd.fsf@turmalin.kubism.ku.dk>
Message-ID: <p06210210bec66f60cbc6@[128.115.153.6]>

At 10:01 PM +0200 6/3/05, Peter Dalgaard wrote:
>"Yi-Xiong Zhou" <yzhou at sdsc.edu> writes:
>
>>  The following contingency table generates p-value > 1 from fisher.test()
>>
>> 
>>
>>  ff = c(0,10,250,5000); dim(ff) = c(2,2); fhisher.test(ff)$p.value
>
>On this system (PIII, Fedora Core 3):
>
>  > ff = c(0,10,250,5000); dim(ff) = c(2,2); fisher.test(ff)$p.value -1
>[1] 1.384892e-12
>
>Is that interesting?

And on this system (powerpc, darwin7.8.0)

>  ff = c(0,10,250,5000); dim(ff) = c(2,2); fisher.test(ff)$p.value -1
[1] -2.603184e-11

About equally interesting.

-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From abunn at whrc.org  Fri Jun  3 22:42:13 2005
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 3 Jun 2005 16:42:13 -0400
Subject: [R] ts.intersect a multivariate and univariate ts
Message-ID: <NEBBIPHDAMMOKDKPOFFICEAMDHAA.abunn@whrc.org>

This seems like a FAQ, but I can't figure it out.

I have a mv ts object:

R > tsp(pg)
[1] 1982 2003    1
R > dim(pg)
[1] 22 12

and a univariate ts:

R > tsp(rw)
[1] 1690 1996    1

Yet, when I try to intersect them:

R > tsp(ts.intersect(rw, pg))
[1] 1982 2176    1

the process goes awry.

How to I get rw and pg to be one ts that runs from 1982 to 1996 and has 13
univariate time series in it. Any help appreciated.

-Andy

R > version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R



From richard.kittler at amd.com  Fri Jun  3 22:47:01 2005
From: richard.kittler at amd.com (Kittler, Richard)
Date: Fri, 3 Jun 2005 13:47:01 -0700
Subject: [R] Lattice xyplot -- footnote font size /  mtext
Message-ID: <2F23B0E9BAD0044BBB4A75DD913EE7A6AFD441@ssvlexmb2.amd.com>

 
Is there a way of controlling the font size and alignment of a footnote
in an xyplot, or alternatively of using 'mtext' to place a footnote at
the bottom of a graph?  

--Rich

Richard Kittler
Advanced Micro Devices, Inc.
Sunnyvale, CA



From mwgrant2001 at yahoo.com  Fri Jun  3 22:52:54 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Fri, 3 Jun 2005 13:52:54 -0700 (PDT)
Subject: [R] geometric mean regression
In-Reply-To: <42A0914C.1050809@acelerate.com>
Message-ID: <20050603205254.85402.qmail@web52002.mail.yahoo.com>


I presume the reference is to the 'geometric mean
functional regression' or the 'line of organic
correlation' or 'reduced major axis regression'.  If
so, this is relatively easy alsmost trivial to
implement in R. Maybe it's in a package, but I never
looked. I worked from Helsel's description in his
classic water resources statistics book. See Chapter
10 here: 

http://water.usgs.gov/pubs/twri/twri4a3/

Now, if you are after confidence intervals or
prediction intervals, I haven't found anything on that
yet. Seems that I did something a couple of year ago
by hacking some approximate residuals using the LOC
line and the data, and then feeding that into the CL
and PL equations for OLS. (Be advised that I'm not a
statistician and did that in the spirit of 
approximation--who knows? :O) )

By coincidence I've been looking at this again
recently. Maybe bootstrapping....

Regards,
Michael Grant

--- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com>
wrote:

> Poizot Emmanuel wrote:
> 
> > Hi,
> >
> > is it possible to perform a geometric mean
> regression with R ?
> > Thanks.
> >
> As has been said on this list before, "This is R,
> there is no if, only 
> how",
> 
> but if you actually wanted to ask how it is
> possible, it would help if
> you explained what is "geometric mean regression".
> 
> Kjetil
> 
> > ------------------------------------------------
> > Emmanuel Poizot
> > Cnam/Intechmer
> > B.P. 324
> > 50103 Cherbourg Cedex
> >
> > Phone (Direct) : (00 33)(0)233887342
> > Fax : (00 33)(0)233887339
> > ------------------------------------------------
> >
>
>------------------------------------------------------------------------
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>
>------------------------------------------------------------------------
> >
> >No virus found in this incoming message.
> >Checked by AVG Anti-Virus.
> >Version: 7.0.322 / Virus Database: 267.4.0 -
> Release Date: 01/06/2005
> >  
> >
> 
> 
> -- 
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass
> construction.
>                --  Mahdi Elmandjra
> 
> 
> 
> 
> -- 
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From yzhou at sdsc.edu  Fri Jun  3 23:00:25 2005
From: yzhou at sdsc.edu (Yi-Xiong Zhou)
Date: Fri, 3 Jun 2005 14:00:25 -0700
Subject: FW: [R] p-value > 1 in fisher.test()
Message-ID: <452F37AE49199D49B1702D7D45038C4D0D9C77@et.ad.sdsc.edu>


The problem is from the most recent version. Here is the platform:

R 2.1.0 2005-04-18, on winXP SP2, 

Sean

-----Original Message-----
From: Douglas Grove [mailto:dgrove at fhcrc.org] 
Sent: Friday, June 03, 2005 12:49 PM
To: Yi-Xiong Zhou
Subject: Re: [R] p-value > 1 in fisher.test()

You need to upgrade to a newer version of R.  
This has already been fixed.


On Fri, 3 Jun 2005, Yi-Xiong Zhou wrote:

> The following contingency table generates p-value > 1 from
fisher.test()
> 
>  
> 
> ff = c(0,10,250,5000); dim(ff) = c(2,2); fhisher.test(ff)$p.value
> 
>  
> 
> Sean
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From deepayan at stat.wisc.edu  Fri Jun  3 23:15:41 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 3 Jun 2005 16:15:41 -0500
Subject: [R] Lattice xyplot -- footnote font size /  mtext
In-Reply-To: <2F23B0E9BAD0044BBB4A75DD913EE7A6AFD441@ssvlexmb2.amd.com>
References: <2F23B0E9BAD0044BBB4A75DD913EE7A6AFD441@ssvlexmb2.amd.com>
Message-ID: <200506031615.41534.deepayan@stat.wisc.edu>

On Friday 03 June 2005 15:47, Kittler, Richard wrote:
> Is there a way of controlling the font size and alignment of a footnote
> in an xyplot, or alternatively of using 'mtext' to place a footnote at
> the bottom of a graph?

Your best bet is using 'page', e.g.

library(grid)
library(lattice)
xyplot(1 ~ 1, 
       page = function(n) 
           grid.text(paste("footnote for page", n), 
                     x = .99, y = 0.01, 
                     default.units = "npc", 
                     just = c("right", "bottom")))

You could also use panel.text instead of grid.text, but the latter is more 
flexible. You could make the font smaller by adding the following in the 
grid.text call:

gp = gpar(fontsize = 8)

Deepayan



From dmb at mrc-dunn.cam.ac.uk  Fri Jun  3 23:08:35 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 3 Jun 2005 22:08:35 +0100 (BST)
Subject: [R] R IRC channel?
In-Reply-To: <5D828055-FEEF-461A-9442-B59869476D8F@alum.calberkeley.org>
Message-ID: <Pine.LNX.4.21.0506032207540.9363-100000@mail.mrc-dunn.cam.ac.uk>


/server irc.freenode.net 
/join #R

:)




On Fri, 3 Jun 2005, Robert Citek wrote:

>
>Is there an IRC channel for R?
>
>I have a bunch of relatively simple questions that I can't find  
>answers to.  So it'd be nice to send them to an IRC channel rather  
>than clutter up the mailing list.  I'm sure after a few examples and  
>pointers to the right docs I'll be able to find my way.
>
>Regards,
>- Robert
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From richard.kittler at amd.com  Fri Jun  3 23:12:36 2005
From: richard.kittler at amd.com (Kittler, Richard)
Date: Fri, 3 Jun 2005 14:12:36 -0700
Subject: [R] Lattice xyplot -- footnote font size /  mtext
Message-ID: <2F23B0E9BAD0044BBB4A75DD913EE7A6AFD442@ssvlexmb2.amd.com>

Great!  That should do it.  Thanks.   

--Rich

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: Friday, June 03, 2005 2:16 PM
To: r-help at stat.math.ethz.ch
Cc: Kittler, Richard
Subject: Re: [R] Lattice xyplot -- footnote font size / mtext

On Friday 03 June 2005 15:47, Kittler, Richard wrote:
> Is there a way of controlling the font size and alignment of a 
> footnote in an xyplot, or alternatively of using 'mtext' to place a 
> footnote at the bottom of a graph?

Your best bet is using 'page', e.g.

library(grid)
library(lattice)
xyplot(1 ~ 1, 
       page = function(n) 
           grid.text(paste("footnote for page", n), 
                     x = .99, y = 0.01, 
                     default.units = "npc", 
                     just = c("right", "bottom")))

You could also use panel.text instead of grid.text, but the latter is
more flexible. You could make the font smaller by adding the following
in the grid.text call:

gp = gpar(fontsize = 8)

Deepayan



From Ted.Harding at nessie.mcc.ac.uk  Fri Jun  3 22:45:24 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 03 Jun 2005 21:45:24 +0100 (BST)
Subject: [R] p-value > 1 in fisher.test()
In-Reply-To: <x2fyvz5hxd.fsf@turmalin.kubism.ku.dk>
Message-ID: <XFMail.050603214524.Ted.Harding@nessie.mcc.ac.uk>

On 03-Jun-05 Peter Dalgaard wrote:
> "Yi-Xiong Zhou" <yzhou at sdsc.edu> writes:
> 
>> The following contingency table generates p-value > 1 from
>> fisher.test()
>> 
>>  
>> 
>> ff = c(0,10,250,5000); dim(ff) = c(2,2); fhisher.test(ff)$p.value
> 
> On this system (PIII, Fedora Core 3):
> 
>> ff = c(0,10,250,5000); dim(ff) = c(2,2); fisher.test(ff)$p.value -1
> [1] 1.384892e-12
> 
> Is that interesting?

And on mine

(A: PII, Red Had 9, R-1.8.0):

 ff <- c(0,10,250,5000); dim(ff) <- c(2,2);

 1-fisher.test(ff)$p.value
 [1] 1.268219e-11

(B: PIII, SuSE 7.2, R-2.1.0beta):

 ff <- c(0,10,250,5000); dim(ff) <- c(2,2);

 1-fisher.test(ff)$p.value
 [1] -1.384892e-12

Peter, I'm not sure which is the more interesting!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 03-Jun-05                                       Time: 21:45:13
------------------------------ XFMail ------------------------------



From chess.player at oninet.pt  Sat Jun  4 00:35:58 2005
From: chess.player at oninet.pt (jose silva)
Date: Fri, 03 Jun 2005 23:35:58 +0100
Subject: [R] rearrange data
Message-ID: <54866b00980948b1b8649c8450d0bdf8@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050603/3c00b166/attachment.pl

From mk_lists at yahoo.ca  Sat Jun  4 00:49:28 2005
From: mk_lists at yahoo.ca (M. K.)
Date: Fri, 3 Jun 2005 18:49:28 -0400 (EDT)
Subject: [R] lattice: change point color in densityplot()?
Message-ID: <20050603224928.432.qmail@web31305.mail.mud.yahoo.com>

When using the "densityplot" function from "lattice" library, is there
a way to pick a different color *for the points only*?  (I'm using
densityplot(..., plot.points=TRUE, ...))  Using "col" parameter changes
the color for both, the points and the curve.



From slusek at o2.pl  Sat Jun  4 00:51:22 2005
From: slusek at o2.pl (Wojtek Slusarski)
Date: Sat, 04 Jun 2005 00:51:22 +0200
Subject: [R] RE: GARCH (1 , 1), Hill estimator of alpha, Pareto estimator]
Message-ID: <42A0DEEA.2060600@o2.pl>

Ukech U. Kidi wrote:
>    dax<- diff(log(DAX_CAC$DAX[1:1865]))
>    m1<- garch(dax)
> Error: couldn't find function "garch"
>    m1<- garch(dax[1:1865])
> Error: couldn't find function "garch"
>    m1<- garch(dax[1:1865])

I am sorry, but I forgot to change the addres to r-help in the reply.
Well, I am not sure, wheere do you want to get those data from, but if
you are able to do that:

>    dax<- diff(log(DAX_CAC$DAX[1:1865]))

Then it means, that you have managed to do that. The thing is, that
garch function is in the tseries package and I haven't seen any call to
it, so first try:

library(tseries)

and then:

m1<- garch(dax)
summary(m1)
plot(m1)

My scripts usually look like that:

library(tseries)

#import data from csv file with first row Open, High, Low, Close, Volume
wig20 <- read.csv("wig20.txt", sep=";", dec=",")

#multiply by 100, because sometimes it's easier to converge the model
r <- 100*diff(log(wig20$CLOSE))

kpss.test(r)
pp.test(r)

acf(r)
pacf(r)

#if there is no significant autocorelation:

y <- r - mean(r)
fit <- garch(y, order = c(1,1))
summary(fit)
plot(fit)

#If you need some particular results for further testing, then use:
ch <- predict(fit, genuine=TRUE)
e <- fit$residuals
#end do what you want or draw any other result this way

That's just the basic, but then you can improve it the way you want.
If you have lot of models to estimate, I suggest using sink() and
postscript(), for saving results, so after writing the code and sourcing
that into R by source(),  you can go and have a coffe and come back
later to browse the results. BTW, check also rmetrics.org. There are
also functions for garch modelling in fSeries.

Best regards,
Wojtek



From andy_liaw at merck.com  Sat Jun  4 00:58:36 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 3 Jun 2005 18:58:36 -0400
Subject: [R] rearrange data
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E920@usctmx1106.merck.com>

reshape() should do it:

> d
  V1 V2 V3 V4 V5
1 A1 B1 C1 D1 E1
2 A2 B2 C2 D2 E2
3 A3 B3 C3 D3 E3
> d2 <- reshape(d, varying=list(names(d)[1:4]), direction="long")[c(3,1)]
> d2[order(d2$V5),]
    V1 V5
1.1 A1 E1
1.2 B1 E1
1.3 C1 E1
1.4 D1 E1
2.1 A2 E2
2.2 B2 E2
2.3 C2 E2
2.4 D2 E2
3.1 A3 E3
3.2 B3 E3
3.3 C3 E3
3.4 D3 E3

Andy 

> From: jose silva
> 
> Dear all: 
> 
> I have this: 
> 
> A1 B1 C1 D1 E1 
> A2 B2 C2 D2 E2 
> A3 B3 C3 D3 E3 
> 
> And I want this 
> 
> A1 E1 
> B1 E1 
> C1 E1 
> D1 E1 
> A2 E2 
> B2 E2 
> C2 E2 
> D2 E2 
> A3 E3 
> B3 E3 
> C3 E3 
> D3 E3 
> 
> Example: 
> 
> m<- matrix(1:15,nrow=3,byrow=T) 
> m 
> v<- unlist(list(t(m[,1:4]))) 
> u<- rep(c(5,10,15),c(4,4,4)) 
> data.frame(v,u) 
> 
> This is the result I want but I would like to learn a simpler 
> way to do it. 
> Any clue? On the other hand, how can I reorganize the data in 
> the way it was in the begining? I hope someone can help me on this...
> Thanks in advance??
> 
> j. silva 
> [[alternative HTML version deleted]] 
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From deepayan at stat.wisc.edu  Sat Jun  4 01:23:08 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 3 Jun 2005 18:23:08 -0500
Subject: [R] lattice: change point color in densityplot()?
In-Reply-To: <20050603224928.432.qmail@web31305.mail.mud.yahoo.com>
References: <20050603224928.432.qmail@web31305.mail.mud.yahoo.com>
Message-ID: <200506031823.08888.deepayan@stat.wisc.edu>

On Friday 03 June 2005 17:49, M. K. wrote:
> When using the "densityplot" function from "lattice" library, is there
> a way to pick a different color *for the points only*?  (I'm using
> densityplot(..., plot.points=TRUE, ...))  Using "col" parameter changes
> the color for both, the points and the curve.

Indirectly. ?panel.densityplot says that there's a parameter for line color, 
so you could do 

densityplot(rnorm(100), col = 'red', col.line = 'cyan')

Deepayan



From Ted.Harding at nessie.mcc.ac.uk  Sat Jun  4 01:33:17 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 04 Jun 2005 00:33:17 +0100 (BST)
Subject: [R] p-value > 1 in fisher.test()
In-Reply-To: <XFMail.050603214524.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.050604003317.Ted.Harding@nessie.mcc.ac.uk>

On 03-Jun-05 Ted Harding wrote:
> And on mine
> 
> (A: PII, Red Had 9, R-1.8.0):
> 
>  ff <- c(0,10,250,5000); dim(ff) <- c(2,2);
> 
>  1-fisher.test(ff)$p.value
>  [1] 1.268219e-11
> 
> (B: PIII, SuSE 7.2, R-2.1.0beta):
> 
>  ff <- c(0,10,250,5000); dim(ff) <- c(2,2);
> 
>  1-fisher.test(ff)$p.value
>  [1] -1.384892e-12

I have a suggestion (maybe it should also go to R-devel).

There are many functions in R whose designated purpose is
to return the value of a probability (or a probability
density). This designated purpose is in the mind of the
person who has coded the function, and is implicit in its
usage.

Therefore I suggest that every such function should have
a built-in internal check that no probability should be
less than 0 (and if the primary computation yields such
a value then the function should set it exactly to zero),
and should not exceed 1 (in which case the function should
set it exactly to 1). [And, in view of recent echanges,
I would suggest exactly +0, not -0!]

Similar for any attempts to return a negative probability
density; while of course a positive value can be allowed
to be anything.

All probabilities would then be guaranteed to be "clean"
and issues like the Fisher exact test above would no longer
be even a tiny problem.

Implementing this in the possibly many cases where it is
not already present is no doubt a long-term (and tedious)
project.

Meanwhile, people who encounter problems due to its absence
can carry out their own checks and adjustments!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 04-Jun-05                                       Time: 00:02:32
------------------------------ XFMail ------------------------------



From chess.player at oninet.pt  Sat Jun  4 01:40:04 2005
From: chess.player at oninet.pt (jose silva)
Date: Sat, 04 Jun 2005 00:40:04 +0100
Subject: [R] rearrange data
Message-ID: <439ad0e11c934868a7defdede756d300@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050604/c09f2098/attachment.pl

From ross at biostat.ucsf.edu  Sat Jun  4 02:04:08 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 03 Jun 2005 17:04:08 -0700
Subject: [R] How to change the value of a class slot
Message-ID: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>

I defined an S4 class with a slot i.  Then I wrote a regular function
that attempted to increment i.

This didn't work, apparently because of the general rule that a function
can't change the values of its arguments outside the function.  I gather
there are ways around it, but the Green book admonishes "cheating on the
S evaluation model is to be avoided" (p. 190).

Thinking that class methods needed to an exception to this rule, I then
tried setMethod with the function I had written.  However, when I called
the function I got
> setMethod("nextPath", "CompletePathMaker", nextPath)
Creating a new generic function for 'nextPath' in '.GlobalEnv'
[1] "nextPath"
> nextPath(pm)
Error: protect(): protection stack overflow

I can change the value of the slot interactively, so the problem does
not appear to be that the slots are considered off-limits.

What do I need to do to update slot values?

Here are some possibly relevant code fragments
setClass("CompletePathMaker",
         representation(i="integer",
                        timeOffset="numeric", # to avoid 0's
                        truePaths="TruePaths")
         )
nextPath <- function(pm){ #pm is a CompletePathMaker
  pm at i <- pm at i+as.integer(1)
[etc]

I'm trying to make the class behave like an iterator, with i keeping
track of its location.  I'm sure there are more R'ish ways to go, but
I'm also pretty sure I'm going to want to be able to update slots.

Thanks.
Ross Boylan



From ggrothendieck at gmail.com  Sat Jun  4 02:18:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 3 Jun 2005 20:18:04 -0400
Subject: [R] factor vector manipulation
In-Reply-To: <cdf817830506031207778bfa3a@mail.gmail.com>
References: <cdf817830506031207778bfa3a@mail.gmail.com>
Message-ID: <971536df05060317187311fe2b@mail.gmail.com>

On 6/3/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> Hi,
> I have one question on factor vector.
> I have 3 factor vectors:
> 
> a<-factor(c("1", "2", "3"))
> b<-factor(c("a", "b", "c"))
> c<-factor(c("b", "a", "c"))
> 
> what I want is like:
>  c x
> 1 b 2
> 2 a 1
> 3 c 3
> 
> which means, I use b as keys and vector a as values and I find values for c.
> 
> I used the following codes:
> x<-c()
> d<-data.frame(b,a)
> for (each in 1:length(c)){   # i don't know why each in c did not work
>  tmp<-d$a[d$b==c[each]]                                  # question
>  x<-append(x, levels(tmp)[as.integer(tmp)])
> }
> 
> data.frame(c,x)
> 
> If someone has a better way to do it, please help!!
> 
> Also, I don't understand why I have to use levels() since w/o it, i
> only added the index for levels in to x.
> 
> BTW, when b is like 60,000, the process is very slow. Should I use hash here?
> 
> Thanks,
> 
> Weiwei
> 
cbind(c, x = a[match(c,b)])



From Achim.Zeileis at wu-wien.ac.at  Sat Jun  4 02:52:38 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 4 Jun 2005 02:52:38 +0200 (CEST)
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <NEBBIPHDAMMOKDKPOFFICEAMDHAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFICEAMDHAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.58.0506040248250.4534@thorin.ci.tuwien.ac.at>

Andy:

> I have a mv ts object:
>
> R > tsp(pg)
> [1] 1982 2003    1
> R > dim(pg)
> [1] 22 12
>
> and a univariate ts:
>
> R > tsp(rw)
> [1] 1690 1996    1
>
> Yet, when I try to intersect them:
>
> R > tsp(ts.intersect(rw, pg))
> [1] 1982 2176    1
>
> the process goes awry.
>
> How to I get rw and pg to be one ts that runs from 1982 to 1996 and has 13
> univariate time series in it. Any help appreciated.

Providing a reproducible example would be a first step...

For me the following works without any problems:

R> pg <- ts(matrix(rnorm(22 * 12), ncol = 12), start = 1982)
R> rw <- ts(rnorm(307), start = 1690)
R> tsp(pg)
[1] 1982 2003    1
R> dim(pg)
[1] 22 12
R> tsp(rw)
[1] 1690 1996    1
R> rwpg <- ts.intersect(rw, pg)
R> tsp(rwpg)
[1] 1982 1996    1
R> dim(rwpg)
[1] 15 13

Best,
Z



From spluque at gmail.com  Sat Jun  4 03:51:23 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Fri, 03 Jun 2005 20:51:23 -0500
Subject: [R] locator() via tcltk
Message-ID: <87slzy6g9w.fsf@gmail.com>

Hello,

I'm trying to write a function using tcltk to interactively modify a plot
and gather locator() data. I've read Peter's articles in Rnews, the help
pages in tcltk, http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/,
plus a post in R-help sometime ago, but haven't found a solution.
The idea goes something like this:

require(tcltk)
testplot <- function() {
  getcoords <- function(...) {
    locator(5)
  }
  x <- 1:1000
  y <- rnorm(1000)
  plot(x, y)
  base <- tktoplevel()
  loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
  quit.but <- tkbutton(base, text = "Quit",
                       command = function() tkdestroy(base))
  tkpack(loc.pts, quit.but)
}

I'd like testplot to return the value from getcoords. The "Get
coordinates" button seems to be correctly calling getcoords, and locator
is doing its job, but I don't know how to store its value. Any help is
very much appreciated.

Regards,
Sebastian
-- 
Sebastian P. Luque



From mk_lists at yahoo.ca  Sat Jun  4 03:57:05 2005
From: mk_lists at yahoo.ca (M. K.)
Date: Fri, 3 Jun 2005 21:57:05 -0400 (EDT)
Subject: [R] lattice: change point color in densityplot()?
In-Reply-To: <200506031823.08888.deepayan@stat.wisc.edu>
Message-ID: <20050604015705.9713.qmail@web31313.mail.mud.yahoo.com>


--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> Indirectly. ?panel.densityplot says that there's a parameter for line
> color, 
> so you could do 
> 
> densityplot(rnorm(100), col = 'red', col.line = 'cyan')

Ah, great, thanks.



From murdoch at stats.uwo.ca  Sat Jun  4 04:31:47 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 03 Jun 2005 22:31:47 -0400
Subject: [R] locator() via tcltk
In-Reply-To: <87slzy6g9w.fsf@gmail.com>
References: <87slzy6g9w.fsf@gmail.com>
Message-ID: <42A11293.4070303@stats.uwo.ca>

Sebastian Luque wrote:
> Hello,
> 
> I'm trying to write a function using tcltk to interactively modify a plot
> and gather locator() data. I've read Peter's articles in Rnews, the help
> pages in tcltk, http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/,
> plus a post in R-help sometime ago, but haven't found a solution.
> The idea goes something like this:
> 
> require(tcltk)
> testplot <- function() {
>   getcoords <- function(...) {
>     locator(5)
>   }
>   x <- 1:1000
>   y <- rnorm(1000)
>   plot(x, y)
>   base <- tktoplevel()
>   loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
>   quit.but <- tkbutton(base, text = "Quit",
>                        command = function() tkdestroy(base))
>   tkpack(loc.pts, quit.but)
> }
> 
> I'd like testplot to return the value from getcoords. The "Get
> coordinates" button seems to be correctly calling getcoords, and locator
> is doing its job, but I don't know how to store its value. Any help is
> very much appreciated.

You can do this with local functions.  For example,

  testplot <- function() {

    coords <- NA

    getcoords <- function(...) {
      coords <<- locator(5)
    }
    x <- 1:1000
    y <- rnorm(1000)
    plot(x, y)
    base <- tktoplevel()
    loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
    quit.but <- tkbutton(base, text = "Quit",
                         command = function() tkdestroy(base))
    tkpack(loc.pts, quit.but)
    return(function() coords)
  }

This stores the results in a variable named coords in the testplot 
frame.  testplot itself returns a function that can read that value. 
You'd use it like this:

lastval <- testplot()

## do some button clicking...

lastval()   # will print the result of the last click.

This is one of the somewhat weird but very nice features of R.  S-PLUS 
doesn't work this way.

Duncan Murdoch



From 0034058 at fudan.edu.cn  Sat Jun  4 04:59:14 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 04 Jun 2005 10:59:14 +0800
Subject: [R] the test result is quite different,why?
Message-ID: <0IHJ003K0H5EVW@mail.fudan.edu.cn>

data:http://fmwww.bc.edu/ec-p/data/wooldridge/CRIME4.dta

> a$call
lm(formula = clcrmrte ~ factor(year) + clprbarr + clprbcon + 
    clprbpri + clavgsen + clpolpc, data = cri)
> bptest(a,st=F)

        Breusch-Pagan test

data:  a 
BP = 34.4936, df = 10, p-value = 0.0001523

> bptest(a,st=T)

        studentized Breusch-Pagan test

data:  a 
BP = 10.9297, df = 10, p-value = 0.363

> ncv.test(a)
$formula
~fitted.values

$formula.name
[1] "Variance"

$ChiSquare
[1] 1.163501

$Df
[1] 1

$p
[1] 0.2807406

$test
[1] "Non-constant Variance Score Test"

attr(,"class")
[1] "chisq.test"
>



From hodgess at gator.dt.uh.edu  Sat Jun  4 05:53:12 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Fri, 3 Jun 2005 22:53:12 -0500
Subject: [R] locator()
Message-ID: <200506040353.j543rCPJ007939@gator.dt.uh.edu>

Dear Sebastian:

Here is a snippet of code that you may find interesting
for labeling points on the plane:

plane1 <- function(x,main1="") {
  plot(x,main=main1,ylab="  ")
  zz <- locator()
  text(zz$x,zz$y,
       labels=paste("(",round(zz$x,1),
         ",",round(zz$y,1),")",sep=""),
       pos=2,cex=.55)
    invisible()
}


Hope this helps!

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From spluque at gmail.com  Sat Jun  4 05:58:57 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Fri, 3 Jun 2005 22:58:57 -0500
Subject: [R] Re: locator() via tcltk
In-Reply-To: <42A11293.4070303@stats.uwo.ca>
References: <87slzy6g9w.fsf@gmail.com> <42A11293.4070303@stats.uwo.ca>
Message-ID: <52856c8c05060320581725c7e6@mail.gmail.com>

That's very interesting, thanks a lot Duncan. I should re-read the R
Language manual in more detail.

Sebastian

On 6/3/05, Duncan Murdoch <murdoch at stats.uwo.ca> wrote:
> Sebastian Luque wrote:
> > Hello,
> > 
> > I'm trying to write a function using tcltk to interactively modify a plot
> > and gather locator() data. I've read Peter's articles in Rnews, the help
> > pages in tcltk, http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/,
> > plus a post in R-help sometime ago, but haven't found a solution.
> > The idea goes something like this:
> > 
> > require(tcltk)
> > testplot <- function() {
> >   getcoords <- function(...) {
> >     locator(5)
> >   }
> >   x <- 1:1000
> >   y <- rnorm(1000)
> >   plot(x, y)
> >   base <- tktoplevel()
> >   loc.pts <- tkbutton(base, text = "Get coordinates", command =
> getcoords)
> >   quit.but <- tkbutton(base, text = "Quit",
> >                        command = function() tkdestroy(base))
> >   tkpack(loc.pts, quit.but)
> > }
> > 
> > I'd like testplot to return the value from getcoords. The "Get
> > coordinates" button seems to be correctly calling getcoords, and locator
> > is doing its job, but I don't know how to store its value. Any help is
> > very much appreciated.
> 
> You can do this with local functions.  For example,
> 
>   testplot <- function() {
> 
>     coords <- NA
> 
>     getcoords <- function(...) {
>       coords <<- locator(5)
>     }
>     x <- 1:1000
>     y <- rnorm(1000)
>     plot(x, y)
>     base <- tktoplevel()
>     loc.pts <- tkbutton(base, text = "Get coordinates", command =
> getcoords)
>     quit.but <- tkbutton(base, text = "Quit",
>                          command = function() tkdestroy(base))
>     tkpack(loc.pts, quit.but)
>     return(function() coords)
>   }
> 
> This stores the results in a variable named coords in the testplot 
> frame.  testplot itself returns a function that can read that value. 
> You'd use it like this:
> 
> lastval <- testplot()
> 
> ## do some button clicking...
> 
> lastval()   # will print the result of the last click.
> 
> This is one of the somewhat weird but very nice features of R.  S-PLUS 
> doesn't work this way.
> 
> Duncan Murdoch
> 


-- 
Sebastian Luque
Department of Biology
Memorial University of Newfoundland



From 0034058 at fudan.edu.cn  Sat Jun  4 10:10:37 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 04 Jun 2005 16:10:37 +0800
Subject: [R] hausman test
Message-ID: <0IHJ007KOVK8QK@mail.fudan.edu.cn>

i am dealing with my panle data,and i have to decided if i should use fixed-effect model or random effect model.i know the hausman test can help.
and anyone knows if any function can do these?

thank you.



From ripley at stats.ox.ac.uk  Sat Jun  4 10:39:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 4 Jun 2005 09:39:33 +0100 (BST)
Subject: [R] locator() via tcltk
In-Reply-To: <87slzy6g9w.fsf@gmail.com>
References: <87slzy6g9w.fsf@gmail.com>
Message-ID: <Pine.LNX.4.61.0506040930530.32138@gannet.stats>

You have two asynchronous processes here: your R function will return, 
leaving a Tk widget up.  Duncan Murdoch's solution is to give you a 
function that will retrieve at some future stage the result of the last 
press (if any) of "Get coordinates" button.  Is that what you want?

I think it is more likely you want to wait for the Tk interaction and then 
return the results, that is use a `modal' widget.  If so, take a look at 
the examples in src/library/tcltk/R/utils.R which are modal and return 
their results.

On Fri, 3 Jun 2005, Sebastian Luque wrote:

> Hello,
>
> I'm trying to write a function using tcltk to interactively modify a plot
> and gather locator() data. I've read Peter's articles in Rnews, the help
> pages in tcltk, http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/,
> plus a post in R-help sometime ago, but haven't found a solution.
> The idea goes something like this:
>
> require(tcltk)
> testplot <- function() {
>  getcoords <- function(...) {
>    locator(5)
>  }
>  x <- 1:1000
>  y <- rnorm(1000)
>  plot(x, y)
>  base <- tktoplevel()
>  loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
>  quit.but <- tkbutton(base, text = "Quit",
>                       command = function() tkdestroy(base))
>  tkpack(loc.pts, quit.but)
> }
>
> I'd like testplot to return the value from getcoords. The "Get
> coordinates" button seems to be correctly calling getcoords, and locator
> is doing its job, but I don't know how to store its value. Any help is
> very much appreciated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jean.Coursol at math.u-psud.fr  Sat Jun  4 11:54:36 2005
From: Jean.Coursol at math.u-psud.fr (Jean.Coursol@math.u-psud.fr)
Date: Sat,  4 Jun 2005 11:54:36 +0200
Subject: [R] locator() via tcltk
In-Reply-To: <Pine.LNX.4.61.0506040930530.32138@gannet.stats>
References: <87slzy6g9w.fsf@gmail.com>
	<Pine.LNX.4.61.0506040930530.32138@gannet.stats>
Message-ID: <1117878876.42a17a5ce3b3a@webmail.math.u-psud.fr>

To illustrate (?) Professor Ripley comments, I send you an example (stolen in
various places...).

Note the tkwm.resizable(tt, 0, 0) directive that prevents the window rescaling
(if not,the coordinates will not be correct).

#
# Getting the mouse coords with TclTk
#

# Two possibilities: tkrplot package or Img library
# ========================================================
# A) USING the tkrplot package
#
# 1a) Opening an X11 device named "XImage" (current device)

     X11("XImage",width=600, height=600)

# 2a) Putting a contour...

     x <- 10*(1:nrow(volcano))
     y <- 10*(1:ncol(volcano))
     image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
     contour(x, y, volcano, levels = seq(90, 200, by = 5),
             add = TRUE, col = "peru")
     axis(1, at = seq(100, 800, by = 100))
     axis(2, at = seq(100, 600, by = 100))
     box()
     title(main = "Maunga Whau Volcano", font.main = 4)

     # image dimensions (before "XImage" deletion)

     parPlotSize <- par('plt')
     usrCoords   <- par('usr')

# 3a) tkrplot loading (that load tcltk package)
#    The tcltk directive extended by tkrplot
#         image create Rplot truc
#    put the "XImage" image in the image Tcl objet named "truc"
#    and close "XImage"

     library(tkrplot)

     .Tcl("image create Rplot truc")

# ========================================================
# B) USING the Tcl Img library (for .jpg and .png)

# 1b) Opening a jpeg (or png) device

      jpeg("monimage.jpg",width=800, height=800)

# 2b)  = 2) + closing jpeg device

     x <- 10*(1:nrow(volcano))
     y <- 10*(1:ncol(volcano))
     image(x, y, volcano, col = terrain.colors(100), axes = FALSE)
     contour(x, y, volcano, levels = seq(90, 200, by = 5),
             add = TRUE, col = "peru")
     axis(1, at = seq(100, 800, by = 100))
     axis(2, at = seq(100, 600, by = 100))
     box()
     title(main = "Maunga Whau Volcano", font.main = 4)

     # image dimensions

     parPlotSize <- par('plt')
     usrCoords   <- par('usr')
     dev.off()

# 3b) Reading the "monimage" file in the Tcl objet named "truc"
#     and deleting the file

      library(tcltk)
      tclRequire('Img')  # to read a .jpg file
      truc <- tclVar()

      tkcmd("image","create","photo",truc,file="monimage.jpg")
      unlink("monimage.jpg")

# ========================================================
# COMMON STEPS for both possibilities

# 4) Opening the Tk window and putting the image inside

     tt <- tktoplevel()
     tkpack(ll <- tklabel(tt, image=truc), fill="both")
     tkwm.resizable(tt, 0, 0)             # non resizable window

# 5) Callback

     # Calculus of dimensions initial image/tk image

     width  <- as.numeric(tclvalue(tkwinfo("reqwidth",ll)))
     height <- as.numeric(tclvalue(tkwinfo("reqheight",ll)))

     xMin   <- parPlotSize[1] * width
     xRange <- (parPlotSize[2] - parPlotSize[1]) * width
     rangeX <- (usrCoords[2] - usrCoords[1])/xRange

     yMin   <- parPlotSize[3] * height
     yRange <- (parPlotSize[4] - parPlotSize[3]) * height
     rangeY <- (usrCoords[4] - usrCoords[3])/yRange

     OnLeftClick = function(x,y) {
       xClick <- as.numeric(x)
       yClick <- height - as.numeric(y)
       xPlotCoord <- usrCoords[1]+(xClick-xMin)*rangeX
       yPlotCoord <- usrCoords[3]+(yClick-yMin)*rangeY

       ##  AN ACTION ##
       print(c(xPlotCoord, yPlotCoord))
     }

tkbind(tt, "<Button-1>", OnLeftClick)                   # Action
tkbind(tt, "<Button-3>", function() tclvalue(done) = 1) # Exit

# 6) Loop waiting the events

done = tclVar(0)
tkwait.variable(done)
tkdestroy(tt)

###

-- 



Selon Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> You have two asynchronous processes here: your R function will return,
> leaving a Tk widget up.  Duncan Murdoch's solution is to give you a
> function that will retrieve at some future stage the result of the last
> press (if any) of "Get coordinates" button.  Is that what you want?
>
> I think it is more likely you want to wait for the Tk interaction and then
> return the results, that is use a `modal' widget.  If so, take a look at
> the examples in src/library/tcltk/R/utils.R which are modal and return
> their results.
>
> On Fri, 3 Jun 2005, Sebastian Luque wrote:
>
> > Hello,
> >
> > I'm trying to write a function using tcltk to interactively modify a plot
> > and gather locator() data. I've read Peter's articles in Rnews, the help
> > pages in tcltk, http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/,
> > plus a post in R-help sometime ago, but haven't found a solution.
> > The idea goes something like this:
> >
> > require(tcltk)
> > testplot <- function() {
> >  getcoords <- function(...) {
> >    locator(5)
> >  }
> >  x <- 1:1000
> >  y <- rnorm(1000)
> >  plot(x, y)
> >  base <- tktoplevel()
> >  loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
> >  quit.but <- tkbutton(base, text = "Quit",
> >                       command = function() tkdestroy(base))
> >  tkpack(loc.pts, quit.but)
> > }
> >
> > I'd like testplot to return the value from getcoords. The "Get
> > coordinates" button seems to be correctly calling getcoords, and locator
> > is doing its job, but I don't know how to store its value. Any help is
> > very much appreciated.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From slist at oomvanlieshout.net  Sat Jun  4 14:13:24 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sat, 04 Jun 2005 14:13:24 +0200
Subject: [R] How to 'de-cross' a table?
In-Reply-To: <Pine.A41.4.61b.0506030929490.263790@homer12.u.washington.edu>
References: <42A06A48.5060609@oomvanlieshout.net>
	<Pine.A41.4.61b.0506030929490.263790@homer12.u.washington.edu>
Message-ID: <42A19AE4.5090804@oomvanlieshout.net>

Hi Thomas,

Your code works perfectly!

Thanks a lot,

Sander.



Thomas Lumley wrote:
> On Fri, 3 Jun 2005, Sander Oom wrote:
> 
>> Dear R users,
>>
>> I have received a table in the following format:
>>
>> id  a   b  c1  c2  d1  d2
>> 1   1   1  65  97  78  98
>> 2   1   2  65  97  42  97
>> 3   2   1  65  68  97  98
>> 4   2   2  65  97  97  98
>>
>> Factors of the design are: a, b, and e, where e has levels c and d. The
>> levels c and d then have 2 replicates (r) each.
>>
>> Now I would like to get:
>>
>> id  a   b   e   r  value
>> 1   1   1   c   1  65
>> 2   1   1   c   2  97
>> 3   1   1   d   1  78
>> 4   1   1   d   2  98
>>
>> Any suggestions on how to tackle this? I'm not sure what the 'task' is
>> called, so struggle to effectively search the web or R help.
>>
> 
> reshape() is the probably function.  It seems you need to use it twice, 
> for the two levels of uncrossing
>> d1<-reshape(d, direction="long", 
> varying=list(c("c1","d1"),c("c2","d2")), 
> v.names=c("rep1","rep2"),timevar="r", times=c("c","d"))
>> d1
>     id a b r rep1 rep2
> 1.c  1 1 1 c   65   97
> 2.c  2 1 2 c   65   97
> 3.c  3 2 1 c   65   68
> 4.c  4 2 2 c   65   97
> 1.d  1 1 1 d   78   98
> 2.d  2 1 2 d   42   97
> 3.d  3 2 1 d   97   98
> 4.d  4 2 2 d   97   98
>> reshape(d1, direction="long", varying=list(c("rep1","rep2")), 
> v.names="value",idvar="tmp", timevar="r")
>     id a b r value tmp
> 1.1  1 1 1 1    65   1
> 2.1  2 1 2 1    65   2
> 3.1  3 2 1 1    65   3
> 4.1  4 2 2 1    65   4
> 5.1  1 1 1 1    78   5
> 6.1  2 1 2 1    42   6
> 7.1  3 2 1 1    97   7
> 8.1  4 2 2 1    97   8
> 1.2  1 1 1 2    97   1
> 2.2  2 1 2 2    97   2
> 3.2  3 2 1 2    68   3
> 4.2  4 2 2 2    97   4
> 5.2  1 1 1 2    98   5
> 6.2  2 1 2 2    97   6
> 7.2  3 2 1 2    98   7
> 8.2  4 2 2 2    98   8
> 
> You can then delete the `tmp` variable if you don't need it.
> 
> An easier way to uncross would be with stack()
>> stack(d[,4:7])
>    values ind
> 1      65  c1
> 2      65  c1
> 3      65  c1
> 4      65  c1
> 5      97  c2
> 6      97  c2
> 7      68  c2
> 8      97  c2
> 9      78  d1
> 10     42  d1
> 11     97  d1
> 12     97  d1
> 13     98  d2
> 14     97  d2
> 15     98  d2
> 16     98  d2
> 
> but you lose the other variables.
> 
> 
>     -thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From 0034058 at fudan.edu.cn  Sat Jun  4 14:42:24 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 04 Jun 2005 20:42:24 +0800
Subject: [R] can R do Fixed-effects (within) regression (panel data)?
Message-ID: <0IHK00763855R4@mail.fudan.edu.cn>

i want to ask 2 questions.

1) can R do Random-effects GLS regression which i can get from Stata?
the following result is frome Stata.can I get the alike result from R?

xtreg lwage educ black hisp exper expersq married union, re

Random-effects GLS regression                   Number of obs      =      4360
Group variable (i) : nr                         Number of groups   =       545

R-sq:  within  = 0.1799                         Obs per group: min =         8
       between = 0.1860                                        avg =       8.0
       overall = 0.1830                                        max =         8

Random effects u_i ~ Gaussian                   Wald chi2(14)      =    957.77
corr(u_i, X)       = 0 (assumed)                Prob > chi2        =    0.0000

------------------------------------------------------------------------------
       lwage |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
        educ |   .0918763   .0106597     8.62   0.000     .0709836    .1127689
......................
         d86 |   .0919476   .0712293     1.29   0.197    -.0476592    .2315544
         d87 |   .1349289   .0813135     1.66   0.097    -.0244427    .2943005
       _cons |   .0235864   .1506683     0.16   0.876     -.271718    .3188907
-------------+----------------------------------------------------------------
     sigma_u |  .32460315
     sigma_e |  .35099001
         rho |  .46100216   (fraction of variance due to u_i)  


2)

can R do Fixed-effects (within) regression as Stata's xtreg?

the followng example is from 
"Introductory Econometrics: A Modern Approach" by Jeffrey M. Wooldridge
Chapter 14 - Advanced Panel Data Methods

use http://fmwww.bc.edu/ec-p/data/wooldridge/JTRAIN
iis fcode
tis year
xtreg lscrap d88 d89 grant grant_1, fe

Fixed-effects (within) regression               Number of obs      =       162
Group variable (i) : fcode                      Number of groups   =        54

R-sq:  within  = 0.2010                         Obs per group: min =         3
       between = 0.0079                                        avg =       3.0
       overall = 0.0068                                        max =         3

                                                F(4,104)           =      6.54
corr(u_i, Xb)  = -0.0714                        Prob > F           =    0.0001

------------------------------------------------------------------------------
      lscrap |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         d88 |  -.0802157   .1094751    -0.73   0.465    -.2973089    .1368776
         d89 |  -.2472028   .1332183    -1.86   0.066    -.5113797     .016974
       grant |  -.2523149    .150629    -1.68   0.097    -.5510178     .046388
     grant_1 |  -.4215895      .2102    -2.01   0.047    -.8384239   -.0047551
       _cons |    .597434   .0677344     8.82   0.000     .4631142    .7317539
-------------+----------------------------------------------------------------
     sigma_u |   1.438982
     sigma_e |   .4977442
         rho |  .89313867   (fraction of variance due to u_i)
------------------------------------------------------------------------------
F test that all u_i=0:     F(53, 104) =    24.66             Prob > F = 0.0000

thank you!!



From dmb at mrc-dunn.cam.ac.uk  Sat Jun  4 15:13:54 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 4 Jun 2005 14:13:54 +0100 (BST)
Subject: [R] barplot and missing values?
Message-ID: <Pine.LNX.4.21.0506041357180.12482-100000@mail.mrc-dunn.cam.ac.uk>


I want to include missing values in my barplot to get the correct x-axis,
for example,

x <- c(1,2,3,4, 9)
y <- c(2,4,6,8,18)

barplot(y)

The above looks wrong because the last height in y should be a long way
over.

So I want to do something like...

x <- c(1,2,3,4,5,6,7,8, 9)
y <- c(2,4,6,8,0,0,0,0,18)

barplot(y)

However... 

I am actually using barplot2 to use the "log='y'" function, so I can't use
zero values on a log scale...

So I need...

x <- c(1,2,3,4, 5, 6, 7, 8, 9)
y <- c(2,4,6,8,NA,NA,NA,NA,18)

barplot(y)

But that don't work.

Am I missing something?


To avoid confusion here is my data...

> dat.y.plot
   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
Ho  653   80  132   10   34    3   10    0    7     2     7     7
He  139   56   69    6   24    3   11    3    2     1     2     6
attr(,"names")
 [1]
"2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12"  ">12"
[13] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
> 


And here is what I call...

barplot(dat.y.plot,
        ylim=c(0,max(dat.y.plot + 50)), # I don't like the default
        beside=T,
        names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
        cex.axis=1.5,
        cex.names=1.5,
        legend=T
        )

Which is fine (except I don't know why I still need names.arg).

Then I try...


library(gregmisc)

barplot2(dat.y.plot+1, log='y',
        beside=T,
        names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
        cex.axis=1.5,
        cex.names=1.5,
        legend=T
        )

Which fails because of the zero. If I try ...

dat.y.plot[dat.y.plot==0] <- NA

It fails because of the NA.

Any suggestions?



From ligges at statistik.uni-dortmund.de  Sat Jun  4 15:27:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 04 Jun 2005 15:27:11 +0200
Subject: [R] barplot and missing values?
In-Reply-To: <Pine.LNX.4.21.0506041357180.12482-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506041357180.12482-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <42A1AC2F.5040104@statistik.uni-dortmund.de>

Dan Bolser wrote:
> I want to include missing values in my barplot to get the correct x-axis,
> for example,
> 
> x <- c(1,2,3,4, 9)
> y <- c(2,4,6,8,18)
> 
> barplot(y)
> 
> The above looks wrong because the last height in y should be a long way
> over.
> 
> So I want to do something like...
> 
> x <- c(1,2,3,4,5,6,7,8, 9)
> y <- c(2,4,6,8,0,0,0,0,18)
> 
> barplot(y)
> 
> However... 
> 
> I am actually using barplot2 to use the "log='y'" function, so I can't use
> zero values on a log scale...
> 
> So I need...
> 
> x <- c(1,2,3,4, 5, 6, 7, 8, 9)
> y <- c(2,4,6,8,NA,NA,NA,NA,18)
> 
> barplot(y)
> 
> But that don't work.


Actually, it works, at least for me (R-2.1.0, WinNT, but you have not 
told us your details!).

BTW: In the meantime package gregmisc has been superseded by the 
gregmisc bundle, and later on by a number of packages (such as gtools, 
gdata, ...).

Your setup seems to be rather outdated.

Uwe Ligges



> Am I missing something?
> 
> 
> To avoid confusion here is my data...
> 
> 
>>dat.y.plot
> 
>    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> Ho  653   80  132   10   34    3   10    0    7     2     7     7
> He  139   56   69    6   24    3   11    3    2     1     2     6
> attr(,"names")
>  [1]
> "2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12"  ">12"
> [13] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
> 
> 
> 
> And here is what I call...
> 
> barplot(dat.y.plot,
>         ylim=c(0,max(dat.y.plot + 50)), # I don't like the default
>         beside=T,
>         names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>         cex.axis=1.5,
>         cex.names=1.5,
>         legend=T
>         )
> 
> Which is fine (except I don't know why I still need names.arg).
> 
> Then I try...
> 
> 
> library(gregmisc)
> 
> barplot2(dat.y.plot+1, log='y',
>         beside=T,
>         names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>         cex.axis=1.5,
>         cex.names=1.5,
>         legend=T
>         )
> 
> Which fails because of the zero. If I try ...
> 
> dat.y.plot[dat.y.plot==0] <- NA
> 
> It fails because of the NA.
> 
> Any suggestions?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luke at stat.uiowa.edu  Sat Jun  4 15:23:04 2005
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Sat, 4 Jun 2005 08:23:04 -0500 (CDT)
Subject: [R] [R-pkgs] New CRAN package misc3d
Message-ID: <Pine.LNX.4.63.0506040820500.5600@itasca2.wildberry.org>

The misc3d package provides a small collection of mostly rgl-based
functions for 3D data:

   contour3d     Uses rgl to render isosurfaces, or three-dimensional
                 contours, computed by the marching cubes algorithm.

   image3d       Crude 3d analog of image() using rgl to plot points on a
 		three dimensional grid representing values in a three
 		dimensional array. Assumes high values are inside and
 		uses alpha blending to make outside points more
 		transparent.

   lines3d       Draws connected line segments in an rgl window.

   parametric3d  Plots a two-parameter surface in three dimensions in
                 rgl.  Based on Mathematica's Param3D

   slices3d      Uses tkrplot to create an interactive slice view of three or
                 four dimensional volume data, such as MRI data.


-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From dmb at mrc-dunn.cam.ac.uk  Sat Jun  4 15:32:47 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 4 Jun 2005 14:32:47 +0100 (BST)
Subject: [R] barplot and missing values?
In-Reply-To: <42A1AC2F.5040104@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.21.0506041431170.12482-100000@mail.mrc-dunn.cam.ac.uk>

On Sat, 4 Jun 2005, Uwe Ligges wrote:

>Dan Bolser wrote:
>> I want to include missing values in my barplot to get the correct x-axis,
>> for example,
>> 
>> x <- c(1,2,3,4, 9)
>> y <- c(2,4,6,8,18)
>> 
>> barplot(y)
>> 
>> The above looks wrong because the last height in y should be a long way
>> over.
>> 
>> So I want to do something like...
>> 
>> x <- c(1,2,3,4,5,6,7,8, 9)
>> y <- c(2,4,6,8,0,0,0,0,18)
>> 
>> barplot(y)
>> 
>> However... 
>> 
>> I am actually using barplot2 to use the "log='y'" function, so I can't use
>> zero values on a log scale...
>> 
>> So I need...
>> 
>> x <- c(1,2,3,4, 5, 6, 7, 8, 9)
>> y <- c(2,4,6,8,NA,NA,NA,NA,18)
>> 
>> barplot(y)
>> 
>> But that don't work.
>
>
>Actually, it works, at least for me (R-2.1.0, WinNT, but you have not 
>told us your details!).
>
>BTW: In the meantime package gregmisc has been superseded by the 
>gregmisc bundle, and later on by a number of packages (such as gtools, 
>gdata, ...).
>
>Your setup seems to be rather outdated.

yeah :(

R 2.0.0 (2004-10-04).

I will upgrade to 2.1.0 (latest stable?)

Instead of gregmisc what should I use to get barplot2?

Will barplot() ever become barplot2()

I will try upgrading....

Dan.


>
>Uwe Ligges
>
>
>
>> Am I missing something?
>> 
>> 
>> To avoid confusion here is my data...
>> 
>> 
>>>dat.y.plot
>> 
>>    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
>> Ho  653   80  132   10   34    3   10    0    7     2     7     7
>> He  139   56   69    6   24    3   11    3    2     1     2     6
>> attr(,"names")
>>  [1]
>> "2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12"  ">12"
>> [13] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
>> 
>> 
>> 
>> And here is what I call...
>> 
>> barplot(dat.y.plot,
>>         ylim=c(0,max(dat.y.plot + 50)), # I don't like the default
>>         beside=T,
>>         names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>>         cex.axis=1.5,
>>         cex.names=1.5,
>>         legend=T
>>         )
>> 
>> Which is fine (except I don't know why I still need names.arg).
>> 
>> Then I try...
>> 
>> 
>> library(gregmisc)
>> 
>> barplot2(dat.y.plot+1, log='y',
>>         beside=T,
>>         names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>>         cex.axis=1.5,
>>         cex.names=1.5,
>>         legend=T
>>         )
>> 
>> Which fails because of the zero. If I try ...
>> 
>> dat.y.plot[dat.y.plot==0] <- NA
>> 
>> It fails because of the NA.
>> 
>> Any suggestions?
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Sat Jun  4 15:40:39 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 04 Jun 2005 15:40:39 +0200
Subject: [R] barplot and missing values?
In-Reply-To: <Pine.LNX.4.21.0506041431170.12482-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506041431170.12482-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <42A1AF57.1010402@statistik.uni-dortmund.de>

Dan Bolser wrote:

> On Sat, 4 Jun 2005, Uwe Ligges wrote:
> 
> 
>>Dan Bolser wrote:
>>
>>>I want to include missing values in my barplot to get the correct x-axis,
>>>for example,
>>>
>>>x <- c(1,2,3,4, 9)
>>>y <- c(2,4,6,8,18)
>>>
>>>barplot(y)
>>>
>>>The above looks wrong because the last height in y should be a long way
>>>over.
>>>
>>>So I want to do something like...
>>>
>>>x <- c(1,2,3,4,5,6,7,8, 9)
>>>y <- c(2,4,6,8,0,0,0,0,18)
>>>
>>>barplot(y)
>>>
>>>However... 
>>>
>>>I am actually using barplot2 to use the "log='y'" function, so I can't use
>>>zero values on a log scale...
>>>
>>>So I need...
>>>
>>>x <- c(1,2,3,4, 5, 6, 7, 8, 9)
>>>y <- c(2,4,6,8,NA,NA,NA,NA,18)
>>>
>>>barplot(y)
>>>
>>>But that don't work.
>>
>>
>>Actually, it works, at least for me (R-2.1.0, WinNT, but you have not 
>>told us your details!).
>>
>>BTW: In the meantime package gregmisc has been superseded by the 
>>gregmisc bundle, and later on by a number of packages (such as gtools, 
>>gdata, ...).
>>
>>Your setup seems to be rather outdated.
> 
> 
> yeah :(
> 
> R 2.0.0 (2004-10-04).


Hmm. I just tested with R-1.9.1, and your last example even works with 
that one...


> I will upgrade to 2.1.0 (latest stable?)
> 
> Instead of gregmisc what should I use to get barplot2?

package "gplots"


You example "y" is also handled perfectly well by barplot2() on my 
system, BTW.

Uwe Ligges




> Will barplot() ever become barplot2()
> 
> I will try upgrading....
> 
> Dan.
> 
> 
> 
>>Uwe Ligges
>>
>>
>>
>>
>>>Am I missing something?
>>>
>>>
>>>To avoid confusion here is my data...
>>>
>>>
>>>
>>>>dat.y.plot
>>>
>>>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
>>>Ho  653   80  132   10   34    3   10    0    7     2     7     7
>>>He  139   56   69    6   24    3   11    3    2     1     2     6
>>>attr(,"names")
>>> [1]
>>>"2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12"  ">12"
>>>[13] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
>>>
>>>
>>>
>>>And here is what I call...
>>>
>>>barplot(dat.y.plot,
>>>        ylim=c(0,max(dat.y.plot + 50)), # I don't like the default
>>>        beside=T,
>>>        names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>>>        cex.axis=1.5,
>>>        cex.names=1.5,
>>>        legend=T
>>>        )
>>>
>>>Which is fine (except I don't know why I still need names.arg).
>>>
>>>Then I try...
>>>
>>>
>>>library(gregmisc)
>>>
>>>barplot2(dat.y.plot+1, log='y',
>>>        beside=T,
>>>        names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>>>        cex.axis=1.5,
>>>        cex.names=1.5,
>>>        legend=T
>>>        )
>>>
>>>Which fails because of the zero. If I try ...
>>>
>>>dat.y.plot[dat.y.plot==0] <- NA
>>>
>>>It fails because of the NA.
>>>
>>>Any suggestions?
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>



From dmb at mrc-dunn.cam.ac.uk  Sat Jun  4 15:50:42 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 4 Jun 2005 14:50:42 +0100 (BST)
Subject: [R] barplot and missing values?
In-Reply-To: <42A1AF57.1010402@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.21.0506041442410.12482-100000@mail.mrc-dunn.cam.ac.uk>

On Sat, 4 Jun 2005, Uwe Ligges wrote:

>Dan Bolser wrote:
>
>> On Sat, 4 Jun 2005, Uwe Ligges wrote:
>> 
>> 
>>>Dan Bolser wrote:
>>>
>>>>I want to include missing values in my barplot to get the correct x-axis,
>>>>for example,
>>>>
>>>>x <- c(1,2,3,4, 9)
>>>>y <- c(2,4,6,8,18)
>>>>
>>>>barplot(y)
>>>>
>>>>The above looks wrong because the last height in y should be a long way
>>>>over.
>>>>
>>>>So I want to do something like...
>>>>
>>>>x <- c(1,2,3,4,5,6,7,8, 9)
>>>>y <- c(2,4,6,8,0,0,0,0,18)
>>>>
>>>>barplot(y)
>>>>
>>>>However... 
>>>>
>>>>I am actually using barplot2 to use the "log='y'" function, so I can't use
>>>>zero values on a log scale...
>>>>
>>>>So I need...
>>>>
>>>>x <- c(1,2,3,4, 5, 6, 7, 8, 9)
>>>>y <- c(2,4,6,8,NA,NA,NA,NA,18)
>>>>
>>>>barplot(y)
>>>>
>>>>But that don't work.
>>>
>>>
>>>Actually, it works, at least for me (R-2.1.0, WinNT, but you have not 
>>>told us your details!).
>>>
>>>BTW: In the meantime package gregmisc has been superseded by the 
>>>gregmisc bundle, and later on by a number of packages (such as gtools, 
>>>gdata, ...).
>>>
>>>Your setup seems to be rather outdated.
>> 
>> 
>> yeah :(
>> 
>> R 2.0.0 (2004-10-04).
>
>
>Hmm. I just tested with R-1.9.1, and your last example even works with 
>that one...
>
>
>> I will upgrade to 2.1.0 (latest stable?)
>> 
>> Instead of gregmisc what should I use to get barplot2?
>
>package "gplots"
>
>
>You example "y" is also handled perfectly well by barplot2() on my 
>system, BTW.
>


This must be because of the "log='y'" option that I am using here.

y <- c(2,4,6,8,NA,NA,NA,NA,18)

barplot2(y,log='y')

Above fails.


I appreciate that what I am trying to do is somewhat artificial (handle
zero values on a log scale), but it does reflect the data I have.

I tried plot(..., type='h'), but that dosn't do the "beside=T" stuff that
I want to do.

I am now trying things like...

barplot2(
  dat.y.plot + 0.11, # Dirty hack
  offset=-0.1,       #
  xpd=F,             #
  log='y',
  beside=T
)

Which looks messy. 

Any way to cleanly handle NA values with barplot2 on a log scale
(log='y')?



Thanks for your help :)



>Uwe Ligges
>
>
>
>
>> Will barplot() ever become barplot2()
>> 
>> I will try upgrading....
>> 
>> Dan.
>> 
>> 
>> 
>>>Uwe Ligges
>>>
>>>
>>>
>>>
>>>>Am I missing something?
>>>>
>>>>
>>>>To avoid confusion here is my data...
>>>>
>>>>
>>>>
>>>>>dat.y.plot
>>>>
>>>>   [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
>>>>Ho  653   80  132   10   34    3   10    0    7     2     7     7
>>>>He  139   56   69    6   24    3   11    3    2     1     2     6
>>>>attr(,"names")
>>>> [1]
>>>>"2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12"  ">12"
>>>>[13] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
>>>>
>>>>
>>>>
>>>>And here is what I call...
>>>>
>>>>barplot(dat.y.plot,
>>>>        ylim=c(0,max(dat.y.plot + 50)), # I don't like the default
>>>>        beside=T,
>>>>        names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>>>>        cex.axis=1.5,
>>>>        cex.names=1.5,
>>>>        legend=T
>>>>        )
>>>>
>>>>Which is fine (except I don't know why I still need names.arg).
>>>>
>>>>Then I try...
>>>>
>>>>
>>>>library(gregmisc)
>>>>
>>>>barplot2(dat.y.plot+1, log='y',
>>>>        beside=T,
>>>>        names.arg=c('2','3','4','5','6','7','8','9','10','11','12','>12'),
>>>>        cex.axis=1.5,
>>>>        cex.names=1.5,
>>>>        legend=T
>>>>        )
>>>>
>>>>Which fails because of the zero. If I try ...
>>>>
>>>>dat.y.plot[dat.y.plot==0] <- NA
>>>>
>>>>It fails because of the NA.
>>>>
>>>>Any suggestions?
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>



From ligges at statistik.uni-dortmund.de  Sat Jun  4 16:07:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 04 Jun 2005 16:07:21 +0200
Subject: [R] barplot and missing values?
In-Reply-To: <Pine.LNX.4.21.0506041442410.12482-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506041442410.12482-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <42A1B599.7030607@statistik.uni-dortmund.de>

Dan Bolser wrote:

[all previous stuff deleted]

I see, what comes out of this longish thread is:

  - barplot() and barplot2() both have deficiencies for you particular 
examples, so it is time to provide patches for both barplot() and 
barplot2() (for the latter, you might want to contact the package 
maintainer as well) ...

  - Please provide *reproducible* examples (yours was not, because 
"log='y'" was missing). Hence the relevant example we were obviously 
talking about is:

   y <- c(2,4,6,8,NA,NA,NA,NA,18)
   barplot(y, log="y")

   library(gplots)
   barplot2(y, log="y")


Uwe Ligges



From MSchwartz at mn.rr.com  Sat Jun  4 16:19:53 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 04 Jun 2005 09:19:53 -0500
Subject: [R] barplot and missing values?
In-Reply-To: <Pine.LNX.4.21.0506041442410.12482-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506041442410.12482-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1117894793.4261.27.camel@horizons.localdomain>

On Sat, 2005-06-04 at 14:50 +0100, Dan Bolser wrote:

<snip>

> This must be because of the "log='y'" option that I am using here.
> 
> y <- c(2,4,6,8,NA,NA,NA,NA,18)
> 
> barplot2(y,log='y')
> 
> Above fails.
> 
> 
> I appreciate that what I am trying to do is somewhat artificial (handle
> zero values on a log scale), but it does reflect the data I have.
> 
> I tried plot(..., type='h'), but that dosn't do the "beside=T" stuff that
> I want to do.
> 
> I am now trying things like...
> 
> barplot2(
>   dat.y.plot + 0.11, # Dirty hack
>   offset=-0.1,       #
>   xpd=F,             #
>   log='y',
>   beside=T
> )
> 
> Which looks messy. 
> 
> Any way to cleanly handle NA values with barplot2 on a log scale
> (log='y')?

<snip>

Dan,

You are actually close in the above example, using the 'offset'
argument.

In this case, you still cannot use "NA"s, since their value is unknown
and so must set these elements to zero. Then using a small offset value,
you can adjust the base value of the y axis so that it is "just above"
zero. This should result in a minimal shift of the bar values above
their actual values and should not materially affect the plot's
representation of the data.

Something like the following "should" work:

  > y <- c(2, 4, 6, 8, NA, NA, NA, NA, 18)
  > y
  [1]  2  4  6  8 NA NA NA NA 18
   
  > y[is.na(y)] <- 0
  > y
  [1]  2  4  6  8  0  0  0  0 18


  barplot2(y, log = "y", offset = 0.01, las = 2)

Note also that if you follow the above with:

  box()

The residual bars from the (0 + 0.01) values are covered with the plot
region box, if that is an issue for you.

This is still something of a "hack", but it is a little cleaner. The key
of course is to avoid the use of a bar value of log(x), where x <= 0.
Selecting the proper offset value based upon your actual data is
important so as to minimally affect the values visually.

HTH,

Marc Schwartz



From dmbates at gmail.com  Sat Jun  4 16:35:53 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sat, 4 Jun 2005 09:35:53 -0500
Subject: [R] can R do Fixed-effects (within) regression (panel data)?
In-Reply-To: <0IHK00763855R4@mail.fudan.edu.cn>
References: <0IHK00763855R4@mail.fudan.edu.cn>
Message-ID: <40e66e0b05060407352de9bca0@mail.gmail.com>

On 6/4/05, ronggui <0034058 at fudan.edu.cn> wrote:
> i want to ask 2 questions.
> 
> 1) can R do Random-effects GLS regression which i can get from Stata?
> the following result is frome Stata.can I get the alike result from R?
> 
> xtreg lwage educ black hisp exper expersq married union, re
> 
> Random-effects GLS regression                   Number of obs      =      4360
> Group variable (i) : nr                         Number of groups   =       545
> 
> R-sq:  within  = 0.1799                         Obs per group: min =         8
>        between = 0.1860                                        avg =       8.0
>        overall = 0.1830                                        max =         8
> 
> Random effects u_i ~ Gaussian                   Wald chi2(14)      =    957.77
> corr(u_i, X)       = 0 (assumed)                Prob > chi2        =    0.0000
> 
> ------------------------------------------------------------------------------
>        lwage |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
> -------------+----------------------------------------------------------------
>         educ |   .0918763   .0106597     8.62   0.000     .0709836    .1127689
> ......................
>          d86 |   .0919476   .0712293     1.29   0.197    -.0476592    .2315544
>          d87 |   .1349289   .0813135     1.66   0.097    -.0244427    .2943005
>        _cons |   .0235864   .1506683     0.16   0.876     -.271718    .3188907
> -------------+----------------------------------------------------------------
>      sigma_u |  .32460315
>      sigma_e |  .35099001
>          rho |  .46100216   (fraction of variance due to u_i)
> 
> 
> 2)
> 
> can R do Fixed-effects (within) regression as Stata's xtreg?
> 
> the followng example is from
> "Introductory Econometrics: A Modern Approach" by Jeffrey M. Wooldridge
> Chapter 14 - Advanced Panel Data Methods
> 
> use http://fmwww.bc.edu/ec-p/data/wooldridge/JTRAIN
> iis fcode
> tis year
> xtreg lscrap d88 d89 grant grant_1, fe
> 
> Fixed-effects (within) regression               Number of obs      =       162
> Group variable (i) : fcode                      Number of groups   =        54
> 
> R-sq:  within  = 0.2010                         Obs per group: min =         3
>        between = 0.0079                                        avg =       3.0
>        overall = 0.0068                                        max =         3
> 
>                                                 F(4,104)           =      6.54
> corr(u_i, Xb)  = -0.0714                        Prob > F           =    0.0001
> 
> ------------------------------------------------------------------------------
>       lscrap |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]
> -------------+----------------------------------------------------------------
>          d88 |  -.0802157   .1094751    -0.73   0.465    -.2973089    .1368776
>          d89 |  -.2472028   .1332183    -1.86   0.066    -.5113797     .016974
>        grant |  -.2523149    .150629    -1.68   0.097    -.5510178     .046388
>      grant_1 |  -.4215895      .2102    -2.01   0.047    -.8384239   -.0047551
>        _cons |    .597434   .0677344     8.82   0.000     .4631142    .7317539
> -------------+----------------------------------------------------------------
>      sigma_u |   1.438982
>      sigma_e |   .4977442
>          rho |  .89313867   (fraction of variance due to u_i)
> ------------------------------------------------------------------------------
> F test that all u_i=0:     F(53, 104) =    24.66             Prob > F = 0.0000

I'm not sure what the models being fit by Stata are but I imagine that
they correspond to models that can be fit in R by lmer (package lme4)
or lme (package nlme).



From dmb at mrc-dunn.cam.ac.uk  Sat Jun  4 16:53:57 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 4 Jun 2005 15:53:57 +0100 (BST)
Subject: [R] barplot and missing values?
In-Reply-To: <1117894793.4261.27.camel@horizons.localdomain>
Message-ID: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>

On Sat, 4 Jun 2005, Marc Schwartz wrote:

>On Sat, 2005-06-04 at 14:50 +0100, Dan Bolser wrote:
>
><snip>
>
>> This must be because of the "log='y'" option that I am using here.
>> 
>> y <- c(2,4,6,8,NA,NA,NA,NA,18)
>> 
>> barplot2(y,log='y')
>> 
>> Above fails.
>> 
>> 
>> I appreciate that what I am trying to do is somewhat artificial (handle
>> zero values on a log scale), but it does reflect the data I have.
>> 
>> I tried plot(..., type='h'), but that dosn't do the "beside=T" stuff that
>> I want to do.
>> 
>> I am now trying things like...
>> 
>> barplot2(
>>   dat.y.plot + 0.11, # Dirty hack
>>   offset=-0.1,       #
>>   xpd=F,             #
>>   log='y',
>>   beside=T
>> )
>> 
>> Which looks messy. 
>> 
>> Any way to cleanly handle NA values with barplot2 on a log scale
>> (log='y')?
>
><snip>
>
>Dan,
>
>You are actually close in the above example, using the 'offset'
>argument.
>
>In this case, you still cannot use "NA"s, since their value is unknown
>and so must set these elements to zero. Then using a small offset value,
>you can adjust the base value of the y axis so that it is "just above"
>zero. This should result in a minimal shift of the bar values above
>their actual values and should not materially affect the plot's
>representation of the data.
>
>Something like the following "should" work:
>
>  > y <- c(2, 4, 6, 8, NA, NA, NA, NA, 18)
>  > y
>  [1]  2  4  6  8 NA NA NA NA 18
>   
>  > y[is.na(y)] <- 0
>  > y
>  [1]  2  4  6  8  0  0  0  0 18
>
>
>  barplot2(y, log = "y", offset = 0.01, las = 2)
>
>Note also that if you follow the above with:
>
>  box()
>
>The residual bars from the (0 + 0.01) values are covered with the plot
>region box, if that is an issue for you.


Actually it looks a bit strange (I guess you didn't check it?) - I see
what is happening. It isn't much different from...

barplot2(y+0.01, log = "y",las = 1)

Which is the essence of the fix, but all that bar (on a log scale) between
1 and 0.1 and 0.01 is as big as 1 to 10, which is a bit artificial.


My previous fix looks best now I check it with the example ...

y
> y
[1]  2  4  6  8  0  0  0  0 18

barplot2(
  y + 0.11,
  ylim=c(1,max(y)),
  offset = -0.10,
  log='y',
  xpd=F
)
box()

Looks like the above is what I need :)

Thanks for teh help - its reasuring to see similar fixes :)




>
>This is still something of a "hack", but it is a little cleaner. The key
>of course is to avoid the use of a bar value of log(x), where x <= 0.
>Selecting the proper offset value based upon your actual data is
>important so as to minimally affect the values visually.
>
>HTH,
>
>Marc Schwartz
>
>



From dmb at mrc-dunn.cam.ac.uk  Sat Jun  4 16:54:24 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 4 Jun 2005 15:54:24 +0100 (BST)
Subject: [R] barplot and missing values?
In-Reply-To: <42A1B599.7030607@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.21.0506041554090.12482-100000@mail.mrc-dunn.cam.ac.uk>

On Sat, 4 Jun 2005, Uwe Ligges wrote:

>Dan Bolser wrote:
>
>[all previous stuff deleted]
>
>I see, what comes out of this longish thread is:
>
>  - barplot() and barplot2() both have deficiencies for you particular 
>examples, so it is time to provide patches for both barplot() and 
>barplot2() (for the latter, you might want to contact the package 
>maintainer as well) ...
>
>  - Please provide *reproducible* examples (yours was not, because 
>"log='y'" was missing). Hence the relevant example we were obviously 
>talking about is:
>
>   y <- c(2,4,6,8,NA,NA,NA,NA,18)
>   barplot(y, log="y")
>
>   library(gplots)
>   barplot2(y, log="y")
>


Sorry bout that. Thanks again!

Dan.

>
>Uwe Ligges
>
>



From jfox at mcmaster.ca  Sat Jun  4 16:58:48 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 4 Jun 2005 10:58:48 -0400
Subject: [R] the test result is quite different,why?
In-Reply-To: <0IHJ003K0H5EVW@mail.fudan.edu.cn>
Message-ID: <20050604145845.IXXF2981.tomts43-srv.bellnexxia.net@JohnDesktop8300>

Dear ronggui,

You're fitted different variance models: in bptest(), you're modeling the
variance using any linear combination of the predictors, while in ncv.test()
you're modeling the variance as a function of the fitted values, which is
more restrictive. BTW, both bptest() and ncv.test() can do both of these
tests.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ronggui
> Sent: Friday, June 03, 2005 9:59 PM
> To: r-help at stat.math.ethz.ch 
> Subject: [R] the test result is quite different,why?
> 
> data:http://fmwww.bc.edu/ec-p/data/wooldridge/CRIME4.dta
> 
> > a$call
> lm(formula = clcrmrte ~ factor(year) + clprbarr + clprbcon + 
>     clprbpri + clavgsen + clpolpc, data = cri)
> > bptest(a,st=F)
> 
>         Breusch-Pagan test
> 
> data:  a
> BP = 34.4936, df = 10, p-value = 0.0001523
> 
> > bptest(a,st=T)
> 
>         studentized Breusch-Pagan test
> 
> data:  a
> BP = 10.9297, df = 10, p-value = 0.363
> 
> > ncv.test(a)
> $formula
> ~fitted.values
> 
> $formula.name
> [1] "Variance"
> 
> $ChiSquare
> [1] 1.163501
> 
> $Df
> [1] 1
> 
> $p
> [1] 0.2807406
> 
> $test
> [1] "Non-constant Variance Score Test"
> 
> attr(,"class")
> [1] "chisq.test"
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From awitney at sgul.ac.uk  Sat Jun  4 18:42:43 2005
From: awitney at sgul.ac.uk (Adam Witney)
Date: Sat, 04 Jun 2005 17:42:43 +0100
Subject: [R] Storing data frame in a RDBMS
Message-ID: <BEC79893.470C2%awitney@sgul.ac.uk>


Hi,

Is there anyway to store a data frame in a database, and by this I mean the
binary itself, not the contents?

I am using PL/R in PostgreSQL amd have written some functions to build my
data frame. However this can take some time with some large datasets and I
would like to not have to repeat the process and so I would like to save the
data frame. Rather than save/load into the file system I would like to be
able to save the entire data frame as a single object in the database

Is this possible?

Thanks for any help

Adam


-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From maechler at stat.math.ethz.ch  Sat Jun  4 19:08:56 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 4 Jun 2005 19:08:56 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
Message-ID: <17057.57384.831362.960765@stat.math.ethz.ch>

>>>>> "Ross" == Ross Boylan <ross at biostat.ucsf.edu>
>>>>>     on Fri, 03 Jun 2005 17:04:08 -0700 writes:

    Ross> I defined an S4 class with a slot i.  Then I wrote a regular function
    Ross> that attempted to increment i.

    Ross> This didn't work, apparently because of the general rule that a function
    Ross> can't change the values of its arguments outside the function.  I gather
    Ross> there are ways around it, but the Green book admonishes "cheating on the
    Ross> S evaluation model is to be avoided" (p. 190).

    Ross> Thinking that class methods needed to an exception to this rule, I then
    Ross> tried setMethod with the function I had written.  However, when I called
    Ross> the function I got
    >> setMethod("nextPath", "CompletePathMaker", nextPath)
    Ross> Creating a new generic function for 'nextPath' in '.GlobalEnv'
    Ross> [1] "nextPath"
    >> nextPath(pm)
    Ross> Error: protect(): protection stack overflow

    Ross> I can change the value of the slot interactively, so the problem does
    Ross> not appear to be that the slots are considered off-limits.

    Ross> What do I need to do to update slot values?

    Ross> Here are some possibly relevant code fragments
    Ross> setClass("CompletePathMaker",
    Ross> representation(i="integer",
    Ross> timeOffset="numeric", # to avoid 0's
    Ross> truePaths="TruePaths")
    Ross> )

    Ross> nextPath <- function(pm){ #pm is a CompletePathMaker
    Ross>    pm at i <- pm at i+as.integer(1)
    Ross> [etc]

If your nextPath   function has  'pm' as its last statement it
will return the updated object, and if you call it
as
	mypm <- nextPath(mypm)

you are
    1) updating  mypm
    2) in a proper S way (i.e. no cheating).

Regards,
Martin

    Ross> I'm trying to make the class behave like an iterator, with i keeping
    Ross> track of its location.  I'm sure there are more R'ish ways to go, but
    Ross> I'm also pretty sure I'm going to want to be able to update slots.

    Ross> Thanks.
    Ross> Ross Boylan



From ggrothendieck at gmail.com  Sat Jun  4 19:09:46 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 4 Jun 2005 13:09:46 -0400
Subject: [R] Storing data frame in a RDBMS
In-Reply-To: <BEC79893.470C2%awitney@sgul.ac.uk>
References: <BEC79893.470C2%awitney@sgul.ac.uk>
Message-ID: <971536df05060410092a09d15e@mail.gmail.com>

On 6/4/05, Adam Witney <awitney at sgul.ac.uk> wrote:
> 
> Hi,
> 
> Is there anyway to store a data frame in a database, and by this I mean the
> binary itself, not the contents?
> 
> I am using PL/R in PostgreSQL amd have written some functions to build my
> data frame. However this can take some time with some large datasets and I
> would like to not have to repeat the process and so I would like to save the
> data frame. Rather than save/load into the file system I would like to be
> able to save the entire data frame as a single object in the database
> 
> Is this possible?
> 
> Thanks for any help
> 

Check out ?serialize



From abunn at whrc.org  Sat Jun  4 20:07:36 2005
From: abunn at whrc.org (Andy Bunn)
Date: Sat, 4 Jun 2005 14:07:36 -0400
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <Pine.LNX.4.58.0506040248250.4534@thorin.ci.tuwien.ac.at>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEEBBDHAA.abunn@whrc.org>

Adam:
> Providing a reproducible example would be a first step...

That's the problem, I can't. But I str has come to the rescue:

R > str(rw)
 Time-Series [1:307] from 1690 to 1996: 0.986 1.347 1.502 1.594 1.475 ...
R > str(pg)
List of 264
 $ : num 0.227
 $ : num 0.189
 $ : num 0.237
 $ : num 0.235

.
.
.
.
 - attr(*, "dim")= int [1:2] 22 12
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:12] "Series 1" "Series 2" "Series 3" "Series 4" ...
 - attr(*, "tsp")= num [1:3] 1982 2003    1
 - attr(*, "class")= chr [1:2] "mts" "ts"

Why is pg a list? pg was created by taking a row out of a much larger
data.frame:

R > pg <- ts(matrix(monthly.pg[1,-c(1:4)], ncol = 12, byrow = T), start =
1982)
R > class(pg)
[1] "mts" "ts"
R > mode(pg)
[1] "list"

So, changing the mode to numeric, allowed me to intersect the ts:

R > pg <- ts(matrix(as.numeric(monthly.pg[1,-c(1:4)]), ncol = 12, byrow =
T), start = 1982)
R > tsp(ts.intersect(rw, pg))
[1] 1982 1996    1

Weird. I suppose keeping on top of every object's mode is important.

Thanks for the push forward.

-Andy



From MSchwartz at mn.rr.com  Sat Jun  4 20:49:42 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 04 Jun 2005 13:49:42 -0500
Subject: [R] barplot and missing values?
In-Reply-To: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1117910982.4261.61.camel@horizons.localdomain>

On Sat, 2005-06-04 at 15:53 +0100, Dan Bolser wrote:
> On Sat, 4 Jun 2005, Marc Schwartz wrote:
> 
> >On Sat, 2005-06-04 at 14:50 +0100, Dan Bolser wrote:
> >
> ><snip>
> >
> >> This must be because of the "log='y'" option that I am using here.
> >> 
> >> y <- c(2,4,6,8,NA,NA,NA,NA,18)
> >> 
> >> barplot2(y,log='y')
> >> 
> >> Above fails.
> >> 
> >> 
> >> I appreciate that what I am trying to do is somewhat artificial (handle
> >> zero values on a log scale), but it does reflect the data I have.
> >> 
> >> I tried plot(..., type='h'), but that dosn't do the "beside=T" stuff that
> >> I want to do.
> >> 
> >> I am now trying things like...
> >> 
> >> barplot2(
> >>   dat.y.plot + 0.11, # Dirty hack
> >>   offset=-0.1,       #
> >>   xpd=F,             #
> >>   log='y',
> >>   beside=T
> >> )
> >> 
> >> Which looks messy. 
> >> 
> >> Any way to cleanly handle NA values with barplot2 on a log scale
> >> (log='y')?
> >
> ><snip>
> >
> >Dan,
> >
> >You are actually close in the above example, using the 'offset'
> >argument.
> >
> >In this case, you still cannot use "NA"s, since their value is unknown
> >and so must set these elements to zero. Then using a small offset value,
> >you can adjust the base value of the y axis so that it is "just above"
> >zero. This should result in a minimal shift of the bar values above
> >their actual values and should not materially affect the plot's
> >representation of the data.
> >
> >Something like the following "should" work:
> >
> >  > y <- c(2, 4, 6, 8, NA, NA, NA, NA, 18)
> >  > y
> >  [1]  2  4  6  8 NA NA NA NA 18
> >   
> >  > y[is.na(y)] <- 0
> >  > y
> >  [1]  2  4  6  8  0  0  0  0 18
> >
> >
> >  barplot2(y, log = "y", offset = 0.01, las = 2)
> >
> >Note also that if you follow the above with:
> >
> >  box()
> >
> >The residual bars from the (0 + 0.01) values are covered with the plot
> >region box, if that is an issue for you.
> 
> 
> Actually it looks a bit strange (I guess you didn't check it?) 

Yes I did...

> - I see
> what is happening. It isn't much different from...

> barplot2(y+0.01, log = "y",las = 1)
> 
> Which is the essence of the fix, but all that bar (on a log scale) between
> 1 and 0.1 and 0.01 is as big as 1 to 10, which is a bit artificial.

That's the way of course it should be by default. The space between
0.01:0.1, 0.1:1 and 1:10 should be the same.

The issue is that you want to be able to modify the default y axis
range, given the presence of the offset value as min(y) instead of it
being 0. This results in the "distracting" (not so much artificial)
space from 0.01 to 1, given the way in which the default axis ranges are
created.

> My previous fix looks best now I check it with the example ...
> 
> y
> > y
> [1]  2  4  6  8  0  0  0  0 18
> 
> barplot2(
>   y + 0.11,
>   ylim=c(1,max(y)),
>   offset = -0.10,
>   log='y',
>   xpd=F
> )
> box()
> 
> Looks like the above is what I need :)
> 
> Thanks for teh help - its reasuring to see similar fixes :)

This works and is still a bit kludgy, since it is not "automatic".

I think that the "best" option would be for me to spend some time
improving the default behavior of barplot2() with <=0 and/or NA values
in the presence of a log axis (x or y) so that it is similar to the way
in which plot() handles it:

> y
[1]  2  4  6  8  0  0  0  0 18

# NOTE THE WARNING MESSAGE HERE

> plot(y, log = "y")
Warning message:
4 y values <= 0 omitted from logarithmic plot in: xy.coords(x, y,
xlabel, ylabel, log)

> y[y == 0] <- NA
> y
[1]  2  4  6  8 NA NA NA NA 18

# NO WARNING MESSAGE HERE, BUT DOES NOT PLOT THE "NA"s

> plot(y, log = "y")


I'll take a look at that.

Marc



From ggrothendieck at myway.com  Sat Jun  4 21:27:58 2005
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 4 Jun 2005 19:27:58 +0000 (UTC)
Subject: [R] ts.intersect a multivariate and univariate ts
References: <Pine.LNX.4.58.0506040248250.4534@thorin.ci.tuwien.ac.at>
	<NEBBIPHDAMMOKDKPOFFIEEBBDHAA.abunn@whrc.org>
Message-ID: <loom.20050604T211147-788@post.gmane.org>

Andy Bunn <abunn <at> whrc.org> writes:

: 
: Adam:
: > Providing a reproducible example would be a first step...
: 
: That's the problem, I can't. But I str has come to the rescue:

We can provide it in a reproducible way like this:

dput(rw)
dput(pg)

That will output both in a format that anyone else can paste
back into R from your email.

: 
: R > str(rw)
:  Time-Series [1:307] from 1690 to 1996: 0.986 1.347 1.502 1.594 1.475 ...
: R > str(pg)
: List of 264
:  $ : num 0.227
:  $ : num 0.189
:  $ : num 0.237
:  $ : num 0.235
: 
: .
: .
: .
: .
:  - attr(*, "dim")= int [1:2] 22 12
:  - attr(*, "dimnames")=List of 2
:   ..$ : NULL
:   ..$ : chr [1:12] "Series 1" "Series 2" "Series 3" "Series 4" ...
:  - attr(*, "tsp")= num [1:3] 1982 2003    1
:  - attr(*, "class")= chr [1:2] "mts" "ts"
: 
: Why is pg a list? pg was created by taking a row out of a much larger
: data.frame:

A data frame is a list with one component per column so taking
a matrix of a data frame gives something like this:

R> matrix(iris)
     [,1]       
[1,] Numeric,150
[2,] Numeric,150
[3,] Numeric,150
[4,] Numeric,150
[5,] factor,150 

which is a matrix each of whose elements is a column of the original
data frame.  What you really want here is 'as.matrix' or 'data.matrix', 
not 'matrix'.



From laet_99 at yahoo.fr  Sat Jun  4 22:16:01 2005
From: laet_99 at yahoo.fr (Laetitia Mestdagh)
Date: Sat, 4 Jun 2005 22:16:01 +0200 (CEST)
Subject: [R] glm with a distribution free family
Message-ID: <20050604201601.54228.qmail@web26803.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050604/b61b33f0/attachment.pl

From mail at joeconway.com  Sat Jun  4 22:18:06 2005
From: mail at joeconway.com (Joe Conway)
Date: Sat, 04 Jun 2005 13:18:06 -0700
Subject: [R] Storing data frame in a RDBMS
In-Reply-To: <971536df05060410092a09d15e@mail.gmail.com>
References: <BEC79893.470C2%awitney@sgul.ac.uk>
	<971536df05060410092a09d15e@mail.gmail.com>
Message-ID: <42A20C7E.1040009@joeconway.com>

Gabor Grothendieck wrote:
> On 6/4/05, Adam Witney <awitney at sgul.ac.uk> wrote:
>>I am using PL/R in PostgreSQL amd have written some functions to build my
>>data frame. However this can take some time with some large datasets and I
>>would like to not have to repeat the process and so I would like to save the
>>data frame. Rather than save/load into the file system I would like to be
>>able to save the entire data frame as a single object in the database
>>
>>Is this possible?
> 
> Check out ?serialize

Looks like serialize should work nicely:

create or replace function test_serialize(text)
  returns text as '
   mydf <- pg.spi.exec(arg1)
   return (serialize(mydf, NULL, ascii = TRUE))
' language 'plr';

create table saved_df (id int, df text);

insert into saved_df
  select 1, f from test_serialize('select oid, typname from pg_type
                                   where typname = ''oid''
                                   or typname = ''text''') as t(f);

create or replace function restore_df(text)
  returns setof record as '
   unserialize(arg1)
' language 'plr';

select * from restore_df((select df from saved_df where id =1))
  as t(oid oid, typname name);
  oid | typname
-----+---------
   25 | text
   26 | oid
(2 rows)

HTH,

Joe



From rxg218 at psu.edu  Sat Jun  4 23:08:58 2005
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Sat, 04 Jun 2005 17:08:58 -0400
Subject: [R] Storing data frame in a RDBMS
In-Reply-To: <42A20C7E.1040009@joeconway.com>
References: <BEC79893.470C2%awitney@sgul.ac.uk>
	<971536df05060410092a09d15e@mail.gmail.com>
	<42A20C7E.1040009@joeconway.com>
Message-ID: <1117919338.3739.16.camel@localhost.localdomain>

On Sat, 2005-06-04 at 13:18 -0700, Joe Conway wrote:
> Gabor Grothendieck wrote:
> > On 6/4/05, Adam Witney <awitney at sgul.ac.uk> wrote:
> >>I am using PL/R in PostgreSQL amd have written some functions to
> build my
> >>data frame. However this can take some time with some large datasets
> and I
> >>would like to not have to repeat the process and so I would like to
> save the
> >>data frame. Rather than save/load into the file system I would like
> to be
> >>able to save the entire data frame as a single object in the
> database
> >>
> >>Is this possible?
> > 
> > Check out ?serialize
> 
> Looks like serialize should work nicely:
> 
> create or replace function test_serialize(text)
>   returns text as '
>    mydf <- pg.spi.exec(arg1)
>    return (serialize(mydf, NULL, ascii = TRUE))
> ' language 'plr';

I was trying to do something similar but I needed to this from R itself.
That is, I'd like to save a data.frame or a lm object to a DB.

I looked at serialize but as it just takes a connection object I'm not
sure as to how I would specify say a table in a given DB.

I'm using R 2.0.1, PostgreSQL and Rdbi and Rdbi.PostgreSQL.

I've looked at the docs and I see that there are helper functions to
write a table containing text, numeric or boolean. But its not clear to
me how I would write an arbitrary object to a DB.

Any pointers would be appreciated.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Eureka!
-- Archimedes



From ggrothendieck at gmail.com  Sun Jun  5 00:18:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 4 Jun 2005 18:18:23 -0400
Subject: [R] Storing data frame in a RDBMS
In-Reply-To: <1117919338.3739.16.camel@localhost.localdomain>
References: <BEC79893.470C2%awitney@sgul.ac.uk>
	<971536df05060410092a09d15e@mail.gmail.com>
	<42A20C7E.1040009@joeconway.com>
	<1117919338.3739.16.camel@localhost.localdomain>
Message-ID: <971536df050604151870eed737@mail.gmail.com>

On 6/4/05, Rajarshi Guha <rxg218 at psu.edu> wrote:
> On Sat, 2005-06-04 at 13:18 -0700, Joe Conway wrote:
> > Gabor Grothendieck wrote:
> > > On 6/4/05, Adam Witney <awitney at sgul.ac.uk> wrote:
> > >>I am using PL/R in PostgreSQL amd have written some functions to
> > build my
> > >>data frame. However this can take some time with some large datasets
> > and I
> > >>would like to not have to repeat the process and so I would like to
> > save the
> > >>data frame. Rather than save/load into the file system I would like
> > to be
> > >>able to save the entire data frame as a single object in the
> > database
> > >>
> > >>Is this possible?
> > >
> > > Check out ?serialize
> >
> > Looks like serialize should work nicely:
> >
> > create or replace function test_serialize(text)
> >   returns text as '
> >    mydf <- pg.spi.exec(arg1)
> >    return (serialize(mydf, NULL, ascii = TRUE))
> > ' language 'plr';
> 
> I was trying to do something similar but I needed to this from R itself.
> That is, I'd like to save a data.frame or a lm object to a DB.
> 
> I looked at serialize but as it just takes a connection object I'm not
> sure as to how I would specify say a table in a given DB.
> 
> I'm using R 2.0.1, PostgreSQL and Rdbi and Rdbi.PostgreSQL.
> 
> I've looked at the docs and I see that there are helper functions to
> write a table containing text, numeric or boolean. But its not clear to
> me how I would write an arbitrary object to a DB.

Look at 

R> example(serialize)



From spencer.graves at pdf.com  Sun Jun  5 00:25:51 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 04 Jun 2005 15:25:51 -0700
Subject: [R] glm with a distribution free family
In-Reply-To: <20050604201601.54228.qmail@web26803.mail.ukl.yahoo.com>
References: <20050604201601.54228.qmail@web26803.mail.ukl.yahoo.com>
Message-ID: <42A22A6F.8030300@pdf.com>

	  Have you considered computing and plotting sample averages and 
variances for different subgroups of the data then plotting variances 
vs. averages?  This was suggested by Fahrmeir and Tutz (2001) 
Multivariate Statistical Modeling Based on Generalized Linear Models, 
2nd ed. (Springer, Example 2.7, p. 59-60).  This analysis led them to 
consider variance(mu) = phi*mu, phi*mu^2 and mu+theta*mu^2 for a 
particular example.  The variance function mu+theta*mu^2 corresponds to 
the negative binomial distribution, as noted by McCullagh and Nelder 
(1989) Generalized Linear Models, 2nd 3d. (Chapman & Hall, Table 9.1, p. 
326).  A family function "negative.binomial" is included in 
library(MASS) and discussed in Venables and Ripley (2002) Modern Applied 
Statistics with S, 4th ed. (Springer, sec. 7.4, pp. 206-208).  R script 
to perform this and other analyses in the first 60 pages of Fahrmeir and 
Tutz (2001) is attached, utilizing library(Fahrmeir) supported by Kjetil 
Halvorsen, who helped me with the attached.  The r-help server will 
probably remove the attachment, but it should reach you, Leatitia, and 
Kjetil.  Kjetil suggested he might include something like this is an 
update to library(Fahrmeir), perhaps in a "demo" subdirectory. 
Therefore, others interested in this might check there first and ping me 
and Kjetil if they don't find it on CRAN.

	  I wish to thank Kjetil for his help with similar questions earlier -- 
and for maintaining the "Farhmeir" package on CRAN.  I could not have 
answered this question 24 hours ago.

	  Best Wishes,
	  spencer graves	

Laetitia Mestdagh wrote:

> Dear R users,
>  
> I am trying to fit a glm with a distribution free family, link = log 
and variance = constant*mu. I guess I have to use the quasi family but
the choices of variance are restricted to constant or mu or mu^2..., I
don't know the way to choose the variance that I need, i.e. constant*mu.
> If you have any ideas or advice, please tell me.
> Thanks,
>  
> Laetitia Mestdagh
> 
> 
> Laetitia Mestdagh
> ??l??ve en 3??me ann??e a l'ISUP( Universit?? de Paris VI)
> 13 rue Ernest Chamblain
> 91250 St Germain les Corbeil
> Tel : 01 69 89 28 15
> Portable : 06 18 44 53 26
> 		
> ---------------------------------
> 
> ils, photos et vid??os !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From 0034058 at fudan.edu.cn  Sun Jun  5 03:50:56 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 05 Jun 2005 09:50:56 +0800
Subject: [R] data transformation
Message-ID: <0IHL007M18NHR4@mail.fudan.edu.cn>

i have data fame da:
> da
    x y
1   1 a
2   2 a
3   3 a
4   4 a
5   5 a
6   6 b
7   7 b
8   8 b
9   9 b
10 10 b
> str(da)
`data.frame':   10 obs. of  2 variables:
 $ x: num  1 2 3 4 5 6 7 8 9 10
 $ y: Factor w/ 2 levels "a","b": 1 1 1 1 1 2 2 2 2 2

and i want to generate new variable da$z,when y=="a",da$z=da$x-mean(x[y=="a"])  ,when   y=="b",da$z=da$x-mean(x[y=="b"]).

this data frame is simple and i can do it by hand,if the y has many levels and i have x1,x2....
can i do it quickly?



From andy_liaw at merck.com  Sun Jun  5 04:20:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 4 Jun 2005 22:20:51 -0400
Subject: [R] data transformation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E928@usctmx1106.merck.com>

Try:

> da$z <- da$x - ave(da$x, da$y)
> da
    x y  z
1   1 a -2
2   2 a -1
3   3 a  0
4   4 a  1
5   5 a  2
6   6 b -2
7   7 b -1
8   8 b  0
9   9 b  1
10 10 b  2

Andy

> From: ronggui
> 
> i have data fame da:
> > da
>     x y
> 1   1 a
> 2   2 a
> 3   3 a
> 4   4 a
> 5   5 a
> 6   6 b
> 7   7 b
> 8   8 b
> 9   9 b
> 10 10 b
> > str(da)
> `data.frame':   10 obs. of  2 variables:
>  $ x: num  1 2 3 4 5 6 7 8 9 10
>  $ y: Factor w/ 2 levels "a","b": 1 1 1 1 1 2 2 2 2 2
> 
> and i want to generate new variable da$z,when 
> y=="a",da$z=da$x-mean(x[y=="a"])  ,when   
> y=="b",da$z=da$x-mean(x[y=="b"]).
> 
> this data frame is simple and i can do it by hand,if the y 
> has many levels and i have x1,x2....
> can i do it quickly?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Sun Jun  5 06:33:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 00:33:30 -0400
Subject: [R] data transformation
In-Reply-To: <0IHL007M18NHR4@mail.fudan.edu.cn>
References: <0IHL007M18NHR4@mail.fudan.edu.cn>
Message-ID: <971536df050604213375f019ce@mail.gmail.com>

On 6/4/05, ronggui <0034058 at fudan.edu.cn> wrote:
> i have data fame da:
> > da
>    x y
> 1   1 a
> 2   2 a
> 3   3 a
> 4   4 a
> 5   5 a
> 6   6 b
> 7   7 b
> 8   8 b
> 9   9 b
> 10 10 b
> > str(da)
> `data.frame':   10 obs. of  2 variables:
>  $ x: num  1 2 3 4 5 6 7 8 9 10
>  $ y: Factor w/ 2 levels "a","b": 1 1 1 1 1 2 2 2 2 2
> 
> and i want to generate new variable da$z,when y=="a",da$z=da$x-mean(x[y=="a"])  ,when   y=="b",da$z=da$x-mean(x[y=="b"]).
> 
> this data frame is simple and i can do it by hand,if the y has many levels and i have x1,x2....
> can i do it quickly?
> 

Try this:

da$z <- resid(lm( x ~ y, da ))



From ripley at stats.ox.ac.uk  Sun Jun  5 09:48:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 5 Jun 2005 08:48:07 +0100 (BST)
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIEEBBDHAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIEEBBDHAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.61.0506050841530.26819@gannet.stats>

some_df[1, ] is actually a data frame: see ?"[.data.frame".  It's hard to 
see what else it could be, as columns of a data frame are of arbitrary 
classes.

When you apply matrix() to a data frame it gets dropped to a list.


On Sat, 4 Jun 2005, Andy Bunn wrote:

> Adam:
>> Providing a reproducible example would be a first step...
>
> That's the problem, I can't. But I str has come to the rescue:
>
> R > str(rw)
> Time-Series [1:307] from 1690 to 1996: 0.986 1.347 1.502 1.594 1.475 ...
> R > str(pg)
> List of 264
> $ : num 0.227
> $ : num 0.189
> $ : num 0.237
> $ : num 0.235
>
> .
> .
> .
> .
> - attr(*, "dim")= int [1:2] 22 12
> - attr(*, "dimnames")=List of 2
>  ..$ : NULL
>  ..$ : chr [1:12] "Series 1" "Series 2" "Series 3" "Series 4" ...
> - attr(*, "tsp")= num [1:3] 1982 2003    1
> - attr(*, "class")= chr [1:2] "mts" "ts"
>
> Why is pg a list? pg was created by taking a row out of a much larger
> data.frame:
>
> R > pg <- ts(matrix(monthly.pg[1,-c(1:4)], ncol = 12, byrow = T), start =
> 1982)
> R > class(pg)
> [1] "mts" "ts"
> R > mode(pg)
> [1] "list"
>
> So, changing the mode to numeric, allowed me to intersect the ts:
>
> R > pg <- ts(matrix(as.numeric(monthly.pg[1,-c(1:4)]), ncol = 12, byrow =
> T), start = 1982)
> R > tsp(ts.intersect(rw, pg))
> [1] 1982 1996    1
>
> Weird. I suppose keeping on top of every object's mode is important.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Jun  5 09:58:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 5 Jun 2005 08:58:24 +0100 (BST)
Subject: [R] glm with a distribution free family
In-Reply-To: <20050604201601.54228.qmail@web26803.mail.ukl.yahoo.com>
References: <20050604201601.54228.qmail@web26803.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506050849320.26819@gannet.stats>

On Sat, 4 Jun 2005, Laetitia Mestdagh wrote:

> I am trying to fit a glm with a distribution free family, link = log and 
> variance = constant*mu. I guess I have to use the quasi family but the 
> choices of variance are restricted to constant or mu or mu^2..., I don't 
> know the way to choose the variance that I need, i.e. constant*mu.

The constant in the variance specification is the dispersion parameter in 
a glm, so quasi() does cover this.  Setting variance = "constant" actually 
gives you 1 (look at the code).

If you know the constant you will need to specify it when calling e.g. 
summary() or predict(): otherwise a moment estimator will be used.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bitwrit at ozemail.com.au  Mon Jun  6 00:18:35 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sun, 05 Jun 2005 22:18:35 +0000
Subject: [R] A long digression on packages
Message-ID: <42A37A3B.6030901@ozemail.com.au>

Hello again,

First, thanks for the help that got the latest plotrix package finished. 
I had been planning to write something about packages since Scott 
Waichler offered the gantt.chart function. Then Ben Bolker (who helped 
me to write the axis.break function) asked if I would be willing to 
include some of his plotting functions and almost immediately after that 
Sander Oom kindly donated the soil texture plotting function in the same 
way. I could procrastinate no longer.

There are now about 500 packages on CRAN. Some are focused, covering a 
particular area well, easy for the prospective user to discover their 
potential usefulness, while others are less so. I consider the plotrix 
package one of the former, and so as not to upset too many people, I 
will use the other package I contributed to CRAN as an example of the 
latter.

When I initially wrote concord, it was intended as a package of 
functions dealing with concordance and reliability. Okay, but I found 
Kendall's W so useful that I couldn't help including it, and somehow 
Page's test of ordered alternatives crept in and invited the Jonckheere 
test to the party and at that point I realized that I had maybe forty or 
fifty more or less useful functions floating around my R directory. Now 
many of these are probably floating around other people's R directories 
as well. Consider Cohen's kappa. The tabular method is included in 
e1071, my version has Cohen's plus two additional methods, and the 
recently contributed psy package has yet another version. Maybe there 
are still more encrypted in packages that I haven't even looked at.

The point of all this is that it would make many user's lives easier if 
there were less pandemonium in packages. The mistakes I have made in 
concord I have tried not to repeat in plotrix. Unless a user search of 
the documentation in packages materializes, it's become mighty hard to 
work out if the function you don't want to write has already been 
written. We also spend a lot of time responding to or deriding 
correspondents who ask about such things.

Would it be an idea to have informal R periphery teams, or even 
individual package lords, who would bear with, or maybe welcome, other 
people's functions? That is, I think plotrix has been greatly enhanced 
by recent contributions. Conversely, I wonder if it would be possible to 
shrink or maybe even evaporate concord by discovering duplicate methods 
in other packages or by contributing concord functions or parts thereof 
myself. It's not that I don't like maintaining concord or think the 
functions are worthless, just that I am mildly embarrassed to be adding 
to the duplication of effort and unnecessary volume of packages.

Feel free to comment upon this, although if you really want to rave, try 
it out on me first before clagging the list. Thanks for your attention.

Jim



From ggrothendieck at gmail.com  Sun Jun  5 15:15:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 09:15:49 -0400
Subject: [R] A long digression on packages
In-Reply-To: <42A37A3B.6030901@ozemail.com.au>
References: <42A37A3B.6030901@ozemail.com.au>
Message-ID: <971536df05060506155bd816d5@mail.gmail.com>

On 6/5/05, Jim Lemon <bitwrit at ozemail.com.au> wrote:
> There are now about 500 packages on CRAN. Some are focused, covering a
> particular area well, easy for the prospective user to discover their
> potential usefulness, while others are less so. 

CRAN Task Views 

   http://cran.r-project.org/src/contrib/Views/

is one way of addressing CRAN organization. 

> I consider the plotrix package 

With respect to graphics, the R command:

   demo(graphics)

provides a demonstration of what can be done in R (maybe plotrix
should have a demo too?) and the site:

   http://addictedtor.free.fr/graphiques/

provides examples of what can be done in R using various packages
and accepts content contributions.

Also the R RSiteSearch function and help.search function are useful for 
discovery although the latter will only find functions in installed libraries.



From murdoch at stats.uwo.ca  Sun Jun  5 15:47:38 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 05 Jun 2005 09:47:38 -0400
Subject: [R] A long digression on packages
In-Reply-To: <42A37A3B.6030901@ozemail.com.au>
References: <42A37A3B.6030901@ozemail.com.au>
Message-ID: <42A3027A.9000007@stats.uwo.ca>

Thanks for posting this; I think you raise good points, but they're more 
appropriate for R-devel, so I've posted my reply there.

Duncan Murdoch



From descall at firenet.uk.net  Sun Jun  5 16:44:42 2005
From: descall at firenet.uk.net (Des Callaghan)
Date: Sun, 5 Jun 2005 15:44:42 +0100
Subject: [R] Creating a large matrix
Message-ID: <NPENJNGMGGPLGDEBIEJMAEFPCHAA.descall@firenet.uk.net>

Dear all,

I have a set of data that is 2 columns wide and 35,000 rows long (see
extract below).  The first column contains codes for tetrads (2km x 2km
squares within NW England) and the second column has the names of moss
species present within each tetrad.  I wish to convert this into a matrix
where rows are labelled by 'Tetrad', columns are labelled by 'Moss species'
and each cell contains a 1 or 0 to denote the presence or absence of the
moss species within each tetrad.  This would be a matrix measuring 796 rows
by 417 columns.  Am I able to do this within R?  I ran into problems trying
to do it using the crosstab query in MS Access and the pivot table function
on MS Excel because there are too many columns for these software to handle.
Thanks very much in advance for any help.

All the best,
Des

Tetrad	Moss species
SD20S	Amblystegium serpens var. serpens
SD20S	Barbula convoluta
SD21S	Brachythecium albicans
SD20J	Brachythecium rutabulum
SD30S	Bryum argenteum
SD20S	Bryum capillare



From vst at vsthost.com  Sun Jun  5 17:15:48 2005
From: vst at vsthost.com (Vehbi Sinan Tunalioglu)
Date: Sun, 05 Jun 2005 18:15:48 +0300
Subject: [R] Creating a large matrix
In-Reply-To: <NPENJNGMGGPLGDEBIEJMAEFPCHAA.descall@firenet.uk.net>
References: <NPENJNGMGGPLGDEBIEJMAEFPCHAA.descall@firenet.uk.net>
Message-ID: <42A31724.4090000@vsthost.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

You are able to store such a huge matrix in R, if you ask for it...
Furthermore, "table" command might help you:

Help page for "table" says:

Cross Tabulation and Table Creation
Description:
     'table' uses the cross-classifying factors to build a contingency
     table of the counts at each combination of factor levels.

Take a look at the examples on the Help page.

- --vst


Des Callaghan wrote:
> Dear all,
> 
> I have a set of data that is 2 columns wide and 35,000 rows long (see
> extract below).  The first column contains codes for tetrads (2km x 2km
> squares within NW England) and the second column has the names of moss
> species present within each tetrad.  I wish to convert this into a matrix
> where rows are labelled by 'Tetrad', columns are labelled by 'Moss species'
> and each cell contains a 1 or 0 to denote the presence or absence of the
> moss species within each tetrad.  This would be a matrix measuring 796 rows
> by 417 columns.  Am I able to do this within R?  I ran into problems trying
> to do it using the crosstab query in MS Access and the pivot table function
> on MS Excel because there are too many columns for these software to handle.
> Thanks very much in advance for any help.
> 
> All the best,
> Des
> 
> Tetrad	Moss species
> SD20S	Amblystegium serpens var. serpens
> SD20S	Barbula convoluta
> SD21S	Brachythecium albicans
> SD20J	Brachythecium rutabulum
> SD30S	Bryum argenteum
> SD20S	Bryum capillare
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.1 (GNU/Linux)
Comment: Using GnuPG with Thunderbird - http://enigmail.mozdev.org

iD8DBQFCoxcibCSiwEAcSbwRAkuqAKCjtJT9mZVsi/4oVesqi40L1ZaEFwCgpiWj
v/HYFWLNg/3+kpv6OW8yfTw=
=J+Vs
-----END PGP SIGNATURE-----



From jari.oksanen at oulu.fi  Sun Jun  5 17:44:19 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sun, 05 Jun 2005 18:44:19 +0300
Subject: [R] A long digression on packages
In-Reply-To: <42A3027A.9000007@stats.uwo.ca>
References: <42A37A3B.6030901@ozemail.com.au> <42A3027A.9000007@stats.uwo.ca>
Message-ID: <185e9e0739088327043d1f12e0b2e1c0@oulu.fi>


On 5 Jun 2005, at 16:47, Duncan Murdoch wrote:

> Thanks for posting this; I think you raise good points, but they're 
> more appropriate for R-devel, so I've posted my reply there.
>
There are diverse opinions about netiquette. One of the most basic, in 
my opinion, is this: if someone posts starts a discussion in a certain 
forum, you shall not divert it to another forum where it may be hidden 
by most readers, perhaps even by the originator of the thread.

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From abunn at whrc.org  Sun Jun  5 19:16:14 2005
From: abunn at whrc.org (Andy Bunn)
Date: Sun, 5 Jun 2005 13:16:14 -0400
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <Pine.LNX.4.61.0506050841530.26819@gannet.stats>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEBEDHAA.abunn@whrc.org>

> some_df[1, ] is actually a data frame: see ?"[.data.frame".  It's hard to
> see what else it could be, as columns of a data frame are of arbitrary
> classes.

I see, I was confusing class and mode. However, since a list can be a ts
object as in this example:

R > w <- list(rnorm(10), rnorm(10))
R > x <- ts(w, start = 1980)
R > y <- ts(rnorm(10), start = 1981)
R > tsp(x); tsp(y)
[1] 1980 1981    1
[1] 1981 1990    1
R > class(x); class(y)
[1] "ts"
[1] "ts"
R > mode(x); mode(y)
[1] "list"
[1] "numeric"
R > z <- ts.intersect(x,y)
Error: incorrect number of subscripts on matrix
R >

What would be the easiest way to make x a mts that could be used with plot
or ts.intersect?

R > z <- ts.intersect(x[[1]], x[[2]],y)
R > z
Time Series:
Start = 1981
End = 1990
Frequency = 1
          x[[1]]     x[[2]]           y
1981 -0.01809700  1.0725645 -2.05558318
1982  3.68646780 -1.7109873  0.99807704
1983 -2.24591782  1.0401845  0.66387636
1984 -1.09823787 -0.5579356  0.09377471
1985  0.25234527  2.9093321 -2.29570622
1986 -0.02486054  0.2631546 -1.16989266
1987 -0.11290772  0.6176711  1.10653249
1988 -1.23308983  0.1578497  0.65200013
1989  0.33531558 -0.8943133  0.84397066
1990  0.35412243 -1.0974815  0.41322884
R >

-Andy



From ggrothendieck at gmail.com  Sun Jun  5 19:57:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 13:57:13 -0400
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEBEDHAA.abunn@whrc.org>
References: <Pine.LNX.4.61.0506050841530.26819@gannet.stats>
	<NEBBIPHDAMMOKDKPOFFIKEBEDHAA.abunn@whrc.org>
Message-ID: <971536df050605105776fb1b9e@mail.gmail.com>

On 6/5/05, Andy Bunn <abunn at whrc.org> wrote:
> > some_df[1, ] is actually a data frame: see ?"[.data.frame".  It's hard to
> > see what else it could be, as columns of a data frame are of arbitrary
> > classes.
> 
> I see, I was confusing class and mode. However, since a list can be a ts
> object as in this example:
> 
> R > w <- list(rnorm(10), rnorm(10))
> R > x <- ts(w, start = 1980)

Even though you don't get an error message this statement is 
erroneous.  ?ts discusses the valid possibilities.

> R > y <- ts(rnorm(10), start = 1981)
> R > tsp(x); tsp(y)
> [1] 1980 1981    1
> [1] 1981 1990    1
> R > class(x); class(y)
> [1] "ts"
> [1] "ts"
> R > mode(x); mode(y)
> [1] "list"
> [1] "numeric"
> R > z <- ts.intersect(x,y)
> Error: incorrect number of subscripts on matrix
> R >
> 
> What would be the easiest way to make x a mts that could be used with plot
> or ts.intersect?

The correct way to do this is to create a valid ts object from the
start such as any of the following:

  ts(do.call(cbind, w), start = 1980)
  ts(cbind(w[[1]], w[[2]]), start = 1980)
  ts(as.data.frame(w), start = 1980)
  ts(data.frame(w[[1]], w[[2]]), start = 1980)

(The resulting column names may differ depending on
which you use.)

If you already have an invalid object of the sort described by x in
your post then the following would do:

ts(do.call(cbind, x), start = start(x), frequency = frequency(x))

although in your case frequency(x) is the default, 1, so it could
be dropped.



From spluque at gmail.com  Sun Jun  5 20:49:18 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Sun, 05 Jun 2005 13:49:18 -0500
Subject: [R] locator() via tcltk
In-Reply-To: <Pine.LNX.4.61.0506040930530.32138@gannet.stats>
References: <87slzy6g9w.fsf@gmail.com>
	<Pine.LNX.4.61.0506040930530.32138@gannet.stats>
Message-ID: <87r7fgvdu9.fsf@gmail.com>

Dear List members,

Thank you so much for your insights.


On Sat, 4 Jun 2005 09:39:33 +0100 (BST),
Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

[...]

> I think it is more likely you want to wait for the Tk interaction and
> then return the results, that is use a `modal' widget. If so, take a
> look at the examples in src/library/tcltk/R/utils.R which are modal and
> return their results.


Yes, this is indeed what I was looking for. Additionally, I needed to be
able to make several calls to getcoords and store the results from each
call, i.e.: be able to repeat the sequence 1. click "Get coordinates", 2.
do some clicks, finish by clicking any button other than left, as many
times as needed, and return the results from each sequence separately. I
looked at the code you pointed me to and came up with this:

require(tcltk)

testplot <- function() {
  coords <- list()
  getcoords <- function(...) {
  coords[[length(coords) + 1]] <<- locator()
  tkgrab.release(base)
  }
  x <- 1:1000
  y <- rnorm(1000)
  plot(x, y)
  base <- tktoplevel()
  tkwm.deiconify(base)
  tkgrab.set(base)
  tkfocus(base)
  loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
  quit.but <- tkbutton(base, text = "Quit",
                       command = function() tkdestroy(base))
  tkpack(loc.pts, quit.but)
  tkwait.window(base)
  return(coords)
}

testplot()

## Do a few sessions of coordinate gathering, and press "Quit". The result
## is a list with the coordinates from each session.

This is exactly the idea I needed! However, I don't understand what
tkwm.deiconify() does there. It would help to know where the full
documentation for all tcltk functions is. The help pages suggest to
consult the tcltk documentation, but I don't know where this is.

Thank you,
Sebastian
-- 
Sebastian P. Luque



From chess.player at oninet.pt  Sun Jun  5 20:56:29 2005
From: chess.player at oninet.pt (jose silva)
Date: Sun, 05 Jun 2005 19:56:29 +0100
Subject: [R] function and apply
Message-ID: <2b509c95ab954f15a8b8b2021168a5ea@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050605/84a27ecb/attachment.pl

From kjetil at acelerate.com  Sun Jun  5 21:16:18 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 05 Jun 2005 15:16:18 -0400
Subject: [R] function and apply
In-Reply-To: <2b509c95ab954f15a8b8b2021168a5ea@oninet.pt>
References: <2b509c95ab954f15a8b8b2021168a5ea@oninet.pt>
Message-ID: <42A34F82.9070801@acelerate.com>

jose silva wrote:

>Dear all
>
>
>  
>
Its very difficult to read your post due to all the blank lines, so I 
cannot really
read it very well, but what about writing your function as (not tried)
 fun <- function(x) c(x[1], ....
or if that doesn't work look into
?mapply

But you should really look into the settings of your mail program!

Kjetil

> 
>
>  
>
>
> 
>
>I think my problem is not complicated but I'm having difficulties to solve it.
>
>
> 
>
>v is a vector: v=c(p1 , p2 , p3 , p4), and f  is a function: f : v -> w , where 
>
>
> 
>
>w=c(p1 , p2*(1-p1) , p3*(1-p2)*(1-p1) , p4*(1-p3)*(1-p2)*(1-p1))
>
>
> 
>
> 
>
>
> 
>
>I write the function f as:
>
>
> 
>
>f<- function(w,x,y,z) {c(w,x*(1-w),y*(1-x)*(1-w),z*(1-y)*(1-x)*(1-w))}
>
>
> 
>
>f(a,b,c,d) it works well.
>
>
> 
>
> 
>
>
> 
>
>But now I want to apply f to each row of a data frame with 4 columns:
>
>
> 
>
>d<-data.frame(a=seq(1,10), b=seq(11,20), c=seq(21,30),d=seq(31,40))
>
>
> 
>
>t(apply(d,1,f)) is not working?
>
>
> 
>
>I think each element of each row is not corresponding to w,x,y,z and now I'm lost?
>
>
> 
>
> 
>
>
> 
>
>Can someone help me?
>
>
> 
>
> 
>
>
> 
>
>thks
>
>
> 
>
> 
>
>
> 
>
>J. Silva
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ggrothendieck at gmail.com  Sun Jun  5 21:38:34 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 15:38:34 -0400
Subject: [R] function and apply
In-Reply-To: <2b509c95ab954f15a8b8b2021168a5ea@oninet.pt>
References: <2b509c95ab954f15a8b8b2021168a5ea@oninet.pt>
Message-ID: <971536df05060512382dc1f0f@mail.gmail.com>

On 6/5/05, jose silva <chess.player at oninet.pt> wrote:
> Dear all
> 
> 
> 
> 
> 
> 
> 
> 
> 
> I think my problem is not complicated but I'm having difficulties to solve it.
> v is a vector: v=c(p1 , p2 , p3 , p4), and f is a function: f : v -> w , where
> w=c(p1 , p2*(1-p1) , p3*(1-p2)*(1-p1) , p4*(1-p3)*(1-p2)*(1-p1))
> 
> I write the function f as:
> f<- function(w,x,y,z) {c(w,x*(1-w),y*(1-x)*(1-w),z*(1-y)*(1-x)*(1-w))}
> f(a,b,c,d) it works well.

> But now I want to apply f to each row of a data frame with 4 columns: 
> d<-data.frame(a=seq(1,10), b=seq(11,20), c=seq(21,30),d=seq(31,40))
> t(apply(d,1,f)) is not working?
> I think each element of each row is not corresponding to w,x,y,z and now I'm lost?
> Can someone help me?

apply passes each of row of d to f as a single vector, not 4 individual
elements, so in terms of your f define fv which is like f but takes a 
vector argument:

fv <- function(x) f(x[1], x[2], x[3], x[4])
apply(d,1,fv)  # or maybe you want t(apply(d,1,fv))

# In fact, since your f works with vectors as arguments this would do:
as.data.frame(fv(d))



From chess.player at oninet.pt  Sun Jun  5 21:55:52 2005
From: chess.player at oninet.pt (jose silva)
Date: Sun, 05 Jun 2005 20:55:52 +0100
Subject: [R] function and apply
Message-ID: <cc82639fad874417a7dbd6e63b175a0d@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050605/d17fcc4e/attachment.pl

From j_brindle at hotmail.com  Sun Jun  5 23:25:41 2005
From: j_brindle at hotmail.com (Jim BRINDLE)
Date: Sun, 5 Jun 2005 17:25:41 -0400
Subject: [R] Superscript text in axis labels
Message-ID: <BAY20-DAV55FA6F278A68205637E2980F80@phx.gbl>

Hello,

I am guessing this is not a difficult question.  But at the same time, I 
haven't had much luck figuring this issue out from the R documentation and 
help pages.

How can I create superscript text in the labels of a plot?  The parameter 
along my y-axis is volume and my desire is for the y-axis label to read 
"Volume (cm^3)" except with the "3" in the superscript form/position.

Any insight or references would be greatly appreciated.

Thanks a million,
Jim



From ggrothendieck at gmail.com  Sun Jun  5 23:32:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 17:32:21 -0400
Subject: [R] Superscript text in axis labels
In-Reply-To: <BAY20-DAV55FA6F278A68205637E2980F80@phx.gbl>
References: <BAY20-DAV55FA6F278A68205637E2980F80@phx.gbl>
Message-ID: <971536df050605143273e62fd6@mail.gmail.com>

On 6/5/05, Jim BRINDLE <j_brindle at hotmail.com> wrote:
> Hello,
> 
> I am guessing this is not a difficult question.  But at the same time, I
> haven't had much luck figuring this issue out from the R documentation and
> help pages.
> 
> How can I create superscript text in the labels of a plot?  The parameter
> along my y-axis is volume and my desire is for the y-axis label to read
> "Volume (cm^3)" except with the "3" in the superscript form/position.

plot(1:10, ylab = quote(Volume ~ (cm^3)))

See ?plotmath and try
demo(plotmath)



From james at bovik.org  Mon Jun  6 01:49:41 2005
From: james at bovik.org (James Salsman)
Date: Sun, 05 Jun 2005 16:49:41 -0700
Subject: [R] need an R-squared from a nls logistic sigmoid fit
In-Reply-To: <426A51E1.1010601@stat.wisc.edu>
References: <426A0184.8040408@bovik.org> <426A51E1.1010601@stat.wisc.edu>
Message-ID: <42A38F95.2060804@bovik.org>

Why doesn't nls() produce any kind of R-squared value?  In the absence
of such information, how are we supposed to compare one fit to another
when the y-axis scale changes?

> sm <- nls(y ~ SSfpl(x, miny, maxy, midx, grad))
> summary(sm)

Formula: y ~ SSfpl(x, miny, maxy, midx, grad)

Parameters:
      Estimate Std. Error t value Pr(>|t|)
miny  -0.5845     4.6104  -0.127  0.90524
maxy   7.2680     1.5512   4.686  0.00941 **
midx  16.9187     2.2340   7.573  0.00163 **
grad   1.7283     1.9150   0.903  0.41782
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 1.13 on 4 degrees of freedom

Correlation of Parameter Estimates:
         miny    maxy    midx
maxy -0.6654
midx  0.8936 -0.3221
grad -0.9068  0.8477 -0.6865

>



From ggrothendieck at gmail.com  Mon Jun  6 02:22:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 20:22:05 -0400
Subject: [R] need an R-squared from a nls logistic sigmoid fit
In-Reply-To: <42A38F95.2060804@bovik.org>
References: <426A0184.8040408@bovik.org> <426A51E1.1010601@stat.wisc.edu>
	<42A38F95.2060804@bovik.org>
Message-ID: <971536df050605172236b5af73@mail.gmail.com>

On 6/5/05, James Salsman <james at bovik.org> wrote:
> Why doesn't nls() produce any kind of R-squared value?  In the absence
> of such information, how are we supposed to compare one fit to another
> when the y-axis scale changes?
> 
> > sm <- nls(y ~ SSfpl(x, miny, maxy, midx, grad))
> > summary(sm)
> 
> Formula: y ~ SSfpl(x, miny, maxy, midx, grad)
> 
> Parameters:
>      Estimate Std. Error t value Pr(>|t|)
> miny  -0.5845     4.6104  -0.127  0.90524
> maxy   7.2680     1.5512   4.686  0.00941 **
> midx  16.9187     2.2340   7.573  0.00163 **
> grad   1.7283     1.9150   0.903  0.41782
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 1.13 on 4 degrees of freedom
> 
> Correlation of Parameter Estimates:
>         miny    maxy    midx
> maxy -0.6654
> midx  0.8936 -0.3221
> grad -0.9068  0.8477 -0.6865
> 

One uses anova (which has an anova.nls method) to compare two
nls models.



From 057448c at acadiau.ca  Mon Jun  6 02:59:35 2005
From: 057448c at acadiau.ca (Krista Chin)
Date: Sun, 5 Jun 2005 21:59:35 -0300
Subject: [R] greater-than-or-equal-to symbol in a legend
Message-ID: <000001c56a33$02153db0$fc87a283@kris>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050605/7d1fd5a8/attachment.pl

From ggrothendieck at gmail.com  Mon Jun  6 03:20:14 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 21:20:14 -0400
Subject: [R] greater-than-or-equal-to symbol in a legend
In-Reply-To: <000001c56a33$02153db0$fc87a283@kris>
References: <000001c56a33$02153db0$fc87a283@kris>
Message-ID: <971536df05060518201d7605f7@mail.gmail.com>

On 6/5/05, Krista Chin <057448c at acadiau.ca> wrote:
> Hello,
> 
> I am unable to get the greater-than-or-equal-to symbol (?) in a legend.
> I am able to get pi, squares, sqrt, but not this particular symbol.  Can
> anyone tell me what I am doing wrong?
> 
> legend(locator(n=1), legend = c("Observed", expression("Area < 5"* m^2),
> expression("Area >= 5"*m^2)), pch=c(1,16,4), bg="white")
> 

Remove the quotes:

legend(locator(n=1), legend = c("Observed", expression(Area < 5 * m^2),
     expression(Area >= 5*m^2)), pch=c(1,16,4), bg="white")



From Ivy_Li at smics.com  Mon Jun  6 03:51:24 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 6 Jun 2005 09:51:24 +0800
Subject: [R] fail in adding library in new version. 
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FEF3@ex120.smic-sh.com>

Hello everybody,
	Could I consult you a question?
	I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version. 
*	I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools> 
*	Download the tools.zip 
*	Unpack tools.zip into c:\cygwin
*	Install Active Perl in c:\Perl
*	Install the mingw32 port of gcc in c:\mingwin
*	Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
*	Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format? 
*	Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
	So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
	
	Could anyone help me to settle this problems?
	Thanks very much!



Best Regards!
Ivy LiÅ£Å®Å¿ÅÓÅËÅ°Å£Å©
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



From spencer.graves at pdf.com  Mon Jun  6 04:10:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 05 Jun 2005 19:10:14 -0700
Subject: [R] need an R-squared from a nls logistic sigmoid fit
In-Reply-To: <971536df050605172236b5af73@mail.gmail.com>
References: <426A0184.8040408@bovik.org>
	<426A51E1.1010601@stat.wisc.edu>	<42A38F95.2060804@bovik.org>
	<971536df050605172236b5af73@mail.gmail.com>
Message-ID: <42A3B086.9080709@pdf.com>

	  Gabor explained, "how are we supposed to compare one fit to another" 
but not "Why doesn't nls() produce any kind of R-squared value".  For 
that, I requested 'RSiteSearch("R-squared from nls")'.  The
seventh hit was a summary of an earlier discussion on this issue 
(http://finzi.psych.upenn.edu/R/Rhelp02a/archive/5120.html).  Perhaps 
the most succinct reply was from Doug Bates:

"There is a good reason that an nls model fit in R does not provide
r-squared - r-squared doesn't make sense for a general nls model.

One way of thinking of r-squared is as a comparison of the residual
sum of squares for the fitted model to the residual sum of squares for
a trivial model that consists of a constant only. You cannot
guarantee that this is a comparison of nested models when dealing with
an nls model. If the models aren't nested this comparison is not
terribly meaningful.


  So the answer is that you probably don't want to do this in the first
place."

	  Doug Bates wrote "nls" in R and is the lead author of my favorite 
book on the subject.  I've learned a lot from him.  That would not 
necessarily stop me from computing an R^2 from nls output, using formula 
contributed by Andy Jaworski to this earlier discussion.  However, I 
would be careful about how I used that R^2.

	  spencer graves

Gabor Grothendieck wrote:

> On 6/5/05, James Salsman <james at bovik.org> wrote:
> 
>>Why doesn't nls() produce any kind of R-squared value?  In the absence
>>of such information, how are we supposed to compare one fit to another
>>when the y-axis scale changes?
>>
>>
>>>sm <- nls(y ~ SSfpl(x, miny, maxy, midx, grad))
>>>summary(sm)
>>
>>Formula: y ~ SSfpl(x, miny, maxy, midx, grad)
>>
>>Parameters:
>>     Estimate Std. Error t value Pr(>|t|)
>>miny  -0.5845     4.6104  -0.127  0.90524
>>maxy   7.2680     1.5512   4.686  0.00941 **
>>midx  16.9187     2.2340   7.573  0.00163 **
>>grad   1.7283     1.9150   0.903  0.41782
>>---
>>Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>
>>Residual standard error: 1.13 on 4 degrees of freedom
>>
>>Correlation of Parameter Estimates:
>>        miny    maxy    midx
>>maxy -0.6654
>>midx  0.8936 -0.3221
>>grad -0.9068  0.8477 -0.6865
>>
> 
> 
> One uses anova (which has an anova.nls method) to compare two
> nls models.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gohidg at gmail.com  Mon Jun  6 04:20:53 2005
From: gohidg at gmail.com (Guohui Ding)
Date: Mon, 6 Jun 2005 10:20:53 +0800
Subject: [R] simplified Chinese and traditional Chinese translation for R
	manuals
Message-ID: <f04a1d1d050605192044144ec1@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/4a1bfe66/attachment.pl

From ggrothendieck at gmail.com  Mon Jun  6 04:21:01 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 5 Jun 2005 22:21:01 -0400
Subject: [R] fail in adding library in new version.
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FEF3@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FEF3@ex120.smic-sh.com>
Message-ID: <971536df050605192156ca3923@mail.gmail.com>

On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Hello everybody,
>        Could I consult you a question?
>        I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.

Getting the latest version of R is strongly recommended.  The suggestions
below all assume the latest version and may or may not work if you do
not upgrade.

> *       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> *       Download the tools.zip
> *       Unpack tools.zip into c:\cygwin
> *       Install Active Perl in c:\Perl
> *       Install the mingw32 port of gcc in c:\mingwin
> *       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin

You may need to put these at the beginning of the path rather than the end.
Also just as a check enter 
     path
at the console to make sure that you have them.  You will likely
have to start a new console session and possibly even reboot.

Also you need the Microsoft Help Compiler, hhc.  Suggest
you reread the material on which tools you need.

> *       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?

In MyRPackages you would have a folder called example, in your case,
that contains the package.  Within folder example, you would have the
DESCRIPTION file, the R folder, etc.

> *       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?

You don't have to run R first.  You do need to make sure that R.exe can
be found on your path or else use the absolute path name in referring to R.
For example, if your path does not include R you could do something like this:

cd \Program Files\R\rw2010
bin\R cmd install /MyRPackages/example

Be sure to use forward slashes where shown above and backslashes
where shown.

>        So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> 

Try all these suggestions including upgrading R and if that does not work
try posting screen dumps of the actual errors you are getting.



From robin at hms.harvard.edu  Mon Jun  6 04:26:17 2005
From: robin at hms.harvard.edu (Robin Colgrove)
Date: Sun, 5 Jun 2005 22:26:17 -0400
Subject: [R] stats and generating a figure for a simple sign test with high
	inter-experiment variance
Message-ID: <a7843e79b65e37fc35ebd9ea365a4eb1@hms.harvard.edu>


Hello all,

Sorry if this is an FAQ. I have been trying to search the archives 
without success.
I have a dataset (ChiPs microarray) where the experiment to experiment 
variability is very high
but where within an experiment, the data nearly always goes in the 
"right" (hypothesis confirming) direction.
I am trying to figure out the right way to use R to do the statistics 
and generate an appropriate figure.

To be specific, we have a virus and a mutant derivative, and the 
hypothesis is that the wild type virus is specifically suppressing 
active transcription in a manner that is abrogated in the mutant.

The experiment is to measure the amount of viral chromatin associated 
with overall histone (should be the same between wild type and mutant), 
vs. transcriptionally active chromatin (mutant should be greater than 
wild type) vs. inactive chromatin (wild type should be greater than 
mutant).

For each experiment there is a histone type (general, active, 
inactive), a specific gene assayed (four different genes), and a virus 
used for infection (wild type or mutant). These are hard experiments to 
do (involving dissecting out small numbers of cells from a mouse) so 
the numbers are small, but in each case, there are 3-5 pairs of wild 
type vs mutant virus for each condition.

If I look at simply whether the hypothesis is confirmed for each 
condition (whether the wild type/mutant difference goes the way you 
would expect), then the sign is right 34/35 times, which is way beyond 
reasonable significance. However, since the inter-experiment variance 
is so high, if I try to do a simple rank-sum test for a particular 
chromatin-gene-virus combination (3-5 pairs), the result is usually 
non-significant, or never significant if Bonferroni corrections for 
multiple tests are applied.

My questions are:

1) what would be the right way to use R to do and report a simple sign 
test on this sort of data (paired samples, non-normal, high-inter 
experiment variability).

2) What is the best way to plot this and how to do it? I was thinking 
of having each (wildtype-mutant) experiment pair as the ends of line 
segments with different colors or line-types for each 
gene-chromatintype combination. I know this is a standard kind of plot 
but I can't figure out how to do it in R.

3) What is the best way to input this data? A 4d array with virus type 
(wild type or mutant) on one axis, chromatin type (non-specific, 
active, inactive) on the second, gene (one of four different genes) on 
the third, and experiment number (1-5) on the last? Is there a good way 
to do this with data frames?

Thanks for any help. I am not trying to be a sponge and am really 
trying to figure this out myself, but as a virologist/bioinformaticist 
I still have a lot to learn statistics-wise.

robin colgrove
dept. of microbiology
harvard medical school



From spencer.graves at pdf.com  Mon Jun  6 04:31:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 05 Jun 2005 19:31:53 -0700
Subject: [R] fail in adding library in new version.
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FEF3@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FEF3@ex120.smic-sh.com>
Message-ID: <42A3B599.2030109@pdf.com>

Hi, Ivy_Li:

	  I also had trouble adding libraries under recent versions of R.  The
new versions of R ask you to select a local CRAN mirror.  Unfortunately,
I was using XEmacs, which failed to pass on the request to select a
local CRAN mirror and instead locked up.  I started using RGui and have
since been able to install and update packages as before.

	  hope this helps.
	  spencer graves

Ivy_Li wrote:

> Hello everybody,
> 	Could I consult you a question?
> 	I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version. 
> *	I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools> 
> *	Download the tools.zip 
> *	Unpack tools.zip into c:\cygwin
> *	Install Active Perl in c:\Perl
> *	Install the mingw32 port of gcc in c:\mingwin
> *	Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> *	Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format? 
> *	Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> 	So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> 	
> 	Could anyone help me to settle this problems?
> 	Thanks very much!
> 
> 
> 
> Best Regards!
> Ivy LiÅ£Å®Å¿ÅÓÅËÅ°Å£Å©
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luan_sheng at yahoo.com  Mon Jun  6 04:58:11 2005
From: luan_sheng at yahoo.com (luan_sheng)
Date: Mon, 06 Jun 2005 10:58:11 +0800
Subject: [R] how to change the current directory to my folder.
Message-ID: <42A3BBC3.8080009@yahoo.com>

I want to th current folder is the specific folder when R start.It is
very boring to modify the current directory manually. How can I do it?



From edd at debian.org  Mon Jun  6 05:16:05 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 5 Jun 2005 22:16:05 -0500
Subject: [R] how to change the current directory to my folder.
In-Reply-To: <42A3BBC3.8080009@yahoo.com>
References: <42A3BBC3.8080009@yahoo.com>
Message-ID: <17059.49141.663787.218260@basebud.nulle.part>


On 6 June 2005 at 10:58, luan_sheng wrote:
| I want to th current folder is the specific folder when R start.It is
| very boring to modify the current directory manually. How can I do it?

The first answer to 

	  > RSiteSearch("change directory")

tells you about setwd() and getwd().

There *are* good help facilities available. Please use them.

Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From yzhang4 at turing.une.edu.au  Mon Jun  6 05:38:15 2005
From: yzhang4 at turing.une.edu.au (Yuandan Zhang)
Date: Mon, 06 Jun 2005 13:38:15 +1000
Subject: [R] simplified Chinese and traditional Chinese translation for
	R	manuals
In-Reply-To: <f04a1d1d050605192044144ec1@mail.gmail.com>
References: <f04a1d1d050605192044144ec1@mail.gmail.com>
Message-ID: <42A3C527.4000408@turing.une.edu.au>

Guohui,

Great effort for translating this document into Chinese. I will 
certainly read through and send some feedback to you.

Best

Yuandan

Guohui Ding wrote:

>Hi, every one,
>I have translated <An Introduction to R > into simplified Chinese and 
>traditional Chinese. You can browse them from these two URL:
>simplified Chinese: 
>http://www.biosino.org/pages/newhtm/r/schtml/index.html#Top
>traditional Chinese: http://www.biosino.org/pages/newhtm/r/tchtml/
>
>I have distributed these document in HTML format. That is because there is 
>something wrong with the format change between the PDF format and texinfo 
>format in Chinese. However, I will overcome this problem soon. If some 
>website on R have some interest to these Chinese translation, you can get 
>all the HTML document of them from me.
>
>The other four manuals is being translated now, and I will distribute them 
>in the future.
>The Chinese translation for <An Introduction to R> is a beta version, and 
>will need some modification and your suggestion. More information was in the 
>document.
>
>R is a great sofeware. I like it and I will do something for it.
>Wish You Will Enjoy It!
>
>You can send a email to me. Any suggestion and comment are welcome!
>
>  
>



From joel3000 at gmail.com  Mon Jun  6 05:48:32 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Sun, 5 Jun 2005 20:48:32 -0700
Subject: [R] segmentation fault - debugging
Message-ID: <1253d67a0506052048a82fd@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050605/8bddacf4/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Jun  6 08:45:11 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 08:45:11 +0200
Subject: [R] how to change the current directory to my folder.
In-Reply-To: <17059.49141.663787.218260@basebud.nulle.part>
References: <42A3BBC3.8080009@yahoo.com>
	<17059.49141.663787.218260@basebud.nulle.part>
Message-ID: <x2wtp8t24o.fsf@turmalin.kubism.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> On 6 June 2005 at 10:58, luan_sheng wrote:
> | I want to th current folder is the specific folder when R start.It is
> | very boring to modify the current directory manually. How can I do it?
> 
> The first answer to 
> 
> 	  > RSiteSearch("change directory")
> 
> tells you about setwd() and getwd().
> 
> There *are* good help facilities available. Please use them.
> 
> Hope this helps, Dirk

I think that's what the poster considers boring. On Windows, you can
also (a) modify the properties of the startup icon and/or start menu
entry (just change the "start in" field) or (b) save an R workspace
(possibly an empty one) in the relevant directory and start R by
double-clicking it.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Jun  6 08:46:18 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 07:46:18 +0100 (BST)
Subject: [R] locator() via tcltk
In-Reply-To: <87r7fgvdu9.fsf@gmail.com>
References: <87slzy6g9w.fsf@gmail.com>
	<Pine.LNX.4.61.0506040930530.32138@gannet.stats>
	<87r7fgvdu9.fsf@gmail.com>
Message-ID: <Pine.LNX.4.61.0506060737330.13936@gannet.stats>

On Sun, 5 Jun 2005, Sebastian Luque wrote:

> Dear List members,
>
> Thank you so much for your insights.
>
>
> On Sat, 4 Jun 2005 09:39:33 +0100 (BST),
> Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>
> [...]
>
>> I think it is more likely you want to wait for the Tk interaction and
>> then return the results, that is use a `modal' widget. If so, take a
>> look at the examples in src/library/tcltk/R/utils.R which are modal and
>> return their results.
>
>
> Yes, this is indeed what I was looking for. Additionally, I needed to be
> able to make several calls to getcoords and store the results from each
> call, i.e.: be able to repeat the sequence 1. click "Get coordinates", 2.
> do some clicks, finish by clicking any button other than left, as many
> times as needed, and return the results from each sequence separately. I
> looked at the code you pointed me to and came up with this:
>
> require(tcltk)
>
> testplot <- function() {
>  coords <- list()
>  getcoords <- function(...) {
>  coords[[length(coords) + 1]] <<- locator()
>  tkgrab.release(base)
>  }
>  x <- 1:1000
>  y <- rnorm(1000)
>  plot(x, y)
>  base <- tktoplevel()
>  tkwm.deiconify(base)
>  tkgrab.set(base)
>  tkfocus(base)
>  loc.pts <- tkbutton(base, text = "Get coordinates", command = getcoords)
>  quit.but <- tkbutton(base, text = "Quit",
>                       command = function() tkdestroy(base))
>  tkpack(loc.pts, quit.but)
>  tkwait.window(base)
>  return(coords)
> }
>
> testplot()
>
> ## Do a few sessions of coordinate gathering, and press "Quit". The result
> ## is a list with the coordinates from each session.
>
> This is exactly the idea I needed! However, I don't understand what
> tkwm.deiconify() does there.

It ensures the window is displayed as a window and not an icon, and on 
Windows that it gets raised.  It may not be necessary, but it is a safety 
measure.

> It would help to know where the full
> documentation for all tcltk functions is. The help pages suggest to
> consult the tcltk documentation, but I don't know where this is.

On R for Windows, in R_HOME/Tcl/doc.
On a Unix-alike, use the man pages.

Unfortunately, you need to be able to work out that tkwm.deiconify maps 
to 'wm deiconify' which means you need the help/man page for 'wm' which 
says

wm deiconify window

Arrange for window to be displayed in normal (non-iconified) form. This is 
done by mapping the window.  If the window has never been mapped then this 
command will not map the window, but it will ensure that when the window 
is first mapped it will be displayed in de-iconified form.  On Windows, a 
deiconified window will also be raised and be given the focus (made the 
active window). Returns an empty string.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun  6 08:52:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 07:52:01 +0100 (BST)
Subject: [R] greater-than-or-equal-to symbol in a legend
In-Reply-To: <000001c56a33$02153db0$fc87a283@kris>
References: <000001c56a33$02153db0$fc87a283@kris>
Message-ID: <Pine.LNX.4.61.0506060748440.13936@gannet.stats>

On Sun, 5 Jun 2005, Krista Chin wrote:

> I am unable to get the greater-than-or-equal-to symbol (?) in a legend.
> I am able to get pi, squares, sqrt, but not this particular symbol.  Can
> anyone tell me what I am doing wrong?
>
> legend(locator(n=1), legend = c("Observed", expression("Area < 5"* m^2),
> expression("Area >= 5"*m^2)), pch=c(1,16,4), bg="white")

You want >= as part of the expression, not part of a character string.
I suspect

 	expression(Area >= 5*m^2)

is what you intended.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From v.demartino2 at virgilio.it  Mon Jun  6 10:10:27 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Mon, 6 Jun 2005 10:10:27 +0200
Subject: [R] R code for performance
Message-ID: <429C8F5C00008211@ims3e.cp.tin.it>

At office I'm cautiously introducing R to be used as the basic statistical
program, getting rid of licensed stuff or reducing the amount of it.
The aim of R would be to run generic statistical programs built & "consumed"
when needed and some static procedure dealing with time-series.
Now, we have substantially 3 OS platforms, win xp, linux and freebsd 5.4,
on similar PCs (pentium 4, 2-2.5 GHz). I have been asked by the boss to
test the "average" performance (in term of speed and memory use) of R on
each of this platform to stick with one of them on a couple of PCs.

Could you please suggest an R source code (apart from the "static procedure"
I will obviously test) to be run on the three platforms to test performance?

If there is nothing of the kind, any suggestion?

Ciao
Vittorio



From navarre_sabine at yahoo.fr  Mon Jun  6 10:13:24 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Mon, 6 Jun 2005 10:13:24 +0200 (CEST)
Subject: [R] Similarity between variables
Message-ID: <20050606081324.75458.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/6137fb04/attachment.pl

From lecoutre at stat.ucl.ac.be  Mon Jun  6 10:21:18 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 6 Jun 2005 10:21:18 +0200
Subject: [R] R code for performance
In-Reply-To: <429C8F5C00008211@ims3e.cp.tin.it>
Message-ID: <028201c56a70$b7090430$6e8b6882@didacdom.stat.ucl.ac.be>


You could use the benchmark created by Philippe Grosjean to compare
various statistical packages. You will find it at:

http://www.sciviews.org/benchmark/

Note that you have to ensure to have installed packages: Matrix and
SuppDist

HTH,

Eric

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> v.demartino2 at virgilio.it
> Sent: lundi 6 juin 2005 10:10
> To: r-help
> Subject: [R] R code for performance
> 
> 
> At office I'm cautiously introducing R to be used as the 
> basic statistical program, getting rid of licensed stuff or 
> reducing the amount of it. The aim of R would be to run 
> generic statistical programs built & "consumed" when needed 
> and some static procedure dealing with time-series. Now, we 
> have substantially 3 OS platforms, win xp, linux and freebsd 
> 5.4, on similar PCs (pentium 4, 2-2.5 GHz). I have been asked 
> by the boss to test the "average" performance (in term of 
> speed and memory use) of R on each of this platform to stick 
> with one of them on a couple of PCs.
> 
> Could you please suggest an R source code (apart from the 
> "static procedure" I will obviously test) to be run on the 
> three platforms to test performance?
> 
> If there is nothing of the kind, any suggestion?
> 
> Ciao
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Jun  6 10:40:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 09:40:40 +0100 (BST)
Subject: [R] R code for performance
In-Reply-To: <429C8F5C00008211@ims3e.cp.tin.it>
References: <429C8F5C00008211@ims3e.cp.tin.it>
Message-ID: <Pine.LNX.4.61.0506060933570.26056@gannet.stats>

On Mon, 6 Jun 2005 v.demartino2 at virgilio.it wrote:

> At office I'm cautiously introducing R to be used as the basic statistical
> program, getting rid of licensed stuff or reducing the amount of it.
> The aim of R would be to run generic statistical programs built & "consumed"
> when needed and some static procedure dealing with time-series.
> Now, we have substantially 3 OS platforms, win xp, linux and freebsd 5.4,
> on similar PCs (pentium 4, 2-2.5 GHz). I have been asked by the boss to
> test the "average" performance (in term of speed and memory use) of R on
> each of this platform to stick with one of them on a couple of PCs.
>
> Could you please suggest an R source code (apart from the "static procedure"
> I will obviously test) to be run on the three platforms to test performance?
>
> If there is nothing of the kind, any suggestion?

'make check' runs a lot of R code and times it.  The tests for the stats 
package look most relevant to you.  Beware of simplistic 'benchmarks' that 
test code snippets not relevant to your usage (and that may apply to the R 
examples which tend to be small datasets).

We know Linux (non-R-shlib) outperforms Windows XP by ca 20%, and some 
comments I have seen here suggest it outperforms FreeBSD as well.  But are 
such differences enough to determine your choice?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ajayshah at mayin.org  Mon Jun  6 07:31:37 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Mon, 6 Jun 2005 11:01:37 +0530
Subject: [R] A performance anomaly
Message-ID: <20050606053137.GA3738@lubyanka.local>

I wrote a simple log likelihood (for the ordinary least squares (OLS)
model), in two ways. The first works out the likelihood. The second
merely calls the first, but after transforming the variance parameter,
so as to allow an unconstrained maximisation. So the second suffers a
slight cost for one exp() and then it pays the cost of calling the first.

I did performance measurement. One would expect the second version to
be slightly (very slightly) slower than the first. But I am finding
this is not the case! The second version is slightly faster. How can
this be?

On my machine (ibook @ 1.2 GHz, OS X "panther", R 2.1), I get:

> measurement(ols.lf1)
[1] 7.45
> measurement(ols.lf1.inlogs)
[1] 6.9

Here is the self-contained bug-demonstration code:

ols.lf1 <- function(theta, y, X) {
  beta <- theta[-1]
  sigma2 <- theta[1]
  if (sigma2 <= 0) return(NA)
  n <- nrow(X)
  e <- y - X%*%beta
  logl <- ((-n/2)*log(2*pi)) - ((n/2)*log(sigma2)) - ((t(e)%*%e)/(2*sigma2))
  return(-logl) # since optim() does minimisation by default.
}

# A variant: Shift to logs for sigma2 so I can do unconstrained maximisation --
ols.lf1.inlogs <- function(truetheta, y, X) {
  ols.lf1(c(exp(truetheta[1]), truetheta[-1]), y, X)
}

# We embark on one numerical experiment
set.seed(101)
X <- cbind(1, runif(20000))
theta.true <- c(2,4,6) # error variance = 2, intercept = 4, slope = 6.
y <- X %*% theta.true[-1] + sqrt(theta.true[1]) * rnorm(20000)

measurement <- function(f) {
  N <- 200
  cost <- system.time(
                      for (i in 1:N) {
                        j <- f(c(2,3,4), y, X)
                      }
                      )
  return(cost[1]*1000/N)                # cost per lf in milliseconds
}

measurement(ols.lf1)
measurement(ols.lf1.inlogs)

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Mon Jun  6 07:58:20 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Mon, 6 Jun 2005 11:28:20 +0530
Subject: [R] The economist's term "fixed effects model" - plain lm() should
	work
In-Reply-To: <0IHN00GP13WQSR@mail.fudan.edu.cn>
References: <0IHN00GP13WQSR@mail.fudan.edu.cn>
Message-ID: <20050606055820.GU7131@lubyanka.local>

> CAN YOU TELL ME HOW TO FIT FIXED-EFFECTS MODEL WITH R? THANK YOU!

Ordinary lm() might suffice. 

In the code below, I try to simulate a dataset from a standard
earnings regression, where log earnings is quadratic in experience,
but the intercept floats by education category - you have 4 intercepts
for 4 education categories.

I think this works as a simple implementation of "the fixed effects
model" in the sense of the term that is used in economics. I will be
happy to hear from R gurus about how this can be done better using
nlme or lme4.

> education <- factor(sample(1:4,1000, replace=TRUE),
                      labels=c("none", "school", "college", "beyond"))
> experience <- 30*runif(1000)            # experience from 0 to 30 years
> intercept <- c(0.5,1,1.5,2)[education]
> log.earnings <- intercept + 2*experience -
    0.05*experience*experience + rnorm(1000)
> 
> summary(lm(log.earnings ~ 
             -1 + education + experience + I(experience*experience)))

Call:
lm(formula = log.earnings ~ -1 + education + experience + I(experience * 
    experience))

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1118 -0.6525 -0.0134  0.6790  4.1763 

Coefficients:
                             Estimate Std. Error  t value Pr(>|t|)    
educationnone               0.5888110  0.1101963    5.343 1.13e-07 ***
educationschool             0.9062839  0.1106103    8.193 7.76e-16 ***
educationcollege            1.3662172  0.1141488   11.969  < 2e-16 ***
educationbeyond             1.9739789  0.1147356   17.205  < 2e-16 ***
experience                  2.0026482  0.0148110  135.214  < 2e-16 ***
I(experience * experience) -0.0499795  0.0004753 -105.152  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 1.015 on 994 degrees of freedom
Multiple R-Squared: 0.9966,     Adjusted R-squared: 0.9966 
F-statistic: 4.818e+04 on 6 and 994 DF,  p-value: < 2.2e-16 


As you see, it pretty much recovers the true parameter vector -- it
gets           c(.588, .906, 1.366, 1.974, 2.003, -0.0499, 1.015)
compared with  c(.5,   1.,   1.5,   2,     2,     -0.05,   1)

I think the standard errors and tests should also be quite fine.

Please do post an informative "summary" of your exploration on the
economist's notation about panel data (fixed effects and random
effects models) on the mailing list, when you are finished learning
this question. :-) We will all benefit. Hope this helps,

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From gohidg at gmail.com  Mon Jun  6 11:03:03 2005
From: gohidg at gmail.com (Guohui Ding)
Date: Mon, 6 Jun 2005 17:03:03 +0800
Subject: [R] Similarity between variables
In-Reply-To: <20050606081324.75458.qmail@web26604.mail.ukl.yahoo.com>
References: <20050606081324.75458.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <f04a1d1d05060602037be234e5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/5bba42af/attachment.pl

From myotisone at gmail.com  Mon Jun  6 11:06:21 2005
From: myotisone at gmail.com (Graham Smith)
Date: Mon, 6 Jun 2005 10:06:21 +0100
Subject: [R] Task View for ecology/environmental science
Message-ID: <2c75873c050606020672efadac@mail.gmail.com>

Having just discovered "Task Views" I see there isn't one for
ecology/environmental science.

I don't have the expertise to help with this, but I think it would be
a very useful addition if there was any person (s) on the list willing
to help create such a Task View.

I am sure the maintainers of the Task Views will pick up on any
responses to this post from volunteers.

I personally would be very grateful, ands wouldn't mind contributing
in some sort of administrative role, but lack the technical expertise
to do the rest.

Graham



From R.P.Clement at westminster.ac.uk  Mon Jun  6 15:28:29 2005
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: 06 Jun 2005 08:28:29 -0500
Subject: [R] Uses of isoMDS()
In-Reply-To: <1117910982.4261.61.camel@horizons.localdomain>
References: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>
	<1117910982.4261.61.camel@horizons.localdomain>
Message-ID: <1118064509.8738.16.camel@staff-pc01.harrowscs.westminster.ac.uk>

Hi. I'm using the isoMDS() function of the MASS library for
multidimensional scaling. I have two questions that I have not been able
to solve by searching through the archives.

(i) What is the exact stress measure used? The text in the help file
says:

     This chooses a k-dimensional (default k = 2) configuration to
     minimize the stress, the square root of the ratio of the sum of
     squared differences between the input distances and those of the
     configuration to the sum of configuration distances squared.
     However, the input distances are allowed a monotonic
     transformation.

I presume that this is Stress-1 from Kruskal's 1964 paper
"Multidimensional scaling by optimising goodness of fit to nonmetric
hypothesis". (I haven't got a copy of this paper but have Borg and
Groenen's book which references it). However, I'm not 100% sure given
the text description in the help file, and note that isoMDS() quotes
stress as a percentage.

As a preparation for using MDS I wrote some MDS code using iterative
majorisation. While I'm sure that isoMDS() produces much better results,
I would still like to do a like-for-like comparison. If I use a straight
correlation measure between the original and recreated distances, then
isoMDS() is considerably (typically about 0.05) better than my program,
but I'd like to have the proper stress comparison.

(ii) I write most of my code in C. I haven't been able to pin down
exactly what the status of linking C to R is. At present I do the
following:

  if ( experiment == MDS )
  {
    writeDistData( "temp.csv" );
    system( "R --no-save < rEuclidean.R >> /dev/null" );
    reread( "temp.points" );
  }
  else if ( experiment == PCA )
  {
    writeRawData( "temp.csv" );
    system( "R --no-save < rEuclidean.pca.R >> /dev/null" );
    reread( "temp.scores" );
  }

with appropriate R commands in the .R files.

This was easy to write, seems to work fine, and doesn't seem too slow.
But, I'd still like to ask if there is a tidier way of achieving the
same effect.

Thanks in anticipation,

Ross-c



From juliebertrand2001 at yahoo.fr  Mon Jun  6 11:20:48 2005
From: juliebertrand2001 at yahoo.fr (Julie BERTRAND)
Date: Mon, 6 Jun 2005 11:20:48 +0200 (CEST)
Subject: [R] multiple comparison test
Message-ID: <20050606092048.19106.qmail@web26808.mail.ukl.yahoo.com>

hello,
 
after an anova I use pairwise.t.test(), it gives only
p.value and I want the t.stat.
I try to get these by computing the Welch
approximation of the degree of freedom and using the
qt(p.value,df) function but when I test this method
with t.test results (the function gives p.value and
t.test), I doesn't find the same t.stat.
 
I also use the simtest(x~y,type="Tukey") function and
it gives me only negative t.stat, resulting in a weard
distribution...
 
thank you in advance for all the attention you 'll
give to my problems.
Julie BERTRAND



From p.dalgaard at biostat.ku.dk  Mon Jun  6 11:25:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 11:25:41 +0200
Subject: [R] R code for performance
In-Reply-To: <Pine.LNX.4.61.0506060933570.26056@gannet.stats>
References: <429C8F5C00008211@ims3e.cp.tin.it>
	<Pine.LNX.4.61.0506060933570.26056@gannet.stats>
Message-ID: <x2vf4r7s6i.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 6 Jun 2005 v.demartino2 at virgilio.it wrote:
> 
> > At office I'm cautiously introducing R to be used as the basic statistical
> > program, getting rid of licensed stuff or reducing the amount of it.
> > The aim of R would be to run generic statistical programs built & "consumed"
> > when needed and some static procedure dealing with time-series.
> > Now, we have substantially 3 OS platforms, win xp, linux and freebsd 5.4,
> > on similar PCs (pentium 4, 2-2.5 GHz). I have been asked by the boss to
> > test the "average" performance (in term of speed and memory use) of R on
> > each of this platform to stick with one of them on a couple of PCs.
> >
> > Could you please suggest an R source code (apart from the "static procedure"
> > I will obviously test) to be run on the three platforms to test performance?
> >
> > If there is nothing of the kind, any suggestion?
> 
> 'make check' runs a lot of R code and times it.  The tests for the
> stats package look most relevant to you.  Beware of simplistic
> 'benchmarks' that test code snippets not relevant to your usage (and
> that may apply to the R examples which tend to be small datasets).
> 
> We know Linux (non-R-shlib) outperforms Windows XP by ca 20%, and some
> comments I have seen here suggest it outperforms FreeBSD as well.  But
> are such differences enough to determine your choice?

The scripts from the MASS package can also be used as an informal
benchmark, perhaps a bit more of a realistic mix than the stats
package. (Or was there a reason that Brian didn't mention them?)

It might also be relevant to note that, at least for a while, there
isn't going to be a 64 bit Windows version (the compiler etc. tool
chain is missing) so if you have large memory requirements, Linux or
BSD is the way to go. They also tend to be much easier to get
configured for building your own packages or just for using C/Fortran
extensions. The flip side is of course the (perceived)
userfriendliness of Windows.

If you have hardcore linear algebra requirements (e.g. inversion of
large matrices), you need to look into builds linked against fast BLAS
code (Goto, ATLAS). Most of the standard builds do not use this, so
benchmarks will be quite misleading.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From subianto at gmail.com  Mon Jun  6 11:28:18 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 06 Jun 2005 11:28:18 +0200
Subject: [R] adaboost more two classes
Message-ID: <42A41732.8060506@gmail.com>

Dear R-Helper,
I want to know, is there any function/package can handle adaboost more 
two classes?
I know packages gbm and boost, but there are only for 2 classes (correct 
me if I mistake).
Regards, Muhammad Subianto



From phgrosjean at sciviews.org  Mon Jun  6 11:29:30 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 06 Jun 2005 11:29:30 +0200
Subject: [R] Task View for ecology/environmental science
In-Reply-To: <2c75873c050606020672efadac@mail.gmail.com>
References: <2c75873c050606020672efadac@mail.gmail.com>
Message-ID: <42A4177A.8070504@sciviews.org>

Hello Graham and all,

I may _contribute_ to this, but not maintain it: I have too much work to 
initiate such a Task View. If someone would like to start it, just 
contact me for help.
Best,

Philippe Grosjean

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Graham Smith wrote:
> Having just discovered "Task Views" I see there isn't one for
> ecology/environmental science.
> 
> I don't have the expertise to help with this, but I think it would be
> a very useful addition if there was any person (s) on the list willing
> to help create such a Task View.
> 
> I am sure the maintainers of the Task Views will pick up on any
> responses to this post from volunteers.
> 
> I personally would be very grateful, ands wouldn't mind contributing
> in some sort of administrative role, but lack the technical expertise
> to do the rest.
> 
> Graham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From awitney at sgul.ac.uk  Mon Jun  6 11:29:43 2005
From: awitney at sgul.ac.uk (Adam Witney)
Date: Mon, 06 Jun 2005 10:29:43 +0100
Subject: [R] Storing data frame in a RDBMS
In-Reply-To: <42A20C7E.1040009@joeconway.com>
Message-ID: <BEC9D617.4719B%awitney@sgul.ac.uk>

On 4/6/05 9:18 pm, "Joe Conway" <mail at joeconway.com> wrote:

> Gabor Grothendieck wrote:
>> On 6/4/05, Adam Witney <awitney at sgul.ac.uk> wrote:
>>> I am using PL/R in PostgreSQL amd have written some functions to build my
>>> data frame. However this can take some time with some large datasets and I
>>> would like to not have to repeat the process and so I would like to save the
>>> data frame. Rather than save/load into the file system I would like to be
>>> able to save the entire data frame as a single object in the database
>>> 
>>> Is this possible?
>> 
>> Check out ?serialize
> 
> Looks like serialize should work nicely:
> 
> create or replace function test_serialize(text)
> returns text as '
>  mydf <- pg.spi.exec(arg1)
>  return (serialize(mydf, NULL, ascii = TRUE))
> ' language 'plr';
> 
> create table saved_df (id int, df text);
> 
> insert into saved_df
> select 1, f from test_serialize('select oid, typname from pg_type
>                                  where typname = ''oid''
>                                  or typname = ''text''') as t(f);
> 
> create or replace function restore_df(text)
> returns setof record as '
>  unserialize(arg1)
> ' language 'plr';
> 
> select * from restore_df((select df from saved_df where id =1))
> as t(oid oid, typname name);
> oid | typname
> -----+---------
>  25 | text
>  26 | oid
> (2 rows)
> 
> HTH,

That looks great Joe, Gabor. Thanks for your help.

Cheers

Adam



-- 
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From ripley at stats.ox.ac.uk  Mon Jun  6 11:35:46 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 10:35:46 +0100 (BST)
Subject: [R] Uses of isoMDS()
In-Reply-To: <1118064509.8738.16.camel@staff-pc01.harrowscs.westminster.ac.uk>
References: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>
	<1117910982.4261.61.camel@horizons.localdomain>
	<1118064509.8738.16.camel@staff-pc01.harrowscs.westminster.ac.uk>
Message-ID: <Pine.LNX.4.61.0506061031060.1374@gannet.stats>

The answer to Q1 is in the reference on the help page: isoMDS is support 
software for a book. It is quoted as a percentage (as the help page says).

Q2 is about linking R to C, not C to R.  You can embed R in your C 
program: see `Writing R Extensions', but it is definitely messier.

On Mon, 6 Jun 2005, Ross Clement wrote:

> Hi. I'm using the isoMDS() function of the MASS library for
> multidimensional scaling. I have two questions that I have not been able
> to solve by searching through the archives.
>
> (i) What is the exact stress measure used? The text in the help file
> says:
>
>     This chooses a k-dimensional (default k = 2) configuration to
>     minimize the stress, the square root of the ratio of the sum of
>     squared differences between the input distances and those of the
>     configuration to the sum of configuration distances squared.
>     However, the input distances are allowed a monotonic
>     transformation.
>
> I presume that this is Stress-1 from Kruskal's 1964 paper
> "Multidimensional scaling by optimising goodness of fit to nonmetric
> hypothesis". (I haven't got a copy of this paper but have Borg and
> Groenen's book which references it). However, I'm not 100% sure given
> the text description in the help file, and note that isoMDS() quotes
> stress as a percentage.
>
> As a preparation for using MDS I wrote some MDS code using iterative
> majorisation. While I'm sure that isoMDS() produces much better results,
> I would still like to do a like-for-like comparison. If I use a straight
> correlation measure between the original and recreated distances, then
> isoMDS() is considerably (typically about 0.05) better than my program,
> but I'd like to have the proper stress comparison.
>
> (ii) I write most of my code in C. I haven't been able to pin down
> exactly what the status of linking C to R is. At present I do the
> following:
>
>  if ( experiment == MDS )
>  {
>    writeDistData( "temp.csv" );
>    system( "R --no-save < rEuclidean.R >> /dev/null" );
>    reread( "temp.points" );
>  }
>  else if ( experiment == PCA )
>  {
>    writeRawData( "temp.csv" );
>    system( "R --no-save < rEuclidean.pca.R >> /dev/null" );
>    reread( "temp.scores" );
>  }
>
> with appropriate R commands in the .R files.
>
> This was easy to write, seems to work fine, and doesn't seem too slow.
> But, I'd still like to ask if there is a tidier way of achieving the
> same effect.


> Thanks in anticipation,
>
> Ross-c

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jun  6 11:45:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 11:45:43 +0200
Subject: [R] A performance anomaly
In-Reply-To: <20050606053137.GA3738@lubyanka.local>
References: <20050606053137.GA3738@lubyanka.local>
Message-ID: <x2r7ff7r94.fsf@turmalin.kubism.ku.dk>

Ajay Narottam Shah <ajayshah at mayin.org> writes:

> I wrote a simple log likelihood (for the ordinary least squares (OLS)
> model), in two ways. The first works out the likelihood. The second
> merely calls the first, but after transforming the variance parameter,
> so as to allow an unconstrained maximisation. So the second suffers a
> slight cost for one exp() and then it pays the cost of calling the first.
> 
> I did performance measurement. One would expect the second version to
> be slightly (very slightly) slower than the first. But I am finding
> this is not the case! The second version is slightly faster. How can
> this be?
> 
> On my machine (ibook @ 1.2 GHz, OS X "panther", R 2.1), I get:
> 
> > measurement(ols.lf1)
> [1] 7.45
> > measurement(ols.lf1.inlogs)
> [1] 6.9
> 
> Here is the self-contained bug-demonstration code:

What "bug"?

You're just not testing carefully enough. Timings are not perfectly
reproducible; you'll need multiple replications. I get

> replicate(50,c(measurement(ols.lf1.inlogs),measurement(ols.lf1)))
     [,1] [,2] [,3] [,4] [,5]  [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
[1,] 9.10 9.60  9.5 9.70 9.95 10.00  9.3 9.85 9.55  9.95 10.05 10.20  9.65
[2,] 9.75 9.65  9.2 9.15 9.60  9.55  9.8 9.85 9.75  9.70  9.35  9.65  9.60
     [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]
[1,]  9.65  9.55   9.1  9.90 10.00  9.45  9.30   9.4   9.2 10.25  9.80 10.10
[2,]  9.95  9.90   9.7  9.55  9.55  9.80  9.95  10.6   9.3 10.30  9.55  9.65
     [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37]
[1,]   9.6  9.50  9.55  9.65  9.85  9.70  9.75   9.7  9.25  9.85   9.8  9.75
[2,]   9.7  9.75  9.10  9.75  9.40  9.55  9.95   9.9  9.55  9.55   9.3  9.60
     [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49]
[1,]   9.7  9.25  9.65  9.35  9.95  9.95  9.75  9.20  9.55  9.65  9.80  9.75
[2,]   9.9  9.65 10.20  9.20  9.65  9.50 10.25  9.75  9.90  9.10  9.25  9.05
     [,50]
[1,]  9.55
[2,]  9.40
> x <- .Last.value
> t.test(x[1,],x[2,])

        Welch Two Sample t-test

data:  x[1, ] and x[2, ]
t = 0.284, df = 96.202, p-value = 0.777
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.1018137  0.1358137
sample estimates:
mean of x mean of y
    9.663     9.646


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Mon Jun  6 11:54:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 06 Jun 2005 11:54:47 +0200
Subject: [R] Task View for ecology/environmental science
In-Reply-To: <2c75873c050606020672efadac@mail.gmail.com>
References: <2c75873c050606020672efadac@mail.gmail.com>
Message-ID: <42A41D67.5030508@statistik.uni-dortmund.de>

Graham Smith wrote:

> Having just discovered "Task Views" I see there isn't one for
> ecology/environmental science.
> 
> I don't have the expertise to help with this, but I think it would be
> a very useful addition if there was any person (s) on the list willing
> to help create such a Task View.
> 
> I am sure the maintainers of the Task Views will pick up on any
> responses to this post from volunteers.
> 
> I personally would be very grateful, ands wouldn't mind contributing
> in some sort of administrative role, but lack the technical expertise
> to do the rest.
> 
> Graham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It's not hard, just read the corresponding article in the latest R News.

Uwe Ligges



From Ted.Harding at nessie.mcc.ac.uk  Mon Jun  6 11:48:36 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 06 Jun 2005 10:48:36 +0100 (BST)
Subject: [R] geometric mean regression
In-Reply-To: <20050603205254.85402.qmail@web52002.mail.yahoo.com>
Message-ID: <XFMail.050606102158.Ted.Harding@nessie.mcc.ac.uk>

On 03-Jun-05 Michael Grant wrote:
> 
> I presume the reference is to the 'geometric mean
> functional regression' or the 'line of organic
> correlation' or 'reduced major axis regression'.  If
> so, this is relatively easy alsmost trivial to
> implement in R.

This somewhat contentious method is indeed trivial to
implement in R. The idea is that if you plot the two
regression lines (y on x, x on y) on the same axes
(y vertical, x horizontal), the slope of the GMR is
the geometric mean of the slopes of these two lines.

Since the slope of the y-on-x line is Sxy/Sxx, and
the slope of the x-on-y line is Syy/Sxy, the GMR slope
is therefore sqrt(Syy/Sxx) = sd(y)/sd(x).

All three lines go through the same point, (mean(x),mean(y)).

> Maybe it's in a package, but I never looked.

It hardly needs a package!

> I worked from Helsel's description in his classic water
> resources statistics book. See Chapter 10 here: 
> 
> http://water.usgs.gov/pubs/twri/twri4a3/

The method goes back a lot further than suggested here.
It seems it was proposed in oceanography by H. Sverdrup
in 1916, and very influentially promoted by W.E. Ricker
(e.g. Jnl Fisheries Research Board of Canada, 1973,
vol. 30, 409-434).

> Now, if you are after confidence intervals or
> prediction intervals, I haven't found anything on that
> yet. Seems that I did something a couple of year ago
> by hacking some approximate residuals using the LOC
> line and the data, and then feeding that into the CL
> and PL equations for OLS. (Be advised that I'm not a
> statistician and did that in the spirit of 
> approximation--who knows? :O) )

The uncertainty properties, and indeed the interpretation,
of this method are elusive. You can, of course, resort to
whatever stochastic modelling you choose (including simulation
and bootstrap) to estimate the variability of the slope
sd(y)/sd(x) and of any predictions you may want to make.

However, the method shows its indeterminate side to the
extent that the relationship between y and x is loose rather
than tight.

At one extreme, where the correlation between x and y = 1,
the two regression lines (y on x and z on y) and the GMR
all coincide. No problem here.

At the other extreme, where there is no correlation, the
GMR method still gives you a definite answer (sd(y)/sd(x))
even though by normal standards there is no relationhip
between y and x. In the latter case, the slope of the
GMR depends solely on the two SDs, and we may well ask
what is being estimated here (apart from the ratio of
the SDs).

(Of course, if you go back to the "primitive" definition, you
find yourself evaluating sqrt(0 * inf), which is indeterminate;
and this is a better outcome than sd(y)/sd(x), but still falls
short of telling you directly that y is independent of x).

As you approach the r=0 situation, you therefore have to be
mindful that the GMR method will appear to provide a definite
answer to a question which in reality has at best a vague
answer, i.e. there is a major problem of interpretation.

Therefore I would be suspicious of results obtained by "blind"
application of the GMR method which were not accompanied by
a good discussion of grounds why the results can be expectd
to be meaningful in the particular case where it has been applied.

The GMR method seems to be well entrenched in the fisheries,
natural resources, and ecology worlds. I suspect that the reasons
for this may be partly "psychological": people are aware that
they are looking for a functional relationship, are put off
(rightly) by the existence of two regression lines, and are
not enthusiastic to tangle with the difficulties (including
the potential indeterminacy) of estimating a linear functional
relationship. The GMR provides a very simple escape route
which, in no doubt many cases, may give you as good a working
answer as you can expect.

Nevertheless, I'm inclined to the view that the linear functional
relationship is usually the best way to go. When the observed
(x,y) points depart from the "true" points on the straight line
by normally distributed amounts, the MLE of the relationship
is well defined provided the ratio of the "departure" variances
is fixed. Therefore it is possible to examine the robustness
of the estimated relationship with respect to variation in the
assumed value of this ratio. To the extent that this is 
acceptably robust within plausible variation of the ratio,
you have an adequate and reliable perspective. Otherwise,
you have to acknowledge that your information is inadquate.

The danger of adopting a formulaic solution like GMY is that
it tends to conceal inadequacy of information!

Best wishes,
Ted.

> By coincidence I've been looking at this again
> recently. Maybe bootstrapping....
> 
> Regards,
> Michael Grant
> 
> --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com>
> wrote:
> 
>> Poizot Emmanuel wrote:
>> 
>> > Hi,
>> >
>> > is it possible to perform a geometric mean
>> regression with R ?
>> > Thanks.
>> >
>> As has been said on this list before, "This is R,
>> there is no if, only 
>> how",
>> 
>> but if you actually wanted to ask how it is
>> possible, it would help if
>> you explained what is "geometric mean regression".
>> 
>> Kjetil
>> 
>> > ------------------------------------------------
>> > Emmanuel Poizot
>> > Cnam/Intechmer
>> > B.P. 324
>> > 50103 Cherbourg Cedex
>> >
>> > Phone (Direct) : (00 33)(0)233887342
>> > Fax : (00 33)(0)233887339
>> > ------------------------------------------------
>> >
>>
>>------------------------------------------------------------------------
>> >
>> >______________________________________________
>> >R-help at stat.math.ethz.ch mailing list
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>> >
>>
>>------------------------------------------------------------------------
>> >
>> >No virus found in this incoming message.
>> >Checked by AVG Anti-Virus.
>> >Version: 7.0.322 / Virus Database: 267.4.0 -
>> Release Date: 01/06/2005
>> >  
>> >
>> 
>> 
>> -- 
>> 
>> Kjetil Halvorsen.
>> 
>> Peace is the most effective weapon of mass
>> construction.
>>                --  Mahdi Elmandjra
>> 
>> 
>> 
>> 
>> -- 
>> No virus found in this outgoing message.
>> Checked by AVG Anti-Virus.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 06-Jun-05                                       Time: 10:20:01
------------------------------ XFMail ------------------------------



From schween at snafu.de  Mon Jun  6 11:58:38 2005
From: schween at snafu.de (Sven C. Koehler)
Date: Mon, 6 Jun 2005 11:58:38 +0200
Subject: [R] #R IRC channel on freenode
Message-ID: <20050606095838.GM30133@boing.buug.de>

Hello!

Someone pointed out that it's not well publicized.  There is an #R
channel on irc.freenode.net:6667, currently with about five users.  You
are welcome to join.

Best regards,

Sven



From Alfredo at ull.es  Mon Jun  6 12:14:30 2005
From: Alfredo at ull.es (Alfredo@ull.es)
Date: Mon,  6 Jun 2005 11:14:30 +0100
Subject: [R] post hoc kruskal wallis
Message-ID: <1118052870.42a422066d576@correoweb.ccti.ull.es>




I??m looking for a program with a post hoc kruskall Wallis test like the Tukey-
type non parametric test of ZAR(Biostatistical Analysis 2??Edition). SSPS 
hasn??t got any non parametric post host test. I don??t know if R has an 
appropiate post hoc non parametric test. I??m not sure if NDWD test of the 
package ?coin? is an appropiate test or if the package ?npmc? is a best option.
Thank you



Alfredo M. Berm??dez Ferrer
Dpto. Ecolog??a. Facultad de Biolog??a
Avda. Astrof??sco Fco. S??nchez s/n
Universidad de Laguna
Tenerife - Spain



From p.dalgaard at biostat.ku.dk  Mon Jun  6 12:16:37 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 12:16:37 +0200
Subject: [R] multiple comparison test
In-Reply-To: <20050606092048.19106.qmail@web26808.mail.ukl.yahoo.com>
References: <20050606092048.19106.qmail@web26808.mail.ukl.yahoo.com>
Message-ID: <x2mzq37ptm.fsf@turmalin.kubism.ku.dk>

Julie BERTRAND <juliebertrand2001 at yahoo.fr> writes:

> hello,
>  
> after an anova I use pairwise.t.test(), it gives only
> p.value and I want the t.stat.
> I try to get these by computing the Welch
> approximation of the degree of freedom and using the
> qt(p.value,df) function but when I test this method
> with t.test results (the function gives p.value and
> t.test), I doesn't find the same t.stat.
>  
> I also use the simtest(x~y,type="Tukey") function and
> it gives me only negative t.stat, resulting in a weard
> distribution...
>  
> thank you in advance for all the attention you 'll
> give to my problems.
> Julie BERTRAND

Notice that the p-values in pairwise.t.test() are adjusted for
multiple comparisons and use a pooled SD. You're not going to get
anything similar to the t.test output unless you set pool.sd=FALSE and
p.adjust.method="none". You probably also want to make the tests
one-sided, or halve the p-value.

It could well be easier just to modify the function to give you the
result that you desire...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From chris at psyctc.org  Mon Jun  6 12:46:28 2005
From: chris at psyctc.org (Chris Evans)
Date: Mon, 06 Jun 2005 11:46:28 +0100
Subject: [R] A long digression on packages
In-Reply-To: <185e9e0739088327043d1f12e0b2e1c0@oulu.fi>
References: <42A3027A.9000007@stats.uwo.ca>
Message-ID: <42A43794.15246.170B9626@localhost>

On 5 Jun 2005 at 18:44, Jari Oksanen wrote:

> There are diverse opinions about netiquette. One of the most basic, in
> my opinion, is this: if someone posts starts a discussion in a certain
> forum, you shall not divert it to another forum where it may be hidden
> by most readers, perhaps even by the originator of the thread.

With the greatest of respect for Duncan and the R-devel list, I think 
Jari has a point here.  This is one of the most important issues I've 
seen raised on this list (R-help) in recent months and I think it may 
be a structural problem for the development of R, in common with that 
of much FLOSS s'ware, that there's a separation of users and authors 
that needs thought.  There are no perfect answers but too big a 
separation and projects go "techno" and it's hard for those of us who 
can't code C and who are mere "users" to help those outstanding 
people on whom we depend hear what we need: sometimes they are so 
clever, so specialised in their knowledge, or simply in the realm of 
genius not the ordinary, that they can't see our problem.  I have 
slowly come to respect that a pretty brusque style from our 
authorities is the only way to prevent this list being a madhouse but 
I think that Jim's point may fall into that class that is worth some 
duplicate bandwidth here.

I know I've found the problem Jim highlights very confusing and 
unhelpful at times.  Views, which I didn't know, seem helpful but not 
a real solution to the key problem: they may tangentially help by 
ensuring that if your needs fit into a view, it becomes more likely 
that you'll install the packages you need and a local search may tell 
you what you need.  I've taken the inefficient route which suits me 
of installing just about every package to make it less likely I'll 
miss something of use to me. That means my search for "kappa" and 
"Cohen" (with ignore.case=FALSE) turns up at least three 
implementations of aspects of Cohen's kappa.

It may already exist, but a web interface that did a help.search over 
all the packages in the current release version would be great.  (If 
it does exist, sorry, but I'm no dunce and use R nearly every day and 
try to read much of r-help every day and don't know it, which may say 
something!)

I think there may be a need for some R improvement and automated 
updating of what I think is Frank Harrell's function finder:
	http://biostat.mc.vanderbilt.edu/s/finder/finder.html
Though I'm not absolutely sure how fitting your works into something 
like that could be imposed on developers!

Another thing that might help would be for a system by which ordinary 
users would volunteer to pair up with developers for packages and try 
to suggest adaptations of the help and such like that might make the 
packages more user friendly.  I wouldn't want to do that for the 
whole of a huge and vital package like MASS or Hmisc (or base or 
stats!) but I'm up for pairing with a developer on a smaller package 
if anyone thinks that would be helpful.

Thoughts for what they're worth.  Thanks a million to all developers 
... asbestos suit on!

Chris
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From baron at psych.upenn.edu  Mon Jun  6 13:02:44 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 6 Jun 2005 07:02:44 -0400
Subject: [R] A long digression on packages
In-Reply-To: <42A43794.15246.170B9626@localhost>
References: <42A3027A.9000007@stats.uwo.ca> <42A43794.15246.170B9626@localhost>
Message-ID: <20050606110244.GA4782@psych>

I haven't followed this thread, but the web interface may exist.
(Perhaps "help.search()" does something that Namazu doesn't do,
but I don't think so.)  See my .sig below.

This is where you get if you click on "Search" in the R home
page.

On 06/06/05 11:46, Chris Evans wrote:
 On 5 Jun 2005 at 18:44, Jari Oksanen wrote:
 
 It may already exist, but a web interface that did a help.search over
 all the packages in the current release version would be great.  (If
 it does exist, sorry, but I'm no dunce and use R nearly every day and
 try to read much of r-help every day and don't know it, which may say
 something!)
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From murdoch at stats.uwo.ca  Mon Jun  6 13:09:23 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Jun 2005 07:09:23 -0400
Subject: [R] segmentation fault - debugging
In-Reply-To: <1253d67a0506052048a82fd@mail.gmail.com>
References: <1253d67a0506052048a82fd@mail.gmail.com>
Message-ID: <42A42EE3.60307@stats.uwo.ca>

Joel Bremson wrote:
> Hi all,
> 
> I'm trying to debug some fortran 95 code that I'm bringing in
> with a dyn.load().
> 
> I'm compiling a number of files using g95 on intel linux w/ R 2.1.0.
> 
> The .so file loads without complaint, but when I try to call it I get a
> seg fault. I was hoping I could get a core dump in order to get some
> more clues about what is going on, but no luck.
> 
> I'm relatively new to fortran programming and have never used gdb before, 
> although
> I'm familiar with other debuggers.
> 
> Any tips on how to proceed from this point would be appreciated.

I find insight (a TCL/TK front end to gdb) easier to use than plain gdb, 
because it's more like other GUI-based debuggers.  I've also heard that 
ddb (I think that's the name) does a good job, but was never able to get 
it to run in Windows.  You might have more luck in Linux.

If you run R under the debugger, it will try to stop at the line that 
caused the seg fault.  This depends on the presence of debug info in the 
file, which might be your .so or R itself.  In Windows, "make distclean; 
make DEBUG=T" gets debug info into R; I don't know if that works for 
builds on other platforms.

Duncan Murdoch



From dmb at mrc-dunn.cam.ac.uk  Mon Jun  6 13:09:44 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 6 Jun 2005 12:09:44 +0100 (BST)
Subject: [R] Strange characters in 2.1.0?
Message-ID: <Pine.LNX.4.21.0506061203300.29150-100000@mail.mrc-dunn.cam.ac.uk>


Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 

Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
<80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1

Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 

hmm... they go away when I paste them in...



From R.P.Clement at westminster.ac.uk  Mon Jun  6 17:34:26 2005
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: 06 Jun 2005 10:34:26 -0500
Subject: [R] Uses of isoMDS()
In-Reply-To: <Pine.LNX.4.61.0506061031060.1374@gannet.stats>
References: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>
	<1117910982.4261.61.camel@horizons.localdomain>
	<1118064509.8738.16.camel@staff-pc01.harrowscs.westminster.ac.uk>
	<Pine.LNX.4.61.0506061031060.1374@gannet.stats>
Message-ID: <1118072066.8738.55.camel@staff-pc01.harrowscs.westminster.ac.uk>

Thanks to all for both the public and private answers on this topic. It
looks like from your answers that I'll have to wait for "Modern Applied
Statistics with S" to arrive at my local library (already requested)
before there's a chance that things will become clear.

As for C/R hybrids, if things get messy if I do them "properly", I'll
stick with what I'm doing.

Cheers,

Ross-c



From chris at psyctc.org  Mon Jun  6 13:25:10 2005
From: chris at psyctc.org (Chris Evans)
Date: Mon, 06 Jun 2005 12:25:10 +0100
Subject: [R] A long digression on packages
In-Reply-To: <20050606110244.GA4782@psych>
References: <42A43794.15246.170B9626@localhost>
Message-ID: <42A440A6.23489.172F06D7@localhost>

On 6 Jun 2005 at 7:02, Jonathan Baron wrote:

> I haven't followed this thread, but the web interface may exist.
> (Perhaps "help.search()" does something that Namazu doesn't do,
> but I don't think so.)  See my .sig below.
> 
> This is where you get if you click on "Search" in the R home
> page.
Not quite: that's wonderful and I use it a lot but it throws up much 
more from r-project than just the search of the current release 
packages would in help.search() so it can give you much more than you 
need ... if there are tuning parameters one can add that will do the 
necessary, and I'm sure there are, then I'd love to see then and 
ideally see another search box that applied them for us! 

But thanks Jonathan!

Chris
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From jarioksa at sun3.oulu.fi  Mon Jun  6 13:38:40 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Mon, 06 Jun 2005 14:38:40 +0300
Subject: [R] Uses of isoMDS()
In-Reply-To: <1118072066.8738.55.camel@staff-pc01.harrowscs.westminster.ac.uk>
References: <Pine.LNX.4.21.0506041536130.12482-100000@mail.mrc-dunn.cam.ac.uk>
	<1117910982.4261.61.camel@horizons.localdomain>
	<1118064509.8738.16.camel@staff-pc01.harrowscs.westminster.ac.uk>
	<Pine.LNX.4.61.0506061031060.1374@gannet.stats>
	<1118072066.8738.55.camel@staff-pc01.harrowscs.westminster.ac.uk>
Message-ID: <1118057920.5143.3.camel@biol102145.oulu.fi>

On Mon, 2005-06-06 at 10:34 -0500, Ross Clement wrote:
> Thanks to all for both the public and private answers on this topic. It
> looks like from your answers that I'll have to wait for "Modern Applied
> Statistics with S" to arrive at my local library (already requested)
> before there's a chance that things will become clear.
> 
And take care that Kruskal, too, is in your local library when MASS
arrives: the book does not tell you which of Kruskal's alternative
stresses is used, but you got to compare the equation against Kruskal's.

cheers, jari oksanen



From v.demartino2 at virgilio.it  Mon Jun  6 13:55:38 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Mon, 6 Jun 2005 13:55:38 +0200
Subject: [R] Unable to compile R-2.1.0
Message-ID: <429C8F5C0000907B@ims3e.cp.tin.it>

I downloaded the latest R-2.1.0 tarball from cran (the one of 18/4/05) to
compile it under FreeBSD. Take into account that I compiled R-2.0.1 in the
same machine and OS like a charme, flawlessly and at the very fiirst shot.

Now with R-2.1.0, ./configure doesn't seem to say anything alarming but
make stops. Here an extract of both ./configure and make:

# ./configure
..................................
R is now configured for i386-unknown-freebsd5.4

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc -D__NO_MATH_INLINES -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          f77  -g -O2

  Interfaces supported:      X11, tcltk
  External libraries:        readline, BLAS(ATLAS)
  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
  Options enabled:           R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build info or html versions of the R manuals


#make
<SNIP>
NLINES  -g -O2 -c util.c -o util.o
gcc -I../../src/extra/zlib -I../../src/extra/bzip2 -I../../src/extra/pcre
  -I. -I../../src/include -I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
-D__NO_MATH_INLINES  -g -O2 -c version.c -o version.o
gcc -I../../src/extra/zlib -I../../src/extra/bzip2 -I../../src/extra/pcre
  -I. -I../../src/include -I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
-D__NO_MATH_INLINES  -g -O2 -c vfonts.c -o vfonts.o
f77   -g -O2 -c xxxpr.f -o xxxpr.o
gcc -export-dynamic -L/usr/local/lib -o R.bin  Rmain.o  CConverters.o CommandLineArgs.o
Rdynload.o Renviron.o RNG.o apply.o arithmetic.o apse.o array.o attrib.o
base.o bind.o builtin.o character.o coerce.o colors.o complex.o connections.o
context.o cov.o cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o
dounzip.o dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o
fourier.o gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o
lapack.o list.o logic.o main.o mapply.o match.o memory.o model.o names.o
objects.o optim.o optimize.o options.o par.o paste.o pcre.o platform.o plot.o
plot3d.o plotmath.o print.o printarray.o printvector.o printutils.o qsort.o
random.o regex.o registration.o relop.o saveload.o scan.o seq.o serialize.o
size.o sort.o source.o split.o sprintf.o startup.o subassign.o subscript.o
subset.o summary.o sysutils.o unique.o util.o version.o vfonts.o xxxpr.o
../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a  -lf77blas -latlas
-lg2c -lm  ../extra/zlib/libz.a ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a
/usr/local/lib/libintl.so -Wl,-rpath -Wl,/usr/local/lib -lreadline -lm -liconv
errors.o(.text+0x154b): In function `do_gettext':
/tmp/R-2.1.0/src/main/errors.c:779: undefined reference to `__builtin_alloca'
errors.o(.text+0x15fa):/tmp/R-2.1.0/src/main/errors.c:752: undefined reference
to `__builtin_alloca'
errors.o(.text+0x1652):/tmp/R-2.1.0/src/main/errors.c:760: undefined reference
to `__builtin_alloca'
errors.o(.text+0x16aa):/tmp/R-2.1.0/src/main/errors.c:770: undefined reference
to `__builtin_alloca'
errors.o(.text+0x1728):/tmp/R-2.1.0/src/main/errors.c:738: undefined reference
to `__builtin_alloca'
errors.o(.text+0x274d):/tmp/R-2.1.0/src/main/errors.c:829: more undefined
references to `__builtin_alloca' follow
*** Error code 1

Stop in /tmp/R-2.1.0/src/main.
*** Error code 1

Stop in /tmp/R-2.1.0/src/main.
*** Error code 1

Stop in /tmp/R-2.1.0/src.
*** Error code 1

Stop in /tmp/R-2.1.0.


What's the matter with it and what should I do next?

VITTORIO



From abunn at whrc.org  Mon Jun  6 14:19:10 2005
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 6 Jun 2005 08:19:10 -0400
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <971536df050605105776fb1b9e@mail.gmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIMEBMDHAA.abunn@whrc.org>

> > 
> > R > w <- list(rnorm(10), rnorm(10))
> > R > x <- ts(w, start = 1980)
> 
> Even though you don't get an error message this statement is 
> erroneous.  ?ts discusses the valid possibilities.

So it does, might I suggest add something like this to ts:
    if (is.list(data)) 
       stop("Data must be a numeric vector or matrix")


Thanks all for the clarifications and discussion,
Andy



From francoisromain at free.fr  Mon Jun  6 14:32:25 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 06 Jun 2005 14:32:25 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
Message-ID: <42A44259.9040309@free.fr>

Hello all,

It seems that the next improvement to the R Graph Gallery is 
categorization of the graphics, that way each graph will be easier to 
find. That step should be done *carefully* if we want to avoid the 
opposite side-effect : graph not reachable through the categories.
That's why the wisdom of the R community is required.
Graphics will be classified in :
   - categories
   - sub-categories within those categories
So far so good.
The problem is the determination of those categories and sub-categories.
Any thoughts are welcome.

The gallery now has a TODO/WISHLIST : 
http://addictedtor.free.fr/graphiques/WISHLIST.php that waits for your 
accurate comments.

Regards.

Romain



-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From navarre_sabine at yahoo.fr  Mon Jun  6 14:46:01 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Mon, 6 Jun 2005 14:46:01 +0200 (CEST)
Subject: [R] Polar Graph
Message-ID: <20050606124601.73933.qmail@web26605.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/da4d4803/attachment.pl

From dmbates at gmail.com  Mon Jun  6 14:51:39 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 6 Jun 2005 07:51:39 -0500
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <Pine.LNX.4.21.0506061203300.29150-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506061203300.29150-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <40e66e0b05060605512e39815d@mail.gmail.com>

On 6/6/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
> 
> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
> 
> Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
> <80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
> 
> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
> 
> hmm... they go away when I paste them in...

Check the character set in use.  You probably are using a UTF-8
encoding in an environment that does not support display of that
character set.

A cheap way of avoiding this particular problem is 

options(show.signif.stars = FALSE)



From v.demartino2 at virgilio.it  Mon Jun  6 14:54:43 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Mon, 6 Jun 2005 14:54:43 +0200
Subject: [R] R code for performance
In-Reply-To: <Pine.LNX.4.61.0506060933570.26056@gannet.stats>
Message-ID: <429C8F5C00009431@ims3e.cp.tin.it>


:-- Messaggio originale --
:Date: Mon, 6 Jun 2005 09:40:40 +0100 (BST)
:From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
:To: v.demartino2 at virgilio.it
:cc: r-help <r-help at stat.math.ethz.ch>
:Subject: Re: [R] R code for performance
:
:
:On Mon, 6 Jun 2005 v.demartino2 at virgilio.it wrote:
:
:> At office I'm cautiously introducing R to be used as the basic statistical
:> program, getting rid of licensed stuff or reducing the amount of it.
:> The aim of R would be to run generic statistical programs built & "consumed"
:> when needed and some static procedure dealing with time-series.
:> Now, we have substantially 3 OS platforms, win xp, linux and freebsd
5.4,
:> on similar PCs (pentium 4, 2-2.5 GHz). I have been asked by the boss
to
:> test the "average" performance (in term of speed and memory use) of R
on
:> each of this platform to stick with one of them on a couple of PCs.
:>
:> Could you please suggest an R source code (apart from the "static procedure"
:> I will obviously test) to be run on the three platforms to test performance?
:>
:> If there is nothing of the kind, any suggestion?
:
:'make check' runs a lot of R code and times it.  The tests for the stats
:
:package look most relevant to you.  Beware of simplistic 'benchmarks' that
:
:test code snippets not relevant to your usage (and that may apply to the
:R 
:examples which tend to be small datasets).
:
:We know Linux (non-R-shlib) outperforms Windows XP by ca 20%, and some

:comments I have seen here suggest it outperforms FreeBSD as well.  But
are
:
:such differences enough to determine your choice?
:

Thinking of my time-series procedure I would answer NO definitely. 
But the fact is that we have to do lots of simulations most of them using
Montecarlo with many iterations, spending our time in modifying and trying
different hypoteses . And the problem is that these simulations must be
done "on the fly" because, as we ironically say, they're needed "for yesterday".
Therefore there's no much time left to refine the R code and get the best
out of it. So, in this case, having a Ferrari makes the difference!

Ciao
Vittorio



From rpeng at jhsph.edu  Mon Jun  6 14:56:18 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 06 Jun 2005 08:56:18 -0400
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <Pine.LNX.4.21.0506061203300.29150-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506061203300.29150-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <42A447F2.60206@jhsph.edu>

What is your operating system?

-roger

Dan Bolser wrote:
> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 
> 
> Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
> <80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
> 
> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 
> 
> hmm... they go away when I paste them in...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeaneid at chass.utoronto.ca  Mon Jun  6 15:26:14 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Mon, 6 Jun 2005 09:26:14 -0400
Subject: [R] write.dta limits
Message-ID: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>

Hope everyone id doing great ..

Just need some clarification over the limit of write.dta. I have some
coauthors that use stata and I need to send them my data in .dta format.
the data.frame is 41706x229 and I get the following

Error in write.dta(Panel, file = "STATADATA/Panel.dta", version = 7) :
	a binary write error occured


Once I subset the data everything works out fine. my question is what are
the limits of write.dta. I tried to find out but no luck..


Thank you

Jean



From jeaneid at chass.utoronto.ca  Mon Jun  6 15:29:05 2005
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Mon, 6 Jun 2005 09:29:05 -0400
Subject: [R] write.dta limits
In-Reply-To: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.SGI.4.40.0506060926390.5569156-100000@origin.chass.utoronto.ca>

did not clarify the system and such ...

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

The foreign package version is  0.8-4





On Mon, 6 Jun 2005, Jean Eid wrote:

> Hope everyone id doing great ..
>
> Just need some clarification over the limit of write.dta. I have some
> coauthors that use stata and I need to send them my data in .dta format.
> the data.frame is 41706x229 and I get the following
>
> Error in write.dta(Panel, file = "STATADATA/Panel.dta", version = 7) :
> 	a binary write error occured
>
>
> Once I subset the data everything works out fine. my question is what are
> the limits of write.dta. I tried to find out but no luck..
>
>
> Thank you
>
> Jean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Mon Jun  6 15:17:20 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Jun 2005 09:17:20 -0400
Subject: [R] A long digression on packages
In-Reply-To: <42A43794.15246.170B9626@localhost>
References: <42A3027A.9000007@stats.uwo.ca> <42A43794.15246.170B9626@localhost>
Message-ID: <42A44CE0.9020700@stats.uwo.ca>

On 6/6/2005 6:46 AM, Chris Evans wrote:
> On 5 Jun 2005 at 18:44, Jari Oksanen wrote:
> 
>> There are diverse opinions about netiquette. One of the most basic, in
>> my opinion, is this: if someone posts starts a discussion in a certain
>> forum, you shall not divert it to another forum where it may be hidden
>> by most readers, perhaps even by the originator of the thread.
> 
> With the greatest of respect for Duncan and the R-devel list, I think 
> Jari has a point here.  This is one of the most important issues I've 
> seen raised on this list (R-help) in recent months and I think it may 
> be a structural problem for the development of R, in common with that 
> of much FLOSS s'ware, that there's a separation of users and authors 
> that needs thought. 

I don't think the existence of two lists implies separate populations of 
users versus authors.  In fact, I suspect most R users are authors and 
vice versa (though some authors publish more than others).  The point is 
that when discussing something about the development of R, it makes more 
sense to do it in the R development list.  When asking for help on how 
to use R, it makes more sense to do it here.

If in a few month's time, I want to look up a vaguely remembered 
discussion about improvements to the package help system, I'll look for 
it in the R-devel archives.  If it took place here, I'd miss it.

> It may already exist, but a web interface that did a help.search over 
> all the packages in the current release version would be great.  (If 
> it does exist, sorry, but I'm no dunce and use R nearly every day and 
> try to read much of r-help every day and don't know it, which may say 
> something!)

I don't think a search that is restricted in exactly that way currently 
exists, but a Google search like

  kappa site:http://cran.r-project.org/src/contrib/Descriptions

might be useful (if you're searching for kappa!).

Duncan Murdoch



From rkoenker at uiuc.edu  Mon Jun  6 15:23:23 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 6 Jun 2005 08:23:23 -0500
Subject: [R] make install on solaris 10
Message-ID: <72A6CB57-4A44-4AF6-A9A6-B87073D53C76@uiuc.edu>

We have recently upgraded to Solaris 10 on a couple of sparc machines  
with the usual
mildly mysterious consequences for library locations, etc, etc.  I've  
managed to configure
R 2.1.0 for a 64 bit version with:

R is now configured for sparc-sun-solaris2.10

   Source directory:          .
   Installation directory:    /usr/local

   C compiler:                gcc -m64 -g -O2
   C++ compiler:              g++  -m64 -fPIC
   Fortran compiler:          g77  -m64 -g -O2

   Interfaces supported:      X11
   External libraries:        readline
   Additional capabilities:   PNG, JPEG, MBCS, NLS
   Options enabled:           R profiling

   Recommended packages:      yes

configure:47559: WARNING: you cannot build info or html versions of  
the R manuals

and make and make check seem to run smoothly, however "make install"  
dies with
the following messages:

ysidro.econ.uiuc.edu# make install
installing doc ...
creating doc/html/resources.html
*** Error code 255
The following command caused the error:
false --html --no-split --no-headers ./resources.texi -o ../html/ 
resources.html
make: Fatal error: Command failed for target `../html/resources.html'
Current working directory /usr/local/encap/R-2.1.0/doc/manual
installing doc/html ...
installing doc/html/search ...
/usr/local/bin/install: resources.html: No such file or directory
*** Error code 1
The following command caused the error:
for f in resources.html; do \
   /usr/local/bin/install -c -m 644 ${f} "/usr/local/lib/R/doc/html"; \
done
make: Fatal error: Command failed for target `install'
Current working directory /usr/local/encap/R-2.1.0/doc/html
*** Error code 1
The following command caused the error:
for d in html manual; do \
   (cd ${d} && make install) || exit 1; \
done
make: Fatal error: Command failed for target `install'
Current working directory /usr/local/encap/R-2.1.0/doc
*** Error code 1
The following command caused the error:
for d in m4 tools doc etc share src po tests; do \
   (cd ${d} && make install) || exit 1; \
done
make: Fatal error: Command failed for target `install'

and running R from the bin directory gives:

 > capabilities()
     jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
    FALSE    FALSE    FALSE    FALSE     TRUE     TRUE     TRUE     TRUE
   cledit  IEEE754    iconv
     TRUE     TRUE    FALSE

Any suggestions would be greatly appreciated.  With solaris 9 we had  
a 64 bit build
but never encounter such problems, and I don't see anything in the  
archives or the
install manual that is relevant -- but of course, I'm not very clear  
about what I'm looking
for either.

Roger


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820



From chris at psyctc.org  Mon Jun  6 15:30:16 2005
From: chris at psyctc.org (Chris Evans)
Date: Mon, 06 Jun 2005 14:30:16 +0100
Subject: [R] A long digression on packages
In-Reply-To: <20050606123833.GA10810@psych>
References: <42A44626.15546.174480BF@localhost>
Message-ID: <42A45DF8.29123.17A18BB9@localhost>

On 6 Jun 2005 at 8:38, Jonathan Baron wrote:

> So use my search engine and unclick all the options except for
> "functions"?  Do I need a different term?
No, I'm being an idiot (as I suspected!) and had looked through your 
particular search interface and jumped to the big CRAN one.  Replying 
this to R-help list for public helping of humble porridge and in case 
anyone else is making same mistake.  Thanks for a great search 
facility!
 
> BTW, there is a package called ltm for IRT, but my search is not
> picking it up.  I think there is a problem!  (It may be fixed by
> the time you read this.)
Yup: ltm has got some of what I want but not all yet!!

All power to you on your search interface and thanks again.

C
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From Dubravko.Dolic at komdat.com  Mon Jun  6 15:30:31 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Mon, 6 Jun 2005 15:30:31 +0200
Subject: [R] Reading huge chunks of data from MySQL into Windows R
Message-ID: <52D1AC81378E9342947189B04176014725512D@agentsmith.komdat.intern>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/b2759f03/attachment.pl

From csad4933 at uibk.ac.at  Mon Jun  6 15:34:16 2005
From: csad4933 at uibk.ac.at (Sebastian Schoenherr)
Date: Mon,  6 Jun 2005 15:34:16 +0200
Subject: [R] Re: R-help Digest, Vol 28, Issue 6
In-Reply-To: <200506061038.j56AQtAd007636@hypatia.math.ethz.ch>
References: <200506061038.j56AQtAd007636@hypatia.math.ethz.ch>
Message-ID: <1118064856.42a450d865d1e@web-mail1.uibk.ac.at>

Dear all,
i have a question as regards to the read.table /table command:

I read in the following file:
Method1 23
Method2 12
Method3 43
Method4 76
Method1 2
Method3 4

If i use the "table" command I get the following matrix:
         V2
V1         2 4 12 23 43 76
  Method1  1 0  0  1  0  0
  Method2  0 0  1  0  0  0
  Method3  0 1  0  0  1  0
  Method4  0 0  0  0  0  1


Is it possible that R groups my values, to get an matrix which looks nearly
like this:
V1
  Method1 23 2
  Method2 12 0
  Method3 43 4
  Method4 76 0


Some ideas?

Thanks in advance

Cheers
Sebastian



From shigesong at gmail.com  Mon Jun  6 15:38:54 2005
From: shigesong at gmail.com (Shige Song)
Date: Mon, 6 Jun 2005 21:38:54 +0800
Subject: [R] simplified Chinese and traditional Chinese translation for R
	manuals
In-Reply-To: <42A3C527.4000408@turing.une.edu.au>
References: <f04a1d1d050605192044144ec1@mail.gmail.com>
	<42A3C527.4000408@turing.une.edu.au>
Message-ID: <5abc11d805060606386dba8a7a@mail.gmail.com>

Thanks for the excellent work!

Shige

On 6/6/05, Yuandan Zhang <yzhang4 at turing.une.edu.au> wrote:
> Guohui,
> 
> Great effort for translating this document into Chinese. I will
> certainly read through and send some feedback to you.
> 
> Best
> 
> Yuandan
> 
> Guohui Ding wrote:
> 
> >Hi, every one,
> >I have translated <An Introduction to R > into simplified Chinese and
> >traditional Chinese. You can browse them from these two URL:
> >simplified Chinese:
> >http://www.biosino.org/pages/newhtm/r/schtml/index.html#Top
> >traditional Chinese: http://www.biosino.org/pages/newhtm/r/tchtml/
> >
> >I have distributed these document in HTML format. That is because there is
> >something wrong with the format change between the PDF format and texinfo
> >format in Chinese. However, I will overcome this problem soon. If some
> >website on R have some interest to these Chinese translation, you can get
> >all the HTML document of them from me.
> >
> >The other four manuals is being translated now, and I will distribute them
> >in the future.
> >The Chinese translation for <An Introduction to R> is a beta version, and
> >will need some modification and your suggestion. More information was in the
> >document.
> >
> >R is a great sofeware. I like it and I will do something for it.
> >Wish You Will Enjoy It!
> >
> >You can send a email to me. Any suggestion and comment are welcome!
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmb at mrc-dunn.cam.ac.uk  Mon Jun  6 15:40:33 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 6 Jun 2005 14:40:33 +0100 (BST)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A44259.9040309@free.fr>
Message-ID: <Pine.LNX.4.21.0506061437300.31061-100000@mail.mrc-dunn.cam.ac.uk>

On Mon, 6 Jun 2005, Romain Francois wrote:

>Hello all,
>
>It seems that the next improvement to the R Graph Gallery is 
>categorization of the graphics, that way each graph will be easier to 
>find. That step should be done *carefully* if we want to avoid the 
>opposite side-effect : graph not reachable through the categories.
>That's why the wisdom of the R community is required.
>Graphics will be classified in :
>   - categories
>   - sub-categories within those categories
>So far so good.
>The problem is the determination of those categories and sub-categories.
>Any thoughts are welcome.
>
>The gallery now has a TODO/WISHLIST : 
>http://addictedtor.free.fr/graphiques/WISHLIST.php that waits for your 
>accurate comments.

I would humbly suggest that the underlying data (graphs/categories/etc) be
organized in such a way as to allow multiple categorizations. That way
there could be any number of different categorizations applied to the same
graphs, as one overall categorization may reflect only one particular
viewpoint.

This way the chosen categories (however you decide to choose) would not be
set in stone, but would be intimatly flexible.

Also a single graph should be allowed to occur under multiple categories
of a particular categorization.

Just what I would like to see :)

Dan.

>
>Regards.
>
>Romain
>
>
>
>



From dmb at mrc-dunn.cam.ac.uk  Mon Jun  6 15:41:29 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 6 Jun 2005 14:41:29 +0100 (BST)
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <42A447F2.60206@jhsph.edu>
Message-ID: <Pine.LNX.4.21.0506061441040.31061-100000@mail.mrc-dunn.cam.ac.uk>

On Mon, 6 Jun 2005, Roger D. Peng wrote:

>What is your operating system?

Very sorry for lack of details...

I am RH 9, unix pc i386.



>
>-roger
>
>Dan Bolser wrote:
>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 
>> 
>> Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
>> <80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
>> 
>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 
>> 
>> hmm... they go away when I paste them in...
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>



From mendes150 at gmail.com  Mon Jun  6 15:43:39 2005
From: mendes150 at gmail.com (Richard Mendes)
Date: Mon, 6 Jun 2005 15:43:39 +0200
Subject: [R] chisq.test and anova problems
Message-ID: <b2381081050606064332a54d9c@mail.gmail.com>

we just started to use R and having some problems that no one in our
school could solve. I hope someone here can help me out.

the first problem is with the chisquare test. we want to exclude the
missing values from the data. we used na.omit and made two new
variables.

now we want to use the chi square method but get the error x and y
must have the same length.

how do i use the chisquare method were i exclude the missing values ?
and can i use this method if there is a difference in length.

the second problem is with anova in the data set we are working on we
have to use this method on multiple variables with a difference in
length, can this be done. this is the syntax we used and the error is
stated behind.

anova(lm(test~test1)) and the error states variable length differ.

i think there has to be a way to use this method on differences in
variable lengths does anybode know how to do this

thanks in advance for your help

richard



From dmb at mrc-dunn.cam.ac.uk  Mon Jun  6 15:48:50 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 6 Jun 2005 14:48:50 +0100 (BST)
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <40e66e0b05060605512e39815d@mail.gmail.com>
Message-ID: <Pine.LNX.4.21.0506061444260.31061-100000@mail.mrc-dunn.cam.ac.uk>

On Mon, 6 Jun 2005, Douglas Bates wrote:

>On 6/6/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
>> 
>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
>> 
>> Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
>> <80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
>> 
>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
>> 
>> hmm... they go away when I paste them in...
>
>Check the character set in use.  You probably are using a UTF-8
>encoding in an environment that does not support display of that
>character set.

I feel it is something utf8-ish, I am using emacs-ess-5.2.3, which was
working fine under R-2.0.0, but developed this problem under R-2.1.0

>A cheap way of avoiding this particular problem is 
>
>options(show.signif.stars = FALSE)

The problem also affects directory/file names, so it breaks stuff in other
ways.

I have gone back to 2.0.0 :)



>



From murdoch at stats.uwo.ca  Mon Jun  6 15:49:22 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Jun 2005 09:49:22 -0400
Subject: [R] Reading huge chunks of data from MySQL into Windows R
In-Reply-To: <52D1AC81378E9342947189B04176014725512D@agentsmith.komdat.intern>
References: <52D1AC81378E9342947189B04176014725512D@agentsmith.komdat.intern>
Message-ID: <42A45462.1000101@stats.uwo.ca>

On 6/6/2005 9:30 AM, Dubravko Dolic wrote:
> Dear List,
> 
>  
> 
> I'm trying to use R under Windows on a huge database in MySQL via ODBC
> (technical reasons for this...). Now I want to read tables with some
> 160.000.000 entries into R. I would be lucky if anyone out there has
> some good hints what to consider concerning memory management. I'm not
> sure about the best methods reading such huge files into R. for the
> moment I spilt the whole table into readable parts stick them together
> in R again. 
> 
>  
> 
> Any hints welcome.

Most values in R are stored in 8 byte doubles, so 160,000,000 entries 
will take roughly a gigabyte of storage.  (Half that if they are 
integers or factors.) You are likely to run into problems manipulating 
something that big in Windows, because users are normally only allowed 2 
GB of the memory address space, and it can be fragmented.

I'd suggest developing algorithms that can work on the data a block at a 
time, so that you never need to stick the whole thing together in R at 
once.  Alternatively, switch to a 64 bit platform and install lots of 
memory -- but there are still various 4 GB limits in R, so you may still 
run into trouble.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Mon Jun  6 15:51:35 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 14:51:35 +0100 (BST)
Subject: [R] Unable to compile R-2.1.0
In-Reply-To: <429C8F5C0000907B@ims3e.cp.tin.it>
References: <429C8F5C0000907B@ims3e.cp.tin.it>
Message-ID: <Pine.LNX.4.61.0506061449310.24586@gannet.stats>

This has been reported to R-bugs twice already (and also on the lists): 
it seems autoconf and FreeBSD are making different assumptions.

If you look in today's R-devel archive you will see the requests for 
information I set on the second report.

It seems no one using FreeBSD ever tested the alphas or betas of R 2.1.0.

On Mon, 6 Jun 2005 v.demartino2 at virgilio.it wrote:

> I downloaded the latest R-2.1.0 tarball from cran (the one of 18/4/05) to
> compile it under FreeBSD. Take into account that I compiled R-2.0.1 in the
> same machine and OS like a charme, flawlessly and at the very fiirst shot.
>
> Now with R-2.1.0, ./configure doesn't seem to say anything alarming but
> make stops. Here an extract of both ./configure and make:
>
> # ./configure
> ..................................
> R is now configured for i386-unknown-freebsd5.4
>
>  Source directory:          .
>  Installation directory:    /usr/local
>
>  C compiler:                gcc -D__NO_MATH_INLINES -g -O2
>  C++ compiler:              g++  -g -O2
>  Fortran compiler:          f77  -g -O2
>
>  Interfaces supported:      X11, tcltk
>  External libraries:        readline, BLAS(ATLAS)
>  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>  Options enabled:           R profiling
>
>  Recommended packages:      yes
>
> configure: WARNING: you cannot build info or html versions of the R manuals
>
>
> #make
> <SNIP>
> NLINES  -g -O2 -c util.c -o util.o
> gcc -I../../src/extra/zlib -I../../src/extra/bzip2 -I../../src/extra/pcre
>  -I. -I../../src/include -I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
> -D__NO_MATH_INLINES  -g -O2 -c version.c -o version.o
> gcc -I../../src/extra/zlib -I../../src/extra/bzip2 -I../../src/extra/pcre
>  -I. -I../../src/include -I../../src/include -I/usr/local/include -DHAVE_CONFIG_H
> -D__NO_MATH_INLINES  -g -O2 -c vfonts.c -o vfonts.o
> f77   -g -O2 -c xxxpr.f -o xxxpr.o
> gcc -export-dynamic -L/usr/local/lib -o R.bin  Rmain.o  CConverters.o CommandLineArgs.o
> Rdynload.o Renviron.o RNG.o apply.o arithmetic.o apse.o array.o attrib.o
> base.o bind.o builtin.o character.o coerce.o colors.o complex.o connections.o
> context.o cov.o cum.o dcf.o datetime.o debug.o deparse.o deriv.o dotcode.o
> dounzip.o dstruct.o duplicate.o engine.o envir.o errors.o eval.o format.o
> fourier.o gevents.o gram.o gram-ex.o graphics.o identical.o internet.o iosupport.o
> lapack.o list.o logic.o main.o mapply.o match.o memory.o model.o names.o
> objects.o optim.o optimize.o options.o par.o paste.o pcre.o platform.o plot.o
> plot3d.o plotmath.o print.o printarray.o printvector.o printutils.o qsort.o
> random.o regex.o registration.o relop.o saveload.o scan.o seq.o serialize.o
> size.o sort.o source.o split.o sprintf.o startup.o subassign.o subscript.o
> subset.o summary.o sysutils.o unique.o util.o version.o vfonts.o xxxpr.o
> ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a  -lf77blas -latlas
> -lg2c -lm  ../extra/zlib/libz.a ../extra/bzip2/libbz2.a ../extra/pcre/libpcre.a
> /usr/local/lib/libintl.so -Wl,-rpath -Wl,/usr/local/lib -lreadline -lm -liconv
> errors.o(.text+0x154b): In function `do_gettext':
> /tmp/R-2.1.0/src/main/errors.c:779: undefined reference to `__builtin_alloca'
> errors.o(.text+0x15fa):/tmp/R-2.1.0/src/main/errors.c:752: undefined reference
> to `__builtin_alloca'
> errors.o(.text+0x1652):/tmp/R-2.1.0/src/main/errors.c:760: undefined reference
> to `__builtin_alloca'
> errors.o(.text+0x16aa):/tmp/R-2.1.0/src/main/errors.c:770: undefined reference
> to `__builtin_alloca'
> errors.o(.text+0x1728):/tmp/R-2.1.0/src/main/errors.c:738: undefined reference
> to `__builtin_alloca'
> errors.o(.text+0x274d):/tmp/R-2.1.0/src/main/errors.c:829: more undefined
> references to `__builtin_alloca' follow
> *** Error code 1
>
> Stop in /tmp/R-2.1.0/src/main.
> *** Error code 1
>
> Stop in /tmp/R-2.1.0/src/main.
> *** Error code 1
>
> Stop in /tmp/R-2.1.0/src.
> *** Error code 1
>
> Stop in /tmp/R-2.1.0.
>
>
> What's the matter with it and what should I do next?
>
> VITTORIO
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Mon Jun  6 15:52:18 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 6 Jun 2005 10:52:18 -0300 (ADT)
Subject: [R] Missing values in argument of .Fortran.
Message-ID: <200506061352.j56DqIJt026716@erdos.math.unb.ca>

I wish to pass a vector ``y'', some of whose entries are NAs to a
fortran subroutine which I am dynamically loading and calling by
means of .Fortran().  The subroutine runs through the vector entry by
entry; obviously I want to have it do one thing if y[i] is present
and a different thing if it is missing.

The way I am thinking of proceeding is along the xlines of:

	ymiss <- is.na(y)
	rslt <- .Fortran(
		"foo",
		NAOK=TRUE,
		as.double(y),
		as.logical(ymiss),
		etc,
		etc
	)

and inside ``foo'' have a logical branch based on the value of
xmiss(i).

Questions:

	(1) Is there a sexier way to proceed?  E.g. is it possible
	within (g77) fortran to detect the fact that y(i) is/was an
	NA (or not) and make the nature of y(i) the basis of an
	if-statement?

	(2) Are there any lurking pitfalls in the use of the NAOK=TRUE
	argument?

	(3) Is there an entirely different and better way to proceed?

TIA.

			cheers,

				Rolf Turner
				rolf at math.unb.ca

P. S. I'm running R 2.0.1 under (Red Hat) Linux.  (Sigh.  Yes I must
get around to upgrading real soon now.)

				R. T.



From rpeng at jhsph.edu  Mon Jun  6 15:53:33 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 06 Jun 2005 09:53:33 -0400
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <Pine.LNX.4.21.0506061441040.31061-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506061441040.31061-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <42A4555D.50705@jhsph.edu>

It may be a problem with your terminal program.  Which one are 
you using?  Does it work if you try a different terminal program?

-roger

Dan Bolser wrote:
> On Mon, 6 Jun 2005, Roger D. Peng wrote:
> 
> 
>>What is your operating system?
> 
> 
> Very sorry for lack of details...
> 
> I am RH 9, unix pc i386.
> 
> 
> 
> 
>>-roger
>>
>>Dan Bolser wrote:
>>
>>>Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 
>>>
>>>Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
>>><80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
>>>
>>>Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1 
>>>
>>>hmm... they go away when I paste them in...
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ripley at stats.ox.ac.uk  Mon Jun  6 15:54:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 14:54:58 +0100 (BST)
Subject: [R] ts.intersect a multivariate and univariate ts
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIMEBMDHAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIMEBMDHAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.61.0506061453060.24586@gannet.stats>

On Mon, 6 Jun 2005, Andy Bunn wrote:

>>>
>>> R > w <- list(rnorm(10), rnorm(10))
>>> R > x <- ts(w, start = 1980)
>>
>> Even though you don't get an error message this statement is
>> erroneous.  ?ts discusses the valid possibilities.
>
> So it does, might I suggest add something like this to ts:
>    if (is.list(data))
>       stop("Data must be a numeric vector or matrix")

Actually, that is not quite the case: logical time series are allowed, 
even through not documented to be allowed.  (The S4 methods start-up code 
generates one, and so a similar check which was once there was removed.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Mon Jun  6 15:57:31 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 06 Jun 2005 14:57:31 +0100
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A44259.9040309@free.fr>
References: <42A44259.9040309@free.fr>
Message-ID: <42A4564B.90002@lancaster.ac.uk>

Romain Francois wrote:

> Graphics will be classified in :
>   - categories
>   - sub-categories within those categories
> So far so good.

  Maybe, maybe not! Would a system of keywords work better than strict 
hierarchical categories, as long as plots can have more than one keyword 
attached? Someone might be interested in all 3d plots, or all plots 
related to model fitting, and these categories may not be distinct!

Barry



From bill.shipley at usherbrooke.ca  Mon Jun  6 16:04:34 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 6 Jun 2005 10:04:34 -0400
Subject: [R] planned means and loess?
Message-ID: <001601c56aa0$aae9ae40$b01ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/be3fcd13/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun  6 16:14:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Jun 2005 15:14:38 +0100 (BST)
Subject: [R] make install on solaris 10
In-Reply-To: <72A6CB57-4A44-4AF6-A9A6-B87073D53C76@uiuc.edu>
References: <72A6CB57-4A44-4AF6-A9A6-B87073D53C76@uiuc.edu>
Message-ID: <Pine.LNX.4.61.0506061458540.24586@gannet.stats>

On Mon, 6 Jun 2005, roger koenker wrote:

> We have recently upgraded to Solaris 10 on a couple of sparc machines with 
> the usual
> mildly mysterious consequences for library locations, etc, etc.  I've managed 
> to configure
> R 2.1.0 for a 64 bit version with:
>
> R is now configured for sparc-sun-solaris2.10
>
>  Source directory:          .
>  Installation directory:    /usr/local
>
>  C compiler:                gcc -m64 -g -O2
>  C++ compiler:              g++  -m64 -fPIC
>  Fortran compiler:          g77  -m64 -g -O2
>
>  Interfaces supported:      X11
>  External libraries:        readline
>  Additional capabilities:   PNG, JPEG, MBCS, NLS
>  Options enabled:           R profiling
>
>  Recommended packages:      yes
>
> configure:47559: WARNING: you cannot build info or html versions of the R 
> manuals
>
> and make and make check seem to run smoothly, however "make install" dies 
> with
> the following messages:

As far as I can see something has deleted doc/html/resources.html: it is 
in the tarball. I cannot immediately guess what: have you done any sort of 
`make clean'?

Copying it from the virgin sources and doing `make install' again should 
fix this: if not perhaps you can keep an eye on what is apparently 
removing it.

BTW, where did /usr/local/bin/install come from?  If that is not doing 
what is expected, it could be the problem.

> ysidro.econ.uiuc.edu# make install
> installing doc ...
> creating doc/html/resources.html
> *** Error code 255
> The following command caused the error:
> false --html --no-split --no-headers ./resources.texi -o ../html/ 
> resources.html
> make: Fatal error: Command failed for target `../html/resources.html'
> Current working directory /usr/local/encap/R-2.1.0/doc/manual
> installing doc/html ...
> installing doc/html/search ...
> /usr/local/bin/install: resources.html: No such file or directory
> *** Error code 1
> The following command caused the error:
> for f in resources.html; do \
>  /usr/local/bin/install -c -m 644 ${f} "/usr/local/lib/R/doc/html"; \
> done
> make: Fatal error: Command failed for target `install'
> Current working directory /usr/local/encap/R-2.1.0/doc/html
> *** Error code 1
> The following command caused the error:
> for d in html manual; do \
>  (cd ${d} && make install) || exit 1; \
> done
> make: Fatal error: Command failed for target `install'
> Current working directory /usr/local/encap/R-2.1.0/doc
> *** Error code 1
> The following command caused the error:
> for d in m4 tools doc etc share src po tests; do \
>  (cd ${d} && make install) || exit 1; \
> done
> make: Fatal error: Command failed for target `install'
>
> and running R from the bin directory gives:
>
>> capabilities()
>    jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
>   FALSE    FALSE    FALSE    FALSE     TRUE     TRUE     TRUE     TRUE
>  cledit  IEEE754    iconv
>    TRUE     TRUE    FALSE
>
> Any suggestions would be greatly appreciated.  With solaris 9 we had a 64 bit 
> build
> but never encounter such problems, and I don't see anything in the archives 
> or the
> install manual that is relevant -- but of course, I'm not very clear about 
> what I'm looking
> for either.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Mon Jun  6 16:15:15 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Mon, 6 Jun 2005 16:15:15 +0200
Subject: [R] ROMA project (R Output MAnager) / Progress report / Call for
	contributors
Message-ID: <02a301c56aa2$296a0f70$6e8b6882@didacdom.stat.ucl.ac.be>



Hello useRs,

This email is about the ROMA project, the implementation in pure S of a
reporting system for R. Several month ago, I did then send on R-help a
survey on that (mainly: what features did you expect for a reporting
system). During the following months, I did try different approaches for
the core functionalities, ending now with a set of generic functions of
which I am satisfied. More details about the coding later for those who
are interested.

Basically, my purpose is to provide to R a mechanism to describe
outputs, in term of structure/content. Then, some driver functions would
handle output object to export them in HTML, LaTeX, RTF, raw text, and
so on.
While progressing in that project, I also did realize that it could even
be an intersting alternative to "print." method: each package creator
who write a "print.myclass" function only needs to describe output. If
he could do that once and benefit from HTML, TeX,... this would be of
great advantage.

I am currently seeking for a new job and I dont know if I will have time
later to go on with that project, which appears to be rather big. That's
why I am searching for collaborators. Anyone interested in reporting may
help. See below for a (non exhaustive) list of tasks / competences. I
have to say that in any cases I am interested in keeping involved in
that problematic.

My latest implementation consists in 2 x 1000 lines of highly commented
code. Mainly 1000 lines for the core functions and 1000 lines for the
beginning of the HTML export driver. Each output object consist of a XML
internal representation (using Duncan Temple Lang XML package), which
gives folowing advantages:
- any output structure could be described (even "non-linear")
- any output "brick" could contain things and have properties (that's
XML node / properties)
- we do have a DOM directly
- XML exportation driver is already functional

Some goals which are already achieved:
- describing complex tables (with merged cells, containing footnotes
etc)
- representing output as internal objects (for storing/further
manipulating purposes)
- manipulating those output objects ("+" operator, changing properties)

There is some work for volonteers in numerous areas:
- reading my actual code and helping to improve it
- thinking about required fundamentals "bricks" to describe outputs
(footnote, p-value, comment, are some sample such "bricks")
- implementing functions that describes some standards outputs
- implementing exporting drivers: LaTeX, native PDF, RTF, raw tex
- ...

Please let me know if you are interested in particpating to this
project. If there are enough participants, I will consider using a
shared platform such as freshmeat or other.


Eric


Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte



From br44114 at yahoo.com  Mon Jun  6 16:26:49 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 6 Jun 2005 07:26:49 -0700 (PDT)
Subject: [R] Reading huge chunks of data from MySQL into Windows R
Message-ID: <20050606142649.27355.qmail@web32113.mail.mud.yahoo.com>

You don't say what you want to do with the data, how many columns you
have etc. However, I would suggest proceeding in this order:
1. Avoid R; do everything in MySQL.
2. Use random samples.
3. If for some reason you need to process all 160 million rows in R, do
it in a loop. Pull no more than, say, 50-100k rows at a time. This
approach would allow you to process billions of rows without the memory
and disk requirements going through the roof.

hth,
b.


-----Original Message-----
From: Dubravko Dolic [mailto:Dubravko.Dolic at komdat.com]
Sent: Monday, June 06, 2005 9:31 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Reading huge chunks of data from MySQL into Windows R


Dear List,

 

I'm trying to use R under Windows on a huge database in MySQL via ODBC
(technical reasons for this...). Now I want to read tables with some
160.000.000 entries into R. I would be lucky if anyone out there has
some good hints what to consider concerning memory management. I'm not
sure about the best methods reading such huge files into R. for the
moment I spilt the whole table into readable parts stick them together
in R again. 

 

Any hints welcome.

 

 

 

Dubravko Dolic

Statistical Analyst

 

Email: dubravko.dolic at komdat.com


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html




		
__________________________________ 

Have fun online with music videos, cool games, IM and more. Check it out!



From Dubravko.Dolic at komdat.com  Mon Jun  6 16:43:26 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Mon, 6 Jun 2005 16:43:26 +0200
Subject: AW: [R] Reading huge chunks of data from MySQL into Windows R
Message-ID: <52D1AC81378E9342947189B04176014725514A@agentsmith.komdat.intern>

In my (limited) experience R is more powerful concerning data manipulation. An example: I have a vector holding a user id. Some user ids can appear more than once. Doing SELECT COUNT(DISTINCT userid) on MySQL will take approx. 15 min. Doing length(unique(userid)) will take (almost) no time...

So I think the other way round will serve best: Do everything in R and avoid using SQL on the database...




-----Urspr??ngliche Nachricht-----
Von: bogdan romocea [mailto:br44114 at yahoo.com] 
Gesendet: Montag, 6. Juni 2005 16:27
An: Dubravko Dolic
Cc: r-help at stat.math.ethz.ch
Betreff: RE: [R] Reading huge chunks of data from MySQL into Windows R

You don't say what you want to do with the data, how many columns you
have etc. However, I would suggest proceeding in this order:
1. Avoid R; do everything in MySQL.
2. Use random samples.
3. If for some reason you need to process all 160 million rows in R, do
it in a loop. Pull no more than, say, 50-100k rows at a time. This
approach would allow you to process billions of rows without the memory
and disk requirements going through the roof.

hth,
b.


-----Original Message-----
From: Dubravko Dolic [mailto:Dubravko.Dolic at komdat.com]
Sent: Monday, June 06, 2005 9:31 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Reading huge chunks of data from MySQL into Windows R


Dear List,

 

I'm trying to use R under Windows on a huge database in MySQL via ODBC
(technical reasons for this...). Now I want to read tables with some
160.000.000 entries into R. I would be lucky if anyone out there has
some good hints what to consider concerning memory management. I'm not
sure about the best methods reading such huge files into R. for the
moment I spilt the whole table into readable parts stick them together
in R again. 

 

Any hints welcome.

 

 

 

Dubravko Dolic

Statistical Analyst

 

Email: dubravko.dolic at komdat.com


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html




		
__________________________________ 
Discover Yahoo! 
Have fun online with music videos, cool games, IM and more. Check it out! 
http://discover.yahoo.com/online.html



From p.dalgaard at biostat.ku.dk  Mon Jun  6 17:08:57 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 17:08:57 +0200
Subject: [R] make install on solaris 10
In-Reply-To: <72A6CB57-4A44-4AF6-A9A6-B87073D53C76@uiuc.edu>
References: <72A6CB57-4A44-4AF6-A9A6-B87073D53C76@uiuc.edu>
Message-ID: <x21x7f7cae.fsf@turmalin.kubism.ku.dk>

roger koenker <rkoenker at uiuc.edu> writes:

> We have recently upgraded to Solaris 10 on a couple of sparc machines
> with the usual
> mildly mysterious consequences for library locations, etc, etc.  I've
> managed to configure
> R 2.1.0 for a 64 bit version with:
> 
> R is now configured for sparc-sun-solaris2.10
> 
>    Source directory:          .
>    Installation directory:    /usr/local
> 
>    C compiler:                gcc -m64 -g -O2
>    C++ compiler:              g++  -m64 -fPIC
>    Fortran compiler:          g77  -m64 -g -O2
> 
>    Interfaces supported:      X11
>    External libraries:        readline
>    Additional capabilities:   PNG, JPEG, MBCS, NLS
>    Options enabled:           R profiling
> 
>    Recommended packages:      yes
> 
> configure:47559: WARNING: you cannot build info or html versions of
> the R manuals
> 
> and make and make check seem to run smoothly, however "make install"
> dies with
> the following messages:
> 
> ysidro.econ.uiuc.edu# make install
> installing doc ...
> creating doc/html/resources.html
> *** Error code 255
> The following command caused the error:
> false --html --no-split --no-headers ./resources.texi -o ../html/
> resources.html
> make: Fatal error: Command failed for target `../html/resources.html'
> Current working directory /usr/local/encap/R-2.1.0/doc/manual
> installing doc/html ...
> installing doc/html/search ...
> /usr/local/bin/install: resources.html: No such file or directory
> *** Error code 1
> The following command caused the error:
> for f in resources.html; do \
>    /usr/local/bin/install -c -m 644 ${f} "/usr/local/lib/R/doc/html"; \
> done
> make: Fatal error: Command failed for target `install'
> Current working directory /usr/local/encap/R-2.1.0/doc/html
> *** Error code 1
> The following command caused the error:
> for d in html manual; do \
>    (cd ${d} && make install) || exit 1; \
> done
> make: Fatal error: Command failed for target `install'
> Current working directory /usr/local/encap/R-2.1.0/doc
> *** Error code 1
> The following command caused the error:
> for d in m4 tools doc etc share src po tests; do \
>    (cd ${d} && make install) || exit 1; \
> done
> make: Fatal error: Command failed for target `install'
> 
> and running R from the bin directory gives:
> 
>  > capabilities()
>      jpeg      png    tcltk      X11 http/ftp  sockets   libxml     fifo
>     FALSE    FALSE    FALSE    FALSE     TRUE     TRUE     TRUE     TRUE
>    cledit  IEEE754    iconv
>      TRUE     TRUE    FALSE
> 
> Any suggestions would be greatly appreciated.  With solaris 9 we had
> a 64 bit build
> but never encounter such problems, and I don't see anything in the
> archives or the
> install manual that is relevant -- but of course, I'm not very clear
> about what I'm looking
> for either.

It's your missing (or outdated) makeinfo that is coming back to bite
you. However, I'm a bit confuzzled because we do ship resources.html
et al. as part of the R tarball, so there shouldn't be a need to build
them. Were you building from an SVN checkout? 

The way out is to install texinfo 4.7 or better. If you have the .html
files, you might be able to get by just by touch-ing or copying them.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From robin at hms.harvard.edu  Mon Jun  6 17:13:53 2005
From: robin at hms.harvard.edu (Robin Colgrove)
Date: Mon, 6 Jun 2005 11:13:53 -0400
Subject: [R] analysis and figure for sign test in setting of high
	inter-experiment variance
Message-ID: <35b68f49bad1b6e7d265369df4eba02f@hms.harvard.edu>

Hello all,

Sorry if this is an FAQ. I have been trying to search the archives 
without success.
I have a dataset (ChiPs microarray) where the experiment to experiment 
variability is very high
but where within an experiment, the data nearly always goes in the 
"right" (hypothesis confirming) direction.
I am trying to figure out the right way to use R to do the statistical 
analysis and generate an appropriate figure.

To be specific, we have a virus and a mutant derivative, and the 
hypothesis is that the wild type virus is specifically suppressing 
transcription activity in a manner that is abrogated in the mutant.

The experiment is to measure the amount of viral chromatin associated 
with overall histone (should be the same between wild type and mutant), 
vs. transcriptionally active chromatin (mutant should be greater than 
wild type) vs. inactive chromatin (wild type should be greater than 
mutant).

For each datapoint there four variables:  a histone type (general, 
active, inactive), a specific gene assayed (four different genes),  a 
virus used for infection (wild type or mutant), and an experiment 
number (each combination repeated 3-5 times) These are hard experiments 
to do (involving dissecting out small numbers of cells from a mouse) so 
the numbers are small, but in each case, there are 3-5 pairs of wild 
type vs mutant virus for each condition.

If I look at simply whether the hypothesis is confirmed for each 
condition (whether the wild type/mutant difference goes the way you 
would expect), then the sign is right 34/35 times, which is way beyond 
reasonable significance. However, since the inter-experiment variance 
is so high, if I try to do a simple rank-sum test for a particular 
chromatin-gene-virus combination (3-5 pairs), the result is usually 
non-significant (never significant if Bonferroni corrections for 
multiple tests are applied).

My questions are:

1) Is there a good way within R to do and report a simple sign test on 
this sort of data (paired samples, non-normal, high-inter experiment 
variability)?

2) What would be a good way to plot this sort of noisy data (and how to 
do it in R)? I was thinking of having each (wildtype-mutant) experiment 
pair as the ends of line segments with different colors or line-types 
for each gene-chromatin type combination. I know this is a standard 
kind of plot but I can't figure out how to do it in R.

3) What is the best way to input this data? A 4d array with virus type 
(wild type or mutant) on one axis, chromatin type (non-specific, 
active, inactive) on the second, gene (one of four different genes) on 
the third, and experiment number (1-5) on the last? Is there a good way 
to do this with data frames?

Thanks for any help or pointers to appropriate how-to's. I am really 
trying to figure this out myself, but as a virologist/bioinformaticist 
new to R, I still have a lot to learn statistics-wise.

robin colgrove
dept. of microbiology
harvard medical school



From p.dalgaard at biostat.ku.dk  Mon Jun  6 17:21:33 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 17:21:33 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A44259.9040309@free.fr>
References: <42A44259.9040309@free.fr>
Message-ID: <x2wtp75x4y.fsf@turmalin.kubism.ku.dk>

Romain Francois <francoisromain at free.fr> writes:

> Hello all,
> 
> It seems that the next improvement to the R Graph Gallery is
> categorization of the graphics, that way each graph will be easier to
> find. That step should be done *carefully* if we want to avoid the
> opposite side-effect : graph not reachable through the categories.
> That's why the wisdom of the R community is required.
> Graphics will be classified in :
>    - categories
>    - sub-categories within those categories
> So far so good.
> The problem is the determination of those categories and sub-categories.
> Any thoughts are welcome.
> 
> The gallery now has a TODO/WISHLIST :
> http://addictedtor.free.fr/graphiques/WISHLIST.php that waits for your
> accurate comments.

BTW, shouldn't there a way there from http://addictedtor.free.fr ? I
seem to get stuck on the home page with nothing to click (except "R"
and the labels on the left).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From slist at oomvanlieshout.net  Mon Jun  6 17:25:33 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 06 Jun 2005 17:25:33 +0200
Subject: [R] A long digression on packages
In-Reply-To: <42A43794.15246.170B9626@localhost>
References: <42A3027A.9000007@stats.uwo.ca> <42A43794.15246.170B9626@localhost>
Message-ID: <42A46AED.6090208@oomvanlieshout.net>

Maybe some of this confusion about search opportunities and pros/cons 
could be avoided if the search page on CRAN 
(http://cran.r-project.org/search.html) would be extended to cover all 
main search tools!

Quickly scanning the discussion, I found these:
1- simply Google: some tips and tricks have been mentioned and would be 
usefully for most users;
2- R site search (external to CRAN) 
http://finzi.psych.upenn.edu/search.html;
3- from R prompt: help.search();
4- browser supported search through local help files: 
R/doc/html/search/SearchEngine.html.

ad1. Google is normally my first source using broad keywords for a 
method or problem.
ad2. just discovered this today!
ad3. help.search() provides a simple overview, printing the command with 
the providing package:
 > help.search("rose")
Help files with alias or concept or title matching ?rose? using
regular expression matching:
hirose(boot)            Failure Time of PET Film
rose.diag(CircStats)    Rose Diagram
rose.diag(circular)     Rose Diagram
windrose(circular)      Windrose Generator
rosavent(climatol)      Wind-rose plot
Kinship82(clue)         Rosenberg-Kim Kinship Terms Partition Data
HolidayDatabase(fCalendar)
                         Holiday Calendars and Utilities
conc(ineq)              Concentration Measures
Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.

ad4. when installing all package locally, the results produced by the 
browser supported search can be overwhelming. Even searching within the 
results often does not help. If commands were printed along with the 
providing package would be a good improvement. Then the apparent random 
order of commands listed might also reveal some order.

Hope this is a useful addition to the debate,

Sander.

Chris Evans wrote:
> On 5 Jun 2005 at 18:44, Jari Oksanen wrote:
> 
>>There are diverse opinions about netiquette. One of the most basic, in
>>my opinion, is this: if someone posts starts a discussion in a certain
>>forum, you shall not divert it to another forum where it may be hidden
>>by most readers, perhaps even by the originator of the thread.
> 
> With the greatest of respect for Duncan and the R-devel list, I think 
> Jari has a point here.  This is one of the most important issues I've 
> seen raised on this list (R-help) in recent months and I think it may 
> be a structural problem for the development of R, in common with that 
> of much FLOSS s'ware, that there's a separation of users and authors 
> that needs thought.  There are no perfect answers but too big a 
> separation and projects go "techno" and it's hard for those of us who 
> can't code C and who are mere "users" to help those outstanding 
> people on whom we depend hear what we need: sometimes they are so 
> clever, so specialised in their knowledge, or simply in the realm of 
> genius not the ordinary, that they can't see our problem.  I have 
> slowly come to respect that a pretty brusque style from our 
> authorities is the only way to prevent this list being a madhouse but 
> I think that Jim's point may fall into that class that is worth some 
> duplicate bandwidth here.
> 
> I know I've found the problem Jim highlights very confusing and 
> unhelpful at times.  Views, which I didn't know, seem helpful but not 
> a real solution to the key problem: they may tangentially help by 
> ensuring that if your needs fit into a view, it becomes more likely 
> that you'll install the packages you need and a local search may tell 
> you what you need.  I've taken the inefficient route which suits me 
> of installing just about every package to make it less likely I'll 
> miss something of use to me. That means my search for "kappa" and 
> "Cohen" (with ignore.case=FALSE) turns up at least three 
> implementations of aspects of Cohen's kappa.
> 
> It may already exist, but a web interface that did a help.search over 
> all the packages in the current release version would be great.  (If 
> it does exist, sorry, but I'm no dunce and use R nearly every day and 
> try to read much of r-help every day and don't know it, which may say 
> something!)
> 
> I think there may be a need for some R improvement and automated 
> updating of what I think is Frank Harrell's function finder:
> 	http://biostat.mc.vanderbilt.edu/s/finder/finder.html
> Though I'm not absolutely sure how fitting your works into something 
> like that could be imposed on developers!
> 
> Another thing that might help would be for a system by which ordinary 
> users would volunteer to pair up with developers for packages and try 
> to suggest adaptations of the help and such like that might make the 
> packages more user friendly.  I wouldn't want to do that for the 
> whole of a huge and vital package like MASS or Hmisc (or base or 
> stats!) but I'm up for pairing with a developer on a smaller package 
> if anyone thinks that would be helpful.
> 
> Thoughts for what they're worth.  Thanks a million to all developers 
> ... asbestos suit on!
> 
> Chris


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From murdoch at stats.uwo.ca  Mon Jun  6 17:26:41 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Jun 2005 11:26:41 -0400
Subject: [R] Missing values in argument of .Fortran.
In-Reply-To: <200506061352.j56DqIJt026716@erdos.math.unb.ca>
References: <200506061352.j56DqIJt026716@erdos.math.unb.ca>
Message-ID: <42A46B31.6060602@stats.uwo.ca>

On 6/6/2005 9:52 AM, Rolf Turner wrote:
> I wish to pass a vector ``y'', some of whose entries are NAs to a
> fortran subroutine which I am dynamically loading and calling by
> means of .Fortran().  The subroutine runs through the vector entry by
> entry; obviously I want to have it do one thing if y[i] is present
> and a different thing if it is missing.
> 
> The way I am thinking of proceeding is along the xlines of:
> 
> 	ymiss <- is.na(y)
> 	rslt <- .Fortran(
> 		"foo",
> 		NAOK=TRUE,
> 		as.double(y),
> 		as.logical(ymiss),
> 		etc,
> 		etc
> 	)
> 
> and inside ``foo'' have a logical branch based on the value of
> xmiss(i).
> 
> Questions:
> 
> 	(1) Is there a sexier way to proceed?  E.g. is it possible
> 	within (g77) fortran to detect the fact that y(i) is/was an
> 	NA (or not) and make the nature of y(i) the basis of an
> 	if-statement?

In C you can use the macros

ISNA(x) True for R?s NA only
ISNAN(x) True for R?s NA and IEEE NaN
R_FINITE(x) False for Inf, -Inf, NA, NaN

where the R function is.na() is closest to ISNAN(), I think.  There's no 
supplied way to do these things in Fortran, but presumably you could 
call a C function which did one of these tests.

> 	(2) Are there any lurking pitfalls in the use of the NAOK=TRUE
> 	argument?

I think the way you did it looks perfectly safe.  Following my advice 
above will be a little trickier, because some other user of your code 
might use a different Fortran compiler, and it might handle C functions 
differently.

> 	(3) Is there an entirely different and better way to proceed?

I'd do it your way if I was using Fortran.

Duncan Murdoch



From h.wickham at gmail.com  Mon Jun  6 17:34:33 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 6 Jun 2005 10:34:33 -0500
Subject: [R] Reading huge chunks of data from MySQL into Windows R
In-Reply-To: <52D1AC81378E9342947189B04176014725514A@agentsmith.komdat.intern>
References: <52D1AC81378E9342947189B04176014725514A@agentsmith.komdat.intern>
Message-ID: <f8e6ff050506060834367127cb@mail.gmail.com>

> In my (limited) experience R is more powerful concerning data manipulation. An example: I have a vector holding a user id. Some user ids can appear more than once. Doing SELECT COUNT(DISTINCT userid) on MySQL will take approx. 15 min. Doing length(unique(userid)) will take (almost) no time...

I think you have it around the wrong way - or you don't have indexes
set up in mysql.  If you're dealing with large quanities of data I'd
strongly recommend learning about sql indexes as it will save you a
LOT of time.

Hadley



From slist at oomvanlieshout.net  Mon Jun  6 17:39:49 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 06 Jun 2005 17:39:49 +0200
Subject: [R] Polar Graph
In-Reply-To: <20050606124601.73933.qmail@web26605.mail.ukl.yahoo.com>
References: <20050606124601.73933.qmail@web26605.mail.ukl.yahoo.com>
Message-ID: <42A46E45.4060808@oomvanlieshout.net>

Navarre Sabine wrote:
> Hi,
>  
> I would like to do a polar graph (=star graph) ! is that graph existing on R?
> Because more softwares can do that but I don't found  it on R!
> 
> Thanks
>  
> Sabine
> 
> 		
> ---------------------------------
> 
> ils, photos et vidÅÈos !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 



Try:   rose.diag {CircStats}!

Sander.


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From laurent.pantera at irsn.fr  Mon Jun  6 15:54:58 2005
From: laurent.pantera at irsn.fr (PANTERA Laurent)
Date: Mon, 6 Jun 2005 15:54:58 +0200 
Subject: [R] How to vectorize
Message-ID: <BF8E83A574F8FF40BD5C0266B2A4DF880153C1E1@canna.proton.intra.irsn.fr>

Dear R-List,

     I would like to write nicely the names of some isotopes on a plot. The
code bellow works fine.

plot(1:10,1:10)
text(c(2,4,8),c(2,4,8),labels=c(expression(italic(phantom(0)^{78}*Ge)),
                                expression(italic(phantom(0)^{137}*Cs)),
                                expression(italic(phantom(0)^{129*m}*Te))),
     cex=3
     )

But, since I have a lot of isotopes to write on the plot, I would like
to construct automatically the labels. So I wrote the code below which
works fine.

listenoms <- list(nom=c("Ge","Cs","Te"),num=c("78","137","129*m"))
n <- length(listenoms$nom)
resu <- "c("
for( i in 1:(n-1))
  {
    resu <- paste(resu,paste("expression(italic(phantom(0)^{",
                         listenoms$num[i],"}*",
                         listenoms$nom[i],")),",sep=""))
}
resu <- paste(resu,paste("expression(italic(phantom(0)^{",
                         listenoms$num[n],"}*",
                         listenoms$nom[n],")))",sep=""))
plot(1:10,1:10)
text(c(2,4,8),c(2,4,8),labels=eval(parse(text=resu)),cex=2)

I assume there is a better way to do that using vectorization.
May you help me to find it ?

Thanks
Laurent



From francoisromain at free.fr  Mon Jun  6 17:34:01 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 06 Jun 2005 17:34:01 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <x2wtp75x4y.fsf@turmalin.kubism.ku.dk>
References: <42A44259.9040309@free.fr> <x2wtp75x4y.fsf@turmalin.kubism.ku.dk>
Message-ID: <42A46CE9.8050906@free.fr>

Le 06.06.2005 17:21, Peter Dalgaard a ??crit :

>Romain Francois <francoisromain at free.fr> writes:
>
>  
>
>>Hello all,
>>
>>It seems that the next improvement to the R Graph Gallery is
>>categorization of the graphics, that way each graph will be easier to
>>find. That step should be done *carefully* if we want to avoid the
>>opposite side-effect : graph not reachable through the categories.
>>That's why the wisdom of the R community is required.
>>Graphics will be classified in :
>>   - categories
>>   - sub-categories within those categories
>>So far so good.
>>The problem is the determination of those categories and sub-categories.
>>Any thoughts are welcome.
>>
>>The gallery now has a TODO/WISHLIST :
>>http://addictedtor.free.fr/graphiques/WISHLIST.php that waits for your
>>accurate comments.
>>    
>>
>
>BTW, shouldn't there a way there from http://addictedtor.free.fr ? I
>seem to get stuck on the home page with nothing to click (except "R"
>and the labels on the left).
>
>  
>
Hello Peter,

My english fails me at understanding your question. I don't get stuck 
anywhere.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From andy_liaw at merck.com  Mon Jun  6 17:44:11 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Jun 2005 11:44:11 -0400
Subject: [R] A long digression on packages
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E92D@usctmx1106.merck.com>

> From: Chris Evans
> 
> On 6 Jun 2005 at 7:02, Jonathan Baron wrote:
> 
> > I haven't followed this thread, but the web interface may exist.
> > (Perhaps "help.search()" does something that Namazu doesn't do,
> > but I don't think so.)  See my .sig below.
> > 
> > This is where you get if you click on "Search" in the R home
> > page.
> Not quite: that's wonderful and I use it a lot but it throws up much 
> more from r-project than just the search of the current release 
> packages would in help.search() so it can give you much more than you 
> need ... if there are tuning parameters one can add that will do the 
> necessary, and I'm sure there are, then I'd love to see then and 
> ideally see another search box that applied them for us! 

Try something like RSiteSearch("your phrase", restrict="function").  The
search page itself has more options.
 
> But thanks Jonathan!

Indeed!

Andy
 
> Chris
> -- 
> Chris Evans <chris at psyctc.org>
> Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
> Research Programmes Director, Nottinghamshire NHS Trust, 
> Hon. SL Institute of Psychiatry
> *** My views are my own and not representative of those institutions 
> ***
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From slist at oomvanlieshout.net  Mon Jun  6 17:48:31 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Mon, 06 Jun 2005 17:48:31 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A4564B.90002@lancaster.ac.uk>
References: <42A44259.9040309@free.fr> <42A4564B.90002@lancaster.ac.uk>
Message-ID: <42A4704F.6080504@oomvanlieshout.net>

Barry Rowlingson wrote:
> Romain Francois wrote:
> 
>> Graphics will be classified in :
>>   - categories
>>   - sub-categories within those categories
>> So far so good.
> 
>  Maybe, maybe not! Would a system of keywords work better than strict 
> hierarchical categories, as long as plots can have more than one keyword 
> attached? Someone might be interested in all 3d plots, or all plots 
> related to model fitting, and these categories may not be distinct!
> 
> Barry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

The whole point of a gallery is to show something to the user before the 
user knows what he is looking for. The R help functions currently 
available are hopeless when you have a picture of a graph in your head 
without knowing the required commands.

Thus a gallery based on some categorization is required. The gallery 
already has a search facility for 'un-categorized' searching. At the 
same time the gallery will provide cross links between different graphs 
using the same command/package or other similarities.

I suggested that the categorization for sigmaplot graphs could be a 
useful start:
http://www.systat.com/products/SigmaPlot/productinfo/?sec=1003

The gallery is based on a database, so keyword searching is no problem.

Of course graphs will likely be members of more then one sub-category. 
This is also no problem thanks to the database approach!

Hopefully we can agree on a categorization, such that we can start 
populating the gallery!

Cheers,

Sander.

-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From hodgess at gator.dt.uh.edu  Mon Jun  6 17:51:14 2005
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Mon, 6 Jun 2005 10:51:14 -0500
Subject: [R] intervals and data frames
Message-ID: <200506061551.j56FpEVL007969@gator.dt.uh.edu>

Dear R people:

I have a vector which runs from -1 to 1, by .1, inclusively.

Easy to set up.  x <- seq(-1,1,.1)

I then sample 3 numbers from x.
y <- sample(x, 3)

Suppose one of my values is -0.7.  I want to set up an interval
around that
y1 <- pmax(y-0.1,-1)
y2 <- pmin(y+0.1,1)
For the value -.7, the interval will run from -.8 to -.6.
There will be several intervals.

Again, nothing interesting so far.

However, now I want to set up something that I will call
r1.  Essentially, r1 "runs" between -1 and 1 as well.
I want to place a value in r1 between -.8 and -.6 for my interval,
and zeros elsewhere.

I'm trying to use a data frame in which the first column represents
the values from -1 to 1.  The remaining columns are initially set to
zero.  Also, I'm trying to avoid loops.

Any suggestions would be much appreciated.

Thanks in advance,
Sincerely
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From br44114 at yahoo.com  Mon Jun  6 18:02:44 2005
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 6 Jun 2005 09:02:44 -0700 (PDT)
Subject: [R] Reading huge chunks of data from MySQL into Windows R
Message-ID: <20050606160244.63018.qmail@web32109.mail.mud.yahoo.com>

On June 06, 2005 Dubravko Dolic wrote:
> So I think the other way round will serve best: 
> Do everything in R and avoid using SQL on the database...

I'm not so sure. How about your MySQL experience? 
What types of queries do you run most often? What indexes do you have?
What kind of indexes? What storage engine(s)? How about the query
cache? What kinds of joins do you do? In what order? Etc etc etc.
Optimizing MySQL queries is a (much) more complex job than it may
appear at first sight. 

An index on userid would cause your query to run in seconds instead of
minutes (the full table scan would be avoided). If you're talking about
counting stuff from very large tables, R can't possibly be faster than
MySQL.



-----Original Message-----
From: Dubravko Dolic [mailto:Dubravko.Dolic at komdat.com]
Sent: Monday, June 06, 2005 10:43 AM
To: br44114 at gmail.com
Cc: r-help at stat.math.ethz.ch
Subject: AW: [R] Reading huge chunks of data from MySQL into Windows R


In my (limited) experience R is more powerful concerning data
manipulation. An example: I have a vector holding a user id. Some user
ids can appear more than once. Doing SELECT COUNT(DISTINCT userid) on
MySQL will take approx. 15 min. Doing length(unique(userid)) will take
(almost) no time...

So I think the other way round will serve best: Do everything in R and
avoid using SQL on the database...




-----Urspr??ngliche Nachricht-----
Von: bogdan romocea [mailto:br44114 at yahoo.com] 
Gesendet: Montag, 6. Juni 2005 16:27
An: Dubravko Dolic
Cc: r-help at stat.math.ethz.ch
Betreff: RE: [R] Reading huge chunks of data from MySQL into Windows R

You don't say what you want to do with the data, how many columns you
have etc. However, I would suggest proceeding in this order:
1. Avoid R; do everything in MySQL.
2. Use random samples.
3. If for some reason you need to process all 160 million rows in R, do
it in a loop. Pull no more than, say, 50-100k rows at a time. This
approach would allow you to process billions of rows without the memory
and disk requirements going through the roof.

hth,
b.


-----Original Message-----
From: Dubravko Dolic [mailto:Dubravko.Dolic at komdat.com]
Sent: Monday, June 06, 2005 9:31 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Reading huge chunks of data from MySQL into Windows R


Dear List,

 

I'm trying to use R under Windows on a huge database in MySQL via ODBC
(technical reasons for this...). Now I want to read tables with some
160.000.000 entries into R. I would be lucky if anyone out there has
some good hints what to consider concerning memory management. I'm not
sure about the best methods reading such huge files into R. for the
moment I spilt the whole table into readable parts stick them together
in R again. 

 

Any hints welcome.

 

 

 

Dubravko Dolic

Statistical Analyst

 

Email: dubravko.dolic at komdat.com


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html







		
__________________________________ 

Get on-the-go sports scores, stock quotes, news and more. Check it out!



From spencer.graves at pdf.com  Mon Jun  6 18:06:11 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 06 Jun 2005 09:06:11 -0700
Subject: [R] Polar Graph
In-Reply-To: <20050606124601.73933.qmail@web26605.mail.ukl.yahoo.com>
References: <20050606124601.73933.qmail@web26605.mail.ukl.yahoo.com>
Message-ID: <42A47473.8030406@pdf.com>

	  I just tried 'RSiteSearch("polar graph")' and got 2 hits, the first 
of which (http://finzi.psych.upenn.edu/R/Rhelp02a/archive/11961.html) 
seemed to contain code for what you want.  (If this does not work for 
you, please read the posting guide! 
http://www.R-project.org/posting-guide.html and tell is why it's not 
satisfactory.)

	  spencer graves

Navarre Sabine wrote:

> Hi,
>  
> I would like to do a polar graph (=star graph) ! is that graph existing on R?
> Because more softwares can do that but I don't found  it on R!
> 
> Thanks
>  
> Sabine
> 
> 		
> ---------------------------------
> 
> ils, photos et vid??os !
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Mon Jun  6 18:11:27 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 6 Jun 2005 09:11:27 -0700
Subject: [R] A long digression on packages
In-Reply-To: <42A46AED.6090208@oomvanlieshout.net>
Message-ID: <200506061611.j56GBRKt027058@ohm.gene.com>

$.02 (no more):

1. R and its packages are big and diverse, growing rapidly. No simple phrase
to describe this, but perhaps " the most important contribution to data
analysis and statistical practice since Fisher" comes close.

2. Ergo lots of diverse information, with little or no way to classify and
organize. But this is THE BIG THING in IT today, is it not? -- witness
Google, Yahoo search, etc. Everyone says understanding and technology to do
this well is in its infancy. So that R and its community struggles too is no
surprise.

3. As desirable as efficiency and redundancy reduction is, R's nature and
design mitigates against it: core R is centrally controlled (of course!),
but the point is **NOT** to restrict contributed packages (core R is the
engine and coherent point of entry into all those packages). So the best we
can hope for is that package contributors will do their homework before
writing to see whether their intended functionality is already there or
could be just added to someone else's. Obviously very subjective -- and
could be difficult due to terminology variation (different disciplines that
use different terms to describe the same statistical functionality).

4. So other than package writers putting as many keywords as they can into
their packages for search engines to hit -- and perhaps some limited
organization by dedicated workers to organize/bring together "obvious" stuff
like graphics or econometrics or geostatistics, etc. -- it seems that all we
can reasonably hope for is to integrate the search functionality into core R
and R directory structures. But wait! -- this is exactly what the R team is
already doing.

Cheers,
Bert



From reid_huntsinger at merck.com  Mon Jun  6 18:59:59 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 6 Jun 2005 12:59:59 -0400
Subject: [R] intervals and data frames
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A947A@uswpmx00.merck.com>

What do you intend r1 to contain? TRUE and FALSE (an indicator vector?) or
say integers 1,2,3 designating your intervals around your 3 sample points?
So you would e.g. set r1[k,1] to 3 if the kth grid point in [-1,1] lies in
interval 3? Etc? Some more detail would help narrow down the possibilities.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Erin Hodgess
Sent: Monday, June 06, 2005 11:51 AM
To: r-help at stat.math.ethz.ch
Subject: [R] intervals and data frames


Dear R people:

I have a vector which runs from -1 to 1, by .1, inclusively.

Easy to set up.  x <- seq(-1,1,.1)

I then sample 3 numbers from x.
y <- sample(x, 3)

Suppose one of my values is -0.7.  I want to set up an interval
around that
y1 <- pmax(y-0.1,-1)
y2 <- pmin(y+0.1,1)
For the value -.7, the interval will run from -.8 to -.6.
There will be several intervals.

Again, nothing interesting so far.

However, now I want to set up something that I will call
r1.  Essentially, r1 "runs" between -1 and 1 as well.
I want to place a value in r1 between -.8 and -.6 for my interval,
and zeros elsewhere.

I'm trying to use a data frame in which the first column represents
the values from -1 to 1.  The remaining columns are initially set to
zero.  Also, I'm trying to avoid loops.

Any suggestions would be much appreciated.

Thanks in advance,
Sincerely
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From luciana at cptec.inpe.br  Mon Jun  6 19:04:22 2005
From: luciana at cptec.inpe.br (luciana)
Date: Mon, 06 Jun 2005 14:04:22 -0300
Subject: [R] Help package pls.pcr
Message-ID: <42A48216.8020204@cptec.inpe.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/adeba304/attachment.pl

From rkoenker at uiuc.edu  Mon Jun  6 19:13:50 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 6 Jun 2005 12:13:50 -0500
Subject: [R] make install on solaris 10
In-Reply-To: <x21x7f7cae.fsf@turmalin.kubism.ku.dk>
References: <72A6CB57-4A44-4AF6-A9A6-B87073D53C76@uiuc.edu>
	<x21x7f7cae.fsf@turmalin.kubism.ku.dk>
Message-ID: <11CE9F0A-7B05-439A-BAE3-126C119FC55C@uiuc.edu>


On Jun 6, 2005, at 10:08 AM, Peter Dalgaard wrote:
>>
>
> It's your missing (or outdated) makeinfo that is coming back to bite
> you. However, I'm a bit confuzzled because we do ship resources.html
> et al. as part of the R tarball, so there shouldn't be a need to build
> them. Were you building from an SVN checkout?
>
> The way out is to install texinfo 4.7 or better. If you have the .html
> files, you might be able to get by just by touch-ing or copying them.

On Jun 6, 2005, at 9:14 AM, Prof Brian Ripley wrote:
>
> As far as I can see something has deleted doc/html/resources.html:  
> it is in the tarball. I cannot immediately guess what: have you  
> done any sort of `make clean'?
>
> Copying it from the virgin sources and doing `make install' again  
> should fix this: if not perhaps you can keep an eye on what is  
> apparently removing it.
>
> BTW, where did /usr/local/bin/install come from?  If that is not  
> doing what is expected, it could be the problem.

Having:

     1.  Downloaded a fresh version of R-devel
     2.  Installed texinfo 4.8
     3.  moved my rogue /usr/local/bin/install file out of the way

R now builds and installs fine.  It looks like X11 support is still  
missing
but presumably just needs -L/usr/openwin/lib/sparcv9.  Some further
investigation is needed for png, jpeg and tctlk support, but this can  
wait
for a little while.

Thanks very much for your help.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820



From andy_liaw at merck.com  Mon Jun  6 19:30:25 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Jun 2005 13:30:25 -0400
Subject: [R] Help package pls.pcr
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E92E@usctmx1106.merck.com>

> From: luciana
> 
> Hello! 
> I need help to use the package pls.pcr in R. 
> 
> I installed R in an IRIX 6.5, using the version of R 0.64.1 from 
> sgifreeware(I didn't get to install the newest version using 
> make).  I 
> need to use the package pls.pcr and when I give the command: 
>  
> # R
> 
> R : Copyright 1999, The R Development Core Team
> Version 0.64.1  (May 8, 1999)

For starter, try upgrading R to a version from this century.  The version
you have seems very strange:  AFAICR by 1999 R version is already close to
1.0.0.

Also, pls.pcr has been superceded by the pls package on CRAN.

Andy
 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type    "?license" or "?licence" for distribution details.
> 
> R is a collaborative project with many contributors.
> Type    "?contributors" for a list.
> 
> Type    "demo()" for some demos, "help()" for on-line help, or
>         "help.start()" for a HTML browser interface to help.
> Type    "q()" to quit R.
> 
>  >
>  > library (pls.pcr)
> Error in parse(file, n, text, prompt) : syntax error on line 291
>  
> 
> 
> I installed the package with the command,
>  R CMD INSTALL pls.pcr
> Installing package `pls.pcr' ...
>  R
>  data
>  help
> ../TITLE - No such file or directory
>  >>> Building/Updating help pages for package `pls.pcr'
>      Formats: text html latex example
>   pcr.model                                 html    latex
>   plot.mvr                          text    html    latex   example
>   predict.mvr                       text    html    latex   example
>   sensory                           text    html    latex
>   simpls                            text    html    latex   example
>   summary.mvr                       text    html    latex   example
>  DONE (pls.pcr)
> 
> DONE (INSTALL)
> 
> What is happening he? Can I help myself? 
> 
> thank you
> 
> Luciana
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From jfox at mcmaster.ca  Mon Jun  6 20:12:32 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 6 Jun 2005 14:12:32 -0400
Subject: [R] chisq.test and anova problems
In-Reply-To: <b2381081050606064332a54d9c@mail.gmail.com>
Message-ID: <20050606181229.OYJP16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Richard,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Richard Mendes
> Sent: Monday, June 06, 2005 8:44 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] chisq.test and anova problems
> 
> we just started to use R and having some problems that no one 
> in our school could solve. I hope someone here can help me out.
> 
> the first problem is with the chisquare test. we want to 
> exclude the missing values from the data. we used na.omit and 
> made two new variables.
> 
> now we want to use the chi square method but get the error x 
> and y must have the same length.
> 
> how do i use the chisquare method were i exclude the missing values ?
> and can i use this method if there is a difference in length.
> 

Take a look at the help file for chisq.test (e.g., ?chisq.test): Missing
data are excluded when the contingency table for x and y is formed, so you
need do nothing special to get what you want -- just chisq.test(x, y).

> the second problem is with anova in the data set we are 
> working on we have to use this method on multiple variables 
> with a difference in length, can this be done. this is the 
> syntax we used and the error is stated behind.
> 
> anova(lm(test~test1)) and the error states variable length differ.
> 

I'm guessing that test and test1 are two samples on the response variable
that you want to compare, though that's not entirely clear from your
question. To do a one-way anova via lm(), you should have all of the
observations on the response in one variable and a factor giving group
membership of each observation; then anova(lm(response~factor)) will give
you the one-way ANOVA table. Also take a look at ?aov. Finally, if I've
correctly guessed what you're trying to do, then t.test(test, test1) is an
alternative.

More generally, have you looked at the introductory manual that comes with
R?

I hope this helps,
 John

> i think there has to be a way to use this method on 
> differences in variable lengths does anybode know how to do this
> 
> thanks in advance for your help
> 
> richard



From mwgrant2001 at yahoo.com  Mon Jun  6 20:46:57 2005
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Mon, 6 Jun 2005 11:46:57 -0700 (PDT)
Subject: [R] geometric mean regression
In-Reply-To: <XFMail.050606102158.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050606184657.21283.qmail@web52009.mail.yahoo.com>

Hi Ted,

Thank you for your informative comments regarding GMR.
   

 
TH:
> This somewhat contentious method 

Contentious...well that says a lot (seriously)!

TH:
> is indeed trivial to implement in R. ...

I implemented it in a simnple brute force
manner--elegance is time--following Helsel and Hirsch.
Get the two slopes to calculate the GMR slope and then
use mean(x) and mean(y) with the new slope to get the
intercept...
 
TH:
> It hardly needs a package!

By itself, no. Your comment is timely given another
help thread currently on the large number of packages
:O). But something like Stats-R-Us and/or the
R-grahpics gallery aimed at useful snippets not worthy
of packages....

MWG:
> > I worked from Helsel's description in his classic
> water
> > resources statistics book. See Chapter 10 here: 
> > 
> > http://water.usgs.gov/pubs/twri/twri4a3/

TH:
> 
> The method goes back a lot further than suggested
> here.

So it seems. After all it has to have been around to
acquire all the different names it goes by. The USGS
book is just good as an online reference. BTW read
'classic' as useful but out of print. I listed the
material because I have found it quite lucid and I
like the emphasis on non-parametric methods. Making
the material available is indeed quite generous of the
authors. I find the book quite thought provoking for
the non-statistics individual. I'm always looking for
insights.

MWG: 
> > Now, if you are after confidence intervals or
> > prediction intervals, I haven't found anything on

TH:
> 
> The uncertainty properties, and indeed the
> interpretation,
> of this method are elusive. 
... 

Now you are getting to the heart of what as been
puzzling me lately. To me the question seemed to be:
does it make sense to even talk about confidence bands
and prediction bands for GMR. It seemed that one can
take a stochastic approach to prediction, i.e., one
can set up simulations and roll the dice over and
over. On one hand it is beyond my knowledge at this
time to ascertain whether or not the the results of
such effort can be couched in the traditional language
of confidence bands and prediction bands about such a
line--neither variable is (in)dependent. Yet if I view
it from the perspective of the minimization of the sum
of the areas of the right triangles (Helsel Fig. 10.8)
determined by each observation and the GMR (LOC), I am
back to a single variable(?)... Oh, well I have not
lost sleep over it, and indeed find your use of the
term 'elusive' reassuring.

> 
> At the other extreme, where there is no correlation,

Being conservative in such matters, little or no
correlation is where I declare defeat and move on to
some other tactic ;O).
> 
> 
> The GMR method seems to be well entrenched in the
> fisheries,
> ... 
> Nevertheless, I'm inclined to the view that the
> linear functional
> relationship is usually the best way to go. When the
> observed
> (x,y) points depart from the "true" points on the
> straight line
> by normally distributed amounts, the MLE of the
> relationship
> is well defined provided the ratio of the
> "departure" variances
> is fixed. Therefore it is possible to examine the
> robustness
> of the estimated relationship with respect to
> variation in the
> assumed value of this ratio. To the extent that this
> is 
> acceptably robust within plausible variation of the
> ratio,
> you have an adequate and reliable perspective.
> Otherwise,
> you have to acknowledge that your information is
> inadquate.
> 
> The danger of adopting a formulaic solution like GMY
> is that
> it tends to conceal inadequacy of information!

Hmmm, more fodder for self study. Thank you very much
for the insights!


Best regards,
Michael Grant



From p.dalgaard at biostat.ku.dk  Mon Jun  6 20:55:38 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Jun 2005 20:55:38 +0200
Subject: [R] Help package pls.pcr
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E92E@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E92E@usctmx1106.merck.com>
Message-ID: <x2r7ff8gd1.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > R : Copyright 1999, The R Development Core Team
> > Version 0.64.1  (May 8, 1999)
> 
> For starter, try upgrading R to a version from this century.

Or millennium for that matter........

> The version
> you have seems very strange:  AFAICR by 1999 R version is already close to
> 1.0.0.

No that's OK, releases were more frequent then and we had a
discontinuity at the run-in to 1.0.

The sequence of releases leading up to 1.0.0 were

 R-0.64.1.tgz            08-May-1999 02:55   1.9M  
 R-0.64.2.tgz            05-Jul-1999 21:15   1.9M  
 R-0.65.0.tgz            28-Aug-1999 00:18   2.1M  
 R-0.65.1.tgz            07-Oct-1999 01:46   2.2M  
 R-0.90.0.tgz            22-Nov-1999 18:07   2.3M  
 R-0.90.1.tgz            15-Dec-1999 14:05   2.4M  
 R-0.99.0.tgz            07-Feb-2000 13:09   2.8M  
 R-0.99.0a.tgz           09-Feb-2000 12:28   2.8M  

and of course 1.0.0 on the exceptionally exceptional Feb 29th.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmb at mrc-dunn.cam.ac.uk  Mon Jun  6 22:01:20 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 6 Jun 2005 21:01:20 +0100 (BST)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A4704F.6080504@oomvanlieshout.net>
Message-ID: <Pine.LNX.4.21.0506062059430.5170-100000@mail.mrc-dunn.cam.ac.uk>

On Mon, 6 Jun 2005, Sander Oom wrote:

>Barry Rowlingson wrote:
>> Romain Francois wrote:
>> 
>>> Graphics will be classified in :
>>>   - categories
>>>   - sub-categories within those categories
>>> So far so good.
>> 
>>  Maybe, maybe not! Would a system of keywords work better than strict 
>> hierarchical categories, as long as plots can have more than one keyword 
>> attached? Someone might be interested in all 3d plots, or all plots 
>> related to model fitting, and these categories may not be distinct!
>> 
>> Barry
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
>The whole point of a gallery is to show something to the user before the 
>user knows what he is looking for. The R help functions currently 
>available are hopeless when you have a picture of a graph in your head 
>without knowing the required commands.
>
>Thus a gallery based on some categorization is required. The gallery 
>already has a search facility for 'un-categorized' searching. At the 
>same time the gallery will provide cross links between different graphs 
>using the same command/package or other similarities.
>
>I suggested that the categorization for sigmaplot graphs could be a 
>useful start:
>http://www.systat.com/products/SigmaPlot/productinfo/?sec=1003

Also cloning the windows graph picking galery couldn't be a bad thing
(could it?)

If you have access to a recent version of excel you will see what I
mean. Click a general 'barplot' to see more specific barplots etc.

Dan.




>
>The gallery is based on a database, so keyword searching is no problem.
>
>Of course graphs will likely be members of more then one sub-category. 
>This is also no problem thanks to the database approach!
>
>Hopefully we can agree on a categorization, such that we can start 
>populating the gallery!
>
>Cheers,
>
>Sander.
>
>



From drf5n at maplepark.com  Mon Jun  6 22:30:29 2005
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 6 Jun 2005 15:30:29 -0500 (CDT)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A46CE9.8050906@free.fr>
References: <42A44259.9040309@free.fr> <x2wtp75x4y.fsf@turmalin.kubism.ku.dk>
	<42A46CE9.8050906@free.fr>
Message-ID: <Pine.LNX.4.58.0506061527430.21844@maplepark.com>

On Mon, 6 Jun 2005, Romain Francois wrote:

> Le 06.06.2005 17:21, Peter Dalgaard a ??crit :
...
> >BTW, shouldn't there a way there from http://addictedtor.free.fr ? I
> >seem to get stuck on the home page with nothing to click (except "R"
> >and the labels on the left).
> >
> >
> >
> Hello Peter,
>
> My english fails me at understanding your question. I don't get stuck
> anywhere.
>
> Romain

Perhaps the javascript menu on the left does not function on some
browsers.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From ross at biostat.ucsf.edu  Mon Jun  6 22:35:17 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Mon, 6 Jun 2005 13:35:17 -0700
Subject: [R] How to change the value of a class slot
In-Reply-To: <17057.57384.831362.960765@stat.math.ethz.ch>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
	<17057.57384.831362.960765@stat.math.ethz.ch>
Message-ID: <20050606203517.GN3134@wheat.boylan.org>

On Sat, Jun 04, 2005 at 07:08:56PM +0200, Martin Maechler wrote:
>     Ross> nextPath <- function(pm){ #pm is a CompletePathMaker
>     Ross>    pm at i <- pm at i+as.integer(1)
>     Ross> [etc]
> 
> If your nextPath   function has  'pm' as its last statement it
> will return the updated object, and if you call it
> as
> 	mypm <- nextPath(mypm)
> 
> you are
>     1) updating  mypm
>     2) in a proper S way (i.e. no cheating).
> 
> Regards,
> Martin

Wow.  This is almost the exact inverse of the usual object behavior,
in which only the class itself can update the slots (aka instance
variables).  None of the methods of the class can update instances of
the class persistently without the help of outsiders, and only
outsiders can change the slot values.

(Yes, I realize that using the idiom you suggest of returning a new
object one can have only class methods actually fiddling with the
slots.)

The inability of a class method to change a class object without
outside help seems unfortunate.

It looks as if instances of class objects are best thought of as
immutable once created.

Ross



From francoisromain at free.fr  Mon Jun  6 22:44:35 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 06 Jun 2005 22:44:35 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.58.0506061527430.21844@maplepark.com>
References: <42A44259.9040309@free.fr> <x2wtp75x4y.fsf@turmalin.kubism.ku.dk>
	<42A46CE9.8050906@free.fr>
	<Pine.LNX.4.58.0506061527430.21844@maplepark.com>
Message-ID: <42A4B5B3.6040800@free.fr>

Le 06.06.2005 22:30, David Forrest a ??crit :

>>>BTW, shouldn't there a way there from http://addictedtor.free.fr ? I
>>>seem to get stuck on the home page with nothing to click (except "R"
>>>and the labels on the left).
>>>
>>>
>>>
>>>      
>>>
>>Hello Peter,
>>
>>My english fails me at understanding your question. I don't get stuck
>>anywhere.
>>
>>Romain
>>    
>>
>
>Perhaps the javascript menu on the left does not function on some
>browsers.
>
>Dave
>  
>
Indeed, that menu doesn't work on certain versions of Mozilla.
Works ok with firefox and IE

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From drf5n at maplepark.com  Mon Jun  6 22:47:40 2005
From: drf5n at maplepark.com (David Forrest)
Date: Mon, 6 Jun 2005 15:47:40 -0500 (CDT)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.21.0506061437300.31061-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506061437300.31061-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.58.0506061536520.21844@maplepark.com>

On Mon, 6 Jun 2005, Dan Bolser wrote:

> On Mon, 6 Jun 2005, Romain Francois wrote:
...
> >It seems that the next improvement to the R Graph Gallery is
> >categorization of the graphics, that way each graph will be easier to
> >find. That step should be done *carefully* if we want to avoid the
> >opposite side-effect : graph not reachable through the categories.
...
>
> I would humbly suggest that the underlying data (graphs/categories/etc) be
> organized in such a way as to allow multiple categorizations. That way
> there could be any number of different categorizations applied to the same
> graphs, as one overall categorization may reflect only one particular
> viewpoint.

Heartily seconded!

> This way the chosen categories (however you decide to choose) would not be
> set in stone, but would be intimatly flexible.
>
> Also a single graph should be allowed to occur under multiple categories
> of a particular categorization.
...

An example of non-hierarchical categorization is the
keyword-categorization method used by Wikis.  See
http://www.usemod.com/cgi-bin/mb.pl?CategoryCategory for an example.

If a graph could be in multiple categories (3d, social science, color,
publication ready/published) it could be identified with multiple keywords
and dynamically listed in each of the sub-galleries.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From uofiowa at gmail.com  Mon Jun  6 22:43:19 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Mon, 6 Jun 2005 16:43:19 -0400
Subject: [R] names of functions in a library
Message-ID: <3f87cc6d050606134368b51090@mail.gmail.com>

How can I get a list of the names of all exported functions in a library?
I load my library using library() and then want to dynamically get all
functions that start with "test."  to dynamically execute them.



From murdoch at stats.uwo.ca  Mon Jun  6 22:55:11 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 06 Jun 2005 16:55:11 -0400
Subject: [R] names of functions in a library
In-Reply-To: <3f87cc6d050606134368b51090@mail.gmail.com>
References: <3f87cc6d050606134368b51090@mail.gmail.com>
Message-ID: <42A4B82F.4090609@stats.uwo.ca>

On 6/6/2005 4:43 PM, Omar Lakkis wrote:
> How can I get a list of the names of all exported functions in a library?
> I load my library using library() and then want to dynamically get all
> functions that start with "test."  to dynamically execute them.

Use search() to see all the _packages_ that have been loaded.  If yours 
is second in the list (the typical spot just after calling the 
unfortunately named library() function), then ls(2) will list all of its 
exports.

Duncan Murdoch



From pwaltman at cs.tufts.edu  Mon Jun  6 23:15:36 2005
From: pwaltman at cs.tufts.edu (Peter Waltman)
Date: Mon, 06 Jun 2005 17:15:36 -0400
Subject: [R] how to generate pairwise plots with data frames - tia
Message-ID: <42A4BCF8.7070604@cs.tufts.edu>

hi -

sorry for a newbie question, but I've tried to go through the 
documentation and couldn't find anything that would address my specific 
need.

in a nutshell, I have a txt file containing a data matrix with 10 
columns of data.  I would like to generate pairwise plots of the data, 
i.e. 1 column against the other 9.  Since this will produce 50 plots, 
I'd like to do it using a loop.  However, I can't figure out how to do 
it as it seems like I have to specify the column name within the plot 
function.

for example, if I have:

    test <- read.delim("matrix.txt")

and then try to generate a plot of the first 2 columns via:

    plot(test[1], test[2)

I get an error message:

    Error in pmatch(x, table, duplicates.ok) :
            argument is not of mode character

but if I specify these columns by their column headers, i.e.:

    attach(test)
    plot(Control_200, Control_201)

it works just fine.  However, with 10 columns, this will produce ~ 50 
pairwise plots, so I'd really like to figure out how to automate this.  
I've tried getting the names of the data frame and using those, i.e.:

    myNames<-names(test)
    plot(test$myNames[1], test$myNames[2])

but that doesn't work either as the myNames members are strings, not the 
variables themselves (I believe).

is it possible to get a listing of the variables themselve, not the 
names and work off of those?

thanks to anyone who can help shed some light on this,

Peter Waltman



From sarah.goslee at gmail.com  Mon Jun  6 23:19:53 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 6 Jun 2005 17:19:53 -0400
Subject: [R] how to generate pairwise plots with data frames - tia
In-Reply-To: <42A4BCF8.7070604@cs.tufts.edu>
References: <42A4BCF8.7070604@cs.tufts.edu>
Message-ID: <efb536d50506061419398ae69c@mail.gmail.com>

Peter,

Please read the intro to R available on the R web page - you've run
into a subsetting problem. The code you specified doesn't select a
column from your matrix.

>     plot(test[1], test[2)

Try:

plot(test[,1], test[,2])

since test is a two-dimensional construct.

Also see

?pairs


-- 
Sarah Goslee
http://www.stringpage.com



From efg at stowers-institute.org  Mon Jun  6 23:31:45 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Mon, 6 Jun 2005 16:31:45 -0500
Subject: [R] p.adjust documentation suggestion: simple statement that new
	"BH" method is an alias for old "fdr" method
Message-ID: <d82f4v$etm$1@sea.gmane.org>

In R 2.0.1

> p.adjust.methods
[1] "holm"       "hochberg"   "hommel"     "bonferroni" "fdr"        "none"

In R 2.1.0

> p.adjust.methods
[1] "holm"       "hochberg"   "hommel"     "bonferroni" "BH"         "BY"
"fdr"
[8] "none"

One  might conclude that two new methods "BH" and "BY" were added.  BUT,
there's a clue in one of the comments of the examples for ?p.adjust:

       ## or all of them at once (dropping the "fdr" alias):
          p.adjust.M <- p.adjust.methods[p.adjust.methods != "fdr"]

Apparently, the old "fdr" that meant "Benjamini & Hochberg" now shares an
alias with the new "BH", which is "Benjamini & Hochberg".

Wouldn't a simple statement in the online documentation be useful that
explained that "fdr" and "BH" are aliases?

Is "fdr" soon to be deprecated and eventually shouldn't be used at all and
that "BH" is the way of the future?

http://cran.r-project.org/src/base/NEWS mentions "p.adjust() has a new
method '"BY'" but was silent on the apparently new alias "BH".

efg



From choudary.jagar at swosu.edu  Mon Jun  6 23:46:17 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Mon, 6 Jun 2005 16:46:17 -0500
Subject: [R] Building a R package under Windows XP
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C230@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050606/56f4c5a6/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Tue Jun  7 00:32:01 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 6 Jun 2005 23:32:01 +0100 (BST)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.58.0506061536520.21844@maplepark.com>
Message-ID: <Pine.LNX.4.21.0506062330490.6258-100000@mail.mrc-dunn.cam.ac.uk>

On Mon, 6 Jun 2005, David Forrest wrote:

>On Mon, 6 Jun 2005, Dan Bolser wrote:
>
>> On Mon, 6 Jun 2005, Romain Francois wrote:
>...
>> >It seems that the next improvement to the R Graph Gallery is
>> >categorization of the graphics, that way each graph will be easier to
>> >find. That step should be done *carefully* if we want to avoid the
>> >opposite side-effect : graph not reachable through the categories.
>...
>>
>> I would humbly suggest that the underlying data (graphs/categories/etc) be
>> organized in such a way as to allow multiple categorizations. That way
>> there could be any number of different categorizations applied to the same
>> graphs, as one overall categorization may reflect only one particular
>> viewpoint.
>
>Heartily seconded!
>
>> This way the chosen categories (however you decide to choose) would not be
>> set in stone, but would be intimatly flexible.
>>
>> Also a single graph should be allowed to occur under multiple categories
>> of a particular categorization.
>...
>
>An example of non-hierarchical categorization is the
>keyword-categorization method used by Wikis.  See
>http://www.usemod.com/cgi-bin/mb.pl?CategoryCategory for an example.

MediaWiki has a nice system of categorization, it also makes galleries of
images which have been categorized. 

It could be nice to have a mediaWiki R page...



>
>If a graph could be in multiple categories (3d, social science, color,
>publication ready/published) it could be identified with multiple keywords
>and dynamically listed in each of the sub-galleries.
>
>Dave
>



From rolf at math.unb.ca  Tue Jun  7 00:48:34 2005
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 6 Jun 2005 19:48:34 -0300 (ADT)
Subject: [R] (Off topic.) Observed Fisher information.
Message-ID: <200506062248.j56MmYs7022561@erdos.math.unb.ca>

I have been building an R function to calculate the ***observed***
(as opposed to expected) Fisher information matrix for parameter
estimates in a rather complicated setting.  I thought I had it
working, but I am getting a result which is not positive definite.
(One negative eigenvalue.  Out of 10.)

Is it the case that the observed Fisher information must be positive
definite --- thereby indicating for certain that there are errors in
my code --- or is it possible for such a matrix not to be pos. def.?

It seems to me that if the log likelihood surface is ***not*** well
approximated by a quadratic in a neighbourhood of the maximum, then
it might well be that case that the observed information could fail
to be positive definite.  Is this known/understood?  Can anyone point
me to appropriate places in the literature?

TIA.
			cheers,

				Rolf Turner



From quinta at cwazy.co.uk  Tue Jun  7 00:53:32 2005
From: quinta at cwazy.co.uk (quinta@cwazy.co.uk)
Date: Mon, 6 Jun 2005 23:53:32 +0100 (BST)
Subject: [R] (no subject)
Message-ID: <33859.212.113.164.97.1118098412.squirrel@212.113.164.97>

dear all:

I want try the vgam function in VGAM library, but I'm stucked since I
can't define the weights.  This is because I dont understand well the
concept of "matrix-band" (even after read the reference card).
I was wondering if someone have some tutorial examples of how to define
weights on a matrix form, in VGAM.

thank you all

jim quinta



From quinta at cwazy.co.uk  Tue Jun  7 01:08:17 2005
From: quinta at cwazy.co.uk (quinta@cwazy.co.uk)
Date: Tue, 7 Jun 2005 00:08:17 +0100 (BST)
Subject: [R] VGAM weights
Message-ID: <18178.212.113.164.97.1118099297.squirrel@212.113.164.97>

sorry...its the same post but this time with subject

dear all:

I want try the vgam function in VGAM library, but I'm stucked since I
can't define the weights.  This is because I dont understand well the
concept of "matrix-band" (even after read the reference card).
I was wondering if someone have some tutorial examples of how to define
weights on a matrix form, in VGAM.

thank you all

jim quinta



From rvaradha at jhsph.edu  Tue Jun  7 01:25:07 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Mon, 6 Jun 2005 19:25:07 -0400
Subject: [R] (Off topic.) Observed Fisher information.
In-Reply-To: <200506062248.j56MmYs7022561@erdos.math.unb.ca>
Message-ID: <OWA-1VvPdLFYxc9NUpu000005be@owa-1.sph.ad.jhsph.edu>

Hi Rolf,

If your data come from exponential family of distributions, then the
log-likelihood is concave and the observed information must be positive
definite.  However, I don't think that this is the case more generally, i.e.
for families such as curved exponential families the log-likelihood doesn't
have to concave.  I remember reading something about this in
Barndorff-Nielsen and Cox's book on Inference and Asymptotics.  There may be
better references.

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Rolf Turner
> Sent: Monday, June 06, 2005 6:49 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] (Off topic.) Observed Fisher information.
> 
> I have been building an R function to calculate the ***observed***
> (as opposed to expected) Fisher information matrix for parameter
> estimates in a rather complicated setting.  I thought I had it
> working, but I am getting a result which is not positive definite.
> (One negative eigenvalue.  Out of 10.)
> 
> Is it the case that the observed Fisher information must be positive
> definite --- thereby indicating for certain that there are errors in
> my code --- or is it possible for such a matrix not to be pos. def.?
> 
> It seems to me that if the log likelihood surface is ***not*** well
> approximated by a quadratic in a neighbourhood of the maximum, then
> it might well be that case that the observed information could fail
> to be positive definite.  Is this known/understood?  Can anyone point
> me to appropriate places in the literature?
> 
> TIA.
> 			cheers,
> 
> 				Rolf Turner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From andy_liaw at merck.com  Tue Jun  7 01:24:41 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Jun 2005 19:24:41 -0400
Subject: [R] names of functions in a library
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E933@usctmx1106.merck.com>

> From: Duncan Murdoch
> 
> On 6/6/2005 4:43 PM, Omar Lakkis wrote:
> > How can I get a list of the names of all exported functions 
> in a library?
> > I load my library using library() and then want to 
> dynamically get all
> > functions that start with "test."  to dynamically execute them.
> 
> Use search() to see all the _packages_ that have been loaded. 
>  If yours 
> is second in the list (the typical spot just after calling the 
> unfortunately named library() function), then ls(2) will list 
> all of its 
> exports.

Just to nitpick a bit:  That lists all _objects_ in position 2, which may
include objects that are not functions, although it's rare, if at all, that
a package database would contain non-functions...

Andy
 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From slusek at o2.pl  Tue Jun  7 01:42:36 2005
From: slusek at o2.pl (Wojtek Slusarski)
Date: Tue, 07 Jun 2005 01:42:36 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A4B5B3.6040800@free.fr>
References: <42A44259.9040309@free.fr>
	<x2wtp75x4y.fsf@turmalin.kubism.ku.dk>	<42A46CE9.8050906@free.fr>	<Pine.LNX.4.58.0506061527430.21844@maplepark.com>
	<42A4B5B3.6040800@free.fr>
Message-ID: <42A4DF6C.1060102@o2.pl>

Romain Francois wrote:
> Le 06.06.2005 22:30, David Forrest a ??crit :
> 
>>>> BTW, shouldn't there a way there from http://addictedtor.free.fr ? I
>>>> seem to get stuck on the home page with nothing to click (except "R"
>>>> and the labels on the left).
>>>>
>>>>
>>>>
>>>>     
>>>
>>> Hello Peter,
>>>
>>> My english fails me at understanding your question. I don't get stuck
>>> anywhere.
>>>
>>> Romain
>>>   
>>
>>
>> Perhaps the javascript menu on the left does not function on some
>> browsers.
>>
>> Dave
>>  
>>
> Indeed, that menu doesn't work on certain versions of Mozilla.
> Works ok with firefox and IE
> 
> Romain
> 
Well, I have a firefox, but sometimes I am working on a bit old screen 
with 800x600 resolution and then it is not easy to use those eye candy 
javascript effects. I used to make some webpages some time ago, but I 
realized, that you never know the capabilities of end users, so you 
should try avoid such scripts, or provide some <noscript> tags.

Best regards,
Wojtek



From Peter.Watkins at foodscience.afisc.csiro.au  Tue Jun  7 01:48:55 2005
From: Peter.Watkins at foodscience.afisc.csiro.au (Peter.Watkins@foodscience.afisc.csiro.au)
Date: Tue, 7 Jun 2005 09:48:55 +1000
Subject: [R] Porting Matlab code to R
Message-ID: <CB3D0020D0BA674C99B5B7E751D1A99903AC41@exvicn1-mel.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/1d4c511a/attachment.pl

From jeff.hamann at forestinformatics.com  Tue Jun  7 01:54:42 2005
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Mon, 6 Jun 2005 16:54:42 -0700 (PDT)
Subject: [R] don't want xtab sorting "numeric" factors...
Message-ID: <2735.128.193.139.85.1118102082.squirrel@www.forestinformatics.com>

r-gurus,

I couldn't find an answer to this and after an hour or so of trying all
types or ways to do this, I've given up for now.

I'm having trouble getting the results from xtabs to generate
"unsorted" factors. I've generated a sample data.frame I want to
create a table from, using xtabs, and the results are presented below,

> temp
   treatment itpa       qmd      tht
1          0  100  7.287263 3.362501
2         25  100 10.461070 4.118217
3         50  100 16.671731 5.814391
4         75  100 24.033238 8.264608
5        100  100 27.554497 9.586341
6          0  200  7.032527 3.355118
7         25  200  9.632828 4.039846
8         50  200 14.821584 5.659913
9         75  200 20.470692 7.791525
10       100  200 22.977827 8.901247
11         0  300  6.839360 3.355788
12        25  300  9.437649 4.070102
13        50  300 13.609004 5.545991
14        75  300 18.159601 7.387135
15       100  300 20.194335 8.380818
16         0  600  6.232509 3.330317
17        25  600  8.371084 3.990557
18        50  600 11.512385 5.235263
19        75  600 14.463214 6.605290
20       100  600 16.030137 7.467160
21         0  900  5.938640 3.338342
22        25  900  7.610695 3.891203
23        50  900 10.209530 4.917330
24        75  900 12.528651 6.157436
25       100  900 13.708622 6.791764
26         0 1200  5.841058 3.389716
27        25 1200  7.174748 3.859241
28        50 1200  9.238635 4.689011
29        75 1200 11.424713 5.688340
30       100 1200 12.349254 6.275864
> t <- xtabs( temp$tht ~ temp$itpa + temp$treatment )

t <- xtabs( temp$tht ~ temp$itpa + temp$treatment )
>
> t
         temp$treatment
temp$itpa        0      100       25       50       75
     100  3.362501 9.586341 4.118217 5.814391 8.264608
     1200 3.389716 6.275864 3.859241 4.689011 5.688340
     200  3.355118 8.901247 4.039846 5.659913 7.791525
     300  3.355788 8.380818 4.070102 5.545991 7.387135
     600  3.330317 7.467160 3.990557 5.235263 6.605290
     900  3.338342 6.791764 3.891203 4.917330 6.157436
>

the factors represent real values and shouldn't be sorted as
strings. The table should read,

> t
         temp$treatment
temp$itpa        0       25       50       75      100
     100  3.362501 4.118217 5.814391 8.264608 9.586341
     200  3.355118 4.039846 5.659913 7.791525 8.901247
     300  3.355788 4.070102 5.545991 7.387135 8.380818
     600  3.330317 3.990557 5.235263 6.605290 7.467160
     900  3.338342 3.891203 4.917330 6.157436 6.791764
     1200 3.389716 3.859241 4.689011 5.688340 6.275864
>

this wouldn't be that big of an issue if I only did this once for the
results, but I'm using this in Sweave and need create the tables after
thousands of iterations.

Any help would be greatly appreciated.

Thanks,
Jeff.

-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
www.forestinformatics.com



From chabotd at globetrotter.net  Tue Jun  7 02:00:56 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Mon, 06 Jun 2005 20:00:56 -0400
Subject: [R] how to create a variable to rank within subgroups
Message-ID: <9DA4A094-8D77-4EB5-B8E1-0CD2A42CDBB8@globetrotter.net>

Hi,

I would like to create a new variable that would be the rank of a  
continuous variable within each level of a grouping variable. Say the  
first level of the grouping variable has 5 observations, I'd like  
them ranked from one to five and a new variable would hold the rank  
value (one to five). So forth for each level of the grouping variable.

I'm quite new with R and cannot figure out this one by myself.

Thanks in advance,

Denis Chabot



From sdavis2 at mail.nih.gov  Tue Jun  7 03:17:41 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Mon, 6 Jun 2005 21:17:41 -0400
Subject: [R] Porting Matlab code to R
References: <CB3D0020D0BA674C99B5B7E751D1A99903AC41@exvicn1-mel.nexus.csiro.au>
Message-ID: <00c901c56afe$b375b3c0$5179f345@WATSON>

Peter,

I haven't used this package, but have you seen:

http://www.omegahat.org/RMatlab/

Sean

----- Original Message ----- 
From: <Peter.Watkins at foodscience.afisc.csiro.au>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 06, 2005 7:48 PM
Subject: [R] Porting Matlab code to R


Hello,



I'm looking at porting code written for Matlab to R.

However, I don't have sufficient knowledge of Matlab to know whether
it's feasible or appropriate.

If there is anyone who has any experience in this and could provide some
advice, I'd be grateful.



TIA, Peter


[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jun  7 03:21:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 6 Jun 2005 21:21:33 -0400
Subject: [R] how to create a variable to rank within subgroups
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E934@usctmx1106.merck.com>

Try something like:

> g <- gl(4, 5)
> x <- sample(20)
> d <- data.frame(g, x)
> d
   g  x
1  1 10
2  1  3
3  1 11
4  1 12
5  1 20
6  2 13
7  2  6
8  2  2
9  2  1
10 2 14
11 3 17
12 3 15
13 3 16
14 3 19
15 3  9
16 4  4
17 4  7
18 4  5
19 4  8
20 4 18
> d$xrank <- ave(d$x, d$g, FUN=rank)
> d
   g  x xrank
1  1 10     2
2  1  3     1
3  1 11     3
4  1 12     4
5  1 20     5
6  2 13     4
7  2  6     3
8  2  2     2
9  2  1     1
10 2 14     5
11 3 17     4
12 3 15     2
13 3 16     3
14 3 19     5
15 3  9     1
16 4  4     1
17 4  7     3
18 4  5     2
19 4  8     4
20 4 18     5

Andy

> From: Denis Chabot
> 
> Hi,
> 
> I would like to create a new variable that would be the rank of a  
> continuous variable within each level of a grouping variable. 
> Say the  
> first level of the grouping variable has 5 observations, I'd like  
> them ranked from one to five and a new variable would hold the rank  
> value (one to five). So forth for each level of the grouping variable.
> 
> I'm quite new with R and cannot figure out this one by myself.
> 
> Thanks in advance,
> 
> Denis Chabot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From tlumley at u.washington.edu  Tue Jun  7 03:35:07 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Jun 2005 18:35:07 -0700 (PDT)
Subject: [R] write.dta limits
In-Reply-To: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>

On Mon, 6 Jun 2005, Jean Eid wrote:

> Hope everyone id doing great ..
>
> Just need some clarification over the limit of write.dta. I have some
> coauthors that use stata and I need to send them my data in .dta format.
> the data.frame is 41706x229 and I get the following
>
> Error in write.dta(Panel, file = "STATADATA/Panel.dta", version = 7) :
> 	a binary write error occured
>
>
> Once I subset the data everything works out fine. my question is what are
> the limits of write.dta. I tried to find out but no luck..


There aren't supposed to be any built-in limits. The error message that 
you report means that the low-level operating system calls to write data 
gave an error, so if a limit was hit it was in the operating system (?disk 
full)

 	-thomas



From MSchwartz at mn.rr.com  Tue Jun  7 04:07:17 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 06 Jun 2005 21:07:17 -0500
Subject: [R] don't want xtab sorting "numeric" factors...
In-Reply-To: <2735.128.193.139.85.1118102082.squirrel@www.forestinformatics.com>
References: <2735.128.193.139.85.1118102082.squirrel@www.forestinformatics.com>
Message-ID: <1118110037.24701.79.camel@localhost.localdomain>

On Mon, 2005-06-06 at 16:54 -0700, Jeff D. Hamann wrote:
> r-gurus,
> 
> I couldn't find an answer to this and after an hour or so of trying all
> types or ways to do this, I've given up for now.
> 
> I'm having trouble getting the results from xtabs to generate
> "unsorted" factors. I've generated a sample data.frame I want to
> create a table from, using xtabs, and the results are presented below,
> 
> > temp
>    treatment itpa       qmd      tht
> 1          0  100  7.287263 3.362501
> 2         25  100 10.461070 4.118217
> 3         50  100 16.671731 5.814391
> 4         75  100 24.033238 8.264608
> 5        100  100 27.554497 9.586341
> 6          0  200  7.032527 3.355118
> 7         25  200  9.632828 4.039846
> 8         50  200 14.821584 5.659913
> 9         75  200 20.470692 7.791525
> 10       100  200 22.977827 8.901247
> 11         0  300  6.839360 3.355788
> 12        25  300  9.437649 4.070102
> 13        50  300 13.609004 5.545991
> 14        75  300 18.159601 7.387135
> 15       100  300 20.194335 8.380818
> 16         0  600  6.232509 3.330317
> 17        25  600  8.371084 3.990557
> 18        50  600 11.512385 5.235263
> 19        75  600 14.463214 6.605290
> 20       100  600 16.030137 7.467160
> 21         0  900  5.938640 3.338342
> 22        25  900  7.610695 3.891203
> 23        50  900 10.209530 4.917330
> 24        75  900 12.528651 6.157436
> 25       100  900 13.708622 6.791764
> 26         0 1200  5.841058 3.389716
> 27        25 1200  7.174748 3.859241
> 28        50 1200  9.238635 4.689011
> 29        75 1200 11.424713 5.688340
> 30       100 1200 12.349254 6.275864
> > t <- xtabs( temp$tht ~ temp$itpa + temp$treatment )
> 
> t <- xtabs( temp$tht ~ temp$itpa + temp$treatment )
> >
> > t
>          temp$treatment
> temp$itpa        0      100       25       50       75
>      100  3.362501 9.586341 4.118217 5.814391 8.264608
>      1200 3.389716 6.275864 3.859241 4.689011 5.688340
>      200  3.355118 8.901247 4.039846 5.659913 7.791525
>      300  3.355788 8.380818 4.070102 5.545991 7.387135
>      600  3.330317 7.467160 3.990557 5.235263 6.605290
>      900  3.338342 6.791764 3.891203 4.917330 6.157436
> >
> 
> the factors represent real values and shouldn't be sorted as
> strings. The table should read,
> 
> > t
>          temp$treatment
> temp$itpa        0       25       50       75      100
>      100  3.362501 4.118217 5.814391 8.264608 9.586341
>      200  3.355118 4.039846 5.659913 7.791525 8.901247
>      300  3.355788 4.070102 5.545991 7.387135 8.380818
>      600  3.330317 3.990557 5.235263 6.605290 7.467160
>      900  3.338342 3.891203 4.917330 6.157436 6.791764
>      1200 3.389716 3.859241 4.689011 5.688340 6.275864
> >
> 
> this wouldn't be that big of an issue if I only did this once for the
> results, but I'm using this in Sweave and need create the tables after
> thousands of iterations.
> 
> Any help would be greatly appreciated.
> 
> Thanks,
> Jeff.


Jeff,

Is 'itpa' supposed to be a factor?  

When using your data above, read into R using read.table, treatment and
itpa are integers and the others are doubles:

> str(temp)
`data.frame':   30 obs. of  4 variables:
 $ treatment: int  0 25 50 75 100 0 25 50 75 100 ...
 $ itpa     : int  100 100 100 100 100 200 200 200 200 200 ...
 $ qmd      : num   7.29 10.46 16.67 24.03 27.55 ...
 $ tht      : num  3.36 4.12 5.81 8.26 9.59 ...


In that case, I get:

> t <- xtabs(temp$tht ~ temp$itpa + temp$treatment)
> t
         temp$treatment
temp$itpa        0       25       50       75      100
     100  3.362501 4.118217 5.814391 8.264608 9.586341
     200  3.355118 4.039846 5.659913 7.791525 8.901247
     300  3.355788 4.070102 5.545991 7.387135 8.380818
     600  3.330317 3.990557 5.235263 6.605290 7.467160
     900  3.338342 3.891203 4.917330 6.157436 6.791764
     1200 3.389716 3.859241 4.689011 5.688340 6.275864


If I convert itpa to a factor with levels as you seem to have, I get:

> temp$itpa <- factor(as.character(temp$itpa))
> temp$itpa
 [1] 100  100  100  100  100  200  200  200  200  200  300  300  300
[14] 300  300  600  600  600  600  600  900  900  900  900  900  1200
[27] 1200 1200 1200 1200
Levels: 100 1200 200 300 600 900

> temp$itpa <- factor(as.character(temp$itpa))
> t <- xtabs( temp$tht ~ temp$itpa + temp$treatment)
> t
         temp$treatment
temp$itpa        0       25       50       75      100
     100  3.362501 4.118217 5.814391 8.264608 9.586341
     1200 3.389716 3.859241 4.689011 5.688340 6.275864
     200  3.355118 4.039846 5.659913 7.791525 8.901247
     300  3.355788 4.070102 5.545991 7.387135 8.380818
     600  3.330317 3.990557 5.235263 6.605290 7.467160
     900  3.338342 3.891203 4.917330 6.157436 6.791764


So it would seem that you either need to change itpa back to a numeric
value in the data frame:

> temp$itpa <- as.numeric(as.character(temp$itpa))
> t <- xtabs(temp$tht ~ temp$itpa + temp$treatment)
> t
         temp$treatment
temp$itpa        0       25       50       75      100
     100  3.362501 4.118217 5.814391 8.264608 9.586341
     200  3.355118 4.039846 5.659913 7.791525 8.901247
     300  3.355788 4.070102 5.545991 7.387135 8.380818
     600  3.330317 3.990557 5.235263 6.605290 7.467160
     900  3.338342 3.891203 4.917330 6.157436 6.791764
     1200 3.389716 3.859241 4.689011 5.688340 6.275864


OR


If you need itpa to be a factor, modify the factor levels, since they
are now ordered by the character values, not the numeric values:


> temp$itpa <- factor(temp$itpa, levels = c(100, 200, 300, 600, 900,
                                            1200))
> temp$itpa
 [1] 100  100  100  100  100  200  200  200  200  200  300  300  300
[14] 300  300  600  600  600  600  600  900  900  900  900  900  1200
[27] 1200 1200 1200 1200
Levels: 100 200 300 600 900 1200

> t <- xtabs(temp$tht ~ temp$itpa + temp$treatment)
> t
         temp$treatment
temp$itpa        0       25       50       75      100
     100  3.362501 4.118217 5.814391 8.264608 9.586341
     200  3.355118 4.039846 5.659913 7.791525 8.901247
     300  3.355788 4.070102 5.545991 7.387135 8.380818
     600  3.330317 3.990557 5.235263 6.605290 7.467160
     900  3.338342 3.891203 4.917330 6.157436 6.791764
     1200 3.389716 3.859241 4.689011 5.688340 6.275864



HTH,

Marc Schwartz



From jholt at uoguelph.ca  Tue Jun  7 04:08:49 2005
From: jholt at uoguelph.ca (John D. Holt)
Date: Mon,  6 Jun 2005 22:08:49 -0400
Subject: [R] (Off topic.) Observed Fisher information.
Message-ID: <1118110129.42a501b148e3e@webmail.uoguelph.ca>

If you compute the observed information for sigma from a sample of size 1
from a N(0,sigma^2) distribution, you will find that the observed information
can be negative definite with a high probability. So it can happen!

Cheers!
John Holt


-----------------------------------------------------
I have been building an R function to calculate the ***observed***
(as opposed to expected) Fisher information matrix for parameter
estimates in a rather complicated setting.  I thought I had it
working, but I am getting a result which is not positive definite.
(One negative eigenvalue.  Out of 10.)

Is it the case that the observed Fisher information must be positive
definite --- thereby indicating for certain that there are errors in
my code --- or is it possible for such a matrix not to be pos. def.?

It seems to me that if the log likelihood surface is ***not*** well
approximated by a quadratic in a neighbourhood of the maximum, then
it might well be that case that the observed information could fail
to be positive definite.  Is this known/understood?  Can anyone point
me to appropriate places in the literature?

TIA.
cheers,

Rolf Turner
-----------------------------------------------------


John Holt, Ph.D.
Dept. Mathematics and Statistics
University of Guelph
Guelph, ON
N1G 2W1

Tel 519-824-4120Ext53297/52155



From spluque at gmail.com  Tue Jun  7 04:11:51 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Mon, 06 Jun 2005 21:11:51 -0500
Subject: [R] intervals and data frames
References: <200506061551.j56FpEVL007969@gator.dt.uh.edu>
Message-ID: <87br6iq5js.fsf@gmail.com>

Hi,

I thought this would be close to what you wanted:

x <- seq(-1,1,.1)
y <- sample(x, 3)
r1 <- vector(mode = "numeric", length = length(x))
sapply(y, function(k) {
  y1 <- max(k - 0.1, -1)
  y2 <- min(k + 0.1, 1)
  r1[x >= y1 & x <= y2] <- 1
  r1
})

but found that it's not replacing 3 elements of r1 consistently. There
seems to be a problem with indexing in that last assignment. Can somebody
please explain why indexing by such a logical vector doesn't work here?


Regards,
Sebastian


Erin Hodgess <hodgess at gator.dt.uh.edu> wrote:
> Dear R people:
>
> I have a vector which runs from -1 to 1, by .1, inclusively.
>
> Easy to set up.  x <- seq(-1,1,.1)
>
> I then sample 3 numbers from x.
> y <- sample(x, 3)
>
> Suppose one of my values is -0.7.  I want to set up an interval
> around that
> y1 <- pmax(y-0.1,-1)
> y2 <- pmin(y+0.1,1)
> For the value -.7, the interval will run from -.8 to -.6.
> There will be several intervals.
>
> Again, nothing interesting so far.
>
> However, now I want to set up something that I will call
> r1.  Essentially, r1 "runs" between -1 and 1 as well.
> I want to place a value in r1 between -.8 and -.6 for my interval,
> and zeros elsewhere.
>
> I'm trying to use a data frame in which the first column represents
> the values from -1 to 1.  The remaining columns are initially set to
> zero.  Also, I'm trying to avoid loops.
>
> Any suggestions would be much appreciated.
>
> Thanks in advance,
> Sincerely
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
>
> ______________________________________________ R-help at stat.math.ethz.ch
> mailing list https://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read
> the posting guide! http://www.R-project.org/posting-guide.html

-- 
Sebastian P. Luque



From ggrothendieck at gmail.com  Tue Jun  7 04:36:14 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 6 Jun 2005 22:36:14 -0400
Subject: [R] How to vectorize
In-Reply-To: <BF8E83A574F8FF40BD5C0266B2A4DF880153C1E1@canna.proton.intra.irsn.fr>
References: <BF8E83A574F8FF40BD5C0266B2A4DF880153C1E1@canna.proton.intra.irsn.fr>
Message-ID: <971536df050606193673641113@mail.gmail.com>

On 6/6/05, PANTERA Laurent <laurent.pantera at irsn.fr> wrote:
> Dear R-List,
> 
>     I would like to write nicely the names of some isotopes on a plot. The
> code bellow works fine.
> 
> plot(1:10,1:10)
> text(c(2,4,8),c(2,4,8),labels=c(expression(italic(phantom(0)^{78}*Ge)),
>                                expression(italic(phantom(0)^{137}*Cs)),
>                                expression(italic(phantom(0)^{129*m}*Te))),
>     cex=3
>     )
> 
> But, since I have a lot of isotopes to write on the plot, I would like
> to construct automatically the labels. So I wrote the code below which
> works fine.
> 
> listenoms <- list(nom=c("Ge","Cs","Te"),num=c("78","137","129*m"))
> n <- length(listenoms$nom)
> resu <- "c("
> for( i in 1:(n-1))
>  {
>    resu <- paste(resu,paste("expression(italic(phantom(0)^{",
>                         listenoms$num[i],"}*",
>                         listenoms$nom[i],")),",sep=""))
> }
> resu <- paste(resu,paste("expression(italic(phantom(0)^{",
>                         listenoms$num[n],"}*",
>                         listenoms$nom[n],")))",sep=""))
> plot(1:10,1:10)
> text(c(2,4,8),c(2,4,8),labels=eval(parse(text=resu)),cex=2)
> 
> I assume there is a better way to do that using vectorization.
> May you help me to find it ?
> 

Assuming your setup:

x <- c(2,4,8)
nom <- c("Ge","Cs","Te")
num <- c("78","137","129*m")
plot(1:10)

# the ith label can be generated via
lab <- function(i)
  as.expression(bquote(italic(phantom(0)^{.(num[i])}*.(nom[i]))))

# giving the following vectorized solution
labs <- lapply(seq(x), lab)
text(x,x,do.call(c,labs))

# although a 'for' loop is arguably simpler
plot(1:10)
for(i in seq(x)) text(x[i], x[i], lab(i))



From stryker.a at comcast.net  Tue Jun  7 05:56:17 2005
From: stryker.a at comcast.net (Andrew Stryker)
Date: Mon, 6 Jun 2005 20:56:17 -0700
Subject: [R] write.dta limits
In-Reply-To: <Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>
References: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
	<Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>
Message-ID: <20050607035617.GA8066@localhost.localdomain>

Thomas Lumley <tlumley at u.washington.edu> wrote on 2005-Jun-06:
> On Mon, 6 Jun 2005, Jean Eid wrote:
> 
> >Hope everyone id doing great ..
> >
> >Just need some clarification over the limit of write.dta. I have some
> >coauthors that use stata and I need to send them my data in .dta format.
> >the data.frame is 41706x229 and I get the following
> >
> >Error in write.dta(Panel, file = "STATADATA/Panel.dta", version = 7) :
> >	a binary write error occured
> >
> >
> >Once I subset the data everything works out fine. my question is what are
> >the limits of write.dta. I tried to find out but no luck..
> 
> 
> There aren't supposed to be any built-in limits. The error message that 
> you report means that the low-level operating system calls to write data 
> gave an error, so if a limit was hit it was in the operating system (?disk 
> full)
> 
> 	-thomas

I have encountered the same error on Windows XP systems when
trying to write large data frame to Stata files.  Using R 2.1 and
the current foreign package.  The disk was not full.

Andrew



From 0034058 at fudan.edu.cn  Tue Jun  7 06:08:22 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 07 Jun 2005 12:08:22 +0800
Subject: [R] write.dta limits
Message-ID: <0IHP0067Z4CPYG@mail.fudan.edu.cn>

i can write the file to dta.
> a<-matrix(rnorm(41706*229),nrow=41706)
> a<-as.data.frame(a)
> write.dta(a,file="c:\\new.dta")
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   Patched        
major    2              
minor    1.0            
year     2005           
month    05             
day      29             
language R              

OS:win 2K

	

======= 2005-06-07 11:56:17 ÅƒÅ˙Å‘Å⁄Å¿Å¥Å–Å≈Å÷Å–Å–Å¥ÅµÅ¿Å£Å∫=======

>Thomas Lumley <tlumley at u.washington.edu> wrote on 2005-Jun-06:
>> On Mon, 6 Jun 2005, Jean Eid wrote:
>> 
>> >Hope everyone id doing great ..
>> >
>> >Just need some clarification over the limit of write.dta. I have some
>> >coauthors that use stata and I need to send them my data in .dta format.
>> >the data.frame is 41706x229 and I get the following
>> >
>> >Error in write.dta(Panel, file = "STATADATA/Panel.dta", version = 7) :
>> >	a binary write error occured
>> >
>> >
>> >Once I subset the data everything works out fine. my question is what are
>> >the limits of write.dta. I tried to find out but no luck..
>> 
>> 
>> There aren't supposed to be any built-in limits. The error message that 
>> you report means that the low-level operating system calls to write data 
>> gave an error, so if a limit was hit it was in the operating system (?disk 
>> full)
>> 
>> 	-thomas
>
>I have encountered the same error on Windows XP systems when
>trying to write large data frame to Stata files.  Using R 2.1 and
>the current foreign package.  The disk was not full.
>
>Andrew
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

= = = = = = = = = = = = = = = = = = = =
			

Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å÷Å¬
Å¿ÅÒÅ£Å°
 
				 
Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°ronggui
Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°0034058 at fudan.edu.cn
Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°Å°2005-06-07



From stryker.a at comcast.net  Tue Jun  7 07:20:12 2005
From: stryker.a at comcast.net (Andrew Stryker)
Date: Mon, 6 Jun 2005 22:20:12 -0700
Subject: [R] write.dta limits
In-Reply-To: <0IHP0067Z4CPYG@mail.fudan.edu.cn>
References: <0IHP0067Z4CPYG@mail.fudan.edu.cn>
Message-ID: <20050607052012.GB8066@localhost.localdomain>

ronggui <0034058 at fudan.edu.cn> wrote on 2005-Jun-07:
> i can write the file to dta.
> > a<-matrix(rnorm(41706*229),nrow=41706)
> > a<-as.data.frame(a)
> > write.dta(a,file="c:\\new.dta")
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status   Patched        
> major    2              
> minor    1.0            
> year     2005           
> month    05             
> day      29             
> language R              
> 
> OS:win 2K

Here is an example from a Linux system; I have had similar results
with Windows systems.

> dim(hhtrips)
> library(foreign)
> write.dta(hhtrips, 'tmp.dta')
Error in write.dta(hhtrips, "tmp.dta") : a binary write error occurred
> version
	_
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

And here is part of the data:

> hhtrips[1:3,]
    sampn perno dayno plano wake1 wake2   locno ptype lu.type lutypo act1 act2
1 1001036     1     1     1   700    NA 9001036     1       8           3    0
2 1001036     1     1     2    NA    NA    1698     5       2          16    0
3 1001036     1     1     3    NA    NA 9001036     1       8           3    0
  act3 act4 act1o act2o arrtim deptim mode othmode party hh.mem per.trp nonhh
1   NA   NA                300   1500   NA            NA     NA            NA
2   NA   NA               1505   1630    3             0     NA             0
3   NA   NA               1635    259    3             0     NA             0
  hhveh vehno highway board bsarr acmod acmoo exit bsdep egmod egmoo route fare
1    NA    NA      NA          NA    NA               NA    NA               NA
2     1     1      NA          NA    NA               NA    NA               NA
3     1     1      NA          NA    NA               NA    NA               NA
  fpmt fpmto trpdur actdur spdflag  hhaddr areatype        phone lang bikes
1   NA           NA    720      NA 9001036        3 480-940-1364    1     0
2   NA            5     85       0 9001036        3 480-940-1364    1     0
3   NA            5    624       0 9001036        3 480-940-1364    1     0
  hhsize ethn o.ethn wrkr dweltype dwelo own o.own tenure comptr internet
1      1    1           1        1         1            2      1        1
2      1    1           1        1         1            2      1        1
3      1    1           1        1         1            2      1        1
  intacc intacco phlns faxmodem nophn length incat income oinc oinco cellp
1      1             1        0     2     NA     2      6   NA           1
2      1             1        0     2     NA     2      6   NA           1
3      1             1        0     2     NA     2      6   NA           1
  recont email emaila diffphon assn dayofwk1 dayofwk2 nstud hhpld1 hhpld2
1      1     9                  178        1        2     0      3      3
2      1     9                  178        1        2     0      3      3
3      1     9                  178        1        2     0      3      3
  pltotal hhtrpd1 hhtrpd2 trtotal  expfact weight X.merge
1       6       2       2       4 278.8232  0.957       3
2       6       2       2       4 278.8232  0.957       3
3       6       2       2       4 278.8232  0.957       3

Also, there is space left on the disk:

Filesystem    Type    Size  Used Avail Use% Mounted on
/dev/hda2     ext3     16G  2.2G   13G  15% /
tmpfs        tmpfs    443M     0  443M   0% /dev/shm
/dev/hda1     ext2     89M  8.8M   75M  11% /boot
/dev/hdb3 reiserfs    131G   27G  104G  21% /home
/dev       unknown     16G  2.2G   13G  15% /.dev
none         tmpfs    5.0M  2.9M  2.2M  57% /dev

Andrew



From david.whiting at ncl.ac.uk  Tue Jun  7 08:56:06 2005
From: david.whiting at ncl.ac.uk (David Whiting)
Date: Tue, 07 Jun 2005 06:56:06 +0000
Subject: [R] write.dta limits
In-Reply-To: <Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>
	(Thomas
	Lumley's message of "Mon, 6 Jun 2005 18:35:07 -0700 (PDT)")
References: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
	<Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>
Message-ID: <m24qcazmd5.fsf@ganymede.home.net>

Thomas Lumley <tlumley at u.washington.edu> writes:

> On Mon, 6 Jun 2005, Jean Eid wrote:
>
>> Hope everyone id doing great ..
>>
>> Just need some clarification over the limit of write.dta. I have some
>> coauthors that use stata and I need to send them my data in .dta format.
>> the data.frame is 41706x229 and I get the following
>>
>> Error in write.dta(Panel, file = "STATADATA/Panel.dta", version = 7) :
>> 	a binary write error occured
>>
>>
>> Once I subset the data everything works out fine. my question is what are
>> the limits of write.dta. I tried to find out but no luck..
>
>
> There aren't supposed to be any built-in limits. The error message
> that you report means that the low-level operating system calls to
> write data gave an error, so if a limit was hit it was in the
> operating system (?disk full)
>
>  	-thomas


When I have encountered this error message in the past seems to have
resulted from a blank/empty level in a factor or an empty character.
For example:

> library(foreign)
> x <- data.frame(x=c("A", "B", "C"), y=c(1,2,3))
> write.dta(x, file="temp.dta")
> levels(x$x)[2]
[1] "B"
> levels(x$x)[2] <- ""
> write.dta(x, file="temp.dta")
Error in write.dta(x, file = "temp.dta") : 
	a binary write error occurred


My work-around at the time was to go through the data replacing ""
with something else that I could then deal with later. 


-- 
David Whiting
University of Newcastle upon Tyne, UK



From ligges at statistik.uni-dortmund.de  Tue Jun  7 08:36:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Jun 2005 08:36:25 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <20050606203517.GN3134@wheat.boylan.org>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>	<17057.57384.831362.960765@stat.math.ethz.ch>
	<20050606203517.GN3134@wheat.boylan.org>
Message-ID: <42A54069.6090108@statistik.uni-dortmund.de>

Ross Boylan wrote:

> On Sat, Jun 04, 2005 at 07:08:56PM +0200, Martin Maechler wrote:
> 
>>    Ross> nextPath <- function(pm){ #pm is a CompletePathMaker
>>    Ross>    pm at i <- pm at i+as.integer(1)
>>    Ross> [etc]
>>
>>If your nextPath   function has  'pm' as its last statement it
>>will return the updated object, and if you call it
>>as
>>	mypm <- nextPath(mypm)
>>
>>you are
>>    1) updating  mypm
>>    2) in a proper S way (i.e. no cheating).
>>
>>Regards,
>>Martin
> 
> 
> Wow.  This is almost the exact inverse of the usual object behavior,
> in which only the class itself can update the slots (aka instance
> variables).  None of the methods of the class can update instances of
> the class persistently without the help of outsiders, and only
> outsiders can change the slot values.
> 
> (Yes, I realize that using the idiom you suggest of returning a new
> object one can have only class methods actually fiddling with the
> slots.)
> 
> The inability of a class method to change a class object without
> outside help seems unfortunate.
> 
> It looks as if instances of class objects are best thought of as
> immutable once created.

Obviously, there are many definition of "object oriented" programming, 
and yours seems to be different from the S4 definition.

I was going to answer your first question at first, but you have not 
given enough details - in particular it was not clear to me why your 
approach did not work. I assumed that you are assigning the new object 
again, which is the S way. You have to think about scoping rules and it 
will be clear that the approach you are expecting is not a clean one in S.

Uwe Ligges


> Ross
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Tue Jun  7 08:54:24 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 07 Jun 2005 07:54:24 +0100
Subject: [R] Porting Matlab code to R
In-Reply-To: <CB3D0020D0BA674C99B5B7E751D1A99903AC41@exvicn1-mel.nexus.csiro.au>
References: <CB3D0020D0BA674C99B5B7E751D1A99903AC41@exvicn1-mel.nexus.csiro.au>
Message-ID: <42A544A0.5060202@ucl.ac.uk>

Peter.Watkins at foodscience.afisc.csiro.au wrote:
> Hello,
> 
>  
> 
> I'm looking at porting code written for Matlab to R. 
> 
> However, I don't have sufficient knowledge of Matlab to know whether
> it's feasible or appropriate.
> 
> If there is anyone who has any experience in this and could provide some
> advice, I'd be grateful.
> 
>  
> 
> TIA, Peter

I guess this depends on what the Matlab code is for and what it does, 
whether it uses any commercial toolboxes etc.

I recently ported some Matlab code fairly easily, with the exception of 
the parts of the Matlab code requiring a toolbox I didn't have access to.

You might find Rob Hankin's R and Octave contributed document, which 
lists many Octave (Matlab) functions and their R equivalents:

http://cran.r-project.org/doc/contrib/R-and-octave-2.txt

I found this very helpful.

HTH

Gavin



From ripley at stats.ox.ac.uk  Tue Jun  7 09:09:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Jun 2005 08:09:14 +0100 (BST)
Subject: [R] names of functions in a library
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E933@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E933@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0506070758450.11231@gannet.stats>

On Mon, 6 Jun 2005, Liaw, Andy wrote:

>> From: Duncan Murdoch
>>
>> On 6/6/2005 4:43 PM, Omar Lakkis wrote:
>>> How can I get a list of the names of all exported functions
>> in a library?
>>> I load my library using library() and then want to
>> dynamically get all
>>> functions that start with "test."  to dynamically execute them.
>>
>> Use search() to see all the _packages_ that have been loaded.
>>  If yours
>> is second in the list (the typical spot just after calling the
>> unfortunately named library() function), then ls(2) will list
>> all of its
>> exports.
>
> Just to nitpick a bit:  That lists all _objects_ in position 2, which may
> include objects that are not functions, although it's rare, if at all, that
> a package database would contain non-functions...

It's actually common: lazy-loaded datasets are there too, and 'base' has

[1] "F"          "LETTERS"        "R.version"      "R.version.string"
[5] "T"          "letters"        "month.abb"      "month.name"
[9] "pi"         "version"

'stats' has "p.adjust.methods" ....

Here's a simple function to find only the functions

lsf <- function(n=2)
{
     tmp <- ls(n, all=TRUE)
     isf <- sapply(tmp, function(x) is.function(get(x, pos=n)))
     tmp[isf]
}

and one could use the 'pattern' arg of ls() to restrict the set.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ajayshah at mayin.org  Tue Jun  7 09:18:24 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 7 Jun 2005 12:48:24 +0530
Subject: [R] R and MLE
Message-ID: <20050607071824.GA7577@lubyanka.local>

I learned R & MLE in the last few days. It is great! I wrote up my
explorations as

  http://www.mayin.org/ajayshah/KB/R/mle/mle.html

I will be most happy if R gurus will look at this and comment on how
it can be improved.



I have a few specific questions:

* Should one use optim() or should one use stats4::mle()?

  I felt that mle() wasn't adding much value compared with optim, and
  in addition, I wasn't able to marry my likelihood functions to it.

* One very nice feature of mle() is that you can specify a few
  parameters which should be fixed in the estimation. How can one
  persuade optim() to behave like that?

* Can one use deriv() and friends to get analytical derivatives of
  these likelihood functions? I found I wasn't able to make headway
  when I was using vector/matrix notation. I think the greatness of R
  lies in a lovely vector/matrix notation, and it seems like a shame
  to have to not use that when trying to do deriv().

* For iid problems, the computation of the likelihood function and
  it's gradient vector are inherently parallelisable. How would one go
  about doing this within R?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From guido.bongi at iro.pg.cnr.it  Mon Jun  6 19:02:04 2005
From: guido.bongi at iro.pg.cnr.it (bongi)
Date: Mon, 06 Jun 2005 19:02:04 +0200
Subject: [R] red noise
Message-ID: <084a01c56ab9$76cb34d0$1696a8c0@irobam2>

help


Dear sirs,I'd like to add a red noise spectra to a periodogram:


#given z as time series of 61 values
>z
[1] 10500.0000 12044.6667 13589.3333 12222.0000  9132.6667  7000.0000
 [7]  7000.0000  5464.6667  2735.1733  1048.3200 18843.0667 38235.2533
[13] 37907.4800 19892.2533  4269.0533  4524.4000  5889.5133  5756.3000
[19]  2924.4000  2527.2000  2011.0000  1054.2000   127.4000  7005.6000
[25] 14607.6000 27384.0000 45817.1000 54225.6000 39942.5000 25961.3667
[31] 22606.0000 12321.1333  7731.2667 23948.4000 39990.2000 25766.3333
[37]   473.6000   788.8933  1131.9000  3209.7800  6968.5000  8114.1667
[43]  4855.4000 10016.6667 10944.9333 11314.0000 12890.2667 13107.4667
[49]  6314.0000  2320.6667  2277.0000  4142.0000  7451.6667  8884.9333
[55]  7292.0000  1407.0000  1142.5333  7927.2000  8964.1333  2380.9227
[61]    28.0000



> s3 <- spec.ar(z) # estimate(plot) spectral density
> s3$spec # retrieves 500 points of powerdensity
> s3$freq #and their f
> plot(s3$freq,s3$spec,type="l") # produces another plot of s3, but i was
unable to find a method for red noise

Guido Bongi CNR ISAFM PERUGIA ITALY



From ligges at statistik.uni-dortmund.de  Tue Jun  7 11:25:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Jun 2005 11:25:15 +0200
Subject: [R] Building a R package under Windows XP
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C230@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C230@swosu-mbx01.admin.swosu.edu>
Message-ID: <42A567FB.60900@statistik.uni-dortmund.de>

Jagarlamudi, Choudary wrote:

> Hi all,
>  
>  I downloaded all the required r-tools from Dr.Brian Ripley's website. RCMD build dAnal

Which version of R?

The tools are no longer hosted on Brian Ripley's website, but on the one 
from Duncan Murdoch. So looks like your set of tools might be outdated.

>  It built the tar file for me.
> After which i tried RCMD check dAnal and i get the following error.
> Making package dAnal
> adding build stamp to DESCRIPTION file
> installing R files
> installing data files
> installing man source files
> installing indices
> Error in .find.package(package,lib.loc,verbose=verbose): none of the packages were found

Does R CMD INSTALL package work for you? If not, R CMD check makes no 
sense so far.

Uwe Ligges



> Execution halted.
> ...
> ...
> Any help will be greatly appreciated.
> Thanks in advance.
>  
>  
>  
> Choudary Jagarlamudi
> Instructor
> Southwestern Oklahoma State University
> STF 254
> 100 campus Drive
> Weatherford OK 73096
> Tel 580-774-7136
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From SAULEAUEA at ch-mulhouse.fr  Tue Jun  7 11:40:15 2005
From: SAULEAUEA at ch-mulhouse.fr (=?iso-8859-1?Q?SAULEAU_Erik-Andr=E9?=)
Date: Tue, 7 Jun 2005 11:40:15 +0200
Subject: [R] Variables values on intersected intervals
Message-ID: <A91EF0B9121F834EA6484582DFE1CF4436F872@messagerie.chm.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/21830d38/attachment.pl

From chris at psyctc.org  Tue Jun  7 11:04:57 2005
From: chris at psyctc.org (Chris Evans)
Date: Tue, 07 Jun 2005 10:04:57 +0100
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A4704F.6080504@oomvanlieshout.net>
References: <42A4564B.90002@lancaster.ac.uk>
Message-ID: <42A57149.31797.1BD4FF37@localhost>

On 6 Jun 2005 at 17:48, Sander Oom wrote:
... much snipped ...
> The whole point of a gallery is to show something to the user before
> the user knows what he is looking for. The R help functions currently
> available are hopeless when you have a picture of a graph in your head
> without knowing the required commands.
... much snipped ...

Belief that good graphics are often as important or more important 
than inferential tests or even CIs was one of the reasons I've moved 
over the last 15 years from SPSS (still use it a bit 'cos most 
colleagues do) through SAS (much better, much better graphics) to S+ 
(same but more so) to R (same and FLOSS!)

The demo graphics and the gallery are wonderful visual arguments for 
R and also great resources to help us learn.  Categories that I think 
might be useful sometimes might be: 
  	describes one variable where that is:
		dichotomous, categorical (n(categories) > 2), polytomous (short),
			polytomous (many levels), or continuous (might allow something on
			superimposing different referential distributions
	describes relationship between two variables where:
		both are dichotomous or polytomous
		one is ditto, other is continuous (box & violin etc: very useful to
			see good e.g.s of how to get most appropriate boxplots as it's 
			always possible to get good ones but not always obvious)
			(pointer to back-to-back histogram in Hmisc here)
		both are continuous (with and without jitter and weighting blobs)
	describe relationships between more than two variables...

However, the gallery idea is a very powerful one and being able to 
scroll through and drill down is a useful trick that M$ have, I 
grudgingly admit, used well so could we mimic their galleries from 
Excel as someone has suggested and perhaps mimic the drop down 
graphics picker in S+ (I no longer have access).

It's not much help but someone could put up the drop down list for 
newbies coming from SPSS ... ooh, just opened up my copy (11.0.1) and 
realised there's a gallery there with the following:
bar, line, area, pie, high-low, pareto, control, boxplots, error bar, 
scatter, histogram, normal P-P, normal Q-Q, sequence, 
autocorrelations, cross-correlations & spectral.  Never knew that the 
was there!  The drop down list below that has essentially the same 
list but with the last three under a sub-heading of "time series" and 
ROC curves added.

I wonder if someone hosted a wiki for a while at least it would get 
people contributing code for examples for some of these?  The results 
could transfer to the wonderful graphics gallery as they accumulated. 
 My skills aren't that hot but I'd throw in a few things happily and 
I'm sure a reward would be hearing of better ways to do things both 
in terms of coding and in terms of better displays/graphics to use.

Cheers all,

C
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Research Programmes Director, Nottinghamshire NHS Trust, 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative of those institutions 
***



From hb at maths.lth.se  Tue Jun  7 12:02:10 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 07 Jun 2005 12:02:10 +0200
Subject: [R] names of functions in a library
In-Reply-To: <Pine.LNX.4.61.0506070758450.11231@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E933@usctmx1106.merck.com>
	<Pine.LNX.4.61.0506070758450.11231@gannet.stats>
Message-ID: <42A570A2.7010706@maths.lth.se>

For a generalization of this, see ll() in the R.oo package;

 > library(R.oo)
 > ll(mode="function", envir="base")

                 member  data.class dimension object.size
  1                   -    function      NULL          28
  2              -.Date    function      NULL        5996
  3            -.POSIXt    function      NULL        6864
  <snip></snip>
  1008         zapsmall    function      NULL        3068
  1009 zip.file.extract    function      NULL        5232

 > help(ll)

Usage:
     ll(pattern=".*", ..., private=FALSE, properties=c("data.class", 
"dimension", "object.size"), sortBy=NULL, envir=parent.frame())

Arguments:

  pattern: Regular expression pattern specifying which members to
           return. If '".*"', all names are matched.

      ...: A named 'vector' of format 'functionName=value', where
           'functionName()' will be called on each member found. If the
           result matches the 'value', the member is returned, otherwise
           not.

properties: Names of properties to be returned. There must exist a
           'function' with the same name, because it will be called.
           This way one can extract any type of property by defining new
           methods.

   sortBy: Name or index of column (property) to be sorted by. If
           'NULL', the objects are listed in the order they are found.

  private: If 'TRUE', also private members, i.e. members with a name
           starting with a '.' (period), will be listed, otherwise not.

    envir: An 'environment', a search path index or a name of a package
           to be scanned.

Thus, stratification on 'mode' in the example abov is taken care of by 
the '...' argument.

Cheers

/Henrik

Prof Brian Ripley wrote:
> On Mon, 6 Jun 2005, Liaw, Andy wrote:
> 
>>> From: Duncan Murdoch
>>>
>>> On 6/6/2005 4:43 PM, Omar Lakkis wrote:
>>>
>>>> How can I get a list of the names of all exported functions
>>>
>>> in a library?
>>>
>>>> I load my library using library() and then want to
>>>
>>> dynamically get all
>>>
>>>> functions that start with "test."  to dynamically execute them.
>>>
>>>
>>> Use search() to see all the _packages_ that have been loaded.
>>>  If yours
>>> is second in the list (the typical spot just after calling the
>>> unfortunately named library() function), then ls(2) will list
>>> all of its
>>> exports.
>>
>>
>> Just to nitpick a bit:  That lists all _objects_ in position 2, which may
>> include objects that are not functions, although it's rare, if at all, 
>> that
>> a package database would contain non-functions...
> 
> 
> It's actually common: lazy-loaded datasets are there too, and 'base' has
> 
> [1] "F"          "LETTERS"        "R.version"      "R.version.string"
> [5] "T"          "letters"        "month.abb"      "month.name"
> [9] "pi"         "version"
> 
> 'stats' has "p.adjust.methods" ....
> 
> Here's a simple function to find only the functions
> 
> lsf <- function(n=2)
> {
>     tmp <- ls(n, all=TRUE)
>     isf <- sapply(tmp, function(x) is.function(get(x, pos=n)))
>     tmp[isf]
> }
> 
> and one could use the 'pattern' arg of ls() to restrict the set.
>



From chabotd at globetrotter.net  Tue Jun  7 12:04:16 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Tue, 07 Jun 2005 06:04:16 -0400
Subject: [R] how to create a variable to rank within subgroups
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E934@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E934@usctmx1106.merck.com>
Message-ID: <66053D63-DEB1-4A33-9050-75F5ED510DE8@globetrotter.net>

Thank you very much Andy, this is exactly what I was looking for. I  
did not know this function.

Sincerely,

Denis Chabot
Le 06 juin 2005 ?? 21:21, Liaw, Andy a ??crit :

> Try something like:
>
>
>> g <- gl(4, 5)
>> x <- sample(20)
>> d <- data.frame(g, x)
>> d
>>
>    g  x
> 1  1 10
> 2  1  3
> 3  1 11
> 4  1 12
> 5  1 20
> 6  2 13
> 7  2  6
> 8  2  2
> 9  2  1
> 10 2 14
> 11 3 17
> 12 3 15
> 13 3 16
> 14 3 19
> 15 3  9
> 16 4  4
> 17 4  7
> 18 4  5
> 19 4  8
> 20 4 18
>
>> d$xrank <- ave(d$x, d$g, FUN=rank)
>> d
>>
>    g  x xrank
> 1  1 10     2
> 2  1  3     1
> 3  1 11     3
> 4  1 12     4
> 5  1 20     5
> 6  2 13     4
> 7  2  6     3
> 8  2  2     2
> 9  2  1     1
> 10 2 14     5
> 11 3 17     4
> 12 3 15     2
> 13 3 16     3
> 14 3 19     5
> 15 3  9     1
> 16 4  4     1
> 17 4  7     3
> 18 4  5     2
> 19 4  8     4
> 20 4 18     5
>
> Andy
>
>
>> From: Denis Chabot
>>
>> Hi,
>>
>> I would like to create a new variable that would be the rank of a
>> continuous variable within each level of a grouping variable.
>> Say the
>> first level of the grouping variable has 5 observations, I'd like
>> them ranked from one to five and a new variable would hold the rank
>> value (one to five). So forth for each level of the grouping  
>> variable.
>>
>> I'm quite new with R and cannot figure out this one by myself.
>>
>> Thanks in advance,
>>
>> Denis Chabot
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
>
>
>
> ---------------------------------------------------------------------- 
> --------
> Notice:  This e-mail message, together with any attachments,  
> contains information of Merck & Co., Inc. (One Merck Drive,  
> Whitehouse Station, New Jersey, USA 08889), and/or its affiliates  
> (which may be known outside the United States as Merck Frosst,  
> Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be  
> confidential, proprietary copyrighted and/or legally privileged. It  
> is intended solely for the use of the individual or entity named on  
> this message.  If you are not the intended recipient, and have  
> received this message in error, please notify us immediately by  
> reply e-mail and then delete it from your system.
> ---------------------------------------------------------------------- 
> --------
>



From p.dalgaard at biostat.ku.dk  Tue Jun  7 12:18:06 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jun 2005 12:18:06 +0200
Subject: [R] how to create a variable to rank within subgroups
In-Reply-To: <66053D63-DEB1-4A33-9050-75F5ED510DE8@globetrotter.net>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E934@usctmx1106.merck.com>
	<66053D63-DEB1-4A33-9050-75F5ED510DE8@globetrotter.net>
Message-ID: <x2ekbejwrl.fsf@biostat.ku.dk>

Denis Chabot <chabotd at globetrotter.net> writes:

> Thank you very much Andy, this is exactly what I was looking for. I
> did not know this function.

It's a horrible misnomer though (ave() is originally for replacing values
with averages, but obviously has other uses). Any suggestions for a
better name (or alias).


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Jimmy.Cela at ichotelsgroup.com  Tue Jun  7 12:18:00 2005
From: Jimmy.Cela at ichotelsgroup.com (Cela, Jimmy (IHG))
Date: Tue, 7 Jun 2005 06:18:00 -0400 
Subject: [R] Conjoint in R
Message-ID: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>


Hello all,


I am trying to apply a conjoint analysis in order to determine the best
profile that captures the most preferred combination of levels of given
categorical factors.

For this a set of factors is given and initially a fractional factorial
design has to be produced as a subset of all possible factor levels
combinations, sufficient to estimate the main effects utilities. 

Then the preference for each chosen combination is assessed via surveys on
subjects (clients). Preferences are given by ranking profiles ordinally.

Conjoint analysis then is applied on the preference data to estimate the
utility values - or the "part worth" for each factor level.

SPSS 13.0 has a module called CONJOINT that handles such problem from
designing the fractional factorial design to the conjoint analysis.

What would be the best way to implement such analysis in R?

Thank you.

Jimmy Cela

Decision Sciences
InterContinental Hotels Group Inc
Atlanta, GA 
USA



From slist at oomvanlieshout.net  Tue Jun  7 12:36:50 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 07 Jun 2005 12:36:50 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A57149.31797.1BD4FF37@localhost>
References: <42A4564B.90002@lancaster.ac.uk>
	<42A57149.31797.1BD4FF37@localhost>
Message-ID: <42A578C2.30301@oomvanlieshout.net>

I agree that a wiki to facilitate submission of graph code could be very 
effective! Still needs to be well protected against vandalism. Seems a 
regular backup, to facilitate a clean restore, is the best approach.

Romain, would you be willing to set up a wiki within the gallery. Think 
the wiki and gallery should be close to each other in cyber space!

I use DokuWiki privately and it works wonders.

Guess the database design will require a bit of thought. For each graph 
it should be possible to enter multiple categories and sub-categories. 
then the gallery interface should dynamically build galleries using 
these categories. Not sure if you would want to create 'meta' categories 
to facilitate multiple categorization approaches!?

We could start with a limited list such as suggested by Chris to 
populate the gallery in the first instance.

Pleased to see this progress!!

Sander.


Chris Evans wrote:
> On 6 Jun 2005 at 17:48, Sander Oom wrote:
> ... much snipped ...
>>The whole point of a gallery is to show something to the user before
>>the user knows what he is looking for. The R help functions currently
>>available are hopeless when you have a picture of a graph in your head
>>without knowing the required commands.
> ... much snipped ...
> 
> Belief that good graphics are often as important or more important 
> than inferential tests or even CIs was one of the reasons I've moved 
> over the last 15 years from SPSS (still use it a bit 'cos most 
> colleagues do) through SAS (much better, much better graphics) to S+ 
> (same but more so) to R (same and FLOSS!)
> 
> The demo graphics and the gallery are wonderful visual arguments for 
> R and also great resources to help us learn.  Categories that I think 
> might be useful sometimes might be: 
>   	describes one variable where that is:
> 		dichotomous, categorical (n(categories) > 2), polytomous (short),
> 			polytomous (many levels), or continuous (might allow something on
> 			superimposing different referential distributions
> 	describes relationship between two variables where:
> 		both are dichotomous or polytomous
> 		one is ditto, other is continuous (box & violin etc: very useful to
> 			see good e.g.s of how to get most appropriate boxplots as it's 
> 			always possible to get good ones but not always obvious)
> 			(pointer to back-to-back histogram in Hmisc here)
> 		both are continuous (with and without jitter and weighting blobs)
> 	describe relationships between more than two variables...
> 
> However, the gallery idea is a very powerful one and being able to 
> scroll through and drill down is a useful trick that M$ have, I 
> grudgingly admit, used well so could we mimic their galleries from 
> Excel as someone has suggested and perhaps mimic the drop down 
> graphics picker in S+ (I no longer have access).
> 
> It's not much help but someone could put up the drop down list for 
> newbies coming from SPSS ... ooh, just opened up my copy (11.0.1) and 
> realised there's a gallery there with the following:
> bar, line, area, pie, high-low, pareto, control, boxplots, error bar, 
> scatter, histogram, normal P-P, normal Q-Q, sequence, 
> autocorrelations, cross-correlations & spectral.  Never knew that the 
> was there!  The drop down list below that has essentially the same 
> list but with the last three under a sub-heading of "time series" and 
> ROC curves added.
> 
> I wonder if someone hosted a wiki for a while at least it would get 
> people contributing code for examples for some of these?  The results 
> could transfer to the wonderful graphics gallery as they accumulated. 
>  My skills aren't that hot but I'd throw in a few things happily and 
> I'm sure a reward would be hearing of better ways to do things both 
> in terms of coding and in terms of better displays/graphics to use.
> 
> Cheers all,
> 
> C


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From murdoch at stats.uwo.ca  Tue Jun  7 12:42:26 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 07 Jun 2005 06:42:26 -0400
Subject: [R] (Off topic.) Observed Fisher information.
In-Reply-To: <200506062248.j56MmYs7022561@erdos.math.unb.ca>
References: <200506062248.j56MmYs7022561@erdos.math.unb.ca>
Message-ID: <42A57A12.7060109@stats.uwo.ca>

Rolf Turner wrote:
> I have been building an R function to calculate the ***observed***
> (as opposed to expected) Fisher information matrix for parameter
> estimates in a rather complicated setting.  I thought I had it
> working, but I am getting a result which is not positive definite.
> (One negative eigenvalue.  Out of 10.)
> 
> Is it the case that the observed Fisher information must be positive
> definite --- thereby indicating for certain that there are errors in
> my code --- or is it possible for such a matrix not to be pos. def.?

If you are at the maximum, it should be at least positive indefinite (or 
nonnegative definite).  Numerical errors could make zero (or small 
positive) eigenvalues look negative.  It's also possible that your 
optimization has missed the maximum by a bit, and then it could have 
truly negative eigenvalues.

In either case I'd expect the negative eigenvalues to be small.
> 
> It seems to me that if the log likelihood surface is ***not*** well
> approximated by a quadratic in a neighbourhood of the maximum, then
> it might well be that case that the observed information could fail
> to be positive definite.  Is this known/understood?  Can anyone point
> me to appropriate places in the literature?

If there is a true negative eigenvalue, then moving along that 
eigenvector should increase the likelihood, so I don't think even 
irregular problems could have true negative eigenvalues at the MLE. The 
problem there would be that a zero score and a positive definite 
observed information matrix don't necessarily imply you're at even a 
local maximum.

Duncan Murdoch



From slist at oomvanlieshout.net  Tue Jun  7 13:13:55 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 07 Jun 2005 13:13:55 +0200
Subject: [R] A long digression on packages
In-Reply-To: <42A46AED.6090208@oomvanlieshout.net>
References: <42A3027A.9000007@stats.uwo.ca> <42A43794.15246.170B9626@localhost>
	<42A46AED.6090208@oomvanlieshout.net>
Message-ID: <42A58173.2000502@oomvanlieshout.net>

Oooops, already missed one:

5. search of the R mailing lists: http://maths.newcastle.edu.au/~rking/R/

ad5. never used this before. Think Google also does an excellent job 
finding these if you start search with "R".

Sander.



Sander Oom wrote:
> Maybe some of this confusion about search opportunities and pros/cons 
> could be avoided if the search page on CRAN 
> (http://cran.r-project.org/search.html) would be extended to cover all 
> main search tools!
> 
> Quickly scanning the discussion, I found these:
> 1- simply Google: some tips and tricks have been mentioned and would be 
> usefully for most users;
> 2- R site search (external to CRAN) 
> http://finzi.psych.upenn.edu/search.html;
> 3- from R prompt: help.search();
> 4- browser supported search through local help files: 
> R/doc/html/search/SearchEngine.html.
> 
> ad1. Google is normally my first source using broad keywords for a 
> method or problem.
> ad2. just discovered this today!
> ad3. help.search() provides a simple overview, printing the command with 
> the providing package:
>  > help.search("rose")
> Help files with alias or concept or title matching ?rose? using
> regular expression matching:
> hirose(boot)            Failure Time of PET Film
> rose.diag(CircStats)    Rose Diagram
> rose.diag(circular)     Rose Diagram
> windrose(circular)      Windrose Generator
> rosavent(climatol)      Wind-rose plot
> Kinship82(clue)         Rosenberg-Kim Kinship Terms Partition Data
> HolidayDatabase(fCalendar)
>                         Holiday Calendars and Utilities
> conc(ineq)              Concentration Measures
> Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.
> 
> ad4. when installing all package locally, the results produced by the 
> browser supported search can be overwhelming. Even searching within the 
> results often does not help. If commands were printed along with the 
> providing package would be a good improvement. Then the apparent random 
> order of commands listed might also reveal some order.
> 
> Hope this is a useful addition to the debate,
> 
> Sander.
> 
> Chris Evans wrote:
>> On 5 Jun 2005 at 18:44, Jari Oksanen wrote:
>>
>>> There are diverse opinions about netiquette. One of the most basic, in
>>> my opinion, is this: if someone posts starts a discussion in a certain
>>> forum, you shall not divert it to another forum where it may be hidden
>>> by most readers, perhaps even by the originator of the thread.
>>
>> With the greatest of respect for Duncan and the R-devel list, I think 
>> Jari has a point here.  This is one of the most important issues I've 
>> seen raised on this list (R-help) in recent months and I think it may 
>> be a structural problem for the development of R, in common with that 
>> of much FLOSS s'ware, that there's a separation of users and authors 
>> that needs thought.  There are no perfect answers but too big a 
>> separation and projects go "techno" and it's hard for those of us who 
>> can't code C and who are mere "users" to help those outstanding people 
>> on whom we depend hear what we need: sometimes they are so clever, so 
>> specialised in their knowledge, or simply in the realm of genius not 
>> the ordinary, that they can't see our problem.  I have slowly come to 
>> respect that a pretty brusque style from our authorities is the only 
>> way to prevent this list being a madhouse but I think that Jim's point 
>> may fall into that class that is worth some duplicate bandwidth here.
>>
>> I know I've found the problem Jim highlights very confusing and 
>> unhelpful at times.  Views, which I didn't know, seem helpful but not 
>> a real solution to the key problem: they may tangentially help by 
>> ensuring that if your needs fit into a view, it becomes more likely 
>> that you'll install the packages you need and a local search may tell 
>> you what you need.  I've taken the inefficient route which suits me of 
>> installing just about every package to make it less likely I'll miss 
>> something of use to me. That means my search for "kappa" and "Cohen" 
>> (with ignore.case=FALSE) turns up at least three implementations of 
>> aspects of Cohen's kappa.
>>
>> It may already exist, but a web interface that did a help.search over 
>> all the packages in the current release version would be great.  (If 
>> it does exist, sorry, but I'm no dunce and use R nearly every day and 
>> try to read much of r-help every day and don't know it, which may say 
>> something!)
>>
>> I think there may be a need for some R improvement and automated 
>> updating of what I think is Frank Harrell's function finder:
>>     http://biostat.mc.vanderbilt.edu/s/finder/finder.html
>> Though I'm not absolutely sure how fitting your works into something 
>> like that could be imposed on developers!
>>
>> Another thing that might help would be for a system by which ordinary 
>> users would volunteer to pair up with developers for packages and try 
>> to suggest adaptations of the help and such like that might make the 
>> packages more user friendly.  I wouldn't want to do that for the whole 
>> of a huge and vital package like MASS or Hmisc (or base or stats!) but 
>> I'm up for pairing with a developer on a smaller package if anyone 
>> thinks that would be helpful.
>>
>> Thoughts for what they're worth.  Thanks a million to all developers 
>> ... asbestos suit on!
>>
>> Chris
> 
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From csae1552 at uibk.ac.at  Tue Jun  7 13:16:18 2005
From: csae1552 at uibk.ac.at (Hansi Weissensteiner)
Date: Tue,  7 Jun 2005 13:16:18 +0200
Subject: [R] Tabelle zu Matrix
Message-ID: <1118142978.42a58202df005@web-mail1.uibk.ac.at>

Hallo!

Ich habe ein Textdokument das folgenden Aufbau aufweisst:

String Integer z.B.
AAA    10
BBB    15
CCC    12
BBB    13
AAA    11
DDD    14

Mein Ziel ist es, die Daten in eine Matrix zu schreiben, ohne dabei mit
Schleifen zu arbeiten. Ist dies m??glich? Die entsprechende Matrix sollte dann
wie folgt aufgebaut sein:

AAA BBB CCC DDD
 10  15  12  14
 11  13   0   0

Ich denke an den Befehl table(), oder gibt es sonstige Befehle?

Danke im Vorraus
MfG
Hansi Weissensteiner



From ozric at web.de  Tue Jun  7 13:17:30 2005
From: ozric at web.de (christian schulz)
Date: Tue, 07 Jun 2005 13:17:30 +0200
Subject: [R] Conjoint in R
In-Reply-To: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
References: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
Message-ID: <42A5824A.3020405@web.de>

Hi,

try  package "MNP"  for a starting point  - which could be used for 
choice-based-conjoint!
And here a paper which show you  that a normal Conjoint design
is nothing others than a regression analysis, which could ready easy used
with little bit programming in R.

www.sawtoothsoftware.com/download/techpap/ca*excel*.pdf

regards,
Christian

>Hello all,
>
>
>I am trying to apply a conjoint analysis in order to determine the best
>profile that captures the most preferred combination of levels of given
>categorical factors.
>
>For this a set of factors is given and initially a fractional factorial
>design has to be produced as a subset of all possible factor levels
>combinations, sufficient to estimate the main effects utilities. 
>
>Then the preference for each chosen combination is assessed via surveys on
>subjects (clients). Preferences are given by ranking profiles ordinally.
>
>Conjoint analysis then is applied on the preference data to estimate the
>utility values - or the "part worth" for each factor level.
>
>SPSS 13.0 has a module called CONJOINT that handles such problem from
>designing the fractional factorial design to the conjoint analysis.
>
>What would be the best way to implement such analysis in R?
>
>Thank you.
>
>Jimmy Cela
>
>Decision Sciences
>InterContinental Hotels Group Inc
>Atlanta, GA 
>USA
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From max.marinucci at ya.com  Tue Jun  7 13:43:06 2005
From: max.marinucci at ya.com (max.marinucci)
Date: Tue, 7 Jun 2005 13:43:06 +0200
Subject: [R] Conjoint in R
In-Reply-To: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
Message-ID: <000001c56b56$11f426a0$f0b46093@MAX>

Hi there
Maybe you can find useful The isoreg {modreg} package which does Monotone
regression. This is probably what you need to model your data.

Nonetheless be aware you need to code properly your design matrix 
(use orthogonal polynomial codes)

The difference in using MNP or ordered probit is that with Monotonic
regression you will have individual partworths utilities. Then you may
aggregate your data on an individual level analysis.

Hope this helps
Regards
**********************************************
Massimiliano Marinucci
http://personales.ya.com/max_mar/
Ph.D Candidate in Economics
Fundamentos del Analisis Econ??mico II
(Econom??a Cuantitativa)
Facultad de CC.EE.
Universidad Complutense Madrid
Campus de Somosaguas
Madrid - Spain
**********************************************


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cela, Jimmy (IHG)
Sent: martes, 07 de junio de 2005 12:18
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Conjoint in R


Hello all,


I am trying to apply a conjoint analysis in order to determine the best
profile that captures the most preferred combination of levels of given
categorical factors.

For this a set of factors is given and initially a fractional factorial
design has to be produced as a subset of all possible factor levels
combinations, sufficient to estimate the main effects utilities. 

Then the preference for each chosen combination is assessed via surveys on
subjects (clients). Preferences are given by ranking profiles ordinally.

Conjoint analysis then is applied on the preference data to estimate the
utility values - or the "part worth" for each factor level.

SPSS 13.0 has a module called CONJOINT that handles such problem from
designing the fractional factorial design to the conjoint analysis.

What would be the best way to implement such analysis in R?

Thank you.

Jimmy Cela

Decision Sciences
InterContinental Hotels Group Inc
Atlanta, GA 
USA

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From drewbrewit at yahoo.com  Tue Jun  7 13:44:27 2005
From: drewbrewit at yahoo.com (Nick Drew)
Date: Tue, 7 Jun 2005 04:44:27 -0700 (PDT)
Subject: [R] R Graph Gallery : categorization of the graphs
Message-ID: <20050607114427.72571.qmail@web50906.mail.yahoo.com>

I like the wiki idea! 

Does it make sense to connect the graph wiki to the
existing R wiki?
(http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome)
This might give the existing wiki a kickstart with
some very valuable content. I realize they are
maintained by different people but the whole point of
a wiki is to foster collaboration.

Just 2 cents . . .
~Nick


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
Sander Oom
Sent: Tuesday, June 07, 2005 2:37 AM
To: Chris Evans
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] R Graph Gallery : categorization of
the graphs


I agree that a wiki to facilitate submission of graph
code could be very
effective! Still needs to be well protected against
vandalism. Seems a
regular backup, to facilitate a clean restore, is the
best approach.

Romain, would you be willing to set up a wiki within
the gallery. Think
the wiki and gallery should be close to each other in
cyber space!

I use DokuWiki privately and it works wonders.

Guess the database design will require a bit of
thought. For each graph
it should be possible to enter multiple categories and
sub-categories.
then the gallery interface should dynamically build
galleries using
these categories. Not sure if you would want to create
'meta' categories
to facilitate multiple categorization approaches!?

We could start with a limited list such as suggested
by Chris to
populate the gallery in the first instance.

Pleased to see this progress!!

Sander.




		
__________________________________ 

Get on-the-go sports scores, stock quotes, news and more. Check it out!



From chabotd at globetrotter.net  Tue Jun  7 13:57:57 2005
From: chabotd at globetrotter.net (Denis Chabot)
Date: Tue, 07 Jun 2005 07:57:57 -0400
Subject: [R] how to create a variable to rank within subgroups
In-Reply-To: <x2ekbejwrl.fsf@biostat.ku.dk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E934@usctmx1106.merck.com>
	<66053D63-DEB1-4A33-9050-75F5ED510DE8@globetrotter.net>
	<x2ekbejwrl.fsf@biostat.ku.dk>
Message-ID: <DDD5FE99-F1B4-4007-BAEA-E2851F8D6D65@globetrotter.net>

I sure agree the name is not very helpful in guessing what it can do.

May I suggest "propagate"?

Denis Chabot
Le 07 juin 2005 ?? 06:18, Peter Dalgaard a ??crit :

> Denis Chabot <chabotd at globetrotter.net> writes:
>
>
>> Thank you very much Andy, this is exactly what I was looking for. I
>> did not know this function.
>>
>
> It's a horrible misnomer though (ave() is originally for replacing  
> values
> with averages, but obviously has other uses). Any suggestions for a
> better name (or alias).
>
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)  
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)  
> 35327907
>



From francoisromain at free.fr  Tue Jun  7 13:55:19 2005
From: francoisromain at free.fr (Romain Francois)
Date: Tue, 07 Jun 2005 13:55:19 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A578C2.30301@oomvanlieshout.net>
References: <42A4564B.90002@lancaster.ac.uk>
	<42A57149.31797.1BD4FF37@localhost>
	<42A578C2.30301@oomvanlieshout.net>
Message-ID: <42A58B27.5000606@free.fr>


Le 07.06.2005 12:36, Sander Oom a ??crit :

> I agree that a wiki to facilitate submission of graph code could be 
> very effective! Still needs to be well protected against vandalism. 
> Seems a regular backup, to facilitate a clean restore, is the best 
> approach.
>
> Romain, would you be willing to set up a wiki within the gallery. 
> Think the wiki and gallery should be close to each other in cyber space!
>
Sander,

A lot of things are done after getting the code. Syntax highlighting for 
example. That can't (at least for now) be done online.
Certain things will be wiki-like, the WISHLIST can be filled online, in 
a few days the keywors thing will come and users of the gallery will be 
able to fill it. Later, people will have the possibility to write a few 
words on a graph and not just give a note.

Nevertheless, I don't think that all should be wikied.

> I use DokuWiki privately and it works wonders.
>
> Guess the database design will require a bit of thought. For each 
> graph it should be possible to enter multiple categories and 
> sub-categories. then the gallery interface should dynamically build 
> galleries using these categories. Not sure if you would want to create 
> 'meta' categories to facilitate multiple categorization approaches!?
>
The keyword thing appears to be really popular and I think it's a good 
idea. Those keywords may be categorized in the future.
That shouldn't be that hard to achieve that. The problem is time. R 
Graph Gallery is just something I am having fun with. ;-)


> We could start with a limited list such as suggested by Chris to 
> populate the gallery in the first instance.
>
> Pleased to see this progress!!
>
> Sander.
>
Thanks to all people who responded to that thread.
It really helped me understanding how the things have to be done.

Regards,

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ggrothendieck at gmail.com  Tue Jun  7 14:08:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 7 Jun 2005 08:08:54 -0400
Subject: [R] how to create a variable to rank within subgroups
In-Reply-To: <x2ekbejwrl.fsf@biostat.ku.dk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E934@usctmx1106.merck.com>
	<66053D63-DEB1-4A33-9050-75F5ED510DE8@globetrotter.net>
	<x2ekbejwrl.fsf@biostat.ku.dk>
Message-ID: <971536df05060705083ef137ce@mail.gmail.com>

On 07 Jun 2005 12:18:06 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Denis Chabot <chabotd at globetrotter.net> writes:
> 
> > Thank you very much Andy, this is exactly what I was looking for. I
> > did not know this function.
> 
> It's a horrible misnomer though (ave() is originally for replacing values
> with averages, but obviously has other uses). Any suggestions for a
> better name (or alias).

ave seems to be a generalized projection as ave(y,x,mean) 
equals fitted(lm(y~x)) and for suitable functions f ave(y,x,f) is
idempotent.   Not sure if that suggests a name.



From reilly at stat.auckland.ac.nz  Tue Jun  7 14:29:13 2005
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Wed, 08 Jun 2005 00:29:13 +1200
Subject: [R] Conjoint in R
In-Reply-To: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
References: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
Message-ID: <42A59319.3040107@stat.auckland.ac.nz>


Hi,

The conf.design package should help you handle the experimental design
side of your problem. Depending on your application, it may be unwise to
assume that main effects will be enough, as interactions can often turn
out to be important (at least in my experience with discrete conjoint).

Hope this helps,
James

On 7/06/2005 10:18 p.m., Cela, Jimmy (IHG) wrote:
> Hello all,
> 
> 
> I am trying to apply a conjoint analysis in order to determine the best
> profile that captures the most preferred combination of levels of given
> categorical factors.
> 
> For this a set of factors is given and initially a fractional factorial
> design has to be produced as a subset of all possible factor levels
> combinations, sufficient to estimate the main effects utilities. 
> 
> Then the preference for each chosen combination is assessed via surveys on
> subjects (clients). Preferences are given by ranking profiles ordinally.
> 
> Conjoint analysis then is applied on the preference data to estimate the
> utility values - or the "part worth" for each factor level.
> 
> SPSS 13.0 has a module called CONJOINT that handles such problem from
> designing the fractional factorial design to the conjoint analysis.
> 
> What would be the best way to implement such analysis in R?
> 
> Thank you.
> 
> Jimmy Cela
> 
> Decision Sciences
> InterContinental Hotels Group Inc
> Atlanta, GA 
> USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand



From slist at oomvanlieshout.net  Tue Jun  7 14:33:36 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 07 Jun 2005 14:33:36 +0200
Subject: [R] Tabelle zu Matrix
In-Reply-To: <1118142978.42a58202df005@web-mail1.uibk.ac.at>
References: <1118142978.42a58202df005@web-mail1.uibk.ac.at>
Message-ID: <42A59420.9030600@oomvanlieshout.net>

Why ask if you already know the answer. Did you try table()?

Other commands related: reshape {stats} and xtabs {stats}

I learned about this only a couple of days ago on this list!

Meanwhile read the posting guide and send your requests in English.

Good luck,

Sander.


Hansi Weissensteiner wrote:
> Hallo!
> 
> Ich habe ein Textdokument das folgenden Aufbau aufweisst:
> 
> String Integer z.B.
> AAA    10
> BBB    15
> CCC    12
> BBB    13
> AAA    11
> DDD    14
> 
> Mein Ziel ist es, die Daten in eine Matrix zu schreiben, ohne dabei mit
> Schleifen zu arbeiten. Ist dies m??glich? Die entsprechende Matrix sollte dann
> wie folgt aufgebaut sein:
> 
> AAA BBB CCC DDD
>  10  15  12  14
>  11  13   0   0
> 
> Ich denke an den Befehl table(), oder gibt es sonstige Befehle?
> 
> Danke im Vorraus
> MfG
> Hansi Weissensteiner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From rafa.sebastian at uv.es  Tue Jun  7 14:46:38 2005
From: rafa.sebastian at uv.es (Rafael Sebastian)
Date: Tue, 7 Jun 2005 14:46:38 +0200
Subject: [R] Smooth monotone estimation on R
Message-ID: <006501c56b5e$f32a9840$451b9c93@DYNAMIN>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/9fea8787/attachment.pl

From chencheva at gmail.com  Tue Jun  7 14:50:42 2005
From: chencheva at gmail.com (Hu Chen)
Date: Tue, 7 Jun 2005 20:50:42 +0800
Subject: [R] how to define functions in such a situation
Message-ID: <6f3fc9ee05060705505f831605@mail.gmail.com>

hi R folks,
I need read a file from hardisk or www web. Then I need to define some
new functions according to the contents of the read file.
For  example, i need write a package name "mypackage" like this:
>library(mypackage)
>read(some_file_on_web) #to see its content, suppose it contains:
eat.drink.sleep
then 3 new functions need to be created and usable.
the problem is, how could I create functions after executing
">library(mypackage)" and make these new functions visible and usable
immediately?

Any information are appreciated. forgive me if this question is very
stupid. but I really need help.
Thank you all.



From mischke at sozpsy.unizh.ch  Tue Jun  7 14:55:57 2005
From: mischke at sozpsy.unizh.ch (Stefan Mischke)
Date: Tue, 7 Jun 2005 14:55:57 +0200
Subject: [R] transform large matrix into list
Message-ID: <b8bfe0cd50370226534bdc3c3facfd23@sozpsy.unizh.ch>

Dear List

I need to transform a large matrix M with many NAs into a list L with 
one row for each non missing cell. Every row should contain the cell 
value in the first column, and its coordinates of the matrix in column 
2 and 3.

M:
	x1	x2
y1	1	2
y2	4	5
y3	7	8

L:
v	x	y
1	1	1
4	1	2
7	1	2
2	2	1
5	2	2
8	2	3

I'm trying to do this with a loop, but since my matrix is quite large 
(around 10k^2) this just takes a very long time.
There must be a more efficient and elegant way to do this. Any hints?

Thanks,
Stefan



From chencheva at gmail.com  Tue Jun  7 14:58:20 2005
From: chencheva at gmail.com (Hu Chen)
Date: Tue, 7 Jun 2005 20:58:20 +0800
Subject: [R] Re: how to define functions in such a situation
In-Reply-To: <6f3fc9ee05060705505f831605@mail.gmail.com>
References: <6f3fc9ee05060705505f831605@mail.gmail.com>
Message-ID: <6f3fc9ee05060705582f96a86a@mail.gmail.com>

I got a dirty way to solve this. 
write a temp .R source file including these new functions, then
>source(this_temp_file)
but I don't know if there are some temp directory for R to store temp files?

On 6/7/05, Hu Chen <chencheva at gmail.com> wrote:
> hi R folks,
> I need read a file from hardisk or www web. Then I need to define some
> new functions according to the contents of the read file.
> For  example, i need write a package name "mypackage" like this:
> >library(mypackage)
> >read(some_file_on_web) #to see its content, suppose it contains:
> eat.drink.sleep
> then 3 new functions need to be created and usable.
> the problem is, how could I create functions after executing
> ">library(mypackage)" and make these new functions visible and usable
> immediately?
> 
> Any information are appreciated. forgive me if this question is very
> stupid. but I really need help.
> Thank you all.
>



From andy_liaw at merck.com  Tue Jun  7 15:09:34 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 09:09:34 -0400
Subject: [R] how to define functions in such a situation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E939@usctmx1106.merck.com>

It't not clear to me exactly what you want and how you envision it could be
done, but it probably helps to realize that functions in R (and S in
general) are first class objects, so you can manipulate them pretty much
like any other objects.  You can "compute" them "on-the-fly", e.g., by
having a fuction returning a function (or a list of functions).  I find
reading the Language Definition manual very helpful for things like this.

As an example, the ecdf() function returns a function that can compute the
estimated ecdf for any input.  There may be simpler examples elsewhere.

Andy

> From: Hu Chen
> 
> hi R folks,
> I need read a file from hardisk or www web. Then I need to define some
> new functions according to the contents of the read file.
> For  example, i need write a package name "mypackage" like this:
> >library(mypackage)
> >read(some_file_on_web) #to see its content, suppose it contains:
> eat.drink.sleep
> then 3 new functions need to be created and usable.
> the problem is, how could I create functions after executing
> ">library(mypackage)" and make these new functions visible and usable
> immediately?
> 
> Any information are appreciated. forgive me if this question is very
> stupid. but I really need help.
> Thank you all.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From navarre_sabine at yahoo.fr  Tue Jun  7 15:14:52 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Tue, 7 Jun 2005 15:14:52 +0200 (CEST)
Subject: [R] htlm3D made4
Message-ID: <20050607131453.20274.qmail@web26602.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/eee10e0f/attachment.pl

From slist at oomvanlieshout.net  Tue Jun  7 15:17:52 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 07 Jun 2005 15:17:52 +0200
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A58B27.5000606@free.fr>
References: <42A4564B.90002@lancaster.ac.uk>	<42A57149.31797.1BD4FF37@localhost>	<42A578C2.30301@oomvanlieshout.net>
	<42A58B27.5000606@free.fr>
Message-ID: <42A59E80.9020304@oomvanlieshout.net>

Hi Romain,

You have stuck your neck out which is great. You are however not 
responsible for all the work! Let others join in!

Thus a wiki could provide opportunities for other people to contribute!

I was suggesting that the wiki be used as a platform for people to 
submit code examples and for people to discuss these examples! Did not 
think this would replace the things you need to do to get the data into 
the gallery!!

Maybe Nick would like to kick off the Rgraph wiki pages in the existing 
Rwiki, following the debates we have had about the categorization!?

Cheers,

Sander.


Romain Francois wrote:
> 
> Le 07.06.2005 12:36, Sander Oom a ??crit :
> 
>> I agree that a wiki to facilitate submission of graph code could be 
>> very effective! Still needs to be well protected against vandalism. 
>> Seems a regular backup, to facilitate a clean restore, is the best 
>> approach.
>>
>> Romain, would you be willing to set up a wiki within the gallery. 
>> Think the wiki and gallery should be close to each other in cyber space!
>>
> Sander,
> 
> A lot of things are done after getting the code. Syntax highlighting for 
> example. That can't (at least for now) be done online.
> Certain things will be wiki-like, the WISHLIST can be filled online, in 
> a few days the keywors thing will come and users of the gallery will be 
> able to fill it. Later, people will have the possibility to write a few 
> words on a graph and not just give a note.
> 
> Nevertheless, I don't think that all should be wikied.
> 
>> I use DokuWiki privately and it works wonders.
>>
>> Guess the database design will require a bit of thought. For each 
>> graph it should be possible to enter multiple categories and 
>> sub-categories. then the gallery interface should dynamically build 
>> galleries using these categories. Not sure if you would want to create 
>> 'meta' categories to facilitate multiple categorization approaches!?
>>
> The keyword thing appears to be really popular and I think it's a good 
> idea. Those keywords may be categorized in the future.
> That shouldn't be that hard to achieve that. The problem is time. R 
> Graph Gallery is just something I am having fun with. ;-)
> 
> 
>> We could start with a limited list such as suggested by Chris to 
>> populate the gallery in the first instance.
>>
>> Pleased to see this progress!!
>>
>> Sander.
>>
> Thanks to all people who responded to that thread.
> It really helped me understanding how the things have to be done.
> 
> Regards,
> 
> Romain
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From sdavis2 at mail.nih.gov  Tue Jun  7 15:20:47 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 7 Jun 2005 09:20:47 -0400
Subject: [R] Re: how to define functions in such a situation
In-Reply-To: <6f3fc9ee05060705582f96a86a@mail.gmail.com>
References: <6f3fc9ee05060705505f831605@mail.gmail.com>
	<6f3fc9ee05060705582f96a86a@mail.gmail.com>
Message-ID: <e163e5b4a63e1bb1f2c2abf38b00a714@mail.nih.gov>

Look at the documentation for "source".  You can source a file directly 
from the web:

source('http://myserver.com/somefile_on_web.R')

If that file contains R code for three functions, the functions will be 
"visible and useable immediately" in your workspace.

Sean

On Jun 7, 2005, at 8:58 AM, Hu Chen wrote:

> I got a dirty way to solve this.
> write a temp .R source file including these new functions, then
>> source(this_temp_file)
> but I don't know if there are some temp directory for R to store temp 
> files?
>
> On 6/7/05, Hu Chen <chencheva at gmail.com> wrote:
>> hi R folks,
>> I need read a file from hardisk or www web. Then I need to define some
>> new functions according to the contents of the read file.
>> For  example, i need write a package name "mypackage" like this:
>>> library(mypackage)
>>> read(some_file_on_web) #to see its content, suppose it contains:
>> eat.drink.sleep
>> then 3 new functions need to be created and usable.
>> the problem is, how could I create functions after executing
>> ">library(mypackage)" and make these new functions visible and usable
>> immediately?
>>
>> Any information are appreciated. forgive me if this question is very
>> stupid. but I really need help.
>> Thank you all.
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jun  7 15:24:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Jun 2005 15:24:43 +0200
Subject: [R] Re: how to define functions in such a situation
In-Reply-To: <6f3fc9ee05060705582f96a86a@mail.gmail.com>
References: <6f3fc9ee05060705505f831605@mail.gmail.com>
	<6f3fc9ee05060705582f96a86a@mail.gmail.com>
Message-ID: <42A5A01B.5080707@statistik.uni-dortmund.de>

Hu Chen wrote:

> I got a dirty way to solve this. 
> write a temp .R source file including these new functions, then
> 
>>source(this_temp_file)

Don't know if you really have to do it that way, but I also really don't 
understand what you are going to do...


> but I don't know if there are some temp directory for R to store temp files?

See ?tempfile and ?tempdir

Uwe Ligges

> On 6/7/05, Hu Chen <chencheva at gmail.com> wrote:
> 
>>hi R folks,
>>I need read a file from hardisk or www web. Then I need to define some
>>new functions according to the contents of the read file.
>>For  example, i need write a package name "mypackage" like this:
>>
>>>library(mypackage)
>>>read(some_file_on_web) #to see its content, suppose it contains:
>>
>>eat.drink.sleep
>>then 3 new functions need to be created and usable.
>>the problem is, how could I create functions after executing
>>">library(mypackage)" and make these new functions visible and usable
>>immediately?
>>
>>Any information are appreciated. forgive me if this question is very
>>stupid. but I really need help.
>>Thank you all.
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ozric at web.de  Tue Jun  7 15:27:29 2005
From: ozric at web.de (christian schulz)
Date: Tue, 07 Jun 2005 15:27:29 +0200
Subject: [R] factor2real strange!?
Message-ID: <42A5A0C1.2030402@web.de>

Hi,

i get factors from oracle instead real or numeric, but have problems
to convert. Where is my mistake? I can't rember have
this difficulties like this?

many thanks,
christian

 > vsub <- subset(dm,select=c(DEBITS_POST3))
 > (vsub$DEBITS_POST3[1:4])
[1] 9,93  0     23,82 32,72
7936 Levels: 0 0,53 0,65 0,73 0,76 1,02 1,03 1,16 1,25 1,28 1,31 1,38 
1,45 1,47 1,49 1,52 1,53 1,62 1,68 1,69 1,71 1,72 1,9 10,03 ... 99,98
 > vsub$DEBITS_POST3   <-  
as.numeric(levels(vsub$DEBITS_POST3))[as.integer(vsub$DEBITS_POST3)]
Warning message:
NAs introduced by coercion
 > (vsub$DEBITS_POST3[1:4])
[1] NA  0 NA NA
 > vsub <- subset(dm,select=c(DEBITS_POST3))
 > vsub$DEBITS_POST3   <-  
as.numeric(levels(vsub$DEBITS_POST3))[as.real(vsub$DEBITS_POST3)]
Warning message:
NAs introduced by coercion
 > (vsub$DEBITS_POST3[1:4])
[1] NA  0 NA NA
 >
 > vsub$DEBITS_POST3   <-  as.real(vsub$DEBITS_POST3)
 > vsub <- subset(dm,select=c(DEBITS_POST3))
 > vsub$DEBITS_POST3   <-  as.real(vsub$DEBITS_POST3)
 >
 > (vsub$DEBITS_POST3[1:4])
[1] 7603    1 2964 4029
 > vsub <- subset(dm,select=c(DEBITS_POST3))
 > vsub$DEBITS_POST3   <-  
as.numeric((vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)]
+ (vsub$DEBITS_POST3[1:4])
+ )
Error in 
as.numeric((vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)](vsub$DEBITS_POST3[1:4])) 
:
        attempt to apply non-function
 > vsub$DEBITS_POST3   <-  
as.numeric(vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)]
 > vsub <- subset(dm,select=c(DEBITS_POST3))
 > vsub$DEBITS_POST3   <-  
as.numeric(vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)]
 > (vsub$DEBITS_POST3[1:4])
[1]    1 7603 5848  917
 >



From mischke at sozpsy.unizh.ch  Tue Jun  7 15:28:26 2005
From: mischke at sozpsy.unizh.ch (Stefan Mischke)
Date: Tue, 7 Jun 2005 15:28:26 +0200
Subject: [R] transform large matrix into list
In-Reply-To: <b8bfe0cd50370226534bdc3c3facfd23@sozpsy.unizh.ch>
References: <b8bfe0cd50370226534bdc3c3facfd23@sozpsy.unizh.ch>
Message-ID: <4f838ad65bd6de718a1e35be1139c2c2@sozpsy.unizh.ch>

Ok, ok, I got it! Please don't laugh!
My question is the example from the posting guide! Which even includes 
the answer.
How embarrassing...
Stefan




Am 07.06.2005 um 14:55 schrieb Stefan Mischke:

> Dear List
>
> I need to transform a large matrix M with many NAs into a list L with 
> one row for each non missing cell. Every row should contain the cell 
> value in the first column, and its coordinates of the matrix in column 
> 2 and 3.
>
> M:
> 	x1	x2
> y1	1	2
> y2	4	5
> y3	7	8
>
> L:
> v	x	y
> 1	1	1
> 4	1	2
> 7	1	2
> 2	2	1
> 5	2	2
> 8	2	3
>
> I'm trying to do this with a loop, but since my matrix is quite large 
> (around 10k^2) this just takes a very long time.
> There must be a more efficient and elegant way to do this. Any hints?
>
> Thanks,
> Stefan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.be  Tue Jun  7 15:29:29 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 7 Jun 2005 15:29:29 +0200
Subject: [R] transform large matrix into list
References: <b8bfe0cd50370226534bdc3c3facfd23@sozpsy.unizh.ch>
Message-ID: <00a001c56b64$ee67f860$0540210a@www.domain>

one approach could be the following (maybe there is something better 
for large matrices):

M <- cbind(c(1, 4, 7, NA, 3, 2), c(2, 5, 8, 4, NA, 3))
##############
d <- dim(M)
L <- cbind(c(M), rep(1:d[2], each = d[1]), rep(1:d[1], d[2]))
L[complete.cases(L), ]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Stefan Mischke" <mischke at sozpsy.unizh.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 07, 2005 2:55 PM
Subject: [R] transform large matrix into list


> Dear List
>
> I need to transform a large matrix M with many NAs into a list L 
> with one row for each non missing cell. Every row should contain the 
> cell value in the first column, and its coordinates of the matrix in 
> column 2 and 3.
>
> M:
> x1 x2
> y1 1 2
> y2 4 5
> y3 7 8
>
> L:
> v x y
> 1 1 1
> 4 1 2
> 7 1 2
> 2 2 1
> 5 2 2
> 8 2 3
>
> I'm trying to do this with a loop, but since my matrix is quite 
> large (around 10k^2) this just takes a very long time.
> There must be a more efficient and elegant way to do this. Any 
> hints?
>
> Thanks,
> Stefan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Jun  7 15:35:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 09:35:00 -0400
Subject: [R] Re: how to define functions in such a situation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E93A@usctmx1106.merck.com>

help.search("temporary") tells you to look at ?tempfile.

Andy

> From: Hu Chen
> 
> I got a dirty way to solve this. 
> write a temp .R source file including these new functions, then
> >source(this_temp_file)
> but I don't know if there are some temp directory for R to 
> store temp files?
> 
> On 6/7/05, Hu Chen <chencheva at gmail.com> wrote:
> > hi R folks,
> > I need read a file from hardisk or www web. Then I need to 
> define some
> > new functions according to the contents of the read file.
> > For  example, i need write a package name "mypackage" like this:
> > >library(mypackage)
> > >read(some_file_on_web) #to see its content, suppose it contains:
> > eat.drink.sleep
> > then 3 new functions need to be created and usable.
> > the problem is, how could I create functions after executing
> > ">library(mypackage)" and make these new functions visible 
> and usable
> > immediately?
> > 
> > Any information are appreciated. forgive me if this question is very
> > stupid. but I really need help.
> > Thank you all.
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From lecoutre at stat.ucl.ac.be  Tue Jun  7 15:32:43 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 7 Jun 2005 15:32:43 +0200
Subject: [R] htlm3D made4
In-Reply-To: <20050607131453.20274.qmail@web26602.mail.ukl.yahoo.com>
Message-ID: <031a01c56b65$6251e3d0$6e8b6882@didacdom.stat.ucl.ac.be>



have you tried using Google using keyword "made4" ???


The second item gives:

made4
Package: made4. Description: Multivariate data analysis and graphical
display of
microarray data. Functions include between group analysis and coinertia
...
www.bioconductor.org/repository/devel/package/html/made4.html - 2k - 6
Jun 2005 - Cached - Similar pages 

And then later:
http://bioinf.ucd.ie/people/aedin/R/


HTH,

Eric
ps: google is your best friend


Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Navarre Sabine
> Sent: mardi 7 juin 2005 15:15
> To: r-help at stat.math.ethz.ch
> Subject: [R] htlm3D made4
> 
> 
> Hi,
>  
> I would like to know if people have found the package made4 
> to load! I would like the .zip
> 
> If you have the @ on internet to load it please give me it!
> 
> Thanks a lot,
>  
> Sabine
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Jun  7 15:41:33 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 09:41:33 -0400
Subject: [R] transform large matrix into list
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E93B@usctmx1106.merck.com>

Here's one way to do it:

> M
   x1 x2
y1  1  2
y2  4  5
y3  7  8
> (L <- cbind(v=c(M), x=c(col(M)), y=c(row(M))))
     v x y
[1,] 1 1 1
[2,] 4 1 2
[3,] 7 1 3
[4,] 2 2 1
[5,] 5 2 2
[6,] 8 2 3

(The seemingly extraneous c()'s are to drop the dimensions of those
matrices.)

Are you sure your L[3, 3] is correct?

Also, for a 10K^2 M, L is going to take up over 1.1GB of memory!  You may
want to look at SparseM and Matrix packages, which have facilities for
handling sparse matrices.

Andy

> From: Stefan Mischke
> 
> Dear List
> 
> I need to transform a large matrix M with many NAs into a list L with 
> one row for each non missing cell. Every row should contain the cell 
> value in the first column, and its coordinates of the matrix 
> in column 
> 2 and 3.
> 
> M:
> 	x1	x2
> y1	1	2
> y2	4	5
> y3	7	8
> 
> L:
> v	x	y
> 1	1	1
> 4	1	2
> 7	1	2
> 2	2	1
> 5	2	2
> 8	2	3
> 
> I'm trying to do this with a loop, but since my matrix is quite large 
> (around 10k^2) this just takes a very long time.
> There must be a more efficient and elegant way to do this. Any hints?
> 
> Thanks,
> Stefan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From corr at fas.harvard.edu  Tue Jun  7 15:48:13 2005
From: corr at fas.harvard.edu (Anders Schwartz Corr)
Date: Tue, 7 Jun 2005 09:48:13 -0400 (EDT)
Subject: [R] ordglm -- simple question
Message-ID: <Pine.LNX.4.58.0506070945240.7441@ls03.fas.harvard.edu>


My attempt to test a model using ordglm code is running into problems, and
I thought if you have a moment you might illucidate the situation.

Here is the data:

http://www.people.fas.harvard.edu/~corr/6.4.05.RData

Here is the code:

# I coerce tcn8 matrix data to a vector, because ordglm will not accept
matrix data.
y<-as.vector(tcn8[,62])
x<-as.vector(tcn8[,60])
wt<-as.vector(tcn8[,61])
cbind(y,x,wt)->tc
colnames(tc) <- c("y","x","wt")
ordglm(y ~ x, weights=wt, data=tc, link = "logit")

Here is the output:

> y<-as.vector(tcn8[,62])
> x<-as.vector(tcn8[,60])
> wt<-as.vector(tcn8[,61])
> cbind(y,x,wt)->tc
> colnames(tc) <- c("y","x","wt")
> ordglm(y ~ x, weights=wt, data=tc, link = "logit")
Error in model.frame.default(mt, data, na.action = na.fail) :
        `data' must be a data.frame, not a matrix or  array
>

Perhaps it is a problem of having NAs and NaNs in my data? I appreciate
any pointers. Do I need to set model.frame first?

Thanks,

Anders Corr



From dmb at mrc-dunn.cam.ac.uk  Tue Jun  7 15:54:59 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 7 Jun 2005 14:54:59 +0100 (BST)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <42A59E80.9020304@oomvanlieshout.net>
Message-ID: <Pine.LNX.4.21.0506071449050.14025-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 7 Jun 2005, Sander Oom wrote:

>Hi Romain,
>
>You have stuck your neck out which is great. You are however not 
>responsible for all the work! Let others join in!
>
>Thus a wiki could provide opportunities for other people to contribute!
>
>I was suggesting that the wiki be used as a platform for people to 
>submit code examples and for people to discuss these examples! Did not 
>think this would replace the things you need to do to get the data into 
>the gallery!!
>
>Maybe Nick would like to kick off the Rgraph wiki pages in the existing 
>Rwiki, following the debates we have had about the categorization!?

I agree, but its hard to get people contributing without something
centralized (some central autohrity). The problem with the existing wiki

http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

Is that it currenlty does not support images (afaik).

My personal favourite is MediaWiki, it has all kinds of gallery, category
functions built in, including levels of 'protection' and 'authorization'
to guide the user contributions in sensible ways. It can hook up to other
media wiki's (like wikipedia) and exchange information in XML format
(which is why you see data from wikipedia all over the web). It even has
RSS streams, so you could subscribe to a graph of the week 'news' channel
:)

It does not have (maybe modable) syntax highlighting or category
clustering. 


>
>Cheers,
>
>Sander.
>
>
>Romain Francois wrote:
>> 
>> Le 07.06.2005 12:36, Sander Oom a ÅÈcrit :
>> 
>>> I agree that a wiki to facilitate submission of graph code could be 
>>> very effective! Still needs to be well protected against vandalism. 
>>> Seems a regular backup, to facilitate a clean restore, is the best 
>>> approach.
>>>
>>> Romain, would you be willing to set up a wiki within the gallery. 
>>> Think the wiki and gallery should be close to each other in cyber space!
>>>
>> Sander,
>> 
>> A lot of things are done after getting the code. Syntax highlighting for 
>> example. That can't (at least for now) be done online.
>> Certain things will be wiki-like, the WISHLIST can be filled online, in 
>> a few days the keywors thing will come and users of the gallery will be 
>> able to fill it. Later, people will have the possibility to write a few 
>> words on a graph and not just give a note.
>> 
>> Nevertheless, I don't think that all should be wikied.
>> 
>>> I use DokuWiki privately and it works wonders.
>>>
>>> Guess the database design will require a bit of thought. For each 
>>> graph it should be possible to enter multiple categories and 
>>> sub-categories. then the gallery interface should dynamically build 
>>> galleries using these categories. Not sure if you would want to create 
>>> 'meta' categories to facilitate multiple categorization approaches!?
>>>
>> The keyword thing appears to be really popular and I think it's a good 
>> idea. Those keywords may be categorized in the future.
>> That shouldn't be that hard to achieve that. The problem is time. R 
>> Graph Gallery is just something I am having fun with. ;-)
>> 
>> 
>>> We could start with a limited list such as suggested by Chris to 
>>> populate the gallery in the first instance.
>>>
>>> Pleased to see this progress!!
>>>
>>> Sander.
>>>
>> Thanks to all people who responded to that thread.
>> It really helped me understanding how the things have to be done.
>> 
>> Regards,
>> 
>> Romain
>> 
>
>
>



From ligges at statistik.uni-dortmund.de  Tue Jun  7 15:59:13 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Jun 2005 15:59:13 +0200
Subject: [R] factor2real strange!?
In-Reply-To: <42A5A0C1.2030402@web.de>
References: <42A5A0C1.2030402@web.de>
Message-ID: <42A5A831.3080309@statistik.uni-dortmund.de>

christian schulz wrote:

> Hi,
> 
> i get factors from oracle instead real or numeric, but have problems
> to convert. Where is my mistake? I can't rember have
> this difficulties like this?
> 
> many thanks,
> christian


Please look at your numbers: "9,93", this includes "," instead of the 
correct decimal point "." ....

Uwe Ligges


>  > vsub <- subset(dm,select=c(DEBITS_POST3))
>  > (vsub$DEBITS_POST3[1:4])
> [1] 9,93  0     23,82 32,72
> 7936 Levels: 0 0,53 0,65 0,73 0,76 1,02 1,03 1,16 1,25 1,28 1,31 1,38 
> 1,45 1,47 1,49 1,52 1,53 1,62 1,68 1,69 1,71 1,72 1,9 10,03 ... 99,98
>  > vsub$DEBITS_POST3   <-  
> as.numeric(levels(vsub$DEBITS_POST3))[as.integer(vsub$DEBITS_POST3)]
> Warning message:
> NAs introduced by coercion
>  > (vsub$DEBITS_POST3[1:4])
> [1] NA  0 NA NA
>  > vsub <- subset(dm,select=c(DEBITS_POST3))
>  > vsub$DEBITS_POST3   <-  
> as.numeric(levels(vsub$DEBITS_POST3))[as.real(vsub$DEBITS_POST3)]
> Warning message:
> NAs introduced by coercion
>  > (vsub$DEBITS_POST3[1:4])
> [1] NA  0 NA NA
>  >
>  > vsub$DEBITS_POST3   <-  as.real(vsub$DEBITS_POST3)
>  > vsub <- subset(dm,select=c(DEBITS_POST3))
>  > vsub$DEBITS_POST3   <-  as.real(vsub$DEBITS_POST3)
>  >
>  > (vsub$DEBITS_POST3[1:4])
> [1] 7603    1 2964 4029
>  > vsub <- subset(dm,select=c(DEBITS_POST3))
>  > vsub$DEBITS_POST3   <-  
> as.numeric((vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)]
> + (vsub$DEBITS_POST3[1:4])
> + )
> Error in 
> as.numeric((vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)](vsub$DEBITS_POST3[1:4])) 
> :
>        attempt to apply non-function
>  > vsub$DEBITS_POST3   <-  
> as.numeric(vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)]
>  > vsub <- subset(dm,select=c(DEBITS_POST3))
>  > vsub$DEBITS_POST3   <-  
> as.numeric(vsub$DEBITS_POST3)[as.real(vsub$DEBITS_POST3)]
>  > (vsub$DEBITS_POST3[1:4])
> [1]    1 7603 5848  917
>  >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfbrennan at rogers.com  Tue Jun  7 16:00:38 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Tue, 7 Jun 2005 10:00:38 -0400
Subject: [R] transform large matrix into list
In-Reply-To: <b8bfe0cd50370226534bdc3c3facfd23@sozpsy.unizh.ch>
Message-ID: <200506071400.j57E0gRK005233@hypatia.math.ethz.ch>

First it looks like in L you need a 3 in row 3 col 2.
Anyway I think this is a non elegant way to do it without a loop that you
can easily improve I imagine and should be faster than the loop. You will
have to adjust the numbers to fit you matrix size etc.

R>mat<-matrix(sample(0:9,100,replace=T),50,2)#your M
R>new<-matrix(mat,100,1)
R>new2<-c(rep(1,50),rep(2,50))
R>new3<-c(rep(1:50,2))
R>result<-cbind(new,new2,new3)# your L

Jim

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Stefan Mischke
Sent: June 7, 2005 8:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] transform large matrix into list

Dear List

I need to transform a large matrix M with many NAs into a list L with 
one row for each non missing cell. Every row should contain the cell 
value in the first column, and its coordinates of the matrix in column 
2 and 3.

M:
	x1	x2
y1	1	2
y2	4	5
y3	7	8

L:
v	x	y
1	1	1
4	1	2
7	1	2
2	2	1
5	2	2
8	2	3

I'm trying to do this with a loop, but since my matrix is quite large 
(around 10k^2) this just takes a very long time.
There must be a more efficient and elegant way to do this. Any hints?

Thanks,
Stefan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Tue Jun  7 16:06:42 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 7 Jun 2005 07:06:42 -0700 (PDT)
Subject: [R] write.dta limits
In-Reply-To: <m24qcazmd5.fsf@ganymede.home.net>
References: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
	<Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>
	<m24qcazmd5.fsf@ganymede.home.net>
Message-ID: <Pine.A41.4.61b.0506070704570.228936@homer05.u.washington.edu>

On Tue, 7 Jun 2005, David Whiting wrote:

>
> When I have encountered this error message in the past seems to have
> resulted from a blank/empty level in a factor or an empty character.

Thanks.  That is useful.

I wasn't claiming that was no bug, just that it wasn't a limit on the file 
size.

 	-thomas

> For example:
>
>> library(foreign)
>> x <- data.frame(x=c("A", "B", "C"), y=c(1,2,3))
>> write.dta(x, file="temp.dta")
>> levels(x$x)[2]
> [1] "B"
>> levels(x$x)[2] <- ""
>> write.dta(x, file="temp.dta")
> Error in write.dta(x, file = "temp.dta") :
> 	a binary write error occurred
>
>
> My work-around at the time was to go through the data replacing ""
> with something else that I could then deal with later.
>
>
> -- 
> David Whiting
> University of Newcastle upon Tyne, UK
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From maechler at stat.math.ethz.ch  Tue Jun  7 16:10:18 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 7 Jun 2005 16:10:18 +0200
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <Pine.LNX.4.21.0506061444260.31061-100000@mail.mrc-dunn.cam.ac.uk>
References: <40e66e0b05060605512e39815d@mail.gmail.com>
	<Pine.LNX.4.21.0506061444260.31061-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <17061.43722.36464.509201@stat.math.ethz.ch>

>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>     on Mon, 6 Jun 2005 14:48:50 +0100 (BST) writes:

    Dan> On Mon, 6 Jun 2005, Douglas Bates wrote:
    >> On 6/6/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
    >>> 
    >>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
    >>> 
    >>> Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
    >>> <80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
    >>> 
    >>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
    >>> 
    >>> hmm... they go away when I paste them in...
    >> 
    >> Check the character set in use.  You probably are using a UTF-8
    >> encoding in an environment that does not support display of that
    >> character set.

    Dan> I feel it is something utf8-ish, I am using emacs-ess-5.2.3, which was
    Dan> working fine under R-2.0.0, but developed this problem under R-2.1.0

ess-5.2.3  is outdated for the purpose of well cooperating with
UTF-8 outputs from R 2.1.x
IIRC, ESS cooperates with   R "with UTF encoding" since about ESS 5.2.6.
but ESS-5.2.8 is current  { --> http://ESS.r-project.org/ )

      ..........

    Dan> I have gone back to 2.0.0 :)

Don't do that!
You've lost tons of nice new features and gained quite an amount
of old bugs by downgrading .. 

Martin



From baron at psych.upenn.edu  Tue Jun  7 16:14:01 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 7 Jun 2005 10:14:01 -0400
Subject: [R] Conjoint in R
In-Reply-To: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
References: <6532FF25B0EDB84AA33DD15434A5C6E107D92784@atlexg09.hiw.com>
Message-ID: <20050607141401.GA13987@psych>

Another approach, not mentioned yet, is to use ace, in the
acepack package.  I have used this in an article (with Andy
Gurmankin) coming out soon in Memory and Cognition, which I could
send by email.  It isn't obvious to me that this will (or that it
won't) work with a fractional factorial design; my hunch is that
it will work.

Jon

On 06/07/05 06:18, Cela, Jimmy (IHG) wrote:
 
 I am trying to apply a conjoint analysis in order to determine the best
 profile that captures the most preferred combination of levels of given
 categorical factors.
 
 For this a set of factors is given and initially a fractional factorial
 design has to be produced as a subset of all possible factor levels
 combinations, sufficient to estimate the main effects utilities.
 
 Then the preference for each chosen combination is assessed via surveys on
 subjects (clients). Preferences are given by ranking profiles ordinally.
 
 Conjoint analysis then is applied on the preference data to estimate the
 utility values - or the "part worth" for each factor level.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From rsubscriber at slcmsr.net  Tue Jun  7 16:46:27 2005
From: rsubscriber at slcmsr.net (rsubscriber)
Date: Tue, 07 Jun 2005 16:46:27 +0200
Subject: [R] How to detect a closed socket when writing?
Message-ID: <42A5B343.4010106@slcmsr.net>

Hi,

is it possible to detect, whether the other side has closed a socket
connection when writing on this socket?

When i am trying this with two R instances R 1 and R 2, i expected
that the last writeLines would give an error, but the call seems to be 
successfull:

R 1>  s<-socketConnection(port=2000, server=T, blocking=T, open="r+b")
R 2>  s<-socketConnection(port=2000, server=F, blocking=T, open="r+b")
R 1>  CloseAllConnections()
R 2>  writeLines("test", s)  # seems to succeed
R 2>  isIncomplete(s)        # returns FALSE

Only if i write a very long strings instead of "test", the writeLines
call blocks and gives a warning after timing out. The same happens with
writeBin.

I tried this with R-2.1.0 and R-2.0.1 under Ubuntu hoary.

This is a problem for me, because i am writing a statistics server
in R, which communicates over a socket connection with a Java web
server. The other side has timeouts and may close the connection
before the result of a long-standing calculation has been sent.
In this case, the R server should detect, that the connection
was closed and try to reconnect. But this is not possible, if the
write seems successfull.

Is this a bug, which will be solved in future releases, or a feature?

Christian



From sabine.navarre at siemens.com  Tue Jun  7 16:47:17 2005
From: sabine.navarre at siemens.com (Navarre Sabine (stu))
Date: Tue, 7 Jun 2005 16:47:17 +0200
Subject: [R] Downloading the basic package
Message-ID: <C0A2DDBDA4904048820B3477D766FE3C01D3143C@tlsm385a.ww011.siemens.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/4491a02a/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Tue Jun  7 16:49:22 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 7 Jun 2005 16:49:22 +0200
Subject: [R] ordglm -- simple question
In-Reply-To: <Pine.LNX.4.58.0506070945240.7441@ls03.fas.harvard.edu>
References: <Pine.LNX.4.58.0506070945240.7441@ls03.fas.harvard.edu>
Message-ID: <20050607164922.4f0d570a.Achim.Zeileis@wu-wien.ac.at>

On Tue, 7 Jun 2005 09:48:13 -0400 (EDT) Anders Schwartz Corr wrote:

> My attempt to test a model using ordglm code is running into problems,
> and I thought if you have a moment you might illucidate the situation.

Please indicate which package you are using, I guess you are referring
to ordglm() in Jim Lindsey's gnlm package.
 
> Here is the data:
> 
> http://www.people.fas.harvard.edu/~corr/6.4.05.RData

You are kidding, right? A 4.9M file with some 30 objects for an example
like the one below?? *Please* do read the posting guide.

> Here is the code:
> 
> # I coerce tcn8 matrix data to a vector, because ordglm will not
> # accept
> matrix data.
> y<-as.vector(tcn8[,62])
> x<-as.vector(tcn8[,60])
> wt<-as.vector(tcn8[,61])
> cbind(y,x,wt)->tc
> colnames(tc) <- c("y","x","wt")
> ordglm(y ~ x, weights=wt, data=tc, link = "logit")
> 
> Here is the output:
> 
> > y<-as.vector(tcn8[,62])
> > x<-as.vector(tcn8[,60])
> > wt<-as.vector(tcn8[,61])
> > cbind(y,x,wt)->tc
> > colnames(tc) <- c("y","x","wt")
> > ordglm(y ~ x, weights=wt, data=tc, link = "logit")
> Error in model.frame.default(mt, data, na.action = na.fail) :
>         `data' must be a data.frame, not a matrix or  array
> 
> Perhaps it is a problem of having NAs and NaNs in my data? I
> appreciate any pointers. Do I need to set model.frame first?

I think the error message is very explicit: `data' (in your case the
object `tc') needs to be a "data.frame", but it is a "matrix". So you
did a good job in replacing the matrix `tcn8' with the matrix `tc'.

You can coerce it relatively easy to a "data.frame" using
as.data.frame(). Then ordglm() will complain about NAs, which you could
for example na.omit(). But then ordglm() says that `y' has to be numeric
which made me realize that your variables `y', `x' and `wt' are in fact
"character"s.

In summary this means:
  1. Hold your variables in an appropriate format, e.g., numeric 
     vectors in a "data.frame". Maybe some basic R manuals like "An
     Introduction to R" can help here.
  2. Read the man page of ordglm() and also read the error messages,
     they are very informative.
  3. Read the posting guide before posting to the lists.

hth,
Z



From uofiowa at gmail.com  Tue Jun  7 17:02:13 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 7 Jun 2005 11:02:13 -0400
Subject: [R] without a loop
Message-ID: <3f87cc6d050607080273116a2@mail.gmail.com>

tmp <- c(-1,NA,NA,1,1,NA,NA,1)

without using a loop, how can I replace all NAs in the list above with
the previous none NA value in the list?



From jsorkin at grecc.umaryland.edu  Tue Jun  7 17:09:09 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 07 Jun 2005 11:09:09 -0400
Subject: [R] Downloading the basic package
Message-ID: <s2a58079.041@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/ed3287f0/attachment.pl

From drf5n at maplepark.com  Tue Jun  7 17:09:51 2005
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 7 Jun 2005 10:09:51 -0500 (CDT)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.21.0506071449050.14025-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506071449050.14025-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.58.0506071004590.21844@maplepark.com>

On Tue, 7 Jun 2005, Dan Bolser wrote:
...
> I agree, but its hard to get people contributing without something
> centralized (some central autohrity). The problem with the existing wiki
>
> http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>
> Is that it currenlty does not support images (afaik).

Counterexample:

http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?Tips_And_Examples


Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From james.holtman at convergys.com  Tue Jun  7 17:12:20 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue, 7 Jun 2005 11:12:20 -0400
Subject: [R] transform large matrix into list
In-Reply-To: <b8bfe0cd50370226534bdc3c3facfd23@sozpsy.unizh.ch>
Message-ID: <OF164B8BB6.A70CAE74-ON85257019.005383E0-85257019.0053871A@nd.convergys.com>





> x.1
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]   NA    6
> cbind(x.1[!is.na(x.1)], which(!is.na(x.1), arr.ind=TRUE))
       row col
[1,] 1   1   1
[2,] 2   2   1
[3,] 4   1   2
[4,] 5   2   2
[5,] 6   3   2
>

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Stefan Mischke                                                                                                       
                      <mischke at sozpsy.unizh        To:       r-help at stat.math.ethz.ch                                                      
                      .ch>                         cc:                                                                                     
                      Sent by:                     Subject:  [R] transform large matrix into list                                          
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/07/2005 08:55                                                                                                     
                                                                                                                                           




Dear List

I need to transform a large matrix M with many NAs into a list L with
one row for each non missing cell. Every row should contain the cell
value in the first column, and its coordinates of the matrix in column
2 and 3.

M:
             x1          x2
y1           1           2
y2           4           5
y3           7           8

L:
v            x           y
1            1           1
4            1           2
7            1           2
2            2           1
5            2           2
8            2           3

I'm trying to do this with a loop, but since my matrix is quite large
(around 10k^2) this just takes a very long time.
There must be a more efficient and elegant way to do this. Any hints?

Thanks,
Stefan

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Tue Jun  7 17:14:55 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 7 Jun 2005 17:14:55 +0200
Subject: [R] without a loop
In-Reply-To: <3f87cc6d050607080273116a2@mail.gmail.com>
References: <3f87cc6d050607080273116a2@mail.gmail.com>
Message-ID: <20050607171455.5ea867ae.Achim.Zeileis@wu-wien.ac.at>

On Tue, 7 Jun 2005 11:02:13 -0400 Omar Lakkis wrote:

> tmp <- c(-1,NA,NA,1,1,NA,NA,1)
> 
> without using a loop, how can I replace all NAs in the list above with
> the previous none NA value in the list?

Package zoo contains a generic function na.locf (last observation
carried forward) which handles this:

R> tmp <- c(-1,NA,NA,1,1,NA,NA,1)
R> library(zoo)
R> na.locf(tmp)
[1] -1 -1 -1  1  1  1  1  1

Z

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Tue Jun  7 17:16:56 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 07 Jun 2005 08:16:56 -0700
Subject: [R] without a loop
In-Reply-To: <3f87cc6d050607080273116a2@mail.gmail.com>
References: <3f87cc6d050607080273116a2@mail.gmail.com>
Message-ID: <42A5BA68.1060505@pdf.com>

How about the following:

tmp <- c(-1,NA,NA,1,1,NA,NA,1)

replaceNA <- function(x){
	iNA <- which(is.na(x))
	if(length(iNA) %in% c(0, length(x)))
		return(x)
	iNA1 <- iNA-1
	iNA1[iNA==0] <- length(x)
	x[iNA] <- x[iNA1]
	replaceNA(x)
}
 > replaceNA(tmp)
[1] -1 -1 -1  1  1  1  1  1
 > replaceNA(NA)
[1] NA

	  spencer graves

Omar Lakkis wrote:

> tmp <- c(-1,NA,NA,1,1,NA,NA,1)
> 
> without using a loop, how can I replace all NAs in the list above with
> the previous none NA value in the list?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Tue Jun  7 17:19:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 7 Jun 2005 11:19:22 -0400
Subject: [R] without a loop
In-Reply-To: <3f87cc6d050607080273116a2@mail.gmail.com>
References: <3f87cc6d050607080273116a2@mail.gmail.com>
Message-ID: <971536df05060708195b59adaf@mail.gmail.com>

On 6/7/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> tmp <- c(-1,NA,NA,1,1,NA,NA,1)
> 
> without using a loop, how can I replace all NAs in the list above with
> the previous none NA value in the list?

This is known as last occurrence carried forward (LOCF) and
is implemented in both the 'zoo' and 'its' packages, e.g.

library(zoo)
na.locf(tmp)



From dmb at mrc-dunn.cam.ac.uk  Tue Jun  7 17:35:31 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 7 Jun 2005 16:35:31 +0100 (BST)
Subject: [R] Strange characters in 2.1.0?
In-Reply-To: <17061.43722.36464.509201@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.21.0506071633560.14637-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 7 Jun 2005, Martin Maechler wrote:

>>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>>     on Mon, 6 Jun 2005 14:48:50 +0100 (BST) writes:
>
>    Dan> On Mon, 6 Jun 2005, Douglas Bates wrote:
>    >> On 6/6/05, Dan Bolser <dmb at mrc-dunn.cam.ac.uk> wrote:
>    >>> 
>    >>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
>    >>> 
>    >>> Signif. codes:  0 <80><98>***<80><99> 0.001 <80><98>**<80><99> 0.01
>    >>> <80><98>*<80>        <99> 0.05 <80><98>.<80><99> 0.1 <80><98> <80><99> 1
>    >>> 
>    >>> Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
>    >>> 
>    >>> hmm... they go away when I paste them in...
>    >> 
>    >> Check the character set in use.  You probably are using a UTF-8
>    >> encoding in an environment that does not support display of that
>    >> character set.
>
>    Dan> I feel it is something utf8-ish, I am using emacs-ess-5.2.3, which was
>    Dan> working fine under R-2.0.0, but developed this problem under R-2.1.0
>
>ess-5.2.3  is outdated for the purpose of well cooperating with
>UTF-8 outputs from R 2.1.x
>IIRC, ESS cooperates with   R "with UTF encoding" since about ESS 5.2.6.
>but ESS-5.2.8 is current  { --> http://ESS.r-project.org/ )
>
>      ..........
>
>    Dan> I have gone back to 2.0.0 :)
>
>Don't do that!
>You've lost tons of nice new features and gained quite an amount
>of old bugs by downgrading .. 

Ahhh ... thank you for making this clear to me now :)

Sadly I can't find R-2.1.x rpm on CRAN? 


>
>Martin
>



From dmb at mrc-dunn.cam.ac.uk  Tue Jun  7 17:39:14 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 7 Jun 2005 16:39:14 +0100 (BST)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.58.0506071004590.21844@maplepark.com>
Message-ID: <Pine.LNX.4.21.0506071638390.14637-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 7 Jun 2005, David Forrest wrote:

>On Tue, 7 Jun 2005, Dan Bolser wrote:
>...
>> I agree, but its hard to get people contributing without something
>> centralized (some central autohrity). The problem with the existing wiki
>>
>> http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>>
>> Is that it currenlty does not support images (afaik).
>
>Counterexample:
>
>http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?Tips_And_Examples

Great! 

We would need an image bin to support this though (for the average user).


>
>
>Dave
>



From drf5n at maplepark.com  Tue Jun  7 18:10:14 2005
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 7 Jun 2005 11:10:14 -0500 (CDT)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.21.0506071638390.14637-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506071638390.14637-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.58.0506071106080.21844@maplepark.com>

On Tue, 7 Jun 2005, Dan Bolser wrote:

> On Tue, 7 Jun 2005, David Forrest wrote:
>
> >On Tue, 7 Jun 2005, Dan Bolser wrote:
> >...
> >> I agree, but its hard to get people contributing without something
> >> centralized (some central autohrity). The problem with the existing wiki
> >>
> >> http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
> >>
> >> Is that it currenlty does not support images (afaik).
> >
> >Counterexample:
> >
> >http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?Tips_And_Examples
>
> Great!
>
> We would need an image bin to support this though (for the average user).

Something like http://www.imageshack.us/  perhaps?

http://img97.echo.cx/img97/4207/teapot24fl.png or
http://img97.echo.cx/my.php?image=teapot24fl.png

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From rhurlin at gwdg.de  Tue Jun  7 18:21:48 2005
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Tue, 07 Jun 2005 18:21:48 +0200
Subject: [R] Unable to compile R-2.1.0
In-Reply-To: <Pine.LNX.4.61.0506061449310.24586@gannet.stats>
References: <429C8F5C0000907B@ims3e.cp.tin.it>
	<Pine.LNX.4.61.0506061449310.24586@gannet.stats>
Message-ID: <42A5C99C.9070001@gwdg.de>

Prof Brian Ripley wrote:
> This has been reported to R-bugs twice already (and also on the lists): 
> it seems autoconf and FreeBSD are making different assumptions.
> 
> If you look in today's R-devel archive you will see the requests for 
> information I set on the second report.
> 
> It seems no one using FreeBSD ever tested the alphas or betas of R 2.1.0.

In the last years there were no serious problems with R on FreeBSD. So 
most of us FreeBSD users never had to test alphas and betas :-)

But you are right, we should test them from now on, thank you for this 
advice.


On Mon, 6 Jun 2005 v.demartino2 at virgilio.it wrote:
>> I downloaded the latest R-2.1.0 tarball from cran (the one of 18/4/05) to
>> compile it under FreeBSD. Take into account that I compiled R-2.0.1 in 
>> the
>> same machine and OS like a charme, flawlessly and at the very fiirst 
>> shot.
>>
>> Now with R-2.1.0, ./configure doesn't seem to say anything alarming but
>> make stops. Here an extract of both ./configure and make:
>>
>> # ./configure
>> ..................................
>> R is now configured for i386-unknown-freebsd5.4
>>
>>  Source directory:          .
>>  Installation directory:    /usr/local
>>
>>  C compiler:                gcc -D__NO_MATH_INLINES -g -O2
>>  C++ compiler:              g++  -g -O2
>>  Fortran compiler:          f77  -g -O2
>>
>>  Interfaces supported:      X11, tcltk
>>  External libraries:        readline, BLAS(ATLAS)
>>  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>>  Options enabled:           R profiling
>>
>>  Recommended packages:      yes
>>
>> configure: WARNING: you cannot build info or html versions of the R 
>> manuals

The last problem, reported by the configure script, has a solution:
To build the html and info versions of the R manuals you need at least 
Makeinfo 4.7. In FreeBSD6-CURRENT there had been an update a few days 
ago from Makeinfo 4.6 to 4.8. For other versions of FreeBSD (including 
5.4) you can install a new version via the ports system (print/texinfo).


-- 
Dr. Rainer Hurling                           Phone: (049) 551 69401-145
Abteilung Waldschutz, Sachgebiet Insekten    Fax:   (049) 551 69401-160
Nieders??chsische Forstliche Versuchsanstalt  Mobil: (049) 160 5835-143
Gr??tzelstra??e 2, D-37079 G??ttingen, GERMANY  Rainer.Hurling at nfv.gwdg.de



From reid_huntsinger at merck.com  Tue Jun  7 18:23:10 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 7 Jun 2005 12:23:10 -0400
Subject: [R] transform large matrix into list
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9481@uswpmx00.merck.com>

This is almost the "triplet representation" of sparse matrices, except the
triplet representation excludes zero entries. If you have a lot of zeros, it
would be worth a look at SparseM and Matrix.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
Sent: Tuesday, June 07, 2005 9:42 AM
To: 'Stefan Mischke'; r-help at stat.math.ethz.ch
Subject: RE: [R] transform large matrix into list


Here's one way to do it:

> M
   x1 x2
y1  1  2
y2  4  5
y3  7  8
> (L <- cbind(v=c(M), x=c(col(M)), y=c(row(M))))
     v x y
[1,] 1 1 1
[2,] 4 1 2
[3,] 7 1 3
[4,] 2 2 1
[5,] 5 2 2
[6,] 8 2 3

(The seemingly extraneous c()'s are to drop the dimensions of those
matrices.)

Are you sure your L[3, 3] is correct?

Also, for a 10K^2 M, L is going to take up over 1.1GB of memory!  You may
want to look at SparseM and Matrix packages, which have facilities for
handling sparse matrices.

Andy

> From: Stefan Mischke
> 
> Dear List
> 
> I need to transform a large matrix M with many NAs into a list L with 
> one row for each non missing cell. Every row should contain the cell 
> value in the first column, and its coordinates of the matrix 
> in column 
> 2 and 3.
> 
> M:
> 	x1	x2
> y1	1	2
> y2	4	5
> y3	7	8
> 
> L:
> v	x	y
> 1	1	1
> 4	1	2
> 7	1	2
> 2	2	1
> 5	2	2
> 8	2	3
> 
> I'm trying to do this with a loop, but since my matrix is quite large 
> (around 10k^2) this just takes a very long time.
> There must be a more efficient and elegant way to do this. Any hints?
> 
> Thanks,
> Stefan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From drf5n at maplepark.com  Tue Jun  7 18:25:20 2005
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 7 Jun 2005 11:25:20 -0500 (CDT)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.21.0506071638390.14637-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0506071638390.14637-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.58.0506071122440.21844@maplepark.com>

On Tue, 7 Jun 2005, Dan Bolser wrote:
...
> >http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?Tips_And_Examples
>
> Great!
>
> We would need an image bin to support this though (for the average user).

Hi Dan & all

   http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?UsePictures

This page demonstrates the use of pictures and the use of a public image
server for hosting graphics files.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From devens8765 at yahoo.com  Tue Jun  7 18:48:27 2005
From: devens8765 at yahoo.com (Dave Evens)
Date: Tue, 7 Jun 2005 09:48:27 -0700 (PDT)
Subject: [R] reading non-existing  files
Message-ID: <20050607164827.28228.qmail@web61315.mail.yahoo.com>


Dear all,

I'm trying to read to a collection of files in a loop
using odbcConnectExcel - but not all of the files
exist. This is the code I have

for(i in 1:no.of.subs){
   channel <- odbcConnectExcel(paste(working.dir,
subs[i], ".xls", sep=""))
   datafiles[[i]] <- as.matrix(sqlFetch(channel,
"Data"))          
   close(channel)
}

I'm not sure how to alter the code to allow for the
fact that some files may not exist - these files
should be ignored.

Currently, I get the following error

Error in odbcTableExists(channel, sqtable) : 
        'Data': table not found on channel

- it however creates an empty file for the first
occurance of a non-existing file then stops.

I would very much apprepriate any help.

Thanks in advance.

Dave



		
__________________________________ 

Get on-the-go sports scores, stock quotes, news and more. Check it out!



From victor.gravenholt at gmail.com  Tue Jun  7 18:54:24 2005
From: victor.gravenholt at gmail.com (Victor Gravenholt)
Date: Tue, 07 Jun 2005 18:54:24 +0200
Subject: [R] Functions within functions in R and S-Plus
Message-ID: <42A5D140.1040803@gmail.com>

Sorry to bother you about a S-Plus related problem, but I hope someone 
can help.
I have tried to "translate" some code from R to S-Plus (I have colleague 
that insists on using S-Plus. And yes, I have tried to make him change 
to R...)
The following code works out fine in R, but in S-Plus (S-PLUS 6.2 for 
Windows Professional Ed.) I get the error message "Problem in 
est.theta(x): Object "func" not found ".
I have tried to keep most of the structure in my original problem (but 
simplified it!), so the code could seem a bit strange.
I suspect that this has something to do with different scoping rules in 
R and S-Plus, but still I have not found a workable solution in S-Plus.

mainfunc <- function(x){

est <- function(x,par){
abs(sum(par*x))
}

func <- function(par,x){
    est(x,par)
 }

est.theta <- function(x){
    optimize(func,lower=-10, upper=20,x=x)$minimum
    }

est.theta(x)

}

x <- 1:10
mainfunc(x)




Any help is greatly appreciated.

Victor



From dscully at fd9ns01.okladot.state.ok.us  Tue Jun  7 18:57:59 2005
From: dscully at fd9ns01.okladot.state.ok.us (dscully@fd9ns01.okladot.state.ok.us)
Date: Tue, 7 Jun 2005 11:57:59 -0500
Subject: [R] Function inside tapply
Message-ID: <OF953D2D38.30427627-ON86257019.005B2654-86257019.005D14C6@fd9ns01.okladot.state.ok.us>





I'm new to R and not an experienced writer of programs, which may help
explain my question.   I wish to create a table or data frame which
contains the quantiles of the columns in the data frame DF.    I wish to
produce a table T where T[1] shows me the quantiles of column DF[1] right
up through the entirety of DF.  Tried several approaches with limited
success. This looked like the best approach but I need a way to specify
which quantiles I'm interested in.  All I am able to achieve are the
default vales. This only seems to be a problem when the quantile function
is inside of tapply.
T<-tapply(DF,colnames(DF),quantile ,na.rm = TRUE)



From swantje.lobel at ebc.uu.se  Tue Jun  7 19:04:04 2005
From: swantje.lobel at ebc.uu.se (=?iso-8859-1?Q?Swantje_L=F6bel?=)
Date: Tue, 7 Jun 2005 19:04:04 +0200
Subject: [R] user-defined spatial correlation structure in geeglm/geese
Message-ID: <20050607170405.5EDD14833@limicola.its.uu.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/f4ef20e4/attachment.pl

From br44114 at gmail.com  Tue Jun  7 19:05:24 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 7 Jun 2005 13:05:24 -0400
Subject: [R] reading non-existing files
Message-ID: <8d5a3635050607100534af244a@mail.gmail.com>

file.exists():
if(!file.exists(your.file)) next

Or, try():
your.data <- try(as.matrix(whatever))
if (class(your.data) == "try-error") {something went wrong / the file
doesn't exist - just for logging, the code will not fail}



-----Original Message-----
From: Dave Evens [mailto:devens8765 at yahoo.com]
Sent: Tuesday, June 07, 2005 12:48 PM
To: r-help at stat.math.ethz.ch
Subject: [R] reading non-existing files



Dear all,

I'm trying to read to a collection of files in a loop
using odbcConnectExcel - but not all of the files
exist. This is the code I have

for(i in 1:no.of.subs){
   channel <- odbcConnectExcel(paste(working.dir,
subs[i], ".xls", sep=""))
   datafiles[[i]] <- as.matrix(sqlFetch(channel,
"Data"))          
   close(channel)
}

I'm not sure how to alter the code to allow for the
fact that some files may not exist - these files
should be ignored.

Currently, I get the following error

Error in odbcTableExists(channel, sqtable) : 
        'Data': table not found on channel

- it however creates an empty file for the first
occurance of a non-existing file then stops.

I would very much apprepriate any help.

Thanks in advance.

Dave



		
__________________________________ 

Get on-the-go sports scores, stock quotes, news and more. Check it out!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jun  7 19:05:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 13:05:31 -0400
Subject: [R] Functions within functions in R and S-Plus
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E944@usctmx1106.merck.com>

The easiest way, perhaps, is to assign the internal functions to frame 1:

> mainfunc <- function(x){
+ 
+ est <- function(x,par){
+ abs(sum(par*x))
+ }
+ 
+ func <- function(par,x){
+     est(x,par)
+  }
+ 
+ est.theta <- function(x){
+     optimize(func,lower=-10, upper=20,x=x)$minimum
+     }
+ 
+ assign("func", func, frame=1)
+ assign("est", est, frame=1)
+ est.theta(x)
+ 
+ }
> 
> x <- 1:10
> mainfunc(x)
[1] 5.286101e-006

You should read about the scoping rule differences in, e.g., S Programming.

Andy

> From: Victor Gravenholt
> 
> Sorry to bother you about a S-Plus related problem, but I 
> hope someone 
> can help.
> I have tried to "translate" some code from R to S-Plus (I 
> have colleague 
> that insists on using S-Plus. And yes, I have tried to make 
> him change 
> to R...)
> The following code works out fine in R, but in S-Plus (S-PLUS 6.2 for 
> Windows Professional Ed.) I get the error message "Problem in 
> est.theta(x): Object "func" not found ".
> I have tried to keep most of the structure in my original 
> problem (but 
> simplified it!), so the code could seem a bit strange.
> I suspect that this has something to do with different 
> scoping rules in 
> R and S-Plus, but still I have not found a workable solution 
> in S-Plus.
> 
> mainfunc <- function(x){
> 
> est <- function(x,par){
> abs(sum(par*x))
> }
> 
> func <- function(par,x){
>     est(x,par)
>  }
> 
> est.theta <- function(x){
>     optimize(func,lower=-10, upper=20,x=x)$minimum
>     }
> 
> est.theta(x)
> 
> }
> 
> x <- 1:10
> mainfunc(x)
> 
> 
> 
> 
> Any help is greatly appreciated.
> 
> Victor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Tue Jun  7 19:09:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 13:09:29 -0400
Subject: [R] Function inside tapply
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E945@usctmx1106.merck.com>

Is this what you want to do?

> DF <- data.frame(matrix(runif(500), ncol=5))
> (DFq <- sapply(DF, quantile, probs=seq(.1, .9, by=.1)))
           X1         X2        X3         X4        X5
10% 0.1175181 0.05375085 0.1023331 0.09187593 0.1508769
20% 0.2139076 0.20185996 0.2193513 0.21401223 0.2211100
30% 0.3117315 0.30172147 0.3470629 0.32661316 0.2838589
40% 0.4309932 0.41003894 0.4008221 0.39889547 0.4093088
50% 0.5445139 0.47162199 0.4923676 0.49485975 0.5254150
60% 0.6459154 0.54170930 0.5615777 0.59523613 0.6014709
70% 0.7077307 0.70178999 0.6502444 0.64961047 0.7031002
80% 0.7625163 0.79307446 0.7496251 0.80094534 0.7800664
90% 0.8602119 0.89546621 0.8271837 0.91172347 0.8579161

Andy

> From: dscully at fd9ns01.okladot.state.ok.us
> 
> I'm new to R and not an experienced writer of programs, which may help
> explain my question.   I wish to create a table or data frame which
> contains the quantiles of the columns in the data frame DF.   
>  I wish to
> produce a table T where T[1] shows me the quantiles of column 
> DF[1] right
> up through the entirety of DF.  Tried several approaches with limited
> success. This looked like the best approach but I need a way 
> to specify
> which quantiles I'm interested in.  All I am able to achieve are the
> default vales. This only seems to be a problem when the 
> quantile function
> is inside of tapply.
> T<-tapply(DF,colnames(DF),quantile ,na.rm = TRUE)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sundar.dorai-raj at pdf.com  Tue Jun  7 19:11:54 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 07 Jun 2005 12:11:54 -0500
Subject: [R] Functions within functions in R and S-Plus
In-Reply-To: <42A5D140.1040803@gmail.com>
References: <42A5D140.1040803@gmail.com>
Message-ID: <42A5D55A.30507@pdf.com>



Victor Gravenholt wrote:
> Sorry to bother you about a S-Plus related problem, but I hope someone 
> can help.
> I have tried to "translate" some code from R to S-Plus (I have colleague 
> that insists on using S-Plus. And yes, I have tried to make him change 
> to R...)
> The following code works out fine in R, but in S-Plus (S-PLUS 6.2 for 
> Windows Professional Ed.) I get the error message "Problem in 
> est.theta(x): Object "func" not found ".
> I have tried to keep most of the structure in my original problem (but 
> simplified it!), so the code could seem a bit strange.
> I suspect that this has something to do with different scoping rules in 
> R and S-Plus, but still I have not found a workable solution in S-Plus.
> 
> mainfunc <- function(x){
> 
> est <- function(x,par){
> abs(sum(par*x))
> }
> 
> func <- function(par,x){
>    est(x,par)
> }
> 
> est.theta <- function(x){
>    optimize(func,lower=-10, upper=20,x=x)$minimum
>    }
> 
> est.theta(x)
> 
> }
> 
> x <- 1:10
> mainfunc(x)
> 
> 
> 

You need to supply func to est.theta. R uses scoping to find func and 
est. S-PLUS scoping is defined differently and cannot, so you must 
assign them to the current frame.

is.R <- !is.null(version$language)
mainfunc <- function(x) {
   est <- function(x, par) {
     abs(sum(par * x))
   }
   func <- function(par, x) {
     est(x, par)
   }
   est.theta <- function(x) {
     optimize(func, lower = -10, upper = 20, x = x)$minimum
   }
   if(!is.R) {
     assign("func", func, frame = 1)
     assign("est", est, frame = 1)
   }
   est.theta(x)
}
mainfunc(1:10)

--sundar



From spencer.graves at pdf.com  Tue Jun  7 19:14:05 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 07 Jun 2005 10:14:05 -0700
Subject: [R] Functions within functions in R and S-Plus
In-Reply-To: <42A5D140.1040803@gmail.com>
References: <42A5D140.1040803@gmail.com>
Message-ID: <42A5D5DD.3070000@pdf.com>

How about the following:

mainfunc <- function(x){

est <- function(x,par){
abs(sum(par*x))
}

func <- function(par,x, est.=est){
    est.(x,par)
}

est.theta <- function(x, func.=func, est.=est){
    optimize(func.,lower=-10, upper=20,x=x,
		est.=est.)$minimum
    }

est.theta(x, func.=func, est.=est)

}

 > x <- 1:10
 > mainfunc(x)
[1] 5.286101e-006

	  spencer graves

Victor Gravenholt wrote:

> Sorry to bother you about a S-Plus related problem, but I hope someone 
> can help.
> I have tried to "translate" some code from R to S-Plus (I have colleague 
> that insists on using S-Plus. And yes, I have tried to make him change 
> to R...)
> The following code works out fine in R, but in S-Plus (S-PLUS 6.2 for 
> Windows Professional Ed.) I get the error message "Problem in 
> est.theta(x): Object "func" not found ".
> I have tried to keep most of the structure in my original problem (but 
> simplified it!), so the code could seem a bit strange.
> I suspect that this has something to do with different scoping rules in 
> R and S-Plus, but still I have not found a workable solution in S-Plus.
> 
> mainfunc <- function(x){
> 
> est <- function(x,par){
> abs(sum(par*x))
> }
> 
> func <- function(par,x){
>    est(x,par)
> }
> 
> est.theta <- function(x){
>    optimize(func,lower=-10, upper=20,x=x)$minimum
>    }
> 
> est.theta(x)
> 
> }
> 
> x <- 1:10
> mainfunc(x)
> 
> 
> 
> 
> Any help is greatly appreciated.
> 
> Victor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Tue Jun  7 19:15:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 07 Jun 2005 10:15:59 -0700
Subject: [R] Functions within functions in R and S-Plus
In-Reply-To: <42A5D5DD.3070000@pdf.com>
References: <42A5D140.1040803@gmail.com> <42A5D5DD.3070000@pdf.com>
Message-ID: <42A5D64F.907@pdf.com>

p.s.  This modification gave the same answer for me in both S-Plus 6.2 
and R 2.1.0 patched under Windows XP.

Spencer Graves wrote:

> How about the following:
> 
> mainfunc <- function(x){
> 
> est <- function(x,par){
> abs(sum(par*x))
> }
> 
> func <- function(par,x, est.=est){
>    est.(x,par)
> }
> 
> est.theta <- function(x, func.=func, est.=est){
>    optimize(func.,lower=-10, upper=20,x=x,
>         est.=est.)$minimum
>    }
> 
> est.theta(x, func.=func, est.=est)
> 
> }
> 
>  > x <- 1:10
>  > mainfunc(x)
> [1] 5.286101e-006
> 
>       spencer graves
> 
> Victor Gravenholt wrote:
> 
>> Sorry to bother you about a S-Plus related problem, but I hope someone 
>> can help.
>> I have tried to "translate" some code from R to S-Plus (I have 
>> colleague that insists on using S-Plus. And yes, I have tried to make 
>> him change to R...)
>> The following code works out fine in R, but in S-Plus (S-PLUS 6.2 for 
>> Windows Professional Ed.) I get the error message "Problem in 
>> est.theta(x): Object "func" not found ".
>> I have tried to keep most of the structure in my original problem (but 
>> simplified it!), so the code could seem a bit strange.
>> I suspect that this has something to do with different scoping rules 
>> in R and S-Plus, but still I have not found a workable solution in 
>> S-Plus.
>>
>> mainfunc <- function(x){
>>
>> est <- function(x,par){
>> abs(sum(par*x))
>> }
>>
>> func <- function(par,x){
>>    est(x,par)
>> }
>>
>> est.theta <- function(x){
>>    optimize(func,lower=-10, upper=20,x=x)$minimum
>>    }
>>
>> est.theta(x)
>>
>> }
>>
>> x <- 1:10
>> mainfunc(x)
>>
>>
>>
>>
>> Any help is greatly appreciated.
>>
>> Victor
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
>



From faceasec at uapar.edu  Tue Jun  7 19:33:33 2005
From: faceasec at uapar.edu (secretario academico FACEA)
Date: Tue, 07 Jun 2005 14:33:33 -0300
Subject: [R] about tables
Message-ID: <42A5DA6D.60906@uapar.edu>

Hi dear all
I'm doing some contingency tables and I'd like to know if it is posible 
to make only one table in R that shows me the responses of people who 
visit a place with some frecuency about one question with two 
posibilities (they are treated like two variables with options). I'm 
thinking of a table similar to the next below:

Frecuencia (Frec)

	

Alternative 1

	

Alternative 2

Good

	

Bad

	

Good

	

Bad

Todos los d??as
Entre 4 y 5 veces por semana
2 veces por semana
Entre 1 y 3 veces al mes
Primera vez
Sin respuesta

	

 

	

 

	

 

	

 



Another thing I'd like to do is to make a contingency table that relates 
a categorical data with a numerical data. They are the frecuency of 
visit to a place with the ages of those people, but I need to summarize 
the age into intervals that outcomes from a Histogram. For instance, 
"hist" function with $breaks and $counts retunrs the intervals limits 
and frecuency of data into each  interval.

Frecuencia (Frec)

	

Edad(Age)

15-20

	

20-25

	

25-30

	

30-35

Todos los d??as
Entre 4 y 5 veces por semana
2 veces por semana
Entre 1 y 3 veces al mes
Primera vez
Sin respuesta

	


	
	
	


Thanks a lot for your help

Adri??n C.

From celebridades at megamail.pt  Tue Jun  7 19:41:33 2005
From: celebridades at megamail.pt (alex diaz)
Date: Tue, 07 Jun 2005 18:41:33 +0100
Subject: [R] multiple plots
Message-ID: <1118166093.42a5dc4d8066b@roma-hme1>

Hi:
I am learning to use R and I am experiencing some 
difficulties in writing a function to produce multiple 
plots.
This is a single plot

Subset(myframe, color==îblueî & class==1)
Plot(myframe$p1, myframe$p2) 

My problem is that I have six colors (blue, red, 
green,....) and 10 classes, that is 60 graphs! I think 
I can use par(mfrow=c(8,8)), but I donít know how to 
generate such number of plots in a parsimonious way.
Can someone give me a hand please?

alex

-------------------------------------------------
Email Enviado utilizando o serviÅÁo MegaMail



From wmwang at gmail.com  Tue Jun  7 20:11:15 2005
From: wmwang at gmail.com (Weimin Wang)
Date: Tue, 7 Jun 2005 20:11:15 +0200
Subject: [R] Error calling "read.table" from Python
Message-ID: <6a88388a05060711117deb6e6f@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/d2341d8e/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Tue Jun  7 20:15:22 2005
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 7 Jun 2005 19:15:22 +0100 (BST)
Subject: [R] Help with possible bug (assigning NA value to data.frame)?
Message-ID: <Pine.LNX.4.21.0506071845400.16744-100000@mail.mrc-dunn.cam.ac.uk>


This 'strange behaviour' manifest itself within some quite complex
code. When I created a *very* simple example the behaviour dissapeared. 

Here is the simplest version I have found which still causes the strange
behaviour (it could be quite unrelated to the boot library, however).


library(boot)
 
## boot statistic function
my.mean.s <- function(data,subset){
  mean(data[subset])
}

## dummy data, deliberatly no variance
my.test.dat.1 <- rep(4,5)
my.test.dat.2 <- rep(8,5)

## not much can happen here
my.test.boot.1 <- boot( my.test.dat.1, my.mean.s, R=10 )
my.test.boot.2 <- boot( my.test.dat.2, my.mean.s, R=10 )

## returns a null object as ci is meaningless for this data
my.test.boot.ci.1 <- boot.ci(my.test.boot.1,type='normal')
my.test.boot.ci.2 <- boot.ci(my.test.boot.2,type='normal')


## now try to store this data (the problem begins)...

## dummy existing data 
a <- data.frame(matrix(c(1,2,3,4),nrow=2))

## make space for new data
a$X3 <- NA
a$X4 <- NA

## try to store the upper and lower ci (not) calculated above
a[a$X1==1,]$X3 <-  my.test.boot.ci.1$normal[2]
a[a$X1==1,]$X4 <-  my.test.boot.ci.1$normal[3]
a[a$X1==2,]$X3 <-  my.test.boot.ci.1$normal[2]
a[a$X1==2,]$X4 <-  my.test.boot.ci.1$normal[3]

a


What I see is 

> a
  X1 X2 X3 X4
1  1  3 NA  1
2  2  4 NA  2


What I expected to see was

> a
  X1 X2 X3 X4
1  1  3 NA  NA
2  2  4 NA  NA

Some how the last assignment of the data from within the null object
assigns the value of the '==x' part of the logical vector subscript.

If I make the following (trivial?) adjustment 

a[a$X1==1,]$X4 <-  my.test.boot.ci.1$normal[3]
a[a$X1==1,]$X3 <-  my.test.boot.ci.a$normal[2]
a[a$X1==2,]$X4 <-  my.test.boot.ci.1$normal[3]
a[a$X1==2,]$X3 <-  my.test.boot.ci.1$normal[2]


The output changes to 

> a
  X1 X2 X3 X4
1  1  3  1  1
2  2  4  2  2

Which is even wronger.



Not sure if this is usefull without the full context, but here is the
output from the real version of this program (where most of the above code
is within a loop). What is printed out for each cycle of the loop is the
value of the '==x' part of the subscript.


[1] 2
[1] 3
[1] 4
[1] 5
[1] "All values of t are equal to  1 \n Cannot calculate confidence
intervals"
[1] 6
[1] 7
[1] "All values of t are equal to  1 \n Cannot calculate confidence
intervals"
[1] 8
[1] 10
[1] 11
[1] "All values of t are equal to  1 \n Cannot calculate confidence
intervals"
> 


Above you see that for some values I can't calculate a ci (but storing it
as above), then...

> dat.5.ho
  CHAINS DOM_PER_CHAIN     lower     upper
1      2      1.416539 1.3626253  1.468387
2      3      1.200000 1.1146014  1.288724
3      4      1.363636 1.2675657  1.462571
4      5      1.000000        NA  5.000000
5      6      1.323529 1.0991974  1.546156
6      7      1.000000        NA  7.000000
7      8      1.100000 0.9037904  1.289210
8     10      1.142857 0.8775104  1.403918
9     11      1.000000        NA 11.000000
> 


Do you spot the same problem? Namely for each value of the 'CHAINS' column
that was unable to calculate a ci, the second assignment to the data table
from the 'null' object assigned the lookup value of CHAINS to that column
instead! The assignment (within the loop) looks like this...

  dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  x.s.ci$normal[2]
  dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  x.s.ci$normal[3]

(where chain is the 'loop variable').


As far as I can tell this is a bug. It dosn't happen when I try...
 
  dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  NA
  dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  NA 


And doing the following (swapping the order) changes the behaviour...

  dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  x.s.ci$normal[3]
  dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  x.s.ci$normal[2]
  

Giving...

> dat.5.ho
  CHAINS DOM_PER_CHAIN      lower     upper
1      2      1.416539  1.3616070  1.472716
2      3      1.200000  1.1134237  1.287601
3      4      1.363636  1.2587204  1.466037
4      5      1.000000  5.0000000  5.000000
5      6      1.323529  1.1082482  1.547222
6      7      1.000000  7.0000000  7.000000
7      8      1.100000  0.9021282  1.287672
8     10      1.142857  0.8766731  1.403327
9     11      1.000000 11.0000000 11.000000


Which is again incorrect and unpredicted (as above). 


Please let me know what to do to report this problem better, or if I just
missed something silly.

I am RH9, R-2.1.0 (compiled from source), latest boot from CRAN (if that
makes a difference).

Cheers,
Dan.



From ross at biostat.ucsf.edu  Tue Jun  7 20:36:17 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 07 Jun 2005 11:36:17 -0700
Subject: [R] How to change the value of a class slot
In-Reply-To: <200506062115.j56LF3Cq026619@faraday.gene.com>
References: <200506062115.j56LF3Cq026619@faraday.gene.com>
Message-ID: <1118169377.2408.74.camel@iron.libaux.ucsf.edu>

On Mon, 2005-06-06 at 14:15 -0700, Berton Gunter wrote:
> I'm puzzled:
> 
> > It looks as if instances of class objects are best thought of as
> > immutable once created.
> > 
> 
> what then is setReplaceMethod() for? 
assignment operators do the whole object replacement behind the scenes,
at least conceptually, as far as I can tell.  I agree: they are
mutators.  But outside of this special case, it seems mutation of slots
is difficult (i.e., requires the assistance of the caller).

By the way, the documentation on setReplaceMethod does not actually say
what it does.  I found out by looking at the code.

Second, in my experiments I couldn't get setReplacementMethod to work:

"bumpIndex<-" <- function(pm, value) {
  pm at i <- pm at i+as.integer(value)
  pm
}

# I get an error without the next function definition
bumpIndex <- function(pm) pm at i

setReplaceMethod("bumpIndex",
                 signature=signature(pm="CompletePathMaker",
                   value="numeric"), bumpIndex) 

When I try to load this, I get
arguments in definition changed from (spec) to (object)
arguments in definition changed from (self) to (object)
arguments in definition changed from (self) to (object)
Creating a new generic function for 'bumpIndex<-' in '.GlobalEnv'
Error in conformMethod(signature, mnames, fnames, f) : 
	In method for function "bumpIndex<-": formal arguments omitted in the
method definition cannot be in the signature (value = "numeric")

All the errors are triggered by setReplaceMethod.  Can anyone help me
interpret them?  Or, maybe better, tell me how to debug the
"compilation"?


> I leave it to language "experts" to say whether S4 formal classes and
> methods are wise or not in comparison to others. From my fairly ignorant
> perspective, that always seems to be a matter of taste.

There are actually two related issues on that score: first, whether the
complex of expectation set up by talking about "objects" and "classes"
are met by what R/S does, and second the wisdom of what R/S does in its
own right.

> 
> Cheers,
> Bert
> 
> 
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From ccleland at optonline.net  Tue Jun  7 20:53:21 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 07 Jun 2005 14:53:21 -0400
Subject: [R] multiple plots
In-Reply-To: <1118166093.42a5dc4d8066b@roma-hme1>
References: <1118166093.42a5dc4d8066b@roma-hme1>
Message-ID: <42A5ED21.3070102@optonline.net>

How about this:

myframe <- expand.grid(p1 = rnorm(10), p2 = rnorm(10),
                        classes = paste("class", 1:10, sep=""),
                        colors = c("blue", "red", "green", "yellow", 
"orange", "purple"))

library(lattice)

xyplot(p2 ~ p1 | classes * colors, data=myframe)

If you want the 60 panels on more than one page, see the layout argument 
to xyplot.  For example,

xyplot(p2 ~ p1 | classes * colors, data=myframe, layout=c(6,5,2))

alex diaz wrote:
> Hi:
> I am learning to use R and I am experiencing some 
> difficulties in writing a function to produce multiple 
> plots.
> This is a single plot
> 
> Subset(myframe, color==îblueî & class==1)
> Plot(myframe$p1, myframe$p2) 
> 
> My problem is that I have six colors (blue, red, 
> green,....) and 10 classes, that is 60 graphs! I think 
> I can use par(mfrow=c(8,8)), but I donít know how to 
> generate such number of plots in a parsimonious way.
> Can someone give me a hand please?
> 
> alex
> 
> -------------------------------------------------
> Email Enviado utilizando o serviÅÁo MegaMail
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ross at biostat.ucsf.edu  Tue Jun  7 20:58:09 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 07 Jun 2005 11:58:09 -0700
Subject: [R] How to change the value of a class slot
In-Reply-To: <42A54069.6090108@statistik.uni-dortmund.de>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
	<17057.57384.831362.960765@stat.math.ethz.ch>
	<20050606203517.GN3134@wheat.boylan.org>
	<42A54069.6090108@statistik.uni-dortmund.de>
Message-ID: <1118170689.2410.94.camel@iron.libaux.ucsf.edu>

On Tue, 2005-06-07 at 08:36 +0200, Uwe Ligges wrote:
> Ross Boylan wrote:
> 
> > On Sat, Jun 04, 2005 at 07:08:56PM +0200, Martin Maechler wrote:
> > 
> >>    Ross> nextPath <- function(pm){ #pm is a CompletePathMaker
> >>    Ross>    pm at i <- pm at i+as.integer(1)
> >>    Ross> [etc]
> >>
> >>If your nextPath   function has  'pm' as its last statement it
> >>will return the updated object, and if you call it
> >>as
> >>	mypm <- nextPath(mypm)
> >>
> >>you are
> >>    1) updating  mypm
> >>    2) in a proper S way (i.e. no cheating).
> >>
> >>Regards,
> >>Martin
> > 
> > 
> > Wow.  This is almost the exact inverse of the usual object behavior,
> > in which only the class itself can update the slots (aka instance
> > variables).  None of the methods of the class can update instances of
> > the class persistently without the help of outsiders, and only
> > outsiders can change the slot values.
> > 
> > (Yes, I realize that using the idiom you suggest of returning a new
> > object one can have only class methods actually fiddling with the
> > slots.)
> > 
> > The inability of a class method to change a class object without
> > outside help seems unfortunate.
> > 
> > It looks as if instances of class objects are best thought of as
> > immutable once created.
> 
> Obviously, there are many definition of "object oriented" programming, 
> and yours seems to be different from the S4 definition.
Yes. And though there are many definitions of "object oriented" (at
least, many implementations),  I'd say the minimum requirement to be
object oriented is to have objects that encapsulate both state (instance
variables/slots) and behavior (methods).

S4 objects do not fully encapsulate state because they require outside
assistance to alter the state of the object (with the exception of
assignment operators).  The smalltalker in me also gets nervous that
code outside the class can access the slots, but there are many object
systems that act that way.

The way in which names of methods of unrelated classes interfere with
each other seems a break-down of the encapsulation of behavior, though
the problem strictly is not with the behavior but just with the name.
To return to the concrete problem that got me started, if class
Specification defines a method likelihood taking as arguments instances
of class Specification, Path and Parameters, then it is awkward to
define a method likelihood for the class Model when that method has
arguments of class Model, Specification, data.frame, and vector,
particularly if different names for the formal arguments are desired.
(I think technically it could be done, but only in a very ugly
way--i.e., better to use different method names for the two classes).

> 
> I was going to answer your first question at first, but you have not 
> given enough details - in particular it was not clear to me why your 
> approach did not work. 
> I assumed that you are assigning the new object 
> again, which is the S way. 
I wasn't, which is why it didn't work.  I wanted the function to return
some other value than the object it was operating on.
> You have to think about scoping rules and it 
> will be clear that the approach you are expecting is not a clean one in S.
Could you say a bit more about that?  I had thought of the issue more in
terms of function calls in S being call by value, preventing updates to
the original arguments.  So the issue isn't so much the scope of the
names of function arguments (that scope being limited to the function
body), but the properties of the thing they refer to (conceptually, a
copy of the argument, not the original).



From david.p.finlayson at gmail.com  Tue Jun  7 21:11:25 2005
From: david.p.finlayson at gmail.com (David Finlayson)
Date: Tue, 7 Jun 2005 12:11:25 -0700
Subject: [R] Specifying medoids in PAM?
Message-ID: <be6d17205060712111a263c56@mail.gmail.com>

I am using the PAM algorithm in the CLUSTER library. 

When I allow PAM to seed the medoids using the default __build__
algorithm things work
well:

> pam(stats.table, metric="euclidean", stand=TRUE, k=5)

But I have some clusters from a Hierarchical analysis that I would
like to use as seeds for the PAM algorithm. I can't figure what the
mediod argument wants. When I put in the five integer indicies for the
observations in stats.table that I would like to use as seeds (the row
numbers), I segfault R.

> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)

*** R Crashes ***

Here is my version info:
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R      

Any guidance would be appreciated.

David

-- 
David Finlayson
Marine Geology & Geophysics
School of Oceanography
Box 357940
University of Washington
Seattle, WA  98195-7940
USA

Office: Marine Sciences Building, Room 112
Phone: (206) 616-9407
Web: http://students.washington.edu/dfinlays



From andy_liaw at merck.com  Tue Jun  7 21:15:14 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 15:15:14 -0400
Subject: [R] Help with possible bug (assigning NA value to
 data.frame) ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E94A@usctmx1106.merck.com>

There's something peculiar that I do not understand here.  However, did you
realize that the thing you are assigning into parts of `a' is NULL?  Check
you're my.test.boot.ci.1:  It's NULL.

Be that as it may, I get:

> a <- data.frame(matrix(1:4, nrow=2), X3=NA, X4=NA)
> a
  X1 X2 X3 X4
1  1  3 NA NA
2  2  4 NA NA
> a[a$X1 == 1,]$X3 <- NULL
> a
  X1 X2 X3 X4
1  1  3 NA  1
2  2  4 NA NA
> a[a$X1 == 1,]$X4 <- NULL
> a
  X1 X2 X3 X4
1  1  3 NA  1
2  2  4 NA NA

which really baffles me...

In any case, that's not how I would assign into part of a data frame.  I
would do either

    a[a$X1 == 1, "X3"] <- something

or

    a$X3[a$X1 == 1] <- something

In either case you'd get an error if `something' is NULL.

Andy

> From: Dan Bolser
> 
> 
> This 'strange behaviour' manifest itself within some quite complex
> code. When I created a *very* simple example the behaviour 
> dissapeared. 
> 
> Here is the simplest version I have found which still causes 
> the strange
> behaviour (it could be quite unrelated to the boot library, however).
> 
> 
> library(boot)
>  
> ## boot statistic function
> my.mean.s <- function(data,subset){
>   mean(data[subset])
> }
> 
> ## dummy data, deliberatly no variance
> my.test.dat.1 <- rep(4,5)
> my.test.dat.2 <- rep(8,5)
> 
> ## not much can happen here
> my.test.boot.1 <- boot( my.test.dat.1, my.mean.s, R=10 )
> my.test.boot.2 <- boot( my.test.dat.2, my.mean.s, R=10 )
> 
> ## returns a null object as ci is meaningless for this data
> my.test.boot.ci.1 <- boot.ci(my.test.boot.1,type='normal')
> my.test.boot.ci.2 <- boot.ci(my.test.boot.2,type='normal')
> 
> 
> ## now try to store this data (the problem begins)...
> 
> ## dummy existing data 
> a <- data.frame(matrix(c(1,2,3,4),nrow=2))
> 
> ## make space for new data
> a$X3 <- NA
> a$X4 <- NA
> 
> ## try to store the upper and lower ci (not) calculated above
> a[a$X1==1,]$X3 <-  my.test.boot.ci.1$normal[2]
> a[a$X1==1,]$X4 <-  my.test.boot.ci.1$normal[3]
> a[a$X1==2,]$X3 <-  my.test.boot.ci.1$normal[2]
> a[a$X1==2,]$X4 <-  my.test.boot.ci.1$normal[3]
> 
> a
> 
> 
> What I see is 
> 
> > a
>   X1 X2 X3 X4
> 1  1  3 NA  1
> 2  2  4 NA  2
> 
> 
> What I expected to see was
> 
> > a
>   X1 X2 X3 X4
> 1  1  3 NA  NA
> 2  2  4 NA  NA
> 
> Some how the last assignment of the data from within the null object
> assigns the value of the '==x' part of the logical vector subscript.
> 
> If I make the following (trivial?) adjustment 
> 
> a[a$X1==1,]$X4 <-  my.test.boot.ci.1$normal[3]
> a[a$X1==1,]$X3 <-  my.test.boot.ci.a$normal[2]
> a[a$X1==2,]$X4 <-  my.test.boot.ci.1$normal[3]
> a[a$X1==2,]$X3 <-  my.test.boot.ci.1$normal[2]
> 
> 
> The output changes to 
> 
> > a
>   X1 X2 X3 X4
> 1  1  3  1  1
> 2  2  4  2  2
> 
> Which is even wronger.
> 
> 
> 
> Not sure if this is usefull without the full context, but here is the
> output from the real version of this program (where most of 
> the above code
> is within a loop). What is printed out for each cycle of the 
> loop is the
> value of the '==x' part of the subscript.
> 
> 
> [1] 2
> [1] 3
> [1] 4
> [1] 5
> [1] "All values of t are equal to  1 \n Cannot calculate confidence
> intervals"
> [1] 6
> [1] 7
> [1] "All values of t are equal to  1 \n Cannot calculate confidence
> intervals"
> [1] 8
> [1] 10
> [1] 11
> [1] "All values of t are equal to  1 \n Cannot calculate confidence
> intervals"
> > 
> 
> 
> Above you see that for some values I can't calculate a ci 
> (but storing it
> as above), then...
> 
> > dat.5.ho
>   CHAINS DOM_PER_CHAIN     lower     upper
> 1      2      1.416539 1.3626253  1.468387
> 2      3      1.200000 1.1146014  1.288724
> 3      4      1.363636 1.2675657  1.462571
> 4      5      1.000000        NA  5.000000
> 5      6      1.323529 1.0991974  1.546156
> 6      7      1.000000        NA  7.000000
> 7      8      1.100000 0.9037904  1.289210
> 8     10      1.142857 0.8775104  1.403918
> 9     11      1.000000        NA 11.000000
> > 
> 
> 
> Do you spot the same problem? Namely for each value of the 
> 'CHAINS' column
> that was unable to calculate a ci, the second assignment to 
> the data table
> from the 'null' object assigned the lookup value of CHAINS to 
> that column
> instead! The assignment (within the loop) looks like this...
> 
>   dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  x.s.ci$normal[2]
>   dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  x.s.ci$normal[3]
> 
> (where chain is the 'loop variable').
> 
> 
> As far as I can tell this is a bug. It dosn't happen when I try...
>  
>   dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  NA
>   dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  NA 
> 
> 
> And doing the following (swapping the order) changes the behaviour...
> 
>   dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  x.s.ci$normal[3]
>   dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  x.s.ci$normal[2]
>   
> 
> Giving...
> 
> > dat.5.ho
>   CHAINS DOM_PER_CHAIN      lower     upper
> 1      2      1.416539  1.3616070  1.472716
> 2      3      1.200000  1.1134237  1.287601
> 3      4      1.363636  1.2587204  1.466037
> 4      5      1.000000  5.0000000  5.000000
> 5      6      1.323529  1.1082482  1.547222
> 6      7      1.000000  7.0000000  7.000000
> 7      8      1.100000  0.9021282  1.287672
> 8     10      1.142857  0.8766731  1.403327
> 9     11      1.000000 11.0000000 11.000000
> 
> 
> Which is again incorrect and unpredicted (as above). 
> 
> 
> Please let me know what to do to report this problem better, 
> or if I just
> missed something silly.
> 
> I am RH9, R-2.1.0 (compiled from source), latest boot from 
> CRAN (if that
> makes a difference).
> 
> Cheers,
> Dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From tlumley at u.washington.edu  Tue Jun  7 21:46:23 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 7 Jun 2005 12:46:23 -0700 (PDT)
Subject: [R] Functions within functions in R and S-Plus
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E944@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E944@usctmx1106.merck.com>
Message-ID: <Pine.A41.4.61b.0506071244110.37764@homer03.u.washington.edu>

On Tue, 7 Jun 2005, Liaw, Andy wrote:

> The easiest way, perhaps, is to assign the internal functions to frame 1:
>

Another easy way is to use the MC() function in section 3.3.1 of the R 
FAQ, which makes function closures in a way that works in S-PLUS.

 	-thomas



From tlumley at u.washington.edu  Tue Jun  7 22:04:12 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 7 Jun 2005 13:04:12 -0700 (PDT)
Subject: [R] write.dta limits
In-Reply-To: <m24qcazmd5.fsf@ganymede.home.net>
References: <Pine.SGI.4.40.0506060922480.5653528-100000@origin.chass.utoronto.ca>
	<Pine.A41.4.61b.0506061826420.280524@homer04.u.washington.edu>
	<m24qcazmd5.fsf@ganymede.home.net>
Message-ID: <Pine.A41.4.61b.0506071300320.37764@homer03.u.washington.edu>

On Tue, 7 Jun 2005, David Whiting wrote:
>
> When I have encountered this error message in the past seems to have
> resulted from a blank/empty level in a factor or an empty character.
> For example:
>

I've sent a fixed version to CRAN.

The C standard says that the return value of fwrite() is equal to the 
number of objects it was asked to write unless an error occurs, but that 
appears not to be the case for some implementations when the objects have 
zero size.

 	-thomas



From gunter.berton at gene.com  Tue Jun  7 22:49:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 7 Jun 2005 13:49:58 -0700
Subject: [R] How to change the value of a class slot
In-Reply-To: <1118169377.2408.74.camel@iron.libaux.ucsf.edu>
Message-ID: <200506072050.j57Knxf9022512@compton.gene.com>

> Second, in my experiments I couldn't get setReplacementMethod to work:
> 
> "bumpIndex<-" <- function(pm, value) {
>   pm at i <- pm at i+as.integer(value)
>   pm
> }
> 
> # I get an error without the next function definition
> bumpIndex <- function(pm) pm at i
> 
> setReplaceMethod("bumpIndex",
>                  signature=signature(pm="CompletePathMaker",
>                    value="numeric"), bumpIndex) 
> 


'bumpIndex' is apparently not a generic function (created by setGeneric)
signature is not an argument to setReplaceMethod 
The ... argument to setReplaceMethod must include a function to do the
replacement, which is why you need the bumpIndex function (I agree that this
is not cleanly documented, but it's fairly easy to see since
setReplaceMethod calls setMethod and that requires the function argument).

so I don't see why you expect this to work at all. At the very least,
shouldn't you do what the documentation tells you to? I think that you are
expecting R to work according to your paradigm rather than trying to
understand how it actually works. If you feel that R's paradigm (and/or
documentation) is too awful to bother with, you might consider the R.oo
package, which does OOP an entirely different way.

-- Bert Gunter



> When I try to load this, I get
> arguments in definition changed from (spec) to (object)
> arguments in definition changed from (self) to (object)
> arguments in definition changed from (self) to (object)
> Creating a new generic function for 'bumpIndex<-' in '.GlobalEnv'
> Error in conformMethod(signature, mnames, fnames, f) : 
> 	In method for function "bumpIndex<-": formal arguments 
> omitted in the
> method definition cannot be in the signature (value = "numeric")
> 
> All the errors are triggered by setReplaceMethod.  Can anyone help me
> interpret them?  Or, maybe better, tell me how to debug the
> "compilation"?
> 
> 
> > I leave it to language "experts" to say whether S4 formal 
> classes and
> > methods are wise or not in comparison to others. From my 
> fairly ignorant
> > perspective, that always seems to be a matter of taste.
> 
> There are actually two related issues on that score: first, 
> whether the
> complex of expectation set up by talking about "objects" and "classes"
> are met by what R/S does, and second the wisdom of what R/S 
> does in its
> own right.
> 
> > 
> > Cheers,
> > Bert
> > 
> > 
> -- 
> Ross Boylan                                      wk:  (415) 502-4031
> 530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
> Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
> University of California, San Francisco
> San Francisco, CA 94143-0840                     hm:  (415) 550-1062
> 
>



From kjetil at acelerate.com  Tue Jun  7 18:40:14 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 07 Jun 2005 12:40:14 -0400
Subject: [R] R and MLE
In-Reply-To: <20050607071824.GA7577@lubyanka.local>
References: <20050607071824.GA7577@lubyanka.local>
Message-ID: <42A5CDEE.50908@acelerate.com>

Ajay Narottam Shah wrote:

>I learned R & MLE in the last few days. It is great! I wrote up my
>explorations as
>
>  http://www.mayin.org/ajayshah/KB/R/mle/mle.html
>
>I will be most happy if R gurus will look at this and comment on how
>it can be improved.
>
>
>
>I have a few specific questions:
>
>* Should one use optim() or should one use stats4::mle()?
>
>  I felt that mle() wasn't adding much value compared with optim, and
>  in addition, I wasn't able to marry my likelihood functions to it.
>
>* One very nice feature of mle() is that you can specify a few
>  parameters which should be fixed in the estimation. How can one
>  persuade optim() to behave like that?
>
>  
>
give optim()  a function to optimize which do not depend on those 
parameters ...

>* Can one use deriv() and friends to get analytical derivatives of
>  these likelihood functions? I found I wasn't able to make headway
>  when I was using vector/matrix notation. I think the greatness of R
>  lies in a lovely vector/matrix notation, and it seems like a shame
>  to have to not use that when trying to do deriv().
>
>* For iid problems, the computation of the likelihood function and
>  it's gradient vector are inherently parallelisable. How would one go
>  about doing this within R?
>
>  
>
Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From hb at maths.lth.se  Tue Jun  7 23:54:14 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 07 Jun 2005 23:54:14 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <200506072050.j57Knxf9022512@compton.gene.com>
References: <200506072050.j57Knxf9022512@compton.gene.com>
Message-ID: <42A61786.7080302@maths.lth.se>

Berton Gunter wrote:
>>Second, in my experiments I couldn't get setReplacementMethod to work:
>>
>>"bumpIndex<-" <- function(pm, value) {
>>  pm at i <- pm at i+as.integer(value)
>>  pm
>>}
>>
>># I get an error without the next function definition
>>bumpIndex <- function(pm) pm at i
>>
>>setReplaceMethod("bumpIndex",
>>                 signature=signature(pm="CompletePathMaker",
>>                   value="numeric"), bumpIndex) 
>>
> 
> 
> 
> 'bumpIndex' is apparently not a generic function (created by setGeneric)
> signature is not an argument to setReplaceMethod 
> The ... argument to setReplaceMethod must include a function to do the
> replacement, which is why you need the bumpIndex function (I agree that this
> is not cleanly documented, but it's fairly easy to see since
> setReplaceMethod calls setMethod and that requires the function argument).
> 
> so I don't see why you expect this to work at all. At the very least,
> shouldn't you do what the documentation tells you to? I think that you are
> expecting R to work according to your paradigm rather than trying to
> understand how it actually works. If you feel that R's paradigm (and/or
> documentation) is too awful to bother with, you might consider the R.oo
> package, which does OOP an entirely different way.

I have not followed the thread before, but here is an how it could look 
like with R.oo:

setConstructorS3("CompletePathMaker", function(index=0) {
   extend(Object(), "CompletePathMaker",
     index = as.integer(index)
   )
})

setMethodS3("increase", "CompletePathMaker", function(this, dindex=+1) {
   this$index <- this$index + as.integer(dindex);
   # You do not have to return anything here....
})

cpm <- CompletePathMaker()
cpm$index <- 3
increase(cpm, 2)
print(cpm$index)

To add some control of what type of values you can assign to 'index', 
you can define a virtual field function set<Field>(), e.g.

setMethodS3("setIndex", "CompletePathMaker", function(this, index) {
   if (!is.numeric(index) || length(index) != 1)
     stop("Argument 'index' must be a single numeric: ", mode(index));
   if (index < 0)
     stop("Argument 'index' must be non-negative: ", index);
   this$index <- as.integer(index);
   # ... or here.
})

cpm$index <- 4
cpm$index <- -3  # Gives an error as wanted!

(Again...) note that setConstructorS3() and setMethodS3() are 
conveniency wrappers to define the CompletePathMaker(), 
increaseBumpIndex.CompletePathMaker() and 
setBumpIndex.CompletePathMaker(). The implementation of references is 
taken care of by the special class Object, which also implements support 
for virtual fields.

/Henrik

> -- Bert Gunter
> 
> 
> 
> 
>>When I try to load this, I get
>>arguments in definition changed from (spec) to (object)
>>arguments in definition changed from (self) to (object)
>>arguments in definition changed from (self) to (object)
>>Creating a new generic function for 'bumpIndex<-' in '.GlobalEnv'
>>Error in conformMethod(signature, mnames, fnames, f) : 
>>	In method for function "bumpIndex<-": formal arguments 
>>omitted in the
>>method definition cannot be in the signature (value = "numeric")
>>
>>All the errors are triggered by setReplaceMethod.  Can anyone help me
>>interpret them?  Or, maybe better, tell me how to debug the
>>"compilation"?
>>
>>
>>
>>>I leave it to language "experts" to say whether S4 formal 
>>
>>classes and
>>
>>>methods are wise or not in comparison to others. From my 
>>
>>fairly ignorant
>>
>>>perspective, that always seems to be a matter of taste.
>>
>>There are actually two related issues on that score: first, 
>>whether the
>>complex of expectation set up by talking about "objects" and "classes"
>>are met by what R/S does, and second the wisdom of what R/S 
>>does in its
>>own right.
>>
>>
>>>Cheers,
>>>Bert
>>>
>>>
>>
>>-- 
>>Ross Boylan                                      wk:  (415) 502-4031
>>530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
>>Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
>>University of California, San Francisco
>>San Francisco, CA 94143-0840                     hm:  (415) 550-1062
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From p.connolly at hortresearch.co.nz  Wed Jun  8 01:15:07 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 8 Jun 2005 11:15:07 +1200
Subject: [R] reading non-existing  files
In-Reply-To: <20050607164827.28228.qmail@web61315.mail.yahoo.com>
References: <20050607164827.28228.qmail@web61315.mail.yahoo.com>
Message-ID: <20050607231507.GK6253@hortresearch.co.nz>

On Tue, 07-Jun-2005 at 09:48AM -0700, Dave Evens wrote:

|> 
|> Dear all,
|> 
|> I'm trying to read to a collection of files in a loop
|> using odbcConnectExcel - but not all of the files
|> exist. This is the code I have

Try try().

Use a condition on what you get back to avoid attempting to do things
to empty files.

?try will be a good place to start.

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From reilly at stat.auckland.ac.nz  Wed Jun  8 01:36:41 2005
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Wed, 08 Jun 2005 11:36:41 +1200
Subject: [R] Help with possible bug (assigning NA value to data.frame)
 ?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E94A@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E94A@usctmx1106.merck.com>
Message-ID: <42A62F89.8030501@stat.auckland.ac.nz>

This seems to have more to do with NULLs than NAs. For instance:
> a <- data.frame(matrix(1:8, nrow=2))
> a
  X1 X2 X3 X4
1  1  3  5  7
2  2  4  6  8
> a[a$X2 == 4,]$X1 <- NULL
> a
  X1 X2 X3 X4
1  1  3  5  7
2  4  6  8  4

James

On 8/06/2005 7:15 a.m., Liaw, Andy wrote:
> There's something peculiar that I do not understand here.  However, did you
> realize that the thing you are assigning into parts of `a' is NULL?  Check
> you're my.test.boot.ci.1:  It's NULL.
> 
> Be that as it may, I get:
> 
> 
>>a <- data.frame(matrix(1:4, nrow=2), X3=NA, X4=NA)
>>a
> 
>   X1 X2 X3 X4
> 1  1  3 NA NA
> 2  2  4 NA NA
> 
>>a[a$X1 == 1,]$X3 <- NULL
>>a
> 
>   X1 X2 X3 X4
> 1  1  3 NA  1
> 2  2  4 NA NA
> 
>>a[a$X1 == 1,]$X4 <- NULL
>>a
> 
>   X1 X2 X3 X4
> 1  1  3 NA  1
> 2  2  4 NA NA
> 
> which really baffles me...
> 
> In any case, that's not how I would assign into part of a data frame.  I
> would do either
> 
>     a[a$X1 == 1, "X3"] <- something
> 
> or
> 
>     a$X3[a$X1 == 1] <- something
> 
> In either case you'd get an error if `something' is NULL.
> 
> Andy
> 
> 
>>From: Dan Bolser
>>
>>
>>This 'strange behaviour' manifest itself within some quite complex
>>code. When I created a *very* simple example the behaviour 
>>dissapeared. 
>>
>>Here is the simplest version I have found which still causes 
>>the strange
>>behaviour (it could be quite unrelated to the boot library, however).
>>
>>
>>library(boot)
>> 
>>## boot statistic function
>>my.mean.s <- function(data,subset){
>>  mean(data[subset])
>>}
>>
>>## dummy data, deliberatly no variance
>>my.test.dat.1 <- rep(4,5)
>>my.test.dat.2 <- rep(8,5)
>>
>>## not much can happen here
>>my.test.boot.1 <- boot( my.test.dat.1, my.mean.s, R=10 )
>>my.test.boot.2 <- boot( my.test.dat.2, my.mean.s, R=10 )
>>
>>## returns a null object as ci is meaningless for this data
>>my.test.boot.ci.1 <- boot.ci(my.test.boot.1,type='normal')
>>my.test.boot.ci.2 <- boot.ci(my.test.boot.2,type='normal')
>>
>>
>>## now try to store this data (the problem begins)...
>>
>>## dummy existing data 
>>a <- data.frame(matrix(c(1,2,3,4),nrow=2))
>>
>>## make space for new data
>>a$X3 <- NA
>>a$X4 <- NA
>>
>>## try to store the upper and lower ci (not) calculated above
>>a[a$X1==1,]$X3 <-  my.test.boot.ci.1$normal[2]
>>a[a$X1==1,]$X4 <-  my.test.boot.ci.1$normal[3]
>>a[a$X1==2,]$X3 <-  my.test.boot.ci.1$normal[2]
>>a[a$X1==2,]$X4 <-  my.test.boot.ci.1$normal[3]
>>
>>a
>>
>>
>>What I see is 
>>
>>
>>>a
>>
>>  X1 X2 X3 X4
>>1  1  3 NA  1
>>2  2  4 NA  2
>>
>>
>>What I expected to see was
>>
>>
>>>a
>>
>>  X1 X2 X3 X4
>>1  1  3 NA  NA
>>2  2  4 NA  NA
>>
>>Some how the last assignment of the data from within the null object
>>assigns the value of the '==x' part of the logical vector subscript.
>>
>>If I make the following (trivial?) adjustment 
>>
>>a[a$X1==1,]$X4 <-  my.test.boot.ci.1$normal[3]
>>a[a$X1==1,]$X3 <-  my.test.boot.ci.a$normal[2]
>>a[a$X1==2,]$X4 <-  my.test.boot.ci.1$normal[3]
>>a[a$X1==2,]$X3 <-  my.test.boot.ci.1$normal[2]
>>
>>
>>The output changes to 
>>
>>
>>>a
>>
>>  X1 X2 X3 X4
>>1  1  3  1  1
>>2  2  4  2  2
>>
>>Which is even wronger.
>>
>>
>>
>>Not sure if this is usefull without the full context, but here is the
>>output from the real version of this program (where most of 
>>the above code
>>is within a loop). What is printed out for each cycle of the 
>>loop is the
>>value of the '==x' part of the subscript.
>>
>>
>>[1] 2
>>[1] 3
>>[1] 4
>>[1] 5
>>[1] "All values of t are equal to  1 \n Cannot calculate confidence
>>intervals"
>>[1] 6
>>[1] 7
>>[1] "All values of t are equal to  1 \n Cannot calculate confidence
>>intervals"
>>[1] 8
>>[1] 10
>>[1] 11
>>[1] "All values of t are equal to  1 \n Cannot calculate confidence
>>intervals"
>>
>>
>>Above you see that for some values I can't calculate a ci 
>>(but storing it
>>as above), then...
>>
>>
>>>dat.5.ho
>>
>>  CHAINS DOM_PER_CHAIN     lower     upper
>>1      2      1.416539 1.3626253  1.468387
>>2      3      1.200000 1.1146014  1.288724
>>3      4      1.363636 1.2675657  1.462571
>>4      5      1.000000        NA  5.000000
>>5      6      1.323529 1.0991974  1.546156
>>6      7      1.000000        NA  7.000000
>>7      8      1.100000 0.9037904  1.289210
>>8     10      1.142857 0.8775104  1.403918
>>9     11      1.000000        NA 11.000000
>>
>>
>>Do you spot the same problem? Namely for each value of the 
>>'CHAINS' column
>>that was unable to calculate a ci, the second assignment to 
>>the data table
>>from the 'null' object assigned the lookup value of CHAINS to 
>>that column
>>instead! The assignment (within the loop) looks like this...
>>
>>  dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  x.s.ci$normal[2]
>>  dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  x.s.ci$normal[3]
>>
>>(where chain is the 'loop variable').
>>
>>
>>As far as I can tell this is a bug. It dosn't happen when I try...
>> 
>>  dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  NA
>>  dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  NA 
>>
>>
>>And doing the following (swapping the order) changes the behaviour...
>>
>>  dat.5.ho[dat.5.ho$CHAINS==chain,]$upper <-  x.s.ci$normal[3]
>>  dat.5.ho[dat.5.ho$CHAINS==chain,]$lower <-  x.s.ci$normal[2]
>>  
>>
>>Giving...
>>
>>
>>>dat.5.ho
>>
>>  CHAINS DOM_PER_CHAIN      lower     upper
>>1      2      1.416539  1.3616070  1.472716
>>2      3      1.200000  1.1134237  1.287601
>>3      4      1.363636  1.2587204  1.466037
>>4      5      1.000000  5.0000000  5.000000
>>5      6      1.323529  1.1082482  1.547222
>>6      7      1.000000  7.0000000  7.000000
>>7      8      1.100000  0.9021282  1.287672
>>8     10      1.142857  0.8766731  1.403327
>>9     11      1.000000 11.0000000 11.000000
>>
>>
>>Which is again incorrect and unpredicted (as above). 
>>
>>
>>Please let me know what to do to report this problem better, 
>>or if I just
>>missed something silly.
>>
>>I am RH9, R-2.1.0 (compiled from source), latest boot from 
>>CRAN (if that
>>makes a difference).
>>
>>Cheers,
>>Dan.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand



From ross at biostat.ucsf.edu  Wed Jun  8 02:15:44 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 07 Jun 2005 17:15:44 -0700
Subject: [R] fun with S4 method recursion
Message-ID: <1118189744.2410.125.camel@iron.libaux.ucsf.edu>

Apparently using an existing function as the definition for a method is
a no-no (at least as I'm doing it), producing infinite recursion.
setClass("A",
         representation(model="ANY")
         )
fiddle <- function(self) {
  1
}

setMethod("fiddle",
          signature(self="A"),
          definition = fiddle  ### problem
          )

setClass("B",
         representation(err="ANY"),
         contains="A"
         )

setMethod("fiddle",
          signature(self="B"),
          definition = function(self){
            callNextMethod()
            2
          })

b <- new("B", model=4, err=5)
fiddle(b)

produces
Error: protect(): protection stack overflow

On the other hand, if the line marked ### problem becomes
	definition = function(self) 1
everything works OK.

I think the definition of the generic is ending up as the method for A.
This surprised me, because I thought this was the standard way to
approach setting up a method ("Any ordinary function can be converted
into a generic by simply setting a method for it; ... the initial,
unique method (the body of the function) becomes the default method" pp.
322-323; "a generic function is created automatically by specifying a
method for any existing function.  This is by far the more common
situation" p 348--all pages in the green book).  It also surprised me
because of the note that setMethod makes a local copy of the function
given as the definition argument (bottom of p. 323).

Does the behavior of R differ from that described in the green book when
the name of a function appears in the definition argument of setMethod?
If not, what have I misunderstood?

Aside: my first real attempt didn't have 
	fiddle <- function(self) {1}
using just 
	setMethod("fiddle", "A", function(self) 1)
That produces no existing definition for function 'fiddle'


Given all this, would this be a better model?
fiddle <- function(... # definition for A
# do NOT setMethod for class A signature
setMethod("fiddle", "B", function(self) #operations for B
So when I do the setMethod my original fiddle becomes the default
generic.

Thanks.
Ross



From ross at biostat.ucsf.edu  Wed Jun  8 02:22:17 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 07 Jun 2005 17:22:17 -0700
Subject: [R] Two ways to inherit in S4?
Message-ID: <1118190137.2408.130.camel@iron.libaux.ucsf.edu>

I'm puzzled that it seems possible to specify inheritance via the
"representation" and "contains" arguments to setClass.  The examples
that I've seen all seem to use contains only.

Is there some subtle distinction between these two approaches?
Originally I thought one had to repeat the same information in both
places, but that seems not to be the case.

Sorry about all these questions.
Ross



From ross at biostat.ucsf.edu  Wed Jun  8 02:44:18 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 07 Jun 2005 17:44:18 -0700
Subject: [R] How to change the value of a class slot [INFO]
In-Reply-To: <200506072050.j57Knxf9022512@compton.gene.com>
References: <200506072050.j57Knxf9022512@compton.gene.com>
Message-ID: <1118191458.2408.138.camel@iron.libaux.ucsf.edu>

On Tue, 2005-06-07 at 13:49 -0700, Berton Gunter wrote:
> > Second, in my experiments I couldn't get setReplacementMethod to work:
> > 
> > "bumpIndex<-" <- function(pm, value) {
> >   pm at i <- pm at i+as.integer(value)
> >   pm
> > }
> > 
> > # I get an error without the next function definition
> > bumpIndex <- function(pm) pm at i
> > 
> > setReplaceMethod("bumpIndex",
> >                  signature=signature(pm="CompletePathMaker",
> >                    value="numeric"), bumpIndex) 
> > 
...
> 
> > When I try to load this, I get
> > arguments in definition changed from (spec) to (object)
> > arguments in definition changed from (self) to (object)
> > arguments in definition changed from (self) to (object)
> > Creating a new generic function for 'bumpIndex<-' in '.GlobalEnv'
> > Error in conformMethod(signature, mnames, fnames, f) : 
> > 	In method for function "bumpIndex<-": formal arguments 
> > omitted in the
> > method definition cannot be in the signature (value = "numeric")
> > 
With some help from Bert, partly offlist, here's a working version:
setReplaceMethod("bumpIndex",
                 signature=signature(pm="CompletePathMaker",
                   value="numeric"), function(pm, value) {
                     pm at i <- pm at i+as.integer(value)
                     pm
                   })
At least 2 problems were caused by my original, final argument of
bumpIndex to setReplaceMethod:
1) This looked for the function bumpIndex, not bumpIndex<-.  That's why
I had to define the bumpIndex function.  With the above change, it is no
longer necessary to define bumpIndex.  I needed to point it to
bumpIndex<-.  I've been unable to find how to quote that properly.

2) The bumpIndex function doesn't have the right arguments.

By the way, the use of "value" as the name for the final argument to the
assignment function is mandatory.

The info about value, as well as an extensive discussion of issues with
mutating objects, appear in this 2003 tutorial by Gentleman:
http://www.stat.auckland.ac.nz/S-Workshop/Gentleman/S4Objects.pdf
(thanks to Bert for the pointer).

Ross



From couxy at yahoo.com  Wed Jun  8 02:44:29 2005
From: couxy at yahoo.com (Mark Wong)
Date: Tue, 7 Jun 2005 17:44:29 -0700 (PDT)
Subject: [R] A question about lars
Message-ID: <20050608004430.151.qmail@web40704.mail.yahoo.com>

Hi,

I am trying to use lars on my data. After getting the
model and did the prediction, I compare the predicted
value and the coefficients and it seems like there
should be a constant term in the model. I have gone
through the documentation of the lars package and I
can't seem to find how to get this piece of
information from the lars object. Can somebody help
me? Thanks a lot!

Mark


		
__________________________________ 

Have fun online with music videos, cool games, IM and more. Check it out!



From mwdavis at nist.gov  Wed Jun  8 02:52:05 2005
From: mwdavis at nist.gov (mwdavis@nist.gov)
Date: Tue,  7 Jun 2005 20:52:05 -0400
Subject: [R] Bounding or constraining parameters in non-linear regressions
Message-ID: <1118191925.42a64135199cc@webmail.nist.gov>

Dear R-Users,

Being an engineer and not a statistician, my desired course of action may 
either be impossible or very simple.

I am attempting to fit a non-linear model to some measured data.  One term in 
the model contains a square-root, but in the course of regression, this term 
turns negative and an error occurs.  I started using Micrsoft's Excel Solver, 
and then I turned to NIST's Datplot statistical package.  I can constrain in 
Solver, but it violates those constraints. :)  Dataplot does not have the 
capability to constrain parameters.

Does R have the capability to constrain or bound parameters in non-linear 
regressions?


Thanks,
Mark Davis



From andy_liaw at merck.com  Wed Jun  8 03:27:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 21:27:52 -0400
Subject: [R] A question about lars
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E951@usctmx1106.merck.com>

It's the mean of the response, which is in object$mu, where `object' is the
lars model object.

Andy

> From: Mark Wong
> 
> Hi,
> 
> I am trying to use lars on my data. After getting the
> model and did the prediction, I compare the predicted
> value and the coefficients and it seems like there
> should be a constant term in the model. I have gone
> through the documentation of the lars package and I
> can't seem to find how to get this piece of
> information from the lars object. Can somebody help
> me? Thanks a lot!
> 
> Mark
> 
> 
> 		
> __________________________________ 
> 
> Have fun online with music videos, cool games, IM and more. 
> Check it out!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Wed Jun  8 03:28:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 7 Jun 2005 21:28:09 -0400
Subject: [R] Variables values on intersected intervals
In-Reply-To: <A91EF0B9121F834EA6484582DFE1CF4436F872@messagerie.chm.com>
References: <A91EF0B9121F834EA6484582DFE1CF4436F872@messagerie.chm.com>
Message-ID: <971536df05060718285d461a73@mail.gmail.com>

On 6/7/05, SAULEAU Erik-Andr?? <SAULEAUEA at ch-mulhouse.fr> wrote:
> Dear R-list,
> 
> i have a problem, in the framework of simulations, i want to vectorize for
> earning time: a variable, say X, has values on intervals and an other
> variable, say Y, has values on other intervals. For example
> 
> Inf     Sup     X
> 0     2     1
> 2     4    2
> 4     6    3
> 
> and
> 
> Inf     Sup     Y
> 1     3     1
> 3     5    2
> 5     7    3
> 
> i want to create a matrix like this
> 
> Inf     Sup     X     Y
> 0     1       1     NA
> 1     2      1     1
> 2     3      2     1
> 3     4      2     2
> 4     5      3     2
> 5     6      3     3
> 6     7      NA     3
> 
> I get it with loops but it seems to me that it can take a lot of time for a
> large number of intervals. I cannot get it with some matrix or vector
> manipulation (quicker than loops). Any solution???
> 
> thank you in advance, with my best regards, erik.
> 

Suppose dx and dy are the input data frames.  The 
intervals of dz, the output data frame, are made up of the
union of boundaries, b, of dx and dy.

Then we use cut to look up each dz interval
in each of dx and dy.

b <- sort(union(unlist(dx[,-3]), unlist(dy[,-3])))
dz <- data.frame(Inf. = b[-length(b)], Sup = b[-1])

kut <- function(g, df) 
   cut(g, c(df$Inf., max(df$Sup)), right = FALSE, lab = FALSE)

dz$X <- dx[kut(dz$Inf., dx), "X"]
dz$Y <- dy[kut(dz$Inf., dy), "Y"]

dz



From chencheva at gmail.com  Wed Jun  8 03:35:15 2005
From: chencheva at gmail.com (Hu Chen)
Date: Wed, 8 Jun 2005 09:35:15 +0800
Subject: [R] Re: how to define functions in such a situation
In-Reply-To: <42A5A01B.5080707@statistik.uni-dortmund.de>
References: <6f3fc9ee05060705505f831605@mail.gmail.com>
	<6f3fc9ee05060705582f96a86a@mail.gmail.com>
	<42A5A01B.5080707@statistik.uni-dortmund.de>
Message-ID: <6f3fc9ee05060718351b6a9cb4@mail.gmail.com>

Sorry for my confusing expression.
I need create some new temp functions after the package is loaded. And
these new functions should be visible and usable.

Functions could be returned as a object, as Liaw mentioned. However I
can't find some examples in R-intro. I also want to know whether
functions returned by their "father function" could be called outside.

Thank u all.
 

On 6/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Hu Chen wrote:
> 
> > I got a dirty way to solve this.
> > write a temp .R source file including these new functions, then
> >
> >>source(this_temp_file)
> 
> Don't know if you really have to do it that way, but I also really don't
> understand what you are going to do...
> 
> 
> > but I don't know if there are some temp directory for R to store temp files?
> 
> See ?tempfile and ?tempdir
> 
> Uwe Ligges
> 
> > On 6/7/05, Hu Chen <chencheva at gmail.com> wrote:
> >
> >>hi R folks,
> >>I need read a file from hardisk or www web. Then I need to define some
> >>new functions according to the contents of the read file.
> >>For  example, i need write a package name "mypackage" like this:
> >>
> >>>library(mypackage)
> >>>read(some_file_on_web) #to see its content, suppose it contains:
> >>
> >>eat.drink.sleep
> >>then 3 new functions need to be created and usable.
> >>the problem is, how could I create functions after executing
> >>">library(mypackage)" and make these new functions visible and usable
> >>immediately?
> >>
> >>Any information are appreciated. forgive me if this question is very
> >>stupid. but I really need help.
> >>Thank you all.
> >>
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Jun  8 03:49:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 21:49:58 -0400
Subject: [R] Re: how to define functions in such a situation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>

I've already given you an example (ecdf).  You can look at the polynom
package as well.  Try:

install.packages("polynom")
library(polynom)
as.function.polynomial

Here's one trivial example:

> f <- function(type) { g <- if (type == 1) cos else sin; g; }
> myfun1 <- f(1)
> myfun2 <- f(2)
> myfun1(pi)
[1] -1
> myfun2(pi)
[1] 1.224606e-16

As I said, reading the R Language Definition manual or S Programming helps a
lot.

Andy

> From: Hu Chen
> 
> Sorry for my confusing expression.
> I need create some new temp functions after the package is loaded. And
> these new functions should be visible and usable.
> 
> Functions could be returned as a object, as Liaw mentioned. However I
> can't find some examples in R-intro. I also want to know whether
> functions returned by their "father function" could be called outside.
> 
> Thank u all.
>  
> 
> On 6/7/05, Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> > Hu Chen wrote:
> > 
> > > I got a dirty way to solve this.
> > > write a temp .R source file including these new functions, then
> > >
> > >>source(this_temp_file)
> > 
> > Don't know if you really have to do it that way, but I also 
> really don't
> > understand what you are going to do...
> > 
> > 
> > > but I don't know if there are some temp directory for R 
> to store temp files?
> > 
> > See ?tempfile and ?tempdir
> > 
> > Uwe Ligges
> > 
> > > On 6/7/05, Hu Chen <chencheva at gmail.com> wrote:
> > >
> > >>hi R folks,
> > >>I need read a file from hardisk or www web. Then I need 
> to define some
> > >>new functions according to the contents of the read file.
> > >>For  example, i need write a package name "mypackage" like this:
> > >>
> > >>>library(mypackage)
> > >>>read(some_file_on_web) #to see its content, suppose it contains:
> > >>
> > >>eat.drink.sleep
> > >>then 3 new functions need to be created and usable.
> > >>the problem is, how could I create functions after executing
> > >>">library(mypackage)" and make these new functions 
> visible and usable
> > >>immediately?
> > >>
> > >>Any information are appreciated. forgive me if this 
> question is very
> > >>stupid. but I really need help.
> > >>Thank you all.
> > >>
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From mike.rstat at gmail.com  Wed Jun  8 04:37:53 2005
From: mike.rstat at gmail.com (Mike R)
Date: Tue, 7 Jun 2005 19:37:53 -0700
Subject: [R] how to run a script at the beginning of an interacive session ?
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
Message-ID: <27db823f050607193776b2cf4a@mail.gmail.com>

Hi All,

How does one create an executable R-script, similar
to an executable python script, or shell script except 
that when the R-script is finished, the R session remains
open and becomes interactive.

If I do this:

  R < script.r

then R exits when finished.

On the other hand, starting R then typing 

   source("script.r",echo=T)

is becoming tedious.

Thanks in advance.

Mike



From tura at centroin.com.br  Wed Jun  8 04:53:18 2005
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Tue, 07 Jun 2005 23:53:18 -0300
Subject: [R] Tree Structured Survival Analysis
Message-ID: <6.1.2.0.2.20050607235020.03805eb0@centroin.com.br>

Hi People!

Have someone  package with: Tree Structured Survival Analysis  in R?


Thanks in advance

Bernardo Rangel Tura, MD, MSc
National Institute of Cardiology Laranjeiras
Rio de Janeiro Brazil 


-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From andy_liaw at merck.com  Wed Jun  8 05:02:37 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 7 Jun 2005 23:02:37 -0400
Subject: [R] how to run a script at the beginning of an interacive
	ses sion ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E956@usctmx1106.merck.com>

See ?Startup.

Andy

> From: Mike R
> 
> Hi All,
> 
> How does one create an executable R-script, similar
> to an executable python script, or shell script except 
> that when the R-script is finished, the R session remains
> open and becomes interactive.
> 
> If I do this:
> 
>   R < script.r
> 
> then R exits when finished.
> 
> On the other hand, starting R then typing 
> 
>    source("script.r",echo=T)
> 
> is becoming tedious.
> 
> Thanks in advance.
> 
> Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Wed Jun  8 05:05:32 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 7 Jun 2005 23:05:32 -0400
Subject: [R] how to run a script at the beginning of an interacive session
	?
In-Reply-To: <27db823f050607193776b2cf4a@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
Message-ID: <971536df05060720054229f866@mail.gmail.com>

On 6/7/05, Mike R <mike.rstat at gmail.com> wrote:
> Hi All,
> 
> How does one create an executable R-script, similar
> to an executable python script, or shell script except
> that when the R-script is finished, the R session remains
> open and becomes interactive.
> 
> If I do this:
> 
>  R < script.r
> 
> then R exits when finished.
> 
> On the other hand, starting R then typing
> 
>   source("script.r",echo=T)
> 
> is becoming tedious.

If you mean that you want to run certain R code every time you 
start up R then in R:

?Startup



From MSchwartz at mn.rr.com  Wed Jun  8 05:05:31 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 07 Jun 2005 22:05:31 -0500
Subject: [R] how to run a script at the beginning of an interacive
	session ?
In-Reply-To: <27db823f050607193776b2cf4a@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
Message-ID: <1118199932.13958.4.camel@localhost.localdomain>

On Tue, 2005-06-07 at 19:37 -0700, Mike R wrote:
> Hi All,
> 
> How does one create an executable R-script, similar
> to an executable python script, or shell script except 
> that when the R-script is finished, the R session remains
> open and becomes interactive.
> 
> If I do this:
> 
>   R < script.r
> 
> then R exits when finished.
> 
> On the other hand, starting R then typing 
> 
>    source("script.r",echo=T)
> 
> is becoming tedious.
> 
> Thanks in advance.
> 
> Mike

See ?Startup, specifically the use of the .First function in .Rprofile.
Note importantly that .First is run before any packages are loaded,
including the base packages.

As an example, if you wanted to create a plot() on startup each time,
create a file called "script.r" containing:

library(graphics)
plot(1:10)


Then in .Rprofile, create an entry as follows:

.First <- function() {source("script.r")}

With this in place, the plot will get created each time you start R.

HTH,

Marc Schwartz



From jsorkin at grecc.umaryland.edu  Wed Jun  8 05:06:33 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 07 Jun 2005 23:06:33 -0400
Subject: [R] All subsets logistic or Poisson regression
Message-ID: <s2a62885.077@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050607/64ac1565/attachment.pl

From mail at bymouth.com  Wed Jun  8 05:48:35 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Wed, 8 Jun 2005 13:48:35 +1000
Subject: [R] logistic regression (glm binary)
Message-ID: <007301c56bdc$f5484ae0$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/45169989/attachment.pl

From andy_liaw at merck.com  Wed Jun  8 06:05:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 8 Jun 2005 00:05:18 -0400
Subject: [R] logistic regression (glm binary)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E95B@usctmx1106.merck.com>




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Stephen Choularton
> Sent: Tuesday, June 07, 2005 11:49 PM
> To: R Help
> Subject: [R] logistic regression (glm binary)
> 
> 
> Hi
>  
> I am looking for a couple of pointers using glm (family = binary).

Do you mean "binomial" instead of "binary"?
  
> 1.  I want to add all the products of my predictive features as
> additional features (and I have 23 of them). Is there some easy way to
> add them?

Probably something along the line:

> dat <- data.frame(y=sample(0:1, 100, replace=TRUE), matrix(runif(300),
ncol=3))
> fm <- glm(y ~ .^2, family="binomial", data=dat)
> summary(fm)

Call:
glm(formula = y ~ .^2, family = "binomial", data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.654  -1.175   0.608   1.116   1.651  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)    3.264      1.536   2.125   0.0336 *
X1            -3.379      2.026  -1.668   0.0953 .
X2            -4.659      2.244  -2.077   0.0378 *
X3            -3.531      2.060  -1.714   0.0865 .
X1:X2          4.535      2.775   1.634   0.1022  
X1:X3          2.123      2.639   0.804   0.4212  
X2:X3          4.315      2.746   1.571   0.1161  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 137.99  on 99  degrees of freedom
Residual deviance: 131.84  on 93  degrees of freedom
AIC: 145.84

Number of Fisher Scoring iterations: 3

  
> 2. I want to drop each feature in turn and get the most significant,
> then drop two and get the next most significant, etc.  Is there some
> function that allows me to do this?

Not that I know of, and most likely for a very, very good reason...

Andy

> Thanks
>  
> Stephen
> 
> -- 
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From mike.rstat at gmail.com  Wed Jun  8 06:33:36 2005
From: mike.rstat at gmail.com (Mike R)
Date: Tue, 7 Jun 2005 21:33:36 -0700
Subject: [R] how to run a script at the beginning of an interacive session
	?
In-Reply-To: <27db823f05060720394f26a39c@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
	<1118199932.13958.4.camel@localhost.localdomain>
	<27db823f05060720394f26a39c@mail.gmail.com>
Message-ID: <27db823f0506072133201d554@mail.gmail.com>

On 6/7/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
On 6/7/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
On 6/7/05, Liaw, Andy <andy_liaw at merck.com> wrote:
<snips>

Thanks Andy, Gabor and Marc.


---- contents of .Rprofile ----
.First <- function() {source("startup.r")}


---- contents of wrapR.v1 ----
#!/bin/bash
ln -s $1 startup.r
R
rm -f startup.r


---- contents of wrapR.v2 ----
#!/bin/bash
echo ".First <- function() {source(\"$1\")}" > .Rprofile
R

---- contents of project_A.r ----
library(graphics)
plot(1:10)


and then start R with either of the following command lines:
## ./wrapR.v1 project_A.r
## ./wrapR.v2 project_A.r


What do you think? is this the most simple way?

Thanks in advance,

Mike



From ligges at statistik.uni-dortmund.de  Wed Jun  8 09:13:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 08 Jun 2005 09:13:23 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <1118170689.2410.94.camel@iron.libaux.ucsf.edu>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>	<17057.57384.831362.960765@stat.math.ethz.ch>	<20050606203517.GN3134@wheat.boylan.org>	<42A54069.6090108@statistik.uni-dortmund.de>
	<1118170689.2410.94.camel@iron.libaux.ucsf.edu>
Message-ID: <42A69A93.4000404@statistik.uni-dortmund.de>

Ross Boylan wrote:

> On Tue, 2005-06-07 at 08:36 +0200, Uwe Ligges wrote:
> 
>>Ross Boylan wrote:
>>
>>
>>>On Sat, Jun 04, 2005 at 07:08:56PM +0200, Martin Maechler wrote:
>>>
>>>
>>>>   Ross> nextPath <- function(pm){ #pm is a CompletePathMaker
>>>>   Ross>    pm at i <- pm at i+as.integer(1)
>>>>   Ross> [etc]
>>>>
>>>>If your nextPath   function has  'pm' as its last statement it
>>>>will return the updated object, and if you call it
>>>>as
>>>>	mypm <- nextPath(mypm)
>>>>
>>>>you are
>>>>   1) updating  mypm
>>>>   2) in a proper S way (i.e. no cheating).
>>>>
>>>>Regards,
>>>>Martin
>>>
>>>
>>>Wow.  This is almost the exact inverse of the usual object behavior,
>>>in which only the class itself can update the slots (aka instance
>>>variables).  None of the methods of the class can update instances of
>>>the class persistently without the help of outsiders, and only
>>>outsiders can change the slot values.
>>>
>>>(Yes, I realize that using the idiom you suggest of returning a new
>>>object one can have only class methods actually fiddling with the
>>>slots.)
>>>
>>>The inability of a class method to change a class object without
>>>outside help seems unfortunate.
>>>
>>>It looks as if instances of class objects are best thought of as
>>>immutable once created.
>>
>>Obviously, there are many definition of "object oriented" programming, 
>>and yours seems to be different from the S4 definition.
> 
> Yes. And though there are many definitions of "object oriented" (at
> least, many implementations),  I'd say the minimum requirement to be
> object oriented is to have objects that encapsulate both state (instance
> variables/slots) and behavior (methods).
> 
> S4 objects do not fully encapsulate state because they require outside
> assistance to alter the state of the object (with the exception of
> assignment operators).  The smalltalker in me also gets nervous that
> code outside the class can access the slots, but there are many object
> systems that act that way.
> 
> The way in which names of methods of unrelated classes interfere with
> each other seems a break-down of the encapsulation of behavior, though
> the problem strictly is not with the behavior but just with the name.
> To return to the concrete problem that got me started, if class
> Specification defines a method likelihood taking as arguments instances
> of class Specification, Path and Parameters, then it is awkward to
> define a method likelihood for the class Model when that method has
> arguments of class Model, Specification, data.frame, and vector,
> particularly if different names for the formal arguments are desired.
> (I think technically it could be done, but only in a very ugly
> way--i.e., better to use different method names for the two classes).
> 
> 
>>I was going to answer your first question at first, but you have not 
>>given enough details - in particular it was not clear to me why your 
>>approach did not work. 
>>I assumed that you are assigning the new object 
>>again, which is the S way. 
> 
> I wasn't, which is why it didn't work.  I wanted the function to return
> some other value than the object it was operating on.
> 
>>You have to think about scoping rules and it 
>>will be clear that the approach you are expecting is not a clean one in S.
> 
> Could you say a bit more about that?  

I meant the following simple example (not related to any object oriented 
programming from the S point of view, but maybe well from your point of 
view?):

Say you want a function foo() that simply incements the argument:

a <- 1
foo(a)
a # now is 2

But what happens if there is (more than) one "a" (in different 
environments), but no "a" in the environment foo(a) is called from. 
Which "a" do you want to change in this case? Seems to be rather dangerous.

Uwe Ligges


> I had thought of the issue more in
> terms of function calls in S being call by value, preventing updates to
> the original arguments.  So the issue isn't so much the scope of the
> names of function arguments (that scope being limited to the function
> body), but the properties of the thing they refer to (conceptually, a
> copy of the argument, not the original).
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From navarre_sabine at yahoo.fr  Wed Jun  8 10:37:15 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Wed, 8 Jun 2005 10:37:15 +0200 (CEST)
Subject: [R] matrix
Message-ID: <20050608083716.84231.qmail@web26602.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/216a1e89/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Wed Jun  8 10:55:11 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 8 Jun 2005 10:55:11 +0200
Subject: [R] matrix
In-Reply-To: <20050608083716.84231.qmail@web26602.mail.ukl.yahoo.com>
References: <20050608083716.84231.qmail@web26602.mail.ukl.yahoo.com>
Message-ID: <20050608105511.33a138bf.Achim.Zeileis@wu-wien.ac.at>

On Wed, 8 Jun 2005 10:37:15 +0200 (CEST) Navarre Sabine wrote:

> hi,
>  
> is it possible to create a matrix with one row and according to add a
> row? in fact, at present, I'm doing an algorithm which fill a matrix.

I think you are looking for
  ?rbind
and might also be interested in looking at "An Introduction to R" or
other introductory manuals.

> On the web site of CRAN, the package basic is impossible to be load!
> can you tell me where i can found it! 

You already asked this question yesterday (and received no reply because
it doesn't make sense). There is no package with the name "basic". There
is only the "base" package which is part of every distribution of R
(which I hope you have installed) and loaded at startup of R.
Z



From ligges at statistik.uni-dortmund.de  Wed Jun  8 11:04:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 08 Jun 2005 11:04:15 +0200
Subject: [R] matrix
In-Reply-To: <20050608105511.33a138bf.Achim.Zeileis@wu-wien.ac.at>
References: <20050608083716.84231.qmail@web26602.mail.ukl.yahoo.com>
	<20050608105511.33a138bf.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <42A6B48F.80406@statistik.uni-dortmund.de>

Achim Zeileis wrote:

> On Wed, 8 Jun 2005 10:37:15 +0200 (CEST) Navarre Sabine wrote:
> 
> 
>>hi,
>> 
>>is it possible to create a matrix with one row and according to add a
>>row? in fact, at present, I'm doing an algorithm which fill a matrix.
> 
> 
> I think you are looking for
>   ?rbind
> and might also be interested in looking at "An Introduction to R" or
> other introductory manuals.
>

Achim is completely correct, I'd like to add:

If you do know the size of the matrix before your algorithm starts 
iterating, you should not use rbind() but generate a fullsize matrix at 
first and replace row by row afterwards.

Uwe Ligges


> 
>>On the web site of CRAN, the package basic is impossible to be load!
>>can you tell me where i can found it! 
> 
> 
> You already asked this question yesterday (and received no reply because
> it doesn't make sense). There is no package with the name "basic". There
> is only the "base" package which is part of every distribution of R
> (which I hope you have installed) and loaded at startup of R.
> Z
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Wed Jun  8 10:56:00 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 08 Jun 2005 10:56:00 +0200
Subject: [R] matrix
In-Reply-To: <20050608083716.84231.qmail@web26602.mail.ukl.yahoo.com>
References: <20050608083716.84231.qmail@web26602.mail.ukl.yahoo.com>
Message-ID: <42A6B2A0.3080302@free.fr>

Le 08.06.2005 10:37, Navarre Sabine a ??crit :

>hi,
> 
>is it possible to create a matrix with one row and according to add a row?
>in fact, at present, I'm doing an algorithm which fill a matrix.
> 
>  
>
?rbind
When you know the final size of your matrix, it is better to create a 
full matrix and then fill it.

>On the web site of CRAN, the package basic is impossible to be load!
>can you tell me where i can found it! 
> 
>  
>
I don't see any package called 'basic' on CRAN. What is that package ?

>Thanks
> 
>Sabine
> 
>  
>

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jun  8 11:10:19 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 8 Jun 2005 11:10:19 +0200
Subject: [R] CRAN task view for genetics
Message-ID: <7FFEE688B57D7346BC6241C55900E730B701FC@pollux.bfro.uni-lj.si>

Hello to everyone!

I have built CRAN task view for genetics. For now I have not submit it 
to CRAN yet and it can be accessible from:

http://www.bfro.uni-lj.si/MR/ggorjan/software/R/Genetics.html
http://www.bfro.uni-lj.si/MR/ggorjan/software/R/Genetics.ctv

I have not submitted it to CRAN, since I would like first some opinion 
about it. Genetics is really so broad field that I belive one person can
not know about every corner. So I would like to get some suggestions,
crtitics, pacthes to presentation from my knowledge limited point view.

Please note that field of microarray/proteomics, ... area is 
substantially supported in R and Bioconductor and if someone is prepaired
to write a CRAN task view for this I can remove that piece text from my 
view and provide a link.

I have sent this mail also to all mentioned/listed package maintainers.

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From maechler at stat.math.ethz.ch  Wed Jun  8 11:32:34 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 8 Jun 2005 11:32:34 +0200
Subject: [R] Specifying medoids in PAM?
In-Reply-To: <be6d17205060712111a263c56@mail.gmail.com>
References: <be6d17205060712111a263c56@mail.gmail.com>
Message-ID: <17062.47922.43476.902879@stat.math.ethz.ch>

>>>>> "David" == David Finlayson <david.p.finlayson at gmail.com>
>>>>>     on Tue, 7 Jun 2005 12:11:25 -0700 writes:

    David> I am using the PAM algorithm in the CLUSTER library. 
    David> When I allow PAM to seed the medoids using the default __build__
    David> algorithm things work
    David> well:

    >> pam(stats.table, metric="euclidean", stand=TRUE, k=5)

    David> But I have some clusters from a Hierarchical analysis that I would
    David> like to use as seeds for the PAM algorithm. I can't figure what the
    David> mediod argument wants. When I put in the five integer indicies for the
    David> observations in stats.table that I would like to use as seeds (the row
    David> numbers), I segfault R.

    >> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)

    David> *** R Crashes ***

this is not very helpful.

Can you please   READ the posting guide and then
do as the guide says :

 -  post a *reproducible* example
 -  tell more about what happens when  "R Crashes"

    David> Here is my version info:
    >> version
    David> _              
    David> platform i386-pc-mingw32
    David> arch     i386           
    David> os       mingw32        
    David> system   i386, mingw32  
    David> status                  
    David> major    2              
    David> minor    0.1            
    David> year     2004           
    David> month    11             
    David> day      15             
    David> language R      

    David> Any guidance would be appreciated.

    David> David

    David> -- 
    David> David Finlayson
    David> Marine Geology & Geophysics
    David> School of Oceanography
    David> Box 357940
    David> University of Washington
    David> Seattle, WA  98195-7940
    David> USA

    David> Office: Marine Sciences Building, Room 112
    David> Phone: (206) 616-9407
    David> Web: http://students.washington.edu/dfinlays

    David> ______________________________________________
    David> R-help at stat.math.ethz.ch mailing list
    David> https://stat.ethz.ch/mailman/listinfo/r-help
    David> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Wed Jun  8 11:20:21 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 8 Jun 2005 11:20:21 +0200
Subject: [ESS] Re: [R] Strange characters in 2.1.0?
In-Reply-To: <20050607233144.GL6253@hortresearch.co.nz>
References: <40e66e0b05060605512e39815d@mail.gmail.com>
	<Pine.LNX.4.21.0506061444260.31061-100000@mail.mrc-dunn.cam.ac.uk>
	<17061.43722.36464.509201@stat.math.ethz.ch>
	<20050607233144.GL6253@hortresearch.co.nz>
Message-ID: <17062.47189.530606.76588@stat.math.ethz.ch>

>>>>> "PaCo" == Patrick Connolly <p.connolly at hortresearch.co.nz>
>>>>>     on Wed, 8 Jun 2005 11:31:44 +1200 writes:

    PaCo> On Tue, 07-Jun-2005 at 04:10PM +0200, Martin Maechler wrote:
    PaCo> |> >>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>

    PaCo> |>       ..........
    PaCo> |> 
    PaCo> |>     Dan> I have gone back to 2.0.0 :)
    PaCo> |> 
    PaCo> |> Don't do that!
    PaCo> |> You've lost tons of nice new features and gained quite an amount
    PaCo> |> of old bugs by downgrading .. 

    PaCo> I get the non-generic quotes to show on the screen, but they won't
    PaCo> print with enscript.  I end up with a lot of wrapped lines and
    PaCo> nonsense where an unknown character should be.

Why is this diverted from R- to ESS-help? 
Printing with enscript is also a topic for printing a
transcript 'output.Rout' resulting e.g. from R CMD BATCH input.R output.Rout
I'm  committing a cross-posting  felony now, by posting back
to R-help {and please drop ESS-help from "cc" when further replying}....

    PaCo> What do I need to do to get enscript to know about such characters?
    PaCo> There is an encoding parameter which defaults to latin1.  Should I
    PaCo> change that to something?

Yes, in principle.  "latin1" aka ISO-latin-1 aka iso-8859-1
is (for western European languages) the predecessor standard of
the new unicode standard where we use the UTF-8 encoding
{and the above is (too) much simplified; also enter  "locale"
settings and standards}

However, my version of enscript does not seem to support UTF-8 (yet).
Nor does 'a2ps' an alternative to enscript which does pretty
print R source files.

So there are basically two options :

1) Get rid of unicode / utf-8
   by setting the locale of your computer / login 
   to use the "old" locales, e.g. en_US instead of en_US.utf-8.
   This will be more or less fine for Emacs and R --- though in
   in our {Redhat Enterprise} setup, the X11-fonts for
   non-utf-locales are quite crippled compared to those for
   utf-8 ones.

   However, as more and more other utilities are based on utf-8
   encoded files, you will see funny characters there
   if you are using locales like "de_*" or "fr_*", at least,
   e.g. for man pages which are only in utf-8 for our Redhat OS setup.

2) Improve the printing tools by 
    a) filtering *.utf-8 to latin-* 
    b) printing the resulting latin-*

   For filtering, there are programs like  'recode' (was "GNU
   recode", now "Free recode") which are extremely flexible and
   'iconv' (less flexible but wider spread) that can translate
   utf-8 to and from all kind of encodings / character sets.

In the future, of course everything will work out of the box
when all the utilities in your computer will be aware of utf
encodings and will automatically send correct stuff to the
printer and display it correctly in all kind of viewers/editors... :-)   

Given my experiences during the last several months
(where I, e.g., also found that our oldish LaTeX setup 
  didn't yet accept  \usepackage[utf8]{inputencoding ),
If I were in New Zeeland and would not need accents or umlauts,
I'd probably stick with latin1  (and would make sure my X
server got proper non-utf8 fonts) for another year or so.

Martin



From schween at snafu.de  Wed Jun  8 12:07:13 2005
From: schween at snafu.de (Sven C. Koehler)
Date: Wed, 8 Jun 2005 12:07:13 +0200
Subject: [R] Overlaying Barplots
Message-ID: <20050608100713.GE31361@boing.buug.de>

Hello!

I would like to overlay barplot(1:10) with a barplot(seq(1:5, each=2)),
indicating that 50% of each bar belongs to category X.  How do I do this
in R?

Best wishes,

Sven C. Koehler



From wildscop at yahoo.com  Wed Jun  8 12:40:22 2005
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Wed, 8 Jun 2005 03:40:22 -0700 (PDT)
Subject: [R] Fitting Theoretical Distributions to Daily Rainfall Data
Message-ID: <20050608104022.11410.qmail@web52605.mail.yahoo.com>



Dear List Members, 

I need a bit help about fitting some theoretical
distributions (such as geometric, exponential,
lognormal or weibull distribution) to the following
*dry spell*, *wet spell*, *cycles (Wet-Dry or
Dry-Wet)* from my meteorological (daily rainfall) data
http://www.angelfire.com/ab5/get5/R.rainfall.txt only
for rainy seasen (july - september) of 14 years only:

dryspell<-c(1,1,4,1,1,4,2,2,3,4,1,2,1,1,1,3,1,3,
2,3,1,2,3,3,2,2,6,2,1,1,3,1,2,1,4,4,1,1,2,1,1,2,
2,4,1,1,1,3,4,1,7,1,3,2,5,1,3,1,1,3,4,8,4,3,1,1,
1,2,3,1,1,2,1,1,2,2,2,3,3,13,13,7,1,1,1,1,7,3,2,
1,3,1,1,2,5,1,2,1,3,1,1,2,3,2,3,2,1,5,1,2,2,2,1,
9,2,2,1,1,4,5,1,1,3,1,3,3,2,1,1,1,17,1,4,5,1,1,
1,1,2,1,2,1,1,7,3,8,2,1,1,2,4,5,1,1,1,2,3,1,1,2,
1,1,3,2,3,1,1,1,3,6,4,1,2,1,2,2,4,2,4,2,1,2,1,3,
1,2,2,1,1,1,1,4,2,1,12,3,1,1,5,1,1,5,1,2,1,1,1,1,
5,3,1,1,3,1,1,6,10,1,1,1,2,1,3,2,2,5,1,1,2,2,1,2,
3,1,3,6,2,1,1,4,6,1,1,1,3,2,2,1,1,1,5,1,1,1,1,3,
1,2,1,7,1,3,1,3,4,1,1,3,4,3,1,4,4,1,3,1,5,3,1,2,
2,1,1,1,2,1,1,6,1,1,1,3,1,3,4,1,1,3,4,1,1,8,1)

wetspell<-c(1,5,6,4,1,5,3,4,5,2,3,1,5,4,1,4,1,2,3
,1,5,4,5,2,1,1,1,6,2,19,5,4,6,5,2,7,1,3,1,1,2,1,
3,8,2,3,1,2,5,1,3,8,9,1,1,7,1,2,3,7,9,4,4,1,2,3,
1,1,1,1,1,2,6,7,1,4,1,6,1,5,5,3,2,3,1,1,1,1,6,1,
3,2,1,3,5,6,3,2,6,1,1,3,1,7,3,5,1,2,2,3,1,12,1,8,
3,1,2,1,1,2,1,2,4,2,3,1,1,3,1,4,1,6,5,2,11,6,2,1,
1,9,2,7,1,7,4,1,6,4,8,2,1,1,1,9,3,3,7,2,1,3,3,8,2,
1,7,1,2,2,1,1,1,1,1,5,1,1,3,1,1,1,9,1,7,1,4,3,1,5,
7,1,5,1,5,6,8,5,3,4,1,2,7,9,3,1,4,2,1,1,2,3,1,1,8,
5,2,1,1,1,4,1,1,1,8,4,9,6,3,1,6,5,3,5,2,2,1,5,9,8,
1,6,4,1,2,8,6,1,3,1,2,2,2,3,1,1,5,2,3,11,1,1,1,5,
3,5,1,2,1,9,3,1,1,1,4,10,6,1,1,1,1,1,3,4,1,2,1,5,
2,1,3,2,9,2,1,1,4,2,1,2,9,3,1,1,1,2,6,6,3)

cycleWetDry<-c(2,9,7,5,5,7,5,7,9,3,5,2,6,5,4,5,4,4,
6,2,7,7,12,5,3,3,7,8,3,20,8,5,8,6,6,11,2,4,3,2,3,3,
5,9,6,4,2,3,8,5,4,15,10,4,3,12,2,5,4,8,12,8,9,5,5,
4,2,2,3,4,2,3,8,8,2,6,3,8,4,8,18,16,9,4,2,2,2,8,9,
3,4,5,2,4,7,11,4,4,7,4,2,4,3,10,5,8,3,3,16,4,3,14,
3,9,12,3,4,2,2,6,6,3,5,5,4,4,4,5,2,5,2,17,6,6,16,
7,3,2,2,11,3,9,2,8,11,4,14,6,16,3,3,5,6,10,4,4,9,
5,2,4,5,9,3,4,9,4,3,3,2,4,7,5,2,10,2,3,5,5,3,5,11,
2,9,2,7,4,3,7,8,2,6,2,9,8,9,9,14,6,5,2,7,8,10,8,2,
6,3,2,2,3,8,4,2,9,8,3,2,7,3,5,2,2,3,9,7,11,8,8,2,
7,7,5,6,4,5,2,8,12,10,2,7,8,7,3,9,7,4,5,3,3,3,3,8,
2,2,6,3,6,18,2,3,2,12,4,8,2,5,5,10,4,4,5,4,5,14,
10,2,4,2,6,4,4,6,3,4,2,6,3,3,4,3,15,3,2,2,7,3,4,
6,10,4,4,5,2,3,14,7,4)

cycleDryWet<-c(2,6,10,5,2,9,5,6,8,6,4,3,6,5,2,7,2,
5,5,4,6,6,8,4,4,3,3,12,4,20,6,7,7,7,3,11,5,4,2,3,3,
2,5,10,3,7,2,3,6,4,7,9,16,2,4,9,6,3,6,8,10,7,8,9,6,
6,2,2,2,3,4,3,7,9,2,5,3,8,3,8,8,16,15,11,2,2,2,2,13,
4,5,3,4,4,6,8,8,3,8,2,4,4,2,9,6,7,4,4,3,8,2,14,3,10,
4,10,4,3,2,3,5,7,5,3,6,2,4,6,3,5,2,7,22,3,15,11,3,2,
2,10,4,8,3,8,5,8,9,12,10,3,2,3,5,14,4,4,8,4,4,4,4,
10,3,2,10,3,5,3,2,2,4,7,5,6,3,2,5,3,5,3,13,3,8,3,
5,6,2,7,9,2,6,2,6,10,10,6,3,15,7,2,3,12,10,4,6,5,
4,2,2,3,4,6,4,9,6,5,2,2,7,14,2,2,2,10,5,12,8,5,6,
7,6,5,7,3,4,4,6,12,14,3,7,5,5,8,9,7,2,6,3,4,3,3,4,
6,2,6,3,4,14,15,2,3,6,10,6,4,3,4,13,4,2,4,5,7,11,
10,5,2,4,2,6,6,5,3,4,3,6,3,2,5,3,10,8,2,2,5,5,2,
5,13,4,2,4,5,3,7,14,4)


Using table() to each dryspell, wetspell, cycleWetDry,
cycleDryWet we find the empirical distribution
functions all of which seem to be positively skewed
with long tail. Therefore, i'd like to fit geometric,
exponential, lognormal or weibull distribution for
each dryspell, wetspell, cycleWetDry, cycleDryWet.
Better fit may be defined by higher p-values of
goodness-of-fit tests.

Is there any way i can do fit data to those
theoretical distributions in R? Is there any existing
program/function/package to solve such problem? 

Any suggestion, direction, references, help, replies
will be highly appreciated. 

Thank you for your time. 


----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD 

----------------------------------



		
__________________________________ 

Get on-the-go sports scores, stock quotes, news and more. Check it out!



From knoblauch at lyon.inserm.fr  Wed Jun  8 12:59:32 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Wed,  8 Jun 2005 12:59:32 +0200
Subject: [R] Fitting Theoretical Distributions to Daily Rainfall Data
Message-ID: <1118228372.42a6cf9448096@webmail.lyon.inserm.fr>

Have a look at the fit.dist function in Jim Lindsey's gnlm package at
http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html

fit.dist {gnlm}	R Documentation
Fit Probability Distributions to Frequency Data

Description

fit.dist fits the distributions in Chapter 4 of Lindsey (1995, 2003 2nd edn): 
binomial, beta-binomial, Poisson, negative binomial, geometric, zeta, normal, 
log normal, inverse Gauss, logistic, Laplace, Cauchy, Student t, exponential, 
Pareto, gamma, and Weibull to frequency (histogram) data, possibly plotting 
the frequency polygon of fitted values with the histogram.

fitdistr from the MASS package works quite well, too.


>Dear List Members, 
>
>I need a bit help about fitting some theoretical
>distributions (such as geometric, exponential,
>lognormal or weibull distribution)

____________________
Ken Knoblauch
Inserm U371, Cerveau et Vision
Department of Cognitive Neurosciences
18 avenue du Doyen Lepine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10
http://www.lyon.inserm.fr/371/



From Arne.Muller at sanofi-aventis.com  Wed Jun  8 13:43:34 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Wed, 8 Jun 2005 13:43:34 +0200
Subject: [R] bug in predict.lme?
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3A0@CRBSMXSUSR04>

Dear All,

I've come across a problem in predict.lme. Assigning a model formula to a  variable and then using this variable in lme (instead of typing the formula into the formula part of lme) works as expect. However, when performing a predict on the fitted model I gan an error messag - predict.lme (but not predictlm) seems to expect a 'properly' typed in formula and a cannot extract the formula from the variable. THe code below demonstrates this.

Is this a known or expected behavour of predict.lme or is this a bug?

	kind regards,

	Arne

(R-2.1.0)
> library(nlme)
...
> mod <- distance ~ age + Sex # example from ?lme
> mod
distance ~ age + Sex
> fm2 <- lme(mod, data = Orthodont, random = ~ 1)
> anova(fm2)
            numDF denDF  F-value p-value
(Intercept)     1    80 4123.156  <.0001
age             1    80  114.838  <.0001
Sex             1    25    9.292  0.0054
> fm2
Linear mixed-effects model fit by REML
  Data: Orthodont 
  Log-restricted-likelihood: -218.7563
  Fixed: mod 
         ^^^^
...

> predict(fm2,  Orthodont)
Error in mCall[["fixed"]][-2] : object is not subsettable

> fm2 <- update(fm2, . ~ .) # this replaces "mod" by the contents of variable mod
> fm2
Linear mixed-effects model fit by REML
  Data: Orthodont 
  Log-restricted-likelihood: -218.7563
  Fixed: distance ~ age + Sex 
  ...

> predict(fm2,  Orthodont)
     M01      M01      M01      M01  ... 
    25.39237 26.71274 28.03311 29.35348 21.61052 ...
 

> fm2 <- lm(mod, data = Orthodont)
> predict(fm2,  Orthodont)
       1        2        3        4 ...
22.98819 24.30856 25.62894 26.94931 ...



From thpe at hhbio.wasser.tu-dresden.de  Wed Jun  8 14:15:56 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 08 Jun 2005 14:15:56 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
Message-ID: <42A6E17C.5050909@hhbio.wasser.tu-dresden.de>

Ross Boylan wrote:
> I defined an S4 class with a slot i.  Then I wrote a regular function
> that attempted to increment i.

[... details deleted ...]

> What do I need to do to update slot values?
>
> Here are some possibly relevant code fragments
> setClass("CompletePathMaker",
>          representation(i="integer",
>                         timeOffset="numeric", # to avoid 0's
>                         truePaths="TruePaths")
>          )
> nextPath <- function(pm){ #pm is a CompletePathMaker
>   pm at i <- pm at i+as.integer(1)
> [etc]
>
> I'm trying to make the class behave like an iterator, with i keeping
> track of its location.  I'm sure there are more R'ish ways to go, but
> I'm also pretty sure I'm going to want to be able to update slots.

Hello Ross,

I see that your question was related to S4, but I just noticed a
solution based on the R.oo package so I thought I would add a solution
based on the proto package too. We had similar problems several times
ago and (to my surprise) found R to be an extremely flexible language
even for these things. Our favorite solution is available as
package(proto). It requires R 2.1, because of several subtle
improvements regarding environments, which made our implementation more
streamlined.

Does the following example do what you intended?

##=====================================================
library(proto)

## 1) define an object
CompletePathMaker <- proto(
      index = 0,
      bumpIndex = function(., dindex = 1)
        .$index <- .$index + as.integer(dindex)
)

## 2) create a child object of CompletePathMaker
cpm <- CompletePathMaker$proto()

## 3) set the index component to 3
cpm$index <- 3

## 4) iterate the index
cpm$bumpIndex(2)

## print the result
cpm$index

##=====================================================

This approach is very compact and needs only one new function: proto.
Also note how simple it is conceptually. We did not even create any
classes. We just created a parent object CompletePathMaker and a child
to it, cpm, and got everything else via delegation (i.e. inheritance).


Hope it helps

Thomas P.



From kyong.ho.park at us.army.mil  Wed Jun  8 14:25:16 2005
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr. RDECOM)
Date: Wed, 8 Jun 2005 08:25:16 -0400 
Subject: [R] Robustness of Segmented Regression Contributed by Muggeo
Message-ID: <E0466E6811B5BA46B92414C94516A7A425BC3A@apgrb1rdg-mail2.nae.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/69dc1fc1/attachment.pl

From plummer at iarc.fr  Wed Jun  8 14:28:45 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 08 Jun 2005 14:28:45 +0200
Subject: [R] Overlaying Barplots
In-Reply-To: <20050608100713.GE31361@boing.buug.de>
References: <20050608100713.GE31361@boing.buug.de>
Message-ID: <1118233725.6267.12.camel@seurat>

On Wed, 2005-06-08 at 12:07 +0200, Sven C. Koehler wrote:
> Hello!
> 
> I would like to overlay barplot(1:10) with a barplot(seq(1:5, each=2)),
> indicating that 50% of each bar belongs to category X.  How do I do this
> in R?

If you pass a matrix to barplot, it will stack values from the same
column on top of each other. So something like

barplot(rbind(1:10, 1:10))

will do what you want.



From rkoenker at uiuc.edu  Wed Jun  8 14:36:18 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Wed, 8 Jun 2005 07:36:18 -0500
Subject: [R] Robustness of Segmented Regression Contributed by Muggeo 
Message-ID: <3E5F0C47-21D9-46A0-BE74-E326DE17FB9B@uiuc.edu>

You might try rqss() in the quantreg package.  It gives piecewise  
linear fits
for a nonparametric form of median regression using total variation  
of the
derivative of the fitted function as a penalty term.  A tuning parameter
(lambda) controls the number of distinct segments.  More details are
available in the vignette for the quantreg package.


url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820


On Jun 8, 2005, at 7:25 AM, Park, Kyong H Mr. RDECOM wrote:


> Hello, R users,
> I applied segmented regression method contributed by Muggeo and got
> different slope estimates depending on the initial break points.  
> The results
> are listed below and I'd like to know what is a reasonable approach  
> handling
> this kinds of problem. I think applying various initial break  
> points is
> certainly not a efficient approach. Is there any other methods to  
> deal with
> segmented regression? From a graph, v shapes are more clear at 1.2  
> and 1.5
> break points than 1.5 and 1.7. Appreciate your help.
>
> Result1:
> Initial break points are 1.2 and 1.5. The estimated break points  
> and slopes:
>
>  Estimated Break-Point(s):
>                  Est.      St.Err
> Mean.Vel 1.285     0.05258
>                1.652    0.01247
>
>                Est.          St.Err.             t value        CI 
> (95%).l
> CI(95%).u
> slope1   0.4248705     0.3027957   1.403159    -0.1685982         
> 1.018339
> slope2   2.3281445     0.3079903   7.559149     1.7244946         
> 2.931794
> slope3   9.5425516     0.7554035   12.632390     8.0619879        
> 11.023115
> Adjusted R-squared: 0.9924.
>
> Result2:
> Initial break points are 1.5 and 1.7. The estimated break points  
> and slopes:
>
> Estimated Break-Point(s):
>                 Est.       St.Err
> Mean.Vel 1.412      0.02195
>                1.699      0.01001
>
>                Est.          St.Err.        t value            CI 
> (95%).l
> CI(95%).u
> slope1  0.7300483   0.1381587    5.284129       0.4592623       
> 1.000834
> slope2  3.4479466   0.2442530    14.116289     2.9692194        
> 3.926674
> slope3 12.5000000   1.7783840     7.028853     9.0144314       
> 15.985569
>
> Adjusted R-squared: 0.995.
>
>
>
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>
>



From Achim.Zeileis at wu-wien.ac.at  Wed Jun  8 14:55:43 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 8 Jun 2005 14:55:43 +0200
Subject: [R] Robustness of Segmented Regression Contributed by Muggeo
In-Reply-To: <E0466E6811B5BA46B92414C94516A7A425BC3A@apgrb1rdg-mail2.nae.ds.army.mil>
References: <E0466E6811B5BA46B92414C94516A7A425BC3A@apgrb1rdg-mail2.nae.ds.army.mil>
Message-ID: <20050608145543.79a69943.Achim.Zeileis@wu-wien.ac.at>

On Wed, 8 Jun 2005 08:25:16 -0400  Park, Kyong H Mr. RDECOM wrote:

> Hello, R users,
> I applied segmented regression method contributed by Muggeo and got
> different slope estimates depending on the initial break points. The
> results are listed below and I'd like to know what is a reasonable
> approach handling this kinds of problem. I think applying various
> initial break points is certainly not a efficient approach. Is there
> any other methods to deal with segmented regression? From a graph, v
> shapes are more clear at 1.2 and 1.5 break points than 1.5 and 1.7.
> Appreciate your help.

When you keep the number of break points fixed, then there is a unique
solution to the problem of fitting a segmented regression: the solution
which maximizes the likelihood (or for linear models equivalently
minimizes the RSS). Vito's segmented package gives an iterative method
which can be shown to converge to this unique solution. If empirically
you find different solutions with different starting values, you can
always compare them using the RSS or log-likelihood and choose the one
which fits better (because the other one can't be the optimal solution).

The function breakpoints() in package strucchange computes (as
opposed to approximates) the unique solution for a fully segmented model
instead of a broken line trend.

Another nonparametric solution using quantreg was already pointed out by
Roger.

hth,
Z
 
> Result1: 
> Initial break points are 1.2 and 1.5. The estimated break points and
> slopes:
> 
>  Estimated Break-Point(s):
>                  Est.      St.Err
> Mean.Vel 1.285     0.05258
>                1.652    0.01247  
>               
>                Est.          St.Err.             t value       
>                CI(95%).l
> CI(95%).u
> slope1   0.4248705     0.3027957   1.403159    -0.1685982       
> 1.018339 slope2   2.3281445     0.3079903   7.559149     1.7244946    
>    2.931794
> slope3   9.5425516     0.7554035   12.632390     8.0619879      
> 11.023115 Adjusted R-squared: 0.9924.
> 
> Result2:
> Initial break points are 1.5 and 1.7. The estimated break points and
> slopes:
> 
> Estimated Break-Point(s):
>                 Est.       St.Err
> Mean.Vel 1.412      0.02195
>                1.699      0.01001
>            
>                Est.          St.Err.        t value           
>                CI(95%).l
> CI(95%).u
> slope1  0.7300483   0.1381587    5.284129       0.4592623     
> 1.000834 slope2  3.4479466   0.2442530    14.116289     2.9692194     
>  3.926674
> slope3 12.5000000   1.7783840     7.028853     9.0144314     
> 15.985569
> 
> Adjusted R-squared: 0.995. 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jun  8 15:27:00 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 8 Jun 2005 15:27:00 +0200
Subject: [R] Random seed problem in MCMC coupling of chains
Message-ID: <7FFEE688B57D7346BC6241C55900E730B701FE@pollux.bfro.uni-lj.si>

Hello!

I am performing coupling of chains in MCMC and I need the same value
of seed for two chains. I will show demo of what I want:

R code, which might show my example is:
niter <- 3
nchain <- 2
tmpSeed <- 123
for (i in 1:niter) { # iterations
  for (j in 1:nchain) { # chains
    set.seed(tmpSeed)
    a <- runif(1)
    cat("iter:", i, "chain:", j, "runif:", a, "\n")
    tmpSeed <- .Random.seed
  }
}

I get this:

iter: 1 chain: 1 runif: 0.43588
iter: 1 chain: 2 runif: 0.43588
iter: 2 chain: 1 runif: 0.43588
iter: 2 chain: 2 runif: 0.43588
iter: 3 chain: 1 runif: 0.43588
iter: 3 chain: 2 runif: 0.43588

but I would like to get:

iter: 1 chain: 1 runif: 0.43588
iter: 1 chain: 2 runif: 0.43588
iter: 2 chain: 1 runif: 0.67676
iter: 2 chain: 2 runif: 0.67676
iter: 3 chain: 1 runif: 0.12368
iter: 3 chain: 2 runif: 0.12368

Note that seed value is of course changing, but it is parallel
between chains.

I am able to do only this, since setting seed at the beginning 
of chain i.e iteration is not a problem, but I want an upper 
scheme, since I compare chains and stop one if some condition is
satisfied.

tmpSeed <- 123
for (i in 1:nchain) { # chains
  set.seed(tmpSeed)
  for (j in 1:niter) { # iterations
    a <- runif(1)
    cat("iter:", j, "chain:", i, "runif:", a, "\n")
  }
}
iter: 1 chain: 1 runif: 0.28758
iter: 2 chain: 1 runif: 0.7883
iter: 3 chain: 1 runif: 0.40898
iter: 1 chain: 2 runif: 0.28758
iter: 2 chain: 2 runif: 0.7883
iter: 3 chain: 2 runif: 0.40898
iter: 1 chain: 3 runif: 0.28758
iter: 2 chain: 3 runif: 0.7883
iter: 3 chain: 3 runif: 0.40898

I was looking in 'rlecuyer', 'rsprng' and 'setRNG', but did not find 
anything usable for me.  From reading on http://sprng.cs.fsu.edu/ 
'rsprng' provides just opposite of what I want, 'rlecuyer' is a bit
to technical for me, but I think it also doesn't give identical
seed for parallels. 'setRNG', especially it's function 'getRNG()'
looks nice but its arguments should have seed stored. How can one
do that?


Thanks in advance!

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From eld at Codan.dk  Wed Jun  8 15:27:16 2005
From: eld at Codan.dk (Martin Englund)
Date: Wed, 8 Jun 2005 15:27:16 +0200
Subject: [R] Converting code from MATLAB to R
Message-ID: <OF258A4C7D.81820B13-ONC125701A.0049216C-C125701A.0049EB0B@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/93e6245c/attachment.pl

From Pierre.Lapointe at nbf.ca  Wed Jun  8 15:42:39 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Wed, 8 Jun 2005 09:42:39 -0400
Subject: [R] RODBC question: problem importing series with blank cells
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF3584C@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/50859728/attachment.pl

From dimitris.rizopoulos at med.kuleuven.be  Wed Jun  8 15:53:37 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 8 Jun 2005 15:53:37 +0200
Subject: [R] Random seed problem in MCMC coupling of chains
References: <7FFEE688B57D7346BC6241C55900E730B701FE@pollux.bfro.uni-lj.si>
Message-ID: <00e701c56c31$77ec2850$0540210a@www.domain>

do you want something like this:

niter <- 3
nchain <- 2
rs <- sample(500, niter, TRUE)
for (i in 1:niter) { # iterations
  for (j in 1:nchain) { # chains
    set.seed(rs[i])
    a <- runif(1)
    cat("iter:", i, "chain:", j, "runif:", a, "\n")
  }
}


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Gorjanc Gregor" <Gregor.Gorjanc at bfro.uni-lj.si>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 08, 2005 3:27 PM
Subject: [R] Random seed problem in MCMC coupling of chains


> Hello!
>
> I am performing coupling of chains in MCMC and I need the same value
> of seed for two chains. I will show demo of what I want:
>
> R code, which might show my example is:
> niter <- 3
> nchain <- 2
> tmpSeed <- 123
> for (i in 1:niter) { # iterations
>  for (j in 1:nchain) { # chains
>    set.seed(tmpSeed)
>    a <- runif(1)
>    cat("iter:", i, "chain:", j, "runif:", a, "\n")
>    tmpSeed <- .Random.seed
>  }
> }
>
> I get this:
>
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.43588
> iter: 2 chain: 2 runif: 0.43588
> iter: 3 chain: 1 runif: 0.43588
> iter: 3 chain: 2 runif: 0.43588
>
> but I would like to get:
>
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.67676
> iter: 2 chain: 2 runif: 0.67676
> iter: 3 chain: 1 runif: 0.12368
> iter: 3 chain: 2 runif: 0.12368
>
> Note that seed value is of course changing, but it is parallel
> between chains.
>
> I am able to do only this, since setting seed at the beginning
> of chain i.e iteration is not a problem, but I want an upper
> scheme, since I compare chains and stop one if some condition is
> satisfied.
>
> tmpSeed <- 123
> for (i in 1:nchain) { # chains
>  set.seed(tmpSeed)
>  for (j in 1:niter) { # iterations
>    a <- runif(1)
>    cat("iter:", j, "chain:", i, "runif:", a, "\n")
>  }
> }
> iter: 1 chain: 1 runif: 0.28758
> iter: 2 chain: 1 runif: 0.7883
> iter: 3 chain: 1 runif: 0.40898
> iter: 1 chain: 2 runif: 0.28758
> iter: 2 chain: 2 runif: 0.7883
> iter: 3 chain: 2 runif: 0.40898
> iter: 1 chain: 3 runif: 0.28758
> iter: 2 chain: 3 runif: 0.7883
> iter: 3 chain: 3 runif: 0.40898
>
> I was looking in 'rlecuyer', 'rsprng' and 'setRNG', but did not find
> anything usable for me.  From reading on http://sprng.cs.fsu.edu/
> 'rsprng' provides just opposite of what I want, 'rlecuyer' is a bit
> to technical for me, but I think it also doesn't give identical
> seed for parallels. 'setRNG', especially it's function 'getRNG()'
> looks nice but its arguments should have seed stored. How can one
> do that?
>
>
> Thanks in advance!
>
> Lep pozdrav / With regards,
>    Gregor Gorjanc
>
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: 
> http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know 
> it,
> you have no certainty until you try." Sophocles ~ 450 B.C.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From murdoch at stats.uwo.ca  Wed Jun  8 15:53:10 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 08 Jun 2005 09:53:10 -0400
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B701FE@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B701FE@pollux.bfro.uni-lj.si>
Message-ID: <42A6F846.2000003@stats.uwo.ca>

On 6/8/2005 9:27 AM, Gorjanc Gregor wrote:
> Hello!
> 
> I am performing coupling of chains in MCMC and I need the same value
> of seed for two chains. I will show demo of what I want:
> 
> R code, which might show my example is:
> niter <- 3
> nchain <- 2
> tmpSeed <- 123
> for (i in 1:niter) { # iterations
>   for (j in 1:nchain) { # chains
>     set.seed(tmpSeed)
>     a <- runif(1)
>     cat("iter:", i, "chain:", j, "runif:", a, "\n")
>     tmpSeed <- .Random.seed
>   }
> }
> 
> I get this:
> 
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.43588
> iter: 2 chain: 2 runif: 0.43588
> iter: 3 chain: 1 runif: 0.43588
> iter: 3 chain: 2 runif: 0.43588
> 
> but I would like to get:
> 
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.67676
> iter: 2 chain: 2 runif: 0.67676
> iter: 3 chain: 1 runif: 0.12368
> iter: 3 chain: 2 runif: 0.12368
> 
> Note that seed value is of course changing, but it is parallel
> between chains.

set.seed takes an integer, but .Random.seed is a complicated vector. 
You need to play with .Random.seed directly, and move your setting of 
tmpSeed out of the inner loop, i.e.

 > niter <- 3
 > nchain <- 2
 > set.seed(123)
 > tmpSeed <- .Random.seed
 > for (i in 1:niter) { # iterations
+   for (j in 1:nchain) { # chains
+     .Random.seed <- tmpSeed
+     a <- runif(1)
+     cat("iter:", i, "chain:", j, "runif:", a, "\n")
+   }
+   tmpSeed <- .Random.seed
+ }
iter: 1 chain: 1 runif: 0.2875775
iter: 1 chain: 2 runif: 0.2875775
iter: 2 chain: 1 runif: 0.7883051
iter: 2 chain: 2 runif: 0.7883051
iter: 3 chain: 1 runif: 0.4089769
iter: 3 chain: 2 runif: 0.4089769

However, heed the warnings in ?set.seed:  with some generators 
.Random.seed *does not* contain the full state of the RNG.  As far as I 
know there is no way to obtain the full state.

Duncan Murdoch



From eld at Codan.dk  Wed Jun  8 15:55:29 2005
From: eld at Codan.dk (Martin Englund)
Date: Wed, 8 Jun 2005 15:55:29 +0200
Subject: [R] Converting code from MATLAB to R
Message-ID: <OF258A4C7D.81820B13-ONC125701A.0049216C-C125701A.004C8016@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/25593b22/attachment.pl

From MSchwartz at mn.rr.com  Wed Jun  8 15:56:34 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 08 Jun 2005 08:56:34 -0500
Subject: [R] how to run a script at the beginning of an interacive
	session	?
In-Reply-To: <27db823f0506072133201d554@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
	<1118199932.13958.4.camel@localhost.localdomain>
	<27db823f05060720394f26a39c@mail.gmail.com>
	<27db823f0506072133201d554@mail.gmail.com>
Message-ID: <1118238995.28986.0.camel@localhost.localdomain>

On Tue, 2005-06-07 at 21:33 -0700, Mike R wrote:
> On 6/7/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On 6/7/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/7/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> <snips>
> 
> Thanks Andy, Gabor and Marc.
> 
> 
> ---- contents of .Rprofile ----
> .First <- function() {source("startup.r")}
> 
> 
> ---- contents of wrapR.v1 ----
> #!/bin/bash
> ln -s $1 startup.r
> R
> rm -f startup.r
> 
> 
> ---- contents of wrapR.v2 ----
> #!/bin/bash
> echo ".First <- function() {source(\"$1\")}" > .Rprofile
> R
> 
> ---- contents of project_A.r ----
> library(graphics)
> plot(1:10)
> 
> 
> and then start R with either of the following command lines:
> ## ./wrapR.v1 project_A.r
> ## ./wrapR.v2 project_A.r
> 
> 
> What do you think? is this the most simple way?
> 
> Thanks in advance,
> 
> Mike

Mike,

There are several options here, and I would try to stay with a simple
approach, minimizing having to modify files, such as you are doing in
wrapR.v2, which BTW will overwrite all other contents of ~/.Rprofile.

You could get very complicated if you wanted and use something like
'sed' to engage in non-interactive editing of the ~./Rprofile file, but
I think that would increase the complexity beyond what is needed here.

One possible approach, subject to comment by others, would be to have
your normal default ~/.Rprofile file containing your base specifications
for a default startup.

Then, create one or more ~/.RprofileX files, where X is a number of 1:n,
for each separate file.

In each of these files, fully construct a .First function (as opposed to
sourcing() a separate file), such as:

# In file ~/.Rprofile1
...
.First <- function()
{
  library(graphics)
  plot(1:10)
}


# In file ~/.Rprofile2
...
.First <- function()
{
  library(graphics)
  barplot(1:10)
}


In each of the above, replace the '...' with the core contents of
~/.Rprofile so that besides .First, all else is the same if you want it
that way.


With the above in place, now run R from a terminal or within a shell
script using:

R_PROFILE=~/.Rprofile1 R --no-init-file


OR

R_PROFILE=~/.Rprofile2 R --no-init-file


In this way, by setting the R_PROFILE environment variable, you can
define which init file to use on startup. The use of "--no-init-file"
ignores your default ~/.Rprofile file to avoid any possible conflicts
there.

Note, importantly, this approach would obviate the use of any "site-
wide" .Rprofile file, which by default would be
$R_HOME/etc/Rprofile.site.

Now all you have to do is to create a new ~/.RprofileX each time you
want to utilize a different .First by modifying the command line call to
R.

If you want to run R with a default startup (ie. no .First function),
just use R as per normal. 

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Wed Jun  8 16:06:00 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Jun 2005 15:06:00 +0100 (BST)
Subject: [R] RODBC question: problem importing series with blank cells
In-Reply-To: <834204C0D7C6D611A3BB000255FC6E9D0DF3584C@lbmsg002.fbn-nbf.local>
References: <834204C0D7C6D611A3BB000255FC6E9D0DF3584C@lbmsg002.fbn-nbf.local>
Message-ID: <Pine.LNX.4.61.0506081502090.30728@gannet.stats>

On Wed, 8 Jun 2005, Lapointe, Pierre wrote:

> Hello,
>
> I have an excel file that I load through RODBC. Some of my columns are
> blank.  They are equity  time series and the stocks did not exist at the
> earlier dates.  My problem is that the whole column becomes <NA> even though
> there are numbers at later dates.

My Excel ODBC driver is sending NULLs for those rows, so I think this is 
an ODBC driver problem.

>
> Here's my excel file
>
> http://www.tradebit.com/download.php/35699
> <http://www.tradebit.com/download.php/35699>
>
> And here's the code I use:
>
> library(RODBC)
> chan <- odbcConnectExcel("C:/book54.xls") #load data
> ts<- sqlFetch(chan, "Sheet1")
> close(chan)
> ts<-ts[-1,]
>
> str(ts)
> head(ts)
> tail(ts)
>
>
>
> Regards,
>
> Pierre Lapointe
> Assistant Market Strategist
> National Bank Financial
>
>
> ***********************************************************************************
> AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.hankin at noc.soton.ac.uk  Wed Jun  8 16:10:31 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 8 Jun 2005 15:10:31 +0100
Subject: [R] Converting code from MATLAB to R
In-Reply-To: <OF258A4C7D.81820B13-ONC125701A.0049216C-C125701A.0049EB0B@codan.dk>
References: <OF258A4C7D.81820B13-ONC125701A.0049216C-C125701A.0049EB0B@codan.dk>
Message-ID: <c6f5733ff0829ae76dac201911c0fae2@soc.soton.ac.uk>

Hi Martin


optim()

should do the trick

[and this isn't in R-and-octave2.txt; I'll update when I get a minute]


HTH

rksh


On Jun 8, 2005, at 02:27 pm, Martin Englund wrote:

>
> Hi,
>
> I'm having trouble converting code from MATLAB to R; I want to find the
> equivalence to MATLAB's function 'fsolve'. I've tried 'nlm', on the
> squared argument, in R but i did not get the same results.
>
> Thankful if helped.
>
> Best regards,
> Martin Englund
>
>
> ----------------------------------------------------------------------- 
> -------
> This e-mail and any attachment may be confidential and may also be  
> privileged.
> If you are not the intended recipient, please notify us immediately  
> and then
> delete this e-mail and any attachment without retaining copies or  
> disclosing
> the contents thereof to any other person.
> Thank you.
> ----------------------------------------------------------------------- 
> -------
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From dmbates at gmail.com  Wed Jun  8 16:39:20 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 8 Jun 2005 09:39:20 -0500
Subject: [R] Bounding or constraining parameters in non-linear regressions
In-Reply-To: <1118191925.42a64135199cc@webmail.nist.gov>
References: <1118191925.42a64135199cc@webmail.nist.gov>
Message-ID: <40e66e0b05060807394ba37b35@mail.gmail.com>

On 6/7/05, mwdavis at nist.gov <mwdavis at nist.gov> wrote:
> Dear R-Users,
> 
> Being an engineer and not a statistician, my desired course of action may
> either be impossible or very simple.
> 
> I am attempting to fit a non-linear model to some measured data.  One term in
> the model contains a square-root, but in the course of regression, this term
> turns negative and an error occurs.  I started using Micrsoft's Excel Solver,
> and then I turned to NIST's Datplot statistical package.  I can constrain in
> Solver, but it violates those constraints. :)  Dataplot does not have the
> capability to constrain parameters.
> 
> Does R have the capability to constrain or bound parameters in non-linear
> regressions?

Sort of.  If you look at the stats package in r-devel you will see
that a function called nlminb has been added.  This function calls
optimization software from the Port package
(http://www.netlib.com/port/).  The Fortran code for constrained
nonlinear least squares problems is included in the package but the
interface code for R has not yet been written.  The energetic could
create such interface code by emulating that for nlminb - it's not
that long.

Alternatively you could use either optim or nlminb on the function
which is the residual sum of squares from your model.



From theato at env.ethz.ch  Wed Jun  8 16:46:43 2005
From: theato at env.ethz.ch (Claude =?iso-8859-1?Q?Th=E9ato?= Kobe)
Date: Wed, 8 Jun 2005 16:46:43 +0200
Subject: [R] variable png-size in multiple figure environment
Message-ID: <a06100503beccb3b1038a@[129.132.129.185]>

If I put only one plot in a png, I can drag the generated png on the 
edges and it changes its size by this. But when I put several plots 
on the same sheet using mfrow, the size can no longer be changed when 
viewing the file, and the resolution is bad. What do I need to do to 
keep the variability of size in a multiple figure environment?



From br44114 at gmail.com  Wed Jun  8 16:56:19 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Wed, 8 Jun 2005 10:56:19 -0400
Subject: [R] get level combinations from "by" list
Message-ID: <8d5a363505060807561bdc444a@mail.gmail.com>

Dear useRs,

Given this code I end up with a list of class "by":

a <- sample(1:5,200,replace=TRUE)
b <- sample(c("v1","v2","v3"),200,replace=TRUE)
c <- sample(c(11,22,33),200,replace=TRUE)
data <- runif(200)
grouped <- by(data,list(a,b,c),function(x) {c(min=min(x),max=max(x),
	median=round(median(x),digits=2),mean=round(mean(x),digits=2))})
dfr <- do.call("rbind",grouped)    #the levels are missing
#----------
grouped
typeof(grouped)
class(grouped)
dimnames(grouped)
 
How do I get at the levels of the 'group by' variables for each
subset? For example, from this part of the "by" list I want 4, v2 and
33:
: 4
: v2
: 33
      min       max    median      mean
0.3897450 0.9215315 0.7300000 0.6700000
---------------------------------------

Thank you,
b.



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jun  8 17:09:16 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 8 Jun 2005 17:09:16 +0200
Subject: [R] Random seed problem in MCMC coupling of chains
Message-ID: <7FFEE688B57D7346BC6241C55900E730B701FF@pollux.bfro.uni-lj.si>

Thanks to Duncan, Dimitris as well as James for answers. I'll provide
here also example from James, which seems to be the easiest of them 
all and was not posted to the list:

niter <- 3
nchain <- 2
for (i in 1:niter) { # iterations
  for (j in 1:nchain) { # chains
    set.seed(i)
    a <- runif(1)
    cat("iter:", i, "chain:", j, "runif:", a, "\n")
  }
}

Note that seed is set with iteration counter. This is really neat and
simple. I am just wondering if this is OK from "RNG point of view". Can
someone comment on that?

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.
----------------------------------------------------------------------

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
Sent: sre 2005-06-08 15:53
To: Gorjanc Gregor
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Random seed problem in MCMC coupling of chains
 
On 6/8/2005 9:27 AM, Gorjanc Gregor wrote:
> Hello!
> 
> I am performing coupling of chains in MCMC and I need the same value
> of seed for two chains. I will show demo of what I want:
> 
> R code, which might show my example is:
> niter <- 3
> nchain <- 2
> tmpSeed <- 123
> for (i in 1:niter) { # iterations
>   for (j in 1:nchain) { # chains
>     set.seed(tmpSeed)
>     a <- runif(1)
>     cat("iter:", i, "chain:", j, "runif:", a, "\n")
>     tmpSeed <- .Random.seed
>   }
> }
> 
> I get this:
> 
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.43588
> iter: 2 chain: 2 runif: 0.43588
> iter: 3 chain: 1 runif: 0.43588
> iter: 3 chain: 2 runif: 0.43588
> 
> but I would like to get:
> 
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.67676
> iter: 2 chain: 2 runif: 0.67676
> iter: 3 chain: 1 runif: 0.12368
> iter: 3 chain: 2 runif: 0.12368
> 
> Note that seed value is of course changing, but it is parallel
> between chains.

set.seed takes an integer, but .Random.seed is a complicated vector. 
You need to play with .Random.seed directly, and move your setting of 
tmpSeed out of the inner loop, i.e.

 > niter <- 3
 > nchain <- 2
 > set.seed(123)
 > tmpSeed <- .Random.seed
 > for (i in 1:niter) { # iterations
+   for (j in 1:nchain) { # chains
+     .Random.seed <- tmpSeed
+     a <- runif(1)
+     cat("iter:", i, "chain:", j, "runif:", a, "\n")
+   }
+   tmpSeed <- .Random.seed
+ }
iter: 1 chain: 1 runif: 0.2875775
iter: 1 chain: 2 runif: 0.2875775
iter: 2 chain: 1 runif: 0.7883051
iter: 2 chain: 2 runif: 0.7883051
iter: 3 chain: 1 runif: 0.4089769
iter: 3 chain: 2 runif: 0.4089769

However, heed the warnings in ?set.seed:  with some generators 
.Random.seed *does not* contain the full state of the RNG.  As far as I 
know there is no way to obtain the full state.

Duncan Murdoch



From pgilbert at bank-banque-canada.ca  Wed Jun  8 17:10:43 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 08 Jun 2005 11:10:43 -0400
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B701FE@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B701FE@pollux.bfro.uni-lj.si>
Message-ID: <42A70A73.9050206@bank-banque-canada.ca>

The tools in setRNG are intended for this kind of problem and I do use 
them regularly in much more complicated situations.  They help save all 
the information, in addition to the seed, that you need for reproducible 
simulations. Try

niter <- 3
  nchain <- 2
  for (i in 1:niter) { # iterations
    tmpSeed <- setRNG()
    for (j in 1:nchain) { # chains
      setRNG(tmpSeed)
      a <- runif(1)
      cat("iter:", i, "chain:", j, "runif:", a, "\n")
    }
  }


iter: 1 chain: 1 runif: 0.8160078
iter: 1 chain: 2 runif: 0.8160078
iter: 2 chain: 1 runif: 0.4909793
iter: 2 chain: 2 runif: 0.4909793
iter: 3 chain: 1 runif: 0.4425924
iter: 3 chain: 2 runif: 0.4425924

HTH,
Paul Gilbert

Gorjanc Gregor wrote:

> Hello!
> 
> I am performing coupling of chains in MCMC and I need the same value
> of seed for two chains. I will show demo of what I want:
> 
> R code, which might show my example is:
> niter <- 3
> nchain <- 2
> tmpSeed <- 123
> for (i in 1:niter) { # iterations
>   for (j in 1:nchain) { # chains
>     set.seed(tmpSeed)
>     a <- runif(1)
>     cat("iter:", i, "chain:", j, "runif:", a, "\n")
>     tmpSeed <- .Random.seed
>   }
> }
> 
> I get this:
> 
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.43588
> iter: 2 chain: 2 runif: 0.43588
> iter: 3 chain: 1 runif: 0.43588
> iter: 3 chain: 2 runif: 0.43588
> 
> but I would like to get:
> 
> iter: 1 chain: 1 runif: 0.43588
> iter: 1 chain: 2 runif: 0.43588
> iter: 2 chain: 1 runif: 0.67676
> iter: 2 chain: 2 runif: 0.67676
> iter: 3 chain: 1 runif: 0.12368
> iter: 3 chain: 2 runif: 0.12368
> 
> Note that seed value is of course changing, but it is parallel
> between chains.
> 
> I am able to do only this, since setting seed at the beginning 
> of chain i.e iteration is not a problem, but I want an upper 
> scheme, since I compare chains and stop one if some condition is
> satisfied.
> 
> tmpSeed <- 123
> for (i in 1:nchain) { # chains
>   set.seed(tmpSeed)
>   for (j in 1:niter) { # iterations
>     a <- runif(1)
>     cat("iter:", j, "chain:", i, "runif:", a, "\n")
>   }
> }
> iter: 1 chain: 1 runif: 0.28758
> iter: 2 chain: 1 runif: 0.7883
> iter: 3 chain: 1 runif: 0.40898
> iter: 1 chain: 2 runif: 0.28758
> iter: 2 chain: 2 runif: 0.7883
> iter: 3 chain: 2 runif: 0.40898
> iter: 1 chain: 3 runif: 0.28758
> iter: 2 chain: 3 runif: 0.7883
> iter: 3 chain: 3 runif: 0.40898
> 
> I was looking in 'rlecuyer', 'rsprng' and 'setRNG', but did not find 
> anything usable for me.  From reading on http://sprng.cs.fsu.edu/ 
> 'rsprng' provides just opposite of what I want, 'rlecuyer' is a bit
> to technical for me, but I think it also doesn't give identical
> seed for parallels. 'setRNG', especially it's function 'getRNG()'
> looks nice but its arguments should have seed stored. How can one
> do that?
> 
> 
> Thanks in advance!
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Wed Jun  8 17:15:02 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 08 Jun 2005 11:15:02 -0400
Subject: [R] get level combinations from "by" list
In-Reply-To: <8d5a363505060807561bdc444a@mail.gmail.com>
References: <8d5a363505060807561bdc444a@mail.gmail.com>
Message-ID: <42A70B76.1020607@optonline.net>

Here is a different approach to achieving what I think you want using 
summarize() in the Hmisc package:

library(Hmisc)

mydata <- data.frame(a = sample(1:5,200,replace=TRUE),
                      b = sample(c("v1","v2","v3"),200,replace=TRUE),
                      c = sample(c(11,22,33),200,replace=TRUE),
                      y = runif(200))

attach(mydata)

summarize(y, llist(a, b, c),
                function(x){c(min=min(x),
                              max=max(x),
                              median=round(median(x),digits=2),
                              mean=round(mean(x),digits=2))},
                              stat.name="min")

detach(mydata)

bogdan romocea wrote:
> Dear useRs,
> 
> Given this code I end up with a list of class "by":
> 
> a <- sample(1:5,200,replace=TRUE)
> b <- sample(c("v1","v2","v3"),200,replace=TRUE)
> c <- sample(c(11,22,33),200,replace=TRUE)
> data <- runif(200)
> grouped <- by(data,list(a,b,c),function(x) {c(min=min(x),max=max(x),
> 	median=round(median(x),digits=2),mean=round(mean(x),digits=2))})
> dfr <- do.call("rbind",grouped)    #the levels are missing
> #----------
> grouped
> typeof(grouped)
> class(grouped)
> dimnames(grouped)
>  
> How do I get at the levels of the 'group by' variables for each
> subset? For example, from this part of the "by" list I want 4, v2 and
> 33:
> : 4
> : v2
> : 33
>       min       max    median      mean
> 0.3897450 0.9215315 0.7300000 0.6700000
> ---------------------------------------
> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From pgilbert at bank-banque-canada.ca  Wed Jun  8 17:19:38 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 08 Jun 2005 11:19:38 -0400
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B701FF@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B701FF@pollux.bfro.uni-lj.si>
Message-ID: <42A70C8A.7080402@bank-banque-canada.ca>

Beware that your easy trick will give you the same result every time you 
run it. You need a better scheme if you actually intend to get a new 
experiment each time you run it.

Paul

Gorjanc Gregor wrote:

> Thanks to Duncan, Dimitris as well as James for answers. I'll provide
> here also example from James, which seems to be the easiest of them 
> all and was not posted to the list:
> 
> niter <- 3
> nchain <- 2
> for (i in 1:niter) { # iterations
>   for (j in 1:nchain) { # chains
>     set.seed(i)
>     a <- runif(1)
>     cat("iter:", i, "chain:", j, "runif:", a, "\n")
>   }
> }
> 
> Note that seed is set with iteration counter. This is really neat and
> simple. I am just wondering if this is OK from "RNG point of view". Can
> someone comment on that?
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> ----------------------------------------------------------------------
> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> Sent: sre 2005-06-08 15:53
> To: Gorjanc Gregor
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Random seed problem in MCMC coupling of chains
>  
> On 6/8/2005 9:27 AM, Gorjanc Gregor wrote:
> 
>>Hello!
>>
>>I am performing coupling of chains in MCMC and I need the same value
>>of seed for two chains. I will show demo of what I want:
>>
>>R code, which might show my example is:
>>niter <- 3
>>nchain <- 2
>>tmpSeed <- 123
>>for (i in 1:niter) { # iterations
>>  for (j in 1:nchain) { # chains
>>    set.seed(tmpSeed)
>>    a <- runif(1)
>>    cat("iter:", i, "chain:", j, "runif:", a, "\n")
>>    tmpSeed <- .Random.seed
>>  }
>>}
>>
>>I get this:
>>
>>iter: 1 chain: 1 runif: 0.43588
>>iter: 1 chain: 2 runif: 0.43588
>>iter: 2 chain: 1 runif: 0.43588
>>iter: 2 chain: 2 runif: 0.43588
>>iter: 3 chain: 1 runif: 0.43588
>>iter: 3 chain: 2 runif: 0.43588
>>
>>but I would like to get:
>>
>>iter: 1 chain: 1 runif: 0.43588
>>iter: 1 chain: 2 runif: 0.43588
>>iter: 2 chain: 1 runif: 0.67676
>>iter: 2 chain: 2 runif: 0.67676
>>iter: 3 chain: 1 runif: 0.12368
>>iter: 3 chain: 2 runif: 0.12368
>>
>>Note that seed value is of course changing, but it is parallel
>>between chains.
> 
> 
> set.seed takes an integer, but .Random.seed is a complicated vector. 
> You need to play with .Random.seed directly, and move your setting of 
> tmpSeed out of the inner loop, i.e.
> 
>  > niter <- 3
>  > nchain <- 2
>  > set.seed(123)
>  > tmpSeed <- .Random.seed
>  > for (i in 1:niter) { # iterations
> +   for (j in 1:nchain) { # chains
> +     .Random.seed <- tmpSeed
> +     a <- runif(1)
> +     cat("iter:", i, "chain:", j, "runif:", a, "\n")
> +   }
> +   tmpSeed <- .Random.seed
> + }
> iter: 1 chain: 1 runif: 0.2875775
> iter: 1 chain: 2 runif: 0.2875775
> iter: 2 chain: 1 runif: 0.7883051
> iter: 2 chain: 2 runif: 0.7883051
> iter: 3 chain: 1 runif: 0.4089769
> iter: 3 chain: 2 runif: 0.4089769
> 
> However, heed the warnings in ?set.seed:  with some generators 
> .Random.seed *does not* contain the full state of the RNG.  As far as I 
> know there is no way to obtain the full state.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From frt at Codan.dk  Wed Jun  8 17:08:35 2005
From: frt at Codan.dk (Fredrik Thuring)
Date: Wed, 8 Jun 2005 17:08:35 +0200
Subject: [R] Solve f(x) = 0
Message-ID: <OF8E140072.815D4B27-ONC125701A.0052960B-C125701A.00533391@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050608/da0d2e12/attachment.pl

From MSchwartz at mn.rr.com  Wed Jun  8 17:29:14 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 08 Jun 2005 10:29:14 -0500
Subject: [R] variable png-size in multiple figure environment
In-Reply-To: <a06100503beccb3b1038a@[129.132.129.185]>
References: <a06100503beccb3b1038a@[129.132.129.185]>
Message-ID: <1118244554.28986.39.camel@localhost.localdomain>

On Wed, 2005-06-08 at 16:46 +0200, Claude ThÅ√Å©ato Kobe wrote:
> If I put only one plot in a png, I can drag the generated png on the 
> edges and it changes its size by this. But when I put several plots 
> on the same sheet using mfrow, the size can no longer be changed when 
> viewing the file, and the resolution is bad. What do I need to do to 
> keep the variability of size in a multiple figure environment?

I have no problem resizing either type of PNG file using ImageMagick
under FC3 by dragging a corner with a mouse, so I suspect the behavior
may be a function of your viewing application. For example, using other
viewers I can zoom in and out, but not drag to resize.

Don't use a bitmapped graphic format if you need to resize, as these do
not resize well, especially diagonal and curved lines.

You should use a vector based format if you need to re-size the image
and maintain image quality. Depending upon your platform (which you do
not indicate) use:

WMF
PS/EPS
PDF

WMF will be available under Windows. The choice may depend upon what you
intend to do with the plots.

Alternatively, pre-specify the graphic device height and width using the
appropriate arguments to the functions so that the output image is as
close as possible to the actual size that you require. Note that for
bitmapped plots, these will be in pixels for the png() function and
inches if you use the bitmap() function, which requires ghostscript. 

The combination of pixels and resolution settings will determine linear
size for a given output device when using bitmapped graphics.

See the help for the specific devices you wish to use or ?Devices for
general help.

HTH,

Marc Schwartz



From dimitris.rizopoulos at med.kuleuven.be  Wed Jun  8 17:44:29 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 8 Jun 2005 17:44:29 +0200
Subject: [R] get level combinations from "by" list
References: <8d5a363505060807561bdc444a@mail.gmail.com>
Message-ID: <016101c56c40$f5218a90$0540210a@www.domain>

I think these are not stored by "by()" and they are produced inside 
"print.by()". One way to get them could be:

dn <- dimnames(grouped)
dn <- expand.grid(dn)
dn <- dn[!sapply(grouped[1:nrow(dn)], is.null), ]
dn

rownames(dfr) <- apply(dn, 1, paste, collapse = "-")
dfr


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "bogdan romocea" <br44114 at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, June 08, 2005 4:56 PM
Subject: [R] get level combinations from "by" list


> Dear useRs,
>
> Given this code I end up with a list of class "by":
>
> a <- sample(1:5,200,replace=TRUE)
> b <- sample(c("v1","v2","v3"),200,replace=TRUE)
> c <- sample(c(11,22,33),200,replace=TRUE)
> data <- runif(200)
> grouped <- by(data,list(a,b,c),function(x) {c(min=min(x),max=max(x),
> median=round(median(x),digits=2),mean=round(mean(x),digits=2))})
> dfr <- do.call("rbind",grouped)    #the levels are missing
> #----------
> grouped
> typeof(grouped)
> class(grouped)
> dimnames(grouped)
>
> How do I get at the levels of the 'group by' variables for each
> subset? For example, from this part of the "by" list I want 4, v2 
> and
> 33:
> : 4
> : v2
> : 33
>      min       max    median      mean
> 0.3897450 0.9215315 0.7300000 0.6700000
> ---------------------------------------
>
> Thank you,
> b.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Wed Jun  8 17:45:55 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 8 Jun 2005 17:45:55 +0200
Subject: [R] Solve f(x) = 0
In-Reply-To: <OF8E140072.815D4B27-ONC125701A.0052960B-C125701A.00533391@codan.dk>
References: <OF8E140072.815D4B27-ONC125701A.0052960B-C125701A.00533391@codan.dk>
Message-ID: <20050608174555.6f6d572c.Achim.Zeileis@wu-wien.ac.at>

On Wed, 8 Jun 2005 17:08:35 +0200 Fredrik Thuring wrote:

> 
> Hi!
> 
> I??m need a function that solves the equation f(x) = 0 (i.e. the root
> of the function) when f is a nonlinear function. Is there any? I??ve
> tried nlm and optim on the square of the function but the solution is
> very unstable.

Look at 
  ?uniroot
hth,
Z

> Thanks before hand.
> 
> / Fredrik Thuring
> 
> 
> ---------------------------------------------------------------------
> --------- This e-mail and any attachment may be confidential and may
> also be privileged. If you are not the intended recipient, please
> notify us immediately and then delete this e-mail and any attachment
> without retaining copies or disclosing the contents thereof to any
> other person. Thank you.
> ---------------------------------------------------------------------
> ---------
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From plummer at iarc.fr  Wed Jun  8 17:55:57 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 08 Jun 2005 17:55:57 +0200
Subject: [R] Solve f(x) = 0
In-Reply-To: <OF8E140072.815D4B27-ONC125701A.0052960B-C125701A.00533391@codan.dk>
References: <OF8E140072.815D4B27-ONC125701A.0052960B-C125701A.00533391@codan.dk>
Message-ID: <1118246157.9433.7.camel@seurat>

On Wed, 2005-06-08 at 17:08 +0200, Fredrik Thuring wrote:
> Hi!
> 
> Im need a function that solves the equation f(x) = 0 (i.e. the root of
> the function) when f is a nonlinear function. Is there any? Ive tried nlm
> and optim on the square of the function but the solution is very unstable.

Your colleague Martin Englund asked the same question.  If x is scalar,
you can use uniroot(). If it's a polynomial then polyroot() finds all
the zeros. I hope that helps.

Martyn



From spencer.graves at pdf.com  Wed Jun  8 18:20:31 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 08 Jun 2005 09:20:31 -0700
Subject: [R] Solve f(x) = 0
In-Reply-To: <1118246157.9433.7.camel@seurat>
References: <OF8E140072.815D4B27-ONC125701A.0052960B-C125701A.00533391@codan.dk>
	<1118246157.9433.7.camel@seurat>
Message-ID: <42A71ACF.8030605@pdf.com>

	  Have you plotted f(x)?  Plot(s) might help reveal why "the solution 
is very unstable."  If f is a function of a univariate x, this is 
trivial.  If f is a function of a bivariate x, use something like 
contour or persp.  If higher dimensions, I might use something like

	  fit <- optim(... hessian=TRUE)
	  eigen(fit$hessian)

	  hope this helps.
	  spencer graves

Martyn Plummer wrote:

> On Wed, 2005-06-08 at 17:08 +0200, Fredrik Thuring wrote:
> 
>>Hi!
>>
>>Im need a function that solves the equation f(x) = 0 (i.e. the root of
>>the function) when f is a nonlinear function. Is there any? Ive tried nlm
>>and optim on the square of the function but the solution is very unstable.
> 
> 
> Your colleague Martin Englund asked the same question.  If x is scalar,
> you can use uniroot(). If it's a polynomial then polyroot() finds all
> the zeros. I hope that helps.
> 
> Martyn
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From david.p.finlayson at gmail.com  Wed Jun  8 18:24:54 2005
From: david.p.finlayson at gmail.com (David Finlayson)
Date: Wed, 8 Jun 2005 09:24:54 -0700
Subject: [R] Specifying medoids in PAM?
In-Reply-To: <17062.47922.43476.902879@stat.math.ethz.ch>
References: <be6d17205060712111a263c56@mail.gmail.com>
	<17062.47922.43476.902879@stat.math.ethz.ch>
Message-ID: <be6d1720506080924373ec772@mail.gmail.com>

Sorry, I wasn't trying to submit a bug report just yet. I wanted to
see if I was using the command correctly. Here is a complete example
(the row names have numbers in them):

> library(cluster)
> stats.table
           slope.for slope.ter prof.len break.elv
1 ALLYN W       0.09      0.05     63.3      1.46
2 ARCADIA       0.12      0.09     40.8      0.43
3 BURFOOT       0.12      0.03     58.9      0.78
4 CAMA BE       0.15      0.05     50.4      0.88
5 DASH PO       0.09      0.01    290.5      3.14
6 EAGLE C       0.09      0.01    226.4      1.49
7 FAY BAI       0.13      0.02     60.3      0.78
8 FORT WA       0.11        NA     36.0        NA
9 FOUL WE       0.15      0.10     31.9      2.38
10 GOLDEN       0.10      0.01    121.1      1.53
11 HERRON       0.13      0.04     69.5      1.59
12 JACOBYS      0.13      0.07     50.9      2.22
13 JOEMMA       0.13      0.04     78.7      2.03
14 KEYSTON      0.15        NA     31.7        NA
15 KITSAP       0.08        NA     41.6        NA
17 OLD MAN      0.13      0.05     39.0      0.74
18 POINT-N      0.13      0.02     76.2      1.00
19 POSSESS      0.10      0.04     60.3      1.11
20 POTLATC      0.07      0.03     55.8      0.09
21 TERMINA      0.09      0.03     88.4      1.80
22 SILVERD      0.06      0.06     57.4      0.50
23 TOLMIE       0.11      0.01    309.2      2.56
24 WARM BE      0.03      0.01    253.5      1.07

> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)

This command crashes RGUI.exe and windows sends an error report to
Microsoft. It also crashes if I first subtract the NA rows from
stats.table.

Thanks,

David 


On 6/8/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "David" == David Finlayson <david.p.finlayson at gmail.com>
> >>>>>     on Tue, 7 Jun 2005 12:11:25 -0700 writes:
> 
>     David> I am using the PAM algorithm in the CLUSTER library.
>     David> When I allow PAM to seed the medoids using the default __build__
>     David> algorithm things work
>     David> well:
> 
>     >> pam(stats.table, metric="euclidean", stand=TRUE, k=5)
> 
>     David> But I have some clusters from a Hierarchical analysis that I would
>     David> like to use as seeds for the PAM algorithm. I can't figure what the
>     David> mediod argument wants. When I put in the five integer indicies for the
>     David> observations in stats.table that I would like to use as seeds (the row
>     David> numbers), I segfault R.
> 
>     >> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)
> 
>     David> *** R Crashes ***
> 
> this is not very helpful.
> 
> Can you please   READ the posting guide and then
> do as the guide says :
> 
>  -  post a *reproducible* example
>  -  tell more about what happens when  "R Crashes"
> 
>     David> Here is my version info:
>     >> version
>     David> _
>     David> platform i386-pc-mingw32
>     David> arch     i386
>     David> os       mingw32
>     David> system   i386, mingw32
>     David> status
>     David> major    2
>     David> minor    0.1
>     David> year     2004
>     David> month    11
>     David> day      15
>     David> language R
> 
>     David> Any guidance would be appreciated.
> 
>     David> David
> 
>     David> --
>     David> David Finlayson
>     David> Marine Geology & Geophysics
>     David> School of Oceanography
>     David> Box 357940
>     David> University of Washington
>     David> Seattle, WA  98195-7940
>     David> USA
> 
>     David> Office: Marine Sciences Building, Room 112
>     David> Phone: (206) 616-9407
>     David> Web: http://students.washington.edu/dfinlays
> 
>     David> ______________________________________________
>     David> R-help at stat.math.ethz.ch mailing list
>     David> https://stat.ethz.ch/mailman/listinfo/r-help
>     David> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
David Finlayson
Marine Geology & Geophysics
School of Oceanography
Box 357940
University of Washington
Seattle, WA  98195-7940
USA

Office: Marine Sciences Building, Room 112
Phone: (206) 616-9407
Web: http://students.washington.edu/dfinlays



From ggrothendieck at gmail.com  Wed Jun  8 18:55:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 8 Jun 2005 12:55:07 -0400
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <42A70C8A.7080402@bank-banque-canada.ca>
References: <7FFEE688B57D7346BC6241C55900E730B701FF@pollux.bfro.uni-lj.si>
	<42A70C8A.7080402@bank-banque-canada.ca>
Message-ID: <971536df0506080955611cbc6@mail.gmail.com>

That could be addressed like this (where changing the offset
changes the experiment).

offset <- 123

niter <- 3
nchain <- 2
for (i in 1:niter) { # iterations
 for (j in 1:nchain) { # chains
   set.seed(i+offset)
   a <- runif(1)
   cat("iter:", i, "chain:", j, "runif:", a, "\n")
 }
}

On 6/8/05, Paul Gilbert <pgilbert at bank-banque-canada.ca> wrote:
> Beware that your easy trick will give you the same result every time you
> run it. You need a better scheme if you actually intend to get a new
> experiment each time you run it.
> 
> Paul
> 
> Gorjanc Gregor wrote:
> 
> > Thanks to Duncan, Dimitris as well as James for answers. I'll provide
> > here also example from James, which seems to be the easiest of them
> > all and was not posted to the list:
> >
> > niter <- 3
> > nchain <- 2
> > for (i in 1:niter) { # iterations
> >   for (j in 1:nchain) { # chains
> >     set.seed(i)
> >     a <- runif(1)
> >     cat("iter:", i, "chain:", j, "runif:", a, "\n")
> >   }
> > }
> >
> > Note that seed is set with iteration counter. This is really neat and
> > simple. I am just wondering if this is OK from "RNG point of view". Can
> > someone comment on that?
> >
> > Lep pozdrav / With regards,
> >     Gregor Gorjanc
> >
> > ----------------------------------------------------------------------
> > University of Ljubljana
> > Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> > Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> > Groblje 3                   tel: +386 (0)1 72 17 861
> > SI-1230 Domzale             fax: +386 (0)1 72 17 888
> > Slovenia, Europe
> > ----------------------------------------------------------------------
> > "One must learn by doing the thing; for though you think you know it,
> >  you have no certainty until you try." Sophocles ~ 450 B.C.
> > ----------------------------------------------------------------------
> >
> > -----Original Message-----
> > From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> > Sent: sre 2005-06-08 15:53
> > To: Gorjanc Gregor
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Random seed problem in MCMC coupling of chains
> >
> > On 6/8/2005 9:27 AM, Gorjanc Gregor wrote:
> >
> >>Hello!
> >>
> >>I am performing coupling of chains in MCMC and I need the same value
> >>of seed for two chains. I will show demo of what I want:
> >>
> >>R code, which might show my example is:
> >>niter <- 3
> >>nchain <- 2
> >>tmpSeed <- 123
> >>for (i in 1:niter) { # iterations
> >>  for (j in 1:nchain) { # chains
> >>    set.seed(tmpSeed)
> >>    a <- runif(1)
> >>    cat("iter:", i, "chain:", j, "runif:", a, "\n")
> >>    tmpSeed <- .Random.seed
> >>  }
> >>}
> >>
> >>I get this:
> >>
> >>iter: 1 chain: 1 runif: 0.43588
> >>iter: 1 chain: 2 runif: 0.43588
> >>iter: 2 chain: 1 runif: 0.43588
> >>iter: 2 chain: 2 runif: 0.43588
> >>iter: 3 chain: 1 runif: 0.43588
> >>iter: 3 chain: 2 runif: 0.43588
> >>
> >>but I would like to get:
> >>
> >>iter: 1 chain: 1 runif: 0.43588
> >>iter: 1 chain: 2 runif: 0.43588
> >>iter: 2 chain: 1 runif: 0.67676
> >>iter: 2 chain: 2 runif: 0.67676
> >>iter: 3 chain: 1 runif: 0.12368
> >>iter: 3 chain: 2 runif: 0.12368
> >>
> >>Note that seed value is of course changing, but it is parallel
> >>between chains.
> >
> >
> > set.seed takes an integer, but .Random.seed is a complicated vector.
> > You need to play with .Random.seed directly, and move your setting of
> > tmpSeed out of the inner loop, i.e.
> >
> >  > niter <- 3
> >  > nchain <- 2
> >  > set.seed(123)
> >  > tmpSeed <- .Random.seed
> >  > for (i in 1:niter) { # iterations
> > +   for (j in 1:nchain) { # chains
> > +     .Random.seed <- tmpSeed
> > +     a <- runif(1)
> > +     cat("iter:", i, "chain:", j, "runif:", a, "\n")
> > +   }
> > +   tmpSeed <- .Random.seed
> > + }
> > iter: 1 chain: 1 runif: 0.2875775
> > iter: 1 chain: 2 runif: 0.2875775
> > iter: 2 chain: 1 runif: 0.7883051
> > iter: 2 chain: 2 runif: 0.7883051
> > iter: 3 chain: 1 runif: 0.4089769
> > iter: 3 chain: 2 runif: 0.4089769
> >
> > However, heed the warnings in ?set.seed:  with some generators
> > .Random.seed *does not* contain the full state of the RNG.  As far as I
> > know there is no way to obtain the full state.
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Wed Jun  8 18:57:55 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 8 Jun 2005 18:57:55 +0200
Subject: [R] Specifying medoids in PAM?
In-Reply-To: <be6d1720506080924373ec772@mail.gmail.com>
References: <be6d17205060712111a263c56@mail.gmail.com>
	<17062.47922.43476.902879@stat.math.ethz.ch>
	<be6d1720506080924373ec772@mail.gmail.com>
Message-ID: <17063.9107.373753.833045@stat.math.ethz.ch>

>>>>> "David" == David Finlayson <david.p.finlayson at gmail.com>
>>>>>     on Wed, 8 Jun 2005 09:24:54 -0700 writes:

    David> Sorry, I wasn't trying to submit a bug report just yet. 

the posting guide asks you to provide reproducible examples, in
any case, not just for bug reports ...
{and strictly speaking, you still haven't provided one, since
 it's a bit painful to read in your table below -- because of the
 extra row names ... but here I'm nit picking a bit }

    David> I wanted to see if I was using the command correctly. 

Yes, you were.

    David> Here is a complete example (the row names have numbers in them):

    >> library(cluster)
    >> stats.table
    David> slope.for slope.ter prof.len break.elv
    David> 1 ALLYN W       0.09      0.05     63.3      1.46
    David> 2 ARCADIA       0.12      0.09     40.8      0.43
    David> 3 BURFOOT       0.12      0.03     58.9      0.78
    David> 4 CAMA BE       0.15      0.05     50.4      0.88
    David> 5 DASH PO       0.09      0.01    290.5      3.14
    David> 6 EAGLE C       0.09      0.01    226.4      1.49
    ..................

    >> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)

    David> This command crashes RGUI.exe and windows sends an error report to
    David> Microsoft. It also crashes if I first subtract the NA rows from
    David> stats.table.

I can confirm to get segmentation faults using this example data
with k=5 ,  so effectively, it seems you've uncovered a bug in pam().
I will investigate and patch eventually.

Thank you for the report.
Martin Maechler, 
ETH Zurich

    David> Thanks,

    David> David 


    David> On 6/8/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >> >>>>> "David" == David Finlayson <david.p.finlayson at gmail.com>
    >> >>>>>     on Tue, 7 Jun 2005 12:11:25 -0700 writes:
    >> 
    David> I am using the PAM algorithm in the CLUSTER library.
    David> When I allow PAM to seed the medoids using the default __build__
    David> algorithm things work
    David> well:
    >> 
    >> >> pam(stats.table, metric="euclidean", stand=TRUE, k=5)
    >> 
    David> But I have some clusters from a Hierarchical analysis that I would
    David> like to use as seeds for the PAM algorithm. I can't figure what the
    David> mediod argument wants. When I put in the five integer indicies for the
    David> observations in stats.table that I would like to use as seeds (the row
    David> numbers), I segfault R.
    >> 
    >> >> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)
    >> 
    David> *** R Crashes ***
    >> 
    >> this is not very helpful.
    >> 
    >> Can you please   READ the posting guide and then
    >> do as the guide says :
    >> 
    >> -  post a *reproducible* example
    >> -  tell more about what happens when  "R Crashes"
    >> 
    David> Here is my version info:
    >> >> version
    David> _
    David> platform i386-pc-mingw32
    David> arch     i386
    David> os       mingw32
    David> system   i386, mingw32
    David> status
    David> major    2
    David> minor    0.1
    David> year     2004
    David> month    11
    David> day      15
    David> language R
    >> 
    David> Any guidance would be appreciated.
    >> 
    David> David
    >> 
    David> --
    David> David Finlayson
    David> Marine Geology & Geophysics
    David> School of Oceanography
    David> Box 357940
    David> University of Washington
    David> Seattle, WA  98195-7940
    David> USA
    >> 
    David> Office: Marine Sciences Building, Room 112
    David> Phone: (206) 616-9407
    David> Web: http://students.washington.edu/dfinlays
    >> 
    David> ______________________________________________
    David> R-help at stat.math.ethz.ch mailing list
    David> https://stat.ethz.ch/mailman/listinfo/r-help
    David> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
    >> 


    David> -- 
    David> David Finlayson
    David> Marine Geology & Geophysics
    David> School of Oceanography
    David> Box 357940
    David> University of Washington
    David> Seattle, WA  98195-7940
    David> USA

    David> Office: Marine Sciences Building, Room 112
    David> Phone: (206) 616-9407
    David> Web: http://students.washington.edu/dfinlays



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jun  8 19:28:17 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 8 Jun 2005 19:28:17 +0200
Subject: [R] Random seed problem in MCMC coupling of chains
Message-ID: <7FFEE688B57D7346BC6241C55900E730B70200@pollux.bfro.uni-lj.si>

Thanks to Paul and Gabor for additional tips/examples. Actually, I find
Pauls suggestion with setRNG also nice and is exactly what I wanted. 
Paul, if I understand this correctly, your suggestion with setRNG does not
alter "RNG flow", it just takes care that chains really have equal seeds.
I remember that I have read somewhere that destroying "RNG flow over and
over to get real randomness" is not a good idea. Can someone confirm this?

niter <- 3
nchain <- 2
for (i in 1:niter) { # iterations
  tmpSeed <- setRNG()
  for (j in 1:nchain) { # chains
    setRNG(tmpSeed)
    a <- runif(1)
    cat("iter:", i, "chain:", j, "runif:", a, "\n")
  }
}

iter: 1 chain: 1 runif: 0.8160078
iter: 1 chain: 2 runif: 0.8160078
iter: 2 chain: 1 runif: 0.4909793
iter: 2 chain: 2 runif: 0.4909793
iter: 3 chain: 1 runif: 0.4425924
iter: 3 chain: 2 runif: 0.4425924

[... removed other stuff ...]

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From mike.rstat at gmail.com  Wed Jun  8 20:32:33 2005
From: mike.rstat at gmail.com (Mike R)
Date: Wed, 8 Jun 2005 11:32:33 -0700
Subject: [R] how to run a script at the beginning of an interacive session
	?
In-Reply-To: <1118238995.28986.0.camel@localhost.localdomain>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
	<1118199932.13958.4.camel@localhost.localdomain>
	<27db823f05060720394f26a39c@mail.gmail.com>
	<27db823f0506072133201d554@mail.gmail.com>
	<1118238995.28986.0.camel@localhost.localdomain>
Message-ID: <27db823f050608113241ad08eb@mail.gmail.com>

On 6/8/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
<snips>

Thanks Marc,

Cool ideas, thanks!  Building on them, here is a twist. 

With this line in .Rprofile:

  .First <- function() {if(Sys.getenv("R_PROJECT")!="")
source(Sys.getenv("R_PROJECT"))}

R can be run with this bash command line:

##  export R_PROJECT=project_A.R;  R; unset R_PROJECT


Alternatively, R could be run with this bash command line:

##  wrapR project_4.R

---- contents of wrapR ----
#! /bin/bash
export R_PROJECT=$1
R 
---- end ----

What do you think? 

Thanks in advance,
Mike



From ggrothendieck at gmail.com  Wed Jun  8 20:42:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 8 Jun 2005 14:42:23 -0400
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B70200@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B70200@pollux.bfro.uni-lj.si>
Message-ID: <971536df050608114216ae2e5e@mail.gmail.com>

Here is a small variation.   We define a list to hold
the last seed for each chain.  Each time we enter the simulation
for a chain we use that seed and each time we exit we update it.
The loop becomes simpler since the setup is all done prior
to looping and everything else is done in the inner loop.

Note that a double loop with nothing between the first and second
for is really like a single loop over the i,j pairs so its presumably
easier to understand.  

library(setRNG)

set.seed(123)
niter <- 3; nchain <- 2
chain <- lapply(1:nchain, function(x) setRNG())

for(i in 1:niter)
  for(j in 1:nchain) {  

    setRNG(chain[[j]])   # get seed

    a <- runif(1)
    cat("iter:", i, "chain:", j, "runif:", a, "\n")

    chain[[j]] <- setRNG()  # save seed

  }

On 6/8/05, Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si> wrote:
> Thanks to Paul and Gabor for additional tips/examples. Actually, I find
> Pauls suggestion with setRNG also nice and is exactly what I wanted.
> Paul, if I understand this correctly, your suggestion with setRNG does not
> alter "RNG flow", it just takes care that chains really have equal seeds.
> I remember that I have read somewhere that destroying "RNG flow over and
> over to get real randomness" is not a good idea. Can someone confirm this?
> 
> niter <- 3
> nchain <- 2
> for (i in 1:niter) { # iterations
>  tmpSeed <- setRNG()
>  for (j in 1:nchain) { # chains
>    setRNG(tmpSeed)
>    a <- runif(1)
>    cat("iter:", i, "chain:", j, "runif:", a, "\n")
>  }
> }
> 
> iter: 1 chain: 1 runif: 0.8160078
> iter: 1 chain: 2 runif: 0.8160078
> iter: 2 chain: 1 runif: 0.4909793
> iter: 2 chain: 2 runif: 0.4909793
> iter: 3 chain: 1 runif: 0.4425924
> iter: 3 chain: 2 runif: 0.4425924
> 
> [... removed other stuff ...]
> 
> Lep pozdrav / With regards,
>    Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> ----------------------------------------------------------------------
> 
> 
> 
> 
> 
> 
> 
>



From jtk at cmp.uea.ac.uk  Wed Jun  8 21:45:21 2005
From: jtk at cmp.uea.ac.uk (Jan T. Kim)
Date: Wed, 8 Jun 2005 20:45:21 +0100
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <971536df0506080955611cbc6@mail.gmail.com>
References: <7FFEE688B57D7346BC6241C55900E730B701FF@pollux.bfro.uni-lj.si>
	<42A70C8A.7080402@bank-banque-canada.ca>
	<971536df0506080955611cbc6@mail.gmail.com>
Message-ID: <20050608194521.GB19565@jtkpc.cmp.uea.ac.uk>

On Wed, Jun 08, 2005 at 12:55:07PM -0400, Gabor Grothendieck wrote:
> That could be addressed like this (where changing the offset
> changes the experiment).
> 
> offset <- 123
> 
> niter <- 3
> nchain <- 2
> for (i in 1:niter) { # iterations
>  for (j in 1:nchain) { # chains
>    set.seed(i+offset)
>    a <- runif(1)
>    cat("iter:", i, "chain:", j, "runif:", a, "\n")
>  }
> }
> 
> On 6/8/05, Paul Gilbert <pgilbert at bank-banque-canada.ca> wrote:
> > Beware that your easy trick will give you the same result every time you
> > run it. You need a better scheme if you actually intend to get a new
> > experiment each time you run it.

That's not a bad thing per se; in fact, it's a good thing to be able
to exactly reproduce the results you obtained with your software.
Personally, I dislike the "convenience" feature of random number
generators of generating a seed, frequently based on the system time,
if none has been set explicitly; I always set a seed and frequently
make the seed a commandline option or part of the control parameter
file or the like.

>From this perspective, Gabor's solution seems perfect to me.

> > Paul
> > 
> > Gorjanc Gregor wrote:
> > 
> > > Thanks to Duncan, Dimitris as well as James for answers. I'll provide
> > > here also example from James, which seems to be the easiest of them
> > > all and was not posted to the list:
> > >
> > > niter <- 3
> > > nchain <- 2
> > > for (i in 1:niter) { # iterations
> > >   for (j in 1:nchain) { # chains
> > >     set.seed(i)
> > >     a <- runif(1)
> > >     cat("iter:", i, "chain:", j, "runif:", a, "\n")
> > >   }
> > > }
> > >
> > > Note that seed is set with iteration counter. This is really neat and
> > > simple. I am just wondering if this is OK from "RNG point of view". Can
> > > someone comment on that?

The only concern I could think about is the case of a bad random number
generator, in which the first couple of values are not entirely
uncorrelated to the seed. But I'd be very surprised if that was a
problem with R's RNGs -- I guess it's memories of lousy implementations
C library rand() functions that make me write this remark.

Best regards, Jan
-- 
 +- Jan T. Kim -------------------------------------------------------+
 |    *NEW*    email: jtk at cmp.uea.ac.uk                               |
 |    *NEW*    WWW:   http://www.cmp.uea.ac.uk/people/jtk             |
 *-----=<  hierarchical systems are for files, not for humans  >=-----*



From steve_adams_sd at yahoo.com  Wed Jun  8 20:51:30 2005
From: steve_adams_sd at yahoo.com (Steve Adams)
Date: Wed, 8 Jun 2005 11:51:30 -0700 (PDT)
Subject: [R] install package "snow"
Message-ID: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>

Hi,

I have difficulty in installing package "snow" from
CRAN. Somehow, this package is not shown up in the
available package list when I tried to use the GUI
interface. So I have to go to CRAN to download the
source file: snow_0.2-1.tar.gz. Then I tried to
install it using the following command:

> install.packages(repos=NULL, pkgs='C:\Documents and
Settings\Desktop\snow_0.2-1.tar.gz', type='source',
lib = 'c:/program files/r/rw2010/library/')

Warning message:
installation of package 'C:Documents and
Settingsyzhang24Desktopsnow_0.2-1.tar.gz' had non-zero
exit status in: install.packages(repos = NULL, pkgs =
"C:Documents and
Settingsyzhang24Desktopsnow_0.2-1.tar.gz",  

What's the problem here, and how should I handle it?

Thanks

Steve



From sue at xlsolutions-corp.com  Wed Jun  8 21:18:17 2005
From: sue at xlsolutions-corp.com (sue@xlsolutions-corp.com)
Date: Wed,  8 Jun 2005 12:18:17 -0700
Subject: [R] Course***R/S-plus Programming at 4 locations - July 2005
Message-ID: <20050608191817.31999.qmail@webmail13.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" at 4 locations nationwide.
www.xlsolutions-corp.com/courselist.htm

****Seattle, WA ------------------------- July 11th-12th, 2005
****San Francisco ---------------------- July 14th-15th, 2005
****New York, NY ---------------------- July 25th - 26th,2005
****Boston, MA ------------------------- July 28th - 29th, 2005


Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava

Interested in R/Splus Advanced course? email us.

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From rpeng at jhsph.edu  Wed Jun  8 21:22:58 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 08 Jun 2005 15:22:58 -0400
Subject: [R] install package "snow"
In-Reply-To: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>
References: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>
Message-ID: <42A74592.1080607@jhsph.edu>

I think for 'snow' you need to download the source and build it 
yourself.  I assume you are on Windows (you didn't say!).  'snow' 
depends on external programs so it would seem unwise to have a 
binary package on CRAN.

-roger

Steve Adams wrote:
> Hi,
> 
> I have difficulty in installing package "snow" from
> CRAN. Somehow, this package is not shown up in the
> available package list when I tried to use the GUI
> interface. So I have to go to CRAN to download the
> source file: snow_0.2-1.tar.gz. Then I tried to
> install it using the following command:
> 
> 
>>install.packages(repos=NULL, pkgs='C:\Documents and
> 
> Settings\Desktop\snow_0.2-1.tar.gz', type='source',
> lib = 'c:/program files/r/rw2010/library/')
> 
> Warning message:
> installation of package 'C:Documents and
> Settingsyzhang24Desktopsnow_0.2-1.tar.gz' had non-zero
> exit status in: install.packages(repos = NULL, pkgs =
> "C:Documents and
> Settingsyzhang24Desktopsnow_0.2-1.tar.gz",  
> 
> What's the problem here, and how should I handle it?
> 
> Thanks
> 
> Steve
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ripley at stats.ox.ac.uk  Wed Jun  8 21:40:48 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Jun 2005 20:40:48 +0100 (BST)
Subject: [R] install package "snow"
In-Reply-To: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>
References: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506082018180.20254@gannet.stats>

Please do read the FAQ Q7.8.

Once you have followed that, it should install from the sources (it does 
for me).  However, to make it work you need other things which are 
non-trivial to get working on Windows: see

 	http://www.stat.uiowa.edu/~luke/R/cluster/cluster.html

for the lower-level interfaces.  I think snow over PVM has been reported 
to work under Windows, but I don't know about the others.

There is no binary version of the package, as testing that would be 
tedious at best.

On Wed, 8 Jun 2005, Steve Adams wrote:

> Hi,
>
> I have difficulty in installing package "snow" from
> CRAN. Somehow, this package is not shown up in the
> available package list when I tried to use the GUI
> interface. So I have to go to CRAN to download the
> source file: snow_0.2-1.tar.gz. Then I tried to
> install it using the following command:
>
>> install.packages(repos=NULL, pkgs='C:\Documents and
> Settings\Desktop\snow_0.2-1.tar.gz', type='source',
> lib = 'c:/program files/r/rw2010/library/')
>
> Warning message:
> installation of package 'C:Documents and
> Settingsyzhang24Desktopsnow_0.2-1.tar.gz' had non-zero
> exit status in: install.packages(repos = NULL, pkgs =
> "C:Documents and
> Settingsyzhang24Desktopsnow_0.2-1.tar.gz",
>
> What's the problem here, and how should I handle it?
>
> Thanks
>
> Steve
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Wed Jun  8 21:46:18 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 08 Jun 2005 21:46:18 +0200
Subject: [R] install package "snow"
In-Reply-To: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>
References: <20050608185131.64182.qmail@web33306.mail.mud.yahoo.com>
Message-ID: <42A74B0A.8080002@free.fr>

Le 08.06.2005 20:51, Steve Adams a ??crit :

>Hi,
>
>I have difficulty in installing package "snow" from
>CRAN. Somehow, this package is not shown up in the
>available package list when I tried to use the GUI
>interface. So I have to go to CRAN to download the
>source file: snow_0.2-1.tar.gz. Then I tried to
>install it using the following command:
>
>  
>
>>install.packages(repos=NULL, pkgs='C:\Documents and
>>    
>>
>Settings\Desktop\snow_0.2-1.tar.gz', type='source',
>lib = 'c:/program files/r/rw2010/library/')
>
>  
>

Double backslash are required here, as in :

install.packages(repos=NULL, pkgs='C:\\Documents and 
Settings\\Desktop\\snow_0.2-1.tar.gz', type='source',lib = 'c:/program 
files/r/rw2010/library/')

(but the tools are required if you want to build from source on windows, 
so that may not solve the problem)
Romain

>Warning message:
>installation of package 'C:Documents and
>Settingsyzhang24Desktopsnow_0.2-1.tar.gz' had non-zero
>exit status in: install.packages(repos = NULL, pkgs =
>"C:Documents and
>Settingsyzhang24Desktopsnow_0.2-1.tar.gz",  
>
>What's the problem here, and how should I handle it?
>
>Thanks
>
>Steve
>  
>


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jun  8 21:58:32 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 8 Jun 2005 21:58:32 +0200
Subject: FW: [R] Random seed problem in MCMC coupling of chains
Message-ID: <7FFEE688B57D7346BC6241C55900E730B70202@pollux.bfro.uni-lj.si>

And a last post from Paul Gilbert. Thanks to all! This disscusion was
really beneficial for me!

-----Original Message-----
From: Paul Gilbert [mailto:pgilbert at bank-banque-canada.ca]
Sent: sre 2005-06-08 21:01
To: Gorjanc Gregor
Subject: Re: [R] Random seed problem in MCMC coupling of chains
 
Gorjanc Gregor wrote:
> Thanks to Paul and Gabor for additional tips/examples. Actually, I find
> Pauls suggestion with setRNG also nice and is exactly what I wanted. 
> Paul, if I understand this correctly, your suggestion with setRNG does not
> alter "RNG flow", it just takes care that chains really have equal seeds.
> I remember that I have read somewhere that destroying "RNG flow over and
> over to get real randomness" is not a good idea. Can someone confirm this?

In general it is a bad idea to make up your own scheme for setting or 
resetting the RNG. People put a lot of work into studying the properties 
of a RNG. When you mess with it then it is unclear what the result will 
be. It certainly won't be tested unless you test it yourself. If your 
intention is to do research on RNGs then you may want to do that, but if 
your intention is to do other research and just use the RNG, then don't 
mess with it by resetting it with your own scheme.

One additional thing you may want to do is record the initial setting of 
the RNG information so that you can reproduce the experiment if you want 
to (see modification below). The idea in setRNG is to not interfere with 
the flow, only add a few utilities to help record and reset everything 
when that is what is required.

In your example it is important that you generate the same number of 
random numbers in each pass through the chain. If that is not the case 
then even with the setRNG utilities there is a subtle change that you 
are introducing.

HTH,
Paul
> 
> niter <- 3
> nchain <- 2
   startingRNG <- setRNG()
> for (i in 1:niter) { # iterations
>   tmpSeed <- setRNG()
>   for (j in 1:nchain) { # chains
>     setRNG(tmpSeed)
>     a <- runif(1)
>     cat("iter:", i, "chain:", j, "runif:", a, "\n")
>   }
> }
> 
> iter: 1 chain: 1 runif: 0.8160078
> iter: 1 chain: 2 runif: 0.8160078
> iter: 2 chain: 1 runif: 0.4909793
> iter: 2 chain: 2 runif: 0.4909793
> iter: 3 chain: 1 runif: 0.4425924
> iter: 3 chain: 2 runif: 0.4425924
> 
> [... removed other stuff ...]
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> ----------------------------------------------------------------------
> 
> 
> 
> 
> 
>



From kjetil at acelerate.com  Wed Jun  8 21:35:29 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 08 Jun 2005 15:35:29 -0400
Subject: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B70200@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B70200@pollux.bfro.uni-lj.si>
Message-ID: <42A74881.6070602@acelerate.com>

Gorjanc Gregor wrote:

>Thanks to Paul and Gabor for additional tips/examples. Actually, I find
>Pauls suggestion with setRNG also nice and is exactly what I wanted. 
>Paul, if I understand this correctly, your suggestion with setRNG does not
>alter "RNG flow", it just takes care that chains really have equal seeds.
>I remember that I have read somewhere that destroying "RNG flow over and
>over to get real randomness" is not a good idea. Can someone confirm this?
>  
>
That's in Brian Ripley's Simulation book, and certainly in other places.

Kjetil

>niter <- 3
>nchain <- 2
>for (i in 1:niter) { # iterations
>  tmpSeed <- setRNG()
>  for (j in 1:nchain) { # chains
>    setRNG(tmpSeed)
>    a <- runif(1)
>    cat("iter:", i, "chain:", j, "runif:", a, "\n")
>  }
>}
>
>iter: 1 chain: 1 runif: 0.8160078
>iter: 1 chain: 2 runif: 0.8160078
>iter: 2 chain: 1 runif: 0.4909793
>iter: 2 chain: 2 runif: 0.4909793
>iter: 3 chain: 1 runif: 0.4425924
>iter: 3 chain: 2 runif: 0.4425924
>
>[... removed other stuff ...]
>
>Lep pozdrav / With regards,
>    Gregor Gorjanc
>
>----------------------------------------------------------------------
>University of Ljubljana
>Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
>Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
>Groblje 3                   tel: +386 (0)1 72 17 861
>SI-1230 Domzale             fax: +386 (0)1 72 17 888
>Slovenia, Europe
>----------------------------------------------------------------------
>"One must learn by doing the thing; for though you think you know it,
> you have no certainty until you try." Sophocles ~ 450 B.C.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From ggrothendieck at gmail.com  Thu Jun  9 00:26:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 8 Jun 2005 18:26:09 -0400
Subject: FW: [R] Random seed problem in MCMC coupling of chains
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730B70202@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730B70202@pollux.bfro.uni-lj.si>
Message-ID: <971536df05060815265b40ab55@mail.gmail.com>

On 6/8/05, Gorjanc Gregor <Gregor.Gorjanc at bfro.uni-lj.si> wrote:
> And a last post from Paul Gilbert. 
> 
> -----Original Message-----
> From: Paul Gilbert [mailto:pgilbert at bank-banque-canada.ca]
> 
> In your example it is important that you generate the same number of
> random numbers in each pass through the chain. If that is not the case
> then even with the setRNG utilities there is a subtle change that you
> are introducing.
> 

Note that this is actually one of the advantages of the last solution
I posted, namely the one with the chain list.  It maintains the flow
along each chain even if different chains use different number of calls
to the random number generator.



From MSchwartz at mn.rr.com  Thu Jun  9 00:41:08 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 08 Jun 2005 17:41:08 -0500
Subject: [R] how to run a script at the beginning of an interacive
	session	?
In-Reply-To: <27db823f050608113241ad08eb@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
	<1118199932.13958.4.camel@localhost.localdomain>
	<27db823f05060720394f26a39c@mail.gmail.com>
	<27db823f0506072133201d554@mail.gmail.com>
	<1118238995.28986.0.camel@localhost.localdomain>
	<27db823f050608113241ad08eb@mail.gmail.com>
Message-ID: <1118270469.17720.32.camel@localhost.localdomain>

On Wed, 2005-06-08 at 11:32 -0700, Mike R wrote:
> On 6/8/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> <snips>
> 
> Thanks Marc,
> 
> Cool ideas, thanks!  Building on them, here is a twist. 
> 
> With this line in .Rprofile:
> 
>   .First <- function() {if(Sys.getenv("R_PROJECT")!="")
> source(Sys.getenv("R_PROJECT"))}
> 
> R can be run with this bash command line:
> 
> ##  export R_PROJECT=project_A.R;  R; unset R_PROJECT
> 
> 
> Alternatively, R could be run with this bash command line:
> 
> ##  wrapR project_4.R
> 
> ---- contents of wrapR ----
> #! /bin/bash
> export R_PROJECT=$1
> R 
> ---- end ----
> 
> What do you think? 
> 
> Thanks in advance,
> Mike
> 

Mike,

At this point, I think whatever approach to implementing and managing
the process you might find easier is the way to go. Whether it be the
above or my approach.

The initial iteration resulted in overwriting the default ~/.Rprofile
file, which could cause problems if you had other options/settings in
it.

The key is ease of implementation, reduction in conflicts/errors and
ongoing maintenance if you find yourself needing multiple startup
options.

HTH,

Marc



From ggrothendieck at gmail.com  Wed Jun  8 19:08:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 8 Jun 2005 13:08:07 -0400
Subject: [R] [R-pkgs] New CRAN package dyn
Message-ID: <971536df050608100869f5196d@mail.gmail.com>

dyn is an R package that facilitates the use of regression
using time series data with lags and diffs (known as dynamic
regression).  It is a lightweight package that has no
facilities of its own but leverages off the various time
series and regression functions in R to make it easier to
use them together.

Its features include:

- many regression functions.  It can be used with lm, glm,
  loess, rq, randomForest, lqs, rlm and any other regression 
  functions that use model.frame and are written in the
  style of lm.

- many time series classes. It can be used with ts, zooreg,
  zoo, its, and irts time series classes.  This covers
  regular, weakly regular and irregular time series classes.

- missing values.  Time series may have missing values including
  internal missing values.  Both na.omit and na.exclude are
  supported.  

- good citizen.  It does not replace the regression
  functions but rather works with them by providing new
  methods to the standard R generics: model.frame, resid,
  fitted, predict, update, anova and $.

- ease of use.  dyn enables one to use the same regression
  functions (lm, glm, etc.) using the same syntax one has
  always used.  Just preface the regression function name with 
  dyn$ and it is transformed into a regression function that 
  can handle time series:

   dyn$lm( y ~ x + lag(x) + diff(w) )     # lm
   dyn$loess( y ~ x + lag(x) + diff(w) )  # loess

- modular.  dyn can be used with any regression function that
  uses model.frame and is written in the style of lm.  Additional
  classes can be added to dyn simply by adding new methods. dyn
  is modular so such updates can be made without changing dyn, 
  itself. 

- documentation.  It includes a help page and six demos.

   ?dyn           # help file
   demo()         # look under dyn for list of demos
   demo("dyn-rq") # runs indicated dyn demo

The package is available on CRAN.  Comments/questions welcome.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From maechler at stat.math.ethz.ch  Thu Jun  9 01:08:50 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 9 Jun 2005 01:08:50 +0200
Subject: [R] Specifying medoids in PAM?
In-Reply-To: <17063.9107.373753.833045@stat.math.ethz.ch>
References: <be6d17205060712111a263c56@mail.gmail.com>
	<17062.47922.43476.902879@stat.math.ethz.ch>
	<be6d1720506080924373ec772@mail.gmail.com>
	<17063.9107.373753.833045@stat.math.ethz.ch>
Message-ID: <17063.31362.222989.262717@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Wed, 8 Jun 2005 18:57:55 +0200 writes:

>>>>> "David" == David Finlayson <david.p.finlayson at gmail.com>
>>>>>     on Wed, 8 Jun 2005 09:24:54 -0700 writes:

    David> Sorry, I wasn't trying to submit a bug report just yet. 

    MM> the posting guide asks you to provide reproducible examples, in
    MM> any case, not just for bug reports ...
    MM> {and strictly speaking, you still haven't provided one, since
    MM> it's a bit painful to read in your table below -- because of the
    MM> extra row names ... but here I'm nit picking a bit }

    David> I wanted to see if I was using the command correctly. 

    MM> Yes, you were.


    >>> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)

    David> This command crashes RGUI.exe and windows sends an error report to
    David> Microsoft. It also crashes if I first subtract the NA rows from
    David> stats.table.

    MM> I can confirm to get segmentation faults using this example data
    MM> with k=5 ,  so effectively, it seems you've uncovered a bug in pam().
    MM> I will investigate and patch eventually.

I found and fixed the bug:  
Some part of the C code was assuming that the indices in
'medoids' were sorted (increasingly).

I.e., for the moment you can easily work around the problem by
using
   pam(stats.table, ...., medoids=c(1,2,3,5,20), k=5)
instead of
   pam(stats.table, ...., medoids=c(1,3,20,2,5), k=5)


The next version of the cluster package which allows to specify
the "fuzzyness exponent" in fanny()  will have this problem
fixed.

Martin Maechler,
ETH Zurich



From mike.rstat at gmail.com  Thu Jun  9 01:17:55 2005
From: mike.rstat at gmail.com (Mike R)
Date: Wed, 8 Jun 2005 16:17:55 -0700
Subject: [R] Feedback requested on a solution to specifying R-scripts on the
	command line
In-Reply-To: <27db823f050608113241ad08eb@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
	<1118199932.13958.4.camel@localhost.localdomain>
	<27db823f05060720394f26a39c@mail.gmail.com>
	<27db823f0506072133201d554@mail.gmail.com>
	<1118238995.28986.0.camel@localhost.localdomain>
	<27db823f050608113241ad08eb@mail.gmail.com>
Message-ID: <27db823f0506081617281bc7c7@mail.gmail.com>

Hi All,

I'm new to R and would like to get started on the right
foot, so to say.  So I am requesting feedback on a
paradigm for organizing R projects. In esence, I am
trying to organize my R projects in the same way I
organize my numerical simulations.  But it feels like
I am imposing 

In particular, I'm not sure why the ability to specify 
R-scripts on the command line is not "built in" to R.  
For example, am I not understanding the traditional 
paradigm for R projects? If so, is there documentation 
that gives a practical orientation to R projects?

>From the command line, I'd like to be able to start an 
interactive R session and at the same time, be able
to specify on the command line a project-specific or 
task-specific script (R code) that is to be executed
at the beginning of the session.

My current solution is the following:

Add the following line to dot-Rprofile:

.First <- function() 
{ 
 if( Sys.getenv("R_CUSTOM") != "" )
 {
  for ( filename in strsplit(Sys.getenv("R_CUSTOM")," ")[[1]] )
  {
   source( filename )
  }
 }
}

Then set the environement variable R_CUSTOM using a wrapper.  
Here is a wrapper for bash:

#! /bin/bash
export R_CUSTOM=$*
R 

If the wrapper file is named wrapR then here are some sample 
command lines (my shell prompt is set to ##):

## wrapR  

## wrapR  project_kit.R

## wrapR  project_kit.R  00task_verify_mikes_datafile.R

## wrapR  project_kit.R  02task_verify_jeffs_datafile.R

## wrapR  project_kit.R  11task_analyze_pooled_data.R

## wrapR  project_kit.R  21task_generate_figures_for_executivesummary.R

## wrapR  project_kit.R  22task_generate_figures_for_paper.R

Normally these could be run using redirection, 
as in the following command line:

## R < 22kit_n_task_generate_figures_for_paper.R

but in that case, R exits when through.

Any comments would be welcome.  

Thanks in advance,
Mike



From medp9193 at nus.edu.sg  Thu Jun  9 02:15:46 2005
From: medp9193 at nus.edu.sg (Tan Hui Hui Jenny)
Date: Thu, 9 Jun 2005 08:15:46 +0800
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
Message-ID: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>

I'm trying to plot Venn diagrams with more than 3 sets (5 actually) in order to describe graphically the genetic variation between populations.
 
I tried the limma library but realised it can only plot 3 sets.
 
Is there any solution? Of course I could plot the chart manually but it'll take too long (have other datasets)..... One of my dataset is given below. 
 
THanks for any advice. 
 
j
 
	AA	 CH	 EA	 IN	 MY	
[1,]	 0	 0	 0	 1	 0	
[2,]	 1	 0	 0	 0	 0	
[3,]	 1	 0	 0	 0	 0	
[4,]	 0	 0	 0	 0	 1	
[5,]	 1	 0	 0	 0	 0	
[6,]	 1	 0	 0	 0	 0	
[7,]	 1	 0	 0	 0	 0	
[8,]	 1	 0	 0	 1	 0	
[9,]	 1	 0	 0	 0	 0	
[10,]	 1	 0	 0	 0	 0	
[11,]	 1	 0	 0	 0	 0	
[12,]	 0	 0	 0	 0	 1	
[13,]	 1	 0	 0	 0	 0	
[14,]	 0	 0	 1	 0	 0	
[15,]	 1	 0	 0	 0	 0	
[16,]	 0	 0	 1	 0	 0	
[17,]	 1	 0	 0	 0	 0	
[18,]	 0	 0	 0	 1	 0	
[19,]	 1	 0	 0	 1	 0	
[20,]	 0	 0	 1	 0	 0	
[21,]	 0	 1	 0	 0	 0	
[22,]	 1	 0	 0	 0	 0	
[23,]	 0	 0	 1	 0	 0	
[24,]	 1	 0	 1	 1	 1	
[25,]	 0	 1	 0	 0	 0	
[26,]	 1	 0	 0	 0	 0	
[27,]	 1	 0	 0	 0	 0	
[28,]	 0	 0	 0	 1	 0	
[29,]	 0	 0	 0	 1	 0	
[30,]	 1	 0	 0	 0	 0	
[31,]	 1	 0	 0	 0	 0	
[32,]	 0	 0	 0	 1	 0	
[33,]	 0	 0	 0	 1	 0	
[34,]	 0	 1	 0	 0	 0	
[35,]	 1	 0	 0	 0	 0	
[36,]	 0	 0	 0	 1	 0	
[37,]	 0	 0	 1	 1	 0	
[38,]	 1	 0	 0	 1	 0	
[39,]	 0	 0	 0	 1	 0	
[40,]	 0	 0	 0	 1	 0	
[41,]	 0	 1	 0	 0	 0	
[42,]	 1	 0	 0	 0	 0	
[43,]	 0	 0	 0	 1	 0	
[44,]	 0	 0	 1	 0	 0	
[45,]	 1	 0	 0	 0	 0	
[46,]	 1	 0	 0	 0	 0	
[47,]	 0	 0	 0	 1	 0	
[48,]	 1	 0	 0	 0	 0	
[49,]	 0	 0	 0	 1	 0	
[50,]	 0	 0	 1	 0	 0	
[51,]	 0	 0	 0	 1	 0	
[52,]	 1	 0	 0	 0	 0	
[53,]	 0	 0	 0	 1	 0	
[54,]	 1	 0	 0	 0	 0	
[55,]	 0	 1	 0	 0	 0



From sms13+ at pitt.edu  Thu Jun  9 02:39:21 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Wed, 08 Jun 2005 20:39:21 -0400
Subject: [R] Weibull survival modeling with covariate
Message-ID: <530728156.1118263161@Lab26.DOMAIN.IE.PITT.EDU>

I was wondering if someone familiar 
with survival analysis can help me with 
the following.
I would like to fit a Weibull curve, 
that may be dependent on a covariate, 
my dataframe "labdata" that has the 
fields "cov", "time", and "censor".  Do 
I do the following?
wieb<-survreg(Surv(labdata$time, 
labadata$censor)~labdata$cov, 
dist="weibull")

This returns:
> weib
Call:
survreg(formula = Surv(labdata$time, 
labdata$censor) ~ labdata$cov,
    dist = "weibull")

Coefficients:
(Intercept) labdata$cov
8.091955112 0.001552897

Scale= 0.7532474

Loglik(model)= -12633.6 
Loglik(intercept only)= -12734.8
        Chisq= 202.41 on 1 degrees of 
freedom, p= 0
n= 5496


I am not quite sure how to use the 
output.  I see that it gives the Scale 
parameter.  How do I find the Shape 
paramater as a function of the 
covariate?

Thank you,
Steven

---------------------------------------
-------------------------
Steven Shechter
PhD Candidate in Industrial Engineering
University of Pittsburgh
www.pitt.edu/~sms13



From p.connolly at hortresearch.co.nz  Thu Jun  9 03:48:22 2005
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 9 Jun 2005 13:48:22 +1200
Subject: [ESS] Re: [R] Strange characters in 2.1.0?
In-Reply-To: <17062.47189.530606.76588@stat.math.ethz.ch>
References: <40e66e0b05060605512e39815d@mail.gmail.com> 
	<Pine.LNX.4.21.0506061444260.31061-100000@mail.mrc-dunn.cam.ac.uk> 
	<17061.43722.36464.509201@stat.math.ethz.ch> 
	<20050607233144.GL6253@hortresearch.co.nz> 
	<17062.47189.530606.76588@stat.math.ethz.ch>
Message-ID: <20050609014822.GN6253@hortresearch.co.nz>

On Wed, 08-Jun-2005 at 11:20AM +0200, Martin Maechler wrote:

|> >>>>> "PaCo" == Patrick Connolly <p.connolly at hortresearch.co.nz>
|> >>>>>     on Wed, 8 Jun 2005 11:31:44 +1200 writes:
|> 
|>     PaCo> On Tue, 07-Jun-2005 at 04:10PM +0200, Martin Maechler wrote:
|>     PaCo> |> >>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
|> 
|>     PaCo> |>       ..........
|>     PaCo> |> 
|>     PaCo> |>     Dan> I have gone back to 2.0.0 :)
|>     PaCo> |> 
|>     PaCo> |> Don't do that!
|>     PaCo> |> You've lost tons of nice new features and gained quite an amount
|>     PaCo> |> of old bugs by downgrading .. 
|> 
|>     PaCo> I get the non-generic quotes to show on the screen, but they won't
|>     PaCo> print with enscript.  I end up with a lot of wrapped lines and
|>     PaCo> nonsense where an unknown character should be.
|> 
|> Why is this diverted from R- to ESS-help? 

I erroneously thought that I could see the quotes only within Emacs
and not in a terminal window, so I thought it was half way there with
Emacs but not started in a terminal.  I'll check more carefully
henceforth.

[...]

|> If I were in New Zeeland and would not need accents or umlauts,
|> I'd probably stick with latin1  (and would make sure my X
|> server got proper non-utf8 fonts) for another year or so.

Thanks for the clarification.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From rncarpio at yahoo.com  Thu Jun  9 04:10:23 2005
From: rncarpio at yahoo.com (Ronaldo Carpio)
Date: Wed, 8 Jun 2005 19:10:23 -0700 (PDT)
Subject: [R] Forecasting with macroeconomic structural equations models?
Message-ID: <20050609021023.39065.qmail@web50506.mail.yahoo.com>


Hello,

Is there a package or sample code that shows how to do ex ante 
forecasts with a macroeconomic structural equations model?  I looked
at the "sem" package, which lets you estimate e.g. Klein's model, but
I'm not sure how to make simulations using the full set of equations,
including the identities.


Thank you,

Ronaldo Carpio
rncarpio at yahoo.com



From murdoch at stats.uwo.ca  Thu Jun  9 04:11:54 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 08 Jun 2005 22:11:54 -0400
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
In-Reply-To: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
References: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
Message-ID: <42A7A56A.8090207@stats.uwo.ca>

Tan Hui Hui Jenny wrote:
> I'm trying to plot Venn diagrams with more than 3 sets (5 actually) in order to describe graphically the genetic variation between populations.
>  
> I tried the limma library but realised it can only plot 3 sets.
>  
> Is there any solution? Of course I could plot the chart manually but it'll take too long (have other datasets)..... One of my dataset is given below. 

The general Venn diagrams with more than 3 sets are ugly.  You can't use 
circles, you need strange shapes to get all the 2^n-1 possible subsets.

Duncan Murdoch



From wataru at crayon.se.uec.ac.jp  Thu Jun  9 04:19:49 2005
From: wataru at crayon.se.uec.ac.jp (Watalu, Y. (aka Wataru))
Date: Thu, 9 Jun 2005 11:19:49 +0900 (JST)
Subject: [R] Weibull survival modeling with covariate
In-Reply-To: <530728156.1118263161@Lab26.DOMAIN.IE.PITT.EDU>
References: <530728156.1118263161@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <3405.219.111.149.61.1118283589.squirrel@lyra.lente.org>

Hi,

I'm also wondering which expression the survreg() uses
for Weibull regression.  Referring to help(survreg) and
help(survreg.distributions), I guess survreg() fits the
following model.

survreg() uses a different parametrization, say
   F(x, Wshape, Wscale) = 1-exp(-Wscale*(x^Wshape))),
and fits a parametric model with these formulas.
       Wshape  = 1/"Scale"  (calculated by survreg())
   log(Wscale) = model with covariates

Is it correct?

Thanks a lot.

Watalu

> I was wondering if someone familiar
> with survival analysis can help me with
> the following.
> I would like to fit a Weibull curve,
> that may be dependent on a covariate,
> my dataframe "labdata" that has the
> fields "cov", "time", and "censor".  Do
> I do the following?
> wieb<-survreg(Surv(labdata$time,
> labadata$censor)~labdata$cov,
> dist="weibull")
>
> This returns:
>> weib
> Call:
> survreg(formula = Surv(labdata$time,
> labdata$censor) ~ labdata$cov,
>     dist = "weibull")
>
> Coefficients:
> (Intercept) labdata$cov
> 8.091955112 0.001552897
>
> Scale= 0.7532474
>
> Loglik(model)= -12633.6
> Loglik(intercept only)= -12734.8
>         Chisq= 202.41 on 1 degrees of
> freedom, p= 0
> n= 5496
>
>
> I am not quite sure how to use the
> output.  I see that it gives the Scale
> parameter.  How do I find the Shape
> paramater as a function of the
> covariate?
>
> Thank you,
> Steven
>
> ---------------------------------------
> -------------------------
> Steven Shechter
> PhD Candidate in Industrial Engineering
> University of Pittsburgh
> www.pitt.edu/~sms13
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From luan_sheng at yahoo.com  Thu Jun  9 04:27:02 2005
From: luan_sheng at yahoo.com (luan_sheng)
Date: Thu, 09 Jun 2005 10:27:02 +0800
Subject: [R] May I ask you a question about matrix population models?
Message-ID: <42A7A8F6.5040002@yahoo.com>

Dear R user:
Now I am studying "matrix population models" and Rmetasim package,but I
find it's very difficult to understand this model fully and can't
find a good teacher.My question is that:Now I do some research about one
marine shrimp: Penaeus chinensis, it's life cycle is one year. after it
breed it's offsprings, it will died. This shrimp's life cycle is dicrete
,if i use project matrix or life cyle graph to express it,how can i do it?

Best regards

luan_sheng



From ggrothendieck at gmail.com  Thu Jun  9 05:10:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 8 Jun 2005 23:10:10 -0400
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
In-Reply-To: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
References: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
Message-ID: <971536df050608201078507b59@mail.gmail.com>

If the data you posted is prototypical of your datasets then note that:

- it has two disconnected components of 1 and 4 sets
- there are only 8 unique rows out of 32
- 5 of these 8 are the regions that contain only the non-intersecting
  portion of each of the 5 sets

> unique(mydata)
   AA CH EA IN MY
1   0  0  0  1  0
2   1  0  0  0  0
4   0  0  0  0  1
8   1  0  0  1  0
14  0  0  1  0  0
21  0  1  0  0  0
24  1  0  1  1  1
37  0  0  1  1  0

Thus these are not very interesting Venn diagrams as they are
quite sparse.  Perhaps you would be better off to represent them 
as bipartite graphs with a node for each row and a node for each 
column with the edges corresponding to the ones.  The RGraphViz 
package (or graphViz, itself, locatable via google) could be used for 
that.


On 6/8/05, Tan Hui Hui Jenny <medp9193 at nus.edu.sg> wrote:
> I'm trying to plot Venn diagrams with more than 3 sets (5 actually) in order to describe graphically the genetic variation between populations.
> 
> I tried the limma library but realised it can only plot 3 sets.
> 
> Is there any solution? Of course I could plot the chart manually but it'll take too long (have other datasets)..... One of my dataset is given below.
> 
> THanks for any advice.
> 
> j
> 
>        AA       CH      EA      IN      MY
> [1,]     0       0       0       1       0
> [2,]     1       0       0       0       0
> [3,]     1       0       0       0       0
> [4,]     0       0       0       0       1
> [5,]     1       0       0       0       0
> [6,]     1       0       0       0       0
> [7,]     1       0       0       0       0
> [8,]     1       0       0       1       0
> [9,]     1       0       0       0       0
> [10,]    1       0       0       0       0
> [11,]    1       0       0       0       0
> [12,]    0       0       0       0       1
> [13,]    1       0       0       0       0
> [14,]    0       0       1       0       0
> [15,]    1       0       0       0       0
> [16,]    0       0       1       0       0
> [17,]    1       0       0       0       0
> [18,]    0       0       0       1       0
> [19,]    1       0       0       1       0
> [20,]    0       0       1       0       0
> [21,]    0       1       0       0       0
> [22,]    1       0       0       0       0
> [23,]    0       0       1       0       0
> [24,]    1       0       1       1       1
> [25,]    0       1       0       0       0
> [26,]    1       0       0       0       0
> [27,]    1       0       0       0       0
> [28,]    0       0       0       1       0
> [29,]    0       0       0       1       0
> [30,]    1       0       0       0       0
> [31,]    1       0       0       0       0
> [32,]    0       0       0       1       0
> [33,]    0       0       0       1       0
> [34,]    0       1       0       0       0
> [35,]    1       0       0       0       0
> [36,]    0       0       0       1       0
> [37,]    0       0       1       1       0
> [38,]    1       0       0       1       0
> [39,]    0       0       0       1       0
> [40,]    0       0       0       1       0
> [41,]    0       1       0       0       0
> [42,]    1       0       0       0       0
> [43,]    0       0       0       1       0
> [44,]    0       0       1       0       0
> [45,]    1       0       0       0       0
> [46,]    1       0       0       0       0
> [47,]    0       0       0       1       0
> [48,]    1       0       0       0       0
> [49,]    0       0       0       1       0
> [50,]    0       0       1       0       0
> [51,]    0       0       0       1       0
> [52,]    1       0       0       0       0
> [53,]    0       0       0       1       0
> [54,]    1       0       0       0       0
> [55,]    0       1       0       0       0



From Tom.Mulholland at dpi.wa.gov.au  Thu Jun  9 07:50:58 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Thu, 9 Jun 2005 13:50:58 +0800
Subject: [R] May I ask you a question about matrix population models?
Message-ID: <4702645135092E4497088F71D9C8F51A128B8C@afhex01.dpi.wa.gov.au>

Have you found the file "Using Rmetasim"? In windows you can access this file by using the help and selecting "browse" directory. There appears to be a reasonable amount of information here. It looks to me as if you need to work your way through these files until you understand what is going on. Once you have done that you might be able to be more specific with your question.

Tom

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of luan_sheng
> Sent: Thursday, 9 June 2005 10:27 AM
> To: R-help
> Subject: [R] May I ask you a question about matrix population models?
> 
> 
> Dear R user:
> Now I am studying "matrix population models" and Rmetasim 
> package,but I
> find it's very difficult to understand this model fully and can't
> find a good teacher.My question is that:Now I do some 
> research about one
> marine shrimp: Penaeus chinensis, it's life cycle is one 
> year. after it
> breed it's offsprings, it will died. This shrimp's life cycle 
> is dicrete
> ,if i use project matrix or life cyle graph to express it,how 
> can i do it?
> 
> Best regards
> 
> luan_sheng
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gb at stat.umu.se  Thu Jun  9 08:01:09 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 9 Jun 2005 08:01:09 +0200
Subject: [R] Weibull survival modeling with covariate
In-Reply-To: <530728156.1118263161@Lab26.DOMAIN.IE.PITT.EDU>
References: <530728156.1118263161@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <20050609060109.GA25276@stat.umu.se>

On Wed, Jun 08, 2005 at 08:39:21PM -0400, sms13+ at pitt.edu wrote:
> I was wondering if someone familiar 
> with survival analysis can help me with 
> the following.
> I would like to fit a Weibull curve, 
> that may be dependent on a covariate, 
> my dataframe "labdata" that has the 
> fields "cov", "time", and "censor".  Do 
> I do the following?
> wieb<-survreg(Surv(labdata$time, 
> labadata$censor)~labdata$cov, 
> dist="weibull")
> 
> This returns:
> >weib
> Call:
> survreg(formula = Surv(labdata$time, 
> labdata$censor) ~ labdata$cov,
>    dist = "weibull")
> 
> Coefficients:
> (Intercept) labdata$cov
> 8.091955112 0.001552897
> 
> Scale= 0.7532474
> 
> Loglik(model)= -12633.6 
> Loglik(intercept only)= -12734.8
>        Chisq= 202.41 on 1 degrees of 
> freedom, p= 0
> n= 5496
> 
> 
> I am not quite sure how to use the 
> output.  I see that it gives the Scale 
> parameter.  How do I find the Shape 
> paramater as a function of the 
> covariate?

You don't. The analysis is performed on the logs of durations, so scale is
transformed to location and shape to scale. For more intuitive output, use 
'weibreg' in package 'eha'. It can also handle left truncated data. But
only Weibull (and exponential) regression.
 
> 
> Thank you,
> Steven
> 
> ---------------------------------------
> -------------------------
> Steven Shechter
> PhD Candidate in Industrial Engineering
> University of Pittsburgh
> www.pitt.edu/~sms13
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
G??ran Brostr??m                    tel: +46 90 786 5223
Professor and Head
Department of Statistics          fax: +46 90 786 6614
Ume?? University                   http://www.stat.umu.se/~goran.brostrom/
SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From Giuseppe.Palermo at bo.infn.it  Thu Jun  9 10:37:41 2005
From: Giuseppe.Palermo at bo.infn.it (Giuseppe.Palermo@bo.infn.it)
Date: Thu,  9 Jun 2005 10:37:41 +0200
Subject: [R] Prediction in Cox Proportional-Hazard Regression
Message-ID: <20050609103741.vtkai819wc8o0gos@lnxm.bo.infn.it>

He,
I used the "coxph" function, with four covariates.

Let's say something like that

> model.1 <- coxph(Surv(Time,Event)~X1+X2+X3+X4,data=DATA)

So I obtain the 4 coefficients B1,B2,B3,B4 such that

h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4).

When I use the function on the same data

> predict.coxph(model.1,type="lp")

how it works in making the prediction?
I mean which is the formula, given the data-point P1=[X1(1),X2(1),X3(1),X4(1)],
that the function "predict.coxph" use to make the prediction of P1.

I really hope that someone will reply to my question.

Best regards to all
Giuseppe



From navarre_sabine at yahoo.fr  Thu Jun  9 10:48:30 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Thu, 9 Jun 2005 10:48:30 +0200 (CEST)
Subject: [R] color on barplot
Message-ID: <20050609084830.84023.qmail@web26607.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/f1a29ff8/attachment.pl

From francoisromain at free.fr  Thu Jun  9 11:08:49 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 09 Jun 2005 11:08:49 +0200
Subject: [R] color on barplot
In-Reply-To: <20050609084830.84023.qmail@web26607.mail.ukl.yahoo.com>
References: <20050609084830.84023.qmail@web26607.mail.ukl.yahoo.com>
Message-ID: <42A80721.5090103@free.fr>

Le 09.06.2005 10:48, Navarre Sabine a ??crit :

>Hello,
>
>On my barplot, I have on the axis y, the names of the rows of my matrix r_mat!
>is it possible to change the color of these names on my barplot? 
>
>barplot2(t(r_mat),beside=TRUE,horiz=TRUE,plot.grid=TRUE,xlab="R",font.lab=4,las=2,xlim=c(0,1))
>
>thanks,
>
>Sabine
>
>  
>
Try
par(col.axis="blue")

And please give *reproductible* examples, here the package gplots must 
be loaded and we don't have access to r_mat.

Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ajayshah at mayin.org  Thu Jun  9 11:13:13 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Thu, 9 Jun 2005 14:43:13 +0530
Subject: [R] optim() does SANN, why not genetic algorithm (genoud)
Message-ID: <20050609091313.GX7131@lubyanka.local>

It is a very nice touch that optim() offers SANN (simulated annealing)
as a random search algorithm.

The R community already has genoud - an implementation of a genetic
algorithm for search.

Wouldn't it be neat if optim() would additionally offer method="GA"
where it internally uses code from genoud?

I glanced at the code and I find the work is all being done in

   res <- .Internal(optim(par, fn1, gr1, method, con, lower, upper))

Should we do it as:

   if (method == "GA") {
     # insert genoud calls here.
   } else {
     res <- .Internal(optim(par, fn1, gr1, method, con, lower, upper))
   }

Who wrote optim, and who maintains it?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ripley at stats.ox.ac.uk  Thu Jun  9 11:13:21 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Jun 2005 10:13:21 +0100 (BST)
Subject: [R] Prediction in Cox Proportional-Hazard Regression
In-Reply-To: <20050609103741.vtkai819wc8o0gos@lnxm.bo.infn.it>
References: <20050609103741.vtkai819wc8o0gos@lnxm.bo.infn.it>
Message-ID: <Pine.LNX.4.61.0506091005520.15507@gannet.stats>

On Thu, 9 Jun 2005 Giuseppe.Palermo at bo.infn.it wrote:

> He,
> I used the "coxph" function, with four covariates.
>
> Let's say something like that
>
>> model.1 <- coxph(Surv(Time,Event)~X1+X2+X3+X4,data=DATA)
>
> So I obtain the 4 coefficients B1,B2,B3,B4 such that
>
> h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4).
>
> When I use the function on the same data
>
>> predict.coxph(model.1,type="lp")

How does that work?  predict.coxph is not an exported function!

> how it works in making the prediction?
> I mean which is the formula, given the data-point P1=[X1(1),X2(1),X3(1),X4(1)],
> that the function "predict.coxph" use to make the prediction of P1.

>From the code (getAnywhere("predict.coxph"))

     if (type == "lp" || type == "risk") {
         if (missing(newdata)) {
             pred <- object$linear.predictors
             names(pred) <- names(object$residuals)
         }
         else pred <- x %*% coef + offset
...

so that is the formula it uses.  As you did not supply 'newdata', it 
quotes the 'linear.predictors' component of the fit: see ?coxph.object.

Effectively it centred the explanatory variables on their means and then 
applied the linear regression formula to give the linear predictor. It is 
the centring that may be non-obvious: effectively h_0(t), the baseline 
hazard, is taken at the average of the subjects.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From amsa36060 at yahoo.com  Thu Jun  9 11:36:35 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Thu, 9 Jun 2005 02:36:35 -0700 (PDT)
Subject: [R] Error to install library( fSeries)
Message-ID: <20050609093635.44770.qmail@web60423.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/bd5311fa/attachment.pl

From navarre_sabine at yahoo.fr  Thu Jun  9 11:48:32 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Thu, 9 Jun 2005 11:48:32 +0200 (CEST)
Subject: [R] plot3d
Message-ID: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/ed2c6ca0/attachment.pl

From Andreas.Friedrich at dit.de  Thu Jun  9 12:01:03 2005
From: Andreas.Friedrich at dit.de (Friedrich, Andreas (dit))
Date: Thu, 9 Jun 2005 12:01:03 +0200 
Subject: [R] plot3d
Message-ID: <D86FDEC0CB88E54391764DDF2169CC9E03360D9D@afwpm005.intradit.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/a2ea1141/attachment.pl

From francoisromain at free.fr  Thu Jun  9 12:06:20 2005
From: francoisromain at free.fr (Romain Francois)
Date: Thu, 09 Jun 2005 12:06:20 +0200
Subject: [R] plot3d
In-Reply-To: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
References: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <42A8149C.9080304@free.fr>

Le 09.06.2005 11:48, Navarre Sabine a ??crit :

>hello,
> 
>to use the function plot3d, i should use the package R.basic!
>
>plot3d {R.basic}
>
>If people know exactly a site to load this package, please give me the URL!
>
>Thanks
>
>Sabine
>  
>
Hello Sabine,

Do you plan to ask that same question everyday.
Today, you are more precise by giving the actual name of the package you 
search. R.basic is a part  of the R.classes bundle :
http://www.maths.lth.se/help/R/R.classes/

Google could have find it for you.


Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From Achim.Zeileis at wu-wien.ac.at  Thu Jun  9 12:09:30 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 9 Jun 2005 12:09:30 +0200
Subject: [R] plot3d
In-Reply-To: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
References: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <20050609120930.02484eab.Achim.Zeileis@wu-wien.ac.at>

On Thu, 9 Jun 2005 11:48:32 +0200 (CEST) Navarre Sabine wrote:

> hello,
>  
> to use the function plot3d, i should use the package R.basic!
> 
> plot3d {R.basic}

Better question than your previous two...

> If people know exactly a site to load this package, please give me the
> URL!

...but not difficult to answer: It's obviously not on CRAN. Entering
"R.basic" into Google returns Henrik Bengtsson's page as the first hit.
And indeed R.basic is part of the R.classes bundle. See
  http://www.maths.lth.se/help/R/R.classes/

To install, use something like:

install.packages("R.classes",
  contriburl="http://www.maths.lth.se/help/R")

Best,
Z

> Thanks
> 
> Sabine
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Giuseppe.Palermo at bo.infn.it  Thu Jun  9 12:13:46 2005
From: Giuseppe.Palermo at bo.infn.it (Giuseppe.Palermo@bo.infn.it)
Date: Thu,  9 Jun 2005 12:13:46 +0200
Subject: [R] Prediction in Cox Proportional-Hazard Regression
In-Reply-To: <Pine.LNX.4.61.0506091005520.15507@gannet.stats>
References: <20050609103741.vtkai819wc8o0gos@lnxm.bo.infn.it>
	<Pine.LNX.4.61.0506091005520.15507@gannet.stats>
Message-ID: <20050609121346.jpvedwkg004okso4@lnxm.bo.infn.it>

Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> On Thu, 9 Jun 2005 Giuseppe.Palermo at bo.infn.it wrote:
>
>> He,
>> I used the "coxph" function, with four covariates.
>>
>> Let's say something like that
>>
>>> model.1 <- coxph(Surv(Time,Event)~X1+X2+X3+X4,data=DATA)
>>
>> So I obtain the 4 coefficients B1,B2,B3,B4 such that
>>
>> h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4).
>>
>> When I use the function on the same data
>>
>>> predict.coxph(model.1,type="lp")
>
> How does that work?  predict.coxph is not an exported function!
>
>> how it works in making the prediction?
>> I mean which is the formula, given the data-point 
>> P1=[X1(1),X2(1),X3(1),X4(1)],
>> that the function "predict.coxph" use to make the prediction of P1.
>
> From the code (getAnywhere("predict.coxph"))
>
>     if (type == "lp" || type == "risk") {
>         if (missing(newdata)) {
>             pred <- object$linear.predictors
>             names(pred) <- names(object$residuals)
>         }
>         else pred <- x %*% coef + offset
> ...
>
> so that is the formula it uses.  As you did not supply 'newdata', it 
> quotes the 'linear.predictors' component of the fit: see 
> ?coxph.object.
>
> Effectively it centred the explanatory variables on their means and 
> then applied the linear regression formula to give the linear 
> predictor. It is the centring that may be non-obvious: effectively 
> h_0(t), the baseline hazard, is taken at the average of the subjects.
>

Dear Prof. Ripley
Thanks for replying to me email.
I only have an other question:

since h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4)
represent the hazard at time t.

In a linear prediction,
what     Value = B1*(X1-mean(X1)) + B2*(X2-mean(X2)) + ....
represent?

> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ligges at statistik.uni-dortmund.de  Thu Jun  9 12:15:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 09 Jun 2005 12:15:03 +0200
Subject: [R] plot3d
In-Reply-To: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
References: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <42A816A7.2030000@statistik.uni-dortmund.de>

Navarre Sabine wrote:

> hello,
>  
> to use the function plot3d, i should use the package R.basic!
> 
> plot3d {R.basic}


I have just googled for
"plot3d R.basic"
and the first hit pointed me to the correct URL ....

Uwe Ligges


> If people know exactly a site to load this package, please give me the URL!
> 
> Thanks
> 
> Sabine
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Jun  9 12:15:42 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 9 Jun 2005 06:15:42 -0400
Subject: [R] plot3d
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E96B@usctmx1106.merck.com>

I do not see R.basic on CRAN, but digging a bit turns up Henrik's R.classes
bundle, which you can get by

install.packages("R.classes", contriburl="http://www.maths.lth.se/help/R")

After that, just do

library(R.basic)
example(plot3d)

Andy

> From: Navarre Sabine
> 
> hello,
>  
> to use the function plot3d, i should use the package R.basic!
> 
> plot3d {R.basic}
> 
> If people know exactly a site to load this package, please 
> give me the URL!
> 
> Thanks
> 
> Sabine
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ripley at stats.ox.ac.uk  Thu Jun  9 12:17:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Jun 2005 11:17:25 +0100 (BST)
Subject: [R] Error to install library( fSeries)
In-Reply-To: <20050609093635.44770.qmail@web60423.mail.yahoo.com>
References: <20050609093635.44770.qmail@web60423.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506091114160.18828@gannet.stats>

It is a file sustem problem on your computer (Windows, unstated).

Either you do not have permission to write in that directory or you have 
some rogue files there.  If you do have permission, go in and delete all 
folders starting with `file', then reboot Windows and try again.

If you do not have permission, you will need to install in a different 
place: see the rw-FAQ.

On Thu, 9 Jun 2005, Amir Safari wrote:

> After trying to install some libraries such as fSeries, I receive this error:
>
> Fehler: unable to create temp directory 'C:/Programme/R/rw2010pat/library\file12763'
>
> Also after trying to install other packages ,I receive similar errors, for example, installation of tseries:
> Fehler: unable to create temp directory 'C:/Programme/R/rw2010pat/library\file30539'
> ...
> what is my problem? Is it related to my computer?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun  9 12:19:41 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Jun 2005 11:19:41 +0100 (BST)
Subject: [R] Prediction in Cox Proportional-Hazard Regression
In-Reply-To: <20050609121346.jpvedwkg004okso4@lnxm.bo.infn.it>
References: <20050609103741.vtkai819wc8o0gos@lnxm.bo.infn.it>
	<Pine.LNX.4.61.0506091005520.15507@gannet.stats>
	<20050609121346.jpvedwkg004okso4@lnxm.bo.infn.it>
Message-ID: <Pine.LNX.4.61.0506091119030.30175@gannet.stats>

On Thu, 9 Jun 2005 Giuseppe.Palermo at bo.infn.it wrote:

> Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:
>
>> On Thu, 9 Jun 2005 Giuseppe.Palermo at bo.infn.it wrote:
>> 
>>> He,
>>> I used the "coxph" function, with four covariates.
>>> 
>>> Let's say something like that
>>> 
>>>> model.1 <- coxph(Surv(Time,Event)~X1+X2+X3+X4,data=DATA)
>>> 
>>> So I obtain the 4 coefficients B1,B2,B3,B4 such that
>>> 
>>> h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4).
>>> 
>>> When I use the function on the same data
>>> 
>>>> predict.coxph(model.1,type="lp")
>> 
>> How does that work?  predict.coxph is not an exported function!
>> 
>>> how it works in making the prediction?
>>> I mean which is the formula, given the data-point 
>>> P1=[X1(1),X2(1),X3(1),X4(1)],
>>> that the function "predict.coxph" use to make the prediction of P1.
>> 
>> From the code (getAnywhere("predict.coxph"))
>> 
>>     if (type == "lp" || type == "risk") {
>>         if (missing(newdata)) {
>>             pred <- object$linear.predictors
>>             names(pred) <- names(object$residuals)
>>         }
>>         else pred <- x %*% coef + offset
>> ...
>> 
>> so that is the formula it uses.  As you did not supply 'newdata', it quotes 
>> the 'linear.predictors' component of the fit: see ?coxph.object.
>> 
>> Effectively it centred the explanatory variables on their means and then 
>> applied the linear regression formula to give the linear predictor. It is 
>> the centring that may be non-obvious: effectively h_0(t), the baseline 
>> hazard, is taken at the average of the subjects.
>> 
>
> Dear Prof. Ripley
> Thanks for replying to me email.
> I only have an other question:
>
> since h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4)
> represent the hazard at time t.
>
> In a linear prediction,
> what     Value = B1*(X1-mean(X1)) + B2*(X2-mean(X2)) + ....
> represent?

The linear predictor, as you asked for.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From slist at oomvanlieshout.net  Thu Jun  9 12:21:45 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Thu, 09 Jun 2005 12:21:45 +0200
Subject: [R] plot3d
In-Reply-To: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
References: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
Message-ID: <42A81839.1050509@oomvanlieshout.net>

Sabine,

It helps us to help you if you tell us what you want to do, preferably 
with a code example, and what system/version of R you have: type version.

On my PC I get:
 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

I can not find plot3d on my installation (updated/installed all package 
from CRAN yesterday). Quite sure you do not want to start out with a 
package that is not on CRAN. Are you sure none of the packages on CRAN 
provide the functionality you need?

Cheers,

Sander.

Navarre Sabine wrote:
> hello,
>  
> to use the function plot3d, i should use the package R.basic!
> 
> plot3d {R.basic}
> 
> If people know exactly a site to load this package, please give me the URL!
> 
> Thanks
> 
> Sabine
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From slist at oomvanlieshout.net  Thu Jun  9 12:57:26 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Thu, 09 Jun 2005 12:57:26 +0200
Subject: [R] plot3d
In-Reply-To: <42A81839.1050509@oomvanlieshout.net>
References: <20050609094832.83662.qmail@web26609.mail.ukl.yahoo.com>
	<42A81839.1050509@oomvanlieshout.net>
Message-ID: <42A82096.1050002@oomvanlieshout.net>

Now you made me curious!

Installed the package myself as well!

Maybe you can send an example graph to Romain, to be included in the 
graph gallery. Then we can all see what plot3d does!

Thanks,

Sander.


Sander Oom wrote:
> Sabine,
> 
> It helps us to help you if you tell us what you want to do, preferably 
> with a code example, and what system/version of R you have: type version.
> 
> On my PC I get:
>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> I can not find plot3d on my installation (updated/installed all package 
> from CRAN yesterday). Quite sure you do not want to start out with a 
> package that is not on CRAN. Are you sure none of the packages on CRAN 
> provide the functionality you need?
> 
> Cheers,
> 
> Sander.
> 
> Navarre Sabine wrote:
>> hello,
>>  
>> to use the function plot3d, i should use the package R.basic!
>>
>> plot3d {R.basic}
>>
>> If people know exactly a site to load this package, please give me the 
>> URL!
>>
>> Thanks
>>
>> Sabine
>>
>>
>>        
>> ---------------------------------
>>
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
> 
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From mik-smith at supanet.com  Thu Jun  9 12:57:58 2005
From: mik-smith at supanet.com (Mike J Smith)
Date: Thu, 9 Jun 2005 11:57:58 +0100
Subject: [R] krig.image help
Message-ID: <16310678487.20050609115758@kingston.ac.uk>

Hi

I have recently been experimenting with the use of kriging, primarily
through Goldensoftware's Surfer. Having had a look around at the
packages available for R I note a couple that are available so have
initially added "fields".

What I am interested in doing is batch processing ~300 datasets using
(I assume) krig.image as an exact interpolator (which I believe has no
nugget). Initially ordinary kriging with a linear model is all I am
after.

As I am not overly familiar with either R or kriging I was wondering
if anyone knew the best command line options to use?

Many thanks

mike

-- 
Dr Mike J Smith
Editor-in-Chief: Journal of Maps
http://www.journalofmaps.com

School of Earth Sciences and Geography
Kingston University
Penrhyn Road
Kingston-upon-Thames
Surrey
KT1 2EE UK
tel +44 (0)208 547 7500
fax +44 (0)208 547 7497
OS X/Y:518098/168540
Lat/Long: 51.403603/-0.303575

Personal Webpage:
http://www.kingston.ac.uk/esg/staff/smith.htm

-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: 2.6

mQCPA0AyMQwBbgEEAMDTRNYB5Zi2SbmOtnBULHWIMXFaK+MVSTqGPi66N8s0ZQaJ
bMggYe5ySdltKLgT1FFmJN48IdSzqvzwPAMD5pQEuyLadnBfFvdfJapBcvHTd1pL
h5ikc/3zUXnK/JegioA4bCPypGnhf/1HFx70GRhpSaEOiul+vK1pQRliQJz1ABEB
AAG0KU1pa2UgU21pdGggPG1pY2hhZWwuc21pdGhAa2luZ3N0b24uYWMudWs+iQCV
AwUQQDIxDK1pQRliQJz1AQHn1QQAqA+203XQM7t0GEhIUh4PFEMysT+s2jMGUkNm
eihQnyK+1ljBcG9Vt8DtzOC5s3Pop1G9nbW9vl7OYzSb4E5uQ5L5wFUz0TJ81mu6
V8u/1CsERi+eXcNre+MON+q1GYoerUSd6LTh/kj+4J+MU05zHpSMXVJQsr2wcaxJ
vDM8DHY=
=oIrO
-----END PGP PUBLIC KEY BLOCK-----



From jfox at mcmaster.ca  Thu Jun  9 13:33:50 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 9 Jun 2005 07:33:50 -0400
Subject: [R] Forecasting with macroeconomic structural equations models?
In-Reply-To: <20050609021023.39065.qmail@web50506.mail.yahoo.com>
Message-ID: <20050609113350.IEVZ27245.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Ronaldo,

As you discovered, there are no predict methods in the sem package, either
for tsls or sem objects. You might have a look at the systemfit package,
which does support predict().

More generally, you should be able to substitute out identities and base
predictions on the reduced form of the model. To do so, you'll need
forecasts for the exogenous variables in the system.

I hope that this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ronaldo Carpio
> Sent: Wednesday, June 08, 2005 9:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Forecasting with macroeconomic structural 
> equations models?
> 
> 
> Hello,
> 
> Is there a package or sample code that shows how to do ex 
> ante forecasts with a macroeconomic structural equations 
> model?  I looked at the "sem" package, which lets you 
> estimate e.g. Klein's model, but I'm not sure how to make 
> simulations using the full set of equations, including the identities.
> 
> 
> Thank you,
> 
> Ronaldo Carpio
> rncarpio at yahoo.com
>



From RRoa at fisheries.gov.fk  Thu Jun  9 12:12:44 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Thu, 9 Jun 2005 08:12:44 -0200
Subject: [R] krig.image help
Message-ID: <03DCBBA079F2324786E8715BE538968A068D3A@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Mike J Smith
> Sent: 09 June 2005 09:58
> To: r-help at stat.math.ethz.ch
> Subject: [R] krig.image help
> 
> 
> Hi
> 
> I have recently been experimenting with the use of kriging, primarily
> through Goldensoftware's Surfer. Having had a look around at the
> packages available for R I note a couple that are available so have
> initially added "fields".
> 
> What I am interested in doing is batch processing ~300 datasets using
> (I assume) krig.image as an exact interpolator (which I believe has no
> nugget). Initially ordinary kriging with a linear model is all I am
> after.
> 
> As I am not overly familiar with either R or kriging I was wondering
> if anyone knew the best command line options to use?
> 
> Many thanks
> 
> mike

Check Paulo Ribeiro and Peter Diggle's geoR package. 
There is quite good documentation (including a detailed
introductory session). You can also join the R-sig-geo list
https://stat.ethz.ch/mailman/listinfo/r-sig-geo
and Paulo and other people will help you through.
Ruben



From navarre_sabine at yahoo.fr  Thu Jun  9 14:40:11 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Thu, 9 Jun 2005 14:40:11 +0200 (CEST)
Subject: [R] plot(corresp(data)...)
Message-ID: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/4e709d23/attachment.pl

From ggrothendieck at gmail.com  Thu Jun  9 15:04:40 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 9 Jun 2005 09:04:40 -0400
Subject: [R] plot(corresp(data)...)
In-Reply-To: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>
References: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>
Message-ID: <971536df050609060470a3c1f6@mail.gmail.com>

On 6/9/05, Navarre Sabine <navarre_sabine at yahoo.fr> wrote:
> hi,
> 
> My code:
> 
> data<-matrix(data=c(0.425,0.5,0.75,0.125,0.25,0.475,0.375,0.25,0.625,0.5,0.1,0.125,0,0.25,0.25),nrow=3,ncol=5,byrow=TRUE, dimnames=list(c("Good","Medium","Bad"),c("Content","Logistic","Trainer","Supply","User contribution")))
> 
> plot(corresp(data,nf=2),xlim=c(-1,1),ylim=c(-1,1));
> 
> The plot is illegible, 

Here is the code I have been using for plotting the columns.  It should be
relatively simple to extend this to plot the rows too. 

colplot <- function(x, names = rownames(x$cs), cex = .6, adj = 1.5, ...) {
   plot(x$cs, ...)
   text(x$cs, names, cex = cex, adj = adj)
   arrows(0,0,x$cs[,1],x$cs[,2],len=.1,col="red")
   abline(h=0, v=0)
   invisible(x$cs)
}
colplot(corresp(data, nf = 2), xlim = c(-2,2), cex = .5)



From dimitrijoe at yahoo.com.br  Thu Jun  9 15:34:58 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Thu, 9 Jun 2005 10:34:58 -0300
Subject: [R] getting more than the coefficients
Message-ID: <002701c56cf8$08f48920$1500a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/0255d5a6/attachment.pl

From jarioksa at sun3.oulu.fi  Thu Jun  9 15:35:19 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 09 Jun 2005 16:35:19 +0300
Subject: [R] plot(corresp(data)...)
In-Reply-To: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>
References: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>
Message-ID: <1118324119.19781.61.camel@biol102145.oulu.fi>

On Thu, 2005-06-09 at 14:40 +0200, Navarre Sabine wrote:
> hi,
>  
> My code:
>  
> data<-matrix(data=c(0.425,0.5,0.75,0.125,0.25,0.475,0.375,0.25,0.625,0.5,0.1,0.125,0,0.25,0.25),nrow=3,ncol=5,byrow=TRUE, dimnames=list(c("Good","Medium","Bad"),c("Content","Logistic","Trainer","Supply","User contribution")))
> 
> plot(corresp(data,nf=2),xlim=c(-1,1),ylim=c(-1,1));
> 
> The plot is illegible, I want to do this in 3d, but I don't know how!
> 
> please help me, send me source if you can!

What's wrong with the biplot function that is used in the example of
corresp help page? That is:

biplot(corresp(data,nf=2))

It seems to give a legible plot with your data (that you named data, a
bad idea). At least in my screen.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From jarioksa at sun3.oulu.fi  Thu Jun  9 15:39:04 2005
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: Thu, 09 Jun 2005 16:39:04 +0300
Subject: [R] plot(corresp(data)...)
In-Reply-To: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>
References: <20050609124011.89849.qmail@web26605.mail.ukl.yahoo.com>
Message-ID: <1118324344.19781.66.camel@biol102145.oulu.fi>

On Thu, 2005-06-09 at 14:40 +0200, Navarre Sabine wrote:

> My code:
>  
> data<-matrix(data=c(0.425,0.5,0.75,0.125,0.25,0.475,0.375,0.25,0.625,0.5,0.1,0.125,0,0.25,0.25),nrow=3,ncol=5,byrow=TRUE, dimnames=list(c("Good","Medium","Bad"),c("Content","Logistic","Trainer","Supply","User contribution")))
> 
> plot(corresp(data,nf=2),xlim=c(-1,1),ylim=c(-1,1));
> 
> The plot is illegible, I want to do this in 3d, but I don't know how!

You need more data to do that in 3d. You have three rows, and you can
get only 2d with corresp.

You can use scatterplot3d or rgl functions for 3d plotting (both in
packages with the same name) after you get data where you have more than
two axes. These figures easily get illegible, though.

cheers (again), jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From marco.zucchelli at biosci.ki.se  Thu Jun  9 15:49:11 2005
From: marco.zucchelli at biosci.ki.se (Marco Zucchelli)
Date: Thu, 9 Jun 2005 15:49:11 +0200
Subject: [R] the svDialogs package
Message-ID: <005501c56cfa$04ff6f40$ec6eed82@pizero>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/b3980a7b/attachment.pl

From tlumley at u.washington.edu  Thu Jun  9 15:50:51 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 9 Jun 2005 06:50:51 -0700 (PDT)
Subject: [R] Weibull survival modeling with covariate
In-Reply-To: <3405.219.111.149.61.1118283589.squirrel@lyra.lente.org>
References: <530728156.1118263161@Lab26.DOMAIN.IE.PITT.EDU>
	<3405.219.111.149.61.1118283589.squirrel@lyra.lente.org>
Message-ID: <Pine.A41.4.61b.0506090649480.48932@homer03.u.washington.edu>

On Thu, 9 Jun 2005, Watalu, Y. (aka Wataru) wrote:

> Hi,
>
> I'm also wondering which expression the survreg() uses
> for Weibull regression.  Referring to help(survreg) and
> help(survreg.distributions), I guess survreg() fits the
> following model.
>
> survreg() uses a different parametrization, say
>   F(x, Wshape, Wscale) = 1-exp(-Wscale*(x^Wshape))),
> and fits a parametric model with these formulas.
>       Wshape  = 1/"Scale"  (calculated by survreg())
>   log(Wscale) = model with covariates
>
> Is it correct?
>

Yes.  survreg() fits location-scale families to censored data, so the 
Weibull has to be parametrized as a location-scale family as described 
above.

 	-thomas


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Thu Jun  9 15:57:49 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 9 Jun 2005 06:57:49 -0700 (PDT)
Subject: [R] Prediction in Cox Proportional-Hazard Regression
In-Reply-To: <20050609121346.jpvedwkg004okso4@lnxm.bo.infn.it>
References: <20050609103741.vtkai819wc8o0gos@lnxm.bo.infn.it>
	<Pine.LNX.4.61.0506091005520.15507@gannet.stats>
	<20050609121346.jpvedwkg004okso4@lnxm.bo.infn.it>
Message-ID: <Pine.A41.4.61b.0506090653390.48932@homer03.u.washington.edu>

On Thu, 9 Jun 2005 Giuseppe.Palermo at bo.infn.it wrote:

> I only have an other question:
>
> since h(t) = h0(t) exp(B1*X1+ B2*X2 + B3*X3 + B4*X4)
> represent the hazard at time t.
>
> In a linear prediction,
> what     Value = B1*(X1-mean(X1)) + B2*(X2-mean(X2)) + ....
> represent?
>

coxph() parametrizes the model so that

     h(t)=h_0(t)exp(B1(X1-mean(X1))+B2(X2-mean(X2))

as Brian pointed out.  This doesn't affect the coefficients B1, B2,..., it 
just redefines h_0 to be the hazard at mean covariates rather than at zero 
covariates.

The reason is that this makes h_0(t) more likely to be a useful thing to 
estimate. For example, if one covariate is age then extrapolating the 
baseline hazard to age zero is numerically unreliable and not very 
interesting.

 	-thomas



From lisawang at uhnres.utoronto.ca  Thu Jun  9 16:10:02 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Thu, 09 Jun 2005 10:10:02 -0400
Subject: [R] question on boot
Message-ID: <42A84DBA.7090900@uhnres.utoronto.ca>

Hello there:

Dear Dr. Murdoch,

I'm a statistician at Princess Margaret Hospital. Could you please help 
me with the bootstrapping?

Can boot function (along with boot.ci) handle multivariate statistics?  
My intention is that I want to group the outcome data and evaluate the 
proportion of patients who are positive for a marker within each group.  
(i.e. I have two variables, a predictor and an outcome.  The predictor 
is continuous and the outcome is binary.  I want a statistic which 
groups patients based on the predictor variable (0-10, 10-20, 20-30, 
etc;) and check the proportion of patients who are positive for the 
outcome variable within each group).

Regards

Lisa Wang Msc.
Princess Margaret Hospital
Toronto, Ca
tel 416 9464501 ext. 5201



From davidr at rhotrading.com  Thu Jun  9 16:53:02 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Thu, 9 Jun 2005 09:53:02 -0500
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FB672@rhosvr02.rhotrading.com>

Here is a link to see what's possible. 
http://www.combinatorics.org/Surveys/ds5/VennSymmEJC.html

Venn did 4 sets with ellipses (see wikipedia.)

(There was an article in the last year (I think) in one of my math
journals that presented someone who made these complex Venn diagrams as
artwork and sold, but I can't remember the person's name right now.)

HTH
David L. Reiner

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tan Hui Hui Jenny
Sent: Wednesday, June 08, 2005 7:16 PM
To: r-help at stat.math.ethz.ch
Subject: [R] How to plot more than 3 sets in Venn Diagrams?

I'm trying to plot Venn diagrams with more than 3 sets (5 actually) in
order to describe graphically the genetic variation between populations.
 
I tried the limma library but realised it can only plot 3 sets.
 
Is there any solution? Of course I could plot the chart manually but
it'll take too long (have other datasets)..... One of my dataset is
given below. 
 
THanks for any advice. 
 
j
 
	AA	 CH	 EA	 IN	 MY	
[1,]	 0	 0	 0	 1	 0	
[2,]	 1	 0	 0	 0	 0	
[3,]	 1	 0	 0	 0	 0	
[4,]	 0	 0	 0	 0	 1	
[5,]	 1	 0	 0	 0	 0	
[6,]	 1	 0	 0	 0	 0	
[7,]	 1	 0	 0	 0	 0	
[8,]	 1	 0	 0	 1	 0	
[9,]	 1	 0	 0	 0	 0	
[10,]	 1	 0	 0	 0	 0	
[11,]	 1	 0	 0	 0	 0	
[12,]	 0	 0	 0	 0	 1	
[13,]	 1	 0	 0	 0	 0	
[14,]	 0	 0	 1	 0	 0	
[15,]	 1	 0	 0	 0	 0	
[16,]	 0	 0	 1	 0	 0	
[17,]	 1	 0	 0	 0	 0	
[18,]	 0	 0	 0	 1	 0	
[19,]	 1	 0	 0	 1	 0	
[20,]	 0	 0	 1	 0	 0	
[21,]	 0	 1	 0	 0	 0	
[22,]	 1	 0	 0	 0	 0	
[23,]	 0	 0	 1	 0	 0	
[24,]	 1	 0	 1	 1	 1	
[25,]	 0	 1	 0	 0	 0	
[26,]	 1	 0	 0	 0	 0	
[27,]	 1	 0	 0	 0	 0	
[28,]	 0	 0	 0	 1	 0	
[29,]	 0	 0	 0	 1	 0	
[30,]	 1	 0	 0	 0	 0	
[31,]	 1	 0	 0	 0	 0	
[32,]	 0	 0	 0	 1	 0	
[33,]	 0	 0	 0	 1	 0	
[34,]	 0	 1	 0	 0	 0	
[35,]	 1	 0	 0	 0	 0	
[36,]	 0	 0	 0	 1	 0	
[37,]	 0	 0	 1	 1	 0	
[38,]	 1	 0	 0	 1	 0	
[39,]	 0	 0	 0	 1	 0	
[40,]	 0	 0	 0	 1	 0	
[41,]	 0	 1	 0	 0	 0	
[42,]	 1	 0	 0	 0	 0	
[43,]	 0	 0	 0	 1	 0	
[44,]	 0	 0	 1	 0	 0	
[45,]	 1	 0	 0	 0	 0	
[46,]	 1	 0	 0	 0	 0	
[47,]	 0	 0	 0	 1	 0	
[48,]	 1	 0	 0	 0	 0	
[49,]	 0	 0	 0	 1	 0	
[50,]	 0	 0	 1	 0	 0	
[51,]	 0	 0	 0	 1	 0	
[52,]	 1	 0	 0	 0	 0	
[53,]	 0	 0	 0	 1	 0	
[54,]	 1	 0	 0	 0	 0	
[55,]	 0	 1	 0	 0	 0

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ernesto at ipimar.pt  Thu Jun  9 17:15:15 2005
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 09 Jun 2005 16:15:15 +0100
Subject: [R] Using transform on spproj package.
Message-ID: <42A85D03.9030304@ipimar.pt>

Hi,

I'm trying to use transform my mercator locations into utm but I'm doing 
something wrong because only x is transformed ... see

 > xy.sp
SpatialPoints:
        loni  lati
  [1,] -8.85 38.16
  [2,] -9.19 37.99
  [3,] -9.11 37.97
  [4,] -9.06 38.15
  [5,] -9.03 37.87
  [6,] -9.14 37.81
  [7,] -9.09 37.70
  [8,] -8.95 37.45
  [9,] -9.17 37.37
[10,] -9.00 37.34
Coordinate Reference System (CRS) arguments: +proj=merc +datum=WGS84
 > transform(xy.sp, CRS("+proj=utm +zone=29"))
SpatialPoints:
          loni     lati
  [1,] 1505638 38.62347
  [2,] 1505638 38.45141
  [3,] 1505638 38.43116
  [4,] 1505638 38.61335
  [5,] 1505638 38.32995
  [6,] 1505638 38.26922
  [7,] 1505638 38.15788
  [8,] 1505638 37.90485
  [9,] 1505638 37.82388
[10,] 1505638 37.79351
Coordinate Reference System (CRS) arguments:  +proj=utm +zone=29 
+ellps=WGS84

What am I doing wrong ?

Thanks

EJ



From 0034058 at fudan.edu.cn  Thu Jun  9 18:21:08 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 10 Jun 2005 00:21:08 +0800
Subject: [R] can nlme do the complex multilevel model?
Message-ID: <0IHT003JORLUH4@mail.fudan.edu.cn>

data from multilevel units,first sample the class ,and then the student in calss.following  is the 2-level model. and the level-1 model deals with the student,and the level-2 model deals with the class level the students belong to.

Level-1 Model

	Y = B0 + B1*(ZLEAD) + B2*(ZBUL) + B3*(ZSHY) + R

Level-2 Model
	B0 = G00 + U0
	B1 = G10 + G11*(ZWARMT) + U1
	B2 = G20 + G21*(ZWARMT) + G22*(ZABLET) + U2
	B3 = G30 + G31*(ZWARMT) + G32*(ZSHYT1) + U3

 				
i have seen the ?lme in nlme,but still have no ideal how to do.

ps:i know how to do if the Level-2 Model are:

	B0 = G00 +  U0
	B1 = G10 +  U1
	B2 = G20 +  U2
	B3 = G30 +  U3



2005-06-10

------
Deparment of Sociology
Fudan University

Blog:www.sociology.yculblog.com



From kevinvol2002 at yahoo.com  Thu Jun  9 18:26:23 2005
From: kevinvol2002 at yahoo.com (Hai Lin)
Date: Thu, 9 Jun 2005 09:26:23 -0700 (PDT)
Subject: [R] ylab on secondary y axis
In-Reply-To: <x2fyxu7tmb.fsf@turmalin.kubism.ku.dk>
Message-ID: <20050609162623.13799.qmail@web32401.mail.mud.yahoo.com>

Dear R users:

I am trying to add a label on axis(4) but I don't find
any reference in R archives. Could you please help me
out?

Thanks in advance.

The followings are the data set and what I have done:

Sample data:

time	 y1 y2
2	9 24.396
4	7.667 19.082	
10	4.667 73.984

Commands:
plot(time,y1,type='l',col='red',ylab="Relative
levels", xlab="Age of Mice",ylim=c(0,20))
points(time,y1 col="red", pch=19)

op <-par(new=T)
plot(time,y2,type='l',
col='green',axes=F,xlab="",ylab="", ylim=c(0,80))
points(time,y2,col='green',pch=24)
axis(4)



From sundar.dorai-raj at pdf.com  Thu Jun  9 18:29:10 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 09 Jun 2005 11:29:10 -0500
Subject: [R] ylab on secondary y axis
In-Reply-To: <20050609162623.13799.qmail@web32401.mail.mud.yahoo.com>
References: <20050609162623.13799.qmail@web32401.mail.mud.yahoo.com>
Message-ID: <42A86E56.1040509@pdf.com>

See ?mtext.

--sundar

Hai Lin wrote:
> Dear R users:
> 
> I am trying to add a label on axis(4) but I don't find
> any reference in R archives. Could you please help me
> out?
> 
> Thanks in advance.
> 
> The followings are the data set and what I have done:
> 
> Sample data:
> 
> time	 y1 y2
> 2	9 24.396
> 4	7.667 19.082	
> 10	4.667 73.984
> 
> Commands:
> plot(time,y1,type='l',col='red',ylab="Relative
> levels", xlab="Age of Mice",ylim=c(0,20))
> points(time,y1 col="red", pch=19)
> 
> op <-par(new=T)
> plot(time,y2,type='l',
> col='green',axes=F,xlab="",ylab="", ylim=c(0,80))
> points(time,y2,col='green',pch=24)
> axis(4)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gchappi at gmail.com  Thu Jun  9 18:35:21 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Thu, 9 Jun 2005 18:35:21 +0200
Subject: [R] dir() and RegEx and gsub()
Message-ID: <47fce06505060909357097319c@mail.gmail.com>

Dear R-Users,

I have two questions:

a)
in a directory there are 3 files:
[1] "Data.~csv"            "Kopie von Data.~csv"  "VorlageTradefile.csv"

The command "dir( fold, pattern = "\.csv" )" gives back *all* the 3 files 
With dir( fold, pattern = "\\.csv" ) I get back only VorlageTradefile.csv. 
I don't understand this behaviour, IMHO the regex expression "\.csv"
becomes the string ".csv" and "\\.csv" becomes "\.csv". So the first
string should catch it. This is also consistent with the result when I
tried with the TRegExpr Tool. Could somebody explain what's going on
here?

b)
I need to handle a copied windows file path. This is certainly often
asked but I didn't find a solution.
How can I convert, e.g.

myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
in either:

myfile
[1]  "D:\\UebungenNDK\\DataMining\\DataMiningSeries.r"

or:
myfile
[1]  "D:/UebungenNDK/DataMining/DataMiningSeries.r"

Would be great to hear about a possibility!

A nice evening to everybody,
Hans-Peter



From MSchwartz at mn.rr.com  Thu Jun  9 18:48:44 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 09 Jun 2005 11:48:44 -0500
Subject: [R] ylab on secondary y axis
In-Reply-To: <20050609162623.13799.qmail@web32401.mail.mud.yahoo.com>
References: <20050609162623.13799.qmail@web32401.mail.mud.yahoo.com>
Message-ID: <1118335725.31477.33.camel@localhost.localdomain>

On Thu, 2005-06-09 at 09:26 -0700, Hai Lin wrote:
> Dear R users:
> 
> I am trying to add a label on axis(4) but I don't find
> any reference in R archives. Could you please help me
> out?
> 
> Thanks in advance.
> 
> The followings are the data set and what I have done:
> 
> Sample data:
> 
> time	 y1 y2
> 2	9 24.396
> 4	7.667 19.082	
> 10	4.667 73.984
> 
> Commands:
> plot(time,y1,type='l',col='red',ylab="Relative
> levels", xlab="Age of Mice",ylim=c(0,20))
> points(time,y1 col="red", pch=19)
> 
> op <-par(new=T)
> plot(time,y2,type='l',
> col='green',axes=F,xlab="",ylab="", ylim=c(0,80))
> points(time,y2,col='green',pch=24)
> axis(4)

There are two options:

First, use:

par(mar = c(5, 4, 4, 5))

before your first plot to increase the margin on the right hand side for
the second Y axis label. You can further adjust these as you may
require. See ?par for more information.


Then, after the second plot:

1. Use mtext() as follows:

  mtext(4, text = "This is Y Axis 2", line = 3)

Note however that the text is facing outward. Adjust the 'line' argument
to move the text in or out as you need.



2. Use text() as follows:

  text(11.5, 40, "This is Y Axis 2", srt = 270, xpd = TRUE)

This enables you to place the text outside the plot region (xpd = TRUE)
and rotate it so that it is facing inward. Adjust the x and y positions
as you need.

See ?mtext and ?text for more information.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Thu Jun  9 19:03:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Jun 2005 18:03:22 +0100 (BST)
Subject: [R] dir() and RegEx and gsub()
In-Reply-To: <47fce06505060909357097319c@mail.gmail.com>
Message-ID: <Pine.GSO.4.31.0506091754180.18383-100000@toucan.stats>

On Thu, 9 Jun 2005, Hans-Peter wrote:

> Dear R-Users,
>
> I have two questions:
>
> a)
> in a directory there are 3 files:
> [1] "Data.~csv"            "Kopie von Data.~csv"  "VorlageTradefile.csv"
>
> The command "dir( fold, pattern = "\.csv" )" gives back *all* the 3 files
> With dir( fold, pattern = "\\.csv" ) I get back only VorlageTradefile.csv.
> I don't understand this behaviour, IMHO the regex expression "\.csv"
> becomes the string ".csv" and "\\.csv" becomes "\.csv". So the first
> string should catch it.

Catch what?  What do you actually want (you have not told us).

> This is also consistent with the result when I tried with the TRegExpr
> Tool. Could somebody explain what's going on here?

See the FAQ Q7.8: you need to double the backslashes.
This is _also_ mentioned in ?regexp.

I think you probably really intended dir( fold, pattern = "\\.csv$" )

> cat("\\.csv$", "\n")
\.csv$

may help illuminate the misconception.

> b)
> I need to handle a copied windows file path. This is certainly often
> asked but I didn't find a solution.

It is so often asked it really is a FAQ.

> How can I convert, e.g.
>
> myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"

I am sure that's not what you intended.  It has to be written as

myfile <- "D:\\UebungenNDK\\DataMining\\DataMiningSeries.r"

> in either:
>
> myfile
> [1]  "D:\\UebungenNDK\\DataMining\\DataMiningSeries.r"
>
> or:
> myfile
> [1]  "D:/UebungenNDK/DataMining/DataMiningSeries.r"
>
> Would be great to hear about a possibility!

It's all over the R code.  E.g.

gsub("\\", "/", myfile, fixed = TRUE)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From carsten.steinhoff at stud.uni-goettingen.de  Thu Jun  9 19:05:54 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Thu, 9 Jun 2005 19:05:54 +0200
Subject: [R] position of a legend-object
Message-ID: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/ec97a2ab/attachment.pl

From sarah.goslee at gmail.com  Thu Jun  9 19:09:00 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 9 Jun 2005 10:09:00 -0700
Subject: [R] Re: dir() and RegEx and gsub()
In-Reply-To: <47fce06505060909357097319c@mail.gmail.com>
References: <47fce06505060909357097319c@mail.gmail.com>
Message-ID: <efb536d5050609100968160e25@mail.gmail.com>

Hi,

> The command "dir( fold, pattern = "\.csv" )" gives back *all* the 3 files 
> With dir( fold, pattern = "\\.csv" ) I get back only VorlageTradefile.csv. 
> I don't understand this behaviour, IMHO the regex expression "\.csv"
> becomes the string ".csv" and "\\.csv" becomes "\.csv". So the first
> string should catch it. This is also consistent with the result when I
> tried with the TRegExpr Tool. Could somebody explain what's going on
> here?

Under R, for reasons I've never quite understood,

"\\." evaluates to . 
(the double backslash is needed to match a single period)
while "\." is the single-character wild card (which seems to be the same as ".")

This is explained in ?strsplit but not in the help for other commands
that use regex.

> b)
> I need to handle a copied windows file path. This is certainly often
> asked but I didn't find a solution.

If you mean you want to change the "\" to either "\\" or "/" I'm
really not sure.
On my linux system, it doesn't seem possible (which I'm sure is wrong). I'd
try sub() or at worst a combination of strsplit() and paste(), except that:

> test <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
> test
[1] "D:UebungenNDKDataMiningDataMiningSeries.r"

Regular expressions under R have always perplexed me just a bit. When
I've run into problems of this sort, I've always just processed the
strings in vim or similar, rather
than fight with R. I'm sure someone here understands them - hopeully
we will both
be enlightened.

Sarah

-- 
Sarah Goslee 
http://www.stringpage.com



From spencer.graves at pdf.com  Thu Jun  9 19:15:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 09 Jun 2005 10:15:38 -0700
Subject: [R] can nlme do the complex multilevel model?
In-Reply-To: <0IHT003JORLUH4@mail.fudan.edu.cn>
References: <0IHT003JORLUH4@mail.fudan.edu.cn>
Message-ID: <42A8793A.7030709@pdf.com>

	  The documentation for "lme" includes the following:
####################################
     lme(fixed, data, random, correlation, weights, subset, method,
         na.action, control, contrasts = NULL)
...
Arguments:
...
 random: optionally, any of the following: (i) a one-sided formula of
          the form '~x1+...+xn | g1/.../gm', with 'x1+...+xn'
          specifying the model for the random effects and 'g1/.../gm'
          the grouping structure
#####################################
	  If you can't figure it out from this and the brief examples in the
help file, I have three other suggestions:

	  (1) Have you considered "lmer" in package "lme4"?  This function is
newer, the specification of random effects is different, and it may or
may not be easier to figure out.

	  (2) Have you seen Pinheiro and Bates (2000) Mixed-Effects Models in S
and S-PLUS (Springer)?  This is the definitive documentation on
library(nlme).  I've learned a lot from this book, and it might help you
even if you use "lme4".

	  (3) If you still can't figure it out, PLEASE do read the posting
guide! "http://www.R-project.org/posting-guide.html".  Many people are
able to answer their own questions in the course of preparing a question
following this guide.  If they still submit a question, I believe they
are more likely to get a useful reply.  In particular, if you submit
another question on this, please try to include a self-contained toy
example illustrating something you've tried that someone can copy from
your email, paste it into R, and test a few minor changes in a matter of
seconds.

	  spencer graves

ronggui wrote:
> data from multilevel units,first sample the class ,and then the student in calss.following  is the 2-level model. and the level-1 model deals with the student,and the level-2 model deals with the class level the students belong to.
> 
> Level-1 Model
> 
> 	Y = B0 + B1*(ZLEAD) + B2*(ZBUL) + B3*(ZSHY) + R
> 
> Level-2 Model
> 	B0 = G00 + U0
> 	B1 = G10 + G11*(ZWARMT) + U1
> 	B2 = G20 + G21*(ZWARMT) + G22*(ZABLET) + U2
> 	B3 = G30 + G31*(ZWARMT) + G32*(ZSHYT1) + U3
> 
>  				
> i have seen the ?lme in nlme,but still have no ideal how to do.
> 
> ps:i know how to do if the Level-2 Model are:
> 
> 	B0 = G00 +  U0
> 	B1 = G10 +  U1
> 	B2 = G20 +  U2
> 	B3 = G30 +  U3
> 
> 
> 
> 2005-06-10
> 
> ------
> Deparment of Sociology
> Fudan University
> 
> Blog:www.sociology.yculblog.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jun  9 19:18:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 09 Jun 2005 19:18:50 +0200
Subject: [R] position of a legend-object
In-Reply-To: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
References: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
Message-ID: <42A879FA.2020905@statistik.uni-dortmund.de>

Carsten Steinhoff wrote:
> Hello,
>  
> I've written a function that plots a few functions in a diagram.
> The xlim and or ylim is not always the same, and set automatically by R.
> A legend is part of this object.
> Now the problem is: where to put the legend? Me would help a function that
> returns the limits and scaling of the axis.

See ?par, in particular par("usr")

Uwe Ligges


> Thanks for your help.
>  
> Carsten
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Thu Jun  9 19:22:46 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 09 Jun 2005 12:22:46 -0500
Subject: [R] position of a legend-object
In-Reply-To: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
References: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
Message-ID: <1118337767.31477.47.camel@localhost.localdomain>

On Thu, 2005-06-09 at 19:05 +0200, Carsten Steinhoff wrote:
> Hello,
>  
> I've written a function that plots a few functions in a diagram.
> The xlim and or ylim is not always the same, and set automatically by R.
> A legend is part of this object.
> Now the problem is: where to put the legend? Me would help a function that
> returns the limits and scaling of the axis.
>  
> Thanks for your help.
>  
> Carsten

You can explicitly set the xlim and ylim values in most plotting
functions. If your function is based upon an underlying R function, just
pass xlim and ylim as arguments from your function so that you can
"leave room" for a legend. 

See ?plot.default for an example.

Alternatively, using par("usr") will get you the ranges of the axes once
a plot is created:

> plot(1:10)

# c(x1, x2, y1, y2)
> par("usr")
[1]  0.64 10.36  0.64 10.36

See ?par for more information. Note that if you might be using log
scaling on one or both axes, the output of par("usr") needs to be
adjusted:

> plot(1:10, log = "xy")

> par("usr")
[1] -0.04  1.04 -0.04  1.04

# Use this correction for both axes in this case
# or just:
# 10 ^ par("usr")[1:2] for x
# 10 ^ par("usr")[3:4] for y

> 10 ^ par("usr")
[1]  0.9120108 10.9647820  0.9120108 10.9647820


HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Thu Jun  9 19:28:17 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 09 Jun 2005 19:28:17 +0200
Subject: [R] getting more than the coefficients
In-Reply-To: <002701c56cf8$08f48920$1500a8c0@thesahajamach>
References: <002701c56cf8$08f48920$1500a8c0@thesahajamach>
Message-ID: <42A87C31.2080302@statistik.uni-dortmund.de>

Dimitri Joe wrote:

> Hi there,
> 
> I am trying to export a regression output to Latex. I am using the xtable function in the xtable library. Doing

You mean the xtable *package*.
> 
> myfit <- lm(myformula, mydata)
> 
> print.xtable(xtable(myfit), file="myfile")
> 
> 
> only returns the estimated coefficients and the correspondent standard erros, t-statiscs and p-values. But I wish to get a bit more, say, the number of observations used in the regresion, the R^2 and F statistics. Any suggestions?

You have to write your own version.
Look at xtable(), hence xtable.lm(), which itself uses 
xtable.summary.lm()...


Uwe Ligges

> Thanks,
> 
> Dimitri Szerman
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu Jun  9 19:10:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 9 Jun 2005 13:10:09 -0400
Subject: [R] dir() and RegEx and gsub()
In-Reply-To: <47fce06505060909357097319c@mail.gmail.com>
References: <47fce06505060909357097319c@mail.gmail.com>
Message-ID: <971536df050609101033e737d4@mail.gmail.com>

On 6/9/05, Hans-Peter <gchappi at gmail.com> wrote:
> Dear R-Users,
> 
> I have two questions:
> 
> a)
> in a directory there are 3 files:
> [1] "Data.~csv"            "Kopie von Data.~csv"  "VorlageTradefile.csv"
> 
> The command "dir( fold, pattern = "\.csv" )" gives back *all* the 3 files
> With dir( fold, pattern = "\\.csv" ) I get back only VorlageTradefile.csv.
> I don't understand this behaviour, IMHO the regex expression "\.csv"
> becomes the string ".csv" and "\\.csv" becomes "\.csv". So the first
> string should catch it. This is also consistent with the result when I
> tried with the TRegExpr Tool. Could somebody explain what's going on
> here?

The dot (.) is a wildcard that matches any character so .csv will 
match the ~csv since the . matches the ~.

By the way, note that

1.  "[.]csv" is one way to specify a literal dot without using backslashes
2.  you probably want "[.]csv$" so that a.csv.txt is not matched.
3. Some regular expression functions have a fixed= argument that
    causes them to regard all special characters like . and * as regular
    characters but unfortunately dir lacks that argument.

> 
> b)
> I need to handle a copied windows file path. This is certainly often
> asked but I didn't find a solution.
> How can I convert, e.g.
> 
> myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"

Variable myfile, as you have written it above, has no backslashes in it 
so there is no way way to know where they are supposed to be.  Maybe \
what you mean is that you have a variable that is _stored_ as:

D:\UebungenNDK\...etc..

In that case its already the same as myfile <- "D:\\UebungenNDK\\...etc.."
Use nchar to check how many characters are stored.

e.g.

nchar("D:\\abc")  # there are 6, not 7, characters in this string

> in either:
> 
> myfile
> [1]  "D:\\UebungenNDK\\DataMining\\DataMiningSeries.r"
> 
> or:
> myfile
> [1]  "D:/UebungenNDK/DataMining/DataMiningSeries.r"
> 
> Would be great to hear about a possibility!

You can convert backslashes to forward slashes using gsub

gsub("\\", "/", "D:\\abc", fixed = TRUE)

Note that internally Windows understands forward slashes
although many of the Windows commands do not.

In case I did not understand your question have a look at ?file.path
and also ?glob2rx in package sfsmisc.  The first one will construct
paths and the second one allows you specify wildcards using globbing
instead of regular expressions.



From 0034058 at fudan.edu.cn  Thu Jun  9 19:40:19 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 10 Jun 2005 01:40:19 +0800
Subject: [R] can nlme do the complex multilevel model?
Message-ID: <0IHT003B0V9WH4@mail.fudan.edu.cn>


>	  (2) Have you seen Pinheiro and Bates (2000) Mixed-Effects Models in S
>and S-PLUS (Springer)?  This is the definitive documentation on
>library(nlme).  I've learned a lot from this book, and it might help you
>even if you use "lme4".

i know this book and also want to read it,but the problem is i have no access to it.

>	  (3) If you still can't figure it out, PLEASE do read the posting
>guide! "http://www.R-project.org/posting-guide.html".  Many people are
>able to answer their own questions in the course of preparing a question
>following this guide.  If they still submit a question, I believe they
>are more likely to get a useful reply.  In particular, if you submit
>another question on this, please try to include a self-contained toy
>example illustrating something you've tried that someone can copy from
>your email, paste it into R, and test a few minor changes in a matter of
>seconds.

i know this guide and also want to give a reproducale example.but i do not know how to.my example is from a text book names applied multilevel data analysis.the notation is mainly from Raudenbush's Hierarchical linear model.and the book use the HLM software to fit the model(my mexample is from the book). i know how to fit the model using HLM,but i would like to fit it with R if possible as i have no HLM software(i have student version,but it has limitation).

many times,i suffer from the differences in the notation,the terminology ....and this time,face the same problem.


2005-06-10

------
Deparment of Sociology
Fudan University

Blog:www.sociology.yculblog.com



From hack at tera.org  Thu Jun  9 19:38:49 2005
From: hack at tera.org (Eric Hack)
Date: Thu, 9 Jun 2005 13:38:49 -0400
Subject: [R] lme model specification
Message-ID: <ABEC59A68A73FA4F9C93E89B308F14CE48D278@TERAS0002.tera.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/3101ea38/attachment.pl

From rvaradha at jhsph.edu  Thu Jun  9 19:52:02 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Thu, 9 Jun 2005 13:52:02 -0400
Subject: [R] position of a legend-object
In-Reply-To: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
Message-ID: <OWA-1Lf4bg5XmllgYBr00000f08@owa-1.sph.ad.jhsph.edu>

You can use locator(1) as an argument in the "legend" function to manually
position the legend, i.e. after your plot is plotted, the system will wait
for you to indicate (by clicking the mouse) where the legend should be
placed.

legend(locator(1), ...)

Hope this helps,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Carsten Steinhoff
> Sent: Thursday, June 09, 2005 1:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] position of a legend-object
> 
> Hello,
> 
> I've written a function that plots a few functions in a diagram.
> The xlim and or ylim is not always the same, and set automatically by R.
> A legend is part of this object.
> Now the problem is: where to put the legend? Me would help a function that
> returns the limits and scaling of the axis.
> 
> Thanks for your help.
> 
> Carsten
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From f.harrell at vanderbilt.edu  Thu Jun  9 20:09:17 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 09 Jun 2005 14:09:17 -0400
Subject: [R] position of a legend-object
In-Reply-To: <OWA-1Lf4bg5XmllgYBr00000f08@owa-1.sph.ad.jhsph.edu>
References: <OWA-1Lf4bg5XmllgYBr00000f08@owa-1.sph.ad.jhsph.edu>
Message-ID: <42A885CD.8020906@vanderbilt.edu>

Ravi Varadhan wrote:
> You can use locator(1) as an argument in the "legend" function to manually
> position the legend, i.e. after your plot is plotted, the system will wait
> for you to indicate (by clicking the mouse) where the legend should be
> placed.
> 
> legend(locator(1), ...)
> 
> Hope this helps,
> Ravi.
> 
> --------------------------------------------------------------------------
> Ravi Varadhan, Ph.D.
> Assistant Professor,  The Center on Aging and Health
> Division of Geriatric Medicine and Gerontology
> Johns Hopkins University
> Ph: (410) 502-2619
> Fax: (410) 614-9625
> Email:  rvaradhan at jhmi.edu
> --------------------------------------------------------------------------
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
>>bounces at stat.math.ethz.ch] On Behalf Of Carsten Steinhoff
>>Sent: Thursday, June 09, 2005 1:06 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] position of a legend-object
>>
>>Hello,
>>
>>I've written a function that plots a few functions in a diagram.
>>The xlim and or ylim is not always the same, and set automatically by R.
>>A legend is part of this object.
>>Now the problem is: where to put the legend? Me would help a function that
>>returns the limits and scaling of the axis.
>>
>>Thanks for your help.
>>
>>Carsten

There are functions in the Hmisc package to help too (especially 
labcurve and xYplot).  You can have a function put a legend in the most 
empty portion of a graph automatically with these functions.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From dmbates at gmail.com  Thu Jun  9 20:06:07 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 9 Jun 2005 13:06:07 -0500
Subject: [R] lme model specification
In-Reply-To: <ABEC59A68A73FA4F9C93E89B308F14CE48D278@TERAS0002.tera.org>
References: <ABEC59A68A73FA4F9C93E89B308F14CE48D278@TERAS0002.tera.org>
Message-ID: <40e66e0b0506091106759dd749@mail.gmail.com>

On 6/9/05, Eric Hack <hack at tera.org> wrote:
> Dear All,
> 
> 
> 
> I am trying to specify the following fixed effects model for lme:

If you have a linear fixed-effects model you should use lm, not lme.

> 
> y ~ constant1 - beta1*(x - beta2)
> 
> where y is the response, x is the independent variable, and the
> operators above are real arithmetic operations of addition, subtraction,
> and multiplication.  I realize that this model is just a
> reparameterization of y=beta0+beta1*x, but I am using this
> parameterization because I am specifically interested in confidence
> bounds for beta2.

You would need to fit that as a nonlinear model.  In reference to such
models "linear" means "linear in the parameters" and that model isn't.

 
> I have looked at the help, but the closest hint I find is the I()
> function, and that does not seem to work this way.
> 
> 
> 
> I confess that I am actually using S-plus, but there does not seem to be
> a resource like this list for S-plus.

Look for the S-news email list (http://www.biostat.wustl.edu/s-news/)



From Robert.McGehee at geodecapital.com  Thu Jun  9 20:10:22 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 9 Jun 2005 14:10:22 -0400
Subject: [R] Subassignments involving NAs in data frames
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C9465F4@MSGBOSCLB2WIN.DMN1.FMR.COM>

I'm seeing some inconsistent behavior when re-assigning values in a data
frame. The first assignment turns all of the 0s in my data frame to 2s,
the second fails to do so. 

> df1 <- data.frame(a = c(NA, 0, 3, 4))
> df2 <- data.frame(a = c(NA, 0, 0, 4))
> df1[df1 == 0] <- 2 ## Works
> df2[df2 == 0] <- 2
Error: NAs are not allowed in subscripted assignments

Checking an old news file I see this:
    o	Subassignments involving NAs and with a replacement value of
	length > 1 are now disallowed.	(They were handled
	inconsistently in R < 2.0.0, see PR#7210.)  For data frames
	they are disallowed altogether, even for logical matrix indices
	(the only case which used to work).

which leaves me to believe that the assignment for both df1 and df2
should fail ("data frame ... disallowed altogether"), however that seems
not to be the case, since the example works for df1. Also, the
vectorized version works as expected (because the replacement value has
a length of 1).

> vec1 <- c(NA, 0, 3, 4)
> vec2 <- c(NA, 0, 0, 4)
> vec1[vec1 == 0] <- 2 ## Works
> vec2[vec2 == 0] <- 2 ## Also works

Is this behavior for data frames intentional? What's the best
alternative to df1[df1 == 0] <- 2 that doesn't fail in situations such
as df2? A simple loop over columns?

Best,
Robert

Robert McGehee
Quantitative Analyst
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com



This e-mail, and any attachments hereto, are intended for us...{{dropped}}



From tlumley at u.washington.edu  Thu Jun  9 20:23:03 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 9 Jun 2005 11:23:03 -0700 (PDT)
Subject: [R] Subassignments involving NAs in data frames
In-Reply-To: <67DCA285A2D7754280D3B8E88EB548020C9465F4@MSGBOSCLB2WIN.DMN1.FMR.COM>
References: <67DCA285A2D7754280D3B8E88EB548020C9465F4@MSGBOSCLB2WIN.DMN1.FMR.COM>
Message-ID: <Pine.A41.4.61b.0506091118450.255562@homer09.u.washington.edu>

On Thu, 9 Jun 2005, McGehee, Robert wrote:

> I'm seeing some inconsistent behavior when re-assigning values in a data
> frame. The first assignment turns all of the 0s in my data frame to 2s,
> the second fails to do so.
>
>> df1 <- data.frame(a = c(NA, 0, 3, 4))
>> df2 <- data.frame(a = c(NA, 0, 0, 4))
>> df1[df1 == 0] <- 2 ## Works
>> df2[df2 == 0] <- 2
> Error: NAs are not allowed in subscripted assignments

Hmm. This looks like a bug to me.

> Checking an old news file I see this:
>    o	Subassignments involving NAs and with a replacement value of
> 	length > 1 are now disallowed.	(They were handled
> 	inconsistently in R < 2.0.0, see PR#7210.)  For data frames
> 	they are disallowed altogether, even for logical matrix indices
> 	(the only case which used to work).
>
> which leaves me to believe that the assignment for both df1 and df2
> should fail ("data frame ... disallowed altogether"), however that seems
> not to be the case, since the example works for df1.

Yes, I think the bug is that it works

> Also, the
> vectorized version works as expected (because the replacement value has
> a length of 1).
>
>> vec1 <- c(NA, 0, 3, 4)
>> vec2 <- c(NA, 0, 0, 4)
>> vec1[vec1 == 0] <- 2 ## Works
>> vec2[vec2 == 0] <- 2 ## Also works

I'm not sure that this is supposed to work, either, but it might be.

> Is this behavior for data frames intentional? What's the best
> alternative to df1[df1 == 0] <- 2 that doesn't fail in situations such
> as df2? A simple loop over columns?

df2[df2 %in% 0] is the recommended method.

 	-thomas



From murdoch at stats.uwo.ca  Thu Jun  9 20:25:57 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 09 Jun 2005 14:25:57 -0400
Subject: [R] position of a legend-object
In-Reply-To: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
References: <E1DgQTh-00020j-8L@s2.stud.uni-goettingen.de>
Message-ID: <42A889B5.3010105@stats.uwo.ca>

On 6/9/2005 1:05 PM, Carsten Steinhoff wrote:
> Hello,
>  
> I've written a function that plots a few functions in a diagram.
> The xlim and or ylim is not always the same, and set automatically by R.
> A legend is part of this object.
> Now the problem is: where to put the legend? Me would help a function that
> returns the limits and scaling of the axis.

Besides the suggestions other have sent, you might find something like

legend("topright", ...)

(which puts it at the top right of the plot) does what you want.

This keyword positioning was new in 2.1.0.

Duncan Murdoch



From hack at tera.org  Thu Jun  9 20:28:30 2005
From: hack at tera.org (Eric Hack)
Date: Thu, 9 Jun 2005 14:28:30 -0400
Subject: [R] lme model specification
Message-ID: <ABEC59A68A73FA4F9C93E89B308F14CE48D27C@TERAS0002.tera.org>

Thanks for the response.  It is actually a repeated measures study, I
just mention the fixed effects specification because I think I know the
random effect specification, i.e.:
Random = ~ 1|subject

And thanks for the tip about the nonlinear model and the S-plus list.  I
will check out nlme and the other list.

Eric

On 6/9/05, Eric Hack <hack at tera.org> wrote:
> Dear All,
> 
> 
> 
> I am trying to specify the following fixed effects model for lme:

If you have a linear fixed-effects model you should use lm, not lme.

> 
> y ~ constant1 - beta1*(x - beta2)
> 
> where y is the response, x is the independent variable, and the
> operators above are real arithmetic operations of addition,
subtraction,
> and multiplication.  I realize that this model is just a
> reparameterization of y=beta0+beta1*x, but I am using this
> parameterization because I am specifically interested in confidence
> bounds for beta2.

You would need to fit that as a nonlinear model.  In reference to such
models "linear" means "linear in the parameters" and that model isn't.

 
> I have looked at the help, but the closest hint I find is the I()
> function, and that does not seem to work this way.
> 
> 
> 
> I confess that I am actually using S-plus, but there does not seem to
be
> a resource like this list for S-plus.

Look for the S-news email list (http://www.biostat.wustl.edu/s-news/)



From jerk_alert at hotmail.com  Thu Jun  9 20:36:58 2005
From: jerk_alert at hotmail.com (Ken Termiso)
Date: Thu, 09 Jun 2005 18:36:58 +0000
Subject: [R] Help with SOM membership
Message-ID: <BAY101-F24CD930E8E97C74B15C0F7E8FC0@phx.gbl>

Hi all,

I originally posted this to the bioconductor group, but maybe it's better 
suited to the r-help...

I'm using som() to partition samples of gene expression data into clusters. 
The point is to classify control vs. experimental cases (sample clustering). 
The original matrix was 22283 x 8. The 8 samples have 4 controls and 4 
experimentals.

I transposed the matrix so that its dim are 8 x 22283, and called that 
"allt." Using the normalize() function from som library, I scaled the data 
to have mean zero and variance 1.

allt.som <- som(allt, xdim=5, ydim=5, topol="hexa", neigh="bubble", alpha=1)
plot(allt.som)

What I cannot figure out how to do is how to determine where each sample has 
clustered, since the plot that i'm using does not include labels...I also 
tried str(allt.som) but cannot determine which attribute calls where each 
sample has gone...all I would like to know is where samples are being placed 
in the SOM grids, to make sure that the controls cluster together and exps 
cluster together. (Also I would eventually like to cluster the genes with 
SOM and also like to know which genes are clustered in which grids, which is 
the same problem as I have with the samples).

Thanks in advance,
Ken


>str(allt.som)
List of 16
$ data      : num [1:8, 1:22277] 1167 1282 1561 1398 1581 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:8] "m577con" "m577exp" "m578con" "m578exp" ...
  .. ..$ : chr [1:22277] "1007_s_at" "1053_at" "117_at" "121_at" ...
$ code      : matrix [1:25, 1:22277] 1050 1222 1411 1504 1722 ...
  ..- attr(*, "class")= chr "matrix"
$ visual    :`data.frame':     8 obs. of  3 variables:
  ..$ x     : num [1:8] 0 1 3 2 3 4 2 1
  ..$ y     : num [1:8] 1 2 0 1 3 3 2 4
  ..$ qerror: num [1:8] 6472 8396 7574 7856 6969 ...
$ qerror    : num 6e+08
$ init      : chr "linear"
$ alpha     : chr "inverse"
$ neigh     : chr "bubble"
$ topol     : chr "hexa"
$ alpha0    : num [1:2] 1 0.5
$ radius0   : num [1:2] 5 3
$ rlen      : num [1:2] 16 80
$ xdim      : num 5
$ ydim      : num 5
$ err.radius: num 1
$ inv.alp.c : num [1:2] 0.16 0.8
$ code.sum  :`data.frame':     25 obs. of  3 variables:
  ..$ x   : num [1:25] 0 1 2 3 4 0 1 2 3 4 ...
  ..$ y   : num [1:25] 0 0 0 0 0 1 1 1 1 1 ...
  ..$ nobs: int [1:25] 0 0 0 1 0 1 0 1 0 0 ...
- attr(*, "class")= chr "som"



From ross at biostat.ucsf.edu  Thu Jun  9 20:56:27 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 09 Jun 2005 11:56:27 -0700
Subject: [R] How to change the value of a class slot
In-Reply-To: <42A6E17C.5050909@hhbio.wasser.tu-dresden.de>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
	<42A6E17C.5050909@hhbio.wasser.tu-dresden.de>
Message-ID: <1118343387.2408.147.camel@iron.libaux.ucsf.edu>

On Wed, 2005-06-08 at 14:15 +0200, Thomas Petzoldt wrote:
...
> Hello Ross,
> 
> I see that your question was related to S4, but I just noticed a
> solution based on the R.oo package so I thought I would add a solution
> based on the proto package too. We had similar problems several times
> ago and (to my surprise) found R to be an extremely flexible language
> even for these things. Our favorite solution is available as
> package(proto). It requires R 2.1, because of several subtle
> improvements regarding environments, which made our implementation more
> streamlined.
> 
> Does the following example do what you intended?

Certainly looks like it.  It seems R has an embarrassment of object
systems.  R.oo, which someone else mentioned, also has the semantics I
was thinking of.  There's an interesting paper here
http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/Bengtsson.pdf
which contrasts what I was looking for, which it calls
class-object-oriented programming (COOP) with what R does, which it
calls functional-object-oriented programming (FOOP).

The paper also notes that call-by-value vs call-by-reference, which is
the root of the slot update problem, is really orthogonal to the
FOOP/COOP distinction.  It's easy to imagine FOOP with
call-by-reference.  R.oo uses references.

That paper also mentions that Omegahat has on OOP package, which
apparently is similar in spirit to R.oo while taking a more low-level
approach to the implementation.

Ross

> 
> ##=====================================================
> library(proto)
> 
> ## 1) define an object
> CompletePathMaker <- proto(
>       index = 0,
>       bumpIndex = function(., dindex = 1)
>         .$index <- .$index + as.integer(dindex)
> )
> 
> ## 2) create a child object of CompletePathMaker
> cpm <- CompletePathMaker$proto()
> 
> ## 3) set the index component to 3
> cpm$index <- 3
> 
> ## 4) iterate the index
> cpm$bumpIndex(2)
> 
> ## print the result
> cpm$index
> 
> ##=====================================================



From ross at biostat.ucsf.edu  Thu Jun  9 21:04:21 2005
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 09 Jun 2005 12:04:21 -0700
Subject: [R] How to change the value of a class slot
In-Reply-To: <42A69A93.4000404@statistik.uni-dortmund.de>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>
	<17057.57384.831362.960765@stat.math.ethz.ch>
	<20050606203517.GN3134@wheat.boylan.org>
	<42A54069.6090108@statistik.uni-dortmund.de>
	<1118170689.2410.94.camel@iron.libaux.ucsf.edu>
	<42A69A93.4000404@statistik.uni-dortmund.de>
Message-ID: <1118343861.2413.156.camel@iron.libaux.ucsf.edu>

On Wed, 2005-06-08 at 09:13 +0200, Uwe Ligges wrote:
[extensive deletions.  Discussion concerned my desire to have a function change the value
 of an object in a way that had effects outside of the function, without returning the object.]

> >>You have to think about scoping rules and it 
> >>will be clear that the approach you are expecting is not a clean one in S.
> > 
> > Could you say a bit more about that?  
> 
> I meant the following simple example (not related to any object oriented 
> programming from the S point of view, but maybe well from your point of 
> view?):
> 
> Say you want a function foo() that simply incements the argument:
> 
> a <- 1
> foo(a)
> a # now is 2
> 
> But what happens if there is (more than) one "a" (in different 
> environments), but no "a" in the environment foo(a) is called from. 
> Which "a" do you want to change in this case? Seems to be rather dangerous.
> 
> Uwe Ligges
> 
I believe your example assumes that foo is updating the outer a by
"cheating" and directly modifying enclosing environments.  (I figure it
also needs to get the name of its actual argument to do this, which
would also involve slightly dirty tricks.) This does seem to be the only
way to do it in standard R.

In contrast, I wanted something like
foo<-function(formalArg){ 
  formalArg <- new value
  return something  else
}
so that if I call foo(a), a has the new value after the call.
Unfortunately for me, that doesn't work.  Call-by-value semantics imply
that it can't.



From gerifalte28 at hotmail.com  Thu Jun  9 21:37:09 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 09 Jun 2005 19:37:09 +0000
Subject: [R] ylab on secondary y axis
In-Reply-To: <1118335725.31477.33.camel@localhost.localdomain>
Message-ID: <BAY103-F17D0A79FAD308BBD8E414BA6FC0@phx.gbl>

To add on Mark's comments, another neat option to intreractivelly place the 
label in the desired location is to use locator(), i.e:

text( locator(1),"This is Y Axis 2", srt = 270, xpd = TRUE)

Once you execute this command, left click on the desired position for the 
label in the plot and enjoy the results!

Cheers

Francisco


>From: Marc Schwartz <MSchwartz at mn.rr.com>
>Reply-To: MSchwartz at mn.rr.com
>To: Hai Lin <kevinvol2002 at yahoo.com>
>CC: R-Help <r-help at stat.math.ethz.ch>
>Subject: Re: [R] ylab on secondary y axis
>Date: Thu, 09 Jun 2005 11:48:44 -0500
>
>On Thu, 2005-06-09 at 09:26 -0700, Hai Lin wrote:
> > Dear R users:
> >
> > I am trying to add a label on axis(4) but I don't find
> > any reference in R archives. Could you please help me
> > out?
> >
> > Thanks in advance.
> >
> > The followings are the data set and what I have done:
> >
> > Sample data:
> >
> > time	 y1 y2
> > 2	9 24.396
> > 4	7.667 19.082
> > 10	4.667 73.984
> >
> > Commands:
> > plot(time,y1,type='l',col='red',ylab="Relative
> > levels", xlab="Age of Mice",ylim=c(0,20))
> > points(time,y1 col="red", pch=19)
> >
> > op <-par(new=T)
> > plot(time,y2,type='l',
> > col='green',axes=F,xlab="",ylab="", ylim=c(0,80))
> > points(time,y2,col='green',pch=24)
> > axis(4)
>
>There are two options:
>
>First, use:
>
>par(mar = c(5, 4, 4, 5))
>
>before your first plot to increase the margin on the right hand side for
>the second Y axis label. You can further adjust these as you may
>require. See ?par for more information.
>
>
>Then, after the second plot:
>
>1. Use mtext() as follows:
>
>   mtext(4, text = "This is Y Axis 2", line = 3)
>
>Note however that the text is facing outward. Adjust the 'line' argument
>to move the text in or out as you need.
>
>
>
>2. Use text() as follows:
>
>   text(11.5, 40, "This is Y Axis 2", srt = 270, xpd = TRUE)
>
>This enables you to place the text outside the plot region (xpd = TRUE)
>and rotate it so that it is facing inward. Adjust the x and y positions
>as you need.
>
>See ?mtext and ?text for more information.
>
>HTH,
>
>Marc Schwartz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Thu Jun  9 21:37:32 2005
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 09 Jun 2005 21:37:32 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <1118343861.2413.156.camel@iron.libaux.ucsf.edu>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>	<17057.57384.831362.960765@stat.math.ethz.ch>	<20050606203517.GN3134@wheat.boylan.org>	<42A54069.6090108@statistik.uni-dortmund.de>	<1118170689.2410.94.camel@iron.libaux.ucsf.edu>	<42A69A93.4000404@statistik.uni-dortmund.de>
	<1118343861.2413.156.camel@iron.libaux.ucsf.edu>
Message-ID: <42A89A7C.9090405@hhbio.wasser.tu-dresden.de>

Ross Boylan wrote:
> 
> I believe your example assumes that foo is updating the outer a by
> "cheating" and directly modifying enclosing environments.  (I figure it
> also needs to get the name of its actual argument to do this, which
> would also involve slightly dirty tricks.) This does seem to be the only
> way to do it in standard R.
> 
> In contrast, I wanted something like
> foo<-function(formalArg){ 
>   formalArg <- new value
>   return something  else
> }
> so that if I call foo(a), a has the new value after the call.
> Unfortunately for me, that doesn't work.  Call-by-value semantics imply
> that it can't.

What about call by reference like this:

library(proto)

arg <- proto(x=1)

foo<-function(formalArg){
   formalArg$x <- formalArg$x + 1.23
   #return something  else
}

foo(arg)

arg$x

[1] 2.23

Best regards

Thomas P.



From murdoch at stats.uwo.ca  Thu Jun  9 21:51:46 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 09 Jun 2005 15:51:46 -0400
Subject: [R] How to change the value of a class slot
In-Reply-To: <1118343861.2413.156.camel@iron.libaux.ucsf.edu>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>	<17057.57384.831362.960765@stat.math.ethz.ch>	<20050606203517.GN3134@wheat.boylan.org>	<42A54069.6090108@statistik.uni-dortmund.de>	<1118170689.2410.94.camel@iron.libaux.ucsf.edu>	<42A69A93.4000404@statistik.uni-dortmund.de>
	<1118343861.2413.156.camel@iron.libaux.ucsf.edu>
Message-ID: <42A89DD2.5050805@stats.uwo.ca>

On 6/9/2005 3:04 PM, Ross Boylan wrote:
> On Wed, 2005-06-08 at 09:13 +0200, Uwe Ligges wrote:
> [extensive deletions.  Discussion concerned my desire to have a function change the value
>  of an object in a way that had effects outside of the function, without returning the object.]
> 
>> >>You have to think about scoping rules and it 
>> >>will be clear that the approach you are expecting is not a clean one in S.
>> > 
>> > Could you say a bit more about that?  
>> 
>> I meant the following simple example (not related to any object oriented 
>> programming from the S point of view, but maybe well from your point of 
>> view?):
>> 
>> Say you want a function foo() that simply incements the argument:
>> 
>> a <- 1
>> foo(a)
>> a # now is 2
>> 
>> But what happens if there is (more than) one "a" (in different 
>> environments), but no "a" in the environment foo(a) is called from. 
>> Which "a" do you want to change in this case? Seems to be rather dangerous.
>> 
>> Uwe Ligges
>> 
> I believe your example assumes that foo is updating the outer a by
> "cheating" and directly modifying enclosing environments.  (I figure it
> also needs to get the name of its actual argument to do this, which
> would also involve slightly dirty tricks.) This does seem to be the only
> way to do it in standard R.
> 
> In contrast, I wanted something like
> foo<-function(formalArg){ 
>   formalArg <- new value
>   return something  else
> }
> so that if I call foo(a), a has the new value after the call.
> Unfortunately for me, that doesn't work.  Call-by-value semantics imply
> that it can't.

Here are some dirty ways to do it:

 > foo <- function(FormalArg, value) {
+   assign(as.character(substitute(FormalArg)), value, 
envir=sys.frame(sys.parent()))
+ }
 > x
Error: Object "x" not found
 > foo(x, 4)
 > x
[1] 4

That one does the assignment in the caller's frame, not necessarily the 
place that FormalArg was found.  If you want to change a value that you 
know exists, you could do something like this:

which.env <- function(name, envir = sys.frame(sys.parent())) {
     if (!exists(name, envir)) stop('not found')
     result <- envir
     while (!is.null(result) && !exists(name, result, inherits = FALSE))
       result <- parent.env(result)
     return(result)
}

foo <- function(FormalArg, value) {
     name <- as.character(substitute(FormalArg))
     assign(name, value, which.env(name, sys.frame(sys.parent())))
}

Duncan Murdoch



From petzoldt at rcs.urz.tu-dresden.de  Thu Jun  9 21:53:49 2005
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 09 Jun 2005 21:53:49 +0200
Subject: [R] How to change the value of a class slot
In-Reply-To: <1118343387.2408.147.camel@iron.libaux.ucsf.edu>
References: <1117843448.2408.58.camel@iron.libaux.ucsf.edu>	<42A6E17C.5050909@hhbio.wasser.tu-dresden.de>
	<1118343387.2408.147.camel@iron.libaux.ucsf.edu>
Message-ID: <42A89E4D.9010703@rcs.urz.tu-dresden.de>

Ross Boylan wrote:

> The paper also notes that call-by-value vs call-by-reference, which is
> the root of the slot update problem, is really orthogonal to the
> FOOP/COOP distinction.  It's easy to imagine FOOP with
> call-by-reference.  R.oo uses references.

Yes, it can be found on http://www.maths.lth.se/help/R/R.oo/


> That paper also mentions that Omegahat has on OOP package, which
> apparently is similar in spirit to R.oo while taking a more low-level
> approach to the implementation.
> 
> Ross

The Omegahat related article was in R News:

"Object-Oriented Programming in R" by John M. Chambers & Duncan Temple 
Lang. R News Vol. 1/3, September 2001, p 17.
http://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf


Thomas



From ripley at stats.ox.ac.uk  Thu Jun  9 22:09:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Jun 2005 21:09:53 +0100 (BST)
Subject: [R] Subassignments involving NAs in data frames
In-Reply-To: <Pine.A41.4.61b.0506091118450.255562@homer09.u.washington.edu>
References: <67DCA285A2D7754280D3B8E88EB548020C9465F4@MSGBOSCLB2WIN.DMN1.FMR.COM>
	<Pine.A41.4.61b.0506091118450.255562@homer09.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0506092011500.5702@gannet.stats>

On Thu, 9 Jun 2005, Thomas Lumley wrote:

> On Thu, 9 Jun 2005, McGehee, Robert wrote:
>
>> I'm seeing some inconsistent behavior when re-assigning values in a data
>> frame. The first assignment turns all of the 0s in my data frame to 2s,
>> the second fails to do so.

But they differ in several ways, so why is this labelled `inconsistent'?
Why not ask `what is the difference'?

The answer to the pertinent question is `the number of items to be 
replaced'.

>>> df1 <- data.frame(a = c(NA, 0, 3, 4))
>>> df2 <- data.frame(a = c(NA, 0, 0, 4))
>>> df1[df1 == 0] <- 2 ## Works
>>> df2[df2 == 0] <- 2
>> Error: NAs are not allowed in subscripted assignments
>
> Hmm. This looks like a bug to me.
>
>> Checking an old news file I see this:
>>    o	Subassignments involving NAs and with a replacement value of
>> 	length > 1 are now disallowed.	(They were handled
>> 	inconsistently in R < 2.0.0, see PR#7210.)  For data frames
>> 	they are disallowed altogether, even for logical matrix indices
>> 	(the only case which used to work).
>> 
>> which leaves me to believe that the assignment for both df1 and df2
>> should fail ("data frame ... disallowed altogether"), however that seems
>> not to be the case, since the example works for df1.
>
> Yes, I think the bug is that it works

It has since been allowed in a few cases to avoid needlessly breaking 
existing code. (The curse of back-compatibility.)

In the first example there is only one value to be replaced, so there is 
no ambiguity in the meaning. In the second the replacement has to be 
replicated to the needed length and so the rules for vectors give the 
error message.

Another case which is allowed is if none of the values are to be replaced: 
that is all the logical indices are FALSE or NA.

>> Also, the
>> vectorized version works as expected (because the replacement value has
>> a length of 1).
>> 
>>> vec1 <- c(NA, 0, 3, 4)
>>> vec2 <- c(NA, 0, 0, 4)
>>> vec1[vec1 == 0] <- 2 ## Works
>>> vec2[vec2 == 0] <- 2 ## Also works
>
> I'm not sure that this is supposed to work, either, but it might be.

Reading help("[") should help alleviate your uncertainty, for this is 
explicitly documented there.

>> Is this behavior for data frames intentional? What's the best
>> alternative to df1[df1 == 0] <- 2 that doesn't fail in situations such
>> as df2? A simple loop over columns?
>
> df2[df2 %in% 0] is the recommended method.

That index is a logical vector of length one.  Try

ind <- df2 == 0
df2[ind & !is.na(ind)] <- 2

but this is really just a loop over columns implemented in [<-.data.frame.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From j.zhao at ucl.ac.uk  Thu Jun  9 22:58:30 2005
From: j.zhao at ucl.ac.uk (J. H. Zhao)
Date: Thu, 09 Jun 2005 21:58:30 +0100
Subject: [R] RE: CRAN task view for genetics
Message-ID: <6.2.1.2.0.20050609211618.0396ceb0@pop2-server.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/abe252eb/attachment.pl

From Roger.Bivand at nhh.no  Thu Jun  9 22:50:22 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 9 Jun 2005 22:50:22 +0200 (CEST)
Subject: [R] Using transform on spproj package.
In-Reply-To: <42A85D03.9030304@ipimar.pt>
Message-ID: <Pine.LNX.4.44.0506092240580.4813-100000@reclus.nhh.no>

On Thu, 9 Jun 2005, Ernesto Jardim wrote:

> Hi,
> 
> I'm trying to use transform my mercator locations into utm but I'm doing 
> something wrong because only x is transformed ... see

Two points: I think you are getting the transform you specify, from 
+proj=merc to +proj=utm with a different x origin - I suspect you meant 
+proj=latlong for the input data. If the coordinates below are somewhere 
off Cabo de Sines, they are latlong.

Second, spproj is not on CRAN, so either the R-spatial-dev mailing list on
sourceforge (where spproj and friends are living while they grow up) or
the R-sig-geo list might have been a good choice. Please consider 
continuing this on R-sig-geo.

Best wishes,

Roger

> 
>  > xy.sp
> SpatialPoints:
>         loni  lati
>   [1,] -8.85 38.16
>   [2,] -9.19 37.99
>   [3,] -9.11 37.97
>   [4,] -9.06 38.15
>   [5,] -9.03 37.87
>   [6,] -9.14 37.81
>   [7,] -9.09 37.70
>   [8,] -8.95 37.45
>   [9,] -9.17 37.37
> [10,] -9.00 37.34
> Coordinate Reference System (CRS) arguments: +proj=merc +datum=WGS84
>  > transform(xy.sp, CRS("+proj=utm +zone=29"))
> SpatialPoints:
>           loni     lati
>   [1,] 1505638 38.62347
>   [2,] 1505638 38.45141
>   [3,] 1505638 38.43116
>   [4,] 1505638 38.61335
>   [5,] 1505638 38.32995
>   [6,] 1505638 38.26922
>   [7,] 1505638 38.15788
>   [8,] 1505638 37.90485
>   [9,] 1505638 37.82388
> [10,] 1505638 37.79351
> Coordinate Reference System (CRS) arguments:  +proj=utm +zone=29 
> +ellps=WGS84
> 
> What am I doing wrong ?
> 
> Thanks
> 
> EJ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From mike.rstat at gmail.com  Thu Jun  9 23:09:30 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 14:09:30 -0700
Subject: [R] Re: Feedback requested on a solution to specifying R-scripts on
	the command line
In-Reply-To: <27db823f0506081617281bc7c7@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E952@usctmx1106.merck.com>
	<27db823f050607193776b2cf4a@mail.gmail.com>
	<1118199932.13958.4.camel@localhost.localdomain>
	<27db823f05060720394f26a39c@mail.gmail.com>
	<27db823f0506072133201d554@mail.gmail.com>
	<1118238995.28986.0.camel@localhost.localdomain>
	<27db823f050608113241ad08eb@mail.gmail.com>
	<27db823f0506081617281bc7c7@mail.gmail.com>
Message-ID: <27db823f0506091409b9e2ffc@mail.gmail.com>

On 6/8/05, Mike R <mike.rstat at gmail.com> wrote:
<snip>
> From the command line, I'd like to be able to start an
> interactive R session and at the same time, be able
> to specify on the command line a project-specific or
> task-specific script (R code) that is to be executed
> at the beginning of the session.

My new version of .First:


.First <- function() 
{ 
 print(version)
 cat("\n")
 CmdLineFilenames <<- strsplit(Sys.getenv("R_CUSTOM")," ")[[1]]
 for ( fn in CmdLineFilenames )
 {
  cat( "sourcing ", fn, "\n" )
  source(fn)
 }
 cat("\n")
}


Best,
Mike



From mike.rstat at gmail.com  Fri Jun 10 00:31:53 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:31:53 -0700
Subject: [R] test of thread behaviour - no need to read this
Message-ID: <27db823f0506091531469a11a2@mail.gmail.com>

test of thread behaviour 

this is the initial post



From mike.rstat at gmail.com  Fri Jun 10 00:34:49 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:34:49 -0700
Subject: [R] test of thread behaviour - no need to read this
In-Reply-To: <27db823f0506091531469a11a2@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
Message-ID: <27db823f050609153463278042@mail.gmail.com>

this is the first reply to the initial thread
subject line and header unchanged



From mike.rstat at gmail.com  Fri Jun 10 00:35:48 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:35:48 -0700
Subject: [R] test of thread behaviour - no need to read this -- version2
In-Reply-To: <27db823f0506091531469a11a2@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
Message-ID: <27db823f05060915353f6eb6c8@mail.gmail.com>

this is the second reply to the initial thread
subject line has been changed



From mike.rstat at gmail.com  Fri Jun 10 00:36:46 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:36:46 -0700
Subject: [R] another thread behaviour test -- no need to read this
In-Reply-To: <27db823f0506091531469a11a2@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
Message-ID: <27db823f05060915362b1d7eff@mail.gmail.com>

this is the third reply to the initial post

subject line has been changed



From mike.rstat at gmail.com  Fri Jun 10 00:38:16 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:38:16 -0700
Subject: [R] test of thread behaviour - no need to read this -- version 3
In-Reply-To: <27db823f0506091531469a11a2@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
Message-ID: <27db823f0506091538521f1b5b@mail.gmail.com>

this is the fourth reply to the initial post

subject line edited ( Re was removed )



From mike.rstat at gmail.com  Fri Jun 10 00:44:18 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:44:18 -0700
Subject: [R] Re: test of thread behaviour - no need to read this
In-Reply-To: <27db823f0506091531469a11a2@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
Message-ID: <27db823f05060915447f24b617@mail.gmail.com>

this is fifth and final (?) reply to the initial post

if this post gets added to the original thread,
then i've found no way to initiate a new thread by 
clicking the reply button

had no idea that this behaviour occurred.  wonder 
how many archives (other email lists) are "corrupted" 
because of this behaviour?

oh well, a little wiser today i guess !

time to get back to work ......



From mike.rstat at gmail.com  Fri Jun 10 00:52:30 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 15:52:30 -0700
Subject: [R] Re: test of thread behaviour - no need to read this
In-Reply-To: <27db823f05060915447f24b617@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
	<27db823f05060915447f24b617@mail.gmail.com>
Message-ID: <27db823f0506091552499bf50d@mail.gmail.com>

okay, one last post.

gmail categorizes email by threads 

gmail's behaviour is different than this R-archive:

https://www.stat.math.ethz.ch/pipermail/r-help/2005-June

in fact, gmail has categorized the posts as i had intended

but then gmail, i think, calls them conversations, not threads

now it is really time to get back to work



From david.p.finlayson at gmail.com  Fri Jun 10 01:15:56 2005
From: david.p.finlayson at gmail.com (David Finlayson)
Date: Thu, 9 Jun 2005 16:15:56 -0700
Subject: [R] Specifying medoids in PAM?
In-Reply-To: <17063.31362.222989.262717@stat.math.ethz.ch>
References: <be6d17205060712111a263c56@mail.gmail.com>
	<17062.47922.43476.902879@stat.math.ethz.ch>
	<be6d1720506080924373ec772@mail.gmail.com>
	<17063.9107.373753.833045@stat.math.ethz.ch>
	<17063.31362.222989.262717@stat.math.ethz.ch>
Message-ID: <be6d1720506091615505ff0b9@mail.gmail.com>

Thanks for your help, that worked.

David

On 6/8/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
> >>>>>     on Wed, 8 Jun 2005 18:57:55 +0200 writes:
> 
> >>>>> "David" == David Finlayson <david.p.finlayson at gmail.com>
> >>>>>     on Wed, 8 Jun 2005 09:24:54 -0700 writes:
> 
>     David> Sorry, I wasn't trying to submit a bug report just yet.
> 
>     MM> the posting guide asks you to provide reproducible examples, in
>     MM> any case, not just for bug reports ...
>     MM> {and strictly speaking, you still haven't provided one, since
>     MM> it's a bit painful to read in your table below -- because of the
>     MM> extra row names ... but here I'm nit picking a bit }
> 
>     David> I wanted to see if I was using the command correctly.
> 
>     MM> Yes, you were.
> 
> 
>     >>> pam(stats.table, metric="euclidean", stand=TRUE, medoids=c(1,3,20,2,5), k=5)
> 
>     David> This command crashes RGUI.exe and windows sends an error report to
>     David> Microsoft. It also crashes if I first subtract the NA rows from
>     David> stats.table.
> 
>     MM> I can confirm to get segmentation faults using this example data
>     MM> with k=5 ,  so effectively, it seems you've uncovered a bug in pam().
>     MM> I will investigate and patch eventually.
> 
> I found and fixed the bug:
> Some part of the C code was assuming that the indices in
> 'medoids' were sorted (increasingly).
> 
> I.e., for the moment you can easily work around the problem by
> using
>    pam(stats.table, ...., medoids=c(1,2,3,5,20), k=5)
> instead of
>    pam(stats.table, ...., medoids=c(1,3,20,2,5), k=5)
> 
> 
> The next version of the cluster package which allows to specify
> the "fuzzyness exponent" in fanny()  will have this problem
> fixed.
> 
> Martin Maechler,
> ETH Zurich
> 


-- 
David Finlayson
Marine Geology & Geophysics
School of Oceanography
Box 357940
University of Washington
Seattle, WA  98195-7940
USA

Office: Marine Sciences Building, Room 112
Phone: (206) 616-9407
Web: http://students.washington.edu/dfinlays



From drf5n at maplepark.com  Fri Jun 10 01:30:59 2005
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 9 Jun 2005 18:30:59 -0500 (CDT)
Subject: [R] Plot/manage spatial boundary data
Message-ID: <Pine.LNX.4.58.0506091731420.21844@maplepark.com>

I have some disconnected boundary data from a finite element ocean model
and I'd like to make a plot.

Maptools looks promising, but since my data is not in a shapefile or a
map, I'm unclear on what the best way to approach the problem.

>geom[1:10,]
         lon      lat  depth
1  -75.42481 35.58192 16.172
2  -75.40726 35.58567 18.045
3  -75.41351 35.60312 17.333
4  -75.38888 35.58959 20.787
5  -75.39495 35.60706 19.834
6  -75.36964 35.59370 20.950
7  -75.37556 35.61159 20.941
8  -75.35530 35.61660 23.107
9  -75.34950 35.59800 22.960
10 -75.33418 35.62194 23.934

>island1<-c(2,3,4,2)
>water<-c(1,3,5,7,8,10)
> land<-c(1,2,4,6,9,10)
> plot(geom$lon[land],geom$lat[land],pch='.',t='l')
 lines(geom$lon[water],geom$lat[water],pch='.',t='l',col="blue")
 lines(geom$lon[island1],geom$lat[island1],pch='.',t='l',col="green")

The above is toy-sized: dim(geom) is on the order of 120000,3 and there
are about 30 different islands.  Maptools seems devoted to shapefiles,
and it is unclear how to create 'polylists'.

Is there a good way to manage and graph data defined on irregular grids?

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From mike.rstat at gmail.com  Fri Jun 10 01:56:20 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 16:56:20 -0700
Subject: [R] test of thread behaviour - no need to read this
In-Reply-To: <27db823f0506091531469a11a2@mail.gmail.com>
References: <27db823f0506091531469a11a2@mail.gmail.com>
Message-ID: <27db823f050609165634eb9d0@mail.gmail.com>

this is the seventh reply to the initial post

subject line is unaltered

I added a ficticious (i think) email to the To: field



From edgar at cs.uprm.edu  Fri Jun 10 02:30:42 2005
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Thu, 9 Jun 2005 20:30:42 -0400 (EDT)
Subject: [R] Help with SOM membership
In-Reply-To: <BAY101-F24CD930E8E97C74B15C0F7E8FC0@phx.gbl>
Message-ID: <Pine.GSO.4.33.0506092028340.15047-100000@cs.uprm.edu>

Hi,
Try allt.som$visual
Edgar Acuna
UPRM

On Thu, 9 Jun 2005, Ken Termiso wrote:

> Hi all,
>
> I originally posted this to the bioconductor group, but maybe it's better
> suited to the r-help...
>
> I'm using som() to partition samples of gene expression data into clusters.
> The point is to classify control vs. experimental cases (sample clustering).
> The original matrix was 22283 x 8. The 8 samples have 4 controls and 4
> experimentals.
>
> I transposed the matrix so that its dim are 8 x 22283, and called that
> "allt." Using the normalize() function from som library, I scaled the data
> to have mean zero and variance 1.
>
> allt.som <- som(allt, xdim=5, ydim=5, topol="hexa", neigh="bubble", alpha=1)
> plot(allt.som)
>
> What I cannot figure out how to do is how to determine where each sample has
> clustered, since the plot that i'm using does not include labels...I also
> tried str(allt.som) but cannot determine which attribute calls where each
> sample has gone...all I would like to know is where samples are being placed
> in the SOM grids, to make sure that the controls cluster together and exps
> cluster together. (Also I would eventually like to cluster the genes with
> SOM and also like to know which genes are clustered in which grids, which is
> the same problem as I have with the samples).
>
> Thanks in advance,
> Ken
>
>
> >str(allt.som)
> List of 16
> $ data      : num [1:8, 1:22277] 1167 1282 1561 1398 1581 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:8] "m577con" "m577exp" "m578con" "m578exp" ...
>   .. ..$ : chr [1:22277] "1007_s_at" "1053_at" "117_at" "121_at" ...
> $ code      : matrix [1:25, 1:22277] 1050 1222 1411 1504 1722 ...
>   ..- attr(*, "class")= chr "matrix"
> $ visual    :`data.frame':     8 obs. of  3 variables:
>   ..$ x     : num [1:8] 0 1 3 2 3 4 2 1
>   ..$ y     : num [1:8] 1 2 0 1 3 3 2 4
>   ..$ qerror: num [1:8] 6472 8396 7574 7856 6969 ...
> $ qerror    : num 6e+08
> $ init      : chr "linear"
> $ alpha     : chr "inverse"
> $ neigh     : chr "bubble"
> $ topol     : chr "hexa"
> $ alpha0    : num [1:2] 1 0.5
> $ radius0   : num [1:2] 5 3
> $ rlen      : num [1:2] 16 80
> $ xdim      : num 5
> $ ydim      : num 5
> $ err.radius: num 1
> $ inv.alp.c : num [1:2] 0.16 0.8
> $ code.sum  :`data.frame':     25 obs. of  3 variables:
>   ..$ x   : num [1:25] 0 1 2 3 4 0 1 2 3 4 ...
>   ..$ y   : num [1:25] 0 0 0 0 0 1 1 1 1 1 ...
>   ..$ nobs: int [1:25] 0 0 0 1 0 1 0 1 0 0 ...
> - attr(*, "class")= chr "som"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From obig at bcgsc.ca  Fri Jun 10 02:56:16 2005
From: obig at bcgsc.ca (Obi Griffith)
Date: Thu, 9 Jun 2005 17:56:16 -0700
Subject: [R] Top N correlations from 'cor' for very large datasets being run
	many times
Message-ID: <FAE11975C8ECF04A855E6EB9FC37625404F6DA@xchange1.phage.bcgsc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/dc2d582f/attachment.pl

From mail at bymouth.com  Fri Jun 10 03:49:49 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Fri, 10 Jun 2005 11:49:49 +1000
Subject: [R] logistic regressioin - course ornotes
Message-ID: <007f01c56d5e$b11f2950$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/e8a82ec1/attachment.pl

From helprhelp at gmail.com  Fri Jun 10 04:29:54 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 9 Jun 2005 21:29:54 -0500
Subject: [R] Rpy and RSPython
Message-ID: <cdf8178305060919294586753e@mail.gmail.com>

Hi,
I am thinking to use one of them but not sure which one is better. I
think Rpy cannot call python from R while the PRPython can in
two-directional calling. Am I right?

thanks,
-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From drf5n at maplepark.com  Fri Jun 10 05:13:05 2005
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 9 Jun 2005 22:13:05 -0500 (CDT)
Subject: [R] R Graph Gallery : categorization of the graphs
In-Reply-To: <Pine.LNX.4.58.0506071122440.21844@maplepark.com>
References: <Pine.LNX.4.21.0506071638390.14637-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.LNX.4.58.0506071122440.21844@maplepark.com>
Message-ID: <Pine.LNX.4.58.0506092202550.21844@maplepark.com>

On Tue, 7 Jun 2005, David Forrest wrote:
...
>    http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?UsePictures
>
> This page demonstrates the use of pictures and the use of a public image
> server for hosting graphics files.

Also, there's a page:
  http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?GraphGallery
which points to a few graphics galleries (R and not-R) around the web.

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From mike.rstat at gmail.com  Fri Jun 10 06:00:42 2005
From: mike.rstat at gmail.com (Mike R)
Date: Thu, 9 Jun 2005 21:00:42 -0700
Subject: [R] linux, controlling device-window placement
Message-ID: <27db823f050609210075e3c440@mail.gmail.com>

Hi All,

Is it possible to control the placement R's device windows,
such as:

>  x()

Or more generally, to passing X-resources?

>  x( geometry="500x200+0+0" )

Has anyone by chance worked out the possibilities for controlling
R windows in .Xresources or .Xdefaults ?

Thanks in advance,
Mike

===========================
> print(version)
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R



From vograno at evafunds.com  Fri Jun 10 06:56:27 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 9 Jun 2005 21:56:27 -0700
Subject: [R] Redirect console to file
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5AB4D11@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050609/4e9d9524/attachment.pl

From vograno at evafunds.com  Fri Jun 10 07:33:31 2005
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 9 Jun 2005 22:33:31 -0700
Subject: [R] Redirect console to file
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5AB4D13@phost015.EVAFUNDS.intermedia.net>

Will work, but as you mentioned there ought to be an easier way :-) 

> -----Original Message-----
> From: Mike R [mailto:mike.rstat at gmail.com] 
> Sent: Thursday, June 09, 2005 10:29 PM
> To: Vadim Ogranovich
> Subject: Re: [R] Redirect console to file
> 
> On 6/9/05, Mike R <mike.rstat at gmail.com> wrote:
> > On 6/9/05, Vadim Ogranovich <vograno at evafunds.com> wrote:
> > > Hi,
> > >
> > > Is it possible to redirect the staff that normally goes to the R 
> > > console window into a file. sink() does this for sdterr 
> and stdout. 
> > > But I need something that redirects "everything" that 
> appear in the 
> > > console window (including the top-level commands).
> > >
> > > I want to achieve the same effect as that of the 
> following command 
> > > under
> > > sh:
> > > R < foo.R &> foo.Rt
> > >
> > > only I want the redirection to be activated within the 
> foo.R script.
> > > (The reason being that the name of the file is computed inside 
> > > foo.R)
> > 
> > 
> > since foo.Rt will be an ascii file, write the name of the file to a 
> > generic name, say foo.Rt
> > 
> > then run your R session with a shell script.   after runnin R,
> > have the shell script use grep or sed to grab the specific filename 
> > from the contents of the generic file (foo.Rt). then have the shell 
> > script rename the generic file.
> > 
> > ...... that being said, there has got be an easier way !!!!
> 
> let me restate that in english (LOL)
> 
> Since the output file (foo.Rt) will be an ascii file, have 
> your R-code write the computed name of the file to the 
> generically named output file (foo.Rt) along with everything else.
> 
> Then run your R session with a shell script.   After running R,
> have the shell script use grep or sed to grab the computed 
> filename from the contents of the generic file (foo.Rt). Then 
> have the shell script rename the generic file to the computed name.
>



From phgrosjean at sciviews.org  Fri Jun 10 07:36:23 2005
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 10 Jun 2005 07:36:23 +0200
Subject: [R] the svDialogs package
In-Reply-To: <005501c56cfa$04ff6f40$ec6eed82@pizero>
References: <005501c56cfa$04ff6f40$ec6eed82@pizero>
Message-ID: <42A926D7.5000906@sciviews.org>

Hello Marco,

For the first error, the message is clear: "not implemented yet!". 
Several dialog boxes are not done yet, but the functions already exist, 
mainly as "placeholders" for future development.

Regarding the second, there was a bug in the function (corrected in 
SciViews 0.8-6 that I have just uploaded to CRAN), and also a 
misunderstanding of the its first argument: "list". This argument should 
be a charactger vector containing the list of items... but not a list! 
So, the correct code is:

 > m_list <- 1:10
 > res <- guiDlgList(m_list) # Need SciViews 0.8-6!
 > res

Note that guiDlgXXX() functions return results _invisibly_. So, you need 
to assign its result to a variable, or use something like:

 > (guiDlgList(m_list))

to see the result printed at the console.

Best,

Philippe

..............................................<??}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
  ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
  ) ) ) ) )          http://www.sciviews.org
( ( ( ( (
..............................................................

Marco Zucchelli wrote:
> Hi Philippe and R community,
> 
>  I am trying to use some functions from the svDialogs package but I get some werid errors I do not understand:
> 
> 
>>library(svDialogs)
> 
> 
>>m_list <- as.list(1:10)
> 
> 
> 
>>guiDlgDoubleList(m_list, m_list)
> 
> Error in guiDlgDoubleList(m_list, m_list) : 
>         Not yet implemented!
> 
> 
> 
>>guiDlgList(m_list)
> 
> Error in guiDlgList(m_list) : couldn't find function "guiSetFonts.tcltk"
> 
> 
> 
> 
> Am I doing anything wrong ?? Do I need some other package?
> 
> 
> 
> Marco
> 
>  
> 
> 
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From pmccask at cyllene.uwa.edu.au  Fri Jun 10 07:43:18 2005
From: pmccask at cyllene.uwa.edu.au (Pamela McCaskie)
Date: Fri, 10 Jun 2005 13:43:18 +0800
Subject: [R] Problems with corARMA
Message-ID: <42A92876.6050508@cyllene.uwa.edu.au>

Dear all
I am tryiing to fit the following lme with an ARMA correlation structure:

test <- lme(fixed=fev1f~year, random=~1|id2, data=pheno2, 
correlation=corARMA(value=0.2, form=~year|id2), na.action=na.omit)

But I get the following error message:

Error in getGroupsFormula.default(correlation, asList = TRUE) :
        "Form" argument must be a formula

I have used this same form argument with differerent correlation 
structures and it has worked fine. Does anyone know why it won't 
recognise ~year | id2 (or even ~ 1 | id2) as a formula?

Any help would be great
Pam

-- 
Pamela A McCaskie
BSc(Mathematical Sciences)(Hons)

Western Australian Institute for Medical Research
University of Western Australia
SCGH Campus
Ground Floor, B Block
QE-II Medical Centre
Hospital Avenue, Nedlands
Western Australia  6009
AUSTRALIA
Email:        pmccask at cyllene.uwa.edu.au
Phone:        +61-8-9346 1612
Mob:          0417 926 607



From renaud.lancelot at cirad.fr  Fri Jun 10 00:40:39 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Fri, 10 Jun 2005 01:40:39 +0300
Subject: [R] [R-pkgs] New package aod: Analysis of Overdispersed Data
Message-ID: <42A8C567.7020902@cirad.fr>

         Information on package 'aod'

Description:

Package:       aod
Version:       1.1-2
Date:          2005-06-08
Title:         Analysis of Overdispersed Data
Author:        Matthieu Lesnoff <matthieu.lesnoff at cirad.fr> and Renaud
                Lancelot <renaud.lancelot at cirad.fr>
Maintainer:    Renaud Lancelot <renaud.lancelot at cirad.fr>
Depends:       R (>= 2.0.0), methods, stats
Suggests:      MASS, nlme
Description:   This package provides a set of functions to analyse
                overdispersed counts or proportions. Most of the
                methods are already available elsewhere but are
                scattered in different packages. The proposed functions
                should be considered as complements to more
                sophisticated methods such as generalized estimating
                equations (GEE) or generalized linear mixed effect
                models (GLMM).

Index (edited):

betabin                 Beta-Binomial Model for Proportions
donner                  Test of Proportion Homogeneity using Donner's
                         Adjustment
icc                     Intra-Cluster Correlation
negbin                  Negative-Binomial Model for Counts
quasibin                Quasi-Likelihood Model for Proportions
quasipois               Quasi-Likelihood Model for Counts
raoscott                Test of Proportion Homogeneity using Rao and
                         Scott's Adjustment
varbin                  Mean, Variance and Confidence Interval of a
                         Proportion
wald.test               Wald Test for Model Coefficients

#################

Best regards,

Renaud

-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pidemio v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Roger.Bivand at nhh.no  Fri Jun 10 09:08:46 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 10 Jun 2005 09:08:46 +0200 (CEST)
Subject: [R] Plot/manage spatial boundary data
In-Reply-To: <Pine.LNX.4.58.0506091731420.21844@maplepark.com>
Message-ID: <Pine.LNX.4.44.0506100855290.5742-100000@reclus.nhh.no>

On Thu, 9 Jun 2005, David Forrest wrote:

> I have some disconnected boundary data from a finite element ocean model
> and I'd like to make a plot.
> 
> Maptools looks promising, but since my data is not in a shapefile or a
> map, I'm unclear on what the best way to approach the problem.

If the line segments are unconnected, you will first need to establish 
(short) lists saying which segments (in which order and direction) bound 
each polygon to be filled with colour when plotting. A package you can 
consider for "rolling your own" spatial rings is sp, which is on CRAN. 

Once you have the list describing segment membership, order and direction 
for each ring, building a SpatialRings object is not difficult. The hard 
bit is going from spaghetti line segments to the list imposing order.

Alternatively, the PBSmapping package may have suitable functions for
coersing polySet objects into rings. If your coordinates were in the 
Pacific, I'd say PBSmapping might already have what you need, but your 
example coordinates are Atlantic.

(Could I suggest moving this discussion to R-sig-geo, referenced in the 
"Spatial" Task View on CRAN (top left corner in navigation bar)?

> 
> >geom[1:10,]
>          lon      lat  depth
> 1  -75.42481 35.58192 16.172
> 2  -75.40726 35.58567 18.045
> 3  -75.41351 35.60312 17.333
> 4  -75.38888 35.58959 20.787
> 5  -75.39495 35.60706 19.834
> 6  -75.36964 35.59370 20.950
> 7  -75.37556 35.61159 20.941
> 8  -75.35530 35.61660 23.107
> 9  -75.34950 35.59800 22.960
> 10 -75.33418 35.62194 23.934
> 
> >island1<-c(2,3,4,2)
> >water<-c(1,3,5,7,8,10)
> > land<-c(1,2,4,6,9,10)
> > plot(geom$lon[land],geom$lat[land],pch='.',t='l')
>  lines(geom$lon[water],geom$lat[water],pch='.',t='l',col="blue")
>  lines(geom$lon[island1],geom$lat[island1],pch='.',t='l',col="green")
> 
> The above is toy-sized: dim(geom) is on the order of 120000,3 and there
> are about 30 different islands.  Maptools seems devoted to shapefiles,
> and it is unclear how to create 'polylists'.
> 
> Is there a good way to manage and graph data defined on irregular grids?
> 
> Dave
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From WeiQiang.Li at seagate.com  Fri Jun 10 10:26:34 2005
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Fri, 10 Jun 2005 16:26:34 +0800
Subject: [R]  Contour Plot
Message-ID: <OF66D078C0.9B387F25-ON4825701C.002D1B4D-4825701C.002E6CA6@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/fb5d43a1/attachment.pl

From jan.sabee at gmail.com  Fri Jun 10 10:32:15 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Fri, 10 Jun 2005 10:32:15 +0200
Subject: [R] How to read a row dataset one by one
Message-ID: <96507a8e050610013258257e98@mail.gmail.com>

Dear all,
How to read a row dataset one by one and then print it.

x1 x2 x3 x4 x5   y
a  b  a  c  c    M1
c  b  b  c  c    M4
c  c  a  c  c    M2
c  a  c  a  a    M2
c  c  a  a  a    M1
c  a  b  c  a    M3
c  c  a  b  c    M3
c  a  c  a  b    M2
c  c  a  b  a    M1

I need a result like
read row no 1,
[1] a  b  a  c  c    M1
read row no 2,
[1] c  b  b  c  c    M4
.
.
.
the last row,
[1] c  c  a  b  a    M1

Kind regards,
Jan Sabee



From vmuggeo at dssm.unipa.it  Fri Jun 10 10:10:02 2005
From: vmuggeo at dssm.unipa.it (vito muggeo)
Date: Fri, 10 Jun 2005 10:10:02 +0200
Subject: [R] Robustness of Segmented Regression Contributed by Muggeo
In-Reply-To: <20050608145543.79a69943.Achim.Zeileis@wu-wien.ac.at>
References: <E0466E6811B5BA46B92414C94516A7A425BC3A@apgrb1rdg-mail2.nae.ds.army.mil>
	<20050608145543.79a69943.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <42A94ADA.7050006@dssm.unipa.it>

Hi,
sorry for my delay..

In addition to valuable Achim's comments.

As Achim said, you can try different starting values to assess how the 
final solution depends on them. Then select one having the best logLik 
(or the minimum RSS).

Everybody dealing with nonlinear models knows that the logLik may be not 
concave. This is particulary true for broken-line model, so different 
starting values (psi0) sometimes can lead to different solutions 
(segmented performs "just" an iterative estimating algorithm..). This 
sensitivity depends on your data: the more clear-cut the relationship, 
the stabler the algorithm, i.e. more indipendent of psi0 the estimates 
are. Of course here, a grid-search is able to fix the problem.

Furthermore comparing the results with a visual inspection of a 
(possibly smoothed) scatterplot can lead to different location of psi. 
Usually visual location of changepoints through plots is based on local 
fitting, while segmented, in its standard usage, performs global fitting 
(on each side of the range of the explanatory variable Z). Influential 
points on extreme limits of the Z-range may influence the slopes and 
then the breakpoint location.

Hope this helps.

vito



Achim Zeileis wrote:
> On Wed, 8 Jun 2005 08:25:16 -0400  Park, Kyong H Mr. RDECOM wrote:
> 
> 
>>Hello, R users,
>>I applied segmented regression method contributed by Muggeo and got
>>different slope estimates depending on the initial break points. The
>>results are listed below and I'd like to know what is a reasonable
>>approach handling this kinds of problem. I think applying various
>>initial break points is certainly not a efficient approach. Is there
>>any other methods to deal with segmented regression? From a graph, v
>>shapes are more clear at 1.2 and 1.5 break points than 1.5 and 1.7.
>>Appreciate your help.
> 
> 
> When you keep the number of break points fixed, then there is a unique
> solution to the problem of fitting a segmented regression: the solution
> which maximizes the likelihood (or for linear models equivalently
> minimizes the RSS). Vito's segmented package gives an iterative method
> which can be shown to converge to this unique solution. If empirically
> you find different solutions with different starting values, you can
> always compare them using the RSS or log-likelihood and choose the one
> which fits better (because the other one can't be the optimal solution).
> 
> The function breakpoints() in package strucchange computes (as
> opposed to approximates) the unique solution for a fully segmented model
> instead of a broken line trend.
> 
> Another nonparametric solution using quantreg was already pointed out by
> Roger.
> 
> hth,
> Z
>  
> 
>>Result1: 
>>Initial break points are 1.2 and 1.5. The estimated break points and
>>slopes:
>>
>> Estimated Break-Point(s):
>>                 Est.      St.Err
>>Mean.Vel 1.285     0.05258
>>               1.652    0.01247  
>>              
>>               Est.          St.Err.             t value       
>>               CI(95%).l
>>CI(95%).u
>>slope1   0.4248705     0.3027957   1.403159    -0.1685982       
>>1.018339 slope2   2.3281445     0.3079903   7.559149     1.7244946    
>>   2.931794
>>slope3   9.5425516     0.7554035   12.632390     8.0619879      
>>11.023115 Adjusted R-squared: 0.9924.
>>
>>Result2:
>>Initial break points are 1.5 and 1.7. The estimated break points and
>>slopes:
>>
>>Estimated Break-Point(s):
>>                Est.       St.Err
>>Mean.Vel 1.412      0.02195
>>               1.699      0.01001
>>           
>>               Est.          St.Err.        t value           
>>               CI(95%).l
>>CI(95%).u
>>slope1  0.7300483   0.1381587    5.284129       0.4592623     
>>1.000834 slope2  3.4479466   0.2442530    14.116289     2.9692194     
>> 3.926674
>>slope3 12.5000000   1.7783840     7.028853     9.0144314     
>>15.985569
>>
>>Adjusted R-squared: 0.995. 
>>
>>
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
====================================
Vito M.R. Muggeo
Dip.to Sc Statist e Matem `Vianelli'
Universit?? di Palermo
viale delle Scienze, edificio 13
90121 Palermo - ITALY
tel: 091 6626240
fax: 091 485726/485612



From slist at oomvanlieshout.net  Fri Jun 10 10:50:06 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 10 Jun 2005 10:50:06 +0200
Subject: [R] Replacing for loop with tapply!?
Message-ID: <42A9543E.3040901@oomvanlieshout.net>

Dear all,

We have a large data set with temperature data for weather stations 
across the globe (15000 stations).

For each station, we need to calculate the number of days a certain 
temperature is exceeded.

So far we used the following S code, where mat88 is a matrix containing 
rows of 365 daily temperatures for each of 15000 weather stations:

	m <- 37
	n <- 2
	outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
	for(i in 1:nrow(mat88)) {
		# i <- 3
		row1 <- as.data.frame(df88[i,  ])
		temprow37 <- select.rows(row1, row1 > m)
		temprow39 <- select.rows(row1, row1 > m + n)
		temprow41 <- select.rows(row1, row1 > m + 2 * n)
		outmat88[i, 1] <- max(row1, na.rm = T)
		outmat88[i, 2] <- count.rows(temprow37)
		outmat88[i, 3] <- count.rows(temprow39)
		outmat88[i, 4] <- count.rows(temprow41)
	}
	outmat88

We have transferred the data to a more potent Linux box running R, but 
still hope to speed up the code.

I know a for loop should be avoided when looking for speed. I also know 
the answer is in something like tapply, but my understanding of these 
commands is still to limited to see the solution. Could someone show me 
the way!?

Thanks in advance,

Sander.
-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From leo.master at caramail.com  Fri Jun 10 10:51:27 2005
From: leo.master at caramail.com (=?ISO-8859-1?Q? L=E9o=20Tranchevent?=)
Date: Fri, 10 Jun 2005 08:51:27 +0000 (GMT)
Subject: [R] R installation
Message-ID: 170225700012150@lycos-europe.com

Hi everybody,

 I am trying to update R on my computer, and I got the following problem:

 After tar the downloaded file, when I try "./configure", I got the following error

 configure: error: --with-readline=yes (default) and headers/libs are not available

 So I try "./configure --with-readline=no", and it works (I have not try "make", but I will), but I think that now I won't have any historic file for R, is that true ?
 How can I manage to install R without this option ?

 My system is:
     Linux mandrake 9.1 with Kde 3.1
     The previous version of R was 2.0.1

 Be sure that I have red all I found about this before posting.
 Thanks in advance
    
 Leo

Protek-on: CaraMail met en oeuvre un nouveau Concept de SÅÈcuritÅÈ Globale - www.caramail.com

From Allan at STATS.uct.ac.za  Fri Jun 10 11:07:07 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Fri, 10 Jun 2005 11:07:07 +0200
Subject: [R] How to read a row dataset one by one
References: <96507a8e050610013258257e98@mail.gmail.com>
Message-ID: <42A9583B.D461F0F6@STATS.uct.ac.za>

use a loop associated with the scan function.

for (i in 1:9)
{

print(scan(file="c:/a.txt",sep="\t",skip=i,nlines=1,fill=T,quiet=T,what="raw"))
}


this works but there has to be a better solution



Jan Sabee wrote:
> 
> Dear all,
> How to read a row dataset one by one and then print it.
> 
> x1 x2 x3 x4 x5   y
> a  b  a  c  c    M1
> c  b  b  c  c    M4
> c  c  a  c  c    M2
> c  a  c  a  a    M2
> c  c  a  a  a    M1
> c  a  b  c  a    M3
> c  c  a  b  c    M3
> c  a  c  a  b    M2
> c  c  a  b  a    M1
> 
> I need a result like
> read row no 1,
> [1] a  b  a  c  c    M1
> read row no 2,
> [1] c  b  b  c  c    M4
> .
> .
> .
> the last row,
> [1] c  c  a  b  a    M1
> 
> Kind regards,
> Jan Sabee
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From dimitris.rizopoulos at med.kuleuven.be  Fri Jun 10 11:14:16 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 10 Jun 2005 11:14:16 +0200
Subject: [R] Replacing for loop with tapply!?
References: <42A9543E.3040901@oomvanlieshout.net>
Message-ID: <012901c56d9c$c6bfc6f0$0540210a@www.domain>

maybe you are looking for something along these lines:

mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
temps <- c(37, 39, 41)
#################
ind <- matrix(0, length(temps), ncol(mat))
for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
ind


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sander Oom" <slist at oomvanlieshout.net>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 10, 2005 10:50 AM
Subject: [R] Replacing for loop with tapply!?


> Dear all,
>
> We have a large data set with temperature data for weather stations
> across the globe (15000 stations).
>
> For each station, we need to calculate the number of days a certain
> temperature is exceeded.
>
> So far we used the following S code, where mat88 is a matrix 
> containing
> rows of 365 daily temperatures for each of 15000 weather stations:
>
> m <- 37
> n <- 2
> outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
> for(i in 1:nrow(mat88)) {
> # i <- 3
> row1 <- as.data.frame(df88[i,  ])
> temprow37 <- select.rows(row1, row1 > m)
> temprow39 <- select.rows(row1, row1 > m + n)
> temprow41 <- select.rows(row1, row1 > m + 2 * n)
> outmat88[i, 1] <- max(row1, na.rm = T)
> outmat88[i, 2] <- count.rows(temprow37)
> outmat88[i, 3] <- count.rows(temprow39)
> outmat88[i, 4] <- count.rows(temprow41)
> }
> outmat88
>
> We have transferred the data to a more potent Linux box running R, 
> but
> still hope to speed up the code.
>
> I know a for loop should be avoided when looking for speed. I also 
> know
> the answer is in something like tapply, but my understanding of 
> these
> commands is still to limited to see the solution. Could someone show 
> me
> the way!?
>
> Thanks in advance,
>
> Sander.
> -- 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gchappi at gmail.com  Fri Jun 10 11:16:22 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Fri, 10 Jun 2005 11:16:22 +0200
Subject: [R] dir() and RegEx and gsub()
In-Reply-To: <efb536d5050609100968160e25@mail.gmail.com>
References: <47fce06505060909357097319c@mail.gmail.com>
	<efb536d5050609100968160e25@mail.gmail.com>
Message-ID: <47fce0650506100216646ed606@mail.gmail.com>

2005/6/9, Sarah Goslee <sarah.goslee at gmail.com>:

> Under R, for reasons I've never quite understood,
> "\\." evaluates to .

Thanks to the answers of B. Ripley and Gabor I think, I understand now:

  1)  the patter-string "\\.csv$" gives the regular expression "\.csv$"
  2)  now the backslash lets the dot to be interpreted literally
      (instad of a metacharacter). As said by Gabor, an alternative is,
      to put the dot between brackets

>If you mean you want to change the "\" to either "\\" or "/" I'm
>really not sure.

Yes that's what I intended. 
Because in windwos I copy the path from the address bar of the
explorer and paste it in the R console window. Now I *have* something
like
myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
that needs to be adjusted manually ;-(

I interpret the answers

>> myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
>I am sure that's not what you intended.  It has to be written as
> [snip] [1]  "D:/UebungenNDK/DataMining/DataMiningSeries.r"

and 

>> myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
>Variable myfile, as you have written it above, has no backslashes in it
>so there is no way way to know where they are supposed to be. 

that it's not possible at all to have a character string with
backslashes in it (because they *will* act as escape characters).

- Maybe I could get along by writing an *external* function that would
give me back a proper formatted path, eg.
fold <- as.path( "D:\UebungenNDK\DataMining\DataMiningSeries.r" )
[1] "D:/UebungenNDK/DataMining/DataMiningSeries.r"

- But then the behaviour would be different from all the other strings
in R which doesn't seem to be a good idea either

> I've always just processed the strings in vim or similar, rather
than fight with R.

I do it in DOS-Batch files. Quite ugly but it works...


Regards,
Hans-Peter



From hb at maths.lth.se  Fri Jun 10 11:32:06 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 10 Jun 2005 11:32:06 +0200
Subject: [R] How to read a row dataset one by one
In-Reply-To: <42A9583B.D461F0F6@STATS.uct.ac.za>
References: <96507a8e050610013258257e98@mail.gmail.com>
	<42A9583B.D461F0F6@STATS.uct.ac.za>
Message-ID: <42A95E16.50707@maths.lth.se>

Open a connection a read line by line from that one, e.g.

myReadPrint <- function(pathname, ...) {
   con <- file(pathname, open="r")
   on.exit(close(con)) # Guarantees to close connection!

   count <- 0;
   while(TRUE) {
     line <- scan(con, sep="\t", nlines=1, fill=TRUE,
                                        quiet=TRUE, what="raw");
     # Alternatively, just...
     # line <- readLines(con, n=1)
     if (length(line) == 0)
       break;
     count <- count + 1;
     cat("read row no ", count, ",\n", sep="");
     print(line);
   }
}

See ?file for more details.

/Henrik

Clark Allan wrote:
> use a loop associated with the scan function.
> 
> for (i in 1:9)
> {
> 
> print(scan(file="c:/a.txt",sep="\t",skip=i,nlines=1,fill=T,quiet=T,what="raw"))
> }
> 
> 
> this works but there has to be a better solution
> 
> 
> 
> Jan Sabee wrote:
> 
>>Dear all,
>>How to read a row dataset one by one and then print it.
>>
>>x1 x2 x3 x4 x5   y
>>a  b  a  c  c    M1
>>c  b  b  c  c    M4
>>c  c  a  c  c    M2
>>c  a  c  a  a    M2
>>c  c  a  a  a    M1
>>c  a  b  c  a    M3
>>c  c  a  b  c    M3
>>c  a  c  a  b    M2
>>c  c  a  b  a    M1
>>
>>I need a result like
>>read row no 1,
>>[1] a  b  a  c  c    M1
>>read row no 2,
>>[1] c  b  b  c  c    M4
>>.
>>.
>>.
>>the last row,
>>[1] c  c  a  b  a    M1
>>
>>Kind regards,
>>Jan Sabee
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>------------------------------------------------------------------------
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From slist at oomvanlieshout.net  Fri Jun 10 12:10:17 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 10 Jun 2005 12:10:17 +0200
Subject: [R] Replacing for loop with tapply!?
In-Reply-To: <012901c56d9c$c6bfc6f0$0540210a@www.domain>
References: <42A9543E.3040901@oomvanlieshout.net>
	<012901c56d9c$c6bfc6f0$0540210a@www.domain>
Message-ID: <42A96709.3010406@oomvanlieshout.net>

Thanks Dimitris,

Very impressive! Much faster than before.

Thanks to new found R.basic, I can simply rotate the result with 
rotate270{R.basic}:

 > mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
 > temps <- c(37, 39, 41)
 > #################
 > #ind <- matrix(0, length(temps), ncol(mat))
 > ind <- matrix(0, 4, ncol(mat))
 > (startDate <- date())
[1] "Fri Jun 10 12:08:01 2005"
 > for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
 > ind[4, ] <- colMeans(max(mat))
Error in colMeans(max(mat)) : 'x' must be an array of at least two 
dimensions
 > (endDate <- date())
[1] "Fri Jun 10 12:08:02 2005"
 > ind <- rotate270(ind)
 > ind[1:10,]
    V4 V3 V2 V1
1   0 56 75 80
2   0 46 53 60
3   0 50 58 67
4   0 60 72 80
5   0 59 68 76
6   0 55 67 74
7   0 62 77 93
8   0 45 57 67
9   0 57 68 75
10  0 61 66 76

However, I have not managed to get the row maximum using your method? It 
should be 50 for most rows, but my first guess code gives an error!

Any suggestions?

Sander



Dimitris Rizopoulos wrote:
> maybe you are looking for something along these lines:
> 
> mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
> temps <- c(37, 39, 41)
> #################
> ind <- matrix(0, length(temps), ncol(mat))
> for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
> ind
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Sander Oom" <slist at oomvanlieshout.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, June 10, 2005 10:50 AM
> Subject: [R] Replacing for loop with tapply!?
> 
> 
>>Dear all,
>>
>>We have a large data set with temperature data for weather stations
>>across the globe (15000 stations).
>>
>>For each station, we need to calculate the number of days a certain
>>temperature is exceeded.
>>
>>So far we used the following S code, where mat88 is a matrix 
>>containing
>>rows of 365 daily temperatures for each of 15000 weather stations:
>>
>>m <- 37
>>n <- 2
>>outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
>>for(i in 1:nrow(mat88)) {
>># i <- 3
>>row1 <- as.data.frame(df88[i,  ])
>>temprow37 <- select.rows(row1, row1 > m)
>>temprow39 <- select.rows(row1, row1 > m + n)
>>temprow41 <- select.rows(row1, row1 > m + 2 * n)
>>outmat88[i, 1] <- max(row1, na.rm = T)
>>outmat88[i, 2] <- count.rows(temprow37)
>>outmat88[i, 3] <- count.rows(temprow39)
>>outmat88[i, 4] <- count.rows(temprow41)
>>}
>>outmat88
>>
>>We have transferred the data to a more potent Linux box running R, 
>>but
>>still hope to speed up the code.
>>
>>I know a for loop should be avoided when looking for speed. I also 
>>know
>>the answer is in something like tapply, but my understanding of 
>>these
>>commands is still to limited to see the solution. Could someone show 
>>me
>>the way!?
>>
>>Thanks in advance,
>>
>>Sander.
>>-- 
>>--------------------------------------------
>>Dr Sander P. Oom
>>Animal, Plant and Environmental Sciences,
>>University of the Witwatersrand
>>Private Bag 3, Wits 2050, South Africa
>>Tel (work)      +27 (0)11 717 64 04
>>Tel (home)      +27 (0)18 297 44 51
>>Fax             +27 (0)18 299 24 64
>>Email   sander at oomvanlieshout.net
>>Web     www.oomvanlieshout.net/sander
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From amsa36060 at yahoo.com  Fri Jun 10 12:13:06 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Fri, 10 Jun 2005 03:13:06 -0700 (PDT)
Subject: [R] Different results in different runs with identical parameters
	in CLARA
Message-ID: <20050610101306.20867.qmail@web60422.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/da1a0eef/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Jun 10 12:50:22 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 10 Jun 2005 12:50:22 +0200
Subject: [R] R installation
References: 170225700012150@lycos-europe.com
Message-ID: <42A9706E.1000705@statistik.uni-dortmund.de>

  LÅÈo Tranchevent wrote:

> Hi everybody,
> 
>  I am trying to update R on my computer, and I got the following problem:
> 
>  After tar the downloaded file, when I try "./configure", I got the following error
> 
>  configure: error: --with-readline=yes (default) and headers/libs are not available


Make the readline headers/libs available by installing them ...

Uwe Ligges


>  So I try "./configure --with-readline=no", and it works (I have not try "make", but I will), but I think that now I won't have any historic file for R, is that true ?
>  How can I manage to install R without this option ?
> 
>  My system is:
>      Linux mandrake 9.1 with Kde 3.1
>      The previous version of R was 2.0.1
> 
>  Be sure that I have red all I found about this before posting.
>  Thanks in advance
>     
>  Leo
> 
> Protek-on: CaraMail met en oeuvre un nouveau Concept de SÅÈcuritÅÈ Globale - www.caramail.com
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jan.sabee at gmail.com  Fri Jun 10 12:49:24 2005
From: jan.sabee at gmail.com (Jan Sabee)
Date: Fri, 10 Jun 2005 12:49:24 +0200
Subject: [R] How to read a row dataset one by one
In-Reply-To: <42A95E16.50707@maths.lth.se>
References: <96507a8e050610013258257e98@mail.gmail.com>
	<42A9583B.D461F0F6@STATS.uct.ac.za> <42A95E16.50707@maths.lth.se>
Message-ID: <96507a8e05061003495942ab1c@mail.gmail.com>

For Henrik and Clark, thanks for your help.
Then If I load to dataframe,
MM16 <- read.table("G:\\Stuff\\data\\MM16.txt")
MM16
x1 x2 x3 x4 x5   y
a  b  a  c  c    M1
c  b  b  c  c    M4
c  c  a  c  c    M2
c  a  c  a  a    M2
c  c  a  a  a    M1
c  a  b  c  a    M3
c  c  a  b  c    M3
c  a  c  a  b    M2
c  c  a  b  a    M1

How can I do it.
Thanks again for your help.
Jan Sabee

On 6/10/05, Henrik Bengtsson <hb at maths.lth.se> wrote:
> Open a connection a read line by line from that one, e.g.
> 
> myReadPrint <- function(pathname, ...) {
>    con <- file(pathname, open="r")
>    on.exit(close(con)) # Guarantees to close connection!
> 
>    count <- 0;
>    while(TRUE) {
>      line <- scan(con, sep="\t", nlines=1, fill=TRUE,
>                                         quiet=TRUE, what="raw");
>      # Alternatively, just...
>      # line <- readLines(con, n=1)
>      if (length(line) == 0)
>        break;
>      count <- count + 1;
>      cat("read row no ", count, ",\n", sep="");
>      print(line);
>    }
> }
> 
> See ?file for more details.
> 
> /Henrik
> 
> Clark Allan wrote:
> > use a loop associated with the scan function.
> >
> > for (i in 1:9)
> > {
> >
> > print(scan(file="c:/a.txt",sep="\t",skip=i,nlines=1,fill=T,quiet=T,what="raw"))
> > }
> >
> >
> > this works but there has to be a better solution
> >
> >
> >
> > Jan Sabee wrote:
> >
> >>Dear all,
> >>How to read a row dataset one by one and then print it.
> >>
> >>x1 x2 x3 x4 x5   y
> >>a  b  a  c  c    M1
> >>c  b  b  c  c    M4
> >>c  c  a  c  c    M2
> >>c  a  c  a  a    M2
> >>c  c  a  a  a    M1
> >>c  a  b  c  a    M3
> >>c  c  a  b  c    M3
> >>c  a  c  a  b    M2
> >>c  c  a  b  a    M1
> >>
> >>I need a result like
> >>read row no 1,
> >>[1] a  b  a  c  c    M1
> >>read row no 2,
> >>[1] c  b  b  c  c    M4
> >>.
> >>.
> >>.
> >>the last row,
> >>[1] c  c  a  b  a    M1
> >>
> >>Kind regards,
> >>Jan Sabee
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> >>------------------------------------------------------------------------
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Fri Jun 10 13:01:36 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 10 Jun 2005 13:01:36 +0200
Subject: [R] dir() and RegEx and gsub()
In-Reply-To: <47fce0650506100216646ed606@mail.gmail.com>
References: <47fce06505060909357097319c@mail.gmail.com>	<efb536d5050609100968160e25@mail.gmail.com>
	<47fce0650506100216646ed606@mail.gmail.com>
Message-ID: <42A97310.1070302@statistik.uni-dortmund.de>

Hans-Peter wrote:

> 2005/6/9, Sarah Goslee <sarah.goslee at gmail.com>:
> 
> 
>>Under R, for reasons I've never quite understood,
>>"\\." evaluates to .
> 
> 
> Thanks to the answers of B. Ripley and Gabor I think, I understand now:
> 
>   1)  the patter-string "\\.csv$" gives the regular expression "\.csv$"
>   2)  now the backslash lets the dot to be interpreted literally
>       (instad of a metacharacter). As said by Gabor, an alternative is,
>       to put the dot between brackets
> 
> 
>>If you mean you want to change the "\" to either "\\" or "/" I'm
>>really not sure.
> 
> 
> Yes that's what I intended. 
> Because in windwos I copy the path from the address bar of the
> explorer and paste it in the R console window. Now I *have* something
> like
> myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
 > that needs to be adjusted manually ;-(


Copy and try
   myfile <- readLines("clipboard")




> I interpret the answers
> 
> 
>>>myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
>>
>>I am sure that's not what you intended.  It has to be written as
>>[snip] [1]  "D:/UebungenNDK/DataMining/DataMiningSeries.r"
> 
> 
> and 
> 
> 
>>>myfile <- "D:\UebungenNDK\DataMining\DataMiningSeries.r"
>>
>>Variable myfile, as you have written it above, has no backslashes in it
>>so there is no way way to know where they are supposed to be. 
> 
> 
> that it's not possible at all to have a character string with
> backslashes in it (because they *will* act as escape characters).

It *is* possible to *have* such a character string,
   x <- "a\\b"
generates an x that contains a *single* backslash.
It's a matter of parsing.


> - Maybe I could get along by writing an *external* function that would
> give me back a proper formatted path, eg.
> fold <- as.path( "D:\UebungenNDK\DataMining\DataMiningSeries.r" )
> [1] "D:/UebungenNDK/DataMining/DataMiningSeries.r"
> 
> - But then the behaviour would be different from all the other strings
> in R which doesn't seem to be a good idea either
> 
> 
>>I've always just processed the strings in vim or similar, rather
> 
> than fight with R.
> 
> I do it in DOS-Batch files. Quite ugly but it works...
> 
> 
> Regards,
> Hans-Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Fri Jun 10 13:00:44 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 10 Jun 2005 13:00:44 +0200
Subject: [R] How to read a row dataset one by one
In-Reply-To: <96507a8e05061003495942ab1c@mail.gmail.com>
References: <96507a8e050610013258257e98@mail.gmail.com>	<42A9583B.D461F0F6@STATS.uct.ac.za>
	<42A95E16.50707@maths.lth.se>
	<96507a8e05061003495942ab1c@mail.gmail.com>
Message-ID: <42A972DC.4090001@maths.lth.se>

Jan,

I'm not sure what you asking for.  First, you cannot make read.table() 
to output verbose message at every line read.  Second, if you have 
trouble to read you file, which looks like what you are showing, then 
read ?read.table carefully.  Most likely you wish to add argument 
header=TRUE and sep="\t" (if it is a tab-delimited file).

/Henrik

Jan Sabee wrote:
> For Henrik and Clark, thanks for your help.
> Then If I load to dataframe,
> MM16 <- read.table("G:\\Stuff\\data\\MM16.txt")
> MM16
> x1 x2 x3 x4 x5   y
> a  b  a  c  c    M1
> c  b  b  c  c    M4
> c  c  a  c  c    M2
> c  a  c  a  a    M2
> c  c  a  a  a    M1
> c  a  b  c  a    M3
> c  c  a  b  c    M3
> c  a  c  a  b    M2
> c  c  a  b  a    M1
> 
> How can I do it.
> Thanks again for your help.
> Jan Sabee
> 
> On 6/10/05, Henrik Bengtsson <hb at maths.lth.se> wrote:
> 
>>Open a connection a read line by line from that one, e.g.
>>
>>myReadPrint <- function(pathname, ...) {
>>   con <- file(pathname, open="r")
>>   on.exit(close(con)) # Guarantees to close connection!
>>
>>   count <- 0;
>>   while(TRUE) {
>>     line <- scan(con, sep="\t", nlines=1, fill=TRUE,
>>                                        quiet=TRUE, what="raw");
>>     # Alternatively, just...
>>     # line <- readLines(con, n=1)
>>     if (length(line) == 0)
>>       break;
>>     count <- count + 1;
>>     cat("read row no ", count, ",\n", sep="");
>>     print(line);
>>   }
>>}
>>
>>See ?file for more details.
>>
>>/Henrik
>>
>>Clark Allan wrote:
>>
>>>use a loop associated with the scan function.
>>>
>>>for (i in 1:9)
>>>{
>>>
>>>print(scan(file="c:/a.txt",sep="\t",skip=i,nlines=1,fill=T,quiet=T,what="raw"))
>>>}
>>>
>>>
>>>this works but there has to be a better solution
>>>
>>>
>>>
>>>Jan Sabee wrote:
>>>
>>>
>>>>Dear all,
>>>>How to read a row dataset one by one and then print it.
>>>>
>>>>x1 x2 x3 x4 x5   y
>>>>a  b  a  c  c    M1
>>>>c  b  b  c  c    M4
>>>>c  c  a  c  c    M2
>>>>c  a  c  a  a    M2
>>>>c  c  a  a  a    M1
>>>>c  a  b  c  a    M3
>>>>c  c  a  b  c    M3
>>>>c  a  c  a  b    M2
>>>>c  c  a  b  a    M1
>>>>
>>>>I need a result like
>>>>read row no 1,
>>>>[1] a  b  a  c  c    M1
>>>>read row no 2,
>>>>[1] c  b  b  c  c    M4
>>>>.
>>>>.
>>>>.
>>>>the last row,
>>>>[1] c  c  a  b  a    M1
>>>>
>>>>Kind regards,
>>>>Jan Sabee
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>------------------------------------------------------------------------
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From pinard at iro.umontreal.ca  Fri Jun 10 13:04:55 2005
From: pinard at iro.umontreal.ca (=?iso-8859-1?Q?Fran=E7ois?= Pinard)
Date: Fri, 10 Jun 2005 07:04:55 -0400
Subject: [R] Rpy and RSPython
In-Reply-To: <cdf8178305060919294586753e@mail.gmail.com>
References: <cdf8178305060919294586753e@mail.gmail.com>
Message-ID: <20050610110455.GA7142@phenix.progiciels-bpi.ca>

[Weiwei Shi]

> I am thinking to use one of them but not sure which one is better. I
> think Rpy cannot call python from R while the PRPython can in
> two-directional calling.  Am I right?

s/PRPython/RSPython/ ? :-)

This is also what I understood.  Yet, despite the uni-directionality of
RPy, this is what I chose for my personal usage (probably more handy to
use, or easy to install -- but the main point was that RPy guaranteed
to be more stable and never crash!).  I think I recently read somewhere
that they were plans for undusting RSPython, and then said to myself:
"Should re-evaluate once done.".

Surely that for now, RPy is quite sufficient for my simple needs.

-- 
Fran??ois Pinard   http://pinard.progiciels-bpi.ca



From Matthias.Templ at statistik.gv.at  Fri Jun 10 13:05:02 2005
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 10 Jun 2005 13:05:02 +0200
Subject: [R] Different results in different runs with identical
	parametersin CLARA
Message-ID: <83536658864BC243BE3C06D7E936ABD5027BAADF@xchg1.statistik.local>

  
> Dear All R Friends,
> When I run my data in any time with the below codes, I
> receive different results. 
 
Of course. See in 
L. Kaufman and P. Rousseeuw. Finding Groups in Data. John 
Wiley & Sons, Inc, 1990. 
There is a "random part" in clara.
 
> My data , k , samples, trace are
> identical in any run.
>  
>  c<- clara(mydata,4, metric= " euclidean " , stand= TRUE,
> samples=5 , trace=3, keep.data=TRUE ,  rngR=TRUE)
>  
> result of first try:
> Average silhouette width per cluster: 0.5881658
> result of second try:
> Average silhouette width of best sample: 0.6294549
> result of third try:
> Average silhouette width of best sample: 0.6609939 
> ...
> I think that only best sample changes in any run. 
> The question is here: 
> Which try ( or run) is optimal? How many try do I need to 
> achive to optimal case? Is it reliable ? Best Regards, Amir
> 
 
See it as *Explorative Data Analysis*. Each of your different 
results give you additional ideas of the structure of your data.
 
Best,
Matthias



From Stef.Wagner at students.uni-mannheim.de  Fri Jun 10 13:08:53 2005
From: Stef.Wagner at students.uni-mannheim.de (Stefan Wagner)
Date: Fri, 10 Jun 2005 13:08:53 +0200
Subject: [R] Sum up the values of a function
Message-ID: <20050610130853.5ys2q22qokwgsk8s@webmail.uni-mannheim.de>

Dear R-Users,

I have to do a maximum-likelihood estimation and have now a problem 
concerning how to sum up my function values in a smart way. I don't 
know how to explain it easyly, so I give you the code and show you 
where my problem is.
I have shorten the code a little bit, so that you only get the 
necessary facts:

ws12 <- function (z, i) (1/(1+exp(z[1]*(z[3]-x1[i]- z[4]*(m1[i]/n1[i]-0.5)))))
ws37 <- function (z, i) (1/(1+exp(z[2]*(z[3]-x2[i]- z[5]*(m2[i]/n2[i]-0.5)))))
wsAttack12 <- function (z,i) (ws12(z,i)*dec1[i]+(1-ws12(z,i))*(1-dec1[i]))
wsAttack37 <- function (z,i) (ws37(z,i)*dec2[i]+(1-ws37(z,i))*(1-dec2[i]))
logwsAttack12 <- function (z,i) (log(wsAttack12(z,i)))
logwsAttack37 <- function (z,i) (log(wsAttack37(z,i)))
ws12sum <- function (z) 
(logwsAttack12(z,i=1)+logwsAttack12(z,i=2)+logwsAttack12(z,i=3)+logwsAttack12(z,i=4)+logwsAttack12(z,i=5)+logwsAttack12(z,i=6))
ws37sum <- function (z) 
(logwsAttack37(z,i=1)+logwsAttack37(z,i=2)+logwsAttack37(z,i=3)+logwsAttack37(z,i=4)+logwsAttack37(z,i=5)+logwsAttack37(z,i=6)+logwsAttack37(z,i=7)+logwsAttack37(z,i=8))
wsLOG <- function (z) (ws12sum(z) + ws37sum(z))
LogSum <- function (z) (-sum(wsLOG(z)))
SP <- c(0.16, 0.10, 44, 0.80, 46)
out <- nlm (LogSum, p=SP)
out

For explanation: x1[i], x2[i], m1[i], m2[i], n1[i], n2[i] are given 
data and z[1:5] are my estimates.
My problem is that I have more than one session with diffent number of 
datas so that I am searching for a general way of summing up my 
logwsAttack12 and logwsAttack37. The program should recognize how many 
data are in my table concerning ws12 and how many concerning ws37 and 
should, in dependency of z, sum them up.
I hope you understand my problem and hopefully someone is able to solve it.
Many thanks for the moment.

Best regards,

Stefan Wagner



From kyong.ho.park at us.army.mil  Fri Jun 10 13:20:37 2005
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr. RDECOM)
Date: Fri, 10 Jun 2005 07:20:37 -0400
Subject: [R] Replies of the question about robustness of segmented regression
Message-ID: <E0466E6811B5BA46B92414C94516A7A425BC3F@apgrb1rdg-mail2.nae.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/f919205d/attachment.pl

From petr.pikal at precheza.cz  Fri Jun 10 13:36:34 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 10 Jun 2005 13:36:34 +0200
Subject: [R] Sum up the values of a function
In-Reply-To: <20050610130853.5ys2q22qokwgsk8s@webmail.uni-mannheim.de>
Message-ID: <42A99762.8396.1D8CF1@localhost>

Hi

Your example is not reproducible as we do not know x1.
Do you by chance seek for something like aggregate? If yes see

?aggregate
or
?by

BTW, do you have some trouble with your space bar?

HTH

Petr

On 10 Jun 2005 at 13:08, Stefan Wagner wrote:

> Dear R-Users,
> 
> I have to do a maximum-likelihood estimation and have now a problem
> concerning how to sum up my function values in a smart way. I don't
> know how to explain it easyly, so I give you the code and show you
> where my problem is. I have shorten the code a little bit, so that you
> only get the necessary facts:
> 
> ws12 <- function (z, i) (1/(1+exp(z[1]*(z[3]-x1[i]-
> z[4]*(m1[i]/n1[i]-0.5))))) ws37 <- function (z, i)
> (1/(1+exp(z[2]*(z[3]-x2[i]- z[5]*(m2[i]/n2[i]-0.5))))) wsAttack12 <-
> function (z,i) (ws12(z,i)*dec1[i]+(1-ws12(z,i))*(1-dec1[i]))
> wsAttack37 <- function (z,i)
> (ws37(z,i)*dec2[i]+(1-ws37(z,i))*(1-dec2[i])) logwsAttack12 <-
> function (z,i) (log(wsAttack12(z,i))) logwsAttack37 <- function (z,i)
> (log(wsAttack37(z,i))) ws12sum <- function (z)
> (logwsAttack12(z,i=1)+logwsAttack12(z,i=2)+logwsAttack12(z,i=3)+logwsA
> ttack12(z,i=4)+logwsAttack12(z,i=5)+logwsAttack12(z,i=6)) ws37sum <-
> function (z)
> (logwsAttack37(z,i=1)+logwsAttack37(z,i=2)+logwsAttack37(z,i=3)+logwsA
> ttack37(z,i=4)+logwsAttack37(z,i=5)+logwsAttack37(z,i=6)+logwsAttack37
> (z,i=7)+logwsAttack37(z,i=8)) wsLOG <- function (z) (ws12sum(z) +
> ws37sum(z)) LogSum <- function (z) (-sum(wsLOG(z))) SP <- c(0.16,
> 0.10, 44, 0.80, 46) out <- nlm (LogSum, p=SP) out
> 
> For explanation: x1[i], x2[i], m1[i], m2[i], n1[i], n2[i] are given
> data and z[1:5] are my estimates. My problem is that I have more than
> one session with diffent number of datas so that I am searching for a
> general way of summing up my logwsAttack12 and logwsAttack37. The
> program should recognize how many data are in my table concerning ws12
> and how many concerning ws37 and should, in dependency of z, sum them
> up. I hope you understand my problem and hopefully someone is able to
> solve it. Many thanks for the moment.
> 
> Best regards,
> 
> Stefan Wagner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Stef.Wagner at students.uni-mannheim.de  Fri Jun 10 13:58:54 2005
From: Stef.Wagner at students.uni-mannheim.de (Stefan Wagner)
Date: Fri, 10 Jun 2005 13:58:54 +0200
Subject: [R] Sum up the values of a function
In-Reply-To: <42A99762.8396.1D8CF1@localhost>
References: <42A99762.8396.1D8CF1@localhost>
Message-ID: <20050610135854.v88ngwzmskw4so4c@webmail.uni-mannheim.de>

I am really sorry, that the code appears in a bad way but it is really 
not my fault (at least I think so). I think my mail-programm is 
responsible for it because I sent it in a "normal" way without havin g 
trouble with my space bar.
Here are the missing values (but please remember, this is only a short 
example, normally there are more than 400 values)
x1 <- c(34.67,19.91,47.48,22.48,17.34,24.42)
m1 <- c(1,0,0,0,0,0)
n1 <- c(1,1,1,1,1,1)
dec1 <- c(0,0,0,0,1,0)
x2 <- c(50.98,30.63,31.37,21.17,21.49,39.41,46.85,38.08)
m2 <- c(1,0,0,0,1,0,2,1)
n2 <- c(3,3,3,3,3,3,3,3)
dec2 <- c(0,0,0,0,0,0,0,0)

I hope this is all you need,

Stefan

Zitat von Petr Pikal <petr.pikal at precheza.cz>:

> Hi
>
> Your example is not reproducible as we do not know x1.
> Do you by chance seek for something like aggregate? If yes see
>
> ?aggregate
> or
> ?by
>
> BTW, do you have some trouble with your space bar?
>
> HTH
>
> Petr
>
> On 10 Jun 2005 at 13:08, Stefan Wagner wrote:
>
>> Dear R-Users,
>>
>> I have to do a maximum-likelihood estimation and have now a problem
>> concerning how to sum up my function values in a smart way. I don't
>> know how to explain it easyly, so I give you the code and show you
>> where my problem is. I have shorten the code a little bit, so that you
>> only get the necessary facts:
>>
>> ws12 <- function (z, i) (1/(1+exp(z[1]*(z[3]-x1[i]-
>> z[4]*(m1[i]/n1[i]-0.5))))) ws37 <- function (z, i)
>> (1/(1+exp(z[2]*(z[3]-x2[i]- z[5]*(m2[i]/n2[i]-0.5))))) wsAttack12 <-
>> function (z,i) (ws12(z,i)*dec1[i]+(1-ws12(z,i))*(1-dec1[i]))
>> wsAttack37 <- function (z,i)
>> (ws37(z,i)*dec2[i]+(1-ws37(z,i))*(1-dec2[i])) logwsAttack12 <-
>> function (z,i) (log(wsAttack12(z,i))) logwsAttack37 <- function (z,i)
>> (log(wsAttack37(z,i))) ws12sum <- function (z)
>> (logwsAttack12(z,i=1)+logwsAttack12(z,i=2)+logwsAttack12(z,i=3)+logwsA
>> ttack12(z,i=4)+logwsAttack12(z,i=5)+logwsAttack12(z,i=6)) ws37sum <-
>> function (z)
>> (logwsAttack37(z,i=1)+logwsAttack37(z,i=2)+logwsAttack37(z,i=3)+logwsA
>> ttack37(z,i=4)+logwsAttack37(z,i=5)+logwsAttack37(z,i=6)+logwsAttack37
>> (z,i=7)+logwsAttack37(z,i=8)) wsLOG <- function (z) (ws12sum(z) +
>> ws37sum(z)) LogSum <- function (z) (-sum(wsLOG(z))) SP <- c(0.16,
>> 0.10, 44, 0.80, 46) out <- nlm (LogSum, p=SP) out
>>
>> For explanation: x1[i], x2[i], m1[i], m2[i], n1[i], n2[i] are given
>> data and z[1:5] are my estimates. My problem is that I have more than
>> one session with diffent number of datas so that I am searching for a
>> general way of summing up my logwsAttack12 and logwsAttack37. The
>> program should recognize how many data are in my table concerning ws12
>> and how many concerning ws37 and should, in dependency of z, sum them
>> up. I hope you understand my problem and hopefully someone is able to
>> solve it. Many thanks for the moment.
>>
>> Best regards,
>>
>> Stefan Wagner
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>



From r.hankin at noc.soton.ac.uk  Fri Jun 10 14:20:57 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 10 Jun 2005 13:20:57 +0100
Subject: [R] contours and cutlines
Message-ID: <42965836e1542508190e6bee6561f044@soc.soton.ac.uk>

Hi

Using contour(), one can plot pretty pictures (of potential flow); my 
problem is removing
the cutline.  If I do:

  x <- seq(from=-4,to=4,len=100)
z <- outer(x,1i*x,"+")
contour(x,x,Im(log(z)))

everything is fine except the discontinuity along the negative real 
axis, which derives
from log()'s cutline.

Before I try to reinvent the wheel, does anyone have a nice way of 
getting rid of the ugly
bunch of contours that correspond to the discontinuity along the cut 
line?

Also,  something like

contour(x,x,Im((1+2i)*log(z^2/(z-2-1i)/(z-2+1i))),nlevel=33)

has a more complicated cut line.  Has anyone got any ideas how to 
improve the plot (ie
allow the flow lines to be nicely continuous everywhere except the 
sources)?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From subianto at gmail.com  Fri Jun 10 14:23:52 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 10 Jun 2005 14:23:52 +0200
Subject: [R] data.frame to character
Message-ID: <42A98658.1020907@gmail.com>

Hi,
Excuse me for this simple question.
How to convert as.data.frame to as.character?

  ?data.frame

 > L3 <- LETTERS[1:3]
 >  L10 <- LETTERS[1:10]
 >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 10, repl=TRUE))
 >  d
      x y fac
1  XYZ A   A
2  XYZ B   A
3  XYZ C   A
4  XYZ D   A
5  XYZ E   B
6  XYZ F   C
7  XYZ G   A
8  XYZ H   C
9  XYZ I   B
10 XYZ J   A
 >  str(d)
`data.frame':   10 obs. of  3 variables:
  $ x  : Factor w/ 1 level "XYZ": 1 1 1 1 1 1 1 1 1 1
  $ y  : Factor w/ 10 levels "A","B","C","D",..: 1 2 3 4 5 6 7 8 9 10
  $ fac: Factor w/ 3 levels "A","B","C": 1 1 1 1 2 3 1 3 2 1
 >  d[3,]
     x y fac
3 XYZ C   A
 >
 >  as.character(d[3,])
[1] "1" "3" "1"
 >

I think as.character the result something like
[3] "XYZ" "C" "A"

I don't know how to convert it.
Any help gratefully received.
Thank you very much in advance.
Kindly regards,
Muhammad Subianto



From andy_liaw at merck.com  Fri Jun 10 14:30:12 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Jun 2005 08:30:12 -0400
Subject: [R] data.frame to character
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E975@usctmx1106.merck.com>

Is this what you want?

> d[] <- lapply(d, as.character)
> str(d)
`data.frame':   10 obs. of  3 variables:
 $ x  : chr  "XYZ" "XYZ" "XYZ" "XYZ" ...
 $ y  : chr  "A" "B" "C" "D" ...
 $ fac: chr  "B" "A" "C" "B" ...

Andy

> From: Muhammad Subianto
> 
> Hi,
> Excuse me for this simple question.
> How to convert as.data.frame to as.character?
> 
>   ?data.frame
> 
>  > L3 <- LETTERS[1:3]
>  >  L10 <- LETTERS[1:10]
>  >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 
> 10, repl=TRUE))
>  >  d
>       x y fac
> 1  XYZ A   A
> 2  XYZ B   A
> 3  XYZ C   A
> 4  XYZ D   A
> 5  XYZ E   B
> 6  XYZ F   C
> 7  XYZ G   A
> 8  XYZ H   C
> 9  XYZ I   B
> 10 XYZ J   A
>  >  str(d)
> `data.frame':   10 obs. of  3 variables:
>   $ x  : Factor w/ 1 level "XYZ": 1 1 1 1 1 1 1 1 1 1
>   $ y  : Factor w/ 10 levels "A","B","C","D",..: 1 2 3 4 5 6 7 8 9 10
>   $ fac: Factor w/ 3 levels "A","B","C": 1 1 1 1 2 3 1 3 2 1
>  >  d[3,]
>      x y fac
> 3 XYZ C   A
>  >
>  >  as.character(d[3,])
> [1] "1" "3" "1"
>  >
> 
> I think as.character the result something like
> [3] "XYZ" "C" "A"
> 
> I don't know how to convert it.
> Any help gratefully received.
> Thank you very much in advance.
> Kindly regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Fri Jun 10 14:35:11 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 08:35:11 -0400
Subject: [R] data.frame to character
In-Reply-To: <42A98658.1020907@gmail.com>
References: <42A98658.1020907@gmail.com>
Message-ID: <971536df050610053546a33148@mail.gmail.com>

On 6/10/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Hi,
> Excuse me for this simple question.
> How to convert as.data.frame to as.character?
> 
>  ?data.frame
> 
>  > L3 <- LETTERS[1:3]
>  >  L10 <- LETTERS[1:10]
>  >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 10, repl=TRUE))


d[] <- as.matrix(d)



From wuming.gong at gmail.com  Fri Jun 10 14:50:29 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Fri, 10 Jun 2005 20:50:29 +0800
Subject: [R] logistic regressioin - course ornotes
In-Reply-To: <007f01c56d5e$b11f2950$9701a8c0@Tablet>
References: <007f01c56d5e$b11f2950$9701a8c0@Tablet>
Message-ID: <b428d06d05061005508010489@mail.gmail.com>

Hi Stephen,

I think the tutorial written by Brett Presnell
(http://web.stat.ufl.edu/~presnell/Teaching/sta4504-2000sp/R/) is a
good start point for categorical data analysis using R.

Wuming

On 6/10/05, Stephen Choularton <mail at bymouth.com> wrote:
> Hi
> 
> I am using R for logistic regression and finding it very useful.
> However, I wondered if anyone could point me to any course or notes on
> this subject using R.
> 
> All help most welcome.
> 
> Stephen
> 
> --
> Internal Virus Database is out-of-date.
> Checked by AVG Anti-Virus.
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bitwrit at ozemail.com.au  Sat Jun 11 00:55:46 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 10 Jun 2005 22:55:46 +0000
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
In-Reply-To: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
References: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
Message-ID: <42AA1A72.4080507@ozemail.com.au>

Tan Hui Hui Jenny wrote:
> I'm trying to plot Venn diagrams with more than 3 sets (5 actually) in order to describe graphically the genetic variation between populations.
>  
> I tried the limma library but realised it can only plot 3 sets.
>  
> Is there any solution? Of course I could plot the chart manually but it'll take too long (have other datasets)..... One of my dataset is given below. 
>  
Hi Jenny,

Best I could do is four sets. The code is a bit rough but consistent 
with the vennDiagram function in the limma package.

Jim
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: venn4.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/7255c49f/venn4.pl

From subianto at gmail.com  Fri Jun 10 15:08:01 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 10 Jun 2005 15:08:01 +0200
Subject: [R] data.frame to character - thanks
In-Reply-To: <42A98658.1020907@gmail.com>
References: <42A98658.1020907@gmail.com>
Message-ID: <42A990B1.8030602@gmail.com>

Dear all,
Thank you very much for your help.
I would like to thank Andy Liaw and Gabor Grothendieck for their fast help.

Regards,
Muhammad Subianto

On this day 6/10/2005 2:30 PM, Liaw, Andy wrote:
 > Is this what you want?
 >
 >
 >>d[] <- lapply(d, as.character)
 >>str(d)
 >
 > `data.frame':   10 obs. of  3 variables:
 >  $ x  : chr  "XYZ" "XYZ" "XYZ" "XYZ" ...
 >  $ y  : chr  "A" "B" "C" "D" ...
 >  $ fac: chr  "B" "A" "C" "B" ...
 >
 > Andy

On this day 6/10/2005 2:35 PM, Gabor Grothendieck wrote:
 > On 6/10/05, Muhammad Subianto <subianto at gmail.com> wrote:
 >
 >>Hi,
 >>Excuse me for this simple question.
 >>How to convert as.data.frame to as.character?
 >>
 >> ?data.frame
 >>
 >> > L3 <- LETTERS[1:3]
 >> >  L10 <- LETTERS[1:10]
 >> >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 10, 
repl=TRUE))
 >
 >
 >
 > d[] <- as.matrix(d)
 >

On this day 6/10/2005 2:23 PM, Muhammad Subianto wrote:
> Hi,
> Excuse me for this simple question.
> How to convert as.data.frame to as.character?
> 
>   ?data.frame
> 
>  > L3 <- LETTERS[1:3]
>  >  L10 <- LETTERS[1:10]
>  >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 10, repl=TRUE))
>  >  d
>       x y fac
> 1  XYZ A   A
> 2  XYZ B   A
> 3  XYZ C   A
> 4  XYZ D   A
> 5  XYZ E   B
> 6  XYZ F   C
> 7  XYZ G   A
> 8  XYZ H   C
> 9  XYZ I   B
> 10 XYZ J   A
>  >  str(d)
> `data.frame':   10 obs. of  3 variables:
>   $ x  : Factor w/ 1 level "XYZ": 1 1 1 1 1 1 1 1 1 1
>   $ y  : Factor w/ 10 levels "A","B","C","D",..: 1 2 3 4 5 6 7 8 9 10
>   $ fac: Factor w/ 3 levels "A","B","C": 1 1 1 1 2 3 1 3 2 1
>  >  d[3,]
>      x y fac
> 3 XYZ C   A
>  >
>  >  as.character(d[3,])
> [1] "1" "3" "1"
>  >
> 
> I think as.character the result something like
> [3] "XYZ" "C" "A"
> 
> I don't know how to convert it.
> Any help gratefully received.
> Thank you very much in advance.
> Kindly regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From joshuacgilbert at gmail.com  Fri Jun 10 15:38:55 2005
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Fri, 10 Jun 2005 09:38:55 -0400
Subject: [R] Error with function lda in package MASS (dimnames not equal?)
Message-ID: <ef96daa30506100638416dfe47@mail.gmail.com>

This question appears to have been asked previously, but not answered.
the last response I can find to this previous thread is here:
http://tolstoy.newcastle.edu.au/R/help/04/07/0126.html. The asnwer was
to provide debugging info, not an answer.

So the problem is that I'm trying to use lda on my dataset. You can
download my data here:
http://northstar-www.dartmouth.edu/~jgilbert/nolda, I used R's save
function to save objects data and classes (yes, I realize that I name
stomped the data function in package utils). To replicate my results,
simply enter the following:
> library(MASS)
> load('nolda')
> lda(data,classes)
Error in lda.default(x, grouping, ...) : length of 'dimnames' [2] not
equal to array extent

Now, I don't know what that means. 
> dimnames(data)
NULL
> dimnames(classes)
NULL

As for debugging, I don't know how. I cannot debug lda.default as I
get the following:
> debug(lda.default)
Error: Object "lda.default" not found

I think that that's pretty much it. Can anyone help me?



From ggrothendieck at gmail.com  Fri Jun 10 15:41:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 09:41:12 -0400
Subject: [R] data.frame to character - thanks
In-Reply-To: <42A990B1.8030602@gmail.com>
References: <42A98658.1020907@gmail.com> <42A990B1.8030602@gmail.com>
Message-ID: <971536df05061006416e9de5e0@mail.gmail.com>

Here is one minor improvement -- it does not overwrite the input:

replace(d,,as.matrix(d))



On 6/10/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Dear all,
> Thank you very much for your help.
> I would like to thank Andy Liaw and Gabor Grothendieck for their fast help.
> 
> Regards,
> Muhammad Subianto
> 
> On this day 6/10/2005 2:30 PM, Liaw, Andy wrote:
>  > Is this what you want?
>  >
>  >
>  >>d[] <- lapply(d, as.character)
>  >>str(d)
>  >
>  > `data.frame':   10 obs. of  3 variables:
>  >  $ x  : chr  "XYZ" "XYZ" "XYZ" "XYZ" ...
>  >  $ y  : chr  "A" "B" "C" "D" ...
>  >  $ fac: chr  "B" "A" "C" "B" ...
>  >
>  > Andy
> 
> On this day 6/10/2005 2:35 PM, Gabor Grothendieck wrote:
>  > On 6/10/05, Muhammad Subianto <subianto at gmail.com> wrote:
>  >
>  >>Hi,
>  >>Excuse me for this simple question.
>  >>How to convert as.data.frame to as.character?
>  >>
>  >> ?data.frame
>  >>
>  >> > L3 <- LETTERS[1:3]
>  >> >  L10 <- LETTERS[1:10]
>  >> >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 10,
> repl=TRUE))
>  >
>  >
>  >
>  > d[] <- as.matrix(d)
>  >
> 
> On this day 6/10/2005 2:23 PM, Muhammad Subianto wrote:
> > Hi,
> > Excuse me for this simple question.
> > How to convert as.data.frame to as.character?
> >
> >   ?data.frame
> >
> >  > L3 <- LETTERS[1:3]
> >  >  L10 <- LETTERS[1:10]
> >  >  d <- data.frame(cbind(x=c("XYZ"), y=L10), fac=sample(L3, 10, repl=TRUE))
> >  >  d
> >       x y fac
> > 1  XYZ A   A
> > 2  XYZ B   A
> > 3  XYZ C   A
> > 4  XYZ D   A
> > 5  XYZ E   B
> > 6  XYZ F   C
> > 7  XYZ G   A
> > 8  XYZ H   C
> > 9  XYZ I   B
> > 10 XYZ J   A
> >  >  str(d)
> > `data.frame':   10 obs. of  3 variables:
> >   $ x  : Factor w/ 1 level "XYZ": 1 1 1 1 1 1 1 1 1 1
> >   $ y  : Factor w/ 10 levels "A","B","C","D",..: 1 2 3 4 5 6 7 8 9 10
> >   $ fac: Factor w/ 3 levels "A","B","C": 1 1 1 1 2 3 1 3 2 1
> >  >  d[3,]
> >      x y fac
> > 3 XYZ C   A
> >  >
> >  >  as.character(d[3,])
> > [1] "1" "3" "1"
> >  >
> >
> > I think as.character the result something like
> > [3] "XYZ" "C" "A"
> >
> > I don't know how to convert it.
> > Any help gratefully received.
> > Thank you very much in advance.
> > Kindly regards,
> > Muhammad Subianto
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
>



From ggrothendieck at gmail.com  Fri Jun 10 15:44:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 09:44:06 -0400
Subject: [R] Error with function lda in package MASS (dimnames not
	equal?)
In-Reply-To: <ef96daa30506100638416dfe47@mail.gmail.com>
References: <ef96daa30506100638416dfe47@mail.gmail.com>
Message-ID: <971536df050610064449224e8f@mail.gmail.com>

On 6/10/05, Joshua Gilbert <joshuacgilbert at gmail.com> wrote:
> This question appears to have been asked previously, but not answered.
> the last response I can find to this previous thread is here:
> http://tolstoy.newcastle.edu.au/R/help/04/07/0126.html. The asnwer was
> to provide debugging info, not an answer.
> 
> So the problem is that I'm trying to use lda on my dataset. You can
> download my data here:
> http://northstar-www.dartmouth.edu/~jgilbert/nolda, I used R's save
> function to save objects data and classes (yes, I realize that I name
> stomped the data function in package utils). To replicate my results,
> simply enter the following:
> > library(MASS)
> > load('nolda')
> > lda(data,classes)
> Error in lda.default(x, grouping, ...) : length of 'dimnames' [2] not
> equal to array extent
> 
> Now, I don't know what that means.
> > dimnames(data)
> NULL
> > dimnames(classes)
> NULL
> 
> As for debugging, I don't know how. I cannot debug lda.default as I
> get the following:
> > debug(lda.default)
> Error: Object "lda.default" not found

debug(MASS:::lda.default)



From ripley at stats.ox.ac.uk  Fri Jun 10 16:10:30 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 10 Jun 2005 15:10:30 +0100 (BST)
Subject: [R] Error with function lda in package MASS (dimnames not
	equal?)
In-Reply-To: <ef96daa30506100638416dfe47@mail.gmail.com>
References: <ef96daa30506100638416dfe47@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506101509330.5321@gannet.stats>

lda.default <- MASS:::lda.default and proceed.

Look at the group means of your data: they are identical to machine
accuracy.

The question has to be `why are you trying to use lda to separate
two groups with identical means'?  Lda is not protected against that
and it is rather unlikely unless you failed to inspect your data in any 
way.

On Fri, 10 Jun 2005, Joshua Gilbert wrote:

> This question appears to have been asked previously, but not answered.
> the last response I can find to this previous thread is here:
> http://tolstoy.newcastle.edu.au/R/help/04/07/0126.html. The asnwer was
> to provide debugging info, not an answer.
>
> So the problem is that I'm trying to use lda on my dataset. You can
> download my data here:
> http://northstar-www.dartmouth.edu/~jgilbert/nolda, I used R's save
> function to save objects data and classes (yes, I realize that I name
> stomped the data function in package utils). To replicate my results,
> simply enter the following:
>> library(MASS)
>> load('nolda')
>> lda(data,classes)
> Error in lda.default(x, grouping, ...) : length of 'dimnames' [2] not
> equal to array extent
>
> Now, I don't know what that means.
>> dimnames(data)
> NULL
>> dimnames(classes)
> NULL
>
> As for debugging, I don't know how. I cannot debug lda.default as I
> get the following:
>> debug(lda.default)
> Error: Object "lda.default" not found
>
> I think that that's pretty much it. Can anyone help me?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alain.ponsero at wanadoo.fr  Fri Jun 10 16:16:03 2005
From: alain.ponsero at wanadoo.fr (alain ponsero)
Date: Fri, 10 Jun 2005 16:16:03 +0200
Subject: [R] In connection with the creation of a grid of point with gstat
Message-ID: <000601c56dc6$fe07e690$0100a8c0@ALAIN>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/adff3f52/attachment.pl

From rkoenker at uiuc.edu  Fri Jun 10 16:17:18 2005
From: rkoenker at uiuc.edu (roger koenker)
Date: Fri, 10 Jun 2005 09:17:18 -0500
Subject: [R] rgl.snapshot "failed"
Message-ID: <3256A5D0-52E9-47B1-850E-DE71D4198476@uiuc.edu>

I've installed the rgl package on a Suse x86-64 machine (further  
details below)
and it produces nice screen images.  Unfortunately, rgl.snapshot   
attempts to
make png files produces only the response "failed".  For other  
graphics png()
works fine, and capabilities indicates that it is there.  If anyone  
has a suggestion
of what might be explored at this point I'd be very appreciative.

platform x86_64-unknown-linux-gnu
arch     x86_64
os       linux-gnu
system   x86_64, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

url:    www.econ.uiuc.edu/~roger            Roger Koenker
email    rkoenker at uiuc.edu            Department of Economics
vox:     217-333-4558                University of Illinois
fax:       217-244-6678                Champaign, IL 61820



From fws4 at cdrh.fda.gov  Fri Jun 10 16:17:27 2005
From: fws4 at cdrh.fda.gov (Frank Samuelson)
Date: Fri, 10 Jun 2005 10:17:27 -0400
Subject: [R] question on boot
In-Reply-To: <42A84DBA.7090900@uhnres.utoronto.ca>
References: <42A84DBA.7090900@uhnres.utoronto.ca>
Message-ID: <d8c71k$5k9$1@sea.gmane.org>

boot will handle multivariate data as well as multistratum data,
though it isn't clear from your question what
is multivariate about your problem or what you are bootstrapping.


Lisa Wang wrote:
> Hello there:
> 
> Dear Dr. Murdoch,
> 
> I'm a statistician at Princess Margaret Hospital. Could you please help 
> me with the bootstrapping?
> 
> Can boot function (along with boot.ci) handle multivariate statistics?  
> My intention is that I want to group the outcome data and evaluate the 
> proportion of patients who are positive for a marker within each group.  
> (i.e. I have two variables, a predictor and an outcome.  The predictor 
> is continuous and the outcome is binary.  I want a statistic which 
> groups patients based on the predictor variable (0-10, 10-20, 20-30, 
> etc;) and check the proportion of patients who are positive for the 
> outcome variable within each group).
> 
> Regards
> 
> Lisa Wang Msc.
> Princess Margaret Hospital
> Toronto, Ca
> tel 416 9464501 ext. 5201
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From remigijus.lapinskas at mif.vu.lt  Fri Jun 10 16:26:07 2005
From: remigijus.lapinskas at mif.vu.lt (Remigijus Lapinskas)
Date: Fri, 10 Jun 2005 17:26:07 +0300
Subject: [R] What is median in survfit
Message-ID: <42A9A2FF.6020208@mif.vu.lt>

Dear All,

A very simple question:

 > library(survival)
 > fit <- coxph(Surv(time, status) ~ x, data=aml)
 > survfit(fit)
Call: survfit.coxph(object = fit)

       n  events  median 0.95LCL 0.95UCL
      23      18      30      18      45

I believe I know what is median here, but how to extract it?

Many thanks,
Rem



From Roger.Bivand at nhh.no  Fri Jun 10 16:56:52 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 10 Jun 2005 16:56:52 +0200 (CEST)
Subject: [R] In connection with the creation of a grid of point with
 gstat
In-Reply-To: <000601c56dc6$fe07e690$0100a8c0@ALAIN>
Message-ID: <Pine.LNX.4.44.0506101651250.5742-100000@reclus.nhh.no>

On Fri, 10 Jun 2005, alain ponsero wrote:

>   
> 
> Hi ALL,
> 
> With the pakage gstat 
> 
>  
> 
> How can one create a grid of points as in the meuse.grid example, from
> measuring sites as in the Meuse example
> 

In the meuse example, the meuse.grid locations are provided. They are 
essentially the same as the code snippet in the example for 
predict.gstat():

     # unconditional simulation on a 100 x 100 grid
     xy <- expand.grid(1:100, 1:100)
     names(xy) <- c("x","y")
     g.dummy <- gstat(formula = z~1, locations = ~x+y, dummy = TRUE, beta = 0,
             model = vgm(1,"Exp",15), nmax = 20)
     yy <- predict(g.dummy, newdata = xy, nsim = 4)

that is using expand.grid() and giving the output object the correct names 
for your coordinates. If you need to mask the grid, you need to choose 
just some rows of xy for prediction (perhaps using the overlay() method in 
the sp package). 

>  
> 
> Thanks in advance!
> 
>  
> 
> Alain PONSERO
> Conservateur
> R??serve Naturelle de la Baie de Saint-Brieuc
> site de l'Etoile
> 22120 HILLION
> tel/fax : 02.96.32.31.40
>  <mailto:aponsero at cabri22.com> aponsero at cabri22.com
>  <http://www.reservebaiedesaintbrieuc.com/reserve.htm>
> http://www.reservebaiedesaintbrieuc.com/reserve.htm
> ><(((??>
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From tlumley at u.washington.edu  Fri Jun 10 17:04:17 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 10 Jun 2005 08:04:17 -0700 (PDT)
Subject: [R] What is median in survfit
In-Reply-To: <42A9A2FF.6020208@mif.vu.lt>
References: <42A9A2FF.6020208@mif.vu.lt>
Message-ID: <Pine.A41.4.61b.0506100757460.87692@homer05.u.washington.edu>

On Fri, 10 Jun 2005, Remigijus Lapinskas wrote:

> Dear All,
>
> A very simple question:
>
> > library(survival)
> > fit <- coxph(Surv(time, status) ~ x, data=aml)
> > survfit(fit)
> Call: survfit.coxph(object = fit)
>
>       n  events  median 0.95LCL 0.95UCL
>      23      18      30      18      45
>
> I believe I know what is median here, but how to extract it?
>

This turns out to be trickier than one would ideally like
   fit$time[min(which(fit$surv<=0.5))]
works if there is a single stratum, as in your example.

Or, even uglier, you can capture then entire table with
> read.table(textConnection(capture.output(survfit(fit))),skip=2,header=TRUE)
    n events median X0.95LCL X0.95UCL
1 23     18     30       18       45

which does work for stratified survfits.


 	-thomas



From murdoch at stats.uwo.ca  Fri Jun 10 17:04:51 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 10 Jun 2005 11:04:51 -0400
Subject: [R] rgl.snapshot "failed"
In-Reply-To: <3256A5D0-52E9-47B1-850E-DE71D4198476@uiuc.edu>
References: <3256A5D0-52E9-47B1-850E-DE71D4198476@uiuc.edu>
Message-ID: <42A9AC13.2010606@stats.uwo.ca>

On 6/10/2005 10:17 AM, roger koenker wrote:
> I've installed the rgl package on a Suse x86-64 machine (further  
> details below)
> and it produces nice screen images.  Unfortunately, rgl.snapshot   
> attempts to
> make png files produces only the response "failed".  For other  
> graphics png()
> works fine, and capabilities indicates that it is there.  If anyone  
> has a suggestion
> of what might be explored at this point I'd be very appreciative.

rgl is a contributed package, with maintainer Daniel Adler. It hasn't 
been updated on CRAN in around a year, though I know he has been working 
on it (because I've been doing some work with him, though I haven't been 
in contact since February).

I don't use Suse at all so I can't help you with your problem, but 
Daniel might be able to.

Duncan Murdoch



From kyong.ho.park at us.army.mil  Fri Jun 10 18:03:11 2005
From: kyong.ho.park at us.army.mil (Park, Kyong H Mr. RDECOM)
Date: Fri, 10 Jun 2005 12:03:11 -0400
Subject: [R] Abrupt shut down of R session
Message-ID: <E0466E6811B5BA46B92414C94516A7A425BC41@apgrb1rdg-mail2.nae.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/d1df1b69/attachment.pl

From charlie at stat.umn.edu  Fri Jun 10 18:08:10 2005
From: charlie at stat.umn.edu (Charles Geyer)
Date: Fri, 10 Jun 2005 11:08:10 -0500
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
In-Reply-To: <mailman.15.1118397601.5014.r-help@stat.math.ethz.ch>
References: <mailman.15.1118397601.5014.r-help@stat.math.ethz.ch>
Message-ID: <20050610160810.GA2531@stat.umn.edu>

> Message: 23
> Date: Thu, 9 Jun 2005 09:53:02 -0500
> From: <davidr at rhotrading.com>
> Subject: RE: [R] How to plot more than 3 sets in Venn Diagrams?
> To: "Tan Hui Hui Jenny" <medp9193 at nus.edu.sg>,
> 	<r-help at stat.math.ethz.ch>
> Message-ID:
> 	<12AE52872B5C5348BE5CF47C707FF53A5FB672 at rhosvr02.rhotrading.com>
> Content-Type: text/plain;	charset="us-ascii"
> 
> Here is a link to see what's possible. 
> http://www.combinatorics.org/Surveys/ds5/VennSymmEJC.html
> 
> Venn did 4 sets with ellipses (see wikipedia.)
> 
> (There was an article in the last year (I think) in one of my math
> journals that presented someone who made these complex Venn diagrams as
> artwork and sold, but I can't remember the person's name right now.)

The book is

    Cogwheels of the Mind : The Story of Venn Diagrams
    by A. W. F. Edwards (foreword by Ian Stewart)
    Hardcover: 136 pages
    Publisher: The Johns Hopkins University Press (April 26, 2004)
    ISBN: 0801874343

It is a really beautiful book.  There are also some journal articles
on the subject by Edwards and by Branko Grunbaum
(http://www.math.washington.edu/~grunbaum/BG228.pdf)

Edwards is my intellectual grandfather (thesis advisor's thesis advisor),
and it is a pleasure to recommend this book.  (A bit irrelevant to R,
I know, but still ...)



From helprhelp at gmail.com  Fri Jun 10 18:25:59 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 10 Jun 2005 11:25:59 -0500
Subject: [R] Rpy and RSPython
In-Reply-To: <20050610110455.GA7142@phenix.progiciels-bpi.ca>
References: <cdf8178305060919294586753e@mail.gmail.com>
	<20050610110455.GA7142@phenix.progiciels-bpi.ca>
Message-ID: <cdf8178305061009251a15c32@mail.gmail.com>

Hi,
I tried to install RSPython, here is my problem:
I load the env variables correctly as below:
#------------------------------
# R section
#------------------------------
R_HOME=/usr/lib/R
PYTHONPATH=${R_HOME}/library/RSPython/Python:${R_HOME}/library/RSPython/libs
LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${R_HOME}/bin

export R_HOME PYTHONPATH LD_LIBRARY_PATH

When I run python and do:
import RS

here is the error:
ImportError: /usr/lib/R/library/RSPython/libs/RSInternal.so: undefined
symbol: R_GlobalEnv

I googled and found I did not have $R_HOME/bin/libR.so since I
installed R from binary package.

My question is, should I have to re-install R from source? ( I think I
have to, but want to make sure here :)


When I tried to re-install R by following R installation and Admin
intro, and I run
./configure --enable-R-shlib
I got the following error:
configure: error: --with-x=yes (default) and X11 headers/libs are not available
I don't know why.

from R, 
version

platform i686-redhat-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

My third question:
can Rpy save this trouble? In my understanding, Rpy needs that libR.so
too. Am I right?

Many thanks for people using python and R.

weiwei

On 6/10/05, Fran??ois Pinard <pinard at iro.umontreal.ca> wrote:
> [Weiwei Shi]
> 
> > I am thinking to use one of them but not sure which one is better. I
> > think Rpy cannot call python from R while the PRPython can in
> > two-directional calling.  Am I right?
> 
> s/PRPython/RSPython/ ? :-)
> 
> This is also what I understood.  Yet, despite the uni-directionality of
> RPy, this is what I chose for my personal usage (probably more handy to
> use, or easy to install -- but the main point was that RPy guaranteed
> to be more stable and never crash!).  I think I recently read somewhere
> that they were plans for undusting RSPython, and then said to myself:
> "Should re-evaluate once done.".
> 
> Surely that for now, RPy is quite sufficient for my simple needs.
> 
> --
> Fran??ois Pinard   http://pinard.progiciels-bpi.ca
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From Brian.Beckage at uvm.edu  Fri Jun 10 18:49:18 2005
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Fri, 10 Jun 2005 12:49:18 -0400
Subject: [R] Fortran compilation error
Message-ID: <p06210202becf708e3211@[10.0.1.2]>

Hello,

I'm trying to install a package that requires a Fortran compiler 
(Hmisc) using R CMD INSTALL.  I downloaded the package source onto my 
Desktop, unzipped it, and then typed:
R CMD INSTALL /Users/brianbeckage/Desktop/Hmisc

* Installing *source* package 'Hmisc' ...
** libs
g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include 
-I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib 
-o Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o 
ranksort.o rcorr.o wclosest.o 
-L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 -lg2c -lSystem 
-lcc_dynamic -framework R
ld: warning -L: directory name 
(/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
ld: can't locate file for: -lg2c
make: *** [Hmisc.so] Error 1
ERROR: compilation failed for package 'Hmisc'
** Removing 
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/Hmisc'
** Restoring previous 
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/Hmisc'

Any ideas as to what file -lg2c represents and how I correct this? 
I'm running R 2.1.0 Patched under OSX 10.3.9.

Thanks,
Brian





-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From rwcitek at alum.calberkeley.org  Fri Jun 10 18:59:07 2005
From: rwcitek at alum.calberkeley.org (Robert Citek)
Date: Fri, 10 Jun 2005 11:59:07 -0500
Subject: [R] discovery (was: data.frame to character)
In-Reply-To: <42A98658.1020907@gmail.com>
References: <42A98658.1020907@gmail.com>
Message-ID: <39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>


How can one discover or list all available built-in objects?

On Jun 10, 2005, at 7:23 AM, Muhammad Subianto wrote:
>> L3 <- LETTERS[1:3]
>>  L10 <- LETTERS[1:10]

LETTERS is apparently a built-in character vector.  ls() and objects 
() only lists the ones I've created.  Is there a function that lists  
all available built-in objects?

For example, "pi" is another built-in, but "e" is not.  A means to  
list them would be nice.

Regards,
- Robert



From pkhomski at wiwi.uni-bielefeld.de  Fri Jun 10 19:00:56 2005
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Fri, 10 Jun 2005 19:00:56 +0200
Subject: [R] gc() and gc trigger
Message-ID: <42A9C748.30404@wiwi.uni-bielefeld.de>

hello,

the question concerning to the memory used and g.c. after having removed 
objects.  What is wrong?

bevor
-------

 > gc()
                   used   (Mb)     gc trigger       (Mb)     max 
used      (Mb)
Ncells   313142     8.4         1801024         48.1       1835812      
  49.1
Vcells    809238     6.2     142909728   1090.4   178426948   1361.3



hier  all  attached objects  detached and other  no more used ones  
removed; also the objects which could change their size (like 
.Traceback) are checked; nothing unusual
-----


after
------

 >  for (i in 1:30) gc()
 >  gc()
                      used    (Mb)     gc trigger      (Mb)      max 
used      (Mb)
Ncells     313149        8.4       1152655       30.8          1835812   
    49.1
Vcells      809261        6.2       3218039       19.7     178426948   
1361.3


 > object.size(mget(ls(all=T),envir=.GlobalEnv)) / 1024^2
[1] 9.829926


N.B.!!! the "max used "  is not put back

 > q()

after having restarted R-prozess
-------------------------------------

 > gc()
                   used    (Mb)      gc trigger   (Mb)     max used      
(Mb)
Ncells  302497        8.1           467875    12.5          350000       
  9.4
Vcells   785346        6.0         1193335      9.2           
923612        7.1



thnks for hint



ps: some more understandable information about how R manages the memory 
and to the output of gc(), specially about  gc trigger would be helpful



From dimitris.rizopoulos at med.kuleuven.be  Fri Jun 10 19:10:32 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 10 Jun 2005 19:10:32 +0200
Subject: [R] Replacing for loop with tapply!?
References: <42A9543E.3040901@oomvanlieshout.net><012901c56d9c$c6bfc6f0$0540210a@www.domain>
	<42A96709.3010406@oomvanlieshout.net>
Message-ID: <007001c56ddf$4f80c1a0$0540210a@www.domain>

for the maximum you could use something like:

ind[, 1] <- apply(mat, 2, max)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Sander Oom" <slist at oomvanlieshout.net>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, June 10, 2005 12:10 PM
Subject: Re: [R] Replacing for loop with tapply!?


> Thanks Dimitris,
>
> Very impressive! Much faster than before.
>
> Thanks to new found R.basic, I can simply rotate the result with
> rotate270{R.basic}:
>
> > mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
> > temps <- c(37, 39, 41)
> > #################
> > #ind <- matrix(0, length(temps), ncol(mat))
> > ind <- matrix(0, 4, ncol(mat))
> > (startDate <- date())
> [1] "Fri Jun 10 12:08:01 2005"
> > for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
> > ind[4, ] <- colMeans(max(mat))
> Error in colMeans(max(mat)) : 'x' must be an array of at least two
> dimensions
> > (endDate <- date())
> [1] "Fri Jun 10 12:08:02 2005"
> > ind <- rotate270(ind)
> > ind[1:10,]
>    V4 V3 V2 V1
> 1   0 56 75 80
> 2   0 46 53 60
> 3   0 50 58 67
> 4   0 60 72 80
> 5   0 59 68 76
> 6   0 55 67 74
> 7   0 62 77 93
> 8   0 45 57 67
> 9   0 57 68 75
> 10  0 61 66 76
>
> However, I have not managed to get the row maximum using your 
> method? It
> should be 50 for most rows, but my first guess code gives an error!
>
> Any suggestions?
>
> Sander
>
>
>
> Dimitris Rizopoulos wrote:
>> maybe you are looking for something along these lines:
>>
>> mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
>> temps <- c(37, 39, 41)
>> #################
>> ind <- matrix(0, length(temps), ncol(mat))
>> for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
>> ind
>>
>>
>> I hope it helps.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/16/336899
>> Fax: +32/16/337015
>> Web: http://www.med.kuleuven.ac.be/biostat/
>>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>
>>
>> ----- Original Message ----- 
>> From: "Sander Oom" <slist at oomvanlieshout.net>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Friday, June 10, 2005 10:50 AM
>> Subject: [R] Replacing for loop with tapply!?
>>
>>
>>>Dear all,
>>>
>>>We have a large data set with temperature data for weather stations
>>>across the globe (15000 stations).
>>>
>>>For each station, we need to calculate the number of days a certain
>>>temperature is exceeded.
>>>
>>>So far we used the following S code, where mat88 is a matrix
>>>containing
>>>rows of 365 daily temperatures for each of 15000 weather stations:
>>>
>>>m <- 37
>>>n <- 2
>>>outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
>>>for(i in 1:nrow(mat88)) {
>>># i <- 3
>>>row1 <- as.data.frame(df88[i,  ])
>>>temprow37 <- select.rows(row1, row1 > m)
>>>temprow39 <- select.rows(row1, row1 > m + n)
>>>temprow41 <- select.rows(row1, row1 > m + 2 * n)
>>>outmat88[i, 1] <- max(row1, na.rm = T)
>>>outmat88[i, 2] <- count.rows(temprow37)
>>>outmat88[i, 3] <- count.rows(temprow39)
>>>outmat88[i, 4] <- count.rows(temprow41)
>>>}
>>>outmat88
>>>
>>>We have transferred the data to a more potent Linux box running R,
>>>but
>>>still hope to speed up the code.
>>>
>>>I know a for loop should be avoided when looking for speed. I also
>>>know
>>>the answer is in something like tapply, but my understanding of
>>>these
>>>commands is still to limited to see the solution. Could someone 
>>>show
>>>me
>>>the way!?
>>>
>>>Thanks in advance,
>>>
>>>Sander.
>>>-- 
>>>--------------------------------------------
>>>Dr Sander P. Oom
>>>Animal, Plant and Environmental Sciences,
>>>University of the Witwatersrand
>>>Private Bag 3, Wits 2050, South Africa
>>>Tel (work)      +27 (0)11 717 64 04
>>>Tel (home)      +27 (0)18 297 44 51
>>>Fax             +27 (0)18 299 24 64
>>>Email   sander at oomvanlieshout.net
>>>Web     www.oomvanlieshout.net/sander
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>
> -- 
> --------------------------------------------
> Dr Sander P. Oom
> Animal, Plant and Environmental Sciences,
> University of the Witwatersrand
> Private Bag 3, Wits 2050, South Africa
> Tel (work)      +27 (0)11 717 64 04
> Tel (home)      +27 (0)18 297 44 51
> Fax             +27 (0)18 299 24 64
> Email   sander at oomvanlieshout.net
> Web     www.oomvanlieshout.net/sander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dgrove at fhcrc.org  Fri Jun 10 19:12:41 2005
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 10 Jun 2005 10:12:41 -0700 (PDT)
Subject: [R] discovery (was: data.frame to character)w
Message-ID: <Pine.LNX.4.58.0506101008100.11653@echidna.fhcrc.org>

Help pages are useful, you should try them

e.g. ?pi or ?LETTERS


> How can one discover or list all available built-in objects?

> On Jun 10, 2005, at 7:23 AM, Muhammad Subianto wrote:
> >> L3 <- LETTERS[1:3]
> >>  L10 <- LETTERS[1:10]

> LETTERS is apparently a built-in character vector.  ls() and objects 
> () only lists the ones I've created.  Is there a function that lists  
> all available built-in objects?

> For example, "pi" is another built-in, but "e" is not.  A means to  
> list them would be nice.

> Regards,
> - Robert



From murdoch at stats.uwo.ca  Fri Jun 10 19:13:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 10 Jun 2005 13:13:50 -0400
Subject: [R] Abrupt shut down of R session
In-Reply-To: <E0466E6811B5BA46B92414C94516A7A425BC41@apgrb1rdg-mail2.nae.ds.army.mil>
References: <E0466E6811B5BA46B92414C94516A7A425BC41@apgrb1rdg-mail2.nae.ds.army.mil>
Message-ID: <42A9CA4E.8010008@stats.uwo.ca>

On 6/10/2005 12:03 PM, Park, Kyong H Mr. RDECOM wrote:
> Dear R users,
> I'm using R 2.1.0 with Windows 2000. In the middle of a R session, an error
> message pops up saying Rgui.exe generated errors and shuts down the R
> session. How can I correct this problem? Appreciate your help.

Make it reproducible, and tell us how you did.  Then we'll try to fix it 
if it's a bug in R, or tell you who else to complain to if it's a bug 
somewhere else, or tell you what you're doing wrong if you're to blame.
But without more information it's unlikely anyone could help you.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Jun 10 19:19:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 10 Jun 2005 18:19:49 +0100 (BST)
Subject: [R] discovery (was: data.frame to character)
In-Reply-To: <39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
References: <42A98658.1020907@gmail.com>
	<39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
Message-ID: <Pine.LNX.4.61.0506101817050.15048@gannet.stats>

On Fri, 10 Jun 2005, Robert Citek wrote:

> How can one discover or list all available built-in objects?

sort(unlist(lapply(search()[-1], ls)))

would be a good start, but with over 2000 what are you going to do with 
the information?  (Actually, there are more ...).

> LETTERS is apparently a built-in character vector.  ls() and objects
> () only lists the ones I've created.  Is there a function that lists
> all available built-in objects?
>
> For example, "pi" is another built-in, but "e" is not.  A means to
> list them would be nice.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Jun 10 19:20:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Jun 2005 13:20:18 -0400
Subject: [R] discovery (was: data.frame to character)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E981@usctmx1106.merck.com>

> From: Robert Citek
> 
> How can one discover or list all available built-in objects?
>
> On Jun 10, 2005, at 7:23 AM, Muhammad Subianto wrote:
> >> L3 <- LETTERS[1:3]
> >>  L10 <- LETTERS[1:10]
> 
> LETTERS is apparently a built-in character vector.  ls() and objects 
> () only lists the ones I've created.  Is there a function that lists  
> all available built-in objects?

ls(pos="package:base") lists all objects that are user-visible in the base
package.  Do that for any package on the search path.  You might only be
interested in those that are not functions, though.
 
> For example, "pi" is another built-in, but "e" is not.  A means to  
> list them would be nice.

Another useful thing is find(); e.g., find("LETTERS").

Andy
 
> Regards,
> - Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From Achim.Zeileis at wu-wien.ac.at  Fri Jun 10 19:21:14 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 10 Jun 2005 19:21:14 +0200
Subject: [R] discovery (was: data.frame to character)
In-Reply-To: <39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
References: <42A98658.1020907@gmail.com>
	<39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
Message-ID: <20050610192114.6be585d2.Achim.Zeileis@wu-wien.ac.at>

On Fri, 10 Jun 2005 11:59:07 -0500 Robert Citek wrote:

> How can one discover or list all available built-in objects?

Depends what you mean with `built-in'...
 
> On Jun 10, 2005, at 7:23 AM, Muhammad Subianto wrote:
> >> L3 <- LETTERS[1:3]
> >>  L10 <- LETTERS[1:10]
> 
> LETTERS is apparently a built-in character vector.  ls() and objects 
> () only lists the ones I've created. 

More precisely, it lists the objects in the global environment.

> Is there a function that lists  all available built-in objects?

R> search()

will show you the search path and in particular all packages that are
currently attached. Using

R> ls("package:base")

you could list all objects in the base packages which includes LETTERS.

> For example, "pi" is another built-in, but "e" is not. 

But "exp" is. So 

R> exp(1)

will give you "e".
Z

> A means to  
> list them would be nice.
> 
> Regards,
> - Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From drf5n at maplepark.com  Fri Jun 10 19:23:42 2005
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 10 Jun 2005 12:23:42 -0500 (CDT)
Subject: [R] How to plot more than 3 sets in Venn Diagrams?
In-Reply-To: <42AA1A72.4080507@ozemail.com.au>
References: <16CDECA355E2FD48A3C5A51E31A78E4D05E964@MBOX23.stu.nus.edu.sg>
	<42AA1A72.4080507@ozemail.com.au>
Message-ID: <Pine.LNX.4.58.0506101217570.21844@maplepark.com>

On Fri, 10 Jun 2005, Jim Lemon wrote:

> Tan Hui Hui Jenny wrote:
> > I'm trying to plot Venn diagrams with more than 3 sets (5 actually) in order to describe graphically the genetic variation between populations.
> >
> > I tried the limma library but realised it can only plot 3 sets.
> >
> > Is there any solution? Of course I could plot the chart manually but it'll take too long (have other datasets)..... One of my dataset is given below.
> >
> Hi Jenny,
>
> Best I could do is four sets. The code is a bit rough but consistent
> with the vennDiagram function in the limma package.
>
> Jim
>

There's a pretty 5-set radially symmetric Venn diagram on
http://www.combinatorics.org/Surveys/ds5/VennSymmEJC.html

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From edd at debian.org  Fri Jun 10 19:21:00 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 10 Jun 2005 17:21:00 +0000 (UTC)
Subject: [R] rgl.snapshot "failed"
References: <3256A5D0-52E9-47B1-850E-DE71D4198476@uiuc.edu>
Message-ID: <loom.20050610T191731-255@post.gmane.org>


Roger,

roger koenker <rkoenker <at> uiuc.edu> writes:
> I've installed the rgl package on a Suse x86-64 machine (further  
> details below)
> and it produces nice screen images.  Unfortunately, rgl.snapshot   
> attempts to
> make png files produces only the response "failed".  For other  
> graphics png()
> works fine, and capabilities indicates that it is there.  If anyone  
> has a suggestion
> of what might be explored at this point I'd be very appreciative.

We have the issue as an open Debian bug report against rgl, and I
have corresponded a little with Daniel on it.  

Unfortunately, there is no solution in sight as far as I can tell. 

Regards, Dirk



From ggrothendieck at gmail.com  Fri Jun 10 19:29:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 13:29:10 -0400
Subject: [R] discovery (was: data.frame to character)
In-Reply-To: <39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
References: <42A98658.1020907@gmail.com>
	<39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
Message-ID: <971536df050610102911e56690@mail.gmail.com>

On 6/10/05, Robert Citek <rwcitek at alum.calberkeley.org> wrote:
> 
> How can one discover or list all available built-in objects?
> 
> On Jun 10, 2005, at 7:23 AM, Muhammad Subianto wrote:
> >> L3 <- LETTERS[1:3]
> >>  L10 <- LETTERS[1:10]
> 
> LETTERS is apparently a built-in character vector.  ls() and objects
> () only lists the ones I've created.  Is there a function that lists
> all available built-in objects?
> 
> For example, "pi" is another built-in, but "e" is not.  A means to
> list them would be nice.
> 

As has been pointed out, getting all objects involves a very long
list.  Since most of them will be of mode "function" we might want to ask 
for just objects that are of mode "character" or just objects that are 
mode "numeric":

apropos("", "character")
apropos("", "numeric")



From murdoch at stats.uwo.ca  Fri Jun 10 19:28:28 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 10 Jun 2005 13:28:28 -0400
Subject: [R] discovery
In-Reply-To: <39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
References: <42A98658.1020907@gmail.com>
	<39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
Message-ID: <42A9CDBC.4040902@stats.uwo.ca>

On 6/10/2005 12:59 PM, Robert Citek wrote:
> How can one discover or list all available built-in objects?

search() tells you all the attached environments.

ls(n) gives you a list of what is in the n'th one.

The 1300+ page R Reference Manual gives you all the help pages for base 
packages.  Usually library(help="package") will give you some sort of 
list of the useful things in a particular package.

Duncan Murdoch



From ggrothendieck at gmail.com  Fri Jun 10 19:30:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 13:30:38 -0400
Subject: [R] discovery (was: data.frame to character)
In-Reply-To: <971536df050610102911e56690@mail.gmail.com>
References: <42A98658.1020907@gmail.com>
	<39FDAD7D-80B1-40F2-AB22-5EBA92F79817@alum.calberkeley.org>
	<971536df050610102911e56690@mail.gmail.com>
Message-ID: <971536df0506101030f6a8d5b@mail.gmail.com>

On 6/10/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/10/05, Robert Citek <rwcitek at alum.calberkeley.org> wrote:
> >
> > How can one discover or list all available built-in objects?
> >
> > On Jun 10, 2005, at 7:23 AM, Muhammad Subianto wrote:
> > >> L3 <- LETTERS[1:3]
> > >>  L10 <- LETTERS[1:10]
> >
> > LETTERS is apparently a built-in character vector.  ls() and objects
> > () only lists the ones I've created.  Is there a function that lists
> > all available built-in objects?
> >
> > For example, "pi" is another built-in, but "e" is not.  A means to
> > list them would be nice.
> >
> 
> As has been pointed out, getting all objects involves a very long
> list.  Since most of them will be of mode "function" we might want to ask
> for just objects that are of mode "character" or just objects that are
> mode "numeric":
> 
> apropos("", "character")
> apropos("", "numeric")
> 

Sorry, that should be:

apropos("", mode = "character")
apropos("", mode = "numeric")



From Brian.Beckage at uvm.edu  Fri Jun 10 19:33:16 2005
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Fri, 10 Jun 2005 13:33:16 -0400
Subject: [R]  Solved: Fortran compilation error
In-Reply-To: <p06210202becf708e3211@[10.0.1.2]>
References: <p06210202becf708e3211@[10.0.1.2]>
Message-ID: <p06210203becf7e6670a0@[10.0.1.2]>

I solved this problem by installing a newer version of the fortran 
(g77 3.4) compiler from hpc.sourceforge.net into usr/local.

Best,
Brian


At 12:49 PM -0400 6/10/05, Brian Beckage wrote:
>Hello,
>
>I'm trying to install a package that requires a Fortran compiler
>(Hmisc) using R CMD INSTALL.  I downloaded the package source onto my
>Desktop, unzipped it, and then typed:
>R CMD INSTALL /Users/brianbeckage/Desktop/Hmisc
>
>* Installing *source* package 'Hmisc' ...
>** libs
>g77   -fno-common  -g -O2 -c cidxcn.f -o cidxcn.o
>g77   -fno-common  -g -O2 -c cidxcp.f -o cidxcp.o
>g77   -fno-common  -g -O2 -c hoeffd.f -o hoeffd.o
>g77   -fno-common  -g -O2 -c jacklins.f -o jacklins.o
>g77   -fno-common  -g -O2 -c largrec.f -o largrec.o
>gcc-3.3 -no-cpp-precomp
>-I/Library/Frameworks/R.framework/Resources/include
>-I/usr/local/include   -fno-common  -g -O2 -c ranksort.c -o ranksort.o
>g77   -fno-common  -g -O2 -c rcorr.f -o rcorr.o
>g77   -fno-common  -g -O2 -c wclosest.f -o wclosest.o
>gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib
>-o Hmisc.so cidxcn.o cidxcp.o hoeffd.o jacklins.o largrec.o
>ranksort.o rcorr.o wclosest.o
>-L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 -lg2c -lSystem
>-lcc_dynamic -framework R
>ld: warning -L: directory name
>(/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
>ld: can't locate file for: -lg2c
>make: *** [Hmisc.so] Error 1
>ERROR: compilation failed for package 'Hmisc'
>** Removing
>'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/Hmisc'
>** Restoring previous
>'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/Hmisc'
>
>Any ideas as to what file -lg2c represents and how I correct this?
>I'm running R 2.1.0 Patched under OSX 10.3.9.
>
>Thanks,
>Brian
>
>
>
>
>
>--
>*********************************************************************
>Brian Beckage
>Department of Botany
>University of Vermont
>Marsh Life Science Building
>Burlington, VT 05405
>
>Phone:  802 656-0197
>Fax  :  802 656-0440
>email:  Brian.Beckage at uvm.edu
>web  :  www.uvm.edu/~bbeckage
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From ramasamy at cancer.org.uk  Fri Jun 10 19:35:02 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 10 Jun 2005 18:35:02 +0100
Subject: [R] ANOVA vs REML approach to variance component estimation
Message-ID: <1118424903.7638.50.camel@ipc143019.lif.icnet.uk>

Can anyone verify my calculations below or explain why they are wrong ?

I have several animals that were measured thrice. The only blocking
variable is the animal itself. I am interested in calculating the 
between and within object variations in R. An artificial example :

y <- c( 2.2, -1.4, -0.5,  # animal 1
       -0.3  -2.1   1.5,  # animal 2
        1.3  -0.3   0.5,  # animal 3
       -1.4  -0.2   1.8)  # animal 4
ID <- factor( rep(1:4, each=3) )


1) Using the ANOVA method

  summary(aov( y ~ ID ))
              Df Sum Sq Mean Sq F value Pr(>F)
  ID           3  0.900   0.300  0.1207 0.9453
  Residuals    8 19.880   2.485               

  => within animal  variation  = 2.485
  => between animal variation  = (0.300 - 2.485)/3 = -0.7283

I am aware that ANOVA can give negative estimates for variances. Is this
such a case or have I coded wrongly ?


2) Using the REML approach 

  library(nlme)
  lme( y ~ 1, rand = ~ 1 | ID)
   ....
  Random effects:
  Formula: ~1 | ID
          (Intercept) Residual
  StdDev:  0.01629769 1.374438

  => within animal variation  = 1.374438^2 = 1.88908
  => between animal variation = 0.01629769^2 = 0.0002656147

Is this the correct way of coding for this problem ? I do not have
access to a copy of Pinheiro & Bates at the moment.

Thank you very much in advance.

Regards, Adai



From graumann at caltech.edu  Fri Jun 10 19:39:36 2005
From: graumann at caltech.edu (Johannes Graumann)
Date: Fri, 10 Jun 2005 10:39:36 -0700
Subject: [R] Linear regression with variation in x and y
Message-ID: <1118425176.22433.5.camel@localhost>

Hello,

According to my archive digging, this issue comes up again and again. I
was just wondering whether somebody had code lying around for a Deming
Regression or a MLFR method for this, which I might peruse. I came
across a post from Brian Ripley stating that something like it was close
to be included into MASS a couple of times and was wondering whether it
made the cut there (or elsewhere) in the meantime ...

Any hint is greatly appreciated.

Joh



From tlumley at u.washington.edu  Fri Jun 10 19:41:15 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 10 Jun 2005 10:41:15 -0700 (PDT)
Subject: [R] gc() and gc trigger
In-Reply-To: <42A9C748.30404@wiwi.uni-bielefeld.de>
References: <42A9C748.30404@wiwi.uni-bielefeld.de>
Message-ID: <Pine.A41.4.61b.0506101008270.331934@homer05.u.washington.edu>

On Fri, 10 Jun 2005, Pavel Khomski wrote:

> hello,
>
> the question concerning to the memory used and g.c. after having removed
> objects.  What is wrong?

Nothing is wrong.

> bevor
> -------
>
> > gc()
>                   used   (Mb)     gc trigger       (Mb)     max used      (Mb)
> Ncells   313142     8.4         1801024         48.1       1835812  49.1
> Vcells    809238     6.2     142909728   1090.4   178426948   1361.3
>

>
> >  for (i in 1:30) gc()
> >  gc()
>                      used    (Mb)     gc trigger      (Mb)      max used      (Mb)
> Ncells     313149        8.4       1152655       30.8          1835812    49.1
> Vcells      809261        6.2       3218039       19.7     178426948  1361.3
>

The gc trigger level is the memory use at which gc() will be triggered. It 
goes down as R releases memory back to the operating system.  The amount 
used doesn't change because you aren't changing the amount used.

The max used doesn't change because, as the help says, it is the maximum 
usage since the last call to gc(reset=TRUE).


> > object.size(mget(ls(all=T),envir=.GlobalEnv)) / 1024^2
> [1] 9.829926

I'm not sure what this is supposed to prove.

>
> N.B.!!! the "max used "  is not put back
>
> > q()
>
> after having restarted R-prozess
> -------------------------------------
>
> > gc()
>                   used    (Mb)      gc trigger   (Mb)     max used (Mb)
> Ncells  302497        8.1           467875    12.5          350000  9.4
> Vcells   785346        6.0         1193335      9.2   923612        7.1
>

So now you start off with 14.1 Mb used.  The max used column shows that 
sometime during startup R was using more memory, and the gc trigger shows 
that garbage collection will happen when you get above 12.5 Mb Ncells or
9.2 Mb Vcells.


 	-thomas



From drf5n at maplepark.com  Fri Jun 10 19:44:32 2005
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 10 Jun 2005 12:44:32 -0500 (CDT)
Subject: [R] Plot/manage spatial boundary data
In-Reply-To: <Pine.LNX.4.44.0506100855290.5742-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0506100855290.5742-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.58.0506101225330.21844@maplepark.com>

On Fri, 10 Jun 2005, Roger Bivand wrote:

> On Thu, 9 Jun 2005, David Forrest wrote:
>
> > I have some disconnected boundary data from a finite element ocean model
> > and I'd like to make a plot.
> >
> > Maptools looks promising, but since my data is not in a shapefile or a
> > map, I'm unclear on what the best way to approach the problem.
>
> If the line segments are unconnected, you will first need to establish
> (short) lists saying which segments (in which order and direction) bound
> each polygon to be filled with colour when plotting. A package you can
> consider for "rolling your own" spatial rings is sp, which is on CRAN.

The segments are largely connected -- my domain is bounded by a list of
open boundary segments, land boundary segments, and several islands.
library(sp) looks like the proper tool.

> Once you have the list describing segment membership, order and direction
> for each ring, building a SpatialRings object is not difficult. The hard
> bit is going from spaghetti line segments to the list imposing order.
>
> Alternatively, the PBSmapping package may have suitable functions for
> coersing polySet objects into rings. If your coordinates were in the
> Pacific, I'd say PBSmapping might already have what you need, but your
> example coordinates are Atlantic.
>
> (Could I suggest moving this discussion to R-sig-geo, referenced in the
> "Spatial" Task View on CRAN (top left corner in navigation bar)?

Sure.  I just subscribed.

How do you find the Task Views?  I found the mailing list off of
http://www.r-project.org/ -- Mailing Lists and the CRAN link brings up
mirrors.

Doh! I just answered my own question; these are different:

   http://www.r-project.org/
   http://cran.r-project.org/

I thought they were simple mirrors of the homepage for downloading, but
the menus are different.

The page at http://cran.r-project.org/ Task Views / Spatial
http://cran.r-project.org/src/contrib/Views/Spatial.html looks like just
what I need.  Thanks again.

> >
> > >geom[1:10,]
> >          lon      lat  depth
> > 1  -75.42481 35.58192 16.172
> > 2  -75.40726 35.58567 18.045
> > 3  -75.41351 35.60312 17.333
> > 4  -75.38888 35.58959 20.787
> > 5  -75.39495 35.60706 19.834
> > 6  -75.36964 35.59370 20.950
> > 7  -75.37556 35.61159 20.941
> > 8  -75.35530 35.61660 23.107
> > 9  -75.34950 35.59800 22.960
> > 10 -75.33418 35.62194 23.934
> >
> > >island1<-c(2,3,4,2)
> > >water<-c(1,3,5,7,8,10)
> > > land<-c(1,2,4,6,9,10)
> > > plot(geom$lon[land],geom$lat[land],pch='.',t='l')
> >  lines(geom$lon[water],geom$lat[water],pch='.',t='l',col="blue")
> >  lines(geom$lon[island1],geom$lat[island1],pch='.',t='l',col="green")
> >
> > The above is toy-sized: dim(geom) is on the order of 120000,3 and there
> > are about 30 different islands.  Maptools seems devoted to shapefiles,
> > and it is unclear how to create 'polylists'.
> >
> > Is there a good way to manage and graph data defined on irregular grids?
> >
> > Dave

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From khobson at fd9ns01.okladot.state.ok.us  Fri Jun 10 19:53:41 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Fri, 10 Jun 2005 12:53:41 -0500
Subject: [R] Default Format for Dates?
Message-ID: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>





Is there anyway to preset date formats?  I have a date from a cover.dbf
that is shown as this:
> cover$FINALREPOR
[1] "2003-06-24"

The numeric value in cover$FINALREPOR is 12227.  I'd rather not create
another vector to hold the properly formatted date.

When I put this in a WordPerfect merge, I want the date to be June 24,
2003.   I could take care of the problem in a WordPerfect macro but I'd
rather do it as an R default date format if possible.

I researched some of this in the archives, FAQ's and manuals but didn't
find anything that met my need.  I would imagine that this type of question
is probably an FAQ but I didn't find it.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From HankeA at mar.dfo-mpo.gc.ca  Fri Jun 10 19:53:05 2005
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Fri, 10 Jun 2005 14:53:05 -0300
Subject: [R] Estimate of baseline hazard in survival
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE02008651@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/6c922261/attachment.pl

From slist at oomvanlieshout.net  Fri Jun 10 20:05:57 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Fri, 10 Jun 2005 20:05:57 +0200
Subject: [R] Replacing for loop with tapply!?
In-Reply-To: <007001c56ddf$4f80c1a0$0540210a@www.domain>
References: <42A9543E.3040901@oomvanlieshout.net><012901c56d9c$c6bfc6f0$0540210a@www.domain>	<42A96709.3010406@oomvanlieshout.net>
	<007001c56ddf$4f80c1a0$0540210a@www.domain>
Message-ID: <42A9D685.5050806@oomvanlieshout.net>

Dear all,

Dimitris and Andy, thanks for your great help. I have progressed to the 
following code which runs very fast and effective:

mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)
mat[mat>45] <- NA
mat<-NA
mat
temps <- c(35, 37, 39)
ind <- rbind(
     t(sapply(temps, function(temp)
       rowSums(mat > temp, na.rm=TRUE) )),
     rowSums(!is.na(mat), na.rm=FALSE),
     apply(mat, 1, max, na.rm=TRUE))
ind <- t(ind)
ind

However, some weather stations have missing values for the whole year. 
Unfortunately, the code breaks down (when uncommenting mat<-NA).

I have tried 'ifelse' statements in the functions, but it becomes even 
more of a mess. I could subset the matrix before hand, but this would 
mean merging with a complete matrix afterwards to make it compatible 
with other years. That would slow things down.

How can I make the code robust for rows containing all missing values?

Thanks for your help,

Sander.

Dimitris Rizopoulos wrote:
> for the maximum you could use something like:
> 
> ind[, 1] <- apply(mat, 2, max)
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> 
> ----- Original Message ----- 
> From: "Sander Oom" <slist at oomvanlieshout.net>
> To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Friday, June 10, 2005 12:10 PM
> Subject: Re: [R] Replacing for loop with tapply!?
> 
> 
>>Thanks Dimitris,
>>
>>Very impressive! Much faster than before.
>>
>>Thanks to new found R.basic, I can simply rotate the result with
>>rotate270{R.basic}:
>>
>>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
>>>temps <- c(37, 39, 41)
>>>#################
>>>#ind <- matrix(0, length(temps), ncol(mat))
>>>ind <- matrix(0, 4, ncol(mat))
>>>(startDate <- date())
>>[1] "Fri Jun 10 12:08:01 2005"
>>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
>>>ind[4, ] <- colMeans(max(mat))
>>Error in colMeans(max(mat)) : 'x' must be an array of at least two
>>dimensions
>>>(endDate <- date())
>>[1] "Fri Jun 10 12:08:02 2005"
>>>ind <- rotate270(ind)
>>>ind[1:10,]
>>   V4 V3 V2 V1
>>1   0 56 75 80
>>2   0 46 53 60
>>3   0 50 58 67
>>4   0 60 72 80
>>5   0 59 68 76
>>6   0 55 67 74
>>7   0 62 77 93
>>8   0 45 57 67
>>9   0 57 68 75
>>10  0 61 66 76
>>
>>However, I have not managed to get the row maximum using your 
>>method? It
>>should be 50 for most rows, but my first guess code gives an error!
>>
>>Any suggestions?
>>
>>Sander
>>
>>
>>
>>Dimitris Rizopoulos wrote:
>>>maybe you are looking for something along these lines:
>>>
>>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
>>>temps <- c(37, 39, 41)
>>>#################
>>>ind <- matrix(0, length(temps), ncol(mat))
>>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
>>>ind
>>>
>>>
>>>I hope it helps.
>>>
>>>Best,
>>>Dimitris
>>>
>>>----
>>>Dimitris Rizopoulos
>>>Ph.D. Student
>>>Biostatistical Centre
>>>School of Public Health
>>>Catholic University of Leuven
>>>
>>>Address: Kapucijnenvoer 35, Leuven, Belgium
>>>Tel: +32/16/336899
>>>Fax: +32/16/337015
>>>Web: http://www.med.kuleuven.ac.be/biostat/
>>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>>
>>>
>>>----- Original Message ----- 
>>>From: "Sander Oom" <slist at oomvanlieshout.net>
>>>To: <r-help at stat.math.ethz.ch>
>>>Sent: Friday, June 10, 2005 10:50 AM
>>>Subject: [R] Replacing for loop with tapply!?
>>>
>>>
>>>>Dear all,
>>>>
>>>>We have a large data set with temperature data for weather stations
>>>>across the globe (15000 stations).
>>>>
>>>>For each station, we need to calculate the number of days a certain
>>>>temperature is exceeded.
>>>>
>>>>So far we used the following S code, where mat88 is a matrix
>>>>containing
>>>>rows of 365 daily temperatures for each of 15000 weather stations:
>>>>
>>>>m <- 37
>>>>n <- 2
>>>>outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
>>>>for(i in 1:nrow(mat88)) {
>>>># i <- 3
>>>>row1 <- as.data.frame(df88[i,  ])
>>>>temprow37 <- select.rows(row1, row1 > m)
>>>>temprow39 <- select.rows(row1, row1 > m + n)
>>>>temprow41 <- select.rows(row1, row1 > m + 2 * n)
>>>>outmat88[i, 1] <- max(row1, na.rm = T)
>>>>outmat88[i, 2] <- count.rows(temprow37)
>>>>outmat88[i, 3] <- count.rows(temprow39)
>>>>outmat88[i, 4] <- count.rows(temprow41)
>>>>}
>>>>outmat88
>>>>
>>>>We have transferred the data to a more potent Linux box running R,
>>>>but
>>>>still hope to speed up the code.
>>>>
>>>>I know a for loop should be avoided when looking for speed. I also
>>>>know
>>>>the answer is in something like tapply, but my understanding of
>>>>these
>>>>commands is still to limited to see the solution. Could someone 
>>>>show
>>>>me
>>>>the way!?
>>>>
>>>>Thanks in advance,
>>>>
>>>>Sander.
>>>>--



From ggrothendieck at gmail.com  Fri Jun 10 20:18:04 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 14:18:04 -0400
Subject: [R] Default Format for Dates?
In-Reply-To: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>
References: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>
Message-ID: <971536df05061011184945cde0@mail.gmail.com>

On 6/10/05, khobson at fd9ns01.okladot.state.ok.us
<khobson at fd9ns01.okladot.state.ok.us> wrote:
> 
> 
> 
> 
> Is there anyway to preset date formats?  I have a date from a cover.dbf
> that is shown as this:
> > cover$FINALREPOR
> [1] "2003-06-24"
> 
> The numeric value in cover$FINALREPOR is 12227.  I'd rather not create
> another vector to hold the properly formatted date.
> 
> When I put this in a WordPerfect merge, I want the date to be June 24,
> 2003.   I could take care of the problem in a WordPerfect macro but I'd
> rather do it as an R default date format if possible.
> 

I am not entirely sure I understand what you are asking but I assume
you want a Date variable such that print, format and as.character,
when applied to it, produce default output of a prespecified format.

Although I do not think Date will do that, chron can
associate a default format with a chron variable.

library(chron)

# custom format for a dates (i.e. chron) object 
my.format <- function(x) format(as.Date(dates(x)), "%B %d %Y")

# test data
my.Date <- Sys.Date() + 0:9

# convert to chron and associate my.format to it
my.chron <- chron(unclass(my.Date), out.format = my.format)

print(my.chron)
my.chron
as.character(my.chron)
format(my.chron)

Note that you can find more information on chron and various
conversions in my article in RNews 4/1 and especially the table 
at the end of that article.



From rpeng at jhsph.edu  Fri Jun 10 20:31:11 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 10 Jun 2005 14:31:11 -0400
Subject: [R] Default Format for Dates?
In-Reply-To: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>
References: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>
Message-ID: <42A9DC6F.7090106@jhsph.edu>

Is something like this what you want?

x <- as.Date("2003-06-24")
format(x, "%B %d %Y")

or perhaps

as.character(x, "%B %d %Y")

-roger

khobson at fd9ns01.okladot.state.ok.us wrote:
> 
> 
> 
> Is there anyway to preset date formats?  I have a date from a cover.dbf
> that is shown as this:
> 
>>cover$FINALREPOR
> 
> [1] "2003-06-24"
> 
> The numeric value in cover$FINALREPOR is 12227.  I'd rather not create
> another vector to hold the properly formatted date.
> 
> When I put this in a WordPerfect merge, I want the date to be June 24,
> 2003.   I could take care of the problem in a WordPerfect macro but I'd
> rather do it as an R default date format if possible.
> 
> I researched some of this in the archives, FAQ's and manuals but didn't
> find anything that met my need.  I would imagine that this type of question
> is probably an FAQ but I didn't find it.
> 
> mailto:khobson at odot.org
> Kenneth Ray Hobson, P.E.
> Oklahoma DOT - QA & IAS Manager
> 200 N.E. 21st Street
> Oklahoma City, OK  73105-3204
> (405) 522-4985, (405) 522-0552 fax
> 
> Visit our website at:
> http://www.okladot.state.ok.us/materials/materials.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From sfalcon at fhcrc.org  Fri Jun 10 20:34:54 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 10 Jun 2005 11:34:54 -0700
Subject: [R] Rpy and RSPython
In-Reply-To: <cdf8178305061009251a15c32@mail.gmail.com> (Weiwei Shi's message
	of "Fri, 10 Jun 2005 11:25:59 -0500")
References: <cdf8178305060919294586753e@mail.gmail.com>
	<20050610110455.GA7142@phenix.progiciels-bpi.ca>
	<cdf8178305061009251a15c32@mail.gmail.com>
Message-ID: <m2y89ikqlt.fsf@macaroni.local>

On 10 Jun 2005, helprhelp at gmail.com wrote:

> Hi,
> I tried to install RSPython, here is my problem:
> I load the env variables correctly as below:
> #------------------------------
> # R section
> #------------------------------
> R_HOME=/usr/lib/R
> PYTHONPATH=${R_HOME}/library/RSPython/Python:${R_HOME}/library/RSPython/libs
> LD_LIBRARY_PATH=$LD_LIBRARY_PATH:${R_HOME}/bin
>
> export R_HOME PYTHONPATH LD_LIBRARY_PATH
>
> When I run python and do:
> import RS
>
> here is the error: ImportError:
> /usr/lib/R/library/RSPython/libs/RSInternal.so: undefined symbol:
> R_GlobalEnv
>
> I googled and found I did not have $R_HOME/bin/libR.so since I
> installed R from binary package.
>
> My question is, should I have to re-install R from source? ( I think
> I have to, but want to make sure here :)
>
>
> When I tried to re-install R by following R installation and Admin
> intro, and I run ./configure --enable-R-shlib I got the following
> error: configure: error: --with-x=yes (default) and X11 headers/libs
> are not available I don't know why.

Presumably because you don't have the X11 devel headers available.
For an rpm system, look for packages like x11-devel or some such.  

> My third question: can Rpy save this trouble? In my understanding,
> Rpy needs that libR.so too. Am I right?

Rpy also needs libR.so.  So regardless, your first task is to setup up
your system for compiling R from sources so you can get the .so.  It
shouldn't be too bad.  Pay attention to the output of configure.  When
it complains about things, try to find a PKGNAME-devel.rpm package to
install and try again.

Best,

+ seth



From tlumley at u.washington.edu  Fri Jun 10 20:47:46 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 10 Jun 2005 11:47:46 -0700 (PDT)
Subject: [R] Estimate of baseline hazard in survival
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE02008651@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE02008651@msgmarsta01.bio.dfo.ca>
Message-ID: <Pine.A41.4.61b.0506101139180.331934@homer05.u.washington.edu>

On Fri, 10 Jun 2005, Hanke, Alex wrote:

> Dear All,
> I'm having just a little terminology problem, relating the language used in
> the Hosmer and Lemeshow text on Applied Survival Analysis to that of the
> help that comes with the survival package.
>
> I am trying to back out the values for the baseline hazard, h_o(t_i), for
> each event time or observation time.
> Now survfit(fit)$surv gives me the value of the  survival function,
> S(t_i|X_i,B), using mean values of the covariates and the coxph() object
> provides me with the estimate of the linear predictors, exp(X'B).
> If S(t_i|X_i,B)=S_o(t_i)^exp(X_iB) is the expression for the survival
> function
> And
> -ln(S_o(t_i) ) is the expression for the cumulative baseline hazard
> function, H_o(t_i)
> Then by rearranging the expression for the survival function I get the
> following:
> -ln(S_o(t_i) ) = -ln( S(t_i|X_i,B) ) / exp(X_iB)
>                   = basehaz(fit)/exp(fit$linear.predictors)
> Am I right so far and is there an easier way?

No, and yes.

You are dividing the centered baseline hazard at each time point by the 
linear predictor for the person who happened to die at that time, rather 
than the linear predictor at the mean covariates.

basehaz(fit, centered=FALSE) will get you the baseline hazard at zero 
covariates.

You don't even need that.  The baseline hazard at zero covariates is 
constant if and only if the centered baseline hazard is constant, so you 
could also work with basehaz(fit), which is often more numerically stable.

> The plot of the cumulative baseline hazard function , H_o(t_i), should be
> linear across time. Once I have, H_o(t_i),   to get at h_o(t_i) I then need
> to reverse the cumsum operation. The corresponding plot should have a
> constant baseline hazard over time.

No. Not at all.

Unless you smooth the h_0(t_i) they are completely useless for what you 
want.

Suppose the hazard rate is constant and you have no covariates in the 
model and not even any censoring. In that case the increments of the 
baseline hazard are 1/n, 1/(n-1), 1/(n-2),..., 1/2, 1, where n is the 
sample size.  So in this simplest possible cause a constant baseline 
hazard rate leads to h_0(t_i) increasing with t.

The proper smoothing is a little tricky, because the failure distribution 
is skewed and has a boundary at zero, and because of censoring.  That's 
why textbooks often recommend graphing the cumulative hazard to see if it 
is linear rather than the increments in the cumulative hazard to see if 
they are constant.


 	-thomas



From ccleland at optonline.net  Fri Jun 10 21:10:10 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 10 Jun 2005 15:10:10 -0400
Subject: [R] ANOVA vs REML approach to variance component estimation
In-Reply-To: <1118424903.7638.50.camel@ipc143019.lif.icnet.uk>
References: <1118424903.7638.50.camel@ipc143019.lif.icnet.uk>
Message-ID: <42A9E592.5050201@optonline.net>

   They look fine to me.  Also, note varcomp() in the ape package and 
VarCorr() in the nlme package.  I think in this case the ANOVA estimate 
of the intercept variance component is negative because the true value 
is close to zero.

 > y <- c( 2.2, -1.4, -0.5,  # animal 1
+        -0.3, -2.1,  1.5,  # animal 2
+         1.3, -0.3,  0.5,  # animal 3
+        -1.4, -0.2,  1.8)  # animal 4

 > ID <- factor( rep(1:4, each=3) )

 > library(nlme)
 > library(ape)

 > summary(aov(y ~ ID))
             Df  Sum Sq Mean Sq F value Pr(>F)
ID           3  0.9625  0.3208  0.1283 0.9406
Residuals    8 20.0067  2.5008

 > (0.3208 - 2.5008) / 3
[1] -0.7266667

 > varcomp(lme(y ~ 1, random = ~ 1 | ID))
           ID       Within
0.0002709644 1.9062505816
attr(,"class")
[1] "varcomp"

 > VarCorr(lme(y ~ 1, random = ~ 1 | ID))
ID = pdLogChol(1)
             Variance     StdDev
(Intercept) 0.0002709644 0.01646100
Residual    1.9062505816 1.38067034

Adaikalavan Ramasamy wrote:
> Can anyone verify my calculations below or explain why they are wrong ?
> 
> I have several animals that were measured thrice. The only blocking
> variable is the animal itself. I am interested in calculating the 
> between and within object variations in R. An artificial example :
> 
> y <- c( 2.2, -1.4, -0.5,  # animal 1
>        -0.3  -2.1   1.5,  # animal 2
>         1.3  -0.3   0.5,  # animal 3
>        -1.4  -0.2   1.8)  # animal 4
> ID <- factor( rep(1:4, each=3) )
> 
> 
> 1) Using the ANOVA method
> 
>   summary(aov( y ~ ID ))
>               Df Sum Sq Mean Sq F value Pr(>F)
>   ID           3  0.900   0.300  0.1207 0.9453
>   Residuals    8 19.880   2.485               
> 
>   => within animal  variation  = 2.485
>   => between animal variation  = (0.300 - 2.485)/3 = -0.7283
> 
> I am aware that ANOVA can give negative estimates for variances. Is this
> such a case or have I coded wrongly ?
> 
> 
> 2) Using the REML approach 
> 
>   library(nlme)
>   lme( y ~ 1, rand = ~ 1 | ID)
>    ....
>   Random effects:
>   Formula: ~1 | ID
>           (Intercept) Residual
>   StdDev:  0.01629769 1.374438
> 
>   => within animal variation  = 1.374438^2 = 1.88908
>   => between animal variation = 0.01629769^2 = 0.0002656147
> 
> Is this the correct way of coding for this problem ? I do not have
> access to a copy of Pinheiro & Bates at the moment.
> 
> Thank you very much in advance.
> 
> Regards, Adai
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From bill.shipley at usherbrooke.ca  Fri Jun 10 21:44:19 2005
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Fri, 10 Jun 2005 15:44:19 -0400
Subject: [R] 1st derivatives using gam.
Message-ID: <002401c56df4$cb208100$b01ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/1efc582e/attachment.pl

From ggrothendieck at gmail.com  Fri Jun 10 22:11:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 16:11:00 -0400
Subject: [R] Default Format for Dates?
In-Reply-To: <971536df05061011184945cde0@mail.gmail.com>
References: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>
	<971536df05061011184945cde0@mail.gmail.com>
Message-ID: <971536df050610131137e4e362@mail.gmail.com>

On 6/10/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/10/05, khobson at fd9ns01.okladot.state.ok.us
> <khobson at fd9ns01.okladot.state.ok.us> wrote:
> >
> >
> >
> >
> > Is there anyway to preset date formats?  I have a date from a cover.dbf
> > that is shown as this:
> > > cover$FINALREPOR
> > [1] "2003-06-24"
> >
> > The numeric value in cover$FINALREPOR is 12227.  I'd rather not create
> > another vector to hold the properly formatted date.
> >
> > When I put this in a WordPerfect merge, I want the date to be June 24,
> > 2003.   I could take care of the problem in a WordPerfect macro but I'd
> > rather do it as an R default date format if possible.
> >
> 
> I am not entirely sure I understand what you are asking but I assume
> you want a Date variable such that print, format and as.character,
> when applied to it, produce default output of a prespecified format.
> 
> Although I do not think Date will do that, chron can
> associate a default format with a chron variable.
> 
> library(chron)
> 
> # custom format for a dates (i.e. chron) object
> my.format <- function(x) format(as.Date(dates(x)), "%B %d %Y")
> 
> # test data
> my.Date <- Sys.Date() + 0:9
> 
> # convert to chron and associate my.format to it
> my.chron <- chron(unclass(my.Date), out.format = my.format)
> 
> print(my.chron)
> my.chron
> as.character(my.chron)
> format(my.chron)
> 
> Note that you can find more information on chron and various
> conversions in my article in RNews 4/1 and especially the table
> at the end of that article.
> 

Here is a different solution that does not involve chron.  In this one
we define a subclass of Date called mdy that has a default
format as you specified implemented by defining print, as.character
and format methods for it:

# define mdy methods
print.mdy <- print.Date
as.character.mdy <- format.mdy <- function(x, format = "%B %d %Y")
	format(structure(x, class = "Date"), format = format)

# test data of class mdy (which is a subclass of Date)
x <- Sys.Date() + 0:9 
class(x) <- c("mdy", "Date")

# test out print, format and as.character methods for class mdy
print(x)
format(x)
as.character(x)
x



From lj22 at u.washington.edu  Fri Jun 10 22:12:49 2005
From: lj22 at u.washington.edu (Lei Jiang)
Date: Fri, 10 Jun 2005 13:12:49 -0700 (PDT)
Subject: [R] mask a matrix
Message-ID: <Pine.A41.4.61b.0506101307320.73704@homer04.u.washington.edu>

Hi, there.

I have two matrix with identical dimentions. matrix A contains 
information of 0 and 1, and matrix B contains data. I only want the data 
in matrix B where matrix A has 1's. the places where matrix A has 0's, I 
want NA's in matrix B.

How do I do that??

Thank you very much.

Lei Jiang

Department of Chemsitry
University of Washington
Box 351700
Seattle, WA 98195
Phone: 206-616-6882
Fax: 206-685-8665



From andy_liaw at merck.com  Fri Jun 10 22:22:50 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Jun 2005 16:22:50 -0400
Subject: [R] mask a matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E985@usctmx1106.merck.com>

If you just multiply the two together, you get 0 in the right positions.  If
you want those to be NAs, just do:

result <- ifelse(A, B, NA)

Andy

> From: Lei Jiang
> 
> Hi, there.
> 
> I have two matrix with identical dimentions. matrix A contains 
> information of 0 and 1, and matrix B contains data. I only 
> want the data 
> in matrix B where matrix A has 1's. the places where matrix A 
> has 0's, I 
> want NA's in matrix B.
> 
> How do I do that??
> 
> Thank you very much.
> 
> Lei Jiang
> 
> Department of Chemsitry
> University of Washington
> Box 351700
> Seattle, WA 98195
> Phone: 206-616-6882
> Fax: 206-685-8665
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From samuel_mwalili at yahoo.com  Fri Jun 10 22:26:57 2005
From: samuel_mwalili at yahoo.com (Mwalili, S. M.)
Date: Fri, 10 Jun 2005 13:26:57 -0700 (PDT)
Subject: [R] mask a matrix
In-Reply-To: <Pine.A41.4.61b.0506101307320.73704@homer04.u.washington.edu>
Message-ID: <20050610202657.70207.qmail@web53404.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/5ba40b5f/attachment.pl

From marc.girondot at ese.u-psud.fr  Fri Jun 10 22:44:28 2005
From: marc.girondot at ese.u-psud.fr (Marc Girondot)
Date: Fri, 10 Jun 2005 22:44:28 +0200
Subject: [R] problem with polr ?
Message-ID: <p06210206becfabe78264@[129.175.106.62]>

I want to fit a multinomial model with logit link.
For example let this matrix to be analyzed:
male   female   aborted   factor
10     12         1         1.2
14     14         4         1.3
15     12         3         1.4

(this is an example, not the true data which are far more complex...)

I suppose the correct function to analyze these data is polr from MASS library.

The data have been entered in a text file like this:

output  factor  n
m       1.2     10
f       1.2     12
a       1.2     1
m       1.3     14
f       1.3     14
a       1.3     4
m       1.4     15
f       1.4     12
a       1.4     3

However, after having performed the analysis, it 
appears this is not correct as the 3 outputs per 
experiment are not linked...

library(MASS)
dt.plr <- polr(output ~ factor, data=dt, weights=n)
>  dt.pr1<-predict(dt.plr, , type="probs")
>  dt.pr1
            a         f         m
1 0.09987167 0.4578184 0.4423099
2 0.09987167 0.4578184 0.4423099
3 0.09987167 0.4578184 0.4423099
4 0.09437078 0.4477902 0.4578390
5 0.09437078 0.4477902 0.4578390
6 0.09437078 0.4477902 0.4578390
7 0.08914287 0.4374067 0.4734505
8 0.08914287 0.4374067 0.4734505
9 0.08914287 0.4374067 0.4734505


---------------------------Another linked problem

Also, I don't understand what the meaning of the 
residual deviance that is displayed.
Let modify the file so that the model can also be 
analyzed using binomial model:

output  factor  n
m       1.2     10
f       1.2     12
a       1.2     0
m       1.3     14
f       1.3     14
a       1.3     0
m       1.4     15
f       1.4     12
a       1.4     0

dt.plr
Call:
polr(formula = output ~ factor, data = dt, weights = n)

Coefficients:
   factor
2.034848

Intercepts:
        a|f        f|m
-16.306511   2.632410

Residual Deviance: 106.2304
AIC: 112.2304

whereas the corresponding scaled deviance for the 
binomial model (removing a column) is 1.842e-3...


-- 

__________________________________________________________
Marc Girondot, Pr
Laboratoire Ecologie, Syst??matique et Evolution
Equipe de Conservation des Populations et des Communaut??s
CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
B??timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
15 56 96   e-mail: marc.girondot at ese.u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot
Fax in US: 1-425-732-6934



From ggrothendieck at gmail.com  Fri Jun 10 22:49:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 10 Jun 2005 16:49:13 -0400
Subject: [R] Default Format for Dates?
In-Reply-To: <971536df050610131137e4e362@mail.gmail.com>
References: <OFC89217FE.3530531E-ON8625701C.00602A36-8625701C.00622EA6@fd9ns01.okladot.state.ok.us>
	<971536df05061011184945cde0@mail.gmail.com>
	<971536df050610131137e4e362@mail.gmail.com>
Message-ID: <971536df050610134935630dea@mail.gmail.com>

On 6/10/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/10/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 6/10/05, khobson at fd9ns01.okladot.state.ok.us
> > <khobson at fd9ns01.okladot.state.ok.us> wrote:
> > >
> > >
> > >
> > >
> > > Is there anyway to preset date formats?  I have a date from a cover.dbf
> > > that is shown as this:
> > > > cover$FINALREPOR
> > > [1] "2003-06-24"
> > >
> > > The numeric value in cover$FINALREPOR is 12227.  I'd rather not create
> > > another vector to hold the properly formatted date.
> > >
> > > When I put this in a WordPerfect merge, I want the date to be June 24,
> > > 2003.   I could take care of the problem in a WordPerfect macro but I'd
> > > rather do it as an R default date format if possible.
> > >
> >
> > I am not entirely sure I understand what you are asking but I assume
> > you want a Date variable such that print, format and as.character,
> > when applied to it, produce default output of a prespecified format.
> >
> > Although I do not think Date will do that, chron can
> > associate a default format with a chron variable.
> >
> > library(chron)
> >
> > # custom format for a dates (i.e. chron) object
> > my.format <- function(x) format(as.Date(dates(x)), "%B %d %Y")
> >
> > # test data
> > my.Date <- Sys.Date() + 0:9
> >
> > # convert to chron and associate my.format to it
> > my.chron <- chron(unclass(my.Date), out.format = my.format)
> >
> > print(my.chron)
> > my.chron
> > as.character(my.chron)
> > format(my.chron)
> >
> > Note that you can find more information on chron and various
> > conversions in my article in RNews 4/1 and especially the table
> > at the end of that article.
> >
> 
> Here is a different solution that does not involve chron.  In this one
> we define a subclass of Date called mdy that has a default
> format as you specified implemented by defining print, as.character
> and format methods for it:
> 
> # define mdy methods
> print.mdy <- print.Date
> as.character.mdy <- format.mdy <- function(x, format = "%B %d %Y")
>        format(structure(x, class = "Date"), format = format)
> 
> # test data of class mdy (which is a subclass of Date)
> x <- Sys.Date() + 0:9
> class(x) <- c("mdy", "Date")
> 
> # test out print, format and as.character methods for class mdy
> print(x)
> format(x)
> as.character(x)
> x
> 

Actually there was no point in defining print.mdy and as.character.mdy
as further examination shows they both operate through format so the 
above can be simplified to just:


format.mdy <- function(x, format = "%B %d %Y")
       format(structure(x, class = "Date"), format = format)

# test data of class mdy (which is a subclass of Date)
x <- Sys.Date() + 0:9
class(x) <- c("mdy", "Date")

# test out print, format and as.character methods for class mdy
print(x)
format(x)
as.character(x)
x



From sgilpin at gmail.com  Fri Jun 10 22:57:29 2005
From: sgilpin at gmail.com (Scott Gilpin)
Date: Fri, 10 Jun 2005 14:57:29 -0600
Subject: [R] Performance difference between 32-bit build and 64-bit build on
	Solaris 8
Message-ID: <5739cc2f05061013574d0fc684@mail.gmail.com>

Hi everyone -

I'm seeing a 32-bit build perform significantly faster (up to 3x) than
a 64 bit build on Solaris 8.  I'm running R version 2.1.0.  Here are
some of my system details, and some resulting timings:

>uname -a
SunOS lonetree 5.8 Generic_117350-16 sun4u sparc SUNW,Sun-Fire-V440

lonetree /home/sgilpin >gcc -v
Reading specs from /usr/local/lib/gcc/sparc-sun-solaris2.8/3.4.2/specs
Configured with: ../configure --with-as=/usr/ccs/bin/as
--with-ld=/usr/ccs/bin/ld --disable-nls
Thread model: posix
gcc version 3.4.2

I built the 32 bit version of R with no changes to config.site.  I
built the 64 bit version with the following in config.site:

CC="gcc -m64"
FFLAGS="-m64 -g -02"
LDFLAGS="-L/usr/local/lib/sparcv9 -L/usr/local/lib"
CXXFLAGS="-m64 -g -02"

neither build uses a BLAS.  Both builds are installed on the same
machine, and the same disk.  The machine has virtually no load; R is
one of the only processes running during these timings:

First comparison:  solve on a large matrix

>echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' |
/disk/loneres01/R-2.1.0-32bit/bin/R -q --vanilla
> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
[1] 713.45   0.38 713.93   0.00   0.00
> 

>echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' |
/disk/loneres01/R-2.1.0-64bit/bin/R -q --vanilla
> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
[1] 2277.05    0.31 2278.38    0.00    0.00
> 

Second comparison:  linear regression

lonetree /home/sgilpin/R >echo 'set.seed(1);
y<-matrix(rnorm(10000*500),500);
x<-matrix(runif(500*100),500);
system.time(fit<-lm(y~x))' | /disk/loneres01/R-2.1.0-32bit/bin/R -q --vanilla  
> set.seed(1);y<-matrix(rnorm(10000*500),500);x<-matrix(runif(500*100),500);system.time(fit<-lm(y~x))
[1] 23.34  0.80 24.17  0.00  0.00
> 

lonetree /home/sgilpin/R >echo 'set.seed(1);
y<-matrix(rnorm(10000*500),500);
x<-matrix(runif(500*100),500);
system.time(fit<-lm(y~x))' | /disk/loneres01/R-2.1.0-64bit/bin/R -q --vanilla 
> set.seed(1);y<-matrix(rnorm(10000*500),500);x<-matrix(runif(500*100),500);system.time(fit<-lm(y~x))
[1] 55.34  0.70 56.21  0.00  0.00
> 

Final comparison:  stats-Ex.R (from R-devel)
lonetree /home/sgilpin/R >time /disk/loneres01/R-2.1.0-32bit/bin/R -q
--vanilla CMD BATCH stats-Ex.R

real    1m4.042s
user    0m47.400s
sys     0m10.390s
lonetree /home/sgilpin/R >time /disk/loneres01/R-2.1.0-64bit/bin/R -q
--vanilla CMD BATCH stats-Ex.R

real    1m20.017s
user    1m3.590s
sys     0m10.130s

I've seen Prof. Ripley and others comment that a 64 bit build will be
a little slower because the pointers are larger, and gc() will take
longer, but these timings seem out of this range.

Any thoughts?



From kjetil at acelerate.com  Fri Jun 10 17:55:53 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 10 Jun 2005 11:55:53 -0400
Subject: [R] Replacing for loop with tapply!?
In-Reply-To: <42A9543E.3040901@oomvanlieshout.net>
References: <42A9543E.3040901@oomvanlieshout.net>
Message-ID: <42A9B809.7030706@acelerate.com>

Sander Oom wrote:

>Dear all,
>
>We have a large data set with temperature data for weather stations 
>across the globe (15000 stations).
>
>For each station, we need to calculate the number of days a certain 
>temperature is exceeded.
>
>So far we used the following S code, where mat88 is a matrix containing 
>rows of 365 daily temperatures for each of 15000 weather stations:
>
>	m <- 37
>	n <- 2
>	outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
>	for(i in 1:nrow(mat88)) {
>		# i <- 3
>		row1 <- as.data.frame(df88[i,  ])
>		temprow37 <- select.rows(row1, row1 > m)
>		temprow39 <- select.rows(row1, row1 > m + n)
>		temprow41 <- select.rows(row1, row1 > m + 2 * n)
>		outmat88[i, 1] <- max(row1, na.rm = T)
>		outmat88[i, 2] <- count.rows(temprow37)
>		outmat88[i, 3] <- count.rows(temprow39)
>		outmat88[i, 4] <- count.rows(temprow41)
>	}
>	outmat88
>
>  
>
What you need is not tapply but apply. Something like
   apply(mat88, 1, function(x) sum(x > 30))

where your treshold should replace 30 and the `1' refers to rows. For 
multiple tresholds:

apply(mat88, 1, function(x) c( sum(x>20), sum(x>25), sum(x>30)))

Kjetil

>We have transferred the data to a more potent Linux box running R, but 
>still hope to speed up the code.
>
>I know a for loop should be avoided when looking for speed. I also know 
>the answer is in something like tapply, but my understanding of these 
>commands is still to limited to see the solution. Could someone show me 
>the way!?
>
>Thanks in advance,
>
>Sander.
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From choudary.jagar at swosu.edu  Fri Jun 10 23:26:26 2005
From: choudary.jagar at swosu.edu (Jagarlamudi, Choudary)
Date: Fri, 10 Jun 2005 16:26:26 -0500
Subject: [R] RCMD Warnings on src directory.
Message-ID: <E03EBB50FF2C024781A6E4460AD58F0607C23E@swosu-mbx01.admin.swosu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050610/6fa99182/attachment.pl

From jfox at mcmaster.ca  Fri Jun 10 23:40:01 2005
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 10 Jun 2005 17:40:01 -0400
Subject: [R] problem with polr ?
In-Reply-To: <p06210206becfabe78264@[129.175.106.62]>
Message-ID: <20050610214000.WMGZ19894.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Marc,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Girondot
> Sent: Friday, June 10, 2005 3:44 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] problem with polr ?
> 
> I want to fit a multinomial model with logit link.
> For example let this matrix to be analyzed:
> male   female   aborted   factor
> 10     12         1         1.2
> 14     14         4         1.3
> 15     12         3         1.4
> 
> (this is an example, not the true data which are far more complex...)
> 
> I suppose the correct function to analyze these data is polr 
> from MASS library.
> 

Actually, polr fits the proportional-odds logistic regression model (or a
similar ordered probit model), which requires an ordinal response. I don't
believe that it makes sense to think of a < f < m. For the multinomial logit
model, see the multinom function in the nnet package.

> The data have been entered in a text file like this:
> 
> output  factor  n
> m       1.2     10
> f       1.2     12
> a       1.2     1
> m       1.3     14
> f       1.3     14
> a       1.3     4
> m       1.4     15
> f       1.4     12
> a       1.4     3
> 
> However, after having performed the analysis, it appears this 
> is not correct as the 3 outputs per experiment are not linked...
> 
> library(MASS)
> dt.plr <- polr(output ~ factor, data=dt, weights=n)
> >  dt.pr1<-predict(dt.plr, , type="probs")
> >  dt.pr1
>             a         f         m
> 1 0.09987167 0.4578184 0.4423099
> 2 0.09987167 0.4578184 0.4423099
> 3 0.09987167 0.4578184 0.4423099
> 4 0.09437078 0.4477902 0.4578390
> 5 0.09437078 0.4477902 0.4578390
> 6 0.09437078 0.4477902 0.4578390
> 7 0.08914287 0.4374067 0.4734505
> 8 0.08914287 0.4374067 0.4734505
> 9 0.08914287 0.4374067 0.4734505
> 

These are the fitted probabilities of response in each of the three
categories. Note that each row sums to 1, as it should. Of course, the model
likely doesn't make sense.

> 
> ---------------------------Another linked problem
> 
> Also, I don't understand what the meaning of the residual 
> deviance that is displayed.
> Let modify the file so that the model can also be analyzed 
> using binomial model:
> 
> output  factor  n
> m       1.2     10
> f       1.2     12
> a       1.2     0
> m       1.3     14
> f       1.3     14
> a       1.3     0
> m       1.4     15
> f       1.4     12
> a       1.4     0
> 
> dt.plr
> Call:
> polr(formula = output ~ factor, data = dt, weights = n)
> 
> Coefficients:
>    factor
> 2.034848
> 
> Intercepts:
>         a|f        f|m
> -16.306511   2.632410
> 
> Residual Deviance: 106.2304
> AIC: 112.2304
> 
> whereas the corresponding scaled deviance for the binomial 
> model (removing a column) is 1.842e-3...
> 
> 

I'm surprised that you were able to get estimates here at all, since there
are no observations at category a; nevetheless, polr is estimating two
thresholds, one between the never-observed a and f. I expect that this is a
numerical artifact. Note that if you simply remove the rows for a rather
than set the counts to 0, polr will complain that there are only two
categories.

I hope this helps,
 John

> -- 
> 
> __________________________________________________________
> Marc Girondot, Pr
> Laboratoire Ecologie, Syst??matique et Evolution
> Equipe de Conservation des Populations et des Communaut??s
> CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
> B??timent 362
> 91405 Orsay Cedex, France
> 
> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
> 15 56 96   e-mail: marc.girondot at ese.u-psud.fr
> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
> Skype: girondot
> Fax in US: 1-425-732-6934
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Sat Jun 11 00:03:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 10 Jun 2005 15:03:45 -0700 (PDT)
Subject: [R] RCMD Warnings on src directory.
In-Reply-To: <E03EBB50FF2C024781A6E4460AD58F0607C23E@swosu-mbx01.admin.swosu.edu>
References: <E03EBB50FF2C024781A6E4460AD58F0607C23E@swosu-mbx01.admin.swosu.edu>
Message-ID: <Pine.A41.4.61b.0506101501290.331934@homer05.u.washington.edu>

On Fri, 10 Jun 2005, Jagarlamudi, Choudary wrote:

> Hi Group,
>
> I performed the following commands to build my package in R 2.0 under Windows XP
> I got all my tools from Dr. Duncan Mudroch's website.
> I did a RCMD build dnal and it built a tar file for me.
> I did a RCMD INSTALL dnal and it installed well.
> When i do RCMD check dnal i get the following 2 WARNINGS with no Errors.
>
> checking package directories..WARNING
> Subdirectory 'src' contains no source files.
> checking for missing documentation entries...WARNING
> Undocumented code object:
> linReg refGroup
>
> Regarding my first Warning ..I have no C/Fortran files so what should i 
> put in 'src' directory.

Nothing. You shouldn't have a src directory.

> Regarding my second Warning ..I stripped the .Rd files of the listed 
> objects line by line and checked and i could not figure it out. Rout 
> file offers no help on Warnings. Any suggestions to clear the 2 warnings 
> will be greatly appreciated.

It is saying that linReg and refGroup aren't documented (don't have an 
\alias line in any .Rd file).  If they really are documented then 
something more complicated must be wrong.

 	-thomas



From sean.xiao.wang at gmail.com  Sat Jun 11 00:15:00 2005
From: sean.xiao.wang at gmail.com (Sean)
Date: Fri, 10 Jun 2005 18:15:00 -0400
Subject: [R] launching RSJava from external application
Message-ID: <54c930b405061015157927a669@mail.gmail.com>

Hi,all,

I have a problem about launching SJava in R from external application
(Cytoscape).

My R version is 2.1.0 and Sjava version is 0.68. R is installed by issuing

   1.  configure --enable-R-shlib
   2. make
   3. make check
   4. make install

SJava is installed by issuing  "R CMD INSTALL -c SJava_version_.tar.gz". 

The LD_LIBRARY_PATH is set in .bash_profile by
"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/lib/R/library/SJava/libs:/usr/java/j2sdk1.4.2_08/jre/lib/i386/client:/usr/java/j2sdk1.4.2_08/jre/lib/i386:/usr/java/j2sdk1.4.2_08/jre/../lib/i386:
export LD_LIBRARY_PATH"

After installation, type the following command
    *  library(SJava)
    * .JavaInit()
    * .Java("java.lang.System", "getProperty", "java.class.path")
    * names(.Java("java.lang.System", "getProperties"))

Get the correct output (omitted here) .

Now the question is:

I am using R to interact with Cytoscape ( a visualization tool), which
has a plugin called "Cytotalk" that can let R and Cytoscape talk to
each other. I successfully ran a R script which need SJava and got the
right output in Cytoscape.

But when I tried to run the R script from the cytoscape menu ( the
plugin enabled a menu in which you can choose a R file to execute it
and display the output in Cytoscape ), the terminal give me the
following error:


Accepting clients from localhost
> port <- 8082
> library(SJava)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
'/usr/local/lib/R/library/SJava/libs/SJava.so':
  libRSNativeJava.so: cannot open shared object file: No such file or directory
Error in library(SJava) : .First.lib failed for 'SJava'
Execution halted
RESULT: 1

I googled around and found it was a common error when people trying to
install SJava. But I installed SJava correctly and it seems work fine,
I can not figure out why it comes up with this error.

To give you some background information, I guess Cytoscape and R talk
with each other via various java stuff. The fact that I can interact
with Cytoscape from within R showing that the .jar directory should be
fine.

The first few lines of the R code is like :

library(SJava)
#Warning message:
#The Java machine is no longer initialized automatically. You must
explicitly load it in: f(libname, pkgname)
                                                                                
a=javaConfig(classPath="/usr/local/bio/cytoscape/plugins/cytoTalk.jar")
.JavaInit(config=a)
#.Java("System","getProperty","java.class.path")
#[1] ":/usr/local/bio/cytoscape/plugins/cytoTalk.jar:/usr/local/lib/R/library/SJava/org/omegahat/Jars/Environment.jar:/usr/local/lib/R/library/SJava/org/..:/usr/local/lib/R/library/SJava/org/omegahat/Jars/antlr.jar:/usr/local/lib/R/library/SJava/org/omegahat/Jars/jas.jar:/usr/local/lib/R/library/SJava/org/omegahat/Jars/jhall.jar"
                                                                                
source("CytoTalkClient-v2.R")
cy <- .JNew("CytoTalkClient","http://localhost:8082")

And by the way, my OS is Redhat Linux. I really appreciate it if
someone has an clue of the  problem. Thanks.

Xiao



From mike.rstat at gmail.com  Sat Jun 11 00:18:26 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 10 Jun 2005 15:18:26 -0700
Subject: [R] us zipcode data map
Message-ID: <27db823f050610151849b39626@mail.gmail.com>

i've search the email archives, searched the documention 
of various map packages and done an R-site search, but 
have been unable to find direct resources for creating maps 
of the US that are colored or annotated or ... by zipcode 
data.  

For example, create a map of the US and color each zipcode 
region by its population using two vectors z,p containing the 
zipcode and population, respectively.  I'm not looking for data 
to fill z, and p.  I'm looking for some highend functions to 
display on a map the data that I already have.

Cheers,
Mike



From gerifalte28 at hotmail.com  Sat Jun 11 00:52:20 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 10 Jun 2005 22:52:20 +0000
Subject: [R] How to read a row dataset one by one
In-Reply-To: <96507a8e050610013258257e98@mail.gmail.com>
Message-ID: <BAY103-F20E8156A2DE7B11438E3AFA6FF0@phx.gbl>

You could use

by(dat,row.names(dat), print) #It will return a list with results for each 
row

Is this what you want?

Cheers

Francisco

>From: Jan Sabee <jan.sabee at gmail.com>
>Reply-To: Jan Sabee <jan.sabee at gmail.com>
>To: R-help at stat.math.ethz.ch
>Subject: [R] How to read a row dataset one by one
>Date: Fri, 10 Jun 2005 10:32:15 +0200
>
>Dear all,
>How to read a row dataset one by one and then print it.
>
>x1 x2 x3 x4 x5   y
>a  b  a  c  c    M1
>c  b  b  c  c    M4
>c  c  a  c  c    M2
>c  a  c  a  a    M2
>c  c  a  a  a    M1
>c  a  b  c  a    M3
>c  c  a  b  c    M3
>c  a  c  a  b    M2
>c  c  a  b  a    M1
>
>I need a result like
>read row no 1,
>[1] a  b  a  c  c    M1
>read row no 2,
>[1] c  b  b  c  c    M4
>.
>.
>.
>the last row,
>[1] c  c  a  b  a    M1
>
>Kind regards,
>Jan Sabee
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Sat Jun 11 01:12:08 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 10 Jun 2005 23:12:08 +0000
Subject: [R] us zipcode data map
In-Reply-To: <27db823f050610151849b39626@mail.gmail.com>
Message-ID: <BAY103-F3DEFE16A93D0716704C9CA6FF0@phx.gbl>

library(maps)
example(match.map) #for coloring

If you want to annotate the map look at ?map.text

I hope this helps

Francisco



>From: Mike R <mike.rstat at gmail.com>
>Reply-To: r-help at stat.math.ethz.ch
>To: r-help at stat.math.ethz.ch
>Subject: [R] us zipcode data map
>Date: Fri, 10 Jun 2005 15:18:26 -0700
>
>i've search the email archives, searched the documention
>of various map packages and done an R-site search, but
>have been unable to find direct resources for creating maps
>of the US that are colored or annotated or ... by zipcode
>data.
>
>For example, create a map of the US and color each zipcode
>region by its population using two vectors z,p containing the
>zipcode and population, respectively.  I'm not looking for data
>to fill z, and p.  I'm looking for some highend functions to
>display on a map the data that I already have.
>
>Cheers,
>Mike
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Jun 11 02:35:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 10 Jun 2005 20:35:31 -0400
Subject: [R] Performance difference between 32-bit build and 64-bit bu
 ild on Solaris 8
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>

I'm not familiar with Solaris, so take this with appropriate dose of NaCl...

For the 64-bit build, why not have the -O2 for gcc, since you have it for
g77 and g++?  If you just run vanilla configure for the 32-bit build, I
believe it uses -O2 for all three compilers by default.  If that's the
difference, perhaps it's sufficient to explain the performance difference.

The other thing is that solve(), and to some extent lm(), can benefit from
an optimized BLAS.  You might want to check if the two builds are using
equivalent BLAS.

Andy

> From: Scott Gilpin
> 
> Hi everyone -
> 
> I'm seeing a 32-bit build perform significantly faster (up to 3x) than
> a 64 bit build on Solaris 8.  I'm running R version 2.1.0.  Here are
> some of my system details, and some resulting timings:
> 
> >uname -a
> SunOS lonetree 5.8 Generic_117350-16 sun4u sparc SUNW,Sun-Fire-V440
> 
> lonetree /home/sgilpin >gcc -v
> Reading specs from /usr/local/lib/gcc/sparc-sun-solaris2.8/3.4.2/specs
> Configured with: ../configure --with-as=/usr/ccs/bin/as
> --with-ld=/usr/ccs/bin/ld --disable-nls
> Thread model: posix
> gcc version 3.4.2
> 
> I built the 32 bit version of R with no changes to config.site.  I
> built the 64 bit version with the following in config.site:
> 
> CC="gcc -m64"
> FFLAGS="-m64 -g -02"
> LDFLAGS="-L/usr/local/lib/sparcv9 -L/usr/local/lib"
> CXXFLAGS="-m64 -g -02"
> 
> neither build uses a BLAS.  Both builds are installed on the same
> machine, and the same disk.  The machine has virtually no load; R is
> one of the only processes running during these timings:
> 
> First comparison:  solve on a large matrix
> 
> >echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' |
> /disk/loneres01/R-2.1.0-32bit/bin/R -q --vanilla
> > set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1] 713.45   0.38 713.93   0.00   0.00
> > 
> 
> >echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' |
> /disk/loneres01/R-2.1.0-64bit/bin/R -q --vanilla
> > set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1] 2277.05    0.31 2278.38    0.00    0.00
> > 
> 
> Second comparison:  linear regression
> 
> lonetree /home/sgilpin/R >echo 'set.seed(1);
> y<-matrix(rnorm(10000*500),500);
> x<-matrix(runif(500*100),500);
> system.time(fit<-lm(y~x))' | 
> /disk/loneres01/R-2.1.0-32bit/bin/R -q --vanilla  
> > 
> set.seed(1);y<-matrix(rnorm(10000*500),500);x<-matrix(runif(50
> 0*100),500);system.time(fit<-lm(y~x))
> [1] 23.34  0.80 24.17  0.00  0.00
> > 
> 
> lonetree /home/sgilpin/R >echo 'set.seed(1);
> y<-matrix(rnorm(10000*500),500);
> x<-matrix(runif(500*100),500);
> system.time(fit<-lm(y~x))' | 
> /disk/loneres01/R-2.1.0-64bit/bin/R -q --vanilla 
> > 
> set.seed(1);y<-matrix(rnorm(10000*500),500);x<-matrix(runif(50
> 0*100),500);system.time(fit<-lm(y~x))
> [1] 55.34  0.70 56.21  0.00  0.00
> > 
> 
> Final comparison:  stats-Ex.R (from R-devel)
> lonetree /home/sgilpin/R >time /disk/loneres01/R-2.1.0-32bit/bin/R -q
> --vanilla CMD BATCH stats-Ex.R
> 
> real    1m4.042s
> user    0m47.400s
> sys     0m10.390s
> lonetree /home/sgilpin/R >time /disk/loneres01/R-2.1.0-64bit/bin/R -q
> --vanilla CMD BATCH stats-Ex.R
> 
> real    1m20.017s
> user    1m3.590s
> sys     0m10.130s
> 
> I've seen Prof. Ripley and others comment that a 64 bit build will be
> a little slower because the pointers are larger, and gc() will take
> longer, but these timings seem out of this range.
> 
> Any thoughts?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From mike.rstat at gmail.com  Sat Jun 11 03:06:39 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 10 Jun 2005 18:06:39 -0700
Subject: [R] us zipcode data map
In-Reply-To: <BAY103-F3DEFE16A93D0716704C9CA6FF0@phx.gbl>
References: <27db823f050610151849b39626@mail.gmail.com>
	<BAY103-F3DEFE16A93D0716704C9CA6FF0@phx.gbl>
Message-ID: <27db823f05061018066c7e353d@mail.gmail.com>

On 6/10/05, Francisco J. Zagmutt <gerifalte28 at hotmail.com> wrote:
> library(maps)
> example(match.map) #for coloring
> 
> If you want to annotate the map look at ?map.text

thanks Francisco,  correct me if i am wrong, but maps_2.0-27.tar.gz
does many many maps, but not any zipcode maps ?



From ramasamy at cancer.org.uk  Sat Jun 11 02:36:43 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 11 Jun 2005 01:36:43 +0100
Subject: [R] Replacing for loop with tapply!?
In-Reply-To: <42A9D685.5050806@oomvanlieshout.net>
References: <42A9543E.3040901@oomvanlieshout.net>
	<012901c56d9c$c6bfc6f0$0540210a@www.domain>
	<42A96709.3010406@oomvanlieshout.net>
	<007001c56ddf$4f80c1a0$0540210a@www.domain>
	<42A9D685.5050806@oomvanlieshout.net>
Message-ID: <1118450203.5850.14.camel@dhcp-63.ccc.ox.ac.uk>

OK, so you want to find some summary statistics for each column, where
some columns could be completely missing. 

Writing a small wrapper should help. When you use apply(), you are
actually applying a function to every column (or row). First, let us
simulate a dataset with 15 days/rows and 10 stations/columns 

### simulate data
set.seed(1)            # for reproducibility 
mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)  
mat[ mat > 45 ] <- NA  # create some missing values
mat[ ,9 ]       <- NA  # station 9's data is completely missing


Here are two example of such wrappers :

find.stats1 <- function( data, threshold=c(37,39,41) ){
  
  n   <- length(threshold)
  out <- matrix(  nrow=(n + 1), ncol=ncol(data) ) # initialise

  out[1, ] <- apply(data, 2, function(x) 
                         ifelse( all(is.na(x)), NA, max(x, na.rm=T) ))

  for(i in 1:n) out[ i+1, ] <- colSums( data > threshold[i], na.rm=T )
  
  rownames(out) <- c( "daily_max", paste("above", threshold, sep="_") )
  colnames(out) <- rownames(data)  # name of the stations
  return( out )
}
  
find.stats2 <- function( data, threshold=c(37,39,41) ){
  
  n      <- length(threshold)
  excess <- numeric( n )
  out    <- matrix(  nrow=(n + 1), ncol=ncol(data) ) # initialise
  good   <- which( apply( data, 2, function(x) !all(is.na(x)) ) )
  # colums that are not completely missing
 
  out[ , good] <- apply( data[ , good], 2, function(x){
    m <- max( x, na.rm=T )
    for(i in 1:n){ excess[i] <- sum( x > threshold[i], na.rm=TRUE ) }
    return( c(m, excess) )
  } ) 
  
  rownames(out) <- c( "daily_max", paste("above", threshold, sep="_") )
  colnames(out) <- rownames(data)  # name of the stations
  return( out )
}

find.stats1( mat )
          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
daily_max   44   42   39   41   45   43   42   45   NA    42
above_37     2    1    2    1    3    2    2    1    0     1
above_39     2    1    0    1    3    2    1    1    0     1
above_41     2    1    0    0    2    2    1    1    0     1

find.stats2( mat )
          [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
daily_max   44   42   39   41   45   43   42   45   NA    42
above_37     2    1    2    1    3    2    2    1   NA     1
above_39     2    1    0    1    3    2    1    1   NA     1
above_41     2    1    0    0    2    2    1    1   NA     1


On my laptop 'find.stats1' and 'find.stats2' (which is more flexible)
takes 7 and 6 seconds respectively to execute on a dataset with 10000
stations and 365 days.

Regards, Adai



On Fri, 2005-06-10 at 20:05 +0200, Sander Oom wrote:
> Dear all,
> 
> Dimitris and Andy, thanks for your great help. I have progressed to the 
> following code which runs very fast and effective:
> 
> mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)
> mat[mat>45] <- NA
> mat<-NA
> mat
> temps <- c(35, 37, 39)
> ind <- rbind(
>      t(sapply(temps, function(temp)
>        rowSums(mat > temp, na.rm=TRUE) )),
>      rowSums(!is.na(mat), na.rm=FALSE),
>      apply(mat, 1, max, na.rm=TRUE))
> ind <- t(ind)
> ind
> 
> However, some weather stations have missing values for the whole year. 
> Unfortunately, the code breaks down (when uncommenting mat<-NA).
> 
> I have tried 'ifelse' statements in the functions, but it becomes even 
> more of a mess. I could subset the matrix before hand, but this would 
> mean merging with a complete matrix afterwards to make it compatible 
> with other years. That would slow things down.
> 
> How can I make the code robust for rows containing all missing values?
> 
> Thanks for your help,
> 
> Sander.
> 
> Dimitris Rizopoulos wrote:
> > for the maximum you could use something like:
> > 
> > ind[, 1] <- apply(mat, 2, max)
> > 
> > I hope it helps.
> > 
> > Best,
> > Dimitris
> > 
> > ----
> > Dimitris Rizopoulos
> > Ph.D. Student
> > Biostatistical Centre
> > School of Public Health
> > Catholic University of Leuven
> > 
> > Address: Kapucijnenvoer 35, Leuven, Belgium
> > Tel: +32/16/336899
> > Fax: +32/16/337015
> > Web: http://www.med.kuleuven.ac.be/biostat/
> >      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> > 
> > 
> > 
> > ----- Original Message ----- 
> > From: "Sander Oom" <slist at oomvanlieshout.net>
> > To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
> > Cc: <r-help at stat.math.ethz.ch>
> > Sent: Friday, June 10, 2005 12:10 PM
> > Subject: Re: [R] Replacing for loop with tapply!?
> > 
> > 
> >>Thanks Dimitris,
> >>
> >>Very impressive! Much faster than before.
> >>
> >>Thanks to new found R.basic, I can simply rotate the result with
> >>rotate270{R.basic}:
> >>
> >>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
> >>>temps <- c(37, 39, 41)
> >>>#################
> >>>#ind <- matrix(0, length(temps), ncol(mat))
> >>>ind <- matrix(0, 4, ncol(mat))
> >>>(startDate <- date())
> >>[1] "Fri Jun 10 12:08:01 2005"
> >>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
> >>>ind[4, ] <- colMeans(max(mat))
> >>Error in colMeans(max(mat)) : 'x' must be an array of at least two
> >>dimensions
> >>>(endDate <- date())
> >>[1] "Fri Jun 10 12:08:02 2005"
> >>>ind <- rotate270(ind)
> >>>ind[1:10,]
> >>   V4 V3 V2 V1
> >>1   0 56 75 80
> >>2   0 46 53 60
> >>3   0 50 58 67
> >>4   0 60 72 80
> >>5   0 59 68 76
> >>6   0 55 67 74
> >>7   0 62 77 93
> >>8   0 45 57 67
> >>9   0 57 68 75
> >>10  0 61 66 76
> >>
> >>However, I have not managed to get the row maximum using your 
> >>method? It
> >>should be 50 for most rows, but my first guess code gives an error!
> >>
> >>Any suggestions?
> >>
> >>Sander
> >>
> >>
> >>
> >>Dimitris Rizopoulos wrote:
> >>>maybe you are looking for something along these lines:
> >>>
> >>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
> >>>temps <- c(37, 39, 41)
> >>>#################
> >>>ind <- matrix(0, length(temps), ncol(mat))
> >>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
> >>>ind
> >>>
> >>>
> >>>I hope it helps.
> >>>
> >>>Best,
> >>>Dimitris
> >>>
> >>>----
> >>>Dimitris Rizopoulos
> >>>Ph.D. Student
> >>>Biostatistical Centre
> >>>School of Public Health
> >>>Catholic University of Leuven
> >>>
> >>>Address: Kapucijnenvoer 35, Leuven, Belgium
> >>>Tel: +32/16/336899
> >>>Fax: +32/16/337015
> >>>Web: http://www.med.kuleuven.ac.be/biostat/
> >>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> >>>
> >>>
> >>>----- Original Message ----- 
> >>>From: "Sander Oom" <slist at oomvanlieshout.net>
> >>>To: <r-help at stat.math.ethz.ch>
> >>>Sent: Friday, June 10, 2005 10:50 AM
> >>>Subject: [R] Replacing for loop with tapply!?
> >>>
> >>>
> >>>>Dear all,
> >>>>
> >>>>We have a large data set with temperature data for weather stations
> >>>>across the globe (15000 stations).
> >>>>
> >>>>For each station, we need to calculate the number of days a certain
> >>>>temperature is exceeded.
> >>>>
> >>>>So far we used the following S code, where mat88 is a matrix
> >>>>containing
> >>>>rows of 365 daily temperatures for each of 15000 weather stations:
> >>>>
> >>>>m <- 37
> >>>>n <- 2
> >>>>outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
> >>>>for(i in 1:nrow(mat88)) {
> >>>># i <- 3
> >>>>row1 <- as.data.frame(df88[i,  ])
> >>>>temprow37 <- select.rows(row1, row1 > m)
> >>>>temprow39 <- select.rows(row1, row1 > m + n)
> >>>>temprow41 <- select.rows(row1, row1 > m + 2 * n)
> >>>>outmat88[i, 1] <- max(row1, na.rm = T)
> >>>>outmat88[i, 2] <- count.rows(temprow37)
> >>>>outmat88[i, 3] <- count.rows(temprow39)
> >>>>outmat88[i, 4] <- count.rows(temprow41)
> >>>>}
> >>>>outmat88
> >>>>
> >>>>We have transferred the data to a more potent Linux box running R,
> >>>>but
> >>>>still hope to speed up the code.
> >>>>
> >>>>I know a for loop should be avoided when looking for speed. I also
> >>>>know
> >>>>the answer is in something like tapply, but my understanding of
> >>>>these
> >>>>commands is still to limited to see the solution. Could someone 
> >>>>show
> >>>>me
> >>>>the way!?
> >>>>
> >>>>Thanks in advance,
> >>>>
> >>>>Sander.
> >>>>--
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mike.rstat at gmail.com  Sat Jun 11 03:50:11 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 10 Jun 2005 18:50:11 -0700
Subject: [R] us zipcode data map
In-Reply-To: <27db823f05061018066c7e353d@mail.gmail.com>
References: <27db823f050610151849b39626@mail.gmail.com>
	<BAY103-F3DEFE16A93D0716704C9CA6FF0@phx.gbl>
	<27db823f05061018066c7e353d@mail.gmail.com>
Message-ID: <27db823f050610185036da4551@mail.gmail.com>

i've found a 3-column text file of zip, long, lat 
from the 2000 US census, so i am part way there.

http://www.cryptnet.net/fsp/zipdy/



From mike.rstat at gmail.com  Sat Jun 11 05:17:24 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 10 Jun 2005 20:17:24 -0700
Subject: [R] us zipcode data map
In-Reply-To: <27db823f050610185036da4551@mail.gmail.com>
References: <27db823f050610151849b39626@mail.gmail.com>
	<BAY103-F3DEFE16A93D0716704C9CA6FF0@phx.gbl>
	<27db823f05061018066c7e353d@mail.gmail.com>
	<27db823f050610185036da4551@mail.gmail.com>
Message-ID: <27db823f05061020171730c8b3@mail.gmail.com>

i'm likely looking for something that does not exist:

http://www.census.gov/cgi-bin/geo/tigerfaq?Q16

==begin quote 
Note that the task of creating a lat/long or polygon file of all ZIP Codes is
not as easy as it seems since ZIP Codes are not designed to be polygons and
can't easily be forced into them - particularly in rural areas. To our knowledge
there are no official internal point or polygon files available.
However, because
of the demand for ZIP Code data and maps, the Census Bureau, for Census 2000,
created a new statistical area called the ZIP Code Tabulation Area (ZCTA).
ZCTAs are close area approximations of U.S. Postal Service ZIP Codes and
were designed to overcome the difficulties of defining clear boundaries to which
data could be attached. For more information on ZCTAs go to:

http://www.census.gov/geo/ZCTA/zcta.html 

==end quote



From ripley at stats.ox.ac.uk  Sat Jun 11 09:14:07 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Jun 2005 08:14:07 +0100 (BST)
Subject: [R] Performance difference between 32-bit build and 64-bit
 build on Solaris 8
In-Reply-To: <5739cc2f05061013574d0fc684@mail.gmail.com>
References: <5739cc2f05061013574d0fc684@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506110757570.29456@gannet.stats>

Your tests are of problems where you really should be using an optimized 
BLAS.  But because those pointers are twice the size, the L1 cache will 
hold half as many and so I am not surprised at a factor of three on a 
naive implementation.

For linear algebra on large matrices the key to good performance is to 
keep L1 cache misses to a minimum.  Using SunPerf and a 1000x1000 problem 
I got

32-bit
[1] 4.99 0.03 5.02 0.00 0.00
64-bit
[1] 5.25 0.03 5.29 0.00 0.00

and for your regression problem
32-bit
[1] 24.97  0.96 26.15  0.00  0.00
64-bit
[1] 26.25  1.06 27.52  0.00  0.00

So the moral appears to be to take the advice in the R-admin manual and 
tune your linear algebra system.


On Fri, 10 Jun 2005, Scott Gilpin wrote:

> Hi everyone -
>
> I'm seeing a 32-bit build perform significantly faster (up to 3x) than
> a 64 bit build on Solaris 8.  I'm running R version 2.1.0.  Here are
> some of my system details, and some resulting timings:
>
>> uname -a
> SunOS lonetree 5.8 Generic_117350-16 sun4u sparc SUNW,Sun-Fire-V440
>
> lonetree /home/sgilpin >gcc -v
> Reading specs from /usr/local/lib/gcc/sparc-sun-solaris2.8/3.4.2/specs
> Configured with: ../configure --with-as=/usr/ccs/bin/as
> --with-ld=/usr/ccs/bin/ld --disable-nls
> Thread model: posix
> gcc version 3.4.2
>
> I built the 32 bit version of R with no changes to config.site.  I
> built the 64 bit version with the following in config.site:
>
> CC="gcc -m64"
> FFLAGS="-m64 -g -02"
> LDFLAGS="-L/usr/local/lib/sparcv9 -L/usr/local/lib"
> CXXFLAGS="-m64 -g -02"
>
> neither build uses a BLAS.  Both builds are installed on the same
> machine, and the same disk.  The machine has virtually no load; R is
> one of the only processes running during these timings:
>
> First comparison:  solve on a large matrix
>
>> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' |
> /disk/loneres01/R-2.1.0-32bit/bin/R -q --vanilla
>> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1] 713.45   0.38 713.93   0.00   0.00
>>
>
>> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' |
> /disk/loneres01/R-2.1.0-64bit/bin/R -q --vanilla
>> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1] 2277.05    0.31 2278.38    0.00    0.00
>>
>
> Second comparison:  linear regression
>
> lonetree /home/sgilpin/R >echo 'set.seed(1);
> y<-matrix(rnorm(10000*500),500);
> x<-matrix(runif(500*100),500);
> system.time(fit<-lm(y~x))' | /disk/loneres01/R-2.1.0-32bit/bin/R -q --vanilla
>> set.seed(1);y<-matrix(rnorm(10000*500),500);x<-matrix(runif(500*100),500);system.time(fit<-lm(y~x))
> [1] 23.34  0.80 24.17  0.00  0.00
>>
>
> lonetree /home/sgilpin/R >echo 'set.seed(1);
> y<-matrix(rnorm(10000*500),500);
> x<-matrix(runif(500*100),500);
> system.time(fit<-lm(y~x))' | /disk/loneres01/R-2.1.0-64bit/bin/R -q --vanilla
>> set.seed(1);y<-matrix(rnorm(10000*500),500);x<-matrix(runif(500*100),500);system.time(fit<-lm(y~x))
> [1] 55.34  0.70 56.21  0.00  0.00
>>
>
> Final comparison:  stats-Ex.R (from R-devel)
> lonetree /home/sgilpin/R >time /disk/loneres01/R-2.1.0-32bit/bin/R -q
> --vanilla CMD BATCH stats-Ex.R
>
> real    1m4.042s
> user    0m47.400s
> sys     0m10.390s
> lonetree /home/sgilpin/R >time /disk/loneres01/R-2.1.0-64bit/bin/R -q
> --vanilla CMD BATCH stats-Ex.R
>
> real    1m20.017s
> user    1m3.590s
> sys     0m10.130s
>
> I've seen Prof. Ripley and others comment that a 64 bit build will be
> a little slower because the pointers are larger, and gc() will take
> longer, but these timings seem out of this range.
>
> Any thoughts?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Jun 11 09:31:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Jun 2005 08:31:44 +0100 (BST)
Subject: [R] Performance difference between 32-bit build and 64-bit bu
 ild on Solaris 8
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0506110817420.29456@gannet.stats>

On Fri, 10 Jun 2005, Liaw, Andy wrote:

> I'm not familiar with Solaris, so take this with appropriate dose of NaCl...
>
> For the 64-bit build, why not have the -O2 for gcc, since you have it for
> g77 and g++?  If you just run vanilla configure for the 32-bit build, I
> believe it uses -O2 for all three compilers by default.  If that's the
> difference, perhaps it's sufficient to explain the performance difference.

CFLAGS was not specified, so should default to "-g -O2".  It is definitely 
worth checking, although almost all the time in these tests will be spent 
in Fortran code.

> The other thing is that solve(), and to some extent lm(), can benefit from
> an optimized BLAS.  You might want to check if the two builds are using
> equivalent BLAS.

He said:

>> neither build uses a BLAS.

Well, they do, the one supplied with R (which is in Fortran).

To supplement my timings earlier, with an optimized BLAS I gave

32-bit
[1] 4.99 0.03 5.02 0.00 0.00
64-bit
[1] 5.25 0.03 5.29 0.00 0.00

and for gcc 3.4.3 and the internal BLAS (which I had to build for these 
tests) I get

32-bit
[1]  9.96  0.09 10.12  0.00  0.00
64-bit
[1]  9.93  0.04 10.04  0.00  0.00

so I am not seeing anything like the same performance difference between 
32- and 64-bit builds (but it could well depend on the particular Sparc 
chip).

There are some problems in which the 64-bit builds _are_ much slower: 
complex matrix arithmetic is one (presumably because less effort has been 
spent on optimizing that).

>
> Andy
>
>> From: Scott Gilpin
>>
>> Hi everyone -
>>
>> I'm seeing a 32-bit build perform significantly faster (up to 3x) than
>> a 64 bit build on Solaris 8.  I'm running R version 2.1.0.  Here are
>> some of my system details, and some resulting timings:
>>
>>> uname -a
>> SunOS lonetree 5.8 Generic_117350-16 sun4u sparc SUNW,Sun-Fire-V440
>>
>> lonetree /home/sgilpin >gcc -v
>> Reading specs from /usr/local/lib/gcc/sparc-sun-solaris2.8/3.4.2/specs
>> Configured with: ../configure --with-as=/usr/ccs/bin/as
>> --with-ld=/usr/ccs/bin/ld --disable-nls
>> Thread model: posix
>> gcc version 3.4.2
>>
>> I built the 32 bit version of R with no changes to config.site.  I
>> built the 64 bit version with the following in config.site:
>>
>> CC="gcc -m64"
>> FFLAGS="-m64 -g -02"
>> LDFLAGS="-L/usr/local/lib/sparcv9 -L/usr/local/lib"
>> CXXFLAGS="-m64 -g -02"
>>
>> neither build uses a BLAS.  Both builds are installed on the same
>> machine, and the same disk.  The machine has virtually no load; R is
>> one of the only processes running during these timings:

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From marc.girondot at ese.u-psud.fr  Sat Jun 11 10:06:20 2005
From: marc.girondot at ese.u-psud.fr (Marc Girondot)
Date: Sat, 11 Jun 2005 10:06:20 +0200
Subject: [R] Problem with multinom ?
In-Reply-To: <20050610214000.WMGZ19894.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20050610214000.WMGZ19894.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <p06210200bed04abc0056@[62.147.109.30]>

Thanks for your response.
OK, multinom() is a more logical in this context.

But similar problem occurs:

Let these data to be analyzed using classical glm with binomial error:

m   f   factor   m theo              f theo 
-Ln L model        -Ln L full          interecept 
f
10  12  1.2      0.452494473  0.547505527 
1.778835688  1.778648963    2.632455675 
-2.034882223
14  14  1.3      0.503222759  0.496777241  1.901401922  1.900820284
15  12  1.4      0.553884782  0.446115218  1.877062369  1.876909821

                                 Sum -Ln L 
5.557299979  5.556379068  Residual deviance
                                 Deviance 
11.11459996  11.11275814   0.001841823

If I try to use multinom() function to analyze 
these data, the fitted parameters are correct but 
the residual deviance not.

>  dt<-read.table('/try.txt'. header=T)
>  dt
   output factor  n
1      m    1.2 10
2      f    1.2 12
3      m    1.3 14
4      f    1.3 14
5      m    1.4 15
6      f    1.4 12
>  dt.plr <- multinom(output ~ factor. data=dt. weights=n. maxit=1000)
# weights:  3 (2 variable)
initial  value 53.372333
iter  10 value 53.115208
iter  10 value 53.115208
iter  10 value 53.115208
final  value 53.115208
converged
>  dt.plr
Call:
multinom(formula = output ~ factor. data = dt. weights = n. maxit = 1000)

Coefficients:
(Intercept)      factor
   -2.632443    2.034873

Residual Deviance: 106.2304
AIC: 110.2304

>  dt.pr1<-predict(dt.plr. . type="probs")
>  dt.pr1
         1         2         3         4         5         6
0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846

Probability for 2. 4 and 6 are not correct and 
its explain the non-correct residual deviance 
obtained in R.
Probably the problem I have is due to an 
incorrect data format... could someone help me... 
Thanks

(I know there is a simple way to analyze binomial 
data. but in fine I want to use multinom() for 5 
categories of outputs.


Thanks a lot

Marc
-- 

__________________________________________________________
Marc Girondot, Pr
Laboratoire Ecologie, Syst??matique et Evolution
Equipe de Conservation des Populations et des Communaut??s
CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
B??timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
15 56 96   e-mail: marc.girondot at ese.u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot
Fax in US: 1-425-732-6934



From Roger.Bivand at nhh.no  Sat Jun 11 10:58:59 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sat, 11 Jun 2005 10:58:59 +0200 (CEST)
Subject: [R] us zipcode data map
In-Reply-To: <27db823f050610151849b39626@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0506111047190.6432-100000@reclus.nhh.no>

On Fri, 10 Jun 2005, Mike R wrote:

> i've search the email archives, searched the documention 
> of various map packages and done an R-site search, but 
> have been unable to find direct resources for creating maps 
> of the US that are colored or annotated or ... by zipcode 
> data.  
> 
> For example, create a map of the US and color each zipcode 
> region by its population using two vectors z,p containing the 
> zipcode and population, respectively.  I'm not looking for data 
> to fill z, and p.  I'm looking for some highend functions to 
> display on a map the data that I already have.

I'm replying to this original message, because your followups have been 
dropping the original query (please don't assume that everybody uses the 
e-mail system you have chosen yourself).

With the points you have found, you can delineate surrounding polygons and 
clip to the coastline using functions in the tripack, gpclib, and maps 
packages (being very careful to find out how to match the output polygon 
IDs to your data).

But unless you are aiming for A0 postscript output, for the continental US
viewed as a whole, almost all raster image processors will merge the
interesting metropolitan zip polygons, leaving the viewer looking at a
mess with only the areas with lowest population density legible. Even zip
code "maps" of small states are very difficult to use for look-up (reading
values from the polygon fill), one of the main functions of thematic
cartography.

So like most tasks, it can be done (this _is_ R, after all), but do you 
really need to do it?

Roger

> 
> Cheers,
> Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From petr.pikal at precheza.cz  Sat Jun 11 12:28:38 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Sat, 11 Jun 2005 12:28:38 +0200
Subject: [R] Replacing for loop with tapply!?
In-Reply-To: <42A9D685.5050806@oomvanlieshout.net>
References: <007001c56ddf$4f80c1a0$0540210a@www.domain>
Message-ID: <42AAD8F6.7121.1E1D2A@localhost>

Hi

On 10 Jun 2005 at 20:05, Sander Oom wrote:

> Dear all,
> 
> Dimitris and Andy, thanks for your great help. I have progressed to
> the following code which runs very fast and effective:
> 
> mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)
> mat[mat>45] <- NA

> mat<-NA

By this you redefine mat as 

> str(mat)
 logi NA
>

and your code gives an error that it has to have some dimensions

+      apply(mat, 1, max, na.rm=TRUE))
Error in rowSums(mat > temp, na.rm = TRUE) : 
        'x' must be an array of at least two dimensions
>

If your matrix has one row full of NA's it only complains but 
computes a value. 

> mat[3,]<-NA
> temps <- c(35, 37, 39)
> ind <- rbind(
+      t(sapply(temps, function(temp)
+        rowSums(mat > temp, na.rm=TRUE) )),
+      rowSums(!is.na(mat), na.rm=FALSE),
+      apply(mat, 1, max, na.rm=TRUE))
Warning message:
no finite arguments to max; returning -Inf 
> ind <- t(ind)
> ind

> ind
      [,1] [,2] [,3] [,4] [,5]
 [1,]    5    5    3    9   48
 [2,]    1    1    1    9   42
 [3,]    0    0    0    0 -Inf
 
> mat
> temps <- c(35, 37, 39)
> ind <- rbind(
>      t(sapply(temps, function(temp)
>        rowSums(mat > temp, na.rm=TRUE) )),
>      rowSums(!is.na(mat), na.rm=FALSE),
>      apply(mat, 1, max, na.rm=TRUE))
> ind <- t(ind)
> ind
> 
> However, some weather stations have missing values for the whole year.
> Unfortunately, the code breaks down (when uncommenting mat<-NA).
> 
> I have tried 'ifelse' statements in the functions, but it becomes even
> more of a mess. I could subset the matrix before hand, but this would
> mean merging with a complete matrix afterwards to make it compatible
> with other years. That would slow things down.
> 
> How can I make the code robust for rows containing all missing values?


which(rowSums(!is.na(mat))==0) 
This gives you indices which lines of your matrix has all values NA 
and you can use it for fine tuning of your code. What you need to 
do depends on what results do you want, how ind matrix should 
look like after processing mat with one or more rows full of NA's.

HTH
Petr


> 
> Thanks for your help,
> 
> Sander.
> 
> Dimitris Rizopoulos wrote:
> > for the maximum you could use something like:
> > 
> > ind[, 1] <- apply(mat, 2, max)
> > 
> > I hope it helps.
> > 
> > Best,
> > Dimitris
> > 
> > ----
> > Dimitris Rizopoulos
> > Ph.D. Student
> > Biostatistical Centre
> > School of Public Health
> > Catholic University of Leuven
> > 
> > Address: Kapucijnenvoer 35, Leuven, Belgium
> > Tel: +32/16/336899
> > Fax: +32/16/337015
> > Web: http://www.med.kuleuven.ac.be/biostat/
> >      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> > 
> > 
> > 
> > ----- Original Message ----- 
> > From: "Sander Oom" <slist at oomvanlieshout.net>
> > To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be> Cc:
> > <r-help at stat.math.ethz.ch> Sent: Friday, June 10, 2005 12:10 PM
> > Subject: Re: [R] Replacing for loop with tapply!?
> > 
> > 
> >>Thanks Dimitris,
> >>
> >>Very impressive! Much faster than before.
> >>
> >>Thanks to new found R.basic, I can simply rotate the result with
> >>rotate270{R.basic}:
> >>
> >>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
> >>>temps <- c(37, 39, 41)
> >>>#################
> >>>#ind <- matrix(0, length(temps), ncol(mat))
> >>>ind <- matrix(0, 4, ncol(mat))
> >>>(startDate <- date())
> >>[1] "Fri Jun 10 12:08:01 2005"
> >>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
> >>>ind[4, ] <- colMeans(max(mat))
> >>Error in colMeans(max(mat)) : 'x' must be an array of at least two
> >>dimensions
> >>>(endDate <- date())
> >>[1] "Fri Jun 10 12:08:02 2005"
> >>>ind <- rotate270(ind)
> >>>ind[1:10,]
> >>   V4 V3 V2 V1
> >>1   0 56 75 80
> >>2   0 46 53 60
> >>3   0 50 58 67
> >>4   0 60 72 80
> >>5   0 59 68 76
> >>6   0 55 67 74
> >>7   0 62 77 93
> >>8   0 45 57 67
> >>9   0 57 68 75
> >>10  0 61 66 76
> >>
> >>However, I have not managed to get the row maximum using your 
> >>method? It
> >>should be 50 for most rows, but my first guess code gives an error!
> >>
> >>Any suggestions?
> >>
> >>Sander
> >>
> >>
> >>
> >>Dimitris Rizopoulos wrote:
> >>>maybe you are looking for something along these lines:
> >>>
> >>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
> >>>temps <- c(37, 39, 41)
> >>>#################
> >>>ind <- matrix(0, length(temps), ncol(mat))
> >>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
> >>>ind
> >>>
> >>>
> >>>I hope it helps.
> >>>
> >>>Best,
> >>>Dimitris
> >>>
> >>>----
> >>>Dimitris Rizopoulos
> >>>Ph.D. Student
> >>>Biostatistical Centre
> >>>School of Public Health
> >>>Catholic University of Leuven
> >>>
> >>>Address: Kapucijnenvoer 35, Leuven, Belgium
> >>>Tel: +32/16/336899
> >>>Fax: +32/16/337015
> >>>Web: http://www.med.kuleuven.ac.be/biostat/
> >>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> >>>
> >>>
> >>>----- Original Message ----- 
> >>>From: "Sander Oom" <slist at oomvanlieshout.net>
> >>>To: <r-help at stat.math.ethz.ch>
> >>>Sent: Friday, June 10, 2005 10:50 AM
> >>>Subject: [R] Replacing for loop with tapply!?
> >>>
> >>>
> >>>>Dear all,
> >>>>
> >>>>We have a large data set with temperature data for weather
> >>>>stations across the globe (15000 stations).
> >>>>
> >>>>For each station, we need to calculate the number of days a
> >>>>certain temperature is exceeded.
> >>>>
> >>>>So far we used the following S code, where mat88 is a matrix
> >>>>containing
> >>>>rows of 365 daily temperatures for each of 15000 weather stations:
> >>>>
> >>>>m <- 37
> >>>>n <- 2
> >>>>outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
> >>>>for(i in 1:nrow(mat88)) {
> >>>># i <- 3
> >>>>row1 <- as.data.frame(df88[i,  ])
> >>>>temprow37 <- select.rows(row1, row1 > m)
> >>>>temprow39 <- select.rows(row1, row1 > m + n)
> >>>>temprow41 <- select.rows(row1, row1 > m + 2 * n)
> >>>>outmat88[i, 1] <- max(row1, na.rm = T)
> >>>>outmat88[i, 2] <- count.rows(temprow37)
> >>>>outmat88[i, 3] <- count.rows(temprow39)
> >>>>outmat88[i, 4] <- count.rows(temprow41)
> >>>>}
> >>>>outmat88
> >>>>
> >>>>We have transferred the data to a more potent Linux box running R,
> >>>>but still hope to speed up the code.
> >>>>
> >>>>I know a for loop should be avoided when looking for speed. I also
> >>>>know the answer is in something like tapply, but my understanding
> >>>>of these commands is still to limited to see the solution. Could
> >>>>someone show me the way!?
> >>>>
> >>>>Thanks in advance,
> >>>>
> >>>>Sander.
> >>>>--
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From nshephard at gmail.com  Sat Jun 11 13:03:54 2005
From: nshephard at gmail.com (Neil Shephard)
Date: Sat, 11 Jun 2005 11:03:54 +0000
Subject: [R] CRAN task view for genetics
Message-ID: <20050611110354.56945e25.nshephard@gmail.com>

Hi Gregor,

The compiled list of genetics packages is a great idea, I'll certainly find it useful as it took me quite some time to track them all down through CRAN.

I've a couple of things that could be added...

qvalue - A procedure for false discovery rate control.  General, but developed for the general area of genetics (see Sotrey & Tibshirani 2003 PNAS 100:9440-9445)

SimHap - still in development (Beta testing), this is a Java GUI front end for performing a range of statistical genetics analyses, very useful for occasional users who can not get their heads round CLI's.  Registration with the author is currently required.

Regards

Neil
-- 
Neil Shephard
Genetics Statistician, ARC Epidemiology Unit
neil.shephard at manchester.ac.uk
nshephard at gmail.com



From petr.pikal at precheza.cz  Sat Jun 11 12:50:58 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Sat, 11 Jun 2005 12:50:58 +0200
Subject: [R] Sum up the values of a function
In-Reply-To: <20050610135854.v88ngwzmskw4so4c@webmail.uni-mannheim.de>
References: <42A99762.8396.1D8CF1@localhost>
Message-ID: <42AADE32.7461.329036@localhost>

Hi

On 10 Jun 2005 at 13:58, Stefan Wagner wrote:

> I am really sorry, that the code appears in a bad way but it is really
> not my fault (at least I think so). I think my mail-programm is
> responsible for it because I sent it in a "normal" way without havin g
> trouble with my space bar. Here are the missing values (but please
> remember, this is only a short example, normally there are more than
> 400 values) x1 <- c(34.67,19.91,47.48,22.48,17.34,24.42) m1 <-
> c(1,0,0,0,0,0) n1 <- c(1,1,1,1,1,1) dec1 <- c(0,0,0,0,1,0) x2 <-
> c(50.98,30.63,31.37,21.17,21.49,39.41,46.85,38.08) m2 <-
> c(1,0,0,0,1,0,2,1) n2 <- c(3,3,3,3,3,3,3,3) dec2 <- c(0,0,0,0,0,0,0,0)
> 
> I hope this is all you need,

Well, actually I am not sure why you do all computations in so 
many functions. It will hide all things and one should go through all 
of them step by step without much knowledge why they are 
constructed this way.

Maybe some other list member can give you any reasonable advice.

Sorry I could not give you some positive answer.

Petr


> 
> Stefan
> 
> Zitat von Petr Pikal <petr.pikal at precheza.cz>:
> 
> > Hi
> >
> > Your example is not reproducible as we do not know x1.
> > Do you by chance seek for something like aggregate? If yes see
> >
> > ?aggregate
> > or
> > ?by
> >
> > BTW, do you have some trouble with your space bar?
> >
> > HTH
> >
> > Petr
> >
> > On 10 Jun 2005 at 13:08, Stefan Wagner wrote:
> >
> >> Dear R-Users,
> >>
> >> I have to do a maximum-likelihood estimation and have now a problem
> >> concerning how to sum up my function values in a smart way. I don't
> >> know how to explain it easyly, so I give you the code and show you
> >> where my problem is. I have shorten the code a little bit, so that
> >> you only get the necessary facts:
> >>
> >> ws12 <- function (z, i) (1/(1+exp(z[1]*(z[3]-x1[i]-
> >> z[4]*(m1[i]/n1[i]-0.5))))) ws37 <- function (z, i)
> >> (1/(1+exp(z[2]*(z[3]-x2[i]- z[5]*(m2[i]/n2[i]-0.5))))) wsAttack12
> >> <- function (z,i) (ws12(z,i)*dec1[i]+(1-ws12(z,i))*(1-dec1[i]))
> >> wsAttack37 <- function (z,i)
> >> (ws37(z,i)*dec2[i]+(1-ws37(z,i))*(1-dec2[i])) logwsAttack12 <-
> >> function (z,i) (log(wsAttack12(z,i))) logwsAttack37 <- function
> >> (z,i) (log(wsAttack37(z,i))) ws12sum <- function (z)
> >> (logwsAttack12(z,i=1)+logwsAttack12(z,i=2)+logwsAttack12(z,i=3)+log
> >> wsA ttack12(z,i=4)+logwsAttack12(z,i=5)+logwsAttack12(z,i=6))
> >> ws37sum <- function (z)
> >> (logwsAttack37(z,i=1)+logwsAttack37(z,i=2)+logwsAttack37(z,i=3)+log
> >> wsA
> >> ttack37(z,i=4)+logwsAttack37(z,i=5)+logwsAttack37(z,i=6)+logwsAttac
> >> k37 (z,i=7)+logwsAttack37(z,i=8)) wsLOG <- function (z) (ws12sum(z)
> >> + ws37sum(z)) LogSum <- function (z) (-sum(wsLOG(z))) SP <- c(0.16,
> >> 0.10, 44, 0.80, 46) out <- nlm (LogSum, p=SP) out
> >>
> >> For explanation: x1[i], x2[i], m1[i], m2[i], n1[i], n2[i] are given
> >> data and z[1:5] are my estimates. My problem is that I have more
> >> than one session with diffent number of datas so that I am
> >> searching for a general way of summing up my logwsAttack12 and
> >> logwsAttack37. The program should recognize how many data are in my
> >> table concerning ws12 and how many concerning ws37 and should, in
> >> dependency of z, sum them up. I hope you understand my problem and
> >> hopefully someone is able to solve it. Many thanks for the moment.
> >>
> >> Best regards,
> >>
> >> Stefan Wagner
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From no-reply at domainz.net.nz  Sat Jun 11 13:35:05 2005
From: no-reply at domainz.net.nz (A-4Service)
Date: Sat, 11 Jun 2005 23:35:05 +1200
Subject: [R] Customer Service enquiry
Message-ID: <E2DB26E46F7AD611829300065B3DDB8527C3B7@dnzpdc>

Dear Valued Customer


Thank you for contacting Domainz. Your email enquiry has been received and
will be attended by a member of our Customer Service Team. 

Please note that this is an system generated email response to confirm the
receipt of your request, if you need to contact us urgently please do not
hesitate to call us on 0800 3662469, or try the 'Frequently asked questions'
on our website
http://www.domainz.net.nz/info.asp?menu=help&content=faq&page=FAQs

Thank you for choosing Domainz.


Domainz Customer Service Team

Domainz Limited - A Melbourne IT Company
4service at domainz.net.nz
p: +64 4 473 4567 f: +64 4 473 4569
Level 4, Greenock House, 102-112 Lambton Quay, Wellington, NZ??????
www.domainz.net.nz



From jfox at mcmaster.ca  Sat Jun 11 13:55:51 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 11 Jun 2005 07:55:51 -0400
Subject: [R] Problem with multinom ?
In-Reply-To: <p06210200bed04abc0056@[62.147.109.30]>
Message-ID: <20050611115554.FYIY26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Marc,

I get the same results -- same coefficients, standard errors, and fitted
probabilities -- from multinom() and glm(). It's true that the deviances
differ, but they, I believe, are defined only up to an additive constant:

> dt
  output factor  n
1      m    1.2 10
2      f    1.2 12
3      m    1.3 14
4      f    1.3 14
5      m    1.4 15
6      f    1.4 12

> dt.m <- multinom(output ~ factor, data=dt, weights=n)
# weights:  3 (2 variable)
initial  value 53.372333 
iter  10 value 53.115208
iter  10 value 53.115208
iter  10 value 53.115208
final  value 53.115208 
converged

> dt2
   m  f factor
1 10 12    1.2
2 14 14    1.3
3 15 12    1.4

> dt.b <- glm(cbind(m,f) ~ factor, data=dt2, family=binomial)

> summary(dt.m)
Call:
multinom(formula = output ~ factor, data = dt, weights = n)

Coefficients:
               Values Std. Err.
(Intercept) -2.632443  3.771265
factor       2.034873  2.881479

Residual Deviance: 106.2304 
AIC: 110.2304 

Correlation of Coefficients:
[1] -0.9981598

> summary(dt.b)

Call:
glm(formula = cbind(m, f) ~ factor, family = binomial, data = dt2)

Deviance Residuals: 
       1         2         3  
 0.01932  -0.03411   0.01747  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   -2.632      3.771  -0.698    0.485
factor         2.035      2.881   0.706    0.480

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 0.5031047  on 2  degrees of freedom
Residual deviance: 0.0018418  on 1  degrees of freedom
AIC: 15.115

Number of Fisher Scoring iterations: 2

> predict(dt.b, type="response")
        1         2         3 
0.4524946 0.5032227 0.5538845 
 
> predict(dt.m, type="probs")
        1         2         3         4         5         6 
0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846 

These fitted probabilities *are* correct: Since the members of each pair
(1,2), (3,4), and (5,6) have identical values of factor they are identical
fitted probabilities.

(Note, by the way, the large negative correlation between the coefficients,
produced by the configuration of factor values.)

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Girondot
> Sent: Saturday, June 11, 2005 3:06 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] Problem with multinom ?
> 
> Thanks for your response.
> OK, multinom() is a more logical in this context.
> 
> But similar problem occurs:
> 
> Let these data to be analyzed using classical glm with binomial error:
> 
> m   f   factor   m theo              f theo 
> -Ln L model        -Ln L full          interecept 
> f
> 10  12  1.2      0.452494473  0.547505527 
> 1.778835688  1.778648963    2.632455675 
> -2.034882223
> 14  14  1.3      0.503222759  0.496777241  1.901401922  1.900820284
> 15  12  1.4      0.553884782  0.446115218  1.877062369  1.876909821
> 
>                                  Sum -Ln L
> 5.557299979  5.556379068  Residual deviance
>                                  Deviance 
> 11.11459996  11.11275814   0.001841823
> 
> If I try to use multinom() function to analyze these data, 
> the fitted parameters are correct but the residual deviance not.
> 
> >  dt<-read.table('/try.txt'. header=T)
> >  dt
>    output factor  n
> 1      m    1.2 10
> 2      f    1.2 12
> 3      m    1.3 14
> 4      f    1.3 14
> 5      m    1.4 15
> 6      f    1.4 12
> >  dt.plr <- multinom(output ~ factor. data=dt. weights=n. maxit=1000)
> # weights:  3 (2 variable)
> initial  value 53.372333
> iter  10 value 53.115208
> iter  10 value 53.115208
> iter  10 value 53.115208
> final  value 53.115208
> converged
> >  dt.plr
> Call:
> multinom(formula = output ~ factor. data = dt. weights = n. 
> maxit = 1000)
> 
> Coefficients:
> (Intercept)      factor
>    -2.632443    2.034873
> 
> Residual Deviance: 106.2304
> AIC: 110.2304
> 
> >  dt.pr1<-predict(dt.plr. . type="probs")
> >  dt.pr1
>          1         2         3         4         5         6
> 0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846
> 
> Probability for 2. 4 and 6 are not correct and its explain 
> the non-correct residual deviance obtained in R.
> Probably the problem I have is due to an incorrect data 
> format... could someone help me... 
> Thanks
> 
> (I know there is a simple way to analyze binomial data. but 
> in fine I want to use multinom() for 5 categories of outputs.
> 
> 
> Thanks a lot
> 
> Marc
> -- 
> 
> __________________________________________________________
> Marc Girondot, Pr
> Laboratoire Ecologie, Syst??matique et Evolution Equipe de 
> Conservation des Populations et des Communaut??s CNRS, ENGREF 
> et Universit?? Paris-Sud 11 , UMR 8079 B??timent 362
> 91405 Orsay Cedex, France
> 
> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
> 15 56 96   e-mail: marc.girondot at ese.u-psud.fr
> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
> Skype: girondot
> Fax in US: 1-425-732-6934
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Sat Jun 11 15:01:26 2005
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Sat, 11 Jun 2005 14:01:26 +0100
Subject: [R] TaskView for ecological and environmental data
Message-ID: <42AAE0A6.2030803@ucl.ac.uk>

Dear list,

Following off-list discussion with Graham Smith (who initiated the 
initial request for a task view on this topic) and Philippe Grosjean I 
have produced a Task View for ecological and environmental data 
analysis. Graham and Philippe commented on a earlier draft and it is now 
in a shape suitable for public scrutiny.

I have placed a mock-up of the html version of a task view page at the 
URI below and would welcome your comments and suggestions before I 
submit it to Achim Zeileis for inclusion in the ctv package.

http://ecrc3.geog.ucl.ac.uk/download/taskview/

I will now be out of the office for a week and am not sure what email 
availability I will have so it may take a little while for me to reply 
to and act on any suggestions you might make.

I would like to thank Graham and Philippe for encouraging me to maintain 
this view and for their helpful suggestions.

All the best,

Gavin



From sumit_m_84 at yahoo.co.uk  Sat Jun 11 15:59:03 2005
From: sumit_m_84 at yahoo.co.uk (SUMIT MALHOTRA)
Date: Sat, 11 Jun 2005 14:59:03 +0100 (BST)
Subject: [R]  Calling R from C/C
Message-ID: <20050611135903.4545.qmail@web25501.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050611/dbbb79f5/attachment.pl

From ripley at stats.ox.ac.uk  Sat Jun 11 16:31:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Jun 2005 15:31:04 +0100 (BST)
Subject: [R] Problem with multinom ?
In-Reply-To: <20050611115554.FYIY26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20050611115554.FYIY26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.61.0506111510081.10254@gannet.stats>

On Sat, 11 Jun 2005, John Fox wrote:

> Dear Marc,
>
> I get the same results -- same coefficients, standard errors, and fitted
> probabilities -- from multinom() and glm(). It's true that the deviances
> differ, but they, I believe, are defined only up to an additive constant:

Yes. There are many variations on the definition of (residual) deviance, 
but it compares -2 log likelihood with a `saturated' model.  For grouped 
data you have a choice: a separate term for each group or for each 
observation.  A binomial GLM uses the first but the second is more
normal in logistic regression (since it has a direct interpretation via 
log-probability scoring).

multinom() is support software for a book (which the R posting guide does 
ask you to consult): this is discussed with a worked example on pp 203-4.

>> dt
>  output factor  n
> 1      m    1.2 10
> 2      f    1.2 12
> 3      m    1.3 14
> 4      f    1.3 14
> 5      m    1.4 15
> 6      f    1.4 12
>
>> dt.m <- multinom(output ~ factor, data=dt, weights=n)
> # weights:  3 (2 variable)
> initial  value 53.372333
> iter  10 value 53.115208
> iter  10 value 53.115208
> iter  10 value 53.115208
> final  value 53.115208
> converged
>
>> dt2
>   m  f factor
> 1 10 12    1.2
> 2 14 14    1.3
> 3 15 12    1.4
>
>> dt.b <- glm(cbind(m,f) ~ factor, data=dt2, family=binomial)
>
>> summary(dt.m)
> Call:
> multinom(formula = output ~ factor, data = dt, weights = n)
>
> Coefficients:
>               Values Std. Err.
> (Intercept) -2.632443  3.771265
> factor       2.034873  2.881479
>
> Residual Deviance: 106.2304
> AIC: 110.2304
>
> Correlation of Coefficients:
> [1] -0.9981598
>
>> summary(dt.b)
>
> Call:
> glm(formula = cbind(m, f) ~ factor, family = binomial, data = dt2)
>
> Deviance Residuals:
>       1         2         3
> 0.01932  -0.03411   0.01747
>
> Coefficients:
>            Estimate Std. Error z value Pr(>|z|)
> (Intercept)   -2.632      3.771  -0.698    0.485
> factor         2.035      2.881   0.706    0.480
>
> (Dispersion parameter for binomial family taken to be 1)
>
>    Null deviance: 0.5031047  on 2  degrees of freedom
> Residual deviance: 0.0018418  on 1  degrees of freedom
> AIC: 15.115
>
> Number of Fisher Scoring iterations: 2
>
>> predict(dt.b, type="response")
>        1         2         3
> 0.4524946 0.5032227 0.5538845
>
>> predict(dt.m, type="probs")
>        1         2         3         4         5         6
> 0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846
>
> These fitted probabilities *are* correct: Since the members of each pair
> (1,2), (3,4), and (5,6) have identical values of factor they are identical
> fitted probabilities.
>
> (Note, by the way, the large negative correlation between the coefficients,
> produced by the configuration of factor values.)
>
> I hope this helps,
> John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Girondot
>> Sent: Saturday, June 11, 2005 3:06 AM
>> To: John Fox
>> Cc: r-help at stat.math.ethz.ch
>> Subject: [R] Problem with multinom ?
>>
>> Thanks for your response.
>> OK, multinom() is a more logical in this context.
>>
>> But similar problem occurs:
>>
>> Let these data to be analyzed using classical glm with binomial error:
>>
>> m   f   factor   m theo              f theo
>> -Ln L model        -Ln L full          interecept
>> f
>> 10  12  1.2      0.452494473  0.547505527
>> 1.778835688  1.778648963    2.632455675
>> -2.034882223
>> 14  14  1.3      0.503222759  0.496777241  1.901401922  1.900820284
>> 15  12  1.4      0.553884782  0.446115218  1.877062369  1.876909821
>>
>>                                  Sum -Ln L
>> 5.557299979  5.556379068  Residual deviance
>>                                  Deviance
>> 11.11459996  11.11275814   0.001841823
>>
>> If I try to use multinom() function to analyze these data,
>> the fitted parameters are correct but the residual deviance not.
>>
>>>  dt<-read.table('/try.txt'. header=T)
>>>  dt
>>    output factor  n
>> 1      m    1.2 10
>> 2      f    1.2 12
>> 3      m    1.3 14
>> 4      f    1.3 14
>> 5      m    1.4 15
>> 6      f    1.4 12
>>>  dt.plr <- multinom(output ~ factor. data=dt. weights=n. maxit=1000)
>> # weights:  3 (2 variable)
>> initial  value 53.372333
>> iter  10 value 53.115208
>> iter  10 value 53.115208
>> iter  10 value 53.115208
>> final  value 53.115208
>> converged
>>>  dt.plr
>> Call:
>> multinom(formula = output ~ factor. data = dt. weights = n.
>> maxit = 1000)
>>
>> Coefficients:
>> (Intercept)      factor
>>    -2.632443    2.034873
>>
>> Residual Deviance: 106.2304
>> AIC: 110.2304
>>
>>>  dt.pr1<-predict(dt.plr. . type="probs")
>>>  dt.pr1
>>          1         2         3         4         5         6
>> 0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846
>>
>> Probability for 2. 4 and 6 are not correct and its explain
>> the non-correct residual deviance obtained in R.
>> Probably the problem I have is due to an incorrect data
>> format... could someone help me...
>> Thanks
>>
>> (I know there is a simple way to analyze binomial data. but
>> in fine I want to use multinom() for 5 categories of outputs.
>>
>>
>> Thanks a lot
>>
>> Marc
>> --
>>
>> __________________________________________________________
>> Marc Girondot, Pr
>> Laboratoire Ecologie, Syst?matique et Evolution Equipe de
>> Conservation des Populations et des Communaut?s CNRS, ENGREF
>> et Universit? Paris-Sud 11 , UMR 8079 B?timent 362
>> 91405 Orsay Cedex, France
>>
>> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69
>> 15 56 96   e-mail: marc.girondot at ese.u-psud.fr
>> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
>> Skype: girondot
>> Fax in US: 1-425-732-6934
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Sat Jun 11 16:34:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Jun 2005 15:34:52 +0100 (BST)
Subject: [R] Calling R from C/C
In-Reply-To: <20050611135903.4545.qmail@web25501.mail.ukl.yahoo.com>
References: <20050611135903.4545.qmail@web25501.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506111531470.10254@gannet.stats>

This is discussed in detail in the R-admin manual for R 2.1.0 (the current 
version).  There is no point in repeating the whole discussion (several 
pages) here: that is the definitive account and you do need the whole 
picture.

On Sat, 11 Jun 2005, SUMIT MALHOTRA wrote:

> hi ppl,
> this somethin very urgent. plz anybody tell me for my problem:-
>
> how to make a module/lib that will allow to call easily R code/functions
> from C++ (C++ Builder 6). Is it possible without using any intermediate
> things.
>
> plz help
>
> srry no time for RTFM. Any idea or hint is appreciated.
>
> n I can help u a lot in projects sort of things.
>
> plz do reply.
>
>
> byee

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ivirandthezivir at gmail.com  Sat Jun 11 16:58:46 2005
From: ivirandthezivir at gmail.com (dwfu)
Date: Sat, 11 Jun 2005 16:58:46 +0200
Subject: [R] R-help Digest, Vol 28, Issue 11
In-Reply-To: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
References: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
Message-ID: <c5fb58b105061107582e77432a@mail.gmail.com>

Dear all,
I'm new using R and in (geo)statistics. I have a problem with solving
my homework questions. We are working with variograms and trying to
write down basic  equations for different models (spherical,
exponential, Gaussian). I tried to use the 'gstat' and 'geoR' packages
to solve the questions but as I said before I'm new in R and always
encountered with some syntax errors (I can send some specific examples
later). If one of you used this packages and could help me,  I will be
very glad.
Best Wishes,
Emre Duran



From RRoa at fisheries.gov.fk  Sat Jun 11 15:04:29 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Sat, 11 Jun 2005 11:04:29 -0200
Subject: [R] R-help Digest, Vol 28, Issue 11
Message-ID: <03DCBBA079F2324786E8715BE538968A068D50@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of dwfu
> Sent: 11 June 2005 13:59
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] R-help Digest, Vol 28, Issue 11
> 
> 
> Dear all,
> I'm new using R and in (geo)statistics. I have a problem with solving
> my homework questions. We are working with variograms and trying to
> write down basic  equations for different models (spherical,
> exponential, Gaussian). I tried to use the 'gstat' and 'geoR' packages
> to solve the questions but as I said before I'm new in R and always
> encountered with some syntax errors (I can send some specific examples
> later). If one of you used this packages and could help me,  I will be
> very glad.
> Best Wishes,
> Emre Duran

For geoR, go here
http://www.est.ufpr.br/geoR/ 
and study the illustrative session.
Ruben



From wildscop at yahoo.com  Sat Jun 11 17:05:19 2005
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Sat, 11 Jun 2005 08:05:19 -0700 (PDT)
Subject: [R] Calculating moments of a distribution
Message-ID: <20050611150519.38522.qmail@web52601.mail.yahoo.com>

Dear All,

Is there any existing package or direct function in R
/ S-plus that calculates moments (raw or central); the
usual measures of skewness and kurtosis, and/or
pearsonian k criterion?

Thank you for your time. 


----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD



From ligges at statistik.uni-dortmund.de  Sat Jun 11 17:09:11 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 11 Jun 2005 17:09:11 +0200
Subject: [R] R-help Digest, Vol 28, Issue 11
In-Reply-To: <c5fb58b105061107582e77432a@mail.gmail.com>
References: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
	<c5fb58b105061107582e77432a@mail.gmail.com>
Message-ID: <42AAFE97.8070004@statistik.uni-dortmund.de>

dwfu wrote:
> Dear all,
> I'm new using R and in (geo)statistics. I have a problem with solving
> my homework questions. We are working with variograms and trying to
> write down basic  equations for different models (spherical,
> exponential, Gaussian). I tried to use the 'gstat' and 'geoR' packages
> to solve the questions but as I said before I'm new in R and always
> encountered with some syntax errors (I can send some specific examples
> later). If one of you used this packages and could help me,  I will be
> very glad.

Please read the posting guide which tells you:
  - "Use an informative subject line."
  - "Basic statistics and classroom homework: R-help is not intended for 
these."
  - Provide small reproducible examples.

Uwe Ligges

> Best Wishes,
> Emre Duran
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sat Jun 11 17:32:53 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 11 Jun 2005 08:32:53 -0700
Subject: [R] Calculating moments of a distribution
In-Reply-To: <20050611150519.38522.qmail@web52601.mail.yahoo.com>
References: <20050611150519.38522.qmail@web52601.mail.yahoo.com>
Message-ID: <42AB0425.9090407@pdf.com>

	  Have you tried RSiteSearch("skewness") and RSiteSearch("kurtosis")? 
The "fBasics" package has functions for that, as do other packages.

	  spencer graves

Mohammad Ehsanul Karim wrote:

> Dear All,
> 
> Is there any existing package or direct function in R
> / S-plus that calculates moments (raw or central); the
> usual measures of skewness and kurtosis, and/or
> pearsonian k criterion?
> 
> Thank you for your time. 
> 
> 
> ----------------------------------
> 
> Mohammad Ehsanul Karim 
> 
> Web: http://snipurl.com/ehsan 
> ISRT, University of Dhaka, BD
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Jun 11 17:39:47 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 11 Jun 2005 17:39:47 +0200
Subject: [R] Calculating moments of a distribution
In-Reply-To: <20050611150519.38522.qmail@web52601.mail.yahoo.com>
References: <20050611150519.38522.qmail@web52601.mail.yahoo.com>
Message-ID: <42AB05C3.7030903@statistik.uni-dortmund.de>

Mohammad Ehsanul Karim wrote:

> Dear All,
> 
> Is there any existing package or direct function in R
> / S-plus that calculates moments (raw or central); the
> usual measures of skewness and kurtosis, and/or
> pearsonian k criterion?


See package e1071.

Uwe Ligges


> Thank you for your time. 
> 
> 
> ----------------------------------
> 
> Mohammad Ehsanul Karim 
> 
> Web: http://snipurl.com/ehsan 
> ISRT, University of Dhaka, BD
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sgilpin at gmail.com  Sat Jun 11 20:13:20 2005
From: sgilpin at gmail.com (Scott Gilpin)
Date: Sat, 11 Jun 2005 12:13:20 -0600
Subject: [R] Performance difference between 32-bit build and 64-bit bu
	ild on Solaris 8
In-Reply-To: <Pine.LNX.4.61.0506110817420.29456@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>
	<Pine.LNX.4.61.0506110817420.29456@gannet.stats>
Message-ID: <5739cc2f0506111113765fc7ee@mail.gmail.com>

Andy, Prof. Ripley - thanks for your replies.

> CFLAGS was not specified, so should default to "-g -O2".  It is definitely
> worth checking, although almost all the time in these tests will be spent
> in Fortran code.
Yes - I verified that's the default.

> >> neither build uses a BLAS.
> 
> Well, they do, the one supplied with R (which is in Fortran).

I guess I should have said that neither build is using an optimized
BLAS.  (which I am planning to install - I just haven't had the chance
yet.)   I also get a message in config.log "ld: fatal: library -lblas:
not found".  I need to investigate this more.


> and for gcc 3.4.3 and the internal BLAS (which I had to build for these
> tests) I get
> 
> 32-bit
> [1]  9.96  0.09 10.12  0.00  0.00
> 64-bit
> [1]  9.93  0.04 10.04  0.00  0.00
> 
> so I am not seeing anything like the same performance difference between
> 32- and 64-bit builds (but it could well depend on the particular Sparc
> chip).

These timings are much, much less than what I reported (~700s and
2200s for 32 bit and 64 bit).  I read the admin manual and didn't see
anything specifically that needs to be set to use the internal BLAS.  
I guess I'll go back and do some more investigation.

Thanks for your help.



From joshuacgilbert at gmail.com  Sat Jun 11 20:19:37 2005
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Sat, 11 Jun 2005 14:19:37 -0400
Subject: [R] Error with function lda in package MASS (dimnames not
	equal?)
In-Reply-To: <Pine.LNX.4.61.0506101509330.5321@gannet.stats>
References: <ef96daa30506100638416dfe47@mail.gmail.com>
	<Pine.LNX.4.61.0506101509330.5321@gannet.stats>
Message-ID: <ef96daa30506111119135eecc1@mail.gmail.com>

This is true, they are equal. I hadn't noticed that. Thank you.

Now, if lda fails on this given input (equal means), shouldn't we
catch it and give a slightly better error message? I've spent a good
while going through the debugging process with lda.default. From that
perspective it appears that there is a simple change to remove the
problem. I am not saying that it is correct in any shape or form, but
there is a point where a single transpose would silence the error.

So, from a usability standpoint, could we add a check for equal means
between classes and throw an error for that specific condition? Yes,
the user should not do that. But users may become more interested in
making the code run than checking on whether it's doing anything sane.

If this isn't the place to do so, tell me. But, I'd like to petition
to alter the code of lda.default.

On 6/10/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> lda.default <- MASS:::lda.default and proceed.
> 
> Look at the group means of your data: they are identical to machine
> accuracy.
> 
> The question has to be `why are you trying to use lda to separate
> two groups with identical means'?  Lda is not protected against that
> and it is rather unlikely unless you failed to inspect your data in any
> way.
> 
> On Fri, 10 Jun 2005, Joshua Gilbert wrote:
> 
> > This question appears to have been asked previously, but not answered.
> > the last response I can find to this previous thread is here:
> > http://tolstoy.newcastle.edu.au/R/help/04/07/0126.html. The asnwer was
> > to provide debugging info, not an answer.
> >
> > So the problem is that I'm trying to use lda on my dataset. You can
> > download my data here:
> > http://northstar-www.dartmouth.edu/~jgilbert/nolda, I used R's save
> > function to save objects data and classes (yes, I realize that I name
> > stomped the data function in package utils). To replicate my results,
> > simply enter the following:
> >> library(MASS)
> >> load('nolda')
> >> lda(data,classes)
> > Error in lda.default(x, grouping, ...) : length of 'dimnames' [2] not
> > equal to array extent
> >
> > Now, I don't know what that means.
> >> dimnames(data)
> > NULL
> >> dimnames(classes)
> > NULL
> >
> > As for debugging, I don't know how. I cannot debug lda.default as I
> > get the following:
> >> debug(lda.default)
> > Error: Object "lda.default" not found
> >
> > I think that that's pretty much it. Can anyone help me?
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From subianto at gmail.com  Sat Jun 11 20:44:07 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Sat, 11 Jun 2005 20:44:07 +0200
Subject: [R] combination which limited
Message-ID: <3635ddc205061111447dee5aa5@mail.gmail.com>

Dear R-helpers,
I am learning about combination in R.
I want to combination all of
possible variable but it limited. 
I am sorry I could not explain exactly.
For usefull I give an example
  interface <- c("usb","fireware","infra","bluetooth")
  screen    <- c("lcd","cube")
  computer  <- c("pc","server","laptop")
  available <- c("yes","no")
  
What the result I need, something like this below,
  usb          lcd     pc      yes
  fireware     lcd     pc      yes
  infra        lcd     pc      yes
  bluetooth    lcd     pc      yes
  usb          cube    pc      yes 
  usb          lcd     server  yes
  usb          lcd     laptop  yes
  usb          lcd     pc      no
  
How can I do that?
I was wondering if someone can help me.
Thanks you for your time and best regards,
Muhammad Subianto



From mrfearless47 at yahoo.com  Sat Jun 11 20:51:24 2005
From: mrfearless47 at yahoo.com (Marc Feldesman)
Date: Sat, 11 Jun 2005 11:51:24 -0700 (PDT)
Subject: [R] Error with function lda in package MASS (dimnames not
	equal?)
In-Reply-To: <ef96daa30506111119135eecc1@mail.gmail.com>
Message-ID: <20050611185124.31282.qmail@web32605.mail.mud.yahoo.com>



--- Joshua Gilbert <joshuacgilbert at gmail.com> wrote:

> If this isn't the place to do so, tell me. But, I'd like to petition
> to alter the code of lda.default.

It seems to me that if you want to alter the code of lda.default, you
have everything you need to do so.  The code is there, it is GPL'd and
you can change it to your heart's content.  But if you are suggesting
that the lda function be changed for everyone, I'd be personally
opposed to doing so.  It behaves the way it should behave and if you're
looking for a way to save yourself from the way it works, then make the
change on a local copy of lda and you'll have solved the problem for
you.  For me, I vote for the status quo.


Dr. Marc R Feldesman
Professor & Chair Emeritus
Department of Anthropology
Portland State University
Portland, OR 97207

Please respond to all emails at:  feldesmanm at pdx.edu

"I've proven who am so many times, the magnetic strip is wearing thin"



From MSchwartz at mn.rr.com  Sat Jun 11 20:56:48 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 11 Jun 2005 13:56:48 -0500
Subject: [R] combination which limited
In-Reply-To: <3635ddc205061111447dee5aa5@mail.gmail.com>
References: <3635ddc205061111447dee5aa5@mail.gmail.com>
Message-ID: <1118516208.647.4.camel@localhost.localdomain>

On Sat, 2005-06-11 at 20:44 +0200, Muhammad Subianto wrote:
> Dear R-helpers,
> I am learning about combination in R.
> I want to combination all of
> possible variable but it limited. 
> I am sorry I could not explain exactly.
> For usefull I give an example
>   interface <- c("usb","fireware","infra","bluetooth")
>   screen    <- c("lcd","cube")
>   computer  <- c("pc","server","laptop")
>   available <- c("yes","no")
>   
> What the result I need, something like this below,
>   usb          lcd     pc      yes
>   fireware     lcd     pc      yes
>   infra        lcd     pc      yes
>   bluetooth    lcd     pc      yes
>   usb          cube    pc      yes 
>   usb          lcd     server  yes
>   usb          lcd     laptop  yes
>   usb          lcd     pc      no
>   
> How can I do that?
> I was wondering if someone can help me.
> Thanks you for your time and best regards,
> Muhammad Subianto

Use:

> expand.grid(interface, screen, computer, available)
        Var1 Var2   Var3 Var4
1        usb  lcd     pc  yes
2   fireware  lcd     pc  yes
3      infra  lcd     pc  yes
4  bluetooth  lcd     pc  yes
5        usb cube     pc  yes
6   fireware cube     pc  yes
7      infra cube     pc  yes
8  bluetooth cube     pc  yes
9        usb  lcd server  yes
10  fireware  lcd server  yes
11     infra  lcd server  yes
12 bluetooth  lcd server  yes
13       usb cube server  yes
14  fireware cube server  yes
15     infra cube server  yes
16 bluetooth cube server  yes
17       usb  lcd laptop  yes
18  fireware  lcd laptop  yes
19     infra  lcd laptop  yes
20 bluetooth  lcd laptop  yes
21       usb cube laptop  yes
22  fireware cube laptop  yes
23     infra cube laptop  yes
24 bluetooth cube laptop  yes
25       usb  lcd     pc   no
26  fireware  lcd     pc   no
27     infra  lcd     pc   no
28 bluetooth  lcd     pc   no
29       usb cube     pc   no
30  fireware cube     pc   no
31     infra cube     pc   no
32 bluetooth cube     pc   no
33       usb  lcd server   no
34  fireware  lcd server   no
35     infra  lcd server   no
36 bluetooth  lcd server   no
37       usb cube server   no
38  fireware cube server   no
39     infra cube server   no
40 bluetooth cube server   no
41       usb  lcd laptop   no
42  fireware  lcd laptop   no
43     infra  lcd laptop   no
44 bluetooth  lcd laptop   no
45       usb cube laptop   no
46  fireware cube laptop   no
47     infra cube laptop   no
48 bluetooth cube laptop   no


See ?expand.grid for more information.

Using:

> help.search("combinations")

would guide you to that function based upon a keyword search.

HTH,

Marc Schwartz



From marc.girondot at ese.u-psud.fr  Sat Jun 11 21:15:18 2005
From: marc.girondot at ese.u-psud.fr (Marc Girondot)
Date: Sat, 11 Jun 2005 21:15:18 +0200
Subject: [R] Problem with multinom ?
In-Reply-To: <Pine.LNX.4.61.0506111510081.10254@gannet.stats>
References: <20050611115554.FYIY26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
	<Pine.LNX.4.61.0506111510081.10254@gannet.stats>
Message-ID: <p06210201bed0e0f95916@[62.147.109.30]>

>On Sat, 11 Jun 2005, John Fox wrote:
>
>>Dear Marc,
>>
>>I get the same results -- same coefficients, standard errors, and fitted
>>probabilities -- from multinom() and glm(). It's true that the deviances
>>differ, but they, I believe, are defined only up to an additive constant:
>
>Yes. There are many variations on the definition 
>of (residual) deviance, but it compares -2 log 
>likelihood with a `saturated' model.  For 
>grouped data you have a choice: a separate term 
>for each group or for each observation.  A 
>binomial GLM uses the first but the second is 
>more
>normal in logistic regression (since it has a 
>direct interpretation via log-probability 
>scoring).
>
>multinom() is support software for a book (which 
>the R posting guide does ask you to consult): 
>this is discussed with a worked example on pp 
>203-4.

Dear Prof. Ripley,

I have your book... but I don't find the answer to my questions...

You propose that the difference in residual 
deviance between two versions of the same model 
(0.001841823 for glm and  106.2304 for 
multinom()) is due to a difference in the 
specification of the satured model. However, as 
RD=-2 Ln L model+2 Ln L saturated and that -2 Ln 
L model=11.1146... it seems impossible to me that 
RD > -2 Ln L model ...

Marc Girondot

Sorry to be so close-minded !
-- 

__________________________________________________________
Marc Girondot, Pr
Laboratoire Ecologie, Syst??matique et Evolution
Equipe de Conservation des Populations et des Communaut??s
CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
B??timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
15 56 96   e-mail: marc.girondot at ese.u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot
Fax in US: 1-425-732-6934



From marc.girondot at ese.u-psud.fr  Sat Jun 11 21:15:50 2005
From: marc.girondot at ese.u-psud.fr (Marc Girondot)
Date: Sat, 11 Jun 2005 21:15:50 +0200
Subject: [R] Problem with multinom ?
In-Reply-To: <20050611115554.FYIY26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20050611115554.FYIY26128.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <p06210203bed0e8541247@[62.147.109.30]>

>Dear Marc,
>
>I get the same results -- same coefficients, standard errors, and fitted
>probabilities -- from multinom() and glm(). It's true that the deviances
>differ, but they, I believe, are defined only up to an additive constant:
>
>>  predict(dt.b, type="response")
>         1         2         3
>0.4524946 0.5032227 0.5538845
>
>>  predict(dt.m, type="probs")
>         1         2         3         4         5         6
>0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846
>
>These fitted probabilities *are* correct: Since the members of each pair
>(1,2), (3,4), and (5,6) have identical values of factor they are identical
>fitted probabilities.

I expected rather to obtain (note 1- before some terms):
>     1         2         3         4         5         6
0.4524948 1-0.4524948 0.5032229 1-0.5032229 0.5538846 1-0.5538846

...

Marc
-- 

__________________________________________________________
Marc Girondot, Pr
Laboratoire Ecologie, Syst??matique et Evolution
Equipe de Conservation des Populations et des Communaut??s
CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
B??timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
15 56 96   e-mail: marc.girondot at ese.u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot
Fax in US: 1-425-732-6934



From ggrothendieck at gmail.com  Sat Jun 11 21:31:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 11 Jun 2005 15:31:54 -0400
Subject: [R] combination which limited
In-Reply-To: <1118516208.647.4.camel@localhost.localdomain>
References: <3635ddc205061111447dee5aa5@mail.gmail.com>
	<1118516208.647.4.camel@localhost.localdomain>
Message-ID: <971536df0506111231183371a7@mail.gmail.com>

On 6/11/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On Sat, 2005-06-11 at 20:44 +0200, Muhammad Subianto wrote:
> > Dear R-helpers,
> > I am learning about combination in R.
> > I want to combination all of
> > possible variable but it limited.
> > I am sorry I could not explain exactly.
> > For usefull I give an example
> >   interface <- c("usb","fireware","infra","bluetooth")
> >   screen    <- c("lcd","cube")
> >   computer  <- c("pc","server","laptop")
> >   available <- c("yes","no")
> >
> > What the result I need, something like this below,
> >   usb          lcd     pc      yes
> >   fireware     lcd     pc      yes
> >   infra        lcd     pc      yes
> >   bluetooth    lcd     pc      yes
> >   usb          cube    pc      yes
> >   usb          lcd     server  yes
> >   usb          lcd     laptop  yes
> >   usb          lcd     pc      no
> >
> > How can I do that?
> > I was wondering if someone can help me.
> > Thanks you for your time and best regards,
> > Muhammad Subianto
> 
> Use:
> 
> > expand.grid(interface, screen, computer, available)
>        Var1 Var2   Var3 Var4
> 1        usb  lcd     pc  yes
> 2   fireware  lcd     pc  yes
> 3      infra  lcd     pc  yes
> 4  bluetooth  lcd     pc  yes
> 5        usb cube     pc  yes
> 6   fireware cube     pc  yes
> 7      infra cube     pc  yes
> 8  bluetooth cube     pc  yes
> 9        usb  lcd server  yes
> 10  fireware  lcd server  yes
> 11     infra  lcd server  yes
> 12 bluetooth  lcd server  yes
> 13       usb cube server  yes
> 14  fireware cube server  yes
> 15     infra cube server  yes
> 16 bluetooth cube server  yes
> 17       usb  lcd laptop  yes
> 18  fireware  lcd laptop  yes
> 19     infra  lcd laptop  yes
> 20 bluetooth  lcd laptop  yes
> 21       usb cube laptop  yes
> 22  fireware cube laptop  yes
> 23     infra cube laptop  yes
> 24 bluetooth cube laptop  yes
> 25       usb  lcd     pc   no
> 26  fireware  lcd     pc   no
> 27     infra  lcd     pc   no
> 28 bluetooth  lcd     pc   no
> 29       usb cube     pc   no
> 30  fireware cube     pc   no
> 31     infra cube     pc   no
> 32 bluetooth cube     pc   no
> 33       usb  lcd server   no
> 34  fireware  lcd server   no
> 35     infra  lcd server   no
> 36 bluetooth  lcd server   no
> 37       usb cube server   no
> 38  fireware cube server   no
> 39     infra cube server   no
> 40 bluetooth cube server   no
> 41       usb  lcd laptop   no
> 42  fireware  lcd laptop   no
> 43     infra  lcd laptop   no
> 44 bluetooth  lcd laptop   no
> 45       usb cube laptop   no
> 46  fireware cube laptop   no
> 47     infra cube laptop   no
> 48 bluetooth cube laptop   no
> 
> 
> See ?expand.grid for more information.
> 


After you do the above you will still want to cut it down to just
the rows you need.

As expained, use expand.grid.  Let's assume you used this statement:

dd <- expand.grid(interface = interface, screen = screen, 
   computer = computer, available = available)

There are several possibilities now:

1. you could list out dd on the console and note the number of the
rows you want to keep:

idx <- c(1,5,7)
dd2 <- dd[,idx]

or if you want most of them it may be easier to record which ones
you do not want:

ndix <- c(2,4,7)
dd2 <- dd[,-ndix]

2. Another possibility is to export it to a spreadsheet and visually 
delete the rows you don't want.  

3. A third possibility is to install JGR (which is a free Java GUI
front end to R).
First download and install JGR from:    http://stats.math.uni-augsburg.de/JGR/
In JGR (I am using Windows and its possible that the instructions vary
slightly on other platforms):

1. create dd as explained 
2. bring up the object browser using the menu Tools | Object Browser
    or just ctrl-B
3. Select dd from the object browser
4. This will put you into a spreadsheet in which you can select the
rows you want
     to delete (hold down ctrl for the 2nd and subsequent selection to have a 
    non-contiguous multi-row selection).
5. Select Tools | Remove Rows
6. Click on Apply in the lower right of the spreadsheet.


7. Click on X on the upper right of the spreadsheet.

the menu entry Tools | Remove Rows.



From p.dalgaard at biostat.ku.dk  Sat Jun 11 21:36:02 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jun 2005 21:36:02 +0200
Subject: [R] Performance difference between 32-bit build and 64-bit bu
	ild on Solaris 8
In-Reply-To: <5739cc2f0506111113765fc7ee@mail.gmail.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>
	<Pine.LNX.4.61.0506110817420.29456@gannet.stats>
	<5739cc2f0506111113765fc7ee@mail.gmail.com>
Message-ID: <x2oeacznx9.fsf@turmalin.kubism.ku.dk>

Scott Gilpin <sgilpin at gmail.com> writes:

> Andy, Prof. Ripley - thanks for your replies.
> 
> > CFLAGS was not specified, so should default to "-g -O2".  It is definitely
> > worth checking, although almost all the time in these tests will be spent
> > in Fortran code.
> Yes - I verified that's the default.
> 
> > >> neither build uses a BLAS.
> > 
> > Well, they do, the one supplied with R (which is in Fortran).
> 
> I guess I should have said that neither build is using an optimized
> BLAS.  (which I am planning to install - I just haven't had the chance
> yet.)   I also get a message in config.log "ld: fatal: library -lblas:
> not found".  I need to investigate this more.

Actually, you did say so... 

Don't worry about error messages like that in config.log; they only
mean that there is no system BLAS and the only way to find out is by
trial and failure. The configure script is full of that sort of code.
 
> 
> > and for gcc 3.4.3 and the internal BLAS (which I had to build for these
> > tests) I get
> > 
> > 32-bit
> > [1]  9.96  0.09 10.12  0.00  0.00
> > 64-bit
> > [1]  9.93  0.04 10.04  0.00  0.00
> > 
> > so I am not seeing anything like the same performance difference between
> > 32- and 64-bit builds (but it could well depend on the particular Sparc
> > chip).
> 
> These timings are much, much less than what I reported (~700s and
> 2200s for 32 bit and 64 bit).  I read the admin manual and didn't see
> anything specifically that needs to be set to use the internal BLAS.  
> I guess I'll go back and do some more investigation.

Yes. If you have hardcore linear algebra needs, those fast-BLAS
speedups can be impressive (Brian might also have a faster machine
than you, mind you). For code that is mainly interpreter-bound, it is
much less so.

While your setup is in place, you might want to play around with the
higher optimization levels. GCC on AMD64 sees a quite substantial
speedup from -O2 to -O3.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From jfox at mcmaster.ca  Sat Jun 11 23:17:52 2005
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 11 Jun 2005 17:17:52 -0400
Subject: [R] Problem with multinom ?
In-Reply-To: <p06210203bed0e8541247@[62.147.109.30]>
Message-ID: <20050611211752.YYCK16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Marc,


> -----Original Message-----
> From: Marc Girondot [mailto:marc.girondot at ese.u-psud.fr] 
> Sent: Saturday, June 11, 2005 2:16 PM
> To: John Fox
> Cc: R-help at r-project.org
> Subject: RE: [R] Problem with multinom ?
> 
> >Dear Marc,
> >
> >I get the same results -- same coefficients, standard errors, and 
> >fitted probabilities -- from multinom() and glm(). It's true 
> that the 
> >deviances differ, but they, I believe, are defined only up 
> to an additive constant:
> >
> >>  predict(dt.b, type="response")
> >         1         2         3
> >0.4524946 0.5032227 0.5538845
> >
> >>  predict(dt.m, type="probs")
> >         1         2         3         4         5         6
> >0.4524948 0.4524948 0.5032229 0.5032229 0.5538846 0.5538846
> >
> >These fitted probabilities *are* correct: Since the members of each 
> >pair (1,2), (3,4), and (5,6) have identical values of factor 
> they are 
> >identical fitted probabilities.
> 
> I expected rather to obtain (note 1- before some terms):
> >     1         2         3         4         5         6
> 0.4524948 1-0.4524948 0.5032229 1-0.5032229 0.5538846 1-0.5538846
> 
> ...

But the fitted probabilities are for each observation in the data set; in
your data, these have identical values of factor for each pair and hence
identical fitted probabilities. When the response is dichotomous, you get
only one of the two fitted probabilities for each observation; for a
polytomous response, you get a matrix of fitted probabilities which sum to 1
row-wise.

Regards,
 John

> 
> Marc
> -- 
> 
> __________________________________________________________
> Marc Girondot, Pr
> Laboratoire Ecologie, Syst??matique et Evolution Equipe de 
> Conservation des Populations et des Communaut??s CNRS, ENGREF 
> et Universit?? Paris-Sud 11 , UMR 8079 B??timent 362
> 91405 Orsay Cedex, France
> 
> Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
> 15 56 96   e-mail: marc.girondot at ese.u-psud.fr
> Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
> Skype: girondot
> Fax in US: 1-425-732-6934



From ripley at stats.ox.ac.uk  Sat Jun 11 23:57:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Jun 2005 22:57:01 +0100 (BST)
Subject: [R] Error with function lda in package MASS (dimnames not
	equal?)
In-Reply-To: <ef96daa30506111119135eecc1@mail.gmail.com>
References: <ef96daa30506100638416dfe47@mail.gmail.com> 
	<Pine.LNX.4.61.0506101509330.5321@gannet.stats>
	<ef96daa30506111119135eecc1@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506112241340.15118@gannet.stats>

The code will be changed to give a more informative error message in a 
future release.  However, do remember that this is volunteer code, and it 
is not reasonable to expect volunteers to anticipate that a user will 
apply it in extremely unlikely circumstances.

If you have a suggestion about code in a contributed package, please send 
it to the package maintainer (as the posting guide asks).

On Sat, 11 Jun 2005, Joshua Gilbert wrote:

> This is true, they are equal. I hadn't noticed that. Thank you.
>
> Now, if lda fails on this given input (equal means), shouldn't we
> catch it and give a slightly better error message? I've spent a good
> while going through the debugging process with lda.default. From that
> perspective it appears that there is a simple change to remove the
> problem. I am not saying that it is correct in any shape or form, but
> there is a point where a single transpose would silence the error.
>
> So, from a usability standpoint, could we add a check for equal means
> between classes and throw an error for that specific condition? Yes,
> the user should not do that. But users may become more interested in
> making the code run than checking on whether it's doing anything sane.
>
> If this isn't the place to do so, tell me. But, I'd like to petition
> to alter the code of lda.default.
>
> On 6/10/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> lda.default <- MASS:::lda.default and proceed.
>>
>> Look at the group means of your data: they are identical to machine
>> accuracy.
>>
>> The question has to be `why are you trying to use lda to separate
>> two groups with identical means'?  Lda is not protected against that
>> and it is rather unlikely unless you failed to inspect your data in any
>> way.
>>
>> On Fri, 10 Jun 2005, Joshua Gilbert wrote:
>>
>>> This question appears to have been asked previously, but not answered.
>>> the last response I can find to this previous thread is here:
>>> http://tolstoy.newcastle.edu.au/R/help/04/07/0126.html. The asnwer was
>>> to provide debugging info, not an answer.
>>>
>>> So the problem is that I'm trying to use lda on my dataset. You can
>>> download my data here:
>>> http://northstar-www.dartmouth.edu/~jgilbert/nolda, I used R's save
>>> function to save objects data and classes (yes, I realize that I name
>>> stomped the data function in package utils). To replicate my results,
>>> simply enter the following:
>>>> library(MASS)
>>>> load('nolda')
>>>> lda(data,classes)
>>> Error in lda.default(x, grouping, ...) : length of 'dimnames' [2] not
>>> equal to array extent
>>>
>>> Now, I don't know what that means.
>>>> dimnames(data)
>>> NULL
>>>> dimnames(classes)
>>> NULL
>>>
>>> As for debugging, I don't know how. I cannot debug lda.default as I
>>> get the following:
>>>> debug(lda.default)
>>> Error: Object "lda.default" not found
>>>
>>> I think that that's pretty much it. Can anyone help me?
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From weihong at ualberta.ca  Sat Jun 11 23:59:10 2005
From: weihong at ualberta.ca (weihong)
Date: Sat, 11 Jun 2005 15:59:10 -0600
Subject: [R] predict function for GLMM
Message-ID: <42AC64D3@webmail.ualberta.ca>

I use "predict" for predictions from glm. I am wondering if there is a 
"predict" function for predictions from the results of GLMM model?

Thanks ahead!

Weihong Li
Undergraduate Student in Statistics
University of Alberta



From ripley at stats.ox.ac.uk  Sun Jun 12 00:07:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Jun 2005 23:07:12 +0100 (BST)
Subject: [R] Performance difference between 32-bit build and 64-bit bu
 ild on Solaris 8
In-Reply-To: <x2oeacznx9.fsf@turmalin.kubism.ku.dk>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>
	<Pine.LNX.4.61.0506110817420.29456@gannet.stats>
	<5739cc2f0506111113765fc7ee@mail.gmail.com>
	<x2oeacznx9.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0506112302510.15118@gannet.stats>

On Sat, 11 Jun 2005, Peter Dalgaard wrote:

> Scott Gilpin <sgilpin at gmail.com> writes:
>
>> Andy, Prof. Ripley - thanks for your replies.
>>
>>> CFLAGS was not specified, so should default to "-g -O2".  It is definitely
>>> worth checking, although almost all the time in these tests will be spent
>>> in Fortran code.
>> Yes - I verified that's the default.
>>
>>>>> neither build uses a BLAS.
>>>
>>> Well, they do, the one supplied with R (which is in Fortran).
>>
>> I guess I should have said that neither build is using an optimized
>> BLAS.  (which I am planning to install - I just haven't had the chance
>> yet.)   I also get a message in config.log "ld: fatal: library -lblas:
>> not found".  I need to investigate this more.
>
> Actually, you did say so...
>
> Don't worry about error messages like that in config.log; they only
> mean that there is no system BLAS and the only way to find out is by
> trial and failure. The configure script is full of that sort of code.
>
>>
>>> and for gcc 3.4.3 and the internal BLAS (which I had to build for these
>>> tests) I get
>>>
>>> 32-bit
>>> [1]  9.96  0.09 10.12  0.00  0.00
>>> 64-bit
>>> [1]  9.93  0.04 10.04  0.00  0.00
>>>
>>> so I am not seeing anything like the same performance difference between
>>> 32- and 64-bit builds (but it could well depend on the particular Sparc
>>> chip).
>>
>> These timings are much, much less than what I reported (~700s and
>> 2200s for 32 bit and 64 bit).  I read the admin manual and didn't see
>> anything specifically that needs to be set to use the internal BLAS.
>> I guess I'll go back and do some more investigation.

For the record, I timed 1000x1000 not 3000x3000 (and said so).  I was not 
proposing to spend several hours running timings at ca 2200s each, not 
least as I used a public machine with a ban on running long jobs (we have 
other much faster machines for that purpose).

> Yes. If you have hardcore linear algebra needs, those fast-BLAS
> speedups can be impressive (Brian might also have a faster machine
> than you, mind you). For code that is mainly interpreter-bound, it is
> much less so.
>
> While your setup is in place, you might want to play around with the
> higher optimization levels. GCC on AMD64 sees a quite substantial
> speedup from -O2 to -O3.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sgilpin at gmail.com  Sun Jun 12 01:32:37 2005
From: sgilpin at gmail.com (Scott Gilpin)
Date: Sat, 11 Jun 2005 17:32:37 -0600
Subject: [R] Performance difference between 32-bit build and 64-bit bu
	ild on Solaris 8
In-Reply-To: <Pine.LNX.4.61.0506112302510.15118@gannet.stats>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E988@usctmx1106.merck.com>
	<Pine.LNX.4.61.0506110817420.29456@gannet.stats>
	<5739cc2f0506111113765fc7ee@mail.gmail.com>
	<x2oeacznx9.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0506112302510.15118@gannet.stats>
Message-ID: <5739cc2f050611163232bdb0a4@mail.gmail.com>

On 6/11/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> For the record, I timed 1000x1000 not 3000x3000 (and said so).  I was not
> proposing to spend several hours running timings at ca 2200s each, not
> least as I used a public machine with a ban on running long jobs (we have
> other much faster machines for that purpose).

My mistake. 

For the 1000x1000 problem on my system, I got the following:

32-bit
[1] 23.37  0.03 23.41  0.00  0.00
64-bit
[1] 80.99  0.02 81.03  0.00  0.00

So it looks like I've still got some work to do.  Thanks for the suggestions.



From joshuacgilbert at gmail.com  Sun Jun 12 01:56:51 2005
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Sat, 11 Jun 2005 19:56:51 -0400
Subject: [R] Error with function lda in package MASS (dimnames not
	equal?)
In-Reply-To: <Pine.LNX.4.61.0506112241340.15118@gannet.stats>
References: <ef96daa30506100638416dfe47@mail.gmail.com>
	<Pine.LNX.4.61.0506101509330.5321@gannet.stats>
	<ef96daa30506111119135eecc1@mail.gmail.com>
	<Pine.LNX.4.61.0506112241340.15118@gannet.stats>
Message-ID: <ef96daa305061116565caf9c32@mail.gmail.com>

I fully understand that this is a volunteer project, I'm a Debian user
(not a developer... yet).

I have read the posting guide, but I forgot the protocol. First
offence, won't happen again.

Thanks.

On 6/11/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> The code will be changed to give a more informative error message in a
> future release.  However, do remember that this is volunteer code, and it
> is not reasonable to expect volunteers to anticipate that a user will
> apply it in extremely unlikely circumstances.
> 
> If you have a suggestion about code in a contributed package, please send
> it to the package maintainer (as the posting guide asks).
> 
> On Sat, 11 Jun 2005, Joshua Gilbert wrote:
> 
> > This is true, they are equal. I hadn't noticed that. Thank you.
> >
> > Now, if lda fails on this given input (equal means), shouldn't we
> > catch it and give a slightly better error message? I've spent a good
> > while going through the debugging process with lda.default. From that
> > perspective it appears that there is a simple change to remove the
> > problem. I am not saying that it is correct in any shape or form, but
> > there is a point where a single transpose would silence the error.
> >
> > So, from a usability standpoint, could we add a check for equal means
> > between classes and throw an error for that specific condition? Yes,
> > the user should not do that. But users may become more interested in
> > making the code run than checking on whether it's doing anything sane.
> >
> > If this isn't the place to do so, tell me. But, I'd like to petition
> > to alter the code of lda.default.
> >
> > On 6/10/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> >> lda.default <- MASS:::lda.default and proceed.
> >>
> >> Look at the group means of your data: they are identical to machine
> >> accuracy.
> >>
> >> The question has to be `why are you trying to use lda to separate
> >> two groups with identical means'?  Lda is not protected against that
> >> and it is rather unlikely unless you failed to inspect your data in any
> >> way.
> >>
> >> On Fri, 10 Jun 2005, Joshua Gilbert wrote:
> >>
> >>> This question appears to have been asked previously, but not answered.
> >>> the last response I can find to this previous thread is here:
> >>> http://tolstoy.newcastle.edu.au/R/help/04/07/0126.html. The asnwer was
> >>> to provide debugging info, not an answer.
> >>>
> >>> So the problem is that I'm trying to use lda on my dataset. You can
> >>> download my data here:
> >>> http://northstar-www.dartmouth.edu/~jgilbert/nolda, I used R's save
> >>> function to save objects data and classes (yes, I realize that I name
> >>> stomped the data function in package utils). To replicate my results,
> >>> simply enter the following:
> >>>> library(MASS)
> >>>> load('nolda')
> >>>> lda(data,classes)
> >>> Error in lda.default(x, grouping, ...) : length of 'dimnames' [2] not
> >>> equal to array extent
> >>>
> >>> Now, I don't know what that means.
> >>>> dimnames(data)
> >>> NULL
> >>>> dimnames(classes)
> >>> NULL
> >>>
> >>> As for debugging, I don't know how. I cannot debug lda.default as I
> >>> get the following:
> >>>> debug(lda.default)
> >>> Error: Object "lda.default" not found
> >>>
> >>> I think that that's pretty much it. Can anyone help me?
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>>
> >>
> >> --
> >> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >> University of Oxford,             Tel:  +44 1865 272861 (self)
> >> 1 South Parks Road,                     +44 1865 272866 (PA)
> >> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> >
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From andy_liaw at merck.com  Sun Jun 12 04:56:06 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 11 Jun 2005 22:56:06 -0400
Subject: [R] Performance difference between 32-bit build and 64-bit bu
 ild on Solaris 8
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E989@usctmx1106.merck.com>

> From: Peter Dalgaard
[snip]
> While your setup is in place, you might want to play around with the
> higher optimization levels. GCC on AMD64 sees a quite substantial
> speedup from -O2 to -O3.

On our SLES8 amd64 boxes, I had trouble with g77 -O3 (build failed).  Have
not tried with newer GCC.

Andy
 
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From kjetil at acelerate.com  Sun Jun 12 04:59:52 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 11 Jun 2005 22:59:52 -0400
Subject: [R] glm with variance = mu+theta*mu^2?
In-Reply-To: <429F4D9D.9000308@pdf.com>
References: <429F4D9D.9000308@pdf.com>
Message-ID: <42ABA528.5080700@acelerate.com>

Spencer Graves wrote:

>       How might you fit a generalized linear model (glm) with variance 
> = mu+theta*mu^2 (where mu = mean of the exponential family random 
> variable and theta is a parameter to be estimated)?
>
>       This appears in Table 2.7 of Fahrmeir and Tutz (2001) 
> Multivariate Statisticial Modeling Based on Generalized Linear Models, 
> 2nd ed. (Springer, p. 60), where they compare "log-linear model fits 
> to cellular differentiation data based on quasi-likelihoods" between 
> variance = phi*mu (quasi-Poisson), variance = phi*mu^2 
> (quasi-exponential), and variance = mu+theta*mu^2.  The "quasi" 
> function accepted for the family argument in "glm" generates functions 
> "variance", "validmu", and "dev.resids".  I can probably write 
> functions  to mimic the "quasi" function.  However, I have two 
> questions in regard to this:
>
>       (1) I don't know what to use for "dev.resids".  This may not 
> matter for fitting.  I can try a couple of different things to see if 
> it matters.
>
>       (2) Might someone else suggest something different, e.g., using 
> something like optim to solve an appropriate quasi-score function?
>
>       Thanks,
>       spencer graves
>
Since nobody has answerd this I will try. The variance function 
mu+theta*mu^2 is the variance function
of the negative binomial family. If this variance function is used to 
construct a quasi-likelihood, the resulting quasi-
likelihood is identical to the negative binomial likelihood, so for 
fitting we can simly use glm.nb from MASS, which
will give the correct estimated values. However, in a quasi-likelihood 
setting the (co)varince estimation from
glm.nb is not appropriate, and from the book (fahrmeir ..) it seems that 
the estimation method used is a
sandwich estimator, so we can try the sandwich package.  This works but 
the numerical results are somewhat different from  the book. Any 
comments on this?

my code follows:

 > library(Fahrmeir)
 > library(help=Fahrmeir)
 > library(MASS)
 >  cells.negbin <- glm(y~TNF+IFN+TNF:IFN, data=cells,
                 family=negative.binomial(1/0.215))
 > summary(cells.negbin)

Call:
glm(formula = y ~ TNF + IFN + TNF:IFN, family = negative.binomial(1/0.215),
    data = cells)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.6714  -0.8301  -0.2153   0.4802   1.4282 

Coefficients:
               Estimate  Std. Error t value Pr(>|t|)   
(Intercept)  3.39874495  0.18791125  18.087  4.5e-10 ***
TNF          0.01616136  0.00360569   4.482  0.00075 ***
IFN          0.00935690  0.00359010   2.606  0.02296 * 
TNF:IFN     -0.00005910  0.00007002  -0.844  0.41515   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for Negative Binomial(4.6512) family taken to be 
1.012271)

    Null deviance: 46.156  on 15  degrees of freedom
Residual deviance: 12.661  on 12  degrees of freedom
AIC: 155.49

Number of Fisher Scoring iterations: 5

 > confint(cells.negbin)
Waiting for profiling to be done...
                    2.5 %       97.5 %
(Intercept)  3.0383197319 3.7890206510
TNF          0.0091335087 0.0238915483
IFN          0.0023292566 0.0170195707
TNF:IFN     -0.0001996824 0.0000960427
 > library(sandwich)
Loading required package: zoo
 > vcovHC( cells.negbin )
               (Intercept)              TNF              IFN           
TNF:IFN
(Intercept)  0.01176249372 -0.0001279740135 -0.0001488223001  
0.00000212541999
TNF         -0.00012797401  0.0000039017282  0.0000021242875 
-0.00000019793137
IFN         -0.00014882230  0.0000021242875  0.0000054314079 
-0.00000013277626
TNF:IFN      0.00000212542 -0.0000001979314 -0.0000001327763  
0.00000002370104
 > cov2cor(vcovHC( cells.negbin ))
            (Intercept)        TNF        IFN    TNF:IFN
(Intercept)   1.0000000 -0.5973702 -0.5887923  0.1272950
TNF          -0.5973702  1.0000000  0.4614542 -0.6508822
IFN          -0.5887923  0.4614542  1.0000000 -0.3700671
TNF:IFN       0.1272950 -0.6508822 -0.3700671  1.0000000
 > cells.negbin2 <- glm.nb( y~TNF+IFN+TNF:IFN, data=cells)
 > summary(cells.negbin)

Call:
glm(formula = y ~ TNF + IFN + TNF:IFN, family = negative.binomial(1/0.215),
    data = cells)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.6714  -0.8301  -0.2153   0.4802   1.4282 

Coefficients:
               Estimate  Std. Error t value Pr(>|t|)   
(Intercept)  3.39874495  0.18791125  18.087  4.5e-10 ***
TNF          0.01616136  0.00360569   4.482  0.00075 ***
IFN          0.00935690  0.00359010   2.606  0.02296 * 
TNF:IFN     -0.00005910  0.00007002  -0.844  0.41515   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for Negative Binomial(4.6512) family taken to be 
1.012271)

    Null deviance: 46.156  on 15  degrees of freedom
Residual deviance: 12.661  on 12  degrees of freedom
AIC: 155.49

Number of Fisher Scoring iterations: 5

 > confint( cells.negbin2 )
Waiting for profiling to be done...
                    2.5 %        97.5 %
(Intercept)  3.0864669072 3.73444363850
TNF          0.0100954189 0.02265622337
IFN          0.0032778815 0.01582969419
TNF:IFN     -0.0001788579 0.00007142582
 > library(lmtest)
 >  coeftest( cells.negbin2, vcov=vcovHC(cells.negbin2, type="HC1"), df=Inf)

z test of coefficients of "negbin" object 'cells.negbin2':

                Estimate   Std. Error z value      Pr(>|z|)   
(Intercept)  3.400428706  0.094904107 35.8302     < 2.2e-16 ***
TNF          0.016130321  0.001213671 13.2905     < 2.2e-16 ***
IFN          0.009333249  0.001632518  5.7171 0.00000001084 ***
TNF:IFN     -0.000058798  0.000019269 -3.0514      0.002278 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

 >  coeftest( cells.negbin2, vcov=vcov(cells.negbin2), df=Inf)

z test of coefficients of "negbin" object 'cells.negbin2':

                Estimate   Std. Error z value    Pr(>|z|)   
(Intercept)  3.400428706  0.188480395 18.0413   < 2.2e-16 ***
TNF          0.016130321  0.003573031  4.5145 0.000006348 ***
IFN          0.009333249  0.003571163  2.6135    0.008962 **
TNF:IFN     -0.000058798  0.000069162 -0.8501    0.395244   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Note different conclusions from the two last commands with respect to
necessity of interaction term in model.

Comments are welcome!

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From subianto at gmail.com  Sun Jun 12 10:48:19 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Sun, 12 Jun 2005 10:48:19 +0200
Subject: [R] combination which limited
In-Reply-To: <971536df0506111231183371a7@mail.gmail.com>
References: <3635ddc205061111447dee5aa5@mail.gmail.com>
	<1118516208.647.4.camel@localhost.localdomain>
	<971536df0506111231183371a7@mail.gmail.com>
Message-ID: <3635ddc20506120148216e7881@mail.gmail.com>

Dear All,
Many thanks to Marc Schwartz and Gabor Grothendieck who have explained
me about using expand.grid function and clearly explain how to use
JGR.

> dd <- expand.grid(interface = interface, screen = screen,
>    computer = computer, available = available)
> 
> There are several possibilities now:
> 
> 1. you could list out dd on the console and note the number of the
> rows you want to keep:
> 
> idx <- c(1,5,7)
> dd2 <- dd[,idx]
> 

I like a possible no. 1, because I can use and explore with my hand,
>  idx <- c(1:5,9,17,25)
>  dd2 <- dd[idx,]
>  dd2
   interface screen computer available
1        usb    lcd       pc       yes
2   fireware    lcd       pc       yes
3      infra    lcd       pc       yes
4  bluetooth    lcd       pc       yes
5        usb   cube       pc       yes
9        usb    lcd   server       yes
17       usb    lcd   laptop       yes
25       usb    lcd       pc        no
> 

Regards,
Muhammad Subianto
Notepad, Copy and Paste are my best friend to use R.2.1.0 on windows 2000

On 6/11/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/11/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> > On Sat, 2005-06-11 at 20:44 +0200, Muhammad Subianto wrote:
> > > Dear R-helpers,
> > > I am learning about combination in R.
> > > I want to combination all of
> > > possible variable but it limited.
> > > I am sorry I could not explain exactly.
> > > For usefull I give an example
> > >   interface <- c("usb","fireware","infra","bluetooth")
> > >   screen    <- c("lcd","cube")
> > >   computer  <- c("pc","server","laptop")
> > >   available <- c("yes","no")
> > >
> > > What the result I need, something like this below,
> > >   usb          lcd     pc      yes
> > >   fireware     lcd     pc      yes
> > >   infra        lcd     pc      yes
> > >   bluetooth    lcd     pc      yes
> > >   usb          cube    pc      yes
> > >   usb          lcd     server  yes
> > >   usb          lcd     laptop  yes
> > >   usb          lcd     pc      no
> > >
> > > How can I do that?
> > > I was wondering if someone can help me.
> > > Thanks you for your time and best regards,
> > > Muhammad Subianto
> >
> > Use:
> >
> > > expand.grid(interface, screen, computer, available)
> >        Var1 Var2   Var3 Var4
> > 1        usb  lcd     pc  yes
> > 2   fireware  lcd     pc  yes
> > 3      infra  lcd     pc  yes
> > 4  bluetooth  lcd     pc  yes
> > 5        usb cube     pc  yes
> > 6   fireware cube     pc  yes
> > 7      infra cube     pc  yes
> > 8  bluetooth cube     pc  yes
> > 9        usb  lcd server  yes
> > 10  fireware  lcd server  yes
> > 11     infra  lcd server  yes
> > 12 bluetooth  lcd server  yes
> > 13       usb cube server  yes
> > 14  fireware cube server  yes
> > 15     infra cube server  yes
> > 16 bluetooth cube server  yes
> > 17       usb  lcd laptop  yes
> > 18  fireware  lcd laptop  yes
> > 19     infra  lcd laptop  yes
> > 20 bluetooth  lcd laptop  yes
> > 21       usb cube laptop  yes
> > 22  fireware cube laptop  yes
> > 23     infra cube laptop  yes
> > 24 bluetooth cube laptop  yes
> > 25       usb  lcd     pc   no
> > 26  fireware  lcd     pc   no
> > 27     infra  lcd     pc   no
> > 28 bluetooth  lcd     pc   no
> > 29       usb cube     pc   no
> > 30  fireware cube     pc   no
> > 31     infra cube     pc   no
> > 32 bluetooth cube     pc   no
> > 33       usb  lcd server   no
> > 34  fireware  lcd server   no
> > 35     infra  lcd server   no
> > 36 bluetooth  lcd server   no
> > 37       usb cube server   no
> > 38  fireware cube server   no
> > 39     infra cube server   no
> > 40 bluetooth cube server   no
> > 41       usb  lcd laptop   no
> > 42  fireware  lcd laptop   no
> > 43     infra  lcd laptop   no
> > 44 bluetooth  lcd laptop   no
> > 45       usb cube laptop   no
> > 46  fireware cube laptop   no
> > 47     infra cube laptop   no
> > 48 bluetooth cube laptop   no
> >
> >
> > See ?expand.grid for more information.
> >
> 
> 
> After you do the above you will still want to cut it down to just
> the rows you need.
> 
> As expained, use expand.grid.  Let's assume you used this statement:
> 
> dd <- expand.grid(interface = interface, screen = screen,
>    computer = computer, available = available)
> 
> There are several possibilities now:
> 
> 1. you could list out dd on the console and note the number of the
> rows you want to keep:
> 
> idx <- c(1,5,7)
> dd2 <- dd[,idx]
> 
> or if you want most of them it may be easier to record which ones
> you do not want:
> 
> ndix <- c(2,4,7)
> dd2 <- dd[,-ndix]
> 
> 2. Another possibility is to export it to a spreadsheet and visually
> delete the rows you don't want.
> 
> 3. A third possibility is to install JGR (which is a free Java GUI
> front end to R).
> First download and install JGR from:    http://stats.math.uni-augsburg.de/JGR/
> In JGR (I am using Windows and its possible that the instructions vary
> slightly on other platforms):
> 
> 1. create dd as explained
> 2. bring up the object browser using the menu Tools | Object Browser
>     or just ctrl-B
> 3. Select dd from the object browser
> 4. This will put you into a spreadsheet in which you can select the
> rows you want
>      to delete (hold down ctrl for the 2nd and subsequent selection to have a
>     non-contiguous multi-row selection).
> 5. Select Tools | Remove Rows
> 6. Click on Apply in the lower right of the spreadsheet.
> 
> 
> 7. Click on X on the upper right of the spreadsheet.
> 
> the menu entry Tools | Remove Rows.
>



From merser at image.dk  Sun Jun 12 12:13:01 2005
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sun, 12 Jun 2005 12:13:01 +0200
Subject: [R] y-axis and resizing window
Message-ID: <000b01c56f37$550a4cf0$6400a8c0@IBM>

hi
using plot(..., las=1), i.e. horizontal axis labels, the labels on the 
y-axis jams if the heigth of the graphics windov becomes too low
while both x-axis and  y-axis kind of removes superflus lables with las=0 
(default)
is there a way to make plot behave alike with horizontal lables?
regards s??ren



From ripley at stats.ox.ac.uk  Sun Jun 12 12:39:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 12 Jun 2005 11:39:25 +0100 (BST)
Subject: [R] y-axis and resizing window
In-Reply-To: <000b01c56f37$550a4cf0$6400a8c0@IBM>
References: <000b01c56f37$550a4cf0$6400a8c0@IBM>
Message-ID: <Pine.LNX.4.61.0506121129150.9638@gannet.stats>

On Sun, 12 Jun 2005, S?ren Merser wrote:

> using plot(..., las=1), i.e. horizontal axis labels, the labels on the
> y-axis jams if the heigth of the graphics windov becomes too low
> while both x-axis and  y-axis kind of removes superflus lables with las=0
> (default)
> is there a way to make plot behave alike with horizontal lables?

It I understand you correctly (what does `jams' mean?), this is nothing to 
do with resizing. The axis labelling code checks for enough width-wise 
space for labels, but not for enough height-wise space. Specifically, 
do_axis for the y axis contains

 			    /* Check room for perpendicular labels. */
 			    if (Rf_gpptr(dd)->las == 1 ||
 				Rf_gpptr(dd)->las == 2 ||
 				tnew - tlast >= gap) {

so y-axis labels are always plotted for las %in% c(1,2) and hence may 
overlap.  (Similar code exists for an x-axis.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From pensterfuzzer at yahoo.de  Sun Jun 12 13:09:57 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 12 Jun 2005 13:09:57 +0200 (CEST)
Subject: [R] delete "-character from strings in matrix
Message-ID: <20050612110957.51435.qmail@web25806.mail.ukl.yahoo.com>

Hi!

I have strings where occasionally some "-chars occur.
How can I delete these chars?

I tried it with gsub but using "" as replace does not
work.

Thanks a lot for any hint!
Regards,
   Werner



From samuel_mwalili at yahoo.com  Sun Jun 12 13:10:15 2005
From: samuel_mwalili at yahoo.com (Mwalili, S. M.)
Date: Sun, 12 Jun 2005 04:10:15 -0700 (PDT)
Subject: [R]  glm with variance = mu+theta*mu^2?
In-Reply-To: <42ABA528.5080700@acelerate.com>
Message-ID: <20050612111015.66481.qmail@web53403.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050612/b3b2be94/attachment.pl

From merser at image.dk  Sun Jun 12 13:13:51 2005
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sun, 12 Jun 2005 13:13:51 +0200
Subject: [R] y-axis and resizing window
References: <000b01c56f37$550a4cf0$6400a8c0@IBM>
	<Pine.LNX.4.61.0506121129150.9638@gannet.stats>
Message-ID: <001c01c56f3f$d2f48470$6400a8c0@IBM>

thanks

with 'jams' i meant messes up, but your term overlap is exactly what i
actually had in mind

though a minor problem, do you think that the code will change to enable
checking for enough height-wise space?

regards s??ren

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "S??ren Merser" <merser at image.dk>
Cc: "R - help" <r-help at stat.math.ethz.ch>
Sent: Sunday, June 12, 2005 12:39 PM
Subject: Re: [R] y-axis and resizing window


On Sun, 12 Jun 2005, S??ren Merser wrote:

> using plot(..., las=1), i.e. horizontal axis labels, the labels on the
> y-axis jams if the heigth of the graphics windov becomes too low
> while both x-axis and  y-axis kind of removes superflus lables with las=0
> (default)
> is there a way to make plot behave alike with horizontal lables?

It I understand you correctly (what does `jams' mean?), this is nothing to
do with resizing. The axis labelling code checks for enough width-wise
space for labels, but not for enough height-wise space. Specifically,
do_axis for the y axis contains

      /* Check room for perpendicular labels. */
      if (Rf_gpptr(dd)->las == 1 ||
  Rf_gpptr(dd)->las == 2 ||
  tnew - tlast >= gap) {

so y-axis labels are always plotted for las %in% c(1,2) and hence may
overlap.  (Similar code exists for an x-axis.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sun Jun 12 13:42:17 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 12 Jun 2005 07:42:17 -0400
Subject: [R] delete "-character from strings in matrix
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E98B@usctmx1106.merck.com>

Please define "does not work".  Here's what I get:

> m <- matrix(paste(letters[1:4], "does not work."), 2, 2)
> m
     [,1]               [,2]              
[1,] "a does not work." "c does not work."
[2,] "b does not work." "d does not work."
> gsub("does not work.", "", m)
[1] "a " "b " "c " "d "
> structure(gsub("does not work.", "", m), dim=dim(m))
     [,1] [,2]
[1,] "a " "c "
[2,] "b " "d "

R-2.1.0 on WinXPPro.

Andy 

> From: Werner Wernersen
> 
> Hi!
> 
> I have strings where occasionally some "-chars occur.
> How can I delete these chars?
> 
> I tried it with gsub but using "" as replace does not
> work.
> 
> Thanks a lot for any hint!
> Regards,
>    Werner
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From pensterfuzzer at yahoo.de  Sun Jun 12 14:10:43 2005
From: pensterfuzzer at yahoo.de (Werner Wernersen)
Date: Sun, 12 Jun 2005 14:10:43 +0200 (CEST)
Subject: [R] delete "-character from strings in matrix
Message-ID: <20050612121043.32507.qmail@web25801.mail.ukl.yahoo.com>

Thanks for the reply, Andy!

My problem was that I could not get rid of a double
quote character within the 
string. I don't know what I have done before, but now
it works...?!?!
Sorry for bothering you.

Best,
   Werner


Liaw, Andy wrote:
> Please define "does not work".  Here's what I get:
> 
> 
>>m <- matrix(paste(letters[1:4], "does not work."),
2, 2)
>>m
> 
>      [,1]               [,2]              
> [1,] "a does not work." "c does not work."
> [2,] "b does not work." "d does not work."
> 
>>gsub("does not work.", "", m)
> 
> [1] "a " "b " "c " "d "
> 
>>structure(gsub("does not work.", "", m), dim=dim(m))
> 
>      [,1] [,2]
> [1,] "a " "c "
> [2,] "b " "d "
> 
> R-2.1.0 on WinXPPro.
> 
> Andy 
> 
> 
>>From: Werner Wernersen
>>
>>Hi!
>>
>>I have strings where occasionally some "-chars
occur.
>>How can I delete these chars?
>>
>>I tried it with gsub but using "" as replace does
not
>>work.
>>
>>Thanks a lot for any hint!
>>Regards,
>>   Werner
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>
> 
> 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
attachments, contains information of Merck & Co., Inc.
(One Merck Drive, Whitehouse Station, New Jersey, USA
08889), and/or its affiliates (which may be known
outside the United States as Merck Frosst, Merck Sharp
& Dohme or MSD and in Japan, as Banyu) that may be
confidential, proprietary copyrighted and/or legally
privileged. It is intended solely for the use of the
individual or entity named on this message.  If you
are not the intended recipient, and have received this
message in error, please notify us immediately by
reply e-mail and then delete it from your system.
>
------------------------------------------------------------------------------
> 
>



From antonio.dinarzo at studio.unibo.it  Sun Jun 12 14:17:27 2005
From: antonio.dinarzo at studio.unibo.it (antonio.dinarzo@studio.unibo.it)
Date: Sun, 12 Jun 2005 14:17:27 +0200
Subject: [R] memory allocation problem under linux
Message-ID: <1118578647.42ac27d78f309@posta.studio.unibo.it>

I have some compiled code that works under winXp but not under linux (kernel
2.6.10-5). I'm also using R 2.1.0
After debugging, I've discovered that this code:
  #define NMAX 256
  long **box;
  ...
  box   = (long **)R_alloc(NMAX,   sizeof(long *));

gives a null pointer, so subsequent line:
  for (i=0; i<NMAX; i++) box[i] = (long *) R_alloc(NMAX, sizeof(long));
gives a SIGSEGV signal.
In the same shared library, I have a function with this code:
  partitions=16;
  ...
  h2=(long **)R_alloc(partitions,sizeof(long *));
  for (i=0;i<partitions;i++) 
      h2[i]=(long *)R_alloc(partitions,sizeof(long));
that works! Naturally, I've tried to change NMAX from 256 to 16, without any
success.

Any idea on where the problem can reside? (Note that this not happens under WinXp).
And just another question. When R_alloc fails, should-it terminate the function
with an error, without returning control to the function?



From slist at oomvanlieshout.net  Sun Jun 12 14:37:12 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Sun, 12 Jun 2005 14:37:12 +0200
Subject: [R] Replacing for loop with tapply!?
In-Reply-To: <1118450203.5850.14.camel@dhcp-63.ccc.ox.ac.uk>
References: <42A9543E.3040901@oomvanlieshout.net>	<012901c56d9c$c6bfc6f0$0540210a@www.domain>	<42A96709.3010406@oomvanlieshout.net>	<007001c56ddf$4f80c1a0$0540210a@www.domain>	<42A9D685.5050806@oomvanlieshout.net>
	<1118450203.5850.14.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <42AC2C78.1020000@oomvanlieshout.net>

Dear Adaikalavan,

Your solution (the second function) is definitely the most elegant and 
generic solution of all replies in this discussion. Robust for missing 
values and flexible to allow as many calculations as desired! It is so 
clear, I even managed to hack it (of course also thanks to the new 
insight from all the other posts)!

As the data consists of weather stations in rows and days in columns, I 
have adapted the function to work on rows instead of columns. Did not 
manage to get the results directly into the right rows/cols layout, so a 
transpose (t) is still required. However this seems instant, so does not 
mean a reduction in speed! Calculating proportions is now a snip!!

Thanks for you help,

Sander.

### simulate data
set.seed(1)            # for reproducibility
mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)
mat[ mat > 45 ] <- NA  # create some missing values
mat[ 9, ]       <- NA  # station 9's data is completely missing
mat

find.stats <- function( data, threshold ){

   n      <- length(threshold)
   excess <- numeric( n )
   out    <- matrix( ncol=nrow(data), nrow=(n + 2) ) # initialise
   good   <- which( apply( data, 1, function(x) !all(is.na(x)) ) )
   # rows that are not completely missing

   out[ ,good ] <- apply( data[ good, ], 1, function(x){
     m <- max( x, na.rm=T )
     # determine maximum value per row
     c <- length(x[!is.na(x)])
     # determine number of non-missing values
     for(i in 1:n){ excess[i] <- sum( x > threshold[i], na.rm=TRUE 
)/length(x[!is.na(x)]) }
     # calc proportion of non-missing values over multiple thresholds
     return( c(m, c, excess) )
   } )

   rownames(out) <- c( "TmpMax", "Count", paste("Over", threshold, sep="") )
   colnames(out) <- rownames(data)  # name of the stations
   return( t(out) )
}

lstTemps=c(37,39,41,43)
tmp <- find.stats( mat, lstTemps )
tmp




Adaikalavan Ramasamy wrote:
> OK, so you want to find some summary statistics for each column, where
> some columns could be completely missing. 
> 
> Writing a small wrapper should help. When you use apply(), you are
> actually applying a function to every column (or row). First, let us
> simulate a dataset with 15 days/rows and 10 stations/columns 
> 
> ### simulate data
> set.seed(1)            # for reproducibility 
> mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)  
> mat[ mat > 45 ] <- NA  # create some missing values
> mat[ ,9 ]       <- NA  # station 9's data is completely missing
> 
> 
> Here are two example of such wrappers :
> 
> find.stats1 <- function( data, threshold=c(37,39,41) ){
>   
>   n   <- length(threshold)
>   out <- matrix(  nrow=(n + 1), ncol=ncol(data) ) # initialise
> 
>   out[1, ] <- apply(data, 2, function(x) 
>                          ifelse( all(is.na(x)), NA, max(x, na.rm=T) ))
> 
>   for(i in 1:n) out[ i+1, ] <- colSums( data > threshold[i], na.rm=T )
>   
>   rownames(out) <- c( "daily_max", paste("above", threshold, sep="_") )
>   colnames(out) <- rownames(data)  # name of the stations
>   return( out )
> }
>   
> find.stats2 <- function( data, threshold=c(37,39,41) ){
>   
>   n      <- length(threshold)
>   excess <- numeric( n )
>   out    <- matrix(  nrow=(n + 1), ncol=ncol(data) ) # initialise
>   good   <- which( apply( data, 2, function(x) !all(is.na(x)) ) )
>   # colums that are not completely missing
>  
>   out[ , good] <- apply( data[ , good], 2, function(x){
>     m <- max( x, na.rm=T )
>     for(i in 1:n){ excess[i] <- sum( x > threshold[i], na.rm=TRUE ) }
>     return( c(m, excess) )
>   } ) 
>   
>   rownames(out) <- c( "daily_max", paste("above", threshold, sep="_") )
>   colnames(out) <- rownames(data)  # name of the stations
>   return( out )
> }
> 
> find.stats1( mat )
>           [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> daily_max   44   42   39   41   45   43   42   45   NA    42
> above_37     2    1    2    1    3    2    2    1    0     1
> above_39     2    1    0    1    3    2    1    1    0     1
> above_41     2    1    0    0    2    2    1    1    0     1
> 
> find.stats2( mat )
>           [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> daily_max   44   42   39   41   45   43   42   45   NA    42
> above_37     2    1    2    1    3    2    2    1   NA     1
> above_39     2    1    0    1    3    2    1    1   NA     1
> above_41     2    1    0    0    2    2    1    1   NA     1
> 
> 
> On my laptop 'find.stats1' and 'find.stats2' (which is more flexible)
> takes 7 and 6 seconds respectively to execute on a dataset with 10000
> stations and 365 days.
> 
> Regards, Adai
> 
> 
> 
> On Fri, 2005-06-10 at 20:05 +0200, Sander Oom wrote:
>>Dear all,
>>
>>Dimitris and Andy, thanks for your great help. I have progressed to the 
>>following code which runs very fast and effective:
>>
>>mat <- matrix(sample(-15:50, 15 * 10, TRUE), 15, 10)
>>mat[mat>45] <- NA
>>mat<-NA
>>mat
>>temps <- c(35, 37, 39)
>>ind <- rbind(
>>     t(sapply(temps, function(temp)
>>       rowSums(mat > temp, na.rm=TRUE) )),
>>     rowSums(!is.na(mat), na.rm=FALSE),
>>     apply(mat, 1, max, na.rm=TRUE))
>>ind <- t(ind)
>>ind
>>
>>However, some weather stations have missing values for the whole year. 
>>Unfortunately, the code breaks down (when uncommenting mat<-NA).
>>
>>I have tried 'ifelse' statements in the functions, but it becomes even 
>>more of a mess. I could subset the matrix before hand, but this would 
>>mean merging with a complete matrix afterwards to make it compatible 
>>with other years. That would slow things down.
>>
>>How can I make the code robust for rows containing all missing values?
>>
>>Thanks for your help,
>>
>>Sander.
>>
>>Dimitris Rizopoulos wrote:
>>>for the maximum you could use something like:
>>>
>>>ind[, 1] <- apply(mat, 2, max)
>>>
>>>I hope it helps.
>>>
>>>Best,
>>>Dimitris
>>>
>>>----
>>>Dimitris Rizopoulos
>>>Ph.D. Student
>>>Biostatistical Centre
>>>School of Public Health
>>>Catholic University of Leuven
>>>
>>>Address: Kapucijnenvoer 35, Leuven, Belgium
>>>Tel: +32/16/336899
>>>Fax: +32/16/337015
>>>Web: http://www.med.kuleuven.ac.be/biostat/
>>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>>
>>>
>>>
>>>----- Original Message ----- 
>>>From: "Sander Oom" <slist at oomvanlieshout.net>
>>>To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
>>>Cc: <r-help at stat.math.ethz.ch>
>>>Sent: Friday, June 10, 2005 12:10 PM
>>>Subject: Re: [R] Replacing for loop with tapply!?
>>>
>>>
>>>>Thanks Dimitris,
>>>>
>>>>Very impressive! Much faster than before.
>>>>
>>>>Thanks to new found R.basic, I can simply rotate the result with
>>>>rotate270{R.basic}:
>>>>
>>>>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
>>>>>temps <- c(37, 39, 41)
>>>>>#################
>>>>>#ind <- matrix(0, length(temps), ncol(mat))
>>>>>ind <- matrix(0, 4, ncol(mat))
>>>>>(startDate <- date())
>>>>[1] "Fri Jun 10 12:08:01 2005"
>>>>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
>>>>>ind[4, ] <- colMeans(max(mat))
>>>>Error in colMeans(max(mat)) : 'x' must be an array of at least two
>>>>dimensions
>>>>>(endDate <- date())
>>>>[1] "Fri Jun 10 12:08:02 2005"
>>>>>ind <- rotate270(ind)
>>>>>ind[1:10,]
>>>>  V4 V3 V2 V1
>>>>1   0 56 75 80
>>>>2   0 46 53 60
>>>>3   0 50 58 67
>>>>4   0 60 72 80
>>>>5   0 59 68 76
>>>>6   0 55 67 74
>>>>7   0 62 77 93
>>>>8   0 45 57 67
>>>>9   0 57 68 75
>>>>10  0 61 66 76
>>>>
>>>>However, I have not managed to get the row maximum using your 
>>>>method? It
>>>>should be 50 for most rows, but my first guess code gives an error!
>>>>
>>>>Any suggestions?
>>>>
>>>>Sander
>>>>
>>>>
>>>>
>>>>Dimitris Rizopoulos wrote:
>>>>>maybe you are looking for something along these lines:
>>>>>
>>>>>mat <- matrix(sample(-15:50, 365 * 15000, TRUE), 365, 15000)
>>>>>temps <- c(37, 39, 41)
>>>>>#################
>>>>>ind <- matrix(0, length(temps), ncol(mat))
>>>>>for(i in seq(along = temps)) ind[i, ] <- colSums(mat > temps[i])
>>>>>ind
>>>>>
>>>>>
>>>>>I hope it helps.
>>>>>
>>>>>Best,
>>>>>Dimitris
>>>>>
>>>>>----
>>>>>Dimitris Rizopoulos
>>>>>Ph.D. Student
>>>>>Biostatistical Centre
>>>>>School of Public Health
>>>>>Catholic University of Leuven
>>>>>
>>>>>Address: Kapucijnenvoer 35, Leuven, Belgium
>>>>>Tel: +32/16/336899
>>>>>Fax: +32/16/337015
>>>>>Web: http://www.med.kuleuven.ac.be/biostat/
>>>>>    http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>>>>
>>>>>
>>>>>----- Original Message ----- 
>>>>>From: "Sander Oom" <slist at oomvanlieshout.net>
>>>>>To: <r-help at stat.math.ethz.ch>
>>>>>Sent: Friday, June 10, 2005 10:50 AM
>>>>>Subject: [R] Replacing for loop with tapply!?
>>>>>
>>>>>
>>>>>>Dear all,
>>>>>>
>>>>>>We have a large data set with temperature data for weather stations
>>>>>>across the globe (15000 stations).
>>>>>>
>>>>>>For each station, we need to calculate the number of days a certain
>>>>>>temperature is exceeded.
>>>>>>
>>>>>>So far we used the following S code, where mat88 is a matrix
>>>>>>containing
>>>>>>rows of 365 daily temperatures for each of 15000 weather stations:
>>>>>>
>>>>>>m <- 37
>>>>>>n <- 2
>>>>>>outmat88 <- matrix(0, ncol = 4, nrow = nrow(mat88))
>>>>>>for(i in 1:nrow(mat88)) {
>>>>>># i <- 3
>>>>>>row1 <- as.data.frame(df88[i,  ])
>>>>>>temprow37 <- select.rows(row1, row1 > m)
>>>>>>temprow39 <- select.rows(row1, row1 > m + n)
>>>>>>temprow41 <- select.rows(row1, row1 > m + 2 * n)
>>>>>>outmat88[i, 1] <- max(row1, na.rm = T)
>>>>>>outmat88[i, 2] <- count.rows(temprow37)
>>>>>>outmat88[i, 3] <- count.rows(temprow39)
>>>>>>outmat88[i, 4] <- count.rows(temprow41)
>>>>>>}
>>>>>>outmat88
>>>>>>
>>>>>>We have transferred the data to a more potent Linux box running R,
>>>>>>but
>>>>>>still hope to speed up the code.
>>>>>>
>>>>>>I know a for loop should be avoided when looking for speed. I also
>>>>>>know
>>>>>>the answer is in something like tapply, but my understanding of
>>>>>>these
>>>>>>commands is still to limited to see the solution. Could someone 
>>>>>>show
>>>>>>me
>>>>>>the way!?
>>>>>>
>>>>>>Thanks in advance,
>>>>>>
>>>>>>Sander.



From stefan.sobernig at wu-wien.ac.at  Sun Jun 12 16:06:24 2005
From: stefan.sobernig at wu-wien.ac.at (Stefan Sobernig)
Date: Sun, 12 Jun 2005 16:06:24 +0200
Subject: [R] linking R to goto blas
Message-ID: <42AC4160.90505@wu-wien.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050612/2f36aea4/attachment.pl

From ripley at stats.ox.ac.uk  Sun Jun 12 17:13:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 12 Jun 2005 16:13:55 +0100 (BST)
Subject: [R] linking R to goto blas
In-Reply-To: <42AC4160.90505@wu-wien.ac.at>
References: <42AC4160.90505@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0506121545050.12510@gannet.stats>

On Sun, 12 Jun 2005, Stefan Sobernig wrote:

> I am currently trying to link R 2.1.0 to the GOTO BLAS 0.99.3 library on
> a box running Fedora Core 3 , basically following the steps indicated in
> the R-Admin document:
>
> 1: I downloaded the current libgoto.xxx.so from
> http://www.cs.utexas.edu/users/kgoto/libraries/libgoto_prescott-32-r0.99-3.so.gz,
> a version suitable for our XEON machine (Nocona core), unpacked it to
> /usr/lib and created a symlink libgoto.so pointing to the library.
>
> 2: Then, I got ready to re-configure and re-compile R (2.1.0) using the
> following configure flags: ./configure --prefix=/usr --enable-R-shlib
> --enable-shared --with-tcltk --with-blas="-lgoto -lpthread -lm"
>
> I did read the R-Admin doc and therefore I am aware of the fact that
> passing "-lgoto" is supposed to be sufficient, but as a matter of fact

Only for single-threaded versions.  For others you need 
--with-blas="-lgoto -lpthread".

> configuring with --with-blas="-lgoto" only ends up in a libR.so being
> linked to the standard libblas.so. config.log reports in this settings
> that libgoto.xxx.so is missing links to libpthread etc. Therefore, I
> added the two flags "-lpthread -lm" as indicated at GOTO's website and I
> got a clean configure run. (Am I concluding correctly that I am using a
> threaded version of goto blas?)

Dunno: the organization of the Goto site has changed since that section 
was written.  Looks like only multi-threaded (2 threads) versions are 
currently available.

> 3: Running make, however, freezed when trying to build "grDevices",
> without throwing any warning or error messages:
>
> [...]
> ../../../../library/grDevices/libs/grDevices.so is unchanged
> make[5]: Leaving directory
> `/home/ssoberni/R-2.1.0/src/library/grDevices/src'
> make[4]: Leaving directory
> `/home/ssoberni/R-2.1.0/src/library/grDevices/src'
> [freeze]
>
> 4: I then rummaged the R mailing list archives and stumbled over a
> thread dating from May this year pointing to a similar issue, concerning
> gcc-3.4 and broken lapack libraries provided by FC3 (see
> https://stat.ethz.ch/pipermail/r-devel/2005-May/033117.html).
>
> Following these opinions/ findings, I did the following (though I knew
> that -- in principle -- R is supposed to handle this issue by passing a
> --ffloat-store flag to the fortran compiler, doesn't it?):

It does.  For me this works with the internal BLAS and with Goto's 
blas versions 0.96-2 and 0.99-3, on an Opteron.  (It also works on i686 
with several other BLASes.)

> * I wanted to remove the FC3 native lapack libraries, and to my
> surprise, they were not installed at all (no liblapack.so.xxx in /usr/lib).
> * I set up an older gcc environment, i.e. the last release from the
> 3.3.x family (3.3.6) and tried to recompile R ending up with the same
> hang-up.

They are not used unless you explicitly asked for them.

> As a last step, I tried to exclude R's internal package explicitly by
> setting --wihtout-lapack, which did not hava a visible effect on the
> building process and did not provide a workaround for the hang-up.

Assuming that is a typo for --without-lapack, it does nothing (it is the 
default and excludes an external LAPACK).

> Please, I highly appreciate any thoughts or hints as my colleagues and I
> are eager to get into GOTO's universe.

First get a version with the internal BLAS working.  That will rule out
any issues about LAPACK.  Then change the BLAS: it looks as if this might 
be a problem with the particular Goto BLAS.

Please note: the R-devel list would be a much better choice for such 
issues -- see the posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Jun 12 17:23:00 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jun 2005 17:23:00 +0200
Subject: [R] linking R to goto blas
In-Reply-To: <42AC4160.90505@wu-wien.ac.at>
References: <42AC4160.90505@wu-wien.ac.at>
Message-ID: <x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>

Stefan Sobernig <stefan.sobernig at wu-wien.ac.at> writes:

> Dear all,
> 
> I am currently trying to link R 2.1.0 to the GOTO BLAS 0.99.3 library on
> a box running Fedora Core 3 , basically following the steps indicated in
> the R-Admin document:
> 
> 1: I downloaded the current libgoto.xxx.so from
> http://www.cs.utexas.edu/users/kgoto/libraries/libgoto_prescott-32-r0.99-3.so.gz,
> a version suitable for our XEON machine (Nocona core), unpacked it to
> /usr/lib and created a symlink libgoto.so pointing to the library.
> 
> 2: Then, I got ready to re-configure and re-compile R (2.1.0) using the
> following configure flags: ./configure --prefix=/usr --enable-R-shlib
> --enable-shared --with-tcltk --with-blas="-lgoto -lpthread -lm"
... 
> Please, I highly appreciate any thoughts or hints as my colleagues and I
> are eager to get into GOTO's universe.

Hmm. Looks over-complicated to me. What works for me on AMD64 is to
have a config.site file in my BUILD-GOTO directory, containing

> cat config.site
BLAS_LIBS="-L/home/pd/GOTO -lgoto_opt64p-r0.96 -lpthread"
CFLAGS="-O3 -g"
#CFLAGS="-g"
FFLAGS=$CFLAGS
CXXFLAGS=$CFLAGS

(the .*FLAGS business is optional, of course). With this in place, a
simple ../R/configure followed by make seems to do the trick.

I'll give it a try on my FC3 system, but it's a 500 MHz PIII, so it
takes a while...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From borgulya at gyer2.sote.hu  Sun Jun 12 17:25:21 2005
From: borgulya at gyer2.sote.hu (BORGULYA =?iso-8859-2?q?G=E1bor?=)
Date: Sun, 12 Jun 2005 17:25:21 +0200
Subject: [R] 0 * NA
Message-ID: <200506121725.23135@gaborgulya>

Hi list!

Debuging one of my R programs I found:

 > 0 * NA
 [1] NA

It this a bug, or intentional? I would expect 0 or 0.0 depending on the type 
of the NA.

Gabor



From andy_liaw at merck.com  Sun Jun 12 17:42:35 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 12 Jun 2005 11:42:35 -0400
Subject: [R] 0 * NA
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E98D@usctmx1106.merck.com>

I believe that's intentional.  NA means we don't know what the value is, so
just about any operation with NA will result in NA.  You might think
anything times 0 is 0, but:

> 0*Inf
[1] NaN

and there's no guarantee that the "true" value not observed is not Inf...

Andy

> From: BORGULYA G??bor
> 
> Hi list!
> 
> Debuging one of my R programs I found:
> 
>  > 0 * NA
>  [1] NA
> 
> It this a bug, or intentional? I would expect 0 or 0.0 
> depending on the type 
> of the NA.
> 
> Gabor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From p.dalgaard at biostat.ku.dk  Sun Jun 12 17:51:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jun 2005 17:51:22 +0200
Subject: [R] linking R to goto blas
In-Reply-To: <x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>
References: <42AC4160.90505@wu-wien.ac.at>
	<x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>
Message-ID: <x2vf4jzi85.fsf@turmalin.kubism.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Stefan Sobernig <stefan.sobernig at wu-wien.ac.at> writes:
> 
> > Dear all,
> > 
> > I am currently trying to link R 2.1.0 to the GOTO BLAS 0.99.3 library on
> > a box running Fedora Core 3 , basically following the steps indicated in
> > the R-Admin document:
> > 
> > 1: I downloaded the current libgoto.xxx.so from
> > http://www.cs.utexas.edu/users/kgoto/libraries/libgoto_prescott-32-r0.99-3.so.gz,
> > a version suitable for our XEON machine (Nocona core), unpacked it to
> > /usr/lib and created a symlink libgoto.so pointing to the library.
> > 
> > 2: Then, I got ready to re-configure and re-compile R (2.1.0) using the
> > following configure flags: ./configure --prefix=/usr --enable-R-shlib
> > --enable-shared --with-tcltk --with-blas="-lgoto -lpthread -lm"
> ... 
> > Please, I highly appreciate any thoughts or hints as my colleagues and I
> > are eager to get into GOTO's universe.
> 
> Hmm. Looks over-complicated to me. What works for me on AMD64 is to
> have a config.site file in my BUILD-GOTO directory, containing
> 
> > cat config.site
> BLAS_LIBS="-L/home/pd/GOTO -lgoto_opt64p-r0.96 -lpthread"
> CFLAGS="-O3 -g"
> #CFLAGS="-g"
> FFLAGS=$CFLAGS
> CXXFLAGS=$CFLAGS
> 
> (the .*FLAGS business is optional, of course). With this in place, a
> simple ../R/configure followed by make seems to do the trick.
> 
> I'll give it a try on my FC3 system, but it's a 500 MHz PIII, so it
> takes a while...

Hmm... That gives me the grDevices issue, which boils down to an R
that segfaults immediately upon startup, in

#0  0x05c0aea7 in tilde_expand () from /usr/lib/libreadline.so.4
#1  0x08170254 in R_ExpandFileName_readline (
    s=0x8bb4020
#"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb",
#buff=0x8295300
#"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/grDevices")
    at ../../../R/src/unix/sys-std.c:406
#2  0x0816f5da in R_ExpandFileName (
    s=0x8bb4020
#"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb") at
#../../../R/src/unix/sys-unix.c:129
#3  0x08167352 in R_FileExists (
    path=0x8bb4020
#"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb") at
#stat.h:365
#4  0x08105da3 in do_fileexists (call=0x84fd544, op=0x82c5f78,
#args=0x0,
    rho=0x8c514a4) at ../../../R/src/main/platform.c:857
#5  0x080edc85 in do_internal (call=0x0, op=0x82ba5d4, args=0x3920,
    env=0x8c514a4) at ../../../R/src/main/names.c:1078
#6  0x080c0daa in Rf_eval (e=0x84fd57c, rho=0x8c514a4)
    at ../../../R/src/main/eval.c:382
#7  0x080c3695 in Rf_applyClosure (call=0x8668c28, op=0x84fd5b4,
    arglist=0x8c50564, rho=0x8ad5cc4, suppliedenv=0x82aa5f0)

running --no-readline gives me another crash

(gdb) bt
#0  0x003fb0da in strcmp () from /lib/ld-linux.so.2
#1  0x003f009a in _dl_map_object () from /lib/ld-linux.so.2
#2  0x004fdb58 in dl_open_worker () from /lib/tls/libc.so.6
#3  0x00000000 in ?? ()

...which suggests that something is up with dynamic linking. 

I'll give it another spin...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From antonio.dinarzo at studio.unibo.it  Sun Jun 12 18:46:23 2005
From: antonio.dinarzo at studio.unibo.it (antonio.dinarzo@studio.unibo.it)
Date: Sun, 12 Jun 2005 18:46:23 +0200
Subject: [R] memory allocation problem under linux
Message-ID: <1118594783.42ac66df0831a@posta.studio.unibo.it>

I've written:

>  #define NMAX 256
>  long **box;
>  ...
>  box   = (long **)R_alloc(NMAX,   sizeof(long *));
>gives a null pointer, so subsequent line:
>  for (i=0; i<NMAX; i++) box[i] = (long *) R_alloc(NMAX, sizeof(long));
>gives a SIGSEGV signal.

Sorry, that's not exact: I have a segmentation fault just *inside* R_alloc!
Substituting R_alloc with malloc and Calloc gives the same error.



From dmbates at gmail.com  Fri Jun 10 21:11:31 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 10 Jun 2005 14:11:31 -0500
Subject: [R] [R-pkgs] New versions of Matrix and lme4 packages
Message-ID: <40e66e0b050610121126020323@mail.gmail.com>

I have uploaded version 0.96-1 of both Matrix and lme4 to CRAN.  The
source package should migrate to CRAN over the weekend and binary
packages should be available some time next week.

As for previous releases, the versions of these two packages are
interdependent.  The lme4 package requires Matrix_0.96-1 or later but
we cannot enforce the other dependency.  Please remember that if you
upgrade the Matrix package you should also upgrade the lme4 package.

The method for fitting generalized linear mixed models using the
Laplacian approximation is considerably faster in this version.  Also,
the packages have been reorganized so the interdependence will not be
as strong in the future.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From aliscla at yahoo.com  Sun Jun 12 21:29:48 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Sun, 12 Jun 2005 12:29:48 -0700 (PDT)
Subject: [R] Essay identification
Message-ID: <20050612192948.58298.qmail@web61212.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050612/5d5436d9/attachment.pl

From gunter.berton at gene.com  Sun Jun 12 23:43:58 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Sun, 12 Jun 2005 14:43:58 -0700
Subject: [R] Essay identification
In-Reply-To: <20050612192948.58298.qmail@web61212.mail.yahoo.com>
Message-ID: <200506122144.j5CLi03Q023145@ohm.gene.com>

I assume that you know the usual procedure is to 'score' each essay by a
vector that gives the frequency of occurrence of commonly used (sometimes
adding subject matter specific) words and phrases. This multivariate
response is then fed in as a "training set" into your favorite supervised
learning/classification procedure. R has many of these -- trees, logisic
regression, boosting, Random Forests,svm's,LDA,SOM's (whoops -- that's an
Unsupervised one),  ... . Try
RSiteSearch('Classification',restrict=('functions').

The devil is in the details as to what works best, I believe. With only 78
exemplars in 10 groups, unless there is a lot of separation (disparate
styles that you could probably detect manually) it may be difficult. It also
depends on how large each group is (balance is generally better).

Cheers,
Bert

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Werner Bier
Sent: Sunday, June 12, 2005 12:30 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Essay identification

Hi R-help,
 
I have a database of 10 students who have written an overall of 78 essays. 
The challenge? I would like to identify who wrote the 79th essay.
 
Has anybody used R in this context? 
 
Even if not, would you suggest me which pattern recognition technique I
might possibly apply?
 
Thanks a lot and regards,
Tom 


		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Mon Jun 13 00:05:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 12 Jun 2005 18:05:47 -0400
Subject: [R] Essay identification
In-Reply-To: <20050612192948.58298.qmail@web61212.mail.yahoo.com>
References: <20050612192948.58298.qmail@web61212.mail.yahoo.com>
Message-ID: <971536df05061215056cd3c423@mail.gmail.com>

On 6/12/05, Werner Bier <aliscla at yahoo.com> wrote:
> Hi R-help,
> 
> I have a database of 10 students who have written an overall of 78 essays.
> The challenge? I would like to identify who wrote the 79th essay.
> 
> Has anybody used R in this context?
> 
> Even if not, would you suggest me which pattern recognition technique I might possibly apply?

Check out

http://xxx.uni-augsburg.de/PS_cache/cond-mat/pdf/0108/0108530.pdf

for a simple method.



From ramasamy at cancer.org.uk  Mon Jun 13 01:30:50 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 13 Jun 2005 00:30:50 +0100
Subject: [R] ANOVA vs REML approach to variance component estimation
In-Reply-To: <42A9E592.5050201@optonline.net>
References: <1118424903.7638.50.camel@ipc143019.lif.icnet.uk>
	<42A9E592.5050201@optonline.net>
Message-ID: <1118619050.5832.9.camel@dhcp-63.ccc.ox.ac.uk>

Thank you for confirming this and introducing me to varcomp().

I have another question that I hope you or someone else can help me
with. I was trying to generalise my codes for variable measurement
levels and discovered that lme() was estimating the within group
variance even with a single measure per subject for all subjects !

Here is an example where we have 12 animals but with single measurement.

  y  <- c(2.2, -1.4, -0.5, -0.3, -2.1, 1.5, 
          1.3, -0.3, 0.5, -1.4, -0.2, 1.8) 
  ID <- factor( 1:12 )


Analysis of variance method correctly says that there is no residual
variance and it equals to total variance.

summary(aov(y ~ ID))
            Df  Sum Sq Mean Sq
ID          11 20.9692  1.9063


However the REML method is giving me a within animal variance when there
is no replication at animal level. It seems like I can get components of
variance for factors that are not replicated.

library(ape)
varcomp(lme(y ~ 1, random = ~ 1 | ID))
       ID    Within 
1.6712661 0.2350218 

Am I reading this correct and can someone kindly explain this to me ?

Thank you again.

Regards, Adai



On Fri, 2005-06-10 at 15:10 -0400, Chuck Cleland wrote:
>    They look fine to me.  Also, note varcomp() in the ape package and 
> VarCorr() in the nlme package.  I think in this case the ANOVA estimate 
> of the intercept variance component is negative because the true value 
> is close to zero.
> 
>  > y <- c( 2.2, -1.4, -0.5,  # animal 1
> +        -0.3, -2.1,  1.5,  # animal 2
> +         1.3, -0.3,  0.5,  # animal 3
> +        -1.4, -0.2,  1.8)  # animal 4
> 
>  > ID <- factor( rep(1:4, each=3) )
> 
>  > library(nlme)
>  > library(ape)
> 
>  > summary(aov(y ~ ID))
>              Df  Sum Sq Mean Sq F value Pr(>F)
> ID           3  0.9625  0.3208  0.1283 0.9406
> Residuals    8 20.0067  2.5008
> 
>  > (0.3208 - 2.5008) / 3
> [1] -0.7266667
> 
>  > varcomp(lme(y ~ 1, random = ~ 1 | ID))
>            ID       Within
> 0.0002709644 1.9062505816
> attr(,"class")
> [1] "varcomp"
> 
>  > VarCorr(lme(y ~ 1, random = ~ 1 | ID))
> ID = pdLogChol(1)
>              Variance     StdDev
> (Intercept) 0.0002709644 0.01646100
> Residual    1.9062505816 1.38067034
> 
> Adaikalavan Ramasamy wrote:
> > Can anyone verify my calculations below or explain why they are wrong ?
> > 
> > I have several animals that were measured thrice. The only blocking
> > variable is the animal itself. I am interested in calculating the 
> > between and within object variations in R. An artificial example :
> > 
> > y <- c( 2.2, -1.4, -0.5,  # animal 1
> >        -0.3  -2.1   1.5,  # animal 2
> >         1.3  -0.3   0.5,  # animal 3
> >        -1.4  -0.2   1.8)  # animal 4
> > ID <- factor( rep(1:4, each=3) )
> > 
> > 
> > 1) Using the ANOVA method
> > 
> >   summary(aov( y ~ ID ))
> >               Df Sum Sq Mean Sq F value Pr(>F)
> >   ID           3  0.900   0.300  0.1207 0.9453
> >   Residuals    8 19.880   2.485               
> > 
> >   => within animal  variation  = 2.485
> >   => between animal variation  = (0.300 - 2.485)/3 = -0.7283
> > 
> > I am aware that ANOVA can give negative estimates for variances. Is this
> > such a case or have I coded wrongly ?
> > 
> > 
> > 2) Using the REML approach 
> > 
> >   library(nlme)
> >   lme( y ~ 1, rand = ~ 1 | ID)
> >    ....
> >   Random effects:
> >   Formula: ~1 | ID
> >           (Intercept) Residual
> >   StdDev:  0.01629769 1.374438
> > 
> >   => within animal variation  = 1.374438^2 = 1.88908
> >   => between animal variation = 0.01629769^2 = 0.0002656147
> > 
> > Is this the correct way of coding for this problem ? I do not have
> > access to a copy of Pinheiro & Bates at the moment.
> > 
> > Thank you very much in advance.
> > 
> > Regards, Adai
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
>



From ramasamy at cancer.org.uk  Mon Jun 13 01:38:39 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 13 Jun 2005 00:38:39 +0100
Subject: [R] delete "-character from strings in matrix
In-Reply-To: <20050612121043.32507.qmail@web25801.mail.ukl.yahoo.com>
References: <20050612121043.32507.qmail@web25801.mail.ukl.yahoo.com>
Message-ID: <1118619519.5832.13.camel@dhcp-63.ccc.ox.ac.uk>

You will need to escape special characters. Here is an example :

 my.string <- "Here is a quote \" in a string"
 my.string
  [1] "Here is a quote \" in a string"

 gsub("\"", "", my.string)
  [1] "Here is a quote  in a string"
 
See help(regexp) for more details.

Regards, Adai



On Sun, 2005-06-12 at 14:10 +0200, Werner Wernersen wrote:
> Thanks for the reply, Andy!
> 
> My problem was that I could not get rid of a double
> quote character within the 
> string. I don't know what I have done before, but now
> it works...?!?!
> Sorry for bothering you.
> 
> Best,
>    Werner
> 
> 
> Liaw, Andy wrote:
> > Please define "does not work".  Here's what I get:
> > 
> > 
> >>m <- matrix(paste(letters[1:4], "does not work."),
> 2, 2)
> >>m
> > 
> >      [,1]               [,2]              
> > [1,] "a does not work." "c does not work."
> > [2,] "b does not work." "d does not work."
> > 
> >>gsub("does not work.", "", m)
> > 
> > [1] "a " "b " "c " "d "
> > 
> >>structure(gsub("does not work.", "", m), dim=dim(m))
> > 
> >      [,1] [,2]
> > [1,] "a " "c "
> > [2,] "b " "d "
> > 
> > R-2.1.0 on WinXPPro.
> > 
> > Andy 
> > 
> > 
> >>From: Werner Wernersen
> >>
> >>Hi!
> >>
> >>I have strings where occasionally some "-chars
> occur.
> >>How can I delete these chars?
> >>
> >>I tried it with gsub but using "" as replace does
> not
> >>work.
> >>
> >>Thanks a lot for any hint!
> >>Regards,
> >>   Werner
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>
> > 
> > 
> > 
> > 
> >
> ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co., Inc.
> (One Merck Drive, Whitehouse Station, New Jersey, USA
> 08889), and/or its affiliates (which may be known
> outside the United States as Merck Frosst, Merck Sharp
> & Dohme or MSD and in Japan, as Banyu) that may be
> confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the
> individual or entity named on this message.  If you
> are not the intended recipient, and have received this
> message in error, please notify us immediately by
> reply e-mail and then delete it from your system.
> >
> ------------------------------------------------------------------------------
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmbates at gmail.com  Mon Jun 13 01:54:42 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 12 Jun 2005 18:54:42 -0500
Subject: [R] ANOVA vs REML approach to variance component estimation
In-Reply-To: <1118619050.5832.9.camel@dhcp-63.ccc.ox.ac.uk>
References: <1118424903.7638.50.camel@ipc143019.lif.icnet.uk>
	<42A9E592.5050201@optonline.net>
	<1118619050.5832.9.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <40e66e0b050612165462c37294@mail.gmail.com>

On 6/12/05, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> Thank you for confirming this and introducing me to varcomp().
> 
> I have another question that I hope you or someone else can help me
> with. I was trying to generalise my codes for variable measurement
> levels and discovered that lme() was estimating the within group
> variance even with a single measure per subject for all subjects !
> 
> Here is an example where we have 12 animals but with single measurement.
> 
>   y  <- c(2.2, -1.4, -0.5, -0.3, -2.1, 1.5,
>           1.3, -0.3, 0.5, -1.4, -0.2, 1.8)
>   ID <- factor( 1:12 )
> 
> 
> Analysis of variance method correctly says that there is no residual
> variance and it equals to total variance.
> 
> summary(aov(y ~ ID))
>             Df  Sum Sq Mean Sq
> ID          11 20.9692  1.9063
> 
> 
> However the REML method is giving me a within animal variance when there
> is no replication at animal level. It seems like I can get components of
> variance for factors that are not replicated.
> 
> library(ape)
> varcomp(lme(y ~ 1, random = ~ 1 | ID))
>        ID    Within
> 1.6712661 0.2350218
> 
> Am I reading this correct and can someone kindly explain this to me ?

It's a spurious convergence in lme.  There is no check in lme for the
number of observations exceeding the number of groups.  There should
be.  I'll add this to the bug reports list.



From Ted.Harding at nessie.mcc.ac.uk  Mon Jun 13 01:47:05 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 13 Jun 2005 00:47:05 +0100 (BST)
Subject: [R] Essay identification
In-Reply-To: <200506122144.j5CLi03Q023145@ohm.gene.com>
Message-ID: <XFMail.050613004313.Ted.Harding@nessie.mcc.ac.uk>


On 12-Jun-05 Berton Gunter wrote:
> I assume that you know the usual procedure is to 'score'
> each essay by a vector that gives the frequency of occurrence
> of commonly used (sometimes adding subject matter specific)
> words and phrases. This multivariate response is then fed in
> as a "training set" into your favorite supervised
> learning/classification procedure. R has many of these -- trees,
> logisic regression, boosting, Random Forests,svm's,LDA,SOM's
> (whoops -- that's an Unsupervised one),  ... . Try
> RSiteSearch('Classification',restrict=('functions').
> 
> The devil is in the details as to what works best, I believe.
> With only 78 exemplars in 10 groups, unless there is a lot of
> separation (disparate styles that you could probably detect
> manually) it may be difficult. It also depends on how large
> each group is (balance is generally better).
> 
> Cheers,
> Bert

I would add to Berton's list such scores as numbers of different
words used, sentence lengths, relative frequencies of verbs,
nouns, adjectives, adverbs, and so on, perhaps scaled by overall
length. Length of Essay might even be a discriminant!

You could also look at more subtle characteristics such as
"Zipf bins"[*] -- the relative numbers of different
words which occur once only, twice, three times, ... (though
I'm not sure how you would score such a thing for classification
purposes).
[*] A term I've just invented inspired by the original instance
    of this by the linguist Zipf, later giving rise to the
    logarithmic distribution in the historic paper by Fisher,
    Corbett & Williams in the "Numbers of Species and Numbers
    of Individuals" in butterfly traps.

If you really want to go to town you can try things related to
grammatical complexity, e.g. numbers of subordinate clauses
per sentence, relative clauses, the "reach" of relative pronouns
(how far from the referring pronoun is the thing referred to)
and so on.

There's quite an extensive literature on this sort of thing.
though it's not as fashionable as it used to be.

Th real problem is that you can get carried away by "good
ideas" of things to try!

The other factor to bear in mind is that if the Essays
can be grouped by subject this is likely to influence many
of the scores (such as the above).

Hoping this helps and does not distract!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 13-Jun-05                                       Time: 00:43:10
------------------------------ XFMail ------------------------------



From ramasamy at cancer.org.uk  Mon Jun 13 03:17:07 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 13 Jun 2005 02:17:07 +0100
Subject: [R] ANOVA vs REML approach to variance component estimation
In-Reply-To: <40e66e0b050612165462c37294@mail.gmail.com>
References: <1118424903.7638.50.camel@ipc143019.lif.icnet.uk>
	<42A9E592.5050201@optonline.net>
	<1118619050.5832.9.camel@dhcp-63.ccc.ox.ac.uk>
	<40e66e0b050612165462c37294@mail.gmail.com>
Message-ID: <1118625427.5832.43.camel@dhcp-63.ccc.ox.ac.uk>

Thank you.

On Sun, 2005-06-12 at 18:54 -0500, Douglas Bates wrote:
> On 6/12/05, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> > Thank you for confirming this and introducing me to varcomp().
> > 
> > I have another question that I hope you or someone else can help me
> > with. I was trying to generalise my codes for variable measurement
> > levels and discovered that lme() was estimating the within group
> > variance even with a single measure per subject for all subjects !
> > 
> > Here is an example where we have 12 animals but with single measurement.
> > 
> >   y  <- c(2.2, -1.4, -0.5, -0.3, -2.1, 1.5,
> >           1.3, -0.3, 0.5, -1.4, -0.2, 1.8)
> >   ID <- factor( 1:12 )
> > 
> > 
> > Analysis of variance method correctly says that there is no residual
> > variance and it equals to total variance.
> > 
> > summary(aov(y ~ ID))
> >             Df  Sum Sq Mean Sq
> > ID          11 20.9692  1.9063
> > 
> > 
> > However the REML method is giving me a within animal variance when there
> > is no replication at animal level. It seems like I can get components of
> > variance for factors that are not replicated.
> > 
> > library(ape)
> > varcomp(lme(y ~ 1, random = ~ 1 | ID))
> >        ID    Within
> > 1.6712661 0.2350218
> > 
> > Am I reading this correct and can someone kindly explain this to me ?
> 
> It's a spurious convergence in lme.  There is no check in lme for the
> number of observations exceeding the number of groups.  There should
> be.  I'll add this to the bug reports list.
>



From 0034058 at fudan.edu.cn  Mon Jun 13 06:36:47 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 13 Jun 2005 12:36:47 +0800
Subject: [R] slow loading with lme4
Message-ID: <0II0001EZ9NTCE@mail.fudan.edu.cn>

it takes a long time to load the lme4 package.anyone else encounter this problem?

> system.time(library(lme4))
Å‘ÅÿÅ»ÅÎÅ–ÅËÅ“Å™ÅµÅƒÅ≥ÅÃÅºÅ≠Å∞Å¸Å£Å∫Matrix
Å‘ÅÿÅ»ÅÎÅ–ÅËÅ“Å™ÅµÅƒÅ≥ÅÃÅºÅ≠Å∞Å¸Å£Å∫lattice
[1] 19.90  0.30 25.56    NA    NA


> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   Patched        
major    2              
minor    1.0            
year     2005           
month    05             
day      29             
language R     

OS:windows 2000
 				


2005-06-13

------
Deparment of Sociology
Fudan University

Blog:www.sociology.yculblog.com



From gerifalte28 at hotmail.com  Mon Jun 13 07:54:11 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 13 Jun 2005 05:54:11 +0000
Subject: [R] us zipcode data map
In-Reply-To: <27db823f05061018066c7e353d@mail.gmail.com>
Message-ID: <BAY103-F1E6B19C301AED43AB491AA6F00@phx.gbl>

Not that I am aware of.  Try library(help="maps") for a list of all the 
functions in the library.  Anyhow, I am not sure that a US map with zipcodes 
will look very good/readable, unless you focus on a very small area (i.e. 
county).

Cheers

Francisco

>From: Mike R <mike.rstat at gmail.com>
>Reply-To: r-help at stat.math.ethz.ch
>To: r-help at stat.math.ethz.ch
>Subject: Re: [R] us zipcode data map
>Date: Fri, 10 Jun 2005 18:06:39 -0700
>
>On 6/10/05, Francisco J. Zagmutt <gerifalte28 at hotmail.com> wrote:
> > library(maps)
> > example(match.map) #for coloring
> >
> > If you want to annotate the map look at ?map.text
>
>thanks Francisco,  correct me if i am wrong, but maps_2.0-27.tar.gz
>does many many maps, but not any zipcode maps ?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jun 13 08:44:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Jun 2005 07:44:03 +0100 (BST)
Subject: [R] slow loading with lme4
In-Reply-To: <0II0001EZ9NTCE@mail.fudan.edu.cn>
References: <0II0001EZ9NTCE@mail.fudan.edu.cn>
Message-ID: <Pine.LNX.4.61.0506130732130.21939@gannet.stats>

On Mon, 13 Jun 2005, ronggui wrote:

> it takes a long time to load the lme4 package.anyone else encounter this 
> problem?

Yes: ca 10 secs on my machine.  Why do you call it a `problem', though?

>> system.time(library(lme4))
> ??????????????????Matrix
> ??????????????????lattice
> [1] 19.90  0.30 25.56    NA    NA

Profiling this shows that almost all the time is spent on setting up S4 
methods:

$by.total
                              total.time total.pct self.time self.pct
"eval"                            10.32     100.0      0.02      0.2
"library"                         10.32     100.0      0.00      0.0
"system.time"                     10.32     100.0      0.00      0.0
"<Anonymous>"                      9.92      96.1      0.00      0.0
"insertMethod"                     8.62      83.5      1.86     18.0
"getAllMethods"                    8.36      81.0      0.00      0.0
"mergeMethods"                     7.94      76.9      0.00      0.0

I am sure the packages' authors are aware of that.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From mike.rstat at gmail.com  Mon Jun 13 08:52:45 2005
From: mike.rstat at gmail.com (Mike R)
Date: Sun, 12 Jun 2005 23:52:45 -0700
Subject: [R] crosshair in scatterplot to mark special points
Message-ID: <27db823f050612235216db1438@mail.gmail.com>

In a plot on an X11 device,  I'd like to mark a few points with 
a large thin "crosshairs".  By "mark" I mean: draw the crosshair
on the plot.

The two unsatisfactory methods that I have are:

METHOD 1:
   points( x, y, pch="+" ,cex=2)

METHOD 2:  
    line(xa, xb, y0, y0)
    line(x0, x0, ya, yb)

Method #1 produces a crosshair with fat lines.  It is not possible
to adequately scale the "plus-sign" for my purposes, due to messages like:

   X11 used font size 34 when 42 was requested

Method #2 produces a thin crosshair, but it is difficult to get the two 
crosshair lines to be of the same physical length on the device.  the 
lines also change length when i change/resize the device.

Thanks in advance for any help or suggestions,

Mike

platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R



From marc.girondot at ese.u-psud.fr  Mon Jun 13 09:14:32 2005
From: marc.girondot at ese.u-psud.fr (Marc Girondot)
Date: Mon, 13 Jun 2005 09:14:32 +0200
Subject: [R] Problem with multinom ?
Message-ID: <p06210202bed2e2900cf9@[62.147.79.56]>

>On Sat, 11 Jun 2005, John Fox wrote:
>
>>Dear Marc,
>>
>>I get the same results -- same coefficients, standard errors, and fitted
>>probabilities -- from multinom() and glm(). It's true that the deviances
>>differ, but they, I believe, are defined only up to an additive constant:
>
>Yes. There are many variations on the definition 
>of (residual) deviance, but it compares -2 log 
>likelihood with a `saturated' model.  For 
>grouped data you have a choice: a separate term 
>for each group or for each observation.  A 
>binomial GLM uses the first but the second is 
>more
>normal in logistic regression (since it has a 
>direct interpretation via log-probability 
>scoring).
>
>multinom() is support software for a book (which 
>the R posting guide does ask you to consult): 
>this is discussed with a worked example on pp 
>203-4.

Dear Prof. Ripley,

I have your book... but I don't find the answer to my questions...

You propose that the difference in residual 
deviance between two versions of the same model 
(0.001841823 for glm and  106.2304 for 
multinom()) is due to a difference in the 
specification of the satured model. However, as 
RD=-2 Ln L model+2 Ln L saturated and that -2 Ln 
L model=11.1146... it seems impossible to me that 
RD > -2 Ln L model ...

Marc Girondot

Sorry to be so close-minded !
-- 

__________________________________________________________
Marc Girondot, Pr
Laboratoire Ecologie, Syst??matique et Evolution
Equipe de Conservation des Populations et des Communaut??s
CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
B??timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
15 56 96   e-mail: marc.girondot at ese.u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot
Fax in US: 1-425-732-6934



From mike_liz.day at tiscali.co.uk  Mon Jun 13 09:22:05 2005
From: mike_liz.day at tiscali.co.uk (Mike Day)
Date: Mon, 13 Jun 2005 08:22:05 +0100
Subject: [R] assignment of inidividual variables from spss save files
Message-ID: <42AD341D.2070102@tiscali.co.uk>

New to R, can't afford SPSS!

Why can't I assign spss variables? Here are the details.

 > ### I've read in my collaborator's sav file using read.spss, eg
 > children = read.spss(filename)
 > ### It has many variables
 > length(children)
[1] 347
 > ### and I would like to assign individual variables.
 > ### I can of course type out all the ones I want, eg
 > ADULTS11<-children[18]
 > ### But I'm lazy, so tried:
 > (function (x,n){assign(names(x[n]),x[n])})(children,2)
 > ### where
 > names(children[2])
[1] "ADNUTS06"
 > ### but
 > ADNUTS06
Error: Object "ADNUTS06" not found
 > ### I've tried lots of variations, including creating and
 > ### evaluating statements such as "ADNUTS06<-children[2]",
 > ### but without apparently yielding the required variable.

I expect the answer will be "You shouldn?t start from here!"
Ideas welcome.

Thanks, Mike



From wilks at dial.pipex.com  Mon Jun 13 09:30:38 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Mon, 13 Jun 2005 08:30:38 +0100
Subject: [R] identify label format problem
Message-ID: <JCEIJNOHMNBPLMGFDHNDCEIICAAA.wilks@dial.pipex.com>

Hi ,

I am attempting to format the 'identify' function labels.
I can format the colour but the 'cex' parameter appears not to work
for me.

example--

> x<-1:5
> y<-1:5
> plot(x,y)
> identify(x,y,cex=0.5,col=2)
[1] 3

The label is coloured red but the 'cex=0.5' does not reduce 
the label size. Why not,, am I doing it wrong ?


> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R     

Thanks for any help,

John



From aliscla at yahoo.com  Mon Jun 13 09:33:49 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 13 Jun 2005 00:33:49 -0700 (PDT)
Subject: [R] Essay identification
In-Reply-To: <XFMail.050613004313.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20050613073350.18546.qmail@web61211.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/695391d5/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 13 09:40:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Jun 2005 08:40:43 +0100 (BST)
Subject: [R] identify label format problem
In-Reply-To: <JCEIJNOHMNBPLMGFDHNDCEIICAAA.wilks@dial.pipex.com>
References: <JCEIJNOHMNBPLMGFDHNDCEIICAAA.wilks@dial.pipex.com>
Message-ID: <Pine.LNX.4.61.0506130834550.22741@gannet.stats>

On Mon, 13 Jun 2005, John Wilkinson (pipex) wrote:

> I am attempting to format the 'identify' function labels.
> I can format the colour but the 'cex' parameter appears not to work
> for me.
>
> example--
>
>> x<-1:5
>> y<-1:5
>> plot(x,y)
>> identify(x,y,cex=0.5,col=2)
> [1] 3
>
> The label is coloured red but the 'cex=0.5' does not reduce
> the label size. Why not,, am I doing it wrong ?

This is one of the longest-standing reported bugs in R:

http://r-bugs.biostat.ku.dk/cgi-bin/R/Graphics?id=660;user=guest

Since it has been reported for nearly five years, I would not expect a fix 
anytime soon.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jun 13 09:39:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jun 2005 09:39:54 +0200
Subject: [R] assignment of inidividual variables from spss save files
In-Reply-To: <42AD341D.2070102@tiscali.co.uk>
References: <42AD341D.2070102@tiscali.co.uk>
Message-ID: <x2hdg2yab9.fsf@turmalin.kubism.ku.dk>

Mike Day <mike_liz.day at tiscali.co.uk> writes:

> New to R, can't afford SPSS!
> 
> Why can't I assign spss variables? Here are the details.
> 
>  > ### I've read in my collaborator's sav file using read.spss, eg
>  > children = read.spss(filename)
>  > ### It has many variables
>  > length(children)
> [1] 347
>  > ### and I would like to assign individual variables.
>  > ### I can of course type out all the ones I want, eg
>  > ADULTS11<-children[18]
>  > ### But I'm lazy, so tried:
>  > (function (x,n){assign(names(x[n]),x[n])})(children,2)
>  > ### where
>  > names(children[2])
> [1] "ADNUTS06"
>  > ### but
>  > ADNUTS06
> Error: Object "ADNUTS06" not found
>  > ### I've tried lots of variations, including creating and
>  > ### evaluating statements such as "ADNUTS06<-children[2]",
>  > ### but without apparently yielding the required variable.
> 
> I expect the answer will be "You shouldn?t start from here!"
> Ideas welcome.

Right. Instead, read up on the attach() function and with(). Read
through "An Introduction to R" (manual, comes with R) and pay
particular attention to chapter 6(.3). Beware the tricky bits
relating to the search path and masking of variables.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From francoisromain at free.fr  Mon Jun 13 09:47:36 2005
From: francoisromain at free.fr (Romain Francois)
Date: Mon, 13 Jun 2005 09:47:36 +0200
Subject: [R] assignment of inidividual variables from spss save files
In-Reply-To: <42AD341D.2070102@tiscali.co.uk>
References: <42AD341D.2070102@tiscali.co.uk>
Message-ID: <42AD3A18.9050505@free.fr>

Le 13.06.2005 09:22, Mike Day a ÅÈcrit :

>New to R, can't afford SPSS!
>
>Why can't I assign spss variables? Here are the details.
>
> > ### I've read in my collaborator's sav file using read.spss, eg
> > children = read.spss(filename)
> > ### It has many variables
> > length(children)
>[1] 347
> > ### and I would like to assign individual variables.
> > ### I can of course type out all the ones I want, eg
> > ADULTS11<-children[18]
> > ### But I'm lazy, so tried:
> > (function (x,n){assign(names(x[n]),x[n])})(children,2)
> > ### where
> > names(children[2])
>[1] "ADNUTS06"
> > ### but
> > ADNUTS06
>Error: Object "ADNUTS06" not found
> > ### I've tried lots of variations, including creating and
> > ### evaluating statements such as "ADNUTS06<-children[2]",
> > ### but without apparently yielding the required variable.
>
>I expect the answer will be "You shouldnít start from here!"
>Ideas welcome.
>
>Thanks, Mike
>  
>
?attach may be what you are looking for


-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ycxu at aphy.iphy.ac.cn  Mon Jun 13 10:27:51 2005
From: ycxu at aphy.iphy.ac.cn (Arthur)
Date: Mon, 13 Jun 2005 08:27:51 GMT
Subject: [R] (no subject)
Message-ID: <20050613082751.27215.eqmail@aphy.iphy.ac.cn>

Dear: 

How do you do?So sorry to bother you. 

Nowadays I deal with my datum by Vis5D,but the datum I have 

is in text style.I find that you had post the programs to 

convert the text to V5D on the WWW.Unfortunately the links to 

the R2v5d.f and v5df.h are void.Can you be so friendly to send them to me? 

Thanks. 

yours 

Arthur xu



From subramanian.vivek at gmail.com  Mon Jun 13 10:08:28 2005
From: subramanian.vivek at gmail.com (Vivek Subramanian)
Date: Mon, 13 Jun 2005 13:38:28 +0530
Subject: [R] Interfacing R
In-Reply-To: <20e69eb705061301064cb01dc5@mail.gmail.com>
References: <20e69eb705061301064cb01dc5@mail.gmail.com>
Message-ID: <20e69eb705061301081cc59ea6@mail.gmail.com>

hi,
i am developing an application on the windows platform with the win32
api. my application has to accept a file and then call R to process
that file.
i would like to know how i can do this? is there a header file that i
can use or is there any other way to do it.

i am writting my application using C and a version using VB. please help.

regards,
vivek



From B.Rowlingson at lancaster.ac.uk  Mon Jun 13 11:19:00 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 13 Jun 2005 10:19:00 +0100
Subject: [R] crosshair in scatterplot to mark special points
In-Reply-To: <27db823f050612235216db1438@mail.gmail.com>
References: <27db823f050612235216db1438@mail.gmail.com>
Message-ID: <42AD4F84.9010901@lancaster.ac.uk>

Mike R wrote:
> In a plot on an X11 device,  I'd like to mark a few points with 
> a large thin "crosshairs".  By "mark" I mean: draw the crosshair
> on the plot.
> 

  Try using the numerical pch symbols. '3' is a crosshair, so see what 
the following do:

  plot(1:10,pch=3)
  plot(1:10,pch=3,cex=2)
  plot(1:10,pch=3,cex=4)

They're probably documented somewhere, but:

  plot(1:26,pch=1:26,cex=2)

  will show you them.

Baz



From HDoran at air.org  Mon Jun 13 11:57:47 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 13 Jun 2005 05:57:47 -0400
Subject: [R] slow loading with lme4
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41B81@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/80b98a97/attachment.pl

From HDoran at air.org  Mon Jun 13 12:05:44 2005
From: HDoran at air.org (Doran, Harold)
Date: Mon, 13 Jun 2005 06:05:44 -0400
Subject: [R] slow loading with lme4
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7407E41B82@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/1184df3f/attachment.pl

From renaud.lancelot at cirad.fr  Mon Jun 13 12:10:12 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 13 Jun 2005 13:10:12 +0300
Subject: [R] slow loading with lme4
In-Reply-To: <0II0001EZ9NTCE@mail.fudan.edu.cn>
References: <0II0001EZ9NTCE@mail.fudan.edu.cn>
Message-ID: <42AD5B84.7000107@cirad.fr>

ronggui a Å®Å¶crit :
> it takes a long time to load the lme4 package.anyone else encounter this problem?
> 
> 
>>system.time(library(lme4))
> 
> Å‘ÅÿÅ»ÅÎÅ–ÅËÅ“Å™ÅµÅƒÅ≥ÅÃÅºÅ≠Å∞Å¸Å£Å∫Matrix
> Å‘ÅÿÅ»ÅÎÅ–ÅËÅ“Å™ÅµÅƒÅ≥ÅÃÅºÅ≠Å∞Å¸Å£Å∫lattice
> [1] 19.90  0.30 25.56    NA    NA
> 
> 
> 
>>version
> 
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status   Patched        
> major    2              
> minor    1.0            
> year     2005           
> month    05             
> day      29             
> language R     
> 
> OS:windows 2000
>  				
> 
> 
> 2005-06-13
> 
> ------
> Deparment of Sociology
> Fudan University
> 
> Blog:www.sociology.yculblog.com
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

No. I use either pre-compiled binaries or I install the packages from
source (Matrix and lme4).

Windows XP + R 2.1.0 unpatched.

Best,

Renaud

-- 
Dr Renaud Lancelot, vÅ®Å¶tÅ®Å¶rinaire
Projet FSP rÅ®Å¶gional Å®Å¶pidemio vÅ®Å¶tÅ®Å¶rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
        +261 20 22 665 36 ext. 225 (work)
        +261 20 22 494 37 (home)



From ripley at stats.ox.ac.uk  Mon Jun 13 12:33:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Jun 2005 11:33:37 +0100 (BST)
Subject: [R] slow loading with lme4
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7407E41B81@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7407E41B81@dc1ex2.air.org>
Message-ID: <Pine.LNX.4.61.0506131121180.24712@gannet.stats>

On Mon, 13 Jun 2005, Doran, Harold wrote:

> It is actually the Matrix package that is taking so long, not lme4.

Not so: lme4 is taking longer than Matrix:

> system.time(library(Matrix))
[1] 4.17 0.11 4.28 0.00 0.00
> system.time(library(lme4))
Loading required package: lattice
[1] 6.55 0.08 6.64 0.00 0.00

(this is the additional time for lme4, of course).

> It is is extremely large and is required for use with lme4. I think Doug 
> Bates or Duncan Murdoch can confirm Matrix() contains more than 6000 or 
> so lines of code. But, it is not a problem, it just is what it is.

Actually, only 2302 lines of R code. That of itself is no issue at all: 
package stats has 23000 lines and loads almost instantly: a default R 
session loads about 65000 lines in well under a second (about 0.4s on the 
machine whose timings are given above).

As I have already replied (with numerical evidence), the issue is handling 
all the S4 methods which the packages define.  The developers have put a 
lot of effort in loading lots of R code fast, but little optimization has 
been done on loading S4 methods.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bxc at steno.dk  Mon Jun 13 12:43:20 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 13 Jun 2005 12:43:20 +0200
Subject: [R] assignment of inidividual variables from spss save files
Message-ID: <40D3930AC1C8EA469E39536E5BC80835701AA3@EXDKBA021.corp.novocorp.net>

The key to solving your problem is that read.spss per default
gives you a *list* and not a *dataframe* (can anyone explain this
choice of default?).

So most likely wou want:

children = read.spss(filename,to.data.frame=TRUE)
attach(children)

or to get things a little more handy:

children <- read.spss(filename,to.data.frame=TRUE)
names(children) <- tolower( names( children ) )
attach(children)

best,
Bendix
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: Romain Francois [mailto:francoisromain at free.fr] 
> Sent: Monday, June 13, 2005 9:48 AM
> To: Mike Day; RHELP
> Subject: Re: [R] assignment of inidividual variables from 
> spss save files
> 
> 
> Le 13.06.2005 09:22, Mike Day a ??crit :
> 
> >New to R, can't afford SPSS!
> >
> >Why can't I assign spss variables? Here are the details.
> >
> > > ### I've read in my collaborator's sav file using read.spss, eg 
> > > children = read.spss(filename) ### It has many variables
> > > length(children)
> >[1] 347
> > > ### and I would like to assign individual variables.
> > > ### I can of course type out all the ones I want, eg 
> > > ADULTS11<-children[18] ### But I'm lazy, so tried:
> > > (function (x,n){assign(names(x[n]),x[n])})(children,2)
> > > ### where
> > > names(children[2])
> >[1] "ADNUTS06"
> > > ### but
> > > ADNUTS06
> >Error: Object "ADNUTS06" not found
> > > ### I've tried lots of variations, including creating and ### 
> > > evaluating statements such as "ADNUTS06<-children[2]", ### but 
> > > without apparently yielding the required variable.
> >
> >I expect the answer will be "You shouldn't start from here!" Ideas 
> >welcome.
> >
> >Thanks, Mike
> >  
> >
> ?attach may be what you are looking for
> 
> 
> -- 
> visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
> ~~~~~~~~ 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> ~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr      
>    ~~~~~~
> ~~~~        Etudiant  ISUP - CS3 - Industrie et Services      
>      ~~~~
> ~~                http://www.isup.cicrp.jussieu.fr/           
>        ~~
> ~~~~           Stagiaire INRIA Futurs - Equipe SELECT         
>      ~~~~
> ~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html 
>    ~~~~~~
> ~~~~~~~~ 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
> 
> 
>



From subianto at gmail.com  Mon Jun 13 13:16:44 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 13 Jun 2005 13:16:44 +0200
Subject: [R] combination which limited
In-Reply-To: <3635ddc20506120148216e7881@mail.gmail.com>
References: <3635ddc205061111447dee5aa5@mail.gmail.com>	<1118516208.647.4.camel@localhost.localdomain>	<971536df0506111231183371a7@mail.gmail.com>
	<3635ddc20506120148216e7881@mail.gmail.com>
Message-ID: <42AD6B1C.1000905@gmail.com>

Dear R-helpers,

On this day 6/12/2005 10:48 AM, Muhammad Subianto wrote:
> Dear All,
> Many thanks to Marc Schwartz and Gabor Grothendieck who have explained
> me about using expand.grid function and clearly explain how to use
> JGR.
> 
> 
>>dd <- expand.grid(interface = interface, screen = screen,
>>   computer = computer, available = available)
>>
>>There are several possibilities now:
>>
>>1. you could list out dd on the console and note the number of the
>>rows you want to keep:
>>
>>idx <- c(1,5,7)
>>dd2 <- dd[,idx]
>>
> 
> 
> I like a possible no. 1, because I can use and explore with my hand,
> 
>> idx <- c(1:5,9,17,25)
>> dd2 <- dd[idx,]
>> dd2
> 
>    interface screen computer available
> 1        usb    lcd       pc       yes
> 2   fireware    lcd       pc       yes
> 3      infra    lcd       pc       yes
> 4  bluetooth    lcd       pc       yes
> 5        usb   cube       pc       yes
> 9        usb    lcd   server       yes
> 17       usb    lcd   laptop       yes
> 25       usb    lcd       pc        no
> 
> 
> Regards,
> Muhammad Subianto
> Notepad, Copy and Paste are my best friend to use R.2.1.0 on windows 2000
> 

As previous mail, using expand.grid can handle all variables in 
datasets. But, if I need only one or more combinations I can choice 
combination (rows) which I need,

interface <- c("usb","fireware","infra","bluetooth")
screen    <- c("lcd","cube")
computer  <- c("pc","server","laptop")
available <- c("yes","no")
dd <- 
expand.grid(interface=interface,screen=screen,computer=computer,available=available)
idx <- c(1:5,9,17,25) # this combination rows) what I need
dd2 <- dd[idx,]
dd2

Because I only need combination (1,2,3,4,5,9,17 and 25), I tried to make 
a simple code to make sure what pattern the combination I have. I was 
wondering if someone can help me to make a simple function.

   smdxc <- rbind(
           c(levels(dd[,1])[1],	#combination 1
             levels(dd[,2])[1],
             levels(dd[,3])[1],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[2],	#combination 2
             levels(dd[,2])[1],
             levels(dd[,3])[1],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[3],	#combination 3
             levels(dd[,2])[1],
             levels(dd[,3])[1],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[4],	#combination 4
             levels(dd[,2])[1],
             levels(dd[,3])[1],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[1],	#combination 5
             levels(dd[,2])[2],
             levels(dd[,3])[1],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[1],	#combination 9
             levels(dd[,2])[1],
             levels(dd[,3])[2],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[1],	#combination 17
             levels(dd[,2])[1],
             levels(dd[,3])[3],
             levels(dd[,4])[1]),

           c(levels(dd[,1])[1],	#combination 25
             levels(dd[,2])[1],
             levels(dd[,3])[1],
             levels(dd[,4])[2]))

smdxc # the result = dd2

Thank you very much in advance.
Kindly regards,
Muhammad Subianto



From ggrothendieck at gmail.com  Mon Jun 13 14:38:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 13 Jun 2005 08:38:10 -0400
Subject: [R] combination which limited
In-Reply-To: <42AD6B1C.1000905@gmail.com>
References: <3635ddc205061111447dee5aa5@mail.gmail.com>
	<1118516208.647.4.camel@localhost.localdomain>
	<971536df0506111231183371a7@mail.gmail.com>
	<3635ddc20506120148216e7881@mail.gmail.com>
	<42AD6B1C.1000905@gmail.com>
Message-ID: <971536df050613053852cf6f90@mail.gmail.com>

On 6/13/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Dear R-helpers,
> 
> On this day 6/12/2005 10:48 AM, Muhammad Subianto wrote:
> > Dear All,
> > Many thanks to Marc Schwartz and Gabor Grothendieck who have explained
> > me about using expand.grid function and clearly explain how to use
> > JGR.
> >
> >
> >>dd <- expand.grid(interface = interface, screen = screen,
> >>   computer = computer, available = available)
> >>
> >>There are several possibilities now:
> >>
> >>1. you could list out dd on the console and note the number of the
> >>rows you want to keep:
> >>
> >>idx <- c(1,5,7)
> >>dd2 <- dd[,idx]
> >>
> >
> >
> > I like a possible no. 1, because I can use and explore with my hand,
> >
> >> idx <- c(1:5,9,17,25)
> >> dd2 <- dd[idx,]
> >> dd2
> >
> >    interface screen computer available
> > 1        usb    lcd       pc       yes
> > 2   fireware    lcd       pc       yes
> > 3      infra    lcd       pc       yes
> > 4  bluetooth    lcd       pc       yes
> > 5        usb   cube       pc       yes
> > 9        usb    lcd   server       yes
> > 17       usb    lcd   laptop       yes
> > 25       usb    lcd       pc        no
> >
> >
> > Regards,
> > Muhammad Subianto
> > Notepad, Copy and Paste are my best friend to use R.2.1.0 on windows 2000
> >
> 
> As previous mail, using expand.grid can handle all variables in
> datasets. But, if I need only one or more combinations I can choice
> combination (rows) which I need,
> 
> interface <- c("usb","fireware","infra","bluetooth")
> screen    <- c("lcd","cube")
> computer  <- c("pc","server","laptop")
> available <- c("yes","no")
> dd <-
> expand.grid(interface=interface,screen=screen,computer=computer,available=available)
> idx <- c(1:5,9,17,25) # this combination rows) what I need
> dd2 <- dd[idx,]
> dd2
> 
> Because I only need combination (1,2,3,4,5,9,17 and 25), I tried to make
> a simple code to make sure what pattern the combination I have. I was
> wondering if someone can help me to make a simple function.
> 
>   smdxc <- rbind(
>           c(levels(dd[,1])[1], #combination 1
>             levels(dd[,2])[1],
>             levels(dd[,3])[1],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[2], #combination 2
>             levels(dd[,2])[1],
>             levels(dd[,3])[1],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[3], #combination 3
>             levels(dd[,2])[1],
>             levels(dd[,3])[1],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[4], #combination 4
>             levels(dd[,2])[1],
>             levels(dd[,3])[1],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[1], #combination 5
>             levels(dd[,2])[2],
>             levels(dd[,3])[1],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[1], #combination 9
>             levels(dd[,2])[1],
>             levels(dd[,3])[2],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[1], #combination 17
>             levels(dd[,2])[1],
>             levels(dd[,3])[3],
>             levels(dd[,4])[1]),
> 
>           c(levels(dd[,1])[1], #combination 25
>             levels(dd[,2])[1],
>             levels(dd[,3])[1],
>             levels(dd[,4])[2]))
> 
> smdxc # the result = dd2

The pattern seems to be that each row contains at most one column
that is not at level 1.  That is the entry at row i and column col[i] has
level lev[i] and all other entries are at level 1.

	col <- c(1,1,1,1,2,3,3,4)
	lev <- c(1:4,2,2,3,2)
	mat <- matrix(1, length(col), 4)
	mat[cbind(seq(col),col)] <- lev
	data.frame(interface = factor(mat[,1], lab = interface),
		screen = factor(mat[,2], lab = screen),
		computer = factor(mat[,3], lab = computer),
		available = factor(mat[,4], lab = available))



From gcendoya at balcarce.inta.gov.ar  Mon Jun 13 15:03:09 2005
From: gcendoya at balcarce.inta.gov.ar (CENDOYA, MarÅÌa Gabriela)
Date: Mon, 13 Jun 2005 10:03:09 -0300
Subject: [R] Warning messages in lmer function (package lme4)
Message-ID: <00e201c57018$3f967c40$a600fd0a@balcarce.inta.gov.ar>

Hi:

I'm using function lmer from package lme4, and I get this message:



" There were 12 warnings (use warnings() to see them)"

So I checked them:

Warnings 1 to 11 said:

1: optim returned message ERROR: ABNORMAL_TERMINATION_IN_LNSRCH

 in: "LMEoptimize<-"(`*tmp*`, value = structure(list(maxIter = 50,   ...



and Warning 12 said:

12: IRLS iterations for glmm did not converge in: lmer(Enfermo ~ Bloque +
Trat * Var * dia + (1 | IDfruto), data = desinf,   ...



There was no error message in the call of lmer function, so:

What do these warnings mean?

Do these errors occur at earlier iterations but finally the model converges?

Can the model I got be trust?



Thanks, Gabriela.



PD: I'm using:

Windows XP ,

R 2.1.0,

Package lme4, version 0.95-6

And the fitted model was:

modelo1 <- lmer(Enfermo ~ Bloque+Trat*Var*dia+(1|IDfruto),

                 data=desinf,
subset=!desinf$dia==1&!desinf$Trat=="Sin_inoculo",

                 family=binomial)



From yves.ahipo at lea.univ-poitiers.fr  Mon Jun 13 17:47:01 2005
From: yves.ahipo at lea.univ-poitiers.fr (Ahipo Yves)
Date: Mon, 13 Jun 2005 15:47:01 +0000
Subject: [R] I need help about the use of gtkgraph
In-Reply-To: <200506120030.alNgDBCxxI56E@cfdreview.com>
References: <200506120030.alNgDBCxxI56E@cfdreview.com>
Message-ID: <05061315470101.24984@europa.sp2mi.univ-poitiers.fr>


> I need some informations about the use of gtkgraph.
>
>   I want to plot a graph with data which are in a file.
>   I don't want some icon like (diamond, cercle or anything else)
>
>   I have done these successives step.
>     gtkgraph
>         windows
>           show data
>              file
>                load data set
>                   setting
>                     ion
>      at this step i don't how i can desactivate the icon choice; because i
> don't want icon.
>
>     thank you for your precious help.
>
>    AHIPO Yves Poitiers University
>
>  PS: excuse me for my bad english



From dmbates at gmail.com  Mon Jun 13 15:36:37 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 13 Jun 2005 08:36:37 -0500
Subject: [R] Warning messages in lmer function (package lme4)
In-Reply-To: <00e201c57018$3f967c40$a600fd0a@balcarce.inta.gov.ar>
References: <00e201c57018$3f967c40$a600fd0a@balcarce.inta.gov.ar>
Message-ID: <40e66e0b0506130636c65b933@mail.gmail.com>

On 6/13/05, CENDOYA, Mar??a Gabriela <gcendoya at balcarce.inta.gov.ar> wrote:
> Hi:
> 
> I'm using function lmer from package lme4, and I get this message:
> 
> " There were 12 warnings (use warnings() to see them)"
> 
> So I checked them:
> 
> Warnings 1 to 11 said:
> 
> 1: optim returned message ERROR: ABNORMAL_TERMINATION_IN_LNSRCH
> 
>  in: "LMEoptimize<-"(`*tmp*`, value = structure(list(maxIter = 50,   ...
> 
> 
> 
> and Warning 12 said:
> 
> 12: IRLS iterations for glmm did not converge in: lmer(Enfermo ~ Bloque +
> Trat * Var * dia + (1 | IDfruto), data = desinf,   ...

> There was no error message in the call of lmer function, so:
> 
> What do these warnings mean?
> 
> Do these errors occur at earlier iterations but finally the model converges?

Yes. The PQL and Laplace methods for fitting generalized linear mixed
models perform optimization of a criterion that is itself determined
by an optimization problem.  These warnings come from earlier
iterations and are warnings.
 
> Can the model I got be trusted?

Well George Box said that "All models are wrong" so in that sense you
can't trust any fitted model.  This type of model is difficult to fit
and you should, as with all model fits, check the parameter values to
see if they make sense to you.

You may want to try the same fit in a test build of R-devel, which
will become R-2.2.0.  In R-2.1.0 part of the optimization problem is
performed by optim() and I have had some peculiar results from optim
using method="L-BFGS-B".  In constrained minimization I have had
optim() declare convergence to a value on the boundary where the
objective function was larger than at the starting point.  I have
added the function nlminb() to the development version of R-2.2.0 and
use that for the constrained optimization problem in lmer().  I have
found the results to be more reliable than those from optim().



From bernd.weiss at uni-koeln.de  Mon Jun 13 16:16:39 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Mon, 13 Jun 2005 16:16:39 +0200
Subject: [R] Lattice: Combining xyplot and histogram
Message-ID: <42ADB167.7750.EA1E53@localhost>

Dear all,

I am trying to create a lattice plot which consists of 1 xyplot and 2 
histograms (each for x and y). 

My first try was like this:

x<-rnorm(1000)
y<-rnorm(1000)
xy <- xyplot(y~x)
hist.x <- histogram(x)
hist.y <- histogram(y)
print(xy, position=c(0, 0.2, 1, 1), more=TRUE)
print(hist.x, position=c(0, 0, 1, 0.33),more=T)
print(hist.y, position=c(0.8, 0, 1, 1))

Ok, this is obviously not the solution. I would appreciate any 
suggestions anyone could give.

Bernd



From r.hankin at noc.soton.ac.uk  Mon Jun 13 16:23:21 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 13 Jun 2005 15:23:21 +0100
Subject: [R] extracting components of a list
Message-ID: <b7817e389d471f374a420e2379326e42@soc.soton.ac.uk>

Hi

how do I extract those components of a list that satisfy a certain 
requirement?  If

jj <- list(list(a=1,b=4:7),list(a=5,b=3:6),list(a=10,b=4:5))


I want just the components of jj that have b[1] ==4 which in this case 
would be the first and
third of jj, viz    list (jj[[1]],jj[[3]]).

How to do this efficiently?

My only idea was to loop through jj, and set unwanted components to 
NULL, but
FAQ 7.1 warns against this.




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From roy.werkman at asml.com  Mon Jun 13 16:25:44 2005
From: roy.werkman at asml.com (Roy Werkman)
Date: Mon, 13 Jun 2005 16:25:44 +0200
Subject: [R] Lattice: Combining xyplot and histogram
Message-ID: <448071208107374B96ED90585EEBA912753312@NLVDHX84.sn-eu.asml.com>

 
Hello Bernd,

Did you look at the help for layout(). This is probably what you need...

Roy 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bernd Weiss
Sent: Monday, June 13, 2005 4:17 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Lattice: Combining xyplot and histogram

Dear all,

I am trying to create a lattice plot which consists of 1 xyplot and 2
histograms (each for x and y). 

My first try was like this:

x<-rnorm(1000)
y<-rnorm(1000)
xy <- xyplot(y~x)
hist.x <- histogram(x)
hist.y <- histogram(y)
print(xy, position=c(0, 0.2, 1, 1), more=TRUE) print(hist.x,
position=c(0, 0, 1, 0.33),more=T) print(hist.y, position=c(0.8, 0, 1,
1))

Ok, this is obviously not the solution. I would appreciate any
suggestions anyone could give.

Bernd

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


-- 
The information contained in this communication and any atta...{{dropped}}



From dimitris.rizopoulos at med.kuleuven.be  Mon Jun 13 16:33:29 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 13 Jun 2005 16:33:29 +0200
Subject: [R] extracting components of a list
References: <b7817e389d471f374a420e2379326e42@soc.soton.ac.uk>
Message-ID: <009f01c57024$ddbfdf40$0540210a@www.domain>

maybe something like this:

jj <- list(list(a = 1, b = 4:7), list(a = 5, b = 3:6), list(a = 10, b 
= 4:5))
###############
jj[sapply(jj,  function(x) x$b[1] == 4)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "r-help" <R-help at stat.math.ethz.ch>
Sent: Monday, June 13, 2005 4:23 PM
Subject: [R] extracting components of a list


> Hi
>
> how do I extract those components of a list that satisfy a certain
> requirement?  If
>
> jj <- list(list(a=1,b=4:7),list(a=5,b=3:6),list(a=10,b=4:5))
>
>
> I want just the components of jj that have b[1] ==4 which in this 
> case
> would be the first and
> third of jj, viz    list (jj[[1]],jj[[3]]).
>
> How to do this efficiently?
>
> My only idea was to loop through jj, and set unwanted components to
> NULL, but
> FAQ 7.1 warns against this.
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Mon Jun 13 16:37:31 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 13 Jun 2005 10:37:31 -0400
Subject: [R] Lattice: Combining xyplot and histogram
In-Reply-To: <42ADB167.7750.EA1E53@localhost>
References: <42ADB167.7750.EA1E53@localhost>
Message-ID: <971536df05061307374a3462bb@mail.gmail.com>

Check out this recent thread:

Solution using names (except you should use the 'with' implementation
from the second link here):
https://www.stat.math.ethz.ch/pipermail/r-devel/2005-June/033439.html

Solution that traverses children directly (somewhat more open
to breaking if grid changes but significantly shorter):
https://www.stat.math.ethz.ch/pipermail/r-devel/2005-June/033508.html

You might want to read the entire thread since it discusses anticipated
enhancements in lattice and grid that will make this even nicer in the
future.

On 6/13/05, Bernd Weiss <bernd.weiss at uni-koeln.de> wrote:
> Dear all,
> 
> I am trying to create a lattice plot which consists of 1 xyplot and 2
> histograms (each for x and y).
> 
> My first try was like this:
> 
> x<-rnorm(1000)
> y<-rnorm(1000)
> xy <- xyplot(y~x)
> hist.x <- histogram(x)
> hist.y <- histogram(y)
> print(xy, position=c(0, 0.2, 1, 1), more=TRUE)
> print(hist.x, position=c(0, 0, 1, 0.33),more=T)
> print(hist.y, position=c(0.8, 0, 1, 1))
> 
> Ok, this is obviously not the solution. I would appreciate any
> suggestions anyone could give.
> 
> Bernd
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From plummer at iarc.fr  Mon Jun 13 16:39:23 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Mon, 13 Jun 2005 16:39:23 +0200
Subject: [R] extracting components of a list
In-Reply-To: <b7817e389d471f374a420e2379326e42@soc.soton.ac.uk>
References: <b7817e389d471f374a420e2379326e42@soc.soton.ac.uk>
Message-ID: <1118673563.20524.4.camel@seurat>

On Mon, 2005-06-13 at 15:23 +0100, Robin Hankin wrote:
> Hi
> 
> how do I extract those components of a list that satisfy a certain 
> requirement?  If
> 
> jj <- list(list(a=1,b=4:7),list(a=5,b=3:6),list(a=10,b=4:5))
> 
> 
> I want just the components of jj that have b[1] ==4 which in this case 
> would be the first and
> third of jj, viz    list (jj[[1]],jj[[3]]).
> 
> How to do this efficiently?
> 
> My only idea was to loop through jj, and set unwanted components to 
> NULL, but
> FAQ 7.1 warns against this.
> 

#Select the vectors named "b" from the elements of jj
bvectors <- lapply(jj, FUN="[[", "b") 
#Take the first element of each b
bfirst <- sapply(bvectors, head, 1)
#Select elements of jj such that bfirst is 4
jj[bfirst == 4]



From tlumley at u.washington.edu  Mon Jun 13 17:00:58 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 13 Jun 2005 08:00:58 -0700 (PDT)
Subject: [R] assignment of inidividual variables from spss save files
In-Reply-To: <40D3930AC1C8EA469E39536E5BC80835701AA3@EXDKBA021.corp.novocorp.net>
References: <40D3930AC1C8EA469E39536E5BC80835701AA3@EXDKBA021.corp.novocorp.net>
Message-ID: <Pine.A41.4.61b.0506130753140.79102@homer03.u.washington.edu>

On Mon, 13 Jun 2005, BXC (Bendix Carstensen) wrote:

> The key to solving your problem is that read.spss per default
> gives you a *list* and not a *dataframe* (can anyone explain this
> choice of default?).
>

The reason for the default is speed.

It's possible that the default could be changed to make it easier to use 
for people who don't read the help page. I haven't measured the speed 
difference recently, but it used to be significant.  IIRC the reasoning 
was that you would often want just a few variables from a data file, in 
which case subsetting before creating a data frame was a worthwhile 
saving.

I have certainly had complaints that reading just a few variables from a 
large Stata file is slow (because it does produce a data frame first)


 	-thomas



From jcole at aipl.arsusda.gov  Mon Jun 13 17:15:01 2005
From: jcole at aipl.arsusda.gov (John B. Cole, Ph.D)
Date: Mon, 13 Jun 2005 11:15:01 -0400
Subject: [R] unixODBC, RODBC, and DB2
Message-ID: <42ADA2F5.5010109@aipl.arsusda.gov>

All-

Does anyone on the list have experience with building RODBC from source 
on a Linux box for use with DB2?

I am using (all from source):
R 2.0.1
unixODBC 2.2.9
RODBC 1.1-3

For example:

[jcole]$ R CMD INSTALL RODBC_1.1-3.tar.gz 2> rodbc.log
* Installing *source* package 'RODBC' ...
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking for library containing SQLTables... -lodbc
configure: creating ./config.status
config.status: creating src/Makevars
** libs
gcc -I/home/jcole/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c 
RODBC.c -o RODBC.o
** Removing '/home/jcole/lib/R/library/RODBC'
** Restoring previous '/home/jcole/lib/R/library/RODBC'

 From rodbc.log it seems that the problem may be with an IBM-supplied 
header file:

In file included from RODBC.c:24:
/home/db2inst1/sqllib/include/sqlext.h:1681: error: parse error before 
"SQL_API"
/home/db2inst1/sqllib/include/sqlext.h:1682: error: parse error before 
"hdbc"
<More gory details available upon request.>

Any thoughts would be very much appreciated.  I have verified that my 
unixODBC installation is good by connecting to a datasource and making 
some successful queries against my DB2 installation.

John.
-- 
Dr. John B. Cole, Research Geneticist
Animal Improvement Programs Laboratory
10300 Baltimore Avenue
BARC-West, Building 005, Room 306
Beltsville, Maryland 20705-2350

Telephone: (301) 504-8665
FAX: (301) 504-8092
E-mail: jcole at aipl.arsusda.gov



From greg.snow at ihc.com  Mon Jun 13 17:42:14 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Mon, 13 Jun 2005 09:42:14 -0600
Subject: [R] us zipcode data map
Message-ID: <s2ad5507.005@lp-msg1.co.ihc.com>

I have had success by downloading the zipcode (approximate) shapefiles
(the .shp files) from: http://www.census.gov/geo/www/cob/z52000.html

Then using the maptools package (rather than the maps package).

hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> mike.rstat at gmail.com 06/10/05 04:18PM >>>
i've search the email archives, searched the documention 
of various map packages and done an R-site search, but 
have been unable to find direct resources for creating maps 
of the US that are colored or annotated or ... by zipcode 
data.  

For example, create a map of the US and color each zipcode 
region by its population using two vectors z,p containing the 
zipcode and population, respectively.  I'm not looking for data 
to fill z, and p.  I'm looking for some highend functions to 
display on a map the data that I already have.

Cheers,
Mike

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Mon Jun 13 17:57:39 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 13 Jun 2005 10:57:39 -0500
Subject: [R] install R 2.1.0 patched from source on FC3
Message-ID: <cdf81783050613085756418161@mail.gmail.com>

Hi, 
I have some problem when I tried to install R from source:
work as root
cd /root/dls/R-patched         # this is where I unzip the file:
R-patched_2005-06-08.tar.gz
make clean

# i need my R_HOME=/usr/lib/R and I need to create libR.so for
RSPython, so I did:
./configure --prefix=/usr/lib/R --enable-R-shlib   
make

But I found the R is built in /root/dls/R-patched/bin instead of /usr/lib/R/bin

Did I miss something here?

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From greg.snow at ihc.com  Mon Jun 13 18:02:25 2005
From: greg.snow at ihc.com (Greg Snow)
Date: Mon, 13 Jun 2005 10:02:25 -0600
Subject: [R] Essay identification
Message-ID: <s2ad59c5.083@lp-msg1.co.ihc.com>

This topic is sometimes called wordprinting or stylometry.  The spring
2003 issue of Chance magazine had several articles on the topic.

A colleague of mine and I have been working on a perl program (along
with various graduate students) to extract many of the common statistics
used in wordprinting (counts/percentages of non-contextual words, word
pattern ratios, vocabulary richness).  The data can then be loaded into
R (or any other stats package) to be analyzed.

The program is currently in a beta state (usable, but we want to
possibly add more features and documentation), but I can send a copy to
anyone who is interested (specify if you have perl, or need a stand
alone copy (windows only)).

hope this helps,

Greg Snow, Ph.D.
Statistical Data Center, LDS Hospital
Intermountain Health Care
greg.snow at ihc.com
(801) 408-8111

>>> Werner Bier <aliscla at yahoo.com> 06/12/05 01:29PM >>>
Hi R-help,
 
I have a database of 10 students who have written an overall of 78
essays. 
The challenge? I would like to identify who wrote the 79th essay.
 
Has anybody used R in this context? 
 
Even if not, would you suggest me which pattern recognition technique I
might possibly apply?
 
Thanks a lot and regards,
Tom 


		
---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Jun 13 18:04:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jun 2005 18:04:22 +0200
Subject: [R] install R 2.1.0 patched from source on FC3
In-Reply-To: <cdf81783050613085756418161@mail.gmail.com>
References: <cdf81783050613085756418161@mail.gmail.com>
Message-ID: <x24qc2dz09.fsf@turmalin.kubism.ku.dk>

Weiwei Shi <helprhelp at gmail.com> writes:

> Hi, 
> I have some problem when I tried to install R from source:
> work as root
> cd /root/dls/R-patched         # this is where I unzip the file:
> R-patched_2005-06-08.tar.gz
> make clean
> 
> # i need my R_HOME=/usr/lib/R and I need to create libR.so for
> RSPython, so I did:
> ./configure --prefix=/usr/lib/R --enable-R-shlib   
> make
> 
> But I found the R is built in /root/dls/R-patched/bin instead of /usr/lib/R/bin
> 
> Did I miss something here?

Yes.

make install

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mzp3769 at yahoo.com  Mon Jun 13 18:10:43 2005
From: mzp3769 at yahoo.com (m p)
Date: Mon, 13 Jun 2005 09:10:43 -0700 (PDT)
Subject: [R] kalman filter
Message-ID: <20050613161043.46625.qmail@web51008.mail.yahoo.com>

Hello,
is there any implementation of Kalman filter in R?
Thanks,
Mark



From subianto at gmail.com  Mon Jun 13 18:13:36 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Mon, 13 Jun 2005 18:13:36 +0200
Subject: [R] combination which limited - thanks again
In-Reply-To: <971536df050613053852cf6f90@mail.gmail.com>
References: <3635ddc205061111447dee5aa5@mail.gmail.com>	<1118516208.647.4.camel@localhost.localdomain>	<971536df0506111231183371a7@mail.gmail.com>	<3635ddc20506120148216e7881@mail.gmail.com>	<42AD6B1C.1000905@gmail.com>
	<971536df050613053852cf6f90@mail.gmail.com>
Message-ID: <42ADB0B0.5060606@gmail.com>

Dear all,
Again, I would like to thank Gabor Grothendieck for your help.
I can improve which you suggest with the others combination.
And thank you for your time.
Sincerely,
Muhammad Subianto

On this day 6/13/2005 2:38 PM, Gabor Grothendieck wrote:
> 
> The pattern seems to be that each row contains at most one column
> that is not at level 1.  That is the entry at row i and column col[i] has
> level lev[i] and all other entries are at level 1.
> 
> 	col <- c(1,1,1,1,2,3,3,4)
> 	lev <- c(1:4,2,2,3,2)
> 	mat <- matrix(1, length(col), 4)
> 	mat[cbind(seq(col),col)] <- lev
> 	data.frame(interface = factor(mat[,1], lab = interface),
> 		screen = factor(mat[,2], lab = screen),
> 		computer = factor(mat[,3], lab = computer),
> 		available = factor(mat[,4], lab = available))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From aliscla at yahoo.com  Mon Jun 13 18:27:30 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 13 Jun 2005 09:27:30 -0700 (PDT)
Subject: [R] kalman filter
In-Reply-To: <20050613161043.46625.qmail@web51008.mail.yahoo.com>
Message-ID: <20050613162730.76482.qmail@web61212.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/cdee2e8c/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 13 18:38:39 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Jun 2005 17:38:39 +0100 (BST)
Subject: [R] unixODBC, RODBC, and DB2
In-Reply-To: <42ADA2F5.5010109@aipl.arsusda.gov>
References: <42ADA2F5.5010109@aipl.arsusda.gov>
Message-ID: <Pine.LNX.4.61.0506131734370.2272@gannet.stats>

Can you fix your include paths?  You want the sql.h and sqlext.h from 
unixODBC.

On Mon, 13 Jun 2005, John B. Cole, Ph.D wrote:

> All-
>
> Does anyone on the list have experience with building RODBC from source
> on a Linux box for use with DB2?
>
> I am using (all from source):
> R 2.0.1
> unixODBC 2.2.9
> RODBC 1.1-3
>
> For example:
>
> [jcole]$ R CMD INSTALL RODBC_1.1-3.tar.gz 2> rodbc.log
> * Installing *source* package 'RODBC' ...
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking for library containing SQLTables... -lodbc
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> gcc -I/home/jcole/lib/R/include  -I/usr/local/include   -fPIC  -g -O2 -c
> RODBC.c -o RODBC.o
> ** Removing '/home/jcole/lib/R/library/RODBC'
> ** Restoring previous '/home/jcole/lib/R/library/RODBC'
>
> From rodbc.log it seems that the problem may be with an IBM-supplied
> header file:
>
> In file included from RODBC.c:24:
> /home/db2inst1/sqllib/include/sqlext.h:1681: error: parse error before
> "SQL_API"
> /home/db2inst1/sqllib/include/sqlext.h:1682: error: parse error before
> "hdbc"
> <More gory details available upon request.>
>
> Any thoughts would be very much appreciated.  I have verified that my
> unixODBC installation is good by connecting to a datasource and making
> some successful queries against my DB2 installation.
>
> John.
> -- 
> Dr. John B. Cole, Research Geneticist
> Animal Improvement Programs Laboratory
> 10300 Baltimore Avenue
> BARC-West, Building 005, Room 306
> Beltsville, Maryland 20705-2350
>
> Telephone: (301) 504-8665
> FAX: (301) 504-8092
> E-mail: jcole at aipl.arsusda.gov
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From helprhelp at gmail.com  Mon Jun 13 18:39:21 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 13 Jun 2005 11:39:21 -0500
Subject: [R] install R 2.1.0 patched from source on FC3
In-Reply-To: <x24qc2dz09.fsf@turmalin.kubism.ku.dk>
References: <cdf81783050613085756418161@mail.gmail.com>
	<x24qc2dz09.fsf@turmalin.kubism.ku.dk>
Message-ID: <cdf81783050613093926b68d0@mail.gmail.com>

Hi,

I got an error message when I run make install:

/usr/bin/install: cannot stat `index.html': No such file or directory
make[2]: *** [install] Error 1

the full description as below:

[root at banana R-patched]# make install
make[1]: Entering directory `/root/dls/R-patched/m4'
make[1]: Nothing to be done for `install'.
make[1]: Leaving directory `/root/dls/R-patched/m4'
make[1]: Entering directory `/root/dls/R-patched/tools'
make[1]: Nothing to be done for `install'.
make[1]: Leaving directory `/root/dls/R-patched/tools'
make[1]: Entering directory `/root/dls/R-patched/doc'
installing doc ...
make[2]: Entering directory `/root/dls/R-patched/doc/html'
installing doc/html ...
/usr/bin/install: cannot stat `index.html': No such file or directory
make[2]: *** [install] Error 1
make[2]: Leaving directory `/root/dls/R-patched/doc/html'
make[1]: *** [install] Error 1
make[1]: Leaving directory `/root/dls/R-patched/doc'
make: *** [install] Error 1


when I run configure, I got two warnings, is it the cause the problem
in make install?

R is now configured for i686-pc-linux-gnu

  Source directory:          .
  Installation directory:    /usr/lib/R

  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          g77  -g -O2

  Interfaces supported:      X11
  External libraries:        readline, BLAS(generic)
  Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
  Options enabled:           shared library, R profiling

  Recommended packages:      yes

configure: WARNING: you cannot build DVI versions of the R manuals
configure: WARNING: you cannot build PDF versions of the R manuals


On 13 Jun 2005 18:04:22 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Weiwei Shi <helprhelp at gmail.com> writes:
> 
> > Hi,
> > I have some problem when I tried to install R from source:
> > work as root
> > cd /root/dls/R-patched         # this is where I unzip the file:
> > R-patched_2005-06-08.tar.gz
> > make clean
> >
> > # i need my R_HOME=/usr/lib/R and I need to create libR.so for
> > RSPython, so I did:
> > ./configure --prefix=/usr/lib/R --enable-R-shlib
> > make
> >
> > But I found the R is built in /root/dls/R-patched/bin instead of /usr/lib/R/bin
> >
> > Did I miss something here?
> 
> Yes.
> 
> make install
> 
> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From p.dalgaard at biostat.ku.dk  Mon Jun 13 18:41:58 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jun 2005 18:41:58 +0200
Subject: [R] install R 2.1.0 patched from source on FC3
In-Reply-To: <cdf81783050613093926b68d0@mail.gmail.com>
References: <cdf81783050613085756418161@mail.gmail.com>
	<x24qc2dz09.fsf@turmalin.kubism.ku.dk>
	<cdf81783050613093926b68d0@mail.gmail.com>
Message-ID: <x2vf4icip5.fsf@turmalin.kubism.ku.dk>

Weiwei Shi <helprhelp at gmail.com> writes:

> Hi,
> 
> I got an error message when I run make install:
> 
> /usr/bin/install: cannot stat `index.html': No such file or directory
> make[2]: *** [install] Error 1
> 
> the full description as below:
> 
> [root at banana R-patched]# make install
> make[1]: Entering directory `/root/dls/R-patched/m4'
> make[1]: Nothing to be done for `install'.
> make[1]: Leaving directory `/root/dls/R-patched/m4'
> make[1]: Entering directory `/root/dls/R-patched/tools'
> make[1]: Nothing to be done for `install'.
> make[1]: Leaving directory `/root/dls/R-patched/tools'
> make[1]: Entering directory `/root/dls/R-patched/doc'
> installing doc ...
> make[2]: Entering directory `/root/dls/R-patched/doc/html'
> installing doc/html ...
> /usr/bin/install: cannot stat `index.html': No such file or directory
> make[2]: *** [install] Error 1
> make[2]: Leaving directory `/root/dls/R-patched/doc/html'
> make[1]: *** [install] Error 1
> make[1]: Leaving directory `/root/dls/R-patched/doc'
> make: *** [install] Error 1
> 
> 
> when I run configure, I got two warnings, is it the cause the problem
> in make install?
> 
> R is now configured for i686-pc-linux-gnu
> 
>   Source directory:          .
>   Installation directory:    /usr/lib/R
> 
>   C compiler:                gcc  -g -O2
>   C++ compiler:              g++  -g -O2
>   Fortran compiler:          g77  -g -O2
> 
>   Interfaces supported:      X11
>   External libraries:        readline, BLAS(generic)
>   Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
>   Options enabled:           shared library, R profiling
> 
>   Recommended packages:      yes
> 
> configure: WARNING: you cannot build DVI versions of the R manuals
> configure: WARNING: you cannot build PDF versions of the R manuals

Yes. You cannot install without having built those, so you need to
install the texinfo tools. Specifically

[pd at titmouse BUILD]$ rpm -q texinfo
texinfo-4.8-2.1


 
> 
> On 13 Jun 2005 18:04:22 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > Weiwei Shi <helprhelp at gmail.com> writes:
> > 
> > > Hi,
> > > I have some problem when I tried to install R from source:
> > > work as root
> > > cd /root/dls/R-patched         # this is where I unzip the file:
> > > R-patched_2005-06-08.tar.gz
> > > make clean
> > >
> > > # i need my R_HOME=/usr/lib/R and I need to create libR.so for
> > > RSPython, so I did:
> > > ./configure --prefix=/usr/lib/R --enable-R-shlib
> > > make
> > >
> > > But I found the R is built in /root/dls/R-patched/bin instead of /usr/lib/R/bin
> > >
> > > Did I miss something here?
> > 
> > Yes.
> > 
> > make install
> > 
> > --
> >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
> >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> >  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> > 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From helprhelp at gmail.com  Mon Jun 13 18:44:10 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 13 Jun 2005 11:44:10 -0500
Subject: [R] install R 2.1.0 patched from source on FC3
In-Reply-To: <x2vf4icip5.fsf@turmalin.kubism.ku.dk>
References: <cdf81783050613085756418161@mail.gmail.com>
	<x24qc2dz09.fsf@turmalin.kubism.ku.dk>
	<cdf81783050613093926b68d0@mail.gmail.com>
	<x2vf4icip5.fsf@turmalin.kubism.ku.dk>
Message-ID: <cdf817830506130944117c1dc6@mail.gmail.com>

never mind:
my sys admin (he is a newbie too, now i think)  tells me I should run
make install then make though i thought in another way. anyway, i
found the problem: I should run make then make install.  :)


On 13 Jun 2005 18:41:58 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Weiwei Shi <helprhelp at gmail.com> writes:
> 
> > Hi,
> >
> > I got an error message when I run make install:
> >
> > /usr/bin/install: cannot stat `index.html': No such file or directory
> > make[2]: *** [install] Error 1
> >
> > the full description as below:
> >
> > [root at banana R-patched]# make install
> > make[1]: Entering directory `/root/dls/R-patched/m4'
> > make[1]: Nothing to be done for `install'.
> > make[1]: Leaving directory `/root/dls/R-patched/m4'
> > make[1]: Entering directory `/root/dls/R-patched/tools'
> > make[1]: Nothing to be done for `install'.
> > make[1]: Leaving directory `/root/dls/R-patched/tools'
> > make[1]: Entering directory `/root/dls/R-patched/doc'
> > installing doc ...
> > make[2]: Entering directory `/root/dls/R-patched/doc/html'
> > installing doc/html ...
> > /usr/bin/install: cannot stat `index.html': No such file or directory
> > make[2]: *** [install] Error 1
> > make[2]: Leaving directory `/root/dls/R-patched/doc/html'
> > make[1]: *** [install] Error 1
> > make[1]: Leaving directory `/root/dls/R-patched/doc'
> > make: *** [install] Error 1
> >
> >
> > when I run configure, I got two warnings, is it the cause the problem
> > in make install?
> >
> > R is now configured for i686-pc-linux-gnu
> >
> >   Source directory:          .
> >   Installation directory:    /usr/lib/R
> >
> >   C compiler:                gcc  -g -O2
> >   C++ compiler:              g++  -g -O2
> >   Fortran compiler:          g77  -g -O2
> >
> >   Interfaces supported:      X11
> >   External libraries:        readline, BLAS(generic)
> >   Additional capabilities:   PNG, JPEG, iconv, MBCS, NLS
> >   Options enabled:           shared library, R profiling
> >
> >   Recommended packages:      yes
> >
> > configure: WARNING: you cannot build DVI versions of the R manuals
> > configure: WARNING: you cannot build PDF versions of the R manuals
> 
> Yes. You cannot install without having built those, so you need to
> install the texinfo tools. Specifically
> 
> [pd at titmouse BUILD]$ rpm -q texinfo
> texinfo-4.8-2.1
> 
> 
> 
> >
> > On 13 Jun 2005 18:04:22 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> > > Weiwei Shi <helprhelp at gmail.com> writes:
> > >
> > > > Hi,
> > > > I have some problem when I tried to install R from source:
> > > > work as root
> > > > cd /root/dls/R-patched         # this is where I unzip the file:
> > > > R-patched_2005-06-08.tar.gz
> > > > make clean
> > > >
> > > > # i need my R_HOME=/usr/lib/R and I need to create libR.so for
> > > > RSPython, so I did:
> > > > ./configure --prefix=/usr/lib/R --enable-R-shlib
> > > > make
> > > >
> > > > But I found the R is built in /root/dls/R-patched/bin instead of /usr/lib/R/bin
> > > >
> > > > Did I miss something here?
> > >
> > > Yes.
> > >
> > > make install
> > >
> > > --
> > >    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
> > >   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
> > >  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
> > > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> 
> --
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From aliscla at yahoo.com  Mon Jun 13 18:47:27 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 13 Jun 2005 09:47:27 -0700 (PDT)
Subject: [R] Perl installation under SuSe
Message-ID: <20050613164727.91298.qmail@web61215.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/7eea8737/attachment.pl

From jimrc at math.montana.edu  Mon Jun 13 19:05:46 2005
From: jimrc at math.montana.edu (Jim Robison-Cox)
Date: Mon, 13 Jun 2005 11:05:46 -0600 (MDT)
Subject: [R] To many NA's from mean(..., na.rm=T) when a column is all NA's
Message-ID: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>

Dear R-help folks,

I am seeing unexpected behaviour from the function mean
with option na.rm =TRUE (which is removing a whole column of a data frame
or matrix.

example:

testcase <- data.frame( x = 1:3, y = rep(NA,3))

mean(testcase[,1], na.rm=TRUE)
[1] 2
mean(testcase[,2], na.rm = TRUE)
[1] NaN

  OK, so far that seems sensible.  Now I'd like to compute both means at
once:

  lapply(testcase, mean, na.rm=T)   ## this works
$x
[1] 2

$y
[1] NaN

  But I thought that this would also work:

apply(testcase, 2, mean, na.rm=T)
 x  y
NA NA
Warning messages:
1: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)
2: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)

 Summary:
  If I have a data frame or a matrix where one entire column is NA's,
mean(x, na.rm=T) works on that column, returning NaN, but fails using
apply, in that apply returns NA for ALL columns.
  lapply works fine on the data frame.

  If you wonder why I'm building data frames with columns that could be
all missing -- they arise as output of a simulation.  The fact that the
entire column is missing is informative in itself.


  I do wonder if this is a bug.

Thanks,
Jim

Jim Robison-Cox               ____________
Department of Math Sciences  |            |       phone: (406)994-5340
2-214 Wilson Hall             \   BZN, MT |       FAX:   (406)994-1789
Montana State University       |  *_______|
Bozeman, MT 59717-2400          \_|      e-mail: jimrc at math.montana.edu



From peteoutside at yahoo.com  Mon Jun 13 19:16:31 2005
From: peteoutside at yahoo.com (Pete Cap)
Date: Mon, 13 Jun 2005 10:16:31 -0700 (PDT)
Subject: [R] Preparing timestamped data for fourier analysis
Message-ID: <20050613171631.29376.qmail@web52406.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/9fe508bc/attachment.pl

From sundar.dorai-raj at pdf.com  Mon Jun 13 19:19:27 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 13 Jun 2005 12:19:27 -0500
Subject: [R] To many NA's from mean(...,
 na.rm=T) when a column is all NA's
In-Reply-To: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>
References: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>
Message-ID: <42ADC01F.3030700@pdf.com>



Jim Robison-Cox wrote:
> Dear R-help folks,
> 
> I am seeing unexpected behaviour from the function mean
> with option na.rm =TRUE (which is removing a whole column of a data frame
> or matrix.
> 
> example:
> 
> testcase <- data.frame( x = 1:3, y = rep(NA,3))
> 
> mean(testcase[,1], na.rm=TRUE)
> [1] 2
> mean(testcase[,2], na.rm = TRUE)
> [1] NaN
> 
>   OK, so far that seems sensible.  Now I'd like to compute both means at
> once:
> 
>   lapply(testcase, mean, na.rm=T)   ## this works
> $x
> [1] 2
> 
> $y
> [1] NaN
> 
>   But I thought that this would also work:
> 
> apply(testcase, 2, mean, na.rm=T)
>  x  y
> NA NA
> Warning messages:
> 1: argument is not numeric or logical: returning NA in:
> mean.default(newX[, i], ...)
> 2: argument is not numeric or logical: returning NA in:
> mean.default(newX[, i], ...)
> 
>  Summary:
>   If I have a data frame or a matrix where one entire column is NA's,
> mean(x, na.rm=T) works on that column, returning NaN, but fails using
> apply, in that apply returns NA for ALL columns.
>   lapply works fine on the data frame.
> 

Did you try this with a "matrix" or just a data.frame?

>   If you wonder why I'm building data frames with columns that could be
> all missing -- they arise as output of a simulation.  The fact that the
> entire column is missing is informative in itself.
> 
> 
>   I do wonder if this is a bug.
> 


Your problem is not ?apply, but ?as.matrix, which apply calls. Hint: Try 
as.matrix(testdata) and see what it returns.

If you need a matrix, why construct a data.frame? The following will 
give you what you want:

x <- matrix(c(1:3, rep(NA, 3)), nc = 2)
apply(x, 2, mean, na.rm = TRUE)

or better yet,

colMeans(x, na.rm = TRUE)

Note, that colMeans may give NA instead of NaN for column 2. See 
?colMeans for an explanation.

HTH,

--sundar



From p.dalgaard at biostat.ku.dk  Mon Jun 13 19:19:54 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Jun 2005 19:19:54 +0200
Subject: [R] To many NA's from mean(...,
	na.rm=T) when a column is all NA's
In-Reply-To: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>
References: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>
Message-ID: <x2r7f6cgxx.fsf@turmalin.kubism.ku.dk>

Jim Robison-Cox <jimrc at math.montana.edu> writes:

>  Summary:
>   If I have a data frame or a matrix where one entire column is NA's,
> mean(x, na.rm=T) works on that column, returning NaN, but fails using
> apply, in that apply returns NA for ALL columns.
>   lapply works fine on the data frame.
> 
>   If you wonder why I'm building data frames with columns that could be
> all missing -- they arise as output of a simulation.  The fact that the
> entire column is missing is informative in itself.
> 
> 
>   I do wonder if this is a bug.

It isn't...

Cutting a long story short:

> testcase <- data.frame( x = 1:3, y = rep(NA,3))
> as.matrix(testcase)
  x   y
1 "1" NA
2 "2" NA
3 "3" NA
> testcase <- data.frame( x = 1:3, y = as.numeric(rep(NA,3)))
> as.matrix(testcase)
  x  y
1 1 NA
2 2 NA
3 3 NA
> apply(testcase,2,mean,na.rm=T)
  x   y
  2 NaN


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Achim.Zeileis at wu-wien.ac.at  Mon Jun 13 19:24:32 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 13 Jun 2005 19:24:32 +0200
Subject: [R] To many NA's from mean(...,
 na.rm=T) when a column is all NA's
In-Reply-To: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>
References: <Pine.GSO.4.44.0506131053310.28068-100000@gauss1.math.montana.edu>
Message-ID: <20050613192432.3fceb0ae.Achim.Zeileis@wu-wien.ac.at>

On Mon, 13 Jun 2005 11:05:46 -0600 (MDT) Jim Robison-Cox wrote:

> Dear R-help folks,
> 
> I am seeing unexpected behaviour from the function mean
> with option na.rm =TRUE (which is removing a whole column of a data
> frame or matrix.
> 
> example:
> 
> testcase <- data.frame( x = 1:3, y = rep(NA,3))

In addition to what Sundar already wrote: In the code above x is numeric
and y logical, hence as.matrix() will not do what you want (create a
"character" matrix). Probably it is more appropriate to do

  testcase <- data.frame( x = 1:3, y = as.numeric(rep(NA,3)))

hth,
Z

> mean(testcase[,1], na.rm=TRUE)
> [1] 2
> mean(testcase[,2], na.rm = TRUE)
> [1] NaN
> 
>   OK, so far that seems sensible.  Now I'd like to compute both means
>   at
> once:
> 
>   lapply(testcase, mean, na.rm=T)   ## this works
> $x
> [1] 2
> 
> $y
> [1] NaN
> 
>   But I thought that this would also work:
> 
> apply(testcase, 2, mean, na.rm=T)
>  x  y
> NA NA
> Warning messages:
> 1: argument is not numeric or logical: returning NA in:
> mean.default(newX[, i], ...)
> 2: argument is not numeric or logical: returning NA in:
> mean.default(newX[, i], ...)
> 
>  Summary:
>   If I have a data frame or a matrix where one entire column is NA's,
> mean(x, na.rm=T) works on that column, returning NaN, but fails using
> apply, in that apply returns NA for ALL columns.
>   lapply works fine on the data frame.
> 
>   If you wonder why I'm building data frames with columns that could
>   be
> all missing -- they arise as output of a simulation.  The fact that
> the entire column is missing is informative in itself.
> 
> 
>   I do wonder if this is a bug.
> 
> Thanks,
> Jim
> 
> Jim Robison-Cox               ____________
> Department of Math Sciences  |            |       phone: (406)994-5340
> 2-214 Wilson Hall             \   BZN, MT |       FAX:   (406)994-1789
> Montana State University       |  *_______|
> Bozeman, MT 59717-2400          \_|      e-mail:
> jimrc at math.montana.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From jimrc at math.montana.edu  Mon Jun 13 19:28:17 2005
From: jimrc at math.montana.edu (Jim Robison-Cox)
Date: Mon, 13 Jun 2005 11:28:17 -0600 (MDT)
Subject: [R] To many NA's from mean(...,
 na.rm=T) when a column is all NA's
In-Reply-To: <x2r7f6cgxx.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.GSO.4.44.0506131124070.28068-100000@gauss1.math.montana.edu>

I see.
   So apply() first changes the dataframe to a matrix, which made it
a matrix of text values, then mean made no sense for any column, so I got
all NA's.

Thanks, Peter,

Jim

On 13 Jun 2005, Peter Dalgaard wrote:

> Jim Robison-Cox <jimrc at math.montana.edu> writes:
>
> >  Summary:
> >   If I have a data frame or a matrix where one entire column is NA's,
> > mean(x, na.rm=T) works on that column, returning NaN, but fails using
> > apply, in that apply returns NA for ALL columns.
> >   lapply works fine on the data frame.
> >
> >   If you wonder why I'm building data frames with columns that could be
> > all missing -- they arise as output of a simulation.  The fact that the
> > entire column is missing is informative in itself.
> >
> >
> >   I do wonder if this is a bug.
>
> It isn't...
>
> Cutting a long story short:
>
> > testcase <- data.frame( x = 1:3, y = rep(NA,3))
> > as.matrix(testcase)
>   x   y
> 1 "1" NA
> 2 "2" NA
> 3 "3" NA
> > testcase <- data.frame( x = 1:3, y = as.numeric(rep(NA,3)))
> > as.matrix(testcase)
>   x  y
> 1 1 NA
> 2 2 NA
> 3 3 NA
> > apply(testcase,2,mean,na.rm=T)
>   x   y
>   2 NaN
>
>
>

Jim Robison-Cox               ____________
Department of Math Sciences  |            |       phone: (406)994-5340
2-214 Wilson Hall             \   BZN, MT |       FAX:   (406)994-1789
Montana State University       |  *_______|
Bozeman, MT 59717-2400          \_|      e-mail: jimrc at math.montana.edu



From ripley at stats.ox.ac.uk  Mon Jun 13 19:35:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Jun 2005 18:35:10 +0100 (BST)
Subject: [R] Perl installation under SuSe
In-Reply-To: <20050613164727.91298.qmail@web61215.mail.yahoo.com>
References: <20050613164727.91298.qmail@web61215.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506131831570.3045@gannet.stats>

On Mon, 13 Jun 2005, Werner Bier wrote:

> Hi R-Help,
>
> I have just downloaded  RSPerl_0.7-0.tar.gz under SuSe.
> If I type
>
> owner at linux: ~work> R CMD INSTALL -c RSPerl_0.7-0.tar.gz
>
> i get the following error message
>
> makedir: cannot create directory '/user/lib/R/library/00LOCK' : Permission denied
> ERROR: failed to lock directory '/usr/lib/R/library' for modifing

Does it really say that?  'makedir'?  'user'?  I don't think so.

> I would really appreciate if somebody can help me please.

You don't have permission to install in that directory.  Use an 
account that owns R and does, or install somewhere you do have permission.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Jun 13 19:49:37 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 13 Jun 2005 19:49:37 +0200
Subject: [R] Perl installation under SuSe
In-Reply-To: <20050613164727.91298.qmail@web61215.mail.yahoo.com>
References: <20050613164727.91298.qmail@web61215.mail.yahoo.com>
Message-ID: <42ADC731.20302@statistik.uni-dortmund.de>

Werner Bier wrote:

> Hi R-Help,
>  
> I have just downloaded  RSPerl_0.7-0.tar.gz under SuSe. 
> If I type 
>  
> owner at linux: ~work> R CMD INSTALL -c RSPerl_0.7-0.tar.gz
>  
> i get the following error message
>  
> makedir: cannot create directory '/user/lib/R/library/00LOCK' : Permission denied
> ERROR: failed to lock directory '/usr/lib/R/library' for modifing

I doubt one directory starts with "user" and the other one with "usr".
Do you have write permission to that directory?

If yes, is 00LOCK already there? Then remove it.
If no, install to a private library.

Uwe Ligges

> I would really appreciate if somebody can help me please.
> Thanks in advance,
> Tom
>  
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From aliscla at yahoo.com  Mon Jun 13 20:18:42 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Mon, 13 Jun 2005 11:18:42 -0700 (PDT)
Subject: [R] Perl installation under SuSe
In-Reply-To: <20050613164727.91298.qmail@web61215.mail.yahoo.com>
Message-ID: <20050613181842.44327.qmail@web61216.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/dfa96b00/attachment.pl

From djanes at oeb.harvard.edu  Mon Jun 13 20:33:26 2005
From: djanes at oeb.harvard.edu (Dan Janes)
Date: Mon, 13 Jun 2005 14:33:26 -0400
Subject: [R] Pseudodataset text file
Message-ID: <6.1.0.6.0.20050613142906.01f55890@mlr.oeb.harvard.edu>

Hi All,

Two weeks ago, I received instructions from the R list about creating 
pseudodatasets by bootstrapping an existing dataset 1000 times.  I received 
code that bootstrapped my dataset and ran an ANOVA on each pseudodataset, 
producing a histogram of F-values.  Here is the code I received that 
produces this F-histogram.

X <- data.frame(b=rnorm(2162),c =factor(rep(1:47,each=46)))

## linear fit with true values
lm0 <- lm(b~c,data=X)
Fvals <- numeric(1000)
for (i in 1:1000) {
    X.boot <- X[sample(1:nrow(X),replace=TRUE),]
    Fvals[i] <- anova(lm(b~c,data=X.boot))$"F value"[1]
}
hist(Fvals)

I am grateful for the assistance but the code does not provide what I 
need.  I need the 1000 pseudodatasets produced by the bootstrap in order to 
work with them in a different context.  Can you someone offer a suggestion 
on how to produce the bootstrapped datasets as a text file?

Thank you.

-Dan



************************************************
Dan Janes, Ph.D.
Harvard University/OEB
26 Oxford St.
Cambridge, MA 02138
Office: 617-496-2375
Fax: 617-495-5667
Email: djanes at oeb.harvard.edu



From SuzieBlatt at netscape.net  Mon Jun 13 21:09:31 2005
From: SuzieBlatt at netscape.net (SuzieBlatt@netscape.net)
Date: Mon, 13 Jun 2005 15:09:31 -0400
Subject: [R] ppinit
Message-ID: <381726F5.6BF59172.0D1322AF@netscape.net>


Probably a simple question, but I can't find the answer to it ...
In the 'ppinit' code it describes how it takes a 'file in standard format' and creates a point process object with it.  Can anyone enlighten me as to what this 'standard format' is?  The example given doesn't allow me to view the '.dat' file that apparently is in this format.
Thanks,
Suzie



From pmbrando at ipam.org.br  Tue Jun 14 00:25:25 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Mon, 13 Jun 2005 15:25:25 -0700
Subject: [R] RES:  install R 2.1.0 patched from source on FC3
In-Reply-To: <x24qc2dz09.fsf@turmalin.kubism.ku.dk>
Message-ID: <005101c57066$cc386430$f10aa8c0@paulobrando>


Dear Rs,

I have tried to use conditional expressions to calculate biomass for
different life forms (trees, lianas, and palms).

Here is an example:

> lifeform

    dbh  form
1   30  tree
2   29  tree
3   28  tree
4   27  tree
5   26  tree
6   25  tree
7   24  tree
8   23  tree
9   22  tree
10  21  tree
11  20  tree
12  15 liana
13  14 liana
14  13 liana
15  12 liana
16  11 liana
17  10 liana
18   9 liana
19   8 liana
20   7 liana
21   6 liana
22   5 liana
23  30  palm
24  29  palm
25  28  palm
26  27  palm
27  26  palm
28  25  palm
29  24  palm
30  23  palm
31  22  palm
32  21  palm
33  20  palm

### I want to include biomass 

lifeform$biomass <- 

{
      if(lifeform$form=="tree")
       exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2) 
       else{ 
        if (lifeform$form=="liana")
       10^(0.07 + 2.17 * log10 (dbh))
       else ("NA")
}
Warning message:
the condition has length > 1 and only the first element will be used in:
if (lifeform$form == "tree") exp(-4.898 + 4.512 * log(dbh) -


### But I always get the message warning message above. 



I looked for similar examples in R mail list archive, but they did not
help a lot.

I am quite new to 'R'. Any material that covers this theme?

Thank you very much!

Paulo
________________________________________
Paulo M. Brando
Instituto de Pesquisa Ambiental da Amazonia (IPAM)
Santarem, PA, Brasil.
Av. Rui Barbosa, 136.
Fone: + 55 93 522 55 38
www.ipam.org.br
E-mail: pmbrando at ipam.org.br



-----Mensagem original-----
De: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Em nome de Peter Dalgaard
Enviada em: Monday, June 13, 2005 9:04 AM
Para: Weiwei Shi
Cc: R-help at stat.math.ethz.ch
Assunto: Re: [R] install R 2.1.0 patched from source on FC3

Weiwei Shi <helprhelp at gmail.com> writes:

> Hi, 
> I have some problem when I tried to install R from source:
> work as root
> cd /root/dls/R-patched         # this is where I unzip the file:
> R-patched_2005-06-08.tar.gz
> make clean
> 
> # i need my R_HOME=/usr/lib/R and I need to create libR.so for
> RSPython, so I did:
> ./configure --prefix=/usr/lib/R --enable-R-shlib   
> make
> 
> But I found the R is built in /root/dls/R-patched/bin instead of
/usr/lib/R/bin
> 
> Did I miss something here?

Yes.

make install

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45)
35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45)
35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pmbrando at ipam.org.br  Tue Jun 14 00:32:49 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Mon, 13 Jun 2005 15:32:49 -0700
Subject: [R] Learning, "if" and "else"
In-Reply-To: <005101c57066$cc386430$f10aa8c0@paulobrando>
Message-ID: <005201c57067$d84610a0$f10aa8c0@paulobrando>


Dear Rs,

I have tried to use conditional expressions to calculate biomass for
different life forms (trees, lianas, and palms).

Here is an example:

> lifeform

    dbh  form
1   30  tree
2   29  tree
3   28  tree
4   27  tree
5   26  tree
6   25  tree
7   24  tree
8   23  tree
9   22  tree
10  21  tree
11  20  tree
12  15 liana
13  14 liana
14  13 liana
15  12 liana
16  11 liana
17  10 liana
18   9 liana
19   8 liana
20   7 liana
21   6 liana
22   5 liana
23  30  palm
24  29  palm
25  28  palm
26  27  palm
27  26  palm
28  25  palm
29  24  palm
30  23  palm
31  22  palm
32  21  palm
33  20  palm

### I want to include biomass 

lifeform$biomass <- 

{
      if(lifeform$form=="tree")
       exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2) 
       else{ 
        if (lifeform$form=="liana")
       10^(0.07 + 2.17 * log10 (dbh))
       else ("NA")
}
Warning message:
the condition has length > 1 and only the first element will be used in:
if (lifeform$form == "tree") exp(-4.898 + 4.512 * log(dbh) -


### But I always get the message warning message above. 



I looked for similar examples in R mail list archive, but they did not
help a lot.

I am quite new to 'R'. Any material that covers this theme?

Thank you very much!

Paulo

PS. Sorry about the last e-mail. I did not change the message title.
________________________________________
Paulo M. Brando
Instituto de Pesquisa Ambiental da Amazonia (IPAM)
Santarem, PA, Brasil.
Av. Rui Barbosa, 136.
Fone: + 55 93 522 55 38
www.ipam.org.br
E-mail: pmbrando at ipam.org.br



From Roger.Bivand at nhh.no  Mon Jun 13 21:47:45 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 13 Jun 2005 21:47:45 +0200 (CEST)
Subject: [R] ppinit
In-Reply-To: <381726F5.6BF59172.0D1322AF@netscape.net>
Message-ID: <Pine.LNX.4.44.0506132135100.18825-100000@reclus.nhh.no>

On Mon, 13 Jun 2005 SuzieBlatt at netscape.net wrote:

> 
> Probably a simple question, but I can't find the answer to it ... In the
> 'ppinit' code it describes how it takes a 'file in standard format' and
> creates a point process object with it.  Can anyone enlighten me as to
> what this 'standard format' is?  The example given doesn't allow me to
> view the '.dat' file that apparently is in this format. Thanks, Suzie

The ppinit() function is in the spatial package, for which the book:  
Venables, W. N. and Ripley, B. D. (2002) _Modern Applied Statistics with
S._ Fourth edition.  Springer, is the documentation. There (p. 433) you 
see that the function reads point coordinates in 2D, and the bounding 
coordinates of a rectangular domain. 

If you just enter the function name at the prompt, you'll see its code,
from which the rest follows, that is you get a list with x and y
components, and the definition of the rectangle. In addition ppinit()
calls ppregion() to set the domain for analysis.

If you'd like to look at an example:

file.show(system.file("ppdata", "towns.dat", package = "spatial"))

will show you the file provided with the package.

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From ccleland at optonline.net  Mon Jun 13 21:50:48 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 13 Jun 2005 15:50:48 -0400
Subject: [R] Learning, "if" and "else"
In-Reply-To: <005201c57067$d84610a0$f10aa8c0@paulobrando>
References: <005201c57067$d84610a0$f10aa8c0@paulobrando>
Message-ID: <42ADE398.5090906@optonline.net>

You could use ifelse() here:

lifeform <- data.frame(dbh = c(30,29,15,14,30,29),
                        form=factor(c("tree",  "tree",
                                      "liana", "liana",
                                      "palm",  "palm")))

lifeform$biomass <-
ifelse(lifeform$form=="tree", 
exp(-4.898+4.512*log(lifeform$dbh)-0.319*(log(lifeform$dbh))^2),
ifelse(lifeform$form=="liana", 10^(0.07 + 2.17 * log10 (lifeform$dbh)),
NA))

 > lifeform$biomass
[1] 860.8883 794.8866 418.9074 360.6599       NA       NA

See ?ifelse

Paulo Brando wrote:
> Dear Rs,
> 
> I have tried to use conditional expressions to calculate biomass for
> different life forms (trees, lianas, and palms).
> 
> Here is an example:
> 
> 
>>lifeform
> 
> 
>     dbh  form
> 1   30  tree
> 2   29  tree
> 3   28  tree
> 4   27  tree
> 5   26  tree
> 6   25  tree
> 7   24  tree
> 8   23  tree
> 9   22  tree
> 10  21  tree
> 11  20  tree
> 12  15 liana
> 13  14 liana
> 14  13 liana
> 15  12 liana
> 16  11 liana
> 17  10 liana
> 18   9 liana
> 19   8 liana
> 20   7 liana
> 21   6 liana
> 22   5 liana
> 23  30  palm
> 24  29  palm
> 25  28  palm
> 26  27  palm
> 27  26  palm
> 28  25  palm
> 29  24  palm
> 30  23  palm
> 31  22  palm
> 32  21  palm
> 33  20  palm
> 
> ### I want to include biomass 
> 
> lifeform$biomass <- 
> 
> {
>       if(lifeform$form=="tree")
>        exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2) 
>        else{ 
>         if (lifeform$form=="liana")
>        10^(0.07 + 2.17 * log10 (dbh))
>        else ("NA")
> }
> Warning message:
> the condition has length > 1 and only the first element will be used in:
> if (lifeform$form == "tree") exp(-4.898 + 4.512 * log(dbh) -
> 
> 
> ### But I always get the message warning message above. 
> 
> 
> 
> I looked for similar examples in R mail list archive, but they did not
> help a lot.
> 
> I am quite new to 'R'. Any material that covers this theme?
> 
> Thank you very much!
> 
> Paulo
> 
> PS. Sorry about the last e-mail. I did not change the message title.
> ________________________________________
> Paulo M. Brando
> Instituto de Pesquisa Ambiental da Amazonia (IPAM)
> Santarem, PA, Brasil.
> Av. Rui Barbosa, 136.
> Fone: + 55 93 522 55 38
> www.ipam.org.br
> E-mail: pmbrando at ipam.org.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From samuel_mwalili at yahoo.com  Mon Jun 13 21:51:16 2005
From: samuel_mwalili at yahoo.com (Mwalili, S. M.)
Date: Mon, 13 Jun 2005 12:51:16 -0700 (PDT)
Subject: [R] Learning, "if" and "else"
In-Reply-To: <005201c57067$d84610a0$f10aa8c0@paulobrando>
Message-ID: <20050613195116.75648.qmail@web53406.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/5856d133/attachment.pl

From stefan.sobernig at wu-wien.ac.at  Mon Jun 13 21:01:05 2005
From: stefan.sobernig at wu-wien.ac.at (Stefan Sobernig)
Date: Mon, 13 Jun 2005 21:01:05 +0200
Subject: [R] linking R to goto blas
In-Reply-To: <x2vf4jzi85.fsf@turmalin.kubism.ku.dk>
References: <42AC4160.90505@wu-wien.ac.at>	<x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>
	<x2vf4jzi85.fsf@turmalin.kubism.ku.dk>
Message-ID: <42ADD7F1.2030905@wu-wien.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050613/2053d061/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 13 23:03:52 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Jun 2005 22:03:52 +0100 (BST)
Subject: [R] linking R to goto blas
In-Reply-To: <42ADD7F1.2030905@wu-wien.ac.at>
References: <42AC4160.90505@wu-wien.ac.at>
	<x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>
	<x2vf4jzi85.fsf@turmalin.kubism.ku.dk> <42ADD7F1.2030905@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0506132202360.5304@gannet.stats>

R already contains a xerbla: you should not be adding one.

On Mon, 13 Jun 2005, Stefan Sobernig wrote:

> Prof. Ritley, Mr. Dalgaard, thank you very much for the immediate reply
> and your efforts!
> We investigated a littlle further, though not directly following Mr.
> Dalgaard's findings
> on dynamic linking. We got aware of another short notice published on
> the R-devel list,
> providing some hints for installing R and goto (see
> http://maths.newcastle.edu.au/~rking/R/devel/03b/1267.html):
>
>
> ############ snip ###################################
> [What I did to link R against Goto's BLAS (mostly following Prof. Bates'
> instruction):
> - Install libgoto*.so and symlink to libgoto.so.
> - Download xerbla.f and compile to xerbla.o.
> - Run the R configure script with --with-blas="-lgoto /path/to/xerbla.o".
> - Edit Makeconf and delete the path to xerbla.o in BLAS_LIB.
> - make; make check
> ]
> ############ snip ###################################
>
> * Therefore, we got the auxiliary lapack routine xerbla offered at
> http://www.cs.utexas.edu/users/kgoto/libraries/xerbla.c
> and compiled it by calling: "gcc -c xerbla.c -o xerbla.o" (please note,
> that I am not that expierenced in C programming as such,
> my knowledge is limited to deploying and fixing c-coded apps). Am I
> right to say that this xerbla extension changes the entry
> point of the goto library?
>
> * Then, we re-configured R using: ./configure --prefix=/usr
> --with-blas="-L/usr/lib/ -lgoto_prescott-32-r0.99-3
> /home/ssoberni/xerbla.o -lpthread"
>
> * Unfortunately, this resulted in another error affecting the configure
> process so that BLAS_LIBS is set to "-lblas" (pointing to standard blas
> libs):
>
> ########### config.log ################################
> configure:32429: gcc -o conftest -g -O2  -I/usr/local/include
> -L/usr/local/lib conftest.c -L/usr/lib/ -lgoto_prescott-32-r0.99-3
> /home/ssoberni/xerbla.o -lpthread  -lg2c -lm -lgcc_s -ldl -lm  >&5
> /home/ssoberni/xerbla.o(.text+0x0): In function `xerbla_':
> : multiple definition of `xerbla_'
> /tmp/ccMxAKbu.o(.text+0x0):/home/ssoberni/R-2.1.0/conftest.c:143: first
> defined here
> /usr/bin/ld: Warning: size of symbol `xerbla_' changed from 5 in
> /tmp/ccMxAKbu.o to 43 in /home/ssoberni/xerbla.o
> collect2: ld returned 1 exit status
> ########### config.log ################################
>
> We keep on trying ...
> Thanks for your thoughts ...
>
> //stefan
>
>
> Peter Dalgaard wrote:
>
>> Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:
>>
>>
>>
>>> Stefan Sobernig <stefan.sobernig at wu-wien.ac.at> writes:
>>>
>>>
>>>
>>>> Dear all,
>>>>
>>>> I am currently trying to link R 2.1.0 to the GOTO BLAS 0.99.3 library on
>>>> a box running Fedora Core 3 , basically following the steps indicated in
>>>> the R-Admin document:
>>>>
>>>> 1: I downloaded the current libgoto.xxx.so from
>>>> http://www.cs.utexas.edu/users/kgoto/libraries/libgoto_prescott-32-r0.99-3.so.gz,
>>>> a version suitable for our XEON machine (Nocona core), unpacked it to
>>>> /usr/lib and created a symlink libgoto.so pointing to the library.
>>>>
>>>> 2: Then, I got ready to re-configure and re-compile R (2.1.0) using the
>>>> following configure flags: ./configure --prefix=/usr --enable-R-shlib
>>>> --enable-shared --with-tcltk --with-blas="-lgoto -lpthread -lm"
>>>>
>>>>
>>> ...
>>>
>>>
>>>> Please, I highly appreciate any thoughts or hints as my colleagues and I
>>>> are eager to get into GOTO's universe.
>>>>
>>>>
>>> Hmm. Looks over-complicated to me. What works for me on AMD64 is to
>>> have a config.site file in my BUILD-GOTO directory, containing
>>>
>>>
>>>
>>>> cat config.site
>>>>
>>>>
>>> BLAS_LIBS="-L/home/pd/GOTO -lgoto_opt64p-r0.96 -lpthread"
>>> CFLAGS="-O3 -g"
>>> #CFLAGS="-g"
>>> FFLAGS=$CFLAGS
>>> CXXFLAGS=$CFLAGS
>>>
>>> (the .*FLAGS business is optional, of course). With this in place, a
>>> simple ../R/configure followed by make seems to do the trick.
>>>
>>> I'll give it a try on my FC3 system, but it's a 500 MHz PIII, so it
>>> takes a while...
>>>
>>>
>>
>> Hmm... That gives me the grDevices issue, which boils down to an R
>> that segfaults immediately upon startup, in
>>
>> #0  0x05c0aea7 in tilde_expand () from /usr/lib/libreadline.so.4
>> #1  0x08170254 in R_ExpandFileName_readline (
>>    s=0x8bb4020
>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb",
>> #buff=0x8295300
>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/grDevices")
>>    at ../../../R/src/unix/sys-std.c:406
>> #2  0x0816f5da in R_ExpandFileName (
>>    s=0x8bb4020
>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb") at
>> #../../../R/src/unix/sys-unix.c:129
>> #3  0x08167352 in R_FileExists (
>>    path=0x8bb4020
>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb") at
>> #stat.h:365
>> #4  0x08105da3 in do_fileexists (call=0x84fd544, op=0x82c5f78,
>> #args=0x0,
>>    rho=0x8c514a4) at ../../../R/src/main/platform.c:857
>> #5  0x080edc85 in do_internal (call=0x0, op=0x82ba5d4, args=0x3920,
>>    env=0x8c514a4) at ../../../R/src/main/names.c:1078
>> #6  0x080c0daa in Rf_eval (e=0x84fd57c, rho=0x8c514a4)
>>    at ../../../R/src/main/eval.c:382
>> #7  0x080c3695 in Rf_applyClosure (call=0x8668c28, op=0x84fd5b4,
>>    arglist=0x8c50564, rho=0x8ad5cc4, suppliedenv=0x82aa5f0)
>>
>> running --no-readline gives me another crash
>>
>> (gdb) bt
>> #0  0x003fb0da in strcmp () from /lib/ld-linux.so.2
>> #1  0x003f009a in _dl_map_object () from /lib/ld-linux.so.2
>> #2  0x004fdb58 in dl_open_worker () from /lib/tls/libc.so.6
>> #3  0x00000000 in ?? ()
>>
>> ...which suggests that something is up with dynamic linking.
>>
>> I'll give it another spin...
>>
>>
>>
>
>
>
> -- 
>
> Stefan Sobernig
> Department of Information Systems and New Media
> Vienna University of Economics
> Augasse 2-6
> A - 1090 Vienna
>
> Phone: +43 - 1 - 31336 - 4878
> Fax: +43 - 1 - 31336 - 746
> Email: stefan.sobernig at wu-wien.ac.at <mailto:stefan.sobernig at wu-wien.ac.at>
> PubKey: http://julia.wu-wien.ac.at/~ssoberni/0x5FC2D3FA.asc <http://julia.wu-wien.ac.at/%7Essoberni/0x5FC2D3FA.asc>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mzarkov at EUnet.yu  Mon Jun 13 23:24:53 2005
From: mzarkov at EUnet.yu (Milos Zarkovic)
Date: Mon, 13 Jun 2005 23:24:53 +0200
Subject: [R] Preparing timestamped data for fourier analysis
References: <20050613171631.29376.qmail@web52406.mail.yahoo.com>
Message-ID: <001401c5705e$6a548d50$0201a8c0@milos>

I believe that FFT is not appropriate. However Lomb-Scargle periodogram 
could be used.

Sincerely

Milos Zarkovic

******************************************************
Milos Zarkovic MD, Ph.D.
Associate Professor of Internal Medicine
Institute of Endocrinology
Dr Subotica 13
11000 Beograd
Serbia

Tel +381-63-202-925
Fax +381-11-685-357

Email mzarkov at eunet.yu
******************************************************


----- Original Message ----- 
From: "Pete Cap" <peteoutside at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 13, 2005 7:16 PM
Subject: [R] Preparing timestamped data for fourier analysis


> Greetings all,
>
> I'm working on a project trying to apply fourier analysis to timestamped 
> router logs, using R to perform the analysis.  The idea is to determine if 
> any type of traffic (say, outgoing ICMP requests) has strong periodic 
> features because it may indicate a compromise somewhere on the network.
>
> The FFT requires all data points to be evenly spaced, but the recorded 
> events do not occur at a consistent constant dt, so I need to "zero pad" 
> the data.  I can do this for small data sets (say, one day's worth of 
> traffic) in excel.
>
> However, I am now attempting it on a large scale using a 25-day router 
> log, which should have 2,160,000 records.  The log actually contains only 
> 56,725 records.
>
> So, I'm looking for ways in which to pad the data in R.  If anyone could 
> just point me at the right man pages to read that would be itself a great 
> help.
>
> Thanks in advance,
>
> Pete
>
>
> ---------------------------------
>
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kerryrekky at yahoo.com  Tue Jun 14 00:51:01 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Mon, 13 Jun 2005 15:51:01 -0700 (PDT)
Subject: [R] use of gam
In-Reply-To: <005201c57067$d84610a0$f10aa8c0@paulobrando>
Message-ID: <20050613225101.85868.qmail@web51806.mail.yahoo.com>


Suppose I fit the following model:

>library(gam)
....
>fit <- gam(y~x1+x2+s(x3),family=binomial)

and then I use

> fitf$coef
       x1        x2     s(x3) 
4.1947460 2.7967200 0.0788252 

are the coefficients for x1 and x2 the estimated
coefficients? what is the meaning of s(x3)? since this
is a non-parametric component, it may not belong here
as a coefficient, am I right?



From gerifalte28 at hotmail.com  Tue Jun 14 01:17:04 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Mon, 13 Jun 2005 23:17:04 +0000
Subject: [R] us zipcode data map
In-Reply-To: <27db823f050613001741d19b46@mail.gmail.com>
Message-ID: <BAY103-F63B484B05E08B4C56B8BBA6F00@phx.gbl>

The first hit in RSiteSearch("html") guides you to the package R2HTML. Take 
a look at HTMLInsertGraph {R2HTML}.  Try RSiteSearch("pdf") for pdf devices. 
  Hint: savePlot("MyPlot","pdf")

Cheers

Francisco


>From: Mike R <mike.rstat at gmail.com>
>Reply-To: r-help at stat.math.ethz.ch
>To: "Francisco J. Zagmutt" <gerifalte28 at hotmail.com>
>Subject: Re: [R] us zipcode data map
>Date: Mon, 13 Jun 2005 00:17:53 -0700
>
>On 6/12/05, Francisco J. Zagmutt <gerifalte28 at hotmail.com> wrote:
> > Not that I am aware of.  Try library(help="maps") for a list of all the
> > functions in the library.  Anyhow, I am not sure that a US map with 
>zipcodes
> > will look very good/readable, unless you focus on a very small area 
>(i.e.
> > county).
>
>Hi Francisco - the map dimensions are about 150x50 miles.
>
>But what i've learned is that a zipcode region has area=0 because
>a zipcode region is a group of street addresses.  instead of a region,
>it is more like a tree.
>
>My current plan of action is to use readily available government
>ascii  tables.  From those tables, I can obtain a single latitude and
>longitude for each zipcode, and a government "estimate" of  the
>"land area" in square miles.
>
>From there, it should be easy to display the data on one of the maps in
>the package that you recommended (thanks).  If not, there are other
>options for the "base map".
>
>My new task (which I have not investigated yet) is to see if facilities
>exist in R to turn the X11 device into an HTML document with an
>html-map .... or into a PDF with hyperlinks.
>
>Cheers,
>Mike



From macq at llnl.gov  Tue Jun 14 01:54:09 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 13 Jun 2005 16:54:09 -0700
Subject: [R] Learning, "if" and "else"
In-Reply-To: <005201c57067$d84610a0$f10aa8c0@paulobrando>
References: <005201c57067$d84610a0$f10aa8c0@paulobrando>
Message-ID: <p0621020fbed3c9542086@[128.115.153.6]>

There are lots of ways to do this, and lots of sources of 
information. Go to the R website and download the "Introduction to R" 
document from the "Manuals" link.

Here is one simple way. Not the most succinct in terms of coding 
style, perhaps, but relatively easy to understand, and relatively 
good for beginning to understand the language, in my opinion.

Starting with your lifeform object as shown, with the two variables, 
dbh and form.

lifeform$biomass <- rep(NA,nrow(lifeform))
is.tree <- lifeform$form=='tree'
lifeform$biomass[is.tree] <- 
exp(-4.898+4.512*log(lifeform$dbh)-0.319*(log(lifeform$dbh))^2)[is.tree]
is.liana <- lifeform$form=='liana'
lifeform$biomass[is.liana] <- 
exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2)[is.liana]

To better understand what is going on, type just this:

    lifeform$form=="tree"

You should see a vector of TRUE and FALSE values, 33 of them. But the 
rules for the "if()" function say that whatever you supply inside the 
parentheses should reduce to a *single* true or false value. You gave 
it 33 of them. See the help page for "if", which you can get to using 
either

   help('if')
or
   ?'if'

The ifelse() function takes a logical vector for its first argument.

At 3:32 PM -0700 6/13/05, Paulo Brando wrote:
>Dear Rs,
>
>I have tried to use conditional expressions to calculate biomass for
>different life forms (trees, lianas, and palms).
>
>Here is an example:
>
>>  lifeform
>
>     dbh  form
>1   30  tree
>2   29  tree
>3   28  tree
>4   27  tree
>5   26  tree
>6   25  tree
>7   24  tree
>8   23  tree
>9   22  tree
>10  21  tree
>11  20  tree
>12  15 liana
>13  14 liana
>14  13 liana
>15  12 liana
>16  11 liana
>17  10 liana
>18   9 liana
>19   8 liana
>20   7 liana
>21   6 liana
>22   5 liana
>23  30  palm
>24  29  palm
>25  28  palm
>26  27  palm
>27  26  palm
>28  25  palm
>29  24  palm
>30  23  palm
>31  22  palm
>32  21  palm
>33  20  palm
>
>### I want to include biomass
>
>lifeform$biomass <-
>
>{
>       if(lifeform$form=="tree")
>        exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2)
>        else{
>         if (lifeform$form=="liana")
>        10^(0.07 + 2.17 * log10 (dbh))
>        else ("NA")
>}
>Warning message:
>the condition has length > 1 and only the first element will be used in:
>if (lifeform$form == "tree") exp(-4.898 + 4.512 * log(dbh) -
>
>
>### But I always get the message warning message above.
>
>
>
>I looked for similar examples in R mail list archive, but they did not
>help a lot.
>
>I am quite new to 'R'. Any material that covers this theme?
>
>Thank you very much!
>
>Paulo
>
>PS. Sorry about the last e-mail. I did not change the message title.
>________________________________________
>Paulo M. Brando
>Instituto de Pesquisa Ambiental da Amazonia (IPAM)
>Santarem, PA, Brasil.
>Av. Rui Barbosa, 136.
>Fone: + 55 93 522 55 38
>www.ipam.org.br
>E-mail: pmbrando at ipam.org.br
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Benjamin.Osborne at uvm.edu  Tue Jun 14 03:42:53 2005
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Mon, 13 Jun 2005 21:42:53 -0400
Subject: [R] superscript in figures - basic question
Message-ID: <1118713373.42ae361d73757@webmail.uvm.edu>

Although I see similar, but more complex, questions addressed in the help
archive, I'm having trouble adding superscripted text to the y-axis labels of
some figures, and I can't find anything in the R documentation on this.

I have:

ylab="BA (m2/ha)"

but I want the "2" to be superscripted.
Thanks in advence for the help, or for pointing out the appropriate help file.

-Ben Osborne



From chencheva at gmail.com  Tue Jun 14 03:46:52 2005
From: chencheva at gmail.com (Hu Chen)
Date: Tue, 14 Jun 2005 09:46:52 +0800
Subject: [R] question about SSOAP
Message-ID: <6f3fc9ee05061318467da3e951@mail.gmail.com>

Dear R folks:

I am trying to use SSOAP (version 0.2-2) package in R (version
2.1.0,linux) to access SOAP service on NCBI
(http://www.ncbi.nlm.nih.gov)

its WSDL file is at http://www.ncbi.nlm.nih.gov/entrez/eutils/soap/eutils.wsdl

but some errors occured:

> ncbi <- processWSDL("http://www.ncbi.nlm.nih.gov/entrez/eutils/soap/eutils.wsdl")

> ff <- genSOAPClientInterface(ncbi at operations[[1]], def = ncbi, ncbi at name, verbose=FALSE)

> ff at functions$run_eInfo("db=pubmed")
Error in toSOAPArray(obj, con, type = type) :
       no slot of name "elType" for this object of class "SOAPVoidType"

what's wrong? I am looking foward to your help.

Thanks very much.

Tiger



From ccleland at optonline.net  Tue Jun 14 03:51:15 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 13 Jun 2005 21:51:15 -0400
Subject: [R] superscript in figures - basic question
In-Reply-To: <1118713373.42ae361d73757@webmail.uvm.edu>
References: <1118713373.42ae361d73757@webmail.uvm.edu>
Message-ID: <42AE3813.6000400@optonline.net>

See ?plotmath.

plot(1:10, 1:10, ylab=expression(BA (m^{2}/ha)))

Benjamin M. Osborne wrote:
> Although I see similar, but more complex, questions addressed in the help
> archive, I'm having trouble adding superscripted text to the y-axis labels of
> some figures, and I can't find anything in the R documentation on this.
> 
> I have:
> 
> ylab="BA (m2/ha)"
> 
> but I want the "2" to be superscripted.
> Thanks in advence for the help, or for pointing out the appropriate help file.
> 
> -Ben Osborne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From d.scott at auckland.ac.nz  Tue Jun 14 04:00:57 2005
From: d.scott at auckland.ac.nz (David Scott)
Date: Tue, 14 Jun 2005 14:00:57 +1200 (NZST)
Subject: [R] superscript in figures - basic question
In-Reply-To: <1118713373.42ae361d73757@webmail.uvm.edu>
References: <1118713373.42ae361d73757@webmail.uvm.edu>
Message-ID: <Pine.LNX.4.61.0506141400310.3745@stat12.stat.auckland.ac.nz>

On Mon, 13 Jun 2005, Benjamin M. Osborne wrote:

> Although I see similar, but more complex, questions addressed in the help
> archive, I'm having trouble adding superscripted text to the y-axis labels of
> some figures, and I can't find anything in the R documentation on this.
>
> I have:
>
> ylab="BA (m2/ha)"
>
> but I want the "2" to be superscripted.
> Thanks in advence for the help, or for pointing out the appropriate help file.
>


?plotmath


_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From PAlspach at hortresearch.co.nz  Tue Jun 14 04:11:47 2005
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Tue, 14 Jun 2005 14:11:47 +1200
Subject: [R] superscript in figures - basic question
Message-ID: <s2aee600.099@hra2.marc.hort.cri.nz>


Ben

Others have pointed out plotmath.  However, for some superscripts (including 2) it may be easier to use the appropriate escape sequence (at in Windows):

ylab = 'BA (m\262/ha)'

Cheers ........

Peter Alspach


>>> "Benjamin M. Osborne" <Benjamin.Osborne at uvm.edu> 14/06/05 13:42:53 >>>
Although I see similar, but more complex, questions addressed in the help
archive, I'm having trouble adding superscripted text to the y-axis labels of
some figures, and I can't find anything in the R documentation on this.

I have:

ylab="BA (m2/ha)"

but I want the "2" to be superscripted.
Thanks in advence for the help, or for pointing out the appropriate help file.

-Ben Osborne

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From bernd.weiss at uni-koeln.de  Tue Jun 14 04:51:27 2005
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Tue, 14 Jun 2005 04:51:27 +0200
Subject: [R] Lattice: Combining xyplot and histogram
In-Reply-To: <20050613184002.3417.qmail@web53405.mail.yahoo.com>
References: <42ADB167.7750.EA1E53@localhost>
Message-ID: <42AE624F.24264.E5F90@localhost>

Am 13 Jun 2005 um 11:40 hat Mwalili, S. M. geschrieben:

> 
> See ?layout

[...]

Thanks to Roy, Gabor and Mwalili for their helpful suggestions. As 
Gabor wrote, I have to make myself familiar with the grid package and 
viewports.

Bernd



From chris at subtlety.com  Tue Jun 14 06:03:00 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Mon, 13 Jun 2005 23:03:00 -0500
Subject: [R] RGui crashes on wle call
Message-ID: <42AE56F4.3080404@subtlety.com>

Hi all --

    I'm seeing the following commands reliably produce a crash in RGui, 
version 2.0.1, for both my home and office machine:

 > rm(list = ls(all = TRUE));
 > load("dataset.R");
 > library("wle");
 > data.wle = wle.lm(abortion ~ year * lib.con + age + gender +
+ urbanism + census + income + church.att + children + educ +
+ religion.imp, data = data.set);

    dataset.R is moderately sized (about 200k compressed), and the 
command works just fine with "lm" rather than "wle.lm".  Since I'm not 
sure where the bug lies -- in wle, in R, or in RGui -- I'm not sure 
where I should report this, or if it's already been reported.
    What should I do?

-- Chris



From ripley at stats.ox.ac.uk  Tue Jun 14 08:23:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 07:23:03 +0100 (BST)
Subject: [R] RGui crashes on wle call
In-Reply-To: <42AE56F4.3080404@subtlety.com>
References: <42AE56F4.3080404@subtlety.com>
Message-ID: <Pine.LNX.4.61.0506140718530.11049@gannet.stats>

On Mon, 13 Jun 2005, Chris Bergstresser wrote:

> Hi all --
>
>    I'm seeing the following commands reliably produce a crash in RGui,
> version 2.0.1, for both my home and office machine:
>
> > rm(list = ls(all = TRUE));
> > load("dataset.R");
> > library("wle");
> > data.wle = wle.lm(abortion ~ year * lib.con + age + gender +
> + urbanism + census + income + church.att + children + educ +
> + religion.imp, data = data.set);
>
>    dataset.R is moderately sized (about 200k compressed), and the
> command works just fine with "lm" rather than "wle.lm".  Since I'm not
> sure where the bug lies -- in wle, in R, or in RGui -- I'm not sure
> where I should report this, or if it's already been reported.
>    What should I do?

There is good advice in the posting guide.  Following that

1) Re-do the tests in the current version of R, preferably a beta of 
2.1.1.

2) Read the rw-FAQ, do the debugging reported there (with Dr MinGW or gdb) 
and find where it is crashing.  (This is very likely to be in wle.)
If it is in wle, send a report to the maintainer.  If it is in R, send a 
report to R-bugs.  In either case, supply enough data to reproduce the 
problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chencheva at gmail.com  Tue Jun 14 09:04:51 2005
From: chencheva at gmail.com (Hu Chen)
Date: Tue, 14 Jun 2005 15:04:51 +0800
Subject: [R] protection stack overflow??
Message-ID: <6f3fc9ee05061400047c3da7c9@mail.gmail.com>

Hi dear Rers,
I am using SSOAP package to access SOAP service at NCBI.
I followed the example code in SSOAP but failed.

> z <- .SOAP("http://www.ncbi.nlm.nih.gov/entrez/eutils/soap/soap_adapter.cgi", method="run_eInfo", db="pubmed", action = I("einfo"))
Error: protect(): protection stack overflow

what's wrong? 
Thanks very much.
Regards



From maechler at stat.math.ethz.ch  Tue Jun 14 09:26:18 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 14 Jun 2005 09:26:18 +0200
Subject: [R] superscript in figures - basic question
In-Reply-To: <s2aee600.099@hra2.marc.hort.cri.nz>
References: <s2aee600.099@hra2.marc.hort.cri.nz>
Message-ID: <17070.34458.263554.25277@stat.math.ethz.ch>

>>>>> "Peter" == Peter Alspach <PAlspach at hortresearch.co.nz>
>>>>>     on Tue, 14 Jun 2005 14:11:47 +1200 writes:

    Peter> Ben

    Peter> Others have pointed out plotmath.  However, for some
    Peter> superscripts (including 2) it may be easier to use
    Peter> the appropriate escape sequence (at in Windows):

    Peter> ylab = 'BA (m\262/ha)'

but please refrain from doing that way.
You should write R code that is portable, and ``plotmath''
has been designed to be portable.  Windows escape codes are not,
and may not even work in future (or current?) versions of
Windows with `unusual' locale settings {fonts, etc}.

Martin Maechler

    Peter> Cheers ........

    Peter> Peter Alspach


    >>>> "Benjamin M. Osborne" <Benjamin.Osborne at uvm.edu>
    >>>> 14/06/05 13:42:53 >>>
    Peter> Although I see similar, but more complex, questions
    Peter> addressed in the help archive, I'm having trouble
    Peter> adding superscripted text to the y-axis labels of
    Peter> some figures, and I can't find anything in the R
    Peter> documentation on this.

    Peter> I have:

    Peter> ylab="BA (m2/ha)"

    Peter> but I want the "2" to be superscripted.  Thanks in
    Peter> advence for the help, or for pointing out the
    Peter> appropriate help file.

    Peter> -Ben Osborne

    Peter> ______________________________________________
    Peter> R-help at stat.math.ethz.ch mailing list
    Peter> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Peter> do read the posting guide!
    Peter> http://www.R-project.org/posting-guide.html

    Peter> ______________________________________________________

    Peter> The contents of this e-mail are privileged and/or
    Peter> confidenti...{{dropped}}

    Peter> ______________________________________________
    Peter> R-help at stat.math.ethz.ch mailing list
    Peter> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Peter> do read the posting guide!
    Peter> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Jun 14 09:31:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2005 09:31:55 +0200
Subject: [R] superscript in figures - basic question
In-Reply-To: <42AE3813.6000400@optonline.net>
References: <1118713373.42ae361d73757@webmail.uvm.edu>
	<42AE3813.6000400@optonline.net>
Message-ID: <x2wtoxwg0k.fsf@turmalin.kubism.ku.dk>

Chuck Cleland <ccleland at optonline.net> writes:

> See ?plotmath.
> 
> plot(1:10, 1:10, ylab=expression(BA (m^{2}/ha)))

Ick... The parser will think that BA is a function and m^2/ha its
argument. This probably has consequences for the spacing.

Try expression("BA" * ("m"^2/"ha"))

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From njiabatih at yahoo.com  Tue Jun 14 09:40:57 2005
From: njiabatih at yahoo.com (Abatih E.)
Date: Tue, 14 Jun 2005 00:40:57 -0700 (PDT)
Subject: [R] New Family object for GLM models...
Message-ID: <20050614074057.3219.qmail@web53009.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050614/d5431c00/attachment.pl

From ripley at stats.ox.ac.uk  Tue Jun 14 10:22:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 09:22:49 +0100 (BST)
Subject: [R] protection stack overflow??
In-Reply-To: <6f3fc9ee05061400047c3da7c9@mail.gmail.com>
References: <6f3fc9ee05061400047c3da7c9@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506140920040.21307@gannet.stats>

On Tue, 14 Jun 2005, Hu Chen wrote:

> Hi dear Rers,
> I am using SSOAP package to access SOAP service at NCBI.
> I followed the example code in SSOAP but failed.
>
>> z <- .SOAP("http://www.ncbi.nlm.nih.gov/entrez/eutils/soap/soap_adapter.cgi", method="run_eInfo", db="pubmed", action = I("einfo"))
> Error: protect(): protection stack overflow
>
> what's wrong?

Your code is overflowing the protection stack.  Most likely due to heavy 
recursion: see ?Startup as to how to increase the stack size.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bry at xdocs.dk  Tue Jun 14 10:42:58 2005
From: bry at xdocs.dk (bry@xdocs.dk)
Date: Tue, 14 Jun 2005 10:42:58 +0200
Subject: [R] why does the unsubscribe not work
In-Reply-To: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
References: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
Message-ID: <1118738578.42ae9892a6100@horde.scannet.dk>


Hi I have tried to unsubscribe from this list now three times, I desperately
need to unsubscribe from this list from this address because the list is choking
this mailbox. 
Below are the various suggested ways of unsubscribing

> To subscribe or unsubscribe via the World Wide Web, visit
> 	https://stat.ethz.ch/mailman/listinfo/r-help
> or, via email, send a message with subject or body 'help' to
> 	r-help-request at stat.math.ethz.ch

I did the second the first time, I received an email suggesting that I send an
email to that address with unsubscribe in the subject nothing in the body, which
I did, I received a confirmation request from that address and I confirmed,
after confirmation I received a response that I had been unsubscribed.  I
continued to receive email from this list. 
So I went ahead and tried to unsubscribe via the https address, unfortunately it
said that it did not recognize my password, as I write down all my passwords I
doubt I've forgotten it and I am assuming that somewhere in the system I have
been unsubscribed but unfortunately not through the whole system. Anyway I
attempted again via the r-help-request, got the same response and of course the
end result is I am still receiving the emails that are choking my system. 
Yesterday in order to deal with this problem I tried to contact 
r-help-owner at stat.math.ethz.ch
and I have not received a response from him. This might very well be because
this mailbox is getting so damned choken by mail from this list that a number of
emails from other lists etc. are being refused. I am cc'ing him on this email. I
would like someone to get me unsubscribed from this list as I don't think it
very fair that I should have to abandon a mailbox that I have set to receiving
mails from several interesting lists and that I have been using for more than a
year now.



From james at bovik.org  Tue Jun 14 11:03:21 2005
From: james at bovik.org (James Salsman)
Date: Tue, 14 Jun 2005 02:03:21 -0700
Subject: [R] ordinary polynomial coefficients from orthogonal polynomials?
Message-ID: <42AE9D59.6090006@bovik.org>

How can ordinary polynomial coefficients be calculated
from an orthogonal polynomial fit?

I'm trying to do something like find a,b,c,d from
  lm(billions ~ a+b*decade+c*decade^2+d*decade^3)
but that gives:  "Error in eval(expr, envir, enclos) :
Object "a" not found"

 > decade <- c(1950, 1960, 1970, 1980, 1990)
 > billions <- c(3.5, 5, 7.5, 13, 40)
 > # source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
 >
 > pm <- lm(billions ~ poly(decade, 3))
 >
 > plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000), 
main="average yearly inflation-adjusted dollar cost of extreme weather 
events worldwide")
 > curve(predict(pm, data.frame(decade=x)), add=TRUE)
 > # output: http://www.bovik.org/storms.gif
 >
 > summary(pm)

Call:
lm(formula = billions ~ poly(decade, 3))

Residuals:
       1       2       3       4       5
  0.2357 -0.9429  1.4143 -0.9429  0.2357

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)
(Intercept)        13.800      0.882  15.647   0.0406 *
poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
poly(decade, 3)3    6.483      1.972   3.287   0.1880
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 1.972 on 1 degrees of freedom
Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317

 > pm

Call:
lm(formula = billions ~ poly(decade, 3))

Coefficients:
      (Intercept)  poly(decade, 3)1  poly(decade, 3)2  poly(decade, 3)3
           13.800            25.614            14.432             6.483



From p.dalgaard at biostat.ku.dk  Tue Jun 14 12:06:53 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2005 12:06:53 +0200
Subject: [R] why does the unsubscribe not work
In-Reply-To: <1118738578.42ae9892a6100@horde.scannet.dk>
References: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
	<1118738578.42ae9892a6100@horde.scannet.dk>
Message-ID: <x2u0k19rr6.fsf@turmalin.kubism.ku.dk>

bry at xdocs.dk writes:

> Hi I have tried to unsubscribe from this list now three times, I desperately
> need to unsubscribe from this list from this address because the list is choking
> this mailbox. 
> Below are the various suggested ways of unsubscribing
> 
> > To subscribe or unsubscribe via the World Wide Web, visit
> > 	https://stat.ethz.ch/mailman/listinfo/r-help
> > or, via email, send a message with subject or body 'help' to
> > 	r-help-request at stat.math.ethz.ch
> 
> I did the second the first time, I received an email suggesting that I send an
> email to that address with unsubscribe in the subject nothing in the body, which
> I did, I received a confirmation request from that address and I confirmed,
> after confirmation I received a response that I had been unsubscribed.  I
> continued to receive email from this list. 
> So I went ahead and tried to unsubscribe via the https address, unfortunately it
> said that it did not recognize my password, as I write down all my passwords I
> doubt I've forgotten it and I am assuming that somewhere in the system I have
> been unsubscribed but unfortunately not through the whole system. Anyway I
> attempted again via the r-help-request, got the same response and of course the
> end result is I am still receiving the emails that are choking my system. 
> Yesterday in order to deal with this problem I tried to contact 
> r-help-owner at stat.math.ethz.ch
> and I have not received a response from him. This might very well be because
> this mailbox is getting so damned choken by mail from this list that a number of
> emails from other lists etc. are being refused. I am cc'ing him on this email. I
> would like someone to get me unsubscribed from this list as I don't think it
> very fair that I should have to abandon a mailbox that I have set to receiving
> mails from several interesting lists and that I have been using for more than a
> year now.

What are the timeframes involved? There is always some time delay. In
particular, there is no way of stopping mails that have already been
sent by the list handling software. And if your mailbox is clogged,
things may stay in the queue for up to five days before the relevant
mail relay gives up.

If your password has stopped working, it suggests to me that you have
in fact already been unsubscribed.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From eddy_lolo at hotmail.com  Tue Jun 14 12:15:53 2005
From: eddy_lolo at hotmail.com (Eddy G.)
Date: Tue, 14 Jun 2005 12:15:53 +0200
Subject: [R] bs() function of the splines package
Message-ID: <BAY103-F23BFFCDECBCD67B75A16D78CF30@phx.gbl>


Laurent 	  14 juin 12:02     afficher les options
De : "Laurent" <eddy_l... at hotmail.com> - Rechercher les messages de cet 
auteur
Date : Tue, 14 Jun 2005 03:02:37 -0700
Local : Mar 14 juin 2005 12:02
Objet : bs() function of the splines package
R??pondre | R??pondre ?? l'auteur | Transf??rer | Imprimer | Message individuel 
| Afficher l'original | Retirer | Signaler un cas d'utilisation abusive

Hello,

I'm implementing a function using non uniform B-Splines. Looking at the
code of the bs() function, I realized that if the intercept was set to
TRUE, the behavior of the function was the following (df is the number
of degrees of freedom that I believe can be interpreted as the number
of control points):

- Compute df- ord + 1 _internal_ knots (ord is the order of the spline)
- Add ord times the boundaries values at each extrem of the knots
vector.
- Compute the design matrix on this vector.

This results in a design matrix with df+1 columns. The bs function then
_removes_ the first column of the matrix (resulting as expected in a
matrix with df columns).

I'm a bit confused, does anyone see a mathematical justification to
this manipulation?

In this case, is it licit tu use df- ord + 2 internal knots and remove
the last columns too?

Thanks a lot,

Laurent



From ramasamy at cancer.org.uk  Tue Jun 14 12:17:20 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 14 Jun 2005 11:17:20 +0100
Subject: [R] Learning, "if" and "else"
In-Reply-To: <005201c57067$d84610a0$f10aa8c0@paulobrando>
References: <005201c57067$d84610a0$f10aa8c0@paulobrando>
Message-ID: <1118744240.6041.22.camel@dhcp-63.ccc.ox.ac.uk>

You can also use switch() instead of ifelse(), which makes the code a
bit easier to read. The downside to this is that switch does not take
vectorised input and thus you need a loop. For example

 # data
 dbh  <- c(30,29,15,14,30,29)
 form <- factor(c("tree", "tree", "liana", "liana", "palm", "palm")) 
 df   <- data.frame(dbh, form)
 
 out <- numeric( nrow(df) )
 for(i in 1:nrow(df)){
 
   x <- as.numeric( df[i, 1] )
   y <- as.character( df[i, 2] )

   out[i] <- switch( y,
              "tree"  = exp( -4.898 + 4.512*log(x) - 0.319*(log(x))^2 ),
              "liana" = 10^(0.07 + 2.17 * log10(x)),
              NA
                   )
 }


Or slightly more efficient solution is

 out <- apply( df, 1, function(z){
   x <- as.numeric(z[1]); y <- as.character(z[2])
   
   switch( y,
          "tree"  = exp( -4.898 + 4.512*log(x) - 0.319*(log(x))^2 ),
          "liana" = 10^(0.07 + 2.17 * log10(x)),
          NA
          )
 })

Regards, Adai



On Mon, 2005-06-13 at 15:32 -0700, Paulo Brando wrote:
> Dear Rs,
> 
> I have tried to use conditional expressions to calculate biomass for
> different life forms (trees, lianas, and palms).
> 
> Here is an example:
> 
> > lifeform
> 
>     dbh  form
> 1   30  tree
> 2   29  tree
> 3   28  tree
> 4   27  tree
> 5   26  tree
> 6   25  tree
> 7   24  tree
> 8   23  tree
> 9   22  tree
> 10  21  tree
> 11  20  tree
> 12  15 liana
> 13  14 liana
> 14  13 liana
> 15  12 liana
> 16  11 liana
> 17  10 liana
> 18   9 liana
> 19   8 liana
> 20   7 liana
> 21   6 liana
> 22   5 liana
> 23  30  palm
> 24  29  palm
> 25  28  palm
> 26  27  palm
> 27  26  palm
> 28  25  palm
> 29  24  palm
> 30  23  palm
> 31  22  palm
> 32  21  palm
> 33  20  palm
> 
> ### I want to include biomass 
> 
> lifeform$biomass <- 
> 
> {
>       if(lifeform$form=="tree")
>        exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2) 
>        else{ 
>         if (lifeform$form=="liana")
>        10^(0.07 + 2.17 * log10 (dbh))
>        else ("NA")
> }
> Warning message:
> the condition has length > 1 and only the first element will be used in:
> if (lifeform$form == "tree") exp(-4.898 + 4.512 * log(dbh) -
> 
> 
> ### But I always get the message warning message above. 
> 
> 
> 
> I looked for similar examples in R mail list archive, but they did not
> help a lot.
> 
> I am quite new to 'R'. Any material that covers this theme?
> 
> Thank you very much!
> 
> Paulo
> 
> PS. Sorry about the last e-mail. I did not change the message title.
> ________________________________________
> Paulo M. Brando
> Instituto de Pesquisa Ambiental da Amazonia (IPAM)
> Santarem, PA, Brasil.
> Av. Rui Barbosa, 136.
> Fone: + 55 93 522 55 38
> www.ipam.org.br
> E-mail: pmbrando at ipam.org.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Jun 14 12:35:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 11:35:17 +0100 (BST)
Subject: [R] ordinary polynomial coefficients from orthogonal
	polynomials?
In-Reply-To: <42AE9D59.6090006@bovik.org>
References: <42AE9D59.6090006@bovik.org>
Message-ID: <Pine.LNX.4.61.0506141126160.26371@gannet.stats>

On Tue, 14 Jun 2005, James Salsman wrote:

> How can ordinary polynomial coefficients be calculated
> from an orthogonal polynomial fit?

Why would you want to do that?  predict() is perfectly happy with an
orthogonal polynomial fit and the `ordinary polynomial coefficients' are 
rather badly determined in your example since the design matrix has a very 
high condition number.

> I'm trying to do something like find a,b,c,d from
>  lm(billions ~ a+b*decade+c*decade^2+d*decade^3)
> but that gives:  "Error in eval(expr, envir, enclos) :
> Object "a" not found"

You could use

lm(billions ~ decade + I(decade^2) + I(decade^3))

except that will be numerically inaccurate, since

> m <- model.matrix(~ decade + I(decade^2) + I(decade^3))
> kappa(m)
[1] 3.506454e+16



> > decade <- c(1950, 1960, 1970, 1980, 1990)
> > billions <- c(3.5, 5, 7.5, 13, 40)
> > # source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
> >
> > pm <- lm(billions ~ poly(decade, 3))
> >
> > plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000),
> main="average yearly inflation-adjusted dollar cost of extreme weather
> events worldwide")
> > curve(predict(pm, data.frame(decade=x)), add=TRUE)
> > # output: http://www.bovik.org/storms.gif
> >
> > summary(pm)
>
> Call:
> lm(formula = billions ~ poly(decade, 3))
>
> Residuals:
>       1       2       3       4       5
>  0.2357 -0.9429  1.4143 -0.9429  0.2357
>
> Coefficients:
>                  Estimate Std. Error t value Pr(>|t|)
> (Intercept)        13.800      0.882  15.647   0.0406 *
> poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
> poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
> poly(decade, 3)3    6.483      1.972   3.287   0.1880
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 1.972 on 1 degrees of freedom
> Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
> F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317
>
> > pm
>
> Call:
> lm(formula = billions ~ poly(decade, 3))
>
> Coefficients:
>      (Intercept)  poly(decade, 3)1  poly(decade, 3)2  poly(decade, 3)3
>           13.800            25.614            14.432             6.483
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 14 13:03:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 12:03:42 +0100 (BST)
Subject: [R] superscript in figures - basic question
In-Reply-To: <17070.34458.263554.25277@stat.math.ethz.ch>
References: <s2aee600.099@hra2.marc.hort.cri.nz>
	<17070.34458.263554.25277@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0506140923170.21307@gannet.stats>

On Tue, 14 Jun 2005, Martin Maechler wrote:

>>>>>> "Peter" == Peter Alspach <PAlspach at hortresearch.co.nz>
>>>>>>     on Tue, 14 Jun 2005 14:11:47 +1200 writes:
>
>    Peter> Ben
>
>    Peter> Others have pointed out plotmath.  However, for some
>    Peter> superscripts (including 2) it may be easier to use
>    Peter> the appropriate escape sequence (at in Windows):
>
>    Peter> ylab = 'BA (m\262/ha)'
>
> but please refrain from doing that way.
> You should write R code that is portable, and ``plotmath''
> has been designed to be portable.  Windows escape codes are not,
> and may not even work in future (or current?) versions of
> Windows with `unusual' locale settings {fonts, etc}.

Indeed, it only works for superscript 2 and 3 and only in Western European 
locales (I just happened to be testing Korean).  Also, the spacing is not 
done as carefully as plotmath.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From subianto at gmail.com  Tue Jun 14 13:31:50 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Tue, 14 Jun 2005 13:31:50 +0200
Subject: [R] update.packages() - gregmisc
Message-ID: <42AEC026.3000203@gmail.com>

Dear all,
I have a problem to update package gregmisc.
After I update,
 > update.packages(ask='graphics')
trying URL 
'http://cran.at.r-project.org/bin/windows/contrib/2.1/gregmisc_2.0.8.zip'
Content type 'application/zip' length 2465 bytes
opened URL
downloaded 2465 bytes

package 'gregmisc' successfully unpacked and MD5 sums checked
...

then try to update again, still I must update package gregmisc, etc.
I have tried 3,4,5, times with the same result.

Best,
Muhammad Subianto

 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    1.0
year     2005
month    04
day      18
language R
 >



From ggrothendieck at gmail.com  Tue Jun 14 13:51:06 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 07:51:06 -0400
Subject: [R] update.packages() - gregmisc
In-Reply-To: <42AEC026.3000203@gmail.com>
References: <42AEC026.3000203@gmail.com>
Message-ID: <971536df0506140451621eaf23@mail.gmail.com>

On 6/14/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Dear all,
> I have a problem to update package gregmisc.
> After I update,
>  > update.packages(ask='graphics')
> trying URL
> 'http://cran.at.r-project.org/bin/windows/contrib/2.1/gregmisc_2.0.8.zip'
> Content type 'application/zip' length 2465 bytes
> opened URL
> downloaded 2465 bytes
> 
> package 'gregmisc' successfully unpacked and MD5 sums checked
> ...
> 
> then try to update again, still I must update package gregmisc, etc.
> I have tried 3,4,5, times with the same result.
> 

This was discussed on r-devel recently.  See:

https://www.stat.math.ethz.ch/pipermail/r-devel/2005-June/033479.html



From eddy_lolo at hotmail.com  Tue Jun 14 14:10:03 2005
From: eddy_lolo at hotmail.com (Eddy G.)
Date: Tue, 14 Jun 2005 14:10:03 +0200
Subject: [R] bs() function of the splines package with intercept=FALSE
Message-ID: <BAY103-F239208C135ED45B89435C88CF30@phx.gbl>

Hello,

I'm implementing a function using non uniform B-Splines. Looking at the
code of the bs() function, I realized that if the intercept was set to
FALSE, the behavior of the function was the following (df is the number
of degrees of freedom that I believe can be interpreted as the number
of control points):

- Compute df- ord + 1 _internal_ knots (ord is the order of the spline)
- Add ord times the boundaries values at each extrem of the knots
vector.
- Compute the design matrix on this vector.

This results in a design matrix with df+1 columns. The bs function the
_removes_ the first column of the matrix (resulting as expected in a
matrix with df columns).

I'm a bit confused, does anyone see the mathematical justification of
this manipulation?

Thanks a lot,

Laurent


http://www.imagine-msn.com/hotmail/default.aspx?locale=fr-FR



From subianto at gmail.com  Tue Jun 14 14:21:00 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Tue, 14 Jun 2005 14:21:00 +0200
Subject: [R] update.packages() - gregmisc
In-Reply-To: <971536df0506140451621eaf23@mail.gmail.com>
References: <42AEC026.3000203@gmail.com>
	<971536df0506140451621eaf23@mail.gmail.com>
Message-ID: <42AECBAC.605@gmail.com>

Thanks.
I do like this,
 > remove.packages("gregmisc", .libPaths()[1])
 > remove.packages("gtools", .libPaths()[1])
 > install.packages("gregmisc", .libPaths()[1])
 > update.packages()
 > update.packages()
 > install.packages("gtools", .libPaths()[1])
 > update.packages()
 > update.packages()
 > update.packages(ask='graphics')

Regards,
Muhammad Subianto
R.2.1.0 on W2K

On this day 6/14/2005 1:51 PM, Gabor Grothendieck wrote:
> On 6/14/05, Muhammad Subianto <subianto at gmail.com> wrote:
> 
>>Dear all,
>>I have a problem to update package gregmisc.
>>After I update,
>> > update.packages(ask='graphics')
>>trying URL
>>'http://cran.at.r-project.org/bin/windows/contrib/2.1/gregmisc_2.0.8.zip'
>>Content type 'application/zip' length 2465 bytes
>>opened URL
>>downloaded 2465 bytes
>>
>>package 'gregmisc' successfully unpacked and MD5 sums checked
>>...
>>
>>then try to update again, still I must update package gregmisc, etc.
>>I have tried 3,4,5, times with the same result.
>>
> 
> 
> This was discussed on r-devel recently.  See:
> 
> https://www.stat.math.ethz.ch/pipermail/r-devel/2005-June/033479.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Tue Jun 14 14:21:39 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 14 Jun 2005 14:21:39 +0200
Subject: [R] why does the unsubscribe not work
In-Reply-To: <x2u0k19rr6.fsf@turmalin.kubism.ku.dk>
References: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
	<1118738578.42ae9892a6100@horde.scannet.dk>
	<x2u0k19rr6.fsf@turmalin.kubism.ku.dk>
Message-ID: <17070.52179.435298.199209@stat.math.ethz.ch>

>>>>> "PD" Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on 14 Jun 2005 12:06:53 +0200 writes:

    PD> bry at xdocs.dk writes:

    >> Hi I have tried to unsubscribe from this list now three
    >> times, I desperately need to unsubscribe from this list
    >> from this address because the list is choking this
    >> mailbox.

    >> Below are the various suggested ways of unsubscribing
    >> 
    ..............
    .............. (whining whining ...)
    ..............

    PD> What are the timeframes involved? There is always some time delay. In
    PD> particular, there is no way of stopping mails that have already been
    PD> sent by the list handling software. And if your mailbox is clogged,
    PD> things may stay in the queue for up to five days before the relevant
    PD> mail relay gives up.

    PD> If your password has stopped working, it suggests to me that you have
    PD> in fact already been unsubscribed.

yes, and that's exactly true. He has been unsubscribed when he
wanted it -- and IIRC the confirmation e-mail even tells you
that you may continue to receive mail for a short while after
unsubscribing.

Yes, I ("r-help-owner") have seen your e-mail -- without any
politeness usually seen in letters including no name of sender --
among many other things that I must do or want to complete.  
If "*-owner" was requested to deal with subscription and unsubscriptions
``manually'' each time someone unsubcscribes or subscribes,
this would amount to deal with about 20 such e-mails per day.
I'm usually inclined to reply to polite e-mail letters, but even
then only when more urgent duties have been fulfilled first.
It seems that 99% of all R-help subscribers can deal with the
system themselves.

Regards,
Martin Maechler, ETH Zurich



From ramasamy at cancer.org.uk  Tue Jun 14 14:32:55 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 14 Jun 2005 13:32:55 +0100
Subject: [R] why does the unsubscribe not work
In-Reply-To: <x2u0k19rr6.fsf@turmalin.kubism.ku.dk>
References: <mailman.9.1118484001.30742.r-help@stat.math.ethz.ch>
	<1118738578.42ae9892a6100@horde.scannet.dk>
	<x2u0k19rr6.fsf@turmalin.kubism.ku.dk>
Message-ID: <1118752375.5948.10.camel@ramasamy.stats>

You can test this suggestion by asking for a password reminder from the
https site. If you do get an email that says what your password is, then
you are still subscribed.

I actually asked the website to send me a password reminder to an
account that is not subscribed. Although it says that "A reminder of
your password has been emailed to you" on the website, the account in
question did not receive any emails. 

Regards, Adai



<SNIP>
> What are the timeframes involved? There is always some time delay. In
> particular, there is no way of stopping mails that have already been
> sent by the list handling software. And if your mailbox is clogged,
> things may stay in the queue for up to five days before the relevant
> mail relay gives up.
> 
> If your password has stopped working, it suggests to me that you have
> in fact already been unsubscribed.
>



From rmott at well.ox.ac.uk  Tue Jun 14 15:02:41 2005
From: rmott at well.ox.ac.uk (Richard Mott)
Date: Tue, 14 Jun 2005 14:02:41 +0100
Subject: [R] load ing and saving R objects
Message-ID: <42AED571.8090805@well.ox.ac.uk>

Does anyone know a way to do the following:

Save a large number of R objects to a file (like load() does) but then 
read back only a small named subset of them . As far as I can see, 
load() reads back everything.

The context is:

I have an application which will generate a large number of large 
matrices (approx 15000 matrices each of dimension 2000*30). I can 
generate these matrices using an R-package I wrote, but it requires a 
large amouint of memory and is slow so I want to do this only once.  
However, I then want to do some subsequent processing, comprising a very 
large number of runs in which small  (~ 10) random selection of matrices 
from the previously computed set are used for linear modeling.  So I 
need a way to load back named objects previously saved in a call to 
save(). I can;t see anyway of doing this. Any ideas?

Thanks

Richard Mott
 

-- 
----------------------------------------------------
Richard Mott       | Wellcome Trust Centre 
tel 01865 287588   | for Human Genetics
fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN



From ajayshah at mayin.org  Tue Jun 14 15:03:37 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Tue, 14 Jun 2005 18:33:37 +0530
Subject: [R] Puzzled in utilising summary.lm() to obtain Var(x)
Message-ID: <20050614130337.GI6989@lubyanka.local>

I have a program which is doing a few thousand runs of lm(). Suppose
it is a simple model
   y = a + bx1 + cx2 + e

I have the R object "d" where
   d <- summary(lm(y ~ x1 + x2))

I would like to obtain Var(x2) out of "d". How might I do it?

I can, of course, always do sd(x2). But it would be much more
convenient if I could snoop around the contents of summary.lm and
extract Var() out of it. I couldn't readily see how. Would you know
what would click?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From matthew_wiener at merck.com  Tue Jun 14 15:15:32 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 14 Jun 2005 09:15:32 -0400
Subject: [R] load ing and saving R objects
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E049946C4@uswsmx03.merck.com>

This may not be quite the answer you're looking for, but I sometimes save
each such object in its own file (usually <object.name>.RData).  Then, if
you know which objects you're looking for, you know their names, and can
load the individual files.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Richard Mott
Sent: Tuesday, June 14, 2005 9:03 AM
To: r-help at stat.math.ethz.ch
Subject: [R] load ing and saving R objects


Does anyone know a way to do the following:

Save a large number of R objects to a file (like load() does) but then 
read back only a small named subset of them . As far as I can see, 
load() reads back everything.

The context is:

I have an application which will generate a large number of large 
matrices (approx 15000 matrices each of dimension 2000*30). I can 
generate these matrices using an R-package I wrote, but it requires a 
large amouint of memory and is slow so I want to do this only once.  
However, I then want to do some subsequent processing, comprising a very 
large number of runs in which small  (~ 10) random selection of matrices 
from the previously computed set are used for linear modeling.  So I 
need a way to load back named objects previously saved in a call to 
save(). I can;t see anyway of doing this. Any ideas?

Thanks

Richard Mott
 

-- 
----------------------------------------------------
Richard Mott       | Wellcome Trust Centre 
tel 01865 287588   | for Human Genetics
fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pmbrando at ipam.org.br  Tue Jun 14 18:26:59 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Tue, 14 Jun 2005 09:26:59 -0700
Subject: [R] RES:  Learning, "if" and "else"
In-Reply-To: <1118744240.6041.22.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <000001c570fd$e6fb8fd0$f10aa8c0@paulobrando>

Thank you very much! All methods are very useful. 

________________________________________
Paulo M. Brando
Instituto de Pesquisa Ambiental da Amazonia (IPAM)
Santarem, PA, Brasil.
Av. Rui Barbosa, 136.
Fone: + 55 93 522 55 38
www.ipam.org.br
E-mail: pmbrando at ipam.org.br



-----Mensagem original-----
De: Adaikalavan Ramasamy [mailto:ramasamy at cancer.org.uk] 
Enviada em: Tuesday, June 14, 2005 3:17 AM
Para: Paulo Brando
Cc: R-help at stat.math.ethz.ch
Assunto: Re: [R] Learning, "if" and "else"

You can also use switch() instead of ifelse(), which makes the code a
bit easier to read. The downside to this is that switch does not take
vectorised input and thus you need a loop. For example

 # data
 dbh  <- c(30,29,15,14,30,29)
 form <- factor(c("tree", "tree", "liana", "liana", "palm", "palm")) 
 df   <- data.frame(dbh, form)
 
 out <- numeric( nrow(df) )
 for(i in 1:nrow(df)){
 
   x <- as.numeric( df[i, 1] )
   y <- as.character( df[i, 2] )

   out[i] <- switch( y,
              "tree"  = exp( -4.898 + 4.512*log(x) - 0.319*(log(x))^2 ),
              "liana" = 10^(0.07 + 2.17 * log10(x)),
              NA
                   )
 }


Or slightly more efficient solution is

 out <- apply( df, 1, function(z){
   x <- as.numeric(z[1]); y <- as.character(z[2])
   
   switch( y,
          "tree"  = exp( -4.898 + 4.512*log(x) - 0.319*(log(x))^2 ),
          "liana" = 10^(0.07 + 2.17 * log10(x)),
          NA
          )
 })

Regards, Adai



On Mon, 2005-06-13 at 15:32 -0700, Paulo Brando wrote:
> Dear Rs,
> 
> I have tried to use conditional expressions to calculate biomass for
> different life forms (trees, lianas, and palms).
> 
> Here is an example:
> 
> > lifeform
> 
>     dbh  form
> 1   30  tree
> 2   29  tree
> 3   28  tree
> 4   27  tree
> 5   26  tree
> 6   25  tree
> 7   24  tree
> 8   23  tree
> 9   22  tree
> 10  21  tree
> 11  20  tree
> 12  15 liana
> 13  14 liana
> 14  13 liana
> 15  12 liana
> 16  11 liana
> 17  10 liana
> 18   9 liana
> 19   8 liana
> 20   7 liana
> 21   6 liana
> 22   5 liana
> 23  30  palm
> 24  29  palm
> 25  28  palm
> 26  27  palm
> 27  26  palm
> 28  25  palm
> 29  24  palm
> 30  23  palm
> 31  22  palm
> 32  21  palm
> 33  20  palm
> 
> ### I want to include biomass 
> 
> lifeform$biomass <- 
> 
> {
>       if(lifeform$form=="tree")
>        exp(-4.898+4.512*log(dbh)-0.319*(log(dbh))^2) 
>        else{ 
>         if (lifeform$form=="liana")
>        10^(0.07 + 2.17 * log10 (dbh))
>        else ("NA")
> }
> Warning message:
> the condition has length > 1 and only the first element will be used
in:
> if (lifeform$form == "tree") exp(-4.898 + 4.512 * log(dbh) -
> 
> 
> ### But I always get the message warning message above. 
> 
> 
> 
> I looked for similar examples in R mail list archive, but they did not
> help a lot.
> 
> I am quite new to 'R'. Any material that covers this theme?
> 
> Thank you very much!
> 
> Paulo
> 
> PS. Sorry about the last e-mail. I did not change the message title.
> ________________________________________
> Paulo M. Brando
> Instituto de Pesquisa Ambiental da Amazonia (IPAM)
> Santarem, PA, Brasil.
> Av. Rui Barbosa, 136.
> Fone: + 55 93 522 55 38
> www.ipam.org.br
> E-mail: pmbrando at ipam.org.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Tue Jun 14 15:24:53 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 14 Jun 2005 14:24:53 +0100
Subject: [R] load ing and saving R objects
In-Reply-To: <42AED571.8090805@well.ox.ac.uk>
References: <42AED571.8090805@well.ox.ac.uk>
Message-ID: <42AEDAA5.6050101@lancaster.ac.uk>

Richard Mott wrote:
> Does anyone know a way to do the following:
> 
> Save a large number of R objects to a file (like load() does) but then 
> read back only a small named subset of them . As far as I can see, 
> load() reads back everything.

  Save them to individual files when you generate them?

  for(i in 1:15000){

   m=generateBigMatrix(i)

   filename=paste("BigMatrix-",i,".Rdata",sep='')
   save(m,file=filename)
  }

Note that load will always overwrite 'm', so to load a sample of them in 
you'll need to do something like this:

  bigSamples=list()

  for(i in sample(15000,N)){
    filename=paste("BigMatrix-",i,".Rdata",sep='')
    load(filename)
    bigSamples[[i]]=m
  }

  But there may be a more efficient way to string up a big list like 
that, I can never remember - get it working, then worry about optimisation.

  I hope your filesystem is happy with 15000 objects in it. I would 
dedicate a folder or directory for just these objects' files, since it 
then becomes near impossible to see anything other than the big matrix 
files...


Baz



From rpeng at jhsph.edu  Tue Jun 14 15:38:27 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 14 Jun 2005 09:38:27 -0400
Subject: [R] load ing and saving R objects
In-Reply-To: <42AED571.8090805@well.ox.ac.uk>
References: <42AED571.8090805@well.ox.ac.uk>
Message-ID: <42AEDDD3.4080603@jhsph.edu>

I would suggest saving each object to an individual file with 
some sort of systematic file name.  That way, you can implement a 
  rudimentary key-value database and load only the objects you 
want.  You might be interested in the 'serialize()' and 
'unserialize()' functions for this purpose.

If having ~15000 files is not desirable, then you need a database 
like GDBM.  If you can live with something simpler, you might 
take a look at my 'filehash' package at 
http://sandybox.typepad.com/software/.  It hasn't been tested 
much but it may suit your needs.

-roger

Richard Mott wrote:
> Does anyone know a way to do the following:
> 
> Save a large number of R objects to a file (like load() does) but then 
> read back only a small named subset of them . As far as I can see, 
> load() reads back everything.
> 
> The context is:
> 
> I have an application which will generate a large number of large 
> matrices (approx 15000 matrices each of dimension 2000*30). I can 
> generate these matrices using an R-package I wrote, but it requires a 
> large amouint of memory and is slow so I want to do this only once.  
> However, I then want to do some subsequent processing, comprising a very 
> large number of runs in which small  (~ 10) random selection of matrices 
> from the previously computed set are used for linear modeling.  So I 
> need a way to load back named objects previously saved in a call to 
> save(). I can;t see anyway of doing this. Any ideas?
> 
> Thanks
> 
> Richard Mott
>  
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From f.harrell at vanderbilt.edu  Tue Jun 14 15:46:50 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 14 Jun 2005 09:46:50 -0400
Subject: [R] ordinary polynomial coefficients from
	orthogonal	polynomials?
In-Reply-To: <Pine.LNX.4.61.0506141126160.26371@gannet.stats>
References: <42AE9D59.6090006@bovik.org>
	<Pine.LNX.4.61.0506141126160.26371@gannet.stats>
Message-ID: <42AEDFCA.5080107@vanderbilt.edu>

Prof Brian Ripley wrote:
> On Tue, 14 Jun 2005, James Salsman wrote:
> 
> 
>>How can ordinary polynomial coefficients be calculated
>>from an orthogonal polynomial fit?
> 
> 
> Why would you want to do that?  predict() is perfectly happy with an
> orthogonal polynomial fit and the `ordinary polynomial coefficients' are 
> rather badly determined in your example since the design matrix has a very 
> high condition number.

Brian - I don't fully see the relevance of the high condition number 
nowadays unless the predictor has a really bad origin.  Orthogonal 
polynomials are a mess for most people to deal with.

Frank

> 
> 
>>I'm trying to do something like find a,b,c,d from
>> lm(billions ~ a+b*decade+c*decade^2+d*decade^3)
>>but that gives:  "Error in eval(expr, envir, enclos) :
>>Object "a" not found"
> 
> 
> You could use
> 
> lm(billions ~ decade + I(decade^2) + I(decade^3))
> 
> except that will be numerically inaccurate, since
> 
> 
>>m <- model.matrix(~ decade + I(decade^2) + I(decade^3))
>>kappa(m)
> 
> [1] 3.506454e+16
> 
> 
> 
> 
>>>decade <- c(1950, 1960, 1970, 1980, 1990)
>>>billions <- c(3.5, 5, 7.5, 13, 40)
>>># source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
>>>
>>>pm <- lm(billions ~ poly(decade, 3))
>>>
>>>plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000),
>>
>>main="average yearly inflation-adjusted dollar cost of extreme weather
>>events worldwide")
>>
>>>curve(predict(pm, data.frame(decade=x)), add=TRUE)
>>># output: http://www.bovik.org/storms.gif
>>>
>>>summary(pm)
>>
>>Call:
>>lm(formula = billions ~ poly(decade, 3))
>>
>>Residuals:
>>      1       2       3       4       5
>> 0.2357 -0.9429  1.4143 -0.9429  0.2357
>>
>>Coefficients:
>>                 Estimate Std. Error t value Pr(>|t|)
>>(Intercept)        13.800      0.882  15.647   0.0406 *
>>poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
>>poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
>>poly(decade, 3)3    6.483      1.972   3.287   0.1880
>>---
>>Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>
>>Residual standard error: 1.972 on 1 degrees of freedom
>>Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
>>F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317
>>
>>
>>>pm
>>
>>Call:
>>lm(formula = billions ~ poly(decade, 3))
>>
>>Coefficients:
>>     (Intercept)  poly(decade, 3)1  poly(decade, 3)2  poly(decade, 3)3
>>          13.800            25.614            14.432             6.483
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From rpeng at jhsph.edu  Tue Jun 14 15:48:41 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 14 Jun 2005 09:48:41 -0400
Subject: [R] ordinary polynomial coefficients from orthogonal
	polynomials?
In-Reply-To: <42AE9D59.6090006@bovik.org>
References: <42AE9D59.6090006@bovik.org>
Message-ID: <42AEE039.5050105@jhsph.edu>

I think this is covered in the FAQ:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Models

-roger

James Salsman wrote:
> How can ordinary polynomial coefficients be calculated
> from an orthogonal polynomial fit?
> 
> I'm trying to do something like find a,b,c,d from
>   lm(billions ~ a+b*decade+c*decade^2+d*decade^3)
> but that gives:  "Error in eval(expr, envir, enclos) :
> Object "a" not found"
> 
>  > decade <- c(1950, 1960, 1970, 1980, 1990)
>  > billions <- c(3.5, 5, 7.5, 13, 40)
>  > # source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
>  >
>  > pm <- lm(billions ~ poly(decade, 3))
>  >
>  > plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000), 
> main="average yearly inflation-adjusted dollar cost of extreme weather 
> events worldwide")
>  > curve(predict(pm, data.frame(decade=x)), add=TRUE)
>  > # output: http://www.bovik.org/storms.gif
>  >
>  > summary(pm)
> 
> Call:
> lm(formula = billions ~ poly(decade, 3))
> 
> Residuals:
>        1       2       3       4       5
>   0.2357 -0.9429  1.4143 -0.9429  0.2357
> 
> Coefficients:
>                   Estimate Std. Error t value Pr(>|t|)
> (Intercept)        13.800      0.882  15.647   0.0406 *
> poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
> poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
> poly(decade, 3)3    6.483      1.972   3.287   0.1880
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 1.972 on 1 degrees of freedom
> Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
> F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317
> 
>  > pm
> 
> Call:
> lm(formula = billions ~ poly(decade, 3))
> 
> Coefficients:
>       (Intercept)  poly(decade, 3)1  poly(decade, 3)2  poly(decade, 3)3
>            13.800            25.614            14.432             6.483
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From r.hillary at imperial.ac.uk  Tue Jun 14 16:07:42 2005
From: r.hillary at imperial.ac.uk (Richard Hillary)
Date: Tue, 14 Jun 2005 15:07:42 +0100
Subject: [R] Manipulating dates
Message-ID: <42AEE4AE.2050800@imperial.ac.uk>

Hello,
          Given a vector of characters, or factors, denoting the date in 
the following way: 28/03/2000, is there a method of
1) Computing the earliest of these dates;
2) Using this as a base, then converting all the other dates into merely 
the number of days after this minimum date
Many thanks
Richard Hillary



From james.holtman at convergys.com  Tue Jun 14 16:14:41 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue, 14 Jun 2005 10:14:41 -0400
Subject: [R] Manipulating dates
In-Reply-To: <42AEE4AE.2050800@imperial.ac.uk>
Message-ID: <OF22D1D749.AE0AAF48-ON85257020.004E06ED-85257020.004E3FA7@nd.convergys.com>





Use POSIX.  To convert:

my.dates <- strptime(your.characters, format='%d/%m/%Y')


once you have that, you can use 'min' to find the minimum.

'difftime' will give you the differences.

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Richard Hillary                                                                                                      
                      <r.hillary at imperial.a        To:       r-help at stat.math.ethz.ch                                                      
                      c.uk>                        cc:                                                                                     
                      Sent by:                     Subject:  [R] Manipulating dates                                                        
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/14/2005 10:07                                                                                                     
                      Please respond to                                                                                                    
                      r.hillary                                                                                                            
                                                                                                                                           




Hello,
          Given a vector of characters, or factors, denoting the date in
the following way: 28/03/2000, is there a method of
1) Computing the earliest of these dates;
2) Using this as a base, then converting all the other dates into merely
the number of days after this minimum date
Many thanks
Richard Hillary

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.be  Tue Jun 14 16:18:52 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 14 Jun 2005 16:18:52 +0200
Subject: [R] Manipulating dates
References: <42AEE4AE.2050800@imperial.ac.uk>
Message-ID: <004801c570eb$fdd494c0$0540210a@www.domain>

try this:

x <- c("28/03/2000", "27/08/2001", "29/05/2002", "15/12/2003")
y <- as.Date(x, "%d/%m/%Y")
y - min(y)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Richard Hillary" <r.hillary at imperial.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 14, 2005 4:07 PM
Subject: [R] Manipulating dates


> Hello,
>          Given a vector of characters, or factors, denoting the date 
> in
> the following way: 28/03/2000, is there a method of
> 1) Computing the earliest of these dates;
> 2) Using this as a base, then converting all the other dates into 
> merely
> the number of days after this minimum date
> Many thanks
> Richard Hillary
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Tue Jun 14 16:28:31 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 14 Jun 2005 22:28:31 +0800
Subject: [R] some suggestion
Message-ID: <0II200ER1VPT0U@mail.fudan.edu.cn>

it seems R has no function to sort the data.frame according to some variable(s),though we can do these by order() and indexing.but why not make sort() the a generic function,making it can sort vector and other type of objects?

maybe this is a silly suggestion,but i think it is quite usefull.

 				


2005-06-14

------
Deparment of Sociology
Fudan University

Blog:www.sociology.yculblog.com



From ripley at stats.ox.ac.uk  Tue Jun 14 16:28:43 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 15:28:43 +0100 (BST)
Subject: [R] ordinary polynomial coefficients from orthogonal
	polynomials?
In-Reply-To: <42AEDFCA.5080107@vanderbilt.edu>
References: <42AE9D59.6090006@bovik.org>
	<Pine.LNX.4.61.0506141126160.26371@gannet.stats>
	<42AEDFCA.5080107@vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0506141525310.32282@gannet.stats>

On Tue, 14 Jun 2005, Frank E Harrell Jr wrote:

> Prof Brian Ripley wrote:
>> On Tue, 14 Jun 2005, James Salsman wrote:
>> 
>> 
>>> How can ordinary polynomial coefficients be calculated
>>> from an orthogonal polynomial fit?
>> 
>> 
>> Why would you want to do that?  predict() is perfectly happy with an
>> orthogonal polynomial fit and the `ordinary polynomial coefficients' are 
>> rather badly determined in your example since the design matrix has a very 
>> high condition number.
>
> Brian - I don't fully see the relevance of the high condition number nowadays 
> unless the predictor has a really bad origin.  Orthogonal polynomials are a 
> mess for most people to deal with.

It means that if you write down the coeffs to a few places and then try to 
reproduce the predictions you will do badly.  The perturbation analysis 
depends on the condition number, and so is saying that the predictions are 
dependent on fine details of the coefficients.

Using (year-2000)/1000 or (year - 1970)/1000 would be a much better idea.

Why do `people' need `to deal with' these, anyway.  We have machines to do 
that.

>
> Frank
>
>> 
>> 
>>> I'm trying to do something like find a,b,c,d from
>>> lm(billions ~ a+b*decade+c*decade^2+d*decade^3)
>>> but that gives:  "Error in eval(expr, envir, enclos) :
>>> Object "a" not found"
>> 
>> 
>> You could use
>> 
>> lm(billions ~ decade + I(decade^2) + I(decade^3))
>> 
>> except that will be numerically inaccurate, since
>> 
>> 
>>> m <- model.matrix(~ decade + I(decade^2) + I(decade^3))
>>> kappa(m)
>> 
>> [1] 3.506454e+16
>> 
>> 
>> 
>> 
>>>> decade <- c(1950, 1960, 1970, 1980, 1990)
>>>> billions <- c(3.5, 5, 7.5, 13, 40)
>>>> # source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
>>>> 
>>>> pm <- lm(billions ~ poly(decade, 3))
>>>> 
>>>> plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000),
>>> 
>>> main="average yearly inflation-adjusted dollar cost of extreme weather
>>> events worldwide")
>>> 
>>>> curve(predict(pm, data.frame(decade=x)), add=TRUE)
>>>> # output: http://www.bovik.org/storms.gif
>>>> 
>>>> summary(pm)
>>> 
>>> Call:
>>> lm(formula = billions ~ poly(decade, 3))
>>> 
>>> Residuals:
>>>      1       2       3       4       5
>>> 0.2357 -0.9429  1.4143 -0.9429  0.2357
>>> 
>>> Coefficients:
>>>                 Estimate Std. Error t value Pr(>|t|)
>>> (Intercept)        13.800      0.882  15.647   0.0406 *
>>> poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
>>> poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
>>> poly(decade, 3)3    6.483      1.972   3.287   0.1880
>>> ---
>>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>> 
>>> Residual standard error: 1.972 on 1 degrees of freedom
>>> Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
>>> F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317
>>> 
>>> 
>>>> pm
>>> 
>>> Call:
>>> lm(formula = billions ~ poly(decade, 3))
>>> 
>>> Coefficients:
>>>     (Intercept)  poly(decade, 3)1  poly(decade, 3)2  poly(decade, 3)3
>>>          13.800            25.614            14.432             6.483
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>> 
>> 
>> 
>
>
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                     Department of Biostatistics   Vanderbilt University
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.be  Tue Jun 14 16:33:38 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 14 Jun 2005 16:33:38 +0200
Subject: [R] some suggestion
References: <0II200ER1VPT0U@mail.fudan.edu.cn>
Message-ID: <007e01c570ee$0dcc0910$0540210a@www.domain>

take a look at this function by Kevin Wright

RSiteSearch("sort.data.frame")


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "ronggui" <0034058 at fudan.edu.cn>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 14, 2005 4:28 PM
Subject: [R] some suggestion


> it seems R has no function to sort the data.frame according to some 
> variable(s),though we can do these by order() and indexing.but why 
> not make sort() the a generic function,making it can sort vector and 
> other type of objects?
>
> maybe this is a silly suggestion,but i think it is quite usefull.
>
>
>
>
> 2005-06-14
>
> ------
> Deparment of Sociology
> Fudan University
>
> Blog:www.sociology.yculblog.com
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From fezzi at stat.unibo.it  Tue Jun 14 16:35:49 2005
From: fezzi at stat.unibo.it (Carlo Fezzi)
Date: Tue, 14 Jun 2005 16:35:49 +0200 (CEST)
Subject: [R] problems with the fitted values of the function gls
Message-ID: <2543400.1118759749920.SLOX.WebMail.wwwrun@magenta.stat.unibo.it>

Dear helpers,
I am a beginner user of R and probably I am not using properly the
function gls() of the library Å‚ÄúnlmeÅ‚Äù.

I estimated a model with AR(1) residuals but apparently the fitted and
the predicted values arenÅ‚Äôt constructed considering the autocorrelation
of the residuals.

It seems that instead of having:

yt hat = (b hat)Å‚Äô xt  + Phi (ut-1 hat) 

the program gives just:

yt hat = (b hat)Å‚Äô xt

To understand better this issue I tried to fit a model for a simulated
AR(1) process with the following code:

--------
library(tseries)
library(nlme)

set.seed(2)
ar1sim <- arima.sim ( list (order = c (1,0,0), ar = 0.5), n = 200 )

reg <- gls ( ar1sim ~ 1, correlation = corAR1() )

plot(ar1sim)
points(predict(reg), type = Å‚ÄúlÅ‚Äù, col = Å‚ÄúredÅ‚Äù)
---------

And I obtained the plot of the fitted values (just the intercept)
compared with the actual ones.

Probably I am doing something wrong, because I donÅ‚Äôt think that the
function should behave in this way, but I donÅ‚Äôt understand what.

Thank you so much for your help,

Carlo Fezzi



From ripley at stats.ox.ac.uk  Tue Jun 14 16:41:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 15:41:10 +0100 (BST)
Subject: [R] load ing and saving R objects
In-Reply-To: <42AEDAA5.6050101@lancaster.ac.uk>
References: <42AED571.8090805@well.ox.ac.uk> <42AEDAA5.6050101@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.61.0506141535120.32282@gannet.stats>

On Tue, 14 Jun 2005, Barry Rowlingson wrote:

> Richard Mott wrote:
>> Does anyone know a way to do the following:
>>
>> Save a large number of R objects to a file (like load() does) but then
>> read back only a small named subset of them . As far as I can see,
>> load() reads back everything.
>
>  Save them to individual files when you generate them?
>
>  for(i in 1:15000){
>
>   m=generateBigMatrix(i)
>
>   filename=paste("BigMatrix-",i,".Rdata",sep='')
>   save(m,file=filename)
>  }
>
> Note that load will always overwrite 'm', so to load a sample of them in
> you'll need to do something like this:
>
>  bigSamples=list()
>
>  for(i in sample(15000,N)){
>    filename=paste("BigMatrix-",i,".Rdata",sep='')
>    load(filename)
>    bigSamples[[i]]=m
>  }
>
>  But there may be a more efficient way to string up a big list like
> that, I can never remember - get it working, then worry about optimisation.

(Yes, use bigSamples <- vector("list", 15000) first.)

>  I hope your filesystem is happy with 15000 objects in it. I would
> dedicate a folder or directory for just these objects' files, since it
> then becomes near impossible to see anything other than the big matrix
> files...

.readRDS/.saveRDS might be a better way to do this, and avoids always 
restoring to "m".

If your file system does not like 15000 files you can always save in a 
DBMS.

I did once look into restoring just some of the objects in a save()ed 
file, but it is not really possible to do so efficiently due to sharing 
between objects.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Tue Jun 14 16:41:20 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 14 Jun 2005 07:41:20 -0700 (PDT)
Subject: [R] New Family object for GLM models...
In-Reply-To: <20050614074057.3219.qmail@web53009.mail.yahoo.com>
References: <20050614074057.3219.qmail@web53009.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0506140735160.92116@homer03.u.washington.edu>

On Tue, 14 Jun 2005, Abatih E. wrote:
> Dear R-Users, I wish to create a new family object based on the Binomial 
> family. The only difference will be with the link function. Thus instead 
> if using the 'logit(u)' link function, i plan to use '-log(i-u)'. So 
> far, i have tried to write the function following that of the Binomial 
> and Negative Binomial families. The major problem i have here is with 
> the definition of the initial values. The definitions i have attempted 
> so far don't yield convergence when implemented in a model.

Are you sure the problem is with initial values?  Your link function 
is capable of producing mu<0 from large enough negative eta.  If the MLE 
is on the boundary of the parameter space it will not solve the likelihood 
equations and so won't be found by iterative reweighted least squares. It 
might be found by step-halving in glm, but that isn't guaranteed.

If the mle is in the interior of the parameter space then in my experience 
the starting values aren't terribly important (though my experience is 
with the log-binomial rather than -log(1-mu) link).

 	-thomas


>								I still 
> can't figure out how the initial values are defined. I don't know if it 
> is based on some property of the fitted model or the domain and /or 
> range of the link function. I wish someone could help me provide a 
> solution to this problem. I have appended the function i wrote.
>
>
> Add.haz<-function () {
>
>
>
>    env <- new.env(parent = .GlobalEnv)
>
>    assign(".nziek", nziek, envir = env)
>
>    famname<-"Addhaza"
>
>    link="addlink"
>
>    linkfun<-function(mu) -log(1-mu)
>
>    linkinv<-function(eta) 1-exp(-eta)
>
>    variance<-function(mu) mu*(1-mu)
>
>    validmu<-function(mu) all(mu > 0) && all(mu < 1)
>
>            mu.eta<-function(mu) 1/(1-mu)
>
>        dev.resids <- function(y, mu, wt) 2 * wt * (y * log(ifelse(y ==
>
>        0, 1, y/mu)) + (1 - y) * log(ifelse(y == 1, 1, (1 - y)/(1 -
>
>        mu))))
>
>    aic <- function(y, n, mu, wt, dev) {
>
>        m <- if (any(n > 1))
>
>            n
>
>        else wt
>
>        -2 * sum(ifelse(m > 0, (wt/m), 0) * dbinom(round(m *
>
>            y), round(m), mu, log = TRUE))
>
>    }
>
>
>
>    initialize<- expression({
>
>            n<-rep(1,nobs)
>
>            if (any(y < 0 | y > 1)) stop("y values must be 0 <= y <= 1")
>
>                         mustart <- (weights * y + 0.5)/(weights + 1)
>
>            m <- weights * y
>
>            if (any(abs(m - round(m)) > 0.001)) warning("non-integer #successes in a binomial glm!")
>
>
>
>
>
> })
>
>
>
>    environment(variance) <- environment(validmu) <- environment(dev.resids) <- environment(aic) <- env
>
>
>
>    structure(list(family = famname, link = link, linkfun = linkfun,
>
>        linkinv = linkinv, variance = variance, dev.resids = dev.resids,
>
>        aic = aic, mu.eta =mu.eta, initialize = initialize,
>
>        validmu = validmu), class = "family")
> }
>
> Thank you for your kind attention.
> Emmanuel
>
>
> EMMANUEL NJI ABATIH
> Rues des deux Eglises 140
> 1210 St Josse-Ten-Node, Bruxelles, BELGIUM
> MOBLIE: 0032486958988(anytime)
> Fix: 0032 2642 5038(8am to 6pm)
> http://www.iph.fgov.be/epidemio/epien
>
> ---------------------------------
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From lecoutre at stat.ucl.ac.be  Tue Jun 14 16:41:20 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 14 Jun 2005 16:41:20 +0200
Subject: [R] c(recursive=TRUE)
Message-ID: <01c701c570ef$21579610$6e8b6882@didacdom.stat.ucl.ac.be>



Hi R users,

I am currently using c(...,recursive=TRUE) to handle list-structured
objects.
This allows to represent something like:

> l1 = list(level1=1,level2=list(sub1=1,sub2=2))
as:
> (l1names = names(c(l1,recursive=TRUE)))
[1] "level1"      "level2.sub1" "level2.sub2"

Then, one can use:
> (l1names = sapply(l1names,FUN=function(element)
strsplit(element,"\\.")))
$level1
[1] "level1"

$level2.sub1
[1] "level2" "sub1"  

$level2.sub2
[1] "level2" "sub2"  


Which allows to access to individuals terminal nodes of list using
indexing, as 
l1[[c("level2","sub2")]] nicely works in R.

For my very specific uses, it unfortunatelt happens that some elements
of my list do have names that contains a dot (for example:
decimal.mark). This is very frustating as a consequence is that my
function does not work in that case...

One workaround would be to add an argument to c(), as delimitor='.' by
default, allowing to put a less usual delimitor (say "??"). But this
would mean changing in C (feature request?). Same for "unlist".

Is there any other easy way to get the structure of a list?

Eric




Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte



From tlumley at u.washington.edu  Tue Jun 14 16:49:39 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 14 Jun 2005 07:49:39 -0700 (PDT)
Subject: [R] use of gam
In-Reply-To: <20050613225101.85868.qmail@web51806.mail.yahoo.com>
References: <20050613225101.85868.qmail@web51806.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0506140741470.92116@homer03.u.washington.edu>

On Mon, 13 Jun 2005, Kerry Bush wrote:

>
> Suppose I fit the following model:
>
>> library(gam)
> ....
>> fit <- gam(y~x1+x2+s(x3),family=binomial)
>
> and then I use
>
>> fitf$coef
>       x1        x2     s(x3)
> 4.1947460 2.7967200 0.0788252
>
> are the coefficients for x1 and x2 the estimated
> coefficients?

Yes.

>		 what is the meaning of s(x3)? since this
> is a non-parametric component, it may not belong here
> as a coefficient, am I right?

I believe that it is coefficient of the smooth term if the smooth term is 
standardized to a standard deviation of 1. If you consider the shape of 
the smooth term as fixed, the model looks like a glm().

This representation can be used to compute approximate standard errors for 
the linear terms (the approximation isn't very good if s(x3) has too many 
degrees of freedom, and Trevor Hastie et al have recently worked out a 
consistent estimator of the standard errors).

 	-thomas



From james at bovik.org  Tue Jun 14 17:08:18 2005
From: james at bovik.org (James Salsman)
Date: Tue, 14 Jun 2005 08:08:18 -0700
Subject: [R] plotting confidence intervals for polynomial models? (was Re:
 ordinary polynomial coefficients from orthogonal polynomials?)
In-Reply-To: <Pine.LNX.4.61.0506141525310.32282@gannet.stats>
References: <42AE9D59.6090006@bovik.org>
	<Pine.LNX.4.61.0506141126160.26371@gannet.stats>
	<42AEDFCA.5080107@vanderbilt.edu>
	<Pine.LNX.4.61.0506141525310.32282@gannet.stats>
Message-ID: <42AEF2E2.5070303@bovik.org>

What I really want now are the 95% confidence intervals that I
mistakenly thought that linesHyperb.lm() would provide:

 > pm <- lm(billions ~ I(decade) + I(decade^2) + I(decade^3))
 > library(sfsmisc)
 > linesHyperb.lm(pm)
Error in "names<-.default"(`*tmp*`, value = c("I(decade)", 
"I(decade^2)",  :
         names attribute [3] must be the same length as the vector [1]
 > pm <- lm(billions ~ poly(decade, 3))
 > linesHyperb.lm(pm)
Warning message:
'newdata' had 100 rows but variable(s) found have 5 rows

Shouldn't curve(predict(...), add=TRUE) be able to plot confidence
interval bands?

Prof Brian Ripley wrote:

> Why do `people' need `to deal with' these, anyway.  We have machines to 
> do that.

Getting a 0.98 adjusted R^2 on the first try, compels me to
try to publish the fitted formula.

>>>>> decade <- c(1950, 1960, 1970, 1980, 1990)
>>>>> billions <- c(3.5, 5, 7.5, 13, 40)
>>>>> # source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
>>>>>
>>>>> pm <- lm(billions ~ poly(decade, 3))
>>>>>
>>>>> plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000),
>>>> main="average yearly inflation-adjusted dollar cost of extreme weather
>>>> events worldwide")
>>>>> curve(predict(pm, data.frame(decade=x)), add=TRUE)
>>>>> # output: http://www.bovik.org/storms.gif
>>>>>
>>>>> summary(pm)
>>>>
>>>> Call:
>>>> lm(formula = billions ~ poly(decade, 3))
>>>>
>>>> Residuals:
>>>>      1       2       3       4       5
>>>> 0.2357 -0.9429  1.4143 -0.9429  0.2357
>>>>
>>>> Coefficients:
>>>>                 Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept)        13.800      0.882  15.647   0.0406 *
>>>> poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
>>>> poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
>>>> poly(decade, 3)3    6.483      1.972   3.287   0.1880
>>>> ---
>>>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>>>
>>>> Residual standard error: 1.972 on 1 degrees of freedom
>>>> Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
>>>> F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317



From rmott at well.ox.ac.uk  Tue Jun 14 17:42:16 2005
From: rmott at well.ox.ac.uk (Richard Mott)
Date: Tue, 14 Jun 2005 16:42:16 +0100
Subject: [R] load ing and saving R objects
In-Reply-To: <Pine.LNX.4.61.0506141535120.32282@gannet.stats>
References: <42AED571.8090805@well.ox.ac.uk> <42AEDAA5.6050101@lancaster.ac.uk>
	<Pine.LNX.4.61.0506141535120.32282@gannet.stats>
Message-ID: <42AEFAD8.3060300@well.ox.ac.uk>

Thanks everyone for help on this. It looks like the solution is a 
zillion files.

Richard

-- 
----------------------------------------------------
Richard Mott       | Wellcome Trust Centre 
tel 01865 287588   | for Human Genetics
fax 01865 287697   | Roosevelt Drive, Oxford OX3 7BN



From Mugdha.Wagle at STJUDE.ORG  Tue Jun 14 17:42:30 2005
From: Mugdha.Wagle at STJUDE.ORG (Wagle, Mugdha)
Date: Tue, 14 Jun 2005 10:42:30 -0500
Subject: [R] t.test using RSPerl
Message-ID: <F2235647AC878D438F09255C39842FBC0D342BC7@SJMEMXMB03.stjude.sjcrh.local>

Hi,
 
I've just started using R and RSPerl. I have some code as follows:
 
&R::initR("--no-save");
&R::call("t.test", (\@array1, \@array2));
 
where @array1 and @array2 are both 1-dimensional arrays in Perl having 54675 elements each. On execution the output is as follows:
 
Calling R function name `t.test', # arguments: 3
1) Arg type 3
Got a reference to a value 10
Here now!2) Arg type 3
Got a reference to a value 10
Here now!Calling R
t.test(c(0, 6.24280675278087, 6.35175793656943, 5.76925805661511,
7.0789316246711, 7.4636498661157, 8.13730810691084, 8.78203131644273,
9.64502765609435, 9.95631242346133, 5.83129579495516, 6.8798700754926,
7.31814159140937.......(REST OF THE ARRAY ELEMENTS).....
4.91632461462501, 3.38099467434464,
3.91800507710569, 3.23867845216438, 3.38439026334577, 4.64918707140487,
3.23474917402449, 3.62966009445396, 3.36729582998647, 3.91999117507732
))
Performed the call, result has length 9

My question is : with other functions such as sum and log10, the actual values of the result are displayed. Here the call seems to have worked but the output is not what you get when running t.test directly on the R command prompt..
 
data:  data4[2] and data4[3] 
t = 0.2186, df = 109.847, p-value = 0.8274
alternative hypothesis: true difference in means is not equal to 0 
95 percent confidence interval:
 -3722.830  4645.723 
sample estimates:
mean of x mean of y 
 6185.139  5723.693
 
which is what I had expected, after seeing the outputs in the case of simpler functions like sum. Could anyone please tell me how I can obtain the output I expect(i.e. the same as the command line output....giving values of t, p-value and the means)?
 
Thank you very much for the help!!
 
Sincerely,
Mugdha Wagle
Hartwell Center for Bioinformatics and Biotechnology,
St.Jude Children's Research Hospital, Memphis TN 38105



From hb at maths.lth.se  Tue Jun 14 18:06:34 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 14 Jun 2005 18:06:34 +0200
Subject: [R] t.test using RSPerl
In-Reply-To: <F2235647AC878D438F09255C39842FBC0D342BC7@SJMEMXMB03.stjude.sjcrh.local>
References: <F2235647AC878D438F09255C39842FBC0D342BC7@SJMEMXMB03.stjude.sjcrh.local>
Message-ID: <42AF008A.1010601@maths.lth.se>

This has nothing to do with RSPerl, instead it has to do what kind of 
object you obtain and how these are print():ed. Typing the name of an 
object, say, 'res', at R prompt and pressing ENTER;

 > res

is equivalent as typing

 > print(res)

This is for convenience to the user.  Basically, this is why you can do

 > 1+1
[1] 2

without having to do

 > print(1+1)
[1] 2

When you "transfer" the object from R to Perl you will, as expect, only 
receive the value of 'res', not the output from 'print(res)'. Here is an 
example illustrating the behavior in R:

 > res <- t.test(1:10,y=c(7:20))
 > length(res)
[1] 9
 > str(res)
List of 9
  $ statistic  : Named num -5.43
   ..- attr(*, "names")= chr "t"
  $ parameter  : Named num 22
   ..- attr(*, "names")= chr "df"
  $ p.value    : num 1.86e-05
  $ conf.int   : atomic [1:2] -11.05  -4.95
   ..- attr(*, "conf.level")= num 0.95
  $ estimate   : Named num [1:2] 5.5 13.5
   ..- attr(*, "names")= chr [1:2] "mean of x" "mean of y"
  $ null.value : Named num 0
   ..- attr(*, "names")= chr "difference in means"
  $ alternative: chr "two.sided"
  $ method     : chr "Welch Two Sample t-test"
  $ data.name  : chr "1:10 and c(7:20)"
  - attr(*, "class")= chr "htest"
 > print(res)

         Welch Two Sample t-test

data:  1:10 and c(7:20)
t = -5.4349, df = 21.982, p-value = 1.855e-05
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  -11.052802  -4.947198
sample estimates:
mean of x mean of y
       5.5      13.5

In your case, to capture what print(res) is outputting, you may want to 
look at ?capture.out, that is

 > output <- capture.output(res)
 > output  # ...that is, print(output)
  [1] ""
  [2] "\tWelch Two Sample t-test"
  [3] ""
  [4] "data:  1:10 and c(7:20) "
  [5] "t = -5.4349, df = 21.982, p-value = 1.855e-05"
  [6] "alternative hypothesis: true difference in means is not equal to 0 "
  [7] "95 percent confidence interval:"
  [8] " -11.052802  -4.947198 "
  [9] "sample estimates:"
[10] "mean of x mean of y "
[11] "      5.5      13.5 "
[12] ""

and transfer 'output' to Perl.

Cheers

Henrik

Wagle, Mugdha wrote:
> Hi,
>  
> I've just started using R and RSPerl. I have some code as follows:
>  
> &R::initR("--no-save");
> &R::call("t.test", (\@array1, \@array2));
>  
> where @array1 and @array2 are both 1-dimensional arrays in Perl having 54675 elements each. On execution the output is as follows:
>  
> Calling R function name `t.test', # arguments: 3
> 1) Arg type 3
> Got a reference to a value 10
> Here now!2) Arg type 3
> Got a reference to a value 10
> Here now!Calling R
> t.test(c(0, 6.24280675278087, 6.35175793656943, 5.76925805661511,
> 7.0789316246711, 7.4636498661157, 8.13730810691084, 8.78203131644273,
> 9.64502765609435, 9.95631242346133, 5.83129579495516, 6.8798700754926,
> 7.31814159140937.......(REST OF THE ARRAY ELEMENTS).....
> 4.91632461462501, 3.38099467434464,
> 3.91800507710569, 3.23867845216438, 3.38439026334577, 4.64918707140487,
> 3.23474917402449, 3.62966009445396, 3.36729582998647, 3.91999117507732
> ))
> Performed the call, result has length 9
> 
> My question is : with other functions such as sum and log10, the actual values of the result are displayed. Here the call seems to have worked but the output is not what you get when running t.test directly on the R command prompt..
>  
> data:  data4[2] and data4[3] 
> t = 0.2186, df = 109.847, p-value = 0.8274
> alternative hypothesis: true difference in means is not equal to 0 
> 95 percent confidence interval:
>  -3722.830  4645.723 
> sample estimates:
> mean of x mean of y 
>  6185.139  5723.693
>  
> which is what I had expected, after seeing the outputs in the case of simpler functions like sum. Could anyone please tell me how I can obtain the output I expect(i.e. the same as the command line output....giving values of t, p-value and the means)?
>  
> Thank you very much for the help!!
>  
> Sincerely,
> Mugdha Wagle
> Hartwell Center for Bioinformatics and Biotechnology,
> St.Jude Children's Research Hospital, Memphis TN 38105
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From wilks at dial.pipex.com  Tue Jun 14 18:15:51 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Tue, 14 Jun 2005 17:15:51 +0100
Subject: [R] extracting components of a list
Message-ID: <JCEIJNOHMNBPLMGFDHNDKEIJCAAA.wilks@dial.pipex.com>

Dimitris,

wouldn't this be more precise ---

> sapply(jj,function(x) which(x$b[1]==4))

[[1]]
[1] 1

[[2]]
numeric(0)

[[3]]
[1] 1

 John

Dimitris wrote ---


maybe something like this:

jj <- list(list(a = 1, b = 4:7), list(a = 5, b = 3:6), list(a = 10, b 
= 4:5))
###############
jj[sapply(jj,  function(x) x$b[1] == 4)]


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Robin Hankin" <r.hankin at noc.soton.ac.uk>
To: "r-help" <R-help at stat.math.ethz.ch>
Sent: Monday, June 13, 2005 4:23 PM
Subject: [R] extracting components of a list


> Hi
>
> how do I extract those components of a list that satisfy a certain
> requirement?  If
>
> jj <- list(list(a=1,b=4:7),list(a=5,b=3:6),list(a=10,b=4:5))
>
>
> I want just the components of jj that have b[1] ==4 which in this 
> case
> would be the first and
> third of jj, viz    list (jj[[1]],jj[[3]]).
>
> How to do this efficiently?
>
> My only idea was to loop through jj, and set unwanted components to
> NULL, but
> FAQ 7.1 warns against this.
>
>
>
>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>



From ville.koskinen at matrex.fi  Tue Jun 14 18:22:56 2005
From: ville.koskinen at matrex.fi (Ville Koskinen)
Date: Tue, 14 Jun 2005 19:22:56 +0300
Subject: [R] Logistic regression with more than two choices
In-Reply-To: <mailman.11.1118743201.15147.r-help@stat.math.ethz.ch>
Message-ID: <20050614162426.4DF61302DB@smtp2.song.fi>

Dear all R-users,

I am a new user of R and I am trying to build a discrete choice model (with
more than two alternatives A, B, C and D) using logistic regression. I have
data that describes the observed choice probabilities and some background
information. An example below describes the data:

Sex	Age	pr(A)	pr(B)	pr(C)	pr(D) ...
1	11	0.5	0.5 	0	0
1	40	1	0	0	0
0	34	0	0	0	1
0	64	0.1	0.5	0.2	0.2
...

I have been able to model a case with only two alternatives "A" and "not A"
by using glm(). 

I do not know what functions are available to estimate such a model with
more than two alternatives. Multinom() is one possibility, but it only
allows the use of binary 0/1-data instead of observed probabilities. Did I
understand this correctly?

Additionally, I am willing to use different independent variables for the
different alternatives in the model. Formally, I mean that:
Pr(A)=exp(uA)/(exp(uA)+exp(uB)+exp(uC)+exp(uD)
Pr(B)=exp(uB)/(exp(uA)+exp(uB)+exp(uC)+exp(uD)
...
where uA, uB, uC and uD are linear functions with different independent
variables, e.g. uA=alpha_A1*Age, uB=alpha_B1*Sex. 

Do you know how to estimate this type of models in R? 

Best regards, Ville Koskinen



From gilles.guillot at inapg.inra.fr  Tue Jun 14 18:23:58 2005
From: gilles.guillot at inapg.inra.fr (Gilles GUILLOT)
Date: Tue, 14 Jun 2005 18:23:58 +0200
Subject: [R] Calling C from Fortran
Message-ID: <200506141823.58603.gilles.guillot@inapg.inra.fr>

I would like to call C routines from Fortran as suggested in section 5.6 of 
the "Writing R extensions" documentation.

I'm familiar with Fortran but not with C.
I understand the example provided in Fortran:

subroutine testit()
double precision normrnd, x
call rndstart()
x = normrnd()
call dblepr("X was", 5, x, 1)
call rndend()
end


but I don't understand the purpose of this C wrapper:
#include <R.h>  
 void F77_SUB(rndstart)(void) { GetRNGstate(); }
 void F77_SUB(rndend)(void) { PutRNGstate(); }
 double F77_SUB(normrnd)(void) { return norm_rand(); }

neither how I should compile it.

Could anyone explain how I should compile and link 
the C and Fortran files above, and call the Fortran subroutine from R.

Thanks,

Gilles



From bld at math.umd.edu  Tue Jun 14 18:27:54 2005
From: bld at math.umd.edu (Bernard L. Dillard)
Date: Tue, 14 Jun 2005 12:27:54 -0400 (EDT)
Subject: [R] Dateticks
Message-ID: <40921.66.92.23.42.1118766474.squirrel@66.92.23.42>

Hello.  I am having the worst time converting x-axis date ticks to real
dates.  I have tried several suggestions in online help tips and books to
no avail.

For example, the x-axis has 0, 50, 100, etc, and I want it to have
"6/17/03", "8/6/03" etc.  See attached (sample).

Can anybody help me with this.

Here's my code:

ts.plot(date.attackmode.table[,1], type="l", col="blue", lty=2,ylab="IED
     Attacks", lwd=2,xlab="Attack Dates",main="Daily Summary of Attack
     Mode")
grid()

Thanks for your help if possible.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample.pdf
Type: application/pdf
Size: 9873 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050614/c781e058/sample.pdf

From br44114 at gmail.com  Tue Jun 14 18:34:34 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Tue, 14 Jun 2005 12:34:34 -0400
Subject: [R] load ing and saving R objects
Message-ID: <8d5a3635050614093438361c6f@mail.gmail.com>

> On Tue, 14 Jun 2005, Prof Brian Ripley wrote:
> If your file system does not like 15000 files you can always 
> save in a DBMS.

Or, switch to a better/more appropriate file system:
http://en.wikipedia.org/wiki/Comparison_of_file_systems
ReiserFS would allow you to store up to about 1.2 million files in a directory.


> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Tuesday, June 14, 2005 10:41 AM
> To: Barry Rowlingson
> Cc: r-help at stat.math.ethz.ch; Richard Mott
> Subject: Re: [R] load ing and saving R objects
> 
> 
> On Tue, 14 Jun 2005, Barry Rowlingson wrote:
> 
> > Richard Mott wrote:
> >> Does anyone know a way to do the following:
> >>
> >> Save a large number of R objects to a file (like load() 
> does) but then
> >> read back only a small named subset of them . As far as I can see,
> >> load() reads back everything.
> >
> >  Save them to individual files when you generate them?
> >
> >  for(i in 1:15000){
> >
> >   m=generateBigMatrix(i)
> >
> >   filename=paste("BigMatrix-",i,".Rdata",sep='')
> >   save(m,file=filename)
> >  }
> >
> > Note that load will always overwrite 'm', so to load a 
> sample of them in
> > you'll need to do something like this:
> >
> >  bigSamples=list()
> >
> >  for(i in sample(15000,N)){
> >    filename=paste("BigMatrix-",i,".Rdata",sep='')
> >    load(filename)
> >    bigSamples[[i]]=m
> >  }
> >
> >  But there may be a more efficient way to string up a big list like
> > that, I can never remember - get it working, then worry 
> about optimisation.
> 
> (Yes, use bigSamples <- vector("list", 15000) first.)
> 
> >  I hope your filesystem is happy with 15000 objects in it. I would
> > dedicate a folder or directory for just these objects' 
> files, since it
> > then becomes near impossible to see anything other than the 
> big matrix
> > files...
> 
> .readRDS/.saveRDS might be a better way to do this, and avoids always 
> restoring to "m".
> 
> If your file system does not like 15000 files you can always 
> save in a 
> DBMS.
> 
> I did once look into restoring just some of the objects in a save()ed 
> file, but it is not really possible to do so efficiently due 
> to sharing 
> between objects.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From faceasec at uapar.edu  Tue Jun 14 18:35:22 2005
From: faceasec at uapar.edu (secretario academico FACEA)
Date: Tue, 14 Jun 2005 13:35:22 -0300
Subject: [R] plots
Message-ID: <42AF074A.6030608@uapar.edu>

Dear all,
Is it possible to change the levels in a mosaic plot, the appearance of 
the level or the levels size?
For instance:
    A               C                  E
             B                  D

Thanks for your help
Adri??n

From obakkalbasi at chainalytics.com  Tue Jun 14 18:40:17 2005
From: obakkalbasi at chainalytics.com (Omer Bakkalbasi)
Date: Tue, 14 Jun 2005 12:40:17 -0400
Subject: [R] KMEANS output...
Message-ID: <000c01c570ff$c3455740$c07ba8c0@chaingang.local>

Using R 2.1.0 on Windows

2 questions:
1. Is there a way to parse the output from kmeans within R?
2. If the answer to 1. is convoluted or impossible, how do you save the
output from kmeans in a plain text file for further processing outside R?
 
Example:
> ktx<-kmeans(x,12, nstart = 200)
I would like to parse ktx within R to extract cluster sizes, sum-of-squares
values, etc., OR save ktx in a plain text file in Windows to post-process it
with a parser I would have to write. 

Thanks!

Omer 
Cell: (914) 671-7447



From reid_huntsinger at merck.com  Tue Jun 14 18:48:00 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 14 Jun 2005 12:48:00 -0400
Subject: [R] KMEANS output...
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A948D@uswpmx00.merck.com>

help(kmeans) describes the return value: a list with a) the complete
classification b) the cluster centers c) the within-cluster sums of squares
b) the cluster sizes. Thankfully, you don't need any parsing.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Omer Bakkalbasi
Sent: Tuesday, June 14, 2005 12:40 PM
To: r-help at stat.math.ethz.ch
Subject: [R] KMEANS output...


Using R 2.1.0 on Windows

2 questions:
1. Is there a way to parse the output from kmeans within R?
2. If the answer to 1. is convoluted or impossible, how do you save the
output from kmeans in a plain text file for further processing outside R?
 
Example:
> ktx<-kmeans(x,12, nstart = 200)
I would like to parse ktx within R to extract cluster sizes, sum-of-squares
values, etc., OR save ktx in a plain text file in Windows to post-process it
with a parser I would have to write. 

Thanks!

Omer 
Cell: (914) 671-7447

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From antoniou at central.ntua.gr  Tue Jun 14 18:50:45 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Tue, 14 Jun 2005 19:50:45 +0300
Subject: [R] problem installing packages with compiled-from-source R.app on
	Mac OS X - Tiger
Message-ID: <E0CEA60D-026D-4EC1-A6E2-E765F19207D0@central.ntua.gr>

Hello all,

This may be aimed for r-devel, but I encountered this as an R-user  
and not an R-developer so I start here (having said that, please  
direct me to R-devel if you think this is appropriate. I am not cross- 
posting, as I believe this is bad netiquette).

I am a recent, but extremely happy R-user (especially after getting  
my own copy of MASS 2002). My adventures started when I wanted to use  
Rpy to use R also from within python. I compiled R (2.1.0) after  
configuring it as follows:

./configure --enable-R-shlib --with-blas='-framework vecLib' --with- 
lapack --with-aqua

where --enable-R-shlib is required by Rpy.

I then compiled Rpy-0.4.2.1 and I was able to call R from within  
python. So far so good...

I then -naively- tried to start R.app. After less than a bounce on  
the dock... it would not start. [Sorry, I do not remember what  
problem was showing up in the console...]

So, I figured I had to also compile R.app. I got the code via:

svn co https://svn.r-project.org/R-packages/trunk/Mac-GUI Mac- 
GUI      (executed this command yesterday)

and compiled it as per the instructions... And R.app launches just fine.

Now, the issue comes when I try to install a(ny) package (via the GUI  
package installer of R.app). I then get the following message:

"Package installation failed. Package installation was not  
successful. Please see the R console for details"

And the R console says:

"Error in install.packages(c("dyn"), lib = "/Library/Frameworks/ 
R.framework/Resources/library",  :
     couldn't find function ".install.macbinary""

I see that this has been encountered before and resolved (at least  
Prof. Ripley saw what was wrong) as per the following thread:

https://stat.ethz.ch/pipermail/r-devel/2005-May/033115.html

But I am not sure what needs to be done on my part so that I am not  
affected from it.

 > str(.Platform)
List of 6
$ OS.type   : chr "unix"
$ file.sep  : chr "/"
$ dynlib.ext: chr ".so"
$ GUI       : chr "X11"
$ endian    : chr "big"
$ pkgType   : chr "source"
 > R.version.string
[1] "R version 2.1.0, 2005-04-18"


gcc version 4.0.0

[Please let me know what other information may be relevant]


Thanking you in advance,

Costas


--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From james.holtman at convergys.com  Tue Jun 14 19:10:24 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue, 14 Jun 2005 13:10:24 -0400
Subject: [R] Dateticks
In-Reply-To: <40921.66.92.23.42.1118766474.squirrel@66.92.23.42>
Message-ID: <OF784C2734.D402600C-ON85257020.005E383F-85257020.005E5612@nd.convergys.com>





try this example:

> x.1 <- strptime("6/17/03",'%m/%d/%y')
> x.1
[1] "2003-06-17"
> plot(0:250, xaxt='n')
> dates <- x.1 + c(0,50,100,150,200,250) * 86400  # 'dates' is in seconds,
so add the appropriate number of days
> dates
[1] "2003-06-17 00:00:00 EDT" "2003-08-06 00:00:00 EDT" "2003-09-25
00:00:00 EDT"
[4] "2003-11-13 23:00:00 EST" "2004-01-02 23:00:00 EST" "2004-02-21
23:00:00 EST"
> axis(1, at=c(0,50,100,150,200,250), labels=format(dates,"%m/%d/%y"))  #
format the output
>

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Bernard L. Dillard"                                                                                                 
                      <bld at math.umd.edu>           To:       r-help at stat.math.ethz.ch                                                      
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] Dateticks                                                                 
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/14/2005 12:27                                                                                                     
                                                                                                                                           




Hello.  I am having the worst time converting x-axis date ticks to real
dates.  I have tried several suggestions in online help tips and books to
no avail.

For example, the x-axis has 0, 50, 100, etc, and I want it to have
"6/17/03", "8/6/03" etc.  See attached (sample).

Can anybody help me with this.

Here's my code:

ts.plot(date.attackmode.table[,1], type="l", col="blue", lty=2,ylab="IED
     Attacks", lwd=2,xlab="Attack Dates",main="Daily Summary of Attack
     Mode")
grid()

Thanks for your help if possible.
(See attached file: sample.pdf)
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample.pdf
Type: application/pdf
Size: 9874 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20050614/24bb6b3f/sample.pdf

From Scott.Waichler at pnl.gov  Tue Jun 14 19:38:02 2005
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 14 Jun 2005 10:38:02 -0700
Subject: [R] using forecast() in dse2 with an ARMA model having a trend
	component
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A02173601@pnlmse35.pnl.gov>

(My apologies if this is a repeated posting.  I couldn't find any trace
of my previous attempt in the archive.)  

I'm having trouble with forecast() in the dse2 package.  It works fine
for me on a model without a trend, but gives me NaN output for the
forecast values when using a model with a trend.  An example:

# Set inputs and outputs for the ARMA model fit and test periods
arma.fit.input <- c(105.3332, 105.3573, 105.3113, 105.1493, 105.1209,
105.2111, 104.9161,
                    105.3654, 105.4682, 105.6789, 105.6297, 106.0155,
105.8454, 105.4322,
                    105.6062, 106.0739, 106.1109, 105.4470, 104.9739,
105.3427, 105.4305,
                    105.2563, 104.8501, 105.0358, 105.2827, 104.8977)

arma.fit.output <- c(106.0376, 106.0514, 106.0716, 106.0570, 106.0442,
106.0414, 106.0375,
                     106.0169, 106.0268, 106.0670, 106.1169, 106.1544,
106.1898, 106.2252,
                     106.2605, 106.2959, 106.3324, 106.3974, 106.3460,
106.2357, 106.1897,
                     106.1811, 106.1556, 106.1130, 106.0805, 106.0791)

arma.pred.input <- c(104.9916, 104.8207, 104.8936, 104.8767, 104.9435,
104.8885, 104.9217,
                     104.9029, 104.9508, 105.0065, 105.0557, 105.1982,
105.3392, 105.4007,
                     105.6212, 105.5979, 105.2410, 105.4832, 105.8735,
105.5944, 105.1063,
                     104.9809, 105.0821, 104.9362, 105.3037, 105.2322)
arma.pred.output <- c(106.0528, 106.0293, 106.0053, 105.9850, 105.9697,
105.9604, 105.9509,
                      105.9430, 105.9357, 105.9314, 105.9333, 105.9420,
105.9640, 105.9994,
                      106.0290, 106.0855, 106.1265, 106.1197, 106.1245,
106.1893, 106.2118,
                      106.1503, 106.0883, 106.0511, 106.0194, 106.0221)

# Set TSdata object
arma.fit.TSdata <- TSdata(input = arma.fit.input, output =
arma.fit.output)

# Fit the model
arma.model.without.trend <- estVARXls(arma.fit.TSdata, max.lag=1,
trend=F)
arma.model.with.trend    <- estVARXls(arma.fit.TSdata, max.lag=1,
trend=T)

# Apply the model for the test period
arma.pred.TSdata <- TSdata(input = arma.pred.input, output =
arma.pred.output[1:2]) arma.pred.without.trend <-
forecast(TSmodel(arma.model.without.trend), arma.pred.TSdata)
arma.pred.with.trend    <- forecast(TSmodel(arma.model.with.trend),
arma.pred.TSdata)

The results:
> arma.pred.without.trend$forecast[[1]][,1]
 [1] 106.0038 105.9789 105.9605 105.9396 105.9224 105.9052 105.8926
105.8849  [9] 105.8812 105.8880 105.9043 105.9240 105.9579 105.9878
105.9901 106.0095 [17] 106.0555 106.0782 106.0644 106.0427 106.0297
106.0072 106.0126 106.0125
> arma.pred.with.trend$forecast[[1]][,1]
 [1] 5.76056e+228          NaN          NaN          NaN          NaN
 [6]          NaN          NaN          NaN          NaN          NaN
[11]          NaN          NaN          NaN          NaN          NaN
[16]          NaN          NaN          NaN          NaN          NaN
[21]          NaN          NaN          NaN          NaN

I read help on this function and the PDF manuals but can't see what I
might be missing.  
Any ideas?  

Thanks, Sott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From 0034058 at fudan.edu.cn  Tue Jun 14 19:43:26 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 15 Jun 2005 01:43:26 +0800
Subject: [R] any function to calculate the lamda coefficient?
Message-ID: <0II300CAH4QLNE@mail.fudan.edu.cn>

i just write one for myself,but the result is different from the spss's.
i anyone can send me a copy of function calc the  lamda coefficient?(i know i should google first,but right now i can surf the internet.)


> lamda
function(table){
     obs1.y<-sum(apply(table,1,max))
     obs2.y<-max(apply(table,1,sum))
     obs1.x<-sum(apply(table,2,max))
     obs2.x<-max(apply(table,2,sum))
     D.x<-(sum(table)-obs2.x)
     D.y<-(sum(table)-obs2.y)
lamda.y<-(obs1.y-obs2.y)/D.y
lamda.x<-(obs1.x-obs2.x)/D.x
    wt<-c(D.x,D.y)
lamda.xy<-weighted.mean(c(lamda.x,lamda.y),wt/sum(wt))
cat(
"the lamda.y coefficient is :",lamda.y,";","the lamda.x coefficient is :",lamda.x,";",
"\n",
"the mean lamda coefficient is :",lamda.xy,".",fill=T)
}

> a<-matrix(c(44,20,54,8),2)
> a
     [,1] [,2]
[1,]   44   54
[2,]   20    8
> lamda(a)
the lamda.y coefficient is : -0.8571429 ; the lamda.x coefficient is : 
0.5483871 ; 
 the mean lamda coefficient is : 0.1111111 .

	



 				


2005-06-15

------
Deparment of Sociology
Fudan University

Blog:www.sociology.yculblog.com



From Camarda at demogr.mpg.de  Tue Jun 14 20:00:32 2005
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Tue, 14 Jun 2005 20:00:32 +0200
Subject: [R] Plotting rows (or columns) from a matrix in different graphs,
 not    using "par"
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050614/e904acc0/attachment.pl

From phiberoptic_br at yahoo.com.br  Tue Jun 14 20:01:15 2005
From: phiberoptic_br at yahoo.com.br (Anderson de Rezende Rocha)
Date: Tue, 14 Jun 2005 15:01:15 -0300 (ART)
Subject: [R] How to fix false positve rates?
Message-ID: <20050614180115.76757.qmail@web60423.mail.yahoo.com>

Dear R-users, 

I have a set of 12000 image samples. I can divide this set into two
categories: training and testing. I need to classify the test set into
a two qualitative outputs: true or false for some characteristic. 

To do the classification I'm using the packages SVM in e1071 library
and LDA in the MASS library. However, I'm with a great number of FALSE
POSITIVE CASES in both classifiers, about 10/15%. 

1) How can I use these classifiers with a fixed 1% FALSE POSITIVE RATE?

2) Could anynone indicate me a non-linear SVM classifier R-package?
3) Could anynone indicate me an ADA-BOOST classifier R-package?

Thanks for all. I'm anxiously wait the answers. Best regards. 

*************************************************
|????????????????| - Anderson de Rezende Rocha
Bacharel em Ci??ncia da Computa????o - UFLA
Mestrando em Ci??ncia da Computa????o - UNICAMP
Esteganografia e estegan??lise digital
UNIVERSIDADE ESTADUAL DE CAMPINAS, SP - BRASIL
< http://andersonrocha.cjb.net >



From sfalcon at fhcrc.org  Tue Jun 14 20:18:02 2005
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Tue, 14 Jun 2005 11:18:02 -0700
Subject: [R] ANN: BioC2005 Conference scheduled for August in Seattle
Message-ID: <m2mzpsssyt.fsf@macaroni.local>

==================================
           BioC2005
Where Software and Biology Connect
==================================

http://bioconductor.org/meeting05/


About BioC2005
==============

This conference will highlight current developments within and beyond
Bioconductor, a world-wide open source and open development software
project for the analysis and comprehension of genomic data.

Our goal is to provide a forum in which to discuss the use and design
of software for analyzing data arising in biology with a focus on
Bioconductor and genomic data.

Where: Fred Hutchinson Cancer Research Center
       Seattle Wa.

When: August 15 and 16, 2005

What: Morning Talks: 8:30-12:00
      Afternoon Practicals: 2:00-5:00

      Tuesday Evening 5:00-7:30 Posters and Wine & Cheese

Fees: 250 USD for academic/research institute attendees
      125 USD for enrolled full-time students

DETAILS: http://bioconductor.org/meeting05/



From HankeA at mar.dfo-mpo.gc.ca  Tue Jun 14 20:38:34 2005
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Tue, 14 Jun 2005 15:38:34 -0300
Subject: [R] Survfit,newdata and continuous time varying covariates
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE02008AC4@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050614/232709c4/attachment.pl

From drf5n at maplepark.com  Tue Jun 14 20:55:24 2005
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 14 Jun 2005 13:55:24 -0500 (CDT)
Subject: [R] Plotting quiver vector tensor arrows 2d field data
Message-ID: <Pine.LNX.4.58.0506141308320.25059@maplepark.com>

Hi All,

I'd like to plot something like
http://www.nawcwpns.navy.mil/~weather/mugu/mesodata/analysis.html

Looking through the galleries at
 http://addictedtor.free.fr/graphiques/allgraph.php
 http://r-spatial.sourceforge.net/gallery/
 http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?GraphGallery

 demo(graphics)

I did not find a function to plot a 2d field on a matrix.  I did find
mention of a quiver function in the archives.  Is this the best solution
or are there other tools I missed?

quiver<- function(u,v,scale=1,length=0.05)
# first stab at matlab's quiver in R
# from http://tolstoy.newcastle.edu.au/R/help/01c/2711.html
# Robin Hankin Tue 20 Nov 2001 - 13:10:28 EST
  {
    xpos <- col(u)
    ypos <- max(row(u))-row(u)

    speed <- sqrt(u*u+v*v)
    maxspeed <- max(speed)

    u <- u*scale/maxspeed
    v <- v*scale/maxspeed

    matplot(xpos,ypos,type="p",cex=0)
    arrows(xpos,ypos,xpos+u,ypos+v,length=length*min(par.uin()))
  }

par.uin <- function()
  # determine scale of inches/userunits in x and y
  # from http://tolstoy.newcastle.edu.au/R/help/01c/2714.html
  # Brian Ripley Tue 20 Nov 2001 - 20:13:52 EST
{
    u <- par("usr")
    p <- par("pin")
    c(p[1]/(u[2] - u[1]), p[2]/(u[4] - u[3]))
}

u <- matrix(rnorm(100),nrow=10)
v <- matrix(rnorm(100),nrow=10)
quiver(u,v)

I added these functions as an example to the Wiki:
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?GraphGallery
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?QuiverPlot

Thanks for your time,
Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From pgilbert at bank-banque-canada.ca  Tue Jun 14 21:10:03 2005
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 14 Jun 2005 15:10:03 -0400
Subject: [R] using forecast() in dse2 with an ARMA model having a trend
 component
In-Reply-To: <7E4C06F49D6FEB49BE4B60E5FC92ED7A02173601@pnlmse35.pnl.gov>
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A02173601@pnlmse35.pnl.gov>
Message-ID: <42AF2B8B.3070702@bank-banque-canada.ca>

Scott

This works for me:

 > arma.pred.without.trend$forecast[[1]][,1]
 [1] 106.0038 105.9789 105.9605 105.9396 105.9224 105.9052 105.8926 105.8849
 [9] 105.8812 105.8880 105.9043 105.9240 105.9579 105.9878 105.9901 106.0095
[17] 106.0555 106.0782 106.0644 106.0427 106.0297 106.0072 106.0126 106.0125
 > arma.pred.with.trend$forecast[[1]][,1]
 [1] 101.49566  97.46626  93.89069  90.70991  87.88602  85.37563  83.14843
 [8]  81.17338  79.42193  77.87562  76.51150  75.30371  74.24589  73.30438
[15]  72.44303  71.69453  71.05658  70.47036  69.91559  69.41396  68.97527
[22]  68.57536  68.24555  67.94755

There was a bug that may be related to this fixed in the version 
dse_2005.4-1, which has been on CRAN for a month or so.  The above 
result was actually with my working copy, but I don't recall any more 
recent changes that would affect this.  If you actually have the 
2005.4-1 version and are still having this problem then please let me 
know and I will check more carefully.  (In that case, OS details would 
be helpful too.)

Paul Gilbert

Waichler, Scott R wrote:

>(My apologies if this is a repeated posting.  I couldn't find any trace
>of my previous attempt in the archive.)  
>
>I'm having trouble with forecast() in the dse2 package.  It works fine
>for me on a model without a trend, but gives me NaN output for the
>forecast values when using a model with a trend.  An example:
>
># Set inputs and outputs for the ARMA model fit and test periods
>arma.fit.input <- c(105.3332, 105.3573, 105.3113, 105.1493, 105.1209,
>105.2111, 104.9161,
>                    105.3654, 105.4682, 105.6789, 105.6297, 106.0155,
>105.8454, 105.4322,
>                    105.6062, 106.0739, 106.1109, 105.4470, 104.9739,
>105.3427, 105.4305,
>                    105.2563, 104.8501, 105.0358, 105.2827, 104.8977)
>
>arma.fit.output <- c(106.0376, 106.0514, 106.0716, 106.0570, 106.0442,
>106.0414, 106.0375,
>                     106.0169, 106.0268, 106.0670, 106.1169, 106.1544,
>106.1898, 106.2252,
>                     106.2605, 106.2959, 106.3324, 106.3974, 106.3460,
>106.2357, 106.1897,
>                     106.1811, 106.1556, 106.1130, 106.0805, 106.0791)
>
>arma.pred.input <- c(104.9916, 104.8207, 104.8936, 104.8767, 104.9435,
>104.8885, 104.9217,
>                     104.9029, 104.9508, 105.0065, 105.0557, 105.1982,
>105.3392, 105.4007,
>                     105.6212, 105.5979, 105.2410, 105.4832, 105.8735,
>105.5944, 105.1063,
>                     104.9809, 105.0821, 104.9362, 105.3037, 105.2322)
>arma.pred.output <- c(106.0528, 106.0293, 106.0053, 105.9850, 105.9697,
>105.9604, 105.9509,
>                      105.9430, 105.9357, 105.9314, 105.9333, 105.9420,
>105.9640, 105.9994,
>                      106.0290, 106.0855, 106.1265, 106.1197, 106.1245,
>106.1893, 106.2118,
>                      106.1503, 106.0883, 106.0511, 106.0194, 106.0221)
>
># Set TSdata object
>arma.fit.TSdata <- TSdata(input = arma.fit.input, output =
>arma.fit.output)
>
># Fit the model
>arma.model.without.trend <- estVARXls(arma.fit.TSdata, max.lag=1,
>trend=F)
>arma.model.with.trend    <- estVARXls(arma.fit.TSdata, max.lag=1,
>trend=T)
>
># Apply the model for the test period
>arma.pred.TSdata <- TSdata(input = arma.pred.input, output =
>arma.pred.output[1:2]) arma.pred.without.trend <-
>forecast(TSmodel(arma.model.without.trend), arma.pred.TSdata)
>arma.pred.with.trend    <- forecast(TSmodel(arma.model.with.trend),
>arma.pred.TSdata)
>
>The results:
>  
>
>>arma.pred.without.trend$forecast[[1]][,1]
>>    
>>
> [1] 106.0038 105.9789 105.9605 105.9396 105.9224 105.9052 105.8926
>105.8849  [9] 105.8812 105.8880 105.9043 105.9240 105.9579 105.9878
>105.9901 106.0095 [17] 106.0555 106.0782 106.0644 106.0427 106.0297
>106.0072 106.0126 106.0125
>  
>
>>arma.pred.with.trend$forecast[[1]][,1]
>>    
>>
> [1] 5.76056e+228          NaN          NaN          NaN          NaN
> [6]          NaN          NaN          NaN          NaN          NaN
>[11]          NaN          NaN          NaN          NaN          NaN
>[16]          NaN          NaN          NaN          NaN          NaN
>[21]          NaN          NaN          NaN          NaN
>
>I read help on this function and the PDF manuals but can't see what I
>might be missing.  
>Any ideas?  
>
>Thanks, Sott Waichler
>Pacific Northwest National Laboratory
>scott.waichler at pnl.gov
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ripley at stats.ox.ac.uk  Tue Jun 14 21:13:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Jun 2005 20:13:17 +0100 (BST)
Subject: [R] problem installing packages with compiled-from-source R.app
 on Mac OS X - Tiger
In-Reply-To: <E0CEA60D-026D-4EC1-A6E2-E765F19207D0@central.ntua.gr>
References: <E0CEA60D-026D-4EC1-A6E2-E765F19207D0@central.ntua.gr>
Message-ID: <Pine.LNX.4.61.0506142012100.5289@gannet.stats>

You need to use R-patched, nowadays 2.1.1 beta, not the released R 2.1.0.
As far as I know it is fixed there.

On Tue, 14 Jun 2005, Constantinos Antoniou wrote:

> Hello all,
>
> This may be aimed for r-devel, but I encountered this as an R-user
> and not an R-developer so I start here (having said that, please
> direct me to R-devel if you think this is appropriate. I am not cross-
> posting, as I believe this is bad netiquette).
>
> I am a recent, but extremely happy R-user (especially after getting
> my own copy of MASS 2002). My adventures started when I wanted to use
> Rpy to use R also from within python. I compiled R (2.1.0) after
> configuring it as follows:
>
> ./configure --enable-R-shlib --with-blas='-framework vecLib' --with-
> lapack --with-aqua
>
> where --enable-R-shlib is required by Rpy.
>
> I then compiled Rpy-0.4.2.1 and I was able to call R from within
> python. So far so good...
>
> I then -naively- tried to start R.app. After less than a bounce on
> the dock... it would not start. [Sorry, I do not remember what
> problem was showing up in the console...]
>
> So, I figured I had to also compile R.app. I got the code via:
>
> svn co https://svn.r-project.org/R-packages/trunk/Mac-GUI Mac-
> GUI      (executed this command yesterday)
>
> and compiled it as per the instructions... And R.app launches just fine.
>
> Now, the issue comes when I try to install a(ny) package (via the GUI
> package installer of R.app). I then get the following message:
>
> "Package installation failed. Package installation was not
> successful. Please see the R console for details"
>
> And the R console says:
>
> "Error in install.packages(c("dyn"), lib = "/Library/Frameworks/
> R.framework/Resources/library",  :
>     couldn't find function ".install.macbinary""
>
> I see that this has been encountered before and resolved (at least
> Prof. Ripley saw what was wrong) as per the following thread:
>
> https://stat.ethz.ch/pipermail/r-devel/2005-May/033115.html
>
> But I am not sure what needs to be done on my part so that I am not
> affected from it.
>
> > str(.Platform)
> List of 6
> $ OS.type   : chr "unix"
> $ file.sep  : chr "/"
> $ dynlib.ext: chr ".so"
> $ GUI       : chr "X11"
> $ endian    : chr "big"
> $ pkgType   : chr "source"
> > R.version.string
> [1] "R version 2.1.0, 2005-04-18"
>
>
> gcc version 4.0.0
>
> [Please let me know what other information may be relevant]
>
>
> Thanking you in advance,
>
> Costas
>
>
> --
> Constantinos Antoniou, Ph.D.
> Department of Transportation Planning and Engineering
> National Technical University of Athens
> 5, Iroon Polytechniou str. GR-15773, Athens, Greece
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From slist at oomvanlieshout.net  Tue Jun 14 21:20:48 2005
From: slist at oomvanlieshout.net (Sander Oom)
Date: Tue, 14 Jun 2005 21:20:48 +0200
Subject: [R] some suggestion
In-Reply-To: <007e01c570ee$0dcc0910$0540210a@www.domain>
References: <0II200ER1VPT0U@mail.fudan.edu.cn>
	<007e01c570ee$0dcc0910$0540210a@www.domain>
Message-ID: <42AF2E10.5020308@oomvanlieshout.net>

Shame nobody has included this function in their package! Would be
useful to have as a standard function!

Sander.


Dimitris Rizopoulos wrote:
> take a look at this function by Kevin Wright
> 
> RSiteSearch("sort.data.frame")
> 
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- From: "ronggui" <0034058 at fudan.edu.cn>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, June 14, 2005 4:28 PM
> Subject: [R] some suggestion
> 
> 
>> it seems R has no function to sort the data.frame according to some 
>> variable(s),though we can do these by order() and indexing.but why not 
>> make sort() the a generic function,making it can sort vector and other 
>> type of objects?
>>
>> maybe this is a silly suggestion,but i think it is quite usefull.
>>
>>
>>
>>
>> 2005-06-14
>>
>> ------
>> Deparment of Sociology
>> Fudan University
>>
>> Blog:www.sociology.yculblog.com
>>
>>
> 
> 
> -------------------------------------------------------------------------------- 
> 
> 
> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------------
Dr Sander P. Oom
Animal, Plant and Environmental Sciences,
University of the Witwatersrand
Private Bag 3, Wits 2050, South Africa
Tel (work)      +27 (0)11 717 64 04
Tel (home)      +27 (0)18 297 44 51
Fax             +27 (0)18 299 24 64
Email   sander at oomvanlieshout.net
Web     www.oomvanlieshout.net/sander



From xl_goskins at yahoo.com  Tue Jun 14 21:30:08 2005
From: xl_goskins at yahoo.com (X. Li)
Date: Tue, 14 Jun 2005 12:30:08 -0700 (PDT)
Subject: [R] bug in rpart?
Message-ID: <20050614193008.99519.qmail@web52406.mail.yahoo.com>

Dear R-helpers,

Can you help me to see why "code 1" gives error
while "code 2" runs fine?  The only difference in
the data is the distribution of age categories.
I am attaching the session after the code.

Many thanks. 

XL

library(survival)
library(rpart)
# code 1
n <- 20 
age <- rep(1:3, c(2, 3, 15))
eg<- data.frame(rexp(n), rbinom(n,1,prob=.3), age=age)
                     
names(eg) <- c("surv", "status", "age")
rpart(Surv(surv, status)~age, data=eg)

# code 2
n <- 20 
age <- rep(1:3, c(5, 5, 10)) 
eg<- data.frame(rexp(n), rbinom(n,1,prob=.3), age=age)
                     
names(eg) <- c("surv", "status", "age")
rpart(Surv(surv, status)~age, data=eg)

# my session:

> library(rpart)
> # code 1
> n <- 20 
> age <- rep(1:3, c(2, 3, 15))
> eg<- data.frame(rexp(n), rbinom(n,1,prob=.3),
age=age)                      
> names(eg) <- c("surv", "status", "age")
> rpart(Surv(surv, status)~age, data=eg)
Error in "$<-.data.frame"(`*tmp*`, "yval2", value =
c(1, 7)) : 
        replacement has 2 rows, data has 1
> 
> # code 2
> n <- 20 
> age <- rep(1:3, c(5, 5, 10)) 
> eg<- data.frame(rexp(n), rbinom(n,1,prob=.3),
age=age)                      
> names(eg) <- c("surv", "status", "age")
> rpart(Surv(surv, status)~age, data=eg)
n= 20 

node), split, n, deviance, yval
      * denotes terminal node

1) root 20 19.007310 1.0000000  
  2) age>=2.5 10  9.673372 0.8230355 *
  3) age< 2.5 10  9.027225 1.1922660 *



From mzp3769 at yahoo.com  Tue Jun 14 21:43:16 2005
From: mzp3769 at yahoo.com (m p)
Date: Tue, 14 Jun 2005 12:43:16 -0700 (PDT)
Subject: [R] trivial installation question
Message-ID: <20050614194316.16370.qmail@web51010.mail.yahoo.com>

Hello,
I try to install udunits and RNetCDF packages
With the first one
after R CMD INSTALL udunits_1.2.tar.gz the script
looks for the library, does not find and says
to edit udunits_1.0/udunits/src/Makevars.in
with links to an include file and location of the
library. When I put the correct path 
the script repeats the previous message still looking
for the library in the default path. How do I
communicate the location of libraries/include files 
to R CMD INSTALL?
Thanks,
Mark



From MSchwartz at mn.rr.com  Tue Jun 14 21:53:05 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Tue, 14 Jun 2005 14:53:05 -0500
Subject: [R] Plotting rows (or columns) from a matrix in
	different	graphs, not    using "par"
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>
Message-ID: <1118778785.3126.23.camel@localhost.localdomain>

On Tue, 2005-06-14 at 20:00 +0200, Camarda, Carlo Giovanni wrote:
> Dear R-users,
> I would like to ask whether it's possible (for sure it would be), to
> plot each rows (or columns) in different graphs and in the same figure
> region without using the function "par" and then struggling around with
> "axes" and labels etc.
> Luckily, I would always have "rows + columns = even number" and the same
> "ylim".
> 
> The next one could be a sort of example on what I would like to avoid
> plotting the rows of the matrix "mat":
> 
> ########### EXAMPLE ######################
> dat <- sort(runif(16, 1, 1000))
> mat <- matrix(dat, ncol=4, byrow = T)
> y   <- seq(1950, 1953)
> par(mfrow=c(2,2))
> plot(y, mat[1,], ylim=c(1,1000), axes=F, xlab="", ylab="simul.")
> box()    
> axis(side=2, tick=T, labels=T)
> axis(side=3, tick=T, labels=T)
> plot(y, mat[2,], ylim=c(1,1000), axes=F, xlab="", ylab="")
> box()
> axis(side=3, tick=T, labels=T)
> plot(y, mat[3,], ylim=c(1,1000), axes=F, xlab="years", ylab="simul.")
> box()
> axis(side=1, tick=T, labels=T)
> axis(side=2, tick=T, labels=T)
> plot(y, mat[4,], ylim=c(1,1000), axes=F, xlab="years", ylab="")
> box()
> axis(side=1, tick=T, labels=T)
> ########### END EXAMPLE ########################
> 
> Naturally something more compact would be even nicer.
> 
> Thanks in advance,
> Carlo Giovanni Camarda

If I am understanding what you are trying to do, then using xyplot()
from the lattice package (part of the standard R installation) may be
best:

Modify 'mat' so that all data is in 'df':

> df <- data.frame("Simul" = as.vector(mat), 
                   "Years" = rep(1950:1953, 4),
                   "Group" = rep(LETTERS[1:4], each = 4))

> df
       Simul Years Group
1   88.67956  1950     A
2  364.73579  1951     A
3  546.63525  1952     A
4  774.05869  1953     A
5  138.09586  1950     B
6  382.20986  1951     B
7  592.85512  1952     B
8  810.45569  1953     B
9  232.43912  1950     C
10 524.91085  1951     C
11 598.49688  1952     C
12 963.97328  1953     C
13 261.80598  1950     D
14 533.91143  1951     D
15 765.72522  1952     D
16 996.72192  1953     D


# Load the lattice package

> library(lattice)

# Draw the plot, note the use of standard R formulae in the syntax
# with the addition of the grouping vector after the '|'

> xyplot(Simul ~ Years | Group, data = df)

See:

library(lattice)
?xyplot

for more information.

HTH,

Marc Schwartz



From antoniou at central.ntua.gr  Tue Jun 14 22:41:29 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Tue, 14 Jun 2005 23:41:29 +0300
Subject: [R] problem installing packages with compiled-from-source R.app
	on Mac OS X - Tiger
In-Reply-To: <Pine.LNX.4.61.0506142012100.5289@gannet.stats>
References: <E0CEA60D-026D-4EC1-A6E2-E765F19207D0@central.ntua.gr>
	<Pine.LNX.4.61.0506142012100.5289@gannet.stats>
Message-ID: <76D609C5-E1CF-4DCC-8D4F-A5478BC9764C@central.ntua.gr>

Thank you. I am compiling it now...

Costas

On 14 ÅŒôÅŒÅøÅœÖÅŒÅΩ 2005, at 10:13 ÅŒúÅŒú, Prof Brian Ripley wrote:

> You need to use R-patched, nowadays 2.1.1 beta, not the released R  
> 2.1.0.
> As far as I know it is fixed there.
>
> On Tue, 14 Jun 2005, Constantinos Antoniou wrote:
>
>
>> Hello all,
>>
>> This may be aimed for r-devel, but I encountered this as an R-user
>> and not an R-developer so I start here (having said that, please
>> direct me to R-devel if you think this is appropriate. I am not  
>> cross-
>> posting, as I believe this is bad netiquette).
>>
>> I am a recent, but extremely happy R-user (especially after getting
>> my own copy of MASS 2002). My adventures started when I wanted to use
>> Rpy to use R also from within python. I compiled R (2.1.0) after
>> configuring it as follows:
>>
>> ./configure --enable-R-shlib --with-blas='-framework vecLib' --with-
>> lapack --with-aqua
>>
>> where --enable-R-shlib is required by Rpy.
>>
>> I then compiled Rpy-0.4.2.1 and I was able to call R from within
>> python. So far so good...
>>
>> I then -naively- tried to start R.app. After less than a bounce on
>> the dock... it would not start. [Sorry, I do not remember what
>> problem was showing up in the console...]
>>
>> So, I figured I had to also compile R.app. I got the code via:
>>
>> svn co https://svn.r-project.org/R-packages/trunk/Mac-GUI Mac-
>> GUI      (executed this command yesterday)
>>
>> and compiled it as per the instructions... And R.app launches just  
>> fine.
>>
>> Now, the issue comes when I try to install a(ny) package (via the GUI
>> package installer of R.app). I then get the following message:
>>
>> "Package installation failed. Package installation was not
>> successful. Please see the R console for details"
>>
>> And the R console says:
>>
>> "Error in install.packages(c("dyn"), lib = "/Library/Frameworks/
>> R.framework/Resources/library",  :
>>     couldn't find function ".install.macbinary""
>>
>> I see that this has been encountered before and resolved (at least
>> Prof. Ripley saw what was wrong) as per the following thread:
>>
>> https://stat.ethz.ch/pipermail/r-devel/2005-May/033115.html
>>
>> But I am not sure what needs to be done on my part so that I am not
>> affected from it.
>>
>> > str(.Platform)
>> List of 6
>> $ OS.type   : chr "unix"
>> $ file.sep  : chr "/"
>> $ dynlib.ext: chr ".so"
>> $ GUI       : chr "X11"
>> $ endian    : chr "big"
>> $ pkgType   : chr "source"
>> > R.version.string
>> [1] "R version 2.1.0, 2005-04-18"
>>
>>
>> gcc version 4.0.0
>>
>> [Please let me know what other information may be relevant]
>>
>>
>> Thanking you in advance,
>>
>> Costas
>>
>>
>> --
>> Constantinos Antoniou, Ph.D.
>> Department of Transportation Planning and Engineering
>> National Technical University of Athens
>> 5, Iroon Polytechniou str. GR-15773, Athens, Greece
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From kerryrekky at yahoo.com  Tue Jun 14 23:28:27 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Tue, 14 Jun 2005 14:28:27 -0700 (PDT)
Subject: [R] within and between subject calculation
Message-ID: <20050614212827.4141.qmail@web51808.mail.yahoo.com>

Dear helpers in this forum,

I have the following question:

Suppose I have the following data set:

id x y 
023 1 2
023 2 5
023 4 6
023 5 7
412 2 5
412 3 4
412 4 6
412 7 9
220 5 7
220 4 8
220 9 8
......

and i want to calculate sum_{i=1}^k
sum_{j=1}^{n_i}x_{ij}*y_{ij}

is there a simple way to do this within and between
subject summation in R?



From Pierre.Lapointe at nbf.ca  Tue Jun 14 23:45:25 2005
From: Pierre.Lapointe at nbf.ca (Lapointe, Pierre)
Date: Tue, 14 Jun 2005 17:45:25 -0400
Subject: [R] Matrix stability problem
Message-ID: <834204C0D7C6D611A3BB000255FC6E9D0DF35873@lbmsg002.fbn-nbf.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050614/7afa81b5/attachment.pl

From ggrothendieck at gmail.com  Wed Jun 15 00:00:52 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 18:00:52 -0400
Subject: [R] Manipulating dates
In-Reply-To: <42AEE4AE.2050800@imperial.ac.uk>
References: <42AEE4AE.2050800@imperial.ac.uk>
Message-ID: <971536df0506141500398b5e03@mail.gmail.com>

On 6/14/05, Richard Hillary <r.hillary at imperial.ac.uk> wrote:
> Hello,
>          Given a vector of characters, or factors, denoting the date in
> the following way: 28/03/2000, is there a method of
> 1) Computing the earliest of these dates;
> 2) Using this as a base, then converting all the other dates into merely
> the number of days after this minimum date


Convert dates to chron (whose default date format is the
one needed here); convert that to numeric (which will be the 
number of days since some origin) and then do the indicated 
subtraction:

	library(chron)
	x <- as.character(x)  # can omit if already character
	x.num <- as.numeric(chron(x))
	x - x.num - min(x.num)

Alternately convert it to Date and use julian (or convert
x.Date to numeric and use subtraction, as with chron):

	x <- as.character(x) # can omit in latest R 2.1.0 patched
	x.Date <- as.Date(x, "%m/%d/%Y")
	julian(x.Date, min(x.Date))

The as.Date.factor method in the latest R 2.1.0 patched
supports the format string (second argument of as.Date) for
factors but older versions of R did not. If your x is a 
factor but you do not want to upgrade right now start off
with the statement: x <- as.character(x)

More info on dates is in R News 4/1.



From ggrothendieck at gmail.com  Wed Jun 15 00:02:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 18:02:55 -0400
Subject: [R] Dateticks
In-Reply-To: <40921.66.92.23.42.1118766474.squirrel@66.92.23.42>
References: <40921.66.92.23.42.1118766474.squirrel@66.92.23.42>
Message-ID: <971536df05061415026aecd7b6@mail.gmail.com>

On 6/14/05, Bernard L. Dillard <bld at math.umd.edu> wrote:
> Hello.  I am having the worst time converting x-axis date ticks to real
> dates.  I have tried several suggestions in online help tips and books to
> no avail.
> 
> For example, the x-axis has 0, 50, 100, etc, and I want it to have
> "6/17/03", "8/6/03" etc.  See attached (sample).
> 
> Can anybody help me with this.
> 
> Here's my code:
> 
> ts.plot(date.attackmode.table[,1], type="l", col="blue", lty=2,ylab="IED
>     Attacks", lwd=2,xlab="Attack Dates",main="Daily Summary of Attack
>     Mode")
> grid()
> 


The default format for dates in chron is the one needed here
so let us use that.  (We could alternately use Date class 
in defining tt and specify a format string as a second argument 
to format in the last line.)

library(chron)

# test data.  values are in y and times are in tt.
y <- rnorm(201)
tt <- chron("8/6/03") + seq(0, length = length(y))

# plot without axis
plot(tt, y, xaxt = "n")

# axis with ticks and labels every 50 days
idx50 <- seq(1, length(tt), 50)
axis(1, tt[idx50], format(tt[idx50]), cex.axis = 0.5)

or, to use Date class replace with this:

tt <- as.Date("2003-08-06") + seq(0, length = length(y))
plot(tt, y, xaxt = "n")
idx50 <- seq(1, length(tt), 50)
axis(1, tt[idx50], format(tt[idx50], "%m/%d/%y), cex.axis = 0.5)


More info on working with dates is in R News 4/1.



From ggrothendieck at gmail.com  Wed Jun 15 00:04:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 18:04:26 -0400
Subject: [R] Plotting rows (or columns) from a matrix in different
	graphs, not using "par"
In-Reply-To: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>
References: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>
Message-ID: <971536df050614150436fc2374@mail.gmail.com>

On 6/14/05, Camarda, Carlo Giovanni <Camarda at demogr.mpg.de> wrote:
> Dear R-users,
> I would like to ask whether it's possible (for sure it would be), to
> plot each rows (or columns) in different graphs and in the same figure
> region without using the function "par" and then struggling around with
> "axes" and labels etc.
> Luckily, I would always have "rows + columns = even number" and the same
> "ylim".
> 
> The next one could be a sort of example on what I would like to avoid
> plotting the rows of the matrix "mat":
> 
> ########### EXAMPLE ######################
> dat <- sort(runif(16, 1, 1000))
> mat <- matrix(dat, ncol=4, byrow = T)
> y   <- seq(1950, 1953)
> par(mfrow=c(2,2))
> plot(y, mat[1,], ylim=c(1,1000), axes=F, xlab="", ylab="simul.")
> box()
> axis(side=2, tick=T, labels=T)
> axis(side=3, tick=T, labels=T)
> plot(y, mat[2,], ylim=c(1,1000), axes=F, xlab="", ylab="")
> box()
> axis(side=3, tick=T, labels=T)
> plot(y, mat[3,], ylim=c(1,1000), axes=F, xlab="years", ylab="simul.")
> box()
> axis(side=1, tick=T, labels=T)
> axis(side=2, tick=T, labels=T)
> plot(y, mat[4,], ylim=c(1,1000), axes=F, xlab="years", ylab="")
> box()
> axis(side=1, tick=T, labels=T)
> ########### END EXAMPLE ########################
> 
> Naturally something more compact would be even nicer.
> 

plot(ts(mat, start = 1950), nc = 2)



From ggrothendieck at gmail.com  Tue Jun 14 23:59:37 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 17:59:37 -0400
Subject: [R] Puzzled in utilising summary.lm() to obtain Var(x)
In-Reply-To: <20050614130337.GI6989@lubyanka.local>
References: <20050614130337.GI6989@lubyanka.local>
Message-ID: <971536df050614145914d01377@mail.gmail.com>

On 6/14/05, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> I have a program which is doing a few thousand runs of lm(). Suppose
> it is a simple model
>   y = a + bx1 + cx2 + e
> 
> I have the R object "d" where
>   d <- summary(lm(y ~ x1 + x2))
> 
> I would like to obtain Var(x2) out of "d". How might I do it?
> 
> I can, of course, always do sd(x2). But it would be much more
> convenient if I could snoop around the contents of summary.lm and
> extract Var() out of it. I couldn't readily see how. Would you know
> what would click?
> 


Is the question how to get the variance of a column of the
model matrix for a model that is the sum of terms given only
summary output and the column name but not the name of the
data frame?  If that is it then try this:

d <- summary(lm(Sepal.Length ~ Sepal.Width, iris)) # test data
var(model.matrix(eval(d$call))[,"Sepal.Width"])

If that's not the question, try examining the contents of d using

	str(d)

and examine the output of lm via

	str(eval(d$call))

and perhaps that will suggest the answer.



From ggrothendieck at gmail.com  Wed Jun 15 00:01:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 18:01:55 -0400
Subject: [R] load ing and saving R objects
In-Reply-To: <42AED571.8090805@well.ox.ac.uk>
References: <42AED571.8090805@well.ox.ac.uk>
Message-ID: <971536df05061415013091f0a3@mail.gmail.com>

On 6/14/05, Richard Mott <rmott at well.ox.ac.uk> wrote:
> Does anyone know a way to do the following:
> 
> Save a large number of R objects to a file (like load() does) but then
> read back only a small named subset of them . As far as I can see,
> load() reads back everything.
> 
> The context is:
> 
> I have an application which will generate a large number of large
> matrices (approx 15000 matrices each of dimension 2000*30). I can
> generate these matrices using an R-package I wrote, but it requires a
> large amouint of memory and is slow so I want to do this only once.
> However, I then want to do some subsequent processing, comprising a very
> large number of runs in which small  (~ 10) random selection of matrices
> from the previously computed set are used for linear modeling.  So I
> need a way to load back named objects previously saved in a call to
> save(). I can;t see anyway of doing this. Any ideas?


Check out the g.data delayed data package on CRAN and the article 
in R News 2/3.



From ggrothendieck at gmail.com  Wed Jun 15 00:03:49 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 18:03:49 -0400
Subject: [R] update.packages() - gregmisc
In-Reply-To: <42AECBAC.605@gmail.com>
References: <42AEC026.3000203@gmail.com>
	<971536df0506140451621eaf23@mail.gmail.com> <42AECBAC.605@gmail.com>
Message-ID: <971536df05061415033003d4ee@mail.gmail.com>

Is the code in your post intended to show what worked so others
will know what to do or is that code intended to show what you
did but did not work?

If its the latter, I successfully did it last week and don't 
clearly remember my precise steps but I may have done this:

	R CMD remove gdata
	R CMD remove gmodels
	R CMD remove gplots
	R CMD remove gtools
	R CMD remove gregmisc

I assume that using remove.packages, viz.

 	remove.packages("gregmisc")
 	etc.

would have given the same result.


On 6/14/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Thanks.
> I do like this,
>  > remove.packages("gregmisc", .libPaths()[1])
>  > remove.packages("gtools", .libPaths()[1])
>  > install.packages("gregmisc", .libPaths()[1])
>  > update.packages()
>  > update.packages()
>  > install.packages("gtools", .libPaths()[1])
>  > update.packages()
>  > update.packages()
>  > update.packages(ask='graphics')
> 
> Regards,
> Muhammad Subianto
> R.2.1.0 on W2K
> 
> On this day 6/14/2005 1:51 PM, Gabor Grothendieck wrote:
> > On 6/14/05, Muhammad Subianto <subianto at gmail.com> wrote:
> >
> >>Dear all,
> >>I have a problem to update package gregmisc.
> >>After I update,
> >> > update.packages(ask='graphics')
> >>trying URL
> >>'http://cran.at.r-project.org/bin/windows/contrib/2.1/gregmisc_2.0.8.zip'
> >>Content type 'application/zip' length 2465 bytes
> >>opened URL
> >>downloaded 2465 bytes
> >>
> >>package 'gregmisc' successfully unpacked and MD5 sums checked
> >>...
> >>
> >>then try to update again, still I must update package gregmisc, etc.
> >>I have tried 3,4,5, times with the same result.
> >>
> >
> >
> > This was discussed on r-devel recently.  See:
> >
> > https://www.stat.math.ethz.ch/pipermail/r-devel/2005-June/033479.html



From gunter.berton at gene.com  Wed Jun 15 00:22:38 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 14 Jun 2005 15:22:38 -0700
Subject: [R] How to fix false positve rates?
In-Reply-To: <20050614180115.76757.qmail@web60423.mail.yahoo.com>
Message-ID: <200506142222.j5EMMfJE000956@hertz.gene.com>

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Anderson de Rezende
Rocha
Sent: Tuesday, June 14, 2005 11:01 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to fix false positve rates?

Dear R-users, 

I have a set of 12000 image samples. I can divide this set into two
categories: training and testing. I need to classify the test set into
a two qualitative outputs: true or false for some characteristic. 

To do the classification I'm using the packages SVM in e1071 library
and LDA in the MASS library. However, I'm with a great number of FALSE
POSITIVE CASES in both classifiers, about 10/15%. 

1) How can I use these classifiers with a fixed 1% FALSE POSITIVE RATE?

Surely you jest! Classification is not hypothesis testing.You do not get to
control either false positive or negative rates. You try to minimize (an
unbiased estimate of) mislclassification rate.

2) Could anynone indicate me a non-linear SVM classifier R-package?
3) Could anynone indicate me an ADA-BOOST classifier R-package?

Look! 
RSiteSearch('boosting', restrict = 'functions')

Cheers,
Bert Gunter



Thanks for all. I'm anxiously wait the answers. Best regards. 

*************************************************
|????????????????| - Anderson de Rezende Rocha
Bacharel em Ci??ncia da Computa????o - UFLA
Mestrando em Ci??ncia da Computa????o - UNICAMP
Esteganografia e estegan??lise digital
UNIVERSIDADE ESTADUAL DE CAMPINAS, SP - BRASIL
< http://andersonrocha.cjb.net >

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mk_lists at yahoo.ca  Wed Jun 15 00:36:36 2005
From: mk_lists at yahoo.ca (M. K.)
Date: Tue, 14 Jun 2005 18:36:36 -0400 (EDT)
Subject: [R] lattice, panel.grid, and scales=list(tick.number=XXX)
Message-ID: <20050614223636.29109.qmail@web31305.mail.mud.yahoo.com>

I have a Lattice plot in which I want to adjust the number of tick
marks used, and I want to have the drawn grid reflect that change. 
Here is what I'm doing:

bwplot(var1 ~ var2, data=df, scales=list(tick.number=10),
       panel=function(...) {
	   panel.grid(h=0,v=-1,...);
	   panel.stripplot(col="gray40", pch="|", cex=2, ...);
	   panel.bwplot(...);
	   })

Unfortunately this doesn't quite work.  Although the bwplot's tick
marks are indeed increased as requested, the panel.grid produces the
same (3 line) grid as before, seemingly unaware of the changed # of
ticks.

Any suggestions on how to achieve what I want?



From reid_huntsinger at merck.com  Wed Jun 15 00:48:25 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Tue, 14 Jun 2005 18:48:25 -0400
Subject: [R] Matrix stability problem
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A948F@uswpmx00.merck.com>

It looks to me like the x measurements are normalized to sum to 1. So
whatever measurement error there is gets "spread around", so to speak. 

It would help if you could explain the setting a little more fully? Why is
it, for example, that you know the A values from experiment to experiment
(they do seem to vary) but there's no measurment error? Why the variation?
Do you know b, or is it an estimate from some measurements x and readings A?


I guess this is probably a (multivariate-response) regression problem, and
the question is where the error is and what its structure is. Imposing the
constraint that x sums to 1 would probably help. This makes an
overdetermined problem (two free parameters, three experiments) so you are
forced into regression.

Would you ever have more than three experiments? If so would that change the
formulation of the problem? More experiments + regression might be the
simplest way to get a more accurate solution.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Lapointe, Pierre
Sent: Tuesday, June 14, 2005 5:45 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Matrix stability problem


Hello, 

This is not a problem with R, the calculated results are mathematically
correct. This a matrix stability problem. Because of measuring errors, my
matrix solution is a bit off.

Here is what my equations look like:
A11 x11+A12 x12 +A13 x13 = b1  
A21 x21+A22 x21 +A23 x23 = b2  
A31 x31+A32 x31 +A33 x33 = b3 
A is a reading, X is a measured weight, and b is total. The 3 experiments
give slightly different X values because of measurement errors.
For reproducibility, here's my A, x and b matrices and vectors
A <-matrix(
c(0.03,0.02,0.04,0.01,0.015,0.03,-0.01,-0.02,0.03),3,3,byrow=TRUE)
x <-matrix( c(0.2,0.3,0.5,0.205,0.305,0.49,0.19,0.29,0.52),3,3,byrow=TRUE)
b <-matrix( c(0.032,0.021325,0.0079),3,1)
As expected, rowSums(A*x) = b
Problem: Let's now assume I don't know x. I'd like to solve for x in Ax=b. I
am aware that my x is a matrix and solve(A,b) will give me a vector.
However, looking at the x matrix, one can easily see that the real x[,1]
(without measurement error) is close to 0.2, x[,2] is close to 0.3 and x[,3]
is close to 0.5
> x
      [,1]  [,2] [,3]
[1,] 0.200 0.300 0.50
[2,] 0.205 0.305 0.49
[3,] 0.190 0.290 0.52 
However, solve(A,b) gives me a vector that is not close to the expected
solution: > solve(A,b)
          [,1]
[1,] 0.2140000
[2,] 0.2612857 # Far from 0.2
[3,] 0.5088571
Do you know any function/package in R that could help me get a result closer
to the expected one?

Regards,

Pierre Lapointe


****************************************************************************
******* 
AVIS DE NON-RESPONSABILITE:\ Ce document transmis par courri...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From efg at stowers-institute.org  Wed Jun 15 00:49:57 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 14 Jun 2005 17:49:57 -0500
Subject: [R] Preparing timestamped data for fourier analysis
References: <20050613171631.29376.qmail@web52406.mail.yahoo.com>
	<001401c5705e$6a548d50$0201a8c0@milos>
Message-ID: <d8nml7$cbi$1@sea.gmane.org>

"Milos Zarkovic" <mzarkov at EUnet.yu> wrote in message
news:001401c5705e$6a548d50$0201a8c0 at milos...
> I believe that FFT is not appropriate. However Lomb-Scargle periodogram
> could be used.

This may interest you:

(Preprint of  submitted paper)
Detecting periodic patterns in unevenly spaced gene expression time series
using Lomb-Scargle periodograms.
http://research.stowers-institute.org/bioinfo/PDF/m2005_lomb-scargle_submitted.pdf

R code here
http://research.stowers-institute.org/efg/2005/LombScargle/

efg



From gunter.berton at gene.com  Wed Jun 15 01:12:51 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 14 Jun 2005 16:12:51 -0700
Subject: [R] lattice, panel.grid, and scales=list(tick.number=XXX)
In-Reply-To: <20050614223636.29109.qmail@web31305.mail.mud.yahoo.com>
Message-ID: <200506142312.j5ENCptZ027524@faraday.gene.com>

If you look at the code of panel.grid, you'll see why  it doesn't work -- it
does not use any of the scale parameters. Moreover the Help page for
panel.grid explicitly warns that the h,v=-1 specification may not work, so
no promises have been broken.

I'm not sure how bwplot handles grids, since one of the axes is determined
by the number of groups and not the range of the data as in xyplot. You
might try specifying the at= argument for both the scales list and at the
top level call for panel.grid to pick up. i.e. bwplot( ..., scales = list
(at = where tic marks go),at= where tic marks go, ...).

Alternatively, wait for Deepayan to give a definitive answer.

-- Bert Gunter

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of M. K.
Sent: Tuesday, June 14, 2005 3:37 PM
To: R-help mailing list
Subject: [R] lattice, panel.grid, and scales=list(tick.number=XXX)

I have a Lattice plot in which I want to adjust the number of tick
marks used, and I want to have the drawn grid reflect that change. 
Here is what I'm doing:

bwplot(var1 ~ var2, data=df, scales=list(tick.number=10),
       panel=function(...) {
	   panel.grid(h=0,v=-1,...);
	   panel.stripplot(col="gray40", pch="|", cex=2, ...);
	   panel.bwplot(...);
	   })

Unfortunately this doesn't quite work.  Although the bwplot's tick
marks are indeed increased as requested, the panel.grid produces the
same (3 line) grid as before, seemingly unaware of the changed # of
ticks.

Any suggestions on how to achieve what I want?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Wed Jun 15 01:32:24 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 14 Jun 2005 18:32:24 -0500
Subject: [R] within and between subject calculation
In-Reply-To: <20050614212827.4141.qmail@web51808.mail.yahoo.com>
References: <20050614212827.4141.qmail@web51808.mail.yahoo.com>
Message-ID: <40e66e0b05061416321f8b610f@mail.gmail.com>

On 6/14/05, Kerry Bush <kerryrekky at yahoo.com> wrote:
> Dear helpers in this forum,
> 
> I have the following question:
> 
> Suppose I have the following data set:
> 
> id x y
> 023 1 2
> 023 2 5
> 023 4 6
> 023 5 7
> 412 2 5
> 412 3 4
> 412 4 6
> 412 7 9
> 220 5 7
> 220 4 8
> 220 9 8
> ......
> 
> and i want to calculate sum_{i=1}^k
> sum_{j=1}^{n_i}x_{ij}*y_{ij}
> 
> is there a simple way to do this within and between
> subject summation in R?

You didn't make it clear what the indices i and j refer to.  It seems
that part of the answer could use

> samp
    id x y
1   23 1 2
2   23 2 5
3   23 4 6
4   23 5 7
5  412 2 5
6  412 3 4
7  412 4 6
8  412 7 9
9  220 5 7
10 220 4 8
11 220 9 8
> with(samp, tapply(x*y, id, sum))
 23 220 412 
 71 139 109 

but if you then sum this result you simply get the sum of x * y.



From mulakken1 at llnl.gov  Wed Jun 15 02:14:35 2005
From: mulakken1 at llnl.gov (Nisha Mulakken)
Date: Tue, 14 Jun 2005 17:14:35 -0700
Subject: [R] anova.lme error
Message-ID: <6.1.2.0.2.20050614170743.09cf2ee8@mail.llnl.gov>

Hi,

I am working with R version 2.1.0, and I seem to have run into what looks 
like a bug. I get the same error message when I run R on Windows as well as 
when I run it on Linux.

When I call anova to do a LR test from inside a function, I get an error. 
The same call works outside of a function. It appears to not find the right 
environment when called from inside a function. I have provided the code 
below.

Thanks,
Nisha Mulakken

############################

myFunction <- function(myDataFrame) {

  # Less restricted
  fit1 <- gls(y ~ dose,
              weights=varIdent(form=~1|dose),
              data=myDataFrame)

  # more restricted
  fit2 <- gls(y ~ dose,
              data=myDataFrame)

  anova.results <- anova(fit1, fit2)
  anova.results
}

df <- data.frame( y=c(12,3,45,1,53,6),
                   dose=c(0,10,200,0,10,200),
                   time=c("4.00 hrs", "4.00 hrs", "6.00 hrs", "6.00 hrs", 
"8.00 hrs", "8.00 hrs"),
                   time.hours=c(4, 4, 6, 6, 8, 8),
                   rep=rep("a", 6)
                 )

## This leads to the following error:
##     Error in anova.lme(object = fit1, fit2) : Object "fit2" not found
results <- myFunction(myDataFrame=df)

#####################################################
## The same thing outside of a function

# Less restricted
fit3 <- gls(y ~ dose,
              weights=varIdent(form=~1|dose),
              data=df)

# more restricted
fit4 <- gls(y ~ dose,
             data=df)

## This works:
anova(fit3, fit4)

## The results:
## > anova(fit3, fit4)
##     Model df      AIC      BIC    logLik   Test L.Ratio p-value
## fit3     1  5 57.98998 54.92145 -23.99499
## fit4     2  3 55.75284 53.91172 -24.87642 1 vs 2 1.76286  0.4142



From phiberoptic_br at yahoo.com.br  Wed Jun 15 02:47:05 2005
From: phiberoptic_br at yahoo.com.br (Anderson de Rezende Rocha)
Date: Tue, 14 Jun 2005 21:47:05 -0300 (ART)
Subject: [R] Reducing the FPR  (false positive rate)
Message-ID: <20050615004705.58600.qmail@web60425.mail.yahoo.com>

Hello R-USERS, 

I think some people didn't understand my question. What I want is to
use the training set to minimize the FALSE POSITIVE RATE. 

I think it is possible. I sacrifice ACCURACY to have less FALSE
POSITIVES. I don't want a classifier result with 5% of FPR and, for
example, 93% of ACCURACY. I want a 1% FPR sacrificing the 93% ACCURACY.


Do you know how can I do this? I really need this because the requisite
of the work I'm doing is only 1% of FPR. 

Thanks and best regards. 

*************************************************
|????????????????| - Anderson de Rezende Rocha
Bacharel em Ci??ncia da Computa????o - UFLA
Mestrando em Ci??ncia da Computa????o - UNICAMP
Esteganografia e estegan??lise digital
UNIVERSIDADE ESTADUAL DE CAMPINAS, SP - BRASIL
< http://andersonrocha.cjb.net >



From wuming.gong at gmail.com  Wed Jun 15 02:57:55 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Wed, 15 Jun 2005 08:57:55 +0800
Subject: [R] Logistic regression with more than two choices
In-Reply-To: <20050614162426.4DF61302DB@smtp2.song.fi>
References: <mailman.11.1118743201.15147.r-help@stat.math.ethz.ch>
	<20050614162426.4DF61302DB@smtp2.song.fi>
Message-ID: <b428d06d05061417577ddae557@mail.gmail.com>

Hi Koskinen

For response variables with multiple categories, you may try polr() in
MASS package, which implement a proportional odds model. And you may
search the R archives, several threads discussed this problem
before...

Wuming

On 6/15/05, Ville Koskinen <ville.koskinen at matrex.fi> wrote:
> Dear all R-users,
> 
> I am a new user of R and I am trying to build a discrete choice model (with
> more than two alternatives A, B, C and D) using logistic regression. I have
> data that describes the observed choice probabilities and some background
> information. An example below describes the data:
> 
> Sex     Age     pr(A)   pr(B)   pr(C)   pr(D) ...
> 1       11      0.5     0.5     0       0
> 1       40      1       0       0       0
> 0       34      0       0       0       1
> 0       64      0.1     0.5     0.2     0.2
> ...
> 
> I have been able to model a case with only two alternatives "A" and "not A"
> by using glm().
> 
> I do not know what functions are available to estimate such a model with
> more than two alternatives. Multinom() is one possibility, but it only
> allows the use of binary 0/1-data instead of observed probabilities. Did I
> understand this correctly?
> 
> Additionally, I am willing to use different independent variables for the
> different alternatives in the model. Formally, I mean that:
> Pr(A)=exp(uA)/(exp(uA)+exp(uB)+exp(uC)+exp(uD)
> Pr(B)=exp(uB)/(exp(uA)+exp(uB)+exp(uC)+exp(uD)
> ...
> where uA, uB, uC and uD are linear functions with different independent
> variables, e.g. uA=alpha_A1*Age, uB=alpha_B1*Sex.
> 
> Do you know how to estimate this type of models in R?
> 
> Best regards, Ville Koskinen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Wed Jun 15 03:16:42 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 21:16:42 -0400
Subject: [R] Plotting rows (or columns) from a matrix in different
	graphs, not using "par"
In-Reply-To: <971536df050614150436fc2374@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>
	<971536df050614150436fc2374@mail.gmail.com>
Message-ID: <971536df05061418166f8bee66@mail.gmail.com>

On 6/14/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/14/05, Camarda, Carlo Giovanni <Camarda at demogr.mpg.de> wrote:
> > Dear R-users,
> > I would like to ask whether it's possible (for sure it would be), to
> > plot each rows (or columns) in different graphs and in the same figure
> > region without using the function "par" and then struggling around with
> > "axes" and labels etc.
> > Luckily, I would always have "rows + columns = even number" and the same
> > "ylim".
> >
> > The next one could be a sort of example on what I would like to avoid
> > plotting the rows of the matrix "mat":
> >
> > ########### EXAMPLE ######################
> > dat <- sort(runif(16, 1, 1000))
> > mat <- matrix(dat, ncol=4, byrow = T)
> > y   <- seq(1950, 1953)
> > par(mfrow=c(2,2))
> > plot(y, mat[1,], ylim=c(1,1000), axes=F, xlab="", ylab="simul.")
> > box()
> > axis(side=2, tick=T, labels=T)
> > axis(side=3, tick=T, labels=T)
> > plot(y, mat[2,], ylim=c(1,1000), axes=F, xlab="", ylab="")
> > box()
> > axis(side=3, tick=T, labels=T)
> > plot(y, mat[3,], ylim=c(1,1000), axes=F, xlab="years", ylab="simul.")
> > box()
> > axis(side=1, tick=T, labels=T)
> > axis(side=2, tick=T, labels=T)
> > plot(y, mat[4,], ylim=c(1,1000), axes=F, xlab="years", ylab="")
> > box()
> > axis(side=1, tick=T, labels=T)
> > ########### END EXAMPLE ########################
> >
> > Naturally something more compact would be even nicer.
> >
> 
> plot(ts(mat, start = 1950), nc = 2)
> 

I just looked at this again and noticed that it did not have
identical y axes.  I tried with ylim= too but plot.ts seemed
to ignore it.

Here is another solution that uses the zoo library.
Be sure to use zoo 1.0-1.  It uses the screens= argument of 
plot.zoo which was not available in earlier versions of zoo.

Instead of creating a ts series with 4 columns create a 
similar zoo series.  Append to that 4 dummy series all
with the same range.  Use the screens= argument
on plot.zoo to plot one real series and one dummy series
in each graph.  Use type = "l" to plot the real series
and type = "n" for the dummy series.  This causes the
real series to be plotted with lines and the dummy
series to be plotted invisibly yet still affect the
y axis range.    Note that we have used the fact that
mat is square so if the real mat is not square be sure
to modify this accordingly.

library(zoo) # 

n <- nrow(mat)
z <- zooreg(cbind(mat, max(mat) * diag(n)), start = 2000)
plot(z, screens = c(1:4, 1:4), type = rep(c("l","n"), each = 4), nc = 2)



From shusong.jin at gmail.com  Wed Jun 15 03:19:28 2005
From: shusong.jin at gmail.com (Shusong Jin)
Date: Wed, 15 Jun 2005 09:19:28 +0800
Subject: [R] Calling C from Fortran
In-Reply-To: <200506141823.58603.gilles.guillot@inapg.inra.fr>
References: <200506141823.58603.gilles.guillot@inapg.inra.fr>
Message-ID: <4fec35e7050614181948192f75@mail.gmail.com>

Dear Gilles,

You can try this.
Save the fortran file as 1.f
Save the C file as 2.c
Then
R CMD SHLIB 1.f 2.c
You should obtain a  file named 1.so.

Then start you R  program, then 
dyn.load('/where/is/your/1.so')
.Fortran("testit")

You should obtain a random number which is normally distributed.

Good luck.

Jin

On 6/15/05, Gilles GUILLOT <gilles.guillot at inapg.inra.fr> wrote:
> I would like to call C routines from Fortran as suggested in section 5.6 of
> the "Writing R extensions" documentation.
> 
> I'm familiar with Fortran but not with C.
> I understand the example provided in Fortran:
> 
> subroutine testit()
> double precision normrnd, x
> call rndstart()
> x = normrnd()
> call dblepr("X was", 5, x, 1)
> call rndend()
> end
> 
> 
> but I don't understand the purpose of this C wrapper:
> #include <R.h>
>  void F77_SUB(rndstart)(void) { GetRNGstate(); }
>  void F77_SUB(rndend)(void) { PutRNGstate(); }
>  double F77_SUB(normrnd)(void) { return norm_rand(); }
> 
> neither how I should compile it.
> 
> Could anyone explain how I should compile and link
> the C and Fortran files above, and call the Fortran subroutine from R.
> 
> Thanks,
> 
> Gilles
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Wed Jun 15 05:06:35 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 14 Jun 2005 23:06:35 -0400
Subject: [R] ordinary polynomial coefficients from orthogonal
	polynomials?
In-Reply-To: <Pine.LNX.4.61.0506141525310.32282@gannet.stats>
References: <42AE9D59.6090006@bovik.org>
	<Pine.LNX.4.61.0506141126160.26371@gannet.stats>
	<42AEDFCA.5080107@vanderbilt.edu>
	<Pine.LNX.4.61.0506141525310.32282@gannet.stats>
Message-ID: <42AF9B3B.6000106@vanderbilt.edu>

Prof Brian Ripley wrote:
> On Tue, 14 Jun 2005, Frank E Harrell Jr wrote:
> 
>> Prof Brian Ripley wrote:
>>
>>> On Tue, 14 Jun 2005, James Salsman wrote:
>>>
>>>
>>>> How can ordinary polynomial coefficients be calculated
>>>> from an orthogonal polynomial fit?
>>>
>>>
>>>
>>> Why would you want to do that?  predict() is perfectly happy with an
>>> orthogonal polynomial fit and the `ordinary polynomial coefficients' 
>>> are rather badly determined in your example since the design matrix 
>>> has a very high condition number.
>>
>>
>> Brian - I don't fully see the relevance of the high condition number 
>> nowadays unless the predictor has a really bad origin.  Orthogonal 
>> polynomials are a mess for most people to deal with.
> 
> 
> It means that if you write down the coeffs to a few places and then try 
> to reproduce the predictions you will do badly.  The perturbation 
> analysis depends on the condition number, and so is saying that the 
> predictions are dependent on fine details of the coefficients.

Right - I carry several digits of precision when I do this.

> 
> Using (year-2000)/1000 or (year - 1970)/1000 would be a much better idea.
> 
> Why do `people' need `to deal with' these, anyway.  We have machines to 
> do that.

The main application I think of is when we publish fitted models, but it 
wouldn't be that bad to restate fitted orthogonal polynomials in simpler 
notation.  -Frank

> 
>>
>> Frank
>>
>>>
>>>
>>>> I'm trying to do something like find a,b,c,d from
>>>> lm(billions ~ a+b*decade+c*decade^2+d*decade^3)
>>>> but that gives:  "Error in eval(expr, envir, enclos) :
>>>> Object "a" not found"
>>>
>>>
>>>
>>> You could use
>>>
>>> lm(billions ~ decade + I(decade^2) + I(decade^3))
>>>
>>> except that will be numerically inaccurate, since
>>>
>>>
>>>> m <- model.matrix(~ decade + I(decade^2) + I(decade^3))
>>>> kappa(m)
>>>
>>>
>>> [1] 3.506454e+16
>>>
>>>
>>>
>>>
>>>>> decade <- c(1950, 1960, 1970, 1980, 1990)
>>>>> billions <- c(3.5, 5, 7.5, 13, 40)
>>>>> # source: http://www.ipcc.ch/present/graphics/2001syr/large/08.17.jpg
>>>>>
>>>>> pm <- lm(billions ~ poly(decade, 3))
>>>>>
>>>>> plot(decade, billions, xlim=c(1950,2050), ylim=c(0,1000),
>>>>
>>>>
>>>> main="average yearly inflation-adjusted dollar cost of extreme weather
>>>> events worldwide")
>>>>
>>>>> curve(predict(pm, data.frame(decade=x)), add=TRUE)
>>>>> # output: http://www.bovik.org/storms.gif
>>>>>
>>>>> summary(pm)
>>>>
>>>>
>>>> Call:
>>>> lm(formula = billions ~ poly(decade, 3))
>>>>
>>>> Residuals:
>>>>      1       2       3       4       5
>>>> 0.2357 -0.9429  1.4143 -0.9429  0.2357
>>>>
>>>> Coefficients:
>>>>                 Estimate Std. Error t value Pr(>|t|)
>>>> (Intercept)        13.800      0.882  15.647   0.0406 *
>>>> poly(decade, 3)1   25.614      1.972  12.988   0.0489 *
>>>> poly(decade, 3)2   14.432      1.972   7.318   0.0865 .
>>>> poly(decade, 3)3    6.483      1.972   3.287   0.1880
>>>> ---
>>>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>>>
>>>> Residual standard error: 1.972 on 1 degrees of freedom
>>>> Multiple R-Squared: 0.9957,     Adjusted R-squared: 0.9829
>>>> F-statistic: 77.68 on 3 and 1 DF,  p-value: 0.08317
>>>>
>>>>
>>>>> pm
>>>>
>>>>
>>>> Call:
>>>> lm(formula = billions ~ poly(decade, 3))
>>>>
>>>> Coefficients:
>>>>     (Intercept)  poly(decade, 3)1  poly(decade, 3)2  poly(decade, 3)3
>>>>          13.800            25.614            14.432             6.483
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>
>>
>> -- 
>> Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                     Department of Biostatistics   Vanderbilt University
>>
>>
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From chris at subtlety.com  Wed Jun 15 05:16:28 2005
From: chris at subtlety.com (Chris Bergstresser)
Date: Tue, 14 Jun 2005 22:16:28 -0500
Subject: [R] RGui crashes on wle call
In-Reply-To: <Pine.LNX.4.61.0506140718530.11049@gannet.stats>
References: <42AE56F4.3080404@subtlety.com>
	<Pine.LNX.4.61.0506140718530.11049@gannet.stats>
Message-ID: <42AF9D8C.2010804@subtlety.com>

Prof Brian Ripley wrote:
> On Mon, 13 Jun 2005, Chris Bergstresser wrote:
>>    I'm seeing the following commands reliably produce a crash in RGui,
>> version 2.0.1, for both my home and office machine:
>>
>> > rm(list = ls(all = TRUE));
>> > load("dataset.R");
>> > library("wle");
>> > data.wle = wle.lm(abortion ~ year * lib.con + age + gender +
>> + urbanism + census + income + church.att + children + educ +
>> + religion.imp, data = data.set);
>>
> 1) Re-do the tests in the current version of R, preferably a beta of 2.1.1.

    Yeah -- I upgraded to R 2.1.0, and it still reliably crashes.

> 2) Read the rw-FAQ, do the debugging reported there (with Dr MinGW or 
> gdb) and find where it is crashing.  (This is very likely to be in wle.)
> If it is in wle, send a report to the maintainer.  If it is in R, send a 
> report to R-bugs.

    I'm a little loath to download and install a debugger, as I've never 
done it before.  I don't even know what to look for if I were to install it.

 > In either case, supply enough data to reproduce the
> problem.

    I can easily provide the datafile which seems to be causing it. 
It's only 200k, so if anyone is interested in pursuing the matter I'd be 
happy to send it to them.  This is on Windows XP, btw.

-- Chris



From ajayshah at mayin.org  Wed Jun 15 04:36:34 2005
From: ajayshah at mayin.org (Ajay Narottam Shah)
Date: Wed, 15 Jun 2005 08:06:34 +0530
Subject: [R] Puzzled in utilising summary.lm() to obtain Var(x)
In-Reply-To: <971536df050614145914d01377@mail.gmail.com>
References: <20050614130337.GI6989@lubyanka.local>
	<971536df050614145914d01377@mail.gmail.com>
Message-ID: <20050615023634.GL6989@lubyanka.local>

> > I have a program which is doing a few thousand runs of lm(). Suppose
> > it is a simple model
> >   y = a + bx1 + cx2 + e
> > 
> > I have the R object "d" where
> >   d <- summary(lm(y ~ x1 + x2))
> > 
> > I would like to obtain Var(x2) out of "d". How might I do it?
> > 
> > I can, of course, always do sd(x2). But it would be much more
> > convenient if I could snoop around the contents of summary.lm and
> > extract Var() out of it. I couldn't readily see how. Would you know
> > what would click?
> 
> Is the question how to get the variance of a column of the
> model matrix for a model that is the sum of terms given only
> summary output and the column name but not the name of the
> data frame?  If that is it then try this:
> 
> d <- summary(lm(Sepal.Length ~ Sepal.Width, iris)) # test data
> var(model.matrix(eval(d$call))[,"Sepal.Width"])

Yes, this is indeed exactly what I was looking for :-) Thanks,

The eval() pays the full cost of running d$call?

     -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at gmail.com  Wed Jun 15 05:52:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 14 Jun 2005 23:52:56 -0400
Subject: [R] Puzzled in utilising summary.lm() to obtain Var(x)
In-Reply-To: <20050615023634.GL6989@lubyanka.local>
References: <20050614130337.GI6989@lubyanka.local>
	<971536df050614145914d01377@mail.gmail.com>
	<20050615023634.GL6989@lubyanka.local>
Message-ID: <971536df05061420525c6ff8f8@mail.gmail.com>

On 6/14/05, Ajay Narottam Shah <ajayshah at mayin.org> wrote:
> > > I have a program which is doing a few thousand runs of lm(). Suppose
> > > it is a simple model
> > >   y = a + bx1 + cx2 + e
> > >
> > > I have the R object "d" where
> > >   d <- summary(lm(y ~ x1 + x2))
> > >
> > > I would like to obtain Var(x2) out of "d". How might I do it?
> > >
> > > I can, of course, always do sd(x2). But it would be much more
> > > convenient if I could snoop around the contents of summary.lm and
> > > extract Var() out of it. I couldn't readily see how. Would you know
> > > what would click?
> >
> > Is the question how to get the variance of a column of the
> > model matrix for a model that is the sum of terms given only
> > summary output and the column name but not the name of the
> > data frame?  If that is it then try this:
> >
> > d <- summary(lm(Sepal.Length ~ Sepal.Width, iris)) # test data
> > var(model.matrix(eval(d$call))[,"Sepal.Width"])
> 
> Yes, this is indeed exactly what I was looking for :-) Thanks,
> 
> The eval() pays the full cost of running d$call?

Yes. It reruns it.  If we can assume that the second arg to lm is data=
then we could do this which simply grabs the indicated column from
the data frame:

f <- function(d, name) eval(substitute(with(eval(d$call[[3]]), name)))
f(d, Sepal.Width)  # same as iris$Sepal.Width

# or

f <- function(d, charname) eval(d$call[[3]])[[charname]]
f(d, "Sepal.Width")



From ggrothendieck at gmail.com  Wed Jun 15 06:02:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 00:02:22 -0400
Subject: [R] Plotting rows (or columns) from a matrix in different
	graphs, not using "par"
In-Reply-To: <971536df05061418166f8bee66@mail.gmail.com>
References: <8B08A3A1EA7AAC41BE24C750338754E6684D3A@HERMES.demogr.mpg.de>
	<971536df050614150436fc2374@mail.gmail.com>
	<971536df05061418166f8bee66@mail.gmail.com>
Message-ID: <971536df0506142102a3b0304@mail.gmail.com>

On 6/14/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/14/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 6/14/05, Camarda, Carlo Giovanni <Camarda at demogr.mpg.de> wrote:
> > > Dear R-users,
> > > I would like to ask whether it's possible (for sure it would be), to
> > > plot each rows (or columns) in different graphs and in the same figure
> > > region without using the function "par" and then struggling around with
> > > "axes" and labels etc.
> > > Luckily, I would always have "rows + columns = even number" and the same
> > > "ylim".
> > >
> > > The next one could be a sort of example on what I would like to avoid
> > > plotting the rows of the matrix "mat":
> > >
> > > ########### EXAMPLE ######################
> > > dat <- sort(runif(16, 1, 1000))
> > > mat <- matrix(dat, ncol=4, byrow = T)
> > > y   <- seq(1950, 1953)
> > > par(mfrow=c(2,2))
> > > plot(y, mat[1,], ylim=c(1,1000), axes=F, xlab="", ylab="simul.")
> > > box()
> > > axis(side=2, tick=T, labels=T)
> > > axis(side=3, tick=T, labels=T)
> > > plot(y, mat[2,], ylim=c(1,1000), axes=F, xlab="", ylab="")
> > > box()
> > > axis(side=3, tick=T, labels=T)
> > > plot(y, mat[3,], ylim=c(1,1000), axes=F, xlab="years", ylab="simul.")
> > > box()
> > > axis(side=1, tick=T, labels=T)
> > > axis(side=2, tick=T, labels=T)
> > > plot(y, mat[4,], ylim=c(1,1000), axes=F, xlab="years", ylab="")
> > > box()
> > > axis(side=1, tick=T, labels=T)
> > > ########### END EXAMPLE ########################
> > >
> > > Naturally something more compact would be even nicer.
> > >
> >
> > plot(ts(mat, start = 1950), nc = 2)
> >
> 
> I just looked at this again and noticed that it did not have
> identical y axes.  I tried with ylim= too but plot.ts seemed
> to ignore it.
> 
> Here is another solution that uses the zoo library.
> Be sure to use zoo 1.0-1.  It uses the screens= argument of
> plot.zoo which was not available in earlier versions of zoo.
> 
> Instead of creating a ts series with 4 columns create a
> similar zoo series.  Append to that 4 dummy series all
> with the same range.  Use the screens= argument
> on plot.zoo to plot one real series and one dummy series
> in each graph.  Use type = "l" to plot the real series
> and type = "n" for the dummy series.  This causes the
> real series to be plotted with lines and the dummy
> series to be plotted invisibly yet still affect the
> y axis range.    Note that we have used the fact that
> mat is square so if the real mat is not square be sure
> to modify this accordingly.
> 
> library(zoo) #
> 
> n <- nrow(mat)
> z <- zooreg(cbind(mat, max(mat) * diag(n)), start = 2000)
> plot(z, screens = c(1:4, 1:4), type = rep(c("l","n"), each = 4), nc = 2)
> 

The problem with ylim= that seems to affect both plot.ts and plot.zoo
has been fixed in zoo so in the next version of zoo the above will
be reducable to just this:

   plot(zooreg(mat, start = 2000), ylim = range(mat), nc = 2)



From marc.girondot at ese.u-psud.fr  Wed Jun 15 07:27:53 2005
From: marc.girondot at ese.u-psud.fr (Marc Girondot)
Date: Wed, 15 Jun 2005 07:27:53 +0200
Subject: [R] Logistic regression with more than two choices
In-Reply-To: <20050614162426.4DF61302DB@smtp2.song.fi>
References: <20050614162426.4DF61302DB@smtp2.song.fi>
Message-ID: <p06210200bed56b531dad@[129.175.106.62]>

>Dear all R-users,
>
>I am a new user of R and I am trying to build a discrete choice model (with
>more than two alternatives A, B, C and D) using logistic regression. I have
>data that describes the observed choice probabilities and some background
>information. An example below describes the data:
>
>Sex	Age	pr(A)	pr(B)	pr(C)	pr(D) ...
>1	11	0.5	0.5	0	0
>1	40	1	0	0	0
>0	34	0	0	0	1
>0	64	0.1	0.5	0.2	0.2
>...

You can use multinom()
Here is an exemple

For example let this matrix to be analyzed:
male   female   aborted   factor
10     12       1         1.2
14     14       4         1.3
15     12       3         1.4

The data are to be entered in a text file like this:

output  factor  n
m       1.2     10
f       1.2     12
a       1.2     1
m       1.3     14
f       1.3     14
a       1.3     4
m       1.4     15
f       1.4     12
a       1.4     3

library(MASS)

dt.plr <- multinom(output ~ factor, data=dt, weights=n, maxit=1000)
dt.pr1<-predict(dt.plr, , type="probs")
dt.pr1

>I have been able to model a case with only two alternatives "A" and "not A"
>by using glm().
>
>I do not know what functions are available to estimate such a model with
>more than two alternatives. Multinom() is one possibility, but it only
>allows the use of binary 0/1-data instead of observed probabilities. Did I
>understand this correctly?
>
>Additionally, I am willing to use different independent variables for the
>different alternatives in the model. Formally, I mean that:
>Pr(A)=exp(uA)/(exp(uA)+exp(uB)+exp(uC)+exp(uD)
>Pr(B)=exp(uB)/(exp(uA)+exp(uB)+exp(uC)+exp(uD)
>...
>where uA, uB, uC and uD are linear functions with different independent
>variables, e.g. uA=alpha_A1*Age, uB=alpha_B1*Sex.
>
>Do you know how to estimate this type of models in R?

I don't think it is possible... (at least simply, 
without writing all the script !)

Note that I don't undrestand where the residual 
deviance from multinom() come from. I cant find 
the logic.

Marc
-- 

__________________________________________________________
Marc Girondot, Pr
Laboratoire Ecologie, Syst??matique et Evolution
Equipe de Conservation des Populations et des Communaut??s
CNRS, ENGREF et Universit?? Paris-Sud 11 , UMR 8079
B??timent 362
91405 Orsay Cedex, France

Tel:  33 1 (0)1.69.15.72.30   Fax: 33 1 (0)1 69 
15 56 96   e-mail: marc.girondot at ese.u-psud.fr
Web: http://www.ese.u-psud.fr/epc/conservation/Marc.html
Skype: girondot
Fax in US: 1-425-732-6934



From ripley at stats.ox.ac.uk  Wed Jun 15 09:09:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 08:09:11 +0100 (BST)
Subject: [R] anova.lme error
In-Reply-To: <6.1.2.0.2.20050614170743.09cf2ee8@mail.llnl.gov>
References: <6.1.2.0.2.20050614170743.09cf2ee8@mail.llnl.gov>
Message-ID: <Pine.LNX.4.61.0506150806180.17672@gannet.stats>

You have already (incorrectly) filed this a bug report on *anova* and been 
asked to read the R FAQ and discuss this with the package maintainer.

Please do as we ask: only package maintainers can change contributed 
packages (here nlme).

On Tue, 14 Jun 2005, Nisha Mulakken wrote:

> Hi,
>
> I am working with R version 2.1.0, and I seem to have run into what looks
> like a bug. I get the same error message when I run R on Windows as well as
> when I run it on Linux.
>
> When I call anova to do a LR test from inside a function, I get an error.
> The same call works outside of a function. It appears to not find the right
> environment when called from inside a function. I have provided the code
> below.
>
> Thanks,
> Nisha Mulakken
>
> ############################
>
> myFunction <- function(myDataFrame) {
>
>  # Less restricted
>  fit1 <- gls(y ~ dose,
>              weights=varIdent(form=~1|dose),
>              data=myDataFrame)
>
>  # more restricted
>  fit2 <- gls(y ~ dose,
>              data=myDataFrame)
>
>  anova.results <- anova(fit1, fit2)
>  anova.results
> }
>
> df <- data.frame( y=c(12,3,45,1,53,6),
>                   dose=c(0,10,200,0,10,200),
>                   time=c("4.00 hrs", "4.00 hrs", "6.00 hrs", "6.00 hrs",
> "8.00 hrs", "8.00 hrs"),
>                   time.hours=c(4, 4, 6, 6, 8, 8),
>                   rep=rep("a", 6)
>                 )
>
> ## This leads to the following error:
> ##     Error in anova.lme(object = fit1, fit2) : Object "fit2" not found
> results <- myFunction(myDataFrame=df)
>
> #####################################################
> ## The same thing outside of a function
>
> # Less restricted
> fit3 <- gls(y ~ dose,
>              weights=varIdent(form=~1|dose),
>              data=df)
>
> # more restricted
> fit4 <- gls(y ~ dose,
>             data=df)
>
> ## This works:
> anova(fit3, fit4)
>
> ## The results:
> ## > anova(fit3, fit4)
> ##     Model df      AIC      BIC    logLik   Test L.Ratio p-value
> ## fit3     1  5 57.98998 54.92145 -23.99499
> ## fit4     2  3 55.75284 53.91172 -24.87642 1 vs 2 1.76286  0.4142
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 15 09:18:17 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 08:18:17 +0100 (BST)
Subject: [R] Puzzled in utilising summary.lm() to obtain Var(x)
In-Reply-To: <20050615023634.GL6989@lubyanka.local>
References: <20050614130337.GI6989@lubyanka.local>
	<971536df050614145914d01377@mail.gmail.com>
	<20050615023634.GL6989@lubyanka.local>
Message-ID: <Pine.LNX.4.61.0506150813020.17672@gannet.stats>

On Wed, 15 Jun 2005, Ajay Narottam Shah wrote:

>>> I have a program which is doing a few thousand runs of lm(). Suppose
>>> it is a simple model
>>>   y = a + bx1 + cx2 + e
>>>
>>> I have the R object "d" where
>>>   d <- summary(lm(y ~ x1 + x2))
>>>
>>> I would like to obtain Var(x2) out of "d". How might I do it?
>>>
>>> I can, of course, always do sd(x2). But it would be much more
>>> convenient if I could snoop around the contents of summary.lm and
>>> extract Var() out of it. I couldn't readily see how. Would you know
>>> what would click?
>>
>> Is the question how to get the variance of a column of the
>> model matrix for a model that is the sum of terms given only
>> summary output and the column name but not the name of the
>> data frame?  If that is it then try this:
>>
>> d <- summary(lm(Sepal.Length ~ Sepal.Width, iris)) # test data
>> var(model.matrix(eval(d$call))[,"Sepal.Width"])
>
> Yes, this is indeed exactly what I was looking for :-) Thanks,
>
> The eval() pays the full cost of running d$call?

A better way is

> d0 <- lm(Sepal.Length ~ Sepal.Width, iris)
> var(model.matrix(d0)[,"Sepal.Width"])

but if you really only have the summary  (it's wasteful to keep the 
summary here, as it is larger than the fit)

> var(model.matrix(structure(d, class="lm"))[,"Sepal.Width"])


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nassar at noos.fr  Wed Jun 15 09:35:14 2005
From: nassar at noos.fr (Naji)
Date: Wed, 15 Jun 2005 09:35:14 +0200
Subject: [R] Interpolate a matrix z=f(x,y)
In-Reply-To: <Pine.LNX.4.61.0506150813020.17672@gannet.stats>
Message-ID: <BED5A6D2.4625%nassar@noos.fr>


Hi all,


I want to interpolate a matrix as function of its row and column
coordinates, i.e.
{u,v,w}=spline2d(x,y,z...)
Where u & v are the new coordinates values and w the matrix extrapolated

Best regards
Naji



From bela_b at gmx.net  Wed Jun 15 09:59:41 2005
From: bela_b at gmx.net (Bela Bauer)
Date: Wed, 15 Jun 2005 09:59:41 +0200 (MEST)
Subject: [R] Anohter anova.mlm problem
Message-ID: <7039.1118822381@www24.gmx.net>

Hi,

yet another anova.mlm problem - it doesn't seem to end.
This time, I have a setup with a few within-subject factors and a
between-subject factor (SGROUP). Consider the most simple case with only one
within-factor (apo):

> mlmfit0 <- lm(data.n ~ 0 + SGROUP)
> mlmfit1 <- lm(data.n ~ 1 + SGROUP)
> anova(mlmfit1,mlmfit0,test="Spherical",M=~hemi,X=~1)
Analysis of Variance Table

Model 1: data.n ~ 1 + SGROUP
Model 2: data.n ~ 0 + SGROUP

Contrasts orthogonal to
~1


Contrasts spanned by
~hemi

Greenhouse-Geisser epsilon: 0.6829
Huynh-Feldt epsilon:        0.7127

  Res.Df Df Gen.var.   F num Df den Df Pr(>F) G-G Pr H-F Pr
1     22     0.54295
2     22  0  0.54295 Inf      0     44
Warning messages:
1: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
2: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
3: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)


Now, those G-G and H-F epsilons it gives me are correct (at least G-G, the
H-F seems to be affected by the R vs SAS difference in the calculation). But
whatever happened to everything else?
I've played around with the source in mlm.R, and it looks to be as though
the problem is related to this:
https://stat.ethz.ch/pipermail/r-devel/2005-April/032925.html

Is there a solution to this problem? Am I doing something wrong? The data
looks like this:

> hemi
 [1] 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3
Levels: 1 2 3
> SGROUP
 [1] 2 2 2 2 2 2 1 1 1 1 2 2 2 1 1 1 2 1 1 1 2 1 2 1
Levels: 1 2
> dim(data.n)
[1] 24 18

It works fine with the univariate analysis (aov())...

Thanks again,

Bela

-- 
Weitersagen: GMX DSL-Flatrates mit Tempo-Garantie!
Ab 4,99 Euro/Monat: http://www.gmx.net/de/go/dsl



From markus.schwarz at wsl.ch  Wed Jun 15 10:16:37 2005
From: markus.schwarz at wsl.ch (Markus Schwarz)
Date: Wed, 15 Jun 2005 10:16:37 +0200
Subject: [R] (no subject)
Message-ID: <5.2.1.1.1.20050615101235.00bc4010@mail.wsl.ch>

Are there functions available computing the gamma-correlation for ordinal 
scaled variables and kappa-accuracy for nominal variables,
thanks, Mark
...........................................................................
Swiss Federal Institute for Forest, Snow and Landscape Research
Zuercherstrasse 111   -   8903 Birmensdorf   -   Switzerland

e-mail: markus.schwarz at wsl.ch
www:  	http://www.wsl.ch/staff/markus.schwarz/
phone:  +41-1-739-2287
fax:    +41-1-737-2215      	

............................................................................



From andrewr at uidaho.edu  Wed Jun 15 10:32:15 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 15 Jun 2005 01:32:15 -0700
Subject: [R] Plotting second axes outside xyplot
Message-ID: <52978152f47f.52f47f529781@uidaho.edu>

Hi all,

I'm trying to find a way to get xyplot to produce a second set of axes outside the right hand side of the graph.  This is my progress so far:

EE <- equal.count(ethanol$E, number=9, overlap=1/4)
xyplot(NOx ~ C | EE, data = ethanol,
       prepanel = function(x, y) prepanel.loess(x, y, span = 1),
       xlab = "Compression Ratio", ylab = "NOx (micrograms/J)",
       panel = function(x, y) {
            panel.grid(h=-1, v= 2)
            panel.xyplot(x, y)
            panel.loess(x,y, span=1)
            panel.axis(side = "right", at = c(1, 3),
                  labels = c(1, 3), outside = T)
            },
        aspect = "xy")

Does anyone have any suggestions?

Thanks much!

Andrew
--
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



----- Original Message -----
From: Werner Bier <aliscla at yahoo.com>
Date: Monday, June 13, 2005 9:27 am
Subject: Re: [R] kalman filter

> yep! please type
> ?KalmanLike
> or check the dse libraries
> Tom
> 
> m p <mzp3769 at yahoo.com> wrote:
> Hello,
> is there any implementation of Kalman filter in R?
> Thanks,
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> 
>        	
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From massitripoli at hotmail.com  Wed Jun 15 10:35:15 2005
From: massitripoli at hotmail.com (Massimiliano Tripoli)
Date: Wed, 15 Jun 2005 08:35:15 +0000
Subject: [R] Graph with values of coordinates of points in x axis
Message-ID: <BAY18-F25D4814AAACC4479E90C5DDFF20@phx.gbl>

Hi all,

I want to draw a line with the values of x marked in X axis.

I tried with
x <- c(0,6,12,18,24,30)  #coordinates of points x
y <- c(2,5,7,5,7,16)     #coordinates of points y
plot(x,type="n",xlab="Months",ylab="Y 
values",main="main",ylim=c(0,16),xlim=c(0,30))
lines(x,y)

The graph shows by default an increment of the sequence in x axis that I'm 
not able to change.
I'would like to have 0,6,12,18,24,30 and not 0,10,15,20,25,30.
Any hint is appreciated.

Thanks in advance.



From gchappi at gmail.com  Wed Jun 15 10:35:55 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Wed, 15 Jun 2005 10:35:55 +0200
Subject: [R] Possible bug in file.choose() - how to tell?
Message-ID: <47fce065050615013566456a41@mail.gmail.com>

Hi,

I run a script file by dropping it on a windows batch file that runs R
in --slave modus. In a subfunction there is the call to file.choose().
The problem is, that the dialog does show only folders but no files at
all. It's quite strange: a) without --slave modus the files are shown,
b) when I copy the whole script file in a different file it was also
ok, but when I renamed the script, the dialog again only showed folder
names.

Questions:
- Is it appreciated if I submit a bug report on this issue? How would
I do it and to whom?
- I would like to have a look at the source code. As the prompt gives me:
      > file.choose
      function (new = FALSE) 
      .Internal(file.choose(new))
      <environment: namespace:base>
  I suppose I had to download the source code of the base package and it would
  be C code. Is this right?

-- 
Best regards,
Hans-Peter



From Stefano.Guazzetti at ausl.re.it  Wed Jun 15 10:44:03 2005
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Wed, 15 Jun 2005 10:44:03 +0200
Subject: [R] R:  Graph with values of coordinates of points in x axis
Message-ID: <B8A1EED732379B44A7E59D22E82E4442FB2DE3@IMHOTEP.ausl.org>

try: 
 plot(x, y, type="l", xlab="Months", 
      xaxt="n", ylab="Y values")
 axis(1, at=0:5*6)

Stefano
   >-----Messaggio originale-----
   >Da: r-help-bounces at stat.math.ethz.ch
   >[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Massimiliano
   >Tripoli
   >Inviato: mercoled?? 15 giugno 2005 10.35
   >A: r-help at stat.math.ethz.ch
   >Oggetto: [R] Graph with values of coordinates of points in x axis
   >
   >
   >Hi all,
   >
   >I want to draw a line with the values of x marked in X axis.
   >
   >I tried with
   >x <- c(0,6,12,18,24,30)  #coordinates of points x
   >y <- c(2,5,7,5,7,16)     #coordinates of points y
   >plot(x,type="n",xlab="Months",main="main",ylim=c(0,16),xlim=c(0,30))
   >lines(x,y)
   >
   >The graph shows by default an increment of the sequence in 
   >x axis that I'm 
   >not able to change.
   >I'would like to have 0,6,12,18,24,30 and not 0,10,15,20,25,30.
   >Any hint is appreciated.
   >
   >Thanks in advance.
   >
   >______________________________________________
   >R-help at stat.math.ethz.ch mailing list
   >https://stat.ethz.ch/mailman/listinfo/r-help
   >PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Wed Jun 15 10:49:56 2005
From: francoisromain at free.fr (Romain Francois)
Date: Wed, 15 Jun 2005 10:49:56 +0200
Subject: [R] Graph with values of coordinates of points in x axis
In-Reply-To: <BAY18-F25D4814AAACC4479E90C5DDFF20@phx.gbl>
References: <BAY18-F25D4814AAACC4479E90C5DDFF20@phx.gbl>
Message-ID: <42AFEBB4.5010204@free.fr>

Le 15.06.2005 10:35, Massimiliano Tripoli a ??crit :

>Hi all,
>
>I want to draw a line with the values of x marked in X axis.
>
>I tried with
>x <- c(0,6,12,18,24,30)  #coordinates of points x
>y <- c(2,5,7,5,7,16)     #coordinates of points y
>plot(x,type="n",xlab="Months",ylab="Y 
>values",main="main",ylim=c(0,16),xlim=c(0,30))
>lines(x,y)
>
>The graph shows by default an increment of the sequence in x axis that I'm 
>not able to change.
>I'would like to have 0,6,12,18,24,30 and not 0,10,15,20,25,30.
>Any hint is appreciated.
>  
>
Hello, you can do what you want by doing a call to axis, thus in the 
plot call you have to use : axes=F

x <- c(0,6,12,18,24,30)  #coordinates of points x
y <- c(2,5,7,5,7,16)     #coordinates of points y
plot(x,type="n",xlab="Months",ylab="Y 
values",main="main",ylim=c(0,16),xlim=c(0,30),axes=F)
axis(2)
axis(1,(0:5)*6)
lines(x,y)
box()


Romain

-- 
visit the R Graph Gallery : http://addictedtor.free.fr/graphiques
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~
~~~~~~      Romain FRANCOIS - http://addictedtor.free.fr         ~~~~~~
~~~~        Etudiant  ISUP - CS3 - Industrie et Services           ~~~~
~~                http://www.isup.cicrp.jussieu.fr/                  ~~
~~~~           Stagiaire INRIA Futurs - Equipe SELECT              ~~~~
~~~~~~   http://www.inria.fr/recherche/equipes/select.fr.html    ~~~~~~
~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~



From ripley at stats.ox.ac.uk  Wed Jun 15 11:05:22 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 10:05:22 +0100 (BST)
Subject: [R] Possible bug in file.choose() - how to tell?
In-Reply-To: <47fce065050615013566456a41@mail.gmail.com>
References: <47fce065050615013566456a41@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506151001020.25307@gannet.stats>

OS? R version? Locale?

This is a known problem with R 2.1.0 on Windows in some locales, solved 
long ago in R-patched.

On Wed, 15 Jun 2005, Hans-Peter wrote:

> Hi,
>
> I run a script file by dropping it on a windows batch file that runs R
> in --slave modus. In a subfunction there is the call to file.choose().
> The problem is, that the dialog does show only folders but no files at
> all. It's quite strange: a) without --slave modus the files are shown,
> b) when I copy the whole script file in a different file it was also
> ok, but when I renamed the script, the dialog again only showed folder
> names.
>
> Questions:
> - Is it appreciated if I submit a bug report on this issue? How would
> I do it and to whom?
> - I would like to have a look at the source code. As the prompt gives me:
>      > file.choose
>      function (new = FALSE)
>      .Internal(file.choose(new))
>      <environment: namespace:base>
>  I suppose I had to download the source code of the base package and it would
>  be C code. Is this right?

No, it is R code.  But internal functions are in C.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Yes, that does apply to you!  Please do supply the basic information it 
asks for.  If you want to allege a bug, so read carefully the section on 
BUGS in the FAQ.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From subramanian.vivek at gmail.com  Wed Jun 15 11:27:57 2005
From: subramanian.vivek at gmail.com (Vivek Subramanian)
Date: Wed, 15 Jun 2005 14:57:57 +0530
Subject: [R] Reading Excel files...Error
Message-ID: <20e69eb70506150227396b098e@mail.gmail.com>

hi,
i am using the RODBC package to read .xls files. now after i installed
the package and loaded the library and tried to read a file this is
the error i got.

>channel<-odbcConnect("D:/rstuff/1.xls")
Warning messages:
1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver
Manager] Data source name not found and no default driver specified
2: ODBC connection failed in: odbcDriverConnect(st, case = case,
believeNRows = believeNRows)

please help me.

regards,
vivek



From p.dalgaard at biostat.ku.dk  Wed Jun 15 11:29:23 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2005 11:29:23 +0200
Subject: [R] Puzzled in utilising summary.lm() to obtain Var(x)
In-Reply-To: <Pine.LNX.4.61.0506150813020.17672@gannet.stats>
References: <20050614130337.GI6989@lubyanka.local>
	<971536df050614145914d01377@mail.gmail.com>
	<20050615023634.GL6989@lubyanka.local>
	<Pine.LNX.4.61.0506150813020.17672@gannet.stats>
Message-ID: <x2u0k07yto.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Wed, 15 Jun 2005, Ajay Narottam Shah wrote:
> 
> >>> I have a program which is doing a few thousand runs of lm(). Suppose
> >>> it is a simple model
> >>>   y = a + bx1 + cx2 + e
> >>>
> >>> I have the R object "d" where
> >>>   d <- summary(lm(y ~ x1 + x2))
> >>>
> >>> I would like to obtain Var(x2) out of "d". How might I do it?
[snip]
> > var(model.matrix(d0)[,"Sepal.Width"])
> 
> but if you really only have the summary  (it's wasteful to keep the 
> summary here, as it is larger than the fit)
> 
> > var(model.matrix(structure(d, class="lm"))[,"Sepal.Width"])

Allow me to interject a little lateral thinking: 

   solve(vcov(d)[-1,-1]/d$sigma)[2,2]

should give the requested variance,I think.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Jun 15 11:38:23 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2005 11:38:23 +0200
Subject: [R] Anohter anova.mlm problem
In-Reply-To: <7039.1118822381@www24.gmx.net>
References: <7039.1118822381@www24.gmx.net>
Message-ID: <x2psuo7yeo.fsf@turmalin.kubism.ku.dk>

"Bela Bauer" <bela_b at gmx.net> writes:

> Hi,
> 
> yet another anova.mlm problem - it doesn't seem to end.
> This time, I have a setup with a few within-subject factors and a
> between-subject factor (SGROUP). Consider the most simple case with only one
> within-factor (apo):
> 
> > mlmfit0 <- lm(data.n ~ 0 + SGROUP)
> > mlmfit1 <- lm(data.n ~ 1 + SGROUP)
> > anova(mlmfit1,mlmfit0,test="Spherical",M=~hemi,X=~1)
> Analysis of Variance Table
> 
> Model 1: data.n ~ 1 + SGROUP
> Model 2: data.n ~ 0 + SGROUP
> 
> Contrasts orthogonal to
> ~1
> 
> 
> Contrasts spanned by
> ~hemi
> 
> Greenhouse-Geisser epsilon: 0.6829
> Huynh-Feldt epsilon:        0.7127
> 
>   Res.Df Df Gen.var.   F num Df den Df Pr(>F) G-G Pr H-F Pr
> 1     22     0.54295
> 2     22  0  0.54295 Inf      0     44
> Warning messages:
> 1: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
> 2: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
> 3: NaNs produced in: pf(q, df1, df2, lower.tail, log.p)
> 
> 
> Now, those G-G and H-F epsilons it gives me are correct (at least G-G, the
> H-F seems to be affected by the R vs SAS difference in the calculation). But
> whatever happened to everything else?
> I've played around with the source in mlm.R, and it looks to be as though
> the problem is related to this:
> https://stat.ethz.ch/pipermail/r-devel/2005-April/032925.html
> 
> Is there a solution to this problem? Am I doing something wrong? The data
> looks like this:

You're fitting the same model twice, so you get zero DF for the
difference. If you want to compare a model with a difference between
the two groups to one with no difference (in the relevant contrasts),
you need to compare

 mlmfit0 <- lm(data.n ~ SGROUP)
 mlmfit1 <- lm(data.n ~ 1)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From uth at zhwin.ch  Wed Jun 15 12:08:39 2005
From: uth at zhwin.ch (Thomas Unternaehrer)
Date: Wed, 15 Jun 2005 12:08:39 +0200
Subject: [R] Setting environment variables (installation problem)
Message-ID: <42AFFE27.9090707@zhwin.ch>


Dear All,

Is it possible under WinXP to set the R binary path (...\R\bin)
as an environment variable during installation?
(rw2010.exe)

The problem is that I've written a program that needs to know the path
of Rterm.exe during installation.
At the moment the user can't choose the R installation path 
("/DIR={pf}\\R /SILENT").

If I would know where the user install R the InnoSetup part would be 
really simple:
Root: HKLM; Subkey: "SYSTEM\CurrentControlSet\Control\Session 
Manager\Environment"; ValueType: string; ValueName: "PATH"; ValueData: 
"{olddata};{app}\R\bin"
(or something like that)

(R-FAQ for Windows)
The R-FAQ#2.5 tells me that I'm able to specify the path where R will be 
installed but nothing about the environment variable.
And the R-FAQ#2.16 is not what I need.

Duncan Murdoch wrote 
(http://tolstoy.newcastle.edu.au/R/help/04/07/1272.html) that it is not 
recommended to edit the R.iss manually and it seems to me to hard to 
edit the makefiles for my problem.

An other possibility seems to be to read and edit the 
HKLM\Software\R-core\R\InstallPath registry entry, but is there no 
easier way to do that?

Thanks for any hints (and sorry for my english).



Thomas



From roy.werkman at asml.com  Wed Jun 15 12:14:11 2005
From: roy.werkman at asml.com (Roy Werkman)
Date: Wed, 15 Jun 2005 12:14:11 +0200
Subject: [R] Chi square convolution?
Message-ID: <448071208107374B96ED90585EEBA9127A57F9@NLVDHX84.sn-eu.asml.com>


Hi,

I want to determine the confidence interval on the sum of two sigma's.
Is there an easy way to do this in R? I guess I have to use some sort of
chisquare convolution algorithm???

Thanx,
Roy


-- 
The information contained in this communication and any atta...{{dropped}}



From subianto at gmail.com  Wed Jun 15 12:36:51 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 15 Jun 2005 12:36:51 +0200
Subject: [R] update.packages() - gregmisc
In-Reply-To: <971536df05061415033003d4ee@mail.gmail.com>
References: <42AEC026.3000203@gmail.com>	<971536df0506140451621eaf23@mail.gmail.com>
	<42AECBAC.605@gmail.com>
	<971536df05061415033003d4ee@mail.gmail.com>
Message-ID: <42B004C3.4080109@gmail.com>



On this day 6/15/2005 12:03 AM, Gabor Grothendieck wrote:
> Is the code in your post intended to show what worked so others
> will know what to do 

Yes, I succes to remove and install gregmisc again like I have posted 
before.
Regards,
Muhammad Subianto

or is that code intended to show what you
> did but did not work?
> 
> If its the latter, I successfully did it last week and don't 
> clearly remember my precise steps but I may have done this:
> 
> 	R CMD remove gdata
> 	R CMD remove gmodels
> 	R CMD remove gplots
> 	R CMD remove gtools
> 	R CMD remove gregmisc
> 
> I assume that using remove.packages, viz.
> 
>  	remove.packages("gregmisc")
>  	etc.
> 
> would have given the same result.
> 
> 
> On 6/14/05, Muhammad Subianto <subianto at gmail.com> wrote:
> 
>>Thanks.
>>I do like this,
>> > remove.packages("gregmisc", .libPaths()[1])
>> > remove.packages("gtools", .libPaths()[1])
>> > install.packages("gregmisc", .libPaths()[1])
>> > update.packages()
>> > update.packages()
>> > install.packages("gtools", .libPaths()[1])
>> > update.packages()
>> > update.packages()
>> > update.packages(ask='graphics')
>>
>>Regards,
>>Muhammad Subianto
>>R.2.1.0 on W2K
>>
>>On this day 6/14/2005 1:51 PM, Gabor Grothendieck wrote:
>>
>>>On 6/14/05, Muhammad Subianto <subianto at gmail.com> wrote:
>>>
>>>
>>>>Dear all,
>>>>I have a problem to update package gregmisc.
>>>>After I update,
>>>>
>>>>>update.packages(ask='graphics')
>>>>
>>>>trying URL
>>>>'http://cran.at.r-project.org/bin/windows/contrib/2.1/gregmisc_2.0.8.zip'
>>>>Content type 'application/zip' length 2465 bytes
>>>>opened URL
>>>>downloaded 2465 bytes
>>>>
>>>>package 'gregmisc' successfully unpacked and MD5 sums checked
>>>>...
>>>>
>>>>then try to update again, still I must update package gregmisc, etc.
>>>>I have tried 3,4,5, times with the same result.
>>>>
>>>
>>>
>>>This was discussed on r-devel recently.  See:
>>>
>>>https://www.stat.math.ethz.ch/pipermail/r-devel/2005-June/033479.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rn001 at cebas.csic.es  Wed Jun 15 12:45:29 2005
From: rn001 at cebas.csic.es (javier garcia)
Date: Wed, 15 Jun 2005 12:45:29 +0200
Subject: [R] ploting error
Message-ID: <200506151245.29420.rn001@cebas.csic.es>

Hello!

I've tried to execute an R script that I used to use with older versions with 
no problem. In the scrip ,I try to create a series of plots in png format, 
and now there is an error (my actual verion is R2.0.1)

Error in title(main=main,sub=sub,xlab=xlab,ylab=ylab,...):
X11 font at size 14 could not be loaded


Perhaps I did something wrong with this new install?

Thanks and best regards,

Javier

-- 
A. Javier Garcia
Water and Soil conservation department
CEBAS-CSIC
Campus Universitario Espinardo
PO BOX 164
30100 Murcia (SPAIN)
Phone: +34 968 39 62 57
Fax: +34 968 39 62 13
email: rn001 at cebas.csic.es



From gchappi at gmail.com  Wed Jun 15 13:14:59 2005
From: gchappi at gmail.com (Hans-Peter)
Date: Wed, 15 Jun 2005 13:14:59 +0200
Subject: [R] Possible bug in file.choose() - how to tell?
In-Reply-To: <Pine.LNX.4.61.0506151001020.25307@gannet.stats>
References: <47fce065050615013566456a41@mail.gmail.com>
	<Pine.LNX.4.61.0506151001020.25307@gannet.stats>
Message-ID: <47fce06505061504147671f971@mail.gmail.com>

2005/6/15, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> OS? R version? Locale?

Win2000, SP4 - 2.1.0 - 
"LC_COLLATE=German_Switzerland.1252; 
LC_CTYPE=German_Switzerland.1252;
LC_MONETARY=German_Switzerland.1252;
LC_NUMERIC=C;LC_TIME=German_Switzerland.1252"

> This is a known problem with R 2.1.0 on Windows in some locales, solved
> long ago in R-patched.

True. It works perfectly with 2.1.0 patched. Thanks.

> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> Yes, that does apply to you!  Please do supply the basic information it
> asks for.  If you want to allege a bug, so read carefully the section on
> BUGS in the FAQ.

Ok. - For the next time: could I have found out myself, that this was solved? 
(I did search the mail archiv and the r-project website; and also had
a look at the NEWS)

Thanks again and best regards,
Hans-Peter



From r.hankin at noc.soton.ac.uk  Wed Jun 15 13:13:21 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 15 Jun 2005 12:13:21 +0100
Subject: [R] umlauts in Rd files
Message-ID: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>

Hi

I'm having difficulty following the advice in section 2.7 of R-exts.

In one of my packages, there is a function called mobius().

I want to refer to it in the Rd file as the M??bius function, and to 
illustrate the
  M??bius  inversion formula (just to be explicit: this is "Mobius" but 
with two dots over the second letter).

R-exts section 2.7  gives

\enc{J??reskog}{Joreskog}

as an example, but when I cut-and-paste this, the dvi file (as produced 
by R CMD Rd2dvi)
shows the umlauted "o" as A and Z with some diacritical marks, not the 
desired o with
two dots on.

Using \"{o} is fine for the dvi output but not the ascii output.

How do I put an umlauted "o" in an Rd file in such a way as to have a 
nice
ascii help page and nice dvi files?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From coforfe at gmail.com  Wed Jun 15 13:27:10 2005
From: coforfe at gmail.com (Carlos Ortega)
Date: Wed, 15 Jun 2005 13:27:10 +0200
Subject: [R] ploting error
In-Reply-To: <200506151245.29420.rn001@cebas.csic.es>
References: <200506151245.29420.rn001@cebas.csic.es>
Message-ID: <7b18cd4d05061504275b7b7922@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050615/a7fd1487/attachment.pl

From tshort at epri-peac.com  Wed Jun 15 13:28:17 2005
From: tshort at epri-peac.com (Tom Short)
Date: Wed, 15 Jun 2005 11:28:17 +0000 (UTC)
Subject: [R] us zipcode data map
References: <27db823f050613001741d19b46@mail.gmail.com>
	<BAY103-F63B484B05E08B4C56B8BBA6F00@phx.gbl>
Message-ID: <loom.20050615T132302-643@post.gmane.org>

> >My new task (which I have not investigated yet) is to see if facilities
> >exist in R to turn the X11 device into an HTML document with an
> >html-map .... or into a PDF with hyperlinks.

To create an HTML imagemap, see B. Rowlingson's imagemap package at:

http://www.maths.lancs.ac.uk/Software/Imagemap/

To see an online demo of an interactive map generated with this package in Rpad,
see:

http://www.rpad.org/Rpad/mapdemo.Rpad 

- Tom



From ligges at statistik.uni-dortmund.de  Wed Jun 15 13:53:16 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 15 Jun 2005 13:53:16 +0200
Subject: [R] Reading Excel files...Error
In-Reply-To: <20e69eb70506150227396b098e@mail.gmail.com>
References: <20e69eb70506150227396b098e@mail.gmail.com>
Message-ID: <42B016AC.20904@statistik.uni-dortmund.de>

Vivek Subramanian wrote:

> hi,
> i am using the RODBC package to read .xls files. now after i installed
> the package and loaded the library and tried to read a file this is
> the error i got.
> 
> 
>>channel<-odbcConnect("D:/rstuff/1.xls")

"D:/rstuff/1.xls" is not a DSN. Probably you are going to use 
odbcConnectExcel() in this case ...

Uwe Ligges


> Warning messages:
> 1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver
> Manager] Data source name not found and no default driver specified
> 2: ODBC connection failed in: odbcDriverConnect(st, case = case,
> believeNRows = believeNRows)
> 
> please help me.
> 
> regards,
> vivek
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Wed Jun 15 13:57:35 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 15 Jun 2005 07:57:35 -0400
Subject: [R] Finding local minima and maxima
Message-ID: <673535f620ce6c5a519164325a6e47f5@mail.nih.gov>

I have data in the form of (x,y) pairs and would like to find local 
minima and maxima (typically the zeros of the 2nd derivative) of the y 
values.  I looked at numericDeriv, but I don't have an "expression" per 
se.  I looked at optim, also, but it looks like it will find only one 
"global" max or min.  I can code up my own piecewise derivatives, but 
wondered if there is an existing function to do this.

Thanks,
Sean



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Jun 15 14:03:01 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 15 Jun 2005 08:03:01 -0400
Subject: [R] Reading Excel files...Error
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F4094@us-arlington-0668.mail.saic.com>

If you need work-around, than you can always save excel file in csv format
and than read that. See 'read.csv' or 'read.csv2'. 

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vivek Subramanian
Sent: Wednesday, June 15, 2005 5:28 AM
To: rhelp
Subject: [R] Reading Excel files...Error

hi,
i am using the RODBC package to read .xls files. now after i installed the
package and loaded the library and tried to read a file this is the error i
got.

>channel<-odbcConnect("D:/rstuff/1.xls")
Warning messages:
1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver
Manager] Data source name not found and no default driver specified
2: ODBC connection failed in: odbcDriverConnect(st, case = case,
believeNRows = believeNRows)

please help me.

regards,
vivek

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jun 15 14:14:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 13:14:10 +0100 (BST)
Subject: [R] Possible bug in file.choose() - how to tell?
In-Reply-To: <47fce06505061504147671f971@mail.gmail.com>
References: <47fce065050615013566456a41@mail.gmail.com>
	<Pine.LNX.4.61.0506151001020.25307@gannet.stats>
	<47fce06505061504147671f971@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506151313230.31345@gannet.stats>

On Wed, 15 Jun 2005, Hans-Peter wrote:

> 2005/6/15, Prof Brian Ripley <ripley at stats.ox.ac.uk>:
>> OS? R version? Locale?
>
> Win2000, SP4 - 2.1.0 -
> "LC_COLLATE=German_Switzerland.1252;
> LC_CTYPE=German_Switzerland.1252;
> LC_MONETARY=German_Switzerland.1252;
> LC_NUMERIC=C;LC_TIME=German_Switzerland.1252"
>
>> This is a known problem with R 2.1.0 on Windows in some locales, solved
>> long ago in R-patched.
>
> True. It works perfectly with 2.1.0 patched. Thanks.
>
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> Yes, that does apply to you!  Please do supply the basic information it
>> asks for.  If you want to allege a bug, so read carefully the section on
>> BUGS in the FAQ.
>
> Ok. - For the next time: could I have found out myself, that this was solved?
> (I did search the mail archiv and the r-project website; and also had
> a look at the NEWS)

That section asks WIndows users to look at the CHANGES file: it is
documented there.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jun 15 14:16:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 15 Jun 2005 14:16:03 +0200
Subject: [R] Finding local minima and maxima
In-Reply-To: <673535f620ce6c5a519164325a6e47f5@mail.nih.gov>
References: <673535f620ce6c5a519164325a6e47f5@mail.nih.gov>
Message-ID: <42B01C03.1060202@statistik.uni-dortmund.de>

Sean Davis wrote:

> I have data in the form of (x,y) pairs and would like to find local 
> minima and maxima (typically the zeros of the 2nd derivative) of the y 
> values.  I looked at numericDeriv, but I don't have an "expression" per 
> se.  I looked at optim, also, but it looks like it will find only one 
> "global" max or min.  I can code up my own piecewise derivatives, but 
> wondered if there is an existing function to do this.

If you don't have derivatives, and you don't have an expression, and you 
don't have a criteria how to minimize, please tell me of any known 
method that can handle your problem well enough.

You could look whether y[i-1] <y[i] and y[i+1]<y[i] if y is ordered 
according to x, if this most trivial and very non-robust method fits to 
your definition...

Uwe Ligges


> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jun 15 14:21:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 13:21:08 +0100 (BST)
Subject: [R] Setting environment variables (installation problem)
In-Reply-To: <42AFFE27.9090707@zhwin.ch>
References: <42AFFE27.9090707@zhwin.ch>
Message-ID: <Pine.LNX.4.61.0506151315280.31345@gannet.stats>

It is unclear what you want to do, but it is clear that this is not the 
appropriate list (see the posting guide) and that your subject line is 
misleading.

Note that the path to R is normally entered in the registry during 
installation, but this is optional as some users have a read-only 
registry.  So using the registry is not a general solution.

On Wed, 15 Jun 2005, Thomas Unternaehrer wrote:

>
> Dear All,
>
> Is it possible under WinXP to set the R binary path (...\R\bin)
> as an environment variable during installation?
> (rw2010.exe)

> The problem is that I've written a program that needs to know the path
> of Rterm.exe during installation.
> At the moment the user can't choose the R installation path
> ("/DIR={pf}\\R /SILENT").
>
> If I would know where the user install R the InnoSetup part would be
> really simple:
> Root: HKLM; Subkey: "SYSTEM\CurrentControlSet\Control\Session
> Manager\Environment"; ValueType: string; ValueName: "PATH"; ValueData:
> "{olddata};{app}\R\bin"
> (or something like that)
>
> (R-FAQ for Windows)
> The R-FAQ#2.5 tells me that I'm able to specify the path where R will be
> installed but nothing about the environment variable.
> And the R-FAQ#2.16 is not what I need.
>
> Duncan Murdoch wrote
> (http://tolstoy.newcastle.edu.au/R/help/04/07/1272.html) that it is not
> recommended to edit the R.iss manually and it seems to me to hard to
> edit the makefiles for my problem.
>
> An other possibility seems to be to read and edit the
> HKLM\Software\R-core\R\InstallPath registry entry, but is there no
> easier way to do that?
>
> Thanks for any hints (and sorry for my english).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 15 14:28:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 13:28:08 +0100 (BST)
Subject: [R] X11 font problem (was Re:  ploting error)
In-Reply-To: <200506151245.29420.rn001@cebas.csic.es>
References: <200506151245.29420.rn001@cebas.csic.es>
Message-ID: <Pine.LNX.4.61.0506151322000.31345@gannet.stats>

On Wed, 15 Jun 2005, javier garcia wrote:

> I've tried to execute an R script that I used to use with older versions with
> no problem. In the scrip ,I try to create a series of plots in png format,
> and now there is an error (my actual verion is R2.0.1)
>
> Error in title(main=main,sub=sub,xlab=xlab,ylab=ylab,...):
> X11 font at size 14 could not be loaded
>
>
> Perhaps I did something wrong with this new install?

What `older version'?  Perhaps you would like to start from the current 
version of R, 2.1.0 or even 2.1.1 beta?  We don't support 2.0.1 any more.
(Do see the posting guide.)

This message indicates a problem with your X11 fontpaths. See the archives 
for many such discussions.  One possibility is that you are using an 
(approximately) 100dpi screen and only have 75dpi fonts installed or in 
your path.

You haven't told us your OS and it is very likely the solution is 
OS-dependent. (And not just `Linux', either, distribution-dependent.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jun 15 14:31:42 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 15 Jun 2005 14:31:42 +0200
Subject: [R] plots
In-Reply-To: <42AF074A.6030608@uapar.edu>
References: <42AF074A.6030608@uapar.edu>
Message-ID: <42B01FAE.3060704@statistik.uni-dortmund.de>

secretario academico FACEA wrote:

> Dear all,
> Is it possible to change the levels in a mosaic plot, 

Change the levels? This depends on the data, not on the plot function, I 
think...


> the appearance of the level

What does apprearance mean? See ?mosaicplot which tells you how to 
re-order using the argument "sort".


> or the levels size?

Do you mean t he font size for annotation? This is descibed in 
?mosaicplot - see its argument cex.axis.
Or the number of levels? This again is a matter of the data, not the 
plot function.

I'm completely lost what you are going to do...

Uwe Ligges



> For instance:
>    A               C                  E
>             B                  D
> 
> Thanks for your help
> Adri??n
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Jun 15 14:36:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2005 14:36:55 +0200
Subject: [R] umlauts in Rd files
In-Reply-To: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>
Message-ID: <x2ll5b94pk.fsf@turmalin.kubism.ku.dk>

Robin Hankin <r.hankin at noc.soton.ac.uk> writes:

> Hi
> 
> I'm having difficulty following the advice in section 2.7 of R-exts.
> 
> In one of my packages, there is a function called mobius().
> 
> I want to refer to it in the Rd file as the M??bius function, and to 
> illustrate the
>   M??bius  inversion formula (just to be explicit: this is "Mobius" but 
> with two dots over the second letter).
> 
> R-exts section 2.7  gives
> 
> \enc{J??reskog}{Joreskog}
> 
> as an example, but when I cut-and-paste this, the dvi file (as produced 
> by R CMD Rd2dvi)
> shows the umlauted "o" as A and Z with some diacritical marks, not the 
> desired o with
> two dots on.
> 
> Using \"{o} is fine for the dvi output but not the ascii output.
> 
> How do I put an umlauted "o" in an Rd file in such a way as to have a 
> nice
> ascii help page and nice dvi files?

Well... You can't. There's no odiaeresis in ASCII. That's exactly the
problem. In UTF-8 or ISO-Latin-1/9 (aka 8859-1 or ditto with the
addition of the Euro) you can display the character and we did
previously implicitly assume Latin-1. However this is of no use to
people in say Latin-2 locales, and in fact we can no longer spell the
entire R Core Team correctly using any of the Latin-N locales (we
lose either M{\"a}chler or {\v S}imon). 

As far as I understand the current situation, we recommend that text
files be pure ASCII (which has also led us to introduce deliberate
misspellings of various people in the NEWS file and similar places).

What is happening to you is something else though: The double
characters are a tell-tale sign that you have provided UTF-8 to
something that expected an 8-bit encoding like Latin-1. The fix for
that should be to put \encoding{UTF-8} somewhere at the beginning of
the .Rd file.

(I may well have gotten some detail wrong here, Brian probably knows
the best.)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From rn001 at cebas.csic.es  Wed Jun 15 15:04:52 2005
From: rn001 at cebas.csic.es (javier garcia)
Date: Wed, 15 Jun 2005 15:04:52 +0200
Subject: [R] Resolved - Re: X11 font problem (was Re:  ploting error)
In-Reply-To: <Pine.LNX.4.61.0506151322000.31345@gannet.stats>
References: <200506151245.29420.rn001@cebas.csic.es> 
	<Pine.LNX.4.61.0506151322000.31345@gannet.stats>
Message-ID: <200506151504.53209.rn001@cebas.csic.es>

Thanks you very much, Prof Ripley.

My actual version is 2.1.0 (I was confused).

The problem was exactly that one. I've installed the 100dpi fonts and now 
there is no problem.

Thanks and best regards,

Javier
-----------------
El Mi??rcoles, 15 de Junio de 2005 14:28, Prof Brian Ripley escribi??:
> On Wed, 15 Jun 2005, javier garcia wrote:
> > I've tried to execute an R script that I used to use with older versions
> > with no problem. In the scrip ,I try to create a series of plots in png
> > format, and now there is an error (my actual verion is R2.0.1)
> >
> > Error in title(main=main,sub=sub,xlab=xlab,ylab=ylab,...):
> > X11 font at size 14 could not be loaded
> >
> >
> > Perhaps I did something wrong with this new install?
>
> What `older version'?  Perhaps you would like to start from the current
> version of R, 2.1.0 or even 2.1.1 beta?  We don't support 2.0.1 any more.
> (Do see the posting guide.)
>
> This message indicates a problem with your X11 fontpaths. See the archives
> for many such discussions.  One possibility is that you are using an
> (approximately) 100dpi screen and only have 75dpi fonts installed or in
> your path.
>
> You haven't told us your OS and it is very likely the solution is
> OS-dependent. (And not just `Linux', either, distribution-dependent.)

-- 
A. Javier Garcia
Water and Soil conservation department
CEBAS-CSIC
Campus Universitario Espinardo
PO BOX 164
30100 Murcia (SPAIN)
Phone: +34 968 39 62 57
Fax: +34 968 39 62 13
email: rn001 at cebas.csic.es



From MSchwartz at mn.rr.com  Wed Jun 15 15:23:51 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 15 Jun 2005 08:23:51 -0500
Subject: [R] Gamma and Kappa Functions? was: (no subject)
In-Reply-To: <5.2.1.1.1.20050615101235.00bc4010@mail.wsl.ch>
References: <5.2.1.1.1.20050615101235.00bc4010@mail.wsl.ch>
Message-ID: <1118841832.9457.50.camel@localhost.localdomain>

On Wed, 2005-06-15 at 10:16 +0200, Markus Schwarz wrote:
> Are there functions available computing the gamma-correlation for ordinal 
> scaled variables and kappa-accuracy for nominal variables,
> thanks, Mark

Mark,

Please use a meaningful subject.

1. For Gamma:

   A. See the rcorr.cens() function in Frank Harrell's Hmisc package on
      CRAN

   B. See my post here:
      http://tolstoy.newcastle.edu.au/R/help/04/02/0051.html

   C. See the partialAssociations() function in Jens Henrik Badsberg's
      CoCo package on CRAN.


2. For Kappa:

   A. See the cohen.kappa() function in Jim Lemon's concord package on
      CRAN

   B. See David Meyer's Kappa() function in the vcd package on CRAN

   C. See the kappa* functions in Matthias Gamer's irr package on CRAN

   D. See the *kappa functions in Bruno Falissard's psy package on CRAN


HTH,

The Other Marc Schwartz  ;-)



From edd at debian.org  Wed Jun 15 15:19:46 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 15 Jun 2005 13:19:46 +0000 (UTC)
Subject: [R] Reading Excel files...Error
References: <CA0BCF3BED56294AB91E3AD74B849FD57F4094@us-arlington-0668.mail.saic.com>
Message-ID: <loom.20050615T151754-520@post.gmane.org>

Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI <at> saic.com> writes:
> If you need work-around, than you can always save excel file in csv format
> and than read that. See 'read.csv' or 'read.csv2'. 

Or use read.xls() from Greg's gdata package on CRAN.  Gdata "embeds" Perl
code for reading xls files. Works on Linux, Windoze, ... provided Perl is 
installed.

Hth, Dirk



From ripley at stats.ox.ac.uk  Wed Jun 15 15:30:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 14:30:51 +0100 (BST)
Subject: [R] umlauts in Rd files
In-Reply-To: <x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>
	<x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0506151420080.4628@gannet.stats>

On Wed, 15 Jun 2005, Peter Dalgaard wrote:

> Robin Hankin <r.hankin at noc.soton.ac.uk> writes:
>
>> Hi
>>
>> I'm having difficulty following the advice in section 2.7 of R-exts.
>>
>> In one of my packages, there is a function called mobius().
>>
>> I want to refer to it in the Rd file as the M?bius function, and to
>> illustrate the
>>   M?bius  inversion formula (just to be explicit: this is "Mobius" but
>> with two dots over the second letter).
>>
>> R-exts section 2.7  gives
>>
>> \enc{J?reskog}{Joreskog}
>>
>> as an example, but when I cut-and-paste this, the dvi file (as produced
>> by R CMD Rd2dvi)
>> shows the umlauted "o" as A and Z with some diacritical marks, not the
>> desired o with
>> two dots on.
>>
>> Using \"{o} is fine for the dvi output but not the ascii output.
>>
>> How do I put an umlauted "o" in an Rd file in such a way as to have a
>> nice
>> ascii help page and nice dvi files?
>
> Well... You can't. There's no odiaeresis in ASCII. That's exactly the
> problem. In UTF-8 or ISO-Latin-1/9 (aka 8859-1 or ditto with the
> addition of the Euro) you can display the character and we did
> previously implicitly assume Latin-1. However this is of no use to
> people in say Latin-2 locales, and in fact we can no longer spell the
> entire R Core Team correctly using any of the Latin-N locales (we
> lose either M{\"a}chler or {\v S}imon).
>
> As far as I understand the current situation, we recommend that text
> files be pure ASCII (which has also led us to introduce deliberate
> misspellings of various people in the NEWS file and similar places).
>
> What is happening to you is something else though: The double
> characters are a tell-tale sign that you have provided UTF-8 to
> something that expected an 8-bit encoding like Latin-1. The fix for
> that should be to put \encoding{UTF-8} somewhere at the beginning of
> the .Rd file.
>
> (I may well have gotten some detail wrong here, Brian probably knows
> the best.)

UTF-8 for latex does not work well (as yet, at least: there is now a utf8 
encoding that allows at least the first plane (Latin-1) to work).  So it 
would be much better to use Latin-1 for the file and mark it with 
\encoding{latin1} and mark specifically with \enc{M?bius}{Mobius} or your 
preferred transliteration.

The problem is not really for Latin-2 (which does have a and o diaeresis), 
but languages such as Japanese and Chinese which only have ASCII.  So the 
transliteration is for people without any accents in their charset.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From subianto at gmail.com  Wed Jun 15 15:58:17 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 15 Jun 2005 15:58:17 +0200
Subject: [R] how to change automatically 0=no and 1=yes
Message-ID: <42B033F9.7080001@gmail.com>

Dear R-helpers,
I have dataset (data.frame) like below,
   x1  x2  x3   x4   x5  x6  x7  x8  x9 ... x1200
    0    0    0    1    1    0    0    1    1
    1    0    0    1    1    0    0    1    1
    0    1    0    1    1    0    0    1    1
    1    1    0    1    1    0    0    1    1
...
How can I change automatically 0=no and 1=yes.

Thank you very much in advance.
Kindly regards,
Muhammad Subianto



From gilles.guillot at inapg.inra.fr  Wed Jun 15 16:04:57 2005
From: gilles.guillot at inapg.inra.fr (Gilles GUILLOT)
Date: Wed, 15 Jun 2005 16:04:57 +0200
Subject: [R] Calling C function from Fortran
Message-ID: <200506151604.57791.gilles.guillot@inapg.inra.fr>

The example in the R doc 
and the hints from Shusong Jin ,  Ingmar Visser and Reid Huntsinger 
(thanks all three) refer to the case where the function does not have 
arguments.
I'm still looking for a proper sequence of commands 
to call C functions with arguemnts from R.


Imagine I want to evaluate the gamma function.
I want to use the C function called by R.
(I guess it is the one corresponding to the source code 
 I found in the directory  R-2.1.0/src/nmath/gamma.c
of the source distribution).

The following programs do not work (it returns fancy values)


#include <R.h>
#include <Rmath.h>
void F77_SUB(mygammac)(double x, double y) { y = gammafn(x); }


   subroutine mygammaf(x,y)
   double precision x,y
   call mygammac(x,y)  
   end

called in R through 
x <- 3
y <- -999
res <- .Fortran("mygammaf",
                as.double(x),
                as.double(y))

While changing the C code into 
#include <R.h>
#include <Rmath.h>
void F77_SUB(mygammac)(double *x, double *y) { *y = gammafn(*x); }

seems to work fine.
But  R-2.1.0/src/nmath/gamma.c does not need a pointer ?

What is wrong whit he first set of lines ?

What is the correct way to call the C function in R-2.1.0/src/nmath/gamma.c ?

Thanks in advance

Gilles



From sdavis2 at mail.nih.gov  Wed Jun 15 16:06:02 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 15 Jun 2005 10:06:02 -0400
Subject: [R] how to change automatically 0=no and 1=yes
In-Reply-To: <42B033F9.7080001@gmail.com>
References: <42B033F9.7080001@gmail.com>
Message-ID: <5f3ca289b8536936fb60728965b5e68b@mail.nih.gov>

 > x <- data.frame(matrix(c(1,0,1,0,1,1),nrow=3))
 > x[x==0] <- 'no'
 > x[x==1] <- 'yes'
 > x
    X1  X2
1 yes  no
2  no yes
3 yes yes
 >


On Jun 15, 2005, at 9:58 AM, Muhammad Subianto wrote:

> Dear R-helpers,
> I have dataset (data.frame) like below,
>    x1  x2  x3   x4   x5  x6  x7  x8  x9 ... x1200
>     0    0    0    1    1    0    0    1    1
>     1    0    0    1    1    0    0    1    1
>     0    1    0    1    1    0    0    1    1
>     1    1    0    1    1    0    0    1    1
> ...
> How can I change automatically 0=no and 1=yes.
>
> Thank you very much in advance.
> Kindly regards,
> Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.be  Wed Jun 15 16:06:30 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 15 Jun 2005 16:06:30 +0200
Subject: [R] how to change automatically 0=no and 1=yes
References: <42B033F9.7080001@gmail.com>
Message-ID: <006701c571b3$6d9d30b0$0540210a@www.domain>

try this:

dat <- data.frame(matrix(sample(0:1, 100 * 20, TRUE), 100, 20))
############
dat[] <- lapply(dat, factor, levels = c(0, 1), labels = c("no", 
"yes"))
dat


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Muhammad Subianto" <subianto at gmail.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, June 15, 2005 3:58 PM
Subject: [R] how to change automatically 0=no and 1=yes


> Dear R-helpers,
> I have dataset (data.frame) like below,
>   x1  x2  x3   x4   x5  x6  x7  x8  x9 ... x1200
>    0    0    0    1    1    0    0    1    1
>    1    0    0    1    1    0    0    1    1
>    0    1    0    1    1    0    0    1    1
>    1    1    0    1    1    0    0    1    1
> ...
> How can I change automatically 0=no and 1=yes.
>
> Thank you very much in advance.
> Kindly regards,
> Muhammad Subianto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Wed Jun 15 16:09:33 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 15 Jun 2005 10:09:33 -0400
Subject: [R] Finding local minima and maxima
In-Reply-To: <42B01C03.1060202@statistik.uni-dortmund.de>
References: <673535f620ce6c5a519164325a6e47f5@mail.nih.gov>
	<42B01C03.1060202@statistik.uni-dortmund.de>
Message-ID: <c98ba74449fd8a76a87c277d7fba7fce@mail.nih.gov>

Thanks, Uwe.

Actually, I didn't mean to make this complicated--simple vector x and 
values at x in a vector y.  Just wanted to determine the local minima 
and maxima of y[x], but y is not continuous (but is a "smooth" function 
of x).  I will probably use hist or density to do it and then look at 
these curves or cuts.

Sean

On Jun 15, 2005, at 8:16 AM, Uwe Ligges wrote:

> Sean Davis wrote:
>
>> I have data in the form of (x,y) pairs and would like to find local 
>> minima and maxima (typically the zeros of the 2nd derivative) of the 
>> y values.  I looked at numericDeriv, but I don't have an "expression" 
>> per se.  I looked at optim, also, but it looks like it will find only 
>> one "global" max or min.  I can code up my own piecewise derivatives, 
>> but wondered if there is an existing function to do this.
>
> If you don't have derivatives, and you don't have an expression, and 
> you don't have a criteria how to minimize, please tell me of any known 
> method that can handle your problem well enough.
>
> You could look whether y[i-1] <y[i] and y[i+1]<y[i] if y is ordered 
> according to x, if this most trivial and very non-robust method fits 
> to your definition...
>
> Uwe Ligges
>
>
>> Thanks,
>> Sean
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Wed Jun 15 16:16:04 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 15 Jun 2005 09:16:04 -0500
Subject: [R] how to change automatically 0=no and 1=yes
In-Reply-To: <42B033F9.7080001@gmail.com>
References: <42B033F9.7080001@gmail.com>
Message-ID: <1118844964.9457.70.camel@localhost.localdomain>

On Wed, 2005-06-15 at 15:58 +0200, Muhammad Subianto wrote:
> Dear R-helpers,
> I have dataset (data.frame) like below,
>    x1  x2  x3   x4   x5  x6  x7  x8  x9 ... x1200
>     0    0    0    1    1    0    0    1    1
>     1    0    0    1    1    0    0    1    1
>     0    1    0    1    1    0    0    1    1
>     1    1    0    1    1    0    0    1    1
> ...
> How can I change automatically 0=no and 1=yes.
> 
> Thank you very much in advance.
> Kindly regards,
> Muhammad Subianto

The easiest might be to use the following, presuming that your data
frame is called 'df' and all entries are 0/1:

> df
   V1 V2 V3 V4 V5 V6 V7 V8 V9 V10
1   1  0  1  1  1  1  0  1  1   0
2   1  0  1  0  0  0  1  1  0   0
3   0  0  1  0  1  1  1  1  1   0
4   0  0  0  0  1  1  0  1  0   0
5   1  1  1  1  0  1  0  1  1   0
6   1  0  1  1  1  1  0  1  1   1
7   0  1  1  1  0  0  1  0  1   0
8   1  1  1  1  0  0  1  1  0   0
9   1  0  1  1  1  0  1  0  1   0
10  1  0  0  1  1  1  1  1  0   1

# Use ifelse(). By default that will return a
# character matrix, so coerce back to a data
# frame. Note that the entries are "factors"

> as.data.frame(ifelse(df == 0, "No", "Yes"))
    V1  V2  V3  V4  V5  V6  V7  V8  V9 V10
1  Yes  No Yes Yes Yes Yes  No Yes Yes  No
2  Yes  No Yes  No  No  No Yes Yes  No  No
3   No  No Yes  No Yes Yes Yes Yes Yes  No
4   No  No  No  No Yes Yes  No Yes  No  No
5  Yes Yes Yes Yes  No Yes  No Yes Yes  No
6  Yes  No Yes Yes Yes Yes  No Yes Yes Yes
7   No Yes Yes Yes  No  No Yes  No Yes  No
8  Yes Yes Yes Yes  No  No Yes Yes  No  No
9  Yes  No Yes Yes Yes  No Yes  No Yes  No
10 Yes  No  No Yes Yes Yes Yes Yes  No Yes

See ?ifelse for more information.

HTH,

Marc Schwartz



From C.BARILLARI at soton.ac.uk  Wed Jun 15 16:23:48 2005
From: C.BARILLARI at soton.ac.uk (Caterina Barillari)
Date: Wed, 15 Jun 2005 15:23:48 +0100
Subject: [R] questions on bootstrap
Message-ID: <200506151523.48780.caterina@soton.ac.uk>


Hi,

I would like to know:

1)  if there is a way of getting an R-squared value for a bootstrap 
crossvalidation. 
2) how I can get the total residual standard error  for a bootstraped model.

Thanks,

Caterina



From chencheva at gmail.com  Wed Jun 15 16:40:46 2005
From: chencheva at gmail.com (Hu Chen)
Date: Wed, 15 Jun 2005 22:40:46 +0800
Subject: [R] how to plot density distribution with a arrow pointer?
Message-ID: <6f3fc9ee050615074037119902@mail.gmail.com>

Hi all,
for example:
> X<- rnorm(1000)
> X0 <- 0.899

I want to draw a density distribution plot with a arrow pointer
indicating the position of X0, meanwhile, giving out the p-value.

any functions?

Thanks very much.



From steve.roberts at manchester.ac.uk  Wed Jun 15 16:55:11 2005
From: steve.roberts at manchester.ac.uk (Steve Roberts)
Date: Wed, 15 Jun 2005 15:55:11 +0100
Subject: [R] lme/gls heteroscedastic correlation structure
Message-ID: <42B04F5F.18345.F936D@fs1.ser.man.ac.uk>

Greetings,

We want to fit lme/gls models with a compound symmetry variance 
structure, but where the ICC takes different values across different 
groups of clusters. (Specifically a two arm clustrered trial with different 
ICC in each arm) Equivalent models can be fitted with heteroscedastic 
variance functions, but these have positivity constraints which we want 
to avoid and we would prefer to use the correlation structure directly. Is 
there a way to do this? Writing a new variance structure seems rather 
non-trivial - but maybe I haven't tried hard enough.. 

Any ideas?

Steve.

  Dr Steve Roberts 
  steve.roberts at manchester.ac.uk

Senior Lecturer in Medical Statistics,
CMMCH NHS Trust and University of Manchester Biostatistics Group,
0161 275 5192/5764 / 0161 276 5785



From halldor at vedur.is  Wed Jun 15 17:17:11 2005
From: halldor at vedur.is (=?ISO-8859-1?Q?Halldor_Bj=F6rnsson?=)
Date: Wed, 15 Jun 2005 15:17:11 +0000
Subject: [R] Getting the character set
Message-ID: <42B04677.20305@vedur.is>

Hi R-gurus,

In python one can do
 >>> import string
 >>> print string.letters

and get the characters in the local character set.

This feature is often useful in Python, and if I could find it in R
I could make good use of it.

In R both Sys.getlocale() and localeToCharset() can tell me which 
charset I am using, but I haven't found a way to print out the
actual characters. Is there a way to do this?

Sincerely,
H.
-- 
------------------------------------------
Halldor Bjornsson   (halldor at vedur.is)
Vedurstofa Islands (Icelandic Met. Office)
Bustadavegur 9, IS-150, Reykjavik, Iceland



From spencer.graves at pdf.com  Wed Jun 15 17:36:51 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Jun 2005 08:36:51 -0700
Subject: [R] Kalman Filtering?
Message-ID: <42B04B13.2060401@pdf.com>

	  1.  The function "KalmanLike" seems to change its inputs AND 
PREVIOUSLY MADE copies of the inputs.  Consider the following (using R 
2.1.0 patched under Windows XP):

 > Fig2.1 <- StructTS(x=Nile, type="level")
 > unlist(Fig2.1$model0[2:3])
         a         P
      1120 286379470
 > tst2 <- tst <- Fig2.1$model0
 > tst23 <- tst[2:3]
 > tst23u <- unlist(tst23)
 > nile.KL <- KalmanLike(nile, tst2)
 > unlist(tst[2:3])
         a         P
  798.3682 4032.1469
 > unlist(tst2[2:3])
         a         P
  798.3682 4032.1469
 > unlist(Fig2.1$model0[2:3])
         a         P
  798.3682 4032.1469
 > unlist(tst23)
         a         P
  798.3682 4032.1469
 > tst23u
         a         P
      1120 286379470

	  "KalmanLike" changed attributes a and P of its input, tst2, as well 
as tst, from which tst2 was created, and Fig2.1$model0, from which tst 
was created:  Fig2.1$model0 was create by "StructTS" as a list that 
included components a = 1120 and P = 286379470.  When I called 
KalmanLike with a copy of a copy of that argument, KalmanLike changed 
all those copies plus one other, tst23.  The only copy that KalmanLike 
did NOT change was "unlisted", tst23u.  The function "KalmanLike" 
essentialy consists of a call to '.Call("KalmanLike", ...)', and I'm not 
eager to trace this behavior into that ".Call".  Would anyone care to 
comment on this behavior?

	  2.  I'm trying to recreate Figure 2.1 in Durbin and Koopman (2001), 
cited in the KalmanLike help file.  This figure consists of four panels, 
the first of which plots the Nile dataset with, apparently, the filtered 
state output by 'StructTS(x=nile, type="level")' and 'its 90% confidence 
interval".  The following is the code I used to try to recreate this 
figure:

Fig2.1 <- StructTS(x=Nile, type="level")
n.nile <- length(Nile)
cols <- c("filtered", "P", "s.e", "L.9", "U.9")
k <- length(cols)
#nile <- ts(nile., start=1871)
Nile. <- ts(array(c(fitted(Fig2.1), rep(NA, n.nile*(k-1))),
	dim=c(n.nile, length(cols)),
	dimnames=list(NULL, cols)), start=1871)

P <- Inf
h <- Fig2.1$model0$h
V <- Fig2.1$model0$V
for(i in 1:n.nile){
	P <- V+1/((1/P)+(1/h))
	Nile.[i, "P"] <- P
}

s2 <- KalmanLike(nile, Fig2.1$model0)$s2
Fig2.1 <- StructTS(x=Nile, type="level")

Nile.[, "s.e"] <- sqrt(s2*Nile.[, "P"])
z.9 <- qnorm(.95)
Nile.[, "L.9"] <- (Nile.[, "filtered"]-z.9*Nile.[, "s.e"])
Nile.[, "U.9"] <- (Nile.[, "filtered"]+z.9*Nile.[, "s.e"])

plot(nile)
lines(fitted(Fig2.1), lwd=2)
lines(Nile.[,"L.9"], col=2, lwd=2)
lines(Nile.[,"U.9"], col=2, lwd=2)

	  Does anyone have any comments on this?  The data for 1892, 1894, and 
1897 seem slightly outside the 90% confidence interval in Durbin and 
Koopman (2001, Fig. 2.1) but just inside the figure created by this code.

	  Am I doing this correctly?  Is there an easier way?

	  Thanks,
	  spencer graves



From jfbrennan at rogers.com  Wed Jun 15 17:39:28 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Wed, 15 Jun 2005 11:39:28 -0400
Subject: [R] how to plot density distribution with a arrow pointer?
In-Reply-To: <6f3fc9ee050615074037119902@mail.gmail.com>
Message-ID: <200506151539.j5FFdT1x026020@hypatia.math.ethz.ch>

This is one way not so good
R>X<-rnorm(1000000)
R>X0<-.899
R>plot(density(X))
R>abline(v=X0)

A better way

R>x<-seq(-5,5,.01)
R>plot(x,dnorm(x))
R>plot(x,dnorm(x),type="l",col=2)
R>abline(v=X0,col=4)
R>?text
R>text(X0,.2,paste("P(X<X0) =",signif(pnorm(X0),3)),pos=2)
R>abline(v=X0,col=4)

If you actually want arrows do ?arrows   
Jim

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hu Chen
Sent: June 15, 2005 10:41 AM
To: R
Subject: [R] how to plot density distribution with a arrow pointer?

Hi all,
for example:
> X<- rnorm(1000)
> X0 <- 0.899

I want to draw a density distribution plot with a arrow pointer
indicating the position of X0, meanwhile, giving out the p-value.

any functions?

Thanks very much.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kerryrekky at yahoo.com  Wed Jun 15 17:40:58 2005
From: kerryrekky at yahoo.com (Kerry Bush)
Date: Wed, 15 Jun 2005 08:40:58 -0700 (PDT)
Subject: [R] need help on computing double summation
Message-ID: <20050615154058.1053.qmail@web51806.mail.yahoo.com>

Dear helpers in this forum,

   This is a clarified version of my previous
questions  in this forum. I really need your generous
help on this issue.

> Suppose I have the following data set:
> 
> id x y 
> 023 1 2
> 023 2 5
> 023 4 6
> 023 5 7
> 412 2 5
> 412 3 4
> 412 4 6
> 412 7 9
> 220 5 7
> 220 4 8
> 220 9 8
> ......
> 

Now I want to compute the following double summation:

sum_{i=1}^k
sum_{j=1}^{n_i}(x_{ij}-mean(x_i))*(y_{ij}-mean(y_i))

i is from 1 to k,
indexing the ith subject id; and j is from 1 to n_i,
indexing the jth observation for the ith subject.

in the above expression, mean(x_i) is the mean of x
values for the ith
subject, mean(y_i) is the mean of y values for the ith
subject. 

Is there a simple way to do this in R?



From thorstensen at gmx.net  Wed Jun 15 17:53:06 2005
From: thorstensen at gmx.net (Thorstensen Nicolas)
Date: Wed, 15 Jun 2005 17:53:06 +0200
Subject: [R] suppress output of neural network for use in Sweave
Message-ID: <42B04EE2.2070207@gmx.net>

Hi!

How can I suppress the output of the function nnet in the library(nnet) ?

I get the following output :

 > nn = nnet(x, y, size=5, maxit=40, linout=TRUE);
# weights:  16
initial  value 33.511291
iter  10 value 5.792440
iter  20 value 5.279584
iter  30 value 4.800890
iter  40 value 4.593410
final  value 4.593410
stopped after 40 iterations

Since I train many networks and use Sweave I don??t want the output in 
the latex code. So how can I suppress this output?


thx
jeff



From subianto at gmail.com  Wed Jun 15 18:07:06 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Wed, 15 Jun 2005 18:07:06 +0200
Subject: [R] how to change automatically 0=no and 1=yes
In-Reply-To: <42B033F9.7080001@gmail.com>
References: <42B033F9.7080001@gmail.com>
Message-ID: <42B0522A.8090509@gmail.com>

Dear all,
Sean Davis, Dimitris Rizopoulos and Marc Schwartz, thanks for your great 
help. It works perfectly. Thanks a lot.
All the best,
Muhammad Subianto

On this day 6/15/2005 4:06 PM, Sean Davis wrote:
 >  > x <- data.frame(matrix(c(1,0,1,0,1,1),nrow=3))
 >  > x[x==0] <- 'no'
 >  > x[x==1] <- 'yes'
 >  > x
 >     X1  X2
 > 1 yes  no
 > 2  no yes
 > 3 yes yes
 >

On this day 6/15/2005 4:06 PM, Dimitris Rizopoulos wrote:
 > try this:
 >
 > dat <- data.frame(matrix(sample(0:1, 100 * 20, TRUE), 100, 20))
 > ############
 > dat[] <- lapply(dat, factor, levels = c(0, 1), labels = c("no",
 > "yes"))
 > dat
 >

On this day 6/15/2005 4:16 PM, Marc Schwartz wrote:
 >
 >
 >>as.data.frame(ifelse(df == 0, "No", "Yes"))
 >


On this day 6/15/2005 3:58 PM, Muhammad Subianto wrote:
> Dear R-helpers,
> I have dataset (data.frame) like below,
>    x1  x2  x3   x4   x5  x6  x7  x8  x9 ... x1200
>     0    0    0    1    1    0    0    1    1
>     1    0    0    1    1    0    0    1    1
>     0    1    0    1    1    0    0    1    1
>     1    1    0    1    1    0    0    1    1
> ...
> How can I change automatically 0=no and 1=yes.
> 
> Thank you very much in advance.
> Kindly regards,
> Muhammad Subianto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Wed Jun 15 18:09:05 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 16 Jun 2005 00:09:05 +0800
Subject: [R] a question about frame
Message-ID: <0II400HBAV1H3M@mail.fudan.edu.cn>

gg <- function(y) {
         ggg <- function() {
             cat("current frame is", sys.nframe(), "\n")
             cat("parents are", sys.parents(), "\n")
             print(sys.function(0)) # ggg
             print(sys.function(2)) # gg
         }
         if(y > 0) gg(y-1) else ggg()
     }


>  gg(3)
current frame is 5 
parents are 0 1 2 3 4 
function() {
             cat("current frame is", sys.nframe(), "\n")
             cat("parents are", sys.parents(), "\n")
             print(sys.function(0)) # ggg
             print(sys.function(2)) # gg
         }
<environment: 071C62FC>
function(y) {
         ggg <- function() {
             cat("current frame is", sys.nframe(), "\n")
             cat("parents are", sys.parents(), "\n")
             print(sys.function(0)) # ggg
             print(sys.function(2)) # gg
         }
         if(y > 0) gg(y-1) else ggg()
     }


my question is ,why the current frame is 5?why the sys.function is ggg.i am quite confused.
anyone can give me some clue?i have read the ?sys.parent,but i still can not get the point exactly.
 				


2005-06-16

------
Deparment of Sociology
Fudan University

Blog:www.sociology.yculblog.com



From ripley at stats.ox.ac.uk  Wed Jun 15 18:09:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 17:09:05 +0100 (BST)
Subject: [R] Getting the character set
In-Reply-To: <42B04677.20305@vedur.is>
References: <42B04677.20305@vedur.is>
Message-ID: <Pine.LNX.4.61.0506151704001.6611@gannet.stats>

On Wed, 15 Jun 2005, Halldor Bj?rnsson wrote:

> Hi R-gurus,
>
> In python one can do
> >>> import string
> >>> print string.letters
>
> and get the characters in the local character set.
>
> This feature is often useful in Python, and if I could find it in R
> I could make good use of it.
>
> In R both Sys.getlocale() and localeToCharset() can tell me which
> charset I am using, but I haven't found a way to print out the
> actual characters. Is there a way to do this?

No.  There could be 2 billion of them, so I doubt Python really does as 
you say.

In an 8-bit character set you can use

rawToChar(as.raw(0:255), TRUE)

but you might have a 21-bit character set.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jg_liao at yahoo.com  Wed Jun 15 18:19:14 2005
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 15 Jun 2005 09:19:14 -0700 (PDT)
Subject: [R] random number generator: same seed used in different sessions
Message-ID: <20050615161914.10457.qmail@web53707.mail.yahoo.com>

I did several simulation sessions and the result turned out to be a
surprise. After some investigation, I found that different R sessions
of the program used the same seed. Simply, in R210, if I start R and
type rnorm(1), I always get the same random number. This is
contradictary to what is in the R document

 Initially, there is no seed;  a new one is created from the
     current time when one is required.  Hence, different sessions will
     give different simulation results, by default.

I just installed the development version R220. Different sessions of R
do use different seeds as expected.





Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
683 Hoes Lane West, Piscataway? NJ 08854
phone 732-235-5429, School of Public Health office
phone 732-235-9824, Cancer Institute of New Jersey office



From tlumley at u.washington.edu  Wed Jun 15 18:21:21 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 15 Jun 2005 09:21:21 -0700 (PDT)
Subject: [R] Kalman Filtering?
In-Reply-To: <42B04B13.2060401@pdf.com>
References: <42B04B13.2060401@pdf.com>
Message-ID: <Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>

On Wed, 15 Jun 2005, Spencer Graves wrote:

> 	  1.  The function "KalmanLike" seems to change its inputs AND
> PREVIOUSLY MADE copies of the inputs.

The strange thing is that it changes its inputs. It would be expected that 
this also changed previously made copies. The copies will just be 
references to the same actual object in memory (which is why changing the 
inputs is undesirable).

Looking at the C code, it does change the a, P, and Pnew components of the 
model.  I'm a little reluctant to mess with this code, but an R-level 
work-around should be to replace mod$P with mod$P+0 and so on in the 
call to .Call, as this will force copying.

 	-thomas



From B.Rowlingson at lancaster.ac.uk  Wed Jun 15 18:23:41 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 15 Jun 2005 17:23:41 +0100
Subject: [R] suppress output of neural network for use in Sweave
In-Reply-To: <42B04EE2.2070207@gmx.net>
References: <42B04EE2.2070207@gmx.net>
Message-ID: <42B0560D.3010703@lancaster.ac.uk>

Thorstensen Nicolas wrote:
> Hi!
> 
> How can I suppress the output of the function nnet in the library(nnet) ?

  ?nnet suggests adding trace=FALSE to the arguments.

  In other cases if functions dont have this sort of option then on a 
Unix box you can wrap your function calls in sink("/dev/null") [do 
stuff] sink() to send output to a black hole. Not sure what the Windows 
equivalent is.

Baz



From ripley at stats.ox.ac.uk  Wed Jun 15 18:31:09 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 17:31:09 +0100 (BST)
Subject: [R] random number generator: same seed used in different
	sessions
In-Reply-To: <20050615161914.10457.qmail@web53707.mail.yahoo.com>
References: <20050615161914.10457.qmail@web53707.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506151726170.7029@gannet.stats>

On Wed, 15 Jun 2005, Jason Liao wrote:

> I did several simulation sessions and the result turned out to be a
> surprise. After some investigation, I found that different R sessions
> of the program used the same seed. Simply, in R210, if I start R and
> type rnorm(1), I always get the same random number. This is
> contradictary to what is in the R document
>
> Initially, there is no seed;  a new one is created from the
>     current time when one is required.  Hence, different sessions will
>     give different simulation results, by default.

That is not a contradiction!  Did you ensure that this really was 
`initially', so there is no saved workspace, no .Rprofile etc?
See if starting with R --vanilla makes a difference.

> I just installed the development version R220. Different sessions of R
> do use different seeds as expected.

There are no R versions R210 or R220: please do read the posting guide
as we ask:

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Jun 15 18:32:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Jun 2005 09:32:43 -0700
Subject: [R] umlauts in Rd files
In-Reply-To: <Pine.LNX.4.61.0506151420080.4628@gannet.stats>
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>	<x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0506151420080.4628@gannet.stats>
Message-ID: <42B0582B.6080800@pdf.com>

Hi, Robin:

	  Have you considered using "moebius"?  In my elementary German class, 
I was taught that "oe" was an acceptable alternative to the umlaut. 
(This may not be relevant, but Google just gave me 788,000 hits for 
"moebius" and 750,000 for "mobius".)  Perhaps one of our German 
contributors could tell us which they prefer.

	  I use a keyboard that is mostly standard US.  I still sometimes write 
things in Spanish, French, or German.  In addition to my limited 
facility with other languages, I'm also crippled when it comes to 
creating special characters like umlauts, to say nothing of Japanese or 
Chinese characters.  If you want "m??bius" for a package that you expect 
others to use, do you want to restrict its usage only to people who know 
how to create your special characters?  One of our Japanese contributors 
could write a function to compute Akaike's information criterion or 
something else named after a prominent Asian and require the rest of us 
to configure our computers to created the Japanese or Chinese or Korean 
characters to "properly" represent the name.  For the names of files and 
functions, it would be much simpler though less elegant if we restricted 
ourselves to the most universal character set;  of course the content of 
those files, etc., may reflect the different culture and heritage of the 
user.

	  Best Wishes,
	  spencer graves	

Prof Brian Ripley wrote:

> On Wed, 15 Jun 2005, Peter Dalgaard wrote:
> 
>> Robin Hankin <r.hankin at noc.soton.ac.uk> writes:
>>
>>> Hi
>>>
>>> I'm having difficulty following the advice in section 2.7 of R-exts.
>>>
>>> In one of my packages, there is a function called mobius().
>>>
>>> I want to refer to it in the Rd file as the M??bius function, and to
>>> illustrate the
>>>   M??bius  inversion formula (just to be explicit: this is "Mobius" but
>>> with two dots over the second letter).
>>>
>>> R-exts section 2.7  gives
>>>
>>> \enc{J??reskog}{Joreskog}
>>>
>>> as an example, but when I cut-and-paste this, the dvi file (as produced
>>> by R CMD Rd2dvi)
>>> shows the umlauted "o" as A and Z with some diacritical marks, not the
>>> desired o with
>>> two dots on.
>>>
>>> Using \"{o} is fine for the dvi output but not the ascii output.
>>>
>>> How do I put an umlauted "o" in an Rd file in such a way as to have a
>>> nice
>>> ascii help page and nice dvi files?
>>
>>
>> Well... You can't. There's no odiaeresis in ASCII. That's exactly the
>> problem. In UTF-8 or ISO-Latin-1/9 (aka 8859-1 or ditto with the
>> addition of the Euro) you can display the character and we did
>> previously implicitly assume Latin-1. However this is of no use to
>> people in say Latin-2 locales, and in fact we can no longer spell the
>> entire R Core Team correctly using any of the Latin-N locales (we
>> lose either M{\"a}chler or {\v S}imon).
>>
>> As far as I understand the current situation, we recommend that text
>> files be pure ASCII (which has also led us to introduce deliberate
>> misspellings of various people in the NEWS file and similar places).
>>
>> What is happening to you is something else though: The double
>> characters are a tell-tale sign that you have provided UTF-8 to
>> something that expected an 8-bit encoding like Latin-1. The fix for
>> that should be to put \encoding{UTF-8} somewhere at the beginning of
>> the .Rd file.
>>
>> (I may well have gotten some detail wrong here, Brian probably knows
>> the best.)
> 
> 
> UTF-8 for latex does not work well (as yet, at least: there is now a 
> utf8 encoding that allows at least the first plane (Latin-1) to work).  
> So it would be much better to use Latin-1 for the file and mark it with 
> \encoding{latin1} and mark specifically with \enc{M??bius}{Mobius} or 
> your preferred transliteration.
> 
> The problem is not really for Latin-2 (which does have a and o 
> diaeresis), but languages such as Japanese and Chinese which only have 
> ASCII.  So the transliteration is for people without any accents in 
> their charset.
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jun 15 18:34:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 17:34:57 +0100 (BST)
Subject: [R] Kalman Filtering?
In-Reply-To: <Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>
References: <42B04B13.2060401@pdf.com>
	<Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>
Message-ID: <Pine.LNX.4.61.0506151731280.7029@gannet.stats>

On Wed, 15 Jun 2005, Thomas Lumley wrote:

> On Wed, 15 Jun 2005, Spencer Graves wrote:
>
>> 	  1.  The function "KalmanLike" seems to change its inputs AND
>> PREVIOUSLY MADE copies of the inputs.
>
> The strange thing is that it changes its inputs. It would be expected that
> this also changed previously made copies. The copies will just be
> references to the same actual object in memory (which is why changing the
> inputs is undesirable).
>
> Looking at the C code, it does change the a, P, and Pnew components of the
> model.  I'm a little reluctant to mess with this code, but an R-level
> work-around should be to replace mod$P with mod$P+0 and so on in the
> call to .Call, as this will force copying.

It is quite intentional: it says so in the sources and there is a Warning 
in the help file.

It was done to avoid making copies of very large matrices.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jg_liao at yahoo.com  Wed Jun 15 18:39:52 2005
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 15 Jun 2005 09:39:52 -0700 (PDT)
Subject: [R] random number generator: same seed used in different
	sessions
In-Reply-To: <Pine.LNX.4.61.0506151726170.7029@gannet.stats>
Message-ID: <20050615163952.17431.qmail@web53707.mail.yahoo.com>

Thank you, Prof. Ripley. Yes, it turned out that different R sessions
(batch mode) loaded the same work space. Inserting
rm(list=ls(all=TRUE))
on top of the code solves the problem.


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Wed, 15 Jun 2005, Jason Liao wrote:
> 
> > I did several simulation sessions and the result turned out to be a
> > surprise. After some investigation, I found that different R
> sessions
> > of the program used the same seed. Simply, in R210, if I start R
> and
> > type rnorm(1), I always get the same random number. This is
> > contradictary to what is in the R document
> >
> > Initially, there is no seed;  a new one is created from the
> >     current time when one is required.  Hence, different sessions
> will
> >     give different simulation results, by default.
> 
> That is not a contradiction!  Did you ensure that this really was 
> `initially', so there is no saved workspace, no .Rprofile etc?
> See if starting with R --vanilla makes a difference.
> 
> > I just installed the development version R220. Different sessions
> of R
> > do use different seeds as expected.
> 
> There are no R versions R210 or R220: please do read the posting
> guide
> as we ask:
> 
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 


Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
683 Hoes Lane West, Piscataway? NJ 08854
phone 732-235-5429, School of Public Health office
phone 732-235-9824, Cancer Institute of New Jersey office



From spencer.graves at pdf.com  Wed Jun 15 18:42:12 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Jun 2005 09:42:12 -0700
Subject: [R] Kalman Filtering?
In-Reply-To: <Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>
References: <42B04B13.2060401@pdf.com>
	<Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>
Message-ID: <42B05A64.3010400@pdf.com>

Hi, Thomas:

	  Thanks much.  I modified the R code for "KalmanLike" as you 
suggested, and the problem went away.

	  Next question:  Should KalmanLike work the way it does without your 
suggested modification?  If not, how do we get your proposal transmitted 
to the appropriate person so it is appropriately considered for adoption 
in a future release of R?

	  Best Wishes,
	  Spencer Graves

Thomas Lumley wrote:

> On Wed, 15 Jun 2005, Spencer Graves wrote:
> 
> 
>>	  1.  The function "KalmanLike" seems to change its inputs AND
>>PREVIOUSLY MADE copies of the inputs.
> 
> 
> The strange thing is that it changes its inputs. It would be expected that 
> this also changed previously made copies. The copies will just be 
> references to the same actual object in memory (which is why changing the 
> inputs is undesirable).
> 
> Looking at the C code, it does change the a, P, and Pnew components of the 
> model.  I'm a little reluctant to mess with this code, but an R-level 
> work-around should be to replace mod$P with mod$P+0 and so on in the 
> call to .Call, as this will force copying.
> 
>  	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Wed Jun 15 19:12:14 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2005 19:12:14 +0200
Subject: [R] umlauts in Rd files
In-Reply-To: <Pine.LNX.4.61.0506151420080.4628@gannet.stats>
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>
	<x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0506151420080.4628@gannet.stats>
Message-ID: <x2hdfz8ryp.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> UTF-8 for latex does not work well (as yet, at least: there is now a
> utf8 encoding that allows at least the first plane (Latin-1) to work).
> So it would be much better to use Latin-1 for the file and mark it
> with \encoding{latin1} and mark specifically with \enc{M??bius}{Mobius}
> or your preferred transliteration.
> 
> The problem is not really for Latin-2 (which does have a and o
> diaeresis), but languages such as Japanese and Chinese which only have
> ASCII.  So the transliteration is for people without any accents in
> their charset.

Right, I misremembered about odiereses (looks like font tables want to
spell it like that). The Danish ae, oslash, aring are absent in
Latin-2 though.

The utf8 encoding for LaTeX seems a bit better than just Latin 1. See
http://www.unruh.de/DniQ/latex/unicode/tables/

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Jason.Piccone at dce.virginia.gov  Wed Jun 15 19:15:59 2005
From: Jason.Piccone at dce.virginia.gov (Piccone, Jason E.)
Date: Wed, 15 Jun 2005 13:15:59 -0400
Subject: [R] cannot coerce class "matchit" into a data.frame"
Message-ID: <45B7EC643773CA4CADFCA4D9E16B05D47098FE@MAIL.dce.state.va.us>


Greetings fellow humans,

I am attempting to export a text file after using the MatchIt package to match control with treatment subjects.  I attempted to write.table and used the following syntax:

"write.table(social,"shaka.txt",sep=" ",quote=FALSE,row.names=FALSE,col.names= FALSE)"

But received the following error message: 
"Error in as.data.frame.default(x[[i]], optional = TRUE) : cannot coerce class "matchit" into a data.frame"

I thought that maybe I had to convert to a data frame, and tinkered with data.frame, as.data.frame, and is.data.frame, but this didn't work (although I could had done it improperly). Please pardon my novice R skills.

Any guidance will be greatly appreciated.  

Cheers,


Jason E. Piccone, Ph.D.
Research and Evaluation Specialist
Department of Correctional Education



From liuwensui at gmail.com  Wed Jun 15 19:24:30 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 15 Jun 2005 13:24:30 -0400
Subject: [R] suppress output of neural network for use in Sweave
In-Reply-To: <42B04EE2.2070207@gmx.net>
References: <42B04EE2.2070207@gmx.net>
Message-ID: <1115a2b00506151024fcaf5d7@mail.gmail.com>

could you set trace = FALSE in the nnet().

On 6/15/05, Thorstensen Nicolas <thorstensen at gmx.net> wrote:
> Hi!
> 
> How can I suppress the output of the function nnet in the library(nnet) ?
> 
> I get the following output :
> 
>  > nn = nnet(x, y, size=5, maxit=40, linout=TRUE);
> # weights:  16
> initial  value 33.511291
> iter  10 value 5.792440
> iter  20 value 5.279584
> iter  30 value 4.800890
> iter  40 value 4.593410
> final  value 4.593410
> stopped after 40 iterations
> 
> Since I train many networks and use Sweave I don??t want the output in
> the latex code. So how can I suppress this output?
> 
> 
> thx
> jeff
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From f.harrell at vanderbilt.edu  Wed Jun 15 19:36:39 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 15 Jun 2005 12:36:39 -0500
Subject: [R] questions on bootstrap
In-Reply-To: <200506151523.48780.caterina@soton.ac.uk>
References: <200506151523.48780.caterina@soton.ac.uk>
Message-ID: <42B06727.5090204@vanderbilt.edu>

Caterina Barillari wrote:
> Hi,
> 
> I would like to know:
> 
> 1)  if there is a way of getting an R-squared value for a bootstrap 
> crossvalidation. 
> 2) how I can get the total residual standard error  for a bootstraped model.
> 
> Thanks,
> 
> Caterina

Install the Design and Hmisc packages and look at documentation for ols 
and validate.ols.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From uofiowa at gmail.com  Wed Jun 15 19:59:46 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 15 Jun 2005 13:59:46 -0400
Subject: [R] abbreviate
Message-ID: <3f87cc6d05061510591bb2a807@mail.gmail.com>

> p = data.frame(high=c(5,2), settle=c(3,4))
> p
  high settle
1    5      3
2    2      4

What is the most abbreviated way to apply:
if (p$high < p$settle) p$high = p$settle

I want to modify p to become:
> p
  high settle
1    5      3
2    4      4



From ripley at stats.ox.ac.uk  Wed Jun 15 20:04:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 19:04:15 +0100 (BST)
Subject: [R] Kalman Filtering?
In-Reply-To: <42B05A64.3010400@pdf.com>
References: <42B04B13.2060401@pdf.com>
	<Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>
	<42B05A64.3010400@pdf.com>
Message-ID: <Pine.LNX.4.61.0506151900440.11992@gannet.stats>

On Wed, 15 Jun 2005, Spencer Graves wrote:

> 	  Thanks much.  I modified the R code for "KalmanLike" as you
> suggested, and the problem went away.
>
> 	  Next question:  Should KalmanLike work the way it does without your
> suggested modification?  If not, how do we get your proposal transmitted
> to the appropriate person so it is appropriately considered for adoption
> in a future release of R?

It works as it is documented to do, for a very good reason.
I've reiterated the comment in the code in the help page, for those who 
don't read the sources (like you: perhaps you should learn to).

> Thomas Lumley wrote:
>
>> On Wed, 15 Jun 2005, Spencer Graves wrote:
>>
>>
>>> 	  1.  The function "KalmanLike" seems to change its inputs AND
>>> PREVIOUSLY MADE copies of the inputs.
>>
>>
>> The strange thing is that it changes its inputs. It would be expected that
>> this also changed previously made copies. The copies will just be
>> references to the same actual object in memory (which is why changing the
>> inputs is undesirable).
>>
>> Looking at the C code, it does change the a, P, and Pnew components of the
>> model.  I'm a little reluctant to mess with this code, but an R-level
>> work-around should be to replace mod$P with mod$P+0 and so on in the
>> call to .Call, as this will force copying.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HankeA at mar.dfo-mpo.gc.ca  Wed Jun 15 20:17:56 2005
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 15 Jun 2005 15:17:56 -0300
Subject: [R] Error using newdata argument in survfit
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE02043770@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050615/cc358812/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 15 20:33:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 19:33:11 +0100 (BST)
Subject: [R] abbreviate
In-Reply-To: <3f87cc6d05061510591bb2a807@mail.gmail.com>
References: <3f87cc6d05061510591bb2a807@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506151929200.12352@gannet.stats>

On Wed, 15 Jun 2005, Omar Lakkis wrote:

>> p = data.frame(high=c(5,2), settle=c(3,4))
>> p
>  high settle
> 1    5      3
> 2    2      4
>
> What is the most abbreviated way to apply:
> if (p$high < p$settle) p$high = p$settle
>
> I want to modify p to become:
>> p
>  high settle
> 1    5      3
> 2    4      4

p[[1]] <- pmax(p[[1]], p[[2]])

seems to need a rather small number of keystrokes at the expense of 
readability (I would otherwise use p$high etc).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at mn.rr.com  Wed Jun 15 20:33:43 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 15 Jun 2005 13:33:43 -0500
Subject: [R] abbreviate
In-Reply-To: <3f87cc6d05061510591bb2a807@mail.gmail.com>
References: <3f87cc6d05061510591bb2a807@mail.gmail.com>
Message-ID: <1118860423.4423.5.camel@localhost.localdomain>

On Wed, 2005-06-15 at 13:59 -0400, Omar Lakkis wrote:
> > p = data.frame(high=c(5,2), settle=c(3,4))
> > p
>   high settle
> 1    5      3
> 2    2      4
> 
> What is the most abbreviated way to apply:
> if (p$high < p$settle) p$high = p$settle
> 
> I want to modify p to become:
> > p
>   high settle
> 1    5      3
> 2    4      4

Probably the easiest would be:

> p
  high settle
1    5      3
2    2      4

> p$high <- with(p, ifelse(high < settle, settle, high))

> p
  high settle
1    5      3
2    4      4


See ?ifelse and ?with

HTH,

Marc Schwartz



From edd at debian.org  Wed Jun 15 20:30:03 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 15 Jun 2005 18:30:03 +0000 (UTC)
Subject: [R] [OT] Re:  umlauts in Rd files
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>	<x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0506151420080.4628@gannet.stats>
	<42B0582B.6080800@pdf.com>
Message-ID: <loom.20050615T201446-461@post.gmane.org>

Spencer Graves <spencer.graves <at> pdf.com> writes:
> (This may not be relevant, but Google just gave me 788,000 hits for 
> "moebius" and 750,000 for "mobius".)  Perhaps one of our German 
> contributors could tell us which they prefer.

As you asked for it:  The former is correct, and the second is a misspelling. So
there is no choice but simply "right" and "wrong".

The only alternative to the full '8-bit' spelling with an Umlaut is the
'transcribed 7-bit' form of replacing the 'two dots over the vowel' with an
'appended e after the vowel'.  As in Maechler, or in the From: field above.

The problem is that most of the world simply drops the dots: E.g. the comparison
 of 'Wolfgang Hardle' and 'Wolfgang Haerdle' at scholar.google.com shows that the 
correct form is (barely) leading with a narrow 227 to 220.   My PhD dissertation
is archived in the French Bibliotheque Nationale under Eddelbuttel as I opted
for the correct 8-bit form (using LaTeX) which got of course transcribed into the
incorrect Eddelbuttel.  Some lessons are learned the hard way ...

Hope this helps,  Dirk



From MSchwartz at mn.rr.com  Wed Jun 15 20:42:08 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 15 Jun 2005 13:42:08 -0500
Subject: [R] abbreviate
In-Reply-To: <Pine.LNX.4.61.0506151929200.12352@gannet.stats>
References: <3f87cc6d05061510591bb2a807@mail.gmail.com>
	<Pine.LNX.4.61.0506151929200.12352@gannet.stats>
Message-ID: <1118860928.4423.7.camel@localhost.localdomain>

On Wed, 2005-06-15 at 19:33 +0100, Prof Brian Ripley wrote:
> On Wed, 15 Jun 2005, Omar Lakkis wrote:
> 
> >> p = data.frame(high=c(5,2), settle=c(3,4))
> >> p
> >  high settle
> > 1    5      3
> > 2    2      4
> >
> > What is the most abbreviated way to apply:
> > if (p$high < p$settle) p$high = p$settle
> >
> > I want to modify p to become:
> >> p
> >  high settle
> > 1    5      3
> > 2    4      4
> 
> p[[1]] <- pmax(p[[1]], p[[2]])
> 
> seems to need a rather small number of keystrokes at the expense of 
> readability (I would otherwise use p$high etc).

I do like that approach. Definitely less keystrokes...

:-)

Marc



From m_nica at hotmail.com  Wed Jun 15 20:50:25 2005
From: m_nica at hotmail.com (Mihai Nica)
Date: Wed, 15 Jun 2005 13:50:25 -0500
Subject: [R] Multiple line plots
Message-ID: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>

Greetings,

I would like to plot three lines on the same figure, and I am lost. There is 
an answer to a similar threadÖ but I tried matplot and it is beyond me. An 
example of the data follows:

Year	EM	IM	BM
1983	9.1	16.8	-7.7
1984	12.0	18.0	-6.0
1985	13.6	19.1	-5.5
1986	12.4	17.3	-4.9
1987	14.6	20.3	-5.7
1988	20.6	23.3	-2.6
1989	25.0	27.2	-2.2
1990	28.4	30.2	-1.8
1991	33.3	31.2	2.1
1992	40.6	35.2	5.4
1993	41.6	39.9	1.7
1994	50.8	49.5	1.3
1995	46.3	61.7	-15.4
1996	56.8	73.0	-16.2
1997	71.4	85.9	-14.5
1998	79.0	94.7	-15.7
1999	87.0	109.7	-22.7
2000	111.7	135.9	-24.2
2001	101.5	131.4	-29.9
2002	97.5	134.6	-37.1
2003	97.4	138.1	-40.6
2004	110.8	155.8	-45.1

Thanks,

Mihai



From lisas at salford-systems.com  Wed Jun 15 20:57:17 2005
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 15 Jun 2005 11:57:17 -0700
Subject: [R] Salford Systems Data Mining Conferences 2006
Message-ID: <42B07A0D.30707@salford-systems.com>

Apologies for cross posting....

**Reserve the Dates**
SALFORD SYSTEMS DATA MINING 2006 CONFERENCES
San Diego, California: March 29 - March 31, 2006
Sydney, Australia: June 7 - June 8, 2006

The conference will offer prominent keynote speakers, including Jerome 
Friedman, coupled with informative real world application stories. There 
will be special courses available for attendees who are new to Data Mining.

If you would like more information, please click here: 
http://www.salforddatamining.com/2006InfoRequest.php

**Announcing First Call for Abstracts: June 1 - October 1, 2005**
Online Submission: http://www.salforddatamining.com/2006Abstract.php
Deadline: October 1, 2005 (no flexibility)

Best regards,
Lisa Solomon
Ph: (619)543-8880 x109



From ales.ziberna at guest.arnes.si  Wed Jun 15 21:05:46 2005
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Wed, 15 Jun 2005 21:05:46 +0200
Subject: [R] Multiple line plots
References: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
Message-ID: <012a01c571dd$3d555200$0100a8c0@ales>

assuming that your matrix is named x, try

matplot(x,type="l")

Ales Ziberna

P.S.: see "?matplot" for help to matplot. That it all it took me!


----- Original Message ----- 
From: "Mihai Nica" <m_nica at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 15, 2005 8:50 PM
Subject: [R] Multiple line plots


> Greetings,
>
> I would like to plot three lines on the same figure, and I am lost. There 
> is
> an answer to a similar thread. but I tried matplot and it is beyond me. An
> example of the data follows:
>
> Year EM IM BM
> 1983 9.1 16.8 -7.7
> 1984 12.0 18.0 -6.0
> 1985 13.6 19.1 -5.5
> 1986 12.4 17.3 -4.9
> 1987 14.6 20.3 -5.7
> 1988 20.6 23.3 -2.6
> 1989 25.0 27.2 -2.2
> 1990 28.4 30.2 -1.8
> 1991 33.3 31.2 2.1
> 1992 40.6 35.2 5.4
> 1993 41.6 39.9 1.7
> 1994 50.8 49.5 1.3
> 1995 46.3 61.7 -15.4
> 1996 56.8 73.0 -16.2
> 1997 71.4 85.9 -14.5
> 1998 79.0 94.7 -15.7
> 1999 87.0 109.7 -22.7
> 2000 111.7 135.9 -24.2
> 2001 101.5 131.4 -29.9
> 2002 97.5 134.6 -37.1
> 2003 97.4 138.1 -40.6
> 2004 110.8 155.8 -45.1
>
> Thanks,
>
> Mihai
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Jun 15 21:10:36 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Jun 2005 12:10:36 -0700
Subject: [R] Chi square convolution?
In-Reply-To: <448071208107374B96ED90585EEBA9127A57F9@NLVDHX84.sn-eu.asml.com>
References: <448071208107374B96ED90585EEBA9127A57F9@NLVDHX84.sn-eu.asml.com>
Message-ID: <42B07D2C.50508@pdf.com>

	  Have you considered the "distr" package?

	  Consider the following:

 > library(distr)
 > C1234 <- Chisq(df=1, ncp=2)+Chisq(df=3, ncp=4)
 > # = Chisq(df=1+3, ncp=2+4)
 > q(C1234)(c(.025, .975))
[1]  1.823846 23.435932
 > qchisq(c(.025, .975), 4, 6)
[1]  1.823846 23.435932
 >
 > C12345 <- Chisq(df=1, ncp=2)+3*Chisq(df=4, ncp=5)
Warning message:
Grid for approxfun too wide, increase DefaultNrFFTGridPointsExponent in: 
Chisq(df = 1, ncp = 2) + 3 * Chisq(df = 4, ncp = 5)
 > # NOT a standard distribution
 > q(C12345)(c(.025, .975))
[1]  6.562148 68.498397

	  spencer graves

Roy Werkman wrote:

> Hi,
> 
> I want to determine the confidence interval on the sum of two sigma's.
> Is there an easy way to do this in R? I guess I have to use some sort of
> chisquare convolution algorithm???
> 
> Thanx,
> Roy
> 
>



From MSchwartz at mn.rr.com  Wed Jun 15 21:12:32 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 15 Jun 2005 14:12:32 -0500
Subject: [R] Multiple line plots
In-Reply-To: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
References: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
Message-ID: <1118862752.4423.17.camel@localhost.localdomain>

On Wed, 2005-06-15 at 13:50 -0500, Mihai Nica wrote:
> Greetings,
> 
> I would like to plot three lines on the same figure, and I am lost. There is 
> an answer to a similar thread but I tried matplot and it is beyond me. An 
> example of the data follows:
> 
> Year	EM	IM	BM
> 1983	9.1	16.8	-7.7
> 1984	12.0	18.0	-6.0
> 1985	13.6	19.1	-5.5
> 1986	12.4	17.3	-4.9
> 1987	14.6	20.3	-5.7
> 1988	20.6	23.3	-2.6
> 1989	25.0	27.2	-2.2
> 1990	28.4	30.2	-1.8
> 1991	33.3	31.2	2.1
> 1992	40.6	35.2	5.4
> 1993	41.6	39.9	1.7
> 1994	50.8	49.5	1.3
> 1995	46.3	61.7	-15.4
> 1996	56.8	73.0	-16.2
> 1997	71.4	85.9	-14.5
> 1998	79.0	94.7	-15.7
> 1999	87.0	109.7	-22.7
> 2000	111.7	135.9	-24.2
> 2001	101.5	131.4	-29.9
> 2002	97.5	134.6	-37.1
> 2003	97.4	138.1	-40.6
> 2004	110.8	155.8	-45.1
> 
> Thanks,
> 
> Mihai


# First argument is x axis vector
# Second argument is the y value vectors

matplot(df$Year, df[, -1], type = "l", xlab = "Year", 
        ylab = "Some Unit of Measure")

If you want a legend, you could use the following:

legend(1985, 150, legend = colnames(df)[-1], lty = 1:3, col = 1:3)

See ?legend as well.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Wed Jun 15 21:24:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 20:24:26 +0100 (BST)
Subject: [R] Error using newdata argument in survfit
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE02043770@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE02043770@msgmarsta01.bio.dfo.ca>
Message-ID: <Pine.LNX.4.61.0506152022470.25458@gannet.stats>

You appear to have a coding for prior.f in newdata rather than the factor 
itself.

It's a bit hard to be sure when we don't have data8 to compare with.

On Wed, 15 Jun 2005, Hanke, Alex wrote:

> Dear R-helpers,
> To get curves for a pseudo cohort other than the one centered at the mean of
> the covariates, I have been trying to use the newdata argument to survfit
> with no success. Here is my model statement, the newdata and the ensuing
> error. What am I doing wrong?
>
>> summary(fit)
> Call:
> coxph(formula = Surv(Start, Stop, Event, type = "counting") ~
>    Week + LagAOO + Prior.f + cluster(interaction(Station, Year)),
>    data = data8, method = "breslow", x = T, y = T)
>
>  n= 1878
>            coef exp(coef) se(coef) robust se     z       p
> Week     0.00582      1.01   0.0323    0.0239 0.244 8.1e-01
> LagAOO   0.71929      2.05   0.1238    0.1215 5.918 3.3e-09
> Prior.f2 0.12927      1.14   0.4402    0.4025 0.321 7.5e-01
> Prior.f3 0.79082      2.21   0.5484    0.4460 1.773 7.6e-02
> Prior.f4 2.04189      7.71   0.6008    0.4685 4.358 1.3e-05
> Prior.f5 1.20450      3.34   0.6423    0.5481 2.198 2.8e-02
>
>         exp(coef) exp(-coef) lower .95 upper .95
> Week          1.01      0.994     0.960      1.05
> LagAOO        2.05      0.487     1.618      2.61
> Prior.f2      1.14      0.879     0.517      2.50
> Prior.f3      2.21      0.453     0.920      5.29
> Prior.f4      7.71      0.130     3.076     19.30
> Prior.f5      3.34      0.300     1.139      9.76
>
> Rsquare= 0.047   (max possible= 0.25 )
> Likelihood ratio test= 91  on 6 df,   p=0
> Wald test            = 209  on 6 df,   p=0
> Score (logrank) test = 142  on 6 df,   p=0,   Robust = 17.4  p=0.00803
>
>  (Note: the likelihood ratio and score tests assume independence of
>     observations within a cluster, the Wald and robust score tests do not).
>>
> newdat
>      Week   LagAOO Prior.f2 Prior.f3 Prior.f4 Prior.f5
> 1 17.55218 1.191693        1        0        0        0
> 2 17.55218 1.191693        0        0        0        0
>
>> survfit(fit,newdata=newdat)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        variable lengths differ
> In addition: Warning message:
> 'newdata' had 2 rows but variable(s) found have 1878 rows
>
> Regards,
> Alex
>
>
> Alex Hanke
> Department of Fisheries and Oceans
> St. Andrews Biological Station
> 531 Brandy Cove Road
> St. Andrews, NB
> Canada
> E5B 2L9
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 15 21:39:28 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Jun 2005 20:39:28 +0100 (BST)
Subject: [R] [OT] Re:  umlauts in Rd files
In-Reply-To: <loom.20050615T201446-461@post.gmane.org>
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>
	<x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0506151420080.4628@gannet.stats>
	<42B0582B.6080800@pdf.com> <loom.20050615T201446-461@post.gmane.org>
Message-ID: <Pine.LNX.4.61.0506151946220.12583@gannet.stats>

As this is a proper name, is not what is `right' what the person concerned 
prefers (if known)?

Guessing on their behalf is not unambiguous: even if you know someone is, 
say, German, their surname may not be or vice versa.

People have even written to me to `correct' the spelling of `Box and 
Muller' in my books.  (Long ago a then colleague checked with Muller 
himself that was his preferred spelling.)

On Wed, 15 Jun 2005, Dirk Eddelbuettel wrote:

> Spencer Graves <spencer.graves <at> pdf.com> writes:
>> (This may not be relevant, but Google just gave me 788,000 hits for
>> "moebius" and 750,000 for "mobius".)  Perhaps one of our German
>> contributors could tell us which they prefer.
>
> As you asked for it:  The former is correct, and the second is a misspelling. So
> there is no choice but simply "right" and "wrong".
>
> The only alternative to the full '8-bit' spelling with an Umlaut is the
> 'transcribed 7-bit' form of replacing the 'two dots over the vowel' with an
> 'appended e after the vowel'.  As in Maechler, or in the From: field above.
>
> The problem is that most of the world simply drops the dots: E.g. the comparison
> of 'Wolfgang Hardle' and 'Wolfgang Haerdle' at scholar.google.com shows that the
> correct form is (barely) leading with a narrow 227 to 220.   My PhD dissertation
> is archived in the French Bibliotheque Nationale under Eddelbuttel as I opted
> for the correct 8-bit form (using LaTeX) which got of course transcribed into the
> incorrect Eddelbuttel.  Some lessons are learned the hard way ...
>
> Hope this helps,  Dirk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pgonzalez at fcnym.unlp.edu.ar  Wed Jun 15 22:05:21 2005
From: pgonzalez at fcnym.unlp.edu.ar (pgonzalez@fcnym.unlp.edu.ar)
Date: Wed, 15 Jun 2005 17:05:21 -0300
Subject: [R] ellipse -confidence interval
Message-ID: <1118865921.42b08a014f1d5@webmail.fcnym.unlp.edu.ar>

Hi!, I made a plot with two variables representing the first two axis of a
principle component analysis. My question is:
How can I superimpose on such plot an ellipse that represent the 95% interval of
confidence of scores?
Best regards,
Paula





--
Lic. Paula Gonzalez
Divisi??n Antropolog??a
Museo de La Plata.
Facultad de Ciencias Naturales y Museo. UNLP.
Paseo del Bosque s/n?? - B1900FWA - La Plata - Argentina.



From macq at llnl.gov  Wed Jun 15 22:11:04 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 15 Jun 2005 13:11:04 -0700
Subject: [R] Multiple line plots
In-Reply-To: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
References: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
Message-ID: <p0621020dbed63b2ad0e8@[128.115.153.6]>

Here is an example. It assumes the data is in a 
data frame, with all variables numeric.

  tst <- data.frame(yr=1981:1990,y1=rnorm(10), y2=5+rnorm(10), y3=rnorm(10)-3)
  plot(tst$yr, tst$y1, ylim=range(tst[,-1]),type='l')
  lines(tst$yr,tst$y2,col=2)
  lines(tst$yr,tst$y3,col=3)

-Don

At 1:50 PM -0500 6/15/05, Mihai Nica wrote:
>Content-Type: text/plain; format=flowed
>X-MIME-Autoconverted: from 8bit to 
>quoted-printable by smtp-4.llnl.gov id 
>j5FIxVe7010769
>
>Greetings,
>
>I would like to plot three lines on the same 
>figure, and I am lost. There is an answer to a 
>similar thread
 but I tried matplot and it is 
>beyond me. An example of the data follows:
>
>Year	EM	IM	BM
>1983	9.1	16.8	-7.7
>1984	12.0	18.0	-6.0
>1985	13.6	19.1	-5.5
>1986	12.4	17.3	-4.9
>1987	14.6	20.3	-5.7
>1988	20.6	23.3	-2.6
>1989	25.0	27.2	-2.2
>1990	28.4	30.2	-1.8
>1991	33.3	31.2	2.1
>1992	40.6	35.2	5.4
>1993	41.6	39.9	1.7
>1994	50.8	49.5	1.3
>1995	46.3	61.7	-15.4
>1996	56.8	73.0	-16.2
>1997	71.4	85.9	-14.5
>1998	79.0	94.7	-15.7
>1999	87.0	109.7	-22.7
>2000	111.7	135.9	-24.2
>2001	101.5	131.4	-29.9
>2002	97.5	134.6	-37.1
>2003	97.4	138.1	-40.6
>2004	110.8	155.8	-45.1
>
>Thanks,
>
>Mihai
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From reid_huntsinger at merck.com  Wed Jun 15 22:27:29 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 15 Jun 2005 16:27:29 -0400
Subject: [R] need help on computing double summation
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9496@uswpmx00.merck.com>

You could do something like

ids <- unique(mydata$id)
ans <- vector(length=length(ids), mode="list")
for (i in ids) {
  g <- which(mydata$id == i)
  ans[[i]] <- (length(g) - 1)*cov(mydata$x[g], mydata$y[g])
}
ans

but cov() returns NA for length 1 vectors, so you'd want an if (length(g) ==
1) ans[i] <- 0 else ans[i] <- ... construction.

This is almost brute force; you could also use tapply, as follows:

sx <- tapply(mydata$x,INDEX=mydata$id,FUN=sum)
sy <- tapply(mydata$y,INDEX=mydata$id,FUN=sum)
sxy <- tapply(mydata$x*mydata$y, INDEX=mydata$id, FUN=sum)
n <- tapply(mydata$id,INDEX=mydata$id,FUN=length) # or use table()!

and now your inner sum is

sxy - 2*sx*(sy/n) + n*(sx/n)*(sy/n) = sxy - sx*sy/n

so 

sum(sxy - sx*sy/n) should do.

One more approach is to make your dataset into a list of data frames, one
for each id, then use lapply(). The list can be created by split(). In one
line,

lapply(split(mydata,f=mydata$id),function(z) (length(z$x) - 1)*cov(z$x,z$y))

and take sum(,na.rm=TRUE) to remove the NAs due to single ids that you want
to be zeros.

Reid Huntsinger




Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kerry Bush
Sent: Wednesday, June 15, 2005 11:41 AM
To: r-help at stat.math.ethz.ch
Subject: [R] need help on computing double summation


Dear helpers in this forum,

   This is a clarified version of my previous
questions  in this forum. I really need your generous
help on this issue.

> Suppose I have the following data set:
> 
> 
> ......
> 

Now I want to compute the following double summation:

sum_{i=1}^k
sum_{j=1}^{n_i}(x_{ij}-mean(x_i))*(y_{ij}-mean(y_i))

i is from 1 to k,
indexing the ith subject id; and j is from 1 to n_i,
indexing the jth observation for the ith subject.

in the above expression, mean(x_i) is the mean of x
values for the ith
subject, mean(y_i) is the mean of y values for the ith
subject. 

Is there a simple way to do this in R?

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From steve_adams_sd at yahoo.com  Wed Jun 15 22:53:43 2005
From: steve_adams_sd at yahoo.com (Steve Adams)
Date: Wed, 15 Jun 2005 13:53:43 -0700 (PDT)
Subject: [R] 2 LDA
Message-ID: <20050615205343.40786.qmail@web33303.mail.mud.yahoo.com>

Hi,

I am using Partek for LDA analysis. For a binary
response variable, it generates 2 discriminant
functions, one for each of the 2 levels of the
response variable. And I can simply calculate 2
discriminant scores (say d1 and d2) for each sampples
using the 2 discriminant functions, then I can use the
following formula to compute the posterior probability
for the sample:

p1=exp(d1-d2)/(1+exp(d1-d2))
p2=1/(1+exp(d1-d2))

In R, the lda function only generates 1 discriminant
function, and exactly the same posterior probability
as the LDA function in Partek does.

My question is what's the difference (or relationship)
between the above 2 LDA functions, where I can find
the reference for the partek LDA function?

Thanks

Steve



From reid_huntsinger at merck.com  Wed Jun 15 23:08:23 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 15 Jun 2005 17:08:23 -0400
Subject: [R] 2 LDA
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9498@uswpmx00.merck.com>

Note that p1 and p2 depend only on d1 - d2. Have a look at a book like
Ripley's "Pattern Recognition and Neural Networks" or Anderson's
multivariate statistics book for further information.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Steve Adams
Sent: Wednesday, June 15, 2005 4:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] 2 LDA


Hi,

I am using Partek for LDA analysis. For a binary
response variable, it generates 2 discriminant
functions, one for each of the 2 levels of the
response variable. And I can simply calculate 2
discriminant scores (say d1 and d2) for each sampples
using the 2 discriminant functions, then I can use the
following formula to compute the posterior probability
for the sample:

p1=exp(d1-d2)/(1+exp(d1-d2))
p2=1/(1+exp(d1-d2))

In R, the lda function only generates 1 discriminant
function, and exactly the same posterior probability
as the LDA function in Partek does.

My question is what's the difference (or relationship)
between the above 2 LDA functions, where I can find
the reference for the partek LDA function?

Thanks

Steve

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Wed Jun 15 23:10:27 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 15 Jun 2005 16:10:27 -0500
Subject: [R] coding to generate a matrix to prepare for chi-sqr test for
	text mining
Message-ID: <cdf8178305061514104b35b3d3@mail.gmail.com>

Hi, there:
I have a dataset like the following:

1412|WINDOW|SHATTER|TORN|SOFT|TOP|WATER|RAIN|LAB|AI|BOLL|CAMP|0
1413|PARK|IV|STRUCK|PARK|PUSH|COD|POLICI|CIA|TB|SIC|0
2412|ACCID|REAREND|MULTI|EH|IV|MIDDL|FAN|DUAL|LOSS|CALM|1
2414|IV|REAREND|CD|COG|LAB|ADVERS|1
2415|ACCID|SINGL|VEHICL|IV|SWERV|AVOID|OBJECT|STRUCK|PHONE|POLE|FAN|0
2417|ACCID|SINGL|VEHICL|ROLL|DUE|FATAL|FAN|DUAL|LOSS|CALM|1
2418|AI|FELL|ASLEEP|WHEEL|VEHICL|RETENT|POND|LAB|ADVERS|1
2419|ACCID|SINGL|VEHICL|TREE|FELL|IV|LIGHTN|STORM|IV|CAMP|CALM|AD|1
2422|THEFT|RECOV|TOTAL|THEFT|0
...

The first column is always id_num, the last one is class label. I want
to do some chi-square test on the dependency between a word (or
further a word combination) on the class label.

for example, my goal is to build a table like the following, ready for
chi-square test
                      ACCID (Yes)                 ACCID(No)
class label
         1                  10                                15
         0                    5                                 9
 
the number is the number of lines (observations).
and later I want to do word-combination like ACCID & WINDOW (this
result was generated from association analysis from my another
program) instead of ACCID only.

My first question is, how to do it automatically in R to build a data
structure (data frame) to represent the table above for each word)
since I am learning R programming and I don't want to do it using
python.  (Don't worry if a word appears twice in one observation, and
I have another version of data set which only lists unique word.)

My target is to find a p-value for each word/class label from
chi-square test and evaluate the significance of feature for later
text mining. I am not sure if this is a good idea and I am reading
some papers on this.

Thanks,

--  
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From jari.oksanen at oulu.fi  Wed Jun 15 23:27:38 2005
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 16 Jun 2005 00:27:38 +0300
Subject: [R] ellipse -confidence interval
In-Reply-To: <1118865921.42b08a014f1d5@webmail.fcnym.unlp.edu.ar>
References: <1118865921.42b08a014f1d5@webmail.fcnym.unlp.edu.ar>
Message-ID: <afc8732f50d803924c321b985d859762@oulu.fi>


On 15 Jun 2005, at 23:05, pgonzalez at fcnym.unlp.edu.ar wrote:

> Hi!, I made a plot with two variables representing the first two axis 
> of a
> principle component analysis. My question is:
> How can I superimpose on such plot an ellipse that represent the 95% 
> interval of
> confidence of scores?
>
You can't, because you don't know the confidence (or s.e.) of the 
scores. Or this is the answer if you mean that each point in the plot 
should be enclosed in its private confidence ellipse (some people may 
come with ideas to get those s.e.'s: go tell the people if you find one 
that works). On the other hand, if you want to enclose all points 
within one ellipse, you can directly use ellipse package which requires 
covariance matrix as input. If you prefer ellipsoid hulls, you can use 
functions in cluster package. Package vegan has some examples of the 
use of ellipse package (see ordiellipse there).

Some people are hypersensitive to "principle" components (beware).

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From thorstensen at gmx.net  Wed Jun 15 23:28:11 2005
From: thorstensen at gmx.net (Thorstensen Nicolas)
Date: Wed, 15 Jun 2005 23:28:11 +0200
Subject: [R] k-fold cross validation with neural networks
Message-ID: <42B09D6B.8040803@gmx.net>

Hi !

how can I do a k-fold crossvalidation with neural networks?
and how can I load matlab data files into R?

thx in advance!



From reid_huntsinger at merck.com  Thu Jun 16 00:27:45 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 15 Jun 2005 18:27:45 -0400
Subject: [R] coding to generate a matrix to prepare for chi-sqr test f
 or text mining
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A949B@uswpmx00.merck.com>

I would compile a table of all the words in the dataset (maybe you have it
already), then create a list where each component is an integer vector of
indices of words. That is, replace words by their positions in the table. 

>From that sparse form you could create binary features to use with standard
classification methods, or for example compute the X'X matrix for linear
regression directly (you would probably want to throw out infrequently
occurring words to keep the matrix small enough to work with in memory). For
your specific question, say "words" is the list of integer vectors as above,
and "class" is the vector of class labels (1 or 2 to make it a valid index)
corresponding to a given vector. Then you can fill in the "present" (==1)
parts of the table class x presence x word via


n <- length(words)
tab <- array(as.integer(0),dim=c(2,2,n))

for (i in 1:n) {
  for (word in words[[i]]) tab[class[i],1,word] <- tab[class[i],1,word] + 1
}

and the "absent" (==2) parts are then easy:

tab[1,2,] <- sum(class == 1) - tab[1,1,]
tab[2,2,] <- sum(class == 2) - tab[2,1,] 

so now you can use chisq.test on each of the 2 x 2 tables tab[,,i] for i a
word index, all at once using apply() if convenient.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
Sent: Wednesday, June 15, 2005 5:10 PM
To: R-help at stat.math.ethz.ch
Subject: [R] coding to generate a matrix to prepare for chi-sqr test for
text mining


Hi, there:
I have a dataset like the following:

1412|WINDOW|SHATTER|TORN|SOFT|TOP|WATER|RAIN|LAB|AI|BOLL|CAMP|0
1413|PARK|IV|STRUCK|PARK|PUSH|COD|POLICI|CIA|TB|SIC|0
2412|ACCID|REAREND|MULTI|EH|IV|MIDDL|FAN|DUAL|LOSS|CALM|1
2414|IV|REAREND|CD|COG|LAB|ADVERS|1
2415|ACCID|SINGL|VEHICL|IV|SWERV|AVOID|OBJECT|STRUCK|PHONE|POLE|FAN|0
2417|ACCID|SINGL|VEHICL|ROLL|DUE|FATAL|FAN|DUAL|LOSS|CALM|1
2418|AI|FELL|ASLEEP|WHEEL|VEHICL|RETENT|POND|LAB|ADVERS|1
2419|ACCID|SINGL|VEHICL|TREE|FELL|IV|LIGHTN|STORM|IV|CAMP|CALM|AD|1
2422|THEFT|RECOV|TOTAL|THEFT|0
...

The first column is always id_num, the last one is class label. I want
to do some chi-square test on the dependency between a word (or
further a word combination) on the class label.

for example, my goal is to build a table like the following, ready for
chi-square test
                      ACCID (Yes)                 ACCID(No)
class label
         1                  10                                15
         0                    5                                 9
 
the number is the number of lines (observations).
and later I want to do word-combination like ACCID & WINDOW (this
result was generated from association analysis from my another
program) instead of ACCID only.

My first question is, how to do it automatically in R to build a data
structure (data frame) to represent the table above for each word)
since I am learning R programming and I don't want to do it using
python.  (Don't worry if a word appears twice in one observation, and
I have another version of data set which only lists unique word.)

My target is to find a p-value for each word/class label from
chi-square test and evaluate the significance of feature for later
text mining. I am not sure if this is a good idea and I am reading
some papers on this.

Thanks,

--  
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Thu Jun 16 01:05:11 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Wed, 15 Jun 2005 19:05:11 -0400
Subject: [R] coding to generate a matrix to prepare for chi-sqr test f
 or text mining
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A949C@uswpmx00.merck.com>

The data structure I described is a sparse (binary) matrix, and I should
have said you could use the SparseM or Matrix package's sparse matrix
classes and methods to do what I suggested below. There are linear algebra
routines available for them but for other things I'm not sure. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Huntsinger, Reid
Sent: Wednesday, June 15, 2005 6:28 PM
To: 'Weiwei Shi'; R-help at stat.math.ethz.ch
Subject: Re: [R] coding to generate a matrix to prepare for chi-sqr test f
or text mining


I would compile a table of all the words in the dataset (maybe you have it
already), then create a list where each component is an integer vector of
indices of words. That is, replace words by their positions in the table. 

>From that sparse form you could create binary features to use with standard
classification methods, or for example compute the X'X matrix for linear
regression directly (you would probably want to throw out infrequently
occurring words to keep the matrix small enough to work with in memory). For
your specific question, say "words" is the list of integer vectors as above,
and "class" is the vector of class labels (1 or 2 to make it a valid index)
corresponding to a given vector. Then you can fill in the "present" (==1)
parts of the table class x presence x word via


n <- length(words)
tab <- array(as.integer(0),dim=c(2,2,n))

for (i in 1:n) {
  for (word in words[[i]]) tab[class[i],1,word] <- tab[class[i],1,word] + 1
}

and the "absent" (==2) parts are then easy:

tab[1,2,] <- sum(class == 1) - tab[1,1,]
tab[2,2,] <- sum(class == 2) - tab[2,1,] 

so now you can use chisq.test on each of the 2 x 2 tables tab[,,i] for i a
word index, all at once using apply() if convenient.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
Sent: Wednesday, June 15, 2005 5:10 PM
To: R-help at stat.math.ethz.ch
Subject: [R] coding to generate a matrix to prepare for chi-sqr test for
text mining


Hi, there:
I have a dataset like the following:

1412|WINDOW|SHATTER|TORN|SOFT|TOP|WATER|RAIN|LAB|AI|BOLL|CAMP|0
1413|PARK|IV|STRUCK|PARK|PUSH|COD|POLICI|CIA|TB|SIC|0
2412|ACCID|REAREND|MULTI|EH|IV|MIDDL|FAN|DUAL|LOSS|CALM|1
2414|IV|REAREND|CD|COG|LAB|ADVERS|1
2415|ACCID|SINGL|VEHICL|IV|SWERV|AVOID|OBJECT|STRUCK|PHONE|POLE|FAN|0
2417|ACCID|SINGL|VEHICL|ROLL|DUE|FATAL|FAN|DUAL|LOSS|CALM|1
2418|AI|FELL|ASLEEP|WHEEL|VEHICL|RETENT|POND|LAB|ADVERS|1
2419|ACCID|SINGL|VEHICL|TREE|FELL|IV|LIGHTN|STORM|IV|CAMP|CALM|AD|1
2422|THEFT|RECOV|TOTAL|THEFT|0
...

The first column is always id_num, the last one is class label. I want
to do some chi-square test on the dependency between a word (or
further a word combination) on the class label.

for example, my goal is to build a table like the following, ready for
chi-square test
                      ACCID (Yes)                 ACCID(No)
class label
         1                  10                                15
         0                    5                                 9
 
the number is the number of lines (observations).
and later I want to do word-combination like ACCID & WINDOW (this
result was generated from association analysis from my another
program) instead of ACCID only.

My first question is, how to do it automatically in R to build a data
structure (data frame) to represent the table above for each word)
since I am learning R programming and I don't want to do it using
python.  (Don't worry if a word appears twice in one observation, and
I have another version of data set which only lists unique word.)

My target is to find a p-value for each word/class label from
chi-square test and evaluate the significance of feature for later
text mining. I am not sure if this is a good idea and I am reading
some papers on this.

Thanks,

--  
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From ggrothendieck at gmail.com  Thu Jun 16 01:31:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:31:47 -0400
Subject: [R] abbreviate
In-Reply-To: <1118860928.4423.7.camel@localhost.localdomain>
References: <3f87cc6d05061510591bb2a807@mail.gmail.com>
	<Pine.LNX.4.61.0506151929200.12352@gannet.stats>
	<1118860928.4423.7.camel@localhost.localdomain>
Message-ID: <971536df0506151631724ea494@mail.gmail.com>

On 6/15/05, Marc Schwartz <MSchwartz at mn.rr.com> wrote:
> On Wed, 2005-06-15 at 19:33 +0100, Prof Brian Ripley wrote:
> > On Wed, 15 Jun 2005, Omar Lakkis wrote:
> >
> > >> p = data.frame(high=c(5,2), settle=c(3,4))
> > >> p
> > >  high settle
> > > 1    5      3
> > > 2    2      4
> > >
> > > What is the most abbreviated way to apply:
> > > if (p$high < p$settle) p$high = p$settle
> > >
> > > I want to modify p to become:
> > >> p
> > >  high settle
> > > 1    5      3
> > > 2    4      4
> >
> > p[[1]] <- pmax(p[[1]], p[[2]])
> >
> > seems to need a rather small number of keystrokes at the expense of
> > readability (I would otherwise use p$high etc).
> 
> I do like that approach. Definitely less keystrokes...
> 
> :-)
> 

Agree that this definitely should be pursued. :)  In fact,
we can shave off several keystrokes by 

- replacing p[[1]] with p[1] on the left hand side
- p$high and p$settle with p$h and p$s on the right hand
  side (which makes use of the matching property of $)
- '=' instead of '<-'
- remove all the spaces

This gets it down to 18 characters:

p[1]=pmax(p$h,p$s)



From ggrothendieck at gmail.com  Thu Jun 16 01:33:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:33:05 -0400
Subject: [R] Kalman Filtering?
In-Reply-To: <42B05A64.3010400@pdf.com>
References: <42B04B13.2060401@pdf.com>
	<Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>
	<42B05A64.3010400@pdf.com>
Message-ID: <971536df05061516331698fbf8@mail.gmail.com>

A simple workaround would be to call KalmanLike this way:

  KalmanLike(y, mod[], nit)

as [] makes a copy.

On 6/15/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> Hi, Thomas:
> 
>          Thanks much.  I modified the R code for "KalmanLike" as you
> suggested, and the problem went away.
> 
>          Next question:  Should KalmanLike work the way it does without your
> suggested modification?  If not, how do we get your proposal transmitted
> to the appropriate person so it is appropriately considered for adoption
> in a future release of R?
> 
>          Best Wishes,
>          Spencer Graves
> 
> Thomas Lumley wrote:
> 
> > On Wed, 15 Jun 2005, Spencer Graves wrote:
> >
> >
> >>        1.  The function "KalmanLike" seems to change its inputs AND
> >>PREVIOUSLY MADE copies of the inputs.
> >
> >
> > The strange thing is that it changes its inputs. It would be expected that
> > this also changed previously made copies. The copies will just be
> > references to the same actual object in memory (which is why changing the
> > inputs is undesirable).
> >
> > Looking at the C code, it does change the a, P, and Pnew components of the
> > model.  I'm a little reluctant to mess with this code, but an R-level
> > work-around should be to replace mod$P with mod$P+0 and so on in the
> > call to .Call, as this will force copying.
> >
> >       -thomas
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jun 16 01:34:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:34:59 -0400
Subject: [R] how to change automatically 0=no and 1=yes
In-Reply-To: <42B0522A.8090509@gmail.com>
References: <42B033F9.7080001@gmail.com> <42B0522A.8090509@gmail.com>
Message-ID: <971536df050615163414f0505e@mail.gmail.com>

One thing to be careful of is that we likely intend to
represent the columns of the data frame as factors in which
case we still want the levels to be No and Yes even if a
column only consists of one level (i.e. all No or all Yes).
Note that one solution specifically provided for that.

On 6/15/05, Muhammad Subianto <subianto at gmail.com> wrote:
> Dear all,
> Sean Davis, Dimitris Rizopoulos and Marc Schwartz, thanks for your great
> help. It works perfectly. Thanks a lot.
> All the best,
> Muhammad Subianto
> 
> On this day 6/15/2005 4:06 PM, Sean Davis wrote:
>  >  > x <- data.frame(matrix(c(1,0,1,0,1,1),nrow=3))
>  >  > x[x==0] <- 'no'
>  >  > x[x==1] <- 'yes'
>  >  > x
>  >     X1  X2
>  > 1 yes  no
>  > 2  no yes
>  > 3 yes yes
>  >
> 
> On this day 6/15/2005 4:06 PM, Dimitris Rizopoulos wrote:
>  > try this:
>  >
>  > dat <- data.frame(matrix(sample(0:1, 100 * 20, TRUE), 100, 20))
>  > ############
>  > dat[] <- lapply(dat, factor, levels = c(0, 1), labels = c("no",
>  > "yes"))
>  > dat
>  >
> 
> On this day 6/15/2005 4:16 PM, Marc Schwartz wrote:
>  >
>  >
>  >>as.data.frame(ifelse(df == 0, "No", "Yes"))
>  >
> 
> 
> On this day 6/15/2005 3:58 PM, Muhammad Subianto wrote:
> > Dear R-helpers,
> > I have dataset (data.frame) like below,
> >    x1  x2  x3   x4   x5  x6  x7  x8  x9 ... x1200
> >     0    0    0    1    1    0    0    1    1
> >     1    0    0    1    1    0    0    1    1
> >     0    1    0    1    1    0    0    1    1
> >     1    1    0    1    1    0    0    1    1
> > ...
> > How can I change automatically 0=no and 1=yes.
> >
> > Thank you very much in advance.
> > Kindly regards,
> > Muhammad Subianto
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jun 16 01:35:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:35:51 -0400
Subject: [R] Finding local minima and maxima
In-Reply-To: <c98ba74449fd8a76a87c277d7fba7fce@mail.nih.gov>
References: <673535f620ce6c5a519164325a6e47f5@mail.nih.gov>
	<42B01C03.1060202@statistik.uni-dortmund.de>
	<c98ba74449fd8a76a87c277d7fba7fce@mail.nih.gov>
Message-ID: <971536df0506151635593e586@mail.gmail.com>

1. See ?turnpoints in package pastecs.

2. do it yourself with rapply in the zoo package:

   library(zoo)
   f <- function(x) x[2] > max(x[-2])
   rapply(zoo(rank(x, ties = "first")), 3, function(x) f(x) - f(-x))

   Note that if a run is an extrema then the last member of a
   run of maxima and the first member of a run of minima will
   be identified as the extremum.

3. There are also a number of other solutions on r-help you
   could find if you look through the result of the R command:
 
   RSiteSearch("peak")

On 6/15/05, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> Thanks, Uwe.
> 
> Actually, I didn't mean to make this complicated--simple vector x and
> values at x in a vector y.  Just wanted to determine the local minima
> and maxima of y[x], but y is not continuous (but is a "smooth" function
> of x).  I will probably use hist or density to do it and then look at
> these curves or cuts.
> 
> Sean
> 
> On Jun 15, 2005, at 8:16 AM, Uwe Ligges wrote:
> 
> > Sean Davis wrote:
> >
> >> I have data in the form of (x,y) pairs and would like to find local
> >> minima and maxima (typically the zeros of the 2nd derivative) of the
> >> y values.  I looked at numericDeriv, but I don't have an "expression"
> >> per se.  I looked at optim, also, but it looks like it will find only
> >> one "global" max or min.  I can code up my own piecewise derivatives,
> >> but wondered if there is an existing function to do this.
> >
> > If you don't have derivatives, and you don't have an expression, and
> > you don't have a criteria how to minimize, please tell me of any known
> > method that can handle your problem well enough.
> >
> > You could look whether y[i-1] <y[i] and y[i+1]<y[i] if y is ordered
> > according to x, if this most trivial and very non-robust method fits
> > to your definition...
> >
> > Uwe Ligges
> >
> >
> >> Thanks,
> >> Sean
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Thu Jun 16 01:38:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:38:00 -0400
Subject: [R] [OT] Re: umlauts in Rd files
In-Reply-To: <loom.20050615T201446-461@post.gmane.org>
References: <2bffa8e3c147fb34b23a960b1ce0e35d@soc.soton.ac.uk>
	<x2ll5b94pk.fsf@turmalin.kubism.ku.dk>
	<Pine.LNX.4.61.0506151420080.4628@gannet.stats>
	<42B0582B.6080800@pdf.com> <loom.20050615T201446-461@post.gmane.org>
Message-ID: <971536df0506151638185d0e53@mail.gmail.com>

On 6/15/05, Dirk Eddelbuettel <edd at debian.org> wrote:
> Spencer Graves <spencer.graves <at> pdf.com> writes:
> > (This may not be relevant, but Google just gave me 788,000 hits for
> > "moebius" and 750,000 for "mobius".)  Perhaps one of our German
> > contributors could tell us which they prefer.
> 
> As you asked for it:  The former is correct, and the second is a misspelling. So
> there is no choice but simply "right" and "wrong".
> 
> The only alternative to the full '8-bit' spelling with an Umlaut is the
> 'transcribed 7-bit' form of replacing the 'two dots over the vowel' with an
> 'appended e after the vowel'.  As in Maechler, or in the From: field above.
> 
> The problem is that most of the world simply drops the dots: E.g. the comparison
>  of 'Wolfgang Hardle' and 'Wolfgang Haerdle' at scholar.google.com shows that the
> correct form is (barely) leading with a narrow 227 to 220.   My PhD dissertation

www.googlefight.com gives roughly 500 to 5000 in favor of "Wolfgang Haerdle".



From ggrothendieck at gmail.com  Thu Jun 16 01:39:50 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:39:50 -0400
Subject: [R] suppress output of neural network for use in Sweave
In-Reply-To: <42B0560D.3010703@lancaster.ac.uk>
References: <42B04EE2.2070207@gmx.net> <42B0560D.3010703@lancaster.ac.uk>
Message-ID: <971536df050615163978568867@mail.gmail.com>

On 6/15/05, Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
> Thorstensen Nicolas wrote:
> > Hi!
> >
> > How can I suppress the output of the function nnet in the library(nnet) ?
> 
>  ?nnet suggests adding trace=FALSE to the arguments.
> 
>  In other cases if functions dont have this sort of option then on a
> Unix box you can wrap your function calls in sink("/dev/null") [do
> stuff] sink() to send output to a black hole. Not sure what the Windows
> equivalent is.


A general way OS independent way to turn off output is via 
capture.output:

	invisible(capture.output(...whatever...))



From trainor at transborder.org  Thu Jun 16 01:45:34 2005
From: trainor at transborder.org (Douglas J. Trainor)
Date: Wed, 15 Jun 2005 19:45:34 -0400
Subject: [R] ellipse -confidence interval
In-Reply-To: <afc8732f50d803924c321b985d859762@oulu.fi>
References: <1118865921.42b08a014f1d5@webmail.fcnym.unlp.edu.ar>
	<afc8732f50d803924c321b985d859762@oulu.fi>
Message-ID: <e9b0c5534540b5c2354712d4f0dd1673@transborder.org>

I think Lic. Paula Gonzalez is after one ellipse representing
Hotelling's T^2 with the major axis of the ellipse oriented
along PC1 and minor axis oriented along PC2.

     douglas

On Jun 15, 2005, at 5:27 PM, Jari Oksanen wrote:

>
> On 15 Jun 2005, at 23:05, pgonzalez at fcnym.unlp.edu.ar wrote:
>
>> Hi!, I made a plot with two variables representing the first two axis
>> of a
>> principle component analysis. My question is:
>> How can I superimpose on such plot an ellipse that represent the 95%
>> interval of
>> confidence of scores?
>>
> You can't, because you don't know the confidence (or s.e.) of the
> scores. Or this is the answer if you mean that each point in the plot
> should be enclosed in its private confidence ellipse (some people may
> come with ideas to get those s.e.'s: go tell the people if you find one
> that works). On the other hand, if you want to enclose all points
> within one ellipse, you can directly use ellipse package which requires
> covariance matrix as input. If you prefer ellipsoid hulls, you can use
> functions in cluster package. Package vegan has some examples of the
> use of ellipse package (see ordiellipse there).
>
> Some people are hypersensitive to "principle" components (beware).



From ggrothendieck at gmail.com  Thu Jun 16 01:34:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 19:34:10 -0400
Subject: [R] a question about frame
In-Reply-To: <0II400HBAV1H3M@mail.fudan.edu.cn>
References: <0II400HBAV1H3M@mail.fudan.edu.cn>
Message-ID: <971536df0506151634409df3bb@mail.gmail.com>

On 6/15/05, ronggui <0034058 at fudan.edu.cn> wrote:
> gg <- function(y) {
>         ggg <- function() {
>             cat("current frame is", sys.nframe(), "\n")
>             cat("parents are", sys.parents(), "\n")
>             print(sys.function(0)) # ggg
>             print(sys.function(2)) # gg
>         }
>         if(y > 0) gg(y-1) else ggg()
>     }
> 
> 
> >  gg(3)
> current frame is 5
> parents are 0 1 2 3 4
> function() {
>             cat("current frame is", sys.nframe(), "\n")
>             cat("parents are", sys.parents(), "\n")
>             print(sys.function(0)) # ggg
>             print(sys.function(2)) # gg
>         }
> <environment: 071C62FC>
> function(y) {
>         ggg <- function() {
>             cat("current frame is", sys.nframe(), "\n")
>             cat("parents are", sys.parents(), "\n")
>             print(sys.function(0)) # ggg
>             print(sys.function(2)) # gg
>         }
>         if(y > 0) gg(y-1) else ggg()
>     }
> 
> 
> my question is ,why the current frame is 5?why the sys.function is ggg.i am quite confused.
> anyone can give me some clue?i have read the ?sys.parent,but i still can not get the point exactly.


We start out at frame 0 and then:
1. we call gg(3) so now we are at frame 1
2. which calls gg(2) so now we are at frame 2
3. which calls gg(1) so now we are at frame 3
4. which calls gg(0) so now we are at frame 4
5. which calls ggg() so now we are at frame 5

sys.function(0) seems to be referring to the current frame
(not frame 0 as the which= documentation in ?sys.function
seems to imply).  Since sys.function(0) is called within ggg
we get the result. Also sys.function(2) refers to frame 2 
and referring to the numbered list above we see it is gg.



From ggrothendieck at gmail.com  Thu Jun 16 02:15:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 15 Jun 2005 20:15:55 -0400
Subject: [R] Multiple line plots
In-Reply-To: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
References: <BAY103-F2F9E8A702448F81E91D53F8F20@phx.gbl>
Message-ID: <971536df05061517153e3e860e@mail.gmail.com>

On 6/15/05, Mihai Nica <m_nica at hotmail.com> wrote:
> Greetings,
> 
> I would like to plot three lines on the same figure, and I am lost. There is
> an answer to a similar thread? but I tried matplot and it is beyond me. An
> example of the data follows:
> 
> Year    EM      IM      BM
> 1983    9.1     16.8    -7.7
> 1984    12.0    18.0    -6.0
> 1985    13.6    19.1    -5.5
> 1986    12.4    17.3    -4.9
> 1987    14.6    20.3    -5.7
> 1988    20.6    23.3    -2.6
> 1989    25.0    27.2    -2.2
> 1990    28.4    30.2    -1.8
> 1991    33.3    31.2    2.1
> 1992    40.6    35.2    5.4
> 1993    41.6    39.9    1.7
> 1994    50.8    49.5    1.3
> 1995    46.3    61.7    -15.4
> 1996    56.8    73.0    -16.2
> 1997    71.4    85.9    -14.5
> 1998    79.0    94.7    -15.7
> 1999    87.0    109.7   -22.7
> 2000    111.7   135.9   -24.2
> 2001    101.5   131.4   -29.9
> 2002    97.5    134.6   -37.1
> 2003    97.4    138.1   -40.6
> 2004    110.8   155.8   -45.1
> 

Suppose the data frame is dd.  Then:

matplot(dd[,1], dd[,-1], type = "l")

Another possibility is to notice that this is a regularly
spaced time series and it could be useful to represent
it as a ts class object for here and for other purposes:

ddts <- ts(as.matrix(dd[,-1]), start = dd[1,1])
plot(ddts, plot.type = "single", col = 1:3)



From spencer.graves at pdf.com  Thu Jun 16 02:32:49 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Jun 2005 17:32:49 -0700
Subject: [R] Kalman Filtering?
In-Reply-To: <971536df05061516331698fbf8@mail.gmail.com>
References: <42B04B13.2060401@pdf.com>	
	<Pine.A41.4.61b.0506150908490.222798@homer07.u.washington.edu>	
	<42B05A64.3010400@pdf.com>
	<971536df05061516331698fbf8@mail.gmail.com>
Message-ID: <42B0C8B1.3030004@pdf.com>

Hi, Gabor:  Great.  Thanks.  spencer graves

Gabor Grothendieck wrote:
> A simple workaround would be to call KalmanLike this way:
> 
>   KalmanLike(y, mod[], nit)
> 
> as [] makes a copy.
> 
> On 6/15/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>Hi, Thomas:
>>
>>         Thanks much.  I modified the R code for "KalmanLike" as you
>>suggested, and the problem went away.
>>
>>         Next question:  Should KalmanLike work the way it does without your
>>suggested modification?  If not, how do we get your proposal transmitted
>>to the appropriate person so it is appropriately considered for adoption
>>in a future release of R?
>>
>>         Best Wishes,
>>         Spencer Graves
>>
>>Thomas Lumley wrote:
>>
>>
>>>On Wed, 15 Jun 2005, Spencer Graves wrote:
>>>
>>>
>>>
>>>>       1.  The function "KalmanLike" seems to change its inputs AND
>>>>PREVIOUSLY MADE copies of the inputs.
>>>
>>>
>>>The strange thing is that it changes its inputs. It would be expected that
>>>this also changed previously made copies. The copies will just be
>>>references to the same actual object in memory (which is why changing the
>>>inputs is undesirable).
>>>
>>>Looking at the C code, it does change the a, P, and Pnew components of the
>>>model.  I'm a little reluctant to mess with this code, but an R-level
>>>work-around should be to replace mod$P with mod$P+0 and so on in the
>>>call to .Call, as this will force copying.
>>>
>>>      -thomas
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>>



From manojsw at gmail.com  Thu Jun 16 02:45:48 2005
From: manojsw at gmail.com (ManojW)
Date: Thu, 16 Jun 2005 09:45:48 +0900
Subject: [R] Vectorization
Message-ID: <002101c5720c$c07a4620$6900a8c0@HCJP.COM>

Greetings,
    Can anyone suggest me if we can vectorize the following problem
effectively?

    I have two datasets, one dataset is portfolio of stocks returns on a
historical basis and another dataset consist of a bunch of factors (again on
a historical basis). I intend to compute a rolling n-day sensitivitiesfor
each stock for each factor, so the output will be a data frame with
<ticker><dt><sensitivities>.

    How would you go onto vector this situation effectively?

    I end up with a psuedo code like this:

            # For each date
            For curr dt in all dates
                # Get Universe of stocks as of that date
                Get Universe for curr date
                # Calculate Sensitivity for each factor between n days back
dt to curr date
                sensitivity    =
sapply(univ{ticker},CalcSensitivity,n_days_back_dt,dt)
            Next date

    I would highly appreciate if the above logic could be improved (if at
all) by a more effective solution since I do get into such situations on a
regular basis.

    Thanks in advance

Cheers

Manoj



From chencheva at gmail.com  Thu Jun 16 03:12:02 2005
From: chencheva at gmail.com (Hu Chen)
Date: Thu, 16 Jun 2005 09:12:02 +0800
Subject: [R] how to plot density distribution with a arrow pointer?
In-Reply-To: <42b04bb0.5223fecc.7551.2c8bSMTPIN_ADDED@mx.gmail.com>
References: <6f3fc9ee050615074037119902@mail.gmail.com>
	<42b04bb0.5223fecc.7551.2c8bSMTPIN_ADDED@mx.gmail.com>
Message-ID: <6f3fc9ee05061518126cae824c@mail.gmail.com>

Thanks.
A small arrow may be more nice-looking than a long cutting line.
and, another question,
how to get the p-value of the position of X0?
If the whole distribution is not as regular as norm, chi-square but a
strange one, how could I get the X0's p-value?

any functions?

Thanks very much.

On 6/15/05, Jim Brennan <jfbrennan at rogers.com> wrote:
> This is one way not so good
> R>X<-rnorm(1000000)
> R>X0<-.899
> R>plot(density(X))
> R>abline(v=X0)
> 
> A better way
> 
> R>x<-seq(-5,5,.01)
> R>plot(x,dnorm(x))
> R>plot(x,dnorm(x),type="l",col=2)
> R>abline(v=X0,col=4)
> R>?text
> R>text(X0,.2,paste("P(X<X0) =",signif(pnorm(X0),3)),pos=2)
> R>abline(v=X0,col=4)
> 
> If you actually want arrows do ?arrows
> Jim
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hu Chen
> Sent: June 15, 2005 10:41 AM
> To: R
> Subject: [R] how to plot density distribution with a arrow pointer?
> 
> Hi all,
> for example:
> > X<- rnorm(1000)
> > X0 <- 0.899
> 
> I want to draw a density distribution plot with a arrow pointer
> indicating the position of X0, meanwhile, giving out the p-value.
> 
> any functions?
> 
> Thanks very much.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From r.shengzhe at gmail.com  Thu Jun 16 04:09:12 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Thu, 16 Jun 2005 04:09:12 +0200
Subject: [R] how to use plot.lda included in MASS package?
Message-ID: <ea57975b05061519092db5b9f9@mail.gmail.com>

Hi there,

I am a master student in Denmark, and apply R to analyze the drug data.

I use the function 'lda' to obtain a linear discriminant object, and
then wish to use 'plot.lda' to do the plot on this object like below.

drug.lda <- lda(Inhibition ~ NET_CHARGE + PKA_1 + MW + MLOGP, 
                      data = drug.class, method = "moment")

plot.lda(drug.lda, panel = panel.lda, cex = 0.7, dimen = 1, abbrev = FALSE, 
            type ="density")

but I got an error message: Error: couldn't find function "plot.lda"
and then I tried 

plot(drug.lda, panel = panel.lda, cex = 0.7, dimen = 1, abbrev = FALSE, 
            type ="density")

still an error message was returned: Error in plot.default(drug.lda,
panel = panel.lda, cex = 0.7,  : argument 2 matches multiple formal
arguments

Also I can't find this panel function, panel.lda anywhere. Do you know
how to use this plot function?

Thanks a lot!
wsz



From jyzz88 at gmail.com  Thu Jun 16 05:18:08 2005
From: jyzz88 at gmail.com (Luke)
Date: Wed, 15 Jun 2005 23:18:08 -0400
Subject: [R] AIC in glm.fit with intercept
Message-ID: <27583b4005061520187897c28d@mail.gmail.com>

Dear R users,

glm.fit() gave me the same AIC's regardless of TRUE or FALSE intercept option.

> myX <- as.matrix(1:10)
> myY <- 3+5*myX
> foo <- glm.fit(x=myX, y=myY, family = gaussian(link = "identity"), intercept=TRUE)
> foo$aic
[1] 38.94657
> foo <- glm.fit(x=myX, y=myY, family = gaussian(link = "identity"), intercept=FALSE)
> foo$aic
[1] 38.94657

Is my code wrong or is it a bug in glm.fit?

-Luke



From jyzz88 at gmail.com  Thu Jun 16 05:35:53 2005
From: jyzz88 at gmail.com (Luke)
Date: Wed, 15 Jun 2005 23:35:53 -0400
Subject: [R] AIC in glm.fit with intercept
In-Reply-To: <27583b4005061520187897c28d@mail.gmail.com>
References: <27583b4005061520187897c28d@mail.gmail.com>
Message-ID: <27583b400506152035aacec23@mail.gmail.com>

More interesting:
lm() works fine to get AIC. I wonder if there is a bug in glm.fit().

>  AIC(lm(myY~0+myX, data=data.frame(myY,myX)))
[1] 38.94657
> AIC(lm(myY~1+myX, data=data.frame(myY,myX)))
[1] -650.9808

-Luke

On 6/15/05, Luke <jyzz88 at gmail.com> wrote:
> Dear R users,
> 
> glm.fit() gave me the same AIC's regardless of TRUE or FALSE intercept option.
> 
> > myX <- as.matrix(1:10)
> > myY <- 3+5*myX
> > foo <- glm.fit(x=myX, y=myY, family = gaussian(link = "identity"), intercept=TRUE)
> > foo$aic
> [1] 38.94657
> > foo <- glm.fit(x=myX, y=myY, family = gaussian(link = "identity"), intercept=FALSE)
> > foo$aic
> [1] 38.94657
> 
> Is my code wrong or is it a bug in glm.fit?
> 
> -Luke
>



From ripley at stats.ox.ac.uk  Thu Jun 16 08:31:51 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 16 Jun 2005 07:31:51 +0100 (BST)
Subject: [R] how to use plot.lda included in MASS package?
In-Reply-To: <ea57975b05061519092db5b9f9@mail.gmail.com>
References: <ea57975b05061519092db5b9f9@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506160720380.21725@gannet.stats>

The main answer is to follow the examples in the book for which this is 
support software.  (That answer is in the R posting guide, too.)

plot.lda is in the MASS namespace, and can be seen via getAnywhere() (so 
you can find them `anywhere').  panel.lda is defined in plot.lda.

You should almost never call methods like plot.lda() directly.  If you ask 
for help on it, you get

      ## S3 method for class 'lda':
      plot(x, panel = panel.lda, ..., cex = 0.7, dimen,
           abbrev = FALSE, xlab = "LD1", ylab = "LD2")

Note that you are supplying default arguments.  Just call

plot(drug.lda, dimen = 1, type ="density")

In particular, you have given a panel function for a 1D plot which does 
not use one.

I am unable to reproduce anything like the message you got, e.g.

example(z)
plot(z, panel = panel.lda, cex = 0.7, dimen = 1, abbrev = FALSE,
      type ="density")

works.  Perhaps your R or MASS package is way out of date?  (You haven't 
told us: see the R posting guide.)

On Thu, 16 Jun 2005, wu sz wrote:

> Hi there,
>
> I am a master student in Denmark, and apply R to analyze the drug data.
>
> I use the function 'lda' to obtain a linear discriminant object, and
> then wish to use 'plot.lda' to do the plot on this object like below.
>
> drug.lda <- lda(Inhibition ~ NET_CHARGE + PKA_1 + MW + MLOGP,
>                      data = drug.class, method = "moment")
>
> plot.lda(drug.lda, panel = panel.lda, cex = 0.7, dimen = 1, abbrev = FALSE,
>            type ="density")
>
> but I got an error message: Error: couldn't find function "plot.lda"
> and then I tried
>
> plot(drug.lda, panel = panel.lda, cex = 0.7, dimen = 1, abbrev = FALSE,
>            type ="density")
>
> still an error message was returned: Error in plot.default(drug.lda,
> panel = panel.lda, cex = 0.7,  : argument 2 matches multiple formal
> arguments
>
> Also I can't find this panel function, panel.lda anywhere. Do you know
> how to use this plot function?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Thu Jun 16 08:55:59 2005
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 16 Jun 2005 07:55:59 +0100
Subject: [R] how to plot density distribution with a arrow pointer?
In-Reply-To: <6f3fc9ee05061518126cae824c@mail.gmail.com>
References: <6f3fc9ee050615074037119902@mail.gmail.com>
	<42b04bb0.5223fecc.7551.2c8bSMTPIN_ADDED@mx.gmail.com>
	<6f3fc9ee05061518126cae824c@mail.gmail.com>
Message-ID: <1118904959.7011.23.camel@dhcp-63.ccc.ox.ac.uk>

I am assuming that you want to do this empirically :

 x0 <- 0.899 
 x  <- c( rnorm(6000), rnorm(4000, mean=3) )
 plot( d <- density(x) )

 y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0

 segments( x0, 0, x0, y0, col=2 )

If you want a shaded effect, you can try the following :

 xx <- seq( x0, max(x), by=0.1 )
 yy <- approx( d$x, d$y, xout=xx )$y

 plot(d)
 polygon( c( xx, rev(xx) ), c( yy, rep(0, length(yy)) ), col=8 )


On Thu, 2005-06-16 at 09:12 +0800, Hu Chen wrote:
> Thanks.
> A small arrow may be more nice-looking than a long cutting line.
> and, another question,
> how to get the p-value of the position of X0?
> If the whole distribution is not as regular as norm, chi-square but a
> strange one, how could I get the X0's p-value?
> 
> any functions?
> 
> Thanks very much.
> 
> On 6/15/05, Jim Brennan <jfbrennan at rogers.com> wrote:
> > This is one way not so good
> > R>X<-rnorm(1000000)
> > R>X0<-.899
> > R>plot(density(X))
> > R>abline(v=X0)
> > 
> > A better way
> > 
> > R>x<-seq(-5,5,.01)
> > R>plot(x,dnorm(x))
> > R>plot(x,dnorm(x),type="l",col=2)
> > R>abline(v=X0,col=4)
> > R>?text
> > R>text(X0,.2,paste("P(X<X0) =",signif(pnorm(X0),3)),pos=2)
> > R>abline(v=X0,col=4)
> > 
> > If you actually want arrows do ?arrows
> > Jim
> > 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Hu Chen
> > Sent: June 15, 2005 10:41 AM
> > To: R
> > Subject: [R] how to plot density distribution with a arrow pointer?
> > 
> > Hi all,
> > for example:
> > > X<- rnorm(1000)
> > > X0 <- 0.899
> > 
> > I want to draw a density distribution plot with a arrow pointer
> > indicating the position of X0, meanwhile, giving out the p-value.
> > 
> > any functions?
> > 
> > Thanks very much.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From henric.nilsson at statisticon.se  Thu Jun 16 10:27:53 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 16 Jun 2005 10:27:53 +0200
Subject: [R] mu^2(1-mu)^2 variance function for GLM
Message-ID: <42B13809.5090905@statisticon.se>

Dear list,

I'm trying to mimic the analysis of Wedderburn (1974) as cited by
McCullagh and Nelder (1989) on p.328-332. This is the leaf-blotch on 
barley example, and the data is available in the `faraway' package.

Wedderburn suggested using the variance function mu^2(1-mu)^2. This
variance function isn't readily available in R's `quasi' family object, 
but it seems to me that the following definition could be used:

}, "mu^2(1-mu)^2" = {
     variance <- function(mu) mu^2 * (1 - mu)^2
     validmu <- function(mu) all(mu > 0) && all(mu < 1)
     dev.resids <- function(y, mu, wt) 2 * wt * ((2 * y - 1) *
         (log(ifelse(y == 0, 1, y/mu)) - log(ifelse(y == 1, 1,
         (1 - y)/(1 - mu)))) - 2 + y/mu + (1 - y)/(1 - mu))

I've modified the `quasi' function accordingly (into `quasi2' given 
below) and my results are very much in line with the ones cited by 
McCullagh and Nelder on p.330-331:

> data(leafblotch, package = "faraway")
> summary(fit <- glm(blotch ~ site + variety,
+         family = quasi2(link = "logit", variance = "mu^2(1-mu)^2"),
+         data = leafblotch))

Call:
glm(formula = blotch ~ site + variety, family = quasi2(link = "logit",
     variance = "mu^2(1-mu)^2"), data = leafblotch)

Deviance Residuals:
      Min        1Q    Median        3Q       Max
-3.23175  -0.65385  -0.09426   0.46946   1.97152

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) -7.92253    0.44463 -17.818  < 2e-16 ***
site2        1.38308    0.44463   3.111  0.00268 **
site3        3.86013    0.44463   8.682 8.18e-13 ***
site4        3.55697    0.44463   8.000 1.53e-11 ***
site5        4.10841    0.44463   9.240 7.48e-14 ***
site6        4.30541    0.44463   9.683 1.13e-14 ***
site7        4.91811    0.44463  11.061  < 2e-16 ***
site8        5.69492    0.44463  12.808  < 2e-16 ***
site9        7.06762    0.44463  15.896  < 2e-16 ***
variety2    -0.46728    0.46868  -0.997  0.32210
variety3     0.07877    0.46868   0.168  0.86699
variety4     0.95418    0.46868   2.036  0.04544 *
variety5     1.35276    0.46868   2.886  0.00514 **
variety6     1.32859    0.46868   2.835  0.00595 **
variety7     2.34066    0.46868   4.994 3.99e-06 ***
variety8     3.26268    0.46868   6.961 1.30e-09 ***
variety9     3.13556    0.46868   6.690 4.10e-09 ***
variety10    3.88736    0.46868   8.294 4.33e-12 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasi family taken to be 0.9884755)

     Null deviance: 339.488  on 89  degrees of freedom
Residual deviance:  71.961  on 72  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 18


Also, the plot of the Pearson residuals against the linear predictor

> plot(residuals(fit, type = "pearson") ~ fit$linear.predictors)
> abline(h = 0, lty = 2)

results in a plot that, to my eyes at least, is very close to Fig. 9.2
on p. 332.

However, I can't seem to find any other published examples using this
variance function. I'd really like to verify that my code above is
working before applying it to real data sets. Can anybody help?

Thanks,
Henric
- - - - -
quasi2 <- function (link = "identity", variance = "constant")
{
     linktemp <- substitute(link)
     if (is.expression(linktemp) || is.call(linktemp))
         linktemp <- link
     else if (!is.character(linktemp))
         linktemp <- deparse(linktemp)
     if (is.character(linktemp))
         stats <- make.link(linktemp)
     else stats <- linktemp
     variancetemp <- substitute(variance)
     if (!is.character(variancetemp)) {
         variancetemp <- deparse(variancetemp)
         if (linktemp == "variance")
             variancetemp <- eval(variance)
     }
     switch(variancetemp, constant = {
         variance <- function(mu) rep.int(1, length(mu))
         dev.resids <- function(y, mu, wt) wt * ((y - mu)^2)
         validmu <- function(mu) TRUE
     }, "mu(1-mu)" = {
         variance <- function(mu) mu * (1 - mu)
         validmu <- function(mu) all(mu > 0) && all(mu < 1)
         dev.resids <- function(y, mu, wt) 2 * wt * (y * log(ifelse(y ==
             0, 1, y/mu)) + (1 - y) * log(ifelse(y == 1, 1, (1 -
             y)/(1 - mu))))
     }, "mu^2(1-mu)^2" = {
         variance <- function(mu) mu^2 * (1 - mu)^2
         validmu <- function(mu) all(mu > 0) && all(mu < 1)
         dev.resids <- function(y, mu, wt) 2 * wt * ((2 * y - 1) *
             (log(ifelse(y == 0, 1, y/mu)) - log(ifelse(y == 1, 1,
             (1 - y)/(1 - mu)))) - 2 + y/mu + (1 - y)/(1 - mu))
     }, mu = {
         variance <- function(mu) mu
         validmu <- function(mu) all(mu > 0)
         dev.resids <- function(y, mu, wt) 2 * wt * (y * log(ifelse(y ==
             0, 1, y/mu)) - (y - mu))
     }, "mu^2" = {
         variance <- function(mu) mu^2
         validmu <- function(mu) all(mu > 0)
         dev.resids <- function(y, mu, wt) pmax(-2 * wt * (log(ifelse(y ==
             0, 1, y)/mu) - (y - mu)/mu), 0)
     }, "mu^3" = {
         variance <- function(mu) mu^3
         validmu <- function(mu) all(mu > 0)
         dev.resids <- function(y, mu, wt) wt * ((y - mu)^2)/(y *
             mu^2)
     }, stop(gettextf("'variance' \"%s\" is invalid: possible values are 
\"mu(1-mu)\", \"mu^2(1-mu)^2\", \"mu\", \"mu^2\", \"mu^3\" and 
\"constant\"",
         variancetemp), domain = NA))
     initialize <- expression({
         n <- rep.int(1, nobs)
         mustart <- y + 0.1 * (y == 0)
     })
     aic <- function(y, n, mu, wt, dev) NA
     structure(list(family = "quasi", link = linktemp, linkfun = 
stats$linkfun,
         linkinv = stats$linkinv, variance = variance, dev.resids = 
dev.resids,
         aic = aic, mu.eta = stats$mu.eta, initialize = initialize,
         validmu = validmu, valideta = stats$valideta, varfun = 
variancetemp),
         class = "family")
}



From tillard at cirad.fr  Thu Jun 16 10:31:25 2005
From: tillard at cirad.fr (Emmanuel Tillard)
Date: Thu, 16 Jun 2005 10:31:25 +0200
Subject: [R] identical results with PQL and Laplace options in lmer function
 (package lme4)
Message-ID: <42B138DD.5030203@cirad.fr>

Dear R users

I encounter a problem when i perform a generalized linear mixed model (binary data) with the lmer function (package lme4)
with R 2.1.0 on windows XP and the latest version of package "lme4" (0.96-1) and "matrix" (0.96-2)
both options "PQL" and "Laplace" for the method argument in lmer function gave me the same results (random and fixed effects estimates, standard error and p.values). However, Loglikelihood and deviance are different.

here is an example reproduced with the bacteria data set available in the MASS package:

library(lme4)
library(MASS)

data(bacteria)
bacteria$week2 <- as.factor(ifelse(bacteria$week <=2, 0, 1))
model.PQL <- lmer(y ~ trt + week2 + (1 | ID), family = binomial, data  = bacteria, method ="PQL")
model.Laplace <- lmer(y ~ trt + week2 + (1 | ID), family = binomial, data = bacteria, method ="Laplace")

model.PQL
model.Laplace

> model.PQL
Generalized linear mixed model fit using PQL 
Formula: y ~ trt + week2 + (1 | ID) 
   Data: bacteria 
 Family: binomial(logit link)
     AIC      BIC   logLik deviance
 152.443 138.8685 -80.2215  160.443
Random effects:
     Groups        Name    Variance    Std.Dev. 
         ID (Intercept)      3.2721      1.8089 
# of obs: 220, groups: ID, 50

Estimated scale (compare to 1)  0.7800484 

Fixed effects:
            Estimate Std. Error z value  Pr(>|z|)    
(Intercept)  3.41227    0.65884  5.1792 2.228e-07 ***
trtdrug     -1.24743    0.81841 -1.5242 0.1274555    
trtdrug+    -0.75440    0.82009 -0.9199 0.3576229    
week21      -1.60737    0.45527 -3.5306 0.0004146 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

> model.Laplace
Generalized linear mixed model fit using Laplace 
Formula: y ~ trt + week2 + (1 | ID) 
   Data: bacteria 
 Family: binomial(logit link)
      AIC      BIC    logLik deviance
 304.9488 291.3743 -156.4744 312.9488
Random effects:
     Groups        Name    Variance    Std.Dev. 
         ID (Intercept)      3.2721      1.8089 
# of obs: 220, groups: ID, 50

Estimated scale (compare to 1)  0.7800484 

Fixed effects:
            Estimate Std. Error z value  Pr(>|z|)    
(Intercept)  3.41227    0.65884  5.1792 2.228e-07 ***
trtdrug     -1.24743    0.81841 -1.5242 0.1274555    
trtdrug+    -0.75440    0.82009 -0.9199 0.3576229    
week21      -1.60737    0.45527 -3.5306 0.0004146 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

is anybody else aware of this? or did I forget something important ?

Many thanks for your help.

-- 
Emmanuel Tillard
Veterinaire
CIRAD-EMVT
Unite de recherche 18

UMR868 Elevage des Ruminants en Regions Chaudes (ERRC)
Campus ENSA-INRA
2 place Viala
34060 Montpellier cedex 1

tel:	0499612265 (fixe)
	0633850598 (gsm)
fax:	0467545694
e-mail: tillard at cirad.fr



From vinum at iinet.net.au  Thu Jun 16 10:33:20 2005
From: vinum at iinet.net.au (John Considine)
Date: Thu, 16 Jun 2005 16:33:20 +0800
Subject: [R] Sweave and sideways
Message-ID: <200506160832.j5G8WA71026274@hypatia.math.ethz.ch>

Hi there,

I'm rying to 'turn' an Schunk in an .Rnw file(Xemacs-21.4.13, ESS-5.2.8,
R-2.1, miktex-2.4.1705).  

Has anyone got the isorot package to work with Sweave?

JC

example test.Rnw:

\documentclass[a4paper]{article}
\usepackage{Sweave}
\usepackage{isorot}
\rotdriver{dvips}
\clockwise
\title{Sweave Example 1}
\author{apologies to Friedrich Leisch }
\begin{document}
\maketitle
\begin{sideways}
In this example we embed parts of the examples from the
\texttt{kruskal.test} help page into a \LaTeX{} document:
<<>>=
data(airquality)
library(ctest)
kruskal.test(Ozone~Month , data = airquality )
@
\end{sideways}
\end{document}

...which I Sweave with:

>  Sweave("C:\\Documents and Settings\\John Considine\\test.Rnw")

...producing:
\documentclass[a4paper]{article}
\usepackage{Sweave}
\usepackage{isorot}
\rotdriver{dvips}
\clockwise
\title{Sweave Example 1}
\author{apologies to Friedrich Leisch }
\begin{document}
\maketitle
\begin{sideways}
In this example we embed parts of the examples from the
\texttt{kruskal.test} help page into a \LaTeX{} document:
\begin{Schunk}
\begin{Sinput}
> data(airquality)
> library(ctest)
> kruskal.test(Ozone ~ Month, data = airquality)
\end{Sinput}
\begin{Soutput}

	Kruskal-Wallis rank sum test

data:  Ozone by Month 
Kruskal-Wallis chi-squared = 29.2666, df = 4, p-value = 6.901e-06


\end{Soutput}
\end{Schunk}
\end{sideways}
\end{document}

Running it through latex gives the following log file:
This is e-TeX, Version 3.141592-2.2 (MiKTeX 2.4) (preloaded format=latex
2005.6.16)  16 JUN 2005 16:09
entering extended mode
**test.tex
(test.tex
LaTeX2e <2003/12/01>
Babel <v3.8a> and hyphenation patterns for english, french, german, ngerman,
du
mylang, nohyphenation, loaded.
(C:\texmf\tex\latex\base\article.cls
Document Class: article 2004/02/16 v1.4f Standard LaTeX document class
(C:\texmf\tex\latex\base\size10.clo
File: size10.clo 2004/02/16 v1.4f Standard LaTeX file (size option)
)
\c at part=\count79
\c at section=\count80
\c at subsection=\count81
\c at subsubsection=\count82
\c at paragraph=\count83
\c at subparagraph=\count84
\c at figure=\count85
\c at table=\count86
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
) (C:\texmf\tex\latex\sweave\Sweave.sty
Package: Sweave 

(C:\texmf\tex\latex\base\ifthen.sty
Package: ifthen 2001/05/26 v1.1c Standard LaTeX ifthen package (DPC)
) (C:\texmf\tex\latex\base\fontenc.sty
Package: fontenc 2004/02/22 v1.99f Standard LaTeX package

(C:\texmf\tex\latex\base\t1enc.def
File: t1enc.def 2004/02/22 v1.99f Standard LaTeX file
LaTeX Font Info:    Redeclaring font encoding T1 on input line 43.
)) (C:\texmf\tex\latex\graphics\graphicx.sty
Package: graphicx 1999/02/16 v1.0f Enhanced LaTeX Graphics (DPC,SPQR)

(C:\texmf\tex\latex\graphics\keyval.sty
Package: keyval 1999/03/16 v1.13 key=value parser (DPC)
\KV at toks@=\toks14
)
(C:\texmf\tex\latex\graphics\graphics.sty
Package: graphics 2001/07/07 v1.0n Standard LaTeX Graphics (DPC,SPQR)
 (C:\texmf\tex\latex\graphics\trig.sty
Package: trig 1999/03/16 v1.09 sin cos tan (DPC)
) (C:\texmf\tex\latex\00miktex\graphics.cfg
File: graphics.cfg 2003/03/12 v1.1 MiKTeX 'graphics' configuration
)
Package graphics Info: Driver file: dvips.def on input line 80.

(C:\texmf\tex\latex\graphics\dvips.def
File: dvips.def 1999/02/16 v3.0i Driver-dependant file (DPC,SPQR)
))
\Gin at req@height=\dimen103
\Gin at req@width=\dimen104
) (C:\texmf\tex\latex\ae\ae.sty
Package: ae 2001/02/12 1.3 Almost European Computer Modern

(C:\texmf\tex\latex\base\fontenc.sty
Package: fontenc 2004/02/22 v1.99f Standard LaTeX package
 (C:\texmf\tex\latex\base\t1enc.def
File: t1enc.def 2004/02/22 v1.99f Standard LaTeX file
LaTeX Font Info:    Redeclaring font encoding T1 on input line 43.
)
LaTeX Font Info:    Try loading font information for T1+aer on input line
100.

(C:\texmf\tex\latex\ae\t1aer.fd
File: t1aer.fd 1997/11/16 Font definitions for T1/aer.
))) (C:\texmf\tex\latex\fancyvrb\fancyvrb.sty
Package: fancyvrb 1998/07/17

Style option: `fancyvrb' v2.6, with DG/SPQR fixes <1998/07/17> (tvz)
\FV at CodeLineNo=\count87
\FV at InFile=\read1
\FV at TabBox=\box26
\c at FancyVerbLine=\count88
\FV at StepNumber=\count89
\FV at OutFile=\write3

No file fancyvrb.cfg.
) (C:\texmf\tex\latex\upquote\upquote.sty
Package: upquote 2003/08/11 v1.1 Covington's upright-quote modification to
verb
atim and verb
 (C:\texmf\tex\latex\base\textcomp.sty
Package: textcomp 2004/02/22 v1.99f Standard LaTeX package
Package textcomp Info: Sub-encoding information:
(textcomp)               5 = only ISO-Adobe without \textcurrency
(textcomp)               4 = 5 + \texteuro
(textcomp)               3 = 4 + \textohm
(textcomp)               2 = 3 + \textestimated + \textcurrency
(textcomp)               1 = TS1 - \textcircled - \t
(textcomp)               0 = TS1 (full)
(textcomp)             Font families with sub-encoding setting implement
(textcomp)             only a restricted character set as indicated.
(textcomp)             Family '?' is the default used for unknown fonts.
(textcomp)             See the documentation for details.
Package textcomp Info: Setting ? sub-encoding to TS1/1 on input line 71.
(C:\texmf\tex\latex\base\ts1enc.def
File: ts1enc.def 2001/06/05 v3.0e (jk/car/fm) Standard LaTeX file
)
LaTeX Info: Redefining \oldstylenums on input line 266.
Package textcomp Info: Setting cmr sub-encoding to TS1/0 on input line 281.
Package textcomp Info: Setting cmss sub-encoding to TS1/0 on input line 282.
Package textcomp Info: Setting cmtt sub-encoding to TS1/0 on input line 283.
Package textcomp Info: Setting cmvtt sub-encoding to TS1/0 on input line
284.
Package textcomp Info: Setting cmbr sub-encoding to TS1/0 on input line 285.
Package textcomp Info: Setting cmtl sub-encoding to TS1/0 on input line 286.
Package textcomp Info: Setting ccr sub-encoding to TS1/0 on input line 287.
Package textcomp Info: Setting ptm sub-encoding to TS1/4 on input line 288.
Package textcomp Info: Setting pcr sub-encoding to TS1/4 on input line 289.
Package textcomp Info: Setting phv sub-encoding to TS1/4 on input line 290.
Package textcomp Info: Setting ppl sub-encoding to TS1/3 on input line 291.
Package textcomp Info: Setting pag sub-encoding to TS1/4 on input line 292.
Package textcomp Info: Setting pbk sub-encoding to TS1/4 on input line 293.
Package textcomp Info: Setting pnc sub-encoding to TS1/4 on input line 294.
Package textcomp Info: Setting pzc sub-encoding to TS1/4 on input line 295.
Package textcomp Info: Setting bch sub-encoding to TS1/4 on input line 296.
Package textcomp Info: Setting put sub-encoding to TS1/5 on input line 297.
Package textcomp Info: Setting uag sub-encoding to TS1/5 on input line 298.
Package textcomp Info: Setting ugq sub-encoding to TS1/5 on input line 299.
Package textcomp Info: Setting ul8 sub-encoding to TS1/4 on input line 300.
Package textcomp Info: Setting ul9 sub-encoding to TS1/4 on input line 301.
Package textcomp Info: Setting augie sub-encoding to TS1/5 on input line
302.
Package textcomp Info: Setting dayrom sub-encoding to TS1/3 on input line
303.
Package textcomp Info: Setting dayroms sub-encoding to TS1/3 on input line
304.

Package textcomp Info: Setting pxr sub-encoding to TS1/0 on input line 305.
Package textcomp Info: Setting pxss sub-encoding to TS1/0 on input line 306.
Package textcomp Info: Setting pxtt sub-encoding to TS1/0 on input line 307.
Package textcomp Info: Setting txr sub-encoding to TS1/0 on input line 308.
Package textcomp Info: Setting txss sub-encoding to TS1/0 on input line 309.
Package textcomp Info: Setting txtt sub-encoding to TS1/0 on input line 310.
Package textcomp Info: Setting futs sub-encoding to TS1/4 on input line 311.
Package textcomp Info: Setting futx sub-encoding to TS1/4 on input line 312.
Package textcomp Info: Setting futj sub-encoding to TS1/4 on input line 313.
Package textcomp Info: Setting hlh sub-encoding to TS1/3 on input line 314.
Package textcomp Info: Setting hls sub-encoding to TS1/3 on input line 315.
Package textcomp Info: Setting hlst sub-encoding to TS1/3 on input line 316.
Package textcomp Info: Setting hlct sub-encoding to TS1/5 on input line 317.
Package textcomp Info: Setting hlx sub-encoding to TS1/5 on input line 318.
Package textcomp Info: Setting hlce sub-encoding to TS1/5 on input line 319.
Package textcomp Info: Setting hlcn sub-encoding to TS1/5 on input line 320.
Package textcomp Info: Setting hlcw sub-encoding to TS1/5 on input line 321.
Package textcomp Info: Setting hlcf sub-encoding to TS1/5 on input line 322.
Package textcomp Info: Setting pplx sub-encoding to TS1/3 on input line 323.
Package textcomp Info: Setting pplj sub-encoding to TS1/3 on input line 324.
Package textcomp Info: Setting ptmx sub-encoding to TS1/4 on input line 325.
Package textcomp Info: Setting ptmj sub-encoding to TS1/4 on input line 326.
))) (C:\texmf\tex\latex\isorot\isorot.sty
Package: isorot 2000/02/15 v2.1 ISO rotation package
\PWRFc at tracing=\count90

(C:\texmf\tex\latex\graphics\lscape.sty
Package: lscape 2000/10/22 v3.01 Landscape Pages (DPC)
)
\c at r@tfl at t=\count91
\rot at float@box=\box27
)
The .dvi file is to be processed by the dvips driver.
Change rotdriver in the source accordingly if you do not have this.
(C:\texmf\tex\latex\graphics\dvips.def
File: dvips.def 1999/02/16 v3.0i Driver-dependant file (DPC,SPQR)
) (test.aux)
LaTeX Font Info:    Checking defaults for OML/cmm/m/it on input line 8.
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 8.
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 8.
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on input line 8.
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on input line 8.
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 8.
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    Checking defaults for TS1/cmr/m/n on input line 8.
LaTeX Font Info:    Try loading font information for TS1+cmr on input line
8.

(C:\texmf\tex\latex\base\ts1cmr.fd
File: ts1cmr.fd 1999/05/25 v2.5h Standard LaTeX font definitions
)
LaTeX Font Info:    ... okay on input line 8.
LaTeX Font Info:    External font `cmex10' loaded for size
(Font)              <12> on input line 9.
LaTeX Font Info:    External font `cmex10' loaded for size
(Font)              <8> on input line 9.
LaTeX Font Info:    External font `cmex10' loaded for size
(Font)              <6> on input line 9.
LaTeX Font Info:    Try loading font information for T1+aett on input line
12.
 (C:\texmf\tex\latex\ae\t1aett.fd
File: t1aett.fd 1997/11/16 Font definitions for T1/aett.
)
LaTeX Font Info:    External font `cmex10' loaded for size
(Font)              <7> on input line 12.
LaTeX Font Info:    External font `cmex10' loaded for size
(Font)              <5> on input line 12.


! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.14 \begin{Sinput}
                   
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.14 \begin{Sinput}
                   
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.18 \end{Sinput}
                 
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.18 \end{Sinput}
                 
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.19 \begin{Soutput}
                    
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.19 \begin{Soutput}
                    
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.27 \end{Soutput}
                  
? 

! LaTeX Error: Something's wrong--perhaps a missing \item.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.27 \end{Soutput}
                  
? 
[1

]
Overfull \vbox (3383.85861pt too high) has occurred while \output is active
[]


[2] (test.aux) ) 
Here is how much of TeX's memory you used:
 1890 strings out of 95898
 23049 string characters out of 1195288
 64391 words of memory out of 1067656
 4842 multiletter control sequences out of 35000
 13777 words of font info for 30 fonts, out of 500000 for 1000
 14 hyphenation exceptions out of 607
 35i,6n,21p,255b,187s stack positions out of 1500i,500n,5000p,200000b,32768s

Output written on test.dvi (2 pages, 1192 bytes).

JC Considine



From subramanian.vivek at gmail.com  Thu Jun 16 12:21:22 2005
From: subramanian.vivek at gmail.com (Vivek Subramanian)
Date: Thu, 16 Jun 2005 15:51:22 +0530
Subject: [R] Excel files first row not being read
Message-ID: <20e69eb7050616032138ec78d8@mail.gmail.com>

hi,

i am using the RODBC package to read excel files using
odbcConnectExcel and susequently sqlFetch to read the contents of the
file.

the file that i use is just a matrix of numbers thats all. no headers
and column names. what happens is that the sqlFetch is not reading my
first row of numbers.
i have tried different combinations of colnames and rownames logical
values but that first row is not being read into my data frame.

i can manually set the first row to blanks in my worksheet, but since
this is part of larger code where a user specifies a file to use this
method is a little clumsy.

i would be grateful if you help me get around this problem. 

regards,
vivek



From 0034058 at fudan.edu.cn  Thu Jun 16 12:48:47 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 16 Jun 2005 18:48:47 +0800
Subject: [R] identical results with PQL and Laplace options in lmer
 function (package lme4)
In-Reply-To: <42B138DD.5030203@cirad.fr>
References: <42B138DD.5030203@cirad.fr>
Message-ID: <20050616184847.75b08eee@localhost.localdomain>

i am aware of this but i don't know why.

On Thu, 16 Jun 2005 10:31:25 +0200
Emmanuel Tillard <tillard at cirad.fr> wrote:

> Dear R users
> 
> I encounter a problem when i perform a generalized linear mixed model (binary data) with the lmer function (package lme4)
> with R 2.1.0 on windows XP and the latest version of package "lme4" (0.96-1) and "matrix" (0.96-2)
> both options "PQL" and "Laplace" for the method argument in lmer function gave me the same results (random and fixed effects estimates, standard error and p.values). However, Loglikelihood and deviance are different.
> 
> here is an example reproduced with the bacteria data set available in the MASS package:
> 
> library(lme4)
> library(MASS)
> 
> data(bacteria)
> bacteria$week2 <- as.factor(ifelse(bacteria$week <=2, 0, 1))
> model.PQL <- lmer(y ~ trt + week2 + (1 | ID), family = binomial, data  = bacteria, method ="PQL")
> model.Laplace <- lmer(y ~ trt + week2 + (1 | ID), family = binomial, data = bacteria, method ="Laplace")
> 
> model.PQL
> model.Laplace
> 
> > model.PQL
> Generalized linear mixed model fit using PQL 
> Formula: y ~ trt + week2 + (1 | ID) 
>    Data: bacteria 
>  Family: binomial(logit link)
>      AIC      BIC   logLik deviance
>  152.443 138.8685 -80.2215  160.443
> Random effects:
>      Groups        Name    Variance    Std.Dev. 
>          ID (Intercept)      3.2721      1.8089 
> # of obs: 220, groups: ID, 50
> 
> Estimated scale (compare to 1)  0.7800484 
> 
> Fixed effects:
>             Estimate Std. Error z value  Pr(>|z|)    
> (Intercept)  3.41227    0.65884  5.1792 2.228e-07 ***
> trtdrug     -1.24743    0.81841 -1.5242 0.1274555    
> trtdrug+    -0.75440    0.82009 -0.9199 0.3576229    
> week21      -1.60737    0.45527 -3.5306 0.0004146 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> > model.Laplace
> Generalized linear mixed model fit using Laplace 
> Formula: y ~ trt + week2 + (1 | ID) 
>    Data: bacteria 
>  Family: binomial(logit link)
>       AIC      BIC    logLik deviance
>  304.9488 291.3743 -156.4744 312.9488
> Random effects:
>      Groups        Name    Variance    Std.Dev. 
>          ID (Intercept)      3.2721      1.8089 
> # of obs: 220, groups: ID, 50
> 
> Estimated scale (compare to 1)  0.7800484 
> 
> Fixed effects:
>             Estimate Std. Error z value  Pr(>|z|)    
> (Intercept)  3.41227    0.65884  5.1792 2.228e-07 ***
> trtdrug     -1.24743    0.81841 -1.5242 0.1274555    
> trtdrug+    -0.75440    0.82009 -0.9199 0.3576229    
> week21      -1.60737    0.45527 -3.5306 0.0004146 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> is anybody else aware of this? or did I forget something important ?
> 
> Many thanks for your help.
> 
> -- 
> Emmanuel Tillard
> Veterinaire
> CIRAD-EMVT
> Unite de recherche 18
> 
> UMR868 Elevage des Ruminants en Regions Chaudes (ERRC)
> Campus ENSA-INRA
> 2 place Viala
> 34060 Montpellier cedex 1
> 
> tel:	0499612265 (fixe)
> 	0633850598 (gsm)
> fax:	0467545694
> e-mail: tillard at cirad.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From subramanian.vivek at gmail.com  Thu Jun 16 13:00:45 2005
From: subramanian.vivek at gmail.com (Vivek Subramanian)
Date: Thu, 16 Jun 2005 16:30:45 +0530
Subject: [R] Excel files first row not being read
In-Reply-To: <20050616184715.7bfa7104@localhost.localdomain>
References: <20e69eb7050616032138ec78d8@mail.gmail.com>
	<20050616184715.7bfa7104@localhost.localdomain>
Message-ID: <20e69eb705061604004c11845@mail.gmail.com>

hi,

the specifications for my application call that the input file be in
excel only in the format i previously mentioned.
now if the approach you mention has to be used then the part of
converting has to be done automatically. this i found to be too
complex.

an alternative to that was to use MS access databases. but again it is
the problem of automating the conversion that is proving difficult

regards and best
vivek

On 6/16/05, ronggui <0034058 at fudan.edu.cn> wrote:
> why not save the file as csv file(using the MS excel application) and use the read.csv function in foreign library?
> 
> On Thu, 16 Jun 2005 15:51:22 +0530
> Vivek Subramanian <subramanian.vivek at gmail.com> wrote:
> 
> > hi,
> >
> > i am using the RODBC package to read excel files using
> > odbcConnectExcel and susequently sqlFetch to read the contents of the
> > file.
> >
> > the file that i use is just a matrix of numbers thats all. no headers
> > and column names. what happens is that the sqlFetch is not reading my
> > first row of numbers.
> > i have tried different combinations of colnames and rownames logical
> > values but that first row is not being read into my data frame.
> >
> > i can manually set the first row to blanks in my worksheet, but since
> > this is part of larger code where a user specifies a file to use this
> > method is a little clumsy.
> >
> > i would be grateful if you help me get around this problem.
> >
> > regards,
> > vivek
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From vinum at iinet.net.au  Thu Jun 16 13:51:45 2005
From: vinum at iinet.net.au (John Considine)
Date: Thu, 16 Jun 2005 19:51:45 +0800
Subject: [R] Excel files first row not being read
In-Reply-To: <20e69eb705061604004c11845@mail.gmail.com>
Message-ID: <200506161150.j5GBokTt010884@hypatia.math.ethz.ch>

Hi Vivek,

I'm sure there must be an elegant way to interface R w ODBC connections, but
I find it simplest to create a macro in Access that exports the data (based
on a query or table) to a csv file, then use the R function read.csv() in
which you can specify header=FALSE if you wish. I've used the importData
function in S+, which can import directly from an access database, but the
two step process in R is quicker. Excel is pretty limiting if you have lots
of data.

JC Considine 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vivek 
> Subramanian
> Sent: Thursday, 16 June 2005 19:01
> To: ronggui; rhelp
> Subject: Re: [R] Excel files first row not being read
> 
> hi,
> 
> the specifications for my application call that the input file be in
> excel only in the format i previously mentioned.
> now if the approach you mention has to be used then the part of
> converting has to be done automatically. this i found to be too
> complex.
> 
> an alternative to that was to use MS access databases. but again it is
> the problem of automating the conversion that is proving difficult
> 
> regards and best
> vivek
> 
> On 6/16/05, ronggui <0034058 at fudan.edu.cn> wrote:
> > why not save the file as csv file(using the MS excel 
> application) and use the read.csv function in foreign library?
> > 
> > On Thu, 16 Jun 2005 15:51:22 +0530
> > Vivek Subramanian <subramanian.vivek at gmail.com> wrote:
> > 
> > > hi,
> > >
> > > i am using the RODBC package to read excel files using
> > > odbcConnectExcel and susequently sqlFetch to read the 
> contents of the
> > > file.
> > >
> > > the file that i use is just a matrix of numbers thats 
> all. no headers
> > > and column names. what happens is that the sqlFetch is 
> not reading my
> > > first row of numbers.
> > > i have tried different combinations of colnames and 
> rownames logical
> > > values but that first row is not being read into my data frame.
> > >
> > > i can manually set the first row to blanks in my 
> worksheet, but since
> > > this is part of larger code where a user specifies a file 
> to use this
> > > method is a little clumsy.
> > >
> > > i would be grateful if you help me get around this problem.
> > >
> > > regards,
> > > vivek
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bld at math.umd.edu  Thu Jun 16 14:04:18 2005
From: bld at math.umd.edu (Bernard L. Dillard)
Date: Thu, 16 Jun 2005 08:04:18 -0400 (EDT)
Subject: [R] Moving average
Message-ID: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>

Good morning all!

I am attempting to superimpose a moving-average smoother onto a graph of
daily plots.  These plots (in table[,2] below) span about 350 days and
looks very noisy.  I'd like for this smoother to plot the average of each
group of 7 consecutive days (weekly) and show a line which joins these
series of averages.  Given the definition of MA, the first and last points
will generally overlap in the average calculation.

It's probably a one-liner, but I still am having some problems with the
syntax.  The only part I have correct is the "lines" statement to ensure
it overlays my original graph.

Here's the code I have thus far:

y <- table[,2]
plot(y,type="l",lty=3)
lines( ......)  {moving average code here to be placed here}

Any time series gurus out there?  Be gentle.  I'm an R beginner.

Thanx!

Bernard



From sdavis2 at mail.nih.gov  Thu Jun 16 14:14:35 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 16 Jun 2005 08:14:35 -0400
Subject: [R] Moving average
In-Reply-To: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>
References: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>
Message-ID: <23a39c14d4740bb34d6246c256323b18@mail.nih.gov>


On Jun 16, 2005, at 8:04 AM, Bernard L. Dillard wrote:

> Good morning all!
>
> I am attempting to superimpose a moving-average smoother onto a graph  
> of
> daily plots.  These plots (in table[,2] below) span about 350 days and
> looks very noisy.  I'd like for this smoother to plot the average of  
> each
> group of 7 consecutive days (weekly) and show a line which joins these
> series of averages.  Given the definition of MA, the first and last  
> points
> will generally overlap in the average calculation.
>
> It's probably a one-liner, but I still am having some problems with the
> syntax.  The only part I have correct is the "lines" statement to  
> ensure
> it overlays my original graph.
>
> Here's the code I have thus far:
>
> y <- table[,2]
> plot(y,type="l",lty=3)
> lines( ......)  {moving average code here to be placed here}
>
> Any time series gurus out there?  Be gentle.  I'm an R beginner.

R site search is your friend:

http://finzi.psych.upenn.edu/cgi-bin/namazu.cgi? 
query=moving+window&max=20&result=normal&sort=score&idxname=functions&id 
xname=docs&idxname=Rhelp02a

Sean



From andy_liaw at merck.com  Thu Jun 16 14:22:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Jun 2005 08:22:53 -0400
Subject: [R] need help on computing double summation
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E99D@usctmx1106.merck.com>

If I understood correctly, the following might be simpler (dat is the data
frame holding the data):

> sum(ave(dat$x, dat$id, FUN=scale, scale=FALSE) * 
+     ave(dat$y, dat$id, FUN=scale, scale=FALSE))
[1] 6.229377

Andy


> From: Huntsinger, Reid
> 
> You could do something like
> 
> ids <- unique(mydata$id)
> ans <- vector(length=length(ids), mode="list")
> for (i in ids) {
>   g <- which(mydata$id == i)
>   ans[[i]] <- (length(g) - 1)*cov(mydata$x[g], mydata$y[g])
> }
> ans
> 
> but cov() returns NA for length 1 vectors, so you'd want an 
> if (length(g) ==
> 1) ans[i] <- 0 else ans[i] <- ... construction.
> 
> This is almost brute force; you could also use tapply, as follows:
> 
> sx <- tapply(mydata$x,INDEX=mydata$id,FUN=sum)
> sy <- tapply(mydata$y,INDEX=mydata$id,FUN=sum)
> sxy <- tapply(mydata$x*mydata$y, INDEX=mydata$id, FUN=sum)
> n <- tapply(mydata$id,INDEX=mydata$id,FUN=length) # or use table()!
> 
> and now your inner sum is
> 
> sxy - 2*sx*(sy/n) + n*(sx/n)*(sy/n) = sxy - sx*sy/n
> 
> so 
> 
> sum(sxy - sx*sy/n) should do.
> 
> One more approach is to make your dataset into a list of data 
> frames, one
> for each id, then use lapply(). The list can be created by 
> split(). In one
> line,
> 
> lapply(split(mydata,f=mydata$id),function(z) (length(z$x) - 
> 1)*cov(z$x,z$y))
> 
> and take sum(,na.rm=TRUE) to remove the NAs due to single ids 
> that you want
> to be zeros.
> 
> Reid Huntsinger
> 
> 
> 
> 
> Reid Huntsinger
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kerry Bush
> Sent: Wednesday, June 15, 2005 11:41 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] need help on computing double summation
> 
> 
> Dear helpers in this forum,
> 
>    This is a clarified version of my previous
> questions  in this forum. I really need your generous
> help on this issue.
> 
> > Suppose I have the following data set:
> > 
> > 
> > ......
> > 
> 
> Now I want to compute the following double summation:
> 
> sum_{i=1}^k
> sum_{j=1}^{n_i}(x_{ij}-mean(x_i))*(y_{ij}-mean(y_i))
> 
> i is from 1 to k,
> indexing the ith subject id; and j is from 1 to n_i,
> indexing the jth observation for the ith subject.
> 
> in the above expression, mean(x_i) is the mean of x
> values for the ith
> subject, mean(y_i) is the mean of y values for the ith
> subject. 
> 
> Is there a simple way to do this in R?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
> 
>



From HankeA at mar.dfo-mpo.gc.ca  Thu Jun 16 14:23:34 2005
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 16 Jun 2005 09:23:34 -0300
Subject: [R] Error using newdata argument in survfit
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE02043804@msgmarsta01.bio.dfo.ca>

Hi Brian,
The factor Prior.f has 5 levels (1,2,3,4,5) which coxph deals with by
creating 4 dummy variables coded with 1 or zero. That's what I see when I
look at fit$x.
fit$x[1:5,]
   Week LagAOO factor(Prior.f)2 factor(Prior.f)3 factor(Prior.f)4
31   22      0                0                0                0
32   22      0                0                0                0
33   22      2                0                0                0
34   22      3                0                0                0
35   22      2                0                0                0
   factor(Prior.f)5
31                0
32                0
33                0
34                0
35                0 
I have played with the formula a bit adding a term at a time and then
checking to see if I can produce the survival curves for pseudo cohorts. I
get as far as the Prior.f term and am successful if I treat it as a
continuous variable. If I introduce it as a factor and assume it wants four
dummy variables as above I get the variable lengths error. If I represent
the term with one variable:
survfit(fit,list(Week=c(15,15),LagAOO=c(0,0),Prior.f=c(1,2)))
I get:
Error in x2 %*% coef : non-conformable arguments
Which is a nice change but still short of knowing what is going on.
 Regards
Alex 

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: June 15, 2005 4:24 PM
To: Hanke, Alex
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Error using newdata argument in survfit

You appear to have a coding for prior.f in newdata rather than the factor 
itself.

It's a bit hard to be sure when we don't have data8 to compare with.

On Wed, 15 Jun 2005, Hanke, Alex wrote:

> Dear R-helpers,
> To get curves for a pseudo cohort other than the one centered at the mean
of
> the covariates, I have been trying to use the newdata argument to survfit
> with no success. Here is my model statement, the newdata and the ensuing
> error. What am I doing wrong?
>
>> summary(fit)
> Call:
> coxph(formula = Surv(Start, Stop, Event, type = "counting") ~
>    Week + LagAOO + Prior.f + cluster(interaction(Station, Year)),
>    data = data8, method = "breslow", x = T, y = T)
>
>  n= 1878
>            coef exp(coef) se(coef) robust se     z       p
> Week     0.00582      1.01   0.0323    0.0239 0.244 8.1e-01
> LagAOO   0.71929      2.05   0.1238    0.1215 5.918 3.3e-09
> Prior.f2 0.12927      1.14   0.4402    0.4025 0.321 7.5e-01
> Prior.f3 0.79082      2.21   0.5484    0.4460 1.773 7.6e-02
> Prior.f4 2.04189      7.71   0.6008    0.4685 4.358 1.3e-05
> Prior.f5 1.20450      3.34   0.6423    0.5481 2.198 2.8e-02
>
>         exp(coef) exp(-coef) lower .95 upper .95
> Week          1.01      0.994     0.960      1.05
> LagAOO        2.05      0.487     1.618      2.61
> Prior.f2      1.14      0.879     0.517      2.50
> Prior.f3      2.21      0.453     0.920      5.29
> Prior.f4      7.71      0.130     3.076     19.30
> Prior.f5      3.34      0.300     1.139      9.76
>
> Rsquare= 0.047   (max possible= 0.25 )
> Likelihood ratio test= 91  on 6 df,   p=0
> Wald test            = 209  on 6 df,   p=0
> Score (logrank) test = 142  on 6 df,   p=0,   Robust = 17.4  p=0.00803
>
>  (Note: the likelihood ratio and score tests assume independence of
>     observations within a cluster, the Wald and robust score tests do
not).
>>
> newdat
>      Week   LagAOO Prior.f2 Prior.f3 Prior.f4 Prior.f5
> 1 17.55218 1.191693        1        0        0        0
> 2 17.55218 1.191693        0        0        0        0
>
>> survfit(fit,newdata=newdat)
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        variable lengths differ
> In addition: Warning message:
> 'newdata' had 2 rows but variable(s) found have 1878 rows
>
> Regards,
> Alex
>
>
> Alex Hanke
> Department of Fisheries and Oceans
> St. Andrews Biological Station
> 531 Brandy Cove Road
> St. Andrews, NB
> Canada
> E5B 2L9
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Thu Jun 16 14:31:21 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 16 Jun 2005 20:31:21 +0800
Subject: [R] Moving average
In-Reply-To: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>
References: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>
Message-ID: <20050616203121.598fc24c@localhost.localdomain>

are you looking the function decompose?
see ?decompose

On Thu, 16 Jun 2005 08:04:18 -0400 (EDT)
"Bernard L. Dillard" <bld at math.umd.edu> wrote:

> Good morning all!
> 
> I am attempting to superimpose a moving-average smoother onto a graph of
> daily plots.  These plots (in table[,2] below) span about 350 days and
> looks very noisy.  I'd like for this smoother to plot the average of each
> group of 7 consecutive days (weekly) and show a line which joins these
> series of averages.  Given the definition of MA, the first and last points
> will generally overlap in the average calculation.
> 
> It's probably a one-liner, but I still am having some problems with the
> syntax.  The only part I have correct is the "lines" statement to ensure
> it overlays my original graph.
> 
> Here's the code I have thus far:
> 
> y <- table[,2]
> plot(y,type="l",lty=3)
> lines( ......)  {moving average code here to be placed here}
> 
> Any time series gurus out there?  Be gentle.  I'm an R beginner.
> 
> Thanx!
> 
> Bernard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From neo27 at t-online.de  Thu Jun 16 14:56:35 2005
From: neo27 at t-online.de (Mark Hempelmann)
Date: Thu, 16 Jun 2005 14:56:35 +0200
Subject: [R] Survey - Cluster Sampling
Message-ID: <42B17703.8080606@t-online.de>

Dear WizaRds,

	I am struggling to compute correctly a cluster sampling design. I want
to do one stage clustering with different parametric changes:

Let M be the total number  of clusters in the population, and m the
number sampled. Let N be the total of elements in the population and n
the number sampled. y are the values sampled. This is my example data:

clus1 <- data.frame(cluster=c(1,1,1,2,2,2,3,3,3), id=seq(1:3,3),
weight=rep(72/9,9), nl=rep(3,9), Nl=rep(3,9), N=rep(72,9), y=c(23,33,77,
25,35,74, 27,37,72) )

1. Let M=m=3 and N=n=9. Then:

dclus1<-svydesign(id=~cluster,  data=clus1)
svymean(~y, dclus1)

     mean    SE
y 44.778 0.294, the unweighted mean, assuming equal probability in the
clusters. ok.

2. Let M=23, m=3 and N=72, n=9, then I am unable to use svydesign correctly:

dclus2<-svydesign(id=~cluster,  data=clus1, fpc=~N)
svymean(~y, dclus2)

     mean     SE
y 44.778 0.2878, but it should be 23/72 * 1/3(133+134+136)=42.91, since
I have to include the total number of clusters/total population M/N into
the estimator. How can I include the information of the total number of
clusters?

3. How do I work with weights correctly? I understand that weights imply
  inverse probability weighting 1/p with p=n/N in simple sampling, in
our case 72/9=8, because I sample 9 units out of a total population of
72. Again, I couldn't tell survey the number of total clusters M. So:

dclus3<-svydesign(id=~cluster,  weights=~weight, data=clus1, fpc=~N)
svymean(~y, dclus3)

     mean     SE
y 44.778 0.2878, still exactly the same numbers, although I provided the
weights. What am I doing wrong?

I am sorry to bother you. Studying Statistics isn't done in a day,
that's for sure. Thank you so much for your understanding and support.

mark



From andy_liaw at merck.com  Thu Jun 16 14:56:43 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Jun 2005 08:56:43 -0400
Subject: [R] Reducing the FPR  (false positive rate)
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E99E@usctmx1106.merck.com>

> From: Anderson de Rezende Rocha
> 
> Hello R-USERS, 
> 
> I think some people didn't understand my question. What I want is to
> use the training set to minimize the FALSE POSITIVE RATE. 
> 
> I think it is possible. I sacrifice ACCURACY to have less FALSE
> POSITIVES. I don't want a classifier result with 5% of FPR and, for
> example, 93% of ACCURACY. I want a 1% FPR sacrificing the 93% 
> ACCURACY.
> 
> 
> Do you know how can I do this? I really need this because the 
> requisite
> of the work I'm doing is only 1% of FPR. 

I'm afraid it's you who's not getting the point.  You don't need any
classifier to get minimum FPR:  just call everything `negative' and you have
0% FPR, which is as minimum as FPR gets.

FPR can not be fixed, as it's an unknown quantity.  It make as much sense to
say you want a classifier with a particular FPR as to say that you want a
classifier with a particular error rate.  What you can do is estimate the
parameters of the classifier that gives you an _approximate_ FPR that's
close to what you want, but there's no guarantee that you will get exactly
that FPR on any test set.

Andy
 
> Thanks and best regards. 
> 
> *************************************************
> |????????????????| - Anderson de Rezende Rocha
> Bacharel em Ci??ncia da Computa????o - UFLA
> Mestrando em Ci??ncia da Computa????o - UNICAMP
> Esteganografia e estegan??lise digital
> UNIVERSIDADE ESTADUAL DE CAMPINAS, SP - BRASIL
> < http://andersonrocha.cjb.net >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From br44114 at gmail.com  Thu Jun 16 15:24:04 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 16 Jun 2005 09:24:04 -0400
Subject: [R] Excel files first row not being read
Message-ID: <8d5a36350506160624580bfbf2@mail.gmail.com>

You could use a VB macro in Excel to automate the data export in CSV
format, and it's not complex at all, for example:
Private Sub CommandButton1_Click()
Dim strB18 As String
strB18 = Me.Cells(18, 2)
'MsgBox "Export Folder = " & strB18
On Error GoTo ErrHandler
Sheets("Inputs").SaveAs FileName:= _
        strB18 & "\Inputs.csv", FileFormat _
        :=xlCSV, CreateBackup:=False
MsgBox "Data Exported"
Exit Sub
ErrHandler:
MsgBox Err.Description
End Sub

On the other hand, if the specifications call that the input file be
in Excel, then the specifications are _wrong_. See
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html 
for details. Keep in mind that when exporting to CSV the numbers may
be written with a limited number of significant digits.



> -----Original Message-----
> From: Vivek Subramanian [mailto:subramanian.vivek at gmail.com]
> Sent: Thursday, June 16, 2005 7:01 AM
> To: ronggui; rhelp
> Subject: Re: [R] Excel files first row not being read
> 
> 
> hi,
> 
> the specifications for my application call that the input file be in
> excel only in the format i previously mentioned.
> now if the approach you mention has to be used then the part of
> converting has to be done automatically. this i found to be too
> complex.
> 
> an alternative to that was to use MS access databases. but again it is
> the problem of automating the conversion that is proving difficult
> 
> regards and best
> vivek
> 
> On 6/16/05, ronggui <0034058 at fudan.edu.cn> wrote:
> > why not save the file as csv file(using the MS excel 
> application) and use the read.csv function in foreign library?
> > 
> > On Thu, 16 Jun 2005 15:51:22 +0530
> > Vivek Subramanian <subramanian.vivek at gmail.com> wrote:
> > 
> > > hi,
> > >
> > > i am using the RODBC package to read excel files using
> > > odbcConnectExcel and susequently sqlFetch to read the 
> contents of the
> > > file.
> > >
> > > the file that i use is just a matrix of numbers thats 
> all. no headers
> > > and column names. what happens is that the sqlFetch is 
> not reading my
> > > first row of numbers.
> > > i have tried different combinations of colnames and 
> rownames logical
> > > values but that first row is not being read into my data frame.
> > >
> > > i can manually set the first row to blanks in my 
> worksheet, but since
> > > this is part of larger code where a user specifies a file 
> to use this
> > > method is a little clumsy.
> > >
> > > i would be grateful if you help me get around this problem.
> > >
> > > regards,
> > > vivek
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Thu Jun 16 15:28:05 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 16 Jun 2005 15:28:05 +0200
Subject: [R] Moving average
In-Reply-To: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>
References: <10664.66.92.23.42.1118923458.squirrel@66.92.23.42>
Message-ID: <20050616152805.6a2a29c7.Achim.Zeileis@wu-wien.ac.at>

On Thu, 16 Jun 2005 08:04:18 -0400 (EDT) Bernard L. Dillard wrote:

> Good morning all!
> 
> I am attempting to superimpose a moving-average smoother onto a graph
> of daily plots.  These plots (in table[,2] below) span about 350 days
> and looks very noisy.  I'd like for this smoother to plot the average
> of each group of 7 consecutive days (weekly) and show a line which
> joins these series of averages.  Given the definition of MA, the first
> and last points will generally overlap in the average calculation.
> 
> It's probably a one-liner, but I still am having some problems with
> the syntax.  The only part I have correct is the "lines" statement to
> ensure it overlays my original graph.
> 
> Here's the code I have thus far:
> 
> y <- table[,2]
> plot(y,type="l",lty=3)
> lines( ......)  {moving average code here to be placed here}

With the zoo package you can do the following:

library(zoo)
## create data
x <- rnorm(365)
## transform to regular zoo series with "Date" index
x <- zooreg(x, start = as.Date("2004-01-01"))
plot(x)

## add rolling/running/moving average with window size 7
lines(rollmean(x, 7), col = 2, lwd = 2)

## if you don't want the rolling mean but rather a weekly
## time series of means you can do
nextfri <- function(x) 7 * ceiling(as.numeric(x - 1)/7) + as.Date(1)
xw <- aggregate(x, nextfri, mean)
## nextfri is a function which computes for a certain "Date"
## the next friday. xw is then the weekly series.
lines(xw, col = 4)

Note, that the differnce between is rolling mean and the aggregated
series is due to different alignments. This can be changed by changing
the `align' argument in rollmean() or the nextfri() function in the
aggregate call.

hth,
Z


> Any time series gurus out there?  Be gentle.  I'm an R beginner.
> 
> Thanx!
> 
> Bernard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From vinum at iinet.net.au  Thu Jun 16 15:34:45 2005
From: vinum at iinet.net.au (John Considine)
Date: Thu, 16 Jun 2005 21:34:45 +0800
Subject: [R] Sweave and sideways
In-Reply-To: <200506161033.j5GAXemN030922@sanode11eth0.rz.Uni-Osnabrueck.DE>
Message-ID: <200506161333.j5GDXqvo005627@hypatia.math.ethz.ch>

Thank you Dietrich. 

The minipage suggestion works a treat. 

Why does pdflatex and latex work differently though? 

If I latex the modified code, the 'sideways' part is still horizontal, yet
when I use pdflatex it appears sideways?

JC Considine 

> -----Original Message-----
> From: Dietrich Trenkler [mailto:dietrich.trenkler at uni-osnabrueck.de] 
> Sent: Thursday, 16 June 2005 18:33
> To: 'John Considine'
> Subject: RE: [R] Sweave and sideways
> 
> Sweave uses the fancyvrb package to display verbatim material. 
> Putting this in an environment can cause problems. Using some kind 
> of protection like putting it in a minipage environment can help.
> 
> The following modification of your code works for me 
> (using pdfLaTeX).
> 
> hth
> 
> D. Trenkler
> 
> \documentclass[a4paper]{article}
> \usepackage{Sweave}
> \usepackage{isorot}
> %\rotdriver{dvips}\clockwise
> \title{Sweave Example 1}
> \author{apologies to Friedrich Leisch }
> \begin{document}
> \maketitle
> \begin{sideways}
> \begin{minipage}{0.7\textwidth}
> 
> In this example we embed parts of the examples 
> from the \texttt{kruskal.test} help page into 
> a \LaTeX{} document:
> 
> <<>>=
> data(airquality)
> library(ctest)
> kruskal.test(Ozone~Month , data = airquality ) 
> @ 
> \end{minipage}
> \end{sideways} 
> \end{document}
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Considine
> Sent: Thursday, June 16, 2005 10:33 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Sweave and sideways
> 
> Hi there,
> 
> I'm rying to 'turn' an Schunk in an .Rnw file(Xemacs-21.4.13, 
> ESS-5.2.8,
> R-2.1, miktex-2.4.1705).  
> 
> Has anyone got the isorot package to work with Sweave?
> 
> JC
> 
> example test.Rnw:
> 
> \documentclass[a4paper]{article}
> \usepackage{Sweave}
> \usepackage{isorot}
> \rotdriver{dvips}
> \clockwise
> \title{Sweave Example 1}
> \author{apologies to Friedrich Leisch }
> \begin{document}
> \maketitle
> \begin{sideways}
> In this example we embed parts of the examples from the
> \texttt{kruskal.test} help page into a \LaTeX{} document:
> <<>>=
> data(airquality)
> library(ctest)
> kruskal.test(Ozone~Month , data = airquality )
> @
> \end{sideways}
> \end{document}
> 
> ...which I Sweave with:
> 
> >  Sweave("C:\\Documents and Settings\\John Considine\\test.Rnw")
> 
> ...producing:
> \documentclass[a4paper]{article}
> \usepackage{Sweave}
> \usepackage{isorot}
> \rotdriver{dvips}
> \clockwise
> \title{Sweave Example 1}
> \author{apologies to Friedrich Leisch }
> \begin{document}
> \maketitle
> \begin{sideways}
> In this example we embed parts of the examples from the
> \texttt{kruskal.test} help page into a \LaTeX{} document:
> \begin{Schunk}
> \begin{Sinput}
> > data(airquality)
> > library(ctest)
> > kruskal.test(Ozone ~ Month, data = airquality)
> \end{Sinput}
> \begin{Soutput}
> 
> 	Kruskal-Wallis rank sum test
> 
> data:  Ozone by Month 
> Kruskal-Wallis chi-squared = 29.2666, df = 4, p-value = 6.901e-06
> 
> 
> \end{Soutput}
> \end{Schunk}
> \end{sideways}
> \end{document}
> 
> Running it through latex gives the following log file:
> This is e-TeX, Version 3.141592-2.2 (MiKTeX 2.4) (preloaded 
> format=latex
> 2005.6.16)  16 JUN 2005 16:09
> entering extended mode
> **test.tex
> (test.tex
> LaTeX2e <2003/12/01>
> Babel <v3.8a> and hyphenation patterns for english, french, 
> german, ngerman,
> du
> mylang, nohyphenation, loaded.
> (C:\texmf\tex\latex\base\article.cls
> Document Class: article 2004/02/16 v1.4f Standard LaTeX document class
> (C:\texmf\tex\latex\base\size10.clo
> File: size10.clo 2004/02/16 v1.4f Standard LaTeX file (size option)
> )
> \c at part=\count79
> \c at section=\count80
> \c at subsection=\count81
> \c at subsubsection=\count82
> \c at paragraph=\count83
> \c at subparagraph=\count84
> \c at figure=\count85
> \c at table=\count86
> \abovecaptionskip=\skip41
> \belowcaptionskip=\skip42
> \bibindent=\dimen102
> ) (C:\texmf\tex\latex\sweave\Sweave.sty
> Package: Sweave 
> 
> (C:\texmf\tex\latex\base\ifthen.sty
> Package: ifthen 2001/05/26 v1.1c Standard LaTeX ifthen package (DPC)
> ) (C:\texmf\tex\latex\base\fontenc.sty
> Package: fontenc 2004/02/22 v1.99f Standard LaTeX package
> 
> (C:\texmf\tex\latex\base\t1enc.def
> File: t1enc.def 2004/02/22 v1.99f Standard LaTeX file
> LaTeX Font Info:    Redeclaring font encoding T1 on input line 43.
> )) (C:\texmf\tex\latex\graphics\graphicx.sty
> Package: graphicx 1999/02/16 v1.0f Enhanced LaTeX Graphics (DPC,SPQR)
> 
> (C:\texmf\tex\latex\graphics\keyval.sty
> Package: keyval 1999/03/16 v1.13 key=value parser (DPC)
> \KV at toks@=\toks14
> )
> (C:\texmf\tex\latex\graphics\graphics.sty
> Package: graphics 2001/07/07 v1.0n Standard LaTeX Graphics (DPC,SPQR)
>  (C:\texmf\tex\latex\graphics\trig.sty
> Package: trig 1999/03/16 v1.09 sin cos tan (DPC)
> ) (C:\texmf\tex\latex\00miktex\graphics.cfg
> File: graphics.cfg 2003/03/12 v1.1 MiKTeX 'graphics' configuration
> )
> Package graphics Info: Driver file: dvips.def on input line 80.
> 
> (C:\texmf\tex\latex\graphics\dvips.def
> File: dvips.def 1999/02/16 v3.0i Driver-dependant file (DPC,SPQR)
> ))
> \Gin at req@height=\dimen103
> \Gin at req@width=\dimen104
> ) (C:\texmf\tex\latex\ae\ae.sty
> Package: ae 2001/02/12 1.3 Almost European Computer Modern
> 
> (C:\texmf\tex\latex\base\fontenc.sty
> Package: fontenc 2004/02/22 v1.99f Standard LaTeX package
>  (C:\texmf\tex\latex\base\t1enc.def
> File: t1enc.def 2004/02/22 v1.99f Standard LaTeX file
> LaTeX Font Info:    Redeclaring font encoding T1 on input line 43.
> )
> LaTeX Font Info:    Try loading font information for T1+aer 
> on input line
> 100.
> 
> (C:\texmf\tex\latex\ae\t1aer.fd
> File: t1aer.fd 1997/11/16 Font definitions for T1/aer.
> ))) (C:\texmf\tex\latex\fancyvrb\fancyvrb.sty
> Package: fancyvrb 1998/07/17
> 
> Style option: `fancyvrb' v2.6, with DG/SPQR fixes <1998/07/17> (tvz)
> \FV at CodeLineNo=\count87
> \FV at InFile=\read1
> \FV at TabBox=\box26
> \c at FancyVerbLine=\count88
> \FV at StepNumber=\count89
> \FV at OutFile=\write3
> 
> No file fancyvrb.cfg.
> ) (C:\texmf\tex\latex\upquote\upquote.sty
> Package: upquote 2003/08/11 v1.1 Covington's upright-quote 
> modification to
> verb
> atim and verb
>  (C:\texmf\tex\latex\base\textcomp.sty
> Package: textcomp 2004/02/22 v1.99f Standard LaTeX package
> Package textcomp Info: Sub-encoding information:
> (textcomp)               5 = only ISO-Adobe without \textcurrency
> (textcomp)               4 = 5 + \texteuro
> (textcomp)               3 = 4 + \textohm
> (textcomp)               2 = 3 + \textestimated + \textcurrency
> (textcomp)               1 = TS1 - \textcircled - \t
> (textcomp)               0 = TS1 (full)
> (textcomp)             Font families with sub-encoding 
> setting implement
> (textcomp)             only a restricted character set as indicated.
> (textcomp)             Family '?' is the default used for 
> unknown fonts.
> (textcomp)             See the documentation for details.
> Package textcomp Info: Setting ? sub-encoding to TS1/1 on 
> input line 71.
> (C:\texmf\tex\latex\base\ts1enc.def
> File: ts1enc.def 2001/06/05 v3.0e (jk/car/fm) Standard LaTeX file
> )
> LaTeX Info: Redefining \oldstylenums on input line 266.
> Package textcomp Info: Setting cmr sub-encoding to TS1/0 on 
> input line 281.
> Package textcomp Info: Setting cmss sub-encoding to TS1/0 on 
> input line 282.
> Package textcomp Info: Setting cmtt sub-encoding to TS1/0 on 
> input line 283.
> Package textcomp Info: Setting cmvtt sub-encoding to TS1/0 on 
> input line
> 284.
> Package textcomp Info: Setting cmbr sub-encoding to TS1/0 on 
> input line 285.
> Package textcomp Info: Setting cmtl sub-encoding to TS1/0 on 
> input line 286.
> Package textcomp Info: Setting ccr sub-encoding to TS1/0 on 
> input line 287.
> Package textcomp Info: Setting ptm sub-encoding to TS1/4 on 
> input line 288.
> Package textcomp Info: Setting pcr sub-encoding to TS1/4 on 
> input line 289.
> Package textcomp Info: Setting phv sub-encoding to TS1/4 on 
> input line 290.
> Package textcomp Info: Setting ppl sub-encoding to TS1/3 on 
> input line 291.
> Package textcomp Info: Setting pag sub-encoding to TS1/4 on 
> input line 292.
> Package textcomp Info: Setting pbk sub-encoding to TS1/4 on 
> input line 293.
> Package textcomp Info: Setting pnc sub-encoding to TS1/4 on 
> input line 294.
> Package textcomp Info: Setting pzc sub-encoding to TS1/4 on 
> input line 295.
> Package textcomp Info: Setting bch sub-encoding to TS1/4 on 
> input line 296.
> Package textcomp Info: Setting put sub-encoding to TS1/5 on 
> input line 297.
> Package textcomp Info: Setting uag sub-encoding to TS1/5 on 
> input line 298.
> Package textcomp Info: Setting ugq sub-encoding to TS1/5 on 
> input line 299.
> Package textcomp Info: Setting ul8 sub-encoding to TS1/4 on 
> input line 300.
> Package textcomp Info: Setting ul9 sub-encoding to TS1/4 on 
> input line 301.
> Package textcomp Info: Setting augie sub-encoding to TS1/5 on 
> input line
> 302.
> Package textcomp Info: Setting dayrom sub-encoding to TS1/3 
> on input line
> 303.
> Package textcomp Info: Setting dayroms sub-encoding to TS1/3 
> on input line
> 304.
> 
> Package textcomp Info: Setting pxr sub-encoding to TS1/0 on 
> input line 305.
> Package textcomp Info: Setting pxss sub-encoding to TS1/0 on 
> input line 306.
> Package textcomp Info: Setting pxtt sub-encoding to TS1/0 on 
> input line 307.
> Package textcomp Info: Setting txr sub-encoding to TS1/0 on 
> input line 308.
> Package textcomp Info: Setting txss sub-encoding to TS1/0 on 
> input line 309.
> Package textcomp Info: Setting txtt sub-encoding to TS1/0 on 
> input line 310.
> Package textcomp Info: Setting futs sub-encoding to TS1/4 on 
> input line 311.
> Package textcomp Info: Setting futx sub-encoding to TS1/4 on 
> input line 312.
> Package textcomp Info: Setting futj sub-encoding to TS1/4 on 
> input line 313.
> Package textcomp Info: Setting hlh sub-encoding to TS1/3 on 
> input line 314.
> Package textcomp Info: Setting hls sub-encoding to TS1/3 on 
> input line 315.
> Package textcomp Info: Setting hlst sub-encoding to TS1/3 on 
> input line 316.
> Package textcomp Info: Setting hlct sub-encoding to TS1/5 on 
> input line 317.
> Package textcomp Info: Setting hlx sub-encoding to TS1/5 on 
> input line 318.
> Package textcomp Info: Setting hlce sub-encoding to TS1/5 on 
> input line 319.
> Package textcomp Info: Setting hlcn sub-encoding to TS1/5 on 
> input line 320.
> Package textcomp Info: Setting hlcw sub-encoding to TS1/5 on 
> input line 321.
> Package textcomp Info: Setting hlcf sub-encoding to TS1/5 on 
> input line 322.
> Package textcomp Info: Setting pplx sub-encoding to TS1/3 on 
> input line 323.
> Package textcomp Info: Setting pplj sub-encoding to TS1/3 on 
> input line 324.
> Package textcomp Info: Setting ptmx sub-encoding to TS1/4 on 
> input line 325.
> Package textcomp Info: Setting ptmj sub-encoding to TS1/4 on 
> input line 326.
> ))) (C:\texmf\tex\latex\isorot\isorot.sty
> Package: isorot 2000/02/15 v2.1 ISO rotation package
> \PWRFc at tracing=\count90
> 
> (C:\texmf\tex\latex\graphics\lscape.sty
> Package: lscape 2000/10/22 v3.01 Landscape Pages (DPC)
> )
> \c at r@tfl at t=\count91
> \rot at float@box=\box27
> )
> The .dvi file is to be processed by the dvips driver.
> Change rotdriver in the source accordingly if you do not have this.
> (C:\texmf\tex\latex\graphics\dvips.def
> File: dvips.def 1999/02/16 v3.0i Driver-dependant file (DPC,SPQR)
> ) (test.aux)
> LaTeX Font Info:    Checking defaults for OML/cmm/m/it on 
> input line 8.
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 8.
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 8.
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on 
> input line 8.
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on 
> input line 8.
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 8.
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    Checking defaults for TS1/cmr/m/n on input line 8.
> LaTeX Font Info:    Try loading font information for TS1+cmr 
> on input line
> 8.
> 
> (C:\texmf\tex\latex\base\ts1cmr.fd
> File: ts1cmr.fd 1999/05/25 v2.5h Standard LaTeX font definitions
> )
> LaTeX Font Info:    ... okay on input line 8.
> LaTeX Font Info:    External font `cmex10' loaded for size
> (Font)              <12> on input line 9.
> LaTeX Font Info:    External font `cmex10' loaded for size
> (Font)              <8> on input line 9.
> LaTeX Font Info:    External font `cmex10' loaded for size
> (Font)              <6> on input line 9.
> LaTeX Font Info:    Try loading font information for T1+aett 
> on input line
> 12.
>  (C:\texmf\tex\latex\ae\t1aett.fd
> File: t1aett.fd 1997/11/16 Font definitions for T1/aett.
> )
> LaTeX Font Info:    External font `cmex10' loaded for size
> (Font)              <7> on input line 12.
> LaTeX Font Info:    External font `cmex10' loaded for size
> (Font)              <5> on input line 12.
> 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.14 \begin{Sinput}
>                    
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.14 \begin{Sinput}
>                    
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.18 \end{Sinput}
>                  
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.18 \end{Sinput}
>                  
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.19 \begin{Soutput}
>                     
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.19 \begin{Soutput}
>                     
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.27 \end{Soutput}
>                   
> ? 
> 
> ! LaTeX Error: Something's wrong--perhaps a missing \item.
> 
> See the LaTeX manual or LaTeX Companion for explanation.
> Type  H <return>  for immediate help.
>  ...                                              
>                                                   
> l.27 \end{Soutput}
>                   
> ? 
> [1
> 
> ]
> Overfull \vbox (3383.85861pt too high) has occurred while 
> \output is active
> []
> 
> 
> [2] (test.aux) ) 
> Here is how much of TeX's memory you used:
>  1890 strings out of 95898
>  23049 string characters out of 1195288
>  64391 words of memory out of 1067656
>  4842 multiletter control sequences out of 35000
>  13777 words of font info for 30 fonts, out of 500000 for 1000
>  14 hyphenation exceptions out of 607
>  35i,6n,21p,255b,187s stack positions out of 
> 1500i,500n,5000p,200000b,32768s
> 
> Output written on test.dvi (2 pages, 1192 bytes).
> 
> JC Considine
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From jjmichael at comcast.net  Thu Jun 16 15:47:29 2005
From: jjmichael at comcast.net (Jacob Michaelson)
Date: Thu, 16 Jun 2005 07:47:29 -0600
Subject: [R] heatmap aspect ratio
Message-ID: <3D57F1CF-54ED-4E05-8963-A9AE78E3961E@comcast.net>

Hi all,

Does anyone know of a fairly easy way to "stretch" a heatmap  
vertically?  I've got 42 arrays and would like to be able to see as  
many significant genes as possible (right now I can only get 50 genes  
with it still being readable).  In some comparisons there are several  
hundred significant genes.

I've fiddled with the "asp" argument, but that doesn't give the  
results I'm looking for -- only scales the images, not the dendrograms.

Is there any way to make the heatmap rectangular rather than square  
without hacking the heatmap function itself (which is where I'm  
headed next)?

Thanks in advance,

Jake



From buser at stat.math.ethz.ch  Thu Jun 16 15:52:03 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Thu, 16 Jun 2005 15:52:03 +0200
Subject: [R] abbreviate
In-Reply-To: <971536df0506151631724ea494@mail.gmail.com>
References: <3f87cc6d05061510591bb2a807@mail.gmail.com>
	<Pine.LNX.4.61.0506151929200.12352@gannet.stats>
	<1118860928.4423.7.camel@localhost.localdomain>
	<971536df0506151631724ea494@mail.gmail.com>
Message-ID: <17073.33795.555125.386908@stat.math.ethz.ch>

Down to 17 if we carry on your idea using the matching property
on the left hand side, too. :-)

p$h=pmax(p$h,p$s)


Gabor Grothendieck writes:
 > Agree that this definitely should be pursued. :)  In fact,
 > we can shave off several keystrokes by 
 > 
 > - replacing p[[1]] with p[1] on the left hand side
 > - p$high and p$settle with p$h and p$s on the right hand
 >   side (which makes use of the matching property of $)
 > - '=' instead of '<-'
 > - remove all the spaces
 > 
 > This gets it down to 18 characters:
 > 
 > p[1]=pmax(p$h,p$s)
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Thu Jun 16 16:00:08 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Jun 2005 07:00:08 -0700 (PDT)
Subject: [R] Error using newdata argument in survfit
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE02043804@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE02043804@msgmarsta01.bio.dfo.ca>
Message-ID: <Pine.A41.4.61b.0506160658390.57662@homer08.u.washington.edu>

On Thu, 16 Jun 2005, Hanke, Alex wrote:

> Hi Brian,
> The factor Prior.f has 5 levels (1,2,3,4,5) which coxph deals with by
> creating 4 dummy variables coded with 1 or zero. That's what I see when I
> look at fit$x.
> fit$x[1:5,]
>   Week LagAOO factor(Prior.f)2 factor(Prior.f)3 factor(Prior.f)4
> 31   22      0                0                0                0
> 32   22      0                0                0                0
> 33   22      2                0                0                0
> 34   22      3                0                0                0
> 35   22      2                0                0                0
>   factor(Prior.f)5
> 31                0
> 32                0
> 33                0
> 34                0
> 35                0
> I have played with the formula a bit adding a term at a time and then
> checking to see if I can produce the survival curves for pseudo cohorts. I
> get as far as the Prior.f term and am successful if I treat it as a
> continuous variable. If I introduce it as a factor and assume it wants four
> dummy variables as above I get the variable lengths error. If I represent
> the term with one variable:
> survfit(fit,list(Week=c(15,15),LagAOO=c(0,0),Prior.f=c(1,2)))
> I get:
> Error in x2 %*% coef : non-conformable arguments

Yes, but it wants a factor with *5* levels.  Try
survfit(fit,list(Week=c(15,15),LagAOO=c(0,0),Prior.f=factor(c(1,2),levels=1:5)))

 	-thomas


> Which is a nice change but still short of knowing what is going on.
> Regards
> Alex
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: June 15, 2005 4:24 PM
> To: Hanke, Alex
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Error using newdata argument in survfit
>
> You appear to have a coding for prior.f in newdata rather than the factor
> itself.
>
> It's a bit hard to be sure when we don't have data8 to compare with.
>
> On Wed, 15 Jun 2005, Hanke, Alex wrote:
>
>> Dear R-helpers,
>> To get curves for a pseudo cohort other than the one centered at the mean
> of
>> the covariates, I have been trying to use the newdata argument to survfit
>> with no success. Here is my model statement, the newdata and the ensuing
>> error. What am I doing wrong?
>>
>>> summary(fit)
>> Call:
>> coxph(formula = Surv(Start, Stop, Event, type = "counting") ~
>>    Week + LagAOO + Prior.f + cluster(interaction(Station, Year)),
>>    data = data8, method = "breslow", x = T, y = T)
>>
>>  n= 1878
>>            coef exp(coef) se(coef) robust se     z       p
>> Week     0.00582      1.01   0.0323    0.0239 0.244 8.1e-01
>> LagAOO   0.71929      2.05   0.1238    0.1215 5.918 3.3e-09
>> Prior.f2 0.12927      1.14   0.4402    0.4025 0.321 7.5e-01
>> Prior.f3 0.79082      2.21   0.5484    0.4460 1.773 7.6e-02
>> Prior.f4 2.04189      7.71   0.6008    0.4685 4.358 1.3e-05
>> Prior.f5 1.20450      3.34   0.6423    0.5481 2.198 2.8e-02
>>
>>         exp(coef) exp(-coef) lower .95 upper .95
>> Week          1.01      0.994     0.960      1.05
>> LagAOO        2.05      0.487     1.618      2.61
>> Prior.f2      1.14      0.879     0.517      2.50
>> Prior.f3      2.21      0.453     0.920      5.29
>> Prior.f4      7.71      0.130     3.076     19.30
>> Prior.f5      3.34      0.300     1.139      9.76
>>
>> Rsquare= 0.047   (max possible= 0.25 )
>> Likelihood ratio test= 91  on 6 df,   p=0
>> Wald test            = 209  on 6 df,   p=0
>> Score (logrank) test = 142  on 6 df,   p=0,   Robust = 17.4  p=0.00803
>>
>>  (Note: the likelihood ratio and score tests assume independence of
>>     observations within a cluster, the Wald and robust score tests do
> not).
>>>
>> newdat
>>      Week   LagAOO Prior.f2 Prior.f3 Prior.f4 Prior.f5
>> 1 17.55218 1.191693        1        0        0        0
>> 2 17.55218 1.191693        0        0        0        0
>>
>>> survfit(fit,newdata=newdat)
>> Error in model.frame(formula, rownames, variables, varnames, extras,
>> extranames,  :
>>        variable lengths differ
>> In addition: Warning message:
>> 'newdata' had 2 rows but variable(s) found have 1878 rows
>>
>> Regards,
>> Alex
>>
>>
>> Alex Hanke
>> Department of Fisheries and Oceans
>> St. Andrews Biological Station
>> 531 Brandy Cove Road
>> St. Andrews, NB
>> Canada
>> E5B 2L9
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From dmbates at gmail.com  Thu Jun 16 16:21:27 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 16 Jun 2005 09:21:27 -0500
Subject: [R] identical results with PQL and Laplace options in lmer
	function (package lme4)
In-Reply-To: <20050616184847.75b08eee@localhost.localdomain>
References: <42B138DD.5030203@cirad.fr>
	<20050616184847.75b08eee@localhost.localdomain>
Message-ID: <40e66e0b05061607212e9e6f0e@mail.gmail.com>

It's a dumb mistake in the code.  I missed a factor of 2 when
converting from deviance to log-likelihood in one place. I'll upload a
new version of the Matrix package.  (It happens that all of the code
for lmer is in the Matrix package now so there is no need to update
the lme4 package.)

On 6/16/05, ronggui <0034058 at fudan.edu.cn> wrote:
> i am aware of this but i don't know why.
> 
> On Thu, 16 Jun 2005 10:31:25 +0200
> Emmanuel Tillard <tillard at cirad.fr> wrote:
> 
> > Dear R users
> >
> > I encounter a problem when i perform a generalized linear mixed model (binary data) with the lmer function (package lme4)
> > with R 2.1.0 on windows XP and the latest version of package "lme4" (0.96-1) and "matrix" (0.96-2)
> > both options "PQL" and "Laplace" for the method argument in lmer function gave me the same results (random and fixed effects estimates, standard error and p.values). However, Loglikelihood and deviance are different.
> >
> > here is an example reproduced with the bacteria data set available in the MASS package:
> >
> > library(lme4)
> > library(MASS)
> >
> > data(bacteria)
> > bacteria$week2 <- as.factor(ifelse(bacteria$week <=2, 0, 1))
> > model.PQL <- lmer(y ~ trt + week2 + (1 | ID), family = binomial, data  = bacteria, method ="PQL")
> > model.Laplace <- lmer(y ~ trt + week2 + (1 | ID), family = binomial, data = bacteria, method ="Laplace")
> >
> > model.PQL
> > model.Laplace
> >
> > > model.PQL
> > Generalized linear mixed model fit using PQL
> > Formula: y ~ trt + week2 + (1 | ID)
> >    Data: bacteria
> >  Family: binomial(logit link)
> >      AIC      BIC   logLik deviance
> >  152.443 138.8685 -80.2215  160.443
> > Random effects:
> >      Groups        Name    Variance    Std.Dev.
> >          ID (Intercept)      3.2721      1.8089
> > # of obs: 220, groups: ID, 50
> >
> > Estimated scale (compare to 1)  0.7800484
> >
> > Fixed effects:
> >             Estimate Std. Error z value  Pr(>|z|)
> > (Intercept)  3.41227    0.65884  5.1792 2.228e-07 ***
> > trtdrug     -1.24743    0.81841 -1.5242 0.1274555
> > trtdrug+    -0.75440    0.82009 -0.9199 0.3576229
> > week21      -1.60737    0.45527 -3.5306 0.0004146 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > > model.Laplace
> > Generalized linear mixed model fit using Laplace
> > Formula: y ~ trt + week2 + (1 | ID)
> >    Data: bacteria
> >  Family: binomial(logit link)
> >       AIC      BIC    logLik deviance
> >  304.9488 291.3743 -156.4744 312.9488
> > Random effects:
> >      Groups        Name    Variance    Std.Dev.
> >          ID (Intercept)      3.2721      1.8089
> > # of obs: 220, groups: ID, 50
> >
> > Estimated scale (compare to 1)  0.7800484
> >
> > Fixed effects:
> >             Estimate Std. Error z value  Pr(>|z|)
> > (Intercept)  3.41227    0.65884  5.1792 2.228e-07 ***
> > trtdrug     -1.24743    0.81841 -1.5242 0.1274555
> > trtdrug+    -0.75440    0.82009 -0.9199 0.3576229
> > week21      -1.60737    0.45527 -3.5306 0.0004146 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > is anybody else aware of this? or did I forget something important ?
> >
> > Many thanks for your help.
> >
> > --
> > Emmanuel Tillard
> > Veterinaire
> > CIRAD-EMVT
> > Unite de recherche 18
> >
> > UMR868 Elevage des Ruminants en Regions Chaudes (ERRC)
> > Campus ENSA-INRA
> > 2 place Viala
> > 34060 Montpellier cedex 1
> >
> > tel:  0499612265 (fixe)
> >       0633850598 (gsm)
> > fax:  0467545694
> > e-mail: tillard at cirad.fr
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Steven.Murdoch at cl.cam.ac.uk  Thu Jun 16 16:26:56 2005
From: Steven.Murdoch at cl.cam.ac.uk (Steven J. Murdoch)
Date: Thu, 16 Jun 2005 15:26:56 +0100
Subject: [R] Drawing information-rich, Tufte style scatterplots and axes
Message-ID: <20050616142656.GA11229@cl.cam.ac.uk>

I have been working on using R to draw information-rich graphs based
on suggestions from "The Visual Display of Quantitative Information"
by Edward Tufte.  Thanks to the members of the list who have helped me
with this.

I have now produced the first version of this code, which I used to
generate graphs for a paper I recently co-authored. There are
descriptions of the new functions, examples of their use and links for
downloading the source here:

 http://www.cl.cam.ac.uk/users/sjm217/projects/graphics/fancyaxis.html

If anyone has any comments or suggestions I would be very interested.

Thank you,
Steven Murdoch.

-- 
w: http://www.cl.cam.ac.uk/users/sjm217/



From sdavis2 at mail.nih.gov  Thu Jun 16 16:36:35 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 16 Jun 2005 10:36:35 -0400
Subject: [R] heatmap aspect ratio
In-Reply-To: <3D57F1CF-54ED-4E05-8963-A9AE78E3961E@comcast.net>
References: <3D57F1CF-54ED-4E05-8963-A9AE78E3961E@comcast.net>
Message-ID: <b57966d6733b99e3f651f0363d0ebab1@mail.nih.gov>


On Jun 16, 2005, at 9:47 AM, Jacob Michaelson wrote:

> Hi all,
>
> Does anyone know of a fairly easy way to "stretch" a heatmap
> vertically?  I've got 42 arrays and would like to be able to see as
> many significant genes as possible (right now I can only get 50 genes
> with it still being readable).  In some comparisons there are several
> hundred significant genes.
>
> I've fiddled with the "asp" argument, but that doesn't give the
> results I'm looking for -- only scales the images, not the dendrograms.
>
> Is there any way to make the heatmap rectangular rather than square
> without hacking the heatmap function itself (which is where I'm
> headed next)?

Jacob,

I use heatmap.2 from the gplots package (part of gregmisc bundle).  It 
is more "customizable" than heatmap.

Sean



From tlumley at u.washington.edu  Thu Jun 16 17:01:08 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Jun 2005 08:01:08 -0700 (PDT)
Subject: [R] Survey - Cluster Sampling
In-Reply-To: <42B17703.8080606@t-online.de>
References: <42B17703.8080606@t-online.de>
Message-ID: <Pine.A41.4.61b.0506160704550.57662@homer08.u.washington.edu>

On Thu, 16 Jun 2005, Mark Hempelmann wrote:

> Dear WizaRds,
>
> 	I am struggling to compute correctly a cluster sampling design. I want
> to do one stage clustering with different parametric changes:
>
> Let M be the total number  of clusters in the population, and m the
> number sampled. Let N be the total of elements in the population and n
> the number sampled. y are the values sampled. This is my example data:
>
> clus1 <- data.frame(cluster=c(1,1,1,2,2,2,3,3,3), id=seq(1:3,3),
> weight=rep(72/9,9), nl=rep(3,9), Nl=rep(3,9), N=rep(72,9), y=c(23,33,77,
> 25,35,74, 27,37,72) )
>
> 1. Let M=m=3 and N=n=9. Then:
>
> dclus1<-svydesign(id=~cluster,  data=clus1)
> svymean(~y, dclus1)
>
>     mean    SE
> y 44.778 0.294, the unweighted mean, assuming equal probability in the
> clusters. ok.

Yes.

> 2. Let M=23, m=3 and N=72, n=9, then I am unable to use svydesign correctly:
>
> dclus2<-svydesign(id=~cluster,  data=clus1, fpc=~N)
> svymean(~y, dclus2)
>
>     mean     SE
> y 44.778 0.2878, but it should be 23/72 * 1/3(133+134+136)=42.91, since
> I have to include the total number of clusters/total population M/N into
> the estimator. How can I include the information of the total number of
> clusters?

The fpc term should be the total number of clusters, so 23 rather than 72.
clus1$M<-rep(23,9)
dclus2a<-svydesign(id=~cluster, data=clus1, fpc=~M)
svymean(~y, dclus2a)

Now, this still gives 44.778, because each observation still has the same 
weight.  It describes a one-stage cluster sampling design where each 
cluster has only three elements.  This is an equal-probability sampling 
design. Any equal-probability sampling design will give the same estimated 
mean.

If your design was to take a simple random sample of three clusters and 
then take all the elements in each cluster then dclus2a is giving the 
correct mean (well, the one I wanted it to give). Estimates of the 
population total will be different, but not the mean.

Your expected estimate of the mean is also a reasonable one. In survey 
statistics there is often more than one reasonable estimator even for 
something as simple as the mean.  My estimator is 
sum(weights*y)/sum(weights), which has some practical advantages: it is 
easy to generalise to more complex designs (including things like 
post-stratification), it can be computed without knowing the sampling 
design (which is important when using replicate weights to compute 
variances), it is the definition of the mean that agrees with linear 
regression models, and it is what Stata uses, making it easier to compare 
results.

Your estimator uses the expected value of the denominator rather than the 
observed value. This probably implies that your estimator is 
design-unbiased and mine isn't.  Since there aren't design-unbiased 
estimators for most statistics more complicated than the mean I don't 
worry so much about it.


You might also have had a sampling design where you took a simple random 
sample of three clusters and then up to three elements from each cluster.
   dclus2b<-svydesign(id=~cluster+id, fpc=~M+nl, data=clus1)
This gives the same mean as dclus2a, because in fact you sampled 100% of 
each sampled cluster.


> 3. How do I work with weights correctly? I understand that weights imply
>  inverse probability weighting 1/p with p=n/N in simple sampling, in
> our case 72/9=8, because I sample 9 units out of a total population of
> 72. Again, I couldn't tell survey the number of total clusters M. So:
>
> dclus3<-svydesign(id=~cluster,  weights=~weight, data=clus1, fpc=~N)
> svymean(~y, dclus3)
>
>     mean     SE
> y 44.778 0.2878, still exactly the same numbers, although I provided the
> weights. What am I doing wrong?

Again, fpc should be M rather than N. The help page says that the relevant 
population size is in "sampling units" (ie, clusters). It used to say PSUs 
before the package was extended to handle multistage fpcs, which was 
probably clearer but now wouldn't be true.

Apart from that you aren't doing anything wrong. The mean should still be 
the same as the unweighted mean because you are giving each observation 
the same weight. And it is.

The total won't be the same as dclus2a and dclus2b, because you are now 
telling R the population size in elements as well as in PSUs.


 	-thomas



From laura.devendictis at uniroma1.it  Fri Jun 17 17:10:58 2005
From: laura.devendictis at uniroma1.it (Laura De Vendictis)
Date: Fri, 17 Jun 2005 17:10:58 +0200
Subject: [R] ccf
Message-ID: <42B2E802.1070001@uniroma1.it>

Hello group,

For my research I should calculate the cross-correlation  between two 
time series.
I don't know if the function ccf can calculate this with series that 
have NA values.
e.g. temperature:

15.5
NA
12.3
10.0
NA
14.2
15,3
....

Can you help me?
Thank you very much!

Laura



From d.firth at warwick.ac.uk  Thu Jun 16 17:22:31 2005
From: d.firth at warwick.ac.uk (David Firth)
Date: Thu, 16 Jun 2005 16:22:31 +0100
Subject: [R] mu^2(1-mu)^2 variance function for GLM
In-Reply-To: <s2b1721d.097@liberator.csv.warwick.ac.uk>
References: <s2b1721d.097@liberator.csv.warwick.ac.uk>
Message-ID: <f30eabe6d0267dd243fdbb072520ce4a@warwick.ac.uk>

Dear Henric

I do not have a ready stock of other examples, but I do have my own 
version of a family function for this, reproduced below.  It differs 
from yours (apart from being a regular family function rather than 
using a modified "quasi") in the definition of deviance residuals.  
These necessarily involve an arbitrary constant (see McCullagh and 
Nelder, 1989, p330); in my function that arbitrariness is in the choice 
eps <- 0.0005.  I don't think the deviance contributions as you 
specified in your code below will have the right derivative (with 
respect to mu) for observations where y=0 or y=1.

Anyway, this at least gives you some kind of check.  I hope it helps.  
This function will be part of a new package which Heather Turner and I 
will submit to CRAN in a few days' time.  Please do let me know if you 
find any problems with it.

Here is my "wedderburn" family function:

"wedderburn" <-
     function (link = "logit")
{
     linktemp <- substitute(link)
     if (!is.character(linktemp)) {
         linktemp <- deparse(linktemp)
         if (linktemp == "link")
             linktemp <- eval(link)
     }
     if (any(linktemp == c("logit", "probit", "cloglog")))
         stats <- make.link(linktemp)
     else stop(paste(linktemp,
                     "link not available for wedderburn quasi-family;",
                     "available links are",
                     "\"logit\", \"probit\" and \"cloglog\""))
     variance <- function(mu)  mu^2 * (1-mu)^2
     validmu <- function(mu) {
         all(mu > 0) && all(mu < 1)}
     dev.resids <- function(y, mu, wt){
         eps <-  0.0005
         2 * wt * (y/mu + (1 - y)/(1 - mu) - 2 +
                   (2 * y - 1) * log((y + eps)*(1 - mu)/((1- y + eps) * 
mu)))
     }
     aic <- function(y, n, mu, wt, dev) NA
     initialize <- expression({
         if (any(y < 0 | y > 1)) stop(paste(
                    "Values for the wedderburn family must be in [0,1]"))
         n <- rep.int(1, nobs)
         mustart <- (y + 0.1)/1.2
     })
     structure(list(family = "wedderburn",
                    link = linktemp,
                    linkfun = stats$linkfun,
                    linkinv = stats$linkinv,
                    variance = variance,
                    dev.resids = dev.resids,
                    aic = aic,
                    mu.eta = stats$mu.eta,
                    initialize = initialize,
                    validmu = validmu,
                    valideta = stats$valideta),
               class = "family")
}

Best wishes,
David
http://www.warwick.ac.uk/go/dfirth

On 16 Jun 2005, at 09:27, Henric Nilsson wrote:

> Dear list,
>
> I'm trying to mimic the analysis of Wedderburn (1974) as cited by
> McCullagh and Nelder (1989) on p.328-332. This is the leaf-blotch on
> barley example, and the data is available in the `faraway' package.
>
> Wedderburn suggested using the variance function mu^2(1-mu)^2. This
> variance function isn't readily available in R's `quasi' family object,
> but it seems to me that the following definition could be used:
>
> }, "mu^2(1-mu)^2" = {
>      variance <- function(mu) mu^2 * (1 - mu)^2
>      validmu <- function(mu) all(mu > 0) && all(mu < 1)
>      dev.resids <- function(y, mu, wt) 2 * wt * ((2 * y - 1) *
>          (log(ifelse(y == 0, 1, y/mu)) - log(ifelse(y == 1, 1,
>          (1 - y)/(1 - mu)))) - 2 + y/mu + (1 - y)/(1 - mu))
>
> I've modified the `quasi' function accordingly (into `quasi2' given
> below) and my results are very much in line with the ones cited by
> McCullagh and Nelder on p.330-331:
>
>> data(leafblotch, package = "faraway")
>> summary(fit <- glm(blotch ~ site + variety,
> +         family = quasi2(link = "logit", variance = "mu^2(1-mu)^2"),
> +         data = leafblotch))
>
> Call:
> glm(formula = blotch ~ site + variety, family = quasi2(link = "logit",
>      variance = "mu^2(1-mu)^2"), data = leafblotch)
>
> Deviance Residuals:
>       Min        1Q    Median        3Q       Max
> -3.23175  -0.65385  -0.09426   0.46946   1.97152
>
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) -7.92253    0.44463 -17.818  < 2e-16 ***
> site2        1.38308    0.44463   3.111  0.00268 **
> site3        3.86013    0.44463   8.682 8.18e-13 ***
> site4        3.55697    0.44463   8.000 1.53e-11 ***
> site5        4.10841    0.44463   9.240 7.48e-14 ***
> site6        4.30541    0.44463   9.683 1.13e-14 ***
> site7        4.91811    0.44463  11.061  < 2e-16 ***
> site8        5.69492    0.44463  12.808  < 2e-16 ***
> site9        7.06762    0.44463  15.896  < 2e-16 ***
> variety2    -0.46728    0.46868  -0.997  0.32210
> variety3     0.07877    0.46868   0.168  0.86699
> variety4     0.95418    0.46868   2.036  0.04544 *
> variety5     1.35276    0.46868   2.886  0.00514 **
> variety6     1.32859    0.46868   2.835  0.00595 **
> variety7     2.34066    0.46868   4.994 3.99e-06 ***
> variety8     3.26268    0.46868   6.961 1.30e-09 ***
> variety9     3.13556    0.46868   6.690 4.10e-09 ***
> variety10    3.88736    0.46868   8.294 4.33e-12 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> (Dispersion parameter for quasi family taken to be 0.9884755)
>
>      Null deviance: 339.488  on 89  degrees of freedom
> Residual deviance:  71.961  on 72  degrees of freedom
> AIC: NA
>
> Number of Fisher Scoring iterations: 18
>
>
> Also, the plot of the Pearson residuals against the linear predictor
>
>> plot(residuals(fit, type = "pearson") ~ fit$linear.predictors)
>> abline(h = 0, lty = 2)
>
> results in a plot that, to my eyes at least, is very close to Fig. 9.2
> on p. 332.
>
> However, I can't seem to find any other published examples using this
> variance function. I'd really like to verify that my code above is
> working before applying it to real data sets. Can anybody help?
>
> Thanks,
> Henric
> - - - - -
> quasi2 <- function (link = "identity", variance = "constant")
> {
>      linktemp <- substitute(link)
>      if (is.expression(linktemp) || is.call(linktemp))
>          linktemp <- link
>      else if (!is.character(linktemp))
>          linktemp <- deparse(linktemp)
>      if (is.character(linktemp))
>          stats <- make.link(linktemp)
>      else stats <- linktemp
>      variancetemp <- substitute(variance)
>      if (!is.character(variancetemp)) {
>          variancetemp <- deparse(variancetemp)
>          if (linktemp == "variance")
>              variancetemp <- eval(variance)
>      }
>      switch(variancetemp, constant = {
>          variance <- function(mu) rep.int(1, length(mu))
>          dev.resids <- function(y, mu, wt) wt * ((y - mu)^2)
>          validmu <- function(mu) TRUE
>      }, "mu(1-mu)" = {
>          variance <- function(mu) mu * (1 - mu)
>          validmu <- function(mu) all(mu > 0) && all(mu < 1)
>          dev.resids <- function(y, mu, wt) 2 * wt * (y * log(ifelse(y 
> ==
>              0, 1, y/mu)) + (1 - y) * log(ifelse(y == 1, 1, (1 -
>              y)/(1 - mu))))
>      }, "mu^2(1-mu)^2" = {
>          variance <- function(mu) mu^2 * (1 - mu)^2
>          validmu <- function(mu) all(mu > 0) && all(mu < 1)
>          dev.resids <- function(y, mu, wt) 2 * wt * ((2 * y - 1) *
>              (log(ifelse(y == 0, 1, y/mu)) - log(ifelse(y == 1, 1,
>              (1 - y)/(1 - mu)))) - 2 + y/mu + (1 - y)/(1 - mu))
>      }, mu = {
>          variance <- function(mu) mu
>          validmu <- function(mu) all(mu > 0)
>          dev.resids <- function(y, mu, wt) 2 * wt * (y * log(ifelse(y 
> ==
>              0, 1, y/mu)) - (y - mu))
>      }, "mu^2" = {
>          variance <- function(mu) mu^2
>          validmu <- function(mu) all(mu > 0)
>          dev.resids <- function(y, mu, wt) pmax(-2 * wt * 
> (log(ifelse(y ==
>              0, 1, y)/mu) - (y - mu)/mu), 0)
>      }, "mu^3" = {
>          variance <- function(mu) mu^3
>          validmu <- function(mu) all(mu > 0)
>          dev.resids <- function(y, mu, wt) wt * ((y - mu)^2)/(y *
>              mu^2)
>      }, stop(gettextf("'variance' \"%s\" is invalid: possible values 
> are
> \"mu(1-mu)\", \"mu^2(1-mu)^2\", \"mu\", \"mu^2\", \"mu^3\" and
> \"constant\"",
>          variancetemp), domain = NA))
>      initialize <- expression({
>          n <- rep.int(1, nobs)
>          mustart <- y + 0.1 * (y == 0)
>      })
>      aic <- function(y, n, mu, wt, dev) NA
>      structure(list(family = "quasi", link = linktemp, linkfun =
> stats$linkfun,
>          linkinv = stats$linkinv, variance = variance, dev.resids =
> dev.resids,
>          aic = aic, mu.eta = stats$mu.eta, initialize = initialize,
>          validmu = validmu, valideta = stats$valideta, varfun =
> variancetemp),
>          class = "family")
> }
>
>
>
> <Header>
>



From rpeng at jhsph.edu  Thu Jun 16 17:41:20 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 16 Jun 2005 11:41:20 -0400
Subject: [R] ccf
In-Reply-To: <42B2E802.1070001@uniroma1.it>
References: <42B2E802.1070001@uniroma1.it>
Message-ID: <42B19DA0.1000808@jhsph.edu>

You might try passing 'na.contiguous' to the 'na.action' argument.

-roger

Laura De Vendictis wrote:
> Hello group,
> 
> For my research I should calculate the cross-correlation  between two 
> time series.
> I don't know if the function ccf can calculate this with series that 
> have NA values.
> e.g. temperature:
> 
> 15.5
> NA
> 12.3
> 10.0
> NA
> 14.2
> 15,3
> ....
> 
> Can you help me?
> Thank you very much!
> 
> Laura
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From HankeA at mar.dfo-mpo.gc.ca  Thu Jun 16 16:49:58 2005
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 16 Jun 2005 11:49:58 -0300
Subject: [R] Error using newdata argument in survfit
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE0204387E@msgmarsta01.bio.dfo.ca>

Thanks to Thomas, Brain and Ales,
Whose advice led me to the solution. I actually had a second problem
preventing Thomas' solution :
survfit(fit,list(Week=c(15,15),LagAOO=c(0,0),Prior.f=factor(c(1,2),levels=1:
5)))
from working. In the model statement I create a factor from Prior.f via
factor(Prior.f). Rather one should predefine the factor variable
Prior.f<-factor(Prior.f) and use that term in the model and then Thomas'
solution works fine.
Alex

-----Original Message-----
From: Thomas Lumley [mailto:tlumley at u.washington.edu] 
Sent: June 16, 2005 11:00 AM
To: Hanke, Alex
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Error using newdata argument in survfit

On Thu, 16 Jun 2005, Hanke, Alex wrote:

> Hi Brian,
> The factor Prior.f has 5 levels (1,2,3,4,5) which coxph deals with by
> creating 4 dummy variables coded with 1 or zero. That's what I see when I
> look at fit$x.
> fit$x[1:5,]
>   Week LagAOO factor(Prior.f)2 factor(Prior.f)3 factor(Prior.f)4
> 31   22      0                0                0                0
> 32   22      0                0                0                0
> 33   22      2                0                0                0
> 34   22      3                0                0                0
> 35   22      2                0                0                0
>   factor(Prior.f)5
> 31                0
> 32                0
> 33                0
> 34                0
> 35                0
> I have played with the formula a bit adding a term at a time and then
> checking to see if I can produce the survival curves for pseudo cohorts. I
> get as far as the Prior.f term and am successful if I treat it as a
> continuous variable. If I introduce it as a factor and assume it wants
four
> dummy variables as above I get the variable lengths error. If I represent
> the term with one variable:
> survfit(fit,list(Week=c(15,15),LagAOO=c(0,0),Prior.f=c(1,2)))
> I get:
> Error in x2 %*% coef : non-conformable arguments

Yes, but it wants a factor with *5* levels.  Try
survfit(fit,list(Week=c(15,15),LagAOO=c(0,0),Prior.f=factor(c(1,2),levels=1:
5)))

 	-thomas


> Which is a nice change but still short of knowing what is going on.
> Regards
> Alex
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: June 15, 2005 4:24 PM
> To: Hanke, Alex
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] Error using newdata argument in survfit
>
> You appear to have a coding for prior.f in newdata rather than the factor
> itself.
>
> It's a bit hard to be sure when we don't have data8 to compare with.
>
> On Wed, 15 Jun 2005, Hanke, Alex wrote:
>
>> Dear R-helpers,
>> To get curves for a pseudo cohort other than the one centered at the mean
> of
>> the covariates, I have been trying to use the newdata argument to survfit
>> with no success. Here is my model statement, the newdata and the ensuing
>> error. What am I doing wrong?
>>
>>> summary(fit)
>> Call:
>> coxph(formula = Surv(Start, Stop, Event, type = "counting") ~
>>    Week + LagAOO + Prior.f + cluster(interaction(Station, Year)),
>>    data = data8, method = "breslow", x = T, y = T)
>>
>>  n= 1878
>>            coef exp(coef) se(coef) robust se     z       p
>> Week     0.00582      1.01   0.0323    0.0239 0.244 8.1e-01
>> LagAOO   0.71929      2.05   0.1238    0.1215 5.918 3.3e-09
>> Prior.f2 0.12927      1.14   0.4402    0.4025 0.321 7.5e-01
>> Prior.f3 0.79082      2.21   0.5484    0.4460 1.773 7.6e-02
>> Prior.f4 2.04189      7.71   0.6008    0.4685 4.358 1.3e-05
>> Prior.f5 1.20450      3.34   0.6423    0.5481 2.198 2.8e-02
>>
>>         exp(coef) exp(-coef) lower .95 upper .95
>> Week          1.01      0.994     0.960      1.05
>> LagAOO        2.05      0.487     1.618      2.61
>> Prior.f2      1.14      0.879     0.517      2.50
>> Prior.f3      2.21      0.453     0.920      5.29
>> Prior.f4      7.71      0.130     3.076     19.30
>> Prior.f5      3.34      0.300     1.139      9.76
>>
>> Rsquare= 0.047   (max possible= 0.25 )
>> Likelihood ratio test= 91  on 6 df,   p=0
>> Wald test            = 209  on 6 df,   p=0
>> Score (logrank) test = 142  on 6 df,   p=0,   Robust = 17.4  p=0.00803
>>
>>  (Note: the likelihood ratio and score tests assume independence of
>>     observations within a cluster, the Wald and robust score tests do
> not).
>>>
>> newdat
>>      Week   LagAOO Prior.f2 Prior.f3 Prior.f4 Prior.f5
>> 1 17.55218 1.191693        1        0        0        0
>> 2 17.55218 1.191693        0        0        0        0
>>
>>> survfit(fit,newdata=newdat)
>> Error in model.frame(formula, rownames, variables, varnames, extras,
>> extranames,  :
>>        variable lengths differ
>> In addition: Warning message:
>> 'newdata' had 2 rows but variable(s) found have 1878 rows
>>
>> Regards,
>> Alex
>>
>>
>> Alex Hanke
>> Department of Fisheries and Oceans
>> St. Andrews Biological Station
>> 531 Brandy Cove Road
>> St. Andrews, NB
>> Canada
>> E5B 2L9
>>
>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From drf5n at maplepark.com  Thu Jun 16 17:49:05 2005
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 16 Jun 2005 10:49:05 -0500 (CDT)
Subject: [R] Drawing information-rich, Tufte style scatterplots and axes
In-Reply-To: <20050616142656.GA11229@cl.cam.ac.uk>
References: <20050616142656.GA11229@cl.cam.ac.uk>
Message-ID: <Pine.LNX.4.58.0506161046580.25059@maplepark.com>

On Thu, 16 Jun 2005, Steven J. Murdoch wrote:

> I have been working on using R to draw information-rich graphs based
> on suggestions from "The Visual Display of Quantitative Information"
> by Edward Tufte.  Thanks to the members of the list who have helped me
> with this.
>
> I have now produced the first version of this code, which I used to
> generate graphs for a paper I recently co-authored. There are
> descriptions of the new functions, examples of their use and links for
> downloading the source here:
>
>  http://www.cl.cam.ac.uk/users/sjm217/projects/graphics/fancyaxis.html

Pretty.  Thanks.

>
> If anyone has any comments or suggestions I would be very interested.

I posted a link to it on the GraphGallery page of the R Wiki:
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?GraphGallery

Why not make it a package?

Dave
-- 
 Dr. David Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From gaurav at virtuaresearch.com  Thu Jun 16 19:59:20 2005
From: gaurav at virtuaresearch.com (Gaurav Gupta)
Date: Thu, 16 Jun 2005 13:59:20 -0400
Subject: [R] Financial toolkit
Message-ID: <200506161759.j5GHxMns004895@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050616/5037e478/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Thu Jun 16 20:16:15 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 16 Jun 2005 20:16:15 +0200
Subject: [R] Financial toolkit
In-Reply-To: <200506161759.j5GHxMns004895@hypatia.math.ethz.ch>
References: <200506161759.j5GHxMns004895@hypatia.math.ethz.ch>
Message-ID: <20050616201615.0f7142f2.Achim.Zeileis@wu-wien.ac.at>

On Thu, 16 Jun 2005 13:59:20 -0400 Gaurav Gupta wrote:

> Is there any financial toolkit available for use  in R ?? I am looking
> at a variety of functions but don't want to recreate each one of them.

Look at the Finance task view at
  http://CRAN.R-project.org/src/contrib/Views/
That should provide some guidance.
Z

> Thanks
> 
> Gaurav
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From khobson at fd9ns01.okladot.state.ok.us  Thu Jun 16 20:44:01 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Thu, 16 Jun 2005 13:44:01 -0500
Subject: [R]  Excel files first row not being read
Message-ID: <OF2D3D4A36.E1926C86-ON86257022.0066B673-86257022.0066C910@fd9ns01.okladot.state.ok.us>





If you would post your R code and explain how a simple Excel matrix is set
up, we might be able to help more.

khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax



From joshuacgilbert at gmail.com  Thu Jun 16 20:45:18 2005
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Thu, 16 Jun 2005 14:45:18 -0400
Subject: [R] Computing generalized eigenvalues
Message-ID: <ef96daa305061611451ecaf99c@mail.gmail.com>

I need to compute generalized eigenvalues. The eigen function in base
doesn't do it and I can't find a package that does.

As I understand it, Lapack __can__ computer them
(http://www.netlib.org/lapack/lawn41/node111.html) and R can use
Lapack. If there is no function already, can I access Lapack from R
and use those routines directly?

Thank you,
Joshua Gilbert.



From DiminishedSmith at vorras.net  Thu Jun 16 21:42:19 2005
From: DiminishedSmith at vorras.net (Kerry  P. Robin, II)
Date: Thu, 16 Jun 2005 20:42:19 +0100
Subject: [R] Tapes... DVDs... Now this...
Message-ID: <889U5O83P1VB92G2N14XK60EX8@48balai%.fastemailer.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050616/2c226b35/attachment.pl

From slusek at o2.pl  Thu Jun 16 21:28:48 2005
From: slusek at o2.pl (Wojtek Slusarski)
Date: Thu, 16 Jun 2005 21:28:48 +0200
Subject: [R] Financial toolkit
In-Reply-To: <200506161759.j5GHxMns004895@hypatia.math.ethz.ch>
References: <200506161759.j5GHxMns004895@hypatia.math.ethz.ch>
Message-ID: <42B1D2F0.3070809@o2.pl>

Gaurav Gupta wrote:
> Is there any financial toolkit available for use  in R ?? I am looking at a
> variety of functions but don't want to recreate each one of them.

I am not sure what kind of financial functions you need, but there is a 
very good toolkit for financial time series analysis in rmetrics. Go to 
rmetrics.org and have a look.

Best regards,
Wojtek



From piotr at majdak.com  Thu Jun 16 22:59:57 2005
From: piotr at majdak.com (Piotr Majdak)
Date: Thu, 16 Jun 2005 22:59:57 +0200
Subject: [R] Analysing ordinal/nominal data
Message-ID: <42B1E84D.1060205@majdak.com>

Hi!

I'm looking for a solution to analyse data, which consists of 
dichotomous  responses (yes/no) for 2 multinomial ordinal variables. I 
was trying glm() and got hierarhical models treating all variables as 
nominal, but I can't figure out how to tell glm() to use a model for 
ordinal data like this:

log(Mij) = intercept + X + Y + Z + beta*(x-x')*(y-y')

where beta is a regression factor for interaction between X and Y.

Do you know a trick to code it in R or point me to some documentation?

thanks a lot,

Piotr Majdak



From gerifalte28 at hotmail.com  Thu Jun 16 23:00:41 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Thu, 16 Jun 2005 21:00:41 +0000
Subject: [R] Potential minor GUI bug
Message-ID: <BAY103-F888F8FB19C68555662773A6F50@phx.gbl>

Is this an interface bug?  Using RGUI for windows I run into a "Not 
Responding" process (I "smartly" coded an infinite loop, yaiks!), I hit esc 
and the interpreter was stopped and I recovered the console functionality 
but the caption on the R icon in my windows taskbar (the individual icon 
shown for every software currently running in the session)  was not updated 
so the caption still reads "RGui (Not Responding)". This behavior is 
repeated everytime I run into a "Not responding" process.  Off course if I 
end the session and open a new session the icon caption goes back to the 
normal "RGui".

I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M, 1.00 Gb Ram.

Cheers

Francisco



From reid_huntsinger at merck.com  Thu Jun 16 23:00:12 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 16 Jun 2005 17:00:12 -0400
Subject: [R] Computing generalized eigenvalues
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A94A1@uswpmx00.merck.com>

If you mean singular values, look at svd(). Otherwise you may need to
explain; a "generalized eigenvector" does have an honest eigenvalue attached
to it, so I'm guessing you mean something else.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joshua Gilbert
Sent: Thursday, June 16, 2005 2:45 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Computing generalized eigenvalues


I need to compute generalized eigenvalues. The eigen function in base
doesn't do it and I can't find a package that does.

As I understand it, Lapack __can__ computer them
(http://www.netlib.org/lapack/lawn41/node111.html) and R can use
Lapack. If there is no function already, can I access Lapack from R
and use those routines directly?

Thank you,
Joshua Gilbert.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mischke at sozpsy.unizh.ch  Thu Jun 16 23:05:56 2005
From: mischke at sozpsy.unizh.ch (Stefan Mischke)
Date: Thu, 16 Jun 2005 23:05:56 +0200
Subject: [R] regressing each column of a matrix on all other columns
Message-ID: <1932ffe9fd9deb5c35ada15b238b5f58@sozpsy.unizh.ch>

DeaR list

I would like to predict the values of each column of a matrix A by 
regressing it on all other columns of the same matrix A. I do this with 
a for loop:

	A <- B <- matrix(round(runif(10*3,1,10),0),10)
	A
	for (i in 1:length(A[1,]))    B[,i] <- as.matrix(predict(lm( A[,i] ~ 
A[,-i] )))
	B

It works fine, but I need it to be faster. I've looked at *apply but 
just can't seem to figure it out.
Maybe the solution could look somewhat like this:

	mylm <- function(y,ci) {
		x <- A[,-ci]
		b <- lm(y~x)
	}	
	B <- apply(A,2,mylm,ci=current_column_index(A))

Is there a way to pass the index of the current column in apply to my 
function? Am I on the right path at all?
Thanks for your help.

Regards, Stefan



From andy_liaw at merck.com  Thu Jun 16 23:26:03 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Jun 2005 17:26:03 -0400
Subject: [R] Potential minor GUI bug
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9A4@usctmx1106.merck.com>

I don't think that's a bug.  Almost every Windows application can do that:
when it's busy with computation, you'll see the "not responding" message.

Andy

> From: Francisco J. Zagmutt
> 
> Is this an interface bug?  Using RGUI for windows I run into a "Not 
> Responding" process (I "smartly" coded an infinite loop, 
> yaiks!), I hit esc 
> and the interpreter was stopped and I recovered the console 
> functionality 
> but the caption on the R icon in my windows taskbar (the 
> individual icon 
> shown for every software currently running in the session)  
> was not updated 
> so the caption still reads "RGui (Not Responding)". This behavior is 
> repeated everytime I run into a "Not responding" process.  
> Off course if I 
> end the session and open a new session the icon caption goes 
> back to the 
> normal "RGui".
> 
> I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M, 
> 1.00 Gb Ram.
> 
> Cheers
> 
> Francisco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From mail at bymouth.com  Thu Jun 16 23:39:30 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Fri, 17 Jun 2005 07:39:30 +1000
Subject: [R] logistic regression - using polys and products of features
Message-ID: <008901c572bb$e1beb5f0$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050617/592b85a6/attachment.pl

From r_hlp at yahoo.com  Thu Jun 16 23:47:44 2005
From: r_hlp at yahoo.com (r help)
Date: Thu, 16 Jun 2005 14:47:44 -0700 (PDT)
Subject: [R] data for becker and chambers?
Message-ID: <20050616214745.2175.qmail@web42410.mail.yahoo.com>

hi

i am reading becker and chambers (1984 edition) and i
was wondering if the datasets are available online
somewhere (or already installed in R?)  if so, please
let me know where i can access them so i can do the
examples in the book.

thanks
-gong



From mwh at indiana.edu  Thu Jun 16 23:50:46 2005
From: mwh at indiana.edu (mwh@indiana.edu)
Date: Thu, 16 Jun 2005 16:50:46 -0500
Subject: [R] lm and time series: simple question
Message-ID: <1118958646.42b1f43638a14@webmail.iu.edu>



Hello:

This question is partly about R and partly out of my ignorance about time series.

I want to regress one time series on another, taking into account the
autocorrelation (in an AR1 model) within each series.  I am interested in how
the standard error changes when the acf is taken into account.

I've made both of my datasets into ts objects and used the basic lm function
(with na.action=NULL) to no effect (i.e. the resulting standard error is the
same as if they were not times series).  I've also looked at binding the two
series together with ts.union or ts.intersect, but then I am left with a single
object, and don't understand how to regress one of the components of this onto
the other.


Any help on this subject would be appreciated.


cheers,
Matt



From fgibbons at hms.harvard.edu  Thu Jun 16 23:59:56 2005
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Thu, 16 Jun 2005 17:59:56 -0400
Subject: [R] possible bug in merge with duplicate blank names in 'by' field.
Message-ID: <5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>

Run this:

>p <- c('a', 'c', '', ''); a <- c(10, 20, 30, 40); d1 <- 
>data.frame(Promoter=p, ip=a) # Note duplicate empty names in p.
>p <- c('b', 'c', 'd', ''); a <- c(15, 20, 30, 40); d2 <- 
>data.frame(Promoter=p, ip=a)
>all <- merge(x=d1, y=d2, by="Promoter", all=T)
>all <- merge(x=all, y=d2, by="Promoter", all=T)
>all

Data is this:

>d1
>   Promoter ip
>1        a 10
>2        c 20
>3          30
>4          40
>
>d2
>   Promoter ip
>1        b 15
>2        c 20
>3        d 30
>4          40

Output looks like this:

>   Promoter ip.x ip.y ip
>1            40   30 30
>2            40   40 30
>3            40   30 40
>4            40   40 40
>5        b   15   NA NA
>6        c   20   20 20
>7        d   30   NA NA
>8        a   NA   10 10

The weird thing about this is (in my view) that each instance of '' is 
considered unique, so with each successive merge, all combinatorial 
possibilities are explored, like a SQL outer join (Cartesian product). For 
non-empty names, an inner join is performed.

Dealing with genomic data (10^4 datapoints), it's easy to have a couple of 
blanks buried in the middle of things, and to combine several replicates 
with successive merges. I couldn't understand how my three replicates of 
6000 points, in which I expected  substantial overlap in the labels, were 
taking so long to merge and ultimately generating 57000 labels. The culprit 
turned out to be a few hundred blanks buried in the middle.

Why does the empty ("null") name merit special treatment? Perhaps I'm 
missing something. I hesitate to submit this as a bug, since technically I 
guess you could say that blank names, especially duplicates, are not 
kosher. But on the other hand, this combinatorial behaviour seems to occur 
only for blanks.

-Frank

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From p.murrell at auckland.ac.nz  Fri Jun 17 04:07:09 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 17 Jun 2005 14:07:09 +1200
Subject: [R] Plotting second axes outside xyplot
References: <52978152f47f.52f47f529781@uidaho.edu>
Message-ID: <42B2304D.6030000@stat.auckland.ac.nz>

Hi


Andrew Robinson wrote:
> Hi all,
> 
> I'm trying to find a way to get xyplot to produce a second set of axes outside the right hand side of the graph.  This is my progress so far:
> 
> EE <- equal.count(ethanol$E, number=9, overlap=1/4)
> xyplot(NOx ~ C | EE, data = ethanol,
>        prepanel = function(x, y) prepanel.loess(x, y, span = 1),
>        xlab = "Compression Ratio", ylab = "NOx (micrograms/J)",
>        panel = function(x, y) {
>             panel.grid(h=-1, v= 2)
>             panel.xyplot(x, y)
>             panel.loess(x,y, span=1)
>             panel.axis(side = "right", at = c(1, 3),
>                   labels = c(1, 3), outside = T)
>             },
>         aspect = "xy")
> 
> Does anyone have any suggestions?


I suspect the output from panel.axis() is getting clipped.  You might 
get it to work as follows ...

EE <- equal.count(ethanol$E, number=9, overlap=1/4)
xyplot(NOx ~ C | EE, data = ethanol,
        prepanel = function(x, y) {
          prepanel.loess(x, y, span = 1)
        },
        xlab = "Compression Ratio",
        ylab = "NOx (micrograms/J)",
        panel = function(x, y) {
          panel.grid(h=-1, v= 2)
          panel.xyplot(x, y)
          panel.loess(x,y, span=1)
          # don't call panel.axis in here
        },
        aspect = "xy")
# return to the right-most panel WITH CLIPPING OFF
trellis.focus("panel", 9, 1, clip.off=TRUE)
# draw the extra axis
panel.axis(side = "right", at = c(1, 3),
            labels = c(1, 3), outside = T)
trellis.unfocus()


Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ggrothendieck at gmail.com  Fri Jun 17 04:55:45 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 16 Jun 2005 22:55:45 -0400
Subject: [R] how to plot density distribution with a arrow pointer?
In-Reply-To: <1118904959.7011.23.camel@dhcp-63.ccc.ox.ac.uk>
References: <6f3fc9ee050615074037119902@mail.gmail.com>
	<42b04bb0.5223fecc.7551.2c8bSMTPIN_ADDED@mx.gmail.com>
	<6f3fc9ee05061518126cae824c@mail.gmail.com>
	<1118904959.7011.23.camel@dhcp-63.ccc.ox.ac.uk>
Message-ID: <971536df050616195571479b99@mail.gmail.com>

On 6/16/05, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> I am assuming that you want to do this empirically :
> 
>  x0 <- 0.899
>  x  <- c( rnorm(6000), rnorm(4000, mean=3) )
>  plot( d <- density(x) )
> 
>  y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
> 
>  segments( x0, 0, x0, y0, col=2 )
> 


The following displays 4 arrows.  Choose the one you
like best:

# next 5 lines from previous post:
x0 <- 0.899 
x  <- c( rnorm(6000), rnorm(4000, mean=3) )
plot( d <- density(x) )
y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
# segments( x0, 0, x0, y0, col=2 )

points(x0,y0, pch=20, col="red") # red dot
text(x0,y0, "\\da",vfont=vf, pos=3, col="green") 
text(x0,y0, "\\ua",vfont=vf, pos=1, col="red")
yax <- par("usr")[3]  # xaxis was drawn at this y level
text(x0,yax, "\\ua", vfont=vf, pos=1, col="blue", xpd=TRUE, offset=0)
text(x0,yax, "\\da", vfont=vf, pos=3, col="red", xpd=TRUE, offset=0)

The above uses xpd= which allows drawing outside of the main
plot area and usr= which we need to get the position of the
y coordinate of the boundary.  It also uses the Hershey font
for the arrow.  Other arguments are described on the
appropriate page.  See:

	?points
	?text
	?par # info on xpd= and usr=
	?Hershey  
	demo(Hershey)  # examples of arrows



From ggrothendieck at gmail.com  Fri Jun 17 05:01:18 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 16 Jun 2005 23:01:18 -0400
Subject: [R] Excel files first row not being read
In-Reply-To: <200506161150.j5GBokTt010884@hypatia.math.ethz.ch>
References: <20e69eb705061604004c11845@mail.gmail.com>
	<200506161150.j5GBokTt010884@hypatia.math.ethz.ch>
Message-ID: <971536df0506162001398a0a53@mail.gmail.com>

Antoher non-OBDC solution is to get the information from Excel 
using Microsoft COM objects.  Look into 

1. the RDCOM package on CRAN, or 

2. the rcom package, rcom_1.0-1.zip, at:
      http://sunsite.univie.ac.at/rcom/download/
   The rcom package has a mailing list at:
      http://mailman.csd.univie.ac.at/pipermail/rcom-l/


On 6/16/05, John Considine <vinum at iinet.net.au> wrote:
> Hi Vivek,
> 
> I'm sure there must be an elegant way to interface R w ODBC connections, but
> I find it simplest to create a macro in Access that exports the data (based
> on a query or table) to a csv file, then use the R function read.csv() in
> which you can specify header=FALSE if you wish. I've used the importData
> function in S+, which can import directly from an access database, but the
> two step process in R is quicker. Excel is pretty limiting if you have lots
> of data.
> 
> JC Considine
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vivek
> > Subramanian
> > Sent: Thursday, 16 June 2005 19:01
> > To: ronggui; rhelp
> > Subject: Re: [R] Excel files first row not being read
> >
> > hi,
> >
> > the specifications for my application call that the input file be in
> > excel only in the format i previously mentioned.
> > now if the approach you mention has to be used then the part of
> > converting has to be done automatically. this i found to be too
> > complex.
> >
> > an alternative to that was to use MS access databases. but again it is
> > the problem of automating the conversion that is proving difficult
> >
> > regards and best
> > vivek
> >
> > On 6/16/05, ronggui <0034058 at fudan.edu.cn> wrote:
> > > why not save the file as csv file(using the MS excel
> > application) and use the read.csv function in foreign library?
> > >
> > > On Thu, 16 Jun 2005 15:51:22 +0530
> > > Vivek Subramanian <subramanian.vivek at gmail.com> wrote:
> > >
> > > > hi,
> > > >
> > > > i am using the RODBC package to read excel files using
> > > > odbcConnectExcel and susequently sqlFetch to read the
> > contents of the
> > > > file.
> > > >
> > > > the file that i use is just a matrix of numbers thats
> > all. no headers
> > > > and column names. what happens is that the sqlFetch is
> > not reading my
> > > > first row of numbers.
> > > > i have tried different combinations of colnames and
> > rownames logical
> > > > values but that first row is not being read into my data frame.
> > > >
> > > > i can manually set the first row to blanks in my
> > worksheet, but since
> > > > this is part of larger code where a user specifies a file
> > to use this
> > > > method is a little clumsy.
> > > >
> > > > i would be grateful if you help me get around this problem.
> > > >
> > > > regards,
> > > > vivek
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Jun 17 05:02:26 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 16 Jun 2005 23:02:26 -0400
Subject: [R] abbreviate
In-Reply-To: <17073.33795.555125.386908@stat.math.ethz.ch>
References: <3f87cc6d05061510591bb2a807@mail.gmail.com>
	<Pine.LNX.4.61.0506151929200.12352@gannet.stats>
	<1118860928.4423.7.camel@localhost.localdomain>
	<971536df0506151631724ea494@mail.gmail.com>
	<17073.33795.555125.386908@stat.math.ethz.ch>
Message-ID: <971536df05061620023b543c98@mail.gmail.com>

The $ replacement function does not work with pattern
matching -- only the $ operator on the right hand side does, e.g.

> irish <- head(iris)
> irish$S <- 1
> irish$Species <- 2
> irish
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species S
1          5.1         3.5          1.4         0.2       2 1
2          4.9         3.0          1.4         0.2       2 1
3          4.7         3.2          1.3         0.2       2 1
4          4.6         3.1          1.5         0.2       2 1
5          5.0         3.6          1.4         0.2       2 1
6          5.4         3.9          1.7         0.4       2 1


On 6/16/05, Christoph Buser <buser at stat.math.ethz.ch> wrote:
> Down to 17 if we carry on your idea using the matching property
> on the left hand side, too. :-)
> 
> p$h=pmax(p$h,p$s)
> 
> 
> Gabor Grothendieck writes:
>  > Agree that this definitely should be pursued. :)  In fact,
>  > we can shave off several keystrokes by
>  >
>  > - replacing p[[1]] with p[1] on the left hand side
>  > - p$high and p$settle with p$h and p$s on the right hand
>  >   side (which makes use of the matching property of $)
>  > - '=' instead of '<-'
>  > - remove all the spaces
>  >
>  > This gets it down to 18 characters:
>  >
>  > p[1]=pmax(p$h,p$s)
>  >
>  > ______________________________________________
>  > R-help at stat.math.ethz.ch mailing list
>  > https://stat.ethz.ch/mailman/listinfo/r-help
>  > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Fri Jun 17 05:03:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 16 Jun 2005 23:03:55 -0400
Subject: [R] possible bug in merge with duplicate blank names in 'by'
	field.
In-Reply-To: <5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>
References: <5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>
Message-ID: <971536df050616200324aa13b4@mail.gmail.com>

What version of R are you using?   I don't get the same
result on my system:

> R.version.string # Windows XP
[1] "R version 2.1.0, 2005-06-10"
> p <- c('a', 'c', '', ''); a <- c(10, 20, 30, 40); d1 <-
+ data.frame(Promoter=p, ip=a) # Note duplicate empty names in p.
> p <- c('b', 'c', 'd', ''); a <- c(15, 20, 30, 40); d2 <-
+ data.frame(Promoter=p, ip=a)
> all <- merge(x=d1, y=d2, by="Promoter", all=T)
> all <- merge(x=all, y=d2, by="Promoter", all=T)
> all
  Promoter ip.x ip.y ip
1            30   40 40
2            40   40 40
3        a   10   NA NA
4        c   20   20 20
5        b   NA   15 15
6        d   NA   30 30


On 6/16/05, Frank Gibbons <fgibbons at hms.harvard.edu> wrote:
> Run this:
> 
> >p <- c('a', 'c', '', ''); a <- c(10, 20, 30, 40); d1 <-
> >data.frame(Promoter=p, ip=a) # Note duplicate empty names in p.
> >p <- c('b', 'c', 'd', ''); a <- c(15, 20, 30, 40); d2 <-
> >data.frame(Promoter=p, ip=a)
> >all <- merge(x=d1, y=d2, by="Promoter", all=T)
> >all <- merge(x=all, y=d2, by="Promoter", all=T)
> >all
> 
> Data is this:
> 
> >d1
> >   Promoter ip
> >1        a 10
> >2        c 20
> >3          30
> >4          40
> >
> >d2
> >   Promoter ip
> >1        b 15
> >2        c 20
> >3        d 30
> >4          40
> 
> Output looks like this:
> 
> >   Promoter ip.x ip.y ip
> >1            40   30 30
> >2            40   40 30
> >3            40   30 40
> >4            40   40 40
> >5        b   15   NA NA
> >6        c   20   20 20
> >7        d   30   NA NA
> >8        a   NA   10 10
> 
> The weird thing about this is (in my view) that each instance of '' is
> considered unique, so with each successive merge, all combinatorial
> possibilities are explored, like a SQL outer join (Cartesian product). For
> non-empty names, an inner join is performed.
> 
> Dealing with genomic data (10^4 datapoints), it's easy to have a couple of
> blanks buried in the middle of things, and to combine several replicates
> with successive merges. I couldn't understand how my three replicates of
> 6000 points, in which I expected  substantial overlap in the labels, were
> taking so long to merge and ultimately generating 57000 labels. The culprit
> turned out to be a few hundred blanks buried in the middle.
> 
> Why does the empty ("null") name merit special treatment? Perhaps I'm
> missing something. I hesitate to submit this as a bug, since technically I
> guess you could say that blank names, especially duplicates, are not
> kosher. But on the other hand, this combinatorial behaviour seems to occur
> only for blanks.
> 
> -Frank
> 
> PhD, Computational Biologist,
> Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
> Tel: 617-432-3555       Fax:
> 617-432-3557       http://llama.med.harvard.edu/~fgibbons
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfbrennan at rogers.com  Fri Jun 17 05:17:26 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Thu, 16 Jun 2005 23:17:26 -0400
Subject: [R] how to plot density distribution with a arrow pointer?
In-Reply-To: <971536df050616195571479b99@mail.gmail.com>
Message-ID: <200506170317.j5H3HQi9026606@hypatia.math.ethz.ch>

I am trying to reproduce your example but we don't have object vf in
vfont=vf. Do we need some package?

R>text(x0,y0, "\\da",vfont=vf, pos=3, col="green")
Error in text.default(x0, y0, "\\da", vfont = vf, pos = 3, col = "green") : 
        Object "vf" not found
R>version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R       

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
Sent: June 16, 2005 10:56 PM
To: ramasamy at cancer.org.uk
Cc: R
Subject: Re: [R] how to plot density distribution with a arrow pointer?

On 6/16/05, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> I am assuming that you want to do this empirically :
> 
>  x0 <- 0.899
>  x  <- c( rnorm(6000), rnorm(4000, mean=3) )
>  plot( d <- density(x) )
> 
>  y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
> 
>  segments( x0, 0, x0, y0, col=2 )
> 


The following displays 4 arrows.  Choose the one you
like best:

# next 5 lines from previous post:
x0 <- 0.899 
x  <- c( rnorm(6000), rnorm(4000, mean=3) )
plot( d <- density(x) )
y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
# segments( x0, 0, x0, y0, col=2 )

points(x0,y0, pch=20, col="red") # red dot
text(x0,y0, "\\da",vfont=vf, pos=3, col="green") 
text(x0,y0, "\\ua",vfont=vf, pos=1, col="red")
yax <- par("usr")[3]  # xaxis was drawn at this y level
text(x0,yax, "\\ua", vfont=vf, pos=1, col="blue", xpd=TRUE, offset=0)
text(x0,yax, "\\da", vfont=vf, pos=3, col="red", xpd=TRUE, offset=0)

The above uses xpd= which allows drawing outside of the main
plot area and usr= which we need to get the position of the
y coordinate of the boundary.  It also uses the Hershey font
for the arrow.  Other arguments are described on the
appropriate page.  See:

	?points
	?text
	?par # info on xpd= and usr=
	?Hershey  
	demo(Hershey)  # examples of arrows

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Fri Jun 17 05:25:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 16 Jun 2005 23:25:53 -0400
Subject: [R] how to plot density distribution with a arrow pointer?
In-Reply-To: <42b240c6.39149632.773c.ffffa2abSMTPIN_ADDED@mx.gmail.com>
References: <971536df050616195571479b99@mail.gmail.com>
	<42b240c6.39149632.773c.ffffa2abSMTPIN_ADDED@mx.gmail.com>
Message-ID: <971536df05061620254c85a279@mail.gmail.com>

Sorry, I copied the first part from the previous post and the second part
from my editor session and must have not copied the entire script.
Here it is again:


# next 5 lines from previous post:
x0 <- 0.899 
x  <- c( rnorm(6000), rnorm(4000, mean=3) )
plot( d <- density(x) )
y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
# segments( x0, 0, x0, y0, col=2 )

vf <- c("serif", "plain")
points(x0,y0, pch=20, col="red") # red dot
text(x0,y0, "\\da",vfont=vf, pos=3, col="green") 
text(x0,y0, "\\ua",vfont=vf, pos=1, col="red")
yax <- par("usr")[3]  # xaxis was drawn at this y level
text(x0,yax, "\\ua", vfont=vf, pos=1, col="blue", xpd=TRUE, offset=0)
text(x0,yax, "\\da", vfont=vf, pos=3, col="red", xpd=TRUE, offset=0)


On 6/16/05, Jim Brennan <jfbrennan at rogers.com> wrote:
> I am trying to reproduce your example but we don't have object vf in
> vfont=vf. Do we need some package?
> 
> R>text(x0,y0, "\\da",vfont=vf, pos=3, col="green")
> Error in text.default(x0, y0, "\\da", vfont = vf, pos = 3, col = "green") :
>        Object "vf" not found
> R>version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor Grothendieck
> Sent: June 16, 2005 10:56 PM
> To: ramasamy at cancer.org.uk
> Cc: R
> Subject: Re: [R] how to plot density distribution with a arrow pointer?
> 
> On 6/16/05, Adaikalavan Ramasamy <ramasamy at cancer.org.uk> wrote:
> > I am assuming that you want to do this empirically :
> >
> >  x0 <- 0.899
> >  x  <- c( rnorm(6000), rnorm(4000, mean=3) )
> >  plot( d <- density(x) )
> >
> >  y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
> >
> >  segments( x0, 0, x0, y0, col=2 )
> >
> 
> 
> The following displays 4 arrows.  Choose the one you
> like best:
> 
> # next 5 lines from previous post:
> x0 <- 0.899
> x  <- c( rnorm(6000), rnorm(4000, mean=3) )
> plot( d <- density(x) )
> y0 <- approx( d$x, d$y, xout=x0 )$y # height at x0
> # segments( x0, 0, x0, y0, col=2 )
> 
> points(x0,y0, pch=20, col="red") # red dot
> text(x0,y0, "\\da",vfont=vf, pos=3, col="green")
> text(x0,y0, "\\ua",vfont=vf, pos=1, col="red")
> yax <- par("usr")[3]  # xaxis was drawn at this y level
> text(x0,yax, "\\ua", vfont=vf, pos=1, col="blue", xpd=TRUE, offset=0)
> text(x0,yax, "\\da", vfont=vf, pos=3, col="red", xpd=TRUE, offset=0)
> 
> The above uses xpd= which allows drawing outside of the main
> plot area and usr= which we need to get the position of the
> y coordinate of the boundary.  It also uses the Hershey font
> for the arrow.  Other arguments are described on the
> appropriate page.  See:
> 
>        ?points
>        ?text
>        ?par # info on xpd= and usr=
>        ?Hershey
>        demo(Hershey)  # examples of arrows
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From gerifalte28 at hotmail.com  Fri Jun 17 07:59:03 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 17 Jun 2005 05:59:03 +0000
Subject: [R] Potential minor GUI bug
Message-ID: <BAY103-F36FCAF6D08165A380538B4A6F40@phx.gbl>

Dear Any

Thanks for your response.  Maybe I did not explain the behavior well.  I am 
aware that the "Not Responding" is a windows default.  What I was trying to 
explain is that once the process that generated the Not Responding is 
finished and I can use R for othe computations the "Not Responding" caption 
will remain in the task bar icon but not in the caption on the main Gui 
form.  Please see the attached screen caption for an example.

Regards

Francisco



>From: "Liaw, Andy" <andy_liaw at merck.com>
>To: "'Francisco J. Zagmutt'" 
><gerifalte28 at hotmail.com>,R-help at stat.math.ethz.ch
>Subject: RE: [R] Potential minor GUI bug
>Date: Thu, 16 Jun 2005 17:26:03 -0400
>
>I don't think that's a bug.  Almost every Windows application can do that:
>when it's busy with computation, you'll see the "not responding" message.
>
>Andy
>
> > From: Francisco J. Zagmutt
> >
> > Is this an interface bug?  Using RGUI for windows I run into a "Not
> > Responding" process (I "smartly" coded an infinite loop,
> > yaiks!), I hit esc
> > and the interpreter was stopped and I recovered the console
> > functionality
> > but the caption on the R icon in my windows taskbar (the
> > individual icon
> > shown for every software currently running in the session)
> > was not updated
> > so the caption still reads "RGui (Not Responding)". This behavior is
> > repeated everytime I run into a "Not responding" process.
> > Off course if I
> > end the session and open a new session the icon caption goes
> > back to the
> > normal "RGui".
> >
> > I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M,
> > 1.00 Gb Ram.
> >
> > Cheers
> >
> > Francisco
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains 
>information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New 
>Jersey, USA 08889), and/or its affiliates (which may be known outside the 
>United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
>Banyu) that may be confidential, proprietary copyrighted and/or legally 
>privileged. It is intended solely for the use of the individual or entity 
>named on this message.  If you are not the intended recipient, and have 
>received this message in error, please notify us immediately by reply 
>e-mail and then delete it from your system.
>------------------------------------------------------------------------------


From andy_liaw at merck.com  Fri Jun 17 08:06:52 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Jun 2005 02:06:52 -0400
Subject: [R] Potential minor GUI bug
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9AB@usctmx1106.merck.com>

Now I understand.  I get the same thing in SDI mode (R-2.1.0 on WinXPPro).
No idea why...

Andy

> From: Francisco J. Zagmutt 
> 
> Dear Any
> 
> Thanks for your response.  Maybe I did not explain the 
> behavior well.  I am 
> aware that the "Not Responding" is a windows default.  What I 
> was trying to 
> explain is that once the process that generated the Not Responding is 
> finished and I can use R for othe computations the "Not 
> Responding" caption 
> will remain in the task bar icon but not in the caption on 
> the main Gui 
> form.  Please see the attached screen caption for an example.
> 
> Regards
> 
> Francisco
> 
> 
> 
> >From: "Liaw, Andy" <andy_liaw at merck.com>
> >To: "'Francisco J. Zagmutt'" 
> ><gerifalte28 at hotmail.com>,R-help at stat.math.ethz.ch
> >Subject: RE: [R] Potential minor GUI bug
> >Date: Thu, 16 Jun 2005 17:26:03 -0400
> >
> >I don't think that's a bug.  Almost every Windows 
> application can do that:
> >when it's busy with computation, you'll see the "not 
> responding" message.
> >
> >Andy
> >
> > > From: Francisco J. Zagmutt
> > >
> > > Is this an interface bug?  Using RGUI for windows I run 
> into a "Not
> > > Responding" process (I "smartly" coded an infinite loop,
> > > yaiks!), I hit esc
> > > and the interpreter was stopped and I recovered the console
> > > functionality
> > > but the caption on the R icon in my windows taskbar (the
> > > individual icon
> > > shown for every software currently running in the session)
> > > was not updated
> > > so the caption still reads "RGui (Not Responding)". This 
> behavior is
> > > repeated everytime I run into a "Not responding" process.
> > > Off course if I
> > > end the session and open a new session the icon caption goes
> > > back to the
> > > normal "RGui".
> > >
> > > I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M,
> > > 1.00 Gb Ram.
> > >
> > > Cheers
> > >
> > > Francisco
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> > >
> >
> >
> >
> >-------------------------------------------------------------
> -----------------
> >Notice:  This e-mail message, together with any attachments, 
> contains 
> >information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New 
> >Jersey, USA 08889), and/or its affiliates (which may be 
> known outside the 
> >United States as Merck Frosst, Merck Sharp & Dohme or MSD 
> and in Japan, as 
> >Banyu) that may be confidential, proprietary copyrighted 
> and/or legally 
> >privileged. It is intended solely for the use of the 
> individual or entity 
> >named on this message.  If you are not the intended 
> recipient, and have 
> >received this message in error, please notify us immediately 
> by reply 
> >e-mail and then delete it from your system.
> >-------------------------------------------------------------
> -----------------
> 
>



From obakkalbasi at chainalytics.com  Fri Jun 17 08:14:43 2005
From: obakkalbasi at chainalytics.com (Omer Bakkalbasi)
Date: Fri, 17 Jun 2005 02:14:43 -0400
Subject: [R] CORRELATION MATRIX CONVERSION
Message-ID: <000001c57303$e0113f50$c07ba8c0@chaingang.local>

How do I convert the output of cor(x) to a columnar format? 
Ex. from format below
    X    Y    Z
X  1.0  0.9  0.5
Y  0.9  1.0  0.1
Z  0.5  0.1  1.0

to format below

X X 1.0
X Y 0.9
X Z 0.5
Y X 0.9
Y Y 1.0
Y Z 0.1
Z X 0.5
Z Y 0.1
Z Z 1.0

Thanks!

Omer



From ligges at statistik.uni-dortmund.de  Fri Jun 17 08:21:05 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Jun 2005 08:21:05 +0200
Subject: [R] Potential minor GUI bug
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9AB@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9AB@usctmx1106.merck.com>
Message-ID: <42B26BD1.3000809@statistik.uni-dortmund.de>

Liaw, Andy wrote:

> Now I understand.  I get the same thing in SDI mode (R-2.1.0 on WinXPPro).
> No idea why...

I guess this is a Windows bug, because I have seen it in other 
applications as well. Hence I don't think we should waste our time here ...

Uwe Ligges



> 
> Andy
> 
> 
>>From: Francisco J. Zagmutt 
>>
>>Dear Any
>>
>>Thanks for your response.  Maybe I did not explain the 
>>behavior well.  I am 
>>aware that the "Not Responding" is a windows default.  What I 
>>was trying to 
>>explain is that once the process that generated the Not Responding is 
>>finished and I can use R for othe computations the "Not 
>>Responding" caption 
>>will remain in the task bar icon but not in the caption on 
>>the main Gui 
>>form.  Please see the attached screen caption for an example.
>>
>>Regards
>>
>>Francisco
>>
>>
>>
>>
>>>From: "Liaw, Andy" <andy_liaw at merck.com>
>>>To: "'Francisco J. Zagmutt'" 
>>><gerifalte28 at hotmail.com>,R-help at stat.math.ethz.ch
>>>Subject: RE: [R] Potential minor GUI bug
>>>Date: Thu, 16 Jun 2005 17:26:03 -0400
>>>
>>>I don't think that's a bug.  Almost every Windows 
>>
>>application can do that:
>>
>>>when it's busy with computation, you'll see the "not 
>>
>>responding" message.
>>
>>>Andy
>>>
>>>
>>>>From: Francisco J. Zagmutt
>>>>
>>>>Is this an interface bug?  Using RGUI for windows I run 
>>
>>into a "Not
>>
>>>>Responding" process (I "smartly" coded an infinite loop,
>>>>yaiks!), I hit esc
>>>>and the interpreter was stopped and I recovered the console
>>>>functionality
>>>>but the caption on the R icon in my windows taskbar (the
>>>>individual icon
>>>>shown for every software currently running in the session)
>>>>was not updated
>>>>so the caption still reads "RGui (Not Responding)". This 
>>
>>behavior is
>>
>>>>repeated everytime I run into a "Not responding" process.
>>>>Off course if I
>>>>end the session and open a new session the icon caption goes
>>>>back to the
>>>>normal "RGui".
>>>>
>>>>I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M,
>>>>1.00 Gb Ram.
>>>>
>>>>Cheers
>>>>
>>>>Francisco
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>>-------------------------------------------------------------
>>
>>-----------------
>>
>>>Notice:  This e-mail message, together with any attachments, 
>>
>>contains 
>>
>>>information of Merck & Co., Inc. (One Merck Drive, 
>>
>>Whitehouse Station, New 
>>
>>>Jersey, USA 08889), and/or its affiliates (which may be 
>>
>>known outside the 
>>
>>>United States as Merck Frosst, Merck Sharp & Dohme or MSD 
>>
>>and in Japan, as 
>>
>>>Banyu) that may be confidential, proprietary copyrighted 
>>
>>and/or legally 
>>
>>>privileged. It is intended solely for the use of the 
>>
>>individual or entity 
>>
>>>named on this message.  If you are not the intended 
>>
>>recipient, and have 
>>
>>>received this message in error, please notify us immediately 
>>
>>by reply 
>>
>>>e-mail and then delete it from your system.
>>>-------------------------------------------------------------
>>
>>-----------------
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jun 17 08:26:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Jun 2005 08:26:29 +0200
Subject: [R] CORRELATION MATRIX CONVERSION
In-Reply-To: <000001c57303$e0113f50$c07ba8c0@chaingang.local>
References: <000001c57303$e0113f50$c07ba8c0@chaingang.local>
Message-ID: <42B26D15.8070501@statistik.uni-dortmund.de>

Omer Bakkalbasi wrote:

> How do I convert the output of cor(x) to a columnar format? 
> Ex. from format below
>     X    Y    Z
> X  1.0  0.9  0.5
> Y  0.9  1.0  0.1
> Z  0.5  0.1  1.0
> 
> to format below
> 
> X X 1.0
> X Y 0.9
> X Z 0.5
> Y X 0.9
> Y Y 1.0
> Y Z 0.1
> Z X 0.5
> Z Y 0.1
> Z Z 1.0


See, e.g., ?reshape

Uwe Ligges


> Thanks!
> 
> Omer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Fri Jun 17 08:48:35 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 17 Jun 2005 06:48:35 +0000
Subject: [R] Potential minor GUI bug
In-Reply-To: <42B26BD1.3000809@statistik.uni-dortmund.de>
Message-ID: <BAY103-F422D1686DB0D96D8F24E37A6F40@phx.gbl>

Dear Uwe

I have not seen this behavior in other windows applications but I 
definitivelly agree with you that it is probably not worth spending time on 
this trivial issue.

Thanks

Francisco


>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: "Liaw, Andy" <andy_liaw at merck.com>
>CC: "'Francisco J. Zagmutt'" 
><gerifalte28 at hotmail.com>,R-help at stat.math.ethz.ch
>Subject: Re: [R] Potential minor GUI bug
>Date: Fri, 17 Jun 2005 08:21:05 +0200
>
>Liaw, Andy wrote:
>
>>Now I understand.  I get the same thing in SDI mode (R-2.1.0 on WinXPPro).
>>No idea why...
>
>I guess this is a Windows bug, because I have seen it in other applications 
>as well. Hence I don't think we should waste our time here ...
>
>Uwe Ligges
>
>
>
>>
>>Andy
>>
>>
>>>From: Francisco J. Zagmutt
>>>
>>>Dear Any
>>>
>>>Thanks for your response.  Maybe I did not explain the behavior well.  I 
>>>am aware that the "Not Responding" is a windows default.  What I was 
>>>trying to explain is that once the process that generated the Not 
>>>Responding is finished and I can use R for othe computations the "Not 
>>>Responding" caption will remain in the task bar icon but not in the 
>>>caption on the main Gui form.  Please see the attached screen caption for 
>>>an example.
>>>
>>>Regards
>>>
>>>Francisco
>>>
>>>
>>>
>>>
>>>>From: "Liaw, Andy" <andy_liaw at merck.com>
>>>>To: "'Francisco J. Zagmutt'" 
>>>><gerifalte28 at hotmail.com>,R-help at stat.math.ethz.ch
>>>>Subject: RE: [R] Potential minor GUI bug
>>>>Date: Thu, 16 Jun 2005 17:26:03 -0400
>>>>
>>>>I don't think that's a bug.  Almost every Windows
>>>
>>>application can do that:
>>>
>>>>when it's busy with computation, you'll see the "not
>>>
>>>responding" message.
>>>
>>>>Andy
>>>>
>>>>
>>>>>From: Francisco J. Zagmutt
>>>>>
>>>>>Is this an interface bug?  Using RGUI for windows I run
>>>
>>>into a "Not
>>>
>>>>>Responding" process (I "smartly" coded an infinite loop,
>>>>>yaiks!), I hit esc
>>>>>and the interpreter was stopped and I recovered the console
>>>>>functionality
>>>>>but the caption on the R icon in my windows taskbar (the
>>>>>individual icon
>>>>>shown for every software currently running in the session)
>>>>>was not updated
>>>>>so the caption still reads "RGui (Not Responding)". This
>>>
>>>behavior is
>>>
>>>>>repeated everytime I run into a "Not responding" process.
>>>>>Off course if I
>>>>>end the session and open a new session the icon caption goes
>>>>>back to the
>>>>>normal "RGui".
>>>>>
>>>>>I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M,
>>>>>1.00 Gb Ram.
>>>>>
>>>>>Cheers
>>>>>
>>>>>Francisco
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide!
>>>>>http://www.R-project.org/posting-guide.html
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>>-------------------------------------------------------------
>>>
>>>-----------------
>>>
>>>>Notice:  This e-mail message, together with any attachments,
>>>
>>>contains
>>>
>>>>information of Merck & Co., Inc. (One Merck Drive,
>>>
>>>Whitehouse Station, New
>>>
>>>>Jersey, USA 08889), and/or its affiliates (which may be
>>>
>>>known outside the
>>>
>>>>United States as Merck Frosst, Merck Sharp & Dohme or MSD
>>>
>>>and in Japan, as
>>>
>>>>Banyu) that may be confidential, proprietary copyrighted
>>>
>>>and/or legally
>>>
>>>>privileged. It is intended solely for the use of the
>>>
>>>individual or entity
>>>
>>>>named on this message.  If you are not the intended
>>>
>>>recipient, and have
>>>
>>>>received this message in error, please notify us immediately
>>>
>>>by reply
>>>
>>>>e-mail and then delete it from your system.
>>>>-------------------------------------------------------------
>>>
>>>-----------------
>>>
>>>
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Jun 17 08:52:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 07:52:13 +0100 (BST)
Subject: [R] Potential minor GUI bug
In-Reply-To: <42B26BD1.3000809@statistik.uni-dortmund.de>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9AB@usctmx1106.merck.com>
	<42B26BD1.3000809@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.61.0506170751090.18030@gannet.stats>

Yes, it is a Windows bug: the frame is controlled by Windows and not by R.

On Fri, 17 Jun 2005, Uwe Ligges wrote:

> Liaw, Andy wrote:
>
>> Now I understand.  I get the same thing in SDI mode (R-2.1.0 on WinXPPro).
>> No idea why...
>
> I guess this is a Windows bug, because I have seen it in other
> applications as well. Hence I don't think we should waste our time here ...
>
> Uwe Ligges
>
>
>
>>
>> Andy
>>
>>
>>> From: Francisco J. Zagmutt
>>>
>>> Dear Any
>>>
>>> Thanks for your response.  Maybe I did not explain the
>>> behavior well.  I am
>>> aware that the "Not Responding" is a windows default.  What I
>>> was trying to
>>> explain is that once the process that generated the Not Responding is
>>> finished and I can use R for othe computations the "Not
>>> Responding" caption
>>> will remain in the task bar icon but not in the caption on
>>> the main Gui
>>> form.  Please see the attached screen caption for an example.
>>>
>>> Regards
>>>
>>> Francisco
>>>
>>>
>>>
>>>
>>>> From: "Liaw, Andy" <andy_liaw at merck.com>
>>>> To: "'Francisco J. Zagmutt'"
>>>> <gerifalte28 at hotmail.com>,R-help at stat.math.ethz.ch
>>>> Subject: RE: [R] Potential minor GUI bug
>>>> Date: Thu, 16 Jun 2005 17:26:03 -0400
>>>>
>>>> I don't think that's a bug.  Almost every Windows
>>>
>>> application can do that:
>>>
>>>> when it's busy with computation, you'll see the "not
>>>
>>> responding" message.
>>>
>>>> Andy
>>>>
>>>>
>>>>> From: Francisco J. Zagmutt
>>>>>
>>>>> Is this an interface bug?  Using RGUI for windows I run
>>>
>>> into a "Not
>>>
>>>>> Responding" process (I "smartly" coded an infinite loop,
>>>>> yaiks!), I hit esc
>>>>> and the interpreter was stopped and I recovered the console
>>>>> functionality
>>>>> but the caption on the R icon in my windows taskbar (the
>>>>> individual icon
>>>>> shown for every software currently running in the session)
>>>>> was not updated
>>>>> so the caption still reads "RGui (Not Responding)". This
>>>
>>> behavior is
>>>
>>>>> repeated everytime I run into a "Not responding" process.
>>>>> Off course if I
>>>>> end the session and open a new session the icon caption goes
>>>>> back to the
>>>>> normal "RGui".
>>>>>
>>>>> I am running R2.1.0 on Windows XP Pro V. 2002 SP2, Pentium M,
>>>>> 1.00 Gb Ram.
>>>>>
>>>>> Cheers
>>>>>
>>>>> Francisco
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide!
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>>
>>>> -------------------------------------------------------------
>>>
>>> -----------------
>>>
>>>> Notice:  This e-mail message, together with any attachments,
>>>
>>> contains
>>>
>>>> information of Merck & Co., Inc. (One Merck Drive,
>>>
>>> Whitehouse Station, New
>>>
>>>> Jersey, USA 08889), and/or its affiliates (which may be
>>>
>>> known outside the
>>>
>>>> United States as Merck Frosst, Merck Sharp & Dohme or MSD
>>>
>>> and in Japan, as
>>>
>>>> Banyu) that may be confidential, proprietary copyrighted
>>>
>>> and/or legally
>>>
>>>> privileged. It is intended solely for the use of the
>>>
>>> individual or entity
>>>
>>>> named on this message.  If you are not the intended
>>>
>>> recipient, and have
>>>
>>>> received this message in error, please notify us immediately
>>>
>>> by reply
>>>
>>>> e-mail and then delete it from your system.
>>>> -------------------------------------------------------------
>>>
>>> -----------------
>>>
>>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 17 09:02:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 08:02:55 +0100 (BST)
Subject: [R] lm and time series: simple question
In-Reply-To: <1118958646.42b1f43638a14@webmail.iu.edu>
References: <1118958646.42b1f43638a14@webmail.iu.edu>
Message-ID: <Pine.LNX.4.61.0506170759130.18030@gannet.stats>

On Thu, 16 Jun 2005 mwh at indiana.edu wrote:

> This question is partly about R and partly out of my ignorance about 
> time series.
>
> I want to regress one time series on another, taking into account the
> autocorrelation (in an AR1 model) within each series.  I am interested in how
> the standard error changes when the acf is taken into account.

This does not happen with least-squares fitting as done by lm. You can use 
arima or gls (in package nlme).  Note that both assume a model for the 
residuals, not for the series themselves.

You could also make a joint model of the two time series.  That is 
probably not what you want.

> I've made both of my datasets into ts objects and used the basic lm 
> function (with na.action=NULL) to no effect (i.e. the resulting standard 
> error is the same as if they were not times series).  I've also looked 
> at binding the two series together with ts.union or ts.intersect, but 
> then I am left with a single object, and don't understand how to regress 
> one of the components of this onto the other.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 17 09:26:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 08:26:15 +0100 (BST)
Subject: [R] possible bug in merge with duplicate blank names in 'by'
 field.
In-Reply-To: <5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>
References: <5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>
Message-ID: <Pine.LNX.4.61.0506170817050.22424@gannet.stats>

What version of R is this (please do see the posting guide)?

In both 2.1.0 and 2.1.1 beta I get

> all
   Promoter ip.x ip.y ip
1            30   40 40
2            40   40 40
3        a   10   NA NA
4        c   20   20 20
5        b   NA   15 15
6        d   NA   30 30

so cannot reproduce your result. Are you sure that the `blanks' really are 
empty and not some character that is printing as empty on your unstated 
OS?

BTW ' ' is what is normally called `blank'.

BTW, these are not `names' but character strings: `names' has other 
meanings in R.

On Thu, 16 Jun 2005, Frank Gibbons wrote:

> Run this:
>
>> p <- c('a', 'c', '', ''); a <- c(10, 20, 30, 40); d1 <-
>> data.frame(Promoter=p, ip=a) # Note duplicate empty names in p.
>> p <- c('b', 'c', 'd', ''); a <- c(15, 20, 30, 40); d2 <-
>> data.frame(Promoter=p, ip=a)
>> all <- merge(x=d1, y=d2, by="Promoter", all=T)
>> all <- merge(x=all, y=d2, by="Promoter", all=T)
>> all
>
> Data is this:
>
>> d1
>>   Promoter ip
>> 1        a 10
>> 2        c 20
>> 3          30
>> 4          40
>>
>> d2
>>   Promoter ip
>> 1        b 15
>> 2        c 20
>> 3        d 30
>> 4          40
>
> Output looks like this:
>
>>   Promoter ip.x ip.y ip
>> 1            40   30 30
>> 2            40   40 30
>> 3            40   30 40
>> 4            40   40 40
>> 5        b   15   NA NA
>> 6        c   20   20 20
>> 7        d   30   NA NA
>> 8        a   NA   10 10
>
> The weird thing about this is (in my view) that each instance of '' is
> considered unique, so with each successive merge, all combinatorial
> possibilities are explored, like a SQL outer join (Cartesian product). For
> non-empty names, an inner join is performed.
>
> Dealing with genomic data (10^4 datapoints), it's easy to have a couple of
> blanks buried in the middle of things, and to combine several replicates
> with successive merges. I couldn't understand how my three replicates of
> 6000 points, in which I expected  substantial overlap in the labels, were
> taking so long to merge and ultimately generating 57000 labels. The culprit
> turned out to be a few hundred blanks buried in the middle.
>
> Why does the empty ("null") name merit special treatment? Perhaps I'm
> missing something. I hesitate to submit this as a bug, since technically I
> guess you could say that blank names, especially duplicates, are not
> kosher. But on the other hand, this combinatorial behaviour seems to occur
> only for blanks.
>
> -Frank
>
> PhD, Computational Biologist,
> Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
> Tel: 617-432-3555       Fax:
> 617-432-3557       http://llama.med.harvard.edu/~fgibbons
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 17 09:36:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 08:36:19 +0100 (BST)
Subject: [R] Analysing ordinal/nominal data
In-Reply-To: <42B1E84D.1060205@majdak.com>
References: <42B1E84D.1060205@majdak.com>
Message-ID: <Pine.LNX.4.61.0506170827080.22424@gannet.stats>

On Thu, 16 Jun 2005, Piotr Majdak wrote:

> I'm looking for a solution to analyse data, which consists of
> dichotomous  responses (yes/no) for 2 multinomial ordinal variables.

Please explain how you get a binary response for a `multinomial ordinal 
variables'?  If you intend these variables to be explanatory variables, in 
what sense are they `multinomial'?

> I was trying glm() and got hierarhical models treating all variables as 
> nominal, but I can't figure out how to tell glm() to use a model for 
> ordinal data like this:
>
> log(Mij) = intercept + X + Y + Z + beta*(x-x')*(y-y')
>
> where beta is a regression factor for interaction between X and Y.

What are Mij, X, x, x', Y, y, y' and Z?  One normally fits a logistic 
regression to a binary response.

> Do you know a trick to code it in R or point me to some documentation?

Probably no `trick' is required, but we need to start from a complete and 
accurate description of the model you want to fit.

This could be a simple as

R    2-level factor
U, V ordered factors

glm(R ~ U + V + as.numeric(U)*as.numeric(V), family = binomial)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 17 09:49:12 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 08:49:12 +0100 (BST)
Subject: [R] regressing each column of a matrix on all other columns
In-Reply-To: <1932ffe9fd9deb5c35ada15b238b5f58@sozpsy.unizh.ch>
References: <1932ffe9fd9deb5c35ada15b238b5f58@sozpsy.unizh.ch>
Message-ID: <Pine.LNX.4.61.0506170836470.22424@gannet.stats>

apply() is just a for() loop internally so why do you expect it to be 
faster?

Some comments:

1) Here predict() is just extracting the fitted values.
2) Using lm.fit will be faster if fitted values is all you want.
3) You are actually regressing each column on all other columns plus an
    intercept.
4) The as.matrix is wasteful.

So it would be faster to use

A1 <- cbind(1, A)
for (i in 2:ncol(A)) B[,i-1] <- lm.fit(A1[,-i], A1[,i])$fitted

You can do this reasonably efficiently in matrix algebra: one way is to 
form the inverse of X^TX after removing column means and use the Goodnight 
sweep operation on each column in turn.

On Thu, 16 Jun 2005, Stefan Mischke wrote:

> DeaR list
>
> I would like to predict the values of each column of a matrix A by
> regressing it on all other columns of the same matrix A. I do this with
> a for loop:
>
> 	A <- B <- matrix(round(runif(10*3,1,10),0),10)
> 	A
> 	for (i in 1:length(A[1,]))    B[,i] <- as.matrix(predict(lm( A[,i] ~
> A[,-i] )))
> 	B
>
> It works fine, but I need it to be faster. I've looked at *apply but
> just can't seem to figure it out.
> Maybe the solution could look somewhat like this:
>
> 	mylm <- function(y,ci) {
> 		x <- A[,-ci]
> 		b <- lm(y~x)
> 	}
> 	B <- apply(A,2,mylm,ci=current_column_index(A))
>
> Is there a way to pass the index of the current column in apply to my
> function? Am I on the right path at all?
> Thanks for your help.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 17 10:33:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 09:33:58 +0100 (BST)
Subject: [R] Computing generalized eigenvalues
In-Reply-To: <ef96daa305061611451ecaf99c@mail.gmail.com>
References: <ef96daa305061611451ecaf99c@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506170850350.22424@gannet.stats>

On Thu, 16 Jun 2005, Joshua Gilbert wrote:

> I need to compute generalized eigenvalues. The eigen function in base
> doesn't do it and I can't find a package that does.

They are very rarely used in statistics, so this is not surprising.

I presume you mean solving Ax = lambda B x: if B is non-singular this 
reduces to a conventional eigenproblem for B^{-1}A.

> As I understand it, Lapack __can__ computer them
> (http://www.netlib.org/lapack/lawn41/node111.html) and R can use
> Lapack. If there is no function already, can I access Lapack from R
> and use those routines directly?

Yes, you can: for real matrices the requisite routines are already 
compiled into R.  See DGGES or DGGEV.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrewr at uidaho.edu  Fri Jun 17 10:43:16 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 17 Jun 2005 01:43:16 -0700
Subject: [R] Plotting second axes outside xyplot
Message-ID: <632feb6348b1.6348b1632feb@uidaho.edu>

Paul,

thanks!  An elegant solution.  
Andrew

--
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



----- Original Message -----
From: Paul Murrell <p.murrell at auckland.ac.nz>
Date: Thursday, June 16, 2005 7:07 pm
Subject: Re: [R] Plotting second axes outside xyplot

> Hi
> 
> 
> Andrew Robinson wrote:
> > Hi all,
> > 
> > I'm trying to find a way to get xyplot to produce a second set 
> of axes outside the right hand side of the graph.  This is my 
> progress so far:
> > 
> > EE <- equal.count(ethanol$E, number=9, overlap=1/4)
> > xyplot(NOx ~ C | EE, data = ethanol,
> >        prepanel = function(x, y) prepanel.loess(x, y, span = 1),
> >        xlab = "Compression Ratio", ylab = "NOx (micrograms/J)",
> >        panel = function(x, y) {
> >             panel.grid(h=-1, v= 2)
> >             panel.xyplot(x, y)
> >             panel.loess(x,y, span=1)
> >             panel.axis(side = "right", at = c(1, 3),
> >                   labels = c(1, 3), outside = T)
> >             },
> >         aspect = "xy")
> > 
> > Does anyone have any suggestions?
> 
> 
> I suspect the output from panel.axis() is getting clipped.  You 
> might 
> get it to work as follows ...
> 
> EE <- equal.count(ethanol$E, number=9, overlap=1/4)
> xyplot(NOx ~ C | EE, data = ethanol,
>        prepanel = function(x, y) {
>          prepanel.loess(x, y, span = 1)
>        },
>        xlab = "Compression Ratio",
>        ylab = "NOx (micrograms/J)",
>        panel = function(x, y) {
>          panel.grid(h=-1, v= 2)
>          panel.xyplot(x, y)
>          panel.loess(x,y, span=1)
>          # don't call panel.axis in here
>        },
>        aspect = "xy")
> # return to the right-most panel WITH CLIPPING OFF
> trellis.focus("panel", 9, 1, clip.off=TRUE)
> # draw the extra axis
> panel.axis(side = "right", at = c(1, 3),
>            labels = c(1, 3), outside = T)
> trellis.unfocus()
> 
> 
> Paul
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> 
>



From skaug at mi.uib.no  Fri Jun 17 10:52:06 2005
From: skaug at mi.uib.no (Hans Skaug)
Date: Fri, 17 Jun 2005 10:52:06 +0200
Subject: [R] glmmADMB: Mixed models for overdispersed and zero-inflated
 count data in R
Message-ID: <42B28F36.1030403@mi.uib.no>

Dear R-users,

Earlier this year I posted a message to this list regarding
negative binomial mixed models in R.  It was suggested that
the program I had written should be turned into an R-package.
This has now been done, in collaboration with David Fournier
and Anders Nielsen.

The R-package glmmADMB provides the following GLMM framework:
- Negative binomial or Poisson responses.
- Zero-inflation (optionally), e.g. a mixture of a Poisson or
         negative binomial distribution and a point mass at zero.

The computational method is based on the Laplace approximation
for integrating out the random effects, together with the
option of employing importance sampling at the posterior mode
of the random effects to permit arbitrarily close approximation
to the exact MLE. (However for these models differences appear to be
very small.)

Some of the generic convenience functions, such as
predict(), fitted.values(), ...  are still missing from
this package, but will hopefully be added in later
versions (contributions/suggestions are most welcome).
Other response distributions than negative binomial or
Poisson could easily be added.

Download site:

http://otter-rsch.com/admbre/examples/glmmadmb/glmmADMB.html

The package is based on the software ADMB-RE, but the full
unrestricted R-package is made freely available by Otter Research Ltd
and does not require ADMB-RE to run with user supplied data.

I you will find this useful,

Hans Skaug

-- 
Hans  Skaug

Department of Mathematics
University of Bergen, Norway
email: 	skaug at mi.uib.no
ph. (+47) 55 58 48 61



From caroline.truntzer at chu-lyon.fr  Fri Jun 17 11:10:07 2005
From: caroline.truntzer at chu-lyon.fr (Caroline TRUNTZER)
Date: Fri, 17 Jun 2005 11:10:07 +0200
Subject: [R] About simulations
Message-ID: <42B2936F.779DF42D@chu-lyon.fr>

Hello
I would like to generate covariance matrix with autoregressive
structure. I saw some functions in nlme such as corAR1 for example but I
don't know how to use it for my goal. Could someone help me or advise me
another function?
Thank you in advance
Caroline



From bistromath.ds at gmail.com  Fri Jun 17 11:29:07 2005
From: bistromath.ds at gmail.com (BoM DS)
Date: Fri, 17 Jun 2005 11:29:07 +0200
Subject: [R] axis labels vertically
Message-ID: <c6039a6105061702296c0e9e40@mail.gmail.com>

Hi,

I have a plot and a custom axis labeling, e.g.

  x<-c(...) 
  plot(x,axes=FALSE)
  axis(2)
  axis(1,1:50,c("label1",...,"label50"))

now since the labels are quite long, only a few fit on the page.
Can I rotate each label by 90 degree counterclockwise 
(so that they are vertical)



From ripley at stats.ox.ac.uk  Fri Jun 17 11:26:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 10:26:59 +0100 (BST)
Subject: [R] About simulations
In-Reply-To: <42B2936F.779DF42D@chu-lyon.fr>
References: <42B2936F.779DF42D@chu-lyon.fr>
Message-ID: <Pine.LNX.4.61.0506171023490.25613@gannet.stats>

ARMAacf() will give you the acf for an autoregression, and toeplitz()
wil turn this into a correlation matrix.  Then just multiply by the 
desired variance.

I am not sure what this has to do with your subject line, and ?arima.sim 
might be helpful for that.

On Fri, 17 Jun 2005, Caroline TRUNTZER wrote:

> Hello
> I would like to generate covariance matrix with autoregressive
> structure. I saw some functions in nlme such as corAR1 for example but I
> don't know how to use it for my goal. Could someone help me or advise me
> another function?
> Thank you in advance
> Caroline

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From piotr at majdak.com  Fri Jun 17 11:41:51 2005
From: piotr at majdak.com (Piotr Majdak)
Date: Fri, 17 Jun 2005 11:41:51 +0200
Subject: [R] Analysing ordinal/nominal data
In-Reply-To: <Pine.LNX.4.61.0506170827080.22424@gannet.stats>
References: <42B1E84D.1060205@majdak.com>
	<Pine.LNX.4.61.0506170827080.22424@gannet.stats>
Message-ID: <42B29ADF.1030704@majdak.com>

Prof Brian Ripley wrote:

> On Thu, 16 Jun 2005, Piotr Majdak wrote:
> 
>> I'm looking for a solution to analyse data, which consists of
>> dichotomous  responses (yes/no) for 2 multinomial ordinal variables.
> 
> 
> Please explain how you get a binary response for a `multinomial ordinal 
> variables'?  If you intend these variables to be explanatory variables, 
> in what sense are they `multinomial'?

My data are results from a pychoacoustical experiment:
- Response: 2 levels, frequencies for "yes"/"no"
- PR: factor, independent variable, 4 levels, ordinal
- ENV: factor, independent variable, 3 levels, ordinal

The hypothesis is that:
- PR is independent of ENV, given Response
- Response and ENV are conditionally dependent, given PR
- Response and PR are conditionally dependent, given ENV

The model:

fit=glm(count ~ PR+ENV+Response + PR:Response + ENV:Response, 
data=table, family=poisson)

fits with p=0.04 only. My explanations are:
- I have a three-way interaction
- I must consider the ordinal information of PR and ENV

> One normally fits a logistic 
> regression to a binary response.
> Probably no `trick' is required, but we need to start from a complete 
> and accurate description of the model you want to fit.

I don't know how to include the interaction to the logit model.
I think the log-linear row effects model for the 3-dim. nominal-ordinal 
table (Agresti 1984, p.89) would be the right one, but please correct me 
if I'm on a wrong way:

log mijk = intercept + lambda^X_i + lambda^Y_j + lambda^Z_k +
            tau^XY_i*(v_j-v') + tau^XZ_i*(w_k-w') +
	   beta^YZ*(v_j-v')*(w_k-w')

mijk: expected frequencies for cell with indicies i,j,k
v_j: scores of Y with {v_1 < v_2 < .. < v_j}
w_k: scores of Z with {w_1 < w_2 < .. < w_k}
lambda^X_i, lambda^Y_j, lambda^Z_k: estimated parameters for X, Y and Z
tau^XY_i, tau^XZ_j: association parameters for XY and XZ
beta^YZ: association parameter for ordinal factors Y and Z

In my case, X==Response, Y==PR and Z==ENV. I presume no interaction 
between Y and Z and would like to test beta. My idea is: if beta is 
significant, this model won't hold, the saturated model fits only and I 
must calculate odds ratios and beta's from partial tables. But: how can 
I tell glm() to use something like beta*PR*ENVm in the formula? And, am 
I on the right way?

Thanks a lot for your response.

Piotr Majdak


Agresti 1984: "Analysis of ordinal categorical data". John Wiley & Sons Inc.


-- 

Piotr Majdak
Acoustics Research Institute
Austrian Academy of Sciences
Reichsratsstr. 17
A-1010 Vienna
AUSTRIA
phone: +43-1-4277-29511
fax: +43-1-4277-9296
email: piotr at majdak.com
WWW: http://www.kfs.oeaw.ac.at



From p.dalgaard at biostat.ku.dk  Fri Jun 17 11:48:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Jun 2005 11:48:46 +0200
Subject: [R] Computing generalized eigenvalues
In-Reply-To: <Pine.LNX.4.61.0506170850350.22424@gannet.stats>
References: <ef96daa305061611451ecaf99c@mail.gmail.com>
	<Pine.LNX.4.61.0506170850350.22424@gannet.stats>
Message-ID: <x2vf4d71q9.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Thu, 16 Jun 2005, Joshua Gilbert wrote:
> 
> > I need to compute generalized eigenvalues. The eigen function in base
> > doesn't do it and I can't find a package that does.
> 
> They are very rarely used in statistics, so this is not surprising.

An aside, going a bit off-topic:

However, there's the related  generalized singular value decomposition:

 K = U Sigma inv(T) 
 L = V M inv(T)

U'U = V'V = I ;  Sigma and M diagonal (Sigma^2 + M^2 = I by
convention) ; T regular

(Look at K'K and L'L to see the connection.)

This is used in Tikhonov regularization, which is penalized least
squares, which is a statistical issue (whether numerical analysts
realize it or not). Smoothing splines is a special case. Deconvolution
is another. 

 
> I presume you mean solving Ax = lambda B x: if B is non-singular this 
> reduces to a conventional eigenproblem for B^{-1}A.

(There are some complications if both A and B are singular. That's
why the GSVD has that peculiar-looking convention.) 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Jun 17 11:55:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 10:55:50 +0100 (BST)
Subject: [R] Analysing ordinal/nominal data
In-Reply-To: <42B29ADF.1030704@majdak.com>
References: <42B1E84D.1060205@majdak.com>
	<Pine.LNX.4.61.0506170827080.22424@gannet.stats>
	<42B29ADF.1030704@majdak.com>
Message-ID: <Pine.LNX.4.61.0506171042310.32594@gannet.stats>

As I suggested before, a binomial logistic model is appropriate here, not 
a Poisson log-linear one.  (They are equivalent, but the binomial version 
is easier to interpret and less wasteful to fit.)

You have still not defined v' and w', nor the scores (are they estimated 
or not).  But the model I suggested is such a model with scores 1,2,...

On Fri, 17 Jun 2005, Piotr Majdak wrote:

> Prof Brian Ripley wrote:
>
>> On Thu, 16 Jun 2005, Piotr Majdak wrote:
>> 
>>> I'm looking for a solution to analyse data, which consists of
>>> dichotomous  responses (yes/no) for 2 multinomial ordinal variables.
>> 
>> 
>> Please explain how you get a binary response for a `multinomial ordinal 
>> variables'?  If you intend these variables to be explanatory variables, in 
>> what sense are they `multinomial'?
>
> My data are results from a pychoacoustical experiment:
> - Response: 2 levels, frequencies for "yes"/"no"
> - PR: factor, independent variable, 4 levels, ordinal
> - ENV: factor, independent variable, 3 levels, ordinal
>
> The hypothesis is that:
> - PR is independent of ENV, given Response
> - Response and ENV are conditionally dependent, given PR
> - Response and PR are conditionally dependent, given ENV
>
> The model:
>
> fit=glm(count ~ PR+ENV+Response + PR:Response + ENV:Response, data=table, 
> family=poisson)
>
> fits with p=0.04 only. My explanations are:
> - I have a three-way interaction
> - I must consider the ordinal information of PR and ENV
>
>> One normally fits a logistic regression to a binary response.
>> Probably no `trick' is required, but we need to start from a complete and 
>> accurate description of the model you want to fit.
>
> I don't know how to include the interaction to the logit model.
> I think the log-linear row effects model for the 3-dim. nominal-ordinal table 
> (Agresti 1984, p.89) would be the right one, but please correct me if I'm on 
> a wrong way:
>
> log mijk = intercept + lambda^X_i + lambda^Y_j + lambda^Z_k +
>           tau^XY_i*(v_j-v') + tau^XZ_i*(w_k-w') +
> 	   beta^YZ*(v_j-v')*(w_k-w')
>
> mijk: expected frequencies for cell with indicies i,j,k
> v_j: scores of Y with {v_1 < v_2 < .. < v_j}
> w_k: scores of Z with {w_1 < w_2 < .. < w_k}
> lambda^X_i, lambda^Y_j, lambda^Z_k: estimated parameters for X, Y and Z
> tau^XY_i, tau^XZ_j: association parameters for XY and XZ
> beta^YZ: association parameter for ordinal factors Y and Z
>
> In my case, X==Response, Y==PR and Z==ENV. I presume no interaction between Y 
> and Z and would like to test beta. My idea is: if beta is significant, this 
> model won't hold, the saturated model fits only and I must calculate odds 
> ratios and beta's from partial tables. But: how can I tell glm() to use 
> something like beta*PR*ENVm in the formula? And, am I on the right way?
>
> Thanks a lot for your response.
>
> Piotr Majdak
>
>
> Agresti 1984: "Analysis of ordinal categorical data". John Wiley & Sons Inc.
>
>
> -- 
>
> Piotr Majdak
> Acoustics Research Institute
> Austrian Academy of Sciences
> Reichsratsstr. 17
> A-1010 Vienna
> AUSTRIA
> phone: +43-1-4277-29511
> fax: +43-1-4277-9296
> email: piotr at majdak.com
> WWW: http://www.kfs.oeaw.ac.at
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alfonso at slafuente.net  Fri Jun 17 12:06:52 2005
From: alfonso at slafuente.net (Alfonso M Sanchez-Lafuente)
Date: Fri, 17 Jun 2005 12:06:52 +0200
Subject: [R] Mixed model question
Message-ID: <42B2A0BC.6000504@slafuente.net>

Hi,

I am new to this list as a poster, but a reader for some time.

I've using R for several weeks now, and I have a lot of questions about 
certain procedures. Here I go:

I want to test if there are differences in the time spent by pollinators 
visiting flowers of a given plant species, according to a number of 
experimental manipulations made on those flowers. All experimental 
manipulations (factor with 5 levels) are replicated within plants (i.e. 
plant is my sampling unit). Further, I have two populations (two level 
factor), and a number of pollinator groups (again, two levels, the same 
in both populations). The response variables in the numbe of seconds 
invested in each probe. Further, I have plant floral display as a 
covariate, as it may influence visitation rates.

I think I have to analyse this desing considering population, pollinator 
group and their interaction as fixed effects, and treatment nested 
within plant, and its interaction with population and pollinator group, 
as random factors.

In SAS terminology, the model looks like this:

proc mixed data=flwfunc.visitflower covtest method=reml;
	class site pollclass treatm plantid;
	model time = site|pollclass flwinflor / chisq;
	random treatm site*treatm pollclass*treatm / subject=plantid;
	lsmeans site pollclass site*pollclass;
run;

I've been successfully trying lm, but I think is not suitable for random 
effects. Thus, I've tried lme, but no success when defining the random 
part or trying to interpret the results...

Any help will be welcome!

-- 

----------------------------------------------
Alfonso M. Sanchez-Lafuente
Departamento de Biologia Vegetal y Ecologia
Facultad de Biologia
Universidad de Sevilla
Avd. Reina Mercedes 9
E-41012, Sevilla, Spain
email: alfonso at slafuente.net / slafuente at us.es



From alfonso at slafuente.net  Fri Jun 17 12:13:39 2005
From: alfonso at slafuente.net (Alfonso M Sanchez-Lafuente)
Date: Fri, 17 Jun 2005 12:13:39 +0200
Subject: [R] Mixed model question
Message-ID: <42B2A253.6020802@slafuente.net>

Hi,

I am new to this list as a poster, but a reader for some time.

I've using R for several weeks now, and I have a lot of questions about 
certain procedures. Here I go:

I want to test if there are differences in the time spent by pollinators 
visiting flowers of a given plant species, according to a number of 
experimental manipulations made on those flowers. All experimental 
manipulations (factor with 5 levels) are replicated within plants (i.e. 
plant is my sampling unit). Further, I have two populations (two level 
factor), and a number of pollinator groups (again, two levels, the same 
in both populations). The response variables in the numbe of seconds 
invested in each probe. Further, I have plant floral display as a 
covariate, as it may influence visitation rates.

I think I have to analyse this desing considering population, pollinator 
group and their interaction as fixed effects, and treatment nested 
within plant, and its interaction with population and pollinator group, 
as random factors.

In SAS terminology, the model looks like this:

proc mixed data=flwfunc.visitflower covtest method=reml;
     class site pollclass treatm plantid;
     model time = site|pollclass flwinflor / chisq;
     random treatm site*treatm pollclass*treatm / subject=plantid;
     lsmeans site pollclass site*pollclass;
run;

I've been successfully trying lm, but I think is not suitable for random 
effects. Thus, I've tried lme, but no success when defining the random 
part or trying to interpret the results...

Any help will be welcome!

-- 

----------------------------------------------
Alfonso M. Sanchez-Lafuente
Departamento de Biologia Vegetal y Ecologia
Facultad de Biologia
Universidad de Sevilla
Avd. Reina Mercedes 9
E-41012, Sevilla, Spain
email: alfonso at slafuente.net / slafuente at us.es



From sundar.dorai-raj at pdf.com  Fri Jun 17 12:30:34 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 17 Jun 2005 05:30:34 -0500
Subject: [R] axis labels vertically
In-Reply-To: <c6039a6105061702296c0e9e40@mail.gmail.com>
References: <c6039a6105061702296c0e9e40@mail.gmail.com>
Message-ID: <42B2A64A.1030009@pdf.com>



BoM DS wrote:
> Hi,
> 
> I have a plot and a custom axis labeling, e.g.
> 
>   x<-c(...) 
>   plot(x,axes=FALSE)
>   axis(2)
>   axis(1,1:50,c("label1",...,"label50"))
> 
> now since the labels are quite long, only a few fit on the page.
> Can I rotate each label by 90 degree counterclockwise 
> (so that they are vertical)

You should (re-)read ?axis, which points you to the "las" parameter:

x <- 1:50
plot(x, axes = FALSE)
axis(1, x, paste("label", x), las = 2, cex.axis = 0.5)
axis(2)
box()


--sundar



From subianto at gmail.com  Fri Jun 17 12:36:59 2005
From: subianto at gmail.com (Muhammad Subianto)
Date: Fri, 17 Jun 2005 12:36:59 +0200
Subject: [R] CORRELATION MATRIX CONVERSION
In-Reply-To: <000001c57303$e0113f50$c07ba8c0@chaingang.local>
References: <000001c57303$e0113f50$c07ba8c0@chaingang.local>
Message-ID: <42B2A7CB.4040201@gmail.com>

Maybe like:

 > dat
     X   Y   Z
X 1.0 0.9 0.5
Y 0.9 1.0 0.1
Z 0.5 0.1 1.0
 > datrow <- stack(as.data.frame(dat))
 > datrow$X=rownames(dat)
 > datrow
   values ind X
1    1.0   X X
2    0.9   X Y
3    0.5   X Z
4    0.9   Y X
5    1.0   Y Y
6    0.1   Y Z
7    0.5   Z X
8    0.1   Z Y
9    1.0   Z Z
 >

Regards,
Muhammad Subianto

On this day 6/17/2005 8:14 AM, Omer Bakkalbasi wrote:
> How do I convert the output of cor(x) to a columnar format? 
> Ex. from format below
>     X    Y    Z
> X  1.0  0.9  0.5
> Y  0.9  1.0  0.1
> Z  0.5  0.1  1.0
> 
> to format below
> 
> X X 1.0
> X Y 0.9
> X Z 0.5
> Y X 0.9
> Y Y 1.0
> Y Z 0.1
> Z X 0.5
> Z Y 0.1
> Z Z 1.0
> 
> Thanks!
> 
> Omer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Jun 17 12:44:21 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Jun 2005 12:44:21 +0200
Subject: [R] axis labels vertically
In-Reply-To: <c6039a6105061702296c0e9e40@mail.gmail.com>
References: <c6039a6105061702296c0e9e40@mail.gmail.com>
Message-ID: <42B2A985.4010104@statistik.uni-dortmund.de>

BoM DS wrote:

> Hi,
> 
> I have a plot and a custom axis labeling, e.g.
> 
>   x<-c(...) 
>   plot(x,axes=FALSE)
>   axis(2)
>   axis(1,1:50,c("label1",...,"label50"))
> 
> now since the labels are quite long, only a few fit on the page.
> Can I rotate each label by 90 degree counterclockwise 
> (so that they are vertical)

See ?par and its argument "las".

Uwe Ligges

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From piotr at majdak.com  Fri Jun 17 12:54:15 2005
From: piotr at majdak.com (Piotr Majdak)
Date: Fri, 17 Jun 2005 12:54:15 +0200
Subject: [R] Analysing ordinal/nominal data
In-Reply-To: <Pine.LNX.4.61.0506171042310.32594@gannet.stats>
References: <42B1E84D.1060205@majdak.com>
	<Pine.LNX.4.61.0506170827080.22424@gannet.stats>
	<42B29ADF.1030704@majdak.com>
	<Pine.LNX.4.61.0506171042310.32594@gannet.stats>
Message-ID: <42B2ABD7.3030309@majdak.com>



Prof Brian Ripley wrote:

> You have still not defined v' and w', nor the scores (are they estimated 
> or not).  But the model I suggested is such a model with scores 1,2,...

Sorry for that, here it is:
scores v and w: integer scores, reflecting the ordering of columns/rows. 
Agresti suggests to use 1,2,3,... as you did.
v' and w': mean of v and w, respectively

 > As I suggested before, a binomial logistic model is appropriate here,
 > not a Poisson log-linear one.  (They are equivalent, but the binomial
 > version is easier to interpret and less wasteful to fit.)

After your suggestions I've read the logit-Chapter in Agresti (1984) and 
tried to fit a logit model to my data:

log(m_ij2/m_ij1) = intercept + beta^PR_i(u_i-u') +
                    beta^ENV_j(v_j-v')

with:
mijk: expected frequencies for cell with indicies i,j,k
u_i: scores of PR with u={0,1,2,3}
v_j: scores of ENV with v={0,1,2}
beta^PR: association parameter for PR with responses
beta^ENV: association parameter for ENV with responses

I wrote in R:

count=c(250,274,285,241,279,299,247,246,255,280,355,362,
         230,207,195,239,200,181,233,235,224,200,125,118)
PR=as.integer(gl(4,3,12))-1
ENV=as.integer(gl(3,1,12))-1
countM=matrix(count,12,2)
fit=glm(countM ~ as.integer(PR)+as.integer(ENV),
	family=binomial(link=logit))
summary(fit)


R gives me:

Coefficients:
                 Estimate Std. Error z value Pr(>|z|)
(Intercept)      0.18435    0.07737   2.383 0.017179 *
as.integer(PR)  -0.10538    0.03085  -3.416 0.000635 ***
as.integer(ENV) -0.07075    0.04748  -1.490 0.136191

Looking at my data, I know that for PR=0 I have a very weak dependence 
on ENV and for PR=3 very strong dependence on ENV. How can I get this 
interpretation from the summary above? I'm new in R: is 
fit$linearpredictors what I'm looking for?




-- 
Piotr Majdak
Institut f??r Schallforschung
??sterreichische Akademie der Wissenschaften
Reichsratsstr. 17
A-1010 Wien
Tel.: +43-1-4277-29511
Fax: +43-1-4277-9296
E-Mail: piotr at majdak.com
WWW: http://www.kfs.oeaw.ac.at



From bistromath.ds at gmail.com  Fri Jun 17 12:53:38 2005
From: bistromath.ds at gmail.com (BoM DS)
Date: Fri, 17 Jun 2005 12:53:38 +0200
Subject: [R] axis labels vertically
In-Reply-To: <42B2A64A.1030009@pdf.com>
References: <c6039a6105061702296c0e9e40@mail.gmail.com>
	<42B2A64A.1030009@pdf.com>
Message-ID: <c6039a6105061703538065169@mail.gmail.com>

2005/6/17, Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>:

> You should (re-)read ?axis, which points you to the "las" parameter:
> 
> x <- 1:50
> plot(x, axes = FALSE)
> axis(1, x, paste("label", x), las = 2, cex.axis = 0.5)
> axis(2)
> box()

thank you very much. This answer helps me in several ways:
 1. it solves my particular problem
 2. I wasn't aware that there is an online-help. (was using the
tutorial on the web, which is nat at all detailed.
 3. ?axis leaded me to ?par

Thank you very much



From luk111111 at yahoo.com  Fri Jun 17 12:58:03 2005
From: luk111111 at yahoo.com (luk)
Date: Fri, 17 Jun 2005 03:58:03 -0700 (PDT)
Subject: [R] Error message from pamr
In-Reply-To: <Pine.A41.4.61b.0506160658390.57662@homer08.u.washington.edu>
Message-ID: <20050617105803.46994.qmail@web30907.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050617/d9a39824/attachment.pl

From thorstensen at gmx.net  Fri Jun 17 13:28:21 2005
From: thorstensen at gmx.net (Thorstensen Nicolas)
Date: Fri, 17 Jun 2005 13:28:21 +0200
Subject: [R] mlbench xor
Message-ID: <42B2B3D5.8060905@gmx.net>

HI!

I have 2D feature vectors and 2 classes. I want to solve this 
classification problem with SVM. I create an object XOR of the mlbench 
library.
Then I replace the values of the XOR object with my values. Then I do a 
plot and I have my data plotted and coloured with the labels from the 
XOR object. So I replace the class labels
in the xor object with my class labels. But when I do a plot with the 
new class labels every object is coloured in one colour. But half of the 
points should be in different colour(because different class label). As 
well when I apply svm on the xor object with th replaced data it finds 
just one class.

someone any idea

thx



From dargosch at gmail.com  Fri Jun 17 13:50:11 2005
From: dargosch at gmail.com (Fredrik Karlsson)
Date: Fri, 17 Jun 2005 13:50:11 +0200
Subject: [R] wapply from gplots -- How do I get a local estimate of variance?
Message-ID: <376e97ec050617045047d89a49@mail.gmail.com>

Dear list,

I am trying to plot the local variance in a moving window on a
dataset. The function that I am trying to use for this is wapply from
gtools.

However, from the lattice panel function code:

<snip>
	cat(x)
	cat(y)
  	wapply(x,y,method="range",width=1/10,fun=sd,na.rm=TRUE) -> outvar
</snip>

I get:

<snip>
109 109 109 109 109 109 116 116 116 116 119 119 123 123 123 123 127
127 133 133 133 133 133 138 138 138 138 138 138 138 138138 138 138 142
142 142 142 142 142 142 147 147 147 147 147 158 158 158 158 158 158
158 158 158 158 158 158 158
20.234 28.431 10.762 45.613 15.028 10.775 18.516 13.371 18.516 13.371
20.61 31.08 9.338 210.77 11.927 39.438 -35.079 15.872 12.272
9.1122.665 7.355 22.057 -49.289 11.212 16.236 20.654 16.236 20.654
15.181 57.271 37.513 57.271 37.513 8.518 -108.517 -14.444 10.702 4.482
16.422 23.003 42.451 12.998 42.451 12.998 14.292 5.945 7.115 16.079
-13.172 10.62 16.079 -13.172 10.62 16.079 -13.172 10.62 20.477 22.656
Error in var(x, na.rm = na.rm) : 'x' is empty
</snip>

What am I doing wrong with this? The variables that I think is
supplied to the var function does not seem to be empty like the error
message suggests.

The gplots package is from the 2.0.7 version of the gregmisc bundle.


/Fredrik Karlsson



From deepayan.sarkar at gmail.com  Fri Jun 17 13:54:11 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 17 Jun 2005 06:54:11 -0500
Subject: [R] Fwd:  lattice, panel.grid, and scales=list(tick.number=XXX)
In-Reply-To: <eb555e660506151013e5bf089@mail.gmail.com>
References: <20050614223636.29109.qmail@web31305.mail.mud.yahoo.com>
	<200506142312.j5ENCptZ027524@faraday.gene.com>
	<eb555e660506151013e5bf089@mail.gmail.com>
Message-ID: <eb555e660506170454171ee805@mail.gmail.com>

I just realised that I had forgotten to copy this to the list.

---------- Forwarded message ----------
From: Deepayan Sarkar <deepayan.sarkar at gmail.com>
Date: Jun 15, 2005 12:13 PM
Subject: Re: [R] lattice, panel.grid, and scales=list(tick.number=XXX)
To: Berton Gunter <gunter.berton at gene.com>


On 6/14/05, Berton Gunter <gunter.berton at gene.com> wrote:
> If you look at the code of panel.grid, you'll see why  it doesn't work -- it
> does not use any of the scale parameters. Moreover the Help page for
> panel.grid explicitly warns that the h,v=-1 specification may not work, so
> no promises have been broken.


Right, the determination of tick mark locations can be arbitrarily
complicated, via the use of methods of the (unexported) generic
'formattedTicksAndLabels'.  The only way I can think of to make
panel.grid aware of that is to store the results of that computation
somewhere. Note that these results could potentially be different for
every panel. While this is not impossible, it violates the principle
that a panel function only has access to the data for that panel (not
that this isn't ever violated, but not to this extent), and I would be
inclined to keep things as they are unless there are better reasons.

In this particular case, the tick computations are simple, so one
could do (using the recently added 'current.panel.limits', only
available if you update lattice to the latest version):

bwplot(voice.part ~ height, data=singer,
       scales = list(tick.number = 10),
       panel = function(...) {
           ref.line <- trellis.par.get("reference.line")
           panel.abline(v = pretty(current.panel.limits()$xlim, n = 10),
                        col = ref.line$col,
                        lty = ref.line$lty,
                        lwd = ref.line$lwd)
           panel.bwplot(...)
       })

[...]
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of M. K.
> Sent: Tuesday, June 14, 2005 3:37 PM
> To: R-help mailing list
> Subject: [R] lattice, panel.grid, and scales=list(tick.number=XXX)
>
> I have a Lattice plot in which I want to adjust the number of tick
> marks used, and I want to have the drawn grid reflect that change.
> Here is what I'm doing:
>
> bwplot(var1 ~ var2, data=df, scales=list(tick.number=10),
>        panel=function(...) {
>            panel.grid(h=0,v=-1,...);
>            panel.stripplot(col="gray40", pch="|", cex=2, ...);
>            panel.bwplot(...);
>            })
>
> Unfortunately this doesn't quite work.  Although the bwplot's tick
> marks are indeed increased as requested, the panel.grid produces the
> same (3 line) grid as before, seemingly unaware of the changed # of
> ticks.



From ligges at statistik.uni-dortmund.de  Fri Jun 17 14:00:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Jun 2005 14:00:44 +0200
Subject: [R] mlbench xor
In-Reply-To: <42B2B3D5.8060905@gmx.net>
References: <42B2B3D5.8060905@gmx.net>
Message-ID: <42B2BB6C.3000905@statistik.uni-dortmund.de>

Thorstensen Nicolas wrote:

> HI!
> 
> I have 2D feature vectors and 2 classes. I want to solve this 
> classification problem with SVM. I create an object XOR of the mlbench 
> library.
> Then I replace the values of the XOR object with my values. Then I do a 
> plot and I have my data plotted and coloured with the labels from the 
> XOR object. So I replace the class labels
> in the xor object with my class labels. But when I do a plot with the 
> new class labels every object is coloured in one colour. But half of the 
> points should be in different colour(because different class label). As 
> well when I apply svm on the xor object with th replaced data it finds 
> just one class.
> 
> someone any idea

No, almost impossible without having a reproducible example ....

Uwe Ligges

> thx
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jun 17 14:02:03 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Jun 2005 14:02:03 +0200
Subject: [R] Error message from pamr
In-Reply-To: <20050617105803.46994.qmail@web30907.mail.mud.yahoo.com>
References: <20050617105803.46994.qmail@web30907.mail.mud.yahoo.com>
Message-ID: <42B2BBBB.9090104@statistik.uni-dortmund.de>

luk wrote:

> Hi, 
>  
> I got the fowllowing error message when I run pamr. Could you please advise me what does this error mean?
>  
> Many thanks
> --------------
> 
> 
>>mydata <- pamr.from.excel("datgrp4", 352, sample.labels=TRUE)
> 
> 
> Read 812768 items
> 
> Read in 2307 genes
> 
> Read in 350 samples
> 
> Read in 350 sample labels
> 
> Make sure these figures are correct!!
> 
> 
>>mytrain <- pamr.train(mydata)
> 
> 
> Error in if (from == to || length.out < 2) by <- 1 :
> 
> missing value where logical needed


There is no reproducible example given, hence we can only guess.
Might be a bug in package "pamr" or user error, if the first, you want 
to contact the package maintainer rather than R help.

Uwe Ligges




> 
> 
> 
> 		
> ---------------------------------
> 
>  Rekindle the Rivalries. Sign up for Fantasy Football
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Friedrich.Leisch at tuwien.ac.at  Fri Jun 17 14:12:59 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 17 Jun 2005 14:12:59 +0200
Subject: [R] mlbench xor
In-Reply-To: <42B2B3D5.8060905@gmx.net>
References: <42B2B3D5.8060905@gmx.net>
Message-ID: <17074.48715.514109.737850@galadriel.ci.tuwien.ac.at>

>>>>> On Fri, 17 Jun 2005 13:28:21 +0200,
>>>>> Thorstensen Nicolas (TN) wrote:

  > HI!
  > I have 2D feature vectors and 2 classes. I want to solve this 
  > classification problem with SVM. I create an object XOR of the mlbench 
  > library.
  > Then I replace the values of the XOR object with my values. Then I do a 
  > plot and I have my data plotted and coloured with the labels from the 
  > XOR object. So I replace the class labels
  > in the xor object with my class labels. But when I do a plot with the 
  > new class labels every object is coloured in one colour. But half of the 
  > points should be in different colour(because different class label). As 
  > well when I apply svm on the xor object with th replaced data it finds 
  > just one class.

It is not the purpose of the r-help mailing list that undergraduate
students ask questions about their classwork. We have a lab session
scheduled for next Wednesday where you can ask questions like this in
person. 

My apologies to the list for the noise that has been generated.

Best,
Fritz Leisch

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From sdavis2 at mail.nih.gov  Fri Jun 17 14:35:48 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 17 Jun 2005 08:35:48 -0400
Subject: [R] Rmpi installation over MPICH
Message-ID: <15f8d5586fbc131d847a2ed4114530ad@mail.nih.gov>

This is a rather obscure question, I realize.  I have written to the 
package author but have not heard back as of yet.  I have read the 
README in the package, as well, but it didn't give me enough detail to 
diagnose the problems that I am having.  (I did edit out the LAM-MPI 
check in zzz.R.in.)

I am working to install Rmpi on top of MPICH on a beowulf cluster.  
Below is the output of the install command (which seems to go OK and 
then the start of the R session, which fails.  I was hoping someone 
with some experience installing Rmpi on MPICH could give me some help 
with doing so.

Any help would be greatly appreciated....

Sean



[sdavis at beowulf Rmpi]$ R CMD INSTALL 
--configure-args=--with-mpi=/opt/lib32/usr/lib 
--library='/home/sdavis/R/library' ../Rmpi_0.4-9.tar.gz
* Installing *source* package 'Rmpi' ...
Try to find mpi.h ...
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking mpi.h usability... yes
checking mpi.h presence... yes
checking for mpi.h... yes
Try to find libmpi ...
checking for main in -lmpi... yes
Try to find liblam ...
checking for main in -llam... no
liblam not found. Probably not LAM-MPI
checking for openpty in -lutil... yes
checking for main in -lpthread... yes
configure: creating ./config.status
config.status: creating src/Makevars
config.status: creating R/zzz.R
** libs
gcc -I/usr/lib/R/include -DPACKAGE_NAME=\"\" -DPACKAGE_TARNAME=\"\" 
-DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -DPACKAGE_BUGREPORT=\"\" 
-DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 
-DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 
-DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 
-DHAVE_UNISTD_H=1    -I/usr/local/include   -fPIC  -O2 -g -pipe 
-march=i386 -mcpu=i686 -c conversion.c -o conversion.o
gcc -I/usr/lib/R/include -DPACKAGE_NAME=\"\" -DPACKAGE_TARNAME=\"\" 
-DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -DPACKAGE_BUGREPORT=\"\" 
-DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 
-DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 
-DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 
-DHAVE_UNISTD_H=1    -I/usr/local/include   -fPIC  -O2 -g -pipe 
-march=i386 -mcpu=i686 -c internal.c -o internal.o
gcc -I/usr/lib/R/include -DPACKAGE_NAME=\"\" -DPACKAGE_TARNAME=\"\" 
-DPACKAGE_VERSION=\"\" -DPACKAGE_STRING=\"\" -DPACKAGE_BUGREPORT=\"\" 
-DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 
-DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 
-DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 
-DHAVE_UNISTD_H=1    -I/usr/local/include   -fPIC  -O2 -g -pipe 
-march=i386 -mcpu=i686 -c Rmpi.c -o Rmpi.o
gcc -shared -L/usr/local/lib -o Rmpi.so conversion.o internal.o Rmpi.o 
-lmpi -lutil -lpthread
** R
** demo
** inst
** preparing package for lazy loading

** help
  >>> Building/Updating help pages for package 'Rmpi'
      Formats: text html latex example
   hosts                             text    html    latex
   internal                          text    html    latex
   mpi.abort                         text    html    latex
   mpi.barrier                       text    html    latex
   mpi.bcast                         text    html    latex
   mpi.bcast.Robj                    text    html    latex
   mpi.bcast.cmd                     text    html    latex
   mpi.comm                          text    html    latex   example
   mpi.comm.disconnect               text    html    latex
   mpi.comm.free                     text    html    latex
   mpi.comm.inter                    text    html    latex
   mpi.comm.set.errhandler           text    html    latex
   mpi.comm.spawn                    text    html    latex
   mpi.const                         text    html    latex
   mpi.exit                          text    html    latex
   mpi.finalize                      text    html    latex
   mpi.gather                        text    html    latex
   mpi.get.count                     text    html    latex
   mpi.get.processor.name            text    html    latex
   mpi.get.sourcetag                 text    html    latex
   mpi.info                          text    html    latex
   mpi.init.sprng                    text    html    latex
   mpi.intercomm.merge               text    html    latex
   mpi.parallel.sim                  text    html    latex
   mpi.probe                         text    html    latex
   mpi.reduce                        text    html    latex
   mpi.remote.exec                   text    html    latex
   mpi.scatter                       text    html    latex
   mpi.send                          text    html    latex
   mpi.send.Robj                     text    html    latex
   mpi.sendrecv                      text    html    latex   example
   mpi.spawn.Rslaves                 text    html    latex
   mpi.universe.size                 text    html    latex
** building package indices ...
* DONE (Rmpi)
[sdavis at beowulf Rmpi]$ mpirun -np 1 R

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.0  (2005-04-18), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

   Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

 > library('Rmpi',lib='~/R/library')

  Rmpi version: 0.4-9
   Rmpi is an interface (wrapper) to MPI APIs
  with interactive R slave functionalities.
  See `library (help=Rmpi)' for details.

  unable to load shared library 
'/home/sdavis/R/library/Rmpi/libs/Rmpi.so':
   /home/sdavis/R/library/Rmpi/libs/Rmpi.so: cannot open shared object 
file: No such file or directory
In addition: Warning message:
Cannot start LAM/MPI. Exit in: f(libname, pkgname)
Error in library("Rmpi", lib = "~/R/library") :
  .First.lib failed for 'Rmpi'
Error in dyn.unload(x) : dynamic/shared library 
'/home/sdavis/R/library/Rmpi/libs/Rmpi.so' was not loaded
 >


And the library file is there....
[sdavis at beowulf libs]$ ls -la
-rwxr-xr-x    1 sdavis   sdavis      46756 Jun 15 17:51 Rmpi.so
[sdavis at beowulf libs]$ pwd
/home/sdavis/R/library/Rmpi/libs



From andy_liaw at merck.com  Fri Jun 17 14:39:38 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Jun 2005 08:39:38 -0400
Subject: [R] CORRELATION MATRIX CONVERSION
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9AC@usctmx1106.merck.com>

Something like:

> dat <- data.frame(x=runif(10), y=runif(10), z=runif(10))
> m <- cor(dat)
> m
          x          y          z
x 1.0000000  0.1183305  0.1096394
y 0.1183305  1.0000000 -0.2819285
z 0.1096394 -0.2819285  1.0000000
> mat2col <- function(m) {
+     m2 <- matrix(m, ncol=1)
+     rownames(m2) <- outer(rownames(m), colnames(m), paste)
+     m2
+ }
> mat2col(m)
          [,1]
x x  1.0000000
y x  0.1183305
z x  0.1096394
x y  0.1183305
y y  1.0000000
z y -0.2819285
x z  0.1096394
y z -0.2819285
z z  1.0000000

Andy 

> From: Omer Bakkalbasi
> 
> How do I convert the output of cor(x) to a columnar format? 
> Ex. from format below
>     X    Y    Z
> X  1.0  0.9  0.5
> Y  0.9  1.0  0.1
> Z  0.5  0.1  1.0
> 
> to format below
> 
> X X 1.0
> X Y 0.9
> X Z 0.5
> Y X 0.9
> Y Y 1.0
> Y Z 0.1
> Z X 0.5
> Z Y 0.1
> Z Z 1.0
> 
> Thanks!
> 
> Omer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From obakkalbasi at chainalytics.com  Fri Jun 17 15:03:54 2005
From: obakkalbasi at chainalytics.com (Omer Bakkalbasi)
Date: Fri, 17 Jun 2005 09:03:54 -0400
Subject: [R] CORRELATION MATRIX CONVERSION
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9AC@usctmx1106.merck.com>
Message-ID: <000001c5733d$0465b1e0$c07ba8c0@chaingang.local>

Excellent!  This is the most flexible and intuitive option. Thanks!

Omer 
Cell: (914) 671-7447

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Friday, June 17, 2005 8:40 AM
To: 'obakkalbasi at chainalytics.com'; r-help at stat.math.ethz.ch
Subject: RE: [R] CORRELATION MATRIX CONVERSION

Something like:

> dat <- data.frame(x=runif(10), y=runif(10), z=runif(10))
> m <- cor(dat)
> m
          x          y          z
x 1.0000000  0.1183305  0.1096394
y 0.1183305  1.0000000 -0.2819285
z 0.1096394 -0.2819285  1.0000000
> mat2col <- function(m) {
+     m2 <- matrix(m, ncol=1)
+     rownames(m2) <- outer(rownames(m), colnames(m), paste)
+     m2
+ }
> mat2col(m)
          [,1]
x x  1.0000000
y x  0.1183305
z x  0.1096394
x y  0.1183305
y y  1.0000000
z y -0.2819285
x z  0.1096394
y z -0.2819285
z z  1.0000000

Andy 

> From: Omer Bakkalbasi
> 
> How do I convert the output of cor(x) to a columnar format? 
> Ex. from format below
>     X    Y    Z
> X  1.0  0.9  0.5
> Y  0.9  1.0  0.1
> Z  0.5  0.1  1.0
> 
> to format below
> 
> X X 1.0
> X Y 0.9
> X Z 0.5
> Y X 0.9
> Y Y 1.0
> Y Z 0.1
> Z X 0.5
> Z Y 0.1
> Z Z 1.0
> 
> Thanks!
> 
> Omer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> 



----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From fgibbons at hms.harvard.edu  Fri Jun 17 15:46:10 2005
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Fri, 17 Jun 2005 09:46:10 -0400
Subject: [R] possible bug in merge with duplicate blank names in 'by'
 field.
In-Reply-To: <Pine.LNX.4.61.0506170817050.22424@gannet.stats>
References: <5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>
	<5.2.1.1.2.20050616175403.010940f8@email.med.harvard.edu>
Message-ID: <5.2.1.1.2.20050617093925.010cf050@email.med.harvard.edu>

Thanks for your quick responses, Gabor and Brian.

I'm currently running R version 1.9.1 on Linux. Actually, I have just 
tested this on R v.2.1.0 running under Windows XP, and indeed, as you both 
indicate, the problem does not exist on that version for that OS. So, at an 
appropriate time I'll upgrade my Linux installation to the most recent 
version (1.9.1 is a year old, I guess).

-Frank

At 03:26 AM 6/17/2005, Prof Brian Ripley wrote:
>What version of R is this (please do see the posting guide)?
>
>In both 2.1.0 and 2.1.1 beta I get
>
>>all
>   Promoter ip.x ip.y ip
>1            30   40 40
>2            40   40 40
>3        a   10   NA NA
>4        c   20   20 20
>5        b   NA   15 15
>6        d   NA   30 30
>
>so cannot reproduce your result. Are you sure that the `blanks' really are 
>empty and not some character that is printing as empty on your unstated OS?
>
>BTW ' ' is what is normally called `blank'.
>
>BTW, these are not `names' but character strings: `names' has other 
>meanings in R.

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From w.northcott at unsw.edu.au  Fri Jun 17 15:42:59 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Fri, 17 Jun 2005 23:42:59 +1000
Subject: [R] Trying to build R sources on Windows
In-Reply-To: <mailman.9.1119002401.15498.r-help@stat.math.ethz.ch>
References: <mailman.9.1119002401.15498.r-help@stat.math.ethz.ch>
Message-ID: <AD460179-3ADA-4683-9D9C-7377E150E588@unsw.edu.au>

I am trying without success to build the R-2.1.0 sources on Windows  
2003 server.

I have MinGW/bin in front of cygwin/bin in the Windows path.

However, I try to build, I get failures trying to include headers  
which are not part of MinGW.  I am definitely using the MinGW  
compilers, so why are the sources trying to access headers which are  
not included?

Examples would be:
argz.h included from l10nflist.c
alloca.h from errors.c
langinfo.h from main.c

Any clues about what I am doing wrong?

Bill Northcott



From jsundvik at mail.student.oulu.fi  Fri Jun 17 15:50:00 2005
From: jsundvik at mail.student.oulu.fi (Johanna Sundvik)
Date: Fri, 17 Jun 2005 16:50:00 +0300
Subject: [R] reading csv-data
Message-ID: <1119016200.42b2d5087e227@webmail.oulu.fi>


Hi!

I have had this problem for a long time. I have tried to study the manuals and
search the mailing lists, but I can not solve this. I think there has to be one
simple solution to this, but I just can not find it. 

I have saved the data in excel (csv-format). Then I read the data in R e.g.

>data <- read.csv2("example.csv", header=TRUE)

I look the data and it looks ok. E.g

>data
     Mean1   
1   4.4332  
2   8.5113  
3  35.1624 
4   9.1693 
5    2.974 
6  65.1578 
7  43.2241 
8   3.1278 
9   5.3364 
10  3.9767 

However, this "Mean1" is categorical when it should be real numbers.

> Mean1
[1] 4.4332  8.5113  35.1624 9.1693  2.974   65.1578 43.2241 3.1278  5.3364 
Levels: 2.974 3.1278 35.1624 4.4332 43.2241 5.3364 65.1578 8.5113 9.1693

Why R does not understand that this should be real numbers? What am I doing
wrong here? Thanks for your help.

Regards,
Johanna Sundvik



From e.pebesma at geog.uu.nl  Fri Jun 17 16:20:15 2005
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 17 Jun 2005 16:20:15 +0200
Subject: [R] [R-pkgs] New CRAN package sp: classes and methods for spatial
	data
Message-ID: <42B2DC1F.7090901@geog.uu.nl>

We're happy to announce the CRAN release of "sp", an R package which
has new-style classes and methods for spatial data, version 0.7-9.

Spatial data types that sp implements are: points, grids, lines, and
polygons (i.e., rings) optionally with holes. Methods include

+ the usual print, summary, plot, [, [[, $, ...
+ coercion between types (e.g. points and grids, matrices, data.frames)
+ coordinates(x), which returns the spatial coordinates of x
+ bbox(x), returns the coordinates bounding box of x
+ overlay, to query the value of e.g. points in polygons or grid
   (essentially does a point-in-polygon or point-in-raster cell)
+ spsample, for random sampling methods over a spatial domain.

An additional package (spproj) provides coordinate reference system
transformation (projection and re-projection) using the PROJ.4 library
[2]. Others (will) provide interfaces to GRASS 6 and gdal.

A good deal of work has also gone into providing plotting methods using
base, grid and lattice graphics, through the spplot function, a front-end
to lattice plots for spatial data (see gallery [1]).

The home page of these packages is found at
http://r-spatial.sourceforge.net/
See also the Task View on Spatial Data Analysis, linked from CRAN.

The reason why we wrote this package is that we think R is an excellent
environment to deal with spatial data, but that it lacks a uniform way
to deal with spatial data. Compared to the handling of dates and times,
which can utilize base classes or those provided in the chron package,
spatial data handling is much more fragmented. As a consequence:

- various packages make their own assumptions about how spatial data
   are organized
- spatial data organized for a certain package cannot easily be used
   for another package
- few (or no) packages address the full range of spatial data types
   (points, grids, lines, polygons) and their interaction
- generic spatial functionality (e.g. I/O to GIS, plotting, projection)
   is scattered and often limited in functionality.

It also means that many different package authors have to use time writing
similar data handling code, rather than concentrating on analytical
functions. If the sp package achieves its goals, data I/O will become
many-to-one, and data access for analysis one-to-many, providing a shared
data object layer for which shared methods can be written.

Classes and methods for spatial data are only useful when the spatial
packages support them. The sp development team includes maintainers of
a number of spatial R packages, on which we will work, and we hope that
over time other spatial package maintainers will also provide support
for the classes provided by this package.

Although we are working towards a fixed set of classes at this moment
of the development of sp we cannot fully guarantee that the exact
representation of the sp classes will not change in the future. Therefore,
we advise users to keep the scripts with instructions of building the
data into sp classes; it is most likely that we will not change the
building functions and methods.

Work in progress currently involves:
- support of sp classes by several spatial statistics packages
- automatic determination of hole polygons from shapefiles
- plotting filled polygons with holes
- gdal read/write support
- GRASS 6.0 support

The development of this package is a joint effort of Virgilio Gomez-Rubio,
Barry Rowlinson, Roger Bivand and Edzer Pebesma, and followed from
discussions held at a pre-DSC2003 workshop [3], announcements on R-sig-geo
[4], and a meeting held last November in Lancaster [5]. A beta release
of sp was announced a while ago on R-sig-geo [6].

With best regards,
--
Roger Bivand and Edzer Pebesma


[1] http://r-spatial.sourceforge.net/
[2] http://www.remotesensing.org/proj/
[3] http://spatial.nhh.no/meetings/vienna/index.html
[4] e.g. https://stat.ethz.ch/pipermail/r-sig-geo/2003-October/000028.html
[5] http://elearning.maths.lancs.ac.uk:8080/RSpatial/
[6] https://stat.ethz.ch/pipermail/r-sig-geo/2005-April/000378.html

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From foster at pop.psu.edu  Fri Jun 17 16:57:35 2005
From: foster at pop.psu.edu (E. Michael Foster)
Date: Fri, 17 Jun 2005 10:57:35 -0400
Subject: [R] drop elements of vector by class
Message-ID: <6.2.0.14.2.20050617105603.0429c360@localhost>

i'm trying to build a little summary table for the contents of a data frame.

t<-sapply(macro, data.class)
c<-sapply(macro, length)
m<-sapply(macro, mean, na.rm=T, digits=2)
cbind(type=t, n=c , mean=m)

I want to drop the variables that are factors so I can include -max- and 
-min- in my table.
-macro- contacts the data--how do I drop the variables according to their 
data.class

thanks,
michael foster




________________________________________________
E. Michael Foster
(W) 814-865-1923
(Fax) 814-863-0000

After 7/1

Work:
Professor of Maternal and Child Health
School of Public Health
University of North Carolina, Chapel Hill
Rosenau Hall, CB# 7445
Chapel Hill, NC 27599-7445

Home:
309 Old Larkspur Way
Chapel Hill, NC  27516


UNC School of Public Health--voted #2 SPH by
         . US News
         . Association of Hand-to-Fin Carp Fisherman
         . Iron Chef (not to be confused with the Iron Dukes!)



From tlumley at u.washington.edu  Fri Jun 17 16:57:19 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Jun 2005 07:57:19 -0700 (PDT)
Subject: [R] reading csv-data
In-Reply-To: <1119016200.42b2d5087e227@webmail.oulu.fi>
References: <1119016200.42b2d5087e227@webmail.oulu.fi>
Message-ID: <Pine.A41.4.61b.0506170752530.371906@homer07.u.washington.edu>

On Fri, 17 Jun 2005, Johanna Sundvik wrote:
> However, this "Mean1" is categorical when it should be real numbers.
>
>> Mean1
> [1] 4.4332  8.5113  35.1624 9.1693  2.974   65.1578 43.2241 3.1278  5.3364
> Levels: 2.974 3.1278 35.1624 4.4332 43.2241 5.3364 65.1578 8.5113 9.1693
>
> Why R does not understand that this should be real numbers? What am I doing
> wrong here? Thanks for your help.
>

Your files must have some entries that are not numbers, such as "." or 
something.  R then can't tell that the field is supposed to be numeric. 
This may happen with missing data, in which case the na.strings= argument 
can be used to tell R how missing data are specified.

You can convert the data to numeric as described in FAQ 7.10

 	-thomas



From jquiroz at ifop.cl  Fri Jun 17 17:03:27 2005
From: jquiroz at ifop.cl (Juan Carlos Quiroz Espinosa)
Date: Fri, 17 Jun 2005 11:03:27 -0400
Subject: [R] How to calculate random matrices from the multivariate normal
	distribution
Message-ID: <27004DDE1590B344855CF773E1D019F113156F@postino.ifop.cl>

Hi R users,

I am trying to calculate MonteCarlo from the multivariate normal
distribution. I am utilizing the parameters vector (how mean) and
covariance matrix (or 1/hessian) how input.  

Can anyone provide guidance on how I could do this?

Thank you.

************************************************
Juan Carlos Quiroz
Instituto de Fomento Pesquero
Blanco 839
Valparaiso, CHILE.
Casilla 8V
Office: 56+032-322497
Email: jquiroz at ifop.cl



From lwacker at uwinst.unizh.ch  Fri Jun 17 17:08:55 2005
From: lwacker at uwinst.unizh.ch (Luca Wacker)
Date: Fri, 17 Jun 2005 17:08:55 +0200
Subject: [R] Fit values for NA's in linear regression
Message-ID: <a2c7fe77a354cef76e3a0e9ff9c1eea2@uwinst.unizh.ch>

Hi,

To obtain estimates for some missing values in my data I fitted a 
linear regression and then used the command fitted(model) to get the 
fitted values from the model, but R doesn't return any values for the 
NA's. I can calculate the fitted values from the estimates obtained 
from the summary of model, but that's not very handy. Is there a way to 
include the missing values in the analysis and get fitted values for 
the NA's? I tried to use with the function na.action, but that didn't 
work.


thanks

Luca



From piotr at majdak.com  Fri Jun 17 17:28:55 2005
From: piotr at majdak.com (Piotr Majdak)
Date: Fri, 17 Jun 2005 17:28:55 +0200
Subject: [R] Analysing ordinal/nominal data
In-Reply-To: <Pine.LNX.4.61.0506171042310.32594@gannet.stats>
References: <42B1E84D.1060205@majdak.com>
	<Pine.LNX.4.61.0506170827080.22424@gannet.stats>
	<42B29ADF.1030704@majdak.com>
	<Pine.LNX.4.61.0506171042310.32594@gannet.stats>
Message-ID: <42B2EC37.8070904@majdak.com>

Hi Brian (and the list of course!),

I still have problems analysing data in R, because I don't know how to 
tell glm() "use row-effect model, please". The models are well defined 
by Agresti, but can't get the link from the theory to the 
implementations in R. Different names, definitions and no explanation of 
results in the documentation of glm() or summary() make it hard to get in...

But I found another solution for my data: I just use the saturated 
log-linear model for categorial data and refer to the estimated 
parameter of the three-way-interaction only. That's enough statistics 
for my paper ;-).

Thanks for your help,

Piotr Majdak

Prof Brian Ripley wrote:

> As I suggested before, a binomial logistic model is appropriate here, 
> not a Poisson log-linear one.  (They are equivalent, but the binomial 
> version is easier to interpret and less wasteful to fit.)
> 
> You have still not defined v' and w', nor the scores (are they estimated 
> or not).  But the model I suggested is such a model with scores 1,2,...

-- 
Piotr Majdak
Acoustics Research Institute
Austrian Academy of Sciences
Reichsratsstr. 17
A-1010 Vienna
AUSTRIA
phone: +43-1-4277-29511
fax: +43-1-4277-9296
email: piotr at majdak.com
WWW: http://www.kfs.oeaw.ac.at



From ivar.herfindal at bio.ntnu.no  Fri Jun 17 17:22:28 2005
From: ivar.herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Fri, 17 Jun 2005 17:22:28 +0200
Subject: [R] reading csv-data
In-Reply-To: <Pine.A41.4.61b.0506170752530.371906@homer07.u.washington.edu>
References: <1119016200.42b2d5087e227@webmail.oulu.fi>
	<Pine.A41.4.61b.0506170752530.371906@homer07.u.washington.edu>
Message-ID: <42B2EAB4.4040802@bio.ntnu.no>



Thomas Lumley wrote:

> On Fri, 17 Jun 2005, Johanna Sundvik wrote:
> 
>>However, this "Mean1" is categorical when it should be real numbers.
>>
>>
>>>Mean1
>>
>>[1] 4.4332  8.5113  35.1624 9.1693  2.974   65.1578 43.2241 3.1278  5.3364
>>Levels: 2.974 3.1278 35.1624 4.4332 43.2241 5.3364 65.1578 8.5113 9.1693
>>
>>Why R does not understand that this should be real numbers? What am I doing
>>wrong here? Thanks for your help.
>>
> 
> 
> Your files must have some entries that are not numbers, such as "." or 
> something.  R then can't tell that the field is supposed to be numeric. 
> This may happen with missing data, in which case the na.strings= argument 
> can be used to tell R how missing data are specified.
> 
> You can convert the data to numeric as described in FAQ 7.10
> 
>  	-thomas
> 
I think the problem can be that you use read.csv2(), which expect a 
comma (",") as decimal-indicator (as is common in Scandinavia), and a 
semi-colon (";") as separator between columns. Either you should try 
read.csv(), or you can try read.csv2("example.csv", dec=".", header=TRUE)

Have a look at ?read.csv (read.csv2 is in the same help-text).

Ivar

______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From macq at llnl.gov  Fri Jun 17 17:22:36 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 17 Jun 2005 08:22:36 -0700
Subject: [R] reading csv-data
In-Reply-To: <1119016200.42b2d5087e227@webmail.oulu.fi>
References: <1119016200.42b2d5087e227@webmail.oulu.fi>
Message-ID: <p06210200bed897ec9075@[128.115.153.6]>

In my experience, this has always been due to the presence of 
non-numeric values in the input.

In the example you show, it is not obvious that there is any. I would 
start by first inspecting the input file very carefully, using a text 
editor outside of R. Since your example appears to have only one 
column of data, you could try reading it with the scan() function. 
This might produce additional information that would help you 
identify any non-numeric data. Using count.fields() on the data file 
might reveal something.

If "Mean1" is an element of "data", then simply typing "Mean1" at the 
prompt should produce a "not found" message. Yet Mean1 was found. 
Have you omitted something in your email, or is there another object 
named "Mean1"?

-Don

At 4:50 PM +0300 6/17/05, Johanna Sundvik wrote:
>Hi!
>
>I have had this problem for a long time. I have tried to study the manuals and
>search the mailing lists, but I can not solve this. I think there 
>has to be one
>simple solution to this, but I just can not find it.
>
>I have saved the data in excel (csv-format). Then I read the data in R e.g.
>
>>data <- read.csv2("example.csv", header=TRUE)
>
>I look the data and it looks ok. E.g
>
>>data
>      Mean1  
>1   4.4332 
>2   8.5113 
>3  35.1624
>4   9.1693
>5    2.974
>6  65.1578
>7  43.2241
>8   3.1278
>9   5.3364
>10  3.9767
>
>However, this "Mean1" is categorical when it should be real numbers.
>
>>  Mean1
>[1] 4.4332  8.5113  35.1624 9.1693  2.974   65.1578 43.2241 3.1278  5.3364
>Levels: 2.974 3.1278 35.1624 4.4332 43.2241 5.3364 65.1578 8.5113 9.1693
>
>Why R does not understand that this should be real numbers? What am I doing
>wrong here? Thanks for your help.
>
>Regards,
>Johanna Sundvik
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From murdoch at stats.uwo.ca  Fri Jun 17 17:23:52 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 17 Jun 2005 11:23:52 -0400
Subject: [R] How to calculate random matrices from the multivariate
 normal distribution
In-Reply-To: <27004DDE1590B344855CF773E1D019F113156F@postino.ifop.cl>
References: <27004DDE1590B344855CF773E1D019F113156F@postino.ifop.cl>
Message-ID: <42B2EB08.3040200@stats.uwo.ca>

On 6/17/2005 11:03 AM, Juan Carlos Quiroz Espinosa wrote:
> Hi R users,
> 
> I am trying to calculate MonteCarlo from the multivariate normal
> distribution. I am utilizing the parameters vector (how mean) and
> covariance matrix (or 1/hessian) how input.  
> 
> Can anyone provide guidance on how I could do this?

You probably want the mvrnorm function from the MASS package.

Duncan Murdoch



From dimitris.rizopoulos at med.kuleuven.be  Fri Jun 17 17:26:09 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 17 Jun 2005 17:26:09 +0200
Subject: [R] drop elements of vector by class
References: <6.2.0.14.2.20050617105603.0429c360@localhost>
Message-ID: <004701c57350$e35b0c20$0540210a@www.domain>

try this

# to get only the factors
macro.f <- macro[sapply(macro, is.factor)]

to drop the factors
macro.nf <- macro[sapply(macro, !is.factor)]

# to get only numerics
macro.n <- macro[sapply(macro, is.numeric)]

and so on.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "E. Michael Foster" <foster at pop.psu.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 17, 2005 4:57 PM
Subject: [R] drop elements of vector by class


> i'm trying to build a little summary table for the contents of a 
> data frame.
>
> t<-sapply(macro, data.class)
> c<-sapply(macro, length)
> m<-sapply(macro, mean, na.rm=T, digits=2)
> cbind(type=t, n=c , mean=m)
>
> I want to drop the variables that are factors so I can include -max- 
> and
> -min- in my table.
> -macro- contacts the data--how do I drop the variables according to 
> their
> data.class
>
> thanks,
> michael foster
>
>
>
>
> ________________________________________________
> E. Michael Foster
> (W) 814-865-1923
> (Fax) 814-863-0000
>
> After 7/1
>
> Work:
> Professor of Maternal and Child Health
> School of Public Health
> University of North Carolina, Chapel Hill
> Rosenau Hall, CB# 7445
> Chapel Hill, NC 27599-7445
>
> Home:
> 309 Old Larkspur Way
> Chapel Hill, NC  27516
>
>
> UNC School of Public Health--voted #2 SPH by
>         . US News
>         . Association of Hand-to-Fin Carp Fisherman
>         . Iron Chef (not to be confused with the Iron Dukes!)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Fri Jun 17 17:29:01 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Jun 2005 08:29:01 -0700
Subject: [R] How to calculate random matrices from the multivariate
 normal distribution
In-Reply-To: <27004DDE1590B344855CF773E1D019F113156F@postino.ifop.cl>
References: <27004DDE1590B344855CF773E1D019F113156F@postino.ifop.cl>
Message-ID: <42B2EC3D.3040509@pdf.com>

	  RSiteSearch("random multivariate normal") produced 82 hits, the 
fourth of which was for "multivariate normal distribution", function 
pmvnorm in library mvtnorm, which also includes a function rmvorm, that 
should do what you want.

	  Buena suerte.  spencer graves

Juan Carlos Quiroz Espinosa wrote:

> Hi R users,
> 
> I am trying to calculate MonteCarlo from the multivariate normal
> distribution. I am utilizing the parameters vector (how mean) and
> covariance matrix (or 1/hessian) how input.  
> 
> Can anyone provide guidance on how I could do this?
> 
> Thank you.
> 
> ************************************************
> Juan Carlos Quiroz
> Instituto de Fomento Pesquero
> Blanco 839
> Valparaiso, CHILE.
> Casilla 8V
> Office: 56+032-322497
> Email: jquiroz at ifop.cl
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Robert.McGehee at geodecapital.com  Fri Jun 17 17:30:48 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Fri, 17 Jun 2005 11:30:48 -0400
Subject: [R] drop elements of vector by class
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C1ED918@MSGBOSCLB2WIN.DMN1.FMR.COM>

a <- data.frame(a = 1:2, b = c("a", "b"), c = I(c("a", "b")))

a <- a[ , !sapply(a, class) %in% "factor"]

-----Original Message-----
From: E. Michael Foster [mailto:foster at pop.psu.edu] 
Sent: Friday, June 17, 2005 10:58 AM
To: r-help at stat.math.ethz.ch
Subject: [R] drop elements of vector by class


i'm trying to build a little summary table for the contents of a data
frame.

t<-sapply(macro, data.class)
c<-sapply(macro, length)
m<-sapply(macro, mean, na.rm=T, digits=2)
cbind(type=t, n=c , mean=m)

I want to drop the variables that are factors so I can include -max- and

-min- in my table.
-macro- contacts the data--how do I drop the variables according to
their 
data.class

thanks,
michael foster




________________________________________________
E. Michael Foster
(W) 814-865-1923
(Fax) 814-863-0000

After 7/1

Work:
Professor of Maternal and Child Health
School of Public Health
University of North Carolina, Chapel Hill
Rosenau Hall, CB# 7445
Chapel Hill, NC 27599-7445

Home:
309 Old Larkspur Way
Chapel Hill, NC  27516


UNC School of Public Health--voted #2 SPH by
         . US News
         . Association of Hand-to-Fin Carp Fisherman
         . Iron Chef (not to be confused with the Iron Dukes!)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Jun 17 17:43:41 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Jun 2005 08:43:41 -0700
Subject: [R] Mixed model question
In-Reply-To: <42B2A253.6020802@slafuente.net>
References: <42B2A253.6020802@slafuente.net>
Message-ID: <42B2EFAD.9000904@pdf.com>

	  Have you also tried "lmer" in library(lme4)?  This is newer and 
better in many ways.  Unfortunately, I see only one example in the Help 
file, but you might be able to figure out how to use lmer from the help 
file and from the 125 hits on RSiteSearch("lmer").

	  The definitive work on this subject, from my perspective, is Pinheiro 
and Bates (2000) Mixed-Effects Models in S and S-PLUS (Springer).  This 
book is primarily how to use lme.  I learned a lot from it (and from 
Doug Bates' other book with Don Watts on nonlinear regression).  I 
couldn't find Pinheiro and Bates in the catalog for the library at 
Universidad de Sevilla (though Bates and Watts was listed).

	  Feel free to submit other questions after you've considered this, but 
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html first.  In particular, you 
might have gotten a more helpful reply to this question if you had 
included one of your attempts with "lme", asking what you did wrong or 
what you don't understand about the results.

	  Buena suerte,
	  spencer graves

Alfonso M Sanchez-Lafuente wrote:
> Hi,
> 
> I am new to this list as a poster, but a reader for some time.
> 
> I've using R for several weeks now, and I have a lot of questions about 
> certain procedures. Here I go:
> 
> I want to test if there are differences in the time spent by pollinators 
> visiting flowers of a given plant species, according to a number of 
> experimental manipulations made on those flowers. All experimental 
> manipulations (factor with 5 levels) are replicated within plants (i.e. 
> plant is my sampling unit). Further, I have two populations (two level 
> factor), and a number of pollinator groups (again, two levels, the same 
> in both populations). The response variables in the numbe of seconds 
> invested in each probe. Further, I have plant floral display as a 
> covariate, as it may influence visitation rates.
> 
> I think I have to analyse this desing considering population, pollinator 
> group and their interaction as fixed effects, and treatment nested 
> within plant, and its interaction with population and pollinator group, 
> as random factors.
> 
> In SAS terminology, the model looks like this:
> 
> proc mixed data=flwfunc.visitflower covtest method=reml;
>      class site pollclass treatm plantid;
>      model time = site|pollclass flwinflor / chisq;
>      random treatm site*treatm pollclass*treatm / subject=plantid;
>      lsmeans site pollclass site*pollclass;
> run;
> 
> I've been successfully trying lm, but I think is not suitable for random 
> effects. Thus, I've tried lme, but no success when defining the random 
> part or trying to interpret the results...
> 
> Any help will be welcome!
>



From spencer.graves at pdf.com  Fri Jun 17 17:48:51 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Jun 2005 08:48:51 -0700
Subject: [R] Fit values for NA's in linear regression
In-Reply-To: <a2c7fe77a354cef76e3a0e9ff9c1eea2@uwinst.unizh.ch>
References: <a2c7fe77a354cef76e3a0e9ff9c1eea2@uwinst.unizh.ch>
Message-ID: <42B2F0E3.6000401@pdf.com>

	  Have you considered:

 > set.seed(1)
 > tstDF <- data.frame(x=1:4, y=c(NA, 2:4+rnorm(3)))
 > fit <- lm(y~x, tstDF)
 > predict(fit)
        2        3        4
1.678441 2.573854 3.469266
 > predict(fit, tstDF)
         1         2         3         4
0.7830284 1.6784410 2.5738536 3.4692662

	  spencer graves

Luca Wacker wrote:

> Hi,
> 
> To obtain estimates for some missing values in my data I fitted a 
> linear regression and then used the command fitted(model) to get the 
> fitted values from the model, but R doesn't return any values for the 
> NA's. I can calculate the fitted values from the estimates obtained 
> from the summary of model, but that's not very handy. Is there a way to 
> include the missing values in the analysis and get fitted values for 
> the NA's? I tried to use with the function na.action, but that didn't 
> work.
> 
> 
> thanks
> 
> Luca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Jun 17 18:30:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Jun 2005 09:30:02 -0700
Subject: [R] reading csv-data
In-Reply-To: <p06210200bed897ec9075@[128.115.153.6]>
References: <1119016200.42b2d5087e227@webmail.oulu.fi>
	<p06210200bed897ec9075@[128.115.153.6]>
Message-ID: <42B2FA8A.1010202@pdf.com>

	  I've struggled with this myself in the past.  I've recently started 
using the following:

File <- "pair.txt"
# File name with path if different from getwd()
readLines(File, n=9)

	  The function	"readLines" reads the first n lines as n individual 
character strings.  From this, you can identify extra headers, the 
separate characters, etc.  Then I can do something like the following:

plot(count.fields(File, sep="\t"))

	  The function "count.fields" also has arguments to specify a number of 
lines to skip before it starts to process the file, which can be helpful 
with multiple headers.  After "count.fields" produces a constant result 
consistent with what I want, then I'm ready to use "read.table" or one 
of its variants like read.csv2.

	  hope this helps.
	  spencer graves

Don MacQueen wrote:

> In my experience, this has always been due to the presence of 
> non-numeric values in the input.
> 
> In the example you show, it is not obvious that there is any. I would 
> start by first inspecting the input file very carefully, using a text 
> editor outside of R. Since your example appears to have only one 
> column of data, you could try reading it with the scan() function. 
> This might produce additional information that would help you 
> identify any non-numeric data. Using count.fields() on the data file 
> might reveal something.
> 
> If "Mean1" is an element of "data", then simply typing "Mean1" at the 
> prompt should produce a "not found" message. Yet Mean1 was found. 
> Have you omitted something in your email, or is there another object 
> named "Mean1"?
> 
> -Don
> 
> At 4:50 PM +0300 6/17/05, Johanna Sundvik wrote:
> 
>>Hi!
>>
>>I have had this problem for a long time. I have tried to study the manuals and
>>search the mailing lists, but I can not solve this. I think there 
>>has to be one
>>simple solution to this, but I just can not find it.
>>
>>I have saved the data in excel (csv-format). Then I read the data in R e.g.
>>
>>
>>>data <- read.csv2("example.csv", header=TRUE)
>>
>>I look the data and it looks ok. E.g
>>
>>
>>>data
>>
>>     Mean1  
>>1   4.4332 
>>2   8.5113 
>>3  35.1624
>>4   9.1693
>>5    2.974
>>6  65.1578
>>7  43.2241
>>8   3.1278
>>9   5.3364
>>10  3.9767
>>
>>However, this "Mean1" is categorical when it should be real numbers.
>>
>>
>>> Mean1
>>
>>[1] 4.4332  8.5113  35.1624 9.1693  2.974   65.1578 43.2241 3.1278  5.3364
>>Levels: 2.974 3.1278 35.1624 4.4332 43.2241 5.3364 65.1578 8.5113 9.1693
>>
>>Why R does not understand that this should be real numbers? What am I doing
>>wrong here? Thanks for your help.
>>
>>Regards,
>>Johanna Sundvik
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
>



From dscully at fd9ns01.okladot.state.ok.us  Fri Jun 17 18:33:53 2005
From: dscully at fd9ns01.okladot.state.ok.us (dscully@fd9ns01.okladot.state.ok.us)
Date: Fri, 17 Jun 2005 11:33:53 -0500
Subject: [R] Data comparison
Message-ID: <OFD3EEB0F5.1BD0FBB5-ON86257023.00594280-86257023.005ADF37@fd9ns01.okladot.state.ok.us>





Question :  Is it possible to create a function, using a for ifelse
function, inside sapply, to compare the values in one data frame to a set
of upper and lower limits in another data frame, same number of columns.,
Take  the values which meet the requirements "TRUE"  and create a new data
frame or table containing the filtered data?   Can you give me a shove in
the correct direction?      Thanks



From andy_liaw at merck.com  Fri Jun 17 18:38:53 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Jun 2005 12:38:53 -0400
Subject: [R] Fit values for NA's in linear regression
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9B0@usctmx1106.merck.com>

It depends on the na.action used in lm(), which defaults to na.omit.  Here's
an example:

> x <- c(1:5, NA, 7:10)
> y <- rnorm(x)
> fitted(lm(y ~ x))
          1           2           3           4           5           7
8 
 0.68680104  0.47913875  0.27147646  0.06381417 -0.14384812 -0.55917271
-0.76683500 
          9          10 
-0.97449729 -1.18215958 
> fitted(lm(y ~ x, na.action=na.exclude))
          1           2           3           4           5           6
7 
 0.68680104  0.47913875  0.27147646  0.06381417 -0.14384812          NA
-0.55917271 
          8           9          10 
-0.76683500 -0.97449729 -1.18215958 

Andy 

> From: Luca Wacker
> 
> Hi,
> 
> To obtain estimates for some missing values in my data I fitted a 
> linear regression and then used the command fitted(model) to get the 
> fitted values from the model, but R doesn't return any values for the 
> NA's. I can calculate the fitted values from the estimates obtained 
> from the summary of model, but that's not very handy. Is 
> there a way to 
> include the missing values in the analysis and get fitted values for 
> the NA's? I tried to use with the function na.action, but that didn't 
> work.
> 
> 
> thanks
> 
> Luca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From dimitrijoe at yahoo.com.br  Fri Jun 17 19:00:41 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Fri, 17 Jun 2005 14:00:41 -0300
Subject: [R] vectorization
Message-ID: <00c701c5735e$1a6872e0$1500a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050617/c5475452/attachment.pl

From andy_liaw at merck.com  Fri Jun 17 19:02:26 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Jun 2005 13:02:26 -0400
Subject: [R] Fit values for NA's in linear regression
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9B1@usctmx1106.merck.com>

I posted a bad example.  The question would only make sense if NAs are all
in the response.  Here's try #2:

I believe you can not get fitted values for cases where y is missing
directly from the model object.  You can, however, use predict(lm.object,
newdata=mydata[!complete.cases(mydata)]) to get the predictions for those
with missing y.

Andy

> From: Luca Wacker
> 
> Hi,
> 
> To obtain estimates for some missing values in my data I fitted a 
> linear regression and then used the command fitted(model) to get the 
> fitted values from the model, but R doesn't return any values for the 
> NA's. I can calculate the fitted values from the estimates obtained 
> from the summary of model, but that's not very handy. Is 
> there a way to 
> include the missing values in the analysis and get fitted values for 
> the NA's? I tried to use with the function na.action, but that didn't 
> work.
> 
> 
> thanks
> 
> Luca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From 0034058 at fudan.edu.cn  Fri Jun 17 19:05:51 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 18 Jun 2005 01:05:51 +0800
Subject: [R] logistic regression - using polys and products of features
In-Reply-To: <008901c572bb$e1beb5f0$9701a8c0@Tablet>
References: <008901c572bb$e1beb5f0$9701a8c0@Tablet>
Message-ID: <20050618010551.079e6c9f@localhost.localdomain>

On Fri, 17 Jun 2005 07:39:30 +1000
Stephen Choularton <mail at bymouth.com> wrote:

> Hi
>  
> I can get all my features by doing this:
>  
> > logistic.model = glm(similarity ~ ., family=binomial, data =
> cData[3001:3800,])
>  
>  
> I can get the product of all my features by this:
>  
> logistic.model = glm(similarity ~ . ^ 2,  family=binomial, data =
> cData[3001:3800,])
>  
> I don't seem to be able to get polys by doing this:
>  
> logistic.model = glm(similarity ~ poly(.,2), family=binomial, data =
> cData[3001:3800,])
> Error in poly(., 2) : Object "." not found
> >
>  
> How can I get polys?
>  
> What do the warnings mean  when I  do this:
>  
> > logistic.model = glm(similarity ~ . + . ^ 2, family=binomial, data =
> cData[3001:3800,])
> Warning messages: 
> 1: Algorithm did not converge in: glm.fit(x = X, y = Y, weights =
> weights, start = start, etastart = etastart,  
> 2: fitted probabilities numerically 0 or 1 occurred in: glm.fit(x = X, y
> = Y, weights = weights, start = start, etastart = etastart,  

.^2 means all the 2-order and 1-order terms,so .+.^2 is meaningless.

> How can I do this?
>  
> logistic.model = glm(similarity ~ . + . ^ 2 + poly(.,2),
> family=binomial, data = cData[3001:3800,])


>  
> Thanks
>  
> Stephen
>  
>  
>  
> 
> --
> Internal Virus Database is out-of-date.
> Checked by AVG Anti-Virus.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From uofiowa at gmail.com  Fri Jun 17 19:34:36 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 17 Jun 2005 13:34:36 -0400
Subject: [R] trim a string
Message-ID: <3f87cc6d05061710347b4164ff@mail.gmail.com>

How to trim the leading and trailing white space off of a string?

 If the variable is "  E              " I need to convert it to "E".



From 0034058 at fudan.edu.cn  Fri Jun 17 19:48:20 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sat, 18 Jun 2005 01:48:20 +0800
Subject: [R] Mixed model question
In-Reply-To: <42B2EFAD.9000904@pdf.com>
References: <42B2A253.6020802@slafuente.net> <42B2EFAD.9000904@pdf.com>
Message-ID: <20050618014820.30864b8a@localhost.localdomain>

and the newest R-new in the www.r-project.org has an article about how to use the lmer function.

On Fri, 17 Jun 2005 08:43:41 -0700
Spencer Graves <spencer.graves at pdf.com> wrote:

> 	  Have you also tried "lmer" in library(lme4)?  This is newer and 
> better in many ways.  Unfortunately, I see only one example in the Help 
> file, but you might be able to figure out how to use lmer from the help 
> file and from the 125 hits on RSiteSearch("lmer").
> 
> 	  The definitive work on this subject, from my perspective, is Pinheiro 
> and Bates (2000) Mixed-Effects Models in S and S-PLUS (Springer).  This 
> book is primarily how to use lme.  I learned a lot from it (and from 
> Doug Bates' other book with Don Watts on nonlinear regression).  I 
> couldn't find Pinheiro and Bates in the catalog for the library at 
> Universidad de Sevilla (though Bates and Watts was listed).
> 
> 	  Feel free to submit other questions after you've considered this, but 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html first.  In particular, you 
> might have gotten a more helpful reply to this question if you had 
> included one of your attempts with "lme", asking what you did wrong or 
> what you don't understand about the results.
> 
> 	  Buena suerte,
> 	  spencer graves
> 
> Alfonso M Sanchez-Lafuente wrote:
> > Hi,
> > 
> > I am new to this list as a poster, but a reader for some time.
> > 
> > I've using R for several weeks now, and I have a lot of questions about 
> > certain procedures. Here I go:
> > 
> > I want to test if there are differences in the time spent by pollinators 
> > visiting flowers of a given plant species, according to a number of 
> > experimental manipulations made on those flowers. All experimental 
> > manipulations (factor with 5 levels) are replicated within plants (i.e. 
> > plant is my sampling unit). Further, I have two populations (two level 
> > factor), and a number of pollinator groups (again, two levels, the same 
> > in both populations). The response variables in the numbe of seconds 
> > invested in each probe. Further, I have plant floral display as a 
> > covariate, as it may influence visitation rates.
> > 
> > I think I have to analyse this desing considering population, pollinator 
> > group and their interaction as fixed effects, and treatment nested 
> > within plant, and its interaction with population and pollinator group, 
> > as random factors.
> > 
> > In SAS terminology, the model looks like this:
> > 
> > proc mixed data=flwfunc.visitflower covtest method=reml;
> >      class site pollclass treatm plantid;
> >      model time = site|pollclass flwinflor / chisq;
> >      random treatm site*treatm pollclass*treatm / subject=plantid;
> >      lsmeans site pollclass site*pollclass;
> > run;
> > 
> > I've been successfully trying lm, but I think is not suitable for random 
> > effects. Thus, I've tried lme, but no success when defining the random 
> > part or trying to interpret the results...
> > 
> > Any help will be welcome!
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From amsa36060 at yahoo.com  Fri Jun 17 20:10:10 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Fri, 17 Jun 2005 11:10:10 -0700 (PDT)
Subject: [R] How to get the values of a vector having the indices?
Message-ID: <20050617181010.94694.qmail@web60412.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050617/9d61c3b5/attachment.pl

From andy_liaw at merck.com  Fri Jun 17 20:21:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Jun 2005 14:21:18 -0400
Subject: [R] vectorization
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9B3@usctmx1106.merck.com>

Here I go again with ave():

mydata$md <- ave(mydata$income, mydata$education, FUN=median, na.rm=TRUE)

IMHO it's one of the most under-rated helper functions in R.

Andy

> From: Dimitri Joe
> 
> Hi there,
> 
> I have a data frame (mydata) with 1 numeric variable (income) 
> and 1 factor (education). I want a new column in this data 
> with the median income for each education level. A obviously 
> inneficient way to do this is
> 
> for ( k in 1: nrow(mydata) )            {
> l <- mydata$education[k]
> mydata$md[k] <- median(mydata$income[mydata$education==l],na.rm=T)
>                                                     }
> 
> Since mydata has nearly 30.000 rows, this will be done not 
> untill the end of this month. I thus need some help for 
> vectorizing this, please.
> 
> Thanks,
> 
> Dimitri
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> 	
> 	
> 		
> _______________________________________________________ 
> 
> Instale o discador agora! http://br.acesso.yahoo.com/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From andy_liaw at merck.com  Fri Jun 17 20:30:55 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Jun 2005 14:30:55 -0400
Subject: [R] trim a string
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9B4@usctmx1106.merck.com>

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/14493.html

Andy

> From: Omar Lakkis
> 
> How to trim the leading and trailing white space off of a string?
> 
>  If the variable is "  E              " I need to convert it to "E".
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From reid_huntsinger at merck.com  Fri Jun 17 20:35:22 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 17 Jun 2005 14:35:22 -0400
Subject: [R] vectorization
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A94A5@uswpmx00.merck.com>

You can use tapply() to compute the medians, as in

meds <- tapply(mydata$inc,INDEX=mydata$ed,FUN=median)

then create a new column with the medians as

medianEd <- meds[mydata$ed]


Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dimitri Joe
Sent: Friday, June 17, 2005 1:01 PM
To: R-Help
Subject: [R] vectorization


Hi there,

I have a data frame (mydata) with 1 numeric variable (income) and 1 factor
(education). I want a new column in this data with the median income for
each education level. A obviously inneficient way to do this is

for ( k in 1: nrow(mydata) )            {
l <- mydata$education[k]
mydata$md[k] <- median(mydata$income[mydata$education==l],na.rm=T)
                                                    }

Since mydata has nearly 30.000 rows, this will be done not untill the end of
this month. I thus need some help for vectorizing this, please.

Thanks,

Dimitri

	[[alternative HTML version deleted]]



	
	
		
_______________________________________________________ 

Instale o discador agora! http://br.acesso.yahoo.com/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bartzk at yahoo-inc.com  Fri Jun 17 20:35:54 2005
From: bartzk at yahoo-inc.com (Kevin Bartz)
Date: Fri, 17 Jun 2005 11:35:54 -0700
Subject: [R] vectorization
Message-ID: <E4EBBAD66D826C41BDED4CCCDCC6D0F13ECCAA@EXCHG01-BUR>

These two lines worked for me:

rst <- tapply(mydata$income, mydata$education, median)
mydata$md <- rst[mydata$education]

Here's my cheesy example:

> mydata <- data.frame(income    = round(rnorm(30000, 55000, 10000)),
+                      education = letters[rbinom(30000, 4, 1/2)+1])
> rst <- tapply(mydata$income, mydata$education, median)
> mydata$md <- rst[mydata$education]
> head(mydata)
  income education      md
1  66223         e 55094.5
2  56830         c 54966.0
3  58035         b 54937.5
4  74045         a 55213.5
5  61327         b 54937.5
6  64150         b 54937.5

Is this what you wanted?

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dimitri Joe
Sent: Friday, June 17, 2005 10:01 AM
To: R-Help
Subject: [R] vectorization

Hi there,

I have a data frame (mydata) with 1 numeric variable (income) and 1
factor (education). I want a new column in this data with the median
income for each education level. A obviously inneficient way to do this
is

for ( k in 1: nrow(mydata) )            {
l <- mydata$education[k]
mydata$md[k] <- median(mydata$income[mydata$education==l],na.rm=T)
                                                    }

Since mydata has nearly 30.000 rows, this will be done not untill the
end of this month. I thus need some help for vectorizing this, please.

Thanks,

Dimitri

	[[alternative HTML version deleted]]



	
	
		
_______________________________________________________ 

Instale o discador agora! http://br.acesso.yahoo.com/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From james.holtman at convergys.com  Fri Jun 17 20:41:53 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Fri, 17 Jun 2005 14:41:53 -0400
Subject: [R] vectorization
In-Reply-To: <00c701c5735e$1a6872e0$1500a8c0@thesahajamach>
Message-ID: <OFAD4BF9F5.AA6CA8A0-ON85257023.006690D9-85257023.0066B68B@nd.convergys.com>





try this:

> x.1 <- data.frame(income=runif(100)*10000,
educ=sample(c('hs','col','none'),100,T))
> x.1
        income educ
1   5930.30882  col
2   5528.83222   hs
3   5967.04041   hs
4   3926.30682   hs
5   2603.75924 none
...........
> x.2 <- tapply(x.1$income, x.1$educ, mean)
> x.2
     col       hs     none
5575.310 4994.921 5481.962
> x.1$median <- x.2[x.1$educ]
> x.1
        income educ   median
1   5930.30882  col 5575.310
2   5528.83222   hs 4994.921
3   5967.04041   hs 4994.921
4   3926.30682   hs 4994.921
5   2603.75924 none 5481.962
6   7398.83325  col 5575.310
7    265.06895   hs 4994.921
.........
>

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Dimitri Joe"                                                                                                        
                      <dimitrijoe at yahoo.com        To:       "R-Help" <r-help at stat.math.ethz.ch>                                           
                      .br>                         cc:                                                                                     
                      Sent by:                     Subject:  [R] vectorization                                                             
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/17/2005 14:00                                                                                                     
                                                                                                                                           




Hi there,

I have a data frame (mydata) with 1 numeric variable (income) and 1 factor
(education). I want a new column in this data with the median income for
each education level. A obviously inneficient way to do this is

for ( k in 1: nrow(mydata) )            {
l <- mydata$education[k]
mydata$md[k] <- median(mydata$income[mydata$education==l],na.rm=T)
                                                    }

Since mydata has nearly 30.000 rows, this will be done not untill the end
of this month. I thus need some help for vectorizing this, please.

Thanks,

Dimitri

             [[alternative HTML version deleted]]






_______________________________________________________

Instale o discador agora! http://br.acesso.yahoo.com/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gerifalte28 at hotmail.com  Fri Jun 17 20:47:37 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Fri, 17 Jun 2005 18:47:37 +0000
Subject: [R] trim a string
In-Reply-To: <3f87cc6d05061710347b4164ff@mail.gmail.com>
Message-ID: <BAY103-F48E302A37234BCA104F78A6F40@phx.gbl>

RSiteSearch("trim") will give you a lot of answers.  You cal also use the 
higher level function trim{R.oo} i.e.:

library(R.oo)
x="    e  "
trim(x)
[1] "e"


>From: Omar Lakkis <uofiowa at gmail.com>
>Reply-To: Omar Lakkis <uofiowa at gmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] trim a string
>Date: Fri, 17 Jun 2005 13:34:36 -0400
>
>How to trim the leading and trailing white space off of a string?
>
>  If the variable is "  E              " I need to convert it to "E".
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jun 17 20:47:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Jun 2005 19:47:37 +0100 (BST)
Subject: [R] trim a string
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9B4@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9B4@usctmx1106.merck.com>
Message-ID: <Pine.LNX.4.61.0506171945280.17897@gannet.stats>

Better code for this purpose is in example(grep).

`white space' and `a blank' are not necessarily the same thing.

On Fri, 17 Jun 2005, Liaw, Andy wrote:

> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/14493.html
>
> Andy
>
>> From: Omar Lakkis
>>
>> How to trim the leading and trailing white space off of a string?
>>
>>  If the variable is "  E              " I need to convert it to "E".

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Rau at demogr.mpg.de  Fri Jun 17 20:53:08 2005
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 17 Jun 2005 20:53:08 +0200
Subject: [R] vectorization
Message-ID: <8B08A3A1EA7AAC41BE24C750338754E69FDA2D@HERMES.demogr.mpg.de>

Hi,


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dimitri Joe
> Sent: Friday, June 17, 2005 7:01 PM
> To: R-Help
> Subject: [R] vectorization
> 
> Hi there,
> 
> I have a data frame (mydata) with 1 numeric variable (income) 
> and 1 factor (education). I want a new column in this data 
> with the median income for each education level. A obviously 
> inneficient way to do this is
> 
I guess the attached code (incl. simulating your data structure) is not
the most efficient way to do this, but at least (I hope so!) it does
what you wanted it to do:


####################### Beginning of Example Code

income <- runif(100)
education <- as.factor(sample(c("high", "middle", "low"),
size=length(income), replace=TRUE))
mydata <- data.frame(inc=income, edu=education)


mymedians <- tapply(X=mydata$inc, INDEX=mydata$edu, FUN=median)

mydata$medians <- ifelse(mydata$edu=="high", mymedians["high"], 0)
mydata$medians <- ifelse(mydata$edu=="middle", mymedians["middle"],
mydata$medians)
mydata$medians <- ifelse(mydata$edu=="low", mymedians["low"],
mydata$medians)

head(mydata)
mymedians

####################### End of Example Code

Maybe one can increase the speed, but I think it is sufficient for your
case of 30,000 cases as you can see from the timing on my desktop
computer here (WinXP Pro SP2, P4, 3GHz, 512MB RAM):

> time.check <- function(){
+   income <- runif(30000)
+   education <- as.factor(sample(c("high", "middle", "low"),
size=length(income), replace=TRUE))
+   mydata <- data.frame(inc=income, edu=education)
+   
+   mymedians <- tapply(X=mydata$inc, INDEX=mydata$edu, FUN=median)
+ 
+   mydata$medians <- ifelse(mydata$edu=="high", mymedians["high"], 0)
+   mydata$medians <- ifelse(mydata$edu=="middle", mymedians["middle"],
mydata$medians)
+   mydata$medians <- ifelse(mydata$edu=="low", mymedians["low"],
mydata$medians)
+   return(NULL)
+ }
> system.time(time.check())
[1] 0.36 0.02 0.38   NA   NA
>   
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status   beta           
major    2              
minor    1.0            
year     2005           
month    04             
day      04             
language R              


Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From mike.rstat at gmail.com  Fri Jun 17 20:53:40 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 17 Jun 2005 11:53:40 -0700
Subject: [R] trim a string
In-Reply-To: <3f87cc6d05061710347b4164ff@mail.gmail.com>
References: <3f87cc6d05061710347b4164ff@mail.gmail.com>
Message-ID: <27db823f05061711531a02c526@mail.gmail.com>

> How to trim the leading and trailing white space off of a string?
> 
>  If the variable is "  E              " I need to convert it to "E".

gsub('^[[:space:]]+', '', "   E     ")
gsub('[[:space:]]+$', '', "   E     ")

as in R-2.1.0/library/base/html/grep.html



From murdoch at stats.uwo.ca  Fri Jun 17 21:24:55 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 17 Jun 2005 15:24:55 -0400
Subject: [R] Data comparison
In-Reply-To: <OFD3EEB0F5.1BD0FBB5-ON86257023.00594280-86257023.005ADF37@fd9ns01.okladot.state.ok.us>
References: <OFD3EEB0F5.1BD0FBB5-ON86257023.00594280-86257023.005ADF37@fd9ns01.okladot.state.ok.us>
Message-ID: <42B32387.5010903@stats.uwo.ca>

On 6/17/2005 12:33 PM, dscully at fd9ns01.okladot.state.ok.us wrote:
> 
> 
> 
> Question :  Is it possible to create a function, using a for ifelse
> function, inside sapply, to compare the values in one data frame to a set
> of upper and lower limits in another data frame, same number of columns.,
> Take  the values which meet the requirements "TRUE"  and create a new data
> frame or table containing the filtered data?   Can you give me a shove in
> the correct direction?      Thanks

I think you don't need a function within sapply.  You just want 
something like this:

 > df1 <- data.frame(x = runif(20))
 > y <- runif(20)
 > df2 <- data.frame(lower = y, upper = y + runif(20))
 >
 > df3 <- cbind(df1, df2)
 >
 > df3[(df2$lower < df1$x) & (df1$x < df2$upper), ]
            x     lower     upper
5  0.6688050 0.4357477 0.6786472
6  0.5608836 0.4649370 0.8596602
8  0.5109508 0.2654933 1.0573998
9  0.5966776 0.3035084 0.9834681
19 0.3787230 0.1894318 0.6783048
20 0.2826356 0.2321261 1.0913582



From spencer.graves at pdf.com  Fri Jun 17 22:06:00 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Jun 2005 13:06:00 -0700
Subject: [R] another aov results interpretation question
In-Reply-To: <20050615214149.3b8997ab@zygiella.local>
References: <20050615214149.3b8997ab@zygiella.local>
Message-ID: <42B32D28.9030900@pdf.com>

	  I commend you to (a) the recent article by Doug Bates on "Fitting 
nonlinear mixed models in R" pp. 27-30 in the latest issue of "R News" 
available from "www.r-project.org" -> Newsletter and (b) Doug's book 
with Pinheiro (2000) Mixed-Effects Models in S and S-PLUS (Springer).  I 
suggest you try the same analysis using in "lmer", library(lme4), and 
"lme", library(nlme), with method = "ML", as explained in Pinheiro and 
Bates.  If you have trouble with this, please post another question on 
this, preferably using either a standard data set distributed with R or 
one of the standard packages or a very simple made-up data set with very 
few observations that you can distribute with your question in a short 
sequence of R commands illustrating something you tried that either 
didn't work or that gave results you don't understand.  I can't do much 
more with the example you've provided below, because I don't know how to 
access the your data.  (And PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html if you haven't already.)

	  hope this helps.
	  spencer graves

RenE J.V. Bertin wrote:

> Hello,
> 
> I'm trying to understand how to interpret the differences in results between two versions of a 2-factor ANOVA with (slightly?) different models, of an observable y, a within-subject factor 'indep' and a grouping factor 'cond' (and a subject 'factor' Snr):
> 
> 
>>summary( aov( y~cond + indep + Error(Snr/indep) ) )
> 
> # example results:
> Error: Snr
>           Df Sum Sq Mean Sq F value Pr(>F)
> cond       1  103.1   103.1   1.425  0.248
> indep      5  159.8    32.0   0.442  0.813
> Residuals 18 1301.6    72.3               
> 
> Error: Snr:indep
>            Df Sum Sq Mean Sq F value Pr(>F)  
> indep       5  20.81    4.16   3.167 0.0104 *
> Residuals 111 145.89    1.31                 
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Error: Within
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 137 22.178   0.162               
> 
> 
>>summary( aov( y~cond * indep + Error(Snr/indep) ) )
> 
> # example results:
> Error: Snr
>            Df Sum Sq Mean Sq F value Pr(>F)
> cond        1  174.6   174.6   1.689  0.213
> indep       5  201.9    40.4   0.391  0.848
> cond:indep  5  124.0    24.8   0.240  0.939
> Residuals  15 1550.8   103.4               
> 
> Error: Snr:indep
>             Df Sum Sq Mean Sq F value Pr(>F)    
> indep        5  73.16   14.63   8.601  5e-07 ***
> cond:indep   5  21.32    4.26   2.507 0.0336 *  
> Residuals  125 212.64    1.70                   
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
> 
> Error: Within
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals 464  507.5     1.1               
> 
> 
> I would like to understand a bit better what the cond:indep line under the second Error:Snr:indep can mean. If I understood correctly, this represents some "higher-order" interaction, but not a real indep/cond interaction. What I also do not grasp is why the indep effect's F and significance is so different between the two models.
> Finally, what does it mean when significant effects are listed under the Error:Within line?
> 
> Is there a good resource available (web, or if not printed) which discusses this kind of question in a way accessible to non statisticians? The last time I checked, manuals like "R for Psychologists" do not really enter into this level of detail...
> 
> Thanks very much in advance,
> R. Bertin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mike.rstat at gmail.com  Fri Jun 17 22:31:31 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 17 Jun 2005 13:31:31 -0700
Subject: [R] an operator for "contains"
Message-ID: <27db823f050617133164fe0140@mail.gmail.com>

k = c(1:9)
if( length( which(k==3) ) ){ print("contained") }else{ print("not contained") }

is therre a simple way to test if a vector/list contains a particular value?

for example an operator, along the lines of: ==

more generally, is the a documentaion page that lists/describes all
such operators?

lastly, if you didn't know the answer to my question, how would you have
gone about searching for an answer?  I tried RSiteSearch() using various
terms, and I opened  R-2.1.0/library/base/html/00Index.html and searched
for various terms.

TIA

======================
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R



From reid_huntsinger at merck.com  Fri Jun 17 22:40:59 2005
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 17 Jun 2005 16:40:59 -0400
Subject: [R] an operator for "contains"
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A94A9@uswpmx00.merck.com>

Yes, %in% or is.element().

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mike R
Sent: Friday, June 17, 2005 4:32 PM
To: r-help at stat.math.ethz.ch
Subject: [R] an operator for "contains"


k = c(1:9)
if( length( which(k==3) ) ){ print("contained") }else{ print("not
contained") }

is therre a simple way to test if a vector/list contains a particular value?

for example an operator, along the lines of: ==

more generally, is the a documentaion page that lists/describes all
such operators?

lastly, if you didn't know the answer to my question, how would you have
gone about searching for an answer?  I tried RSiteSearch() using various
terms, and I opened  R-2.1.0/library/base/html/00Index.html and searched
for various terms.

TIA

======================
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Jun 17 22:41:46 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Jun 2005 13:41:46 -0700
Subject: [R] an operator for "contains"
In-Reply-To: <27db823f050617133164fe0140@mail.gmail.com>
References: <27db823f050617133164fe0140@mail.gmail.com>
Message-ID: <42B3358A.1020309@pdf.com>

	  Does '?"%in%"' or '?match' meet your needs?

	  spencer graves

Mike R wrote:

> k = c(1:9)
> if( length( which(k==3) ) ){ print("contained") }else{ print("not contained") }
> 
> is therre a simple way to test if a vector/list contains a particular value?
> 
> for example an operator, along the lines of: ==
> 
> more generally, is the a documentaion page that lists/describes all
> such operators?
> 
> lastly, if you didn't know the answer to my question, how would you have
> gone about searching for an answer?  I tried RSiteSearch() using various
> terms, and I opened  R-2.1.0/library/base/html/00Index.html and searched
> for various terms.
> 
> TIA
> 
> ======================
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    2                
> minor    1.0              
> year     2005             
> month    04               
> day      18               
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Fri Jun 17 22:42:21 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 17 Jun 2005 16:42:21 -0400
Subject: [R] an operator for "contains"
In-Reply-To: <27db823f050617133164fe0140@mail.gmail.com>
References: <27db823f050617133164fe0140@mail.gmail.com>
Message-ID: <20050617204221.GA26480@psych>

See match().  Also intersect().

The documentation for operators is, I think mostly at the top of
the index page for the base package, the one you searched.  The
relevant one is %in%.  I guess "Value matching" didn't ring the
right bell.

On 06/17/05 13:31, Mike R wrote:
 k = c(1:9)
 if( length( which(k==3) ) ){ print("contained") }else{ print("not contained") }
 
 is therre a simple way to test if a vector/list contains a particular value?
 
 for example an operator, along the lines of: ==
 
 more generally, is the a documentaion page that lists/describes all
 such operators?
 
 lastly, if you didn't know the answer to my question, how would you have
 gone about searching for an answer?  I tried RSiteSearch() using various
 terms, and I opened  R-2.1.0/library/base/html/00Index.html and searched
 for various terms.
 
 TIA
 
 ======================
 platform i686-pc-linux-gnu
 arch     i686
 os       linux-gnu
 system   i686, linux-gnu
 status
 major    2
 minor    1.0
 year     2005
 month    04
 day      18
 language R
 
 ______________________________________________
 R-help at stat.math.ethz.ch mailing list
 https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From ahenningsen at agric-econ.uni-kiel.de  Fri Jun 17 22:42:50 2005
From: ahenningsen at agric-econ.uni-kiel.de (Arne Henningsen)
Date: Fri, 17 Jun 2005 22:42:50 +0200
Subject: [R] an operator for "contains"
In-Reply-To: <27db823f050617133164fe0140@mail.gmail.com>
References: <27db823f050617133164fe0140@mail.gmail.com>
Message-ID: <200506172242.50810.ahenningsen@agric-econ.uni-kiel.de>

%in%

R> k <- 1:9
R> 3 %in% k
[1] TRUE
R> 33 %in% k
[1] FALSE

Arne

On Friday 17 June 2005 22:31, Mike R wrote:
> k = c(1:9)
> if( length( which(k==3) ) ){ print("contained") }else{ print("not
> contained") }
>
> is therre a simple way to test if a vector/list contains a particular
> value?
>
> for example an operator, along the lines of: ==
>
> more generally, is the a documentaion page that lists/describes all
> such operators?
>
> lastly, if you didn't know the answer to my question, how would you have
> gone about searching for an answer?  I tried RSiteSearch() using various
> terms, and I opened  R-2.1.0/library/base/html/00Index.html and searched
> for various terms.
>
> TIA
>
> ======================
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From sarah.goslee at gmail.com  Fri Jun 17 22:45:18 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 17 Jun 2005 16:45:18 -0400
Subject: [R] an operator for "contains"
In-Reply-To: <27db823f050617133164fe0140@mail.gmail.com>
References: <27db823f050617133164fe0140@mail.gmail.com>
Message-ID: <efb536d505061713454d31e412@mail.gmail.com>

On 6/17/05, Mike R <mike.rstat at gmail.com> wrote:
> k = c(1:9)
> if( length( which(k==3) ) ){ print("contained") }else{ print("not contained") }
> 
> is therre a simple way to test if a vector/list contains a particular value?

Yes, several. Here's one:
k <- 1:9
if(any(k == 3)) {
cat("is an element\n") } else {
cat("not an element\n") }

I don't recommend that for floating point numbers, though.

> more generally, is the a documentaion page that lists/describes all
> such operators?

help("==") would get you the help page for the actual binary
operators. Since any() isn't an operator, that won't help.

> lastly, if you didn't know the answer to my question, how would you have
> gone about searching for an answer? 

There are a variety of ways to find answers in R, although if you
aren't certain what your keyword should be, it may take a few tries.

help(), help.search(), and apropos() may all be useful.

Sarah

-- 
Sarah Goslee
http://www.stringpage.com



From tlumley at u.washington.edu  Fri Jun 17 22:47:55 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Jun 2005 13:47:55 -0700 (PDT)
Subject: [R] an operator for "contains"
In-Reply-To: <27db823f050617133164fe0140@mail.gmail.com>
References: <27db823f050617133164fe0140@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0506171342310.385614@homer06.u.washington.edu>

On Fri, 17 Jun 2005, Mike R wrote:

> k = c(1:9)
> if( length( which(k==3) ) ){ print("contained") }else{ print("not contained") }
>
> is therre a simple way to test if a vector/list contains a particular value?

  value %in% vector

> more generally, is the a documentaion page that lists/describes all
> such operators?

No.  You can find binary operators fairly effectively by looking at the 
html help, because they have to have non-alphabetic names (either single 
characters or beginning and ending with %).

However, there are functions such as setdiff() that you might think of as 
binary operators that you wouldn't find this way.

 	-thomas



From james at bovik.org  Fri Jun 17 23:16:32 2005
From: james at bovik.org (James Salsman)
Date: Fri, 17 Jun 2005 14:16:32 -0700
Subject: [R] adjusted R^2 vs. ordinary R^2
Message-ID: <42B33DB0.3010805@bovik.org>

I thought the point of adjusting the R^2 for degrees of
freedom is to allow comparisons about goodness of fit between
similar models with different numbers of data points.  Someone
has suggested to me off-list that this might not be the case.

Is an ADJUSTED R^2 for a four-parameter, five-point model
reliably comparable to the adjusted R^2 of a four-parameter,
100-point model?  If such values can't be reliably compared
with one another, then what is the reasoning behind adjusting
R^2 for degrees of freedom?

What are the good published authorities on this topic?

Sincerely,
James Salsman



From Peter.Lauren at Essexcorp.com  Fri Jun 17 23:21:49 2005
From: Peter.Lauren at Essexcorp.com (Lauren, Peter)
Date: Fri, 17 Jun 2005 17:21:49 -0400
Subject: [R] 3D Scatter Plot
Message-ID: <599093BB9416BA4F93619FAE038A031A039801B5@exchange.essexcorp.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050617/3da7b501/attachment.pl

From mike.rstat at gmail.com  Sat Jun 18 00:28:38 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 17 Jun 2005 15:28:38 -0700
Subject: [R] an operator for "contains"
In-Reply-To: <efb536d505061713454d31e412@mail.gmail.com>
References: <27db823f050617133164fe0140@mail.gmail.com>
	<efb536d505061713454d31e412@mail.gmail.com>
Message-ID: <27db823f05061715282c198fd7@mail.gmail.com>

wow.  thanks everyone for the multitude of suggestions !



From p.dalgaard at biostat.ku.dk  Sat Jun 18 00:42:43 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jun 2005 00:42:43 +0200
Subject: [R] adjusted R^2 vs. ordinary R^2
In-Reply-To: <42B33DB0.3010805@bovik.org>
References: <42B33DB0.3010805@bovik.org>
Message-ID: <x2u0jwmwpo.fsf@turmalin.kubism.ku.dk>

James Salsman <james at bovik.org> writes:

> I thought the point of adjusting the R^2 for degrees of
> freedom is to allow comparisons about goodness of fit between
> similar models with different numbers of data points.  Someone
> has suggested to me off-list that this might not be the case.
> 
> Is an ADJUSTED R^2 for a four-parameter, five-point model
> reliably comparable to the adjusted R^2 of a four-parameter,
> 100-point model?  If such values can't be reliably compared
> with one another, then what is the reasoning behind adjusting
> R^2 for degrees of freedom?


Well, the adjusted R^2 is the percent variance explained by
covariates. So it compares the conditional variance (given covariates)
to the marginal variance. This is less sensitive to DF issues than the
usual R^2, but it does still require that both quantities make sense.
This is not a given, and in particular the R^2 (either one) is quite
dubious when the covariates are chosen by design.

 
> What are the good published authorities on this topic?

Dunno. Common sense should really suffice in this matter.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From mike.rstat at gmail.com  Sat Jun 18 01:11:49 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 17 Jun 2005 16:11:49 -0700
Subject: [R] axis labels vertically
In-Reply-To: <c6039a6105061703538065169@mail.gmail.com>
References: <c6039a6105061702296c0e9e40@mail.gmail.com>
	<42B2A64A.1030009@pdf.com> <c6039a6105061703538065169@mail.gmail.com>
Message-ID: <27db823f05061716115fb4f143@mail.gmail.com>

>  2. I wasn't aware that there is an online-help. (was using the
> tutorial on the web, which is nat at all detailed.
  
help(), help.search(), and apropos() may all be useful.    (thanks Sarah !!)

and RSiteSearch()

You might find this handy too 

example(axis)    (thanks Brian !!)



From mike.rstat at gmail.com  Sat Jun 18 01:13:57 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 17 Jun 2005 16:13:57 -0700
Subject: [R] axis labels vertically
In-Reply-To: <27db823f05061716115fb4f143@mail.gmail.com>
References: <c6039a6105061702296c0e9e40@mail.gmail.com>
	<42B2A64A.1030009@pdf.com> <c6039a6105061703538065169@mail.gmail.com>
	<27db823f05061716115fb4f143@mail.gmail.com>
Message-ID: <27db823f05061716134743f56f@mail.gmail.com>

> >  2. I wasn't aware that there is an online-help. (was using the
> > tutorial on the web, which is nat at all detailed.
> 
> help(), help.search(), and apropos() may all be useful.    (thanks Sarah !!)
> 
> and RSiteSearch()
> 
> You might find this handy too
> 
> example(axis)    (thanks Brian !!)


forgot to mention .....

there are a number of excellent "contributed" tutorials here:

   http://cran.r-project.org/other-docs.html



From lj22 at u.washington.edu  Sat Jun 18 01:41:16 2005
From: lj22 at u.washington.edu (Lei Jiang)
Date: Fri, 17 Jun 2005 16:41:16 -0700 (PDT)
Subject: [R] one factor multiple level anova
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E985@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E985@usctmx1106.merck.com>
Message-ID: <Pine.A41.4.61b.0506171630440.8074@homer03.u.washington.edu>

I am having trouble figuring out this one.

I want to do a one way anvoa with 13 levels. mydata is in a vector with 
length 65. each level has 5 repeats. but it contains NA.

I made mygroup<-gl(13, 5, 65, labels=(...))

anova(lm(mydata ~ mygroup))

it gives following error:
Error in model.frame(formula, rownames, variables, varnames, extras, 
extranames,  :
         invalid variable type

Can you please help?? Millions of thanks.

Lei Jiang

Department of Chemsitry
University of Washington
Box 351700
Seattle, WA 98195
Phone: 206-616-6882
Fax: 206-685-8665



From celebridades at megamail.pt  Sat Jun 18 01:56:35 2005
From: celebridades at megamail.pt (alex diaz)
Date: Sat, 18 Jun 2005 00:56:35 +0100
Subject: [R] aggregate?
Message-ID: <1119052595.42b36333de3f1@paris-hme1>

Dear all:

Here is my problem:

Example data:
dat<-data.frame(x=rep(c("a","b","c","d"),2),y=c(10:17))

If I wanted to aggregate each level of column dat$x I 
could use:
aggregate(dat$y,list(x=dat$x),sum)

But I just want to aggregate two levels (ìcî and ìdî) 
to obtain a new level  ìeî
I am expecting something like:

  x  y
1 a 10
2 b 11
3 e 25
4 a 14
5 b 15
6 e 33


How can I make it?
Thanks in advance and best for all

A. Diaz



-------------------------------------------------
Email Enviado utilizando o serviÅÁo MegaMail



From ap at solarrain.com  Sat Jun 18 02:20:09 2005
From: ap at solarrain.com (ap)
Date: Fri, 17 Jun 2005 20:20:09 -0400 (EDT)
Subject: [R] hist single block plot issue
Message-ID: <Pine.LNX.4.63.0506172017380.25481@localhost.localdomain>


# CASE 1
# The following plots a single cell or block for all three location 0,1,2.
y <- rep(2,8)
hist(y)                # why is this a single block?
hist(y,xlim=c(0,2))    # same thing
hist(y,breaks=2)       # same

# CASE 2
# adding a different value, plots as expected
y <- append(y,0)
hist(y)                # plots as expected


In most cases of the data I have variances in the data so this is not an
issue. In some situations case #1 appears and I would like to 
differentiate it among the other values.

Thanks



From gohidg at gmail.com  Sat Jun 18 03:58:30 2005
From: gohidg at gmail.com (Guohui Ding)
Date: Sat, 18 Jun 2005 09:58:30 +0800
Subject: [R] one factor multiple level anova
In-Reply-To: <Pine.A41.4.61b.0506171630440.8074@homer03.u.washington.edu>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E985@usctmx1106.merck.com>
	<Pine.A41.4.61b.0506171630440.8074@homer03.u.washington.edu>
Message-ID: <f04a1d1d0506171858ae2f51a@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050618/ae073b85/attachment.pl

From deepayan.sarkar at gmail.com  Sat Jun 18 04:11:15 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 17 Jun 2005 21:11:15 -0500
Subject: [R] 3D Scatter Plot
In-Reply-To: <599093BB9416BA4F93619FAE038A031A039801B5@exchange.essexcorp.com>
References: <599093BB9416BA4F93619FAE038A031A039801B5@exchange.essexcorp.com>
Message-ID: <eb555e66050617191142a418cc@mail.gmail.com>

On 6/17/05, Lauren, Peter <Peter.Lauren at essexcorp.com> wrote:
> Hello:
> 
> 
> 
> I would like to be able to do a 3D scatter plot from 3 variables, 2
> independent and 1 dependent.  The closest R function I could find for
> this is "cloud".  However cloud uses, as input, a matrix where the value
> of each matrix element is the dependent variable value at that matrix
> coordinate.  

What makes you think that?

> My problem is that the independent variable values are
> floating point and can be of any value.  Consequently some of the matrix
> bins may not have a value assigned.  An example of the sort of data I
> may have is as follows
> 
> 
> 
> Independent 1          Independent 2            Dependent
> 
> 0.145674                 0.526482534              1.676986
> 
> 0.325634                 0.326385237              2.384384
> 
> 0.235267                 0.352653288              0.356483
> 
> 
> 
> Is there any way to do a 3D scatter plot with this sort of data?

The following should work fine:

df <- 
    data.frame(x = c(0.145674, 0.325634, 0.235267),
               y = c(0.526482534, 0.326385237, 0.352653288),
               z = c(1.676986, 2.384384, 0.356483))

cloud(z ~ x * y, df)

Deepayan



From gohidg at gmail.com  Sat Jun 18 04:21:07 2005
From: gohidg at gmail.com (Guohui Ding)
Date: Sat, 18 Jun 2005 10:21:07 +0800
Subject: [R] hist single block plot issue
In-Reply-To: <Pine.LNX.4.63.0506172017380.25481@localhost.localdomain>
References: <Pine.LNX.4.63.0506172017380.25481@localhost.localdomain>
Message-ID: <f04a1d1d05061719216674cccc@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050618/e11e5598/attachment.pl

From ggrothendieck at gmail.com  Sat Jun 18 05:18:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 17 Jun 2005 23:18:38 -0400
Subject: [R] aggregate?
In-Reply-To: <1119052595.42b36333de3f1@paris-hme1>
References: <1119052595.42b36333de3f1@paris-hme1>
Message-ID: <971536df050617201818259df7@mail.gmail.com>

On 6/17/05, alex diaz <celebridades at megamail.pt> wrote:
> Dear all:
> 
> Here is my problem:
> 
> Example data:
> dat<-data.frame(x=rep(c("a","b","c","d"),2),y=c(10:17))
> 
> If I wanted to aggregate each level of column dat$x I
> could use:
> aggregate(dat$y,list(x=dat$x),sum)
> 
> But I just want to aggregate two levels ("c" and "d")
> to obtain a new level  "e"
> I am expecting something like:
> 
>  x  y
> 1 a 10
> 2 b 11
> 3 e 25
> 4 a 14
> 5 b 15
> 6 e 33


In the example 
- dat$y[3:4] are summed and 
- dat$y[7:8] are summed 
so we assume that what is being requested is that "d" is to
be replaced by "c" and runs of any level are to be summed.

To do that:
- create xx such that a, b, c and d in dat$x are replaced with
  with 1, 2, 3 and 3 in xx.  
- in the second statement calculate a running sum except if the 
  last observation was the same as the current observation then 
  the Last Observation is Carried Forward (locf) so that all entries 
  in a run have the same number. e.g. in this case locf is
  c(1, 2, 3, 3, 4, 5, 6, 6)
- Finally the 'by' collapses dat using locf rbinds the
  resulting rows together to create a data frame.

xx <- ifelse(dat$x == "d", 3, dat$x)
locf <- cumsum(c(TRUE, xx[-1] != xx[-length(xx)]))
f <- function(x) data.frame(x=x[1,1], y=sum(x[,2]))
dat2 <- do.call("rbind", by(dat, locf, f))



From baron at psych.upenn.edu  Sat Jun 18 11:25:56 2005
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 18 Jun 2005 05:25:56 -0400
Subject: [R] Fedora Core 4
Message-ID: <20050618092556.GA12568@psych>

I had installed R from source on Fedora Core 3.  Then I upgraded
to Fedora Core 4, but left R alone.  R worked fine, until I trued
to update.packages().  Even then, many packages updated just
fine, but two of them, cluster and mgcv, failed with the
following error message (using cluster as an example):

gcc -shared -L/usr/local/lib -o cluster.so clara.o daisy.o
dysta.o fanny.o meet.o mona.o pam.o spannel.o twins.o  -lg2c -lm -lgcc_s
/usr/bin/ld: cannot find -lg2c
collect2: ld returned 1 exit status
make: *** [cluster.so] Error 1

I tracked this down as far as I could and got to this page:
http://gcc.gnu.org/fortran/usage.html
in the section about "Compatability with g77."  The problem seems 
to have something to do with Fortran.

Before I invest more time, I wonder if someone knows what to do.
Options:
1. Re-install R from source.
2. Try the RPM for Fedora Core 4.  It isn't in CRAN, but it is in 
Fedora "extras".  R is now part of the Fedora distribution,
albeit one of the many things they have put in "extras" in an
effort to limit the distribution to 4 CDs.
3. Report a bug.  But where?  Is it a bug in R?
4. Something else.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From ripley at stats.ox.ac.uk  Sat Jun 18 14:00:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 18 Jun 2005 13:00:27 +0100 (BST)
Subject: [R] Fedora Core 4
In-Reply-To: <20050618092556.GA12568@psych>
References: <20050618092556.GA12568@psych>
Message-ID: <Pine.LNX.4.61.0506181238030.5152@gannet.stats>

There has been discussion of this on R-devel in the context of the release 
of 2.1.1.

FC4 contains gfortran, not g77 (unless you install everything when you 
get g77 from gcc-3.2.3 in some compatibility RPM).  This is not a bug: R 
compiled on one OS (or even machine) is not expected to work on another.
-lg2c is part of gcc-3.x.y, and gfortran uses something like -lgfortran 
(FC3's gcc4 didn't use the standard name, so I am being cautious).

I think it is fortuitous that R runs in your tests without crashing: 
libRlapack.so is linked against libg2c.0.0 and actually calls entry points 
in it.  (The R executable is also linked but seems to contain no such 
entry points.)

I would suggest installing 2.1.1 on Monday from the sources, or waiting 
for a 2.1.1 RPM.  I believe gcc-3.4.4 is still a better choice than 
gcc-4.0.0 on ix86: it runs faster and there are several cases in which 
4.0.0 appears to give incorrect answers.  I am waiting for the eventual 
release of 4.0.1 to test these more thoroughly.  (See the notes in the 
R-admin manual: fortunately FC4's gcc4 is just recent enough to avoid the 
showstopper bug.)

On Sat, 18 Jun 2005, Jonathan Baron wrote:

> I had installed R from source on Fedora Core 3.  Then I upgraded
> to Fedora Core 4, but left R alone.  R worked fine, until I trued
> to update.packages().  Even then, many packages updated just
> fine, but two of them, cluster and mgcv, failed with the
> following error message (using cluster as an example):
>
> gcc -shared -L/usr/local/lib -o cluster.so clara.o daisy.o
> dysta.o fanny.o meet.o mona.o pam.o spannel.o twins.o  -lg2c -lm -lgcc_s
> /usr/bin/ld: cannot find -lg2c
> collect2: ld returned 1 exit status
> make: *** [cluster.so] Error 1
>
> I tracked this down as far as I could and got to this page:
> http://gcc.gnu.org/fortran/usage.html
> in the section about "Compatability with g77."  The problem seems
> to have something to do with Fortran.

Interesting spelling!

> Before I invest more time, I wonder if someone knows what to do.
> Options:
> 1. Re-install R from source.
> 2. Try the RPM for Fedora Core 4.  It isn't in CRAN, but it is in
> Fedora "extras".  R is now part of the Fedora distribution,
> albeit one of the many things they have put in "extras" in an
> effort to limit the distribution to 4 CDs.
> 3. Report a bug.  But where?  Is it a bug in R?
> 4. Something else.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Jun 18 14:07:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 18 Jun 2005 14:07:53 +0200
Subject: [R] cannot coerce class "matchit" into a data.frame"
In-Reply-To: <45B7EC643773CA4CADFCA4D9E16B05D47098FE@MAIL.dce.state.va.us>
References: <45B7EC643773CA4CADFCA4D9E16B05D47098FE@MAIL.dce.state.va.us>
Message-ID: <42B40E99.1090503@statistik.uni-dortmund.de>

Piccone, Jason E. wrote:

> Greetings fellow humans,
> 
> I am attempting to export a text file after using the MatchIt package to match control with treatment subjects.  I attempted to write.table and used the following syntax:
> 
> "write.table(social,"shaka.txt",sep=" ",quote=FALSE,row.names=FALSE,col.names= FALSE)"
> 
> But received the following error message: 
> "Error in as.data.frame.default(x[[i]], optional = TRUE) : cannot coerce class "matchit" into a data.frame"
> 
> I thought that maybe I had to convert to a data frame, and tinkered with data.frame, as.data.frame, and is.data.frame, but this didn't work (although I could had done it improperly). Please pardon my novice R skills.
> 
> Any guidance will be greatly appreciated.  


"MatchIt" is a special package, so you might have to contact the author 
/ maintainer directly.

[You can save() a MatchIt object, of course, and load() it afterwards, 
if this is sufficient.]

Uwe Ligges

> Cheers,
> 
> 
> Jason E. Piccone, Ph.D.
> Research and Evaluation Specialist
> Department of Correctional Education
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Jun 18 14:27:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 18 Jun 2005 14:27:01 +0200
Subject: [R] k-fold cross validation with neural networks
In-Reply-To: <42B09D6B.8040803@gmx.net>
References: <42B09D6B.8040803@gmx.net>
Message-ID: <42B41315.2010808@statistik.uni-dortmund.de>

Thorstensen Nicolas wrote:

> Hi !
> 
> how can I do a k-fold crossvalidation with neural networks?

For "feed-forward neural networks with a single hidden layer" see 
package "nnet" in bundle "VR" (already installed). For "automatical" 
crossvalidation see, e.g., function errorest() in package "ipred".


> and how can I load matlab data files into R?

Dunno, maybe the "R.matlab" package is of any help.

Uwe Ligges

> thx in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Sat Jun 18 14:33:27 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Sat, 18 Jun 2005 07:33:27 -0500
Subject: [R] Fedora Core 4
In-Reply-To: <Pine.LNX.4.61.0506181238030.5152@gannet.stats>
References: <20050618092556.GA12568@psych>
	<Pine.LNX.4.61.0506181238030.5152@gannet.stats>
Message-ID: <1119098007.822.14.camel@localhost.localdomain>

In follow up to Prof. Ripley's post, having participated in the
referenced r-devel thread, I would note that I have been able to build
Version 2.1.1 beta (2005-06-17) and all CRAN packages from source using
both FC4 fortran compilers.

However, I am back to using g77 based upon Prof. Ripley's
recommendations.

As mentioned, the problem below is likely the mixing and matching of
compiled R executables from FC3 and FC4.

Jon, I would recommend re-compiling a full and clean installation of R
(including any CRAN packages), using which ever version of R you are
running at the moment.

If you did not install the referenced compatibility RPMs during the
initial installation, it is available via the Add/Remove Applications
GUI:

GNOME menu -> Desktop -> System Settings -> Add/Remove Applications

Once the GUI is up, scroll down and select "Development". Then select
the 2 packages in the "Legacy Software Development" set, which includes
the two "compat" sets of programs. You will need your install CD/DVD to
load the RPMS.

Alternatively, you can use:

# yum groupinstall "Legacy Software Development"

from a console as root.

HTH,

Marc Schwartz


On Sat, 2005-06-18 at 13:00 +0100, Prof Brian Ripley wrote:
> There has been discussion of this on R-devel in the context of the release 
> of 2.1.1.
> 
> FC4 contains gfortran, not g77 (unless you install everything when you 
> get g77 from gcc-3.2.3 in some compatibility RPM).  This is not a bug: R 
> compiled on one OS (or even machine) is not expected to work on another.
> -lg2c is part of gcc-3.x.y, and gfortran uses something like -lgfortran 
> (FC3's gcc4 didn't use the standard name, so I am being cautious).
> 
> I think it is fortuitous that R runs in your tests without crashing: 
> libRlapack.so is linked against libg2c.0.0 and actually calls entry points 
> in it.  (The R executable is also linked but seems to contain no such 
> entry points.)
> 
> I would suggest installing 2.1.1 on Monday from the sources, or waiting 
> for a 2.1.1 RPM.  I believe gcc-3.4.4 is still a better choice than 
> gcc-4.0.0 on ix86: it runs faster and there are several cases in which 
> 4.0.0 appears to give incorrect answers.  I am waiting for the eventual 
> release of 4.0.1 to test these more thoroughly.  (See the notes in the 
> R-admin manual: fortunately FC4's gcc4 is just recent enough to avoid the 
> showstopper bug.)
> 
> On Sat, 18 Jun 2005, Jonathan Baron wrote:
> 
> > I had installed R from source on Fedora Core 3.  Then I upgraded
> > to Fedora Core 4, but left R alone.  R worked fine, until I trued
> > to update.packages().  Even then, many packages updated just
> > fine, but two of them, cluster and mgcv, failed with the
> > following error message (using cluster as an example):
> >
> > gcc -shared -L/usr/local/lib -o cluster.so clara.o daisy.o
> > dysta.o fanny.o meet.o mona.o pam.o spannel.o twins.o  -lg2c -lm -lgcc_s
> > /usr/bin/ld: cannot find -lg2c
> > collect2: ld returned 1 exit status
> > make: *** [cluster.so] Error 1
> >
> > I tracked this down as far as I could and got to this page:
> > http://gcc.gnu.org/fortran/usage.html
> > in the section about "Compatability with g77."  The problem seems
> > to have something to do with Fortran.
> 
> Interesting spelling!
> 
> > Before I invest more time, I wonder if someone knows what to do.
> > Options:
> > 1. Re-install R from source.
> > 2. Try the RPM for Fedora Core 4.  It isn't in CRAN, but it is in
> > Fedora "extras".  R is now part of the Fedora distribution,
> > albeit one of the many things they have put in "extras" in an
> > effort to limit the distribution to 4 CDs.
> > 3. Report a bug.  But where?  Is it a bug in R?
> > 4. Something else.
>



From ligges at statistik.uni-dortmund.de  Sat Jun 18 14:34:08 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 18 Jun 2005 14:34:08 +0200
Subject: [R] Trying to build R sources on Windows
In-Reply-To: <AD460179-3ADA-4683-9D9C-7377E150E588@unsw.edu.au>
References: <mailman.9.1119002401.15498.r-help@stat.math.ethz.ch>
	<AD460179-3ADA-4683-9D9C-7377E150E588@unsw.edu.au>
Message-ID: <42B414C0.5000208@statistik.uni-dortmund.de>

Bill Northcott wrote:

> I am trying without success to build the R-2.1.0 sources on Windows  
> 2003 server.
> 
> I have MinGW/bin in front of cygwin/bin in the Windows path.
> 
> However, I try to build, I get failures trying to include headers  
> which are not part of MinGW.  I am definitely using the MinGW  
> compilers, so why are the sources trying to access headers which are  
> not included?
> 
> Examples would be:
> argz.h included from l10nflist.c
> alloca.h from errors.c
> langinfo.h from main.c
> 
> Any clues about what I am doing wrong?

I don't think it is a good idea to leave cygwin in your path for 
compiling R, even using it prior to the build process might be 
dangerous, because you might use another version of cygwin(1).dll than 
the one in the tools collection etc.

Just set your path again without having cygwin in it..., this should be 
sufficient if you have all other relevant tools installed and in your path.

Uwe Ligges


> Bill Northcott
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Jun 18 14:36:50 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 18 Jun 2005 14:36:50 +0200
Subject: [R] How to get the values of a vector having the indices?
In-Reply-To: <20050617181010.94694.qmail@web60412.mail.yahoo.com>
References: <20050617181010.94694.qmail@web60412.mail.yahoo.com>
Message-ID: <42B41562.4030209@statistik.uni-dortmund.de>

Amir Safari wrote:

>  
>  
> Hi
> I want to get the values of a vector which I have its indices. How it is possible?
> For example after clustering , I can access to the indices of the first cluster using:
> first<- which(clusters$clustering==1)
> first give me the indices, but how can I access to the values?

By indexing the original object with these indices. See, e.g., help("[")

If you mean something different, please provide a short reproducible 
example. and explain what you like to get exactly.

Uwe Ligges



> Thanks a lot and have a fun.
> Amir
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From snuggleduck at freemail.hu  Sat Jun 18 16:59:37 2005
From: snuggleduck at freemail.hu (=?ISO-8859-2?Q?B=F3kony_Veronika?=)
Date: Sat, 18 Jun 2005 16:59:37 +0200 (CEST)
Subject: [R] how 'stepAIC' selects?
Message-ID: <freemail.20050518165937.1246@fm14.freemail.hu>

Dear all,
Could anyone please tell me how 'step' or 'stepAIC' works? Does it 
simply select the model with the smallest AIC from all the possible 
models? Or does it perform any test eg. whether the decrease 
in "information content" between a model with a given predictor and 
another without it is "significant"?
Thanks for help!
VB



From ligges at statistik.uni-dortmund.de  Sat Jun 18 17:17:15 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 18 Jun 2005 17:17:15 +0200
Subject: [R] how 'stepAIC' selects?
In-Reply-To: <freemail.20050518165937.1246@fm14.freemail.hu>
References: <freemail.20050518165937.1246@fm14.freemail.hu>
Message-ID: <42B43AFB.5060403@statistik.uni-dortmund.de>

B??kony Veronika wrote:

> Dear all,
> Could anyone please tell me how 'step' or 'stepAIC' works? Does it 
> simply select the model with the smallest AIC from all the possible 
> models? Or does it perform any test eg. whether the decrease 

No. It works *stepwise*, hence does not inspect all possible models.


> in "information content" between a model with a given predictor and 
> another without it is "significant"?

Well, if you believe AIC is the right criteria, an improvement of the 
AIC is enough to choose another model - no further test will be applied.

Uwe Ligges



> Thanks for help!
> VB
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From RRoa at fisheries.gov.fk  Sat Jun 18 15:42:27 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Sat, 18 Jun 2005 11:42:27 -0200
Subject: [R] how 'stepAIC' selects?
Message-ID: <03DCBBA079F2324786E8715BE538968A068D82@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of B??kony Veronika
> Sent: 18 June 2005 14:00
> To: r-help at stat.math.ethz.ch
> Subject: [R] how 'stepAIC' selects?
> 
> 
> Dear all,
> Could anyone please tell me how 'step' or 'stepAIC' works? Does it 
> simply select the model with the smallest AIC from all the possible 
> models? Or does it perform any test eg. whether the decrease 
> in "information content" between a model with a given predictor and 
> another without it is "significant"?
> Thanks for help!
> VB

As a complement to what Uwe said, you may want to consider that Sakamoto 
et al., in p. 84, Ch. 4, Remark 2, of "Akaike Information
Criterion Statistics", 1986, KTK Scientific Publishers, write "From the relation
between AIC and the entropy, if the differences of AIC's for MODEL(j) and 
MODEL(k) is larger than 1~2, then the difference is considered to be 
significant". Unfortunately I could not find any further ellaboration of this 
corollary anywhere in the book, but maybe I didn't look hard enough. Additionally, 
you may want to work with "evidence ratios" and Akaike weights, as recommended 
in Burnham and Anderson, "Model Selection and Multimodel Inference", 2002, 
Springer. I am under the impression that to try to interpret the AIC in terms of 
sampling-distribution inference theory, such as in signficance tests, is to miss the 
point.
Ruben



From peter.rossi at gsb.uchicago.edu  Wed Jun 15 16:10:17 2005
From: peter.rossi at gsb.uchicago.edu (Peter E. Rossi)
Date: Wed, 15 Jun 2005 09:10:17 -0500
Subject: [R] [R-pkgs] Version 1.1-0 of bayesm
Message-ID: <268ce92673d0.2673d0268ce9@gsb.uchicago.edu>

Version 1.1-0 of bayesm is now available on CRAN.

This version includes Bayesian inference for the NBD(Poisson) regression
model and a hierarchical version of the same.  It also includes an additional
dataset with count data and various covariates. 

Comments and suggestions for improvement are most welcome.

................................
 Peter E. Rossi
 Joseph T. and Bernice S. Lewis Professor of Marketing and Statistics
 Editor, Quantitative Marketing and Economics
 Rm 360, Graduate School of Business, U of Chicago
 5807 S. Woodlawn Ave, Chicago IL 60637
 Tel: (773) 702-7513   |   Fax: (773) 834-2081

 peter.rossi at ChicagoGsb.edu
 WWW: http://ChicagoGsb.edu/fac/peter.rossi
SSRN: http://ssrn.com/author=22862
 QME: http://www.kluweronline.com/issn/1570-7156

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Jun 17 21:59:25 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 17 Jun 2005 15:59:25 -0400
Subject: [R] [R-pkgs] Release of new version of caMassClass package and
	new	package caT	ools
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F409E@us-arlington-0668.mail.saic.com>

Hi,

A new version of "caMassClass" package was released today. The package
contain pipeline for processing and classification of protein mass spectra
data.
The main change is off-spinning from the library collection of generic
functions into a new package "caTools". This package, which might be useful
to broader group, is much smaller than "caMassClass" and has fewer
dependencies.

Other changes include:
- Adding to "caMassClass" support for reading and writing mzXML data - a
standard for storing proteomics MS data. 
- Adding more error checking to 'runmean', 'runmin', 'runmax', 'runmad' &
'runquantile' functions
- Writing 3 new functions for round-off-error free sumation: 'sum.exact',
'cumsum.exact' and 'runsum.exact'. 
- Added option to "runmean" function to allow use of above round-off-error
free computation

Jarek
=====================================\====                 
 Jarek Tuszynski, PhD.                               o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                        ">  \
 Jaroslaw.W.Tuszynski at saic.com                   `    \



	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From w.northcott at unsw.edu.au  Sun Jun 19 01:12:48 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Sun, 19 Jun 2005 09:12:48 +1000
Subject: [R] Trying to build R sources on Windows
In-Reply-To: <42B414C0.5000208@statistik.uni-dortmund.de>
References: <mailman.9.1119002401.15498.r-help@stat.math.ethz.ch>
	<AD460179-3ADA-4683-9D9C-7377E150E588@unsw.edu.au>
	<42B414C0.5000208@statistik.uni-dortmund.de>
Message-ID: <4760B03D-3348-4915-B50B-786C8D7B6A9E@unsw.edu.au>

On 18/06/2005, at 10:34 PM, Uwe Ligges wrote:
>> I am trying without success to build the R-2.1.0 sources on  
>> Windows  2003 server.
>> I have MinGW/bin in front of cygwin/bin in the Windows path.

> I don't think it is a good idea to leave cygwin in your path for  
> compiling R, even using it prior to the build process might be  
> dangerous, because you might use another version of cygwin(1).dll  
> than the one in the tools collection etc.
>
> Just set your path again without having cygwin in it..., this  
> should be sufficient if you have all other relevant tools installed  
> and in your path.

Thanks for the thoughts.  I have now got past this road block, and  
have almost managed to build everything.  Along the way, it is clear  
there are some glaring omissions/misleading statements in the current  
documentation.

When I have it all working, I will write it up on R-devel and perhaps  
some one could update the documentation.

Bill Northcott

PS The immediate cause of the header file problems was src/gnuwin32/ 
fixed/h/config.h.  This needs to be regenerated for every build  
configuration.  This is not documented except obliquely in src/ 
gnuwin32/Maintainer.notes



From murdoch at stats.uwo.ca  Sun Jun 19 03:31:58 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 18 Jun 2005 21:31:58 -0400
Subject: [R] Trying to build R sources on Windows
In-Reply-To: <4760B03D-3348-4915-B50B-786C8D7B6A9E@unsw.edu.au>
References: <mailman.9.1119002401.15498.r-help@stat.math.ethz.ch>	<AD460179-3ADA-4683-9D9C-7377E150E588@unsw.edu.au>	<42B414C0.5000208@statistik.uni-dortmund.de>
	<4760B03D-3348-4915-B50B-786C8D7B6A9E@unsw.edu.au>
Message-ID: <42B4CB0E.9090602@stats.uwo.ca>

Bill Northcott wrote:
> On 18/06/2005, at 10:34 PM, Uwe Ligges wrote:
> 
>>>I am trying without success to build the R-2.1.0 sources on  
>>>Windows  2003 server.
>>>I have MinGW/bin in front of cygwin/bin in the Windows path.
> 
> 
>>I don't think it is a good idea to leave cygwin in your path for  
>>compiling R, even using it prior to the build process might be  
>>dangerous, because you might use another version of cygwin(1).dll  
>>than the one in the tools collection etc.
>>
>>Just set your path again without having cygwin in it..., this  
>>should be sufficient if you have all other relevant tools installed  
>>and in your path.
> 
> 
> Thanks for the thoughts.  I have now got past this road block, and  
> have almost managed to build everything.  Along the way, it is clear  
> there are some glaring omissions/misleading statements in the current  
> documentation.
> 
> When I have it all working, I will write it up on R-devel and perhaps  
> some one could update the documentation.
> 
> Bill Northcott
> 
> PS The immediate cause of the header file problems was src/gnuwin32/ 
> fixed/h/config.h.  This needs to be regenerated for every build  
> configuration.  This is not documented except obliquely in src/ 
> gnuwin32/Maintainer.notes

Are you running configure?  Why?

Duncan Murdoch



From w.northcott at unsw.edu.au  Sun Jun 19 06:08:33 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Sun, 19 Jun 2005 14:08:33 +1000
Subject: [R] Trying to build R sources on Windows
In-Reply-To: <42B4CB0E.9090602@stats.uwo.ca>
References: <mailman.9.1119002401.15498.r-help@stat.math.ethz.ch>	<AD460179-3ADA-4683-9D9C-7377E150E588@unsw.edu.au>	<42B414C0.5000208@statistik.uni-dortmund.de>
	<4760B03D-3348-4915-B50B-786C8D7B6A9E@unsw.edu.au>
	<42B4CB0E.9090602@stats.uwo.ca>
Message-ID: <6F430767-2D4F-4B77-84FE-5D96A7A0BB03@unsw.edu.au>

On 19/06/2005, at 11:31 AM, Duncan Murdoch wrote:

>> PS The immediate cause of the header file problems was src/ 
>> gnuwin32/ fixed/h/config.h.  This needs to be regenerated for  
>> every build  configuration.  This is not documented except  
>> obliquely in src/ gnuwin32/Maintainer.notes
>>
>>
>
> Are you running configure?  Why?
>

Yes.  Because it is needed to construct a src/gnuwin32/fixed/h/ 
config.h which is consistent with one's installation.  The config.h  
in the sources won't work on my set up because it assumes a load of  
packages which are not installed.  I started with a clean Windows  
2003 Server and installed, MinGW, Msys and the Rtools, nothing else.

I do now have everything built except the CHM documentation.

Bill Northcott



From 0034058 at fudan.edu.cn  Sun Jun 19 08:05:49 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 19 Jun 2005 14:05:49 +0800
Subject: [R] what does this syntax mean?
Message-ID: <20050619140549.17788823@localhost.localdomain>

i study the code of function ave,but i can understand one line of the syntax.

> ave
function (x, ..., FUN = mean)
{
    n <- length(list(...))
    if (n) {
        g <- interaction(...)
        split(x, g) <- lapply(split(x, g), FUN)
    }
    else x[] <- FUN(x)
    x
}

my question is : what does "split(x, g) <- lapply(split(x, g), FUN)" mean?

thank you!



-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From sympa at cines.fr  Sun Jun 19 13:40:29 2005
From: sympa at cines.fr (SYMPA)
Date: Sun, 19 Jun 2005 13:40:29 +0200
Subject: [R] =?iso-8859-1?q?Mod=E9ration_de_votre_message?=
Message-ID: <200506191140.j5JBeTfM009323@cindns.cines.fr>

Votre message pour la liste vingtcinquante a ??t?? transmis au(x) mod??rateur(s)



From 0034058 at fudan.edu.cn  Sun Jun 19 14:57:35 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 19 Jun 2005 20:57:35 +0800
Subject: [R] how to get the mean of each group?
Message-ID: <20050619205735.24b94917@localhost.localdomain>

i have the data x.1

>x.1 <- data.frame(income=runif(100)*10000,
+ edu=sample(c('hs','col','none'),100,T),y=rnorm(100)*100)

and i want to get :

       income         y
col  5526.726 -11.00956
hs   4196.036 -10.03861
none 4308.111 -28.69549

5526.726 is the mean income for col, 4196.036 is the  mean income for hs......



-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From antoniou at central.ntua.gr  Sun Jun 19 15:04:30 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Sun, 19 Jun 2005 16:04:30 +0300
Subject: [R] error loading Matrix in Mac OSX 10.4
Message-ID: <84E37BB7-1A39-46AC-8959-5B0AC09D522F@central.ntua.gr>

Hello,

I am having trouble loading Matrix (0.96-3) in self-compiled R-2.1.1- 
beta, and self compiled R.app (got it a couple of days ago via:

svn co https://svn.r-project.org/R-packages/trunk/Mac-GUI Mac-GUI )

on Mac OS 10.4.1.

The problem I get when I try to load Matrix is the following: (I know  
I do not need to worry about the warnings, even though it might be  
nice to know why they occur, if there is something simple):


Error in dyn.load(x, as.logical(local), as.logical(now)) :
     unable to load shared library '/Library/Frameworks/R.framework/ 
Resources/library/Matrix/libs/Matrix.so':
   dlopen(/Library/Frameworks/R.framework/Resources/library/Matrix/ 
libs/Matrix.so, 6): Symbol not found: _libintl_dgettext
   Referenced from: /Library/Frameworks/R.framework/Resources/library/ 
Matrix/libs/Matrix.so
   Expected in: flat namespace
In addition: Warning messages:
1: cannot create HTML package index in: make.packages.html()
2: cannot create HTML package index in: make.packages.html()
Error: package/namespace load failed for 'Matrix'

Any suggestions would be greatly appreciated,

Thanks,

Constantinos Antoniou


PS.

 > str(.Platform)
List of 6
$ OS.type   : chr "unix"
$ file.sep  : chr "/"
$ dynlib.ext: chr ".so"
$ GUI       : chr "X11"
$ endian    : chr "big"
$ pkgType   : chr "mac.binary"
 > R.version.string
[1] "R version 2.1.1, 2005-06-14"
 >


--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From ligges at statistik.uni-dortmund.de  Sun Jun 19 15:31:25 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 19 Jun 2005 15:31:25 +0200
Subject: [R] how to get the mean of each group?
In-Reply-To: <20050619205735.24b94917@localhost.localdomain>
References: <20050619205735.24b94917@localhost.localdomain>
Message-ID: <42B573AD.8040802@statistik.uni-dortmund.de>

ronggui wrote:
> i have the data x.1
> 
> 
>>x.1 <- data.frame(income=runif(100)*10000,
> 
> + edu=sample(c('hs','col','none'),100,T),y=rnorm(100)*100)
> 
> and i want to get :
> 
>        income         y
> col  5526.726 -11.00956
> hs   4196.036 -10.03861
> none 4308.111 -28.69549
> 
> 5526.726 is the mean income for col, 4196.036 is the  mean income for hs......
> 
> 
> 

See ?aggregate, ?by and ?tapply

E.g.:
  aggregate(x.1[,c(1,3)], list(x.1[,2]), mean)


Uwe Liggges



From ligges at statistik.uni-dortmund.de  Sun Jun 19 15:39:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 19 Jun 2005 15:39:46 +0200
Subject: [R] what does this syntax mean?
In-Reply-To: <20050619140549.17788823@localhost.localdomain>
References: <20050619140549.17788823@localhost.localdomain>
Message-ID: <42B575A2.2020504@statistik.uni-dortmund.de>

ronggui wrote:

> i study the code of function ave,but i can understand one line of the syntax.
> 
> 
>>ave
> 
> function (x, ..., FUN = mean)
> {
>     n <- length(list(...))
>     if (n) {
>         g <- interaction(...)
>         split(x, g) <- lapply(split(x, g), FUN)
>     }
>     else x[] <- FUN(x)
>     x
> }
> 
> my question is : what does "split(x, g) <- lapply(split(x, g), FUN)" mean?

See ?split and ?lapply.

x is grouped by g, FUNis applied on each group and assigned back to the 
original values.

Uwe Ligges


> thank you!
> 
> 
>



From andy_liaw at merck.com  Sun Jun 19 15:41:38 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 19 Jun 2005 09:41:38 -0400
Subject: [R] what does this syntax mean?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9BB@usctmx1106.merck.com>

That's a call to the replacement function "split<-"(), which is defined as:

> get("split<-.default")
function (x, f, value) 
{
    ix <- split(seq(along = x), f)
    n <- length(value)
    j <- 0
    for (i in ix) {
        j <- j%%n + 1
        x[i] <- value[[j]]
    }
    x
}

Andy

> From: ronggui
> 
> i study the code of function ave,but i can understand one 
> line of the syntax.
> 
> > ave
> function (x, ..., FUN = mean)
> {
>     n <- length(list(...))
>     if (n) {
>         g <- interaction(...)
>         split(x, g) <- lapply(split(x, g), FUN)
>     }
>     else x[] <- FUN(x)
>     x
> }
> 
> my question is : what does "split(x, g) <- lapply(split(x, 
> g), FUN)" mean?
> 
> thank you!
> 
> 
> 
> -- 
> Department of Sociology
> Fudan University,Shanghai
> Blog:http://sociology.yculblog.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From 0034058 at fudan.edu.cn  Sun Jun 19 15:47:48 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Sun, 19 Jun 2005 21:47:48 +0800
Subject: [R] tank you
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9BB@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9BB@usctmx1106.merck.com>
Message-ID: <20050619214748.20f23dc0@localhost.localdomain>

thank you.

when i examined the ?split, i have figure out the answer.
still,thank you very much.and sorry for my igorance.


On Sun, 19 Jun 2005 09:41:38 -0400
"Liaw, Andy" <andy_liaw at merck.com> wrote:

> That's a call to the replacement function "split<-"(), which is defined as:
> 
> > get("split<-.default")
> function (x, f, value) 
> {
>     ix <- split(seq(along = x), f)
>     n <- length(value)
>     j <- 0
>     for (i in ix) {
>         j <- j%%n + 1
>         x[i] <- value[[j]]
>     }
>     x
> }
> 
> Andy
> 
> > From: ronggui
> > 
> > i study the code of function ave,but i can understand one 
> > line of the syntax.
> > 
> > > ave
> > function (x, ..., FUN = mean)
> > {
> >     n <- length(list(...))
> >     if (n) {
> >         g <- interaction(...)
> >         split(x, g) <- lapply(split(x, g), FUN)
> >     }
> >     else x[] <- FUN(x)
> >     x
> > }
> > 
> > my question is : what does "split(x, g) <- lapply(split(x, 
> > g), FUN)" mean?
> > 
> > thank you!
> > 
> > 
> > 
> > -- 
> > Department of Sociology
> > Fudan University,Shanghai
> > Blog:http://sociology.yculblog.com
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> > 
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From ripley at stats.ox.ac.uk  Sun Jun 19 17:43:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 19 Jun 2005 16:43:05 +0100 (BST)
Subject: [R] error loading Matrix in Mac OSX 10.4
In-Reply-To: <84E37BB7-1A39-46AC-8959-5B0AC09D522F@central.ntua.gr>
References: <84E37BB7-1A39-46AC-8959-5B0AC09D522F@central.ntua.gr>
Message-ID: <Pine.LNX.4.61.0506191623290.30747@gannet.stats>

I think you need to ask on the R-sig-mac list with many more details.

My understanding is that Matrix.so should import something like
libintl_dgettext and that the R executable should export it.  Hence my 
suspicion is that your build of R is the problem rather than the 
installation of Matrix.  So the first point to check is if 
libintl_dgettext is being exported by your executable.

On Sun, 19 Jun 2005, Constantinos Antoniou wrote:

> Hello,
>
> I am having trouble loading Matrix (0.96-3) in self-compiled R-2.1.1-
> beta, and self compiled R.app (got it a couple of days ago via:
>
> svn co https://svn.r-project.org/R-packages/trunk/Mac-GUI Mac-GUI )
>
> on Mac OS 10.4.1.
>
> The problem I get when I try to load Matrix is the following: (I know
> I do not need to worry about the warnings, even though it might be
> nice to know why they occur, if there is something simple):
>
>
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>     unable to load shared library '/Library/Frameworks/R.framework/
> Resources/library/Matrix/libs/Matrix.so':
>   dlopen(/Library/Frameworks/R.framework/Resources/library/Matrix/
> libs/Matrix.so, 6): Symbol not found: _libintl_dgettext
>   Referenced from: /Library/Frameworks/R.framework/Resources/library/
> Matrix/libs/Matrix.so
>   Expected in: flat namespace
> In addition: Warning messages:
> 1: cannot create HTML package index in: make.packages.html()
> 2: cannot create HTML package index in: make.packages.html()
> Error: package/namespace load failed for 'Matrix'
>
> Any suggestions would be greatly appreciated,
>
> Thanks,
>
> Constantinos Antoniou
>
>
> PS.
>
> > str(.Platform)
> List of 6
> $ OS.type   : chr "unix"
> $ file.sep  : chr "/"
> $ dynlib.ext: chr ".so"
> $ GUI       : chr "X11"
> $ endian    : chr "big"
> $ pkgType   : chr "mac.binary"
> > R.version.string
> [1] "R version 2.1.1, 2005-06-14"
> >
>
>
> --
> Constantinos Antoniou, Ph.D.
> Department of Transportation Planning and Engineering
> National Technical University of Athens
> 5, Iroon Polytechniou str. GR-15773, Athens, Greece
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From antoniou at central.ntua.gr  Sun Jun 19 17:51:47 2005
From: antoniou at central.ntua.gr (Constantinos Antoniou)
Date: Sun, 19 Jun 2005 18:51:47 +0300
Subject: [R] error loading Matrix in Mac OSX 10.4
In-Reply-To: <Pine.LNX.4.61.0506191623290.30747@gannet.stats>
References: <84E37BB7-1A39-46AC-8959-5B0AC09D522F@central.ntua.gr>
	<Pine.LNX.4.61.0506191623290.30747@gannet.stats>
Message-ID: <B0F397B1-0DB2-4FA9-803F-863F24CBEFC8@central.ntua.gr>

Dear Prof. Ripley,

Thank you for the suggestion. I will subscribe to that list as well  
(and direct my mac-specific issues there)

Constantinos Antoniou

On 19 ÅŒôÅŒÅøÅœÖÅŒÅΩ 2005, at 6:43 ÅŒúÅŒú, Prof Brian Ripley wrote:

> I think you need to ask on the R-sig-mac list with many more details.
>
> My understanding is that Matrix.so should import something like
> libintl_dgettext and that the R executable should export it.  Hence my
> suspicion is that your build of R is the problem rather than the
> installation of Matrix.  So the first point to check is if
> libintl_dgettext is being exported by your executable.
>
> On Sun, 19 Jun 2005, Constantinos Antoniou wrote:
>
>
>> Hello,
>>
>> I am having trouble loading Matrix (0.96-3) in self-compiled R-2.1.1-
>> beta, and self compiled R.app (got it a couple of days ago via:
>>
>> svn co https://svn.r-project.org/R-packages/trunk/Mac-GUI Mac-GUI )
>>
>> on Mac OS 10.4.1.
>>
>> The problem I get when I try to load Matrix is the following: (I know
>> I do not need to worry about the warnings, even though it might be
>> nice to know why they occur, if there is something simple):
>>
>>
>> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>>     unable to load shared library '/Library/Frameworks/R.framework/
>> Resources/library/Matrix/libs/Matrix.so':
>>   dlopen(/Library/Frameworks/R.framework/Resources/library/Matrix/
>> libs/Matrix.so, 6): Symbol not found: _libintl_dgettext
>>   Referenced from: /Library/Frameworks/R.framework/Resources/library/
>> Matrix/libs/Matrix.so
>>   Expected in: flat namespace
>> In addition: Warning messages:
>> 1: cannot create HTML package index in: make.packages.html()
>> 2: cannot create HTML package index in: make.packages.html()
>> Error: package/namespace load failed for 'Matrix'
>>
>> Any suggestions would be greatly appreciated,
>>
>> Thanks,
>>
>> Constantinos Antoniou
>>
>>
>> PS.
>>
>>
>>> str(.Platform)
>>>
>> List of 6
>> $ OS.type   : chr "unix"
>> $ file.sep  : chr "/"
>> $ dynlib.ext: chr ".so"
>> $ GUI       : chr "X11"
>> $ endian    : chr "big"
>> $ pkgType   : chr "mac.binary"
>>
>>> R.version.string
>>>
>> [1] "R version 2.1.1, 2005-06-14"
>>
>>>
>>>
>>
>>
>> --
>> Constantinos Antoniou, Ph.D.
>> Department of Transportation Planning and Engineering
>> National Technical University of Athens
>> 5, Iroon Polytechniou str. GR-15773, Athens, Greece
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting- 
>> guide.html
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>
>



--
Constantinos Antoniou, Ph.D.
Department of Transportation Planning and Engineering
National Technical University of Athens
5, Iroon Polytechniou str. GR-15773, Athens, Greece



From jpai at cc.umanitoba.ca  Sun Jun 19 18:07:44 2005
From: jpai at cc.umanitoba.ca (Jeffrey Pai)
Date: Sun, 19 Jun 2005 11:07:44 -0500
Subject: [R] Creating a R package for Windows XP
Message-ID: <000301c574e9$0a76d890$e41a4c18@JSPaiR40>

Hi:

I'm trying to create a package to pass to someone else to use a group of
functions with help files.  I'm working on Windows XP.

Step One:  I use the example

>       ## two functions and two "data sets" :
>       f <- function(x,y) x+y
>       g <- function(x,y) x-y
>       d <- data.frame(a=1, b=2)
>       e <- rnorm(1000)
> 
>       package.skeleton(name="mypkg", list=c("f","g","d","e"), 
+ path="c:/R/rw2010/library", force=TRUE)
Creating directories ...
Creating DESCRIPTION ...
Creating READMEs ...
Saving functions and data ...
Making help files ...
Created file named 'c:/R/rw2010/library/mypkg/man/f.Rd'.
Edit the file and move it to the appropriate directory.
Created file named 'c:/R/rw2010/library/mypkg/man/g.Rd'.
Edit the file and move it to the appropriate directory.
Created file named 'c:/R/rw2010/library/mypkg/man/d.Rd'.
Edit the file and move it to the appropriate directory.
Created file named 'c:/R/rw2010/library/mypkg/man/e.Rd'.
Edit the file and move it to the appropriate directory.
Done.
Further steps are described in c:/R/rw2010/library/mypkg/README


Step Two:  I load the package "mypkg" from "Packages - load package..." and
get

> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Error in library(pkg, character.only = TRUE) : 
        there is no package called 'mypkg'

The DESCRIPTION file in /library/mypkg contains:

Package: mypkgType: Package
Title: What the package does (short line)
Version: 1.0
Date: 2005-06-19
Author: Who wrote it
Maintainer: Who to complain to <yourfault at somewhere.net>
Description: More about what it does (maybe more than one line)
License: What license is it under?

The first line looks suspicious so I change it to two lines

Package: mypkg
Type: Package

I load the package again and get

> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Error in library(pkg, character.only = TRUE) : 
        'mypkg' is not a valid package -- installed < 2.0.0?


Step-Three:  I read the README file under \mypkg and do

> R CMD build
Error: syntax error


Am I on the right track?  I've read several instructions regarding creating
R packages for Windows on web and installed several files.  I must be
missing something.
Thank.


Best Regards!

Jeffrey Pai
L.A.H. Warren Professor



From ripley at stats.ox.ac.uk  Sun Jun 19 18:27:54 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 19 Jun 2005 17:27:54 +0100 (BST)
Subject: [R] Creating a R package for Windows XP
In-Reply-To: <000301c574e9$0a76d890$e41a4c18@JSPaiR40>
References: <000301c574e9$0a76d890$e41a4c18@JSPaiR40>
Message-ID: <Pine.LNX.4.61.0506191723400.786@gannet.stats>

You have created a source package, and c:/R/rw2010/library is not the 
right place for a source package.  You need to move it somewhere else and 
INSTALL it, as described in the rw-FAQ.  (You need to move it because it 
would be installed into that directory and you are not allowed to clobber 
the sources by doing so.)

Both `Writing R Extensions' and `R Installation and Administration' 
manuals give more details: in particular the latter describes the tools 
you will need to collect.

On Sun, 19 Jun 2005, Jeffrey Pai wrote:

> Hi:
>
> I'm trying to create a package to pass to someone else to use a group of
> functions with help files.  I'm working on Windows XP.
>
> Step One:  I use the example
>
>>       ## two functions and two "data sets" :
>>       f <- function(x,y) x+y
>>       g <- function(x,y) x-y
>>       d <- data.frame(a=1, b=2)
>>       e <- rnorm(1000)
>>
>>       package.skeleton(name="mypkg", list=c("f","g","d","e"),
> + path="c:/R/rw2010/library", force=TRUE)
> Creating directories ...
> Creating DESCRIPTION ...
> Creating READMEs ...
> Saving functions and data ...
> Making help files ...
> Created file named 'c:/R/rw2010/library/mypkg/man/f.Rd'.
> Edit the file and move it to the appropriate directory.
> Created file named 'c:/R/rw2010/library/mypkg/man/g.Rd'.
> Edit the file and move it to the appropriate directory.
> Created file named 'c:/R/rw2010/library/mypkg/man/d.Rd'.
> Edit the file and move it to the appropriate directory.
> Created file named 'c:/R/rw2010/library/mypkg/man/e.Rd'.
> Edit the file and move it to the appropriate directory.
> Done.
> Further steps are described in c:/R/rw2010/library/mypkg/README
>
>
> Step Two:  I load the package "mypkg" from "Packages - load package..." and
> get
>
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in library(pkg, character.only = TRUE) :
>        there is no package called 'mypkg'
>
> The DESCRIPTION file in /library/mypkg contains:
>
> Package: mypkgType: Package
> Title: What the package does (short line)
> Version: 1.0
> Date: 2005-06-19
> Author: Who wrote it
> Maintainer: Who to complain to <yourfault at somewhere.net>
> Description: More about what it does (maybe more than one line)
> License: What license is it under?
>
> The first line looks suspicious so I change it to two lines
>
> Package: mypkg
> Type: Package
>
> I load the package again and get
>
>> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> Error in library(pkg, character.only = TRUE) :
>        'mypkg' is not a valid package -- installed < 2.0.0?
>
>
> Step-Three:  I read the README file under \mypkg and do
>
>> R CMD build
> Error: syntax error
>
>
> Am I on the right track?  I've read several instructions regarding creating
> R packages for Windows on web and installed several files.  I must be
> missing something.
> Thank.
>
>
> Best Regards!
>
> Jeffrey Pai
> L.A.H. Warren Professor
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at gmail.com  Sun Jun 19 19:11:29 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 19 Jun 2005 13:11:29 -0400
Subject: [R] Creating a R package for Windows XP
In-Reply-To: <Pine.LNX.4.61.0506191723400.786@gannet.stats>
References: <000301c574e9$0a76d890$e41a4c18@JSPaiR40>
	<Pine.LNX.4.61.0506191723400.786@gannet.stats>
Message-ID: <971536df05061910116b8291f9@mail.gmail.com>

On 6/19/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> You have created a source package, and c:/R/rw2010/library is not the
> right place for a source package.  You need to move it somewhere else and
> INSTALL it, as described in the rw-FAQ.  (You need to move it because it
> would be installed into that directory and you are not allowed to clobber
> the sources by doing so.)
> 
> Both `Writing R Extensions' and `R Installation and Administration'
> manuals give more details: in particular the latter describes the tools
> you will need to collect.
> 
> On Sun, 19 Jun 2005, Jeffrey Pai wrote:
> 
> > Hi:
> >
> > I'm trying to create a package to pass to someone else to use a group of
> > functions with help files.  I'm working on Windows XP.
> >
> > Step One:  I use the example
> >
> >>       ## two functions and two "data sets" :
> >>       f <- function(x,y) x+y
> >>       g <- function(x,y) x-y
> >>       d <- data.frame(a=1, b=2)
> >>       e <- rnorm(1000)
> >>
> >>       package.skeleton(name="mypkg", list=c("f","g","d","e"),
> > + path="c:/R/rw2010/library", force=TRUE)
> > Creating directories ...
> > Creating DESCRIPTION ...
> > Creating READMEs ...
> > Saving functions and data ...
> > Making help files ...
> > Created file named 'c:/R/rw2010/library/mypkg/man/f.Rd'.
> > Edit the file and move it to the appropriate directory.
> > Created file named 'c:/R/rw2010/library/mypkg/man/g.Rd'.
> > Edit the file and move it to the appropriate directory.
> > Created file named 'c:/R/rw2010/library/mypkg/man/d.Rd'.
> > Edit the file and move it to the appropriate directory.
> > Created file named 'c:/R/rw2010/library/mypkg/man/e.Rd'.
> > Edit the file and move it to the appropriate directory.
> > Done.
> > Further steps are described in c:/R/rw2010/library/mypkg/README
> >
> >
> > Step Two:  I load the package "mypkg" from "Packages - load package..." and
> > get
> >
> >> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> > + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> > Error in library(pkg, character.only = TRUE) :
> >        there is no package called 'mypkg'
> >
> > The DESCRIPTION file in /library/mypkg contains:
> >
> > Package: mypkgType: Package
> > Title: What the package does (short line)
> > Version: 1.0
> > Date: 2005-06-19
> > Author: Who wrote it
> > Maintainer: Who to complain to <yourfault at somewhere.net>
> > Description: More about what it does (maybe more than one line)
> > License: What license is it under?
> >
> > The first line looks suspicious so I change it to two lines
> >
> > Package: mypkg
> > Type: Package
> >
> > I load the package again and get
> >
> >> local({pkg <- select.list(sort(.packages(all.available = TRUE)))
> > + if(nchar(pkg)) library(pkg, character.only=TRUE)})
> > Error in library(pkg, character.only = TRUE) :
> >        'mypkg' is not a valid package -- installed < 2.0.0?
> >
> >
> > Step-Three:  I read the README file under \mypkg and do
> >
> >> R CMD build
> > Error: syntax error
> >
> >
> > Am I on the right track?  I've read several instructions regarding creating
> > R packages for Windows on web and installed several files.  I must be
> > missing something.
> > Thank.

You need to ensure that all the tools are installed and that you have
set your path correctly at which point:
   Rcmd build mypkg --binary
is the minimum to build package mypkg 
(assuming you constructed it right). 

It will be necessary to spend some time to get up speed by reading
all of the following:
- releveant portions of the R manual:    Writing R Extensions
- several short guides found by googling for:  building making R packages
- check out first hit when googling for:  Rtools
- download the source to some other packages as examples



From r.shengzhe at gmail.com  Sun Jun 19 19:34:48 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Sun, 19 Jun 2005 19:34:48 +0200
Subject: [R] plot qda object
Message-ID: <ea57975b05061910341d01fa36@mail.gmail.com>

Hello there,

I use function qda (MASS package) to obtain a qda object, but how to
plot this object?

Thank you,
Shengzhe



From r.shengzhe at gmail.com  Sun Jun 19 19:45:16 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Sun, 19 Jun 2005 19:45:16 +0200
Subject: [R] plot the standard deviation of a specific dot on the figure
Message-ID: <ea57975b05061910456fde365c@mail.gmail.com>

Hello there,

I plot a curve together with the specific dots on the line, but how to
plot the standard deviation on these dots just as the green bands in
the attached file, and also the purple dotted lines?

Thank you,
Shengzhe

From ggrothendieck at gmail.com  Sun Jun 19 20:07:51 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 19 Jun 2005 14:07:51 -0400
Subject: [R] plot qda object
In-Reply-To: <ea57975b05061910341d01fa36@mail.gmail.com>
References: <ea57975b05061910341d01fa36@mail.gmail.com>
Message-ID: <971536df050619110731d39ef9@mail.gmail.com>

On 6/19/05, wu sz <r.shengzhe at gmail.com> wrote:

> I use function qda (MASS package) to obtain a qda object, but how to
> plot this object?

Check out predplot in this file:

system.file("scripts/ch12.R", package = "MASS")



From ggrothendieck at gmail.com  Sun Jun 19 20:09:08 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 19 Jun 2005 14:09:08 -0400
Subject: [R] plot the standard deviation of a specific dot on the figure
In-Reply-To: <ea57975b05061910456fde365c@mail.gmail.com>
References: <ea57975b05061910456fde365c@mail.gmail.com>
Message-ID: <971536df05061911091ca04c02@mail.gmail.com>

On 6/19/05, wu sz <r.shengzhe at gmail.com> wrote:

> I plot a curve together with the specific dots on the line, but how to
> plot the standard deviation on these dots just as the green bands in
> the attached file, and also the purple dotted lines?

There is nothing attached.



From fgibbons at hms.harvard.edu  Sun Jun 19 22:27:16 2005
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Sun, 19 Jun 2005 16:27:16 -0400
Subject: [R] Trouble building R2.1.0 from source on Linux: package VR
Message-ID: <5.2.1.1.2.20050619162028.01094650@email.med.harvard.edu>

Hi,

Following on from suggestions made last week, I decided to install R 2.1.0 
on my Linux machine. I'm running into a problem there however, as shown:

make[1]: Entering directory 
`/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
make[2]: Entering directory 
`/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
begin installing recommended package VR
WARNING: ignoring environment value of R_HOME
tar: Skipping to next header
tar: Archive contains obsolescent base-64 headers
  incomplete literal tree

gzip: VR.tgz: invalid compressed data--format violated
tar: Error exit delayed from previous errors
ERROR: cannot extract package from 'VR.tgz'
make[2]: *** [VR.ts] Error 1
make[2]: Leaving directory 
`/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
make[1]: *** [recommended-packages] Error 2
make[1]: Leaving directory 
`/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
make: *** [stamp-recommended] Error 2

This is most unusual - I must have built R from source five or six times 
going back to 1.5.0 and don't recall any problems like this. Does anyone 
have any suggestions about where I might look for the source of this problem.

In particular, I'm interested in using package 'vsn', whose installation is 
(I believe) blocked by this problem. I downloaded the source from CRAN this 
afternoon, choosing the version 2.1.0 'stable' code. I don't have sysadmin 
privileges on the build machine, but this has never been a problem before - 
I just pass --prefix=$HOME to the configure script. FWIW, I believe the 
sysadmins use Debian.

This is a pretty time-critical matter for me (I wouldn't have chosen to 
upgrade now, were it not for my earlier problem with merge when there are 
empty labels), any assistance greatly appreciated. Thanks in advance.

-Frank

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From plummer at iarc.fr  Sun Jun 19 22:29:48 2005
From: plummer at iarc.fr (plummer@iarc.fr)
Date: Sun, 19 Jun 2005 22:29:48 +0200
Subject: [R] Fedora Core 4
In-Reply-To: <1119098007.822.14.camel@localhost.localdomain>
References: <20050618092556.GA12568@psych>
	<Pine.LNX.4.61.0506181238030.5152@gannet.stats>
	<1119098007.822.14.camel@localhost.localdomain>
Message-ID: <1119212988.42b5d5bc6444f@webmail.iarc.fr>

The Fedora upgrade process should normally install backward compatibility
libraries when it finds an RPM linked to a library (or library version) that
isn't in the new release.  In this case "compat-libf2c-32" provides libg2c on
FC4, and it should be installed on your system if you previously had R for FC3
installed. So R should continue to work.  But it will break when you start
compiling packages on the new system because the configuration information in
the file /usr/lib/R/etc/Makeconf is obsolete. Hopefully this situation will
occur rarely, because there isn't really a way to deal with it automatically.

I'm pretty sure the current R RPM for FC4 on Red Hat Extras is broken. The RPM
build process adds a lot of compiler flags. R 2.1.1 doesn't pass "make
check-all" when built with these flags (and if you try the legacy compilers
Marc mentioned, it doesn't even get past the configure step). A close look at
the spec file from Red Hat shows that this line has been commented out, which
was a mistake.  I'll try to ensure that their next release works. In the mean
time, I am bumping up the Epoch number of my RPMs to 1 so they should be
considered an "upgrade" of the Red Hat ones.

Martyn.

Compiling R 2.1.1 on FC4 with gcc4


Quoting Marc Schwartz <MSchwartz at mn.rr.com>:
> In follow up to Prof. Ripley's post, having participated in the
> referenced r-devel thread, I would note that I have been able to build
> Version 2.1.1 beta (2005-06-17) and all CRAN packages from source using
> both FC4 fortran compilers.
>
> However, I am back to using g77 based upon Prof. Ripley's
> recommendations.
>
> As mentioned, the problem below is likely the mixing and matching of
> compiled R executables from FC3 and FC4.
>
> Jon, I would recommend re-compiling a full and clean installation of R
> (including any CRAN packages), using which ever version of R you are
> running at the moment.
>
> If you did not install the referenced compatibility RPMs during the
> initial installation, it is available via the Add/Remove Applications
> GUI:
>
> GNOME menu -> Desktop -> System Settings -> Add/Remove Applications
>
> Once the GUI is up, scroll down and select "Development". Then select
> the 2 packages in the "Legacy Software Development" set, which includes
> the two "compat" sets of programs. You will need your install CD/DVD to
> load the RPMS.
>
> Alternatively, you can use:
>
> # yum groupinstall "Legacy Software Development"
>
> from a console as root.
>
> HTH,
>
> Marc Schwartz
>
>
> On Sat, 2005-06-18 at 13:00 +0100, Prof Brian Ripley wrote:
> > There has been discussion of this on R-devel in the context of the release
> > of 2.1.1.
> >
> > FC4 contains gfortran, not g77 (unless you install everything when you
> > get g77 from gcc-3.2.3 in some compatibility RPM).  This is not a bug: R
> > compiled on one OS (or even machine) is not expected to work on another.
> > -lg2c is part of gcc-3.x.y, and gfortran uses something like -lgfortran
> > (FC3's gcc4 didn't use the standard name, so I am being cautious).
> >
> > I think it is fortuitous that R runs in your tests without crashing:
> > libRlapack.so is linked against libg2c.0.0 and actually calls entry points
> > in it.  (The R executable is also linked but seems to contain no such
> > entry points.)
> >
> > I would suggest installing 2.1.1 on Monday from the sources, or waiting
> > for a 2.1.1 RPM.  I believe gcc-3.4.4 is still a better choice than
> > gcc-4.0.0 on ix86: it runs faster and there are several cases in which
> > 4.0.0 appears to give incorrect answers.  I am waiting for the eventual
> > release of 4.0.1 to test these more thoroughly.  (See the notes in the
> > R-admin manual: fortunately FC4's gcc4 is just recent enough to avoid the
> > showstopper bug.)
> >
> > On Sat, 18 Jun 2005, Jonathan Baron wrote:
> >
> > > I had installed R from source on Fedora Core 3.  Then I upgraded
> > > to Fedora Core 4, but left R alone.  R worked fine, until I trued
> > > to update.packages().  Even then, many packages updated just
> > > fine, but two of them, cluster and mgcv, failed with the
> > > following error message (using cluster as an example):
> > >
> > > gcc -shared -L/usr/local/lib -o cluster.so clara.o daisy.o
> > > dysta.o fanny.o meet.o mona.o pam.o spannel.o twins.o  -lg2c -lm -lgcc_s
> > > /usr/bin/ld: cannot find -lg2c
> > > collect2: ld returned 1 exit status
> > > make: *** [cluster.so] Error 1
> > >
> > > I tracked this down as far as I could and got to this page:
> > > http://gcc.gnu.org/fortran/usage.html
> > > in the section about "Compatability with g77."  The problem seems
> > > to have something to do with Fortran.
> >
> > Interesting spelling!
> >
> > > Before I invest more time, I wonder if someone knows what to do.
> > > Options:
> > > 1. Re-install R from source.
> > > 2. Try the RPM for Fedora Core 4.  It isn't in CRAN, but it is in
> > > Fedora "extras".  R is now part of the Fedora distribution,
> > > albeit one of the many things they have put in "extras" in an
> > > effort to limit the distribution to 4 CDs.
> > > 3. Report a bug.  But where?  Is it a bug in R?
> > > 4. Something else.
> >
>
-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From carsten.steinhoff at stud.uni-goettingen.de  Sun Jun 19 22:58:05 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Sun, 19 Jun 2005 22:58:05 +0200
Subject: [R] practical help ... solving a system...
Message-ID: <E1Dk6s5-0004Xk-1V@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050619/8e382dc6/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Jun 19 23:22:55 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Jun 2005 23:22:55 +0200
Subject: [R] Trouble building R2.1.0 from source on Linux: package VR
In-Reply-To: <5.2.1.1.2.20050619162028.01094650@email.med.harvard.edu>
References: <5.2.1.1.2.20050619162028.01094650@email.med.harvard.edu>
Message-ID: <x2ekayxcr4.fsf@turmalin.kubism.ku.dk>

Frank Gibbons <fgibbons at hms.harvard.edu> writes:

> Hi,
> 
> Following on from suggestions made last week, I decided to install R 2.1.0 
> on my Linux machine. I'm running into a problem there however, as shown:
> 
> make[1]: Entering directory 
> `/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
> make[2]: Entering directory 
> `/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
> begin installing recommended package VR
> WARNING: ignoring environment value of R_HOME
> tar: Skipping to next header
> tar: Archive contains obsolescent base-64 headers
>   incomplete literal tree
> 
> gzip: VR.tgz: invalid compressed data--format violated
> tar: Error exit delayed from previous errors
> ERROR: cannot extract package from 'VR.tgz'
> make[2]: *** [VR.ts] Error 1
> make[2]: Leaving directory 
> `/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory 
> `/d0/home/fgibbons/tmp/R2.1.0/R-2.1.0/src/library/Recommended'
> make: *** [stamp-recommended] Error 2
> 
> This is most unusual - I must have built R from source five or six times 
> going back to 1.5.0 and don't recall any problems like this. Does anyone 
> have any suggestions about where I might look for the source of this problem.
> 
> In particular, I'm interested in using package 'vsn', whose installation is 
> (I believe) blocked by this problem. I downloaded the source from CRAN this 
> afternoon, choosing the version 2.1.0 'stable' code. I don't have sysadmin 
> privileges on the build machine, but this has never been a problem before - 
> I just pass --prefix=$HOME to the configure script. FWIW, I believe the 
> sysadmins use Debian.
> 
> This is a pretty time-critical matter for me (I wouldn't have chosen to 
> upgrade now, were it not for my earlier problem with merge when there are 
> empty labels), any assistance greatly appreciated. Thanks in advance.

Well, at this point, you might as well wait for 2.1.1 to come out
tomorrow around noon (CEST). If you have R_HOME in your environment,
you should get rid of it, but I don't see how that could cause the
subsequent errors. My best guess is that the tarball got botched in
the download somehow.

BTW, "Linux" does not suffice for us to identify your distribution
much less the version. I think /etc/issue will tell you in most cases
if you don't know.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Soren.Hojsgaard at agrsci.dk  Mon Jun 20 00:26:00 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Mon, 20 Jun 2005 00:26:00 +0200
Subject: [R] How to sample from a linear mixed model
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FB@DJFPOST01.djf.agrsci.dk>

I would like to draw a sample from a linear mixed model y=Xb+Zu+e which has been fitted with lme(), i.e. a model y ~ N(Xb, C), where C=Z cov(u) Z' + cov(e).
I've tried to figure out how to extract C from an lme object, because that would solve my problem when also using the predict() function, but without any luck.
Can anyone help on that?



From spencer.graves at pdf.com  Mon Jun 20 01:05:29 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 19 Jun 2005 16:05:29 -0700
Subject: [R] practical help ... solving a system...
In-Reply-To: <E1Dk6s5-0004Xk-1V@s2.stud.uni-goettingen.de>
References: <E1Dk6s5-0004Xk-1V@s2.stud.uni-goettingen.de>
Message-ID: <42B5FA39.2020405@pdf.com>

	  "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html".  If you had provided more 
information about what you tried, it might be easier for someone else to 
offer effective help.  In particular, what did you try with fitdistr, 
and why do you think it didn't work (e.g., what error message did you 
get)?

	  The following is a minor modification of examples in the "fitdistr" 
help page:

 > library(MASS)
 > set.seed(123)
 > x <- rbinom(100, 10, 0.4)
 > (fit <- fitdistr(x, dbinom, start=list(prob=0.5), size=10))
       prob
   0.39804687
  (0.01547943)
Warning message:
one-diml optimization by Nelder-Mead is unreliable: use optimize in: 
optim(start, mylogfn, x = x, hessian = TRUE, ...)
 >
	  The difference between prob=0.4 that I simulated and the 0.39804687 
estimated is shockingly small.  (No, I didn't run 100 simulations with 
different seeds and pick the best one.)

	  If I were concerned about the warning, I could list "fitdist" and 
read the code.  I might even copy it into a script file and walk through 
it line by line.  When checked the code for "fitdistr" just now, I 
learned that it calls "optim", and "optim" provides other estimation 
methods, e.g., BFGR and CG.  The following are the results from trying 
those two:

(fit.BFGS <- fitdistr(x, dbinom, start=list(prob=0.5),
+ size=10, method="BFGS"))
       prob
   0.39800028
  (0.01547882)
Warning messages:
1: NaNs produced in: dbinom(x, size, prob, log)
2: NaNs produced in: dbinom(x, size, prob, log)
3: NaNs produced in: dbinom(x, size, prob, log)
4: NaNs produced in: dbinom(x, size, prob, log)
5: NaNs produced in: dbinom(x, size, prob, log)
 > (fit.CG <- fitdistr(x, dbinom, start=list(prob=0.5),
+ size=10, method="CG"))
       prob
   0.39800001
  (0.01547881)
Warning messages:
1: NaNs produced in: dbinom(x, size, prob, log)
2: NaNs produced in: dbinom(x, size, prob, log)
3: NaNs produced in: dbinom(x, size, prob, log)
4: NaNs produced in: dbinom(x, size, prob, log)
5: NaNs produced in: dbinom(x, size, prob, log)

	  If I'm concerned about these warnings, I can replace "dbinom" by a 
local copy that prints the value of "prob" each time it's called:

dbinom. <- function (x, size, prob, log = FALSE){
	cat(prob, "")
.Internal(dbinom(x, size, prob, log))
}
(fit.BFGS <- fitdistr(x, dbinom., start=list(prob=0.5),
	size=10, method="BFGS"))
0.5 0.501 0.499 -407.5005 -81.10011 -15.82002 -2.764004 -0.1528009 
0.3694398 0.3704398 0.3684398 0.3996073 0.4006073 0.3986073 0.3980445 
0.3990445 0.3970445 0.2133659 0.3611088 0.3906574 0.3965671 0.3977490 
0.3979854 0.3989854 0.3969854 0.3980003 0.45997 0.4103942 0.4004791 
0.3984960 0.3980994 0.3980201 0.3980043 0.3980011 0.3980004 0.3980003 
0.3980003 0.3980003 0.3980003 0.3980003 0.4000003 0.3980003 0.3980003 
0.3960003       prob
   0.39800028
  (0.01547882)
Warning messages:
1: NaNs produced in: dbinom(x, size, prob, log)
2: NaNs produced in: dbinom(x, size, prob, log)
3: NaNs produced in: dbinom(x, size, prob, log)
4: NaNs produced in: dbinom(x, size, prob, log)
5: NaNs produced in: dbinom(x, size, prob, log)
 >
	  It's no wonder dbinom produced NaNs:  It does that when prob < 0.

	  hope this helps.
	  spencer graves

p.s.  MASS = "Modern Applied Statistics with S" by Venables and Ripley.. 
  Have you seen this book?  I've learned a lot from it.

Carsten Steinhoff wrote:
> Hello,
>  
> I want to estimate the parameters of a binomial distributed rv using MLE.
> Other distributions will follow.
> The equation system to solve is not very complex, but I've never done such
> work in R and don't have any idea how to start...
>  
> The system is:
>  
> (1)     n*P = X
>  
> (2)    [sum {from j=0 to J-1} Y{j} /(n-j)] = -n * ln (1-X / n)
>  
>  
> where    * only X is given (empirical mean)
>              * J is maximum observed
>              * Y is the number of observations above j
>              * n and P are the parameters of the binomial distribution to
> find....
>  
> Who could help me with an example-solution for my "first" distribution ? I
> also need a hint how to make the sum-element.
>  
> Maybe there's another - more simple - way to estimate the parameters...
> first I tried via "FITDISTR" but without success.
>  
> Thanx a lot for your help.
>  
> Carsten
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r.shengzhe at gmail.com  Mon Jun 20 02:21:45 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Mon, 20 Jun 2005 02:21:45 +0200
Subject: [R] plot the deviation of a specific dot on the figure
Message-ID: <ea57975b05061917216af3e057@mail.gmail.com>

Hello there,

I plot a curve together with the specific dots on the line, but how to
plot the standard deviation on these dots just as the green bands in
the attached file, and also the purple dotted lines?

Thank you,
Shengzhe

From ggrothendieck at gmail.com  Mon Jun 20 02:33:36 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 19 Jun 2005 20:33:36 -0400
Subject: [R] plot the deviation of a specific dot on the figure
In-Reply-To: <ea57975b05061917216af3e057@mail.gmail.com>
References: <ea57975b05061917216af3e057@mail.gmail.com>
Message-ID: <971536df05061917334fca5bb5@mail.gmail.com>

On 6/19/05, wu sz <r.shengzhe at gmail.com> wrote:
> Hello there,
> 
> I plot a curve together with the specific dots on the line, but how to
> plot the standard deviation on these dots just as the green bands in
> the attached file, and also the purple dotted lines?
> 

There is still no attached file.  Try:

library(gplots)
example(plotCI)
example(barplot2)

in case any of those examples are what you are looking for.



From dmbates at gmail.com  Mon Jun 20 02:56:17 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 19 Jun 2005 19:56:17 -0500
Subject: [R] Fwd:  How to sample from a linear mixed model
In-Reply-To: <40e66e0b050619175510957b79@mail.gmail.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FB@DJFPOST01.djf.agrsci.dk>
	<40e66e0b050619175510957b79@mail.gmail.com>
Message-ID: <40e66e0b05061917563ec5e28d@mail.gmail.com>

I forgot to cc: R-help on this reply.

---------- Forwarded message ----------
From: Douglas Bates <dmbates at gmail.com>
Date: Jun 19, 2005 7:55 PM
Subject: Re: [R] How to sample from a linear mixed model
To: S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk>


On 6/19/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> I would like to draw a sample from a linear mixed model y=Xb+Zu+e which has been fitted with lme(), i.e. a model y ~ N(Xb, C), where C=Z cov(u) Z' + cov(e).
> I've tried to figure out how to extract C from an lme object, because that would solve my problem when also using the predict() function, but without any luck.
> Can anyone help on that?

C is not stored in an lme object.  In fact it is never created.
(Consider the dimensions of this matrix.  It could be huge.)

The easiest way to simulate data from a linear mixed model is to
simulate u and e then form Xb+Zu+e



From wuming.gong at gmail.com  Mon Jun 20 03:45:43 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Mon, 20 Jun 2005 09:45:43 +0800
Subject: [R] how 'stepAIC' selects?
In-Reply-To: <freemail.20050518165937.1246@fm14.freemail.hu>
References: <freemail.20050518165937.1246@fm14.freemail.hu>
Message-ID: <b428d06d0506191845f5fba5c@mail.gmail.com>

Hi Veronika,

StepAIC does not do any test. But you may do a likelihood ratio test
on the difference between deviance of two models to judge which model
to use, simply like this,

> 1 - pchisq(deviance.i - deviance.j, df.i - df.j)

And then check whether the returned p value is small enough... 

Wuming

On 6/18/05, B??kony Veronika <snuggleduck at freemail.hu> wrote:
> Dear all,
> Could anyone please tell me how 'step' or 'stepAIC' works? Does it
> simply select the model with the smallest AIC from all the possible
> models? Or does it perform any test eg. whether the decrease
> in "information content" between a model with a given predictor and
> another without it is "significant"?
> Thanks for help!
> VB
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Mon Jun 20 04:07:12 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 20 Jun 2005 10:07:12 +0800
Subject: [R] error when installing Matrix
Message-ID: <20050620100712.1a14a0cb@localhost.localdomain>

> version
         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

OS:debian linux
# R CMD INSTALL /home/ronggui/Matrix_0.96-3.tar.gz 

.........
s-3  -L/usr/lib/R/lib -lR
/usr/bin/ld: cannot find -lblas-3
collect2: ld returned 1 exit status
make: *** [Matrix.so] Error 1
ERROR: compilation failed for package 'Matrix'
** Removing '/usr/local/lib/R/site-library/Matrix'
** Restoring previous '/usr/local/lib/R/site-library/Matrix'

what have i missed?thank you!


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From Meredith.Briggs at team.telstra.com  Mon Jun 20 05:04:39 2005
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Mon, 20 Jun 2005 13:04:39 +1000
Subject: [R] Which package contains Fishers Exact Test?
Message-ID: <3B5823541A25D311B3B90008C7F905641A72AA5C@ntmsg0092.corpmail.telstra.com.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050620/3ed01eb3/attachment.pl

From edd at debian.org  Mon Jun 20 05:18:19 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 19 Jun 2005 22:18:19 -0500
Subject: [R] error when installing Matrix
In-Reply-To: <20050620100712.1a14a0cb@localhost.localdomain>
References: <20050620100712.1a14a0cb@localhost.localdomain>
Message-ID: <17078.13691.776041.568542@basebud.nulle.part>


On 20 June 2005 at 10:07, ronggui wrote:
| OS:debian linux
| # R CMD INSTALL /home/ronggui/Matrix_0.96-3.tar.gz 
| 
| .........
| s-3  -L/usr/lib/R/lib -lR
| /usr/bin/ld: cannot find -lblas-3
| collect2: ld returned 1 exit status
| make: *** [Matrix.so] Error 1
| ERROR: compilation failed for package 'Matrix'
| ** Removing '/usr/local/lib/R/site-library/Matrix'
| ** Restoring previous '/usr/local/lib/R/site-library/Matrix'
| 
| what have i missed?thank you!

$ apt-get install r-base-dev

r-base-dev is an 'almost virtual' package with depends that will be satisfied
by either the refblas or any of the atlas-* (cpu-specific) optimised
libraries.  You are lacking these.

Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From andy_liaw at merck.com  Mon Jun 20 05:23:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 19 Jun 2005 23:23:04 -0400
Subject: [R] Which package contains Fishers Exact Test?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9C0@usctmx1106.merck.com>

help.search("fisher exact test") says:

fisher.test(stats)       Fisher's Exact Test for Count Data

Andy

> From: Briggs, Meredith M
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From edd at debian.org  Mon Jun 20 05:30:06 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 19 Jun 2005 22:30:06 -0500
Subject: [R] error when installing Matrix
In-Reply-To: <17078.13691.776041.568542@basebud.nulle.part>
References: <20050620100712.1a14a0cb@localhost.localdomain>
	<17078.13691.776041.568542@basebud.nulle.part>
Message-ID: <17078.14398.729662.141488@basebud.nulle.part>


On 19 June 2005 at 22:18, Dirk Eddelbuettel wrote:
| 
| On 20 June 2005 at 10:07, ronggui wrote:
| | OS:debian linux
| | # R CMD INSTALL /home/ronggui/Matrix_0.96-3.tar.gz 
| | 
| | .........
| | s-3  -L/usr/lib/R/lib -lR
| | /usr/bin/ld: cannot find -lblas-3
| | collect2: ld returned 1 exit status
| | make: *** [Matrix.so] Error 1
| | ERROR: compilation failed for package 'Matrix'
| | ** Removing '/usr/local/lib/R/site-library/Matrix'
| | ** Restoring previous '/usr/local/lib/R/site-library/Matrix'
| | 
| | what have i missed?thank you!
| 
| $ apt-get install r-base-dev
| 
| r-base-dev is an 'almost virtual' package with depends that will be satisfied
| by either the refblas or any of the atlas-* (cpu-specific) optimised
| libraries.  You are lacking these.

Oh, and of course also for the Matrix package thanks to Doug's Debian
package:

$ apt-get install r-cran-matrix

Hope this helps, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From jsundvik at mail.student.oulu.fi  Mon Jun 20 07:30:58 2005
From: jsundvik at mail.student.oulu.fi (Johanna Sundvik)
Date: Mon, 20 Jun 2005 08:30:58 +0300
Subject: [R] reading csv-data
In-Reply-To: <42B2EAB4.4040802@bio.ntnu.no>
References: <1119016200.42b2d5087e227@webmail.oulu.fi>
	<Pine.A41.4.61b.0506170752530.371906@homer07.u.washington.edu>
	<42B2EAB4.4040802@bio.ntnu.no>
Message-ID: <1119245458.42b65492dc170@webmail.oulu.fi>

Thank you all for your help. 

There was an easy solution to my problem:

read.csv2("example.csv", dec=".", header=TRUE)
or 
Mean1 <- as.numeric(as.character(Mean1))

-Johanna


Lainaus Ivar Herfindal <ivar.herfindal at bio.ntnu.no>:

> 
> 
> Thomas Lumley wrote:
> 
> > On Fri, 17 Jun 2005, Johanna Sundvik wrote:
> > 
> >>However, this "Mean1" is categorical when it should be real numbers.
> >>
> >>
> >>>Mean1
> >>
> >>[1] 4.4332  8.5113  35.1624 9.1693  2.974   65.1578 43.2241 3.1278  5.3364
> >>Levels: 2.974 3.1278 35.1624 4.4332 43.2241 5.3364 65.1578 8.5113 9.1693
> >>
> >>Why R does not understand that this should be real numbers? What am I
> doing
> >>wrong here? Thanks for your help.
> >>
> > 
> > 
> > Your files must have some entries that are not numbers, such as "." or 
> > something.  R then can't tell that the field is supposed to be numeric. 
> > This may happen with missing data, in which case the na.strings= argument 
> > can be used to tell R how missing data are specified.
> > 
> > You can convert the data to numeric as described in FAQ 7.10
> > 
> >  	-thomas
> > 
> I think the problem can be that you use read.csv2(), which expect a 
> comma (",") as decimal-indicator (as is common in Scandinavia), and a 
> semi-colon (";") as separator between columns. Either you should try 
> read.csv(), or you can try read.csv2("example.csv", dec=".", header=TRUE)
> 
> Have a look at ?read.csv (read.csv2 is in the same help-text).
> 
> Ivar
> 
> ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Mon Jun 20 08:37:26 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 20 Jun 2005 08:37:26 +0200
Subject: [R] how 'stepAIC' selects?
In-Reply-To: <b428d06d0506191845f5fba5c@mail.gmail.com>
References: <freemail.20050518165937.1246@fm14.freemail.hu>
	<b428d06d0506191845f5fba5c@mail.gmail.com>
Message-ID: <42B66426.3030602@statistik.uni-dortmund.de>

Wuming Gong wrote:
> Hi Veronika,
> 
> StepAIC does not do any test. But you may do a likelihood ratio test
> on the difference between deviance of two models to judge which model
> to use, simply like this,
> 
> 
>>1 - pchisq(deviance.i - deviance.j, df.i - df.j)
> 
> 
> And then check whether the returned p value is small enough... 


... but then you are mixing different concepts.

Uwe Ligges


> Wuming
> 
> On 6/18/05, B??kony Veronika <snuggleduck at freemail.hu> wrote:
> 
>>Dear all,
>>Could anyone please tell me how 'step' or 'stepAIC' works? Does it
>>simply select the model with the smallest AIC from all the possible
>>models? Or does it perform any test eg. whether the decrease
>>in "information content" between a model with a given predictor and
>>another without it is "significant"?
>>Thanks for help!
>>VB
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From szlevine at nana.co.il  Mon Jun 20 10:29:11 2005
From: szlevine at nana.co.il (Stephen)
Date: Mon, 20 Jun 2005 11:29:11 +0300
Subject: [R] Mixed model
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD649@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050620/fbd9d983/attachment.pl

From guillaume.allain at cbconseil.com  Mon Jun 20 11:28:25 2005
From: guillaume.allain at cbconseil.com (Guillaume Allain)
Date: Mon, 20 Jun 2005 11:28:25 +0200
Subject: [R] [interfacing C][dist]
In-Reply-To: <mailman.8.1119175200.23925.r-help@stat.math.ethz.ch>
References: <mailman.8.1119175200.23925.r-help@stat.math.ethz.ch>
Message-ID: <7d397c764d8c3c430613a92630ac2f48@cbconseil.com>

Hi,

Is it possible to access to the C source code of the "dist" function? I 
mean when u do read the dist code, it points almost directly to a C 
function  :

d <- .C("R_distance", x = as.double(x), nr = N, nc = ncol(x),
         d = double(N * (N - 1)/2), diag = as.integer(FALSE),
         method = as.integer(method), p = as.double(p), DUP = FALSE,
         NAOK = TRUE, PACKAGE = "stats")$d

I would like to access to the R_distance.c in order to add a home made 
distance for classification distance.

Thanks for your help, Guillaume



From r.hankin at noc.soton.ac.uk  Mon Jun 20 11:32:44 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 20 Jun 2005 10:32:44 +0100
Subject: [R] sweep() and recycling
Message-ID: <f86a72b14e266a46a22c3db3f5d8bc89@soc.soton.ac.uk>

Hi


I had a hard-to-find bug in some of my code the other day, which I 
eventually
traced to my misusing of sweep().

I would expect sweep() to give
me a warning if the elements don't recycle nicely, but

  X <- matrix(1:36,6,6)
  sweep(X,1,1:5,"+")
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    9   16   23   30   32
[2,]    4   11   18   25   27   34
[3,]    6   13   20   22   29   36
[4,]    8   15   17   24   31   38
[5,]   10   12   19   26   33   40
[6,]    7   14   21   28   35   37

gives no warning, even though (5,36)=1.

Also,


sweep(X,1,c(1,1000),"+")

and

sweep(X,2,c(1,1000),"+")


behave as expected,   But


sweep(X,1,1:5,"+")

and

  sweep(X,2,1:5,"+")



are identical!


I find this confusing, because I interpret sweep()'s second
argument as the margin along which the third argument is fourth 
argumented
this interpretation breaks down in a non-obvious way if elements
are left over after recycling.

If sweep() had given a warning, I would have found my mistake much 
faster.

Could we  make sweep() issue a warning
under these circumstances or, failing that, add a warning to sweep.Rd 
saying
that no warning is given in cases like this?



--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From dimitris.rizopoulos at med.kuleuven.be  Mon Jun 20 11:49:04 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 20 Jun 2005 11:49:04 +0200
Subject: [R] [interfacing C][dist]
References: <mailman.8.1119175200.23925.r-help@stat.math.ethz.ch>
	<7d397c764d8c3c430613a92630ac2f48@cbconseil.com>
Message-ID: <009901c5757d$4b84bfc0$0540210a@www.domain>

you can access "distance.c" in R-sources in the directory 
\src\library\stats\src

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Guillaume Allain" <guillaume.allain at cbconseil.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 20, 2005 11:28 AM
Subject: [R] [interfacing C][dist]


> Hi,
>
> Is it possible to access to the C source code of the "dist" 
> function? I
> mean when u do read the dist code, it points almost directly to a C
> function  :
>
> d <- .C("R_distance", x = as.double(x), nr = N, nc = ncol(x),
>         d = double(N * (N - 1)/2), diag = as.integer(FALSE),
>         method = as.integer(method), p = as.double(p), DUP = FALSE,
>         NAOK = TRUE, PACKAGE = "stats")$d
>
> I would like to access to the R_distance.c in order to add a home 
> made
> distance for classification distance.
>
> Thanks for your help, Guillaume
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Mon Jun 20 11:53:59 2005
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 20 Jun 2005 11:53:59 +0200 (CEST)
Subject: [R] [interfacing C][dist]
In-Reply-To: <7d397c764d8c3c430613a92630ac2f48@cbconseil.com>
Message-ID: <Pine.LNX.4.44.0506201151150.13814-100000@reclus.nhh.no>

On Mon, 20 Jun 2005, Guillaume Allain wrote:

Either download the source code and unpack the file you need, or use the 
online anonymous SVN facility:

https://svn.r-project.org/R/trunk/src/library/stats/src/distance.c


> Hi,
> 
> Is it possible to access to the C source code of the "dist" function? I 
> mean when u do read the dist code, it points almost directly to a C 
> function  :
> 
> d <- .C("R_distance", x = as.double(x), nr = N, nc = ncol(x),
>          d = double(N * (N - 1)/2), diag = as.integer(FALSE),
>          method = as.integer(method), p = as.double(p), DUP = FALSE,
>          NAOK = TRUE, PACKAGE = "stats")$d
> 
> I would like to access to the R_distance.c in order to add a home made 
> distance for classification distance.
> 
> Thanks for your help, Guillaume
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Helleveien 30, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 95 43
e-mail: Roger.Bivand at nhh.no



From Allan at STATS.uct.ac.za  Mon Jun 20 12:11:05 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 20 Jun 2005 12:11:05 +0200
Subject: [R] r: integration question
Message-ID: <42B69639.29261DA1@STATS.uct.ac.za>

hi all

at the outset i must APOLOGIZE for sending the following mail. it is not
R related but since there are many stats and maths buffs that use the
list i decided to send the following question.

integrate ((1+((y-bx)^2)/(av))*(1+(x^2)/(bv)))^(-0.5*(v+1))

over the interval 0 to inf


a>0, b>0 and v>4
y treated as a constant over the real line.

i could integrate the function using "integrate". do so for a large
number of y values and plot the function BUT i would prefer an exact
solution if possible.

any help will be appreciated.
i've attached a word file with the formula


\
allan

From koeberle at mpiib-berlin.mpg.de  Mon Jun 20 12:11:47 2005
From: koeberle at mpiib-berlin.mpg.de (=?ISO-8859-1?Q?Christian_K=F6berle?=)
Date: Mon, 20 Jun 2005 12:11:47 +0200
Subject: [R] R from JAVA IDE
Message-ID: <42B69663.5000501@mpiib-berlin.mpg.de>

Hi,

how can i run SJava from an IDE like Netbeans? If i try to run a example 
class from RJava i get this message:
Exception in thread "main" java.lang.UnsatisfiedLinkError: no 
RInterpreter in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1682)
        at java.lang.Runtime.loadLibrary0(Runtime.java:822)
        at java.lang.System.loadLibrary(System.java:992)
        at 
org.omegahat.R.Java.ROmegahatInterpreter.<clinit>(ROmegahatInterpreter.java:34)
        at org.omegahat.R.Java.Examples.mean.main(mean.java:11)

thaks
Christian

-- 
----------------------------------------
Christian K??berle

Max Planck Institut for Infection Biology
Department: Immunology
Schumannstr. 21/22
10117 Berlin

Tel: +49 30 28 460 562
e-mail: koeberle at mpiib-berlin.mpg.de



From bxc at steno.dk  Sun Jun 19 23:02:14 2005
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Sun, 19 Jun 2005 23:02:14 +0200
Subject: [R] [R-pkgs] Epi package
Message-ID: <40D3930AC1C8EA469E39536E5BC80835701D25@EXDKBA021.corp.novocorp.net>

There is now an Epi-package on CRAN.

It is intended for epidemiological analysis in R.
It has its own homepage, http://www.pubhealth.ku.dk/~bxc/Epi

The package has been used at the course "Statistical practise 
in Epidemiology with R", see http://www.pubhealth.ku.dk/~bxc/SPE.

A mailing list for people using R for epidemiology, R-sig-Epi, has 
been set up, see https://stat.ethz.ch/mailman/listinfo/r-sig-epi
or through the R-homepage -> Mailing lists

Bendix Carstensen
maintainer
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From p.dalgaard at biostat.ku.dk  Mon Jun 20 12:18:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jun 2005 12:18:39 +0200
Subject: [R] R 2.1.1 is released
Message-ID: <x2fyvde3gg.fsf@turmalin.kubism.ku.dk>


I've rolled up R-2.1.1.tar.gz a short while ago. This is a maintenance
release containing mainly bugfixes.

See the full list of changes below.

You can get it from

http://cran.r-project.org/src/base/R-2/R-2.1.1.tar.gz

(give it some time to arrive there) or wait for it to be mirrored at a
CRAN site nearer to you. If you're *really* impatient,
www.biostat.ku.dk/~pd/R-release should work too. Binaries for various
platforms will appear in due course.
 
There is also a version split for floppies.

        For the R Core Team

        Peter Dalgaard

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

94d55d512a9ba36caa9b7df079bae19f  COPYING
d8045f3b8f929c1cb29a1e3fd737b499  COPYING.LIB
70447ae7f2c35233d3065b004aa4f331  INSTALL
1504edf0958138c034375749c3a189ee  NEWS
88bbd6781faedc788a1cbd434194480c  ONEWS
4f004de59e24a52d0f500063b4603bcb  OONEWS
e755b0ba5851ec261000af6b5c510335  R-2.1.1.tar.gz
97ff97d7094ef8d4cb0615bc9c3c4fe0  R-2.1.1.tar.gz-split.aa
40bbb45da53c04fefcaa8370ca84df81  R-2.1.1.tar.gz-split.ab
1fe3281097b6bce8c5d1eef2d1105f6a  R-2.1.1.tar.gz-split.ac
eaaa59c6f78b47ca45439f59f4bc99a2  R-2.1.1.tar.gz-split.ad
7b3f0fba58926784052632133b926311  R-2.1.1.tar.gz-split.ae
7bf0703625535bec3da0a5e66e6c1d28  R-2.1.1.tar.gz-split.af
d1da10e9d40f1266a5aa8d71e5def67e  R-2.1.1.tar.gz-split.ag
e90ea3c12153908184a4cb5307ffde09  R-2.1.1.tar.gz-split.ah
03f19a0d058aba30c686400fc3f85d76  R-2.1.1.tar.gz-split.ai
e755b0ba5851ec261000af6b5c510335  R-latest.tar.gz
56a780cdec835c5c598f8dfc0738f7f3  README



Here is the relevant bit of the NEWS file:


		CHANGES IN R VERSION 2.1.1


NEW FEATURES

    o	bug.report() now reports the locale in use.

    o	upgrade.packageStatus() allows user input "c" to cancel the
	upgrade, just as update.packages() does.

    o	glm() now accepts 1D arrays (e.g. tables) as a response,
	dropping them to a vector whilst preserving names.

    o	df() with one infinite df now works (to match pf()).

    o	Added tclServiceMode() function to the tcltk package to allow
    	updating to be suspended.

    o	The Encoding: field of a DESCRIPTION file is now documented,
	and used by packageDescription() and library(help=).

    o	There has been progress on translations: existing translations
	have been revised and expanded, and French and Korean have
	been added.

	The Windows installer supports a wide range of languages for
	installation.


BUG FIXES

    o	lm(qr=FALSE) now works.

    o	predict.glm() not longer loses names for "response" predictions.
	(PR#7792)

    o	Typo in menu(graphics=TRUE) meant it failed on Unix if tcltk
	was not available.

    o	When names.dist() was removed, the result of cmdscale() lost
	its rownames.  The example also lost the labels.

    o	R CMD check assumed 'tar' was GNU tar and so supported -z.

    o	read.table() was not handing escaped quotes inside quoted fields
	in the first five lines of the file.  (PR#7789)

	It was also not handling correctly EOF in the first five lines
	when reading from stdin().  (PR#7772)

    o	'make uninstall' was incomplete.

    o	make.packages.html() called by help.start() was failing if
	there were installed packages with help titles invalid in the
	current locale.

    o	printCoefmat(signif.legend = FALSE) was non-functional. (PR#7802)

    o	Some as.date.frame() methods failed because the expression
	deparsed into multiple lines. (PR#7808)

    o	setRepositories() had a typo. (PR#7810)

    o	Printing arrays/data frames with multibyte characters in the
	column labels was sometimes misaligned or using excessive space.
	(PR#7803)

    o	The Tcl/Tk console did not support multibyte characters.

    o	as.POSIXlt() could give infinite recursion if passed a corrupt
	"POSIXct" object (generated by an incorrect call to c.POSIXct,
	PR#7826).

    o	update.packages() was not passing 'type' correctly to
	install.packages().

    o	Printing the result of an unbalanced model.tables() call
	sometimes got confused if terms() had rearranged interaction
	terms.	(PR#7829)

    o	.Platform$pkgType was wrong on the CRAN MacOS X build, and
	.install.macbinary() was missing.

    o	as.personList() as used by citation() got confused by names
	containing "and".  (PR#7797)

    o	Subscripting an array by a matrix containing zero or negative
	values or the wrong number of columns was not handled
	consistently. (PR#7824)

    o	select.list(multiple=TRUE) now detects and tries again for invalid
	text input.

    o	add1.[g]lm could give strange results with interaction terms
	when the model and the upper scope had different orders for
	the main effects.  (PR#7842)

    o	A bug had sneaked into the anova.mlmlist() code, affecting the
	Greenhouse-Geisser epsilon. Code wrongly assumed a matrix to
	be symmetric. (Thanks to Bela Bauer.)

    o	anova.mlmlist() and mauchley.test() are now more tolerant to rank
	deficiency in the M and X matrices (also when they are implicitly
	generated via model.matrix()).

    o   anova.mlm had a scoping issue (PR#7898)

    o	pf() with infinite df is allowed again.	 It is now more accurate
	for extreme ratios of dfs, especially when there is a
	non-centrality parameter.

    o	df() was inaccurate for large df (1e16 or greater).

    o	dt() was inaccurate for large df (1e9 or greater) with a
	non-centrality parameter.

    o	runmed(*, algorithm="Turlach") seg.faulted in rare cases.

    o	strwrap() now makes a reasonable job of text that is invalid in the
	current locale.

    o	Reading with encoding "UCS-2LE" will remove any Byte Order
	Mark, as most implementations of iconv fail to handle BOMs
	(which are present in 'Windows Unicode' files).

    o	unique() for a list was incorrectly reporting `unimplemented'.

    o	The parser's contextstack was not protected against overflow,
	e.g. more than 50 unmatched '('.  (PR#7859)

    o	source(file, chdir = TRUE) was not checking that 'file' was a
	filepath (rather than a URL).  For 2.1.0 only, it did not work
	even if 'file' was a filepath.

    o	Hershey fonts were being sized based on pixels not points so came
        out too small on devices where pixels were noticeably different
	from points (e.g., win.printer() and high-resolution screens).

	Fix means that default size of Hershey fonts may be slightly
	different, for example, smaller by default on PostScript and PDF.

    o	The branch cuts in the complex versions of the inverse
	trigonometric and hyperbolic functions were non-standard.
	(PR#7871)

    o	truncate() on file() connections was limited to files < 2Gb.  It
	now works for larger files at least on 64-bit OSes and others
	where ftruncate supports such files.  (Related to PR#7879)

    o	proj.aovlist() did not work correctly on objects fitted from a
	data frame with row names.

    o	The coding standards recommendations had
	nuke-trailing-whitespace where newer versions of ESS need
	ess-nuke-trailing-whitespace.  (PR#7888)

    o	package.skeleton() missed the first newline in the DESCRIPTION file.

    o	pbirthday() reported p = 1 too often when coincident > 2.

    o	plot(1:3, exp(1:3), log = "y", ylim = c(30,1)) {reversed
	log-scale axis} now works, based on Uwe Ligges' suggestions.
	(PR#7894)

    o	install.packages() was aborting when a package in a bundle was
	chosen from a menu.  It failed if more than one package in a
	bundle was chosen from the command line.

    o	qcauchy() suffered from underflow in the extreme tails.  (PR#7902)

    o	Printing of raw matrices/arrays was not implemented. (PR#7912)

    o	getCallingDLL()'s default first argument did not correspond to
	its description and has been changed.  The mismatch caused
	symbols in .C/.Call/.Fortran calls without a PACKAGE= argument 
	to be potentially looked up in the wrong namespace.

    o	Binary save() of raw vectors was not working correctly on
	big-endian platforms.  (PR#7812)

    o   as.Date.factor() now accepts a format argument.

    o	Workaround added for FreeBSD which does not have alloca.h _and_
	does not allow alloca() to be declared.

    o	identify() now respects 'cex'.  (PR#660)

	Warnings from identify() are now printed immediately even on
	consoles with delayed printing.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph:  (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From br44114 at gmail.com  Mon Jun 20 14:29:02 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Mon, 20 Jun 2005 08:29:02 -0400
Subject: [R] how to make R faster under GNU/Linux
Message-ID: <8d5a3635050620052977297708@mail.gmail.com>

Dear useRs,

I timed the same code (simulation with for loops) on the same box
(dual Xeon EM64T, 1.5 Gb RAM) under 3 OSs and was surprised by the
results:
  Windows XP Pro (32-bit): Time difference of 5.966667 mins
  64-bit GNU/Linux (Fedora Core 4): Time difference of 6.966667 mins
  32-bit GNU/Linux (FC4): Time difference of 9.2 mins
(R 2.1.0 binaries downloaded from CRAN)

I searched the archives and found out I should compile a non-SHLIB R
binary. Is there something else I should pay attention to when trying
to make R faster under GNU/Linux?

Thank you,
b.



From tolga.uzuner at csfb.com  Mon Jun 20 14:50:22 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Mon, 20 Jun 2005 13:50:22 +0100
Subject: [R] Bucketting data
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>

Hi,

Am sure this is a trivial question but for some reason, haven't been able to figure it out.

I want to bucket data in a vector, and then iterate over the buckets.

Say the data set is:

> cleandata[,4]
 [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61  61  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180 362 544 544 544
[39] 544 544 544 544 544 544 544

This has the buckets: 

26 61 89 180 362 544

I'd like something which gives me a vector of these buckets, i.e. bucket(cleandata[,4])=vector of 26 61 89 180 362 544

and length(bucket(cleandata[,4]))=6

Thanks,
Tolga









==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From fabia at oregondude.com  Sun Jun 19 22:49:25 2005
From: fabia at oregondude.com (tom kroening)
Date: Mon, 20 Jun 2005 04:49:25 +0800
Subject: [R] Cartiers are the finest picks for your lovers. You will luv our
	lovvprice collections.
Message-ID: <B277CDAC.BCC3045@oregondude.com>

It is a nevv category called smart luxury goods. They are from Rolexes,
Cartiers, Bvlgaries, FrankMullers, Harry Winstons, Breguets,
Jaeger-lecoultre, Brietilings, TagHeuers and Tudors. With one closer l00k,
you will be convinced at these beauties. They also have their SerialNumbers
and logo. We pay attention to details as the ccustomers do.

Distinctive styles on our cybrezone have thesame fea-tures and attributes.
GGet one that are waterproof with sapphire crystal surface novv.

The only difference you will notice is our lovverprice.

Our modules have all the fea-tures the classicss have like HACK mechanism,
stainlesssteelback and others.


http://tqxq.gftT.arealleader.com/2gl/

Our durable vvatches madeof durable stainlesssteel sastisfy all your
requirements for high durability. Cchoose the ant0-rotating &winding ones?
Or the battery&quartz ones? We have assorted modules for ourcustomers.


her school of beau As soon as she could, she went after Mary, and having
found,  of the room from where the two ladies were sitting, and though
nearer  ty, I considered hand walked back with her to their former station,
by the stile,  to Captain Wentworth's table, not very near.  As she joined
him,  er a perfect example
Captain Harville's countenance re-assumed the serious, thoughtful  . There
was a red velvet footsto
NICK ol in the best par lour, on w While they were speaking, Laska, with
ears pricked up, was looking upward at the sky, and, reproachfully, at them.
 hich my mother had painte d a nosegay. T



From sarah.goslee at gmail.com  Mon Jun 20 14:55:25 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 20 Jun 2005 08:55:25 -0400
Subject: [R] Bucketting data
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
Message-ID: <efb536d505062005557c3180b2@mail.gmail.com>

I'm not certain what "buckets" are, but based on your example try
?unique

On 6/20/05, Uzuner, Tolga <tolga.uzuner at csfb.com> wrote:
> Hi,
> 
> Am sure this is a trivial question but for some reason, haven't been able to figure it out.
> 
> I want to bucket data in a vector, and then iterate over the buckets.
> 
> Say the data set is:
> 
> > cleandata[,4]
>  [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61  61  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180 362 544 544 544
> [39] 544 544 544 544 544 544 544
> 
> This has the buckets:
> 
> 26 61 89 180 362 544
> 
> I'd like something which gives me a vector of these buckets, i.e. bucket(cleandata[,4])=vector of 26 61 89 180 362 544
> 
> and length(bucket(cleandata[,4]))=6
> 
> Thanks,
> Tolga
> 
> 
> 
> 


-- 
Sarah Goslee 
http://www.stringpage.com



From jfbrennan at rogers.com  Mon Jun 20 15:02:03 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Mon, 20 Jun 2005 09:02:03 -0400
Subject: [R] Bucketting data
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
Message-ID: <200506201302.j5KD21tV007787@hypatia.math.ethz.ch>

?table maybe is what you want

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uzuner, Tolga
Sent: June 20, 2005 8:50 AM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] Bucketting data

Hi,

Am sure this is a trivial question but for some reason, haven't been able to
figure it out.

I want to bucket data in a vector, and then iterate over the buckets.

Say the data set is:

> cleandata[,4]
 [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61  61  61
61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180 362 544 544
544
[39] 544 544 544 544 544 544 544

This has the buckets: 

26 61 89 180 362 544

I'd like something which gives me a vector of these buckets, i.e.
bucket(cleandata[,4])=vector of 26 61 89 180 362 544

and length(bucket(cleandata[,4]))=6

Thanks,
Tolga









============================================================================
==
Please access the attached hyperlink for an important electronic
communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.be  Mon Jun 20 15:02:19 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 20 Jun 2005 15:02:19 +0200
Subject: [R] Bucketting data
References: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
Message-ID: <003101c57598$4a6a81e0$0540210a@www.domain>

try this:

unique(cleandata[, 4])
length(unique(cleandata[, 4]))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Uzuner, Tolga" <tolga.uzuner at csfb.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 20, 2005 2:50 PM
Subject: [R] Bucketting data


> Hi,
>
> Am sure this is a trivial question but for some reason, haven't been 
> able to figure it out.
>
> I want to bucket data in a vector, and then iterate over the 
> buckets.
>
> Say the data set is:
>
>> cleandata[,4]
> [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61 
> 61  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 
> 180 362 544 544 544
> [39] 544 544 544 544 544 544 544
>
> This has the buckets:
>
> 26 61 89 180 362 544
>
> I'd like something which gives me a vector of these buckets, i.e. 
> bucket(cleandata[,4])=vector of 26 61 89 180 362 544
>
> and length(bucket(cleandata[,4]))=6
>
> Thanks,
> Tolga
>
>
>
>
>
>
>
>
>
> ==============================================================================
> Please access the attached hyperlink for an important electronic 
> communications disclaimer:
>
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Mon Jun 20 15:02:25 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 20 Jun 2005 08:02:25 -0500
Subject: [R] Bucketting data
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
Message-ID: <1119272545.31407.2.camel@localhost.localdomain>

On Mon, 2005-06-20 at 13:50 +0100, Uzuner, Tolga wrote:
> Hi,
> 
> Am sure this is a trivial question but for some reason, haven't been
> able to figure it out.
> 
> I want to bucket data in a vector, and then iterate over the buckets.
> 
> Say the data set is:
> 
> > cleandata[,4]
>  [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61
> 61  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180
> 362 544 544 544
> [39] 544 544 544 544 544 544 544
> 
> This has the buckets: 
> 
> 26 61 89 180 362 544
> 
> I'd like something which gives me a vector of these buckets, i.e.
> bucket(cleandata[,4])=vector of 26 61 89 180 362 544
> 
> and length(bucket(cleandata[,4]))=6
> 
> Thanks,
> Tolga

> MyData
 [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61  61
[18]  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180
[35] 362 544 544 544 544 544 544 544 544 544 544

> bucket <- unique(MyData)

> bucket
[1]  26  61  89 180 362 544

> length(bucket)
[1] 6

# If you want a count of each unique value:

> table(MyData)
MyData
 26  61  89 180 362 544 
 12  11   7   4   1  10 


HTH,

Marc Schwartz



From tolga.uzuner at csfb.com  Mon Jun 20 15:03:59 2005
From: tolga.uzuner at csfb.com (Uzuner, Tolga)
Date: Mon, 20 Jun 2005 14:03:59 +0100
Subject: [R] Bucketting data
Message-ID: <BDF571786CAD224F966FEB86BEDED52F1433DE8F@elon12p32001.csfp.co.uk>

Many thanks all, that was it.
Regards,
Tolga


-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at mn.rr.com]
Sent: 20 June 2005 14:02
To: Uzuner, Tolga
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Bucketting data


On Mon, 2005-06-20 at 13:50 +0100, Uzuner, Tolga wrote:
> Hi,
> 
> Am sure this is a trivial question but for some reason, haven't been
> able to figure it out.
> 
> I want to bucket data in a vector, and then iterate over the buckets.
> 
> Say the data set is:
> 
> > cleandata[,4]
>  [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61
> 61  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180
> 362 544 544 544
> [39] 544 544 544 544 544 544 544
> 
> This has the buckets: 
> 
> 26 61 89 180 362 544
> 
> I'd like something which gives me a vector of these buckets, i.e.
> bucket(cleandata[,4])=vector of 26 61 89 180 362 544
> 
> and length(bucket(cleandata[,4]))=6
> 
> Thanks,
> Tolga

> MyData
 [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61  61
[18]  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180
[35] 362 544 544 544 544 544 544 544 544 544 544

> bucket <- unique(MyData)

> bucket
[1]  26  61  89 180 362 544

> length(bucket)
[1] 6

# If you want a count of each unique value:

> table(MyData)
MyData
 26  61  89 180 362 544 
 12  11   7   4   1  10 


HTH,

Marc Schwartz



==============================================================================
Please access the attached hyperlink for an important electronic communications disclaimer: 

http://www.csfb.com/legal_terms/disclaimer_external_email.shtml



From Heather.Turner at warwick.ac.uk  Mon Jun 20 15:01:07 2005
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Mon, 20 Jun 2005 14:01:07 +0100
Subject: [R] sweep() and recycling
Message-ID: <s2b6cdaf.059@spirit.csv.warwick.ac.uk>



Dr H Turner
Research Assistant
Dept. of Statistics
The University of Warwick
Coventry
CV4 7AL

Tel: 024 76575870

>>> Robin Hankin <r.hankin at noc.soton.ac.uk> 06/20/05 10:32am >>>
Hi


I had a hard-to-find bug in some of my code the other day, which I 
eventually
traced to my misusing of sweep().

I would expect sweep() to give
me a warning if the elements don't recycle nicely, but

  X <- matrix(1:36,6,6)
  sweep(X,1,1:5,"+")
      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    9   16   23   30   32
[2,]    4   11   18   25   27   34
[3,]    6   13   20   22   29   36
[4,]    8   15   17   24   31   38
[5,]   10   12   19   26   33   40
[6,]    7   14   21   28   35   37

gives no warning, even though (5,36)=1.

Also,


sweep(X,1,c(1,1000),"+")

and

sweep(X,2,c(1,1000),"+")


behave as expected,   But


sweep(X,1,1:5,"+")

and

  sweep(X,2,1:5,"+")



are identical!

...which is also expected since the matrix is square. Perhaps the
following will help you see why:
> matrix(1:5,6,6)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    2    3    4    5    1
[2,]    2    3    4    5    1    2
[3,]    3    4    5    1    2    3
[4,]    4    5    1    2    3    4
[5,]    5    1    2    3    4    5
[6,]    1    2    3    4    5    1
Warning message:
data length [5] is not a sub-multiple or multiple of the number of rows
[6] in matrix 
> matrix(1:5,6,6, byrow = TRUE)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    2    3    4    5    1
[2,]    2    3    4    5    1    2
[3,]    3    4    5    1    2    3
[4,]    4    5    1    2    3    4
[5,]    5    1    2    3    4    5
[6,]    1    2    3    4    5    1
Warning message:
data length [5] is not a sub-multiple or multiple of the number of rows
[6] in matrix 
i.e. recycling a vector of length 5 down the columns gives the same
result as recycling the same vector along the rows.

I agree with your main point however, it would be useful if 'sweep'
would give a warning when the length of the vector of statistics to be
swept out was not a sub-multiple or multiple of the corresponding array
dimension.

Heather



From 0034058 at fudan.edu.cn  Mon Jun 20 14:59:20 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Mon, 20 Jun 2005 20:59:20 +0800
Subject: [R] Bucketting data
In-Reply-To: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
References: <BDF571786CAD224F966FEB86BEDED52F1433DE8E@elon12p32001.csfp.co.uk>
Message-ID: <20050620205920.36c41ee2@localhost.localdomain>


?unique


On Mon, 20 Jun 2005 13:50:22 +0100
"Uzuner, Tolga" <tolga.uzuner at csfb.com> wrote:

> Hi,
> 
> Am sure this is a trivial question but for some reason, haven't been able to figure it out.
> 
> I want to bucket data in a vector, and then iterate over the buckets.
> 
> Say the data set is:
> 
> > cleandata[,4]
>  [1]  26  26  26  26  26  26  26  26  26  26  26  26  61  61  61  61  61  61  61  61  61  61  61  89  89  89  89  89  89  89 180 180 180 180 362 544 544 544
> [39] 544 544 544 544 544 544 544
> 
> This has the buckets: 
> 
> 26 61 89 180 362 544
> 
> I'd like something which gives me a vector of these buckets, i.e. bucket(cleandata[,4])=vector of 26 61 89 180 362 544
> 
> and length(bucket(cleandata[,4]))=6
> 
> Thanks,
> Tolga
> 
> 
> 
> 
> 
> 
> 
> 
> 
> ==============================================================================
> Please access the attached hyperlink for an important electronic communications disclaimer: 
> 
> http://www.csfb.com/legal_terms/disclaimer_external_email.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From r.hankin at noc.soton.ac.uk  Mon Jun 20 15:38:13 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Mon, 20 Jun 2005 14:38:13 +0100
Subject: [R] sweep() and recycling
In-Reply-To: <s2b6cdaf.058@spirit.csv.warwick.ac.uk>
References: <s2b6cdaf.058@spirit.csv.warwick.ac.uk>
Message-ID: <28ea07fefc11dfc344b6401c7c89698d@soc.soton.ac.uk>

Hi Heather

thanks for this.  (I have messed up quote level somewhere.  Sorry for 
that).


On Jun 20, 2005, at 02:01 pm, Heather Turner wrote:
>
>>>> Robin Hankin <r.hankin at noc.soton.ac.uk> 06/20/05 10:32am >>>
>> Hi
>>
>> [snip]
>> I would expect sweep() to give
>> me a warning if the elements don't recycle nicely
>
[snip]
>
>> behave as expected,   But
>>
>>
>> sweep(X,1,1:5,"+")
>>
>> and
>>
>>   sweep(X,2,1:5,"+")
>>
>>
>>
>> are identical!
>>
> ...which is also expected since the matrix is square. Perhaps the
>> following will help you see why:
>

> matrix(1:5,6,6)
>  matrix(1:5,6,6, byrow = TRUE)



Yes,  after thinking about your example for a few minutes, it was a bit 
foolish of me
to use a square matrix to illustrate this problem.
Thinking about it, it is the coincidence that ncol(X) %% n == 
nrow(X)%%n == 1 that
caused the phenomenon I remarked on [here n=length(x)].

I should have known better, as I use this phenomenon deliberately in 
magic square
construction! (although not by using  sweep()).

> I agree with your main point however, it would be useful if 'sweep'
> would give a warning when the length of the vector of statistics to be
> swept out was not a sub-multiple or multiple of the corresponding array
> dimension.


yes, that would be good.   Can anyone think of  a situation in which 
one would want to ignore
such a warning?  Conversely, does anyone deliberately use sweep() with
   non-recycling vector?

>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From pmbrando at ipam.org.br  Mon Jun 20 06:46:15 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Sun, 19 Jun 2005 21:46:15 -0700
Subject: [R] RES:  another aov results interpretation question
In-Reply-To: <20050615214149.3b8997ab@zygiella.local>
Message-ID: <003501c57553$015bdca0$f10aa8c0@paulobrando>


Dear All,

I created a script to calculate averages - two groups: "parcel" and
"date" - and, based on these averages, make a graph. The problem is that
'R' does not recognize the first column even if I try to insert one. 

A brief example

Raw data: 

Data <- sample(1:100, 30, replace = FALSE, prob = NULL)
Date <- rep(c("02/30/2004","03/15/2004", "04/16/2004", "06/14/2004",
"07/08/2004"), 6)
Parcel <- rep(c("DRY", "CONTROL"),15)

Df <- data.frame(Parcel, Date, Data)

Res <- tapply(Df$Data, list(Date = Df$Date, Parcel = Df$Parcel), mean) 


> Res
            Parcel
Date          CONTROL      DRY
  02/30/2004 53.00000 52.33333
  03/15/2004 54.00000 67.66667
  04/16/2004 54.66667 30.00000
  06/14/2004 27.66667 20.00000
  07/08/2004 59.00000 38.00000

> colnames(Res)
[1] "CONTROL" "DRY"

> matplot(Res[,1], Res[,-1], type = "l") 

### It does not recognize the colunm "Date". Why?

Thanks in advance!

Paulo

________________________________________
Paulo M. Brando
Instituto de Pesquisa Ambiental da Amazonia (IPAM)
Santarem, PA, Brasil.
Av. Rui Barbosa, 136.
Fone: + 55 93 522 55 38
www.ipam.org.br
E-mail: pmbrando at ipam.org.br



From LUCKE at uthscsa.edu  Mon Jun 20 15:42:37 2005
From: LUCKE at uthscsa.edu (Lucke, Joseph F)
Date: Mon, 20 Jun 2005 08:42:37 -0500
Subject: [R] adjusted R^2 vs. ordinary R^2
Message-ID: <C4A57662D47C7B44B781D39E4C8F06941441A4@SAIGA.win.uthscsa.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050620/4037d0ae/attachment.pl

From joshuacgilbert at gmail.com  Mon Jun 20 16:44:22 2005
From: joshuacgilbert at gmail.com (Joshua Gilbert)
Date: Mon, 20 Jun 2005 10:44:22 -0400
Subject: [R] Computing generalized eigenvalues
In-Reply-To: <Pine.LNX.4.61.0506170850350.22424@gannet.stats>
References: <ef96daa305061611451ecaf99c@mail.gmail.com>
	<Pine.LNX.4.61.0506170850350.22424@gannet.stats>
Message-ID: <ef96daa30506200744c4f93a8@mail.gmail.com>

On 6/17/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Thu, 16 Jun 2005, Joshua Gilbert wrote:
> 
> > I need to compute generalized eigenvalues. The eigen function in base
> > doesn't do it and I can't find a package that does.
> 
> They are very rarely used in statistics, so this is not surprising.
> 
> I presume you mean solving Ax = lambda B x: if B is non-singular this
> reduces to a conventional eigenproblem for B^{-1}A.
> 

Exactly.

> > As I understand it, Lapack __can__ computer them
> > (http://www.netlib.org/lapack/lawn41/node111.html) and R can use
> > Lapack. If there is no function already, can I access Lapack from R
> > and use those routines directly?
> 
> Yes, you can: for real matrices the requisite routines are already
> compiled into R.  See DGGES or DGGEV.
> 

help.search('dggev') doesn't return anything. How do I access these routines?

> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From spencer.graves at pdf.com  Mon Jun 20 16:53:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Jun 2005 07:53:47 -0700
Subject: [R] r: integration question
In-Reply-To: <42B69639.29261DA1@STATS.uct.ac.za>
References: <42B69639.29261DA1@STATS.uct.ac.za>
Message-ID: <42B6D87B.2090603@pdf.com>

	  If I read your question correctly, you want to integrate the 
indicated expression over x = 0 to Inf.  If I substitute z = x/sqrt(b), 
your integral becomes one constant plus another times the expected value 
of (y-z/sqrt(b))^2, where z follows Student's t with v degrees of 
freedom.  Expand the quadratic form to get y^2 + (Ez^2)/b and find 
something that gives the variance of Student's t.

	  I don't have time now to work out the details, but this should work 
if I've understood your question correctly.

	  spencer graves

Clark Allan wrote:
> hi all
> 
> at the outset i must APOLOGIZE for sending the following mail. it is not
> R related but since there are many stats and maths buffs that use the
> list i decided to send the following question.
> 
> integrate ((1+((y-bx)^2)/(av))*(1+(x^2)/(bv)))^(-0.5*(v+1))
> 
> over the interval 0 to inf
> 
> 
> a>0, b>0 and v>4
> y treated as a constant over the real line.
> 
> i could integrate the function using "integrate". do so for a large
> number of y values and plot the function BUT i would prefer an exact
> solution if possible.
> 
> any help will be appreciated.
> i've attached a word file with the formula
> 
> 
> \
> allan
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Mon Jun 20 16:54:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Jun 2005 07:54:03 -0700
Subject: [R] Mixed model
In-Reply-To: <E76EB96029DCAE4A9CB967D7F6712D1DBFD649@NANAMAILBACK1.nanamail.co.il>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD649@NANAMAILBACK1.nanamail.co.il>
Message-ID: <42B6D88B.1020409@pdf.com>

(comments in line)

Stephen wrote:
> Dear Fellow R users,
> 
>  
> 
> I am fairly new to R and am currently conducting a mixed model.
> 
>  
> 
> I have 7 repeated measures on a simulated clinical trial 
> 
>  
> 
> If I understand the model correctly, the outcome is the measure (as a
> factor) the predictors are clinical group and trial (1-7). The fixed
> factors are the measure and group. The random factors are the intercept
> and id and group.
> 
>  
> 
> Based on this
> 
> Dataset <- read.table("C:/Program Files/R/rw2010/data/miss/model1.dat",
> header=TRUE, sep="\t", na.strings="NA", dec=".", strip.white=TRUE)
> 
> require (nlme)
> 
> model.mix <- lme (trans1 ~ Index1 + grp, 
>                   random = ~ constant | id / grp ,
>                   data = Dataset,
>                   na.action = "na.exclude")

	  I'm not familiar with this syntax.  I would replace your "random" 
formula with "~1|id/grp".  Did you get sensible results from your 
attempt to compute "model.mix"?  How do the results compare with the 
results from replacing your "random" with "~1|id/grp"?  Also, I'd try 
the same thing with lmer;  please see "Fitting Linear Mixed Models in R" 
by Doug Bates in the latest R News, downloadable from 
"www.r-project.org" -> Newsletter.
> 
>  
> 
> # where trans1 is the factor of the repeated measures of the scale.
> 
> # Index is the trial number, grp the group, and id the subject number.
> 
>  
> 
> I would like to split the results, just like SPSS splitfile by a
> variable in the Dataset called runnb
> 
> I have tried using:
> 
>  
> 
>       by (Dataset, runnb, 
> 
>             function (x) (lme (trans1 ~ Index1 + grp, 
> 
>             random = ~ constant | id / grp ,
> 
>             data = Dataset,
> 
>             na.action = "na.exclude") )
> 
> )
> 
	  I haven't used "by" enough to comment on this.  If I had problems 
with something like this, I might do something like the following:

	  with(Dataset, table(runnb, id, grp))

	  Do you have enough observations in all cells to be able to estimate 
all these individual models?  If yes, I might proceed as follows:

	  b.lvls <- table(Dataset$runnb)
	  nb <- length(b.lvls)
	  fit <- vector(mode="list", nb)
	  for(i in 1:nb)
		    fit[[i]] <- lme(...)	
	
	  If I still had problems with this, I might manually step through this 
until I found the "i" that created the problem, etc.
>  
> 
> but to no avail . as my computer hangs and I set my GUI to --mdi
> --max-mem-size=1200M.
> 
>  
> 
> Any ideas as to how to splitfile the results SPSS style would be most
> appreciated?
> 
>  
> 
> Also, does lme do pairwise deletion?
> 
>  
> 
> By the way
> 
> 
>>version
> 
> 
> platform i386-pc-mingw32
> 
> arch     i386           
> 
> os       mingw32        
> 
> system   i386, mingw32  
> 
> status                  
> 
> major    2              
> 
> minor    1.0            
> 
> year     2005           
> 
> month    04             
> 
> day      18             
> 
> language R    
> 
> Windows XP Pro.
> 
>  
> 
> Many thanks
> 
> Stephen
> 
> Ps as its my first time on this group - neat program!
> 
> 
> ???? ?"? ???? ????
> http://mail.nana.co.il
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Soren.Hojsgaard at agrsci.dk  Mon Jun 20 17:03:11 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Mon, 20 Jun 2005 17:03:11 +0200
Subject: [R] Fwd:  How to sample from a linear mixed model
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FC@DJFPOST01.djf.agrsci.dk>

Thanks. I wonder if there is a general way of extracting var(u) and var(e), which would be needed to simulate u and e. Clearly, one can get the estimated parameters, but is there a clever way of 'setting up' the matrices??
Best
S??ren

On 6/19/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> I would like to draw a sample from a linear mixed model y=Xb+Zu+e which has been fitted with lme(), i.e. a model y ~ N(Xb, C), where C=Z cov(u) Z' + cov(e).
> I've tried to figure out how to extract C from an lme object, because that would solve my problem when also using the predict() function, but without any luck.
> Can anyone help on that?

C is not stored in an lme object.  In fact it is never created.
(Consider the dimensions of this matrix.  It could be huge.)

The easiest way to simulate data from a linear mixed model is to
simulate u and e then form Xb+Zu+e

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Mon Jun 20 17:10:48 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 20 Jun 2005 10:10:48 -0500
Subject: [R] Mixed model
In-Reply-To: <42B6D88B.1020409@pdf.com>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD649@NANAMAILBACK1.nanamail.co.il>
	<42B6D88B.1020409@pdf.com>
Message-ID: <40e66e0b05062008104cf3a674@mail.gmail.com>

On 6/20/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> (comments in line)
> 
> Stephen wrote:
> > Dear Fellow R users,
> >
> >
> >
> > I am fairly new to R and am currently conducting a mixed model.
> >
> >
> >
> > I have 7 repeated measures on a simulated clinical trial
> >
> >
> >
> > If I understand the model correctly, the outcome is the measure (as a
> > factor) the predictors are clinical group and trial (1-7). The fixed
> > factors are the measure and group. The random factors are the intercept
> > and id and group.
> >
> >
> >
> > Based on this
> >
> > Dataset <- read.table("C:/Program Files/R/rw2010/data/miss/model1.dat",
> > header=TRUE, sep="\t", na.strings="NA", dec=".", strip.white=TRUE)
> >
> > require (nlme)
> >
> > model.mix <- lme (trans1 ~ Index1 + grp,
> >                   random = ~ constant | id / grp ,
> >                   data = Dataset,
> >                   na.action = "na.exclude")
> 
>           I'm not familiar with this syntax.  I would replace your "random"
> formula with "~1|id/grp".  Did you get sensible results from your
> attempt to compute "model.mix"?  How do the results compare with the
> results from replacing your "random" with "~1|id/grp"?  Also, I'd try
> the same thing with lmer;  please see "Fitting Linear Mixed Models in R"
> by Doug Bates in the latest R News, downloadable from
> "www.r-project.org" -> Newsletter.

The syntax in lmer would be

model.mix <- lmer(trans1 ~ Index1 + grp + (1|id:grp) + (1|id),
Dataset, na.action = na.exclude)


> >
> > # where trans1 is the factor of the repeated measures of the scale.
> >
> > # Index is the trial number, grp the group, and id the subject number.
> >
> >
> >
> > I would like to split the results, just like SPSS splitfile by a
> > variable in the Dataset called runnb
> >
> > I have tried using:
> >
> >
> >
> >       by (Dataset, runnb,
> >
> >             function (x) (lme (trans1 ~ Index1 + grp,
> >
> >             random = ~ constant | id / grp ,
> >
> >             data = Dataset,
> >
> >             na.action = "na.exclude") )
> >
> > )
> >
>           I haven't used "by" enough to comment on this.  If I had problems
> with something like this, I might do something like the following:
> 
>           with(Dataset, table(runnb, id, grp))
> 
>           Do you have enough observations in all cells to be able to estimate
> all these individual models?  If yes, I might proceed as follows:
> 
>           b.lvls <- table(Dataset$runnb)
>           nb <- length(b.lvls)
>           fit <- vector(mode="list", nb)
>           for(i in 1:nb)
>                     fit[[i]] <- lme(...)
> 
>           If I still had problems with this, I might manually step through this
> until I found the "i" that created the problem, etc.
> >
> >
> > but to no avail . as my computer hangs and I set my GUI to --mdi
> > --max-mem-size=1200M.
> >
> >
> >
> > Any ideas as to how to splitfile the results SPSS style would be most
> > appreciated?
> >
> >
> >
> > Also, does lme do pairwise deletion?
> >
> >
> >
> > By the way
> >
> >
> >>version
> >
> >
> > platform i386-pc-mingw32
> >
> > arch     i386
> >
> > os       mingw32
> >
> > system   i386, mingw32
> >
> > status
> >
> > major    2
> >
> > minor    1.0
> >
> > year     2005
> >
> > month    04
> >
> > day      18
> >
> > language R
> >
> > Windows XP Pro.
> >
> >
> >
> > Many thanks
> >
> > Stephen
> >
> > Ps as its my first time on this group - neat program!
> >
> >
> > ???? ?"? ???? ????
> > http://mail.nana.co.il
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bld at math.umd.edu  Mon Jun 20 17:15:17 2005
From: bld at math.umd.edu (Bernard L. Dillard)
Date: Mon, 20 Jun 2005 11:15:17 -0400 (EDT)
Subject: [R] Data Parsing
Message-ID: <39626.66.92.23.42.1119280517.squirrel@66.92.23.42>

Hello.  I have looked at R Site Search for this problem, and it didn't
give me exactly what I needed.

Consider this dataset called "results".  It has the following information:

Student      Day        Subject        Score

Mary          1          Math          Failed
David         2          Science       Passed
Bob           4          Reading       Passed
Marie         4          Reading       Failed
Jesse         3          Spelling      Borderline
Et cetera


My goal is to only do analysis of the data having to do those who passed
or who are borderline.  I want to ignore all of the data having to do with
those who failed.  I think part of the syntax is something like
results[-c("Failed")], but I'm not getting anywhere with it.

How do I create a separate data set containing all information on students
who either passed or were borderline?

Thanks.

Bernard

-- 
Do all you can with what you have in the time you have in the place you are!

-Nkosi Johnson, 12-year old African hero



From rpeng at jhsph.edu  Mon Jun 20 17:27:38 2005
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 20 Jun 2005 11:27:38 -0400
Subject: [R] Data Parsing
In-Reply-To: <39626.66.92.23.42.1119280517.squirrel@66.92.23.42>
References: <39626.66.92.23.42.1119280517.squirrel@66.92.23.42>
Message-ID: <42B6E06A.70800@jhsph.edu>

You can use 'subset()' if you have a data frame.

d <- read.table(...)
subset(d, Score %in% c("Passed", "Borderline"))

-roger

Bernard L. Dillard wrote:
> Hello.  I have looked at R Site Search for this problem, and it didn't
> give me exactly what I needed.
> 
> Consider this dataset called "results".  It has the following information:
> 
> Student      Day        Subject        Score
> 
> Mary          1          Math          Failed
> David         2          Science       Passed
> Bob           4          Reading       Passed
> Marie         4          Reading       Failed
> Jesse         3          Spelling      Borderline
> Et cetera
> 
> 
> My goal is to only do analysis of the data having to do those who passed
> or who are borderline.  I want to ignore all of the data having to do with
> those who failed.  I think part of the syntax is something like
> results[-c("Failed")], but I'm not getting anywhere with it.
> 
> How do I create a separate data set containing all information on students
> who either passed or were borderline?
> 
> Thanks.
> 
> Bernard
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From dimitris.rizopoulos at med.kuleuven.be  Mon Jun 20 17:29:13 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Mon, 20 Jun 2005 17:29:13 +0200
Subject: [R] Data Parsing
References: <39626.66.92.23.42.1119280517.squirrel@66.92.23.42>
Message-ID: <00c201c575ac$cfe78b10$0540210a@www.domain>

try this:

results.pas <- results[results$Score != "Failed", ]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bernard L. Dillard" <bld at math.umd.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, June 20, 2005 5:15 PM
Subject: [R] Data Parsing


> Hello.  I have looked at R Site Search for this problem, and it 
> didn't
> give me exactly what I needed.
>
> Consider this dataset called "results".  It has the following 
> information:
>
> Student      Day        Subject        Score
>
> Mary          1          Math          Failed
> David         2          Science       Passed
> Bob           4          Reading       Passed
> Marie         4          Reading       Failed
> Jesse         3          Spelling      Borderline
> Et cetera
>
>
> My goal is to only do analysis of the data having to do those who 
> passed
> or who are borderline.  I want to ignore all of the data having to 
> do with
> those who failed.  I think part of the syntax is something like
> results[-c("Failed")], but I'm not getting anywhere with it.
>
> How do I create a separate data set containing all information on 
> students
> who either passed or were borderline?
>
> Thanks.
>
> Bernard
>
> -- 
> Do all you can with what you have in the time you have in the place 
> you are!
>
> -Nkosi Johnson, 12-year old African hero
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Jun 20 17:32:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Jun 2005 16:32:03 +0100 (BST)
Subject: [R] Computing generalized eigenvalues
In-Reply-To: <ef96daa30506200744c4f93a8@mail.gmail.com>
References: <ef96daa305061611451ecaf99c@mail.gmail.com> 
	<Pine.LNX.4.61.0506170850350.22424@gannet.stats>
	<ef96daa30506200744c4f93a8@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506201631190.18917@gannet.stats>

On Mon, 20 Jun 2005, Joshua Gilbert wrote:

> On 6/17/05, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>> On Thu, 16 Jun 2005, Joshua Gilbert wrote:
>>
>>> I need to compute generalized eigenvalues. The eigen function in base
>>> doesn't do it and I can't find a package that does.
>>
>> They are very rarely used in statistics, so this is not surprising.
>>
>> I presume you mean solving Ax = lambda B x: if B is non-singular this
>> reduces to a conventional eigenproblem for B^{-1}A.
>>
>
> Exactly.
>
>>> As I understand it, Lapack __can__ computer them
>>> (http://www.netlib.org/lapack/lawn41/node111.html) and R can use
>>> Lapack. If there is no function already, can I access Lapack from R
>>> and use those routines directly?
>>
>> Yes, you can: for real matrices the requisite routines are already
>> compiled into R.  See DGGES or DGGEV.
>>
>
> help.search('dggev') doesn't return anything. How do I access these routines?

They are _compiled_ in: via .Fortran() perhaps.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jun 20 17:41:13 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jun 2005 17:41:13 +0200
Subject: [R] Computing generalized eigenvalues
In-Reply-To: <ef96daa30506200744c4f93a8@mail.gmail.com>
References: <ef96daa305061611451ecaf99c@mail.gmail.com>
	<Pine.LNX.4.61.0506170850350.22424@gannet.stats>
	<ef96daa30506200744c4f93a8@mail.gmail.com>
Message-ID: <x2zmtlc9ye.fsf@turmalin.kubism.ku.dk>

Joshua Gilbert <joshuacgilbert at gmail.com> writes:

> > > As I understand it, Lapack __can__ computer them
> > > (http://www.netlib.org/lapack/lawn41/node111.html) and R can use
> > > Lapack. If there is no function already, can I access Lapack from R
> > > and use those routines directly?
> > 
> > Yes, you can: for real matrices the requisite routines are already
> > compiled into R.  See DGGES or DGGEV.
> > 
> 
> help.search('dggev') doesn't return anything. How do I access these routines?

It's not *that* easy! You need to roll your own interface; take a look
at the implementation in solve.default. You likely also need to
construct the counterpart to La_dgesv.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Mon Jun 20 17:43:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 20 Jun 2005 11:43:13 -0400
Subject: [R] RES: another aov results interpretation question
In-Reply-To: <003501c57553$015bdca0$f10aa8c0@paulobrando>
References: <20050615214149.3b8997ab@zygiella.local>
	<003501c57553$015bdca0$f10aa8c0@paulobrando>
Message-ID: <971536df05062008435aa96497@mail.gmail.com>

On 6/20/05, Paulo Brando <pmbrando at ipam.org.br> wrote:
> 
> Dear All,
> 
> I created a script to calculate averages - two groups: "parcel" and
> "date" - and, based on these averages, make a graph. The problem is that
> 'R' does not recognize the first column even if I try to insert one.
> 
> A brief example
> 
> Raw data:
> 
> Data <- sample(1:100, 30, replace = FALSE, prob = NULL)
> Date <- rep(c("02/30/2004","03/15/2004", "04/16/2004", "06/14/2004",
> "07/08/2004"), 6)
> Parcel <- rep(c("DRY", "CONTROL"),15)
> 
> Df <- data.frame(Parcel, Date, Data)
> 
> Res <- tapply(Df$Data, list(Date = Df$Date, Parcel = Df$Parcel), mean)
> 
> 
> > Res
>            Parcel
> Date          CONTROL      DRY
>  02/30/2004 53.00000 52.33333
>  03/15/2004 54.00000 67.66667
>  04/16/2004 54.66667 30.00000
>  06/14/2004 27.66667 20.00000
>  07/08/2004 59.00000 38.00000
> 
> > colnames(Res)
> [1] "CONTROL" "DRY"
> 
> > matplot(Res[,1], Res[,-1], type = "l")
> 
> ### It does not recognize the colunm "Date". Why?

Date is a not a column in the matrix -- it is the name
of the row dimension (and Parcel is the name of the
column dimension).   Res is a 5 by 2 matrix whose
colnames are CONTROL and DRY.  Try this:

   dimnames(Res)
   dim(Res)  # 5 by 2

Also note that the first date (Feb 30th) is illegal and matplot
does not work with dates in any case.  Fix up the erroneous
date and convert the row names to an object, xx, of chron or Date 
class, using plot for the axes and matlines for the lines:

   xx <- format(rownames(Res), "%m/%d/%Y")  # Date class
   plot(range(xx), range(Res), type = "n")  # axes
   matlines(xx, Res)

More on dates is in R News 4/1.



From ggrothendieck at gmail.com  Mon Jun 20 17:47:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 20 Jun 2005 11:47:28 -0400
Subject: [R] RES: another aov results interpretation question
In-Reply-To: <971536df05062008435aa96497@mail.gmail.com>
References: <20050615214149.3b8997ab@zygiella.local>
	<003501c57553$015bdca0$f10aa8c0@paulobrando>
	<971536df05062008435aa96497@mail.gmail.com>
Message-ID: <971536df05062008473895472b@mail.gmail.com>

On 6/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/20/05, Paulo Brando <pmbrando at ipam.org.br> wrote:
> >
> > Dear All,
> >
> > I created a script to calculate averages - two groups: "parcel" and
> > "date" - and, based on these averages, make a graph. The problem is that
> > 'R' does not recognize the first column even if I try to insert one.
> >
> > A brief example
> >
> > Raw data:
> >
> > Data <- sample(1:100, 30, replace = FALSE, prob = NULL)
> > Date <- rep(c("02/30/2004","03/15/2004", "04/16/2004", "06/14/2004",
> > "07/08/2004"), 6)
> > Parcel <- rep(c("DRY", "CONTROL"),15)
> >
> > Df <- data.frame(Parcel, Date, Data)
> >
> > Res <- tapply(Df$Data, list(Date = Df$Date, Parcel = Df$Parcel), mean)
> >
> >
> > > Res
> >            Parcel
> > Date          CONTROL      DRY
> >  02/30/2004 53.00000 52.33333
> >  03/15/2004 54.00000 67.66667
> >  04/16/2004 54.66667 30.00000
> >  06/14/2004 27.66667 20.00000
> >  07/08/2004 59.00000 38.00000
> >
> > > colnames(Res)
> > [1] "CONTROL" "DRY"
> >
> > > matplot(Res[,1], Res[,-1], type = "l")
> >
> > ### It does not recognize the colunm "Date". Why?
> 
> Date is a not a column in the matrix -- it is the name
> of the row dimension (and Parcel is the name of the
> column dimension).   Res is a 5 by 2 matrix whose
> colnames are CONTROL and DRY.  Try this:
> 
>   dimnames(Res)
>   dim(Res)  # 5 by 2
> 
> Also note that the first date (Feb 30th) is illegal and matplot
> does not work with dates in any case.  Fix up the erroneous
> date and convert the row names to an object, xx, of chron or Date
> class, using plot for the axes and matlines for the lines:
> 
>   xx <- format(rownames(Res), "%m/%d/%Y")  # Date class

That should read:

     xx <- as.Date(rownames(Res), "%m/%d/%Y") # Date class

>   plot(range(xx), range(Res), type = "n")  # axes
>   matlines(xx, Res)
> 
> More on dates is in R News 4/1.
>



From ripley at stats.ox.ac.uk  Mon Jun 20 17:58:15 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Jun 2005 16:58:15 +0100 (BST)
Subject: [R] sweep() and recycling
In-Reply-To: <s2b6cdaf.059@spirit.csv.warwick.ac.uk>
References: <s2b6cdaf.059@spirit.csv.warwick.ac.uk>
Message-ID: <Pine.LNX.4.61.0506201653001.19145@gannet.stats>

The issue here is that the equivalent command array(1:5, c(6,6)) (to 
matrix(1:5,6,6)) gives no warning, and sweep uses array().

I am not sure either should: fractional recycling was normally allowed in 
S3 (S4 tightened up a bit).

Perhaps someone who thinks sweep() should warn could contribute a tested 
patch?

On Mon, 20 Jun 2005, Heather Turner wrote:

>
>
> Dr H Turner
> Research Assistant
> Dept. of Statistics
> The University of Warwick
> Coventry
> CV4 7AL
>
> Tel: 024 76575870
>
>>>> Robin Hankin <r.hankin at noc.soton.ac.uk> 06/20/05 10:32am >>>
> Hi
>
>
> I had a hard-to-find bug in some of my code the other day, which I
> eventually
> traced to my misusing of sweep().
>
> I would expect sweep() to give
> me a warning if the elements don't recycle nicely, but
>
>  X <- matrix(1:36,6,6)
>  sweep(X,1,1:5,"+")
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    2    9   16   23   30   32
> [2,]    4   11   18   25   27   34
> [3,]    6   13   20   22   29   36
> [4,]    8   15   17   24   31   38
> [5,]   10   12   19   26   33   40
> [6,]    7   14   21   28   35   37
>
> gives no warning, even though (5,36)=1.
>
> Also,
>
>
> sweep(X,1,c(1,1000),"+")
>
> and
>
> sweep(X,2,c(1,1000),"+")
>
>
> behave as expected,   But
>
>
> sweep(X,1,1:5,"+")
>
> and
>
>  sweep(X,2,1:5,"+")
>
>
>
> are identical!
>
> ...which is also expected since the matrix is square. Perhaps the
> following will help you see why:
>> matrix(1:5,6,6)
>     [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    1    2    3    4    5    1
> [2,]    2    3    4    5    1    2
> [3,]    3    4    5    1    2    3
> [4,]    4    5    1    2    3    4
> [5,]    5    1    2    3    4    5
> [6,]    1    2    3    4    5    1
> Warning message:
> data length [5] is not a sub-multiple or multiple of the number of rows
> [6] in matrix
>> matrix(1:5,6,6, byrow = TRUE)
>     [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    1    2    3    4    5    1
> [2,]    2    3    4    5    1    2
> [3,]    3    4    5    1    2    3
> [4,]    4    5    1    2    3    4
> [5,]    5    1    2    3    4    5
> [6,]    1    2    3    4    5    1
> Warning message:
> data length [5] is not a sub-multiple or multiple of the number of rows
> [6] in matrix
> i.e. recycling a vector of length 5 down the columns gives the same
> result as recycling the same vector along the rows.
>
> I agree with your main point however, it would be useful if 'sweep'
> would give a warning when the length of the vector of statistics to be
> swept out was not a sub-multiple or multiple of the corresponding array
> dimension.
>
> Heather
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.ghezzo at staff.mcgill.ca  Mon Jun 20 17:58:10 2005
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Mon, 20 Jun 2005 11:58:10 -0400
Subject: [R] (no subject)
Message-ID: <1119283090.42b6e7925b062@webmail.mcgill.ca>


R friends,
I am using R 2.1.0 in a Win XP . I have a problem working with lists, probably I
do not understand how to use them.

Lets suppose that a set of patients visit a clinic once a year for 4 years
on each visit a test, say 'eib' is performed with results 0 or 1
The patients do not all visit the clinic the 4 times but they missed a lot
of visits.
The test is considered positive if it is positive at the last 2 visits of that
patient, or a more lenient definition, it is positive in the last visit, and
never before.
Otherwise it is Negative = always negative or is a YoYo = unstable = changes
from positive to negative.
So, if I codify the visits with codes 1,2,4,8 if present at year 1,2,3,4 and
similarly the tests positive I get the last2 list codifying the test code
corresponding to the visits patterns possible, similarly the last1 list
20 here means NULL

nobs <- 400
#  visits       0   1   2    3     4    5      6      7       8    9
last1 <- list((20),(1),(2),c(3,2),(4),c(5,4),c(6,4),c(7,6,4),(8),c(9,8),
#  visits      10      11         12      13         14         15
             c(10,8),c(11,10,8),c(12,8),c(13,12,8),c(14,12,8),c(15,14,12,8))
#  visits       0   1    2    3   4    5   6    7     8    9
last2 <- list((20),(20),(20),(3),(20),(5),(6),c(7,6),(20),(9),
#  visits      10    11      12    13       14       15
              (10),c(11,10),(12),c(13,12),c(14,12),c(15,14,12))
#
#     simulate the visits
#
visit <- rbinom(nobs,1,0.7)
eib <- visit
#
#     simulate a positive test at a given visit
#
eib <- ifelse(runif(nobs) > 0.7,visit,0)
#
#     create the codes
#
viskode <- matrix(visit,ncol=4) %*% c(1,2,4,8)
eibkode <- matrix(eib,ncol=4) %*% c(1,2,4,8)
#
#    this is the brute force method, slow, of computing the Results according to
#    the 2 definitions above. Add 16 to the test kode to signify YoYos, Exactly
#    16 will be the negatives
#
 eibnoyoyo <- eibkode+16
 eiblst2 <- eibkode+16
 for(i in 1:nobs){
   if(eibkode[i] %in% last1[[viskode[i]+1]])
      eibnoyoyo[i] <- eibkode[i]
   if(eibkode[i] %in% last2[[viskode[i]+1]])
      eiblast2[i] <- eibkode[i]
 }
#
#    why is that these statements do not work?
#
eeibnoyoyo <- eeiblst2 <- rep(0,nobs)
eeibnoyoyo <- ifelse(eibkode %in% last1[viskode+1],eibkode,eibkode+16)
eeiblast2   <- ifelse(eibkode %in% last2[viskode+1],eibkode,eibkode+16)
#
table(viskode,eibkode)
table(viskode,eibnoyoyo)
table(viskode,eiblast2)
#
#  these two tables must be diagonal!!
#
table(eibnoyoyo,eeibnoyoyo)
table(eiblast2,eeiblast2)
#
Thanks for any help
Heberto Ghezzo
McGill University
Canada



From spencer.graves at pdf.com  Mon Jun 20 18:02:50 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Jun 2005 09:02:50 -0700
Subject: [R] practical help ... solving a system...
In-Reply-To: <42B5FA39.2020405@pdf.com>
References: <E1Dk6s5-0004Xk-1V@s2.stud.uni-goettingen.de>
	<42B5FA39.2020405@pdf.com>
Message-ID: <42B6E8AA.9040000@pdf.com>

	  The problem is that "size" in "dbinom" must be an integer:

 > dbinom(x=2, size=4.5, prob=.5)
[1] NaN
Warning message:
NaNs produced in: dbinom(x, size, prob, log)

	  Consider the following:
 > library(MASS)
 > set.seed(123)
 > x <- rbinom(100, 10, 0.4)
 > nx <- max(x)
 > fit <- fitdistr(x, dbinom, start=list(prob=0.5),
+ size = nx)
Warning message:
one-diml optimization by Nelder-Mead is unreliable: use optimize in: 
optim(start, mylogfn, x = x, hessian = TRUE, ...)
 > attributes(fit)
$names
[1] "estimate" "sd"

$class
[1] "fitdistr"
#### From this, we learn that fit$est is the MLE for prob given size
#### Try different a range of values for "size"
 > lglk <- rep(NA, 12)
#### Start with nx, since it can't be less than that.
 > names(lglk) <- nx:(nx+11)
 > for(i in 1:12){
+ fit <- fitdistr(x, dbinom, start=list(prob=0.5),
+ size = nx+i-1)
+ lglk[i] <- sum(dbinom(x, size=nx+i-1,
+ prob=fit$estimate, log=TRUE))
+ }
There were 15 warnings (use warnings() to see them)
 > qchisq(0.95, 1)
[1] 3.841459
 > 2*(max(lglk)-lglk)
           8           9          10          11          12          13
1.314439853 0.000000000 0.004998298 0.434260975 1.000371029 1.594030787
          14          15          16          17          18          19
2.170873826 2.713847684 3.217152046 3.680715184 4.106493564 4.497508846

# 2*log(likelihood ratio) is approximately chi-square,
# so a 95% confidence interval for "size" is 8:17.

	  If I needed this for a scientific research paper, I'd do more 
research on how to estimate "size" for the binomial -- if I really 
thought it should be estimated.  If I needed an answer today, I'd 
probably go with what I just did here.

	  hope this helps,
	  spencer graves

##################
Carsten Steinhoff wrote:
 > Hello Spencer,
 >
 > thank you for your reply. As I've seen you have optimized only one 
value of
 > the binomial distribution by using Fitdistr.
 > My problem is that I have to find both size AND prob.
 > The error message always is: error in using optim. non-finite finite
 > difference value [1]
 >
 > So I thought to do a MLE "by hand" ...
 >
 > Do you have an idea what do modify on the error above?
 >
 > Greetings, Carsten

Spencer Graves wrote:
>       "PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html".  If you had provided more 
> information about what you tried, it might be easier for someone else to 
> offer effective help.  In particular, what did you try with fitdistr, 
> and why do you think it didn't work (e.g., what error message did you get)?
> 
>       The following is a minor modification of examples in the 
> "fitdistr" help page:
> 
>  > library(MASS)
>  > set.seed(123)
>  > x <- rbinom(100, 10, 0.4)
>  > (fit <- fitdistr(x, dbinom, start=list(prob=0.5), size=10))
>       prob
>   0.39804687
>  (0.01547943)
> Warning message:
> one-diml optimization by Nelder-Mead is unreliable: use optimize in: 
> optim(start, mylogfn, x = x, hessian = TRUE, ...)
>  >
>       The difference between prob=0.4 that I simulated and the 
> 0.39804687 estimated is shockingly small.  (No, I didn't run 100 
> simulations with different seeds and pick the best one.)
> 
>       If I were concerned about the warning, I could list "fitdist" and 
> read the code.  I might even copy it into a script file and walk through 
> it line by line.  When checked the code for "fitdistr" just now, I 
> learned that it calls "optim", and "optim" provides other estimation 
> methods, e.g., BFGR and CG.  The following are the results from trying 
> those two:
> 
> (fit.BFGS <- fitdistr(x, dbinom, start=list(prob=0.5),
> + size=10, method="BFGS"))
>       prob
>   0.39800028
>  (0.01547882)
> Warning messages:
> 1: NaNs produced in: dbinom(x, size, prob, log)
> 2: NaNs produced in: dbinom(x, size, prob, log)
> 3: NaNs produced in: dbinom(x, size, prob, log)
> 4: NaNs produced in: dbinom(x, size, prob, log)
> 5: NaNs produced in: dbinom(x, size, prob, log)
>  > (fit.CG <- fitdistr(x, dbinom, start=list(prob=0.5),
> + size=10, method="CG"))
>       prob
>   0.39800001
>  (0.01547881)
> Warning messages:
> 1: NaNs produced in: dbinom(x, size, prob, log)
> 2: NaNs produced in: dbinom(x, size, prob, log)
> 3: NaNs produced in: dbinom(x, size, prob, log)
> 4: NaNs produced in: dbinom(x, size, prob, log)
> 5: NaNs produced in: dbinom(x, size, prob, log)
> 
>       If I'm concerned about these warnings, I can replace "dbinom" by a 
> local copy that prints the value of "prob" each time it's called:
> 
> dbinom. <- function (x, size, prob, log = FALSE){
>     cat(prob, "")
> .Internal(dbinom(x, size, prob, log))
> }
> (fit.BFGS <- fitdistr(x, dbinom., start=list(prob=0.5),
>     size=10, method="BFGS"))
> 0.5 0.501 0.499 -407.5005 -81.10011 -15.82002 -2.764004 -0.1528009 
> 0.3694398 0.3704398 0.3684398 0.3996073 0.4006073 0.3986073 0.3980445 
> 0.3990445 0.3970445 0.2133659 0.3611088 0.3906574 0.3965671 0.3977490 
> 0.3979854 0.3989854 0.3969854 0.3980003 0.45997 0.4103942 0.4004791 
> 0.3984960 0.3980994 0.3980201 0.3980043 0.3980011 0.3980004 0.3980003 
> 0.3980003 0.3980003 0.3980003 0.3980003 0.4000003 0.3980003 0.3980003 
> 0.3960003       prob
>   0.39800028
>  (0.01547882)
> Warning messages:
> 1: NaNs produced in: dbinom(x, size, prob, log)
> 2: NaNs produced in: dbinom(x, size, prob, log)
> 3: NaNs produced in: dbinom(x, size, prob, log)
> 4: NaNs produced in: dbinom(x, size, prob, log)
> 5: NaNs produced in: dbinom(x, size, prob, log)
>  >
>       It's no wonder dbinom produced NaNs:  It does that when prob < 0.
> 
>       hope this helps.
>       spencer graves
> 
> p.s.  MASS = "Modern Applied Statistics with S" by Venables and Ripley.. 
>  Have you seen this book?  I've learned a lot from it.
> 
> Carsten Steinhoff wrote:
> 
>> Hello,
>>  
>> I want to estimate the parameters of a binomial distributed rv using MLE.
>> Other distributions will follow.
>> The equation system to solve is not very complex, but I've never done 
>> such
>> work in R and don't have any idea how to start...
>>  
>> The system is:
>>  
>> (1)     n*P = X
>>  
>> (2)    [sum {from j=0 to J-1} Y{j} /(n-j)] = -n * ln (1-X / n)
>>  
>>  
>> where    * only X is given (empirical mean)
>>              * J is maximum observed
>>              * Y is the number of observations above j
>>              * n and P are the parameters of the binomial distribution to
>> find....
>>  
>> Who could help me with an example-solution for my "first" distribution 
>> ? I
>> also need a hint how to make the sum-element.
>>  
>> Maybe there's another - more simple - way to estimate the parameters...
>> first I tried via "FITDISTR" but without success.
>>  
>> Thanx a lot for your help.
>>  
>> Carsten
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Mon Jun 20 18:07:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Jun 2005 09:07:43 -0700
Subject: [R] Fwd:  How to sample from a linear mixed model
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FC@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FC@DJFPOST01.djf.agrsci.dk>
Message-ID: <42B6E9CF.9010407@pdf.com>

	  Have you considered "simulate.lme"?

	  Also, have you read Pinheiro and Bates (2000) Mixed-Effects Models in 
S and S-Plus (Springer).  It is excellent.

	  spencer graves

S??ren H??jsgaard wrote:

> Thanks. I wonder if there is a general way of extracting var(u) and var(e), which would be needed to simulate u and e. Clearly, one can get the estimated parameters, but is there a clever way of 'setting up' the matrices??
> Best
> S??ren
> 
> On 6/19/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> 
>>I would like to draw a sample from a linear mixed model y=Xb+Zu+e which has been fitted with lme(), i.e. a model y ~ N(Xb, C), where C=Z cov(u) Z' + cov(e).
>>I've tried to figure out how to extract C from an lme object, because that would solve my problem when also using the predict() function, but without any luck.
>>Can anyone help on that?
> 
> 
> C is not stored in an lme object.  In fact it is never created.
> (Consider the dimensions of this matrix.  It could be huge.)
> 
> The easiest way to simulate data from a linear mixed model is to
> simulate u and e then form Xb+Zu+e
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From plummer at iarc.fr  Mon Jun 20 18:14:50 2005
From: plummer at iarc.fr (Martyn Plummer)
Date: Mon, 20 Jun 2005 18:14:50 +0200
Subject: [R] how to make R faster under GNU/Linux
In-Reply-To: <8d5a3635050620052977297708@mail.gmail.com>
References: <8d5a3635050620052977297708@mail.gmail.com>
Message-ID: <1119284090.3558.42.camel@seurat>

On Mon, 2005-06-20 at 08:29 -0400, bogdan romocea wrote:
> Dear useRs,
> 
> I timed the same code (simulation with for loops) on the same box
> (dual Xeon EM64T, 1.5 Gb RAM) under 3 OSs and was surprised by the
> results:
>   Windows XP Pro (32-bit): Time difference of 5.966667 mins
>   64-bit GNU/Linux (Fedora Core 4): Time difference of 6.966667 mins
>   32-bit GNU/Linux (FC4): Time difference of 9.2 mins
> (R 2.1.0 binaries downloaded from CRAN)
> 
> I searched the archives and found out I should compile a non-SHLIB R
> binary. Is there something else I should pay attention to when trying
> to make R faster under GNU/Linux?

On Fedora Core 4, try using gcc 3.4.4 (which you must install from
source) instead of the default gcc 4.0.0.

Martyn

-----------------------------------------------------------------------
This message and its attachments are strictly confidential. ...{{dropped}}



From stefan.sobernig at wu-wien.ac.at  Mon Jun 20 18:47:27 2005
From: stefan.sobernig at wu-wien.ac.at (Stefan Sobernig)
Date: Mon, 20 Jun 2005 18:47:27 +0200
Subject: [R] linking R to goto blas
In-Reply-To: <Pine.LNX.4.61.0506132202360.5304@gannet.stats>
References: <42AC4160.90505@wu-wien.ac.at>
	<x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>
	<x2vf4jzi85.fsf@turmalin.kubism.ku.dk>
	<42ADD7F1.2030905@wu-wien.ac.at>
	<Pine.LNX.4.61.0506132202360.5304@gannet.stats>
Message-ID: <42B6F31F.5020904@wu-wien.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050620/37a57bd3/attachment.pl

From dmbates at gmail.com  Mon Jun 20 18:54:29 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 20 Jun 2005 11:54:29 -0500
Subject: [R] Computing generalized eigenvalues
In-Reply-To: <x2zmtlc9ye.fsf@turmalin.kubism.ku.dk>
References: <ef96daa305061611451ecaf99c@mail.gmail.com>
	<Pine.LNX.4.61.0506170850350.22424@gannet.stats>
	<ef96daa30506200744c4f93a8@mail.gmail.com>
	<x2zmtlc9ye.fsf@turmalin.kubism.ku.dk>
Message-ID: <40e66e0b050620095450b9067c@mail.gmail.com>

On 20 Jun 2005 17:41:13 +0200, Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> Joshua Gilbert <joshuacgilbert at gmail.com> writes:
> 
> > > > As I understand it, Lapack __can__ computer them
> > > > (http://www.netlib.org/lapack/lawn41/node111.html) and R can use
> > > > Lapack. If there is no function already, can I access Lapack from R
> > > > and use those routines directly?
> > >
> > > Yes, you can: for real matrices the requisite routines are already
> > > compiled into R.  See DGGES or DGGEV.
> > >
> >
> > help.search('dggev') doesn't return anything. How do I access these routines?
> 
> It's not *that* easy! You need to roll your own interface; take a look
> at the implementation in solve.default. You likely also need to
> construct the counterpart to La_dgesv.

Generally it is easier to put the interface code into a package and include

PKG_LIBS = ${LAPACK_LIBS} ${BLAS_LIBS}

in src/Makevars



From james.holtman at convergys.com  Mon Jun 20 19:04:38 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 20 Jun 2005 13:04:38 -0400
Subject: [R] (no subject)
In-Reply-To: <1119283090.42b6e7925b062@webmail.mcgill.ca>
Message-ID: <OFE9EBC48E.BF61A3B7-ON85257026.005C7B86-85257026.005DCEE2@nd.convergys.com>





'rle' might be your friend.  This will find the 'run of a sequence'

Here is some code working off the 'visit' data that you created.

# $Log$
x.1 <- matrix(visit, ncol=4)  # your data
x.rle <- apply(x.1, 1, rle)  # compute 'rle' for each row
Passed <- lapply(x.rle, function(x){  # now process each row see if it
meets the criteria
    .len <- length(x$lengths)
    if (x$lengths[.len] > 1 && x$values[.len] == 1) return(TRUE)  # last
two passed
    else if (.len == 2){ # two sequences
        if (x$lengths[.len] == 1 && x$values[.len] == 1) return(TRUE) #
only last passed
    }
    return(FALSE)
})
cbind(unlist(Passed), x.1)  # put results in first column with the data

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      r.ghezzo at staff.mcgill                                                                                                
                      .ca                          To:       r-help at stat.math.ethz.ch                                                      
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] (no subject)                                                              
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/20/2005 11:58                                                                                                     
                                                                                                                                           





R friends,
I am using R 2.1.0 in a Win XP . I have a problem working with lists,
probably I
do not understand how to use them.

Lets suppose that a set of patients visit a clinic once a year for 4 years
on each visit a test, say 'eib' is performed with results 0 or 1
The patients do not all visit the clinic the 4 times but they missed a lot
of visits.
The test is considered positive if it is positive at the last 2 visits of
that
patient, or a more lenient definition, it is positive in the last visit,
and
never before.
Otherwise it is Negative = always negative or is a YoYo = unstable =
changes
from positive to negative.
So, if I codify the visits with codes 1,2,4,8 if present at year 1,2,3,4
and
similarly the tests positive I get the last2 list codifying the test code
corresponding to the visits patterns possible, similarly the last1 list
20 here means NULL

nobs <- 400
#  visits       0   1   2    3     4    5      6      7       8    9
last1 <- list((20),(1),(2),c(3,2),(4),c(5,4),c(6,4),c(7,6,4),(8),c(9,8),
#  visits      10      11         12      13         14         15

c(10,8),c(11,10,8),c(12,8),c(13,12,8),c(14,12,8),c(15,14,12,8))
#  visits       0   1    2    3   4    5   6    7     8    9
last2 <- list((20),(20),(20),(3),(20),(5),(6),c(7,6),(20),(9),
#  visits      10    11      12    13       14       15
              (10),c(11,10),(12),c(13,12),c(14,12),c(15,14,12))
#
#     simulate the visits
#
visit <- rbinom(nobs,1,0.7)
eib <- visit
#
#     simulate a positive test at a given visit
#
eib <- ifelse(runif(nobs) > 0.7,visit,0)
#
#     create the codes
#
viskode <- matrix(visit,ncol=4) %*% c(1,2,4,8)
eibkode <- matrix(eib,ncol=4) %*% c(1,2,4,8)
#
#    this is the brute force method, slow, of computing the Results
according to
#    the 2 definitions above. Add 16 to the test kode to signify YoYos,
Exactly
#    16 will be the negatives
#
 eibnoyoyo <- eibkode+16
 eiblst2 <- eibkode+16
 for(i in 1:nobs){
   if(eibkode[i] %in% last1[[viskode[i]+1]])
      eibnoyoyo[i] <- eibkode[i]
   if(eibkode[i] %in% last2[[viskode[i]+1]])
      eiblast2[i] <- eibkode[i]
 }
#
#    why is that these statements do not work?
#
eeibnoyoyo <- eeiblst2 <- rep(0,nobs)
eeibnoyoyo <- ifelse(eibkode %in% last1[viskode+1],eibkode,eibkode+16)
eeiblast2   <- ifelse(eibkode %in% last2[viskode+1],eibkode,eibkode+16)
#
table(viskode,eibkode)
table(viskode,eibnoyoyo)
table(viskode,eiblast2)
#
#  these two tables must be diagonal!!
#
table(eibnoyoyo,eeibnoyoyo)
table(eiblast2,eeiblast2)
#
Thanks for any help
Heberto Ghezzo
McGill University
Canada

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From r.shengzhe at gmail.com  Mon Jun 20 20:16:37 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Mon, 20 Jun 2005 20:16:37 +0200
Subject: [R] how to do a test of the null hypothesis of equal covariance
	across groups
Message-ID: <ea57975b050620111616b5a8e5@mail.gmail.com>

Hello there,

Which function can be used to do the test of equal covariance matrix
between two groups or among several groups?

Thank you,
Shengzhe



From ying at u.washington.edu  Mon Jun 20 20:26:06 2005
From: ying at u.washington.edu (Ying Huang)
Date: Mon, 20 Jun 2005 11:26:06 -0700 (PDT)
Subject: [R] weights in glm for binomial model
Message-ID: <Pine.LNX.4.43.0506201126060.28679@hymn08.u.washington.edu>

I have a question about weights in glm for binomial model. Can I use any arbitrary number between 0 and 1 as weights instead of integer? I kept getting warning message when I did that.

Thanks.



From ying at u.washington.edu  Mon Jun 20 20:27:57 2005
From: ying at u.washington.edu (Ying Huang)
Date: Mon, 20 Jun 2005 11:27:57 -0700 (PDT)
Subject: [R] weights in glm for binomial model
Message-ID: <Pine.LNX.4.43.0506201127570.28679@hymn08.u.washington.edu>

I have a question about weights in glm for binomial model. Can I use any arbitrary number between 0 and 1 as weights instead of integers? I kept getting warning message when I tried that.

Thanks.



From kemerson at darkwing.uoregon.edu  Mon Jun 20 20:38:33 2005
From: kemerson at darkwing.uoregon.edu (Kevin J Emerson)
Date: 20 Jun 2005 11:38:33 -0700
Subject: [R] frequency tables
Message-ID: <1119292713.4269.39.camel@d43-8.uoregon.edu>

R-masters,

I have a problem that I have been working on for a while and it seems
that there may be a simple solution that I have yet to figure out, so I
thought that I would venture to post to the help list.

Let's say there was a data.frame with three vectors, two that are
factors identifying the data, and one that holds the frequency of
occurrence (the events are binary, yes or no).  I would like to perform
logistic regression on this data, and it seems that I need a vector of
0s and 1s for input into lrm.  How might I convert between a frequency
table and a vector of binary data while still maintaining all identifier
information?

I have thought about using the rep command over and over again and
basically building the data.frame "by hand" but that seems long and
tedious.  Is there a quick and dirty way of doing this?

Thanks in advance!
Kevin
-- 
------------------------------------
------------------------------------
Kevin J Emerson
Center for Ecology and Evolutionary Biology
1210 University of Oregon
University of Oregon
Eugene, OR 97403
kemerson at dakrwing.uoregon.edu



From dmbates at gmail.com  Mon Jun 20 20:52:49 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 20 Jun 2005 13:52:49 -0500
Subject: [R] Fwd: How to sample from a linear mixed model
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FC@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FC@DJFPOST01.djf.agrsci.dk>
Message-ID: <40e66e0b05062011526596907a@mail.gmail.com>

On 6/20/05, S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> Thanks. I wonder if there is a general way of extracting var(u) and var(e), which would be needed to simulate u and e. Clearly, one can get the estimated parameters, but is there a clever way of 'setting up' the matrices??
> Best
> S??ren

The short answer is that the result of VarCorr applied to a fitted
lmer model gives the variance-covariance matrix of the random effects
and the variance of the noise term.  I enclose a function that can be
used to simulate data somewhat more efficiently from a fitted lmer
model.  It uses some of the intermediate values that have been stored
in the object.

At least I think this lmerSim function works.  It takes two arguments,
the first is the fitted model and the second is the number of data
sets to simulate.  It uses the estimated fixed effects from the model.
 An enhancement would be to allow the fixed effects to be given as a
third argument.

 
> On 6/19/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> > I would like to draw a sample from a linear mixed model y=Xb+Zu+e which has been fitted with lme(), i.e. a model y ~ N(Xb, C), where C=Z cov(u) Z' + cov(e).
> > I've tried to figure out how to extract C from an lme object, because that would solve my problem when also using the predict() function, but without any luck.
> > Can anyone help on that?
> 
> C is not stored in an lme object.  In fact it is never created.
> (Consider the dimensions of this matrix.  It could be huge.)
> 
> The easiest way to simulate data from a linear mixed model is to
> simulate u and e then form Xb+Zu+e
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 
>

From MSchwartz at mn.rr.com  Mon Jun 20 21:05:39 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 20 Jun 2005 14:05:39 -0500
Subject: [R] Fedora Core 4
In-Reply-To: <1119212988.42b5d5bc6444f@webmail.iarc.fr>
References: <20050618092556.GA12568@psych>
	<Pine.LNX.4.61.0506181238030.5152@gannet.stats>
	<1119098007.822.14.camel@localhost.localdomain>
	<1119212988.42b5d5bc6444f@webmail.iarc.fr>
Message-ID: <1119294339.31407.29.camel@localhost.localdomain>

On Sun, 2005-06-19 at 22:29 +0200, plummer at iarc.fr wrote:
> The Fedora upgrade process should normally install backward compatibility
> libraries when it finds an RPM linked to a library (or library version) that
> isn't in the new release.  In this case "compat-libf2c-32" provides libg2c on
> FC4, and it should be installed on your system if you previously had R for FC3
> installed. So R should continue to work.  But it will break when you start
> compiling packages on the new system because the configuration information in
> the file /usr/lib/R/etc/Makeconf is obsolete. Hopefully this situation will
> occur rarely, because there isn't really a way to deal with it automatically.
> 
> I'm pretty sure the current R RPM for FC4 on Red Hat Extras is broken. The RPM
> build process adds a lot of compiler flags. R 2.1.1 doesn't pass "make
> check-all" when built with these flags (and if you try the legacy compilers
> Marc mentioned, it doesn't even get past the configure step). A close look at
> the spec file from Red Hat shows that this line has been commented out, which
> was a mistake.  I'll try to ensure that their next release works. In the mean
> time, I am bumping up the Epoch number of my RPMs to 1 so they should be
> considered an "upgrade" of the Red Hat ones.
> 
> Martyn.
> 
> Compiling R 2.1.1 on FC4 with gcc4

Martyn,

Just a quick heads up that Tom appears to be in the midst of the updates
for 2.1.1, as there have been some commits to the R CVS at Extras today.

The CVS commit e-mail list has an archive here:

https://www.redhat.com/archives/fedora-extras-commits/2005-June/thread.html

He has some commit posts that being with "rpms/R"

If you have not yet seen it, a web interface to the Extras CVS for R is
at:

http://cvs.fedora.redhat.com/viewcvs/rpms/R/?root=extras#dirlist

I thought that I would mention that if you have not had any
communications with him since the exchange last Thursday, relative to
the compiler flag issues.

Best regards,

Marc



From renaud.lancelot at cirad.fr  Mon Jun 20 21:19:54 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Mon, 20 Jun 2005 22:19:54 +0300
Subject: [R] frequency tables
In-Reply-To: <1119292713.4269.39.camel@d43-8.uoregon.edu>
References: <1119292713.4269.39.camel@d43-8.uoregon.edu>
Message-ID: <42B716DA.1020100@cirad.fr>

Kevin J Emerson a ??crit :
> R-masters,
> 
> I have a problem that I have been working on for a while and it seems
> that there may be a simple solution that I have yet to figure out, so I
> thought that I would venture to post to the help list.
> 
> Let's say there was a data.frame with three vectors, two that are
> factors identifying the data, and one that holds the frequency of
> occurrence (the events are binary, yes or no).  I would like to perform
> logistic regression on this data, and it seems that I need a vector of
> 0s and 1s for input into lrm.  How might I convert between a frequency
> table and a vector of binary data while still maintaining all identifier
> information?
> 
> I have thought about using the rep command over and over again and
> basically building the data.frame "by hand" but that seems long and
> tedious.  Is there a quick and dirty way of doing this?
> 
> Thanks in advance!
> Kevin

Hi Kevin,

I don't know lrm so can't answer on this point. However, you can fit log 
reg models using the "regular" glm. See ?glm. There are 3 ways to fit 
the model:

1. Fit binomial data with the syntax cbind(y, n - y) ~ x1 + x2, where y 
is the count of events of interest, and n the sample size for the 
covariate patterns defined by x1 and x2

2. Fit proportions, with the syntax y/n ~ x1 + x2, weights = n

3. "Unfold" the data as you suggest. I guess many people wrote utility 
functions for this purpose. One of them is available in the package aod 
(on CRAN) and is called splitbin:

 > data(orob2)
 > head(orob2)
   seed     root  n  y
1  O75     BEAN 39 10
2  O75     BEAN 62 23
3  O75     BEAN 81 23
4  O75     BEAN 51 26
5  O75     BEAN 39 17
6  O75 CUCUMBER  6  5
 > res <- splitbin(cbind(y, n - y) ~ root + seed, orob2)
 > res[1:39, ]
    id y root seed
1   1 0 BEAN  O75
2   1 0 BEAN  O75
3   1 0 BEAN  O75
4   1 0 BEAN  O75
5   1 0 BEAN  O75
6   1 0 BEAN  O75
7   1 0 BEAN  O75
8   1 0 BEAN  O75
9   1 0 BEAN  O75
10  1 0 BEAN  O75
11  1 0 BEAN  O75
12  1 0 BEAN  O75
13  1 0 BEAN  O75
14  1 0 BEAN  O75
15  1 0 BEAN  O75
16  1 0 BEAN  O75
17  1 0 BEAN  O75
18  1 0 BEAN  O75
19  1 0 BEAN  O75
20  1 0 BEAN  O75
21  1 0 BEAN  O75
22  1 0 BEAN  O75
23  1 0 BEAN  O75
24  1 0 BEAN  O75
25  1 0 BEAN  O75
26  1 0 BEAN  O75
27  1 0 BEAN  O75
28  1 0 BEAN  O75
29  1 0 BEAN  O75
30  1 1 BEAN  O75
31  1 1 BEAN  O75
32  1 1 BEAN  O75
33  1 1 BEAN  O75
34  1 1 BEAN  O75
35  1 1 BEAN  O75
36  1 1 BEAN  O75
37  1 1 BEAN  O75
38  1 1 BEAN  O75
39  1 1 BEAN  O75


-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From f.calboli at imperial.ac.uk  Mon Jun 20 22:15:43 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 20 Jun 2005 21:15:43 +0100
Subject: [R] vectorisation suggestion
Message-ID: <11A03FC9-7A86-405A-A05D-351EE7F9A97B@imperial.ac.uk>

Hi All,

I am counting the number of occurrences of the terms listed in one  
vector in another vector.

My code runs:

for( i in 1:length(vector3)){
   vector3[i]  = sum(1*is.element(vector2,  vector1[i]))
}

where

vector1 = vector containing the terms whose occurrences I want to count
vector2 = made up of a number of repetitions of all the elements of  
vector1
vector3 = a vector of NAs that is meant to get the result of the  
counting

My problem is that vector1 is about 60000 terms, and vector2 is  
620000... can anyone suggest a faster code than the one I wrote?

Cheers,

Federico Calboli


--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From gunter.berton at gene.com  Mon Jun 20 22:24:43 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 20 Jun 2005 13:24:43 -0700
Subject: [R] How to define S4 methods for '['
Message-ID: <200506202024.j5KKOhF4025468@faraday.gene.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050620/7188a0d6/attachment.pl

From f.calboli at imperial.ac.uk  Mon Jun 20 22:29:53 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Mon, 20 Jun 2005 21:29:53 +0100
Subject: [R] vectorisation suggestion
In-Reply-To: <200506202024.j5KKOEEj003132@gator.dt.uh.edu>
References: <200506202024.j5KKOEEj003132@gator.dt.uh.edu>
Message-ID: <CF72337E-6331-4726-A126-CCCF0FA6497F@imperial.ac.uk>


On 20 Jun 2005, at 21:24, Erin Hodgess wrote:

> Hello, Federico!
>
> I'm a bit confused about your question, please:
>
> What sorts of things are in Vector1, please?

numbers (as in "numeric") that code individuals

>
> Why are you counting NAs in Vector3, please?

I am counting how many times the code for each individual (as listed  
in vector1) is present in vector2, and save the number of counts in  
vector3

Cheers,

Federico

--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From ggrothendieck at gmail.com  Mon Jun 20 22:31:12 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 20 Jun 2005 16:31:12 -0400
Subject: [R] How to define S4 methods for '['
In-Reply-To: <200506202024.j5KKOhF4025468@faraday.gene.com>
References: <200506202024.j5KKOhF4025468@faraday.gene.com>
Message-ID: <971536df05062013312673118@mail.gmail.com>

On 6/20/05, Berton Gunter <gunter.berton at gene.com> wrote:
> Folks:
> 
> This is a question about the S4 formal class system.
> 
> Suppose I have a class, 'foo', defined by:
> 
> setClass('foo',representation(dat='matrix', id='character') )
> 
> I wish to define a '[' method for foo that will extract from the 'dat' slot.
> I would have thought that the following would work, but it doesn't:
> 
> setMethod("[","foo",function(x,i, j, .,drop=TRUE)callGeneric(x at dat,i,
> j,drop=drop) )
> 
> The only way I have succeeded in defining this method is using brute force
> eval(parse(. :
> 
> {eval(parse(text=paste('.dat(x)[',
>        ifelse(missing(i),',','i,'),
>        ifelse(missing(j),']','j]'))))
>        }
> 
> This works. However, I am not able under any circumstances to pass the drop
> argument -- it is ignored.
> 
> I would appreciate any pointers about how to do this properly. If  this is
> explicitly in the Green Book (I do not have it with me at the moment), that
> will suffice.
> 

Download the source to the 'its' package where an S4 [ method is defined.



From james.holtman at convergys.com  Mon Jun 20 22:31:29 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 20 Jun 2005 16:31:29 -0400
Subject: [R] vectorisation suggestion
In-Reply-To: <11A03FC9-7A86-405A-A05D-351EE7F9A97B@imperial.ac.uk>
Message-ID: <OF6DE009C5.E221E5FA-ON85257026.00708DF5-85257026.0070BF06@nd.convergys.com>





v3 <- numeric()
v3[v1] <- table(v2)[v1]


Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Federico Calboli                                                                                                     
                      <f.calboli at imperial.a        To:       r-help <r-help at stat.math.ethz.ch>                                             
                      c.uk>                        cc:                                                                                     
                      Sent by:                     Subject:  [R] vectorisation suggestion                                                  
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/20/2005 16:15                                                                                                     
                                                                                                                                           




Hi All,

I am counting the number of occurrences of the terms listed in one
vector in another vector.

My code runs:

for( i in 1:length(vector3)){
   vector3[i]  = sum(1*is.element(vector2,  vector1[i]))
}

where

vector1 = vector containing the terms whose occurrences I want to count
vector2 = made up of a number of repetitions of all the elements of
vector1
vector3 = a vector of NAs that is meant to get the result of the
counting

My problem is that vector1 is about 60000 terms, and vector2 is
620000... can anyone suggest a faster code than the one I wrote?

Cheers,

Federico Calboli


--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From r.shengzhe at gmail.com  Mon Jun 20 22:38:12 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Mon, 20 Jun 2005 22:38:12 +0200
Subject: [R] how to do the test for a significant difference across group
	mean vectors
Message-ID: <ea57975b05062013383f6ac048@mail.gmail.com>

Hello there,

how to do the test for a significant difference between two groups
mean vectors or across several group means vectors?

Thank you,
Shengzhe



From matthew_wiener at merck.com  Mon Jun 20 22:38:10 2005
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 20 Jun 2005 16:38:10 -0400
Subject: [R] vectorisation suggestion
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04994729@uswsmx03.merck.com>

Federico -

"match" will give you the (first) index of each element of its first
argument in its second argument.  So  match(vector.1, vector.2)  tells you
where each element of vector.1 appears in vector.2.  So if you use "table"
on that vector, you'll see how many times each element of vector.2 appears. 

Something like:

first.occur <- match(vector.1, vector.2)
table(factor(vector.2[first.occur], levels = sort(unique(vector.2)))

(changing it into a factor means you won't lose values of vector.2 that
never appear in vector.1.)

An example:

> vec1 <- sample(1:10, 500, replace = TRUE)
> table(vec1)
vec1
 1  2  3  4  5  6  7  8  9 10 
61 37 43 47 49 59 51 48 53 52 
> vec2 <- 0:11
> vec3 <- match(vec1, vec2)
> table(factor(vec2[vec3], levels = sort(unique(vec2))))
 0  1  2  3  4  5  6  7  8  9 10 11 
 0 61 37 43 47 49 59 51 48 53 52  0 
> 

This also works if one of the members of vec1 is not in vec2 -- that member
simply gets ignored. (As you can see if add, say, a "20" at the end of
vec1.)

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico Calboli
Sent: Monday, June 20, 2005 4:16 PM
To: r-help
Subject: [R] vectorisation suggestion


Hi All,

I am counting the number of occurrences of the terms listed in one  
vector in another vector.

My code runs:

for( i in 1:length(vector3)){
   vector3[i]  = sum(1*is.element(vector2,  vector1[i]))
}

where

vector1 = vector containing the terms whose occurrences I want to count
vector2 = made up of a number of repetitions of all the elements of  
vector1
vector3 = a vector of NAs that is meant to get the result of the  
counting

My problem is that vector1 is about 60000 terms, and vector2 is  
620000... can anyone suggest a faster code than the one I wrote?

Cheers,

Federico Calboli


--
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St. Mary's Campus
Norfolk Place, London W2 1PG

Tel +44 (0)20 75941602   Fax +44 (0)20 75943193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Jerry.Hikel at sac.com  Mon Jun 20 22:44:38 2005
From: Jerry.Hikel at sac.com (Hikel, Jerry)
Date: Mon, 20 Jun 2005 16:44:38 -0400
Subject: [R] Plotting lines with shapes
Message-ID: <45486D498573904491AAA0C06DF66BAA02EF5640@MAILISCT13.saccap.int>

I am currently attempting to work on some graphs whose plotted lines
have simple polygons at regular intervals, such as a triangle or a
square.

I haven't been able to find anything in the base R plotting packages, or
in any extensions that would allow me to do this easily. I am familiar
with polygon drawing and shading, but i was wondering if anyone knew of
any packages that have these types of graphing capabilities built in to
the line functions themselves. Thanks.


DISCLAIMER: This e-mail message and any attachments are inte...{{dropped}}



From Robert.McGehee at geodecapital.com  Mon Jun 20 22:46:47 2005
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Mon, 20 Jun 2005 16:46:47 -0400
Subject: [R] How to define S4 methods for '['
Message-ID: <67DCA285A2D7754280D3B8E88EB548020C946605@MSGBOSCLB2WIN.DMN1.FMR.COM>

setMethod("[","foo",function(x,i, j,
.,drop=TRUE)callGeneric(x at dat,i,j,drop=drop) )
                                    ^^^
You have a typo. Use ... instead of . (note ?"[").

Best,
Robert

Robert McGehee
Quantitative Analyst
Geode Capital Management, LLC
53 State Street, 5th Floor | Boston, MA | 02109
Tel: 617/392-8396    Fax:617/476-6389
mailto:robert.mcgehee at geodecapital.com


-----Original Message-----
From: Berton Gunter [mailto:gunter.berton at gene.com] 
Sent: Monday, June 20, 2005 4:25 PM
To: r-help at stat.math.ethz.ch
Subject: [R] How to define S4 methods for '['


Folks:

This is a question about the S4 formal class system.

Suppose I have a class, 'foo', defined by:

setClass('foo',representation(dat='matrix', id='character') )

I wish to define a '[' method for foo that will extract from the 'dat'
slot.
I would have thought that the following would work, but it doesn't:

setMethod("[","foo",function(x,i, j, .,drop=TRUE)callGeneric(x at dat,i,
j,drop=drop) )

The only way I have succeeded in defining this method is using brute
force
eval(parse(. :

{eval(parse(text=paste('.dat(x)[',
        ifelse(missing(i),',','i,'),
        ifelse(missing(j),']','j]'))))
        }

This works. However, I am not able under any circumstances to pass the
drop
argument -- it is ignored.

I would appreciate any pointers about how to do this properly. If  this
is
explicitly in the Green Book (I do not have it with me at the moment),
that
will suffice.

Thanks.

Bert Gunter


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From r+Steven.Murdoch at cl.cam.ac.uk  Mon Jun 20 22:59:57 2005
From: r+Steven.Murdoch at cl.cam.ac.uk (Steven J. Murdoch)
Date: Mon, 20 Jun 2005 21:59:57 +0100
Subject: [R] Drawing information-rich, Tufte style scatterplots and axes
In-Reply-To: <Pine.LNX.4.58.0506161046580.25059@maplepark.com>
References: <20050616142656.GA11229@cl.cam.ac.uk>
	<Pine.LNX.4.58.0506161046580.25059@maplepark.com>
Message-ID: <20050620205957.GA15253@cl.cam.ac.uk>

On Thu, Jun 16, 2005 at 10:49:05AM -0500, David Forrest wrote:
> I posted a link to it on the GraphGallery page of the R Wiki:  
> http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?GraphGallery

Thanks.

> Why not make it a package?

This is high on the todo list, but first I need to define more options
for the functions (currently I often need to modify some hard-coded
parameters) and write some proper documentation. I also haven't
produced an R package before, so while I have read the documentation,
I suspect it will take a few tries before I get it working.

Thank you,
Steven Murdoch.

-- 
w: http://www.cl.cam.ac.uk/users/sjm217/



From BEN at SSANET.COM  Mon Jun 20 23:09:31 2005
From: BEN at SSANET.COM (Ben Fairbank)
Date: Mon, 20 Jun 2005 16:09:31 -0500
Subject: [R] Factanal loadings as large as 1.2 with promax -- how unusual?
Message-ID: <CA612484A337C6479EA341DF9EEE14AC03A6076E@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050620/74a20da2/attachment.pl

From macq at llnl.gov  Mon Jun 20 23:24:23 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 20 Jun 2005 14:24:23 -0700
Subject: [R] frequency tables
In-Reply-To: <1119292713.4269.39.camel@d43-8.uoregon.edu>
References: <1119292713.4269.39.camel@d43-8.uoregon.edu>
Message-ID: <p06210204bedce2670939@[128.115.153.6]>

Here's an example of how to replicate rows according to a count that 
is provided in one of the variables.

>  foo <- data.frame(id=letters[1:3],cl=LETTERS[1:3],n.yes=c(3,5,2))
>  foo
   id cl n.yes
1  a  A     3
2  b  B     5
3  c  C     2

>  cbind(foo[rep(1:nrow(foo),foo$n.yes),c('id','cl')],res=rep(1,sum(foo$n.yes)))
     id cl res
1    a  A   1
1.1  a  A   1
1.2  a  A   1
2    b  B   1
2.1  b  B   1
2.2  b  B   1
2.3  b  B   1
2.4  b  B   1
3    c  C   1
3.1  c  C   1

I assumed your third column is the frequency of "yes" events; I don't 
know w here you meant the frequency of "no" events to come from.

-Don

At 11:38 AM -0700 6/20/05, Kevin J Emerson wrote:
>R-masters,
>
>I have a problem that I have been working on for a while and it seems
>that there may be a simple solution that I have yet to figure out, so I
>thought that I would venture to post to the help list.
>
>Let's say there was a data.frame with three vectors, two that are
>factors identifying the data, and one that holds the frequency of
>occurrence (the events are binary, yes or no).  I would like to perform
>logistic regression on this data, and it seems that I need a vector of
>0s and 1s for input into lrm.  How might I convert between a frequency
>table and a vector of binary data while still maintaining all identifier
>information?
>
>I have thought about using the rep command over and over again and
>basically building the data.frame "by hand" but that seems long and
>tedious.  Is there a quick and dirty way of doing this?
>
>Thanks in advance!
>Kevin
>--
>------------------------------------
>------------------------------------
>Kevin J Emerson
>Center for Ecology and Evolutionary Biology
>1210 University of Oregon
>University of Oregon
>Eugene, OR 97403
>kemerson at dakrwing.uoregon.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From mvfpadilla at hotmail.com  Mon Jun 20 23:37:28 2005
From: mvfpadilla at hotmail.com (Matthew Padilla)
Date: Mon, 20 Jun 2005 21:37:28 +0000
Subject: [R] memory allocation failures
Message-ID: <BAY106-F17DAA9F774FED1119DAE35CFE90@phx.gbl>

Hi,

I am running R version rw2010 on a Windows 2000 desktop.  I am invoking R 
from Java via the JGR JRI tools.  My process consists of repeated calls to R 
in order to create linear models and process the resulting statistics.  I 
find, however, that the process often dies due to memory allocation errors:

lm command: 
.model_resp10_1=lm(resp10_1~rannor10+rannor11+rannor13+rannor14+rannor15+rannor18+rannor23+rannor26+rannor28+rannor29+rannor32+rannor33+rannor35+rannor36+rannor39+rannor40+rannor43+rannor44+rannor46+rannor47+rannor48+rannor50+rannor51+rannor53+rannor55+rannor56+rannor57+rannor59).
Garbage collection 172 = 54+41+77 (level 2) ...
626936 cons cells free (75%)
12.5 Mbytes of heap free (15%)
Error: cannot allocate vector of size 2265 Kb

I have tried to remedy this situation by corresponding calls to gc(), but 
this does not seem to fix the problem.  My files are not that large - about 
1000 records.  Additionally, general communication to R from Java from JRI 
does seem to work - I only run into problems when repeatedly creating models 
as demonstrated above.

Any help would be greatly appreciated - I am very much an R newbie.

Thank you,
Matthew



From helprhelp at gmail.com  Tue Jun 21 00:34:48 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 20 Jun 2005 17:34:48 -0500
Subject: [R] match multiple records
Message-ID: <cdf8178305062015346a86cd9a@mail.gmail.com>

Hi,

I have a question, explained by the following example:
> a<-c(1,2,3)
> b<-c(1,1,2,4)
> b[match(a,b, nomatch=0)]
[1] 1 2

which means it returns "the first match", but I want to get
1 1 2 instead of 1 2

In a word, how to do multiple matching?

thanks,

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From MSchwartz at mn.rr.com  Tue Jun 21 00:42:34 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 20 Jun 2005 17:42:34 -0500
Subject: [R] Plotting lines with shapes
In-Reply-To: <45486D498573904491AAA0C06DF66BAA02EF5640@MAILISCT13.saccap.int>
References: <45486D498573904491AAA0C06DF66BAA02EF5640@MAILISCT13.saccap.int>
Message-ID: <1119307354.31407.96.camel@localhost.localdomain>

On Mon, 2005-06-20 at 16:44 -0400, Hikel, Jerry wrote:
> I am currently attempting to work on some graphs whose plotted lines
> have simple polygons at regular intervals, such as a triangle or a
> square.
> 
> I haven't been able to find anything in the base R plotting packages, or
> in any extensions that would allow me to do this easily. I am familiar
> with polygon drawing and shading, but i was wondering if anyone knew of
> any packages that have these types of graphing capabilities built in to
> the line functions themselves. Thanks.


There are multiple approaches here, but using the base graphics package:

# plot type = 'b' uses both lines and symbols
plot(10:30, type = "b", pch = 24, bg = "black")

plot(10:30, type = "b", pch = 22, cex = 1.5)



You also have the option of doing them separately, which may give you
more control in some situations:

1. Draw the line(s) first and then add the symbols:

 plot(10:30, type = "l")
 points(10:30, pch = 22, bg = "black")


2. Draw the symbols and then add the line(s):

 plot(10:30, pch = 24, bg = "black") 
 lines(10:30)


See ?plot.default, ?lines and ?points for more information on plots and
symbols. Note the 'cex' and 'bg' arguments, which enable you to adjust
the size of the symbols and the fill color for the symbols.

HTH,

Marc Schwartz



From MSchwartz at mn.rr.com  Tue Jun 21 00:45:59 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 20 Jun 2005 17:45:59 -0500
Subject: [R] match multiple records
In-Reply-To: <cdf8178305062015346a86cd9a@mail.gmail.com>
References: <cdf8178305062015346a86cd9a@mail.gmail.com>
Message-ID: <1119307559.31407.99.camel@localhost.localdomain>

On Mon, 2005-06-20 at 17:34 -0500, Weiwei Shi wrote:
> Hi,
> 
> I have a question, explained by the following example:
> > a<-c(1,2,3)
> > b<-c(1,1,2,4)
> > b[match(a,b, nomatch=0)]
> [1] 1 2
> 
> which means it returns "the first match", but I want to get
> 1 1 2 instead of 1 2
> 
> In a word, how to do multiple matching?
> 
> thanks,

> b[b %in% a]
[1] 1 1 2

See ?"%in%" for more information.

HTH,

Marc Schwartz



From jfbrennan at rogers.com  Tue Jun 21 00:50:33 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Mon, 20 Jun 2005 18:50:33 -0400
Subject: [R] match multiple records
In-Reply-To: <cdf8178305062015346a86cd9a@mail.gmail.com>
Message-ID: <200506202250.j5KMoXtP007272@hypatia.math.ethz.ch>

R>a<-c(1,2,3)
R> b<-c(1,1,2,4)
R>1*b%in%a*b
[1] 1 1 2 0
R> b<-c(1,1,2,4,5,1,2,3,4)
R>1*b%in%a*b
[1] 1 1 2 0 0 1 2 3 0

I think this works

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
Sent: June 20, 2005 6:35 PM
To: R-help at stat.math.ethz.ch
Subject: [R] match multiple records

Hi,

I have a question, explained by the following example:
> a<-c(1,2,3)
> b<-c(1,1,2,4)
> b[match(a,b, nomatch=0)]
[1] 1 2

which means it returns "the first match", but I want to get
1 1 2 instead of 1 2

In a word, how to do multiple matching?

thanks,

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Tue Jun 21 01:15:46 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Mon, 20 Jun 2005 18:15:46 -0500
Subject: [R] tapply
Message-ID: <cdf8178305062016153aa7e988@mail.gmail.com>

hi,
i have another question on tapply:
i have a dataset z like this:
5540 389100307391      2600
5541 389100307391      2600
5542 389100307391      2600
5543 389100307391      2600
5544 389100307391      2600
5546 381300302513        NA
5547 387000307470        NA
5548 387000307470        NA
5549 387000307470        NA
5550 387000307470        NA
5551 387000307470        NA
5552 387000307470        NA

I want to sum the column 3 by column 2.
I removed NA by calling:
tapply(z[[3]], z[[2]], sum, na.rm=T)
but it does not work.

then, i used
z1<-z[!is.na(z[[3]],]
and repeat
still doesn't work.

please help.

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From gunter.berton at gene.com  Tue Jun 21 01:29:53 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 20 Jun 2005 16:29:53 -0700
Subject: [R] How to define S4 methods for '['
In-Reply-To: <971536df05062013312673118@mail.gmail.com>
Message-ID: <200506202329.j5KNTqZS013614@hertz.gene.com>

 
Thanks to Robert and Gabor for their replies, but neither was what I was
looking for, undoubtedly because of the poor phrasing of my question (+ a
typo -- however, even if correctly typed it doesn't work). I finally
realized that the "elegant" approach I sought can easily be done without
setGeneric:

setMethod('[','foo',function(x,i,j,drop,...){
    cl<-as.list(sys.call())
    cl[[2]]<-x at dat
    eval(as.call(cl))
    })

Indeed, this is a general template for this sort of thing. Should have
thought of this before, as V&R's S PROGRAMMING has numerous such examples...
Sigh...

-- Bert

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com] 
Sent: Monday, June 20, 2005 1:31 PM
To: Berton Gunter
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to define S4 methods for '['

On 6/20/05, Berton Gunter <gunter.berton at gene.com> wrote:
> Folks:
> 
> This is a question about the S4 formal class system.
> 
> Suppose I have a class, 'foo', defined by:
> 
> setClass('foo',representation(dat='matrix', id='character') )
> 
> I wish to define a '[' method for foo that will extract from the 'dat'
slot.
> I would have thought that the following would work, but it doesn't:
> 
> setMethod("[","foo",function(x,i, j, .,drop=TRUE)callGeneric(x at dat,i,
> j,drop=drop) )
> 
> The only way I have succeeded in defining this method is using brute force
> eval(parse(. :
> 
> {eval(parse(text=paste('.dat(x)[',
>        ifelse(missing(i),',','i,'),
>        ifelse(missing(j),']','j]'))))
>        }
> 
> This works. However, I am not able under any circumstances to pass the
drop
> argument -- it is ignored.
> 
> I would appreciate any pointers about how to do this properly. If  this is
> explicitly in the Green Book (I do not have it with me at the moment),
that
> will suffice.
> 

Download the source to the 'its' package where an S4 [ method is defined.



From wasquith at austin.rr.com  Tue Jun 21 01:41:04 2005
From: wasquith at austin.rr.com (William H. Asquith)
Date: Mon, 20 Jun 2005 18:41:04 -0500
Subject: [R] R: over/under flow
Message-ID: <99495eac32fc4354930677650d9a6337@austin.rr.com>

I am porting some FORTRAN to R in which an Inf triggers an if().  The 
trigger is infinite on exp().  What is the canonical R style of 
determining OVER when exp(OVER)== Inf?  The code structure that I am 
porting is best left intact--so I need to query R somehow to the value 
of OFL that causes exp(OFL) to equal Inf.

On my system,
exp(710) is first to equal Inf.

wa



From jfbrennan at rogers.com  Tue Jun 21 01:42:04 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Mon, 20 Jun 2005 19:42:04 -0400
Subject: [R] tapply
In-Reply-To: <cdf8178305062016153aa7e988@mail.gmail.com>
Message-ID: <200506202342.j5KNg1vp017949@hypatia.math.ethz.ch>

This may help
R>wei
     V1           V2   V3
1  5540 389100307391 2600
2  5541 389100307391 2600
3  5542 389100307391 2600
4  5543 389100307391 2600
5  5544 389100307391 2600
6  5546 381300302513   NA
7  5547 387000307470   NA
8  5548 387000307470   NA
9  5549 387000307470   NA
10 5550 387000307470   NA
11 5551 387000307470   NA
12 5552 387000307470   NA
R>ave(wei[,3],wei[,2],FUN=sum)
 [1] 13000 13000 13000 13000 13000    NA    NA    NA    NA    NA    NA    NA
R>

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Weiwei Shi
Sent: June 20, 2005 7:16 PM
To: R-help at stat.math.ethz.ch
Subject: [R] tapply

hi,
i have another question on tapply:
i have a dataset z like this:
5540 389100307391      2600
5541 389100307391      2600
5542 389100307391      2600
5543 389100307391      2600
5544 389100307391      2600
5546 381300302513        NA
5547 387000307470        NA
5548 387000307470        NA
5549 387000307470        NA
5550 387000307470        NA
5551 387000307470        NA
5552 387000307470        NA

I want to sum the column 3 by column 2.
I removed NA by calling:
tapply(z[[3]], z[[2]], sum, na.rm=T)
but it does not work.

then, i used
z1<-z[!is.na(z[[3]],]
and repeat
still doesn't work.

please help.

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Tue Jun 21 01:46:58 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Mon, 20 Jun 2005 18:46:58 -0500
Subject: [R] tapply
In-Reply-To: <cdf8178305062016153aa7e988@mail.gmail.com>
References: <cdf8178305062016153aa7e988@mail.gmail.com>
Message-ID: <1119311218.31407.114.camel@localhost.localdomain>

On Mon, 2005-06-20 at 18:15 -0500, Weiwei Shi wrote:
> hi,
> i have another question on tapply:
> i have a dataset z like this:
> 5540 389100307391      2600
> 5541 389100307391      2600
> 5542 389100307391      2600
> 5543 389100307391      2600
> 5544 389100307391      2600
> 5546 381300302513        NA
> 5547 387000307470        NA
> 5548 387000307470        NA
> 5549 387000307470        NA
> 5550 387000307470        NA
> 5551 387000307470        NA
> 5552 387000307470        NA
> 
> I want to sum the column 3 by column 2.
> I removed NA by calling:
> tapply(z[[3]], z[[2]], sum, na.rm=T)
> but it does not work.
> 
> then, i used
> z1<-z[!is.na(z[[3]],]
> and repeat
> still doesn't work.
> 
> please help.


The index vector(s) in tapply() need to be a "list". See the description
of the INDEX argument in ?tapply:

> tapply(z[[3]],list(z[[2]]), sum, na.rm = TRUE)
381300302513 387000307470 389100307391 
           0            0        13000 


Note that the use of na.rm = TRUE here results in misleading values of 0
for the other two groups, which are all NA's and this is not
self-evident unless you know the data.

You may be better off with:

> tapply(z[[3]],list(z[[2]]), sum)
381300302513 387000307470 389100307391 
          NA           NA        13000 

unless your real data is a mix of NA's and measured values.

Also see ?complete.cases and ?na.omit for further approaches to dealing
with such data sets.

HTH,

Marc Schwartz



From dmbates at gmail.com  Tue Jun 21 01:49:59 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 20 Jun 2005 18:49:59 -0500
Subject: [R] tapply
In-Reply-To: <cdf8178305062016153aa7e988@mail.gmail.com>
References: <cdf8178305062016153aa7e988@mail.gmail.com>
Message-ID: <40e66e0b05062016498129bba@mail.gmail.com>

On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> hi,
> i have another question on tapply:
> i have a dataset z like this:
> 5540 389100307391      2600
> 5541 389100307391      2600
> 5542 389100307391      2600
> 5543 389100307391      2600
> 5544 389100307391      2600
> 5546 381300302513        NA
> 5547 387000307470        NA
> 5548 387000307470        NA
> 5549 387000307470        NA
> 5550 387000307470        NA
> 5551 387000307470        NA
> 5552 387000307470        NA
> 
> I want to sum the column 3 by column 2.
> I removed NA by calling:
> tapply(z[[3]], z[[2]], sum, na.rm=T)
> but it does not work.
> 
> then, i used
> z1<-z[!is.na(z[[3]],]
> and repeat
> still doesn't work.

Can you be more explicit about "doesn't work"?



From dmbates at gmail.com  Tue Jun 21 02:09:46 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Mon, 20 Jun 2005 19:09:46 -0500
Subject: [R] R: over/under flow
In-Reply-To: <99495eac32fc4354930677650d9a6337@austin.rr.com>
References: <99495eac32fc4354930677650d9a6337@austin.rr.com>
Message-ID: <40e66e0b05062017095e98c820@mail.gmail.com>

On 6/20/05, William H. Asquith <wasquith at austin.rr.com> wrote:
> I am porting some FORTRAN to R in which an Inf triggers an if().  The
> trigger is infinite on exp().  What is the canonical R style of
> determining OVER when exp(OVER)== Inf?  The code structure that I am
> porting is best left intact--so I need to query R somehow to the value
> of OFL that causes exp(OFL) to equal Inf.
> 
> On my system,
> exp(710) is first to equal Inf.

log(.Machine$double.xmax)



From edd at debian.org  Tue Jun 21 02:39:23 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 20 Jun 2005 19:39:23 -0500
Subject: [R] how to make R faster under GNU/Linux
In-Reply-To: <8d5a3635050620052977297708@mail.gmail.com>
References: <8d5a3635050620052977297708@mail.gmail.com>
Message-ID: <17079.25019.922502.221358@basebud.nulle.part>


On 20 June 2005 at 08:29, bogdan romocea wrote:
| I timed the same code (simulation with for loops) on the same box
| (dual Xeon EM64T, 1.5 Gb RAM) under 3 OSs and was surprised by the
| results:
|   Windows XP Pro (32-bit): Time difference of 5.966667 mins
|   64-bit GNU/Linux (Fedora Core 4): Time difference of 6.966667 mins
|   32-bit GNU/Linux (FC4): Time difference of 9.2 mins
| (R 2.1.0 binaries downloaded from CRAN)
| 
| I searched the archives and found out I should compile a non-SHLIB R
| binary. Is there something else I should pay attention to when trying
| to make R faster under GNU/Linux?

See 'help(Rprof)' for profiling support. It will tell how the time is spent
by your loop, and I see no reason why you can't compare that across
architectures too.

Hth, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From jfox at mcmaster.ca  Tue Jun 21 03:00:12 2005
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 20 Jun 2005 21:00:12 -0400
Subject: [R] Factanal loadings as large as 1.2 with promax -- how
	unusual?
In-Reply-To: <CA612484A337C6479EA341DF9EEE14AC03A6076E@hercules.ssainfo>
Message-ID: <20050621010010.LADX27508.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Ben,

To get big factor loadings like this, I'd guess that you have large
communalities (not a bad thing, of course, and probably less rather than
more likely with crudely measured variables) and strong correlations between
factors. I suppose that if the latter get too large that might suggest that
you could get by with fewer factors.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ben Fairbank
> Sent: Monday, June 20, 2005 4:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Factanal loadings as large as 1.2 with promax -- 
> how unusual?
> 
> I am performing a large (105 variable) factor analysis with 
> factanal, specifying promax rotation.  I kow that some 
> loadings over 1.0 are not unsual with that rotation, but I 
> have some as large as 1.2, which seems extreme.  I am 
> skirting the assumptions of the model by using responses on a 
> 7-point rating scale as data; I may have to go back and 
> compute polychoric correlations instead of product moment, 
> but before doing that I would like to know if others have had 
> equally large factor loadings using data that are truly 
> interval level data on continuous scales.
>  
> Thanks for suggestions or information,
>  
> Ben Fairbank
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.murrell at auckland.ac.nz  Tue Jun 21 04:08:07 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 21 Jun 2005 14:08:07 +1200
Subject: [R] contourLines() starts a plot device
References: <42929877.6040502@honlab.nmfs.hawaii.edu>
	<4292F7D8.3000201@lancaster.ac.uk>
Message-ID: <42B77687.4070403@stat.auckland.ac.nz>

Hi


Barry Rowlingson wrote:
> Pierre Kleiber wrote:
> 
>> I want to use contourLines() to get contour line coordinate vectors, 
>> but I don't want to make a plot.  However contourLines() insists on
>> opening a graphics device.  Is there a way tell it not to do this?
> 
> 
> contourLines() calls .Internal(contourLines(...)), and that calls 
> do_contourlines in plot3d.c, and that gets the current graphics device, 
> hence activating one if there isn't one (I think), and passes it to 
> GEcontourLines... which then seems to do nothing with it at all.
> 
>  I just removed 'dd' from the call to GEcontourLines and the arg list to 
> GEcontourLines, fixed GraphicsEngine.h to match, recompiled, and now my 
> contourLines() function doesn't try to make a new graphics window. I 
> can't see GEcontourLines being called from anywhere else, but there may 
> be code that calls it, so possibly a better idea may be to pass a NULL 
> to it from GEcontourLines to keep the API the same.


Thanks for the diagnosis and suggested fix!
This change has now been made in the development version of R.

Paul


>  There are some comments scattered around the code about how 
> contourLines shouldnt really be a graphics function, so I'm guessing 
> there's some thought going into this already by the developers.
> 
>  Another possible quick fix for unix systems may be to start a 
> PostScript graphics device with file="/dev/null" so that the graphics 
> output disappears into thin air.
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ggrothendieck at gmail.com  Tue Jun 21 04:26:55 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 20 Jun 2005 22:26:55 -0400
Subject: [R] tapply
In-Reply-To: <cdf8178305062016153aa7e988@mail.gmail.com>
References: <cdf8178305062016153aa7e988@mail.gmail.com>
Message-ID: <971536df05062019264b9786e5@mail.gmail.com>

On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> hi,
> i have another question on tapply:
> i have a dataset z like this:
> 5540 389100307391      2600
> 5541 389100307391      2600
> 5542 389100307391      2600
> 5543 389100307391      2600
> 5544 389100307391      2600
> 5546 381300302513        NA
> 5547 387000307470        NA
> 5548 387000307470        NA
> 5549 387000307470        NA
> 5550 387000307470        NA
> 5551 387000307470        NA
> 5552 387000307470        NA
> 
> I want to sum the column 3 by column 2.
> I removed NA by calling:
> tapply(z[[3]], z[[2]], sum, na.rm=T)
> but it does not work.
> 
> then, i used
> z1<-z[!is.na(z[[3]],]
> and repeat
> still doesn't work.
> 
> please help.
> 

Depending on what you want you may be able to use rowsum:

- display only groups that have at least one non-NA with the sum
  being the sum of the non-NAs:

	with(na.omit(z), rowsum(V3, V2))

- display all groups with the sum being NA if any member is NA:

	rowsum(z$V3, z$V2)



From szlevine at nana.co.il  Tue Jun 21 08:30:22 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 21 Jun 2005 09:30:22 +0300
Subject: [R] Mixed model
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64B@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050621/2c8d9a52/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jun 21 08:45:46 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 21 Jun 2005 08:45:46 +0200
Subject: [R] how to do the test for a significant difference across
 group mean vectors
In-Reply-To: <ea57975b05062013383f6ac048@mail.gmail.com>
References: <ea57975b05062013383f6ac048@mail.gmail.com>
Message-ID: <42B7B79A.3090405@statistik.uni-dortmund.de>

wu sz wrote:
> Hello there,
> 
> how to do the test for a significant difference between two groups
> mean vectors or across several group means vectors?


This is not an R question.

You cannot test on mean differences without assumptions re. the 
variance. Please read a basic textbook on statistics.

Uwe Ligges



> Thank you,
> Shengzhe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun 21 09:34:38 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Jun 2005 08:34:38 +0100 (BST)
Subject: [R] memory allocation failures
In-Reply-To: <BAY106-F17DAA9F774FED1119DAE35CFE90@phx.gbl>
References: <BAY106-F17DAA9F774FED1119DAE35CFE90@phx.gbl>
Message-ID: <Pine.LNX.4.61.0506210829030.29854@gannet.stats>

Most likely your linear model fitting is too large for the amount of 
memory you have in your Windows box.  However, you should check ?Memory 
and in particular how your maximum RAM usage is set.  (I don't know how 
the `JGR JRI tools' give you access to R's startup settings.)

It may be that you are creating lots of large objects and so filling 
memory that way: try running gc() before the call to check your memory 
usage.  If so, you can remove (or save()) previous work.

On Mon, 20 Jun 2005, Matthew Padilla wrote:

> Hi,
>
> I am running R version rw2010 on a Windows 2000 desktop.  I am invoking R
> from Java via the JGR JRI tools.  My process consists of repeated calls to R
> in order to create linear models and process the resulting statistics.  I
> find, however, that the process often dies due to memory allocation errors:
>
> lm command:
> .model_resp10_1=lm(resp10_1~rannor10+rannor11+rannor13+rannor14+rannor15+rannor18+rannor23+rannor26+rannor28+rannor29+rannor32+rannor33+rannor35+rannor36+rannor39+rannor40+rannor43+rannor44+rannor46+rannor47+rannor48+rannor50+rannor51+rannor53+rannor55+rannor56+rannor57+rannor59).
> Garbage collection 172 = 54+41+77 (level 2) ...
> 626936 cons cells free (75%)
> 12.5 Mbytes of heap free (15%)
> Error: cannot allocate vector of size 2265 Kb
>
> I have tried to remedy this situation by corresponding calls to gc(), but
> this does not seem to fix the problem.  My files are not that large - about
> 1000 records.  Additionally, general communication to R from Java from JRI
> does seem to work - I only run into problems when repeatedly creating models
> as demonstrated above.
>
> Any help would be greatly appreciated - I am very much an R newbie.
>
> Thank you,
> Matthew
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alfonso at slafuente.net  Tue Jun 21 09:57:07 2005
From: alfonso at slafuente.net (Alfonso M Sanchez-Lafuente)
Date: Tue, 21 Jun 2005 09:57:07 +0200
Subject: [R] Another Mix Model Question
Message-ID: <42B7C853.70803@slafuente.net>

Hi again,

thank you for your previous answers. Just another question, though ...

I get the following variance components after fitting a mixed model.


Groups   Name              Variance Std.Dev. Corr
PlantID  TreatmCtrl        0.51784  0.71961
          TreatmNoAccess    4.77469  2.18511  -0.063
          TreatmNoKeel      4.22726  2.05603   0.513  0.751
          TreatmNoSpur      0.45918  0.67763   0.158  0.303  0.319
          TreatmNoStand     3.45357  1.85838  -0.736 -0.070 -0.435 0.495
PlantID  PollClassApis     1.12364  1.06002
          PollClassBombAnth 0.42769  0.65398  -0.759
Residual                   3.09669  1.75974

My question is: if n random effects are included in a model, how can I 
test the hypothesis that the variance of such effects is 0 ?

Some sort of COVTEST option in Proc MIXED in SAS (sorry, SAS is still 
more familar to me than R).

-- 

----------------------------------------------
Alfonso M. Sanchez-Lafuente
Departamento de Biologia Vegetal y Ecologia
Facultad de Biologia
Universidad de Sevilla
Avd. Reina Mercedes 9
E-41012, Sevilla, Spain
email: alfonso at slafuente.net / slafuente at us.es



From dimitris.rizopoulos at med.kuleuven.be  Tue Jun 21 10:22:31 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Tue, 21 Jun 2005 10:22:31 +0200
Subject: [R] Another Mix Model Question
References: <42B7C853.70803@slafuente.net>
Message-ID: <009f01c5763a$5e4ef220$0540210a@www.domain>

AFAIK the COVTEST option just computes a Wald test! Since you want 
test for variance components (which is on the boundary of the 
parameter space), I'd suggest to use a LRT (i.e., anova.lme(model.1, 
model.2)) and moreover consider the simulate.lme() function of the 
"nlme" package.


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Alfonso M Sanchez-Lafuente" <alfonso at slafuente.net>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 21, 2005 9:57 AM
Subject: [R] Another Mix Model Question


> Hi again,
>
> thank you for your previous answers. Just another question, though 
> ...
>
> I get the following variance components after fitting a mixed model.
>
>
> Groups   Name              Variance Std.Dev. Corr
> PlantID  TreatmCtrl        0.51784  0.71961
>          TreatmNoAccess    4.77469  2.18511  -0.063
>          TreatmNoKeel      4.22726  2.05603   0.513  0.751
>          TreatmNoSpur      0.45918  0.67763   0.158  0.303  0.319
>          TreatmNoStand     3.45357  1.85838  -0.736 -0.070 -0.435 
> 0.495
> PlantID  PollClassApis     1.12364  1.06002
>          PollClassBombAnth 0.42769  0.65398  -0.759
> Residual                   3.09669  1.75974
>
> My question is: if n random effects are included in a model, how can 
> I
> test the hypothesis that the variance of such effects is 0 ?
>
> Some sort of COVTEST option in Proc MIXED in SAS (sorry, SAS is 
> still
> more familar to me than R).
>
> -- 
>
> ----------------------------------------------
> Alfonso M. Sanchez-Lafuente
> Departamento de Biologia Vegetal y Ecologia
> Facultad de Biologia
> Universidad de Sevilla
> Avd. Reina Mercedes 9
> E-41012, Sevilla, Spain
> email: alfonso at slafuente.net / slafuente at us.es
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ecoinformatics at gmail.com  Tue Jun 21 11:31:46 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Tue, 21 Jun 2005 11:31:46 +0200
Subject: [R] How to plot circular data in the directions of 0, 0.5pi,
	pi and 1.5pi
Message-ID: <15f8e67d05062102316a9d0bed@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050621/981d3630/attachment.pl

From mailpuls at gmx.net  Tue Jun 21 11:57:39 2005
From: mailpuls at gmx.net (Christfried Kunath)
Date: Tue, 21 Jun 2005 11:57:39 +0200 (MEST)
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
	plinear - PI curve fitting
Message-ID: <25219.1119347859@www24.gmx.net>

Hello,

i have a problem with the function nls().

This are my data in "k":
        V1    V2
 [1,]    0 0.367
 [2,]   85 0.296
 [3,]  122 0.260
 [4,]  192 0.244
 [5,]  275 0.175
 [6,]  421 0.140
 [7,]  603 0.093
 [8,]  831 0.068
 [9,] 1140 0.043
 
With the nls()-function i want to fit following formula whereas a,b, and c
are variables: y~1/(a*x^2+b*x+c)

With the standardalgorithm "Newton-Gauss" the fitted curve contain an peak
near the second x,y-point.
This peak is not correct for my purpose. The fitted curve should descend
from the maximum y to the minimum y given in my data.

The algorithm "plinear" give me following error:


   phi function(x,y) {  
k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
       coef(k.nls)
   }

   phi(k[,1],k[,2])

   Error in qr.solve(QR.B, cc) : singular matrix `a' in solve


I have found in the mailinglist
"https://stat.ethz.ch/pipermail/r-help/2001-July/012196.html" that is if t
he data are artificial. But the data are from my measurment.

The commercial software "Origin V.6.1" solved this problem with the
Levenberg-Marquardt algorithm how i want.
The reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982

What are the right way or algorithm for me to solve this problem and what
means this error with alg="plinear"?

Thanks in advance.

-- 
Weitersagen: GMX DSL-Flatrates mit Tempo-Garantie!
Ab 4,99 Euro/Monat: http://www.gmx.net/de/go/dsl



From ripley at stats.ox.ac.uk  Tue Jun 21 12:08:40 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Jun 2005 11:08:40 +0100 (BST)
Subject: [R] linking R to goto blas
In-Reply-To: <42B6F31F.5020904@wu-wien.ac.at>
References: <42AC4160.90505@wu-wien.ac.at>
	<x2zmtvzjjf.fsf@turmalin.kubism.ku.dk>
	<x2vf4jzi85.fsf@turmalin.kubism.ku.dk> <42ADD7F1.2030905@wu-wien.ac.at>
	<Pine.LNX.4.61.0506132202360.5304@gannet.stats>
	<42B6F31F.5020904@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0506211101370.5070@gannet.stats>

OTOH, the Opteron Goto BLAS has no such problems on FC3, so I suspect this 
is specific to only some of the architectures.

I've added a comment in the R-admin manual that says

   It has been reported that on some RedHat-based Linux systems it is
   necessary to set @code{OMP_NUM_THREADS=1} in the environment when using
   a multi-threaded Goto BLAS, but others run happily with multiple threads.

Note the `when using': it will need to be set whenever R is run.


On Mon, 20 Jun 2005, Stefan Sobernig wrote:

> Dear all,
>
> Finally -- after one week of trial & error -- the fog lifted thanks to
> the hints of Kazushige Goto.
> Therefore, I would like to report back the workaround (for Fedora Core
> based systems, including FC4) to the list and a pre-mature analysis,
> inspired
> by K. Goto:
>
> A brief howto (applicable to and tested on Fedore Core 4, R-2.1.0,
> libgoto 0.99-3):
>
> 1) Get the appropriate GOTO BLAS library for  your  processor core and
> place it in an arbitrary directory (either /usr/lib or your home directory).
>
> 2) set the environment variable OMP_NUM_THREADS=1 (in bash: "export
> OMP_NUM_THREADS=1")
>
> 3) change to your R source directory and run configure according to your
> needs: pass the following values to the "--with-blas-flag":
> --with-blas="-L</your/path/to/libgoto> -lgoto_xxx-xx-r0.99-3 -lpthread"
> (in my case: --with-blas="-L/usr/lib/ -lgoto_northwood-32-r0.99-3
> -lpthread").
>
> 4) run make & make install
>
> 5) enjoy GOTO performance
>
> BACKGROUND:
>
> According to K. Goto setting OMP_NUM_THREADS=1 results in libgoto not
> using the pthread library, therefore it is running in
> single-thread-mode, while R might still be supporting threads in
> general. He concludes that there is a major incompatibility with pthread
> which might be due to RedHat's/ Fedora's home-brewed (since RH 9)
> reliance on multithreading simulation provided by their Native POSIX
> Thread Library (NLTP). Libgoto as such is not NLTP-compatible according
> to K. Goto. Other distributions (and thread implementations) don't show
> this misbehaviour (on my Debian based system libgoto links perfectly
> without the workaround described above).
>
> Thanks for all your comments and contributions.
>
> //stefan at vienna
>
>
>
> Prof Brian Ripley wrote:
>
>> R already contains a xerbla: you should not be adding one.
>>
>> On Mon, 13 Jun 2005, Stefan Sobernig wrote:
>>
>>> Prof. Ritley, Mr. Dalgaard, thank you very much for the immediate reply
>>> and your efforts!
>>> We investigated a littlle further, though not directly following Mr.
>>> Dalgaard's findings
>>> on dynamic linking. We got aware of another short notice published on
>>> the R-devel list,
>>> providing some hints for installing R and goto (see
>>> http://maths.newcastle.edu.au/~rking/R/devel/03b/1267.html):
>>>
>>>
>>> ############ snip ###################################
>>> [What I did to link R against Goto's BLAS (mostly following Prof. Bates'
>>> instruction):
>>> - Install libgoto*.so and symlink to libgoto.so.
>>> - Download xerbla.f and compile to xerbla.o.
>>> - Run the R configure script with --with-blas="-lgoto
>>> /path/to/xerbla.o".
>>> - Edit Makeconf and delete the path to xerbla.o in BLAS_LIB.
>>> - make; make check
>>> ]
>>> ############ snip ###################################
>>>
>>> * Therefore, we got the auxiliary lapack routine xerbla offered at
>>> http://www.cs.utexas.edu/users/kgoto/libraries/xerbla.c
>>> and compiled it by calling: "gcc -c xerbla.c -o xerbla.o" (please note,
>>> that I am not that expierenced in C programming as such,
>>> my knowledge is limited to deploying and fixing c-coded apps). Am I
>>> right to say that this xerbla extension changes the entry
>>> point of the goto library?
>>>
>>> * Then, we re-configured R using: ./configure --prefix=/usr
>>> --with-blas="-L/usr/lib/ -lgoto_prescott-32-r0.99-3
>>> /home/ssoberni/xerbla.o -lpthread"
>>>
>>> * Unfortunately, this resulted in another error affecting the configure
>>> process so that BLAS_LIBS is set to "-lblas" (pointing to standard blas
>>> libs):
>>>
>>> ########### config.log ################################
>>> configure:32429: gcc -o conftest -g -O2  -I/usr/local/include
>>> -L/usr/local/lib conftest.c -L/usr/lib/ -lgoto_prescott-32-r0.99-3
>>> /home/ssoberni/xerbla.o -lpthread  -lg2c -lm -lgcc_s -ldl -lm  >&5
>>> /home/ssoberni/xerbla.o(.text+0x0): In function `xerbla_':
>>> : multiple definition of `xerbla_'
>>> /tmp/ccMxAKbu.o(.text+0x0):/home/ssoberni/R-2.1.0/conftest.c:143: first
>>> defined here
>>> /usr/bin/ld: Warning: size of symbol `xerbla_' changed from 5 in
>>> /tmp/ccMxAKbu.o to 43 in /home/ssoberni/xerbla.o
>>> collect2: ld returned 1 exit status
>>> ########### config.log ################################
>>>
>>> We keep on trying ...
>>> Thanks for your thoughts ...
>>>
>>> //stefan
>>>
>>>
>>> Peter Dalgaard wrote:
>>>
>>>> Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:
>>>>
>>>>
>>>>
>>>>> Stefan Sobernig <stefan.sobernig at wu-wien.ac.at> writes:
>>>>>
>>>>>
>>>>>
>>>>>> Dear all,
>>>>>>
>>>>>> I am currently trying to link R 2.1.0 to the GOTO BLAS 0.99.3
>>>>>> library on
>>>>>> a box running Fedora Core 3 , basically following the steps
>>>>>> indicated in
>>>>>> the R-Admin document:
>>>>>>
>>>>>> 1: I downloaded the current libgoto.xxx.so from
>>>>>> http://www.cs.utexas.edu/users/kgoto/libraries/libgoto_prescott-32-r0.99-3.so.gz,
>>>>>>
>>>>>> a version suitable for our XEON machine (Nocona core), unpacked it to
>>>>>> /usr/lib and created a symlink libgoto.so pointing to the library.
>>>>>>
>>>>>> 2: Then, I got ready to re-configure and re-compile R (2.1.0)
>>>>>> using the
>>>>>> following configure flags: ./configure --prefix=/usr --enable-R-shlib
>>>>>> --enable-shared --with-tcltk --with-blas="-lgoto -lpthread -lm"
>>>>>>
>>>>>>
>>>>> ...
>>>>>
>>>>>
>>>>>> Please, I highly appreciate any thoughts or hints as my colleagues
>>>>>> and I
>>>>>> are eager to get into GOTO's universe.
>>>>>>
>>>>>>
>>>>> Hmm. Looks over-complicated to me. What works for me on AMD64 is to
>>>>> have a config.site file in my BUILD-GOTO directory, containing
>>>>>
>>>>>
>>>>>
>>>>>> cat config.site
>>>>>>
>>>>>>
>>>>> BLAS_LIBS="-L/home/pd/GOTO -lgoto_opt64p-r0.96 -lpthread"
>>>>> CFLAGS="-O3 -g"
>>>>> #CFLAGS="-g"
>>>>> FFLAGS=$CFLAGS
>>>>> CXXFLAGS=$CFLAGS
>>>>>
>>>>> (the .*FLAGS business is optional, of course). With this in place, a
>>>>> simple ../R/configure followed by make seems to do the trick.
>>>>>
>>>>> I'll give it a try on my FC3 system, but it's a 500 MHz PIII, so it
>>>>> takes a while...
>>>>>
>>>>>
>>>>
>>>> Hmm... That gives me the grDevices issue, which boils down to an R
>>>> that segfaults immediately upon startup, in
>>>>
>>>> #0  0x05c0aea7 in tilde_expand () from /usr/lib/libreadline.so.4
>>>> #1  0x08170254 in R_ExpandFileName_readline (
>>>>    s=0x8bb4020
>>>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb",
>>>> #buff=0x8295300
>>>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/grDevices")
>>>>    at ../../../R/src/unix/sys-std.c:406
>>>> #2  0x0816f5da in R_ExpandFileName (
>>>>    s=0x8bb4020
>>>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb") at
>>>> #../../../R/src/unix/sys-unix.c:129
>>>> #3  0x08167352 in R_FileExists (
>>>>    path=0x8bb4020
>>>> #"/home/pd/r-patched/BUILD-GOTO/library/grDevices/R/sysdata.rdb") at
>>>> #stat.h:365
>>>> #4  0x08105da3 in do_fileexists (call=0x84fd544, op=0x82c5f78,
>>>> #args=0x0,
>>>>    rho=0x8c514a4) at ../../../R/src/main/platform.c:857
>>>> #5  0x080edc85 in do_internal (call=0x0, op=0x82ba5d4, args=0x3920,
>>>>    env=0x8c514a4) at ../../../R/src/main/names.c:1078
>>>> #6  0x080c0daa in Rf_eval (e=0x84fd57c, rho=0x8c514a4)
>>>>    at ../../../R/src/main/eval.c:382
>>>> #7  0x080c3695 in Rf_applyClosure (call=0x8668c28, op=0x84fd5b4,
>>>>    arglist=0x8c50564, rho=0x8ad5cc4, suppliedenv=0x82aa5f0)
>>>>
>>>> running --no-readline gives me another crash
>>>>
>>>> (gdb) bt
>>>> #0  0x003fb0da in strcmp () from /lib/ld-linux.so.2
>>>> #1  0x003f009a in _dl_map_object () from /lib/ld-linux.so.2
>>>> #2  0x004fdb58 in dl_open_worker () from /lib/tls/libc.so.6
>>>> #3  0x00000000 in ?? ()
>>>>
>>>> ...which suggests that something is up with dynamic linking.
>>>>
>>>> I'll give it another spin...
>>>>
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>>
>>> Stefan Sobernig
>>> Department of Information Systems and New Media
>>> Vienna University of Economics
>>> Augasse 2-6
>>> A - 1090 Vienna
>>>
>>> Phone: +43 - 1 - 31336 - 4878
>>> Fax: +43 - 1 - 31336 - 746
>>> Email: stefan.sobernig at wu-wien.ac.at
>>> <mailto:stefan.sobernig at wu-wien.ac.at>
>>> PubKey: http://julia.wu-wien.ac.at/~ssoberni/0x5FC2D3FA.asc
>>> <http://julia.wu-wien.ac.at/%7Essoberni/0x5FC2D3FA.asc>
>>>
>>>
>>>
>>
>
>
> -- 
>
> Stefan Sobernig
> Department of Information Systems and New Media
> Vienna University of Economics
> Augasse 2-6
> A - 1090 Vienna
>
> Phone: +43 - 1 - 31336 - 4878
> Fax: +43 - 1 - 31336 - 746
> Email: stefan.sobernig at wu-wien.ac.at <mailto:stefan.sobernig at wu-wien.ac.at>
> PubKey: http://julia.wu-wien.ac.at/~ssoberni/0x5FC2D3FA.asc <http://julia.wu-wien.ac.at/%7Essoberni/0x5FC2D3FA.asc>
>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From berwin at maths.uwa.edu.au  Tue Jun 21 12:31:04 2005
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Tue, 21 Jun 2005 18:31:04 +0800
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
	plinear - PI curve fitting
In-Reply-To: <25219.1119347859@www24.gmx.net>
References: <25219.1119347859@www24.gmx.net>
Message-ID: <17079.60520.920990.997837@bossiaea.maths.uwa.edu.au>

G'day Chris,

>>>>> "CK" == Christfried Kunath <mailpuls at gmx.net> writes:

    CK> With the nls()-function i want to fit following formula
    CK> whereas a,b, and c are variables: y~1/(a*x^2+b*x+c)

    CK> [...]

    CK> The algorithm "plinear" give me following error:
The algorithm "plinear" is inappropriate for your data and your model
since none of the parameters are linear.  You are actually trying to
fit the model
                y ~ d/(a*x^2+b*x+c)

where `d' would be the linear parameter.

    CK> phi function(x,y) {
    CK> k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
    CK> coef(k.nls) }

    CK> [...]

    CK> The commercial software "Origin V.6.1" solved this problem
    CK> with the Levenberg-Marquardt algorithm how i want.  The
    CK> reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982

    CK> What are the right way or algorithm for me to solve this
    CK> problem and what means this error with alg="plinear"?
The error means that at some point along the way a matrix was
calculated that needed to be inverted but was for all practical
purposes singular.  This can happen in numerical optimisation
problems, in particular if derivatives have to be calculated
numerically.

How to solve this problem:

1) Don't use the algorithm "plinear" since it is inappropriate for
   your model.

2) You may want to specify the gradient of the function that you are
   minimising to make life easier for nls(), see Venables & Ripley
   (2002, page 215) for an example.

3) You can call nls() directly without specifying the plinear option:
   (I renamed the variables in the data frame to x and y for
   simplicity) 

      > nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),data=k)
      Nonlinear regression model
        model:  y ~ 1/(a * (x^2) + b * x + c) 
         data:  k 
                  a             b             c 
      -7.326117e-05  4.770514e-02  2.490643e+00 
       residual sum-of-squares:  0.1120086

   But the results seem to be highly depended on your starting values:

      > nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.00005,b=0.002,c=2.5),data=k)
      Nonlinear regression model
        model:  y ~ 1/(a * (x^2) + b * x + c) 
         data:  k 
                 a            b            c 
      9.690204e-06 6.885570e-03 2.729825e+00 
       residual sum-of-squares:  0.000547369 

   Which is of some concern.

4) If you really want to fit the above model, you may also consider to
   just use the glm() command and fit it within a generalised linear
   model framework:

      > glm(y ~ I(x^2) + x, data=k, family=gaussian(link="inverse"))
      
      Call:  glm(formula = y ~ I(x^2) + x, family = gaussian(link = "inverse"),      data = k) 
      
      Coefficients:
      (Intercept)       I(x^2)            x  
        2.730e+00    9.690e-06    6.886e-03  
      
      Degrees of Freedom: 8 Total (i.e. Null);  6 Residual
      Null Deviance:	    0.09894 
      Residual Deviance: 0.0005474 	AIC: -53.83 

HTH.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 6488 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 6488 3383 (self)      
The University of Western Australia   FAX : +61 (8) 6488 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From Kurt.Hornik at wu-wien.ac.at  Tue Jun 21 12:43:53 2005
From: Kurt.Hornik at wu-wien.ac.at (Kurt Hornik)
Date: Tue, 21 Jun 2005 12:43:53 +0200
Subject: [R] Job Openings at WU Wien
Message-ID: <17079.61289.522370.563726@mithrandir.hornik.net>


The Department of Statistics and Mathematics at the Vienna University of
Economics and Business Administration invites applications for two new
faculty positions in computational statistics and quantitative research
methodology, to begin in fall 2005.  The positions will be at the
Assistant level.  Candidates should have a strong potential for
statistical computing or intramural research support and statistical
consulting, be interested in involving graduate and undergraduate
students in their research, have completed their Ph.D. or a comparable
degree by June 2005, and be citizens of the European Union.

We seek candidates who can teach a graduate course in advanced applied
statistics and quantitative research methodology and other courses at
the graduate and undergraduate level, mentor students in undergraduate
and graduate research projects as well as Master's and Ph.D. theses.
Candidates should have a strong background in one of the following
areas: psychometrics, computational management or social sciences, and
information systems.  Desirable knowledge and skills include topics such
as statistical software development, quantitative research methodology,
and advanced applied statistical techniques.

The Department of Statistics and Mathematics at the Vienna University of
Economics and Business Administration (WU Wien) has a strong research
focus with currently 14 full time faculty with substantial graduate and
undergraduate teaching responsibilities.  It maintains a leading
position in the development of R, a comprehensive open source
environment for statistical computing.  WU Wien
(http://www.wu-wien.ac.at/english/about) is one of the leading Central
European institutions for international business education with about
20,000 students and more than 1,000 full-time and adjunct faculty and
staff members.

Applicants should submit a letter of interest (with reference numbers
43448 [6-yr position] or 42948 [4-yr position]), current vitae, recent
papers, etc., by July 18, 2005 to

  PERSONALABTEILUNG
  Wirtschaftsuniversitaet Wien
  Augasse 2-6
  1090 Vienna
  Austria


Kurt Hornik, Chair
Department of Statistics and Mathematics
Wirtschaftsuniversitaet Wien



From powdersoul at web.de  Tue Jun 21 12:49:44 2005
From: powdersoul at web.de (powdersoul@web.de)
Date: Tue, 21 Jun 2005 12:49:44 +0200
Subject: [R] X11, interactive device
Message-ID: <2039689176@web.de>


Hey,

i'm trying to set up an cgi web-application that produces maps and other graphics with R. for local testing i used my laptop which has an win xp OS. there everything works fine. now i try to move the whole system over on an unix machine, mainly because i want to establish a connection to a database. anyway, i'm using R 2.0.0 on an other computer in a local network. here comes my problem:

i can't create jpeg-files, neither by starting R via my cgi-script nor by pasting the syntax directly into R. both ways i get the following error message:

error in X11; unable to start device JPEG; could not open JPEG file.

i found out that there is no interactive graphic device for the installed R-Version or it is not set correctly. as i tried dev.interactive(), the response was [1] FALSE.

i'm a bit lost at the moment. i wonder if there is a chance to change or modify the interactive graphic device or if i really need to run Xvfb "device" as someone suggested before on the R-help mailing list. does someone has experience with this kind of trouble, any help available from somewhere?

thanks in advance, lars



From ggrothendieck at gmail.com  Tue Jun 21 12:57:19 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 21 Jun 2005 06:57:19 -0400
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
	plinear - PI curve fitting
In-Reply-To: <25219.1119347859@www24.gmx.net>
References: <25219.1119347859@www24.gmx.net>
Message-ID: <971536df0506210357213a33c5@mail.gmail.com>

On 6/21/05, Christfried Kunath <mailpuls at gmx.net> wrote:
> Hello,
> 
> i have a problem with the function nls().
> 
> This are my data in "k":
>        V1    V2
>  [1,]    0 0.367
>  [2,]   85 0.296
>  [3,]  122 0.260
>  [4,]  192 0.244
>  [5,]  275 0.175
>  [6,]  421 0.140
>  [7,]  603 0.093
>  [8,]  831 0.068
>  [9,] 1140 0.043
> 
> With the nls()-function i want to fit following formula whereas a,b, and c
> are variables: y~1/(a*x^2+b*x+c)
> 
> With the standardalgorithm "Newton-Gauss" the fitted curve contain an peak
> near the second x,y-point.
> This peak is not correct for my purpose. The fitted curve should descend
> from the maximum y to the minimum y given in my data.
> 
> The algorithm "plinear" give me following error:
> 
> 
>   phi function(x,y) {
> k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
>       coef(k.nls)
>   }
> 
>   phi(k[,1],k[,2])
> 
>   Error in qr.solve(QR.B, cc) : singular matrix `a' in solve
> 
> 
> I have found in the mailinglist
> "https://stat.ethz.ch/pipermail/r-help/2001-July/012196.html" that is if t
> he data are artificial. But the data are from my measurment.
> 
> The commercial software "Origin V.6.1" solved this problem with the
> Levenberg-Marquardt algorithm how i want.
> The reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982
> 
> What are the right way or algorithm for me to solve this problem and what
> means this error with alg="plinear"?
> 
> Thanks in advance.

This is not a direct answer to your question but log(y) looks nearly linear
in x when plotting them together and log(y) ~ a + b*x or
y ~ a*exp(b*x) will always be monotonic.  Also, this model uses only 2
rather than 3 parameters.



From r.shengzhe at gmail.com  Tue Jun 21 13:00:12 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 21 Jun 2005 13:00:12 +0200
Subject: [R] test for equality of two data sets with multidimensional
	variables
Message-ID: <ea57975b05062104002afca49@mail.gmail.com>

Hello there,

I have two data sets with 14 variables each, and wish to do the test
for equality of their covariance matrices and mean vectors. Normally
these tests should be done by chi square test (box provided) and
Hotelling's T square test respectively. Which R functions could do
this kind of test? I just find some functions could do for one
dimension, but no for multidimension. Some one suggests bartlett.test,
but it seems just works for one dimension. Do you know which ones
could do that, or I have to do R programming by myself?

Thank you,
Shengzhe



From jmoreira at fe.up.pt  Tue Jun 21 13:09:21 2005
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Tue, 21 Jun 2005 12:09:21 +0100
Subject: [R] Data.frames with different line's length
Message-ID: <003801c57651$ad9ca540$5e7aa8c0@FEUPsig.fe.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050621/70a04052/attachment.pl

From ggrothendieck at gmail.com  Tue Jun 21 13:12:44 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 21 Jun 2005 07:12:44 -0400
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
	plinear - PI curve fitting
In-Reply-To: <971536df0506210357213a33c5@mail.gmail.com>
References: <25219.1119347859@www24.gmx.net>
	<971536df0506210357213a33c5@mail.gmail.com>
Message-ID: <971536df05062104125781ab84@mail.gmail.com>

On 6/21/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/21/05, Christfried Kunath <mailpuls at gmx.net> wrote:
> > Hello,
> >
> > i have a problem with the function nls().
> >
> > This are my data in "k":
> >        V1    V2
> >  [1,]    0 0.367
> >  [2,]   85 0.296
> >  [3,]  122 0.260
> >  [4,]  192 0.244
> >  [5,]  275 0.175
> >  [6,]  421 0.140
> >  [7,]  603 0.093
> >  [8,]  831 0.068
> >  [9,] 1140 0.043
> >
> > With the nls()-function i want to fit following formula whereas a,b, and c
> > are variables: y~1/(a*x^2+b*x+c)
> >
> > With the standardalgorithm "Newton-Gauss" the fitted curve contain an peak
> > near the second x,y-point.
> > This peak is not correct for my purpose. The fitted curve should descend
> > from the maximum y to the minimum y given in my data.
> >
> > The algorithm "plinear" give me following error:
> >
> >
> >   phi function(x,y) {
> > k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
> >       coef(k.nls)
> >   }
> >
> >   phi(k[,1],k[,2])
> >
> >   Error in qr.solve(QR.B, cc) : singular matrix `a' in solve
> >
> >
> > I have found in the mailinglist
> > "https://stat.ethz.ch/pipermail/r-help/2001-July/012196.html" that is if t
> > he data are artificial. But the data are from my measurment.
> >
> > The commercial software "Origin V.6.1" solved this problem with the
> > Levenberg-Marquardt algorithm how i want.
> > The reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982
> >
> > What are the right way or algorithm for me to solve this problem and what
> > means this error with alg="plinear"?
> >
> > Thanks in advance.
> 
> This is not a direct answer to your question but log(y) looks nearly linear
> in x when plotting them together and log(y) ~ a + b*x or
> y ~ a*exp(b*x) will always be monotonic.  Also, this model uses only 2
> rather than 3 parameters.
> 

One other comment.  If you do want to use your model try fitting 1/y
first to get your starting value since that model has a unique solution:

> res1 <- nls(1/y ~ a*x^2 + b*x + c, start = list(a=0,b=0,c=0))
> res2 <- nls(y ~ 1/(a*x^2 + b*x + c), start = as.list(coef(res1)))
> res2
Nonlinear regression model
  model:  y ~ 1/(a * x^2 + b * x + c) 
   data:  parent.frame() 
           a            b            c 
9.690187e-06 6.885577e-03 2.729825e+00 
 residual sum-of-squares:  0.000547369



From r.hankin at noc.soton.ac.uk  Tue Jun 21 13:16:58 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 21 Jun 2005 12:16:58 +0100
Subject: [R] sweep() and recycling
In-Reply-To: <Pine.LNX.4.61.0506201653001.19145@gannet.stats>
References: <s2b6cdaf.059@spirit.csv.warwick.ac.uk>
	<Pine.LNX.4.61.0506201653001.19145@gannet.stats>
Message-ID: <9e7563d19a8dfc3428bbb2576f323f6d@soc.soton.ac.uk>


On Jun 20, 2005, at 04:58 pm, Prof Brian Ripley wrote:

> The issue here is that the equivalent command array(1:5, c(6,6)) (to 
> matrix(1:5,6,6)) gives no warning, and sweep uses array().
>
> I am not sure either should: fractional recycling was normally allowed 
> in S3 (S4 tightened up a bit).
>
> Perhaps someone who thinks sweep() should warn could contribute a 
> tested patch?
>


OK,  modified R code and Rd file below (is this the best way to do 
this?)




"sweep" <-
   function (x, MARGIN, STATS, FUN = "-", give.warning = FALSE, ...)
{
   FUN <- match.fun(FUN)
   dims <- dim(x)
   if(give.warning & length(STATS)>1 & any(dims[MARGIN] != 
dim(as.array(STATS)))){
     warning("array extents do not recycle exactly")
   }
   perm <- c(MARGIN, (1:length(dims))[-MARGIN])
   FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
}







\name{sweep}
\alias{sweep}
\title{Sweep out Array Summaries}
\description{
   Return an array obtained from an input array by sweeping out a summary
   statistic.
}
\usage{
sweep(x, MARGIN, STATS, FUN="-", give.warning = FALSE, \dots)
}
\arguments{
   \item{x}{an array.}
   \item{MARGIN}{a vector of indices giving the extents of \code{x}
     which correspond to \code{STATS}.}
   \item{STATS}{the summary statistic which is to be swept out.}
   \item{FUN}{the function to be used to carry out the sweep.  In the
     case of binary operators such as \code{"/"} etc., the function name
     must be quoted.}
   \item{give.warning}{Boolean, with default \code{FALSE} meaning to
   give no warning, even if array extents do not match.  If
   \code{TRUE}, check for the correct dimensions and if a
   mismatch is detected, give a suitable warning.}
   \item{\dots}{optional arguments to \code{FUN}.}
}
\value{
   An array with the same shape as \code{x}, but with the summary
   statistics swept out.
}
\note{
   If \code{STATS} is of length 1, recycling is carried out with no
   warning irrespective of the value of \code{give.warning}.
}

\references{
   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
   \emph{The New S Language}.
   Wadsworth \& Brooks/Cole.
}
\seealso{
   \code{\link{apply}} on which \code{sweep} used to be based;
   \code{\link{scale}} for centering and scaling.
}
\examples{
require(stats) # for median
med.att <- apply(attitude, 2, median)
sweep(data.matrix(attitude), 2, med.att)# subtract the column medians

a <- array(0, c(2, 3, 4))
b <- matrix(1:8, c(2, 4))
sweep(a, c(1, 3), b, "+", give.warning = TRUE)  # no warning: 
all(dim(a)[c(1,3)] == dim(b))
sweep(a, c(1, 2), b, "+", give.warning = TRUE)  # warning given

}
\keyword{array}
\keyword{iteration}




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From giles.heywood at uk.abnamro.com  Tue Jun 21 13:19:24 2005
From: giles.heywood at uk.abnamro.com (giles.heywood@uk.abnamro.com)
Date: Tue, 21 Jun 2005 12:19:24 +0100
Subject: [R] How to define S4 methods for '['
Message-ID: <OF1F783713.6ACDAFCC-ONC1257027.003D61F4@abnamro.com>





I have found this a useful correspondence.  My own wish is to define a new
S4 class which differs from
class 'array' only in its default handling of the 'drop' argument i.e.
drop=FALSE - not an unusual wish.

My solution is the following:

setClass("noDropArray",representation("array"),prototype=array(NA,2:4))

setMethod('[','noDropArray',function(x,i,j,drop,...){
    cl<-as.list(sys.call())
    if( !("drop"%in%names(as.list(sys.call()))) )
        cl <-  c(cl,drop=FALSE)
    cl[[2]]<-x at .Data
    eval(as.call(cl))
    })

new("noDropArray")[1,1,1]

This behaves as I would expect.  However I have an uncomfortable feeling
that this is not the most
elegant solution.

Any comments?

- Giles


> Thanks to Robert and Gabor for their replies, but neither was what I was
> looking for, undoubtedly because of the poor phrasing of my question (+ a
> typo -- however, even if correctly typed it doesn't work). I finally
> realized that the "elegant" approach I sought can easily be done without
> setGeneric:

> setMethod('[','foo',function(x,i,j,drop,...){
>    cl<-as.list(sys.call())
>    cl[[2]]<-x at dat
>    eval(as.call(cl))
>    })

> Indeed, this is a general template for this sort of thing. Should have
> thought of this before, as V&R's S PROGRAMMING has numerous such
examples...
> Sigh...

> -- Bert

-----Original Message-----
From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
Sent: Monday, June 20, 2005 1:31 PM
To: Berton Gunter
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] How to define S4 methods for '['

On 6/20/05, Berton Gunter <gunter.berton at gene.com> wrote:
> Folks:
>
> This is a question about the S4 formal class system.
>
> Suppose I have a class, 'foo', defined by:
>
> setClass('foo',representation(dat='matrix', id='character') )
>
> I wish to define a '[' method for foo that will extract from the 'dat'
slot.
> I would have thought that the following would work, but it doesn't:
>
> setMethod("[","foo",function(x,i, j, .,drop=TRUE)callGeneric(x at dat,i,
> j,drop=drop) )
>
> The only way I have succeeded in defining this method is using brute
force
> eval(parse(. :
>
> {eval(parse(text=paste('.dat(x)[',
>        ifelse(missing(i),',','i,'),
>        ifelse(missing(j),']','j]'))))
>        }
>
> This works. However, I am not able under any circumstances to pass the
drop
> argument -- it is ignored.
>
> I would appreciate any pointers about how to do this properly. If  this
is
> explicitly in the Green Book (I do not have it with me at the moment),
that
> will suffice.
>

Download the source to the 'its' package where an S4 [ method is defined.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


---------------------------------------------------------------------------
This message (including any attachments) is confidential and...{{dropped}}



From ggrothendieck at gmail.com  Tue Jun 21 14:10:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 21 Jun 2005 08:10:28 -0400
Subject: [R] How to define S4 methods for '['
In-Reply-To: <OF1F783713.6ACDAFCC-ONC1257027.003D61F4@abnamro.com>
References: <OF1F783713.6ACDAFCC-ONC1257027.003D61F4@abnamro.com>
Message-ID: <971536df05062105105063b723@mail.gmail.com>

I don't think conversion to a list is necessary, e.g.

f <- function(x, y, ...) {
	cl <- sys.call()
	if (missing(y)) cl$y <- FALSE
	cl[[1]] <- as.name("cat")
	cl[[2]] <- rev(x)
	eval(cl)
}
f(1:4)  # 4 3 2 1 FALSE



On 6/21/05, giles.heywood at uk.abnamro.com <giles.heywood at uk.abnamro.com> wrote:
> 
> 
> 
> 
> I have found this a useful correspondence.  My own wish is to define a new
> S4 class which differs from
> class 'array' only in its default handling of the 'drop' argument i.e.
> drop=FALSE - not an unusual wish.
> 
> My solution is the following:
> 
> setClass("noDropArray",representation("array"),prototype=array(NA,2:4))
> 
> setMethod('[','noDropArray',function(x,i,j,drop,...){
>    cl<-as.list(sys.call())
>    if( !("drop"%in%names(as.list(sys.call()))) )
>        cl <-  c(cl,drop=FALSE)
>    cl[[2]]<-x at .Data
>    eval(as.call(cl))
>    })
> 
> new("noDropArray")[1,1,1]
> 
> This behaves as I would expect.  However I have an uncomfortable feeling
> that this is not the most
> elegant solution.
> 
> Any comments?
> 
> - Giles
> 
> 
> > Thanks to Robert and Gabor for their replies, but neither was what I was
> > looking for, undoubtedly because of the poor phrasing of my question (+ a
> > typo -- however, even if correctly typed it doesn't work). I finally
> > realized that the "elegant" approach I sought can easily be done without
> > setGeneric:
> 
> > setMethod('[','foo',function(x,i,j,drop,...){
> >    cl<-as.list(sys.call())
> >    cl[[2]]<-x at dat
> >    eval(as.call(cl))
> >    })
> 
> > Indeed, this is a general template for this sort of thing. Should have
> > thought of this before, as V&R's S PROGRAMMING has numerous such
> examples...
> > Sigh...
> 
> > -- Bert
> 
> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Sent: Monday, June 20, 2005 1:31 PM
> To: Berton Gunter
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to define S4 methods for '['
> 
> On 6/20/05, Berton Gunter <gunter.berton at gene.com> wrote:
> > Folks:
> >
> > This is a question about the S4 formal class system.
> >
> > Suppose I have a class, 'foo', defined by:
> >
> > setClass('foo',representation(dat='matrix', id='character') )
> >
> > I wish to define a '[' method for foo that will extract from the 'dat'
> slot.
> > I would have thought that the following would work, but it doesn't:
> >
> > setMethod("[","foo",function(x,i, j, .,drop=TRUE)callGeneric(x at dat,i,
> > j,drop=drop) )
> >
> > The only way I have succeeded in defining this method is using brute
> force
> > eval(parse(. :
> >
> > {eval(parse(text=paste('.dat(x)[',
> >        ifelse(missing(i),',','i,'),
> >        ifelse(missing(j),']','j]'))))
> >        }
> >
> > This works. However, I am not able under any circumstances to pass the
> drop
> > argument -- it is ignored.
> >
> > I would appreciate any pointers about how to do this properly. If  this
> is
> > explicitly in the Green Book (I do not have it with me at the moment),
> that
> > will suffice.
> >
> 
> Download the source to the 'its' package where an S4 [ method is defined.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> ---------------------------------------------------------------------------
> This message (including any attachments) is confidential and...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at gmail.com  Tue Jun 21 14:25:24 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 21 Jun 2005 08:25:24 -0400
Subject: [R] sweep() and recycling
In-Reply-To: <9e7563d19a8dfc3428bbb2576f323f6d@soc.soton.ac.uk>
References: <s2b6cdaf.059@spirit.csv.warwick.ac.uk>
	<Pine.LNX.4.61.0506201653001.19145@gannet.stats>
	<9e7563d19a8dfc3428bbb2576f323f6d@soc.soton.ac.uk>
Message-ID: <971536df050621052516900fb7@mail.gmail.com>

\
Perhaps the signature should be:

   sweep(...other args go here..., warn=getOption("warn"))

so that the name and value of the argument are consistent with
the R warn option.

On 6/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> 
> On Jun 20, 2005, at 04:58 pm, Prof Brian Ripley wrote:
> 
> > The issue here is that the equivalent command array(1:5, c(6,6)) (to
> > matrix(1:5,6,6)) gives no warning, and sweep uses array().
> >
> > I am not sure either should: fractional recycling was normally allowed
> > in S3 (S4 tightened up a bit).
> >
> > Perhaps someone who thinks sweep() should warn could contribute a
> > tested patch?
> >
> 
> 
> OK,  modified R code and Rd file below (is this the best way to do
> this?)
> 
> 
> 
> 
> "sweep" <-
>   function (x, MARGIN, STATS, FUN = "-", give.warning = FALSE, ...)
> {
>   FUN <- match.fun(FUN)
>   dims <- dim(x)
>   if(give.warning & length(STATS)>1 & any(dims[MARGIN] !=
> dim(as.array(STATS)))){
>     warning("array extents do not recycle exactly")
>   }
>   perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>   FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
> }
> 
> 
> 
> 
> 
> 
> 
> \name{sweep}
> \alias{sweep}
> \title{Sweep out Array Summaries}
> \description{
>   Return an array obtained from an input array by sweeping out a summary
>   statistic.
> }
> \usage{
> sweep(x, MARGIN, STATS, FUN="-", give.warning = FALSE, \dots)
> }
> \arguments{
>   \item{x}{an array.}
>   \item{MARGIN}{a vector of indices giving the extents of \code{x}
>     which correspond to \code{STATS}.}
>   \item{STATS}{the summary statistic which is to be swept out.}
>   \item{FUN}{the function to be used to carry out the sweep.  In the
>     case of binary operators such as \code{"/"} etc., the function name
>     must be quoted.}
>   \item{give.warning}{Boolean, with default \code{FALSE} meaning to
>   give no warning, even if array extents do not match.  If
>   \code{TRUE}, check for the correct dimensions and if a
>   mismatch is detected, give a suitable warning.}
>   \item{\dots}{optional arguments to \code{FUN}.}
> }
> \value{
>   An array with the same shape as \code{x}, but with the summary
>   statistics swept out.
> }
> \note{
>   If \code{STATS} is of length 1, recycling is carried out with no
>   warning irrespective of the value of \code{give.warning}.
> }
> 
> \references{
>   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
>   \emph{The New S Language}.
>   Wadsworth \& Brooks/Cole.
> }
> \seealso{
>   \code{\link{apply}} on which \code{sweep} used to be based;
>   \code{\link{scale}} for centering and scaling.
> }
> \examples{
> require(stats) # for median
> med.att <- apply(attitude, 2, median)
> sweep(data.matrix(attitude), 2, med.att)# subtract the column medians
> 
> a <- array(0, c(2, 3, 4))
> b <- matrix(1:8, c(2, 4))
> sweep(a, c(1, 3), b, "+", give.warning = TRUE)  # no warning:
> all(dim(a)[c(1,3)] == dim(b))
> sweep(a, c(1, 2), b, "+", give.warning = TRUE)  # warning given
> 
> }
> \keyword{array}
> \keyword{iteration}
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Tue Jun 21 14:29:34 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 21 Jun 2005 14:29:34 +0200
Subject: [R] test for equality of two data sets with
	multidimensional	variables
In-Reply-To: <ea57975b05062104002afca49@mail.gmail.com>
Message-ID: <42B8244E.28219.17464C3@localhost>

Hi

searching in CRAN homepage for hotelling gave me this response

Thanks everyone for your help on this question. I solved the 
problem by 
writing a procedure to calculate Hotelling's T^2 for a one-sample, 
multivariate t-test. Here's how it looks, perhaps it will be useful to 
others. 


data <- cbind(rnorm(50, 0.1, .01), rnorm(50,.1,.01), 
rnorm(50,.1,.01)) 
k <- ncol(data) 
n <- nrow(data) 
xbar <- apply(data, 2, mean) 
mubar <- rep(0,k) #hypothesized means are zero 
dbar <- xbar - mubar 
v <- var(data) 
t2 <- n*dbar%*%solve(v)%*%dbar 
F <- (n-k)*t2/((n-1)*k) 
P <- 1-pf(F,k,n-k) 


A previous post by Peter B. Mandeville was very helpful, as well as 
the 
Johnson/Wichern book on multivariate stats. 
-S. Schultz 

and this

cran.r-project.org/doc/packages/agce.pdf - Podobn?? str??nky 

CRAN - Package SharedHT2
SharedHT2: Shared Hotelling T2 test for small sample microarray 
experiments ...
Derives a Hotelling T2 statistic having an F-distribution using an 
empirical ...

Maybe this is what you want.

HTH
Petr





On 21 Jun 2005 at 13:00, wu sz wrote:

> Hello there,
> 
> I have two data sets with 14 variables each, and wish to do the test
> for equality of their covariance matrices and mean vectors. Normally
> these tests should be done by chi square test (box provided) and
> Hotelling's T square test respectively. Which R functions could do
> this kind of test? I just find some functions could do for one
> dimension, but no for multidimension. Some one suggests bartlett.test,
> but it seems just works for one dimension. Do you know which ones
> could do that, or I have to do R programming by myself?
> 
> Thank you,
> Shengzhe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Tue Jun 21 14:34:38 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 21 Jun 2005 14:34:38 +0200
Subject: [R] Data.frames with different line's length
In-Reply-To: <003801c57651$ad9ca540$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <42B8257E.5593.179096C@localhost>

Hi

from help page

Details:

     A data frame is a list of variables of the same length with unique
						^^^^^^^^^^
     row names, given class '"data.frame"'.

So you need to use a different sructure (e.g.list) or you have to 
ensure that you rbind vectors of same length.

HTH
Petr


On 21 Jun 2005 at 12:09, Jo??o Mendes Moreira wrote:

> Hello,
> 
> I want to create a data.frame with different number of columns per
> line. What I want is something like:
> 
> example <- NULL
> begin <- 1
> while (end < nrow(orig.data))
> {
> end <- next.day(orig.data,begin) # my own function. It returns the
> first index from the next day. Each day has a different number of
> records. example <- rbind(example, c(begin, end,
> predictions[c(begin:end)], ...)) begin <- end+1 }
> 
> It gives the following warning:
> 
> > number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(example,
>         c(begin, end, predictions[c(begin:end)], ...
> 
> I believe this happen once the number of elements by line are
> different. The example data.frame repeats the same values for shorter
> lines until the line is full, i.e., it tries to create lines with the
> same number of columns.
> 
> The question is: How can I create a data.frame with different number
> of columns?
> 
> Any help is very much appreciated
> 
> Thanks in advance
> 
> Joao Moreira
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From lzhtom at hotmail.com  Tue Jun 21 14:46:25 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Tue, 21 Jun 2005 12:46:25 +0000
Subject: [R] how to count "associated" factors?
Message-ID: <BAY12-F21552BCCE24BF9686EFDABC7E80@phx.gbl>

hi netters

Suppose I have a factor X, with 10 elements and 3 levels: A B B C A C B A C 
C .

It is easy to count the number of elements for each level: 
tapply(X,X,length).

Now I have another factor Y, which formed a matrix with X:

 X| A B B C A C B A C C
 Y| B B C C C A A A B B

I wanna count the number of elements for each of these conditions: when X=A 
and Y=A; when X=A and Y=B; when X=A and Y=C; when X=B and Y=A; when X=B and 
Y=B; when X=B and Y=C; when X=C and Y=A; when X=C and Y=B; when X=C and 
Y=C.

The code I have written for this task is too complicated, involving a lot 
of for loops and if conditions. I believe there's some nice code that can 
do it far more efficiently. Can anyone give me a hint?

Thanks a lot!



From andy_liaw at merck.com  Tue Jun 21 14:50:55 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Jun 2005 08:50:55 -0400
Subject: [R] how to count "associated" factors?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9CC@usctmx1106.merck.com>

table() is your friend:


> X <- factor(scan(what=""))
1: A B B C A C B A C C
11: 
Read 10 items
> Y <- factor(scan(what=""))
1: B B C C C A A A B B
11: 
Read 10 items
> table(X)
X
A B C 
3 3 4 
> table(X, Y)
   Y
X   A B C
  A 1 1 1
  B 1 1 1
  C 1 2 1

Andy

> From: zhihua li
> 
> hi netters
> 
> Suppose I have a factor X, with 10 elements and 3 levels: A B 
> B C A C B A C 
> C .
> 
> It is easy to count the number of elements for each level: 
> tapply(X,X,length).
> 
> Now I have another factor Y, which formed a matrix with X:
> 
>  X| A B B C A C B A C C
>  Y| B B C C C A A A B B
> 
> I wanna count the number of elements for each of these 
> conditions: when X=A 
> and Y=A; when X=A and Y=B; when X=A and Y=C; when X=B and 
> Y=A; when X=B and 
> Y=B; when X=B and Y=C; when X=C and Y=A; when X=C and Y=B; 
> when X=C and 
> Y=C.
> 
> The code I have written for this task is too complicated, 
> involving a lot 
> of for loops and if conditions. I believe there's some nice 
> code that can 
> do it far more efficiently. Can anyone give me a hint?
> 
> Thanks a lot!
> 
>



From r.shengzhe at gmail.com  Tue Jun 21 14:53:16 2005
From: r.shengzhe at gmail.com (wu sz)
Date: Tue, 21 Jun 2005 14:53:16 +0200
Subject: [R] test for equality of two data sets with multidimensional
	variables
In-Reply-To: <42B8244E.28219.17464C3@localhost>
References: <ea57975b05062104002afca49@mail.gmail.com>
	<42B8244E.28219.17464C3@localhost>
Message-ID: <ea57975b05062105531f86f9de@mail.gmail.com>

Hi,

I just find another package "shapes" for solving the Hotelling T^2
test, but what about the R function for the test of covariance
equality(chi ^ 2 box's test) between two multidimensional data sets?

Shengzhe



From dimitrios.rizopoulos at student.kuleuven.ac.be  Tue Jun 21 14:54:48 2005
From: dimitrios.rizopoulos at student.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 21 Jun 2005 14:54:48 +0200
Subject: [R] how to count "associated" factors?
References: <BAY12-F21552BCCE24BF9686EFDABC7E80@phx.gbl>
Message-ID: <004d01c57660$68294400$0540210a@www.domain>

try this:

X  <- factor(sample(c("A", "B", "C"), 10, TRUE), levels = c("A", "B", 
"C"))
Y  <- factor(sample(c("A", "B", "C"), 10, TRUE), levels = c("A", "B", 
"C"))
#############
table(X, Y)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "zhihua li" <lzhtom at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 21, 2005 2:46 PM
Subject: [R] how to count "associated" factors?


> hi netters
>
> Suppose I have a factor X, with 10 elements and 3 levels: A B B C A 
> C B A C
> C .
>
> It is easy to count the number of elements for each level:
> tapply(X,X,length).
>
> Now I have another factor Y, which formed a matrix with X:
>
> X| A B B C A C B A C C
> Y| B B C C C A A A B B
>
> I wanna count the number of elements for each of these conditions: 
> when X=A
> and Y=A; when X=A and Y=B; when X=A and Y=C; when X=B and Y=A; when 
> X=B and
> Y=B; when X=B and Y=C; when X=C and Y=A; when X=C and Y=B; when X=C 
> and
> Y=C.
>
> The code I have written for this task is too complicated, involving 
> a lot
> of for loops and if conditions. I believe there's some nice code 
> that can
> do it far more efficiently. Can anyone give me a hint?
>
> Thanks a lot!
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Jun 21 15:03:39 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jun 2005 15:03:39 +0200
Subject: [R] test for equality of two data sets with multidimensional
	variables
In-Reply-To: <ea57975b05062105531f86f9de@mail.gmail.com>
References: <ea57975b05062104002afca49@mail.gmail.com>
	<42B8244E.28219.17464C3@localhost>
	<ea57975b05062105531f86f9de@mail.gmail.com>
Message-ID: <x23brbx3o4.fsf@turmalin.kubism.ku.dk>

wu sz <r.shengzhe at gmail.com> writes:

> Hi,
> 
> I just find another package "shapes" for solving the Hotelling T^2
> test, but what about the R function for the test of covariance
> equality(chi ^ 2 box's test) between two multidimensional data sets?
> 
> Shengzhe

Rather than looking for a package, it might be more expedient to grab
TW Anderson's book and code the couple of lines it takes to generate
the test statistic and its asymptotic p-value.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dmbates at gmail.com  Tue Jun 21 15:18:57 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 21 Jun 2005 08:18:57 -0500
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
	plinear - PI curve fitting
In-Reply-To: <971536df05062104125781ab84@mail.gmail.com>
References: <25219.1119347859@www24.gmx.net>
	<971536df0506210357213a33c5@mail.gmail.com>
	<971536df05062104125781ab84@mail.gmail.com>
Message-ID: <40e66e0b050621061859dcb65f@mail.gmail.com>

On 6/21/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/21/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 6/21/05, Christfried Kunath <mailpuls at gmx.net> wrote:
> > > Hello,
> > >
> > > i have a problem with the function nls().
> > >
> > > This are my data in "k":
> > >        V1    V2
> > >  [1,]    0 0.367
> > >  [2,]   85 0.296
> > >  [3,]  122 0.260
> > >  [4,]  192 0.244
> > >  [5,]  275 0.175
> > >  [6,]  421 0.140
> > >  [7,]  603 0.093
> > >  [8,]  831 0.068
> > >  [9,] 1140 0.043
> > >
> > > With the nls()-function i want to fit following formula whereas a,b, and c
> > > are variables: y~1/(a*x^2+b*x+c)
> > >
> > > With the standardalgorithm "Newton-Gauss" the fitted curve contain an peak
> > > near the second x,y-point.
> > > This peak is not correct for my purpose. The fitted curve should descend
> > > from the maximum y to the minimum y given in my data.
> > >
> > > The algorithm "plinear" give me following error:
> > >
> > >
> > >   phi function(x,y) {
> > > k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
> > >       coef(k.nls)
> > >   }
> > >
> > >   phi(k[,1],k[,2])
> > >
> > >   Error in qr.solve(QR.B, cc) : singular matrix `a' in solve
> > >
> > >
> > > I have found in the mailinglist
> > > "https://stat.ethz.ch/pipermail/r-help/2001-July/012196.html" that is if t
> > > he data are artificial. But the data are from my measurment.
> > >
> > > The commercial software "Origin V.6.1" solved this problem with the
> > > Levenberg-Marquardt algorithm how i want.
> > > The reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982
> > >
> > > What are the right way or algorithm for me to solve this problem and what
> > > means this error with alg="plinear"?
> > >
> > > Thanks in advance.
> >
> > This is not a direct answer to your question but log(y) looks nearly linear
> > in x when plotting them together and log(y) ~ a + b*x or
> > y ~ a*exp(b*x) will always be monotonic.  Also, this model uses only 2
> > rather than 3 parameters.
> >
> 
> One other comment.  If you do want to use your model try fitting 1/y
> first to get your starting value since that model has a unique solution:
> 
> > res1 <- nls(1/y ~ a*x^2 + b*x + c, start = list(a=0,b=0,c=0))
> > res2 <- nls(y ~ 1/(a*x^2 + b*x + c), start = as.list(coef(res1)))
> > res2
> Nonlinear regression model
>   model:  y ~ 1/(a * x^2 + b * x + c)
>    data:  parent.frame()
>            a            b            c
> 9.690187e-06 6.885577e-03 2.729825e+00
>  residual sum-of-squares:  0.000547369

In cases where you are having difficulty getting nls to converge
(which is not the case the way that Gabor is fitting the model) it is
best to use trace = TRUE to see what is happening to the parameter
estimates.  You can use the alg = 'plinear' option for this model if
you divide the numerator and denominator of the rational function by
one of the parameters.  For example, if you divide by a then you have
a monic quadratic in the denominator and a parameter equivalent to 1/a
in the numerator.  I chose to divide by c but it could work with any
of the parameters

> res1 <- nls(1/y ~ a*x*x + b*x + c, dd, start=c(a=0, b=0, c=0), trace = TRUE)
1006.817 :  0 0 0 
0.597423 :  9.751120e-06 6.755636e-03 2.760013e+00 
> coef(res1)
           a            b            c 
9.751120e-06 6.755636e-03 2.760013e+00 
> coef(res1)[-3]/coef(res1)[3]
           a            b 
3.532998e-06 2.447683e-03 
> res2 <- nls(y ~ 1/(a*x*x+b*x+1), dd, start = coef(res1)[-3]/coef(res1)[3], alg = 'plinear', trace = TRUE)
0.0005553948 : 3.532998e-06 2.447683e-03 3.643117e-01 
0.0005473694 : 3.549814e-06 2.521742e-03 3.663088e-01 
0.000547369 : 3.549764e-06 2.522343e-03 3.663238e-01 
> res3 <- nls(y ~ 1/(a*x*x+b*x+c), dd, start = coef(res1), trace = TRUE)
0.0005678106 :  9.751120e-06 6.755636e-03 2.760013e+00 
0.0005473708 :  9.690171e-06 6.886612e-03 2.729552e+00 
0.000547369 :  9.690188e-06 6.885577e-03 2.729825e+00 

The solutions are equivalent.

> coef(res3)[-3]/coef(res3)[3]
           a            b 
3.549747e-06 2.522351e-03



From Heather.Turner at warwick.ac.uk  Tue Jun 21 15:33:31 2005
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Tue, 21 Jun 2005 14:33:31 +0100
Subject: [R] sweep() and recycling
Message-ID: <s2b82542.020@liberator.csv.warwick.ac.uk>

I think the warning condition in Robin's patch is too harsh - the following examples seem reasonable to me, but all produce warnings

sweep(array(1:24, dim = c(4,3,2)), 1, 1:2, give.warning = TRUE)
sweep(array(1:24, dim = c(4,3,2)), 1, 1:12, give.warning = TRUE)
sweep(array(1:24, dim = c(4,3,2)), 1, 1:24, give.warning = TRUE)

I have written an alternative (given below) which does not give warnings in the above cases, but does warn in the following case

> sweep(array(1:24, dim = c(4,3,2)), 1:2, 1:3)
, , 1

     [,1] [,2] [,3]
[1,]    0    3    6
[2,]    0    3    9
[3,]    0    6    9
[4,]    3    6    9

, , 2

     [,1] [,2] [,3]
[1,]   12   15   18
[2,]   12   15   21
[3,]   12   18   21
[4,]   15   18   21

Warning message:
STATS does not recycle exactly across MARGIN

The code could be easily modified to warn in other cases, e.g. when length of STATS is a divisor of the corresponding array extent (as in the first example above, with length(STATS) = 2).

The code also includes Gabor's suggestion.

Heather

sweep <- function (x, MARGIN, STATS, FUN = "-", warn = getOption("warn"), ...) 
{
    FUN <- match.fun(FUN)
    dims <- dim(x)
    perm <- c(MARGIN, (1:length(dims))[-MARGIN])
    if (warn >= 0) {
        s <- length(STATS)
        cumDim <- c(1, cumprod(dims[perm]))
        if (s > max(cumDim))
            warning("length of STATS greater than length of array",
                    call. = FALSE)
        else {
            upper <- min(ifelse(cumDim > s, cumDim, max(cumDim)))
            lower <- max(ifelse(cumDim < s, cumDim, min(cumDim)))
            if (any(upper %% s != 0, s %% lower != 0)) 
                warning("STATS does not recycle exactly across MARGIN",
                        call. = FALSE)
        }
    }
    FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
}

>>> Gabor Grothendieck <ggrothendieck at gmail.com> 06/21/05 01:25pm >>>
\
Perhaps the signature should be:

   sweep(...other args go here..., warn=getOption("warn"))

so that the name and value of the argument are consistent with
the R warn option.

On 6/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
> 
> On Jun 20, 2005, at 04:58 pm, Prof Brian Ripley wrote:
> 
> > The issue here is that the equivalent command array(1:5, c(6,6)) (to
> > matrix(1:5,6,6)) gives no warning, and sweep uses array().
> >
> > I am not sure either should: fractional recycling was normally allowed
> > in S3 (S4 tightened up a bit).
> >
> > Perhaps someone who thinks sweep() should warn could contribute a
> > tested patch?
> >
> 
> 
> OK,  modified R code and Rd file below (is this the best way to do
> this?)
> 
> 
> 
> 
> "sweep" <-
>   function (x, MARGIN, STATS, FUN = "-", give.warning = FALSE, ...)
> {
>   FUN <- match.fun(FUN)
>   dims <- dim(x)
>   if(give.warning & length(STATS)>1 & any(dims[MARGIN] !=
> dim(as.array(STATS)))){
>     warning("array extents do not recycle exactly")
>   }
>   perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>   FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
> }
> 
> 
> 
> 
> 
> 
> 
> \name{sweep}
> \alias{sweep}
> \title{Sweep out Array Summaries}
> \description{
>   Return an array obtained from an input array by sweeping out a summary
>   statistic.
> }
> \usage{
> sweep(x, MARGIN, STATS, FUN="-", give.warning = FALSE, \dots)
> }
> \arguments{
>   \item{x}{an array.}
>   \item{MARGIN}{a vector of indices giving the extents of \code{x}
>     which correspond to \code{STATS}.}
>   \item{STATS}{the summary statistic which is to be swept out.}
>   \item{FUN}{the function to be used to carry out the sweep.  In the
>     case of binary operators such as \code{"/"} etc., the function name
>     must be quoted.}
>   \item{give.warning}{Boolean, with default \code{FALSE} meaning to
>   give no warning, even if array extents do not match.  If
>   \code{TRUE}, check for the correct dimensions and if a
>   mismatch is detected, give a suitable warning.}
>   \item{\dots}{optional arguments to \code{FUN}.}
> }
> \value{
>   An array with the same shape as \code{x}, but with the summary
>   statistics swept out.
> }
> \note{
>   If \code{STATS} is of length 1, recycling is carried out with no
>   warning irrespective of the value of \code{give.warning}.
> }
> 
> \references{
>   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
>   \emph{The New S Language}.
>   Wadsworth \& Brooks/Cole.
> }
> \seealso{
>   \code{\link{apply}} on which \code{sweep} used to be based;
>   \code{\link{scale}} for centering and scaling.
> }
> \examples{
> require(stats) # for median
> med.att <- apply(attitude, 2, median)
> sweep(data.matrix(attitude), 2, med.att)# subtract the column medians
> 
> a <- array(0, c(2, 3, 4))
> b <- matrix(1:8, c(2, 4))
> sweep(a, c(1, 3), b, "+", give.warning = TRUE)  # no warning:
> all(dim(a)[c(1,3)] == dim(b))
> sweep(a, c(1, 2), b, "+", give.warning = TRUE)  # warning given
> 
> }
> \keyword{array}
> \keyword{iteration}
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html 
>



From MBock at arcadis-us.com  Tue Jun 21 15:39:43 2005
From: MBock at arcadis-us.com (Bock, Michael)
Date: Tue, 21 Jun 2005 07:39:43 -0600
Subject: [R] Constructing formula using nls
Message-ID: <0016F5677B1F1D4281EEBC034993595103342F01@CORPEXBE1.arcadis-us.com>

I need to do some simulations based on a nls model. I have been able to
use nls to determine the model and I have almost gotten to the point at
which I can construct the function. When I get the function done I will
use uniroot to solve the function at a specific "y" value for x. I found
an example in the archive using the search tools but I can seem to load
Args with the fitted parameters. The function fdose was constructed
manually based on the fitted parameters, ndose is the "dynamic" version
of the function that will change when I fit the model using different
data sets. Thanks in advance for any help you can offer. 
Here is the code to generate the model function:

library(boot)
library(lattice)
setwd("C:/Documents and Settings/mbock/My Documents/Mink")
#Mink <- read.csv("MinkStatsa.csv")
#Mink <- subset(Mink, QTEQW != "NR")
Mink <-
structure(list(Authors = structure(as.integer(c(1, 1, 1, 1, 1,
1, 1, 8, 3, 3, 3, 3, 6, 6, 6, 6, 6, 2, 12, 10, 10, 4, 4, 9, 7,
7, 7, 11, 11, 11, 11, 11, 11, 11, 11, 11, 4, 5, 5, 5, 5, 5)), .Label =
c("Aulerich and Ringer 1977",
"Aulerich et al. 1985", "Bleavins et al. 1980", "Brunstrom et al. 2001",
"Bursian et al. 2003", "Den Boer 1984", "Heaton 1992", "Jensen et al.
1977",
"Kakela et al. 2002", "Kihlstrom et al. 1992", "Restum et al. 1999",
"Wren et al. 1987"), class = "factor"), Form = structure(as.integer(c(3,
4, 4, 4, 1, 2, 4, 10, 2, 2, 2, 2, 11, 11, 11, 8, 8, 4, 4, 9,
4, 9, 9, 2, 6, 7, 5, 6, 7, 5, 6, 7, 5, 6, 7, 5, 17, 14, 13, 16,
15, 12)), .Label = c("Aroclor 1221", "Aroclor 1242", "Aroclor
1242:1248:1254",
"Aroclor 1254", "Carp (high)", "Carp (low)", "Carp (medium)",
"Clophen A30", "Clophen A50", "Clophen A50/A60", "Clophen A60",
"Goldfish/carp (high)", "Goldfish/carp (low-med)", "Goldfish/carp
(low)",
"Goldfish/carp (med-high)", "Goldfish/carp (medium)", "Seal blubber
extract"
), class = "factor"), ADD = c(3913.0435, 723.9382, 1491.0537,
1956.5217, 260.8696, 260.8696, 260.8696, 3300, 937.5, 1875, 3750,
7500, 25.2, 6076, 2025, 6076, 2025, 307.4866, 180, 2094.2408,
1307.815, 81.2348, 267.0623, 826, 130, 260, 320, 58.8235, 117.6471,
235.2941, 58.8235, 117.6471, 235.2941, 58.7544, 120.048, 252.5253,
53.1856, 36, 63, 103, 169, 414), ADDTEQ = c(55.582, 18.2961,
37.6834, 49.4472, 0.0265, 1.1447, 6.593, 913.8127, 4.1139, 8.2277,
16.4555, 32.9109, 11.8593, 2859.4033, 952.9776, 19.8479, 6.6149,
7.7711, 4.5491, 62.0308, 33.0639, 2.3558, 7.4777, 45.4545, 4.6523,
7.8409, 12.118, 2.0393, 4.0785, 8.1571, 2.0393, 4.0785, 8.1571,
2.0369, 4.1618, 8.7544, 0.0914, 0.367, 0.588, 0.978, 1.686, 7.671
), WPCB = structure(as.integer(c(16, 22, 9, 10, 1, 2, 13, 20,
6, 11, 19, 23, 3, 24, 18, 21, 12, 17, 7, 15, 8, 4, 14, 5, 25,
25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,
25)), .Label = c("0.2678", "0.2998", "0.6144", "0.8402", "0.9493",
"1.0775", "1.854", "12.7837", "15.4173", "19.8375", "2.1549",
"2.3273", "2.6976", "2.7621", "20.2492", "25.8584", "3.0412",
"37.8885", "4.3098", "52.5134", "6.983", "7.4854", "8.6196",
"93.9869", "NR"), class = "factor"), QTEQW = c(0.5168, 0.1787,
0.3682, 0.479, 2e-04, 0.008, 0.0644, 6.8409, 0.0287, 0.0575,
0.1149, 0.2295, 0.0858, 19.8437, 6.76, 0.1473, 0.0491, 0.0744,
0.0444, 0.6962, 0.3149, 0.0251, 0.0805, 0.123, 0.053, 0.0806,
0.1311, 0.0183, 0.0366, 0.0732, 0.0183, 0.0367, 0.0734, 0.0183,
0.0374, 0.0787, 0.0016, 0.0047, 0.0072, 0.0119, 0.0205, 0.0907
), WTEQa = c(0.2251, 0.0873, 0.1797, 0.235, 1e-04, 0.0033, 0.0314,
0.9427, 0.0118, 0.0235, 0.0471, 0.0941, 0.0074, 1.4622, 0.524,
0.1515, 0.0505, 0.0368, 0.0217, 0.2226, 0.1561, 0.0109, 0.0331,
0.0334, 0.0262, 0.0352, 0.0552, 0.0079, 0.0158, 0.0316, 0.0079,
0.0159, 0.0317, 0.0079, 0.0162, 0.034, 4e-04, 0.0015, 0.0021,
0.003, 0.0048, 0.0204), Surv = c(0, 0, 0, 0, 235, 203, 0, 0,
0, 0, 0, 0, 99, 0, 0, 0, 0, 0, 111, 0, 0, 56.78, 0, 73, 25, 12,
0, 147, 99, 37, 92, 4, 7, 71, 4, 0, 104, 92, 70, 114, 147, 52
), Surva = c(0, 0, 0, 0, 100, 100, 0, 0, 0, 0, 0, 0, 99, 0, 0,
0, 0, 0, 100, 0, 0, 56.77647059, 0, 73, 25, 12, 0, 100, 99, 37,
92, 4, 7, 71, 4, 0, 100, 92, 70, 100, 100, 52), Effect = c(100,
100, 100, 100, 0, 0, 100, 100, 100, 100, 100, 100, 1, 100, 100,
100, 100, 100, 0, 100, 100, 43.22352941, 100, 27, 75, 88, 100,
0, 1, 63, 8, 96, 93, 29, 96, 100, 0, 8, 30, 0, 0, 48)), .Names =
c("Authors",
"Form", "ADD", "ADDTEQ", "WPCB", "QTEQW", "WTEQa", "Surv", "Surva",
"Effect"), row.names = c("1", "2", "3", "4", "5", "6", "7", "8",
"9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19",
"20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30",
"31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41",
"42"), class = "data.frame")
TS <- (nls( Effect/100 ~ (Po +((1-Po)/(1+exp(-(a+b*log(QTEQW)))))),
      data = Mink,,start = list(a = 5,b = 2,Po = 0),trace = FALSE ) )
fdose = function (x)
                   {(-0.01899614
+((1-(-0.01899614))/(1+exp(-(5.88112869+1.71111*log(x))))))}
plot(Effect/100 ~ log(QTEQW), Mink)
tt <- seq(-8,2,length = 100)
lines(tt,predict(TS,list (QTEQW = exp(tt))))
#code to creat dynamic function
alist(all.vars(summary(TS)$formula[[3]]))
xx <- all.vars(summary(TS)$formula[[3]])
xxx <- vector("list", length(xx))
names(xxx) <- xx
Args <- do.call("alist", xxx)
ndose <- as.function(c(Args, summary(TS)$formula[[3]]))



From r.hankin at noc.soton.ac.uk  Tue Jun 21 15:47:35 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Tue, 21 Jun 2005 14:47:35 +0100
Subject: [R] sweep() and recycling
In-Reply-To: <s2b82542.019@liberator.csv.warwick.ac.uk>
References: <s2b82542.019@liberator.csv.warwick.ac.uk>
Message-ID: <5d5ece8b66ea9de7712bef57a4f014d7@soc.soton.ac.uk>

Hi

On Jun 21, 2005, at 02:33 pm, Heather Turner wrote:

> I think the warning condition in Robin's patch is too harsh - the 
> following examples seem reasonable to me, but all produce warnings
>
> sweep(array(1:24, dim = c(4,3,2)), 1, 1:2, give.warning = TRUE)
> sweep(array(1:24, dim = c(4,3,2)), 1, 1:12, give.warning = TRUE)
> sweep(array(1:24, dim = c(4,3,2)), 1, 1:24, give.warning = TRUE)
>


The examples above do give warnings (as intended) but I think all three 
cases above
are inimical to the spirit of sweep(): nothing is being "swept" out.

So a warning is appropriate, IMO.

In any case, one can always suppress (or ignore!) a warning if one knows
what one is doing.  YMMV, but if I wanted to do the above operations I 
would
replace


sweep(array(0, dim = c(4,3,2)), c(1,3), 1:12, "+" , give.warning = 
FALSE)

with

  aperm(array(1:12,c(4,2,3)),c(1,3,2))


best wishes

rksh







> I have written an alternative (given below) which does not give 
> warnings in the above cases, but does warn in the following case
>
>> sweep(array(1:24, dim = c(4,3,2)), 1:2, 1:3)
> , , 1
>
>      [,1] [,2] [,3]
> [1,]    0    3    6
> [2,]    0    3    9
> [3,]    0    6    9
> [4,]    3    6    9
>
> , , 2
>
>      [,1] [,2] [,3]
> [1,]   12   15   18
> [2,]   12   15   21
> [3,]   12   18   21
> [4,]   15   18   21
>
> Warning message:
> STATS does not recycle exactly across MARGIN
>
> The code could be easily modified to warn in other cases, e.g. when 
> length of STATS is a divisor of the corresponding array extent (as in 
> the first example above, with length(STATS) = 2).
>
> The code also includes Gabor's suggestion.
>
> Heather
>
> sweep <- function (x, MARGIN, STATS, FUN = "-", warn = 
> getOption("warn"), ...)
> {
>     FUN <- match.fun(FUN)
>     dims <- dim(x)
>     perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>     if (warn >= 0) {
>         s <- length(STATS)
>         cumDim <- c(1, cumprod(dims[perm]))
>         if (s > max(cumDim))
>             warning("length of STATS greater than length of array",
>                     call. = FALSE)
>         else {
>             upper <- min(ifelse(cumDim > s, cumDim, max(cumDim)))
>             lower <- max(ifelse(cumDim < s, cumDim, min(cumDim)))
>             if (any(upper %% s != 0, s %% lower != 0))
>                 warning("STATS does not recycle exactly across MARGIN",
>                         call. = FALSE)
>         }
>     }
>     FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
> }
>
>>>> Gabor Grothendieck <ggrothendieck at gmail.com> 06/21/05 01:25pm >>>
> \
> Perhaps the signature should be:
>
>    sweep(...other args go here..., warn=getOption("warn"))
>
> so that the name and value of the argument are consistent with
> the R warn option.
>
> On 6/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
>>
>> On Jun 20, 2005, at 04:58 pm, Prof Brian Ripley wrote:
>>
>>> The issue here is that the equivalent command array(1:5, c(6,6)) (to
>>> matrix(1:5,6,6)) gives no warning, and sweep uses array().
>>>
>>> I am not sure either should: fractional recycling was normally 
>>> allowed
>>> in S3 (S4 tightened up a bit).
>>>
>>> Perhaps someone who thinks sweep() should warn could contribute a
>>> tested patch?
>>>
>>
>>
>> OK,  modified R code and Rd file below (is this the best way to do
>> this?)
>>
>>
>>
>>
>> "sweep" <-
>>   function (x, MARGIN, STATS, FUN = "-", give.warning = FALSE, ...)
>> {
>>   FUN <- match.fun(FUN)
>>   dims <- dim(x)
>>   if(give.warning & length(STATS)>1 & any(dims[MARGIN] !=
>> dim(as.array(STATS)))){
>>     warning("array extents do not recycle exactly")
>>   }
>>   perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>>   FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
>> }
>>
>>
>>
>>
>>
>>
>>
>> \name{sweep}
>> \alias{sweep}
>> \title{Sweep out Array Summaries}
>> \description{
>>   Return an array obtained from an input array by sweeping out a 
>> summary
>>   statistic.
>> }
>> \usage{
>> sweep(x, MARGIN, STATS, FUN="-", give.warning = FALSE, \dots)
>> }
>> \arguments{
>>   \item{x}{an array.}
>>   \item{MARGIN}{a vector of indices giving the extents of \code{x}
>>     which correspond to \code{STATS}.}
>>   \item{STATS}{the summary statistic which is to be swept out.}
>>   \item{FUN}{the function to be used to carry out the sweep.  In the
>>     case of binary operators such as \code{"/"} etc., the function 
>> name
>>     must be quoted.}
>>   \item{give.warning}{Boolean, with default \code{FALSE} meaning to
>>   give no warning, even if array extents do not match.  If
>>   \code{TRUE}, check for the correct dimensions and if a
>>   mismatch is detected, give a suitable warning.}
>>   \item{\dots}{optional arguments to \code{FUN}.}
>> }
>> \value{
>>   An array with the same shape as \code{x}, but with the summary
>>   statistics swept out.
>> }
>> \note{
>>   If \code{STATS} is of length 1, recycling is carried out with no
>>   warning irrespective of the value of \code{give.warning}.
>> }
>>
>> \references{
>>   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
>>   \emph{The New S Language}.
>>   Wadsworth \& Brooks/Cole.
>> }
>> \seealso{
>>   \code{\link{apply}} on which \code{sweep} used to be based;
>>   \code{\link{scale}} for centering and scaling.
>> }
>> \examples{
>> require(stats) # for median
>> med.att <- apply(attitude, 2, median)
>> sweep(data.matrix(attitude), 2, med.att)# subtract the column medians
>>
>> a <- array(0, c(2, 3, 4))
>> b <- matrix(1:8, c(2, 4))
>> sweep(a, c(1, 3), b, "+", give.warning = TRUE)  # no warning:
>> all(dim(a)[c(1,3)] == dim(b))
>> sweep(a, c(1, 2), b, "+", give.warning = TRUE)  # warning given
>>
>> }
>> \keyword{array}
>> \keyword{iteration}
>>
>>
>>
>>
>> --
>> Robin Hankin
>> Uncertainty Analyst
>> National Oceanography Centre, Southampton
>> European Way, Southampton SO14 3ZH, UK
>>  tel  023-8059-7743
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From pcampbell at econ.bbk.ac.uk  Tue Jun 21 16:13:26 2005
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Tue, 21 Jun 2005 15:13:26 +0100
Subject: [R] Syntax error with .First.lib
Message-ID: <NGECIFANPOJAGABBAEAPEEMHELAA.pcampbell@econ.bbk.ac.uk>

I am trying to build a package which calls some c code.  I want to call the
dyn.load function when the package is loaded.  To do this I create a
.First.lib function in MyConv.R file:


.First.lib<-function("/home/phineas/R_HOME/R-2.0.1/library/", "MyConvolve"){
	### Intentionally empty body.  To be replaced by dyn.load function
}

R CMD check MyConvolve gives an R syntax error.

libname is the directory where the package is saved.  The package is being
stored in the standard library directory, is possible to pass NULL as this
value?

pkgname is the string representing the name of the package.


To clarify some notation; a package is a set of R data, code and objects to
be used when the package is installed, but a library is a compiled c or
Fortran binary which can be called by R


Should this question be posted to R-devel?

>version

platform sparc-sun-solaris2.9
arch     sparc
os       solaris2.9
system   sparc, solaris2.9
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

Phineas Campbell
pcampbell at econ.bbk.ac.uk



From jfox at mcmaster.ca  Tue Jun 21 16:22:57 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 21 Jun 2005 10:22:57 -0400
Subject: [R] test for equality of two data sets
	withmultidimensional	variables
In-Reply-To: <42B8244E.28219.17464C3@localhost>
Message-ID: <20050621142258.JFIH16985.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Petr and Shengzhe,

Also see ?manova.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Petr Pikal
> Sent: Tuesday, June 21, 2005 7:30 AM
> To: wu sz; r-help at stat.math.ethz.ch
> Subject: Re: [R] test for equality of two data sets 
> withmultidimensional variables
> 
> Hi
> 
> searching in CRAN homepage for hotelling gave me this response
> 
> Thanks everyone for your help on this question. I solved the 
> problem by writing a procedure to calculate Hotelling's T^2 
> for a one-sample, multivariate t-test. Here's how it looks, 
> perhaps it will be useful to others. 
> 
> 
> data <- cbind(rnorm(50, 0.1, .01), rnorm(50,.1,.01),
> rnorm(50,.1,.01))
> k <- ncol(data)
> n <- nrow(data)
> xbar <- apply(data, 2, mean)
> mubar <- rep(0,k) #hypothesized means are zero dbar <- xbar - 
> mubar v <- var(data)
> t2 <- n*dbar%*%solve(v)%*%dbar
> F <- (n-k)*t2/((n-1)*k)
> P <- 1-pf(F,k,n-k) 
> 
> 
> A previous post by Peter B. Mandeville was very helpful, as 
> well as the Johnson/Wichern book on multivariate stats. 
> -S. Schultz 
> 
> and this
> 
> cran.r-project.org/doc/packages/agce.pdf - Podobn?? str??nky 
> 
> CRAN - Package SharedHT2
> SharedHT2: Shared Hotelling T2 test for small sample 
> microarray experiments ...
> Derives a Hotelling T2 statistic having an F-distribution 
> using an empirical ...
> 
> Maybe this is what you want.
> 
> HTH
> Petr
> 
> 
> 
> 
> 
> On 21 Jun 2005 at 13:00, wu sz wrote:
> 
> > Hello there,
> > 
> > I have two data sets with 14 variables each, and wish to do 
> the test 
> > for equality of their covariance matrices and mean vectors. 
> Normally 
> > these tests should be done by chi square test (box provided) and 
> > Hotelling's T square test respectively. Which R functions could do 
> > this kind of test? I just find some functions could do for one 
> > dimension, but no for multidimension. Some one suggests 
> bartlett.test, 
> > but it seems just works for one dimension. Do you know which ones 
> > could do that, or I have to do R programming by myself?
> > 
> > Thank you,
> > Shengzhe
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Manuel.A.Morales at williams.edu  Tue Jun 21 16:22:56 2005
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 21 Jun 2005 10:22:56 -0400
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
 plinear - PI	curve fitting
In-Reply-To: <971536df0506210357213a33c5@mail.gmail.com>
References: <25219.1119347859@www24.gmx.net>
	<971536df0506210357213a33c5@mail.gmail.com>
Message-ID: <1119363776.26624.6.camel@localhost.localdomain>

On Tue, 2005-06-21 at 06:57 -0400, Gabor Grothendieck wrote:
> On 6/21/05, Christfried Kunath <mailpuls at gmx.net> wrote:
> > Hello,
> > 
> > i have a problem with the function nls().
> > 
> > This are my data in "k":
> >        V1    V2
> >  [1,]    0 0.367
> >  [2,]   85 0.296
> >  [3,]  122 0.260
> >  [4,]  192 0.244
> >  [5,]  275 0.175
> >  [6,]  421 0.140
> >  [7,]  603 0.093
> >  [8,]  831 0.068
> >  [9,] 1140 0.043
> > 
> > With the nls()-function i want to fit following formula whereas a,b, and c
> > are variables: y~1/(a*x^2+b*x+c)
> > 
> > With the standardalgorithm "Newton-Gauss" the fitted curve contain an peak
> > near the second x,y-point.
> > This peak is not correct for my purpose. The fitted curve should descend
> > from the maximum y to the minimum y given in my data.
> > 
> > The algorithm "plinear" give me following error:
> > 
> > 
> >   phi function(x,y) {
> > k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
> >       coef(k.nls)
> >   }
> > 
> >   phi(k[,1],k[,2])
> > 
> >   Error in qr.solve(QR.B, cc) : singular matrix `a' in solve
> > 
> > 
> > I have found in the mailinglist
> > "https://stat.ethz.ch/pipermail/r-help/2001-July/012196.html" that is if t
> > he data are artificial. But the data are from my measurment.
> > 
> > The commercial software "Origin V.6.1" solved this problem with the
> > Levenberg-Marquardt algorithm how i want.
> > The reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982
> > 
> > What are the right way or algorithm for me to solve this problem and what
> > means this error with alg="plinear"?
> > 
> > Thanks in advance.
> 
> This is not a direct answer to your question but log(y) looks nearly linear
> in x when plotting them together and log(y) ~ a + b*x or
> y ~ a*exp(b*x) will always be monotonic.  Also, this model uses only 2
> rather than 3 parameters.

If you want to use the original model, you could also use optim() and
specify your own minimization function. The default algorithm is slow
but has worked well for me where the faster algorithms in nls() have
choked. I adapted one of my functions for your data below:

minimize.fn<-function(parms) {

  fit.model<-function(parms) {
    y.pred={}; for (i in 1:9) {
      a=parms[1];b=parms[2];c=parms[3];
      y=1/(a*V1[i]^2+b*V1[i]+c)
      y.pred=append(y.pred,y)}
    list(fitted.values=y.pred)}

  minimize.model<-function(parms) {
    Resids=V2-fit.model(parms)$fitted.values
    ss=sum(Resids^2)
    list(c(ss))}

  fit=optim(c(parms[1],parms[2],parms[3]),minimize.model,
    control=list(trace=1,maxit=10000))
  results=fit.model(c(fit$par[1],fit$par[2],fit$par[3]))
 
  list(parms=fit$par,SS=fit$value,residuals=results$fitted.values-V2,
    fitted.values=results$fitted.values)}

result<-minimize.fn(c(0.0005,.02,1.5))
result<-minimize.fn(result$parms)

This gives the results:

> > result$parms
> [1] 1.184172e-05 4.878992e-03 3.045663e+00
> > result$SS
> [1] 0.005436355

Which is very similar to what you want.

HTH,

Manuel



From ligges at statistik.uni-dortmund.de  Tue Jun 21 16:29:41 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 21 Jun 2005 16:29:41 +0200
Subject: [R] Syntax error with .First.lib
In-Reply-To: <NGECIFANPOJAGABBAEAPEEMHELAA.pcampbell@econ.bbk.ac.uk>
References: <NGECIFANPOJAGABBAEAPEEMHELAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <42B82455.3060200@statistik.uni-dortmund.de>

Phineas Campbell wrote:

> I am trying to build a package which calls some c code.  I want to call the
> dyn.load function when the package is loaded.  To do this I create a
> .First.lib function in MyConv.R file:
> 
> 
> .First.lib<-function("/home/phineas/R_HOME/R-2.0.1/library/", "MyConvolve"){

Your arguments do not have names, and they have to in function 
declarations ....

Uwe Ligges



> 	### Intentionally empty body.  To be replaced by dyn.load function
> }
> 
> R CMD check MyConvolve gives an R syntax error.
> 
> libname is the directory where the package is saved.  The package is being
> stored in the standard library directory, is possible to pass NULL as this
> value?
> 
> pkgname is the string representing the name of the package.
> 
> 
> To clarify some notation; a package is a set of R data, code and objects to
> be used when the package is installed, but a library is a compiled c or
> Fortran binary which can be called by R
> 
> 
> Should this question be posted to R-devel?
> 
> 
>>version
> 
> 
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
> 
> Phineas Campbell
> pcampbell at econ.bbk.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From macq at llnl.gov  Tue Jun 21 16:35:41 2005
From: macq at llnl.gov (Don MacQueen)
Date: Tue, 21 Jun 2005 07:35:41 -0700
Subject: [R] X11, interactive device
In-Reply-To: <2039689176@web.de>
References: <2039689176@web.de>
Message-ID: <p06210200beddd488ca0b@[128.115.153.6]>

As far as I know, your options are (1) run Xvfb on the unix box, or 
(2) use the bitmap() device instead of jpeg(). Option 2 will require 
that ghostscript also be installed on the unix box.

Strictly speaking, it doesn't have to to be Xvfb. The requirement is 
that an X server be running.

The R on that unix box probably does have an installed interactive 
device, but it is not available unless an X server is running, hence 
your FALSE from dev.interactive().

-Don

At 12:49 PM +0200 6/21/05, powdersoul at web.de wrote:
>Hey,
>
>i'm trying to set up an cgi web-application that produces maps and 
>other graphics with R. for local testing i used my laptop which has 
>an win xp OS. there everything works fine. now i try to move the 
>whole system over on an unix machine, mainly because i want to 
>establish a connection to a database. anyway, i'm using R 2.0.0 on 
>an other computer in a local network. here comes my problem:
>
>i can't create jpeg-files, neither by starting R via my cgi-script 
>nor by pasting the syntax directly into R. both ways i get the 
>following error message:
>
>error in X11; unable to start device JPEG; could not open JPEG file.
>
>i found out that there is no interactive graphic device for the 
>installed R-Version or it is not set correctly. as i tried 
>dev.interactive(), the response was [1] FALSE.
>
>i'm a bit lost at the moment. i wonder if there is a chance to 
>change or modify the interactive graphic device or if i really need 
>to run Xvfb "device" as someone suggested before on the R-help 
>mailing list. does someone has experience with this kind of trouble, 
>any help available from somewhere?
>
>thanks in advance, lars
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From dmbates at gmail.com  Tue Jun 21 17:02:44 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 21 Jun 2005 10:02:44 -0500
Subject: [R] nls(): Levenberg-Marquardt, Gauss-Newton,
	plinear - PI curve fitting
In-Reply-To: <1119363776.26624.6.camel@localhost.localdomain>
References: <25219.1119347859@www24.gmx.net>
	<971536df0506210357213a33c5@mail.gmail.com>
	<1119363776.26624.6.camel@localhost.localdomain>
Message-ID: <40e66e0b050621080220499751@mail.gmail.com>

On 6/21/05, Manuel Morales <Manuel.A.Morales at williams.edu> wrote:
> On Tue, 2005-06-21 at 06:57 -0400, Gabor Grothendieck wrote:
> > On 6/21/05, Christfried Kunath <mailpuls at gmx.net> wrote:
> > > Hello,
> > >
> > > i have a problem with the function nls().
> > >
> > > This are my data in "k":
> > >        V1    V2
> > >  [1,]    0 0.367
> > >  [2,]   85 0.296
> > >  [3,]  122 0.260
> > >  [4,]  192 0.244
> > >  [5,]  275 0.175
> > >  [6,]  421 0.140
> > >  [7,]  603 0.093
> > >  [8,]  831 0.068
> > >  [9,] 1140 0.043
> > >
> > > With the nls()-function i want to fit following formula whereas a,b, and c
> > > are variables: y~1/(a*x^2+b*x+c)
> > >
> > > With the standardalgorithm "Newton-Gauss" the fitted curve contain an peak
> > > near the second x,y-point.
> > > This peak is not correct for my purpose. The fitted curve should descend
> > > from the maximum y to the minimum y given in my data.
> > >
> > > The algorithm "plinear" give me following error:
> > >
> > >
> > >   phi function(x,y) {
> > > k.nls<-nls(y~1/(a*(x^2)+b*x+c),start=c(a=0.0005,b=0.02,c=1.5),alg="plinear")
> > >       coef(k.nls)
> > >   }
> > >
> > >   phi(k[,1],k[,2])
> > >
> > >   Error in qr.solve(QR.B, cc) : singular matrix `a' in solve
> > >
> > >
> > > I have found in the mailinglist
> > > "https://stat.ethz.ch/pipermail/r-help/2001-July/012196.html" that is if t
> > > he data are artificial. But the data are from my measurment.
> > >
> > > The commercial software "Origin V.6.1" solved this problem with the
> > > Levenberg-Marquardt algorithm how i want.
> > > The reference results are: a = 9.6899E-6, b = 0.00689, c = 2.72982
> > >
> > > What are the right way or algorithm for me to solve this problem and what
> > > means this error with alg="plinear"?
> > >
> > > Thanks in advance.
> >
> > This is not a direct answer to your question but log(y) looks nearly linear
> > in x when plotting them together and log(y) ~ a + b*x or
> > y ~ a*exp(b*x) will always be monotonic.  Also, this model uses only 2
> > rather than 3 parameters.
> 
> If you want to use the original model, you could also use optim() and
> specify your own minimization function. The default algorithm is slow
> but has worked well for me where the faster algorithms in nls() have
> choked. I adapted one of my functions for your data below:
> 
> minimize.fn<-function(parms) {
> 
>   fit.model<-function(parms) {
>     y.pred={}; for (i in 1:9) {
>       a=parms[1];b=parms[2];c=parms[3];
>       y=1/(a*V1[i]^2+b*V1[i]+c)
>       y.pred=append(y.pred,y)}
>     list(fitted.values=y.pred)}
> 
>   minimize.model<-function(parms) {
>     Resids=V2-fit.model(parms)$fitted.values
>     ss=sum(Resids^2)
>     list(c(ss))}
> 
>   fit=optim(c(parms[1],parms[2],parms[3]),minimize.model,
>     control=list(trace=1,maxit=10000))
>   results=fit.model(c(fit$par[1],fit$par[2],fit$par[3]))
> 
>   list(parms=fit$par,SS=fit$value,residuals=results$fitted.values-V2,
>     fitted.values=results$fitted.values)}
> 
> result<-minimize.fn(c(0.0005,.02,1.5))
> result<-minimize.fn(result$parms)
> 
> This gives the results:
> 
> > > result$parms
> > [1] 1.184172e-05 4.878992e-03 3.045663e+00
> > > result$SS
> > [1] 0.005436355
> 
> Which is very similar to what you want.

One reason that will be slow is because you are doing an unnecessary
loop in your fit.model function.  You can write the sum of squares
function as

function(pars) sum((y - 1/(pars[1]*x*x + pars[2]*x + pars[3]))^2)

Remember that arithmetic operations in the S language are vectorized.



From helprhelp at gmail.com  Tue Jun 21 17:02:50 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 21 Jun 2005 10:02:50 -0500
Subject: [R] tapply
In-Reply-To: <971536df05062019264b9786e5@mail.gmail.com>
References: <cdf8178305062016153aa7e988@mail.gmail.com>
	<971536df05062019264b9786e5@mail.gmail.com>
Message-ID: <cdf81783050621080245ccff7f@mail.gmail.com>

hi
i tried all the methods suggested above:
ave and rowsum with "with" function works for my situation. I think
the problem might not be due to tapply.
My data z comes from
z<-y[y[[1]] %in% x[[2]], c(1,9)]

while z is supposed to have no entries for those non-matched between x and y.

however, when I run tapply, and the result also includes those
non-matched entries. I use is.na function to remove those entry from z
first and then use tapply again, but the result is the same: those
NA's and those non-matched results are still there. That's what I mean
by "it doesn't work".

Is there something I missed here so that z "implicitly" has some
"trace" back to y dataset?

thanks,

On 6/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > hi,
> > i have another question on tapply:
> > i have a dataset z like this:
> > 5540 389100307391      2600
> > 5541 389100307391      2600
> > 5542 389100307391      2600
> > 5543 389100307391      2600
> > 5544 389100307391      2600
> > 5546 381300302513        NA
> > 5547 387000307470        NA
> > 5548 387000307470        NA
> > 5549 387000307470        NA
> > 5550 387000307470        NA
> > 5551 387000307470        NA
> > 5552 387000307470        NA
> >
> > I want to sum the column 3 by column 2.
> > I removed NA by calling:
> > tapply(z[[3]], z[[2]], sum, na.rm=T)
> > but it does not work.
> >
> > then, i used
> > z1<-z[!is.na(z[[3]],]
> > and repeat
> > still doesn't work.
> >
> > please help.
> >
> 
> Depending on what you want you may be able to use rowsum:
> 
> - display only groups that have at least one non-NA with the sum
>   being the sum of the non-NAs:
> 
>         with(na.omit(z), rowsum(V3, V2))
> 
> - display all groups with the sum being NA if any member is NA:
> 
>         rowsum(z$V3, z$V2)
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From Heather.Turner at warwick.ac.uk  Tue Jun 21 17:33:20 2005
From: Heather.Turner at warwick.ac.uk (Heather Turner)
Date: Tue, 21 Jun 2005 16:33:20 +0100
Subject: [R] sweep() and recycling
Message-ID: <s2b8415a.001@liberator.csv.warwick.ac.uk>

Agreed my examples may be trivial and I'm sure there are more efficient ways to do the same thing, but I disagree that my examples are against the spirit of sweep (). In the first example a vector is swept out from the rows, with one value for odd rows and one value for even rows. In the second example an array of values is swept out across the third dimension. In the third example an array of values is swept out from the full array.

The first example is a natural use of recycling. E.g. 

sweep(matrix(1:100, 50, 2), 1, c(1, 1000), "+", give.warning = TRUE)

is a quicker way of writing

sweep(matrix(1:100, 50, 2), 1, rep(c(1, 1000), 25), "+", give.warning = TRUE)

but your code would give a warning in the first case, even though the intent and the result are exactly the same as in the second case.

As you say, it is only a warning, that can be ignored. However the warning should at least reflect the warning condition used, i.e. warn that the length of STATS does not equal the extent of MARGIN, rather warning that STATS does not recycle exactly.

Heather

>>> Robin Hankin <r.hankin at noc.soton.ac.uk> 06/21/05 02:47pm >>>
Hi

On Jun 21, 2005, at 02:33 pm, Heather Turner wrote:

> I think the warning condition in Robin's patch is too harsh - the 
> following examples seem reasonable to me, but all produce warnings
>
> sweep(array(1:24, dim = c(4,3,2)), 1, 1:2, give.warning = TRUE)
> sweep(array(1:24, dim = c(4,3,2)), 1, 1:12, give.warning = TRUE)
> sweep(array(1:24, dim = c(4,3,2)), 1, 1:24, give.warning = TRUE)
>


The examples above do give warnings (as intended) but I think all three 
cases above
are inimical to the spirit of sweep(): nothing is being "swept" out.

So a warning is appropriate, IMO.

In any case, one can always suppress (or ignore!) a warning if one knows
what one is doing.  YMMV, but if I wanted to do the above operations I 
would
replace


sweep(array(0, dim = c(4,3,2)), c(1,3), 1:12, "+" , give.warning = 
FALSE)

with

  aperm(array(1:12,c(4,2,3)),c(1,3,2))


best wishes

rksh







> I have written an alternative (given below) which does not give 
> warnings in the above cases, but does warn in the following case
>
>> sweep(array(1:24, dim = c(4,3,2)), 1:2, 1:3)
> , , 1
>
>      [,1] [,2] [,3]
> [1,]    0    3    6
> [2,]    0    3    9
> [3,]    0    6    9
> [4,]    3    6    9
>
> , , 2
>
>      [,1] [,2] [,3]
> [1,]   12   15   18
> [2,]   12   15   21
> [3,]   12   18   21
> [4,]   15   18   21
>
> Warning message:
> STATS does not recycle exactly across MARGIN
>
> The code could be easily modified to warn in other cases, e.g. when 
> length of STATS is a divisor of the corresponding array extent (as in 
> the first example above, with length(STATS) = 2).
>
> The code also includes Gabor's suggestion.
>
> Heather
>
> sweep <- function (x, MARGIN, STATS, FUN = "-", warn = 
> getOption("warn"), ...)
> {
>     FUN <- match.fun(FUN)
>     dims <- dim(x)
>     perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>     if (warn >= 0) {
>         s <- length(STATS)
>         cumDim <- c(1, cumprod(dims[perm]))
>         if (s > max(cumDim))
>             warning("length of STATS greater than length of array",
>                     call. = FALSE)
>         else {
>             upper <- min(ifelse(cumDim > s, cumDim, max(cumDim)))
>             lower <- max(ifelse(cumDim < s, cumDim, min(cumDim)))
>             if (any(upper %% s != 0, s %% lower != 0))
>                 warning("STATS does not recycle exactly across MARGIN",
>                         call. = FALSE)
>         }
>     }
>     FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
> }
>
>>>> Gabor Grothendieck <ggrothendieck at gmail.com> 06/21/05 01:25pm >>>
> \
> Perhaps the signature should be:
>
>    sweep(...other args go here..., warn=getOption("warn"))
>
> so that the name and value of the argument are consistent with
> the R warn option.
>
> On 6/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
>>
>> On Jun 20, 2005, at 04:58 pm, Prof Brian Ripley wrote:
>>
>>> The issue here is that the equivalent command array(1:5, c(6,6)) (to
>>> matrix(1:5,6,6)) gives no warning, and sweep uses array().
>>>
>>> I am not sure either should: fractional recycling was normally 
>>> allowed
>>> in S3 (S4 tightened up a bit).
>>>
>>> Perhaps someone who thinks sweep() should warn could contribute a
>>> tested patch?
>>>
>>
>>
>> OK,  modified R code and Rd file below (is this the best way to do
>> this?)
>>
>>
>>
>>
>> "sweep" <-
>>   function (x, MARGIN, STATS, FUN = "-", give.warning = FALSE, ...)
>> {
>>   FUN <- match.fun(FUN)
>>   dims <- dim(x)
>>   if(give.warning & length(STATS)>1 & any(dims[MARGIN] !=
>> dim(as.array(STATS)))){
>>     warning("array extents do not recycle exactly")
>>   }
>>   perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>>   FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
>> }
>>
>>
>>
>>
>>
>>
>>
>> \name{sweep}
>> \alias{sweep}
>> \title{Sweep out Array Summaries}
>> \description{
>>   Return an array obtained from an input array by sweeping out a 
>> summary
>>   statistic.
>> }
>> \usage{
>> sweep(x, MARGIN, STATS, FUN="-", give.warning = FALSE, \dots)
>> }
>> \arguments{
>>   \item{x}{an array.}
>>   \item{MARGIN}{a vector of indices giving the extents of \code{x}
>>     which correspond to \code{STATS}.}
>>   \item{STATS}{the summary statistic which is to be swept out.}
>>   \item{FUN}{the function to be used to carry out the sweep.  In the
>>     case of binary operators such as \code{"/"} etc., the function 
>> name
>>     must be quoted.}
>>   \item{give.warning}{Boolean, with default \code{FALSE} meaning to
>>   give no warning, even if array extents do not match.  If
>>   \code{TRUE}, check for the correct dimensions and if a
>>   mismatch is detected, give a suitable warning.}
>>   \item{\dots}{optional arguments to \code{FUN}.}
>> }
>> \value{
>>   An array with the same shape as \code{x}, but with the summary
>>   statistics swept out.
>> }
>> \note{
>>   If \code{STATS} is of length 1, recycling is carried out with no
>>   warning irrespective of the value of \code{give.warning}.
>> }
>>
>> \references{
>>   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
>>   \emph{The New S Language}.
>>   Wadsworth \& Brooks/Cole.
>> }
>> \seealso{
>>   \code{\link{apply}} on which \code{sweep} used to be based;
>>   \code{\link{scale}} for centering and scaling.
>> }
>> \examples{
>> require(stats) # for median
>> med.att <- apply(attitude, 2, median)
>> sweep(data.matrix(attitude), 2, med.att)# subtract the column medians
>>
>> a <- array(0, c(2, 3, 4))
>> b <- matrix(1:8, c(2, 4))
>> sweep(a, c(1, 3), b, "+", give.warning = TRUE)  # no warning:
>> all(dim(a)[c(1,3)] == dim(b))
>> sweep(a, c(1, 2), b, "+", give.warning = TRUE)  # warning given
>>
>> }
>> \keyword{array}
>> \keyword{iteration}
>>
>>
>>
>>
>> --
>> Robin Hankin
>> Uncertainty Analyst
>> National Oceanography Centre, Southampton
>> European Way, Southampton SO14 3ZH, UK
>>  tel  023-8059-7743
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html 
>>
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From med at aghmed.fsnet.co.uk  Tue Jun 21 17:49:59 2005
From: med at aghmed.fsnet.co.uk (Michael Dewey)
Date: Tue, 21 Jun 2005 16:49:59 +0100
Subject: [R] Problem trying to use boot and lme together
Message-ID: <6.2.1.2.0.20050621163524.02902090@pop.freeserve.net>

The outcome variable in my dataset is cost. I prefer not to transform it as 
readers will really be interested in pounds, not log(pounds) or 
sqrt(pounds). I have fitted my model using lme and now wish to use boot on 
it. I have therefore plagiarised the example in the article in RNews 2/3 
December 2002

after loading the dataset I source this file

====================================================
require(boot)
require(nlme)
model.0 <- lme(tot ~ time + timepost + pct + totpat
    + (time + timepost) : single + single
    + (time + timepost) : train + train
    + time:gpattend + timepost:gpattend + gpattend,
    data = common,
    random = ~time + timepost | gp
)
ints.9 <- intervals(model.0, which="fixed")$fixed[9,]
#
common$fit <- fitted(model.0)
common$res <- resid(model.0, type = "response")
cats.fit <- function(data) {
    mod <- lme(tot ~ time + timepost + pct + totpat
       + (time + timepost) : single + single
       + (time + timepost) : train + train
       + time:gpattend + timepost:gpattend + gpattend,
    data = data,
    random = ~ time + timepost | gp)
    summ <- summary(mod)
    c(fixef(summ), diag(summ$varFix))
}
model.fun <- function(d, i) {
    d$tot <- d$fit+d$res[i]
    cats.fit(d)
}
tot.boot <- boot(common, model.fun, R=999)
============================================
So I fit the model and then generate fitted values and residuals which I 
use within the model.fun function to generate the bootstrap resample.

If I do this the plot looks fine as does the jack.after.boot plot but the 
confidence intervals are about 10% of the width of the ones from the lme 
output. I wondered whether I was using the wrong fitted and residuals so I 
added level = 0 to the calls of fitted and resid respectively (level = 1 is 
the default) but this seems to lead to resamples to which lme cannot fit 
the model.

Can anyone spot what I am doing wrong?

I would appreciate a cc'ed response as my ISP has taken to bouncing the 
digest posts from R-help with probability approximately 0.3.

FWIW I am using 2.1.0 under XP (SP2) with the versions of boot and nlme 
which shipped with the binary.


Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html



From andy_liaw at merck.com  Tue Jun 21 18:47:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Jun 2005 12:47:51 -0400
Subject: [R] tapply
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9D2@usctmx1106.merck.com>

What does str(z) say?  I suspect the second column is a factor, which, after
the subsetting, has some empty levels.  If so, just drop those levels.

Andy

> From: Weiwei Shi
> 
> hi
> i tried all the methods suggested above:
> ave and rowsum with "with" function works for my situation. I think
> the problem might not be due to tapply.
> My data z comes from
> z<-y[y[[1]] %in% x[[2]], c(1,9)]
> 
> while z is supposed to have no entries for those non-matched 
> between x and y.
> 
> however, when I run tapply, and the result also includes those
> non-matched entries. I use is.na function to remove those entry from z
> first and then use tapply again, but the result is the same: those
> NA's and those non-matched results are still there. That's what I mean
> by "it doesn't work".
> 
> Is there something I missed here so that z "implicitly" has some
> "trace" back to y dataset?
> 
> thanks,
> 
> On 6/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > hi,
> > > i have another question on tapply:
> > > i have a dataset z like this:
> > > 5540 389100307391      2600
> > > 5541 389100307391      2600
> > > 5542 389100307391      2600
> > > 5543 389100307391      2600
> > > 5544 389100307391      2600
> > > 5546 381300302513        NA
> > > 5547 387000307470        NA
> > > 5548 387000307470        NA
> > > 5549 387000307470        NA
> > > 5550 387000307470        NA
> > > 5551 387000307470        NA
> > > 5552 387000307470        NA
> > >
> > > I want to sum the column 3 by column 2.
> > > I removed NA by calling:
> > > tapply(z[[3]], z[[2]], sum, na.rm=T)
> > > but it does not work.
> > >
> > > then, i used
> > > z1<-z[!is.na(z[[3]],]
> > > and repeat
> > > still doesn't work.
> > >
> > > please help.
> > >
> > 
> > Depending on what you want you may be able to use rowsum:
> > 
> > - display only groups that have at least one non-NA with the sum
> >   being the sum of the non-NAs:
> > 
> >         with(na.omit(z), rowsum(V3, V2))
> > 
> > - display all groups with the sum being NA if any member is NA:
> > 
> >         rowsum(z$V3, z$V2)
> > 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ripley at stats.ox.ac.uk  Tue Jun 21 18:53:58 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Jun 2005 17:53:58 +0100 (BST)
Subject: [R] Problem trying to use boot and lme together
In-Reply-To: <6.2.1.2.0.20050621163524.02902090@pop.freeserve.net>
References: <6.2.1.2.0.20050621163524.02902090@pop.freeserve.net>
Message-ID: <Pine.LNX.4.61.0506211741350.22099@gannet.stats>

We don't have the structure of your dataset.  But it seems pretty clear 
that your resampling is not preserving the random effects structure: you 
are always fitting to the same clusters labelled by gp and so missing the 
major source of variability.

You either need to resample clusters, or resample the cluster random 
effects, as well as resample the within-cluster effects.  I doubt if there 
is much theoretical support for such bootstrapping schemes, nor software 
support: I would prefer a so-called parametric bootstrapping approach, 
that is resample from the fitted model -- see simulate.lme.



On Tue, 21 Jun 2005, Michael Dewey wrote:

> The outcome variable in my dataset is cost. I prefer not to transform it as
> readers will really be interested in pounds, not log(pounds) or
> sqrt(pounds). I have fitted my model using lme and now wish to use boot on
> it. I have therefore plagiarised the example in the article in RNews 2/3
> December 2002
>
> after loading the dataset I source this file
>
> ====================================================
> require(boot)
> require(nlme)
> model.0 <- lme(tot ~ time + timepost + pct + totpat
>    + (time + timepost) : single + single
>    + (time + timepost) : train + train
>    + time:gpattend + timepost:gpattend + gpattend,
>    data = common,
>    random = ~time + timepost | gp
> )
> ints.9 <- intervals(model.0, which="fixed")$fixed[9,]
> #
> common$fit <- fitted(model.0)
> common$res <- resid(model.0, type = "response")
> cats.fit <- function(data) {
>    mod <- lme(tot ~ time + timepost + pct + totpat
>       + (time + timepost) : single + single
>       + (time + timepost) : train + train
>       + time:gpattend + timepost:gpattend + gpattend,
>    data = data,
>    random = ~ time + timepost | gp)
>    summ <- summary(mod)
>    c(fixef(summ), diag(summ$varFix))
> }
> model.fun <- function(d, i) {
>    d$tot <- d$fit+d$res[i]
>    cats.fit(d)
> }
> tot.boot <- boot(common, model.fun, R=999)
> ============================================
> So I fit the model and then generate fitted values and residuals which I
> use within the model.fun function to generate the bootstrap resample.
>
> If I do this the plot looks fine as does the jack.after.boot plot but the
> confidence intervals are about 10% of the width of the ones from the lme
> output. I wondered whether I was using the wrong fitted and residuals so I
> added level = 0 to the calls of fitted and resid respectively (level = 1 is
> the default) but this seems to lead to resamples to which lme cannot fit
> the model.
>
> Can anyone spot what I am doing wrong?
>
> I would appreciate a cc'ed response as my ISP has taken to bouncing the
> digest posts from R-help with probability approximately 0.3.
>
> FWIW I am using 2.1.0 under XP (SP2) with the versions of boot and nlme
> which shipped with the binary.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Tue Jun 21 19:14:08 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 21 Jun 2005 10:14:08 -0700
Subject: [R] Another Mix Model Question
In-Reply-To: <009f01c5763a$5e4ef220$0540210a@www.domain>
References: <42B7C853.70803@slafuente.net>
	<009f01c5763a$5e4ef220$0540210a@www.domain>
Message-ID: <42B84AE0.5020807@pdf.com>

	  To reinforce, Dimitris' excellent suggestion, I'd like to refer you 
to sect. 2.4 in Pinheiro and Bates (2000) Mixed-Effects Models in S and 
S-Plus (Springer, pp. 92-96).  Testing for parameters on a boundary of a 
parameter space, as, e.g., whether a variance component is zero, 
involves a violation of the assumptions for the stanadard asymptotic 
theory.  In this section, Pinheiro and Bates compare the output of 
simulate.mle with improved theory.  I highly recommend this section (and 
the book more generally).

	  spencer graves

Dimitris Rizopoulos wrote:

> AFAIK the COVTEST option just computes a Wald test! Since you want 
> test for variance components (which is on the boundary of the 
> parameter space), I'd suggest to use a LRT (i.e., anova.lme(model.1, 
> model.2)) and moreover consider the simulate.lme() function of the 
> "nlme" package.
> 
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Alfonso M Sanchez-Lafuente" <alfonso at slafuente.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, June 21, 2005 9:57 AM
> Subject: [R] Another Mix Model Question
> 
> 
> 
>>Hi again,
>>
>>thank you for your previous answers. Just another question, though 
>>...
>>
>>I get the following variance components after fitting a mixed model.
>>
>>
>>Groups   Name              Variance Std.Dev. Corr
>>PlantID  TreatmCtrl        0.51784  0.71961
>>         TreatmNoAccess    4.77469  2.18511  -0.063
>>         TreatmNoKeel      4.22726  2.05603   0.513  0.751
>>         TreatmNoSpur      0.45918  0.67763   0.158  0.303  0.319
>>         TreatmNoStand     3.45357  1.85838  -0.736 -0.070 -0.435 
>>0.495
>>PlantID  PollClassApis     1.12364  1.06002
>>         PollClassBombAnth 0.42769  0.65398  -0.759
>>Residual                   3.09669  1.75974
>>
>>My question is: if n random effects are included in a model, how can 
>>I
>>test the hypothesis that the variance of such effects is 0 ?
>>
>>Some sort of COVTEST option in Proc MIXED in SAS (sorry, SAS is 
>>still
>>more familar to me than R).
>>
>>-- 
>>
>>----------------------------------------------
>>Alfonso M. Sanchez-Lafuente
>>Departamento de Biologia Vegetal y Ecologia
>>Facultad de Biologia
>>Universidad de Sevilla
>>Avd. Reina Mercedes 9
>>E-41012, Sevilla, Spain
>>email: alfonso at slafuente.net / slafuente at us.es
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From helprhelp at gmail.com  Tue Jun 21 19:25:36 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Tue, 21 Jun 2005 12:25:36 -0500
Subject: [R] tapply
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9D2@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9D2@usctmx1106.merck.com>
Message-ID: <cdf817830506211025405a30db@mail.gmail.com>

Even before I tried, I already realize it must be true when I read
this reply! Great job! thanks, Andy.

> str(z)
`data.frame':   235 obs. of  2 variables:
 $ CLAIMNUM : Factor w/ 1907 levels "0","10000001849",..: 1083 1083
1083 1582 1582 1084 1681 1681 1391 1391 ...
 $ SIU.SAVED: int  475 3000 3000 0 0 4352 0 0 4500 3000 ...

So, I have another general question: how to avoid this when I do the matching?
In my case, claimnum does not have to be a factor.  I think I can do
as.integer on it to de-factor it. But, I want to know how to do it w/
keeping is as factor? btw, what's your way to drop those levels?  :)

weiwei 


On 6/21/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> What does str(z) say?  I suspect the second column is a factor, which, after
> the subsetting, has some empty levels.  If so, just drop those levels.
> 
> Andy
> 
> > From: Weiwei Shi
> >
> > hi
> > i tried all the methods suggested above:
> > ave and rowsum with "with" function works for my situation. I think
> > the problem might not be due to tapply.
> > My data z comes from
> > z<-y[y[[1]] %in% x[[2]], c(1,9)]
> >
> > while z is supposed to have no entries for those non-matched
> > between x and y.
> >
> > however, when I run tapply, and the result also includes those
> > non-matched entries. I use is.na function to remove those entry from z
> > first and then use tapply again, but the result is the same: those
> > NA's and those non-matched results are still there. That's what I mean
> > by "it doesn't work".
> >
> > Is there something I missed here so that z "implicitly" has some
> > "trace" back to y dataset?
> >
> > thanks,
> >
> > On 6/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > hi,
> > > > i have another question on tapply:
> > > > i have a dataset z like this:
> > > > 5540 389100307391      2600
> > > > 5541 389100307391      2600
> > > > 5542 389100307391      2600
> > > > 5543 389100307391      2600
> > > > 5544 389100307391      2600
> > > > 5546 381300302513        NA
> > > > 5547 387000307470        NA
> > > > 5548 387000307470        NA
> > > > 5549 387000307470        NA
> > > > 5550 387000307470        NA
> > > > 5551 387000307470        NA
> > > > 5552 387000307470        NA
> > > >
> > > > I want to sum the column 3 by column 2.
> > > > I removed NA by calling:
> > > > tapply(z[[3]], z[[2]], sum, na.rm=T)
> > > > but it does not work.
> > > >
> > > > then, i used
> > > > z1<-z[!is.na(z[[3]],]
> > > > and repeat
> > > > still doesn't work.
> > > >
> > > > please help.
> > > >
> > >
> > > Depending on what you want you may be able to use rowsum:
> > >
> > > - display only groups that have at least one non-NA with the sum
> > >   being the sum of the non-NAs:
> > >
> > >         with(na.omit(z), rowsum(V3, V2))
> > >
> > > - display all groups with the sum being NA if any member is NA:
> > >
> > >         rowsum(z$V3, z$V2)
> > >
> >
> >
> > --
> > Weiwei Shi, Ph.D
> >
> > "Did you always know?"
> > "No, I did not. But I believed..."
> > ---Matrix III
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> >
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From andy_liaw at merck.com  Tue Jun 21 19:30:54 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Jun 2005 13:30:54 -0400
Subject: [R] tapply
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9D4@usctmx1106.merck.com>

Try:

> (x <- factor(1:2, levels=1:5))
[1] 1 2
Levels: 1 2 3 4 5
> (x <- x[, drop=TRUE])
[1] 1 2
Levels: 1 2

Andy

> From: Weiwei Shi [mailto:helprhelp at gmail.com] 
> 
> Even before I tried, I already realize it must be true when I read
> this reply! Great job! thanks, Andy.
> 
> > str(z)
> `data.frame':   235 obs. of  2 variables:
>  $ CLAIMNUM : Factor w/ 1907 levels "0","10000001849",..: 1083 1083
> 1083 1582 1582 1084 1681 1681 1391 1391 ...
>  $ SIU.SAVED: int  475 3000 3000 0 0 4352 0 0 4500 3000 ...
> 
> So, I have another general question: how to avoid this when I 
> do the matching?
> In my case, claimnum does not have to be a factor.  I think I can do
> as.integer on it to de-factor it. But, I want to know how to do it w/
> keeping is as factor? btw, what's your way to drop those levels?  :)
> 
> weiwei 
> 
> 
> On 6/21/05, Liaw, Andy <andy_liaw at merck.com> wrote:
> > What does str(z) say?  I suspect the second column is a 
> factor, which, after
> > the subsetting, has some empty levels.  If so, just drop 
> those levels.
> > 
> > Andy
> > 
> > > From: Weiwei Shi
> > >
> > > hi
> > > i tried all the methods suggested above:
> > > ave and rowsum with "with" function works for my 
> situation. I think
> > > the problem might not be due to tapply.
> > > My data z comes from
> > > z<-y[y[[1]] %in% x[[2]], c(1,9)]
> > >
> > > while z is supposed to have no entries for those non-matched
> > > between x and y.
> > >
> > > however, when I run tapply, and the result also includes those
> > > non-matched entries. I use is.na function to remove those 
> entry from z
> > > first and then use tapply again, but the result is the same: those
> > > NA's and those non-matched results are still there. 
> That's what I mean
> > > by "it doesn't work".
> > >
> > > Is there something I missed here so that z "implicitly" has some
> > > "trace" back to y dataset?
> > >
> > > thanks,
> > >
> > > On 6/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> > > > On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
> > > > > hi,
> > > > > i have another question on tapply:
> > > > > i have a dataset z like this:
> > > > > 5540 389100307391      2600
> > > > > 5541 389100307391      2600
> > > > > 5542 389100307391      2600
> > > > > 5543 389100307391      2600
> > > > > 5544 389100307391      2600
> > > > > 5546 381300302513        NA
> > > > > 5547 387000307470        NA
> > > > > 5548 387000307470        NA
> > > > > 5549 387000307470        NA
> > > > > 5550 387000307470        NA
> > > > > 5551 387000307470        NA
> > > > > 5552 387000307470        NA
> > > > >
> > > > > I want to sum the column 3 by column 2.
> > > > > I removed NA by calling:
> > > > > tapply(z[[3]], z[[2]], sum, na.rm=T)
> > > > > but it does not work.
> > > > >
> > > > > then, i used
> > > > > z1<-z[!is.na(z[[3]],]
> > > > > and repeat
> > > > > still doesn't work.
> > > > >
> > > > > please help.
> > > > >
> > > >
> > > > Depending on what you want you may be able to use rowsum:
> > > >
> > > > - display only groups that have at least one non-NA with the sum
> > > >   being the sum of the non-NAs:
> > > >
> > > >         with(na.omit(z), rowsum(V3, V2))
> > > >
> > > > - display all groups with the sum being NA if any member is NA:
> > > >
> > > >         rowsum(z$V3, z$V2)
> > > >
> > >
> > >
> > > --
> > > Weiwei Shi, Ph.D
> > >
> > > "Did you always know?"
> > > "No, I did not. But I believed..."
> > > ---Matrix III
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> > >
> > 
> > 
> > 
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments, contains information of Merck & Co., Inc. (One 
> Merck Drive, Whitehouse Station, New Jersey, USA 08889), 
> and/or its affiliates (which may be known outside the United 
> States as Merck Frosst, Merck Sharp & Dohme or MSD and in 
> Japan, as Banyu) that may be confidential, proprietary 
> copyrighted and/or legally privileged. It is intended solely 
> for the use of the individual or entity named on this 
> message.  If you are not the intended recipient, and have 
> received this message in error, please notify us immediately 
> by reply e-mail and then delete it from your system.
> > 
> --------------------------------------------------------------
> ----------------
> > 
> 
> 
> -- 
> Weiwei Shi, Ph.D
> 
> "Did you always know?"
> "No, I did not. But I believed..."
> ---Matrix III
> 
> 
>



From elvis at xlsolutions-corp.com  Tue Jun 21 19:32:45 2005
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 21 Jun 2005 10:32:45 -0700
Subject: [R] Course***R/S System: Advanced Programming***July 2005 at 3
	locations
Message-ID: <20050621173245.31704.qmail@webmail12.prod.mesa1.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
our "Advanced R/Splus programming" course taught by R Development
Core Team Guru!

www.xlsolutions-corp.com/Radv.htm



*********New York ---------------  July 28th-29th, 2005

*********San Francisco ----------  July 18th-19th, 2005

*********Boston, MA    ----------  TBD

Early-bird discount ends Feb 25th!
Ask for group discount and reserve your seat Now  (payment due after
the class)

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578


Course Outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function

It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount. Over half of the seats in this
class
are currently reserved.  Register now to secure your seat in this
course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From ecoinformatics at gmail.com  Tue Jun 21 20:33:50 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Tue, 21 Jun 2005 20:33:50 +0200
Subject: [R] How to plot circular data in the directions of 0, 0.5pi,
	pi and 1.5pi
In-Reply-To: <15f8e67d05062102316a9d0bed@mail.gmail.com>
References: <15f8e67d05062102316a9d0bed@mail.gmail.com>
Message-ID: <15f8e67d05062111334f729a6e@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050621/5f6feaed/attachment.pl

From Soren.Hojsgaard at agrsci.dk  Tue Jun 21 22:28:42 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Tue, 21 Jun 2005 22:28:42 +0200
Subject: [R] Problem trying to use boot and lme together
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FE@DJFPOST01.djf.agrsci.dk>

The problem with simulate.lme is that it only returns logL for a given model fitted to a simulated data set  - not the simulated data set itself (which one might have expected a function with that name to do...). It would be nice with that functionality...
S??ren
 

________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Prof Brian Ripley
Sendt: ti 21-06-2005 18:53
Til: Michael Dewey
Cc: r-help-stat.math.ethz.ch
Emne: Re: [R] Problem trying to use boot and lme together



We don't have the structure of your dataset.  But it seems pretty clear
that your resampling is not preserving the random effects structure: you
are always fitting to the same clusters labelled by gp and so missing the
major source of variability.

You either need to resample clusters, or resample the cluster random
effects, as well as resample the within-cluster effects.  I doubt if there
is much theoretical support for such bootstrapping schemes, nor software
support: I would prefer a so-called parametric bootstrapping approach,
that is resample from the fitted model -- see simulate.lme.



On Tue, 21 Jun 2005, Michael Dewey wrote:

> The outcome variable in my dataset is cost. I prefer not to transform it as
> readers will really be interested in pounds, not log(pounds) or
> sqrt(pounds). I have fitted my model using lme and now wish to use boot on
> it. I have therefore plagiarised the example in the article in RNews 2/3
> December 2002
>
> after loading the dataset I source this file
>
> ====================================================
> require(boot)
> require(nlme)
> model.0 <- lme(tot ~ time + timepost + pct + totpat
>    + (time + timepost) : single + single
>    + (time + timepost) : train + train
>    + time:gpattend + timepost:gpattend + gpattend,
>    data = common,
>    random = ~time + timepost | gp
> )
> ints.9 <- intervals(model.0, which="fixed")$fixed[9,]
> #
> common$fit <- fitted(model.0)
> common$res <- resid(model.0, type = "response")
> cats.fit <- function(data) {
>    mod <- lme(tot ~ time + timepost + pct + totpat
>       + (time + timepost) : single + single
>       + (time + timepost) : train + train
>       + time:gpattend + timepost:gpattend + gpattend,
>    data = data,
>    random = ~ time + timepost | gp)
>    summ <- summary(mod)
>    c(fixef(summ), diag(summ$varFix))
> }
> model.fun <- function(d, i) {
>    d$tot <- d$fit+d$res[i]
>    cats.fit(d)
> }
> tot.boot <- boot(common, model.fun, R=999)
> ============================================
> So I fit the model and then generate fitted values and residuals which I
> use within the model.fun function to generate the bootstrap resample.
>
> If I do this the plot looks fine as does the jack.after.boot plot but the
> confidence intervals are about 10% of the width of the ones from the lme
> output. I wondered whether I was using the wrong fitted and residuals so I
> added level = 0 to the calls of fitted and resid respectively (level = 1 is
> the default) but this seems to lead to resamples to which lme cannot fit
> the model.
>
> Can anyone spot what I am doing wrong?
>
> I would appreciate a cc'ed response as my ISP has taken to bouncing the
> digest posts from R-help with probability approximately 0.3.
>
> FWIW I am using 2.1.0 under XP (SP2) with the versions of boot and nlme
> which shipped with the binary.


--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Tue Jun 21 22:56:17 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Tue, 21 Jun 2005 15:56:17 -0500
Subject: [R] Problem trying to use boot and lme together
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FE@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FE@DJFPOST01.djf.agrsci.dk>
Message-ID: <40e66e0b05062113564139403f@mail.gmail.com>

On 6/21/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> The problem with simulate.lme is that it only returns logL for a given model fitted to a simulated data set  - not the simulated data set itself (which one might have expected a function with that name to do...). It would be nice with that functionality...
> S??ren

You could add it.  You just need to create a matrix that will be large
enough to hold all the simulated data sets and fill a column (or row
if you prefer but column is probably better because of the way that
matrices are stored) during each iteration of the simulation and
remember to include that matrix in the returned object.

The reason that we didn't do that in the original design is because
that matrix can become rather large when you have either a lot of data
or a lot of simulations and we were working on the computers or 5 to
10 years ago.



From ripley at stats.ox.ac.uk  Wed Jun 22 00:09:36 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Jun 2005 23:09:36 +0100 (BST)
Subject: [R] Problem trying to use boot and lme together
In-Reply-To: <40e66e0b05062113564139403f@mail.gmail.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FE@DJFPOST01.djf.agrsci.dk>
	<40e66e0b05062113564139403f@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506212302450.25553@gannet.stats>

On Tue, 21 Jun 2005, Douglas Bates wrote:

> On 6/21/05, S?ren H?jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
>> The problem with simulate.lme is that it only returns logL for a given model fitted to a simulated data set  - not the simulated data set itself (which one might have expected a function with that name to do...). It would be nice with that functionality...
>> S?ren
>
> You could add it.  You just need to create a matrix that will be large
> enough to hold all the simulated data sets and fill a column (or row
> if you prefer but column is probably better because of the way that
> matrices are stored) during each iteration of the simulation and
> remember to include that matrix in the returned object.

Note: you don't need to store it: you can do the analysis at that point 
and return the statistics you want, rather than just logL.

I did say `see simulate.lme', not `use simulate.lme'.  I know nlme is no 
longer being developed, but if it were I would be suggesting/contributing 
a modification that allowed the user to specify an `extraction' function 
from the fit -- quite a few pieces of bootstrap code work that way.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From p.murrell at auckland.ac.nz  Wed Jun 22 00:28:17 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 22 Jun 2005 10:28:17 +1200
Subject: [R] Getting tick positions
References: <20050226195338.VXYR24950.smta03.mail.ozemail.net@there>
	<4221E24F.5080301@statistik.uni-dortmund.de>
Message-ID: <42B89481.1070200@stat.auckland.ac.nz>

Hi


Uwe Ligges wrote:
> Jim Lemon wrote:
> 
>> Thanks for the answers - I should have been more specific as I had 
>> already tried axTicks and pretty.
>>
>> The function in question is gantt.chart() in the latest plotrix 
>> package (Thanks to Scott Waichler for the original code). I settled on 
>> axis.POSIXct as it seemed the most appropriate for this function, but 
>> couldn't find a way to get the positions of the ticks so that I could 
>> then draw grid lines along the months|days|hours. While the trick of 
>> copying the original function works, I wondered why the axis* 
>> functions don't return the tick positions as barplot returns the bar 
>> positions. Is there any reason not to return "z", or is it just 
>> historical?
> 
> 
> I think historical.
> 
> Replacing
>     return R_NilValue;
> by
>     return at;
> in do_axis (plot.c) should do the trick (well, I'm a bit afraid 
> concerning the UNPROTECT, you might need to do that later, after the 
> recording stuff?).


Thanks for the suggestion Uwe (you were right about the UNPROTECT).
This change is now in the development version of R.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From mcclatchie.sam at saugov.sa.gov.au  Wed Jun 22 01:35:05 2005
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Wed, 22 Jun 2005 09:05:05 +0930
Subject: [R] R-help
Message-ID: <BEA6A7E18959A04385DC14D24619F89F01D73AF1@sagemsg0008.sagemsmrd01.sa.gov.au>

Background:
OS: Linux Mandrake 10.1
release: R 2.0.0
editor: GNU Emacs 21.3.2
front-end: ESS 5.2.3
---------------------------------
Colleagues

Is there a function in R that is an equivalent of zoom in matlab? This is
very useful for being able to magnify details in a plot.

I have searched the help for "zoom", "interactive zooming", and "magnify".
The R search engine (reached from help.start() )  keywords by topic entry
for dynamic graphics seems to have no content, unless I made a mistake. 

Here is the matlab function description: 
-------------------------------
zoom 	

Zoom in and out on a 2-D plot

Syntax

zoom on 
zoom off
zoom out
zoom reset
zoom
zoom xon
zoom yon
zoom(factor)
zoom(fig, option)

Description

zoom on turns on interactive zooming. When interactive zooming is enabled in
a figure, pressing a mouse button while your cursor is within an axes zooms
into the point or out from the point beneath the mouse. Zooming changes the
axes limits.


*	For a single-button mouse, zoom in by pressing the mouse button and
zoom out by simultaneously pressing Shift and the mouse button. 
*	
*	For a two- or three-button mouse, zoom in by pressing the left mouse
button and zoom out by pressing the right mouse button. 

Clicking and dragging over an axes when interactive zooming is enabled draws
a rubber-band box. When the mouse button is released, the axes zoom in to
the region enclosed by the rubber-band box.

Double-clicking over an axes returns the axes to its initial zoom setting.

zoom off turns interactive zooming off.

zoom out returns the plot to its initial zoom setting.

zoom reset remembers the current zoom setting as the initial zoom setting.
Later calls to zoom out, or double-clicks when interactive zoom mode is
enabled, will return to this zoom level.

zoom toggles the interactive zoom status. 

zoom xon and zoom yon set zoom on for the x- and y-axis, respectively.

zoom(factor) zooms in or out by the specified zoom factor, without affecting
the interactive zoom mode. Values greater than 1 zoom in by that amount,
while numbers greater than 0 and less than 1 zoom out by 1/factor.

zoom(fig, option) Any of the above options can be specified on a figure
other than the current figure using this syntax. 

Remarks

zoom changes the axes limits by a factor of two (in or out) each time you
press the mouse button while the cursor is within an axes. You can also
click and drag the mouse to define a zoom area, or double-click to return to
the initial zoom level. 
--------------------

Best fishes

Sam
----
Sam McClatchie,
Biological oceanography 
South Australian Aquatic Sciences Centre
PO Box 120, Henley Beach 5022
Adelaide, South Australia
email <mcclatchie.sam at saugov.sa.gov.au>
Telephone: (61-8) 8207 5448
FAX: (61-8) 8207 5481
Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
  
                   /\
      ...>><xX(??> 
                //// \\\\
                   <??)Xx><<
              /////  \\\\\\
                        ><(((??> 
  >><(((??>   ...>><xX(??>O<??)Xx><<



From ggrothendieck at gmail.com  Wed Jun 22 01:41:53 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Tue, 21 Jun 2005 19:41:53 -0400
Subject: [R] R-help
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F01D73AF1@sagemsg0008.sagemsmrd01.sa.gov.au>
References: <BEA6A7E18959A04385DC14D24619F89F01D73AF1@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <971536df0506211641761687dc@mail.gmail.com>

On 6/21/05, McClatchie, Sam (PIRSA-SARDI)
<mcclatchie.sam at saugov.sa.gov.au> wrote:
> Background:
> OS: Linux Mandrake 10.1
> release: R 2.0.0
> editor: GNU Emacs 21.3.2
> front-end: ESS 5.2.3
> ---------------------------------
> Colleagues
> 
> Is there a function in R that is an equivalent of zoom in matlab? This is
> very useful for being able to magnify details in a plot.
> 
> I have searched the help for "zoom", "interactive zooming", and "magnify".
> The R search engine (reached from help.start() )  keywords by topic entry
> for dynamic graphics seems to have no content, unless I made a mistake.

>From within R try:

   RSiteSearch("zoom")



From gerifalte28 at hotmail.com  Wed Jun 22 01:52:47 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 21 Jun 2005 23:52:47 +0000
Subject: [R] R-help
In-Reply-To: <BEA6A7E18959A04385DC14D24619F89F01D73AF1@sagemsg0008.sagemsmrd01.sa.gov.au>
Message-ID: <BAY103-F14F2116F4760D9C2DD88B1A6E80@phx.gbl>

Try RSiteSearch("zoom plot").  There are some good suggestions there.

Cheers

Francisco

>From: "McClatchie, Sam (PIRSA-SARDI)" <mcclatchie.sam at saugov.sa.gov.au>
>To: "R-Help-Request (E-mail)" <r-help at stat.math.ethz.ch>
>Subject: [R] R-help
>Date: Wed, 22 Jun 2005 09:05:05 +0930
>
>Background:
>OS: Linux Mandrake 10.1
>release: R 2.0.0
>editor: GNU Emacs 21.3.2
>front-end: ESS 5.2.3
>---------------------------------
>Colleagues
>
>Is there a function in R that is an equivalent of zoom in matlab? This is
>very useful for being able to magnify details in a plot.
>
>I have searched the help for "zoom", "interactive zooming", and "magnify".
>The R search engine (reached from help.start() )  keywords by topic entry
>for dynamic graphics seems to have no content, unless I made a mistake.
>
>Here is the matlab function description:
>-------------------------------
>zoom
>
>Zoom in and out on a 2-D plot
>
>Syntax
>
>zoom on
>zoom off
>zoom out
>zoom reset
>zoom
>zoom xon
>zoom yon
>zoom(factor)
>zoom(fig, option)
>
>Description
>
>zoom on turns on interactive zooming. When interactive zooming is enabled 
>in
>a figure, pressing a mouse button while your cursor is within an axes zooms
>into the point or out from the point beneath the mouse. Zooming changes the
>axes limits.
>
>
>*	For a single-button mouse, zoom in by pressing the mouse button and
>zoom out by simultaneously pressing Shift and the mouse button.
>*
>*	For a two- or three-button mouse, zoom in by pressing the left mouse
>button and zoom out by pressing the right mouse button.
>
>Clicking and dragging over an axes when interactive zooming is enabled 
>draws
>a rubber-band box. When the mouse button is released, the axes zoom in to
>the region enclosed by the rubber-band box.
>
>Double-clicking over an axes returns the axes to its initial zoom setting.
>
>zoom off turns interactive zooming off.
>
>zoom out returns the plot to its initial zoom setting.
>
>zoom reset remembers the current zoom setting as the initial zoom setting.
>Later calls to zoom out, or double-clicks when interactive zoom mode is
>enabled, will return to this zoom level.
>
>zoom toggles the interactive zoom status.
>
>zoom xon and zoom yon set zoom on for the x- and y-axis, respectively.
>
>zoom(factor) zooms in or out by the specified zoom factor, without 
>affecting
>the interactive zoom mode. Values greater than 1 zoom in by that amount,
>while numbers greater than 0 and less than 1 zoom out by 1/factor.
>
>zoom(fig, option) Any of the above options can be specified on a figure
>other than the current figure using this syntax.
>
>Remarks
>
>zoom changes the axes limits by a factor of two (in or out) each time you
>press the mouse button while the cursor is within an axes. You can also
>click and drag the mouse to define a zoom area, or double-click to return 
>to
>the initial zoom level.
>--------------------
>
>Best fishes
>
>Sam
>----
>Sam McClatchie,
>Biological oceanography
>South Australian Aquatic Sciences Centre
>PO Box 120, Henley Beach 5022
>Adelaide, South Australia
>email <mcclatchie.sam at saugov.sa.gov.au>
>Telephone: (61-8) 8207 5448
>FAX: (61-8) 8207 5481
>Research home page <http://www.members.iinet.net.au/~s.mcclatchie/>
>
>                    /\
>       ...>><xX(Å∞>
>                 //// \\\\
>                    <Å∞)Xx><<
>               /////  \\\\\\
>                         ><(((Å∞>
>   >><(((Å∞>   ...>><xX(Å∞>O<Å∞)Xx><<
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From dlvanbrunt at gmail.com  Wed Jun 22 03:24:28 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Tue, 21 Jun 2005 20:24:28 -0500
Subject: [R] (slightly off topic, but...) More of a stat design question...
In-Reply-To: <42B89481.1070200@stat.auckland.ac.nz>
References: <20050226195338.VXYR24950.smta03.mail.ozemail.net@there>
	<4221E24F.5080301@statistik.uni-dortmund.de>
	<42B89481.1070200@stat.auckland.ac.nz>
Message-ID: <d332d3e10506211824bf6174c@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050621/f04e181e/attachment.pl

From ray.xiong at gmail.com  Wed Jun 22 05:16:53 2005
From: ray.xiong at gmail.com (Ray Xiong)
Date: Tue, 21 Jun 2005 22:16:53 -0500
Subject: [R] PCA and MDS
Message-ID: <d6d6fd4405062120167f09aeb6@mail.gmail.com>

Dear All,

I am not familar with R. I want to use PCA (principal components
analysis) and MDS (multidimensional scaling). Can someone tell me
which R package I should use for PCA and MDS? I appreciate your help
in advance.

Ray



From gohidg at gmail.com  Wed Jun 22 06:05:00 2005
From: gohidg at gmail.com (Guohui Ding)
Date: Wed, 22 Jun 2005 12:05:00 +0800
Subject: [R] PCA and MDS
In-Reply-To: <d6d6fd4405062120167f09aeb6@mail.gmail.com>
References: <d6d6fd4405062120167f09aeb6@mail.gmail.com>
Message-ID: <f04a1d1d0506212105416ccbab@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050622/6213165d/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 22 08:34:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Jun 2005 07:34:42 +0100 (BST)
Subject: [R] PCA and MDS
In-Reply-To: <f04a1d1d0506212105416ccbab@mail.gmail.com>
References: <d6d6fd4405062120167f09aeb6@mail.gmail.com>
	<f04a1d1d0506212105416ccbab@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506220730530.30866@gannet.stats>

On Wed, 22 Jun 2005, Guohui Ding wrote:

> prcomp(stats) Principal Components Analysis
> princomp(stats) Principal Components Analysis
> scale(base) Scaling and Centering of Matrix-like Objects

The first two are correct for PCA, but scale is not MDS.

MDS is available in cmdscale (stats), isoMDS (MASS), sammon (MASS) and 
elsewhere.

>> I am not familar with R. I want to use PCA (principal components
>> analysis) and MDS (multidimensional scaling). Can someone tell me
>> which R package I should use for PCA and MDS? I appreciate your help
>> in advance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From laurent.pantera at irsn.fr  Wed Jun 22 09:14:03 2005
From: laurent.pantera at irsn.fr (PANTERA Laurent)
Date: Wed, 22 Jun 2005 09:14:03 +0200
Subject: [R] How to use expression in label with xYplot
Message-ID: <BF8E83A574F8FF40BD5C0266B2A4DF880153C21E@canna.proton.intra.irsn.fr>

Dear R-List,

 I want to use the label function (from Hmisc library) to allow for the
names of my isotopes.

library(Hmisc)
library(lattice)
library(grid)
num <- c("78","137","129m")
nom <- c("Ge","Cs","Te")
df <- data.frame(GE78=seq(nom),CS137=seq(nom),TE129m=seq(nom))

if I use this function to create the labels :

lab <- function(i)
  as.expression(bquote(italic(phantom(0)^{.(num[i])}*.(nom[i]))))

label(df$GE78) <-  lab(1)
label(df$CS137) <-  lab(2)
label(df$TE129m) <-  lab(3)

all works fine when I use text and xyplot :

plot(1:10)
text(6,6,labels=label(df$CS137))
xyplot(CS137~TE129m,data=df,xlab=label(df$CS137),ylab=label(df$TE129m))

but xYplot doesn't work fine

xYplot(CS137~TE129m,data=df)

I have the message :
Error in parse(file, n, text, prompt) : parse error

 if I change the lab function

lab <- function(i)
 
as.character(paste("italic(phantom(0)^{\"",num[i],"\"}*\"",nom[i],"\")",sep=
""))

text and xyplot work fine if I use the parse function and xYplot works fine.

label(df$GE78) <-  lab(1)
label(df$CS137) <-  lab(2)
label(df$TE129m) <-  lab(3)
plot(1:10)
text(6,6,labels=parse(text=label(df$CS137)))
xyplot(CS137~TE129m,data=df,xlab=parse(text=label(df$CS137)),ylab=parse(text
=label(df$TE129m)))
xYplot(CS137~TE129m,data=df)

What is the good way to use expression in the labels of variables with the
label function ?

thanks



From maechler at stat.math.ethz.ch  Wed Jun 22 10:13:27 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 22 Jun 2005 10:13:27 +0200
Subject: [R] tapply
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9D4@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9D4@usctmx1106.merck.com>
Message-ID: <17081.7591.466518.40197@stat.math.ethz.ch>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Tue, 21 Jun 2005 13:30:54 -0400 writes:

    AndyL> Try:
    >> (x <- factor(1:2, levels=1:5))
    AndyL> [1] 1 2
    AndyL> Levels: 1 2 3 4 5
    >> (x <- x[, drop=TRUE])
    AndyL> [1] 1 2
    AndyL> Levels: 1 2

or  
    (x <- factor(1:2, levels=1:5))
    (x2 <- factor(x))

which also drops the level
Martin

    AndyL> Andy

    >> From: Weiwei Shi [mailto:helprhelp at gmail.com] 
    >> 
    >> Even before I tried, I already realize it must be true when I read
    >> this reply! Great job! thanks, Andy.
    >> 
    >> > str(z)
    >> `data.frame':   235 obs. of  2 variables:
    >> $ CLAIMNUM : Factor w/ 1907 levels "0","10000001849",..: 1083 1083
    >> 1083 1582 1582 1084 1681 1681 1391 1391 ...
    >> $ SIU.SAVED: int  475 3000 3000 0 0 4352 0 0 4500 3000 ...
    >> 
    >> So, I have another general question: how to avoid this when I 
    >> do the matching?
    >> In my case, claimnum does not have to be a factor.  I think I can do
    >> as.integer on it to de-factor it. But, I want to know how to do it w/
    >> keeping is as factor? btw, what's your way to drop those levels?  :)
    >> 
    >> weiwei 
    >> 
    >> 
    >> On 6/21/05, Liaw, Andy <andy_liaw at merck.com> wrote:
    >> > What does str(z) say?  I suspect the second column is a 
    >> factor, which, after
    >> > the subsetting, has some empty levels.  If so, just drop 
    >> those levels.
    >> > 
    >> > Andy
    >> > 
    >> > > From: Weiwei Shi
    >> > >
    >> > > hi
    >> > > i tried all the methods suggested above:
    >> > > ave and rowsum with "with" function works for my 
    >> situation. I think
    >> > > the problem might not be due to tapply.
    >> > > My data z comes from
    >> > > z<-y[y[[1]] %in% x[[2]], c(1,9)]
    >> > >
    >> > > while z is supposed to have no entries for those non-matched
    >> > > between x and y.
    >> > >
    >> > > however, when I run tapply, and the result also includes those
    >> > > non-matched entries. I use is.na function to remove those 
    >> entry from z
    >> > > first and then use tapply again, but the result is the same: those
    >> > > NA's and those non-matched results are still there. 
    >> That's what I mean
    >> > > by "it doesn't work".
    >> > >
    >> > > Is there something I missed here so that z "implicitly" has some
    >> > > "trace" back to y dataset?
    >> > >
    >> > > thanks,
    >> > >
    >> > > On 6/20/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
    >> > > > On 6/20/05, Weiwei Shi <helprhelp at gmail.com> wrote:
    >> > > > > hi,
    >> > > > > i have another question on tapply:
    >> > > > > i have a dataset z like this:
    >> > > > > 5540 389100307391      2600
    >> > > > > 5541 389100307391      2600
    >> > > > > 5542 389100307391      2600
    >> > > > > 5543 389100307391      2600
    >> > > > > 5544 389100307391      2600
    >> > > > > 5546 381300302513        NA
    >> > > > > 5547 387000307470        NA
    >> > > > > 5548 387000307470        NA
    >> > > > > 5549 387000307470        NA
    >> > > > > 5550 387000307470        NA
    >> > > > > 5551 387000307470        NA
    >> > > > > 5552 387000307470        NA
    >> > > > >
    >> > > > > I want to sum the column 3 by column 2.
    >> > > > > I removed NA by calling:
    >> > > > > tapply(z[[3]], z[[2]], sum, na.rm=T)
    >> > > > > but it does not work.
    >> > > > >
    >> > > > > then, i used
    >> > > > > z1<-z[!is.na(z[[3]],]
    >> > > > > and repeat
    >> > > > > still doesn't work.
    >> > > > >
    >> > > > > please help.
    >> > > > >
    >> > > >
    >> > > > Depending on what you want you may be able to use rowsum:
    >> > > >
    >> > > > - display only groups that have at least one non-NA with the sum
    >> > > >   being the sum of the non-NAs:
    >> > > >
    >> > > >         with(na.omit(z), rowsum(V3, V2))
    >> > > >
    >> > > > - display all groups with the sum being NA if any member is NA:
    >> > > >
    >> > > >         rowsum(z$V3, z$V2)
    >> > > >
    >> > >
    >> > >
    >> > > --
    >> > > Weiwei Shi, Ph.D
    >> > >
    >> > > "Did you always know?"
    >> > > "No, I did not. But I believed..."
    >> > > ---Matrix III
    >> > >
    >> > > ______________________________________________
    >> > > R-help at stat.math.ethz.ch mailing list
    >> > > https://stat.ethz.ch/mailman/listinfo/r-help
    >> > > PLEASE do read the posting guide!
    >> > > http://www.R-project.org/posting-guide.html
    >> > >
    >> > >
    >> > >
    >> > 
    >> > 
    >> > 
    >> > 
    >> --------------------------------------------------------------
    >> ----------------
    >> > Notice:  This e-mail message, together with any 
    >> attachments, contains information of Merck & Co., Inc. (One 
    >> Merck Drive, Whitehouse Station, New Jersey, USA 08889), 
    >> and/or its affiliates (which may be known outside the United 
    >> States as Merck Frosst, Merck Sharp & Dohme or MSD and in 
    >> Japan, as Banyu) that may be confidential, proprietary 
    >> copyrighted and/or legally privileged. It is intended solely 
    >> for the use of the individual or entity named on this 
    >> message.  If you are not the intended recipient, and have 
    >> received this message in error, please notify us immediately 
    >> by reply e-mail and then delete it from your system.
    >> > 
    >> --------------------------------------------------------------
    >> ----------------
    >> > 
    >> 
    >> 
    >> -- 
    >> Weiwei Shi, Ph.D
    >> 
    >> "Did you always know?"
    >> "No, I did not. But I believed..."
    >> ---Matrix III
    >> 
    >> 
    >> 

    AndyL> ______________________________________________
    AndyL> R-help at stat.math.ethz.ch mailing list
    AndyL> https://stat.ethz.ch/mailman/listinfo/r-help
    AndyL> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From danbebber at yahoo.co.uk  Wed Jun 22 11:02:55 2005
From: danbebber at yahoo.co.uk (Dan Bebber)
Date: Wed, 22 Jun 2005 10:02:55 +0100 (BST)
Subject: [R] predict.coxph fitted values for failure times
Message-ID: <20050622090255.95934.qmail@web26301.mail.ukl.yahoo.com>

I would like to extract predicted failure times from a
coxph model in library(survival). However, none of the
prediction options ("lp", "risk", "expected", "terms")
seem to bear any relationship to failure time.

Perhaps I am asking the wrong question, but can coxph
provide predicted failure times?

Thanks,
Dan Bebber

Department of Plant Sciences
University of Oxford




		
___________________________________________________________ 
How much free photo storage do you get? Store your holiday



From m_323stat at hotmail.com  Wed Jun 22 11:08:05 2005
From: m_323stat at hotmail.com (Marianne dk)
Date: Wed, 22 Jun 2005 09:08:05 +0000
Subject: [R] A question on time-dependent covariates  in the Cox model.
Message-ID: <BAY13-F434DC32CCCC78AB9C349DDDEB0@phx.gbl>

I have a dataset with

event=death
time (from medical examination until death/censoring)
dose (given at examination time)

Two groups are considered, a non-exposed group (dose=0), an exposed group 
(dose between 5 and 60).

For some reason there is a theory of the dose increasing its effect over 
time (however it was only given (and measured) once = at the time of 
examination).

I tested a model:

coxph(Surv(time,dod)~dose + dose:time)

Previously I tested the model in SAS:

proc phreg data=test;
	model time*dod(0)=dose dosetime /rl ties=efron;
	dosetime=time*dose;
	run;

Without the interaction terms I get the same results for the two models. By 
including the interaction terms I do not. The model in R gives a negative 
coefficient for the interaction term which is expected to be positive (and 
is so in SAS). The LRTs are also completely different.

TWO QUESTIONS:

1) Is it reasonable to bring in an interaction term when dose is only 
measured once?

2) If yes, can anyone give a hint on explaining the difference between the 
models in R and SAS?

Thanx in advance,
marianne

_________________________________________________________________
Nyhet! Hotmail direkt i Mobilen! http://mobile.msn.com/



From ripley at stats.ox.ac.uk  Wed Jun 22 11:18:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Jun 2005 10:18:14 +0100 (BST)
Subject: [R] predict.coxph fitted values for failure times
In-Reply-To: <20050622090255.95934.qmail@web26301.mail.ukl.yahoo.com>
References: <20050622090255.95934.qmail@web26301.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506221010300.15665@gannet.stats>

On Wed, 22 Jun 2005, Dan Bebber wrote:

> I would like to extract predicted failure times from a
> coxph model in library(survival). However, none of the
> prediction options ("lp", "risk", "expected", "terms")
> seem to bear any relationship to failure time.
>
> Perhaps I am asking the wrong question, but can coxph
> provide predicted failure times?

Strictly no, as a Cox proportional hazards model does not model the 
baseline hazard.  However, there is a coxph method for survfit() which
allows you to predict the survival distribution for the fitted or new 
data, presumably using Breslow's estimator of the baseline hazard.
There's an example on the help page and more in MASS (the book and its 
scripts).

Unfortunately the help page is misleading, documenting only the
"km" method as if it were the generic (and using \synopsis to avoid
this being flagged).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From henric.nilsson at statisticon.se  Wed Jun 22 12:11:43 2005
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 22 Jun 2005 12:11:43 +0200
Subject: [R] mu^2(1-mu)^2 variance function for GLM
In-Reply-To: <f30eabe6d0267dd243fdbb072520ce4a@warwick.ac.uk>
References: <s2b1721d.097@liberator.csv.warwick.ac.uk>
	<f30eabe6d0267dd243fdbb072520ce4a@warwick.ac.uk>
Message-ID: <42B9395F.7010303@statisticon.se>

Dear Professor Firth,

David Firth said the following on 2005-06-16 17:22:

> I do not have a ready stock of other examples, but I do have my own 
> version of a family function for this, reproduced below.  It differs 
> from yours (apart from being a regular family function rather than using 
> a modified "quasi") in the definition of deviance residuals.  These 
> necessarily involve an arbitrary constant (see McCullagh and Nelder, 
> 1989, p330); in my function that arbitrariness is in the choice eps <- 
> 0.0005.  I don't think the deviance contributions as you specified in 
> your code below will have the right derivative (with respect to mu) for 
> observations where y=0 or y=1.

I'm sorry for the late reply.

You're right -- my definition of the deviance residuals isn't correct. 
Your code, on the other hand, seems to do the right thing.

Many thanks for this note and the provided `wedderburn' function.


Henric



From ripley at stats.ox.ac.uk  Wed Jun 22 12:12:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Jun 2005 11:12:06 +0100 (BST)
Subject: [R] sweep() and recycling
In-Reply-To: <s2b8415a.001@liberator.csv.warwick.ac.uk>
References: <s2b8415a.001@liberator.csv.warwick.ac.uk>
Message-ID: <Pine.LNX.4.61.0506221110480.17212@gannet.stats>

I think that as the proponents do not agree, we need to leave this as is.

BTW, R-devel is the place to discuss patched to R, rather than R-help.

On Tue, 21 Jun 2005, Heather Turner wrote:

> Agreed my examples may be trivial and I'm sure there are more efficient ways to do the same thing, but I disagree that my examples are against the spirit of sweep (). In the first example a vector is swept out from the rows, with one value for odd rows and one value for even rows. In the second example an array of values is swept out across the third dimension. In the third example an array of values is swept out from the full array.
>
> The first example is a natural use of recycling. E.g.
>
> sweep(matrix(1:100, 50, 2), 1, c(1, 1000), "+", give.warning = TRUE)
>
> is a quicker way of writing
>
> sweep(matrix(1:100, 50, 2), 1, rep(c(1, 1000), 25), "+", give.warning = TRUE)
>
> but your code would give a warning in the first case, even though the intent and the result are exactly the same as in the second case.
>
> As you say, it is only a warning, that can be ignored. However the warning should at least reflect the warning condition used, i.e. warn that the length of STATS does not equal the extent of MARGIN, rather warning that STATS does not recycle exactly.
>
> Heather
>
>>>> Robin Hankin <r.hankin at noc.soton.ac.uk> 06/21/05 02:47pm >>>
> Hi
>
> On Jun 21, 2005, at 02:33 pm, Heather Turner wrote:
>
>> I think the warning condition in Robin's patch is too harsh - the
>> following examples seem reasonable to me, but all produce warnings
>>
>> sweep(array(1:24, dim = c(4,3,2)), 1, 1:2, give.warning = TRUE)
>> sweep(array(1:24, dim = c(4,3,2)), 1, 1:12, give.warning = TRUE)
>> sweep(array(1:24, dim = c(4,3,2)), 1, 1:24, give.warning = TRUE)
>>
>
>
> The examples above do give warnings (as intended) but I think all three
> cases above
> are inimical to the spirit of sweep(): nothing is being "swept" out.
>
> So a warning is appropriate, IMO.
>
> In any case, one can always suppress (or ignore!) a warning if one knows
> what one is doing.  YMMV, but if I wanted to do the above operations I
> would
> replace
>
>
> sweep(array(0, dim = c(4,3,2)), c(1,3), 1:12, "+" , give.warning =
> FALSE)
>
> with
>
>  aperm(array(1:12,c(4,2,3)),c(1,3,2))
>
>
> best wishes
>
> rksh
>
>
>
>
>
>
>
>> I have written an alternative (given below) which does not give
>> warnings in the above cases, but does warn in the following case
>>
>>> sweep(array(1:24, dim = c(4,3,2)), 1:2, 1:3)
>> , , 1
>>
>>      [,1] [,2] [,3]
>> [1,]    0    3    6
>> [2,]    0    3    9
>> [3,]    0    6    9
>> [4,]    3    6    9
>>
>> , , 2
>>
>>      [,1] [,2] [,3]
>> [1,]   12   15   18
>> [2,]   12   15   21
>> [3,]   12   18   21
>> [4,]   15   18   21
>>
>> Warning message:
>> STATS does not recycle exactly across MARGIN
>>
>> The code could be easily modified to warn in other cases, e.g. when
>> length of STATS is a divisor of the corresponding array extent (as in
>> the first example above, with length(STATS) = 2).
>>
>> The code also includes Gabor's suggestion.
>>
>> Heather
>>
>> sweep <- function (x, MARGIN, STATS, FUN = "-", warn =
>> getOption("warn"), ...)
>> {
>>     FUN <- match.fun(FUN)
>>     dims <- dim(x)
>>     perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>>     if (warn >= 0) {
>>         s <- length(STATS)
>>         cumDim <- c(1, cumprod(dims[perm]))
>>         if (s > max(cumDim))
>>             warning("length of STATS greater than length of array",
>>                     call. = FALSE)
>>         else {
>>             upper <- min(ifelse(cumDim > s, cumDim, max(cumDim)))
>>             lower <- max(ifelse(cumDim < s, cumDim, min(cumDim)))
>>             if (any(upper %% s != 0, s %% lower != 0))
>>                 warning("STATS does not recycle exactly across MARGIN",
>>                         call. = FALSE)
>>         }
>>     }
>>     FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
>> }
>>
>>>>> Gabor Grothendieck <ggrothendieck at gmail.com> 06/21/05 01:25pm >>>
>> \
>> Perhaps the signature should be:
>>
>>    sweep(...other args go here..., warn=getOption("warn"))
>>
>> so that the name and value of the argument are consistent with
>> the R warn option.
>>
>> On 6/21/05, Robin Hankin <r.hankin at noc.soton.ac.uk> wrote:
>>>
>>> On Jun 20, 2005, at 04:58 pm, Prof Brian Ripley wrote:
>>>
>>>> The issue here is that the equivalent command array(1:5, c(6,6)) (to
>>>> matrix(1:5,6,6)) gives no warning, and sweep uses array().
>>>>
>>>> I am not sure either should: fractional recycling was normally
>>>> allowed
>>>> in S3 (S4 tightened up a bit).
>>>>
>>>> Perhaps someone who thinks sweep() should warn could contribute a
>>>> tested patch?
>>>>
>>>
>>>
>>> OK,  modified R code and Rd file below (is this the best way to do
>>> this?)
>>>
>>>
>>>
>>>
>>> "sweep" <-
>>>   function (x, MARGIN, STATS, FUN = "-", give.warning = FALSE, ...)
>>> {
>>>   FUN <- match.fun(FUN)
>>>   dims <- dim(x)
>>>   if(give.warning & length(STATS)>1 & any(dims[MARGIN] !=
>>> dim(as.array(STATS)))){
>>>     warning("array extents do not recycle exactly")
>>>   }
>>>   perm <- c(MARGIN, (1:length(dims))[-MARGIN])
>>>   FUN(x, aperm(array(STATS, dims[perm]), order(perm)), ...)
>>> }
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> \name{sweep}
>>> \alias{sweep}
>>> \title{Sweep out Array Summaries}
>>> \description{
>>>   Return an array obtained from an input array by sweeping out a
>>> summary
>>>   statistic.
>>> }
>>> \usage{
>>> sweep(x, MARGIN, STATS, FUN="-", give.warning = FALSE, \dots)
>>> }
>>> \arguments{
>>>   \item{x}{an array.}
>>>   \item{MARGIN}{a vector of indices giving the extents of \code{x}
>>>     which correspond to \code{STATS}.}
>>>   \item{STATS}{the summary statistic which is to be swept out.}
>>>   \item{FUN}{the function to be used to carry out the sweep.  In the
>>>     case of binary operators such as \code{"/"} etc., the function
>>> name
>>>     must be quoted.}
>>>   \item{give.warning}{Boolean, with default \code{FALSE} meaning to
>>>   give no warning, even if array extents do not match.  If
>>>   \code{TRUE}, check for the correct dimensions and if a
>>>   mismatch is detected, give a suitable warning.}
>>>   \item{\dots}{optional arguments to \code{FUN}.}
>>> }
>>> \value{
>>>   An array with the same shape as \code{x}, but with the summary
>>>   statistics swept out.
>>> }
>>> \note{
>>>   If \code{STATS} is of length 1, recycling is carried out with no
>>>   warning irrespective of the value of \code{give.warning}.
>>> }
>>>
>>> \references{
>>>   Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988)
>>>   \emph{The New S Language}.
>>>   Wadsworth \& Brooks/Cole.
>>> }
>>> \seealso{
>>>   \code{\link{apply}} on which \code{sweep} used to be based;
>>>   \code{\link{scale}} for centering and scaling.
>>> }
>>> \examples{
>>> require(stats) # for median
>>> med.att <- apply(attitude, 2, median)
>>> sweep(data.matrix(attitude), 2, med.att)# subtract the column medians
>>>
>>> a <- array(0, c(2, 3, 4))
>>> b <- matrix(1:8, c(2, 4))
>>> sweep(a, c(1, 3), b, "+", give.warning = TRUE)  # no warning:
>>> all(dim(a)[c(1,3)] == dim(b))
>>> sweep(a, c(1, 2), b, "+", give.warning = TRUE)  # warning given
>>>
>>> }
>>> \keyword{array}
>>> \keyword{iteration}
>>>
>>>
>>>
>>>
>>> --
>>> Robin Hankin
>>> Uncertainty Analyst
>>> National Oceanography Centre, Southampton
>>> European Way, Southampton SO14 3ZH, UK
>>>  tel  023-8059-7743
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton
> European Way, Southampton SO14 3ZH, UK
>  tel  023-8059-7743
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jun 22 12:34:46 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Jun 2005 12:34:46 +0200
Subject: [R] A question on time-dependent covariates  in the Cox model.
In-Reply-To: <BAY13-F434DC32CCCC78AB9C349DDDEB0@phx.gbl>
References: <BAY13-F434DC32CCCC78AB9C349DDDEB0@phx.gbl>
Message-ID: <x2wtomllx5.fsf@turmalin.kubism.ku.dk>

"Marianne dk" <m_323stat at hotmail.com> writes:

> I have a dataset with
> 
> event=death
> time (from medical examination until death/censoring)
> dose (given at examination time)
> 
> Two groups are considered, a non-exposed group (dose=0), an exposed group 
> (dose between 5 and 60).
> 
> For some reason there is a theory of the dose increasing its effect over 
> time (however it was only given (and measured) once = at the time of 
> examination).
> 
> I tested a model:
> 
> coxph(Surv(time,dod)~dose + dose:time)
> 
> Previously I tested the model in SAS:
> 
> proc phreg data=test;
> 	model time*dod(0)=dose dosetime /rl ties=efron;
> 	dosetime=time*dose;
> 	run;
> 
> Without the interaction terms I get the same results for the two models. By 
> including the interaction terms I do not. The model in R gives a negative 
> coefficient for the interaction term which is expected to be positive (and 
> is so in SAS). The LRTs are also completely different.
> 
> TWO QUESTIONS:
> 
> 1) Is it reasonable to bring in an interaction term when dose is only 
> measured once?
> 
> 2) If yes, can anyone give a hint on explaining the difference between the 
> models in R and SAS?

I don't know what SAS does, maybe it second-guesses your intentions,
but R will definitely get it completely wrong. If you use time as a
covariate, the same time (of death/censoring) will be applied at all
death times. Pretty obviously, long observation times tend to be
associated with low mortalities! With interactions you get, er,
similarly incorrect effects.

To do coxph with time-dependent variables, you need to split data
into little time segments, according to the death time of every death,
inserting a new variable (ntime, say) which is the time of the
endpoint of the interval. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From Morten.Sickel at nrpa.no  Wed Jun 22 13:40:32 2005
From: Morten.Sickel at nrpa.no (Morten Sickel)
Date: Wed, 22 Jun 2005 13:40:32 +0200
Subject: [R] Subsetting across a frame for plotting
Message-ID: <2326C830ADA651438DC694248E5FEF6026F56C@mailix.NRPA.LOCAL>

I have a huge frame holding holding model results for a number of
locations and time series:

> str(tonedata)
`data.frame':   434 obs. of  339 variables:
 $ VALUE   : int  101 104 105 106 111 118 119 121 122 123 ...
 $ COUNT   : int  2443 184 1539 1016 132 1208 1580 654 864 560 ...
 $ AREA    : num  6.11e+08 4.60e+07 3.85e+08 2.54e+08 3.30e+07 ...
 $ D1_1958 : num  470 446 452 457 407 ...
 $ D2_1958 : num  480 455 461 467 416 ...
 $ D3_1958 : num  493 469 475 480 429 ...
 $ D4_1958 : num  542 517 522 526 475 ...
 $ D5_1958 : num  585 560 565 568 517 ...

I would like to be able to take all values, except the three first
(value, count, area) and be able to plot them. I have managed to make a
subset that looks like what I want, by doing tonedata[11,4:339], but
that data set is not a vector that can be plottet, it is treated like a
set of single values. I tried to use as.vector on the set, bot to no
help. I am probably overlooking somehing quite simple, (not to mention
not really understanding R's data model..) so help would be appreciated.

-- 
Morten Sickel
Norwegian Radiation Protection Authority



From e9826064 at student.tuwien.ac.at  Wed Jun 22 13:44:09 2005
From: e9826064 at student.tuwien.ac.at (Thomas Steiner)
Date: Wed, 22 Jun 2005 13:44:09 +0200
Subject: [R]  legend
Message-ID: <iihhxl.stn69u@webmail.tuwien.ac.at>

I color some area grey with polygon() (with a red border) and then I
want to have the dashed red border in the legend as well. How do I
manage it?

And I want to mix (latex) expressions with text in my legend.

Just execute my lines below and you know want I mean. Or pass by at
http://de.wikipedia.org/wiki/Bild:GBM.png to see the picture online.

Thomas


bm <- function(n=500, from=0, to=1) {
  x=seq(from=from,to=to,length=n)
  BM<-c(0,cumsum(rnorm(n-1,mean=0,sd=sqrt(to/n))))
  cbind(x,BM)
}
gbm <- function(bm,S0=1,sigma=0.1,mu=1) {
  gbm=S0
  for (t in 2:length(bm[,1])) {
    gbm[t]=S0*exp((mu-sigma^2/2)*bm[t,1]+sigma*bm[t,2])
  }
  cbind(bm[,1],gbm)
}

set.seed(9826064)
cs=c("dark green", "steelblue", "red", "yellow")

#png(filename = "GBM.png", width=1600, height=1200, pointsize = 12)
par(bg="lightgrey")
x=seq(from=0,to=1,length=500)
plot(x=x, y=exp(0.7*x), type="n", xlab="Zeit", ylab="", ylim=c(1,3.5))
polygon(x=c(x,rev(x)),
y=c(exp(0.7*x)+0.4*sqrt(x),rev(exp(0.7*x)-0.4*sqrt(x))), col="grey",
border=cs[3], lty="dashed")
lines(x=x,y=exp(0.7*x), type="l", lwd=3, col=cs[1])
lines(gbm(bm(),S0=1,mu=0.7,sigma=0.4), lwd=3, col=cs[2])
lines(gbm(bm(),S0=1,mu=0.7,sigma=0.2), lwd=3, col=cs[3])
lines(gbm(bm(),S0=1,mu=0.7,sigma=0.1), lwd=3, col=cs[4])
title(main="Geometrische Brownsche Bewegung",cex.main=2.5)
legend(x=0,y=3.5,legend=c("exp(0.7x)","mu=0.7, sigma=0.4","mu=0.7,
sigma=0.2","mu=0.7, sigma=0.1","Standardabweichung f??r 
sigma=0.2"),lwd=c(4,4,4,4,12),col=c(cs,"grey"),bg="transparent",cex=1.15)
#dev.off()



From petr.pikal at precheza.cz  Wed Jun 22 14:06:14 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 22 Jun 2005 14:06:14 +0200
Subject: [R] nlme leading minor error
Message-ID: <42B97056.13040.141D279@localhost>

Dear all

I am struggling with nlme and error message. Even going through 
Pinheiro, Bates nlme book did not gave me a clue how to avoid 
this.

fit <- nlme(ce ~ fi1 / ((1+exp(fi2-fi3*tepl))^(1/fi4)), data = 
temp1na.gr,
start = c(fi1=30, fi2=-100, fi3=-.05, fi4=40), 
fixed = fi1+fi2+fi3+fi4~1, 
random = pdDiag(fi2+fi4~1),
groups = ~spol.f)

gives

Error in chol((value + t(value))/2) : the leading minor of order 1 is 
not positive definite

Is this error due to lack of experimental points?
Here you have one typical part of my data. It is for level spol.f = 
"3/11".

tepl	ce
800	28.87
800	29.35
825	29
850	28.73
875	26.83
900	24.07

I have 1-5 points for each level (2 levels with 5 points, 1 level with 
4 points, several levels with 2 and 3 points and few with only one 
point.

Fitting this model to each level separately led to several sets of 
coeficients fi1-fi4 and the separate fits were quite OK.

Please give me a hint what can be the cause for this error message 
and how I shall organize my data to avoid this. (Lack of 
experimental points is also an answer as I can do some subsequent 
measurement.

R 2.1.0, W 2000, nlme package

Best regards



Petr Pikal
petr.pikal at precheza.cz



From andy_liaw at merck.com  Wed Jun 22 14:13:31 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 08:13:31 -0400
Subject: [R] Subsetting across a frame for plotting
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9DC@usctmx1106.merck.com>

You might save yourself some headaches by turning it into a matrix instead,
since all the columns are either integer or numeric:

tonedata <- data.matrix(tonedata)

Data frames are really lists, so even when you get a one-row subset, it's
still a one-row data frame.  You can use unlist() to turn that into a
vector.

Andy

> From: Morten Sickel
> 
> I have a huge frame holding holding model results for a number of
> locations and time series:
> 
> > str(tonedata)
> `data.frame':   434 obs. of  339 variables:
>  $ VALUE   : int  101 104 105 106 111 118 119 121 122 123 ...
>  $ COUNT   : int  2443 184 1539 1016 132 1208 1580 654 864 560 ...
>  $ AREA    : num  6.11e+08 4.60e+07 3.85e+08 2.54e+08 3.30e+07 ...
>  $ D1_1958 : num  470 446 452 457 407 ...
>  $ D2_1958 : num  480 455 461 467 416 ...
>  $ D3_1958 : num  493 469 475 480 429 ...
>  $ D4_1958 : num  542 517 522 526 475 ...
>  $ D5_1958 : num  585 560 565 568 517 ...
> 
> I would like to be able to take all values, except the three first
> (value, count, area) and be able to plot them. I have managed 
> to make a
> subset that looks like what I want, by doing tonedata[11,4:339], but
> that data set is not a vector that can be plottet, it is 
> treated like a
> set of single values. I tried to use as.vector on the set, bot to no
> help. I am probably overlooking somehing quite simple, (not to mention
> not really understanding R's data model..) so help would be 
> appreciated.
> 
> -- 
> Morten Sickel
> Norwegian Radiation Protection Authority
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From marco.zucchelli at biosci.ki.se  Wed Jun 22 14:27:58 2005
From: marco.zucchelli at biosci.ki.se (Marco Zucchelli)
Date: Wed, 22 Jun 2005 14:27:58 +0200
Subject: [R] the svDialogs package
References: <005501c56cfa$04ff6f40$ec6eed82@pizero>
	<42A926D7.5000906@sciviews.org>
Message-ID: <00f601c57725$d440a9b0$bf6eed82@pizero>

Hello Philippe,

Thanks for the explanation!

  did you change the guidlgopen as well?

Now I get :

file <- guiDlgOpen(title= "Open case/control 
file",defaultFile="",defaultDir="",multi=FALSE, filters = c("All files 
(*.*)", "*.*"))

gdata <- read.table(file,as.is=T,header=T)

Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file 'C:/ProgramFiles/R/Work/dcdc2.txt'


and the reason is that

> file
[1] "C:/ProgramFiles/R/Work/dcdc2.txt"

so the space between Program and Files has disappeared and the path is wrong

correct path would be "C:/Program Files/R/Work/dcdc2.txt"

Marco

----- Original Message ----- 
From: "Philippe Grosjean" <phgrosjean at sciviews.org>
To: "Marco Zucchelli" <marco.zucchelli at biosci.ki.se>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, June 10, 2005 7:36 AM
Subject: Re: [R] the svDialogs package


> Hello Marco,
>
> For the first error, the message is clear: "not implemented yet!".
> Several dialog boxes are not done yet, but the functions already exist,
> mainly as "placeholders" for future development.
>
> Regarding the second, there was a bug in the function (corrected in
> SciViews 0.8-6 that I have just uploaded to CRAN), and also a
> misunderstanding of the its first argument: "list". This argument should
> be a charactger vector containing the list of items... but not a list!
> So, the correct code is:
>
> > m_list <- 1:10
> > res <- guiDlgList(m_list) # Need SciViews 0.8-6!
> > res
>
> Note that guiDlgXXX() functions return results _invisibly_. So, you need
> to assign its result to a variable, or use something like:
>
> > (guiDlgList(m_list))
>
> to see the result printed at the console.
>
> Best,
>
> Philippe
>
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
>
> Marco Zucchelli wrote:
>> Hi Philippe and R community,
>>
>>  I am trying to use some functions from the svDialogs package but I get 
>> some werid errors I do not understand:
>>
>>
>>>library(svDialogs)
>>
>>
>>>m_list <- as.list(1:10)
>>
>>
>>
>>>guiDlgDoubleList(m_list, m_list)
>>
>> Error in guiDlgDoubleList(m_list, m_list) :
>>         Not yet implemented!
>>
>>
>>
>>>guiDlgList(m_list)
>>
>> Error in guiDlgList(m_list) : couldn't find function "guiSetFonts.tcltk"
>>
>>
>>
>>
>> Am I doing anything wrong ?? Do I need some other package?
>>
>>
>>
>> Marco
>>
>>
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Wed Jun 22 14:30:09 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 22 Jun 2005 08:30:09 -0400
Subject: [R] How to use expression in label with xYplot
In-Reply-To: <BF8E83A574F8FF40BD5C0266B2A4DF880153C21E@canna.proton.intra.irsn.fr>
References: <BF8E83A574F8FF40BD5C0266B2A4DF880153C21E@canna.proton.intra.irsn.fr>
Message-ID: <42B959D1.2080200@vanderbilt.edu>

PANTERA Laurent wrote:
> Dear R-List,
> 
>  I want to use the label function (from Hmisc library) to allow for the
> names of my isotopes.
> 
> library(Hmisc)
> library(lattice)
> library(grid)
> num <- c("78","137","129m")
> nom <- c("Ge","Cs","Te")
> df <- data.frame(GE78=seq(nom),CS137=seq(nom),TE129m=seq(nom))
> 
> if I use this function to create the labels :
> 
> lab <- function(i)
>   as.expression(bquote(italic(phantom(0)^{.(num[i])}*.(nom[i]))))
> 
> label(df$GE78) <-  lab(1)
> label(df$CS137) <-  lab(2)
> label(df$TE129m) <-  lab(3)
> 
> all works fine when I use text and xyplot :
> 
> plot(1:10)
> text(6,6,labels=label(df$CS137))
> xyplot(CS137~TE129m,data=df,xlab=label(df$CS137),ylab=label(df$TE129m))
> 
> but xYplot doesn't work fine
> 
> xYplot(CS137~TE129m,data=df)
> 
> I have the message :
> Error in parse(file, n, text, prompt) : parse error
> 
>  if I change the lab function
> 
> lab <- function(i)
>  
> as.character(paste("italic(phantom(0)^{\"",num[i],"\"}*\"",nom[i],"\")",sep=
> ""))
> 
> text and xyplot work fine if I use the parse function and xYplot works fine.
> 
> label(df$GE78) <-  lab(1)
> label(df$CS137) <-  lab(2)
> label(df$TE129m) <-  lab(3)
> plot(1:10)
> text(6,6,labels=parse(text=label(df$CS137)))
> xyplot(CS137~TE129m,data=df,xlab=parse(text=label(df$CS137)),ylab=parse(text
> =label(df$TE129m)))
> xYplot(CS137~TE129m,data=df)
> 
> What is the good way to use expression in the labels of variables with the
> label function ?
> 
> thanks

In Hmisc, labels are used for many contexts in which expressions aren't 
allowed, so as of this moment labels must be ordinary character strings. 
  I typically pass expressions to xYplot using xlab or ylab directly 
without using the label.

Frank

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From halldor at vedur.is  Wed Jun 22 14:38:14 2005
From: halldor at vedur.is (=?ISO-8859-1?Q?Halldor_Bj=F6rnsson?=)
Date: Wed, 22 Jun 2005 12:38:14 +0000
Subject: [R] A polar.plot BUG in plotrix 1.3.3 ?
Message-ID: <42B95BB6.4090507@vedur.is>

Hi,

I just updated to R-2.1.1 and updated packages acordingly

However, after the update, routines that use polar.plot
did not function as correctly.

In plotrix 1.3.3 the polar.plot function does scale label.pos
to radians prior to calling radial.plot

Hence, the command
polar.plot(c(5,10,5,0),c(-10,0,10,20),rp.type='P',
     labels=c("A","N","V","S"),label.pos=c(0,90,180,270))

produces absurd compass lines but the correct results are obtained if 
label.pos is scaled with pi/180 :

polar.plot(c(5,10,5,0),c(-10,0,10,20),rp.type='P',
     labels=c("A","N","V","S"),label.pos=c(0,90,180,270)*pi/180)

I have attatched the polar.plot function from the two different versions 
of the package.

It seems that if not missing  then label.pos is not changed at all in 
version 1.3.3

So, a feature or a bug?

Sincerely,
-- 
------------------------------------------
Halldor Bjornsson   (halldor at vedur.is)
Vedurstofa Islands (Icelandic Met. Office)
Bustadavegur 9, IS-150, Reykjavik, Iceland
------------------------------------------

In version 1.3.3

polar.plot<-function(lengths,polar.pos,labels,label.pos,rp.type="r",...) {
  npos<-length(lengths)
  # if no positions are given, add the average distance between 
positions so that
  # the first and last line don't overlap
  if(missing(polar.pos)) radial.pos<-seq(0,(2-2/(npos+1))*pi,length=npos)
  else radial.pos<-pi*polar.pos/180
  if(missing(labels)) {
   labels<-as.character(seq(0,340,by=20))
   label.pos<-seq(0,1.9*pi,length=18)
  }
  if(missing(label.pos)) label.pos<-pi*label.pos/180
  radial.plot(lengths,radial.pos,range(radial.pos),labels,label.pos,
   rp.type=rp.type,...)
}



In version 1.2

polar.plot<-function(lengths,polar.pos,labels,label.pos,rp.type="r",...) {
  npos<-length(lengths)
  # if no positions are given, add the average distance between 
positions so that
  # the first and last line don't overlap
  if(missing(polar.pos)) polar.pos<-seq(0,360-360/(npos+1),length=npos)
  if(missing(labels)) {
   label.pos<-seq(0,340,by=20)
   labels<-as.character(label.pos)
   label.range<-c(0,pi*340/180)
  }
  if(missing(label.pos)) label.pos<-polar.pos
  polar.range<-range(polar.pos)
  newrange<-c(pi*polar.range[1]/180,pi*(2-(360-polar.range[2])/180))
  # rescale to radians
  radial.pos<-rescale(c(polar.pos,polar.range),newrange)[1:npos]
  nlabels<-length(labels)
  label.pos<-rescale(c(label.pos,0,360),c(0,2*pi))[1:nlabels]

radial.plot(lengths,radial.pos,newrange,labels,label.pos,rp.type=rp.type,...)
}



From marco.zucchelli at biosci.ki.se  Wed Jun 22 14:42:41 2005
From: marco.zucchelli at biosci.ki.se (Marco Zucchelli)
Date: Wed, 22 Jun 2005 14:42:41 +0200
Subject: [R] the svDialogs package
References: <005501c56cfa$04ff6f40$ec6eed82@pizero>
	<42A926D7.5000906@sciviews.org>
Message-ID: <010701c57727$e26273a0$bf6eed82@pizero>

Hello again,

 I am using the guidlglist

I noticed (on windows) that if you scroll the list by using the up and down 
arrows of the gui by clicking with the mouse, if you click twice fast the 
gui disappears and the value on top of the list is selected.

It is meant to be like that ?

Marco



----- Original Message ----- 
From: "Philippe Grosjean" <phgrosjean at sciviews.org>
To: "Marco Zucchelli" <marco.zucchelli at biosci.ki.se>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Friday, June 10, 2005 7:36 AM
Subject: Re: [R] the svDialogs package


> Hello Marco,
>
> For the first error, the message is clear: "not implemented yet!".
> Several dialog boxes are not done yet, but the functions already exist,
> mainly as "placeholders" for future development.
>
> Regarding the second, there was a bug in the function (corrected in
> SciViews 0.8-6 that I have just uploaded to CRAN), and also a
> misunderstanding of the its first argument: "list". This argument should
> be a charactger vector containing the list of items... but not a list!
> So, the correct code is:
>
> > m_list <- 1:10
> > res <- guiDlgList(m_list) # Need SciViews 0.8-6!
> > res
>
> Note that guiDlgXXX() functions return results _invisibly_. So, you need
> to assign its result to a variable, or use something like:
>
> > (guiDlgList(m_list))
>
> to see the result printed at the console.
>
> Best,
>
> Philippe
>
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone (3D08)
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.30.54
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )          http://www.sciviews.org
> ( ( ( ( (
> ..............................................................
>
> Marco Zucchelli wrote:
>> Hi Philippe and R community,
>>
>>  I am trying to use some functions from the svDialogs package but I get 
>> some werid errors I do not understand:
>>
>>
>>>library(svDialogs)
>>
>>
>>>m_list <- as.list(1:10)
>>
>>
>>
>>>guiDlgDoubleList(m_list, m_list)
>>
>> Error in guiDlgDoubleList(m_list, m_list) :
>>         Not yet implemented!
>>
>>
>>
>>>guiDlgList(m_list)
>>
>> Error in guiDlgList(m_list) : couldn't find function "guiSetFonts.tcltk"
>>
>>
>>
>>
>> Am I doing anything wrong ?? Do I need some other package?
>>
>>
>>
>> Marco
>>
>>
>>
>>
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nassar at noos.fr  Wed Jun 22 15:12:16 2005
From: nassar at noos.fr (Naji)
Date: Wed, 22 Jun 2005 15:12:16 +0200
Subject: [R] Snow package  --> Results
In-Reply-To: <0A11038C-5A00-44D0-AB98-B6B84956ACA6@r-project.org>
Message-ID: <BEDF3050.4963%nassar@noos.fr>

First, many thanks to Simon Urbanek for his help.

A simulation case (50x34x5x3 & 250 replications) each based on behavior of
2500 consumers takes
- 2h20mins on a laptop (2 MHz, 1Go, WinXp)
- 1h on a G5 bi-proc (2.5 Mhz, 2Go)
A loop was used for the laptop, the same loop was transposed as function for
the Mac

Best regards
Naji



From jacob.etches at utoronto.ca  Wed Jun 22 15:21:43 2005
From: jacob.etches at utoronto.ca (Jacob Etches)
Date: Wed, 22 Jun 2005 09:21:43 -0400
Subject: [R] A question on time-dependent covariates  in the Cox model.
In-Reply-To: <x2wtomllx5.fsf@turmalin.kubism.ku.dk>
References: <BAY13-F434DC32CCCC78AB9C349DDDEB0@phx.gbl>
	<x2wtomllx5.fsf@turmalin.kubism.ku.dk>
Message-ID: <343af8b2adc6767f57bbfbd05feb588c@utoronto.ca>

This is a question about time-varying effects rather than time-varying 
covariates, even if the SAS method tests for the former by using the 
latter.  SAS evaluates the line

>> dosetime=time*dose;

for all observations at each event time as it estimates the model, such 
that you are not using future information.  It has the effect of 
testing for a linear change in the magnitude of the effect of dose over 
time.  I believe Paul Allison's survival book recommends this as a 
quick and dirty test for constancy of effect.  Had you put that line in 
a datastep prior to PHREG, rather than in PHREG, you'd get a completely 
different (and uninformative) result (probably the same as R is giving 
you), because each observation's total survival time would be used to 
create a single value for the interaction term.  You could manually 
replicate SAS's behaviour in R if you wanted, but every observation 
would have to start a new time interval whenever any other observation 
has an event, as Peter explained below.

You might also want to look at Aalen's additive survival model for 
non-linear changes in effect over time:
http://www.med.uio.no/imb/stat/addreg/

hope that helps,
Jacob Etches


On 2005/06/22, at 06:34, Peter Dalgaard wrote:

> "Marianne dk" <m_323stat at hotmail.com> writes:
>
>> I have a dataset with
>>
>> event=death
>> time (from medical examination until death/censoring)
>> dose (given at examination time)
>>
>> Two groups are considered, a non-exposed group (dose=0), an exposed 
>> group
>> (dose between 5 and 60).
>>
>> For some reason there is a theory of the dose increasing its effect 
>> over
>> time (however it was only given (and measured) once = at the time of
>> examination).
>>
>> I tested a model:
>>
>> coxph(Surv(time,dod)~dose + dose:time)
>>
>> Previously I tested the model in SAS:
>>
>> proc phreg data=test;
>> 	model time*dod(0)=dose dosetime /rl ties=efron;
>> 	dosetime=time*dose;
>> 	run;
>>
>> Without the interaction terms I get the same results for the two 
>> models. By
>> including the interaction terms I do not. The model in R gives a 
>> negative
>> coefficient for the interaction term which is expected to be positive 
>> (and
>> is so in SAS). The LRTs are also completely different.
>>
>> TWO QUESTIONS:
>>
>> 1) Is it reasonable to bring in an interaction term when dose is only
>> measured once?
>>
>> 2) If yes, can anyone give a hint on explaining the difference 
>> between the
>> models in R and SAS?
>
> I don't know what SAS does, maybe it second-guesses your intentions,
> but R will definitely get it completely wrong. If you use time as a
> covariate, the same time (of death/censoring) will be applied at all
> death times. Pretty obviously, long observation times tend to be
> associated with low mortalities! With interactions you get, er,
> similarly incorrect effects.
>
> To do coxph with time-dependent variables, you need to split data
> into little time segments, according to the death time of every death,
> inserting a new variable (ntime, say) which is the time of the
> endpoint of the interval.
>
> -- 
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 
> 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dmbates at gmail.com  Wed Jun 22 15:23:05 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 22 Jun 2005 08:23:05 -0500
Subject: [R] nlme leading minor error
In-Reply-To: <42B97056.13040.141D279@localhost>
References: <42B97056.13040.141D279@localhost>
Message-ID: <40e66e0b05062206233eea1439@mail.gmail.com>

On 6/22/05, Petr Pikal <petr.pikal at precheza.cz> wrote:
> Dear all
> 
> I am struggling with nlme and error message. Even going through
> Pinheiro, Bates nlme book did not gave me a clue how to avoid
> this.
> 
> fit <- nlme(ce ~ fi1 / ((1+exp(fi2-fi3*tepl))^(1/fi4)), data =
> temp1na.gr,
> start = c(fi1=30, fi2=-100, fi3=-.05, fi4=40),
> fixed = fi1+fi2+fi3+fi4~1,
> random = pdDiag(fi2+fi4~1),
> groups = ~spol.f)
> 
> gives
> 
> Error in chol((value + t(value))/2) : the leading minor of order 1 is
> not positive definite
> 
> Is this error due to lack of experimental points?
> Here you have one typical part of my data. It is for level spol.f =
> "3/11".
> 
> tepl    ce
> 800     28.87
> 800     29.35
> 825     29
> 850     28.73
> 875     26.83
> 900     24.07
> 
> I have 1-5 points for each level (2 levels with 5 points, 1 level with
> 4 points, several levels with 2 and 3 points and few with only one
> point.
> 
> Fitting this model to each level separately led to several sets of
> coeficients fi1-fi4 and the separate fits were quite OK.
> 
> Please give me a hint what can be the cause for this error message
> and how I shall organize my data to avoid this. (Lack of
> experimental points is also an answer as I can do some subsequent
> measurement.

The first thing to do is to plot the data for each level of spol.f and
see if it is reasonable that you would be able to estimate four
parameters from such a curve.

Then try setting verbose = TRUE, control = list(msVerbose = TRUE) in
your call to nlme to see how the parameters are being changed during
the iterations.



From ligges at statistik.uni-dortmund.de  Wed Jun 22 15:30:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 22 Jun 2005 15:30:57 +0200
Subject: [R] A polar.plot BUG in plotrix 1.3.3 ?
In-Reply-To: <42B95BB6.4090507@vedur.is>
References: <42B95BB6.4090507@vedur.is>
Message-ID: <42B96811.1040905@statistik.uni-dortmund.de>

Halldor Bj??rnsson wrote:

> Hi,
> 
> I just updated to R-2.1.1 and updated packages acordingly
> 
> However, after the update, routines that use polar.plot
> did not function as correctly.
> 
> In plotrix 1.3.3 the polar.plot function does scale label.pos
> to radians prior to calling radial.plot
> 
> Hence, the command
> polar.plot(c(5,10,5,0),c(-10,0,10,20),rp.type='P',
>      labels=c("A","N","V","S"),label.pos=c(0,90,180,270))
> 
> produces absurd compass lines but the correct results are obtained if 
> label.pos is scaled with pi/180 :
> 
> polar.plot(c(5,10,5,0),c(-10,0,10,20),rp.type='P',
>      labels=c("A","N","V","S"),label.pos=c(0,90,180,270)*pi/180)
> 
> I have attatched the polar.plot function from the two different versions 
> of the package.
> 
> It seems that if not missing  then label.pos is not changed at all in 
> version 1.3.3
> 
> So, a feature or a bug?
> 
> Sincerely,

Obviously this is a question for the author and maintainer of the 
package (in CC).

Uwe Ligges



From Morten.Sickel at nrpa.no  Wed Jun 22 15:30:54 2005
From: Morten.Sickel at nrpa.no (Morten Sickel)
Date: Wed, 22 Jun 2005 15:30:54 +0200
Subject: [R] Subsetting across a frame for plotting
Message-ID: <2326C830ADA651438DC694248E5FEF6026F56D@mailix.NRPA.LOCAL>

 

Fra: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] P?? vegne av Liaw, Andy

>You might save yourself some headaches by turning it into a matrix instead, since all the columns are either integer or numeric:
>tonedata <- data.matrix(tonedata)

>Data frames are really lists, so even when you get a one-row subset, it's still a one-row data frame.  You can use unlist() to turn 
>that into a vector.

>Andy

Great, Andy, thanks! unlist was what I was looking for. 

Morten



From wildscop at yahoo.com  Wed Jun 22 15:36:47 2005
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Wed, 22 Jun 2005 06:36:47 -0700 (PDT)
Subject: [R] r programming help
Message-ID: <20050622133647.59278.qmail@web52611.mail.yahoo.com>

Dear list,

Is there anyway i can make the following formula short
by r-programming?

CYCLE.n<-c(NA,
WET[1]*DRY[1],
WET[1]*DRY[2]+WET[2]*DRY[1],
WET[1]*DRY[3]+WET[2]*DRY[2]+WET[3]*DRY[1],
WET[1]*DRY[4]+WET[2]*DRY[3]+WET[3]*DRY[2]+WET[4]*DRY[1],
WET[1]*DRY[5]+WET[2]*DRY[4]+WET[3]*DRY[3]+WET[4]*DRY[2]+WET[5]*DRY[1],
WET[1]*DRY[6]+WET[2]*DRY[5]+WET[3]*DRY[4]+WET[4]*DRY[3]+WET[5]*DRY[2]+WET[6]*DRY[1],
WET[1]*DRY[7]+WET[2]*DRY[6]+WET[3]*DRY[5]+WET[4]*DRY[4]+WET[5]*DRY[3]+WET[6]*DRY[2]+WET[7]*DRY[1],
WET[1]*DRY[8]+WET[2]*DRY[7]+WET[3]*DRY[6]+WET[4]*DRY[5]+WET[5]*DRY[4]+WET[6]*DRY[3]+WET[7]*DRY[2]+WET[8]*DRY[1],
WET[1]*DRY[9]+WET[2]*DRY[8]+WET[3]*DRY[7]+WET[4]*DRY[6]+WET[5]*DRY[5]+WET[6]*DRY[4]+WET[7]*DRY[3]+WET[8]*DRY[2]+WET[9]*DRY[1],
WET[1]*DRY[10]+WET[2]*DRY[9]+WET[3]*DRY[8]+WET[4]*DRY[7]+WET[5]*DRY[6]+WET[6]*DRY[5]+WET[7]*DRY[4]+WET[8]*DRY[3]+WET[9]*DRY[2]+WET[10]*DRY[1],
WET[1]*DRY[11]+WET[2]*DRY[10]+WET[3]*DRY[9]+WET[4]*DRY[8]+WET[5]*DRY[7]+WET[6]*DRY[6]+WET[7]*DRY[5]+WET[8]*DRY[4]+WET[9]*DRY[3]+WET[10]*DRY[2]+WET[11]*DRY[1],
WET[1]*DRY[12]+WET[2]*DRY[11]+WET[3]*DRY[10]+WET[4]*DRY[9]+WET[5]*DRY[8]+WET[6]*DRY[7]+WET[7]*DRY[6]+WET[8]*DRY[5]+WET[9]*DRY[4]+WET[10]*DRY[3]+WET[11]*DRY[2]+WET[12]*DRY[1],
WET[1]*DRY[13]+WET[2]*DRY[12]+WET[3]*DRY[11]+WET[4]*DRY[10]+WET[5]*DRY[9]+WET[6]*DRY[8]+WET[7]*DRY[7]+WET[8]*DRY[6]+WET[9]*DRY[5]+WET[10]*DRY[4]+WET[11]*DRY[3]+WET[12]*DRY[2]+WET[13]*DRY[1],
WET[1]*DRY[14]+WET[2]*DRY[13]+WET[3]*DRY[12]+WET[4]*DRY[11]+WET[5]*DRY[10]+WET[6]*DRY[9]+WET[7]*DRY[8]+WET[8]*DRY[7]+WET[9]*DRY[6]+WET[10]*DRY[5]+WET[11]*DRY[4]+WET[12]*DRY[3]+WET[13]*DRY[2]+WET[14]*DRY[1],
WET[1]*DRY[15]+WET[2]*DRY[14]+WET[3]*DRY[13]+WET[4]*DRY[12]+WET[5]*DRY[11]+WET[6]*DRY[10]+WET[7]*DRY[9]+WET[8]*DRY[8]+WET[9]*DRY[7]+WET[10]*DRY[6]+WET[11]*DRY[5]+WET[12]*DRY[4]+WET[13]*DRY[3]+WET[14]*DRY[2]+WET[15]*DRY[1],
WET[1]*DRY[16]+WET[2]*DRY[15]+WET[3]*DRY[14]+WET[4]*DRY[13]+WET[5]*DRY[12]+WET[6]*DRY[11]+WET[7]*DRY[10]+WET[8]*DRY[9]+WET[9]*DRY[8]+WET[10]*DRY[7]+WET[11]*DRY[6]+WET[12]*DRY[5]+WET[13]*DRY[4]+WET[14]*DRY[3]+WET[15]*DRY[2]+WET[16]*DRY[1],
WET[1]*DRY[17]+WET[2]*DRY[16]+WET[3]*DRY[15]+WET[4]*DRY[14]+WET[5]*DRY[13]+WET[6]*DRY[12]+WET[7]*DRY[11]+WET[8]*DRY[10]+WET[9]*DRY[9]+WET[10]*DRY[8]+WET[11]*DRY[7]+WET[12]*DRY[6]+WET[13]*DRY[5]+WET[14]*DRY[4]+WET[15]*DRY[3]+WET[16]*DRY[2]+WET[17]*DRY[1],
WET[1]*DRY[18]+WET[2]*DRY[17]+WET[3]*DRY[16]+WET[4]*DRY[15]+WET[5]*DRY[14]+WET[6]*DRY[13]+WET[7]*DRY[12]+WET[8]*DRY[11]+WET[9]*DRY[10]+WET[10]*DRY[9]+WET[11]*DRY[8]+WET[12]*DRY[7]+WET[13]*DRY[6]+WET[14]*DRY[5]+WET[15]*DRY[4]+WET[16]*DRY[3]+WET[17]*DRY[2]+WET[18]*DRY[1],
WET[1]*DRY[19]+WET[2]*DRY[18]+WET[3]*DRY[17]+WET[4]*DRY[16]+WET[5]*DRY[15]+WET[6]*DRY[15]+WET[7]*DRY[13]+WET[8]*DRY[12]+WET[9]*DRY[11]+WET[10]*DRY[10]+WET[11]*DRY[9]+WET[12]*DRY[8]+WET[13]*DRY[7]+WET[14]*DRY[6]+WET[15]*DRY[5]+WET[16]*DRY[4]+WET[17]*DRY[3]+WET[18]*DRY[2]+WET[19]*DRY[1],
WET[1]*DRY[20]+WET[2]*DRY[19]+WET[3]*DRY[18]+WET[4]*DRY[17]+WET[5]*DRY[16]+WET[6]*DRY[15]+WET[7]*DRY[14]+WET[8]*DRY[13]+WET[9]*DRY[12]+WET[10]*DRY[11]+WET[11]*DRY[10]+WET[12]*DRY[9]+WET[13]*DRY[8]+WET[14]*DRY[7]+WET[15]*DRY[6]+WET[16]*DRY[5]+WET[17]*DRY[4]+WET[18]*DRY[3]+WET[19]*DRY[2]+WET[20]*DRY[1],
WET[1]*DRY[21]+WET[2]*DRY[20]+WET[3]*DRY[19]+WET[4]*DRY[18]+WET[5]*DRY[17]+WET[6]*DRY[16]+WET[7]*DRY[15]+WET[8]*DRY[14]+WET[9]*DRY[13]+WET[10]*DRY[12]+WET[11]*DRY[11]+WET[12]*DRY[10]+WET[13]*DRY[9]+WET[14]*DRY[8]+WET[15]*DRY[7]+WET[16]*DRY[6]+WET[17]*DRY[5]+WET[18]*DRY[4]+WET[19]*DRY[3]+WET[20]*DRY[2]+WET[21]*DRY[1],
WET[1]*DRY[22]+WET[2]*DRY[21]+WET[3]*DRY[20]+WET[4]*DRY[19]+WET[5]*DRY[18]+WET[6]*DRY[17]+WET[7]*DRY[16]+WET[8]*DRY[15]+WET[9]*DRY[14]+WET[10]*DRY[13]+WET[11]*DRY[12]+WET[12]*DRY[11]+WET[13]*DRY[10]+WET[14]*DRY[9]+WET[15]*DRY[8]+WET[16]*DRY[7]+WET[17]*DRY[6]+WET[18]*DRY[5]+WET[19]*DRY[4]+WET[20]*DRY[3]+WET[21]*DRY[2]+WET[22]*DRY[1],
WET[1]*DRY[23]+WET[2]*DRY[22]+WET[3]*DRY[21]+WET[4]*DRY[20]+WET[5]*DRY[19]+WET[6]*DRY[18]+WET[7]*DRY[17]+WET[8]*DRY[16]+WET[9]*DRY[15]+WET[10]*DRY[14]+WET[11]*DRY[13]+WET[12]*DRY[12]+WET[13]*DRY[11]+WET[14]*DRY[10]+WET[15]*DRY[9]+WET[16]*DRY[8]+WET[17]*DRY[7]+WET[18]*DRY[6]+WET[19]*DRY[5]+WET[20]*DRY[4]+WET[21]*DRY[3]+WET[22]*DRY[2]+WET[23]*DRY[1],
WET[1]*DRY[24]+WET[2]*DRY[23]+WET[3]*DRY[22]+WET[4]*DRY[21]+WET[5]*DRY[20]+WET[6]*DRY[19]+WET[7]*DRY[18]+WET[8]*DRY[17]+WET[9]*DRY[16]+WET[10]*DRY[15]+WET[11]*DRY[14]+WET[12]*DRY[13]+WET[13]*DRY[12]+WET[14]*DRY[11]+WET[15]*DRY[10]+WET[16]*DRY[9]+WET[17]*DRY[8]+WET[18]*DRY[7]+WET[19]*DRY[6]+WET[20]*DRY[5]+WET[21]*DRY[4]+WET[22]*DRY[3]+WET[23]*DRY[2]+WET[24]*DRY[1],
WET[1]*DRY[25]+WET[2]*DRY[24]+WET[3]*DRY[23]+WET[4]*DRY[22]+WET[5]*DRY[21]+WET[6]*DRY[20]+WET[7]*DRY[19]+WET[8]*DRY[18]+WET[9]*DRY[17]+WET[10]*DRY[16]+WET[11]*DRY[15]+WET[12]*DRY[14]+WET[13]*DRY[13]+WET[14]*DRY[12]+WET[15]*DRY[11]+WET[16]*DRY[10]+WET[17]*DRY[9]+WET[18]*DRY[8]+WET[19]*DRY[7]+WET[20]*DRY[6]+WET[21]*DRY[5]+WET[22]*DRY[4]+WET[23]*DRY[3]+WET[24]*DRY[2]+WET[25]*DRY[1],
WET[1]*DRY[26]+WET[2]*DRY[25]+WET[3]*DRY[24]+WET[4]*DRY[23]+WET[5]*DRY[22]+WET[6]*DRY[21]+WET[7]*DRY[20]+WET[8]*DRY[19]+WET[9]*DRY[18]+WET[10]*DRY[17]+WET[11]*DRY[16]+WET[12]*DRY[15]+WET[13]*DRY[14]+WET[14]*DRY[13]+WET[15]*DRY[12]+WET[16]*DRY[11]+WET[17]*DRY[10]+WET[18]*DRY[9]+WET[19]*DRY[8]+WET[20]*DRY[7]+WET[21]*DRY[6]+WET[22]*DRY[5]+WET[23]*DRY[4]+WET[24]*DRY[3]+WET[25]*DRY[2]+WET[26]*DRY[1],
WET[1]*DRY[27]+WET[2]*DRY[26]+WET[3]*DRY[25]+WET[4]*DRY[24]+WET[5]*DRY[23]+WET[6]*DRY[22]+WET[7]*DRY[21]+WET[8]*DRY[20]+WET[9]*DRY[19]+WET[10]*DRY[18]+WET[11]*DRY[17]+WET[12]*DRY[16]+WET[13]*DRY[15]+WET[14]*DRY[14]+WET[15]*DRY[13]+WET[16]*DRY[12]+WET[17]*DRY[11]+WET[18]*DRY[10]+WET[19]*DRY[9]+WET[20]*DRY[8]+WET[21]*DRY[7]+WET[22]*DRY[6]+WET[23]*DRY[5]+WET[24]*DRY[4]+WET[25]*DRY[3]+WET[26]*DRY[2]+WET[27]*DRY[1],
WET[1]*DRY[28]+WET[2]*DRY[27]+WET[3]*DRY[26]+WET[4]*DRY[25]+WET[5]*DRY[24]+WET[6]*DRY[23]+WET[7]*DRY[22]+WET[8]*DRY[21]+WET[9]*DRY[20]+WET[10]*DRY[19]+WET[11]*DRY[18]+WET[12]*DRY[17]+WET[13]*DRY[16]+WET[14]*DRY[15]+WET[15]*DRY[14]+WET[16]*DRY[13]+WET[17]*DRY[12]+WET[18]*DRY[11]+WET[19]*DRY[10]+WET[20]*DRY[9]+WET[21]*DRY[8]+WET[22]*DRY[7]+WET[23]*DRY[6]+WET[24]*DRY[5]+WET[25]*DRY[4]+WET[26]*DRY[3]+WET[27]*DRY[2]+WET[28]*DRY[1],
WET[1]*DRY[29]+WET[2]*DRY[28]+WET[3]*DRY[27]+WET[4]*DRY[26]+WET[5]*DRY[25]+WET[6]*DRY[24]+WET[7]*DRY[23]+WET[8]*DRY[22]+WET[9]*DRY[21]+WET[10]*DRY[20]+WET[11]*DRY[19]+WET[12]*DRY[18]+WET[13]*DRY[17]+WET[14]*DRY[16]+WET[15]*DRY[15]+WET[16]*DRY[14]+WET[17]*DRY[13]+WET[18]*DRY[12]+WET[19]*DRY[11]+WET[20]*DRY[10]+WET[21]*DRY[9]+WET[22]*DRY[8]+WET[23]*DRY[7]+WET[24]*DRY[6]+WET[25]*DRY[5]+WET[26]*DRY[4]+WET[27]*DRY[3]+WET[28]*DRY[2]+WET[29]*DRY[1]
)




Thank you for your time. 
PS: if the above formula can not be seen correctly,
one can see details at
http://www.angelfire.com/ab5/get5/alt.ren.txt

----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD



From christian_mora at arauco.cl  Wed Jun 22 15:37:54 2005
From: christian_mora at arauco.cl (Christian Mora)
Date: Wed, 22 Jun 2005 09:37:54 -0400
Subject: [R] Extract Coeff, Std Error, etc from gnls output
Message-ID: <OF45F23C7D.5FE4D571-ON04257028.004A895D@arauco.cl>

Dear list members;

Is there any trick to extract the coefficients along with std errors,
t-values and p-values for each beta from a gnls fit model (similar to the
results obtained using summary(lm)$coeff for linear models)?

Thanks for any hint

cm



From bld at math.umd.edu  Wed Jun 22 15:38:53 2005
From: bld at math.umd.edu (Bernard L. Dillard)
Date: Wed, 22 Jun 2005 09:38:53 -0400 (EDT)
Subject: [R] Contour Plots
Message-ID: <38355.66.92.23.42.1119447533.squirrel@66.92.23.42>

Hello all.

I'm confused a bit about contour plots.  After reading the help at
"?contour", it seems as though the contour plot is for 3D plots (x,y, and
z).  My data is in the form of grid coordinates (x,y), and I want to see a
contour plot of the data so that I can tell where most observations lie. 
My question is simple but still evasive.

Say my data is called places.  One column is X and another is Y.  Or,

                  places$X          places$Y

Coordinate 1          32               50
Coordinate 2          15               33
Coordinate 3          28               20
etc

How do I get R to do the contour plot with no third "z" coordinate?

Thanks.

-- 
Do all you can with what you have in the time you have in the place you are!

-Nkosi Johnson, 12-year old African hero



From med at aghmed.fsnet.co.uk  Wed Jun 22 15:18:11 2005
From: med at aghmed.fsnet.co.uk (Michael Dewey)
Date: Wed, 22 Jun 2005 14:18:11 +0100
Subject: [R] Problem trying to use boot and lme together
In-Reply-To: <Pine.LNX.4.61.0506212302450.25553@gannet.stats>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD99FE@DJFPOST01.djf.agrsci.dk>
	<40e66e0b05062113564139403f@mail.gmail.com>
	<Pine.LNX.4.61.0506212302450.25553@gannet.stats>
Message-ID: <6.2.1.2.0.20050622140911.02913cf0@pop.freeserve.net>

At 23:09 21/06/05, Prof Brian Ripley wrote:
>On Tue, 21 Jun 2005, Douglas Bates wrote:
>
>>On 6/21/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:

Thanks everyone for your help, more comments at the foot

>>>The problem with simulate.lme is that it only returns logL for a given 
>>>model fitted to a simulated data set  - not the simulated data set 
>>>itself (which one might have expected a function with that name to 
>>>do...). It would be nice with that functionality...
>>>S??ren
>>
>>You could add it.  You just need to create a matrix that will be large
>>enough to hold all the simulated data sets and fill a column (or row
>>if you prefer but column is probably better because of the way that
>>matrices are stored) during each iteration of the simulation and
>>remember to include that matrix in the returned object.
>
>Note: you don't need to store it: you can do the analysis at that point 
>and return the statistics you want, rather than just logL.
>
>I did say `see simulate.lme', not `use simulate.lme'.  I know nlme is no 
>longer being developed, but if it were I would be suggesting/contributing 
>a modification that allowed the user to specify an `extraction' function 
>from the fit -- quite a few pieces of bootstrap code work that way.
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595

This has been most informative for me. Given the rather stern comments in 
Pinheiro and Bates that most things do not have the reference distribution 
you might naively think I now realise that simulate.lme is more important 
than its rather cursory treatment in the book. As suggested I have looked 
at the code but although I can see broadly what each section does I lack 
the skill to modify it myself. I will have to wait for someone more gifted.

If there is to be a successor edition to Pinheiro and Bates perhaps I could 
suggest that this topic merits a bit more discussion?


Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html



From andy_liaw at merck.com  Wed Jun 22 15:52:30 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 09:52:30 -0400
Subject: [R] Contour Plots
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9DD@usctmx1106.merck.com>

I guess what you want is contours of the density over the x-y plane.  There
are a few choices that I know of:

kde2d in MASS (part of the `VR' bundle)
bkde2d in KernSmooth
sm.density in sm
locfit in locfit

Andy

> From: Bernard L. Dillard
> 
> Hello all.
> 
> I'm confused a bit about contour plots.  After reading the help at
> "?contour", it seems as though the contour plot is for 3D 
> plots (x,y, and
> z).  My data is in the form of grid coordinates (x,y), and I 
> want to see a
> contour plot of the data so that I can tell where most 
> observations lie. 
> My question is simple but still evasive.
> 
> Say my data is called places.  One column is X and another is Y.  Or,
> 
>                   places$X          places$Y
> 
> Coordinate 1          32               50
> Coordinate 2          15               33
> Coordinate 3          28               20
> etc
> 
> How do I get R to do the contour plot with no third "z" coordinate?
> 
> Thanks.
> 
> -- 
> Do all you can with what you have in the time you have in the 
> place you are!
> 
> -Nkosi Johnson, 12-year old African hero
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From owzar001 at mc.duke.edu  Wed Jun 22 16:02:26 2005
From: owzar001 at mc.duke.edu (Kouros Owzar)
Date: Wed, 22 Jun 2005 10:02:26 -0400
Subject: [R] Kouros Owzar is ooo.
Message-ID: <OF8245276B.0F524736-ON85257028.004D20CE-85257028.004D20CE@notes.duke.edu>


I will be out of the office starting  06/21/2005 and will not return until
06/29/2005.



From p.dalgaard at biostat.ku.dk  Wed Jun 22 16:03:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Jun 2005 16:03:56 +0200
Subject: [R] r programming help
In-Reply-To: <20050622133647.59278.qmail@web52611.mail.yahoo.com>
References: <20050622133647.59278.qmail@web52611.mail.yahoo.com>
Message-ID: <x2fyvalc8j.fsf@turmalin.kubism.ku.dk>

Mohammad Ehsanul Karim <wildscop at yahoo.com> writes:

> Dear list,
> 
> Is there anyway i can make the following formula short
> by r-programming?
> 
> CYCLE.n<-c(NA,
> WET[1]*DRY[1],
> WET[1]*DRY[2]+WET[2]*DRY[1],
> WET[1]*DRY[3]+WET[2]*DRY[2]+WET[3]*DRY[1],
....

As far as I can see: 

z <- toeplitz(DRY)
z[upper.tri(z)] <- 0
c(NA, z %*% WET)

or convolve() with suitable options, padding, and/or cutting (but
beware, there could be devils in the details). convolve(WET,DRY,
type="o") gives you about twice what you need, I believe.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From dlvanbrunt at gmail.com  Wed Jun 22 16:18:52 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Wed, 22 Jun 2005 09:18:52 -0500
Subject: [R] (slightly off topic, but...) More of a stat design question...
In-Reply-To: <d332d3e10506211824bf6174c@mail.gmail.com>
References: <20050226195338.VXYR24950.smta03.mail.ozemail.net@there>
	<4221E24F.5080301@statistik.uni-dortmund.de>
	<42B89481.1070200@stat.auckland.ac.nz>
	<d332d3e10506211824bf6174c@mail.gmail.com>
Message-ID: <d332d3e10506220718415b8a64@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050622/6b891a53/attachment.pl

From tlumley at u.washington.edu  Wed Jun 22 16:27:45 2005
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 22 Jun 2005 07:27:45 -0700 (PDT)
Subject: [R] predict.coxph fitted values for failure times
In-Reply-To: <20050622090255.95934.qmail@web26301.mail.ukl.yahoo.com>
References: <20050622090255.95934.qmail@web26301.mail.ukl.yahoo.com>
Message-ID: <Pine.A41.4.61b.0506220723280.259056@homer12.u.washington.edu>

On Wed, 22 Jun 2005, Dan Bebber wrote:

> I would like to extract predicted failure times from a
> coxph model in library(survival). However, none of the
> prediction options ("lp", "risk", "expected", "terms")
> seem to bear any relationship to failure time.
>
> Perhaps I am asking the wrong question, but can coxph
> provide predicted failure times?
>

It's tricky, because it depends what you mean by "predicted". It's 
typically impossible to estimate the mean survival time for given 
covariates when there is censoring.  You can use survfit() on your Cox 
model to get predicted survival curves.

 	-thomas



From navarre_sabine at yahoo.fr  Wed Jun 22 16:42:31 2005
From: navarre_sabine at yahoo.fr (Navarre Sabine)
Date: Wed, 22 Jun 2005 16:42:31 +0200 (CEST)
Subject: [R] Is it possible to get the first letter of a word?
Message-ID: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050622/db28064b/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Jun 22 16:52:01 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 22 Jun 2005 16:52:01 +0200
Subject: [R] Is it possible to get the first letter of a word?
In-Reply-To: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
References: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42B97B11.4080805@statistik.uni-dortmund.de>

Navarre Sabine wrote:

> Hi,
> I would to get the first letter of a word like:
> 
> 
>>title_cat
> 
>        TitleCat
> 1     Training
>  
> I would like T from Training!
> 
> Thnaks a lot for your help
>


substr(title_cat,1,1)

Uwe Ligges

> Sabine
> 
> 		
> ---------------------------------
> 
>  T??l??chargez le ici !  
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jun 22 16:52:29 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 10:52:29 -0400
Subject: [R] Is it possible to get the first letter of a word?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9E0@usctmx1106.merck.com>

Use substring() or substr().

Andy

> From: Navarre Sabine
> 
> Hi,
> I would to get the first letter of a word like:
> 
> > title_cat
>        TitleCat
> 1     Training
>  
> I would like T from Training!
> 
> Thnaks a lot for your help
>  
> Sabine
> 
> 		
> ---------------------------------
> 
>  T??l??chargez le ici !  
> 	[[alternative HTML version deleted]]
> 
>



From dimitris.rizopoulos at med.kuleuven.be  Wed Jun 22 16:53:28 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 22 Jun 2005 16:53:28 +0200
Subject: [R] Is it possible to get the first letter of a word?
References: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <006301c5773a$26681110$0540210a@www.domain>

you could use substr(), e.g.,

x <- c("asf", "dfds", "fdfdf dfd", "ehf rf ngfjvf", "gfg ")
substr(x, 1, 1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "Navarre Sabine" <navarre_sabine at yahoo.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 22, 2005 4:42 PM
Subject: [R] Is it possible to get the first letter of a word?


> Hi,
> I would to get the first letter of a word like:
>
>> title_cat
>       TitleCat
> 1     Training
>
> I would like T from Training!
>
> Thnaks a lot for your help
>
> Sabine
>
>
> ---------------------------------
>
> T??l??chargez le ici !
> [[alternative HTML version deleted]]
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Wed Jun 22 16:53:38 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 22 Jun 2005 10:53:38 -0400
Subject: [R] Is it possible to get the first letter of a word?
In-Reply-To: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
References: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <42B97B72.9080405@optonline.net>

?substring

Navarre Sabine wrote:
> Hi,
> I would to get the first letter of a word like:
> 
> 
>>title_cat
> 
>        TitleCat
> 1     Training
>  
> I would like T from Training!
> 
> Thnaks a lot for your help
>  
> Sabine
> 
> 		
> ---------------------------------
> 
>  T??l??chargez le ici !  
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From knoblauch at lyon.inserm.fr  Wed Jun 22 16:55:22 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Wed, 22 Jun 2005 16:55:22 +0200
Subject: [R]  Is it possible to get the first letter of a word?
Message-ID: <1119452122.42b97bdaa899c@webmail.lyon.inserm.fr>

or What about;

> strsplit("Training", split="")[[1]][1]
[1] "T"

____________________
Ken Knoblauch
Inserm U371, Cerveau et Vision
Department of Cognitive Neurosciences
18 avenue du Doyen Lepine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10
http://www.lyon.inserm.fr/371/



From MSchwartz at mn.rr.com  Wed Jun 22 16:55:35 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 22 Jun 2005 09:55:35 -0500
Subject: [R] Is it possible to get the first letter of a word?
In-Reply-To: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
References: <20050622144231.32331.qmail@web26604.mail.ukl.yahoo.com>
Message-ID: <1119452135.19120.37.camel@localhost.localdomain>

On Wed, 2005-06-22 at 16:42 +0200, Navarre Sabine wrote:
> Hi,
> I would to get the first letter of a word like:
> 
> > title_cat
>        TitleCat
> 1     Training
>  
> I would like T from Training!
> 
> Thnaks a lot for your help
>  
> Sabine


There are multiple approaches, but you need to be careful, since it
appears that your object is a factor. Thus you may need to convert to a
character vector first:

> title_cat <- factor("Training")

> substr(as.character(title_cat), 1, 1)
[1] "T"


Otherwise:

> title_cat <- "Training"

> substr(title_cat, 1, 1)
[1] "T"

See ?substr for more information.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Wed Jun 22 16:57:53 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 22 Jun 2005 16:57:53 +0200
Subject: [R] Contour Plots
In-Reply-To: <38355.66.92.23.42.1119447533.squirrel@66.92.23.42>
References: <38355.66.92.23.42.1119447533.squirrel@66.92.23.42>
Message-ID: <42B97C71.7090103@statistik.uni-dortmund.de>

Bernard L. Dillard wrote:

> Hello all.
> 
> I'm confused a bit about contour plots.  After reading the help at
> "?contour", it seems as though the contour plot is for 3D plots (x,y, and
> z).  My data is in the form of grid coordinates (x,y), and I want to see a
> contour plot of the data so that I can tell where most observations lie. 
> My question is simple but still evasive.
> 
> Say my data is called places.  One column is X and another is Y.  Or,
> 
>                   places$X          places$Y
> 
> Coordinate 1          32               50
> Coordinate 2          15               33
> Coordinate 3          28               20
> etc
> 
> How do I get R to do the contour plot with no third "z" coordinate?

For just 2 columns, a contour plot does not make much sense, I guess.
I think an image() makes much more sense in this case.

Your z matrix consists of the entries of the data.frame/matrix given 
above, the x and y vectors are the column and row numbers.


Uwe Ligges



> Thanks.
>



From v.demartino2 at virgilio.it  Wed Jun 22 17:12:53 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Wed, 22 Jun 2005 17:12:53 +0200
Subject: [R] Howto crosstable-ing......
Message-ID: <429C8F5C0002E932@ims3e.cp.tin.it>

I receive the following meteo dataset regularly, containing the average
daily temperatures (tMedia) of a certain month for 24 selected meteo-stations
(COD_WMO) whose human-readable names are in (NOME).

str(tabella)
`data.frame':	1038 obs. of  4 variables:
 $ COD_WMO: int  16045 16045 16045 16045 16045 16045 16045 16045 16045 16045
...
 $ NOME   : Factor w/ 24 levels "ALGHERO","BARI/PALESE MACCHIE",..: 22 22
22 22 22 22 22 22 22 22 ...
 $ DATE   :'POSIXct', format: chr  "2005-05-01" "2005-05-02" "2005-05-03"
"2005-05-04" ...
 $ tMedia : num  11.7 18.6 16.9 19.7 15.0 ...


Here you are a short list of it:
    COD_WMO      NOME       DATE   tMedia
505   16191 FALCONARA 2005-06-01  20.95
506   16191 FALCONARA 2005-06-02  20.15
507   16191 FALCONARA 2005-06-03  18.60
506   16191 FALCONARA 2005-06-02  20.15
507   16191 FALCONARA 2005-06-03  18.60
508   16191 FALCONARA 2005-06-04  22.30
509   16191 FALCONARA 2005-06-05     NA
510   16191 FALCONARA 2005-06-06     NA
511   16191 FALCONARA 2005-06-07  18.20
549   16206 GROSSETO 2005-06-01  20.65
550   16206 GROSSETO 2005-06-02  21.95
551   16206 GROSSETO 2005-06-03  22.25
552   16206 GROSSETO 2005-06-04  20.15
553   16206 GROSSETO 2005-06-05     NA
554   16206 GROSSETO 2005-06-06     NA
555   16206 GROSSETO 2005-06-07  22.35
.....................................................................
.....................................................................


I need to rearrange tMedia into a new dataframe whose column names are COD_WMO
 (or   NOME) and the row is DATE.

ex.
DATE            ALGHERO   BARI/PALESE ....  FALCONARA  GROSSETO ..........
2005-06-01       16.3             12.8                   17.3          
 14.0       ...........
2005-06-02       18.2               8.9                    18.0        
  17.9       ..........
...............................................................................................


I read some pieces of R-docs in the internet and run the MASS chapter 2
examples but without finding anything suitable to my purpose.

Could you please help me?

Ciao

Vittorio



From ligges at statistik.uni-dortmund.de  Wed Jun 22 17:26:32 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 22 Jun 2005 17:26:32 +0200
Subject: [R] legend
In-Reply-To: <iihhxl.stn69u@webmail.tuwien.ac.at>
References: <iihhxl.stn69u@webmail.tuwien.ac.at>
Message-ID: <42B98328.7070503@statistik.uni-dortmund.de>

Thomas Steiner wrote:

> I color some area grey with polygon() (with a red border) and then I
> want to have the dashed red border in the legend as well. How do I
> manage it?
> 
> And I want to mix (latex) expressions with text in my legend.


Both points are not that easy to solve, hence I'd like to suggest to 
write your own little function that generates the legend.

Starting at the upper left, calculating the stringheight, painting the 
(party very special) symbols, and adding the text line by line seems to 
be the most easiest solution here (which is not that nice, though.

Uwe Ligges


> Just execute my lines below and you know want I mean. Or pass by at
> http://de.wikipedia.org/wiki/Bild:GBM.png to see the picture online.
> 
> Thomas
> 
> 
> bm <- function(n=500, from=0, to=1) {
>   x=seq(from=from,to=to,length=n)
>   BM<-c(0,cumsum(rnorm(n-1,mean=0,sd=sqrt(to/n))))
>   cbind(x,BM)
> }
> gbm <- function(bm,S0=1,sigma=0.1,mu=1) {
>   gbm=S0
>   for (t in 2:length(bm[,1])) {
>     gbm[t]=S0*exp((mu-sigma^2/2)*bm[t,1]+sigma*bm[t,2])
>   }
>   cbind(bm[,1],gbm)
> }
> 
> set.seed(9826064)
> cs=c("dark green", "steelblue", "red", "yellow")
> 
> #png(filename = "GBM.png", width=1600, height=1200, pointsize = 12)
> par(bg="lightgrey")
> x=seq(from=0,to=1,length=500)
> plot(x=x, y=exp(0.7*x), type="n", xlab="Zeit", ylab="", ylim=c(1,3.5))
> polygon(x=c(x,rev(x)),
> y=c(exp(0.7*x)+0.4*sqrt(x),rev(exp(0.7*x)-0.4*sqrt(x))), col="grey",
> border=cs[3], lty="dashed")
> lines(x=x,y=exp(0.7*x), type="l", lwd=3, col=cs[1])
> lines(gbm(bm(),S0=1,mu=0.7,sigma=0.4), lwd=3, col=cs[2])
> lines(gbm(bm(),S0=1,mu=0.7,sigma=0.2), lwd=3, col=cs[3])
> lines(gbm(bm(),S0=1,mu=0.7,sigma=0.1), lwd=3, col=cs[4])
> title(main="Geometrische Brownsche Bewegung",cex.main=2.5)
> legend(x=0,y=3.5,legend=c("exp(0.7x)","mu=0.7, sigma=0.4","mu=0.7,
> sigma=0.2","mu=0.7, sigma=0.1","Standardabweichung f??r 
> sigma=0.2"),lwd=c(4,4,4,4,12),col=c(cs,"grey"),bg="transparent",cex=1.15)
> #dev.off()
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jun 22 17:27:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 22 Jun 2005 17:27:49 +0200
Subject: [R] Howto crosstable-ing......
In-Reply-To: <429C8F5C0002E932@ims3e.cp.tin.it>
References: <429C8F5C0002E932@ims3e.cp.tin.it>
Message-ID: <42B98375.4060807@statistik.uni-dortmund.de>

See ?reshape

Uwe Ligges

v.demartino2 at virgilio.it wrote:

> I receive the following meteo dataset regularly, containing the average
> daily temperatures (tMedia) of a certain month for 24 selected meteo-stations
> (COD_WMO) whose human-readable names are in (NOME).
> 
> str(tabella)
> `data.frame':	1038 obs. of  4 variables:
>  $ COD_WMO: int  16045 16045 16045 16045 16045 16045 16045 16045 16045 16045
> ...
>  $ NOME   : Factor w/ 24 levels "ALGHERO","BARI/PALESE MACCHIE",..: 22 22
> 22 22 22 22 22 22 22 22 ...
>  $ DATE   :'POSIXct', format: chr  "2005-05-01" "2005-05-02" "2005-05-03"
> "2005-05-04" ...
>  $ tMedia : num  11.7 18.6 16.9 19.7 15.0 ...
> 
> 
> Here you are a short list of it:
>     COD_WMO      NOME       DATE   tMedia
> 505   16191 FALCONARA 2005-06-01  20.95
> 506   16191 FALCONARA 2005-06-02  20.15
> 507   16191 FALCONARA 2005-06-03  18.60
> 506   16191 FALCONARA 2005-06-02  20.15
> 507   16191 FALCONARA 2005-06-03  18.60
> 508   16191 FALCONARA 2005-06-04  22.30
> 509   16191 FALCONARA 2005-06-05     NA
> 510   16191 FALCONARA 2005-06-06     NA
> 511   16191 FALCONARA 2005-06-07  18.20
> 549   16206 GROSSETO 2005-06-01  20.65
> 550   16206 GROSSETO 2005-06-02  21.95
> 551   16206 GROSSETO 2005-06-03  22.25
> 552   16206 GROSSETO 2005-06-04  20.15
> 553   16206 GROSSETO 2005-06-05     NA
> 554   16206 GROSSETO 2005-06-06     NA
> 555   16206 GROSSETO 2005-06-07  22.35
> .....................................................................
> .....................................................................
> 
> 
> I need to rearrange tMedia into a new dataframe whose column names are COD_WMO
>  (or   NOME) and the row is DATE.
> 
> ex.
> DATE            ALGHERO   BARI/PALESE ....  FALCONARA  GROSSETO ..........
> 2005-06-01       16.3             12.8                   17.3          
>  14.0       ...........
> 2005-06-02       18.2               8.9                    18.0        
>   17.9       ..........
> ...............................................................................................
> 
> 
> I read some pieces of R-docs in the internet and run the MASS chapter 2
> examples but without finding anything suitable to my purpose.
> 
> Could you please help me?
> 
> Ciao
> 
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Wed Jun 22 17:30:06 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 22 Jun 2005 10:30:06 -0500
Subject: [R] chisq test and fisher exact test
Message-ID: <cdf817830506220830e9ce363@mail.gmail.com>

Hi,
I have a text mining project and currently I am working on feature
generation/selection part.
My plan is selecting a set of words or word combinations which have
better discriminant capability than other words in telling the group
id's (2 classes in this case) for a dataset which has 2,000,000
documents.

One approach is using "contrast-set association rule mining" while the
other is using chisqr or fisher exact test.

An example which has 3 contingency tables for 3 words as followed
(word coded by number):
> tab[,,1:3]
, , 1

      [,1]    [,2]
[1,] 11266 2151526
[2,]   125   31734

, , 2

      [,1]    [,2]
[1,] 43571 2119221
[2,]    52   31807

, , 3

     [,1]    [,2]
[1,]  427 2162365
[2,]    5   31854


I have some questions on this:
1. What's the thumb of rule to use chisq test instead of Fisher exact
test. I have a  vague memory which said for each cell, the count needs
to be over 50 if chisq instead of fisher exact test is going to be
used. In the case of word 3,  I think I should use fisher test.
However, running chisq like below is fine:
> tab[,,3]
     [,1]    [,2]
[1,]  427 2162365
[2,]    5   31854
> chisq.test(tab[,,3])

        Pearson's Chi-squared test with Yates' continuity correction

data:  tab[, , 3]
X-squared = 0.0963, df = 1, p-value = 0.7564

but running on the whole set of words (including 14240 words) has the
following warnings:
> p.chisq<-as.double(lapply(1:N, function(i) chisq.test(tab[,,i])$p.value))
There were 50 or more warnings (use warnings() to see the first 50)
> warnings()
Warning messages:
1: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
2: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
3: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
4: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])


2. So, my second question is, is this warning b/c I am against the
assumption of using chisq. But why Word 3 is fine? How to trace the
warning to see which word caused this warning?

3. My result looks like this (after some mapping treating from number
id to word and some words are stemmed here, like ACCID is accident):
 > of[1:50,]
      map...2.      p.fisher
21       ACCID  0.000000e+00
30          CD  0.000000e+00
67        ROCK  0.000000e+00
104      CRACK  0.000000e+00
111       CHIP  0.000000e+00
179      GLASS  0.000000e+00
84        BACK 4.199878e-291
395   DRIVEABL 5.335989e-287
60         CAP 9.405235e-285
262 WINDSHIELD 2.691641e-254
13          IV 3.905186e-245
110         HZ 2.819713e-210
11        CAMP 9.086768e-207
2      SHATTER 5.273994e-202
297        ALP 1.678521e-177
162        BED 1.822031e-173
249        BCD 1.398391e-160
493       RACK 4.178617e-156
59        CAUS 7.539031e-147

3.1 question: Should I use two-sided test instead of one-sided for
fisher test? I read some material which suggests using two-sided.

3.2 A big question: Even though the result looks very promising since
this is case of classiying fraud cases and the words selected by this
approach make sense. However, I think p-values here just indicate the
strength to reject null hypothesis, not the strength of association
between word and class of document. So, what kind of statistics I
should use here to evaluate the strength of association? odds ratio?

Any suggestions are welcome!

Thanks!
-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From andy_liaw at merck.com  Wed Jun 22 17:31:40 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 11:31:40 -0400
Subject: [R] Howto crosstable-ing......
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9E2@usctmx1106.merck.com>

Looks to me like you want something like reshape()...

Andy

> From: v.demartino2 at virgilio.it
> 
> I receive the following meteo dataset regularly, containing 
> the average
> daily temperatures (tMedia) of a certain month for 24 
> selected meteo-stations
> (COD_WMO) whose human-readable names are in (NOME).
> 
> str(tabella)
> `data.frame':	1038 obs. of  4 variables:
>  $ COD_WMO: int  16045 16045 16045 16045 16045 16045 16045 
> 16045 16045 16045
> ...
>  $ NOME   : Factor w/ 24 levels "ALGHERO","BARI/PALESE 
> MACCHIE",..: 22 22
> 22 22 22 22 22 22 22 22 ...
>  $ DATE   :'POSIXct', format: chr  "2005-05-01" "2005-05-02" 
> "2005-05-03"
> "2005-05-04" ...
>  $ tMedia : num  11.7 18.6 16.9 19.7 15.0 ...
> 
> 
> Here you are a short list of it:
>     COD_WMO      NOME       DATE   tMedia
> 505   16191 FALCONARA 2005-06-01  20.95
> 506   16191 FALCONARA 2005-06-02  20.15
> 507   16191 FALCONARA 2005-06-03  18.60
> 506   16191 FALCONARA 2005-06-02  20.15
> 507   16191 FALCONARA 2005-06-03  18.60
> 508   16191 FALCONARA 2005-06-04  22.30
> 509   16191 FALCONARA 2005-06-05     NA
> 510   16191 FALCONARA 2005-06-06     NA
> 511   16191 FALCONARA 2005-06-07  18.20
> 549   16206 GROSSETO 2005-06-01  20.65
> 550   16206 GROSSETO 2005-06-02  21.95
> 551   16206 GROSSETO 2005-06-03  22.25
> 552   16206 GROSSETO 2005-06-04  20.15
> 553   16206 GROSSETO 2005-06-05     NA
> 554   16206 GROSSETO 2005-06-06     NA
> 555   16206 GROSSETO 2005-06-07  22.35
> .....................................................................
> .....................................................................
> 
> 
> I need to rearrange tMedia into a new dataframe whose column 
> names are COD_WMO
>  (or   NOME) and the row is DATE.
> 
> ex.
> DATE            ALGHERO   BARI/PALESE ....  FALCONARA  
> GROSSETO ..........
> 2005-06-01       16.3             12.8                   17.3 
>          
>  14.0       ...........
> 2005-06-02       18.2               8.9                    
> 18.0        
>   17.9       ..........
> ..............................................................
> .................................
> 
> 
> I read some pieces of R-docs in the internet and run the MASS 
> chapter 2
> examples but without finding anything suitable to my purpose.
> 
> Could you please help me?
> 
> Ciao
> 
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From s-plus at wiwi.uni-bielefeld.de  Wed Jun 22 17:53:03 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 22 Jun 2005 17:53:03 +0200
Subject: [R] r programming help
References: <20050622133647.59278.qmail@web52611.mail.yahoo.com>
Message-ID: <42B9895F.1020000@wiwi.uni-bielefeld.de>

Try:

 > DRY<-c(2,5,3,7,11)
 > WET<-(1:5)*10
 > print(filter(c(rep(0,length(WET)),DRY),WET))

Time Series:
Start = 1
End = 10
Frequency = 1
[1]  NA  NA   0  20  90 190 360 640  NA  NA

 > CYCLE.n<-c(NA,
 > WET[1]*DRY[1],
 > WET[1]*DRY[2]+WET[2]*DRY[1],
 > WET[1]*DRY[3]+WET[2]*DRY[2]+WET[3]*DRY[1],
 > WET[1]*DRY[4]+WET[2]*DRY[3]+WET[3]*DRY[2]+WET[4]*DRY[1],
 > WET[1]*DRY[5]+WET[2]*DRY[4]+WET[3]*DRY[3]+WET[4]*DRY[2]+WET[5]*DRY[1])
 > print(CYCLE.n)
[1]  NA  20  90 190 360 640

Peter Wolf

Mohammad Ehsanul Karim wrote:

>Dear list,
>
>Is there anyway i can make the following formula short
>by r-programming?
>
>CYCLE.n<-c(NA,
>WET[1]*DRY[1],
>WET[1]*DRY[2]+WET[2]*DRY[1],
>WET[1]*DRY[3]+WET[2]*DRY[2]+WET[3]*DRY[1],
>WET[1]*DRY[4]+WET[2]*DRY[3]+WET[3]*DRY[2]+WET[4]*DRY[1],
>WET[1]*DRY[5]+WET[2]*DRY[4]+WET[3]*DRY[3]+WET[4]*DRY[2]+WET[5]*DRY[1],
>WET[1]*DRY[6]+WET[2]*DRY[5]+WET[3]*DRY[4]+WET[4]*DRY[3]+WET[5]*DRY[2]+WET[6]*DRY[1],
>WET[1]*DRY[7]+WET[2]*DRY[6]+WET[3]*DRY[5]+WET[4]*DRY[4]+WET[5]*DRY[3]+WET[6]*DRY[2]+WET[7]*DRY[1],
>WET[1]*DRY[8]+WET[2]*DRY[7]+WET[3]*DRY[6]+WET[4]*DRY[5]+WET[5]*DRY[4]+WET[6]*DRY[3]+WET[7]*DRY[2]+WET[8]*DRY[1],
>WET[1]*DRY[9]+WET[2]*DRY[8]+WET[3]*DRY[7]+WET[4]*DRY[6]+WET[5]*DRY[5]+WET[6]*DRY[4]+WET[7]*DRY[3]+WET[8]*DRY[2]+WET[9]*DRY[1],
>WET[1]*DRY[10]+WET[2]*DRY[9]+WET[3]*DRY[8]+WET[4]*DRY[7]+WET[5]*DRY[6]+WET[6]*DRY[5]+WET[7]*DRY[4]+WET[8]*DRY[3]+WET[9]*DRY[2]+WET[10]*DRY[1],
>WET[1]*DRY[11]+WET[2]*DRY[10]+WET[3]*DRY[9]+WET[4]*DRY[8]+WET[5]*DRY[7]+WET[6]*DRY[6]+WET[7]*DRY[5]+WET[8]*DRY[4]+WET[9]*DRY[3]+WET[10]*DRY[2]+WET[11]*DRY[1],
>WET[1]*DRY[12]+WET[2]*DRY[11]+WET[3]*DRY[10]+WET[4]*DRY[9]+WET[5]*DRY[8]+WET[6]*DRY[7]+WET[7]*DRY[6]+WET[8]*DRY[5]+WET[9]*DRY[4]+WET[10]*DRY[3]+WET[11]*DRY[2]+WET[12]*DRY[1],
>WET[1]*DRY[13]+WET[2]*DRY[12]+WET[3]*DRY[11]+WET[4]*DRY[10]+WET[5]*DRY[9]+WET[6]*DRY[8]+WET[7]*DRY[7]+WET[8]*DRY[6]+WET[9]*DRY[5]+WET[10]*DRY[4]+WET[11]*DRY[3]+WET[12]*DRY[2]+WET[13]*DRY[1],
>WET[1]*DRY[14]+WET[2]*DRY[13]+WET[3]*DRY[12]+WET[4]*DRY[11]+WET[5]*DRY[10]+WET[6]*DRY[9]+WET[7]*DRY[8]+WET[8]*DRY[7]+WET[9]*DRY[6]+WET[10]*DRY[5]+WET[11]*DRY[4]+WET[12]*DRY[3]+WET[13]*DRY[2]+WET[14]*DRY[1],
>WET[1]*DRY[15]+WET[2]*DRY[14]+WET[3]*DRY[13]+WET[4]*DRY[12]+WET[5]*DRY[11]+WET[6]*DRY[10]+WET[7]*DRY[9]+WET[8]*DRY[8]+WET[9]*DRY[7]+WET[10]*DRY[6]+WET[11]*DRY[5]+WET[12]*DRY[4]+WET[13]*DRY[3]+WET[14]*DRY[2]+WET[15]*DRY[1],
>WET[1]*DRY[16]+WET[2]*DRY[15]+WET[3]*DRY[14]+WET[4]*DRY[13]+WET[5]*DRY[12]+WET[6]*DRY[11]+WET[7]*DRY[10]+WET[8]*DRY[9]+WET[9]*DRY[8]+WET[10]*DRY[7]+WET[11]*DRY[6]+WET[12]*DRY[5]+WET[13]*DRY[4]+WET[14]*DRY[3]+WET[15]*DRY[2]+WET[16]*DRY[1],
>WET[1]*DRY[17]+WET[2]*DRY[16]+WET[3]*DRY[15]+WET[4]*DRY[14]+WET[5]*DRY[13]+WET[6]*DRY[12]+WET[7]*DRY[11]+WET[8]*DRY[10]+WET[9]*DRY[9]+WET[10]*DRY[8]+WET[11]*DRY[7]+WET[12]*DRY[6]+WET[13]*DRY[5]+WET[14]*DRY[4]+WET[15]*DRY[3]+WET[16]*DRY[2]+WET[17]*DRY[1],
>WET[1]*DRY[18]+WET[2]*DRY[17]+WET[3]*DRY[16]+WET[4]*DRY[15]+WET[5]*DRY[14]+WET[6]*DRY[13]+WET[7]*DRY[12]+WET[8]*DRY[11]+WET[9]*DRY[10]+WET[10]*DRY[9]+WET[11]*DRY[8]+WET[12]*DRY[7]+WET[13]*DRY[6]+WET[14]*DRY[5]+WET[15]*DRY[4]+WET[16]*DRY[3]+WET[17]*DRY[2]+WET[18]*DRY[1],
>WET[1]*DRY[19]+WET[2]*DRY[18]+WET[3]*DRY[17]+WET[4]*DRY[16]+WET[5]*DRY[15]+WET[6]*DRY[15]+WET[7]*DRY[13]+WET[8]*DRY[12]+WET[9]*DRY[11]+WET[10]*DRY[10]+WET[11]*DRY[9]+WET[12]*DRY[8]+WET[13]*DRY[7]+WET[14]*DRY[6]+WET[15]*DRY[5]+WET[16]*DRY[4]+WET[17]*DRY[3]+WET[18]*DRY[2]+WET[19]*DRY[1],
>WET[1]*DRY[20]+WET[2]*DRY[19]+WET[3]*DRY[18]+WET[4]*DRY[17]+WET[5]*DRY[16]+WET[6]*DRY[15]+WET[7]*DRY[14]+WET[8]*DRY[13]+WET[9]*DRY[12]+WET[10]*DRY[11]+WET[11]*DRY[10]+WET[12]*DRY[9]+WET[13]*DRY[8]+WET[14]*DRY[7]+WET[15]*DRY[6]+WET[16]*DRY[5]+WET[17]*DRY[4]+WET[18]*DRY[3]+WET[19]*DRY[2]+WET[20]*DRY[1],
>WET[1]*DRY[21]+WET[2]*DRY[20]+WET[3]*DRY[19]+WET[4]*DRY[18]+WET[5]*DRY[17]+WET[6]*DRY[16]+WET[7]*DRY[15]+WET[8]*DRY[14]+WET[9]*DRY[13]+WET[10]*DRY[12]+WET[11]*DRY[11]+WET[12]*DRY[10]+WET[13]*DRY[9]+WET[14]*DRY[8]+WET[15]*DRY[7]+WET[16]*DRY[6]+WET[17]*DRY[5]+WET[18]*DRY[4]+WET[19]*DRY[3]+WET[20]*DRY[2]+WET[21]*DRY[1],
>WET[1]*DRY[22]+WET[2]*DRY[21]+WET[3]*DRY[20]+WET[4]*DRY[19]+WET[5]*DRY[18]+WET[6]*DRY[17]+WET[7]*DRY[16]+WET[8]*DRY[15]+WET[9]*DRY[14]+WET[10]*DRY[13]+WET[11]*DRY[12]+WET[12]*DRY[11]+WET[13]*DRY[10]+WET[14]*DRY[9]+WET[15]*DRY[8]+WET[16]*DRY[7]+WET[17]*DRY[6]+WET[18]*DRY[5]+WET[19]*DRY[4]+WET[20]*DRY[3]+WET[21]*DRY[2]+WET[22]*DRY[1],
>WET[1]*DRY[23]+WET[2]*DRY[22]+WET[3]*DRY[21]+WET[4]*DRY[20]+WET[5]*DRY[19]+WET[6]*DRY[18]+WET[7]*DRY[17]+WET[8]*DRY[16]+WET[9]*DRY[15]+WET[10]*DRY[14]+WET[11]*DRY[13]+WET[12]*DRY[12]+WET[13]*DRY[11]+WET[14]*DRY[10]+WET[15]*DRY[9]+WET[16]*DRY[8]+WET[17]*DRY[7]+WET[18]*DRY[6]+WET[19]*DRY[5]+WET[20]*DRY[4]+WET[21]*DRY[3]+WET[22]*DRY[2]+WET[23]*DRY[1],
>WET[1]*DRY[24]+WET[2]*DRY[23]+WET[3]*DRY[22]+WET[4]*DRY[21]+WET[5]*DRY[20]+WET[6]*DRY[19]+WET[7]*DRY[18]+WET[8]*DRY[17]+WET[9]*DRY[16]+WET[10]*DRY[15]+WET[11]*DRY[14]+WET[12]*DRY[13]+WET[13]*DRY[12]+WET[14]*DRY[11]+WET[15]*DRY[10]+WET[16]*DRY[9]+WET[17]*DRY[8]+WET[18]*DRY[7]+WET[19]*DRY[6]+WET[20]*DRY[5]+WET[21]*DRY[4]+WET[22]*DRY[3]+WET[23]*DRY[2]+WET[24]*DRY[1],
>WET[1]*DRY[25]+WET[2]*DRY[24]+WET[3]*DRY[23]+WET[4]*DRY[22]+WET[5]*DRY[21]+WET[6]*DRY[20]+WET[7]*DRY[19]+WET[8]*DRY[18]+WET[9]*DRY[17]+WET[10]*DRY[16]+WET[11]*DRY[15]+WET[12]*DRY[14]+WET[13]*DRY[13]+WET[14]*DRY[12]+WET[15]*DRY[11]+WET[16]*DRY[10]+WET[17]*DRY[9]+WET[18]*DRY[8]+WET[19]*DRY[7]+WET[20]*DRY[6]+WET[21]*DRY[5]+WET[22]*DRY[4]+WET[23]*DRY[3]+WET[24]*DRY[2]+WET[25]*DRY[1],
>WET[1]*DRY[26]+WET[2]*DRY[25]+WET[3]*DRY[24]+WET[4]*DRY[23]+WET[5]*DRY[22]+WET[6]*DRY[21]+WET[7]*DRY[20]+WET[8]*DRY[19]+WET[9]*DRY[18]+WET[10]*DRY[17]+WET[11]*DRY[16]+WET[12]*DRY[15]+WET[13]*DRY[14]+WET[14]*DRY[13]+WET[15]*DRY[12]+WET[16]*DRY[11]+WET[17]*DRY[10]+WET[18]*DRY[9]+WET[19]*DRY[8]+WET[20]*DRY[7]+WET[21]*DRY[6]+WET[22]*DRY[5]+WET[23]*DRY[4]+WET[24]*DRY[3]+WET[25]*DRY[2]+WET[26]*DRY[1],
>WET[1]*DRY[27]+WET[2]*DRY[26]+WET[3]*DRY[25]+WET[4]*DRY[24]+WET[5]*DRY[23]+WET[6]*DRY[22]+WET[7]*DRY[21]+WET[8]*DRY[20]+WET[9]*DRY[19]+WET[10]*DRY[18]+WET[11]*DRY[17]+WET[12]*DRY[16]+WET[13]*DRY[15]+WET[14]*DRY[14]+WET[15]*DRY[13]+WET[16]*DRY[12]+WET[17]*DRY[11]+WET[18]*DRY[10]+WET[19]*DRY[9]+WET[20]*DRY[8]+WET[21]*DRY[7]+WET[22]*DRY[6]+WET[23]*DRY[5]+WET[24]*DRY[4]+WET[25]*DRY[3]+WET[26]*DRY[2]+WET[27]*DRY[1],
>WET[1]*DRY[28]+WET[2]*DRY[27]+WET[3]*DRY[26]+WET[4]*DRY[25]+WET[5]*DRY[24]+WET[6]*DRY[23]+WET[7]*DRY[22]+WET[8]*DRY[21]+WET[9]*DRY[20]+WET[10]*DRY[19]+WET[11]*DRY[18]+WET[12]*DRY[17]+WET[13]*DRY[16]+WET[14]*DRY[15]+WET[15]*DRY[14]+WET[16]*DRY[13]+WET[17]*DRY[12]+WET[18]*DRY[11]+WET[19]*DRY[10]+WET[20]*DRY[9]+WET[21]*DRY[8]+WET[22]*DRY[7]+WET[23]*DRY[6]+WET[24]*DRY[5]+WET[25]*DRY[4]+WET[26]*DRY[3]+WET[27]*DRY[2]+WET[28]*DRY[1],
>WET[1]*DRY[29]+WET[2]*DRY[28]+WET[3]*DRY[27]+WET[4]*DRY[26]+WET[5]*DRY[25]+WET[6]*DRY[24]+WET[7]*DRY[23]+WET[8]*DRY[22]+WET[9]*DRY[21]+WET[10]*DRY[20]+WET[11]*DRY[19]+WET[12]*DRY[18]+WET[13]*DRY[17]+WET[14]*DRY[16]+WET[15]*DRY[15]+WET[16]*DRY[14]+WET[17]*DRY[13]+WET[18]*DRY[12]+WET[19]*DRY[11]+WET[20]*DRY[10]+WET[21]*DRY[9]+WET[22]*DRY[8]+WET[23]*DRY[7]+WET[24]*DRY[6]+WET[25]*DRY[5]+WET[26]*DRY[4]+WET[27]*DRY[3]+WET[28]*DRY[2]+WET[29]*DRY[1]
>)
>
>
>
>
>Thank you for your time. 
>PS: if the above formula can not be seen correctly,
>one can see details at
>http://www.angelfire.com/ab5/get5/alt.ren.txt
>
>----------------------------------
>
>Mohammad Ehsanul Karim 
>
>Web: http://snipurl.com/ehsan 
>ISRT, University of Dhaka, BD
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at mn.rr.com  Wed Jun 22 17:55:04 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 22 Jun 2005 10:55:04 -0500
Subject: [R] Howto crosstable-ing......
In-Reply-To: <429C8F5C0002E932@ims3e.cp.tin.it>
References: <429C8F5C0002E932@ims3e.cp.tin.it>
Message-ID: <1119455704.19120.56.camel@localhost.localdomain>

On Wed, 2005-06-22 at 17:12 +0200, v.demartino2 at virgilio.it wrote:
> I receive the following meteo dataset regularly, containing the average
> daily temperatures (tMedia) of a certain month for 24 selected meteo-stations
> (COD_WMO) whose human-readable names are in (NOME).
> 
> str(tabella)
> `data.frame':	1038 obs. of  4 variables:
>  $ COD_WMO: int  16045 16045 16045 16045 16045 16045 16045 16045 16045 16045
> ...
>  $ NOME   : Factor w/ 24 levels "ALGHERO","BARI/PALESE MACCHIE",..: 22 22
> 22 22 22 22 22 22 22 22 ...
>  $ DATE   :'POSIXct', format: chr  "2005-05-01" "2005-05-02" "2005-05-03"
> "2005-05-04" ...
>  $ tMedia : num  11.7 18.6 16.9 19.7 15.0 ...
> 
> 
> Here you are a short list of it:
>     COD_WMO      NOME       DATE   tMedia
> 505   16191 FALCONARA 2005-06-01  20.95
> 506   16191 FALCONARA 2005-06-02  20.15
> 507   16191 FALCONARA 2005-06-03  18.60
> 506   16191 FALCONARA 2005-06-02  20.15
> 507   16191 FALCONARA 2005-06-03  18.60
> 508   16191 FALCONARA 2005-06-04  22.30
> 509   16191 FALCONARA 2005-06-05     NA
> 510   16191 FALCONARA 2005-06-06     NA
> 511   16191 FALCONARA 2005-06-07  18.20
> 549   16206 GROSSETO 2005-06-01  20.65
> 550   16206 GROSSETO 2005-06-02  21.95
> 551   16206 GROSSETO 2005-06-03  22.25
> 552   16206 GROSSETO 2005-06-04  20.15
> 553   16206 GROSSETO 2005-06-05     NA
> 554   16206 GROSSETO 2005-06-06     NA
> 555   16206 GROSSETO 2005-06-07  22.35
> .....................................................................
> .....................................................................
> 
> 
> I need to rearrange tMedia into a new dataframe whose column names are COD_WMO
>  (or   NOME) and the row is DATE.
> 
> ex.
> DATE            ALGHERO   BARI/PALESE ....  FALCONARA  GROSSETO ..........
> 2005-06-01       16.3             12.8                   17.3          
>  14.0       ...........
> 2005-06-02       18.2               8.9                    18.0        
>   17.9       ..........
> ...............................................................................................
> 
> 
> I read some pieces of R-docs in the internet and run the MASS chapter 2
> examples but without finding anything suitable to my purpose.
> 
> Could you please help me?
> 
> Ciao
> 
> Vittorio

I believe that the following will get you there, based upon your example
output above:

> reshape(tabella[, -1], idvar = "DATE", timevar = "NOME", 
          v.names = "tMedia", direction = "wide")
        DATE tMedia.FALCONARA tMedia.GROSSETO
1 2005-06-01            20.95           20.65
2 2005-06-02            20.15           21.95
3 2005-06-03            18.60           22.25
6 2005-06-04            22.30           20.15
7 2005-06-05               NA              NA
8 2005-06-06               NA              NA
9 2005-06-07            18.20           22.35

See ?reshape for more information.

HTH,

Marc Schwartz



From macq at llnl.gov  Wed Jun 22 17:56:20 2005
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 22 Jun 2005 08:56:20 -0700
Subject: [R] r programming help
In-Reply-To: <x2fyvalc8j.fsf@turmalin.kubism.ku.dk>
References: <20050622133647.59278.qmail@web52611.mail.yahoo.com>
	<x2fyvalc8j.fsf@turmalin.kubism.ku.dk>
Message-ID: <p06210201bedf37b70a2e@[128.115.153.6]>

In addition to Peter's suggestion of converting 
to a matrix operation, here is a simple solution 
using naive R programming.

Using the 7th element of CYCLE.n as an example:
     replace this:
WET[1]*DRY[6]+WET[2]*DRY[5]+WET[3]*DRY[4]+WET[4]*DRY[3]+WET[5]*DRY[2]+WET[6]*DRY[1]
    with this:
sum(WET[1:6]*DRY[6:1])

which then suggests a loop.

nin <- length(WET)
CYCLE.n <- rep(NA,nin+1)
for (i in 1:nin) CYCLE.n[i+1] <- sum(WET[1:i]*DRY[i:1])


-Don

At 4:03 PM +0200 6/22/05, Peter Dalgaard wrote:
>Mohammad Ehsanul Karim <wildscop at yahoo.com> writes:
>
>>  Dear list,
>>
>>  Is there anyway i can make the following formula short
>>  by r-programming?
>>
>>  CYCLE.n<-c(NA,
>>  WET[1]*DRY[1],
>>  WET[1]*DRY[2]+WET[2]*DRY[1],
>>  WET[1]*DRY[3]+WET[2]*DRY[2]+WET[3]*DRY[1],
>....
>
>As far as I can see:
>
>z <- toeplitz(DRY)
>z[upper.tri(z)] <- 0
>c(NA, z %*% WET)
>
>or convolve() with suitable options, padding, and/or cutting (but
>beware, there could be devils in the details). convolve(WET,DRY,
>type="o") gives you about twice what you need, I believe.
>
>
>--
>    O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
>   c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
>  (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From dmbates at gmail.com  Wed Jun 22 18:38:16 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 22 Jun 2005 11:38:16 -0500
Subject: [R] Extract Coeff, Std Error, etc from gnls output
In-Reply-To: <OF45F23C7D.5FE4D571-ON04257028.004A895D@arauco.cl>
References: <OF45F23C7D.5FE4D571-ON04257028.004A895D@arauco.cl>
Message-ID: <40e66e0b05062209382e81c7a@mail.gmail.com>

On 6/22/05, Christian Mora <christian_mora at arauco.cl> wrote:
> Dear list members;
> 
> Is there any trick to extract the coefficients along with std errors,
> t-values and p-values for each beta from a gnls fit model (similar to the
> results obtained using summary(lm)$coeff for linear models)?

The best way to get the coefficients is with the extractor function
coef.  There is no extractor for the t-values and the p-values so you
need to look at the result of str(summary(gnlsfit)) to find the name
of the component.



From uofiowa at gmail.com  Wed Jun 22 20:36:03 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Wed, 22 Jun 2005 14:36:03 -0400
Subject: [R] monitoring objects sizes
Message-ID: <3f87cc6d050622113616038e55@mail.gmail.com>

I have an R script that loops over market contracts. The script runs
well for markets with relatively small number of contracts but seg
faults when the number of contracts (loop iterations) is large.
Is there a way for me to monitor my objects and their sizes from
within the R script?
How can I get all of the objects and their sizes?



From kjetil at acelerate.com  Wed Jun 22 20:50:00 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 22 Jun 2005 14:50:00 -0400
Subject: [R] chisq test and fisher exact test
In-Reply-To: <cdf817830506220830e9ce363@mail.gmail.com>
References: <cdf817830506220830e9ce363@mail.gmail.com>
Message-ID: <42B9B2D8.3040007@acelerate.com>

Weiwei Shi wrote:

>Hi,
>I have a text mining project and currently I am working on feature
>generation/selection part.
>My plan is selecting a set of words or word combinations which have
>better discriminant capability than other words in telling the group
>id's (2 classes in this case) for a dataset which has 2,000,000
>documents.
>
>One approach is using "contrast-set association rule mining" while the
>other is using chisqr or fisher exact test.
>
>An example which has 3 contingency tables for 3 words as followed
>(word coded by number):
>  
>
>>tab[,,1:3]
>>    
>>
>, , 1
>
>      [,1]    [,2]
>[1,] 11266 2151526
>[2,]   125   31734
>
>, , 2
>
>      [,1]    [,2]
>[1,] 43571 2119221
>[2,]    52   31807
>
>, , 3
>
>     [,1]    [,2]
>[1,]  427 2162365
>[2,]    5   31854
>
>
>I have some questions on this:
>1. What's the thumb of rule to use chisq test instead of Fisher exact
>test. I have a  vague memory which said for each cell, the count needs
>to be over 50 if chisq instead of fisher exact test is going to be
>used. In the case of word 3,  I think I should use fisher test.
>However, running chisq like below is fine:
>  
>
>>tab[,,3]
>>    
>>
>     [,1]    [,2]
>[1,]  427 2162365
>[2,]    5   31854
>  
>
>>chisq.test(tab[,,3])
>>    
>>
>
>        Pearson's Chi-squared test with Yates' continuity correction
>
>data:  tab[, , 3]
>X-squared = 0.0963, df = 1, p-value = 0.7564
>
>but running on the whole set of words (including 14240 words) has the
>following warnings:
>  
>
>>p.chisq<-as.double(lapply(1:N, function(i) chisq.test(tab[,,i])$p.value))
>>    
>>
>There were 50 or more warnings (use warnings() to see the first 50)
>  
>
>>warnings()
>>    
>>
>Warning messages:
>1: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
>2: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
>3: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
>4: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
>
>
>2. So, my second question is, is this warning b/c I am against the
>assumption of using chisq. But why Word 3 is fine? How to trace the
>warning to see which word caused this warning?
>
>3. My result looks like this (after some mapping treating from number
>id to word and some words are stemmed here, like ACCID is accident):
> > of[1:50,]
>      map...2.      p.fisher
>21       ACCID  0.000000e+00
>30          CD  0.000000e+00
>67        ROCK  0.000000e+00
>104      CRACK  0.000000e+00
>111       CHIP  0.000000e+00
>179      GLASS  0.000000e+00
>84        BACK 4.199878e-291
>395   DRIVEABL 5.335989e-287
>60         CAP 9.405235e-285
>262 WINDSHIELD 2.691641e-254
>13          IV 3.905186e-245
>110         HZ 2.819713e-210
>11        CAMP 9.086768e-207
>2      SHATTER 5.273994e-202
>297        ALP 1.678521e-177
>162        BED 1.822031e-173
>249        BCD 1.398391e-160
>493       RACK 4.178617e-156
>59        CAUS 7.539031e-147
>
>3.1 question: Should I use two-sided test instead of one-sided for
>fisher test? I read some material which suggests using two-sided.
>
>3.2 A big question: Even though the result looks very promising since
>this is case of classiying fraud cases and the words selected by this
>approach make sense. However, I think p-values here just indicate the
>strength to reject null hypothesis, not the strength of association
>between word and class of document. So, what kind of statistics I
>should use here to evaluate the strength of association? odds ratio?
>
>Any suggestions are welcome!
>
>Thanks!
>  
>
You can use chisq.test with sim=TRUE, or call it as usual first, see if 
there is a warning, and then recall
with sim=TRUE.

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra




-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From andy_liaw at merck.com  Wed Jun 22 20:50:18 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 14:50:18 -0400
Subject: [R] monitoring objects sizes
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9E4@usctmx1106.merck.com>

Do RSiteSearch("ls.obj") and click on the first hit.

Andy

> From: Omar Lakkis
> 
> I have an R script that loops over market contracts. The script runs
> well for markets with relatively small number of contracts but seg
> faults when the number of contracts (loop iterations) is large.
> Is there a way for me to monitor my objects and their sizes from
> within the R script?
> How can I get all of the objects and their sizes?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From spencer.graves at pdf.com  Wed Jun 22 20:55:02 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 22 Jun 2005 11:55:02 -0700
Subject: [R] monitoring objects sizes
In-Reply-To: <3f87cc6d050622113616038e55@mail.gmail.com>
References: <3f87cc6d050622113616038e55@mail.gmail.com>
Message-ID: <42B9B406.7090407@pdf.com>

	  hel.search("object size") found a function "object.size" that should 
do what you want.

	  spencer graves

Omar Lakkis wrote:

> I have an R script that loops over market contracts. The script runs
> well for markets with relatively small number of contracts but seg
> faults when the number of contracts (loop iterations) is large.
> Is there a way for me to monitor my objects and their sizes from
> within the R script?
> How can I get all of the objects and their sizes?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spluque at gmail.com  Wed Jun 22 22:50:50 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Wed, 22 Jun 2005 15:50:50 -0500
Subject: [R] classes with chron slots
Message-ID: <878y1286ad.fsf@gmail.com>

I'd like to define a class with a chron slot, but:

R> require(chron)
R> setClass("myclass", representation(datetime = "chron"))
[1] "myclass"
Warning message:
undefined slot classes in definition of "myclass": datetime(class "chron")
in: .completeClassSlots(ClassDef, where)

How should such a class be defined?

Sebastian
-- 
Sebastian P. Luque



From Wittner.Ben at mgh.harvard.edu  Wed Jun 22 23:00:19 2005
From: Wittner.Ben at mgh.harvard.edu (Wittner, Ben)
Date: Wed, 22 Jun 2005 17:00:19 -0400
Subject: [R] analyzing suvival data using splines (a.k.a.,
	piecewise log-hazard-ratio models)
Message-ID: <B1D5C2D0D1D6AE4C9DF88E81330D71C60DA30F@PHSXMB23.partners.org>

I'm looking for software that makes plots such as fig 4 (a)-(e), fig 5 anf fig 7
of
Gray, Robert, "Flexible Methods for Analyzing Survival Data Using Splines, with
Applications to Breast Cancer Prognosis," 1992, J Am Stat Assoc, pp 942-51.

In other words, I'm looking for software that takes survival data and a
continuous
covariate as input and computes a curve giving log hazard ratio (or rate of
failure for
a specific time) as a function of the continuous covariate, as well as curves
giving
+-SE for that curve.

A plot of this nature can also be found as figure 4 in Paik, et al., "A
Multigene Assay to
Predict Recurrence of Tamoxifen-Treated, Node-Negative Breast Cancer," 2004, New
England
Journal of Medicine, pp 2817-26.

Any help would be greatly appreciated.

Thanks.

-Ben

Ben Wittner
Research fellow, MGH & Harvard Medical School
wittner.ben at mgh.harvard.edu



From f.harrell at vanderbilt.edu  Wed Jun 22 23:05:46 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 22 Jun 2005 16:05:46 -0500
Subject: [R] analyzing suvival data using splines (a.k.a.,
 piecewise log-hazard-ratio models)
In-Reply-To: <B1D5C2D0D1D6AE4C9DF88E81330D71C60DA30F@PHSXMB23.partners.org>
References: <B1D5C2D0D1D6AE4C9DF88E81330D71C60DA30F@PHSXMB23.partners.org>
Message-ID: <42B9D2AA.5090108@vanderbilt.edu>

Wittner, Ben wrote:
> I'm looking for software that makes plots such as fig 4 (a)-(e), fig 5 anf fig 7
> of
> Gray, Robert, "Flexible Methods for Analyzing Survival Data Using Splines, with
> Applications to Breast Cancer Prognosis," 1992, J Am Stat Assoc, pp 942-51.
> 
> In other words, I'm looking for software that takes survival data and a
> continuous
> covariate as input and computes a curve giving log hazard ratio (or rate of
> failure for
> a specific time) as a function of the continuous covariate, as well as curves
> giving
> +-SE for that curve.

library(Design)
d <- datadist(mydata); options(datadist='d')
f <- cph(Surv(dtime,death) ~ age + sex + rcs(blood.pressure,5))
plot(f, blood.pressure=NA)

Frank

> 
> A plot of this nature can also be found as figure 4 in Paik, et al., "A
> Multigene Assay to
> Predict Recurrence of Tamoxifen-Treated, Node-Negative Breast Cancer," 2004, New
> England
> Journal of Medicine, pp 2817-26.
> 
> Any help would be greatly appreciated.
> 
> Thanks.
> 
> -Ben
> 
> Ben Wittner
> Research fellow, MGH & Harvard Medical School
> wittner.ben at mgh.harvard.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From chrysopa at gmail.com  Wed Jun 22 23:11:41 2005
From: chrysopa at gmail.com (Ronaldo Reis-Jr.)
Date: Wed, 22 Jun 2005 18:11:41 -0300
Subject: [R] Rsquare from glmmPQL or another GLMM?
Message-ID: <200506221811.41828.chrysopa@gmail.com>

Hi,

I know that Rsquare in glm or in non-linear models is "wrong", but some people 
like this.

How I make to estimate the Rsquare from a model ajusted with glmmPQL or 
another GLMM?

Thanks for all
Ronaldo
-- 
A simplicidade ?? o ??ltimo degrau da sabedoria.
                -- Victor Hugo
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36570-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-4007                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From sstoddar at life.uiuc.edu  Wed Jun 22 23:14:07 2005
From: sstoddar at life.uiuc.edu (Steven T. Stoddard)
Date: Wed, 22 Jun 2005 16:14:07 -0500
Subject: [R] load history does not work on OS X
Message-ID: <64860ED0-9C3F-4C53-9EA4-7F1AC6D62E61@life.uiuc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050622/a74c8020/attachment.pl

From Soren.Hojsgaard at agrsci.dk  Wed Jun 22 23:33:03 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Wed, 22 Jun 2005 23:33:03 +0200
Subject: [R] substitute in a named expression
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A09@DJFPOST01.djf.agrsci.dk>

I have a 'named expression' like
  expr <- expression(rep(1,d))
and would like to replace the argument d with say 5 without actually evaluating the expression. So I try  substitute(expr, list(d=5)) in which case R simply returns expr which when I 'evaluate' it gives
 eval(expr)
 Error in rep.default(1, d) : invalid number of copies in rep()

I've looked at ?substitute and ?expression (and other places) for ideas, but - well I guess there are some details which I haven't quite understood. Can anyone point me in the right direction?
Thanks
S??ren



From ljin at lbl.gov  Wed Jun 22 23:34:21 2005
From: ljin at lbl.gov (Ling Jin)
Date: Wed, 22 Jun 2005 14:34:21 -0700
Subject: [R] How to sort a dataset by one column?
Message-ID: <42B9D95D.2000201@lbl.gov>

I understand how to sort a vector, but I could not find how to sort a 
data frame or matrix by one variable (column). Could you give me some 
examples? Thanks!

Ling



From jjmichael at comcast.net  Wed Jun 22 23:35:07 2005
From: jjmichael at comcast.net (Jake Michaelson)
Date: Wed, 22 Jun 2005 15:35:07 -0600
Subject: [R] string/character to number
Message-ID: <3419b5c08aaff81ddd2469673dd2951c@comcast.net>

I did a very quick search of the archive and couldn't find a readily 
available answer to this one:

I'd like to convert, for example:

c("a", "b", "a", "b")

to

c(1, -1, 1, -1)

In the case of the first vector, it may be any length, but will always 
only have two unique values.  It must always be replaced by 
corresponding values of 1 and -1.

Any thoughts?

Thanks in advance,

Jake



From jjmichael at comcast.net  Wed Jun 22 23:46:02 2005
From: jjmichael at comcast.net (Jake Michaelson)
Date: Wed, 22 Jun 2005 15:46:02 -0600
Subject: [R] string/character to number
In-Reply-To: <3419b5c08aaff81ddd2469673dd2951c@comcast.net>
References: <3419b5c08aaff81ddd2469673dd2951c@comcast.net>
Message-ID: <eec38e298411806eb0f8cf3117487571@comcast.net>

Duh!

sub()

--Jake

On Jun 22, 2005, at 3:35 PM, Jake Michaelson wrote:

> I did a very quick search of the archive and couldn't find a readily
> available answer to this one:
>
> I'd like to convert, for example:
>
> c("a", "b", "a", "b")
>
> to
>
> c(1, -1, 1, -1)
>
> In the case of the first vector, it may be any length, but will always
> only have two unique values.  It must always be replaced by
> corresponding values of 1 and -1.
>
> Any thoughts?
>
> Thanks in advance,
>
> Jake
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From MSchwartz at mn.rr.com  Wed Jun 22 23:50:36 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 22 Jun 2005 16:50:36 -0500
Subject: [R] How to sort a dataset by one column?
In-Reply-To: <42B9D95D.2000201@lbl.gov>
References: <42B9D95D.2000201@lbl.gov>
Message-ID: <1119477036.19120.116.camel@localhost.localdomain>

On Wed, 2005-06-22 at 14:34 -0700, Ling Jin wrote:
> I understand how to sort a vector, but I could not find how to sort a 
> data frame or matrix by one variable (column). Could you give me some 
> examples? Thanks!
> 
> Ling

See the examples in ?order

HTH,

Marc Schwartz



From Soren.Hojsgaard at agrsci.dk  Wed Jun 22 23:49:58 2005
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Wed, 22 Jun 2005 23:49:58 +0200
Subject: [R] string/character to number
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A0A@DJFPOST01.djf.agrsci.dk>

This works
x<-c("a", "b", "a", "b")
x[x=="a"]<-1
x[x=="b"]<- -1
as.numeric(x)
[1]  1 -1  1 -1


________________________________

Fra: r-help-bounces at stat.math.ethz.ch p?? vegne af Jake Michaelson
Sendt: on 22-06-2005 23:35
Til: R-help at stat.math.ethz.ch
Emne: [R] string/character to number



I did a very quick search of the archive and couldn't find a readily
available answer to this one:

I'd like to convert, for example:

c("a", "b", "a", "b")

to

c(1, -1, 1, -1)

In the case of the first vector, it may be any length, but will always
only have two unique values.  It must always be replaced by
corresponding values of 1 and -1.

Any thoughts?

Thanks in advance,

Jake

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jun 22 23:54:13 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 17:54:13 -0400
Subject: [R] How to sort a dataset by one column?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9E6@usctmx1106.merck.com>

See RSiteSearch("sort.data.frame"), or more generally, ?order.

Andy

> From: Ling Jin
> 
> I understand how to sort a vector, but I could not find how to sort a 
> data frame or matrix by one variable (column). Could you give me some 
> examples? Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From gunter.berton at gene.com  Wed Jun 22 23:57:07 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 22 Jun 2005 14:57:07 -0700
Subject: [R] string/character to number
In-Reply-To: <eec38e298411806eb0f8cf3117487571@comcast.net>
Message-ID: <200506222157.j5MLv5nt029093@compton.gene.com>

Note: sub() returns a character vector not a numeric vector. as.numeric()
will convert it.

Slightly slicker and faster is: 2*(z=='a')-1   where z is your vector,
c('a','b','a','b')

Cheers,
Bert

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jake Michaelson
Sent: Wednesday, June 22, 2005 2:46 PM
To: Jake Michaelson
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] string/character to number

Duh!

sub()

--Jake

On Jun 22, 2005, at 3:35 PM, Jake Michaelson wrote:

> I did a very quick search of the archive and couldn't find a readily
> available answer to this one:
>
> I'd like to convert, for example:
>
> c("a", "b", "a", "b")
>
> to
>
> c(1, -1, 1, -1)
>
> In the case of the first vector, it may be any length, but will always
> only have two unique values.  It must always be replaced by
> corresponding values of 1 and -1.
>
> Any thoughts?
>
> Thanks in advance,
>
> Jake
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at mn.rr.com  Wed Jun 22 23:58:55 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Wed, 22 Jun 2005 16:58:55 -0500
Subject: [R] string/character to number
In-Reply-To: <eec38e298411806eb0f8cf3117487571@comcast.net>
References: <3419b5c08aaff81ddd2469673dd2951c@comcast.net>
	<eec38e298411806eb0f8cf3117487571@comcast.net>
Message-ID: <1119477535.19120.120.camel@localhost.localdomain>

To do this in one step, it would be easier to use ifelse():

> Chars
[1] "a" "b" "a" "b"

> ifelse(Chars == "a", 1, -1)
[1]  1 -1  1 -1

HTH,

Marc Schwartz


On Wed, 2005-06-22 at 15:46 -0600, Jake Michaelson wrote:
> Duh!
> 
> sub()
> 
> --Jake
> 
> On Jun 22, 2005, at 3:35 PM, Jake Michaelson wrote:
> 
> > I did a very quick search of the archive and couldn't find a readily
> > available answer to this one:
> >
> > I'd like to convert, for example:
> >
> > c("a", "b", "a", "b")
> >
> > to
> >
> > c(1, -1, 1, -1)
> >
> > In the case of the first vector, it may be any length, but will always
> > only have two unique values.  It must always be replaced by
> > corresponding values of 1 and -1.
> >
> > Any thoughts?
> >
> > Thanks in advance,
> >
> > Jake
> >



From andy_liaw at merck.com  Wed Jun 22 23:58:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 22 Jun 2005 17:58:51 -0400
Subject: [R] string/character to number
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9E7@usctmx1106.merck.com>

You can do indexing by name:

> x <- c("a", "b", "a", "b")
> v <- c(a=1, b=-1)
> v[x]
 a  b  a  b 
 1 -1  1 -1 

Andy 

> From: Jake Michaelson
> 
> Duh!
> 
> sub()
> 
> --Jake
> 
> On Jun 22, 2005, at 3:35 PM, Jake Michaelson wrote:
> 
> > I did a very quick search of the archive and couldn't find a readily
> > available answer to this one:
> >
> > I'd like to convert, for example:
> >
> > c("a", "b", "a", "b")
> >
> > to
> >
> > c(1, -1, 1, -1)
> >
> > In the case of the first vector, it may be any length, but 
> will always
> > only have two unique values.  It must always be replaced by
> > corresponding values of 1 and -1.
> >
> > Any thoughts?
> >
> > Thanks in advance,
> >
> > Jake
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Thu Jun 23 00:47:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 22 Jun 2005 18:47:07 -0400
Subject: [R] substitute in a named expression
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A09@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A09@DJFPOST01.djf.agrsci.dk>
Message-ID: <971536df0506221547489c8bcb@mail.gmail.com>

On 6/22/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> I have a 'named expression' like
>  expr <- expression(rep(1,d))
> and would like to replace the argument d with say 5 without actually evaluating the expression. So I try  substitute(expr, list(d=5)) in which case R simply returns expr which when I 'evaluate' it gives
>  eval(expr)
>  Error in rep.default(1, d) : invalid number of copies in rep()
> 
> I've looked at ?substitute and ?expression (and other places) for ideas, but - well I guess there are some details which I haven't quite understood. Can anyone point me in the right direction?

Try this:

eval(substitute(substitute(qq, list(d=5)), list(qq = expr[[1]])))

This aspect of R drove me crazy some time ago but Tony Plate finally figured 
it out and discussed it some time back:
   http://tolstoy.newcastle.edu.au/R/help/04/03/1247.html
There is also a handy utility routine, esub, defined there.

The key points are:

- substitute won't go inside expressions but it will go inside call objects.
  In this case your expr is an expression but expr[[1]] is a call object with
  the desired contents.  Note that quote will return a call
  object so you can avoid the [[1]] if you define expr as cl <- quote(rep(1,d))
  i.e.  
   cl <- quote(rep(1,d))
   eval(substitute(substitute(cl, list(d=5)), list(cl = cl)))

- substitute autoquotes anything inside it so one must substitute in 
  the first argument to the inner substitute using a second outer substitute.  
  That is, the outer substitute substitutes expr[[1]] (which is evaluated) into 
  the first argument of the inner substitute.

- the outer substitute wraps the result of the inner one in a call so we must 
  perform an eval to get what is within the call.  This part is explained in
  ?substitute

Sorry if this is complicated but that seems to be how it works.  Using
the esub function defined in the link above you can simplify it substantially
like this:

esub(cl, list(d=5))

# or

esub(expr[[1]], list(d=5))



From helprhelp at gmail.com  Thu Jun 23 01:08:04 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 22 Jun 2005 18:08:04 -0500
Subject: [R] chisq test and fisher exact test
In-Reply-To: <42B9B2D8.3040007@acelerate.com>
References: <cdf817830506220830e9ce363@mail.gmail.com>
	<42B9B2D8.3040007@acelerate.com>
Message-ID: <cdf817830506221608617e8e26@mail.gmail.com>

Is it b/c my question is too long so no one answers it? I should have
splitted it. :(

On 6/22/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> Weiwei Shi wrote:
> 
> >Hi,
> >I have a text mining project and currently I am working on feature
> >generation/selection part.
> >My plan is selecting a set of words or word combinations which have
> >better discriminant capability than other words in telling the group
> >id's (2 classes in this case) for a dataset which has 2,000,000
> >documents.
> >
> >One approach is using "contrast-set association rule mining" while the
> >other is using chisqr or fisher exact test.
> >
> >An example which has 3 contingency tables for 3 words as followed
> >(word coded by number):
> >
> >
> >>tab[,,1:3]
> >>
> >>
> >, , 1
> >
> >      [,1]    [,2]
> >[1,] 11266 2151526
> >[2,]   125   31734
> >
> >, , 2
> >
> >      [,1]    [,2]
> >[1,] 43571 2119221
> >[2,]    52   31807
> >
> >, , 3
> >
> >     [,1]    [,2]
> >[1,]  427 2162365
> >[2,]    5   31854
> >
> >
> >I have some questions on this:
> >1. What's the thumb of rule to use chisq test instead of Fisher exact
> >test. I have a  vague memory which said for each cell, the count needs
> >to be over 50 if chisq instead of fisher exact test is going to be
> >used. In the case of word 3,  I think I should use fisher test.
> >However, running chisq like below is fine:
> >
> >
> >>tab[,,3]
> >>
> >>
> >     [,1]    [,2]
> >[1,]  427 2162365
> >[2,]    5   31854
> >
> >
> >>chisq.test(tab[,,3])
> >>
> >>
> >
> >        Pearson's Chi-squared test with Yates' continuity correction
> >
> >data:  tab[, , 3]
> >X-squared = 0.0963, df = 1, p-value = 0.7564
> >
> >but running on the whole set of words (including 14240 words) has the
> >following warnings:
> >
> >
> >>p.chisq<-as.double(lapply(1:N, function(i) chisq.test(tab[,,i])$p.value))
> >>
> >>
> >There were 50 or more warnings (use warnings() to see the first 50)
> >
> >
> >>warnings()
> >>
> >>
> >Warning messages:
> >1: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
> >2: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
> >3: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
> >4: Chi-squared approximation may be incorrect in: chisq.test(tab[, , i])
> >
> >
> >2. So, my second question is, is this warning b/c I am against the
> >assumption of using chisq. But why Word 3 is fine? How to trace the
> >warning to see which word caused this warning?
> >
> >3. My result looks like this (after some mapping treating from number
> >id to word and some words are stemmed here, like ACCID is accident):
> > > of[1:50,]
> >      map...2.      p.fisher
> >21       ACCID  0.000000e+00
> >30          CD  0.000000e+00
> >67        ROCK  0.000000e+00
> >104      CRACK  0.000000e+00
> >111       CHIP  0.000000e+00
> >179      GLASS  0.000000e+00
> >84        BACK 4.199878e-291
> >395   DRIVEABL 5.335989e-287
> >60         CAP 9.405235e-285
> >262 WINDSHIELD 2.691641e-254
> >13          IV 3.905186e-245
> >110         HZ 2.819713e-210
> >11        CAMP 9.086768e-207
> >2      SHATTER 5.273994e-202
> >297        ALP 1.678521e-177
> >162        BED 1.822031e-173
> >249        BCD 1.398391e-160
> >493       RACK 4.178617e-156
> >59        CAUS 7.539031e-147
> >
> >3.1 question: Should I use two-sided test instead of one-sided for
> >fisher test? I read some material which suggests using two-sided.
> >
> >3.2 A big question: Even though the result looks very promising since
> >this is case of classiying fraud cases and the words selected by this
> >approach make sense. However, I think p-values here just indicate the
> >strength to reject null hypothesis, not the strength of association
> >between word and class of document. So, what kind of statistics I
> >should use here to evaluate the strength of association? odds ratio?
> >
> >Any suggestions are welcome!
> >
> >Thanks!
> >
> >
> You can use chisq.test with sim=TRUE, or call it as usual first, see if
> there is a warning, and then recall
> with sim=TRUE.
> 
> Kjetil
> 
> --
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass construction.
>                --  Mahdi Elmandjra
> 
> 
> 
> 
> --
> No virus found in this outgoing message.
> Checked by AVG Anti-Virus.
> Version: 7.0.323 / Virus Database: 267.7.7/20 - Release Date: 16/06/2005
> 
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From p.dalgaard at biostat.ku.dk  Thu Jun 23 01:09:45 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2005 01:09:45 +0200
Subject: [R] substitute in a named expression
In-Reply-To: <971536df0506221547489c8bcb@mail.gmail.com>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD9A09@DJFPOST01.djf.agrsci.dk>
	<971536df0506221547489c8bcb@mail.gmail.com>
Message-ID: <x2u0jqaszq.fsf@turmalin.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> On 6/22/05, S??ren H??jsgaard <Soren.Hojsgaard at agrsci.dk> wrote:
> > I have a 'named expression' like
> >  expr <- expression(rep(1,d))
> > and would like to replace the argument d with say 5 without actually evaluating the expression. So I try  substitute(expr, list(d=5)) in which case R simply returns expr which when I 'evaluate' it gives
> >  eval(expr)
> >  Error in rep.default(1, d) : invalid number of copies in rep()
> > 
> > I've looked at ?substitute and ?expression (and other places) for ideas, but - well I guess there are some details which I haven't quite understood. Can anyone point me in the right direction?
> 
> Try this:
> 
> eval(substitute(substitute(qq, list(d=5)), list(qq = expr[[1]])))
> 
> This aspect of R drove me crazy some time ago but Tony Plate finally figured 
> it out and discussed it some time back:
>    http://tolstoy.newcastle.edu.au/R/help/04/03/1247.html
> There is also a handy utility routine, esub, defined there.
> 
> The key points are:
> 
> - substitute won't go inside expressions but it will go inside call objects.
>   In this case your expr is an expression but expr[[1]] is a call object with
>   the desired contents.  Note that quote will return a call
>   object so you can avoid the [[1]] if you define expr as cl <- quote(rep(1,d))
>   i.e.  
>    cl <- quote(rep(1,d))
>    eval(substitute(substitute(cl, list(d=5)), list(cl = cl)))
> 
> - substitute autoquotes anything inside it so one must substitute in 
>   the first argument to the inner substitute using a second outer substitute.  
>   That is, the outer substitute substitutes expr[[1]] (which is evaluated) into 
>   the first argument of the inner substitute.
> 
> - the outer substitute wraps the result of the inner one in a call so we must 
>   perform an eval to get what is within the call.  This part is explained in
>   ?substitute
> 
> Sorry if this is complicated but that seems to be how it works.  Using
> the esub function defined in the link above you can simplify it substantially
> like this:
> 
> esub(cl, list(d=5))
> 
> # or
> 
> esub(expr[[1]], list(d=5))

Yes, substitute() is a bass-ackward design and the automatic quoting
of the first arg is a pain. It would have been much cleaner if
standard semantics were used and you'd just quote() the argument when
needed.

Your explanation of 

> eval(substitute(substitute(qq, list(d=5)), list(qq = expr[[1]])))
 
is a tad long-winded though.

What happens is that the inner unevaluated 

substitute(qq, list(d=5))

gets the qq replaced by the value of expr[[1]]. In casu it becomes

substitute(rep(1,d),list(d=5))

this then needs to be evaluated, yielding

rep(1,5)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ljin at lbl.gov  Thu Jun 23 01:46:48 2005
From: ljin at lbl.gov (Ling Jin)
Date: Wed, 22 Jun 2005 16:46:48 -0700
Subject: [R] How to read an excel data into R?
Message-ID: <42B9F868.5080807@lbl.gov>

Hi all,

Does anybody know the easiest way to import excel data into R? I copied 
and pasted the excel data into a txt file, and tried read.table, but R 
reported that

Error in read.table("data_support.txt", sep = " ", header = T) :
         more columns than column names

Thanks!

Ling



From spencer.graves at pdf.com  Thu Jun 23 02:14:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 22 Jun 2005 17:14:34 -0700
Subject: [R] How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
References: <42B9F868.5080807@lbl.gov>
Message-ID: <42B9FEEA.1060503@pdf.com>

	  Your error message tells me that you have different numbers of fields 
in different lines.  You say you, "copied and pasted the excel data into 
a txt file".  I usually copy what I want into a clean sheet then File -> 
Save, then File -> "Save As" -> "Save as type" = "CSV (Comma delimited) 
(*.csv)" or "Text (Tab delimited) (*.txt)".  Excel will ask if I'm sure 
a couple of times, and I say yes.  If that's what you've done and still 
have a problem, then I have other tools:

	  First, I'll assign the file name to something like "File".  Then, 
'readLines(File, n=9)' tells me if the file starts as I think it does. 
If I've got extra headers, it will tell me that.

	  Then, I do something like the following:

	  n.flds <- count.fields(File, sep="\t")
	  plot(n.flds)
	  sd(n.flds)

	  Then I play with the arguments to "count.fields" until 'sd(n.flds)' 
is 0.  Then I use "read.table" with arguments as I used to get 
everything right in 'count.fields'.  If I can't get sd(n.flds) to 0, you 
can try read.table with 'fill=TRUE'.  However, when you do that, you 
need to check to make sure all the columns line up correctly with the 
shorter lines.

	  Also, this issue has been discussed many times.  'RSiteSearch("read 
excel")' just produced 1196 hits for me.  If the above doesn't work, you 
might try skimming a few from that list.

	  hope this helps.
	  spencer graves

Ling Jin wrote:

> Hi all,
> 
> Does anybody know the easiest way to import excel data into R? I copied 
> and pasted the excel data into a txt file, and tried read.table, but R 
> reported that
> 
> Error in read.table("data_support.txt", sep = " ", header = T) :
>          more columns than column names
> 
> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From p.murrell at auckland.ac.nz  Thu Jun 23 02:24:49 2005
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 23 Jun 2005 12:24:49 +1200
Subject: [R] legend
References: <iihhxl.stn69u@webmail.tuwien.ac.at>
	<42B98328.7070503@statistik.uni-dortmund.de>
Message-ID: <42BA0151.3080307@stat.auckland.ac.nz>

Hi


Uwe Ligges wrote:
> Thomas Steiner wrote:
> 
> 
>>I color some area grey with polygon() (with a red border) and then I
>>want to have the dashed red border in the legend as well. How do I
>>manage it?
>>
>>And I want to mix (latex) expressions with text in my legend.
> 
> 
> 
> Both points are not that easy to solve, hence I'd like to suggest to 
> write your own little function that generates the legend.
> 
> Starting at the upper left, calculating the stringheight, painting the 
> (party very special) symbols, and adding the text line by line seems to 
> be the most easiest solution here (which is not that nice, though.


I don't think it's too bad.  For example, try replacing the original ...

legend(x=0,y=3.5,legend=c("exp(0.7x)","mu=0.7, sigma=0.4","mu=0.7,
sigma=0.2","mu=0.7, sigma=0.1","Standardabweichung f????r 
sigma=0.2"),lwd=c(4,4,4,4,12),col=c(cs,"grey"),bg="transparent",cex=1.15)

... with ...

# Use grid and gridBase so you've got some sensible
# coordinate systems to work within
library(grid)
library(gridBase)
# Align a grid viewport with the plotting region
vps <- baseViewports()
pushViewport(vps$inner, vps$figure, vps$plot)
# Define labels and colours
# Labels are mathematical expressions
labels <- expression("exp(0.7x)",
     list(mu == 0.7,sigma == 0.4),
     list(mu == 0.7,sigma == 0.2),
     list(mu == 0.7, sigma == 0.1),
     paste("Standardabweichung f????r ",sigma == 0.2))
cols <- cs
# Draw each legend item on its own line
# Top line 1cm in from top-left corner
for (i in 1:5) {
   x <- unit(1, "cm")
   y <- unit(1, "npc") - unit(1, "cm") - unit(i, "lines")
   if (i < 5) {
     grid.lines(unit.c(x, unit(2, "cm")), y + unit(0.5, "lines"),
                gp=gpar(col=cols[i], lwd=3))
   } else {
     grid.rect(x, y, width=unit(1, "cm"),
               height=unit(1, "lines"),
               gp=gpar(fill="grey", col=cs[3], lty="dashed"),
               just=c("left", "bottom"))
   }
   grid.text(labels[i], x + unit(1.5, "cm"), y,
             just=c("left", "bottom"))
}
# clean up
popViewport(3)

... that's a bit of typing, but if you need to do more than one, it 
would go inside a function with labels and cols as arguments (and '5' 
replaced by 'length(labels)') without too much trouble.

(In this case, you could also pretty easily just do the main plot using 
grid and avoid having to use gridBase.)

Paul


>>Just execute my lines below and you know want I mean. Or pass by at
>>http://de.wikipedia.org/wiki/Bild:GBM.png to see the picture online.
>>
>>Thomas
>>
>>
>>bm <- function(n=500, from=0, to=1) {
>>  x=seq(from=from,to=to,length=n)
>>  BM<-c(0,cumsum(rnorm(n-1,mean=0,sd=sqrt(to/n))))
>>  cbind(x,BM)
>>}
>>gbm <- function(bm,S0=1,sigma=0.1,mu=1) {
>>  gbm=S0
>>  for (t in 2:length(bm[,1])) {
>>    gbm[t]=S0*exp((mu-sigma^2/2)*bm[t,1]+sigma*bm[t,2])
>>  }
>>  cbind(bm[,1],gbm)
>>}
>>
>>set.seed(9826064)
>>cs=c("dark green", "steelblue", "red", "yellow")
>>
>>#png(filename = "GBM.png", width=1600, height=1200, pointsize = 12)
>>par(bg="lightgrey")
>>x=seq(from=0,to=1,length=500)
>>plot(x=x, y=exp(0.7*x), type="n", xlab="Zeit", ylab="", ylim=c(1,3.5))
>>polygon(x=c(x,rev(x)),
>>y=c(exp(0.7*x)+0.4*sqrt(x),rev(exp(0.7*x)-0.4*sqrt(x))), col="grey",
>>border=cs[3], lty="dashed")
>>lines(x=x,y=exp(0.7*x), type="l", lwd=3, col=cs[1])
>>lines(gbm(bm(),S0=1,mu=0.7,sigma=0.4), lwd=3, col=cs[2])
>>lines(gbm(bm(),S0=1,mu=0.7,sigma=0.2), lwd=3, col=cs[3])
>>lines(gbm(bm(),S0=1,mu=0.7,sigma=0.1), lwd=3, col=cs[4])
>>title(main="Geometrische Brownsche Bewegung",cex.main=2.5)
>>legend(x=0,y=3.5,legend=c("exp(0.7x)","mu=0.7, sigma=0.4","mu=0.7,
>>sigma=0.2","mu=0.7, sigma=0.1","Standardabweichung f??r 
>>sigma=0.2"),lwd=c(4,4,4,4,12),col=c(cs,"grey"),bg="transparent",cex=1.15)
>>#dev.off()
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From liuwensui at gmail.com  Thu Jun 23 02:38:23 2005
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 22 Jun 2005 20:38:23 -0400
Subject: [R] How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
References: <42B9F868.5080807@lbl.gov>
Message-ID: <1115a2b00506221738473812ce@mail.gmail.com>

Ling,

You might take a look at the function read.xls() in gdata library.

HTH.


On 6/22/05, Ling Jin <ljin at lbl.gov> wrote:
> Hi all,
> 
> Does anybody know the easiest way to import excel data into R? I copied
> and pasted the excel data into a txt file, and tried read.table, but R
> reported that
> 
> Error in read.table("data_support.txt", sep = " ", header = T) :
>          more columns than column names
> 
> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
WenSui Liu, MS MA
Senior Decision Support Analyst
Division of Health Policy and Clinical Effectiveness
Cincinnati Children Hospital Medical Center



From davidoff at haas.berkeley.edu  Thu Jun 23 02:58:20 2005
From: davidoff at haas.berkeley.edu (Thomas Davidoff)
Date: Wed, 22 Jun 2005 17:58:20 -0700
Subject: [R] Large number of covariates in survival regressions
Message-ID: <6819D6F8-CA31-4135-AD16-E885C622D033@haas.berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050622/a91b99c6/attachment.pl

From joel3000 at gmail.com  Thu Jun 23 04:15:27 2005
From: joel3000 at gmail.com (Joel Bremson)
Date: Wed, 22 Jun 2005 19:15:27 -0700
Subject: [R] mac osx, g95 package port problem
Message-ID: <1253d67a05062219152f8b2795@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050622/ae12780b/attachment.pl

From lists at revelle.net  Thu Jun 23 04:50:07 2005
From: lists at revelle.net (William Revelle)
Date: Wed, 22 Jun 2005 21:50:07 -0500
Subject: [R] How to read an excel data into R?
In-Reply-To: <1115a2b00506221738473812ce@mail.gmail.com>
References: <42B9F868.5080807@lbl.gov>
	<1115a2b00506221738473812ce@mail.gmail.com>
Message-ID: <p06210225bedfce873797@[165.124.163.215]>

Ling,

   If  any column has text with spaces between words, this will lead 
to the "more columns ..." problem.
Delete the spaces and try again.

e.g., if the Excel file is
Var1	Var2	Var3
text	1	2
more text	3	4
yet more	5	6
and more	7	8
blahblah	9	10

On a Mac, this will lead to the error message
"Error in scan(file = file, what = what, sep = sep, quote = quote, 
dec = dec,  :
	line 1 did not have 4 elements"
(which I believe is the equivalent message to what you are getting on a PC)

But, if your remove the blanks in column 1, this reads as

>  x <- read.table("test.txt",header=T)
>  x
       Var1 Var2 Var3
1     text    1    2
2 moretext    3    4
3  yetmore    5    6
4  andmore    7    8
5 blahblah    9   10

with no error message.

Alternatively,  for small files, if using a PC try copying the Excel 
spreadsheet to your clipboard and

x  <- read.table(file("clipboard"), header = TRUE) or, if using a Mac

x  <- read.table(pipe("pbpaste"), header = TRUE)

Bill


At 8:38 PM -0400 6/22/05, Wensui Liu wrote:
>Ling,
>
>You might take a look at the function read.xls() in gdata library.
>
>HTH.
>
>
>On 6/22/05, Ling Jin <ljin at lbl.gov> wrote:
>>  Hi all,
>>
>>  Does anybody know the easiest way to import excel data into R? I copied
>>  and pasted the excel data into a txt file, and tried read.table, but R
>>  reported that
>>
>>  Error in read.table("data_support.txt", sep = " ", header = T) :
>>           more columns than column names
>>
>>  Thanks!
>>
>  > Ling
>>


-- 
William Revelle		http://pmc.psych.northwestern.edu/revelle.html   
Professor			http://personality-project.org/personality.html
Department of Psychology       http://www.wcas.northwestern.edu/psych/
Northwestern University	http://www.northwestern.edu/



From shigesong at gmail.com  Thu Jun 23 07:18:45 2005
From: shigesong at gmail.com (Shige Song)
Date: Thu, 23 Jun 2005 13:18:45 +0800
Subject: [R] How to save changed options in Rcmdr
Message-ID: <5abc11d80506222218de16589@mail.gmail.com>

Dear All,

I want to change the default options of Rcmdr; it seemed to work when
I made changes and click the "Exit and Restart R Commander". However,
next time I open Rcmdr, it automatically restored to the default
options. Is there a way to change Rcmdr's options permanently? Thanks!

Shige



From ripley at stats.ox.ac.uk  Thu Jun 23 08:01:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 07:01:53 +0100 (BST)
Subject: [R] mac osx, g95 package port problem
In-Reply-To: <1253d67a05062219152f8b2795@mail.gmail.com>
References: <1253d67a05062219152f8b2795@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506230656140.16242@gannet.stats>

This isn't really the appropriate list - r-sig-mac or perhaps r-devel.
See the posting guide.

The first problem is the -shared flag that somehow is being used, and as 
you use a Makefile it seems you have taken charge of that.  Looks like the 
Makefile you quote is not the one actually in use.

The next problem is that a dynamic library on MacOS X has extension 
.dylib, not .so.

On Wed, 22 Jun 2005, Joel Bremson wrote:

> Hi all,
>
> I have a working package for linux, including fortran 95 code compiled with
> g95,
> that I need to port to OS X. The package works on Linux and seems to load on
> the Mac,
> but when I try to run a function that calls C or Fortran I'm told that the
> symbol is not loaded.
>
> I'm developing via a shell account on an OS X system, I don't have access to
> a desktop.
> The set up is:
>
> R 2.1.0 Patched (2005-05-12).
> Darwin Kernel Version 8.1.0
> G95 (GCC 4.0.0 20050124 (experimental) (g95!) 06/20/05)
>
> Here is the Makefile:
>
> F90_FILES=\
> dbest_dbase_class.f90 \
> ...
> r_estimate.f90 \
>
> FORTRAN_FILES=\
> dgletc.f \
> ...
> mecdf.f
>
> C_FILES=init.c
>
> CFLAGS=-g -fPIC
>
> %.o: %.f90
> g95 -c -g $<
>
> %.o: %.f
> g95 -c -g $<
>
> %.o: %.c
> gcc -c -g $<
>
> bpkg.so: $(F90_FILES:%.f90=%.o) $(C_FILES:%.c=%.o) $(FORTRAN_FILES:%.f=%.o)
> g95 -L/Library/Frameworks -o $@ $^ -L/Library/Frameworks/R.framewo\
> rk/Resources
>
> -----end makefile------------------------
>
> Here is the relevant output of R CMD CHECK:
>
> * checking for working latex ...sh: line 1: latex: command not found
> NO
> * using log directory '/Users/jbremson/dev/bpkg.Rcheck'
> * using R version 2.1.0, 2005-05-12
> * checking for file 'bpkg/DESCRIPTION' ... OK
> * this is package 'bpkg' version '1.0-1'
> * checking if this is a source package ... OK
>
> * Installing *source* package 'bpkg' ...
> ** libs
> g95 -c -g dbest_dbase_class.f90
> g95 -c -g cm_class.f90
> ...
> gcc -c -g init.c
> init.c: In function 'R_g95_init':
> init.c:22: warning: incompatible implicit declaration of built-in function
> 'strdup'
> g95 -c -g dgletc.f
> g95 -c -g dglfgb.f
> g95 -c -g dglfg.f
>
> g95 -c -g dglfgb.f
> g95 -c -g dglfg.f
> g95 -c -g dmdc.f
> g95 -c -g mecdf.f
> g95 -shared -L/Library/Frameworks -o bpkg.so dbest_dbase_class.o cm_class.o
> bgw_cla\
> ss.o cm_mle_class.o pcm_dglg_o1.o cm_main.o r_estimate.o init.o dgletc.o
> dglfgb.o dg\
> lfg.o dmdc.o mecdf.o -L/Library/Frameworks/R.framework/Resources
> g95: unrecognized option '-shared'

[...]
>
> How can I get R to see the symbols for the package?

By supplying a valid DLL.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lzhtom at hotmail.com  Thu Jun 23 08:37:44 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Thu, 23 Jun 2005 06:37:44 +0000
Subject: [R] quotient and remainder
Message-ID: <BAY12-F29667E9E3FE9F8992DA3ADC7EA0@phx.gbl>

hi netters

Is there a function in R that can compute the quotient and remainder of a 
division calculation?   such that when 11 is given as the dividend and 5 
the divider, the function returns 2(quotient) and 1(remainder).

Thanks a lot!

_________________________________________________________________
Å√Å‚Å∑Å—ÅœÅ¬Å‘Åÿ MSN Explorer:   http://explorer.msn.com/lccn/



From dimitris.rizopoulos at med.kuleuven.be  Thu Jun 23 09:01:08 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 23 Jun 2005 09:01:08 +0200
Subject: [R] quotient and remainder
References: <BAY12-F29667E9E3FE9F8992DA3ADC7EA0@phx.gbl>
Message-ID: <00b501c577c1$54ad3c80$0540210a@www.domain>

> 11%/%5
[1] 2
> 11%%5
[1] 1
>

Best,
Dimitris

p.s., I'd suggest you to take a look at the "An Introduction to R" doc

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "zhihua li" <lzhtom at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 23, 2005 8:37 AM
Subject: [R] quotient and remainder


> hi netters
>
> Is there a function in R that can compute the quotient and remainder 
> of a
> division calculation?   such that when 11 is given as the dividend 
> and 5
> the divider, the function returns 2(quotient) and 1(remainder).
>
> Thanks a lot!
>
> _________________________________________________________________
> ÅÂÖçÅËÅ¥ÅπÅ‰Å∏ãÅËÅΩÅΩ MSN Explorer:   http://explorer.msn.com/lccn/
>
>


--------------------------------------------------------------------------------


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jun 23 09:06:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 08:06:03 +0100 (BST)
Subject: [R] quotient and remainder
In-Reply-To: <BAY12-F29667E9E3FE9F8992DA3ADC7EA0@phx.gbl>
References: <BAY12-F29667E9E3FE9F8992DA3ADC7EA0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0506230803570.18251@gannet.stats>

On Thu, 23 Jun 2005, zhihua li wrote:

> hi netters
>
> Is there a function in R that can compute the quotient and remainder of a 
> division calculation?   such that when 11 is given as the dividend and 5 the 
> divider, the function returns 2(quotient) and 1(remainder).


> 11 %% 5
[1] 1
> 11 %/% 5
[1] 2

See ?Arithmetic.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rlee at inverness.fpcc.net  Wed Jun 22 23:08:39 2005
From: rlee at inverness.fpcc.net (Lopaka Lee (Rob))
Date: Wed, 22 Jun 2005 15:08:39 -0600 (MDT)
Subject: [R] [R-pkgs] The GNU Linear Programming Kit for R
Message-ID: <33577.136.177.22.105.1119474519.squirrel@webmail.fpcc.net>

The R interface to the GNU Linear Programming Kit (GLPK) is now available
on CRAN.

The R interface functions are almost identical to the native GLPK C API
making it easy to move code and concepts between R and C.

The GLPK package is intended for solving large-scale linear programming
(LP), mixed integer linear programming (MIP), and other related problems.

The GLPK library includes the following main components:

* implementation of the simplex method;
* implementation of the primal-dual interior-point method;
* implementation of the branch-and-bound method;
* application program interface (API);
* GNU MathProg modeling language (a subset of AMPL);
* GLPSOL, a stand-alone LP/MIP solver.

See GLPK webpage <http://www.gnu.org/software/glpk/glpk.html>.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From p.dalgaard at biostat.ku.dk  Thu Jun 23 09:29:41 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2005 09:29:41 +0200
Subject: [R] How to read an excel data into R?
In-Reply-To: <p06210225bedfce873797@[165.124.163.215]>
References: <42B9F868.5080807@lbl.gov>
	<1115a2b00506221738473812ce@mail.gmail.com>
	<p06210225bedfce873797@[165.124.163.215]>
Message-ID: <x28y114jkq.fsf@turmalin.kubism.ku.dk>

William Revelle <lists at revelle.net> writes:

> Ling,
> 
>    If  any column has text with spaces between words, this will lead 
> to the "more columns ..." problem.
> Delete the spaces and try again.
> 
> e.g., if the Excel file is
> Var1	Var2	Var3
> text	1	2
> more text	3	4
> yet more	5	6
> and more	7	8
> blahblah	9	10
> 
> On a Mac, this will lead to the error message
> "Error in scan(file = file, what = what, sep = sep, quote = quote, 
> dec = dec,  :
> 	line 1 did not have 4 elements"
> (which I believe is the equivalent message to what you are getting on a PC)
> 
> But, if your remove the blanks in column 1, this reads as
> 
> >  x <- read.table("test.txt",header=T)
> >  x
>        Var1 Var2 Var3
> 1     text    1    2
> 2 moretext    3    4
> 3  yetmore    5    6
> 4  andmore    7    8
> 5 blahblah    9   10
> 
> with no error message.
> 
> Alternatively,  for small files, if using a PC try copying the Excel 
> spreadsheet to your clipboard and
> 
> x  <- read.table(file("clipboard"), header = TRUE) or, if using a Mac
> 
> x  <- read.table(pipe("pbpaste"), header = TRUE)

PLEASE! There are functions read.csv(), and read.delim() specifically
for the purpose of reading exported files. They have the options set
exactly to handle issues of missing fields at end of line and embedded
blanks. Do use them. It's all on the help page for read.table...

(read.csv2, read.delim2 in locales that use comma as decimal point)

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From shigesong at gmail.com  Thu Jun 23 09:52:33 2005
From: shigesong at gmail.com (Shige Song)
Date: Thu, 23 Jun 2005 15:52:33 +0800
Subject: [R] Another question about Rcmdr
Message-ID: <5abc11d80506230052ab81d22@mail.gmail.com>

Hi,

I downloaded the new R 2.1.1 source, compiled and installed on my suse
9.3 box. Then I installed the package Rcmdr. Surprisingly, I cannot
type anything onto the script window. I uninstalled R and grabbed an
rpm version from CRAN, installed it, and installed Rcmdr, same
problem!

Has anyone else had this problem?

Shige



From maechler at stat.math.ethz.ch  Thu Jun 23 10:12:06 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 23 Jun 2005 10:12:06 +0200
Subject: [R] classes with chron slots
In-Reply-To: <878y1286ad.fsf@gmail.com>
References: <878y1286ad.fsf@gmail.com>
Message-ID: <17082.28374.15302.156128@stat.math.ethz.ch>

>>>>> "Sebastian" == Sebastian Luque <spluque at gmail.com>
>>>>>     on Wed, 22 Jun 2005 15:50:50 -0500 writes:

    Sebastian> I'd like to define a class with a chron slot, but:
    R> require(chron)
    R> setClass("myclass", representation(datetime = "chron"))
    Sebastian> [1] "myclass"
    Sebastian> Warning message:
    Sebastian> undefined slot classes in definition of "myclass": datetime(class "chron")
    Sebastian> in: .completeClassSlots(ClassDef, where)

    Sebastian> How should such a class be defined?

You need to first make "chron" into a (pseudo-) S4 class, 
using  setOldClass()  

Then you can extend it or use it as slot class.

Martin Maechler, ETH Zurich



From c18g at zfn.uni-bremen.de  Thu Jun 23 11:11:05 2005
From: c18g at zfn.uni-bremen.de (Patrick Hausmann)
Date: Thu, 23 Jun 2005 11:11:05 +0200
Subject: [R]  How to read an excel data into R?
Message-ID: <1119517865.42ba7ca9dce33@www2.zfn.uni-bremen.de>

Hi,

you can use the library "RODBC" to import Excel-Files. This works for me:

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(RODBC)
setwd("C:\\R1B2")
channel <-  odbcConnectExcel("pk2003.xls")
tab     <-  sqlFetch(channel, "Tabelle3")
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

HTH
Patrick



From v.demartino2 at virgilio.it  Thu Jun 23 12:30:08 2005
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Thu, 23 Jun 2005 12:30:08 +0200
Subject: [R] Building a vector of dates between extreme dates
Message-ID: <429C8F5C0003040D@ims3e.cp.tin.it>

I would like to build a data.frame with a column of POSIXct dates orderly
differing  one day per row exactly, that is:

Data            dato1
01/06/2005   .......
02/06/2005   .......
..............
29/06/2005   ........
30/06/2005  etc


giving the extreme dates 01/06/2005 & 30/06/2005.


How  can i obtain this result?

Vittorio



From jenny_edmondson at hotmail.com  Thu Jun 23 12:49:10 2005
From: jenny_edmondson at hotmail.com (Jenny Edmondson)
Date: Thu, 23 Jun 2005 10:49:10 +0000
Subject: [R] errorest
Message-ID: <BAY106-F19DB1E341088A336FA1D2C9DEA0@phx.gbl>

Hi,

I am using errorest function from ipred package.
I am hoping to perform "bootstrap 0.632+" and "bootstrap leave one out". 
According to the manual page for errorest, i use the following command:

ce632[i]<-errorest(ytrain ~., data=mydata, model=lda, 
estimator=c("boot","632plus"),  predict=mypredict.lda)$error

It didn't work. I then tried the following two commands:

ce632[i]<-errorest(ytrain ~., data=mydata, model=lda, 
estimator=c("632plus"), est.para=control.errorest(nboot=B), 
predict=mypredict.lda)$error

ceLOOB[i]<-errorest(ytrain ~., data=mydata, model=lda, estimator=c("cv"), 
est.para=control.errorest(k=cv.k,nboot=B), predict=mypredict.lda)$error

They worked. However, as I didn't specify "boot" in estimator, I am 
wondering if "bootstrap" is still perfomred. I set nboot=B in est.para.

Thanks in advance.

Jenny



From ripley at stats.ox.ac.uk  Thu Jun 23 13:03:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 12:03:01 +0100 (BST)
Subject: [R] Building a vector of dates between extreme dates
In-Reply-To: <429C8F5C0003040D@ims3e.cp.tin.it>
Message-ID: <Pine.GSO.4.31.0506231200550.4814-100000@toucan.stats>

Use seq.POSIXct (or seq.Date since you seem to have dates and not
date-times), in just the same way you would use seq() for numbers.

On Thu, 23 Jun 2005 v.demartino2 at virgilio.it wrote:

> I would like to build a data.frame with a column of POSIXct dates orderly
> differing  one day per row exactly, that is:
>
> Data            dato1
> 01/06/2005   .......
> 02/06/2005   .......
> ..............
> 29/06/2005   ........
> 30/06/2005  etc
>
>
> giving the extreme dates 01/06/2005 & 30/06/2005.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Jun 23 13:24:37 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2005 13:24:37 +0200
Subject: [R] Building a vector of dates between extreme dates
In-Reply-To: <Pine.GSO.4.31.0506231200550.4814-100000@toucan.stats>
References: <Pine.GSO.4.31.0506231200550.4814-100000@toucan.stats>
Message-ID: <x2k6klb9je.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> Use seq.POSIXct (or seq.Date since you seem to have dates and not
> date-times), in just the same way you would use seq() for numbers.

Hmm, not quite. I have to admit that the thought didn't even occur to
me, but


> seq(from=as.Date("2005-06-01"), to=as.Date("2005-06-30"))
Error in seq.Date(from = as.Date("2005-06-01"), to = as.Date("2005-06-30")) :
        exactly two of 'to', 'by' and 'length.out' / 'along.with' must be specified

For some odd reason, by=1 needs to be given explicitly:

> seq(from=as.Date("2005-06-01"), to=as.Date("2005-06-30"), by=1)
 [1] "2005-06-01" "2005-06-02" "2005-06-03" "2005-06-04" "2005-06-05"
 [6] "2005-06-06" "2005-06-07" "2005-06-08" "2005-06-09" "2005-06-10"
[11] "2005-06-11" "2005-06-12" "2005-06-13" "2005-06-14" "2005-06-15"
[16] "2005-06-16" "2005-06-17" "2005-06-18" "2005-06-19" "2005-06-20"
[21] "2005-06-21" "2005-06-22" "2005-06-23" "2005-06-24" "2005-06-25"
[26] "2005-06-26" "2005-06-27" "2005-06-28" "2005-06-29" "2005-06-30"

or, of course, you don't really need keyword matching: 

> seq(as.Date("2005-06-01"), as.Date("2005-06-30"), 1)


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Jun 23 13:29:20 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 12:29:20 +0100 (BST)
Subject: [R] Building a vector of dates between extreme dates
In-Reply-To: <x2k6klb9je.fsf@turmalin.kubism.ku.dk>
References: <Pine.GSO.4.31.0506231200550.4814-100000@toucan.stats>
	<x2k6klb9je.fsf@turmalin.kubism.ku.dk>
Message-ID: <Pine.LNX.4.61.0506231225310.3610@gannet.stats>

On Thu, 23 Jun 2005, Peter Dalgaard wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>> Use seq.POSIXct (or seq.Date since you seem to have dates and not
>> date-times), in just the same way you would use seq() for numbers.
>
> Hmm, not quite. I have to admit that the thought didn't even occur to
> me, but
>
>
>> seq(from=as.Date("2005-06-01"), to=as.Date("2005-06-30"))
> Error in seq.Date(from = as.Date("2005-06-01"), to = as.Date("2005-06-30")) :
>        exactly two of 'to', 'by' and 'length.out' / 'along.with' must be specified
>
> For some odd reason, by=1 needs to be given explicitly:
>
>> seq(from=as.Date("2005-06-01"), to=as.Date("2005-06-30"), by=1)
> [1] "2005-06-01" "2005-06-02" "2005-06-03" "2005-06-04" "2005-06-05"
> [6] "2005-06-06" "2005-06-07" "2005-06-08" "2005-06-09" "2005-06-10"
> [11] "2005-06-11" "2005-06-12" "2005-06-13" "2005-06-14" "2005-06-15"
> [16] "2005-06-16" "2005-06-17" "2005-06-18" "2005-06-19" "2005-06-20"
> [21] "2005-06-21" "2005-06-22" "2005-06-23" "2005-06-24" "2005-06-25"
> [26] "2005-06-26" "2005-06-27" "2005-06-28" "2005-06-29" "2005-06-30"
>
> or, of course, you don't really need keyword matching:
>
>> seq(as.Date("2005-06-01"), as.Date("2005-06-30"), 1)

Yes, because time units are needed: for POSIXct you also have to decide if 
you need days or DSTdays.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bitwrit at ozemail.com.au  Thu Jun 23 23:54:22 2005
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 23 Jun 2005 21:54:22 +0000
Subject: [R] A polar.plot BUG in plotrix 1.3.3 ?
In-Reply-To: <42B95BB6.4090507@vedur.is>
References: <42B95BB6.4090507@vedur.is>
Message-ID: <42BB2F8E.3020904@ozemail.com.au>

Halldor Bj??rnsson wrote:
 >...
> I have attatched the polar.plot function from the two different versions 
> of the package.
> 
> It seems that if not missing  then label.pos is not changed at all in 
> version 1.3.3
> 
> So, a feature or a bug?
> 
Looks like a typo to me. Thanks for pointing it out. I always run the 
examples after building a new version of the package, but this didn't 
pick up the missing "!". I'll upload plotrix_1.3-5 tonight with this fixed.

Jim



From carsten.steinhoff at stud.uni-goettingen.de  Thu Jun 23 13:56:03 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Thu, 23 Jun 2005 13:56:03 +0200
Subject: [R] solving equation system
Message-ID: <E1DlQJp-00031h-Q1@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050623/e21cf4f2/attachment.pl

From ggrothendieck at gmail.com  Thu Jun 23 14:18:59 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 08:18:59 -0400
Subject: [R] classes with chron slots
In-Reply-To: <17082.28374.15302.156128@stat.math.ethz.ch>
References: <878y1286ad.fsf@gmail.com>
	<17082.28374.15302.156128@stat.math.ethz.ch>
Message-ID: <971536df05062305184ad4a390@mail.gmail.com>

On 6/23/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Sebastian" == Sebastian Luque <spluque at gmail.com>
> >>>>>     on Wed, 22 Jun 2005 15:50:50 -0500 writes:
> 
>    Sebastian> I'd like to define a class with a chron slot, but:
>    R> require(chron)
>    R> setClass("myclass", representation(datetime = "chron"))
>    Sebastian> [1] "myclass"
>    Sebastian> Warning message:
>    Sebastian> undefined slot classes in definition of "myclass": datetime(class "chron")
>    Sebastian> in: .completeClassSlots(ClassDef, where)
> 
>    Sebastian> How should such a class be defined?
> 
> You need to first make "chron" into a (pseudo-) S4 class,
> using  setOldClass()
> 
> Then you can extend it or use it as slot class.
> 

Also if you are referring to the chron package then 
the names of the chron classes are 'dates' and 'times' -- not 'chron'.

R> library(chron)
R> example(chron)
...snip...
R> class(dts)
[1] "dates" "times"



From ggrothendieck at gmail.com  Thu Jun 23 14:20:44 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 08:20:44 -0400
Subject: [R] classes with chron slots
In-Reply-To: <17082.28374.15302.156128@stat.math.ethz.ch>
References: <878y1286ad.fsf@gmail.com>
	<17082.28374.15302.156128@stat.math.ethz.ch>
Message-ID: <971536df05062305205a1b9512@mail.gmail.com>

On 6/23/05, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >>>>> "Sebastian" == Sebastian Luque <spluque at gmail.com>
> >>>>>     on Wed, 22 Jun 2005 15:50:50 -0500 writes:
> 
>    Sebastian> I'd like to define a class with a chron slot, but:
>    R> require(chron)
>    R> setClass("myclass", representation(datetime = "chron"))
>    Sebastian> [1] "myclass"
>    Sebastian> Warning message:
>    Sebastian> undefined slot classes in definition of "myclass": datetime(class "chron")
>    Sebastian> in: .completeClassSlots(ClassDef, where)
> 
>    Sebastian> How should such a class be defined?
> 
> You need to first make "chron" into a (pseudo-) S4 class,
> using  setOldClass()
> 
> Then you can extend it or use it as slot class.
> 


Also if you are referring to the chron package then 
the names of the chron classes are 'dates' and 'times' -- not 'chron'.

R> library(chron)
R> example(chron)
...snip...
R> class(dts)
[1] "dates" "times"



From roger.bos at gmail.com  Thu Jun 23 14:29:40 2005
From: roger.bos at gmail.com (roger bos)
Date: Thu, 23 Jun 2005 08:29:40 -0400
Subject: [R] How to read an excel data into R?
In-Reply-To: <1119517865.42ba7ca9dce33@www2.zfn.uni-bremen.de>
References: <1119517865.42ba7ca9dce33@www2.zfn.uni-bremen.de>
Message-ID: <1db7268005062305291dd971af@mail.gmail.com>

This is really great.  I use odbc for sql all the time, but I never
needed to read in excel files before.  I needed to yesterday and I
looked at read.xls() from library(gdata) and it took 5-10 minutes to
read in the file and odbc did it in 5 seconds!

I guess that is the good thing about having duplication in function in
R, we can try several methods and choose which one is best/fastest. 
Thanks for the example.

On 6/23/05, Patrick Hausmann <c18g at zfn.uni-bremen.de> wrote:
> Hi,
> 
> you can use the library "RODBC" to import Excel-Files. This works for me:
> 
> #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> library(RODBC)
> setwd("C:\\R1B2")
> channel <-  odbcConnectExcel("pk2003.xls")
> tab     <-  sqlFetch(channel, "Tabelle3")
> #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> HTH
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From charles-r-nospam at plessy.org  Thu Jun 23 14:32:46 2005
From: charles-r-nospam at plessy.org (Charles Plessy)
Date: Thu, 23 Jun 2005 21:32:46 +0900
Subject: [R] heatmap not symmetric ?
Message-ID: <20050623123246.GF27261@kunpuu.plessy.org>

Dear list,

I hope it is not a FAQ, but I searched the archives and Google, and
found nothing. The question is simple :

I do not understand why, starting from a symmetrical correlation matrix,
heatmap produces an asymmetrical image.

Best,

Charles

-- 
Charles Plessy, Ph.D. - Genome Science Laboratory
The Institute for Physical and Chemical Research (RIKEN)
2-1 Hirosawa, Wako, Saitama 351-0198, Japan
Fax: 048-462-4686  --  Tel: 048-467-9515



From andy_liaw at merck.com  Thu Jun 23 14:36:58 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 23 Jun 2005 08:36:58 -0400
Subject: [R] heatmap not symmetric ?
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64E9EE@usctmx1106.merck.com>

> From: Charles Plessy
> 
> Dear list,
> 
> I hope it is not a FAQ, but I searched the archives and Google, and
> found nothing. The question is simple :
> 
> I do not understand why, starting from a symmetrical 
> correlation matrix,
> heatmap produces an asymmetrical image.

Umm... because you haven't read the help page for heatmap carefully enough?
It says:

symm    logical indicating if x should be treated symmetrically; can only be
true when x is a square matrix. 

I believe Martin added this after some discussion on R-help.

Andy


> Best,
> 
> Charles
> 
> -- 
> Charles Plessy, Ph.D. - Genome Science Laboratory
> The Institute for Physical and Chemical Research (RIKEN)
> 2-1 Hirosawa, Wako, Saitama 351-0198, Japan
> Fax: 048-462-4686  --  Tel: 048-467-9515
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From pmbrando at ipam.org.br  Thu Jun 23 14:50:58 2005
From: pmbrando at ipam.org.br (Paulo Brando)
Date: Thu, 23 Jun 2005 05:50:58 -0700
Subject: [R] RES:  How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
Message-ID: <001801c577f2$373f3f50$f10aa8c0@paulobrando>

Hi ling, save your file as 'csv'.

I always use the following script:


NAME <- read.table("C:LOCATION/FILE NAME.csv", header = TRUE,  sep =
",", na.string=".")



________________________________________
Paulo M. Brando
Instituto de Pesquisa Ambiental da Amazonia (IPAM)
Santarem, PA, Brasil.
Av. Rui Barbosa, 136.
Fone: + 55 93 522 55 38
www.ipam.org.br
E-mail: pmbrando at ipam.org.br



-----Mensagem original-----
De: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] Em nome de Ling Jin
Enviada em: Wednesday, June 22, 2005 4:47 PM
Para: r-help at stat.math.ethz.ch
Assunto: [R] How to read an excel data into R?

Hi all,

Does anybody know the easiest way to import excel data into R? I copied 
and pasted the excel data into a txt file, and tried read.table, but R 
reported that

Error in read.table("data_support.txt", sep = " ", header = T) :
         more columns than column names

Thanks!

Ling

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From leinwebe at eva.mpg.de  Thu Jun 23 14:59:31 2005
From: leinwebe at eva.mpg.de (Marcus Leinweber)
Date: Thu, 23 Jun 2005 14:59:31 +0200
Subject: [R] grep negation
Message-ID: <42BAB233.4050906@eva.mpg.de>

hi,

using the example in the grep help:
txt <- c("arm","foot","lefroo", "bafoobar")
i <- grep("foo",txt); i
[1] 2 4

but how can i get the negation (1,3) when looking for 'foo'?

thanks,
m.



From james.holtman at convergys.com  Thu Jun 23 15:04:14 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Thu, 23 Jun 2005 09:04:14 -0400
Subject: [R] grep negation
In-Reply-To: <42BAB233.4050906@eva.mpg.de>
Message-ID: <OF03128DB5.FAD776C4-ON85257029.004798E0-85257029.0047CDF7@nd.convergys.com>





?setdiff

e.g.,

> txt <- c("arm","foot","lefroo", "bafoobar")
> i <- grep("foo",txt); i
[1] 2 4
> setdiff(seq(length(txt)),grep("foo",txt))
[1] 1 3
>


Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Marcus Leinweber                                                                                                     
                      <leinwebe at eva.mpg.de>        To:       "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>                       
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] grep negation                                                             
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      06/23/2005 08:59                                                                                                     
                                                                                                                                           




hi,

using the example in the grep help:
txt <- c("arm","foot","lefroo", "bafoobar")
i <- grep("foo",txt); i
[1] 2 4

but how can i get the negation (1,3) when looking for 'foo'?

thanks,
m.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From khobson at fd9ns01.okladot.state.ok.us  Thu Jun 23 15:09:11 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Thu, 23 Jun 2005 08:09:11 -0500
Subject: [R] Stop Warnings for Invalid Factor Level, NAs generated?
Message-ID: <OF9F1111A8.02426A50-ON86257029.0046D4C9-86257029.00482247@fd9ns01.okladot.state.ok.us>





How can I stop the following warning from occuring?
invalid factor level, NAs generated in: "[<-.factor"(`*tmp*`, iseq, value =
structure(1, .Label = "12", class = "factor"))

The Label messages are for "5", "8", "12" and "46". I want the NAs to be
generated as needed.

Is this causing R to slow down by generating the warning messages?  There
are over 50.  I loop through up to 10 datasets and append one row from each
to create a summary dataset.

The full code and data is too long to post.  The snippet below might
explain what I'm doing somewhat.  I'll work up some sample data and code if
no solutions are found.

...snip
> # Get info on first and last pair sets and lab names in last pair
> nlabs <- labdata[["LABNUMBER"]] #e.g. 1, 2, 5, 6, ...
> nolabs <- length(nlabs) #Total number of labs in last pair of labdata
> #dpdata <- labdata[which(labdata[["LABNUMBER"]] == 13),] #Dummy paired
data row
> dpdata <- rbind(labdata[0,], NA)
> pdata <- dpdata
> # Loop through pairs of labdata to build pdata for pairs (last 10 years
or less)
> k=0
> for(j in nlabs){
+   for(i in ifpair:ilpair){
+     k <- k + 1  #Counter for number of pairs.  Start at 1.
+     setwd(dirs[i]); load("labdata.Rdata")    #No trailing "/" in wd
+     setwd(cdir) #Go back to original ESN pair (last pair's folder in
analysis)
+     tpdata <- labdata[which(labdata[["LABNUMBER"]] == j),]
+     if(NROW(tpdata) == 0){
+       tpdata <- dpdata
+       tpdata[which(names(labdata) == "LABNUMBER")] <- j
+       tpdata[which(names(labdata) == "ESN")] <-
ESN-((ilpair-ifpair)+1-k)*2
+             tpdata[which(names(labdata) == "ESNm1")] <-
ESN-((ilpair-ifpair)+1-k)*2-1
+     }
+   #pdata[nrow(pdata)+1,names(tpdata)] <- NA
+   pdata[nrow(pdata)+1,names(tpdata)] <- tpdata
+   }
+ }
There were 50 or more warnings (use warnings() to see the first 50)
...snip

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From dimitris.rizopoulos at med.kuleuven.be  Thu Jun 23 15:25:20 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Thu, 23 Jun 2005 15:25:20 +0200
Subject: [R] grep negation
References: <42BAB233.4050906@eva.mpg.de>
Message-ID: <004401c577f7$00deb530$0540210a@www.domain>

try this:

seq(along = txt)[-i]


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Marcus Leinweber" <leinwebe at eva.mpg.de>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 23, 2005 2:59 PM
Subject: [R] grep negation


> hi,
>
> using the example in the grep help:
> txt <- c("arm","foot","lefroo", "bafoobar")
> i <- grep("foo",txt); i
> [1] 2 4
>
> but how can i get the negation (1,3) when looking for 'foo'?
>
> thanks,
> m.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spluque at gmail.com  Thu Jun 23 15:29:09 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Thu, 23 Jun 2005 08:29:09 -0500
Subject: [R] classes with chron slots
In-Reply-To: <971536df05062305205a1b9512@mail.gmail.com>
References: <878y1286ad.fsf@gmail.com>
	<17082.28374.15302.156128@stat.math.ethz.ch>
	<971536df05062305205a1b9512@mail.gmail.com>
Message-ID: <87psud9p7e.fsf@gmail.com>

Thanks a lot Martin and Gabor!


On Thu, 23 Jun 2005 08:20:44 -0400,
Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

[...]

> Also if you are referring to the chron package then 
> the names of the chron classes are 'dates' and 'times' -- not 'chron'.

However, when the object has both dates and times, then the class becomes
'chron', inheriting both 'dates' and 'times':

R> class(x)
[1] "chron" "dates" "times"

in the same example.

Cheers,
Sebastian
-- 
Sebastian P. Luque



From dmbates at gmail.com  Thu Jun 23 15:32:41 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 23 Jun 2005 08:32:41 -0500
Subject: [R] contrats hardcoded in aov()?
In-Reply-To: <20050623150759.0b74ac48@zygiella.local>
References: <20050623150759.0b74ac48@zygiella.local>
Message-ID: <40e66e0b050623063246dbf7f9@mail.gmail.com>

On 6/23/05, RenE J.V. Bertin <rjvbertin at gmail.com> wrote:
> Hello,
> 
> I was just having a look at the aov function source code, and see that when the model used does not have an Error term, Helmert contrasts are imposed:
> 
>     if (is.null(indError)) {
> ...
>    }
>     else {
>         opcons <- options("contrasts")
>         options(contrasts = c("contr.helmert", "contr.poly"))
>         on.exit(options(opcons))
> ...
> 
> 
> My reading of several contributed user guides' sections on ANOVA is that Helmert contrasts are not intuitive at all and best avoided by non-expert users. This explains why I didn't see any influence of the various contrast settings on my results, and I wonder why this local shadowing of global settings is done?

An aov model is intended to produce just the analysis of variance
table for which the choice of contrasts is irrelevant.  If you do want
to examine individual coefficients then fit the model using lm().



From jfox at mcmaster.ca  Thu Jun 23 15:34:51 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 23 Jun 2005 09:34:51 -0400
Subject: [R] How to save changed options in Rcmdr
In-Reply-To: <5abc11d80506222218de16589@mail.gmail.com>
Message-ID: <20050623133450.YADN26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Shige,

You can set appropriate options when R starts up. In R for Windows, the
easiest thing to do is probably to put an options() command in the Rprofile
file in R's etc directory. For more information on setting Rcmdr options,
see ?Commander (also accessible from the Rcmdr Help menu). For more
information on R initialization, see ?Startup.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shige Song
> Sent: Thursday, June 23, 2005 12:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to save changed options in Rcmdr
> 
> Dear All,
> 
> I want to change the default options of Rcmdr; it seemed to 
> work when I made changes and click the "Exit and Restart R 
> Commander". However, next time I open Rcmdr, it automatically 
> restored to the default options. Is there a way to change 
> Rcmdr's options permanently? Thanks!
> 
> Shige
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at gmail.com  Thu Jun 23 16:01:15 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 10:01:15 -0400
Subject: [R] classes with chron slots
In-Reply-To: <87psud9p7e.fsf@gmail.com>
References: <878y1286ad.fsf@gmail.com>
	<17082.28374.15302.156128@stat.math.ethz.ch>
	<971536df05062305205a1b9512@mail.gmail.com> <87psud9p7e.fsf@gmail.com>
Message-ID: <971536df05062307014f3823b@mail.gmail.com>

On 6/23/05, Sebastian Luque <spluque at gmail.com> wrote:
> Thanks a lot Martin and Gabor!
> 
> 
> On Thu, 23 Jun 2005 08:20:44 -0400,
> Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> [...]
> 
> > Also if you are referring to the chron package then
> > the names of the chron classes are 'dates' and 'times' -- not 'chron'.
> 
> However, when the object has both dates and times, then the class becomes
> 'chron', inheriting both 'dates' and 'times':
> 
> R> class(x)
> [1] "chron" "dates" "times"


Did you try the example I posted?  Its self contained and reproducible and 
it inherits from both "dates" and "times" yet there is no "chron" in the class 
vector, at least in that case.



From p.dalgaard at biostat.ku.dk  Thu Jun 23 16:12:22 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2005 16:12:22 +0200
Subject: [R] contrats hardcoded in aov()?
In-Reply-To: <20050623150759.0b74ac48@zygiella.local>
References: <20050623150759.0b74ac48@zygiella.local>
Message-ID: <x2wtolnovt.fsf@turmalin.kubism.ku.dk>

"RenE J.V. Bertin" <rjvbertin at gmail.com> writes:

> Hello,
> 
> I was just having a look at the aov function source code, and see that when the model used does not have an Error term, Helmert contrasts are imposed:
> 
>     if (is.null(indError)) {
> ... 
>    }
>     else {
>         opcons <- options("contrasts")
>         options(contrasts = c("contr.helmert", "contr.poly"))
>         on.exit(options(opcons))
> ...
> 
> 
> My reading of several contributed user guides' sections on ANOVA is
> that Helmert contrasts are not intuitive at all and best avoided by
> non-expert users. This explains why I didn't see any influence of
> the various contrast settings on my results, and I wonder why this
> local shadowing of global settings is done?

I believe the reason is that you get in trouble further down the line
if the contrast matrices do not sum to zero over rows, which basically
implies contr.helmert or contr.sum. 

There is some disagreement over whether this is a natural requirement
for contrast matrices (Brian and I have discussed this previously, on
this list I believe), but this is technical: the code assumes it and
gives wrong results if it isn't true.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ggrothendieck at gmail.com  Thu Jun 23 16:13:31 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 10:13:31 -0400
Subject: [R] classes with chron slots
In-Reply-To: <971536df05062307014f3823b@mail.gmail.com>
References: <878y1286ad.fsf@gmail.com>
	<17082.28374.15302.156128@stat.math.ethz.ch>
	<971536df05062305205a1b9512@mail.gmail.com> <87psud9p7e.fsf@gmail.com>
	<971536df05062307014f3823b@mail.gmail.com>
Message-ID: <971536df05062307135023b3ba@mail.gmail.com>

On 6/23/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/23/05, Sebastian Luque <spluque at gmail.com> wrote:
> > Thanks a lot Martin and Gabor!
> >
> >
> > On Thu, 23 Jun 2005 08:20:44 -0400,
> > Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> >
> > [...]
> >
> > > Also if you are referring to the chron package then
> > > the names of the chron classes are 'dates' and 'times' -- not 'chron'.
> >
> > However, when the object has both dates and times, then the class becomes
> > 'chron', inheriting both 'dates' and 'times':
> >
> > R> class(x)
> > [1] "chron" "dates" "times"
> 
> 
> Did you try the example I posted?  Its self contained and reproducible and
> it inherits from both "dates" and "times" yet there is no "chron" in the class
> vector, at least in that case.
> 

Just to be clear, the point is that you need to be sure that your chron
objects really have class "chron" since one often speaks of chron
objects that are of class dates or times but not chron.

This may or may not be a problem for you but I thought it worthwhile pointing
out just in case.



From jfox at mcmaster.ca  Thu Jun 23 16:27:51 2005
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 23 Jun 2005 10:27:51 -0400
Subject: [R] Another question about Rcmdr
In-Reply-To: <5abc11d80506230052ab81d22@mail.gmail.com>
Message-ID: <20050623142750.YYXO26102.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Shige,

I'm afraid that my Linux system isn't up-to-date (and isn't a Suse system).
Since I'm about to leave town, I won't be able to check this out further
now. I expect that if the problem were general, I'd have heard of it before,
but I can't be sure that's the case. Perhaps someone else has used the Rcmdr
under R 2.1.1 on a Linux system and can report success or failure.

I'm sorry that I can't be of more help,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Shige Song
> Sent: Thursday, June 23, 2005 2:53 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Another question about Rcmdr
> 
> Hi,
> 
> I downloaded the new R 2.1.1 source, compiled and installed on my suse
> 9.3 box. Then I installed the package Rcmdr. Surprisingly, I 
> cannot type anything onto the script window. I uninstalled R 
> and grabbed an rpm version from CRAN, installed it, and 
> installed Rcmdr, same problem!
> 
> Has anyone else had this problem?
> 
> Shige
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jun 23 16:40:25 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 15:40:25 +0100 (BST)
Subject: [R] contrats hardcoded in aov()?
In-Reply-To: <20050623150759.0b74ac48@zygiella.local>
References: <20050623150759.0b74ac48@zygiella.local>
Message-ID: <Pine.LNX.4.61.0506231507510.5521@gannet.stats>

The source code actually says

         ##  helmert contrasts can be helpful: do we want to force them?
         ##  this version does for the Error model.
         opcons <- options("contrasts")
         ....

and do note it is unset again a few lines later.  (Listing a function 
inside R is not giving you the `source code'.)

So Helmert contrasts are only used for computing the strata, nothing to do 
with your comment as the user never gets to see anything other than the 
projections onto strata.  Having a nearly orthogonal parametrization helps 
numerical stability in e.g. establishing if projections are orthogonal.

On Thu, 23 Jun 2005, RenE J.V. Bertin wrote:

> Hello,
>
> I was just having a look at the aov function source code, and see that 
> when the model used does not have an Error term, Helmert contrasts are 
> imposed:

If this is really the `source code' as you claim, why did you remove the 
very helpful comment?

>    if (is.null(indError)) {
> ...
>   }
>    else {
>        opcons <- options("contrasts")
>        options(contrasts = c("contr.helmert", "contr.poly"))
>        on.exit(options(opcons))
> ...
>
>
> My reading of several contributed user guides' sections on ANOVA is that 
> Helmert contrasts are not intuitive at all and best avoided by 
> non-expert users. This explains why I didn't see any influence of the 
> various contrast settings on my results, and I wonder why this local 
> shadowing of global settings is done?

As others have noted:

1) The AoV itself does not depend on the coding used.

2) Some other things you can do with an aov fit do, and there you want an
orthogonal parametrization if possible, both for ease of interpretation 
and numerical stability.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spluque at gmail.com  Thu Jun 23 17:01:13 2005
From: spluque at gmail.com (Sebastian Luque)
Date: Thu, 23 Jun 2005 10:01:13 -0500
Subject: [R] classes with chron slots
In-Reply-To: <971536df05062307135023b3ba@mail.gmail.com>
References: <878y1286ad.fsf@gmail.com>
	<17082.28374.15302.156128@stat.math.ethz.ch>
	<971536df05062305205a1b9512@mail.gmail.com> <87psud9p7e.fsf@gmail.com>
	<971536df05062307014f3823b@mail.gmail.com>
	<971536df05062307135023b3ba@mail.gmail.com>
Message-ID: <87aclh9kxy.fsf@gmail.com>

On Thu, 23 Jun 2005 10:13:31 -0400,
Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

[...]

> Just to be clear, the point is that you need to be sure that your chron
> objects really have class "chron" since one often speaks of chron
> objects that are of class dates or times but not chron.

That's right, I was just pointing out that there *is* a 'chron' class,
which is an extension of both 'dates' and 'times', as shown by object 'x'
in the examples we were talking about.

Cheers,
-- 
Sebastian P. Luque



From 0034058 at fudan.edu.cn  Thu Jun 23 17:12:17 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 23 Jun 2005 23:12:17 +0800
Subject: [R] the dimname of a table
Message-ID: <20050623231217.06fbbffb@localhost.localdomain>

i have a data frame(dat) which has many variables.and i use the following script to get the crosstable.

>danx2<-c("x1.1","x1.2","x1.3","x1.4","x1.5","x2","x4","x5","x6","x7","x8.1","x8.2","x8.3","x8.4","x11",
"x13","x17","x19","x20","x21")
>indep<-c("x23","x24","x25","x26","x27","x28.1","x28.2","x29")
>for (k in indep){
  for (i in danx2){
     a<-chisq.test(dat[,i],dat[,k])$p.v<=0.05
     if (a) {CrossTable(dat[,i],dat[,k],chisq=T,format="SPSS");cat(rep("=",50),"\n","\n")}
 }

  it has a little pitfall:the dimnames of table is dat[,i] and dat[,k],but i want it to be like x2,x23... 
is there any good way to do this?
  and in the command CrossTable(dat[,i],dat[,k],chisq=T,format="SPSS") in the loop,is there any other way to get the variable other than dat[,i] and dat[,k]?
  thank you !


 
             | dat[,k] 
     dat[,i] |        1  |        2  |        3  |        4  |        5  |       10  | Row Total | 
-------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
           1 |       97  |       10  |       38  |       21  |       15  |        0  |      181  | 
             |    2.184  |    0.266  |    2.155  |    0.033  |    0.030  |      NaN  |           | 
             |   53.591% |    5.525% |   20.994% |   11.602% |    8.287% |    0.000% |   56.037% | 
             |   65.101% |   47.619% |   44.186% |   53.846% |   53.571% |      NaN% |           | 
             |   30.031% |    3.096% |   11.765% |    6.502% |    4.644% |    0.000% |           | 
-------------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
           2 |       29  |        5  |       27  |        4  |        5  |        0  |       70  | 
             |    0.335  |    0.044  |    3.752  |    2.345  |    0.188  |      NaN  |           | 

..........

-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From macq at llnl.gov  Thu Jun 23 17:15:12 2005
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 23 Jun 2005 08:15:12 -0700
Subject: [R] grep negation
In-Reply-To: <42BAB233.4050906@eva.mpg.de>
References: <42BAB233.4050906@eva.mpg.de>
Message-ID: <p06210200bee0813e46ea@[128.115.153.6]>

If all you need to do is extract the subset of elements of txt that 
do not contain 'foo', then

   txt[-i]

will do the job. Provided that at east one element of txt contains 
'foo', that is.

-Don

At 2:59 PM +0200 6/23/05, Marcus Leinweber wrote:
>hi,
>
>using the example in the grep help:
>txt <- c("arm","foot","lefroo", "bafoobar")
>i <- grep("foo",txt); i
>[1] 2 4
>
>but how can i get the negation (1,3) when looking for 'foo'?
>
>thanks,
>m.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Max.Kuhn at pfizer.com  Thu Jun 23 17:17:58 2005
From: Max.Kuhn at pfizer.com (Kuhn, Max)
Date: Thu, 23 Jun 2005 11:17:58 -0400
Subject: [R]  errorest
Message-ID: <71257D09F114DA4A8E134DEAC70F25D30134D3BA@groamrexm03.amer.pfizer.com>

Jenny,

"It didn't work" and "They worked" aren't very specific. Also, the package
name is ipred and the function is errorest.

The estimator entry on the man page for errorest has:

   'cv' cross-validation, 'boot' bootstrap or '632plus' bias corrected
bootstrap (classification only). 

Note the *or*. I tried the analysis of the iris data from the man page with
your estimator specification:

> testing <-   errorest(Species ~ ., data=iris, model=lda, 
+ estimator = c("boot","632plus"), predict= mypredict.lda)
> testing

Call:
errorest.data.frame(formula = Species ~ ., data = iris, model = lda, 
    predict = mypredict.lda, estimator = c("boot", "632plus"))

         Bootstrap estimator of misclassification error 
         with 25 bootstrap replications

Misclassification error:  0.0235 
Standard deviation: 0.0028 


Call:
errorest.data.frame(formula = Species ~ ., data = iris, model = lda, 
    predict = mypredict.lda, estimator = c("boot", "632plus"))

         .632+ Bootstrap estimator of misclassification error 
         with 25 bootstrap replications

Misclassification error:  0.0222 
>               
> unclass(testing)
$boot
$boot$error
[1] 0.02351852

$boot$sd
[1] 0.002847447

$boot$bc632plus
[1] FALSE

$boot$nboot
[1] 25


$"632plus"
$"632plus"$error
[1] 0.02222817

$"632plus"$nboot
[1] 25

$"632plus"$bc632plus
[1] TRUE


$call
errorest.data.frame(formula = Species ~ ., data = iris, model = lda, 
    predict = mypredict.lda, estimator = c("boot", "632plus"))

Is this consistent with your results?

Max


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From 0034058 at fudan.edu.cn  Thu Jun 23 17:22:15 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Thu, 23 Jun 2005 23:22:15 +0800
Subject: [R] how to get such crosstable?
Message-ID: <20050623232215.0e11490f@localhost.localdomain>


i use the CrossTable (frome gregmic package) function to get such a table as below.
but the percentage of the non-NA levels(here 1,2,3,4,5) is not totally 100%.

is there any way to get a table that percentage of  the non-NA levelsis totally 100%,as the SPSS' valid percentage.thank you!



   Cell Contents
|-------------------------|
|                   Count |
|             Row Percent |
|-------------------------|

Total Observations in Table:  650 

          |        1  |        2  |        3  |        4  |        5  | 
          |-----------|-----------|-----------|-----------|-----------|
          |      169  |      294  |      151  |       31  |        5  | 
          |    0.260% |    0.452% |    0.232% |    0.048% |    0.008% | 
          |-----------|-----------|-----------|-----------|-----------|

Number of Missing Observations: 4 (0.6116208%)

-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From bld at math.umd.edu  Thu Jun 23 17:42:57 2005
From: bld at math.umd.edu (Bernard L. Dillard)
Date: Thu, 23 Jun 2005 11:42:57 -0400 (EDT)
Subject: [R] (no subject)
Message-ID: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>

Hello Rers:

Let's say I have a column that looks like this:

Column
167.8
292.8
363.3
1.9
115.25

I want to manipulate the above data such that I only end up with this same
column but only with its decimals and not its whole numbers.  So the new
column would look like this (but the real column is VERY long):

Column
.8
.8
.3
.9
.25

Any suggestions about how I could do this?


-- 
Do all you can with what you have in the time you have in the place you are!

-Nkosi Johnson, 12-year old African hero



From p.dalgaard at biostat.ku.dk  Thu Jun 23 17:49:28 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2005 17:49:28 +0200
Subject: [R] (no subject)
In-Reply-To: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>
References: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>
Message-ID: <x2k6klnkdz.fsf@turmalin.kubism.ku.dk>

"Bernard L. Dillard" <bld at math.umd.edu> writes:

> Hello Rers:
> 
> Let's say I have a column that looks like this:
> 
> Column
> 167.8
> 292.8
> 363.3
> 1.9
> 115.25
> 
> I want to manipulate the above data such that I only end up with this same
> column but only with its decimals and not its whole numbers.  So the new
> column would look like this (but the real column is VERY long):
> 
> Column
> .8
> .8
> .3
> .9
> .25
> 
> Any suggestions about how I could do this?

Column - floor(Column)

or even

Column %% 1

negative numbers may get you in some trouble, though.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From sundar.dorai-raj at pdf.com  Thu Jun 23 17:54:02 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 23 Jun 2005 08:54:02 -0700
Subject: [R] (no subject)
In-Reply-To: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>
References: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>
Message-ID: <42BADB1A.9030106@pdf.com>


Bernard L. Dillard wrote:
> Hello Rers:
> 
> Let's say I have a column that looks like this:
> 
> Column
> 167.8
> 292.8
> 363.3
> 1.9
> 115.25
> 
> I want to manipulate the above data such that I only end up with this same
> column but only with its decimals and not its whole numbers.  So the new
> column would look like this (but the real column is VERY long):
> 
> Column
> .8
> .8
> .3
> .9
> .25
> 
> Any suggestions about how I could do this?
> 
> 

(Please use an informative subject next time).

Try

x - floor(x)

--sundar



From ripley at stats.ox.ac.uk  Thu Jun 23 17:55:19 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 16:55:19 +0100 (BST)
Subject: [R] contrats hardcoded in aov()?
In-Reply-To: <20050623173714.5849632c@zygiella.local>
References: <20050623150759.0b74ac48@zygiella.local>
	<Pine.LNX.4.61.0506231507510.5521@gannet.stats>
	<20050623173714.5849632c@zygiella.local>
Message-ID: <Pine.LNX.4.61.0506231648040.6640@gannet.stats>

On Thu, 23 Jun 2005, RenE J.V. Bertin wrote:

> On Thu, 23 Jun 2005 15:40:25 +0100 (BST), Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote regarding
> "Re: [R] contrats hardcoded in aov()?"
>
> Thanks to all the others who replied.
>
> 8-) and do note it is unset again a few lines later.  (Listing a function
> 8-) inside R is not giving you the `source code'.)
>
> 	Indeed. Ahem. So Yuelin was right (even if his example didn't use 
> an Error term). I was misled by the onexit statement, I suppose. Stupid.

The on.exit (sic) is needed in case you interrupt R at that point.

>
> 8-)
> 8-) If this is really the `source code' as you claim, why did you remove the
> 8-) very helpful comment?
>
> 	Trust me, I didn't, at least not on a purpose that I'm aware of. I did a fix(aov), and this is what I got. I did run an update of my installed packages last week (via the Mac GUI's R Package Installer interface). Maybe that stripped comments? I do not appear to unset something like keep.source in my startup files.


Let me say it again:

     Listing a function inside R is not giving you the `source code'

You need to look in the *sources* to get the source code.

>
> 8-) 2) Some other things you can do with an aov fit do, and there you want an
> 8-) orthogonal parametrization if possible, both for ease of interpretation
> 8-) and numerical stability.
>
> 	In other words, you are advising against changing this setting, if 
> I understand correctly?

If you are trying to do expert things with aov() you will know all this, 
and if not perhaps you should read up some of the expert accounts.  There 
are things for which contr.helmert() is a good choice, and some for which 
contr.treatment does not meet the traditional definition of `contrast' 
used in this area and so will be misleading.



From ccleland at optonline.net  Thu Jun 23 17:55:37 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 23 Jun 2005 11:55:37 -0400
Subject: [R] (no subject)
In-Reply-To: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>
References: <59035.66.92.23.42.1119541377.squirrel@66.92.23.42>
Message-ID: <42BADB79.9090008@optonline.net>

 > x <- c(167.8, 292.8, 363.3, 1.9, 115.25)

 > my.decimals <- x - trunc(x)

 > my.decimals
[1] 0.80 0.80 0.30 0.90 0.25

Bernard L. Dillard wrote:
> Hello Rers:
> 
> Let's say I have a column that looks like this:
> 
> Column
> 167.8
> 292.8
> 363.3
> 1.9
> 115.25
> 
> I want to manipulate the above data such that I only end up with this same
> column but only with its decimals and not its whole numbers.  So the new
> column would look like this (but the real column is VERY long):
> 
> Column
> .8
> .8
> .3
> .9
> .25
> 
> Any suggestions about how I could do this?
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From spencer.graves at pdf.com  Thu Jun 23 18:30:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Jun 2005 09:30:30 -0700
Subject: [R] solving equation system
In-Reply-To: <E1DlQJp-00031h-Q1@s2.stud.uni-goettingen.de>
References: <E1DlQJp-00031h-Q1@s2.stud.uni-goettingen.de>
Message-ID: <42BAE3A6.3010604@pdf.com>

	  Have you considered writing a function to compute the sum of squares 
of deviations from equality and using "optim"?  I use sum of squares not 
sum of absolute values, because if my functions are differentiable, the 
sum of squares will also be differentible while the sum of absolute 
values will not be.  This means that sum of absolute values will not 
work well with a quasi-Newton algorithm.

	  Also, have you considered making plots?  If I understand your 
example, you can solve for lambda using (II) as lambda = x/mean(X). 
Then you can use (I) to solve for "c".  To understand this, it would 
help to plot the digamma function.  If you do this (e.g., 
http://mathworld.wolfram.com/DigammaFunction.html), you will see that 
there are countably infinite solutions to this equation.  If you want 
the positive solution, I suggest you try to solve for ln.c = log(c) 
rather than "c" directly, because that should make "optim" more stable. 
  More generally, it often helps to make, e.g., contour or perspective 
plots and to try to find a parameterization that will make the sum of 
squares of errors approximatly parabolic in your parameters.

	  My favorite reference on this is Bates and Watts (1988) Nonlinear 
Regression Analysis and Its Applications (Wiley).  There may be better, 
more recent treatments of this subject, but I am not familiar with them.

	  spencer graves
p.s.  I never (no never, not ever) use "c" as a variable name, because 
it is the name of a common R function.  R is smart enough to distinguish 
between a function and a non-function in some contexts but not in all. 
When I want a name for a new object, I routinely ask R to print my 
proposed name.  If it returns "Error:  object ... not found", I can use 
"...".  	

Carsten Steinhoff wrote:

> Hello,
>  
> I want to solve some two dimensional equation system with R. Some systems
> are not solvable analytically.
>  
> Here is an example:
>  
> (I)        1/n*sum{from_i=1_to_n}(Xi) = ln lambda + digamma(c)
>  
> (II)        mean(X) = x / lambda
>  
> I want to find lambda and c,
>  
> which R-function could do that task?
>  
> Carsten
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From maechler at stat.math.ethz.ch  Thu Jun 23 18:57:27 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 23 Jun 2005 18:57:27 +0200
Subject: [R] source of R functions {was ... aov ...}
In-Reply-To: <20050623173714.5849632c@zygiella.local>
References: <20050623150759.0b74ac48@zygiella.local>
	<Pine.LNX.4.61.0506231507510.5521@gannet.stats>
	<20050623173714.5849632c@zygiella.local>
Message-ID: <17082.59895.128725.632022@stat.math.ethz.ch>

>>>>> "RenE" == RenE J V Bertin <rjvbertin at gmail.com>
>>>>>     on Thu, 23 Jun 2005 17:37:14 +0200 writes:

    RenE> Thanks to all the others who replied.
 
      BDR> and do note it is unset again a few lines later.
      BDR> (Listing a function inside R is not giving you the
      BDR> `source code'.)

nor does RenE's   fix(aov)  {mentioned later in his post}.

We have been on this topic many times: The source code is in
*files*, publicly available from CRAN (and even with your web
browser from https://svn.R-project.org/R/ where e.g. tags/R-2-1-1/
contains the source of R 2.1.1).
And this applies to CRAN (or Bioconductor) packages as well as
the "base R" code:  The real source is in files.

Looking at the object {via "typing its name" or e.g. fix() or an
"object browser"} basically (not quite!) provides the result of
source() {+  save() and load() in some cases} of the original
source files; and this should be about the same as what you'd
get from  print( parse(...) ) of the source text.

What's the difference?   At least these:

1a) All the comments inside functions are lost.
1b) all other comments between object {typically function} definitions,
    and the grouping {into files; sections inside files, ...} 
    and logical sequence of the definitions is lost.
2) The author's formatting is lost.

3) only " ... " are used for strings (even when the original had '...')
4) numerical constants are shown in a uniform way, not as
   entered  { .1 |-> 0.1   1e1 |-> 10   etc }

where "1)" and "2)" can be important and I consider a drawback;
3 and 4 are less important and typically an advantage, 
and '2)' is an advantage for some people's R code because it
makes it prettier. (! ;-)

Martin Maechler, ETH Zurich



From murdoch at stats.uwo.ca  Thu Jun 23 19:13:30 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 23 Jun 2005 13:13:30 -0400
Subject: [R] source of R functions {was ... aov ...}
In-Reply-To: <17082.59895.128725.632022@stat.math.ethz.ch>
References: <20050623150759.0b74ac48@zygiella.local>	<Pine.LNX.4.61.0506231507510.5521@gannet.stats>	<20050623173714.5849632c@zygiella.local>
	<17082.59895.128725.632022@stat.math.ethz.ch>
Message-ID: <42BAEDBA.1000407@stats.uwo.ca>

On 6/23/2005 12:57 PM, Martin Maechler wrote:
>>>>>> "RenE" == RenE J V Bertin <rjvbertin at gmail.com>
>>>>>>     on Thu, 23 Jun 2005 17:37:14 +0200 writes:
> 
>     RenE> Thanks to all the others who replied.
>  
>       BDR> and do note it is unset again a few lines later.
>       BDR> (Listing a function inside R is not giving you the
>       BDR> `source code'.)
> 
> nor does RenE's   fix(aov)  {mentioned later in his post}.
> 
> We have been on this topic many times: The source code is in
> *files*, publicly available from CRAN (and even with your web
> browser from https://svn.R-project.org/R/ where e.g. tags/R-2-1-1/
> contains the source of R 2.1.1).
> And this applies to CRAN (or Bioconductor) packages as well as
> the "base R" code:  The real source is in files.
> 
> Looking at the object {via "typing its name" or e.g. fix() or an
> "object browser"} basically (not quite!) provides the result of
> source() {+  save() and load() in some cases} of the original
> source files; and this should be about the same as what you'd
> get from  print( parse(...) ) of the source text.
> 
> What's the difference?   At least these:
> 
> 1a) All the comments inside functions are lost.
> 1b) all other comments between object {typically function} definitions,
>     and the grouping {into files; sections inside files, ...} 
>     and logical sequence of the definitions is lost.
> 2) The author's formatting is lost.
> 
> 3) only " ... " are used for strings (even when the original had '...')
> 4) numerical constants are shown in a uniform way, not as
>    entered  { .1 |-> 0.1   1e1 |-> 10   etc }
> 
> where "1)" and "2)" can be important and I consider a drawback;
> 3 and 4 are less important and typically an advantage, 
> and '2)' is an advantage for some people's R code because it
> makes it prettier. (! ;-)

Everything you say is correct, but one point might confuse some people: 
  it is possible to attach the source to a function definition, so that 
fix() will keep comments, etc.  "options(keep.source=TRUE)" makes this
happen, and that's the default setting for interactive use.  The point 
is that the installed packages are normally built with this option set 
to FALSE, so the source is only in the files.

Duncan Murdoch



From petr.pikal at precheza.cz  Thu Jun 23 19:27:47 2005
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 23 Jun 2005 19:27:47 +0200
Subject: [R] How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
Message-ID: <42BB0D33.31051.280231F@localhost>



On 22 Jun 2005 at 16:46, Ling Jin wrote:

> Hi all,
> 
> Does anybody know the easiest way to import excel data into R? I
> copied and pasted the excel data into a txt file, and tried
> read.table, but R reported that
> 
> Error in read.table("data_support.txt", sep = " ", header = T) :
>          more columns than column names
> 

Or simply

Open Excell file
Decide what you want to copy and put it to clipboard by Ctrl-C
In R issue

temp1<-read.delim("clipboard")

and you have your data in temp1

HTH
Petr





> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From 0034058 at fudan.edu.cn  Thu Jun 23 20:16:08 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 24 Jun 2005 02:16:08 +0800
Subject: [R] how can i deal with multiple response data?
Message-ID: <20050624021608.59354c01@localhost.localdomain>

in a questionair i ask:
Which of the following software packages do you use for data analysis?
1 	R
2 	S-Plus
3 	SAS
4 	SPSS
5 	Stata
6 	others
(In this question, respondents are asked to mark the name of each package they use. Respondents may mark any number of packages)

and code the answer as:
                q1_R   q1_SPlus     q1_SAS    q1_SPSS   q1_Stata  q1_others   
        1.         1          0          0          0          0          1    
        2.         1          1          0          0          1          0     
        3.         0          0          0          0          1          0    
        4.         0          0          1          0          0          0    
        5.         0          0          1          0          0          1    


my question: is there any function  dealing with this data?



-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From kye at aecom.yu.edu  Thu Jun 23 20:19:50 2005
From: kye at aecom.yu.edu (Kenny Ye)
Date: Thu, 23 Jun 2005 14:19:50 -0400
Subject: [R] compiling gap on mac os x
Message-ID: <42BAFD46.9000601@aecom.yu.edu>

Hi, I am having trouble compiling package gap  
http://www.hgmp.mrc.ac.uk/~jzhao/r-progs.htm on Tiger. I have installed 
XcodeTools 2.1. The binary version of gap currently available on CRAN 
has some bug and is fixed in the latest version.

The message I get from R is below. Any help is greatly appreciated.

best regards,

Kenny Ye

tar: Read 1536 bytes from -
* Installing *source* package 'gap' ...
** libs
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c 2k.c -o 2k.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c 2ld.c -o 2ld.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c cline.c -o cline.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c gc.c -o gc.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c gcontrol.c -o gcontrol.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c gif.c -o gif.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c hap.c -o hap.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c hwe.hardy.c -o hwe.hardy.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c kin.morgan.c -o kin.morgan.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c makeped.c -o makeped.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c mia.c -o mia.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c muvar.c -o muvar.o
g77   -fno-common  -g -O2 -c pfc.f -o pfc.o
g77   -fno-common  -g -O2 -c pfc.sim.f -o pfc.sim.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c pgc.c -o pgc.o
gcc-3.3 -no-cpp-precomp 
-I/Library/Frameworks/R.framework/Resources/include  
-I/usr/local/include   -fno-common  -g -O2 -c whscore.c -o whscore.o
gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
gap.so 2k.o 2ld.o cline.o gc.o gcontrol.o gif.o hap.o hwe.hardy.o 
kin.morgan.o makeped.o mia.o muvar.o pfc.o pfc.sim.o pgc.o whscore.o  
-L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 -lg2c -lSystem 
-framework R
** Removing 
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/gap'
    npackage installation failed
ld: warning multiple definitions of symbol _i1mach_
pfc.sim.o definition of _i1mach_ in section (__TEXT,__text)
/Library/Frameworks/R.framework/R(i1mach.lo) definition of _i1mach_
ld: 2ld.o has external relocation entries in non-writable section 
(__TEXT,__text) for symbols:
restFP
saveFP
make: *** [gap.so] Error 1
ERROR: compilation failed for package 'gap'
** Restoring previous 
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/gap'
** Removing 
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/pathmix'
** Removing 
'/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/pointer'



From renaud.lancelot at cirad.fr  Thu Jun 23 20:27:59 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 23 Jun 2005 21:27:59 +0300
Subject: [R] How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
References: <42B9F868.5080807@lbl.gov>
Message-ID: <42BAFF2F.5010102@cirad.fr>

Ling Jin a ??crit :
> Hi all,
> 
> Does anybody know the easiest way to import excel data into R? I copied 
> and pasted the excel data into a txt file, and tried read.table, but R 
> reported that
> 
> Error in read.table("data_support.txt", sep = " ", header = T) :
>          more columns than column names
> 
> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

Here is a function from a not-yet-released package written by a 
colleague and I, based on package RODBC written by Pr Ripley. The idea 
is to wrap - in the same function, GUI (suite of pop-up windows) and 
command-line facilities.

It is a preliminary, unoptimized version. Suggestions for improvements 
and bug reports are welcome.

Let me know if you want the packaged version.

Best,

Renaud

####

query <- function(tab = NULL, db = NULL, query = "all"){
# load the RODBC package and stops the program if not available
   if(!require(RODBC))
     stop("This function requires the RODBC package.\n")
# close all databases in case of error
   on.exit(odbcCloseAll())
## name of the database is not provided
   if(is.null(db)){
     Databases <- matrix(c("MS Access database (*.mdb)", "*.mdb",
                           "MS Excel file (*.xls)",      "*.xls",
                           "dBase-like file (*.dbf)",    "*.dbf"), nrow 
= 3, byrow = TRUE)
     File <- choose.files(filters = Databases, multi = FALSE, caption = 
"Select a database")
     sop <- match(".", rev(strsplit(File, NULL)[[1]]))[1]
     ext <- tolower(substring(File, nchar(File) - sop + 2, nchar(File)))
     channel <- switch(EXPR = ext,
                       xls = odbcConnectExcel(File),
                       mdb = odbcConnectAccess(File),
                       dbf = odbcConnectDbase(File))
# For Excel and Access cases, need to select a particular sheet or table
     if(ext != "dbf"){
       # sheet or table name is not provided
       if(is.null(tab)){
         tabdat <- sqlTables(channel)
         names(tabdat) <- tolower(names(tabdat))
         if(ext == "mdb")
           tabdat <- tabdat[tabdat$table_type == "TABLE", 3]
         if(ext == "xls"){
           tabname <- tabdat$table_name
           namfil <- tabdat[substring(tabname, nchar(tabname), 
nchar(tabname)) == "$", 3]
           tabdat <- substring(namfil, 1, nchar(namfil) - 1)
           }
         fil <- select.list(sort(tabdat))
         if(length(fil) == 0)
           stop("No file was selected.")
         if(ext == "xls")
           fil <- paste("[", fil, "$]", sep = "")
         }
       else
       # sheet or table name is provided
         fil <- if(ext != "xls") tab else paste("[", tab, "$]", sep = "")
       }
     else{
# dBase file
       sop <- match(".", rev(strsplit(File, NULL)[[1]]))[1]
       root <- tolower(substring(File, 1, nchar(File) - sop))
       revstr <- rev(strsplit(root, NULL)[[1]])
       sop <- if(is.na(match(c("/", "\\"), revstr)[1])) length(revstr) 
else match(c("/", "\\"), revstr)[1] - 1
       toor <- revstr[seq(sop)]
       fil <- paste(rev(toor), collapse = "")
       }
     }

## name of the database is provided
   else{
     sop <- match(".", rev(strsplit(db, NULL)[[1]]))[1]
     if(is.na(sop))
       stop("You must provide the full path and the extension for the 
database.\n")
     else{
       ext <- tolower(substring(db, nchar(db) - sop + 2, nchar(db)))
       channel <- switch(EXPR = ext,
                         xls = odbcConnectExcel(db),
                         mdb = odbcConnectAccess(db),
                         dbf = odbcConnectDbase(db),
                         stop("query not yet implemented for databases 
of format .", ext, "\n"))
# dBase file
     if(ext == "dbf"){
       sop <- match(".", rev(strsplit(db, NULL)[[1]]))[1]
       root <- tolower(substring(db, 1, nchar(db) - sop))
       revstr <- rev(strsplit(root, NULL)[[1]])
       sop <- if(is.na(match(c("/", "\\"), revstr)[1])) length(revstr) 
else match(c("/", "\\"), revstr)[1] - 1
       toor <- revstr[seq(sop)]
       fil <- paste(rev(toor), collapse = "")
       }
     else{
# name of the table is not provided (Excel or Access)
       if(is.null(tab)){
         tabdat <- sqlTables(channel)
         names(tabdat) <- tolower(names(tabdat))
         if(ext == "mdb")
           tabdat <- tabdat[tabdat$table_type == "TABLE", 3]
         if(ext == "xls"){
           tabname <- tabdat$table_name
           namfil <- tabdat[substring(tabname, nchar(tabname), 
nchar(tabname)) == "$", 3]
           tabdat <- substring(namfil, 1, nchar(namfil) - 1)
           }
         fil <- select.list(sort(tabdat))
         if(length(fil) == 0)
           stop("No file was selected.")
         if(ext == "xls")
           fil <- paste("[", fil, "$]", sep = "")
         }
       else
         fil <- if(ext != "xls") tab else paste("[", tab, "$]", sep = "")
       }
     }
   }
# retrieve the data
   if(query == "all")
     dat <- sqlQuery(channel = channel, query = paste("select * from", fil))
   else
     dat <- sqlQuery(channel = channel, query = query)
   odbcCloseAll()
   dat
   }



-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From renaud.lancelot at cirad.fr  Thu Jun 23 20:30:52 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Thu, 23 Jun 2005 21:30:52 +0300
Subject: [R] How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
References: <42B9F868.5080807@lbl.gov>
Message-ID: <42BAFFDC.7080505@cirad.fr>

Ling Jin a ??crit :
> Hi all,
> 
> Does anybody know the easiest way to import excel data into R? I copied 
> and pasted the excel data into a txt file, and tried read.table, but R 
> reported that
> 
> Error in read.table("data_support.txt", sep = " ", header = T) :
>          more columns than column names
> 
> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

Here is a function from a not-yet-released package written by a 
colleague and I, based on package RODBC written by Pr Ripley. The idea 
is to wrap - in the same function, GUI (suite of pop-up windows) and 
command-line facilities. It allows the importation of MS Excel and MS 
Access sheet or tables (within databases), and dBase-like files.

It is a preliminary, unoptimized version. Suggestions for improvements 
and bug reports are welcome.

Let me know if you want the packaged version (with help file).

Best,

Renaud

####

query <- function(tab = NULL, db = NULL, query = "all"){
# load the RODBC package and stops the program if not available
   if(!require(RODBC))
     stop("This function requires the RODBC package.\n")
# close all databases in case of error
   on.exit(odbcCloseAll())
## name of the database is not provided
   if(is.null(db)){
     Databases <- matrix(c("MS Access database (*.mdb)", "*.mdb",
                           "MS Excel file (*.xls)",      "*.xls",
                           "dBase-like file (*.dbf)",    "*.dbf"), nrow 
= 3, byrow = TRUE)
     File <- choose.files(filters = Databases, multi = FALSE, caption = 
"Select a database")
     sop <- match(".", rev(strsplit(File, NULL)[[1]]))[1]
     ext <- tolower(substring(File, nchar(File) - sop + 2, nchar(File)))
     channel <- switch(EXPR = ext,
                       xls = odbcConnectExcel(File),
                       mdb = odbcConnectAccess(File),
                       dbf = odbcConnectDbase(File))
# For Excel and Access cases, need to select a particular sheet or table
     if(ext != "dbf"){
       # sheet or table name is not provided
       if(is.null(tab)){
         tabdat <- sqlTables(channel)
         names(tabdat) <- tolower(names(tabdat))
         if(ext == "mdb")
           tabdat <- tabdat[tabdat$table_type == "TABLE", 3]
         if(ext == "xls"){
           tabname <- tabdat$table_name
           namfil <- tabdat[substring(tabname, nchar(tabname), 
nchar(tabname)) == "$", 3]
           tabdat <- substring(namfil, 1, nchar(namfil) - 1)
           }
         fil <- select.list(sort(tabdat))
         if(length(fil) == 0)
           stop("No file was selected.")
         if(ext == "xls")
           fil <- paste("[", fil, "$]", sep = "")
         }
       else
       # sheet or table name is provided
         fil <- if(ext != "xls") tab else paste("[", tab, "$]", sep = "")
       }
     else{
# dBase file
       sop <- match(".", rev(strsplit(File, NULL)[[1]]))[1]
       root <- tolower(substring(File, 1, nchar(File) - sop))
       revstr <- rev(strsplit(root, NULL)[[1]])
       sop <- if(is.na(match(c("/", "\\"), revstr)[1])) length(revstr) 
else match(c("/", "\\"), revstr)[1] - 1
       toor <- revstr[seq(sop)]
       fil <- paste(rev(toor), collapse = "")
       }
     }

## name of the database is provided
   else{
     sop <- match(".", rev(strsplit(db, NULL)[[1]]))[1]
     if(is.na(sop))
       stop("You must provide the full path and the extension for the 
database.\n")
     else{
       ext <- tolower(substring(db, nchar(db) - sop + 2, nchar(db)))
       channel <- switch(EXPR = ext,
                         xls = odbcConnectExcel(db),
                         mdb = odbcConnectAccess(db),
                         dbf = odbcConnectDbase(db),
                         stop("query not yet implemented for databases 
of format .", ext, "\n"))
# dBase file
     if(ext == "dbf"){
       sop <- match(".", rev(strsplit(db, NULL)[[1]]))[1]
       root <- tolower(substring(db, 1, nchar(db) - sop))
       revstr <- rev(strsplit(root, NULL)[[1]])
       sop <- if(is.na(match(c("/", "\\"), revstr)[1])) length(revstr) 
else match(c("/", "\\"), revstr)[1] - 1
       toor <- revstr[seq(sop)]
       fil <- paste(rev(toor), collapse = "")
       }
     else{
# name of the table is not provided (Excel or Access)
       if(is.null(tab)){
         tabdat <- sqlTables(channel)
         names(tabdat) <- tolower(names(tabdat))
         if(ext == "mdb")
           tabdat <- tabdat[tabdat$table_type == "TABLE", 3]
         if(ext == "xls"){
           tabname <- tabdat$table_name
           namfil <- tabdat[substring(tabname, nchar(tabname), 
nchar(tabname)) == "$", 3]
           tabdat <- substring(namfil, 1, nchar(namfil) - 1)
           }
         fil <- select.list(sort(tabdat))
         if(length(fil) == 0)
           stop("No file was selected.")
         if(ext == "xls")
           fil <- paste("[", fil, "$]", sep = "")
         }
       else
         fil <- if(ext != "xls") tab else paste("[", tab, "$]", sep = "")
       }
     }
   }
# retrieve the data
   if(query == "all")
     dat <- sqlQuery(channel = channel, query = paste("select * from", fil))
   else
     dat <- sqlQuery(channel = channel, query = query)
   odbcCloseAll()
   dat
   }



-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From MSchwartz at mednetstudy.com  Thu Jun 23 20:34:25 2005
From: MSchwartz at mednetstudy.com (Marc Schwartz)
Date: Thu, 23 Jun 2005 13:34:25 -0500
Subject: [R] the dimname of a table
In-Reply-To: <20050623231217.06fbbffb@localhost.localdomain>
References: <20050623231217.06fbbffb@localhost.localdomain>
Message-ID: <1119551665.5352.35.camel@localhost.localdomain>

On Thu, 2005-06-23 at 23:12 +0800, ronggui wrote:
> i have a data frame(dat) which has many variables.and i use the
> following script to get the crosstable.
> 
> >danx2<-c("x1.1","x1.2","x1.3","x1.4","x1.5","x2","x4","x5","x6","x7","x8.1","x8.2","x8.3","x8.4","x11",
> "x13","x17","x19","x20","x21")
> >indep<-c("x23","x24","x25","x26","x27","x28.1","x28.2","x29")
> >for (k in indep){
>   for (i in danx2){
>      a<-chisq.test(dat[,i],dat[,k])$p.v<=0.05
>      if (a)
> {CrossTable(dat[,i],dat[,k],chisq=T,format="SPSS");cat(rep("=",50),"\n","\n")}
>  }
> 
>   it has a little pitfall:the dimnames of table is dat[,i] and
> dat[,k],but i want it to be like x2,x23... 
> is there any good way to do this?
>   and in the command CrossTable(dat[,i],dat[,k],chisq=T,format="SPSS")
> in the loop,is there any other way to get the variable other than
> dat[,i] and dat[,k]?
>   thank you !

Hi,

I am in between meetings here. Sorry for the delay in my reply to your
query.

The best solution is for me to add two new args to CrossTable() to allow
you to specify these names explicitly, rather than having them as the
way they are now, which simply takes the x and y args and does:

     RowData <- deparse(substitute(x))
     ColData <- deparse(substitute(y))

The result is that whatever is passed as the x and y arguments, will be
used as the titles for the row and column labels as you have noted.

In the mean time, I am attaching an update to CrossTable (which I have
not extensively tested yet), that you can source() into R via the
console. The update has two new args called "RowData" and "ColData"
which will default to NULL, so as to not impact current default
behavior. You can then set these as part of your loop by passing the
index values.


Using one of the examples in ?CrossTable:

> CrossTable(infert$education, infert$induced, RowData = "Education",
             ColData = "Induced")


   Cell Contents
|-------------------------|
|                       N |
| Chi-square contribution |
|           N / Row Total |
|           N / Col Total |
|         N / Table Total |
|-------------------------|


Total Observations in Table:  248


             | Induced
   Education |         0 |         1 |         2 | Row Total |
-------------|-----------|-----------|-----------|-----------|
      0-5yrs |         4 |         2 |         6 |        12 |
             |     1.232 |     0.506 |     9.898 |           |
             |     0.333 |     0.167 |     0.500 |     0.048 |
             |     0.028 |     0.029 |     0.162 |           |
             |     0.016 |     0.008 |     0.024 |           |
-------------|-----------|-----------|-----------|-----------|
     6-11yrs |        78 |        27 |        15 |       120 |
             |     1.121 |     1.059 |     0.471 |           |
             |     0.650 |     0.225 |     0.125 |     0.484 |
             |     0.545 |     0.397 |     0.405 |           |
             |     0.315 |     0.109 |     0.060 |           |
-------------|-----------|-----------|-----------|-----------|
     12+ yrs |        61 |        39 |        16 |       116 |
             |     0.518 |     1.627 |     0.099 |           |
             |     0.526 |     0.336 |     0.138 |     0.468 |
             |     0.427 |     0.574 |     0.432 |           |
             |     0.246 |     0.157 |     0.065 |           |
-------------|-----------|-----------|-----------|-----------|
Column Total |       143 |        68 |        37 |       248 |
             |     0.577 |     0.274 |     0.149 |           |
-------------|-----------|-----------|-----------|-----------|


Let me know if this works or you find a problem. I will do further
testing here as soon as time permits and get an update to Greg and Nitin
to include into gregmisc.

HTH,

Marc Schwartz

-------------- next part --------------
CrossTable <- function (x, y,
                        digits = 3,
                        max.width = 5,
                        expected = FALSE,
                        prop.r = TRUE,
                        prop.c = TRUE,
                        prop.t = TRUE,
                        prop.chisq=TRUE,
                        chisq = FALSE,
                        fisher = FALSE,
                        mcnemar = FALSE,
                        resid = FALSE,
                        sresid = FALSE,
                        asresid = FALSE,
                        missing.include = FALSE,
                        format=c("SAS","SPSS"),
                        RowData=NULL,
                        ColData=NULL,
                        ...
                        )
{

  format=match.arg(format)
  
  ## Ensure that max.width >= 1
  if (max.width < 1)
    stop("max.width must be >= 1")
  ## Set 'x' vector flag
  vector.x <- FALSE
  ## Ensure that if (expected), a chisq is done
  if (expected)
    chisq <- TRUE

  if (missing(y))
    {
      ## is x a vector?
      if (is.null(dim(x)))
        {
                                        #TotalN <- length(x)
          if (missing.include)
            x <- factor(x,exclude=NULL)
          else
            ## Remove any unused factor levels
            x <- factor(x)
          t <- t(as.matrix(table(x)))
          vector.x <- TRUE
        }
      ## is x a matrix?
      else if (length(dim(x) == 2))
        {
          if(any(x < 0) || any(is.na(x)))
            stop("all entries of x must be nonnegative and finite")

          ## Add generic dimnames if required
          ## check each dimname separately, in case user has defined one or the other
          if (is.null(rownames(x)))
            rownames(x) <- paste("[", 1:nrow(x), ",]", sep = "")
          if (is.null(colnames(x)))
            colnames(x) <- paste("[,", 1:ncol(x), "]", sep = "")

          t <- x
        }
      else
        stop("x must be either a vector or a 2 dimensional matrix, if y is not given")
    }
  else
    {
      if(length(x) != length(y))
        stop("x and y must have the same length")

      ## Create Titles for Table From Vector Names
      if(is.null(RowData))
        RowData <- deparse(substitute(x))
      if (is.null(ColData))
        ColData <- deparse(substitute(y))
      
      if (missing.include)
        {
          x <- factor(x,exclude=c())
          y <- factor(y,exclude=c())
        }
      else
        {
          ## Remove unused factor levels from vectors
          x <- factor(x)
          y <- factor(y)
        }
      ## Generate table
      t <- table(x, y)
    }

  ## if t is not at least a 2 x 2, do not do stats
  ## even if any set to TRUE. Do not do col/table props
  if (any(dim(t) < 2))
    {
      prop.c <- prop.r <- prop.chisq <- chisq <- expected <- fisher <- mcnemar <- FALSE
    }

  ## Generate cell proportion of row
  CPR <- prop.table(t, 1)

  ## Generate cell proportion of col
  CPC <- prop.table(t, 2)

  ## Generate cell proportion of total
  CPT <- prop.table(t)

  ## Generate summary counts
  GT <- sum(t)
  RS <- rowSums(t)
  CS <- colSums(t)

  if (length(dim(x) == 2))
    TotalN <- GT
  else
    TotalN <- length(x)
  
  
  ## Column and Row Total Headings
  ColTotal <- "Column Total"
  RowTotal <- "Row Total"

  ## Set consistent column widths based upon dimnames and table values
  CWidth <- max(digits + 2, c(nchar(t), nchar(dimnames(t)[[2]]), nchar(RS), nchar(CS), nchar(RowTotal)))
  RWidth <- max(c(nchar(dimnames(t)[[1]]), nchar(ColTotal)))

  ## Adjust first column width if Data Titles present
  if (exists("RowData"))
    RWidth <- max(RWidth, nchar(RowData))

  ## Create row separators
  RowSep <- paste(rep("-", CWidth + 2), collapse = "")
  RowSep1 <- paste(rep("-", RWidth + 1), collapse = "")
  SpaceSep1 <- paste(rep(" ", RWidth), collapse = "")
  SpaceSep2 <- paste(rep(" ", CWidth), collapse = "")

  ## Create formatted Names
  FirstCol <- formatC(dimnames(t)[[1]], width = RWidth, format = "s")
  ColTotal <- formatC(ColTotal, width = RWidth, format = "s")
  RowTotal <- formatC(RowTotal, width = CWidth, format = "s")

  ## Perform Chi-Square Tests
  ## Needs to be before the table output, in case (expected = TRUE)
  if (chisq)
    {
      if (all(dim(t) == 2))
        CSTc <- chisq.test(t, correct = TRUE, ...)

      CST <- chisq.test(t, correct = FALSE, ...)
    }
  else   
    CST <- suppressWarnings(chisq.test(t, correct = FALSE))
  if (asresid & !vector.x)
    ASR <- (CST$observed-CST$expected)/sqrt(CST$expected*((1-RS/GT) %*% t(1-CS/GT)))
  
  print.CrossTable.SAS <- function()
    {
      if (exists("RowData"))
        {
          cat(SpaceSep1, "|", ColData, "\n")
          cat(formatC(RowData, width = RWidth, format= "s"), 
              formatC(dimnames(t)[[2]], width = CWidth, format = "s"), 
              RowTotal, sep = " | ", collapse = "\n")
        }
      else
        cat(SpaceSep1, formatC(dimnames(t)[[2]], width = CWidth, 
                               format = "s"), RowTotal, sep = " | ",
            collapse = "\n")
      cat(RowSep1, rep(RowSep, ncol(t) + 1), sep = "|", collapse = "\n")
      ## Print table cells

      for (i in 1:nrow(t))
        {
          cat(FirstCol[i], formatC(c(t[i, ], RS[i]), width = CWidth, format = "d"), 
              sep = " | ", collapse = "\n")
          if (expected) 
            cat(SpaceSep1, formatC(CST$expected[i, ], digits = digits, 
                                   format = "f", width = CWidth),
                SpaceSep2, sep = " | ", 
                collapse = "\n")
          if (prop.chisq)
            cat(SpaceSep1, formatC((((CST$expected[i, ]-t[i, ])^2)/CST$expected[i, ]),
                width = CWidth, digits = digits, format = "f"), SpaceSep2, 
                sep = " | ", collapse = "\n")
          if (prop.r) 
            cat(SpaceSep1, formatC(c(CPR[i, ], RS[i]/GT), 
                                   width = CWidth, digits = digits, format = "f"), 
                sep = " | ", collapse = "\n")
          if (prop.c) 
            cat(SpaceSep1, formatC(CPC[i, ], width = CWidth, 
                                   digits = digits, format = "f"), SpaceSep2, 
                sep = " | ", collapse = "\n")
          if (prop.t) 
            cat(SpaceSep1, formatC(CPT[i, ], width = CWidth, 
                                   digits = digits, format = "f"), SpaceSep2, 
                sep = " | ", collapse = "\n")
          cat(RowSep1, rep(RowSep, ncol(t) + 1), sep = "|", 
              collapse = "\n")
        }
      
      ## Print Column Totals
      cat(ColTotal, formatC(c(CS, GT), width = CWidth, format = "d"), sep = " | ", 
          collapse = "\n")
      if (prop.c) 
        cat(SpaceSep1, formatC(CS/GT, width = CWidth, digits = digits, 
                               format = "f"), SpaceSep2, sep = " | ", collapse = "\n")
      cat(RowSep1, rep(RowSep, ncol(t) + 1), sep = "|", collapse = "\n")
    } ## End Of print.Crosstable.SAS function
  
  print.CrossTable.SPSS <- function()
    {
      ## similar to SPSS behaviour
      
      ## Print Column headings
      if (exists("RowData"))
        {
          cat(SpaceSep1, "|", ColData, "\n")
          cat(cat(formatC(RowData, width = RWidth, format = "s"),sep=" | ",
                  collapse=""),
              cat(formatC(dimnames(t)[[2]], width = CWidth-1, format = "s"),
                  sep="  | ", collapse=""),
              cat(RowTotal, sep = " | ", collapse = "\n"), sep="", collapse="")
        }
      else
        cat(SpaceSep1, formatC(dimnames(t)[[2]], width = CWidth, format = "s"), RowTotal,
            sep = " | ", collapse = "\n")

      cat(RowSep1, rep(RowSep, ncol(t) + 1), sep = "|", collapse = "\n")

      ## Print table cells
      for (i in 1:nrow(t))
        {
          cat(cat(FirstCol[i], sep=" | ", collapse=""),
              cat(formatC(c(t[i, ], RS[i]), width = CWidth-1, format = "d"),
                  sep = "  | ", collapse = "\n"), sep="", collapse="")

          if (expected)
            cat(cat(SpaceSep1, sep=" | ", collapse=""),
                cat(formatC(CST$expected[i, ], digits = digits, format = "f",
                            width = CWidth-1), sep="  | ", collapse=""),
                cat(SpaceSep2, sep = " | ", collapse = "\n"), sep="", collapse="")

          if (prop.chisq)
            cat(cat(SpaceSep1, sep=" | ", collapse=""),
                cat(formatC((((CST$expected[i, ]-t[i, ])^2)/CST$expected[i, ]),
                    digits = digits, format = "f",
                            width = CWidth-1), sep="  | ", collapse=""),
                cat(SpaceSep2, sep = " | ", collapse = "\n"), sep="", collapse="")
      if (prop.r)
            cat(cat(SpaceSep1, sep=" | ", collapse=""),
                cat(formatC(c(CPR[i, ]*100, 100*RS[i] / GT),
                            width = CWidth-1, digits = digits, format = "f"),
                    sep = "% | ", collapse = "\n"), sep="", collapse="")

          if (prop.c)
            cat(cat(SpaceSep1, sep=" | ", collapse=""),
                cat(formatC(CPC[i, ]*100, width = CWidth-1,
                            digits = digits, format = "f"), sep="% | ", collapse=""),
                cat(SpaceSep2, sep = " | ", collapse = "\n"), sep="", collapse="")

          if (prop.t)
            cat(cat(SpaceSep1, sep=" | ", collapse=""),
                cat(formatC(CPT[i, ]*100, width = CWidth-1, digits = digits,
                            format = "f"), sep="% | ", collapse=""),
                cat(SpaceSep2, sep = " | ", collapse = "\n"), sep="", collapse="")
          
          if (resid)
            cat(cat(SpaceSep1,sep=" | ",collapse = ""),
                cat(formatC(CST$observed[i, ]-CST$expected[i, ], digits = digits,
                            format = "f", width = CWidth-1), sep = "  | ",
                    collapse = ""),
                cat(SpaceSep2,sep = " | ", collapse = "\n"),sep="",collapse="")

          if (sresid)
            cat(cat(SpaceSep1,sep=" | ",collapse = ""),
                cat(formatC(CST$residual[i, ], digits = digits,
                            format = "f", width = CWidth-1), sep = "  | ",
                    collapse = ""),
                cat(SpaceSep2,sep = " | ", collapse = "\n"),sep="",collapse="")

          if (asresid)
            cat(cat(SpaceSep1,sep=" | ",collapse = ""),
                cat(formatC(ASR[i, ], digits = digits,
                            format = "f", width = CWidth-1), sep = "  | ",
                    collapse = ""),
                cat(SpaceSep2,sep = " | ", collapse = "\n"),sep="",collapse="")

          cat(RowSep1, rep(RowSep, ncol(t) + 1), sep = "|", collapse = "\n")
        }

      ## Print Column Totals
      cat(cat(ColTotal,sep=" | ",collapse=""),
          cat(formatC(c(CS, GT), width = CWidth-1, format = "d"), sep = "  | ",
              collapse = "\n"),sep="",collapse="")

      if (prop.c)
        cat(cat(SpaceSep1,sep=" | ",collapse=""),
            cat(formatC(100*CS/GT, width = CWidth-1, digits = digits,
                        format = "f"),sep = "% | ", collapse = ""),
            cat(SpaceSep2,sep = " | ", collapse = "\n"),sep="",collapes="")

      cat(RowSep1, rep(RowSep, ncol(t) + 1), sep = "|", collapse = "\n")
    } ## End of print.CrossTable.SPSS function

  ## Print Function For 1 X N Vector In SAS Format
  print.CrossTable.vector.SAS <- function()
    {
      if (length(t) > max.width)
        {
          ## set breakpoints for output based upon max.width
          final.row <- length(t) %% max.width
          max <- length(t) - final.row
          ## Define breakpoint indices for each row
          start <- seq(1, max, max.width)
          end <- start + (max.width - 1)
          ## Add final.row if required
          if (final.row > 0)
            {
              start <- c(start, end[length(end)] + 1)
              end <- c(end, end[length(end)] + final.row)
            }
        }
      else
        {
          ## Each value printed horizontally in a single row
          start <- 1
          end <- length(t)
        }

      SpaceSep3 <- paste(SpaceSep2, " ", sep = "")

      for (i in 1:length(start))
        {
          ## print column labels
          cat(SpaceSep2, formatC(dimnames(t)[[2]][start[i]:end[i]], width = CWidth, format = "s"),
              sep = " | ", collapse = "\n")

          cat(SpaceSep3, rep(RowSep, (end[i] - start[i]) + 1), sep = "|", collapse = "\n")
          cat(SpaceSep2, formatC(t[, start[i]:end[i]], width = CWidth, format = "d"), sep = " | ", collapse = "\n")
          cat(SpaceSep2, formatC(CPT[, start[i]:end[i]], width = CWidth, digits = digits, format = "f"),
              sep = " | ", collapse = "\n")
          cat(SpaceSep3, rep(RowSep, (end[i] - start[i]) + 1), sep = "|", collapse = "\n")
          cat("\n\n")
        }

    } ## End of print.Crosstable.vector.SAS function

  
  ## Print function for 1 X N vector in SPSS format
  print.CrossTable.vector.SPSS <- function()
    {
      if (length(t) > max.width)
        {
          ## set breakpoints for output based upon max.width
          final.row <- length(t) %% max.width
          max <- length(t) - final.row
          ## Define breakpoint indices for each row
          start <- seq(1, max, max.width)
          end <- start + (max.width - 1)
          ## Add final.row if required
          if (final.row > 0)
            {
              start <- c(start, end[length(end)] + 1)
              end <- c(end, end[length(end)] + final.row)
            }
        }
      else
        {
          ## Each value printed horizontally in a single row
          start <- 1
          end <- length(t)
        }

      SpaceSep3 <- paste(SpaceSep2, " ", sep = "")
      

      for (i in 1:length(start))
        {
          cat(cat(SpaceSep2,sep=" | ",collapse=""),
              cat(formatC(dimnames(t)[[2]][start[i]:end[i]],
                          width = CWidth-1, format = "s"), sep = "  | ", collapse = "\n"),
              sep="",collapse="")
          cat(SpaceSep3, rep(RowSep, (end[i] - start[i]) +
                             1), sep = "|", collapse = "\n")
          cat(cat(SpaceSep2,sep=" | ",collapse=""),
              cat(formatC(t[, start[i]:end[i]], width = CWidth-1, format = "d"),
                  sep = "  | ", collapse = "\n"),
              sep="",collapse="")
          cat(cat(SpaceSep2, sep=" | ",collapse=""),
              cat(formatC(CPT[, start[i]:end[i]], width = CWidth-1,
                          digits = digits, format = "f"), sep = "% | ",
                  collapse = ""),sep="",collapse="\n")
          cat(SpaceSep3, rep(RowSep, (end[i] - start[i]) +
                             1), sep = "|", collapse = "\n")

        }  ## End of for (i in 1:length(start))

      if (GT < TotalN)
        cat("\nNumber of Missing Observations: ",TotalN-GT," (",100*(TotalN-GT)/TotalN,"%)\n",sep="")


    } ## End of print.CrossTable.vector.SPSS Function




  
  print.statistics <- function()
    {
      ## Print Statistics
      if (chisq)
        {
          cat(rep("\n", 2))
          cat("Statistics for All Table Factors\n\n\n")

          cat(CST$method,"\n")
          cat("------------------------------------------------------------\n")
          cat("Chi^2 = ", CST$statistic, "    d.f. = ", CST$parameter, "    p = ", CST$p.value, "\n\n")

          if (all(dim(t) == 2))
            {
              cat(CSTc$method,"\n")
              cat("------------------------------------------------------------\n")
              cat("Chi^2 = ", CSTc$statistic, "    d.f. = ", CSTc$parameter, "    p = ", CSTc$p.value, "\n")
            }
        }

      ## Perform McNemar tests
      if (mcnemar)
        {
          McN <- mcnemar.test(t, correct = FALSE)
          cat(rep("\n", 2))
          cat(McN$method,"\n")
          cat("------------------------------------------------------------\n")
          cat("Chi^2 = ", McN$statistic, "    d.f. = ", McN$parameter, "    p = ", McN$p.value, "\n\n")

          if (all(dim(t) == 2))
            {
              McNc <- mcnemar.test(t, correct = TRUE)
              cat(McNc$method,"\n")
              cat("------------------------------------------------------------\n")
              cat("Chi^2 = ", McNc$statistic, "    d.f. = ", McNc$parameter, "    p = ", McNc$p.value, "\n")
            }
        }

      ## Perform Fisher Tests
      if (fisher)
        {
          cat(rep("\n", 2))
          FTt <- fisher.test(t, alternative = "two.sided")

          if (all(dim(t) == 2))
            {
              FTl <- fisher.test(t, alternative = "less")
              FTg <- fisher.test(t, alternative = "greater")
            }

          cat("Fisher's Exact Test for Count Data\n")
          cat("------------------------------------------------------------\n")

          if (all(dim(t) == 2))
            {
              cat("Sample estimate odds ratio: ", FTt$estimate, "\n\n")

              cat("Alternative hypothesis: true odds ratio is not equal to 1\n")
              cat("p = ", FTt$p.value, "\n")
              cat("95% confidence interval: ", FTt$conf.int, "\n\n")

              cat("Alternative hypothesis: true odds ratio is less than 1\n")
              cat("p = ", FTl$p.value, "\n")
              cat("95% confidence interval: ", FTl$conf.int, "\n\n")

              cat("Alternative hypothesis: true odds ratio is greater than 1\n")
              cat("p = ", FTg$p.value, "\n")
              cat("95% confidence interval: ", FTg$conf.int, "\n\n")
            }
          else
            {
              cat("Alternative hypothesis: two.sided\n")
              cat("p = ", FTt$p.value, "\n")
            }
        } ## End Of If(Fisher) Loop

      cat(rep("\n", 2))

      ## Create list of results for invisible()

      CT <- list(t = t, prop.row = CPR, prop.col = CPC, prop.tbl = CPT)

      if (any(chisq, fisher, mcnemar))
        {
          if (all(dim(t) == 2))
            {
              if (chisq)
                CT <- c(CT, list(chisq = CST, chisq.corr = CSTc))

              if (fisher)
                CT <- c(CT, list(fisher.ts = FTt, fisher.tl = FTl, fisher.gt = FTg))

              if (mcnemar)
                CT <- c(CT, list(mcnemar = McN, mcnemar.corr = McNc))
            }
          else
            {
              if (chisq)
                CT <- c(CT, list(chisq = CST))

              if (fisher)
                CT <- c(CT, list(fisher.ts = FTt))

              if (mcnemar)
                CT <- c(CT, list(mcnemar = McN))
            }
        } ## End of if(any(chisq, fisher, mcnemar)) loop

      
      ## return list(CT)
      invisible(CT)

    } ## End of print.statistics function


  ## Printing the tables
  if (format=="SAS")
    {
      ## Print Cell Layout
      
      cat(rep("\n", 2))
      cat("   Cell Contents\n")

      cat("|-------------------------|\n")
      cat("|                       N |\n")
      if (expected)
        cat("|              Expected N |\n")
      if (prop.chisq)
        cat("| Chi-square contribution |\n")
      if (prop.r)
        cat("|           N / Row Total |\n")
      if (prop.c)
        cat("|           N / Col Total |\n")
      if (prop.t)
        cat("|         N / Table Total |\n")
      cat("|-------------------------|\n")
      cat(rep("\n", 2))
      cat("Total Observations in Table: ", GT, "\n")
      cat(rep("\n", 2))

      if (!vector.x)
        print.CrossTable.SAS()
      else
        print.CrossTable.vector.SAS()

      print.statistics()
    }
  else if (format == "SPSS")
    {
      
      ## Print Cell Layout
      cat("\n")
      cat("   Cell Contents\n")
      cat("|-------------------------|\n")
      cat("|                   Count |\n")
      if (!vector.x)
        {
          if (expected)
            cat("|         Expected Values |\n")
          if (prop.chisq)
            cat("| Chi-square contribution |\n")
          if (prop.r)
            cat("|             Row Percent |\n")
          if (prop.c)
            cat("|          Column Percent |\n")
          if (prop.t)
            cat("|           Total Percent |\n")
          if (resid)
            cat("|                Residual |\n")
          if (sresid)
            cat("|            Std Residual |\n")
          if (asresid)
            cat("|           Adj Std Resid |\n")
        }
      else
        cat("|             Row Percent |\n")
      cat("|-------------------------|\n")
      cat("\n")
      cat("Total Observations in Table: ", GT, "\n")
      cat("\n")
      if (!vector.x)
        print.CrossTable.SPSS()
      else print.CrossTable.vector.SPSS()

      print.statistics()

      
      if (any(dim(t) >= 2) & any(chisq,mcnemar,fisher))
        {
          MinExpF = min(CST$expected)
          cat('       Minimum expected frequency:',MinExpF,"\n")
          NMinExpF = length(CST$expected[which(CST$expected<5)])
          if (NMinExpF > 0)
            {
              NCells = length(CST$expected)
              cat('Cells with Expected Frequency < 5: ',NMinExpF,' of ',NCells," (",100*NMinExpF/NCells,"%)\n",sep="")
            }
          cat("\n")

        } ## End of if (any(dim(t)...
      
    } ## End of if(format=="SPSS") loop
  else
    stop("unknown format")

  
  
} ## End of the main function Crosstable.R

From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Jun 23 20:55:04 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 23 Jun 2005 14:55:04 -0400
Subject: [R] Stop Warnings for Invalid Factor Level, NAs generated?
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40A8@us-arlington-0668.mail.saic.com>

Check suppressWarnings function.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
khobson at fd9ns01.okladot.state.ok.us
Sent: Thursday, June 23, 2005 9:09 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Stop Warnings for Invalid Factor Level, NAs generated?





How can I stop the following warning from occuring?
invalid factor level, NAs generated in: "[<-.factor"(`*tmp*`, iseq, value =
structure(1, .Label = "12", class = "factor"))

The Label messages are for "5", "8", "12" and "46". I want the NAs to be
generated as needed.

Is this causing R to slow down by generating the warning messages?  There
are over 50.  I loop through up to 10 datasets and append one row from each
to create a summary dataset.

The full code and data is too long to post.  The snippet below might explain
what I'm doing somewhat.  I'll work up some sample data and code if no
solutions are found.

...snip
> # Get info on first and last pair sets and lab names in last pair 
> nlabs <- labdata[["LABNUMBER"]] #e.g. 1, 2, 5, 6, ...
> nolabs <- length(nlabs) #Total number of labs in last pair of labdata 
> #dpdata <- labdata[which(labdata[["LABNUMBER"]] == 13),] #Dummy paired
data row
> dpdata <- rbind(labdata[0,], NA)
> pdata <- dpdata
> # Loop through pairs of labdata to build pdata for pairs (last 10 
> years
or less)
> k=0
> for(j in nlabs){
+   for(i in ifpair:ilpair){
+     k <- k + 1  #Counter for number of pairs.  Start at 1.
+     setwd(dirs[i]); load("labdata.Rdata")    #No trailing "/" in wd
+     setwd(cdir) #Go back to original ESN pair (last pair's folder in
analysis)
+     tpdata <- labdata[which(labdata[["LABNUMBER"]] == j),]
+     if(NROW(tpdata) == 0){
+       tpdata <- dpdata
+       tpdata[which(names(labdata) == "LABNUMBER")] <- j
+       tpdata[which(names(labdata) == "ESN")] <-
ESN-((ilpair-ifpair)+1-k)*2
+             tpdata[which(names(labdata) == "ESNm1")] <-
ESN-((ilpair-ifpair)+1-k)*2-1
+     }
+   #pdata[nrow(pdata)+1,names(tpdata)] <- NA
+   pdata[nrow(pdata)+1,names(tpdata)] <- tpdata
+   }
+ }
There were 50 or more warnings (use warnings() to see the first 50) ...snip

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Jun 23 21:07:59 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Jun 2005 20:07:59 +0100 (BST)
Subject: [R] compiling gap on mac os x
In-Reply-To: <42BAFD46.9000601@aecom.yu.edu>
References: <42BAFD46.9000601@aecom.yu.edu>
Message-ID: <Pine.LNX.4.61.0506231958140.13281@gannet.stats>

Isn't this the age-old story of the MacOS X Fortran compiler not fully 
supporting compilation for dynamic libraries?  Looks like the same
sort of problem it has with LAPACK code.  You could try rewriting the code 
to avoid DATA statements.

Asking MacOS-specific questions on the r-sig-mac is a better idea than 
using a general list.

On Thu, 23 Jun 2005, Kenny Ye wrote:

> Hi, I am having trouble compiling package gap
> http://www.hgmp.mrc.ac.uk/~jzhao/r-progs.htm on Tiger. I have installed
> XcodeTools 2.1. The binary version of gap currently available on CRAN
> has some bug and is fixed in the latest version.
>
> The message I get from R is below. Any help is greatly appreciated.
>
> best regards,
>
> Kenny Ye
>
> tar: Read 1536 bytes from -
> * Installing *source* package 'gap' ...
> ** libs
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c 2k.c -o 2k.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c 2ld.c -o 2ld.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c cline.c -o cline.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c gc.c -o gc.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c gcontrol.c -o gcontrol.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c gif.c -o gif.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c hap.c -o hap.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c hwe.hardy.c -o hwe.hardy.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c kin.morgan.c -o kin.morgan.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c makeped.c -o makeped.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c mia.c -o mia.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c muvar.c -o muvar.o
> g77   -fno-common  -g -O2 -c pfc.f -o pfc.o
> g77   -fno-common  -g -O2 -c pfc.sim.f -o pfc.sim.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c pgc.c -o pgc.o
> gcc-3.3 -no-cpp-precomp
> -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c whscore.c -o whscore.o
> gcc-3.3 -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
> gap.so 2k.o 2ld.o cline.o gc.o gcontrol.o gif.o hap.o hwe.hardy.o
> kin.morgan.o makeped.o mia.o muvar.o pfc.o pfc.sim.o pgc.o whscore.o
> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 -lg2c -lSystem
> -framework R
> ** Removing
> '/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/gap'
>    npackage installation failed
> ld: warning multiple definitions of symbol _i1mach_
> pfc.sim.o definition of _i1mach_ in section (__TEXT,__text)
> /Library/Frameworks/R.framework/R(i1mach.lo) definition of _i1mach_
> ld: 2ld.o has external relocation entries in non-writable section
> (__TEXT,__text) for symbols:
> restFP
> saveFP
> make: *** [gap.so] Error 1
> ERROR: compilation failed for package 'gap'
> ** Restoring previous
> '/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/gap'
> ** Removing
> '/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/pathmix'
> ** Removing
> '/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/pointer'
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at mednetstudy.com  Thu Jun 23 21:23:33 2005
From: MSchwartz at mednetstudy.com (Marc Schwartz)
Date: Thu, 23 Jun 2005 14:23:33 -0500
Subject: [R] how to get such crosstable?
In-Reply-To: <20050623232215.0e11490f@localhost.localdomain>
References: <20050623232215.0e11490f@localhost.localdomain>
Message-ID: <1119554613.5352.62.camel@localhost.localdomain>

On Thu, 2005-06-23 at 23:22 +0800, ronggui wrote:
> i use the CrossTable (frome gregmic package) function to get such a
> table as below.
> but the percentage of the non-NA levels(here 1,2,3,4,5) is not totally
> 100%.
> 
> is there any way to get a table that percentage of  the non-NA
> levelsis totally 100%,as the SPSS' valid percentage.thank you!
> 
> 
> 
>    Cell Contents
> |-------------------------|
> |                   Count |
> |             Row Percent |
> |-------------------------|
> 
> Total Observations in Table:  650 
> 
>           |        1  |        2  |        3  |        4  |        5
> | 
> 
> |-----------|-----------|-----------|-----------|-----------|
>           |      169  |      294  |      151  |       31  |        5
> | 
>           |    0.260% |    0.452% |    0.232% |    0.048% |    0.008%
> | 
> 
> |-----------|-----------|-----------|-----------|-----------|
> 
> Number of Missing Observations: 4 (0.6116208%)


I may be misunderstanding what you are referring to, but I am guessing
that it is that the output is showing proportions, even thought the "%"
symbol is there?

I am not familiar with SPSS' output (Dirk Enzmann contributed the SPSS
output format code to CrossTable), but a quick look suggests that it
works properly in a 2d table, but there is a bug when the object to be
tabulated is 1d, in that the multiplication by 100 is not done in the
SPSS output sub-function.

The ability to tabulate a 1d object was added independent of Dirk's
code, so it looks as if this was missed.

If correct, I can review that and include that as an update, along with
the prior modification to Greg and Nitin.

HTH,

Marc Schwartz



From mkocherg at gmail.com  Thu Jun 23 21:57:14 2005
From: mkocherg at gmail.com (Masha Kocherginsky)
Date: Thu, 23 Jun 2005 14:57:14 -0500
Subject: [R] multinomial logistic regression with survey data
Message-ID: <e15813d505062312574bfff5dd@mail.gmail.com>

Hello,

Is there a function/package that can do multinomial logistic
regression using survey weights, similar to "svymlogit" in Stata? It
appears that only "svyglm" function (which does not allow multinomial
response?) is available in the "survey" package.

Thank you!
Masha Kocherginsky



From jh_168 at yahoo.com  Thu Jun 23 21:58:24 2005
From: jh_168 at yahoo.com (Janet Huang)
Date: Thu, 23 Jun 2005 12:58:24 -0700 (PDT)
Subject: [R] how to reconstruct the discriminant funciton from lda$prior,
	$means and $scaling
Message-ID: <20050623195825.17086.qmail@web50713.mail.yahoo.com>

Hi R folks,

How can I generate the discriment function from lda? 

I have an unbalanced data set. one class has about 25
entries and another class has about 200 entries. 

I used lda for classification 
> z<- lda(V3 ~ V1+V2, data) 
> z
Prior probabilities of groups:
         0          1 
0.91111111 0.08888889 

Group means:
         V1         V2
0 0.4445161 0.04723951
1 0.4058900 0.06934000

Coefficients of linear discriminants:
         LD1
V1 -30.24734
V2  12.56484

predict(z) only give me 11 errors.

I used the following equations to reconstruct the
discrimiat function: 

>gmean <- z$prior %*% z$means
>const <- as.numeric(gmean %*% z$scaling)
>slope <- -z$scaling[1]/z$scaling[2]
>intercept <- const/z$scaling[2] 
>abline(intercept, slope) 

however, this line gives about 50 errors, not the same
one used by the predict(z).

Any suggestions? 

Thanks. 
Janet



From greeksquared at hotmail.com  Thu Jun 23 22:03:43 2005
From: greeksquared at hotmail.com (Teddy Petrou)
Date: Thu, 23 Jun 2005 15:03:43 -0500
Subject: [R] Error in stepAIC function using a survival model
Message-ID: <BAY103-F9A3839E4D0EFF44F028AAD4EA0@phx.gbl>

I keep getting the same error in my survival analysis. I have access to a 
very large database but am just using small subsets to get some results. In 
this particular subset there is 50 explanatory variables(both factors of 
many levels and covariates) and 117 data pieces with some of the data being 
censored.  I am using the stepAIC command to find my model.  My initial 
model is built from all variables that are solely significant in the cox PH 
model.  Well, the stepAIC command works for this particular dataset but it 
crashes when I bootstrap and I come up with a different model set. I get 
another dataset in the following way:

samp = sample(1:117, 117, replace =T)
newdataset = dataset[samp,]

The error I get is

         Error in fitter(X, Y, strats, offset, init, control, weights = 
weights,  :
         NA/NaN/Inf in foreign function call (arg 6)

The dataset that is inputted into the model is itself rid from all NA 
values.  The initial model put into the stepAIC function is comprised of 
about 20 different variables.  When solely modeled by a coxph using:

coxph(Surv(time, censored)~ var1, data = dataset)

a satisfactory result is ouputted.  But when running an additive model with 
many more variables, I get something to the following effect:


                              coef exp(coef) se(coef)      z       p

var1Unknown     0.000  1.00e+00   0.0000    NaN     NaN
varlevel2     -3.242  3.91e-02   2.5701 -1.262 2.1e-01
var2level3      4.730  1.13e+02   3.7318  1.267 2.1e-01
varr2level4      0.314  1.37e+00   2.9936  0.105 9.2e-01
var2level5      0.000  1.00e+00   0.0000    NaN     NaN

This is just cut from the output. Even though it seems the data I am working 
with is not the best, I am still confused as to why the stepAIC function is 
crashing.  It appears to be with the variables that have an NaN for their 
p-values.  But when the variables are modeled separately the model produces 
nice results.

thanks for any help



From spencer.graves at pdf.com  Thu Jun 23 22:28:49 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Jun 2005 13:28:49 -0700
Subject: [R] How to read an excel data into R?
In-Reply-To: <42B9F868.5080807@lbl.gov>
References: <42B9F868.5080807@lbl.gov>
Message-ID: <42BB1B81.6000507@pdf.com>

	  'RSiteSearch' is an R command new with R 2.0.0 or 2.1.0, I believe. 
It essentially passes the argument string to "www.r-project.org" -> 
Search -> "R site search".  Consequently, it requires internet access to 
work.  When I have an R (or S-Plus) question for which I do not already 
know where to find the answer, "R site search" has been my primary 
search tool for some time.

	  spencer graves

Ling Jin wrote:
 > Could you be more specific about RSiteSearch("read excel")? I think it
 > must be useful.
 >
################################
	  Your error message tells me that you have different numbers of fields
in different lines.  You say you, "copied and pasted the excel data into
a txt file".  I usually copy what I want into a clean sheet then File ->
Save, then File -> "Save As" -> "Save as type" = "CSV (Comma delimited)
(*.csv)" or "Text (Tab delimited) (*.txt)".  Excel will ask if I'm sure
a couple of times, and I say yes.  If that's what you've done and still
have a problem, then I have other tools:

	  First, I'll assign the file name to something like "File".  Then,
'readLines(File, n=9)' tells me if the file starts as I think it does.
If I've got extra headers, it will tell me that.

	  Then, I do something like the following:

	  n.flds <- count.fields(File, sep="\t")
	  plot(n.flds)
	  sd(n.flds)

	  Then I play with the arguments to "count.fields" until 'sd(n.flds)'
is 0.  Then I use "read.table" with arguments as I used to get
everything right in 'count.fields'.  If I can't get sd(n.flds) to 0, you
can try read.table with 'fill=TRUE'.  However, when you do that, you
need to check to make sure all the columns line up correctly with the
shorter lines.

	  Also, this issue has been discussed many times.  'RSiteSearch("read
excel")' just produced 1196 hits for me.  If the above doesn't work, you
might try skimming a few from that list.

	  hope this helps.
	  spencer graves

Ling Jin wrote:

> Hi all,
> 
> Does anybody know the easiest way to import excel data into R? I copied 
> and pasted the excel data into a txt file, and tried read.table, but R 
> reported that
> 
> Error in read.table("data_support.txt", sep = " ", header = T) :
>          more columns than column names
> 
> Thanks!
> 
> Ling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From a_mani_sc_gs at vsnl.net  Thu Jun 23 22:43:45 2005
From: a_mani_sc_gs at vsnl.net (A. Mani)
Date: Fri, 24 Jun 2005 02:13:45 +0530
Subject: [R] A. Mani : Read
Message-ID: <200506240213.45246.a_mani_sc_gs@vsnl.net>

Hello,
       What is the best way to read a .rdat file with data in the following 
form :

$xyz1
column names
columns(numeric)

$xyz2
column names
columns(numeric)

$xyz3
column names
columns(numeric)

and so on.

A. Mani
Member, Cal. Math. Soc



From jquiroz at ifop.cl  Thu Jun 23 22:34:32 2005
From: jquiroz at ifop.cl (Juan Carlos Quiroz Espinosa)
Date: Thu, 23 Jun 2005 16:34:32 -0400
Subject: [R] How to create ts..
Message-ID: <27004DDE1590B344855CF773E1D019F117417D@postino.ifop.cl>

Hi user's

How to create a date vector utilized only month and year data. I am
trying to plotting time series without day data.
[Juan Quiroz]



From ggrothendieck at gmail.com  Thu Jun 23 22:59:41 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 16:59:41 -0400
Subject: [R] how to get such crosstable?
In-Reply-To: <1119554613.5352.62.camel@localhost.localdomain>
References: <20050623232215.0e11490f@localhost.localdomain>
	<1119554613.5352.62.camel@localhost.localdomain>
Message-ID: <971536df0506231359f08db05@mail.gmail.com>

On 6/23/05, Marc Schwartz <MSchwartz at mednetstudy.com> wrote:
> On Thu, 2005-06-23 at 23:22 +0800, ronggui wrote:
> > i use the CrossTable (frome gregmic package) function to get such a
> > table as below.
> > but the percentage of the non-NA levels(here 1,2,3,4,5) is not totally
> > 100%.
> >
> > is there any way to get a table that percentage of  the non-NA
> > levelsis totally 100%,as the SPSS' valid percentage.thank you!
> >
> >
> >
> >    Cell Contents
> > |-------------------------|
> > |                   Count |
> > |             Row Percent |
> > |-------------------------|
> >
> > Total Observations in Table:  650
> >
> >           |        1  |        2  |        3  |        4  |        5
> > |
> >
> > |-----------|-----------|-----------|-----------|-----------|
> >           |      169  |      294  |      151  |       31  |        5
> > |
> >           |    0.260% |    0.452% |    0.232% |    0.048% |    0.008%
> > |
> >
> > |-----------|-----------|-----------|-----------|-----------|
> >
> > Number of Missing Observations: 4 (0.6116208%)
> 
> 
> I may be misunderstanding what you are referring to, but I am guessing
> that it is that the output is showing proportions, even thought the "%"
> symbol is there?
> 
> I am not familiar with SPSS' output (Dirk Enzmann contributed the SPSS
> output format code to CrossTable), but a quick look suggests that it
> works properly in a 2d table, but there is a bug when the object to be
> tabulated is 1d, in that the multiplication by 100 is not done in the
> SPSS output sub-function.
> 
> The ability to tabulate a 1d object was added independent of Dirk's
> code, so it looks as if this was missed.
> 
> If correct, I can review that and include that as an update, along with
> the prior modification to Greg and Nitin.
> 


In the interim, here is a workaround.  The function
SPSSCrossTable below defines 

- the format argument to be set to "SPSS"

- a local version of prop.table which gets multiplied 
  by 100 (since looking inside CrossTable we see that 
  the proportions come from prop.table).  

- a local version of CrossTable whose environment is
  set so that it finds the local prop.table before
  the one in base

Finally the do.call runs the local CrossTable:

SPSSCrossTable <- function(...) {
	args <- list(...)
	args$format <- "SPSS"
	prop.table <- function(...) 100*base::prop.table(...)
	environment(CrossTable) <- environment()
	do.call("CrossTable", args)
}
SPSSCrossTable(c(1,1,2,NA))  # test

Note that the above should only be used in cases where the
proportions in CrossTable are incorrectly not multiplied by 100.



From ggrothendieck at gmail.com  Thu Jun 23 23:00:38 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 17:00:38 -0400
Subject: [R] the dimname of a table
In-Reply-To: <1119551665.5352.35.camel@localhost.localdomain>
References: <20050623231217.06fbbffb@localhost.localdomain>
	<1119551665.5352.35.camel@localhost.localdomain>
Message-ID: <971536df05062314005dee24ee@mail.gmail.com>

On 6/23/05, Marc Schwartz <MSchwartz at mednetstudy.com> wrote:
> On Thu, 2005-06-23 at 23:12 +0800, ronggui wrote:
> > i have a data frame(dat) which has many variables.and i use the
> > following script to get the crosstable.
> >
> > >danx2<-c("x1.1","x1.2","x1.3","x1.4","x1.5","x2","x4","x5","x6","x7","x8.1","x8.2","x8.3","x8.4","x11",
> > "x13","x17","x19","x20","x21")
> > >indep<-c("x23","x24","x25","x26","x27","x28.1","x28.2","x29")
> > >for (k in indep){
> >   for (i in danx2){
> >      a<-chisq.test(dat[,i],dat[,k])$p.v<=0.05
> >      if (a)
> > {CrossTable(dat[,i],dat[,k],chisq=T,format="SPSS");cat(rep("=",50),"\n","\n")}
> >  }
> >
> >   it has a little pitfall:the dimnames of table is dat[,i] and
> > dat[,k],but i want it to be like x2,x23...
> > is there any good way to do this?
> >   and in the command CrossTable(dat[,i],dat[,k],chisq=T,format="SPSS")
> > in the loop,is there any other way to get the variable other than
> > dat[,i] and dat[,k]?
> >   thank you !
> 
> Hi,
> 
> I am in between meetings here. Sorry for the delay in my reply to your
> query.
> 
> The best solution is for me to add two new args to CrossTable() to allow
> you to specify these names explicitly, rather than having them as the
> way they are now, which simply takes the x and y args and does:
> 
>     RowData <- deparse(substitute(x))
>     ColData <- deparse(substitute(y))
> 
> The result is that whatever is passed as the x and y arguments, will be
> used as the titles for the row and column labels as you have noted.
> 
> In the mean time, I am attaching an update to CrossTable (which I have
> not extensively tested yet), that you can source() into R via the
> console. The update has two new args called "RowData" and "ColData"
> which will default to NULL, so as to not impact current default
> behavior. You can then set these as part of your loop by passing the
> index values.
> 
> 
> Using one of the examples in ?CrossTable:
> 
> > CrossTable(infert$education, infert$induced, RowData = "Education",
>             ColData = "Induced")
> 
> 
>   Cell Contents
> |-------------------------|
> |                       N |
> | Chi-square contribution |
> |           N / Row Total |
> |           N / Col Total |
> |         N / Table Total |
> |-------------------------|
> 
> 
> Total Observations in Table:  248
> 
> 
>             | Induced
>   Education |         0 |         1 |         2 | Row Total |
> -------------|-----------|-----------|-----------|-----------|
>      0-5yrs |         4 |         2 |         6 |        12 |
>             |     1.232 |     0.506 |     9.898 |           |
>             |     0.333 |     0.167 |     0.500 |     0.048 |
>             |     0.028 |     0.029 |     0.162 |           |
>             |     0.016 |     0.008 |     0.024 |           |
> -------------|-----------|-----------|-----------|-----------|
>     6-11yrs |        78 |        27 |        15 |       120 |
>             |     1.121 |     1.059 |     0.471 |           |
>             |     0.650 |     0.225 |     0.125 |     0.484 |
>             |     0.545 |     0.397 |     0.405 |           |
>             |     0.315 |     0.109 |     0.060 |           |
> -------------|-----------|-----------|-----------|-----------|
>     12+ yrs |        61 |        39 |        16 |       116 |
>             |     0.518 |     1.627 |     0.099 |           |
>             |     0.526 |     0.336 |     0.138 |     0.468 |
>             |     0.427 |     0.574 |     0.432 |           |
>             |     0.246 |     0.157 |     0.065 |           |
> -------------|-----------|-----------|-----------|-----------|
> Column Total |       143 |        68 |        37 |       248 |
>             |     0.577 |     0.274 |     0.149 |           |
> -------------|-----------|-----------|-----------|-----------|
> 
> 
> Let me know if this works or you find a problem. I will do further
> testing here as soon as time permits and get an update to Greg and Nitin
> to include into gregmisc.
> 



1. Assuming that the names of the data frame, dat, are 
set as desired in the output, then with RowData= and ColData= 
arguments implemented the key portion of the poster's problem 
could be written 

  CrossTable(dat[,i], dat[,j], 
    RowData = names(dat)[i], ColData = names(dat)[j])

2. However, instead of naming the new args as RowData= and
ColData= they might be named consistently with the table
function using the argument named dnn= .  In that case the
above could be shortened to:

   CrossTable(dat[,i], dat[,j], dnn = names(dat)[c(i,j)])

3. Even better would be to allow a data frame argument with
automatic use of the names in that data frame in which
case the example becomes just:

   CrossTable(dat[,c(i,j)])


By the way, here is a solution that can be used even with
the existing version of CrossTable. The first portion just
sets up test data assuming we want to specify columns 1 and
3.  The second portion substitutes the column names into the
expression giving s and then evaluates s in the context of dat.

   library(gmodels)

   dat <- data.frame(a = 1:2, b = 1:2, c = 1:2) 
   i <- 1; j <- 3

   nm <- lapply(names(dat), as.name)
   s <- substitute(CrossTable(coli,colj,chisq=TRUE,format="SPSS"),
      list(coli=nm[[i]], colj=nm[[j]]) )
   eval(s, dat)



From ggrothendieck at gmail.com  Thu Jun 23 23:01:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 17:01:47 -0400
Subject: [R] How to create ts..
In-Reply-To: <27004DDE1590B344855CF773E1D019F117417D@postino.ifop.cl>
References: <27004DDE1590B344855CF773E1D019F117417D@postino.ifop.cl>
Message-ID: <971536df05062314012830a80d@mail.gmail.com>

On 6/23/05, Juan Carlos Quiroz Espinosa <jquiroz at ifop.cl> wrote:
> Hi user's
> 
> How to create a date vector utilized only month and year data. I am
> trying to plotting time series without day data.


Check out ?ts and, in particular, look at the 
variable z in the example toward the end of the page.

For the plotting see ?plot.ts or if you have several series
then ?ts.plot .

If your time series is not regularly spaced then check out
the zoo package:

	install.packages("zoo")
	library(zoo)
	vignette("zoo")



From spencer.graves at pdf.com  Thu Jun 23 23:33:33 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Jun 2005 14:33:33 -0700
Subject: [R] How to create ts..
In-Reply-To: <971536df05062314012830a80d@mail.gmail.com>
References: <27004DDE1590B344855CF773E1D019F117417D@postino.ifop.cl>
	<971536df05062314012830a80d@mail.gmail.com>
Message-ID: <42BB2AAD.1060504@pdf.com>

(see below)	

Gabor Grothendieck wrote:

> On 6/23/05, Juan Carlos Quiroz Espinosa <jquiroz at ifop.cl> wrote:
> 
>>Hi user's
>>
>>How to create a date vector utilized only month and year data. I am
>>trying to plotting time series without day data.
> 
> 
> 
> Check out ?ts and, in particular, look at the 
> variable z in the example toward the end of the page.
> 
> For the plotting see ?plot.ts or if you have several series
> then ?ts.plot .
> 
> If your time series is not regularly spaced then check out
> the zoo package:
> 
> 	install.packages("zoo")
> 	library(zoo)
> 	vignette("zoo")
and
	edit(vignette("zoo"))

... which is, I think, new with R 2.0.0 or 2.1.0 and is quite helpful 
for learning a new package that has a vignette.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From fernando.espindola at ifop.cl  Fri Jun 24 00:14:41 2005
From: fernando.espindola at ifop.cl (=?iso-8859-1?Q?Fernando_Esp=EDndola?=)
Date: Thu, 23 Jun 2005 18:14:41 -0400
Subject: [R] Two axes y in same plot
Message-ID: <27004DDE1590B344855CF773E1D019F115BEFA@postino.ifop.cl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050623/b1c8d891/attachment.pl

From Erik_Lamontagne at uqac.ca  Thu Jun 23 20:27:12 2005
From: Erik_Lamontagne at uqac.ca (Erik Lamontagne)
Date: Thu, 23 Jun 2005 14:27:12 -0400
Subject: [R] need help on building packages
Message-ID: <9eaa9a3275.a32759eaa9@uqac.ca>

Good day, I have been trying to build packages for R unix, and the check always failed around the LaTeX area, altho LaTex was installed. 

I dont know wich test dosent pass check because there are a few test made and only one error message for all those test done in 3rd step of the check.

Also could you refer me to a valid format of the package, because I am starting to believe the package format(directories+files) might have changed for version 2.0.0. 

I use R 2.0.1, I am completing a package for R-windows XP, and porting it to R for Unix. I need help to complete those tasks cause I have tried all I could to debug the package building error I was getting.

Thank you

Å…ric Lamontagne
UniversitÅÈ du QuÅÈbec Å‡ Chicoutimi



From 0034058 at fudan.edu.cn  Fri Jun 24 02:58:59 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 24 Jun 2005 08:58:59 +0800
Subject: [R] how can i deal with multiple response data?
Message-ID: <20050624085859.4d2c5ce0@localhost.localdomain>

in a questionair i ask:
Which of the following software packages do you use for data analysis?
1 	R
2 	S-Plus
3 	SAS
4 	SPSS
5 	Stata
6 	others
(In this question, respondents are asked to mark the name of each package they use. Respondents may mark any number of packages)

and code the answer as:
                q1_R   q1_SPlus     q1_SAS    q1_SPSS   q1_Stata  q1_others   
        1.         1          0          0          0          0          1    
        2.         1          1          0          0          1          0     
        3.         0          0          0          0          1          0    
        4.         0          0          1          0          0          0    
        5.         0          0          1          0          0          1    


my question: is there any function  dealing with this data?



-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From ggrothendieck at gmail.com  Fri Jun 24 03:43:10 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 23 Jun 2005 21:43:10 -0400
Subject: [R] A. Mani : Read
In-Reply-To: <200506240213.45246.a_mani_sc_gs@vsnl.net>
References: <200506240213.45246.a_mani_sc_gs@vsnl.net>
Message-ID: <971536df050623184371820282@mail.gmail.com>

On 6/23/05, A. Mani <a_mani_sc_gs at vsnl.net> wrote:
> Hello,
>       What is the best way to read a .rdat file with data in the following
> form :
> 
> $xyz1
> column names
> columns(numeric)
> 
> $xyz2
> column names
> columns(numeric)
> 
> $xyz3
> column names
> columns(numeric)
> 

RRead in the data using readLines.  starting lines are those
1 after those that start with $ and ending lines are one
before those that are empty.  In case there are multiple
empty lines find the smallest empty line greater than each
starting line using cut.  Now for each data frame use read.table 
to read in line numbers starts[i] through ends[i] into a list
called tables, one element per data frame, and set the names
of tables to the lines beginning with $ (without the $).

Lines <- readLines(myfile)
starts <- grep("^\\$", Lines)  + 1
ends <- grep("^[[:space:]]*$", c(Lines, ""))
ends <- ends[cut(starts, c(0, ends), lab = FALSE)]-1
tables <- lapply(seq(along = starts), function(i) 
	read.table(textConnection(Lines[starts[i]:ends[i]]), header = TRUE))
names(tables) <- substr(Lines[starts-1],2)



From murdoch at stats.uwo.ca  Fri Jun 24 03:55:08 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 23 Jun 2005 21:55:08 -0400
Subject: [R] need help on building packages
In-Reply-To: <9eaa9a3275.a32759eaa9@uqac.ca>
References: <9eaa9a3275.a32759eaa9@uqac.ca>
Message-ID: <42BB67FC.4010607@stats.uwo.ca>

Erik Lamontagne wrote:
> Good day, I have been trying to build packages for R unix, and the check always failed around the LaTeX area, altho LaTex was installed. 
> 
> I dont know wich test dosent pass check because there are a few test made and only one error message for all those test done in 3rd step of the check.

Usually there is useful information in the pkg.Rcheck directory that is 
left after the check.  In particular, you'll see the .log file that 
latex produced, and it might be able to help you.

> Also could you refer me to a valid format of the package, because I am starting to believe the package format(directories+files) might have changed for version 2.0.0. 

See the R extensions manual.

> I use R 2.0.1, I am completing a package for R-windows XP, and porting it to R for Unix. I need help to complete those tasks cause I have tried all I could to debug the package building error I was getting.

Does it pass Rcmd check in Windows?

Duncan Murdoch



From lzhtom at hotmail.com  Fri Jun 24 04:07:10 2005
From: lzhtom at hotmail.com (zhihua li)
Date: Fri, 24 Jun 2005 02:07:10 +0000
Subject: [R] quotient and remainder
In-Reply-To: <00b501c577c1$54ad3c80$0540210a@www.domain>
Message-ID: <BAY12-F18A69C95A338DEBFB93EE9C7ED0@phx.gbl>

Dear Dimitris,

I've read <the introduction to R> thoroughly and gooogled in the internet 
about my question, but got no answer. I think it would be great if there's 
a doc grouping R functions into different functional categories.

Thanks a lot for your replies!


>From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
>To: "zhihua li" <lzhtom at hotmail.com>
>CC: <r-help at stat.math.ethz.ch>
>Subject: Re: [R] quotient and remainder
>Date: Thu, 23 Jun 2005 09:01:08 +0200
>
>>11%/%5
>[1] 2
>>11%%5
>[1] 1
>>
>
>Best,
>Dimitris
>
>p.s., I'd suggest you to take a look at the "An Introduction to R" 
>doc
>
>----
>Dimitris Rizopoulos
>Ph.D. Student
>Biostatistical Centre
>School of Public Health
>Catholic University of Leuven
>
>Address: Kapucijnenvoer 35, Leuven, Belgium
>Tel: +32/16/336899
>Fax: +32/16/337015
>Web: http://www.med.kuleuven.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
>----- Original Message ----- From: "zhihua li" <lzhtom at hotmail.com>
>To: <r-help at stat.math.ethz.ch>
>Sent: Thursday, June 23, 2005 8:37 AM
>Subject: [R] quotient and remainder
>
>
>>hi netters
>>
>>Is there a function in R that can compute the quotient and 
>>remainder of a
>>division calculation?   such that when 11 is given as the dividend 
>>and 5
>>the divider, the function returns 2(quotient) and 1(remainder).
>>
>>Thanks a lot!
>>
>>_________________________________________________________________
>>ÅÂÖçÅËÅ¥ÅπÅ‰Å∏ãÅËÅΩÅΩ MSN Explorer:   http://explorer.msn.com/lccn/
>>
>>
>
>
>--------------------------------------------------------------------------------

>
>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>



From simonb at cres10.anu.edu.au  Fri Jun 24 04:55:21 2005
From: simonb at cres10.anu.edu.au (Simon Blomberg)
Date: Fri, 24 Jun 2005 12:55:21 +1000
Subject: [R] quotient and remainder
In-Reply-To: <BAY12-F18A69C95A338DEBFB93EE9C7ED0@phx.gbl>
References: <00b501c577c1$54ad3c80$0540210a@www.domain>
	<BAY12-F18A69C95A338DEBFB93EE9C7ED0@phx.gbl>
Message-ID: <6.2.1.2.0.20050624125136.0308f5d8@mail.ozemail.com.au>

You may find the Reference Cards in the Contributed Documentation section 
of CRAN to be of interest.

Simon.

At 12:07 PM 24/06/2005, zhihua li wrote:
>Dear Dimitris,
>
>I've read <the introduction to R> thoroughly and gooogled in the internet 
>about my question, but got no answer. I think it would be great if there's 
>a doc grouping R functions into different functional categories.
>
>Thanks a lot for your replies!
>
>
>>From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.be>
>>To: "zhihua li" <lzhtom at hotmail.com>
>>CC: <r-help at stat.math.ethz.ch>
>>Subject: Re: [R] quotient and remainder
>>Date: Thu, 23 Jun 2005 09:01:08 +0200
>>
>>>11%/%5
>>[1] 2
>>>11%%5
>>[1] 1
>>
>>Best,
>>Dimitris
>>
>>p.s., I'd suggest you to take a look at the "An Introduction to R" doc
>>
>>----
>>Dimitris Rizopoulos
>>Ph.D. Student
>>Biostatistical Centre
>>School of Public Health
>>Catholic University of Leuven
>>
>>Address: Kapucijnenvoer 35, Leuven, Belgium
>>Tel: +32/16/336899
>>Fax: +32/16/337015
>>Web: http://www.med.kuleuven.be/biostat/
>>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>
>>
>>----- Original Message ----- From: "zhihua li" <lzhtom at hotmail.com>
>>To: <r-help at stat.math.ethz.ch>
>>Sent: Thursday, June 23, 2005 8:37 AM
>>Subject: [R] quotient and remainder
>>
>>
>>>hi netters
>>>
>>>Is there a function in R that can compute the quotient and remainder of a
>>>division calculation?   such that when 11 is given as the dividend and 5
>>>the divider, the function returns 2(quotient) and 1(remainder).
>>>
>>>Thanks a lot!
>>>
>>>_________________________________________________________________
>>>??
?????????????????? MSN Explorer:   http://explorer.msn.com/lccn/
>>>
>>
>>
>>--------------------------------------------------------------------------------
>
>>
>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From larsenmtl at comcast.net  Fri Jun 24 05:25:47 2005
From: larsenmtl at comcast.net (Mark Larsen)
Date: Thu, 23 Jun 2005 23:25:47 -0400
Subject: [R] X11, interactive device
Message-ID: <42BB7D3B.7060303@comcast.net>

lars,

I use R CMD BATCH as a backend to perl and python CGI all the time.  I usually allow R to create post-script (ps) graphics and then use ImageMagick to convert these to PNGs (or JPEGs if you desire).  ImageMagick has some nice interfaces to both perl and python. This method produces very sharp looking images.

Mark Larsen

At 12:49 PM +0200 6/21/05, powdersoul at web.de wrote:

>>Hey,
>>
>>i'm trying to set up an cgi web-application that produces maps and 
>>other graphics with R. for local testing i used my laptop which has 
>>an win xp OS. there everything works fine. now i try to move the 
>>whole system over on an unix machine, mainly because i want to 
>>establish a connection to a database. anyway, i'm using R 2.0.0 on 
>>an other computer in a local network. here comes my problem:
>>
>>i can't create jpeg-files, neither by starting R via my cgi-script 
>>nor by pasting the syntax directly into R. both ways i get the 
>>following error message:
>>
>>error in X11; unable to start device JPEG; could not open JPEG file.
>>
>>i found out that there is no interactive graphic device for the 
>>installed R-Version or it is not set correctly. as i tried 
>>dev.interactive(), the response was [1] FALSE.
>>
>>i'm a bit lost at the moment. i wonder if there is a chance to 
>>change or modify the interactive graphic device or if i really need 
>>to run Xvfb "device" as someone suggested before on the R-help 
>>mailing list. does someone has experience with this kind of trouble, 
>>any help available from somewhere?
>>
>>thanks in advance, lars
>



From 0034058 at fudan.edu.cn  Fri Jun 24 05:54:00 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 24 Jun 2005 11:54:00 +0800
Subject: [R] the quick way to report the invalid value
Message-ID: <20050624115400.116cd0b7@localhost.localdomain>

for example,in my data d,the value 1:9,NA is valid and the others are invalid.
and i want to report something like:
the variable z has invalid value.

is there any function to do so?

i though this will work but fails:

> d
    x  y  z
1   1 NA  1
2   2  2  2
3   3  3  3
4   4  4  4
5   5  5  5
6   6  6  6
7   7  7  7
8   8  8  8
9   9  9  9
10 NA 10  9

>myfun<-function(x) {
if(any(a<-match(x,NA,nomatch=0)>0))
cat(deparse(substitute(x)),"has invalid value","\n")}
>apply(d,2,FUN=myfun)
newX[, i] has invalid value
newX[, i] has invalid value



-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From jenny_edmondson at hotmail.com  Fri Jun 24 06:02:08 2005
From: jenny_edmondson at hotmail.com (Jenny Edmondson)
Date: Fri, 24 Jun 2005 04:02:08 +0000
Subject: [R] mypredict.
Message-ID: <BAY106-F19DD36856E6220F1BF77D49DED0@phx.gbl>

Hi,

I am wondering what does "mypredict.lda<-function(object, 
newdata)predict(object, newdata=newdata)$class" actually do?

I run a few errorest commands in the same function on the same dataset using 
the same classifier lda. The only difference is some use "cv", other use 
"boot" and "632plus". They all share one mypredict.lda.

Will it cause any problem?

Regards, justin



From wildscop at yahoo.com  Fri Jun 24 06:19:07 2005
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Thu, 23 Jun 2005 21:19:07 -0700 (PDT)
Subject: [R] r programming help II
Message-ID: <20050624041907.55719.qmail@web52610.mail.yahoo.com>

Dear List,

Suppose we have a variable K.JUN defined as (with
1=wet, 0=dry):

K.JUN1984 = c(1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) 
K.JUN1985 = c(0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1) 
K.JUN1986 = c(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1)
K.JUN1987 = c(0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0)
K.JUN1988 = c(1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0)
K.JUN1989 = c(0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1)
K.JUN1990 = c(1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0)
K.JUN1991 = c(0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 	0)
K.JUN1992 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0)
K.JUN1993 = c(0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0)
K.JUN1994 = c(0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1)
K.JUN1995 = c(0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0)
K.JUN1996 = c(0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1)
K.JUN1997 = c(0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1)
K.JUN1998 = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0)
K.JUN1999 = c(0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0)
K.JUN2000 = c(1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0)
K.JUN2001 = c(1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1)
K.JUN2002 = c(1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1)

K.JUN<-c(K.JUN1984,K.JUN1985,K.JUN1986,K.JUN1987,K.JUN1988,K.JUN1989,K.JUN1990,K.JUN1991,K.JUN1992,K.JUN1993,K.JUN1994,K.JUN1995,K.JUN1996,K.JUN1997,K.JUN1998,K.JUN1999,K.JUN2000,K.JUN2001,K.JUN2002)

Our motivation is to count number of wet days (1's) in
each weeks. But counting number of wet days for entire
K.JUN will not do.
Thus in r console,

> k<-0;j<-0;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
[1] 1 0 1 1 1 1 1
Run Length Encoding
  lengths: int [1:3] 1 1 5
  values : num [1:3] 1 0 1

where k=0,1,2,3 for each j=0 to 18 (k indicating weeks
of any June, and j indicates years 1984-2002
respectively).

Now we need to sum the run 'lengths' corresponding to
each 'values' "1" (that is 'lengths' of each "0"
'values' need to be excluded) for all k=0,1,2,3 for
each j=0 to 18 (for example, for k<-0;j<-0 we find
'lengths' 1 and 5 for 'values' "1" in the above: then
we sum it as
sum(rle(K.JUN[(1:7)+30*0+7*0])$lengths[c(1,3)])
manually).

Doing so for all k=0,1,2,3 for each j=0 to 18 manually
like this
k<-0;j<-0;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
k<-1;j<-0;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
...
k<-3;j<-18;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
we observe the run 'lengths' corresponding to all "1"
'values' and sum them under a new variable JUN.w as
follows (a cumbersome process obviously):


JUN.w<-c(sum(rle(K.JUN[(1:7)+30*0+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*0+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*0+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*0+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*1+7*0])$lengths[c(2,4,6)]),sum(rle(K.JUN[(1:7)+30*1+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*1+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*1+7*3])$lengths[2]),
sum(rle(K.JUN[(1:7)+30*2+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*2+7*1])$lengths[2]),sum(rle(K.JUN[(1:7)+30*2+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*2+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*3+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*3+7*1])$lengths[c(1,3,5)]),sum(rle(K.JUN[(1:7)+30*3+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*3+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*4+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*4+7*1])$lengths[1]),sum(rle(K.JUN[(1:7)+30*4+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*4+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*5+7*0])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*5+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*5+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*5+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*6+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*6+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*6+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*6+7*3])$lengths[c(1,3)]),
sum(rle(K.JUN[(1:7)+30*7+7*0])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*7+7*1])$lengths[1]),sum(rle(K.JUN[(1:7)+30*7+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*7+7*3])$lengths[2]),
0,sum(rle(K.JUN[(1:7)+30*8+7*1])$lengths[2]),sum(rle(K.JUN[(1:7)+30*8+7*2])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*8+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*9+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*9+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*9+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*9+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*10+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*10+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*10+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*10+7*3])$lengths[c(1,3)]),
sum(rle(K.JUN[(1:7)+30*11+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*11+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*11+7*2])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*11+7*3])$lengths[c(1,3)]),
sum(rle(K.JUN[(1:7)+30*12+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*12+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*12+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*12+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*13+7*0])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*13+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*13+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*13+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*14+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*14+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*14+7*2])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*14+7*3])$lengths[c(2,4)]),
sum(rle(K.JUN[(1:7)+30*15+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*15+7*1])$lengths[2]),sum(rle(K.JUN[(1:7)+30*15+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*15+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*16+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*16+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*16+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*16+7*3])$lengths[1]),
sum(rle(K.JUN[(1:7)+30*17+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*17+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*17+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*17+7*3])$lengths[c(1,3)]),
sum(rle(K.JUN[(1:7)+30*18+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*18+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*18+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*18+7*3])$lengths[c(1,3)]))
# calculating the observed distribution
# This only considers 4 weeks a month, leaving 2 days
of each June
> table(JUN.w)
JUN.w
 0  1  2  3  4  5  6  7 
 1  6  9 13  8 17 15  7 


Now, i know this is a huge problem and time consuming
for us, but is there any way we can solve this
automatically only by specifying K.JUN data variable
(or its particles K.JUN1984,...,K.JUN2002) by means of
R programming?

Thank you for your time. Any hint, help, support,
references will be highly appreciated.

----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD



From ggrothendieck at gmail.com  Fri Jun 24 06:33:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 00:33:20 -0400
Subject: [R] the quick way to report the invalid value
In-Reply-To: <20050624115400.116cd0b7@localhost.localdomain>
References: <20050624115400.116cd0b7@localhost.localdomain>
Message-ID: <971536df0506232133c514287@mail.gmail.com>

On 6/23/05, ronggui <0034058 at fudan.edu.cn> wrote:
> for example,in my data d,the value 1:9,NA is valid and the others are invalid.
> and i want to report something like:
> the variable z has invalid value.
> 
> is there any function to do so?
> 
> i though this will work but fails:
> 
> > d
>    x  y  z
> 1   1 NA  1
> 2   2  2  2
> 3   3  3  3
> 4   4  4  4
> 5   5  5  5
> 6   6  6  6
> 7   7  7  7
> 8   8  8  8
> 9   9  9  9
> 10 NA 10  9
> 
> >myfun<-function(x) {
> if(any(a<-match(x,NA,nomatch=0)>0))
> cat(deparse(substitute(x)),"has invalid value","\n")}
> >apply(d,2,FUN=myfun)
> newX[, i] has invalid value
> newX[, i] has invalid value

Try looping over the names of d:

junk <- lapply(names(d), function(n) cat(n, d[,n], "\n"))



From ggrothendieck at gmail.com  Fri Jun 24 07:20:00 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 01:20:00 -0400
Subject: [R] r programming help II
In-Reply-To: <20050624041907.55719.qmail@web52610.mail.yahoo.com>
References: <20050624041907.55719.qmail@web52610.mail.yahoo.com>
Message-ID: <971536df0506232220149e23ad@mail.gmail.com>

On 6/24/05, Mohammad Ehsanul Karim <wildscop at yahoo.com> wrote:
> Dear List,
> 
> Suppose we have a variable K.JUN defined as (with
> 1=wet, 0=dry):
> 
> K.JUN1984 = c(1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
> K.JUN1985 = c(0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
> 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1)
> K.JUN1986 = c(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
> 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1)
> K.JUN1987 = c(0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
> 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0)
> K.JUN1988 = c(1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0)
> K.JUN1989 = c(0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
> 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1)
> K.JUN1990 = c(1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
> 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0)
> K.JUN1991 = c(0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
> 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,         0)
> K.JUN1992 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
> 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0)
> K.JUN1993 = c(0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
> 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0)
> K.JUN1994 = c(0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
> 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1)
> K.JUN1995 = c(0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
> 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0)
> K.JUN1996 = c(0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
> 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1)
> K.JUN1997 = c(0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
> 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1)
> K.JUN1998 = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
> 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0)
> K.JUN1999 = c(0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
> 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0)
> K.JUN2000 = c(1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
> 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0)
> K.JUN2001 = c(1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
> 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1)
> K.JUN2002 = c(1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
> 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1)
> 
> K.JUN<-c(K.JUN1984,K.JUN1985,K.JUN1986,K.JUN1987,K.JUN1988,K.JUN1989,K.JUN1990,K.JUN1991,K.JUN1992,K.JUN1993,K.JUN1994,K.JUN1995,K.JUN1996,K.JUN1997,K.JUN1998,K.JUN1999,K.JUN2000,K.JUN2001,K.JUN2002)
> 
> Our motivation is to count number of wet days (1's) in
> each weeks. But counting number of wet days for entire
> K.JUN will not do.
> Thus in r console,
> 
> > k<-0;j<-0;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
> [1] 1 0 1 1 1 1 1
> Run Length Encoding
>  lengths: int [1:3] 1 1 5
>  values : num [1:3] 1 0 1
> 
> where k=0,1,2,3 for each j=0 to 18 (k indicating weeks
> of any June, and j indicates years 1984-2002
> respectively).
> 
> Now we need to sum the run 'lengths' corresponding to
> each 'values' "1" (that is 'lengths' of each "0"
> 'values' need to be excluded) for all k=0,1,2,3 for
> each j=0 to 18 (for example, for k<-0;j<-0 we find
> 'lengths' 1 and 5 for 'values' "1" in the above: then
> we sum it as
> sum(rle(K.JUN[(1:7)+30*0+7*0])$lengths[c(1,3)])
> manually).
> 
> Doing so for all k=0,1,2,3 for each j=0 to 18 manually
> like this
> k<-0;j<-0;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
> k<-1;j<-0;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
> ...
> k<-3;j<-18;i<-(1:7)+30*j+7*k;K.JUN[i];rle(K.JUN[i])
> we observe the run 'lengths' corresponding to all "1"
> 'values' and sum them under a new variable JUN.w as
> follows (a cumbersome process obviously):
> 
> 
> JUN.w<-c(sum(rle(K.JUN[(1:7)+30*0+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*0+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*0+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*0+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*1+7*0])$lengths[c(2,4,6)]),sum(rle(K.JUN[(1:7)+30*1+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*1+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*1+7*3])$lengths[2]),
> sum(rle(K.JUN[(1:7)+30*2+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*2+7*1])$lengths[2]),sum(rle(K.JUN[(1:7)+30*2+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*2+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*3+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*3+7*1])$lengths[c(1,3,5)]),sum(rle(K.JUN[(1:7)+30*3+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*3+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*4+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*4+7*1])$lengths[1]),sum(rle(K.JUN[(1:7)+30*4+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*4+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*5+7*0])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*5+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*5+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*5+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*6+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*6+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*6+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*6+7*3])$lengths[c(1,3)]),
> sum(rle(K.JUN[(1:7)+30*7+7*0])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*7+7*1])$lengths[1]),sum(rle(K.JUN[(1:7)+30*7+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*7+7*3])$lengths[2]),
> 0,sum(rle(K.JUN[(1:7)+30*8+7*1])$lengths[2]),sum(rle(K.JUN[(1:7)+30*8+7*2])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*8+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*9+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*9+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*9+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*9+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*10+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*10+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*10+7*2])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*10+7*3])$lengths[c(1,3)]),
> sum(rle(K.JUN[(1:7)+30*11+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*11+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*11+7*2])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*11+7*3])$lengths[c(1,3)]),
> sum(rle(K.JUN[(1:7)+30*12+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*12+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*12+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*12+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*13+7*0])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*13+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*13+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*13+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*14+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*14+7*1])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*14+7*2])$lengths[c(2,4)]),sum(rle(K.JUN[(1:7)+30*14+7*3])$lengths[c(2,4)]),
> sum(rle(K.JUN[(1:7)+30*15+7*0])$lengths[2]),sum(rle(K.JUN[(1:7)+30*15+7*1])$lengths[2]),sum(rle(K.JUN[(1:7)+30*15+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*15+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*16+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*16+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*16+7*2])$lengths[2]),sum(rle(K.JUN[(1:7)+30*16+7*3])$lengths[1]),
> sum(rle(K.JUN[(1:7)+30*17+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*17+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*17+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*17+7*3])$lengths[c(1,3)]),
> sum(rle(K.JUN[(1:7)+30*18+7*0])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*18+7*1])$lengths[c(1,3)]),sum(rle(K.JUN[(1:7)+30*18+7*2])$lengths[1]),sum(rle(K.JUN[(1:7)+30*18+7*3])$lengths[c(1,3)]))
> # calculating the observed distribution
> # This only considers 4 weeks a month, leaving 2 days
> of each June
> > table(JUN.w)
> JUN.w
>  0  1  2  3  4  5  6  7
>  1  6  9 13  8 17 15  7
> 
> 
> Now, i know this is a huge problem and time consuming
> for us, but is there any way we can solve this
> automatically only by specifying K.JUN data variable
> (or its particles K.JUN1984,...,K.JUN2002) by means of
> R programming?
> 
> Thank you for your time. Any hint, help, support,
> references will be highly appreciated.
> 

My understanding is that you want to create a sum for each
of the first 4 weeks in each of 19 months and then tabulate the
frequencies of those 76 numbers.   

1. First create a 30x19
matrix of the numbers where each column is a month.
Take the first 28 rows, i.e. the first 28 days of
each month. Call this j2.  

2. Now reshape j2 into a 7x4x19 array such that the
first dimension is the 7 days of the week, the second
dimension is the 4 weeks in the month and the last
dimension is the 19 years.  Call this 3 dimensional
array, jj3.

3. perform the required sum using apply and then
string out the results with 'c' calling the answer jj.
Note that this is the same as your JUN.w.

4. finally create a table of frequencies.  This gives
the same answer as you displayed.

Here is the code:

j2 <- matrix(K.JUN, 30)[1:28,]
j3 <- array(j2, c(7,4,19))
jj <- c(apply(j3, 2:3, sum))
table(jj)



From ripley at stats.ox.ac.uk  Fri Jun 24 08:32:57 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 07:32:57 +0100 (BST)
Subject: [R] quotient and remainder
In-Reply-To: <BAY12-F18A69C95A338DEBFB93EE9C7ED0@phx.gbl>
References: <BAY12-F18A69C95A338DEBFB93EE9C7ED0@phx.gbl>
Message-ID: <Pine.LNX.4.61.0506240727040.25946@gannet.stats>

On Fri, 24 Jun 2005, zhihua li wrote:

> I've read <the introduction to R> thoroughly and gooogled in the internet 
> about my question, but got no answer. I think it would be great if there's a 
> doc grouping R functions into different functional categories.

help.start, go to the search function and it has categories.  One is
"arith" which contains what you want.

help.search("remainder") would have found this too.

So the search tools are there: we do expect people to use them before 
posting.  Note that there are so many R functions that a static document 
is not going to be very useful.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 24 08:38:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 07:38:44 +0100 (BST)
Subject: [R] how can i deal with multiple response data?
In-Reply-To: <20050624085859.4d2c5ce0@localhost.localdomain>
References: <20050624085859.4d2c5ce0@localhost.localdomain>
Message-ID: <Pine.LNX.4.61.0506240733200.25946@gannet.stats>

You asked this yesterday: repeatedly asking the same question is just 
annoying to the readership.  Please see the posting guide and try to 
improve a question that receives no response.

You have not said what analysis you intend to do with such data.  Unless 
you do we cannot really help you more than answer your actual question 
(see below).  We cannot even guess for you as we don't know the purpose of 
the study.

On Fri, 24 Jun 2005, ronggui wrote:

> in a questionair i ask:
> Which of the following software packages do you use for data analysis?
> 1 	R
> 2 	S-Plus
> 3 	SAS
> 4 	SPSS
> 5 	Stata
> 6 	others
> (In this question, respondents are asked to mark the name of each package they use. Respondents may mark any number of packages)
>
> and code the answer as:
>                q1_R   q1_SPlus     q1_SAS    q1_SPSS   q1_Stata  q1_others
>        1.         1          0          0          0          0          1
>        2.         1          1          0          0          1          0
>        3.         0          0          0          0          1          0
>        4.         0          0          1          0          0          0
>        5.         0          0          1          0          0          1
>
>
> my question: is there any function  dealing with this data?

Yes.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From temiz at deprem.gov.tr  Fri Jun 24 09:15:22 2005
From: temiz at deprem.gov.tr (orkun)
Date: Fri, 24 Jun 2005 10:15:22 +0300
Subject: [R] definition of  variogram
Message-ID: <42BBB30A.60403@deprem.gov.tr>

hello

I need basic explanation of  variogram
and how to  use in  R.

I will appreciate if you supply them.

regards

Ahmet Temiz

______________________________________
XamimeLT - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr
______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From p.dalgaard at biostat.ku.dk  Fri Jun 24 09:27:04 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jun 2005 09:27:04 +0200
Subject: [R] the quick way to report the invalid value
In-Reply-To: <971536df0506232133c514287@mail.gmail.com>
References: <20050624115400.116cd0b7@localhost.localdomain>
	<971536df0506232133c514287@mail.gmail.com>
Message-ID: <x27jgkdxkn.fsf@turmalin.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> On 6/23/05, ronggui <0034058 at fudan.edu.cn> wrote:
> > for example,in my data d,the value 1:9,NA is valid and the others are invalid.
> > and i want to report something like:
> > the variable z has invalid value.
> > 
> > is there any function to do so?
> > 
> > i though this will work but fails:
> > 
> > > d
> >    x  y  z
> > 1   1 NA  1
> > 2   2  2  2
> > 3   3  3  3
> > 4   4  4  4
> > 5   5  5  5
> > 6   6  6  6
> > 7   7  7  7
> > 8   8  8  8
> > 9   9  9  9
> > 10 NA 10  9
> > 
> > >myfun<-function(x) {
> > if(any(a<-match(x,NA,nomatch=0)>0))
> > cat(deparse(substitute(x)),"has invalid value","\n")}
> > >apply(d,2,FUN=myfun)
> > newX[, i] has invalid value
> > newX[, i] has invalid value
> 
> Try looping over the names of d:
> 
> junk <- lapply(names(d), function(n) cat(n, d[,n], "\n"))

or something along the lines of which(sapply(d,myfun)) where myfun
returns logical. This works only if d is a data frame (not matrix),
but you can also use apply(...,2,..) as in
 
> which(apply(is.na(airquality),2,any))
  Ozone Solar.R
      1       2
> which(sapply(airquality, function(x) any(is.na(x)) ))
  Ozone Solar.R
      1       2

If you want the names as a character vector, names(which(...)) gets
you there, or you can do

> z <- apply(is.na(airquality),2,any)
> names(z)[z]
[1] "Ozone"   "Solar.R"
 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Fri Jun 24 09:33:45 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 24 Jun 2005 09:33:45 +0200
Subject: [R] definition of  variogram
In-Reply-To: <42BBB30A.60403@deprem.gov.tr>
References: <42BBB30A.60403@deprem.gov.tr>
Message-ID: <42BBB759.5080503@statistik.uni-dortmund.de>

orkun wrote:

> hello
> 
> I need basic explanation of  variogram
> and how to  use in  R.

Why do you want to use something (variogram) you do not even know?
Please read the posting guide, in particular the sections on "homework" 
and "cross posting".

Uwe Ligges



> 
> I will appreciate if you supply them.
> 
> regards
> 
> Ahmet Temiz
> 
> ______________________________________
> XamimeLT - installed on mailserver for domain @deprem.gov.tr
> Queries to: postmaster at deprem.gov.tr
> ______________________________________
> The views and opinions expressed in this e-mail message are ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jun 24 09:35:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 24 Jun 2005 09:35:18 +0200
Subject: [R] mypredict.
In-Reply-To: <BAY106-F19DD36856E6220F1BF77D49DED0@phx.gbl>
References: <BAY106-F19DD36856E6220F1BF77D49DED0@phx.gbl>
Message-ID: <42BBB7B6.4080103@statistik.uni-dortmund.de>

Jenny Edmondson wrote:

> Hi,
> 
> I am wondering what does "mypredict.lda<-function(object, 
> newdata)predict(object, newdata=newdata)$class" actually do?
> 
> I run a few errorest commands in the same function on the same dataset using 
> the same classifier lda. The only difference is some use "cv", other use 
> "boot" and "632plus". They all share one mypredict.lda.
> 
> Will it cause any problem?


No.

mypredict.lda() is a wrapper for errorest() which expects to get the 
class from predict(), but not all the other "stuff" predict.lda() provides.

Uwe Ligges



> Regards, justin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Jun 24 09:37:29 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 24 Jun 2005 09:37:29 +0200
Subject: [R] Two axes y in same plot
In-Reply-To: <27004DDE1590B344855CF773E1D019F115BEFA@postino.ifop.cl>
References: <27004DDE1590B344855CF773E1D019F115BEFA@postino.ifop.cl>
Message-ID: <42BBB839.6050100@statistik.uni-dortmund.de>

Fernando Esp??ndola wrote:

> Hi user,
> 
> I try to make plot with two axes y, it is posibility?. Can anyboby help me....

Please:
- read the posting guide
- search the R-help archives (this questions has been asked several times)
- see ?axis


Uwe Ligges

> thank for all
> 
> fernando 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wuming.gong at gmail.com  Fri Jun 24 10:04:08 2005
From: wuming.gong at gmail.com (Wuming Gong)
Date: Fri, 24 Jun 2005 16:04:08 +0800
Subject: [R] How to find the significant differences among interactions in
	logit model?
Message-ID: <b428d06d050624010446aa6bac@mail.gmail.com>

Hi, 

I have a question about interpret the results from logistic regression
model. I used a dataset from the book Categorical Data Analysis (2nd
Edition) by Alan Agresti.

> summary(crabs)
 color  spine       width          satell           weight        psat        
 2:12   1: 37   Min.   :21.0   Min.   : 0.000   Min.   :1200   Mode :logical  
 3:95   2: 15   1st Qu.:24.9   1st Qu.: 0.000   1st Qu.:2000   FALSE:62       
 4:44   3:121   Median :26.1   Median : 2.000   Median :2350   TRUE :111      
 5:22           Mean   :26.3   Mean   : 2.919   Mean   :2437                  
                3rd Qu.:27.7   3rd Qu.: 5.000   3rd Qu.:2850                  
                Max.   :33.5   Max.   :15.000   Max.   :5200                  

> crabs.glm <- glm(psat ~ color*width, family=binomial(), data=crabs)
> summary(crabs.glm)

Call:
glm(formula = psat ~ color * width, family = binomial(), data = crabs)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0546  -0.9129   0.5285   0.8140   1.9657  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)
(Intercept)   -1.75261   11.46409  -0.153    0.878
color3        -8.28735   12.00363  -0.690    0.490
color4       -19.76545   13.34251  -1.481    0.139
color5        -4.10122   13.27532  -0.309    0.757
width          0.10600    0.42656   0.248    0.804
color3:width   0.31287    0.44794   0.698    0.485
color4:width   0.75237    0.50435   1.492    0.136
color5:width   0.09443    0.50042   0.189    0.850

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 225.76  on 172  degrees of freedom
Residual deviance: 183.08  on 165  degrees of freedom
AIC: 199.08

Number of Fisher Scoring iterations: 5

Note the predictors are mixture of continuous data and categorical
data. Here, I wonder whether there is *significant difference* among
the four interactions of color and width (say, to get a p-value). In a
two-way ANOVA, we may do a F-test. But is there an "equivalent" method
for logit model?

Thanks,

Wuming



From ripley at stats.ox.ac.uk  Fri Jun 24 10:22:33 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 09:22:33 +0100 (BST)
Subject: [R] How to find the significant differences among interactions
 in logit model?
In-Reply-To: <b428d06d050624010446aa6bac@mail.gmail.com>
References: <b428d06d050624010446aa6bac@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506240918170.31464@gannet.stats>

Use an analysis of deviance test for the term color:width.  Probably most
clearly by (untested)

crabs.glm2 <- update(crabs.glm, . ~ . - color:width)
anova(crabs.glm2, crabs.glm, test="Chisq")

This is covered with several examples in MASS.

On Fri, 24 Jun 2005, Wuming Gong wrote:

> I have a question about interpret the results from logistic regression
> model.

Not really: this is about comparing two such models.

> I used a dataset from the book Categorical Data Analysis (2nd
> Edition) by Alan Agresti.
>
>> summary(crabs)
> color  spine       width          satell           weight        psat
> 2:12   1: 37   Min.   :21.0   Min.   : 0.000   Min.   :1200   Mode :logical
> 3:95   2: 15   1st Qu.:24.9   1st Qu.: 0.000   1st Qu.:2000   FALSE:62
> 4:44   3:121   Median :26.1   Median : 2.000   Median :2350   TRUE :111
> 5:22           Mean   :26.3   Mean   : 2.919   Mean   :2437
>                3rd Qu.:27.7   3rd Qu.: 5.000   3rd Qu.:2850
>                Max.   :33.5   Max.   :15.000   Max.   :5200
>
>> crabs.glm <- glm(psat ~ color*width, family=binomial(), data=crabs)
>> summary(crabs.glm)
>
> Call:
> glm(formula = psat ~ color * width, family = binomial(), data = crabs)
>
> Deviance Residuals:
>    Min       1Q   Median       3Q      Max
> -2.0546  -0.9129   0.5285   0.8140   1.9657
>
> Coefficients:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept)   -1.75261   11.46409  -0.153    0.878
> color3        -8.28735   12.00363  -0.690    0.490
> color4       -19.76545   13.34251  -1.481    0.139
> color5        -4.10122   13.27532  -0.309    0.757
> width          0.10600    0.42656   0.248    0.804
> color3:width   0.31287    0.44794   0.698    0.485
> color4:width   0.75237    0.50435   1.492    0.136
> color5:width   0.09443    0.50042   0.189    0.850
>
> (Dispersion parameter for binomial family taken to be 1)
>
>    Null deviance: 225.76  on 172  degrees of freedom
> Residual deviance: 183.08  on 165  degrees of freedom
> AIC: 199.08
>
> Number of Fisher Scoring iterations: 5
>
> Note the predictors are mixture of continuous data and categorical
> data. Here, I wonder whether there is *significant difference* among
> the four interactions of color and width (say, to get a p-value). In a
> two-way ANOVA, we may do a F-test. But is there an "equivalent" method
> for logit model?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From john.marsland at wmgfunds.com  Fri Jun 24 10:29:49 2005
From: john.marsland at wmgfunds.com (John Marsland)
Date: Fri, 24 Jun 2005 09:29:49 +0100
Subject: [R] Packages and their Management
Message-ID: <322246A1-C6AA-4E10-A45E-16F970286699@wmgfunds.com>

I was reading with interest Prof. Ripley's article in the May 2005  
edition R News entitled "Packages and their Management in R2.1.0".

Does anybody know if it is possible to install.packages() directly  
from a Subversion repository; I thought it might be possible to add  
an option method="svn" to the function?

Ideally, one might want to add the Subversion version number to the R  
package version in the DESCRIPTION file for consistency.

Any ideas?

John Marsland



From luc_tardieu at yahoo.fr  Fri Jun 24 10:41:49 2005
From: luc_tardieu at yahoo.fr (luc tardieu)
Date: Fri, 24 Jun 2005 10:41:49 +0200 (CEST)
Subject: [R] how can i deal with multiple response data?
In-Reply-To: <Pine.LNX.4.61.0506240733200.25946@gannet.stats>
Message-ID: <20050624084149.68737.qmail@web25805.mail.ukl.yahoo.com>

Hi,

What do you want to do with these data?

If you just need to know what share of the respondants
use each software, I suggest to make means of columns,
the sum of lines will give you the number of softwares
used by each respondent, the mean of the sums of lines
will thus give you the average number of softwares
used, etc.

Luc



> On Fri, 24 Jun 2005, ronggui wrote:
> 
> > in a questionair i ask:
> > Which of the following software packages do you
> use for data analysis?
> > 1 	R
> > 2 	S-Plus
> > 3 	SAS
> > 4 	SPSS
> > 5 	Stata
> > 6 	others
> > (In this question, respondents are asked to mark
> the name of each package they use. Respondents may
> mark any number of packages)
> >
> > and code the answer as:
> >                q1_R   q1_SPlus     q1_SAS   
> q1_SPSS   q1_Stata  q1_others
> >        1.         1          0          0         
> 0          0          1
> >        2.         1          1          0         
> 0          1          0
> >        3.         0          0          0         
> 0          1          0
> >        4.         0          0          1         
> 0          0          0
> >        5.         0          0          1         
> 0          0          1
> >
> >
> > my question: is there any function  dealing with
> this data?



From jalumley at gmail.com  Fri Jun 24 10:42:08 2005
From: jalumley at gmail.com (james lumley)
Date: Fri, 24 Jun 2005 09:42:08 +0100
Subject: [R] mulitple mixed regression with spline
Message-ID: <1cb0e9c405062401424a842b4@mail.gmail.com>

Hi, I'm looking to implement a regression with mixed terms. I have 2
biological endpoints for a dataset of n=77, one linearly related and
the other fits a spline.  I want to combine these two terms in a
linear regression for prediction, then apply the model to a test set.

this works fine, good r2 and I've graphed the spline.
m1<-lm(y~x1,data=train)
m2<-smooth.spline(x2,y); (spl)

what i want is
y=x+bilin(x2)

any tips? sorry, fairly new user.

J.



From r.hankin at noc.soton.ac.uk  Fri Jun 24 10:55:11 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 24 Jun 2005 09:55:11 +0100
Subject: [R] hcl()
Message-ID: <e9ea76d728facff81812a8694dd27563@soc.soton.ac.uk>

Hello everyone

I am struggling with hcl().

It says on the manpage that   "240 yields blue".

With this:

plot(1:50,pch=16,col=hcl(h=240, c=50, l=1:50))

I get mostly blue, but also some red, dots.  Note that h=240 
throughout.  If 240 is blue,
how come there's a red dot there?  Or is it just my monitor?

Can some colour expert help me on this?

--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Fri Jun 24 10:58:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 09:58:50 +0100 (BST)
Subject: [R] Packages and their Management
In-Reply-To: <322246A1-C6AA-4E10-A45E-16F970286699@wmgfunds.com>
References: <322246A1-C6AA-4E10-A45E-16F970286699@wmgfunds.com>
Message-ID: <Pine.LNX.4.61.0506240953010.31964@gannet.stats>

On Fri, 24 Jun 2005, John Marsland wrote:

> I was reading with interest Prof. Ripley's article in the May 2005
> edition R News entitled "Packages and their Management in R2.1.0".
>
> Does anybody know if it is possible to install.packages() directly
> from a Subversion repository; I thought it might be possible to add
> an option method="svn" to the function?

It's download.file that is used to retrieve packages (see the help file). 
You would need a suitable client and a suitable syntax to specify the tree 
you wanted, but it would be possible.

I would be surprised if you put actual sources of a package into svn:
R CMD build both adds things and changes the DESCRIPTION file.

> Ideally, one might want to add the Subversion version number to the R
> package version in the DESCRIPTION file for consistency.

You are allowed to do that: just add a field for your own use. The vast 
majority of packages are not under svn, of course, and I tag a released 
package with its released number (just as the R sources do).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 24 11:02:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 10:02:55 +0100 (BST)
Subject: [R] mulitple mixed regression with spline
In-Reply-To: <1cb0e9c405062401424a842b4@mail.gmail.com>
References: <1cb0e9c405062401424a842b4@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506240959130.31964@gannet.stats>

On Fri, 24 Jun 2005, james lumley wrote:

> Hi, I'm looking to implement a regression with mixed terms. I have 2

What do you mean by `mixed'?  Not I think  in the sense of Pinheiro & 
Bates' book or nlme.

> biological endpoints for a dataset of n=77, one linearly related and
> the other fits a spline.  I want to combine these two terms in a
> linear regression for prediction, then apply the model to a test set.
>
> this works fine, good r2 and I've graphed the spline.
> m1<-lm(y~x1,data=train)
> m2<-smooth.spline(x2,y); (spl)
>
> what i want is
> y=x+bilin(x2)

You can see several ways to do this in MASS.  Most simply

lm(y ~ x1 + ns(s2, df=?))

for regression splines.  For smoothing splines, see the functions gam() in
packages mgcv and gam (which differ considerably), or bruto() in mda or 
package gss or ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 24 11:14:47 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 10:14:47 +0100 (BST)
Subject: [R] hcl()
In-Reply-To: <e9ea76d728facff81812a8694dd27563@soc.soton.ac.uk>
References: <e9ea76d728facff81812a8694dd27563@soc.soton.ac.uk>
Message-ID: <Pine.LNX.4.61.0506241005590.31964@gannet.stats>

On Fri, 24 Jun 2005, Robin Hankin wrote:

> Hello everyone
>
> I am struggling with hcl().
>
> It says on the manpage that   "240 yields blue".
>
> With this:
>
> plot(1:50,pch=16,col=hcl(h=240, c=50, l=1:50))
>
> I get mostly blue, but also some red, dots.  Note that h=240
> throughout.  If 240 is blue,
> how come there's a red dot there?  Or is it just my monitor?

Looks at the values

> hcl(h=240, c=50, l=1:50)
  [1] "#130100" "#220100" "#2F0000" "#3E0000" "#530000" "#790000" "#FF0000"
  [8] "#0040BB" "#00338E" "#002F7C" "#002D73" "#002D6E" "#002E6A" "#002F69"
[15] "#003068" "#003167" "#003367" "#003568" "#003669" "#00386A" "#003A6B"
[22] "#003C6C" "#003E6E" "#00406F" "#004271" "#004473" "#004674" "#004976"
[29] "#004B78" "#004D7A" "#004F7C" "#00517E" "#005481" "#005683" "#005885"
[36] "#005B87" "#005D89" "#005F8C" "#08628E" "#126490" "#196693" "#1F6995"
[43] "#246B97" "#286E9A" "#2D709C" "#31729F" "#3575A1" "#3877A4" "#3C7AA6"
[50] "#3F7CA9"

3:7 are pure red.

Then try

> hcl(h=240, c=50, l=1:50, fixup=F)
  [1] NA        NA        NA        NA        NA        NA        NA
  [8] NA        NA        NA        NA        NA        NA        NA
[15] NA        NA        NA        NA        NA        NA        NA
[22] NA        NA        NA        NA        NA        NA        NA
[29] NA        NA        NA        NA        NA        NA        NA
[36] NA        NA        NA        "#08628E" "#126490" "#196693" "#1F6995"
[43] "#246B97" "#286E9A" "#2D709C" "#31729F" "#3575A1" "#3877A4" "#3C7AA6"
[50] "#3F7CA9"

You have mainly used invalid values: you cannot have high chroma and low 
luminance (and there are warnings to that effect on the help page).

Not sure the chosen mapping of out-of-gamut specs onto the sRGB gamut is 
particularly helpful, though.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From frt at Codan.dk  Fri Jun 24 11:14:45 2005
From: frt at Codan.dk (Fredrik Thuring)
Date: Fri, 24 Jun 2005 11:14:45 +0200
Subject: [R] Counterpart for Matlab's 'feval'?
Message-ID: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/52c70761/attachment.pl

From wildscop at yahoo.com  Fri Jun 24 11:15:56 2005
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Fri, 24 Jun 2005 02:15:56 -0700 (PDT)
Subject: [R] r programming help III
Message-ID: <20050624091556.40362.qmail@web52611.mail.yahoo.com>

Dear List,

Is there any way we can make the following formula any
shorter by means of R programming:


# Transition Probabilities observed for 19 years
p01<-0.4052863; p11<-0.7309942 
# Now we try to find expected probabilities
# for number of wet days in a week
# as defined by Gabriel, Neumann (1962)
N<-7
S<-1; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p1N0
S<-2; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p2N0
S<-3; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p3N0
S<-4; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p4N0
S<-5; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p5N0
S<-6; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p6N0
S<-7; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1-p01))^a)*((p01/p11)^b))->p7N0
# we obtain probabilities for dry case
pN0<-c(p11^N,p1N0,p2N0,p3N0,p4N0,p5N0,p6N0,p7N0)

S<-0; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p0N1
S<-1; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p1N1
S<-2; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p2N1
S<-3; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p3N1
S<-4; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p4N1
S<-5; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p5N1
S<-6; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p01))^b)*((p01/p11)^a))->p6N1
# we obtain probabilities for wet case
pN1<-c(p0N1,p1N1,p2N1,p3N1,p4N1,p5N1,p6N1, p11^N)
# Use of transition probabilities to get expected
probabilities
d<-p11-p01
P<-p01/(1-d)
pN<-(1-P)*pN0+P*pN1 
# Expected Probabilities for number of wet days in a
week is pN
pN # gives the following output:


[1] 0.05164856 0.05126159 0.10424380 0.16317247
0.20470403 0.20621178 0.16104972
[8] 0.09170593




Any hint, help, support, references will be highly
appreciated.
Thank you for your time. 

----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD



From r.hankin at noc.soton.ac.uk  Fri Jun 24 11:24:18 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 24 Jun 2005 10:24:18 +0100
Subject: [R] Counterpart for Matlab's 'feval'?
In-Reply-To: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>
References: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>
Message-ID: <35c54e07a79f052da1713386d8fa4784@soc.soton.ac.uk>

Hello Fredrik

do.call()  constructs and executes a function call from the name of
      the function and a list of arguments to be passed to it.

R's equivalent to fminsearch is optim() and that of fmin is optimize()


HTH

[neither of these are in R-and-octave2.txt, which I really really ought  
to update]

rksh

On Jun 24, 2005, at 10:14 am, Fredrik Thuring wrote:

>
> Hi!
>
> I've just begun writing a program that searches for the minimum of a
> function with golden section search. In order to do this in a nice way  
> I
> need a function that takes a function name and an argument and returns  
> the
> function value for that argument, i .e just like Matlab's 'feval'. Is
> there any?
>
> Thanks before hand!
>
> Best regards,
> Fredrik Thuring, Codan Insurance A/S Research Department
>
>
> ----------------------------------------------------------------------- 
> -------
> This e-mail and any attachment may be confidential and may also be  
> privileged.
> If you are not the intended recipient, please notify us immediately  
> and then
> delete this e-mail and any attachment without retaining copies or  
> disclosing
> the contents thereof to any other person.
> Thank you.
> ----------------------------------------------------------------------- 
> -------
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From vehbisinan at gmail.com  Fri Jun 24 11:33:05 2005
From: vehbisinan at gmail.com (Vehbi Sinan Tunalioglu)
Date: Fri, 24 Jun 2005 12:33:05 +0300
Subject: [R] loops in R - about inefficiency
Message-ID: <42BBD351.4000105@gmail.com>

Hi,

Can someone technically explain, why does it take so long with loops in R?

Thanks,
Vehbi Sinan Tunalioglu



From ripley at stats.ox.ac.uk  Fri Jun 24 11:34:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 10:34:32 +0100 (BST)
Subject: [R] Counterpart for Matlab's 'feval'?
In-Reply-To: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>
References: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>
Message-ID: <Pine.LNX.4.61.0506241027480.32411@gannet.stats>

On Fri, 24 Jun 2005, Fredrik Thuring wrote:

> I've just begun writing a program that searches for the minimum of a
> function with golden section search. In order to do this in a nice way I
> need a function that takes a function name and an argument and returns the
> function value for that argument, i .e just like Matlab's 'feval'. Is
> there any?

Really?  Take a look at the interface for optimize(): in R you almost 
always want to pass the function not its name.

You could use

feval <- function(fn, x) get(fn)(x)

but it would be rather inefficient to keep looking up the function (and 
there are scoping issues to consider here).

You could also use do.call(fn, list(x)), but that is even less direct.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.be  Fri Jun 24 11:34:59 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Fri, 24 Jun 2005 11:34:59 +0200
Subject: [R] Counterpart for Matlab's 'feval'?
References: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>
Message-ID: <00c901c5789f$fd9f83f0$0540210a@www.domain>

maybe you want something like this:

f <- function(x, FUN, ...){
    FUN <- match.fun(FUN)
    FUN(x, ...)
}

f(seq(-3, 3, 0.5), dnorm, sd = 1.1)
f(seq(-3, 3, 0.5), "pnorm", sd = 1.1)


but also take a look at ?optimize(), which will probably take you out 
of the trouble of writing a new function yourself.


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Fredrik Thuring" <frt at Codan.dk>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 24, 2005 11:14 AM
Subject: [R] Counterpart for Matlab's 'feval'?


>
> Hi!
>
> I've just begun writing a program that searches for the minimum of a
> function with golden section search. In order to do this in a nice 
> way I
> need a function that takes a function name and an argument and 
> returns the
> function value for that argument, i .e just like Matlab's 'feval'. 
> Is
> there any?
>
> Thanks before hand!
>
> Best regards,
> Fredrik Thuring, Codan Insurance A/S Research Department
>
>
> ------------------------------------------------------------------------------
> This e-mail and any attachment may be confidential and may also be 
> privileged.
> If you are not the intended recipient, please notify us immediately 
> and then
> delete this e-mail and any attachment without retaining copies or 
> disclosing
> the contents thereof to any other person.
> Thank you.
> ------------------------------------------------------------------------------
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Fri Jun 24 11:44:09 2005
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 24 Jun 2005 10:44:09 +0100
Subject: [R] Counterpart for Matlab's 'feval'?
In-Reply-To: <35c54e07a79f052da1713386d8fa4784@soc.soton.ac.uk>
References: <OFA8F32A0E.47A99CDE-ONC125702A.0031E73A-C125702A.0032CCA4@codan.dk>
	<35c54e07a79f052da1713386d8fa4784@soc.soton.ac.uk>
Message-ID: <42BBD5E9.8090106@lancaster.ac.uk>

Robin Hankin wrote:

>>I've just begun writing a program that searches for the minimum of a
>>function with golden section search. In order to do this in a nice way  
>>I
>>need a function that takes a function name and an argument and returns  
>>the
>>function value for that argument, i .e just like Matlab's 'feval'. Is
>>there any?

It might be better to write a function that takes a function as an 
argument, instead of the function name:

  > doit = function(f,...){f(...)}
  > doit(sqrt,2)
  [1] 1.414214
  > doit(sum,1,2,3)
  [1] 6

This way you can write little inline anonymous functions:

  > doit(function(a,b,c){a^3+b^2+c},1,4,3)
  [1] 20

The equivalent way with do.call, and function names is:

  > docall=function(fc,...){do.call(fc,list(...))}
  > docall("sqrt",2)
  [1] 1.414214
  > docall("sum",1,2,3)
  [1] 6

But:

  > docall("function(a,b,c){a^3+b^2+c}",1,4,3)
  Error in do.call(fc, list(...)) : couldn't find function 
"function(a,b,c){a^3+b^2+c}"

This can be fixed by using eval and parse instead:

  > doe=function(fc,...){eval(parse(text=fc))(...)}
  > doe("sqrt",2)
  [1] 1.414214
  > doe("sum",1,2,3)
  [1] 6
  > doe("function(a,b,c){a^3+b^2+c}",1,4,3)
  [1] 20

I see other solutions have been posted already. They're probably better.

Baz



From ripley at stats.ox.ac.uk  Fri Jun 24 11:53:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 10:53:06 +0100 (BST)
Subject: [R] loops in R - about inefficiency
In-Reply-To: <42BBD351.4000105@gmail.com>
References: <42BBD351.4000105@gmail.com>
Message-ID: <Pine.LNX.4.61.0506241040280.32411@gannet.stats>

On Fri, 24 Jun 2005, Vehbi Sinan Tunalioglu wrote:

> Can someone technically explain, why does it take so long with loops in R?

First please demonstrate that it does!

a <- 0
system.time(for(i in 1:1e6) a <- a+1)

takes 1 second for a million iterations and you think that is `so long'?
If you do, the technical explanation is that R is interpreted rather than 
compiled.

More likely the problem is *how* you are using loops.

The FAQ mentions reference material on how to use R efficiently and it 
would be more productive to learn to do so rather than claim it is 
inefficient.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.hankin at noc.soton.ac.uk  Fri Jun 24 12:32:49 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Fri, 24 Jun 2005 11:32:49 +0100
Subject: [R] hcl()
In-Reply-To: <Pine.LNX.4.61.0506241005590.31964@gannet.stats>
References: <e9ea76d728facff81812a8694dd27563@soc.soton.ac.uk>
	<Pine.LNX.4.61.0506241005590.31964@gannet.stats>
Message-ID: <49b40ce90b5f93e0da9caa5858cd9568@soc.soton.ac.uk>

Professor Ripley

thanks for this.

>>

>> plot(1:50,pch=16,col=hcl(h=240, c=50, l=1:50))
>>
>> I get mostly blue, but also some red, dots.  Note that h=240
>> throughout.  If 240 is blue,
>> how come there's a red dot there?  Or is it just my monitor?
>

>> hcl(h=240, c=50, l=1:50, fixup=F)
>  [1] NA        NA        NA        NA        NA        NA        NA
>  [8] NA        NA        NA        NA        NA        NA        NA
> [15] NA        NA        NA        NA        NA        NA        NA
> [22] NA        NA        NA        NA        NA        NA        NA
> [29] NA        NA        NA        NA        NA        NA        NA
> [36] NA        NA        NA        "#08628E" "#126490" "#196693" 
> "#1F6995"
> [43] "#246B97" "#286E9A" "#2D709C" "#31729F" "#3575A1" "#3877A4" 
> "#3C7AA6"
> [50] "#3F7CA9"
>
> You have mainly used invalid values: you cannot have high chroma and 
> low luminance (and there are warnings to that effect on the help 
> page).
>
> Not sure the chosen mapping of out-of-gamut specs onto the sRGB gamut 
> is particularly helpful, though.
>


Yes, this is what I was missing: with fixup taking its default value
of TRUE, out-of-range combinations are silently mapped to real RGB
values, [although the description of argument fixup in the
manpage conveys this information to me only now that I have
  your example above to look at].

  If I understand your comment, my odd-looking colours are a result of  
this mapping.

So, how best to determine the maximum chroma for a given luminance and 
hue?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From maechler at stat.math.ethz.ch  Fri Jun 24 14:26:52 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 24 Jun 2005 14:26:52 +0200
Subject: [R] hcl()
In-Reply-To: <49b40ce90b5f93e0da9caa5858cd9568@soc.soton.ac.uk>
References: <e9ea76d728facff81812a8694dd27563@soc.soton.ac.uk>
	<Pine.LNX.4.61.0506241005590.31964@gannet.stats>
	<49b40ce90b5f93e0da9caa5858cd9568@soc.soton.ac.uk>
Message-ID: <17083.64524.608816.593233@stat.math.ethz.ch>

I have written a nice (IMO) function that lets you explore the
hcl space quite nicely, and show its calls.

hcl.wheel <-
    function(chroma = 35, lums = 0:100, hues = 1:360, asp = 1,
             p.cex = 0.6, do.label = FALSE, rev.lum = FALSE,
             fixup = TRUE)
{
    ## Purpose: show chroma "sections" of hcl() color space; see  ?hcl
    ## ----------------------------------------------------------------------
    ## Arguments: chroma: can be vector -> multiple plots are done,
    ##            lums, hues, fixup : all corresponding to hcl()'s args
    ##            rev.lum: logical indicating if luminance
    ## 			should go from outer to inner
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date: 24 Jun 2005

    stopifnot(is.numeric(lums), lums >= 0, lums <= 100,
              is.numeric(hues), hues >= 0, hues <= 360,
              is.numeric(chroma), chroma >= 0, (nch <- length(chroma)) >= 1)
    if(is.unsorted(hues)) hues <- sort(hues)
    if(nch > 1) {
        op <- par(mfrow= n2mfrow(nch), mar = c(0,0,0,0))
        on.exit(par(op))
    }
    for(i.c in 1:nch) {
        plot(-1:1,-1:1, type="n", axes = FALSE, xlab="",ylab="", asp = asp)
        ## main = sprintf("hcl(h = <angle>, c = %g)", chroma[i.c]),
        text(0.4, 0.99, paste("chroma =", format(chroma[i.c])),
             adj = 0, font = 4)
        l.s <- (if(rev.lum) rev(lums) else lums) / max(lums) # <= 1
        for(ang in hues) { # could do all this using outer() instead of for()...
            a. <- ang * pi/180
            z.a <- exp(1i * a.)
            cols <- hcl(ang, c = chroma[i.c], l = lums, fixup = fixup)
            points(l.s * z.a, pch = 16, col = cols, cex = p.cex)
            ##if(do."text") : draw the 0,45,90,... angle "lines"
            if(do.label)
                text(z.a*1.05, labels = ang, col = cols[length(cols)/2],
                     srt = ang)
        }
        if(!fixup) ## show the outline
            lines(exp(1i * hues * pi/180))
   }
   invisible()
}

##-- and now a few interesting calls

hcl.wheel() # and watch it redraw when you fiddle with the graphic window
hcl.wheel(rev.lum= TRUE) # dito
hcl.wheel(do.lab = TRUE) # dito


## Now watch:
hcl.wheel(ch = c(25,35,45,55))

hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.4)
hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.3, fixup = FALSE)
hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.3, rev.lum = TRUE)
x11() # new device -- in order to compare with previous :
hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.3, rev.lum = TRUE, fixup=FALSE)

## the last two, in my eyes show that
## 1) fixup = TRUE {the default!} works quite nicely in most cases
## 2) Robin's original problem was a sample of a much larger "problem"
##    where IMO the 'fixup' algorithm ``breaks down'' and I
##    think should be improvable.

Martin Maechler, ETH Zurich


>>>>> "Robin" == Robin Hankin <r.hankin at noc.soton.ac.uk>
>>>>>     on Fri, 24 Jun 2005 11:32:49 +0100 writes:

    Robin> Professor Ripley thanks for this.

    >>>

    >>> plot(1:50,pch=16,col=hcl(h=240, c=50, l=1:50))
    >>> 
    >>> I get mostly blue, but also some red, dots.  Note that
    >>> h=240 throughout.  If 240 is blue, how come there's a
    >>> red dot there?  Or is it just my monitor?
    >>

    >>> hcl(h=240, c=50, l=1:50, fixup=F)
    >> [1] NA NA NA NA NA NA NA [8] NA NA NA NA NA NA NA [15] NA
    >> NA NA NA NA NA NA [22] NA NA NA NA NA NA NA [29] NA NA NA
    >> NA NA NA NA [36] NA NA NA "#08628E" "#126490" "#196693"
    >> "#1F6995" [43] "#246B97" "#286E9A" "#2D709C" "#31729F"
    >> "#3575A1" "#3877A4" "#3C7AA6" [50] "#3F7CA9"
    >> 
    >> You have mainly used invalid values: you cannot have high
    >> chroma and low luminance (and there are warnings to that
    >> effect on the help page).
    >> 
    >> Not sure the chosen mapping of out-of-gamut specs onto
    >> the sRGB gamut is particularly helpful, though.
    >> 


    Robin> Yes, this is what I was missing: with fixup taking
    Robin> its default value of TRUE, out-of-range combinations
    Robin> are silently mapped to real RGB values, [although the
    Robin> description of argument fixup in the manpage conveys
    Robin> this information to me only now that I have your
    Robin> example above to look at].

    Robin>   If I understand your comment, my odd-looking
    Robin> colours are a result of this mapping.

    Robin> So, how best to determine the maximum chroma for a
    Robin> given luminance and hue?




    Robin> -- Robin Hankin Uncertainty Analyst National
    Robin> Oceanography Centre, Southampton European Way,
    Robin> Southampton SO14 3ZH, UK tel 023-8059-7743

    Robin> ______________________________________________
    Robin> R-help at stat.math.ethz.ch mailing list
    Robin> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    Robin> do read the posting guide!
    Robin> http://www.R-project.org/posting-guide.html


    Robin> !DSPAM:42bbe27a59521197013008!



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Jun 24 14:34:08 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 24 Jun 2005 08:34:08 -0400
Subject: [R] r programming help III
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40AA@us-arlington-0668.mail.saic.com>

I am not sure if you can make this code shorter through R specific
programming, but You can through simple programming technique of using
functions:

pFunc = function (S, d, N, p01, p11)
{ 
  c1<-N+1/2-abs(2*S-N-1/2+d); c<-1:c1;
  a<-ceiling((c-1+d)/2); b<-ceiling((c-d)/2);
 
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1+d),(b-1+d))*choose((N-S-d),a-d)*(((1
-p11)/(1-p01))^a)*((p01/p11)^b))
}

# Transition Probabilities observed for 19 years p01<-0.4052863;
p11<-0.7309942 # Now we try to find expected probabilities # for number of
wet days in a week # as defined by Gabriel, Neumann (1962)
N<-7
p01=0.4052863
p11=0.7309942
pDry <- function(S) pFunc(S,0,N,p01, p11)
pWet <- function(S) pFunc(S,1,N,p01, p11)
# we obtain probabilities for dry case
pN0<-c(p11^N, pDry(1), pDry(2), pDry(3), pDry(4), pDry(5), pDry(6), pDry(7)
)
# we obtain probabilities for wet case
pN1<-c(pWet(0), pWet(1), pWet(2), pWet(3), pWet(4), pWet(5), pWet(6), p11^N)
# Use of transition probabilities to get expected probabilities
d<-p11-p01
P<-p01/(1-d)
pN<-(1-P)*pN0+P*pN1

It is possible that you can find some ways of simplifying the pFunc.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mohammad Ehsanul
Karim
Sent: Friday, June 24, 2005 5:16 AM
To: r help
Subject: [R] r programming help III

Dear List,

Is there any way we can make the following formula any shorter by means of R
programming:


# Transition Probabilities observed for 19 years p01<-0.4052863;
p11<-0.7309942 # Now we try to find expected probabilities # for number of
wet days in a week # as defined by Gabriel, Neumann (1962)
N<-7
S<-1; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p1N0
S<-2; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p2N0
S<-3; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p3N0
S<-4; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p4N0
S<-5; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p5N0
S<-6; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p6N0
S<-7; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
-p01))^a)*((p01/p11)^b))->p7N0
# we obtain probabilities for dry case
pN0<-c(p11^N,p1N0,p2N0,p3N0,p4N0,p5N0,p6N0,p7N0)

S<-0; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p0N1
S<-1; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p1N1
S<-2; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p2N1
S<-3; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p3N1
S<-4; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p4N1
S<-5; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p5N1
S<-6; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
a<-ceiling((c-1)/2); b<-ceiling(c/2);
(p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
01))^b)*((p01/p11)^a))->p6N1
# we obtain probabilities for wet case
pN1<-c(p0N1,p1N1,p2N1,p3N1,p4N1,p5N1,p6N1, p11^N) # Use of transition
probabilities to get expected probabilities
d<-p11-p01
P<-p01/(1-d)
pN<-(1-P)*pN0+P*pN1
# Expected Probabilities for number of wet days in a week is pN pN # gives
the following output:


[1] 0.05164856 0.05126159 0.10424380 0.16317247
0.20470403 0.20621178 0.16104972
[8] 0.09170593




Any hint, help, support, references will be highly
appreciated.
Thank you for your time. 

----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From yunusbb at inbox.ru  Fri Jun 24 14:42:28 2005
From: yunusbb at inbox.ru (unknown)
Date: Fri, 24 Jun 2005 16:42:28 +0400
Subject: [R] (no subject)
Message-ID: <E1DlnVs-000AuR-00.yunusbb-inbox-ru@f13.mail.ru>

There is a function 'simMD()' in 'popgen' library which 
simulates a sample of genotype data as follows:
> library(popgen)
> x <- simMD(20, 2, 2, p = NULL, c(0.09, 0.05), ac = 2, beta = 1)
> x
  , , 1
 
      [,1] [,2]
 [1,]    1    1
 [2,]    1    1
      ... 
[37,]    1    2
[38,]    2    2
[39,]    2    2
[40,]    2    2
 
, , 2
 
      [,1] [,2]
 [1,]    2    2
 [2,]    1    2
 [3,]    1    2
      ...
[38,]    2    1
[39,]    1    2
[40,]    1    2
>
How can I repeat this function, for example, 1000 times to generate 1000 samples and assign each output to distinct 'vector' Xi, where i=1,2,...,1000
The goal is to generate a large number of samples using this function and then use them in further analysis.
Any suggestions would be appreciated
Sitdikov Mansor



From ggrothendieck at gmail.com  Fri Jun 24 14:45:03 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 08:45:03 -0400
Subject: [R] r programming help III
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40AA@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40AA@us-arlington-0668.mail.saic.com>
Message-ID: <971536df050624054547b95e01@mail.gmail.com>

Admittedly this is not much of an additional simplification but one other
thing that can be done is that pN0 and pN1 can be written in terms of 
sapply, e.g.

pN0<-c(p11^N, sapply(1:7, pDry))


On 6/24/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> I am not sure if you can make this code shorter through R specific
> programming, but You can through simple programming technique of using
> functions:
> 
> pFunc = function (S, d, N, p01, p11)
> {
>  c1<-N+1/2-abs(2*S-N-1/2+d); c<-1:c1;
>  a<-ceiling((c-1+d)/2); b<-ceiling((c-d)/2);
> 
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1+d),(b-1+d))*choose((N-S-d),a-d)*(((1
> -p11)/(1-p01))^a)*((p01/p11)^b))
> }
> 
> # Transition Probabilities observed for 19 years p01<-0.4052863;
> p11<-0.7309942 # Now we try to find expected probabilities # for number of
> wet days in a week # as defined by Gabriel, Neumann (1962)
> N<-7
> p01=0.4052863
> p11=0.7309942
> pDry <- function(S) pFunc(S,0,N,p01, p11)
> pWet <- function(S) pFunc(S,1,N,p01, p11)
> # we obtain probabilities for dry case
> pN0<-c(p11^N, pDry(1), pDry(2), pDry(3), pDry(4), pDry(5), pDry(6), pDry(7)
> )
> # we obtain probabilities for wet case
> pN1<-c(pWet(0), pWet(1), pWet(2), pWet(3), pWet(4), pWet(5), pWet(6), p11^N)
> # Use of transition probabilities to get expected probabilities
> d<-p11-p01
> P<-p01/(1-d)
> pN<-(1-P)*pN0+P*pN1
> 
> It is possible that you can find some ways of simplifying the pFunc.
> 
> Jarek
> ====================================================\=======
> 
>  Jarek Tuszynski, PhD.                           o / \
>  Science Applications International Corporation  <\__,|
>  (703) 676-4192                                   ">   \
>  Jaroslaw.W.Tuszynski at saic.com                     `    \
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mohammad Ehsanul
> Karim
> Sent: Friday, June 24, 2005 5:16 AM
> To: r help
> Subject: [R] r programming help III
> 
> Dear List,
> 
> Is there any way we can make the following formula any shorter by means of R
> programming:
> 
> 
> # Transition Probabilities observed for 19 years p01<-0.4052863;
> p11<-0.7309942 # Now we try to find expected probabilities # for number of
> wet days in a week # as defined by Gabriel, Neumann (1962)
> N<-7
> S<-1; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p1N0
> S<-2; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p2N0
> S<-3; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p3N0
> S<-4; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p4N0
> S<-5; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p5N0
> S<-6; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p6N0
> S<-7; c0<-N+1/2-abs(2*S-N-1/2);c<-1:c0;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose((S-1),(b-1))*choose((N-S),a)*(((1-p11)/(1
> -p01))^a)*((p01/p11)^b))->p7N0
> # we obtain probabilities for dry case
> pN0<-c(p11^N,p1N0,p2N0,p3N0,p4N0,p5N0,p6N0,p7N0)
> 
> S<-0; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p0N1
> S<-1; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p1N1
> S<-2; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p2N1
> S<-3; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p3N1
> S<-4; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p4N1
> S<-5; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p5N1
> S<-6; c1<-N+1/2-abs(2*S-N+1/2);c<-1:c1;
> a<-ceiling((c-1)/2); b<-ceiling(c/2);
> (p11^S)*((1-p01)^(N-S))*sum(choose(S,a)*choose((N-S-1),(b-1))*(((1-p11)/(1-p
> 01))^b)*((p01/p11)^a))->p6N1
> # we obtain probabilities for wet case
> pN1<-c(p0N1,p1N1,p2N1,p3N1,p4N1,p5N1,p6N1, p11^N) # Use of transition
> probabilities to get expected probabilities
> d<-p11-p01
> P<-p01/(1-d)
> pN<-(1-P)*pN0+P*pN1
> # Expected Probabilities for number of wet days in a week is pN pN # gives
> the following output:
> 
> 
> [1] 0.05164856 0.05126159 0.10424380 0.16317247
> 0.20470403 0.20621178 0.16104972
> [8] 0.09170593
> 
> 
> 
> 
> Any hint, help, support, references will be highly
> appreciated.
> Thank you for your time.
> 
> ----------------------------------
> 
> Mohammad Ehsanul Karim
> 
> Web: http://snipurl.com/ehsan
> ISRT, University of Dhaka, BD
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From thomas.fabbro at unifr.ch  Fri Jun 24 14:47:47 2005
From: thomas.fabbro at unifr.ch (FabbroThomas)
Date: Fri, 24 Jun 2005 14:47:47 +0200
Subject: [R] lme4 extracting individual variance components
Message-ID: <ab9f9859884529a0c531ee8da8ffe96a@unifr.ch>

Hi,
For further calculations I need to extract indivdual Variances of 
different random effects from a fitted model.
I found out how to extract the correlations 
(VarCorr(m1)@reSumry$group1) but I was not able to find a way to 
extract the other components individually.

To extract the Residuals I tried: (ranef(m1)@ stdErr) which 
unfortunately did not work.
Thank you very much for your help!

Thomas


My model:

 > m1<- lmer(y ~ trtt + (trtt-1|group3) + (trtt-1|group2) + 
(trtt-1|group1), d1)
 > m1
Linear mixed-effects model fit by REML
Formula: y ~ trtt + (trtt - 1 | group3) + (trtt - 1 | group2) + (trtt - 
      1 | group1)
    Data: d1
       AIC      BIC    logLik MLdeviance REMLdeviance
  1819.454 2003.915 -874.7269   1736.421     1749.454
Random effects:
  Groups   Name                Variance Std.Dev. Corr
  group1      trtt1/TR1       0.115094 0.33926
           trtt1/TR2 0.338576 0.58187   0.177
           trtt2/TR1       0.141726 0.37647  -0.002 -0.007
           trtt2/TR2 0.327869 0.57260  -0.007 -0.002  0.321
  group2     trtt1/TR1       0.026259 0.16205
           trtt1/TR2 0.021771 0.14755  0.325
           trtt2/TR1       0.025403 0.15938  0.898  0.062
           trtt2/TR2 0.024479 0.15646  0.048  0.012  0.156
  group3   trtt1/TR1       0.028206 0.16795
           trtt1/TR2 0.133718 0.36567  0.848
           trtt2/TR1       0.057741 0.24029  0.990  0.905
           trtt2/TR2 0.110075 0.33178  0.638  0.944  0.731
  Residual                     0.021744 0.14746
# of obs: 1437, groups: group1, 1088; group2, 31; group3, 3

Fixed effects:
                        Estimate  Std. Error   DF t value  Pr(>|t|)
(Intercept)            4.875320    0.102476 1433  47.575 < 2.2e-16 ***
trtt1/TR2   -4.647211    0.149408 1433 -31.104 < 2.2e-16 ***
trtt2/TR1          0.577186    0.052293 1433  11.037 < 2.2e-16 ***
trtt2/TR2   -4.237753    0.166187 1433 -25.500 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



I am working with:

R 2.1.0
lme4 0.96-1
Matrix 0.96-3
lattice 0.11-6
latticeExtra 0.1-3

On Mac OS X 10.3.9



From ripley at stats.ox.ac.uk  Fri Jun 24 14:58:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 13:58:37 +0100 (BST)
Subject: [R] hcl()
In-Reply-To: <17083.64524.608816.593233@stat.math.ethz.ch>
References: <e9ea76d728facff81812a8694dd27563@soc.soton.ac.uk>
	<Pine.LNX.4.61.0506241005590.31964@gannet.stats>
	<49b40ce90b5f93e0da9caa5858cd9568@soc.soton.ac.uk>
	<17083.64524.608816.593233@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0506241355030.14148@gannet.stats>

On Fri, 24 Jun 2005, Martin Maechler wrote:

> I have written a nice (IMO) function that lets you explore the
> hcl space quite nicely, and show its calls.
>
> hcl.wheel <-
>    function(chroma = 35, lums = 0:100, hues = 1:360, asp = 1,
>             p.cex = 0.6, do.label = FALSE, rev.lum = FALSE,
>             fixup = TRUE)
> {
>    ## Purpose: show chroma "sections" of hcl() color space; see  ?hcl
>    ## ----------------------------------------------------------------------
>    ## Arguments: chroma: can be vector -> multiple plots are done,
>    ##            lums, hues, fixup : all corresponding to hcl()'s args
>    ##            rev.lum: logical indicating if luminance
>    ## 			should go from outer to inner
>    ## ----------------------------------------------------------------------
>    ## Author: Martin Maechler, Date: 24 Jun 2005
>
>    stopifnot(is.numeric(lums), lums >= 0, lums <= 100,
>              is.numeric(hues), hues >= 0, hues <= 360,
>              is.numeric(chroma), chroma >= 0, (nch <- length(chroma)) >= 1)
>    if(is.unsorted(hues)) hues <- sort(hues)
>    if(nch > 1) {
>        op <- par(mfrow= n2mfrow(nch), mar = c(0,0,0,0))
>        on.exit(par(op))
>    }
>    for(i.c in 1:nch) {
>        plot(-1:1,-1:1, type="n", axes = FALSE, xlab="",ylab="", asp = asp)
>        ## main = sprintf("hcl(h = <angle>, c = %g)", chroma[i.c]),
>        text(0.4, 0.99, paste("chroma =", format(chroma[i.c])),
>             adj = 0, font = 4)
>        l.s <- (if(rev.lum) rev(lums) else lums) / max(lums) # <= 1
>        for(ang in hues) { # could do all this using outer() instead of for()...
>            a. <- ang * pi/180
>            z.a <- exp(1i * a.)
>            cols <- hcl(ang, c = chroma[i.c], l = lums, fixup = fixup)
>            points(l.s * z.a, pch = 16, col = cols, cex = p.cex)
>            ##if(do."text") : draw the 0,45,90,... angle "lines"
>            if(do.label)
>                text(z.a*1.05, labels = ang, col = cols[length(cols)/2],
>                     srt = ang)
>        }
>        if(!fixup) ## show the outline
>            lines(exp(1i * hues * pi/180))
>   }
>   invisible()
> }
>
> ##-- and now a few interesting calls
>
> hcl.wheel() # and watch it redraw when you fiddle with the graphic window
> hcl.wheel(rev.lum= TRUE) # dito
> hcl.wheel(do.lab = TRUE) # dito
>
>
> ## Now watch:
> hcl.wheel(ch = c(25,35,45,55))
>
> hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.4)
> hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.3, fixup = FALSE)
> hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.3, rev.lum = TRUE)
> x11() # new device -- in order to compare with previous :
> hcl.wheel(ch = seq(10, 90, by = 10), p.cex = 0.3, rev.lum = TRUE, fixup=FALSE)
>
> ## the last two, in my eyes show that
> ## 1) fixup = TRUE {the default!} works quite nicely in most cases
> ## 2) Robin's original problem was a sample of a much larger "problem"
> ##    where IMO the 'fixup' algorithm ``breaks down'' and I
> ##    think should be improvable.

It shows that there are slow but massive hue shifts at low luminance.
There are better algorithms than the C code uses but maybe not much better 
for colours way out of gamut.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From karen at biology.biol.wits.ac.za  Fri Jun 24 16:22:13 2005
From: karen at biology.biol.wits.ac.za (Karen Kotschy)
Date: Fri, 24 Jun 2005 16:22:13 +0200
Subject: [R] Mahalanobis distances
Message-ID: <200506241622.13468.karen@gecko.biol.wits.ac.za>

Dear R community

Have just recently got back into R after a long break and have been amazed at 
how much it has grown, and how active the list is! Thank you so much to all 
those who contribute to this amazing project.

My question:
I am trying to calculate Mahalanobis distances for a matrix called "fgmatrix"

>dim(fgmatrix)
[1] 76 15

>fg.cov <- cov.wt(fgmatrix)
>mahalanobis(fgmatrix, center = fg.cov$center, cov = fg.cov$cov)

Then I get an error message "Covariance matrix is apparently singular"

What does this mean? I can't see anything strange about the covariance matrix, 
and am not getting anywhere with the help files.

>dim(fg.cov$cov)
[1] 15 15
>length(fg.cov$center)
[1] 15


Thanks
-- 
Karen Kotschy
Centre for Water in the Environment
University of the Witwatersrand
Johannesburg
South Africa

P/Bag X3, Wits, 2050
Tel: +2711 717-6425



From scabannes at free.fr  Fri Jun 24 16:13:22 2005
From: scabannes at free.fr (Cabannes)
Date: Fri, 24 Jun 2005 16:13:22 +0200
Subject: [R]  Rep mahalanobis distance
Message-ID: <42BC1502.4010001@free.fr>

Dear Karen,

> I am trying to calculate Mahalanobis distances for a matrix called 
> "fgmatrix"
>

> Then I get an error message "Covariance matrix is apparently singular"
>
> What does this mean? I can't see anything strange about the covariance 
> matrix, and am not getting anywhere with the help files.
>
As you can read in the help for "mahalanobis distance" :

    Returns the Mahalanobis distance of all rows in 'x' and the vector
    mu='center' with respect to Sigma='cov'. This is (for vector 'x')
    defined as

                D2 = (x - mu)' Sigma^{-1} (x - mu)

that need to invert the cov matrix, so if not invertible...

Sam



From f.calboli at imperial.ac.uk  Fri Jun 24 16:38:40 2005
From: f.calboli at imperial.ac.uk (Federico Calboli)
Date: Fri, 24 Jun 2005 15:38:40 +0100
Subject: [R] R demos
Message-ID: <1119623920.14681.58.camel@localhost.localdomain>

Hi All,

I am currently preparing some form of slideshow introducing R and its
capabilities for some colleagues. The thing will be about 30 mins, and
I'd like to have some "pretty pictures" and some "amazing facts" (I'm
trying to sell, obviously :)).

Can I ask if it's possible to easily retrieve a gross figure of the
number of functions in R considering the "base" install and all the
libraries available?

Apart from graphics and lattice, are there any more packages producing
eye catching graphics (possibly with a survival analysis/epidemiological
bend)?

Cheers,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com



From domenico.cozzetto at uniroma1.it  Fri Jun 24 16:52:12 2005
From: domenico.cozzetto at uniroma1.it (Domenico Cozzetto)
Date: Fri, 24 Jun 2005 16:52:12 +0200
Subject: [R] help with PCA and classical scaling
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
Message-ID: <BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>

Dear all,
I have a matrix of dissimilarities and I'd like to get a 2d embedding. I'm
not an expert of these techniques, so I'm a bit confused... Furthermore I
was not able to find on the web any satisfactory tutorial. So even though
this may be not the most appropriate place to discuss about this issues, I'd
be very grateful to those who will reply.

My first question is: Do I need an initial embedding of my data before
applying PCA through the methods princomp or prcomp in the stats package?

What does it mean that PCA and classical scaling are equivalent? And where
can I find a proof of this fact?

If this is true, I should get the same results by applying the methods
prcomp() or cmdscale() in stats. If it may help, use the dissimilarities in
the attached "diss.tab" file.

Thank you very much in advance for your attention.
Domenico

From dmbates at gmail.com  Fri Jun 24 16:51:14 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 24 Jun 2005 09:51:14 -0500
Subject: [R] lme4 extracting individual variance components
In-Reply-To: <ab9f9859884529a0c531ee8da8ffe96a@unifr.ch>
References: <ab9f9859884529a0c531ee8da8ffe96a@unifr.ch>
Message-ID: <40e66e0b0506240751685c61fa@mail.gmail.com>

On 6/24/05, FabbroThomas <thomas.fabbro at unifr.ch> wrote:
> Hi,
> For further calculations I need to extract indivdual Variances of
> different random effects from a fitted model.
> I found out how to extract the correlations
> (VarCorr(m1)@reSumry$group1) but I was not able to find a way to
> extract the other components individually.

Then look at

str(VarCorr(m1))

to find out what the structure is.  It contains a slot 'scale' which
is the estimated standard error of the noise term and a slot called
reSumry which is a list with components corresponding to the grouping
factors.  Each of those components has a slot named StdDev which gives
the relative standard deviation (relative to the value of the scale
slot).

> To extract the Residuals I tried: (ranef(m1)@ stdErr) which
> unfortunately did not work.
> Thank you very much for your help!
> 
> Thomas
> 
> 
> My model:
> 
>  > m1<- lmer(y ~ trtt + (trtt-1|group3) + (trtt-1|group2) +
> (trtt-1|group1), d1)
>  > m1
> Linear mixed-effects model fit by REML
> Formula: y ~ trtt + (trtt - 1 | group3) + (trtt - 1 | group2) + (trtt -
>       1 | group1)
>     Data: d1
>        AIC      BIC    logLik MLdeviance REMLdeviance
>   1819.454 2003.915 -874.7269   1736.421     1749.454
> Random effects:
>   Groups   Name                Variance Std.Dev. Corr
>   group1      trtt1/TR1       0.115094 0.33926
>            trtt1/TR2 0.338576 0.58187   0.177
>            trtt2/TR1       0.141726 0.37647  -0.002 -0.007
>            trtt2/TR2 0.327869 0.57260  -0.007 -0.002  0.321
>   group2     trtt1/TR1       0.026259 0.16205
>            trtt1/TR2 0.021771 0.14755  0.325
>            trtt2/TR1       0.025403 0.15938  0.898  0.062
>            trtt2/TR2 0.024479 0.15646  0.048  0.012  0.156
>   group3   trtt1/TR1       0.028206 0.16795
>            trtt1/TR2 0.133718 0.36567  0.848
>            trtt2/TR1       0.057741 0.24029  0.990  0.905
>            trtt2/TR2 0.110075 0.33178  0.638  0.944  0.731
>   Residual                     0.021744 0.14746
> # of obs: 1437, groups: group1, 1088; group2, 31; group3, 3
> 
> Fixed effects:
>                         Estimate  Std. Error   DF t value  Pr(>|t|)
> (Intercept)            4.875320    0.102476 1433  47.575 < 2.2e-16 ***
> trtt1/TR2   -4.647211    0.149408 1433 -31.104 < 2.2e-16 ***
> trtt2/TR1          0.577186    0.052293 1433  11.037 < 2.2e-16 ***
> trtt2/TR2   -4.237753    0.166187 1433 -25.500 < 2.2e-16 ***
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> 
> 
> I am working with:
> 
> R 2.1.0
> lme4 0.96-1
> Matrix 0.96-3
> lattice 0.11-6
> latticeExtra 0.1-3
> 
> On Mac OS X 10.3.9
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From helprhelp at gmail.com  Fri Jun 24 16:59:13 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Fri, 24 Jun 2005 09:59:13 -0500
Subject: [R] comparing strength of association instead of strength of
	evidence?
Message-ID: <cdf817830506240759cd3ae6a@mail.gmail.com>

Hi,
I asked this question before, which was hidden in a bunch of
questions. I repharse it here and hope I can get some help this time:

I have 2 contingency tables which have the same group variable Y. I
want to compare the strength of association between X1/Y and X2/Y. I
am not sure if comparing p-values IS the way  even though the
probability of seeing such "weird" observation under H0 defines
p-value and it might relate to the strength of association somehow.
But I read the following statement from Alan Agresti's "An
Introduction to Categorical Data Analysis" :
"Chi-squared tests simply indicate the degree of EVIDENCE for an
association....It is sensible to decompose chi-squared into
components, study residuals, and estimate parameters such as odds
ratios that describe the STRENGTH OF ASSOCIATION".

Can I do this "decomposition" in R for the following example including
2 contingency tables?

> tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> tab1
      [,1]    [,2]
[1,] 11266 2151526
[2,]   125   31734

> tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> tab2
      [,1]    [,2]
[1,] 43571 2119221
[2,]    52   31807


BTW, is there some good forum on the theory of statistics? r-help is a
good one but I don't want to bother people by asking some questions
weakly associated with R here.

Thanks,

-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From sarah.goslee at gmail.com  Fri Jun 24 17:09:04 2005
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 24 Jun 2005 11:09:04 -0400
Subject: [R] help with PCA and classical scaling
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>
References: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
	<BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>
Message-ID: <efb536d50506240809531afb28@mail.gmail.com>

On 6/24/05, Domenico Cozzetto <domenico.cozzetto at uniroma1.it> wrote:
> Furthermore I
> was not able to find on the web any satisfactory tutorial. So even though
> this may be not the most appropriate place to discuss about this issues, I'd
> be very grateful to those who will reply.

http://ordination.okstate.edu
http://www.nicholas.duke.edu/landscape/classes/env358/env358.html

Neither are R tutorials, but if you understand the method, you can
better understand how to use the tools available.

Sarah
-- 
Sarah Goslee
http://www.stringpage.com



From murdoch at stats.uwo.ca  Fri Jun 24 17:13:36 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 24 Jun 2005 11:13:36 -0400
Subject: [R] R demos
In-Reply-To: <1119623920.14681.58.camel@localhost.localdomain>
References: <1119623920.14681.58.camel@localhost.localdomain>
Message-ID: <42BC2320.3060703@stats.uwo.ca>

On 6/24/2005 10:38 AM, Federico Calboli wrote:
> Hi All,
> 
> I am currently preparing some form of slideshow introducing R and its
> capabilities for some colleagues. The thing will be about 30 mins, and
> I'd like to have some "pretty pictures" and some "amazing facts" (I'm
> trying to sell, obviously :)).
> 
> Can I ask if it's possible to easily retrieve a gross figure of the
> number of functions in R considering the "base" install and all the
> libraries available?

I think it depends on what you mean by "all the libraries".  If you mean 
  all the contributed packages on CRAN, then one way to get something 
along those lines is to look at the index.txt file in 
R_HOME/doc/html/search.  It lists one entry per help topic in whatever 
packages you have installed.  Help topics often document more than one 
function, and sometimes document data or other non-functions, so this 
isn't exactly what you were asking for.

If you really want the number of functions, you could do the following.

 > sapply(library()$results[,1], library, character.only=TRUE)

This attaches all installed packages.

 > apropos('.*', mode='function')

This gives the list of all functions on the search list.  I get 3011 in 
R-patched with the base and recommended packages.  I can't do it in a 
copy that has most of CRAN and Bioconductor loaded, because after 
attaching about 190 packages, library() dies with an error "Maximal 
number of DLLs reached.."  At that point it's finding around 9600 
functions.  (The number of topics in index.txt was 14768.)

Duncan Murdoch



From jfbrennan at rogers.com  Fri Jun 24 17:16:35 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 24 Jun 2005 11:16:35 -0400
Subject: [R] R demos
In-Reply-To: <1119623920.14681.58.camel@localhost.localdomain>
Message-ID: <200506241516.j5OFGaSn006867@hypatia.math.ethz.ch>

?demo may give you some ideas.




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico Calboli
Sent: June 24, 2005 10:39 AM
To: r-help
Subject: [R] R demos

Hi All,

I am currently preparing some form of slideshow introducing R and its
capabilities for some colleagues. The thing will be about 30 mins, and
I'd like to have some "pretty pictures" and some "amazing facts" (I'm
trying to sell, obviously :)).

Can I ask if it's possible to easily retrieve a gross figure of the
number of functions in R considering the "base" install and all the
libraries available?

Apart from graphics and lattice, are there any more packages producing
eye catching graphics (possibly with a survival analysis/epidemiological
bend)?

Cheers,

Federico Calboli

-- 
Federico C. F. Calboli
Department of Epidemiology and Public Health
Imperial College, St Mary's Campus
Norfolk Place, London W2 1PG

Tel  +44 (0)20 7594 1602     Fax (+44) 020 7594 3193

f.calboli [.a.t] imperial.ac.uk
f.calboli [.a.t] gmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jfbrennan at rogers.com  Fri Jun 24 17:20:36 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 24 Jun 2005 11:20:36 -0400
Subject: [R] R demos
In-Reply-To: <1119623920.14681.58.camel@localhost.localdomain>
Message-ID: <200506241520.j5OFKaQD007844@hypatia.math.ethz.ch>

I have noticed that some data sets seem not to be available to me when I am
running some of the examples or demos since I have changed versions. For
example:
demo(persp)


        demo(persp)
        ---- ~~~~~

Type  <Return>   to start : 

R>if (dev.cur() <= 1) get(getOption("device"))()

R>is.dev.interactive <- .Device %in% c("X11", "GTK", 
    "gnome", "quartz", "windows", "JavaGD")

R>op <- par(ask = is.dev.interactive)

R>x <- seq(-10, 10, length = 50)

R>y <- x

R>rotsinc <- function(x, y) {
    sinc <- function(x) {
        y <- sin(x)/x
        y[is.na(y)] <- 1
        y
    }
    10 * sinc(sqrt(x^2 + y^2))
}

R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))

R>z <- outer(x, y, rotsinc)

R>par(bg = "white")

R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5, 
    col = "lightblue")
Waiting to confirm page change...

R>title(sub = ".")

R>title(main = sinc.exp)

R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5, 
    col = "lightblue", ltheta = 120, shade = 0.75, ticktype = "detailed", 
    xlab = "X", ylab = "Y", zlab = "Z")
Waiting to confirm page change...

R>title(sub = ".")

R>title(main = sinc.exp)

R>z <- 2 * volcano
Error in eval.with.vis(expr, envir, enclos) : 
        Object "volcano" not found

So most of this demo works but volcano is not found.
Any ideas what may be going wrong.
Also for example
R>?reshape
R>summary(Indometh)
Error in summary(Indometh) : Object "Indometh" not found

version         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    1.0            
year     2005           
month    04             
day      18             
language R



From sms13+ at pitt.edu  Fri Jun 24 17:27:28 2005
From: sms13+ at pitt.edu (sms13+@pitt.edu)
Date: Fri, 24 Jun 2005 11:27:28 -0400
Subject: [R] interpreting Weibull survival regression
Message-ID: <83375937.1119612448@Lab26.DOMAIN.IE.PITT.EDU>

Hi,
I was wondering if someone can help me 
interpret the results of running 
weibreg.

I run the following and get the 
following R output.
> weibreg(Surv(time, censor)~covar)
fit$fail =  0
Call:
weibreg(formula = Surv(time, 
censor)~covar)

Covariate           Mean       Coef 
Rel.Risk      L-R p   Wald p
covar     319.880    -0.002     0.998 
0.000

log(scale)          0.000     8.239 
3786.326               0.000
log(shape)          0.000     0.265 
1.304               0.000

Events                    172
Total time at risk        845891
Max. log. likelihood      -1609.4
LR test statistic         34.4
Degrees of freedom        3
Overall p-value           1.65026e-07


I would just like to find the estimated 
mean survival time as a function of the 
covariate in the model, but am not sure 
how to use this output to find that.
Any help would be greatly appreciated.

Thank you,
Steven



From dmbates at gmail.com  Fri Jun 24 17:24:34 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Fri, 24 Jun 2005 10:24:34 -0500
Subject: [R] R demos
In-Reply-To: <200506241520.j5OFKaQD007844@hypatia.math.ethz.ch>
References: <1119623920.14681.58.camel@localhost.localdomain>
	<200506241520.j5OFKaQD007844@hypatia.math.ethz.ch>
Message-ID: <40e66e0b05062408242bde8905@mail.gmail.com>

On 6/24/05, Jim Brennan <jfbrennan at rogers.com> wrote:
> I have noticed that some data sets seem not to be available to me when I am
> running some of the examples or demos since I have changed versions. For
> example:
> demo(persp)
> 
> 
>         demo(persp)
>         ---- ~~~~~
> 
> Type  <Return>   to start :
> 
> R>if (dev.cur() <= 1) get(getOption("device"))()
> 
> R>is.dev.interactive <- .Device %in% c("X11", "GTK",
>     "gnome", "quartz", "windows", "JavaGD")
> 
> R>op <- par(ask = is.dev.interactive)
> 
> R>x <- seq(-10, 10, length = 50)
> 
> R>y <- x
> 
> R>rotsinc <- function(x, y) {
>     sinc <- function(x) {
>         y <- sin(x)/x
>         y[is.na(y)] <- 1
>         y
>     }
>     10 * sinc(sqrt(x^2 + y^2))
> }
> 
> R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))
> 
> R>z <- outer(x, y, rotsinc)
> 
> R>par(bg = "white")
> 
> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>     col = "lightblue")
> Waiting to confirm page change...
> 
> R>title(sub = ".")
> 
> R>title(main = sinc.exp)
> 
> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>     col = "lightblue", ltheta = 120, shade = 0.75, ticktype = "detailed",
>     xlab = "X", ylab = "Y", zlab = "Z")
> Waiting to confirm page change...
> 
> R>title(sub = ".")
> 
> R>title(main = sinc.exp)
> 
> R>z <- 2 * volcano
> Error in eval.with.vis(expr, envir, enclos) :
>         Object "volcano" not found
> 
> So most of this demo works but volcano is not found.
> Any ideas what may be going wrong.
> Also for example
> R>?reshape
> R>summary(Indometh)
> Error in summary(Indometh) : Object "Indometh" not found

For some reason you do not have the datasets package attached.

> find(Indometh)
[1] "package:datasets"
> find(volcano)
[1] "package:datasets"



From ldimitro at wfubmc.edu  Fri Jun 24 17:38:58 2005
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Fri, 24 Jun 2005 11:38:58 -0400
Subject: [R] Memory limits using read.table on Windows XP Pro
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>

Hello,

When I try:

 geno
<-read.table("2500.geno.tab",header=TRUE,sep="\t",na.strings=".",quote="
",comment.char="",colClasses=c("factor"),nrows=2501)

I get, after hour(s) of work:

Error: cannot allocate vector of size 9 Kb

I have:

Rgui.exe --max-mem-size=3Gb

and

multi(0)disk(0)rdisk(0)partition(1)\WINDOWS="Microsoft Windows XP
Professional" /fastdetect /NoExecute=OptIn /PAE /3GB

in boot.ini

2500.geno.tab is a tab-delimited text table with 2500 x 125000 =
312,500,000 3-level (two alphabet characters) factors (x 4 bites =
1,250,000,000 (1.25GB). Even if we double it (as per read.table help)
it's still 2.5GB < 3Gb. And actually Windows Task Manager shows peak mem
use for Rgui 2,056,992K (~2.057GB) and total memory used 2.62GB. And the
total physical memory is 4GB (of which windows recognizes above 3GB) 

Any help or suggestions?

Thanks,
Latchezar



From jfbrennan at rogers.com  Fri Jun 24 17:46:26 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 24 Jun 2005 11:46:26 -0400
Subject: [R] R demos
In-Reply-To: <40e66e0b05062408242bde8905@mail.gmail.com>
Message-ID: <200506241546.j5OFkQYE015556@hypatia.math.ethz.ch>

Thanks!
 I realized the datasets were not available, but not that they were in a
separate package. I have never had to load this package in previous versions
and assumed they were part of one of the default main packages.
 
An easy fix was to adjust my default packages in the Rprofile file to
include "datasets"
This is for those who don't know a file in the etc directory.

options(defaultPackages=c("utils"  ,"datasets",  "graphics" ,"stats",
"methods", "MASS", "gtools"))

Not sure how this happened as I never had to do this before.

-----Original Message-----
From: Douglas Bates [mailto:dmbates at gmail.com] 
Sent: June 24, 2005 11:25 AM
To: Jim Brennan
Cc: r-help
Subject: Re: [R] R demos

On 6/24/05, Jim Brennan <jfbrennan at rogers.com> wrote:
> I have noticed that some data sets seem not to be available to me when I
am
> running some of the examples or demos since I have changed versions. For
> example:
> demo(persp)
> 
> 
>         demo(persp)
>         ---- ~~~~~
> 
> Type  <Return>   to start :
> 
> R>if (dev.cur() <= 1) get(getOption("device"))()
> 
> R>is.dev.interactive <- .Device %in% c("X11", "GTK",
>     "gnome", "quartz", "windows", "JavaGD")
> 
> R>op <- par(ask = is.dev.interactive)
> 
> R>x <- seq(-10, 10, length = 50)
> 
> R>y <- x
> 
> R>rotsinc <- function(x, y) {
>     sinc <- function(x) {
>         y <- sin(x)/x
>         y[is.na(y)] <- 1
>         y
>     }
>     10 * sinc(sqrt(x^2 + y^2))
> }
> 
> R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))
> 
> R>z <- outer(x, y, rotsinc)
> 
> R>par(bg = "white")
> 
> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>     col = "lightblue")
> Waiting to confirm page change...
> 
> R>title(sub = ".")
> 
> R>title(main = sinc.exp)
> 
> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>     col = "lightblue", ltheta = 120, shade = 0.75, ticktype = "detailed",
>     xlab = "X", ylab = "Y", zlab = "Z")
> Waiting to confirm page change...
> 
> R>title(sub = ".")
> 
> R>title(main = sinc.exp)
> 
> R>z <- 2 * volcano
> Error in eval.with.vis(expr, envir, enclos) :
>         Object "volcano" not found
> 
> So most of this demo works but volcano is not found.
> Any ideas what may be going wrong.
> Also for example
> R>?reshape
> R>summary(Indometh)
> Error in summary(Indometh) : Object "Indometh" not found

For some reason you do not have the datasets package attached.

> find(Indometh)
[1] "package:datasets"
> find(volcano)
[1] "package:datasets"



From f.harrell at vanderbilt.edu  Fri Jun 24 17:46:42 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 24 Jun 2005 10:46:42 -0500
Subject: [R] R demos
In-Reply-To: <1119623920.14681.58.camel@localhost.localdomain>
References: <1119623920.14681.58.camel@localhost.localdomain>
Message-ID: <42BC2AE2.5080809@vanderbilt.edu>

Federico Calboli wrote:
> Hi All,
> 
> I am currently preparing some form of slideshow introducing R and its
> capabilities for some colleagues. The thing will be about 30 mins, and
> I'd like to have some "pretty pictures" and some "amazing facts" (I'm
> trying to sell, obviously :)).
> 
> Can I ask if it's possible to easily retrieve a gross figure of the
> number of functions in R considering the "base" install and all the
> libraries available?
> 
> Apart from graphics and lattice, are there any more packages producing
> eye catching graphics (possibly with a survival analysis/epidemiological
> bend)?

For survival analysis/epi graphics examples you might look at 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/BioMod/notes.pdf, 
Chapters 10 and 19.  Also see the Alzola-Harrell book's chapter on the 
Design package at 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf

Frank

> 
> Cheers,
> 
> Federico Calboli
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From spencer.graves at pdf.com  Fri Jun 24 17:56:50 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 24 Jun 2005 08:56:50 -0700
Subject: [R] Mahalanobis distances
In-Reply-To: <200506241622.13468.karen@gecko.biol.wits.ac.za>
References: <200506241622.13468.karen@gecko.biol.wits.ac.za>
Message-ID: <42BC2D42.2050707@pdf.com>

	  The first thing I'd try is "scale", as that should not affect the 
Mahalinobis distances:

	  Fgmat <- scale(fgmatrix)
	  fg.cov <- cov.wt(Fgmat)
	  mahalanobis(Fgmat, center = Fg.cov$center, cov = Fg.cov$cov)

	  Does this give you the same result.  If no, the problem was that 
fgmatrix was not sufficiently well conditioned to support this 
computation.

	  If this does NOT solve the problem, I'd manually contruct a ginverse 
of Fg.cov$cov, proceeding roughly as outlined in the following example:

set.seed(1)
X10 <- array(rnorm(760), dim=c(76, 10))
X15.10 <- cbind(X10, X10[,1:5])

fg.cov <- cov.wt(X15.10)
mahalanobis(X15.10, center = fg.cov$center, cov = fg.cov$cov)

(S15.10 <- eigen(fg.cov$cov, symmetric=TRUE))
# Only 10 non-zero eigenvalues
fg.Info <- crossprod(S15.10$vectors[,1:10] / 
rep(sqrt(S15.10$values[1:10]), 15))
mahalanobis(X15.10, center = fg.cov$center,
	cov = fg.cov$cov, inverted=TRUE)

	  The key is computing your own generalized inverse and using that with 
"inverted=TRUE".

	  spencer graves

Karen Kotschy wrote:
> Dear R community
> 
> Have just recently got back into R after a long break and have been amazed at 
> how much it has grown, and how active the list is! Thank you so much to all 
> those who contribute to this amazing project.
> 
> My question:
> I am trying to calculate Mahalanobis distances for a matrix called "fgmatrix"
> 
> 
>>dim(fgmatrix)
> 
> [1] 76 15
> 
> 
>>fg.cov <- cov.wt(fgmatrix)
>>mahalanobis(fgmatrix, center = fg.cov$center, cov = fg.cov$cov)
> 
> 
> Then I get an error message "Covariance matrix is apparently singular"
> 
> What does this mean? I can't see anything strange about the covariance matrix, 
> and am not getting anywhere with the help files.
> 
> 
>>dim(fg.cov$cov)
> 
> [1] 15 15
> 
>>length(fg.cov$center)
> 
> [1] 15
> 
> 
> Thanks

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From chrish at stats.ucl.ac.uk  Fri Jun 24 18:02:53 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Fri, 24 Jun 2005 17:02:53 +0100 (BST)
Subject: [R] Mahalanobis distances
In-Reply-To: <42BC2D42.2050707@pdf.com>
References: <200506241622.13468.karen@gecko.biol.wits.ac.za>
	<42BC2D42.2050707@pdf.com>
Message-ID: <Pine.LNX.4.58.0506241702080.10414@egon.stats.ucl.ac.uk>

On Fri, 24 Jun 2005, Spencer Graves wrote:

(...)
> 	  The key is computing your own generalized inverse and using that with
> "inverted=TRUE".
(...)

One method to do this is function solvecov in package fpc.

Christian

>
> 	  spencer graves
>
> Karen Kotschy wrote:
> > Dear R community
> >
> > Have just recently got back into R after a long break and have been amazed at
> > how much it has grown, and how active the list is! Thank you so much to all
> > those who contribute to this amazing project.
> >
> > My question:
> > I am trying to calculate Mahalanobis distances for a matrix called "fgmatrix"
> >
> >
> >>dim(fgmatrix)
> >
> > [1] 76 15
> >
> >
> >>fg.cov <- cov.wt(fgmatrix)
> >>mahalanobis(fgmatrix, center = fg.cov$center, cov = fg.cov$cov)
> >
> >
> > Then I get an error message "Covariance matrix is apparently singular"
> >
> > What does this mean? I can't see anything strange about the covariance matrix,
> > and am not getting anywhere with the help files.
> >
> >
> >>dim(fg.cov$cov)
> >
> > [1] 15 15
> >
> >>length(fg.cov$center)
> >
> > [1] 15
> >
> >
> > Thanks
>
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
>
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

*** NEW ADDRESS! ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From ripley at stats.ox.ac.uk  Fri Jun 24 18:21:53 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 17:21:53 +0100 (BST)
Subject: [R] R demos
In-Reply-To: <200506241546.j5OFkQYE015556@hypatia.math.ethz.ch>
References: <200506241546.j5OFkQYE015556@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0506241716190.25063@gannet.stats>

On Fri, 24 Jun 2005, Jim Brennan wrote:

> Thanks!
> I realized the datasets were not available, but not that they were in a
> separate package. I have never had to load this package in previous versions
> and assumed they were part of one of the default main packages.

They are!  datasets *is* `one of the default main packages'.

> An easy fix was to adjust my default packages in the Rprofile file to
> include "datasets"
> This is for those who don't know a file in the etc directory.

Not really: you should be using Rprofile.site: see ?Startup.

> options(defaultPackages=c("utils"  ,"datasets",  "graphics" ,"stats",
> "methods", "MASS", "gtools"))
>
> Not sure how this happened as I never had to do this before.

The default is actually

c("datasets", "utils", "grDevices", "graphics", "stats", "methods")

so you have changed it. What happens if you start R --vanilla?  That might 
help track down what you did.

>
> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: June 24, 2005 11:25 AM
> To: Jim Brennan
> Cc: r-help
> Subject: Re: [R] R demos
>
> On 6/24/05, Jim Brennan <jfbrennan at rogers.com> wrote:
>> I have noticed that some data sets seem not to be available to me when I
> am
>> running some of the examples or demos since I have changed versions. For
>> example:
>> demo(persp)
>>
>>
>>         demo(persp)
>>         ---- ~~~~~
>>
>> Type  <Return>   to start :
>>
>> R>if (dev.cur() <= 1) get(getOption("device"))()
>>
>> R>is.dev.interactive <- .Device %in% c("X11", "GTK",
>>     "gnome", "quartz", "windows", "JavaGD")
>>
>> R>op <- par(ask = is.dev.interactive)
>>
>> R>x <- seq(-10, 10, length = 50)
>>
>> R>y <- x
>>
>> R>rotsinc <- function(x, y) {
>>     sinc <- function(x) {
>>         y <- sin(x)/x
>>         y[is.na(y)] <- 1
>>         y
>>     }
>>     10 * sinc(sqrt(x^2 + y^2))
>> }
>>
>> R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))
>>
>> R>z <- outer(x, y, rotsinc)
>>
>> R>par(bg = "white")
>>
>> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>     col = "lightblue")
>> Waiting to confirm page change...
>>
>> R>title(sub = ".")
>>
>> R>title(main = sinc.exp)
>>
>> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>     col = "lightblue", ltheta = 120, shade = 0.75, ticktype = "detailed",
>>     xlab = "X", ylab = "Y", zlab = "Z")
>> Waiting to confirm page change...
>>
>> R>title(sub = ".")
>>
>> R>title(main = sinc.exp)
>>
>> R>z <- 2 * volcano
>> Error in eval.with.vis(expr, envir, enclos) :
>>         Object "volcano" not found
>>
>> So most of this demo works but volcano is not found.
>> Any ideas what may be going wrong.
>> Also for example
>> R>?reshape
>> R>summary(Indometh)
>> Error in summary(Indometh) : Object "Indometh" not found
>
> For some reason you do not have the datasets package attached.
>
>> find(Indometh)
> [1] "package:datasets"
>> find(volcano)
> [1] "package:datasets"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gilbert.wu at sabrefund.com  Fri Jun 24 18:36:07 2005
From: gilbert.wu at sabrefund.com (Gilbert Wu)
Date: Fri, 24 Jun 2005 17:36:07 +0100
Subject: [R] Make matrix from SQL query result
Message-ID: <C7FF4EF92D5A794EA5820C75CFB938F963032E@MAILSERVER.sabrefund.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/bd964640/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jun 24 18:47:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 17:47:08 +0100 (BST)
Subject: [R] Memory limits using read.table on Windows XP Pro
In-Reply-To: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
References: <F160BE32A2E5E04497668BBDFE83FEDF08DA8324@EXCHVS1.medctr.ad.wfubmc.edu>
Message-ID: <Pine.LNX.4.61.0506241739410.32410@gannet.stats>

On Fri, 24 Jun 2005, Latchezar Dimitrov wrote:

> Hello,
>
> When I try:
>
> geno
> <-read.table("2500.geno.tab",header=TRUE,sep="\t",na.strings=".",quote="
> ",comment.char="",colClasses=c("factor"),nrows=2501)
>
> I get, after hour(s) of work:
>
> Error: cannot allocate vector of size 9 Kb
>
> I have:
>
> Rgui.exe --max-mem-size=3Gb
>
> and
>
> multi(0)disk(0)rdisk(0)partition(1)\WINDOWS="Microsoft Windows XP
> Professional" /fastdetect /NoExecute=OptIn /PAE /3GB
>
> in boot.ini
>
> 2500.geno.tab is a tab-delimited text table with 2500 x 125000 =
> 312,500,000 3-level (two alphabet characters) factors (x 4 bites =
> 1,250,000,000 (1.25GB). Even if we double it (as per read.table help)
> it's still 2.5GB < 3Gb. And actually Windows Task Manager shows peak mem
> use for Rgui 2,056,992K (~2.057GB) and total memory used 2.62GB. And the
> total physical memory is 4GB (of which windows recognizes above 3GB)
>
> Any help or suggestions?

Do check the rw-FAQ.  If you modified R to address more than 2GB, you 
omitted to tell us a vital fact, so I guess you did not.

I think you need to check the actual meaning of G and K, although they are 
much misused. 1,250,000,000 is 1.16GB in the units you are using for 3GB.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfbrennan at rogers.com  Fri Jun 24 18:47:46 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 24 Jun 2005 12:47:46 -0400
Subject: [R] R demos
In-Reply-To: <Pine.LNX.4.61.0506241716190.25063@gannet.stats>
Message-ID: <200506241647.j5OGlkiI030760@hypatia.math.ethz.ch>

OK, actually this is a textbook example of why it should be done as you said
as I just add on the packages I want in Rprofile.site and that way don't
screw up the default by cutting and pasting into Rprofile.

So I have restored Rprofile to default and added Rprofile.site as follows to
etc directory.

local({
       old <- getOption("defaultPackages")
       options(defaultPackages = c(old, "MASS","gtools","gregmisc"))
     })

I think this is what you meant!

As for running R --vanilla  I don't know how to do that.
Is it possible with a windows version.?
Anyway it is working and thanks for the help.
 
-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: June 24, 2005 12:22 PM
To: Jim Brennan
Cc: 'Douglas Bates'; 'r-help'
Subject: Re: [R] R demos

On Fri, 24 Jun 2005, Jim Brennan wrote:

> Thanks!
> I realized the datasets were not available, but not that they were in a
> separate package. I have never had to load this package in previous
versions
> and assumed they were part of one of the default main packages.

They are!  datasets *is* `one of the default main packages'.

> An easy fix was to adjust my default packages in the Rprofile file to
> include "datasets"
> This is for those who don't know a file in the etc directory.

Not really: you should be using Rprofile.site: see ?Startup.

> options(defaultPackages=c("utils"  ,"datasets",  "graphics" ,"stats",
> "methods", "MASS", "gtools"))
>
> Not sure how this happened as I never had to do this before.

The default is actually

c("datasets", "utils", "grDevices", "graphics", "stats", "methods")

so you have changed it. What happens if you start R --vanilla?  That might 
help track down what you did.

>
> -----Original Message-----
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: June 24, 2005 11:25 AM
> To: Jim Brennan
> Cc: r-help
> Subject: Re: [R] R demos
>
> On 6/24/05, Jim Brennan <jfbrennan at rogers.com> wrote:
>> I have noticed that some data sets seem not to be available to me when I
> am
>> running some of the examples or demos since I have changed versions. For
>> example:
>> demo(persp)
>>
>>
>>         demo(persp)
>>         ---- ~~~~~
>>
>> Type  <Return>   to start :
>>
>> R>if (dev.cur() <= 1) get(getOption("device"))()
>>
>> R>is.dev.interactive <- .Device %in% c("X11", "GTK",
>>     "gnome", "quartz", "windows", "JavaGD")
>>
>> R>op <- par(ask = is.dev.interactive)
>>
>> R>x <- seq(-10, 10, length = 50)
>>
>> R>y <- x
>>
>> R>rotsinc <- function(x, y) {
>>     sinc <- function(x) {
>>         y <- sin(x)/x
>>         y[is.na(y)] <- 1
>>         y
>>     }
>>     10 * sinc(sqrt(x^2 + y^2))
>> }
>>
>> R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))
>>
>> R>z <- outer(x, y, rotsinc)
>>
>> R>par(bg = "white")
>>
>> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>     col = "lightblue")
>> Waiting to confirm page change...
>>
>> R>title(sub = ".")
>>
>> R>title(main = sinc.exp)
>>
>> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>     col = "lightblue", ltheta = 120, shade = 0.75, ticktype = "detailed",
>>     xlab = "X", ylab = "Y", zlab = "Z")
>> Waiting to confirm page change...
>>
>> R>title(sub = ".")
>>
>> R>title(main = sinc.exp)
>>
>> R>z <- 2 * volcano
>> Error in eval.with.vis(expr, envir, enclos) :
>>         Object "volcano" not found
>>
>> So most of this demo works but volcano is not found.
>> Any ideas what may be going wrong.
>> Also for example
>> R>?reshape
>> R>summary(Indometh)
>> Error in summary(Indometh) : Object "Indometh" not found
>
> For some reason you do not have the datasets package attached.
>
>> find(Indometh)
> [1] "package:datasets"
>> find(volcano)
> [1] "package:datasets"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uofiowa at gmail.com  Fri Jun 24 18:54:29 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 24 Jun 2005 12:54:29 -0400
Subject: [R] seq in R
Message-ID: <3f87cc6d050624095418c16ea3@mail.gmail.com>

I want to generate a sequence from 1 to x by 1
seq(1,x,by=1)
I want the above to return an empty list if x is zero
In other languages I can do 1:x:1 to force the increment by to be a
positive 1. This syntax does not work in R. In R 1:x gives me
1 0
when x is zero, this is not what I want.
The seq statement above throws an error when x is 0. 
How can I generate a sequence where if the "to" value is less than the
"from" value an empty list is generated with no errors?



From ggrothendieck at gmail.com  Fri Jun 24 18:58:56 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 12:58:56 -0400
Subject: [R] seq in R
In-Reply-To: <3f87cc6d050624095418c16ea3@mail.gmail.com>
References: <3f87cc6d050624095418c16ea3@mail.gmail.com>
Message-ID: <971536df050624095869b1de61@mail.gmail.com>

On 6/24/05, Omar Lakkis <uofiowa at gmail.com> wrote:
> I want to generate a sequence from 1 to x by 1
> seq(1,x,by=1)
> I want the above to return an empty list if x is zero

seq(length = x)



From p.dalgaard at biostat.ku.dk  Fri Jun 24 19:01:30 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jun 2005 19:01:30 +0200
Subject: [R] seq in R
In-Reply-To: <3f87cc6d050624095418c16ea3@mail.gmail.com>
References: <3f87cc6d050624095418c16ea3@mail.gmail.com>
Message-ID: <x27jgjsn85.fsf@turmalin.kubism.ku.dk>

Omar Lakkis <uofiowa at gmail.com> writes:

> I want to generate a sequence from 1 to x by 1
> seq(1,x,by=1)
> I want the above to return an empty list if x is zero
> In other languages I can do 1:x:1 to force the increment by to be a
> positive 1. This syntax does not work in R. In R 1:x gives me
> 1 0
> when x is zero, this is not what I want.
> The seq statement above throws an error when x is 0. 
> How can I generate a sequence where if the "to" value is less than the
> "from" value an empty list is generated with no errors?

Provide a length argument. In your case, seq(length=x) should suffice. 

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Jun 24 19:04:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 18:04:45 +0100 (BST)
Subject: [R] R demos
In-Reply-To: <200506241647.j5OGlmjJ025161@markov.stats.ox.ac.uk>
References: <200506241647.j5OGlmjJ025161@markov.stats.ox.ac.uk>
Message-ID: <Pine.LNX.4.61.0506241804340.26458@gannet.stats>

On Fri, 24 Jun 2005, Jim Brennan wrote:

> OK, actually this is a textbook example of why it should be done as you said
> as I just add on the packages I want in Rprofile.site and that way don't
> screw up the default by cutting and pasting into Rprofile.
>
> So I have restored Rprofile to default and added Rprofile.site as follows to
> etc directory.
>
> local({
>       old <- getOption("defaultPackages")
>       options(defaultPackages = c(old, "MASS","gtools","gregmisc"))
>     })
>
> I think this is what you meant!
>
> As for running R --vanilla  I don't know how to do that.
> Is it possible with a windows version.?

see the rw-FAQ.

> Anyway it is working and thanks for the help.
>
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: June 24, 2005 12:22 PM
> To: Jim Brennan
> Cc: 'Douglas Bates'; 'r-help'
> Subject: Re: [R] R demos
>
> On Fri, 24 Jun 2005, Jim Brennan wrote:
>
>> Thanks!
>> I realized the datasets were not available, but not that they were in a
>> separate package. I have never had to load this package in previous
> versions
>> and assumed they were part of one of the default main packages.
>
> They are!  datasets *is* `one of the default main packages'.
>
>> An easy fix was to adjust my default packages in the Rprofile file to
>> include "datasets"
>> This is for those who don't know a file in the etc directory.
>
> Not really: you should be using Rprofile.site: see ?Startup.
>
>> options(defaultPackages=c("utils"  ,"datasets",  "graphics" ,"stats",
>> "methods", "MASS", "gtools"))
>>
>> Not sure how this happened as I never had to do this before.
>
> The default is actually
>
> c("datasets", "utils", "grDevices", "graphics", "stats", "methods")
>
> so you have changed it. What happens if you start R --vanilla?  That might
> help track down what you did.
>
>>
>> -----Original Message-----
>> From: Douglas Bates [mailto:dmbates at gmail.com]
>> Sent: June 24, 2005 11:25 AM
>> To: Jim Brennan
>> Cc: r-help
>> Subject: Re: [R] R demos
>>
>> On 6/24/05, Jim Brennan <jfbrennan at rogers.com> wrote:
>>> I have noticed that some data sets seem not to be available to me when I
>> am
>>> running some of the examples or demos since I have changed versions. For
>>> example:
>>> demo(persp)
>>>
>>>
>>>         demo(persp)
>>>         ---- ~~~~~
>>>
>>> Type  <Return>   to start :
>>>
>>> R>if (dev.cur() <= 1) get(getOption("device"))()
>>>
>>> R>is.dev.interactive <- .Device %in% c("X11", "GTK",
>>>     "gnome", "quartz", "windows", "JavaGD")
>>>
>>> R>op <- par(ask = is.dev.interactive)
>>>
>>> R>x <- seq(-10, 10, length = 50)
>>>
>>> R>y <- x
>>>
>>> R>rotsinc <- function(x, y) {
>>>     sinc <- function(x) {
>>>         y <- sin(x)/x
>>>         y[is.na(y)] <- 1
>>>         y
>>>     }
>>>     10 * sinc(sqrt(x^2 + y^2))
>>> }
>>>
>>> R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))
>>>
>>> R>z <- outer(x, y, rotsinc)
>>>
>>> R>par(bg = "white")
>>>
>>> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>>     col = "lightblue")
>>> Waiting to confirm page change...
>>>
>>> R>title(sub = ".")
>>>
>>> R>title(main = sinc.exp)
>>>
>>> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
>>>     col = "lightblue", ltheta = 120, shade = 0.75, ticktype = "detailed",
>>>     xlab = "X", ylab = "Y", zlab = "Z")
>>> Waiting to confirm page change...
>>>
>>> R>title(sub = ".")
>>>
>>> R>title(main = sinc.exp)
>>>
>>> R>z <- 2 * volcano
>>> Error in eval.with.vis(expr, envir, enclos) :
>>>         Object "volcano" not found
>>>
>>> So most of this demo works but volcano is not found.
>>> Any ideas what may be going wrong.
>>> Also for example
>>> R>?reshape
>>> R>summary(Indometh)
>>> Error in summary(Indometh) : Object "Indometh" not found
>>
>> For some reason you do not have the datasets package attached.
>>
>>> find(Indometh)
>> [1] "package:datasets"
>>> find(volcano)
>> [1] "package:datasets"
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Fri Jun 24 19:09:57 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 24 Jun 2005 12:09:57 -0500
Subject: [R] Representative curves
Message-ID: <42BC3E65.4050009@vanderbilt.edu>

Can someone point me to R code or recent publications dealing with 
selection of representative time-response profiles in longitudinal data 
from datasets containing a large number of subject's profiles?

Thanks

Frank
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Fri Jun 24 19:09:00 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 24 Jun 2005 13:09:00 -0400
Subject: [R] R demos
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA03@usctmx1106.merck.com>

> From: Jim Brennan
> 
> OK, actually this is a textbook example of why it should be 
> done as you said
> as I just add on the packages I want in Rprofile.site and 
> that way don't
> screw up the default by cutting and pasting into Rprofile.
> 
> So I have restored Rprofile to default and added 
> Rprofile.site as follows to
> etc directory.
> 
> local({
>        old <- getOption("defaultPackages")
>        options(defaultPackages = c(old, "MASS","gtools","gregmisc"))
>      })
> 
> I think this is what you meant!
> 
> As for running R --vanilla  I don't know how to do that.
> Is it possible with a windows version.?

Absolutely.  If you use Rgui, right-click on the Rgui icon and click on
"Properties".  In the "target" field, add "--vanilla" (or any command line
argument to R).  If the command is quoted, make sure you add the option
outside the quotes.

Andy


> Anyway it is working and thanks for the help.
>  
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: June 24, 2005 12:22 PM
> To: Jim Brennan
> Cc: 'Douglas Bates'; 'r-help'
> Subject: Re: [R] R demos
> 
> On Fri, 24 Jun 2005, Jim Brennan wrote:
> 
> > Thanks!
> > I realized the datasets were not available, but not that 
> they were in a
> > separate package. I have never had to load this package in previous
> versions
> > and assumed they were part of one of the default main packages.
> 
> They are!  datasets *is* `one of the default main packages'.
> 
> > An easy fix was to adjust my default packages in the 
> Rprofile file to
> > include "datasets"
> > This is for those who don't know a file in the etc directory.
> 
> Not really: you should be using Rprofile.site: see ?Startup.
> 
> > options(defaultPackages=c("utils"  ,"datasets",  "graphics" 
> ,"stats",
> > "methods", "MASS", "gtools"))
> >
> > Not sure how this happened as I never had to do this before.
> 
> The default is actually
> 
> c("datasets", "utils", "grDevices", "graphics", "stats", "methods")
> 
> so you have changed it. What happens if you start R 
> --vanilla?  That might 
> help track down what you did.
> 
> >
> > -----Original Message-----
> > From: Douglas Bates [mailto:dmbates at gmail.com]
> > Sent: June 24, 2005 11:25 AM
> > To: Jim Brennan
> > Cc: r-help
> > Subject: Re: [R] R demos
> >
> > On 6/24/05, Jim Brennan <jfbrennan at rogers.com> wrote:
> >> I have noticed that some data sets seem not to be 
> available to me when I
> > am
> >> running some of the examples or demos since I have changed 
> versions. For
> >> example:
> >> demo(persp)
> >>
> >>
> >>         demo(persp)
> >>         ---- ~~~~~
> >>
> >> Type  <Return>   to start :
> >>
> >> R>if (dev.cur() <= 1) get(getOption("device"))()
> >>
> >> R>is.dev.interactive <- .Device %in% c("X11", "GTK",
> >>     "gnome", "quartz", "windows", "JavaGD")
> >>
> >> R>op <- par(ask = is.dev.interactive)
> >>
> >> R>x <- seq(-10, 10, length = 50)
> >>
> >> R>y <- x
> >>
> >> R>rotsinc <- function(x, y) {
> >>     sinc <- function(x) {
> >>         y <- sin(x)/x
> >>         y[is.na(y)] <- 1
> >>         y
> >>     }
> >>     10 * sinc(sqrt(x^2 + y^2))
> >> }
> >>
> >> R>sinc.exp <- expression(z == Sinc(sqrt(x^2 + y^2)))
> >>
> >> R>z <- outer(x, y, rotsinc)
> >>
> >> R>par(bg = "white")
> >>
> >> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
> >>     col = "lightblue")
> >> Waiting to confirm page change...
> >>
> >> R>title(sub = ".")
> >>
> >> R>title(main = sinc.exp)
> >>
> >> R>persp(x, y, z, theta = 30, phi = 30, expand = 0.5,
> >>     col = "lightblue", ltheta = 120, shade = 0.75, 
> ticktype = "detailed",
> >>     xlab = "X", ylab = "Y", zlab = "Z")
> >> Waiting to confirm page change...
> >>
> >> R>title(sub = ".")
> >>
> >> R>title(main = sinc.exp)
> >>
> >> R>z <- 2 * volcano
> >> Error in eval.with.vis(expr, envir, enclos) :
> >>         Object "volcano" not found
> >>
> >> So most of this demo works but volcano is not found.
> >> Any ideas what may be going wrong.
> >> Also for example
> >> R>?reshape
> >> R>summary(Indometh)
> >> Error in summary(Indometh) : Object "Indometh" not found
> >
> > For some reason you do not have the datasets package attached.
> >
> >> find(Indometh)
> > [1] "package:datasets"
> >> find(volcano)
> > [1] "package:datasets"
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From murdoch at stats.uwo.ca  Fri Jun 24 19:15:54 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 24 Jun 2005 13:15:54 -0400
Subject: [R] R demos
In-Reply-To: <200506241647.j5OGlkiI030760@hypatia.math.ethz.ch>
References: <200506241647.j5OGlkiI030760@hypatia.math.ethz.ch>
Message-ID: <42BC3FCA.7070106@stats.uwo.ca>

On 6/24/2005 12:47 PM, Jim Brennan wrote:

> As for running R --vanilla  I don't know how to do that.
> Is it possible with a windows version.?
> Anyway it is working and thanks for the help.

There are a few ways.  The simplest is probably to edit the shortcut you 
usually use.  Just right click on it, click on Properties, and add 
"--vanilla" to the Target string, which is usually something like

F:\R\rw2011\bin\Rgui.exe

so it becomes

F:\R\rw2011\bin\Rgui.exe --vanilla

(Actually, you should probably do this to a copy of the shortcut, rather 
than the original.)

You can also type the whole string into Start|Run... (but your path is 
probably different from mine), or do it from a command shell, etc.

Duncan Murdoch



From pburns at pburns.seanet.com  Fri Jun 24 19:15:49 2005
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 24 Jun 2005 18:15:49 +0100
Subject: [R] Make matrix from SQL query result
In-Reply-To: <C7FF4EF92D5A794EA5820C75CFB938F963032E@MAILSERVER.sabrefund.com>
References: <C7FF4EF92D5A794EA5820C75CFB938F963032E@MAILSERVER.sabrefund.com>
Message-ID: <42BC3FC5.6020902@pburns.seanet.com>

Gilbert,

This is untested, but something like:

function(x) {
    uniq.col <- unique(x[,1])
    uniq.row <- unique(x[,2])
    ans <- array(NA, c(length(uniq.row), length(uniq.col)), 
list(uniq.row, uniq.col))
    ans[cbind(match(x[,2], uniq.row), match(x[,1], uniq.col))] <- x[,3]
    ans
}

Pat


Gilbert Wu wrote:

>Hi,
> 
>I am trying to form a matrix from a SQL query result.
> 
>The SQL query result looks like this:
> 
>equity_id         date              returns
>1                 20050619          0.12
>2                 20050619          0.03
>1                 20050620          -0.01
>2                 20050620          0.01
> 
>The target matrix looks like this:
> 
> 
>date                     1               2
>20050619           0.12            0.03
>20050620           -0.01           0.01
> 
>Any suggestion would be much appreciated.
> 
>Many Thanks.
> 
>Gilbert
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From jfbrennan at rogers.com  Fri Jun 24 19:40:17 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 24 Jun 2005 13:40:17 -0400
Subject: [R] Make matrix from SQL query result
In-Reply-To: <C7FF4EF92D5A794EA5820C75CFB938F963032E@MAILSERVER.sabrefund.com>
Message-ID: <200506241740.j5OHeHaR015088@hypatia.math.ethz.ch>

You can use reshape which seems popular this week!

> rr
  equity_id     date returns
1         1 20050619    0.12
2         2 20050619    0.03
3         1 20050620   -0.01
4         2 20050620    0.01


> reshape(rr,timevar="equity_id",direction="wide",idvar="date")
      date returns.1 returns.2
1 20050619      0.12      0.03
3 20050620     -0.01      0.01


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gilbert Wu
Sent: June 24, 2005 12:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Make matrix from SQL query result

Hi,
 
I am trying to form a matrix from a SQL query result.
 
The SQL query result looks like this:
 
equity_id         date              returns
1                 20050619          0.12
2                 20050619          0.03
1                 20050620          -0.01
2                 20050620          0.01
 
The target matrix looks like this:
 
 
date                     1               2
20050619           0.12            0.03
20050620           -0.01           0.01
 
Any suggestion would be much appreciated.
 
Many Thanks.
 
Gilbert

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From br44114 at gmail.com  Fri Jun 24 19:49:19 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Fri, 24 Jun 2005 13:49:19 -0400
Subject: [R] Make matrix from SQL query result
Message-ID: <8d5a363505062410497e6e6b2c@mail.gmail.com>

It may be better to do this in SQL. The code below works for an
arbitrary number of IDs and handles missing values.

test <- data.frame(id=rep(c(1,2),10),date=sort(c(1:10,1:10)),ret=0.01*-9:10)
idret <- list()
ids <- sort(unique(test$id))
for (i in ids) {
	idret[[as.character(i)]] <- test[test$id == i,]
	idret[[as.character(i)]] <- idret[[as.character(i)]][,-1]
	colnames(idret[[as.character(i)]])[2] <- paste("ret",i,sep="")
	}
allret <- idret[[as.character(ids[1])]]
for (i in ids[2:length(ids)]) {
	allret <- merge(allret,idret[[as.character(i)]],all=TRUE)
	}



> -----Original Message-----
> From: Gilbert Wu [mailto:gilbert.wu at sabrefund.com]
> Sent: Friday, June 24, 2005 12:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Make matrix from SQL query result
> 
> 
> Hi,
>  
> I am trying to form a matrix from a SQL query result.
>  
> The SQL query result looks like this:
>  
> equity_id         date              returns
> 1                 20050619          0.12
> 2                 20050619          0.03
> 1                 20050620          -0.01
> 2                 20050620          0.01
>  
> The target matrix looks like this:
>  
>  
> date                     1               2
> 20050619           0.12            0.03
> 20050620           -0.01           0.01
>  
> Any suggestion would be much appreciated.
>  
> Many Thanks.
>  
> Gilbert
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ldimitro at wfubmc.edu  Fri Jun 24 20:09:10 2005
From: ldimitro at wfubmc.edu (Latchezar Dimitrov)
Date: Fri, 24 Jun 2005 14:09:10 -0400
Subject: [R] Memory limits using read.table on Windows XP Pro
Message-ID: <F160BE32A2E5E04497668BBDFE83FEDF08DA8326@EXCHVS1.medctr.ad.wfubmc.edu>

Thank you very much for your attention. I checked rw-FAQ, did not
mention it though. Since it's common req. I thought it is a common
practice too and decided not to abuse bandwidth. Apparently wrong.
However from what I presented you can easily (I guess) infer it as well.
Your guess is about what I used is absolutely correct as I expected BTW.
Or yeah, the water is wet although I did not mention it either :-)

R FAQ

Frequently Asked Questions on R
Version 2.1.2005-06-22
ISBN 3-900051-08-9:

"7.28 Why is read.table() so inefficient?

By default, read.table() needs to read in everything as character data,
and then try to figure out which variables to convert to numerics or
factors. For a large data set, this takes condiderable amounts of time
and memory. Performance can substantially be improved by using the
colClasses argument to specify the classes to be assumed for the columns
of the table."

(The vital word "condiderable" above is not explained anywhere, so I
guess it means considerable. I think you (all) need to check the
spelling of the words you (all) use. Although spelling-checkers are much
misused they are sometimes useful.)

Is my use of read.table() in accordance with the above? Can it be
improved with respect of my problem? 

R for Windows FAQ
Version for rw2011
B. D. Ripley and D. J. Murdoch:
(it does not say Prof. but I guess it is "Prof. B. D. Ripley", isn't
it?)

"2.11 There seems to be a limit on the memory it uses!

Indeed there is. It is set by the command-line flag --max-mem-size (see
How do I install R for Windows?) and defaults to the smaller of the
amount of physical RAM in the machine and 1Gb. It can be set to any
amount over 16M. (R will not run in less.) Be aware though that Windows
has (in most versions) a maximum amount of user virtual memory of 2Gb,
and parts of this can be reserved by processes but not used."

So what is wrong if at all in my configuration, settings, parameters,
flags, etc. (you name them) with respect of the above?

Although I did not mention it I know very well the diff. b/n GiB, GB,
and Gb (as used in rw-FAQ, wrongly I suppose) and your guess is
incorrect here. Anyway my estimates as you can see are conservative and
so your note does not contribute essential info.   

Despite your blunder about my knowledge I suspect that you secretly knew
about the conservativeness above so I wonder why after your correct
interpretation of my e-mail I did not get plain answer in straight
English.

Best regards,
Latchezar Dimitrov

PS. Please do not reply if you do not have any help or suggestions to
solve the problem (not about my education, experience, not mentioning
all the trivia, etc). Thanks

PPS. I also wonder if you have ever heard about "the magic word" or
there is no such thing as magic for Prof.'s

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Friday, June 24, 2005 12:47 PM
> To: Latchezar Dimitrov
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Memory limits using read.table on Windows XP Pro
> 
> On Fri, 24 Jun 2005, Latchezar Dimitrov wrote:
> 
> > Hello,
> >
> > When I try:
> >
> > geno
> > 
> <-read.table("2500.geno.tab",header=TRUE,sep="\t",na.strings="
> .",quote="
> > ",comment.char="",colClasses=c("factor"),nrows=2501)
> >
> > I get, after hour(s) of work:
> >
> > Error: cannot allocate vector of size 9 Kb
> >
> > I have:
> >
> > Rgui.exe --max-mem-size=3Gb
> >
> > and
> >
> > multi(0)disk(0)rdisk(0)partition(1)\WINDOWS="Microsoft Windows XP 
> > Professional" /fastdetect /NoExecute=OptIn /PAE /3GB
> >
> > in boot.ini
> >
> > 2500.geno.tab is a tab-delimited text table with 2500 x 125000 = 
> > 312,500,000 3-level (two alphabet characters) factors (x 4 bites = 
> > 1,250,000,000 (1.25GB). Even if we double it (as per 
> read.table help) 
> > it's still 2.5GB < 3Gb. And actually Windows Task Manager 
> shows peak 
> > mem use for Rgui 2,056,992K (~2.057GB) and total memory 
> used 2.62GB. 
> > And the total physical memory is 4GB (of which windows recognizes 
> > above 3GB)
> >
> > Any help or suggestions?
> 
> Do check the rw-FAQ.  If you modified R to address more than 
> 2GB, you omitted to tell us a vital fact, so I guess you did not.
> 
> I think you need to check the actual meaning of G and K, 
> although they are much misused. 1,250,000,000 is 1.16GB in 
> the units you are using for 3GB.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From rvaradha at jhsph.edu  Fri Jun 24 20:09:30 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Fri, 24 Jun 2005 14:09:30 -0400
Subject: [R] Representative curves
In-Reply-To: <42BC3E65.4050009@vanderbilt.edu>
Message-ID: <OWA-1fqfuY7qfksTWuP00003ac9@owa-1.sph.ad.jhsph.edu>

Following articles may be relevant:

1.  M. C. Jones; John A. Rice, Displaying the Important Features of Large
Collections of Similar Curves, The American Statistician, Vol. 46, No. 2
(May, 1992), pp. 140-145.
2. M.R. Segal, Representative Curves for Longitudinal Data via Regression
Trees, Journal of Computational and Graphical Statistics, Vol. 3, No. 2
(Jun., 1994), pp. 214-233.

I don't know of any more recent ones.  

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Frank E Harrell Jr
> Sent: Friday, June 24, 2005 1:10 PM
> To: RHELP
> Subject: [R] Representative curves
> 
> Can someone point me to R code or recent publications dealing with
> selection of representative time-response profiles in longitudinal data
> from datasets containing a large number of subject's profiles?
> 
> Thanks
> 
> Frank
> --
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ssherman at cemml.colostate.edu  Fri Jun 24 20:10:17 2005
From: ssherman at cemml.colostate.edu (Steve Sherman)
Date: Fri, 24 Jun 2005 12:10:17 -0600
Subject: [R] text file output
Message-ID: <42BC4C89.3070405@cemml.colostate.edu>

Hello,

I would like to create an ascii text file that looks exactly like the 
Console output of a split() action. I have tried save, dput, dump, 
write.foreign and write to no avail. The resulting text files are a 
jumble of data that I would have to edit heavily in order to use it for 
my proposed purpose. How can I, short of copying and pasting the 
contents of the console (which will not work since the file is very 
large), output the information to a text file in a format like that on 
my screen?

I am very new to R so please be kind.

Thank you
Steve



From sdavis2 at mail.nih.gov  Fri Jun 24 20:24:30 2005
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 24 Jun 2005 14:24:30 -0400
Subject: [R] text file output
References: <42BC4C89.3070405@cemml.colostate.edu>
Message-ID: <00c601c578e9$f68cc1a0$5179f345@WATSON>

See ?sink....

Sean

----- Original Message ----- 
From: "Steve Sherman" <ssherman at cemml.colostate.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 24, 2005 2:10 PM
Subject: [R] text file output


> Hello,
>
> I would like to create an ascii text file that looks exactly like the
> Console output of a split() action. I have tried save, dput, dump,
> write.foreign and write to no avail. The resulting text files are a
> jumble of data that I would have to edit heavily in order to use it for
> my proposed purpose. How can I, short of copying and pasting the
> contents of the console (which will not work since the file is very
> large), output the information to a text file in a format like that on
> my screen?
>
> I am very new to R so please be kind.
>
> Thank you
> Steve
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Fri Jun 24 20:24:29 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 24 Jun 2005 11:24:29 -0700
Subject: [R] text file output
In-Reply-To: <42BC4C89.3070405@cemml.colostate.edu>
References: <42BC4C89.3070405@cemml.colostate.edu>
Message-ID: <42BC4FDD.8090200@pdf.com>

	  Is "sink" what you want?  spencer graves

Steve Sherman wrote:

> Hello,
> 
> I would like to create an ascii text file that looks exactly like the 
> Console output of a split() action. I have tried save, dput, dump, 
> write.foreign and write to no avail. The resulting text files are a 
> jumble of data that I would have to edit heavily in order to use it for 
> my proposed purpose. How can I, short of copying and pasting the 
> contents of the console (which will not work since the file is very 
> large), output the information to a text file in a format like that on 
> my screen?
> 
> I am very new to R so please be kind.
> 
> Thank you
> Steve
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ligges at statistik.uni-dortmund.de  Fri Jun 24 20:28:23 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 24 Jun 2005 20:28:23 +0200
Subject: [R] text file output
In-Reply-To: <42BC4C89.3070405@cemml.colostate.edu>
References: <42BC4C89.3070405@cemml.colostate.edu>
Message-ID: <42BC50C7.7040504@statistik.uni-dortmund.de>

Steve Sherman wrote:
> Hello,
> 
> I would like to create an ascii text file that looks exactly like the 
> Console output of a split() action. I have tried save, dput, dump, 
> write.foreign and write to no avail. The resulting text files are a 
> jumble of data that I would have to edit heavily in order to use it for 
> my proposed purpose. How can I, short of copying and pasting the 
> contents of the console (which will not work since the file is very 
> large), output the information to a text file in a format like that on 
> my screen?

Two ways:

1. You can sink() the console output.

2. Probably you really want something like
   object <- split(...)
   for(i in seq(along=object))
     write.table(SplitObject[[i]], file = names(SplitObject)[i])


Uwe Ligges


> I am very new to R so please be kind.
> 
> Thank you
> Steve
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chrish at stats.ucl.ac.uk  Fri Jun 24 20:44:34 2005
From: chrish at stats.ucl.ac.uk (Christian Hennig)
Date: Fri, 24 Jun 2005 19:44:34 +0100 (BST)
Subject: [R] help with PCA and classical scaling
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>
References: <BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>
Message-ID: <Pine.LNX.4.58.0506241937410.10414@egon.stats.ucl.ac.uk>

Hi Domenico,

multidimensional scaling methods (functions cmdscale, isoMDS, sammon)
start with a dissimilarity matrix while PCA starts from a data
matrix with variables. Equivalence of PCA and classical MDS refers to
the situation where you use the Euclidean distances between the data
points on which you apply PCA. (Note: I cite this from my memory, so check
it before believing it.)

> My first question is: Do I need an initial embedding of my data before
> applying PCA through the methods princomp or prcomp in the stats package?

I don't think that you need PCA at all (depends on what exactly you want
to do) - an MDS method alone could be enough for you
(cmdscale, if you want to do something analogous to PCA).

Best,
Christian



*** NEW ADDRESS! ***
Christian Hennig
University College London, Department of Statistical Science
Gower St., London WC1E 6BT, phone +44 207 679 1698
chrish at stats.ucl.ac.uk, www.homepages.ucl.ac.uk/~ucakche



From kangs at ornl.gov  Fri Jun 24 20:49:48 2005
From: kangs at ornl.gov (Kang, Sang-Hoon)
Date: Fri, 24 Jun 2005 14:49:48 -0400
Subject: [R] definition of  variogram
Message-ID: <74700A2004B18A459CDA46BF6BDEDEF50181EE5A@ORNLEXCHANGE.ornl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/22baaf47/attachment.pl

From uofiowa at gmail.com  Fri Jun 24 21:45:16 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 24 Jun 2005 15:45:16 -0400
Subject: [R] exception handeling
Message-ID: <3f87cc6d0506241245f6372e@mail.gmail.com>

Does anyone have a good simple or a link to a tutorial in how to use
exception handling in R?
I see the try and tryCatch methods but it does not work quite similar
to what I am used to in Java.



From ssherman at cemml.colostate.edu  Fri Jun 24 21:48:44 2005
From: ssherman at cemml.colostate.edu (Steve Sherman)
Date: Fri, 24 Jun 2005 13:48:44 -0600
Subject: [R] text file output
In-Reply-To: <42BC50C7.7040504@statistik.uni-dortmund.de>
References: <42BC4C89.3070405@cemml.colostate.edu>
	<42BC50C7.7040504@statistik.uni-dortmund.de>
Message-ID: <42BC639C.1020905@cemml.colostate.edu>

Sink is exactly what I needed.
Thanks  to all who suggested it. I was also impressed by the quick 
response to question.

Uwe Ligges wrote:

> Steve Sherman wrote:
>
>> Hello,
>>
>> I would like to create an ascii text file that looks exactly like the 
>> Console output of a split() action. I have tried save, dput, dump, 
>> write.foreign and write to no avail. The resulting text files are a 
>> jumble of data that I would have to edit heavily in order to use it 
>> for my proposed purpose. How can I, short of copying and pasting the 
>> contents of the console (which will not work since the file is very 
>> large), output the information to a text file in a format like that 
>> on my screen?
>
>
> Two ways:
>
> 1. You can sink() the console output.
>
> 2. Probably you really want something like
>   object <- split(...)
>   for(i in seq(along=object))
>     write.table(SplitObject[[i]], file = names(SplitObject)[i])
>
>
> Uwe Ligges
>
>
>> I am very new to R so please be kind.
>>
>> Thank you
>> Steve
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>



From kjetil at acelerate.com  Fri Jun 24 21:27:28 2005
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 24 Jun 2005 15:27:28 -0400
Subject: [R] comparing strength of association instead of strength of
 evidence?
In-Reply-To: <cdf817830506240759cd3ae6a@mail.gmail.com>
References: <cdf817830506240759cd3ae6a@mail.gmail.com>
Message-ID: <42BC5EA0.2030202@acelerate.com>

Weiwei Shi wrote:

>Hi,
>I asked this question before, which was hidden in a bunch of
>questions. I repharse it here and hope I can get some help this time:
>
>I have 2 contingency tables which have the same group variable Y. I
>want to compare the strength of association between X1/Y and X2/Y. I
>am not sure if comparing p-values IS the way  even though the
>probability of seeing such "weird" observation under H0 defines
>p-value and it might relate to the strength of association somehow.
>But I read the following statement from Alan Agresti's "An
>Introduction to Categorical Data Analysis" :
>"Chi-squared tests simply indicate the degree of EVIDENCE for an
>association....It is sensible to decompose chi-squared into
>components, study residuals, and estimate parameters such as odds
>ratios that describe the STRENGTH OF ASSOCIATION".
>
>  
>
Here are some things you can do:

 > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))

 > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
 > library(epitools) # on CRAN
 > ?odds.ratio
Help for 'odds.ratio' is shown in the browser
 > library(help=epitools) # on CRAN
 > tab1
      [,1]    [,2]
[1,] 11266 2151526
[2,]   125   31734
 > odds.ratio(11266, 125, 2151526, 31734)
Error in fisher.test(tab) : FEXACT error 40.
Out of workspace.                 # so this are evidently for tables 
with smaller counts
 > library(vcd) # on CRAN

 > ?oddsratio
Help for 'oddsratio' is shown in the browser
 > oddsratio( tab1)  # really is logodds ratio
[1] 0.2807548
 > plot(oddsratio( tab1) )
 > library(help=vcd) # on CRAN  Read this for many nice functions.
 > fourfoldplot(tab1)
 > mosaicplot(tab1)     # not really usefull for this table

Also has a look at function Crosstable in package gmodels.

To decompose the chisqure you can program yourselves:

decomp.chi <- function(tab) {
        rows <-  rowSums(tab)
        cols <-   colSums(tab)
        N <-   sum(rows)
         E <- rows %o% cols / N
         contrib <- (tab-E)^2/E
         contrib }


 > decomp.chi(tab1)
          [,1]         [,2]
[1,] 0.1451026 0.0007570624
[2,] 9.8504915 0.0513942218
 >

So you can easily see what cell contributes most to the overall chisquared.

Kjetil





>Can I do this "decomposition" in R for the following example including
>2 contingency tables?
>
>  
>
>>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
>>tab1
>>    
>>
>      [,1]    [,2]
>[1,] 11266 2151526
>[2,]   125   31734
>
>  
>
>>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
>>tab2
>>    
>>
>      [,1]    [,2]
>[1,] 43571 2119221
>[2,]    52   31807
>
>
>BTW, is there some good forum on the theory of statistics? r-help is a
>good one but I don't want to bother people by asking some questions
>weakly associated with R here.
>
>Thanks,
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra





-- 
No virus found in this outgoing message.
Checked by AVG Anti-Virus.



From dimitrijoe at yahoo.com.br  Fri Jun 24 22:57:46 2005
From: dimitrijoe at yahoo.com.br (Dimitri Joe)
Date: Fri, 24 Jun 2005 17:57:46 -0300
Subject: [R] Gini with frequencies
Message-ID: <003801c578ff$60fec4b0$1600a8c0@thesahajamach>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/16db07c7/attachment.pl

From iidn01 at yahoo.com  Fri Jun 24 23:19:52 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Fri, 24 Jun 2005 14:19:52 -0700 (PDT)
Subject: [R] "Error in contrasts" in step wise regression
Message-ID: <20050624211952.57161.qmail@web31113.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/e087e5fd/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jun 24 23:44:56 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Jun 2005 22:44:56 +0100 (BST)
Subject: [R] "Error in contrasts" in step wise regression
In-Reply-To: <20050624211952.57161.qmail@web31113.mail.mud.yahoo.com>
References: <20050624211952.57161.qmail@web31113.mail.mud.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506242233400.17824@gannet.stats>

On Fri, 24 Jun 2005, Young Cho wrote:

> Hi,
>
> I have a problem in getting step function work.

This is not coming from step(), but (AFAIK) from model.matrix() called by 
lm(). One way to debug it is to try fitting the models directly.

> I am getting the following error:
>
>> fit1 <- lm(Response~1)
>> fmla <- as.formula(paste(" ~ ",paste(colnames,collapse="+")))
>> sfit <- step(fit1,scope=list(upper= fmla,lower= ~1),k=log(nrow(dat)))
> Start:  AIC= -1646.66
> Response ~ 1
> Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
>        contrasts can be applied only to factors with 2 or more levels
>
> But if i count the unique values in each column by
>
> A <- NULL
> for (ii in 1:length(colnames)){
>        A[ii] <- length(unique( eval(parse(text=paste('dat$',colnames[ii])))))
> }
>
> I do not see any column with only 1 value. Is there some other possible 
> reason why I am getting the error? Thanks a lot!

It says `levels', not values.  So try

 	sapply(dat, nlevels)

The values can include NA, which is not a level (usually).  E.g.

> x <- factor(c(1, NA))
> nlevels(x)
[1] 1
> length(unique(x))
[1] 2

(Incidentally, you are assuming variables are found in dat, and you should 
use

 	lm(Response ~ 1, data=dat)

to ensure that.  And your calculation can be done more legibly as

 	sapply(dat, function(x) length(unique(x)))

.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uofiowa at gmail.com  Fri Jun 24 23:54:56 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 24 Jun 2005 17:54:56 -0400
Subject: [R] R servlet engine ?!!!
Message-ID: <3f87cc6d05062414543c4b2c95@mail.gmail.com>

I use CGIwithR to run a website that executes R scripts under cgi-bin. 
Are there any projects that implement a work frame using R similar to
Java's servlet engine?
Initialization times are hurting. Can initialize once process and
treat all web requests as threads of this process?
Any help with this subject is greatly appreciated.



From macq at llnl.gov  Sat Jun 25 00:11:03 2005
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 24 Jun 2005 15:11:03 -0700
Subject: [R] R servlet engine ?!!!
In-Reply-To: <3f87cc6d05062414543c4b2c95@mail.gmail.com>
References: <3f87cc6d05062414543c4b2c95@mail.gmail.com>
Message-ID: <p06210205bee2354e7c40@[128.115.153.6]>

 From last year:

At 3:42 AM -0300 2/22/04, Graciliano M. P. wrote:
>From: "Graciliano M. P." <gmpowers at terra.com.br>
>To: "Module Authors" <module-authors at perl.org>
>Date: Sun, 22 Feb 2004 03:42:09 -0300
>Cc: r-help at stat.math.ethz.ch
>Subject: [R] New Perl module Statistics::R
>X-BeenThere: r-help at stat.math.ethz.ch
>List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
>List-Unsubscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>,
>	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
>List-Archive: <https://www.stat.math.ethz.ch/pipermail/r-help>
>List-Post: <mailto:r-help at stat.math.ethz.ch>
>List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
>List-Subscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>,
>	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>Sender: r-help-bounces+macq=llnl.gov at stat.math.ethz.ch
>
>I have released the new module Statistics::R and need some feedback of it's
>status.
>This will permit the control of the the R (R-project) interpreter through
>Perl in different architectures and OS.
>
>You can for example, start only one instance of the R interpreter and have
>different Perl process accessing it. What will save the initiation time of R
>and memory.
>
>Soo, I will appreciate if some one can test it in different OS. Tested with
>Win32 and Linux.
>
>http://search.cpan.org/~gmpassos/Statistics-R-0.01/
>
>Thanks in advance.
>
>Regards,
>Graciliano M. P.
>



At 5:54 PM -0400 6/24/05, Omar Lakkis wrote:
>I use CGIwithR to run a website that executes R scripts under cgi-bin.
>Are there any projects that implement a work frame using R similar to
>Java's servlet engine?
>Initialization times are hurting. Can initialize once process and
>treat all web requests as threads of this process?
>Any help with this subject is greatly appreciated.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ggrothendieck at gmail.com  Sat Jun 25 01:15:13 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 19:15:13 -0400
Subject: [R] R demos
In-Reply-To: <1119623920.14681.58.camel@localhost.localdomain>
References: <1119623920.14681.58.camel@localhost.localdomain>
Message-ID: <971536df05062416157838bffb@mail.gmail.com>

On 6/24/05, Federico Calboli <f.calboli at imperial.ac.uk> wrote:
> Hi All,
> 
> I am currently preparing some form of slideshow introducing R and its
> capabilities for some colleagues. The thing will be about 30 mins, and
> I'd like to have some "pretty pictures" and some "amazing facts" (I'm
> trying to sell, obviously :)).
> 
> Can I ask if it's possible to easily retrieve a gross figure of the
> number of functions in R considering the "base" install and all the
> libraries available?
> 
> Apart from graphics and lattice, are there any more packages producing
> eye catching graphics (possibly with a survival analysis/epidemiological
> bend)?
> 
> Cheers,
> 
> Federico Calboli
> 
> --
> Federico C. F. Calboli
> Department of Epidemiology and Public Health
> Impe

Here are a few sources:

- links to R graphics demos:
   http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?GraphGallery

- R command giving number of CRAN packages (excludes BioC & other repositories):
   nrow(CRAN.packages())

- graph of r-help activity:
   http://dir.gmane.org/gmane.comp.lang.r.general

- data for another measure of this:
   https://stat.ethz.ch/pipermail/r-help/2004-April/048385.html



From ggrothendieck at gmail.com  Sat Jun 25 01:17:45 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 19:17:45 -0400
Subject: [R] help with PCA and classical scaling
In-Reply-To: <BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>
References: <BHEOLDJKKPGKLNNLPCLMGEENCAAA.domenico.cozzetto@uniroma1.it>
	<BHEOLDJKKPGKLNNLPCLMOEDDCBAA.domenico.cozzetto@uniroma1.it>
Message-ID: <971536df05062416174e30537e@mail.gmail.com>

On 6/24/05, Domenico Cozzetto <domenico.cozzetto at uniroma1.it> wrote:
> Dear all,
> I have a matrix of dissimilarities and I'd like to get a 2d embedding. I'm
> not an expert of these techniques, so I'm a bit confused... Furthermore I
> was not able to find on the web any satisfactory tutorial. So even though
> this may be not the most appropriate place to discuss about this issues, I'd
> be very grateful to those who will reply.
> 
> My first question is: Do I need an initial embedding of my data before
> applying PCA through the methods princomp or prcomp in the stats package?
> 
> What does it mean that PCA and classical scaling are equivalent? And where
> can I find a proof of this fact?
> 
> If this is true, I should get the same results by applying the methods
> prcomp() or cmdscale() in stats. If it may help, use the dissimilarities in
> the attached "diss.tab" file.


We can verify the equivalence of the PCA components and the
MDS components on the iris data set.   Eigenvectors are not
unique but it seems that changing the signs of the
components is all we will need to do here:

iris4 <- iris[,-5] # test data
std <- function(x) x %*% sign(diag(x[1,])) # standardize sign
iris4.pca <- std(predict(prcomp(iris4)))
iris4.mds <- std(cmdscale(dist(iris4), k = ncol(iris4)))
all.equal(iris4.pca, iris4.mds) # TRUE



From ggrothendieck at gmail.com  Sat Jun 25 01:21:21 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 19:21:21 -0400
Subject: [R] comparing strength of association instead of strength of
	evidence?
In-Reply-To: <42BC5EA0.2030202@acelerate.com>
References: <cdf817830506240759cd3ae6a@mail.gmail.com>
	<42BC5EA0.2030202@acelerate.com>
Message-ID: <971536df05062416214e404045@mail.gmail.com>

On 6/24/05, Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> wrote:
> Weiwei Shi wrote:
> 
> >Hi,
> >I asked this question before, which was hidden in a bunch of
> >questions. I repharse it here and hope I can get some help this time:
> >
> >I have 2 contingency tables which have the same group variable Y. I
> >want to compare the strength of association between X1/Y and X2/Y. I
> >am not sure if comparing p-values IS the way  even though the
> >probability of seeing such "weird" observation under H0 defines
> >p-value and it might relate to the strength of association somehow.
> >But I read the following statement from Alan Agresti's "An
> >Introduction to Categorical Data Analysis" :
> >"Chi-squared tests simply indicate the degree of EVIDENCE for an
> >association....It is sensible to decompose chi-squared into
> >components, study residuals, and estimate parameters such as odds
> >ratios that describe the STRENGTH OF ASSOCIATION".
> >
> >
> >
> Here are some things you can do:
> 
>  > tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> 
>  > tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
>  > library(epitools) # on CRAN
>  > ?odds.ratio
> Help for 'odds.ratio' is shown in the browser
>  > library(help=epitools) # on CRAN
>  > tab1
>      [,1]    [,2]
> [1,] 11266 2151526
> [2,]   125   31734
>  > odds.ratio(11266, 125, 2151526, 31734)
> Error in fisher.test(tab) : FEXACT error 40.
> Out of workspace.                 # so this are evidently for tables
> with smaller counts
>  > library(vcd) # on CRAN
> 
>  > ?oddsratio
> Help for 'oddsratio' is shown in the browser
>  > oddsratio( tab1)  # really is logodds ratio
> [1] 0.2807548
>  > plot(oddsratio( tab1) )
>  > library(help=vcd) # on CRAN  Read this for many nice functions.
>  > fourfoldplot(tab1)
>  > mosaicplot(tab1)     # not really usefull for this table
> 
> Also has a look at function Crosstable in package gmodels.
> 
> To decompose the chisqure you can program yourselves:
> 
> decomp.chi <- function(tab) {
>        rows <-  rowSums(tab)
>        cols <-   colSums(tab)
>        N <-   sum(rows)
>         E <- rows %o% cols / N
>         contrib <- (tab-E)^2/E
>         contrib }
> 
> 
>  > decomp.chi(tab1)
>          [,1]         [,2]
> [1,] 0.1451026 0.0007570624
> [2,] 9.8504915 0.0513942218
>  >
> 
> So you can easily see what cell contributes most to the overall chisquared.
> 
> Kjetil
> 
> 
> 
> 
> 
> >Can I do this "decomposition" in R for the following example including
> >2 contingency tables?
> >
> >
> >
> >>tab1<-array(c(11266, 125, 2151526, 31734), dim=c(2,2))
> >>tab1
> >>
> >>
> >      [,1]    [,2]
> >[1,] 11266 2151526
> >[2,]   125   31734
> >
> >
> >
> >>tab2<-array(c(43571, 52, 2119221, 31807), dim=c(2,2))
> >>tab2
> >>
> >>
> >      [,1]    [,2]
> >[1,] 43571 2119221
> >[2,]    52   31807
> >


Here are a few more ways of doing this using chisq.test,
glm and assocplot:

> ## chisq.test ###

> tab1.chisq <- chisq.test(tab1)

> # decomposition of chisq
> resid(tab1.chisq)^2
          [,1]         [,2]
[1,] 0.1451026 0.0007570624
[2,] 9.8504915 0.0513942218

> # same
> with(tab1.chisq, (observed - expected)^2/expected) 
          [,1]         [,2]
[1,] 0.1451026 0.0007570624
[2,] 9.8504915 0.0513942218


> # Pearson residuals
> resid(tab1.chisq) 
           [,1]        [,2]
[1,]  0.3809234 -0.02751477
[2,] -3.1385493  0.22670294

> # same
> with(tab1.chisq, (observed - expected)/sqrt(expected)) 
           [,1]        [,2]
[1,]  0.3809234 -0.02751477
[2,] -3.1385493  0.22670294


> ### glm ###
> # Pearson residuals via glm

> tab1.df <- data.frame(count = c(tab1), A = gl(2,2), B = gl(2,1,4))
> tab1.glm <- glm(count ~ ., tab1.df, family = poisson())
> resid(tab1.glm, type = "pearson")
          1           2           3           4 
 0.38092339 -3.13854927 -0.02751477  0.22670294 
> plot(tab1.glm)

> ### assocplot ###
> # displaying Pearson residuals via an assocplot
> assocplot(t(tab1))



From ggrothendieck at gmail.com  Sat Jun 25 01:22:22 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 24 Jun 2005 19:22:22 -0400
Subject: [R] Gini with frequencies
In-Reply-To: <003801c578ff$60fec4b0$1600a8c0@thesahajamach>
References: <003801c578ff$60fec4b0$1600a8c0@thesahajamach>
Message-ID: <971536df0506241622406a6e81@mail.gmail.com>

On 6/24/05, Dimitri Joe <dimitrijoe at yahoo.com.br> wrote:
> Hi there,
> 
> I am trying to compute Gini coefficients for vectors containing income classes. The data I possess look loke this:
> 
> yit <- c(135, 164, 234, 369)
> piit <-  c(367, 884, 341, 74 )
> 
> where yit is the vector of income classes, and fit is the vector of associated frequencies.(This data is from Rustichini, Ichino and Checci (Journal of Public Economics, 1999) ). In ineq pacakge, Gini( ) doesn't seem to handle frequencies. On the other had, Lc() compute the Lorenz curve using these frequencies, but I don't manage to use its output to compute the Gini coefficient. I wonder if you have already writen some function to do this, or if you could shed some light here and help me.
> 

Assuming x and n as in the args to Lc, use Brown's formula:
	with(Lc(x,n), 1 - sum(diff(p) * (L[-1] + L[-length(L)])))
or equivalently:
	with(Lc(x,n), 1 - sum(c(0,diff(p),0) * (c(L,0) + c(0,L))))

See 
  http://en.wikipedia.org/wiki/Gini_coefficient
and the online inequality calculator which can be used to
double check results: 
  http://www.poorcity.richcity.org/calculator.htm



From davidoff at haas.berkeley.edu  Sat Jun 25 02:11:21 2005
From: davidoff at haas.berkeley.edu (Thomas Davidoff)
Date: Fri, 24 Jun 2005 17:11:21 -0700
Subject: [R] r equivalent of egen?  Not tapply
Message-ID: <FDFA1C0D-D078-419F-9B68-DB9F1175FD8F@haas.berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/dee3b3a1/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Sat Jun 25 02:24:34 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Sat, 25 Jun 2005 02:24:34 +0200 (CEST)
Subject: [R] Gini with frequencies
In-Reply-To: <003801c578ff$60fec4b0$1600a8c0@thesahajamach>
References: <003801c578ff$60fec4b0$1600a8c0@thesahajamach>
Message-ID: <Pine.LNX.4.58.0506250219130.21696@thorin.ci.tuwien.ac.at>

On Fri, 24 Jun 2005, Dimitri Joe wrote:

> Hi there,
>
> I am trying to compute Gini coefficients for vectors containing income classes. The data I possess look loke this:
>
> yit <- c(135, 164, 234, 369)
> piit <-  c(367, 884, 341, 74 )
>
> where yit is the vector of income classes, and fit is the vector of
> associated frequencies.(This data is from Rustichini, Ichino and Checci
> (Journal of Public Economics, 1999) ). In ineq pacakge, Gini( ) doesn't
> seem to handle frequencies.

Yeah, this is on our todo list, it would also be nice to support interval
data. Hopefully we'll be able to provide something like that in future
versions...

>  On the other had, Lc() compute the Lorenz
> curve using these frequencies, but I don't manage to use its output to
> compute the Gini coefficient. I wonder if you have already writen some
> function to do this, or if you could shed some light here and help me.

The naive approach would be to do
  yit_piit <- rep(yit, piit)
i.e., to simply expand the full vector of income observations and then do
  Gini(yit_piit)
etc.
If the frequencies are not too large (and they only have to be specified
up to a factor), this is a workaround which is not nice, but works
reasonably well.
Z

> Thank you very much,
>
> Dimitri Szerman
>
> 	[[alternative HTML version deleted]]
>
>
>
>
>
>
> _______________________________________________________
>
> Instale o discador agora! http://br.acesso.yahoo.com/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mike.rstat at gmail.com  Sat Jun 25 02:39:33 2005
From: mike.rstat at gmail.com (Mike R)
Date: Fri, 24 Jun 2005 17:39:33 -0700
Subject: [R] Off Topic: Keeling Curve: Charles David Keeling
Message-ID: <27db823f05062417395dabd82b@mail.gmail.com>

Charles David Keeling, the world's leading authority and pioneer
on atmospheric greenhouse gas accumulation, died Monday 
evening of a sudden and unexpected heart attack, following a 
short hike in his beloved Montana.

http://scrippsnews.ucsd.edu/article_detail.cfm?article_num=687

Mike



From uofiowa at gmail.com  Sat Jun 25 03:42:30 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Fri, 24 Jun 2005 21:42:30 -0400
Subject: [R] unprotect(): stack imbalance
Message-ID: <3f87cc6d0506241842f719d68@mail.gmail.com>

Error in "[<-.data.frame"(`*tmp*`, j, "signal", value = 1) : 
        unprotect(): stack imbalance

What could have geerated this error? Where should I start looking?



From davidoff at haas.berkeley.edu  Sat Jun 25 03:45:54 2005
From: davidoff at haas.berkeley.edu (Thomas Davidoff)
Date: Fri, 24 Jun 2005 18:45:54 -0700
Subject: [R] group means: split and unsplit
Message-ID: <C1336C0A-4826-4C16-825E-C7EA1731163D@haas.berkeley.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/cffc127c/attachment.pl

From andy_liaw at merck.com  Sat Jun 25 04:25:27 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 24 Jun 2005 22:25:27 -0400
Subject: [R] group means: split and unsplit
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA09@usctmx1106.merck.com>

I didn't quite understand what you were looking for, but you may want to
check out ave()...

Andy

> From: Thomas Davidoff
> 
> Took me a while but I figured out how to put in common values of  
> group means/counts, etc. to do the same thing as egen.  lapply with  
> split and then unsplit.
> Thomas Davidoff
> Assistant Professor
> Haas School of Business
> UC Berkeley
> Berkeley, CA 94720
> phone:     (510) 643-1425
> fax:        (510) 643-7357
> davidoff at haas.berkeley.edu
> http://faculty.haas.berkeley.edu/davidoff
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From murdoch at stats.uwo.ca  Sat Jun 25 04:48:50 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 24 Jun 2005 22:48:50 -0400
Subject: [R] unprotect(): stack imbalance
In-Reply-To: <3f87cc6d0506241842f719d68@mail.gmail.com>
References: <3f87cc6d0506241842f719d68@mail.gmail.com>
Message-ID: <42BCC612.6020506@stats.uwo.ca>

Omar Lakkis wrote:
> Error in "[<-.data.frame"(`*tmp*`, j, "signal", value = 1) : 
>         unprotect(): stack imbalance
> 
> What could have geerated this error? Where should I start looking?

That's a bug in the C code.  Try to make it reproducible, then simplify 
it as much as you can.  (If it only happens for certain random inputs, 
use set.seed() to make it reproducible.)  If it happens in code that's 
in a contributed package, send it to the maintainer.  If it happens in 
base R code, post it here.

Duncan Murdoch



From w.northcott at unsw.edu.au  Sat Jun 25 04:51:07 2005
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Sat, 25 Jun 2005 12:51:07 +1000
Subject: [R] compiling gap on mac os x
In-Reply-To: <mailman.13.1119607201.28537.r-help@stat.math.ethz.ch>
References: <mailman.13.1119607201.28537.r-help@stat.math.ethz.ch>
Message-ID: <D18E9C1E-E831-4AD5-81F9-593B4D81A990@unsw.edu.au>

On 24/06/2005, at 8:00 PM, Kenny Ye wrote:
> Hi, I am having trouble compiling package gap
> http://www.hgmp.mrc.ac.uk/~jzhao/r-progs.htm on Tiger. I have  
> installed
> XcodeTools 2.1. The binary version of gap currently available on CRAN
> has some bug and is fixed in the latest version.
> ........
> '/Library/Frameworks/R.framework/Versions/2.1.0/Resources/library/gap'
>     npackage installation failed
> ld: warning multiple definitions of symbol _i1mach_
> pfc.sim.o definition of _i1mach_ in section (__TEXT,__text)
> /Library/Frameworks/R.framework/R(i1mach.lo) definition of _i1mach_
> ld: 2ld.o has external relocation entries in non-writable section
> (__TEXT,__text) for symbols:
> restFP
> saveFP
> make: *** [gap.so] Error 1
> ERROR: compilation failed for package 'gap'

This is caused by using a gcc Fortran compiler build from FSF code  
with a gcc C compiler built from Apple code.

The Apple gcc library has the restFP, saveFP symbols and these are  
used by the Apple compiler.  So if you try link Fortran code built  
with an FSF compiler, with an Apple C code library, it will fail  
because the FSF gcc library included in the link by the Fortran  
compiler does not have these symbols to satisfy the undefined symbols  
in the C library.  For the full story see:
http://www.astro.gla.ac.uk/users/norman/note/2004/restFP/

There are two possible fixes:
1.  Use a gcc Fortran compiler built with Apple code such as the one  
available at www.swarm.org.  You might also try recent builds from  
hpc.sourceforge.net.
or
2.  Include the Apple gcc library in the link.  The standard way of  
doing this was via a symlink at /usr/lib/libcc_dynamic.a which linked  
to libgcc.a in the Apple compiler.  Then -lcc_dynamic was included in  
the LDFLAGS.  I think this link was dropped in recent versions of  
MacOS X, around the time they stopped shipping gcc 2.95.
You can put in the symlink yourself if its not there, and add the ld  
flag to the R configuration.

Bill Northcott



From gregory_gentlemen at yahoo.ca  Sat Jun 25 05:58:42 2005
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Fri, 24 Jun 2005 23:58:42 -0400 (EDT)
Subject: [R] optimization problem in R ... can this be done?
Message-ID: <20050625035843.95518.qmail@web31201.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050624/214d5348/attachment.pl

From p.dalgaard at biostat.ku.dk  Sat Jun 25 10:01:56 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jun 2005 10:01:56 +0200
Subject: [R] group means: split and unsplit
In-Reply-To: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA09@usctmx1106.merck.com>
References: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA09@usctmx1106.merck.com>
Message-ID: <x264w2985n.fsf@turmalin.kubism.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> I didn't quite understand what you were looking for, but you may want to
> check out ave()...

...which is implemented as split-lapply-unsplit (OK, "split<-").
 
> Andy
> 
> > From: Thomas Davidoff
> > 
> > Took me a while but I figured out how to put in common values of  
> > group means/counts, etc. to do the same thing as egen.  lapply with  
> > split and then unsplit.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From tchur at optushome.com.au  Sat Jun 25 11:11:21 2005
From: tchur at optushome.com.au (Tim Churches)
Date: Sat, 25 Jun 2005 19:11:21 +1000
Subject: [R] Confidence interval bars on Lattice barchart with groups
Message-ID: <42BD1FB9.90309@optushome.com.au>

I am trying to add confidence (error) bars to lattice barcharts (and
dotplots, and xyplots). I found this helpful message from Deepayan
Sarkar and based teh code below on it:
http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50299.html

However, I can't get it to work with groups, as illustrated. I am sure I
am missing something elementary, but I am unsure what.

Using R 2.1.1 on various platforms. I am aware of xYplot in the Hmisc
library but would prefer to avoid any dependency on a non-core R
library, if possible.

Tim C

##################################################################
# set up dummy test data
testdata <- data.frame(
dsr=c(1,2,3,4,5,6,7,8,9,10,0,1,2,3,4,5,6,7,8,9,
      2,3,4,5,6,7,8,9,10,11,3,4,5,6,7,8,9,10,11,12),
year=as.factor(c(1998,1998,1998,1998,1998,1998,1998,1998,1998,1998,
                 1999,1999,1999,1999,1999,1999,1999,1999,1999,1999,
                 2000,2000,2000,2000,2000,2000,2000,2000,2000,2000,
                 2001,2001,2001,2001,2001,2001,2001,2001,2001,2001)),
geog_area=c('North','South','East','West','Middle',
            'North','South','East','West','Middle',
            'North','South','East','West','Middle',
            'North','South','East','West','Middle',
            'North','South','East','West','Middle',
            'North','South','East','West','Middle',
            'North','South','East','West','Middle',
            'North','South','East','West','Middle'),
sex=c('Male','Male','Male','Male','Male',
      'Female','Female','Female','Female','Female',
      'Male','Male','Male','Male','Male',
      'Female','Female','Female','Female','Female',
      'Male','Male','Male','Male','Male',
      'Female','Female','Female','Female','Female',
      'Male','Male','Male','Male','Male',
      'Female','Female','Female','Female','Female'),
age=c('Old','Old','Old','Old','Old',
      'Young','Young','Young','Young','Young',
      'Old','Old','Old','Old','Old',
      'Young','Young','Young','Young','Young',
      'Old','Old','Old','Old','Old',
      'Young','Young','Young','Young','Young',
      'Old','Old','Old','Old','Old',
      'Young','Young','Young','Young','Young'))

# add dummy lower and upper confidence limits
testdata$dsr_ll <- testdata$dsr - 0.7
testdata$dsr_ul <- testdata$dsr + 0.5

# examine the test data
testdata

# check that a normal barchart with groups works OK - it does
barchart(geog_area ~ dsr | year, testdata, groups=sex, origin = 0)

# this works as expected, but not sure what teh error messages mean
with(testdata,barchart(geog_area ~ dsr | year + sex,
              origin = 0,
              dsr_ul = dsr_ul,
              dsr_ll = dsr_ll,
              panel = function(x, y, ..., dsr_ll, dsr_ul, subscripts) {
                  panel.barchart(x, y, subscripts, ...)
                  dsr_ll <- dsr_ll[subscripts]
                  dsr_ul <- dsr_ul[subscripts]
                  panel.segments(dsr_ll,
                                 as.numeric(y),
                                 dsr_ul,
                                 as.numeric(y),
                                 col = 'red', lwd = 2)}
              ))

# no idea what I am doing wrong here, but there is not one bar per
group... something
# to do with panel.groups???
with(testdata,barchart(geog_area ~ dsr | year, groups=sex,
              origin = 0,
              dsr_ul = dsr_ul,
              dsr_ll = dsr_ll,
              panel = function(x, y, ..., dsr_ll, dsr_ul, subscripts,
groups) {
                  panel.barchart(x, y, subscripts, groups, ...)
                  dsr_ll <- dsr_ll[subscripts]
                  dsr_ul <- dsr_ul[subscripts]
                  panel.segments(dsr_ll,
                                 as.numeric(y),
                                 dsr_ul,
                                 as.numeric(y),
                                 col = 'red', lwd = 2)}
              ))
##################################################################



From deepayan.sarkar at gmail.com  Sat Jun 25 14:23:59 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Sat, 25 Jun 2005 07:23:59 -0500
Subject: [R] Confidence interval bars on Lattice barchart with groups
In-Reply-To: <42BD1FB9.90309@optushome.com.au>
References: <42BD1FB9.90309@optushome.com.au>
Message-ID: <eb555e660506250523471631d0@mail.gmail.com>

On 6/25/05, Tim Churches <tchur at optushome.com.au> wrote:
> I am trying to add confidence (error) bars to lattice barcharts (and
> dotplots, and xyplots). I found this helpful message from Deepayan
> Sarkar and based teh code below on it:
> http://finzi.psych.upenn.edu/R/Rhelp02a/archive/50299.html
> 
> However, I can't get it to work with groups, as illustrated. I am sure I
> am missing something elementary, but I am unsure what.
> 
> Using R 2.1.1 on various platforms. I am aware of xYplot in the Hmisc
> library but would prefer to avoid any dependency on a non-core R
> library, if possible.
> 
> Tim C
> 
> ##################################################################
> # set up dummy test data
> testdata <- data.frame(
> dsr=c(1,2,3,4,5,6,7,8,9,10,0,1,2,3,4,5,6,7,8,9,
>       2,3,4,5,6,7,8,9,10,11,3,4,5,6,7,8,9,10,11,12),
> year=as.factor(c(1998,1998,1998,1998,1998,1998,1998,1998,1998,1998,
>                  1999,1999,1999,1999,1999,1999,1999,1999,1999,1999,
>                  2000,2000,2000,2000,2000,2000,2000,2000,2000,2000,
>                  2001,2001,2001,2001,2001,2001,2001,2001,2001,2001)),
> geog_area=c('North','South','East','West','Middle',
>             'North','South','East','West','Middle',
>             'North','South','East','West','Middle',
>             'North','South','East','West','Middle',
>             'North','South','East','West','Middle',
>             'North','South','East','West','Middle',
>             'North','South','East','West','Middle',
>             'North','South','East','West','Middle'),
> sex=c('Male','Male','Male','Male','Male',
>       'Female','Female','Female','Female','Female',
>       'Male','Male','Male','Male','Male',
>       'Female','Female','Female','Female','Female',
>       'Male','Male','Male','Male','Male',
>       'Female','Female','Female','Female','Female',
>       'Male','Male','Male','Male','Male',
>       'Female','Female','Female','Female','Female'),
> age=c('Old','Old','Old','Old','Old',
>       'Young','Young','Young','Young','Young',
>       'Old','Old','Old','Old','Old',
>       'Young','Young','Young','Young','Young',
>       'Old','Old','Old','Old','Old',
>       'Young','Young','Young','Young','Young',
>       'Old','Old','Old','Old','Old',
>       'Young','Young','Young','Young','Young'))
> 
> # add dummy lower and upper confidence limits
> testdata$dsr_ll <- testdata$dsr - 0.7
> testdata$dsr_ul <- testdata$dsr + 0.5
> 
> # examine the test data
> testdata
> 
> # check that a normal barchart with groups works OK - it does
> barchart(geog_area ~ dsr | year, testdata, groups=sex, origin = 0)
> 
> # this works as expected, but not sure what teh error messages mean
> with(testdata,barchart(geog_area ~ dsr | year + sex,
>               origin = 0,
>               dsr_ul = dsr_ul,
>               dsr_ll = dsr_ll,
>               panel = function(x, y, ..., dsr_ll, dsr_ul, subscripts) {
>                   panel.barchart(x, y, subscripts, ...)

This is where the warnings are coming from. You are using positional
matching to supply (unnamed) arguments, but the third argument of
panel.barchart is not 'subscripts'. You should use

                  panel.barchart(x, y, subscripts = subscripts, ...)

instead.


>                   dsr_ll <- dsr_ll[subscripts]
>                   dsr_ul <- dsr_ul[subscripts]
>                   panel.segments(dsr_ll,
>                                  as.numeric(y),
>                                  dsr_ul,
>                                  as.numeric(y),
>                                  col = 'red', lwd = 2)}
>               ))
> 
> # no idea what I am doing wrong here, but there is not one bar per
> group... something
> # to do with panel.groups???
> with(testdata,barchart(geog_area ~ dsr | year, groups=sex,
>               origin = 0,
>               dsr_ul = dsr_ul,
>               dsr_ll = dsr_ll,
>               panel = function(x, y, ..., dsr_ll, dsr_ul, subscripts,
> groups) {
>                   panel.barchart(x, y, subscripts, groups, ...)
>                   dsr_ll <- dsr_ll[subscripts]
>                   dsr_ul <- dsr_ul[subscripts]
>                   panel.segments(dsr_ll,
>                                  as.numeric(y),
>                                  dsr_ul,
>                                  as.numeric(y),
>                                  col = 'red', lwd = 2)}
>               ))

panel.groups will not help here, since placing multiple bars side by
side needs specialized calculations, which are done within
panel.barchart. To add bars to these, you will need to reproduce those
calculations.

Things are much easier with dotplot, e.g.:

with(testdata,
     dotplot(geog_area ~ dsr | year,
             groups=sex, pch = 16,
             dsr_ul = dsr_ul,
             dsr_ll = dsr_ll,
             panel.groups =
             function(x, y, ..., 
                      dsr_ll, dsr_ul,
                      subscripts) {
                 dsr_ll <- dsr_ll[subscripts]
                 dsr_ul <- dsr_ul[subscripts]
                 panel.segments(dsr_ll,
                                as.numeric(y),
                                dsr_ul,
                                as.numeric(y),
                                ...)
                 panel.xyplot(x, y, ...)
             }))

Deepayan



From yunusbb at inbox.ru  Sat Jun 25 16:40:29 2005
From: yunusbb at inbox.ru (Sotdikov Mansor)
Date: Sat, 25 Jun 2005 20:40:29 +0600
Subject: [R] r-help
Message-ID: <000a01c57993$d626fd40$acd0ae53@2122LAB>

There is a function 'simMD()' in 'popgen' library which
simulates a sample of genotype data as follows:
> library(popgen)
> x <- simMD(20, 2, 2, p = NULL, c(0.09, 0.05), ac = 2, beta = 1)
> x
  , , 1

      [,1] [,2]
 [1,]    1    1
 [2,]    1    1
      ...
[37,]    1    2
[38,]    2    2
[39,]    2    2
[40,]    2    2

, , 2

      [,1] [,2]
 [1,]    2    2
 [2,]    1    2
 [3,]    1    2
      ...
[38,]    2    1
[39,]    1    2
[40,]    1    2
>
How can I repeat this function, for example, 1000 times to generate 1000
samples and assign each output to distinct 'vector' Xi, where i=1,2,...,1000
The goal is to generate a large number of samples using this function and
then use them in further analysis.
Any suggestions would be appreciated
Sitdikov Mansor



From ligges at statistik.uni-dortmund.de  Sat Jun 25 17:02:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 25 Jun 2005 17:02:43 +0200
Subject: [R] r-help
In-Reply-To: <000a01c57993$d626fd40$acd0ae53@2122LAB>
References: <000a01c57993$d626fd40$acd0ae53@2122LAB>
Message-ID: <42BD7213.5010008@statistik.uni-dortmund.de>

Sotdikov Mansor wrote:

> There is a function 'simMD()' in 'popgen' library which

"package", not "library".



> simulates a sample of genotype data as follows:
> 
>>library(popgen)
>>x <- simMD(20, 2, 2, p = NULL, c(0.09, 0.05), ac = 2, beta = 1)
>>x
> 
>   , , 1
> 
>       [,1] [,2]
>  [1,]    1    1
>  [2,]    1    1
>       ...
> [37,]    1    2
> [38,]    2    2
> [39,]    2    2
> [40,]    2    2
> 
> , , 2
> 
>       [,1] [,2]
>  [1,]    2    2
>  [2,]    1    2
>  [3,]    1    2
>       ...
> [38,]    2    1
> [39,]    1    2
> [40,]    1    2
> 
> How can I repeat this function, for example, 1000 times to generate 1000
> samples and assign each output to distinct 'vector' Xi, where i=1,2,...,1000

The above does not look like a vector (even if internally represented as 
such).


> The goal is to generate a large number of samples using this function and
> then use them in further analysis.

I'd write them into a list by
replicate(1000, YourCall)

If you really want objects X1, ..., Xn, you should read the FAQ and ?assign.

Uwe Ligges




> Any suggestions would be appreciated
> Sitdikov Mansor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Sat Jun 25 17:13:18 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jun 2005 17:13:18 +0200
Subject: [R] r-help
In-Reply-To: <000a01c57993$d626fd40$acd0ae53@2122LAB>
References: <000a01c57993$d626fd40$acd0ae53@2122LAB>
Message-ID: <x21x6q8o6p.fsf@turmalin.kubism.ku.dk>

"Sotdikov Mansor" <yunusbb at inbox.ru> writes:

> There is a function 'simMD()' in 'popgen' library which
> simulates a sample of genotype data as follows:
> > library(popgen)
> > x <- simMD(20, 2, 2, p = NULL, c(0.09, 0.05), ac = 2, beta = 1)
> > x
...
> >
> How can I repeat this function, for example, 1000 times to generate 1000
> samples and assign each output to distinct 'vector' Xi, where i=1,2,...,1000
> The goal is to generate a large number of samples using this function and
> then use them in further analysis.
> Any suggestions would be appreciated
> Sitdikov Mansor

replicate(5, list(simMD(20, 2, 2, p = NULL, c(0.09, 0.05), 
                       ac = 2, beta = 1)))

for larger values of 5 ;-)

The list() is there to prevent dimensions from getting lost.


-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From brianlunergan at yahoo.ca  Sat Jun 25 19:56:50 2005
From: brianlunergan at yahoo.ca (Brian Lunergan)
Date: Sat, 25 Jun 2005 13:56:50 -0400 (EDT)
Subject: [R] Listing knowledge of R as a skill on a resume...
Message-ID: <20050625175650.45115.qmail@web54506.mail.yahoo.com>

Afternoon folks:

This question may be a touch off-topic, but I had to start
somewhere.

As I become more proficient with R I would like to play up
the point in the computer skills section of my resume. Can
anybody on the list provide an idea or two as to what might
be an appropriate way to do this? Is it a language or an
application? What's a proper description for it?

I leave to everyone's judgement as to where to respond, but
I suspect off-list would be proper and leave the list
bandwidth for more directly R topics.

Thanks in advance for any help possible.

Regards...


---
Brian Lunergan
Nepean, Ontario
Canada



From spencer.graves at pdf.com  Sat Jun 25 20:04:55 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 25 Jun 2005 11:04:55 -0700
Subject: [R] Listing knowledge of R as a skill on a resume...
In-Reply-To: <20050625175650.45115.qmail@web54506.mail.yahoo.com>
References: <20050625175650.45115.qmail@web54506.mail.yahoo.com>
Message-ID: <42BD9CC7.9010007@pdf.com>


Brian Lunergan wrote:

> Afternoon folks:
> 
> This question may be a touch off-topic, but I had to start
> somewhere.
> 
> As I become more proficient with R I would like to play up
> the point in the computer skills section of my resume. Can
> anybody on the list provide an idea or two as to what might
> be an appropriate way to do this? Is it a language or an
> application? What's a proper description for it?

	  On "http://www.r-project.org/", I find, "R is a free software 
environment for statistical computing and graphics."
> 
> I leave to everyone's judgement as to where to respond, but
> I suspect off-list would be proper and leave the list
> bandwidth for more directly R topics.
> 
> Thanks in advance for any help possible.
> 
> Regards...
> 
> 
> ---
> Brian Lunergan
> Nepean, Ontario
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From hb at maths.lth.se  Sat Jun 25 20:07:19 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sat, 25 Jun 2005 20:07:19 +0200
Subject: [R] exception handeling
In-Reply-To: <3f87cc6d0506241245f6372e@mail.gmail.com>
References: <3f87cc6d0506241245f6372e@mail.gmail.com>
Message-ID: <42BD9D57.3060207@maths.lth.se>

I have some examples on

   http://www.maths.lth.se/help/R/ExceptionHandlingInR/

I know that Luke Tierney, who wrote tryCatch(), has some slides on his 
homepage at http://www.stat.uiowa.edu/~luke/.

/Henrik

Omar Lakkis wrote:
> Does anyone have a good simple or a link to a tutorial in how to use
> exception handling in R?
> I see the try and tryCatch methods but it does not work quite similar
> to what I am used to in Java.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Sat Jun 25 20:16:11 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 25 Jun 2005 11:16:11 -0700
Subject: [R] optimization problem in R ... can this be done?
In-Reply-To: <20050625035843.95518.qmail@web31201.mail.mud.yahoo.com>
References: <20050625035843.95518.qmail@web31201.mail.mud.yahoo.com>
Message-ID: <42BD9F6B.7050500@pdf.com>

	  Part of the R culture is a statement by Simon Blomberg immortalized 
in library(fortunes) as, "This is R. There is no if. Only how."

	  I can't see now how I would automate a complete solution to your 
problem in general.  However, given a specific g(x, n), I would start by 
writing a function to use "expand.grid" and "contour" to make a contour 
plot of g(x, n) over specified ranges for x = seq(0, x.max, length=npts) 
and n = seq(0, n.max, npts) for a specified number of points npts.  Then 
I'd play with x.max, n.max, and npts until I got what I wanted.  With 
the right choices for x.max, n.max, and npts, the solution will be 
obvious from the plot.  In some cases, nothing more will be required.

	  If I wanted more than that, I would need to exploit further some 
specifics of the problem.  For that, permit me to restate some of what I 
think I understood of your specific problem:

	  (1) For fixed n, g(x, n) is monotonically decreasing in x>0.

	  (2) For fixed x, g(x, n) has only two local maxima, one at n=0 (or 
n=eps>0, esp arbitrarily small) and the other at n2(x), say, with a 
local minimum in between at n1(x), say.

	  With this, I would write functions to find n1(x) and n2(x) given x. 
I might not even need n1(x) if I could figure out how to obtain n2(x) 
without it.  Then I'd make a plot with two lines (using "plot" and 
"lines") of g(x, 0) and g(x, n2(x)) vs. x.

	  By the time I'd done all that, if I still needed more, I'd probably 
have ideas about what else to do.

	  hope this helps.  		
	  spencer graves


Gregory Gentlemen wrote:

> Im trying to ascertain whether or not the facilities of R are sufficient for solving an optimization problem I've come accross. Because of my limited experience with R, I would greatly appreciate some feedback from more frequent users.
> The problem can be delineated as such:
>  
> A utility function, we shall call g is a function of x, n ... g(x,n). g has the properties: n > 0, x lies on the real line. g may take values along the real line. g is such that g(x,n)=g(-x,n). g is a decreasing function of x for any n; for fixed x, g(x,n) is smooth and intially decreases upon reaching an inflection point, thereafter increasing until it reaches a maxima and then declinces (neither concave nor convex).
>  
> My optimization problem is to find the largest positive x such that g(x,n) is less than zero for all n. In fact, because of the symmetry of g around x, we need only consider x > 0. Such an x does exists in this problem, and of course g obtains a maximum value of 0 at some n for this value of x. my issue is writing some code to systematically obtain this value. 
>  
> Is R capable of handling such a problem? (i.e. through some sort of optimization fucntion, or some sort of grid search with the relevant constraints)
>  
> Any suggestions would be appreciated.
>  
> Gregory Gentlemen
> gregory_gentlemen at yahoo.ca
> 
>  
>  
> The following is a sketch of an optimization problem I need to solve.
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From aliscla at yahoo.com  Sat Jun 25 21:02:05 2005
From: aliscla at yahoo.com (Werner Bier)
Date: Sat, 25 Jun 2005 12:02:05 -0700 (PDT)
Subject: [R] Cross-validation
Message-ID: <20050625190205.72310.qmail@web61221.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050625/554f9fa7/attachment.pl

From brianlunergan at yahoo.ca  Sat Jun 25 21:43:48 2005
From: brianlunergan at yahoo.ca (Brian Lunergan)
Date: Sat, 25 Jun 2005 15:43:48 -0400 (EDT)
Subject: [R] Let me put this another way... was Listing knowledge of R as a
	skill on a resume...
In-Reply-To: <20050625175650.45115.qmail@web54506.mail.yahoo.com>
Message-ID: <20050625194348.36544.qmail@web54504.mail.yahoo.com>

Okay, Spencer's response points up my mistake in my
explanation. Let me put the question another way.

Here's the computer skills sections of my cv:

Office Suites: Microsoft Office, Lotus SmartSuite,
OpenOffice
 Applications: SAP, Microsoft Project, AutoCAD, DesignCAD
 The Internet: Internet Explorer, Outlook Express, Mozilla,
Firefox, Thunderbird
    Languages: XHTML, Cascading Style Sheets, Perl, C++,
COBOL, Fortran

What concise entry would I add to this section
demonstrating knowledge of the R environment and language?

--- Brian Lunergan <brianlunergan at yahoo.ca> wrote:

> Afternoon folks:
> 
> This question may be a touch off-topic, but I had to
> start
> somewhere.
> 
> As I become more proficient with R I would like to play
> up
> the point in the computer skills section of my resume.
> Can
> anybody on the list provide an idea or two as to what
> might
> be an appropriate way to do this? Is it a language or an
> application? What's a proper description for it?
> 
> I leave to everyone's judgement as to where to respond,
> but
> I suspect off-list would be proper and leave the list
> bandwidth for more directly R topics.
> 
> Thanks in advance for any help possible.
> 
> Regards...
> 
> 
> ---
> Brian Lunergan
> Nepean, Ontario
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


---
Brian Lunergan
Nepean, Ontario
Canada



From spencer.graves at pdf.com  Sat Jun 25 22:55:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 25 Jun 2005 13:55:52 -0700
Subject: [R] Cross-validation
In-Reply-To: <20050625190205.72310.qmail@web61221.mail.yahoo.com>
References: <20050625190205.72310.qmail@web61221.mail.yahoo.com>
Message-ID: <42BDC4D8.8000609@pdf.com>

	  I would hesitate long before doing that.  People do similar things, 
but:

"Cross-validation and bootstrapping become considerably more complicated 
for time series data; see Hjorth (1994) and Snijders (1988)."
http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html

	  I just tried www.r-project.org -> search -> "R site search" for "time 
series cross validation", "jackknife time series" and "bootstrap time 
series".  I found the above using Google for the same terms.

	  spencer graves

Werner Bier wrote:

> Dear R-help,
>  
> I was wondering if somebody has a strong opinion on the following matter:
>  
> Would you see appropriate to apply the leave-one-out cross validation techinque in time series modelling?
>  
> Thanks in advance,
> Tom
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sat Jun 25 23:21:59 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 25 Jun 2005 14:21:59 -0700
Subject: [R] Let me put this another way... was Listing knowledge of R
 as a	skill on a resume...
In-Reply-To: <20050625194348.36544.qmail@web54504.mail.yahoo.com>
References: <20050625194348.36544.qmail@web54504.mail.yahoo.com>
Message-ID: <42BDCAF7.1060100@pdf.com>

	  What I would say might depend on the job and the organizition to 
which I was applying.  At a minimum, I might list under "languages" "R 
(a primary platform of choice for new statistical algorithm development 
internationally)".  This may be too wordy, but knowledge of R gives you 
instant access to some of the best statistical techniques available in 
the world today, without having to justify spending more money to buy 
software when you don't know if it will help you or not.

	  However, your question makes me wonder if you solving the wrong 
problem (sometimes calld a Type III error;
http://core.ecu.edu/psyc/wuenschk/StatHelp/Type_III.htm).

	  Years ago, Dick Bolles (http://www.jobhuntersbible.com/) observed 
that Personel Departments (or whatever they are called) typically 
receive 1,000 resumes for every person hired.  They throw away  roughly 
90% of them.  The remaining 10% or less go to potential hiring managers, 
who do roughly the same before they start contacting people.  If 10 
people are contacted for every person hired, that one in 1,000.

	  Bolles recommends you not send out very many resumes.  Instead, 
figure out what you most enjoy doing, find an organization that would 
pay you to do that, tailor your resume and cover letter to what they do. 
  In my last job search, I spent a few days on the web and in a library 
just researching the one specific company I planned to contact.  My 
cover letter read, in essence, "I know something about what you do and 
why it's important.  And here is how I can add value ... ."  My letter 
did NOT go in the trash.  Instead I was scheduled for a "phone screen". 
  By the time of that call, I'd been to the library, gotten a recent 
publication by the person who was to call, and had skimmed it.  I passed 
the audition.

	  spencer graves

Brian Lunergan wrote:

> Okay, Spencer's response points up my mistake in my
> explanation. Let me put the question another way.
> 
> Here's the computer skills sections of my cv:
> 
> Office Suites: Microsoft Office, Lotus SmartSuite,
> OpenOffice
>  Applications: SAP, Microsoft Project, AutoCAD, DesignCAD
>  The Internet: Internet Explorer, Outlook Express, Mozilla,
> Firefox, Thunderbird
>     Languages: XHTML, Cascading Style Sheets, Perl, C++,
> COBOL, Fortran
> 
> What concise entry would I add to this section
> demonstrating knowledge of the R environment and language?
> 
> --- Brian Lunergan <brianlunergan at yahoo.ca> wrote:
> 
> 
>>Afternoon folks:
>>
>>This question may be a touch off-topic, but I had to
>>start
>>somewhere.
>>
>>As I become more proficient with R I would like to play
>>up
>>the point in the computer skills section of my resume.
>>Can
>>anybody on the list provide an idea or two as to what
>>might
>>be an appropriate way to do this? Is it a language or an
>>application? What's a proper description for it?
>>
>>I leave to everyone's judgement as to where to respond,
>>but
>>I suspect off-list would be proper and leave the list
>>bandwidth for more directly R topics.
>>
>>Thanks in advance for any help possible.
>>
>>Regards...
>>
>>
>>---
>>Brian Lunergan
>>Nepean, Ontario
>>Canada
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> 
> ---
> Brian Lunergan
> Nepean, Ontario
> Canada
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From faheem at email.unc.edu  Sun Jun 26 00:06:30 2005
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sat, 25 Jun 2005 18:06:30 -0400 (EDT)
Subject: [R] comment in src/nmath/dgamma.c
Message-ID: <Pine.LNX.4.62.0506251713300.21613@Chrestomanci>


Hi,

In src/nmath/dgamma.c the comment at the top says

  * DESCRIPTION
  *
  *   Computes the density of the gamma distribution,
  *
  *                   1/s (x/s)^{a-1} exp(-x/s)
  *        p(x;a,s) = -----------------------
  *                            (a-1)!
  *
  *   where `s' is the scale (= 1/lambda in other parametrizations)
  *     and `a' is the shape parameter ( = alpha in other contexts).

Maybe I'm missing something, but shouldn't the denominator be Gamma(a) 
(since a is not necessarily an integer)?

                                                               Faheem.



From spencer.graves at pdf.com  Sun Jun 26 00:41:34 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 25 Jun 2005 15:41:34 -0700
Subject: [R] r equivalent of egen?  Not tapply
In-Reply-To: <FDFA1C0D-D078-419F-9B68-DB9F1175FD8F@haas.berkeley.edu>
References: <FDFA1C0D-D078-419F-9B68-DB9F1175FD8F@haas.berkeley.edu>
Message-ID: <42BDDD9E.6060304@pdf.com>

	  I don't know Stata, and I don't know if I understand your question. 
Consider the following:

 > (AList <- as.list(rep(letters[1:3], 2))
+ )
[[1]]
[1] "a"

[[2]]
[1] "b"

[[3]]
[1] "c"

[[4]]
[1] "a"

[[5]]
[1] "b"

[[6]]
[1] "c"

 > (unlist(AList))
[1] "a" "b" "c" "a" "b" "c"
 > table(unlist(AList))

a b c
2 2 2

	  Does this answer your question?  If not, PLEASE do read the posting 
guide! "http://www.R-project.org/posting-guide.html".  The discipline of 
following that guide might lead you to an answer.  If it doesn't, it 
will likely increase the chances of getting a useful reply.

	  spencer graves

Thomas Davidoff wrote:

> I have a list that has about 50 elements, each repeated many times (a  
> list of states by observation).  I want to create a vector with the  
> same length with the same value repeated over and over again within  
> states.  The value I want is the number of observations that are in  
> the same state as each observation.
> In stata, this is simply egen statesum = count(state), by(state).
> What is the r equivalent?  tapply just gives me 50 observations.  No  
> luck yet trying apply, lapply, etc.
> Thomas Davidoff
> Assistant Professor
> Haas School of Business
> UC Berkeley
> Berkeley, CA 94720
> phone:     (510) 643-1425
> fax:        (510) 643-7357
> davidoff at haas.berkeley.edu
> http://faculty.haas.berkeley.edu/davidoff
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gregory_gentlemen at yahoo.ca  Sun Jun 26 01:26:15 2005
From: gregory_gentlemen at yahoo.ca (Gregory Gentlemen)
Date: Sat, 25 Jun 2005 19:26:15 -0400 (EDT)
Subject: [R] optimization problem in R ... can this be done?
In-Reply-To: <42BD9F6B.7050500@pdf.com>
Message-ID: <20050625232615.65838.qmail@web31211.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050625/e9f74468/attachment.pl

From jsorkin at grecc.umaryland.edu  Sun Jun 26 02:00:07 2005
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sat, 25 Jun 2005 20:00:07 -0400
Subject: [R] Components of variance
Message-ID: <s2bdb7d7.009@grecc.umaryland.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050625/53b7a5fd/attachment.pl

From spencer.graves at pdf.com  Sun Jun 26 04:13:25 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 25 Jun 2005 19:13:25 -0700
Subject: [R] Components of variance
In-Reply-To: <s2bdb7d7.009@grecc.umaryland.edu>
References: <s2bdb7d7.009@grecc.umaryland.edu>
Message-ID: <42BE0F45.5010500@pdf.com>

	  As far as I know, the best available is lme in library(nlme).  For 
more information, see the the following:

	  Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer)

	  Consider the following example:

 > set.seed(2)
 > lot <- rep(LETTERS[1:3], each=9)
 > lot.e <- rep(rnorm(3), each=9)
 > wf <- paste(lot, rep(1:9, each=3))
 > wf.e <- rep(rnorm(9), each=3)
 > DF <- data.frame(lot=lot, wafer=wf,
+ Vt=lot.e+wf.e+rnorm(27))
 > (fit <- lme(Vt~1, random=~1|lot/wafer, DF))
Linear mixed-effects model fit by REML
   Data: DF
   Log-restricted-likelihood: -48.44022
   Fixed: Vt ~ 1
(Intercept)
   0.6083933

Random effects:
  Formula: ~1 | lot
         (Intercept)
StdDev:    1.230572

  Formula: ~1 | wafer %in% lot
         (Intercept) Residual
StdDev:   0.9801403 1.161218

Number of Observations: 27
Number of Groups:
            lot wafer %in% lot
              3              9
 > (CI.fit <- intervals(fit))
Approximate 95% confidence intervals

  Fixed effects:
                 lower      est.    upper
(Intercept) -1.100281 0.6083933 2.317068
attr(,"label")
[1] "Fixed effects:"

  Random Effects:
   Level: lot
                     lower     est.    upper
sd((Intercept)) 0.3368174 1.230572 4.495931
   Level: wafer
                    lower      est.    upper
sd((Intercept)) 0.426171 0.9801403 2.254201

  Within-group standard error:
     lower      est.     upper
0.8378296 1.1612179 1.6094289
 > str(CI.fit)
List of 3
  $ fixed   : num [1, 1:3] -1.100  0.608  2.317
   ..- attr(*, "dimnames")=List of 2
   .. ..$ : chr "(Intercept)"
   .. ..$ : chr [1:3] "lower" "est." "upper"
   ..- attr(*, "label")= chr "Fixed effects:"
  $ reStruct:List of 2
   ..$ lot  :`data.frame':       1 obs. of  3 variables:
   .. ..$ lower: num 0.337
   .. ..$ est. : num 1.23
   .. ..$ upper: num 4.5
   ..$ wafer:`data.frame':       1 obs. of  3 variables:
   .. ..$ lower: num 0.426
   .. ..$ est. : num 0.98
   .. ..$ upper: num 2.25
   ..- attr(*, "label")= chr "Random Effects:"
  $ sigma   : atomic [1:3] 0.838 1.161 1.609
   ..- attr(*, "label")= chr "Within-group standard error:"
  - attr(*, "level")= num 0.95
  - attr(*, "class")= chr "intervals.lme"
 > diff(log(CI.fit$sigma))
    est.   upper
0.32641 0.32641

	  The last line combined with help for intervals.lme shows that the 
confidence interval for sigma (and doubtless also for lot and wafer 
variance components is based on a normal approximation for the 
distribution of log(sigma).

	  The state of the art is reflected in "lmer" in library(lme4), 
described in the following:

	  Doug Bates (2005) "Fitting linear mixed models in R" in R News 5/1 
available from "www.r-project.org" -> Newsletter

	  However, an "intervals" function is not yet available for "lmer" 
objects.
	
	  spencer graves

John Sorkin wrote:

> Could someone identify a function that I might use to perform a
> components of variance analysis? In addition to the variance
> attributable to each factor, I would also like to obtain the SE of the
> variances.
> Thank you,
> John
>  
> John Sorkin M.D., Ph.D.
> Chief, Biostatistics and Informatics
> Baltimore VA Medical Center GRECC and
> University of Maryland School of Medicine Claude Pepper OAIC
>  
> University of Maryland School of Medicine
> Division of Gerontology
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
>  
> 410-605-7119 
> -- NOTE NEW EMAIL ADDRESS:
> jsorkin at grecc.umaryland.edu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From renaud.lancelot at cirad.fr  Sun Jun 26 08:23:16 2005
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 26 Jun 2005 09:23:16 +0300
Subject: [R] Components of variance
In-Reply-To: <42BE0F45.5010500@pdf.com>
References: <s2bdb7d7.009@grecc.umaryland.edu> <42BE0F45.5010500@pdf.com>
Message-ID: <42BE49D4.4000502@cirad.fr>

Spencer Graves a ??crit :
> 	  As far as I know, the best available is lme in library(nlme).  For 
> more information, see the the following:
> 
> 	  Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
> (Springer)
> 
> 	  Consider the following example:
> 
>  > set.seed(2)
>  > lot <- rep(LETTERS[1:3], each=9)
>  > lot.e <- rep(rnorm(3), each=9)
>  > wf <- paste(lot, rep(1:9, each=3))
>  > wf.e <- rep(rnorm(9), each=3)
>  > DF <- data.frame(lot=lot, wafer=wf,
> + Vt=lot.e+wf.e+rnorm(27))
>  > (fit <- lme(Vt~1, random=~1|lot/wafer, DF))
> Linear mixed-effects model fit by REML
>    Data: DF
>    Log-restricted-likelihood: -48.44022
>    Fixed: Vt ~ 1
> (Intercept)
>    0.6083933
> 
> Random effects:
>   Formula: ~1 | lot
>          (Intercept)
> StdDev:    1.230572
> 
>   Formula: ~1 | wafer %in% lot
>          (Intercept) Residual
> StdDev:   0.9801403 1.161218
> 
> Number of Observations: 27
> Number of Groups:
>             lot wafer %in% lot
>               3              9
>  > (CI.fit <- intervals(fit))
> Approximate 95% confidence intervals
> 
>   Fixed effects:
>                  lower      est.    upper
> (Intercept) -1.100281 0.6083933 2.317068
> attr(,"label")
> [1] "Fixed effects:"
> 
>   Random Effects:
>    Level: lot
>                      lower     est.    upper
> sd((Intercept)) 0.3368174 1.230572 4.495931
>    Level: wafer
>                     lower      est.    upper
> sd((Intercept)) 0.426171 0.9801403 2.254201
> 
>   Within-group standard error:
>      lower      est.     upper
> 0.8378296 1.1612179 1.6094289
>  > str(CI.fit)
> List of 3
>   $ fixed   : num [1, 1:3] -1.100  0.608  2.317
>    ..- attr(*, "dimnames")=List of 2
>    .. ..$ : chr "(Intercept)"
>    .. ..$ : chr [1:3] "lower" "est." "upper"
>    ..- attr(*, "label")= chr "Fixed effects:"
>   $ reStruct:List of 2
>    ..$ lot  :`data.frame':       1 obs. of  3 variables:
>    .. ..$ lower: num 0.337
>    .. ..$ est. : num 1.23
>    .. ..$ upper: num 4.5
>    ..$ wafer:`data.frame':       1 obs. of  3 variables:
>    .. ..$ lower: num 0.426
>    .. ..$ est. : num 0.98
>    .. ..$ upper: num 2.25
>    ..- attr(*, "label")= chr "Random Effects:"
>   $ sigma   : atomic [1:3] 0.838 1.161 1.609
>    ..- attr(*, "label")= chr "Within-group standard error:"
>   - attr(*, "level")= num 0.95
>   - attr(*, "class")= chr "intervals.lme"
>  > diff(log(CI.fit$sigma))
>     est.   upper
> 0.32641 0.32641
> 
> 	  The last line combined with help for intervals.lme shows that the 
> confidence interval for sigma (and doubtless also for lot and wafer 
> variance components is based on a normal approximation for the 
> distribution of log(sigma).
> 
> 	  The state of the art is reflected in "lmer" in library(lme4), 
> described in the following:
> 
> 	  Doug Bates (2005) "Fitting linear mixed models in R" in R News 5/1 
> available from "www.r-project.org" -> Newsletter
> 
> 	  However, an "intervals" function is not yet available for "lmer" 
> objects.

The issue of variance components in lme4 was recently by D. Bates:

http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html

Also, a colleague wrote (in French) a nice report - with data and R code 
on how to compute variance components, their bias and distribution with 
lme (normal approximation , Monte Carlo simulation and delta method). 
Let me know if you want it.

Best,

Renaud


> 	
> 	  spencer graves
> 
> John Sorkin wrote:
> 
> 
>>Could someone identify a function that I might use to perform a
>>components of variance analysis? In addition to the variance
>>attributable to each factor, I would also like to obtain the SE of the
>>variances.
>>Thank you,
>>John
>> 
>>John Sorkin M.D., Ph.D.
>>Chief, Biostatistics and Informatics
>>Baltimore VA Medical Center GRECC and
>>University of Maryland School of Medicine Claude Pepper OAIC
>> 
>>University of Maryland School of Medicine
>>Division of Gerontology
>>Baltimore VA Medical Center
>>10 North Greene Street
>>GRECC (BT/18/GR)
>>Baltimore, MD 21201-1524
>> 
>>410-605-7119 
>>--- NOTE NEW EMAIL ADDRESS:
>>jsorkin at grecc.umaryland.edu
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 
Dr Renaud Lancelot, v??t??rinaire
Projet FSP r??gional ??pid??miologie v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From ripley at stats.ox.ac.uk  Sun Jun 26 09:04:55 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 26 Jun 2005 08:04:55 +0100 (BST)
Subject: [R] comment in src/nmath/dgamma.c
In-Reply-To: <Pine.LNX.4.62.0506251713300.21613@Chrestomanci>
References: <Pine.LNX.4.62.0506251713300.21613@Chrestomanci>
Message-ID: <Pine.LNX.4.61.0506260756540.14581@gannet.stats>

On Sat, 25 Jun 2005, Faheem Mitha wrote:

>
> Hi,
>
> In src/nmath/dgamma.c the comment at the top says
>
>  * DESCRIPTION
>  *
>  *   Computes the density of the gamma distribution,
>  *
>  *                   1/s (x/s)^{a-1} exp(-x/s)
>  *        p(x;a,s) = -----------------------
>  *                            (a-1)!
>  *
>  *   where `s' is the scale (= 1/lambda in other parametrizations)
>  *     and `a' is the shape parameter ( = alpha in other contexts).
>
> Maybe I'm missing something, but shouldn't the denominator be Gamma(a)
> (since a is not necessarily an integer)?

\Gamma(x+1) _is_ the definition of x! for x not an integer.  It has not 
defined x! and you have not defined Gamma(x) and I don't see one is 
necessarily preferable to the other.  The user-level documentation uses 
Gamma(a) but it can refer to the definition in ?gamma.


Please see the posting guide and consider which list you use: comments in 
the R source code are not really end-user questions and R-devel is the 
appropriate place to discuss the R sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sun Jun 26 11:59:18 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 26 Jun 2005 11:59:18 +0200
Subject: [R] optimization problem in R ... can this be done?
In-Reply-To: <20050625232615.65838.qmail@web31211.mail.mud.yahoo.com>
References: <20050625232615.65838.qmail@web31211.mail.mud.yahoo.com>
Message-ID: <42BE7C76.5080803@statistik.uni-dortmund.de>

Gregory Gentlemen wrote:

> Spencer: Thank you for the helpful suggestions. I have another
> question following some code I wrote. The function below gives a
> crude approximation for the x of interest (that value of x such that
> g(x,n) is less than 0 for all n).
> 
> # // btilda optimize g(n,x) for some fixed x, and then approximately
> finds that g(n,x) #    such that abs(g(n*,x)=0 // btilda <-
> function(range,len) { # range: over which to look for x bb <-
> seq(range[1],range[2],length=len) OBJ <- sapply(bb,function(x) {fixed
> <- c(x,1000000,692,500000,1600,v1,217227);
> return(optimize(g,c(1,1000),maximum=TRUE,tol=0.0000001,x=fixed)$objective)})
>  tt <- data.frame(b=bb,obj=OBJ) tt$absobj <- abs(tt$obj) d <-
> tt[order(tt$absobj),][1:3,] return(as.vector(d)) }
> 
> For instance a run of
> 
>> btilda(c(20.55806,20.55816),10000) .... returns:
> 
> b                  obj                    absobj 5834   20.55812
> -0.0004942848    0.0004942848 5833   20.55812     0.0011715433
> 0.0011715433 5835   20.55812    -0.0021601140    0.0021601140
> 
> My question is how to improve the precision of b (which is x) here.
> It seems that seq(20.55806,20.55816,length=10000 ) is only precise to
> 5 or so digits, and thus, is equivalent for numerous succesive

Why do you think so? It is much more accurate! See ?.Machine

Uwe Ligges



> values. How can I get around this?
> 
> 
> Spencer Graves <spencer.graves at pdf.com> wrote: Part of the R culture
> is a statement by Simon Blomberg immortalized in library(fortunes)
> as, "This is R. There is no if. Only how."
> 
> I can't see now how I would automate a complete solution to your 
> problem in general. However, given a specific g(x, n), I would start
> by writing a function to use "expand.grid" and "contour" to make a
> contour plot of g(x, n) over specified ranges for x = seq(0, x.max,
> length=npts) and n = seq(0, n.max, npts) for a specified number of
> points npts. Then I'd play with x.max, n.max, and npts until I got
> what I wanted. With the right choices for x.max, n.max, and npts, the
> solution will be obvious from the plot. In some cases, nothing more
> will be required.
> 
> If I wanted more than that, I would need to exploit further some 
> specifics of the problem. For that, permit me to restate some of what
> I think I understood of your specific problem:
> 
> (1) For fixed n, g(x, n) is monotonically decreasing in x>0.
> 
> (2) For fixed x, g(x, n) has only two local maxima, one at n=0 (or 
> n=eps>0, esp arbitrarily small) and the other at n2(x), say, with a 
> local minimum in between at n1(x), say.
> 
> With this, I would write functions to find n1(x) and n2(x) given x. I
> might not even need n1(x) if I could figure out how to obtain n2(x) 
> without it. Then I'd make a plot with two lines (using "plot" and 
> "lines") of g(x, 0) and g(x, n2(x)) vs. x.
> 
> By the time I'd done all that, if I still needed more, I'd probably 
> have ideas about what else to do.
> 
> hope this helps. spencer graves
> 
> 
> Gregory Gentlemen wrote:
> 
> 
>> Im trying to ascertain whether or not the facilities of R are
>> sufficient for solving an optimization problem I've come accross.
>> Because of my limited experience with R, I would greatly appreciate
>> some feedback from more frequent users. The problem can be
>> delineated as such:
>> 
>> A utility function, we shall call g is a function of x, n ...
>> g(x,n). g has the properties: n > 0, x lies on the real line. g may
>> take values along the real line. g is such that g(x,n)=g(-x,n). g
>> is a decreasing function of x for any n; for fixed x, g(x,n) is
>> smooth and intially decreases upon reaching an inflection point,
>> thereafter increasing until it reaches a maxima and then declinces
>> (neither concave nor convex).
>> 
>> My optimization problem is to find the largest positive x such that
>> g(x,n) is less than zero for all n. In fact, because of the
>> symmetry of g around x, we need only consider x > 0. Such an x does
>> exists in this problem, and of course g obtains a maximum value of
>> 0 at some n for this value of x. my issue is writing some code to
>> systematically obtain this value.
>> 
>> Is R capable of handling such a problem? (i.e. through some sort of
>> optimization fucntion, or some sort of grid search with the
>> relevant constraints)
>> 
>> Any suggestions would be appreciated.
>> 
>> Gregory Gentlemen gregory_gentlemen at yahoo.ca
>> 
>> 
>> 
>> The following is a sketch of an optimization problem I need to
>> solve.
>> 
>> __________________________________________________
>> 
>> 
>> 
>> [[alternative HTML version deleted]]
>> 
>> ______________________________________________ 
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
>> posting guide! http://www.R-project.org/posting-guide.html
> 
>



From wildscop at yahoo.com  Sun Jun 26 12:00:15 2005
From: wildscop at yahoo.com (Mohammad Ehsanul Karim)
Date: Sun, 26 Jun 2005 03:00:15 -0700 (PDT)
Subject: [R] chisq.test using amalgamation automatically (possible ?!?)
Message-ID: <20050626100015.97251.qmail@web52602.mail.yahoo.com>

Dear List,


If any of observed and/or expected data has less than
5 frequencies, then  chisq.test (Pearson's Chi-squared
Test for Count Data from package:stats) gives warning
messages. For example,

x<-c(10, 14, 10, 11, 11, 7, 8, 4, 1, 4, 4, 2, 1, 1, 2,
1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)
y<-c(9.13112391745095, 13.1626482033341,
12.6623267638188, 11.0130706413029, 9.16415925139016,
7.47441794889028, 6.03743388141852, 4.85350508692505,
3.89248001363859, 3.11803140037476, 2.49617540962629,
1.99774139023269, 1.5985926374167, 1.27909653584089,
1.02341602646530, 0.818828097315106,
0.655132353196336, 0.524159229418155,
0.418022824890164, 0.335528136508225,
0.268448671671046, 0.214779801990545,
0.171840507806838, 0.137485729582785,
0.109999238967747, 0.0880079144684513,
0.070413150156564)

Chi.Sq<-sum((c(x[1:7], sum(x[8:9]), sum(x[10:11]),
sum(x[12:27]))-c(y[1:7], sum(y[8:9]), sum(y[10:11]),
sum(y[12:27])))^2/c(y[1:7], sum(y[8:9]),
sum(y[10:11]), sum(y[12:27]))) # using amalgamation
pchisq(Chi.Sq, df=9, ncp=0, lower.tail = FALSE, log.p
= FALSE) # result being 0.8830207

but chisq.test(x,y) gives the following output with
incorrect df:

        Pearson's Chi-squared test

data:  x and y 
X-squared = 216, df = 208, p-value = 0.3373

Warning message:
Chi-squared approximation may be incorrect in:
chisq.test(x, y) 



Is there any way that we can use directly chisq.test
without having warning message in such cases (that is,
using amalgamation conveniently so that we don't have
to check each elements if they are less than 5 or not
- the whole process being automatic, may be by means
of programming)?



Any hint, help, support, references will be highly
appreciated.
Thank you for your time. 

----------------------------------

Mohammad Ehsanul Karim 

Web: http://snipurl.com/ehsan 
ISRT, University of Dhaka, BD 

----------------------------------




		
____________________________________________________ 

Rekindle the Rivalries. Sign up for Fantasy Football



From szlevine at nana.co.il  Sun Jun 26 12:02:04 2005
From: szlevine at nana.co.il (Stephen)
Date: Sun, 26 Jun 2005 13:02:04 +0300
Subject: [R] Mixed model
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64D@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050626/8aec9aa5/attachment.pl

From pir2.jv at wanadoo.fr  Sun Jun 26 12:12:11 2005
From: pir2.jv at wanadoo.fr (pir2.jv)
Date: Sun, 26 Jun 2005 12:12:11 +0200
Subject: [R] (sans objet)
Message-ID: <2CEEDF52-C795-46EC-B066-CA91BAD3E909@wanadoo.fr>

Ma premi??re question: puis-je ??crire en fran??ais, mon anglais est  
pire que pire. J'essaie en fran??ais; sinon, j'essaierai une traduction!

Je sduis d??butant. Ma question est donc simple, j'imagine.

Je travaille sur des tableaux de statistiques. Je prends un exemple:
J'ai un "frame" que j'appelle "eurostat" qui me donne des  
statistiques commerciales:

eurostat: Pays    Produit    Tonnage
             CI    801        10123
.......


J'ai un autre frame , cp qui m'indique
Produit    Nom
801        Coconut
....
et un autre qui m'indique que CI est "C??te d'Ivoire"
Je veux cr??er une nouvelle table, par exemple "Importation" qui me  
donne les donn??es suivantes:

Importation    Pays              Produit     Tonnage
                 C??te d'Ivoire    Coconut    10123

... et je n'y arrive pas. Ce doit pourtant ??tre basique.

Merci pour votre aide.

Jacques Vernin
PiR2
10, Boulevard de Brazza
13008 Marseille
0491 734 736
pir2.jv at wanadoo.fr



From t.muhlhofer at lse.ac.uk  Sun Jun 26 12:33:05 2005
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Sun, 26 Jun 2005 11:33:05 +0100
Subject: [R] r equivalent of egen?  Not tapply
Message-ID: <42BE8461.1030302@lse.ac.uk>

Thomas,

I usually use the aggregate() function, proceeding as follows.

Construct a vector of 1s that is the same length as your entire data.

Then use aggregate() with sum as a function on this vector and your 
grouping variable as a by argument (note the use of na.rm in sum). This 
will create a data.frame with the counts and the panel markers that 
represent the subcategories (one row for each).

Then use merge() to put these back into your original data.frame, as 
this will create the necessary duplicates.

Toby



From wilks at dial.pipex.com  Sun Jun 26 13:31:20 2005
From: wilks at dial.pipex.com (John Wilkinson (pipex))
Date: Sun, 26 Jun 2005 12:31:20 +0100
Subject: [R] Re  Components of variance
Message-ID: <JCEIJNOHMNBPLMGFDHNDIEJPCAAA.wilks@dial.pipex.com>


John,

In addition to 'VarCorr(nlme) and VarCorr(Matrix)', you could also try
'varcomp' function in the 'ape' package.This requires an 'lme' class of
file, from the 'nlme package as input.It additionally gives the options to
scale the components by normalizing them (both cumulatively and in total).

I hope this helps,

John

John Sorkin Wrote-----


Could someone identify a function that I might use to perform a
components of variance analysis? In addition to the variance
attributable to each factor, I would also like to obtain the SE of the
variances.
Thank you,
John

John Sorkin M.D., Ph.D.
Chief, Biostatistics and Informatics
Baltimore VA Medical Center GRECC and
University of Maryland School of Medicine Claude Pepper OAIC

University of Maryland School of Medicine
Division of Gerontology
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524

410-605-7119
-- NOTE NEW EMAIL ADDRESS:
jsorkin at grecc.umaryland.edu



From ferri.leberl at gmx.at  Sun Jun 26 14:55:13 2005
From: ferri.leberl at gmx.at (Mag. Ferri Leberl)
Date: Sun, 26 Jun 2005 14:55:13 +0200
Subject: [R] visualizing frequencies
Message-ID: <200506261455.14184.ferri.leberl@gmx.at>

Dear everybody,

In our game-theory lesson we have run several classroom-experiments where the 
students had to decide for a natural number between one and seven. I have 
troubles now to visualize the results: be a the vector of answers.

hist(a) will not assume natural numbers as answers, but rational. It will make 
the brakes exactly at the natural numbers,  which is difficult to interpret, 
as only natural numbers may be employed.

barplot(a) or barplot(a,1:7) will not aggregate the answers. If three students 
returned the number seven, it will show three bars to the size of seven 
instead one bar to the size of three on index seven.

barplot(1:7,a), in this case, will show the bar at index seven, wut it will be 
to the hight of seven and to the width of three.

I also wanted to show the results of the different versions of the experiment 
in ONE plot. As the number of participants varied I googled around in the 
R-Archives and got to recognize plot.edf. As a pitty, this function seems to 
set the index the wrong way round: the function starts with the number of 
students deciding for seven, but indexes them with one.

Can anybody help me with these two problems?
Thank you in advance!



From ggrothendieck at gmail.com  Sun Jun 26 15:16:09 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 26 Jun 2005 09:16:09 -0400
Subject: [R] visualizing frequencies
In-Reply-To: <200506261455.14184.ferri.leberl@gmx.at>
References: <200506261455.14184.ferri.leberl@gmx.at>
Message-ID: <971536df05062606165077898c@mail.gmail.com>

On 6/26/05, Mag. Ferri Leberl <ferri.leberl at gmx.at> wrote:
> Dear everybody,
> 
> In our game-theory lesson we have run several classroom-experiments where the
> students had to decide for a natural number between one and seven. I have
> troubles now to visualize the results: be a the vector of answers.
> 
> hist(a) will not assume natural numbers as answers, but rational. It will make
> the brakes exactly at the natural numbers,  which is difficult to interpret,
> as only natural numbers may be employed.
> 
> barplot(a) or barplot(a,1:7) will not aggregate the answers. If three students
> returned the number seven, it will show three bars to the size of seven
> instead one bar to the size of three on index seven.
> 
> barplot(1:7,a), in this case, will show the bar at index seven, wut it will be
> to the hight of seven and to the width of three.
> 
> I also wanted to show the results of the different versions of the experiment
> in ONE plot. As the number of participants varied I googled around in the
> R-Archives and got to recognize plot.edf. As a pitty, this function seems to
> set the index the wrong way round: the function starts with the number of
> students deciding for seven, but indexes them with one.
> 

Make sure that the data are factors so that numbers with 0 frequency still
show up and then tabulate frequencies using 'table'.

# test data
x1 <- c(1, 1, 1, 2, 5, 6)
x2 <- c(1, 3, 4, 5, 4)

# tabulate frequencies ensuring that 0 frequencies are included
x1t <- table(factor(x1, lev = 1:7))
x2t <- table(factor(x2, lev = 1:7))

# plot.table of x1t, barplot of x1t and barplot of both
plot(x1t)
barplot(x1t)
barplot(rbind(x1t, x2t),beside = TRUE)



From murdoch at stats.uwo.ca  Sun Jun 26 15:23:56 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 26 Jun 2005 09:23:56 -0400
Subject: [R] visualizing frequencies
In-Reply-To: <200506261455.14184.ferri.leberl@gmx.at>
References: <200506261455.14184.ferri.leberl@gmx.at>
Message-ID: <42BEAC6C.4050507@stats.uwo.ca>

Mag. Ferri Leberl wrote:
> Dear everybody,
> 
> In our game-theory lesson we have run several classroom-experiments where the 
> students had to decide for a natural number between one and seven. I have 
> troubles now to visualize the results: be a the vector of answers.
> 
> hist(a) will not assume natural numbers as answers, but rational. It will make 
> the brakes exactly at the natural numbers,  which is difficult to interpret, 
> as only natural numbers may be employed.

Read ?hist.  The "breaks" argument allows you to control where the 
breaks occur.  I'd guess you'd want hist(a, breaks = 0:7 + 0.5).
> 
> barplot(a) or barplot(a,1:7) will not aggregate the answers. If three students 
> returned the number seven, it will show three bars to the size of seven 
> instead one bar to the size of three on index seven.
> 
> barplot(1:7,a), in this case, will show the bar at index seven, wut it will be 
> to the hight of seven and to the width of three.
> 
> I also wanted to show the results of the different versions of the experiment 
> in ONE plot. As the number of participants varied I googled around in the 
> R-Archives and got to recognize plot.edf. As a pitty, this function seems to 
> set the index the wrong way round: the function starts with the number of 
> students deciding for seven, but indexes them with one.

There are many different ways to do this.  You can have multiple panels 
in one plot with par(mfrow=...) or using the lattice package, you can 
overlap multiple histograms in one plot with "add=T" in the call to 
hist() (but the results don't look very good), you can construct 
multiple side-by-side histograms using barplot, etc.

You should decide exactly what you want the plot to look like, then if 
you can't figure out how to draw it after reading the docs, ask here.

Duncan Murdoch

P.S. Your email address ferri.leberl at gmx.at fails with a "mailbox 
disabled" message.



From gb at stat.umu.se  Sun Jun 26 15:52:21 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Sun, 26 Jun 2005 15:52:21 +0200
Subject: [R] interpreting Weibull survival regression
In-Reply-To: <83375937.1119612448@Lab26.DOMAIN.IE.PITT.EDU>
References: <83375937.1119612448@Lab26.DOMAIN.IE.PITT.EDU>
Message-ID: <20050626135221.GA22893@stat.umu.se>

On Fri, Jun 24, 2005 at 11:27:28AM -0400, sms13+ at pitt.edu wrote:
> Hi,
> I was wondering if someone can help me 
> interpret the results of running 
> weibreg.
> 
> I run the following and get the 
> following R output.
> > weibreg(Surv(time, censor)~covar)
> fit$fail =  0
> Call:
> weibreg(formula = Surv(time, 
> censor)~covar)
> 
> Covariate           Mean       Coef 
> Rel.Risk      L-R p   Wald p
> covar     319.880    -0.002     0.998 
> 0.000
> 
> log(scale)          0.000     8.239 
> 3786.326               0.000
> log(shape)          0.000     0.265 
> 1.304               0.000
> 
> Events                    172
> Total time at risk        845891
> Max. log. likelihood      -1609.4
> LR test statistic         34.4
> Degrees of freedom        3
> Overall p-value           1.65026e-07
> 
> 
> I would just like to find the estimated 
> mean survival time as a function of the 
> covariate in the model, but am not sure 
> how to use this output to find that.
> Any help would be greatly appreciated.

The fitted model is a distribution with hazard function

h(t; a, b, z) = (a/b)(t/b)^(a-1)exp(beta*z),

where a = "baseline shape" and b = "baseline scale". z is your "covar" and
beta is the estimated regression coefficient. It is an easy exercise to
show that this is the hazard function of a Weibull distribution with shape
a  and scale  b*exp(-beta*z/a). Thus the mean is 

E(T) = b*exp(-beta*z/a)*gamma(1+1/a) ## See ?Weibull

Here gamma is the usual gamma function, see ?gamma. (I notice in the R
documentation of the Weibull distribution that "E(X) = b Gamma(1+1/a)",
which is an error; the G should be g (lowercase).) 

In your case, a = 1.304, b = 3786.326, beta = -0.002, so

E(T) = 3495 * exp(0.00153 * z) 

(given my calculations are correct).

Hth,

G??ran

> 
> Thank you,
> Steven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
G??ran Brostr??m                    tel: +46 90 786 5223
Professor and Head
Department of Statistics          fax: +46 90 786 6614
Ume?? University                   http://www.stat.umu.se/~goran.brostrom/
SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From dmbates at gmail.com  Sun Jun 26 16:01:01 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 26 Jun 2005 09:01:01 -0500
Subject: [R] Mixed model
In-Reply-To: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64D@NANAMAILBACK1.nanamail.co.il>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64D@NANAMAILBACK1.nanamail.co.il>
Message-ID: <40e66e0b050626070132c6e99@mail.gmail.com>

On 6/26/05, Stephen <szlevine at nana.co.il> wrote:
> 
> 
> 
> Hi All,
> 
> 
> 
> I am currently conducting a mixed model. I have 7 repeated measures on a
> simulated clinical trial. If I understand the model correctly, the
> outcome is the measure (as a factor) the predictors are clinical group
> and trial (1-7). The fixed factors are the measure and group. The random
> factors are the intercept and id and group.
> 
> 
> 
> I tried using 2 functions to calculate mixed effects.
> 
> Following previous correspondence .
> 
> 
> 
> Dataset <- read.table("C:/Program Files/R/rw2011/data/miss/model1a.dat",
> header=TRUE, sep="\t", na.strings="NA", dec=".", strip.white=TRUE)
> 
> attach(Dataset)
> 
> 
> 
> require (nlme)
> 
> with(Dataset, table(runnb, id, grp))
> 
> b.lvls <- table(Dataset$runnb)
> 
> nb <- length(b.lvls)
> 
> fit <- vector(mode="list", nb)
> 
> 
> 
> for(i in 1:nb)
> 
>  fit[[i]]<- lme (trans1 ~ Index1 + grp,
> 
>             random = ~ 1 | id / grp ,
> 
>             data = Dataset,
> 
>             na.action = "na.exclude")
> 
> 
> 
> 
> 
> This (above) worked OK only I am having memory problems.
> 
> I have a gig of RAM set at --sdi --max-mem-size=512M (complete version
> below)
> 
> I am wondering if running the file as a database be slower / faster?
> 
> 
> 
> Then I read that lme4 does it quicker and more accurately
> 
> so I thought that I should re-run the code but from the for line:
> 
> 
> 
> > for (i in 1:nb)
> 
> +  fit[[i]]  <- lmer(trans1 ~ Index1 + grp + (1|id:grp) + (1|id),
> 
> + Dataset, na.action = na.exclude)
> 
> 
> 
> Producing
> 
> 
> 
> Error in lmer(trans1 ~ Index1 + grp + (1 | id:grp) + (1 | id), Dataset,
> :
> 
>         flist[[2]] must be a factor of length 200000
> 
> In addition: Warning messages:
> 
> 1: numerical expression has 200000 elements: only the first used in:
> id:grp
> 
> 2: numerical expression has 200000 elements: only the first used in:
> id:grp

Check

str(Dataset)

and, if necessary, convert id to a factor with

Dataset$id <- factor(Dataset$id)


In is not surprising that you are running into memory problems.  Look
at the size of one of the fitted objects from lme or from lmer.  They
are very large because they contain a copy of the model frame (the
parts of Dataset that are needed to evaluate the model) plus a lot of
other information.  You have a large Dataset and you are saving
multiple copies of it although I must admit that I don't understand
why the calls to lme or lmer are in a loop.



From h.wickham at gmail.com  Sun Jun 26 17:57:43 2005
From: h.wickham at gmail.com (hadley wickham)
Date: Sun, 26 Jun 2005 10:57:43 -0500
Subject: [R] is.all.equal
Message-ID: <f8e6ff0505062608571a13da36@mail.gmail.com>

Hi,

The description of all.equal states "is.all.equal should be used for
programming, typically in if expressions. It is a simple wrapper using
identical as shown in the documentation there.", but is.all.equal is
not explicitly defined there (although there is a hint in the comments
that is.all.equal <- function(x,y) isTRUE(all.equal(x,y))).

Could the documentation be corrected? (or even better, how about
defining is.all.equal by default)

Thanks,

Hadley



From spencer.graves at pdf.com  Sun Jun 26 18:10:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 09:10:52 -0700
Subject: [R] Components of variance
In-Reply-To: <42BE49D4.4000502@cirad.fr>
References: <s2bdb7d7.009@grecc.umaryland.edu> <42BE0F45.5010500@pdf.com>
	<42BE49D4.4000502@cirad.fr>
Message-ID: <42BED38C.1080201@pdf.com>

Hi, Renaud:

	  Thanks for the link to Doug Bates' recent comment on this.
Unfortunately, the information contained therein is not enough for me to
easily obtain all the required standard errors and write code to make
confidence intervals.  To be more specific, I'm able to produce the same
answers as before using lmer in place of lme as follows:

> library(lme4)
> set.seed(2)
> lot <- rep(LETTERS[1:3], each=9)
> lot.e <- rep(rnorm(3), each=9)
> wf <- paste(lot, rep(1:9, each=3))
> wf.e <- rep(rnorm(9), each=3)
> DF <- data.frame(lot=lot, wafer=wf,
+ Vt=lot.e+wf.e+rnorm(27))
> (fitr <- lmer(Vt~1+(1|lot)+(1|wafer), DF))
<snip>
Random effects:
  Groups   Name        Variance Std.Dev.
  wafer    (Intercept) 0.96054  0.98007
  lot      (Intercept) 1.51434  1.23058
  Residual             1.34847  1.16124
# of obs: 27, groups: wafer, 9; lot, 3

Fixed effects:
             Estimate Std. Error DF t value Pr(>|t|)
(Intercept)  0.60839    0.81329 26  0.7481   0.4611

	  These numbers agree with what I got with lme.  I then followed your
suggetion to "http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html".
This suggested I try str(VarCorr(fitr)):

> vcFit <- VarCorr(fitr)
> str(vcFit)
Formal class 'VarCorr' [package "Matrix"] with 3 slots
   ..@ scale   : num 1.16
   ..@ reSumry :List of 2
   .. ..$ wafer:Formal class 'corrmatrix' [package "Matrix"] with 2 slots
   .. .. .. ..@ .Data : num [1, 1] 1
   .. .. .. .. ..- attr(*, "dimnames")=List of 2
   .. .. .. .. .. ..$ : chr "(Intercept)"
   .. .. .. .. .. ..$ : chr "(Intercept)"
   .. .. .. ..@ stdDev: Named num 0.844
   .. .. .. .. ..- attr(*, "names")= chr "(Intercept)"
   .. ..$ lot  :Formal class 'corrmatrix' [package "Matrix"] with 2 slots
   .. .. .. ..@ .Data : num [1, 1] 1
   .. .. .. .. ..- attr(*, "dimnames")=List of 2
   .. .. .. .. .. ..$ : chr "(Intercept)"
   .. .. .. .. .. ..$ : chr "(Intercept)"
   .. .. .. ..@ stdDev: Named num 1.06
   .. .. .. .. ..- attr(*, "names")= chr "(Intercept)"
   ..@ useScale: logi TRUE
>

	  Unfortunately, I've been so far unable to translate this into 
confidence intervals for the variance components that seem to match 
intervals(lme(...)).  The closest I came was as follows:
	
> 1.23058*sqrt(exp(qnorm(c(0.025, 0.975))*vcFit at reSumry$lot at stdDev))
[1] 0.4356044 3.4763815
> 0.98007*sqrt(exp(qnorm(c(0.025, 0.975))*vcFit at reSumry$wafer at stdDev))
[1] 0.4286029 2.2410887
>
	  The discrepancy between these numbers and intervals(lme(...)) is 25% 
for "lot", which suggest that I'm doing something wrong.  Moreover, I 
don't know how to get a similar interval for "scale", and I don't know 
how to access the estimated variance components other than manually.

	  Comments?
	  Thanks,
	  spencer graves
p.s.  R 2.1.0 patched under Windows XP.

Renaud Lancelot wrote:

> Spencer Graves a ??crit :
> 
>>	  As far as I know, the best available is lme in library(nlme).  For 
>>more information, see the the following:
>>
>>	  Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus 
>>(Springer)
>>
>>	  Consider the following example:
>>
>> > set.seed(2)
>> > lot <- rep(LETTERS[1:3], each=9)
>> > lot.e <- rep(rnorm(3), each=9)
>> > wf <- paste(lot, rep(1:9, each=3))
>> > wf.e <- rep(rnorm(9), each=3)
>> > DF <- data.frame(lot=lot, wafer=wf,
>>+ Vt=lot.e+wf.e+rnorm(27))
>> > (fit <- lme(Vt~1, random=~1|lot/wafer, DF))
>>Linear mixed-effects model fit by REML
>>   Data: DF
>>   Log-restricted-likelihood: -48.44022
>>   Fixed: Vt ~ 1
>>(Intercept)
>>   0.6083933
>>
>>Random effects:
>>  Formula: ~1 | lot
>>         (Intercept)
>>StdDev:    1.230572
>>
>>  Formula: ~1 | wafer %in% lot
>>         (Intercept) Residual
>>StdDev:   0.9801403 1.161218
>>
>>Number of Observations: 27
>>Number of Groups:
>>            lot wafer %in% lot
>>              3              9
>> > (CI.fit <- intervals(fit))
>>Approximate 95% confidence intervals
>>
>>  Fixed effects:
>>                 lower      est.    upper
>>(Intercept) -1.100281 0.6083933 2.317068
>>attr(,"label")
>>[1] "Fixed effects:"
>>
>>  Random Effects:
>>   Level: lot
>>                     lower     est.    upper
>>sd((Intercept)) 0.3368174 1.230572 4.495931
>>   Level: wafer
>>                    lower      est.    upper
>>sd((Intercept)) 0.426171 0.9801403 2.254201
>>
>>  Within-group standard error:
>>     lower      est.     upper
>>0.8378296 1.1612179 1.6094289
>> > str(CI.fit)
>>List of 3
>>  $ fixed   : num [1, 1:3] -1.100  0.608  2.317
>>   ..- attr(*, "dimnames")=List of 2
>>   .. ..$ : chr "(Intercept)"
>>   .. ..$ : chr [1:3] "lower" "est." "upper"
>>   ..- attr(*, "label")= chr "Fixed effects:"
>>  $ reStruct:List of 2
>>   ..$ lot  :`data.frame':       1 obs. of  3 variables:
>>   .. ..$ lower: num 0.337
>>   .. ..$ est. : num 1.23
>>   .. ..$ upper: num 4.5
>>   ..$ wafer:`data.frame':       1 obs. of  3 variables:
>>   .. ..$ lower: num 0.426
>>   .. ..$ est. : num 0.98
>>   .. ..$ upper: num 2.25
>>   ..- attr(*, "label")= chr "Random Effects:"
>>  $ sigma   : atomic [1:3] 0.838 1.161 1.609
>>   ..- attr(*, "label")= chr "Within-group standard error:"
>>  - attr(*, "level")= num 0.95
>>  - attr(*, "class")= chr "intervals.lme"
>> > diff(log(CI.fit$sigma))
>>    est.   upper
>>0.32641 0.32641
>>
>>	  The last line combined with help for intervals.lme shows that the 
>>confidence interval for sigma (and doubtless also for lot and wafer 
>>variance components is based on a normal approximation for the 
>>distribution of log(sigma).
>>
>>	  The state of the art is reflected in "lmer" in library(lme4), 
>>described in the following:
>>
>>	  Doug Bates (2005) "Fitting linear mixed models in R" in R News 5/1 
>>available from "www.r-project.org" -> Newsletter
>>
>>	  However, an "intervals" function is not yet available for "lmer" 
>>objects.
> 
> 
> The issue of variance components in lme4 was recently by D. Bates:
> 
> http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html
> 
> Also, a colleague wrote (in French) a nice report - with data and R code 
> on how to compute variance components, their bias and distribution with 
> lme (normal approximation , Monte Carlo simulation and delta method). 
> Let me know if you want it.
> 
> Best,
> 
> Renaud
> 
> 
> 
>>	
>>	  spencer graves
>>
>>John Sorkin wrote:
>>
>>
>>
>>>Could someone identify a function that I might use to perform a
>>>components of variance analysis? In addition to the variance
>>>attributable to each factor, I would also like to obtain the SE of the
>>>variances.
>>>Thank you,
>>>John
>>>
>>>John Sorkin M.D., Ph.D.
>>>Chief, Biostatistics and Informatics
>>>Baltimore VA Medical Center GRECC and
>>>University of Maryland School of Medicine Claude Pepper OAIC
>>>
>>>University of Maryland School of Medicine
>>>Division of Gerontology
>>>Baltimore VA Medical Center
>>>10 North Greene Street
>>>GRECC (BT/18/GR)
>>>Baltimore, MD 21201-1524
>>>
>>>410-605-7119 
>>>---- NOTE NEW EMAIL ADDRESS:
>>>jsorkin at grecc.umaryland.edu
>>>
>>>	[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Sun Jun 26 18:23:05 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 26 Jun 2005 12:23:05 -0400
Subject: [R] chisq.test using amalgamation automatically (possible ?!?)
In-Reply-To: <20050626100015.97251.qmail@web52602.mail.yahoo.com>
References: <20050626100015.97251.qmail@web52602.mail.yahoo.com>
Message-ID: <971536df05062609234078dbfe@mail.gmail.com>

On 6/26/05, Mohammad Ehsanul Karim <wildscop at yahoo.com> wrote:
> Dear List,
> 
> 
> If any of observed and/or expected data has less than
> 5 frequencies, then  chisq.test (Pearson's Chi-squared
> Test for Count Data from package:stats) gives warning
> messages. For example,
> 
> x<-c(10, 14, 10, 11, 11, 7, 8, 4, 1, 4, 4, 2, 1, 1, 2,
> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)
> y<-c(9.13112391745095, 13.1626482033341,
> 12.6623267638188, 11.0130706413029, 9.16415925139016,
> 7.47441794889028, 6.03743388141852, 4.85350508692505,
> 3.89248001363859, 3.11803140037476, 2.49617540962629,
> 1.99774139023269, 1.5985926374167, 1.27909653584089,
> 1.02341602646530, 0.818828097315106,
> 0.655132353196336, 0.524159229418155,
> 0.418022824890164, 0.335528136508225,
> 0.268448671671046, 0.214779801990545,
> 0.171840507806838, 0.137485729582785,
> 0.109999238967747, 0.0880079144684513,
> 0.070413150156564)
> 
> Chi.Sq<-sum((c(x[1:7], sum(x[8:9]), sum(x[10:11]),
> sum(x[12:27]))-c(y[1:7], sum(y[8:9]), sum(y[10:11]),
> sum(y[12:27])))^2/c(y[1:7], sum(y[8:9]),
> sum(y[10:11]), sum(y[12:27]))) # using amalgamation
> pchisq(Chi.Sq, df=9, ncp=0, lower.tail = FALSE, log.p
> = FALSE) # result being 0.8830207
> 
> but chisq.test(x,y) gives the following output with
> incorrect df:
> 
>        Pearson's Chi-squared test
> 
> data:  x and y
> X-squared = 216, df = 208, p-value = 0.3373
> 
> Warning message:
> Chi-squared approximation may be incorrect in:
> chisq.test(x, y)
> 
> 
> 
> Is there any way that we can use directly chisq.test
> without having warning message in such cases (that is,
> using amalgamation conveniently so that we don't have
> to check each elements if they are less than 5 or not
> - the whole process being automatic, may be by means
> of programming)?
> 
> 
> 
> Any hint, help, support, references will be highly
> appreciated.
> Thank you for your time.
> 

Check out ?combine.levels in package Hmisc.
Also, in the chisq.test call above perhaps you meant this:
  chisq.test(x,p=y/sum(y))



From spencer.graves at pdf.com  Sun Jun 26 19:56:06 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 10:56:06 -0700
Subject: [R] better code?
In-Reply-To: <BAY105-F139BB1067DDC777D3E8A26D6EF0@phx.gbl>
References: <BAY105-F139BB1067DDC777D3E8A26D6EF0@phx.gbl>
Message-ID: <42BEEC36.1020805@pdf.com>

Hi, Laura:

	  How about the following:

 > sapply(x1.df, class)
         d         x         y
  "factor" "numeric" "numeric"
 > (byLH <- by(x1.df[-1], x1.df$d, var))
x1.df$d: 1/1/2005
           x         y
x  1.272688 -0.783719
y -0.783719  0.884866
-------------------------------------------------------------
x1.df$d: 1/15/2005
           x         y
x 0.9992097 0.8719848
y 0.8719848 0.7753879
-------------------------------------------------------------
x1.df$d: 1/22/2005
            x          y
x  1.2269211 -0.8694323
y -0.8694323  0.6161053
-------------------------------------------------------------
x1.df$d: 1/29/2005
             x           y
x 0.072721473 0.006551483
y 0.006551483 1.003923812
-------------------------------------------------------------
x1.df$d: 1/8/2005
           x         y
x 1.4893139 0.2845754
y 0.2845754 0.6273839

	  spencer graves

Laura Holt wrote:

> Hi R!
> 
> I have a data.frame with dates in the first column and numeric values in 
> columns 2 -3.
> 
> I want to have a covariance matrix for each date.  The following code 
> works fine:
> 
>> x1.df
> 
>          d           x          y
> 1  01/01/05  1.06014788 0.72595670
> 2  01/01/05 -1.56330741 2.44930531
> 3  01/01/05 -0.58001696 0.48626682
> 4  01/01/05  0.27653308 0.52676239
> 5  01/08/05  1.91849382 1.72136239
> 6  01/08/05 -0.74774661 0.16657346
> 7  01/08/05  0.18505727 1.33570129
> 8  01/08/05  1.47015895 1.59666054
> 9  01/08/05 -0.67119562 2.31980255
> 10 01/15/05  0.39728456 2.48849586
> 11 01/15/05 -0.96484152 1.47565372
> 12 01/15/05 -1.55109398 0.73436620
> 13 01/22/05 -1.36172373 1.12015635
> 14 01/22/05  0.20475072 0.01010656
> 15 01/29/05 -0.49909855 0.15583279
> 16 01/29/05 -0.07834782 0.99454434
> 17 01/29/05  0.12845272 0.09605443
> 18 01/29/05 -0.44926053 1.48959860
> 19 01/29/05 -0.07033900 2.50253296
> 
>> for(i in 1:5) {
> 
> + yy <- x1.df$d == u3[i]
> + zz <- cov(x1.df[yy,2:3])
> + print(dates(u3[i]))
> + prmatrix(zz)
> + }
> [1] 01/01/05
>          x         y
> x  1.272688 -0.783719
> y -0.783719  0.884866
> [1] 01/08/05
>          x         y
> x 1.4893139 0.2845754
> y 0.2845754 0.6273839
> [1] 01/15/05
>          x         y
> x 0.9992097 0.8719848
> y 0.8719848 0.7753879
> [1] 01/22/05
>           x          y
> x  1.2269211 -0.8694323
> y -0.8694323  0.6161053
> [1] 01/29/05
>            x           y
> x 0.072721473 0.006551485
> y 0.006551485 1.003923812
> 
>>
> However, I'm sure that there is a MUCH better way to accomplish this task.
> 
> Any suggestions, please?
> 
> R V 2.1.0 Windows
> 
> Thank you in advance!
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> _________________________________________________________________
> On the road to retirement? Check out MSN Life Events for advice on how 
> to get there! http://lifeevents.msn.com/category.aspx?cid=Retirement
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jun 26 20:06:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 11:06:47 -0700
Subject: [R] (sans objet)
In-Reply-To: <2CEEDF52-C795-46EC-B066-CA91BAD3E909@wanadoo.fr>
References: <2CEEDF52-C795-46EC-B066-CA91BAD3E909@wanadoo.fr>
Message-ID: <42BEEEB7.4030006@pdf.com>

	  Have you studied "An Introduction to R" [available via help.start()]? 
  Note especially the sections on "Factors", "Arrays and matrices", and 
"Lists and data frames".  There are also a couple of French-language 
introductions to R available via  "www.r-project.org" -> Documentation: 
  Manuals -> "contributed documentation" -> French ... .  However, I 
have not studied them, so I can't comment further on their content.

	  spencer graves

pir2.jv wrote:

> Ma premi??re question: puis-je ??crire en fran??ais, mon anglais est  
> pire que pire. J'essaie en fran??ais; sinon, j'essaierai une traduction!
> 
> Je sduis d??butant. Ma question est donc simple, j'imagine.
> 
> Je travaille sur des tableaux de statistiques. Je prends un exemple:
> J'ai un "frame" que j'appelle "eurostat" qui me donne des  
> statistiques commerciales:
> 
> eurostat: Pays    Produit    Tonnage
>              CI    801        10123
> .......
> 
> 
> J'ai un autre frame , cp qui m'indique
> Produit    Nom
> 801        Coconut
> ....
> et un autre qui m'indique que CI est "C??te d'Ivoire"
> Je veux cr??er une nouvelle table, par exemple "Importation" qui me  
> donne les donn??es suivantes:
> 
> Importation    Pays              Produit     Tonnage
>                  C??te d'Ivoire    Coconut    10123
> 
> ... et je n'y arrive pas. Ce doit pourtant ??tre basique.
> 
> Merci pour votre aide.
> 
> Jacques Vernin
> PiR2
> 10, Boulevard de Brazza
> 13008 Marseille
> 0491 734 736
> pir2.jv at wanadoo.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Sun Jun 26 20:11:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 11:11:52 -0700
Subject: [R] optimization problem in R ... can this be done?
In-Reply-To: <20050626175432.95232.qmail@web31201.mail.mud.yahoo.com>
References: <20050626175432.95232.qmail@web31201.mail.mud.yahoo.com>
Message-ID: <42BEEFE8.6060808@pdf.com>

	  The precision is not a problem, only the display, as Uwe indicated. 
Consider the following:

 > (seq(25.5,25.6,length=200000)-25.5)[c(1, 2, 199999, 200000)]
[1] 0.000000e+00 5.000025e-07 9.999950e-02 1.000000e-01
 > ?options
 > options(digits=20)
 > seq(25.5,25.6,length=200000)[c(1, 2, 199999, 200000)]
[1] 25.5000000000000 25.5000005000025 25.5999994999975 25.6000000000000
 >
	  spencer graves

Gregory Gentlemen wrote:

> Okay let me attempt to be clear:
> if i construct the following sequence in R:
>  
> seq(25.5,25.6,length=200000)
>  
> For instance, the last 10 elements of the sequence are all 26, and the 
> preceding 20 are all 25.59999. Presumably some rounding up is being
> done. How do I adjust the precision here such that each element is distinct?
>  
> Thanks in advance guys,
> Gregory
> gregory_gentlemen at yahoo.ca <mailto:gregory_gentlemen at yahoo.ca>
> 
> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>     Gregory Gentlemen wrote:
> 
>      > Spencer: Thank you for the helpful suggestions. I have another
>      > question following some code I wrote. The function below gives a
>      > crude approximation for the x of interest (that value of x such that
>      > g(x,n) is less than 0 for all n).
>      >
>      > # // btilda optimize g(n,x) for some fixed x, and then approximately
>      > finds that g(n,x) # such that abs(g(n*,x)=0 // btilda <-
>      > function(range,len) { # range: over which to look for x bb <-
>      > seq(range[1],range[2],length=len) OBJ <- sapply(bb,function(x) {fixed
>      > <- c(x,1000000,692,500000,1600,v1,217227);
>      >
>     return(optimize(g,c(1,1000),maximum=TRUE,tol=0.0000001,x=fixed)$objective)})
>      > tt <- data.frame(b=bb,obj=OBJ) tt$absobj <- abs(tt$obj) d <-
>      > tt[order(tt$absobj),][1:3,] return(as.vector(d)) }
>      >
>     ! > For instance a run of
>      >
>      >> btilda(c(20.55806,20.55816),10000) .... returns:
>      >
>      > b obj absobj 5834 20.55812
>      > -0.0004942848 0.0004942848 5833 20.55812 0.0011715433
>      > 0.0011715433 5835 20.55812 -0.0021601140 0.0021601140
>      >
>      > My question is how to improve the precision of b (which is x) here.
>      > It seems that seq(20.55806,20.55816,length=10000 ) is only precise to
>      > 5 or so digits, and thus, is equivalent for numerous succesive
> 
>     Why do you think so? It is much more accurate! See ?.Machine
> 
>     Uwe Ligges
> 
> 
> 
>      > values. How can I get around this?
>      >
>      >
>      > Spencer Graves wrote: Part of the R culture
>      > is a statement by Simon Blomberg immortalized in library(fortunes)
>      > as, "This is R. There is no if. Only how."
>      >
>      > I can't see now how I would automate a complete solution to your
>      > problem in general. However, given a specific ! g(x, n), I would
>     start
>      > by writing a function to use "expand.grid" and "contour" to make a
>      > contour plot of g(x, n) over specified ranges for x = seq(0, x.max,
>      > length=npts) and n = seq(0, n.max, npts) for a specified number of
>      > points npts. Then I'd play with x.max, n.max, and npts until I got
>      > what I wanted. With the right choices for x.max, n.max, and npts, the
>      > solution will be obvious from the plot. In some cases, nothing more
>      > will be required.
>      >
>      > If I wanted more than that, I would need to exploit further some
>      > specifics of the problem. For that, permit me to restate some of what
>      > I think I understood of your specific problem:
>      >
>      > (1) For fixed n, g(x, n) is monotonically decreasing in x>0.
>      >
>      > (2) For fixed x, g(x, n) has only two local maxima, one at n=0 (or
>      > n=eps>0, esp arbitrarily small) and the other at n2(x), say, with a
>      > local minimum in betwee! n at n1(x), say.
>      >
>      > With this, I would write functions to find n1(x) and n2(x) given x. I
>      > might not even need n1(x) if I could figure out how to obtain n2(x)
>      > without it. Then I'd make a plot with two lines (using "plot" and
>      > "lines") of g(x, 0) and g(x, n2(x)) vs. x.
>      >
>      > By the time I'd done all that, if I still needed more, I'd probably
>      > have ideas about what else to do.
>      >
>      > hope this helps. spencer graves
>      >
>      >
>      > Gregory Gentlemen wrote:
>      >
>      >
>      >> Im trying to ascertain whether or not the facilities of R are
>      >> sufficient for solving an optimization problem I've come accross.
>      >> Because of my limited experience with R, I would greatly appreciate
>      >> some feedback from more frequent users. The problem can be
>      >> delineated as such:
>      >>
>      >> A utility function, we shall call g is a function of x, n ...
>      >> g(x,n)! . g has the properties: n > 0, x lies on the real line.
>     g may
>      >> take values along the real line. g is such that g(x,n)=g(-x,n). g
>      >> is a decreasing function of x for any n; for fixed x, g(x,n) is
>      >> smooth and intially decreases upon reaching an inflection point,
>      >> thereafter increasing until it reaches a maxima and then declinces
>      >> (neither concave nor convex).
>      >>
>      >> My optimization problem is to find the largest positive x such that
>      >> g(x,n) is less than zero for all n. In fact, because of the
>      >> symmetry of g around x, we need only consider x > 0. Such an x does
>      >> exists in this problem, and of course g obtains a maximum value of
>      >> 0 at some n for this value of x. my issue is writing some code to
>      >> systematically obtain this value.
>      >>
>      >> Is R capable of handling such a problem? (i.e. through some sort of
>      >> optimization fucntion, ! or some sort of grid search with the
>      >> relevant constraints)
>      >>
>      >> Any suggestions would be appreciated.
>      >>
>      >> Gregory Gentlemen gregory_gentlemen at yahoo.ca
>      >>
>      >>
>      >>
>      >> The following is a sketch of an optimization problem I need to
>      >> solve.
>      >>
>      >> __________________________________________________
>      >>
>      >>
>      >>
>      >> [[alternative HTML version deleted]]
>      >>
>      >> ______________________________________________
>      >> R-help at stat.math.ethz.ch mailing list
>      >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
>      >> posting guide! http://www.R-project.org/posting-guide.html
>      >
>      >
> 
> __________________________________________________
> Do You Yahoo!?
> Tired of spam? Yahoo! Mail has the best spam protection around
> http://mail.yahoo.com
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mi2kelgrum at yahoo.com  Sun Jun 26 20:36:14 2005
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sun, 26 Jun 2005 11:36:14 -0700 (PDT)
Subject: [R] Sweave with layout() and loop
Message-ID: <20050626183614.73552.qmail@web60221.mail.yahoo.com>

When I try the following code with the Windows
graphics window, a new window is opened for each
multiple of four images I produce.

par(layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE)),
mar = c(2, 3, 2, 3))

for (i in 1:n) {
 image(... )
 }

When I try to do the same with Sweave to produce a pdf
document, I only get one graphic with the first four
graphs.  How do I get the rest when n is greater than
four?

<<Plots, fig=TRUE, eps=FALSE, echo=FALSE,   
results=hide, width=6.8, height=9.8>>=

par(layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE)),
mar = c(2, 3, 2, 3))

for (i in 1:n) {
 image(... )
 }

Any ideas?
cheers,
Mikkel



From dmbates at gmail.com  Sun Jun 26 20:40:36 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Sun, 26 Jun 2005 13:40:36 -0500
Subject: [R] Components of variance
In-Reply-To: <42BED38C.1080201@pdf.com>
References: <s2bdb7d7.009@grecc.umaryland.edu> <42BE0F45.5010500@pdf.com>
	<42BE49D4.4000502@cirad.fr> <42BED38C.1080201@pdf.com>
Message-ID: <40e66e0b05062611407e169a27@mail.gmail.com>

On 6/26/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> Hi, Renaud:
> 
>           Thanks for the link to Doug Bates' recent comment on this.
> Unfortunately, the information contained therein is not enough for me to
> easily obtain all the required standard errors and write code to make
> confidence intervals.  To be more specific, I'm able to produce the same
> answers as before using lmer in place of lme as follows:
> 
> > library(lme4)
> > set.seed(2)
> > lot <- rep(LETTERS[1:3], each=9)
> > lot.e <- rep(rnorm(3), each=9)
> > wf <- paste(lot, rep(1:9, each=3))
> > wf.e <- rep(rnorm(9), each=3)
> > DF <- data.frame(lot=lot, wafer=wf,
> + Vt=lot.e+wf.e+rnorm(27))
> > (fitr <- lmer(Vt~1+(1|lot)+(1|wafer), DF))
> <snip>
> Random effects:
>   Groups   Name        Variance Std.Dev.
>   wafer    (Intercept) 0.96054  0.98007
>   lot      (Intercept) 1.51434  1.23058
>   Residual             1.34847  1.16124
> # of obs: 27, groups: wafer, 9; lot, 3
> 
> Fixed effects:
>              Estimate Std. Error DF t value Pr(>|t|)
> (Intercept)  0.60839    0.81329 26  0.7481   0.4611
> 
>           These numbers agree with what I got with lme.  I then followed your
> suggetion to "http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html".
> This suggested I try str(VarCorr(fitr)):
> 
> > vcFit <- VarCorr(fitr)
> > str(vcFit)
> Formal class 'VarCorr' [package "Matrix"] with 3 slots
>    ..@ scale   : num 1.16
>    ..@ reSumry :List of 2
>    .. ..$ wafer:Formal class 'corrmatrix' [package "Matrix"] with 2 slots
>    .. .. .. ..@ .Data : num [1, 1] 1
>    .. .. .. .. ..- attr(*, "dimnames")=List of 2
>    .. .. .. .. .. ..$ : chr "(Intercept)"
>    .. .. .. .. .. ..$ : chr "(Intercept)"
>    .. .. .. ..@ stdDev: Named num 0.844
>    .. .. .. .. ..- attr(*, "names")= chr "(Intercept)"
>    .. ..$ lot  :Formal class 'corrmatrix' [package "Matrix"] with 2 slots
>    .. .. .. ..@ .Data : num [1, 1] 1
>    .. .. .. .. ..- attr(*, "dimnames")=List of 2
>    .. .. .. .. .. ..$ : chr "(Intercept)"
>    .. .. .. .. .. ..$ : chr "(Intercept)"
>    .. .. .. ..@ stdDev: Named num 1.06
>    .. .. .. .. ..- attr(*, "names")= chr "(Intercept)"
>    ..@ useScale: logi TRUE
> >
> 
>           Unfortunately, I've been so far unable to translate this into
> confidence intervals for the variance components that seem to match
> intervals(lme(...)).  The closest I came was as follows:
> 
> > 1.23058*sqrt(exp(qnorm(c(0.025, 0.975))*vcFit at reSumry$lot at stdDev))
> [1] 0.4356044 3.4763815
> > 0.98007*sqrt(exp(qnorm(c(0.025, 0.975))*vcFit at reSumry$wafer at stdDev))
> [1] 0.4286029 2.2410887
> >
>           The discrepancy between these numbers and intervals(lme(...)) is 25%
> for "lot", which suggest that I'm doing something wrong.  Moreover, I
> don't know how to get a similar interval for "scale", and I don't know
> how to access the estimated variance components other than manually.
> 
>           Comments?
>           Thanks,
>           spencer graves
> p.s.  R 2.1.0 patched under Windows XP.
> 
> Renaud Lancelot wrote:
> 
> > Spencer Graves a ??crit :
> >
> >>        As far as I know, the best available is lme in library(nlme).  For
> >>more information, see the the following:
> >>
> >>        Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus
> >>(Springer)
> >>
> >>        Consider the following example:
> >>
> >> > set.seed(2)
> >> > lot <- rep(LETTERS[1:3], each=9)
> >> > lot.e <- rep(rnorm(3), each=9)
> >> > wf <- paste(lot, rep(1:9, each=3))
> >> > wf.e <- rep(rnorm(9), each=3)
> >> > DF <- data.frame(lot=lot, wafer=wf,
> >>+ Vt=lot.e+wf.e+rnorm(27))
> >> > (fit <- lme(Vt~1, random=~1|lot/wafer, DF))
> >>Linear mixed-effects model fit by REML
> >>   Data: DF
> >>   Log-restricted-likelihood: -48.44022
> >>   Fixed: Vt ~ 1
> >>(Intercept)
> >>   0.6083933
> >>
> >>Random effects:
> >>  Formula: ~1 | lot
> >>         (Intercept)
> >>StdDev:    1.230572
> >>
> >>  Formula: ~1 | wafer %in% lot
> >>         (Intercept) Residual
> >>StdDev:   0.9801403 1.161218
> >>
> >>Number of Observations: 27
> >>Number of Groups:
> >>            lot wafer %in% lot
> >>              3              9
> >> > (CI.fit <- intervals(fit))
> >>Approximate 95% confidence intervals
> >>
> >>  Fixed effects:
> >>                 lower      est.    upper
> >>(Intercept) -1.100281 0.6083933 2.317068
> >>attr(,"label")
> >>[1] "Fixed effects:"
> >>
> >>  Random Effects:
> >>   Level: lot
> >>                     lower     est.    upper
> >>sd((Intercept)) 0.3368174 1.230572 4.495931
> >>   Level: wafer
> >>                    lower      est.    upper
> >>sd((Intercept)) 0.426171 0.9801403 2.254201
> >>
> >>  Within-group standard error:
> >>     lower      est.     upper
> >>0.8378296 1.1612179 1.6094289
> >> > str(CI.fit)
> >>List of 3
> >>  $ fixed   : num [1, 1:3] -1.100  0.608  2.317
> >>   ..- attr(*, "dimnames")=List of 2
> >>   .. ..$ : chr "(Intercept)"
> >>   .. ..$ : chr [1:3] "lower" "est." "upper"
> >>   ..- attr(*, "label")= chr "Fixed effects:"
> >>  $ reStruct:List of 2
> >>   ..$ lot  :`data.frame':       1 obs. of  3 variables:
> >>   .. ..$ lower: num 0.337
> >>   .. ..$ est. : num 1.23
> >>   .. ..$ upper: num 4.5
> >>   ..$ wafer:`data.frame':       1 obs. of  3 variables:
> >>   .. ..$ lower: num 0.426
> >>   .. ..$ est. : num 0.98
> >>   .. ..$ upper: num 2.25
> >>   ..- attr(*, "label")= chr "Random Effects:"
> >>  $ sigma   : atomic [1:3] 0.838 1.161 1.609
> >>   ..- attr(*, "label")= chr "Within-group standard error:"
> >>  - attr(*, "level")= num 0.95
> >>  - attr(*, "class")= chr "intervals.lme"
> >> > diff(log(CI.fit$sigma))
> >>    est.   upper
> >>0.32641 0.32641
> >>
> >>        The last line combined with help for intervals.lme shows that the
> >>confidence interval for sigma (and doubtless also for lot and wafer
> >>variance components is based on a normal approximation for the
> >>distribution of log(sigma).
> >>
> >>        The state of the art is reflected in "lmer" in library(lme4),
> >>described in the following:
> >>
> >>        Doug Bates (2005) "Fitting linear mixed models in R" in R News 5/1
> >>available from "www.r-project.org" -> Newsletter
> >>
> >>        However, an "intervals" function is not yet available for "lmer"
> >>objects.
> >
> >
> > The issue of variance components in lme4 was recently by D. Bates:
> >
> > http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html
> >
> > Also, a colleague wrote (in French) a nice report - with data and R code
> > on how to compute variance components, their bias and distribution with
> > lme (normal approximation , Monte Carlo simulation and delta method).
> > Let me know if you want it.
> >
> > Best,
> >
> > Renaud
> >
> >
> >
> >>
> >>        spencer graves
> >>
> >>John Sorkin wrote:
> >>
> >>
> >>
> >>>Could someone identify a function that I might use to perform a
> >>>components of variance analysis? In addition to the variance
> >>>attributable to each factor, I would also like to obtain the SE of the
> >>>variances.
> >>>Thank you,
> >>>John
> >>>
> >>>John Sorkin M.D., Ph.D.
> >>>Chief, Biostatistics and Informatics
> >>>Baltimore VA Medical Center GRECC and
> >>>University of Maryland School of Medicine Claude Pepper OAIC
> >>>
> >>>University of Maryland School of Medicine
> >>>Division of Gerontology
> >>>Baltimore VA Medical Center
> >>>10 North Greene Street
> >>>GRECC (BT/18/GR)
> >>>Baltimore, MD 21201-1524
> >>>
> >>>410-605-7119
> >>>----- NOTE NEW EMAIL ADDRESS:
> >>>jsorkin at grecc.umaryland.edu

Current versions of lmer do not return a Hessian matrix from the
optimization step so it would not be easy to create such intervals.  I
do have an analytic gradient from which a Hessian could be derived by
finite differences but even that wouldn't help without documentation
on how the profiled deviance is defined and the particular
parameterization used for the variance components.  These are not
arbitrary - they are the results of a considerable amount of research
and experimentation but I have not yet written it up.  It is not
overly complicated but it is also not entirely straightforward.

However, I do not regard getting standard errors of the estimates of
variance components as being important.  In fact I think I am doing a
service to the community by *not* providing them because they are
inevitably misused.

Think back to your first course in statistics when confidence
intervals were introduced.  You learned that a confidence interval on
the mean of a population (normally distributed or reasonably close to
normally distributed) is derived as the sample mean plus or minus a
multiple of the standard error of the mean.  What about a confidence
interval on the variance of such a population?  If you calculated that
as the estimate of the variance plus or minus a multiple of the
standard error of that estimate then you probably lost points on that
part of the assignment or exam because we don't calculate a confidence
interval in that way.  We know that the distribution of S^2 is a
scaled chi-squared distribution and often very far from symmetric and
a confidence interval constructed in the manner described above would
have poor properties and would not reflect the extent of our
uncertainty about the parameter.  And we never discussed at all a
hypothesis test on whether the population variance was zero.

Now let's go to a much more complicated situation with mixed effects
models.  We know that in the simplest possible case it would be
extremely misleading to create confidence intervals or perform
hypothesis tests based on an estimate of a variance and a standard
error of that estimate but we are perfectly happy to do so in the much
more complicated situation of variance components or, even worse,
estimates of covariances.  It doesn't make sense.  Even though SAS
PROC MIXED and HLM and MLWiN and Stata and ... all produce such tests
and such confidence intervals for mixed-effects models they still
don't make sense.  There is no good justification for their use other
than a vague hand-waving about asymptotics.

Until someone can explain to me a good use of quantities such as
standard errors of estimates of variances or covariances, I'm not
going to stop working on all of the other aspects of the code and
documentation and redirect my effort to producing these.  If anyone
wants to do it they can contact me off-list and I'll explain the
optimization problem and how to get access to the parameterization and
the gradient.  If you are really energetic I can point you to the
formulae for an analytic Hessian that I haven't yet had time to
implement.



From spencer.graves at pdf.com  Sun Jun 26 21:18:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 12:18:30 -0700
Subject: [R] Components of variance
In-Reply-To: <40e66e0b05062611407e169a27@mail.gmail.com>
References: <s2bdb7d7.009@grecc.umaryland.edu>
	<42BE0F45.5010500@pdf.com>	<42BE49D4.4000502@cirad.fr>
	<42BED38C.1080201@pdf.com>
	<40e66e0b05062611407e169a27@mail.gmail.com>
Message-ID: <42BEFF86.9090405@pdf.com>

Hi, Doug:

	  Thanks for your reply.  I'm keenly aware of many of the issues you've 
mentioned.  (I've learned much of what I know about this from your 
innovation research.)

	  Besides, "lme" provides one answer to that question, if not a great 
one.  Moreover, if someone is really concerned, couldn't they use 
"simulate.lme" in library(nlme), as you did in ch. 4(?) of Pinheiro and 
Bates (2000) Mixed-Effects Models in S and S-Plus (Springer) [Or they 
can request the French-language report that Renaud mentioned].

	  I only tried VarCorr(lmer(...)), because the email from you that 
Renaud cided sounded to me like it might be doable.  If I really wanted 
it, I could read your code.  I don't want it that badly.

	  Thanks again,
	  spencer graves

Douglas Bates wrote:

> On 6/26/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>Hi, Renaud:
>>
>>          Thanks for the link to Doug Bates' recent comment on this.
>>Unfortunately, the information contained therein is not enough for me to
>>easily obtain all the required standard errors and write code to make
>>confidence intervals.  To be more specific, I'm able to produce the same
>>answers as before using lmer in place of lme as follows:
>>
>>
>>>library(lme4)
>>>set.seed(2)
>>>lot <- rep(LETTERS[1:3], each=9)
>>>lot.e <- rep(rnorm(3), each=9)
>>>wf <- paste(lot, rep(1:9, each=3))
>>>wf.e <- rep(rnorm(9), each=3)
>>>DF <- data.frame(lot=lot, wafer=wf,
>>
>>+ Vt=lot.e+wf.e+rnorm(27))
>>
>>>(fitr <- lmer(Vt~1+(1|lot)+(1|wafer), DF))
>>
>><snip>
>>Random effects:
>>  Groups   Name        Variance Std.Dev.
>>  wafer    (Intercept) 0.96054  0.98007
>>  lot      (Intercept) 1.51434  1.23058
>>  Residual             1.34847  1.16124
>># of obs: 27, groups: wafer, 9; lot, 3
>>
>>Fixed effects:
>>             Estimate Std. Error DF t value Pr(>|t|)
>>(Intercept)  0.60839    0.81329 26  0.7481   0.4611
>>
>>          These numbers agree with what I got with lme.  I then followed your
>>suggetion to "http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html".
>>This suggested I try str(VarCorr(fitr)):
>>
>>
>>>vcFit <- VarCorr(fitr)
>>>str(vcFit)
>>
>>Formal class 'VarCorr' [package "Matrix"] with 3 slots
>>   ..@ scale   : num 1.16
>>   ..@ reSumry :List of 2
>>   .. ..$ wafer:Formal class 'corrmatrix' [package "Matrix"] with 2 slots
>>   .. .. .. ..@ .Data : num [1, 1] 1
>>   .. .. .. .. ..- attr(*, "dimnames")=List of 2
>>   .. .. .. .. .. ..$ : chr "(Intercept)"
>>   .. .. .. .. .. ..$ : chr "(Intercept)"
>>   .. .. .. ..@ stdDev: Named num 0.844
>>   .. .. .. .. ..- attr(*, "names")= chr "(Intercept)"
>>   .. ..$ lot  :Formal class 'corrmatrix' [package "Matrix"] with 2 slots
>>   .. .. .. ..@ .Data : num [1, 1] 1
>>   .. .. .. .. ..- attr(*, "dimnames")=List of 2
>>   .. .. .. .. .. ..$ : chr "(Intercept)"
>>   .. .. .. .. .. ..$ : chr "(Intercept)"
>>   .. .. .. ..@ stdDev: Named num 1.06
>>   .. .. .. .. ..- attr(*, "names")= chr "(Intercept)"
>>   ..@ useScale: logi TRUE
>>
>>          Unfortunately, I've been so far unable to translate this into
>>confidence intervals for the variance components that seem to match
>>intervals(lme(...)).  The closest I came was as follows:
>>
>>
>>>1.23058*sqrt(exp(qnorm(c(0.025, 0.975))*vcFit at reSumry$lot at stdDev))
>>
>>[1] 0.4356044 3.4763815
>>
>>>0.98007*sqrt(exp(qnorm(c(0.025, 0.975))*vcFit at reSumry$wafer at stdDev))
>>
>>[1] 0.4286029 2.2410887
>>
>>          The discrepancy between these numbers and intervals(lme(...)) is 25%
>>for "lot", which suggest that I'm doing something wrong.  Moreover, I
>>don't know how to get a similar interval for "scale", and I don't know
>>how to access the estimated variance components other than manually.
>>
>>          Comments?
>>          Thanks,
>>          spencer graves
>>p.s.  R 2.1.0 patched under Windows XP.
>>
>>Renaud Lancelot wrote:
>>
>>
>>>Spencer Graves a ??crit :
>>>
>>>
>>>>       As far as I know, the best available is lme in library(nlme).  For
>>>>more information, see the the following:
>>>>
>>>>       Pinheiro and Bates (2000) Mixed-Effects Models in S and S-Plus
>>>>(Springer)
>>>>
>>>>       Consider the following example:
>>>>
>>>>
>>>>>set.seed(2)
>>>>>lot <- rep(LETTERS[1:3], each=9)
>>>>>lot.e <- rep(rnorm(3), each=9)
>>>>>wf <- paste(lot, rep(1:9, each=3))
>>>>>wf.e <- rep(rnorm(9), each=3)
>>>>>DF <- data.frame(lot=lot, wafer=wf,
>>>>
>>>>+ Vt=lot.e+wf.e+rnorm(27))
>>>>
>>>>>(fit <- lme(Vt~1, random=~1|lot/wafer, DF))
>>>>
>>>>Linear mixed-effects model fit by REML
>>>>  Data: DF
>>>>  Log-restricted-likelihood: -48.44022
>>>>  Fixed: Vt ~ 1
>>>>(Intercept)
>>>>  0.6083933
>>>>
>>>>Random effects:
>>>> Formula: ~1 | lot
>>>>        (Intercept)
>>>>StdDev:    1.230572
>>>>
>>>> Formula: ~1 | wafer %in% lot
>>>>        (Intercept) Residual
>>>>StdDev:   0.9801403 1.161218
>>>>
>>>>Number of Observations: 27
>>>>Number of Groups:
>>>>           lot wafer %in% lot
>>>>             3              9
>>>>
>>>>>(CI.fit <- intervals(fit))
>>>>
>>>>Approximate 95% confidence intervals
>>>>
>>>> Fixed effects:
>>>>                lower      est.    upper
>>>>(Intercept) -1.100281 0.6083933 2.317068
>>>>attr(,"label")
>>>>[1] "Fixed effects:"
>>>>
>>>> Random Effects:
>>>>  Level: lot
>>>>                    lower     est.    upper
>>>>sd((Intercept)) 0.3368174 1.230572 4.495931
>>>>  Level: wafer
>>>>                   lower      est.    upper
>>>>sd((Intercept)) 0.426171 0.9801403 2.254201
>>>>
>>>> Within-group standard error:
>>>>    lower      est.     upper
>>>>0.8378296 1.1612179 1.6094289
>>>>
>>>>>str(CI.fit)
>>>>
>>>>List of 3
>>>> $ fixed   : num [1, 1:3] -1.100  0.608  2.317
>>>>  ..- attr(*, "dimnames")=List of 2
>>>>  .. ..$ : chr "(Intercept)"
>>>>  .. ..$ : chr [1:3] "lower" "est." "upper"
>>>>  ..- attr(*, "label")= chr "Fixed effects:"
>>>> $ reStruct:List of 2
>>>>  ..$ lot  :`data.frame':       1 obs. of  3 variables:
>>>>  .. ..$ lower: num 0.337
>>>>  .. ..$ est. : num 1.23
>>>>  .. ..$ upper: num 4.5
>>>>  ..$ wafer:`data.frame':       1 obs. of  3 variables:
>>>>  .. ..$ lower: num 0.426
>>>>  .. ..$ est. : num 0.98
>>>>  .. ..$ upper: num 2.25
>>>>  ..- attr(*, "label")= chr "Random Effects:"
>>>> $ sigma   : atomic [1:3] 0.838 1.161 1.609
>>>>  ..- attr(*, "label")= chr "Within-group standard error:"
>>>> - attr(*, "level")= num 0.95
>>>> - attr(*, "class")= chr "intervals.lme"
>>>>
>>>>>diff(log(CI.fit$sigma))
>>>>
>>>>   est.   upper
>>>>0.32641 0.32641
>>>>
>>>>       The last line combined with help for intervals.lme shows that the
>>>>confidence interval for sigma (and doubtless also for lot and wafer
>>>>variance components is based on a normal approximation for the
>>>>distribution of log(sigma).
>>>>
>>>>       The state of the art is reflected in "lmer" in library(lme4),
>>>>described in the following:
>>>>
>>>>       Doug Bates (2005) "Fitting linear mixed models in R" in R News 5/1
>>>>available from "www.r-project.org" -> Newsletter
>>>>
>>>>       However, an "intervals" function is not yet available for "lmer"
>>>>objects.
>>>
>>>
>>>The issue of variance components in lme4 was recently by D. Bates:
>>>
>>>http://tolstoy.newcastle.edu.au/R/help/05/06/7291.html
>>>
>>>Also, a colleague wrote (in French) a nice report - with data and R code
>>>on how to compute variance components, their bias and distribution with
>>>lme (normal approximation , Monte Carlo simulation and delta method).
>>>Let me know if you want it.
>>>
>>>Best,
>>>
>>>Renaud
>>>
>>>
>>>
>>>
>>>>       spencer graves
>>>>
>>>>John Sorkin wrote:
>>>>
>>>>
>>>>
>>>>
>>>>>Could someone identify a function that I might use to perform a
>>>>>components of variance analysis? In addition to the variance
>>>>>attributable to each factor, I would also like to obtain the SE of the
>>>>>variances.
>>>>>Thank you,
>>>>>John
>>>>>
>>>>>John Sorkin M.D., Ph.D.
>>>>>Chief, Biostatistics and Informatics
>>>>>Baltimore VA Medical Center GRECC and
>>>>>University of Maryland School of Medicine Claude Pepper OAIC
>>>>>
>>>>>University of Maryland School of Medicine
>>>>>Division of Gerontology
>>>>>Baltimore VA Medical Center
>>>>>10 North Greene Street
>>>>>GRECC (BT/18/GR)
>>>>>Baltimore, MD 21201-1524
>>>>>
>>>>>410-605-7119
>>>>>------ NOTE NEW EMAIL ADDRESS:
>>>>>jsorkin at grecc.umaryland.edu
> 
> 
> Current versions of lmer do not return a Hessian matrix from the
> optimization step so it would not be easy to create such intervals.  I
> do have an analytic gradient from which a Hessian could be derived by
> finite differences but even that wouldn't help without documentation
> on how the profiled deviance is defined and the particular
> parameterization used for the variance components.  These are not
> arbitrary - they are the results of a considerable amount of research
> and experimentation but I have not yet written it up.  It is not
> overly complicated but it is also not entirely straightforward.
> 
> However, I do not regard getting standard errors of the estimates of
> variance components as being important.  In fact I think I am doing a
> service to the community by *not* providing them because they are
> inevitably misused.
> 
> Think back to your first course in statistics when confidence
> intervals were introduced.  You learned that a confidence interval on
> the mean of a population (normally distributed or reasonably close to
> normally distributed) is derived as the sample mean plus or minus a
> multiple of the standard error of the mean.  What about a confidence
> interval on the variance of such a population?  If you calculated that
> as the estimate of the variance plus or minus a multiple of the
> standard error of that estimate then you probably lost points on that
> part of the assignment or exam because we don't calculate a confidence
> interval in that way.  We know that the distribution of S^2 is a
> scaled chi-squared distribution and often very far from symmetric and
> a confidence interval constructed in the manner described above would
> have poor properties and would not reflect the extent of our
> uncertainty about the parameter.  And we never discussed at all a
> hypothesis test on whether the population variance was zero.
> 
> Now let's go to a much more complicated situation with mixed effects
> models.  We know that in the simplest possible case it would be
> extremely misleading to create confidence intervals or perform
> hypothesis tests based on an estimate of a variance and a standard
> error of that estimate but we are perfectly happy to do so in the much
> more complicated situation of variance components or, even worse,
> estimates of covariances.  It doesn't make sense.  Even though SAS
> PROC MIXED and HLM and MLWiN and Stata and ... all produce such tests
> and such confidence intervals for mixed-effects models they still
> don't make sense.  There is no good justification for their use other
> than a vague hand-waving about asymptotics.
> 
> Until someone can explain to me a good use of quantities such as
> standard errors of estimates of variances or covariances, I'm not
> going to stop working on all of the other aspects of the code and
> documentation and redirect my effort to producing these.  If anyone
> wants to do it they can contact me off-list and I'll explain the
> optimization problem and how to get access to the parameterization and
> the gradient.  If you are really energetic I can point you to the
> formulae for an analytic Hessian that I haven't yet had time to
> implement.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From mike.rstat at gmail.com  Sun Jun 26 21:52:30 2005
From: mike.rstat at gmail.com (Mike R)
Date: Sun, 26 Jun 2005 12:52:30 -0700
Subject: [R] dates and time formats
Message-ID: <27db823f0506261252114fd96b@mail.gmail.com>

Is there a way in R to globally setup a default format for dates for all, or
nearly all, or just "many" R date/time-related functions?

For example, dates in library(chron) expects, by default, MDY with
forward-slash delimiters, whereas my habit  and my data sets tend to 
be YMD with dash delimiters, as in 2005-06-24 

Currently, I use the following practice:

dates("2005-06-24", format="y-m-d")
 
Is there a recommended documentation page that thoroughly describes
all the possible "environment" settings in addition to some such "date" 
setting? 

TIA,
Mike

       _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    1.0              
year     2005             
month    04               
day      18               
language R



From ggrothendieck at gmail.com  Sun Jun 26 22:51:28 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 26 Jun 2005 16:51:28 -0400
Subject: [R] dates and time formats
In-Reply-To: <27db823f0506261252114fd96b@mail.gmail.com>
References: <27db823f0506261252114fd96b@mail.gmail.com>
Message-ID: <971536df050626135159b04231@mail.gmail.com>

On 6/26/05, Mike R <mike.rstat at gmail.com> wrote:
> Is there a way in R to globally setup a default format for dates for all, or
> nearly all, or just "many" R date/time-related functions?
> 
> For example, dates in library(chron) expects, by default, MDY with
> forward-slash delimiters, whereas my habit  and my data sets tend to
> be YMD with dash delimiters, as in 2005-06-24
> 
> Currently, I use the following practice:
> 
> dates("2005-06-24", format="y-m-d")
> 
> Is there a recommended documentation page that thoroughly describes
> all the possible "environment" settings in addition to some such "date"
> setting?

The Date class uses the yyyy-mm-dd format by default so you
could just use the Date class.  

The global variables used by chron do not include a default format but
the ones that do exist and other information on dates is discussed in 
the Help Desk article in R News 4/1.



From ripley at stats.ox.ac.uk  Mon Jun 27 00:28:04 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 26 Jun 2005 23:28:04 +0100 (BST)
Subject: [R] interpreting Weibull survival regression
In-Reply-To: <20050626135221.GA22893@stat.umu.se>
References: <83375937.1119612448@Lab26.DOMAIN.IE.PITT.EDU>
	<20050626135221.GA22893@stat.umu.se>
Message-ID: <Pine.LNX.4.61.0506262321180.27847@gannet.stats>

> Here gamma is the usual gamma function, see ?gamma. (I notice in the R
> documentation of the Weibull distribution that "E(X) = b Gamma(1+1/a)",
> which is an error; the G should be g (lowercase).)

It is not an error.  R's function gamma() is said to implement 
\eqn{\Gamma}{Gamma}: `see ?gamma'!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Jun 27 00:51:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 15:51:14 -0700
Subject: [R] better code?
In-Reply-To: <BAY105-F139BB1067DDC777D3E8A26D6EF0@phx.gbl>
References: <BAY105-F139BB1067DDC777D3E8A26D6EF0@phx.gbl>
Message-ID: <42BF3162.5070300@pdf.com>

	  OK.  How about the following:

R> x1.df$d. <- as.Date(as.character(x1.df$d), format="%m/%d/%Y")
R> sapply(x1.df, class)
         d         x         y        d.
  "factor" "numeric" "numeric"    "Date"
R>
R> (byLH <- by(x1.df[2:3], x1.df$d., var))
x1.df$d.: 2005-01-01
           x         y
x  1.272688 -0.783719
y -0.783719  0.884866
------------------------------------------------------------
x1.df$d.: 2005-01-08
           x         y
x 1.4893139 0.2845754
y 0.2845754 0.6273839
------------------------------------------------------------
x1.df$d.: 2005-01-15
           x         y
x 0.9992097 0.8719848
y 0.8719848 0.7753879
------------------------------------------------------------
x1.df$d.: 2005-01-22
            x          y
x  1.2269211 -0.8694323
y -0.8694323  0.6161053
------------------------------------------------------------
x1.df$d.: 2005-01-29
             x           y
x 0.072721473 0.006551483
y 0.006551483 1.003923812

spencer graves
##########################################
Hi again!

The solution is good except that the dates are in alpha and not date order.
But this is way better.

Thanks

############################
Hi, Laura:

	  How about the following:

> sapply(x1.df, class)
         d         x         y
  "factor" "numeric" "numeric"
> (byLH <- by(x1.df[-1], x1.df$d, var))
x1.df$d: 1/1/2005
           x         y
x  1.272688 -0.783719
y -0.783719  0.884866
-------------------------------------------------------------
x1.df$d: 1/15/2005
           x         y
x 0.9992097 0.8719848
y 0.8719848 0.7753879
-------------------------------------------------------------
x1.df$d: 1/22/2005
            x          y
x  1.2269211 -0.8694323
y -0.8694323  0.6161053
-------------------------------------------------------------
x1.df$d: 1/29/2005
             x           y
x 0.072721473 0.006551483
y 0.006551483 1.003923812
-------------------------------------------------------------
x1.df$d: 1/8/2005
           x         y
x 1.4893139 0.2845754
y 0.2845754 0.6273839

	  spencer graves

Laura Holt wrote:

> Hi R!
> 
> I have a data.frame with dates in the first column and numeric values in 
> columns 2 -3.
> 
> I want to have a covariance matrix for each date.  The following code 
> works fine:
> 
>> x1.df
> 
>          d           x          y
> 1  01/01/05  1.06014788 0.72595670
> 2  01/01/05 -1.56330741 2.44930531
> 3  01/01/05 -0.58001696 0.48626682
> 4  01/01/05  0.27653308 0.52676239
> 5  01/08/05  1.91849382 1.72136239
> 6  01/08/05 -0.74774661 0.16657346
> 7  01/08/05  0.18505727 1.33570129
> 8  01/08/05  1.47015895 1.59666054
> 9  01/08/05 -0.67119562 2.31980255
> 10 01/15/05  0.39728456 2.48849586
> 11 01/15/05 -0.96484152 1.47565372
> 12 01/15/05 -1.55109398 0.73436620
> 13 01/22/05 -1.36172373 1.12015635
> 14 01/22/05  0.20475072 0.01010656
> 15 01/29/05 -0.49909855 0.15583279
> 16 01/29/05 -0.07834782 0.99454434
> 17 01/29/05  0.12845272 0.09605443
> 18 01/29/05 -0.44926053 1.48959860
> 19 01/29/05 -0.07033900 2.50253296
> 
>> for(i in 1:5) {
> 
> + yy <- x1.df$d == u3[i]
> + zz <- cov(x1.df[yy,2:3])
> + print(dates(u3[i]))
> + prmatrix(zz)
> + }
> [1] 01/01/05
>          x         y
> x  1.272688 -0.783719
> y -0.783719  0.884866
> [1] 01/08/05
>          x         y
> x 1.4893139 0.2845754
> y 0.2845754 0.6273839
> [1] 01/15/05
>          x         y
> x 0.9992097 0.8719848
> y 0.8719848 0.7753879
> [1] 01/22/05
>           x          y
> x  1.2269211 -0.8694323
> y -0.8694323  0.6161053
> [1] 01/29/05
>            x           y
> x 0.072721473 0.006551485
> y 0.006551485 1.003923812
> 
>>
> However, I'm sure that there is a MUCH better way to accomplish this task.
> 
> Any suggestions, please?
> 
> R V 2.1.0 Windows
> 
> Thank you in advance!
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> _________________________________________________________________
> On the road to retirement? Check out MSN Life Events for advice on how 
> to get there! http://lifeevents.msn.com/category.aspx?cid=Retirement
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gtg757i at mail.gatech.edu  Mon Jun 27 04:10:14 2005
From: gtg757i at mail.gatech.edu (Tudor Bodea)
Date: Sun, 26 Jun 2005 22:10:14 -0400
Subject: [R] Convert a "by" list into a data frame
In-Reply-To: <mailman.7.1119780001.32591.r-help@stat.math.ethz.ch>
References: <mailman.7.1119780001.32591.r-help@stat.math.ethz.ch>
Message-ID: <1119838214.42bf60061a93f@webmail.mail.gatech.edu>

Dear useRs,

Is there a way to convert a list generated by "by" command into a data frame?
Or, more generally, are there any functions that operate on "by" lists?
My "by" list consists of numeric vectors of equal length and the objective is
to calculate the sum of the same index elements (i.e., column-wise sums). For
example, if the "by" list were

--------------------------
Data_DF$CASE: 1
       IVTT     COST
[1,]     1        2
--------------------------
Data_DF$CASE: 2
       IVTT     COST
[1,]     2        4
---------------------------

I would like to finally obtain a numeric vector:

result
[1]  1.5   3

Thank you.

--
Tudor Dan Bodea
Georgia Institute of Technology
School of Civil and Environmental Engineering



From ggrothendieck at gmail.com  Mon Jun 27 04:32:43 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 26 Jun 2005 22:32:43 -0400
Subject: [R] Convert a "by" list into a data frame
In-Reply-To: <1119838214.42bf60061a93f@webmail.mail.gatech.edu>
References: <mailman.7.1119780001.32591.r-help@stat.math.ethz.ch>
	<1119838214.42bf60061a93f@webmail.mail.gatech.edu>
Message-ID: <971536df05062619322ddaadd4@mail.gmail.com>

On 6/26/05, Tudor Bodea <gtg757i at mail.gatech.edu> wrote:
> Dear useRs,
> 
> Is there a way to convert a list generated by "by" command into a data frame?

Here is an example which creates a by object whose elements
are the first row of each Species of iris.  The next line converts
that to a data frame:

   by.object <- by(iris, iris$Species, head, 1)
   do.call("rbind", by.object)



From spencer.graves at pdf.com  Mon Jun 27 05:30:52 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 26 Jun 2005 20:30:52 -0700
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
Message-ID: <42BF72EC.4040204@pdf.com>

	  How can one convert back slashes to forward slashes, e.g, changing 
"c:\a\b" to "c:/a/b"?  I tried the following:

 > gsub("\\\\", "/", "c:\a\b")
[1] "c:\a\b"

	  Since this didn't work, I eliminated the replacement as a contributor 
as follows:

 > gsub("X", "/", "c:XaXb")
[1] "c:/a/b"

	  The following seems to illustrate the problem:

  strsplit("c:\a\b", "")
[[1]]
[1] "c"  ":"  "\a" "\b"

	  How can one automate this?

	  Thanks,
	  spencer graves
-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From gb at stat.umu.se  Mon Jun 27 05:35:36 2005
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Mon, 27 Jun 2005 05:35:36 +0200
Subject: [R] interpreting Weibull survival regression
In-Reply-To: <Pine.LNX.4.61.0506262321180.27847@gannet.stats>
References: <83375937.1119612448@Lab26.DOMAIN.IE.PITT.EDU>
	<20050626135221.GA22893@stat.umu.se>
	<Pine.LNX.4.61.0506262321180.27847@gannet.stats>
Message-ID: <20050627033536.GA15416@stat.umu.se>

On Sun, Jun 26, 2005 at 11:28:04PM +0100, Prof Brian Ripley wrote:
> >Here gamma is the usual gamma function, see ?gamma. (I notice in the R
> >documentation of the Weibull distribution that "E(X) = b Gamma(1+1/a)",
> >which is an error; the G should be g (lowercase).)
> 
> It is not an error.  R's function gamma() is said to implement 
> \eqn{\Gamma}{Gamma}: `see ?gamma'!

I see; it's supposed to be mathematics, not R code (of course, since a
'*' otherwise is missing!). But then the documentation is at least
somewhat inconsistent, because in the variance expression, 'gamma' is used.

> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-- 
G??ran Brostr??m                    tel: +46 90 786 5223
Professor and Head
Department of Statistics          fax: +46 90 786 6614
Ume?? University                   http://www.stat.umu.se/~goran.brostrom/
SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From edd at debian.org  Mon Jun 27 06:09:01 2005
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 26 Jun 2005 23:09:01 -0500
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42BF72EC.4040204@pdf.com>
References: <42BF72EC.4040204@pdf.com>
Message-ID: <17087.31709.399479.651692@basebud.nulle.part>


On 26 June 2005 at 20:30, Spencer Graves wrote:
| 	  How can one convert back slashes to forward slashes, e.g, changing 
| "c:\a\b" to "c:/a/b"?  I tried the following:
| 
|  > gsub("\\\\", "/", "c:\a\b")
| [1] "c:\a\b"

This does work, provided you remember that single backslashed "don't exist"
as e.g. \a is a character in itself. So use doubles are you should be fine:

> gsub("\\\\", "/", "c:\\a\\b")
[1] "c:/a/b"

Hth, Dirk

-- 
Statistics: The (futile) attempt to offer certainty about uncertainty.
         -- Roger Koenker, 'Dictionary of Received Ideas of Statistics'



From ggrothendieck at gmail.com  Mon Jun 27 07:23:02 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 27 Jun 2005 01:23:02 -0400
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <17087.31709.399479.651692@basebud.nulle.part>
References: <42BF72EC.4040204@pdf.com>
	<17087.31709.399479.651692@basebud.nulle.part>
Message-ID: <971536df050626222318376e24@mail.gmail.com>

On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
> 
> On 26 June 2005 at 20:30, Spencer Graves wrote:
> |         How can one convert back slashes to forward slashes, e.g, changing
> | "c:\a\b" to "c:/a/b"?  I tried the following:
> |
> |  > gsub("\\\\", "/", "c:\a\b")
> | [1] "c:\a\b"
> 
> This does work, provided you remember that single backslashed "don't exist"
> as e.g. \a is a character in itself. So use doubles are you should be fine:
> 
> > gsub("\\\\", "/", "c:\\a\\b")
> [1] "c:/a/b"
> 

Also, if one finds four backslashes confusing one can avoid the use 
of four via any of these:

gsub("[\\]", "/", "c:\\a\\b")
gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
chartr("\\", "/", "c:\\a\\b")



From szlevine at nana.co.il  Mon Jun 27 07:40:02 2005
From: szlevine at nana.co.il (Stephen)
Date: Mon, 27 Jun 2005 08:40:02 +0300
Subject: [R] Mixed model
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64E@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050627/6ac37a0e/attachment.pl

From buser at stat.math.ethz.ch  Mon Jun 27 08:44:30 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Mon, 27 Jun 2005 08:44:30 +0200
Subject: [R] is.all.equal
In-Reply-To: <f8e6ff0505062608571a13da36@mail.gmail.com>
References: <f8e6ff0505062608571a13da36@mail.gmail.com>
Message-ID: <17087.41038.303453.260691@stat.math.ethz.ch>

Hi

It is corrected in the newer R-version. The help page now says: 

'all.equal(x,y)' is a utility to compare R objects 'x' and 'y'
testing "near equality".  If they are different, comparison is
still made to some extent, and a report of the differences is
returned.    Don't use 'all.equal' directly in 'if'
expressions-either use 'isTRUE(all.equal(....))' or 'identical' if
appropriate.

My R version is:

> version

platform x86_64-unknown-linux-gnu
arch     x86_64                  
os       linux-gnu               
system   x86_64, linux-gnu       
status                           
major    2                       
minor    1.1                     
year     2005                    
month    06                      
day      20                      
language R                       

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


hadley wickham writes:
 > Hi,
 > 
 > The description of all.equal states "is.all.equal should be used for
 > programming, typically in if expressions. It is a simple wrapper using
 > identical as shown in the documentation there.", but is.all.equal is
 > not explicitly defined there (although there is a hint in the comments
 > that is.all.equal <- function(x,y) isTRUE(all.equal(x,y))).
 > 
 > Could the documentation be corrected? (or even better, how about
 > defining is.all.equal by default)
 > 
 > Thanks,
 > 
 > Hadley
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Jun 27 08:53:56 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 27 Jun 2005 08:53:56 +0200
Subject: [R] is.all.equal
In-Reply-To: <f8e6ff0505062608571a13da36@mail.gmail.com>
References: <f8e6ff0505062608571a13da36@mail.gmail.com>
Message-ID: <42BFA284.1040602@statistik.uni-dortmund.de>

hadley wickham wrote:

> Hi,
> 
> The description of all.equal states "is.all.equal should be used for
> programming, typically in if expressions. It is a simple wrapper using
> identical as shown in the documentation there.", but is.all.equal is
> not explicitly defined there (although there is a hint in the comments
> that is.all.equal <- function(x,y) isTRUE(all.equal(x,y))).

Where is this documented (including version of the source)?
My recent versions of ?all.equal do not mentio it (at least, I cannot 
find it).

Uwe Ligges

> Could the documentation be corrected? (or even better, how about
> defining is.all.equal by default)
> 
> Thanks,
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From subramanian.vivek at gmail.com  Mon Jun 27 09:18:14 2005
From: subramanian.vivek at gmail.com (Vivek Subramanian)
Date: Mon, 27 Jun 2005 12:48:14 +0530
Subject: [R] SVM parameters...
Message-ID: <20e69eb705062700184d38fc3e@mail.gmail.com>

hi,

i am really sorry to ask this on the list, but i havent been able to
find anything on this topic.

i would like to know how the various parameters in the svm function
call in library e1071 work. all the literature that i was able to find
on the internet have been on the mathematics and derivation of
equations of the SVM or some very specific examples that relate to
biostatistics. i have been unable to find a concise description of how
these parameters affect the model.
so far i have been using a brute force method, running all possible
permutations and combinations , but this is taking enormous amounts of
time.

i would be grateful for any help that you could provide. 

thanks and regards ,
vivek.



From ripley at stats.ox.ac.uk  Mon Jun 27 09:32:26 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Jun 2005 08:32:26 +0100 (BST)
Subject: [R] chisq.test using amalgamation automatically (possible ?!?)
In-Reply-To: <20050626100015.97251.qmail@web52602.mail.yahoo.com>
References: <20050626100015.97251.qmail@web52602.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0506261244560.17919@gannet.stats>

You have actually used chisq.test to test independence of the cross 
tabulation of x and y as factors, a table with 1 on the diagonal and 0 
elsewhere.  I doubt this was your intention, but unfortunately you have 
not told us your actual intention.

Perhaps you intended y to be the expected values, but as they do not have 
the same sum as x it is not clear what distribution is appropriate.
(The standard theory assumes that the total count was used in determining 
the expected values from supplying probabilities, which is why df=9 would 
be used with 10 categories.)

You can use the expected values _if known in advance_ to amalgamate 
categories, but in most uses of chisq.test they are not known in advance.
In any case, without some knowledge of the context, you cannot decide 
which categories should be merged: your choices are arbitrary unless the 
categories are ordered.  Suppose they applied to types of fruit?
If you know that, then certainly you can program R to do the amalgamation 
for you.

BTW, it is just confusing (at least to your readers) to supply the default 
values of arguments explicitly.  pchisq(Chi.sq, df=9) would suffice.


On Sun, 26 Jun 2005, Mohammad Ehsanul Karim wrote:

> Dear List,
>
>
> If any of observed and/or expected data has less than
> 5 frequencies, then  chisq.test (Pearson's Chi-squared
> Test for Count Data from package:stats) gives warning
> messages. For example,
>
> x<-c(10, 14, 10, 11, 11, 7, 8, 4, 1, 4, 4, 2, 1, 1, 2,
> 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1)
> y<-c(9.13112391745095, 13.1626482033341,
> 12.6623267638188, 11.0130706413029, 9.16415925139016,
> 7.47441794889028, 6.03743388141852, 4.85350508692505,
> 3.89248001363859, 3.11803140037476, 2.49617540962629,
> 1.99774139023269, 1.5985926374167, 1.27909653584089,
> 1.02341602646530, 0.818828097315106,
> 0.655132353196336, 0.524159229418155,
> 0.418022824890164, 0.335528136508225,
> 0.268448671671046, 0.214779801990545,
> 0.171840507806838, 0.137485729582785,
> 0.109999238967747, 0.0880079144684513,
> 0.070413150156564)
>
> Chi.Sq<-sum((c(x[1:7], sum(x[8:9]), sum(x[10:11]),
> sum(x[12:27]))-c(y[1:7], sum(y[8:9]), sum(y[10:11]),
> sum(y[12:27])))^2/c(y[1:7], sum(y[8:9]),
> sum(y[10:11]), sum(y[12:27]))) # using amalgamation
> pchisq(Chi.Sq, df=9, ncp=0, lower.tail = FALSE, log.p
> = FALSE) # result being 0.8830207
>
> but chisq.test(x,y) gives the following output with
> incorrect df:
>
>        Pearson's Chi-squared test
>
> data:  x and y
> X-squared = 216, df = 208, p-value = 0.3373
>
> Warning message:
> Chi-squared approximation may be incorrect in:
> chisq.test(x, y)
>
> Is there any way that we can use directly chisq.test
> without having warning message in such cases (that is,
> using amalgamation conveniently so that we don't have
> to check each elements if they are less than 5 or not
> - the whole process being automatic, may be by means
> of programming)?
>
> Any hint, help, support, references will be highly
> appreciated.
> Thank you for your time.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 27 09:59:44 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Jun 2005 08:59:44 +0100 (BST)
Subject: [R] interpreting Weibull survival regression
In-Reply-To: <20050627033536.GA15416@stat.umu.se>
References: <83375937.1119612448@Lab26.DOMAIN.IE.PITT.EDU>
	<20050626135221.GA22893@stat.umu.se>
	<Pine.LNX.4.61.0506262321180.27847@gannet.stats>
	<20050627033536.GA15416@stat.umu.se>
Message-ID: <Pine.LNX.4.61.0506270856440.4683@gannet.stats>

On Mon, 27 Jun 2005, G?ran Brostr?m wrote:

> On Sun, Jun 26, 2005 at 11:28:04PM +0100, Prof Brian Ripley wrote:
>>> Here gamma is the usual gamma function, see ?gamma. (I notice in the R
>>> documentation of the Weibull distribution that "E(X) = b Gamma(1+1/a)",
>>> which is an error; the G should be g (lowercase).)
>>
>> It is not an error.  R's function gamma() is said to implement
>> \eqn{\Gamma}{Gamma}: `see ?gamma'!
>
> I see; it's supposed to be mathematics, not R code (of course, since a
> '*' otherwise is missing!). But then the documentation is at least
> somewhat inconsistent, because in the variance expression, 'gamma' is used.

Yes, in the variance expression \Gamma is transliterated as 'gamma'.  That 
is the only case I can find (e.g. the Beta,Chiqsuare,Fdist,GammaDist,
NegBinomial,Tdist Rd files).  I've made the variance expression 
consistent.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From p.dalgaard at biostat.ku.dk  Mon Jun 27 11:17:03 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2005 11:17:03 +0200
Subject: [R] Convert a "by" list into a data frame
In-Reply-To: <971536df05062619322ddaadd4@mail.gmail.com>
References: <mailman.7.1119780001.32591.r-help@stat.math.ethz.ch>
	<1119838214.42bf60061a93f@webmail.mail.gatech.edu>
	<971536df05062619322ddaadd4@mail.gmail.com>
Message-ID: <x2y88wi2gg.fsf@turmalin.kubism.ku.dk>

Gabor Grothendieck <ggrothendieck at gmail.com> writes:

> On 6/26/05, Tudor Bodea <gtg757i at mail.gatech.edu> wrote:
> > Dear useRs,
> > 
> > Is there a way to convert a list generated by "by" command into a data frame?
> 
> Here is an example which creates a by object whose elements
> are the first row of each Species of iris.  The next line converts
> that to a data frame:
> 
>    by.object <- by(iris, iris$Species, head, 1)
>    do.call("rbind", by.object)

Also, check if aggregate() does the trick:

>  aggregate(iris, list(iris$Species), head, 1)
     Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1     setosa          5.1         3.5          1.4         0.2       1
2 versicolor          7.0         3.2          4.7         1.4       2
3  virginica          6.3         3.3          6.0         2.5       3


>    by.object <- by(iris, iris$Species, head, 1)
>    do.call("rbind", by.object)
           Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
setosa              5.1         3.5          1.4         0.2     setosa
versicolor          7.0         3.2          4.7         1.4 versicolor
virginica           6.3         3.3          6.0         2.5  virginica

But notice the differences...

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From marco.zucchelli at biosci.ki.se  Mon Jun 27 11:20:13 2005
From: marco.zucchelli at biosci.ki.se (Marco Zucchelli)
Date: Mon, 27 Jun 2005 11:20:13 +0200
Subject: [R] graphic window with tabs
Message-ID: <037901c57af9$6cc19b00$bf6eed82@pizero>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050627/5365199e/attachment.pl

From s0454869 at sms.ed.ac.uk  Mon Jun 27 11:48:28 2005
From: s0454869 at sms.ed.ac.uk (JEB Halliday)
Date: Mon, 27 Jun 2005 10:48:28 +0100
Subject: [R] delta-beta's
Message-ID: <1119865708.42bfcb6c43b22@sms.ed.ac.uk>


Hi there

I have created a multivariate logistic regression model looking at the
presence/absence of disease on farms.  I would like to plot the diagnostic
plots recommended by Hosmer & Lemeshow to look particularly for any points of
high influence.  In order to do this I need to extract values for delta-beta.
The function dfbeta gives a value for change in each coefficient but I am
looking for a composite value that gives an overall measure of change to the
coefficients.  It may be that dffits is this function but I cannot gather what
this is or what it is calculated from?

If anyone has any advice re plotting H&L logistic regression diagnostics in R
that would be greatly appreciated.
Also, can any of these functions be applied to glmmPQL mixed models?

Thanks very much
Jo Halliday



From med at aghmed.fsnet.co.uk  Mon Jun 27 12:29:34 2005
From: med at aghmed.fsnet.co.uk (Michael Dewey)
Date: Mon, 27 Jun 2005 11:29:34 +0100
Subject: [R] (sans objet)
In-Reply-To: <2CEEDF52-C795-46EC-B066-CA91BAD3E909@wanadoo.fr>
References: <2CEEDF52-C795-46EC-B066-CA91BAD3E909@wanadoo.fr>
Message-ID: <6.2.1.2.0.20050627112858.02908010@pop.freeserve.net>

At 11:12 26/06/05, pir2.jv wrote:
>Ma premi??re question: puis-je ??crire en fran??ais, mon anglais est
>pire que pire. J'essaie en fran??ais; sinon, j'essaierai une traduction!
>
>Je sduis d??butant. Ma question est donc simple, j'imagine.

I think ?merge is your friend here.

>Je travaille sur des tableaux de statistiques. Je prends un exemple:
>J'ai un "frame" que j'appelle "eurostat" qui me donne des
>statistiques commerciales:
>
>eurostat: Pays    Produit    Tonnage
>             CI    801        10123
>.......
>
>
>J'ai un autre frame , cp qui m'indique
>Produit    Nom
>801        Coconut
>....
>et un autre qui m'indique que CI est "C??te d'Ivoire"
>Je veux cr??er une nouvelle table, par exemple "Importation" qui me
>donne les donn??es suivantes:
>
>Importation    Pays              Produit     Tonnage
>                 C??te d'Ivoire    Coconut    10123
>
>... et je n'y arrive pas. Ce doit pourtant ??tre basique.
>
>Merci pour votre aide.
>
>Jacques Vernin
>PiR2
>10, Boulevard de Brazza
>13008 Marseille
>0491 734 736
>pir2.jv at wanadoo.fr
>
>

Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html



From murdoch at stats.uwo.ca  Mon Jun 27 12:43:25 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 27 Jun 2005 06:43:25 -0400
Subject: [R] graphic window with tabs
In-Reply-To: <037901c57af9$6cc19b00$bf6eed82@pizero>
References: <037901c57af9$6cc19b00$bf6eed82@pizero>
Message-ID: <42BFD84D.8030602@stats.uwo.ca>

Marco Zucchelli wrote:
> Hi,
> 
>  I need to display several plots and I wonder if it is possible to create a graphic window with tabs so that the plots can be scrolled clicking on the tabs

You don't say what platform you're working on.  On Windows, the standard 
graphics driver doesn't support tabs, but it does allow you to record a 
history, so PgUp and PgDn let you scroll between multiple plots.

Duncan Murdoch



From Friedrich.Leisch at tuwien.ac.at  Mon Jun 27 12:52:11 2005
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Mon, 27 Jun 2005 12:52:11 +0200
Subject: [R] Sweave with layout() and loop
In-Reply-To: <20050626183614.73552.qmail@web60221.mail.yahoo.com>
References: <20050626183614.73552.qmail@web60221.mail.yahoo.com>
Message-ID: <17087.55899.153233.722116@galadriel.ci.tuwien.ac.at>

>>>>> On Sun, 26 Jun 2005 11:36:14 -0700 (PDT),
>>>>> Mikkel Grum (MG) wrote:

  > When I try the following code with the Windows
  > graphics window, a new window is opened for each
  > multiple of four images I produce.

  > par(layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE)),
  > mar = c(2, 3, 2, 3))

  > for (i in 1:n) {
  >  image(... )
  >  }

  > When I try to do the same with Sweave to produce a pdf
  > document, I only get one graphic with the first four
  > graphs.  How do I get the rest when n is greater than
  > four?

  > <<Plots, fig=TRUE, eps=FALSE, echo=FALSE,   
  > results=hide, width=6.8, height=9.8>>=

  > par(layout(matrix(c(1,2,3,4), 2, 2, byrow = TRUE)),
  > mar = c(2, 3, 2, 3))

  > for (i in 1:n) {
  >  image(... )
  >  }

  > Any ideas?

FAQ A.8:

A.8  Creating several figures from one figure chunk does not work

Consider that you want to create several graphs in a loop similar to

<<fig=TRUE>>  
for (i in 1:4) plot(rnorm(100)+i)  
@

This will currently not work, because Sweave allows only one graph per
figure chunk. The simple reason is that Sweave opens a postscript
device before executing the code and closes it afterwards. If you need
to plot in a loop, you have to program it along the lines of

<<results=tex,echo=FALSE>>=  
for(i in 1:4){  
    file=paste("myfile", i, ".eps", sep="")  
    postscript(file=file, paper="special", width=6, height=6)  
    plot(rnorm(100)+i)  
    dev.off()  
    cat("\\includegraphics{", file, "}\n\n", sep="")  
}  
@

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From usenet at s-boehringer.de  Mon Jun 27 12:52:21 2005
From: usenet at s-boehringer.de (usenet@s-boehringer.de)
Date: Mon, 27 Jun 2005 12:52:21 +0200 (CEST)
Subject: [R] Reconstructing LD function
Message-ID: <56169.132.252.149.100.1119869541.squirrel@webmail.loomes.de>

Dear all,

in an LDA analysis with n groups n-1 LD functions result. Implicitly this
defines an LD fucntion for the last group. Does there exist code already
to explictly construct this LD function?

Thanks, Stefan



From ripley at stats.ox.ac.uk  Mon Jun 27 13:18:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Jun 2005 12:18:16 +0100 (BST)
Subject: [R] Reconstructing LD function
In-Reply-To: <56169.132.252.149.100.1119869541.squirrel@webmail.loomes.de>
References: <56169.132.252.149.100.1119869541.squirrel@webmail.loomes.de>
Message-ID: <Pine.LNX.4.61.0506271211140.7081@gannet.stats>

On Mon, 27 Jun 2005 usenet at s-boehringer.de wrote:

> in an LDA analysis with n groups n-1 LD functions result. Implicitly this
> defines an LD fucntion for the last group. Does there exist code already
> to explictly construct this LD function?

What `LDA analysis' are our discussing here?  (LDA is usually
`linear discriminant analysis', so what did you mean and what R function 
are you nor referring to?)

R has lda in package MASS, and that works with n LD functions.  To reduce 
it to n-1, subtract the last one from the others, in which case LD_n == 0.

Anything you do in LD analysis only depends on differences in LD
functions, and there really are n of them.  With two groups one is
conventionally taken to be zero (the first, usually, not the last).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From david.meyer at wu-wien.ac.at  Mon Jun 27 13:28:30 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Mon, 27 Jun 2005 13:28:30 +0200
Subject: [R]  SVM parameters...
Message-ID: <20050627132830.40e12d44.david.meyer@wu-wien.ac.at>


Vivek,

I certainly would agree that every help page, including the one of
svm(), could be improved, but I think it is not _that_ deficient. In
particular, it tells you which parameters are used in the various
kernels available.

Have you read the corresponding article in R News (basically contained
as a vignette in the package)?

In addition, you could have a look at the documentation of libsvm, the
library that is interfaced by the svm()-function in e1071.

Best,
David

----------------

hi,

i am really sorry to ask this on the list, but i havent been able to
find anything on this topic.

i would like to know how the various parameters in the svm function
call in library e1071 work. all the literature that i was able to find
on the internet have been on the mathematics and derivation of
equations of the SVM or some very specific examples that relate to
biostatistics. i have been unable to find a concise description of how
these parameters affect the model.
so far i have been using a brute force method, running all possible
permutations and combinations , but this is taking enormous amounts of
time.

i would be grateful for any help that you could provide. 

thanks and regards ,
vivek.



From saeedeh_m_d at yahoo.co.uk  Mon Jun 27 16:15:55 2005
From: saeedeh_m_d at yahoo.co.uk (saeedeh maleki)
Date: Mon, 27 Jun 2005 15:15:55 +0100 (BST)
Subject: [R] pca
Message-ID: <20050627141555.15217.qmail@web25505.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050627/0620c33b/attachment.pl

From f.harrell at vanderbilt.edu  Mon Jun 27 16:55:38 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 27 Jun 2005 09:55:38 -0500
Subject: [R] delta-beta's
In-Reply-To: <1119865708.42bfcb6c43b22@sms.ed.ac.uk>
References: <1119865708.42bfcb6c43b22@sms.ed.ac.uk>
Message-ID: <42C0136A.5000803@vanderbilt.edu>

JEB Halliday wrote:
> Hi there
> 
> I have created a multivariate logistic regression model looking at the
> presence/absence of disease on farms.  I would like to plot the diagnostic
> plots recommended by Hosmer & Lemeshow to look particularly for any points of
> high influence.  In order to do this I need to extract values for delta-beta.
> The function dfbeta gives a value for change in each coefficient but I am
> looking for a composite value that gives an overall measure of change to the
> coefficients.  It may be that dffits is this function but I cannot gather what
> this is or what it is calculated from?
> 
> If anyone has any advice re plotting H&L logistic regression diagnostics in R
> that would be greatly appreciated.

In the Design package do ?residuals.lrm

What to do about overly influential observations is a big question 
though.  You can bias the model by removing them.

Frank

> Also, can any of these functions be applied to glmmPQL mixed models?
> 
> Thanks very much
> Jo Halliday
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ftorrei2 at uiuc.edu  Mon Jun 27 16:55:47 2005
From: ftorrei2 at uiuc.edu (ftorrei2@uiuc.edu)
Date: Mon, 27 Jun 2005 09:55:47 -0500
Subject: [R] Clustered boxplots
Message-ID: <4b0acae0.13073ef1.830e600@expms2.cites.uiuc.edu>

Hello,

Is there a way to obtain clustered boxplots similar to the
ones provided by SPSS? I have a dependent variable (y axis),
an independent factor (which I intend to represent with
different colors, no help needed here) and a grouping factor
for the clusters (which I don??t know how to display) I would
like to leave a bigger space between the boxplots of each
cluster than between teh boxplots within each of them, and I
would also like to add labels below each cluster. Then I will
add a legend for the colors representing the independent
factor (this is something I can do, no help needed here). I
would appreciate any hint pointing me in the right direction.

Francisco   
Francisco Torreira
Spanish, Italian and Portuguese
Univ. of Illinois at Urbana-Champaign
707 South Mathews Aven.
4031 FLB
Urbana, IL, 61801



From hrust at pik-potsdam.de  Mon Jun 27 16:58:57 2005
From: hrust at pik-potsdam.de (Henning Rust)
Date: Mon, 27 Jun 2005 16:58:57 +0200
Subject: [R] problem with postcript() and umlauts
Message-ID: <42C01431.6020605@pik-potsdam.de>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello,
I am running R 2.01 on SuSE 9.2 and I use GNU-Emacs and ESS.
When using umlauts like ?? ?? ?? ?? for e.g. plot titles , I enter them
directly in the code, like

plot(seq(from=-4,to=4,length=100),dnorm(seq(from=-4,to=4,length=100)),main="Gau??-Verteilung")

I get a dotted box just after the "Gau" and before the "??" in the title.

Using the postscript device

postscript()
plot(seq(from=-4,to=4,length=100),dnorm(seq(from=-4,to=4,length=100)),main="Gau??-Verteilung")
dev.off()

I get two warning messages:

Warning messages:
1: font width unknown for character 142
2: font width unknown for character 142
and a space after the "Gau" and before the "??".

I'd like to get rid of the extra space and the box before umlauts.
Does anyone know how to solve the problem?

Thanks,
~      Henning

- --
Henning Rust
Potsdam Institute for Climate Impact Research
Dept. Integrated Systems Analysis
P.O. Box 60 12 03, D-14412 Potsdam, Germany
Tel.: #49/331/288-2596	
Fax.: #49/331/288-2640
http://www.pik-potsdam.de/~hrust
PGP : blackhole.pca.dfn.de (www.gnupg.org)

Please avoid sending me Word or PowerPoint attachments,
send plain text or PDF instead.
See http://www.fsf.org/philosophy/no-word-attachments.html


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.2-rc1-SuSE (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFCwBQxppqlS9kAiT4RAoNRAKCdIFcDVJb/mW3MeK6EXhChqqI1dgCgn0Fh
A76WZDlFBswox5uoblYkx7o=
=Bxb6
-----END PGP SIGNATURE-----



From tpcl at tci.ufal.br  Mon Jun 27 16:57:11 2005
From: tpcl at tci.ufal.br (Talita Perciano Costa Leite)
Date: Mon, 27 Jun 2005 11:57:11 -0300
Subject: [R] Numerical accuracy
Message-ID: <1119884231.42c013c7458e2@www.ufal.br>

Hi people,

I need to prove the good quality of numerical accuracy of R. Anyone knows a
paper or anything else comparing R to other statistical softwares in terms of
numerical accuracy.
I've made a long search about that but I found nothing. Please help me!!

Thanx,

Talita Leite

-------------------------------------------------
Este e-mail foi enviado pelo Webmail da UFAL
IMP: http://horde.org/imp/



From ccleland at optonline.net  Mon Jun 27 17:16:26 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 27 Jun 2005 11:16:26 -0400
Subject: [R] Clustered boxplots
In-Reply-To: <4b0acae0.13073ef1.830e600@expms2.cites.uiuc.edu>
References: <4b0acae0.13073ef1.830e600@expms2.cites.uiuc.edu>
Message-ID: <42C0184A.4010805@optonline.net>

You might consider bwplot() in the lattice package instead.  For example:

mydata <- data.frame(Y = rnorm(3*1000),
                      INDFACT =rep(c("A", "B", "C"), each=1000),
                      CLUSFACT=factor(rep(c("M","F"), 1500)))

library(lattice)
trellis.device(new=FALSE, col=FALSE)

bwplot(Y ~ INDFACT | CLUSFACT, data=mydata, layout=c(2,1))

ftorrei2 at uiuc.edu wrote:
> Hello,
> 
> Is there a way to obtain clustered boxplots similar to the
> ones provided by SPSS? I have a dependent variable (y axis),
> an independent factor (which I intend to represent with
> different colors, no help needed here) and a grouping factor
> for the clusters (which I don??t know how to display) I would
> like to leave a bigger space between the boxplots of each
> cluster than between teh boxplots within each of them, and I
> would also like to add labels below each cluster. Then I will
> add a legend for the colors representing the independent
> factor (this is something I can do, no help needed here). I
> would appreciate any hint pointing me in the right direction.
> 
> Francisco   
> Francisco Torreira
> Spanish, Italian and Portuguese
> Univ. of Illinois at Urbana-Champaign
> 707 South Mathews Aven.
> 4031 FLB
> Urbana, IL, 61801
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From JAROSLAW.W.TUSZYNSKI at saic.com  Mon Jun 27 17:47:48 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Mon, 27 Jun 2005 11:47:48 -0400
Subject: [R] Numerical accuracy
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40B1@us-arlington-0668.mail.saic.com>

I do not know about details of numerical accuracy of R, it is probable
related to numerical accuracy of C compiler that was used to compile it (gcc
?). 
However I would like to mention that I run into some problems with accuracy
of sum and cumsum functions. See:  
   >   x = c(1, 1e20, 1e40, -1e40, -1e20, -1)
   >   sum(x)
   [1] -1e+20
   >   cumsum(x)
   [1]  1e+00  1e+20  1e+40  0e+00 -1e+20 -1e+20
And wrote functions (with help of R-Help list) to address the problem. See
sum.exact and cumsum.exact in caTools package:
  >   sum.exact(x)
  [1] 0
  >   cumsum.exact(x)
  [1] 1e+00 1e+20 1e+40 1e+20 1e+00 0e+00
 
Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Talita Perciano Costa
Leite
Sent: Monday, June 27, 2005 10:57 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Numerical accuracy

Hi people,

I need to prove the good quality of numerical accuracy of R. Anyone knows a
paper or anything else comparing R to other statistical softwares in terms
of numerical accuracy.
I've made a long search about that but I found nothing. Please help me!!

Thanx,

Talita Leite

-------------------------------------------------
Este e-mail foi enviado pelo Webmail da UFAL
IMP: http://horde.org/imp/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From chris.knight at manchester.ac.uk  Mon Jun 27 18:33:44 2005
From: chris.knight at manchester.ac.uk (Chris Knight)
Date: Mon, 27 Jun 2005 17:33:44 +0100
Subject: [R] SSlogis problem with min(y)==0
Message-ID: <1119890024.7899.202.camel@localhost.localdomain>

Hi, I think this is a problem solved but I would be interested to know
if there is some good reason why SSlogis() behaves like this (apologies
if this has been noticed before- I'm not confident my archive searches
were effective):

I have been fitting large numbers of regressions using nls with a
self-starting 3 parameter logistic model (SSlogis()). I got a series of
unexpected errors of the sort:

model<-nls(y~SSlogis(x, Asym, xmid, scal))
Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
        NA/NaN/Inf in foreign function call (arg 1)

Investigation seemed to suggest that this occurs for versions of y where
the minimum value of y was precisely zero. and sure enough, the
following produces that error:

x<-c(1,2,3,4,5,6,7)
y<-c(0,0,1,2,3,4,4)
model<-nls(y~SSlogis(x, Asym, xmid, scal))

whereas the following succeed:

model<-nls(y-0.01~SSlogis(x, Asym, xmid, scal))
model<-nls(y+0.01~SSlogis(x, Asym, xmid, scal))

I don't claim to understand the source code, but it does contain the
lines

if (min(z) <= 0) {
        z <- z - 1.05 * min(z)
    }
    z <- z/(1.05 * max(z))
    xy[["z"]] <- log(z/(1 - z))
    aux <- coef(lm(x ~ z, xy))

Which would seem to explain it given that if the minimum of z is
precisely zero it will remain the same after z <- z - 1.05 * min(z) so
produce a -Inf for log(z/(1 - z)) going into the lm call and producing
the error. Changing the beginning of the above to:

if (min(z) < 0) {
        z <- z - 1.05 * min(z)
    }
    if (min(z) == 0) {
        z <- z + 0.01 * (max(z)-min(z))
    }

Seems to produce a function that does what I'm after, so I'm now happy,
though I haven't gone through to check other self-starting functions.

Chris



From ripley at stats.ox.ac.uk  Mon Jun 27 18:38:01 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Jun 2005 17:38:01 +0100 (BST)
Subject: [R] problem with postcript() and umlauts
In-Reply-To: <42C01431.6020605@pik-potsdam.de>
References: <42C01431.6020605@pik-potsdam.de>
Message-ID: <Pine.LNX.4.61.0506271723120.12949@gannet.stats>

That's the first time I have heard ? called an umlaut!

This sounds like an encoding problem, perhaps using Latin-1 in a UTF-8 
locale or vice versa.  In particular, R 2.0.1 (there is no 2.01) is out of 
date and has far less support for non-English locales than 2.1.1, and none 
for UTF-8 locales (which I believe SuSE 9.x uses by default).

So please follow the advice in the posting guide and update your R.
Your example works for me in 2.1.1 in both Latin-1 and UTF-8 locales.

Your email has been re-encoded several times en route to me, and does not 
contain character 142 (which is not a valid char in Latin-1 or UTF-8).

On Mon, 27 Jun 2005, Henning Rust wrote:

> Hello,
> I am running R 2.01 on SuSE 9.2 and I use GNU-Emacs and ESS.
> When using umlauts like ? ? ? ? for e.g. plot titles , I enter them
> directly in the code, like
>
> plot(seq(from=-4,to=4,length=100),dnorm(seq(from=-4,to=4,length=100)),
        main="Gau?-Verteilung")
>
> I get a dotted box just after the "Gau" and before the "?" in the title.
>
> Using the postscript device
>
> postscript()
> plot(seq(from=-4,to=4,length=100),dnorm(seq(from=-4,to=4,length=100)),main="Gau?-Verteilung")
> dev.off()
>
> I get two warning messages:
>
> Warning messages:
> 1: font width unknown for character 142
> 2: font width unknown for character 142
> and a space after the "Gau" and before the "?".
>
> I'd like to get rid of the extra space and the box before umlauts.
> Does anyone know how to solve the problem?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From spencer.graves at pdf.com  Mon Jun 27 18:41:19 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Jun 2005 09:41:19 -0700
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <971536df050626222318376e24@mail.gmail.com>
References: <42BF72EC.4040204@pdf.com>	<17087.31709.399479.651692@basebud.nulle.part>
	<971536df050626222318376e24@mail.gmail.com>
Message-ID: <42C02C2F.8070907@pdf.com>

	  Thanks, Dirk, Gabor, Eric:

	  You all provided appropriate solutions for the stated problem. 
Sadly, I oversimplified the problem I was trying to solve:  I copy a 
character string giving a DOS path from MS Windows Explorer into an R 
script file, and I get something like the following:

	  D:\spencerg\statmtds\R\Rnews

	  I want to be able to use this in R with its non-R meaning, e.g., in 
readLine, count.fields, read.table, etc., after appending a file name. 
Your three solutions all work for my oversimplified toy example but are 
inadequate for the problem I really want to solve.

	  Thanks,
	  spencer graves

Gabor Grothendieck wrote:

> On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
> 
>>On 26 June 2005 at 20:30, Spencer Graves wrote:
>>|         How can one convert back slashes to forward slashes, e.g, changing
>>| "c:\a\b" to "c:/a/b"?  I tried the following:
>>|
>>|  > gsub("\\\\", "/", "c:\a\b")
>>| [1] "c:\a\b"
>>
>>This does work, provided you remember that single backslashed "don't exist"
>>as e.g. \a is a character in itself. So use doubles are you should be fine:
>>
>>
>>>gsub("\\\\", "/", "c:\\a\\b")
>>
>>[1] "c:/a/b"
>>
> 
> 
> Also, if one finds four backslashes confusing one can avoid the use 
> of four via any of these:
> 
> gsub("[\\]", "/", "c:\\a\\b")
> gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
> chartr("\\", "/", "c:\\a\\b")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From spencer.graves at pdf.com  Mon Jun 27 18:52:16 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Jun 2005 09:52:16 -0700
Subject: [R] Mixed model
In-Reply-To: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64E@NANAMAILBACK1.nanamail.co.il>
References: <E76EB96029DCAE4A9CB967D7F6712D1DBFD64E@NANAMAILBACK1.nanamail.co.il>
Message-ID: <42C02EC0.2050105@pdf.com>

	  I often think carefully about what I want and store only that.  For 
example, I might do something like the following:

	  b1 <- coef(lme(...))
	  kb <- length(b1)
	  B <- array(NA, dim=c(nb, kb))
	  for(i in 1:nb){

		    B[i, ] <- coef(lme(...))	
	  }

	  With "fit[[i]] <- lme(...)", you store, as Doug said, "a copy of the 
model frame (the parts of Dataset that are needed to evaluate the model) 
plus a lot of other information)" for each pass through the loop.  Since 
you are doing a simulation, you probably only really care about a few 
numbers for each "i".  Identify those and only store those each time 
through the loop.

	  spencer graves
p.s.  Have you considered "simulate.lme"?

Stephen wrote:

> Hi
> Thank you for your comments.
> Yes you are correct its a very big data set.
> Perhaps I am best splitting it up and then importing to R.
> The reason for the loop is that I am conducting the equivalent of Split
> file in SPSS.
> Specifically, I am conducting the analysis for each value of on the
> grouping variable 'runnb'.
> If there is a less memory intensive way of doing this I'd appreciate
> knowing about it.
> Many Thanks and comments appreciated
> Regards
> Stephen
> 
> ________________________________
> 
> From: Douglas Bates [mailto:dmbates at gmail.com]
> Sent: Sun 26/06/2005 17:01
> To: Stephen
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Mixed model
> 
> 
> 
> On 6/26/05, Stephen <szlevine at nana.co.il> wrote:
> 
>>
>>
>>Hi All,
>>
>>
>>
>>I am currently conducting a mixed model. I have 7 repeated measures on
> 
> a
> 
>>simulated clinical trial. If I understand the model correctly, the
>>outcome is the measure (as a factor) the predictors are clinical group
>>and trial (1-7). The fixed factors are the measure and group. The
> 
> random
> 
>>factors are the intercept and id and group.
>>
>>
>>
>>I tried using 2 functions to calculate mixed effects.
>>
>>Following previous correspondence .
>>
>>
>>
>>Dataset <- read.table("C:/Program
> 
> Files/R/rw2011/data/miss/model1a.dat",
> 
>>header=TRUE, sep="\t", na.strings="NA", dec=".", strip.white=TRUE)
>>
>>attach(Dataset)
>>
>>
>>
>>require (nlme)
>>
>>with(Dataset, table(runnb, id, grp))
>>
>>b.lvls <- table(Dataset$runnb)
>>
>>nb <- length(b.lvls)
>>
>>fit <- vector(mode="list", nb)
>>
>>
>>
>>for(i in 1:nb)
>>
>> fit[[i]]<- lme (trans1 ~ Index1 + grp,
>>
>>            random = ~ 1 | id / grp ,
>>
>>            data = Dataset,
>>
>>            na.action = "na.exclude")
>>
>>
>>
>>
>>
>>This (above) worked OK only I am having memory problems.
>>
>>I have a gig of RAM set at --sdi --max-mem-size=512M (complete version
>>below)
>>
>>I am wondering if running the file as a database be slower / faster?
>>
>>
>>
>>Then I read that lme4 does it quicker and more accurately
>>
>>so I thought that I should re-run the code but from the for line:
>>
>>
>>
>>
>>>for (i in 1:nb)
>>
>>+  fit[[i]]  <- lmer(trans1 ~ Index1 + grp + (1|id:grp) + (1|id),
>>
>>+ Dataset, na.action = na.exclude)
>>
>>
>>
>>Producing
>>
>>
>>
>>Error in lmer(trans1 ~ Index1 + grp + (1 | id:grp) + (1 | id),
> 
> Dataset,
> 
>>:
>>
>>        flist[[2]] must be a factor of length 200000
>>
>>In addition: Warning messages:
>>
>>1: numerical expression has 200000 elements: only the first used in:
>>id:grp
>>
>>2: numerical expression has 200000 elements: only the first used in:
>>id:grp
> 
> 
> Check
> 
> str(Dataset)
> 
> and, if necessary, convert id to a factor with
> 
> Dataset$id <- factor(Dataset$id)
> 
> 
> In is not surprising that you are running into memory problems.  Look
> at the size of one of the fitted objects from lme or from lmer.  They
> are very large because they contain a copy of the model frame (the
> parts of Dataset that are needed to evaluate the model) plus a lot of
> other information.  You have a large Dataset and you are saving
> multiple copies of it although I must admit that I don't understand
> why the calls to lme or lmer are in a loop.
> 
> 
> 
> ???? ?"? ???? ????
> http://mail.nana.co.il
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From steve_adams_sd at yahoo.com  Mon Jun 27 19:24:36 2005
From: steve_adams_sd at yahoo.com (Steve Adams)
Date: Mon, 27 Jun 2005 10:24:36 -0700 (PDT)
Subject: [R] test of hazard ratios
Message-ID: <20050627172436.95603.qmail@web33313.mail.mud.yahoo.com>

Hi,

Is there a R test available that tests whether 2
hazard ratios obtained from Cox regressions on the
same patient sample by 2 different classifiers are
significantly different?

Thanks

Steve



From wayoung at softbase.math.uwaterloo.ca  Mon Jun 27 19:27:42 2005
From: wayoung at softbase.math.uwaterloo.ca (Tony Young)
Date: Mon, 27 Jun 2005 13:27:42 -0400
Subject: [R] Linear Modeling and Predictions
Message-ID: <F253AC1C-0C7E-46D2-AA68-535049184BA0@db.uwaterloo.ca>

Hello all,

I need some help with linear modeling and making predictions using my  
model. What I have done is the following:

-Read my variable data into R using read.table
-Assigned the values to variable names so that each variable is a  
vector from the table
-Generated my model using the lm command and each of my variables
-Read some new data with which to make predictions into a new data  
frame (the variables have the same names as the ones used to build  
the model, but the new variables are in a frame called "new")

Now, what I want to do is use the model that R constructed and feed  
it new values for the variables, then get back the estimated response  
variable value. I can't seem to figure out how to do this using  
predict or predict.lm. I get a warning that the new data contained 36  
values (there are 36 new sets of values that I want predictions for  
using the model) when the original data set contains 5136 values.  
Then, the fitted values are the ones that were from the model creation

Any help would be appreciated!

ty



From hb at maths.lth.se  Mon Jun 27 19:29:05 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 27 Jun 2005 19:29:05 +0200
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C02C2F.8070907@pdf.com>
References: <42BF72EC.4040204@pdf.com>	<17087.31709.399479.651692@basebud.nulle.part>	<971536df050626222318376e24@mail.gmail.com>
	<42C02C2F.8070907@pdf.com>
Message-ID: <42C03761.7070800@maths.lth.se>

Spencer Graves wrote:
> 	  Thanks, Dirk, Gabor, Eric:
> 
> 	  You all provided appropriate solutions for the stated problem. 
> Sadly, I oversimplified the problem I was trying to solve:  I copy a 
> character string giving a DOS path from MS Windows Explorer into an R 
> script file, and I get something like the following:
> 
> 	  D:\spencerg\statmtds\R\Rnews
> 
> 	  I want to be able to use this in R with its non-R meaning, e.g., in 
> readLine, count.fields, read.table, etc., after appending a file name. 
> Your three solutions all work for my oversimplified toy example but are 
> inadequate for the problem I really want to solve.

Hmmm. It should work as long as you do not source() the file (see 
below).  There are two things to watch out for here.

First, you have to be careful with backslashes, that is, a backslash is 
a single character ('\') in memory, but to be typed at the R prompt, you 
have to escape it (with a backslash), which is why we type "\\", cf. 
nchar("\\") == 0.  Consider the file foo.txt containing the 28 
characters (==28 bytes in plain ASCII format)

D:\spencerg\statmtds\R\Rnews

You can create such a file in R by

 > cat(file="foo.txt", "D:\\spencerg\\statmtds\\R\\Rnews")
 > str(file.info("foo.txt"))
`data.frame':   1 obs. of  6 variables:
  $ size : num 28
  $ isdir: logi FALSE
  $ mode :Class 'octmode'  int 438
  $ mtime:'POSIXct', format: chr "2005-06-27 19:14:20"
  $ ctime:'POSIXct', format: chr "2005-06-27 19:14:20"
  $ atime:'POSIXct', format: chr "2005-06-27 19:14:20"

Re-read it into R:
 > bfr <- readLines("foo.txt")
Warning message:
incomplete final line found by readLines on 'foo.txt'
 > bfr
[1] "D:\\spencerg\\statmtds\\R\\Rnews"
 > cat("bfr='", bfr, "'\n", sep="")
bfr='D:\spencerg\statmtds\R\Rnews'

Now, convert backslashes to "forwardslashes":
bfr2 <- gsub("\\\\", "/", bfr)
 > bfr2
[1] "D:/spencerg/statmtds/R/Rnews"
 > cat("bfr2='", bfr2, "'\n", sep="")
bfr2='D:/spencerg/statmtds/R/Rnews'

Second, regular expression patterns have their own escaping rules. This 
is why the following happens:

bfr3 <- gsub("\\", "/", bfr)
Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
         invalid regular expression '\'

The pattern "\\", which is a single '\' in memory, is passed to gsub(). 
Then gsub() tries to interpret this single backslash as a pattern, but 
it is invalid. gsub() uses backslashed to escape some characters in 
patterns.  So, when you think what gsub() needs, this about the 
characters (bytes) that are really stored in memory, not what you see.

A side comment: Wouldn't it be nice if the R parser had an alternative 
way to quote string such that, say, Perl strings could be used? Example:

bfr3 <- gsub("\\\\", "/", bfr)
bfr3 <- gsub('\\', "/", bfr)

would be equal (if now single quotes wouldn't have been reserved already).

Back to your problem: You must not paste the 28 characters into an R 
script that you source()! If you want to include you pathname (copied 
from the command prompt), you have to escape each '\' with a '\' to 
'\\'. Thus, if you use Emacs or another text editor, you pretty much 
should see '\\' if you want R(!) to interpret this as the single 
character '\'. Note the difference between using source() and, say, 
readLines().

Hope this helps

Henrik


> 	  Thanks,
> 	  spencer graves
> 
> Gabor Grothendieck wrote:
> 
> 
>>On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>>
>>
>>>On 26 June 2005 at 20:30, Spencer Graves wrote:
>>>|         How can one convert back slashes to forward slashes, e.g, changing
>>>| "c:\a\b" to "c:/a/b"?  I tried the following:
>>>|
>>>|  > gsub("\\\\", "/", "c:\a\b")
>>>| [1] "c:\a\b"
>>>
>>>This does work, provided you remember that single backslashed "don't exist"
>>>as e.g. \a is a character in itself. So use doubles are you should be fine:
>>>
>>>
>>>
>>>>gsub("\\\\", "/", "c:\\a\\b")
>>>
>>>[1] "c:/a/b"
>>>
>>
>>
>>Also, if one finds four backslashes confusing one can avoid the use 
>>of four via any of these:
>>
>>gsub("[\\]", "/", "c:\\a\\b")
>>gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
>>chartr("\\", "/", "c:\\a\\b")
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From iidn01 at yahoo.com  Mon Jun 27 19:49:34 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Mon, 27 Jun 2005 10:49:34 -0700 (PDT)
Subject: [R] "Error in contrasts" in step wise regression
In-Reply-To: <Pine.LNX.4.61.0506242233400.17824@gannet.stats>
Message-ID: <20050627174934.64513.qmail@web31114.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050627/e228185f/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 27 20:01:37 2005
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 27 Jun 2005 19:01:37 +0100 (GMT Standard Time)
Subject: [R] "Error in contrasts" in step wise regression
In-Reply-To: <20050627174934.64513.qmail@web31114.mail.mud.yahoo.com>
References: <20050627174934.64513.qmail@web31114.mail.mud.yahoo.com>
Message-ID: <Pine.WNT.4.58.0506271856100.3308@Petrel>

On Mon, 27 Jun 2005, Young Cho wrote:

> Thanks for the reply. I created a new dataframe and ran step on it. But, still it does not work.
>
> > detach(dat)
> > attach(ds)
> > dat <- ds[,sapply(ds,nlevels)>=2]
> > dat$Y <- Response
> > detach(ds)
> > attach(dat)
> > fmla <- as.formula(paste(" ~ ",paste(collist1[sapply(ds,nlevels)>=2],collapse="+")))
> > fit.s <- step(fit.1, direction="forward",scope=list(upper= fmla,lower= ~1))
> Start:  AIC= -1651.18
>  Y ~ 1
> Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
>         contrasts can be applied only to factors with 2 or more levels
> >

R does have debugging tools: please use them.

> Also, I was wondering if you know why the followings behave differently
> from the above:

Yes, as I have read the help page for step().  Have you?  It is discussed
there.

> > fit.s <- step(lm(Y~1),scope=list(upper=~.,lower=~1),)
> Start:  AIC= -1651.18
>  Y ~ 1
> > fit.s <- step(fit.1,scope=list(upper=~.,lower=~1),)
> Start:  AIC= -1651.18
>  Y ~ 1
>
> I thought "~." uses "all other variables in the data frame" according to
> "Introduciton to R."

In contexts where there is a data frame and there is no more specific
documentation, it means `all remaining variables separated by +'.

>
> -Young.
>
>
> Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Fri, 24 Jun 2005, Young Cho wrote:
>
> > Hi,
> >
> > I have a problem in getting step function work.
>
> This is not coming from step(), but (AFAIK) from model.matrix() called by
> lm(). One way to debug it is to try fitting the models directly.
>
> > I am getting the following error:
> >
> >> fit1 <- lm(Response~1)
> >> fmla <- as.formula(paste(" ~ ",paste(colnames,collapse="+")))
> >> sfit <- step(fit1,scope=list(upper= fmla,lower= ~1),k=log(nrow(dat)))
> > Start: AIC= -1646.66
> > Response ~ 1
> > Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
> > contrasts can be applied only to factors with 2 or more levels
> >
> > But if i count the unique values in each column by
> >
> > A <- NULL
> > for (ii in 1:length(colnames)){
> > A[ii] <- length(unique( eval(parse(text=paste('dat$',colnames[ii])))))
> > }
> >
> > I do not see any column with only 1 value. Is there some other possible
> > reason why I am getting the error? Thanks a lot!
>
> It says `levels', not values. So try
>
> sapply(dat, nlevels)
>
> The values can include NA, which is not a level (usually). E.g.
>
> > x <- factor(c(1, NA))
> > nlevels(x)
> [1] 1
> > length(unique(x))
> [1] 2
>
> (Incidentally, you are assuming variables are found in dat, and you should
> use
>
> lm(Response ~ 1, data=dat)
>
> to ensure that. And your calculation can be done more legibly as
>
> sapply(dat, function(x) length(unique(x)))
>
> .)
>
> --
> Brian D. Ripley, ripley at stats.ox.ac.uk
> Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
> University of Oxford, Tel: +44 1865 272861 (self)
> 1 South Parks Road, +44 1865 272866 (PA)
> Oxford OX1 3TG, UK Fax: +44 1865 272595
>
>
> ---------------------------------
> Yahoo! Sports
>  Rekindle the Rivalries. Sign up for Fantasy Football



From spencer.graves at pdf.com  Mon Jun 27 20:32:30 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Jun 2005 11:32:30 -0700
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C03761.7070800@maths.lth.se>
References: <42BF72EC.4040204@pdf.com>	<17087.31709.399479.651692@basebud.nulle.part>	<971536df050626222318376e24@mail.gmail.com>
	<42C02C2F.8070907@pdf.com> <42C03761.7070800@maths.lth.se>
Message-ID: <42C0463E.3040406@pdf.com>

Hi, Henrik:

	  Several functions, e.g., "grep", "sub", "gsub", and "regexpr", have 
an argument "perl", FALSE by default.  Moreover, "?regexp" has a section 
on "Perl Regular Expressions".  If you can do it in perl, might that 
transfer to "gsub(..., perl=TRUE)"?

	  Thanks,
	  spencer graves
p.s.  I skimmed the discussion of "Pearl Regular Expressions", and 
experimented with "gsub(..., perl=TRUE)" without success.  However, 
there may be a way to do it, and I just don't know perl and regexp well 
enough to have figured it out in the time available.

Henrik Bengtsson wrote:

> Spencer Graves wrote:
> 
>>       Thanks, Dirk, Gabor, Eric:
>>
>>       You all provided appropriate solutions for the stated problem. 
>> Sadly, I oversimplified the problem I was trying to solve:  I copy a 
>> character string giving a DOS path from MS Windows Explorer into an R 
>> script file, and I get something like the following:
>>
>>       D:\spencerg\statmtds\R\Rnews
>>
>>       I want to be able to use this in R with its non-R meaning, e.g., 
>> in readLine, count.fields, read.table, etc., after appending a file 
>> name. Your three solutions all work for my oversimplified toy example 
>> but are inadequate for the problem I really want to solve.
> 
> 
> Hmmm. It should work as long as you do not source() the file (see 
> below).  There are two things to watch out for here.
> 
> First, you have to be careful with backslashes, that is, a backslash is 
> a single character ('\') in memory, but to be typed at the R prompt, you 
> have to escape it (with a backslash), which is why we type "\\", cf. 
> nchar("\\") == 0.  Consider the file foo.txt containing the 28 
> characters (==28 bytes in plain ASCII format)
> 
> D:\spencerg\statmtds\R\Rnews
> 
> You can create such a file in R by
> 
>  > cat(file="foo.txt", "D:\\spencerg\\statmtds\\R\\Rnews")
>  > str(file.info("foo.txt"))
> `data.frame':   1 obs. of  6 variables:
>  $ size : num 28
>  $ isdir: logi FALSE
>  $ mode :Class 'octmode'  int 438
>  $ mtime:'POSIXct', format: chr "2005-06-27 19:14:20"
>  $ ctime:'POSIXct', format: chr "2005-06-27 19:14:20"
>  $ atime:'POSIXct', format: chr "2005-06-27 19:14:20"
> 
> Re-read it into R:
>  > bfr <- readLines("foo.txt")
> Warning message:
> incomplete final line found by readLines on 'foo.txt'
>  > bfr
> [1] "D:\\spencerg\\statmtds\\R\\Rnews"
>  > cat("bfr='", bfr, "'\n", sep="")
> bfr='D:\spencerg\statmtds\R\Rnews'
> 
> Now, convert backslashes to "forwardslashes":
> bfr2 <- gsub("\\\\", "/", bfr)
>  > bfr2
> [1] "D:/spencerg/statmtds/R/Rnews"
>  > cat("bfr2='", bfr2, "'\n", sep="")
> bfr2='D:/spencerg/statmtds/R/Rnews'
> 
> Second, regular expression patterns have their own escaping rules. This 
> is why the following happens:
> 
> bfr3 <- gsub("\\", "/", bfr)
> Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
>         invalid regular expression '\'
> 
> The pattern "\\", which is a single '\' in memory, is passed to gsub(). 
> Then gsub() tries to interpret this single backslash as a pattern, but 
> it is invalid. gsub() uses backslashed to escape some characters in 
> patterns.  So, when you think what gsub() needs, this about the 
> characters (bytes) that are really stored in memory, not what you see.
> 
> A side comment: Wouldn't it be nice if the R parser had an alternative 
> way to quote string such that, say, Perl strings could be used? Example:
> 
> bfr3 <- gsub("\\\\", "/", bfr)
> bfr3 <- gsub('\\', "/", bfr)
> 
> would be equal (if now single quotes wouldn't have been reserved already).
> 
> Back to your problem: You must not paste the 28 characters into an R 
> script that you source()! If you want to include you pathname (copied 
> from the command prompt), you have to escape each '\' with a '\' to 
> '\\'. Thus, if you use Emacs or another text editor, you pretty much 
> should see '\\' if you want R(!) to interpret this as the single 
> character '\'. Note the difference between using source() and, say, 
> readLines().
> 
> Hope this helps
> 
> Henrik
> 
> 
>>       Thanks,
>>       spencer graves
>>
>> Gabor Grothendieck wrote:
>>
>>
>>> On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>>>
>>>
>>>> On 26 June 2005 at 20:30, Spencer Graves wrote:
>>>> |         How can one convert back slashes to forward slashes, e.g, 
>>>> changing
>>>> | "c:\a\b" to "c:/a/b"?  I tried the following:
>>>> |
>>>> |  > gsub("\\\\", "/", "c:\a\b")
>>>> | [1] "c:\a\b"
>>>>
>>>> This does work, provided you remember that single backslashed "don't 
>>>> exist"
>>>> as e.g. \a is a character in itself. So use doubles are you should 
>>>> be fine:
>>>>
>>>>
>>>>
>>>>> gsub("\\\\", "/", "c:\\a\\b")
>>>>
>>>>
>>>> [1] "c:/a/b"
>>>>
>>>
>>>
>>> Also, if one finds four backslashes confusing one can avoid the use 
>>> of four via any of these:
>>>
>>> gsub("[\\]", "/", "c:\\a\\b")
>>> gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
>>> chartr("\\", "/", "c:\\a\\b")
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From celebridades at megamail.pt  Mon Jun 27 20:49:35 2005
From: celebridades at megamail.pt (alex diaz)
Date: Mon, 27 Jun 2005 19:49:35 +0100
Subject: [R] simplifying the code
Message-ID: <1119898175.42c04a3f3f984@roma-hme1>

dear list:

I need help to achieve a simpler code to complete a 
task I'm performing.
here is an example:

dat<-expand.grid(a=seq(1,5),b=seq(1000,1005))

I want to add a new column dat$c in that:

t1<-ifelse(dat$a==1&dat$b==1001,1001,0)
t2<-ifelse(dat$a==2&dat$b==1002,1001,0)
t3<-ifelse(dat$a==3&dat$b==1003,1001,0)
t4<-ifelse(dat$a==1&dat$b==1002,1002,0)
t5<-ifelse(dat$a==2&dat$b==1003,1002,0)
t6<-ifelse(dat$a==1&dat$b==1003,1003,0)
dat$c<-t1+t2+t3+t4+t5+t6


My real data frame is much larger... I hope someone 
can help me with this.

thanks for your help

a. diaz

-------------------------------------------------
Email Enviado utilizando o serviÅÁo MegaMail



From hb at maths.lth.se  Mon Jun 27 20:53:58 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 27 Jun 2005 20:53:58 +0200
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C0463E.3040406@pdf.com>
References: <42BF72EC.4040204@pdf.com>	<17087.31709.399479.651692@basebud.nulle.part>	<971536df050626222318376e24@mail.gmail.com>
	<42C02C2F.8070907@pdf.com> <42C03761.7070800@maths.lth.se>
	<42C0463E.3040406@pdf.com>
Message-ID: <42C04B46.8070705@maths.lth.se>

Spencer Graves wrote:
> Hi, Henrik:
> 
>       Several functions, e.g., "grep", "sub", "gsub", and "regexpr", 
> have an argument "perl", FALSE by default.  Moreover, "?regexp" has a 
> section on "Perl Regular Expressions".  If you can do it in perl, might 
> that transfer to "gsub(..., perl=TRUE)"?

I do not know the details behind the different "dialects" of regular 
expressions, but you can _not_ get the R parser to interpret the two 
ASCII characters "\n", as the two characters "\" and "n". The R parser 
is used when code is read by source() or when expressions are typed at 
the R prompt.  The parser will always read it as the newline character 
(ASCII 10). The results from the parser is then passed to the R enginee. 
  Thus, you cannot write your program such that it fools the parser, 
because your program is evaluated first after the parser.  In other 
words, there is no way you can get nchar("\n") to equal 2.

Cheers

Henrik


>       Thanks,
>       spencer graves
> p.s.  I skimmed the discussion of "Pearl Regular Expressions", and 
> experimented with "gsub(..., perl=TRUE)" without success.  However, 
> there may be a way to do it, and I just don't know perl and regexp well 
> enough to have figured it out in the time available.
> 
> Henrik Bengtsson wrote:
> 
>> Spencer Graves wrote:
>>
>>>       Thanks, Dirk, Gabor, Eric:
>>>
>>>       You all provided appropriate solutions for the stated problem. 
>>> Sadly, I oversimplified the problem I was trying to solve:  I copy a 
>>> character string giving a DOS path from MS Windows Explorer into an R 
>>> script file, and I get something like the following:
>>>
>>>       D:\spencerg\statmtds\R\Rnews
>>>
>>>       I want to be able to use this in R with its non-R meaning, 
>>> e.g., in readLine, count.fields, read.table, etc., after appending a 
>>> file name. Your three solutions all work for my oversimplified toy 
>>> example but are inadequate for the problem I really want to solve.
>>
>>
>>
>> Hmmm. It should work as long as you do not source() the file (see 
>> below).  There are two things to watch out for here.
>>
>> First, you have to be careful with backslashes, that is, a backslash 
>> is a single character ('\') in memory, but to be typed at the R 
>> prompt, you have to escape it (with a backslash), which is why we type 
>> "\\", cf. nchar("\\") == 0.  Consider the file foo.txt containing the 
>> 28 characters (==28 bytes in plain ASCII format)
>>
>> D:\spencerg\statmtds\R\Rnews
>>
>> You can create such a file in R by
>>
>>  > cat(file="foo.txt", "D:\\spencerg\\statmtds\\R\\Rnews")
>>  > str(file.info("foo.txt"))
>> `data.frame':   1 obs. of  6 variables:
>>  $ size : num 28
>>  $ isdir: logi FALSE
>>  $ mode :Class 'octmode'  int 438
>>  $ mtime:'POSIXct', format: chr "2005-06-27 19:14:20"
>>  $ ctime:'POSIXct', format: chr "2005-06-27 19:14:20"
>>  $ atime:'POSIXct', format: chr "2005-06-27 19:14:20"
>>
>> Re-read it into R:
>>  > bfr <- readLines("foo.txt")
>> Warning message:
>> incomplete final line found by readLines on 'foo.txt'
>>  > bfr
>> [1] "D:\\spencerg\\statmtds\\R\\Rnews"
>>  > cat("bfr='", bfr, "'\n", sep="")
>> bfr='D:\spencerg\statmtds\R\Rnews'
>>
>> Now, convert backslashes to "forwardslashes":
>> bfr2 <- gsub("\\\\", "/", bfr)
>>  > bfr2
>> [1] "D:/spencerg/statmtds/R/Rnews"
>>  > cat("bfr2='", bfr2, "'\n", sep="")
>> bfr2='D:/spencerg/statmtds/R/Rnews'
>>
>> Second, regular expression patterns have their own escaping rules. 
>> This is why the following happens:
>>
>> bfr3 <- gsub("\\", "/", bfr)
>> Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
>>         invalid regular expression '\'
>>
>> The pattern "\\", which is a single '\' in memory, is passed to 
>> gsub(). Then gsub() tries to interpret this single backslash as a 
>> pattern, but it is invalid. gsub() uses backslashed to escape some 
>> characters in patterns.  So, when you think what gsub() needs, this 
>> about the characters (bytes) that are really stored in memory, not 
>> what you see.
>>
>> A side comment: Wouldn't it be nice if the R parser had an alternative 
>> way to quote string such that, say, Perl strings could be used? Example:
>>
>> bfr3 <- gsub("\\\\", "/", bfr)
>> bfr3 <- gsub('\\', "/", bfr)
>>
>> would be equal (if now single quotes wouldn't have been reserved 
>> already).
>>
>> Back to your problem: You must not paste the 28 characters into an R 
>> script that you source()! If you want to include you pathname (copied 
>> from the command prompt), you have to escape each '\' with a '\' to 
>> '\\'. Thus, if you use Emacs or another text editor, you pretty much 
>> should see '\\' if you want R(!) to interpret this as the single 
>> character '\'. Note the difference between using source() and, say, 
>> readLines().
>>
>> Hope this helps
>>
>> Henrik
>>
>>
>>>       Thanks,
>>>       spencer graves
>>>
>>> Gabor Grothendieck wrote:
>>>
>>>
>>>> On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>>>>
>>>>
>>>>> On 26 June 2005 at 20:30, Spencer Graves wrote:
>>>>> |         How can one convert back slashes to forward slashes, e.g, 
>>>>> changing
>>>>> | "c:\a\b" to "c:/a/b"?  I tried the following:
>>>>> |
>>>>> |  > gsub("\\\\", "/", "c:\a\b")
>>>>> | [1] "c:\a\b"
>>>>>
>>>>> This does work, provided you remember that single backslashed 
>>>>> "don't exist"
>>>>> as e.g. \a is a character in itself. So use doubles are you should 
>>>>> be fine:
>>>>>
>>>>>
>>>>>
>>>>>> gsub("\\\\", "/", "c:\\a\\b")
>>>>>
>>>>>
>>>>>
>>>>> [1] "c:/a/b"
>>>>>
>>>>
>>>>
>>>> Also, if one finds four backslashes confusing one can avoid the use 
>>>> of four via any of these:
>>>>
>>>> gsub("[\\]", "/", "c:\\a\\b")
>>>> gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
>>>> chartr("\\", "/", "c:\\a\\b")
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>
>>
>



From james.holtman at convergys.com  Mon Jun 27 21:03:05 2005
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 27 Jun 2005 15:03:05 -0400
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C04B46.8070705@maths.lth.se>
Message-ID: <OF51AB1548.34BE5773-ON8525702D.00687E21-8525702D.0068A766@nd.convergys.com>





If you have 'copied' the path from DOS, then you can use 'scan' to read it
into a variable with the proper characters.

Here is the string that I 'copied'

D:\spencerg\statmtds\R\Rnews


Here is the results after 'scan':

> x.1 <- scan('clipboard', what='', allowEscapes=FALSE)
Read 1 item
> x.1
[1] "D:\\spencerg\\statmtds\\R\\Rnews"
>

Jim
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Convergys Labs
james.holtman at convergys.com
+1 (513) 723-2929


                                                                           
             Henrik Bengtsson                                              
             <hb at maths.lth.se>                                             
             Sent by:                                                   To 
             r-help-bounces at st         Spencer Graves                      
             at.math.ethz.ch           <spencer.graves at pdf.com>            
                                                                        cc 
                                       r-help at stat.math.ethz.ch, Dirk      
             06/27/2005 14:53          Eddelbuettel <edd at debian.org>       
                                                                   Subject 
                                       Re: [R] How to convert "c:\a\b" to  
                                       "c:/a/b"?                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Spencer Graves wrote:
> Hi, Henrik:
>
>       Several functions, e.g., "grep", "sub", "gsub", and "regexpr",
> have an argument "perl", FALSE by default.  Moreover, "?regexp" has a
> section on "Perl Regular Expressions".  If you can do it in perl, might
> that transfer to "gsub(..., perl=TRUE)"?

I do not know the details behind the different "dialects" of regular
expressions, but you can _not_ get the R parser to interpret the two
ASCII characters "\n", as the two characters "\" and "n". The R parser
is used when code is read by source() or when expressions are typed at
the R prompt.  The parser will always read it as the newline character
(ASCII 10). The results from the parser is then passed to the R enginee.
  Thus, you cannot write your program such that it fools the parser,
because your program is evaluated first after the parser.  In other
words, there is no way you can get nchar("\n") to equal 2.

Cheers

Henrik


>       Thanks,
>       spencer graves
> p.s.  I skimmed the discussion of "Pearl Regular Expressions", and
> experimented with "gsub(..., perl=TRUE)" without success.  However,
> there may be a way to do it, and I just don't know perl and regexp well
> enough to have figured it out in the time available.
>
> Henrik Bengtsson wrote:
>
>> Spencer Graves wrote:
>>
>>>       Thanks, Dirk, Gabor, Eric:
>>>
>>>       You all provided appropriate solutions for the stated problem.
>>> Sadly, I oversimplified the problem I was trying to solve:  I copy a
>>> character string giving a DOS path from MS Windows Explorer into an R
>>> script file, and I get something like the following:
>>>
>>>       D:\spencerg\statmtds\R\Rnews
>>>
>>>       I want to be able to use this in R with its non-R meaning,
>>> e.g., in readLine, count.fields, read.table, etc., after appending a
>>> file name. Your three solutions all work for my oversimplified toy
>>> example but are inadequate for the problem I really want to solve.
>>
>>
>>
>> Hmmm. It should work as long as you do not source() the file (see
>> below).  There are two things to watch out for here.
>>
>> First, you have to be careful with backslashes, that is, a backslash
>> is a single character ('\') in memory, but to be typed at the R
>> prompt, you have to escape it (with a backslash), which is why we type
>> "\\", cf. nchar("\\") == 0.  Consider the file foo.txt containing the
>> 28 characters (==28 bytes in plain ASCII format)
>>
>> D:\spencerg\statmtds\R\Rnews
>>
>> You can create such a file in R by
>>
>>  > cat(file="foo.txt", "D:\\spencerg\\statmtds\\R\\Rnews")
>>  > str(file.info("foo.txt"))
>> `data.frame':   1 obs. of  6 variables:
>>  $ size : num 28
>>  $ isdir: logi FALSE
>>  $ mode :Class 'octmode'  int 438
>>  $ mtime:'POSIXct', format: chr "2005-06-27 19:14:20"
>>  $ ctime:'POSIXct', format: chr "2005-06-27 19:14:20"
>>  $ atime:'POSIXct', format: chr "2005-06-27 19:14:20"
>>
>> Re-read it into R:
>>  > bfr <- readLines("foo.txt")
>> Warning message:
>> incomplete final line found by readLines on 'foo.txt'
>>  > bfr
>> [1] "D:\\spencerg\\statmtds\\R\\Rnews"
>>  > cat("bfr='", bfr, "'\n", sep="")
>> bfr='D:\spencerg\statmtds\R\Rnews'
>>
>> Now, convert backslashes to "forwardslashes":
>> bfr2 <- gsub("\\\\", "/", bfr)
>>  > bfr2
>> [1] "D:/spencerg/statmtds/R/Rnews"
>>  > cat("bfr2='", bfr2, "'\n", sep="")
>> bfr2='D:/spencerg/statmtds/R/Rnews'
>>
>> Second, regular expression patterns have their own escaping rules.
>> This is why the following happens:
>>
>> bfr3 <- gsub("\\", "/", bfr)
>> Error in gsub(pattern, replacement, x, ignore.case, extended, fixed) :
>>         invalid regular expression '\'
>>
>> The pattern "\\", which is a single '\' in memory, is passed to
>> gsub(). Then gsub() tries to interpret this single backslash as a
>> pattern, but it is invalid. gsub() uses backslashed to escape some
>> characters in patterns.  So, when you think what gsub() needs, this
>> about the characters (bytes) that are really stored in memory, not
>> what you see.
>>
>> A side comment: Wouldn't it be nice if the R parser had an alternative
>> way to quote string such that, say, Perl strings could be used? Example:
>>
>> bfr3 <- gsub("\\\\", "/", bfr)
>> bfr3 <- gsub('\\', "/", bfr)
>>
>> would be equal (if now single quotes wouldn't have been reserved
>> already).
>>
>> Back to your problem: You must not paste the 28 characters into an R
>> script that you source()! If you want to include you pathname (copied
>> from the command prompt), you have to escape each '\' with a '\' to
>> '\\'. Thus, if you use Emacs or another text editor, you pretty much
>> should see '\\' if you want R(!) to interpret this as the single
>> character '\'. Note the difference between using source() and, say,
>> readLines().
>>
>> Hope this helps
>>
>> Henrik
>>
>>
>>>       Thanks,
>>>       spencer graves
>>>
>>> Gabor Grothendieck wrote:
>>>
>>>
>>>> On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>>>>
>>>>
>>>>> On 26 June 2005 at 20:30, Spencer Graves wrote:
>>>>> |         How can one convert back slashes to forward slashes, e.g,
>>>>> changing
>>>>> | "c:\a\b" to "c:/a/b"?  I tried the following:
>>>>> |
>>>>> |  > gsub("\\\\", "/", "c:\a\b")
>>>>> | [1] "c:\a\b"
>>>>>
>>>>> This does work, provided you remember that single backslashed
>>>>> "don't exist"
>>>>> as e.g. \a is a character in itself. So use doubles are you should
>>>>> be fine:
>>>>>
>>>>>
>>>>>
>>>>>> gsub("\\\\", "/", "c:\\a\\b")
>>>>>
>>>>>
>>>>>
>>>>> [1] "c:/a/b"
>>>>>
>>>>
>>>>
>>>> Also, if one finds four backslashes confusing one can avoid the use
>>>> of four via any of these:
>>>>
>>>> gsub("[\\]", "/", "c:\\a\\b")
>>>> gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
>>>> chartr("\\", "/", "c:\\a\\b")
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>
>>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rchandler at forwild.umass.edu  Mon Jun 27 21:35:30 2005
From: rchandler at forwild.umass.edu (Richard Chandler)
Date: Mon, 27 Jun 2005 15:35:30 -0400
Subject: [R] function for "two-part" or "two-condition" models
Message-ID: <1119900930.42c05502c3893@mail-www2.oit.umass.edu>

Hello,

This is an (hopefully) improved question of one I posted several weeks
ago. Does anyone know of a function for fitting "two-part" models?
These models are designed to handle count data with so many zeroes
that they can't be fit well with zero-inflated Poisson models or other
'typical' GLMs. My understanding is that they work by first fitting a
binomial model to separate the zeros from the occurrences (positive
integers) before fitting a Poisson model to account for variation in
abundance. 

I have tried help.search("two-part") and many other similar guesses.

Thanks,
Richard

-- 
Richard Chandler, M.S. Candidate
Department of Natural Resources Conservation
UMass Amherst
(413)545-1237



From spencer.graves at pdf.com  Mon Jun 27 22:09:36 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Jun 2005 13:09:36 -0700
Subject: [R] Linear Modeling and Predictions
In-Reply-To: <F253AC1C-0C7E-46D2-AA68-535049184BA0@db.uwaterloo.ca>
References: <F253AC1C-0C7E-46D2-AA68-535049184BA0@db.uwaterloo.ca>
Message-ID: <42C05D00.40400@pdf.com>

	  Have you considered "predict" (including the examples with 
"?predict.lm")?

	  spencer graves

Tony Young wrote:

> Hello all,
> 
> I need some help with linear modeling and making predictions using my  
> model. What I have done is the following:
> 
> -Read my variable data into R using read.table
> -Assigned the values to variable names so that each variable is a  
> vector from the table
> -Generated my model using the lm command and each of my variables
> -Read some new data with which to make predictions into a new data  
> frame (the variables have the same names as the ones used to build  
> the model, but the new variables are in a frame called "new")
> 
> Now, what I want to do is use the model that R constructed and feed  
> it new values for the variables, then get back the estimated response  
> variable value. I can't seem to figure out how to do this using  
> predict or predict.lm. I get a warning that the new data contained 36  
> values (there are 36 new sets of values that I want predictions for  
> using the model) when the original data set contains 5136 values.  
> Then, the fitted values are the ones that were from the model creation
> 
> Any help would be appreciated!
> 
> ty
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From a_mani_sc_gs at vsnl.net  Mon Jun 27 22:30:29 2005
From: a_mani_sc_gs at vsnl.net (A. Mani)
Date: Tue, 28 Jun 2005 02:00:29 +0530
Subject: [R] A. Mani : colours in Silhouette
Message-ID: <200506280200.29422.a_mani_sc_gs@vsnl.net>

Hello,
       In cluster analysis with cluster, how does one colour the silhouette 
plots ? For example in using pam. There seems to be some problem there. 
Everything else can be coloured.

Thanks,

A. Mani
Member, Cal. Math. Soc



From celebridades at megamail.pt  Mon Jun 27 22:53:50 2005
From: celebridades at megamail.pt (alex diaz)
Date: Mon, 27 Jun 2005 21:53:50 +0100
Subject: [R] simplifying the code
Message-ID: <1119905630.42c0675eafc3b@madrid-hme1>

dear list:

I made some corrections in the previous post that had 
some mistakes.

I need help to achieve a simpler code to complete a 
task I'm performing.
here is an example:

dat<-expand.grid(a=seq(1,4),b=seq(1001,1004))

#I want to add a new column dat$c in that:

t1<-ifelse(dat$a==1&dat$b==1001,1001,0)
t2<-ifelse(dat$a==2&dat$b==1002,1001,0)
t3<-ifelse(dat$a==3&dat$b==1003,1001,0)
t4<-ifelse(dat$a==4&dat$b==1004,1001,0)
t5<-ifelse(dat$a==1&dat$b==1002,1002,0)
t6<-ifelse(dat$a==2&dat$b==1003,1002,0)
t7<-ifelse(dat$a==3&dat$b==1004,1002,0)
t8<-ifelse(dat$a==1&dat$b==1003,1003,0)
t9<-ifelse(dat$a==2&dat$b==1004,1003,0)
t10<-ifelse(dat$a==1&dat$b==1004,1004,0)

dat$c<-t1+t2+t3+t4+t5+t6+t7+t8+t9+t10

My real data frame is much larger... I hope someone 
can help me with this.

thanks for your help

a. diaz


-------------------------------------------------
Email Enviado utilizando o serviÅÁo MegaMail



From jfbrennan at rogers.com  Mon Jun 27 23:13:00 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Mon, 27 Jun 2005 17:13:00 -0400
Subject: [R] simplifying the code
In-Reply-To: <1119905630.42c0675eafc3b@madrid-hme1>
Message-ID: <200506272112.j5RLCwBG020478@hypatia.math.ethz.ch>

This gives the same answer for what you have supplied so far...
> dat$d<-ifelse((dat$b-dat$a)>=1000,dat$b-dat$a+1,0)
> dat
   a    b    c    d
1  1 1001 1001 1001
2  2 1001    0    0
3  3 1001    0    0
4  4 1001    0    0
5  1 1002 1002 1002
6  2 1002 1001 1001
7  3 1002    0    0
8  4 1002    0    0
9  1 1003 1003 1003
10 2 1003 1002 1002
11 3 1003 1001 1001
12 4 1003    0    0
13 1 1004 1004 1004
14 2 1004 1003 1003
15 3 1004 1002 1002
16 4 1004 1001 1001

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of alex diaz
Sent: June 27, 2005 4:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] simplifying the code

dear list:

I made some corrections in the previous post that had 
some mistakes.

I need help to achieve a simpler code to complete a 
task I'm performing.
here is an example:

dat<-expand.grid(a=seq(1,4),b=seq(1001,1004))

#I want to add a new column dat$c in that:

t1<-ifelse(dat$a==1&dat$b==1001,1001,0)
t2<-ifelse(dat$a==2&dat$b==1002,1001,0)
t3<-ifelse(dat$a==3&dat$b==1003,1001,0)
t4<-ifelse(dat$a==4&dat$b==1004,1001,0)
t5<-ifelse(dat$a==1&dat$b==1002,1002,0)
t6<-ifelse(dat$a==2&dat$b==1003,1002,0)
t7<-ifelse(dat$a==3&dat$b==1004,1002,0)
t8<-ifelse(dat$a==1&dat$b==1003,1003,0)
t9<-ifelse(dat$a==2&dat$b==1004,1003,0)
t10<-ifelse(dat$a==1&dat$b==1004,1004,0)

dat$c<-t1+t2+t3+t4+t5+t6+t7+t8+t9+t10

My real data frame is much larger... I hope someone 
can help me with this.

thanks for your help

a. diaz


-------------------------------------------------
Email Enviado utilizando o servigo MegaMail



From melomane28 at yahoo.com  Mon Jun 27 23:17:17 2005
From: melomane28 at yahoo.com (kapo coulibaly)
Date: Mon, 27 Jun 2005 14:17:17 -0700 (PDT)
Subject: [R] ks.test() output interpretation
Message-ID: <20050627211717.70116.qmail@web30001.mail.mud.yahoo.com>

I'm using ks.test() to compare two different
measurement methods. I don't really know how to
interpret the output in the absence of critical value
table of the D statistic. I guess I could use the
p-value when available. But I also get the message
"cannot compute correct p-values with ties ..." does
it mean I can't use ks.test() for these data or I can
still use the D statistic computed to make a decision
whether the two samples come from the same
distribution.

Thanks!!


		
____________________________________________________ 

Rekindle the Rivalries. Sign up for Fantasy Football



From spencer.graves at pdf.com  Mon Jun 27 23:25:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Jun 2005 14:25:14 -0700
Subject: [R] simplifying the code
In-Reply-To: <200506272112.j5RLCwBG020478@hypatia.math.ethz.ch>
References: <200506272112.j5RLCwBG020478@hypatia.math.ethz.ch>
Message-ID: <42C06EBA.9070105@pdf.com>

	  Have you considered a generalization of the following:

dat <- expand.grid(a=1:2, b=1001:1002); class(dat)
Xlate <- array(4:1, dim=c(2,2))
dat$c. <- Xlate[cbind(dat$a, dat$b-1000)]

	  spencer graves

Jim Brennan wrote:

> This gives the same answer for what you have supplied so far...
> 
>>dat$d<-ifelse((dat$b-dat$a)>=1000,dat$b-dat$a+1,0)
>>dat
> 
>    a    b    c    d
> 1  1 1001 1001 1001
> 2  2 1001    0    0
> 3  3 1001    0    0
> 4  4 1001    0    0
> 5  1 1002 1002 1002
> 6  2 1002 1001 1001
> 7  3 1002    0    0
> 8  4 1002    0    0
> 9  1 1003 1003 1003
> 10 2 1003 1002 1002
> 11 3 1003 1001 1001
> 12 4 1003    0    0
> 13 1 1004 1004 1004
> 14 2 1004 1003 1003
> 15 3 1004 1002 1002
> 16 4 1004 1001 1001
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of alex diaz
> Sent: June 27, 2005 4:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] simplifying the code
> 
> dear list:
> 
> I made some corrections in the previous post that had 
> some mistakes.
> 
> I need help to achieve a simpler code to complete a 
> task I'm performing.
> here is an example:
> 
> dat<-expand.grid(a=seq(1,4),b=seq(1001,1004))
> 
> #I want to add a new column dat$c in that:
> 
> t1<-ifelse(dat$a==1&dat$b==1001,1001,0)
> t2<-ifelse(dat$a==2&dat$b==1002,1001,0)
> t3<-ifelse(dat$a==3&dat$b==1003,1001,0)
> t4<-ifelse(dat$a==4&dat$b==1004,1001,0)
> t5<-ifelse(dat$a==1&dat$b==1002,1002,0)
> t6<-ifelse(dat$a==2&dat$b==1003,1002,0)
> t7<-ifelse(dat$a==3&dat$b==1004,1002,0)
> t8<-ifelse(dat$a==1&dat$b==1003,1003,0)
> t9<-ifelse(dat$a==2&dat$b==1004,1003,0)
> t10<-ifelse(dat$a==1&dat$b==1004,1004,0)
> 
> dat$c<-t1+t2+t3+t4+t5+t6+t7+t8+t9+t10
> 
> My real data frame is much larger... I hope someone 
> can help me with this.
> 
> thanks for your help
> 
> a. diaz
> 
> 
> -------------------------------------------------
> Email Enviado utilizando o servigo MegaMail
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From andrewr at uidaho.edu  Tue Jun 28 01:22:43 2005
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 28 Jun 2005 09:22:43 +1000
Subject: [R] function for "two-part" or "two-condition" models
In-Reply-To: <1119900930.42c05502c3893@mail-www2.oit.umass.edu>
References: <1119900930.42c05502c3893@mail-www2.oit.umass.edu>
Message-ID: <20050627232243.GB600@uidaho.edu>

Hi Richard,

I'm not sure that I can imagine how data can have too many zeros to be
fit well with zero-inflated Poisson models. Won't the excess zeros be
accommodated by increasing the the inflation?

In any case, if you want a model that separates the zeros from the
occurrences before fitting a Poisson model to account for variation in
abundance then it might be safest to do that split manually.

Another angle to try is to treat it as a special case of a finite
mixture regression.  I think that some of Jim Lindsey's code will fit
such models. Google can help you find his wbsite.

An MS student of mine explored these models for regeneration modeling.
I'd be happy to send you a pdf of his thesis if it would help.

Cheers,

Andrew

On Mon, Jun 27, 2005 at 03:35:30PM -0400, Richard Chandler wrote:
> Hello,
> 
> This is an (hopefully) improved question of one I posted several weeks
> ago. Does anyone know of a function for fitting "two-part" models?
> These models are designed to handle count data with so many zeroes
> that they can't be fit well with zero-inflated Poisson models or other
> 'typical' GLMs. My understanding is that they work by first fitting a
> binomial model to separate the zeros from the occurrences (positive
> integers) before fitting a Poisson model to account for variation in
> abundance. 
> 
> I have tried help.search("two-part") and many other similar guesses.
> 
> Thanks,
> Richard
> 
> -- 
> Richard Chandler, M.S. Candidate
> Department of Natural Resources Conservation
> UMass Amherst
> (413)545-1237
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From celebridades at megamail.pt  Tue Jun 28 00:29:35 2005
From: celebridades at megamail.pt (alex diaz)
Date: Mon, 27 Jun 2005 23:29:35 +0100
Subject: [R] simplifying the code
In-Reply-To: <20050627211346.0AB2831B09@mail2.megamail.pt>
References: <20050627211346.0AB2831B09@mail2.megamail.pt>
Message-ID: <1119911375.42c07dcf3071c@madrid-hme1>

problem solved. thank you Jim and Spencer for your 
answers.



> This gives the same answer for what you have
> supplied so far...
> >
> dat$d<-ifelse((dat$b-dat$a)>=1000,dat$b-dat$a+1,0)
> > dat
>    a    b    c    d
> 1  1 1001 1001 1001
> 2  2 1001    0    0
> 3  3 1001    0    0
> 4  4 1001    0    0
> 5  1 1002 1002 1002
> 6  2 1002 1001 1001
> 7  3 1002    0    0
> 8  4 1002    0    0
> 9  1 1003 1003 1003
> 10 2 1003 1002 1002
> 11 3 1003 1001 1001
> 12 4 1003    0    0
> 13 1 1004 1004 1004
> 14 2 1004 1003 1003
> 15 3 1004 1002 1002
> 16 4 1004 1001 1001
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On
> Behalf Of alex diaz
> Sent: June 27, 2005 4:54 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] simplifying the code
> 
> dear list:
> 
> I made some corrections in the previous post
> that had 
> some mistakes.
> 
> I need help to achieve a simpler code to
> complete a 
> task I'm performing.
> here is an example:
> 
> dat<-expand.grid(a=seq(1,4),b=seq(1001,1004))
> 
> #I want to add a new column dat$c in that:
> 
> t1<-ifelse(dat$a==1&dat$b==1001,1001,0)
> t2<-ifelse(dat$a==2&dat$b==1002,1001,0)
> t3<-ifelse(dat$a==3&dat$b==1003,1001,0)
> t4<-ifelse(dat$a==4&dat$b==1004,1001,0)
> t5<-ifelse(dat$a==1&dat$b==1002,1002,0)
> t6<-ifelse(dat$a==2&dat$b==1003,1002,0)
> t7<-ifelse(dat$a==3&dat$b==1004,1002,0)
> t8<-ifelse(dat$a==1&dat$b==1003,1003,0)
> t9<-ifelse(dat$a==2&dat$b==1004,1003,0)
> t10<-ifelse(dat$a==1&dat$b==1004,1004,0)
> 
> dat$c<-t1+t2+t3+t4+t5+t6+t7+t8+t9+t10
> 
> My real data frame is much larger... I hope
> someone 
> can help me with this.
> 
> thanks for your help
> 
> a. diaz
 
 

-------------------------------------------------
Email Enviado utilizando o serviÅÁo MegaMail



From macq at llnl.gov  Tue Jun 28 01:10:46 2005
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 27 Jun 2005 16:10:46 -0700
Subject: [R] Numerical accuracy
In-Reply-To: <1119884231.42c013c7458e2@www.ufal.br>
References: <1119884231.42c013c7458e2@www.ufal.br>
Message-ID: <p06210206bee63698cd78@[128.115.153.6]>

The following might be helpful.

Statistical Reference Datasets (StRD) website 
http://www.nist.gov/itl/div898/strd

  http://www.amstat.org/publications/tas/mccull-1.pdf
  http://www.amstat.org/publications/tas/mccull.pdf

-Don

At 11:57 AM -0300 6/27/05, Talita Perciano Costa Leite wrote:
>Hi people,
>
>I need to prove the good quality of numerical accuracy of R. Anyone knows a
>paper or anything else comparing R to other statistical softwares in terms of
>numerical accuracy.
>I've made a long search about that but I found nothing. Please help me!!
>
>Thanx,
>
>Talita Leite
>
>-------------------------------------------------
>Este e-mail foi enviado pelo Webmail da UFAL
>IMP: http://horde.org/imp/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From Tom.Mulholland at dpi.wa.gov.au  Tue Jun 28 03:08:24 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 28 Jun 2005 09:08:24 +0800
Subject: [R] A. Mani : colours in Silhouette
Message-ID: <4702645135092E4497088F71D9C8F51A128BAF@afhex01.dpi.wa.gov.au>

It's not so much a problem, as not working the way you expected. cluster:::plot.partition is used to do the plotting. If you look at the code for this you can see the difficulty in putting every possible permutation into the code. If for example you want the silhouette plot to be red using col = "red" is not intuitive as the cluster plot (which comes up first) has more than one colour. If you have a look at methods(plot) (assuming that you have loaded the cluster package) you will see that there is a specific piece of code in the form of plot.silhouette. It has an asterisk next to it so you need to use cluster:::plot.silhouette to see the code. It has what you need.

args(cluster:::plot.silhouette)
> function (x, nmax.lab = 40, max.strlen = 5, main = NULL, sub = NULL, 
    xlab = expression("Silhouette width " * s[i]), col = "gray", 
    do.col.sort = length(col) > 1, border = 0, cex.names = par("cex.axis"), 
    do.n.k = TRUE, do.clus.stat = TRUE, ...) 


 data(ruspini)
      pr4 <- pam(ruspini, 4)
      si <- silhouette(pr4)
      plot(si,col = "red")

The issue is that whenever code is written there is always a choice as to what functionality is put in place. Just because something can be done, does not mean it will or in some cases should be done. In this case the help for plot.partition notes that "For more flexibility, use 'plot(silhouette(x), ...)', see 'plot.silhouette'."

Tom

Thanks for that I found out something I will find useful in the future.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of A. Mani
> Sent: Tuesday, 28 June 2005 4:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] A. Mani : colours in Silhouette
> 
> 
> Hello,
>        In cluster analysis with cluster, how does one colour 
> the silhouette 
> plots ? For example in using pam. There seems to be some 
> problem there. 
> Everything else can be coloured.
> 
> Thanks,
> 
> A. Mani
> Member, Cal. Math. Soc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From medp9193 at nus.edu.sg  Tue Jun 28 03:13:11 2005
From: medp9193 at nus.edu.sg (Tan Hui Hui Jenny)
Date: Tue, 28 Jun 2005 09:13:11 +0800
Subject: [R] Crosstabs in R
Message-ID: <16CDECA355E2FD48A3C5A51E31A78E4D05E969@MBOX23.stu.nus.edu.sg>

HI,

	I have the data in the following format. My aim is to determine the concordance in genotype calls (SNP1, 2, 3,etc) at two centers.
	 
	DNA center snp1 snp2
	NA07019 1 A A
	NA07348 1 M G
	NA10830 1 A G
	NA10851 1 M G
	NA10857 1 A G
	NA10860 1 A G
	NA10861 1 A G
	NA12761 1 ? R
	NA07019 2 A A
	NA07348 2 M G
	NA10830 2 A G
	NA10851 2 M G
	NA10857 2 A G
	NA10860 2 A G
	NA10861 2 A G
	NA12761 2 A A
	 
	In SPSS, I would create individual crosstabulation tables and any positive integer values (counts) falling outside of the diagonal are disagreements in calls by the 2 centers (SNP2).

	At SNP1, there are total of 7 genotype counts to compare (one DNA did not have genotype readout). And 7/7 are in agreement.
	 
	At SNP2, all 8 DNA samples were successfully typed by both centers. There is one discrepant call between centers.

	Q1: how do I create the same crosstabs in R?
	 
	Eventually, I would summarize the data as follows:
	 
	SNP N_to_compare N_agreement %Concordance
	snp1 7  7  100
	snp2 8  7  87.5
	etc
	 
	Q2: From the individual crosstabs tables, can I automatically extract those numbers in columns 2 and 3?
	 
	 
	rgds,
	jenny



From pierrelap at gmail.com  Tue Jun 28 03:24:17 2005
From: pierrelap at gmail.com (Pierre Lapointe)
Date: Mon, 27 Jun 2005 21:24:17 -0400
Subject: [R] Index (which) of last non-NA data in dataframe
Message-ID: <42c0a6c0.473d8f00.1447.ffffc49e@mx.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050627/11950d26/attachment.pl

From andy_liaw at merck.com  Tue Jun 28 03:31:16 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 27 Jun 2005 21:31:16 -0400
Subject: [R] Index (which) of last non-NA data in dataframe
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA1D@usctmx1106.Merck.com>

If you can be sure that the non-NAs are contiguous, then the following
should work:

sapply(d, function(x) max(which(!is.na(x))))

Andy

> From: Pierre Lapointe
> 
> Hello,
> 
>  
> 
> In a dataframe, I want the index if the last non-NA data.   Example:
> 
>  
> 
> d<-data.frame(matrix(c(1,3,4,2,7,8,1,NA,2),3,3,byrow=TRUE))
> 
>  
> 
> gives:
> 
> > d
> 
>   X1 X2 X3
> 
> 1  1  3  4
> 
> 2  2  7  8
> 
> 3  1 NA  2
> 
>  
> 
> I want a vector that gives me  3 2 3
> 
>  
> 
> I know about tail and which, but I don't know how not to 
> consider the NA
> 
>  
> 
> e.g.: last<-tail(d,1,na.rm=TRUE) still gives me a NA
> 
> > last
> 
>   X1 X2 X3
> 
> 3  1 NA  2
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Tue Jun 28 04:25:36 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Mon, 27 Jun 2005 22:25:36 -0400
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C02C2F.8070907@pdf.com>
References: <42BF72EC.4040204@pdf.com>
	<17087.31709.399479.651692@basebud.nulle.part>
	<971536df050626222318376e24@mail.gmail.com> <42C02C2F.8070907@pdf.com>
Message-ID: <971536df05062719251e97b319@mail.gmail.com>

Try using file.choose() to locate the file instead of Windows Explorer.  That 
will return the name in a form useable within R.

On 6/27/05, Spencer Graves <spencer.graves at pdf.com> wrote:
>          Thanks, Dirk, Gabor, Eric:
> 
>          You all provided appropriate solutions for the stated problem.
> Sadly, I oversimplified the problem I was trying to solve:  I copy a
> character string giving a DOS path from MS Windows Explorer into an R
> script file, and I get something like the following:
> 
>          D:\spencerg\statmtds\R\Rnews
> 
>          I want to be able to use this in R with its non-R meaning, e.g., in
> readLine, count.fields, read.table, etc., after appending a file name.
> Your three solutions all work for my oversimplified toy example but are
> inadequate for the problem I really want to solve.
> 
>          Thanks,
>          spencer graves
> 
> Gabor Grothendieck wrote:
> 
> > On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
> >
> >>On 26 June 2005 at 20:30, Spencer Graves wrote:
> >>|         How can one convert back slashes to forward slashes, e.g, changing
> >>| "c:\a\b" to "c:/a/b"?  I tried the following:
> >>|
> >>|  > gsub("\\\\", "/", "c:\a\b")
> >>| [1] "c:\a\b"
> >>
> >>This does work, provided you remember that single backslashed "don't exist"
> >>as e.g. \a is a character in itself. So use doubles are you should be fine:
> >>
> >>
> >>>gsub("\\\\", "/", "c:\\a\\b")
> >>
> >>[1] "c:/a/b"
> >>
> >
> >
> > Also, if one finds four backslashes confusing one can avoid the use
> > of four via any of these:
> >
> > gsub("[\\]", "/", "c:\\a\\b")
> > gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
> > chartr("\\", "/", "c:\\a\\b")
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>



From jfbrennan at rogers.com  Tue Jun 28 04:25:55 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Mon, 27 Jun 2005 22:25:55 -0400
Subject: [R] Crosstabs in R
In-Reply-To: <16CDECA355E2FD48A3C5A51E31A78E4D05E969@MBOX23.stu.nus.edu.sg>
Message-ID: <200506280225.j5S2Pr28030448@hypatia.math.ethz.ch>

Here is one way to do it.
> jen2<-reshape(jen,idvar="DNA",timevar="center",direction="wide")
This makes it easier to work with.

> jen2<-replace(jen2,jen2=="?",NA)

Change the question marks to NA.

> jen2$match1<-((jen2[,2]==jen2[,4])*1)
> jen2$match2<-((jen2[,3]==jen2[,5])*1)
> jen2
      DNA snp1.1 snp2.1 snp1.2 snp2.2 match1 match2
1 NA07019      A      A      A      A      1      1
2 NA07348      M      G      M      G      1      1
3 NA10830      A      G      A      G      1      1
4 NA10851      M      G      M      G      1      1
5 NA10857      A      G      A      G      1      1
6 NA10860      A      G      A      G      1      1
7 NA10861      A      G      A      G      1      1
8 NA12761   <NA>      R      A      A     NA      0
> mean(jen2[,6:7])
match1 match2 
    NA  0.875 
> mean(jen2[,6:7],na.rm=T)
match1 match2 
 1.000  0.875 
>
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tan Hui Hui Jenny
Sent: June 27, 2005 9:13 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Crosstabs in R

HI,

	I have the data in the following format. My aim is to determine the
concordance in genotype calls (SNP1, 2, 3,etc) at two centers.
	 
	DNA center snp1 snp2
	NA07019 1 A A
	NA07348 1 M G
	NA10830 1 A G
	NA10851 1 M G
	NA10857 1 A G
	NA10860 1 A G
	NA10861 1 A G
	NA12761 1 ? R
	NA07019 2 A A
	NA07348 2 M G
	NA10830 2 A G
	NA10851 2 M G
	NA10857 2 A G
	NA10860 2 A G
	NA10861 2 A G
	NA12761 2 A A
	 
	In SPSS, I would create individual crosstabulation tables and any
positive integer values (counts) falling outside of the diagonal are
disagreements in calls by the 2 centers (SNP2).

	At SNP1, there are total of 7 genotype counts to compare (one DNA
did not have genotype readout). And 7/7 are in agreement.
	 
	At SNP2, all 8 DNA samples were successfully typed by both centers.
There is one discrepant call between centers.

	Q1: how do I create the same crosstabs in R?
	 
	Eventually, I would summarize the data as follows:
	 
	SNP N_to_compare N_agreement %Concordance
	snp1 7  7  100
	snp2 8  7  87.5
	etc
	 
	Q2: From the individual crosstabs tables, can I automatically
extract those numbers in columns 2 and 3?
	 
	 
	rgds,
	jenny

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From LI at nsabp.pitt.edu  Tue Jun 28 05:18:54 2005
From: LI at nsabp.pitt.edu (Li, Jia)
Date: Mon, 27 Jun 2005 23:18:54 -0400
Subject: [R] help--package:MIX
Message-ID: <3D0B2434377E984E9C85CAA316F8B183357CAF@nsabpmail>

Hi all,
 
The package MIX  is for estimation and multiple imputation for mixed categorical and continuous data. I want to use multiple imputation to impute the missing values in the covariates of Cox model, I am wondering if I could use MIX to do that. I mean that my dataset is failure time distribution, dose MIX limit the dataset only for multivariate normal datasets with missing values as package NORM?
 
Thank you for help,
 
Jia



From spencer.graves at pdf.com  Tue Jun 28 05:38:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Jun 2005 20:38:47 -0700
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <971536df05062719251e97b319@mail.gmail.com>
References: <42BF72EC.4040204@pdf.com>	
	<17087.31709.399479.651692@basebud.nulle.part>	
	<971536df050626222318376e24@mail.gmail.com>
	<42C02C2F.8070907@pdf.com>
	<971536df05062719251e97b319@mail.gmail.com>
Message-ID: <42C0C647.7070201@pdf.com>

Hi, Gabor, James, et al.:

	  Thanks for the help.  I'm unable to get "scan('clipboard', what='', 
allowEscapes=FALSE))" to work reliably on my system using Rgui under 
Windows XP, R 2.1.1 patched.  However, "file.choose()" works like a charm.

	  Thanks again, everyone.
	  spencer graves

Gabor Grothendieck wrote:
> Try using file.choose() to locate the file instead of Windows Explorer.  That 
> will return the name in a form useable within R.
> 
> On 6/27/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> 
>>         Thanks, Dirk, Gabor, Eric:
>>
>>         You all provided appropriate solutions for the stated problem.
>>Sadly, I oversimplified the problem I was trying to solve:  I copy a
>>character string giving a DOS path from MS Windows Explorer into an R
>>script file, and I get something like the following:
>>
>>         D:\spencerg\statmtds\R\Rnews
>>
>>         I want to be able to use this in R with its non-R meaning, e.g., in
>>readLine, count.fields, read.table, etc., after appending a file name.
>>Your three solutions all work for my oversimplified toy example but are
>>inadequate for the problem I really want to solve.
>>
>>         Thanks,
>>         spencer graves
>>
>>Gabor Grothendieck wrote:
>>
>>
>>>On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
>>>
>>>
>>>>On 26 June 2005 at 20:30, Spencer Graves wrote:
>>>>|         How can one convert back slashes to forward slashes, e.g, changing
>>>>| "c:\a\b" to "c:/a/b"?  I tried the following:
>>>>|
>>>>|  > gsub("\\\\", "/", "c:\a\b")
>>>>| [1] "c:\a\b"
>>>>
>>>>This does work, provided you remember that single backslashed "don't exist"
>>>>as e.g. \a is a character in itself. So use doubles are you should be fine:
>>>>
>>>>
>>>>
>>>>>gsub("\\\\", "/", "c:\\a\\b")
>>>>
>>>>[1] "c:/a/b"
>>>>
>>>
>>>
>>>Also, if one finds four backslashes confusing one can avoid the use
>>>of four via any of these:
>>>
>>>gsub("[\\]", "/", "c:\\a\\b")
>>>gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
>>>chartr("\\", "/", "c:\\a\\b")
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>--
>>Spencer Graves, PhD
>>Senior Development Engineer
>>PDF Solutions, Inc.
>>333 West San Carlos Street Suite 700
>>San Jose, CA 95110, USA
>>
>>spencer.graves at pdf.com
>>www.pdf.com <http://www.pdf.com>
>>Tel:  408-938-4420
>>Fax: 408-280-7915
>>
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Tom.Mulholland at dpi.wa.gov.au  Tue Jun 28 06:24:35 2005
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 28 Jun 2005 12:24:35 +0800
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
Message-ID: <4702645135092E4497088F71D9C8F51A128BB3@afhex01.dpi.wa.gov.au>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Henrik Bengtsson
> Sent: Tuesday, 28 June 2005 2:54 AM
> To: Spencer Graves
> Cc: r-help at stat.math.ethz.ch; Dirk Eddelbuettel
> Subject: Re: [R] How to convert "c:\a\b" to "c:/a/b"?
> 
... snipped
 
>   Thus, you cannot write your program such that it fools the parser, 
> because your program is evaluated first after the parser.  In other 
> words, there is no way you can get nchar("\n") to equal 2.
> 

I had been waiting for this answer because it was the conclusion I had come to. Given that I mainly work in a windows world this has been a problem. For various reasons I receive files liberally sprinkled with such pathnames. I generally pre-process them using whatever is at hand. It's not a big problem, just annoying to have to explain to collegues that this is something R can't do. Not a good advertisment for those who have no idea about escape codes. 

However I can't believe that this problem cannot be solved. The thoughts that have come through my head are to write a c routine that effectively ignores the possibility that \n means newline and thus remaps all the escape codes into text (\\ and the character code.) 

I've never written in C which is one of the reasons that I have never attempted this. I would be interested in any thoughts about the viability of my proposal. It seems an awful lot of work (at least for someone who hasn't done this sort of stuff before) for something that can be achieved in many other ways.

Tom



From jyzz88 at gmail.com  Tue Jun 28 06:51:34 2005
From: jyzz88 at gmail.com (Luke)
Date: Tue, 28 Jun 2005 00:51:34 -0400
Subject: [R] add transformed columns into a matrix
Message-ID: <27583b4005062721515844877b@mail.gmail.com>

Dear R users,

My input is a matrix. The matrx rows are sample; the matrix columns
are features. I need an output matrix which include the original
features and some kinds of transformed features. For example, there
are only three features x1, x2, and x3 in my original matrix (input
matrix). The features of output matrix consist of base feature
"block", interaction terms "block", log-transformed feature "block",
etc., i.e., the features of output matrix are:

x1, x2, x3, x1:x2, x1:x3, x2:x3, log(x1), log(x2), log(x3), sqrt(x1),
sqrt(x2), sqrt(x3), ...

Is there any handy method in R to expand the matrix in this way?

Thanks,

-Luke



From jusung at andrew.cmu.edu  Tue Jun 28 07:07:48 2005
From: jusung at andrew.cmu.edu (Ju-Sung Lee)
Date: Tue, 28 Jun 2005 01:07:48 -0400 (EDT)
Subject: [R] GetRNGstate() crashes in Windows
Message-ID: <Pine.LNX.4.60-041.0506280053240.12581@unix44.andrew.cmu.edu>

Hi,

Has anyone managed to successfully call GetRNGstate() / PutRNGstate() 
without crashing in a Windows environment (spec. XP)?  I've compiled 
successfully using both the latest Cygwin, latest Mingw, and the version 
of Mingw suggested in "Building R for Windows" website, but when the 
executable runs, it crashes; the functions themselves can run when I omit 
GenRNGstate()/PutRNGstate, but return the same values every time.  My 
compilation command is, having copied R.dll to my current directory:

gcc -o prog.exe prog.c -I{R's include path} R.dll

I've also tried:

gcc -o prog.exe prog.c -I{R's include path} -L./ -lR

I've managed to successfully compile Rmath.dll and call R functions that 
way, but remain puzzled why the first way doesn't work.  I've read mention 
of "initializing" R (within a C program I assume) but haven't found the 
documentation that explains this.

My test program is simply:
#include <R.h>
int main() {
   GetRNGstate();
   PutRNGstate();
   return 0;
}

I've also tried #include-ing host of the other .h files.   Thanks!

Juice



From jusung at andrew.cmu.edu  Tue Jun 28 07:38:15 2005
From: jusung at andrew.cmu.edu (Ju-Sung Lee)
Date: Tue, 28 Jun 2005 01:38:15 -0400 (EDT)
Subject: [R] GetRNGstate() crashes in Windows
In-Reply-To: <Pine.LNX.4.60-041.0506280053240.12581@unix44.andrew.cmu.edu>
References: <Pine.LNX.4.60-041.0506280053240.12581@unix44.andrew.cmu.edu>
Message-ID: <Pine.LNX.4.60-041.0506280134350.12581@unix44.andrew.cmu.edu>

I think I've misread the intent of the GetRNGstate() and other API 
functions; these only work when embedded in C functions eventually called 
from R.  Apologies for the mistake.

On Tue, 28 Jun 2005, Ju-Sung Lee wrote:

> Hi,
>
> Has anyone managed to successfully call GetRNGstate() / PutRNGstate()
> without crashing in a Windows environment (spec. XP)?  I've compiled
> successfully using both the latest Cygwin, latest Mingw, and the version
> of Mingw suggested in "Building R for Windows" website, but when the
> executable runs, it crashes; the functions themselves can run when I omit
> GenRNGstate()/PutRNGstate, but return the same values every time.  My
> compilation command is, having copied R.dll to my current directory:
>
> gcc -o prog.exe prog.c -I{R's include path} R.dll
>
> I've also tried:
>
> gcc -o prog.exe prog.c -I{R's include path} -L./ -lR
>
> I've managed to successfully compile Rmath.dll and call R functions that
> way, but remain puzzled why the first way doesn't work.  I've read mention
> of "initializing" R (within a C program I assume) but haven't found the
> documentation that explains this.
>
> My test program is simply:
> #include <R.h>
> int main() {
>   GetRNGstate();
>   PutRNGstate();
>   return 0;
> }
>
> I've also tried #include-ing host of the other .h files.   Thanks!
>
> Juice
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From szlevine at nana.co.il  Tue Jun 28 08:01:37 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 28 Jun 2005 09:01:37 +0300
Subject: [R] Mixed model
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD650@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/0ea54bcd/attachment.pl

From ripley at stats.ox.ac.uk  Tue Jun 28 08:43:13 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 07:43:13 +0100 (BST)
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <4702645135092E4497088F71D9C8F51A128BB3@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128BB3@afhex01.dpi.wa.gov.au>
Message-ID: <Pine.LNX.4.61.0506280731250.27080@gannet.stats>

This is based on a false premise.  R _can_ read files containing such 
names: use readLines() or scan(allowEscapes=FALSE).  Someone else wrote 
the C for you!

The restriction is that if you pass character strings to the parser, they 
are interpreted according to the documented rules, including interpreting 
escapes.

On Tue, 28 Jun 2005, Mulholland, Tom wrote:

>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Henrik Bengtsson
>> Sent: Tuesday, 28 June 2005 2:54 AM
>> To: Spencer Graves
>> Cc: r-help at stat.math.ethz.ch; Dirk Eddelbuettel
>> Subject: Re: [R] How to convert "c:\a\b" to "c:/a/b"?
>>
> ... snipped
>
>>   Thus, you cannot write your program such that it fools the parser,
>> because your program is evaluated first after the parser.  In other
>> words, there is no way you can get nchar("\n") to equal 2.
>>
>
> I had been waiting for this answer because it was the conclusion I had 
> come to. Given that I mainly work in a windows world this has been a 
> problem. For various reasons I receive files liberally sprinkled with 
> such pathnames. I generally pre-process them using whatever is at hand. 
> It's not a big problem, just annoying to have to explain to collegues 
> that this is something R can't do. Not a good advertisment for those who 
> have no idea about escape codes.
>
> However I can't believe that this problem cannot be solved. The thoughts 
> that have come through my head are to write a c routine that effectively 
> ignores the possibility that \n means newline and thus remaps all the 
> escape codes into text (\\ and the character code.)
>
> I've never written in C which is one of the reasons that I have never 
> attempted this. I would be interested in any thoughts about the 
> viability of my proposal. It seems an awful lot of work (at least for 
> someone who hasn't done this sort of stuff before) for something that 
> can be achieved in many other ways.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 28 08:50:50 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 07:50:50 +0100 (BST)
Subject: [R] GetRNGstate() crashes in Windows
In-Reply-To: <Pine.LNX.4.60-041.0506280134350.12581@unix44.andrew.cmu.edu>
References: <Pine.LNX.4.60-041.0506280053240.12581@unix44.andrew.cmu.edu>
	<Pine.LNX.4.60-041.0506280134350.12581@unix44.andrew.cmu.edu>
Message-ID: <Pine.LNX.4.61.0506280744100.27080@gannet.stats>

On Tue, 28 Jun 2005, Ju-Sung Lee wrote:

> I think I've misread the intent of the GetRNGstate() and other API
> functions; these only work when embedded in C functions eventually called
> from R.  Apologies for the mistake.

First, the posting guide asks you to send programming questions to the 
R-devel list.

You can call some of the API functions from a C program, but you need to 
build a standalone libRmath: see src/nmath/standalone/README in the R 
sources.  But GetRNGstate is not one of them, and `Writing R Extensions' 
states clearly which they are.  See its section `Standalone Mathlib1'.

Linking like

gcc -o prog.exe prog.c -I{R's include path} R.dll

is not supported by MinGW (even though it is documented, it fails far too 
often and the reply to my bug report was that it was not supposed to 
work!).

> On Tue, 28 Jun 2005, Ju-Sung Lee wrote:
>
>> Hi,
>>
>> Has anyone managed to successfully call GetRNGstate() / PutRNGstate()
>> without crashing in a Windows environment (spec. XP)?  I've compiled
>> successfully using both the latest Cygwin, latest Mingw, and the version
>> of Mingw suggested in "Building R for Windows" website, but when the
>> executable runs, it crashes; the functions themselves can run when I omit
>> GenRNGstate()/PutRNGstate, but return the same values every time.  My
>> compilation command is, having copied R.dll to my current directory:
>>
>> gcc -o prog.exe prog.c -I{R's include path} R.dll
>>
>> I've also tried:
>>
>> gcc -o prog.exe prog.c -I{R's include path} -L./ -lR
>>
>> I've managed to successfully compile Rmath.dll and call R functions that
>> way, but remain puzzled why the first way doesn't work.  I've read mention
>> of "initializing" R (within a C program I assume) but haven't found the
>> documentation that explains this.
>>
>> My test program is simply:
>> #include <R.h>
>> int main() {
>>   GetRNGstate();
>>   PutRNGstate();
>>   return 0;
>> }
>>
>> I've also tried #include-ing host of the other .h files.   Thanks!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ajmi_isg at yahoo.fr  Mon Jun 27 21:36:38 2005
From: ajmi_isg at yahoo.fr (noomen ajmi)
Date: Mon, 27 Jun 2005 21:36:38 +0200 (CEST)
Subject: [R] Help
Message-ID: <20050627193638.55310.qmail@web26507.mail.ukl.yahoo.com>

Hi,
Please, can you tell if there are any package or R
code for STAR models estimation and test
misspecification.
Thanks in advance
Best regards
AJMI Noomen
PHD Student
TUNISIA



From hb at maths.lth.se  Tue Jun 28 09:09:04 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 28 Jun 2005 09:09:04 +0200
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <4702645135092E4497088F71D9C8F51A128BB3@afhex01.dpi.wa.gov.au>
References: <4702645135092E4497088F71D9C8F51A128BB3@afhex01.dpi.wa.gov.au>
Message-ID: <42C0F790.7030301@maths.lth.se>

Mulholland, Tom wrote:
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Henrik Bengtsson
>>Sent: Tuesday, 28 June 2005 2:54 AM
>>To: Spencer Graves
>>Cc: r-help at stat.math.ethz.ch; Dirk Eddelbuettel
>>Subject: Re: [R] How to convert "c:\a\b" to "c:/a/b"?
>>
> 
> ... snipped
>  
> 
>>  Thus, you cannot write your program such that it fools the parser, 
>>because your program is evaluated first after the parser.  In other 
>>words, there is no way you can get nchar("\n") to equal 2.
>>
> 
> 
> I had been waiting for this answer because it was the conclusion I had come to. Given that I mainly work in a windows world this has been a problem. For various reasons I receive files liberally sprinkled with such pathnames. I generally pre-process them using whatever is at hand. It's not a big problem, just annoying to have to explain to collegues that this is something R can't do. Not a good advertisment for those who have no idea about escape codes. 

Please, note this basically only applies to source(), expressions at the 
R prompt (and unfortunately read.table()), and therefore you should not 
have to pre-process you files.  Here are some illustrating example. It 
is a good exercise to convince yourself that you understand why you get 
the different results;

code <- "x <- \"D:\\spencerg\\statmtds\\R\\Rnews\""
cat(file="foo.R", code)

file.show("foo.R")   # x <- "D:\spencerg\statmtds\R\Rnews"

x <- NA
eval(parse(text=code))
print(x)
rm(x)
[1] "D:spencergstatmtdsRRnews"

source("foo.R")
print(x)
[1] "D:spencergstatmtdsRRnews"

print(readLines("foo.R"))
[1] "x <- \"D:\\spencerg\\statmtds\\R\\Rnews\""

print(scan("foo.R", what=character(0), allowEscapes=FALSE))
[1] "x"                        "<-"
[3] "D:spencergstatmtdsRRnews"

print(read.table("foo.R"))
   V1 V2                       V3
1  x <- D:spencergstatmtdsRRnews

print(readChar("foo.R", nchar=256))
[1] "x <- \"D:\\spencerg\\statmtds\\R\\Rnews\""

 > print(readBin("foo.R", what=integer(0), size=1, n=256))
[1]  120  32  60  45  32  34  68  58  92 115 112 101 110  99 101 114 103 
  92 115
[20] 116  97 116 109 116 100 115  92  82  92  82 110 101 119 115  34


Comment/suggestion: It would be nice if read.table() would pass argument 
'allowEscapes' (or just '...') to scan().

/Henrik

> However I can't believe that this problem cannot be solved. The thoughts that have come through my head are to write a c routine that effectively ignores the possibility that \n means newline and thus remaps all the escape codes into text (\\ and the character code.) 
> 
> I've never written in C which is one of the reasons that I have never attempted this. I would be interested in any thoughts about the viability of my proposal. It seems an awful lot of work (at least for someone who hasn't done this sort of stuff before) for something that can be achieved in many other ways.
> 
> Tom
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From pir2.jv at wanadoo.fr  Tue Jun 28 09:20:54 2005
From: pir2.jv at wanadoo.fr (pir2.jv)
Date: Tue, 28 Jun 2005 09:20:54 +0200
Subject: [R] .ts
Message-ID: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>

Goog morning!
How to construct a time serie in hours or in minuts, ...
The examples are all in years
Excuse for questions so basic!

Jver
pir2.jv at wanadoo.fr



From fernando.tusell at ehu.es  Tue Jun 28 09:24:41 2005
From: fernando.tusell at ehu.es (F.Tusell)
Date: Tue, 28 Jun 2005 07:24:41 +0000
Subject: [R] help--package:MIX
Message-ID: <1119943481.4355.3.camel@agesi.bs.ehu.es>

The general location model is assumed: continuous variables are assumed
normal conditional on the categorical variables. ft.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica       fernando.tusell at ehu.es
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From ajmi_isg at yahoo.fr  Tue Jun 28 09:44:15 2005
From: ajmi_isg at yahoo.fr (noomen ajmi)
Date: Tue, 28 Jun 2005 09:44:15 +0200 (CEST)
Subject: [R] STAR models estimation with R
Message-ID: <20050628074415.43091.qmail@web26509.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/1c84d06a/attachment.pl

From christoph.lehmann at gmx.ch  Tue Jun 28 09:53:21 2005
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 28 Jun 2005 09:53:21 +0200 (MEST)
Subject: [R] nonparametric 2way repeated-measures anova
Message-ID: <28781.1119945201@www11.gmx.net>

Dear useRs
is there any nonparametric test for the analysis of variance in a design
with two within-factors (repeated measures on both factors)? Friedman is not
appropriate here, therefore I am grateful for any alternative test.

thanks for any hint
cheers
christoph

--



From wangtong at usc.edu  Tue Jun 28 10:05:23 2005
From: wangtong at usc.edu (tong wang)
Date: Tue, 28 Jun 2005 01:05:23 -0700
Subject: [R] How to import data as numeric array?
Message-ID: <5d960aa73ed.42c0a253@usc.edu>

Did some search but couldn't find useful result.

I am trying to read a n*m dimension data with read.table,  what i need is a numeric array, 
is there any efficient way to allow me get this array directly instead of a list? 
I tried to use as.array() to change the mode, but seems it doesn't work and i got this error message:

"Error in "dimnames<-.data.frame"(`*tmp*`, value = list(function (M)  : 
        invalid dimnames given for data frame "
 
what's wrong with the dimension names , should i delete them? how can i do that?

Thanks a lot for any help.

tong wang



From Torsten.Hothorn at rzmail.uni-erlangen.de  Tue Jun 28 10:18:38 2005
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Tue, 28 Jun 2005 10:18:38 +0200 (CEST)
Subject: [R] [R-pkgs] New package `party': A Laboratory for
	Recursive	Part(y)itioning
Message-ID: <Pine.LNX.4.51.0506281014210.28909@artemis.imbe.med.uni-erlangen.de>


Dear useRs,

a new package for tree-structured regression is available on CRAN.

This package implements a unified framework for recursive partitioning
which embeds tree-structured regression models into a well defined
theory of conditional inference procedures. Stopping criteria based on
multiple test procedures are implemented. The methodology is applicable
to all kinds of regression problems, including nominal, ordinal, numeric,
censored as well as multivariate response variables and arbitrary
measurement scales of the covariates. Extensible functionality for
visualizing tree-structured regression models is available.

Best,

Torsten

__________________________________________________________________________


Package: party
Title: A Laboratory for Recursive Part(y)itioning
Date: $Date: 2005/06/27 06:38:16 $
Version: 0.2-2
Author: Torsten Hothorn, Kurt Hornik and Achim Zeileis
Maintainer: Torsten Hothorn <Torsten.Hothorn at R-project.org>
Description: Unbiased recursive partitioning in a conditional
  inference framework.
Depends: R (>= 2.0.1), survival, grid, modeltools, coin
Suggests: ipred
SaveImage: yes
License: GPL
Packaged: Sun Jun 12 10:38:21 2005; hothorn

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Tue Jun 28 10:49:16 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 09:49:16 +0100 (BST)
Subject: [R] How to import data as numeric array?
In-Reply-To: <5d960aa73ed.42c0a253@usc.edu>
References: <5d960aa73ed.42c0a253@usc.edu>
Message-ID: <Pine.LNX.4.61.0506280944060.1341@gannet.stats>

Please consult the `R Data Import/Export Manual'.  You could be using 
scan(), as in

   matrix(scan("some_file", 0), nrow=n, byrow=TRUE)

On Tue, 28 Jun 2005, tong wang wrote:

> Did some search but couldn't find useful result.
>
> I am trying to read a n*m dimension data with read.table, what i need is 
> a numeric array, is there any efficient way to allow me get this array 
> directly instead of a list? I tried to use as.array() to change the 
> mode, but seems it doesn't work and i got this error message:
>
> "Error in "dimnames<-.data.frame"(`*tmp*`, value = list(function (M)  :
>        invalid dimnames given for data frame "
>
> what's wrong with the dimension names , should i delete them? how can i do that?

I think you wanted as.matrix here, not as.array: probably the call to the 
latter was incorrect, but we were not shown it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From szlevine at nana.co.il  Tue Jun 28 11:41:15 2005
From: szlevine at nana.co.il (Stephen)
Date: Tue, 28 Jun 2005 12:41:15 +0300
Subject: [R] Mixed model
Message-ID: <E76EB96029DCAE4A9CB967D7F6712D1DBFD651@NANAMAILBACK1.nanamail.co.il>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/2e33dfa5/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Tue Jun 28 11:38:17 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 28 Jun 2005 11:38:17 +0200
Subject: [R] .ts
In-Reply-To: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>
References: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>
Message-ID: <20050628113817.34ab4852.Achim.Zeileis@wu-wien.ac.at>

On Tue, 28 Jun 2005 09:20:54 +0200 pir2.jv wrote:

> Goog morning!
> How to construct a time serie in hours or in minuts, ...
> The examples are all in years
> Excuse for questions so basic!

Look at the zoo package for time series with arbitrary time scale (e.g.,
"Date", "chron", "POSIXct", ...). The zoo vignette also explains the
relation to other irregular time series classes available in package
its, fCalendar and tseries.
Z

> Jver
> pir2.jv at wanadoo.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From karen at biology.biol.wits.ac.za  Tue Jun 28 13:13:31 2005
From: karen at biology.biol.wits.ac.za (Karen Kotschy)
Date: Tue, 28 Jun 2005 13:13:31 +0200
Subject: [R] enhanced multidimensional scaling?
Message-ID: <200506281313.31121.karen@gecko.biol.wits.ac.za>

Dear R list

Would anyone be able to tell me whether it is possible to do "enhanced 
multidimensional scaling" (enhanced MDS) in R? In other words, something that 
goes beyond "cmdscale" by iteratively improving the fit between observed 
dissimilarities and inter-object distances, using the KYST algorithm 
(Kruskal, 1964).

I have found several implementations of non-metric MDS in various packages but 
nothing like what I have described above.

Thanks in advance
-- 
Karen Kotschy
Centre for Water in the Environment
University of the Witwatersrand
Johannesburg

P/Bag X3, Wits, 2050
Tel: +2711 717-6425



From ripley at stats.ox.ac.uk  Tue Jun 28 11:53:11 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 10:53:11 +0100 (BST)
Subject: [R] .ts
In-Reply-To: <20050628113817.34ab4852.Achim.Zeileis@wu-wien.ac.at>
References: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>
	<20050628113817.34ab4852.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0506281050440.2024@gannet.stats>

On Tue, 28 Jun 2005, Achim Zeileis wrote:

> On Tue, 28 Jun 2005 09:20:54 +0200 pir2.jv wrote:
>
>> Goog morning!
>> How to construct a time serie in hours or in minuts, ...
>> The examples are all in years
>> Excuse for questions so basic!
>
> Look at the zoo package for time series with arbitrary time scale (e.g.,
> "Date", "chron", "POSIXct", ...). The zoo vignette also explains the
> relation to other irregular time series classes available in package
> its, fCalendar and tseries.

However, this might be a regular time series.  For an example, see
example(beav1, package="MASS").  This a series measured every 10 mins.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Tue Jun 28 10:44:35 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 28 Jun 2005 16:44:35 +0800
Subject: [R] How to import data as numeric array?
In-Reply-To: <5d960aa73ed.42c0a253@usc.edu>
References: <5d960aa73ed.42c0a253@usc.edu>
Message-ID: <20050628164435.165bf360@localhost.localdomain>

maybe you can use the array function,like
>array(CO2,dim(CO2),list(rownames(CO2),colnames(CO2)))
and matrix is just a specif type of array,so maybe you can use as.matrix
>as.matrix(CO2)

the above tow,the CO2 is a data.frame which can use read.table to read in.

the third way is:use the scan the read the data and use array to change the data to array.


On Tue, 28 Jun 2005 01:05:23 -0700
tong wang <wangtong at usc.edu> wrote:

> Did some search but couldn't find useful result.
> 
> I am trying to read a n*m dimension data with read.table,  what i need is a numeric array, 
> is there any efficient way to allow me get this array directly instead of a list? 
> I tried to use as.array() to change the mode, but seems it doesn't work and i got this error message:
> 
> "Error in "dimnames<-.data.frame"(`*tmp*`, value = list(function (M)  : 
>         invalid dimnames given for data frame "
>  
> what's wrong with the dimension names , should i delete them? how can i do that?
> 
> Thanks a lot for any help.
> 
> tong wang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From Achim.Zeileis at wu-wien.ac.at  Tue Jun 28 11:52:13 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 28 Jun 2005 11:52:13 +0200
Subject: [R] .ts
In-Reply-To: <Pine.LNX.4.61.0506281050440.2024@gannet.stats>
References: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>
	<20050628113817.34ab4852.Achim.Zeileis@wu-wien.ac.at>
	<Pine.LNX.4.61.0506281050440.2024@gannet.stats>
Message-ID: <20050628115213.4bf02c44.Achim.Zeileis@wu-wien.ac.at>

On Tue, 28 Jun 2005 10:53:11 +0100 (BST) Prof Brian Ripley wrote:

> On Tue, 28 Jun 2005, Achim Zeileis wrote:
> 
> > On Tue, 28 Jun 2005 09:20:54 +0200 pir2.jv wrote:
> >
> >> Goog morning!
> >> How to construct a time serie in hours or in minuts, ...
> >> The examples are all in years
> >> Excuse for questions so basic!
> >
> > Look at the zoo package for time series with arbitrary time scale
> > (e.g.,"Date", "chron", "POSIXct", ...). The zoo vignette also
> > explains the relation to other irregular time series classes
> > available in package its, fCalendar and tseries.
> 
> However, this might be a regular time series.  For an example, see
> example(beav1, package="MASS").  This a series measured every 10 mins.

...which you could handle by creating a regular zoo series (of class
"zooreg").
Z

> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From karen at biology.biol.wits.ac.za  Tue Jun 28 13:38:34 2005
From: karen at biology.biol.wits.ac.za (Karen Kotschy)
Date: Tue, 28 Jun 2005 13:38:34 +0200
Subject: [R] enhanced MDS
Message-ID: <200506281338.34767.karen@gecko.biol.wits.ac.za>

Hi again

Sorry, in looking again at sammon and isoMDS I see that they seem to do 
exactly what I want, except that they are non-metric, which means, as I 
understand it, that they relate the rank orders of the variables rather than 
the actual distances. 

Could I use these non-metric MDS packages even if my distances are metric?

Thanks
Karen
-- 
Karen Kotschy
Centre for Water in the Environment
University of the Witwatersrand
Johannesburg

P/Bag X3, Wits, 2050
Tel: +2711 717-6425



From ripley at stats.ox.ac.uk  Tue Jun 28 12:26:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 11:26:14 +0100 (BST)
Subject: [R] .ts
In-Reply-To: <20050628115213.4bf02c44.Achim.Zeileis@wu-wien.ac.at>
References: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>
	<20050628113817.34ab4852.Achim.Zeileis@wu-wien.ac.at>
	<Pine.LNX.4.61.0506281050440.2024@gannet.stats>
	<20050628115213.4bf02c44.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <Pine.LNX.4.61.0506281118370.2364@gannet.stats>

On Tue, 28 Jun 2005, Achim Zeileis wrote:

> On Tue, 28 Jun 2005 10:53:11 +0100 (BST) Prof Brian Ripley wrote:
>
>> On Tue, 28 Jun 2005, Achim Zeileis wrote:
>>
>>> On Tue, 28 Jun 2005 09:20:54 +0200 pir2.jv wrote:
>>>
>>>> Goog morning!
>>>> How to construct a time serie in hours or in minuts, ...
>>>> The examples are all in years
>>>> Excuse for questions so basic!
>>>
>>> Look at the zoo package for time series with arbitrary time scale
>>> (e.g.,"Date", "chron", "POSIXct", ...). The zoo vignette also
>>> explains the relation to other irregular time series classes
>>> available in package its, fCalendar and tseries.
>>
>> However, this might be a regular time series.  For an example, see
>> example(beav1, package="MASS").  This a series measured every 10 mins.
>
> ...which you could handle by creating a regular zoo series (of class
> "zooreg").

Yes, but the standard ts analysis functions are for objects of class "ts", 
and those suffice for regular time series.

Some work for class zooreg and some do not.  (More would if zooreg had a 
tsp() method.)

> Z
>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 28 12:35:14 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 11:35:14 +0100 (BST)
Subject: [R] enhanced MDS
In-Reply-To: <200506281338.34767.karen@gecko.biol.wits.ac.za>
References: <200506281338.34767.karen@gecko.biol.wits.ac.za>
Message-ID: <Pine.LNX.4.61.0506281133340.2364@gannet.stats>

On Tue, 28 Jun 2005, Karen Kotschy wrote:

> Hi again
>
> Sorry, in looking again at sammon and isoMDS I see that they seem to do
> exactly what I want, except that they are non-metric, which means, as I
> understand it, that they relate the rank orders of the variables rather than
> the actual distances.
>
> Could I use these non-metric MDS packages even if my distances are metric?

Yes.

BTW, Sammon is not ordinal.  What `non-metric' means depends on who is 
using the term.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Allan at STATS.uct.ac.za  Tue Jun 28 12:43:44 2005
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Tue, 28 Jun 2005 12:43:44 +0200
Subject: [R] solving equation system
References: <E1DlQJp-00031h-Q1@s2.stud.uni-goettingen.de>
	<42BAE3A6.3010604@pdf.com>
Message-ID: <42C129E0.BBC850@STATS.uct.ac.za>

HI ALL

i would like to solve a complex set of equations. i have four parameters
and four equations. i could set up more equations since they are derived
from the momnets of a particular distribution.

the parameters are NON LINEAR!!!

AND the eqautions are of the form:

phi(i)=function(a,x,y,z)

is there a package or group of commands that might be used in order to
solve the system directly?

thanking you in advance

/
allan





Spencer Graves wrote:
> 
>           Have you considered writing a function to compute the sum of squares
> of deviations from equality and using "optim"?  I use sum of squares not
> sum of absolute values, because if my functions are differentiable, the
> sum of squares will also be differentible while the sum of absolute
> values will not be.  This means that sum of absolute values will not
> work well with a quasi-Newton algorithm.
> 
>           Also, have you considered making plots?  If I understand your
> example, you can solve for lambda using (II) as lambda = x/mean(X).
> Then you can use (I) to solve for "c".  To understand this, it would
> help to plot the digamma function.  If you do this (e.g.,
> http://mathworld.wolfram.com/DigammaFunction.html), you will see that
> there are countably infinite solutions to this equation.  If you want
> the positive solution, I suggest you try to solve for ln.c = log(c)
> rather than "c" directly, because that should make "optim" more stable.
>   More generally, it often helps to make, e.g., contour or perspective
> plots and to try to find a parameterization that will make the sum of
> squares of errors approximatly parabolic in your parameters.
> 
>           My favorite reference on this is Bates and Watts (1988) Nonlinear
> Regression Analysis and Its Applications (Wiley).  There may be better,
> more recent treatments of this subject, but I am not familiar with them.
> 
>           spencer graves
> p.s.  I never (no never, not ever) use "c" as a variable name, because
> it is the name of a common R function.  R is smart enough to distinguish
> between a function and a non-function in some contexts but not in all.
> When I want a name for a new object, I routinely ask R to print my
> proposed name.  If it returns "Error:  object ... not found", I can use
> "...".
> 
> Carsten Steinhoff wrote:
> 
> > Hello,
> >
> > I want to solve some two dimensional equation system with R. Some systems
> > are not solvable analytically.
> >
> > Here is an example:
> >
> > (I)        1/n*sum{from_i=1_to_n}(Xi) = ln lambda + digamma(c)
> >
> > (II)        mean(X) = x / lambda
> >
> > I want to find lambda and c,
> >
> > which R-function could do that task?
> >
> > Carsten
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

From temiz at deprem.gov.tr  Tue Jun 28 12:46:47 2005
From: temiz at deprem.gov.tr (orkun)
Date: Tue, 28 Jun 2005 13:46:47 +0300
Subject: [R] R-postgresql
Message-ID: <42C12A97.7040207@deprem.gov.tr>

hello

where can download R-postgresql from ?


regards

Ahmet Temiz

______________________________________
XamimeLT - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr
______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From ligges at statistik.uni-dortmund.de  Tue Jun 28 13:06:59 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Jun 2005 13:06:59 +0200
Subject: [R] R-postgresql
In-Reply-To: <42C12A97.7040207@deprem.gov.tr>
References: <42C12A97.7040207@deprem.gov.tr>
Message-ID: <42C12F53.9090306@statistik.uni-dortmund.de>

orkun wrote:

> hello
> 
> where can download R-postgresql from ?

Please read the posting guide.
What is "R-postgresql"? For which version of R on which platform?

The following two web pages might help you to precise your question:
  http://www.omegahat.org/RSPostgres/
  http://rpgsql.sourceforge.net/

Uwe Ligges

> 
> regards
> 
> Ahmet Temiz
> 
> ______________________________________
> XamimeLT - installed on mailserver for domain @deprem.gov.tr
> Queries to: postmaster at deprem.gov.tr
> ______________________________________
> The views and opinions expressed in this e-mail message are ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bhx2 at mevik.net  Tue Jun 28 13:12:27 2005
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Tue, 28 Jun 2005 13:12:27 +0200
Subject: [R] add transformed columns into a matrix
In-Reply-To: <27583b4005062721515844877b@mail.gmail.com> (jyzz88@gmail.com's
	message of "Tue, 28 Jun 2005 00:51:34 -0400")
References: <27583b4005062721515844877b@mail.gmail.com>
Message-ID: <m0r7emivl0.fsf@bar.nemo-project.org>

Supposing 'inmatrix' is a matrix with coloumn names 'x1', 'x2' and
'x3'; how about something like

model.matrix(~ (x1 + x2 + x3)^2 + log(x1) + log(x2) + log(x3) +
             sqrt(x1) + sqrt(x2) + sqrt(x3) - 1,
             as.data.frame(inmatrix))

-- 
Bj??rn-Helge Mevik



From fred-l at poleto.com  Tue Jun 28 13:31:42 2005
From: fred-l at poleto.com (Frederico Zanqueta Poleto)
Date: Tue, 28 Jun 2005 08:31:42 -0300
Subject: [R] nonparametric 2way repeated-measures anova
In-Reply-To: <28781.1119945201@www11.gmx.net>
References: <28781.1119945201@www11.gmx.net>
Message-ID: <42C1351E.9010608@poleto.com>

Cristoph:
Take a look on
Brunner, E., Domhof, S. and Langer, F. (2002). Nonparametric analysis of 
longitudinal data in factorial experiments. John Wiley & Sons, New York.
for the theory.

The authors have R and SAS code available on
http://www.ams.med.uni-goettingen.de/de/sof/ld/makros.html

Sincerely,

-- 
Frederico Zanqueta Poleto
fred at poleto.com
--
"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." J. W. Tukey



Christoph Lehmann wrote:

>Dear useRs
>is there any nonparametric test for the analysis of variance in a design
>with two within-factors (repeated measures on both factors)? Friedman is not
>appropriate here, therefore I am grateful for any alternative test.
>
>thanks for any hint
>cheers
>christoph
>
>--
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>

-- 
Frederico Zanqueta Poleto
fred at poleto.com
--
"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." J. W. Tukey



From markus.schwarz at wsl.ch  Tue Jun 28 13:46:39 2005
From: markus.schwarz at wsl.ch (Markus Schwarz)
Date: Tue, 28 Jun 2005 13:46:39 +0200
Subject: [R] index of dispersion
Message-ID: <5.2.1.1.1.20050628132720.00c04d28@mail.wsl.ch>

Hi list

I'm looking for statisitic measurements describing the pattern of points 
within a given polygon. Is there a function calculating the Index of 
Dispersion and/or are there other functions summarising an observed pattern?



Markus Schwarz
.....................................................................
Markus Schwarz
Wissenschaftliche Mitarbeiterin
Eidg. Forschungsanstalt WSL
Forschungsprogramm Musterland
Z??rcherstrasse 111
CH-8903 Birmensdorf

Telefon +41-44-739 22 87
Fax +41-44-739 22 15
markus.schwarz at wsl.ch
http://www.wsl.ch/staff/markus.schwarz/    	
.....................................................................



From pberming at research.ryerson.ca  Tue Jun 28 14:41:23 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Tue, 28 Jun 2005 08:41:23 -0400
Subject: [R] Programming Books
Message-ID: <42C14573.2080902@csca.ryerson.ca>

I am about to embark on the writing of a program for R and I wanted to 
know a good reference book that would be useful.  There has been some 
talk about S programming the Springer publication ISBN:0387989668.  Is 
that what most people are using or are there other books that might be 
dedicated to R.

Philip Bermingham



From Ted.Harding at nessie.mcc.ac.uk  Tue Jun 28 14:38:49 2005
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 28 Jun 2005 13:38:49 +0100 (BST)
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C02C2F.8070907@pdf.com>
Message-ID: <XFMail.050628133849.Ted.Harding@nessie.mcc.ac.uk>

On 27-Jun-05 Spencer Graves wrote:
>         Thanks, Dirk, Gabor, Eric:
> 
>         You all provided appropriate solutions for the stated problem. 
> Sadly, I oversimplified the problem I was trying to solve:  I copy a 
> character string giving a DOS path from MS Windows Explorer into an R 
> script file, and I get something like the following:
> 
>         D:\spencerg\statmtds\R\Rnews
> 
>         I want to be able to use this in R with its non-R meaning,
e.g., in 
> readLine, count.fields, read.table, etc., after appending a file name. 
> Your three solutions all work for my oversimplified toy example but are
> inadequate for the problem I really want to solve.

The various responses show that solving this problem directly within
R may be, well, problematic!

I'm not a perl user, but possibly Spencer Graves's speculation might
eventually be brought into a straightforward solution.

Unfortunately, for this query, I'm not a Windows users either, so
can't be authoritative about how practical the following may be for
Spencer's problem (and similar).

In the Unix world, the "string editor" program 'sed' simply mops
up problems of this kind. For example, I just did:

  echo "D:\spencerg\statmtds\R\Rnews" | sed 's@\\@/@g'

and got the response:

  D:/spencerg/statmtds/R/Rnews

Note: the parsing of the 'sed' command is as follows:

  s at x@y at g means substitute "y" for every ("g" = "global") occurrence
          of "x".

  The character to be replaced (x="\") is the escape character so
  needs to be escaped ("\\"); but apart from this it's straightforward.

  The usual separator is "/" instead of "@", but I used "@"
  to simplify things since the substitute itself is y="/".

  An alternative using "/" as separator would be

    echo "D:\spencerg\statmtds\R\Rnews" | sed 's/\\/\//g'
    D:/spencerg/statmtds/R/Rnews

  which is a bit more complicated but still straightforward
  (depending on your eyesight). Here, both x="\" and y="/"
  need to be escaped.

'sed', and a lot of other useful stuff, is available as "GNU tools"
which can be installed on Windows, and allow this kind of thing to
be very slickly done, but outside of R of course.

I also make much use of 'awk' (but you can equally use perl) for
tidying up CSV files exported from Excel, and for all sorts of
rearrangements and substitutions in data files. While Unix users
take this sort of thing for granted, I suggest to Windows users
that it could be well worth installing the GNU tools, since they
are very useful indeed.

It's not clear from Spencer's follow-up whether doing this kind
of preliminary work outside of R, and then bringing the results
into R (e.g. via the clipboard or a file) is practical in his
context. If not (i.e. all the work has to be done inside R), then
of course my sugestion above is not helpful in this case!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861
Date: 28-Jun-05                                       Time: 13:38:37
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Tue Jun 28 14:55:44 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Jun 2005 14:55:44 +0200
Subject: [R] Programming Books
In-Reply-To: <42C14573.2080902@csca.ryerson.ca>
References: <42C14573.2080902@csca.ryerson.ca>
Message-ID: <42C148D0.90305@statistik.uni-dortmund.de>

Philip Bermingham wrote:

> I am about to embark on the writing of a program for R and I wanted to 
> know a good reference book that would be useful.  There has been some 
> talk about S programming the Springer publication ISBN:0387989668.  Is 
> that what most people are using or are there other books that might be 
> dedicated to R.

Yes, this is a ggod book, quite a lot of other books, online material 
and paper are mentioned on CRAN, e.g. CRAN/other-docs.html
the latter points to, e.g.,
http://www.r-project.org/other-docs.html
and
http://www.r-project.org/doc/bib/R-publications.html

Uwe Ligges

> Philip Bermingham
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From friedm69 at msu.edu  Tue Jun 28 14:55:13 2005
From: friedm69 at msu.edu (Steven K Friedman)
Date: Tue, 28 Jun 2005 08:55:13 -0400
Subject: [R] function for cumulative occurrence of elements
Message-ID: <E1DnFcQ-0004aC-8u@sys02.mail.msu.edu>


Hello, 

I have a data set with 9700 records, and 7 parameters. 

The data were collected for a survey of forest communities.  Sample plots 
(1009) and species (139) are included in this data set. I need to determine 
how species are accumulated as new plots are considered. Basically, I want 
to develop a species area curve. 

I've included the first 20 records from the data set.  Point represents the 
plot id. The other parameters are parts of the information statistic H'. 

Using "Table", I can construct a data set that lists the occurrence of a 
species at any Point (it produces a binary 0/1 data table). From there it 
get confusing, regarding the most efficient approach to determining the 
addition of new and or repeated species occurrences. 

ptcount <-  table(sppoint.freq$species, sppoint.freq$Point) 

 From here I've played around with colSums to calculate the number of species 
at each Point.  The difficulty is determining if a species is new or 
repeated.  Also since there are 1009 points a function is needed to screen 
every Point. 

Two goals are of interest: 1) the species accumulation curve, and 2) an 
accumulation curve when random Points are considered. 

Any help would be greatly appreciated. 

Thank you
Steve Friedman 


 Point        species frequency point.list point.prop   log.prop 
point.hprime
1      7   American elm         7         27 0.25925926 -1.3499267    
0.3499810
2      7          apple         2         27 0.07407407 -2.6026897    
0.1927918
3      7   black cherry         8         27 0.29629630 -1.2163953    
0.3604134
4      7      black oak         1         27 0.03703704 -3.2958369    
0.1220680
5      7    chokecherry         1         27 0.03703704 -3.2958369    
0.1220680
6      7         oak sp         1         27 0.03703704 -3.2958369    
0.1220680
7      7 pignut hickory         1         27 0.03703704 -3.2958369    
0.1220680
8      7      red maple         1         27 0.03703704 -3.2958369    
0.1220680
9      7      white oak         5         27 0.18518519 -1.6863990    
0.3122961
10     9   black spruce         2         27 0.07407407 -2.6026897    
0.1927918
11     9    blue spruce         2         27 0.07407407 -2.6026897    
0.1927918
12     9        missing        12         27 0.44444444 -0.8109302    
0.3604134
13     9  Norway spruce         8         27 0.29629630 -1.2163953    
0.3604134
14     9   white spruce         3         27 0.11111111 -2.1972246    
0.2441361
15    12          apple         2         27 0.07407407 -2.6026897    
0.1927918
16    12   black cherry         1         27 0.03703704 -3.2958369    
0.1220680
17    12   black locust         1         27 0.03703704 -3.2958369    
0.1220680
18    12   black walnut         1         27 0.03703704 -3.2958369    
0.1220680
19    12          lilac         3         27 0.11111111 -2.1972246    
0.2441361
20    12        missing         2         27 0.07407407 -2.6026897    
0.1927918



From pcampbell at econ.bbk.ac.uk  Tue Jun 28 15:10:59 2005
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Tue, 28 Jun 2005 14:10:59 +0100
Subject: [R] STAR models estimation with R
In-Reply-To: <20050628074415.43091.qmail@web26509.mail.ukl.yahoo.com>
Message-ID: <NGECIFANPOJAGABBAEAPOECJEMAA.pcampbell@econ.bbk.ac.uk>

The only STAR model I have come across is Smooth Threshold Autoregressive
time series model, see Tong, Non Linear Time Series.  I am not aware of any
package that has implemented threshold models.

Because statistical techniques are used widely across many different
disciplines it is inevitable that naming conventions will diverge, so the
same technique may have different names in different areas of study.

Perhaps it should be added to the posting guide that most general name of a
technique should be used, and in the case of more obscure techniques a brief
reference to a standard text or paper.

Hamilton, see Time Series Analysis, uses the EM algorithm to estimate such
models so it should be possible to do in R.

HTH

Phineas Campbell



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of noomen ajmi
Sent: Tuesday, June 28, 2005 8:44 AM
To: r-help at stat.math.ethz.ch
Subject: [R] STAR models estimation with R


Hi,

Can you tell me if there are an R package or code for STAR model estimation
and test misspecification. If no, how i could do this.

Thanks in advance
Best regards
AJMI Noomen
Phd student
TUNISIA


---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From 0034058 at fudan.edu.cn  Tue Jun 28 14:55:44 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Tue, 28 Jun 2005 20:55:44 +0800
Subject: [R] where can i download the metrics package?
Message-ID: <20050628205544.07e2c645@localhost.localdomain>

i learn it from "metrics: Towards a package for doing econometrics in R"but i can not find it in cran.


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From khobson at fd9ns01.okladot.state.ok.us  Tue Jun 28 15:58:09 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Tue, 28 Jun 2005 08:58:09 -0500
Subject: [R]  where can i download the metrics package
Message-ID: <OFCB099E4C.8DC6E0E0-ON8625702E.004CB1E9-8625702E.004C9E9C@fd9ns01.okladot.state.ok.us>





Look for package Ecdat.



From BEN at SSANET.COM  Tue Jun 28 16:39:48 2005
From: BEN at SSANET.COM (Ben Fairbank)
Date: Tue, 28 Jun 2005 09:39:48 -0500
Subject: [R] Using data frames for EDA:  Insert, Change name,
	delete columns? (Newcomer's question)
Message-ID: <CA612484A337C6479EA341DF9EEE14AC03AC1BB5@hercules.ssainfo>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/0384a939/attachment.pl

From hannu.kahra at mpsgr.it  Tue Jun 28 16:42:53 2005
From: hannu.kahra at mpsgr.it (KAHRA HANNU)
Date: Tue, 28 Jun 2005 16:42:53 +0200
Subject: [R] STAR models estimation with R
Message-ID: <FFAAD1A807317540A06142D8CD168E9D54BC76@se000010010038.servinternet.local>

It is likely that here STAR means the Smooth Transition Autoregressive model which generalizes the Threshold Autoregressive (TAR) model to allow for gradual switching between regimes.

The model can be estimated in R by applying maximum likelihood (ML) estimation or Monte Carlo Markov Chain (MCMC) estimation. Use Google to find related papers.

Regards,
Hannu Kahra

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Phineas Campbell
Sent: 28. kes??kuuta 2005 15:11
To: r-help at stat.math.ethz.ch
Subject: Re: [R] STAR models estimation with R

The only STAR model I have come across is Smooth Threshold Autoregressive
time series model, see Tong, Non Linear Time Series.  I am not aware of any
package that has implemented threshold models.

Because statistical techniques are used widely across many different
disciplines it is inevitable that naming conventions will diverge, so the
same technique may have different names in different areas of study.

Perhaps it should be added to the posting guide that most general name of a
technique should be used, and in the case of more obscure techniques a brief
reference to a standard text or paper.

Hamilton, see Time Series Analysis, uses the EM algorithm to estimate such
models so it should be possible to do in R.

HTH

Phineas Campbell



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of noomen ajmi
Sent: Tuesday, June 28, 2005 8:44 AM
To: r-help at stat.math.ethz.ch
Subject: [R] STAR models estimation with R


Hi,

Can you tell me if there are an R package or code for STAR model estimation
and test misspecification. If no, how i could do this.

Thanks in advance
Best regards
AJMI Noomen
Phd student
TUNISIA


---------------------------------


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From uofiowa at gmail.com  Tue Jun 28 16:44:23 2005
From: uofiowa at gmail.com (Omar Lakkis)
Date: Tue, 28 Jun 2005 10:44:23 -0400
Subject: [R] factor to character
Message-ID: <3f87cc6d05062807445ab79bf8@mail.gmail.com>

Using RODBC, when I select from a table strings (chars and varchars)
come as factors. What is the best way, speed wise, to convert these
columns back to strings (perhaps using as.character).



From spencer.graves at pdf.com  Tue Jun 28 16:57:55 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 07:57:55 -0700
Subject: [R] .ts
In-Reply-To: <Pine.LNX.4.61.0506281118370.2364@gannet.stats>
References: <0C685A2B-797E-49C8-834D-1DD0C6E7AFA7@wanadoo.fr>	<20050628113817.34ab4852.Achim.Zeileis@wu-wien.ac.at>	<Pine.LNX.4.61.0506281050440.2024@gannet.stats>	<20050628115213.4bf02c44.Achim.Zeileis@wu-wien.ac.at>
	<Pine.LNX.4.61.0506281118370.2364@gannet.stats>
Message-ID: <42C16573.4090906@pdf.com>

	  R News (www.r-project.org -> "Documentation:  Newsletter", (4/1) June 
2004, pp. 29-32, and (2/2) June 2002, pp. 2-10 contain excellent 
articles on date-time classes and time series capabilities in R.  In 
addition, I have found ch. 14 in Venables and Ripley (2002) Modern 
Applied Statistics in S, 4th ed. (Springer) and the vignette for the zoo 
package to be excellent.

	  Greater flexibility in modeling and analysis are provided with the 
Rmetrics project (www.rmetrics.org or 
http://www.itp.phys.ethz.ch/econophysics/R/), but that also requires 
greater effort to learn.

	  spencer graves

Prof Brian Ripley wrote:

> On Tue, 28 Jun 2005, Achim Zeileis wrote:
> 
> 
>>On Tue, 28 Jun 2005 10:53:11 +0100 (BST) Prof Brian Ripley wrote:
>>
>>
>>>On Tue, 28 Jun 2005, Achim Zeileis wrote:
>>>
>>>
>>>>On Tue, 28 Jun 2005 09:20:54 +0200 pir2.jv wrote:
>>>>
>>>>
>>>>>Goog morning!
>>>>>How to construct a time serie in hours or in minuts, ...
>>>>>The examples are all in years
>>>>>Excuse for questions so basic!
>>>>
>>>>Look at the zoo package for time series with arbitrary time scale
>>>>(e.g.,"Date", "chron", "POSIXct", ...). The zoo vignette also
>>>>explains the relation to other irregular time series classes
>>>>available in package its, fCalendar and tseries.
>>>
>>>However, this might be a regular time series.  For an example, see
>>>example(beav1, package="MASS").  This a series measured every 10 mins.
>>
>>...which you could handle by creating a regular zoo series (of class
>>"zooreg").
> 
> 
> Yes, but the standard ts analysis functions are for objects of class "ts", 
> and those suffice for regular time series.
> 
> Some work for class zooreg and some do not.  (More would if zooreg had a 
> tsp() method.)
> 
> 
>>Z
>>
>>
>>>--
>>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>>1 South Parks Road,                     +44 1865 272866 (PA)
>>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>
>>
>>
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From s-plus at wiwi.uni-bielefeld.de  Tue Jun 28 16:58:38 2005
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 28 Jun 2005 16:58:38 +0200
Subject: [R] R demos
References: <1119623920.14681.58.camel@localhost.localdomain>
Message-ID: <42C1659E.2030408@wiwi.uni-bielefeld.de>

Federico Calboli wrote:

>Hi All,
>
>I am currently preparing some form of slideshow introducing R and its
>capabilities for some colleagues. The thing will be about 30 mins, and
>I'd like to have some "pretty pictures" and some "amazing facts" (I'm
>trying to sell, obviously :)).
>
>Can I ask if it's possible to easily retrieve a gross figure of the
>number of functions in R considering the "base" install and all the
>libraries available?
>
>Apart from graphics and lattice, are there any more packages producing
>eye catching graphics (possibly with a survival analysis/epidemiological
>bend)?
>
>Cheers,
>
>Federico Calboli
>
>  
>
In the package relax you find the function slider().
The help page of slider shows a nice application: R.veil.in.the.wind()
Here are the definitions of slider and R.veil.in.the.wind:

# definition of slider
slider<-function (sl.functions, sl.names, sl.mins, sl.maxs, sl.deltas,
    sl.defaults, but.functions, but.names, no, set.no.value,
    obj.name, obj.value, reset.function, title)
{
    if (!missing(no))
        return(as.numeric(tclvalue(get(paste("slider", no, sep = ""),
            env = slider.env))))
    if (!missing(set.no.value)) {
        try(eval(parse(text = paste("tclvalue(slider", set.no.value[1],
            ")<-", set.no.value[2], sep = "")), env = slider.env))
        return(set.no.value[2])
    }
    if (!exists("slider.env"))
        slider.env <<- new.env()
    if (!missing(obj.name)) {
        if (!missing(obj.value))
            assign(obj.name, obj.value, env = slider.env)
        else obj.value <- get(obj.name, env = slider.env)
        return(obj.value)
    }
    if (missing(title))
        title <- "slider control widget"
    require(tcltk)
    nt <- tktoplevel()
    tkwm.title(nt, title)
    tkwm.geometry(nt, "+0+0")
    if (missing(sl.names))
        sl.names <- NULL
    if (missing(sl.functions))
        sl.functions <- function(...) {
        }
    for (i in seq(sl.names)) {
        eval(parse(text = paste("assign('slider", i, 
"',tclVar(sl.defaults[i]),env=slider.env)",
            sep = "")))
        tkpack(fr <- tkframe(nt))
        lab <- tklabel(fr, text = sl.names[i], width = "25")
        sc <- tkscale(fr, from = sl.mins[i], to = sl.maxs[i],
            showvalue = T, resolution = sl.deltas[i], orient = "horiz")
        tkpack(lab, sc, side = "right")
        assign("sc", sc, env = slider.env)
        eval(parse(text = paste("tkconfigure(sc,variable=slider",
            i, ")", sep = "")), env = slider.env)
        sl.fun <- if (length(sl.functions) > 1)
            sl.functions[[i]]
        else sl.functions
        if (!is.function(sl.fun))
            sl.fun <- eval(parse(text = paste("function(...){",
                sl.fun, "}")))
        tkconfigure(sc, command = sl.fun)
    }
    assign("slider.values.old", sl.defaults, env = slider.env)
    tkpack(f.but <- tkframe(nt), fill = "x")
    tkpack(tkbutton(f.but, text = "Exit", command = function() 
tkdestroy(nt)),
        side = "right")
    if (missing(reset.function))
        reset.function <- function(...) print("relax")
    if (!is.function(reset.function))
        reset.function <- eval(parse(text = paste("function(...){",
            reset.function, "}")))
    tkpack(tkbutton(f.but, text = "Reset", command = function() {
        for (i in seq(sl.names)) eval(parse(text = paste("tclvalue(slider",
            i, ")<-", sl.defaults[i], sep = "")), env = slider.env)
        reset.function()
    }), side = "right")
    if (missing(but.names))
        but.names <- NULL
    for (i in seq(but.names)) {
        but.fun <- if (length(but.functions) > 1)
            but.functions[[i]]
        else but.functions
        if (!is.function(but.fun))
            but.fun <- eval(parse(text = paste("function(...){",
                but.fun, "}")))
        tkpack(tkbutton(f.but, text = but.names[i], command = but.fun),
            side = "left")
    }
    invisible(nt)
}
# definition of R.veil.in.the.wind
 R.veil.in.the.wind<-function(){
       # Mark Hempelmann / Peter Wolf
       par(bg="blue4", col="white", col.main="white",
           col.sub="white", font.sub=2, fg="white") # set colors and fonts
       samp  <- function(N,D) N*(1/4+D)/(1/4+D*N)
       z<-outer(seq(1, 800, by=10), seq(.0025, 0.2, .0025)^2/1.96^2, 
samp) # create 3d matrix
       h<-100
       z[10:70,20:25]<-z[10:70,20:25]+h; z[65:70,26:45]<-z[65:70,26:45]+h
       z[64:45,43:48]<-z[64:45,43:48]+h; z[44:39,26:45]<-z[44:39,26:45]+h
       x<-26:59; y<-11:38; zz<-outer(x,y,"+"); zz<-zz*(65<zz)*(zz<73)
       cz<-10+col(zz)[zz>0];rz<-25+row(zz)[zz>0]; 
z[cbind(cz,rz)]<-z[cbind(cz,rz)]+h
       refresh.code<-function(...){
         theta<-slider(no=1); phi<-slider(no=2)
         
persp(x=seq(1,800,by=10),y=seq(.0025,0.2,.0025),z=z,theta=theta,phi=phi,
               scale=T, shade=.9, box=F, ltheta = 45,
               lphi = 45, col="aquamarine", border="NA",ticktype="detailed")
       }
       slider(refresh.code, c("theta", "phi"), c(0, 0),c(360, 360),c(.2, 
.2),c(85, 270)  )
     }


# now let's  test it!

R.veil.in.the.wind()



From efg at stowers-institute.org  Tue Jun 28 17:08:55 2005
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Tue, 28 Jun 2005 10:08:55 -0500
Subject: [R] Using data frames for EDA:  Insert, Change name,
	delete columns? (Newcomer's question)
References: <CA612484A337C6479EA341DF9EEE14AC03AC1BB5@hercules.ssainfo>
Message-ID: <d9roop$s7s$1@sea.gmane.org>

"Ben Fairbank" <BEN at SSANET.COM> wrote in message
news:CA612484A337C6479EA341DF9EEE14AC03AC1BB5 at hercules.ssainfo...

> I ... cannot find commands to easily insert,
> remove (delete), rename, and re-order (arbitrarily, not sort) columns.
...
> Could a reader provide a reference where such commands are
> documented?

There's a lot of info in the old R-Help postings, but searching and finding
an answer for a particular problem can be a bit of a pain.
Here's some info from some old R-Help postings that may help on your
question:

DELETE TWO COLUMNS
-------------------------------------------------------
 I have a dataframe 'd2004' and I want to remove two columns:
'd2004$concentration' and 'd2004$stade".

I could do it just as follows:

> names(d2004)

 [1] "Localite"       "Date"           "parcelle"       "maille"
"presence.plant" "concentration"  "stade.culture"
 [8] "stade"          "Trou"           "Horizon"        "Profondeur"

> d2004 <- d2004[, -c(6, 8)]

but I'd like to use column names (to avoid finding column numbers each
time).

I cannot find an easy way to operate...

I wonder why that works:
> d2004[, "concentration"]

and this don't:
> d2004 <- d2004[, -c("concentration", "stade")]



SOLUTIONS:

d2004$concentration <- NULL
d2004$stade         <- NULL

or



Newdata <- subset(d2004, select=-c(concentration,stade))







RENAMING COLUMNS
-------------------------------------------------------
This is a sample data frame:

> myData <- data.frame( col1 = 1:3, col2 = 2:4, col3 = 3:5 )

> myData

  col1 col2 col3

1    1    2    3

2    2    3    4

3    3    4    5



You can change all names by:

> names( myData )<- c( "newcol1", "newcol2", "newcol3" )

> myData

  newcol1 newcol2 newcol3

1       1       2       3

2       2       3       4

3       3       4       5



Or a single name by:

> names( myData )[ 2 ] <- "newcol2"

> myData

  col1 newcol2 col3

1    1       2    3

2    2       3    4

3    3       4    5



Or if you know the name, but not the column number:

> names( myData )[ which( names( myData ) == "newcol2" ) ] <- "verynewcol2"

> myData

  col1 verynewcol2 col3

1    1           2    3

2    2           3    4

3    3           4    5



REORDERING COLUMNS
-------------------------------------------------------
I don't have a clipping for this one, but here's what I'd try:

> myData <- data.frame( col1 = 1:3, col2 = 2:4, col3 = 3:5 )
>
> myData
  col1 col2 col3
1    1    2    3
2    2    3    4
3    3    4    5
> MyData <- myData[,c(3,1,2)]
> MyData
  col3 col1 col2
1    3    1    2
2    4    2    3
3    5    3    4


--
efg
Earl F. Glynn
Bioinformatics
Stowers Institute for Medical Research



From Achim.Zeileis at wu-wien.ac.at  Tue Jun 28 17:10:45 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 28 Jun 2005 17:10:45 +0200
Subject: [R] where can i download the metrics package?
In-Reply-To: <20050628205544.07e2c645@localhost.localdomain>
References: <20050628205544.07e2c645@localhost.localdomain>
Message-ID: <20050628171045.69c11f85.Achim.Zeileis@wu-wien.ac.at>

On Tue, 28 Jun 2005 20:55:44 +0800 ronggui wrote:

> i learn it from "metrics: Towards a package for doing econometrics in
> R"but i can not find it in cran.

I guess you are referring to the talk of Hiroyuki Kawakatsu at useR!
2004. Contact him directly to ask for a version of the package.

Some of the functionality contained in the package is by now also
provided by some CRAN packages, look at the Econometrics view for
further information:
  http://CRAN.R-project.org/src/contrib/Views/Econometrics.html
Z



From spencer.graves at pdf.com  Tue Jun 28 17:27:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 08:27:47 -0700
Subject: [R] where can i download the metrics package?
In-Reply-To: <20050628205544.07e2c645@localhost.localdomain>
References: <20050628205544.07e2c645@localhost.localdomain>
Message-ID: <42C16C73.9080203@pdf.com>

	  How about "Rmetrics", listed under "misc:  related projects" on 
"www.r-project.org".

	  spencer graves

ronggui wrote:

> i learn it from "metrics: Towards a package for doing econometrics in R"but i can not find it in cran.
> 
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From tplate at acm.org  Tue Jun 28 17:30:56 2005
From: tplate at acm.org (Tony Plate)
Date: Tue, 28 Jun 2005 09:30:56 -0600
Subject: [R] function for cumulative occurrence of elements
In-Reply-To: <E1DnFcQ-0004aC-8u@sys02.mail.msu.edu>
References: <E1DnFcQ-0004aC-8u@sys02.mail.msu.edu>
Message-ID: <42C16D30.4070401@acm.org>

I'm not entirely sure what you want, but is it "9 5 3" for this data? (9 
"new" species occur at the first point, 5 "new" at the second, and 3 
"new" at the third).  If this is right, then to get "accumulation curve 
when random Points are considered", you can probably just index rows of 
dt appropriately.

 > dd <- read.table("clipboard", header=T)
 > dd[,1:3]
    Point        species frequency
1      7   American_elm         7
2      7          apple         2
3      7   black_cherry         8
4      7      black_oak         1
5      7    chokecherry         1
6      7         oak_sp         1
7      7 pignut_hickory         1
8      7      red_maple         1
9      7      white_oak         5
10     9   black_spruce         2
11     9    blue_spruce         2
12     9        missing        12
13     9  Norway_spruce         8
14     9   white_spruce         3
15    12          apple         2
16    12   black_cherry         1
17    12   black_locust         1
18    12   black_walnut         1
19    12          lilac         3
20    12        missing         2
 > # dt: table of which species occur at which "Points"
 > dt <- table(dd$Point, dd$species)
 > # doc: for each species, the index of the "Point" where
 > # it first occurs
 > doc <- apply(dt, 2, function(x) which(x==1)[1])
 > doc
   American_elm          apple   black_cherry   black_locust      black_oak
              1              1              1              3              1
   black_spruce   black_walnut    blue_spruce    chokecherry          lilac
              2              3              2              1              3
        missing  Norway_spruce         oak_sp pignut_hickory      red_maple
              2              2              1              1              1
      white_oak   white_spruce
              1              2
 > table(doc)
doc
1 2 3
9 5 3
 >

hope this helps,

Tony Plate

Steven K Friedman wrote:
> Hello, 
> 
> I have a data set with 9700 records, and 7 parameters. 
> 
> The data were collected for a survey of forest communities.  Sample plots 
> (1009) and species (139) are included in this data set. I need to determine 
> how species are accumulated as new plots are considered. Basically, I want 
> to develop a species area curve. 
> 
> I've included the first 20 records from the data set.  Point represents the 
> plot id. The other parameters are parts of the information statistic H'. 
> 
> Using "Table", I can construct a data set that lists the occurrence of a 
> species at any Point (it produces a binary 0/1 data table). From there it 
> get confusing, regarding the most efficient approach to determining the 
> addition of new and or repeated species occurrences. 
> 
> ptcount <-  table(sppoint.freq$species, sppoint.freq$Point) 
> 
>  From here I've played around with colSums to calculate the number of species 
> at each Point.  The difficulty is determining if a species is new or 
> repeated.  Also since there are 1009 points a function is needed to screen 
> every Point. 
> 
> Two goals are of interest: 1) the species accumulation curve, and 2) an 
> accumulation curve when random Points are considered. 
> 
> Any help would be greatly appreciated. 
> 
> Thank you
> Steve Friedman 
> 
> 
>  Point        species frequency point.list point.prop   log.prop 
> point.hprime
> 1      7   American elm         7         27 0.25925926 -1.3499267    
> 0.3499810
> 2      7          apple         2         27 0.07407407 -2.6026897    
> 0.1927918
> 3      7   black cherry         8         27 0.29629630 -1.2163953    
> 0.3604134
> 4      7      black oak         1         27 0.03703704 -3.2958369    
> 0.1220680
> 5      7    chokecherry         1         27 0.03703704 -3.2958369    
> 0.1220680
> 6      7         oak sp         1         27 0.03703704 -3.2958369    
> 0.1220680
> 7      7 pignut hickory         1         27 0.03703704 -3.2958369    
> 0.1220680
> 8      7      red maple         1         27 0.03703704 -3.2958369    
> 0.1220680
> 9      7      white oak         5         27 0.18518519 -1.6863990    
> 0.3122961
> 10     9   black spruce         2         27 0.07407407 -2.6026897    
> 0.1927918
> 11     9    blue spruce         2         27 0.07407407 -2.6026897    
> 0.1927918
> 12     9        missing        12         27 0.44444444 -0.8109302    
> 0.3604134
> 13     9  Norway spruce         8         27 0.29629630 -1.2163953    
> 0.3604134
> 14     9   white spruce         3         27 0.11111111 -2.1972246    
> 0.2441361
> 15    12          apple         2         27 0.07407407 -2.6026897    
> 0.1927918
> 16    12   black cherry         1         27 0.03703704 -3.2958369    
> 0.1220680
> 17    12   black locust         1         27 0.03703704 -3.2958369    
> 0.1220680
> 18    12   black walnut         1         27 0.03703704 -3.2958369    
> 0.1220680
> 19    12          lilac         3         27 0.11111111 -2.1972246    
> 0.2441361
> 20    12        missing         2         27 0.07407407 -2.6026897    
> 0.1927918
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From buser at stat.math.ethz.ch  Tue Jun 28 17:32:28 2005
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Tue, 28 Jun 2005 17:32:28 +0200
Subject: [R] ks.test() output interpretation
In-Reply-To: <20050627211717.70116.qmail@web30001.mail.mud.yahoo.com>
References: <20050627211717.70116.qmail@web30001.mail.mud.yahoo.com>
Message-ID: <17089.28044.385702.324932@stat.math.ethz.ch>

Hi

I would recommend graphical methods to compare two samples from 
possible different distributions. See ?qqplot
Since the Kolmogorov-Smirnov test has in many cases very small
power, you can not conclude that two sample come from the same
distribution only because the ks.test is not significant.

The following example shows you one problem:
In a short simulation we generate 1000 times two samples (with
100 observation per sample). The first sample has a standard
normal distribution, the second a t-distribution with 1 degree
of freedom. For each of these 1000 pairs we calculate the
ks.test and save the p.value.

x1 <- matrix(nrow = 100, ncol = 1000)
y1 <- matrix(nrow = 100, ncol = 1000)
test1 <- numeric(1000)
for(i in 1:1000) {
  set.seed(i)
  x1[,i] <- rnorm(100)
  y1[,i] <- rt(100, df = 1)
  test1[i] <- ks.test(x1[,i],y1[,i])$p.value
}
sum(test1<0.05)


Only in 309 of 1000 cases the test shows a significant
difference of the two samples. In all other cases we would
conclude that the two sample have the same distribution.
This is an example with 100 observation per group. If you have
smaller groups the power is even worse.

If we look at 10 randomly drawn pairs of the 1000 simulations
and plot the qqplot:

par(mfrow = c(3,3))
ind <- sample(1:1000, 9)
tmp <- sapply(ind, function(j) qqplot(x1[,j],y1[,j], xlab = paste("x1[,",j,"]"),
                                      ylab = paste("y1[,",j,"]")))

In many cases we see that the two distributions are
different. Compare it to the qqplot of two normal distributed
random variables:

x2 <- matrix(rnorm(900), nrow = 100, ncol = 9)
y2 <- matrix(rnorm(900), nrow = 100, ncol = 9)
par(mfrow = c(3,3))
tmp <- sapply(1:9, function(j) qqplot(x2[,j],y2[,j], xlab = paste("x2[,",j,"]"),
                                      ylab = paste("y2[,",j,"]")))

Of course there are situations for which the graphical methods
fail, too, but it becomes apparent that it is a descriptive way
to describe two distributions.
Calculating the Kolmogorov-Smirnov test pretends a clear test
result (that two distribution are the same) which is wrong or at
least misleading.

Best regards,

Christoph Buser

--------------------------------------------------------------
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO C13
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-44-632-4673		fax: 632-1228
http://stat.ethz.ch/~buser/
--------------------------------------------------------------


kapo coulibaly writes:
 > I'm using ks.test() to compare two different
 > measurement methods. I don't really know how to
 > interpret the output in the absence of critical value
 > table of the D statistic. I guess I could use the
 > p-value when available. But I also get the message
 > "cannot compute correct p-values with ties ..." does
 > it mean I can't use ks.test() for these data or I can
 > still use the D statistic computed to make a decision
 > whether the two samples come from the same
 > distribution.
 > 
 > Thanks!!
 > 
 > 
 > 		
 > ____________________________________________________ 
 > 
 > Rekindle the Rivalries. Sign up for Fantasy Football
 > 
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hb at maths.lth.se  Tue Jun 28 17:37:45 2005
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 28 Jun 2005 17:37:45 +0200
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <XFMail.050628133849.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.050628133849.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <42C16EC9.6070909@maths.lth.se>

(Ted Harding) wrote:
> On 27-Jun-05 Spencer Graves wrote:
> 
>>        Thanks, Dirk, Gabor, Eric:
>>
>>        You all provided appropriate solutions for the stated problem. 
>>Sadly, I oversimplified the problem I was trying to solve:  I copy a 
>>character string giving a DOS path from MS Windows Explorer into an R 
>>script file, and I get something like the following:
>>
>>        D:\spencerg\statmtds\R\Rnews
>>
>>        I want to be able to use this in R with its non-R meaning,
> 
> e.g., in 
> 
>>readLine, count.fields, read.table, etc., after appending a file name. 
>>Your three solutions all work for my oversimplified toy example but are
>>inadequate for the problem I really want to solve.
> 
> 
> The various responses show that solving this problem directly within
> R may be, well, problematic!
> 
> I'm not a perl user, but possibly Spencer Graves's speculation might
> eventually be brought into a straightforward solution.
> 
> Unfortunately, for this query, I'm not a Windows users either, so
> can't be authoritative about how practical the following may be for
> Spencer's problem (and similar).
> 
> In the Unix world, the "string editor" program 'sed' simply mops
> up problems of this kind. For example, I just did:
> 
>   echo "D:\spencerg\statmtds\R\Rnews" | sed 's@\\@/@g'
 >
> and got the response:
> 
>   D:/spencerg/statmtds/R/Rnews
> 
> Note: the parsing of the 'sed' command is as follows:
> 
>   s at x@y at g means substitute "y" for every ("g" = "global") occurrence
>           of "x".
> 
>   The character to be replaced (x="\") is the escape character so
>   needs to be escaped ("\\"); but apart from this it's straightforward.
> 
>   The usual separator is "/" instead of "@", but I used "@"
>   to simplify things since the substitute itself is y="/".
> 
>   An alternative using "/" as separator would be
> 
>     echo "D:\spencerg\statmtds\R\Rnews" | sed 's/\\/\//g'
>     D:/spencerg/statmtds/R/Rnews
> 
>   which is a bit more complicated but still straightforward
>   (depending on your eyesight). Here, both x="\" and y="/"
>   need to be escaped.
> 
> 'sed', and a lot of other useful stuff, is available as "GNU tools"
> which can be installed on Windows, and allow this kind of thing to
> be very slickly done, but outside of R of course.
> 
> I also make much use of 'awk' (but you can equally use perl) for
> tidying up CSV files exported from Excel, and for all sorts of
> rearrangements and substitutions in data files. While Unix users
> take this sort of thing for granted, I suggest to Windows users
> that it could be well worth installing the GNU tools, since they
> are very useful indeed.
> 
> It's not clear from Spencer's follow-up whether doing this kind
> of preliminary work outside of R, and then bringing the results
> into R (e.g. via the clipboard or a file) is practical in his
> context. If not (i.e. all the work has to be done inside R), then
> of course my sugestion above is not helpful in this case!

The closest you can get to the above using R (and on Windows) is

echo D:/spencerg/statmtds/R/Rnews> tmp.txt
echo cat(gsub("\\\\", "/", readLines("tmp.txt"))) | R --slave


I do have a question/request to r-devel:  In my example, I ideally would 
like to be able to start an *.R script or alternatively send R 
expression at the prompt, and then read data from stdin. Two examples:

echo D:/spencerg/statmtds/R/Rnews | R --slave --code 'cat(gsub("\\\\", 
"/", readLines("tmp.txt")))'

or

echo cat(gsub("\\\\", "/", readLines("tmp.txt"))) > main.R
echo D:/spencerg/statmtds/R/Rnews | R --slave --file main.R

Note that R CMD BATCH does not provide this.  Either you send your R 
code via standard input or the data, but you cannot send both.  If you 
want to have and dynamic interaction between two applications via stdin 
and stdout, except from ellaborating with .Rprofile/.First I'm not sure 
how to start my script in the first place.

Henrik

> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861
> Date: 28-Jun-05                                       Time: 13:38:37
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From Gregor.Gorjanc at bfro.uni-lj.si  Tue Jun 28 17:42:27 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Tue, 28 Jun 2005 17:42:27 +0200
Subject: [R] Producing character "given" i.e. "| " with plotmath
Message-ID: <7FFEE688B57D7346BC6241C55900E730F31892@pollux.bfro.uni-lj.si>

Hello!

Does someone know how to produce

  L(y|mu)

with plotmath?

Some code with unsuccessfull results:

plot(dnorm(x = seq(from = -4, to = 4, by = 0.1)), type = "l")
## Not what I want
legend(legend = c(expression(L(y:mu))), x = "topright")

## Strange, is this a bug?
legend(legend = c(expression(L(y|mu))), x = "top")

## Group produces an error
legend(legend = c(expression(group(L(y, "|", mu)))), x = "topleft")

## Paste keeps commas in expression
legend(legend = c(expression(paste(L(y, "|", mu)))), x = "bottomleft")

## This one is OK, but braces are not as they should be 
legend(legend = c(expression(paste("L(y", "|", "mu)"))), x = "bottom")

Thanks!

Lep pozdrav / With regards,
    Gregor Gorjanc

----------------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                   tel: +386 (0)1 72 17 861
SI-1230 Domzale             fax: +386 (0)1 72 17 888
Slovenia, Europe
----------------------------------------------------------------------
"One must learn by doing the thing; for though you think you know it,
 you have no certainty until you try." Sophocles ~ 450 B.C.



From ligges at statistik.uni-dortmund.de  Tue Jun 28 18:03:12 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Jun 2005 18:03:12 +0200
Subject: [R] Producing character "given" i.e. "| " with plotmath
In-Reply-To: <7FFEE688B57D7346BC6241C55900E730F31892@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E730F31892@pollux.bfro.uni-lj.si>
Message-ID: <42C174C0.5010905@statistik.uni-dortmund.de>

Gorjanc Gregor wrote:

> Hello!
> 
> Does someone know how to produce
> 
>   L(y|mu)
> 
> with plotmath?
> 
> Some code with unsuccessfull results:
> 
> plot(dnorm(x = seq(from = -4, to = 4, by = 0.1)), type = "l")
> ## Not what I want
> legend(legend = c(expression(L(y:mu))), x = "topright")
> 
> ## Strange, is this a bug?
> legend(legend = c(expression(L(y|mu))), x = "top")


No, "|" is a logical Operator that can be rewritten in its original 
function form as follows:

"|"(FALSE, TRUE)

Hence the result is expected.

> ## Group produces an error
> legend(legend = c(expression(group(L(y, "|", mu)))), x = "topleft")

You have not specified any delimiter.


> ## Paste keeps commas in expression
> legend(legend = c(expression(paste(L(y, "|", mu)))), x = "bottomleft")

correct

> ## This one is OK, but braces are not as they should be 
> legend(legend = c(expression(paste("L(y", "|", "mu)"))), x = "bottom")

What's wrong with the braces?`


What you really want is:
   legend(legend = c(expression(L(group("", y, "|") * mu))),
     x = "center")

Uwe Ligges


> Thanks!
> 
> Lep pozdrav / With regards,
>     Gregor Gorjanc
> 
> ----------------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty        URI: http://www.bfro.uni-lj.si/MR/ggorjan
> Zootechnical Department     mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                   tel: +386 (0)1 72 17 861
> SI-1230 Domzale             fax: +386 (0)1 72 17 888
> Slovenia, Europe
> ----------------------------------------------------------------------
> "One must learn by doing the thing; for though you think you know it,
>  you have no certainty until you try." Sophocles ~ 450 B.C.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Tue Jun 28 18:16:17 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 28 Jun 2005 11:16:17 -0500
Subject: [R] Possible bug in summary of residuals with lm and weights
Message-ID: <42C177D1.1030208@vanderbilt.edu>

I sent this to r-devel the other day but didn't get any takers.  This 
may not be a bug but rather an inconsistency.
I'm not sure if this is intentional.  summary.lm stores weighted
residuals whereas I think most users will want print.summary.lm to
summarize unweighted ones as if saying summary(resid(fit)).

> set.seed(1)
> dat <- data.frame(y = rnorm(15), x = rnorm(15), w = 1:15)
> f <- lm(y ~ x, weights = w, data = dat)
> summary(f)
. . . .
Residuals:
    Min     1Q Median     3Q    Max
-8.260 -1.565  0.117  2.105  4.666
. . . .
> resid(f)

           1           2           3           4           5           6
-0.73429677  0.06818092 -1.20558034  1.25783256  0.05231879 -1.18383039
           7           8           9          10          11          12
  0.16034166  0.59880438  0.98337588 -0.58944957  1.40690588  0.31138819
          13          14          15
-0.35111933 -2.20770335  0.89438636

> version
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    1.0
year     2005
month    04
day      18
language R

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ripley at stats.ox.ac.uk  Tue Jun 28 18:34:45 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 17:34:45 +0100 (BST)
Subject: [R] Possible bug in summary of residuals with lm and weights
In-Reply-To: <42C177D1.1030208@vanderbilt.edu>
References: <42C177D1.1030208@vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0506281730140.24932@gannet.stats>

On Tue, 28 Jun 2005, Frank E Harrell Jr wrote:

> I sent this to r-devel the other day but didn't get any takers.  This
> may not be a bug but rather an inconsistency.
> I'm not sure if this is intentional.  summary.lm stores weighted
> residuals whereas I think most users will want print.summary.lm to
> summarize unweighted ones as if saying summary(resid(fit)).

It seems no one agreed with you!

I think most users will want S-compatibility here, and after that
residuals that are in some sense on the same scale (that is taking the
weights into account). In short, the status quo.

I suspect no one wants the definition of the "summary.lm" class changed.

There is a bug in the print method, which has

     cat(if (!is.null(x$w) && diff(range(x$w)))
         "Weighted ", "Residuals:\n", sep = "")

so it is intended to say Weighted Residuals, but w is not in a
"summary.lm" object.


>
>> set.seed(1)
>> dat <- data.frame(y = rnorm(15), x = rnorm(15), w = 1:15)
>> f <- lm(y ~ x, weights = w, data = dat)
>> summary(f)
> . . . .
> Residuals:
>    Min     1Q Median     3Q    Max
> -8.260 -1.565  0.117  2.105  4.666
> . . . .
>> resid(f)
>
>           1           2           3           4           5           6
> -0.73429677  0.06818092 -1.20558034  1.25783256  0.05231879 -1.18383039
>           7           8           9          10          11          12
>  0.16034166  0.59880438  0.98337588 -0.58944957  1.40690588  0.31138819
>          13          14          15
> -0.35111933 -2.20770335  0.89438636
>
>> version
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    1.0
> year     2005
> month    04
> day      18
> language R
>
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angeliki.martinou at imperial.ac.uk  Tue Jun 28 18:36:28 2005
From: angeliki.martinou at imperial.ac.uk (Martinou, Angeliki)
Date: Tue, 28 Jun 2005 17:36:28 +0100
Subject: [R] (no subject)
Message-ID: <CAAD15FC4F69AC419F16028EBBCCC36FDE628E@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/e2d8325f/attachment.pl

From ripley at stats.ox.ac.uk  Tue Jun 28 18:38:03 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Jun 2005 17:38:03 +0100 (BST)
Subject: [R] factor to character
In-Reply-To: <3f87cc6d05062807445ab79bf8@mail.gmail.com>
References: <3f87cc6d05062807445ab79bf8@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506281735040.24932@gannet.stats>

On Tue, 28 Jun 2005, Omar Lakkis wrote:

> Using RODBC, when I select from a table strings (chars and varchars)
> come as factors. What is the best way, speed wise, to convert these
> columns back to strings (perhaps using as.character).

FAQ Q7.10, replacing "numeric" by "character".

You do have control in RODBC, in fact: see ?sqlFetch.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Arne.Muller at sanofi-aventis.com  Tue Jun 28 18:59:37 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Tue, 28 Jun 2005 18:59:37 +0200
Subject: [R] svm and scaling input
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3B9@CRBSMXSUSR04>

Dear All,

I've a question about scaling the input variables for an analysis with svm (package e1071). Most of my variables are factors with 4 to 6 levels but there are also some numeric variables.

I'm not familiar with the math behind svms, so my assumtions maybe completely wrong ... or obvious. Will the svm automatically expand the factors into a binary matrix? If I add numeric variables outside the range of 0 to 1 do I have to scale them to have 0 to 1 range?

thanks a lot for help,

	+kind regards,

	Arne



From 0034058 at fudan.edu.cn  Tue Jun 28 19:02:54 2005
From: 0034058 at fudan.edu.cn (ronggui)
Date: Wed, 29 Jun 2005 01:02:54 +0800
Subject: [R] Using data frames for EDA:  Insert, Change name,
	delete columns? (Newcomer's question)
In-Reply-To: <d9roop$s7s$1@sea.gmane.org>
References: <CA612484A337C6479EA341DF9EEE14AC03AC1BB5@hercules.ssainfo>
	<d9roop$s7s$1@sea.gmane.org>
Message-ID: <20050629010254.015e1420@localhost.localdomain>

if i want to rename the data frame,say data1,to data2,have i use those
>data2<-data1
>rm(dat1)
or these is simpler way to do this? thank you !

On Tue, 28 Jun 2005 10:08:55 -0500
"Earl F. Glynn" <efg at stowers-institute.org> wrote:

> "Ben Fairbank" <BEN at SSANET.COM> wrote in message
> news:CA612484A337C6479EA341DF9EEE14AC03AC1BB5 at hercules.ssainfo...
> 
> > I ... cannot find commands to easily insert,
> > remove (delete), rename, and re-order (arbitrarily, not sort) columns.
> ...
> > Could a reader provide a reference where such commands are
> > documented?
> 
> There's a lot of info in the old R-Help postings, but searching and finding
> an answer for a particular problem can be a bit of a pain.
> Here's some info from some old R-Help postings that may help on your
> question:
> 
> DELETE TWO COLUMNS
> -------------------------------------------------------
>  I have a dataframe 'd2004' and I want to remove two columns:
> 'd2004$concentration' and 'd2004$stade".
> 
> I could do it just as follows:
> 
> > names(d2004)
> 
>  [1] "Localite"       "Date"           "parcelle"       "maille"
> "presence.plant" "concentration"  "stade.culture"
>  [8] "stade"          "Trou"           "Horizon"        "Profondeur"
> 
> > d2004 <- d2004[, -c(6, 8)]
> 
> but I'd like to use column names (to avoid finding column numbers each
> time).
> 
> I cannot find an easy way to operate...
> 
> I wonder why that works:
> > d2004[, "concentration"]
> 
> and this don't:
> > d2004 <- d2004[, -c("concentration", "stade")]
> 
> 
> 
> SOLUTIONS:
> 
> d2004$concentration <- NULL
> d2004$stade         <- NULL
> 
> or
> 
> 
> 
> Newdata <- subset(d2004, select=-c(concentration,stade))
> 
> 
> 
> 
> 
> 
> 
> RENAMING COLUMNS
> -------------------------------------------------------
> This is a sample data frame:
> 
> > myData <- data.frame( col1 = 1:3, col2 = 2:4, col3 = 3:5 )
> 
> > myData
> 
>   col1 col2 col3
> 
> 1    1    2    3
> 
> 2    2    3    4
> 
> 3    3    4    5
> 
> 
> 
> You can change all names by:
> 
> > names( myData )<- c( "newcol1", "newcol2", "newcol3" )
> 
> > myData
> 
>   newcol1 newcol2 newcol3
> 
> 1       1       2       3
> 
> 2       2       3       4
> 
> 3       3       4       5
> 
> 
> 
> Or a single name by:
> 
> > names( myData )[ 2 ] <- "newcol2"
> 
> > myData
> 
>   col1 newcol2 col3
> 
> 1    1       2    3
> 
> 2    2       3    4
> 
> 3    3       4    5
> 
> 
> 
> Or if you know the name, but not the column number:
> 
> > names( myData )[ which( names( myData ) == "newcol2" ) ] <- "verynewcol2"
> 
> > myData
> 
>   col1 verynewcol2 col3
> 
> 1    1           2    3
> 
> 2    2           3    4
> 
> 3    3           4    5
> 
> 
> 
> REORDERING COLUMNS
> -------------------------------------------------------
> I don't have a clipping for this one, but here's what I'd try:
> 
> > myData <- data.frame( col1 = 1:3, col2 = 2:4, col3 = 3:5 )
> >
> > myData
>   col1 col2 col3
> 1    1    2    3
> 2    2    3    4
> 3    3    4    5
> > MyData <- myData[,c(3,1,2)]
> > MyData
>   col3 col1 col2
> 1    3    1    2
> 2    4    2    3
> 3    5    3    4
> 
> 
> --
> efg
> Earl F. Glynn
> Bioinformatics
> Stowers Institute for Medical Research
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Department of Sociology
Fudan University,Shanghai
Blog:http://sociology.yculblog.com



From ferd_principia at yahoo.ca  Tue Jun 28 19:03:06 2005
From: ferd_principia at yahoo.ca (ferdinand principia)
Date: Tue, 28 Jun 2005 13:03:06 -0400 (EDT)
Subject: [R] faster algorithm for Kendall's tau
Message-ID: <20050628170306.2942.qmail@web31201.mail.mud.yahoo.com>

Hi,

I need to calculate Kendall's tau for large data
vectors (length > 100'000). 
Is somebody aware of a faster algorithm or package
function than "cor(, method="kendall")"? 
There are ties in the data to be considered (Kendall's
tau-b).

Any suggestions?

Regards
Ferdinand



From rnews at kernstat.com  Tue Jun 28 19:15:11 2005
From: rnews at kernstat.com (Remington, Richard)
Date: Tue, 28 Jun 2005 11:15:11 -0600
Subject: [R] function for "two-part" or "two-condition" models
In-Reply-To: <20050627232243.GB600@uidaho.edu>
References: <1119900930.42c05502c3893@mail-www2.oit.umass.edu>
	<20050627232243.GB600@uidaho.edu>
Message-ID: <42C1859F.5050203@kernstat.com>


Two alternatives to the zero inflated Poisson (ZIP) model are mentioned 
in Jung, Jhun, and Lee (Biometrics, vol 61, no 2, June 2005, p626):

"Although the ZIP model is more general than the standard Poisson, count 
data with many zeros are often more dispersed than the ZIP model.  In 
this case, the use of a zero-inflated negative binomial (ZINB) 
distribution or a zero-inflated generalized Poisson distribution is a 
good alternative."

best,
Richard

-- 
Richard E. Remington
Statistician
KERN Statistical Services, Inc.
PO Box 1046
Boise, ID 83701
Tel: 208.426.0113
KernStat.com

Andrew Robinson wrote:
> Hi Richard,
> 
> I'm not sure that I can imagine how data can have too many zeros to be
> fit well with zero-inflated Poisson models. Won't the excess zeros be
> accommodated by increasing the the inflation?
> 
> In any case, if you want a model that separates the zeros from the
> occurrences before fitting a Poisson model to account for variation in
> abundance then it might be safest to do that split manually.
> 
> Another angle to try is to treat it as a special case of a finite
> mixture regression.  I think that some of Jim Lindsey's code will fit
> such models. Google can help you find his wbsite.
> 
> An MS student of mine explored these models for regeneration modeling.
> I'd be happy to send you a pdf of his thesis if it would help.
> 
> Cheers,
> 
> Andrew
> 
> On Mon, Jun 27, 2005 at 03:35:30PM -0400, Richard Chandler wrote:
> 
>>Hello,
>>
>>This is an (hopefully) improved question of one I posted several weeks
>>ago. Does anyone know of a function for fitting "two-part" models?
>>These models are designed to handle count data with so many zeroes
>>that they can't be fit well with zero-inflated Poisson models or other
>>'typical' GLMs. My understanding is that they work by first fitting a
>>binomial model to separate the zeros from the occurrences (positive
>>integers) before fitting a Poisson model to account for variation in
>>abundance. 
>>
>>I have tried help.search("two-part") and many other similar guesses.
>>
>>Thanks,
>>Richard
>>
>>-- 
>>Richard Chandler, M.S. Candidate
>>Department of Natural Resources Conservation
>>UMass Amherst
>>(413)545-1237
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From michael.peckford at acadiau.ca  Tue Jun 28 19:34:33 2005
From: michael.peckford at acadiau.ca (Michael Peckford)
Date: Tue, 28 Jun 2005 14:34:33 -0300
Subject: [R] Circular Mean Question
Message-ID: <EC6501D4A6BB5442B583E3FABC9D131F034F1191@exchange.ad.acadiau.ca>


Hi, a question about the circular mean function in the package
CircStats:

Can anyone shed some light on why the circ mean function seems to make
sense for the first 2 set of bearings and then the mean of 225 and 45
degrees gives an unexpected 180 deg. 

> deg(circ.mean(c(rad(222),rad(45))))%%360
[1] 133.5
> deg(circ.mean(c(rad(224),rad(45))))%%360
[1] 134.5
> deg(circ.mean(c(rad(225),rad(45))))%%360
[1] 180
> deg(circ.mean(c(rad(226),rad(45))))%%360
[1] 315.5

Can anyone explain this???

This problem was first detected when I was trying to take the circ
weighted means of my data:

With 2 groups of bearings:
x <- c(270,180)
y <- c(45,270)

the circular mean of these bearings gives:
> deg(circ.mean(c(rad(x),rad(y))))%%360
[1] 257.2356

When finding the weighted means I get this:
> meany <- circ.mean(rad(y))
> meanx <- circ.mean(rad(x))

> deg(circ.weighted.mean(c(meanx,meany),c(2,2)))%%360
[1] 281.25

The function for weighted mean I am using:

circ.weighted.mean <- function (x,w) 
{
    sinr <- sum(w*sin(x))
    cosr <- sum(w*cos(x))
    circmean <- atan(sinr, cosr)
    circmean
}

I am assuming that the problem that mention above is the cause of the
different mean bearings.

Am I missing something fundamental here?

Thanks,
Mike



From jose_fcunam at hotmail.com  Tue Jun 28 20:34:07 2005
From: jose_fcunam at hotmail.com (=?iso-8859-1?B?Sm9zZSBIZXJyZXJhIEJheuFu?=)
Date: Tue, 28 Jun 2005 13:34:07 -0500
Subject: [R] Brown-Forsythe Test
In-Reply-To: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
Message-ID: <BAY7-F83AA45E17B9B957A6E9A591E10@phx.gbl>

Hi,

do anyone know which statistical software have the Brown-Forsythe Test for 
differences of numerical continue data groups wich variance heterogenity? 
not ofr evaluate the homogenity of variances, for evaluated the differences 
between groups as ANOVA.
thanks

Jos?? Herrera
M??xico

_________________________________________________________________
T1msn Search. Todo lo que buscas ahora m??s rapido  
http://search.t1msn.com.mx/



From ccleland at optonline.net  Tue Jun 28 20:42:11 2005
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 28 Jun 2005 14:42:11 -0400
Subject: [R] Brown-Forsythe Test
In-Reply-To: <BAY7-F83AA45E17B9B957A6E9A591E10@phx.gbl>
References: <BAY7-F83AA45E17B9B957A6E9A591E10@phx.gbl>
Message-ID: <42C19A03.1080901@optonline.net>

?oneway.test

Jose Herrera Baz??n wrote:
> Hi,
> 
> do anyone know which statistical software have the Brown-Forsythe Test for 
> differences of numerical continue data groups wich variance heterogenity? 
> not ofr evaluate the homogenity of variances, for evaluated the differences 
> between groups as ANOVA.
> thanks
> 
> Jos?? Herrera
> M??xico
> 
> _________________________________________________________________
> T1msn Search. Todo lo que buscas ahora m??s rapido  
> http://search.t1msn.com.mx/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Tue Jun 28 21:30:43 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Jun 2005 21:30:43 +0200
Subject: [R] svm and scaling input
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3B9@CRBSMXSUSR04>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF3B9@CRBSMXSUSR04>
Message-ID: <42C1A563.1040005@statistik.uni-dortmund.de>

Arne.Muller at sanofi-aventis.com wrote:
> Dear All,
> 
> I've a question about scaling the input variables for an analysis with svm (package e1071). Most of my variables are factors with 4 to 6 levels but there are also some numeric variables.
> 
> I'm not familiar with the math behind svms, so my assumtions maybe completely wrong ... or obvious. Will the svm automatically expand the factors into a binary matrix? If I add numeric variables outside the range of 0 to 1 do I have to scale them to have 0 to 1 range?


Well, this depends on the kernel in use.
For radial basis functions (as an example), you do not have to rescale, 
but a transformation of variables might make sense in order to get 
better results, though.

Uwe Ligges


> 
> thanks a lot for help,
> 
> 	+kind regards,
> 
> 	Arne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mschwartz at mn.rr.com  Tue Jun 28 21:31:00 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 28 Jun 2005 14:31:00 -0500
Subject: [R] faster algorithm for Kendall's tau
In-Reply-To: <20050628170306.2942.qmail@web31201.mail.mud.yahoo.com>
References: <20050628170306.2942.qmail@web31201.mail.mud.yahoo.com>
Message-ID: <1119987060.4850.76.camel@localhost.localdomain>

On Tue, 2005-06-28 at 13:03 -0400, ferdinand principia wrote:
> Hi,
> 
> I need to calculate Kendall's tau for large data
> vectors (length > 100'000). 
> Is somebody aware of a faster algorithm or package
> function than "cor(, method="kendall")"? 
> There are ties in the data to be considered (Kendall's
> tau-b).
> 
> Any suggestions?
> 
> Regards
> Ferdinand


The time intensive part of the process is typically the ranking/ordering
of the vector pairs to calculate the numbers of concordant and
discordant pairs.

If the number of _unique pairs_ in your data is substantially less than
the number of total pairs (in other words, creating a smaller 2d
contingency table from a pair of your vectors makes sense), then the
following may be of help.

# Calculate CONcordant Pairs in a table
# cycle through x[r, c] and multiply by
# sum(x elements below and to the right of x[r, c])
# x = table
concordant <- function(x)
{
  x <- matrix(as.numeric(x), dim(x))
  
  # get sum(matrix values > r AND > c)
  # for each matrix[r, c]
  mat.lr <- function(r, c)
  { 
    lr <- x[(r.x > r) & (c.x > c)]
    sum(lr)
  }

  # get row and column index for each
  # matrix element
  r.x <- row(x)
  c.x <- col(x)

  # return the sum of each matrix[r, c] * sums
  # using mapply to sequence thru each matrix[r, c]
  sum(x * mapply(mat.lr, r = r.x, c = c.x))
}

# Calculate DIScordant Pairs in a table
# cycle through x[r, c] and multiply by
# sum(x elements below and to the left of x[r, c])
# x = table
discordant <- function(x)
{
  x <- matrix(as.numeric(x), dim(x))
  
  # get sum(matrix values > r AND < c)
  # for each matrix[r, c]
  mat.ll <- function(r, c)
  { 
    ll <- x[(r.x > r) & (c.x < c)]
    sum(ll)
  }

  # get row and column index for each
  # matrix element
  r.x <- row(x)
  c.x <- col(x)

  # return the sum of each matrix[r, c] * sums
  # using mapply to sequence thru each matrix[r, c]
  sum(x * mapply(mat.ll, r = r.x, c = c.x))
}


# Calculate Kendall's Tau-b
# x = table
calc.KTb <- function(x)
{
  x <- matrix(as.numeric(x), dim(x))
  
  c <- concordant(x)
  d <- discordant(x)

  n <- sum(x)
  SumR <- rowSums(x)
  SumC <- colSums(x)

  KTb <- (2 * (c - d)) / sqrt(((n ^ 2) -
         (sum(SumR ^ 2))) * ((n ^ 2) -
         (sum(SumC ^ 2))))

  KTb
}


Note that I made some modifications of the above, relative to prior
versions that I have posted to handle large numbers of pairs to avoid
integer overflows in summations. Hence the:

  x <- matrix(as.numeric(x), dim(x))

conversion in each function.

Now, create some random test data, with 100,000 elements in each vector,
sampling from 'letters', which would yield a 26 x 26 table:

 a <- sample(letters, 100000, replace = TRUE)
 b <- sample(letters, 100000, replace = TRUE)
 
 > dim(table(a, b))
 [1] 26 26

 > system.time(print(calc.KTb(table(a, b))))
[1] 0.0006906088
[1] 0.77 0.02 0.83 0.00 0.00

Note that in the above, the initial table takes most of the time:

> system.time(table(a, b))
[1] 0.55 0.00 0.56 0.00 0.00

Hence:

> tab.ab <- table(a, b)
> system.time(print(calc.KTb(tab.ab)))
[1] 0.0006906088
[1] 0.25 0.01 0.27 0.00 0.00


I should note that I also ran:

> system.time(print(cor(a, b, method = "kendall")))
[1] 0.0006906088 
[1] 694.80   7.72 931.89   0.00   0.00 

Nice to know the results work out at least...  :-)


I have not tested with substantially larger 2d matrices, but would
envision that as the dimensions of the resultant tabulation increases,
my method probably approaches and may even become less efficient than
the approach implemented in cor(). Some testing would validate this and
perhaps point to coding the concordant() and discordant() functions in C
for improvement in timing.

HTH,

Marc Schwartz



From edwardsm at exponent.com  Tue Jun 28 21:35:35 2005
From: edwardsm at exponent.com (Melanie Edwards)
Date: Tue, 28 Jun 2005 12:35:35 -0700
Subject: [R] 3D ellipsoid confidence region
Message-ID: <8E110A8F4A95AD46AEE54E8A195F0E2C01889A93@bellemail.exponent.com>

I am curious if there is code developed to plot confidence regions in 3D.
The scatterplot3d function generates the plot I want, but would like an 3D
equivalent to the data.ellipse function.

Any help in this direction would be appreciated, be it theoretical,
graphical, or otherwise.

Melanie Edwards
Senior Statistician

Exponent
15375 SE 30th PL, Suite 250
Bellevue, WA  98007
Tel:  (425) 519-8714
Fax:  (425) 643-9827
Cell:  (206) 852-5739
www.exponent.com



From sue at xlsolutions-corp.com  Tue Jun 28 21:35:40 2005
From: sue at xlsolutions-corp.com (sue@xlsolutions-corp.com)
Date: Tue, 28 Jun 2005 12:35:40 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techinques:
	Seattle July 11-12
Message-ID: <20050628193540.13346.qmail@gem-wbe06.prod.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to
announce  2-day "R/S-plus Fundamentals and Programming
Techniques" at 4 locations nationwide.
www.xlsolutions-corp.com/Rfund.htm

****Seattle, WA ------------------------- July 11th-12th, 2005


Reserve your seat now at the early bird rates! Payment due AFTER
the class

Course Description:

This two-day beginner to intermediate R/S-plus course focuses on a
broad spectrum of topics, from reading raw data to a comparison of R
and S. We will learn the essentials of data manipulation, graphical
visualization and R/S-plus programming. We will explore statistical
data analysis tools,including graphics with data sets. How to enhance
your plots, build your own packages (librairies) and connect via
ODBC,etc.
We will perform some statistical modeling and fit linear regression
models. Participants are encouraged to bring data for interactive
sessions

With the following outline:

- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)
- Connecting; ODBC, Rweb, Orca via sockets and via Rjava

Interested in R/Splus Advanced course? email us.

Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this
classto take advantage of group discount. Register now to secure your
seat!

Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From rvaradha at jhsph.edu  Tue Jun 28 21:36:22 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Tue, 28 Jun 2005 15:36:22 -0400
Subject: [R] Circular Mean Question
In-Reply-To: <EC6501D4A6BB5442B583E3FABC9D131F034F1191@exchange.ad.acadiau.ca>
Message-ID: <OWA-1f43rjW9xs0J1Dt00004471@owa-1.sph.ad.jhsph.edu>

Hi Mike,

You should think of "vector" sums rather than arithmetic sum.  So the mean
is really the direction of the resultant vector of all the unit vectors.
When two unit vectors are exactly opposing each other (as in your example
with angles 45 and 225 degrees), they cancel each other out and the
resultant vector has length zero, and hence the direction is arbitrary and
undefined.  However, there are two issues which give rise to seemingly
anomalous results:  (1) due to finite numerical precision in computing the
sines and cosines, exact cancellation does not happen, and (2) discontinuity
in atan function due to branch-point (0, 0) and a branch cut along negative
real axis.

For example,
> deg(circ.mean(c(rad(30),rad(210))))
[1] -56.30993247402022
> sx <- sin(rad(210)) + sin(rad(30))
> cx <- cos(rad(210)) + cos(rad(30))
> sx
[1] -1.665334536937735e-16
> cx
[1] 1.110223024625157e-16
> atan(sx,cx)  # this should really be 0
[1] -0.9827937232473291
>
If you now look at the resultant vector it has components (sin(-0.9828),
cos(-0.9828)) = (-0.83,0.55), which is completely wrong.  

However, the above example is only a minor perturbation away from the
following (where the resultant vector is not zero):
> deg(circ.mean(c(rad(211),rad(30)))) # note 211 = -149 (mod 360)
[1] -59.49999999999996
which is perfectly alright.

Hope this helps,
Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Michael Peckford
> Sent: Tuesday, June 28, 2005 1:35 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Circular Mean Question
> 
> 
> Hi, a question about the circular mean function in the package
> CircStats:
> 
> Can anyone shed some light on why the circ mean function seems to make
> sense for the first 2 set of bearings and then the mean of 225 and 45
> degrees gives an unexpected 180 deg.
> 
> > deg(circ.mean(c(rad(222),rad(45))))%%360
> [1] 133.5
> > deg(circ.mean(c(rad(224),rad(45))))%%360
> [1] 134.5
> > deg(circ.mean(c(rad(225),rad(45))))%%360
> [1] 180
> > deg(circ.mean(c(rad(226),rad(45))))%%360
> [1] 315.5
> 
> Can anyone explain this???
> 
> This problem was first detected when I was trying to take the circ
> weighted means of my data:
> 
> With 2 groups of bearings:
> x <- c(270,180)
> y <- c(45,270)
> 
> the circular mean of these bearings gives:
> > deg(circ.mean(c(rad(x),rad(y))))%%360
> [1] 257.2356
> 
> When finding the weighted means I get this:
> > meany <- circ.mean(rad(y))
> > meanx <- circ.mean(rad(x))
> 
> > deg(circ.weighted.mean(c(meanx,meany),c(2,2)))%%360
> [1] 281.25
> 
> The function for weighted mean I am using:
> 
> circ.weighted.mean <- function (x,w)
> {
>     sinr <- sum(w*sin(x))
>     cosr <- sum(w*cos(x))
>     circmean <- atan(sinr, cosr)
>     circmean
> }
> 
> I am assuming that the problem that mention above is the cause of the
> different mean bearings.
> 
> Am I missing something fundamental here?
> 
> Thanks,
> Mike
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From jquiroz at ifop.cl  Tue Jun 28 21:45:48 2005
From: jquiroz at ifop.cl (Juan Carlos Quiroz Espinosa)
Date: Tue, 28 Jun 2005 15:45:48 -0400
Subject: [R] lattice graph background
Message-ID: <27004DDE1590B344855CF773E1D019F117478F@postino.ifop.cl>

Hi user's

How to modify the color background on lattice plot. I am trying to
plotting histogram from lattice package.

Thank very much

*********************************
Juan Carlos Quiroz
Instituto de Fomento Pesquero
CHILE



From sundar.dorai-raj at pdf.com  Tue Jun 28 21:51:16 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 28 Jun 2005 14:51:16 -0500
Subject: [R] lattice graph background
In-Reply-To: <27004DDE1590B344855CF773E1D019F117478F@postino.ifop.cl>
References: <27004DDE1590B344855CF773E1D019F117478F@postino.ifop.cl>
Message-ID: <42C1AA34.1010509@pdf.com>



Juan Carlos Quiroz Espinosa wrote:
> Hi user's
> 
> How to modify the color background on lattice plot. I am trying to
> plotting histogram from lattice package.
> 
> Thank very much
> 

Try:

trellis.par.set(theme = col.whitebg())

or create your own theme.

See ?trellis.par.set.

HTH,

--sundar



From liqiurhelp at yahoo.com  Tue Jun 28 22:31:08 2005
From: liqiurhelp at yahoo.com (liqiu jiang)
Date: Tue, 28 Jun 2005 13:31:08 -0700 (PDT)
Subject: [R] How to extract the within group correlation structure matrix in
	"lme"
Message-ID: <20050628203108.57973.qmail@web33806.mail.mud.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/8f69bdff/attachment.pl

From jfox at mcmaster.ca  Tue Jun 28 22:32:21 2005
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 28 Jun 2005 16:32:21 -0400
Subject: [R] 3D ellipsoid confidence region
In-Reply-To: <8E110A8F4A95AD46AEE54E8A195F0E2C01889A93@bellemail.exponent.com>
Message-ID: <web-96949043@cgpsrv2.cis.mcmaster.ca>

Dear Melanie,

I think that you're referring to the data.ellipse function in the car
package. What you're suggesting is a nice idea but I haven't done it. I
think that it might be easier to draw the ellipsoid using the rgl
package rather than scatterplot3d. (See, e.g., the scatter3d function
in the Rcmdr package.)

Regards,
 John

On Tue, 28 Jun 2005 12:35:35 -0700
 Melanie Edwards <edwardsm at exponent.com> wrote:
> I am curious if there is code developed to plot confidence regions in
> 3D.
> The scatterplot3d function generates the plot I want, but would like
> an 3D
> equivalent to the data.ellipse function.
> 
> Any help in this direction would be appreciated, be it theoretical,
> graphical, or otherwise.
> 
> Melanie Edwards
> Senior Statistician
> 
> Exponent
> 15375 SE 30th PL, Suite 250
> Bellevue, WA  98007
> Tel:  (425) 519-8714
> Fax:  (425) 643-9827
> Cell:  (206) 852-5739
> www.exponent.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From spencer.graves at pdf.com  Tue Jun 28 23:07:03 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 14:07:03 -0700
Subject: [R] How to extract the within group correlation structure
 matrix in	"lme"
In-Reply-To: <20050628203108.57973.qmail@web33806.mail.mud.yahoo.com>
References: <20050628203108.57973.qmail@web33806.mail.mud.yahoo.com>
Message-ID: <42C1BBF7.2040607@pdf.com>

	  Have you tried "VarCorr(f)"?

	  spencer graves

liqiu jiang wrote:

> Dear R users,
>  I fitted a repeated measure model without random effects by using lme. I will use the estimates from that model as an initial estimates to do multiple imputation for missing values of the response variable in the model.  I am trying to extract the within group correlation matrix or covariance matrix. 
>  
> here is my code:
> f = lme(y ~x0+x1+trt+tim+x1:tim +tim:trt,random=~-1|subj, data=dat,corr =corAR1())
> 
>>f$sigma
> 
> [1] 2.330854
> 
> b=summary(f)$modelStruct$corStruct
> 
>>b
> 
> Correlation structure of class corAR1 representing
>       Phi 
> 0.8518711 
> 
> I think  0.8518711 and f$sigma is what I need to reconstruct the variance-cavariance matrix. So, How can I extract 0.8518711 so that I can assign it to a variable? Also, I don't understand what the following parameters estimates mean?  
>  
>  >summary(f)$modelStruct
> reStruct  parameters:
>      subj 
> -20.15833 
> corStruct  parameters:
> [1] 2.525869
>  
> I appreciate your time on this. 
>  
> Best wishes,
> Liqiu 
>  
>  
>  
>  
>  
>  
> 
> 		
> ---------------------------------
> 
>  Rekindle the Rivalries. Sign up for Fantasy Football
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From Mike.Prager at noaa.gov  Tue Jun 28 23:10:42 2005
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Tue, 28 Jun 2005 17:10:42 -0400
Subject: [R] JGR font rendering on Windows
Message-ID: <42C1BCD2.4050108@noaa.gov>

JGR users on Windows may be interested that Sun has released a preview 
of its latest Java development kit and runtime engine at

http://www.java.net/download/jdk6/binaries/

This removes the main problem I have had with JGR and other Java 
programs on Windows: poor font rendering.  On Windows XP with an LCD 
display, the improvement is striking, as this uses Windows native font 
smoothing (CoolType).  I believe it uses the default Windows font 
smoothing on other versions of Windows or other displays.

MHP

-- 
Michael Prager, Ph.D.
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
*** Opinions are personal, not official. No government endorsement
is made of any commercial or noncommercial product. ***



From aoganyan at niss.org  Tue Jun 28 23:23:10 2005
From: aoganyan at niss.org (Anna Oganyan)
Date: Tue, 28 Jun 2005 17:23:10 -0400
Subject: [R] conversion
Message-ID: <42C1BFBE.3020506@niss.org>

Dear List,
How can I convert a list with elements being character strings, like: 
"c(1,2,3,4)", ?c(1,3,4,2) ? to a list with elements as numerical 
vectors: c(1,2,3,4), c(1,3,4,2)??
Thanks!
Anna



From gunter.berton at gene.com  Tue Jun 28 23:22:40 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 28 Jun 2005 14:22:40 -0700
Subject: [R] Reference Card (especially useful for Newbies)
Message-ID: <200506282122.j5SLMfmD014589@volta.gene.com>

 

Newbies (and others!) may find useful the R Reference Card made available by
Tom Short and Rpad at http://www.rpad.org/Rpad/Rpad-refcard.pdf  or through
the "Contributed" link on CRAN (where some other reference cards are also
linked). It categorizes and organizes a bunch of R's basic, most used
functions so that they can be easily found. For example, paste() is under
the "Strings" heading and expand.grid() is under "Data Creation." For
newbies struggling to find the right R function as well as veterans who
can't quite remember the function name, it's very handy.
 
-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Tue Jun 28 23:24:43 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 14:24:43 -0700
Subject: [R] test of hazard ratios
In-Reply-To: <20050627172436.95603.qmail@web33313.mail.mud.yahoo.com>
References: <20050627172436.95603.qmail@web33313.mail.mud.yahoo.com>
Message-ID: <42C1C01B.4060502@pdf.com>

	  Can you fit a single model that includes the two classifiers from 
which the model with one classifier could be obtained as a special case? 
  If yes (and if neither parameter was at a boundary crudely similar to 
a zero variance component in "lme"), then you could use the theory that 
2*ln(likelihood ratio) is approximately chi-square.  For many 
capabilities in R, this is implemented as a function anova(fit2, fit1) 
where fit2 and fit1 are the results of fitting the general and the 
specialized models.

	  If you don't see how to make this work, PLEASE do read the posting 
guide! "http://www.R-project.org/posting-guide.html".  It might help you 
find a better answer.  Failing that, if you submit another post after 
following the process described therein, it should increase the chances 
that you will get a useful reply.

	  spencer graves

Steve Adams wrote:

> Hi,
> 
> Is there a R test available that tests whether 2
> hazard ratios obtained from Cox regressions on the
> same patient sample by 2 different classifiers are
> significantly different?
> 
> Thanks
> 
> Steve
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From sghosh at lexgen.com  Tue Jun 28 23:28:15 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Tue, 28 Jun 2005 16:28:15 -0500
Subject: [R] Help with stripplot
Message-ID: <2B47B68F97330841AC8C670749084A7D06C503@wdexchmb01.lexicon.lexgen.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/7225d191/attachment.pl

From sundar.dorai-raj at pdf.com  Tue Jun 28 23:30:11 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 28 Jun 2005 16:30:11 -0500
Subject: [R] conversion
In-Reply-To: <42C1BFBE.3020506@niss.org>
References: <42C1BFBE.3020506@niss.org>
Message-ID: <42C1C163.3000601@pdf.com>



Anna Oganyan wrote:
> Dear List,
> How can I convert a list with elements being character strings, like: 
> "c(1,2,3,4)", ?c(1,3,4,2) ? to a list with elements as numerical 
> vectors: c(1,2,3,4), c(1,3,4,2)??
> Thanks!
> Anna
> 

Try:

x <- list("c(1,2,3,4)", "c(1,3,4,2)")
lapply(x, function(x) eval(parse(text = x)))

HTH,

--sundar



From dseung at andrew.cmu.edu  Tue Jun 28 23:32:41 2005
From: dseung at andrew.cmu.edu (David Hwang)
Date: Tue, 28 Jun 2005 17:32:41 -0400 (EDT)
Subject: [R] sample R code for multiple imputation
Message-ID: <Pine.LNX.4.60-041.0506281729120.5597@unix46.andrew.cmu.edu>

Hi,

I have a big dataset which has many missing values and want to implement 
Multiple imputation via Monte carlo markov chain by following J Schafer's 
"Analysis of incomplete multivariate data". I don't know where to begin 
and is looking for a sample R code that implements multiple imputation 
with EM, MCMC, etc....

Any help / suggestion will be greatly appreciated.

David



From mschwartz at mn.rr.com  Tue Jun 28 23:36:58 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Tue, 28 Jun 2005 16:36:58 -0500
Subject: [R] conversion
In-Reply-To: <42C1BFBE.3020506@niss.org>
References: <42C1BFBE.3020506@niss.org>
Message-ID: <1119994618.4850.103.camel@localhost.localdomain>

On Tue, 2005-06-28 at 17:23 -0400, Anna Oganyan wrote:
> Dear List,
> How can I convert a list with elements being character strings, like: 
> "c(1,2,3,4)", Å‚Äúc(1,3,4,2) Å‚ÄÅ¶ to a list with elements as numerical 
> vectors: c(1,2,3,4), c(1,3,4,2)Å‚ÄÅ¶?
> Thanks!
> Anna

> l <- list("c(1,2,3,4)", "c(1,3,4,2)")

> l
[[1]]
[1] "c(1,2,3,4)"

[[2]]
[1] "c(1,3,4,2)"

Now use lapply() over each list element in 'l', converting the character
vectors to R expressions and then evaluating them:

> lapply(l, function(x) eval(parse(text = x)))
[[1]]
[1] 1 2 3 4

[[2]]
[1] 1 3 4 2


See ?lapply, ?eval and ?parse.

HTH,

Marc Schwartz



From lisawang at uhnres.utoronto.ca  Tue Jun 28 23:39:40 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Tue, 28 Jun 2005 17:39:40 -0400
Subject: [R] How to sort a dataframe by one variable
Message-ID: <42C1C39C.2040102@uhnres.utoronto.ca>

Hi there,

Could anybody help me on how to sort a dataframe by one variable in the 
dataframe?

Thank you

Lisa Wang
Princess Margaret Hospital
Toronto, Ca
tel: 416 946 4501



From spencer.graves at pdf.com  Tue Jun 28 23:43:15 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 14:43:15 -0700
Subject: [R] conversion
In-Reply-To: <42C1BFBE.3020506@niss.org>
References: <42C1BFBE.3020506@niss.org>
Message-ID: <42C1C473.4050807@pdf.com>

	  "as.numeric"?

	  spencer graves

Anna Oganyan wrote:

> Dear List,
> How can I convert a list with elements being character strings, like: 
> "c(1,2,3,4)", ?c(1,3,4,2) ? to a list with elements as numerical 
> vectors: c(1,2,3,4), c(1,3,4,2)??
> Thanks!
> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From sundar.dorai-raj at pdf.com  Tue Jun 28 23:48:06 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 28 Jun 2005 16:48:06 -0500
Subject: [R] Help with stripplot
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C503@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C503@wdexchmb01.lexicon.lexgen.com>
Message-ID: <42C1C596.8080502@pdf.com>



Ghosh, Sandeep wrote:
> For the following code is there a way to make the jitter all line up horizontally, instead of them being just randomly spread around a value. So for ex if there are multiple values at 63 for genotype wt then all the values should be plotted on the same y value of 63 but spaced apart by a certain factor or noise..
> 
<snip>
> 

Yes, remove the jitter function call:

stripplot(marbles_buried ~ genotype, data = dataFrame, aspect = 1,
           jitter = TRUE, xlab='Genotype', ylab = "Marbles Buried",
           main='MBA WTs Vs HOMs')


--sundar



From a_mani_sc_gs at vsnl.net  Wed Jun 29 00:22:31 2005
From: a_mani_sc_gs at vsnl.net (A. Mani)
Date: Wed, 29 Jun 2005 03:52:31 +0530
Subject: [R] R-help Digest, Vol 28, Issue 28
In-Reply-To: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
References: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
Message-ID: <200506290352.32902.a_mani_sc_gs@vsnl.net>

On Tuesday 28 June 2005 15:30, r-help-request at stat.math.ethz.ch wrote:
Re :   37. Re: A. Mani : colours in Silhouette (Mulholland, Tom)
>   
> Message: 37
> Date: Tue, 28 Jun 2005 09:08:24 +0800
> From: "Mulholland, Tom" <Tom.Mulholland at dpi.wa.gov.au>
> Subject: Re: [R] A. Mani : colours in Silhouette
> To: <a_mani_sc_gs at vsnl.net>, <r-help at stat.math.ethz.ch>
> Message-ID:
>  <4702645135092E4497088F71D9C8F51A128BAF at afhex01.dpi.wa.gov.au>
> Content-Type: text/plain; charset="iso-8859-1"
>
> It's not so much a problem, as not working the way you expected.
> cluster:::plot.partition is used to do the plotting. If you look at the
> code for this you can see the difficulty in putting every possible
> permutation into the code. If for example you want the silhouette plot to
> be red using col = "red" is not intuitive as the cluster plot (which comes
> up first) has more than one colour. If you have a look at methods(plot)
> (assuming that you have loaded the cluster package) you will see that there
> is a specific piece of code in the form of plot.silhouette. It has an
> asterisk next to it so you need to use cluster:::plot.silhouette to see the
> code. It has what you need.
>
> args(cluster:::plot.silhouette)
>
> > function (x, nmax.lab = 40, max.strlen = 5, main = NULL, sub = NULL,
>
>     xlab = expression("Silhouette width " * s[i]), col = "gray",
>     do.col.sort = length(col) > 1, border = 0, cex.names = par("cex.axis"),
>     do.n.k = TRUE, do.clus.stat = TRUE, ...)
>
>
>  data(ruspini)
>       pr4 <- pam(ruspini, 4)
>       si <- silhouette(pr4)
>       plot(si,col = "red")
>
I tried that before with many more options and got a blank image. It must have 
been due to the options.
> The issue is that whenever code is written there is always a choice as to
> what functionality is put in place. Just because something can be done,
> does not mean it will or in some cases should be done. In this case the
> help for plot.partition notes that "For more flexibility, use
> 'plot(silhouette(x), ...)', see 'plot.silhouette'."
>
> Tom
>
> Thanks for that I found out something I will find useful in the future.
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of A. Mani
> > Sent: Tuesday, 28 June 2005 4:30 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] A. Mani : colours in Silhouette
> >
> >
> > Hello,
> >        In cluster analysis with cluster, how does one colour
> > the silhouette
> > plots ? For example in using pam. There seems to be some
> > problem there.
> > Everything else can be coloured.
> >
> > Thanks,
> >

 A. Mani
 Member, Cal. Math. Soc



From sundar.dorai-raj at pdf.com  Wed Jun 29 00:06:32 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 28 Jun 2005 17:06:32 -0500
Subject: [R] How to sort a dataframe by one variable
In-Reply-To: <42C1C39C.2040102@uhnres.utoronto.ca>
References: <42C1C39C.2040102@uhnres.utoronto.ca>
Message-ID: <42C1C9E8.7070201@pdf.com>



Lisa Wang wrote:
> Hi there,
> 
> Could anybody help me on how to sort a dataframe by one variable in the 
> dataframe?
> 
> Thank you
> 

See ?order.

x <- data.frame(a = runif(10), b = 1:10)
x[order(x$a), ]


--sundar



From gerifalte28 at hotmail.com  Wed Jun 29 00:11:11 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Tue, 28 Jun 2005 22:11:11 +0000
Subject: [R] How to sort a dataframe by one variable
In-Reply-To: <42C1C39C.2040102@uhnres.utoronto.ca>
Message-ID: <BAY103-F17C4F0B3A3030ADC7FE596A6E10@phx.gbl>

Try RSiteSearch("Sort data frame").

Anyhow, use order() or sort.list() as follows:

dat[order(dat[,'myColumn']),]#Sorts data.frame dat by column "myColumn"
dat<-dat[sort.list(dat$myColumn, decreasing = T),] #Another option to do the 
same.  You can specify decreasing or increasing within both function

Cheers

Francisco

>From: Lisa Wang <lisawang at uhnres.utoronto.ca>
>To: R-Help <r-help at stat.math.ethz.ch>
>Subject: [R] How to sort a dataframe by one variable
>Date: Tue, 28 Jun 2005 17:39:40 -0400
>
>Hi there,
>
>Could anybody help me on how to sort a dataframe by one variable in the
>dataframe?
>
>Thank you
>
>Lisa Wang
>Princess Margaret Hospital
>Toronto, Ca
>tel: 416 946 4501
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ferd_principia at yahoo.ca  Wed Jun 29 00:14:07 2005
From: ferd_principia at yahoo.ca (ferdinand principia)
Date: Tue, 28 Jun 2005 18:14:07 -0400 (EDT)
Subject: [R] faster algorithm for Kendall's tau
In-Reply-To: <1119987060.4850.76.camel@localhost.localdomain>
Message-ID: <20050628221407.90022.qmail@web31215.mail.mud.yahoo.com>

Sorry,

I should specifiy in more detail what my data looks
like. The data vectors (simulations) are mostly
composed of floats (for which it's pretty unlikely to
produce ties), but there are integer values to be
found as well (up to 10% of vector elements). 
As I undestand, Marc's algo is not suited for this
case. 

Is there another solution?

Thanks
Ferdinand


--- "Marc Schwartz (via MN)" <mschwartz at mn.rr.com>
wrote:

> On Tue, 2005-06-28 at 13:03 -0400, ferdinand
> principia wrote:
> > Hi,
> > 
> > I need to calculate Kendall's tau for large data
> > vectors (length > 100'000). 
> > Is somebody aware of a faster algorithm or package
> > function than "cor(, method="kendall")"? 
> > There are ties in the data to be considered
> (Kendall's
> > tau-b).
> > 
> > Any suggestions?
> > 
> > Regards
> > Ferdinand
> 
> 
> The time intensive part of the process is typically
> the ranking/ordering
> of the vector pairs to calculate the numbers of
> concordant and
> discordant pairs.
> 
> If the number of _unique pairs_ in your data is
> substantially less than
> the number of total pairs (in other words, creating
> a smaller 2d
> contingency table from a pair of your vectors makes
> sense), then the
> following may be of help.
> 
> # Calculate CONcordant Pairs in a table
> # cycle through x[r, c] and multiply by
> # sum(x elements below and to the right of x[r, c])
> # x = table
> concordant <- function(x)
> {
>   x <- matrix(as.numeric(x), dim(x))
>   
>   # get sum(matrix values > r AND > c)
>   # for each matrix[r, c]
>   mat.lr <- function(r, c)
>   { 
>     lr <- x[(r.x > r) & (c.x > c)]
>     sum(lr)
>   }
> 
>   # get row and column index for each
>   # matrix element
>   r.x <- row(x)
>   c.x <- col(x)
> 
>   # return the sum of each matrix[r, c] * sums
>   # using mapply to sequence thru each matrix[r, c]
>   sum(x * mapply(mat.lr, r = r.x, c = c.x))
> }
> 
> # Calculate DIScordant Pairs in a table
> # cycle through x[r, c] and multiply by
> # sum(x elements below and to the left of x[r, c])
> # x = table
> discordant <- function(x)
> {
>   x <- matrix(as.numeric(x), dim(x))
>   
>   # get sum(matrix values > r AND < c)
>   # for each matrix[r, c]
>   mat.ll <- function(r, c)
>   { 
>     ll <- x[(r.x > r) & (c.x < c)]
>     sum(ll)
>   }
> 
>   # get row and column index for each
>   # matrix element
>   r.x <- row(x)
>   c.x <- col(x)
> 
>   # return the sum of each matrix[r, c] * sums
>   # using mapply to sequence thru each matrix[r, c]
>   sum(x * mapply(mat.ll, r = r.x, c = c.x))
> }
> 
> 
> # Calculate Kendall's Tau-b
> # x = table
> calc.KTb <- function(x)
> {
>   x <- matrix(as.numeric(x), dim(x))
>   
>   c <- concordant(x)
>   d <- discordant(x)
> 
>   n <- sum(x)
>   SumR <- rowSums(x)
>   SumC <- colSums(x)
> 
>   KTb <- (2 * (c - d)) / sqrt(((n ^ 2) -
>          (sum(SumR ^ 2))) * ((n ^ 2) -
>          (sum(SumC ^ 2))))
> 
>   KTb
> }
> 
> 
> Note that I made some modifications of the above,
> relative to prior
> versions that I have posted to handle large numbers
> of pairs to avoid
> integer overflows in summations. Hence the:
> 
>   x <- matrix(as.numeric(x), dim(x))
> 
> conversion in each function.
> 
> Now, create some random test data, with 100,000
> elements in each vector,
> sampling from 'letters', which would yield a 26 x 26
> table:
> 
>  a <- sample(letters, 100000, replace = TRUE)
>  b <- sample(letters, 100000, replace = TRUE)
>  
>  > dim(table(a, b))
>  [1] 26 26
> 
>  > system.time(print(calc.KTb(table(a, b))))
> [1] 0.0006906088
> [1] 0.77 0.02 0.83 0.00 0.00
> 
> Note that in the above, the initial table takes most
> of the time:
> 
> > system.time(table(a, b))
> [1] 0.55 0.00 0.56 0.00 0.00
> 
> Hence:
> 
> > tab.ab <- table(a, b)
> > system.time(print(calc.KTb(tab.ab)))
> [1] 0.0006906088
> [1] 0.25 0.01 0.27 0.00 0.00
> 
> 
> I should note that I also ran:
> 
> > system.time(print(cor(a, b, method = "kendall")))
> [1] 0.0006906088 
> [1] 694.80   7.72 931.89   0.00   0.00 
> 
> Nice to know the results work out at least...  :-)
> 
> 
> I have not tested with substantially larger 2d
> matrices, but would
> envision that as the dimensions of the resultant
> tabulation increases,
> my method probably approaches and may even become
> less efficient than
> the approach implemented in cor(). Some testing
> would validate this and
> perhaps point to coding the concordant() and
> discordant() functions in C
> for improvement in timing.
> 
> HTH,
> 
> Marc Schwartz
> 
> 
>



From deepayan at cs.wisc.edu  Wed Jun 29 00:31:36 2005
From: deepayan at cs.wisc.edu (deepayan@cs.wisc.edu)
Date: Tue, 28 Jun 2005 17:31:36 -0500 (CDT)
Subject: [R] Help with stripplot
In-Reply-To: <42C1C596.8080502@pdf.com>
References: <2B47B68F97330841AC8C670749084A7D06C503@wdexchmb01.lexicon.lexgen.com>
	<42C1C596.8080502@pdf.com>
Message-ID: <32788.128.104.216.187.1119997896.squirrel@128.104.216.187>

>
>
> Ghosh, Sandeep wrote:
>> For the following code is there a way to make the jitter all line up
>> horizontally, instead of them being just randomly spread around a value.
>> So for ex if there are multiple values at 63 for genotype wt then all
>> the values should be plotted on the same y value of 63 but spaced apart
>> by a certain factor or noise..
>>
> <snip>
>>
>
> Yes, remove the jitter function call:
>
> stripplot(marbles_buried ~ genotype, data = dataFrame, aspect = 1,
>            jitter = TRUE, xlab='Genotype', ylab = "Marbles Buried",
>            main='MBA WTs Vs HOMs')
>

Also, since you are not using conditioning, consider using stripchart,
which has a 'method = "stack"' argument.

Deepayan



From lindajun_liu at hotmail.com  Wed Jun 29 00:50:52 2005
From: lindajun_liu at hotmail.com (Liu Jun)
Date: Wed, 29 Jun 2005 06:50:52 +0800
Subject: [R] the function of mle
Message-ID: <BAY12-F269E0630CE6592BCDB6994FFE10@phx.gbl>

Hi,
I am a user of R computer language.Several days ago,I download R-2.1.0.From 
the help manual,I saw there is a function "mle" to find the maximum 
likelihood estimators.But when I try to use this function,there is no this 
function.Could you help me to solve this problem?
I am looking forward to your answer.

Thank you.

Jun Liu

_________________________________________________________________
Å√Å‚Å∑Å—ÅœÅ¬Å‘Åÿ MSN Explorer:   http://explorer.msn.com/lccn/



From reilly at stat.auckland.ac.nz  Wed Jun 29 01:30:59 2005
From: reilly at stat.auckland.ac.nz (James Reilly)
Date: Wed, 29 Jun 2005 11:30:59 +1200
Subject: [R] sample R code for multiple imputation
In-Reply-To: <Pine.LNX.4.60-041.0506281729120.5597@unix46.andrew.cmu.edu>
References: <Pine.LNX.4.60-041.0506281729120.5597@unix46.andrew.cmu.edu>
Message-ID: <42C1DDB3.2050002@stat.auckland.ac.nz>


Schafer's functions have been ported to R in the packages norm, cat, mix
and pan. Their documentation includes sample code illustrating how to
use them.

The aregImpute function in Hmisc provides a range of other imputation
models, if you're not set on using MCMC. The mitools package might also
be useful for dealing with the imputed values.

Hope this helps,
James

On 29/06/2005 9:32 a.m., David Hwang wrote:
> Hi,
> 
> I have a big dataset which has many missing values and want to implement 
> Multiple imputation via Monte carlo markov chain by following J Schafer's 
> "Analysis of incomplete multivariate data". I don't know where to begin 
> and is looking for a sample R code that implements multiple imputation 
> with EM, MCMC, etc....
> 
> Any help / suggestion will be greatly appreciated.
> 
> David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
James Reilly
Department of Statistics, University of Auckland
Private Bag 92019, Auckland, New Zealand



From spencer.graves at pdf.com  Wed Jun 29 02:07:47 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 17:07:47 -0700
Subject: [R] the function of mle
In-Reply-To: <BAY12-F269E0630CE6592BCDB6994FFE10@phx.gbl>
References: <BAY12-F269E0630CE6592BCDB6994FFE10@phx.gbl>
Message-ID: <42C1E653.1060300@pdf.com>

	  Did you do "install.packages('nlme')" and then 'library(nlme)' before
trying to use 'lme'?

	  spencer graves

Liu Jun wrote:
> Hi,
> I am a user of R computer language.Several days ago,I download 
> R-2.1.0.From the help manual,I saw there is a function "mle" to find the 
> maximum likelihood estimators.But when I try to use this function,there 
> is no this function.Could you help me to solve this problem?
> I am looking forward to your answer.
> 
> Thank you.
> 
> Jun Liu
> 
> _________________________________________________________________
> Å√Å‚Å∑Å—ÅœÅ¬Å‘Åÿ MSN Explorer:   http://explorer.msn.com/lccn/
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From andy_liaw at merck.com  Wed Jun 29 02:34:24 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 28 Jun 2005 20:34:24 -0400
Subject: [R] the function of mle
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA2C@usctmx1106.Merck.com>

I'd guess Jun meant the mle() in the stats4 package.  It comes with R, but
is not loaded by default, so one would need to do library(stats4) first.

Andy

> From: Spencer Graves
> 
> 	  Did you do "install.packages('nlme')" and then 
> 'library(nlme)' before
> trying to use 'lme'?
> 
> 	  spencer graves
> 
> Liu Jun wrote:
> > Hi,
> > I am a user of R computer language.Several days ago,I download 
> > R-2.1.0.From the help manual,I saw there is a function 
> "mle" to find the 
> > maximum likelihood estimators.But when I try to use this 
> function,there 
> > is no this function.Could you help me to solve this problem?
> > I am looking forward to your answer.
> > 
> > Thank you.
> > 
> > Jun Liu
> > 
> > _________________________________________________________________
> > Å√Å‚Å∑Å—ÅœÅ¬Å‘Åÿ MSN Explorer:   http://explorer.msn.com/lccn/
> > 
> > 
> > 
> --------------------------------------------------------------
> ----------
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> -- 
> Spencer 
> Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
>



From alopez at cds.caltech.edu  Wed Jun 29 03:20:28 2005
From: alopez at cds.caltech.edu (Adrian L. Garcia-Lomana)
Date: Tue, 28 Jun 2005 18:20:28 -0700 (PDT)
Subject: [R] setting yranges of boxplots
Message-ID: <37836.131.215.42.77.1120008028.squirrel@www.cds.caltech.edu>

Hi all,

I was wondering how to create a graph (boxplot) setting the y range of the
graph manually. Something like this:

>x <- c(1:100)
>boxplot(x, yrange(0, 1000))


Thanks for your time,

Adrian



From andy_liaw at merck.com  Wed Jun 29 03:26:04 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 28 Jun 2005 21:26:04 -0400
Subject: [R] setting yranges of boxplots
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA2D@usctmx1106.Merck.com>

Something like this?

    boxplot(runif(100, 0, 100), ylim=c(0, 1000))

Andy

> From: Adrian L. Garcia-Lomana
> 
> Hi all,
> 
> I was wondering how to create a graph (boxplot) setting the y 
> range of the
> graph manually. Something like this:
> 
> >x <- c(1:100)
> >boxplot(x, yrange(0, 1000))
> 
> 
> Thanks for your time,
> 
> Adrian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From pberming at research.ryerson.ca  Wed Jun 29 04:36:04 2005
From: pberming at research.ryerson.ca (Philip Bermingham)
Date: Tue, 28 Jun 2005 22:36:04 -0400
Subject: [R] looking for the source
Message-ID: <EC3EEED49432A54990181E8E8B470727549165@mail2.arts.ryerson.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/44c21246/attachment.pl

From dlvanbrunt at gmail.com  Wed Jun 29 04:46:04 2005
From: dlvanbrunt at gmail.com (David L. Van Brunt, Ph.D.)
Date: Tue, 28 Jun 2005 21:46:04 -0500
Subject: [R] Random Forests theoretical question
Message-ID: <d332d3e105062819462ea2e599@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050628/cec573d1/attachment.pl

From jyzz88 at gmail.com  Wed Jun 29 04:48:34 2005
From: jyzz88 at gmail.com (Luke)
Date: Tue, 28 Jun 2005 22:48:34 -0400
Subject: [R] quick way to construct formula
Message-ID: <27583b4005062819483afce3e@mail.gmail.com>

Dear R users,

I have a data with 1000 variables named "x1", "x2", ..., "x1000", and
I want to construct a formula like this format:

~x1+x2+...+x1000+x1:x2+x1:x3+x999:x1000+log(x1)+...+log(x1000)

That is: the base variables followed by all interaction terms and all
base feature log-transformations. I know I can use several paste
functions to construct it. But is there any other handy way to do it?

-Luke



From gerifalte28 at hotmail.com  Wed Jun 29 04:48:50 2005
From: gerifalte28 at hotmail.com (Francisco J. Zagmutt)
Date: Wed, 29 Jun 2005 02:48:50 +0000
Subject: [R] looking for the source
In-Reply-To: <EC3EEED49432A54990181E8E8B470727549165@mail2.arts.ryerson.ca>
Message-ID: <BAY103-F3005BDAC8F8EA4BD085A4BA6E00@phx.gbl>

Off course.  That's the beauty of open source :)
You will find the source code at http://cran.r-project.org/ under "R 
Sources".

Cheers

Francisco

>From: "Philip Bermingham" <pberming at research.ryerson.ca>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] looking for the source
>Date: Tue, 28 Jun 2005 22:36:04 -0400
>
>I'm working with the kmeans function in the stats package.  This
>function calls complied fortran program:
>
>
>
>.Fortran("kmns", as.double(x).....)
>
>
>
>Is there a way to get the source code for this compiled program "kmns"??
>
>
>
>Philip Bermingham
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From David.Duffy at qimr.edu.au  Wed Jun 29 05:02:47 2005
From: David.Duffy at qimr.edu.au (David Duffy)
Date: Wed, 29 Jun 2005 13:02:47 +1000 (EST)
Subject: [R] How to convert "c:\a\b" to "c:/a/b"
In-Reply-To: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
References: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>

I couldn't resist adding a more literal answer

unback <- function(x) {
  chars <- unlist(strsplit(deparse(x),""))
  chars <- chars[-c(1,length(chars))]
  paste(gsub("\\\\","/",chars),collapse="")
}

unback("\n")


| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From spencer.graves at pdf.com  Wed Jun 29 05:45:36 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Jun 2005 20:45:36 -0700
Subject: [R] How to convert "c:\a\b" to "c:/a/b"
In-Reply-To: <Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>
References: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>
Message-ID: <42C21960.6040909@pdf.com>

	  Close, but does not work generally with RGui 2.1.1 patched, Windows XP:

 > unback(x="n\o")
[1] "no"

	  I'm also unable to parse "echo", suggested by Ten Harding and Henrik 
Bengtsson:

 > echo D:/spencerg/statmtds/R/Rnews> tmp.txt
Error: syntax error
 > echo cat(gsub("\\\\", "/", readLines("tmp.txt"))) | R --slave
Error: syntax error

	  Earlier today, Sundar Dorai-Raj helped me with the following:

 > (File0 <- file.choose())
[1] "D:\\spencerg\\dataPOWER\\stats\\Tukey\\Boxplot_missing_Tukey2.txt"
 > strsplit(File0, "\\\\")
[[1]]
[1] "D:"                         "spencerg"
[3] "dataPOWER"                  "stats"
[5] "Tukey"                      "Boxplot_missing_Tukey2.txt"

 > fp. <- strsplit(File0, "\\\\")[[1]]
 > (path <- paste(fp.[-length(fp.)], collapse="/"))
[1] "D:/spencerg/dataPOWER/stats/Tukey"
 > setwd(path)
 > getwd()
[1] "D:/spencerg/dataPOWER/stats/Tukey"
 > File <- fp.[length(fp.)]
 > File
[1] "Boxplot_missing_Tukey2.txt"

	  Thanks to everyone who has contributed (or even read) this thread. 
I'm confident that a better method exists.

	  Best Wishes,
	  Spencer Graves
	
David Duffy wrote:

> I couldn't resist adding a more literal answer
> 
> unback <- function(x) {
>   chars <- unlist(strsplit(deparse(x),""))
>   chars <- chars[-c(1,length(chars))]
>   paste(gsub("\\\\","/",chars),collapse="")
> }
> 
> unback("\n")
> 
> 
> | David Duffy (MBBS PhD)                                         ,-_|\
> | email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
> | Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
> | 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From simonb at cres10.anu.edu.au  Wed Jun 29 07:38:35 2005
From: simonb at cres10.anu.edu.au (Simon Blomberg)
Date: Wed, 29 Jun 2005 15:38:35 +1000
Subject: [R] sample R code for multiple imputation
In-Reply-To: <42C1DDB3.2050002@stat.auckland.ac.nz>
References: <Pine.LNX.4.60-041.0506281729120.5597@unix46.andrew.cmu.edu>
	<42C1DDB3.2050002@stat.auckland.ac.nz>
Message-ID: <6.2.1.2.0.20050629153428.01cc3eb0@mail.ozemail.com.au>

There is also the mice package at http://www.multiple-imputation.com/
which uses mcmc but is different to Schafer's packages.

Simon.

At 09:30 AM 29/06/2005, James Reilly wrote:

>Schafer's functions have been ported to R in the packages norm, cat, mix
>and pan. Their documentation includes sample code illustrating how to
>use them.
>
>The aregImpute function in Hmisc provides a range of other imputation
>models, if you're not set on using MCMC. The mitools package might also
>be useful for dealing with the imputed values.
>
>Hope this helps,
>James
>
>On 29/06/2005 9:32 a.m., David Hwang wrote:
> > Hi,
> >
> > I have a big dataset which has many missing values and want to implement
> > Multiple imputation via Monte carlo markov chain by following J Schafer's
> > "Analysis of incomplete multivariate data". I don't know where to begin
> > and is looking for a sample R code that implements multiple imputation
> > with EM, MCMC, etc....
> >
> > Any help / suggestion will be greatly appreciated.
> >
> > David

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From ripley at stats.ox.ac.uk  Wed Jun 29 08:46:27 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Jun 2005 07:46:27 +0100 (BST)
Subject: [R] How to convert "c:\a\b" to "c:/a/b"
In-Reply-To: <Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>
References: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>
Message-ID: <Pine.LNX.4.61.0506290732540.6049@gannet.stats>

On Wed, 29 Jun 2005, David Duffy wrote:

> I couldn't resist adding a more literal answer

This can only work for escapes which are preserved.  The parser maps
\n to a character (LF) and the deparser maps it back to \n.
This happens to be true of \a \b \f \n \r \t \v \\ but no others.

For example, \s is mapped to s, and there is no difference between \s and
s in the parsed input.

>
> unback <- function(x) {
>  chars <- unlist(strsplit(deparse(x),""))
>  chars <- chars[-c(1,length(chars))]
>  paste(gsub("\\\\","/",chars),collapse="")
> }
>
> unback("\n")

> unback("\s")
[1] "s"

Spencer Graves keeps on insisting there is a better way, but all the
solutions are to avoid sending the string to the parser, and hence
avoiding having the string directly in an R script.  This is common in 
shell scripts, which use 'here' documents to avoid 'quoting hell'.

We can do that in R too. Here are two variants I have not seen in the 
thread

test1.R:
scan("", "", allowEscapes=FALSE, n=1, quiet=TRUE)
D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
catIx, "\n", sep="")

R --slave --vanilla < test1.R
D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt

(This one does not allow quoted strings.)

test2.R:
x <- readLines(stdin(), n=1)
"D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt"
x <- gsub('^"(.*)"$', "\\1", x)
cat(x, "\n")

R --slave --vanilla < test2.R
D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt

(This one allows surrounding quotes or not.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Wed Jun 29 08:44:20 2005
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 29 Jun 2005 08:44:20 +0200
Subject: [R] quick way to construct formula
In-Reply-To: <27583b4005062819483afce3e@mail.gmail.com>
Message-ID: <02c901c57c75$fa9da360$6e8b6882@didacdom.stat.ucl.ac.be>



Here is a way: it uses 'paste' but I dont think it is a problem anyway
to use it.
Nevertheless, it is surely a bad idea to fit any model with more than
250000 terms...


>   main_effects = paste(nam,collapse="+")
>   inter <- outer(nam,nam,paste,sep=":")
>   inter <- paste(inter[upper.tri(inter)],collapse="+")
>   log_effects <- paste("log(",nam,")",sep="",collapse="+")
>
as.formula(paste("~",main_effects,"+",inter,"+",log_effects,sep=""))
~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x1:x2 + x1:x3 + 
    x2:x3 + x1:x4 + x2:x4 + x3:x4 + x1:x5 + x2:x5 + x3:x5 + x4:x5 + 
    x1:x6 + x2:x6 + x3:x6 + x4:x6 + x5:x6 + x1:x7 + x2:x7 + x3:x7 + 
    x4:x7 + x5:x7 + x6:x7 + x1:x8 + x2:x8 + x3:x8 + x4:x8 + x5:x8 + 
    x6:x8 + x7:x8 + x1:x9 + x2:x9 + x3:x9 + x4:x9 + x5:x9 + x6:x9 + 
    x7:x9 + x8:x9 + x1:x10 + x2:x10 + x3:x10 + x4:x10 + x5:x10 + 
    x6:x10 + x7:x10 + x8:x10 + x9:x10 + log(x1) + log(x2) + log(x3) + 
    log(x4) + log(x5) + log(x6) + log(x7) + log(x8) + log(x9) + 
    log(x10)




HTH,

Eric



Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward
Tufte   


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luke
> Sent: mercredi 29 juin 2005 4:49
> To: R-help at stat.math.ethz.ch
> Subject: [R] quick way to construct formula
> 
> 
> Dear R users,
> 
> I have a data with 1000 variables named "x1", "x2", ..., 
> "x1000", and I want to construct a formula like this format:
> 
> ~x1+x2+...+x1000+x1:x2+x1:x3+x999:x1000+log(x1)+...+log(x1000)
> 
> That is: the base variables followed by all interaction terms 
> and all base feature log-transformations. I know I can use 
> several paste functions to construct it. But is there any 
> other handy way to do it?
> 
> -Luke
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From falissard_b at wanadoo.fr  Wed Jun 29 09:03:01 2005
From: falissard_b at wanadoo.fr (falissard)
Date: Wed, 29 Jun 2005 09:03:01 +0200
Subject: [R] enhanced MDS
In-Reply-To: <200506281338.34767.karen@gecko.biol.wits.ac.za>
Message-ID: <20050629070302.ADCD61C000CF@mwinf0612.wanadoo.fr>

Try also ?sphpca (library(psy) --> new version 0.7)
Best
Bruno


----------------------------------------------------------------------------
Bruno Falissard
INSERM U669, PSIGIAM
"Paris Sud Innovation Group in Adolescent Mental Health"
Maison de Solenn
97 Boulevard de Port Royal
75679 Paris cedex 14, France
tel : (+33) 6 81 82 70 76
fax : (+33) 1 45 59 34 18
web site : http://perso.wanadoo.fr/bruno.falissard/
----------------------------------------------------------------------------
 

-----Message d'origine-----
De??: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] De la part de Karen Kotschy
Envoy????: mardi 28 juin 2005 13:39
????: r-help at stat.math.ethz.ch
Objet??: [R] enhanced MDS

Hi again

Sorry, in looking again at sammon and isoMDS I see that they seem to do 
exactly what I want, except that they are non-metric, which means, as I 
understand it, that they relate the rank orders of the variables rather than

the actual distances. 

Could I use these non-metric MDS packages even if my distances are metric?

Thanks
Karen
-- 
Karen Kotschy
Centre for Water in the Environment
University of the Witwatersrand
Johannesburg

P/Bag X3, Wits, 2050
Tel: +2711 717-6425

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jun 29 09:05:42 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Jun 2005 08:05:42 +0100 (BST)
Subject: [R] quick way to construct formula
In-Reply-To: <27583b4005062819483afce3e@mail.gmail.com>
References: <27583b4005062819483afce3e@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0506290746390.6049@gannet.stats>

On Tue, 28 Jun 2005, Luke wrote:

> Dear R users,
>
> I have a data with 1000 variables named "x1", "x2", ..., "x1000", and
> I want to construct a formula like this format:
>
> ~x1+x2+...+x1000+x1:x2+x1:x3+x999:x1000+log(x1)+...+log(x1000)
>
> That is: the base variables followed by all interaction terms and all
> base feature log-transformations. I know I can use several paste
> functions to construct it. But is there any other handy way to do it?

Do you really want a formula with over half a million terms in?  I think 
it is likely that R will hit recursion limits in handing such a formula, 
quite possibly C-level stack overflow.

For a more modest example:

dd <- data.frame(x1=1, x2=2, x3=3)
terms(~ . + .^2, data=dd, simplify=TRUE)

will get you all terms and their interactions.  Using paste() to get the 
log terms is the easiest way I know.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Jun 29 09:06:44 2005
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 29 Jun 2005 09:06:44 +0200
Subject: [R] enhanced multidimensional scaling?
In-Reply-To: <200506281313.31121.karen@gecko.biol.wits.ac.za>
References: <200506281313.31121.karen@gecko.biol.wits.ac.za>
Message-ID: <17090.18564.919623.455868@stat.math.ethz.ch>

>>>>> "Karen" == Karen Kotschy <karen at biology.biol.wits.ac.za>
>>>>>     on Tue, 28 Jun 2005 13:13:31 +0200 writes:

    Karen> Dear R list Would anyone be able to tell me whether
    Karen> it is possible to do "enhanced multidimensional
    Karen> scaling" (enhanced MDS) in R? In other words,
    Karen> something that goes beyond "cmdscale" by iteratively
    Karen> improving the fit between observed dissimilarities
    Karen> and inter-object distances, using the KYST algorithm
    Karen> (Kruskal, 1964).

Since you know about cmdscale(), do read it's help page more
carefully, in particular the section  "See Also:"
{which, in an ESS help buffer, is reachable by simple "s s"
The help page of the first function mentioned there is entitled
`` Kruskal's Non-metric Multidimensional Scaling ''

    Karen> I have found several implementations of non-metric
    Karen> MDS in various packages but nothing like what I have
    Karen> described above.

well, you've mentioned Kruskal(1964), but not described much
more, hence I hope the above hint does help.

Regards,
Martin Maechler, ETH Zurich



From lauraholt_983 at hotmail.com  Tue Jun 28 17:46:20 2005
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 28 Jun 2005 10:46:20 -0500
Subject: [R] where can i download the metrics package?
In-Reply-To: <42C16C73.9080203@pdf.com>
Message-ID: <BAY105-F19DD476318BF373597908AD6E10@phx.gbl>

There are several packages within rmetrics such as fSeries, fBasics, 
fExtremes, and so on.

You can download those in the usual way.


>From: Spencer Graves <spencer.graves at pdf.com>
>To: ronggui <0034058 at fudan.edu.cn>
>CC: R <r-help at r-project.org>
>Subject: Re: [R] where can i download the metrics package?
>Date: Tue, 28 Jun 2005 08:27:47 -0700
>
>	  How about "Rmetrics", listed under "misc:  related projects" on
>"www.r-project.org".
>
>	  spencer graves
>
>ronggui wrote:
>
> > i learn it from "metrics: Towards a package for doing econometrics in 
>R"but i can not find it in cran.
> >
> >
>
>--
>Spencer Graves, PhD
>Senior Development Engineer
>PDF Solutions, Inc.
>333 West San Carlos Street Suite 700
>San Jose, CA 95110, USA
>
>spencer.graves at pdf.com
>www.pdf.com <http://www.pdf.com>
>Tel:  408-938-4420
>Fax: 408-280-7915
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From amsa36060 at yahoo.com  Wed Jun 29 12:08:23 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Wed, 29 Jun 2005 03:08:23 -0700 (PDT)
Subject: [R] comparison of packages for Unit Root test
Message-ID: <20050629100823.11569.qmail@web60419.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050629/ba66350a/attachment.pl

From amsa36060 at yahoo.com  Wed Jun 29 12:28:18 2005
From: amsa36060 at yahoo.com (Amir Safari)
Date: Wed, 29 Jun 2005 03:28:18 -0700 (PDT)
Subject: [R] Running SVM {e1071}
Message-ID: <20050629102818.61593.qmail@web60421.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050629/fed14d98/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Wed Jun 29 12:26:30 2005
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 29 Jun 2005 12:26:30 +0200
Subject: [R] comparison of packages for Unit Root test
In-Reply-To: <20050629100823.11569.qmail@web60419.mail.yahoo.com>
References: <20050629100823.11569.qmail@web60419.mail.yahoo.com>
Message-ID: <20050629122630.44b1a5c9.Achim.Zeileis@wu-wien.ac.at>

On Wed, 29 Jun 2005 03:08:23 -0700 (PDT) Amir Safari wrote:

>  
>  
> Dear R Users,
>  
> Could somebody please compare the packages of unit root test ( Uroot,
> Ucra, tseries and fseries ) regarding the type of test ( without
> constant and trend, with constant , and with constant and trend ) ?

IMHO, the tseries implementation is still the reference implementation
for the significance tests which also have a slim interface.
urca (sic!) offers some more tools in an integrated way. If you are not
just interested in one test or the other, urca certainly offers much
tools in a unit root analysis.
If you use the Rmetrics suite of packages, then fSeries (sic!) is
probably most useful.
uroot (sic!) offers a GUI which is a bit cumbersome to set up and the
output is a bit crude, e.g., doesn't offer p-values.

my 2 cent,
Z

> Regards,
> Amir Safari
>  
> 
> 		
> ---------------------------------
> 
>  Rekindle the Rivalries. Sign up for Fantasy Football
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Jun 29 12:48:57 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 29 Jun 2005 12:48:57 +0200
Subject: [R] Running SVM {e1071}
In-Reply-To: <20050629102818.61593.qmail@web60421.mail.yahoo.com>
References: <20050629102818.61593.qmail@web60421.mail.yahoo.com>
Message-ID: <42C27C99.5080708@statistik.uni-dortmund.de>

Amir Safari wrote:
>  
> Dear David, Dear Friends,
>  
> After any running svm I receive different results of Error estimation of 'svm' using 10-fold cross validation. What is the reason ? It is caused by the algorithm, libsvm , e1071 or something els? Which value can be optimal one ? How much run can reach to the optimality.And finally, what is difference between Error estimation of svm using 10-fold cross validation and MSE ( Mean Square Error ) ?
> So many thanks in advance for your help.

10-fold cross validation chooses the traing/test sets randomly, hence 
you won't get the same results each time you run it ...

Uwe Ligges


> Best Regards,
> Amir 
>  
>  
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From david.meyer at wu-wien.ac.at  Wed Jun 29 12:49:40 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Wed, 29 Jun 2005 12:49:40 +0200
Subject: [R] svm and scaling input
Message-ID: <20050629124940.435fa265.david.meyer@wu-wien.ac.at>


Arne.Muller at sanofi-aventis.com wrote:
> Dear All,
> 
> I've a question about scaling the input variables for an analysis with
> svm (package e1071). Most of my variables are factors with 4 to 6
> levels but there are also some numeric variables.
> 
> I'm not familiar with the math behind svms, so my assumtions maybe
> completely wrong ... or obvious. Will the svm automatically expand the
> factors into a binary matrix? 

yes.

> If I add numeric variables outside the range of 0 to 1 do I have to
> scale them to have 0 to 1 range?

svm() will scale your data by default.

Cheers, 
David

-- 
Dr. David Meyer
Department of Information Systems and Operations

Vienna University of Economics and Business Administration
Augasse 2-6, A-1090 Wien, Austria, Europe
Fax: +43-1-313 36x746 
Tel: +43-1-313 36x4393
HP:  http://wi.wu-wien.ac.at/~meyer/



From david.meyer at wu-wien.ac.at  Wed Jun 29 12:58:54 2005
From: david.meyer at wu-wien.ac.at (David Meyer)
Date: Wed, 29 Jun 2005 12:58:54 +0200
Subject: [R] Running SVM {e1071}
In-Reply-To: <20050629102818.61593.qmail@web60421.mail.yahoo.com>
References: <20050629102818.61593.qmail@web60421.mail.yahoo.com>
Message-ID: <20050629125854.37eb86b5.david.meyer@wu-wien.ac.at>


>  
> Dear David, Dear Friends,
>  
> After any running svm I receive different results of Error estimation
> of 'svm' using 10-fold cross validation. 

using tune.svm(), or the `cross' parameter of svm()?

> What is the reason ? It is caused by the algorithm, libsvm , e1071 or
> something els? 

The splits are chosen randomly.

> Which value can be optimal one? 

The Bayes Error.

> How much run can reach to the optimality.

What do you mean by `How much run'?

> And finally, what is difference between Error estimation of svm using
> 10-fold cross validation and MSE ( Mean Square Error ) ?

the former is an error estimation _procedure_, the latter is an error
_measure.

Cheers,
David



From Dubravko.Dolic at komdat.com  Wed Jun 29 13:26:55 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Wed, 29 Jun 2005 13:26:55 +0200
Subject: [R] Memory Management under Linux: Problems to allocate large
	amounts of data
Message-ID: <52D1AC81378E9342947189B04176014728F70A@agentsmith.komdat.intern>

Dear Group

I'm still trying to bring many data into R (see older postings). After solving some troubles with the database I do most of the work in MySQL. But still I could be nice to work on some data using R. Therefore I can use a dedicated Server with Gentoo Linux as OS hosting only R. This Server is a nice machine with two CPU and 4GB RAM which should do the job:

Dual Intel XEON 3.06 GHz
4 x 1 GB RAM PC2100 CL2
HP Proliant DL380-G3

I read the R-Online help on memory issues and the article on garbage collection from the R-News 01-2001 (Luke Tierney). Also the FAQ and some newsgroup postings were very helpful on understanding memory issues using R.

Now I try to read data from a database. The data I wanted to read consists of 158902553 rows and one field (column) and is of type bigint(20) in the database. I received the message that R could not allocate the 2048000 Kb (almost 2GB) sized vector. As I have 4BG of RAM I could not imagine why this happened. In my understanding R under Linux (32bit) should be able to use the full RAM. As there is not much space used by OS and R as such ("free" shows the use of app. 670 MB after dbSendQuery and fetch) there are 3GB to be occupied by R. Is that correct?

After that I started R by setting n/vsize explicitly

R --min-vsize=10M --max-vsize=3G --min-nsize=500k --max-nsize=100M

> mem.limits()
    nsize     vsize
104857600        NA

and received the same message.


A garbage collection delivered the following information:

> gc()
         used (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
Ncells 217234  5.9     500000   13.4       2800    500000   13.4
Vcells  87472  0.7  157650064 1202.8       3072 196695437 1500.7


Now I'm at a loss. Maybe anyone could give me a hint where I should read further or which Information can take me any further 





Dubravko Dolic
Statistical Analyst
Tel:?? ?????? +49 (0)89-55 27 44 - 4630
Fax: ?????? +49 (0)89-55 27 44 - 2463
Email: dubravko.dolic at komdat.com
Komdat GmbH
Nymphenburger Stra??e 86
80636 M??nchen
---------------------------------------------
ONLINE MARKETING THAT WORKS
---------------------------------------------
This electronic message contains information from Komdat Gmb...{{dropped}}



From s.henderson at ucl.ac.uk  Wed Jun 29 13:22:02 2005
From: s.henderson at ucl.ac.uk (Stephen Henderson)
Date: Wed, 29 Jun 2005 12:22:02 +0100
Subject: [R] coxph, survfit and Brier score
Message-ID: <E7CF6BC2744CBE41AE4E87635C7C893C1C69E5@exc.cruciform.wibr.ucl.ac.uk>

Hello and apologies for a very long question. I thought it better to be
verbose and clear than short and imprecise. I am trying to compute the brier
score comparing true surv object of test data to predictions from train data
(using sbrier in ipred package). I am having trouble getting the right
format for my prediction object I think. I have split the DLBCL dataset and
fit a coxph on a training set then I am trying to get out a prediction using
the survfit function with newdata=test. My code is below. 

>library(survival)
>library(ipred)


>data(DLBCL)
>DLBCL[complete.cases(DLBCL),]->DLBCL
>train<-DLBCL[1:19,]
>test<-DLBCL[20:38,]
>train.surv<-Surv(train$time, train$cens)
>test.surv<-Surv(test$time, test$cens)


>train.mod<-coxph(train.surv~IPI, data=train)
> pred<- survfit(train.mod, newdata=test)

>class(pred)
[1] "survfit.cox" "survfit"

>pred

Call: survfit.coxph(object = train.mod, newdata = test)

       n events median 0.95LCL 0.95UCL
 [1,] 19     10    Inf    27.1     Inf
 [2,] 19     10    Inf     Inf     Inf
 [3,] 19     10    Inf     Inf     Inf
 [4,] 19     10   23.7     4.1     Inf
 [5,] 19     10   23.7     4.1     Inf
 [6,] 19     10    Inf    27.1     Inf
 [7,] 19     10    Inf     Inf     Inf
 [8,] 19     10    Inf     Inf     Inf
 [9,] 19     10    Inf    27.1     Inf
[10,] 19     10    4.1     2.9     Inf
[11,] 19     10    Inf    27.1     Inf
[12,] 19     10    4.1     2.9     Inf
[13,] 19     10    Inf     Inf     Inf
[14,] 19     10   23.7     4.1     Inf
[15,] 19     10    Inf    27.1     Inf
[16,] 19     10    Inf    27.1     Inf
[17,] 19     10    Inf     Inf     Inf
[18,] 19     10    Inf     Inf     Inf
[19,] 19     10    Inf    27.1     Inf


>sbrier(test.surv, pred)
Error in switch(ptype, survfit = { : switch: EXPR must return a length 1
vector

The sbrier function clearly does not recognize the format of this survfit
object "pred". Indeed it seems to have the same variables but does not look
like the object you receive from a normal survfit call e.g. 


>KM <- survfit(train.surv)
>KM
Call: survfit(formula = train.surv)

      n  events  median 0.95LCL 0.95UCL 
   19.0    10.0    71.3    15.5     Inf 

>sbrier(train.surv, KM)
integrated Brier score 
             0.2220228 
attr(,"time")
[1]   2.4 102.4

My question is can I either force survfit(etc..., newdata=x) to return a
useful survfit object for use with sbrier, or alternatively coerce the pred
object above into a survfit object? Or am I missing something ..is there a
reason survfit returns the predicted object in a different format?

PS I realize the fit is not good but just want to get the code to work.

Thank You

Stephen Henderson


**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From Gregor.Gorjanc at bfro.uni-lj.si  Wed Jun 29 13:29:10 2005
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Wed, 29 Jun 2005 13:29:10 +0200
Subject: [R] Producing character "given" i.e. "| " with plotmath
Message-ID: <7FFEE688B57D7346BC6241C55900E730F3189C@pollux.bfro.uni-lj.si>

>> Hello!
>> 
>> Does someone know how to produce
>> 
>>   L(y|mu)
>> 
>> with plotmath?
>> 
>> Some code with unsuccessfull results:
>> 
>> plot(dnorm(x = seq(from = -4, to = 4, by = 0.1)), type = "l")
>> ## Not what I want
>> legend(legend = c(expression(L(y:mu))), x = "topright")
>> 
>> ## Strange, is this a bug?
>> legend(legend = c(expression(L(y|mu))), x = "top")

> No, "|" is a logical Operator that can be rewritten in its original 
> function form as follows:
>
> "|"(FALSE, TRUE)
>
> Hence the result is expected.

Yes, that makes sense. Thanks!

>> ## Group produces an error
>> legend(legend = c(expression(group(L(y, "|", mu)))), x = "topleft")

> You have not specified any delimiter.
Ok, I got this point wrong from ?plotmath .

>> ## Paste keeps commas in expression
>> legend(legend = c(expression(paste(L(y, "|", mu)))), x = "bottomleft")

>correct

>> ## This one is OK, but braces are not as they should be 
>> legend(legend = c(expression(paste("L(y", "|", "mu)"))), x = "bottom")

> What's wrong with the braces?`
They are not the same as in previous cases. They are not "bold".

> What you really want is:
>    legend(legend = c(expression(L(group("", y, "|") * mu))),
>      x = "center")

Yes, that is what I want. Thanks!

I got additionally response from Roger D. Peng, which stated:

> How about
> legend(legend = expression("L(y | " * mu * ")"), x = "topleft")

That's also OK, but note above comment about "bold" braces.

Regards, Gregor



From r.hankin at noc.soton.ac.uk  Wed Jun 29 13:32:16 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 29 Jun 2005 12:32:16 +0100
Subject: [R] x*x*x*...  vs x^n
Message-ID: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>

Hi

I have been wondering if there one can speed up calculating small powers
of numbers such as x^8 using multiplication.

In addition, one can be a bit clever and calculate x^8 using only 3  
multiplies.

look at this:


 > f1 <- function(x){x*x*x*x*x*x*x*x}
 > f2 <- function(x){x^8}
 > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}

[so f1() and f2() and f3() are algebraically identical]


 > a <- rnorm(1000000)
 > system.time(ignore <- f1(a))
[1] 0.50 0.17 2.88 0.00 0.00

 > system.time(ignore <- f2(a))
[1] 0.31 0.03 1.40 0.00 0.00

 > system.time(ignore <- f3(a))
[1] 0.10 0.07 0.18 0.00 0.00


[these figures show little variance from trial to trial]


I was expecting f2() and f3() to be about the same.
I was not expecting a factor of 3 there!

anyone got any comments?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From knoblauch at lyon.inserm.fr  Wed Jun 29 14:16:49 2005
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Wed, 29 Jun 2005 14:16:49 +0200
Subject: [R]  x*x*x*... vs x^n
Message-ID: <1120047409.42c291317fe0e@webmail.lyon.inserm.fr>

Something like this is exploited very nicely in the mtx.exp 
for matrix powers in the Malmig package, actually.


****************************
Hi

I have been wondering if there one can speed up calculating small powers
of numbers such as x^8 using multiplication.

In addition, one can be a bit clever and calculate x^8 using only 3  
multiplies.

look at this:


 > f1 <- function(x){x*x*x*x*x*x*x*x}
 > f2 <- function(x){x^8}
 > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}

[so f1() and f2() and f3() are algebraically identical]


 > a <- rnorm(1000000)
 > system.time(ignore <- f1(a))
[1] 0.50 0.17 2.88 0.00 0.00

 > system.time(ignore <- f2(a))
[1] 0.31 0.03 1.40 0.00 0.00

 > system.time(ignore <- f3(a))
[1] 0.10 0.07 0.18 0.00 0.00


[these figures show little variance from trial to trial]


I was expecting f2() and f3() to be about the same.
I was not expecting a factor of 3 there!

anyone got any comments?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743

____________________
Ken Knoblauch
Inserm U371, Cerveau et Vision
Department of Cognitive Neurosciences
18 avenue du Doyen Lepine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10
http://www.lyon.inserm.fr/371/



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Jun 29 14:24:38 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 29 Jun 2005 08:24:38 -0400
Subject: [R] x*x*x*...  vs x^n
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40B9@us-arlington-0668.mail.saic.com>

I tried your code and got different results:
	 system.time(ignore <- f1(a))
	[1] 0.83 0.09 1.08   NA   NA
	> system.time(ignore <- f2(a))
	[1] 0.38 0.01 0.41   NA   NA
	> system.time(ignore <- f3(a))
	[1] 0.32 0.04 0.43   NA   NA

So I tried it again but with a loop and got:

	>  for(i in 1:10) cat(system.time(ignore <- f2(a)), "\n")
	0.36 0.04 0.44 NA NA 
	0.32 0.01 0.34 NA NA 
	0.28 0.03 0.32 NA NA 
	0.29 0.03 0.35 NA NA 
	0.3 0.02 0.32 NA NA 
	0.28 0.03 0.32 NA NA 
	0.3 0.02 0.32 NA NA 
	0.29 0.02 0.34 NA NA 
	0.23 0.03 0.32 NA NA 
	0.42 0 0.45 NA NA  

	> for(i in 1:10) cat(system.time(ignore <- f3(a)), "\n")
	0.19 0.04 0.25 NA NA 
	0.17 0.04 0.25 NA NA 
	0.21 0.02 0.25 NA NA 
	0.21 0.02 0.23 NA NA 
	0.18 0.04 0.23 NA NA 
	0.18 0.05 0.23 NA NA 
	0.18 0.04 0.25 NA NA 
	0.17 0.06 0.23 NA NA 
	0.2 0.02 0.23 NA NA 
	0.14 0.06 0.25 NA NA 

It seems to me that f3 is 50% slower than f2 not 300%.

My System is:
- R version: R 2.1.1
- Operating System: Win XP
- Compiler: mingw32-gcc-3.4.2

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
Sent: Wednesday, June 29, 2005 7:32 AM
To: r-help
Subject: [R] x*x*x*... vs x^n

Hi

I have been wondering if there one can speed up calculating small powers of
numbers such as x^8 using multiplication.

In addition, one can be a bit clever and calculate x^8 using only 3
multiplies.

look at this:


 > f1 <- function(x){x*x*x*x*x*x*x*x}
 > f2 <- function(x){x^8}
 > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}

[so f1() and f2() and f3() are algebraically identical]


 > a <- rnorm(1000000)
 > system.time(ignore <- f1(a))
[1] 0.50 0.17 2.88 0.00 0.00

 > system.time(ignore <- f2(a))
[1] 0.31 0.03 1.40 0.00 0.00

 > system.time(ignore <- f3(a))
[1] 0.10 0.07 0.18 0.00 0.00


[these figures show little variance from trial to trial]


I was expecting f2() and f3() to be about the same.
I was not expecting a factor of 3 there!

anyone got any comments?




--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton European Way, Southampton SO14
3ZH, UK
  tel  023-8059-7743

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Wed Jun 29 15:04:30 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 29 Jun 2005 09:04:30 -0400
Subject: [R] x*x*x*...  vs x^n
In-Reply-To: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>
References: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>
Message-ID: <42C29C5E.6010303@stats.uwo.ca>

On 6/29/2005 7:32 AM, Robin Hankin wrote:
> Hi
> 
> I have been wondering if there one can speed up calculating small powers
> of numbers such as x^8 using multiplication.
> 
> In addition, one can be a bit clever and calculate x^8 using only 3  
> multiplies.
> 
> look at this:
> 
> 
>  > f1 <- function(x){x*x*x*x*x*x*x*x}
>  > f2 <- function(x){x^8}
>  > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}
> 
> [so f1() and f2() and f3() are algebraically identical]
> 
> 
>  > a <- rnorm(1000000)
>  > system.time(ignore <- f1(a))
> [1] 0.50 0.17 2.88 0.00 0.00
> 
>  > system.time(ignore <- f2(a))
> [1] 0.31 0.03 1.40 0.00 0.00
> 
>  > system.time(ignore <- f3(a))
> [1] 0.10 0.07 0.18 0.00 0.00
> 
> 
> [these figures show little variance from trial to trial]
> 
> 
> I was expecting f2() and f3() to be about the same.
> I was not expecting a factor of 3 there!
> 
> anyone got any comments?

If you look in src/main/arithmetic.c, you'll see that R does a general 
real-valued power (using C's pow() function) whenever either one of the 
args is real (except for a few special cases, e.g. non-numbers, or 
powers of 2 or 0.5).  There is an internal R function equivalent to your 
f3, but it is not used in the situation of real^integer (and in any 
case, x^8 is real^real).

I suppose if you wanted to submit a patch someone would take a look, but 
the question is whether there is really any calculation whose execution 
time would be materially affected by this.  Most computations are not 
dominated by integer power calculations, so is this really worth the 
trouble?

Duncan Murdoch



From carsten.steinhoff at stud.uni-goettingen.de  Wed Jun 29 15:10:00 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Wed, 29 Jun 2005 15:10:00 +0200
Subject: [R] range of input data
Message-ID: <E1DncKn-0000I5-0v@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050629/cb508369/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 29 15:18:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Jun 2005 14:18:05 +0100 (BST)
Subject: [R] Memory Management under Linux: Problems to allocate large
 amounts of data
In-Reply-To: <52D1AC81378E9342947189B04176014728F70A@agentsmith.komdat.intern>
References: <52D1AC81378E9342947189B04176014728F70A@agentsmith.komdat.intern>
Message-ID: <Pine.LNX.4.61.0506291352430.21726@gannet.stats>

Let's assume this is a 32-bit Xeon and a 32-bit OS (there are 
64-bit-capable Xeons).  Then a user process like R gets a 4GB address 
space, 1GB of which is reserved for the kernel.  So R has a 3GB address 
space, and it is trying to allocate a 2GB contigous chunk.  Because of 
memory fragmentation that is quite unlikely to succeed.

We run 64-bit OSes on all our machines with 2GB or more RAM, for this 
reason.

On Wed, 29 Jun 2005, Dubravko Dolic wrote:

> Dear Group
>
> I'm still trying to bring many data into R (see older postings). After 
> solving some troubles with the database I do most of the work in MySQL. 
> But still I could be nice to work on some data using R. Therefore I can 
> use a dedicated Server with Gentoo Linux as OS hosting only R. This 
> Server is a nice machine with two CPU and 4GB RAM which should do the 
> job:
>
> Dual Intel XEON 3.06 GHz
> 4 x 1 GB RAM PC2100 CL2
> HP Proliant DL380-G3
>
> I read the R-Online help on memory issues and the article on garbage 
> collection from the R-News 01-2001 (Luke Tierney). Also the FAQ and some 
> newsgroup postings were very helpful on understanding memory issues 
> using R.
>
> Now I try to read data from a database. The data I wanted to read 
> consists of 158902553 rows and one field (column) and is of type 
> bigint(20) in the database. I received the message that R could not 
> allocate the 2048000 Kb (almost 2GB) sized vector. As I have 4BG of RAM 
> I could not imagine why this happened. In my understanding R under Linux 
> (32bit) should be able to use the full RAM. As there is not much space 
> used by OS and R as such ("free" shows the use of app. 670 MB after 
> dbSendQuery and fetch) there are 3GB to be occupied by R. Is that 
> correct?

Not really.  The R executable code and the Ncells are already in the 
address space, and this is a virtual memory OS, so the amount of RAM is 
not relevant (it would still be a 3GB limit with 12GB of RAM).

> After that I started R by setting n/vsize explicitly
>
> R --min-vsize=10M --max-vsize=3G --min-nsize=500k --max-nsize=100M
>
>> mem.limits()
>    nsize     vsize
> 104857600        NA
>
> and received the same message.
>
>
> A garbage collection delivered the following information:
>
>> gc()
>         used (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
> Ncells 217234  5.9     500000   13.4       2800    500000   13.4
> Vcells  87472  0.7  157650064 1202.8       3072 196695437 1500.7
>
>
> Now I'm at a loss. Maybe anyone could give me a hint where I should read 
> further or which Information can take me any further

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.hankin at noc.soton.ac.uk  Wed Jun 29 15:31:02 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 29 Jun 2005 14:31:02 +0100
Subject: [R] x*x*x*...  vs x^n
In-Reply-To: <42C29C5E.6010303@stats.uwo.ca>
References: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>
	<42C29C5E.6010303@stats.uwo.ca>
Message-ID: <CB4E7E98-43AC-4DF3-81D0-325812E6E0D3@soc.soton.ac.uk>

Hi  Duncan


On Jun 29, 2005, at 02:04 pm, Duncan Murdoch wrote:

> On 6/29/2005 7:32 AM, Robin Hankin wrote:
>
>> Hi
>> I have been wondering if there one can speed up calculating small  
>> powers
>> of numbers such as x^8 using multiplication.
>> In addition, one can be a bit clever and calculate x^8 using only  
>> 3  multiplies.
>> look at this:
>>  > f1 <- function(x){x*x*x*x*x*x*x*x}
>>  > f2 <- function(x){x^8}
>>  > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}
>>

[snip]

>
> If you look in src/main/arithmetic.c, you'll see that R does a  
> general real-valued power (using C's pow() function) whenever  
> either one of the args is real (except for a few special cases,  
> e.g. non-numbers, or powers of 2 or 0.5).  There is an internal R  
> function equivalent to your f3, but it is not used in the situation  
> of real^integer (and in any case, x^8 is real^real).
>
> I suppose if you wanted to submit a patch someone would take a  
> look, but the question is whether there is really any calculation  
> whose execution time would be materially affected by this.  Most  
> computations are not dominated by integer power calculations, so is  
> this really worth the trouble?
>
> Duncan Murdoch
>


well, the Gnu Scientific Library has the pow_int() functions, which  
are a generalization
of f3(), so someone thinks so. I did a speed test of them but they  
were much slower than
R (for any of f1(), f2(), f3()):

library(gsl)
system.time(ignore <- pow_int(a,8))
[1] 1.07 1.11 3.08 0.00 0.00

<why the slow execution time?>

But I'm far more interested in the philosophy behind your comments.  I
would say that it  definitely *is* worth the trouble because someone,
somewhere, will want fast integer powers, and possibly use R for  
nothing else.

Ken's point about matrix exponentiation is relevant here too.

This is a stated design consideration in Mathematica, I think.

(Of course, I'm not suggesting that other programming tasks be  
suspended!  All I'm pointing
out is that there may exist a user to whom fast integer powers are  
very very important)


very best wishes

rksh


--
Robin Hankin
Uncertainty Analyst
National Oceanography Centre, Southampton
European Way, Southampton SO14 3ZH, UK
  tel  023-8059-7743



From ripley at stats.ox.ac.uk  Wed Jun 29 15:39:08 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Jun 2005 14:39:08 +0100 (BST)
Subject: [R] x*x*x*...  vs x^n
In-Reply-To: <42C29C5E.6010303@stats.uwo.ca>
References: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>
	<42C29C5E.6010303@stats.uwo.ca>
Message-ID: <Pine.LNX.4.61.0506291420200.21726@gannet.stats>

On Wed, 29 Jun 2005, Duncan Murdoch wrote:

> On 6/29/2005 7:32 AM, Robin Hankin wrote:

>> I have been wondering if there one can speed up calculating small powers
>> of numbers such as x^8 using multiplication.
>>
>> In addition, one can be a bit clever and calculate x^8 using only 3
>> multiplies.
>>
>> look at this:
>>
>>
>> > f1 <- function(x){x*x*x*x*x*x*x*x}
>> > f2 <- function(x){x^8}
>> > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}
>>
>> [so f1() and f2() and f3() are algebraically identical]
>>
>>
>> > a <- rnorm(1000000)
>> > system.time(ignore <- f1(a))
>> [1] 0.50 0.17 2.88 0.00 0.00
>>
>> > system.time(ignore <- f2(a))
>> [1] 0.31 0.03 1.40 0.00 0.00
>>
>> > system.time(ignore <- f3(a))
>> [1] 0.10 0.07 0.18 0.00 0.00
>>
>>
>> [these figures show little variance from trial to trial]
>>
>>
>> I was expecting f2() and f3() to be about the same.
>> I was not expecting a factor of 3 there!
>>
>> anyone got any comments?
>
> If you look in src/main/arithmetic.c, you'll see that R does a general
> real-valued power (using C's pow() function) whenever either one of the
> args is real (except for a few special cases, e.g. non-numbers, or
> powers of 2 or 0.5).  There is an internal R function equivalent to your
> f3, but it is not used in the situation of real^integer (and in any
> case, x^8 is real^real).
>
> I suppose if you wanted to submit a patch someone would take a look, but
> the question is whether there is really any calculation whose execution
> time would be materially affected by this.  Most computations are not
> dominated by integer power calculations, so is this really worth the
> trouble?

As Luke Tierney frequently points out, selecting special cases can take 
more time than you save.  The assembler code used by modern OSes will
have worked out that compromise pretty effectively: even for real^real it 
spots simple cases of the exponent.

Also, it depends on the CPU: I get on Athlon 2600

> system.time(ignore <- f1(a), gcFirst=T)
[1] 0.08 0.05 0.14 0.00 0.00
> system.time(ignore <- f2(a), gcFirst=T)
[1] 0.20 0.01 0.20 0.00 0.00
> system.time(ignore <- f3(a), gcFirst=T)
[1] 0.03 0.02 0.05 0.00 0.00

and Opteron 248

> system.time(ignore <- f1(a), gcFirst=T)
[1] 0.08 0.06 0.14 0.00 0.00
> system.time(ignore <- f2(a), gcFirst=T)
[1] 0.19 0.01 0.20 0.00 0.00
> system.time(ignore <- f3(a), gcFirst=T)
[1] 0.04 0.01 0.05 0.00 0.00

Note

1) the use of gcFirst=T
2) these need to be run several times to tune the gc() behaviour.  After 
which f1(a) caused a couple of gc()s and the others none.
3) the Opteron is in general a much faster machine so these are all 
surprisingly slow.

Occasionally the C-level pow() is slow: that happened in one MinGW update
and for R Windows users we replaced it.  Even there I was unsure that it 
would make enough difference on a real problem, but eventually found one 
where it did (MDS with Minkowski distances).  It was because of that real 
problem that I added the special case for 0.5, after careful timing
(and hearing Luke's comment alluded to above).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Wed Jun 29 15:47:43 2005
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 29 Jun 2005 09:47:43 -0400
Subject: [R] x*x*x*...  vs x^n
In-Reply-To: <CB4E7E98-43AC-4DF3-81D0-325812E6E0D3@soc.soton.ac.uk>
References: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>
	<42C29C5E.6010303@stats.uwo.ca>
	<CB4E7E98-43AC-4DF3-81D0-325812E6E0D3@soc.soton.ac.uk>
Message-ID: <42C2A67F.20002@stats.uwo.ca>

On 6/29/2005 9:31 AM, Robin Hankin wrote:
> Hi  Duncan
> 
> 
> On Jun 29, 2005, at 02:04 pm, Duncan Murdoch wrote:
> 
>> On 6/29/2005 7:32 AM, Robin Hankin wrote:
>>
>>> Hi
>>> I have been wondering if there one can speed up calculating small  
>>> powers
>>> of numbers such as x^8 using multiplication.
>>> In addition, one can be a bit clever and calculate x^8 using only  
>>> 3  multiplies.
>>> look at this:
>>>  > f1 <- function(x){x*x*x*x*x*x*x*x}
>>>  > f2 <- function(x){x^8}
>>>  > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}
>>>
> 
> [snip]
> 
>>
>> If you look in src/main/arithmetic.c, you'll see that R does a  
>> general real-valued power (using C's pow() function) whenever  
>> either one of the args is real (except for a few special cases,  
>> e.g. non-numbers, or powers of 2 or 0.5).  There is an internal R  
>> function equivalent to your f3, but it is not used in the situation  
>> of real^integer (and in any case, x^8 is real^real).
>>
>> I suppose if you wanted to submit a patch someone would take a  
>> look, but the question is whether there is really any calculation  
>> whose execution time would be materially affected by this.  Most  
>> computations are not dominated by integer power calculations, so is  
>> this really worth the trouble?
>>
>> Duncan Murdoch
>>
> 
> 
> well, the Gnu Scientific Library has the pow_int() functions, which  
> are a generalization
> of f3(), so someone thinks so. I did a speed test of them but they  
> were much slower than
> R (for any of f1(), f2(), f3()):
> 
> library(gsl)
> system.time(ignore <- pow_int(a,8))
> [1] 1.07 1.11 3.08 0.00 0.00
> 
> <why the slow execution time?>

Shouldn't you ask the gsl maintainer that? :-)

> But I'm far more interested in the philosophy behind your comments.  I
> would say that it  definitely *is* worth the trouble because someone,
> somewhere, will want fast integer powers, and possibly use R for  
> nothing else.
> 
> Ken's point about matrix exponentiation is relevant here too.
> 
> This is a stated design consideration in Mathematica, I think.
> 
> (Of course, I'm not suggesting that other programming tasks be  
> suspended!  All I'm pointing
> out is that there may exist a user to whom fast integer powers are  
> very very important)

Then that user should submit the patch, not you.  But whoever does it 
should include an argument to convince an R core member that the change 
is worth looking at, and do it well enough that the patch is accepted.
Changes to the way R does arithmetic affect everyone, so they had better 
be done right, and checking them takes time.

Duncan Murdoch



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Jun 29 15:56:54 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 29 Jun 2005 09:56:54 -0400
Subject: [R] "all connections are in use" error during lazyload stage of
	packa	ge installation
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40BA@us-arlington-0668.mail.saic.com>

Hi,

I suddenly started getting strange errors while working on my caTools
package:

     >RCMD install C:/programs/R/rw2011/src/library/caTools
      ......
	preparing package caTools for lazy loading
	Error in file(file, "r", encoding = encoding) :
	        all connections are in use
	Execution halted
	make: *** [lazyload] Error 1
	*** Installation of caTools failed ***

I searched through R website and mailing lists but did not found many clues.
I am not running R environment so showConnections and CloseAllConnections
functions are not helpful. Did I run into some OS specific limit on number
of files allowed? I closed all other programs and rebooted my machine but it
did not help.

Any hints would be appreciated.

My System is:
- R version: R 2.1.1
- Operating System: Win XP
- Compiler: mingw32-gcc-3.4.2

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



From ggrothendieck at gmail.com  Wed Jun 29 16:07:20 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 29 Jun 2005 10:07:20 -0400
Subject: [R] "all connections are in use" error during lazyload stage of
	packa ge installation
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40BA@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40BA@us-arlington-0668.mail.saic.com>
Message-ID: <971536df05062907074811f1de@mail.gmail.com>

On 6/29/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> Hi,
> 
> I suddenly started getting strange errors while working on my caTools
> package:
> 
>     >RCMD install C:/programs/R/rw2011/src/library/caTools
>      ......
>        preparing package caTools for lazy loading
>        Error in file(file, "r", encoding = encoding) :
>                all connections are in use
>        Execution halted
>        make: *** [lazyload] Error 1
>        *** Installation of caTools failed ***
> 
> I searched through R website and mailing lists but did not found many clues.
> I am not running R environment so showConnections and CloseAllConnections
> functions are not helpful. Did I run into some OS specific limit on number
> of files allowed? I closed all other programs and rebooted my machine but it
> did not help.

I don't know what the problem is but you could see if adding

LazyLoad: no

to the DESCRIPTION file causes it to go away.  Even if it does it
would probably be best to track down what the cause is and in the past 
when I have had problems running R CMD on a package of mine I 
repeatedly removed half my package and reran R CMD until I found the 
offending portion.



From r.hankin at noc.soton.ac.uk  Wed Jun 29 16:12:36 2005
From: r.hankin at noc.soton.ac.uk (Robin Hankin)
Date: Wed, 29 Jun 2005 15:12:36 +0100
Subject: [R] x*x*x*...  vs x^n
In-Reply-To: <42C2A67F.20002@stats.uwo.ca>
References: <9979E6C1-B65B-49BB-A046-3434E884EF5C@soc.soton.ac.uk>
	<42C29C5E.6010303@stats.uwo.ca>
	<CB4E7E98-43AC-4DF3-81D0-325812E6E0D3@soc.soton.ac.uk>
	<42C2A67F.20002@stats.uwo.ca>
Message-ID: <6968BCED-B745-4694-A45A-E33C87B53B81@soc.soton.ac.uk>


On Jun 29, 2005, at 02:47 pm, Duncan Murdoch wrote:

> On 6/29/2005 9:31 AM, Robin Hankin wrote:
>
>> Hi  Duncan
>>
>>
>> library(gsl)
>> system.time(ignore <- pow_int(a,8))
>> [1] 1.07 1.11 3.08 0.00 0.00
>>
>> <why the slow execution time?>
>>
>
> Shouldn't you ask the gsl maintainer that? :-)
>

well  I did ask myself, but  in this case the gsl maintainer
told me to ask the gsl co-author, who
is no doubt much better informed in these matters ;-)

>>
>> (Of course, I'm not suggesting that other programming tasks be
>> suspended!  All I'm pointing
>> out is that there may exist a user to whom fast integer powers are
>> very very important)
>>
>
> Then that user should submit the patch, not you.  But whoever does it
> should include an argument to convince an R core member that the  
> change
> is worth looking at, and do it well enough that the patch is accepted.
> Changes to the way R does arithmetic affect everyone, so they had  
> better
> be done right, and checking them takes time.
>

yes, that's a fair point.
But including a native R command pow.int(), say, wouldn't  affect  
anyone, would it?
One could even use the (tested) GSL code, as it is GPL'ed.

This would just be a new function that users could use at their  
discretion, and no
existing code would break.

I assume that such a function would  not suffer whatever performance  
disadvantage
that the GSL package approach had, so it may well be quite a  
significant improvement
over the method used by R_pow_di() in arithmetic.c especially for  
large n.


best wishes

rksh

> Duncan Murdoch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting- 
> guide.html
>



From vincent at 7d4.com  Wed Jun 29 16:23:25 2005
From: vincent at 7d4.com (vincent)
Date: Wed, 29 Jun 2005 16:23:25 +0200
Subject: [R] moving correlation coef ?
Message-ID: <42C2AEDD.6050207@7d4.com>

Hello,

R gives us the correlation functions cor(). (Many thanks ;-))
Does it also exist a "moving correlation" coefficient ?
(like the moving average).
If not, could someone give me some infos or link
on how to practically implement such a function in R.

(I did search for "moving correlation" on the R homepage
but didn't find anything.)

Thank you.
Vincent



From sundar.dorai-raj at pdf.com  Wed Jun 29 16:31:54 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 29 Jun 2005 09:31:54 -0500
Subject: [R] moving correlation coef ?
In-Reply-To: <42C2AEDD.6050207@7d4.com>
References: <42C2AEDD.6050207@7d4.com>
Message-ID: <42C2B0DA.6050804@pdf.com>



vincent wrote:
> Hello,
> 
> R gives us the correlation functions cor(). (Many thanks ;-))
> Does it also exist a "moving correlation" coefficient ?
> (like the moving average).
> If not, could someone give me some infos or link
> on how to practically implement such a function in R.
> 
> (I did search for "moving correlation" on the R homepage
> but didn't find anything.)
> 
> Thank you.
> Vincent
> 


See ?running in the gtools package:

library(gtools)
X <- rnorm(100); Y <- rnorm(100)
running(X, Y, cor)

HTH,

--sundar



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Jun 29 16:33:52 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 29 Jun 2005 10:33:52 -0400
Subject: [R] "all connections are in use" error during lazyload stage of
	packa ge installation
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40BB@us-arlington-0668.mail.saic.com>

 I found the problem, by doing comparison of directories and files of
working and not working versions, and applying changes one by one until one
caused install to fail. It was a case of having a call to a "source"
function somewhere in my code, that I forgot about. 

I was definitely doing something silly , but it was not a meaningful error
message.

Jarek
====================================================\=======

 Jarek Tuszynski, PhD.                           o / \ 
 Science Applications International Corporation  <\__,|  
 (703) 676-4192                                   ">   \
 Jaroslaw.W.Tuszynski at saic.com                     `    \



-----Original Message-----
From: ggrothendieck at gmail.com [mailto:ggrothendieck at gmail.com] 
Sent: Wednesday, June 29, 2005 10:07 AM
To: Tuszynski, Jaroslaw W.
Cc: r help
Subject: Re: [R] "all connections are in use" error during lazyload stage of
packa ge installation

On 6/29/05, Tuszynski, Jaroslaw W. <JAROSLAW.W.TUSZYNSKI at saic.com> wrote:
> Hi,
> 
> I suddenly started getting strange errors while working on my caTools
> package:
> 
>     >RCMD install C:/programs/R/rw2011/src/library/caTools
>      ......
>        preparing package caTools for lazy loading
>        Error in file(file, "r", encoding = encoding) :
>                all connections are in use
>        Execution halted
>        make: *** [lazyload] Error 1
>        *** Installation of caTools failed ***
> 
> I searched through R website and mailing lists but did not found many
clues.
> I am not running R environment so showConnections and 
> CloseAllConnections functions are not helpful. Did I run into some OS 
> specific limit on number of files allowed? I closed all other programs 
> and rebooted my machine but it did not help.

I don't know what the problem is but you could see if adding

LazyLoad: no

to the DESCRIPTION file causes it to go away.  Even if it does it would
probably be best to track down what the cause is and in the past when I have
had problems running R CMD on a package of mine I repeatedly removed half my
package and reran R CMD until I found the offending portion.



From rvaradha at jhsph.edu  Wed Jun 29 16:42:25 2005
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Wed, 29 Jun 2005 10:42:25 -0400
Subject: [R] x*x*x*...  vs x^n
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40B9@us-arlington-0668.mail.saic.com>
Message-ID: <OWA-16xfPTeip68V8zD000046e9@owa-1.sph.ad.jhsph.edu>


I ran 100 repetitions of the 3 multiplications that Robin had compared.
Here are the summaries of system times (I only took the first component of
system.time) that I obtained.  It is clear that f1() is nearly twice as slow
as f2() which is slightly slower (not 3 times slower as claimed by Robin)
than f3(). So, I don't think that there is much to choose between the
"cleverer" way and the most obvious way to compute integer powers.

> summary(f1time)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.060   0.170   0.210   0.199   0.230   0.300 
> summary(f2time)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.060   0.100   0.110   0.128   0.170   0.190 
> summary(f3time)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.0000  0.0300  0.0950  0.0779  0.1100  0.1300 

Ravi.

--------------------------------------------------------------------------
Ravi Varadhan, Ph.D.
Assistant Professor,  The Center on Aging and Health
Division of Geriatric Medicine and Gerontology
Johns Hopkins University
Ph: (410) 502-2619
Fax: (410) 614-9625
Email:  rvaradhan at jhmi.edu
--------------------------------------------------------------------------
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Tuszynski, Jaroslaw W.
> Sent: Wednesday, June 29, 2005 8:25 AM
> To: 'Robin Hankin'; r-help
> Subject: Re: [R] x*x*x*... vs x^n
> 
> I tried your code and got different results:
> 	 system.time(ignore <- f1(a))
> 	[1] 0.83 0.09 1.08   NA   NA
> 	> system.time(ignore <- f2(a))
> 	[1] 0.38 0.01 0.41   NA   NA
> 	> system.time(ignore <- f3(a))
> 	[1] 0.32 0.04 0.43   NA   NA
> 
> So I tried it again but with a loop and got:
> 
> 	>  for(i in 1:10) cat(system.time(ignore <- f2(a)), "\n")
> 	0.36 0.04 0.44 NA NA
> 	0.32 0.01 0.34 NA NA
> 	0.28 0.03 0.32 NA NA
> 	0.29 0.03 0.35 NA NA
> 	0.3 0.02 0.32 NA NA
> 	0.28 0.03 0.32 NA NA
> 	0.3 0.02 0.32 NA NA
> 	0.29 0.02 0.34 NA NA
> 	0.23 0.03 0.32 NA NA
> 	0.42 0 0.45 NA NA
> 
> 	> for(i in 1:10) cat(system.time(ignore <- f3(a)), "\n")
> 	0.19 0.04 0.25 NA NA
> 	0.17 0.04 0.25 NA NA
> 	0.21 0.02 0.25 NA NA
> 	0.21 0.02 0.23 NA NA
> 	0.18 0.04 0.23 NA NA
> 	0.18 0.05 0.23 NA NA
> 	0.18 0.04 0.25 NA NA
> 	0.17 0.06 0.23 NA NA
> 	0.2 0.02 0.23 NA NA
> 	0.14 0.06 0.25 NA NA
> 
> It seems to me that f3 is 50% slower than f2 not 300%.
> 
> My System is:
> - R version: R 2.1.1
> - Operating System: Win XP
> - Compiler: mingw32-gcc-3.4.2
> 
> Jarek
> ====================================================\=======
> 
>  Jarek Tuszynski, PhD.                           o / \
>  Science Applications International Corporation  <\__,|
>  (703) 676-4192                                   ">   \
>  Jaroslaw.W.Tuszynski at saic.com                     `    \
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Wednesday, June 29, 2005 7:32 AM
> To: r-help
> Subject: [R] x*x*x*... vs x^n
> 
> Hi
> 
> I have been wondering if there one can speed up calculating small powers
> of
> numbers such as x^8 using multiplication.
> 
> In addition, one can be a bit clever and calculate x^8 using only 3
> multiplies.
> 
> look at this:
> 
> 
>  > f1 <- function(x){x*x*x*x*x*x*x*x}
>  > f2 <- function(x){x^8}
>  > f3 <- function(x){x2 <- x*x;x4 <- x2*x2;return(x4*x4)}
> 
> [so f1() and f2() and f3() are algebraically identical]
> 
> 
>  > a <- rnorm(1000000)
>  > system.time(ignore <- f1(a))
> [1] 0.50 0.17 2.88 0.00 0.00
> 
>  > system.time(ignore <- f2(a))
> [1] 0.31 0.03 1.40 0.00 0.00
> 
>  > system.time(ignore <- f3(a))
> [1] 0.10 0.07 0.18 0.00 0.00
> 
> 
> [these figures show little variance from trial to trial]
> 
> 
> I was expecting f2() and f3() to be about the same.
> I was not expecting a factor of 3 there!
> 
> anyone got any comments?
> 
> 
> 
> 
> --
> Robin Hankin
> Uncertainty Analyst
> National Oceanography Centre, Southampton European Way, Southampton SO14
> 3ZH, UK
>   tel  023-8059-7743
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From davidr at rhotrading.com  Wed Jun 29 16:49:31 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 29 Jun 2005 09:49:31 -0500
Subject: [R] x*x*x*...  vs x^n
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FB9C8@rhosvr02.rhotrading.com>

In general, the "Russian peasant algorithm", which requires only O(log
n) multiplications, is very good. Section 4.6.3 of Knuth's The Art of
Computer Programming. Volume 2: Seminumerical Algorithms has an in depth
discussion.
I have had to use this in the past, when computers were slower and
compilers were not so clever. It is also better when x is not just a
real number, say complex or matrix (as has been mentioned.)
In many cases though, one needs many powers sequentially, and then it
may be more efficient to just multiply the previous power by x and use
the power, etc. (unless you have a parallel computer.)
So 

pows <- x^(1:1000) 
# use pows in calculations

could be sped up by employing a faster algorithm, but probably a loop
will be faster:

pows <- 1
for(i in 1:1000) {
  pows <- pows * x
  # use this power
}

David L. Reiner, Ph.D.
Rho Trading
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> Sent: Wednesday, June 29, 2005 9:13 AM
> To: Duncan Murdoch
> Cc: r-help; Robin Hankin
> Subject: Re: [R] x*x*x*... vs x^n
> 
> 
> On Jun 29, 2005, at 02:47 pm, Duncan Murdoch wrote:
> 
> > On 6/29/2005 9:31 AM, Robin Hankin wrote:
> >
> >> Hi  Duncan
> >>
> >>
> >> library(gsl)
> >> system.time(ignore <- pow_int(a,8))
> >> [1] 1.07 1.11 3.08 0.00 0.00
> >>
> >> <why the slow execution time?>
> >>
> >
> > Shouldn't you ask the gsl maintainer that? :-)
> >
> 
> well  I did ask myself, but  in this case the gsl maintainer
> told me to ask the gsl co-author, who
> is no doubt much better informed in these matters ;-)
> 
> >>
> >> (Of course, I'm not suggesting that other programming tasks be
> >> suspended!  All I'm pointing
> >> out is that there may exist a user to whom fast integer powers are
> >> very very important)
> >>
> >
> > Then that user should submit the patch, not you.  But whoever does
it
> > should include an argument to convince an R core member that the
> > change
> > is worth looking at, and do it well enough that the patch is
accepted.
> > Changes to the way R does arithmetic affect everyone, so they had
> > better
> > be done right, and checking them takes time.
> >
> 
> yes, that's a fair point.
> But including a native R command pow.int(), say, wouldn't  affect
> anyone, would it?
> One could even use the (tested) GSL code, as it is GPL'ed.
> 
> This would just be a new function that users could use at their
> discretion, and no
> existing code would break.
> 
> I assume that such a function would  not suffer whatever performance
> disadvantage
> that the GSL package approach had, so it may well be quite a
> significant improvement
> over the method used by R_pow_di() in arithmetic.c especially for
> large n.
> 
> 
> best wishes
> 
> rksh
> 
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From ghislainv at gmail.com  Wed Jun 29 16:53:51 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Wed, 29 Jun 2005 16:53:51 +0200
Subject: [R] Selecting rows regarding the frequency of a factor variable.
Message-ID: <42C2B5FF.1020700@gmail.com>

Hi and sorry to disturb,

I'll try to be as clear as possible:
I want to select rows of a data frame called "Data2.Iso" regarding the 
frequency of a factor variable called "Variete" that I want ">=4".

I used function table to have the frequency:
 > FRAMEVARIETE<-as.data.frame(table(Data2.Iso$Variete))
Then I selected the modalities with a frequency >=4:
 > FRAMEVARIETE2<-FRAMEVARIETE[FRAMEVARIETE$Freq>=4,]
 > as.character(FRAMEVARIETE2[,"Variete"])

But then, how to select the rows with those modalities?
Does anyone can help me?

Thanks!

Ghislain.



From spencer.graves at pdf.com  Wed Jun 29 16:57:38 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 29 Jun 2005 07:57:38 -0700
Subject: [R] moving correlation coef ?
In-Reply-To: <42C2B0DA.6050804@pdf.com>
References: <42C2AEDD.6050207@7d4.com> <42C2B0DA.6050804@pdf.com>
Message-ID: <42C2B6E2.1080502@pdf.com>

or ?rapply in package zoo.

	  spencer graves

Sundar Dorai-Raj wrote:

> 
> vincent wrote:
> 
>>Hello,
>>
>>R gives us the correlation functions cor(). (Many thanks ;-))
>>Does it also exist a "moving correlation" coefficient ?
>>(like the moving average).
>>If not, could someone give me some infos or link
>>on how to practically implement such a function in R.
>>
>>(I did search for "moving correlation" on the R homepage
>>but didn't find anything.)
>>
>>Thank you.
>>Vincent
>>
> 
> 
> 
> See ?running in the gtools package:
> 
> library(gtools)
> X <- rnorm(100); Y <- rnorm(100)
> running(X, Y, cor)
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From andy_liaw at merck.com  Wed Jun 29 17:09:51 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 29 Jun 2005 11:09:51 -0400
Subject: [R] Selecting rows regarding the frequency of a factor variab
 le.
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA3E@usctmx1106.Merck.com>

See if this does what you want:

> dat <- data.frame(f=factor(sample(letters[1:10], 100, replace=TRUE)),
x=runif(100))
> str(dat)
`data.frame':   100 obs. of  2 variables:
 $ f: Factor w/ 10 levels "a","b","c","d",..: 2 5 10 9 10 3 9 8 3 1 ...
 $ x: num  0.9162 0.0481 0.3048 0.0938 0.8599 ...
> g <- names(which(table(dat$f) > 11))
> g
[1] "c" "j"
> dat[dat$f %in% g,]
   f          x
3  j 0.30477413
5  j 0.85992597
6  c 0.86881528
9  c 0.87317095
16 c 0.84252048
18 j 0.24039606
19 j 0.58927414
21 j 0.10077745
32 j 0.72275870
35 c 0.26001549
37 j 0.09608521
40 c 0.15481625
44 c 0.70203309
47 c 0.95907223
50 j 0.35258966
54 c 0.93422614
58 c 0.36546841
61 c 0.55123183
64 j 0.82995122
65 c 0.89104229
66 j 0.81661377
77 j 0.21134708
87 c 0.16602335
92 c 0.02175573
96 j 0.97864088

Andy

> From:  Ghislain Vieilledent
> 
> 
> Hi and sorry to disturb,
> 
> I'll try to be as clear as possible:
> I want to select rows of a data frame called "Data2.Iso" 
> regarding the 
> frequency of a factor variable called "Variete" that I want ">=4".
> 
> I used function table to have the frequency:
>  > FRAMEVARIETE<-as.data.frame(table(Data2.Iso$Variete))
> Then I selected the modalities with a frequency >=4:
>  > FRAMEVARIETE2<-FRAMEVARIETE[FRAMEVARIETE$Freq>=4,]
>  > as.character(FRAMEVARIETE2[,"Variete"])
> 
> But then, how to select the rows with those modalities?
> Does anyone can help me?
> 
> Thanks!
> 
> Ghislain.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From carsten.steinhoff at stud.uni-goettingen.de  Wed Jun 29 17:19:51 2005
From: carsten.steinhoff at stud.uni-goettingen.de (Carsten Steinhoff)
Date: Wed, 29 Jun 2005 17:19:51 +0200
Subject: [R] MLE with optim
Message-ID: <E1DneMS-0003rY-Du@s2.stud.uni-goettingen.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050629/484438bd/attachment.pl

From davidr at rhotrading.com  Wed Jun 29 17:23:54 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 29 Jun 2005 10:23:54 -0500
Subject: [R] x*x*x*...  vs x^n
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FB9CE@rhosvr02.rhotrading.com>

Looking at the code for gsl_pow_int, I see they do use that method.

David L. Reiner
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of David Reiner
> <davidr at rhotrading.com>
> Sent: Wednesday, June 29, 2005 9:50 AM
> To: r-help
> Subject: Re: [R] x*x*x*... vs x^n
> 
> In general, the "Russian peasant algorithm", which requires only O(log
> n) multiplications, is very good. Section 4.6.3 of Knuth's The Art of
> Computer Programming. Volume 2: Seminumerical Algorithms has an in
depth
> discussion.
> I have had to use this in the past, when computers were slower and
> compilers were not so clever. It is also better when x is not just a
> real number, say complex or matrix (as has been mentioned.)
> In many cases though, one needs many powers sequentially, and then it
> may be more efficient to just multiply the previous power by x and use
> the power, etc. (unless you have a parallel computer.)
> So
> 
> pows <- x^(1:1000)
> # use pows in calculations
> 
> could be sped up by employing a faster algorithm, but probably a loop
> will be faster:
> 
> pows <- 1
> for(i in 1:1000) {
>   pows <- pows * x
>   # use this power
> }
> 
> David L. Reiner, Ph.D.
> Rho Trading
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> > Sent: Wednesday, June 29, 2005 9:13 AM
> > To: Duncan Murdoch
> > Cc: r-help; Robin Hankin
> > Subject: Re: [R] x*x*x*... vs x^n
> >
> >
> > On Jun 29, 2005, at 02:47 pm, Duncan Murdoch wrote:
> >
> > > On 6/29/2005 9:31 AM, Robin Hankin wrote:
> > >
> > >> Hi  Duncan
> > >>
> > >>
> > >> library(gsl)
> > >> system.time(ignore <- pow_int(a,8))
> > >> [1] 1.07 1.11 3.08 0.00 0.00
> > >>
> > >> <why the slow execution time?>
> > >>
> > >
> > > Shouldn't you ask the gsl maintainer that? :-)
> > >
> >
> > well  I did ask myself, but  in this case the gsl maintainer
> > told me to ask the gsl co-author, who
> > is no doubt much better informed in these matters ;-)
> >
> > >>
> > >> (Of course, I'm not suggesting that other programming tasks be
> > >> suspended!  All I'm pointing
> > >> out is that there may exist a user to whom fast integer powers
are
> > >> very very important)
> > >>
> > >
> > > Then that user should submit the patch, not you.  But whoever does
> it
> > > should include an argument to convince an R core member that the
> > > change
> > > is worth looking at, and do it well enough that the patch is
> accepted.
> > > Changes to the way R does arithmetic affect everyone, so they had
> > > better
> > > be done right, and checking them takes time.
> > >
> >
> > yes, that's a fair point.
> > But including a native R command pow.int(), say, wouldn't  affect
> > anyone, would it?
> > One could even use the (tested) GSL code, as it is GPL'ed.
> >
> > This would just be a new function that users could use at their
> > discretion, and no
> > existing code would break.
> >
> > I assume that such a function would  not suffer whatever performance
> > disadvantage
> > that the GSL package approach had, so it may well be quite a
> > significant improvement
> > over the method used by R_pow_di() in arithmetic.c especially for
> > large n.
> >
> >
> > best wishes
> >
> > rksh
> >
> > > Duncan Murdoch
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
http://www.R-project.org/posting-
> > > guide.html
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From sundar.dorai-raj at pdf.com  Wed Jun 29 17:28:43 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 29 Jun 2005 10:28:43 -0500
Subject: [R] MLE with optim
In-Reply-To: <E1DneMS-0003rY-Du@s2.stud.uni-goettingen.de>
References: <E1DneMS-0003rY-Du@s2.stud.uni-goettingen.de>
Message-ID: <42C2BE2B.9010904@pdf.com>



Carsten Steinhoff wrote:
> Hello,
>  
> I tried to fit a lognormal distribution by using optim. But sadly the output
> seems to be incorrect.
> Who can tell me where the "bug" is?
>  
> test     =     rlnorm(100,5,3)
> logL    =     function(parm, x,...) -sum(log(dlnorm(x,parm,...)))
> start    =     list(meanlog=5, sdlog=3)
> optim(start,logL,x=test)$par
>  
> Carsten.
> 

You are only supplying the meanlog argument to dlnorm. Also you can set 
log = TRUE in dlnorm to avoid the log computation after calling dlnorm.

set.seed(1)
x <- rlnorm(100, 5, 3)
logL <- function(par, x) -sum(dlnorm(x, par[1], par[2], TRUE))
start <- list(meanlog = 5, sdlog = 2)
optim(start, logL, x = x)

Why not just use MASS::fitdistr instead?

library(MASS)
fitdistr(x, dlnorm, start)

HTH,

--sundar



From dimitris.rizopoulos at med.kuleuven.be  Wed Jun 29 17:36:09 2005
From: dimitris.rizopoulos at med.kuleuven.be (Dimitris Rizopoulos)
Date: Wed, 29 Jun 2005 17:36:09 +0200
Subject: [R] MLE with optim
References: <E1DneMS-0003rY-Du@s2.stud.uni-goettingen.de>
Message-ID: <013001c57cc0$45bc4aa0$0540210a@www.domain>

the following work for me:


x <- rlnorm(1000, 5, 3)

fn <- function(parms, dat) -sum(dlnorm(dat, parms[1], parms[2], log = 
TRUE))
optim(c(5, 3), fn, dat = x)

library(MASS)
fitdistr(x, "log-normal", list(meanlog = 5,  sdlog = 3))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Carsten Steinhoff" <carsten.steinhoff at stud.uni-goettingen.de>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 29, 2005 5:19 PM
Subject: [R] MLE with optim


> Hello,
>
> I tried to fit a lognormal distribution by using optim. But sadly 
> the output
> seems to be incorrect.
> Who can tell me where the "bug" is?
>
> test     =     rlnorm(100,5,3)
> logL    =     function(parm, x,...) -sum(log(dlnorm(x,parm,...)))
> start    =     list(meanlog=5, sdlog=3)
> optim(start,logL,x=test)$par
>
> Carsten.
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From vincent at 7d4.com  Wed Jun 29 18:02:00 2005
From: vincent at 7d4.com (vincent)
Date: Wed, 29 Jun 2005 18:02:00 +0200
Subject: [R] moving correlation coef ?
In-Reply-To: <42C2B6E2.1080502@pdf.com>
References: <42C2AEDD.6050207@7d4.com> <42C2B0DA.6050804@pdf.com>
	<42C2B6E2.1080502@pdf.com>
Message-ID: <42C2C5F8.8050505@7d4.com>

Thank you for your answers.

In fact, i believe my question wasn't precise enough.
I don't want to have a moving/sliding windows over the data
to correlate (i am already doing that).

If I have 2 vectors
X = (x1, x2, x3, ..., xt)
Y = (y1, y2, x3, ..., yt)
I want the most recent elements (t) to have a heavier weight
in the correlation calculus than the older ones (1).

I think that one simple way to do that is for example to
compute cor() over XP, YP where
XP = (x1, x2, x2, x3, x3, x3, ..., xt, xt, xt)
YP = (y1, y2, y2, y3, y3, y3, ..., yt, yt, yt)
ie where each element is repeated several times according
to its freshness.
It's quite naive ! so if there is a cleaver idea, many thanks.

Thanks
Vincent



From Andreas.Neumann at em.uni-karlsruhe.de  Wed Jun 29 18:19:07 2005
From: Andreas.Neumann at em.uni-karlsruhe.de (Andreas Neumann)
Date: Wed, 29 Jun 2005 18:19:07 +0200 (CEST)
Subject: [R] poly() in lm() leads to wrong coefficients (but correct
	residuals)
Message-ID: <Pine.LNX.4.30.0506291751230.16787-100000@em-tobin.em.uni-karlsruhe.de>

Dear all,

I am using poly() in lm() in the following form.

1> DelsDPWOS.lm3 <- lm(DelsPDWOS[,1] ~ poly(DelsPDWOS[,4],3))

2> DelsDPWOS.I.lm3 <- lm(DelsPDWOS[,1] ~ poly(I(DelsPDWOS[,4]),3))

3> DelsDPWOS.2.lm3 <-
   lm(DelsPDWOS[,1]~DelsPDWOS[,4]+I(DelsPDWOS[,4]^2)+I(DelsPDWOS[,4]^3))

1 and 2 lead to identical but wrong results. 3 is correct. Surprisingly
(to me) the residuals are the same (correct) in all cases although the
coefficients of 1 (and 2) are wrong and do not in any way match the
stated regression polynomial. (summaries below)

QUESTION:
Is there a correct way to use poly() in lm() since version 3 becomes quite
tedious if used more often especially with higher order polynomials?

I am using R Version 1.9.0 (2004-04-12).

Thank you for your time,

    Andreas Neumann



SUMMARIES:
> summary(DelsDPWOS.lm3)

Call:
lm(formula = DelsPDWOS[, 1] ~ poly(DelsPDWOS[, 4], 3))

Residuals:
       1        2        3        4        5        6        7        8
-0.11414  0.12756 -0.21060  0.04636 -0.03244  0.16030  0.04290 -0.03944
       9
 0.01949

Coefficients:
                         Estimate Std. Error t value Pr(>|t|)
(Intercept)               1.33333    0.04861  27.430 1.21e-06 ***
poly(DelsPDWOS[, 4], 3)1 -1.27464    0.14583  -8.741 0.000325 ***
poly(DelsPDWOS[, 4], 3)2  0.27483    0.14583   1.885 0.118175
poly(DelsPDWOS[, 4], 3)3  0.11590    0.14583   0.795 0.462774
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.1458 on 5 degrees of freedom
Multiple R-Squared: 0.9416,	Adjusted R-squared: 0.9065
F-statistic: 26.86 on 3 and 5 DF,  p-value: 0.001645

> summary(DelsDPWOS.2.lm3)

Call:
lm(formula = DelsPDWOS[, 1] ~ DelsPDWOS[, 4] + I(DelsPDWOS[,
    4]^2) + I(DelsPDWOS[, 4]^3))

Residuals:
       1        2        3        4        5        6        7        8
-0.11414  0.12756 -0.21060  0.04636 -0.03244  0.16030  0.04290 -0.03944
       9
 0.01949

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)
(Intercept)           -1.486      7.242  -0.205    0.846
DelsPDWOS[, 4]         8.640     14.161   0.610    0.568
I(DelsPDWOS[, 4]^2)   -6.526      8.799  -0.742    0.492
I(DelsPDWOS[, 4]^3)    1.375      1.730   0.795    0.463

Residual standard error: 0.1458 on 5 degrees of freedom
Multiple R-Squared: 0.9416,	Adjusted R-squared: 0.9065
F-statistic: 26.86 on 3 and 5 DF,  p-value: 0.001645



From markus.jantti at iki.fi  Wed Jun 29 18:43:56 2005
From: markus.jantti at iki.fi (Markus =?ISO-8859-1?Q?J=E4ntti?=)
Date: Wed, 29 Jun 2005 19:43:56 +0300
Subject: [R] poly() in lm() leads to wrong coefficients (but
	correct	residuals)
In-Reply-To: <Pine.LNX.4.30.0506291751230.16787-100000@em-tobin.em.uni-karlsruhe.de>
References: <Pine.LNX.4.30.0506291751230.16787-100000@em-tobin.em.uni-karlsruhe.de>
Message-ID: <1120063436.7098.21.camel@maynard>

On Wed, 2005-06-29 at 18:19 +0200, Andreas Neumann wrote:
> Dear all,
> 
> I am using poly() in lm() in the following form.
> 
> 1> DelsDPWOS.lm3 <- lm(DelsPDWOS[,1] ~ poly(DelsPDWOS[,4],3))
> 
> 2> DelsDPWOS.I.lm3 <- lm(DelsPDWOS[,1] ~ poly(I(DelsPDWOS[,4]),3))
> 
> 3> DelsDPWOS.2.lm3 <-
>    lm(DelsPDWOS[,1]~DelsPDWOS[,4]+I(DelsPDWOS[,4]^2)+I(DelsPDWOS[,4]^3))
> 
> 1 and 2 lead to identical but wrong results. 3 is correct. Surprisingly
> (to me) the residuals are the same (correct) in all cases although the
> coefficients of 1 (and 2) are wrong and do not in any way match the
> stated regression polynomial. (summaries below)
> 
> QUESTION:
> Is there a correct way to use poly() in lm() since version 3 becomes quite
> tedious if used more often especially with higher order polynomials?
> 

The coefficients  using 1 and 2 are not incorrect.  
poly() defines orthogonal polynomials, whereas your
DelsPDWOS[,4]+I(DelsPDWOS[,4]^2)+I(DelsPDWOS[,4]^3 
contruct defines an ordinary polynomial. 

You should be able to recover version 3 coefficients from 1 and 2.
See help(poly)

> x <- runif(10)
> x
 [1] 0.1878 0.2415 0.5834 0.6556 0.4112 0.3399 0.8144 0.1134 0.7360
0.0463
> model.matrix(~ poly(x, 2))
   (Intercept) poly(x, 2)1 poly(x, 2)2
1            1    -0.27648     -0.0452
2            1    -0.21052     -0.1899
3            1     0.20937     -0.2708
4            1     0.29799     -0.1021
5            1    -0.00212     -0.4117
6            1    -0.08970     -0.3621
7            1     0.49297      0.4968
8            1    -0.36790      0.2148
9            1     0.39672      0.1620
10           1    -0.45033      0.5082
attr(,"assign")
[1] 0 1 1
> model.matrix(~ x + I(x^2))
   (Intercept)      x  I(x^2)
1            1 0.1878 0.03528
2            1 0.2415 0.05834
3            1 0.5834 0.34040
4            1 0.6556 0.42982
5            1 0.4112 0.16911
6            1 0.3399 0.11554
7            1 0.8144 0.66320
8            1 0.1134 0.01286
9            1 0.7360 0.54169
10           1 0.0463 0.00214
attr(,"assign")
[1] 0 1 2
>


Regards,

-- 
Markus Jantti
Abo Akademi University
markus.jantti at iki.fi
http://www.iki.fi/~mjantti



From sundar.dorai-raj at pdf.com  Wed Jun 29 18:50:19 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 29 Jun 2005 11:50:19 -0500
Subject: [R] moving correlation coef ?
In-Reply-To: <42C2C5F8.8050505@7d4.com>
References: <42C2AEDD.6050207@7d4.com>
	<42C2B0DA.6050804@pdf.com>	<42C2B6E2.1080502@pdf.com>
	<42C2C5F8.8050505@7d4.com>
Message-ID: <42C2D14B.9040301@pdf.com>



vincent wrote:
> Thank you for your answers.
> 
> In fact, i believe my question wasn't precise enough.
> I don't want to have a moving/sliding windows over the data
> to correlate (i am already doing that).
> 
> If I have 2 vectors
> X = (x1, x2, x3, ..., xt)
> Y = (y1, y2, x3, ..., yt)
> I want the most recent elements (t) to have a heavier weight
> in the correlation calculus than the older ones (1).
> 
> I think that one simple way to do that is for example to
> compute cor() over XP, YP where
> XP = (x1, x2, x2, x3, x3, x3, ..., xt, xt, xt)
> YP = (y1, y2, y2, y3, y3, y3, ..., yt, yt, yt)
> ie where each element is repeated several times according
> to its freshness.
> It's quite naive ! so if there is a cleaver idea, many thanks.
> 
> Thanks
> Vincent
> 

Perhaps ?cov.wt will work for you? Your example would be identical to:

set.seed(1)
X <- rnorm(100); Y <- rnorm(100)
# using cov.wt
rho1 <- cov.wt(cbind(X, Y), 1:100, cor = TRUE)$cor[1, 2]
# your weighting scheme
rho2 <- cor(X[rep(1:100, 1:100)], Y[rep(1:100, 1:100)])
all.equal(rho1, rho2)
# [1] TRUE

HTH,

--sundar



From davidr at rhotrading.com  Wed Jun 29 19:56:00 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 29 Jun 2005 12:56:00 -0500
Subject: [R] moving correlation coef ?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FB9DC@rhosvr02.rhotrading.com>


One common weighting scheme is exponentially weighted, i.e., wt =
L^(0:m) ,
where 0 < L <= 1 .

David L. Reiner

p.s.
If your question is coming from a financial application, you might be
interested in the R-sig-finance list, as well as reading the RiskMetrics
(r)
document "Return to RiskMetrics: The Evolution of a Standard", by Jorge
Mina and Jerry Yi Xiao (available at their site, after a free
registration) where exponentially weighted moving statistics are
discussed at length.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of vincent
> Sent: Wednesday, June 29, 2005 11:02 AM
> Cc: r-help at r-project.org
> Subject: Re: [R] moving correlation coef ?
> 
> Thank you for your answers.
> 
> In fact, i believe my question wasn't precise enough.
> I don't want to have a moving/sliding windows over the data
> to correlate (i am already doing that).
> 
> If I have 2 vectors
> X = (x1, x2, x3, ..., xt)
> Y = (y1, y2, x3, ..., yt)
> I want the most recent elements (t) to have a heavier weight
> in the correlation calculus than the older ones (1).
> 
> I think that one simple way to do that is for example to
> compute cor() over XP, YP where
> XP = (x1, x2, x2, x3, x3, x3, ..., xt, xt, xt)
> YP = (y1, y2, y2, y3, y3, y3, ..., yt, yt, yt)
> ie where each element is repeated several times according
> to its freshness.
> It's quite naive ! so if there is a cleaver idea, many thanks.
> 
> Thanks
> Vincent
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From wayoung at softbase.math.uwaterloo.ca  Wed Jun 29 20:51:10 2005
From: wayoung at softbase.math.uwaterloo.ca (Tony Young)
Date: Wed, 29 Jun 2005 14:51:10 -0400
Subject: [R] Help with regression modeling
Message-ID: <000001c57cdb$8410e930$8b4a6181@CSGRADPC13>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050629/0c7e3cb1/attachment.pl

From davidr at rhotrading.com  Wed Jun 29 20:55:10 2005
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Wed, 29 Jun 2005 13:55:10 -0500
Subject: [R] x*x*x*...  vs x^n
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A5FB9DE@rhosvr02.rhotrading.com>

I was surprised to find that I was wrong about powers of complexes:

> seq.pow1 <- function(x,n) {
+   y <- rep(x,n)
+   for(i in 2:n) y[i] <- y[i-1] * x
+   y
+ }
> seq.pow2 <- function(x,n) x^(1:n)
> x <- 1.001 + 1i * 0.999
# several reps of the following
> system.time(ignore <- seq.pow1(x,100000),gcFirst=TRUE)
[1] 0.73 0.00 0.74   NA   NA
# several reps of the following
> system.time(ignore <- seq.pow2(x,100000),gcFirst=TRUE)
[1] 0.35 0.00 0.35   NA   NA

I apologize for using "probably" below when "not" was correct (modulo
grammar).

David L. Reiner
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of David Reiner
> <davidr at rhotrading.com>
> Sent: Wednesday, June 29, 2005 10:24 AM
> To: r-help
> Subject: Re: [R] x*x*x*... vs x^n
> 
> Looking at the code for gsl_pow_int, I see they do use that method.
> 
> David L. Reiner
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of David Reiner
> > <davidr at rhotrading.com>
> > Sent: Wednesday, June 29, 2005 9:50 AM
> > To: r-help
> > Subject: Re: [R] x*x*x*... vs x^n
> >
> > In general, the "Russian peasant algorithm", which requires only
O(log
> > n) multiplications, is very good. Section 4.6.3 of Knuth's The Art
of
> > Computer Programming. Volume 2: Seminumerical Algorithms has an in
> depth
> > discussion.
> > I have had to use this in the past, when computers were slower and
> > compilers were not so clever. It is also better when x is not just a
> > real number, say complex or matrix (as has been mentioned.)
> > In many cases though, one needs many powers sequentially, and then
it
> > may be more efficient to just multiply the previous power by x and
use
> > the power, etc. (unless you have a parallel computer.)
> > So
> >
> > pows <- x^(1:1000)
> > # use pows in calculations
> >
> > could be sped up by employing a faster algorithm, but probably a
loop
> > will be faster:
> >
> > pows <- 1
> > for(i in 1:1000) {
> >   pows <- pows * x
> >   # use this power
> > }
> >
> > David L. Reiner, Ph.D.
> > Rho Trading
> >
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > > bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
> > > Sent: Wednesday, June 29, 2005 9:13 AM
> > > To: Duncan Murdoch
> > > Cc: r-help; Robin Hankin
> > > Subject: Re: [R] x*x*x*... vs x^n
> > >
> > >
> > > On Jun 29, 2005, at 02:47 pm, Duncan Murdoch wrote:
> > >
> > > > On 6/29/2005 9:31 AM, Robin Hankin wrote:
> > > >
> > > >> Hi  Duncan
> > > >>
> > > >>
> > > >> library(gsl)
> > > >> system.time(ignore <- pow_int(a,8))
> > > >> [1] 1.07 1.11 3.08 0.00 0.00
> > > >>
> > > >> <why the slow execution time?>
> > > >>
> > > >
> > > > Shouldn't you ask the gsl maintainer that? :-)
> > > >
> > >
> > > well  I did ask myself, but  in this case the gsl maintainer
> > > told me to ask the gsl co-author, who
> > > is no doubt much better informed in these matters ;-)
> > >
> > > >>
> > > >> (Of course, I'm not suggesting that other programming tasks be
> > > >> suspended!  All I'm pointing
> > > >> out is that there may exist a user to whom fast integer powers
> are
> > > >> very very important)
> > > >>
> > > >
> > > > Then that user should submit the patch, not you.  But whoever
does
> > it
> > > > should include an argument to convince an R core member that the
> > > > change
> > > > is worth looking at, and do it well enough that the patch is
> > accepted.
> > > > Changes to the way R does arithmetic affect everyone, so they
had
> > > > better
> > > > be done right, and checking them takes time.
> > > >
> > >
> > > yes, that's a fair point.
> > > But including a native R command pow.int(), say, wouldn't  affect
> > > anyone, would it?
> > > One could even use the (tested) GSL code, as it is GPL'ed.
> > >
> > > This would just be a new function that users could use at their
> > > discretion, and no
> > > existing code would break.
> > >
> > > I assume that such a function would  not suffer whatever
performance
> > > disadvantage
> > > that the GSL package approach had, so it may well be quite a
> > > significant improvement
> > > over the method used by R_pow_di() in arithmetic.c especially for
> > > large n.
> > >
> > >
> > > best wishes
> > >
> > > rksh
> > >
> > > > Duncan Murdoch
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-
> > > > guide.html
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
http://www.R-project.org/posting-
> > > guide.html
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From Francisco.Redelico at fi.austral.edu.ar  Wed Jun 29 21:05:13 2005
From: Francisco.Redelico at fi.austral.edu.ar (Francisco.Redelico@fi.austral.edu.ar)
Date: Wed, 29 Jun 2005 16:05:13 -0300
Subject: [R] Generalized Linear Mixed Models
Message-ID: <OFF2FAB617.4F6466F8-ON0325702F.00688CFD-0325702F.0068D96C@austral.edu.ar>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050629/181159a0/attachment.pl

From gunter.berton at gene.com  Wed Jun 29 21:28:28 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 29 Jun 2005 12:28:28 -0700
Subject: [R] Generalized Linear Mixed Models
In-Reply-To: <OFF2FAB617.4F6466F8-ON0325702F.00688CFD-0325702F.0068D96C@austral.edu.ar>
Message-ID: <200506291928.j5TJSTZU000787@meitner.gene.com>

submit the following in R:

RSiteSearch('glmm',restr='functions')

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Francisco.Redelico at fi.austral.edu.ar
> Sent: Wednesday, June 29, 2005 12:05 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Generalized Linear Mixed Models
> 
> Hello! 
> 
> I am trying to fit a Generalized Linear Mixed Model, ordinally I use 
> GLIMMIX macros in SAS System, but I would like to fit this 
> kind of models 
> in R. 
> 
> Could  anyone  help me on what package I should use to?
> 
> Thanks in advance
> 
> Francisco
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From RRoa at fisheries.gov.fk  Wed Jun 29 19:38:28 2005
From: RRoa at fisheries.gov.fk (Ruben Roa)
Date: Wed, 29 Jun 2005 15:38:28 -0200
Subject: [R] Generalized Linear Mixed Models
Message-ID: <03DCBBA079F2324786E8715BE538968A068DF2@FIGMAIL-CLUS01.FIG.FK>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> Francisco.Redelico at fi.austral.edu.ar
> Sent: 29 June 2005 18:05
> To: r-help at stat.math.ethz.ch
> Subject: [R] Generalized Linear Mixed Models
> 
> 
> Hello! 
> 
> I am trying to fit a Generalized Linear Mixed Model, ordinally I use 
> GLIMMIX macros in SAS System, but I would like to fit this 
> kind of models in R. 
> 
> Could  anyone  help me on what package I should use to?
> 
> Thanks in advance
> 
> Francisco

Si el modelo es de cualquier clase, revisa el package lme4.
Si el modelo es espacial en las familias Poisson o binomial, revisa package geoRglm.
R.



From dscully at fd9ns01.okladot.state.ok.us  Wed Jun 29 21:50:27 2005
From: dscully at fd9ns01.okladot.state.ok.us (dscully@fd9ns01.okladot.state.ok.us)
Date: Wed, 29 Jun 2005 14:50:27 -0500
Subject: [R] return NA
Message-ID: <OFC5250184.4CEEB9E1-ON8625702F.0068791F-8625702F.006CDF79@fd9ns01.okladot.state.ok.us>





A<-c(1,2,NA,7,5)
B<-c(3,4,1,4,1)
C<-c(6,5,6,NA,9)
D<-c(8,7,4,6,2)
df1<-cbind(A,B,C,D)
for(i in seq(1,ncol(df1)-1, by=2)) {
   ifelse(df1[,i]=="NA",df1[,i+1]=="NA",df1[,] )  }


Tried several variations but none worked.  I wish to find any NA's in
column's 1 or 3 and change the numerical value to the right  of  the "
NA"'s .  In this case I wish to replace the 1 and 6 respectively with "NA".
Any help would be appreciated.



From andy_liaw at merck.com  Wed Jun 29 21:56:23 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 29 Jun 2005 15:56:23 -0400
Subject: [R] return NA
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA46@usctmx1106.Merck.com>

You need to use is.na(df1[,i]) to test for NA.

Andy

> From: dscully at fd9ns01.okladot.state.ok.us
> 
> 
> 
> 
> 
> A<-c(1,2,NA,7,5)
> B<-c(3,4,1,4,1)
> C<-c(6,5,6,NA,9)
> D<-c(8,7,4,6,2)
> df1<-cbind(A,B,C,D)
> for(i in seq(1,ncol(df1)-1, by=2)) {
>    ifelse(df1[,i]=="NA",df1[,i+1]=="NA",df1[,] )  }
> 
> 
> Tried several variations but none worked.  I wish to find any NA's in
> column's 1 or 3 and change the numerical value to the right  of  the "
> NA"'s .  In this case I wish to replace the 1 and 6 
> respectively with "NA".
> Any help would be appreciated.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From sundar.dorai-raj at pdf.com  Wed Jun 29 21:57:13 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 29 Jun 2005 14:57:13 -0500
Subject: [R] return NA
In-Reply-To: <OFC5250184.4CEEB9E1-ON8625702F.0068791F-8625702F.006CDF79@fd9ns01.okladot.state.ok.us>
References: <OFC5250184.4CEEB9E1-ON8625702F.0068791F-8625702F.006CDF79@fd9ns01.okladot.state.ok.us>
Message-ID: <42C2FD19.5050600@pdf.com>



dscully at fd9ns01.okladot.state.ok.us wrote:
> 
> 
> 
> A<-c(1,2,NA,7,5)
> B<-c(3,4,1,4,1)
> C<-c(6,5,6,NA,9)
> D<-c(8,7,4,6,2)
> df1<-cbind(A,B,C,D)
> for(i in seq(1,ncol(df1)-1, by=2)) {
>    ifelse(df1[,i]=="NA",df1[,i+1]=="NA",df1[,] )  }
> 
> 
> Tried several variations but none worked.  I wish to find any NA's in
> column's 1 or 3 and change the numerical value to the right  of  the "
> NA"'s .  In this case I wish to replace the 1 and 6 respectively with "NA".
> Any help would be appreciated.
> 

I think you want ?is.na.

for(i in seq(1, ncol(df1) - 1, by = 2))
   df1[, i + 1] <- ifelse(is.na(df1[, i]), df1[, i], df1[, i + 1])

--sundar



From lisawang at uhnres.utoronto.ca  Wed Jun 29 22:20:26 2005
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Wed, 29 Jun 2005 16:20:26 -0400
Subject: [R] predicted survival curve from a cox model
Message-ID: <42C3028A.2080007@uhnres.utoronto.ca>

Hi there,

I have a predictor varible "class" which is a categorical variable and  
a ' coxph' is used to find the coeffients. How can I plot the predicted 
survival proportion based on this model?

Thanks

Lisa Wang
Princess Margaret Hospital
Toronto
tel 416 946 4501



From jshen6 at gmail.com  Wed Jun 29 22:20:25 2005
From: jshen6 at gmail.com (Jing Shen)
Date: Wed, 29 Jun 2005 15:20:25 -0500
Subject: [R] plot (log scale on y-axis)
Message-ID: <f8b847f0506291320ff15932@mail.gmail.com>

I am planning to plot my data on log scale (y-axis). There is a
parameter in plot function, which is
plot( ..., log="y", ...)
While, the problem is that it is with base of e. Is there a way to let
me change it to 10 instead of e?

Thanks



From jfbrennan at rogers.com  Wed Jun 29 22:53:57 2005
From: jfbrennan at rogers.com (Jim Brennan)
Date: Wed, 29 Jun 2005 16:53:57 -0400
Subject: [R] return NA
In-Reply-To: <OFC5250184.4CEEB9E1-ON8625702F.0068791F-8625702F.006CDF79@fd9ns01.okladot.state.ok.us>
Message-ID: <200506292053.j5TKrvPr022062@hypatia.math.ethz.ch>

Here is a way to do it without a loop that could save some time for a big
dataset.
> df1
      A B  C D
[1,]  1 3  6 8
[2,]  2 4  5 7
[3,] NA 1  6 4
[4,]  7 4 NA 6
[5,]  5 1  9 2

> df2<-cbind(0,ifelse(is.na(df1),NA,0))[,-ncol(df1)-1]
> df2
        A B  C
[1,] 0  0 0  0
[2,] 0  0 0  0
[3,] 0 NA 0  0
[4,] 0  0 0 NA
[5,] 0  0 0  0
> df3<-df1+df2
> df3
      A  B  C  D
[1,]  1  3  6  8
[2,]  2  4  5  7
[3,] NA NA  6  4
[4,]  7  4 NA NA
[5,]  5  1  9  2

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
dscully at fd9ns01.okladot.state.ok.us
Sent: June 29, 2005 3:50 PM
To: r-help at stat.math.ethz.ch
Subject: [R] return NA





A<-c(1,2,NA,7,5)
B<-c(3,4,1,4,1)
C<-c(6,5,6,NA,9)
D<-c(8,7,4,6,2)
df1<-cbind(A,B,C,D)
for(i in seq(1,ncol(df1)-1, by=2)) {
   ifelse(df1[,i]=="NA",df1[,i+1]=="NA",df1[,] )  }


Tried several variations but none worked.  I wish to find any NA's in
column's 1 or 3 and change the numerical value to the right  of  the "
NA"'s .  In this case I wish to replace the 1 and 6 respectively with "NA".
Any help would be appreciated.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From s.henderson at ucl.ac.uk  Wed Jun 29 22:51:11 2005
From: s.henderson at ucl.ac.uk (Stephen Henderson)
Date: Wed, 29 Jun 2005 21:51:11 +0100
Subject: [R] sbrier (Brier score) and coxph
Message-ID: <E7CF6BC2744CBE41AE4E87635C7C893C1C69EA@exc.cruciform.wibr.ucl.ac.uk>

Hello
I've decided to try and distill an earlier rather ill focused question to
try and elicit a response. Any help is greatly appreciated. Why does mod.cox
not work with sbrier whilst mod.km does? Can I make it work?



> data(DLBCL)
> DLBCL.surv<-Surv(DLBCL$time,DLBCL$cens)
> 
> mod.km<-survfit(DLBCL.surv)
> mod.cox<-survfit(coxph(DLBCL.surv~IPI, data=DLBCL))
> 
> 
> sbrier(DLBCL.surv, mod.km)
integrated Brier score 
             0.2076454 
attr(,"time")
[1]   1.3 129.9
> sbrier(DLBCL.surv, mod.cox)
Error in switch(ptype, survfit = { : switch: EXPR must return a length 1
vector

Thanks in advance
Stephen Henderson



**********************************************************************
This email and any files transmitted with it are confidentia...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Jun 29 23:20:10 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Jun 2005 22:20:10 +0100 (BST)
Subject: [R] sbrier (Brier score) and coxph
In-Reply-To: <E7CF6BC2744CBE41AE4E87635C7C893C1C69EA@exc.cruciform.wibr.ucl.ac.uk>
References: <E7CF6BC2744CBE41AE4E87635C7C893C1C69EA@exc.cruciform.wibr.ucl.ac.uk>
Message-ID: <Pine.LNX.4.61.0506292215300.10120@gannet.stats>

Is this sbrier from package ipred?

The short answer is that it contains

     ptype <- class(pred)

and assumes that is of length one.  For a survfit.coxph fit it is of class
c("survfit.cox", "survfit").  I suspect from the help page that this is 
not supported, but you need to contact the authors (as the posting guide 
suggests).

On Wed, 29 Jun 2005, Stephen Henderson wrote:

> Hello
> I've decided to try and distill an earlier rather ill focused question to
> try and elicit a response. Any help is greatly appreciated. Why does mod.cox
> not work with sbrier whilst mod.km does? Can I make it work?
>
>
>
>> data(DLBCL)
>> DLBCL.surv<-Surv(DLBCL$time,DLBCL$cens)
>>
>> mod.km<-survfit(DLBCL.surv)
>> mod.cox<-survfit(coxph(DLBCL.surv~IPI, data=DLBCL))
>>
>>
>> sbrier(DLBCL.surv, mod.km)
> integrated Brier score
>             0.2076454
> attr(,"time")
> [1]   1.3 129.9
>> sbrier(DLBCL.surv, mod.cox)
> Error in switch(ptype, survfit = { : switch: EXPR must return a length 1
> vector
>
> Thanks in advance
> Stephen Henderson
>
>
>
> **********************************************************************
> This email and any files transmitted with it are confidentia...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu Jun 30 00:06:14 2005
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 29 Jun 2005 15:06:14 -0700
Subject: [R] How to convert "c:\a\b" to "c:/a/b"
In-Reply-To: <Pine.LNX.4.61.0506290732540.6049@gannet.stats>
References: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>	<Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>
	<Pine.LNX.4.61.0506290732540.6049@gannet.stats>
Message-ID: <42C31B56.1070808@pdf.com>

Thank You, Prof. Ripley!

	  Both "test1.R" and "test2.R" worked for me just now, as did the 
following minor modification:

(x <- readLines(stdin(), n=1))
D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt

	  Thanks again.

	  spencer graves

Prof Brian Ripley wrote:
> On Wed, 29 Jun 2005, David Duffy wrote:
> 
> 
>>I couldn't resist adding a more literal answer
> 
> 
> This can only work for escapes which are preserved.  The parser maps
> \n to a character (LF) and the deparser maps it back to \n.
> This happens to be true of \a \b \f \n \r \t \v \\ but no others.
> 
> For example, \s is mapped to s, and there is no difference between \s and
> s in the parsed input.
> 
> 
>>unback <- function(x) {
>> chars <- unlist(strsplit(deparse(x),""))
>> chars <- chars[-c(1,length(chars))]
>> paste(gsub("\\\\","/",chars),collapse="")
>>}
>>
>>unback("\n")
> 
> 
>>unback("\s")
> 
> [1] "s"
> 
> Spencer Graves keeps on insisting there is a better way, but all the
> solutions are to avoid sending the string to the parser, and hence
> avoiding having the string directly in an R script.  This is common in 
> shell scripts, which use 'here' documents to avoid 'quoting hell'.
> 
> We can do that in R too. Here are two variants I have not seen in the 
> thread
> 
> test1.R:
> scan("", "", allowEscapes=FALSE, n=1, quiet=TRUE)
> D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> catIx, "\n", sep="")
> 
> R --slave --vanilla < test1.R
> D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> 
> (This one does not allow quoted strings.)
> 
> test2.R:
> x <- readLines(stdin(), n=1)
> "D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt"
> x <- gsub('^"(.*)"$', "\\1", x)
> cat(x, "\n")
> 
> R --slave --vanilla < test2.R
> D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> 
> (This one allows surrounding quotes or not.)
> 

-- 
Spencer Graves, PhD
Senior Development Engineer
PDF Solutions, Inc.
333 West San Carlos Street Suite 700
San Jose, CA 95110, USA

spencer.graves at pdf.com
www.pdf.com <http://www.pdf.com>
Tel:  408-938-4420
Fax: 408-280-7915



From ggrothendieck at gmail.com  Thu Jun 30 00:16:23 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 29 Jun 2005 18:16:23 -0400
Subject: [R] How to convert "c:\a\b" to "c:/a/b"?
In-Reply-To: <42C0C647.7070201@pdf.com>
References: <42BF72EC.4040204@pdf.com>
	<17087.31709.399479.651692@basebud.nulle.part>
	<971536df050626222318376e24@mail.gmail.com> <42C02C2F.8070907@pdf.com>
	<971536df05062719251e97b319@mail.gmail.com> <42C0C647.7070201@pdf.com>
Message-ID: <971536df05062915166fc54d60@mail.gmail.com>

One other comment.  Ninotech Path Copy, which can be found at:

  http://home.worldonline.dk/ninotech/

is a free Windows utility that appears in the Windows Explorer context
menu (i.e. it appears as the Copy Path menu entry when you right click 
any file in Windows Explorer).  I had forgotten about it since even though 
I had installed it a long time ago I hardly ever use it.  When one right clicks
and chooses Copy Path one is given a number of different ways in which
to copy the path.  Using the Setup submenu (i.e. right click any file and
choose Setup) one can define additional ways and its quite easy to define 
a new method which replaces backslashes with forward slashes as it 
copies -- in fact, there is a check box specifically for this.  

On 6/27/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> Hi, Gabor, James, et al.:
> 
>          Thanks for the help.  I'm unable to get "scan('clipboard', what='',
> allowEscapes=FALSE))" to work reliably on my system using Rgui under
> Windows XP, R 2.1.1 patched.  However, "file.choose()" works like a charm.
> 
>          Thanks again, everyone.
>          spencer graves
> 
> Gabor Grothendieck wrote:
> > Try using file.choose() to locate the file instead of Windows Explorer.  That
> > will return the name in a form useable within R.
> >
> > On 6/27/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> >
> >>         Thanks, Dirk, Gabor, Eric:
> >>
> >>         You all provided appropriate solutions for the stated problem.
> >>Sadly, I oversimplified the problem I was trying to solve:  I copy a
> >>character string giving a DOS path from MS Windows Explorer into an R
> >>script file, and I get something like the following:
> >>
> >>         D:\spencerg\statmtds\R\Rnews
> >>
> >>         I want to be able to use this in R with its non-R meaning, e.g., in
> >>readLine, count.fields, read.table, etc., after appending a file name.
> >>Your three solutions all work for my oversimplified toy example but are
> >>inadequate for the problem I really want to solve.
> >>
> >>         Thanks,
> >>         spencer graves
> >>
> >>Gabor Grothendieck wrote:
> >>
> >>
> >>>On 6/27/05, Dirk Eddelbuettel <edd at debian.org> wrote:
> >>>
> >>>
> >>>>On 26 June 2005 at 20:30, Spencer Graves wrote:
> >>>>|         How can one convert back slashes to forward slashes, e.g, changing
> >>>>| "c:\a\b" to "c:/a/b"?  I tried the following:
> >>>>|
> >>>>|  > gsub("\\\\", "/", "c:\a\b")
> >>>>| [1] "c:\a\b"
> >>>>
> >>>>This does work, provided you remember that single backslashed "don't exist"
> >>>>as e.g. \a is a character in itself. So use doubles are you should be fine:
> >>>>
> >>>>
> >>>>
> >>>>>gsub("\\\\", "/", "c:\\a\\b")
> >>>>
> >>>>[1] "c:/a/b"
> >>>>
> >>>
> >>>
> >>>Also, if one finds four backslashes confusing one can avoid the use
> >>>of four via any of these:
> >>>
> >>>gsub("[\\]", "/", "c:\\a\\b")
> >>>gsub("\\", "/", "c:\\a\\b", fixed = TRUE)
> >>>chartr("\\", "/", "c:\\a\\b")
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>--
> >>Spencer Graves, PhD
> >>Senior Development Engineer
> >>PDF Solutions, Inc.
> >>333 West San Carlos Street Suite 700
> >>San Jose, CA 95110, USA
> >>
> >>spencer.graves at pdf.com
> >>www.pdf.com <http://www.pdf.com>
> >>Tel:  408-938-4420
> >>Fax: 408-280-7915
> >>
> >
> >
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
>



From ggrothendieck at gmail.com  Thu Jun 30 00:33:47 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 29 Jun 2005 18:33:47 -0400
Subject: [R] How to convert "c:\a\b" to "c:/a/b"
In-Reply-To: <42C31B56.1070808@pdf.com>
References: <mailman.13.1119952801.1160.r-help@stat.math.ethz.ch>
	<Pine.LNX.4.58.0506291257160.7756@orpheus.qimr.edu.au>
	<Pine.LNX.4.61.0506290732540.6049@gannet.stats>
	<42C31B56.1070808@pdf.com>
Message-ID: <971536df05062915332e50f51f@mail.gmail.com>

Note that if you want to source it rather than than run it as a batch
job from the command line you will something like this.   The way
it works is that one puts the file name into comments that are
marked with tags and the script rereads itself as data picking out
the tagged lines and removing the tags from those lines.
script.name() returns the name of the script its running in and
my.stdin sets up a connection to the script as data after stripping out
all lines that do not match the tag and removing the tag from
those lines that do.  (This is a variation of some code that I previously
posted.)

# source the following from R

script.name <- function() 
 showConnections()[as.character(eval.parent(quote(file), n=3)), "description"]
my.stdin <- function( tag = "^#>", script.name. = script.name())
 textConnection( sub(tag, "", grep(tag,readLines(script.name.),value=TRUE)) )

x <- readLines( my.stdin() )
#>c:\a\b
cat(x, "\n")



On 6/29/05, Spencer Graves <spencer.graves at pdf.com> wrote:
> Thank You, Prof. Ripley!
> 
>          Both "test1.R" and "test2.R" worked for me just now, as did the
> following minor modification:
> 
> (x <- readLines(stdin(), n=1))
> D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> 
>          Thanks again.
> 
>          spencer graves
> 
> Prof Brian Ripley wrote:
> > On Wed, 29 Jun 2005, David Duffy wrote:
> >
> >
> >>I couldn't resist adding a more literal answer
> >
> >
> > This can only work for escapes which are preserved.  The parser maps
> > \n to a character (LF) and the deparser maps it back to \n.
> > This happens to be true of \a \b \f \n \r \t \v \\ but no others.
> >
> > For example, \s is mapped to s, and there is no difference between \s and
> > s in the parsed input.
> >
> >
> >>unback <- function(x) {
> >> chars <- unlist(strsplit(deparse(x),""))
> >> chars <- chars[-c(1,length(chars))]
> >> paste(gsub("\\\\","/",chars),collapse="")
> >>}
> >>
> >>unback("\n")
> >
> >
> >>unback("\s")
> >
> > [1] "s"
> >
> > Spencer Graves keeps on insisting there is a better way, but all the
> > solutions are to avoid sending the string to the parser, and hence
> > avoiding having the string directly in an R script.  This is common in
> > shell scripts, which use 'here' documents to avoid 'quoting hell'.
> >
> > We can do that in R too. Here are two variants I have not seen in the
> > thread
> >
> > test1.R:
> > scan("", "", allowEscapes=FALSE, n=1, quiet=TRUE)
> > D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> > catIx, "\n", sep="")
> >
> > R --slave --vanilla < test1.R
> > D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> >
> > (This one does not allow quoted strings.)
> >
> > test2.R:
> > x <- readLines(stdin(), n=1)
> > "D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt"
> > x <- gsub('^"(.*)"$', "\\1", x)
> > cat(x, "\n")
> >
> > R --slave --vanilla < test2.R
> > D:\spencerg\dataPOWER\stats\Tukey\Boxplot_missing_Tukey2.txt
> >
> > (This one allows surrounding quotes or not.)
> >
> 
> --
> Spencer Graves, PhD
> Senior Development Engineer
> PDF Solutions, Inc.
> 333 West San Carlos Street Suite 700
> San Jose, CA 95110, USA
> 
> spencer.graves at pdf.com
> www.pdf.com <http://www.pdf.com>
> Tel:  408-938-4420
> Fax: 408-280-7915
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lasortef at missouri.edu  Thu Jun 30 01:11:20 2005
From: lasortef at missouri.edu (La Sorte, Frank A.)
Date: Wed, 29 Jun 2005 18:11:20 -0500
Subject: [R] Extract fixed effects SE from lmer
Message-ID: <7E9755805195484DA088CB948157635901C12008@UM-EMAIL05.um.umsystem.edu>

Hi,
 
Does anyone know how to extract fixed effects SE values from generalized linear mixed models estimated using the lmer function in the lme4 library?  I searched attributes and structure with no luck.
 
Thanks
 
Frank A. La Sorte, Ph.D.
Department of Fisheries and Wildlife Sciences
University of Missouri
Columbia, MO 65211 USA



From dmbates at gmail.com  Thu Jun 30 01:29:37 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Wed, 29 Jun 2005 18:29:37 -0500
Subject: [R] Fwd:  Extract fixed effects SE from lmer
In-Reply-To: <40e66e0b0506291628439bf1d9@mail.gmail.com>
References: <7E9755805195484DA088CB948157635901C12008@UM-EMAIL05.um.umsystem.edu>
	<40e66e0b0506291628439bf1d9@mail.gmail.com>
Message-ID: <40e66e0b05062916292c61df56@mail.gmail.com>

I forgot to cc: the list on this reply.

---------- Forwarded message ----------
From: Douglas Bates <dmbates at gmail.com>
Date: Jun 29, 2005 6:28 PM
Subject: Re: [R] Extract fixed effects SE from lmer
To: "La Sorte, Frank A." <lasortef at missouri.edu>


On 6/29/05, La Sorte, Frank A. <lasortef at missouri.edu> wrote:
> Hi,
>
> Does anyone know how to extract fixed effects SE values from generalized linear mixed models estimated using the lmer function in the lme4 library?  I searched attributes and structure with no luck.
>

Try vcov(myLmerModel).  That returns a variance-covariance matrix for
the fixed effects.  The usual manipulations are used to get the
standard errors.  For the binomial and Poisson families you should use
the optional argument useScale=FALSE as these families do not have a
separately estimated scale parameter in the variance function.

In the "show" method for the summary.lmer class the relevant piece of code is

              corF <- as(as(vcov(object, useScale = useScale), "pdmatrix"),
                         "corrmatrix")



From helprhelp at gmail.com  Thu Jun 30 01:30:42 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Wed, 29 Jun 2005 18:30:42 -0500
Subject: [R] deal package
Message-ID: <cdf817830506291630186d1342@mail.gmail.com>

Hi,
I am wondering if anyone here used deal package in R to do the
bayesian network. I am curious about its scalability: how many
variables and how many observations can it handle in a reasonable
time. If you have some good experience, please share your data
configurations.


thanks,


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From iidn01 at yahoo.com  Thu Jun 30 05:45:48 2005
From: iidn01 at yahoo.com (Young Cho)
Date: Wed, 29 Jun 2005 20:45:48 -0700 (PDT)
Subject: [R] Finding out collinearity in regression
Message-ID: <20050630034548.79898.qmail@web31113.mail.mud.yahoo.com>

Hi, I am trying to find out a collinearity in
explanatory variables with alias(). 

I creat a dataframe:

dat <- ds[,sapply(ds,nlevels)>=2]
dat$Y <- Response

Explanatory variables are factor and response is
continuous random variable. When I run a regression, I
have the following error:

fit <- aov( Y ~ . , data = dat)
Error in "contrasts<-"(`*tmp*`, value =
"contr.treatment") :
        contrasts can be applied only to factors with
2 or more levels

I think there is a dependency in explanatory
variables. So, I wanted to use alias to find out a
dependency in design matrix but I can't because I
cannot create "fit" in the first place.

One of examples I found is:

carprice1.lm <- lm(gpm100 ~
Type+Min.Price+Price+Max.Price+Range.Price,data=carprice)
alias(carprice1.lm)

But, what if I can create lm object ? Then is there a
way to find out a dependency in design matrix? Thanks
a lot for help in advance!

-Young.



From Kevin.Wang at maths.anu.edu.au  Thu Jun 30 05:54:00 2005
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 30 Jun 2005 13:54:00 +1000
Subject: [R] Finding out collinearity in regression
In-Reply-To: <20050630034548.79898.qmail@web31113.mail.mud.yahoo.com>
References: <20050630034548.79898.qmail@web31113.mail.mud.yahoo.com>
Message-ID: <42C36CD8.1060108@maths.anu.edu.au>

Hi,

Young Cho wrote:

> fit <- aov( Y ~ . , data = dat)
> Error in "contrasts<-"(`*tmp*`, value =
> "contr.treatment") :
>         contrasts can be applied only to factors with
> 2 or more levels
> 
> I think there is a dependency in explanatory
> variables. So, I wanted to use alias to find out a
> dependency in design matrix but I can't because I
> cannot create "fit" in the first place.

The error message actually looks like you have got (at least) a variable 
that only has 1 level, e.g. a factor with only one level.

Cheers,

Kev

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Bioinformation Science
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 2601
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From simonb at cres10.anu.edu.au  Thu Jun 30 06:10:30 2005
From: simonb at cres10.anu.edu.au (Simon Blomberg)
Date: Thu, 30 Jun 2005 14:10:30 +1000
Subject: [R] Finding out collinearity in regression
In-Reply-To: <20050630034548.79898.qmail@web31113.mail.mud.yahoo.com>
References: <20050630034548.79898.qmail@web31113.mail.mud.yahoo.com>
Message-ID: <6.2.1.2.0.20050630140733.01c383e0@mail.ozemail.com.au>

At 01:45 PM 30/06/2005, Young Cho wrote:
>Hi, I am trying to find out a collinearity in
>explanatory variables with alias().
>
>I creat a dataframe:
>
>dat <- ds[,sapply(ds,nlevels)>=2]
>dat$Y <- Response
>
>Explanatory variables are factor and response is
>continuous random variable. When I run a regression, I
>have the following error:
>
>fit <- aov( Y ~ . , data = dat)
>Error in "contrasts<-"(`*tmp*`, value =
>"contr.treatment") :
>         contrasts can be applied only to factors with
>2 or more levels

1. Sounds like at least one of your factors has only one level. This should 
be easy to spot.

2. Have you considered package perturb?

HTH,

Simon.



>I think there is a dependency in explanatory
>variables. So, I wanted to use alias to find out a
>dependency in design matrix but I can't because I
>cannot create "fit" in the first place.
>
>One of examples I found is:
>
>carprice1.lm <- lm(gpm100 ~
>Type+Min.Price+Price+Max.Price+Range.Price,data=carprice)
>alias(carprice1.lm)
>
>But, what if I can create lm object ? Then is there a
>way to find out a dependency in design matrix? Thanks
>a lot for help in advance!
>
>-Young.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Simon Blomberg, B.Sc.(Hons.), Ph.D, M.App.Stat.
Centre for Resource and Environmental Studies
The Australian National University
Canberra ACT 0200
Australia
T: +61 2 6125 7800 email: Simon.Blomberg_at_anu.edu.au
F: +61 2 6125 0757
CRICOS Provider # 00120C



From Nongluck.Klibbua at newcastle.edu.au  Thu Jun 30 06:36:45 2005
From: Nongluck.Klibbua at newcastle.edu.au (Nongluck Klibbua)
Date: Thu, 30 Jun 2005 14:36:45 +1000
Subject: [R] how to call egarch of sas in R
Message-ID: <s2c4038f.078@MC-GWDOM2.newcastle.edu.au>

I use R to generate data and I need to estimate the data by egarch (that
doesn't have in R). So how I can call egarch from SAS in R. 
Regards,
luck



From vincent at 7d4.com  Thu Jun 30 07:15:55 2005
From: vincent at 7d4.com (vincent)
Date: Thu, 30 Jun 2005 07:15:55 +0200
Subject: [R] moving correlation coef ?
In-Reply-To: <42C2D14B.9040301@pdf.com>
References: <42C2AEDD.6050207@7d4.com>
	<42C2B0DA.6050804@pdf.com>	<42C2B6E2.1080502@pdf.com>
	<42C2C5F8.8050505@7d4.com> <42C2D14B.9040301@pdf.com>
Message-ID: <42C3800B.1030304@7d4.com>

Sundar Dorai-Raj a ??crit :

> Perhaps ?cov.wt will work for you? Your example would be identical to:
> 
> set.seed(1)
> X <- rnorm(100); Y <- rnorm(100)
> # using cov.wt
> rho1 <- cov.wt(cbind(X, Y), 1:100, cor = TRUE)$cor[1, 2]
> # your weighting scheme
> rho2 <- cor(X[rep(1:100, 1:100)], Y[rep(1:100, 1:100)])
> all.equal(rho1, rho2)
> # [1] TRUE
> 
> HTH,
> 
> --sundar

Thank you very much Sundar for your advices.
I will try this way.
Have a good day.
Vincent



From edward.m at psu.ac.th  Thu Jun 30 07:50:37 2005
From: edward.m at psu.ac.th (Edward McNeil)
Date: Thu, 30 Jun 2005 12:50:37 +0700
Subject: [R] Dispersion parameter in Neg Bin GLM
Message-ID: <001101c57d37$a41a7dc0$3e021dac@psu.ac.th>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/fe12905a/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jun 30 08:27:05 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Jun 2005 07:27:05 +0100 (BST)
Subject: [R] Dispersion parameter in Neg Bin GLM
In-Reply-To: <001101c57d37$a41a7dc0$3e021dac@psu.ac.th>
References: <001101c57d37$a41a7dc0$3e021dac@psu.ac.th>
Message-ID: <Pine.LNX.4.61.0506300704190.20105@gannet.stats>


On Thu, 30 Jun 2005, Edward McNeil wrote:

> Hi, Can someone tell me if it is possible to set the dispersion 
> parameter constant when fitting a negative binomial glm in R? I've 
> looked at the documentation and can't find the appropriate argument to 
> pass. In STATA I can type: nbreg depvar [indepvar...], offset(offset) 
> dispersion(constant). Thank you

I had to read the STATA documentation.  nbreg does not claim to fit a `Neg 
Bin GLM' and with option dispersion(constant) it is not fitting a GLM at 
all.  See http://www.stata.com/help.cgi?nbreg and
http://www.stata.com/support/faqs/stat/nbreg1.html

So I think you need to think carefully about what you actually want, 
as STATA is not doing what you asked for.

If I had to fit the STATA model in R I would directly maximize the 
likelihood, modifying the example code in MASS4 chapter 16.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vincent at 7d4.com  Thu Jun 30 08:33:24 2005
From: vincent at 7d4.com (vincent)
Date: Thu, 30 Jun 2005 08:33:24 +0200
Subject: [R] moving correlation coef ?
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A5FB9DC@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A5FB9DC@rhosvr02.rhotrading.com>
Message-ID: <42C39234.1070800@7d4.com>

davidr at rhotrading.com a ??crit :

> One common weighting scheme is exponentially weighted, i.e., wt =
> L^(0:m) ,
> where 0 < L <= 1 .
> 
> David L. Reiner
> 
> p.s.
> If your question is coming from a financial application, you might be
> interested in the R-sig-finance list, as well as reading the RiskMetrics
> (r)
> document "Return to RiskMetrics: The Evolution of a Standard", by Jorge
> Mina and Jerry Yi Xiao (available at their site, after a free
> registration) where exponentially weighted moving statistics are
> discussed at length.

Thank you for the advice and for the link.
Vincent



From Ivy_Li at smics.com  Thu Jun 30 08:44:11 2005
From: Ivy_Li at smics.com (Ivy_Li)
Date: Thu, 30 Jun 2005 14:44:11 +0800
Subject: =?utf-8?Q?=E7=AD=94=E5=A4=8D=3A_=5BR=5D_fail_in_adding_library_i?=
	=?utf-8?Q?n_new_version=2E?=
Message-ID: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>

Dear Gabor,
	Thank your for helping me so much!
	I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\ 
1. unpack tools.zip into c:\cygwin
2. install Active perl in c:\Perl
3. install the mingw32 in c:\mingwin
4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?) 

5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))

6. I opened the DOS environment. Into the "D:\>"  Type the following code:
cd \Program Files\R\rw2010
But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"

I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?

I still need your and others help. Thank you very much!



-----ÅÂéüÅÂÅßãÅÈÇÅÆÅ‰ÅªÅ∂-----
ÅÂèëÅ‰ÅªÅ∂Å‰Å∫Å∫: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
ÅÂèëÅÈÄÅÅÊóÅ∂ÅÈóÅ¥: 2005ÅÂÅπÅ¥6ÅÊúà6ÅÊóÅ• 10:21
ÅÊîÅ∂Å‰ÅªÅ∂Å‰Å∫Å∫: Ivy_Li
ÅÊäÑÅÈÄÅ: r-help at stat.math.ethz.ch
Å‰Å∏ÅªÅÈÅ¢ò: Re: [R] fail in adding library in new version.


On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Hello everybody,
>        Could I consult you a question?
>        I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.

Getting the latest version of R is strongly recommended.  The suggestions
below all assume the latest version and may or may not work if you do
not upgrade.

> *       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> *       Download the tools.zip
> *       Unpack tools.zip into c:\cygwin
> *       Install Active Perl in c:\Perl
> *       Install the mingw32 port of gcc in c:\mingwin
> *       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin

You may need to put these at the beginning of the path rather than the end.
Also just as a check enter 
     path
at the console to make sure that you have them.  You will likely
have to start a new console session and possibly even reboot.

Also you need the Microsoft Help Compiler, hhc.  Suggest
you reread the material on which tools you need.

> *       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?

In MyRPackages you would have a folder called example, in your case,
that contains the package.  Within folder example, you would have the
DESCRIPTION file, the R folder, etc.

> *       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?

You don't have to run R first.  You do need to make sure that R.exe can
be found on your path or else use the absolute path name in referring to R.
For example, if your path does not include R you could do something like this:

cd \Program Files\R\rw2010
bin\R cmd install /MyRPackages/example

Be sure to use forward slashes where shown above and backslashes
where shown.

>        So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> 

Try all these suggestions including upgrading R and if that does not work
try posting screen dumps of the actual errors you are getting.



From m.lesnoff at cgiar.org  Thu Jun 30 08:51:55 2005
From: m.lesnoff at cgiar.org (Lesnoff, Matthieu (ILRI))
Date: Thu, 30 Jun 2005 09:51:55 +0300
Subject: =?iso-8859-1?Q?RE=A0=3A_=5BR=5D_Dispersion_parameter_in_Neg_Bin_GLM?=
Message-ID: <6C9B7DF9F7933144A6346A85FD5A6966D1D994@ilriethx.ILRI.CGIARAD.ORG>

Edward, you also can use the package aod on CRAN, see the help page of the function negbin.
 
Best
 
Matthieu
 
An example:
 
> library(aod)
> data(dja)
> negbin(y ~ group + offset(log(trisk)), ~group, dja, fixpar = list(4, 0))

Negative-binomial model
-----------------------
negbin(formula = y ~ group + offset(log(trisk)), random = ~group, 
    data = dja, fixpar = list(4, 0))
Convergence was obtained after 113 iterations.
Fixed-effect coefficients:
            Estimate Std. Error z value Pr(> |z|)
(Intercept)  -0.5526     0.2277 -2.4267    0.0152
groupTREAT   -1.0205     0.2598 -3.9287    < 1e-4
Overdispersion coefficients:
              Estimate Std. Error z value Pr(> z)
phi.groupCTRL   0.8287      0.412  2.0117  0.0221
Overdispersion coefficients set to fixed values:
               Value
phi.groupTREAT     0
Log-likelihood = -121.149; nbpar = 3; df.residual = 72; Deviance = 111.826; AIC = 248.297

 
 
 

________________________________

De: r-help-bounces at stat.math.ethz.ch de la part de Edward McNeil
Date: jeu. 6/30/2005 8:50
??: r-help at r-project.org
Objet : [R] Dispersion parameter in Neg Bin GLM



Hi,
Can someone tell me if it is possible to set the dispersion parameter constant when fitting a negative binomial glm in R? I've looked at the documentation and can't find the appropriate argument to pass.
In STATA I can type: nbreg depvar [indepvar...], offset(offset) dispersion(constant).
Thank you

        [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Dubravko.Dolic at komdat.com  Thu Jun 30 09:15:16 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Thu, 30 Jun 2005 09:15:16 +0200
Subject: [R] Memory Management under Linux: Problems to allocate large
	amounts of data
Message-ID: <52D1AC81378E9342947189B04176014728F77E@agentsmith.komdat.intern>

Dear Prof. Ripley.

Thank You for Your quick answer. Your right by assuming that we run R on a 32bit System. My technician tried to install R on a emulated 64bit Opteron machine which led into some trouble. Maybe because the Opteron includes a 32bit Processor which emulates 64bit (AMD64 x86_64). As You seem to have good experience with running R on a 64bit OS I feel encouraged to have another try for this.



-----Urspr??ngliche Nachricht-----
Von: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Gesendet: Mittwoch, 29. Juni 2005 15:18
An: Dubravko Dolic
Cc: r-help at stat.math.ethz.ch
Betreff: Re: [R] Memory Management under Linux: Problems to allocate large amounts of data

Let's assume this is a 32-bit Xeon and a 32-bit OS (there are 
64-bit-capable Xeons).  Then a user process like R gets a 4GB address 
space, 1GB of which is reserved for the kernel.  So R has a 3GB address 
space, and it is trying to allocate a 2GB contigous chunk.  Because of 
memory fragmentation that is quite unlikely to succeed.

We run 64-bit OSes on all our machines with 2GB or more RAM, for this 
reason.

On Wed, 29 Jun 2005, Dubravko Dolic wrote:

> Dear Group
>
> I'm still trying to bring many data into R (see older postings). After 
> solving some troubles with the database I do most of the work in MySQL. 
> But still I could be nice to work on some data using R. Therefore I can 
> use a dedicated Server with Gentoo Linux as OS hosting only R. This 
> Server is a nice machine with two CPU and 4GB RAM which should do the 
> job:
>
> Dual Intel XEON 3.06 GHz
> 4 x 1 GB RAM PC2100 CL2
> HP Proliant DL380-G3
>
> I read the R-Online help on memory issues and the article on garbage 
> collection from the R-News 01-2001 (Luke Tierney). Also the FAQ and some 
> newsgroup postings were very helpful on understanding memory issues 
> using R.
>
> Now I try to read data from a database. The data I wanted to read 
> consists of 158902553 rows and one field (column) and is of type 
> bigint(20) in the database. I received the message that R could not 
> allocate the 2048000 Kb (almost 2GB) sized vector. As I have 4BG of RAM 
> I could not imagine why this happened. In my understanding R under Linux 
> (32bit) should be able to use the full RAM. As there is not much space 
> used by OS and R as such ("free" shows the use of app. 670 MB after 
> dbSendQuery and fetch) there are 3GB to be occupied by R. Is that 
> correct?

Not really.  The R executable code and the Ncells are already in the 
address space, and this is a virtual memory OS, so the amount of RAM is 
not relevant (it would still be a 3GB limit with 12GB of RAM).

> After that I started R by setting n/vsize explicitly
>
> R --min-vsize=10M --max-vsize=3G --min-nsize=500k --max-nsize=100M
>
>> mem.limits()
>    nsize     vsize
> 104857600        NA
>
> and received the same message.
>
>
> A garbage collection delivered the following information:
>
>> gc()
>         used (Mb) gc trigger   (Mb) limit (Mb)  max used   (Mb)
> Ncells 217234  5.9     500000   13.4       2800    500000   13.4
> Vcells  87472  0.7  157650064 1202.8       3072 196695437 1500.7
>
>
> Now I'm at a loss. Maybe anyone could give me a hint where I should read 
> further or which Information can take me any further

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 30 09:43:32 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Jun 2005 08:43:32 +0100 (BST)
Subject: [R] Memory Management under Linux: Problems to allocate large
 amounts of data
In-Reply-To: <52D1AC81378E9342947189B04176014728F77E@agentsmith.komdat.intern>
References: <52D1AC81378E9342947189B04176014728F77E@agentsmith.komdat.intern>
Message-ID: <Pine.LNX.4.61.0506300834290.21263@gannet.stats>

On Thu, 30 Jun 2005, Dubravko Dolic wrote:

> Dear Prof. Ripley.
>
> Thank You for Your quick answer. Your right by assuming that we run R on 
> a 32bit System. My technician tried to install R on a emulated 64bit 
> Opteron machine which led into some trouble. Maybe because the Opteron 
> includes a 32bit Processor which emulates 64bit (AMD64 x86_64). As You 
> seem to have good experience with running R on a 64bit OS I feel 
> encouraged to have another try for this.

It should work out of the box on an Opteron Linux systen: it does for 
example on FC3 and SuSE 9.x.  Some earlier Linux distros for x86_64 are
not fully 64-bit, but we ran R on FC2 (although some packages could not be 
installed).

Trying to build a 32-bit version of R on FC3 does not work for me: the 
wrong libgcc_s is found.  (One might want a 32-bit version for speed on 
small tasks.)

> -----Urspr?ngliche Nachricht-----
> Von: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Gesendet: Mittwoch, 29. Juni 2005 15:18
> An: Dubravko Dolic
> Cc: r-help at stat.math.ethz.ch
> Betreff: Re: [R] Memory Management under Linux: Problems to allocate large amounts of data
>
> Let's assume this is a 32-bit Xeon and a 32-bit OS (there are
> 64-bit-capable Xeons).  Then a user process like R gets a 4GB address
> space, 1GB of which is reserved for the kernel.  So R has a 3GB address
> space, and it is trying to allocate a 2GB contigous chunk.  Because of
> memory fragmentation that is quite unlikely to succeed.
>
> We run 64-bit OSes on all our machines with 2GB or more RAM, for this
> reason.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From christian.ritter at shell.com  Thu Jun 30 10:16:59 2005
From: christian.ritter at shell.com (Ritter, Christian C GSMCIL-GSTMS/2)
Date: Thu, 30 Jun 2005 10:16:59 +0200
Subject: [R] request/suggestion: modified names
Message-ID: <156CDC8CCFD1894295D2907F16337A48B2AF06@bru-s-006.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/882d5297/attachment.pl

From temiz at deprem.gov.tr  Thu Jun 30 10:21:59 2005
From: temiz at deprem.gov.tr (orkun)
Date: Thu, 30 Jun 2005 11:21:59 +0300
Subject: [R] installation error
Message-ID: <42C3ABA7.3020600@deprem.gov.tr>

hello

when I follow this procedure:
~~~~~~~~~~
rS <- "http://godel.cs.bilgi.edu.tr/mirror/cran/src/contrib/"
install.packages(c("Matrix"), repos=rS, dependencies=TRUE)
~~~~~~~~~~

installation gives this error:
~~~~~~~~~~~~~
make: *** [Matrix.so] Error 1
ERROR: compilation failed for package 'Matrix'
** Removing '/usr/local/lib/R/site-library/Matrix'
** Restoring previous '/usr/local/lib/R/site-library/Matrix'

The downloaded packages are in
        /tmp/RtmpPx5hkE/downloaded_packages
Warning message:
installation of package 'Matrix' had non-zero exit status in: 
install.packages(c("Matrix"), repos = rS, dependencies = TRUE)
~~~~~~~~~~
I am using debian and  had installed R in debian way.

what might possible solutions be ?

regards

Ahmet Temiz



______________________________________
XamimeLT - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr
______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From ripley at stats.ox.ac.uk  Thu Jun 30 10:41:06 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Jun 2005 09:41:06 +0100 (BST)
Subject: =?iso-8859-1?Q?RE=A0=3A_=5BR=5D_Dispersion_parameter_in_Neg_Bin_GLM?=
In-Reply-To: <6C9B7DF9F7933144A6346A85FD5A6966D1D994@ilriethx.ILRI.CGIARAD.ORG>
References: <6C9B7DF9F7933144A6346A85FD5A6966D1D994@ilriethx.ILRI.CGIARAD.ORG>
Message-ID: <Pine.LNX.4.61.0506300923230.21727@gannet.stats>

This does not appear to be the same model as STATA.

Your model (as documented) has dispersion (var/mean) of 1 + phi*mu, so you 
need to constrain phi = a/mu to get a constant dispersion.  The output 
shown is not at all consistent with that.

On Thu, 30 Jun 2005, Lesnoff, Matthieu (ILRI) wrote:

> Edward, you also can use the package aod on CRAN, see the help page of 
> the function negbin.
>
> Best
>
> Matthieu
>
> An example:
>
>> library(aod)
>> data(dja)
>> negbin(y ~ group + offset(log(trisk)), ~group, dja, fixpar = list(4, 0))
>
> Negative-binomial model
> -----------------------
> negbin(formula = y ~ group + offset(log(trisk)), random = ~group,
>    data = dja, fixpar = list(4, 0))
> Convergence was obtained after 113 iterations.
> Fixed-effect coefficients:
>            Estimate Std. Error z value Pr(> |z|)
> (Intercept)  -0.5526     0.2277 -2.4267    0.0152
> groupTREAT   -1.0205     0.2598 -3.9287    < 1e-4
> Overdispersion coefficients:
>              Estimate Std. Error z value Pr(> z)
> phi.groupCTRL   0.8287      0.412  2.0117  0.0221
> Overdispersion coefficients set to fixed values:
>               Value
> phi.groupTREAT     0
> Log-likelihood = -121.149; nbpar = 3; df.residual = 72; Deviance = 111.826; AIC = 248.297

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Jun 30 11:39:34 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 30 Jun 2005 11:39:34 +0200
Subject: =?UTF-8?B?562U5aSNOiBbUl0gZmFpbCBpbiBhZGRpbmcgbGlicmFyeSBpbiA=?=
	=?UTF-8?B?bmV3IHZlcnNpb24u?=
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>
Message-ID: <42C3BDD6.1050609@statistik.uni-dortmund.de>

Ivy_Li wrote:

> Dear Gabor, Thank your for helping me so much! I have loaded R the
> newest version 2.1.1. Then I setup it in the path of D:\program
> files\R\ 1. unpack tools.zip into c:\cygwin 2. install Active perl in
> c:\Perl 3. install the mingw32 in c:\mingwin 4. add 
 > "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced ->

No, without the blank(!!!): "c:\cygwin;c:\mingwin\bin"


> Environment Variables -> Path -> Variable" (In your previous mail,
> you said "put these at the beginning of the path", I don't understand
> what is your meaning. Which path?)

Well, the PATH variable already contains a couple of paths, add the two 
given above in front of all others, separated by ";".
Don't see what is difficult to understand here.


> 5. I tried an library example. I set a new folder named "example" in
> the "c:\MyRpackages\". And In the "example" folder, it contain an
> "DESCRIPTION" file and "R" folder. in "R" folder contain a "example"
> file. I just write very simple script in it: a<-2; b<-3;sum <-
> sum(a,b); print(paste(a,"+",b,"=",sum))
> 
> 6. I opened the DOS environment. Into the "D:\>"  Type the following
> code: cd \Program Files\R\rw2010 But I don't understand the second
> line you writed in your previous mail: "bin\R cmd install
> /MyRPackages/example"

Then you should read the R Installation and Administration manual, you 
missed it, obviously!


> I am not sure that I set up R in "D:\" But I do so much action in C:\
> Did I do the correct action? Did I do the action into the correct
> path?
> 
> I still need your and others help. Thank you very much!


I think you need to read the manuals before, and additionally seek 
advice by a local Windows expert who can tell you relationships between 
paths, environment variables, executable files etc.

Uwe Ligges


> 
> 
> -----ÅÂéüÅÂÅßãÅÈÇÅÆÅ‰ÅªÅ∂----- ÅÂèëÅ‰ÅªÅ∂Å‰Å∫Å∫: Gabor Grothendieck
> [mailto:ggrothendieck at gmail.com] ÅÂèëÅÈÄÅÅÊóÅ∂ÅÈóÅ¥: 2005ÅÂÅπÅ¥6ÅÊúà6ÅÊóÅ• 10:21 ÅÊîÅ∂Å‰ÅªÅ∂Å‰Å∫Å∫: Ivy_Li ÅÊäÑ
> ÅÈÄÅ: r-help at stat.math.ethz.ch Å‰Å∏ÅªÅÈÅ¢ò: Re: [R] fail in adding library in new
> version.
> 
> 
> On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> 
>> Hello everybody, Could I consult you a question? I always use R old
>> version 1.9.1 . Because I can not add my library into the new
>> version 2.0.0 by the same method as old version.
> 
> 
> Getting the latest version of R is strongly recommended.  The
> suggestions below all assume the latest version and may or may not
> work if you do not upgrade.
> 
> 
>> *       I have read the webpage
>> <http://www.stats.ox.ac.uk/pub/Rtools> *       Download the
>> tools.zip *       Unpack tools.zip into c:\cygwin *       Install
>> Active Perl in c:\Perl *       Install the mingw32 port of gcc in
>> c:\mingwin *       Then go to "Control Panel -> System -> Advanced
>> -> Environment Variables -> Path -> Variable Balue" add
>> ;c:\cygwin;c:\mingwin\bin
> 
> 
> You may need to put these at the beginning of the path rather than
> the end. Also just as a check enter path at the console to make sure
> that you have them.  You will likely have to start a new console
> session and possibly even reboot.
> 
> Also you need the Microsoft Help Compiler, hhc.  Suggest you reread
> the material on which tools you need.
> 
> 
>> *       Save my library "example" into "c:\MyRpackages\" . But I am
>> not sure what type it is, is it need suffix?" And I don't what its
>> content, just my function script, no special format?
> 
> 
> In MyRPackages you would have a folder called example, in your case, 
> that contains the package.  Within folder example, you would have the
>  DESCRIPTION file, the R folder, etc.
> 
> 
>> *       Then I don't know where should I do this step: Type R CMD
>> INSTALL --build example. Need I run R first?
> 
> 
> You don't have to run R first.  You do need to make sure that R.exe
> can be found on your path or else use the absolute path name in
> referring to R. For example, if your path does not include R you
> could do something like this:
> 
> cd \Program Files\R\rw2010 bin\R cmd install /MyRPackages/example
> 
> Be sure to use forward slashes where shown above and backslashes 
> where shown.
> 
> 
>> So There is a error after I do this step.  It said it can not find
>> somethig. I don't which step is wrong. It costed me much time.
>> 
> 
> 
> Try all these suggestions including upgrading R and if that does not
> work try posting screen dumps of the actual errors you are getting.
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Jun 30 11:43:31 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 30 Jun 2005 11:43:31 +0200
Subject: [R] deal package
In-Reply-To: <cdf817830506291630186d1342@mail.gmail.com>
References: <cdf817830506291630186d1342@mail.gmail.com>
Message-ID: <42C3BEC3.40005@statistik.uni-dortmund.de>

Weiwei Shi wrote:

> Hi,
> I am wondering if anyone here used deal package in R to do the
> bayesian network. I am curious about its scalability: how many
> variables and how many observations can it handle in a reasonable
> time. If you have some good experience, please share your data
> configurations.

Questions like this should better be sent to the package mainatiner or 
author of the corresponding function, who probably has more experiences 
and background knowledge on the code than (almost?) all other readers of 
this list.

Uwe Ligges


> 
> 
> thanks,
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jun 30 11:46:49 2005
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 30 Jun 2005 11:46:49 +0200
Subject: [R] plot (log scale on y-axis)
In-Reply-To: <f8b847f0506291320ff15932@mail.gmail.com>
References: <f8b847f0506291320ff15932@mail.gmail.com>
Message-ID: <42C3BF89.5040106@statistik.uni-dortmund.de>

Jing Shen wrote:

> I am planning to plot my data on log scale (y-axis). There is a
> parameter in plot function, which is
> plot( ..., log="y", ...)
> While, the problem is that it is with base of e. Is there a way to let
> me change it to 10 instead of e?


But this is simpley a fixed multiple, and the axis annotation shows the 
true values. Hence there would be absolutely NO difference in the plot, 
and we do not need to add redundancy here!

Uwe Ligges


> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Jun 30 11:48:01 2005
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2005 11:48:01 +0200
Subject: [R] Memory Management under Linux: Problems to allocate large
	amounts of data
In-Reply-To: <Pine.LNX.4.61.0506300834290.21263@gannet.stats>
References: <52D1AC81378E9342947189B04176014728F77E@agentsmith.komdat.intern>
	<Pine.LNX.4.61.0506300834290.21263@gannet.stats>
Message-ID: <x2k6kc2n1q.fsf@turmalin.kubism.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Thu, 30 Jun 2005, Dubravko Dolic wrote:
> 
> > Dear Prof. Ripley.
> >
> > Thank You for Your quick answer. Your right by assuming that we run
> > R on a 32bit System. My technician tried to install R on a emulated
> > 64bit Opteron machine which led into some trouble. Maybe because the
> > Opteron includes a 32bit Processor which emulates 64bit (AMD64
> > x86_64). As You seem to have good experience with running R on a
> > 64bit OS I feel encouraged to have another try for this.

Er? What is an "emulated Opteron" machine? Opterons are 64 bit.
 
> It should work out of the box on an Opteron Linux systen: it does for
> example on FC3 and SuSE 9.x.  Some earlier Linux distros for x86_64 are
> not fully 64-bit, but we ran R on FC2 (although some packages could
> not be installed).
> 
> Trying to build a 32-bit version of R on FC3 does not work for me: the
> wrong libgcc_s is found.  (One might want a 32-bit version for speed
> on small tasks.)

On FC4 it is even easier: "yum install R R-devel" gets you a working R
2.1.1 straight away (from Fedora Extras). Only if you want to include
hardcore optimized BLAS or do not like the performance hit of having R
as a shared library do you need to compile at all.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From john.marsland at wmgfunds.com  Thu Jun 30 12:10:18 2005
From: john.marsland at wmgfunds.com (John Marsland)
Date: Thu, 30 Jun 2005 11:10:18 +0100
Subject: [R] download.file() / install.packages() from a url with a username
	and password
Message-ID: <7D85A100-8B30-4E19-BFBC-4372E9B89AC0@wmgfunds.com>

I am trying to create a repository for my own packages as an easy way  
to auto install packages on a number of servers.

Obviously, I am able to connect using install.packages() to CRAN  
without problems but when I specify my own repos I get an error:

 > install.packages(pkgs, repos="http://some.site.com")
Warnings message:
cannot open: HTTP status was '401 Authorisation Required'

I get a similar message if I try to use download.file() on the  
PACKAGES file.

I have tried the url in the form http://username:password at url.com.

Any ideas?

Regards,

John Marsland



From Dubravko.Dolic at komdat.com  Thu Jun 30 12:29:06 2005
From: Dubravko.Dolic at komdat.com (Dubravko Dolic)
Date: Thu, 30 Jun 2005 12:29:06 +0200
Subject: [R] Memory Management under Linux: Problems to allocate large
	amounts of data
Message-ID: <52D1AC81378E9342947189B04176014728F7B6@agentsmith.komdat.intern>

Dear Peter,

AMD64 and EM64T (Intel) were designed as 32bit CPUs which are able to address 64bit registers. So they are nut "pure" 64bit Systems. This is why they are much cheaper than a real 64bit machine.

-----Urspr??ngliche Nachricht-----
Von: pd at pubhealth.ku.dk [mailto:pd at pubhealth.ku.dk] Im Auftrag von Peter Dalgaard
Gesendet: Donnerstag, 30. Juni 2005 11:48
An: Prof Brian Ripley
Cc: Dubravko Dolic; r-help at stat.math.ethz.ch
Betreff: Re: [R] Memory Management under Linux: Problems to allocate large amounts of data

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Thu, 30 Jun 2005, Dubravko Dolic wrote:
> 
> > Dear Prof. Ripley.
> >
> > Thank You for Your quick answer. Your right by assuming that we run
> > R on a 32bit System. My technician tried to install R on a emulated
> > 64bit Opteron machine which led into some trouble. Maybe because the
> > Opteron includes a 32bit Processor which emulates 64bit (AMD64
> > x86_64). As You seem to have good experience with running R on a
> > 64bit OS I feel encouraged to have another try for this.

Er? What is an "emulated Opteron" machine? Opterons are 64 bit.
 
> It should work out of the box on an Opteron Linux systen: it does for
> example on FC3 and SuSE 9.x.  Some earlier Linux distros for x86_64 are
> not fully 64-bit, but we ran R on FC2 (although some packages could
> not be installed).
> 
> Trying to build a 32-bit version of R on FC3 does not work for me: the
> wrong libgcc_s is found.  (One might want a 32-bit version for speed
> on small tasks.)

On FC4 it is even easier: "yum install R R-devel" gets you a working R
2.1.1 straight away (from Fedora Extras). Only if you want to include
hardcore optimized BLAS or do not like the performance hit of having R
as a shared library do you need to compile at all.

-- 
   O__  ---- Peter Dalgaard             ??ster Farimagsgade 5, Entr.B
  c/ /'_ --- Dept. of Biostatistics     PO Box 2099, 1014 Cph. K
 (*) \(*) -- University of Copenhagen   Denmark          Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)                  FAX: (+45) 35327907



From osklyar at ebi.ac.uk  Thu Jun 30 12:41:30 2005
From: osklyar at ebi.ac.uk (Oleg Sklyar)
Date: Thu, 30 Jun 2005 11:41:30 +0100
Subject: [R] parsing '...' function argument?
Message-ID: <1120128090.8578.9.camel@localhost.localdomain>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/44298de5/attachment.pl

From ggrothendieck at gmail.com  Thu Jun 30 13:16:07 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 30 Jun 2005 07:16:07 -0400
Subject: =?GB2312?B?UmU6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZGlu?=
	=?GB2312?B?ZyBsaWJyYXJ5IGluIG5ldyB2ZXJzaW9uLg==?=
In-Reply-To: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>
Message-ID: <971536df05063004163d9e82c2@mail.gmail.com>

On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> Dear Gabor,
>        Thank your for helping me so much!
>        I have loaded R the newest version 2.1.1. Then I setup it in the path of D:\program files\R\
> 1. unpack tools.zip into c:\cygwin
> 2. install Active perl in c:\Perl
> 3. install the mingw32 in c:\mingwin
> 4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable" (In your previous mail, you said "put these at the beginning of the path", I don't understand what is your meaning. Which path?)

If in the console you enter the command:

path

then it will display a semicolon separated list of folders.  You want the folder
that contains the tools to be at the beginning so that you eliminate
the possibility
of finding a different program of the same name first in a folder that comes
prior to the one where the tools are stored.

> 
> 5. I tried an library example. I set a new folder named "example" in the "c:\MyRpackages\". And In the "example" folder, it contain an "DESCRIPTION" file and "R" folder. in "R" folder contain a "example" file. I just write very simple script in it:
> a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> 
> 6. I opened the DOS environment. Into the "D:\>"  Type the following code:
> cd \Program Files\R\rw2010
> But I don't understand the second line you writed in your previous mail: "bin\R cmd install /MyRPackages/example"

I was assuming that MyRPackages and R are on the same disk.  If they are not
then you need to specify the disk too.  That is if MyRPackages is on C and R
is installed on D then install your package via:

d:
cd \Program Files\R\rw2010
bin\R CMD install c:/MyRPackages/example

Note that bin\R means to run R.exe in the bin subfolder of the current folder 
using command script install and the indicated source package.

> 
> I am not sure that I set up R in "D:\" But I do so much action in C:\  Did I do the correct action? Did I do the action into the correct path?

If you are not sure where R is installed then enter the following at the Windows
console prompt to find out (this will work provided you let it install the key
into the registry when you installed R initially).  The reg command is a command
built into Windows (I used XP but I assume its the same on other versions)
that will query the Windows registry:

reg query hklm\software\r-core\r /v InstallPath

> 
> I still need your and others help. Thank you very much!
> 
> 
> 
> -----Å‘Å≠Å ÅºÅ”Å ÅºÅ˛-----
> Å∑Å¢ÅºÅ˛Å»ÅÀ: Gabor Grothendieck [mailto:ggrothendieck at gmail.com]
> Å∑Å¢ÅÀÅÕÅ Å±ÅºÅ‰: 2005ÅƒÅÍ6Å‘Å¬6Å»Å’ 10:21
> Å Å’ÅºÅ˛Å»ÅÀ: Ivy_Li
> Å≥Å≠ÅÀÅÕ: r-help at stat.math.ethz.ch
> Å÷Å˜ÅÃÅ‚: Re: [R] fail in adding library in new version.
> 
> 
> On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > Hello everybody,
> >        Could I consult you a question?
> >        I always use R old version 1.9.1 . Because I can not add my library into the new version 2.0.0 by the same method as old version.
> 
> Getting the latest version of R is strongly recommended.  The suggestions
> below all assume the latest version and may or may not work if you do
> not upgrade.
> 
> > *       I have read the webpage <http://www.stats.ox.ac.uk/pub/Rtools>
> > *       Download the tools.zip
> > *       Unpack tools.zip into c:\cygwin
> > *       Install Active Perl in c:\Perl
> > *       Install the mingw32 port of gcc in c:\mingwin
> > *       Then go to "Control Panel -> System -> Advanced -> Environment Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> 
> You may need to put these at the beginning of the path rather than the end.
> Also just as a check enter
>     path
> at the console to make sure that you have them.  You will likely
> have to start a new console session and possibly even reboot.
> 
> Also you need the Microsoft Help Compiler, hhc.  Suggest
> you reread the material on which tools you need.
> 
> > *       Save my library "example" into "c:\MyRpackages\" . But I am not sure what type it is, is it need suffix?" And I don't what its content, just my function script, no special format?
> 
> In MyRPackages you would have a folder called example, in your case,
> that contains the package.  Within folder example, you would have the
> DESCRIPTION file, the R folder, etc.
> 
> > *       Then I don't know where should I do this step: Type R CMD INSTALL --build example. Need I run R first?
> 
> You don't have to run R first.  You do need to make sure that R.exe can
> be found on your path or else use the absolute path name in referring to R.
> For example, if your path does not include R you could do something like this:
> 
> cd \Program Files\R\rw2010
> bin\R cmd install /MyRPackages/example

Sorry, there is an error in the above.  It should be:

bin\R CMD install c:/MyRPackages/example

or 

bin\Rcmd install c:/MyRPackages/example


> 
> Be sure to use forward slashes where shown above and backslashes
> where shown.
> 
> >        So There is a error after I do this step.  It said it can not find somethig. I don't which step is wrong. It costed me much time.
> >
> 
> Try all these suggestions including upgrading R and if that does not work
> try posting screen dumps of the actual errors you are getting.
> 

Also try googling for

  making creating R packages

and you will find some privately written tutorials on all this.



From ggrothendieck at gmail.com  Thu Jun 30 13:28:30 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 30 Jun 2005 07:28:30 -0400
Subject: [R] plot (log scale on y-axis)
In-Reply-To: <f8b847f0506291320ff15932@mail.gmail.com>
References: <f8b847f0506291320ff15932@mail.gmail.com>
Message-ID: <971536df0506300428cf8191a@mail.gmail.com>

On 6/29/05, Jing Shen <jshen6 at gmail.com> wrote:
> I am planning to plot my data on log scale (y-axis). There is a
> parameter in plot function, which is
> plot( ..., log="y", ...)
> While, the problem is that it is with base of e. Is there a way to let
> me change it to 10 instead of e?
> 

Is your question how to get the axis labels to be powers of 10?
In that case,

plot(1:100, log = "y", yaxt = "n")  # do not show y axis 
axis(2, c(1,10,100))  # draw y axis with required labels



From ggrothendieck at gmail.com  Thu Jun 30 13:34:17 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 30 Jun 2005 07:34:17 -0400
Subject: [R] parsing '...' function argument?
In-Reply-To: <1120128090.8578.9.camel@localhost.localdomain>
References: <1120128090.8578.9.camel@localhost.localdomain>
Message-ID: <971536df050630043427f29273@mail.gmail.com>

On 6/30/05, Oleg Sklyar <osklyar at ebi.ac.uk> wrote:
> Dear community,
> 
> I am writing a wrapper for '[' operator, which is a generic method
> declared as function(x, i, j, ..., drop). It turns out that I need to
> parse the '...' argument and this is where I am stuck. Generally what I
> need is the following. Say the call is obj[1, 1, 1:10, 3] - here '1:10,
> 3' is passed into '...'. What I need to evaluate that '...' contains now
> 2 arguments, first is a vector of 10 elements and second is a single
> value. Even nicer situation is in call obj[1, 1, , 3] - where ' , 3' is
> passed in '...' with one missing argument, which for an array would mean
> the full range. Any ideas? (Just to mention - if there were only one
> argument in '...', say '1:10', then length(...) would return 10 and one
> could access elements by ...[[i]], but all this fails if there are two
> arguments or more).
> 

Check out the technique used in this post:

http://tolstoy.newcastle.edu.au/R/help/04/06/1430.html



From ripley at stats.ox.ac.uk  Thu Jun 30 13:38:24 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Jun 2005 12:38:24 +0100 (BST)
Subject: [R] download.file() / install.packages() from a url with a
 username and password
In-Reply-To: <7D85A100-8B30-4E19-BFBC-4372E9B89AC0@wmgfunds.com>
References: <7D85A100-8B30-4E19-BFBC-4372E9B89AC0@wmgfunds.com>
Message-ID: <Pine.LNX.4.61.0506301235450.18453@gannet.stats>

On Thu, 30 Jun 2005, John Marsland wrote:

> I am trying to create a repository for my own packages as an easy way
> to auto install packages on a number of servers.
>
> Obviously, I am able to connect using install.packages() to CRAN
> without problems but when I specify my own repos I get an error:
>
> > install.packages(pkgs, repos="http://some.site.com")
> Warnings message:
> cannot open: HTTP status was '401 Authorisation Required'
>
> I get a similar message if I try to use download.file() on the
> PACKAGES file.
>
> I have tried the url in the form http://username:password at url.com.
>
> Any ideas?

You need to use a method which supports this form of access.

The basic internal method in R does not.
It is likely that --internet2 on Windows does.
method="wget" does.

Since the internal method can handle authentication on proxies, it would 
not be hard to add this based on the existing code - patches are welcome.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From robin.smit at tno.nl  Thu Jun 30 14:40:39 2005
From: robin.smit at tno.nl (Smit, R. (Robin))
Date: Thu, 30 Jun 2005 14:40:39 +0200
Subject: [R] Graphically centering confidence interval in barplot
Message-ID: <2395774549BBDA40AC83BC9E6223FBFF22F9C9@MS-DT01VS01.tsn.tno.nl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/f1fc3fb1/attachment.pl

From dmbates at gmail.com  Thu Jun 30 14:48:43 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 30 Jun 2005 07:48:43 -0500
Subject: [R] [OT] gmail filter for R-help and R-devel lists
Message-ID: <40e66e0b0506300548188cd468@mail.gmail.com>

This is slightly off-topic but I would be interested in whether anyone
has succeeded in creating a filter expression for Google's gmail
system that will select messages sent through the R-help and R-devel
lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
in the subject line but I haven't been able to work out the exact
syntax that would do this and not select messages that have an 'R'
anywhere in the subject.



From MSchwartz at mn.rr.com  Thu Jun 30 14:53:10 2005
From: MSchwartz at mn.rr.com (Marc Schwartz)
Date: Thu, 30 Jun 2005 07:53:10 -0500
Subject: [R] Graphically centering confidence interval in barplot
In-Reply-To: <2395774549BBDA40AC83BC9E6223FBFF22F9C9@MS-DT01VS01.tsn.tno.nl>
References: <2395774549BBDA40AC83BC9E6223FBFF22F9C9@MS-DT01VS01.tsn.tno.nl>
Message-ID: <1120135990.30960.22.camel@localhost.localdomain>

On Thu, 2005-06-30 at 14:40 +0200, Smit, R. (Robin) wrote:
> Hello,
>  
> I have got a simple "cosmetic" question.
> I have created a bar plot with confidence intervals using:
>  
> barplot(mean, ylim = c(0,0.2), las = 3, space = 0)
> 
> arrows(1:17 ,X95p_low, 1:17, X95p_high, length = 0.07, angle = 90, code
> = 3, lty = 1)
> 
>  
> 
> Unfortunately, the confidence bars align with the right side of the bars
> and I would like to shift these to the middle of the bars.
> 
> Is there someone who can tell me how I can do this?
> 
>  
> 
> Kind regards,
> Robin Smit


barplot() returns the bar midpoints, so you need to use these as your
'x' axis positions for the CI's:

mp <- barplot(mean, ylim = c(0,0.2), las = 3, space = 0)

arrows(mp, X95p_low, mp, X95p_high, length = 0.07, angle = 90, 
       code = 3, lty = 1)


Alternatively, you can use the barplot2() function in the 'gplots'
package from CRAN:

barplot2(mean, ylim = c(0,0.2), las = 3, space = 0, plot.ci = TRUE,
         ci.l = X95p_low, ci.u = X95p_high)


BTW, you should avoid using 'mean' for your data vector, since that is
also the name of a R function. R is smart enough to know the difference
in most cases, but it could get you into trouble at some point.

HTH,

Marc Schwartz



From dmbates at gmail.com  Thu Jun 30 15:13:07 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 30 Jun 2005 08:13:07 -0500
Subject: [R] installation error
In-Reply-To: <42C3ABA7.3020600@deprem.gov.tr>
References: <42C3ABA7.3020600@deprem.gov.tr>
Message-ID: <40e66e0b0506300613416572e9@mail.gmail.com>

On 6/30/05, orkun <temiz at deprem.gov.tr> wrote:
> hello
> 
> when I follow this procedure:
> ~~~~~~~~~~
> rS <- "http://godel.cs.bilgi.edu.tr/mirror/cran/src/contrib/"
> install.packages(c("Matrix"), repos=rS, dependencies=TRUE)
> ~~~~~~~~~~
> 
> installation gives this error:
> ~~~~~~~~~~~~~
> make: *** [Matrix.so] Error 1
> ERROR: compilation failed for package 'Matrix'
> ** Removing '/usr/local/lib/R/site-library/Matrix'
> ** Restoring previous '/usr/local/lib/R/site-library/Matrix'
> 
> The downloaded packages are in
>         /tmp/RtmpPx5hkE/downloaded_packages
> Warning message:
> installation of package 'Matrix' had non-zero exit status in:
> install.packages(c("Matrix"), repos = rS, dependencies = TRUE)
> ~~~~~~~~~~
> I am using debian and  had installed R in debian way.
> 
> what might possible solutions be ?

Check that you have the Debian package r-base-dev installed.  This is
a virtual package whose installation ensures that you have the
necessary tools installed so that you can compile R packages.

Alternatively, you could install the precompiled Debian package r-cran-matrix



From marten.winter at ufz.de  Thu Jun 30 15:26:03 2005
From: marten.winter at ufz.de (Marten Winter)
Date: Thu, 30 Jun 2005 15:26:03 +0200
Subject: [R] mApply instead of by ???
Message-ID: <42C3F2EB.7060605@ufz.de>



From ecoinformatics at gmail.com  Thu Jun 30 15:30:19 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Thu, 30 Jun 2005 15:30:19 +0200
Subject: =?GB2312?B?UmU6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZGlu?=
	=?GB2312?B?ZyBsaWJyYXJ5IGluIG5ldyB2ZXJzaW9uLg==?=
In-Reply-To: <971536df0506300549b1ba57f@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>
	<971536df05063004163d9e82c2@mail.gmail.com>
	<15f8e67d050630054154623363@mail.gmail.com>
	<971536df0506300549b1ba57f@mail.gmail.com>
Message-ID: <15f8e67d05063006307b32b6fe@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/4054d817/attachment.pl

From ghislainv at gmail.com  Thu Jun 30 15:35:30 2005
From: ghislainv at gmail.com (Ghislain Vieilledent)
Date: Thu, 30 Jun 2005 15:35:30 +0200
Subject: [R] Linear Models with mean as Intercept.
Message-ID: <42C3F522.3030609@gmail.com>

Dear advanced statisticians,

*******Objectif********

I try to set up linear models with mean as intercept:
Answer: y
Variable: x, as factor of two modalities: x(1), x(2).

I would like to have a model as:
y = mean(y)+A(i)+residuals,
with i in (1,2) and A(1) coefficient for x(1) and A(2) coefficient for x(2).

*******Trials in R*******

## Firstly:

I write in R:
 >Model<-lm(y~x,Data)
 >summary(Model)
...
I've got the coefficients for:
- the intercept (x(1) as been choosen) that we can call B(1)
- the second modality: x(2) that we can call B(2)

If I have well understood we have for the model and predictions:
if x(1): y=B(1)
if x(2): y=B(1)+B(2)
which is quite different as y=mean(y)+A(i)

## Secondly
I tried to skip the intercept
 >Model2<-lm(y~0+x,Data)
 >summary(Model2)
...
I've got the coefficients for:
- the first modality: x(1) that we can call C(1)
- the second modality: x(2) that we can call C(2)

And the model and predictions, if I'm right, are:
if x(1): y=C(1)
if x(2): y=C(2)

******* Questions ***********
How can I obtain a predictive model y=mean(y)+A(i) ?
Is it possible to settle mean(y) as intercept?

Thanks for your help.

Ghislain V., retarded statistician.



From khobson at fd9ns01.okladot.state.ok.us  Thu Jun 30 15:44:12 2005
From: khobson at fd9ns01.okladot.state.ok.us (khobson@fd9ns01.okladot.state.ok.us)
Date: Thu, 30 Jun 2005 08:44:12 -0500
Subject: [R]  how to call egarch of sas in R
Message-ID: <OF56A2A187.1E740A0F-ON86257030.004B6B94-86257030.004B57CA@fd9ns01.okladot.state.ok.us>





>From past posts:
There are a number of GARCH models available in the fSeries package --
including models with t an skew-t distributions.

mailto:khobson at odot.org
Kenneth Ray Hobson, P.E.
Oklahoma DOT - QA & IAS Manager
200 N.E. 21st Street
Oklahoma City, OK  73105-3204
(405) 522-4985, (405) 522-0552 fax

Visit our website at:
http://www.okladot.state.ok.us/materials/materials.htm



From I.Visser at uva.nl  Thu Jun 30 15:50:54 2005
From: I.Visser at uva.nl (Ingmar Visser)
Date: Thu, 30 Jun 2005 15:50:54 +0200
Subject: [R] Linear Models with mean as Intercept.
In-Reply-To: <42C3F522.3030609@gmail.com>
Message-ID: <BEE9C55E.4B95%I.Visser@uva.nl>

you could model: (y-mean(y))~x
hth, ingmar

> From: Ghislain Vieilledent <ghislainv at gmail.com>
> Date: Thu, 30 Jun 2005 15:35:30 +0200
> To: r-help at stat.math.ethz.ch
> Subject: [R] Linear Models with mean as Intercept.
> 
> ******* Questions ***********
> How can I obtain a predictive model y=mean(y)+A(i) ?
> Is it possible to settle mean(y) as intercept?



From Arne.Muller at sanofi-aventis.com  Thu Jun 30 15:51:11 2005
From: Arne.Muller at sanofi-aventis.com (Arne.Muller@sanofi-aventis.com)
Date: Thu, 30 Jun 2005 15:51:11 +0200
Subject: [R] randomForest error
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE01846A83@CRBSMXSUSR04>

Hello,

I'm using the random forest package. One of my factors in the data set contains 41 levels (I can't code this as a numeric value - in terms of linear models this would be a random factor). The randomForest call comes back with an error telling me that the limit is 32 categories.

Is there any reason for this particular limit? Maybe it's possible to recompile the module with a different cutoff?

	thanks a  lot for your help,
	kind regards,


	Arne



From andy_liaw at merck.com  Thu Jun 30 16:20:08 2005
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 30 Jun 2005 10:20:08 -0400
Subject: [R] randomForest error
Message-ID: <39B6DDB9048D0F4DAD42CB26AAFF0AFA64EA4E@usctmx1106.Merck.com>

The limitation comes from the way categorical splits are represented in the
code:  For a categorical variable with k categories, the split is
represented by k binary digits: 0=right, 1=left.  So it takes k bits to
store each split on k categories.  To save storage, this is `packed' into a
4-byte integer (32-bit), thus the limit of 32 categories.

The current Fortran code (version 5.x) by Breiman and Cutler gets around
this limitation by storing the split in an integer array.  While this lifts
the 32-category limit, it takes much more memory to store the splits.  I'm
still trying to figure out a more memory efficient way of storing the splits
without imposing the 32-category limit.  If anyone has suggestions, I'm all
ears.

Best,
Andy

> From: Arne.Muller at sanofi-aventis.com
> 
> Hello,
> 
> I'm using the random forest package. One of my factors in the 
> data set contains 41 levels (I can't code this as a numeric 
> value - in terms of linear models this would be a random 
> factor). The randomForest call comes back with an error 
> telling me that the limit is 32 categories.
> 
> Is there any reason for this particular limit? Maybe it's 
> possible to recompile the module with a different cutoff?
> 
> 	thanks a  lot for your help,
> 	kind regards,
> 
> 
> 	Arne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
>



From ggrothendieck at gmail.com  Thu Jun 30 16:21:54 2005
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Thu, 30 Jun 2005 10:21:54 -0400
Subject: =?GB2312?B?UmU6ILTwuLQ6IFtSXSBmYWlsIGluIGFkZGlu?=
	=?GB2312?B?ZyBsaWJyYXJ5IGluIG5ldyB2ZXJzaW9uLg==?=
In-Reply-To: <15f8e67d05063006307b32b6fe@mail.gmail.com>
References: <AAE1B4226B64D743925F5E0BAD982B4E03FF1A@ex120.smic-sh.com>
	<971536df05063004163d9e82c2@mail.gmail.com>
	<15f8e67d050630054154623363@mail.gmail.com>
	<971536df0506300549b1ba57f@mail.gmail.com>
	<15f8e67d05063006307b32b6fe@mail.gmail.com>
Message-ID: <971536df050630072133aaa2c0@mail.gmail.com>

It seems one must use 'R CMD install ...' where the CMD must be capitalized. 
If CMD is not capitalized then one gets the behavior you got. 

On 6/30/05, ecoinfo <ecoinformatics at gmail.com> wrote:
> It is interesting that in my PC
>  
> >R cmd
>  
> is same as
>  
> >R
>  
> Which leads me into the R interface. 
>  
> ------------------------------------------------------------------------------
> 
> 
> Microsoft Windows XP [Version 5.1.2600]
> (C) Copyright 1985-2001 Microsoft Corp.
> 
> C:\Documents and Settings\xiaohuadai>r cmd
> ARGUMENT 'cmd' __ignored__
> 
> 
> R : Copyright 2005, The R Foundation for Statistical Computing
> Version 2.1.1  (2005-06-20), ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
>   Natural language support but running in an English locale
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> Working directory is C:/Documents and Settings/xiaohuadai
> bugs() function loaded from file c:\bugsR\bugs.R
> display() and sim() functions loaded from file c:\bugsR\regression.R
> >
> -------------------------------------------------------------------------------
>  
> Xiaohua
> 
>  
> On 6/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote: 
> > They both work for me.  I just installed a package using R CMD
> > and then did it again using Rcmd.  Here is the input and 
> > first few lines of output for the first one:
> > 
> >
> -------------------------------------------------------------------------------------------------
> > C:\Program Files\R\rw2011pat>bin\R CMD install /Rpkgs/vcd
> > 
> > 
> > ---------- Making package vcd ------------
> > 
> > adding build stamp to DESCRIPTION
> > installing R files
> > installing demos
> > ...etc...
> >
> -------------------------------------------------------------------------------------------------
> > 
> > and here it is for the second one:
> > 
> >
> -------------------------------------------------------------------------------------------------
> > C:\Program Files\R\rw2011pat>bin\Rcmd install /Rpkgs/vcd
> > 
> > 
> > ---------- Making package vcd ------------
> > adding build stamp to DESCRIPTION
> > installing R files
> > installing demos
> > ...etc..
> >
> -------------------------------------------------------------------------------------------------
> > 
> > 
> > On 6/30/05, ecoinfo <ecoinformatics at gmail.com> wrote:
> > > In my Windows XP, the commond is RCMD not R CMD.
> > >
> > >
> > > On 6/30/05, Gabor Grothendieck < ggrothendieck at gmail.com> wrote:
> > > >
> > > > On 6/30/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > > > > Dear Gabor, 
> > > > >        Thank your for helping me so much!
> > > > >        I have loaded R the newest version 2.1.1. Then I setup it in
> the
> > > path of D:\program files\R\
> > > > > 1. unpack tools.zip into c:\cygwin
> > > > > 2. install Active perl in c:\Perl
> > > > > 3. install the mingw32 in c:\mingwin
> > > > > 4. add "c:\cygwin; c:\mingwin\bin" in "Control Panel -> System -> 
> > > Advanced -> Environment Variables -> Path -> Variable" (In your previous
> > > mail, you said "put these at the beginning of the path", I don't
> understand
> > > what is your meaning. Which path?) 
> > > >
> > > > If in the console you enter the command:
> > > >
> > > > path
> > > >
> > > > then it will display a semicolon separated list of folders.  You want
> the
> > > folder
> > > > that contains the tools to be at the beginning so that you eliminate 
> > > > the possibility
> > > > of finding a different program of the same name first in a folder that
> > > comes
> > > > prior to the one where the tools are stored.
> > > >
> > > > >
> > > > > 5. I tried an library example. I set a new folder named "example" in
> the 
> > > "c:\MyRpackages\". And In the "example" folder, it contain an
> "DESCRIPTION"
> > > file and "R" folder. in "R" folder contain a "example" file. I just
> write 
> > > very simple script in it:
> > > > > a<-2; b<-3;sum <- sum(a,b); print(paste(a,"+",b,"=",sum))
> > > > >
> > > > > 6. I opened the DOS environment. Into the "D:\>"  Type the following
> > > code:
> > > > > cd \Program Files\R\rw2010
> > > > > But I don't understand the second line you writed in your previous
> mail:
> > > "bin\R cmd install /MyRPackages/example"
> > > > 
> > > > I was assuming that MyRPackages and R are on the same disk.  If they
> are
> > > not
> > > > then you need to specify the disk too.  That is if MyRPackages is on C
> and
> > > R
> > > > is installed on D then install your package via: 
> > > >
> > > > d:
> > > > cd \Program Files\R\rw2010
> > > > bin\R CMD install c:/MyRPackages/example
> > > >
> > > > Note that bin\R means to run R.exe in the bin subfolder of the current
> > > folder
> > > > using command script install and the indicated source package.
> > > >
> > > > >
> > > > > I am not sure that I set up R in "D:\" But I do so much action in
> C:\
> > > Did I do the correct action? Did I do the action into the correct path? 
> > > >
> > > > If you are not sure where R is installed then enter the following at
> the
> > > Windows
> > > > console prompt to find out (this will work provided you let it install
> the
> > > key
> > > > into the registry when you installed R initially).  The reg command is
> a 
> > > command
> > > > built into Windows (I used XP but I assume its the same on other
> versions)
> > > > that will query the Windows registry:
> > > >
> > > > reg query hklm\software\r-core\r /v InstallPath 
> > > >
> > > > >
> > > > > I still need your and others help. Thank you very much!
> > > > >
> > > > >
> > > > >
> > > > > -----Å‘Å≠Å ÅºÅ”Å ÅºÅ˛-----
> > > > > Å∑Å¢ÅºÅ˛Å»ÅÀ: Gabor Grothendieck [mailto: ggrothendieck at gmail.com]
> > > > > Å∑Å¢ÅÀÅÕÅ Å±ÅºÅ‰: 2005ÅƒÅÍ6Å‘Å¬6Å»Å’ 10:21
> > > > > Å Å’ÅºÅ˛Å»ÅÀ: Ivy_Li
> > > > > Å≥Å≠ÅÀÅÕ: r-help at stat.math.ethz.ch 
> > > > > Å÷Å˜ÅÃÅ‚: Re: [R] fail in adding library in new version.
> > > > >
> > > > >
> > > > > On 6/5/05, Ivy_Li <Ivy_Li at smics.com> wrote:
> > > > > > Hello everybody, 
> > > > > >        Could I consult you a question?
> > > > > >        I always use R old version 1.9.1 . Because I can not add my
> > > library into the new version 2.0.0 by the same method as old version. 
> > > > >
> > > > > Getting the latest version of R is strongly recommended.  The
> > > suggestions
> > > > > below all assume the latest version and may or may not work if you
> do
> > > > > not upgrade. 
> > > > >
> > > > > > *       I have read the webpage
> > > <http://www.stats.ox.ac.uk/pub/Rtools >
> > > > > > *       Download the tools.zip
> > > > > > *       Unpack tools.zip into c:\cygwin
> > > > > > *       Install Active Perl in c:\Perl
> > > > > > *       Install the mingw32 port of gcc in c:\mingwin
> > > > > > *       Then go to "Control Panel -> System -> Advanced ->
> Environment 
> > > Variables -> Path -> Variable Balue" add ;c:\cygwin;c:\mingwin\bin
> > > > >
> > > > > You may need to put these at the beginning of the path rather than
> the
> > > end.
> > > > > Also just as a check enter 
> > > > >     path
> > > > > at the console to make sure that you have them.  You will likely
> > > > > have to start a new console session and possibly even reboot.
> > > > >
> > > > > Also you need the Microsoft Help Compiler, hhc.  Suggest 
> > > > > you reread the material on which tools you need.
> > > > >
> > > > > > *       Save my library "example" into "c:\MyRpackages\" . But I
> am
> > > not sure what type it is, is it need suffix?" And I don't what its
> content, 
> > > just my function script, no special format?
> > > > >
> > > > > In MyRPackages you would have a folder called example, in your case,
> > > > > that contains the package.  Within folder example, you would have
> the 
> > > > > DESCRIPTION file, the R folder, etc.
> > > > >
> > > > > > *       Then I don't know where should I do this step: Type R CMD
> > > INSTALL --build example. Need I run R first?
> > > > > 
> > > > > You don't have to run R first.  You do need to make sure that R.exe
> can
> > > > > be found on your path or else use the absolute path name in
> referring to
> > > R.
> > > > > For example, if your path does not include R you could do something
> like 
> > > this:
> > > > >
> > > > > cd \Program Files\R\rw2010
> > > > > bin\R cmd install /MyRPackages/example
> > > >
> > > > Sorry, there is an error in the above.  It should be:
> > > > 
> > > > bin\R CMD install c:/MyRPackages/example
> > > >
> > > > or
> > > >
> > > > bin\Rcmd install c:/MyRPackages/example
> > > >
> > > >
> > > > >
> > > > > Be sure to use forward slashes where shown above and backslashes 
> > > > > where shown.
> > > > >
> > > > > >        So There is a error after I do this step.  It said it can
> not
> > > find somethig. I don't which step is wrong. It costed me much time.
> > > > > >
> > > > >
> > > > > Try all these suggestions including upgrading R and if that does not
> > > work
> > > > > try posting screen dumps of the actual errors you are getting.
> > > > >
> > > >
> > > > Also try googling for
> > > >
> > > > making creating R packages
> > > >
> > > > and you will find some privately written tutorials on all this.
> > > >
> > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > > >
> > >
> > >
> > >
> > > --
> > > Xiaohua Dai
> > > Centre for Systems Research, Durban Institute of Technology
> > > P.O.Box 953, Durban 4000, South Africa
> > >
> > 
> 
> 
> 
> -- 
> Xiaohua Dai, Dr.
>



From jshen6 at gmail.com  Thu Jun 30 16:23:21 2005
From: jshen6 at gmail.com (Jing Shen)
Date: Thu, 30 Jun 2005 09:23:21 -0500
Subject: [R] plot (log scale on y-axis)
In-Reply-To: <971536df0506300428cf8191a@mail.gmail.com>
References: <f8b847f0506291320ff15932@mail.gmail.com>
	<971536df0506300428cf8191a@mail.gmail.com>
Message-ID: <f8b847f05063007233577f407@mail.gmail.com>

Thanks to all of you! Yes, I want to get the axis labels to be powers
of 10. I am going to try the code you recommended. Thanks a lot,
Jing

On 6/30/05, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> On 6/29/05, Jing Shen <jshen6 at gmail.com> wrote:
> > I am planning to plot my data on log scale (y-axis). There is a
> > parameter in plot function, which is
> > plot( ..., log="y", ...)
> > While, the problem is that it is with base of e. Is there a way to let
> > me change it to 10 instead of e?
> >
> 
> Is your question how to get the axis labels to be powers of 10?
> In that case,
> 
> plot(1:100, log = "y", yaxt = "n")  # do not show y axis
> axis(2, c(1,10,100))  # draw y axis with required labels
>



From ecoinformatics at gmail.com  Thu Jun 30 16:36:13 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Thu, 30 Jun 2005 16:36:13 +0200
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <40e66e0b0506300548188cd468@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
Message-ID: <15f8e67d05063007368c08da5@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/c9378f3f/attachment.pl

From dmbates at gmail.com  Thu Jun 30 16:42:10 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 30 Jun 2005 09:42:10 -0500
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <15f8e67d05063007368c08da5@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<15f8e67d05063007368c08da5@mail.gmail.com>
Message-ID: <40e66e0b05063007421aae2676@mail.gmail.com>

On 6/30/05, ecoinfo <ecoinformatics at gmail.com> wrote:
> The subject keywords in my filter is 
>   
> "[R]" OR "[Rd]" 
>   
> and it works well.

It may appear to be working but I think the '[' and ']' are ignored
and the filter will select messages with subjects like 'Rare books'.
 
>   
> On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote: 
> > 
> > This is slightly off-topic but I would be interested in whether anyone
> > has succeeded in creating a filter expression for Google's gmail 
> > system that will select messages sent through the R-help and R-devel
> > lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
> > in the subject line but I haven't been able to work out the exact
> > syntax that would do this and not select messages that have an 'R' 
> > anywhere in the subject.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> 
>  -- 
> Xiaohua Dai, Dr.
> --------------------------------------------------------------------------------
> * Postdoctoral in elephant-tree ecosystem simulation
> * PhD in forest ecology
> * Master in entomological ecology 
> ---------------------------------------------------------------------------------
> Centre for Systems Research, Durban Institute of Technology
> P.O.Box 953, Durban 4000, South Africa
> Tel: +27-31-2042737(O) Fax: +27-31-2042736(O) 
> Mobile: +27-723682954
> Email: daixiaohua at nju.org.cn, xiaohua at dit.ac.za
> Publications: http://www.getcited.org/?MBR=11061629
> Personla Blog:
> http://www.blog.edu.cn/blog.asp?name=ecoinformatics
> My Son's Blog:
> http://www.xinxinbao.com/blog/blog.asp?name=steed
> 
> Post Address: 
> c/o Ms Thelma Roos
> 11-148 Musgrave Road, Berea, Durban 4001, South Africa



From ecoinformatics at gmail.com  Thu Jun 30 16:49:31 2005
From: ecoinformatics at gmail.com (ecoinfo)
Date: Thu, 30 Jun 2005 16:49:31 +0200
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <40e66e0b05063007421aae2676@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<15f8e67d05063007368c08da5@mail.gmail.com>
	<40e66e0b05063007421aae2676@mail.gmail.com>
Message-ID: <15f8e67d050630074944356427@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/11c625e6/attachment.pl

From sundar.dorai-raj at pdf.com  Thu Jun 30 16:54:51 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 30 Jun 2005 09:54:51 -0500
Subject: [R] Linear Models with mean as Intercept.
In-Reply-To: <42C3F522.3030609@gmail.com>
References: <42C3F522.3030609@gmail.com>
Message-ID: <42C407BB.6090004@pdf.com>



Ghislain Vieilledent wrote:
> Dear advanced statisticians,
> 
> *******Objectif********
> 
> I try to set up linear models with mean as intercept:
> Answer: y
> Variable: x, as factor of two modalities: x(1), x(2).
> 
> I would like to have a model as:
> y = mean(y)+A(i)+residuals,
> with i in (1,2) and A(1) coefficient for x(1) and A(2) coefficient for x(2).
> 
> *******Trials in R*******
> 
> ## Firstly:
> 
> I write in R:
>  >Model<-lm(y~x,Data)
>  >summary(Model)
> ...
> I've got the coefficients for:
> - the intercept (x(1) as been choosen) that we can call B(1)
> - the second modality: x(2) that we can call B(2)
> 
> If I have well understood we have for the model and predictions:
> if x(1): y=B(1)
> if x(2): y=B(1)+B(2)
> which is quite different as y=mean(y)+A(i)
> 
> ## Secondly
> I tried to skip the intercept
>  >Model2<-lm(y~0+x,Data)
>  >summary(Model2)
> ...
> I've got the coefficients for:
> - the first modality: x(1) that we can call C(1)
> - the second modality: x(2) that we can call C(2)
> 
> And the model and predictions, if I'm right, are:
> if x(1): y=C(1)
> if x(2): y=C(2)
> 
> ******* Questions ***********
> How can I obtain a predictive model y=mean(y)+A(i) ?
> Is it possible to settle mean(y) as intercept?
> 
> Thanks for your help.
> 
> Ghislain V., retarded statistician.


You have to set the contrasts differently:

set.seed(1)
y <- rnorm(10)
x <- factor(rep(letters[1:5], each = 2))
fit <- lm(y ~ x, contrasts = list(x = "contr.sum"))
all.equal(mean(y), coef(fit)[1])
#[1] TRUE

"contr.helmert" will work too. The default "contr.treatment" does not 
have columns that sum to zero. Be forewarned: the coefficients need to 
be interpretted differently.

HTH,

--sundar



From dmbates at gmail.com  Thu Jun 30 17:06:13 2005
From: dmbates at gmail.com (Douglas Bates)
Date: Thu, 30 Jun 2005 10:06:13 -0500
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <15f8e67d050630074944356427@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<15f8e67d05063007368c08da5@mail.gmail.com>
	<40e66e0b05063007421aae2676@mail.gmail.com>
	<15f8e67d050630074944356427@mail.gmail.com>
Message-ID: <40e66e0b050630080642d4c024@mail.gmail.com>

On 6/30/05, ecoinfo <ecoinformatics at gmail.com> wrote:
> definitely not true. There are only emails from r-help and r-devel lists in
> my R lable. I just sent an email with the subject "Rare book" to myself, and
> it didnot go to my R lable.

I should have checked more carefully.  The subject 'Rare book' will
not be selected by the filter but the subject 'A question about R' (a
subject that occurs all too frequently in my email) does as well as
the subject 'Trouble with R-help'.  It appears that the algorithm
splits the subject into tokens on boundaries of non-alphabetic
characters and will match any token 'R'.

I really do want to filter just the mail sent through the list.  As I
mentioned above, I get a large quantity of R-related mail that doesn't
necessarily come through a list.

> 
> 
>  
> On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote: 
> > On 6/30/05, ecoinfo <ecoinformatics at gmail.com> wrote:
> > > The subject keywords in my filter is
> > >
> > > "[R]" OR "[Rd]"
> > >
> > > and it works well.
> > 
> > It may appear to be working but I think the '[' and ']' are ignored
> > and the filter will select messages with subjects like 'Rare books'. 
> > 
> > >
> > > On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote:
> > > >
> > > > This is slightly off-topic but I would be interested in whether anyone
> > > > has succeeded in creating a filter expression for Google's gmail 
> > > > system that will select messages sent through the R-help and R-devel
> > > > lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
> > > > in the subject line but I haven't been able to work out the exact 
> > > > syntax that would do this and not select messages that have an 'R'
> > > > anywhere in the subject.
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > >
> > >
> > >  --
> > > Xiaohua Dai, Dr.
> > >
> --------------------------------------------------------------------------------
> > > * Postdoctoral in elephant-tree ecosystem simulation 
> > > * PhD in forest ecology
> > > * Master in entomological ecology
> > >
> ---------------------------------------------------------------------------------
> > > Centre for Systems Research, Durban Institute of Technology 
> > > P.O.Box 953, Durban 4000, South Africa
> > > Tel: +27-31-2042737(O) Fax: +27-31-2042736(O)
> > > Mobile: +27-723682954
> > > Email: daixiaohua at nju.org.cn, xiaohua at dit.ac.za
> > > Publications: http://www.getcited.org/?MBR=11061629
> > > Personla Blog:
> > > http://www.blog.edu.cn/blog.asp?name=ecoinformatics
> > > My Son's Blog:
> > > http://www.xinxinbao.com/blog/blog.asp?name=steed
> > >
> > > Post Address: 
> > > c/o Ms Thelma Roos
> > > 11-148 Musgrave Road, Berea, Durban 4001, South Africa
> > 
> 
> 
> 
> -- 
> Xiaohua Dai, Dr.
> 
>



From dr.mike at ntlworld.com  Thu Jun 30 17:10:35 2005
From: dr.mike at ntlworld.com (Mike Waters)
Date: Thu, 30 Jun 2005 16:10:35 +0100
Subject: [R] deal package
In-Reply-To: <42C3BEC3.40005@statistik.uni-dortmund.de>
Message-ID: <20050630151041.PKAB11226.aamta11-winn.ispmail.ntl.com@d600>

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
Sent: 30 June 2005 10:44
To: Weiwei Shi
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] deal package

Weiwei Shi wrote:

> Hi,
> I am wondering if anyone here used deal package in R to do the 
> bayesian network. I am curious about its scalability: how many 
> variables and how many observations can it handle in a reasonable 
> time. If you have some good experience, please share your data 
> configurations.

Questions like this should better be sent to the package mainatiner or
author of the corresponding function, who probably has more experiences and
background knowledge on the code than (almost?) all other readers of this
list.

Uwe Ligges


> 
> 
> thanks,
> 
>
There are resources and a link to subscribe to the mailing list for DEAL at
the following URL:

http://www.math.aau.dk/novo/deal/

That's the best place for you to go for help here.

Regards,

Mike
_____________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From helprhelp at gmail.com  Thu Jun 30 17:16:03 2005
From: helprhelp at gmail.com (Weiwei Shi)
Date: Thu, 30 Jun 2005 10:16:03 -0500
Subject: [R] deal package
In-Reply-To: <20050630151041.PKAB11226.aamta11-winn.ispmail.ntl.com@d600>
References: <42C3BEC3.40005@statistik.uni-dortmund.de>
	<20050630151041.PKAB11226.aamta11-winn.ispmail.ntl.com@d600>
Message-ID: <cdf817830506300816405f55c8@mail.gmail.com>

I was there and subscribed to the list. But thanks anyway for the
info. I will forward this question to there. I just wished I could get
some suggestion too from here.

On 6/30/05, Mike Waters <dr.mike at ntlworld.com> wrote:
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Uwe Ligges
> Sent: 30 June 2005 10:44
> To: Weiwei Shi
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] deal package
> 
> Weiwei Shi wrote:
> 
> > Hi,
> > I am wondering if anyone here used deal package in R to do the
> > bayesian network. I am curious about its scalability: how many
> > variables and how many observations can it handle in a reasonable
> > time. If you have some good experience, please share your data
> > configurations.
> 
> Questions like this should better be sent to the package mainatiner or
> author of the corresponding function, who probably has more experiences and
> background knowledge on the code than (almost?) all other readers of this
> list.
> 
> Uwe Ligges
> 
> 
> >
> >
> > thanks,
> >
> >
> There are resources and a link to subscribe to the mailing list for DEAL at
> the following URL:
> 
> http://www.math.aau.dk/novo/deal/
> 
> That's the best place for you to go for help here.
> 
> Regards,
> 
> Mike
> _____________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 


-- 
Weiwei Shi, Ph.D

"Did you always know?"
"No, I did not. But I believed..."
---Matrix III



From p.campbell at econ.bbk.ac.uk  Thu Jun 30 17:11:05 2005
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Thu, 30 Jun 2005 16:11:05 +0100
Subject: [R] Relationship between dyn.load and library.dynam
Message-ID: <s2c419b7.026@markets.econ.bbk.ac.uk>

I am a little confused about the relationship between library.dynam and
dyn.load

>From the documentation:

library.dynam(chname,

Load the specified file of compiled code if it has not been loaded
already, or unloads it. Where chname is a character string naming a
shared library to load.

and

dyn.load(x, local = TRUE, now = TRUE)

Load or unload shared libraries, and test whether a C function or
Fortran subroutine is available. Where x is a character string giving
the pathname to a shared library or DLL.

Perhaps the problems is caused by the relationship between 'shared
libraries' and 'complied code'.  Given 
that the compiled code is not an executable then it must be a shared
library. 

Both these functions would appear to do the same thing; load code
complied using R CMD SHLIB into R's 
memory space and and make the functions that are defined in the code
available to R.  In Windows there 
are 2 distinct operations: loading the shared object and then making the
names of the functions that
the object exports available to applications.

What I am trying to do is create a compiled shared library that is able
to access the SEXP data type.
To do this I wrtie the code in C, run R CMD SHLIB to generate the file
MyOut.so.  I then build a package
with the following function:

.First.lib<-function(lib, pkg){ 
  dyn.load(x="/home/phineas/pkg/CallRFromC/R/MyOut.so")
  library.dynam(chname="MyOut")
}

The use of hard wired paths is to simplify development

When I run R CMD check on the package I get 


* checking S3 generic/method consistency ... WARNING
Error: .First.lib failed for 'CallRFromC'
Call sequence:
2: stop(gettextf(".First.lib failed for '%s'", libraryPkgName(package)),

       domain = NA)
1: library(package, lib.loc = lib.loc, character.only = TRUE, verbose =
FALSE)
Execution halted
See section 'Generic functions and methods' of the 'Writing R
Extensions'
manual.

A little debugging shows that the problem here is caused by
library.dynam as R CMD check runs fine when 
this is exluded.

To be more concrete:

Are both library.dynam and dyn.load necessary?
If they are does it matter in which order they are called?
What different roles do they play in making the compiled functions
available to R?
I notice that many of the packages that use shared objects use
namespaces, is the extra cost of learning
about namespaces justufued by the gains from their use?

Any help or pointers to manuals gratefully received

Phineas Campbell

> version  
         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    2                   
minor    1.1                 
year     2005                
month    06                  
day      20                  
language R



From mmcgee at mail.smu.edu  Thu Jun 30 17:25:07 2005
From: mmcgee at mail.smu.edu (McGee, Monnie)
Date: Thu, 30 Jun 2005 10:25:07 -0500
Subject: [R] R-Aqua can't open history file
Message-ID: <3F58FDDAE273C644B7386E9A3CFCD5A301AEEF54@s31xe5.systems.smu.edu>

Dear List,

I am running R-Aqua 2.1.0 on a powerbook with Mac OS/X 10.3.9.  Here is what I get when I type  "version" at the R-prompt.

platform powerpc-apple-darwin7.9.0
arch     powerpc                  
os       darwin7.9.0              
system   powerpc, darwin7.9.0     
status   Patched                  
major    2                        
minor    1.0                      
year     2005                     
month    05                       
day      12                       
language R  

My problem is that Aqua cannot access any history files.  Therefore, anything I do in R is lost when I close an R session.   It has been doing this since I installed it a month ago.  Here is the message I receive when I double-click on the R-aqua icon on the dock:

2005-06-30 09:49:50.075 R[376] Can't open history file 2

R : Copyright 2005, The R Foundation for Statistical Computing
Version 2.1.0 Patched (2005-05-12), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> 

Thanks for your help.

Monnie


Monnie McGee, Ph.D.
Assistant Professor
Department of Statistical Science
Southern Methodist University
Ph: 214-768-2462
Fax: 214-768-4035



From mnelsonxy at gmail.com  Thu Jun 30 17:29:22 2005
From: mnelsonxy at gmail.com (Matthew Nelson)
Date: Thu, 30 Jun 2005 11:29:22 -0400
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <40e66e0b0506300548188cd468@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
Message-ID: <82659b3c05063008295b71d0e4@mail.gmail.com>

Doug,

I was able to accomplish this for r-help by filtering on the "To"
field with the following addresses:
r-help at stat.math.ethz.ch, r-help at r-project.org

I created this a week ago and it has so far filtered every mailing
list messages successfully.  Gmail conversations are a wonderful way
to catch up on list activity after periods of neglect.

Best regards,
Matt

On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote:
> This is slightly off-topic but I would be interested in whether anyone
> has succeeded in creating a filter expression for Google's gmail
> system that will select messages sent through the R-help and R-devel
> lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
> in the subject line but I haven't been able to work out the exact
> syntax that would do this and not select messages that have an 'R'
> anywhere in the subject.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From JAROSLAW.W.TUSZYNSKI at saic.com  Thu Jun 30 17:30:11 2005
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Thu, 30 Jun 2005 11:30:11 -0400
Subject: [R] Is.integer and testing for integers
Message-ID: <CA0BCF3BED56294AB91E3AD74B849FD57F40BD@us-arlington-0668.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/2d0144d7/attachment.pl

From sundar.dorai-raj at pdf.com  Thu Jun 30 17:39:11 2005
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 30 Jun 2005 10:39:11 -0500
Subject: [R] Is.integer and testing for integers
In-Reply-To: <CA0BCF3BED56294AB91E3AD74B849FD57F40BD@us-arlington-0668.mail.saic.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40BD@us-arlington-0668.mail.saic.com>
Message-ID: <42C4121F.8080206@pdf.com>



Tuszynski, Jaroslaw W. wrote:
>  Hi,
> 
> I was trying to figure out if there is a function in R that tests if R
> object contains only integers. I though "is.integer" would be it, but this
> function only checks "whether its argument is of integer type or not". As a
> result
> 	x = (1:5)^2
> 	is.integer(x)
> Returns false. Of course I can write my own function like
> "!any(x!=as.integer(x))" but I am just trying to make sure I am not
> reinventing the wheel.
> 
> Jarek

Since "^" returns a double, it's no wonder is.integer(x) didn't work as 
you expected. Perhaps you want something more like

is.int <- function(x, tol = .Machine$double.eps) {
   (x - floor(x)) < tol
}
is.int((1:5)^2)

HTH,

--sundar



From f.harrell at vanderbilt.edu  Thu Jun 30 18:03:18 2005
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 30 Jun 2005 11:03:18 -0500
Subject: [R] mApply instead of by ???
In-Reply-To: <42C3F2EB.7060605@ufz.de>
References: <42C3F2EB.7060605@ufz.de>
Message-ID: <42C417C6.20601@vanderbilt.edu>

Marten Winter wrote:

Now that's a brief note.  mApply in the Hmisc package may have large 
speed advantages over by for large datasets.  Also look at the summarize 
function in Hmisc, which uses mApply.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From mnelsonxy at gmail.com  Thu Jun 30 18:17:14 2005
From: mnelsonxy at gmail.com (Matthew Nelson)
Date: Thu, 30 Jun 2005 12:17:14 -0400
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <82659b3c05063008295b71d0e4@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
	<82659b3c05063008295b71d0e4@mail.gmail.com>
Message-ID: <82659b3c050630091798f97af@mail.gmail.com>

A clarification: this only works properly when the "To" addresses are
in separate filters.

Sorry for the confusion.

Matt

On 6/30/05, Matthew Nelson <mnelsonxy at gmail.com> wrote:
> Doug,
> 
> I was able to accomplish this for r-help by filtering on the "To"
> field with the following addresses:
> r-help at stat.math.ethz.ch, r-help at r-project.org
> 
> I created this a week ago and it has so far filtered every mailing
> list messages successfully.  Gmail conversations are a wonderful way
> to catch up on list activity after periods of neglect.
> 
> Best regards,
> Matt
> 
> On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote:
> > This is slightly off-topic but I would be interested in whether anyone
> > has succeeded in creating a filter expression for Google's gmail
> > system that will select messages sent through the R-help and R-devel
> > lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
> > in the subject line but I haven't been able to work out the exact
> > syntax that would do this and not select messages that have an 'R'
> > anywhere in the subject.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From gunter.berton at gene.com  Thu Jun 30 18:31:31 2005
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 30 Jun 2005 09:31:31 -0700
Subject: [R] parsing '...' function argument?
In-Reply-To: <971536df050630043427f29273@mail.gmail.com>
Message-ID: <200506301631.j5UGVWv7025467@ohm.gene.com>

See pages 45 and 46 of V&R's S PROGRAMMING for a definitive discussion of
how to do this.

(Confession: I treat anything that V&R say as definitive).

Cheers,
Bert Gunter
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gabor 
> Grothendieck
> Sent: Thursday, June 30, 2005 4:34 AM
> To: Oleg Sklyar
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] parsing '...' function argument?
> 
> On 6/30/05, Oleg Sklyar <osklyar at ebi.ac.uk> wrote:
> > Dear community,
> > 
> > I am writing a wrapper for '[' operator, which is a generic method
> > declared as function(x, i, j, ..., drop). It turns out that 
> I need to
> > parse the '...' argument and this is where I am stuck. 
> Generally what I
> > need is the following. Say the call is obj[1, 1, 1:10, 3] - 
> here '1:10,
> > 3' is passed into '...'. What I need to evaluate that '...' 
> contains now
> > 2 arguments, first is a vector of 10 elements and second is a single
> > value. Even nicer situation is in call obj[1, 1, , 3] - 
> where ' , 3' is
> > passed in '...' with one missing argument, which for an 
> array would mean
> > the full range. Any ideas? (Just to mention - if there were only one
> > argument in '...', say '1:10', then length(...) would 
> return 10 and one
> > could access elements by ...[[i]], but all this fails if 
> there are two
> > arguments or more).
> > 
> 
> Check out the technique used in this post:
> 
> http://tolstoy.newcastle.edu.au/R/help/04/06/1430.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Jun 30 18:40:49 2005
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Jun 2005 17:40:49 +0100 (BST)
Subject: [R] Is.integer and testing for integers
In-Reply-To: <42C4121F.8080206@pdf.com>
References: <CA0BCF3BED56294AB91E3AD74B849FD57F40BD@us-arlington-0668.mail.saic.com>
	<42C4121F.8080206@pdf.com>
Message-ID: <Pine.LNX.4.61.0506301720550.19557@gannet.stats>

That allows values just bigger than an integer but not just less than an 
integer.  Also, since .Machine$double.eps is a relative tolerance, there 
are no non-integers greater than one meeting the criterion, and the
only non-integers it lets through are like

> is.int(c(1e-20, -1e-20))
[1]  TRUE FALSE

Since integers a lot bigger than allowed by as.integer can be expressed 
exactly as reals, the best suggestion appears to be

(x == round(x))

> x <- 2^c(30, 50, 200)
> x == as.integer(x)
[1] TRUE   NA   NA
Warning message:
NAs introduced by coercion
> x == round(x)
[1] TRUE TRUE TRUE


On Thu, 30 Jun 2005, Sundar Dorai-Raj wrote:

>
>
> Tuszynski, Jaroslaw W. wrote:
>>  Hi,
>>
>> I was trying to figure out if there is a function in R that tests if R
>> object contains only integers. I though "is.integer" would be it, but this
>> function only checks "whether its argument is of integer type or not". As a
>> result
>> 	x = (1:5)^2
>> 	is.integer(x)
>> Returns false. Of course I can write my own function like
>> "!any(x!=as.integer(x))" but I am just trying to make sure I am not
>> reinventing the wheel.
>>
>> Jarek
>
> Since "^" returns a double, it's no wonder is.integer(x) didn't work as
> you expected. Perhaps you want something more like
>
> is.int <- function(x, tol = .Machine$double.eps) {
>   (x - floor(x)) < tol
> }
> is.int((1:5)^2)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Thu Jun 30 19:04:03 2005
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 30 Jun 2005 12:04:03 -0500
Subject: [R] Help with stripplot
In-Reply-To: <2B47B68F97330841AC8C670749084A7D06C506@wdexchmb01.lexicon.lexgen.com>
References: <2B47B68F97330841AC8C670749084A7D06C506@wdexchmb01.lexicon.lexgen.com>
Message-ID: <200506301204.03428.deepayan@stat.wisc.edu>

On Thursday 30 June 2005 11:40, Ghosh, Sandeep wrote:
> Thanks for the prompt reply. I need some more help. I want the datapoints
> for each group to show up in a different color, like all wt data points to
> be green, het to be blue and hom to be red. Also I need the median point of
> all the data points plotted as well by a short line as shown in attachment.

stripchart takes you almost all the way there:


with(dataFrame, 
     stripchart(marbles_buried ~ genotype, 
                vert = TRUE, method = "jitter", 
                col = c('blue', 'red', 'green')))
meds <- as.vector(with(dataFrame, 
                       by(marbles_buried, genotype, median)))
segments((1:3)-0.25, meds, (1:3)+0.25, meds, col = c('blue', 'red', 'green'))


Deepayan



From deepayan.sarkar at gmail.com  Thu Jun 30 19:07:22 2005
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 30 Jun 2005 12:07:22 -0500
Subject: [R] [OT] gmail filter for R-help and R-devel lists
In-Reply-To: <40e66e0b0506300548188cd468@mail.gmail.com>
References: <40e66e0b0506300548188cd468@mail.gmail.com>
Message-ID: <eb555e6605063010075617c608@mail.gmail.com>

On 6/30/05, Douglas Bates <dmbates at gmail.com> wrote:
> This is slightly off-topic but I would be interested in whether anyone
> has succeeded in creating a filter expression for Google's gmail
> system that will select messages sent through the R-help and R-devel
> lists.  It seems as if it should be easy to select on '[R]' or '[Rd]'
> in the subject line but I haven't been able to work out the exact
> syntax that would do this and not select messages that have an 'R'
> anywhere in the subject.

I filter on the To field, which mostly works:

Matches: to:(r-help at stat.math.ethz.ch)
Do this: Skip Inbox, Apply label "r-help"

Matches: to:(r-help at r-project.org)
Do this: Skip Inbox, Apply label "r-help"

Deepayan



From bernhard at pfaffikus.de  Thu Jun 30 19:23:05 2005
From: bernhard at pfaffikus.de (Bernhard Pfaff)
Date: Thu, 30 Jun 2005 19:23:05 +0200
Subject: [R] comparison of packages for Unit Root test
Message-ID: <42C42A79.1040004@pfaffikus.de>

Dear Amir,

two more cents to add to Achim's comment:

1) Some of the functions contained in the fseries package are ported 
from either the tseries or the urca package. Hence, it might be more 
convenient to you by just utilsing the fseries package.

2) A GUI for the Rcmdr package is shipped within the 'inst' directory of 
the urca package. And -- please do not mistake this note as an 
advertisement --, a book about this topic (including a comparison of 
the different implementations) will be published by Springer, N.Y. quite 
soon. For more information see: http://www.pfaffikus.de

HTH,
Bernhard



From ssherman at cemml.colostate.edu  Thu Jun 30 20:46:12 2005
From: ssherman at cemml.colostate.edu (Steve Sherman)
Date: Thu, 30 Jun 2005 12:46:12 -0600
Subject: [R] Trouble with Excel table connection
Message-ID: <42C43DF4.2010102@cemml.colostate.edu>

Hello,

I've been using odbcConnectExcel to connect to a spreadsheet database 
containing a single worksheet. Unfortunately when I try to access the 
data R cannot see the table. When I run sqlTables it shows the worksheet 
as a SYSTEM TABLE instead of as a TABLE that I can access. I am fairly 
certain that I am do all of the same things I've done in the past when 
it worked just fine. What might I be doing wrong?

btw: it works just fine with MS Access, but I'd prefer to work directly 
from my Excel spreadsheets.

Thanks in advance
Steve



From deb37 at columbia.edu  Thu Jun 30 21:21:42 2005
From: deb37 at columbia.edu (Daniel E. Bunker)
Date: Thu, 30 Jun 2005 15:21:42 -0400
Subject: [R] vecortizing uniroot() for numerical solutions
Message-ID: <42C44646.1@columbia.edu>

#    Hi All,
#   
#    I need to solve a somewhat complex equation at many parameter 
values for
#    a number of different parameters.
#    A simplified version of the equation is: 0= (d1/(h1^2))-(h2*(d2^2))
#    I'd like to solve it across a parameter space of d1 and d2, holding
#    h1 and h2 constant.

#     It seems that uniroot() can do it, but I don't see how to 
vectorize it.

#    This works just fine:

R12 = function(d1, d2, h1, h2) (d1/(h1^2))-(h2*(d2^2))
uniroot(R12, interval=c(-100,100), tol=0.0001, d2=1, h1=1, h2=1)

#    but to explore the entire parameter space it seems I would need a 
series of loops

R12 = function(d1, d2, h1, h2) (d1/(h1^2))-(h2*(d2^2))
df1=data.frame(h1=numeric(0), h2=numeric(0),d1=numeric(0),d2=numeric(0))
h1=1
h2=1
for(i in 1:5) {
    output=uniroot(R12, interval=c(-100,100), tol=0.0001, d2=i, h1=1, h2=1)
    d2=i
    df1[i,"h1"]=h1
    df1[i,"h2"]=h2
    df1[i,"d2"]=i
    df1[i,3]=output$root
    }
df1

#    what I'd really like to do is something like:

R12 = function(d1, d2, h1, h2) (d1/(h1^2))-(h2*(d2^2))
uniroot(R12, interval=c(-100,100), tol=0.0001, d2=c(1:5), h1=c(1:5), 
h2=c(1:5))

#    but R rejects the multiple values for d2.

#    I could use a numerical solver such as XPPAUT, but would rather be 
able to do it all in R.

#    Any thoughts on vectorizing this would be greatly appreciated.


#    Thanks!  -Dan

-- 

Daniel E. Bunker
Associate Coordinator - BioMERGE
Post-Doctoral Research Scientist
Columbia University
Department of Ecology, Evolution and Environmental Biology
1020 Schermerhorn Extension
1200 Amsterdam Avenue
New York, NY 10027-5557

212-854-9881
212-854-8188 fax
deb37ATcolumbiaDOTedu



From sghosh at lexgen.com  Thu Jun 30 21:21:38 2005
From: sghosh at lexgen.com (Ghosh, Sandeep)
Date: Thu, 30 Jun 2005 14:21:38 -0500
Subject: [R] Help with stripplot
Message-ID: <2B47B68F97330841AC8C670749084A7D06C508@wdexchmb01.lexicon.lexgen.com>

Another question, in stripchart is there a way to draw a legends. I need legends that gives the mice count for each genotype wt/het/hom, something like the xyplot plot support for key/auto.key.

-Sandeep


-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
Sent: Thursday, June 30, 2005 12:04 PM
To: Ghosh, Sandeep
Cc: Sundar Dorai-Raj; r-help at stat.math.ethz.ch
Subject: Re: [R] Help with stripplot


On Thursday 30 June 2005 11:40, Ghosh, Sandeep wrote:
> Thanks for the prompt reply. I need some more help. I want the datapoints
> for each group to show up in a different color, like all wt data points to
> be green, het to be blue and hom to be red. Also I need the median point of
> all the data points plotted as well by a short line as shown in attachment.

stripchart takes you almost all the way there:


with(dataFrame, 
     stripchart(marbles_buried ~ genotype, 
                vert = TRUE, method = "jitter", 
                col = c('blue', 'red', 'green')))
meds <- as.vector(with(dataFrame, 
                       by(marbles_buried, genotype, median)))
segments((1:3)-0.25, meds, (1:3)+0.25, meds, col = c('blue', 'red', 'green'))


Deepayan



From br44114 at gmail.com  Thu Jun 30 22:08:38 2005
From: br44114 at gmail.com (bogdan romocea)
Date: Thu, 30 Jun 2005 16:08:38 -0400
Subject: [R] Trouble with Excel table connection
Message-ID: <8d5a363505063013085dae054b@mail.gmail.com>

The best 3 things you can do in this situation are:
1. don't use Excel.
2. never use Excel.
3. never ever use Excel again.

Spreadsheets are _not_ databases. In particular, Excel is a time bomb
- use it long enough and you'll get burned (perhaps without even
realizing it). See
http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html for
some details.

hth,
b.


> -----Original Message-----
> From: Steve Sherman [mailto:ssherman at cemml.colostate.edu] 
> Sent: Thursday, June 30, 2005 2:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Trouble with Excel table connection
> 
> 
> Hello,
> 
> I've been using odbcConnectExcel to connect to a spreadsheet database 
> containing a single worksheet. Unfortunately when I try to access the 
> data R cannot see the table. When I run sqlTables it shows 
> the worksheet 
> as a SYSTEM TABLE instead of as a TABLE that I can access. I 
> am fairly 
> certain that I am do all of the same things I've done in the 
> past when 
> it worked just fine. What might I be doing wrong?
> 
> btw: it works just fine with MS Access, but I'd prefer to 
> work directly 
> from my Excel spreadsheets.
> 
> Thanks in advance
> Steve
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From mail at bymouth.com  Thu Jun 30 22:20:01 2005
From: mail at bymouth.com (Stephen Choularton)
Date: Fri, 1 Jul 2005 06:20:01 +1000
Subject: [R] ranking predictive features in logsitic regression
Message-ID: <006d01c57db1$1bbcd250$9701a8c0@Tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050701/ce84331c/attachment.pl

From ssherman at cemml.colostate.edu  Thu Jun 30 22:31:31 2005
From: ssherman at cemml.colostate.edu (Steve Sherman)
Date: Thu, 30 Jun 2005 14:31:31 -0600
Subject: [R] Trouble with Excel table connection
In-Reply-To: <8d5a363505063013085dae054b@mail.gmail.com>
References: <8d5a363505063013085dae054b@mail.gmail.com>
Message-ID: <42C456A3.4090802@cemml.colostate.edu>

That's the conclusion that I've come to. It was working fine, then it 
wasn't.

I know I was using the same commands because I wrote them into a small 
script.

I'll just import it into Access and stop pulling my hair out over this.

Thanks.

bogdan romocea wrote:

>The best 3 things you can do in this situation are:
>1. don't use Excel.
>2. never use Excel.
>3. never ever use Excel again.
>
>Spreadsheets are _not_ databases. In particular, Excel is a time bomb
>- use it long enough and you'll get burned (perhaps without even
>realizing it). See
>http://www.burns-stat.com/pages/Tutor/spreadsheet_addiction.html for
>some details.
>
>hth,
>b.
>
>
>  
>
>>-----Original Message-----
>>From: Steve Sherman [mailto:ssherman at cemml.colostate.edu] 
>>Sent: Thursday, June 30, 2005 2:46 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Trouble with Excel table connection
>>
>>
>>Hello,
>>
>>I've been using odbcConnectExcel to connect to a spreadsheet database 
>>containing a single worksheet. Unfortunately when I try to access the 
>>data R cannot see the table. When I run sqlTables it shows 
>>the worksheet 
>>as a SYSTEM TABLE instead of as a TABLE that I can access. I 
>>am fairly 
>>certain that I am do all of the same things I've done in the 
>>past when 
>>it worked just fine. What might I be doing wrong?
>>
>>btw: it works just fine with MS Access, but I'd prefer to 
>>work directly 
>>from my Excel spreadsheets.
>>
>>Thanks in advance
>>Steve
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From hulubu at gmx.de  Thu Jun 30 23:05:15 2005
From: hulubu at gmx.de (hulubu@gmx.de)
Date: Thu, 30 Jun 2005 23:05:15 +0200 (MEST)
Subject: [R] =?iso-8859-1?q?How_to_rotate_the_axisnames_in_a_BARPLOT?=
Message-ID: <12806.1120165515@www8.gmx.net>

Hi all,

- how can I do a barplot with rotated axis labels? I've seen the example for
just a plot in the FAQ, but I'll missing the coordinates to plot my text at
the right position beneath the bars. 
Is there any (easy?) solution?

- how can I set the y-axis in a barplot to logarithmic scale?

Many thanks in advance!

Best Regards
 Tom

--



From mschwartz at mn.rr.com  Thu Jun 30 23:23:06 2005
From: mschwartz at mn.rr.com (Marc Schwartz (via MN))
Date: Thu, 30 Jun 2005 16:23:06 -0500
Subject: [R] How to rotate the axisnames in a BARPLOT
In-Reply-To: <12806.1120165515@www8.gmx.net>
References: <12806.1120165515@www8.gmx.net>
Message-ID: <1120166587.3750.37.camel@localhost.localdomain>

On Thu, 2005-06-30 at 23:05 +0200, hulubu at gmx.de wrote:
> Hi all,
> 
> - how can I do a barplot with rotated axis labels? I've seen the example for
> just a plot in the FAQ, but I'll missing the coordinates to plot my text at
> the right position beneath the bars. 
> Is there any (easy?) solution?
> 
> - how can I set the y-axis in a barplot to logarithmic scale?
> 
> Many thanks in advance!
> 
> Best Regards
>  Tom

The same concept for rotating axis labels in a regular plot per the FAQ
can be used in a barplot, as long as you know that barplot() returns the
bar midpoints. So:

     ## Increase bottom margin to make room for rotated labels
     par(mar = c(7, 4, 4, 2) + 0.1)

     ## Create plot and get bar midpoints in 'mp'
     mp <- barplot(1:10)

     ## Set up x axis with tick marks alone
     axis(1, at = mp, labels = FALSE)

     ## Create some text labels
     labels <- paste("Label", 1:10, sep = " ")

     ## Plot x axis labels at mp
     text(mp, par("usr")[3] - 0.5, srt = 45, adj = 1,
          labels = labels, xpd = TRUE)

     ## Plot x axis label at line 4
     mtext(1, text = "X Axis Label", line = 4)

See ?barplot for more information.

With respect to the log scales, you would need to use barplot2(), which
is in the 'gplots' package on CRAN. barplot2() supports log scales using
the same 'log' argument as plot().

HTH,

Marc Schwartz



From wilmotb at ohsu.edu  Thu Jun 30 23:24:33 2005
From: wilmotb at ohsu.edu (Beth Wilmot)
Date: Thu, 30 Jun 2005 14:24:33 -0700
Subject: [R] ts.plot data labels
Message-ID: <s2c400ae.071@ohsu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20050630/6e497b1a/attachment.pl

